[
  {
    "title": "SQL-PaLM: Improved Large Language ModelAdaptation for Text-to-SQL",
    "link": "https://arxiv.org/pdf/2306.00739.pdf",
    "upvote": "15",
    "text": "SQL-PALM: IMPROVED LARGE LANGUAGE MODEL\nADAPTATION FOR TEXT-TO-SQL\nRuoxi Sun1, Sercan \u00d6. Arik1, Hootan Nakhost1, Hanjun Dai2, Rajarishi Sinha1, Pengcheng Yin2, Tomas Pfister1\n1 Cloud AI Research Team\n2 Google DeepMind\n{ruoxis, soarik, hootan, hadai, sinharaj, pcyin, tpfister }@google.com\nABSTRACT\nOne impressive emergent capability of large language models (LLMs) is generation of code, including\nStructured Query Language (SQL) for databases. For the task of converting natural language text\nto SQL queries, Text-to-SQL, adaptation of LLMs is of paramount importance, both in in-context\nlearning and fine-tuning settings, depending on the amount of adaptation data used. In this paper,\nwe propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on PaLM-2, that pushes the\nstate-of-the-art in both settings. Few-shot SQL-PaLM is based on an execution-based self-consistency\nprompting approach designed for Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider,\nwhich to our best knowledge is the first to outperform previous state-of-the-art with fine-tuning by a\nsignificant margin, 4%. Furthermore, we demonstrate that the fine-tuned SQL-PALM outperforms\nit further by another 1%. Towards applying SQL-PaLMto real-world scenarios we further evaluate\nits robustness on other challenging variants of Spider and demonstrate the superior generalization\ncapability of SQL-PaLM. In addition, via extensive case studies, we demonstrate the impressive\nintelligent capabilities and various success enablers of LLM-based Text-to-SQL.\nKeywords Text-to-SQL \u00b7 Natural Language Processing \u00b7 Language Parsing\n1\nIntroduction\nText-to-SQL aims to automate the process of generating Structured Query Language (SQL) queries for databases\nfrom natural language text. It is a long-standing challenge, crucial to enhance database accessibility without requiring\nexpertise of SQL. By automating the query generation, Text-to-SQL enables the development of conversational agents\nwith advanced data analytics.\nAs a sequence-to-sequence application, language models can be extended to the Text-to-SQL task. With SQL-specific\ndesigns and domain knowledge, fine-tuning median-sized models, such as T5[1], have achieved strong results. They\nhave dominated the state-of-the-art for a long time, and have been only very recently outperformed by one in-context\nlearning approach [2]. Among the notable ones, PICARD [3] employs incremental parsing to constrain auto-regressive\ndecoding; RASAT [4] integrates database schema relation-aware self-attention and constrained auto-regressive decoders;\nand RESDSQL [5] decouples database schema linking and skeleton parsing using a ranking-enhanced encoding and\nskeleton-aware decoding framework.\nWith their increased model sizes, large language models, such as GPT-3 [6], PaLM [7], and ChatGPT[8, 9], GPT-4[10],\nPaLM-2 [11] have shown significant achievements with zero-shot and few-shot prompting, or in-context learning [12].\nThe advantages of few-shot prompting, compared with fine-tuning, is that no training is involved (lower computation\nrequirements), is unlikely to over-fit to train data, and is easy to adapt to new data, which is particular useful for Text-to-\nSQL as SQL has different dialects; though the disadvantages are the performance can be sub-optimal. For example,\nCodeX [13] and ChatGPT [14] have shown successful results with in-context learning for Text-to-SQL, although they\nstill have a clear gap with the fine-tuned alternatives with median-sized LLMs. Recently, more advanced prompt\ndesign approaches, including multiple-rounds of few-shot prompting (denoted as \u201ccomposite prompting\u201d) have showed\nimpressive results \u2013 for example, MixPrompt combines different formats of prompts for the same task to enhance\narXiv:2306.00739v3  [cs.CL]  25 Jun 2023\nLLM\nSchema 1 + Question 1\nSELECT T1.song_name , T1.song_release_year FROM singer AS T1 ORDER BY T1.age LIMIT 1\nSELECT T1.song_name , T1.song_release_year FROM singer AS T1 JOIN singer_in_concert AS\nT2 ON T1.singer_id = T2.singer_id JOIN concert AS T3 ON T2.concert_id = T3.concert_id\nORDER BY T1.age LIMIT 1\nSELECT song_name , song_release_year FROM singer ORDER BY age ASC LIMIT 1\nSchema 2 + Question 2\nSchema 3 + Question 3\nSQL1\nSQL2\nPrompt\nOutput\nSinger\nSinger_ID\nName\nCountry\nSong\nName\nSong\nrelease yea\nAge\nIs_male\n1\nJoe\nSharp\nNetherland\nYou\n1922\n52\nF\nStadium\nStadium_ID Location\nName\nCapacity\nHighest\nLowest\nAverage\n3\nEast\nFife\nBayview\nStadium\n2000\n1980\n533\n864\nConcert\nConcert_ID\nConcert\n_Name\nTheme\nStadium_ID\nYear\n1\nAuditions\nFree\nChoice\n1\n2014\nSinger_in_concert\nConcert_ID Singer_ID\n1\n2\nSQL3\nLLM\nSchema 3 + Question 3\nSQL3\nB\nWhat are the names and release years for all the\nsongs of the youngest singer?\nOutput samples generated by Few-shot SQL-PaLM:\nSELECT T1.song_name , T1.song_release_year FROM singer AS T1 JOIN singer_in_concert AS\nT2 ON T1.singer_id = T2.singer_id JOIN concert AS T3 ON T2.concert_id = T3.concert_id\nWHERE T1.age = (SELECT min(T4.age) FROM singer AS T4)\n1\n2\n3\n4\nA\nC\nFigure 1: SQL-PaLM: A. Few-shot SQL-PaLM B. Fine-tuned SQL-PaLM C. Left: An example of database schema. Right:\ngenerated SQL outputs by Few-shot SQL-PaLM . The outputs are diverse with the majority of the samples being the 1st or 2nd\nsamples. The 3rd and 4th ones are similar to each other, with only difference being at the end. The 4th one is incorrect due to\nduplicate rows when joining multiple tables (the prediction is [(\u2019Love\u2019, \u20192016\u2019), (\u2019Love\u2019, \u20192016\u2019)], whereas the ground true output:\n[(\u2019Love\u2019, \u20192016\u2019)].)\ndiverse of sampling for consistency [15, 16]. DIN [2], following least-to-most prompting[17], decomposes Text-to-SQL\ntask into sub-tasks (e.g. sub-tasks of schema linking, query classification and decomposition etc.), and applies few-shot\nprompting to solve each sub-tasks with different sub-task prompts. Remarkably, DIN [2] is the first few-shot prompting\nmethod that outperforms fine-tuned SOTA on Test-Suite evaluation. Another effort, \u201cself-debugging\u201d, incorporates\nerror messages in the prompts to encourage LLM to correct errors of the generated SQL outputs through multiple\nfollowing rounds of prompting [18, 2]. Despite interesting perspectives and promising results, composite prompting\nrequires creating multiple handcrafted prompt templates (e.g. for \"sub-tasks\" or \"errors\"), which can be tedious. In\naddition, since the output of one sub-prompt influences the input of another, composite prompts are more sensitive to\nLLMs and test samples, limiting their use to large scale.\nIn this work, we propose SQL-PaLM, a state-of-the-art Text-to-SQL model adopted from PaLM-2 [11] for both\nfew-shot prompting and fine-tuning scenarios. Few-shot SQL-PaLM is an execution-based consistency-decoding\nfew-shot prompting approach with a designed prompt, which outperforms previous fine-tuned SOTA by 3.8% for\ntest-suite accuracy on Spider and latest in-context learning SOTA DIN[2] with composite prompting by 3.1%.\nWhen only comparing with simple-prompt methods, Few-shot SQL-PaLM outperforms few-shot GPT-4 by 9.9%,\nfew-shot CodeX-Davinci by 15.8%, and zero-shot ChatGPT by 17%.\nFurther, to the best of our knowledge,\nthere hasn\u2019t been any prior work on fine-tuning large-sized LLMs such as PaLM-2 on Text-to-SQL tasks. Fine-\ntuned SQL-PaLM is the fine-tuned version on Spider training dataset [19], which outperforms Few-shot SQL-\nPaLM by another 0.9%, improving over the fine-tuned SOTA by 4.7%. We also evaluate the proposed approach\nSQL-PaLM on various challenging Spider variants to demonstrate its robustness. The output of Few-shot SQL-\nPaLM is given https://docs.google.com/spreadsheets/d/1HActPo1PZ-QqjT3eN4VlDXXkoK8-jvAL/edit?\nusp=sharing&ouid=104635556596516643740&rtpof=true&sd=true\n2\nSQL-PaLM\n2.1\nPrimer on LLM adaptation\nLLM\u2019s zero-shot and few-shot learning ability via prompting, known as in-context learning, was first observed in [6].\nBy incorporating a few demonstrations (along with instructions) in the prompt text to serve as the \u201ccontext\", LLMs\nhave been shown to generalize to new examples and new tasks in the same format without any model adaptation. [12]\nshows that the few-shot prompting ability is more prominent for the LLMs with sizes larger than a certain threshold.\nThe remarkable success of in-context learning has inspired the design of advanced prompting strategies such as\n2\nchain-of-thought prompting (CoT) [20], least-to-most prompting [17], and self-consistency-prompting [21]. As a way\nof efficient large-shot adaptation method, FLAN-instruction fine-tuning converts tasks into instructions and fine-tunes\non the multi-tasks instruction datasets mixture [22, 23]. Achieving strong performance often encourages performing\nfurther fine-tuning.\n2.2\nProblem Setup for Text-to-SQL\nThe input of Text-to-SQL is a natural language query question q and the database information Dq. The output is the\nSQL corresponding to the question. The database Dq = {S, Kp, Kf} includes database schema S, primary keys Kp,\nand foreign keys Kf. S usually contains multiple tables Tt: S = {T1, T2, ...Tt...}. Each table Tt has table name Nt,\ncolumn names cj and column data types tj. Therefore, Tk = {Nk, (ck1, tk1), (ck2, tk2), (ckj, tkj)...}). Primary keys\nKp uniquely identify rows of each table, and foreign keys Kf used to combine (\"join\") multiple tables.\n2.3\nFew-shot SQL-PaLM\nIn Few-shot SQL-PaLM, the prompts are designed to prepend the natural questions of interest with a list of demonstra-\ntions (inputs, SQL) pairs, where the inputs are designed to contain text to describe the database and the natural questions\n(Figure 1A). In particular, database information contains data schema (tables, columns and data types), primary and\nforeign keys \u2013 see the prompt example in Appendix 9.1. When the model is asked to predict subsequent tokens of the\ngiven prompt, it follows the prompt format to generate the SQL output corresponding to the question of interest at the\nend of the prompt.\nFor few-shot prompting, one straightforward way to improve performance is to sample LLMs multiple times and\nselect the answers that have the most number of occurrences, i.e. majority (plurality) vote. This \"self-consistency\"\nidea was first proposed in [21]. The rationale is that multiple reasoning paths can lead to the same correct answer,\nso by marginalizing out the reasoning paths to solution, the most consistent answer is likely to be the true one. For\ncoding tasks, execution-based approaches have been shown to improve code generation performance [18, 24]. In this\nwork, we apply execution-based self-consistency for Text-to-SQL. Since multiple different SQLs can be written for\nthe same natural question (Fig. 1C), the rationale of execution-based self-consistency is the most consist execution\noutcomes generated by different SQLs is likely to be the true execution outcome. Therefore, we select the SQL output\ncorresponding to the consistent execution outcome. In particular, we sample Few-shot SQL-PaLM multiple times, with\neach generating an SQL output. Then, the SQL outputs are executed on the database and each SQL output yields an\nexecution outcome or an error. After removing the errors from all the execution outcomes, we perform a majority vote\non them, and select the SQL that outputs the majority execution outcomes.\n2.4\nFine-tuned SQL-PaLM\nLLMs have shown remarkable performance on variety of challenging tasks, such as BIG-bench [25] due to knowledge\ntransferred from large-scale pre-training followed by instruction fine-tuning with multi-diverse tasks (i.e. FLAN-\nfine-tuning). However, it is necessary to adapt LLMs to domain specific Text-to-SQL data, so that LLMs can better\nunderstand the format of prompt and yield further improved results. To the best of our knowledge, we are the first to\nreport results on fine-tuning large-size LLMs on text-to-SQL tasks. We fine-tune PaLM-2 on Spider train split with\ninputs including description of database and natural question (similar with in-context learning), and the output is the\ntarget SQL (see Fig. 1B). To evaluate the robustness of the model Fine-tuned SQL-PaLM (trained on Spider), and we\ntest on other Spider variants, where there are perturbations in natural questions and/or database schema, as a form of\ndistribution shift between train and evaluation.\n3\nExperiments\n3.1\nTasks and datasets\nWe consider the cross-domain large-scale Text-to-SQL benchmark, Spider [19] that contains 7000 training samples\nacross 166 databases and 1034 evaluation samples (\u2018Dev split\u2019) across 20 databases. Spider-SYN [26] is a complex\nvariant of the Spider Dev dataset. It is created through the manual replacement of synonym substitutions in natural\nlanguage questions. Spider-realistic [27] samples 508 text-SQL pairs from Spider Dev Split removing explicit mentions\nof column names in natural language questions. Spider-DK [28] samples 535 question-SQL pairs on 10 databases from\nSpider Dev split and incorporate domain knowledge to them.\n3\n3.2\nModels\nPaLM-2 [11] is a Transformer-based model trained using a mixture of objectives similar to UL2 [29], which is an\nimproved version of its predecessor PaLM [7] by efficiently applying compute-optimal scaling, improved training\ndataset mixture, improved model architecture and objective. The PaLM-2 used here is a Unicorn variant [30] fine-tuned\non a collection of improved datasets mixture phrased as instructions following [23, 22].\n3.3\nBaselines\nFine-tuning baselines: PICARD [3] employs incremental parsing to constrain auto-regressive decoding. RASAT [4] is\na transformer model that integrates relation-aware self-attention and constrained auto-regressive decoders. RESDSQL\n[5] decouples schema linking and skeleton parsing using a ranking-enhanced encoding and skeleton-aware decoding\nframework. In-context learning baselines: [31] comprehensively evaluate the Text-to-SQL ability of CodeX and\nGPT3, while [14] conduct a comprehensive evaluation on ChatGPT [8]. DIN [2] decompose the Text-to-SQL tasks into\nsub-tasks: schema linking, query classification and decomposition, SQL generation, and self-correction; then perform\nfew-shot prompting with GPT-4 [10]. Self-debugging [18] appends error messages to the prompt and performance\nmultiple rounds of few-shot prompting to self-correct the errors. For results showed in Table 2, ChatGPT [14] applies\nthe prompt design approach recommended on OpenAI\u2019s website1. Self-debugging [18] only has EX. [2] only provides\nTS results, so we run EX evaluation with their provided SQL outputs.\n3.4\nEvaluation\nWe consider two commonly-used evaluation metrics: execution accuracy (EX) and test-suite accuracy (TS) [32]. EX\nmeasures whether SQL execution outcome matches ground truth (GT), whereas TS measures whether the SQL passes all\nEX evaluation for multiple tests, generated by database-augmentation. Since EX contains false positives, we consider\nTS as a more reliable evaluation metric, although we provide EX evaluation as it is the conventional measurement.\nExact match evaluation is not performed, as multiple correct SQLs exist for one query. We follow the official evaluation\nprotocol of Spider 2.\n4\nResults\n4.1\nSpider dataset\nWe demonstrate SQL-PaLM results compared to state-of-the-art (SOTA) methods in Table 2.\nSQL-\nPaLM\nexhibits\nstrong\nperformance\non\nText-to-SQL\nfor\nboth\nfew-shot\nprompting\nand\nfine-tuning.\nEX\nTS\nFew-shot SQL-PaLM\n82.7\n77.3\nNo Consistency\n77.3\n72.4 (\u2193 4.9)\nNo Execution Filtering\n79.0\n73.8 (\u2193 3.5)\nTable 1: Ablation Study for Few-shot SQL-\nPaLM . Remove one factor at a time.\nFew-shot SQL-PaLM achieves 77.3% for text suite accuracy on SPIDER\ndev split, which is 3.8% above current fine-tuned SOTA (RESDSQL-3B\n+ NatSQL [5]) and 3.1% above more recent in-context learning SOTA\n(DIN-SQL (w/ GPT-4) [2]). A remarkable observation is that Few-shot\nSQL-PaLM outperforms fine-tuned SOTA, which is trained on Spider\ntraining datasets, and also with a simple prompt, Few-shot SQL-PaLM\ncan outperforms multiple-rounds composite prompts for sub-tasks in [2].\nTo avoid the confounding factors of composite prompt designs, if we only\nevaluate with simple prompt, the Few-shot SQL-PaLM outperforms zero-shot ChatGPT by 17% and few-shot GPT-4 by\n9.9%, and few-shot CodeX-Davinci by 15.8%. Additionally, ablation studies in Table 1 show both that self-consistency\nand execution filtering are important contributors.\nFine-tuned SQL-PaLM achieves 78.2% for test suite accuracy on Spider dev split, which outperforms Few-shot\nSQL-PaLM by 1%, current in-context learning SOTA by 4.7%, and current fine-tuned SOTA by 4%. The major concern\nfor fine-tuned model is over-fitting and whether it generalizes well to other datasets. We evaluate our fine-tuned\nSQL-PALM trained on Spider train split, on other Spider variants (see Sec. 6) to analyze its robustness to text data shifts,\nand demonstrate generalization capabilities.\nRobustness and fewer false positives One observation is that SQL-PALM exhibits a smaller gap between execution\naccuracy (EX) and test suite accuracy (TS). The gap is the result of the generated SQL outputs pass one test contained\nwith original Spider database, but fail on more augmented tests in TS. The gap indeed is due to the number of false\npositives in EX. For example, [5] achieves 84.1% in execution accuracy, but only 73.5% in test suite accuracy, with\n1https://platform.openai.com/examples/default-sql-translate\n2https://yale-lily.github.io/spider\n4\nMethods/Datasets\nSPIDER\nEX\nTS\nFine-tuning\nT5-3B + PICARD [3]\n79.3\n69.4\nRASAT + PICARD [4]\n80.5\n70.3\nRESDSQL-3B + NatSQL [5] (fine-tuned SOTA)\n84.1\n73.5\nFew-shot prompting\nGPT-3 ada (0-shot)[31]\n2.3\n0.3\nGPT-3 babbage (0-shot) [31]\n5.7\n3.9\nGPT-3 curie (0-shot) [31]\n12.6\n8.3\nGPT-3 davinci (0-shot) [31]\n26.3\n21.7\nCodeX cushman (0-shot) [31]\n63.7\n53.0\nCodeX davinci (0-shot)[2]\n67.0\n55.1\nCodeX davinci (few-shot) [2]\n71.0\n61.5\nChatGPT (w/ OpenAI-default Prompt) [14]\n70.1\n60.1\nGPT-4 (Zero-shot) [2]\n72.9\n64.9\nGPT-4 (Few-shot) [2]\n76.8\n67.4\nSelf-Debug [18]\n84.1\n-\nDIN-SQL (w/ CodeX Davinci) [2]\n75.6\n69.9\nDIN-SQL (w/ GPT-4) [2] (few-shot SOTA)\n82.8\n74.2\nFew-shot SQL-PaLM (Ours)\n82.7\n77.3 (\u2191 3.8)\nFine-tuned SQL-PaLM (Ours)\n82.8\n78.2 (\u2191 4.7)\nTable 2: Performance comparison on Test Suite accuracy on Spider Dev Split. Results of DIN, GPT-4, and CodeX\nDavinci is from [2]. GPT3 and the rest of the CodeX results are from [31]. ChatGPT results are from [14].\nroughly 10% plausible but wrong SQL outputs generated, whereas, Few-shot SQL-PaLM only has 5% regression from\n82.7 to 77.3. indicating its superior ability in generating robust SQL outputs.\nPerformance for different difficulty levels We investigate the performance of the proposed approach at varying\ndegrees of difficulty, determined by the count of SQL keywords employed, whether there are nested sub-queries, and if\nthere are any column selections or aggregations utilized. Table 3 displays the results of our proposed method versus a\nbasic few-shot prompting on GPT-4 and CodeX-davinci, and DIN-SQL[2] on Spider\u2019s development set. Our proposed\napproach outperforms alternatives in all difficulty levels with significant improvement, indicating the method does not\nin favor of a category.\nMethods\nModel\nEasy\nMedium\nHard\nExtra Hard\nAll\nFew-shot\nCodeX-davinci\n84.7\n67.3\n47.1\n26.5\n61.5\nFew-shot\nGPT-4\n86.7\n73.1\n59.2\n31.9\n67.4\nDIN-SQL[2]\nCodeX-davinci\n89.1\n75.6\n58.0\n38.6\n69.9\nDIN-SQL[2]\nGPT-4\n91.1\n79.8\n64.9\n43.4\n74.2\nFew-shot SQL-PaLM\nPaLM2\n93.5\n84.8\n62.6\n48.2\n77.3\nFine-tuned SQL-PaLM\nPaLM2\n93.5\n85.2\n68.4\n47.0\n78.2\nTable 3: Test-suite accuracy on Spider development split: SQL outputs are categorized by levels. First two rows are\nstandard few-shot prompting. First four rows are taken from [2]\nPrompt design and zero-shot performance\nPrompt design\nAdaptation setting\nEX\nTS\nConcise\n0-shot\n81.2\n76.0\nVerbose\n0-shot\n78.5\n70.9\nConcise\n4-shot\n82.7\n77.3\nVerbose\n4-shot\n81.3\n73.7\nTable 4: Test-suite accuracy for different prompt design approaches in\nzero- and few-shot set-up on Spider Dev.\nIn Table 4, we analyze the performance of\nFew-shot SQL-PaLM method on queries with\ndifferent prompt designs and different num-\nber of demonstrations (zero- vs. few-shot).\nAs expected, few-shot setup yields better per-\nformance over zero-shot but the gap is ob-\nserved to be small. We also explore the ef-\nfect of different prompt design approaches\non performance. The explored prompt de-\nsign approaches are from [15]: \u201cVerbose\u201d\nprompts are based on using natural language\nto describe database schema, which is closer to the way LLMs were trained, whereas \u201cConcise\u201d prompts use the syntax\n5\nto describe the database schema, which has advantages of clearly presenting table structure. More details are provided\nin Appendix 9.1 and 9.2. For PaLM-2, \u201cConcise\u201d prompts yield superior results.\n5\nQualitative analyses\nWe present case studies of Few-shot SQL-PaLM in Table 5 and 6 for \u201ccorrect\u201d and \u201cwrong\u201d SQL generated by Few-shot\nSQL-PaLM based on test-suite accuracy. Surprisingly, the many examples classified as \"errors\" by Few-shot SQL-PaLM\nwere actually correct when evaluated by human experts, indicating the scores of SQL-PaLM might be higher. The\nevaluation fails due to (1) ambiguous natural questions, exemplified by 1st example in Table 6, and (2) official test-suite\nevaluation struggles on evaluating creative solutions with output which deviate from that of the ground truth. For\ninstance, the 2nd example has multiple valid ground truths; the 3rd example experiences type-related issues; the 4th\nexample presents different formats (e.g. \u201cfull name\u201d and \u201cfirst name, second name\u201d are equally semantically correct for\nthe question. They both should be considered correct); and the 5th example is false negative due to the omission of\nthe \"distinct\" keyword. Regarding the \"real\" mistakes made by Few-shot SQL-PaLM , such as the sixth and seventh\nexamples, we observed a departure from simple errors like syntax errors commonly found in other methods [14].\nInstead, the mistakes made by Few-shot SQL-PaLM closely resemble those that human experts would make when\ndeveloping the same solution, demonstrating its profound expertise in SQL. Another source of errors is the presence of\na \"confusing database schema,\" where Few-shot SQL-PaLM encounters difficulties in selecting the appropriate table or\ncolumn when multiple equivalent options contain similar content (as illustrated in the 5th example of Table 6).\nTables 5 and 6 show the capabilities of Few-shot SQL-PaLM , demonstrating that it can efficiently handle complex SQL\nqueries. It successfully deals with tasks such as joining multiple tables using various keywords (as observed in the 1st,\n2nd, 4th, and 5th examples in Table 5 and all examples in Table 6), as well as employing nested SQL structures (as seen\nin the 3rd example of Table 5). Moreover, Few-shot SQL-PaLM exhibits the ability to generate creative and diverse\nSQL outputs that differ from the ground truth but remain equally correct. This suggests a deep understanding of SQL\ncontent rather than mere memorization. Notable examples include the 3rd example in Table 5 and the 2nd, 3rd, 4th, and\n5th examples in Table 6. Even in cases of errors, such as the 6th and 7th examples in Table 6, Few-shot SQL-PaLM\npresents alternative solutions distinct from the ground truth. Furthermore, Few-shot SQL-PaLM demonstrates the ability\nto infer relevant SQL expression based on semantic meaning, i.e. \"French singers\" and \"country=France,\" as well as\n\"young to old\" and \"OrderBy age ASC\" (as evident in the 1st and 2nd examples). This capability is attributed to the\nlarge-scale pre-training of language models.\n6\nRobustness analyses for real-world scenarios\nEvaluation of robustness of the Text-to-SQL solution is highly important when its usage in real world scenarios is\nconsidered. There could be challenges like the vocabulary used in the natural questions and the database schema being\ndifferent (syndrome replacement); or table containing domain specific knowledge with niche language; or the database\nschema being perturbed. Demonstrating robustness against these can be important for real-world use of Text-to-SQL.\nTowards this end, we select Spider variants over-viewed in Table 7. Compared to the original Spider dataset, Spider-Syn\nand Spider-Realistic modifies natural language questions to eliminate explicit appearance of the database schema, by\nreplacing with synonym or removing explicit mentioning. Spider-DK incorporates domain knowledge into the database\nschema.\nTable 8 compares Few-shot SQL-PaLM and Fine-tuned SQL-PaLM with baseline methods on Spider variants. Although\nprevious work with ChatGPT [14] show few-shot prompting is significantly lower than the SOTA with fine-tuning, we\nfind Few-shot SQL-PaLM demonstrates impressive robust performance, consistently outperforming the SOTA on all\nthree Spider variants. Remarkably, Few-shot SQL-PaLM has 24% improved over ChatGPT and 2.3% improvement on\nprevious fine-tuned SOTA on Spider-Realistic. Also, we find Fine-tuned SQL-PaLM performs surprisingly well with\n3.1% improvement over previous SOTA on Spider-Realistic and 1.6% on Spider-DK. Since Fine-tuned SQL-PaLM is\nbeing evaluated differently with training on Spider, and evaluation on Spider variants, these results further indicate that\nthe fine-tuned SQL-PaLM preserves the superior generalization ability on challenging distribution shift scenarios.\n7\nDiscussions and future work\nThe sampling diversity significantly reduces after fine-tuning\nThe sampling diversity is critical in deciding the\ngain from self-consistency-like approach, which selects the answer with most occurrence outcome among a diverse\npool; However, surprisingly, our experiments show, after fine-tuning, LLM\u2019s sampling output converges to one single\n6\nData Schema\nQuestion\nGround truth\nSQL-PaLM\nComment\nQ1: stadium: Stadium_ID,\nLocation, Name, Capacity,\nHighest, Lowest, Average\nsinger: Singer_ID, Name,\nCountry, Song_Name,\nSong_release_year, Age, Is_male\nconcert: concert_ID,\nconcert_Name, Theme,\nStadium_ID, Year\nsinger_in_concert: concert_ID,\nSinger_ID\nWhat is the average, min-\nimum,\nand\nmaximum\nage of all singers from\nFrance?\nSELECT avg(age) , min(age\n) , max(age) FROM singer\nWHERE country = \u2019France\u2019\nSELECT avg(age) , min(age\n) , max(age) FROM singer\nWHERE country = \"France\"\nInference on the\nrelationship be-\ntween \"French\"\nand \"France\"\nShow name, country, age\nfor all singers ordered by\nage from the oldest to the\nyoungest.\nSELECT name , country ,\nage FROM singer ORDER BY\nage DESC\nSELECT name , country ,\nage FROM singer ORDER BY\nage DESC\nInference based\non\nunderstand-\ning\nthe\nage\nranking via\n\u201corder\nby\nage\nDESC\u201d:\nQ2:\ncity:\nID, Name, Coun-\ntryCode, District, Population\nsqlite_sequence: name, seq\ncountry: Code, Name, Continent,\nRegion, SurfaceArea, IndepYear,\nPopulation, LifeExpectancy, GNP,\nGNPOld, LocalName, Govern-\nmentForm, HeadOfState, Capital,\nCode2\ncountrylanguage: CountryCode,\nLanguage, IsOfficial, Percentage\nWhat are the names of\nnations where both En-\nglish and French are of-\nficial languages?\nSELECT T1.Name FROM\ncountry AS T1 JOIN\ncountrylanguage AS\nT2 ON T1.Code = T2.\nCountryCode WHERE T2.\nLanguage = \"English\"\nAND T2.IsOfficial = \"T\"\nINTERSECT SELECT T1.Name\nFROM country AS T1 JOIN\ncountrylanguage AS T2 ON\nT1.Code = T2.CountryCode\nWHERE T2.Language = \"\nFrench\" AND T2.IsOfficial\n= \"T\"\nSELECT T1.name FROM\ncountry AS T1 JOIN\ncountrylanguage AS\nT2 ON T1.code = T2.\ncountrycode WHERE T2.\nlanguage = \"English\"\nAND T2.isofficial = \"T\"\nINTERSECT SELECT T1.name\nFROM country AS T1 JOIN\ncountrylanguage AS T2 ON\nT1.code = T2.countrycode\nWHERE T2.language = \"\nFrench\" AND T2.isofficial\n= \"T\"\nGenerate\nexact\nGT\nQ3: stadium: Stadium_ID, Lo-\ncation, Name, Capacity, Highest,\nLowest, Average\nsinger:\nSinger_ID,\nName,\nCountry,\nSong_Name,\nSong_release_year, Age, Is_male\nconcert:\nconcert_ID,\ncon-\ncert_Name, Theme, Stadium_ID,\nYear\nsinger_in_concert:\ncon-\ncert_ID,Singer_ID\nWhat are the number\nof concerts that occurred\nin the stadium with the\nlargest capacity ?\nselect count(*) from\nconcert where stadium_id\n= (select stadium_id\nfrom stadium order by\ncapacity desc limit 1)\nSELECT count(*) FROM\nconcert AS T1 JOIN\nstadium AS T2 ON\nT1.stadium_id = T2\n.stadium_id WHERE T2.\ncapacity = ( SELECT max(\nT3.capacity) FROM stadium\nAS T3 )\nCreative \u2013 differ-\nent from ground-\ntruth but accu-\nrate\nQ4:\nStudent:\nStuID, LName,\nFname, Age, Sex, Major, Advisor,\ncity_code\nHas_Pet: StuID, PetID\nPets: PetID, PetType, pet_age,\nweight\nFind the first name of stu-\ndents who have both cat\nand dog pets .\nselect t1.fname from\nstudent as t1 join\nhas_pet as t2 on t1.stuid\n= t2.stuid join pets\nas t3 on t3.petid = t2.\npetid where t3.pettype =\n\u2019cat\u2019 intersect select\nt1.fname from student as\nt1 join has_pet as t2 on\nt1.stuid = t2.stuid join\npets as t3 on t3.petid =\nt2.petid where t3.pettype\n= \u2019dog\u2019\nSELECT T1.fname FROM\nstudent AS T1 JOIN\nhas_pet AS T2 ON T1.stuid\n= T2.stuid JOIN pets\nAS T3 ON T2.petid = T3.\npetid WHERE T3.pettype =\n\"cat\" INTERSECT SELECT\nT1.fname FROM student AS\nT1 JOIN has_pet AS T2 ON\nT1.stuid = T2.stuid JOIN\npets AS T3 ON T2.petid =\nT3.petid WHERE T3.pettype\n= \"dog\"\nComplex\nSQL\ncontaining mul-\ntiple SQL key\nwords.\nQ5: continents: ContId, Conti-\nnent\ncountries: CountryId, Country-\nName, Continent\ncar_makers:\nId, Maker, Full-\nName, Country\nmodel_list:\nModelId,\nMaker,\nModel\ncar_names:\nMakeId,\nModel,\nMake\ncars_data: Id, MPG, Cylinders,\nEdispl, Horsepower, Weight, Ac-\ncelerate, Year\nWhat are the ids and\nnames of all countries\nthat either have more than\n3 car makers or produce\nfiat model ?\nselect t1.countryid ,\nt1.countryname from\ncountries as t1 join\ncar_makers as t2 on t1\n.countryid = t2.country\ngroup by t1.countryid\nhaving count(*) > 3\nunion select t1.countryid\n, t1.countryname from\ncountries as t1 join\ncar_makers as t2 on t1.\ncountryid = t2.country\njoin model_list as t3 on\nt2.id = t3.maker where t3\n.model = \u2019fiat\u2019;\nSELECT T1.countryid ,\nT1.countryname FROM\ncountries AS T1 JOIN\ncar_makers AS T2 ON T1\n.countryid = T2.country\nGROUP BY T1.countryid\nHAVING count(*) > 3\nUNION SELECT T1.countryid\n, T1.countryname FROM\ncountries AS T1 JOIN\ncar_makers AS T2 ON T1.\ncountryid = T2.country\nJOIN model_list AS T3 ON\nT2.id = T3.maker WHERE T3\n.model = \"fiat\"\nComplex\nSQL\ncontaining mul-\ntiple SQL key\nwords\nTable 5: Few-shot SQL-PaLM examples that are correct according to test suite accuracy.\n7\nQuestion 1:\nWhat is the maximum capacity and the average of all stadiums ? (id=16, database=concert_singer)\nGround truth:\nselect max(capacity), average from stadium\nSQL-PALM:\nSELECT max(capacity) , avg(capacity) FROM stadium\nComment:\nAmbiguous question. The Ground truth query would give the average occupancy column of each stadium, where as the SQL-PALM\none would give the average of the capacity column of all stadiums. Without specific context, the SQL-PALM output matches the\nquestion better.\nHuman Expert\nTrue (Ground truth issue)\nQuestion 2:\nWhich airline has most number of flights? (id=229, database=flight_2)\nGround truth:\nSELECT T1.Airline FROM AIRLINES AS T1 JOIN FLIGHTS AS T2 ON T1.uid = T2.Airline GROUP BY T1.Airline\nORDER BY count(*) DESC LIMIT 1\nSQL-PALM:\nSELECT T1.airline FROM airlines AS T1 JOIN flights AS T2 ON T1.uid = T2.airline GROUP BY T2.airline\nORDER BY count(*) DESC LIMIT 1\nComment:\nBoth T1.Airline (the Airlines.Airline column, which contains names of airlines) and T2.airline (the flights.airline\ncolumn, which contains the uid of airlines) uniquely identify airlines, and grouping by either one gives the same results. The evaluation\nfailed because the data contains 12 airlines each with 100 flights (i.e. every one is the maximum), and instability of the ordering produces\ndifferent outcomes when both queries pick one of the maximums.\nHuman Expert\nTrue (Database and Eval issue)\nQuestion 3:\nWhat are the id, name and membership level of visitors who have spent the largest amount of money in total in all museum\ntickets? (id=420, database=museum_visit)\nGround truth:\nSELECT t2.visitor_id , t1.name , t1.Level_of_membership FROM visitor AS t1 JOIN visit AS t2 ON t1.id\n= t2.visitor_id GROUP BY t2.visitor_id ORDER BY sum(t2.Total_spent) DESC LIMIT 1\nExcution result:\n(\u20193\u2019, \u2019Arjen Robben\u2019, 1), where \u20193\u2019 is a string\nSQL-PALM:\nSELECT T1.id , T1.name , T1.level_of_membership FROM visitor AS T1 JOIN visit AS T2 ON T1.id =\nT2.visitor_id GROUP BY T1.id ORDER BY sum(T2.total_spent) DESC LIMIT 1\nExcution result:\n(3, \u2019Arjen Robben\u2019, 1), where 3 is an integer\nComment:\nThe result is actually correct. The highlighted differences t2.visitor_id and t1.id are equivalent because the two tables are joined\non t1.id = t2.visitor_id. Evaluation failed because of the field type differences, as t1.id is int and t2.visitor_id is text.\nHuman Expert\nTrue (Database and Eval issue)\nQuestion 4:\nList the names of all winners who played in both 2013 and 2016. (id=447, database=wta_1)\nGround truth:\nSELECT winner_name FROM matches WHERE YEAR = 2013 INTERSECT SELECT winner_name FROM matches WHERE\nYEAR = 2016\nExcution result:\n(\u2019Angelique Kerber\u2019,), (\u2019Petra Kvitova\u2019,) ...\nSQL-PALM:\nSELECT T1.first_name , T1.last_name FROM players AS T1 JOIN matches AS T2 ON T1.player_id =\nT2.winner_id WHERE T2.year = 2013 INTERSECT SELECT T1.first_name , T1.last_name FROM players AS T1\nJOIN matches AS T2 ON T1.player_id = T2.winner_id WHERE T2.year = 2016\nExecution result:\n(\u2019Angelique\u2019, \u2019Kerber\u2019), (\u2019Petra\u2019, \u2019Kvitova\u2019) ...\nComment:\nThe result is actually correct. The highlighted differences, as both execution results make sense from semantic perspective of the query.\nwinner_name and T1.first_name , T1.last_name are equivalent for representation of \u201cname\". Evaluation failed because of the differences\nin the output format type. If multiple-ground truths are provided considering different output formats, this is not an error.\nHuman Expert\nTrue (Eval issue)\nQuestion 5:\nWhat are the different template type codes? (id=322, database=cre_Doc_Template_Mgt):\nGround truth:\nSELECT DISTINCT template_type_code FROM Templates\nSQL-PALM:\nSELECT DISTINCT template_type_code FROM Ref_Template_Types\nComment:\nThe results are actually correct. Few-shot SQL-PaLM selects a different table Ref_Template_Types, instead of Templates. The same\n\u201ctemplate types\u201d appear in multiple entries of Templates. The two SQL outputs generate the same execution result when evaluating with\n\u201cDISTINCT\"\". Evaluation failed because Spider official evaluation removes \u2018DISTINCT\u2018 during evaluation.\nHuman Expert\nTrue (Eval issue)\nQuestion 6:\nFind the number of professionals who have not treated any dogs. (id=983, database=dog_kennels):\nGround truth:\nSELECT count(*) FROM Professionals WHERE professional_id NOT IN ( SELECT professional_id FROM\nTreatments )\nSQL-PALM:\nSELECT count(*) FROM Professionals EXCEPT SELECT professional_id FROM Treatments\nComment:\nLeft and Right sides of \u201c\u2019EXCEPT\u201d need equivalent content.\nCorrected SQL is SELECT count(*) FROM (SELECT\nprofessional_id FROM Professionals EXCEPT SELECT professional_id FROM Treatments)\nHuman Expert\nFalse (Wrong Use of KEY Words)\nQuestion 7:\nFind the number of professionals who have not treated any dogs. (id=754, database=world_1):\nGround truth:\nselect t1.name from country as t1 join countrylanguage as t2 on t1.code = t2.countrycode where\nt2.language = \"english\" and isofficial = \"t\" union select t1.name from country as t1 join\ncountrylanguage as t2 on t1.code = t2.countrycode where t2.language = \"dutch\" and isofficial = \"t\"\nSQL-PALM:\nSELECT T1.name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.code = T2.countrycode WHERE\nT2.language = \"English\" OR T2.language = \"Dutch\" AND T2.isofficial = \"T\"\nComment:\nOperator Precedence: ADD > OR. Need to add parenthesis over \u201cOR\u201d. Corrected SQL is SELECT T1.name FROM country\nAS T1 JOIN countrylanguage AS T2 ON T1.code = T2.countrycode WHERE (T2.language = \"English\" OR\nT2.language = \"Dutch\" ) AND T2.isofficial = \"T\".\nSpider evaluation normalizes the ground truth\noutputs to all lowercase for easier evaluation, but mismatch exists when referring to database\ncontent.\nChanges:english->English,dutch->Dutch, t->T\nHuman Expert\nFalse (Wrong operator precedence and eval issue)\nTable 6: Few-shot SQL-PaLM outputs that are incorrect according to test suite evaluation. Indeed, according to human\nexperts, the majority of these \u201cerror cases\u201d are actually the correct SQL outputs. More examples are provided in\nAppendix.\n8\nCounts\nModification Category\nSource\nModify\nNatural\nQuestion?\nModify\nDatabase\nSchema?\nAdd\nNew\nDatabase\nSchema?\nExamples\nSpider-\nSYN\n1034\nManually modifying\nnatural language\nquestions with\nsynonym substitutions\nSpider Dev.\nYes\nNo\nNo\nSpider\n# Database Schema: concert_singer\n# stadium(Stadium_ID, Location, Name, Capacity, Highest, Lowest, Average)\n# singer(Singer_ID, Name, Country, Song_Name, Song_release_year, Age, Is_male)\n# concert(concert_ID, concert_Name, Theme, Stadium_ID, Year)\n# singer_in_concert(concert_ID, Singer_ID)\n#\nQ: How many singers do we have?\nSpider-SYN\nQ: How many vocalists do we have?\nSpider-\nRealistic\n508\nModify natural\nlanguage questions\nto remove explicitly\nmentioning column\nnames\nSubset of\nSpider Dev\nYes\nNo\nNo\nSpider\n# Database Schema: concert_singer\nQ: How many concerts are there in year 2014 or 2015?\nQ: How many concerts are there in 2014 or 2015?\n# No year\nSpider-\nDK\n535\nModify database\nschema to\nincorporate the\ndomain knowledge\nSubset of\nSpider Dev\nYes\nYes\nYes\n# Database Schema: concert_singer\nModify database column \"Age\" into \"Birthday\";\nReplace its values from \"52\" to \"1971-02-09 00:00:00\"\nQ: List all song names by singers above the average age.\n# hard to answer \"age\"-related question\nTable 7: Information on different variants of Spider datasets with the purpose of evaluating robustness.\nMethods/Datasets\nSPIDER-SYN\nSPIDER-REALISTIC\nSPIDER-DK\nEX\nTS\nEX\nTS\nEX\nTS\nT5-3B + PICARD\n69.8\n61.8\n71.4\n61.7\n62.5\n-\nRASAT + PICARD\n70.7\n62.4\n71.9\n62.6\n63.9\n-\nRESDSQL-3B + NatSQL (SOTA)\n76.9\n66.8\n81.9\n70.1\n66.0\n-\nChatGPT (OpenAI default Prompt) [14]\n58.6\n48.5 (\u2193 18.3)\n63.4\n49.2 (\u2193 20.9)\n62.6 (\u2193 3.4)\nFew-shot SQL-Palm (Ours)\n74.6\n67.4 (\u2191 0.6)\n77.6\n72.4 (\u2191 2.3)\n66.5 (\u2191 0.5)\n-\nFine-tuned SQL-Palm (Ours)\n70.9\n66.4 (\u2193 0.4)\n77.4\n73.2 (\u2191 3.1)\n67.5 (\u2191 1.6)\n-\nTable 8: Evaluation of SQL-PaLM on Spider variants: Spider-Syn, Spider-Realistic and Spider-DK. Lines 1-4 are taken\nfrom [14]. Spider-DK does not have many more augmented tests, as Spider does so TS is not available.\nanswer even using very high sampling temperature. This indicates after fine-tuning, the model is rather certain with its\nchoice. Because of that, self-consistency decoding is observed not to help much.\nSelf-correction\nDespite promising results of self-correction shown with CodeX and GPT4[2, 18], with PaLM-2, we\nhaven\u2019t observed improvements appending error message to the prompt. We speculate that LLMs needs fine-tuning to\nlearn how to utilize feedback, e.g. with RLHF [33, 34], and to benefit from multiple rounds of additional feedback. We\nleave this to future work.\nWhat is the actual performance of SQL-PaLM according to human experts?\nWe sample a random subset of the\n\u201cerrors\u201d of SQL-PaLM and examine them with human experts. Many \u201cerrors\u201d are actually correct according to human\nexperts. This encourages us to dive deeper into all the error cases manually to understand what the performance of\nSQL-PaLM is according human experts, which may result in a higher accuracy than reported.\n8\nConclusion\nWe propose an LLM-based Text-to-SQL model SQL-PaLM, which leverages on PaLM-2\u2019s few-shot and fine-tuning\nability. We demonstrate significant improvements with a simple prompting approach for Few-shot SQL-PaLM as well\nas Fine-tuned SQL-PaLM . Few-shot SQL-PaLM outperforms previous fine-tuning based SOTA by 3.8% in test suite\naccuracy and latest in-context learning SOTA 3.1%. Few-shot SQL-PaLM employs a simple yet effective prompting\napproach resulting in significant accuracy improvements compared to alternative methods: Few-shot SQL-PaLM\noutperforms few-shot GPT-4 by 9.9%, few-shot CodeX-Davinci by 15.8%, and zero-shot ChatGPT by 17%. For\nFine-tuned SQL-PaLM , the accuracy is even 1% better than Few-shot SQL-PaLM , setting the new SOTA. We also\ndemonstrate qualitative analyses on superior SQL generation capabilities. SQL-PaLM is able to generate complex\nSQL outputs which joins multiple tables with multiple keywords. More interestingly, SQL-PaLM can generate many\nsolutions that are different from ground truth, indicating it has in-depth understanding of the SQL language. SQL-PaLM\nrarely makes simple syntax errors, instead the errors are similar with those human experts would make when developing\na novel solution.\n9\nReferences\n[1] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of\nMachine Learning Research, 21(1):5485\u20135551, 2020.\n[2] Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with\nself-correction. arXiv preprint arXiv:2304.11015, 2023.\n[3] Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. Picard: Parsing incrementally for constrained\nauto-regressive decoding from language models. arXiv preprint arXiv:2109.05093, 2021.\n[4] Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, and\nZhouhan Lin. Rasat: Integrating relational structures into pretrained seq2seq model for text-to-sql. arXiv preprint\narXiv:2205.06983, 2022.\n[5] Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Decoupling the skeleton parsing and schema linking for\ntext-to-sql. arXiv preprint arXiv:2302.05965, 2023.\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances\nin neural information processing systems, 33:1877\u20131901, 2020.\n[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022.\n[8] ChatGPT. https://chat.openai.com/chat, 2023.\n[9] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario\nAmodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information\nProcessing Systems, 33:3008\u20133021, 2020.\n[10] GPT-4 technical report. https://arxiv.org/pdf/2303.08774.pdf, 2023. Accessed: 2023-03-27.\n[11] PaLM2 technical report. https://ai.google/static/documents/palm2techreport.pdf, 2023. Accessed:\n2023-05-10.\n[12] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\nBosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint\narXiv:2206.07682, 2022.\n[13] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code.\narXiv preprint arXiv:2107.03374, 2021.\n[14] Aiwei Liu, Xuming Hu, Lijie Wen, and Philip S Yu. A comprehensive evaluation of chatgpt\u2019s zero-shot text-to-sql\ncapability. arXiv preprint arXiv:2303.13547, 2023.\n[15] Anonymous. SQLPrompt: In-context text-to-sql with minimal labeled data. ARR under-review, 2023.\n[16] Chunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Prompt consistency for\nzero-shot task generalization. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages\n2613\u20132626, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\n[17] Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625,\n2022.\n[18] Xinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. Teaching large language models to self-debug.\narXiv preprint arXiv:2304.05128, 2023.\n[19] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao,\nShanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic\nparsing and text-to-sql task. arXiv preprint arXiv:1809.08887, 2018.\n[20] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of\nthought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n[21] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain\nof thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n10\n[22] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint\narXiv:2210.11416, 2022.\n[23] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai,\nand Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\n[24] Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I Wang. Natural language to code\ntranslation with execution. arXiv preprint arXiv:2204.11454, 2022.\n[25] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R\nBrown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game: Quantifying and\nextrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\n[26] Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R Woodward, Jinxia Xie, and Pengsheng Huang.\nTowards robustness of text-to-sql models against synonym substitution. arXiv preprint arXiv:2106.01065, 2021.\n[27] Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, and Matthew Richard-\nson. Structure-grounded pretraining for text-to-sql. arXiv preprint arXiv:2010.12773, 2020.\n[28] Yujian Gan, Xinyun Chen, and Matthew Purver. Exploring underexplored limitations of cross-domain text-to-sql\ngeneralization. arXiv preprint arXiv:2109.05157, 2021.\n[29] Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri,\nTal Schuster, Steven Zheng, et al. Ul2: Unifying language learning paradigms. In The Eleventh International\nConference on Learning Representations, 2022.\n[30] PaLM-2\ngoogle\nai\nblog.\nhttps://blog.google/technology/ai/\ngoogle-palm-2-ai-large-language-model/, 2023. Accessed: 2023-05-10.\n[31] Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. Evaluating the text-to-sql capabilities of large language\nmodels. arXiv preprint arXiv:2204.00498, 2022.\n[32] Ruiqi Zhong, Tao Yu, and Dan Klein. Semantic evaluation for text-to-sql with distilled test suites. arXiv preprint\narXiv:2010.02840, 2020.\n[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human\nfeedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\n[34] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and\nGeoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.\n11\n9\nSupplementary Material\n9.1\nConcise Prompt Design: 4 shot\nThis is a task converting text into SQL statement. We will first given the dataset schema and then ask a question in\ntext. You are asked to generate SQL statement.\nHere is an example: Convert text to SQL:\n[Schema (values)]:\n| farm | city : city_id , official_name , status , area_km_2 , population ,\ncensus_ranking | farm : farm_id , year , total_horses , working_horses , total_cattle , oxen\n, bulls , cows , pigs , sheep_and_goats | farm_competition : competition_id , year , theme ,\nhost_city_id , hosts | competition_record : competition_id , farm_id , rank;\n[Column names (type)]:\ncity : city_id (number)| city : official_name (text)| city : status (text\n)| city : area_km_2 (number)| city : population (number)| city : census_ranking (text)| farm\n: farm_id (number)| farm : year (number)| farm : total_horses (number)| farm : working_horses\n(number)| farm : total_cattle (number)| farm : oxen (number)| farm : bulls (number)| farm\n: cows (number)| farm : pigs (number)| farm : sheep_and_goats (number)| farm_competition :\ncompetition_id (number)| farm_competition : year (number)| farm_competition : theme (text)|\nfarm_competition : host_city_id (number)| farm_competition : hosts (text)| competition_record\n: competition_id (number)| competition_record : farm_id (number)| competition_record : rank (\nnumber);\n[Primary Keys]:\ncity : city_id | farm : farm_id | farm_competition : competition_id |\ncompetition_record : competition_id;\n[Foreign Keys]:\nfarm_competition : host_city_id equals city : city_id | competition_record :\nfarm_id equals farm : farm_id | competition_record : competition_id equals farm_competition :\ncompetition_id\n[Q]: What are the themes of farm competitions sorted by year in ascending order?;\n[SQL]: select theme from farm_competition order by year asc;\nHere is an example: Convert text to SQL:\n[Schema (values)]:\n| farm | city : city_id , official_name , status , area_km_2 , population ,\ncensus_ranking | farm : farm_id , year , total_horses , working_horses , total_cattle , oxen\n, bulls , cows , pigs , sheep_and_goats | farm_competition : competition_id , year , theme ,\nhost_city_id , hosts | competition_record : competition_id , farm_id , rank;\n[Column names (type)]:\ncity : city_id (number)| city : official_name (text)| city : status (text\n)| city : area_km_2 (number)| city : population (number)| city : census_ranking (text)| farm\n: farm_id (number)| farm : year (number)| farm : total_horses (number)| farm : working_horses\n(number)| farm : total_cattle (number)| farm : oxen (number)| farm : bulls (number)| farm\n: cows (number)| farm : pigs (number)| farm : sheep_and_goats (number)| farm_competition :\ncompetition_id (number)| farm_competition : year (number)| farm_competition : theme (text)|\nfarm_competition : host_city_id (number)| farm_competition : hosts (text)| competition_record\n: competition_id (number)| competition_record : farm_id (number)| competition_record : rank (\nnumber);\n[Primary Keys]:\ncity : city_id | farm : farm_id | farm_competition : competition_id |\ncompetition_record : competition_id; [Foreign Keys]: farm_competition : host_city_id equals\ncity : city_id | competition_record : farm_id equals farm : farm_id | competition_record :\ncompetition_id equals farm_competition : competition_id\n[Q]: What are the maximum and minimum number of cows across all farms.;\n[SQL]: select max(cows), min(cows) from farm;\nHere is an example: Convert text to SQL:\n[Schema (values)]:\n| department_management | department : department_id , name , creation ,\nranking , budget_in_billions , num_employees | head : head_id , name , born_state , age |\nmanagement : department_id , head_id , temporary_acting ( Yes );\n12\n[Column names (type)]:\ndepartment : department_id (number)| department : name (text)| department\n: creation (text)| department : ranking (number)| department : budget_in_billions (number)\n| department : num_employees (number)| head : head_id (number)| head : name (text)| head :\nborn_state (text)| head : age (number)| management : department_id (number)| management :\nhead_id (number)| management : temporary_acting (text);\n[Primary Keys]: department : department_id | head : head_id | management : department_id;\n[Foreign Keys]:\nmanagement : head_id equals head : head_id | management : department_id equals\ndepartment : department_id\n[Q]: Show the name and number of employees for the departments managed by heads whose temporary\nacting value is \u2019Yes\u2019?;\n[SQL]:\nselect t1.name, t1.num_employees from department as t1 join management as t2 on t1.\ndepartment_id = t2.department_id where t2.temporary_acting = \u2019Yes\u2019;\nHere is an example: Convert text to SQL:\n[Schema (values)]:\n| farm | city : city_id , official_name , status , area_km_2 , population ,\ncensus_ranking | farm : farm_id , year , total_horses , working_horses , total_cattle , oxen\n, bulls , cows , pigs , sheep_and_goats | farm_competition : competition_id , year , theme ,\nhost_city_id , hosts | competition_record : competition_id , farm_id , rank;\n[Column names (type)]:\ncity : city_id (number)| city : official_name (text)| city : status (text\n)| city : area_km_2 (number)| city : population (number)| city : census_ranking (text)| farm\n: farm_id (number)| farm : year (number)| farm : total_horses (number)| farm : working_horses\n(number)| farm : total_cattle (number)| farm : oxen (number)| farm : bulls (number)| farm\n: cows (number)| farm : pigs (number)| farm : sheep_and_goats (number)| farm_competition :\ncompetition_id (number)| farm_competition : year (number)| farm_competition : theme (text)|\nfarm_competition : host_city_id (number)| farm_competition : hosts (text)| competition_record\n: competition_id (number)| competition_record : farm_id (number)| competition_record : rank (\nnumber);\n[Primary Keys]:\ncity : city_id | farm : farm_id | farm_competition : competition_id |\ncompetition_record : competition_id;\n[Foreign Keys]:\nfarm_competition : host_city_id equals city : city_id | competition_record :\nfarm_id equals farm : farm_id | competition_record : competition_id equals farm_competition :\ncompetition_id\n[Q]: Show the status of the city that has hosted the greatest number of competitions.;\n[SQL]:\nselect t1.status from city as t1 join farm_competition as t2 on t1.city_id = t2.\nhost_city_id group by t2.host_city_id order by count(*) desc limit 1;\nHere is the test question to be answered: Convert text to SQL:\n[Schema (values)]: | concert_singer | stadium : stadium_id , location , name , capacity , highest\n, lowest , average | singer : singer_id , name , country , song_name , song_release_year\n, age , is_male | concert : concert_id , concert_name , theme , stadium_id , year |\nsinger_in_concert : concert_id , singer_id;\n[Column names (type)]:\nstadium : stadium_id (number)| stadium : location (text)| stadium : name\n(text)| stadium : capacity (number)| stadium : highest (number)| stadium : lowest (number)|\nstadium : average (number)| singer : singer_id (number)| singer : name (text)| singer : country\n(text)| singer : song_name (text)| singer : song_release_year (text)| singer : age (number)\n| singer : is_male (others)| concert : concert_id (number)| concert : concert_name (text)|\nconcert : theme (text)| concert : stadium_id (text)| concert : year (text)| singer_in_concert :\nconcert_id (number)| singer_in_concert : singer_id (text);\n[Primary Keys]:\nstadium : stadium_id | singer : singer_id | concert : concert_id |\nsinger_in_concert : concert_id;\n[Foreign Keys]: concert : stadium_id equals stadium : stadium_id | singer_in_concert : singer_id\nequals singer : singer_id | singer_in_concert : concert_id equals concert : concert_id\n13\n[Q]: How many singers do we have?;\n[SQL]:\n9.2\nVerbose Pompt Design: 4 shot\nThis is a task\nconverting\ntext into SQL\nstatement. We will\nfirst\ngiven the\ndataset\nschema and then ask a question in text. You are asked to generate\nSQL\nstatement.\nHere is an example: Let us take a question\nand turn it into a SQL\nstatement\nabout\ndatabase\ntables.\nThere are 4 tables\n. Their\ntitles are: city , farm , farm_competition , competition_record . Table\n1 is city , and its column\nnames and types are: City_ID (Type is number),\nOfficial_Name (Type is text), Status (Type is text), Area_km_2 (Type is\nnumber), Population (Type is number), Census_Ranking (Type is text). Table\n2 is farm , and its column\nnames and types are: Farm_ID (Type is number),\nYear (Type is number), Total_Horses (Type is number), Working_Horses (Type\nis number), Total_Cattle (Type is number), Oxen (Type is number), Bulls (\nType is number), Cows (Type is number), Pigs (Type is number),\nSheep_and_Goats (Type is number). Table 3 is farm_competition , and its\ncolumn\nnames and types are: Competition_ID (Type is number), Year (Type is\nnumber), Theme (Type is text), Host_city_ID (Type is number), Hosts (Type\nis text). Table 4 is competition_record , and its column\nnames and types are\n: Competition_ID (Type is number), Farm_ID (Type is number), Rank (Type is\nnumber).\nThe primary keys are: city_id\nfrom\nTable city , farm_id\nfrom\nTable farm ,\ncompetition_id\nfrom\nTable\nfarm_competition , competition_id\nfrom\nTable\ncompetition_record .\nThe foreign keys are: host_city_id\nfrom\nTable\nfarm_competition is equivalent\nwith\ncity_id\nfrom\nTable city , farm_id\nfrom\nTable\ncompetition_record is\nequivalent\nwith\nfarm_id\nfrom\nTable farm , competition_id\nfrom\nTable\ncompetition_record\nis equivalent\nwith\ncompetition_id\nfrom\nTable\nfarm_competition . Use\nforeign\nkeys to join\nTables.\nLet us take a text\nquestion\nand turn it into a SQL\nstatement\nabout\ndatabase\ntables. The\nquestion is: What are the themes of farm\ncompetitions\nsorted by year in\nascending\norder? The\ncorresponding\nSQL is: SELECT\nTheme\nFROM\nfarm_competition\nORDER BY YEAR ASC;\nHere is an example: Let us take a question\nand turn it into a SQL\nstatement\nabout\ndatabase\ntables.\nThere are 4 tables\n. Their\ntitles are: city , farm , farm_competition , competition_record . Table\n1 is city , and its column\nnames and types are: City_ID (Type is number),\nOfficial_Name (Type is text), Status (Type is text), Area_km_2 (Type is\nnumber), Population (Type is number), Census_Ranking (Type is text). Table\n2 is farm , and its column\nnames and types are: Farm_ID (Type is number),\nYear (Type is number), Total_Horses (Type is number), Working_Horses (Type\nis number), Total_Cattle (Type is number), Oxen (Type is number), Bulls (\nType is number), Cows (Type is number), Pigs (Type is number),\nSheep_and_Goats (Type is number). Table 3 is farm_competition , and its\ncolumn\nnames and types are: Competition_ID (Type is number), Year (Type is\nnumber), Theme (Type is text), Host_city_ID (Type is number), Hosts (Type\nis text). Table 4 is competition_record , and its column\nnames and types are\n: Competition_ID (Type is number), Farm_ID (Type is number), Rank (Type is\nnumber).\nThe primary keys are: city_id\nfrom\nTable city , farm_id\nfrom\nTable farm ,\ncompetition_id\nfrom\nTable\nfarm_competition , competition_id\nfrom\nTable\ncompetition_record .\nThe foreign keys are: host_city_id\nfrom\nTable\nfarm_competition is equivalent\nwith\ncity_id\nfrom\nTable city , farm_id\nfrom\nTable\ncompetition_record is\nequivalent\nwith\nfarm_id\nfrom\nTable farm , competition_id\nfrom\nTable\ncompetition_record\nis equivalent\nwith\ncompetition_id\nfrom\nTable\nfarm_competition . Use\nforeign\nkeys to join\nTables.\nLet us take a text\nquestion\nand turn it into a SQL\nstatement\nabout\ndatabase\ntables. The\n14\nquestion is: What are the\nmaximum\nand\nminimum\nnumber of cows\nacross all\nfarms. The\ncorresponding\nSQL is: SELECT max(Cows) ,\nmin(Cows) FROM farm;\nHere is an example: Let us take a question\nand turn it into a SQL\nstatement\nabout\ndatabase\ntables.\nThere are 3 tables\n. Their\ntitles are: department , head , management. Table 1 is department ,\nand its column\nnames and types are: Department_ID (Type is number), Name (\nType is text), Creation (Type is text), Ranking (Type is number),\nBudget_in_Billions (Type is number), Num_Employees (Type is number). Table\n2 is head , and its column\nnames and types are: head_ID (Type is number),\nname (Type is text), born_state (Type is text), age (Type is number). Table\n3 is management , and its column\nnames and types are: department_ID (Type\nis number), head_ID (Type is number), temporary_acting (Type is text).\nThe primary keys are: department_id\nfrom\nTable\ndepartment , head_id\nfrom\nTable\nhead , department_id\nfrom\nTable\nmanagement.\nThe foreign keys are: head_id\nfrom\nTable\nmanagement is equivalent\nwith\nhead_id\nfrom\nTable head , department_id\nfrom\nTable\nmanagement is equivalent\nwith\ndepartment_id\nfrom\nTable\ndepartment. Use\nforeign\nkeys to join\nTables.\nColumns\nwith\nrelevant\nvalues: Table\nmanagement\nColumn\ntemporary_acting\nhave\nvalues: Yes;\nOnly use\ncolumns\nwith\nrelevant\nvalues to generate\nSQL.\nLet\nus take a text\nquestion\nand turn it into a SQL\nstatement\nabout\ndatabase\ntables. The\nquestion is: Show the name and number of employees\nfor the\ndepartments\nmanaged by heads\nwhose\ntemporary\nacting\nvalue is \u2019Yes \u2019? The\ncorresponding\nSQL is: SELECT T1.name ,\nT1. num_employees\nFROM\ndepartment AS\nT1 JOIN\nmanagement AS T2 ON T1. department_id\n=\nT2. department_id\nWHERE T2\n. temporary_acting\n=\n\u2019Yes \u2019;\nHere is an example: Let us take a question\nand turn it into a SQL\nstatement\nabout\ndatabase\ntables.\nThere are 4 tables\n. Their\ntitles are: city , farm , farm_competition , competition_record . Table\n1 is city , and its column\nnames and types are: City_ID (Type is number),\nOfficial_Name (Type is text), Status (Type is text), Area_km_2 (Type is\nnumber), Population (Type is number), Census_Ranking (Type is text). Table\n2 is farm , and its column\nnames and types are: Farm_ID (Type is number),\nYear (Type is number), Total_Horses (Type is number), Working_Horses (Type\nis number), Total_Cattle (Type is number), Oxen (Type is number), Bulls (\nType is number), Cows (Type is number), Pigs (Type is number),\nSheep_and_Goats (Type is number). Table 3 is farm_competition , and its\ncolumn\nnames and types are: Competition_ID (Type is number), Year (Type is\nnumber), Theme (Type is text), Host_city_ID (Type is number), Hosts (Type\nis text). Table 4 is competition_record , and its column\nnames and types are\n: Competition_ID (Type is number), Farm_ID (Type is number), Rank (Type is\nnumber).\nThe primary keys are: city_id\nfrom\nTable city , farm_id\nfrom\nTable farm ,\ncompetition_id\nfrom\nTable\nfarm_competition , competition_id\nfrom\nTable\ncompetition_record .\nThe foreign keys are: host_city_id\nfrom\nTable\nfarm_competition is equivalent\nwith\ncity_id\nfrom\nTable city , farm_id\nfrom\nTable\ncompetition_record is\nequivalent\nwith\nfarm_id\nfrom\nTable farm , competition_id\nfrom\nTable\ncompetition_record\nis equivalent\nwith\ncompetition_id\nfrom\nTable\nfarm_competition . Use\nforeign\nkeys to join\nTables.\nLet us take a text\nquestion\nand turn it into a SQL\nstatement\nabout\ndatabase\ntables. The\nquestion is: Show the status of the city that has hosted the\ngreatest\nnumber of competitions. The\ncorresponding\nSQL is: SELECT T1.Status\nFROM\ncity AS T1 JOIN\nfarm_competition AS T2 ON T1.City_ID\n=\nT2. Host_city_ID\nGROUP BY T2.Host_city_ID\nORDER BY COUNT (*) DESC\nLIMIT 1;\nHere is the test question to be answered: Let us take a question\nand turn it into a\nSQL\nstatement\nabout\ndatabase\ntables.\nThere are 4 tables. Their\ntitles are: stadium , singer , concert ,\nsinger_in_concert . Table 1 is stadium , and its column\nnames and types are:\nStadium_ID (Type is number), Location (Type is text), Name (Type is text)\n, Capacity (Type is number), Highest (Type is number), Lowest (Type is\nnumber), Average (Type is number). Table 2 is singer , and its column\nnames\nand types are: Singer_ID (Type is number), Name (Type is text), Country (\nType is text), Song_Name (Type is text), Song_release_year (Type is text),\n15\nAge (Type is number), Is_male (Type is others). Table 3 is concert , and\nits column\nnames and types are: concert_ID (Type is number), concert_Name\n(Type is text), Theme (Type is text), Stadium_ID (Type is text), Year (\nType is text). Table 4 is singer_in_concert , and its column\nnames and\ntypes are: concert_ID (Type is number), Singer_ID (Type is text).\nThe primary keys are: stadium_id\nfrom\nTable stadium , singer_id\nfrom\nTable\nsinger , concert_id\nfrom\nTable concert , concert_id\nfrom\nTable\nsinger_in_concert .\nThe foreign keys are: stadium_id\nfrom\nTable\nconcert is equivalent\nwith\nstadium_id\nfrom\nTable stadium , singer_id\nfrom\nTable\nsinger_in_concert is\nequivalent\nwith\nsinger_id\nfrom\nTable singer , concert_id\nfrom\nTable\nsinger_in_concert is equivalent\nwith\nconcert_id\nfrom\nTable\nconcert. Use\nforeign\nkeys to join\nTables.\nLet us take a text\nquestion\nand turn it into\na SQL\nstatement\nabout\ndatabase\ntables. The\nquestion is: How many\nsingers\ndo we have? The\ncorresponding\nSQL is:\n16\n"
  },
  {
    "title": "SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds",
    "link": "https://arxiv.org/pdf/2306.00980.pdf",
    "upvote": "13",
    "text": "SnapFusion: Text-to-Image Diffusion Model on Mobile\nDevices within Two Seconds\nYanyu Li1,2,\u2020\nHuan Wang1,2,\u2020\nQing Jin1,\u2020\nJu Hu1\nPavlo Chemerys1\nYun Fu2\nYanzhi Wang2\nSergey Tulyakov1\nJian Ren1,\u2020\n1Snap Inc.\n2Northeastern University\nProject Page: https://snap-research.github.io/SnapFusion\nFigure 1: Example generated images by using our efficient text-to-image diffusion model.\nAbstract\nText-to-image diffusion models can create stunning images from natural language\ndescriptions that rival the work of professional artists and photographers. However,\nthese models are large, with complex network architectures and tens of denoising\niterations, making them computationally expensive and slow to run. As a result,\nhigh-end GPUs and cloud-based inference are required to run diffusion models at\nscale. This is costly and has privacy implications, especially when user data is sent\nto a third party. To overcome these challenges, we present a generic approach that,\nfor the first time, unlocks running text-to-image diffusion models on mobile devices\nin less than 2 seconds. We achieve so by introducing efficient network architecture\nand improving step distillation. Specifically, we propose an efficient UNet by\nidentifying the redundancy of the original model and reducing the computation of\nthe image decoder via data distillation. Further, we enhance the step distillation\nby exploring training strategies and introducing regularization from classifier-free\nguidance. Our extensive experiments on MS-COCO show that our model with 8\ndenoising steps achieves better FID and CLIP scores than Stable Diffusion v1.5\nwith 50 steps. Our work democratizes content creation by bringing powerful\ntext-to-image diffusion models to the hands of users.\n1\nIntroduction\nDiffusion-based text-to-image models [1, 2, 3, 4] show remarkable progress in synthesizing pho-\ntorealistic content using text prompts. They profoundly impact the content creation [5, 6], image\n\u2020Equal contribution.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.00980v3  [cs.CV]  16 Oct 2023\nediting and in-painting [7, 8, 9, 10, 11], super-resolution [12], video synthesis [13, 14], and 3D assets\ngeneration [15, 16, 17], to name a few. This impact comes at the cost of the substantial increase in the\ncomputation requirements to run such models [18, 19, 20, 21]. As a result, to satisfy the necessary\nlatency constraints large scale, often cloud-based inference platforms with high-end GPU are required.\nThis incurs high costs and brings potential privacy concerns, motivated by the sheer fact of sending\nprivate images, videos, and prompts to a third-party service.\nNot surprisingly, there are emerging efforts to speed up the inference of text-to-image diffusion\nmodels on mobile devices. Recent works use quantization [22, 23] or GPU-aware optimization\nto reduce the run time, i.e., accelerating the diffusion pipeline to 11.5s on Samsung Galaxy S23\nUltra [24]. While these methods effectively achieve a certain speed-up on mobile platforms, the\nobtained latency does not allow for a seamless user experience. Besides, none of the existing studies\nsystematically examine the generation quality of on-device models through quantitative analysis.\nIn this work, we present the first text-to-image diffusion model that generates an image on mobile\ndevices in less than 2 seconds. To achieve this, we mainly focus on improving the slow inference speed\nof the UNet and reducing the number of necessary denoising steps. First, the architecture of UNet,\nwhich is the major bottleneck for the conditional diffusion model (as we show in Tab. 1), is rarely\noptimized in the literature. Existing works primarily focus on post-training optimizations [25, 26].\nConventional compression techniques, e.g., model pruning [27, 28] and architecture search [29, 30],\nreduce the performance of pre-trained diffusion models [31], which is difficult to recover without\nheavy fine-tuning. Consequently, the architecture redundancies are not fully exploited, resulting in\na limited acceleration ratio. Second, the flexibility of the denoising diffusion process is not well\nexplored for the on-device model. Directly reducing the number of denoising steps impacts the\ngenerative performance, while progressively distilling the steps can mitigate the impacts [32, 33].\nHowever, the learning objectives for step distillation and the strategy for training the on-device model\nhave yet to be thoroughly studied, especially for models trained using large-scale datasets.\nThis work proposes a series of contributions to address the aforementioned challenges:\n\u2022 We provide an in-depth analysis of the denoising UNet and identify the architecture redundancies.\n\u2022 We propose a novel evolving training framework to obtain an efficient UNet that performs better\nthan the original Stable Diffusion v1.51 while being significantly faster. We also introduce a data\ndistillation pipeline to compress and accelerate the image decoder.\n\u2022 We improve the learning objective during step distillation by proposing additional regularization,\nincluding losses from the v-prediction and classifier-free guidance [34].\n\u2022 Finally, we explore the training strategies for step distillation, especially the best teacher-student\nparadigm for training the on-device model.\nThrough the improved Step distillation and network architecture development for the difFusion\nmodel, our introduced model, SnapFusion, generates a 512 \u00d7 512 image from the text on mobile\ndevices in less than 2 seconds, while with image quality similar to Stable Diffusion v1.5 [4] (see\nexample images from our approach in Fig. 1).\n2\nModel Analysis of Stable Diffusion\n2.1\nPrerequisites of Stable Diffusion\nDiffusion Models gradually convert the sample x from a real data distribution pdata(x) into a noisy\nversion, i.e., the diffusion process, and learn to reverse this process by denoising the noisy data step\nby step [35]. Therefore, the model transforms a simple distribution, e.g., random Gaussian noise, to\nthe desired more complicated distribution, e.g., real images. Specifically, given a (noise-prediction)\ndiffusion model \u02c6\u03f5\u03b8(\u00b7) parameterized by \u03b8, which is typically structured as a UNet [36, 1], the training\ncan be formulated as the following noise prediction problem [35, 1, 2]:\nmin\n\u03b8\nEt\u223cU[0,1],x\u223cpdata(x),\u03f5\u223cN (0,I) ||\u02c6\u03f5\u03b8(t, zt) \u2212 \u03f5||2\n2,\n(1)\nwhere t refers to the time step; \u03f5 is the ground-truth noise; zt = \u03b1tx + \u03c3t\u03f5 is the noisy data; \u03b1t\nand \u03c3t are the strengths of signal and noise, respectively, decided by a noise scheduler. A trained\n1https://github.com/runwayml/stable-diffusion\n2\ndiffusion model can generate samples from noise with various samplers. In our experiments, we use\nDDIM [37] to sample with the following iterative denoising process from t to a previous time step t\u2032,\nzt\u2032 = \u03b1t\u2032 zt \u2212 \u03c3t\u02c6\u03f5\u03b8(t, zt)\n\u03b1t\n+ \u03c3t\u2032 \u02c6\u03f5\u03b8(t, zt),\n(2)\nwhere zt\u2032 will be fed into \u02c6\u03f5\u03b8(\u00b7) again until t\u2032 becomes 0, i.e., the denoising process finishes.\nLatent Diffusion Model / Stable Diffusion. The recent latent diffusion model (LDM) [4] reduces\nthe inference computation and steps by performing the denoising process in the latent space, which\nis encoded from a pre-trained variational autoencoder (VAE) [38, 39]. During inference, the image is\nconstructed through the decoder from the latent. LDM also explores the text-to-image generation,\nwhere a text prompt embedding c is fed into the diffusion model as the condition. When synthesizing\nimages, an important technique, classifier-free guidance (CFG) [34], is adopted to improve quality,\n\u02dc\u03f5\u03b8(t, zt, c) = w\u02c6\u03f5\u03b8(t, zt, c) \u2212 (w \u2212 1)\u02c6\u03f5\u03b8(t, zt, \u2205),\n(3)\nwhere \u02c6\u03f5\u03b8(t, zt, \u2205) represents the unconditional output obtained by using null text \u2205. The guidance\nscale w can be adjusted to control the strength of conditional information on the generated images to\nachieve the trade-off between quality and diversity. LDM is further trained on large-scale datasets [40],\ndelivering a series of Stable Diffusion (SD) models [4]. We choose Stable Diffusion v1.5 (SD-v1.5)\nas the baseline. Next, we perform detailed analyses to diagnose the latency bottleneck of SD-v1.5.\n2.2\nBenchmark and Analysis\nHere we comprehensively study the parameter and computation intensity of the SD-v1.5. The in-depth\nanalysis helps us understand the bottleneck to deploying text-to-image diffusion models on mobile\ndevices from the scope of network architecture and algorithm paradigms. Meanwhile, the micro-level\nbreakdown of the networks serves as the basis of the architecture redesign and search.\nTable 1: Latency Comparison between Stable Diffu-\nsion v1.5 and our proposed efficient diffusion models\n(UNet and Image Decoder) on iPhone 14 Pro.\nStable Diffusion v1.5\nText Encoder\nUNet\nVAE Decoder\nInput Resolution\n77 tokens\n64 \u00d7 64\n64 \u00d7 64\n#Parameters (M)\n123\n860\n50\nLatency (ms)\n4\n\u223c1,7002\n369\nInference Steps\n2\n50\n1\nTotal Latency (ms)\n8\n85,000\n369\nOur Model\nText Encoder\nOur UNet\nOur Image Decoder\nInput Resolution\n77 tokens\n64 \u00d7 64\n64 \u00d7 64\n#Parameters (M)\n123\n848\n13\nLatency (ms)\n4\n230\n116\nInference Steps\n2\n8\n1\nTotal Latency (ms)\n8\n1,840\n116\n0\n200\n400\n600\n800\n1000\n0\n100\n200\n300\n400\nDown-1/8\nDown-1/16\nDown-1/32\nMid-1/64\nUp-1/32\nUp-1/16\nUp-1/8\nCA params\nResNet params\nCA latency\nResNet latency\nFigure 2: Latency (iPhone 14 Pro, ms) and pa-\nrameter (M) analysis for cross-attention (CA) and\nResNet blocks in the UNet of Stable Diffusion.\nMacro Prospective. As shown in Tab. 1 and Fig. 3, the networks of stable diffusion consist of three\nmajor components. Text encoder employs a ViT-H model [41] for converting input text prompt\ninto embedding and is executed in two steps (with one for CFG) for each image generation process,\nconstituting only a tiny portion of inference latency (8 ms). The VAE decoder takes the latent feature\nto generate an image, which runs as 369 ms. Unlike the above two models, the denoising UNet is\nnot only intensive in computation (1.7 seconds latency) but also demands iterative forwarding steps\nto ensure generative quality. For instance, the total denoising timesteps is set to 50 for inference in\nSD-v1.5, significantly slowing down the on-device generation process to the minute level.\nBreakdown for UNet. The time-conditional (t) UNet consists of cross-attention and ResNet blocks.\nSpecifically, a cross-attention mechanism is employed at each stage to integrate text embedding\n(c) into spatial features: Cross-Attention(Qzt, Kc, Vc) = Softmax( Qzt\u00b7K\u22a4\nc\n\u221a\nd\n) \u00b7 Vc, where Q is projected\nfrom noisy data zt, K and V are projected from text condition, and d is the feature dimension. UNet\nalso uses ResNet blocks to capture locality, and we can formulate the forward of UNet as:\n\u02c6\u03f5\u03b8(t, zt) =\nY\n{Cross-Attention(zt, c), ResNet(zt, t)}.\n(4)\n2We notice the latency varies depending on the phones and use three phones to get the average speed.\n3\nPrediction\nD\nIterative Denoising\nUNet\nDDIM\nVAE\nPrompt\nViT\nOur Step Distillation\nImage\nE\nDiffusion\nNN\nLatents\nTeacher:\nStudent:\nFigure 3: Workflow of text-to-image diffusion model (left) and the proposed step distillation (right).\nThe distribution of parameters and computations of UNet is illustrated in Fig. 2, showing that\nparameters are concentrated on the middle (downsampled) stages because of the expanded channel\ndimensions, among which ResNet blocks constitute the majority. In contrast, the slowest parts of\nUNet are the input and output stages with the largest feature resolution, as spatial cross-attentions\nhave quadratic computation complexity with respect to feature size (tokens).\n3\nArchitecture Optimizations\nHere we investigate the architecture redundancy of SD-v1.5 to obtain efficient neural networks.\nHowever, it is non-trivial to apply conventional pruning [42, 43, 44, 45] or architecture search [46, 47,\n30] techniques, given the tremendous training cost of SD. Any permutation in architecture may lead to\ndegraded performance that requires fine-tuning with hundreds or thousands of GPUs days. Therefore,\nwe propose an architecture-evolving method that preserves the performance of the pre-trained UNet\nmodel while gradually improving its efficacy. As for the deterministic image decoder, we apply\ntailored compression strategies and a simple yet effective prompt-driven distillation approach.\n3.1\nEfficient UNet\nFrom our empirical observation, the operator changes resulting from network pruning or searching\nlead to degraded synthesized images, asking for significant training costs to recover the performance.\nThus, we propose a robust training, and evaluation and evolving pipeline to alleviate the issue.\nRobust Training. Inspired by the idea of elastic depth [48, 49], we apply stochastic forward\npropagation to execute each cross-attention and ResNet block by probability p(\u00b7, I), where I refers\nto identity mapping that skips the corresponding block. Thus, we have Eq. (4) becomes as follows:\n\u02c6\u03f5\u03b8(t, zt) =\nY\n{p(Cross-Attention(zt, c), I), p(ResNet(zt, t), I)}.\n(5)\nWith this training augmentation, the network is robust to architecture permutations, which enables an\naccurate assessment of each block and a stable architectural evolution (more examples in Fig. 5).\nEvaluation and Architecture Evolving. We perform online network changes of UNet using the\nmodel from robust training with the constructed evolution action set: A \u2208 {A+,\u2212\nCross-Attention[i,j], A+,\u2212\nResNet[i,j]},\nwhere A+,\u2212 denotes the action to remove (\u2212) or add (+) a cross-attention or ResNet block at the\ncorresponding position (stage i, block j). Each action is evaluated by its impact on execution latency\nand generative performance. For latency, we use the lookup table built in Sec. 2.2 for each possible\nconfiguration of cross-attention and ResNet blocks. Note we improve the UNet for on-device speed;\nthe optimization of model size can be performed similarly and is left as future work. For generative\nperformance, we choose CLIP score [41] to measure the correlation between generated images and\nthe text condition. We use a small subset (2K images) of MS-COCO validation set [50], fixed steps\n(50), and CFG scale as 7.5 to benchmark the score, and it takes about 2.5 A100 GPU hours to test\neach action. For simplicity, the value score of each action is defined as\n\u2206CLIP\n\u2206Latency, where a block with\nlower latency and higher contribution to CLIP tends to be preserved, and the opposite is removed in\n4\narchitecture evolving (more details in Alg. 1). To further reduce the cost for network optimization,\nwe perform architecture evolving, i.e., removing redundant blocks or adding extra blocks at valuable\npositions by executing a group of actions at a time. Our training paradigm successfully preserves the\nperformance of pre-trained UNet while tolerating large network permutations (Fig. 5). The details of\nour final architecture is presented in Sec. A.\n3.2\nEfficient Image Decoder\nAlgorithm 1 Optimizing UNet Architecture\nRequire: UNet: \u02c6\u03f5\u03b8; validation set: Dval; latency lookup\ntable T : {Cross-Attention[i, j], ResNet[i, j]}.\nEnsure: \u02c6\u03f5\u03b8 converges and satisfies latency objective S.\nwhile \u02c6\u03f5\u03b8 not converged do\nPerform robust training.\n\u2192 Architecture optimization:\nif perform architecture evolving at this iteration then\n\u2192 Evaluate blocks:\nfor each block[i, j] do\n\u2206CLIP \u2190 eval(\u02c6\u03f5\u03b8, A\u2212\nblock[i,j], Dval),\n\u2206Latency \u2190 eval(\u02c6\u03f5\u03b8, A\u2212\nblock[i,j], T)\nend for\n\u2192 Sort actions based on\n\u2206CLIP\n\u2206Latency, execute ac-\ntion, and evolve architecture to get latency T:\nif latency objective S is not satisfied then\n{ \u02c6A\u2212} \u2190 arg minA\u2212\n\u2206CLIP\n\u2206Latency,\nelse\n{ \u02c6A+} \u2190 copy(arg maxA\u2212\n\u2206CLIP\n\u2206Latency),\n\u02c6\u03f5\u03b8 \u2190 evolve(\u02c6\u03f5\u03b8, { \u02c6A})\nend if\nend if\nend while\nFor the image decoder, we propose a distilla-\ntion pipeline that uses synthetic data to learn the\nefficient image decoder obtained via channel re-\nduction, which has 3.8\u00d7 fewer parameters and\nis 3.2\u00d7 faster than the one from SD-v1.5. The\nefficient image decoder is obtained by applying\n50% uniform channel pruning to the original\nimage decoder, resulting in a compressed ef-\nficient image decoder with approximately 1/4\nsize and MACs of the original one. Here we\nonly train the efficient decoder instead of fol-\nlowing the training of VAE [4, 38, 39] that also\nlearns the image encoder. We use text prompts\nto get the latent representation from the UNet\nof SD-v1.5 after 50 denoising steps with DDIM\nand forward it to our efficient image decoder\nand the one of SD-v1.5 to generate two images.\nWe then optimize the decoder by minimizing\nthe mean squared error between the two images.\nUsing synthetic data for distillation brings the\nadvantage of augmenting the dataset on-the-fly\nwhere each prompt be used to obtain unlimited\nimages by sampling various noises. Quantita-\ntive analysis of the compressed decoder can be\nfound in Sec. B.2.\n4\nStep Distillation\nBesides proposing the efficient architecture of the diffusion model, we further consider reducing\nthe number of iterative denoising steps for UNet to achieve more speedup. We follow the research\ndirection of step distillation [33], where the inference steps are reduced by distilling the teacher,\ne.g., at 32 steps, to a student that runs at fewer steps, e.g., 16 steps. This way, the student enjoys 2\u00d7\nspeedup against the teacher. Here we employ different distillation pipelines and learning objectives\nfrom existing works [33, 32] to improve the image quality, which we elaborate on as follows.\n4.1\nOverview of Distillation Pipeline\nCiting the wisdom from previous studies [33, 32], step distillation works best with the v-prediction\ntype, i.e., UNet outputs velocity v [33] instead of the noise \u03f5. Thus, we fine-tune SD-v1.5 to v-\nprediction (for notation clarity, we use \u02c6v\u03b8 to mean the SD model in v-prediction vs. its \u03f5-prediction\ncounterpart \u02c6\u03f5\u03b8) before step distillation, with the following original loss Lori:\nLori = Et\u223cU[0,1],x\u223cpdata(x),\u03f5\u223cN (0,I) ||\u02c6v\u03b8(t, zt, c) \u2212 v||2\n2,\n(6)\nwhere v is the ground-truth target velocity, which can be derived analytically from the clean latent x\nand noise \u03f5 given time step t: v \u2261 \u03b1t\u03f5 \u2212 \u03c3tx.\nOur distillation pipeline includes three steps. First, we do step distillation on SD-v1.5 to obtain the\nUNet with 16 steps that reaches the performance of the 50-step model. Note here we use a 32-step\nSD-v1.5 to perform distillation directly, instead of doing it progressively, e.g., using a 128-step model\nas a teacher to obtain the 64-step model and redo the distillation progressively. The reason is that we\nempirically observe that progressive distillation is slightly worse than direct distillation (see Fig. 6(a)\n5\nfor details). Second, we use the same strategy to get our 16-step efficient UNet. Finally, we use the\n16-step SD-v1.5 as the teacher to conduct step distillation on the efficient UNet that is initialized from\nits 16-step counterpart. This will give us the 8-step efficient UNet, which is our final UNet model.\n4.2\nCFG-Aware Step Distillation\nWe introduce the vanilla step distillation loss first, then elaborate more details on our proposed\nCFG-aware step distillation (Fig. 3).\nVanilla Step Distillation. Given the UNet inputs, time step t, noisy latent zt, and text embedding c,\nthe teacher UNet performs two DDIM denoising steps, from time t to t\u2032 and then to t\u2032\u2032 (0 \u2264 t\u2032\u2032 <\nt\u2032 < t \u2264 1). This process can be formulated as (see the Sec. C for detailed derivations),\n\u02c6vt = \u02c6v\u03b8(t, zt, c) \u21d2 zt\u2032 = \u03b1t\u2032(\u03b1tzt \u2212 \u03c3t\u02c6vt) + \u03c3t\u2032(\u03c3tzt + \u03b1t\u02c6vt),\n\u02c6vt\u2032 = \u02c6v\u03b8(t\u2032, zt\u2032, c) \u21d2 zt\u2032\u2032 = \u03b1t\u2032\u2032(\u03b1t\u2032zt\u2032 \u2212 \u03c3t\u2032 \u02c6vt\u2032) + \u03c3t\u2032\u2032(\u03c3t\u2032zt\u2032 + \u03b1t\u2032 \u02c6vt\u2032).\n(7)\nThe student UNet, parameterized by \u03b7, performs only one DDIM denoising step,\n\u02c6v(s)\nt\n= \u02c6v\u03b7(t, zt, c) \u21d2 \u02c6x(s)\nt\n= \u03b1tzt \u2212 \u03c3t\u02c6v(s)\nt ,\n(8)\nwhere the super-script (s) indicates these variables are for the student UNet. The student UNet is\nsupposed to predict the teacher\u2019s noisy latent zt\u2032\u2032 from zt with just one denoising step. This goal\ntranslates to the following vanilla distillation loss objective calculated in the x-space [33, 32],\nLvani_dstl = \u03d6(\u03bbt) || \u02c6x(s)\nt\n\u2212\nzt\u2032\u2032 \u2212 \u03c3t\u2032\u2032\n\u03c3t zt\n\u03b1t\u2032\u2032 \u2212 \u03c3t\u2032\u2032\n\u03c3t \u03b1t\n||2\n2,\n(9)\nwhere \u03d6(\u03bbt) = max( \u03b12\nt\n\u03c32\nt , 1) is the truncated SNR weighting coefficients [33].\nCFG-Aware Step Distillation. The above vanilla step distillation can improve the inference speed\nwith no (or only little) FID compromised. However, we do observe the CLIP score turns obviously\nworse. As a remedy, this section introduces a classifier-free guidance-aware (CFG-aware) distillation\nloss objective function, which will be shown to improve the CLIP score significantly.\nWe propose to perform classifier-free guidance to both the teacher and student before calculating the\nloss. Specifically, for Eq. (7) and (8), after obtaining the v-prediction output of UNet, we add the\nCFG step. Take Eq. (8) for an example, \u02c6v(s)\nt\nis replaced with the following guided version,\n\u02dcv(s)\nt\n= w\u02c6v\u03b7(t, zt, c) \u2212 (w \u2212 1)\u02c6v\u03b7(t, zt, \u2205),\n(10)\nwhere w is the CFG scale. In the experiments, w is randomly sampled from a uniform distribution\nover a range ([2, 14] by default) \u2013 this range is called CFG range, which will be shown to provide a\nway to tradeoff FID and CLIP score during training.\nAfter replacing the UNet output with its guided version, all the other procedures remain the same for\nboth the teacher and the student. This gives us a counterpart version of Lvani_dstl \u2013 which we term\nCFG distillation loss, denoted as Lcfg_dstl.\nTotal Loss Function. Empirically, we find Lvani_dstl helps to achieve low FID while Lcfg_dstl helps\nto achieve high CLIP score (see Fig. 6(c)). To get the best of both worlds, we introduce a loss\nmixing scheme to use the two losses at the same time \u2013 A predefined CFG probability p is introduced,\nindicating the probability of using the CFG distillation loss in each training iteration (so with 1 \u2212 p\nprobability, the vanilla distillation loss is used). Now, the overall loss can be summarized:\nL = Ldstl + \u03b3Lori,\nLdstl = Lcfg_dstl if P \u223c U[0, 1] < p else Lvani_dstl,\n(11)\nwhere Lori represents the original denoising loss in Eq. (6) and \u03b3 is its weighting factor; and U[0, 1]\nrepresents the uniform distribution over range (0, 1).\nDiscussion. As far as we know, only one very recent work [32] studies how to distill the guided\ndiffusion models. They propose to distill CFG into a student model with extra parameters (called\nw-condition) to mimic the behavior of CFG. Thus, the network evaluation cost is reduced by 2\u00d7\nwhen generating an image. Our proposed solution here is distinct from theirs [32] for at least four\n6\nperspectives. (1) The general motivations are different. Their w-condition model intends to reduce\nthe number of network evaluations of UNet, while ours aims to improve the image quality during\ndistillation. (2) The specific proposed techniques are different \u2013 they integrate the CFG scale as an\ninput to the UNet, which results in more parameters, while we do not. (3) Empirically, w-condition\nmodel cannot achieve high CLIP scores when the CFG scale is large (as in Fig. 6(b)), while our\nmethod is particularly good at generating samples with high CLIP scores. (4) Notably, the trade-off\nof diversity-quality is previously enabled only during inference by adjusting the CFG scale, while our\nscheme now offers a nice property to realize such trade-off during training (see Fig. 6(d)), which\nw-condition cannot achieve. This can be very useful for model providers to train different models in\nfavor of quality or diversity.\n5\nExperiment\nImplementation Details. Our code is developed based on diffusers library3. Given step distillation\nis mostly conducted on v-prediction models [33, 32], we fine-tune UNet in our experiments to v-\nprediction. Similar to SD, we train our models on public datasets [51, 40] to report the quantitative\nresults, i.e., FID and CLIP scores (ViT-g/14), on MS-COCO 2014 validation set [50] for zero-shot\nevaluation, following the common practice [20, 19, 6, 18]. In addition, we collect an internal dataset\nwith high-resolution images to fine-tune our model for more pleasing visual quality. We use 16 or 32\nnodes for most of the training. Each node has 8 NVIDIA A100 GPUs with 40GB or 80GB memory.\nWe use AdamW optimizer [52], set weight decay as 0.01, and apply training batch size as 2, 048.\nTable 2: Zero-shot evaluation on\nMS-COCO 2017 5K subset. Our\nefficient model is compared against\nrecent arts in the 8-step configura-\ntion. Note the compared works use\nthe same model as SD-v1.5, which\nis much slower than our approach.\nMethod\nSteps\nFID\nCLIP\nDPM [53]\n8\n31.7\n0.32\nDPM++ [54]\n8\n25.6\n0.32\nMeng et al. [32]\n8\n26.9\n0.30\nOurs\n8\n24.2\n0.30\n0.24\n0.26\n0.28\n0.30\nCLIP score\n12\n14\n16\n18\n20\n22\n24\n26\n28\nFID\n2.5\n4.5\nStable Di\ufb00usion v1.5, 50 steps, 1.4min\nOurs, 8 steps, 1.84s\n0.22\n0.24\n0.26\n0.28\n0.30\nCLIP score\n20\n22\n24\n26\n28\n30\n32\n34\nFID\n2.53.5\n4.5\n3.5\n3.54.0 4.5\nSD-v1.5, 50 steps\nOurs 16 steps, nondistilled\nOurs 8 steps, nondistilled\nOurs 8 steps, progressive distill\nOurs 8 steps, CFG distill, self distill\nOurs 8 steps, CFG distill, SD-v1.5 teacher\nOurs 8 steps, CFG distill, CFG teacher\nFigure 4: FID vs. CLIP on MS-COCO 2014 validation set with CFG scale\nfrom 1.0 to 10.0. Left: Comparison with SD-v1.5 on full set (30K). Right:\nDifferent settings for step and teacher models tested on 6K samples.\n5.1\nText-to-Image Generation\nWe first show the comparison with SD-v1.5 on the full MS-COCO 2014 validation set [50] with 30K\nimage-caption pairs. As in Fig. 4 (left), thanks to the architecture improvements and the dedicated\nloss design for step distillation, our final 8-step, 230ms per step UNet outperforms the original\nSD-v1.5 in terms of the trade-off between FID vs. CLIP. For the most user-preferable guidance scales\n(ascending part of the curve), our UNet gives about 0.004 \u2212 0.010 higher CLIP score under the same\nFID level. In addition, with an aligned sampling schedule (8 DDIM denoising steps), our method also\noutperforms the very recent distillation work [32] by 2.7 FID with on-par CLIP score, as in Tab. 2.\nExample synthesized images from our approach are presented in Fig. 1. Our model can generate\nimages from text prompts with high fidelity. More examples are shown in Fig. 9.\nWe then provide more results for performing step distillation on our efficient UNet. As in Fig. 4\n(right), we demonstrate that our 16-step undistilled model provides competitive performance against\nSD-v1.5. However, we can see a considerable performance drop when the denoising step is reduced\nto 8. We apply progressive (vanilla) distillation [33, 32] and observe improvements in scores. Though\nmostly comparable to the SD-v1.5 baseline, the performance of the 8-step model gets saturated for\nthe CLIP score as the guidance scale increases, and is capped at 0.30. Finally, we use the proposed\nCFG-aware step distillation and find it consistently boosts the CLIP score of the 8-step model with\nvaried configurations. Under the best-observed configuration (CFG distilled 16-step teacher), our\n8-step model is able to surpass SD-v1.5 by 0.002\u22120.007 higher CLIP under similar FID. Discussions\non the hyperparameters can be found in ablation studies.\n3https://github.com/huggingface/diffusers\n7\n\uff08a\uff09\n\uff08b\uff09\n\uff08c\uff09\n\uff08d\uff09\n\uff08e\uff09\n\uff08f\uff09\n\uff08g\uff09\n\uff08h\uff09\n(a) SD\n(b) SD\nremoves CA\n(c) Remove\nCA in DS\n(d) Remove\nCA in mid.\n(e) Remove\nCA in US\n(f) Remove\nRB in DS\n(g) Remove\nRB in mid.\n(h) Remove\nRB in US\nFigure 5: Advantages of robust training. Prompts of top row: a photo of an astronaut riding a\nhorse on mars and bottom row: A pikachu fine dining with a view to the Eiffel Tower. (a) Images\nfrom SD-v1.5. (b) Removing cross-attention (CA) blocks in downsample stage of SD-v1.5. (c) - (e)\nRemoving cross-attention (CA) blocks in {downsample (DS), middle (mid.), upsample (US)} using\nour model after robust training. (f) - (h) Removing ResNet blocks (RB) in different stages using our\nmodel. The model with robust training maintains reasonable performance after dropping blocks.\n5.2\nAblation Analysis\nHere we present the key ablation studies for the proposed approach. For faster evaluation, we test the\nsettings on 6K image-caption pairs randomly sampled from the MS-COCO 2014 validation set [50].\nRobust Training. As in Fig. 5, we verify the effectiveness of the proposed robust training paradigm.\nThe original model is sensitive to architecture permutations, which makes it difficult to assess the\nvalue score of the building blocks (Fig. 5(b)). In contrast, our robust trained model can be evaluated\nunder the actions of architecture evolution, even if multiple blocks are ablated at a time. With the\nproposed strategy, we preserve the performance of pre-trained SD and save the fine-tuning cost\nto recover the performance of candidate offspring networks. In addition, we gather some insights\ninto the effect of different building blocks and ensure the architecture permutation is interpretable.\nNamely, cross-attention is responsible for semantic coherency (Fig. 5(c)-(e)), while ResNet blocks\ncapture local information and are critical to the reconstruction of details (Fig. 5(f)-(h)), especially in\nthe output upsampling stage.\nStep Distillation. We perform comprehensive comparisons for step distillation discussed in Sec. 4.\nFor the following comparisons, we use the same model as SD-v1.5 to study step distillation.\n\u2022 Fig. 6(a) presents the comparison of progressive distillation to 8 steps vs. direct distillation to\n8 steps. As seen, direct distillation wins in terms of both FID and CLIP score. Besides, it is\nprocedurally simpler. Thus, we adopt direct distillation in our proposed algorithm.\n\u2022 Fig. 6(b) depicts the results of w-conditioned models [32] at different inference steps. They are\nobtained through progressive distillation, i.e., 64 \u2192 32 \u2192 16 \u2192 8. As seen, there is a clear\ngap between w-conditioned models and the other two, especially in terms of CLIP score. In\ncontrast, our 8-step model can significantly outperform the 50-step SD-v1.5 in terms of CLIP score\nand maintain a similar FID. Comparing ours (8-step model) to the w-conditioned 16-step model,\none point of particular note is that, these two schemes have the same inference cost, while ours\nobviously wins in terms of both FID and CLIP score, suggesting that our method offers a better\nsolution to distilling CFG guided diffusion models.\n\u2022 Fig. 6(c) shows the effect of our proposed CFG distillation loss vs. the vanilla distillation loss. As\nseen, the vanilla loss achieves the lowest FID, while the CFG loss achieves the highest CLIP score.\nTo get the best of both worlds, the proposed loss mixing scheme (see \u201cvanilla + CFG distill\u201d)\nsuccessfully delivers a better tradeoff: it achieves the similar highest CLIP score as the CFG loss\nalone and the similar lowest FID as the vanilla loss alone.\n\u2022 There are two hyper-parameters in the proposed CFG distillation loss: CFG range and CFG\nprobability. Fig. 6(d) shows the effect of adjusting them. Only using the vanilla loss (the blue line)\nand only using the CFG loss (the purple line) lay down two extremes. By adjusting the CFG range\nand probability, we can effectively find solutions in the middle of the two extremes. As a rule of\nthumb, higher CFG probability and larger CFG range will increase the impact of CFG loss, leading\nto better CLIP score but worse FID. Actually, for the 7 lines listed top to down in the legend, the\n8\n0.22\n0.24\n0.26\n0.28\n0.30\nCLIP score\n20\n22\n24\n26\n28\n30\n32\n34\nFID\nSD-v1.5 (50 steps)\ndirect distill (8 steps)\nprogressive distill (8 steps)\n0.22\n0.24\n0.26\n0.28\n0.30\nCLIP score\n20\n22\n24\n26\n28\n30\n32\n34\nFID\nSD-v1.5 (50 steps)\nw-conditioned (50 steps)\nw-conditioned (32 steps)\nw-conditioned (16 steps)\nw-conditioned (8 steps)\nOurs (8 steps)\n0.22\n0.24\n0.26\n0.28\n0.30\nCLIP score\n20\n22\n24\n26\n28\n30\n32\n34\nFID\nSD-v1.5 (50 steps)\nvanilla distill (8 steps)\nCFG distill (8 steps, ours)\nvanilla + CFG distill (8 steps, ours)\n0.22\n0.24\n0.26\n0.28\n0.30\nCLIP score\n20\n22\n24\n26\n28\n30\n32\n34\nFID\nvanilla distill\nCFG distill (range [3], prob. 0.1)\nCFG distill (range [2, 14], prob. 0.1)\nCFG distill (range [2, 14], prob. 0.2)\nCFG distill (range [7, 14], prob. 0.2)\nCFG distill (range [2, 14], prob. 0.5)\nCFG distill (range [2, 14], prob. 1)\n(a) Direct vs. progressive (b) w-conditioned vs. ours (c) Vanilla vs. CFG distill (d) CFG hyper-parameters\nFigure 6: Ablation studies in step distillation (best viewed in color). For each line, from left to right,\nthe CFG scales starts from 1.0 to 10.5 with interval 0.5. (a) To obtain the same 8-step student model,\nin direct distillation, the teacher only distills once (16 \u2192 8), while progressive distillation [33, 32]\nstarts from the 64-step teacher, distills 3 times to 8 steps (64 \u2192 32 \u2192 16 \u2192 8). (b) w-conditioned\nmodel [32] struggles at achieving high CLIP scores (such as over 0.30) while the original SD-v1.5\nand our distilled 8-step SD-v1.5 can easily achieve so. (c) Comparison between vanilla distillation\nloss Lvani_dstl, the proposed CFG distillation loss Lcfg_dstl, and their mixed version Ldstl. (d) Effect of\nadjusting the two hyper-parameters, CFG range and CFG probability, in CFG distillation. As seen,\nthese hyper-parameters can effectively tradeoff FID and CLIP score.\nimpact of CFG loss is gradually raised, and we observe the corresponding lines move steadily to\nthe upper right, fully in line with our expectation, suggesting these two hyper-parameters provide a\nvery reliable way to tradeoff FID and CLIP score during training \u2013 this feature, as far as we know,\nhas not been reported by any previous works.\nAnalysis of Original Loss for Distillation. In Eq. (11), we apply the original denoising loss (Lori in\nEq. (6)) during the step distillation. Here we show more analysis for the using Lori in step distillation.\n\u2022 Fig. 7(a) shows the comparison between using and not using the original loss in our proposed CFG\ndistillation method. To our best knowledge, existing step distillation approaches [33, 32] do not\ninclude the original loss in their total loss objectives, which is actually sub-optimal. Our results in\nFig. 7(a) suggest that using the original loss can help lower the FID at no loss of CLIP score.\n\u2022 Fig. 7(b) provides a detailed analysis using different \u03b3 to balance the original denoising loss and\nthe CFG distillation loss in Eq. (11). We empirically set a dynamic gamma to adjust the original\nloss into a similar scale to step distillation loss.\nAnalysis for the Number of Inference Steps of the Teacher Model. For the default training setting\nof the step distillation, the student runs one DDIM step while the teacher runs two steps, e.g., distilling\na 16-step teacher to an 8-step student. At the first glance, if the teacher runs more steps, it possibly\nprovides better supervision to the student, e.g., distilling a 32-step teacher to the 8-step student. Here\nwe provide empirical results to show that the approach actually does not perform well.\nFig. 7(c) presents the FID and CLIP score plots of different numbers of steps of the teacher model in\nvanilla step distillation. As seen, these teachers achieve similar lowest FID, while the 16-step teacher\n(blue line) achieves the best CLIP score. A clear pattern is that the more steps of the teacher model,\nthe worse CLIP score of the student. Based on this empirical evidence, we adopt the 16-step teacher\nsetting in our pipeline to get 8-step models.\nApplying Step Distillation to Other Model. Lastly, we conduct the experiments by applying our\nproposed CFG-aware distillation on SD-v2, where the student model has the same architecture as\nSD-v2. The results are provided in Fig. 7(d). As can be seen, our 8-step distilled model achieves\ncomparable performance to the 50-step SD-v2 model. We use the same hyper-parameters from the\ntraining of SD-v1.5 for the step distillation of SD-v2, and further tuning might lead to better results.\n6\nRelated Work\nRecent efforts on text-to-image generation utilize denoising diffusion probabilistic models [55, 35,\n56, 1, 2, 4] to improve the synthesis quality by conducting training on the large-scale dataset [40].\nHowever, the deployment of these models requests high-end GPUs for reasonable inference speed\ndue to the tens or hundreds of iterative denoising steps and the huge computation cost of the diffusion\n9\n0.22\n0.24\n0.26\n0.28\n0.30\nCLIP score\n20\n22\n24\n26\n28\n30\n32\n34\nFID\nSD-v1.5 (50 steps)\nOurs (8 steps, w/ original loss)\nOurs (8 steps, w/o original loss)\n0.26\n0.27\n0.28\n0.29\n0.30\n0.31\nCLIP score\n21\n22\n23\n24\n25\n26\n27\n28\nFID\n8 steps, nondistilled\n8 steps, CFG distill, gamma=0.2 (ours)\n8 steps, CFG distill, gamma=0.01\n8 steps, CFG distill, gamma=1.0\n8 steps, CFG distill, constant scale 0.0\n8 steps, CFG distill, constant scale 1.0\n0.22\n0.24\n0.26\n0.28\n0.30\nCLIP score\n20\n22\n24\n26\n28\n30\n32\n34\nFID\nvanilla distill (8 steps, teacher 16 steps)\nvanilla distill (8 steps, teacher 32 steps)\nvanilla distill (8 steps, teacher 64 steps)\n0.25\n0.26\n0.27\n0.28\n0.29\n0.30\n0.31\n0.32\nCLIP score\n20\n21\n22\n23\n24\n25\n26\nFID\nSD-v2, 50 steps\nSD-v2, 8 steps, CFG-aware step distill\n(a) Lori in Eq. (11).\n(b) \u03b3 in Eq. (11).\n(c) Number of teacher steps. (d) Distillation on SD-v2.\nFigure 7: FID and CLIP results on the 6K samples from the MS-COCO 2014 validation set [50] for\nvarious models and experimental settings. (a) Comparison between using (red line) and not using\n(orange line) the original loss in our proposed CFG distillation method. The hyper-parameter setup\nof \u201cours\u201d experiments: CFG range [2, 14] and CFG probability 0.1. (b) Analysis of loss scaling \u03b3 in\nEq. (11). Note that we employ dynamic scaling to adjust original loss (Lori) into a similar scale of\nstep distillation loss (Ldstl). We show \u03b3 as 0.01, 0.2, 1.0. Our choice (0.2) gives slightly better FID,\ndespite all dynamic scalings resulting in very similar results. We further show results of constant\nscaling. Here 0.0 indicates no Lori added, while 1.0 refers to non-scaled Lori where Lori dominates\nthe optimization and degrades the effect of step distillation. (c) Analysis for the number of steps for\nthe teacher model in vanilla step distillation. The student is supposed to run at 8 steps, and we can\nactually employ different teachers that run at different numbers of steps during the step distillation.\nThe default setting in our experiment is that teacher 16 steps, student 8 steps, i.e., the blue line, which\nturns out to be the best. (d) Results of our proposed CFG-aware step distillation applied on SD-v2.\nmodel. This limitation has spurred interest from both the academic community and industry to\noptimize the efficiency of diffusion models, with two primary approaches being explored: improving\nthe sampling process [57, 58, 59, 60, 53, 61] and investigating on-device solutions [62].\nOne promising area for reducing the denoising steps is through progressive distillation, where the\nsampling steps are gradually reduced by distillation that starts from a pre-trained teacher [33]. The\nlater work further improves the inference cost of classifier-free guidance [34] by introducing the\nw-condition [32]. Our work follows the path of step distillation while holding significant differences\nwith existing work, which is discussed above (Sec. 4). Another direction studies the methods for\noptimizing the model runtime on devices [63], such as post-training quantization [22, 23] and GPU-\naware optimization [24]. Nonetheless, these works require specific hardware or compiler support.\nOur work is orthogonal to post optimizations and can be combined with them for further speed up.\nWe target developing a generic and efficient network architecture that can run fast on mobile devices\nwithout relying on specific bit width or compiler support. We identify the redundancy in the SD and\nintroduce one with a similar quality while being significantly faster.\n7\nDiscussion and Conclusion\nThis work proposes the fastest on-device text-to-image model that runs denoising in 1.84 seconds\nwith image quality on par with Stable Diffusion. To build such a model, we propose a series of novel\ntechniques, including analyzing redundancies in the denoising UNet, proposing the evolving-training\nframework to obtain the efficient UNet model, and improving the step distillation by introducing\nthe CFG-aware distillation loss. We perform extensive experiments and validate that our model can\nachieve similar or even better quality compared to Stable Diffusion while being significantly faster.\nLimitation. While our approach is able to run the large-scale text-to-image diffusion model on\nmobile devices with ultra-fast speed, the model still holds a relatively large number of parameters.\nAnother promising direction is to reduce the model size to make it more compatible with various\nedge devices. Furthermore, most of our latency analysis is conducted on iPhone 14 Pro, which has\nmore computation power than many other phones. How to optimize our models for other mobile\ndevices to achieve fast inference speed is also an interesting topic to study.\nBroader Impacts. Similar to existing studies on content generation, our approach must be applied\ncautiously so that it will not be used for malicious applications. Such concerns can also be alleviated\nby approaches that could automatically detect image content that violates specific regulations.\n10\nReferences\n[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\nInformation Processing Systems, 33:6840\u20136851, 2020. 1, 2, 9\n[2] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.\nScore-based generative modeling through stochastic differential equations.\narXiv preprint\narXiv:2011.13456, 2020. 1, 2, 9\n[3] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in\nNeural Information Processing Systems, 34:8780\u20138794, 2021. 1\n[4] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10684\u201310695, 2022. 1, 2, 3, 5, 9, 14, 16\n[5] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning,\npages 8821\u20138831. PMLR, 2021. 1\n[6] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1, 7\n[7] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya\nSutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided\ndiffusion models. arXiv preprint arXiv:2112.10741, 2021. 2\n[8] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and\nMohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference\nProceedings, pages 1\u201310, 2022. 2\n[9] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,\nKevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked\ngenerative transformers. arXiv preprint arXiv:2301.00704, 2023. 2\n[10] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool.\nRepaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 11461\u201311471, 2022. 2\n[11] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N Metaxas, and Jian Ren. Sine: Single image editing\nwith text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 6027\u20136037, 2023. 2\n[12] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi.\nImage super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2022. 2\n[13] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video\ngeneration with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2\n[14] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,\nOron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv\npreprint arXiv:2209.14792, 2022. 2\n[15] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis.\nLion: Latent point diffusion models for 3d shape generation. arXiv preprint arXiv:2210.06978, 2022. 2\n[16] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,\nSanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. arXiv\npreprint arXiv:2211.10440, 2022. 2\n[17] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.\narXiv preprint arXiv:2209.14988, 2022. 2\n[18] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich\ntext-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 2, 7\n[19] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-\nimage diffusion models with deep language understanding. Advances in Neural Information Processing\nSystems, 35:36479\u201336494, 2022. 2, 7, 15\n[20] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala,\nTimo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble\nof expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 2, 7, 15\n11\n[21] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023. 2\n[22] Xiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt\nKeutzer. Q-diffusion: Quantizing diffusion models. arXiv preprint arXiv:2302.04304, 2023. 2, 10\n[23] Jilei Hou and Ziad Asghar. World\u2019s first on-device demonstration of stable diffusion on an android phone,\n2023. 2, 10\n[24] Yu-Hui Chen, Raman Sarokin, Juhyun Lee, Jiuqiang Tang, Chuo-Ling Chang, Andrei Kulik, and Matthias\nGrundmann. Speed is all you need: On-device acceleration of large diffusion models via gpu-aware\noptimizations. arXiv preprint arXiv:2304.11267, 2023. 2, 10\n[25] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion\nmodels. arXiv preprint arXiv:2211.15736, 2022. 2\n[26] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient\nexact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359,\n2022. 2\n[27] Qing Jin, Jian Ren, Oliver J Woodford, Jiazhuo Wang, Geng Yuan, Yanzhi Wang, and Sergey Tulyakov.\nTeachers do more than teach: Compressing image-to-image models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 13600\u201313611, 2021. 2\n[28] Geng Yuan, Yanyu Li, Sheng Li, Zhenglun Kong, Sergey Tulyakov, Xulong Tang, Yanzhi Wang, and Jian\nRen. Layer freezing & data sieving: Missing pieces of a generic framework for sparse training. arXiv\npreprint arXiv:2209.11204, 2022. 2\n[29] Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian\nRen. Efficientformer: Vision transformers at mobilenet speed. arXiv preprint arXiv:2206.01191, 2022. 2\n[30] Yanyu Li, Ju Hu, Yang Wen, Georgios Evangelidis, Kamyar Salahi, Yanzhi Wang, Sergey Tulyakov, and\nJian Ren. Rethinking vision transformers for mobilenet size and speed. arXiv preprint arXiv:2212.08059,\n2022. 2, 4\n[31] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. On architectural compression\nof text-to-image diffusion models. arXiv preprint arXiv:2305.15798, 2023. 2\n[32] Chenlin Meng, Ruiqi Gao, Diederik P Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On\ndistillation of guided diffusion models. arXiv preprint arXiv:2210.03142, 2022. 2, 5, 6, 7, 8, 9, 10\n[33] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv\npreprint arXiv:2202.00512, 2022. 2, 5, 6, 7, 9, 10, 16\n[34] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\n2, 3, 10\n[35] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages\n2256\u20132265. PMLR, 2015. 2, 9\n[36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In MICAI, 2015. 2\n[37] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020. 3\n[38] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,\n2013. 3, 5\n[39] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-\nproximate inference in deep generative models. In International conference on machine learning, pages\n1278\u20131286. PMLR, 2014. 3, 5\n[40] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. 3, 7, 9\n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR,\n2021. 3, 4\n[42] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning\nefficient convolutional networks through network slimming. In ICCV, 2017. 4\n[43] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In\nICCV, 2017. 4\n12\n[44] Huan Wang, Can Qin, Yulun Zhang, and Yun Fu. Neural pruning via growing regularization. In ICLR,\n2021. 4\n[45] Huan Wang and Yun Fu. Trainability preserving neural pruning. In ICLR, 2023. 4\n[46] Barret Zoph and Quoc Le. Neural architecture search with reinforcement learning. In ICLR, 2017. 4\n[47] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. JMLR,\n20(55):1\u201321, 2019. 4\n[48] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic\ndepth. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October\n11\u201314, 2016, Proceedings, Part IV 14, pages 646\u2013661. Springer, 2016. 4\n[49] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang. Slimmable neural networks. arXiv\npreprint arXiv:1812.08928, 2018. 4\n[50] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014:\n13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages\n740\u2013755. Springer, 2014. 4, 7, 8, 10, 15\n[51] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.\nCoyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022. 7\n[52] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 7\n[53] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode\nsolver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927,\n2022. 7, 10\n[54] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver\nfor guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 7\n[55] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based\ngenerative models. arXiv preprint arXiv:2206.00364, 2022. 9\n[56] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nAdvances in neural information processing systems, 32, 2019. 9\n[57] Alexia Jolicoeur-Martineau, Ke Li, R\u00e9mi Pich\u00e9-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta go\nfast when generating data with score-based models. arXiv preprint arXiv:2105.14080, 2021. 10\n[58] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved\nsampling speed. arXiv preprint arXiv:2101.02388, 2021. 10\n[59] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Genie: Higher-order denoising diffusion solvers. arXiv\npreprint arXiv:2210.05475, 2022. 10\n[60] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on\nmanifolds. arXiv preprint arXiv:2202.09778, 2022. 10\n[61] Daniel Watson, William Chan, Jonathan Ho, and Mohammad Norouzi. Learning fast samplers for diffusion\nmodels by differentiating through sample quality. In International Conference on Learning Representations,\n2022. 10\n[62] Atila Orhon, Michael Siracusa, and Aseem Wadhwa. Stable diffusion with core ml on apple silicon, 2022.\n10\n[63] Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song Han, and Jun-Yan Zhu. Efficient spatially sparse\ninference for conditional gans and diffusion models. arXiv preprint arXiv:2211.02048, 2022. 10\n[64] Tensorrt. https://developer.nvidia.com/tensorrt. 14\n13\nA\nEfficient UNet\nWe provide the detailed architecture of our efficient UNet in Tab. 3. We perform denoising diffusion\nin latent space [4]. Consequently, the input and output resolution for UNet is H\n8 \u00d7 W\n8 , which is\n64 \u00d7 64 for generating an image of 512 \u00d7 512.\nIn the main paper, we mainly benchmark the latency on iPhone 14 pro. Here we provide the runtine\nof the model on more mobile devices in Tab. 4.\nIn addition to mobile phones, we show the latency and memory benchmarks on Nvidia A100 40G\nGPU, as in Tab. 5. We demonstrate that our efficient UNet achieves over 12\u00d7 speedup compared\nto the original SD-v1.5 on a server-level GPU and shrinks 46% running memory. The analysis is\nperformed via the public TensorRT [64] library in single precision.\nTable 3: Detailed architecture of our efficient UNet model.\nStage\nResolution\nType\nConfig\nUNet Model\nOrigin\nOurs\nDown-1\nH\n8 \u00d7 W\n8\nCross\nAttention\nDimension\n320\n# Blocks\n2\n0\nResNet\nDimension\n320\n# Blocks\n2\n2\nDown-2\nH\n16 \u00d7 W\n16\nCross\nAttention\nDimension\n640\n# Blocks\n2\n2\nResNet\nDimension\n640\n# Blocks\n2\n2\nDown-3\nH\n32 \u00d7 W\n32\nCross\nAttention\nDimension\n1280\n# Blocks\n2\n2\nResNet\nDimension\n1280\n# Blocks\n2\n1\nMid\nH\n64 \u00d7 W\n64\nCross\nAttention\nDimension\n1280\n# Blocks\n1\n1\nResNet\nDimension\n1280\n# Blocks\n7\n4\nUp-1\nH\n32 \u00d7 W\n32\nCross\nAttention\nDimension\n1280\n# Blocks\n3\n6\nResNet\nDimension\n1280\n# Blocks\n3\n2\nUp-2\nH\n16 \u00d7 W\n16\nCross\nAttention\nDimension\n640\n# Blocks\n3\n3\nResNet\nDimension\n640\n# Blocks\n3\n3\nUp-3\nH\n8 \u00d7 W\n8\nCross\nAttention\nDimension\n320\n# Blocks\n3\n0\nResNet\nDimension\n320\n# Blocks\n3\n3\nTable 4: Latency benchmark on iPhone12 Pro Max, iPhone13 Pro Max, and iPhone 14 Pro.\nDevice\nText Encoder (ms)\nUNet (ms)\nVAE Decoder (ms)\nOverall (s)\niPhone14 Pro\n4.0\n230\n116\n1.96\niPhone13 Pro Max\n5.7\n315\n148\n2.67\niPhone12 Pro Max\n6.3\n526\n187\n4.40\nTable 5: Latency analysis on Nvidia A100 40G GPU with the TensorRT [64] library, tested with\nsingle precision (FP32).\nUNet\nBatch Size\nLatency (ms)\nMemory (MB)\nIters\nTotal Latency (ms)\nSpeedup\nSD-v1.5\n2\n51.2\n6634\n50\n2,560\n-\nOurs\n2\n26.2\n3549\n8\n209.6\n12.2\u00d7\n14\nB\nDiscussions of Text Encoder and VAE Decoder\nB.1\nText Encoder\nExiting works have explored the importance of the pre-trained text encoder for generating images [19,\n20]. In our work, considering the negligible inference latency (4ms) of the text encoder compared to\nthe UNet and VAE Decoder, we do not compress the text encoder in the released pipeline.\nB.2\nVAE Decoder\nWe provide qualitative visualizations and quantitive results of our compressed VAE decoder in Fig. 8.\nThe main paper shows that the image decoder constitutes a small portion of inference latency (369ms)\ncompared to the original UNet from SD-v1.5. However, regarding our optimized pipeline (230ms \u00d7\n8 steps), the decoder consumes a considerable portion of overall latency. We propose an effective\ndistillation paradigm to compress the VAE decoder. Specifically, we obtain the latent-image pairs by\nforwarding the text prompts into the original SD-v1.5 model. The student, which is the compressed\ndecoder, takes the latent from the teacher model as input and generates an output image that is\noptimized with the ones from the teacher model by the mean squared error. Our proposed method\nwields the following advantages. First, our approach does not demand paired text-image samples, and\nit can generate unlimited data on-they-fly, benefiting the generalization of the compressed decoder.\nSecond, the distillation paradigm is simple and straightforward, requiring minimal implementation\nefforts compared to conventional VAE training. As in Fig. 8, our compressed decoder (116ms)\nprovides comparable generative quality, and the performance degradation compared to the original\nVAE decoder is negligible.\n0.22\n0.24\n0.26\n0.28\n0.30\nCLIP score\n20\n22\n24\n26\n28\n30\n32\n34\nFID\n2.5\n4.0\n4.5\n4.5\nSD-v1.5, 50 steps\nSD-v1.5, 8 steps\nOurs, 8 steps\nOurs, 8 steps w. compressed decoder\n(a) Original (top) vs. our compressed (bottom) decoder.\n(b) FID vs. CLIP score.\nFigure 8: Evaluation using MS-COCO 2014 validation set [50]. (a) Generated images by using the\ndecoder from SD-v1.5 and our compressed image decoder. The UNet is our efficient UNet, and the\nguidance scale for CFG is 9.0. (b) Quantitative comparison on the 6K samples. Our compressed\ndecoder performs similarly to the original one considering the widely used CFG scale, i.e., from 7 to\n9, and still performs better than the SD-v1.5.\nC\nDetailed Derivations of Step Distillation\nThe following are the detailed derivations of Eq. (7) \u223c Eq. (9) in the main paper.\nGiven the UNet inputs, time step t, noisy latent zt, and text embedding c, the teacher UNet performs\ntwo DDIM denoising steps, from time t to t\u2032 and then to t\u2032\u2032 (0 \u2264 t\u2032\u2032 < t\u2032 < t \u2264 1).\nWe first examine the process from t to t\u2032, which can be formulated as,\n\u02c6vt = \u02c6v\u03b8(t, zt, c)\n\u25b7 Teacher UNet first forward\n\u21d2 \u02c6xt = \u03b1tzt \u2212 \u03c3t\u02c6vt,\n\u25b7 Teacher predicted clean latent at time t\n\u02c6\u03f5\u03f5\u03f5t = \u03c3tzt + \u03b1t\u02c6vt,\n\u25b7 Teacher predicted noise at time t\n\u21d2 zt\u2032 = \u03b1t\u2032 \u02c6xt + \u03c3t\u2032\u02c6\u03f5\u03f5\u03f5t\n\u25b7 Teacher predicted noisy latent at time t\u2032\n= \u03b1t\u2032(\u03b1tzt \u2212 \u03c3t\u02c6vt) + \u03c3t\u2032(\u03c3tzt + \u03b1t\u02c6vt).\n(12)\n15\nThe process from t\u2032 to t\u2032\u2032 can be derived just like the above, by replacing t and t\u2032 with t\u2032 and t\u2032\u2032,\nrespectively:\n\u02c6vt\u2032 = \u02c6v\u03b8(t\u2032, zt\u2032, c)\n\u25b7 Teacher UNet second forward\n\u21d2 \u02c6xt\u2032 = \u03b1t\u2032zt\u2032 \u2212 \u03c3t\u2032 \u02c6vt\u2032,\n\u25b7 Teacher predicted clean latent at time t\u2032\n\u02c6\u03f5\u03f5\u03f5t\u2032 = \u03c3t\u2032zt\u2032 + \u03b1t\u2032 \u02c6vt\u2032,\n\u25b7 Teacher predicted noise at time t\u2032\n\u21d2 zt\u2032\u2032 = \u03b1t\u2032\u2032 \u02c6xt\u2032 + \u03c3t\u2032\u2032\u02c6\u03f5\u03f5\u03f5t\u2032\n\u25b7 Teacher predicted noisy latent at time t\u2032\u2032\n= \u03b1t\u2032\u2032(\u03b1t\u2032zt\u2032 \u2212 \u03c3t\u2032 \u02c6vt\u2032) + \u03c3t\u2032\u2032(\u03c3tzt\u2032 + \u03b1t\u02c6vt\u2032).\n(13)\nThe student UNet, parameterized by \u03b7, performs only one DDIM denoising step,\n\u02c6v(s)\nt\n= \u02c6v\u03b7(t, zt, c)\n\u25b7 Student UNet forward\n\u21d2 \u02c6x(s)\nt\n= \u03b1tzt \u2212 \u03c3t\u02c6v(s)\nt ,\n\u25b7 Student predicted clean latent at time t\n\u02c6\u03f5\u03f5\u03f5(s)\nt\n= (zt \u2212 \u03b1t\u02c6x(s)\nt )/\u03c3t,\n\u25b7 Student predicted noise at time t\n\u21d2 z(s)\nt\u2032\u2032 = \u03b1t\u2032\u2032 \u02c6x(s)\nt\n+ \u03c3t\u2032\u2032\u02c6\u03f5\u03f5\u03f5(s)\nt\n\u25b7 Student predicted noisy latent at time t\u2032\u2032\n= \u03b1t\u2032\u2032 \u02c6x(s)\nt\n+ \u03c3t\u2032\u2032\n\u03c3t (zt \u2212 \u03b1t\u02c6x(s)\nt )\n= (\u03b1t\u2032\u2032 \u2212 \u03c3t\u2032\u2032\n\u03c3t \u03b1t)\u02c6x(s)\nt\n+ \u03c3t\u2032\u2032\n\u03c3t zt,\n\u21d2 \u02c6x(s)\nt\n=\nz(s)\nt\u2032\u2032 \u2212\n\u03c3t\u2032\u2032\n\u03c3t zt\n\u03b1t\u2032\u2032 \u2212\n\u03c3t\u2032\u2032\n\u03c3t \u03b1t\n,\n(14)\nwhere the super-script (s) indicates these variables are for the student UNet. The student UNet is\nsupposed to predict the noisy latent zt\u2032\u2032 from zt of the teacher with just one denoising step, namely,\nz(s)\nt\u2032\u2032 = zt\u2032\u2032.\n(15)\nReplacing z(s)\nt\u2032\u2032 with zt\u2032\u2032 in the final equation of Eq. (14), we arrive at the following loss objective,\nLvani_dstl = \u03d6(\u03bbt) || \u02c6x(s)\nt\n\u2212\nzt\u2032\u2032 \u2212\n\u03c3t\u2032\u2032\n\u03c3t zt\n\u03b1t\u2032\u2032 \u2212\n\u03c3t\u2032\u2032\n\u03c3t \u03b1t\n||2\n2,\n(16)\nwhere \u03d6(\u03bbt) = max( \u03b12\nt\n\u03c32\nt , 1) is the truncated SNR weighting coefficients [33].\nD\nDifferent Teacher Options for Step Distillation\nIt is non-trivial to decide the best teacher model to distill our final 8-step efficient UNet. In Fig. 4, we\nconduct several experiments to explore different teacher options. As straightforward choices, self-\ndistillation from our 16-step efficient UNet or distillation from the 16-step SD-v1.5 baseline model\ncan effectively boost the performance of our 8-step model. Additionally, we investigate whether\nstronger teachers can further boost performance by training a CFG-aware distilled 16-step SD-v1.5\nmodel, as discussed in Sec. 4. We obtain significant improvements in CLIP scores, demonstrating\nthe potential of employing better teacher models. We would like to mention that we also experiment\nwith SD-v2 as the teacher model. Surprisingly, we observe much worse results. We attribute this to\nthe different text embeddings used in SD-v1.5 and SD-v2 pipelines. Distillation between different\ninfrastructures might be a possible future direction to explore.\nE\nAdditional Qualitative Results\nWe provide more generated images from our text-to-image diffusion model in Fig. 9. As an accelera-\ntion work for generic Stable Diffusion [4], our efficient model demonstrates a sufficient capability to\nsynthesize various contents with high aesthetics, such as realistic objects (food, animals), scenery,\nand artistic and cartoon styles.\n16\nflower field, macro, bokeh, \npanoramic view, camera close to \nground--ar_2/1\ndog illustrations\nelephant illustration AI\nfantasy painting of islands and \ncliffs overseeing a vast ocean\nlarge plate of delicious greasy \nfried chicken, with a side of \ndipping sauces, realistic \nadvertising photography, 4K \nresolution, spot lighting\nGrand Teton Landscape ar 16/9 --\nhd\nA beautiful image of a cute animal \nsurrounded by natural light, \ncapturing their delicate beauty and \ncharm\nSwedish meatballs\nWaiting in traffic, oil painting\nunderground fantasy village dark \n8k cinematic --ar_21/9 --hd\nThe eldritch landscape with \nmountains in the background, Sci-\nFi fantasy desktop wallpaper, \npainted, 4k, high detail, sharp \nfocus\nphotograph of a modern advanced \nglass house in the middle of a \njungle, small stream flowing, little \nfoggy, atmospheric\nbatman in the rain, cinematic shot, \nbeautiful night, photorealistic, \nconcept art, high quality, 8 k\n2045 train station city landscale, \nconcepta rt, illustration, highly \ndetailed, artwork, hyper realistic, in \nstyle of Ivan aivazovsky\na beautiful picture of a dark distant \nthundercloud lit from below in a \ndesert, intricate detail, impending \ndoom, sunset, serene, volumetric \nlighting, volumetric clouds, 8 k, \nhyperrealistic, digital art trending \non artstation\n3d render of voxel pink \nelephant\nA photo of an astronaut riding \na horse on mars\nan aerial view of a forest in spiral, \nwith a giant tree in the center, \nreferencing the movie Avatar, \nrealistic render, detailed. ar 16/9\na beautiful hyperrealistic vase with \na huge bouquet of celestial \nflowers, still life classical \nrenaissance lighting ar 3/4\nCat\nconcept art of a single bowl filled \nwith a few moist freshly picked \nplums on a wooden table, digital \npainting, volumetric lighting, \nsmall scale, artistic, trending on \nartstation.\nan astronaut resting on mars in a \nbeach chair, vibrant lighting, \nelegant, highly detailed, smooth, \nsharp focus, illustration, beautiful, \ngeometric, trending on artstation, \nfull body, cinematic, artwork by \nborovikovsky\nelder scrolls shiver isles concept art \nunreal 5 render\ncream cheese smoked salmon \nbagel, splash art, movie still, \ncinematic lighting, dramatic, \noctane render, long lens, shallow \ndepth of field, bokeh, anamorphic \nlens flare, 8 k, hyper detailed, 3 5 \nmm film grain\nFigure 9: Example generated images by using our efficient text-to-image diffusion model.\n17\n"
  },
  {
    "title": "Wuerstchen: Efficient Pretraining of Text-to-Image Models",
    "link": "https://arxiv.org/pdf/2306.00637.pdf",
    "upvote": "9",
    "text": "W \u00a8URSTCHEN:\nAN EFFICIENT ARCHITECTURE FOR LARGE-SCALE\nTEXT-TO-IMAGE DIFFUSION MODELS\nPablo Pertinas\u2217\nIndpendent researcher, Sant Joan d\u2019Alacant, Spain\nDominic Rampas\u2217\nTechnische Hochschule Ingolstadt, Ingolstadt, Germany\nWand Technologies Inc., New York, USA\nMats L. Richter\u2217\nUniversit\u00b4e de Montr\u00b4eal, Montreal, Canada\nMila, Quebec AI Institute, Montreal, Canada\nChristopher J. Pal\nPolytechnique Montr\u00b4eal, Montreal, Canada\nMila, Quebec AI Institute, Quebec, Canada\nCanada CIFAR AI Chair\nMarc Aubreville\nTechnische Hochschule Ingolstadt, Ingolstadt, Germany\nABSTRACT\nWe introduce W\u00a8urstchen, a novel architecture for text-to-image synthesis that\ncombines competitive performance with unprecedented cost-effectiveness for large-\nscale text-to-image diffusion models. A key contribution of our work is to develop\na latent diffusion technique in which we learn a detailed but extremely compact\nsemantic image representation used to guide the diffusion process. This highly\ncompressed representation of an image provides much more detailed guidance\ncompared to latent representations of language and this significantly reduces the\ncomputational requirements to achieve state-of-the-art results. Our approach also\nimproves the quality of text-conditioned image generation based on our user\npreference study. The training requirements of our approach consists of 24,602\nA100-GPU hours \u2013 compared to Stable Diffusion 2.1\u2019s 200,000 GPU hours. Our\napproach also requires less training data to achieve these results. Furthermore,\nour compact latent representations allows us to perform inference over twice as\nfast, slashing the usual costs and carbon footprint of a state-of-the-art (SOTA)\ndiffusion model significantly, without compromising the end performance. In\na broader comparison against SOTA models our approach is substantially more\nefficient and compares favorably in terms of image quality. We believe that this\nwork motivates more emphasis on the prioritization of both performance and\ncomputational accessibility.\n1\nINTRODUCTION\nState-of-the-art diffusion models (Ho et al., 2020; Saharia et al., 2022; Ramesh et al., 2022) have\nadvanced the field of image synthesis considerably, achieving remarkable results that closely approxi-\n\u2217equal contribution\n1\narXiv:2306.00637v2  [cs.CV]  29 Sep 2023\nFigure 1: Text-conditional generations using W\u00a8urstchen. Note the various art styles and aspect ratios.\nmate photorealism. However, these foundation models, while impressive in their capabilities, carry\na significant drawback: they are computationally demanding. For instance, Stable Diffusion (SD)\n1.4, one of the most notable models in the field, used 150,000 GPU hours for training (Rombach &\nEsser, 2022). While more economical text-to-image models do exist (Ding et al., 2021; 2022; Tao\net al., 2023; 2022), the image quality of these models can be considered inferior in terms of lower\nresolution and overall aesthetic features.\nThe core dilemma for this discrepancy is that increasing the resolution also increases visual complexity\nand computational cost, making image synthesis more expensive and data-intensive to train. Encoder-\nbased Latent Diffusion Models (LDMs) partially address this by operating on a compressed latent\nspace instead of directly on the pixel-space (Rombach et al., 2022), but are ultimately limited by how\nmuch the encoder-decoder model can compress the image without degradation (Richter et al., 2021a).\nAgainst this backdrop, we propose a novel three-stage architecture named \u201dW\u00a8urstchen\u201d, which\ndrastically reduces the computational demands while maintaining competitive performance. We\nachieve this by training a diffusion model on a very low dimensional latent space with a high\ncompression ratio of 42:1. This very low dimensional latent-space is used to condition the second\ngenerative latent model, effectively helping it to navigate a higher dimensional latent space of a\nVector-quantized Generative Adversarial Network (VQGAN), which operates at a compression ratio\nof 4:1. More concretely, the approach uses three distinct stages for image synthesis (see Figure 2):\ninitially, a text-conditional LDM is used to create a low dimensional latent representation of the\nimage (Stage C). This latent representation is used to condition another LDM (Stage B), producing\na latent image in a latent space of higher dimensionality. Finally, the latent image is decoded by a\nVQGAN-decoder to yield the full-resolution output image (Stage A).\nTraining is performed in reverse order to the inference (Figure 3): The initial training is carried out\non Stage A and employs a VQGAN to create a latent space. This compact representation facilitates\nlearning and inference speed (Rombach et al., 2022; Chang et al., 2023; Rampas et al., 2023). The\nnext phase (Stage B) involves a first latent diffusion process (Rombach et al., 2022), conditioned on\nthe outputs of a Semantic Compressor (an encoder operating at a very high spatial compression rate)\nand on text embeddings. This diffusion process is tasked to reconstruct the latent space established\nby the training of Stage A, which is strongly guided by the detailed semantic information provided by\nthe Semantic Compressor. Finally, for the construction of Stage C, the strongly compressed latents\nof the Semantic Compressor from Stage B are used to project images into the condensed latent\nspace where a text-conditional LDM (Rombach et al., 2022) is trained. The significant reduction in\nspace dimensions in Stage C allows for more efficient training and inference of the diffusion model,\nconsiderably reducing both the computational resources required and the time taken for the process.\nOur proposed W\u00a8urstchen model thus introduces a thoughtfully designed approach to address the\nhigh computational burden of current state-of-the-art models, providing a significant leap forward\nin text-to-image synthesis. With this approach we are able to train a 1B parameter Stage C text-\nconditional diffusion model within approximately 24,602 GPU hours, resembling a 8x reduction in\ncomputation compared to the amount SD 2.1 used for training (200,000 GPU hours), while showing\nsimilar fidelity both visually and numerically. Throughout this paper, we provide a comprehensive\nevaluation of W\u00a8urstchen\u2019s efficacy, demonstrating its potential to democratize the deployment &\ntraining of high-quality image synthesis models.\n2\nFigure 2: Inference architecture for text-conditional image generation.\nOur main contributions are the following:\n1. We propose a novel three-stage architecture for text-to-image synthesis at strong compression\nratio, consisting of two conditional latent diffusion stages and a latent image decoder.\n2. We show that by using a text-conditional diffusion model in a strongly compressed latent\nspace we can achieve state-of-the-art model performance at a significantly reduced training\ncost and inference speed.\n3. We provide comprehensive experimental validation of the model\u2019s efficacy based on auto-\nmated metrics and human feedback.\n4. We are publicly releasing the source code and the entire suite of model weights.\n2\nRELATED WORK\n2.1\nCONDITIONAL IMAGE GENERATION\nThe field of image generation guided by text prompts has undergone significant progression in recent\nyears. Initial approaches predominantly leveraged Generative Adversarial Networks (GANs) (Reed\net al., 2016; Zhang et al., 2017). More recently, however, a paradigm shift in the field of image\ngeneration towards diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) has occurred. These\napproaches, in some cases, have not only met but even exceeded the performance of GANs in both\nconditional and unconditional image generation (Dhariwal & Nichol, 2021). Diffusion models put\nforth a score-based scheme that gradually eliminates perturbations (e.g., noise) from a target image,\nwith the training objective framed as a reweighted variational lower-bound. Next to diffusion models,\nanother dominant choice for training text-to-image models is transformers. In their early stages,\ntransformer-based models utilized an autoregressive approach, leading to a significant slowdown\nin inference due to the requirement for each token to be sampled individually. Current strategies,\nhowever, employ a bidirectional transformer (Ding et al., 2022; Chang et al., 2022; 2023) to address\nthe challenges that traditional autoregressive models present. As a result, image generation can be\nexecuted using fewer steps, while also benefiting from a global context during the generative phase.\nOther recent work has shown that convolution-based approaches for image generation can yield\nsimilar results (Rampas et al., 2023).\n2.2\nCOMPRESSED LATENT SPACES\nThe majority of approaches in the visual modality of generative models use some way to train at a\nsmaller space, followed by upscaling to high resolutions, as training at large pixel resolutions can\nbecome exponentially more expensive with the size of images. For text-conditional image generation,\nthere are two established categories of approaches: encoder-based and upsampler-based. LDMs\n(Rombach et al., 2022), DALL-E (Ramesh et al., 2021), CogView (Ding et al., 2021; 2022), MUSE\n(Chang et al., 2023) belong to the first category and employ a two-stage training process. Initially,\nan autoencoder (Rumelhart et al., 1985) is trained to provide a lower-dimensional, yet perceptually\nequivalent, representation of the data. This representation forms the basis for the subsequent training\nof a diffusion or transformer model. Eventually, generated latent representations can be decoded\nwith the decoder branch of the autoencoder to the pixel space. The result is a significant reduction in\ncomputational complexity for the diffusion/sampling process and efficient image decoding from the\nlatent space using a single network pass. On the contrary, upsampler-based methods generate images\nat low resolution in the pixel space and use subsequent models for upscaling the images to higher\n3\nresolution. UnClip (Ramesh et al., 2022) and Imagen (Saharia et al., 2022) both generate images\nat 64x64 and upscale using two models to 256 and 1024 pixels. The former model is the largest in\nterms of parameter count, while the latter models are smaller due to working at higher resolution and\nonly being responsible for upscaling.\n2.3\nCONDITIONAL GUIDANCE\nThe conditional guidance of models in text-based scenarios is typically facilitated through the\nencoding of textual prompts via a pretrained language model. Two major categories of text encoders\nare employed: contrastive text encoders and uni-modal text encoders. Contrastive Language-Image\nPretraining (CLIP) (Radford et al., 2021) is a representative of the contrastive multimodal models\nthat strives to align text descriptions and images bearing semantic resemblance within a common\nlatent space. A host of image generation methodologies have adopted a frozen CLIP model as their\nexclusive conditioning method in recent literature. The hierarchical DALL-E 2 by Ramesh et al.\n(2022) specifically harnesses CLIP image embeddings as input for their diffusion model, while a\n\u2019prior\u2019 performs the conversion of CLIP text embeddings to image embeddings. SD (Rombach et al.,\n2022), on the other hand, makes use of un-pooled CLIP text embeddings to condition its LDM. In\ncontrast, the works of Saharia et al. (2022), Liu et al. (2022a) and Chang et al. (2023) leverage a\nlarge, uni-modal language model such as T5 (Raffel et al., 2020) or ByT5 (Xue et al., 2022) that can\nencode textual prompts with notable accuracy, leading to image generations of superior precision in\nterms of composition, style, and layout.\n3\nMETHOD\nOur method comprises three stages, all implemented as deep neural networks. For image generation,\nwe first generate a latent image at a strong compression ratio using a text-conditional LDM (Stage C).\nSubsequently, this representation is transformed to a less-compressed latent space by the means of a\nsecondary model which is tasked for this reconstruction (Stage B). Finally, the tokens that comprise\nthe latent image in this intermediate resolution are decoded to yield the output image (Stage A).\nThe training of this architecture is performed in reverse order, starting with Stage A, then following\nup with Stage B and finally Stage C (see Figure 3). Text conditioning is applied on Stage C using\nCLIP-H (Ilharco et al., 2021). Details on the training procedure can be found in Appendix E.\n3.1\nSTAGE A AND B\nIt is a known and well-studied technique to reduce the computational burden by compressing data\ninto a smaller representation(Richter et al., 2021a;b; Chang et al., 2022). Our approach follows\nthis paradigm, too, and makes use of Stages A & B to achieve a notably higher compression than\nusual. Let H \u00d7 W \u00d7 C be the dimensions of images. A spatial compression maps images to a\nlatent representation with a resolution of h \u00d7 w \u00d7 z with h = H/f, w = W/f, where f defines the\ncompression rate. Common approaches for modeling image synthesis use a one-stage compression\nbetween f4 and f16 (Esser et al., 2021; Chang et al., 2023; Rombach et al., 2022), with higher factors\nusually resulting in worse reconstructions. Our Stage A consists of a f4 VQGAN (Esser et al., 2021)\nwith parameters \u0398 and initially encodes images X \u2208 R3\u00d71024\u00d71024 into 256 \u00d7 256 discrete tokens\nfrom a learned codebook of size 8,192.\nXq = f\u0398(X)\nThe network is trained as described by Esser et al. and tries to reconstruct the image based on the\nquantized latents, so that:\nf \u22121\n\u0398 (f\u0398 (X)) = f \u22121\n\u0398 (Xq) \u2248 X\nwhere f \u22121\n\u0398\nresembles the decoder part of the VQGAN.\nAfterward, the quantization is dropped from Stage A, and Stage B is trained in the unquantized\nlatent space of the Stage A-encoder as a conditioned LDM. In stage B, we utilize a Semantic\nCompressor, i.e., an encoder-type network that is tasked to create latent representations at a strong\nspatial compression rate that can be used to create a latent representation to guide the diffusion\nprocess. The unquantized image embeddings are noised following an LDM training procedure. The\n4\nFigure 3: Training objectives of our model. Initially, a VQGAN is trained. Secondly, Stage B is trained\nas a diffusion model inside Stage A\u2019s latent space. Stage B is conditioned on text-embeddings and the\noutput of the Semantic Compressor, which produces strongly downsampled latent representations\nof the same image. Finally, Stage C is trained on the latents of the Semantic Compressor as a\ntext-conditional LDM, effectively operating on a compression ratio of 42 : 1.\nnoised representation \u02dc\nXt, together with the visual embeddings from the Semantic Compressor, Csc,\ntext conditioning Ctext and the timestep t are given to the model.\nThe highly compressed visual embeddings extracted by the Semantic Compressor will act as an\ninterface for Stage C, which will be trained to generate them. The embeddings will have a shape\nof R1280\u00d724\u00d724 obtained by encoding images with shape X \u2208 R3\u00d7786\u00d7786. We use simple bicubic\ninterpolation for the resizing of the images from 1024\u00d71024 to 786\u00d7786, which is a sufficiently high\nresolution to fully utilize the parameters of the Semantic Compressor (Richter et al., 2023; Richter &\nPal, 2022), while also reducing the latent representation size. Moreover, we further compress the\nlatents with a 1 \u00d7 1 convolution that normalizes and projects the embeddings to Csc \u2208 R16\u00d724\u00d724.\nThis compressed representation of the images is given to the Stage B decoder as conditioning to\nguide the decoding process.\n\u00af\nX0 = f\u03d1( \u02dc\nXt, Csc, Ctext, t)\nBy conditioning Stage B on low-dimensional latent representations, we can effectively decode images\nfrom a R16\u00d724\u00d724 latent space to a resolution of X \u2208 R3\u00d71024\u00d71024, resulting in a total spatial\ncompression of 42:1.\nWe initialized the Semantic Compressor with weights pre-trained on ImageNet, which, however, does\nnot capture the broad distribution of images present in large text-image datasets and is not well-suited\nfor semantic image projection, since it was trained with an objective to discriminate the ImageNet\ncategories. Hence we updated the weights of the Semantic Compressor during training, establishing a\nlatent space with high-precision semantic information. We use Cross-Attention (Vaswani et al., 2017)\nfor conditioning and project Csc (flattened) to the same dimension in each block of the model and\nconcatenate them. Furthermore, during training Stage B, we intermittently add noise to the Semantic\nCompressor\u2019s embeddings, to teach the model to understand non-perfect embeddings, which is likely\nto be the case when generating these embeddings with Stage C. Lastly, we also randomly drop Csc to\nbe able to sample with classifier-free-guidance (Ho & Salimans, 2022) during sampling.\n5\n3.2\nSTAGE C\nAfter Stage A and Stage B were trained, training of the text-conditional last stage started. In our\nimplementation, Stage C consists of 16 ConvNeXt-block (Liu et al., 2022b) without downsampling,\ntext and time step conditionings are applied after each block via cross-attention. We follow a standard\ndiffusion process, applied in the latent space of the finetuned Semantic Compressor. Images are\nencoded into their latent representation Xsc = Csc, representing the target. The latents are noised by\nusing the following forward diffusion formula:\nXsc,t = \u221a\u00af\u03b1t \u00b7 Xsc +\n\u221a\n1 \u2212 \u00af\u03b1t \u00b7 \u03f5\nwhere \u03f5 represents noise from a zero mean unit variance normal distribution. We use a cosine schedule\n(Nichol & Dhariwal, 2021) to generate \u00af\u03b1t and use continuous timesteps. The diffusion model takes\nin the noised embeddings Xsc,t, the text conditioning Ctext and the timestep t. The model returns\nthe prediction for the noise in the following form:\n\u00af\u03f5 =\nXsc,t \u2212 A\n| 1 \u2212 B | +1e\u22125\nwith\nA, B = f\u03b8(Xsc,t, Ctext, t)\nWe decided to formulate the objective as such, since it made the training more stable. We hypothesize\nthis occurs because the model parameters are initialized to predict 0 at the beginning, enlarging the\ndifference to timesteps with a lot of noise. By reformulating to the A & B objective, the model\ninitially returns the input, making the loss small for very noised inputs. We use the standard mean-\nsquared-error loss between the predicted noise and the ground truth noise. Additionally, we employ\nthe p2 loss weighting (Choi et al., 2022):\np2(t)\u00b7 || \u03f5 \u2212 \u00af\u03f5 ||2\nwhere p2(t) is defined as 1\u2212\u00af\u03b1t\n1+\u00af\u03b1t , making higher noise levels contribute more to the loss. Text\nconditioning Ctext are dropped randomly for 5% of the time and replaced with a null-label in order\nto use classifier-free-guidance (Ho & Salimans, 2022)\n3.3\nIMAGE GENERATION (SAMPLING)\nA depiction of the sampling pipeline can be seen in Figure 2. Sampling starts at Stage C, which is\nprimarily responsible for image-synthesis (see Appendix D), from initial random noise Xsc,\u03c4C =\nN(0, I). We use the DDPM (Ho et al., 2020) algorithm to sample the Semantic Compressor latents\nconditioned on text-embeddings. To do so, we run the following operation for \u03c4C steps:\n\u02c6\nXsc,t\u22121 =\n1\n\u221a\u03b1t\n\u00b7 ( \u02c6\nXsc,t \u2212 1 \u2212 \u03b1t\n\u221a1 \u2212 \u00af\u03b1t\n\u00af\u03f5) +\nr\n(1 \u2212 \u03b1t)1 \u2212 \u00af\u03b1t\u22121\n1 \u2212 \u00af\u03b1t\n\u03f5\nWe denote the outcome as \u00af\nXsc which is of shape 16 \u00d7 24 \u00d7 24. This output is flattened to a shape\nof 576 \u00d7 16 and given as conditioning, along with the same text embeddings used to sample \u00af\nXsc,\nto Stage B. This stage operates at 4 \u00d7 256 \u00d7 256 unquantized VQGAN latent space. We initialize\nXq,\u03c4B to random tokens drawn from the VQGAN codebook. We sample \u02dc\nX for \u03c4B steps using the\nstandard LDM scheme.\n\u02dc\nXt\u22121 = f\u03d1( \u02dc\nXt, Csc, Ctext, t)\nFinally \u02dc\nX is projected back to the pixel space using the decoder f \u22121\n\u0398\nof the VQGAN (Stage A):\n\u00af\nX = f \u22121\n\u0398 ( \u02dc\nX)\n6\n3.4\nMODEL DECISIONS\nTheoretically, any feature extractor could be used as backbone for the Semantic Compressor. However,\nwe hypothesize that it is beneficial to use a backbone that already has a good feature representation of\na wide variety of images. Furthermore, having a small Semantic Compressor makes training of Stage\nB & C faster. Finally, the feature dimension is vital. If it is excessively small, it may fail to capture\nsufficient image details or will underutilize parameters (Richter & Pal, 2022); conversely, if it is overly\nlarge, it may unnecessarily increase computational requirements and extend training duration (Richter\net al., 2021a). For this reason, we decided to use an ImageNet1k pre-trained EfficientV2 (S) as the\nbackbone for our Semantic Compressor, as it combines high compression with well generalizing\nfeature representations and computational efficiency.\nFurthermore, we deviate in Stage C from the U-Net standard architecture. As the image is already\ncompressed by a factor of 42, and we find further compression harmful to the model quality. Instead,\nthe model is a simple sequence of 16 ConvNeXt blocks (Liu et al., 2022b) without downsampling.\nTime and text conditioning is applied after each block.\n4\nEXPERIMENTS AND EVALUATION\nTo demonstrate W\u00a8urstchen\u2019s capabilities on text-to-image generation, we trained an 18M parameter\nStage A, a 1B parameter Stage B and a 1B parameter Stage C. We employed an EfficientNet2-Small\nas Semantic Compressor (Tan & Le, 2020) during training. Stage B and C are conditioned on\nun-pooled CLIP-H (Ilharco et al., 2021) text-embeddings. The setup is designed to produce images\nof variable aspect ratio with up to 1538 pixels per side. All stages were trained on subsets of the\nimproved-aesthetic LAION-5B (Schuhmann et al., 2022) dataset.\nAll the experiments use the standard DDPM (Ho et al., 2020) algorithm to sample latents in Stage B\nand C. Both stages also make use of classifier-free-guidance (Ho & Salimans, 2022) with guidance\nscale w. We fix the hyperparameters for Stage B sampling to \u03c4B = 12 and w = 4, Stage C uses\n\u03c4C = 60 for sampling. Images are generated using a 1024 \u00d7 1024 resolution.\nBaselines\nTo better assess the efficacy of our architecture, we additionally train a U-Net-based 1B\nparameter LDM on SD 2.1 first stage and text-conditioning model. We refer to this model as Baseline\nLDM, it is trained for \u2248 25,000 GPU-hours (same as Stage C) using an 512 \u00d7 512 input resolution.\nAdditionally, we evaluate our model against various state-of-the-art models that were publicly\navailable at the time of writing (see Tables 1 and Table 2). All these models were used in their\nrespective default configuration for text-to-image synthesis. Whenever possible, the evaluation\nmetrics published by the original authors were used.\nEvaluation Metrics\nWe used the Fr\u00b4echet Inception Distance (FID) (Heusel et al., 2018) and\nInception Score (IS) to evaluate all our models on COCO-30K, similar to (Tao et al., 2023; Ding\net al., 2021; 2022). For evaluating the FID score, all images were downsampled to 256 \u00d7 256 pixels\nto allow for a fair comparison between other models in the literature. However, both metrics suffer\nfrom inconsistencies and are known to be not necessarily well correlated with the aesthetic quality\nperceived by humans (Podell et al. (2023); Ding et al. (2021; 2022), see also Appendix B). For this\nreason, we chose PickScore (Kirstain et al., 2023) as our primary automated metric. PickScore is\ndesigned to imitate human preferences, when selecting from a set of images given the same prompt.\nWe applied PickScore to compare W\u00a8urstchen to various other models on various datasets. We provide\nthe percentage of images, where PickScore preferred the image of W\u00a8urstchen over the image of the\nother model. To also evaluate the environmental impact of our model we estimated the carbon emitted\nduring training based on the work of (Lacoste et al., 2019).\nFinally, we also conducted a study with human participants, where the participants chose between\ntwo images from two different models given the prompt.\nDatasets\nTo assess the zero-shot text-to-image capabilities of our model, we use three distinct sets\nof captions alongside their corresponding images. The COCO-validation is the de-facto standard\ndataset to evaluate the zero-shot performance for text-to-image models. For MS COCO we generate\n30,000 images based on prompts randomly chosen from the validation set. We refer to this set of\n7\nFigure 4: Inference time for 1024 \u00d7 1024 images on an A100-GPUs. Left plot shows performance\nwithout specific optimization, right plot shows performance using torch.compile().\nTable 1: Evaluation of Image Quality on MS-COCO and Localized Narratives (Pont-Tuset et al.,\n2020) using the PickScore (Kirstain et al., 2023) to binary select images generated from the same\ncaptions by two different models. W\u00a8urstchen outperforms all models of equal and smaller size,\ndespite Stable Diffusion models using a significantly higher compute budget.\nPickScore(COCO-30k) \u2191\nModel\nBaseline LDM (ours)\nDF-GAN\nGALIP\nSD 1.4\nSD 2.1\nSD XL\n(train cost)\n(\u224825,000 gpu-h)\n-\n-\n(150.000 gpu-h)\n(200.000 gpu-h)\n-\nW\u00a8urstchen\n96.5%\n99.8%\n98.1%\n78.1%\n64.4%\n39.4%\n(24,602 gpu-h)\nPickScore (Localized Narratives-COCO-5K) \u2191\nModel\nBaseline LDM (ours)\nDF-GAN\nGALIP\nSD 1.4\nSD 2.1\nSD XL\n(train cost)\n(\u224825,000 gpu-h)\n-\n-\n(150.000 gpu-h)\n(200.000 gpu-h)\n-\nW\u00a8urstchen\n96.6%\n98.0%\n95.5%\n79.9%\n70.0%\n39.1%\n(24,602 gpu-h)\nPickScore (Parti-prompts) \u2191\nModel\nBaseline LDM (ours)\nDF-GAN\nGALIP\nSD 1.4\nSD 2.1\nSD XL\n(train cost)\n(\u224825,000 gpu-h)\n-\n-\n(150.000 gpu-h)\n(200.000 gpu-h)\n-\nW\u00a8urstchen\n98.6%\n99.6%\n97.9%\n82.1%\n74.6%\n39.0%\n(24,602 gpu-h)\nimages as COCO30K. Since the prompts of MS COCO are quite short and frequently lack detail, we\nalso generate 5,000 images from the Localized Narrative MS COCO subset, we refer to his dataset\nas Localized Narratives-COCO-5K. Finally, we also use Parti-prompts (Yu et al., 2022b), a highly\ndiverse set of 1633 captions, which closely reflects the usage scenario we intend for our model.\n4.1\nAUTOMATED TEXT-TO-IMAGE EVALUATION\nWe evaluate the quality of the generated images using automated metrics in comparison to other,\npublicly available models (see Appendix A for random examples). The PickScores in Table 1 paint\na consistent picture over the three datasets the models were evaluated on. W\u00a8urstchen is preferred\nvery significantly over smaller models like DF-GAN and GALIP, which is expected. The LDM is\noutperformed dramatically in all cases, highlighting that the architecture had a significant impact on\nthe model\u2019s computational training efficiency. W\u00a8urstchen is also preferred in all three scenarios\nover SD 1.4 and 2.1, despite their significantly higher compute-budget at a similar model-\ncapacity. While SD XL is still superior in image quality, our inference speed is significantly faster\n(see Figure 4). This comparison is not entirely fair, as it\u2019s a higher capacity model and its data and\ncompute budget is unknown. For this reason, we are omitting SD XL from the following experiments.\nWhile we achieve a higher Inception Score (IC) on COCO30K compared to all other models in our\nbroader comparison in Table 2 also shows a relatively high FID on the same dataset. While still\noutperforming larger models like CogView2 (Ding et al., 2022) and our Baseline LDM, the FID\nis substantially lower compared to other state-of-the-art models. We attribute this discrepancy to\nhigh-frequency features in the images. During visual inspections we find that images generates by\nW\u00a8urstchen tend smoother than in other text-to-image models. This difference is most noticeable in\nreal-world images like COCO, on which we compute the FID-metric.\n8\nW\u00fcrstchen Stable\nDiffusion 2.1\nBoth Equal\n0\n20\n40\n60\n80\n100\n% Preference\n41.3%\n40.6%\n18.1%\n49.5%\n32.8%\n17.7%\nImages from MS COCO Captions\nImages from Parti-prompts\n(a) Overall Preference\nW\u00fcrstchen Stable\nDiffusion 2.1\nBoth Equal\n0\n20\n40\n60\n80\n100\n% Preferred by Participants\n44.44%\n27.78%\n27.78%\n72.22%\n5.56%\n22.22%\nImages from MS COCO Captions\nImages from Parti-prompts\n(b) Individual Preference\nIndividuals\n0\n50\n100\n150\n200\n250\n300\n350\n# Comparisons (Parti-prompts)\n50th Percentile,\npreference statistic\ncutoff\n(c) Histogram (MS COCO)\nIndividuals\n0\n50\n100\n150\n200\n250\n300\n350\n# Comparisons (MS COCO)\n50th Percentile,\npreference statistic\ncutoff\n(d) Histogram (Parti)\nFigure 5: Overall human preferences (left) and by users (middle). The preference by users considered\nonly users with a large number of comparisons (right).\n4.2\nHUMAN PREFERENCE EVALUATION\nWhile most metrics evaluated in the previous section are correlated with human preference (Kirstain\net al., 2023; Heusel et al., 2018; Salimans et al., 2016), we follow the example of other works\nand also conducted two brief studies on human preference. To simplify the evaluation, we solely\ncompared W\u00a8urstchen against SD 2.1, its closest capacity and performance competitor, and evaluated\nthe human preference between these two models following the setup of Kirstain et al. (2023). In total,\nwe conducted two studies using the generated images from Parti-prompts and COCO30K images.\nParticipants were presented randomly chosen pairs of images in randomized order. For each pair the\nparticipants selected a preference for one or neither of the images (see Appendix C for details). In\ntotal, 3343 (Parti-prompts) and 2262 (COCO Captions) comparisons by 90 participants were made.\nWe evaluate results in two distinct ways. First, by counting the total number of preferences indepen-\ndent of user-identity. In Figure 5 (a) we can see that images generated by our model on Parti-prompts\nwere clearly preferred. This is important to us, since Parti-prompt closely reflects the intended use\ncase of the model. However, for MS-COCO this statistic is inconclusive. We hypothesize that this is\ndue to the vague prompts generating a more diverse set of images, making the preference more subject\nto personal taste, biasing this statistics towards users that completed more comparisons (Figure 5 (c,\nd)). For this reason, we conducted a second analysis, where we evaluated the personally preferred\nmodel for each individual. In an effort to only include participants that completed a representative\nnumber of comparisons, we only include users from the upper 50th percentile and above. By doing\nso, we include only individuals with at least 30 (MS-COCO) and 51 (Parti-prompts) comparisons in\nthe statistic. Under these circumstances, we observed a light preference for MS-COCO in favor of\nW\u00a8urstchen and a strong preference for our model on Parti-prompts (Figure 16 (b)). In summary, the\nhuman preference experiments confirm the observation made in the PickScore experiments. While\nthe real-world results were in-part less decisive, the image generation quality of W\u00a8urstchen was\noverall preferred by the participants of both studies over SD 2.1.\nTable 2: Comparison to other architectures. \u2217 computed from own evaluation. \u2020 based on official\nmodel cards (Rombach & Esser, 2022; Rombach et al., 2023).\nModel\nParams\nSampling\nSteps\nFID\n@2562 \u2193\nIS\n@2992\u2191\nOpen\nSource\nGPU Hours\n@ A100 \u2193\nTrain\n\u2193\nSamples\nEst. Carbon Em.\n[kg CO2 eq.]\nGLIDE (Nichol et al., 2021)\n3.5B\n250\n12.24\n\u2013\n\u2013\n\u2013\n\u2013\nMake-A-Scene (Gafni et al., 2022)\n4B\n1024\n11.84\n\u2013\n\u2013\n\u2013\n\u2013\nParti (Yu et al., 2022a)\n20B\n1024\n7.23\n\u2013\n\u2013\n\u2013\n\u2013\nCogView (Ramesh et al., 2021)\n4B\n1024\n27.1\n22.4\n\u2713\n\u2013\n\u2013\n\u2013\nCogView2 (Ding et al., 2022)\n6B\n-\n24.0\n25.2\n-\n\u2013\n\u2013\n\u2013\nDF-GAN (Tao et al., 2022)\n19M\n-\n19.3\n18.6\n\u2713\n\u2013\n\u2013\n\u2013\nGALIP (Tao et al., 2023)\n240M\n-\n12.5\n26.3*\n\u2713\n\u2013\n\u2013\n\u2013\nDALL-E (Ramesh et al., 2021)\n12B\n256\n17.89\n17.9\n\u2013\n\u2013\n\u2013\n\u2013\nLDM (Rombach et al., 2022)\n1.45B\n250\n12.63\n30.3\n\u2713\n\u2013\n\u2013\n\u2013\nBaseline LDM (ours)\n0.99B\n60\n43.5*\n20.1*\n-\n\u224825,000\n\u22482,300\nW\u00a8urstchen (ours)\n0.99B\n60\n23.6*\n40.9*\n\u2713\n24,602\n1.42B\n2,276\nSD 1.4 (Rombach et al., 2022)\n0.8B\n50\n16.2*\n40.6*\n\u2713\n150,000 \u2020\n4.8B \u2020\n11,250 \u2020\nSD 2.1 (Rombach et al., 2022)\n0.8B\n50\n15.1*\n40.1*\n\u2713\n200,000 \u2020\n3.9B \u2020\n15,000 \u2020\nSD XL (Podell et al., 2023)\n2.6B\n50\n> 18\n\u2013\n\u2713\n\u2013\n\u2013\n\u2013\n9\n4.3\nEFFICIENCY\nTable 2 shows the computational costs for training W\u00a8urstchen compared to the original SD 1.4 and\n2.1. Based on the evaluations in Section 4.1, it can be seen that the proposed setup of decoupling\nhigh-resolution image projection from the actual text-conditional generation can be leveraged even\nmore as done in the past (Esser et al., 2021; Saharia et al., 2022; Ramesh et al., 2022), while still\nstaying on-par or outperforming in terms of quality, fidelity and alignment. Stage C, being the most\nexpensive stage to train from scratch, required only 24,602 GPU hours, compared to 200,000 GPU\nhours (Rombach et al., 2023) for SD 2.1, making it a 8x improvement. Additionally, SD 1.4 and 2.1\nprocessed significantly more image samples. The latter metric is based on the total number of steps\nof all trainings and finetunings and multiplied with the respective batch sizes. Even when accounting\nfor 11,000 GPU hours and 318M train samples used for training Stage B, W\u00a8urstchen is significantly\nmore efficient to train than the SD models. Moreover, although needing to sample with both Stage A\n& B to generate the VQGAN latents \u00afxq, the total inference is still significantly faster than SD 2.1 and\nXL (see Figure 4).\n5\nCONCLUSION\nIn this work, we presented our text-conditional image generation model W\u00a8urstchen, which employs a\nthree stage process of decoupling text-conditional image generation from high-resolution spaces. The\nproposed process enables to train large-scale models efficiently, substantially reducing computational\nrequirements, while at the same time providing high-fidelity images. Our trained model achieved\ncomparable performance to models trained using significantly more computational resources, illus-\ntrating the viability of this approach and suggesting potential efficient scalability to even larger model\nparameters. We hope our work can serve as a starting point for further research into a more sustainable\nand computationally more efficient domain of generative AI and open up more possibilities into\ntraining, finetuning & deploying large-scale models on consumer hardware. We will provide all of\nour source code, including training-, and inference scripts and trained models on GitHub.\nACKNOWLEDGEMENTS\nThe authors wish to express their thanks to Stability AI Inc. for providing generous computational\nresources for our experiments and LAION gemeinn\u00a8utziger e.V. for dataset access and support. This\nwork was supported by a fellowship within the IFI program of the German Academic Exchange\nService (DAAD).\nAUTHOR CONTRIBUTIONS\nThe model architecture was designed by PP and DR. The model training was carried out by PP and\nDR. The baseline model was trained and implemented by MR. The evaluation was carried out by MR\nand MA. The manuscript was written by PP, DR, MR, CP and MA.\nREPRODUCIBILITY STATEMENT\nWe release the entire source code of our pipeline, together with the model weights used to generate\nthese results in our GitHub repository. We also include instructions on how to train the model and an\ninference notebook. As described in Appendix E, we only used dedublicated publicly available data\nto train the model. The methodology that was used to conduct the study on human preference can be\nfound in Appendix C. The setup of the comparison between other open-source baselines is described\nin Section 4. We exclusively used open-soruce models with their official repositories and weights,\nwhen computing metrics for other models.\nREFERENCES\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. MaskGIT: Masked generative\nimage transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n10\nRecognition, pp. 11315\u201311325, 2022.\nHuiwen Chang et al.\nMuse: Text-to-image generation via masked generative transformers.\narXiv:2301.00704, 2023.\nJooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, and Sungroh Yoon.\nPerception prioritized training of diffusion models, 2022.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\nin Neural Information Processing Systems, 34:8780\u20138794, 2021.\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou,\nZhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers.\nAdvances in Neural Information Processing Systems, 34:19822\u201319835, 2021.\nMing Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image\ngeneration via hierarchical transformers. arXiv:2204.14217, 2022.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution im-\nage synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 12873\u201312883, 2021.\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-\nscene: Scene-based text-to-image generation with human priors. arXiv:2203.13131, 2022.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npp. 770\u2013778, 2016.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium, 2018.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv:2207.12598, 2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nNeural Information Processing Systems, 33:6840\u20136851, 2020.\nGabriel Ilharco, Mitchell Wortsman, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar,\nHongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.\nOpenCLIP. Zenodo, July 2021. doi: 10.5281/zenodo.5143773. URL https://doi.org/10.\n5281/zenodo.5143773.\nYuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-\npic: An open dataset of user preferences for text-to-image generation, 2023.\nAlexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the\ncarbon emissions of machine learning, 2019.\nRosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina\nBlok, RJ Mical, Mohammad Norouzi, and Noah Constant. Character-aware models improve visual\ntext rendering. arXiv:2212.10562, 2022a.\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.\nA convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 11976\u201311986, 2022b.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. International Conference\non Learning Representations (ICLR), 2019. doi: 10.48550/ARXIV.1711.05101. URL https:\n//arxiv.org/abs/1711.05101.\nAlex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models, 2021.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv:2112.10741, 2021.\n11\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis, 2023.\nJordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting\nvision and language with localized narratives. In ECCV, 2020.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning, pp.\n8748\u20138763. PMLR, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res., 21(140):1\u201367, 2020.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine\nLearning, pp. 8821\u20138831. PMLR, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with CLIP latents. arXiv:2204.06125, 2022.\nDominic Rampas, Pablo Pernias, and Marc Aubreville. A novel sampling scheme for text- and\nimage-conditional image synthesis in quantized latent spaces. arXiv:2211.07292, 2023.\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.\nGenerative adversarial text to image synthesis. In International conference on machine learning,\npp. 1060\u20131069. PMLR, 2016.\nMats L. Richter and Christopher Pal. Receptive field refinement for convolutional neural networks\nreliably improves predictive performance, 2022.\nMats L. Richter, Wolf Byttner, Ulf Krumnack, Anna Wiedenroth, Ludwig Schallner, and Justin Shenk.\n(input) size matters for cnn classifiers. In Igor Farka\u02c7s, Paolo Masulli, Sebastian Otte, and Stefan\nWermter (eds.), Artificial Neural Networks and Machine Learning \u2013 ICANN 2021, pp. 133\u2013144,\nCham, 2021a. Springer International Publishing. ISBN 978-3-030-86340-1.\nMats L. Richter, Julius Sch\u00a8oning, and Ulf Krumnack.\nShould you go deeper?\noptimizing\nconvolutional neural network architectures without training.\n2021 20th IEEE International\nConference on Machine Learning and Applications (ICMLA), pp. 964\u2013971, 2021b.\nURL\nhttps://api.semanticscholar.org/CorpusID:235606454.\nMats L. Richter, Julius Sch\u00a8oning, Anna Wiedenroth, and Ulf Krumnack. Receptive Field Anal-\nysis for Optimizing Convolutional Neural Network Architectures Without Training, pp. 235\u2013\n261. Springer Nature Singapore, Singapore, 2023. ISBN 978-981-19-6153-3. doi: 10.1007/\n978-981-19-6153-3 10. URL https://doi.org/10.1007/978-981-19-6153-3_10.\nRobin Rombach and Patrick Esser. Stable diffusion 1.4 model card, 2022. URL https://\nhuggingface.co/CompVis/stable-diffusion-v-1-4-original.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\nRobin Rombach, Patrick Esser, and David Ha.\nStable diffusion 2.1 model card, 2023.\nURL\nhttps://huggingface.co/stabilityai/stable-diffusion-2-1.\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by\nerror propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science,\n1985.\n12\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kam-\nyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Pho-\ntorealistic text-to-image diffusion models with deep language understanding. arXiv:2205.11487,\n2022.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans. Advances in neural information processing systems, 29,\n2016.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models. arXiv:2210.08402, 2022.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learning,\npp. 2256\u20132265. PMLR, 2015.\nMingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In International\nConference on Machine Learning, pp. 10096\u201310106. PMLR, 2021.\nMingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural\nnetworks, 2020.\nMing Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan: A simple\nand effective baseline for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 16515\u201316525, 2022.\nMing Tao, Bing-Kun Bao, Hao Tang, and Changsheng Xu. Galip: Generative adversarial clips for\ntext-to-image synthesis. arXiv preprint arXiv:2301.12959, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing\nSystems, 30, 2017.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam\nRoberts, and Colin Raffel. Byt5: Towards a token-free future with pre-trained byte-to-byte models.\nTransactions of the Association for Computational Linguistics, 10:291\u2013306, 2022.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin\nLi, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich\ntext-to-image generation. arXiv:2206.10789, 2022a. doi: 10.48550/ARXIV.2206.10789. URL\nhttps://arxiv.org/abs/2206.10789.\nJiahui Yu et al.\nScaling autoregressive models for content-rich text-to-image generation.\narXiv:2206.10789, 2022b.\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N\nMetaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial\nnetworks. In Proceedings of the IEEE international conference on computer vision, pp. 5907\u20135915,\n2017.\n13\nA\nCOLLAGES (RANDOMLY CHOSEN FROM PARTI-PROMPTS)\nFigure 6: Collage # 1\n14\nFigure 7: Collage # 2\n15\nFigure 8: Collage # 3\n16\nFigure 9: Collage # 4\n17\nFigure 10: Collage # 5\n18\nFigure 11: Collage # 6\n19\nFigure 12: Collage # 7\n20\nFigure 13: Collage # 8\n21\nFigure 14: Collage # 9\n22\nFigure 15: Collage # 10\n23\nB\nROBUSTNESS ASSESSMENT OF FR\u00b4ECHET INCEPTION DISTANCE\nThe Fr\u00b4echet Inception Distance (FID) is commonly used to evaluate the fidelity of text-conditional\nimage generators. For this, a large quantity (e.g., 30k) of prompts are retrieved from a dataset that was\nnot used for the training of the system under test. The original images corresponding to those prompts\nas well as the generated images are then fed to an independent model (typically Inception V3) that\nwas trained on another independent dataset (typically ImageNet). As this model was typically trained\non a reduced input resolution (e.g., 299x299 in the case of Inception v3), resampling of the input\nimages is necessary. The FID is calculated from the first and second order statistics of the features\nfound as output of the feature extractor of that model.\nUsage of a metric, however, also implies knowledge of eventual misbehaviors of said metric, hence\nwe compare in this section the reaction of the FID towards manipulations that we assumed could be\nassumed not to be linked towards strong changes in image fidelity. In particular, we stored the images\nusing various quality settings for JPEG compression, used different methods for resampling of the\nimages and performed mild changes in image color, brightness and contrast.\nAs shown in Table 3, while changes that are part of standard image augmentation like mild changes\nin brightness and contrast do not impact the FID significantly, a moderate JPEG compression as well\nas just a change of resampling do impact the metric strongly. In particular, a JPEG compression with\n70% quality, which impedes the image quality only to a small degree, yields an FID score in the\nrange of our results, without involving any image generation.\nManipulation\nConfiguration\nFID @ COCO 30k\nFID @ CelebA-HQ\nJPEG compression\nquality=95%\n0.268\n0.560\nquality=90%\n1.713\n2.381\nquality=80%\n6.658\n6.291\nquality=70%\n10.469\n9.156\nquality=60%\n13.274\n11.617\nquality=50%\n15.129\n13.519\nResampling\nNN interpolation\n5.239\n3.705\nbilinear interpolation\n0.330\n0.569\nColor change\n8-bit color palette\n27.989\n31.289\nbrightness +10%\n0.085\n0.112\nbrightness -10%\n0.054\n0.101\ncontrast +10%\n0.051\n0.077\ncontrast -10%\n0.072\n0.098\nTable 3: Assessment of Fr\u00b4echet Inception Distance (FID) following minor image manipulations that\nare not expected to significantly alter the fidelity and composition of the images.\n24\nC\nMETHDOLOGY OF THE HUMAN PREFERENCE EXPERIMENTS\nUsed Models:\nThe studies were conducted with images generated with SD 2.1 and W\u00a8urstchen.\nData Displayed to User:\nWe generated 30,000 images based on the COCO-validation set prompts\nfor each model for the first study and 1,633 images each based on Partiprompts for the second study.\nAll images were scanned manually for harmful and graphic and pornographic content.\nSetup:\nBoth studies are conducted online. Participants are presented an image generated from\nboth models using the same prompt. The prompt is also displayed. Neither the model that generated\nthe images nor the number of models used for the image generation as a whole is known and never\ndisplayed to the participants. The displayed images are randomly chosen every time and displayed in\na random order.\nFigure 16: A screenshot from the human preference study. Users can click on either image or the\nbutton below to select a preference.\nParticipants can answer that they prefer the left, right image or perceive both as equal. participants\nare not paid and are only given the instruction to select images based on the prompt and their personal\ntaste. The second study, which was conducted on Parti-prompts, also urged the participants to\nannotate 50 pairs to avoid a long-tailed turnout, which happened in the first study. For gamification\nreasons, a leaderboard was added which allows users which achieved a high number of votes to enter\na personal alias.\nParticipation:\nBased on the randomly generated pseudonyms, in total of 90 unique users partici-\npated in both studies combined, 33 of which participated exclusively in the first study using images\ngenerated from COCO-prompts, 58 participated exclusively in the second study, which used images\ngenerated from Parti-prompts, 3 users participated in both studies.\nOver both datasets, a total of 4026 comparisons were evaluated. 2490 comparison were done on\nMS-COCO generated images and 1604 on images generated from Parti-prompt captions.\nParticipants received no compensation of any form. Participation was pseudonymous.\n25\nD\nHOW ARE STAGE B AND C SHARING THEIR WORKLOAD?\nIn our work we view Stage C as the primary working part of the model, when it cames to generating\nimages from text. However, this is not immediately clear from the architecture as Stage B and C are\nboth generative models and have similar capacities. In this section, we are going to briefly explore\nhow Stage B and Stage C share the workload of image generation. By doing so, we demonstrate\nthat Stage C is responsible for the content of the image, while Stage B acts as a refining model,\nadding details and increasing the resolution, but ultimately not changing the image in a semantically\nmeaningful way. To investigate, we trained a small (3.9M parameter) decoder to reconstruct the\nimages from the latents produced by Stage C and compared the reconstructions with reconstructions\nfrom Stage B conditioned on Stage C. The results in Figures 17, 18, 19 and 20 show that the images\ngenerated by Stage C are very similar to the images generated from Stage B and C combined. From\nthe visual inspection we can observe that the main difference are minor details as well as a reduction\nin blurriness. These changes can be viewed as the main contributions of Stage B. From this we\nconclude that Stage C is the primary factor when it comes to transforming text into images. This is\nfurther supported by the fact that short experiments conducted on alternative training regimes suggest\nthat the text conditioning on Stage B does not enhance the quality of the images and could be dropped\nin future generations our model.\nThe Decoder Architecture:\nThis decoder is very simple, to mitigate the influence on the latents as\nlittle as possible, consisting of 4 stages, composed of 2 convolutional layers. the first downsampling\nlayer is 2\u00d72 convolution with stride size 2. The second convolution is a 3\u00d73 convolution with stride\nsize 1 and GELU-activation function and batch norm. The first stage has 512 channels, consecutive\nstages half the channel width. A final 1 \u00d7 1 convolution squeezes the channels to 3 color channels.\n26\n(a) Stage C\n(b) Stage B and C\nFigure 17: Caption: Macro photography of a tiny businessman.\n(a) Stage C\n(b) Stage B and C\nFigure 18: Caption: Dramatic photography of a frog evolving into a crab, crab legs, macro photogra-\nphy.\n(a) Stage C\n(b) Stage B and C\nFigure 19: Caption: Cute woman smiling wearing a Sailor Moon cosplay.\n(a) Stage C\n(b) Stage B and C\nFigure 20: Caption: a black man with a t-shirt with the letter W\n(a) Stage C\n(b) Stage B and C\nFigure 21: Caption: Cute cat, big eyes pixar style.\n27\nE\nA DETAILED LOOK AT THE NEURAL ARCHITECTURES\nE.1\nSTAGE A\nNeural Architecture:\nThe VQGAN is composed of an encoder and decoder with 2 stages each,\nseparated with a downsampling or upsampling layer with a 4 \u00d7 4-kernel and stride size 2. The\nencoder starts, and the decoder ends with a pixel shuffle operation with a scale factor of 2. In the\nencoder, each stage consists of a single ConvNeXt block (Liu et al., 2022b) with 384 input channels\nand 1536 embedding channels. the final layer of the encoders is a 1 \u00d7 1 convolution followed by a\nBatchNorm-layer, reducing the number of channels to the dimensionality of the encoding layer (4).\nThe decoder reverses this operation with a similar combination of layers. The decoding layer uses 16\nblocks in the first stage and 1 block in the second stage. All blocks have 384 input channels and 1536\nembedding channels.\nTraining Details:\nThe model is trained in a single run with 500,000 iterations using the AdamW\noptimizer with a learning rate of 1e \u2212 4 and a batch size of 256. The model is fed with 128 \u00d7 128\npixel crops from images taken from the deduplicated subsets of the improved-aesthetic LAION-5B\n(Schuhmann et al., 2022) dataset, which were previously resized to 256 \u00d7 256 images. As we\nanticipate the removal of the quantization, we randomly drop the quantization during training with a\ndrop-chance 10%. For training we use three distinct losses. Mean Squared Error (MSE), Adverserial\nLoss (AL) and Perceptual Loss (PL). for the first 10.000 iterations we use the loss weights of (1.0, 0.0,\n0.1) ofr MSE, AL and PL respectively. We activate the AL after 10,000 training steps by increasing\nthe weight to 0.01.\nE.2\nSTAGE B\nNeural Architecture:\nStage B is a U-Net architecture with 4 stages in the encoder and decoder\non the latents of the (unquantized) latent of the Stage A VQGAN. The stages have a channel width\nof 320, 640, 1280 and 1280 respectively. Each stage starts with a single convolutional layer with a\n2 \u00d7 2 kernel and a stride size of 2 acting as downsampling layer. The stages consist of a number\nof building blocks. Except for the first stage, a building block consists of a ConvNeXt block (Liu\net al., 2022b), a time-step block, applying a linear conditioning (Rombach et al., 2022) on the latents\nand a cross-attention block for conditioning on text-embeddings and image-embeddings. The first\nstage omits the cross-attention for conditioning on text and images. GlobalResponseNorm and\nGELU activation functions are used for normalization and as activation functions respectively. The\ncross-attention mechanisms of each stage have a different number of attention-heads: -, 10, 20, 20 (as\nthe first stage is only conditioned on time). For all stages with cross-attention, the latents of Semantic\nCompressor are also concatenated to each residual block before the channelwise-convolution is\napplied. To fit the respective feature-map size, bicubic interpolation is used to resize the latent of the\nSemantic Compressor. The encoder and decoder stages have 4, 4, 14 and 4 blocks respectively. The\nclip-embeddings have a dimensionality of 1024. The Semantic Compressor produces latents with 16\nchannels.\nThe Semantic Compressor is composed of an EfficientNetV2 S (Tan & Le, 2021) backbone. The\nfinal global-pooling and classification-head is replaced by a 1 \u00d7 1-convolution with stride size 1,\ncompressing the channels down to 16. It is worth noting that EfficientNetV2 S is dropped after the\ntraining of Stage B and C is complete, as it is replaced by Stage C during inference time.\nTraining Details:\nThe model is trained using the AdamW optimizer (Loshchilov & Hutter, 2019)\nwith a learning rate of 1e\u22124 using a linear warm-up schedule for 10k steps. In total, the model is\ntrained for 457,000 iterations with an input resolution of 512 \u00d7 512 and a batch size of 512. The\nmodel is trained for an additional 300,000 iterations with an input resolution of 1024 \u00d7 1024 and a\nbatch size of 128. The resolutions described in this work are the image sizes fed into the VQGAN for\nencoding. The Semantic Compressor is fed an input resolution of 384 \u00d7 384 pixels and 768 \u00d7 768\nduring these two phases of training respectively. The training time and compute budget listed in\nthe paper reflects this entire training process. All images fed into the semantic compressor are\nnormalized using \u00b5 = (0.485, 0.456, 0.406) and \u03c3 = (0.229, 0.224, 0.225). The model is trained on\na deduplicated subsets of the improved-aesthetic LAION-5B (Schuhmann et al., 2022) dataset.\n28\nE.3\nSTAGE C\nNeural Architecture:\nStage C consists of a sequence of 16 building blocks. Each building block is\ncomposed of ConvNeXt block (Liu et al., 2022b) a time-conditioning block and a cross-attention\nblock for text-conditioning, similar to Stage B. Text-Conditioning is applied from an unpooled\nCLIP-H model. Text-embedding has a dimensionality of 1024, each cross-attention block has 16\nheads. The width of the network is 1280 channels.\nThe Semantic Compressor is composed of an EfficientNetV2 S (Tan & Le, 2021) backbone trained\nduring Stage B training and otherwise unchanged. It is worth noting that EfficientNetV2 S is dropped\nafter the training of Stage B and C is complete. During inference time, the output of Stage C instead\nused to condition Stage B, replacing the Semantic Compressor entirely. The Semantic Compressor is\nnot trained during Stage B training.\nTraining Details:\nThe model is trained a total of 4 consecutive times using the AdamW optimizer\nwith a learning rate of 1e \u2212 4. The first three consecutive trainings are conducted on a deduplicated\nsubsets of the improved-aesthetic LAION-5B (Schuhmann et al., 2022) dataset. The final training\nis conducted on the dataset but further filtered by aesthetical artworks. The images are fed into the\nfrozen Semantic Compressor to produce latents of a specific resolution. We provide the resolution of\nthe latents alongside the resolution fed into the Semantic Compressor. The preprocessing done on\nimages of the compressor is identical to Stage B.\nThe first training is conducted for 500,000 iterations on 12 \u00d7 12 latents, which corresponds to an\n384 \u00d7 384 input resolution for the semantic compressor using a batch size of 1536. The second\ntraining is conducted for an additional 364,000 iterations on 24 \u00d7 24 latents, corresponding to\n768 \u00d7 768 images being fed into the Semantic Compressor, using a batch size of 1536. The third\ntraining is run for only 4,000 steps and is done to adapt the model to various aspect ratios. The aspect\nratio is randomized uniformly for each batch of 768 images to one of the three following values:\n768 \u00d7 1280, 1280 \u00d7 768 and 768 \u00d7 768. The fourth and final training is designed to improve the\naesthetical quality of images and is conducted for another 50,000 iterations using a batch size of 384\nand a resolution of 768 \u00d7 768 (24 \u00d7 24 latents).\nThe final model is a 50:50 interpolation between the weights after the 3rd training and the final\ntraining run. This allows the model to generate a blend of aesthetic/artistic and realistic images.\nHowever, we open source the two models this interpolation is based on besides this final model.\nE.4\nBASELINE LDM\nNeural Architecture:\nThe LDM is a U-Net architecture with 4 stages in the encoder and decoder\non the latents of the VAE used by Stable-Diffusion 1.4. The stages have a channel width of 320, 640,\n1280 and 1280 respectively. Each stage starts with a single convolutional layer with a 2 \u00d7 2 kernel\nand a stride size of 2 acting as downsampling layer. The stages consist of a number of building blocks.\nA building block consists of a ConvNeXt block (He et al., 2016) and two cross attention blocks\nfor conditioning on time and text-embeddings, using GlobalResponseNorm and GELU activation\nfunctions. The first cross-attention in a building block conditions on the time step t, while the second\none on the text embeddings ctext. The cross-attention mechanisms of each stage have a different\nnumber of attention-heads: 5, 10, 20, 20. The encoder stages have 2, 4, 14 and 4 blocks respectively,\nwhile the corresponding decoder stages have 5, 15, 5 and 3 blocks. Like for the other models, the\nclip-embeddings have a dimensionality of 768. Dropout is applied with a probability of 10% on a\nfeatures of the text and image embeddings as well as the 3 \u00d7 3-convolution in the ConvNeXt-block.\nTraining Setup:\nThe model is trained using the AdamW optimizer (Loshchilov & Hutter, 2019)\nwith a learning rate of 1e\u22124 using a linear warm-up schedule for 10k steps and a batch size of 1280.\nThe training is conducted for approximatly 25,000 GPU hours, which roughly corresponds to 1.5\nmillion training steps.\nThe model is trained on subsets of the improved-aesthetic LAION-5B (Schuhmann et al., 2022)\ndataset. We use a dropout of 5% on the CLIP-H-text embeddings.\n29\n"
  },
  {
    "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
    "link": "https://arxiv.org/pdf/2306.00890.pdf",
    "upvote": "9",
    "text": "LLaVA-Med: Training a Large Language-and-Vision\nAssistant for Biomedicine in One Day\nChunyuan Li\u2217, Cliff Wong\u2217, Sheng Zhang\u2217, Naoto Usuyama, Haotian Liu, Jianwei Yang\nTristan Naumann, Hoifung Poon, Jianfeng Gao\nMicrosoft\nhttps://aka.ms/llava-med\nAbstract\nConversational generative AI has demonstrated remarkable promise for empow-\nering biomedical practitioners, but current investigations focus on unimodal text.\nMultimodal conversational AI has seen rapid progress by leveraging billions of\nimage-text pairs from the public web, but such general-domain vision-language\nmodels still lack sophistication in understanding and conversing about biomedical\nimages. In this paper, we propose a cost-efficient approach for training a vision-\nlanguage conversational assistant that can answer open-ended research questions\nof biomedical images. The key idea is to leverage a large-scale, broad-coverage\nbiomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to\nself-instruct open-ended instruction-following data from the captions, and then\nfine-tune a large general-domain vision-language model using a novel curriculum\nlearning method. Specifically, the model first learns to align biomedical vocabulary\nusing the figure-caption pairs as is, then learns to master open-ended conversational\nsemantics using GPT-4 generated instruction-following data, broadly mimicking\nhow a layperson gradually acquires biomedical knowledge. This enables us to train\na Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less\nthan 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal con-\nversational capability and can follow open-ended instruction to assist with inquiries\nabout a biomedical image. On three standard biomedical visual question answering\ndatasets, fine-tuning LLaVA-Med outperforms previous supervised state-of-the-art\non certain metrics. To facilitate biomedical multimodal research, we will release\nour instruction-following data and the LLaVA-Med model.\n1\nIntroduction\nParallel image-text data is abundantly available in the general domain, such as web images and their\nassociated captions. Generative pretraining has proven effective to leverage this parallel data for\nself-supervised vision-language modeling, as demonstrated by multimodal GPT-4 [32] and open-\nsourced efforts such as LLaVA [24]. By instruction-tuning models to align with human intents\nbased on multimodal inputs, the resulting large multimodal models (LMMs) exhibit strong zero-shot\ntask completion performance on a variety of user-oriented vision-language tasks such as image\nunderstanding and reasoning, paving the way to develop general-purpose multimodal conversational\nassistants [2, 21, 9].\nWhile successful in the general domains, such LMMs are less effective for biomedical scenarios\nbecause biomedical image-text pairs are drastically different from general web content. As a result,\ngeneral-domain visual assistants may behave like a layperson, who would refrain from answering\n\u2217Equal Contribution\nPreprint. Work in progress\narXiv:2306.00890v1  [cs.CV]  1 Jun 2023\nbiomedical questions, or worse, produce incorrect responses or complete hallucinations. Much\nprogress has been made in biomedical visual question answering (VQA), but prior methods typically\nformulate the problem as classification (e.g., among distinct answers observed in the training set) and\nare not well equipped for open-ended instruction-following. Consequently, although conversational\ngenerative AI has demonstrated great potential for biomedical applications [19, 30, 18], current\ninvestigations are often limited to unimodal text.\nIn this paper, we present Large Language and Vision Assistant for BioMedicine (LLaVA-Med), a\nfirst attempt to extend multimodal instruction-tuning to the biomedical domain for end-to-end training\nof a biomedical multimodal conversational assistant. Domain-specific pretraining has been shown\nto be effective for biomedical natural language processing (NLP) applications [17, 14, 10, 28] and\nbiomedical vision-language (VL) tasks [15, 7, 38, 49, 8]. Most recently, large-scale biomedical VL\nlearning has been made possible by the creation of PMC-15M [49], a broad-coverage dataset with 15\nmillion biomedical image-text pairs extracted from PubMed Central1. This dataset is two orders of\nmagnitude larger than the next largest public dataset, MIMIC-CXR [15], and covers a diverse image\ntypes. Inspired by recent work in instruction-tuning [34, 24], LLaVA-Med uses GPT-4 to generate\ndiverse biomedical multimodal instruction-following data using image-text pairs from PMC-15M,\nand fine-tune a large biomedical-domain VL model [24] using a novel curriculum learning method.\nSpecifically, our paper makes the following contributions:\n\u2022 Biomedical multimodal instruction-following data. We present a novel data generation pipeline\nto create diverse (image, instruction, output) instances, by sampling biomedical image-text pairs\nfrom PMC-15M and using GPT-4 to create instructions from the text alone (which becomes\nthe intended output). This requires zero manual annotations and creates an extremely diverse\nvisual instruction-following dataset by piggybacking on PMC-15 that covers the full spectrum\nof research findings over biomedical images.\n\u2022 LLaVA-Med. We propose a novel curriculum learning method for adapting LLaVA [24] to\nthe biomedical domain using our self-generated biomedical multi-modal instruction-following\ndataset. Specifically, we first fine-tune LLaVA to align biomedical vocabulary using the image-\ntext pairs as is (with the generic instruction that simply asks for a description of the image). We\nthen continue training the model using our self-generated instruction-following data to learn\nopen-ended conversational semantics. In this way, we were able to train LLaVA-Med in less than\n15 hours with eight A100s. Our empirical study validates the effectiveness of domain-specific\ninstruction-tuning, and reveals best practice and interesting findings for adapting multimodal\nconversational assistant to high-value verticals. On well-established biomedical VQA datasets,\nfine-tuning LLaVA-Med often outperforms supervised state-of-the-art (SoTA).\n\u2022 Open-source. To facilitate research in biomedical multimodal learning, we will release the\nfollowing assets to the public: the biomedical multimodal instruction-following dataset and the\ncodebase for data generation and model training.\n2\nRelated Work\nBiomedical Chatbots.\nInspired by ChatGPT [31]/GPT-4 [32] and the success of open-sourced\ninstruction-tuned large language models (LLMs) in the general domain, several biomedical LLM\nchatbots have been developed, including ChatDoctor [47], Med-Alpaca [12], PMC-LLaMA [45],\nClinical Camel [1], DoctorGLM [46], and Huatuo [44]. They are initialized with open-sourced LLM\nand fine-tuned on customized sets of biomedical instruction-following data. The resulting LLMs\nemerge with great potential to offer assistance in a variety of biomedical-related fields/settings, such\nas understanding patients\u2019 needs and providing informed advice.\nTo our knowledge, Visual Med-Alpaca [39] is the only existing multimodal biomedical chatbot that\naccepts image inputs. Though Visual Med-Alpaca and the proposed LLaVA-Med share a similar\ninput-output data format, they differ in key aspects: (i) Model architectures. LLaVA-Med is an\nend-to-end neural model and Visual Med-Alpaca is a system that connect multiple image captioning\nmodels with a LLM, using a classifier to determine if or which biomedical captioning model is\nresponsible for the image. The text prompt subsequently merges the converted visual information\nwith the textual query, enabling Med-Alpaca to generate an appropriate response. (ii) Biomedical\ninstruction-following data. While Visual Med-Alpaca is trained on 54K samples from limited\nbiomedical subject domains, LLaVA-Med is trained a more diverse set.\n1https://www.ncbi.nlm.nih.gov/pmc/\n2\nBiomedical Visual Question Answering.\nAn automated approach to building models that can\nanswer questions based on biomedical images stands to support clinicians and patients. To describe\nexisting biomedical VQA methods, we make a distinction between discriminative and generative\nmethods. For discriminative methods, VQA is treated a classification problem: models make\npredictions from a predefined set of answers. While discriminative methods yield good performance,\nthey deal with closed-set predictions [13], and require mitigation when a customized answer set\nis provided in at inference [22, 49, 8]. The discriminative formulation is suboptimal towards the\ngoal of developing a general-purpose biomedical assistant that can answer open questions in the\nwild. To this end, generative methods have been developed to predict answers as a free-form text\nsequence [5, 26, 41]. Generative methods are more versatile because they naturally cast the close-set\nquestions as as special case where candidate answers are in language instructions.\nModel Architecture.\nLLaVA-Med is similar to prefix tuning of language models (LMs) in [41] in\nthat a new trainable module connects frozen image encoder and causal LM. In [41], a three-layer\nMLP network is used to map the visual features into a visual prefix, and the pre-trained LM are\nGPT2-XL [37], BioMedLM [42] and BioGPT [28], with size varying from 1.5B to 2.7B. By contrast,\nLLaVA-Med uses a linear projection and a 7B LM [43, 40]. Most importantly, [41] only considers\nstandard supervised fine-tuning and focuses efforts on exploring various modeling choices. Our\nmain contributions instead comprise proposing a novel data generation method that uses GPT-4 to\nself-instruct biomedical multimodal instruction-following data using freely-available broad-coverage\nbiomedical image-text pairs extracted from PubMed Central [49].\n3\nBiomedical Visual Instruction-Following Data\nThere are a lack of multimodal biomedical datasets to train an instruction-following assistant. To\nfill this gap, we create the first dataset of its kind from widely existing biomedical image-text pairs,\nthrough a machine-human co-curation procedure. It consists of two sets, concept alignment and\ninstruction-following, which are used at different training stages, described in Section 4.\nBiomedical Concept Alignment Data.\nFor a biomedical image Xv and its associated caption Xc,\nwe sample a question Xq, which asks to describe the biomedical image. With (Xv, Xc, Xq), we\ncreate a single-round instruction-following example:\nHuman : Xq Xv<STOP>\\n Assistant : Xc<STOP>\\n\nDepending on the length of caption, the question that is sampled either asks to describe the image\nconcisely or in detail. Two lists of questions are provided in Appendix A. In practice, 25% of captions\nhave length less than 30 words in PMC-15M [49], and thus 30 words is used as the cutoff point to\ndetermine which list to choose. We sample 600K image-text pairs from PMC-15M. Though this\ndataset only presents one-single task instructions, i.e., image captioning, it contains a diverse and\nrepresentative set of biomedical concept samples from the original PMC-15M [49].\nBiomedical Instruction-Tuning Data.\nTo align the model to follow a variety of instructions,\nwe present and curate diverse instruction-following data with multi-round conversations about the\nprovided biomedical images, by prompting language-only GPT-4. Specifically, given an image\ncaption, we design instructions in a prompt that asks GPT-4 to generate multi-round questions and\nanswers in a tone as if it could see the image (even though it only has access to the text). Sometimes\nthe image caption is too short for GPT-4 to generate meaningful questions and answers. To provide\nmore context regarding the image, we also create a prompt that includes not only captions but also\nsentences from the original PubMed paper that mentions the image. We also manually curate few-shot\nexamples in the prompt to demonstrate how to generate high-quality conversations based on the\nprovided caption and context. See Appendix B for the prompt and few-shot examples. To collect\nimage captions and their context, we filter PMC-15M to retain the images that only contain a single\nplot. From them, we sample 60K image-text pairs from the five most common imaging modalities:\nCXR (chest X-ray), CT (computed tomography), MRI (magnetic resonance imaging), histopathology,\nand gross (i.e., macroscopic) pathology. We then extract sentences that mention the image from the\noriginal PubMed paper as additional context to the caption, inspired by the observations that external\nknowledge helps generalization [20, 25].\nAn example of instruction-following data is shown in Figure 1 shows, and the data statistics is shown\nFigure 2. We have produced three versions of instruct data when iteratively improving the data\n3\nBiomedical Visual Instruction-Following Example\nCaption:\nFigure 4: Contrast-enhanced CT scan of the chest for patient #1. A\nlarge cavitating lesion was identified in the right upper lobe with pleu-\nral involvement and likely extension into the chest wall. In addition,\nright hilar lymphadenopathy was suspected. This was the primary\nmalignant lesion in this patient.\nSentences with in-line figure mentions:\n- Computerized tomography ( CT ) scans of the chest ( Figure 4 )\nwere obtained.\n- ( Figure 4 ) and demonstrated a large cavitating lesion in the pos-\nterior aspect of the right upper lobe with probable pleural involve-\nment likely representing the primary lesion.\nConversation:\nUser: Where is the lesion located?\nAssistant: The lesion is located in the posterior aspect of the right upper lobe of the lung.\nUser: Is it extending into the chest wall?\nAssistant: Yes, the CT scan suggests that the large cavitating lesion in the right upper lobe is\nlikely extending into the chest wall.\nUser: Can you explain more about the cavitating lesion?\nAssistant: Certainly. A cavitating lesion refers to an abnormal mass or growth within the lung\ntissue that has a cavity or hollow space inside it. This cavity can be formed due to the necrosis\n(death) of the tissue inside the lesion, leading to the breakdown and disintegration of the\naffected tissue. Cavitating lesions can be caused by various factors, such as infections (like\ntuberculosis or fungal infections), autoimmune diseases, or malignancies (cancer). In the context\nof the CT scan provided, the large cavitating lesion in the right upper lobe of the lung is\nsuggestive of a malignant (cancerous) growth. The cavity within the lesion may be a result of the\nrapid and uncontrolled growth of cancer cells, which can lead to the destruction of surrounding\nhealthy tissue. It\u2019s important to note that further diagnostic tests, such as a biopsy, would be\nrequired to confirm the malignancy and determine the specific type of cancer involved.\nFigure 1: An instance of our GPT-4 generated instruction-following data. Top: The figure and caption\nwere extracted from a PubMed Central full-text article [35], along with the corresponding citances\n(mentions of the given figure in the article). Bottom: The instruction-following data generated by\nGPT-4 using the text only (caption and citances). Note that the image is not used to prompt GPT-4;\nwe only show it here as a reference.\nquality: (i) 60K-IM. The aforemenioned dataset that considers inline mentions (IM) as the context.\n(ii) 60K. A dataset of similar size (60K samples) without IM in self-instruct generation. (iii) 10K. A\nsmaller dataset (10 samples) without IM. They are used to ablate our data generation strategies and\ntheir impact on trained LLaVA-Med in experiments.\n4\nAdapting Multimodal Conversational Models to the Biomedical Domain\nWe employ LLaVA,a general-domain multimodal conversation model [24], as the initial general-\ndomain LM, and continuously train the model to the biomedical domain. The same network architec-\nture is utilized, where a linear projection layer connects the vision encoder and the language model.\nFor LLaVA-Med model training, we use a two-stage procedure, illustrated in Figure 3.\nStage 1: Biomedical Concept Feature Alignment.\nTo balance between concept coverage and\ntraining efficiency, we filter PMC-15M to 600K image-text pairs. These pairs are converted to\ninstruction-following data using a naive expansion method: instructions simply presents the task of\ndescribing the image. For each sample, given the language instruction and image input, we ask the\nmodel to predict the original caption. In training, we keep both the visual encoder and LM weights\nfrozen, and only update the projection matrix. In this way, the image features of vast novel biomedical\n4\ndescribe\nhave\nshow\naffect\nsee\nreceive\nprovide\ntell\nexplain\nline\nundergo\ninterpret\nsurround\ninfiltrate\ncause\nappearance\ndifference\nfinding\nsize\ntype\neffect\naccuracy\nrate\nimprovement\nuptake\nvolume\nlung\nnumber\nstructure\npatient\ntube\npacemaker\ndifference\nimprovement\ntreatment\nkind\ntype\ninformation\nsize\ndifference\ncyst\nluman\nkind\nprocedure\nperformance\nnodule\nvessel\ntissue\ncompression\nshow\nindicate\nrepresent\nuse\nprovide\nreveal\nhave\nplay\nsuggest\ninclude\ncompare\nsection\nlesion\ntissue\ncell\narea\npresence\ncondition\nabnormality\nmicrometer\narea\nregion\nvalue\nhematoxylin\nstain\nstaining\nimaging\ninformation\nrepresentation\nview\ndetail\npresence\nlesion\nmass\nfinding\nappearance\ncause\nmagnification\nshape\nrole\npresence\nimage\ntomography\nscore\n(a) Instruction\n(b) Responses\nChest X-Ray\nMRI\nHistology\nGross pathology\nCT\n(c) Frequencies of images and QA pairs on the five domains.\nFigure 2: The data statistics of biomedical multimodal instruction-following data: (a,b) The root\nverb-noun pairs of instruction and responses, where the inner circle of the plot represents the root\nverb of the output response, and the outer circle represents the direct nouns. (c) The distribution of\nimages and QA pairs on the five domains, one image is shown per domain. The domain example\nimages are from [3, 33, 4, 29, 48].\nStage 1\nStage 2\nLLaVA\nLLaVA-Med\nMedical Concept Alignment\nMedical Instruction Tuning\n1 epoch on 600K samples\n3 epochs on 60K samples\n7 Hours\n8 Hours\nDownstream\nq Medical VQA\no\nVQA-Radiology\no\nSLAKE\no\nPathology-VQA\nq Medical Visual Chat\nFigure 3: LLaVA-Med was initialized with the general-domain LLaVA and then continuously trained\nin a curriculum learning fashion (first biomedical concept alignment then full-blown instruction-\ntuning). We evaluated LLaVA-Med on standard visual conversation and question answering tasks.\nvisual concepts can be aligned to their textual word embeddings in the pre-trained LM. This stage can\nbe understood as expanding the vocabulary of aligned image-text tokens to the biomedical domain.\nStage 2: End-to-End Instruction-Tuning.\nWe only keep the visual encoder weights frozen, and\ncontinue to update both the pre-trained weights of the projection layer and LM. To train the model to\nfollow various instructions and complete tasks in a conversational manner, we develop a biomedical\nchatbot by fine-tuning our model on the biomedical language-image instruction-following data\ncollected in Section 3. As demonstrated in the experiments to be described later, the LLaVA-Med\nmodel at this stage is able to not only be served as a biomedical visual assistant to interact with\nusers, but also achieve good zero-shot task transfer performance when evaluated on well-established\nbiomedical VQA datasets.\nFine-tuning to Downstream Datasets.\nFor some specific biomedical scenarios, there is a need of\ndeveloping highly accurate and dataset-specific models to improve the service quality of the assistant.\nWe fine-tune LLaVA-Med after the two-stage training on three biomedical VQA datasets [27],\ncovering varied dataset sizes and diverse biomedical subjects. Given a biomedical image as context,\n5\nmultiple natural language questions are provided, the assistant responds in free-form text for both the\nclose-set and open-set questions, with a list of candidate answers constructed in the prompt for each\nclose-set question.\nDiscussion.\nWe discuss three favorable properties/implications of LLaVA-Med: (i) Affordable\ndevelopment cost. Instead of scaling up data/model for the best performance, we aim to provide\naffordable and reasonable solutions with low development cost: it takes 7 and 8 hours for stage 1 and 2\non 8 40G A100 GPUs, respectively (see Table 5 for detailed numbers). (ii) A recipe for many domains.\nThough this paper focuses on biomedical domains, the proposed adaptation procedure is generalizable\nto other vertical domains such as gaming and education, where novel concepts and domain knowledge\nare needed to build a helpful assistant. Similar to the don\u2019t stop pre-training argument in [11], we\nconsider a scalable pipeline to create domain-specific instruct data from large unlabelled data, and\nadvocate don\u2019t stop instruction-tuning to build customized LMM. (iii) Low serving cost. While the\nmodel size of general LMM can be giant and serving cost can be prohibitively high, customized LMM\nhas its unique advantages in low serving cost. (iv) Smooth Model Adaptation. Alternatively, the\nnetwork architecture allows us to initialize the vision encoder from BioMedCLIP [49], or initialize\nthe language model from Vicuna [43], which may lead to higher performance. However, adapting\nfrom LLaVA smooth adaptation as a chatbot, where model\u2019s behaviors transit from layperson to a\nprofessional assistant that is able to provide helpful domain-specific response.\n5\nExperiments\nWe conduct experiments to study two key components, the quality of the produced multimodal\nbiomedical instruction-following data, and performance of LLaVA-Med. We consider two research\nevaluation settings: (1) What is the performance of LLaVA-Med as an open-ended biomedcal visual\nchatbot? (2) How does LLaVA-Med compare to existing methods on standard benchmarks? To\nclarify, throughout the entire experiments, we only utilize the language-only GPT-4.\n5.1\nBiomedical Visual Chatbot\nTo evaluate the performance of LLaVA-Med on biomedical multimodal conversation, we construct\nan evaluation dataset with 193 novel questions. For this test dataset, we randomly selected 50 unseen\nimage and caption pairs from PMC-15M, and generate two types of questions: conversation and\ndetailed description. The conversation data is collected using the same self-instruct data generation\npipeline as for the 2nd stage. Detailed description questions were randomly selected from a fixed set\n[24] of questions to elicit detailed description responses.\nWe leverage GPT-4 to quantify the correctness of the model answer to a question when given the\nimage context and caption. GPT-4 makes a reference prediction, setting the upper bound answer\nfor the teacher model. We then generate response to the same question from another LMM. Given\nresponses from the two assistants (the candidate LMM and GPT-4), the question, figure caption, and\nfigure context, we ask GPT-4 to score the helpfulness, relevance, accuracy, and level of details of the\nresponses from the two assistants, and give an overall score on a scale of 1 to 10, where a higher score\nindicates better overall performance. GPT-4 is also asked to provide a comprehensive explanation the\nevaluation, for us to better understand the models. We then compute the relative score using GPT-4\nreference score for normalization.\nQuestion Types\nDomains\nOverall\nConversation\nDescription\nCXR\nMRI\nHistology\nGross\nCT\n(Question Count)\n(143)\n(50)\n(37)\n(38)\n(44)\n(34)\n(40)\n(193)\nLLaVA\n39.4\n26.2\n41.6\n33.4\n38.4\n32.9\n33.4\n36.1\nLLaVA-Med\nStage 1\n22.6\n25.2\n25.8\n19.0\n24.8\n24.7\n22.2\n23.3\n10K\n42.4\n32.5\n46.1\n36.7\n43.5\n34.7\n37.5\n39.9\n60K\n53.7\n36.9\n57.3\n39.8\n49.8\n47.4\n52.4\n49.4\n60K-IM\n55.1\n36.4\n56.2\n40.4\n52.7\n51.8\n50.1\n50.2\nTable 1: Performance comparison of mulitmodal chat instruction-following abilities, measured by the\nrelative score via language GPT-4 evaluation.\n6\nVisual input example, Biomedical image from [6]\nSource: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7398722/\nUser\nWhat is shown in this image?\nLLaVA-Med\nThe image is a chest X-ray (CXR) that shows bilateral patchy infiltrates, which are areas of increased\nopacity in the lungs. These infiltrates can be indicative of various lung conditions, such as infections,\ninflammation, or other lung diseases.\nUser\nAre there any devices implanted in this patient?\nLLaVA-Med\nYes, it appears that the patient has several devices implanted. These devices may include endotracheal\ntubes, central venous catheters, and/or pulmonary artery catheters. These devices are typically used for\nmonitoring and managing various medical conditions, as well as for providing support during surgical\nprocedures.\nUser\nWhat is shown in this image?\nLLaVA\nThe image shows a ribcage with wires coming out of it, indicating that it is likely a medical illustration or\ndiagram.\nUser\nAre there any devices implanted in this patient?\nLLaVA\nYes, there are several devices protruding from the patient\u2019s chest, which are likely medical devices used\nfor treatment or monitoring.\nUser\nWhat is shown in this image?\nLanguage GPT-4\nThe image shows a chest x-ray taken on Day 2 of a patient\u2019s admission after intubation. You can see the\nendotracheal tube marked by a yellow line, and rapidly worsening ground-glass opacities throughout the\nlungs. There is also relative subpleural sparing, which is indicated by red arrows.\nUser\nAre there any devices implanted in this patient?\nLanguage GPT-4\nYes, there is an endotracheal tube implanted in the patient, as indicated by the yellow line in the image.\nTable 2: Example comparison of medical visual chat and reasoning capabilities. The language-only\nGPT-4 is considered as the performance upper bound, as the golden captions and inline mentions\nare fed into GPT-4 as the context, without requiring the model to understand the raw image.\nThe results are reported in Table 1. LLaVA-Med with Stage-1 training alone is insufficient as a\nchatbot, as it loses its ability to follow diverse instructions, though biomedical concept coverage is\nimproved. LLaVA-Med with the full two-stage training consistently outperforms the general domain\nLLaVA, and training with larger instruct data (from 10K to 60K samples) leads to higher performance.\nWhen inline mentions are considered in self-instruct, the generated data 60K-IM slightly improves\nthe chat ability. The results demonstrate the effectiveness of the strategies in biomedical instruction-\nfollowing data collection as well as the value of dataset assets. Overall, for the best LLaVA-Med,\nit matches the 50.2% performance of GPT-4. Note that GPT-4 generates response by considering\nground-truth caption and golden inline mentions, without understanding the images. Though not a\nfair comparison between LMMs and GPT-4, GPT-4 is a consistent and reliable evaluation tool.\nIn Table 2, we provide examples on the biomed visual conversations of different chatbots. LLaVA-\nMed precisely answers the questions with biomedical knowledge, while LLaVA behaves like a\nlayperson, who hallucinate based on commonsense. Since the multimodal GPT-4 is not publicly\navailable, we resort to language-only GPT-4 for comparison. We feed golden captions and inline\nmentions into GPT-4 as the context, it generates knowledgeable response through re-organizing the\ninformation in the conversational manner.\n5.2\nPerformance on Established Benchmarks\nDataset Description.\nWe train and evaluate LLaVA-Med on three biomedical VQA datasets. The\ndetailed data statistics are summarized in Table 3.\n\u2022 VQA-RAD [16] contains 3515 QA pairs generated by clinicians and 315 radiology images that\nare evenly distributed over the head, chest, and abdomen. Each image is associated with multiple\nquestions. Questions are categorized into 11 categories: abnormality, attribute, modality, organ\nsystem, color, counting, object/condition presence, size, plane, positional reasoning, and other.\nHalf of the answers are closed-ended (i.e., yes/no type), while the rest are open- ended with either\none-word or short phrase answers.\n7\nVQA-RAD\nSLAKE\nPathVQA\nDataset\nTrain\nTest\nTrain\nVal\nTest\nTrain\nVal\nTest\n# Images\n313\n203\n450\n96\n96\n2599\n858\n858\n# QA Pairs\n1797\n451\n4919\n1053\n1061\n19,755\n6279\n6761\n# Open\n770\n179\n2976\n631\n645\n9949\n3144\n3370\n# Closed\n1027\n272\n1943\n422\n416\n9806\n3135\n3391\nTable 3: Dataset statistics. For SLAKE, only the English subset is considered for head-to-head\ncomparison with existing methods.\n\u2022 SLAKE [23] is a Semantically-Labeled Knowledge-Enhanced dataset for medical VQA. It\nconsists of 642 radiology images and over 7000 diverse QA pairs annotated by experienced\nphysicians, where the questions may involve external medical knowledge (solved by provided\nmedical knowledge graph), and the images are associated with rich visual annotations, including\nsemantic segmentation masks and object detection bounding boxes. Besides, SLAKE includes\nricher modalities and covers more human body parts than the currently available dataset, including\nbrain, neck, chest, abdomen, and pelvic cavity. Note SLAKE is bilingual dataset with English\nand Chinese. When compared with existing methods, we only consider the English subset.\n\u2022 PathVQA [13] is a dataset of pathology images. It contains a total of 4998 pathology images\nwith 32,799 QA pairs. Every image has several questions that relate to multiple aspects such as\nlocation, shape, color, appearance, etc. The questions are categorized into two types, with several\nvarieties: open-ended questions such as why, what, how, where, etc., and closed-ended questions.\nEvaluation Metrics.\nFor the closed-set questions, we report the accuracy. For open-set questions,\nwe use recall to evaluate the ratio that ground-truth tokens appear in the generated sequences. In\nthe literature, the unique answers in the training set are considered as the answer candidates, from\nwhich the models can select to predict answers for testing questions. Since we do not provide any\nconstraint for the responses to open-set questions, our formulation is closer to open-set nature, but is\nintrinsically harder.\nComparisons with SoTA.\nWe compare LLaVA-Med with the general domain LLaVA and existing\nrepresentative methods in Table 4 (a). First, All LLaVA-Med variants outperform LLaVA. While\nthe difference of language model initialization from LLaVA or Vicuna is minor, the initialization of\nvision encoder from BioMed CLIP is slightly better than from general-domain CLIP. Second, the\nfine-tuning performance of LLaVA-Med is higher than supervised SoTA on the closed-set questions\non VQA-RAD and PathVQA. This validates LLaVA-Med\u2019s strong ability in following instruction\nto complete biomedical tasks, when clear instructions are provided (e.g., , yes or no). Third, for\nopen-set questions, LLaVA-Med achieves SoTA on SLAKE, while its performance is limited on\nother datasets, especially compared with existing methods. This is perhaps because the open-set\nbiomedical questions can be ambiguous without constraining their excepted answer options.\nAblation Studies.\nTo study the impact of our curated instruction data and hyper-parameters in the\ntraining pipeline, we report the performance of different model variants in Table 4 (b). Several findings\nare confirmed: (i) LLaVA-Med consistently outperforms LLaVA by a large margin, indicating the\neffectiveness of our biomedical domain-specific adaptation. The performance gaps on zero-shot\nare larger than that in fine-tuned settings, showing that LLaVA-Med is clearly a better option than\nLLaVA when deploying one model for various scenarios in the wild. (ii) Training longer in Stage 1\nimproves zero-shot transfer, but Stage 1 alone is not sufficient, because the single image captioning\ninstruction in Stage 1 may encourage the model to lose its ability in follow diverse instructions. (iii)\nInstruction-following data in Stage 2 is critical, and the performance is generally improved, when\nthe instruct data amount increases from 10K to 60K. The 60K-IM data provides the best averaged\nzero-shot and fine-tuned performance, respectively, validating the effectiveness of considering inline\nmention as external knowledge in data creation. (iv) Fine-tuning longer on downstream datasets\ntill 9 epochs benefits the performance, especially on checkpoints with 3-epoch training in Stage 2.\nIncreasing language model size from 7B to 13B improves the overall zero-shot performance and\nfine-tuned performance. We suggest practitioners to choose the appropriate quality-cost trade-off, by\nreferring to the running time in Table 5.\n8\nVQA-RAD\nSLAKE\nPathVQA\nMethod\nRef\nOpen Closed\nRef\nOpen Closed\nRef\nOpen Closed\nSupervised finet-tuning results with our own experiment runs\nLLaVA\n50.00\n65.07\n78.18\n63.22\n7.74\n63.20\nLLaVA-Med (From LLaVA)\n61.52\n84.19\n83.08\n85.34\n37.95\n91.21\nLLaVA-Med (From Vicuna)\n64.39\n81.98\n84.71\n83.17\n38.87\n91.65\nLLaVA-Med (BioMed CLIP)\n64.75\n83.09\n87.11\n86.78\n39.60\n91.09\nRepresentative & SoTA methods with numbers reported in the literature\nVL Encoder\u2013Decoder [5]\n71.49\n82.47\n71.49\n85.61\nQ2ATransformer [26]\n79.19\n81.20\n54.85\n88.85\nPrefix T. Medical LM [41]\n84.30\n82.01\n40.00\n87.00\nPubMedCLIP [8]\n60.10\n80.00\n78.40\n82.50\nBiomedCLIP [49]\n67.60\n79.80\n82.05\n89.70\nM2I2 [22]\n66.50\n83.50\n74.70\n91.10\n36.30\n88.00\n(a) Comparison with prior state-of-the-art supervised methods. For open-ended questions, prior methods still\nformulate the problem as classification among distinct answers in the training set, which may overestimate their\ngeneralizability as these datasets are unusual in that the test answers are almost always present in training.\nLLaVA-Med Model Variants\nVQA-RAD\nSLAKE\nPathVQA\nAverage\nInstruct\nStage 1 Stage 2 FT Open Closed Open Closed Open Closed\nCLIP Vision Encoder [36], 7B Language Model\n0\n1\n0\n0\n15.27\n12.50 18.55\n13.46\n6.26\n13.51\n13.26\n0\n3\n0\n0\n15.33\n15.44 23.61\n15.38\n6.35\n14.74\n15.14\n10K\n1\n3\n0\n25.79\n57.35 31.50\n51.68\n8.49\n59.66\n39.08\n10K\n3\n3\n0\n28.44\n59.56 22.63\n43.99\n5.40\n52.67\n35.45\n10K\n1\n3\n1\n36.39\n55.88 71.64\n56.49 25.50\n82.87\n54.79\n10K\n1\n3\n3\n18.59\n55.51 78.60\n63.46 34.02\n86.94\n56.19\n60K\n1\n1\n0\n29.80\n55.15 38.08\n50.00 11.70\n59.66\n40.73\n60K\n1\n3\n0\n29.67\n60.29 35.53\n53.85 11.76\n53.20\n40.72\n60K\n1\n3\n1\n22.63\n58.09 72.75\n54.33 24.19\n71.60\n50.60\n60K\n1\n3\n3\n54.12\n64.71 79.33\n64.90 17.18\n71.37\n58.60\n60K-IM\n1\n1\n0\n29.67\n61.40 38.44\n52.40 11.41\n56.24\n41.59\n60K-IM\n1\n3\n0\n28.23\n61.40 39.17\n52.16 12.30\n54.05\n41.22\n60K-IM\n1\n3\n1\n28.61\n56.25 70.58\n54.57 11.17\n59.19\n46.73\n60K-IM\n1\n3\n3\n55.50\n66.54 80.57\n64.18 35.88\n89.15\n65.30\n60K-IM\n1\n3\n9\n66.26\n80.88 82.30\n84.86 37.59\n91.54\n73.90\n60K-IM\n1\n3\n15 61.53\n84.19 83.08\n85.34 37.95\n91.21\n73.88\n60K-IM\n1\n3\n18 61.37\n81.25 84.24\n83.17 37.88\n91.39\n73.22\nCLIP Vision Encoder [36], 13B Language Model\n60K-IM\n1\n3\n0\n31.66\n61.40 37.71\n49.76 11.34\n49.63\n40.25\n60K-IM\n1\n3\n9\n64.58\n77.94 84.97\n85.58 38.82\n92.39\n74.05\nBioMed CLIP Vision Encoder [49], 7B Language Model\n60K-IM\n1\n3\n0\n37.84\n60.66 39.73\n54.33 11.65\n49.07\n42.21\n60K-IM\n1\n3\n9\n64.75\n83.09 87.11\n86.78 39.60\n91.09\n75.40\nLLaVA\n0\n0\n0\n20.74\n59.19 26.82\n50.24\n8.74\n45.65\n35.23\n(b) Ablation studies with varying number of training epochs at different stages. \u201cFT\u201d is Fine-Tuning. 60K-IM\nindicates the instruct data generated with inline mentions. The gray rows are zero-shot performance of LLaVA-\nMed trained with different instruct data, they are selected to show in subtable (a).\nTable 4: Quantitative results on three established biomedical VQA datasets. For open-set questions,\nwe report the recall for our free-form text generation method in column Open. For closed-set\nquestions, we report the accuracy in column Closed. Bold indicates LLaVA-Med achieves new SoTA.\n9\nStage 1\nStage 2\nVQA-RAD\nSLAKE\nPathVQA\n1\n3\nInstruct\n1\n3\n1\n3\n1\n3\n1\n3\n6.8\n19.4\n10K\n0.6\n1.8\n0.3\n0.6\n0.6\n1.0\n1.0\n2.5\n60K\n2.6\n8.0\nTable 5: Running time (hours) for 1 and 3-epoch training at each stage, with batch size 128 on eight\nA100 GPUs.\nBiomedical image from the SLAKE Bilingual dataset. [23]\nUser\n\u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\nLLaVA-Med\nThe image is an abdominal computed tomography (CT)\nscan, which is a cross-sectional view of the patient\u2019s ab-\ndomen. The image is taken in the portal phase, which\nmeans it is captured after the administration of contrast ma-\nterial to enhance the visibility of certain structures within\nthe abdomen.\nAnswer\nCT\nUser\n\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u6838\u78c1\u5171\u632f\u7684\u54ea\u79cd\u7c7b\u578b?\nLLaVA\nThe image shows a typical appearance of a nodular lesion\non T1-weighted MR images. The lesion appears hyper-\nintense, which means it is brighter than the surrounding\ntissue. This can be indicative of various types of lesions,\nand further evaluation would be needed to determine the\nspecific nature and cause of the lesion.\nAnswer\nT1\nTable 6: Examples on zero-shot on Chinese questions. Though LLaVA-Med training data itself does\nnot include Chinese, it is able to correctly understand the Chinese questions.\nCase Study I: Zero-shot on Chinese Questions.\nFor the LLaVA-Med trained on 60K-IM data, we\nprovide Chinese questions on SLAKE dataset. Though LLaVA-Med training does not include Chinese\ninstruction-following data, we show in Table 6 that LLaVA-Med is able to correctly understand the\nChinese questions and respond the correct answers, probably due to the multilingual knowledge\nlearned in LLaMA/Vicuna. Existing models will fail when zero-shot transfer cross languages.\n6\nConclusions\nWe present LLaVA-Med, a large language-and-vision model for the biomedical domain. To create\nthis model, we create high-quality biomedical language-image instruction-following dataset using\na self-instruct approach to build a data curation pipeline using language-only GPT-4 and external\nknowledge. LLaVA-Med demonstrates strong excellent chat abilities with domain knowledge, and\noutperforms previous supervised SoTA on three VQA datasets on certain metrics with subsequent\nfine-tuning.\nWhile we believe that LLaVA-Med represents a significant step towards building a useful biomedical\nvisual assistant, we note that LLaVA-Med is limited by hallucinations and weak in-depth reasoning\ncommon to many LMMs. Future work is directed toward improving quality and reliability.\nReferences\n[1] Clinical Camel. https://wanglab.ml/clinical_camel.html, 2023. 2\n[2] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy\nJones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a\nlaboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. 1\n[3] Malek Ayoub, Megan Quamme, Abdul-Rahman K Abdel-Reheem, Poe Lwin, and Megan K\nQuamme. Covid or not covid? a great mimicker behind the smoke screen. Cureus, 13(11),\n2021. 5\n[4] Bappy Basak, Alexander Haragan, Michael Shackcloth, and Joyce Thekkudan. Chondromyxoid\nfibroma of the rib: A rare benign tumor with potential for local recurrence. Cureus, 13(10),\n2021. 5\n10\n[5] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Laila Bashmal, and Mansour Zuair. Vision\u2013\nlanguage model for visual question answering in medical imagery. Bioengineering, 2023. 3,\n9\n[6] Anchit Bharat, Nikita Jain, Belaal Sheikh, Hafiz Jeelani, and Maryna Shayuk. Vaping-induced\nlung injury: An uncharted territory. Cureus, 12, 07 2020. 7\n[7] Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C Castro, Anton Schwaighofer,\nStephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle,\net al. Making the most of text semantics to improve biomedical vision\u2013language processing. In\nECCV. Springer, 2022. 2\n[8] Sedigheh Eslami, Christoph Meinel, and Gerard De Melo. Pubmedclip: How much does clip\nbenefit visual question answering in the medical domain? In Findings of the Association for\nComputational Linguistics: EACL 2023, pages 1151\u20131163, 2023. 2, 3, 9\n[9] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision-\nlanguage pre-training: Basics, recent advances, and future trends. Foundations and Trends\u00ae in\nComputer Graphics and Vision, 2022. 1\n[10] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan\nNaumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining\nfor biomedical natural language processing. ACM Transactions on Computing for Healthcare\n(HEALTH), 3(1):1\u201323, 2021. 2\n[11] Suchin Gururangan, Ana Marasovi\u00b4c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. Don\u2019t stop pretraining: Adapt language models to domains and tasks. arXiv\npreprint arXiv:2004.10964, 2020. 6\n[12] Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser,\nAlexander L\u00f6ser, Daniel Truhn, and Keno K Bressem. Medalpaca\u2013an open-source collection of\nmedical conversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023. 2\n[13] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+\nquestions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020. 3, 8\n[14] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling clinical notes and\npredicting hospital readmission. arXiv preprint arXiv:1904.05342, 2019. 2\n[15] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P\nLungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified\npublicly available database of chest radiographs with free-text reports. Scientific data, page 317,\n2019. 2\n[16] Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of\nclinically generated visual questions and answers about radiology images. Scientific data, 2018.\n7\n[17] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234\u20131240, 2020. 2\n[18] Peter Lee, Sebastien Bubeck, and Joseph Petro. Benefits, limits, and risks of gpt-4 as an ai\nchatbot for medicine. New England Journal of Medicine, 388(13):1233\u20131239, 2023. 2\n[19] Peter Lee, Carey Goldberg, and Isaac Kohane. The ai revolution in medicine: Gpt-4 and beyond.\n2023. 2\n[20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented\ngeneration for knowledge-intensive NLP tasks. NeurIPS, 2020. 3\n11\n[21] Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang,\nPing Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, and Jianfeng Gao. ELEVATER: A bench-\nmark and toolkit for evaluating language-augmented visual models. In NeurIPS Track on\nDatasets and Benchmarks, 2022. 1\n[22] Pengfei Li, Gang Liu, Lin Tan, Jinying Liao, and Shenjun Zhong. Self-supervised vision-\nlanguage pretraining for medical visual question answering. arXiv preprint arXiv:2211.13594,\n2022. 3, 9\n[23] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: A semantically-\nlabeled knowledge-enhanced dataset for medical visual question answering. In International\nSymposium on Biomedical Imaging (ISBI). IEEE, 2021. 8, 10\n[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023. 1, 2, 4, 6\n[25] Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee, and Chunyuan\nLi. Learning customized visual models with retrieval-augmented knowledge. arXiv preprint\narXiv:2301.07094, 2023. 3\n[26] Yunyi Liu, Zhanyu Wang, Dong Xu, and Luping Zhou. Q2atransformer: Improving medical\nvqa via an answer querying decoder. arXiv preprint arXiv:2304.01611, 2023. 3, 9\n[27] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind\nTafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought\nchains for science question answering. Advances in Neural Information Processing Systems,\n2022. 5\n[28] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu.\nBiogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings\nin Bioinformatics, 2022. 2, 3\n[29] Hassan Mirmohammad Sadeghi, Abbas Karimi, Samira Derakhshan, Pouyan Aminishakib, and\nKiarash Parchami. Conventional osteosarcoma of the mandible: Report of a rare case. Clinical\nCase Reports, 9(9):e04843, 2021. 5\n[30] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Ca-\npabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.\n2\n[31] OpenAI. ChatGPT. https://openai.com/blog/chatgpt/, 2022. 2\n[32] OpenAI. GPT-4 technical report. https://arxiv.org/abs/2303.08774, 2023. 1, 2\n[33] Kyriakos A Papavasiliou, Dimitrios Stamiris, Stavros Stamiris, Antonia Bintoudi, and Elefthe-\nrios Tsiridis. Quadratus femoris partial tear secondary to occult ischiofemoral impingement.\nJournal of Orthopaedic Case Reports, 11(9):7, 2021. 5\n[34] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\nwith GPT-4. arXiv preprint arXiv:2304.03277, 2023. 2\n[35] Roger Kevin Pringle and Lawrence H Wyatt. The appropriate use of radiography in clinical\npractice: a report of two cases of biomechanical versus malignant spine pain. Chiropractic &\nOsteopathy, 14(1):1\u20138, 2006. 4\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. 9\n[37] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 2019. 3\n12\n[38] George Shih, Carol C Wu, Safwan S Halabi, Marc D Kohli, Luciano M Prevedello, Tessa S\nCook, Arjun Sharma, Judith K Amorosa, Veronica Arteaga, Maya Galperin-Aizenberg, et al.\nAugmenting the national institutes of health chest radiograph dataset with expert annotations of\npossible pneumonia. Radiology: Artificial Intelligence, 2019. 2\n[39] Chang Shu, Baian Chen, Fangyu Liu, Zihao Fu, Ehsan Shareghi, and Nigel Collier. Visual\nmed-alpaca: A parameter-efficient biomedical llm with visual capabilities. 2023. 2\n[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3\n[41] Tom van Sonsbeek, Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees GM Snoek,\nand Marcel Worring. Open-ended medical visual question answering through prefix tuning of\nlanguage models. arXiv preprint arXiv:2303.05977, 2023. 3, 9\n[42] A Venigalla, J Frankle, and M Carbin. BiomedLM: a domain-specific large language model for\nbiomedical text. MosaicML. Accessed: Dec, 23, 2022. 3\n[43] Vicuna. Vicuna: An open-source chatbot impressing GPT-4 with 90%* chatgpt quality. https:\n//vicuna.lmsys.org/, 2023. 3, 6\n[44] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu.\nHuatuo: Tuning llama model with chinese medical knowledge, 2023. 2\n[45] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Further\nfinetuning llama on medical papers. arXiv preprint arXiv:2304.14454, 2023. 2\n[46] Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Qian Wang, and Dinggang\nShen. Doctorglm: Fine-tuning your chinese doctor is not a herculean task. arXiv preprint\narXiv:2304.01097, 2023. 2\n[47] Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. Chatdoctor: A medical\nchat model fine-tuned on llama model using medical domain knowledge. arXiv preprint\narXiv:2303.14070, 2023. 2\n[48] Mansoor Zafar, Abdul Wahab Paracha, Muteeb Ashraf, Tila Muhammad, Mark Whitehead,\nMuhammad Toqeer, and Abdul Paracha. Delayed spontaneous regression of metastatic gastric\ncancer: A case report of a rare finding. Cureus, 13(12), 2021. 5\n[49] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh\nRao, Mu Wei, Naveen Valluri, Cliff Wong, et al. Large-scale domain-specific pretraining for\nbiomedical vision-language processing. arXiv preprint arXiv:2303.00915, 2023. 2, 3, 6, 9\n13\nA\nData\nInstructions for brief image description.\nThe list of instructions used to briefly describe the image\ncontent are shown in Table 7. They present the same meaning with natural language variance.\n\u2022 \"Describe the image concisely.\"\n\u2022 \"Provide a brief description of the given image.\"\n\u2022 \"Offer a succinct explanation of the picture presented.\"\n\u2022 \"Summarize the visual content of the image.\"\n\u2022 \"Give a short and clear explanation of the subsequent image.\"\n\u2022 \"Share a concise interpretation of the image provided.\"\n\u2022 \"Present a compact description of the photo\u2019s key features.\"\n\u2022 \"Relay a brief, clear account of the picture shown.\"\n\u2022 \"Render a clear and concise summary of the photo.\"\n\u2022 \"Write a terse but informative summary of the picture.\"\n\u2022 \"Create a compact narrative representing the image presented.\"\nTable 7: The list of instructions for brief image description.\nInstructions for detailed image description.\nThe list of instructions used to describe the image\ncontent in detail are shown in Table 8. They present the same meaning with natural language variance.\n\u2022 \"Describe the following image in detail\"\n\u2022 \"Provide a detailed description of the given image\"\n\u2022 \"Give an elaborate explanation of the image you see\"\n\u2022 \"Share a comprehensive rundown of the presented image\"\n\u2022 \"Offer a thorough analysis of the image\"\n\u2022 \"Explain the various aspects of the image before you\"\n\u2022 \"Clarify the contents of the displayed image with great detail\"\n\u2022 \"Characterize the image using a well-detailed description\"\n\u2022 \"Break down the elements of the image in a detailed manner\"\n\u2022 \"Walk through the important details of the image\"\n\u2022 \"Portray the image with a rich, descriptive narrative\"\n\u2022 \"Narrate the contents of the image with precision\"\n\u2022 \"Analyze the image in a comprehensive and detailed manner\"\n\u2022 \"Illustrate the image through a descriptive explanation\"\n\u2022 \"Examine the image closely and share its details\"\n\u2022 \"Write an exhaustive depiction of the given image\"\nTable 8: The list of instructions for detailed image description.\n14\nB\nPrompts\nPrompting GPT-4 to generate medical visual instruction-following data\nmessages = [ {\"role\":\"system\", \"content\": \"\"\"You are an AI assistant specialized in\nbiomedical topics.\nYou are provided with a text description (Figure Caption) of a figure image from a biomedical\nresearch paper. In some cases, you may have additional text (Figure Context) that mentions\nthe image. Unfortunately, you don\u2019t have access to the actual image.\nBelow are requirements for generating the questions and answers in the conversation:\n- Avoid quoting or referring to specific facts, terms, abbreviations, dates, numbers, or\nnames, as these may reveal the conversation is based on the text information, rather than\nthe image itself. Focus on the visual aspects of the image that can be inferred without\nthe text information.\n- Do not use phrases like \"mentioned\", \"caption\", \"context\" in the conversation. Instead,\nrefer to the information as being \"in the image.\"\n- Ensure that questions are diverse and cover a range of visual aspects of the image.\n- The conversation should include at least 2-3 turns of questions and answers about the\nvisual aspects of the image.\n- Answer responsibly, avoiding overconfidence, and do not provide medical advice or\ndiagnostic information. Encourage the user to consult a healthcare professional for\nadvice.\"\"\"}\n]\nfor sample in fewshot_samples:\nmessages.append({\"role\":\"user\", \"content\":sample[\u2018context\u2019]})\nmessages.append({\"role\":\"assistant\", \"content\":sample[\u2018response\u2019]}\n)\nmessages.append({\"role\":\"user\", \"content\":query})\nFigure 4: messages we use to prompt GPT-4 to generate medical visual instruction-following\ndata. Manually curated few-shot examples are included in the prompt, where each example has\ninput sample[\u2018context\u2019] and output sample[\u2018response\u2019]. Please see Figure 5 for one of the\nfew-shot examples.\n15\nOne of the few-shot examples used in the prompt\nExample input:\nFigure Caption:\nFigure 2: Chest X-ray. Cardiomegaly with diffuse bilateral interstitial infiltrates and a\nright-sided pleural effusion.\nFigure Context:\n- His respiratory status continued to worsen and on hospital day number 14 , he required\nintubation and mechanical ventilation for hypoxemic respiratory failure ( Figure 2 ) .\nExample output:\nUser: Which side of the lung has a pleural effusion?\nAssistant: The right side of the lung has a pleural effusion.\nUser: According to the CT image, does the patient has cardiomegaly?\nAssistant: The image appeares to be a chest x-ray. However, it\nindicates that the patient has cardiomegaly, which means an enlarged\nheart. Additionally, there are diffuse bilateral interstitial\ninfiltrates and a right-sided pleural effusion present in the image.\nUser: Additional Info: The patient\u2019s respiratory status continued\nto worsen and on hospital day number 14, requiring intubation and\nmechanical ventilation for hypoxemic respiratory failure.\nAssistant: Thank you for providing additional information about\nthe patient\u2019s condition. Based on the information you provided, it\nappears that the patient\u2019s respiratory status continued to worsen and\non hospital day number 14, they required intubation and mechanical\nventilation for hypoxemic respiratory failure. This suggests that\nthe patient\u2019s lung function has significantly declined, and they\nmay be experiencing respiratory distress. It is important for the\nmedical team to closely monitor the patient\u2019s condition and provide\nappropriate treatment to manage their symptoms and improve their lung\nfunction.\nFigure 5: One of the few-shot examples used in our prompt to construct medical visual instruction-\nfollowing data. \u201cFigure Context\u201c in the input provides additional context by including sentences from\nthe corresponding PubMed paper that mention the figure.\n16\nVisual input example, Biomedical image:\nUser\nWhat is this image about?\nLLaVA-Med\nThe image is a coronal view of a computed tomography (CT) scan of the abdomen and\npelvis. It provides a detailed view of the internal organs and structures within the abdominal\nand pelvic cavities.\nUser\nWhat is abnormal about this image?\nLLaVA-Med\nThe abnormality in this image is the presence of a large left-sided pelvic mass. This mass\ncould be due to various reasons, such as a tumor, cyst, or other growth. Further evaluation\nand tests would be needed to determine the exact cause and appropriate treatment for this\nabnormality.\nUser\nWhat is this image about?\nLLaVA\nThe image is a medical illustration showing a person\u2019s internal anatomy, especially focusing\non their large and small intestines, stomach, liver and kidneys. The organs are highlighted\nin different colors, making it easy to identify each organ\nUser\nWhat is abnormal about this image?\nLLaVA\nAs a visual AI assistant, I cannot determine the specific abnormality on the chest X-ray\nbased on the provided image description. The image description mainly focuses on the\norgans and their colors...\nTable 9: Example comparison of medical visual chat and reasoning capabilities.\n17\n"
  },
  {
    "title": "Example-based Motion Synthesis via Generative Motion Matching",
    "link": "https://arxiv.org/pdf/2306.00378.pdf",
    "upvote": "6",
    "text": "Example-based Motion Synthesis via Generative Motion Matching\nWEIYU LI\u2217\u2020, Shandong University, China\nXUELIN CHEN\u2217\u2021, Tencent AI Lab, China\nPEIZHUO LI, ETH Zurich, Switzerland\nOLGA SORKINE-HORNUNG, ETH Zurich, Switzerland\nBAOQUAN CHEN, Peking University, China\nSkeleton\n(c) Key frame-guided generation\n(d) Infinite looping\n(a) Random generation\n(b) Motion completion\nFig. 1. Our generative framework enables a variety of example-based motion synthesis tasks, that usually require long offline training for existing data-driven\nmethods. Given a single or few examples, even with a highly complex skeletal structure (middle), our framework can (a) synthesize a high-quality novel\nmotion, within a fraction of a second; (b) complete a partial motion (lower-body motion) with example motion patches; (c) synthesize a coherent sequence\nguided by a sparse set of keyframes (in blue clothes); (d) generate an infinitely looping animation that starts and ends with a specified pose (in blue clothes).\nWe present GenMM, a generative model that \u201cmines\u201d as many diverse mo-\ntions as possible from a single or few example sequences. In stark contrast to\nexisting data-driven methods, which typically require long offline training\ntime, are prone to visual artifacts, and tend to fail on large and complex\nskeletons, GenMM inherits the training-free nature and the superior qual-\nity of the well-known Motion Matching method. GenMM can synthesize a\nhigh-quality motion within a fraction of a second, even with highly complex\nand large skeletal structures. At the heart of our generative framework lies\n\u2217Joint first authors\n\u2020Work done during an internship at Tencent AI Lab\n\u2021Corresponding author\nAuthors\u2019 addresses: Weiyu Li, weiyuli.cn@gmail.com, Shandong University, China;\nXuelin Chen, xuelin.chen.3d@gmail.com, Tencent AI Lab, China; Peizhuo Li, peizhuo.\nli@inf.ethz.ch, ETH Zurich, Switzerland; Olga Sorkine-Hornung, sorkine@inf.ethz.ch,\nETH Zurich, Switzerland; Baoquan Chen, baoquan@pku.edu.cn, Peking University,\nChina.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n0730-0301/2023/8-ART1 $15.00\nhttps://doi.org/10.1145/3592395\nthe generative motion matching module, which utilizes the bidirectional\nvisual similarity as a generative cost function to motion matching, and op-\nerates in a multi-stage framework to progressively refine a random guess\nusing exemplar motion matches. In addition to diverse motion generation,\nwe show the versatility of our generative framework by extending it to\na number of scenarios that are not possible with motion matching alone,\nincluding motion completion, key frame-guided generation, infinite looping,\nand motion reassembly.\nCCS Concepts: \u2022 Computing methodologies \u2192 Motion processing.\nAdditional Key Words and Phrases: motion synthesis, generative model,\nmotion matching\nACM Reference Format:\nWeiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan\nChen. 2023. Example-based Motion Synthesis via Generative Motion Match-\ning . ACM Trans. Graph. 42, 4, Article 1 (August 2023), 12 pages. https:\n//doi.org/10.1145/3592395\n1\nINTRODUCTION\nThe generation of natural, varied, and detailed motions is a core\nproblem in computer animation. Acquiring large volumes of motion\ndata via a motion capture (mocap) system or manually authoring\nsophisticated animations is known to be costly and tedious. As\nsuch, motion datasets are generally limited, especially in terms of\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\narXiv:2306.00378v1  [cs.GR]  1 Jun 2023\n1:2\n\u2022\nWeiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen\nthe diversity of style, skeletal structures, or creature types, which\nhamper the effectiveness of existing data-driven motion synthesis\nmethods. Advancing generative abilities of synthesizing diverse and\nextensive motions from limited example sequences has therefore\nbecome an important research problem.\nIn recent years, deep learning has taken the field of computer\nanimation by storm. Deep learning methods have demonstrated the\nability to synthesize diverse and natural motions when training on\nlarge and comprehensive datasets [Holden et al. 2016, 2017; Henter\net al. 2020; Tevet et al. 2022a; Raab et al. 2023a; Tseng et al. 2022].\nMore encouragingly, the success was recently reproduced in an\nextremely reduced setting [Li et al. 2022], where only one sequence\nis provided for training, yet, the neural network learns the sample\u2019s\ninternal distribution, and demonstrates the ability to synthesize\ndiverse variants of the input example sequence. Nevertheless, neural\nmotion synthesis methods carry several drawbacks that limit their\napplicability in practice: (i) they require long training time; (ii) they\nare prone to visual artifacts such as jittering or over-smoothing; (iii)\nthey do not scale well to large and complex skeleton structures.\nIn this paper, we explore an alternative approach to the problem.\nWe revisit the classical idea in computer animation \u2013 motion nearest\nneighbors [Lee et al. 2010], which dates back long before the deep\nlearning era and on which the state-of-the-art industrial solution for\ncharacter animation \u2013 motion matching \u2013 was founded [B\u00fcttner and\nClavet 2015], delivering exceptionally high-quality motion synthesis.\nMotion matching produces character animations that appear natural\nand respond to varying local contexts. Using a large mocap database\nas a local approximate of the entire natural motion space, motion\nmatching simply searches for a motion patch that best fits a given\nlocal context. The dependence on a large dataset is, however, at odds\nwith our goal: we are after a generative model that \u201cmines\u201d as many\ndiverse motions as possible from a single or few examples. Inspired\nby the work of Granot et al. [2022] in image synthesis, we take the\nfollowing insights for casting motion matching into our generative\nmodel and yield generative motion matching (GenMM, pronounced\n\"gem\"). First, to retain the motion quality of motion matching and\ninject generative capabilities, we exploit bidirectional similarity in-\ntroduced in [Simakov et al. 2008] as a new generative cost function\nfor motion matching. The bidirectional similarity serves the purpose\nof comparing the patch distribution between the example and the\nsynthesized sequence. Specifically, it encourages the synthesized se-\nquence to contain only motion patches from the example sequences,\nand vice versa, the examples should only contain motion patches\nfrom the synthesis. Consequently, no artifacts are introduced in\nthe synthesis, and importantly, no critical motion patches are lost\neither. Second, we use a multi-stage framework to progressively\nsynthesize a motion sequence that has minimal patch distribution\ndiscrepancy with the example, capturing patch distributions from\nvarying temporal resolutions. Lastly, we utilize the observation that\nthe generative diversity of GAN-based methods stems primarily\nfrom the unconditional noise input [Granot et al. 2022]: we input\nnoise to the coarsest synthesis stage, and achieve highly diverse\nsynthesis results.\nWe demonstrate that GenMM is more than competent in produc-\ning diverse motions from only a small set of input examples. Notably,\ncompared to existing works, GenMM offers several advantages:\n\u2022 GenMM runs very fast, without any pre-training. A motion\nsequence can be synthesized within a fraction of a second.\n\u2022 GenMM inherits the appealing nature of motion matching,\nproducing motions of high quality and fidelity.\n\u2022 GenMM scales smoothly to highly complex skeletons (see the\ncharacter with 433 joints in Figure 1), where neural networks\nstruggle [Li et al. 2022].\n\u2022 It is easy to extend GenMM to inputs with multiple sequences\nand encourage the synthesis to cover all examples, which is\nnon-trivial for GAN-based methods [Li et al. 2022].\nIn addition to diverse motion generation, we also demonstrate the\nversatility of our generative framework by extending to an array of\nscenarios that are unachievable with motion matching alone, such as\nmotion completion, key frame-guided generation, infinite looping,\nand motion reassembly, all enabled by the shared foundation of\ngenerative motion matching.\n2\nRELATED WORK\nWe review the most related work on kinematics-based motion syn-\nthesis. We also briefly cover recent advancements in image synthesis,\nparticularly patch-based ones, from which we take inspiration.\nMotion Synthesis. Generating novel motions via diversifying ex-\nisting ones can date back to the work of Perlin and Goldberg [1996],\nwhere the Perlin noise [Perlin 1985] is added to motion clips for ob-\ntaining variants with local diversity. Pullen and Bregler [2002] show\nthat mocap data can be used to enhance a coarse key-framed motion,\nby matching low-frequency patches and blending high-frequency\ndetails. Li et al. [2002] construct a graph model by matching similar\npatches in the dataset, and create a stochastic model for generating\nrandom samples with local and structural variations. Due to the\nuse of a linear dynamics model, the model requires a large train-\ning dataset to achieve satisfactory results and faces a dilemma of\nquality and diversity. Contemporarily, motion graphs [Kovar et al.\n2002; Lee et al. 2002; Arikan and Forsyth 2002] use a similar discrete\ngraph model while keeping it deterministic, namely a state machine,\nand demonstrate characters that respond interactively to the user\ninput. However, the discrete space inherently limits their agility\nand responsiveness. Hence, efforts on summarizing large datasets\ninto statistical models have also been made since then [Pullen and\nBregler 2000; Brand and Hertzmann 2000; Bowden 2000; Grochow\net al. 2004; Chai and Hodgins 2007; Wang et al. 2007]. Instead of\nsorting the dataset into an organized but discrete structure or a\nstatistical model, motion nearest neighbors [Lee et al. 2010; Levine\net al. 2012] operate directly on contiguous motion fields to learn a\ncontrol policy that interpolates the nearest neighbors of the current\npose. Following that, [B\u00fcttner and Clavet 2015] introduce Motion\nMatching, which is a method searching a large database of ani-\nmations for the animation which best fits the given context. This\nmethod has quickly been adopted by many studios due to its sim-\nplicity, flexibility, controllability, and the quality of the motion it\nproduces [Harrower 2018; Buttner 2019]. Motion matching plays\nback the animation data stored in the database as-is, rendering it\nthe de facto state-of-the-art in the industry. Nevertheless, its goal\ndiffers significantly from ours, as we target a generative model that\nsynthesizes diverse motions from examples.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nExample-based Motion Synthesis via Generative Motion Matching\n\u2022\n1:3\n\u2026\nStage 1\nExample motion !!\nExample motion !\"\nGenerative \nmotion matching\nExample motion !#\nInput \"#!\nSynthesized ##\nSynthesized #!\nSynthesized #\"\nGenerative \nmotion matching\nGenerative \nmotion matching\nStage 2\nStage !\nUpsample\nUpsample\nFig. 2. Multi-stage motion synthesis. Starting from the coarsest stage, the generative motion matching at each stage \ud835\udc60 takes in an upsampled version of the\noutput from the preceding stage as the initial guess, refines it with motion patches in the example motion T\ud835\udc60, and outputs a finer motion sequence F\ud835\udc60. Note\nthe coarsest stage is purely generative, as the input is merely a Gaussian noise.\nRecent advancements in deep learning also greatly impact the\nmotion synthesis field. Early attempts [Holden et al. 2015, 2016] use\ndeep neural networks to learn from animation data. Deep neural\nnetworks can learn a strong prior from a large dataset [Rempe et al.\n2021; He et al. 2022], solving many ill-posed generative tasks includ-\ning motion prediction [Fragkiadaki et al. 2015; Pavllo et al. 2018],\nmotion in-betweening [Harvey et al. 2020; Duan et al. 2022; Qin et al.\n2022], motion reassembling [Jang et al. 2022; Lee et al. 2022], text-\nguided motion synthesis [Tevet et al. 2022a,b; Zhao et al. 2023], etc.\nHolden et al. [2015, 2016] apply modern deep learning techniques\nfor learning from animation data. Meanwhile, combining motion\nmatching with deep learning has also resulted in variants that are\ncomputationally less expensive [Holden et al. 2020] and more versa-\ntile [Habibie et al. 2022]. Notably, all these works require a large and\ncomprehensive dataset for training. Another noticeable line of work\nadapts deep reinforcement learning techniques to train a physically\nsimulated character with a small set of example motions [Peng et al.\n2018, 2021]. More recently, [Li et al. 2022], the most related work to\nus, proposes to use a patch GAN-based [Isola et al. 2017; Shaham\net al. 2019] approach to train a generative model with a single ex-\nample. Concurrently, Raab et al. [2023b] introduce a diffusion-based\nmodel that learns the internal motifs of a single motion clip for pro-\nducing diverse outputs. Nonetheless, these two methods struggle to\nproduce results with sharp motions, and are not suitable for training\nwith multiple examples due to the discontinuous underlining latent\nspace presented to it. We show the superiority of our method over\nGANimator by an in-depth comparison in Section 4.1.\nImage Synthesis. Our work adopts several algorithmic designs\nfrom texture image synthesis, that shares a similar goal with motion\nsynthesis. For an in-depth survey of this extensive body we refer\nreaders to surveys [Wei et al. 2009; Barnes and Zhang 2017]. The\nimage pyramid, also known as progressive generation [Karras et al.\n2018] in deep learning, had been used in texture synthesis long\nago. Heeger and Bergen [1995]; De Bonet [1997] use a Laplacian\npyramid [Burt and Adelson 1987] for texture synthesis, realizing\nprogressive generation on the spatial frequency domain. Wei and\nLevoy [2000] use a Gaussian pyramid for a similar purpose. Han\net al. [2008] push the multi-scale generation to a new height, where\ngigapixel-sized images with great details can be synthesized. We also\nadopt the progressive synthesis, allowing our generative motion\nmatching module to capture details of different levels. Progressive\nsynthesis has also become popular in today\u2019s era of deep learning,\nleading to impressive generative models that learn to progressively\nrefine random noise into images resembling a single natural image.\nSpecifically, a series of GANs [Goodfellow et al. 2014] are trained\nto capture the patch distribution of the example at varying scales.\nFollowing that, Granot et al. [2022] show that bidirectional visual\nsimilarity [Simakov et al. 2008] can serve the purpose of measuring\nthe patch distribution discrepancy between the example and the\nsynthesized image, leading to diverse images of much higher quality\nand fast synthesis, compared to GAN-based methods.\n3\nMETHOD\nWe elaborate details of our generative framework, that can syn-\nthesize high-quality motions resembling given examples, in large\nquantities and varieties. Although our method can take as input\nmultiple exemplar motions, in our coverage, to ease the understand-\ning of the algorithm, we mainly describe in the single input setting.\nMoreover, the synthesized motion does not need to match exactly\nthe length of the example and can be of arbitrary length.\n3.1\nMotion Representation\nA motion sequence is defined by a temporal set of \ud835\udc47 poses that each\nconsists of root joint displacements O \u2208 R\ud835\udc47 \u00d73 and joint rotations\nR \u2208 R\ud835\udc47 \u00d7\ud835\udc3d \ud835\udc44, where \ud835\udc3d is the number of joints and \ud835\udc44 is the num-\nber of rotation features. Instead of directly using the global root\ndisplacements O, we convert O to local root displacements V, that\nare temporal-invariant and calculated as the difference between\nevery two consecutive poses. The joint rotations are defined in the\ncoordinate frame of their parent in the kinematic chain, and we use\nthe 6D rotation representation (i.e., \ud835\udc44 = 6) proposed by Zhou et al.\n[2019].\nAs human eyes are rather sensitive to implausible interactions\nbetween the end-effector and the ground, existing neural-based\nmethods usually establish geometric losses on the locations and\nvelocities of the end-effectors, i.e., the foot contact loss [Shi et al.\n2020; Li et al. 2022; Tevet et al. 2022b], whereas our method does not\ndemand such a design as the internal structure of exemplar motion\npatches, such as the high correlation between the end-effectors and\nthe root motion, are inherently preserved in the synthesis. That\nsaid, it is also trivial to incorporate foot contact labels as in [Li et al.\n2022] into our representation for improvements in rare cases, where\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n1:4\n\u2022\nWeiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen\n!\nSkeleton !\nPartial-body motions \"#!\nFull-body motion #!\n\u2026\n\"\n!#$! % & % '!\nContact labels !\n(\n)\nLocal root displacements \"\n&\n(\n\"\n!#$\" % '\"\n(\n(\nFlatten\nFlatten\n: Motion patches\n#$!\" (with root joint)\n$\nJoint rotations %\n(\n#$!#\nFig. 3. Skeleton-aware motion patch extraction. The skeleton is partitioned\ninto several overlapping skeletal parts (two coloreds on the left), with which\nthe full-body motion can be split into a set of partial-body motions accord-\ningly. Then motion patches (yellow boxes) with a temporal size of \ud835\udc5d frames\ncan be extracted from each partial-body motion.\nsliding feet could occur in the synthesis with large root-motion\nexamples. The contact label enables the IK post-process to avoid\nfloating feet. Specifically, the contact labels L can be easily retrieved\nfrom the input motion by setting a threshold of the magnitude of\nthe velocity. Assume the number of foot joints is \ud835\udc36, for each foot\njoint at a timestamp, we calculate a binary vector and append it to\nthe per-frame feature (See Figure 3 and 4).\nFor convenience, we let M\ud835\udc3b \u2261 R\ud835\udc3b \u00d7(\ud835\udc3d \ud835\udc44+3+\ud835\udc36) denote the metric\nspace of concatenated motion features of \ud835\udc3b frames, T \u2261 [R, V] \u2208\nM\ud835\udc47 the original input motion features, T\ud835\udc56 \u2208 M\ud835\udc47\ud835\udc56 a corresponding\ndownsampled version of the input, F \u2208 M\ud835\udc39 the synthesized motion\nfeatures of \ud835\udc39 frames, and F\ud835\udc60 a corresponding downsampled version\nof F.\n3.2\nMulti-stage Motion Synthesis\nFigure 2 presents the overall pipeline of our approach, which con-\nsists of \ud835\udc46 stages to progressively synthesize a motion of \ud835\udc39 frames.\nSpecifically, given an input motion, we build an exemplar pyra-\nmid {T1, ..., T\ud835\udc46}, where T\ud835\udc46 = T is the original input sequence and\nT\ud835\udc60 \u2208 M\ud835\udc47\ud835\udc60 is T\ud835\udc60+1 downsampled by a factor \ud835\udc5f > 1. Then, a synthe-\nsis pyramid {F1, ..., F\ud835\udc46}, where F\ud835\udc46 \u2208 M\ud835\udc39 is the final synthesized\nsequence of \ud835\udc39 frames and F\ud835\udc60 \u2208 M\ud835\udc39\ud835\udc60 is an intermediate sequence\nof \ud835\udc39 \u00b7 \ud835\udc5f\ud835\udc60\u2212\ud835\udc46 frames, will be synthesized in a coarse-to-fine manner,\nstarting from the coarsest stage and up to the finest. At each stage \ud835\udc60,\nthe generative motion matching module (Section 3.3) takes in an up-\nsampled version of the output from the preceding stage as the initial\nguess, \u02dcF\ud835\udc60 = F\ud835\udc60\u22121 \u2191\ud835\udc5f, refines it with exemplar motion patches in T\ud835\udc60,\nand outputs a finer motion sequence F\ud835\udc60. Note that the synthesis at\nthe coarsest stage is purely generative, as the input is merely a noise\ndrawn from a Gaussian distribution, i.e., \u02dcF1 \u223c N (\ud835\udf07, \ud835\udf0e2) \u2208 M\ud835\udc391.\n3.3\nGenerative Motion Matching\nTypically, patch-based image synthesis consists of three steps, namely\nthe patch extraction, nearest neighbor matching, and blending, that\nSynthesized motion !!\n!!\n\"!\nExample motion #\"\nGenerative matching and blending\nExtract\npatches\nInitial guess $%\"\nAvg.\nvoting\nContact labels &\nC\n3\nLocal root displacements '\nF\nJ\nJoint rotations (\n%\nF\nF\nExtract\npatches\nNormalized\ndistance matrix\n)*!\n: Aggregate nearest patches\nFig. 4. Generative matching and blending. Each motion patch in the initial\nguess finds the best-matched motion patch in the example motion, according\nto the normalized distance matrix. Then we blend the overlapping matched\npatches to form a novel partial motion. Finally, we blend multiple resultant\npartial motions to get the final full-body motion (see right).\nwork in sequence to produce a converging result in multiple itera-\ntions. Our method follows a similar approach but with algorithmic\ndesigns specific to our task. Specifically, at each stage\ud835\udc60, the following\nsteps are invoked sequentially during \ud835\udc38 iterations.\nSkeleton-aware Motion Patch Extraction. A motion patch can be\ndefined trivially as a sub-sequence of \ud835\udc5d consecutive frames in the\nexample sequence, which is a common practice in motion synthe-\nsis [Li et al. 2022; B\u00fcttner and Clavet 2015]. While our approach\ncan simply work with this definition, we further propose to extract\nskeleton-aware motion patches from the motion sequence, which\ndecomposes the skeleton into multiple sub-sets, i.e., skeletal parts,\ninstead of treating it as a whole, and eventually leads to more diverse\nposes. Specifically, let B denote the skeletal tree used by the exam-\nple full-body motion T\ud835\udc60, a set of skeletal parts { \u00afB1, ..., \u00afB\ud835\udc35}, where\n\u00afB\ud835\udc4f \u2282 B is a sub-tree of the whole skeleton and has \ud835\udc3d\ud835\udc4f joints, can be\ndefined to divide the full-body motion into a set of partial-body mo-\ntions { \u00afT1\n\ud835\udc60, ..., \u00afT\ud835\udc35\n\ud835\udc60 }, from which we crop sub-sequences of \ud835\udc5d frames\nwith stride size 1 as our motion patches (See Figure 3).\nUsually, skeletons across different animations do not necessarily\nfollow specific rules and can be of extremely high variability, for\nexample, bipeds vs. hexapods, a pure biological skeleton vs. one\nwith more artistic joints, etc. Hence, our approach allows the user\nto manually divide the whole skeletal structure into sub-parts with\noverlapping joints, similar as in [Jang et al. 2022; Lee et al. 2022].\nGenerative Matching. Let \ud835\udc4b denote the set of motion patches\nextracted from \u02dcF\ud835\udc60, \ud835\udc4c the set of motion patches extracted from the\nexample motion T\ud835\udc60. We calculate pairwise patch distance matrices\nusing squared-\ud835\udc3f2 distance, which provides the foundation for the\nmeasurement of the similarity between each exemplar motion patch\nand each synthesized motion patch. Note that the patch distance\nmatrix is calculated per skeletal part:\n\ud835\udc37\ud835\udc4f\n\ud835\udc56,\ud835\udc57 = \u2225\ud835\udc4b\ud835\udc4f\n\ud835\udc56 \u2212 \ud835\udc4c\ud835\udc4f\n\ud835\udc57 \u22252\n2,\n(1)\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nExample-based Motion Synthesis via Generative Motion Matching\n\u2022\n1:5\nSynthesized sequence 2\n433-joint skeleton\n143-joint skeleton\nSynthesized sequence 1\nSynthesized sequence 1\nSynthesized sequence 2\nFig. 5. From a single example, our framework generates, within a second, diverse motion sequences for even highly complex and large skeletons, including the\nanimation of the clothes and the wings. Please refer to the accompanying video for more animation results.\nwhere \ud835\udc4b\ud835\udc4f and \ud835\udc4c\ud835\udc4f denote the set of motion patches extracted from\ncorresponding partial-body motions. Then, the bidirectional simi-\nlarity as in [Simakov et al. 2008; Granot et al. 2022] is introduced to\nencourage that all exemplar motion patches appear in the synthesis\nand all motion patches in the synthesis do not deviate from the\nexample, i.e., high completeness and coherence. This is achieved\nby normalizing the distance matrices using a per-example-patch\nfactor:\n\u02c6\ud835\udc37\ud835\udc4f\n\ud835\udc56,\ud835\udc57 =\n\ud835\udc37\ud835\udc4f\n\ud835\udc56,\ud835\udc57\n(\ud835\udefc + min\u2113 (\ud835\udc37\ud835\udc4f\n\u2113,\ud835\udc57))\n,\n(2)\nwhere \ud835\udefc controls the degree of completeness, and smaller \ud835\udefc encour-\nages completeness. An in-depth study on the effect of \ud835\udefc is conducted\nin Section 4.3.\nBlending. For each motion patch in \ud835\udc4b\ud835\udc4f, we find its nearest (as de-\nfined by Equation 2) motion patch in \ud835\udc4c\ud835\udc4f, and then blend the values\nof collected motion patches using average voting, forming a synthe-\nsized partial-body motion \u00afF\ud835\udc4f. Finally, we average the values over\noverlapping joints between skeletal parts to assemble all synthesized\npartial-body motions into the final result F\ud835\udc60 (See Figure 4).\n3.4\nExtension to More Settings\nOur method can also be easily extended for various settings.\nSkeleton Partition for Motion Patches. In addition to the skeleton-\naware motion patch defined above, our method can also work with\nthe traditional definition of a motion patch, i.e., treating the skeleton\nas a whole and then extracting \ud835\udc5d consecutive poses.\nMultiple Examples. As aforementioned, our method can not only\nbe applied to a single motion input but also works with multiple\nsequences of different numbers of frames. This can be achieved by\nsimply extracting motion patches from all input motions to form\nthe set of exemplar patches used in Equation 1. In this setting, the\ncompleteness control knob \ud835\udefc plays a crucial role in ensuring that\nall motion patches across examples are utilized in the output.\nHeterogeneous Skeletons. Interestingly, under the setting of multi-\nple examples, the skeletons across different motions do not neces-\nsarily share the same one. For example, we can take motion clips of\na monster and a zombie, and synthesizes a moving Frankenstein, via\nharmonizing different partial-body motions extracted from these\ntwo creatures. To achieve this, the user can split the skeleton of\neach input with overlap and manually specify the skeletal parts of\ninterest to be used in the generative motion matching and blending.\nThe overlapping region plays a crucial role in bridging different\nskeletal parts during the generation process. Then, our method can\nsynthesize the novel motion for the new creature by combining the\npartial-body motions as discussed in Section 3.3.\n4\nEXPERIMENTS\nWe evaluate the effectiveness of our method on example-based mo-\ntion synthesis, compare to other motion generation techniques, and\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n1:6\n\u2022\nWeiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen\nExample sequence\nGANimator [2022]\nMotionTexture [2002]\nacRNN [2018]\nOurs\nFig. 6. Visual comparisons. MotionTexture [2002] generates motions with unnatural transitions. acRNN [2018] produces noisy motions or sometimes quickly\nconverges to a static pose. GANimator [2022] struggles to handle complex skeletons, producing over-smoothing results. Our method outperforms these\nmethods with diverse and high-quality results, where highly dynamic motions are well preserved. Please refer to the accompanying video for animation results.\ndemonstrate its versatility by applying it in various settings and ap-\nplications. We highly recommend readers refer to the accompanying\nvideo for more qualitative evaluations. Code and data will be released\nto ease the understanding of our implementation and facilitate future\nstudies.\nData. We collected a diverse set of example animations featuring\nvaried motion styles and highly complex and large skeletal struc-\ntures from Mixamo [2022] and Truebones [2022]. The motion styles\nwe experimented with include sharp motions of a popping dance,\nsubtle motions of fanning wings, etc. Some examples are authored\nwith highly sophisticated skeletal structures, such as the 433-joint\nand 143-joint skeletons as visualized in Figure 5. The number of\nframes ranges from 140 to 1000 frames at 30 fps.\nImplementation Details. Our framework is lightweight and does\nnot require any training. Due to its simplicity and efficiency, we\nsimply implement our method with Python. We also develop an add-\non in the open source software Blender [Blender Online Community\n2023], which is ready to take animations from users and synthesizes\ndiverse and high-quality variants. In our implementation, a motion\nsample with around 1000 frames can be generated in \u223c 0.2s with\nan Apple M1 CPU or \u223c 0.05s with a modern GPU (NVIDIA V100).\nBy default, we run experiments using an Apple M1 CPU, except\nthat the comparison experiments are conducted using an NVIDIA\nV100 GPU for fair comparisons with neural network-based methods.\nWe set the length of T1 at the coarsest stage to \ud835\udc3e times the patch\nsize \ud835\udc5d. Thus the receptive field (a similar concept as in the image)\nalways occupies the same proportion of example motions with\ndifferent lengths. Then, T1 is gradually upsampled using the factor\n\ud835\udc5f until it reaches the final length of T\ud835\udc46. Unless otherwise specified,\nwe use a patch size \ud835\udc5d = 11, \ud835\udc3e = 4, a completeness control knob\n\ud835\udefc = 0.01 and a number of iterations \ud835\udc38 = 5. These are the empirically\nbest hyper-parameters we have found. For more discussions on the\nhyper-parameters, please refer to the supplementary material.\n4.1\nNovel Motion Synthesis\nWe first evaluate the performance of our framework on novel mo-\ntion synthesis, and compare it to a classical statistical model and\nrecent neural-based models, namely MotionTexture [Li et al. 2002],\nacRNN [Zhou et al. 2018] and GANimator [Li et al. 2022].\nSettings. Although our method can use multiple inputs, for fair\ncomparisons, we conduct the evaluation on divers motion synthesis\nfrom a single example and disable the skeleton-aware motion patch\nextraction. We use three example sequences containing highly dy-\nnamic and agile movements for evaluation. The character consists\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nExample-based Motion Synthesis via Generative Motion Matching\n\u2022\n1:7\nExample sequence\nOurs (with skeleton-aware motion patch extraction)\nOurs (without skeleton-aware motion patch extraction)\nSkeleton parts\nFig. 7. The effectiveness of skeleton-aware motion patch extraction. An artist manually divides the skeleton into three overlapping parts (top left). Given an\nexample of a character waving two hands simultaneously, only a sequence with two waving hands can be synthesized without the skeleton-aware component.\nHowever, with the skeleton-aware motion patch extraction, a more diverse sequence, including waving with only one hand (in red boxes), can be generated.\nof 65 joints, and each sequence has around 500 frames. For each ex-\nample sequence, we use all methods to synthesize a novel sequence\nthat doubles the length of the example.\nQualitative Comparison. (i) To accomplish diverse motion synthe-\nsis, MotionTexture organizes similar motion patches from training\nmotions into linear dynamics models known as textons, and mod-\nels the probability of transition between textons using a Markov\nChain. However, it faces the challenge of balancing diversity and\nquality, particularly when there is only one example sequence due\nto the choice of linear dynamics model. We follow the procedure\nas done in [Li et al. 2022] to apply MotinTexture to a single ex-\nample. As a result, MotionTexture produces unnatural transitions\nbetween textons. (ii) acRNN uses an RNN-based network structure.\nThe lack of data leads to a model that is prone to overfitting and\nis not robust to perturbation or error accumulation. Consequently,\nacRNN can only stably generate a limited number of frames. (iii)\nGANimator utilizes a series of GANs to capture the distribution of\nmotion patches at different scales, in order to progressively synthe-\nsize motions that closely resemble the input. In our experiments\nwith complex and large skeletons, as shown in Table 3, GANimator\nstruggles to produce high-quality results, often resulting in jittery\nor over-smoothed motions. Additionally, it requires a significant\namount of training time, typically from several hours to a day. In\ncontrast, our method can adapt to these complex skeletal structures\nand various motion styles, and synthesize diverse and high-quality\nvariations as shown Figure 6. Notably, highly dynamic motions, in\nparticular sharp and agile movements, are well preserved in our\nsynthesized results. For more qualitative results, please refer to the\naccompanying video.\nTable 1. Quantitative comparisons on single example-based generation.\nCoverage Set\nDiv.\nGlobal\nPatch Dist.\nLocal\nPatch Dist.\nTraining\nTime\nInference\nTime\nMotionTexture [2002]\n84.12\n0.05\n1.12\n1.13\n32.3s\n0.03s\nMotionTexture (Single)\n100.00\n0.01\n0.45\n0.44\n0.08s\n0.07s\nacRNN [2018]\n5.13\n0.75\n13.62\n13.55\n25 hrs\n0.21s\nGANimator\n49.07\n0.24\n2.18\n2.08\n6 hrs\n0.12s\nOurs\n99.89\n0.28\n0.22\n0.18\nN/A\n0.08s\nQuantitative Comparison. Measuring the quality of generated re-\nsults against a few examples is known to be difficult [Li et al. 2022].\nAs one of the pioneers, GANimator uses a combination of estab-\nlished metrics, namely coverage, diversity, and reconstruction loss,\nto rate the performance, since a single metric does not suffice the\nneed of measuring the overall quality. However, the reconstruction\nloss is only suitable as a quality indicator for neural network-based\nmethods. The diversity is measured with the average distance be-\ntween generated motion patches and their nearest neighbor in the\nexamples, and different motion patch size corresponds to local and\nglobal diversity. As a result, they tend to increase if the generated\nmotion becomes unnatural, and favor results with minor perturba-\ntion or over-smoothed results generated by neural networks. We\nthus use neutral names for the diversity metrics, namely local patch\ndistance and global patch distance in our experiments. We refer read-\ners to [Li et al. 2022] for more details about these metrics. In addition,\nfollowing the well-established metric in 2D image synthesis [Sha-\nham et al. 2019], we also report the set diversity, which measures\nthe diversity among the generated results and is calculated as the\naveraged standard deviation of the rotation of each joint over 200\nsynthesized motions and normalized by the standard deviation of\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n1:8\n\u2022\nWeiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen\nOurs\nGANimator [2022]\nSynthesized motion\nCoverage of example 1\nCoverage of example 2\nCoverage of example 3\nSynthesized motion\nCoverage of example 1\nCoverage of example 2\nCoverage of example 3\nFig. 8. Comparison under the multi-example setting. GANimator struggles to incorporate all examples, resulting in the loss of a significant portion of exemplar\nmotion patches in the synthesis (marked in gray) and a low coverage score. In contrast, our method effectively covers all examples (marked as colorized),\nresulting in high coverage score.\nall joint rotations of the input example. Note that, while this metric\nalso has a preference for noisy output, we mainly rate the methods\nusing the combination of the coverage and set diversity.\nThe quantitative comparison results are shown in Table 1. No-\ntably, our method produces a significantly high coverage score,\nwhile still exhibiting sufficiently diverse results (evidenced by a\nhigh set diversity score). For a more comprehensive comparison of\nthe quality, we refer the readers to the accompanying video. Note,\nwe also report the computation time in Table 1, where we can see\nthat our method is highly efficient, as it is both training-free and\nextremely fast during inference.\n4.2\nMore Generation Settings\nIn addition to the basic setting used above, we further evaluate our\nmethod in the following aspects.\nSkeleton-aware Motion Patches. In addition to the temporal axis,\nour method can also extract motion patches from examples along\nthe skeletal axis, thus allowing obtaining diversity also on the spatial\ndimension as shown in Figure 7 and the accompanying video.\nMultiple Examples. Unlike existing methods, which struggle when\npresented with multiple example sequences due to the lack of ex-\nplicit encouragement of completeness, our method can handle multi-\nple examples with the completeness control knob in Equation 2. We\ncollect five dancing sequences ranging from 120-220 frames at 30 fps.\nIt can be seen in Table 2 that when more examples are given, existing\nmethods generate results with lower coverage while our method\nremains a high coverage. MotionTexture produces unnatural transi-\ntions, similar to its results in the single-example setting. acRNN fails\non the task and produces noisy motions due to the diverse but scarce\nTable 2. Coverage rates of different numbers of example sequences.\nNumber of Example Sequences\n2\n3\n4\n5\nMotionTexture [2002]\n100\n29.19\n10.03\n27.69\nacRNN [2018]\n7.34\n4.02\n1.38\n0.41\nGANimator [2022]\n63.55\n23.90\n17.30\n16.22\nOurs\n99.71\n99.95\n99.91\n99.64\nTable 3. Coverage rate on skeletons with different complexity.\nNumber of Joints\n24\n65\n433\nGANimator [2022]\n92.10\n44.10\n2.38\nOurs\n97.82\n99.84\n86.90\nmotion data. GANimator requires a corresponding pre-defined la-\ntent variable for each sequence. However, the structure of the given\nsequences is not taken into consideration for defining these latent\nvariables, hindering the network from generating various and com-\nplete samples. In contrast, our method produces motions that cover\na large portion of the examples with a properly set completeness\ncontrol knob as shown in Figure 8 and the accompanying video.\nComplex Skeletons. Our method can work with skeletons of high\ncomplexities (See Figure 5), on which the GAN-base method GAN-\nimator fails to produce reasonable results as demonstrated in the\naccompanying video. Specifically, we experiment with skeletons\nconsisting of 24, 65, and 433 joints. It can be seen in Table 3 that\nGANimator performs normally on the 24-joint skeletons, while\nits performance drops dramatically when presented with complex\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nExample-based Motion Synthesis via Generative Motion Matching\n\u2022\n1:9\nTable 4. The different settings of hyperparameters.\nCoverage Set Diversity Global Patch Dist. Local Patch Dist.\nOurs (not use \ud835\udefc)\n87.45\n0.27\n0.18\n0.17\nOurs (\ud835\udefc = 5)\n87.89\n0.27\n0.18\n0.17\nOurs (\ud835\udefc = 0.5)\n88.47\n0.27\n0.18\n0.17\nOurs (\ud835\udefc = 0.05)\n93.80\n0.27\n0.18\n0.17\nOurs (\ud835\udefc = 0.005)\n99.96\n0.27\n0.30\n0.23\nOurs (\ud835\udefc = 0.0)\n36.78\n0.25\n2.17\n1.60\nOurs (\ud835\udc3e = 20)\n87.74\n0.25\n1.03\n0.71\nOurs (\ud835\udc3e = 15)\n92.04\n0.26\n0.73\n0.54\nOurs (\ud835\udc3e = 10)\n96.07\n0.27\n0.42\n0.36\nOurs (\ud835\udc5d = 23)\n99.85\n0.26\n0.27\n0.24\nOurs (\ud835\udc5d = 17)\n99.88\n0.26\n0.23\n0.21\nOurs (\ud835\udc5d = 5)\n99.66\n0.27\n0.41\n0.32\nOurs (\ud835\udc5f = 2)\n99.61\n0.27\n0.22\n0.20\nOurs (\ud835\udc5f = 4)\n99.16\n0.27\n0.34\n0.28\nOurs (\ud835\udc5f = 8)\n97.97\n0.26\n0.57\n0.42\nskeletons. Whereas our method maintains a consistent performance\nfor different skeletons, evidenced by the fluttering effects of the\nskirt and dragon wings in the accompanying video.\n4.3\nEffects of Hyper-parameters\nOur framework involves several hyper-parameters during the syn-\nthesis process. In this section, we discuss the effects of these hyper-\nparameters. The quantitative results are presented in Table 4.\nEffects of \ud835\udefc. As rarely-appearing patches have a larger minimal\ndistance, the completeness of the synthesis is encouraged by nor-\nmalizing the distance of patches extracted from examples with their\nminimal distance to the initial guess. Therefore, the \ud835\udefc in Equation 2\nserves as a control knob for the completeness of exemplar patches in\nthe synthesized result. As it restricts the lower bound of the normal-\nizing denominator, a smaller \ud835\udefc value encourages more preservation\nof the example content in the synthesis. As shown in Table 4, when\n\ud835\udefc decreases to a certain level, a higher coverage score is achieved.\nHowever, if the value \ud835\udefc is too small, an excessive emphasis on\ncompleteness (especially for patches with almost zero distance to\nthe generated motion) can overwhelm the similarity measure used\nfor the matching process, resulting in unstable generation and low-\nquality motion (evidenced by low coverage and high patch distances\nof the corrupted results).\"\nEffects of \ud835\udc3e. The ratio of the patch size to the length of input\nexample motion at the coarsest stage controls the receptive field\nfor synthesis, similar to the concept in image domain. A larger \ud835\udc3e\ncauses a smaller receptive field, leading to more diverse results. In\nparticular, a large \ud835\udc3e allows only capturing fine-level movements\nand leads to some unnatural transitions, while a small \ud835\udc3e leads to\noverfitting of the original sequence. Table 4 shows the global and\nlocal patch distance increase as \ud835\udc3e increases. This is because the\ngenerated result deviates further from the input sequence when the\nreceptive field is smaller.\nEffects of Patch Size\ud835\udc5d. The patch size\ud835\udc5d defines the temporal length\nof patches used in the generative matching and blending. Patch size\ncontrols the receptive field jointly with \ud835\udc3e, and a smaller patch size\n0\n5\n10\n15\n20\n25\n0\n20000\n40000\n60000\n80000\n100000\nNumber of synthesized frames\nTime (s)\nMemory (GB)\nFig. 9. Time and memory consumption with respect to increasing numbers\nof generated frames.\nleads to a smaller receptive field, which creates less coherent result\nas shown by the increase of global and local patch distance in Table 4.\nEffects of \ud835\udc5f. The factor \ud835\udc5f controls the step size of transition be-\ntween stages. A large step size, controlled by a large \ud835\udc5f, may result in\nunstable generation due to big gaps between consecutive scales. On\nthe other hand, a small step size causes unnecessary running time.\n4.4\nTime and Memory Consumption\nThe memory footprint of the distance metrics described in Sec-\ntion 3.3 increases as the number of generated frames, \ud835\udc39, grows. To\nfurther investigate the time and memory consumption, we stress-\ntest our method under extreme conditions, where \ud835\udc38\ud835\udc41 comprises 522\nmotion frames of a 65-joint character and we set \ud835\udc39 ranging from\n1,000 to 100,000. These tests are conducted using an NVIDIA V100\nGPU equipped with 32GB memory. Figure 9 illustrates that both\ntime and memory consumption exhibit a linear growth pattern with\nrespect to the number of generated frames. Owing to the highly par-\nalleled computation of the distance metrics in the GPU, our method\ntakes only around 3 seconds to synthesize a high-quality sample\neven consisting of 100,000 frames.\n5\nAPPLICATIONS\nIn this section, we demonstrate the versatility of our framework by\nadapting it to various applications, such as motion completion, key\nframe-guided generation, infinite looping, and motion reassembly.\nThe results are presented in Figure 10. A more detailed demonstra-\ntion is available in the accompanying video.\nMotion Completion. Our framework, which utilizes skeleton-aware\nmotion patch extraction, enables the completion of partial motions\nthat contain only the movement of specific body parts. For ex-\nample, when a lower-body motion sequence \u00afTlower is provided,\nthe upper-body motion can be completed using the example mo-\ntion. Specifically, we build a pyramid for the partial-body motion\n{ \u00afTlower\n1\n, ..., \u00afTlower\n\ud835\udc46\n}, and the corresponding partial motion in the\noutput F\ud835\udc60 is fixed to \u00afTlower\n\ud835\udc60\nat each stage \ud835\udc60. Our framework then\nautomatically synthesizes the movements of the rest by parts, com-\npleting the partial constraints with a coherent and natural motion.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n1:10\n\u2022\nWeiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen\nInfinite looping\nMotion completion\nMotion reassembly\nKey frame-guided generation\nKey frame #1\nKey frame #2\nExample 1\nExample 2\nFixed\n\uff1a\nFig. 10. Applications. (1) Motion completion. Users can provide the lower-body motion (marked in yellow), and our method completes with diverse motions.\n(2) Key frame-guided generation. Given a set of key frames (marked in red boxes), we can generate diverse novel motion sequence that follow the key frame\nposes. (3) Infinite looping. By simply specifying the starting and ending pose to be identical, our method can generate a infinitely looping animation, which\ncan be useful in crowd simulation. (4) Motion reassembly. Given two motion sequences with heterogeneous skeletons, our method can combine them to form\na new creature with coherent and natural motion.\nKey Frame-guided Generation. Our method also allows users to\nmanually specify a sparse set of key frames to guide the content\nof the synthesized motion. Our method can then effectively handle\nthese sparse pose constraints distributed throughout the sequence\nand generate smooth, highly-detailed motion. Given a set of key\nframes at the coarsest stage, we simply realize it by replacing corre-\nsponding frames in F1 with the specified frames, and fixing them\nthrough the whole generation process. Note that these manually\nspecified key frames should not deviate significantly from the distri-\nbution of the poses in the example. In practice, they can be obtained\nby simply selecting existing poses in the example, possibly with\nslight manual modifications by the user.\nInfinite Looping. Our framework can easily synthesize endless\nlooping motion by fixing the ending pose to be identical to the\nbeginning pose at every stage in the synthesis. This allows for the\nseamless looping of the entire motion sequence. It can be useful\nin animation production, such as creating repetitive crowds like\nspectators cheering outside an arena.\nMotion Reassembly. As aforementioned in Section 3.4, our method\nhas the ability to synthesize a Frankenstein. We demonstrate an\nexample that stitches the right arm of a monster to a zombie; See Fig-\nure 10 and the accompanying video. Note the example sequence of\nthese two characters is different and the zombie does not have any\nFig. 11. Random locomotion generation. Given an example locomotion\nsequence of a character walking in a circular path (left), we show a high-\nquality novel motion sequence generated by our method, in which the\ncharacter walks along a different trajectory (right).\nmovement in its partially missing right arm, yet our method is still\nable to successfully synthesize a natural and meaningful motion.\nRandom Locomotion Generation. Our method can also generate\nhigh-quality novel motion sequences when given a locomotion clip.\nAs can be seen in Figure 11, while the example sequence contains a\nperson walking in a circular path, our method can generate novel\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nExample-based Motion Synthesis via Generative Motion Matching\n\u2022\n1:11\nFig. 12. Top: screenshot of our web-based interface, characters in grey\ndenote synthesized motions. Bottom: screenshot of our Blender add-on, the\nsynthesized motion is highlighted in the middle.\noutputs with a different trajectory (See the difference between the\ncorresponding trajectories at the top row). More animation results\ncan be found in the accompanying video.\n6\nUSER INTERFACE\nOur framework is general, lightweight, and easy to integrate into\nmany production tools. For novice users, we build a user-friendly\nwebsite where users can upload their motion files and then synthe-\nsize diverse novel motions with a single click (See the top in Fig-\nure 12). We also develop a Blender add-on for professional artists,\nwhich seamlessly integrates into their existing workflow as demon-\nstrated at the bottom of Figure 12. Note both interfaces can run\nefficiently on a consumer-level laptop. Please refer to the accompa-\nnying video for the results.\n7\nDISCUSSION AND CONCLUSION\nWe presented a generative framework for synthesizing diverse mo-\ntion sequences from only a small set of examples. We achieve this\nvia injecting generative capabilities into the industry state-of-the-\nart technique for character animation \u2013 motion matching. As a\nresult, our framework inherits the training-free nature and superior\nquality, and is able to produce a high-quality sample within just a\nfraction of a second, even with highly complex and large skeletons.\nWe demonstrate the utility of our framework on a variety of appli-\ncations. Despite its advantages, our method in its current form has\na few shortcomings: It uses a discrete patch distribution, whereas\nGANimator [Li et al. 2022] learns a continuous distribution. There-\nfore, GANimator can generate novel poses with high likelihood from\nthe learned distribution. Although the skeleton-aware component\ncan be a remedy, this capability is missing in our method. Neverthe-\nless, we argue that such generalization can be disadvantageous in\nmotion synthesis, as sequences formed by novel poses often contain\nvisual artifacts such as jittering and incoherence, which are highly\nnoticeable to human eyes. We prioritized the motion quality at the\noutset, which led us to the motion matching approach.\nOur method seeks to synthesize as many variants as can be mined\nfrom the examples, rather than struggle to balance quality with\nnovelty of motion. As a consequence, although diverse results of\nour method are shown, the generative diversity of our method\nis lower than that of GANimator. Hence, a future work direction\nis to inject the high quality of motion matching into generative\nneural models, possibly with discrete neural representation learning\ntechniques [Van Den Oord et al. 2017], and thus obtain the best of\nboth worlds.\nOur method favors example motions with sufficient intrinsic pe-\nriodicity, which has been increasingly recognized as an important\nproperty of common human motion [Holden et al. 2017; Starke\net al. 2022], to generate highly diverse novel variations. It seeks\nto exploit such patterns in a single example for mining as many\ncoherent variations as possible. In extreme cases where the exam-\nple only involves a single pose change, it may be meaningless to\ncreate temporal variations based solely on such input. Nonetheless,\nour skeleton-aware component may introduce variations along the\nskeletal axis, as evidenced by the asynchronized waving hands in\nthe supplementary video.\nRegarding the manual constraints required in the key frame-\nguided application, the manually specified key frames cannot differ\nsignificantly from those example poses as aforementioned, other-\nwise the generated sequence may not faithfully follow those con-\nstraining poses due to the lack of ability to generate completely\nnovel poses as discussed above.\nLast, our method cannot deal with overly long example sequences,\nas the normalized similarity matrices grow excessively large. Adopt-\ning approximate nearest neighbors search, such as [Barnes et al.\n2009], may help alleviate this issue.\nACKNOWLEDGMENTS\nWe thank the anonymous reviewers for their constructive comments.\nThis work was supported in part by National Key R&D Program of\nChina 2022ZD0160801, and the European Research Council (ERC)\nunder the European Union\u2019s Horizon 2020 Research and Innovation\nProgramme (ERC Consolidator Grant, agreement No. 101003104,\nMYCLOTH). We would also like to thank Han Liu from Tencent AI\nLab for providing the motion data of the avatar Ailing (Figure 1).\nREFERENCES\nAdobe Systems Inc. 2022. Mixamo. https://www.mixamo.com Accessed: 2022-03-25.\nOkan Arikan and David A Forsyth. 2002. Interactive motion generation from examples.\nACM Transactions on Graphics (TOG) 21, 3 (2002), 483\u2013490.\nConnelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. 2009. Patch-\nMatch: A randomized correspondence algorithm for structural image editing. ACM\nTransactions on Graphics (TOG) 28, 3 (2009), 24.\nConnelly Barnes and Fang-Lue Zhang. 2017. A survey of the state-of-the-art in patch-\nbased synthesis. Computational Visual Media 3, 1 (2017), 3\u201320.\nBlender Online Community. 2023. Blender - a 3D modelling and rendering package.\nBlender Foundation, Blender Institute, Amsterdam.\nRichard Bowden. 2000. Learning statistical models of human motion. In IEEE Workshop\non Human Modeling, Analysis and Synthesis, CVPR, Vol. 2000. Citeseer.\nMatthew Brand and Aaron Hertzmann. 2000. Style machines. In Proceedings of the 27th\nannual conference on Computer graphics and interactive techniques. 183\u2013192.\nPeter J Burt and Edward H Adelson. 1987. The Laplacian pyramid as a compact image\ncode. In Readings in computer vision. Elsevier, 671\u2013679.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n1:12\n\u2022\nWeiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen\nMichael Buttner. 2019. Machine learning for motion synthesis and character control in\ngames. Proc. of I3D 2019 (2019).\nMichael B\u00fcttner and Simon Clavet. 2015. Motion Matching - The Road to Next Gen\nAnimation. https://www.youtube.com/watch?v=z_wpgHFSWss&t=658s\nJinxiang Chai and Jessica K Hodgins. 2007. Constraint-based motion optimization using\na statistical dynamic model. In ACM SIGGRAPH 2007 papers. 8\u2013es.\nJeremy S De Bonet. 1997. Multiresolution sampling procedure for analysis and synthesis\nof texture images. In Proceedings of the 24th annual conference on Computer graphics\nand interactive techniques. 361\u2013368.\nYinglin Duan, Yue Lin, Zhengxia Zou, Yi Yuan, Zhehui Qian, and Bohan Zhang. 2022.\nA Unified Framework for Real Time Motion Completion. (2022).\nKaterina Fragkiadaki, Sergey Levine, Panna Felsen, and Jitendra Malik. 2015. Recur-\nrent network models for human dynamics. In Proceedings of the IEEE International\nConference on Computer Vision. 4346\u20134354.\nIan J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron C Courville, and Yoshua Bengio. 2014. Generative Adversarial\nNets. In NIPS.\nNiv Granot, Ben Feinstein, Assaf Shocher, Shai Bagon, and Michal Irani. 2022. Drop\nthe gan: In defense of patches nearest neighbors as single image generative models.\nIn Conference on Computer Vision and Pattern Recognition (CVPR). 13460\u201313469.\nKeith Grochow, Steven L Martin, Aaron Hertzmann, and Zoran Popovi\u0107. 2004. Style-\nbased inverse kinematics. In ACM SIGGRAPH 2004 Papers. 522\u2013531.\nIkhsanul Habibie, Mohamed Elgharib, Kripasindhu Sarkar, Ahsan Abdullah, Simbarashe\nNyatsanga, Michael Neff, and Christian Theobalt. 2022. A Motion Matching-based\nFramework for Controllable Gesture Synthesis from Speech. In ACM SIGGRAPH\n2022 Conference Proceedings. 1\u20139.\nCharles Han, Eric Risser, Ravi Ramamoorthi, and Eitan Grinspun. 2008. Multiscale\ntexture synthesis. In ACM SIGGRAPH 2008 papers. 1\u20138.\nGeof Harrower. 2018. Real player motion tech in\u2019ea sports ufc 3\u2019. Proc. of GDC 2018\n(2018).\nF\u00e9lix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. 2020. Robust\nmotion in-betweening. ACM Transactions on Graphics (TOG) 39, 4 (2020), 60\u20131.\nChengan He, Jun Saito, James Zachary, Holly Rushmeier, and Yi Zhou. 2022. NeMF:\nNeural Motion Fields for Kinematic Animation. In Advances in Neural Information\nProcessing Systems.\nDavid J Heeger and James R Bergen. 1995. Pyramid-based texture analysis/synthesis.\nIn Proceedings of the 22nd annual conference on Computer graphics and interactive\ntechniques. 229\u2013238.\nGustav Eje Henter, Simon Alexanderson, and Jonas Beskow. 2020. Moglow: Probabilistic\nand controllable motion synthesis using normalising flows. ACM Transactions on\nGraphics (TOG) 39, 6 (2020), 1\u201314.\nDaniel Holden, Oussama Kanoun, Maksym Perepichka, and Tiberiu Popa. 2020. Learned\nmotion matching. ACM Transactions on Graphics (TOG) 39, 4 (2020), 53\u20131.\nDaniel Holden, Taku Komura, and Jun Saito. 2017. Phase-functioned neural networks\nfor character control. ACM Transactions on Graphics (TOG) 36, 4 (2017), 1\u201313.\nDaniel Holden, Jun Saito, and Taku Komura. 2016. A deep learning framework for\ncharacter motion synthesis and editing. ACM Transactions on Graphics (TOG) 35, 4\n(2016), 1\u201311.\nDaniel Holden, Jun Saito, Taku Komura, and Thomas Joyce. 2015. Learning motion\nmanifolds with convolutional autoencoders. In SIGGRAPH Asia 2015 technical briefs.\n1\u20134.\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-Image\nTranslation with Conditional Adversarial Networks. CVPR (2017).\nDeok-Kyeong Jang, Soomin Park, and Sung-Hee Lee. 2022. Motion Puzzle: Arbitrary\nMotion Style Transfer by Body Part. ACM Transactions on Graphics (TOG) (2022).\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2018. Progressive Growing\nof GANs for Improved Quality, Stability, and Variation. In International Conference\non Learning Representations.\nLucas Kovar, Michael Gleicher, and Fr\u00e9d\u00e9ric Pighin. 2002. Motion Graphs. In Proceedings\nof the 29th Annual Conference on Computer Graphics and Interactive Techniques (San\nAntonio, Texas) (SIGGRAPH \u201902). Association for Computing Machinery, New York,\nNY, USA, 473\u2013482. https://doi.org/10.1145/566570.566605\nJehee Lee, Jinxiang Chai, Paul SA Reitsma, Jessica K Hodgins, and Nancy S Pollard. 2002.\nInteractive control of avatars animated with human motion data. In Proceedings of\nthe 29th annual conference on Computer graphics and interactive techniques. 491\u2013500.\nSeyoung Lee, Jiye Lee, and Jehee Lee. 2022. Learning Virtual Chimeras by Dynamic\nMotion Reassembly. ACM Trans. Graph. 41, 6, Article 182 (2022).\nYongjoon Lee, Kevin Wampler, Gilbert Bernstein, Jovan Popovi\u0107, and Zoran Popovi\u0107.\n2010. Motion fields for interactive character locomotion. In ACM Transactions on\nGraphics (TOG). 1\u20138.\nSergey Levine, Jack M Wang, Alexis Haraux, Zoran Popovi\u0107, and Vladlen Koltun. 2012.\nContinuous character control with low-dimensional embeddings. ACM Transactions\non Graphics (TOG) 31, 4 (2012), 1\u201310.\nPeizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, and Olga Sorkine-Hornung.\n2022. GANimator: Neural Motion Synthesis from a Single Sequence. ACM Transac-\ntions on Graphics (TOG) 41, 4 (2022), 138.\nYan Li, Tianshu Wang, and Heung-Yeung Shum. 2002. Motion texture: a two-level\nstatistical model for character motion synthesis. In Proceedings of the 29th annual\nconference on Computer graphics and interactive techniques. 465\u2013472.\nDario Pavllo, David Grangier, and Michael Auli. 2018. Quaternet: A quaternion-based\nrecurrent model for human motion. arXiv preprint arXiv:1805.06485 (2018).\nXue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. 2018. Deepmimic:\nExample-guided deep reinforcement learning of physics-based character skills. ACM\nTransactions On Graphics (TOG) 37, 4 (2018), 1\u201314.\nXue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. 2021.\nAmp: Adversarial motion priors for stylized physics-based character control. ACM\nTransactions on Graphics (TOG) 40, 4 (2021), 1\u201320.\nKen Perlin. 1985. An image synthesizer. ACM Siggraph Computer Graphics 19, 3 (1985),\n287\u2013296.\nKen Perlin and Athomas Goldberg. 1996. Improv: A system for scripting interactive\nactors in virtual worlds. In Proceedings of the 23rd annual conference on Computer\ngraphics and interactive techniques. 205\u2013216.\nKatherine Pullen and Christoph Bregler. 2000. Animating by multi-level sampling. In\nProceedings Computer Animation 2000. IEEE, 36\u201342.\nKatherine Pullen and Christoph Bregler. 2002. Motion capture assisted animation:\nTexturing and synthesis. In Proceedings of the 29th annual conference on Computer\ngraphics and interactive techniques. 501\u2013508.\nJia Qin, Youyi Zheng, and Kun Zhou. 2022. Motion In-betweening via Two-stage\nTransformers. ACM Transactions on Graphics (TOG) 41, 6 (2022), 1\u201316.\nSigal Raab, Inbal Leibovitch, Peizhuo Li, Kfir Aberman, Olga Sorkine-Hornung, and\nDaniel Cohen-Or. 2023a. MoDi: Unconditional Motion Synthesis from Diverse Data.\n(2023).\nSigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H Bermano, and Daniel\nCohen-Or. 2023b. Single Motion Diffusion. arXiv preprint arXiv:2302.05905 (2023).\nDavis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and\nLeonidas J Guibas. 2021. Humor: 3d human motion model for robust pose esti-\nmation. In Proceedings of the IEEE/CVF international conference on computer vision.\n11488\u201311499.\nTamar Rott Shaham, Tali Dekel, and Tomer Michaeli. 2019. Singan: Learning a genera-\ntive model from a single natural image. In Proceedings of the IEEE/CVF International\nConference on Computer Vision. 4570\u20134580.\nMingyi Shi, Kfir Aberman, Andreas Aristidou, Taku Komura, Dani Lischinski, Daniel\nCohen-Or, and Baoquan Chen. 2020. Motionet: 3d human motion reconstruction\nfrom monocular video with skeleton consistency. ACM Transactions on Graphics\n(TOG) 40, 1 (2020), 1\u201315.\nDenis Simakov, Yaron Caspi, Eli Shechtman, and Michal Irani. 2008. Summarizing\nvisual data using bidirectional similarity. In Conference on Computer Vision and\nPattern Recognition (CVPR). IEEE, 1\u20138.\nSebastian Starke, Ian Mason, and Taku Komura. 2022. Deepphase: Periodic autoencoders\nfor learning motion phase manifolds. ACM Transactions on Graphics (TOG) 41, 4\n(2022), 1\u201313.\nGuy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. 2022a.\nMotionCLIP: Exposing Human Motion Generation to CLIP Space. arXiv preprint\narXiv:2203.08063 (2022).\nGuy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H\nBermano. 2022b. Human motion diffusion model. arXiv preprint arXiv:2209.14916\n(2022).\nTruebones Motions Animation Studios. 2022. Truebones. https://truebones.gumroad.\ncom/ Accessed: 2022-9-2.\nJonathan Tseng, Rodrigo Castellon, and C Karen Liu. 2022. EDGE: Editable Dance\nGeneration From Music. arXiv preprint arXiv:2211.10658 (2022).\nAaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning.\nAdvances in neural information processing systems 30 (2017).\nJack M Wang, David J Fleet, and Aaron Hertzmann. 2007. Gaussian process dynamical\nmodels for human motion. IEEE transactions on pattern analysis and machine\nintelligence 30, 2 (2007), 283\u2013298.\nLi-Yi Wei, Sylvain Lefebvre, Vivek Kwatra, and Greg Turk. 2009. State of the art in\nexample-based texture synthesis. Eurographics 2009, State of the Art Report, EG-STAR\n(2009), 93\u2013117.\nLi-Yi Wei and Marc Levoy. 2000. Fast texture synthesis using tree-structured vector\nquantization. In Proceedings of the 27th annual conference on Computer graphics and\ninteractive techniques. 479\u2013488.\nMengyi Zhao, Mengyuan Liu, Bin Ren, Shuling Dai, and Nicu Sebe. 2023. Modiff:\nAction-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic\nModels. arXiv preprint arXiv:2301.03949 (2023).\nYi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. 2019. On the continuity\nof rotation representations in neural networks. In Conference on Computer Vision\nand Pattern Recognition (CVPR). 5745\u20135753.\nYi Zhou, Zimo Li, Shuangjiu Xiao, Chong He, Zeng Huang, and Hao Li. 2018. Auto-\nconditioned recurrent networks for extended complex human motion synthesis. In\nInternational Conference on Learning Representations.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n"
  },
  {
    "title": "Bytes Are All You Need: Transformers Operating Directly On File Bytes",
    "link": "https://arxiv.org/pdf/2306.00238.pdf",
    "upvote": "5",
    "text": "Bytes Are All You Need: Transformers Operating Directly On File Bytes\nMaxwell Horton, Sachin Mehta, Ali Farhadi, Mohammad Rastegari\nApple\nAbstract\nModern deep learning approaches usually transform\ninputs into a modality-specific form.\nFor example, the\nmost common deep learning approach to image classi-\nfication involves decoding image file bytes into an RGB\ntensor which is passed into a neural network.\nInstead,\nwe investigate performing classification directly on file\nbytes, without the need for decoding files at inference\ntime.\nUsing file bytes as model inputs enables the de-\nvelopment of models which can operate on multiple\ninput modalities.\nOur model, ByteFormer, achieves an\nImageNet Top-1 classification accuracy of 77.33% when\ntraining and testing directly on TIFF file bytes using\na transformer backbone with configuration similar to\nDeiT-Ti (72.2% accuracy when operating on RGB im-\nages).\nWithout modifications or hyperparameter tuning,\nByteFormer achieves 95.42% classification accuracy when\noperating on WAV files from the Speech Commands v2\ndataset (compared to state-of-the-art accuracy of 98.7%).\nAdditionally, we demonstrate that ByteFormer has appli-\ncations in privacy-preserving inference.\nByteFormer is\ncapable of performing inference on particular obfuscated\ninput representations with no loss of accuracy.\nWe also\ndemonstrate ByteFormer\u2019s ability to perform inference\nwith a hypothetical privacy-preserving camera which\navoids forming full images by consistently masking 90%\nof pixel channels, while still achieving 71.35% accu-\nracy on ImageNet.\nOur code will be made available\nat\nhttps://github.com/apple/ml-cvnets/\ntree/main/examples/byteformer.\n1. Introduction\nDeep learning inference usually involves explicit model-\ning of the input modality. For example, Vision Transform-\ners (ViTs) [7] explicitly model the 2D spatial structure of\nimages by encoding image patches into vectors. Similarly,\naudio inference often involves computing spectral features\n(such as MFCCs [25]) to pass into a network [10, 18]. When\na user wants to perform inference on a file stored on disk\n(e.g. a JPEG image file or an MP3 audio file), the user must\nfirst decode the file into a modality-specific representation\n(e.g. an RGB tensor or MFCCs), as in Figure 1a.\nThe practice of decoding inputs into a modality-specific\nrepresentation has two main drawbacks. First, it requires\nhand-crafting an input representation and a model stem for\neach input modality.\nRecent works such as PerceiverIO\n[14] and UnifiedIO [24] have shown that Transformer back-\nbones can be used for a variety of different tasks. However,\nthese methods still require modality-specific input prepro-\ncessing. For instance, PerceiverIO decodes image files into\n[H \u00d7 W, C] tensors before passing them into the network.\nOther modalities input to PerceiverIO are processed into\ndifferent forms. We hypothesize that it\u2019s possible to remove\nall modality-specific input preprocessing by performing in-\nference directly on file bytes.\nThe second drawback of decoding inputs into a\nmodality-specific representation is that it reduces privacy\nby exposing the data being analyzed. Consider the case of a\nsmart-home device that performs inference on RGB images.\nIf an adversary accesses this model input, the user\u2019s privacy\nmight be compromised. We hypothesize that inference can\ninstead be performed on privacy-preserving inputs.\nTo address these drawbacks, we note that a common\nproperty of many input modalities is that they can be stored\nas file bytes. Thus, we use file bytes (without any decoding)\nas inputs to our model at inference time (Figure 1b). We\nuse a modified Transformer [39] architecture for our model,\ngiven their ability to handle a variety of modalities [14, 24]\nand variable-length inputs. We call our model ByteFormer.\nWe demonstrate the efficacy of ByteFormer on Ima-\ngeNet [6] classification, achieving 77.33% accuracy on files\nstored in the TIFF format.\nOur model uses transformer\nbackbone hyperparameters chosen in DeiT-Ti [38] (which\nachieves 72.2% accuracy on RGB inputs). We also demon-\nstrate strong results on PNG and JPEG files. Additionally,\nwe demonstrate that our classification model can achieve\n95.8% accuracy on Speech Commands v2 [42], compara-\nble to state-of-the-art (98.7%) [18], without any architec-\nture changes or hyperparameter tuning.\nBecause ByteFormer can handle a variety of input rep-\nresentations, we can also use it to operate on privacy-\npreserving inputs. We demonstrate that we can remap in-\narXiv:2306.00238v1  [cs.CV]  31 May 2023\n0xFF\n0x01\n...\n0x8A\n...\nDeiT\nFile Decoding\nFile Bytes\nRGB Tensor\nTokenized Representation\nPatch Embedding\nA\n0xFF\n0x01\n...\n0x8A\n...\nBF\nToken Embedding\nFile Bytes\nTokenized Representation\nB\n0xFF 0x01\n...\n0x8A\n...\nBF\nObfuscation \u03d5\nFile Bytes\nTokenized Representation\nC\n0x1C 0x78\n...\n0xB2\nToken Embedding\n...\nBF\nTokenized Representation\nD\n0xAF 0x29\n...\n0x31\nToken Embedding\nPrivacy-Preserving Capture\nPrivacy-Preserving Camera\nObfuscated File Bytes\nPrivacy-Preserving Representation\nFigure 1. An overview of our ByteFormer (BF) compared to standard inference with DeiT [38]. (A): File bytes are read from disk and\nconverted to an RGB tensor using a standard image decoder. A patch embedding creates tokens from the RGB representation. (B): File\nbytes on disk are directly used as tokens, and projected into learned embeddings. (C): Similar to (B), but we apply an obfuscation function\n\u03d5. (D): We capture a privacy-preserving representation with a custom camera, and perform token embedding from this representation.\nput byte values using a permutation function \u03d5 : [0, 255] \u2192\n[0, 255] (Figure 1c) to obfuscate inputs without losing accu-\nracy. Although this does not guarantee cryptography-level\nsecurity, we demonstrate how this method can be used as a\nbuilding block for obfuscating inputs to a learning system.\nStronger privacy can be obtained by performing infer-\nence with ByteFormer on a partially-formed image (Fig-\nure 1d).\nWe demonstrate that ByteFormer is capable of\ntraining on images with 90% of the pixels masked while still\nachieving 71.35% accuracy on ImageNet [6]. ByteFormer\ndoes not require information about the specific location of\nunmasked pixels. Our representation passed to our model\nmaintains privacy by avoiding a standard image capture.\nIn summary, our contributions are: (1) we develop Byte-\nFormer, a model capable of performing inference on file\nbytes. (2) We show that ByteFormer achieves strong per-\nformance on a variety of image and audio file encod-\nings, without the need for architectural changes or hy-\nperparameter tuning.\n(3) We demonstrate application of\nByteFormer to privacy-preserving inputs.\n(4) We ana-\nlyze properties of ByteFormers trained to classify im-\nages and audio directly from file bytes.\n(5) We will\nrelease our code at https://github.com/apple/\nml-cvnets/tree/main/examples/byteformer.\n2. Related Work\nArchitectures With Multimodal Inputs: A few meth-\nods have explored the idea of feeding different input modal-\nities into the same network for processing. Perceiver IO\n[14] demonstrates that a Transformer [39] architecture with\ncross-attention input can be used for a variety of different\ntasks. Their method differs from ours because their inputs\nare processed with modality-specific preprocessing. For ex-\nample, images are loaded into a [H \u00d7 W, C] buffer, which\ndiffers from the model\u2019s treatment of text. By contrast, our\nmethod can classify images directly from file bytes. To our\nknowledge, we are the first to explore models that directly\nconsume file bytes without modality-specific processing.\nOther recent works that model multiple modalities with\na single model or a single embedding space (but still require\ninput-specific processing) include Unified IO [24], CLIP\n[33], and LQAE [21].\nAlternate Image Input Representations:\nPrevious\nworks have explored using alternate input representations\nfor images. In [11], the authors perform partial JPEG de-\ncoding, stopping when Discrete Cosine Transform [26] co-\nefficients are formed. They modify CNNs [12] to ingest\nthis new representation. In [31], a similar method is used\nwith Transformers. Our work differs in that we perform no\ndecoding of file bytes at inference time.\nPrivacy-Preserving Inference: We demonstrate appli-\ncations of our model to privacy-preserving inference. A few\nworks have examined secure inference in Multi-Party Com-\nmunication (MPC) settings [41, 19, 35, 37, 16]. The focus\nof these works is to perform inference securely on a remote\nmachine using decrypted data on a local machine. We differ\nfrom these methods in that our privacy-preserving systems\nadd a layer of privacy to the data on a local machine, and\ninference is performed locally. Thus, our approach comple-\nmentary to theirs.\nCompressive Sensing: Our privacy-preserving camera\nis inspired by works in compressive sensing [8]. Related\nworks use image input masking with a single-pixel camera\nto capture an image over multiple exposures with different\nmasks [9, 13]. Instead, we experiment with a single masked\ncapture on a multi-pixel camera.\n3. Overview of Common File Encodings\nWhen performing inference with a standard model, the\nchoice of file encoding is irrelevant. For example, it doesn\u2019t\nmatter whether an image is stored as a JPEG or PNG file if\nit will be decoded into an RGB tensor. By contrast, Byte-\nFormer performs inference on file bytes. In this case, the\nchoice of file encoding matters. This section provides an\noverview of common file encodings for images (subsec-\ntion 3.1) and audio (subsection 3.2). File encoding methods\ntypically contain a large number of optional settings that in-\nfluence the resulting file bytes. We use default settings pro-\nvided by PIL [4] or scipy [40] software packages unless\notherwise stated.\n3.1. Image File Encodings\nfHWC: We use fHWC as an abbreviation for \u201cflattened\ntensors in height, width, channel order.\u201d It refers to uint8\nimage bytes stored in HWC order without any file headers.\nIt is not common to store images in this way, since they\ncannot be decoded without pre-existing knowledge of their\nheight and width. This serves as a strong baseline.\nfCHW: This format is similar to fHWC, but images are\nstored in \u201cCHW\u201d order.\nTIFF: The TIFF file encoding [32] allows for many cus-\ntom configurations. For our experimentation, we use the de-\nfault settings provided by PIL. This results in a format sim-\nilar to fHWC, but with the addition of TIFF image headers\ndescribing configuration options and the image size.\nPNG: The PNG format [2] contains headers describing\nPNG configuration options, followed by rows of image data\nstored in \u201cIDAT\u201d chunks. Each IDAT chunk contains a byte\ndescribing the filtering method used for that row\u2019s data. The\nfiltering method applies an offset to the row\u2019s data based on\nneighboring pixel values. Thus, our PNG file contains rows\nof RGB data, with offsets applied, interrupted by occasional\nbytes that contain file encoding settings. We do not use the\noptional zlib compression that PNG allows.\nJPEG: JPEG [43] encodes images by applying a series\nof transformations to compress the image before serializa-\ntion. The RGB image is converted to YCbCr, then down-\nsampled in the chroma channels, then passed through a Dis-\ncrete Cosine Transform [26], then quantized using coeffi-\ncients determined by the JPEG quality factor. The quality\nfactor determines the level of compression, with 100 denot-\ning no compression due to quantization, and lower values\nindicating stronger compression. After quantization, the co-\nefficients are encoded via a run-length encoding, followed\nby a Huffman encoding [34]. Note that Huffman codes are\nnot byte-aligned, e.g. they can cross byte boundaries. We\nexpect this to make our modeling task more difficult.\n3.2. Audio File Encodings\nWAV: The WAV file encoding [17] stores audio signals\nrepresented as a sequence of amplitudes. We use single-\nchannel (mono) audio files. The most common configu-\nration options are the bit depth and the frequency.\nThe\nbit depth corresponds to the precision with which ampli-\ntude values are stored. We experiment with a variety of bit\ndepths, storing audio with 8-bit unsigned integers, 16-bit in-\ntegers, 32-bit integers, and 32-bit floating-point values. The\nfrequency corresponds to how often amplitude values are\nchosen. We use 16 kHz, a standard choice for audio [42].\nMP3: MP3 [30] uses a perceptual compression method\nthat removes portions of audio that are difficult for humans\nto detect. The remaining portions are recorded in frequency\nspace. An MP3 file contains a series of frames. Each frame\ncontains a header with encoding settings, followed by the\nencoded signal in frequency space. We use standard settings\nfor MP3 provided by the pydub [36] software package. We\nexpect MP3 encodings to be more difficult to handle than\nWAV files due to the compression applied.\n0xFF 0xAC 0x17\n0xFA\n0x28\n0x01\n0xB2\n0x87\nToken Embedding\nConv1D\nTransformer\nFigure 2. An overview of ByteFormer. We map byte values to\nlearned vectors using a learned token embedding. Next, we ap-\nply a Conv1D to reduce the token dimension. Finally, we apply a\ntransformer with shifted window attention and downsampling.\n4. Methods\nFirst, we discuss our method for performing inference\non file bytes (subsection 4.1). Then, we discuss how to use\nour method with image obfuscation techniques to enable\nprivacy-preserving inference (subsection 4.2). Finally, we\ndiscuss how to use our method with a privacy-preserving\ncamera to perform inference without constructing full im-\nages (subsection 4.3).\n4.1. Inference on File Bytes\n4.1.1\nPreprocessing\nSome of our file encodings such as TIFF are not frequently\nused in machine learning datasets. To allow for compar-\nisons on a single dataset across a variety of file encodings,\nwe must re-encode files with different file encodings.\nAt training time, we decode the file (e.g. read the con-\ntents into an RGB tensor in the case of images), then per-\nform standard training augmentation (e.g. random cropping\nin the case of images), then save the result in the desired file\nencoding. We find that standard training augmentation is\nimportant for model accuracy. Thus, our training method is\nimplicitly dependent on the input modality due to our aug-\nmentation.\nAt inference time, we do not need knowledge of the in-\nput modality. We only need to ensure that our model inputs\nuse the correct file encoding. For example, for TIFF ex-\nperiments on ImageNet, we precompute 224 \u00d7 224 crops\nModel\nData format\nE[S]\nE[Lt]\nTop-1\nDeiT-Ti\nRGB Tensor\n3 \u00d7 224 \u00d7 224\n196\n72.2\nDeiT-Ti\u22c6\nRGB Tensor\n3 \u00d7 224 \u00d7 224\n196\n74.35\nBF-Ti (Ours)\nfHWC\n150528\n9407\n77.06\nfCHW\n150528\n9407\n74.65\nTIFF\n150668\n9415\n77.33\nPNG\n150864\n9428\n74.94\nJPEG\n48564\n12140\n65.92\nTable 1. ImageNet Top-1 accuracy of ByteFormer Tiny (BF-Ti)\nusing various file encodings, compared to DeiT-Ti. E[S] denotes\nthe input shape, and E[Lt] denotes the token length passed to the\ntransformer backbone. (\u22c6) denotes our implementation of DeiT-Ti.\nWe set BF-Ti\u2019s Conv1D kernel size to k = 32 for all experiments\nexcept JPEG (k = 8).\nof the validation images and save them in the TIFF format.\nSuch preprocessing is only necessary because the ImageNet\nvalidation set is not already stored in the desired format.\n4.1.2\nByteFormer\nWe describe our ByteFormer model for inference on file\nbytes. An overview of our model is given in Figure 2. The\nmain challenge in using file bytes as inputs is the long se-\nquence lengths. In Table 1, we observe that input sizes E[S]\nfor various file encodings can exceed 150, 000 tokens. As\ndescribed below, we use strided Conv1D and shifted win-\ndow attention [22] to handle long sequence lengths.\nThe first step of our model is to use a learned token em-\nbedding with a vocabulary size of 256 (corresponding to 28\nunique byte values) to produce embeddings. This choice\nallows our model to handle a variety of input modalities.\nThe next step of our model is to perform a Conv1D\nto reduce the sequence length. Our intuition for choosing\nConv1D is that neighboring file bytes often contain related\ninformation. Reducing our sequence length with Conv1D\ngreatly improves memory usage. In Table 1, E[Lt] denotes\nthe input size to our Transformer, which is significantly\nsmaller than E[S]. Typically, we set our kernel size k = 32.\nOur stride is always k/2.\nNext, we add positional embeddings to the token em-\nbeddings, then pass our embeddings to a Transformer. We\nchoose Transformer size parameters to match the 12-layer\nDeiT-Ti [38] architecture with embedding dimension 192.\nWe call this particular version of our architecture Byte-\nFormer Tiny (BF-Ti). To compensate for our long sequence\nlength (9417 for TIFF, compared to 196 for DeiT-Ti), we\nuse shifted window attention [22] to limit the attention win-\ndow size w, alleviating the quadratic complexity of atten-\ntion layers on sequence length. We also add down-sampling\nlayers to halve the sequence length, as in [22]. We add them\nafter transformer blocks 0, 1, 3, 5, 7, and 9. After passing\nour tokens through the transformer, we average the embed-\ndings across the sequence dimension.\n4.2. Inference on Obfuscated Inputs\nByteFormer is designed to perform inference on file en-\ncodings without converting them into a standard input rep-\nresentation (e.g.\nan RGB tensor in the case of images).\nTherefore, we explore whether ByteFormer can be used for\ninference on privacy-preserving representations that obfus-\ncate information about the underlying data (Figure 1c).\nConsider a permutation \u03d5\n:\n{0, 1, 2, . . . , 255}\n\u2192\n{0, 1, 2, . . . , 255}. Let \u03c4 denote a token embedding, and let\nf\u03b8 denote the subsequent transformer. It\u2019s easy to see that,\nfor a given \u03d5, there exists a \u03c4\u03d5\u22121 such that \u03c4\u03d5\u22121(\u03d5(x)) =\n\u03c4(x).\n\u03c4\u03d5\u22121 is simply a copy of \u03c4 with embedding vec-\ntors reassigned to different indices. Thus, f(\u03c4\u03d5\u22121(\u03d5(x))) =\nf(\u03c4(x)). The implication of this statement is that our net-\nwork f\u03b8 can operate on re-encoded inputs \u03d5(x) without re-\nquiring any retraining as long as the network\u2019s token em-\nbedding \u03c4 is reordered to \u03c4\u03d5\u22121.\nTo take advantage of this property, we choose a permuta-\ntion \u03d5 at random before training. All training and inference\ninputs are remapped using \u03d5. We optionally apply uniform\nnoise before applying \u03d5. Without uniform noise, \u03d5 can be\napplied to a standard ByteFormer without retraining (as ex-\nplained above). However, we find uniform noise helpful in\nobfuscating regions of constant color in our experiments.\nMore generally, we can use more sophisticated methods\nfor altering input representations. As our method can handle\nhighly nonlinear JPEG encodings, we expect it to perform\nwell on a variety of alternative encodings that an outside\nobserver might not be able to easily guess. How secure are\nsuch methods against an adversary? This analysis depends\non the threat model used. For example, if an adversary has\naccess to a large number of encoded samples \u03d5(x), analy-\nsis of byte statistics might suggest that strings of common\nbits correspond to patches of blue sky in images. The ad-\nversary\u2019s task is made more difficult given certain file en-\ncodings (e.g. the highly nonlinear JPEG encoding). We\ndo not make strong claims regarding the level of security\nprovided by different choices of \u03d5. Secure systems should\nbe designed and analyzed by security researchers. Instead,\nwe simply suggest that decoupling the input representation\nfrom the model can lead to new possibilities for building\nmore secure systems.\n4.3. Privacy-Preserving Camera\nWe describe another application of ByteFormer to\nprivacy-preserving inference (Figure 1d). In this scenario, a\ncustom camera captures a non-standard, privacy-preserving\nrepresentation to allow for inference without building a full\nRGB image. This custom representation could take a vari-\nety of forms. In our experimentation, we consider a hypo-\nthetical camera that masks out a large fraction of its pixel\nchannels. The camera stores the remaining unmasked pixel\nchannels in an array without retaining the coordinates of\npixel channels on the image sensor. In this scenario, an ad-\nversary could not obtain a faithful reconstruction of the in-\nput image. Even if the adversary could guess pixel channel\nlocations, the low resolution of captured data prevents the\nadversary from recovering a high-fidelity image.\n5. Experiments\nWe evaluate ByteFormer on 1000-way classification on\nImageNet [6].\nWe also evaluate 12-way audio keyword\nclassification (including \u201cbackground\u201d and \u201cunknown\u201d\nclasses) of 1-second audio clips sampled at 16 khz using\nSpeech Commands v2 [42].\nFor all experiments, Byte-\nFormer\u2019s backbone uses hyperparameters that match DeiT-\nTi [38]. We refer to this architecture as BF-Ti.\nWe train using CVNets [27]. For ImageNet, we use batch\nsize 48 on 8 NVIDIA A100 GPU machines. At training\ntime, we use random resized cropping, random horizontal\nflipping, RandAugment [5], and RandomErase [45] before\nstoring the image in the desired file encoding (subsubsec-\ntion 4.1.1). We train with AdamW [23] with weight decay\n0.05, and a cosine annealing learning rate schedule from\n0.001 to 0.00002, with 7500 warmup iterations.\nWe train our Speech Commands v2 with MixUp [44],\nnoise augmentation, and time shifting augmentation, as in\n[29]. Our training and architecture hyperparameters match\nour ImageNet experiments.\nWe train these models on 4\nNVidia A100 GPU machines.\nFor ImageNet experiments, we report Top-1 accuracy of\nmodels trained with exponential moving average of weights\nwith momentum 0.0001, which typically increased accu-\nracy by roughly 0.25%. For Speech Commands V2, we\nfound EMA to sometimes increase and sometimes decrease\naccuracy, so we omit it.\n5.1. ImageNet File Encodings\nTable 1 summarizes results for a variety of file encodings\non the ImageNet dataset. For BF-Ti, we use w = 128 and\nk = 32 for all models except JPEG, for which we find k = 8\nto perform better. Our method surpasses DeiT-Ti accuracies\nfor TIFF, PNG, fCHW, and fHWC encodings.\nWe find training on JPEG to be more difficult. This is\nlikely due to the highly nonlinear and variable-length JPEG\nencoding. We investigate the influence of our model\u2019s ker-\nnel size k on JPEG accuracy in Table 2. We find that re-\nducing k from its default value of 32 increases accuracy.\nSince JPEG images have a smaller token length than TIFF\nor PNG, they are likely less compressible. To further ex-\nplore this, we investigate two settings for JPEG quality fac-\ntor in Table 2. We find that lower quality factors result in\nlower token lengths, thus reducing k improves accuracy. We\nalso try reducing w, but accuracy does not improve.\nq\nw\nk\nE[S]\nTop-1\n100\n128\n32\n48564\n60.86\n100\n128\n16\n48564\n64.86\n100\n128\n8\n48564\n65.92\n60\n128\n32\n8436\n31.8\n60\n128\n16\n8436\n50.11\n60\n128\n8\n8436\n56.26\n60\n128\n4\n8436\n62.52\n60\n32\n32\n8436\n37.23\n60\n32\n16\n8436\n50.24\n60\n32\n8\n8436\n56.74\n60\n32\n4\n8436\n59.52\nTable 2. ImageNet Top-1 accuracy for ByteFormer Tiny (BF-Ti)\nfor different JPEG quality factors q, window sizes w, and convo-\nlutional kernel sizes k. E[S] denotes the expected shape of the\ninputs during validation.\nModel\nInput\nw\nk\nE[S]\nTop-1\nBC-ResNet-8\nlog Mel\n-\n-\n40 \u00d7 98\n98.70\nBF-Ti (Ours)\nW-FP32\n128\n32\n64058\n95.80\n128\n16\n64058\n95.51\nBF-Ti (Ours)\nW-INT32\n128\n32\n64044\n94.90\n128\n16\n64044\n95.27\nBF-Ti (Ours)\nW-INT16\n128\n32\n32044\n94.81\n128\n16\n32044\n95.51\n128\n8\n32044\n95.13\nBF-Ti (Ours)\nW-UINT8\n128\n32\n16044\n92.28\n128\n16\n16044\n94.39\n128\n8\n16044\n94.81\n128\n4\n16044\n93.99\nBF-Ti (Ours)\nMP3\n128\n8\n3465\n88.39\n128\n4\n3465\n88.00\n32\n8\n3465\n88.69\n32\n4\n3465\n89.19\nTable 3. Results for audio classification with BF-Ti on the Speech\nCommands v2 dataset. \u201cW-\u201d denotes WAV files with the given bit\nwidth. E[S] denotes the shape of network inputs.\nWe present our method\u2019s computational efficiency com-\npared to related works in Appendix A.\n5.2. Speech Commands v2 File Encodings\nResults for audio classification on Speech Commands\nv2 [42] are given in Table 3. BF-Ti achieves accuracies\nof up to 95.51% on WAV files, comparable to the state-of-\nthe-art method BC-ResNet-8 [18]. Note that BC-ResNet-8\nis specifically designed for audio processing. By contrast,\nwe performed no parameter tuning relative to our ImageNet\ntraining recipe (besides ablating choices of w and k). Our\nbest-performing model has w = 128 and k = 32. Our\nNoise level\nModel\nDeiT-Ti\nBF-Ti\nNone\n51.61\n77.39\nU[\u22125, 5]\n50.77\n77.27\nU[\u221210, 10]\n49.50\n77.17\nU[\u221220, 20]\n43.84\n76.31\nTable 4. ImageNet Top-1 results for obfuscation with \u03d5. We show\nresults with no noise, and with uniform noise in [\u2212a, a] added. We\nuse the fHWC encoding.\noriginal\nNone\nU[-5, 5]\nU[-10, 10]\nU[-20, 20]\nobfuscated\nFigure 3. A sample image from the ImageNet validation set, with\nuniform noise applied (top row), and with byte remapping \u03d5 addi-\ntionally applied (bottom row).\nmodel performs best on floating-point values. In this case,\nsince each 32-bit floating-point value in the audio signal\nwill be encoded as 4 file bytes, each audio sample will be\nrepresented by 4 neighboring tokens before our Conv1D.\nWe investigate the influence of k on model accuracy. In\ngeneral, the optimal k decreases when the expected number\nof input tokens decreases. This matches our observations in\nImageNet JPEG experiments. For MP3 files, we observed\nthat k = 32 resulted in unstable models due to the drastic\nreduction in token length. For MP3, we additionally exper-\niment with w = 32, but it does not improve results.\n5.3. Image Obfuscation\nResults for our image obfuscation method (subsec-\ntion 4.2) on ImageNet are summarized in Table 4. After\nobtaining our fHWC encoding, we apply a randomly cho-\nsen obfuscation function \u03d5.\nExamples of obfuscated images are shown in Figure 3.\nWe observe that byte remapping retains shape information.\nA region of the image that is dominated by a single pixel\nvalue will continue to be dominated by a new (remapped)\npixel value. To alleviate this, we add noise from a uniform\ndistribution U[\u2212a, a] sampled from \u2212a to a (inclusive) to\neach pixel channel independently, then compute the result\nmodulo 256. Afterwards, we apply \u03d5. This prevents re-\ngions of constant pixel value from being remapped to a sin-\ngle value. As shown in Figure 3, the upper right corner of\nKept\n75%\n50%\n25%\n10%\n5%\n3%\nk\n32\n16\n8\n4\n4\n4\nTop-1\n74.77\n75.36\n74.04\n71.35\n68.11\n64.15\nTable 5.\nImageNet Top-1 accuracy for our privacy-preserving\ncamera experiment with BF-Ti when the given fraction of pixel\nchannels are kept.\n100%\n25%\n10%\n3%\nFigure 4.\nAn ImageNet validation image captured by our hy-\npothetical privacy-preserving camera in which the given fraction\nof pixel channels are kept. Note that positions of retained pixel\nchannels is discarded by the camera. To make visualization pos-\nsible, we include the positional information implicitly by placing\nunmasked pixels in the correct position.\nthe image becomes less recognizable as noise from progres-\nsively larger ranges is used. In Table 4, we observe that our\nmethod is resilient to this transformation, but DeiT is not.\n5.4. Privacy Preserving Camera\nTable 5 summarizes our results for our privacy-\npreserving camera (subsection 4.3). We emulate the cam-\nera setup by masking pixel channels of ImageNet images at\nrandom, then storing unmasked pixels in a buffer (in fHWC\norder) and passing that buffer into our network. For these\nexperiments, we cannot provide DeiT-Ti baselines because\nDeiT-Ti is not capable of ingesting pixel values without any\nindication of their placement in the image.\nIn Figure 4, we show masked inputs before the unmasked\npixels are rasterized. At 10% pixel retention, the content\nof image is hard to visually perceive even though active\npixels are placed side-by-side in a new buffer.\nEven if\nan adversary correctly guessed the positions of unmasked\npixel channels in the original image, the adversary could\nnot former a high-fidelity image.\nAs shown in Table 5,\nour accuracy at 10% pixel retention is 71.35%, compara-\nble to the original DeiT-Ti model operating on non-privacy-\npreserving (unmasked) images.\nNote that this privacy-preserving technique can be com-\nbined with the byte remapping technique (subsection 4.2) to\nfurther obfuscate network inputs.\n6. Analysis\nAlternate Attention Methods: We study three state-of-\nthe-art self-attention methods for handling long sequence\nlengths in Transformers: (1) full self-attention [39, 7, 20]\nwhere each token attends on every other token, (2) shifted\nAttention\nTop-1\nFull\nOOM\nWindow\n77.33\nBag\n75.20\nTable 6. ImageNet Top-1 accuracy of BF-Ti with different types\nof attention. We run out of memory with full attention.\n0\n1\n3\n2\n0\n1\n2\n3\nFull Attention\nB1\n0\n1\n0\n1\n0\n1\n3\n2\n0\n1\n2\n3\nWindow Attention\nB2\n2\n3\n2\n3\nB3\n4\n5\n4\n5\nB4\n6\n7\n6\n7\nBag Attention\nB1\nB2\nB3\nB3\nB1\nB2\nB3\nB3\nFigure 5. Illustration of the types of attention we experiment with.\nBag attention is computed in two stages. First, individual bags\ncompute attention. Then, attention is computed across bags.\nAugmentation\nTop-1\nRandom Shuffle\n3.06\nStride\n5.64\nWindow Shuffle\n18.14\nCyclic\n60.97\nReverse\n61.23\nBaseline\n60.81\nTable 7. Ablation showing the Top-1 ImageNet accuracy of BF-Ti\non JPEG images (k = 32, quality factor 100). See text for details.\nwindow attention [22, 1] where tokens are divided into local\nwindows and each local window computes self-attention in-\ndependently, and (3) bag (or hierarchical) attention [28, 3]\nwhere tokens are broken up into bags and each bag com-\nputes intra-bag self-attention.\nInter-bag self-attention is\nthen computed on the resultant output.\nThese different\nmethods are visualized in Figure 5 while results on TIFF\ndata are summarized in Table 6. We choose TIFF for these\nexperiments because of its long sequence length (Table 1).\nWe find window attention to outperform bag attention. Note\nthat full attention cannot be run due to its O(n2) depen-\ndence on sequence length n. In our main experiments, we\n0\n128\ntoken\nIN TIFF\nIN fCHW\nIN PNG\nIN JPEG\nSC FP32\nSC UINT8\n0\n128\n0\n128\npositional\n0\n128\n0\n128\n0\n128\n0\n128\n0\n128\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 6. |x \u00b7 y|/(||x|| \u00b7 ||y||) for pairs x, y of token embeddings (top row) and positional embeddings (bottom row) learned by BF-Ti. We\nshow results for various file encodings on ImageNet (IN) and Speech Commands (SC).\nused shifted window attention.\nEffect of Byte Ordering: To better understand Byte-\nFormer\u2019s behavior, we ask, does ByteFormer simply learn\nbyte frequencies, or is the byte ordering relevant? In Ta-\nble 7, we apply a series of augmentations during training\nand validation. We focus on the case of JPEG compression\nat quality factor 100 with our standard kernel size k = 32.\nEach augmentation modifies the byte order of the inputs\nin some way.\nIn random shuffle, we randomly re-\norder the bytes during training and validation. The order\nis redrawn every iteration. This severely degrades accuracy.\nNext, we perform a strided sampling with stride size 1024\n(e.g. [0, 1024, 2048, . . . , 1, 1025, 2049, . . .]). This slightly\nimproves accuracy over the previous method by improving\nbyte order consistency. Next, we experiment with window\nshuffle, in which the bytes from each window of size\n1024 are consistently permuted. This increases accuracy\nto 18.14%. Next we experiment with a cyclic shift in\nwhich the second half of the image bytes are moved to\nthe beginning. Accuracy matches the baseline (unaltered\nJPEG bytes) closely. Similarly, reverse, in which the\nbyte order is reversed, preserves locality well and matches\nthe baseline. We find that our model is sensitive to locality,\nand does not only learn byte frequencies.\nLearned Token Embeddings: We study the token em-\nbeddings learned by ByteFormer. These embeddings are\nused to project file bytes into vector representations. In Fig-\nure 6 (top row), we observe the absolute value of the cosine\ndistance |x \u00b7 y|/(||x|| \u00b7 ||y||) between each pair of token em-\nbeddings x, y on a variety of file encodings. We choose this\nmetric to highlight the difference between (anti-)correlated\nembeddings (bright patches) and uncorrelated embeddings\n(dark patches). The pattern varies substantially across in-\nput encodings and tasks. In TIFF, PNG, and fCHW, we\nobserve a bright band off of the diagonal, corresponding to\nhigh correlation between bytes and their neighbors. This\nmatches our expectations, since replacing a byte with its\nneighbor only slightly alters the image. This does not hold\nfor JPEG due to the Huffman encoding step. We also ob-\nserve that the correlation between token embeddings in the\nfloat32 encoding of Speech Commands is generally weak.\nWe believe this occurs because the float32 audio amplitude\nvalue is split across four bytes in the file encoding, weaken-\ning the association between byte values and amplitudes.\nLearned position embeddings\nWe visualize the absolute\nvalue of the cosine distance between the first 256 posi-\ntional embeddings learned by ByteFormer in Figure 6 (bot-\ntom row). For JPEG, we see a strong band of highly un-\ncorrelated values at early positions, corresponding to the\nfile header.\nLater positions demonstrate interesting pat-\nterns that may arise due to the Huffman encodings crossing\nbyte boundaries. In TIFF, a small band of highly uncorre-\nlated values is visible early on, corresponding to the header\n(which is shorter than in the JPEG case).\n7. Limitations\nThe accuracy of ByteFormer depends on the file en-\ncoding chosen.\nAs shown in section 5, choosing JPEG\nover TIFF results in a reduction of accuracy on ImageNet.\nAdding invariance to file encodings is future work.\nAs discussed in subsection 4.2, our choice of \u03d5 for our\nobfuscation method does not provide cryptography-level se-\ncurity against an attacker with access to a large set of model\ninputs. We view this method as a building block for security\nexperts to design thoroughly analyzed, secure systems.\nFinally, our method has only been evaluated on classi-\nfication for images and audio. Experimenting with other\ndomains (video, text) and tasks that require fine-grained lo-\ncalization (detection, segmentation) is exciting future work.\n8. Conclusion\nWe present ByteFormer, a model that consumes only\nbytes and does not explicitly model the input modality. We\nshow that it achieves strong performance on image and au-\ndio classification without hyperparameter tuning or archi-\ntecture modifications. We show how ByteFormer can be\nused in conjunction with image obfuscation techniques with\nlittle or no loss in accuracy. We also demonstrate how Byte-\nFormer can be incorporated into a privacy-preserving cam-\nera to enable inference without forming a full image at cap-\nture time.\nReferences\n[1] Iz Beltagy, Matthew E Peters, and Arman Cohan.\nLong-\nformer: The long-document transformer.\narXiv preprint\narXiv:2004.05150, 2020.\n[2] T. Boutell.\nPng (portable network graphics) spec-\nification.\nhttps://www.rfc-editor.org/rfc/\nrfc2083. Accessed: 2023-03-05.\n[3] Richard J Chen, Chengkuan Chen, Yicong Li, Tiffany Y\nChen, Andrew D Trister, Rahul G Krishnan, and Faisal\nMahmood. Scaling vision transformers to gigapixel images\nvia hierarchical self-supervised learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16144\u201316155, 2022.\n[4] Alex Clark. Pillow (pil fork) documentation, 2015.\n[5] Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and\nQuoc V. Le. Randaugment: Practical automated data aug-\nmentation with a reduced search space.\n2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\nWorkshops (CVPRW), pages 3008\u20133017, 2019.\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and\nLi Fei-Fei.\nImagenet: A large-scale hierarchical image\ndatabase. 2009 IEEE Conference on Computer Vision and\nPattern Recognition, pages 248\u2013255, 2009.\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. ArXiv, abs/2010.11929, 2020.\n[8] Simon Foucart and Holger Rauhut. A Mathematical Intro-\nduction to Compressive Sensing. Springer, 2013.\n[9] Graham M. Gibson, Steven D. Johnson, and Miles J. Padgett.\nSingle-pixel imaging 12 years on: a review. Opt. Express,\n28(19):28190\u201328208, Sep 2020.\n[10] Yuan Gong, Yu-An Chung, and James R. Glass. Ast: Audio\nspectrogram transformer. ArXiv, abs/2104.01778, 2021.\n[11] Lionel Gueguen, Alex Sergeev, Ben Kadlec, Rosanne Liu,\nand Jason Yosinski.\nFaster neural networks straight from\njpeg.\nIn S. Bengio, H. Wallach, H. Larochelle, K. Grau-\nman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in\nNeural Information Processing Systems, volume 31. Curran\nAssociates, Inc., 2018.\n[12] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. 2016 IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 770\u2013778, 2015.\n[13] Catherine Higham, Roderick Murray-Smith, Miles Padgett,\nand Matthew Edgar. Deep learning for real-time single-pixel\nvideo. Scientific Reports, 8, 02 2018.\n[14] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac,\nCarl Doersch, Catalin Ionescu, David Ding, Skanda Kop-\npula, Andrew Brock, Evan Shelhamer, Olivier J. H\u2019enaff,\nMatthew M. Botvinick, Andrew Zisserman, Oriol Vinyals,\nand Jo\u02dcao Carreira. Perceiver io: A general architecture for\nstructured inputs & outputs. ArXiv, abs/2107.14795, 2021.\n[15] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zis-\nserman, Oriol Vinyals, and Jo\u02dcao Carreira. Perceiver: General\nperception with iterative attention. In International Confer-\nence on Machine Learning, 2021.\n[16] Neha Jawalkar, Kanav Gupta, Arkaprava Basu, Nishanth\nChandran, Divya Gupta, and Rahul Sharma. Orca: Fss-based\nsecure training with gpus. Cryptology ePrint Archive, Paper\n2023/206, 2023. https://eprint.iacr.org/2023/\n206.\n[17] Peter\nKabal.\nWave\nfile\nspecifications.\nhttps:\n//www.mmsp.ece.mcgill.ca/Documents/\nAudioFormats/WAVE/WAVE.html.\nAccessed:\n2023-03-05.\n[18] Byeonggeun Kim, Simyung Chang, Jinkyu Lee, and Dooy-\nong Sung. Broadcasted residual learning for efficient key-\nword spotting. In Interspeech, 2021.\n[19] Nishant Kumar, Mayank Rathee, Nishanth Chandran, Divya\nGupta, Aseem Rastogi, and Rahul Sharma. Cryptflow: Se-\ncure tensorflow inference. 2020 IEEE Symposium on Secu-\nrity and Privacy (SP), pages 336\u2013353, 2019.\n[20] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object de-\ntection. In Computer Vision\u2013ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23\u201327, 2022, Proceed-\nings, Part IX, pages 280\u2013296. Springer, 2022.\n[21] Hao Liu, Wilson Yan, and Pieter Abbeel. Language quan-\ntized autoencoders: Towards unsupervised text-image align-\nment, 2023.\n[22] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. 2021\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 9992\u201310002, 2021.\n[23] Ilya Loshchilov and Frank Hutter.\nDecoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations, 2017.\n[24] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mot-\ntaghi, and Aniruddha Kembhavi.\nUnified-io: A unified\nmodel for vision, language, and multi-modal tasks. ArXiv,\nabs/2206.08916, 2022.\n[25] James\nLyons.\nMel\nfrequency\ncep-\nstral\ncoefficient\n(mfcc)\ntutorial.\nhttp://practicalcryptography.com/miscellaneous/machine-\nlearning/guide-mel-frequency-cepstral-coefficients-mfccs/.\nAccessed: 2023-03-06.\n[26] Dave\nMarshall.\nThe\ndiscrete\ncosine\ntransform.\nhttps://users.cs.cf.ac.uk/Dave.Marshall/Multimedia/node231.html.\nAccessed: 2023-03-06.\n[27] Sachin Mehta, Farzad Abdolhosseini, and Mohammad\nRastegari. Cvnets: High performance library for computer\nvision. Proceedings of the 30th ACM International Confer-\nence on Multimedia, 2022.\n[28] Sachin Mehta, Ximing Lu, Donald L. Weaver, Joann G. El-\nmore, Hannaneh Hajishirzi, and Linda G. Shapiro.\nHat-\nnet: An end-to-end holistic attention network for diagnosis\nof breast biopsy images. ArXiv, abs/2007.13007, 2020.\n[29] Dianwen Ng, Yunqi Chen, Biao Tian, Qiang Fu, and\nChng Eng Siong. Convmixer: Feature interactive convolu-\ntion with curriculum learning for small footprint and noisy\nfar-field keyword spotting. ICASSP 2022 - 2022 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pages 3603\u20133607, 2022.\n[30] M. Nilsson. The audio/mpeg media type. https://www.\nrfc-editor.org/rfc/rfc3003.html.\nAccessed:\n2023-03-05.\n[31] Jeongsoon Park and Justin Johnson.\nRgb no more:\nMinimally-decoded jpeg vision transformers.\nArXiv,\nabs/2211.16421, 2022.\n[32] G.\nParsons\nand\nJ.\nRafferty.\nTag\nimage\nfile\nfor-\nmat (tiff).\nhttps://www.rfc-editor.org/rfc/\nrfc3302. Accessed: 2023-03-05.\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021.\n[34] Vijay\nRaghunathan.\nEce264:\nHuffman\ncoding.\nhttps://engineering.purdue.edu/ece264/17au/hw/HW13?alt=\nhuffman. Accessed: 2023-03-07.\n[35] Deevashwer Rathee, Mayank Rathee, Nishant Kumar, Nis-\nhanth Chandran, Divya Gupta, Aseem Rastogi, and Rahul\nSharma. Cryptflow2: Practical 2-party secure inference. In\n27th Annual Conference on Computer and Communications\nSecurity (ACM CCS 2020). ACM, August 2020.\n[36] James Robert, Marc Webbie, et al. Pydub, 2018.\n[37] Akash Shah, Nishanth Chandran, Mesfin Dema, Divya\nGupta, Arun Gururajan, and Huan Yu. Secure featurization\nand applications to secure phishing detection. In Proceedings\nof the 2021 on Cloud Computing Security Workshop, CCSW\n\u201921, page 83\u201395, New York, NY, USA, 2021. Association for\nComputing Machinery.\n[38] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv\u2019e J\u2019egou. Training\ndata-efficient image transformers & distillation through at-\ntention. In International Conference on Machine Learning,\n2020.\n[39] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin.\nAttention is all you need.\nArXiv,\nabs/1706.03762, 2017.\n[40] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt\nHaberland,\nTyler\nReddy,\nDavid\nCournapeau,\nEvgeni\nBurovski, Pearu Peterson, Warren Weckesser, Jonathan\nBright, St\u00b4efan J. van der Walt, Matthew Brett, Joshua Wil-\nson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J.\nNelson, Eric Jones, Robert Kern, Eric Larson, C J Carey,\n\u02d9Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, De-\nnis Laxalde, Josef Perktold, Robert Cimrman, Ian Henrik-\nsen, E. A. Quintero, Charles R. Harris, Anne M. Archibald,\nAnt\u02c6onio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt,\nand SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algo-\nrithms for Scientific Computing in Python. Nature Methods,\n17:261\u2013272, 2020.\n[41] Sameer Wagh, Divya Gupta, and Nishanth Chandran. Se-\ncurenn:\n3-party secure computation for neural network\ntraining. Proceedings on Privacy Enhancing Technologies,\n2019:26 \u2013 49, 2019.\n[42] Pete Warden.\nSpeech commands: A dataset for limited-\nvocabulary speech recognition.\nArXiv, abs/1804.03209,\n2018.\n[43] Wikipedia.\nJpeg.\nhttps://en.wikipedia.org/\nwiki/JPEG. Accessed: 2023-03-05.\n[44] Hongyi Zhang, Moustapha Ciss\u00b4e, Yann Dauphin, and David\nLopez-Paz.\nmixup: Beyond empirical risk minimization.\nArXiv, abs/1710.09412, 2017.\n[45] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In AAAI Con-\nference on Artificial Intelligence, 2017.\nA. Performance\nOur goal is to directly model file bytes, with privacy-\npreserving applications. Since all previous methods involve\nsome level of domain-specific modeling at inference time\n(including file decoding and different stems for different in-\nput domains), direct comparison disadvantages our model.\nNevertheless, it\u2019s important to characterize the runtime of\nour model, and compare to previous approaches. This helps\nto contextualize our model\u2019s performance.\nWe compare BF-Ti to related works in Table 8. We show\nperformance on ImageNet [6], including efficiency and ac-\ncuracy metrics. We only report train time and throughput\nfor models we trained ourselves. This is to avoid hardware\ndifferences creating inconsistent results. For these experi-\nments, we tuned BF-Ti\u2019s batch sizes to maximize GPU uti-\nlization. Note that this improved training time by a rela-\ntively small amount (less than 10%) compared to the exper-\niments in section 5.\nOur model\u2019s size and accuracy (8.82 million parameters,\n77.27%) falls between DeiT-Ti (5.72 million parameters,\n78.62%) and DeiT-S (22.05 million parameters, 73.20%).\nOur model\u2019s forward pass time is slower due to the large\nnumber of tokens being modeled. Domain-specific model-\ning (which our model avoids) can drastically reduce com-\npute time.\nCompared to other multi-modal models [15, 14], our\nmodel achieves a comparable accuracy with far fewer flops\nand a far smaller model. Note that our model performs no\nModel\nM\nE[Lt]\nTop-1\nSec\nP (M)\nF (B)\nIm/s\nMobileNetv3 Large\n\u2718\nN/A\n75.1\n-\n5.43\n0.22\n9615\nResNet-50\n\u2718\nN/A\n78.12\n-\n25.55\n4.017\n3488\nDeiT-S p16\n\u2718\n196\n78.62\n336\n22.05\n4.61\n3594\nDeiT-Ti p=16\n\u2718\n196\n73.20\n334\n5.72\n1.26\n6885\nDeiT-Ti p=14\n\u2718\n256\n74.62\n331\n5.69\n1.70\n4970\nDeiT-Ti p=8\n\u2718\n784\n77.44\n824\n5.72\n7.06\n1243\nRGB No More DeiT-Ti [31]\n\u2718\n196\n75.1\n-\n5.72\n1.26\n6885\nPerceiver (learned pos) [15]\n\u2714\nN/A\n67.6\n-\n55.9\n62.3\n-\nPerceiver IO (learned pos) [14]\n\u2714\nN/A\n72.7\n-\n62.3\n407\n-\nPerceiver (conv) [15]\n\u2714\nN/A\n77.4\n-\n42.1\n367\n-\nPerceiver IO (conv) [14]\n\u2714\nN/A\n82.1\n-\n48.6\n369\n-\nBF-Ti k=32\n\u2714\n9415\n77.27\n1314\n8.82\n23.74\n373\nBF-Ti k=32 -C\n\u2714\n9415\n74.54\n1122\n7.64\n12.63\n370\nBF-Ti k=32 -C -NPE\n\u2714\n9415\n68.42\n1121\n5.83\n12.63\n372\nBF-Ti k=4 f0.05\n\u2714\n3762\n67.53\n368\n6.70\n5.70\n1687\nBF-Ti k=4 f=0.1\n\u2714\n7524\n71.26\n580\n7.42\n11.07\n875\nBF-Ti k=8 f=0.25\n\u2714\n9407\n73.65\n769\n7.93\n15.40\n634\nTable 8. ImageNet Top-1 accuracy. M: whether the model accepts\nvarious modalities (\u2718: No. \u2714: Yes, but with modality-specific\nmodeling. \u2714: Yes). E[Lt]: length of token inputs to transformer\n(after Conv1D for BF-Ti. Note, Perceiver feeds inputs through\ncross-attention, so this concept doesn\u2019t directly apply). Sec: Train\nepoch time (only reported for models we train to avoid hardware\ndifferences affecting results). P (M): Number of parameters (mil-\nlions).\nF (B): Number of flops (billions).\nIm/s: Throughput\n(images/sec) on an A100 80GB Nvidia GPU. \u201c-\u201d means \u201cnot re-\nported\u201d. For DeiT, p is patch size. Choosing p \u2264 4 is unfeasible\n(epochs take hours or days). For BF-Ti, k is conv kernel size,\nand f indicates fraction of retained pixels for privacy-preserving\ncamera experiments (subsection 5.4). \u201c-C\u201d indicates replacement\nof Conv1D with a windowed mean. \u201c-NPE\u201d indicates an ablation\nthat removes the positional embedding.\ndomain-specific modeling at inference time. By contrast,\nPerceiver includes domain-specific modeling (file decoding,\ntensor reshaping, and optionally convolutions) at inference\ntime.\n"
  },
  {
    "title": "StyleDrop: Text-to-Image Generation in Any Style",
    "link": "https://arxiv.org/pdf/2306.00983.pdf",
    "upvote": "5",
    "text": "StyleDrop: Text-to-Image Generation in Any Style\nKihyuk Sohn\nNataniel Ruiz\nKimin Lee\nDaniel Castro Chin\nIrina Blok\nHuiwen Chang\nJarred Barber\nLu Jiang\nGlenn Entis\nYuanzhen Li\nYuan Hao\nIrfan Essa\nMichael Rubinstein\nDilip Krishnan\nGoogle Research\nFigure 1: Visualization of StyleDrop outputs generated by personalized text-to-image models for 18 different\nstyles. Each model is tuned on a single style reference image, which is shown in the white insert box of each\nimage. The per-style text descriptor is appended to the content text prompt: \u201cA fluffy baby sloth with a knitted\nhat trying to figure out a laptop, close up\u201d. Generated images capture many nuances such as colors, shading,\ntextures and 3D appearance.\nAbstract\nPre-trained large text-to-image models synthesize impressive images with an appro-\npriate use of text prompts. However, ambiguities inherent in natural language and\nout-of-distribution effects make it hard to synthesize image styles, that leverage a\nspecific design pattern, texture or material. In this paper, we introduce StyleDrop, a\nmethod that enables the synthesis of images that faithfully follow a specific style us-\ning a text-to-image model. The proposed method is extremely versatile and captures\nnuances and details of a user-provided style, such as color schemes, shading, design\npatterns, and local and global effects. It efficiently learns a new style by fine-tuning\nvery few trainable parameters (less than 1% of total model parameters) and im-\nproving the quality via iterative training with either human or automated feedback.\nBetter yet, StyleDrop is able to deliver impressive results even when the user sup-\nplies only a single image that specifies the desired style. An extensive study shows\nthat, for the task of style tuning text-to-image models, StyleDrop implemented on\nMuse [4] convincingly outperforms other methods, including DreamBooth [30]\nand textual inversion [8] on Imagen [31] or Stable Diffusion [29]. More results are\navailable at our project website: https://styledrop.github.io.\nPreprint. Under review.\narXiv:2306.00983v1  [cs.CV]  1 Jun 2023\n1\nIntroduction\nText-to-image models trained on large image and text pairs have enabled the creation of rich and\ndiverse images encompassing many genres and themes [2, 4, 29, 31, 38]. The resulting creations have\nbecome a sensation, with Midjourney [2] reportedly being the largest Discord server in the world [1].\nThe styles of famous artists, such as Vincent Van Gogh, might be captured due to the presence of their\nwork in the training data. Moreover, popular styles such as \u201canime\u201d or \u201csteampunk\u201d, when added to\nthe input text prompt, may translate to specific visual outputs based on the training data. While many\nefforts have been put into \u201cprompt engineering\", a wide range of styles are simply hard to describe in\ntext form, due to the nuances of color schemes, illumination and other characteristics. As an example,\nVan Gogh has paintings in different styles (e.g., Fig. 1, top row, rightmost three columns). Thus, a\ntext prompt that simply says \u201cVan Gogh\u201d may either result in one specific style (selected at random),\nor in an unpredictable mix of several styles. Neither of these is a desirable outcome.\nIn this paper, we introduce StyleDrop1 which allows significantly higher level of stylized text-to-image\nsynthesis, using as few as one image as an example of a given style. Our experiments (Fig. 1) show\nthat StyleDrop achieves unprecedented accuracy and fidelity in stylized image synthesis. StyleDrop\nis built on a few crucial components: (1) a transformer-based text-to-image generation model [4];\n(2) adapter tuning [12]; and (3) iterative training with feedback. For the first component, we find\nthat Muse [4], a transformer modeling a discrete visual token sequence, shows an advantage over\ndiffusion models such as Imagen [31] and Stable Diffusion [29] for learning fine-grained styles\nfrom single images. For the second component, we employ adapter tuning [12] to style-tune a large\ntext-to-image transformer efficiently. Specifically, we construct a text input of a style reference image\nby composing content and style text descriptors to promote content-style disentanglement, which\nis crucial for compositional image synthesis [28, 36]. Finally, for the third component, we propose\nan iterative training framework, which trains a new adapter on images sampled from a previously\ntrained adapter. We find that, when trained on a small set of high-quality synthesized images, iterative\ntraining effectively alleviates overfitting, a prevalent issue for fine-tuning a text-to-image model on a\nvery few (e.g., one) images. We study high-quality sample selection methods using CLIP score (e.g.,\nimage-text alignment) and human feedback in Sec. 4.4.3, verifying the complementary benefit.\nIn addition to handling various styles, we extend our approach to customize not only style but also\ncontent (e.g., the identifying/distinctive features of a given object or subject), leveraging DreamBooth\n[30]. We propose a novel approach that samples an image of my content in my style from two adapters\ntrained for content and style independently. This compositional approach voids the need to jointly\noptimize on both content and style images [17, 10] and is therefore very flexible. We show in Fig. 5\nthat this approach produces compelling results that combines personalized generation respecting both\nobject identity and object style.\nWe test StyleDrop on Muse on a diverse set of style reference images, as shown in Fig. 1. We compare\nwith other recent methods including DreamBooth [30] and Textual Inversion [8], using Imagen [31]\nand Stable Diffusion [29] as pre-trained text-to-image backbones. An extensive evaluation based on\nprompt and style fidelity metrics using CLIP [25] and a user study shows the superiority of StyleDrop\nto other methods. Please visit our website and Appendix for more results.\n2\nRelated Work\nPersonalized Text-to-Image Synthesis has been studied to edit images of personal assets by leverag-\ning the power of pre-trained text-to-image models. Textual inversion [8] and Hard prompt made easy\n(PEZ) [34] find text representations (e.g., embedding, token) corresponding to a set of images of an\nobject without changing parameters of the text-to-image model.\nDreamBooth [30] fine-tunes an entire text-to-image model on a few images describing the subject\nof interest. As such, it is more expressive and captures the subject with greater details. Parameter-\nefficient fine-tuning (PEFT) methods, such as LoRA [13] or adapter tuning [12], are adopted to\nimprove its efficiency [3, 23]. Custom diffusion [17] and SVDiff [10] have extended DreamBooth to\nsynthesize multiple subjects simultaneously. Unlike these methods built on text-to-image diffusion\nmodels, we build StyleDrop on Muse [4], a generative vision transformer. [8, 34, 17] have shown\n1\u201cStyleDrop\u201d is inspired by eyedropper (a.k.a color picker), which allows users to quickly pick colors from\nvarious sources. Likewise, StyleDrop lets users quickly and painlessly \u2018pick\u2019 styles from a single (or very few)\nreference image(s), building a text-to-image model for generating images in that style.\n2\nlearning styles with text-to-image diffusion models, but from a handful or a dozen of style reference\nimages, and are limited to painting styles. We demonstrate on a wide variety of visual styles, including\n3d rendering, design illustration, and sculpture, using a single style reference image.\nNeural Style Transfer (NST). A large body of work [9, 15, 21, 6] has investigated style transfer\nusing deep networks by solving a composite objective of style and content consistency [9]. Recently,\n[14] has shown that quantizing the latent space leads to improved visual and style fidelity of NST\ncompared to continuous latent spaces. While both output stylized images, StyleDrop is different from\nNST in many ways; ours is based on text-to-image models to generate content, whereas NST uses an\nimage to guide content (e.g., spatial structure) for synthesis; we use adapters to capture fine-grained\nvisual style properties; we incorporate feedback signals to refine the style from a single input image.\nParameter Efficient Fine Tuning (PEFT) is a new paradigm for fine-tuning of deep learning models\nby only tuning a much smaller number of parameters, instead of the entire model. These parameters\nare either subsets of the original trained model, or small number of parameters that are added for the\nfine-tuning stage. PEFT has been introduced in the context of large language models [12, 20, 13],\nand then applied to text-to-image diffusion models [31, 29] with LoRA [3] or adapter tuning [23].\nFine-tuning of autoregressive (AR) [7, 38, 18] and non-autoregressive (NAR) [5, 4, 33] generative\nvision transformers has been studied recently [32], but without the text modality.\n3\nStyleDrop: Style Tuning for Text-to-Image Synthesis\nStyleDrop is built on Muse [4], reviewed in Sec. 3.1. There are two key parts. The parameter-efficient\nfine-tuning of a generative vision transformer (Sec. 3.2) and an iterative training with feedback\n(Sec. 3.3). Finally, we discuss how to synthesize images from two fine-tuned models in Sec. 3.4.\n3.1\nPreliminary: Muse [4], a masked Transformer for Text-to-Image Synthesis\nMuse [4] is a state-of-the-art text-to-image synthesis model based on the masked generative image\ntransformer, or MaskGIT [5]. It contains two synthesis modules for base image generation (256 \u00d7 256)\nand super-resolution (512 \u00d7 512 or 1024 \u00d7 1024). Each module is composed of a text encoder T,\na transformer G, a sampler S, an image encoder E, and decoder D. T maps a text prompt t \u2208 T\nto a continuous embedding space E. G processes a text embedding e \u2208 E to generate logits l \u2208 L\nfor the visual token sequence. S draws a sequence of visual tokens v \u2208 V from logits via iterative\ndecoding [5, 4], which runs a few steps of transformer inference conditioned on the text embeddings\ne and visual tokens decoded from previous steps. Finally, D maps the sequence of discrete tokens to\npixel space I.2 To summarize, given a text prompt t, an image I is synthesized as follows:\nI = D\n\u0000S (G, T(t))\n\u0001\n, lk = G (vk, T(t)) + \u03bb\n\u0000G (vk, T(t)) \u2212 G (vk, T(n))\n\u0001\n,\n(1)\nwhere n \u2208 T is a negative prompt, \u03bb is a guidance scale, k is the synthesis step, and lk\u2019s are logits,\nfrom which the next set of visual tokens vk+1\u2019s are sampled. We refer to [5, 4] for details on the\niterative decoding process. The T5-XXL [26] encoder for T and VQGAN [7, 37] for E and D are used.\nG is trained on a large amount of (image, text) pairs D using masked visual token modeling loss [5]:\nL = E(x,t)\u223cD,m\u223cM\nh\nCEm\n\u0010\nG\n\u0000M (E(x), m) , T(t)\n\u0001\n, E(x)\n\u0011i\n,\n(2)\nwhere M is a masking operator that applies masks to the tokens in vi. CEm is a weighted cross-entropy\ncalculated by summing only over the unmasked tokens.\n3.2\nParameter-Efficient Fine-Tuning of Text-to-Image Generative Vision Transformers\nNow we present a unified framework for parameter-efficient fine-tuning of generative vision transform-\ners. The proposed framework is not limited to a specific model and application, and is easily applied\nto the fine-tuning of text-to-image (e.g., Muse [4], Paella [27], Parti [38], RQ-Transformer [18]) and\ntext-to-video (e.g., Phenaki [33], CogVideo [11]) transformers, with a variety of PEFT methods,\nsuch as prompt tuning [20], LoRA [13], or adapter tuning [12], as in [32]. Nonetheless, we focus on\nMuse [4], an NAR text-to-image transformer, using adapter tuning [12].\nFollowing [32], we are interested in adapting a transformer G, while the rest (E, D, T) remain fixed.\nLet bG : V \u00d7 E \u00d7 \u0398 \u2192 L a modified version of a transformer G that takes learnable parameters \u03b8 \u2208 \u0398 as\n2We omit the description of the super-resolution module for concise presentation, and point readers to [4] for\na full description of the Muse model.\n3\nSelf-Attn\nCross-Attn\ne\nx L\nA\n\u03b8\nFigure 2: A simplified architecture of transformer layers of Muse [4]\nwith modification to support parameter-efficient fine-tuning (PEFT)\nwith adapter [12, 32]. L layers of transformers are used to process a\nsequence of visual tokens in green conditioned on the text embedding e.\nLearnable parameters \u03b8 are used to construct weights for adapter tuning.\nSee Appendix B.1.1 for details on adapter architecture.\nan additional input. Here, \u03b8 would represent parameters for learnable soft prompts of prompt tuning\nor weights of adapter tuning. Fig. 2 provides an intuitive description of bG with adapter tuning.\nFine-tuning of the transformer bG involves learning of newly introduced parameters \u03b8, while existing\nparameters of G (e.g., parameters of self-attention and cross-attention layers) remain fixed, with the\nlearning objective as follows:\n\u03b8 = arg min\n\u03b8\u2208\u0398\nL\u03b8 , L\u03b8 = E(x,t)\u223cDtr,m\u223cM\nh\nCEm\n\u0010\nbG\n\u0000M (E(x), m) , T(t), \u03b8\n\u0001\n, E(x)\n\u0011i\n,\n(3)\nwhere Dtr contains a few (image, text) pairs for fine-tuning. Unlike DreamBooth [30] where the same\ntext prompt is used to represent a set of training images, we use different text prompts for each input\nimage to better disentangle content and style. Once trained, similarly to the procedure in Eq. (2), we\nsynthesize images from the generation distribution of bG(\u00b7, \u00b7, \u03b8). Specifically, at each decoding step k,\nwe generate logits lk as follows:\nlk = bG (vk, T(t), \u03b8) + \u03bbA\n\u0000bG (vk, T(t), \u03b8) \u2212 G (vk, T(t))\n\u0001\n+ \u03bbB\n\u0000G (vk, T(t)) \u2212 G (vk, T(n))\n\u0001\n,\n(4)\nwhere \u03bbA controls the level of adaptation to the target distribution by contrasting the two generation\ndistributions, one that is fine-tuned bG (vk, T(t), \u03b8) and another that is not G (vk, T(t)), and \u03bbB controls\nthe textual alignment by contrasting the positive (t) and negative (n) text prompts.\n3.2.1\nConstructing Text Prompts\nimage\ntext prompt\n\u201cA cat in\nwatercolor painting style\u201d\nTable 1: An example text prompt at training. We construct a text\nprompt by composing descriptions of content (e.g., an object) and\nstyle (e.g., watercolor painting).\nTo train \u03b8, we require training data Dtr = {(Ii, ti)}N\ni=1 composed of (image, text) pairs for style\nreference. In many scenarios, we may be given only images as a style reference. In such cases, we\nneed to manually append text prompts.\nWe propose a simple, templated approach to construct text prompts, consisting of the description of a\ncontent (e.g., object, scene) followed by the phrase describing the style. For example, we use a \u201ccat\u201d\nto describe an object in Tab. 1 and append \u201cwatercolor painting\u201d as a style descriptor. Incorporating\ndescriptions of both content and style in the text prompt is critical, as it helps to disentangle the\ncontent from style and let learned parameters \u03b8 model the style, which is our primary goal. While we\nfind that using a rare token identifier [30] in place of a style descriptor (e.g., \u201cwatercolor painting\u201d)\nworks as well, having such a descriptive style descriptor provides an extra flexibility of style property\nediting, which will be shown in Sec. 4.4.2 and Fig. 7.\n3.3\nIterative Training with Feedback\nWhile our framework is generic and works well even on small training sets, the generation quality\nof the style-tuned model from a single image can sometimes be sub-optimal. The text construction\nmethod in Sec. 3.2.1 helps the quality, but we still find that overfitting to content is a concern. As in\nred boxes of Fig. 3 where the same house is rendered in the background, it is hard to perfectly avoid\nthe content leakage. However, we see that many of the rendered images successfully disentangle\nstyle from content, as shown in the blue boxes of Fig. 3.\nFor such a scenario, we leverage this finding of high precision when successful and introduce an\niterative training (IT) of StyleDrop using synthesized images by StyleDrop trained at an earlier stage\nto improve the recall (more disentanglement). We opt for a simple solution: construct a new training\nset with a few dozen successful (image, text) pairs (e.g., images in blue box of Fig. 3) while using the\n4\nFigure 3: Iterative Training with Feedback. When trained on a single style reference image (orange box), some\ngenerated images by StyleDrop may exhibit leaked content from the style reference image (red box, images\ncontain in the background a similar-looking house as in the style image), while other images (blue box) have\nbetter dismantlement of style from content. Iterative training of StyleDrop with the good samples (blue box)\nresults in an overall better balance between style and text fidelity (green box).\nsame objective in Eq. (3). IT results in an immediate improvement with a reduced content leakage, as\nin Fig. 3 green box. The key question is how to assess the quality of synthesized images.\nCLIP score [25] measures the image-text alignment. As such, it could be used to assess the quality\nof generated images by measuring the CLIP score (i.e., cosine similarity of visual and textual CLIP\nembeddings). We select images with the highest CLIP scores and we call this method an iterative\ntraining with CLIP feedback (CF). In our experiments, we find that the CLIP score to assess the\nquality of synthesized images is an efficient way of improving the recall (i.e., textual fidelity) without\nlosing too much style fidelity. On the other hand, CLIP score may not be perfectly aligned with the\nhuman intention [19, 35] and would not capture the subtle style property.\nHuman Feedback (HF) is a more direct way of injecting user intention into the quality evaluation of\nsynthetic images. HF is shown to be powerful and effective in LLM fine-tuning with reinforcement\nlearning [24]. In our case, HF could be used to compensate the CLIP score not being able to capture\nsubtle style properties. Empirically, selecting less than a dozen images is enough for IT, and it only\ntakes about 3 minutes per style. As shown in Sec. 4.4.4 and Fig. 9, HF is critical for some applications,\nsuch as illustration designs, where capturing subtle differences is important to correctly reflect the\ndesigner\u2019s intention. Nevertheless, due to human selection bias, style may drift or be reduced.\n3.4\nSampling from Two \u03b8\u2019s\nThere has been an extensive study on personalization of text-to-image diffusion models to synthesize\nimages containing multiple personal assets [17, 22, 10]. In this section, we show how to combine\nDreamBooth and StyleDrop in a simple manner, thereby enabling personalization of both style and\ncontent. This is done by sampling from two modified generation distributions, guided by \u03b8s for style\nand \u03b8c for content, each of which are adapter parameters trained independently on style and content\nreference images, respectively. Unlike existing works [17, 10], our approach does not require joint\ntraining of learnable parameters on multiple concepts, leading to a greater compositional power with\npre-trained adapters, which are separately trained on individual subject and style assets.\nOur overall sampling procedure follows the iterative decoding of Eq. (1), with differences in how we\nsample logits at each decoding step. Let t be the text prompt and c be the text prompt without the\nstyle descriptor.3 We compute logits at step k as follows: lk = (1 \u2212 \u03b3)ls\nk + \u03b3lc\nk, where\nls\nk = bG (vk, T(t), \u03b8s) + \u03bbA\n\u0000bG (vk, T(t), \u03b8s) \u2212 G (vk, T(t))\n\u0001\n+ \u03bbB\n\u0000G (vk, T(t)) \u2212 G (vk, T(n))\n\u0001\n(5)\nlc\nk = bG (vk, T(c), \u03b8c) + \u03bbA\n\u0000bG (vk, T(c), \u03b8c) \u2212 G (vk, T(c))\n\u0001\n+ \u03bbB\n\u0000G (vk, T(c)) \u2212 G (vk, T(n))\n\u0001\n(6)\nwhere \u03b3 balances the StyleDrop and DreamBooth \u2013 if \u03b3 is 0, we get StyleDrop, and DreamBooth if 1.\nBy properly setting \u03b3 (e.g., 0.5 \u223c 0.7), we get images of my content in my style (see Fig. 5).\n4\nExperiments\nWe report results of StyleDrop on a variety of styles and compare with existing methods in Sec. 4.2.\nIn Sec. 4.3 we show results on \u201cmy object in my style\u201d combining the capability of DreamBooth and\nStyleDrop. Finally, we conduct an ablation study on the design choices of StyleDrop in Sec. 4.4.\n4.1\nExperimental Setting\nTo the best of our knowledge, there has not been an extensive study of style-tuning for text-to-image\ngeneration models. As such, we suggest a new experimental protocol.\n3For example, t is \u201cA teapot in watercolor painting style\u201d and c is \u201cA teapot\u201d.\n5\nFigure 4: Qualitative comparison of style-tuned text-to-image synthesis on various styles, including \u201cmelting\ngolden 3d rendering\u201d, \u201c3d rendering\u201d, \u201cwooden sculpture\u201d, \u201cflat cartoon illustration\u201d, and \u201ccartoon line drawing\u201d,\nshown on the first column. Text prompts used for synthesis are \u201cthe Golden Gate bridge\u201d, \u201cthe letter \u2018G\u201d\u2019, and\n\u201ca man riding a snowboard\u201d. Image sources are in Tab. S1. We see that StyleDrop (HF) consistently captures\nnuances such as the \u201cmelting\u201d effect in the top row.\nData collection. We collect a few dozen images of various styles, from watercolor and oil painting,\nflat illustrations, 3d rendering to sculptures with varying materials. While painting styles have been a\nmajor focus for neural style transfer research [9, 6], we go beyond and include a more diverse set of\nvisual styles in our experiments. We provide image sources in Tab. S1 and attribute their ownership.\nModel configuration. As in Sec. 3.2, we base StyleDrop on Muse [4] using adapter tuning [12, 32].\nFor all experiments, we update adapter weights for 1000 steps using Adam optimizer [16] with a\nlearning rate of 0.00003. Unless otherwise stated, we use \u201cStyleDrop\u201d to denote the second round\nmodel trained on as many as 10 synthetic images with human feedback, as in Sec. 3.3. Nevertheless,\nto mitigate confusion, we append \u201cHF\u201d (human feedback), \u201cCF\u201d (CLIP feedback) or \u201cR1\u201d (first\nround model) to StyleDrop whenever there needs a clarity. More training details are in Appendix B.1.\nEvaluation. We report quantitative metrics based on CLIP [25] that measures the style consistency\nand textual alignment. In addition, we conduct the user preference study to assess style consistency\nand textual alignment. Appendix B.2 summarizes details on the human evaluation protocol.\n4.2\nStyleDrop Results\nFig. 1 shows results of our default approach on the 18 different style images that we collected, for the\nsame text prompt. We see that StyleDrop is able to capture nuances of texture, shading, and structure\nacross a wide range of styles, significantly better than previous approaches, enabling significantly\nmore control over style than previously possible. Fig. 4 shows synthesized images of StyleDrop using\n3 different style reference images. For comparison, we also present results of (b) DreamBooth [30]\non Imagen [31], (c) a LoRA implementation of DreamBooth [30, 3, 13] and (d) textual inversion [8],\nboth on Stable Diffusion [29].4,5 More results are available in Figs. S8 to S13.\n4Colab implementation of textual inversion [8] is used with stable-diffusion-2.\n5More baselines using hard prompt made easy (PEZ) [34] are in Appendix B.\n6\nTable 2: Evaluation metrics of (top) human evaluation and (bottom) CLIP scores [25] for image-text alignment\n(Text) and visual style alignment (Style). We test on 6 styles from Fig. 1. For human evaluation, preferences\nare reported. For CLIP scores, we report the mean and standard error. We report scores for Muse [4] and\nImagen [31] with styles guided by the text prompt. DB: DreamBooth, SDRP: StyleDrop, and iterative training\nwith human feedback (HF), CLIP feedback (CF), and random selection (Random).\nSDRP (R1)\ntie\nDB on Imagen\nSDRP (R1)\ntie\nSDRP (HF)\nSDRP (HF)\ntie\nSDRP (CF)\nText\n31.7%\n45.0%\n23.3%\n20.7%\n56.0%\n23.3%\n19.4%\n58.2%\n22.4%\nStyle\n86.0%\n4.3%\n9.7%\n62.3%\n7.4%\n30.3%\n60.9%\n8.4%\n30.8%\nMethod\nImagen\nDB on Imagen\nMuse\nStyleDrop on Muse\nRound 1\nHF\nCF\nRandom\nText (\u2191)\n0.337\u00b10.001\n0.335\u00b10.001\n0.323\u00b10.001\n0.313\u00b10.001\n0.322\u00b10.001\n0.329\u00b10.001\n0.316\u00b10.001\nStyle (\u2191)\n0.569\u00b10.002\n0.644\u00b10.002\n0.556\u00b10.001\n0.705\u00b10.002\n0.694\u00b10.001\n0.673\u00b10.001\n0.678\u00b10.001\nFor baselines, we follow instructions from the respective papers and open-source implementations,\nbut with a few modifications. For example, instead of using a rare token (e.g., \u201ca watermelon slice in\n[V*] style\u201d), we use the style descriptor (e.g., \u201ca watermelon slice in 3d rendering style\u201d), similarly\nto StyleDrop. We train DreamBooth on Imagen for 300 steps after performing grid-search. This is\nless than 1000 steps recommended in [30], but is chosen to alleviate overfitting to image content\nand to better capture style. For LoRA DreamBooth on Stable Diffusion, we train for 400 steps with\nlearning rates of 0.0002 for UNet and 0.000005 for CLIP. We do not adopt the iterative training for\nbaselines in Fig. 4. StyleDrop results without iterative training are in Sec. 4.4.3. It is clear from Fig. 4\nthat StyleDrop on Muse convincingly outperforms other methods that are geared towards solving\nsubject-driven personalization of text-to-image synthesis using diffusion models.\nWe see that style-tuning on Stable Diffusion with LoRA DreamBooth (Fig. 4(c)) and textual inversion\n(Fig. 4(d)) show poor style consistency to reference images. While DreamBooth on Imagen (Fig. 4(b))\nimproves over those on Stable Diffusion, it still lacks the style consistency over StyleDrop on Muse\nacross text prompts and style references. It is interesting to see such a difference as both Muse [4] and\nImagen [31] are trained on the same set of image/text pairs using the same text encoder (T5-XXL [26]).\nWe provide an ablation study to understand where the difference comes from in Sec. 4.4.1.\n4.2.1\nQuantitative Results\nFor quantitative evaluation, we synthesize images from a subset of Parti prompts [38]. This includes\n190 text prompts of basic text compositions, while removing some categories such as abstract, arts,\npeople or world knowledge. We test on 6 style reference images from Fig. 1.6\nCLIP scores. We employ two metrics using CLIP [25], (Text) and (Style) scores. For Text score,\nwe measure the cosine similarity between image and text embeddings. For Style score, we measure\nthe cosine similarity between embeddings of style reference and synthesized images. We generate 8\nimages per prompt for 190 text prompts, 1520 images in total. While we desire high scores, these\nmetrics are not perfect. For example, Style score can easily get to 1.0 if mode collapses.\nStyleDrop results in competitive Text scores to Muse (e.g., 0.323 vs 0.322 of StyleDrop (HF)) while\nachieving significantly higher Style scores (e.g., 0.556 vs 0.694 of StyleDrop (HF)), implying that\nsynthesized images by StyleDrop are consistent in style with style reference images, without losing\ntext-to-image generation capability. For the 6 styles we test, we see a light mode collapse from the\nfirst round of StyleDrop, resulting in a slightly reduced Text score. Iterative training (IT) improves\nthe Text score, which is aligned with our motivation. As a trade-off, however, they show reduced\nStyle scores over Round 1 models, as they are trained on synthetic images and styles may have been\ndrifted due to a selection bias.\nDreamBooth on Imagen falls short of StyleDrop in Style score (0.644 vs 0.694 of HF). We note\nthat the increment in Style score for DreamBooth on Imagen is less significant (0.569 \u2192 0.644)\nthan StyleDrop on Muse (0.556 \u2192 0.694). We think that the fine-tuning for style on Muse is more\neffective than that on Imagen. We revisit this in Sec. 4.4.1.\nHuman evaluation. We formulate 3 binary comparison tasks for user preference among StyleDrop\n(R1), StyleDrop with different feedback signals, and DreamBooth on Imagen. Users are asked to\nselect their preferred result in terms of style and text fidelity between images generated from two\n6Images used are (1, 2), (1, 6), (2, 3), (3, 1), (3, 5), (3, 6) of Fig. 1, and are also visualized in Fig. S7.\n7\n(a) A teapot in\n3d rendering style\n(b) A teapot in\n3d rendering style\n(c) A teapot in\n3d rendering style\n(a) A teapot in \nwatercolor painting style\n(b) A teapot in\nwatercolor painting style\n(c) A teapot in\nwatercolor painting style\n(a) A vase in\n3d rendering style\n(b) A vase in\n3d rendering style\n(c) A vase in\n3d rendering style\n(a) A vase in \nwatercolor painting style\n(b) A vase in\nwatercolor painting style\n(c) A vase in\nwatercolor painting style\n(a) A dog in\n3d rendering style\n(b) A dog in\n3d rendering style\n(c) A dog in\n3d rendering style\n(a) A dog in \nwatercolor painting style\n(b) A dog in\nwatercolor painting style\n(c) A dog in\nwatercolor painting style\nFigure 5: Qualitative comparison of (a) DreamBooth, (b) StyleDrop, and (c) DreamBooth + StyleDrop. For\nDreamBooth and StyleDrop, style and subject are guided by text prompts, respectively, whereas DreamBooth\n+ StyleDrop, both style (blue inset box at bottom left) and subject (red inset box at top right) are guided by\nrespective reference images. Image sources are in Tab. S1.\ndifferent models (i.e., an A/B test), while given a style reference image and the text prompt. Details\non the study is in Appendix B.2. Results are in Tab. 2 (top). Compared to DreamBooth on Imagen,\nimages by StyleDrop are significantly more preferred by users in Style score. The user study also\nshows style drifting more clearly when comparing StyleDrop (R1) and StyleDrop IT either by HF or\nCF. Between HF and CF, HF retains better Style and CLIP retained better Text. Overall, we find\nthat CLIP scores are a good proxy to the user study.\n4.3\nMy Object in My Style\nWe show in Fig. 5 synthesized images by sampling from two personalized generation distributions,\none for an object and another for the style, as described in Sec. 3.4. To learn object adapters, we\nuse 5\u223c6 images per object.7 Style adapters from Sec. 4.2 are used without any modification. The\nvalue of \u03b3 (to balance the contribution of object and style adapters) is chosen in the range 0.5\u20130.7.\nWe show synthesized images from (a) object adapter only (i.e., DreamBooth), (b) style adapter only\n(i.e., StyleDrop), and (c) both object and style adapters. We see from Fig. 5(a) that text prompts are\nnot sufficient to generate images with styles we desire. From Fig. 5(b), though StyleDrop gets style\ncorrect, it generates objects that are inconsistent with reference subjects. The proposed sampling\nmethod from two distributions successfully captures both my object and my style, as in Fig. 5(c).\n4.4\nAblations\nWe conduct ablations to better understand StyleDrop. In Sec. 4.4.1 we compare the behavior of\nthe Imagen and Muse models. In Sec. 4.4.2 we highlight the importance of a style descriptor. In\nSec. 4.4.3, we compare choices of feedback signals for iterative training. In Sec. 4.4.4, we show to\nwhat extent StyleDrop learns distinctive styles properties from reference images.\n4.4.1\nComparative Study of DreamBooth on Imagen and StyleDrop on Muse\nWe see in Sec. 4.2 that StyleDrop on Muse convincingly outperforms DreamBooth on Imagen. To\nbetter understand where the difference comes from, we conduct some control experiments.\nImpact of training text prompt. We note that both experiments in Sec. 4.2 are carried out using the\nproposed descriptive style descriptors. To understand the contribution of fine-tuning, rather than the\nprompt engineering, we conduct control experiments, one with a rare token as in [30] (i.e., \u201cA flower\nin [V*] style\u201d) and another with descriptive style prompt.\n7teapot and vase images from DreamBooth [30] are used.\n8\nFigure 6: Ablation study using Imagen [31]. (a, b) are trained with a rare token and (c, d) are trained with a style\ndescriptor. (a, c) are trained on a single style reference image. (b, d) are trained on 10 synthetic images from\nStyleDrop. With Imagen, we need 10 images and a descriptive style descriptor to capture the style, as in (d).\nMuse\n+: \u201c[V*]\u201d\n-: \u201c \u201d\n+: \u201cmelting golden 3d rendering\u201d\n-: \u201c \u201d\nMuse\n+: \u201cgolden 3d rendering\u201d\n-: \u201cmelting\u201d\nMuse\n+: \u201c[V*]\u201d\n-: \u201cmelting\u201d\nMuse\n(a)\n(b)\n(c)\n(d)\nStyle reference\nFigure 7: StyleDrop on Muse. All models are trained on 10 images from StyleDrop, which in turn was trained\non a single style image. (a, b) are trained with a rare token and (c, d) are trained with a style descriptor. When\ntrained with a descriptive style descriptor, StyleDrop can support additional applications such as style editing (d),\nhere removing the \u201cmelting\u201d component of the reference style.\nResults on Muse are in Fig. 7. Comparing (a) and (c), we do not find a substantial change, suggesting\nthat the style can be learned via an adapter tuning without too much help of a text prior. On the other\nhand, as seen in Fig. 6, comparing (a) and (c) with Imagen as a backbone model, we see a notable\ndifference. For example, \u201cmelting\u201d property only appears for some images synthesized from a model\ntrained with the descriptive style descriptor. This suggests that the learning capability of fine-tuning\non Imagen may not be as powerful as that of Muse, when given only a few training images.\nData efficiency. Next, we study whether the quality of the fine-tuning on Imagen could be improved\nwith more training data. In this study, we train a DreamBooth on Imagen using 10 human selected,\nsynthetic images from StyleDrop on Muse. Results are in Fig. 6. Two models are trained with (b)\na rare token and (d) a descriptive style descriptor. We see that the style consistency improves a lot\nwhen comparing (c) and (d) of Fig. 6, both in terms melting and golden properties. However, when\nusing a rare token, we do not see any notable improvement from (a) to (b). This suggests that the\nsuperiority of StyleDrop on Muse may be coming from its extraordinary fine-tuning data efficiency.\n4.4.2\nStyle Property Edit with Concept Disentanglement\nWe show in Sec. 4.4.1 that StyleDrop on Muse is able to learn the style using a rare token identifier.\nThen, what is the benefit of descriptive style descriptor? We argue that not all styles are described in\na single word and the user may want to learn style properties selectively. For example, the style of an\nimage in Fig. 7 may be written as a composite of \u201cmelting\u201d, \u201cgolden\u201d, and \u201c3d rendering\u201d, but the\nuser may want to learn its \u201cgolden 3d rendering\u201d style without \u201cmelting\u201d.\nWe show that such a style property edit can be naturally done with a descriptive style descriptor. As\nin Fig. 7(d), learning with a descriptive style descriptor provides an extra knob to edit a style by\nomitting certain words (e.g., \u201cmelting\u201d) from the style descriptor at synthesis. This clearly shows\nthe benefit of descriptive style descriptors in disentangling visual concepts and creating a new style\nbased on an existing one. This is less amenable when trained with the rare token, as in Fig. 7(b).\n4.4.3\nIterative Training with Different Feedback Signals\nFigure 8: Qualitative comparison of StyleDrop. (a) Round 1, (b) IT with random selection, (c) CLIP and (d)\nHuman feedback. Generated images of \u201ca banana\u201d and \u201ca bottle\u201d in \u201c3d rendering style\u201d are visualized. While\nStyleDrop Round 1 model captures the style very well, it often suffer from a content leakage (e.g., a banana and\nwomen are mixed). (c, d) IT with a careful selection of synthetic images reduces content leakage and improves.\nWe study how different feedback signals affects the performance of StyleDrop. We compare three\nfeedback signals, including human, CLIP, and random. For CLIP and random signals, we synthesize\n9\n64 images per prompt from 30 text prompts and select one image per prompt. For human, we select\n10 images from the same pool, which takes about 3 minutes per style. See Appendix B.2 for details.\nQualitative results are in Fig. 8. We observe that some images in (a) from a Round 1 model show a\nmix of banana or bottle with a human. Such concept leakage is alleviated with IT, though we still\nsee a banana with arms and legs with Random strategy. The reduction in concept leakage could be\nverified with the Text score, achieving (a) 0.303, (b) 0.322, (c) 0.339, and (d) 0.328. On the other\nhand, Style score, (a) 0.560, (b) 0.567, (c) 0.542, and (d) 0.549, could be misleading in this case, as\nwe compute the visual similarity to the style reference image, favoring a content leakage. Between\nCLIP and human feedback, we see a clear trade-off between text and style fidelity from quantitative\nmetrics.\n4.4.4\nFine-Grained Style Control with User Intention\n(a) flat color\n(b) flat color with offset\n(c) gradation\n(d) sharp corner\nFigure 9: Fine-grained style control. StyleDrop captures subtle style differences, such as (b) color offset, (c)\ngradation, or (d) sharp corner, reflecting designer\u2019s intention in text-to-image synthesis.\nMoreover, human feedback is more critical when trying to capture subtle style properties. In this\nstudy, we conduct experiments on four images in Fig. 9 inside orange boxes, created by the same\ndesigner with varying style properties, such as color offset (Fig. 9(b)), gradation (Fig. 9(c)), and\nsharp corners (Fig. 9(d)). We train two more rounds of StyleDrop with human feedback. We use the\nsame style descriptor of \u201cminimal flat illustration style\u201d to make sure the same text prior is given to\nall experiments. As in Fig. 9, style properties such as color offset, gradation, and corner shape are\ncaptured correctly. This suggests that StyleDrop offers the control of fine-grained style variations.\n5\nConclusion\nWe have presented StyleDrop, a novel approach to enable synthesis of any style through the use of\na few user-provided images of that style and a text description. Built on Muse [4] using adapter\ntuning [12], StyleDrop achieves remarkable style consistency at text-to-image synthesis. Training\nStyleDrop is efficient both in the number of learnable parameters (e.g., < 1%) and the number of\nstyle samples (e.g., 1) required.\nLimitations. Visual styles are of course even more diverse than what is possible to explore in our\npaper. More study with a well-defined system of visual styles, including, but not limited to, the\nformal attributes (e.g., use of color, composition, shading), media (e.g., line drawing, etching, oil\npainting), history and era (e.g., Renaissance painting, medieval mosaics, Art Deco), and style of art\n(e.g., Cubism, Minimalism, Pop Art), would broaden the scope. While we show in part the superiority\nof a generative vision transformer to diffusion models at few-shot transfer learning, it is by no means\nconclusive. We leave an in-depth study among text-to-image generation models as a future work.\nSocietal impact. As illustrated in Fig. 4, StyleDrop could be used to improve the productivity and\ncreativity of art directors and graphic designers when generating various visual assets in their own\nstyle. StyleDrop makes it easy to reproduce many personalized visual assets from as little as one seed\nimage. We recognize potential pitfalls such as the ability to copy individual artists\u2019 styles without\ntheir consent, and urge the responsible use of our technology.\nAcknowledgement. We thank Varun Jampani, Jason Baldridge, Forrester Cole, Jos\u00e9 Lezama, Steven\nHickson, Kfir Aberman for their valuable feedback on our manuscript.\n10\nReferences\n[1] Discord top servers worldwide by number of members. https://www.statista.com/statistics/\n1327141/discord-top-servers-worldwide-by-number-of-members/. 2\n[2] Midjourney. http://www.midjourney.com. 2\n[3] Low-rank adaptation for fast text-to-image diffusion fine-tuning, 2022. 2, 3, 6\n[4] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,\nKevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked\ngenerative transformers. arXiv preprint arXiv:2301.00704, 2023. 1, 2, 3, 4, 6, 7, 10\n[5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image\ntransformer. arXiv preprint arXiv:2202.04200, 2022. 3\n[6] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, and Changsheng Xu.\nStytr2: Image style transfer with transformers. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11326\u201311336, 2022. 3, 6\n[7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 12873\u201312883, 2021. 3\n[8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion.\narXiv preprint arXiv:2208.01618, 2022. 1, 2, 6\n[9] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style. arXiv preprint\narXiv:1508.06576, 2015. 3, 6\n[10] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact\nparameter space for diffusion fine-tuning. arXiv preprint arXiv:2303.11305, 2023. 2, 5\n[11] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for\ntext-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 3\n[12] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International\nConference on Machine Learning, pages 2790\u20132799. PMLR, 2019. 2, 3, 4, 6, 10, 13\n[13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021. 2, 3, 6\n[14] Siyu Huang, Jie An, Donglai Wei, Jiebo Luo, and Hanspeter Pfister. Quantart: Quantizing image style\ntransfer towards high visual fidelity. arXiv preprint arXiv:2212.10431, 2022. 3\n[15] Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu, and Mingli Song. Neural style\ntransfer: A review. IEEE transactions on visualization and computer graphics, 26(11):3365\u20133385, 2019. 3\n[16] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014. 6\n[17] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\ncustomization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022. 2, 5\n[18] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image\ngeneration using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 11523\u201311532, 2022. 3\n[19] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel,\nMohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback.\narXiv preprint arXiv:2302.12192, 2023. 5\n[20] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages\n3045\u20133059, 2021. 3\n[21] Yanghao Li, Naiyan Wang, Jiaying Liu, and Xiaodi Hou. Demystifying neural style transfer. arXiv preprint\narXiv:1701.01036, 2017. 3\n[22] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, and\nYang Cao. Cones: Concept neurons in diffusion models for customized generation. arXiv preprint\narXiv:2303.05125, 2023. 5\n[23] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv\npreprint arXiv:2302.08453, 2023. 2, 3\n11\n[24] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022. 5\n[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR,\n2021. 2, 5, 6, 7\n[26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\narXiv preprint arXiv:1910.10683, 2019. 3, 7\n[27] Dominic Rampas, Pablo Pernias, Elea Zhong, and Marc Aubreville. Fast text-conditional discrete denoising\non vector-quantized latent spaces. arXiv preprint arXiv:2211.07292, 2022. 3\n[28] Scott Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee. Learning to disentangle factors of variation\nwith manifold interaction. In International conference on machine learning, pages 1431\u20131439. PMLR,\n2014. 2\n[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10684\u201310695, 2022. 1, 2, 3, 6\n[30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint\narXiv:2208.12242, 2022. 1, 2, 4, 6, 7, 8\n[31] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-\nimage diffusion models with deep language understanding. Advances in Neural Information Processing\nSystems, 35:36479\u201336494, 2022. 1, 2, 3, 6, 7, 9\n[32] Kihyuk Sohn, Yuan Hao, Jos\u00e9 Lezama, Luisa Polania, Huiwen Chang, Han Zhang, Irfan Essa, and Lu Jiang.\nVisual prompt tuning for generative transfer learning. arXiv preprint arXiv:2210.00990, 2022. 3, 4, 6\n[33] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Moham-\nmad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video\ngeneration from open domain textual description. arXiv preprint arXiv:2210.02399, 2022. 3\n[34] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard\nprompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. arXiv preprint\narXiv:2302.03668, 2023. 2, 6, 16\n[35] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.\nImagereward: Learning and evaluating human preferences for text-to-image generation. arXiv preprint\narXiv:2304.05977, 2023. 5\n[36] Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee. Attribute2image: Conditional image generation\nfrom visual attributes. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The\nNetherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pages 776\u2013791. Springer, 2016. 2\n[37] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu,\nJason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. arXiv\npreprint arXiv:2110.04627, 2021. 3\n[38] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich\ntext-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 2, 3, 7\n12\nA\nImage Attributions\nWe provide the links to images used for style references (please click through on each number).\nTable S1: Image sources.\nFig. 1\n(row 1) 1, 2, 3, 4, 5, 6; (row 2) 1, 2, 3, 4, 5, 6; (row 3) 1, 2, 3, 4, 5, 6\nFigs. S5 and S6\n(row 1) 1, 2, 3, 4, 5, 6, 7, 8; (row 2) 1, 2, 3, 4, 5, 6, 7, 8; (row 3) 1, 2, 3, 4, 5, 6, 7, 8\nFig. 9\nSpecial thanks to Elizabeth Cruz for help designing the initial style image.\nTable S2: A list of text prompts used to synthesize images for additional round of training of StyleDrop. {}\u2019s are\nfilled with the style descriptors, e.g., \u201cin watercolor painting style\u201d.\n\u201cA chihuahua {}\u201d\n\u201cA chihuahua walking on the street {}\u201d\n\u201cA chihuahua walking in the forest {}\u201d\n\u201cA tabby cat {}\u201d\n\u201cA tabby cat walking on the street {}\u201d\n\u201cA tabby cat walking in the forest {}\u201d\n\u201cA portrait of chihuahua {}\u201d\n\u201cA portrait of tabby cat {}\u201d\n\u201cA portrait of human face {}\u201d\n\u201cAn apple on the table {}\u201d\n\u201cAn apple on the dish {}\u201d\n\u201cAn apple on the ground {}\u201d\n\u201cA banana on the table {}\u201d\n\u201cA banana on the dish {}\u201d\n\u201cA banana on the ground {}\u201d\n\u201cA human {}\u201d\n\u201cA human walking on the street {}\u201d\n\u201cA human walking in the forest {}\u201d\n\u201cA church on the street {}\u201d\n\u201cA temple on the street {}\u201d\n\u201cA cabin on the street {}\u201d\n\u201cA church in the mountain {}\u201d\n\u201cA temple in the mountain {}\u201d\n\u201cA cabin in the mountain {}\u201d\n\u201cA church in the field {}\u201d\n\u201cA temple in the field {}\u201d\n\u201cA cabin in the field {}\u201d\n\u201cA church on the beach {}\u201d\n\u201cA temple on the beach {}\u201d\n\u201cA cabin on the beach {}\u201d\nB\nExperiments\nB.1\nDetails on Model Training\nB.1.1\nAdapter Architecture\nWe apply an adapter at every layer of transformer. Specifically, following [12], we apply two adapters\nfor each layer, one after the cross-attention block, and another after the MLP block. An example code\nexplaining how to apply an adapter to the output of an attention layer and how to generate adapter\nweights are in Fig. S1. All up weights (wu) are initialized with zeros, and down weights (wd) are\ninitialized from truncated normal distribution with standard deviation of 0.02.\nWe note that adapter weights are generated in a parameter-efficient way via weight sharing across\ntransformer layers. This is triggered by setting is_shared to True, and the total number of parame-\nters would be reduced roughly by the number of transformer layers. The number of parameters of\nadapter weights are given in Tab. S3. While we use these settings on all experiments, one can easily\nreduce the number of parameters by setting is_shared to True for Base (Round 2) and Super-res\nfine-tuning without loss in quality.\nB.1.2\nHyperparameters\nWe provide in Tab. S3 hyperparameters for optimizer, adapter architecture, and synthesis. Note that\nwe use the batch size of 8, 1 per core of TPU v3, but StyleDrop can be also optimized on a single\nGPU (e.g., A100) with batch size of 1. We find that learning rate higher than 0.00003 for the base\nmodel often results in overfitting to content of a style reference image. Learning rate lower than\n0.00003 for the base model leads to slower convergence and we suggest to increase the number of\ntrain steps in such a case.\nB.1.3\nStyle Descriptors\nWe provide full description on descriptive style descriptors for images used in our experiments in\nTab. S4. As discussed in Sec. 4.4.1 and Sec. 4.4.2, StyleDrop works well without descriptive style\ndescriptors, but they add additional capability such as style editing.\n13\n1 import\nflax.linen as nn\n2 import jax\n3 import jax.numpy as jnp\n4\n5 def\napply_adapter(emb , wd , wu):\n6\n\"\"\" Applies\nadapter.\n7\n8\nArgs:\n9\nemb: token\nembedding , B x S x D.\n10\nwd: down weight , D x H.\n11\nwu: up weight , H x D.\n12\n13\nReturns:\n14\ntensor , B x S x D.\n15\n\"\"\"\n16\n17\nprj = jnp.einsum(\u2019...d,dh - >...h\u2019, emb , wd)\n18\nprj = jax.nn.gelu(prj)\n19\nprj = jnp.einsum(\u2019...h,hd - >...d\u2019, prj , wu)\n20\nreturn emb + prj\n21\n22\n23 class\nAdapterGenerator (nn.Module):\n24\n\"\"\" Generates\nAdapter\nWeights.\"\"\"\n25\n26\nd_emb: int\n# Embedding\ndimension.\n27\nd_prj: int\n# Projection\ndimension.\n28\nn_layer: int\n# Number of transformer\nlayers.\n29\nis_shared: bool\n# Share\nadapter\nparameters\nacross\nlayers.\n30\n31\n@nn.compact\n32\ndef\n__call__(self):\n33\nD, H, L = self.d_emb , self.d_prj , self.n_layer\n34\nidx = jnp.arange(L)\n35\nif self.is_shared:\n36\nidx0 = jnp.zeros_like(idx)\n37\n# Factorize\ndepth , emb and prj.\n38\ndd = nn.Embed(L, H)(idx).reshape(L, 1, H)\n39\ndu = nn.Embed(L, D)(idx).reshape(L, 1, D)\n40\nwd = nn.Embed(1, D * H)(idx0).reshape(L, D, H) + dd\n41\nwu = nn.Embed(1, H * D)(idx0).reshape(L, H, D) + du\n42\nelse:\n43\nwd = nn.Embed(L, D * H)(idx).reshape(L, D, H)\n44\nwu = nn.Embed(L, H * D)(idx).reshape(L, H, D)\n45\nreturn wd , wu\nFigure S1: An example code on how to apply an adapter and how adapter weights are generated in Flax-ish\nformat.\nB.2\nDetails on Human Evaluation\nIn this section, we provide more details on the user preference study discussed in Sec. 4.2.1. 3\nbinary comparison tasks are conducted between DreamBooth on Imagen and StyleDrop (Round 1),\nStyleDrop (Round 1) and StyleDrop (IT human), StyleDrop (IT human) and StyleDrop (IT CLIP).\n300 queries (50 queries per style from 6 styles, as shown in Fig. S7 are uploaded. The same query is\nasked to 5 raters independently to mitigate the human selection bias and variance. In total, we collect\n4500 answers.\nWe show in Fig. S2 the screenshot of the interface. We provide an instruction, examples, and task,\ncomposed of a reference image, text prompt, and two images that raters are asked to compare.\nInstructions.\n14\nTable S3: Hyperparameters for optimizer, adapter architecture, and synthesis.\nBase (Round 1)\nBase (Round 2)\nSuper-res\nLearning rate\n0.00003\n0.00003\n0.0001\nBatch size\n8\n8\n8\n# steps\n1000\n1000\n1000\nd_prj\n4\n32\n32\nis_shared\nTrue\nFalse\nFalse\n# adapter parameters\n0.23M\n12.6M\n6.3M\n# decoding step\n36\n36\n12\ntemperature\n4.5\n4.5\n4.5\n\u03bbA\n0.0\u20132.0\n2.0\n1.0\n\u03bbB\n5.0\n5.0\n0.0\n\u2022 Task: Given a reference image and two machine-generated output images, select which\nmachine-generated output better matches the style of the reference image.\n\u2022 Review this definition of a style: style (from Merriam-Webster): A particular manner or\ntechnique by which something is done, created or performed. Then choose either Image A,\nImage B, or Cannot Determine / Both Equally.\n\u2022 Next, review the reference text. Select which generated output is best described by the\nreference text. If you\u2019re again not sure, select Cannot Determine / Both Equally.\nQuestions.\n\u2022 Which Machine-Generated Image best matches the style of the reference image? Image A,\nImage B, Cannot Determine / Both Equally\n\u2022 Which Machine-Generated Image is best described by the reference text? Image A, Image\nB, Cannot Determine / Both Equally\nAdditional Analysis. We provide an additional analysis on the user preference study results. Note\nthat the numbers reported in Tab. 2 are based on the majority voting and claimed tie only if two\nmodels received the same number of votes. To provide a full picture, we draw a diagram that shows\nindividual vote counts in Fig. S3. We find that there are more \u201ctie\u201d counts, but confirm overall a\nconsistent trend with the results by the majority vote in Tab. 2.\nB.3\nExtended Ablation Study\nB.3.1\nClassifier-Free Guidance\nWe conduct an ablation study on classifier-free guidance (CFG) parameters, \u03bbA and \u03bbB, of Eq. (4).\nThey play different roles: \u03bbA controls the level of style adaptation and \u03bbB controls the text prompt\nfidelity. We conduct two sets of experiments, one with the StyleDrop Round 1 model and another\nwith the model trained with a human feedback (IT, human).\nStyleDrop (Round 1) model. In this study, we use StyleDrop (Round 1) model, which is trained on\na single style reference image.\n1. \u03bbA with fixed \u03bbB. Firstly, we vary \u03bbA while fixing \u03bbB to 5.0. Results are in Fig. S4a.\nWhen \u03bbA = 0.0, we find synthesized images having less faithful styles to the style reference\nimages. As we increase \u03bbA, we see the style of synthesized images getting more consistent.\nHowever, when \u03bbA becomes too large (e.g., 5.0) the style factor dominates the sampling\nprocess, making the content of synthesized images collapsed to that of the style reference\nimage and hard to make it follow the text condition.\n2. \u03bbB with fixed \u03bbA. Subsequently, we investigate the impact of \u03bbB, while fixing \u03bbA = 2.0.\nResults are in Fig. S4c. When \u03bbB = 0.0, we see that synthesized images being collapsed to\n15\nTable S4: Text prompts used for experiments in Fig. 1. We construct a text prompt by composing descriptions of\na content (e.g., object) and style (e.g., watercolor painting).\nimage\ntext prompt\nimage\ntext prompt\n\u201cA house in\nwatercolor painting style\u201d\n\u201cA cat in\nwatercolor painting style\u201d\n\u201cFlowers in\nwatercolor painting style\u201d\n\u201cA village in\noil painting style\u201d\n\u201cA village in\nline drawing style\u201d\n\u201cA portrait of a person\nwearing a hat in\noil painting style\u201d\n\u201cA person drowning\ninto the phone in\ncartoon line drawing style\u201d\n\u201cA woman walking a dog in\nflat cartoon illustration style\u201d\n\u201cA woman working on a laptop\nin flat cartoon illustration style\u201d\n\u201cA Christmas tree in\nsticker style\u201d\n\u201cA wave in\nabstract rainbow colored\nflowing smoke wave design\u201d\n\u201cA mushroom\nin glowing style\u201d\n\u201cSlices of watermelon and\nclouds in the background in\n3d rendering style\u201d\n\u201cA thumbs up in\nglowing 3d rendering style\u201d\n\u201cA woman in\n3d rendering style\u201d\n\u201cA bear in\nkid crayon drawing style\u201d\n\u201cA flower in\nmelting golden\n3d rendering style\u201d\n\u201cA Viking face with beard in\nwooden sculpture\u201d\nthe style reference image without being too much text controlled. Increasing \u03bbB improves\nthe text fidelity, but eventually override the learned style to a more generic, text-guided style\nof Muse model.\nOverall, we verify that \u03bbA and \u03bbB play roles as intended to control the style adaptation and the text\nprompt conditioning. Nonetheless, these two parameters impact each other and we find that fixing\n\u03bbB = 5.0 and control \u03bbA to trade-off the style and text fidelity is sufficient for most cases.\nStyleDrop IT (human) model. Next, we use StyleDrop (IT, human), which is trained on synthetic\nimages manually selected with a human feedback.\nB.4\nExtended Baseline Comparison\nIn addition to Sec. 4.2 and Fig. 4, we provide additional qualitative comparison with hard prompt\nmade easy (PEZ) [34] in Fig. S14. We do not find PEZ to be better than any of the compared methods\nincluding StyleDrop and DreamBooth.\n16\nFigure S2: User preference study interface. We cast the problem as a binary comparison task and ask raters to\nchoose one from two images that is best aligned with the style reference image or the text prompt. The same\nquery is asked to 5 different raters, with location of two images randomized.\n17\nSDRP, R1\nDB on Imagen\nTie\n20%\n40%\n60%\n80%\n100%\n77%\n2%\n16%\nStyle\nSDRP, R1\nDB on Imagen\nTie\n20%\n40%\n60%\n80%\n100%\n15%\n6%\n74%\nText\nNumber of \n     votes\n5/5\n4/5\n3/5\n2/5\n1/5\n0/5\n(a) StyleDrop Round 1 (SDRP, R1) vs. DreamBooth on Imagen.\nSDRP, R1\nSDRP, IT\nTie\n20%\n40%\n60%\n80%\n100%\n47%\n17%\n25%\nStyle\nSDRP, R1\nSDRP, IT\nTie\n20%\n40%\n60%\n80%\n100%\n3%\n6%\n89%\nText\nNumber of \n     votes\n5/5\n4/5\n3/5\n2/5\n1/5\n0/5\n(b) StyleDrop Round 1 (SDRP, R1) vs. StyleDrop IT (human) (SDRP, IT).\nSDRP, IT \n (human)\nSDRP, IT \n (CLIP)\nTie\n20%\n40%\n60%\n80%\n100%\n37%\n19%\n28%\nStyle\nSDRP, IT \n (human)\nSDRP, IT \n (CLIP)\nTie\n20%\n40%\n60%\n80%\n100%\n3%\n5%\n89%\nText\nNumber of \n     votes\n5/5\n4/5\n3/5\n2/5\n1/5\n0/5\n(c) StyleDrop IT (human) (SDRP, IT (human)) vs. StyleDrop IT (CLIP) (SDRP, IT (CLIP)).\nFigure S3: Comprehensive analysis on user preference study.\n18\n\u03bbA = 0.0\n\u03bbA = 0.2\n\u03bbA = 0.5\n\u03bbA = 1.0\n\u03bbA = 1.5\n\u03bbA = 2.0\n\u03bbA = 5.0\nStyle reference\n(a) Varying \u03bbA while fixing \u03bbB = 5.0 on StyleDrop (R1) model.\nStyle reference\n\u03bbA = 0.0\n\u03bbA = 0.2\n\u03bbA = 0.5\n\u03bbA = 1.0\n\u03bbA = 1.5\n\u03bbA = 2.0\n\u03bbA = 5.0\n(b) Varying \u03bbA while fixing \u03bbB = 5.0 on StyleDrop (HF) model.\nStyle reference\n\u03bb B= 0.0\n\u03bbB = 1.0\n\u03bbB = 2.0\n\u03bbB = 5.0\n\u03bb B= 10.0\n\u03bbB = 20.0\n\u03bbB = 50.0\n(c) Varying \u03bbB while fixing \u03bbA = 2.0 on StyleDrop (R1) model.\n(d) Varying \u03bbB while fixing \u03bbA = 2.0 on StyleDrop (HF) model.\nFigure S4: Ablation study on the classifier-free guidance (CFG) scales \u03bbA and \u03bbB of Eq. (4) on (a, c) StyleDrop\n(R1) and (b, d) StyleDrop (HF). \u03bbA controls the style adaptation and \u03bbB controls the text prompt condition.\nStyleDrop (R1) model responds to CFG scales sensibly and shows issues such as a content leakage (e.g., with\nlarge values of \u03bbA, or small values of \u03bbB). On the other hand, StyleDrop (HF) model shows robustness to the\nchange of guidance scales. Text prompts are \u201cAn Opera house in Sydney\u201d and \u201cA temple\u201d.\n19\n(a) \u201cA fluffy baby sloth with a knitted hat trying to figure out a laptop, close up\u201d\n(b) \u201cA Golden Gate bridge\u201d\n(c) \u201cThe letter \u2018G\u2019\u201d\nFigure S5: StyleDrop on 24 styles. Style descriptors are appended to each text prompt.\n20\n(a) \u201cA man riding a snowboard\u201d\n(b) \u201cA panda eating bamboo\u201d\n(c) \u201cA friendly robot\u201d\nFigure S6: StyleDrop on 24 styles. Style descriptors are appended to each text prompt.\n(a) source\n(b) source\n(c) source\n(d) source\n(e) source\n(f) source\nFigure S7: Style reference images. (a) \u201cmelting golden 3d rendering\u201d, (b) \u201c3d rendering\u201d, (c) \u201coil painting\u201d, (d)\n\u201cwatercolor painting\u201d, (e) \u201cwooden sculpture\u201d, (f) \u201cflat cartoon illustration\u201d.\n21\n(a) source\n(b) A baby penguin, animals\n(c) A moose, animals\n(d) A towel, artifacts\n(e) An espresso machine, artifacts\n(f) An avocado, produce and plants\n(g) A crown, artifacts\nFigure S8: Results comparison without cherry-picking. From left to right, DreamBooth on Imagen, LoRA\nDreamBooth on Stable Diffusion, StyleDrop (round 1), StyleDrop (round 2, HF), StyleDrop (round 2, CLIP\nscore feedback). A style reference image is shown in Fig. S7a.\n22\n(a) source\n(b) A baby penguin, animals\n(c) A moose, animals\n(d) A towel, artifacts\n(e) An espresso machine, artifacts\n(f) An avocado, produce and plants\n(g) A crown, artifacts\nFigure S9: Results comparison without cherry-picking. From left to right, DreamBooth on Imagen, LoRA\nDreamBooth on Stable Diffusion, StyleDrop (round 1), StyleDrop (round 2, HF), StyleDrop (round 2, CLIP\nscore feedback). A style reference image is shown in Fig. S7b.\n23\n(a) source\n(b) A baby penguin, animals\n(c) A moose, animals\n(d) A towel, artifacts\n(e) An espresso machine, artifacts\n(f) An avocado, produce and plants\n(g) A crown, artifacts\nFigure S10: Results comparison without cherry-picking. From left to right, DreamBooth on Imagen, LoRA\nDreamBooth on Stable Diffusion, StyleDrop (round 1), StyleDrop (round 2, HF), StyleDrop (round 2, CLIP\nscore feedback). A style reference image is shown in Fig. S7c.\n24\n(a) source\n(b) A baby penguin, animals\n(c) A moose, animals\n(d) A towel, artifacts\n(e) An espresso machine, artifacts\n(f) An avocado, produce and plants\n(g) A crown, artifacts\nFigure S11: Results comparison without cherry-picking. From left to right, DreamBooth on Imagen, LoRA\nDreamBooth on Stable Diffusion, StyleDrop (round 1), StyleDrop (round 2, HF), StyleDrop (round 2, CLIP\nscore feedback). A style reference image is shown in Fig. S7d.\n25\n(a) source\n(b) A baby penguin, animals\n(c) A moose, animals\n(d) A towel, artifacts\n(e) An espresso machine, artifacts\n(f) An avocado, produce and plants\n(g) A crown, artifacts\nFigure S12: Results comparison without cherry-picking. From left to right, DreamBooth on Imagen, LoRA\nDreamBooth on Stable Diffusion, StyleDrop (round 1), StyleDrop (round 2, HF), StyleDrop (round 2, CLIP\nscore feedback). A style reference image is shown in Fig. S7e.\n26\n(a) source\n(b) A baby penguin, animals\n(c) A moose, animals\n(d) A towel, artifacts\n(e) An espresso machine, artifacts\n(f) An avocado, produce and plants\n(g) A crown, artifacts\nFigure S13: Results comparison without cherry-picking. From left to right, DreamBooth on Imagen, LoRA\nDreamBooth on Stable Diffusion, StyleDrop (round 1), StyleDrop (round 2, HF), StyleDrop (round 2, CLIP\nscore feedback). A style reference image is shown in Fig. S7f.\n27\n(a) StyleDrop\nStyle references\n(e) Hard Prompt Made Easy (PEZ) on Stable Diffusion\n(b) Dreambooth on Imagen\n(c) Dreambooth (LoRA) on Stable Diffusion\n(d) Textual Inversion on Stable Diffusion\nFigure S14: Style tuning comparison to baseline methods, including (b) DreamBooth on Imagen, (b) DreamBooth\n(LoRA) on Stable Diffusion, (c) Textual Inversion on Stable Diffusion, and (e) Hard Prompt Made Easy (PEZ)\nin Stable Diffusion.\n28\n"
  },
  {
    "title": "The Hidden Language of Diffusion Models",
    "link": "https://arxiv.org/pdf/2306.00966.pdf",
    "upvote": "3",
    "text": "The Hidden Language of Diffusion Models\nHila Chefer\u22171,2\nOran Lang1\nMor Geva3\nVolodymyr Polosukhin1\nAssaf Shocher1\nMichal Irani1,4\nInbar Mosseri1\nLior Wolf2\n1Google Research\n2Tel-Aviv University\n3Google DeepMind\n4Weizmann Institute\nhttps://hila-chefer.github.io/Conceptor/\nConcept: painter\nConcept: sweet peppers\nfingers\npepper\nCONCEPTOR\nCONCEPTOR\nConcept: beetle\nhornet\nemerald\nCONCEPTOR\nPicasso\nMonet\nImpressionism\nbrushes\npaint\nportrait\n+\n+\n(a)   \nConcept decomposition with CONCEPTOR\n(b) \nSingle-image decomposition with CONCEPTOR\nFigure 1. Concept interpretation with CONCEPTOR. (a) Given a set of representative concept images, CONCEPTOR learns to decompose\nthe concept into a weighted combination of interpretable elements (font sizes indicate weights). The decomposition exposes interesting\nbehaviors such as reliance on prominent painters and renowned artistic styles (e.g., \u201cMonet\u201d, \u201cImpressionism\u201d). (b) Given a specific\ngenerated image, CONCEPTOR extracts its primary contributing elements, revealing surprising visual connections (e.g., \u201csweet peppers\u201d are\nlinked to \u201cfingers\u201d due to their common shape).\nAbstract\nText-to-image diffusion models have demonstrated an un-\nparalleled ability to generate high-quality, diverse images\nfrom a textual prompt. However, the internal representations\nlearned by these models remain an enigma. In this work,\nwe present CONCEPTOR, a novel method to interpret the\ninternal representation of a textual concept by a diffusion\nmodel. This interpretation is obtained by decomposing the\nconcept into a small set of human-interpretable textual el-\nements. Applied over the state-of-the-art Stable Diffusion\nmodel, CONCEPTOR reveals non-trivial structures in the\nrepresentations of concepts. For example, we find surpris-\ning visual connections between concepts, that transcend\ntheir textual semantics. We additionally discover concepts\nthat rely on mixtures of exemplars, biases, renowned artis-\ntic styles, or a simultaneous fusion of multiple meanings of\nthe concept. Through a large battery of experiments, we\ndemonstrate CONCEPTOR\u2019s ability to provide meaningful,\nrobust, and faithful decompositions for a wide variety of\nabstract, concrete, and complex textual concepts, while al-\n*The first author performed this work as an intern at Google Research.\nlowing to naturally connect each decomposition element to\nits corresponding visual impact on the generated images.\n1. Introduction\nGenerative models have demonstrated unprecedented ca-\npabilities to create high-quality, diverse imagery based on\ntextual descriptions [2,11,34,37,40]. While revolutionary, re-\ncent works have demonstrated that these models often suffer\nfrom heavy reliance on biases [7,28] and occasionally also\ndata memorization [3,42]. However, all these works draw\nconclusions by a simple external evaluation of the output\nimages, while research on understanding the internal repre-\nsentations learned by the model remains scarce. Thus, our\nunderstanding of these impressive models remains limited.\nIn this work, we aim to develop a method to interpret the\ninner representations of text-to-image diffusion models. Our\napproach draws inspiration from the field of concept-based\ninterpretability [16,21,24], which proposes to interpret the\nmodel\u2019s decision-making for a given input by decomposing\nit into a set of elements that impact the prediction (e.g., de-\ncomposing \u201ccat\u201d into \u201cwhiskers\u201d, \u201cpaws\u201d, etc.). Under\n1\narXiv:2306.00966v3  [cs.CV]  5 Oct 2023\nthis setting, an \u201cinterpretation\u201d can be considered to be a\nmapping function from the internal state of the model to a\nset of concepts humans can understand [21]. Notably, ex-\nisting approaches for concept-based interpretability are not\ndirectly applicable to generative models, and, as we demon-\nstrate, fall short of producing meaningful interpretations for\nsuch models. Therefore, we propose a novel method to\nproduce concept-based explanations for diffusion models,\nwhich leverages their unique structure and properties.\nOur method, CONCEPTOR, learns a pseudo-token, re-\nalized as a combination of interpretable textual elements\n(Fig. 1(a)). To obtain the pseudo-token, CONCEPTOR trains\na neural network to map each word embedding in the vocab-\nulary of the model to a corresponding coefficient, with the\nobjective of denoising the concept images. The pseudo-token\nis then constructed as a linear combination of the top vocab-\nulary elements weighted by their learned coefficients. This\nformulation allows us to exploit both the model\u2019s powerful\nability to link between text and image, and the rich semantic\ninformation encapsulated in the text encoder. Through a\nlarge battery of experiments, we demonstrate that CONCEP-\nTOR produces decompositions that are meaningful, robust,\nand faithful to the model.\nWe use CONCEPTOR to analyze the state-of-the-art text-\nto-image diffusion model, Stable Diffusion [37] across vari-\nous concepts, including concrete (e.g., \u201ca nurse\u201d), abstract\n(e.g., \u201caffection\u201d) and complex (e.g., \u201celegance on a plate\u201d)\nconcepts, as well as homograph concepts (e.g., \u201ca crane\u201d).\nCONCEPTOR reveals many interesting observations about\nthe learned representations. (i) As demonstrated in Fig. 1(b),\nCONCEPTOR can be used to decompose a generated image\nto its own subset of driving elements. We find non-trivial\ncombinations of features that control different visual aspects\nsuch as shapes, textures, and colors. (ii) We observe that\nsome concepts such as \u201ca president\u201d or \u201ca rapper\u201d are rep-\nresented mostly by exemplars, i.e., well-known instances\nfrom the concept, such that the generated images are inter-\npolations of those instances. (iii) We additionally find that\nconsistent with previous work [35], the model learns to mix\nthe multiple meanings of homograph concepts. We expand\nthose findings and discover cases where these meanings are\nleveraged simultaneously, creating images that mix both\nmeanings in a single object. (iv) Finally, we demonstrate our\nmethod\u2019s effectiveness in the detection of non-trivial biases.\nTo conclude, our work makes the following contributions:\n\u2022 We present CONCEPTOR, a novel method to decompose a\ntextual concept into a set of interpretable elements. Our\nmethod utilizes a unique form of decomposition in which\na linear combination is learned as a mapping from the\ntextual embedding space to a coefficient.\n\u2022 We demonstrate profound learned connections between\nconcepts that transcend textual correlations.\n\u2022 We discover non-trivial structures in the learned decom-\npositions such as interpolations of exemplars, reliance on\nrenowned artistic styles, and mixing of different meanings\nof the concept.\n\u2022 We demonstrate the detection of biases that are not easily\nobservable visually. These observations can help discuss\nimportant ethical questions on a factual basis.\n2. Related Work\nText-guided image generation\nRecently, impressive re-\nsults were achieved for text-guided image generation with\nlarge-scale auto-regressive models [34, 46] and diffusion\nmodels [29,33,37,40]. In the context of text-to-image dif-\nfusion models, a related line of work aims to introduce per-\nsonalized concepts to a pre-trained text-to-image model by\nlearning to map a set of images to a \u201ctoken\u201d in the text space\nof the model [12, 23, 39]. Importantly, these methods do\nnot produce decomposable or interpretable information and\nmostly result in a rigid learned vector that resides outside of\nthe model\u2019s distribution [44].\nConcept-based interpretability\nA similar analysis to\nours was conducted on concept representations in the context\nof language models [24,26,27,30], often through projections\nto the vocabulary space [14, 15, 32]. For image classifiers\nbased on CNNs, TCAV [21] propose the first concept-based\nexplainability method by training a linear classifier over\nuser-defined concepts. ACE [16] leverages multi-resolution\nsegmentation of the class images and clusters the bottleneck\nrepresentations of the crops. ICE [49] and CRAFT [10]\napply matrix factorization methods on a feature map matrix\nextracted from a set of patches to obtain the decomposition.\nNote that all these methods are not applicable directly to\ndiffusion models. First, most of the methods are based on\nCNN architectures [16] with non-negative activations [10,\n49], thus cannot be directly generalized to negative feature\nmatrices. Most importantly, all methods above perform\nconcept importance estimation by measuring the shift in\nprediction or by employing saliency methods, both are not\ntrivially applicable to a generative model.\nDiffusion model interpretability\nShared text-image rep-\nresentations such as CLIP [31] have been analyzed in the\npast [5,6,47]. However, none of these works has been gen-\neralized to generative models. As far as we can ascertain,\nthe closest effort to explaining text-to-image models is a\nsimple visualization of the cross-attention maps [4,17,18].\nSome works [3,35] have attempted to investigate the images\nproduced by text-to-image diffusion models, and have even\nfound evidence of memorization [3]. However, these works\nrely entirely on the generated images, and are, therefore,\ntime-consuming and require access to the data. For exam-\nple [42] were able to show memorization for less than 2% of\nthe generations with SD. Additionally, these works do not\nattempt to dissect the model\u2019s inner representations.\n2\n3. Method\nPreliminaries: Latent Diffusion Models (LDMs)\nWe\napply our method over the state-of-the-art Stable Diffusion\n(SD) model [37]. SD employs a denoising diffusion proba-\nbilistic model (DDPM) [20,41] over an input latent vector\nzT \u223c N(0, 1) and gradually denoises it. Namely, at each\ntimestep, t = T, . . . , 1, the DDPM receives a noised latent\nvector zt and produces a less noisy vector zt\u22121, which serves\nas the input to the next step.\nDuring the denoising process, the model is typically con-\nditioned on a text encoding for an input prompt P, produced\nby a frozen CLIP text encoder [31], which we denote by C.\nThe text encoder converts the textual prompt P to a sequence\nof tokens, which can be words, sub-words, or punctuation\nmarks. Then, the encoder\u2019s vocabulary, V \u2208 RN,d, is used\nto map each token in the prompt to an embedding vector\nw \u2208 Rd, where d is the embedding dimension of the en-\ncoder, and N is the number of tokens in the vocabulary. The\nDDPM model is trained to minimize the loss,\nLrec = Ez,P,\u03b5\u223cN (0,1),t\n\u0002\n||\u03b5 \u2212 \u03b5\u03b8(zt, t, C(P))||2\u0003\n,\n(1)\nfor,\nzt = \u221a\u03b1tz +\n\u221a\n1 \u2212 \u03b1t\u03b5,\n(2)\nwhere \u03b5\u03b8 is a trained UNet [38], and 0 = \u03b1T < \u03b1T \u22121 <\n\u00b7 \u00b7 \u00b7 < \u03b10 = 1. In other words, during training, the input\nimage x is encoded to its corresponding latent vector z. A\nnoise vector \u03b5 and a timestep t are drawn randomly. The\nnoise vector \u03b5 is then added to the latent vector z as specified\nin Eq. 2, and the UNet is trained to predict the added noise\n\u03b5.\nCONCEPTOR\nOur goal is to interpret the internal repre-\nsentation of an input concept c in a text-to-image diffusion\nmodel \u03b5\u03b8. Formally, given a prompt Pc for the concept c, we\nlearn a decomposition for the concept using the vocabulary\nV. This decomposition is realized as a pseudo-token w\u2217 /\u2208 V\nthat is constructed as a weighted combination of a subset of\ntokens from V, i.e.,\nw\u2217 =\nn\nX\ni=1\n\u03b1iwi\ns.t.\nwi \u2208 V, \u03b11, . . . , \u03b1n \u2265 0\n(3)\nwhere n << N is a hyperparameter that determines the\nnumber of tokens to use in the combination.\nLearning the set of n vocabulary elements wi and their\nassociated coefficients \u03b1i is done separately for each con-\ncept c. We begin by collecting a training set T of 100 con-\ncept images. These images provide the statistics for the\nconcept features we wish to learn. Next, we construct our\nlearned pseudo-token over the vocabulary V. This construc-\ntion serves two purposes. First, note that the vocabulary V is\nexpressive and diverse, containing roughly 50, 000 tokens,\nranging from famous personalities to emojis and even names\nof video games (e.g., Fortnite is a single token in V). By\nlearning our decomposition over this rich set of candidate\nconcepts, we facilitate a meaningful yet non-restrictive opti-\nmization over semantic, human-understandable information.\nThus, our choice of candidate concepts achieves the best of\nboth worlds \u2013 it is both expressive and interpretable. Sec-\nond, we make a design choice to optimize the coefficients\nas a function of the word embeddings. This choice is crit-\nical to the success of our method since it utilizes the rich\ntextual embedding space of CLIP in determining the coef-\nficients. Effectively, this reduces the optimization problem\nfrom optimizing 50, 000 unrelated coefficients to learning\na mapping from a smaller semantic space, Rd. Specifically,\nour method assigns a coefficient \u03b1 for each word embedding\nw using a learned 2-layer MLP, which takes as input the\nword embedding vector w as follows,\n\u2200w \u2208 V : \u03b1 = f(w) = \u03c3 (W2(\u03c3(W1(w)))) ,\n(4)\nwhere \u03c3 is the ReLU non-linearity [1], and W1, W2 are linear\nmappings. Based on f, we compute w\u2217\nN = PN\ni=1 f(wi)wi.\nNote that this pseudo-token is not identical to the output\ntoken w\u2217 since w\u2217\nN contains all tokens in V. w\u2217 is obtained\nby the top tokens from w\u2217\nN, as described in Eq. 5.\nTo learn a meaningful pseudo-token w\u2217, we optimize our\nMLP to reconstruct the images generated from Pc. This\nchoice encourages our pseudo-token to imitate the denoising\nprocess of the concept images. We draw a random noise\n\u03b5 \u223c N(0, 1) and a timestep t \u2208 {1, . . . , T} for each image,\nand noise the images according to Eq. 2. We then employ\nthe model\u2019s reconstruction objective from Eq. 1.\nAs mentioned, w\u2217\nN considers all the tokens in the vo-\ncabulary. However, for better interpretability, we wish to\nrepresent the input concept with a small set of n << N to-\nkens. Notate by w1, . . . , wn \u2208 V the tokens with the highest\nlearned coefficients. We add a regularization loss to encour-\nage the pseudo-token w\u2217\nN to be dominated by these top n\ntokens, i.e.,\nLsparsity = 1 \u2212 cosine (w\u2217, w\u2217\nN) .\n(5)\nThis encourages the pseudo-token w\u2217, defined by the top\nn tokens in V, to be semantically similar to w\u2217\nN, which\nis defined by the entire vocabulary. Our overall objective\nfunction is, therefore,\nL = Lrec + \u03bbsparsityLsparsity,\n(6)\nIn our experiments, we set \u03bbsparsity = 0.001, n = 50. At\ninference time, we employ the MLP on the vocabulary V and\nconsider the top n = 50 tokens to compose w\u2217, as specified\nin Eq. 3. Implementation details and a figure describing our\nmethod can be found in Appendix A.\nSingle-image decomposition\nGiven an image I that was\ngenerated by SD for a concept c, we wish to determine the\n3\ncamel\ngiraffe\ncashmere\n+\n=\nsnake\ntwisted\ngecko\nsnail\nladybug\nwinding\n+\n=\nreflections of earth\nsphere\ncivilization\ndietitian\npharmacist\nnutritious\n+\n=\nfear\nscream\nwolf\npainter\nelectrician\n- Monet\n- brush\nimpression of\nJapanese serenity\nbonsai\n- pagoda\n- sakura\n+\n=\n+\n=\n+\n=\nFigure 2. Decompositions of single images by CONCEPTOR. The top rows present images found to contain two elements. The last row shows\nmore complex mixtures by removing one element at a time. The examples demonstrate the meaningfulness of our learned decompositions.\nsubset of the tokens from the decomposition w\u2217, that drove\nthe generation of this specific image. This is done via an\niterative process over the tokens wj \u2208 w\u2217 as follows; at each\nstep, we attempt to remove a single token from the decom-\nposition, w\u2217\nj = P\ni\u0338=j \u03b1iwi, and generate the corresponding\nimage Ij with the prompt Pw\u2217\nj and the same seed. Next,\nwe use CLIP\u2019s image encoder to determine if Ij is semanti-\ncally identical to I. If the CLIP score of the two images is\nhigher than 95, we remove wj from w\u2217 and continue to the\nnext token. This criterion avoids incorporating tokens whose\nremoval only causes minor non-semantic modifications to\nthe image I (such as a slight shift in pose). This process is\nrepeated for all tokens in w\u2217 until no tokens are removed.\n4. Experiments\nConcept-based explanation desiderata\nFollowing previ-\nous literature on concept-based explainability [16], we begin\nby defining a set of desired properties that concept-based\nexplanations for diffusion models should uphold. These\nproperties will be the criteria to evaluate our method. (i)\nMeaningfulness- Each decomposition element should be\nsemantically meaningful and human-understandable. (ii)\nFaithfulness- The decomposition should be faithful to the\nconcept representation by the model. In other words, the\ndecomposition should reconstruct the features manifested\nin each of the concept images, and produce images that are\nin the distribution of the concept images. (iii) Robustness-\nThe decomposition should be independent of the selection\nof the training set and the initialization. Next, we conduct\nextensive experiments to demonstrate our method\u2019s ability\nto produce meaningful, faithful, and robust decompositions\nfor diverse types of concepts. Throughout this section, we\nnotate by wc the token(s) corresponding to the concept c.\nData\nWe construct a diverse and comprehensive dataset\nof 188 concepts, comprised of the basic classes from CIFAR-\n10 [22], a list of 28 professions from the Bias in Bios\ndataset [8], 10 basic emotions and 10 basic actions, all 30\nprompts from the website Best 30 Stable Diffusion Prompts\nfor Great Images*, which contains complex prompts that\n*https://mspoweruser.com/best-stable-diffusion-prompts/\n4\n- \u2018stork\u2019\nInput Image\n+\u2018stork\u2019\nConcept: crane\n- \u2018fish\u2019\n+\u2018fish\u2019\nConcept: bass\n- \u2018nerdy\u2019\n+\u2018nerdy\u2019\nConcept: professor\n- \u2018dating\u2019\nInput Image\n+\u2018dating\u2019\nConcept: date\n- \u2018apple\u2019\n+\u2018apple\u2019\nConcept: big apple\n- \u2018Trump\u2019\n+\u2018Trump\u2019\nConcept: president\nFigure 3. Coefficient manipulation. For each of the listed concepts, we manipulate the coefficient of a single element from the decomposition\nand observe its visual impact on the generated image. The examples demonstrate the meaningfulness of our learned decompositions.\nrequire hierarchical reasoning (e.g., \u201cMedieval village life\u201d,\n\u201cimpression of Japanese serenity\u201d), and, finally, we consider\n100 random concepts from the ConceptNet [43] knowledge\ngraph to allow for large-scale evaluation of the methods. A\nfull list of all concepts is provided in Appendix B.\n4.1. Qualitative Results\n4.1.1\nFrom Text to Multi-Modal Explanations\nThe ability to link between textual decomposition elements\nand their visual manifestation is important to facilitate hu-\nman understanding and establish meaningfulness. We pro-\npose two strategies to obtain these connections. First is our\nsingle-image decomposition scheme, described in Sec. 3.\nSecond, we propose to gradually manipulate the element\u2019s\ncoefficient to observe the visual changes it induces.\nSingle-image decomposition\nFigs. 1 and 2 and Ap-\npendix D contain examples of decompositions over images\ngenerated by SD. The first three rows of Fig. 2 present ex-\namples of images that decompose into two elements, while\nthe last row contains more complex concepts that decom-\npose into three elements, where we visualize the removal\nof one element at a time. Note that the results show non-\ntrivial and profound links between concepts. For example,\nthe \u201csnake\u201d in Fig. 2 is constructed as a \u201ctwisted gecko\u201d,\nthe \u201ccamel\u201d borrows its skin texture and color from the\n\u201ccashmere\u201d, etc. These examples demonstrate associations be-\nyond textual correlations, based on visual similarities such as\nshape, texture, and color. Additionally, note that the decom-\npositions represent various different representation strate-\ngies. Some involve a mixture of shape and appearance (e.g.,\n\u201csnail\u201d), others involve compositional features added gradu-\nally (e.g., \u201cimpression of Japanese serenity\u201d), etc. Finally,\nobserve that the \u201cpainter\u201d decomposition demonstrates re-\nliance on renowned artistic styles, such that when \u201cMonet\u201d\nis removed, the painting disappears, even though \u201cMonet\u201d\ndid not appear explicitly in the input prompt. Appendix D.1\npresents a further investigation of this reliance on artistic\nstyles.\nCoefficient manipulation\nFig. 3 presents examples of\ncoefficient manipulations. First, we present examples of\nhomograph concepts (first, second row of Fig. 3). Observe\nthat in some cases, e.g., \u201cbig apple\u201d, both meanings are\ngenerated separately in the image (an apple, New York City),\nwhile other cases, e.g., \u201ccrane\u201d, \u201cdate\u201d generate a single\nobject. Even in the latter case, the manipulation shows that\nboth meanings impact the generated image, implicitly. For\nexample, when reducing the element \u201cstork\u201d from \u201ccrane\u201d,\nthe structure of the crane changes. Evidently, the model\nemploys both meanings simultaneously, borrowing the ap-\npearance from the machine and the shape from the bird.\nNext, we present examples of element inspection. In the\nfirst example, we visualize the impact of \u201cnerdy\u201d on the\nconcept \u201cprofessor\u201d. As can be observed, it controls the\nprofessor\u2019s baldness, the glasses, the suit, and the library\nin the background. Secondly, we inspect exemplar inter-\npolations. Observe that the element \u201cTrump\u201d in an image\nof a \u201cpresident\u201d controls semantic features borrowed from\nthe identity of Donald Trump while removing this element\nresults in a president that resembles Obama. This example di-\nrectly demonstrates an interpolation of exemplars. Features\nfrom both Obama and Trump are employed simultaneously\nin the image generation process. This phenomenon sug-\ngests that diffusion models can also memorize by mixing\ninputs, beyond exact single-sample reconstruction. Further\ninvestigation of exemplar-based representations can be found\nin Appendix E.\n5\nTable 1. Quantitative evaluation of CONCEPTOR and the baselines.\nMethod\nCLIP pairwise\u2191\nLPIPS\u2193\nFID per concept\u2193\nFID entire set\u2193\nToken diversity\nBLIP-2 token\n66.3 \u00b1 16.8\n0.60 \u00b1 0.13\n218.1 \u00b1 93.4\n46.6\n52.1 \u00b1 9.7\nBLIP-2 sentence\n78.7 \u00b1 11.2\n0.57 \u00b1 0.8\n158.6 \u00b1 74.0\n23.7\n65.8 \u00b1 1.9\nPEZ\n79.1 \u00b1 9.9\n0.56 \u00b1 0.06\n150.6 \u00b1 63.4\n18.2\n75.9 \u00b1 1.2\nNMF\n80.0 \u00b1 9.7\n0.53 \u00b1 0.06\n147.0 \u00b1 68.9\n21.6\n\u2014\nk-means\n82.5\u00b1 8.4\n0.53 \u00b1 0.05\n132.5 \u00b1 60.0\n21.3\n\u2014\nPCA\n83.0 \u00b1 8.0\n0.53 \u00b1 0.09\n130.8 \u00b1 52.9\n19.8\n\u2014\nCONCEPTOR\n86.2 \u00b1 8.3\n0.44 \u00b1 0.09\n109.5 \u00b1 51.8\n9.8\n69.8 \u00b1 3.4\n4.2. Quantitative Results\nBaselines\nAs far as we can ascertain, our work is the first\nto tackle concept representations in text-to-image diffusion\nmodels. We, therefore, compare our method with reasonable\nbaselines and adaptations of existing methods. First, we\nconsider a prompt tuning method, Hard Prompts Made Easy\n(PEZ) [45], which aims to learn a prompt that will reproduce\nan input set of training images. Second, we consider two\nbaselines that leverage the state-of-the-art image captioning\nmodel BLIP-2 [25]: (i) BLIP-2 sentence extracts a single\ncaption per concept by decoding the mean CLIP embedding\nof the training images. (ii) BLIP-2 token creates one caption\nper image and constructs a single pseudo-token from the\ncaptions, where each token is weighted by its frequency in\nthe captions. Finally, we consider CNN-based concept inter-\npretability methods. As mentioned in Sec. 2, these methods\nare not directly applicable to diffusion models. Therefore,\nwe present comparisons to the closest adaptations of these\nmethods. We decompose the activation of the UNet features\nfrom different denoising steps into concept activation vectors\n(CAVs) using k-means [16], PCA [49] and NMF [10]. At\ninference, we project the intermediate activations into the\nlearned space, see Appendix A.1 for more details.\nMetrics\nFor each concept, we test the faithfulness and\nthe diversity of the decompositions. We use a test set of\n100 seeds to generate images with wc and with each method.\nThen, we employ three types of metrics: (i) Pairwise Simi-\nlarity, to measure the faithfulness of the decomposition w.r.t.\neach of the concept images. We report the mean CLIP [31]\nimage similarity and the LPIPS [48] score. (ii) Distribution\nsimilarity, to measure the faithfulness of the decomposition\nw.r.t. the concept distribution. We report the FID [19] score\nwith respect to the concept images for each concept sepa-\nrately (FID per concept) and for all concepts as a dataset\n(FID entire set). (iii) We employ SentenceBERT [36] to\nmeasure the element diversity by estimating the dissimilarity\nof the tokens in the decomposition (Token diversity). This\nmetric further substantiates the meaningfulness by showing\nthat the decomposition is diverse.\nResults\nThe results, averaged across all 188 concepts, are\nreported in Tab. 1. As can be seen, our method outperforms\nall baselines across all faithfulness metrics. Notably, our\nLPIPS score is at least 10% lower than that of all baselines,\nindicating that our method is faithful to the concept images.\nImportantly, both FID metrics obtained by our method are\nthe lowest by a big margin (at least 20 per concept and 8\nfor the entire set). The scales of the FID scores are very\ndifferent since the per concept FID was calculated on sets\nof merely 100 images. However, we report both metrics\nfor completeness. Focusing on the more reliable FID score\non the entire set, we calculate the \u201cground-truth\u201d FID be-\ntween the train and test set and obtain a score of 7.1, which\nis fairly close to our score. These results establish CON-\nCEPTOR\u2019s ability to provide faithful decompositions. Con-\nsidering the scores by SentenceBERT, CONCEPTOR is only\nsuperseded by PEZ. However, PEZ produces a significant\namount of uninterpretable tokens (see Tab. 2). The matrix\nfactorization methods do not produce text, therefore we can-\nnot compute the SentenceBERT score. Instead, we enclose\nin Appendix H the top principal components learned for the\nconcepts from Fig. 4. As can be seen, the obtained compo-\nnents do not appear to be coherent or interpretable, i.e., these\nmethods violate the meaningfulness criterion.\nNext, we conduct qualitative comparisons between CON-\nCEPTOR and the leading baselines. Tab. 2 compares the\ntextual decompositions, showing that CONCEPTOR learns di-\nverse and meaningful decompositions. Some concepts, such\nas \u201ca rapper\u201d are dominated by exemplars (e.g. \u201cDrake\u201d,\n\u201cTupac\u201d), while others, such as \u201cMedival village life\u201d, are a\ncomposition of semantically related concepts (e.g., \u201ccara-\nvan\u201d, \u201cpeasant\u201d). In contrast, the baselines either produce\ndecompositions that are not interpretable, i.e., violate the\nmeaningfulness criterion (PEZ), or are oversimplistic (BLIP-\n2). Please refer to Appendix F for word cloud visualizations\nof CONCEPTOR over complex prompts. Fig. 4 presents a\nvisual comparison to the leading baselines given the same\nseeds. As can be observed, CONCEPTOR successfully pre-\nserves the image features (i.e., upholds faithfulness), even\nwhen the concept entails detailed features. For example, the\n\u201cpainter\u201d images demonstrate a reconstruction of the paint-\nings. Conversely, the baseline methods do not accurately\nembody all features of the concept.\nUser study\nTo further substantiate our meaningfulness\nand humans\u2019 ability to understand our decompositions, we\nconduct a user study. In the study, we randomly draw 8\nconcepts, 2 from each of our data categories: (a) profes-\n6\nTable 2. Qualitative comparison of the decompositions by our method and the leading baselines.\nConcept\nPEZ\nBLIP-2 sentence\nCONCEPTOR\nRapper\nmarin, prodigy,\nsturridge, noneeminem\nman, hat, shirt\nTupac, Drake, Khalifa,\nWeekend, Khaled, hood, hat\nMedieval\nvillage life\nmoderated, humpday,\ngiftideas, shistory\npeople,\nmedieval\nclothing, village\ncaravan, peasant, medieval,\ncountryside, farming\nElegance on\na plate\nsilver, chesterfield,\ndinner, moschbizitalk\nplate,\nvegetable,\nmeat\nplating, chef, beauty,\ndessert, saucer, porcelain\nOriginal \nImages\nOurs\nPEZ\nbird\npainter\nhappiness\nmarketplace colors of Marrakech \nPCA\nFigure 4. Feature reconstruction comparison to the leading baselines. For each concept (column) we generate the images using the same\nrandom noise with our method and the leading baselines, and compare to the original concept images generated by Stable Diffusion (Original\nImages).\nsions, (b) abstract concepts, (c) basic concepts (ConceptNet,\nCIFAR-10), and (d) complex concepts. For each concept,\nthe users were presented with 6 random concept images and\n3 possible textual decompositions, as provided by CONCEP-\nTOR, and the two leading baselines that extract text (BLIP-2\nsentence and PEZ). The users were asked to select the de-\ncomposition that best captures all the features presented in\nthe images. Overall, we collected 160 responses for all ques-\ntions. Of those 160 responses, 70% favored the decomposi-\ntion by CONCEPTOR above all alternatives, 26.9% favored\nthe captioning by BLIP-2, and 3.1% selected PEZ, further\nestablishing CONCEPTOR\u2019s meaningfulness, even compared\nto the natural captioning alternative.\nRobustness experiments\nIn Appendix C we conduct ex-\ntensive robustness tests, to demonstrate two important prop-\nerties: (i) CONCEPTOR is robust to different choices of the\ntraining set and initialization, and (ii) CONCEPTOR general-\nizes the training task to test images. Experiment (i) employs\n3 different training sets and initializations and tests the exact\nmatch of tokens between the 3 resulting decompositions.\nThe results demonstrate that the decomposition is consis-\ntent across all sets- 72 \u2212 80% of the top-10 elements, and\n63 \u2212 70% of the top-25 elements are preserved across all\nchoices. Experiment (ii) shows that w\u2217 is able to denoise\ntest concept images from any denoising step, thus w\u2217 indeed\ncaptures the concept features beyond the selection of training\nimages.\nIn conclusion, we find that the baselines either violate the\nmeaningfulness criterion, i.e., produce uninterpretable de-\ncompositions (k-means, PCA, NMF, PEZ, see Tab. 2 and Ap-\npendix H, user study) or the faithfulness criterion (BLIP-2,\nsee poor performance in Tab. 1). We note that for both crite-\nria and in all experiments, CONCEPTOR significantly outper-\nforms all baselines and obtains robust, faithful, and mean-\ningful decompositions (see Tabs. 1 and 2, Fig. 4, and Ap-\npendix C, user study).\n4.2.1\nAblation Study\nWe conduct an ablation study to examine the impact of each\ncomponent on our method. First, we ablate the choice of\nemploying an MLP to learn the coefficients and instead\nlearn them directly. Next, we ablate our sparsity loss and the\nchoice of n = 50. Last, we ablate our choice of vocabulary V\nand instead extract the top 50 tokens by their CLIP similarity\nto the mean image.\nThe results are listed in Tab. 3. Replacing the MLP with\na vector of weights is detrimental to all faithfulness metrics.\nThis demonstrates the importance of our learned MLP, as\nit leverages the rich semantic information learned by CLIP,\nrather than optimizing a huge set of coefficients without any\nsemantic understanding. Without the sparsity loss (Eq. (5)),\nthe top 50 tokens do not necessarily reflect the learned to-\nken w\u2217\nN, and all metrics except for token diversity deterio-\n7\nTable 3. Ablation study of our method, conducted on the professions subset [8].\nMethod\nCLIP pairwise\u2191\nLPIPS\u2193\nFID per concept\u2193\nToken diversity\u2191\nCONCEPTOR\n87.0 \u00b1 5.5\n0.45 \u00b1 0.07\n107.96 \u00b1 31.0\n69.7 \u00b1 3.4\nw/o MLP\n78.0 \u00b1 6.7\n0.55 \u00b1 0.06\n142.88 \u00b1 45.1\n75.9 \u00b1 3.0\nw/o Eq. (5)\n80.3 \u00b1 11.6\n0.52 \u00b1 0.09\n146.4 \u00b1 63.4\n73.2 \u00b1 2.1\nn = 10\n82.9 \u00b1 7.8\n0.49 \u00b1 0.11\n129.41 \u00b1 55.3\n54.6 \u00b1 9.4\nn = 100\n85.6 \u00b1 6.9\n0.47 \u00b1 0.07\n114.36 \u00b1 39.7\n72.8 \u00b1 1.8\nCLIP top words\n80.1 \u00b1 9.9\n0.513 \u00b1 0.1\n130.9 \u00b1 57.2\n66.3 \u00b1 3.9\nTable 4. Decomposition elements obtained by CONCEPTOR that\nreveal potential biases.\nConcept\nDecomposition\nSecretary\nclerk,\nwife,\nwomens,\ngirl,\nladies...\nOpera singer\nobese,\nsoprano,\noverweight,\nfat...\nPastor\nNigerian,\ngospel,\nworship...\nJournalist\nstranger,\nrefugee,\njews,\ntripod...\nDrinking\ncheating,\nmillennials,\nblonde...\nrate. Additionally, observe that the performance decreases\nwhen employing n = 10 since the decomposition is not\nrich enough to represent all features. For n = 100, the re-\nsults are similar to the full method, other than the diversity\nwhich improves a little. This indicates that CONCEPTOR is\nrelatively stable to this parameter. Finally, when only con-\nsidering the top words by CLIP similarity to the images, the\nperformance decreases substantially, supporting the reliance\nof our method on a wide variety of tokens from the vocab-\nulary, beyond the ones most correlated with the images in\nterms of textual semantics.\n4.3. Bias Detection\nAn important capability of our method is bias discov-\nery. Text-to-image models have been shown to represent\nsocial biases [28]. The decompositions obtained by CON-\nCEPTOR can be used to discover such biases by analyzing\nthe decomposition elements. Tab. 4 lists some concepts that\nexpose social insensitivities. Note that our method detects\nbehaviors that are not necessarily observable visually such\nas a connection between \u201cJews\u201d and \u201cjournalists\u201d. These\nfindings substantiate the need to conduct more research on\nconcept representations in text-to-image models, as biases\ncan impact the generation even if they are hard to detect\nvisually. Using our method, users can also choose to gen-\nerate debiased versions of these concepts by decreasing the\ncoefficients corresponding to the biased tokens. More details\non such manipulations can be found in Appendix G.\n5. Discussion and Limitations\nWhile our method provides faithful and interpretable con-\ncept decompositions, there are some limitations to consider.\nFirst, we find that the visual impact of an element is not\nalways completely aligned with its impact as a single token,\ni.e., the impact of each token depends on the context of the\n- \u2018link\u2019\nInput Image\n+\u2018link\u2019\nConcept: bridge\nlink\nSingle Token\n- \u2018coconut\u2019\nConcept: palm tree +\u2018coconut\u2019\ncoconut\nFigure 5. Limitations. Examples of elements that impact the\ngenerated image differently from their single-token meaning. On\nits own, each token generates an image that is completely different\nthan the input image (Single Token). Given the context of the\ndecomposition, the effect of the token changes.\ndecomposition. Fig. 5 demonstrates such cases. For each\nconcept, we visualize the effect of manipulating the token,\nand the result of keeping only the token of interest (Single\nToken). Note that the influence of a token on the generated\nimage differs from its influence as a sole token. For example,\nthe token \u201clink\u201d on its own produces an image of the video\ngame character, Link. However, in the context of a \u201cbridge\u201d,\nit adds a solid bridge (a link) between the two edges of the\nimage.\nSecond, our method is limited to elements that are single\ntokens, therefore a complex phrase (e.g., \u201cpalm tree\u201d) will\nnot be included as a single element in our decomposition.\nHowever, as mentioned, our construction of the linear com-\nbination mitigates the impact of this limitation. Since the\ncontext of the decomposition can change the meaning of an\nelement, complex relations can be formed between single\ntokens by leveraging the context. This is exemplified by our\nability to decompose complex concepts, that require hier-\narchical reasoning (e.g., \u201celegance on a plate\u201d, \u201crainbow\ndewdrops\u201d).\n6. Conclusions\nHow does a generative model perceive the world?\nFocusing on text-to-image diffusion models, we investi-\ngate the model\u2019s internal knowledge of real-world con-\ncepts. We present CONCEPTOR, a method to provide a\n8\nhuman-understandable decomposition for a textual concept.\nThrough extensive experiments, we show that CONCEPTOR\nprovides interpretations that are meaningful, robust, and\nfaithful to the concept representation by the model. Using\nCONCEPTOR, we obtain various interesting observations\non the learned concept representations. Via a per-image\ndecomposition scheme, we observe non-trivial connections\nbetween concepts in ways that transcend the lexical meaning\nof the tokens. Furthermore, our method exposes less intuitive\nbehaviors such as the reliance on exemplars, mixing dual\nmeanings of concepts, or non-trivial biases. In all cases, the\nnovel paradigm allows us to shed new light on a model that,\nsimilar to other foundation models, can still be considered\nan enigma.\nReferences\n[1] Abien Fred Agarap. Deep learning using rectified linear units\n(relu). ArXiv, abs/1803.08375, 2018.\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-\naming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli\nLaine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-\ni: Text-to-image diffusion models with an ensemble of expert\ndenoisers. ArXiv, abs/2211.01324, 2022.\n[3] Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagiel-\nski, Vikash Sehwag, Florian Tram\u00e8r, Borja Balle, Daphne\nIppolito, and Eric Wallace. Extracting training data from\ndiffusion models. ArXiv, abs/2301.13188, 2023.\n[4] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel\nCohen-Or. Attend-and-excite: Attention-based semantic guid-\nance for text-to-image diffusion models, 2023.\n[5] Hila Chefer, Shir Gur, and Lior Wolf. Generic attention-model\nexplainability for interpreting bi-modal and encoder-decoder\ntransformers. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), pages 397\u2013406, Oc-\ntober 2021.\n[6] Hila Chefer, Shir Gur, and Lior Wolf. Transformer inter-\npretability beyond attention visualization. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 782\u2013791, June 2021.\n[7] Ching-Yao Chuang, Varun Jampani, Yuanzhen Li, Antonio\nTorralba, and Stefanie Jegelka. Debiasing vision-language\nmodels via biased prompts. arXiv preprint arXiv:2302.00070,\n2023.\n[8] Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jen-\nnifer Chayes, Christian Borgs, Alexandra Chouldechova,\nSahin Geyik, Krishnaram Kenthapadi, and Adam Tauman\nKalai. Bias in bios: A case study of semantic representation\nbias in a high-stakes setting. 2019.\n[9] Thomas Fel, Victor Boutin, Mazda Moayeri, R\u00e9mi Cad\u00e8ne,\nLouis B\u00e9thune, L\u00e9o And\u00e9ol, Mathieu Chalvidal, and Thomas\nSerre.\nA holistic approach to unifying automatic con-\ncept extraction and concept importance estimation. CoRR,\nabs/2306.07304, 2023.\n[10] Thomas Fel, Agustin Picard, Louis B\u00e9thune, Thibaut Boissin,\nDavid Vigouroux, Julien Colin, R\u2019emi Cadene, and Thomas\nSerre. Craft: Concept recursive activation factorization for\nexplainability. 2023 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 2711\u20132721,\n2022.\n[11] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\nDevi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based\ntext-to-image generation with human priors. arXiv preprint\narXiv:2203.13131, 2022.\n[12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image genera-\ntion using textual inversion. arXiv preprint arXiv:2208.01618,\n2022.\n[13] Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna\nMaterzy\u00b4nska, and David Bau. Unified concept editing in\ndiffusion models. arXiv preprint arXiv:2308.14761, 2023.\n[14] Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir\nGloberson.\nDissecting recall of factual associations\nin auto-regressive language models.\narXiv preprint\narXiv:2304.14767, 2023.\n[15] Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg.\nTransformer feed-forward layers build predictions by promot-\ning concepts in the vocabulary space. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language\nProcessing, pages 30\u201345, Abu Dhabi, United Arab Emirates,\nDec. 2022. Association for Computational Linguistics.\n[16] Amirata Ghorbani, James Wexler, James Y. Zou, and Been\nKim. Towards automatic concept-based explanations. In\nNeural Information Processing Systems, 2019.\n[17] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,\nDimitris Metaxas, and Feng Yang.\nSvdiff: Compact pa-\nrameter space for diffusion fine-tuning.\narXiv preprint\narXiv:2303.11305, 2023.\n[18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022.\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nIn NIPS, 2017.\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020.\n[21] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie J. Cai,\nJames Wexler, Fernanda B. Vi\u00e9gas, and Rory Sayres. Inter-\npretability beyond feature attribution: Quantitative testing\nwith concept activation vectors (tcav). In International Con-\nference on Machine Learning, 2017.\n[22] Alex Krizhevsky. Learning multiple layers of features from\ntiny images. 2009.\n[23] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shecht-\nman, and Jun-Yan Zhu. Multi-concept customization of text-\nto-image diffusion. arXiv preprint arXiv:2212.04488, 2022.\n[24] Belinda Z. Li, Maxwell Nye, and Jacob Andreas. Implicit\nrepresentations of meaning in neural language models. In\nProceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International\n9\nJoint Conference on Natural Language Processing (Volume\n1: Long Papers), pages 1813\u20131827, Online, Aug. 2021. Asso-\nciation for Computational Linguistics.\n[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. ArXiv,\nabs/2301.12597, 2023.\n[26] Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Vi\u00e9gas,\nHanspeter Pfister, and Martin Wattenberg. Emergent world\nrepresentations: Exploring a sequence model trained on a\nsynthetic task. In The Eleventh International Conference on\nLearning Representations, 2023.\n[27] Charles Lovering and Ellie Pavlick. Unit testing for con-\ncepts in neural networks. Transactions of the Association for\nComputational Linguistics, 10:1193\u20131208, 2022.\n[28] Alexandra Sasha Luccioni, Christopher Akiki, Margaret\nMitchell, and Yacine Jernite. Stable bias: Analyzing so-\ncietal representations in diffusion models. arXiv preprint\narXiv:2303.11408, 2023.\n[29] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021.\n[30] Roma Patel and Ellie Pavlick. Mapping language models to\ngrounded conceptual spaces. In International Conference on\nLearning Representations, 2022.\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision.\nIn International Conference on Machine Learning, pages\n8748\u20138763. PMLR, 2021.\n[32] Ori Ram, Liat Bezalel, Adi Zicher, Yonatan Belinkov,\nJonathan Berant, and Amir Globerson. What are you token\nabout? dense retrieval as distributions over the vocabulary.\narXiv preprint arXiv:2212.10380, 2022.\n[33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image genera-\ntion with clip latents. ArXiv, 2022.\n[34] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n[35] Royi Rassin, Shauli Ravfogel, and Yoav Goldberg. Dalle-\n2 is seeing double: Flaws in word-to-concept mapping in\ntext2image models, 2022.\n[36] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence\nembeddings using siamese bert-networks. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing. Association for Computational Linguistics,\n11 2019.\n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022.\n[38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn International Conference on Medical image computing\nand computer-assisted intervention, pages 234\u2013241. Springer,\n2015.\n[39] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven gen-\neration. arXiv preprint arXiv:2208.12242, 2022.\n[40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\nWhang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour,\nBurcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gon-\ntijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet,\nand Mohammad Norouzi. Photorealistic text-to-image dif-\nfusion models with deep language understanding. ArXiv,\nabs/2205.11487, 2022.\n[41] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Conference\non Machine Learning, pages 2256\u20132265. PMLR, 2015.\n[42] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas\nGeiping, and Tom Goldstein. Diffusion art or digital forgery?\ninvestigating data replication in diffusion models.\narXiv\npreprint arXiv:2212.03860, 2022.\n[43] Robyn Speer and Catherine Havasi. Conceptnet 5: A large\nsemantic network for relational knowledge. In The People\u2019s\nWeb Meets NLP, 2013.\n[44] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir\nAberman. P+: Extended textual conditioning in text-to-image\ngeneration, 2023.\n[45] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum,\nJonas Geiping, and Tom Goldstein. Hard prompts made easy:\nGradient-based discrete optimization for prompt tuning and\ndiscovery. ArXiv, abs/2302.03668, 2023.\n[46] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei\nYang, Burcu Karagol Ayan, et al. Scaling autoregressive mod-\nels for content-rich text-to-image generation. arXiv preprint\narXiv:2206.10789, 2022.\n[47] Tian Yun, Usha Bhalla, Ellie Pavlick, and Chen Sun. Do\nvision-language pretrained models learn composable primi-\ntive concepts?, 2023.\n[48] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n586\u2013595, 2018.\n[49] Ruihan Zhang, Prashan Madumal, Tim Miller, Krista A.\nEhinger, and Benjamin I. P. Rubinstein. Invertible concept-\nbased explanations for cnn models with non-negative concept\nactivation vectors. In AAAI Conference on Artificial Intelli-\ngence, 2020.\n10\nA. Implementation Details\nA photo of\na president\nStable Diffusion (SD)\nSD UNet\nSD UNet\nSD UNet\n+\n+\n+\n+\nSD Vocabulary\n(\u201dhouse\u201d)\n(\u201dflower\u201d)\nMLP\nA photo of\na     \nFigure 6. Illustration of the CONCEPTOR method. Given the concept of interest (e.g., \u201ca president\u201d), we generate 100 concept images.\nNext, a learned MLP network maps each word embedding wi to a coefficient f(wi), and the pseudo token w\u2217\nN is constructed as a linear\ncombination of the vocabulary. We then add random noises \u03b51, . . . , \u03b5|T | to the images, and use the model to predict the noise based on the\ntext \u201ca photo of a <w\u2217\nN>\u201d. We train the MLP with the objective of reconstructing the images (Lreconstruction) and add a sparsity loss to\nencourage sparse coefficients (Lsparsity).\nFig. 6 describes the pipeline of our method. Given an input concept (e.g., \u201ca president\u201d), we obtain a set of representative\nconcept images. At each optimization iteration, we add random noises to each of the images individually. Our learned MLP\noperates over the model\u2019s vocabulary to produce a coefficient for each of the word embeddings, and the resulting pseudo-token\nis used to denoise the training images. We employ a reconstruction loss to encourage the pseudo-token to contain the concept\nelements learned by the model and add a sparsity loss to maintain an interpretable and stable decomposition.\nAll of our experiments were conducted using a single A100 GPU with 40GB of memory. We train our MLP as specified\nin Sec. 3 of the main paper with 100 images generated from the concept using seed 1024 for a maximum of 500 training\nsteps with a batch size of 6 (which is the largest batch size that could fit on our GPU). Additionally, we use a learning rate of\n1e \u2212 3 (grid searched on 5 concepts between 1e \u2212 2, 1e \u2212 3, 1e \u2212 4). We conduct validation every 50 optimization steps on 20\nimages with a validation seed and select the iteration with the best CLIP pairwise similarity between the reconstruction and the\nconcept images. We use the latest Stable Diffusion v2.1* text-to-image model employing the pre-trained text encoder from the\nOpenCLIP ViT-H model*, with a fixed guidance scale of 7.5.\nAdditionally, to filter out meaningless tokens such as punctuation marks we consider the vocabulary to be the top 5, 000\ntokens by their CLIP similarity to the mean training image. We note that this filtering method is fairly coarse, and meaningless\ntokens remain in the vocabulary, however, we find that this choice improves the convergence time of our MLP such that 500\niterations are enough to obtain meaningful decompositions. This choice is ablated in the main paper (see the CLIP top words\nablation). Our single-image decomposition scheme employs a CLIP ViT-B/32 model [31]. Please find our code attached as a\nZIP file to reproduce our results.\nA.1. Concept-based baselines implementation details\nTo implement the concept-based baseline approaches, we follow the formulation in [9]. During the generation of the\ntraining concept images, we collect the intermediate representations from the UNet at different denoising timesteps. Then, we\ndecompose those representations using PCA, following [49], NMF, following [10] or k-means, following [16]. We use a total\n*https://github.com/Stability-AI/stablediffusion\n*https://github.com/mlfoundations/open_clip\n11\nof n = 50 components, to match our decomposition size. Finally, at inference, we generate images from the concept using the\ntest seed, 2, while projecting and reconstructing the aforementioned intermediate representations both before passing them\nto the next block and to the upsampling (decoding) part of the UNet (via the skip-connections). At training and inference\nwhen using NMF we drop the negative activations. At inference, when using k-means, we employ the closest cluster as a\nreconstruction of an intermediate representation.\n\u2022 We compute PCA using 5 iterations of randomized SVD solver with 10 oversamples.\n\u2022 We compute k-means using 10 k-means++ initialization and 100 iterations of the Lloyd algorithm.\n\u2022 We compute NMF using NNDSVD initialization and run the Coordinate Descent solver for 200 iterations.\nB. Dataset\nIn the following, we enclose the full list of concepts in each of our data subsets.\nBias in Bios concepts\n\u201cprofessor\u201d, \u201cphysician\u201d, \u201cattorney\u201d, \u201cphotographer\u201d, \u201cjournalist\u201d, \u201cnurse\u201d, \u201cpsychologist\u201d,\n\u201cteacher\u201d, \u201cdentist\u201d, \u201csurgeon\u201d, \u201carchitect\u201d, \u201cpainter\u201d, \u201cmodel\u201d, \u201cpoet\u201d, \u201cfilmmaker\u201d, \u201csoftware engineer\u201d, \u201caccountant\u201d,\n\u201ccomposer\u201d, \u201cdietitian\u201d, \u201ccomedian\u201d, \u201cchiropractor\u201d, \u201cpastor\u201d, \u201cparalegal\u201d, \u201cyoga teacher\u201d, \u201cdj\u201d, \u201cinterior designer\u201d,\n\u201cpersonal trainer\u201d, \u201crapper\u201d\nCIFAR-10 concepts\n\u201cairplane\u201d, \u201cautomobile\u201d, \u201cbird\u201d, \u201ccat\u201d, \u201cdeer\u201d, \u201cdog\u201d, \u201cfrog\u201d, \u201chorse\u201d, \u201cship\u201d, \u201ctruck\u201d\nEmotions\n\u201caffection\u201d, \u201canger\u201d, \u201cdisgust\u201d, \u201cfear\u201d, \u201chappiness\u201d, \u201chonor\u201d,\u201cjoy\u201d, \u201cjustice\u201d, \u201csadness\u201d, \u201cbeauty\u201d\nActions\n\u201cclapping\u201d, \u201cclimbing\u201d, \u201cdrinking\u201d, \u201chugging\u201d,\u201cjumping\u201d, \u201cpouring\u201d, \u201crunning\u201d, \u201csitting\u201d, \u201cthrowing\u201d, \u201cwalk-\ning\u201d\nComplex concepts\n\u201cRainy New York Nights\u201d, \u201cThe Fashion of Abandoned Places\u201d, \u201cRainbow Dewdrops\u201d, \u201cAerial\nAutumn River\u201d, \u201cSkateboarder\u2019s Urban Flight\u201d, \u201cDive into Coral Reefs\u201d, \u201cVintage European Transit\u201d, \u201cStar Trails over\nMountains\u201d, \u201cMarketplace Colors of Marrakesh\u201d, \u201cElegance on a Plate\u201d, \u201cThe Renaissance Astronaut\u201d, \u201cThe Surreal\nFloating Island\u201d, \u201cImpression of Japanese Serenity\u201d, \u201cJazz in Abstract Colors\u201d, \u201cThe Confluence of Pop Art\u201d, \u201cThe Robotic\nBaroque Battle\u201d, \u201cCubist Bustling Market\u201d, \u201cThe Romantic Stormy Voyage\u201d, \u201cThe Botanist in Art Nouveau\u201d, \u201cThe Gothic\nMoonlit Castle\u201d, \u201cNeon-Soaked Cyberpunk City\u201d, \u201cDragon\u2019s Stormy Perch\u201d, \u201cReflections of Earth\u201d, \u201cAfter The Fall\u201d, \u201cRetro\nGaming Nostalgia\u201d, \u201cMedieval Village Life\u201d, \u201cSamurai and the Mystical\u201d, \u201cMinimalistic Geometry\u201d, \u201cAlien Flora and\nFauna\u201d, \u201cThe Inventor\u2019s Steampunk Workshop\u201d\nConceptNet concepts\n\u201ccurling iron\u201d, \u201cbaseball stadium\u201d, \u201cflowers\u201d, \u201csubmarine\u201d, \u201cpoliceman\u201d, \u201cprojectile\u201d, \u201ctissue\nholder\u201d, \u201cjogging\u201d, \u201cstorey\u201d, \u201csickness\u201d, \u201cparlor\u201d, \u201cships\u201d, \u201cconductor\u201d, \u201cbooze\u201d, \u201ckey\u201d, \u201cmetal\u201d, \u201cprostitute\u201d, \u201cwings\u201d,\n\u201ctools\u201d, \u201croad\u201d, \u201cmain\u201d, \u201cleader\u201d, \u201cradio\u201d, \u201csurprise\u201d, \u201cchips\u201d, \u201ccastle\u201d, \u201cbathroom\u201d, \u201ccompete against\u201d, \u201cleather\u201d,\n\u201cscience\u201d, \u201crich\u201d, \u201csponge\u201d, \u201cbell\u201d, \u201celoquent\u201d, \u201cnightclub\u201d, \u201cwater\u201d, \u201cpatient\u201d, \u201ceat vegetables\u201d, \u201crespect\u201d, \u201clemur\u201d,\n\u201cbum\u201d, \u201cmammoth\u201d, \u201cbirthday\u201d, \u201cchain\u201d, \u201ccats\u201d, \u201cfrogs\u201d, \u201carkansas\u201d, \u201cbasketball\u201d, \u201clistening\u201d, \u201cdream\u201d, \u201cticket office\u201d,\n\u201cfailure\u201d, \u201ctext\u201d, \u201cnow\u201d, \u201coven\u201d, \u201cleg\u201d, \u201cmundane\u201d, \u201ccopulate\u201d, \u201ctree\u201d, \u201cwood\u201d, \u201cmail\u201d, \u201cwooden rod\u201d, \u201cclippers\u201d,\n\u201ccompeting against\u201d, \u201cdull\u201d, \u201cbook\u201d, \u201cwatch television\u201d, \u201cwinning baseball game\u201d, \u201ciphone\u201d, \u201cdance club\u201d, \u201csecurity\u201d,\n\u201cpolitician\u201d, \u201csubway station\u201d, \u201cfall\u201d, \u201cjunk\u201d, \u201csleighing ride\u201d, \u201ccall\u201d, \u201cmosquitoes\u201d, \u201cnational highway\u201d, \u201ccontraceptive\ndevice\u201d, \u201cstatement\u201d, \u201ckill\u201d, \u201cseeing old things\u201d, \u201clift\u201d, \u201cadults\u201d, \u201cpillowcase\u201d, \u201cwedding ring\u201d, \u201ceyes\u201d, \u201ccountry\u201d,\n\u201cstepladder\u201d, \u201cmandolin\u201d, \u201creception area\u201d, \u201cchief\u201d, \u201cplastic\u201d, \u201cprojector\u201d, \u201chub\u201d, \u201ccard catalog\u201d, \u201cnegligible\u201d, \u201crook\u201d,\n\u201cllano estacado\u201d\nC. Robustness Experiments\nIn the following sections, we conduct experiments to demonstrate our method\u2019s ability to provide robust concept interpreta-\ntions. First, we show that the obtained decomposition is stable, i.e., that the same elements are learned across different choices\nof training sets and initialization. Second, we test our decomposition\u2019s ability to generalize w.r.t. the denoising training task,\ni.e., we test the denoising capabilities of w\u2217 on test images. This shows that the elements in w\u2217 represent the entire concept,\nbeyond the training images.\n12\nC.1. Robustness to Training Data\nNote that since N >> d, there are many linear combinations that yield w\u2217. However, due to the specific MLP-based\nstructure and the sparsity constraints, the decomposition is stable across multiple draws of training sets. In this section, we aim\nto verify that empirically. For each concept, we generate 2 alternative training sets with different random seeds, in addition to\nour original training set, to test the consistency of our results. For each alternative training set, we decompose the concept\nusing our method as described in Sec. 3, with a different initialization for the MLP*. This process results in 3 decompositions\nof n = 50 tokens for each concept.\nWe then analyze the intersection of the top k = 10, 25, and 50 tokens between the original decomposition and each of\nthe alternative decompositions. The concept intersection score for k is defined to be the average of the intersections with\nthe two alternative sets. In other words, we calculate two intersection sizes for k: between the top k tokens of the original\ndecomposition and the first alternative decomposition, and between the top k tokens of the original decomposition and the\nsecond alternative. The overall concept intersection score for k is the average of the two. Standard Deviation is computed\nacross the concepts. Note that this experiment measures an exact match of the tokens, therefore the actual intersection may be\neven higher, e.g., if a synonym is used in one of the alternatives.\nTable 5. Decomposition consistency experiment. For each number of tokens (k = 10, 25, 50) we test the intersection between our learned\ntop tokens and those learned by employing two different training sets of concept images, with different random initializations. The results\ndemonstrate that the top tokens are consistent and robust across different training sets and seeds.\nNo. of Tokens\nIntersection\nConcrete\nTop 10\n8.03 (80.3%) \u00b1 2.07\nTop 25\n17.68 (70.7%) \u00b1 4.47\nTop 50\n28.96 (57.9%) \u00b1 8.05\nAbstract\nTop 10\n7.20 (72.0%) \u00b1 1.86\nTop 25\n15.95 (63.8%) \u00b1 3.97\nTop 50\n25.65 (51.3%) \u00b1 5.41\nThe average intersection scores across all concrete concepts and all abstract concepts are presented in Tab. 5. As can\nbe seen, for the concrete concepts, an average of 8.03(80.3%) of the top 10 tokens are present in all the decompositions,\neven when considering an entirely different training set, indicating that the top tokens obtained by our method are stable.\nAdditionally, when considering the top 25 tokens, an average of 17.68(70.7%) of the tokens are present in all decompositions,\nwhich is a large majority. We note that the bottom tokens are typically less influential on the decomposition, as they are\nassigned relatively low coefficients by the MLP. Accordingly, when considering all 50 tokens, an average of 28.96(57.9%) of\nthe tokens appears in all decompositions. The results for the abstract concepts are slightly lower, yet demonstrate a similar\nbehavior. Overall, the results demonstrate that our method is relatively robust to different training sets and random seeds, such\nthat even in the face of such changes, the top-ranked tokens remain in the decomposition.\nC.2. Denoising Generalization\nWe wish to ascertain that the learned decomposition w\u2217 can generalize its training task to test images. Recall that during\ntraining, w\u2217 accumulated the elements necessary to denoise the concept images for each denoising step. Next, we wish to\ndemonstrate that these features can perform this same training task on test images. We compare the denoising quality with\nw\u2217 to that of the concept prompt token(s), wc. This comparison serves two purposes: (1) establish CONCEPTOR\u2019s ability to\nlearn the actual concept features, beyond a set of features that simply reconstruct the training set, (2) motivate the fact that,\nas can be seen in our results, w\u2217 \u0338= wc. To intuitively motivate the latter, note that even though wc was used to generate the\nconcept images, it is not necessarily the best prompt to denoise them, since: (1) wc generates each image using a specific\ninitial random noise, but is not guaranteed to be better in denoising them after applying other random noises. (2) Unlike wc,\nw\u2217 is constructed as a linear combination of tokens. Thus, our optimization is performed over a larger, continuous space of\nembeddings, and therefore is more expressive than a simple selection of tokens.\nWe begin with a quantitative comparison. We sample a test set of 100 images for each concept in the dataset. Then, for\neach denoising step t \u2208 {1, . . . , T} and each test image, we draw a random noise and apply it as in Eq. 2. Finally, we test the\n*Our code does not employ a fixed random seed, thus each run implies a different random initialization for the MLP and a different set of random noises\nand timesteps for the training images.\n13\n0\n200\n400\n600\n800\nStep\n1.6\n1.4\n1.2\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n1e3*MSE\nConcept token\nOurs\nOptimized Token\n(a)\n(b)\nFigure 7. Generalization tests comparing the concept prompt, wc, and our pseudo-token, w\u2217. (a) Quantitative test on all concepts. For each\ntimestep, we add random noises to the images and compare the reconstruction with w\u2217, wc, and wo, a continuous token optimized with the\nsame objective as w\u2217 (Optimized Token). We report the MSE after subtracting the score of a random token. (b) Qualitative comparison. An\nimage I is generated from the concept \u201ca nurse\u201d, and different noises are added \u03b51, . . . , \u03b55 (1st row). We then compare the denoising with\nwc (2nd row) and with w\u2217 (3rd row).\nreconstruction loss specified in Eq. 1 with the pseudo-token w\u2217 compared to the concept prompt wc. To provide a lower bound\non the obtainable error, we additionally compare to wo, a vector optimized with the same reconstruction objective on the entire\ncontinuous embedding space Rd without restrictions, similar to [12]. Note that, unlike w\u2217, wo does not offer interpretable\ninformation, since it is non-decomposable, and often out-of-distribution (see Sec. 2). However, it demonstrates our claim that\noptimization over a larger domain yields better reconstruction. Additionally, we observe that there is a large variance in the\nMSE score across timesteps. Latents in early steps are very noisy, and therefore obtain a very high loss (\u223c 0.8), while the last\nsteps contain virtually no noise, and the MSE is very low (\u223c 1e\u22123). Therefore, we compute a baseline score to normalize\nthe scale. We subtract from each score the denoising score for the same images using a random token which serves as an\nupper bound for the MSE. Fig. 7(a) presents the results averaged across all concepts, showing that the concept wc obtains\na score worse than both w\u2217 and the optimized token wo, which obtains the best results. These differences are statistically\nsignificant, as shown by the error bars marked on every timestep. Evidently, by optimizing a token over a larger domain, we\ncan outperform the original concept token in the denoising task. Importantly, this motivates the different features learned by\nw\u2217 while also demonstrating the remarkable denoising capability of w\u2217 on any concept image, which indicates that w\u2217 indeed\ncaptures the concept features.\nFig. 7(b) provides a qualitative comparison between wc and w\u2217. An input image I generated by \u201ca photo of a nurse\u201d is\nnoised and then denoised back from different denoising steps, using the concept token wc and our pseudo-token w\u2217. As can be\nseen, there are cases where, given a different random seed, wc does not preserve the features in the original image I (e.g., it\nadds hats, face masks, and black and white effects), while w\u2217 does. Intuitively, this can be attributed to the rich representation\nlearned by w\u2217, which can include both semantic and style features. Both experiments motivate the diversity of the learned\ndecomposition. Since wc is not necessarily optimal for Eq. 1, w\u2217 learns additional features to improve the denoising quality.\nThus, w\u2217 balances two objectives\u2013 interpretability and faithfulness to the model\u2019s internal representations.\nD. Single-image Decomposition\nIn this section, we provide additional examples of single-image decompositions obtained by our method, as described\nin Sec. 3 of the main paper. Fig. 8 presents the obtained decompositions for images generated by Stable Diffusion for a given\nconcept. As can be seen, the phenomena demonstrated in the main paper are reproduced in the presented decompositions. For\nexample, \u201csweet peppers\u201d borrow the appearance from the \u201cpepper\u201d and the shape from the \u201cfingers\u201d, the \u201cfrog\u201d borrows\nthe overall appearance of a \u201ctoad\u201d with the color of \u201cKermit\u201d, etc. Other concepts are constructed as a composition of related\nelements, for example, \u201chappiness\u201d is constructed as \u201cchildren\u201d who are \u201claughing\u201d, and a \u201clamp\u201d is a \u201cchandelier\u201d with a\n\u201cbulb\u201d in it.\n14\ndj\nnightclub\nturntable\n+\n=\nbee\nwasp\nhoney\n+\n=\ncorn\nmaize\ncomb\n+\n=\nsweet peppers\nfingers\npepper\n+\n=\noak\nstag\nsequoia\n+\n=\nwardrobe\npantry\nrobe\n+\n=\nsnake\nhose\ngecko\n+\n=\ndragon\u2019s stormy perch\nPegasus\nGodzilla\n+\n=\nhappiness\nchildren\nlaugh\n+\n=\nchair\nstool\ncello\n+\n=\ncamel\nostrich\ncashmere\n+\n=\nlamp\nbulb\nchandelier\n+\n=\nfrog\ntoad\nKermit\n+\n=\npalm tree\ncoconut\nmahogany\n+\n=\nFigure 8. Decompositions of single images by CONCEPTOR. Each of the examples depicts an image generated by Stable Diffusion for the\nconcept, and its corresponding decomposition.\n15\nD.1. Reliance on Renowned Artistic Styles\nAs demonstrated in Figs. 1 and 2, we find that the representation of the concept \u201cpainter\u201d relies on the renowned artistic\nstyles of famous artists. In this section, we further verify this observation through the lens of our single-image decomposition\nscheme. We generate 100 test images for the concept \u201cpainter\u201d and apply our single-image decomposition scheme to all\nimages. Out of the tested images, we found that 67 images contain at least one name of a famous artist in their decomposition.\nThis result empirically demonstrates the reliance on existing artistic styles, even when the prompt does not specify the artist\u2019s\nname explicitly. Fig. 9 demonstrates the impact of the experiment described above on the generated images. We observe that\nremoving the names of renowned artists modifies the painting generated in the image entirely (top row) or, in some cases,\nremoves it from the image altogether (bottom row).\nw/o Renowned Painters\nOriginal Images\nFigure 9. Generated images for the concept \u201cpainter\u201d before and after removing the names of famous painters from the single-image\ndecomposition. As can be observed, this removal results in a significant modification to the painting in the image.\nE. Representation by Exemplars\nTab. 6 and Fig. 10 present examples of concepts that rely on famous instances for their representations. For example, \u201cmovie\nstar\u201d is represented by names of famous actors such as Marilyn Monroe or Lucille Ball. Similarly, the concept \u201cpresident\u201d is\ndominated by American presidents such as Obama and Biden, and the concept \u201cbasketball player\u201d relies on famous players\nsuch as Kobe Bryant and LeBron James. To demonstrate the reliance on the exemplars, Fig. 10 shows the reconstruction by\nour method with and without the famous instance names from Tab. 6. As can be observed, the reconstruction quality heavily\nrelies on the identities of the instances, and when we remove those instances the reconstruction is harmed significantly.\nTable 6. Exemplar-based decomposition elements obtained by CONCEPTOR.\nConcept\nCONCEPTOR\nComposer\nSchubert, Beethoven, Chopin, Mozart, Brahms, Wagner\nMovie Star\nAubrey, Bourne, Lucille, Gloria, Marilyn, Monroe, Oswald\nPresident\nObama, Trump, Biden, Nixon, Lincoln, Clinton, Washington\nRapper\nTupac, Drake, Khalifa, Weekend, Khaled, Eminem, Wayne\nBasketball player\nKobe, LeBron, Shaq, Bryant, Jordan, Donovan, Kyrie\n16\nOriginal \nImages\nOurs\nw/o \nExemplars\nmovie star\ncomposer\nOriginal \nImages\nOurs\nw/o \nExemplars\nrapper\npresident\nFigure 10. Examples of concepts that rely on famous instances. When removing the exemplars, the reconstruction quality is significantly\nharmed.\n17\nF. Decomposition of Complex Concepts\nIn Fig. 11 we present the decomposition results for the first 12 prompts from our complex prompts subset.\nConcept:\nrainy New York nights\nConcept:\nfashion of abandoned places\nConcept:\nrainbow dewdrops\nConcept:\naerial autumn river\nConcept:\nskateboarder\u2019s urban flight\nConcept:\ndive into coral reefs\nConcept:\nvintage European transit\nConcept:\nstar trails over mountains\nConcept:\nmarketplace colors of Marrakech\nConcept:\nelegance on a plate\nConcept:\nthe Renaissance astronaut\nConcept:\nthe surreal floating island\nFigure 11. Decompositions obtained by CONCEPTOR for the complex concepts from our dataset.\n18\nG. Concept Debiasing\nAs mentioned in the main paper, CONCEPTOR is capable of detecting biases that are otherwise difficult to capture by simply\nobserving the generated images. After detecting a bias, one could employ a coefficient manipulation with our method, or opt to\nemploy other methods for concept editing such as [13], based on the information provided by CONCEPTOR. Fig. 12 presents\nexamples of concept debiasing by employing coefficient manipulation with CONCEPTOR. For each concept (row), we decrease\nthe coefficient of the biased tokens in the decomposition until an unbiased representation is achieved. Fig. 12 demonstrates\na comparison of the generated images by SD without intervention (Original Images), and the images reconstructed by our\nmethod after reducing the bias coefficients, on the same set of 8 random seeds. As can be observed, our method is able to\nmitigate the different biases while maintaining the other features represented by the concept.\nprofessor\nOriginal Images\nDebiased\nnurse\nsecretary\nopera singer\nFigure 12. Examples of concept debiasing using CONCEPTOR with the same set of 8 random seeds. The first three examples demonstrate\ngender debiasing, while the last one demonstrates a decoupling between \u201copera singer\u201d and obesity.\n19\nH. Principal Component Visualization for Baselines\nIn this section, we enclose the concept activation vectors (CAVs) learned by the leading concept-based explainability\nmethod adapted from CNNs (PCA). Fig. 13 presents the reconstruction and components learned by PCA. To visualize a\ncomponent, we remove it and generate the corresponding images without it to examine its impact on the generations. As can\nbe seen, most of the learned components are not easily interpretable and do not demonstrate a coherent semantic change across\nthe different test images. Thus, these methods violate the meaningfulness criterion. We note that meaningfulness is a critical\ncriterion for an interpretation, as without it, humans cannot understand the decomposition, and it does not serve its basic goal\nof giving insights into the model.\nOriginal \nImages\nPCA\nComponent \n1\nComponent\n2\nComponent\n3\nComponent\n4\nComponent\n5\nBird\nPainter\nOriginal \nImages\nPCA\nComponent \n1\nComponent\n2\nComponent\n3\nComponent\n4\nComponent\n5\nMarketplace colors of Marrakesh\nHappiness\nFigure 13. PCA top 5 extracted principal components for 4 concepts. The first row depicts SD\u2019s original images, the second row shows the\nreconstruction by PCA, and the last 5 rows demonstrate the impact of removing each of the top 5 principal components learned by PCA.\n20\n"
  },
  {
    "title": "Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance",
    "link": "https://arxiv.org/pdf/2306.00943.pdf",
    "upvote": "3",
    "text": "Make-Your-Video:\nCustomized Video Generation Using Textual and Structural Guidance\nJinbo Xing1 Menghan Xia2,\u2217 Yuxin Liu1 Yuechen Zhang1 Yong Zhang2 Yingqing He3 Hanyuan Liu1\nHaoxin Chen2 Xiaodong Cun2 Xintao Wang2 Ying Shan2 Tien-Tsin Wong1\n1CUHK\n2Tencent AI Lab\n3HKUST\n\u2217Corresponding Author.\nProject page:\nhttps://doubiiu.github.io/projects/Make-Your-Video\n\u201cA stone door and a stone are placed outdoors, near a rivulet in the forest, photorealistic\u201d\n\u201cA robot bird walking in technology exhibition hall, cyberpunk\u201d\n\u201cA whale carrying an ancient Chinese palace flying in the sky, Shinkai Makoto animation\u201d\nFigure 1. Given the text descriptions and motion structure as guidance, our model can generate temporally coherent videos adhering to the\nguidance intentions. By building structural guidance from distinct sources, we show the video generation results in different applications,\nincluding (top) real-life scene setup to video, (middle) dynamic 3D scene modeling to video, and (bottom) video re-rendering.\nAbstract\nCreating a vivid video from the event or scenario in our\nimagination is a truly fascinating experience. Recent ad-\nvancements in text-to-video synthesis have unveiled the po-\ntential to achieve this with prompts only. While text is con-\nvenient in conveying the overall scene context, it may be\ninsufficient to control precisely. In this paper, we explore\ncustomized video generation by utilizing text as context de-\nscription and motion structure (e.g. frame-wise depth) as\nconcrete guidance. Our method, dubbed Make-Your-Video,\ninvolves joint-conditional video generation using a Latent\nDiffusion Model that is pre-trained for still image synthesis\nand then promoted for video generation with the introduc-\ntion of temporal modules. This two-stage learning scheme\nnot only reduces the computing resources required, but also\nimproves the performance by transferring the rich concepts\navailable in image datasets solely into video generation.\nMoreover, we use a simple yet effective causal attention\nmask strategy to enable longer video synthesis, which miti-\ngates the potential quality degradation effectively. Exper-\nimental results show the superiority of our method over\nexisting baselines, particularly in terms of temporal co-\nherence and fidelity to users\u2019 guidance. In addition, our\nmodel enables several intriguing applications that demon-\nstrate potential for practical usage.\n1\narXiv:2306.00943v1  [cs.CV]  1 Jun 2023\n1. Introduction\nAs a widely embraced digital medium, videos are highly\nregarded for their ability to deliver vibrant and immersive\nvisual experiences. Capturing real-world events in video\nhas become effortless with the widespread availability of\nsmartphones and digital cameras. However, when it comes\nto creating a video to express the idea aesthetically, the\nprocess becomes considerably more challenging and costly,\nwhich usually requires professional expertise in computer\ngraphics, modeling, and animation production. Fortunately,\nrecent advancements in text-to-video [11, 41] shed light on\nthe possibility of simplifying this process as textual prompts\nalone. Although text is recognized as a standard and ver-\nsatile description tool, we argue that it excels primarily in\nconveying abstract global context, while it may be less ef-\nfective in providing precise and detailed control. This moti-\nvates us to explore customized video generation by utilizing\ntext as context description and motion structure as concrete\nguidance.\nSpecifically, we choose frame-wise depth maps to repre-\nsent the motion structure, as they are 3D-aware 2D data that\nalign effectively with the task of video generation. In our\napproach, the structural guidance can be quite rough, so as\nto allow non-professionals to easily prepare it. This design\noffers the flexibility for the generative model to produce\nplausible content without depending on intricately crafted\ninput.\nFor instance, a scene setup using office common\nitems can be used to guide the generation of a photorealistic\noutdoor landscape (Figure 1(top)). With 3D modeling soft-\nware, the physical objects can be replaced with simple geo-\nmetric elements or any accessible 3D assets (Figure 1(mid-\ndle)). Naturally, another alternative is to make use of the es-\ntimated depth from existing videos (Figure 1(bottom)). So,\nthe combination of textual and structural guidance provides\nusers with both flexibility and controllability to customize\ntheir videos as intended.\nTo achieve this, we formulate the conditional video\ngeneration using a Latent Diffusion Model (LDM) [36]\nthat adopts a diffusion model in a compressed lower-\ndimensional latent space to reduce the computational ex-\npenses. For training an open-world video generation model,\nwe propose to separate the training of spatial modules (for\nimage synthesis) and temporal modules (for temporal co-\nherence). This design is based on two primary considera-\ntions: (i) training the model components separately eases\nthe computational resource requirements, which is espe-\ncially crucial for resource-intensive tasks; (ii) as image\ndatasets encompass a much enriched variety of concepts\nthan the existing video datasets, hence, pre-training the\nmodel for image synthesis helps to inherit the diverse vi-\nsual concepts and transfers them to video generation. A\nmajor challenge is to achieve temporal coherence. Specifi-\ncally, using a pre-trained image LDM, we maintain them as\nthe frozen spatial blocks and introduce the temporal blocks\ndedicated to learning inter-frame coherence over the video\ndataset. Notably, we combine the temporal convolutions\nwith the additional spatial convolutions, which improves\nthe temporal stability by increasing the adaptability to the\npre-trained modules. We also adopt a simple yet effective\ncausal attention mask strategy to enable longer (i.e., 4\u00d7 the\ntraining length) video synthesis and it mitigates the poten-\ntial quality degradation significantly.\nBoth qualitative and quantitative evaluations evidence\nthe superiority of our proposed method over existing base-\nlines, particularly in terms of temporal coherence and fi-\ndelity to users\u2019 guidance. Ablation studies confirm the ef-\nfectiveness of our proposed designs, which play a crucial\nrole in the performance of our method. Additionally, we\nshowcased several intriguing applications facilitated by our\napproach, and the results indicate the potential for practical\nscenarios.\nOur contributions are summarized as below:\n\u2022 We present an efficient approach for customized video\ngeneration by introducing textual and structural guid-\nance. Our method achieves top performance in con-\ntrollable text-to-video generation both quantitatively\nand qualitatively.\n\u2022 We propose a mechanism to leverage pre-trained im-\nage LDMs for video generation, which inherits the rich\nvisual concepts while achieving a decent temporal co-\nherence.\n\u2022 We introduce a temporal masking mechanism to al-\nlow longer video synthesis while alleviating the quality\ndegradation.\n2. Related Work\n2.1. Diffusion Models for Text-to-Image (T2I) Syn-\nthesis\nDiffusion models [12, 42, 44] (DMs) have recently\nshown unprecedented generative semantic and composi-\ntional power, attracting attention from both academia and\nindustry. By absorbing the text embedding from CLIP [31]\nor T5 [32], they have been successfully adopted for text-to-\nimage synthesis [2,7,28,33,36,38]. GLIDE [28] introduces\nclassifier-free guidance [13] to the text-conditioned image\nsynthesis DMs, improving image quality in both photore-\nalism and text-image alignment, which are further boosted\nby using CLIP [31] feature space in DALL\u00b7E 2. Moreover,\na line of works [20, 27, 51\u201353] improves the controllabil-\nity of T2I by introducing additional conditional inputs, e.g.,\npose, depth, and normal map.\nSince DMs generally re-\nquire iterative denoising processes through a large U-Net,\nthe training becomes computationally expensive.\nTo ad-\n2\ndress this, cascaded (Imagen [38]) and latent diffusion mod-\nels (LDMs [36]) have been proposed. Specifically, Imagen\nadopts cascaded diffusion models in pixel space to gener-\nate high-definition videos, while LDMs first compress the\nimage data using an autoencoder and learn the DMs on the\nresultant latent space to improve efficiency. To inherit the\ndiverse visual concepts and reduce the training cost, our\nmethod builds upon LDMs by introducing video awareness\nthrough additional learnable temporal modules into the ar-\nchitecture and training on text-video data, while keeping the\noriginal weights of LDMs frozen.\n2.2. Diffusion Models for Text-to-Video (T2V) Syn-\nthesis\nAlthough there have been significant advancements in\nT2I, text-to-video generation (T2V) is still lagging behind\ndue to the scarcity of large-scale high-quality paired text-\nvideo data, the inherent complexity of modeling tempo-\nral consistency, and resource-intensive training. As a pi-\noneering work, Video Diffusion Model [14] models low-\nresolution videos with DMs using a space-time factor-\nized U-Net in pixel space and trains jointly on image and\nvideo data.\nTo generate high-definition videos, Imagen-\nVideo [11] proposes effective cascaded diffusion models\nand the v-prediction parameterization method.\nTo reduce the training cost, many subsequent stud-\nies, including Make-A-Video [41], MagicVideo [54] and\nLVDM [9], transfer T2I knowledge to T2V generation by\ninitiating from pre-trained T2I models and fine-tuning the\nentire model. Differently, Khachatryan et al. [19] present\na training-free transfer by leveraging pre-trained T2I mod-\nels with manual pseudo motion dynamics to generate short\nvideos.\nHowever, the generated videos suffer from low\nquality and inconsistency. Besides adopting the pre-trained\nT2I, Gen-1 [6] and FollowYourPose [23] propose to control\nthe structure and motion dynamics of synthesized videos by\ndepth and pose, respectively. With the same conditions as\nus, Gen-1, however, trains the entire model, which can be\nboth time- and resource-consuming, potentially leading to a\ndegradation of the inherited rich visual concepts. As a con-\ncurrent work, Video-LDM [4] shares a similar motivation\nwith ours, i.e., extending the image LDMs to video genera-\ntors by introducing temporal layers and keeping the original\nweights frozen. However, solely using temporal layers may\nnot be sufficient for adapting LDMs to video generators.\nAs for longer video synthesis, interpolation [4,9,41,54]\nand prediction [4,8] strategies are commonly adopted in ex-\nisting diffusion-based T2V approaches. Unlike prediction,\ninterpolation does not increase the physical time span of\nsynthesized videos but only makes enhance their smooth-\nness. However, the current prediction mechanism in video\ndiffusion models is still limited in the curated domain, e.g.,\ndriving [4] and indoor scene [8]. Our work aims to effi-\nciently and effectively adapt a pre-trained T2I model to a\njoint text-structure-guided video generator, and investigate\ngeneral video prediction mechanisms for longer video syn-\nthesis.\n2.3. Text-driven Video Editing\nIn recent studies, DMs have demonstrated their efficacy\nin image editing tasks, as evidenced by several works [5,\n10, 18, 24, 46]. However, their application to video edit-\ning on individual frames can result in temporal inconsis-\ntency issues.\nTo address this, Text2LIVE [3] combines\nLayered Neural Atlases [17] and the proposed text-driven\nimage editing method, allowing texture-based video edit-\ning but struggling to accurately reflect the intended edits.\nTo improve the video quality, recent diffusion-based video\nediting methods rely on either pre-trained large-scale video\ndiffusion models [26], which are usually inaccessible and\nhard to reproduce due to the unaffordable training, or the\ninversion [25,43] followed by attention manipulation mech-\nanism [21, 30, 49] using pre-trained T2I model, rendering\ntricky prompt engineering or manual hyper-parameter tun-\ning process. Although both our conditional video generator\nand video editing methods can edit video content, a notable\ndistinction is their reliance on the original video (e.g., for in-\nversion purposes), whereas our model does not necessitate\nthe source video as input.\n3. Method\nThe goal of this work is to study controllable text-to-\nvideo synthesis so that the generated video could align with\nthe users\u2019 intention faithfully.\nTo achieve this, we pro-\npose a conditional video generation model that takes text\nprompts and frame-wise depths as conditional input. The\ntext prompt describes the video appearance and depth se-\nquence specifies the overall motion structure.\nIn the following, we start with a brief introduction to\nDMs and LDMs (Section 3.1), as the preliminary knowl-\nedge to our proposed controllable text-to-video generation\nmodel (Section 3.2).\nAt last, we discuss on a temporal\nmasking mechanism to facilitate longer video generation\n(Section 3.3).\n3.1. Preliminaries\nDiffusion models (DMs) are probabilistic models de-\nsigned to learn a target data distribution pdata(x) by grad-\nually denoising a normally distributed variable. This de-\nnoising process corresponds to learning the reverse process\nof a fixed Markov Chain of length T with denoising score\nmatching [16,22,44]. The most successful models in the im-\nage synthesis field rely on a reweighted variant of the vari-\national lower bound on pdata(x). These models can be in-\nterpreted as an equally weighted sequence of denoising au-\ntoencoders \u03f5\u03b8(xt, t), parameterized with learnable parame-\n3\nDenoising \nU-Net\n\ud835\udc31\nDepth \nestimator\n\u2130\nDiffusion\nSpatio-Temporal Residual Block\nSpatial Layer \n(LDM)\nPseudo\n3D Conv\nSelf-Attn\nwith CAM\nFeed \nForward\nTemporal Transformer\n\ud835\udc330\n\ud835\udc33\ud835\udc61\n\ud835\udf07\ud835\udf03,\ud835\udf19(\ud835\udc33\ud835\udc61, \ud835\udc61)\nText\nText encoder\nSpatio-Temporal Residual Block\nTemporal Transformer\nDenoising \nU-Net\n\ud835\udc9f\n\ud835\udca9\n\ud835\udc33\ud835\udc47\n\ud835\udc330\n\u201cA camel walks on the  snow field, anime style\u201d\nDDIM steps\nInference\n~\nTraining\nText encoder\nVideo\nDynamic scene modeling\nManual setup of real-life scene\n\u2026\nFrozen weights\nFigure 2. Flowchart of the proposed method. During training (left), the input video x is first encoded into latent feature z0 with a fixed\npre-trained encoder E and diffused to zt. Meanwhile, the depth sequence will be extracted with the off-the-shelf depth estimator MiDas\nand concatenated with zt, and the text is encoded by a frozen OpenCLIP text encoder. Then the model learns to reverse the diffusion\nprocess conditioned on the depth and text prompt. As for inference (right), videos can be generated by recurrently denoising a random\ntensor sampled from normal distribution, under the guidance of text prompt and frame-wise depth obtained in multiple ways.\nters \u03b8; t = 1 . . . T, which are trained to predict a denoised\nvariant of their input xt, where xt is a noisy version of the\ninput x \u223c pdata. The corresponding denoising objective is\nEx\u223cpdata,\u03f5\u223cN (0,I),t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(xt; c, t)\u22252\n2\n\u0003\n,\n(1)\nwhere c is optional conditioning information (e.g.\ntext\nprompt), t denotes a timestep uniformly sampled from\n{1, . . . , T}, and \u03f5 is the noise tensor used during the diffu-\nsion from x0 to xt. The neural backbone implementation of\n\u03f5\u03b8(\u25e6; c, t) is generally a 2D U-Net [37] with cross-attention\nconditioning mechanisms.\nLatent diffusion models (LDMs) [36] are proposed to\nimprove the computational and memory efficiency over a\nlearned compact latent space instead of the original pixel\nspace. It is realized through perceptual compression with\nan autoencoder E and D for efficient and spatially lower-\ndimensional feature representations. The autoencoder is de-\nfined to reconstruct inputs x, such that \u02c6x = D(E(x)) \u2248 x.\nA DM can then be trained in the compressed latent space\nand turned into a LDM. The corresponding objective is sim-\nilar to Eq. 1, except for replacing x with its latent represen-\ntaton z = E(x).\n3.2. Adapting LDMs for Conditional T2V Genera-\ntion\nWe employ an LDM to formulate our conditional video\ngeneration task, which involves synthesizing content for\neach frame while maintaining their temporal coherence to\nproduce plausible dynamics.\nOur key insight is to har-\nness the power of pre-trained conditional T2I LDMs as a\nlanguage-visual generative prior for conditional video syn-\nthesis. The main challenges are two-fold: (i) although the\nNormal distribution\nSDE trajectories\n\u201cAn Origami boat \nmoving on the sea\u201d\nFigure 3. Concept depiction of the effect of temporal modules \u03f5\u03d5.\nGiven a batch of noises, \u03f5\u03d5 pushes the SDE trajectories toward\ntarget data distributions with cross-frame coherence. We draw a\nsingle trajectory for a batch for conciseness.\npre-trained image LDM can synthesize high-quality indi-\nvidual frames, it is not ready for generating temporally\nconsistent video frames; and (ii) there is a shortage of\nlarge-scale text-video datasets with rich concept coverage\nand high quality when compared to image datasets like\nLAION [39]. To address the first issue, we promote the\noriginal depth-conditioned LDM (CLDM) to video gener-\nators by introducing additional temporal modules. In prin-\nciple, these modules push the reverse diffusion process of\na pre-trained image CLDM toward a certain required sub-\nspace, which is conceptually shown in Figure 3. As for the\nsecond one, we freeze the weights of the pre-trained CLDM\nto lock the learned image prior and only fine-tune the tem-\nporal modules over a video dataset. The proposed frame-\nwork is illustrated in Figure 2.\nSpecifically, during training, the input video x is first\nencoded into the latent feature z0 \u2208 RL\u00d7C\u00d7H\u2032\u00d7W \u2032 in a\nframe-wise manner using pre-trained encoder E, where C\nis the number of latent channel dimensions, H\u2032 and W \u2032 are\n4\nthe latent spatial size, i.e., height and width, respectively.\nTo inject structural guidance into denoising, we concatenate\nthe frame-wise depth s extracted from the input video with\nan off-the-shelf depth estimator MiDas DPT-Hybrid [34]\nwith zt that is diffused from latent feature z0. The textu-\nral guidance is introduced through cross-attention layers in\nthe OpenCLIP text embedding space. The original image\nCLDM layers process the video data as a batch of indepen-\ndent input images by shifting the temporal dimension to the\nbatch dimension for frame-wise processing. Accordingly,\nthe batched feature output will be shifted back to video be-\nfore being fed to the temporal modules.\nWe implement two types of temporal modules \u03f5\u03d5,\nnamely, Spatio-Temporal Residual Block (STRB) and Tem-\nporal Transformer (TT), shown at the bottom of Figure 2.\nSTRB is an extension of the original residual block con-\ntaining spatial layers only. To make it capable of capturing\ntemporal priors in videos, we introduce additional pseudo\n3D convolution layers, i.e., 2D spatial conv followed by 1D\ntemporal conv, based on two considerations: (i) there is a\ndomain gap between the modeled distribution from text-\nimage dataset and target distribution in text-video data, as\nleast in content; and (ii) the additional spatial layers could\nassist the temporal layers in learning motion dynamics by\nincreasing the adaptability to those pre-trained spatial mod-\nules, which is evidenced by the experiments in Section 4.3.\nThe Temporal Transformer is located behind the original\nspatial transformer and its design is to leverage the property\nof temporal self-similarity to learn the inherent motion dy-\nnamics along the temporal axis in video data. Concretely, it\nconsists of temporal self-attention modules with learnable\npositional embeddings [40], and a feed-forward layer [48].\nThen our controllable T2V framework with denoiser\n\u03f5\u03b8,\u03d5 can be trained with a similar setting to the underlying\nCLDM. It is worth noting that the original weights \u03f5\u03b8 are\nfrozen and only the added temporal modules \u03f5\u03d5 are learned\nvia the optimization objective:\narg min\n\u03d5\nEE(x),\u03f5\u223cN (0,I),t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8,\u03d5(zt; c, s, t)\u22252\n2\n\u0003\n,\n(2)\nwhere zt indicates the diffused latent z = E(x) with noise\nlevel t.\nDuring inference, as depicted in Figure 2 (right), the\ndepth s obtained from multiple sources for structure control\nand the text prompt c describing the target appearance serve\nas conditions for the reverse diffusion process. This process\nstarts with randomly sampled noise zT and the denoised la-\ntent z0 is converted into a video in pixel space using the\npre-trained decoder D.\n3.3. Temporal Masking for Longer Video Synthesis\nOur conditional video LDM can generate satisfactory\nvideos with the same number of frames (i.e., 16 frames)\nInference\nMax temporal\nreceptive field\nTraining\n: 0\n: \u2212\u221e\nFigure 4. Illustration of the causal attention mask during training\n& inference.\nas in the training phase. However, when using it to gener-\nate longer videos during inference, we observe significant\nquality degradation. The possible reason is that the tempo-\nral self-attention is conducted in an N-to-N manner and the\nlearned parameters are well-fitted to process a fixed length\nof tokens. According to this conjecture, longer token se-\nquences tend to disturb each other because of the confused\nattention across frames.\nTo alleviate this problem, we propose to introduce a tem-\nporal masking mechanism so that the learned temporal at-\ntention module can better adapt to the cases involved in\nlonger video synthesis. As shown in Figure 4, we adopt the\ncausal attention mask (CAM) strategy to achieve this. The\ntemporal attention Ft of an input feature zt is calculated\nvia:\nFt = Attention(Qt, Kt, Vt) = softmax(QtK\u22a4\nt\n\u221a\nd\n+ M)Vt,\n(3)\nwhere Qt, Kt, Vt are linearly projected features from zt,\nd denotes the head dimension, and M is a lower triangu-\nlar matrix (Mi,j = 0 if i > j else \u2212\u221e) during train-\ning. For longer video synthesis during inference, the mask\nis modified to ensure the present token is only affected by\nthe previous LM tokens, i.e., maximum temporal receptive\nfield. With the help of CAM, the self-attention layers can\nbe aware of different lengths of tokens, making the causal\nreceptive field adjustable. It can thus effectively mitigate\nthe quality degradation and temporal inconsistency problem\nfor longer video synthesis. Still, larger LM leads to better\ncross-frame coherence while hinders the video quality, and\nvice versa.\nMoreover, the CAM is an explicit way to inject video\nprior (i.e., motions in the video data are directional) into\nthe adapted image LDM to learn temporal coherence, which\ncould benefit even when generating short sequences, as evi-\ndenced in Section 4.3. Thus we also adopt the causal atten-\ntion mask in all experiments.\n5\nTable 1.\nQuantitative comparisons with state-of-the-art condi-\ntional video generation methods on UCF-101 for the zero-shot set-\nting.\nMethod\nCondition\nFVD \u2193\nKVD \u2193\nCogVideo (Chinese)\nText\n751.34\n-\nCogVideo (English)\nText\n701.59\n-\nMagicVideo\nText\n699.00\n-\nMake-A-Video\nText\n367.23\n-\nVideo LDM\nText\n550.61\n-\nT2V-zero+CtrlNet\nText+Depth\n951.38\n115.55\nLVDMExt+Adapter\nText+Depth\n537.85\n85.47\nOurs w/o CAM\nText+Depth\n390.63\n36.57\nOurs\nText+Depth\n330.49\n29.52\n4. Experiments\n4.1. Implementation Details\nOur development is based on depth-conditioned La-\ntent Diffusion Models [36] (a.k.a Stable-Diffusion-Depth)\nimplemented with PyTorch and the public pre-trained\nweights*. We train the newly introduced layers with 50K\nsteps on the learning rate 1\u00d710\u22124 and valid mini-batch size\n512 with DeepSpeed [35].\nAt inference, we use DDIM\nsampler [43] with classifier-free guidance [13] in all exper-\niments.\nDatasets. We use the WebVid-10M [1] dataset to turn\nthe depth-conditioned LDM into a controllable text-to-\nvideo generator.\nWebVid-10M consists of 10.7 million\nvideo-caption pairs with a total of 52K video hours and is\ndiverse and rich in content. During training, we sample 16\nframes with a frame stride of 4 (assuming 30 FPS) and a\nresolution of 256 x 256 from input videos.\n4.2. Evaluation on Video Generation\nJoint text-structure-conditioned video synthesis is a\nnascent area of computer vision and graphics, thus we find a\nlimited number of publicly available research works to com-\npare against. The extension version of LVDM [9] and T2V-\nzero [19] are general text-to-video methods but capable of\ngenerating videos with additional conditions supported by\nControlNet [53] or Adapter [27], and we denote them as\nLVDMExt+Depth Adapter and T2V-zero+Depth CtrlNet, re-\nspectively. Meanwhile, we benchmark against pure text-to-\nvideo synthesis methods, including CogVideo [15], Mag-\nicVideo [54], Make-A-Video [41], and Video LDM [4].\nSince there is no structure control for these approaches, we\ninclude them here for reference and to examine the perfor-\nmance differences concerning structural guidance in text-\n*https : / / huggingface . co / stabilityai / stable -\ndiffusion-2-depth\nII\nOurs\nInput depth\n\u201cA camel walking on the snow field, Miyazaki Hayao anime style\u201d\nIII\nI\nIV\nFigure 5. Visual comparisons of the videos synthesized by differ-\nent variants of our approach.\nto-video synthesis.\nTo evaluate the performance of video generation,\nwe report the commonly-used Fr\u00b4echet Video Distance\n(FVD) [47] and Kernel Video Distance (KVD) [47], which\nevaluate video quality by measuring the feature-level sim-\nilarity between synthesized and real videos based on the\nFr\u00b4echet distance and kernel methods, respectively. Specif-\nically, they are computed by comparing 2K model sam-\nples (16 frames) with samples from the common evalua-\ntion [4, 54] dataset UCF-101 [45]. Following [4], we di-\nrectly use UCF class names as text conditioning.\nWe evaluate in the zero-shot setting and tabulate the\nquantitative performance in Table 1. According to the re-\nsults, our method significantly outperforms all baselines\nwith lower FVD and KVD. It is worth noting that although\nMake-A-Video with text condition only achieves an FVD\nvalue close to ours, they train on an additional extremely-\nlarge-scale dataset containing 100M text-video pairs, i.e.,\nHD-VILA-100M [50]. The superiority of our method in-\ndicates the effectiveness of the proposed video generation\nframework with textual and structural guidance and the\nadapting strategy that turns image LDMs into video gen-\nerators. The qualitative comparison is made in the context\nof several application scenarios, as shown in Figure 7 and\nFigure 8.\n6\nTable 2. Ablation study of different adapting strategies on UCF-\n101 for zero-shot setting.\n(Note: TT=Temporal Transformer,\nTC=1D Temporal Conv, P3D=Pseudo 3D Conv)\nVariant\nFine-tuned para. FVD \u2193 KVD \u2193\nI. SD-Depth\nNone\n1422.30 316.25\nII.\nw/ TT\nTT\n803.24\n89.26\nIII. w/ TT\nFull U-Net\n500.96\n77.72\nIV. w/ TT+TC\nTT+TC\n443.63\n40.86\nV.\nw/ TT+P3D (Ours)\nTT+P3D\n330.49\n29.52\n4.3. Ablation Studies\nWe study several key designs of our proposed method\nin this section, including the adapting strategy and causal\nattention mask.\nAdapting strategy. To study the effectiveness and supe-\nriority of our adapting strategy, we construct several base-\nlines: (I.): SD-Depth, the pre-trained image LDM that we\nbuild upon, (II.): adding Temporal Transformer (TT) to\nbaseline (I.) and fine-tuning this module, (III.): the same\narchitecture as (II.) but fine-tune the entire model, (IV.):\nadding both TT and 1D Temporal Convolutions (TC) to (I.)\nand fine-tuning these two modules, and (V.): introducing\nboth TT and Pseudo 3D modules to (I.) and fine-tuning\nthem, which is our full method.\nThe quantitative com-\nparison is shown in Table 2. By comparing baselines (I.),\n(II.), and (IV.), we can observe the improved performance\nin terms of both FVD and KVD by introducing more tem-\nporal modules that increase the temporal modeling capabil-\nity. The comparison between (V.) and (IV.) highlights the\nbenefits of incorporating additional spatial layers with TC,\nwhich enhances the adaptability between the newly intro-\nduced temporal modules and the fixed spatial modules. It\nis worth noting that although fine-tuning the entire model\n(III.) improves the performance quantitatively over its coun-\nterpart (II.), it cause a severe concept forgetting issue, as ev-\nidenced by Figure 5 where (III.) fails to reflect the \u2018anime\nstyle\u2019. The visual comparison also tells the superiority of\nour full method in terms of both temporal coherence and\nconformity to the text prompt.\nCausal attention mask. We further investigate the ef-\nfectiveness of causal attention mask (CAM). We construct\na baseline without CAM and train it with the same sequence\nlength as our full method (16 frames). As shown in Table 1\n(bottom), CAM can boost the performance of conditional\nvideo generation in a 16-frame setting, owing to the intro-\nduced directional video motion prior. We directly apply the\ntrained models for longer video synthesis (64 frames). The\nqualitative comparison is presented in Figure 6, wherein our\nbaseline variant suffers from significant quality degradation\nand temporal inconsistency (more evident in the supple-\nmentary video) due to the mismatched training and infer-\n\u201cA tiger walks \nin the green \nforest\u201d\n\u201cA GTA game \nstreet with \nbuildings, \nlow saturation\u201d\nOurs w/o CAM\nOurs\nOurs\nOurs w/o CAM\nFigure 6.\nVisual comparisons of longer video synthesis (64\nframes) produced by our baseline variant (w/o CAM) and our\nmethod (w/ CAM). Each frame is selected with a stride of 16.\nence setting. In contrast, our method could produce plau-\nsible longer videos with greater detail and improved cross-\nframe coherence. Readers are recommended to check our\nsupplementary video for better comparison.\n5. Applications\n5.1. Video Creation\nOur approach allows customized video creation guided\nby rough motion structures. So, it is feasible to capture a\nvideo of manually constructed miniature setup and use it\nas structural control for text-to-video synthesis. Some ex-\namples are illustrated in Figure 1 (top) and Figure 7 (top).\nIn comparison to other baseline methods, our approach is\ncapable of generating high-fidelity, temporally consistent\nvideos that closely adhere to the target textual descriptions\nand scene structure. In contrast, T2V-zero+CtrlNet mainly\nsuffers from inconsistency due to its weak temporal con-\nstraint, and LVDMExt+Adapter tends to cause lower visual\nquality that inherits from the base text-to-video model. One\ncan also construct the scene structure with 3D modeling\nsoftware, e.g., Unity, as shown in Figure 1 (middle) and\nFigure 7 (bottom). For LVDMExt+Adapter, apart from the\naforementioned problems, it also fails to synthesize stylized\nvideos, e.g., \u20182D cartoon\u2019 and \u2018Chinese ink wash\u2019 at the\nbottom-left corner of Figure 7. It is worth noting that our\nmethod can achieve both decent text alignment and cross-\nframe coherence.\n5.2. Video Re-rendering\nVideo re-rendering here means changing the video ap-\npearance based on the text prompts while still preserv-\ning its motion structure. In addition to LVDMExt+Depth\nAdapter and T2V-zero+Depth CtrlNet, we also compare\nagainst the depth-conditioned image LDM (i.e., SD-Depth)\n7\nTable 3. Quantitative comparisons for video re-rendering.\nMetric\nSD-Depth T2V-zero LVDMExt Tune-A-Video\nOurs\n+CtrlNet +Adapter\nTemp.\u2191\n0.8624\n0.9109\n0.9613\n0.9659\n0.9698\nPrompt\u2191\n0.3695\n0.3379\n0.3372\n0.3473\n0.3547\nthat is used by our model as spatial modules, and a video\nediting method, Tune-A-Video [49] combined with DDIM\ninversion [43], which requires per-video optimization and\nthe original RGB video for inversion operation. Following\nprevious works [3, 6, 49], we use videos from DAVIS [29]\nand other in-the-wild videos, from which 11 representative\nvideos with manually designed text prompts are utilized for\nevaluation.\nQuantitative evaluation.\nWe measure temporal con-\nsistency and prompt conformity for performance compar-\nison. Following [6], temporal consistency is calculated as\nthe average cosine similarity between consecutive frame\nembeddings of CLIP image encoder, while prompt consis-\ntency is calculated as the average cosine similarity between\ntext and image CLIP embeddings across all frames. Ta-\nble 3 shows the results of each model, where \u2018Temp.\u2019 and\n\u2018Prompt\u2019 indicate the temporal coherence and prompt con-\nformity, respectively. Our model outperforms the baseline\nmodels in temporal consistency and except SD-Depth in\nprompt conformity. Anyhow, as an image synthesis model,\nSD-Depth fails to synthesize consistent video frames. Be-\nsides, although achieves the second-best temporal consis-\ntency performance, Tune-A-Video suffers from overfitting\nto the original video, as evidenced by the results in Figure 8.\nQualitative evaluation.\nWe visually compare our\nmethod with other competitors in Figure 8. In the \u2018camel\u2019\ncase, the videos produced by our method are temporally co-\nherent and well-align with the text description, i.e., \u2018anime\nstyle\u2019.\nIn contrast, SD-Depth cannot produce consistent\nvideo frames; Tune-A-Video, LVDMExt+Adapter, and T2V-\nzero+CtrlNet suffer from low text-video conformity, visual\nconcept forgetting, and structure deviations respectively. In\n\u2018waterfall\u2019 case, our method shows similar superiority in\nthe comprehensive quality including video quality, struc-\nture preservation, text-video conformity, and temporal co-\nherence. Readers are recommended to check our supple-\nmentary video for better comparison.\nUser study. The human perception system is still the\nmost reliable measure for video generation tasks. We con-\nduct a user study, where 32 participants with good vi-\nsion ability complete the evaluation successfully. For each\nexample, the participants are first shown with the input\nvideo and target prompt, followed by five randomly ordered\nvideos re-rendered by different methods. Then, they are\nasked to rank the results in terms of temporal coherence,\nTable 4. User study statistics of average ranking\u2193 and preference\nrate\u2191.\nProperty\nSD-Depth T2V-zero LVDMExt Tune-A-Video\nOurs\n+CtrlNet +Adapter\nTemporal\n3.95\n3.40\n2.84\n2.54\n2.27\ncoherence\n6.53%\n6.53%\n12.78%\n33.52%\n40.62%\nStructure&\n3.54\n3.01\n3.10\n2.89\n2.45\nText align.\n13.07%\n15.34%\n12.50%\n20.17%\n38.92%\nFrame\n3.45\n3.09\n3.07\n2.84\n2.55\nquality\n13.64%\n13.07%\n14.20%\n24.15%\n34.94%\ntext & structure guidance conformity, and frame quality:\n{1: the best, 2: the second-best, . . ., 5: the worst}. We\nanalyze the collected evaluation result in two aspects: (i)\naverage ranking: the average ranking score of each method\naccording to the rank-score table, and (ii) preference rate:\nthe percentage of being selected as the best. The statistics\nare tabulated in Table 4. Our method earns the best ranking\nscores and preference rates in all three aspects.\n6. Limitation\nOur method also has certain limitations. Firstly, our cus-\ntomized video generation model supports no precise control\nover visual appearance. For instance, it is intractable to syn-\nthesize videos featuring a specific individual or object. This\nrelates to the concept customization technique, which has\nbeen extensively studied in T2I synthesis but remains less\nexplored in T2V synthesis. Additionally, our controllable\ntext-to-video model explicitly demands frame-wise depth\nguidance, which can be overly intensive and may increase\nusage challenges in certain situations. A more efficient for-\nmulation could be supplying the model only with sparse\nkeyframe guidance, which could help broaden the scope of\npotential applications. We leave these directions as future\nworks.\n7. Conclusion\nWe presented an efficient approach for customized video\ngeneration with textual and structural guidance. By employ-\ning a pre-trained image LDM as frozen spatial modules, our\nvideo generator exhibits a significant advantage in inherit-\ning the wealth of visual concepts while maintaining satis-\nfactory temporal coherence. Furthermore, we introduced\na temporal masking mechanism to facilitate the synthesis\nof longer videos, which is a non-trivial extension for dif-\nfusion models. Ablation studies confirm the effectiveness\nof our proposed designs. Comparisons with state-of-the-art\nmethods reveal our superiority in controllable text-to-video\ngeneration both quantitatively and qualitatively. Several ap-\nplications show practical usage.\n8\n\u201cA dam discharging water\u201d\n\u201cA futuristic rocket ship on a launchpad, with sleek design, glowing lights\u201d\nLVDMExt+Adapter T2V-zero+CtrlNet\nOurs\nReal-life scene\n\u201cA train on the rail, 2D cartoon style\u201d\n\u201cA Van Gogh style painting on drawing board in park, \nsome books on the picnic blanket, photorealistic\u201d\n\u201cA Chinese ink wash landscape painting\u201d\n\u201cA crashed UFO on the snowfield, it is snowing\u201d\nLVDMExt+Adapter T2V-zero+CtrlNet\nOurs\n3D scene modeling\nLVDMExt+Adapter T2V-zero+CtrlNet\nOurs\n3D scene modeling\nFigure 7. Visual comparison on videos generated in two applications, i.e., real-life scene to video (top) and 3D scene modeling to video\n(middle & bottom).\n9\nOriginal video\nSD-Depth\nLVDMExt+Adapter T2V-zero+CtrlNet\nTune-A-Video\nOurs\nOriginal video\nSD-Depth\nLVDMExt+Adapter T2V-zero+CtrlNet\nTune-A-Video\nOurs\n\u201cA camel walking on the snow field, Miyazaki Hayao anime style\u201d\n\u201cA waterfall in the middle of a glacier\u201d\nFigure 8. Visual comparison on examples of video re-rendering application, i.e., \u2018camel\u2019 (top) and \u2018waterfall\u2019 (bottom).\n10\nReferences\n[1] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In IEEE/CVF International Conference\non Computer Vision (ICCV), 2021. 6\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022. 2\n[3] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image\nand video editing.\nIn European Conference on Computer\nVision (ECCV), 2022. 3, 8\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models.\narXiv preprint arXiv:2304.08818,\n2023. 3, 6\n[5] Guillaume Couairon, Jakob Verbeek, Holger Schwenk,\nand Matthieu Cord.\nDiffedit:\nDiffusion-based seman-\ntic image editing with mask guidance.\narXiv preprint\narXiv:2210.11427, 2022. 3\n[6] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\narXiv preprint arXiv:2302.03011, 2023. 3, 8\n[7] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 2\n[8] Xianfan Gu, Chuan Wen, Jiaming Song, and Yang Gao. Seer:\nLanguage instructed video prediction with latent diffusion\nmodels. arXiv preprint arXiv:2303.14897, 2023. 3\n[9] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022. 3, 6\n[10] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image\nediting with cross attention control. In International Confer-\nence on Learning Representations (ICLR), 2023. 3\n[11] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2, 3\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In Conference on Neural Infor-\nmation Processing Systems (NeurIPS), 2020. 2\n[13] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. In Conference on Neural Information Processing\nSystems Workshop (NIPSW), 2021. 2, 6\n[14] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. In Conference on Neural Information Pro-\ncessing Systems (NeurIPS), 2022. 3\n[15] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and\nJie Tang. Cogvideo: Large-scale pretraining for text-to-video\ngeneration via transformers. In International Conference on\nLearning Representations (ICLR), 2023. 6\n[16] Aapo Hyv\u00a8arinen and Peter Dayan.\nEstimation of non-\nnormalized statistical models by score matching.\nJournal\nof Machine Learning Research, 6(4), 2005. 3\n[17] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Lay-\nered neural atlases for consistent video editing. ACM Trans-\nactions on Graphics (TOG), 40(6):1\u201312, 2021. 3\n[18] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Hui-\nwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani.\nImagic: Text-based real image editing with diffusion mod-\nels. In IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2023. 3\n[19] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 3, 6\n[20] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation. arXiv\npreprint arXiv:2301.07093, 2023. 2\n[21] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya\nJia. Video-p2p: Video editing with cross-attention control.\narXiv preprint arXiv:2303.04761, 2023. 3\n[22] Siwei Lyu. Interpretation and generalization of score match-\ning. arXiv preprint arXiv:1205.2629, 2012. 3\n[23] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying\nShan, Xiu Li, and Qifeng Chen.\nFollow your pose:\nPose-guided text-to-video generation using pose-free videos.\narXiv preprint arXiv:2304.01186, 2023. 3\n[24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In International Conference on Learning Representa-\ntions (ICLR), 2021. 3\n[25] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real images\nusing guided diffusion models. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023. 3\n[26] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid\nHoshen. Dreamix: Video diffusion models are general video\neditors. arXiv preprint arXiv:2302.01329, 2023. 3\n[27] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2,\n6\n[28] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen.\nGlide: Towards photorealis-\ntic image generation and editing with text-guided diffusion\n11\nmodels. In International Conference on Machine Learning\n(ICML), 2022. 2\n[29] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel\u00b4aez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017\ndavis challenge on video object segmentation. arXiv preprint\narXiv:1704.00675, 2017. 8\n[30] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arXiv:2303.09535, 2023. 3\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning\n(ICML), 2021. 2\n[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. The Journal of Machine\nLearning Research, 21(1):5485\u20135551, 2020. 2\n[33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 2\n[34] Ren\u00b4e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence (PAMI), 44(3):1623\u20131637, 2020. 5\n[35] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. Deepspeed: System optimizations enable train-\ning deep learning models with over 100 billion parameters.\nIn Proceedings of the 26th ACM SIGKDD International Con-\nference on Knowledge Discovery & Data Mining, 2020. 6\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2022.\n2, 3, 4, 6\n[37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn International Conference on Medical Image Computing\nand Computer Assisted Intervention, 2015. 4\n[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. Conference on Neural Information\nProcessing Systems (NeurIPS), 2022. 2, 3\n[39] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\nIn Conference on Neural Information Processing Systems\nWorkshop (NIPSW), 2021. 4\n[40] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\nSelf-\nattention with relative position representations. In ACL Con-\nference of the North American Chapter of the Association for\nComputational Linguistics, 2018. 5\n[41] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al.\nMake-a-video: Text-to-video genera-\ntion without text-video data. In International Conference on\nLearning Representations (ICLR), 2023. 2, 3, 6\n[42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning (ICML), 2015. 2\n[43] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations (ICLR), 2021. 3, 6, 8\n[44] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In International Conference on Learning Represen-\ntations (ICLR), 2021. 2, 3\n[45] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. arXiv preprint arXiv:1212.0402, 2012. 6\n[46] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation.\nIn IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023. 3\n[47] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRapha\u00a8el Marinier, Marcin Michalski, and Sylvain Gelly.\nFvd:\nA new metric for video generation.\nIn Interna-\ntional Conference on Learning Representations Workshop\n(ICLRW), 2019. 6\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Conference on\nNeural Information Processing Systems (NeurIPS), 2017. 5\n[49] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2212.11565, 2022. 3, 8\n[50] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun,\nBei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Ad-\nvancing high-resolution video-language representation with\nlarge-scale video transcriptions. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2022.\n6\n[51] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin\nLin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael\nZeng, et al. Reco: Region-controlled text-to-image genera-\ntion. arXiv preprint arXiv:2211.15518, 2022. 2\n[52] Yu Zeng, Zhe Lin, Jianming Zhang, Qing Liu, John Col-\nlomosse, Jason Kuen, and Vishal M Patel.\nScenecom-\nposer: Any-level semantic image synthesis. arXiv preprint\narXiv:2211.11742, 2022. 2\n[53] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023. 2, 6\n12\n[54] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 3, 6\n13\n"
  },
  {
    "title": "StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners",
    "link": "https://arxiv.org/pdf/2306.00984.pdf",
    "upvote": "2",
    "text": "StableRep: Synthetic Images from Text-to-Image\nModels Make Strong Visual Representation Learners\nYonglong Tian1,\u2217\nLijie Fan1,2,\u2217\nPhillip Isola2\nHuiwen Chang1\nDilip Krishnan1\n1Google Research,\n2MIT CSAIL,\n\u2217equal contribution\nCode: https://github.com/google-research/syn-rep-learn\nAbstract\nWe investigate the potential of learning visual representations using synthetic\nimages generated by text-to-image models. This is a natural question in the light of\nthe excellent performance of such models in generating high-quality images. We\nconsider specifically the Stable Diffusion, one of the leading open source text-to-\nimage models. We show that (1) when the generative model is configured with\nproper classifier-free guidance scale, training self-supervised methods on synthetic\nimages can match or beat the real image counterpart; (2) by treating the multiple\nimages generated from the same text prompt as positives for each other, we develop\na multi-positive contrastive learning method, which we call StableRep. With solely\nsynthetic images, the representations learned by StableRep surpass the performance\nof representations learned by SimCLR and CLIP using the same set of text prompts\nand corresponding real images, on large scale datasets. When we further add\nlanguage supervision, StableRep trained with 20M synthetic images achieves better\naccuracy than CLIP trained with 50M real images.\nGenerative Models\nStable Diffusion (SD)\nData Engine\nEmbedding\nReal data\n(A) Traditional Representation Learning\n(B) Representation Learning with Synthetic Data\nSynthetic Data\nReal data\nEmbedding\nSynthetic data\nEncoder\nEncoder\nFigure 1: Left: traditional visual representation learning relies on a dataset of real images to train an image\nembedding function. Right: we view generative models as datasets that allow us to sample images from the data\ndistribution. In our study, we leverage text-to-image models (Stable Diffusion [61]) and treat multiple images\nsynthesized from the same prompt as positives for contrastive representation learning.\n1\nIntroduction\nData has assumed a paramount role as the key component for the success of modern machine learning\nsystems. Such systems, especially foundation models in various domains, heavily rely on vast and\ndiverse datasets to acquire knowledge, make accurate predictions, and generate content. The quality,\nquantity, and diversity of the data significantly impacts the performance and effectiveness of these\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.00984v2  [cs.CV]  26 Oct 2023\nmodels, as they learn from the collective information encapsulated within the data. In this data-centric\nera, a central question is: how can we collect such large amounts of varied data to train AI models?\nAs an example, suppose we are trying to solve a new computer vision problem, and need to collect\ndata (images) for it. An ideal situation is to place a camera anywhere in the wold and capture whatever\nwe need. But in reality, collecting data is historically not easy. In the 1990s, researchers needed to\ntake photos by themselves to create datasets for objects [52] and faces [68, 24]. To collect data in the\n2000s, people crawled the Internet [15]. Noisy, uncurated data collected in such a manner can exhibit\ndomain gaps with the real world problem and reflect imbalances due to societal bias. Removing or\nreducing such imperfection in data of high volume by human labeling is costly and can be prohibitive.\nHowever, what if data collection could be simplified to the utterance of a natural language command,\nspecifying what you want? What if, for hardly any cost, you could take a photo every few millisec-\nonds? This sounds fanciful, but modern text-to-image generative models are approaching this vision.\nIt has long been a dream that someday we could use these as our data sources, rather than taking\nphotos [75, 30, 35]. In this paper, we study if this is now a practical option in the context of large\nscale visual representation learning.\nTo achieve this, we choose to work with Stable Diffusion [61], one of the leading open source\ntext-to-image models. We synthesize images by prompting Stable Diffusion with text from large\nscale image-text datasets, such as CC12M [9] and RedCaps [16]. Surprisingly, our investigation\nreveals that when the classifier-free guidance scale is properly configured for Stable Diffusion, it\nis able to synthesize images on which training self-supervised methods can perform at par with\nor better than training on real images of the same sample size. Inspired by the idea of contrastive\nself-supervised learning, which promotes intra-image invariance, we develop a representation learning\napproach that promotes intra-caption invariance. We achieve this by treating the multiple images\ngenerated from the same text prompt as positives for each other and use them in a multi-positive\ncontrastive loss (see Figure 1). Despite training with solely synthetic images, this approach, called\nStableRep, even outperforms state-of-the-art methods such as CLIP [58] using the same text set, but\nwith corresponding real images, on various representation evaluation benchmarks.\nIntuitively, one reason that synthetic data can be better than real data is because we are able to achieve\na greater degree of control in the sampling, such as via the guidance scale in Stable Diffusion, or\nvia text prompts and latent noise variables. Furthermore, generative models have the potential to\ngeneralize beyond their training data and therefore provide a richer (synthetic) training set than the\ncorresponding real data alone. Our key contributions are:\n1. We discover that training modern self-supervised methods on synthetic images from Stable Diffu-\nsion can be surprisingly effective. The learned representations are often better than representations\nlearned from real images of the same sample size.\n2. We develop StableRep, a novel representation learning approach by capturing invariance between\nimages generated from the same text prompt, and propose a multi-positive contrastive loss.\n3. With StableRep, we are able to achieve 76.7% linear accuracy on ImageNet with ViT-B/16, using\nsolely synthetic images.\n4. When coupled with language supervision, our StableRep trained with 20M synthetic images (10M\ncaptions) achieves better accuracy than CLIP trained with 50M real images (50M captions).\n2\nStandard Self-supervised Learning on Synthetic Images\nA typical visual representation learning algorithm takes an image dataset {xi}N\ni=1 as input, and yields\nan image encoder F : x \u2192 e, which embeds an image x into a vector e. In this paper, we instead try\nto produce a good F by using a generative model G rather than a real image dataset. Specifically, we\nfocus on text-to-image generative models G : (t, z) \u2192 x, which maps a pair of text t and latent noise\nz to an image x. While there are several top performing text-to-image models [59, 67, 88, 7, 36, 3],\nwe conduct our exploration with the Stable Diffusion [61] since it is publicly available and widely\nused. The version we used is v1-5.\n2.1\nSynthetic images from Stable diffusion\nStable diffusion [61] is a denoising diffusion probabilistic model [73, 31] that runs the diffusion\nprocess in the latent space of an autoencoder. It improves the sample quality and text-image alignment\n2\n2\n3\n4\n6\n8\n10\n12\nguidance scale w\n60.0\n60.5\n61.0\n61.5\n62.0\nImageNet Linear Acc.\n59.7\n59.8\n60.4\n61.7\n62.0\n62.0\n61.6\nSimCLR\nSynthetic\nReal\n2\n3\n4\n6\n8\n10\n12\nguidance scale w\n51.0\n52.0\n53.0\n54.0\n55.0\n56.0\n51.8\n53.6\n55.4\n56.2\n55.6\n55.8\n55.4\nMAE\nSynthetic\nReal\nFigure 2: Performance of linear probes on ImageNet as a function of the guidance scale of Stable Diffusion\ngeneration. Left: using SimCLR as pre-training; Right: using MAE as pre-training. In both cases, we see\npre-training on synthetic images that are generated by Stable Diffusion with a guidance scale between 6 and 8,\ngives a significant boost over training only on real images. We used the CC3M dataset for these experiments.\nvia classifier-free guidance [32], which linearly combines conditional score estimate \u03f5(t, z\u03bb) and\nunconditional estimate \u03f5(z\u03bb) with the guidance scale w at each step \u03bb:\n\u02dc\u03f5(t, z\u03bb) = w\u03f5(t, z\u03bb) + (1 \u2212 w)\u03f5(z\u03bb)\n(1)\nThe Stable Diffusion model Gsd relies on text sources to generate images. Instead of collecting a\ncorpus of captions from scratch, we use the text part of existing uncurated image-text pair datasets,\nsuch as CC3M [71] and CC12M [9]. Formally, given an image caption dataset {ti}N\ni=1, we generate\none image per caption, forming a synthetic image dataset of the same size.\n2.2\nSelf-supervised learning on synthetic images\nRecent representative self-supervised learning algorithms are mostly from two families: (1) con-\ntrastive learning which encourages invariance between embeddings of different augmentations of the\nsame image ; (2) masked image modeling where model uses unmasked patches to predict masked\npatches (although there are other methods that fall into neither category, such as BYOL [25] and\nDINO [6]). For our study, we choose SimCLR [10] from the former family and MAE [26] from\nthe latter due to their simplicity and strong performance. We focus on the Vision Transformer\narchitecture [18], and use captions from CC3M [71] except when noted.\nSimCLR [10]. We directly train SimCLR with ViT-B/16 on the synthetic image dataset, and\nmeasure the representation quality by linear probing evaluation on ImageNet [15] 1. One factor to\nconsider is the classifier-free guidance scale w, as it trades off between diversity and quality of the\nsynthesized images and thus can affect the learned representations. To study this, for each w in the\nset {2, 3, 4, 6, 8, 10, 12}, we generate a copy of size N (one image per caption) to train SimCLR.\nFigure 2(left) visualizes the influence of w. The optimal w is around 8 (both 8 and 10 give an accuracy\nof 62.0%). This is different from the FID metric where w = 2 is the optimal.\nThe captions {ti}N\ni=1 used to generate synthetic images are also paired with N real images. We train\na SimCLR model with these real images. This model achieves 60.4% accuracy, experiencing a 13%\ndrop in linear accuracy compared to pre-training on ImageNet. Such gap has been generally observed\nfor uncurated pre-training data [77]. However, both interestingly and surprisingly, synthetic images\nwith w = 8 have 1.6% higher accuracy than real images (62.0% v.s. 60.4%).\nMAE [26]. Following the default hyperparameters in MAE [26], we train a ViT-B/16 model for each\nguidance scale w. Figure 2(right) reports the linear probing results. The accuracy of synthetic images\nincreases quickly with w after 2, and gradually drops when w is large, e.g., w \u2265 10. The optimal\nguidance scale for MAE is 6, and this is different from SimCLR where the accuracy peaks at 8 or 10.\nThis suggests that different methods may require different w. With w = 6, synthetic images have a\n4.2% better accuracy than real images.\nWhile the linear probing accuracy of MAE is lower than that of contrastive methods, its effectiveness\noften comes with fine-tuning. When fine-tuning pre-trained MAE models on ImageNet, we found\nsynthetic images are still able to outperform real images. For instance, synthetic images with w = 6\nis 0.3% higher than real images (82.9% v.s. 82.6%).\n1We verify our SimCLR implementation by pre-training on ImageNet. It achieves 74.3% linear probing\naccuracy. As a comparison, SimCLR in [11] with the same architecture and epochs achieved 73.9%.\n3\nMAE\nDINO\nSimCLR\nBYOL\nMoCo v3\n50\n52\n54\n56\n58\n60\n62\n64\n66\nImageNet Linear Acc.\n+5.2\n+2.4\n+1.6\n+0.2\n-0.5\nSynthetic vs. Real on CC3M\nSynthetic\nReal\nMAE\nDINO\nSimCLR\nBYOL\nMoCo v3\n52\n54\n56\n58\n60\n62\n64\n66\n+3.4\n+2.1\n+2.1\n-0.3\n-0.2\nSynthetic vs. Real on CC12M\nSynthetic\nReal\nFigure 3: Training self-supervised methods on synthetic images can be better than, or on par with, real images\nof the same sample size. Left: CC3M dataset; Right: CC12M dataset\n.\n(A) SimCLR\n\u201cClay Animation, Adorable Dragon\u201d\n(C) StableRep (Ours)\n...\n...\n(B) CLIP\nReal Image\nReal Image\n\u201cClay Animation,\n Adorable Dragon\u201d\nReal Caption\nEncoder\nEncoder\nText\nEncoder\nEncoder\nEncoder\nEncoder\nEncoder\nAug\nAug\nWeak Aug\nAug\nAug\nAug\nSD\n...\nSD\nSD\nFigure 4: We compare our pipeline (C) to that of (A) SimCLR; (B) CLIP. In SimCLR, the real image is\naugmented to give two views which are contrasted against each other through the same encoder. For CLIP, a real\nimage and corresponding real caption are passed into image and text encoder, the image is augmented (usually\nmore weakly than for SimCLR) followed by a contrastive loss. In our pipeline, each real caption is passed into\nStable Diffusion (SD) to generate a number of synthetic images. These synthetic images are then augmented as\nin SimCLR, and treated as positives for each other in a multi-positive contrastive loss.\nOther SSL methods. To test if synthetic images can be generically applied to different self-supervised\nlearning methods, we try three more representative approaches: BYOL [25], MoCo-v3 [11], and\nDINO [6]. We do not tune w for each method, and instead apply the optimal w (= 8) discovered for\nSimCLR. The results on CC3M and CC12M are visualized in Figure 3. Synthetic images significantly\nimprove over real for MAE, DINO, and SimCLR, and performs on par with real for BYOL, and\nslightly worse for MoCo-v3 (which could be attributed to not tuning the guidance scale w).\n3\nMulti-Positive Contrastive Learning with Synthetic Images\nText-to-image generative models offer a new way to compose positive samples for contrastive learning.\nGiven an image caption, we can create multiple diverse samples by starting the reverse diffusion\nprocess with different latent noise z. Since these images are produced using the same prompt, they\npossess similar visual semantics, making them suitable for use as multiple positive samples for each\nother in contrastive learning. This property is unique to generative models, since collecting multiple\nimages for each caption in large scale is infeasible. Figure 4 compares our StableRep pipeline with\nthat of SimCLR and CLIP.\nMulti-positive contrastive loss. We describe multi-positive contrastive learning as a matching\nproblem. Consider an encoded anchor sample a, and a set of encoded candidates {b1, b2, ..., bK}.\n4\nWe compute a contrastive categorical distribution q that describes how likely a is to match each b:\nqi =\nexp(a \u00b7 bi/\u03c4)\nPK\nj=1 exp(a \u00b7 bj/\u03c4)\n(2)\nwhere \u03c4 \u2208 R+ is the scalar temperature hyper-parameter, and a and all b have been \u21132 normalized.\nIntuitively, this is a K-way softmax classification distribution over all encoded candidates. Assume\nthere is at least one candidate that the anchor a matches. Then we know the ground-truth categorical\ndistribution p is:\npi =\n1match(a,bi)\nPK\nj=1 1match(a,bj)\n(3)\nwhere the indicator function 1match(\u00b7,\u00b7) indicates whether the anchor and candiate match. Then the\nmulti-positive contrastive loss is the cross-entropy between the ground-truth distribution p and the\ncontrastive distribution q:\nL = H(p, q) = \u2212\nK\nX\ni=1\npi log qi\n(4)\nAlgorithm 1 Multi-Pos CL: PyTorch-like Pseudocode\n# f: encoder: backbone + proj mlp\n# tau: temperature\n# minibatch x: [n, m, 3, h, w]\n# n captions, m images per caption\nfor x in loader:\nx = augment(x)\nx = cat(unbind(x, dim=1)) # [n*m, 3, h, w]\nh = f(x)\n# compute ground-truth distribution p\nlabel = range(n * m) %\np = (label.view(-1, 1) == label.view(1, -1))\np.fill_diagonal(0) # self masking\np /= p.sum(1)\n# compute contrastive distribution q\nlogits = h @ h.T / tau\nlogits.fill_diagonal(-1e9) # self masking\nq = softmax(logits, dim=1)\nH(p, q).backward()\ndef H(p, q): # cross-entropy\nreturn - (p * log(q)).sum(1).mean()\nNotes: h.T is h\u2019s transpose. The \u21132 normalization operator is\nincluded in the encoder f.\nThis is a generalized form of the widely-\nused single-positive contrastive loss [54],\nwhere p reduces to a one-hot vector. This\nloss is closely related to that in [39], but\na key distinction here is that we have no\nimage class labels, and only assume im-\nages generated from the same caption are\nmatched.\nThe PyTorch-like pseudocode of the\nbatched multi-positive contrastive learn-\ning algorithm is described in Algo. 1.\nEach batch consists of n \u2217 m images,\nmeaning that we sample m images for\neach of the n captions. Here we still ap-\nply data augmentation, even though im-\nages from the same caption are different.\nThis is to reduce overfitting since we per-\nform many epochs of training over pre-\ngenerated synthetic images. However, if\nin the future the image generator is ca-\npable of producing images fast enough,\nthen we can draw batches online and data\naugmentation may not be necessary. The\nmulti-positive contrastive learning algorithm is also generic such that SimCLR can also be described\nby it \u2013 we begin by randomly selecting a set of n images and subsequently apply m (set as 2) crops to\neach of the chosen images. However, in our StableRep we only utilize a single crop from each image.\n4\nExperiments\nWe perform StableRep pre-training on synthetic images synthesized from texts in the CC3M (2.7\nmillion samples) [71], CC12M (10 million) [9], or RedCaps datasets (11.6 million) [16]. We then\nevaluate the frozen representations by (1) linear probing on ImageNet-1k and other smaller scale\nimage classification benchmark, and (2) few-shot image recognition that measures the generalization\nability of the representations.\nBackbone. We use ViT models [18] as the backbone for our approach StableRep. On top of the CLS\ntoken, we apply a 3-layer MLP projection head with hidden layers of 4096 dimensions and an output\nof 256 dimensions. Batch Normalization [33] is used in this projection head.\n5\nTraining. In most of our experiments, we adopt a batch size of 8192 images (i.e. m \u2217 n = 8192).\nThis way the computation of each batch is equivalent to SimCLR with a batch size of 4096, because\neach image in SimCLR has two crops. We use AdamW optimizer [46] with a learning rate of 0.0032\nand weight decay of 0.1, and set \u03b21, \u03b22 as 0.9, 0.98 respectively. We pre-generate 10 images for\neach text prompt. In each iteration, we randomly sample 6 out of the 10 for each sampled caption\nto form the training batch, i.e., m = 6 in Algo. 1. Recall that for SimCLR m = 2. As a result, one\nepoch training of StableRep is computationally equivalent to 3 epochs of SimCLR. To provide easy\ncomparison, we report SimCLR-equivalent epochs for StableRep in all of our analysis.\n4.1\nMain results on CC12M and RedCaps\nIn this section, we perform StableRep on images synthesized by either CC12M or RedCaps. For\nStableRep, we first removed duplicate captions from each dataset, resulting in a reduced number of\ncaptions: from 10.0M to 8.3M for CC12M and from 11.7M to 10.5M for RedCaps. We compared\nStableRep to SimCLR, which was trained on either synthetic or original real images. We also included\nCLIP with a synthetic and a real version 2. For SimCLR and CLIP, we did not perform de-duplication\nfor either real or synthetic setting. We train for 35 epochs for all methods using ViT-B/16 (for\nStableRep, this refers to 35 SimCLR-equivalent epochs). We observed that CLIP started to overfit\naround 30 epochs. But StableRep did not overfit with this schedule (see Table 6c for results with\nlonger training). For StableRep, we additionally apply random downsample augmentation (see\nAppendix A.1 for details and how such downsample affects different methods).\nImageNet. Table 1 presents the results of linear probing on ImageNet. For StableRep, we prepend a\nBatchNorm layer without affine transformation to the linear classifier (see Appendix A.5 for more\ndetails). We observed that training SimCLR on synthetic images yields an improvement of 2.2%\ntop-1 accuracy on CC12M and 1.0% on RedCaps when compared to real images. However, the\naccuracy of CLIP drops by 2.6% on CC12M and 2.7% on RedCaps when trained on synthetic images\n(see Section 5 for more discussion). On the other hand, our method StableRep outperforms CLIP\ntrained on real images, with improvements of 3.2% and 2.6% for CC12M and RedCaps, respectively.\nReal\nSyn\nSimCLR CLIP\nSimCLR CLIP\nStableRep\nacc.\n61.5\n70.3\n63.7\n67.8\n73.5\n(a) CC12M\nReal\nSyn\nSimCLR CLIP\nSimCLR CLIP\nStableRep\nacc.\n61.8\n71.9\n62.8\n69.2\n74.5\n(b) RedCaps\nTable 1: Comparison under the linear probing protocol on ImageNet [15]; measuring top-1 accuracy on a frozen\npre-trained backbone. We compare our StableRep with SimCLR [10] and CLIP [58] with either synthetic or real\nimages, on CC12M [9] and RedCaps [16]. All models are pre-trained with 35 epochs using ViT-B/16 [18].\nLinear classification on more datasets. We followed the approach of SimCLR [10] and BYOL [25]\nto assess the generality of our learned representations across different image domains. Specifically,\nwe performed linear classification on 11 image classification datasets introduced by [40]. The results\nare reported in Table 2, and the relative performance is consistent with that on ImageNet. Notably,\nour proposed method, StableRep, achieves the highest accuracy on all of the 11 datasets.\nCIFAR-10\nCIFAR-100\nAircraft\nCars\nDTD\nFlowers\nPets\nSUN397\nCaltech-101\nFood-101\nVOC2007\nAverage\nReal\nSimCLR\n88.3\n70.3\n47.1\n45.5\n76.2\n92.5\n70.1\n65.4\n83.8\n75.0\n81.2\n72.3\nCLIP\n94.0\n79.0\n53.2\n75.8\n75.7\n96.0\n86.7\n72.5\n92.7\n81.6\n86.1\n81.2\nSyn\nSimCLR\n84.8\n65.2\n51.0\n53.2\n74.5\n93.3\n74.2\n65.0\n81.7\n74.8\n81.8\n72.7\nCLIP\n87.3\n69.5\n53.5\n79.5\n75.8\n95.4\n85.8\n69.2\n90.9\n78.3\n84.5\n79.1\nStableRep\n96.2\n84.1\n58.3\n80.9\n78.1\n97.2\n87.5\n73.0\n94.6\n83.6\n87.2\n83.7\nTable 2: Linear probing experiments on image datasets from various domains. Pre-training is conduceted on\nCC12M, with either synthetic or real images. Best results for each dataset are highlighted with bold.\n2We verified our CLIP implementation by comparing to prior work [51] on CC12M. With ViT-B/16, our\nCLIP achieved 40.2% zero-shot and 70.3% linear accuracy on ImageNet (v.s. 36.0% and 69.0% in [51]).\n6\nCIFAR-10\nCIFAR-100\nAircraft\nCars\nDTD\nFlowers\nPets\nSUN397\nCaltech-101\nFood-101\nAverage\nReal\nSimCLR\n64.0\n70.4\n40.7\n50.9\n82.2\n92.1\n74.4\n94.0\n90.4\n70.4\n73.0\nCLIP\n77.5\n82.1\n62.0\n90.9\n83.3\n97.6\n91.1\n97.2\n98.2\n87.0\n86.7\nSyn\nSimCLR\n50.0\n58.9\n45.2\n54.2\n79.8\n92.0\n74.6\n92.9\n89.1\n71.0\n70.8\nCLIP\n63.1\n73.5\n61.3\n92.5\n81.7\n96.9\n91.5\n96.7\n96.8\n82.5\n83.7\nStableRep\n92.3\n91.8\n62.6\n91.8\n86.4\n98.2\n91.7\n97.3\n98.8\n87.3\n89.8\nTable 3: Few-shot experiments. We report 5-way, 5-shot classification performance. Best results for each\ndataset are highlighted with bold.\nFew-shot image classification. Prior work [80, 83, 17] has shown that representation learning is the\nkey for few-shot image classification. A simple classifier on top of frozen representation is sufficient\nto achieve strong results. We perform 5-way, 5-shot classification following the setup in [83, 19]. As\nshown in Table 3, StableRep stands out on 9 out of the 10 datasets.\nMAE\nStableRep\nIN1k, Real\ncc12m, 35ep\ncc12m, 105ep\nredcaps, 35ep\nredcaps, 105ep\nmIoU\n48.1\n48.8\n49.4\n47.3\n48.4\nTable 4: ADE20k semantic segmentation (mIoU) using UperNet. StableRep models are trained by 35 or 105\nSimCLR-equivalent epochs.\nSemantic segmentation. We fine-tune pre-trained StableRep models on ADE20k [91] using Uper-\nNet [86]. For this evaluation, StableRep is pre-trained for 35 or 105 epochs. Table 4 shows that\nStableRep trained on synthetic data is able to outperform MAE trained on the real ImageNet images,\ndespite StableRep has no masked image modeling which benefits dense prediction tasks.\n4.2\nAblation analysis\nFor simplicity, ablation studies in this section do not use the random downsample augmentation in\npre-training or prepend an extra BatchNorm layer to the linear classifier.\nThe design choice of m (number of synthetic images per caption) is one of the key design choices for\nour approach. Therefore we study the following two factors relevant to m on CC3M captions (2.7\nmillion after de-duplication).\nl\n1\n2\n4\n6\n8\n10\nacc.\n61.2\n64.2\n65.6\n66.0\n66.2\n66.2\n(a) Given a generation budget T, we use T/l cap-\ntions and generate l images per caption. When l = 1,\nwe train a SimCLR model.\nm\n1\n2\n4\n6\n8\n10\nacc.\n60.5\n68.7\n69.6\n69.6\n69.8\n69.5\n(b) Given a batch size of C, we form each batch by\nsampling C/m captions and m images per caption.\nWe \u201cabuse\u201d m = 1 to represent SimCLR.\nTable 5: Ablation experiments on CC3M. ImageNet linear probing results with design choices relevant to\ndata generation parameter l, and batch sampling parameter m.\nFixed generation budget. We first study the question: given a fixed value for the number of total\nsynthetic images generated (T), should we generate more images per caption (l), and therefore use\nfewer captions (T/l) or the reverse. We assume an image budget of T = 2.7 million. During training,\nwe use the same total batch size (8192) for all l, and set the sampling parameter m as l. Table 5a\npresents the results. There is a clear benefit of generating more than 1 image per caption, e.g., l = 8\nimproves over l = 1 by 4.8%. But this benefit saturates around l = 10. We thus generate 10 images\nper caption for our final experiments.\nHow to form the batch. Suppose we have generated 10 images for each of the 2.7 million captions.\nNow given a fixed batch size, i.e., n\u2217m = C (recall that n is the number of captions, m is the number\nof images per caption, inside each batch), a larger m encourages stronger invariance of images from\nthe same caption, while larger n incorporates more negatives and thus encourages better separability\n7\n1\n2\n3\n4\n6\n8\n10\nguidance scale w\n20.0\n25.0\n30.0\n35.0\n40.0\nImageNet zero-shot Acc.\n22.9\n34.9\n33.4\n30.1\n25.0\n21.6\n18.8\n40.2\nSynthetic\nReal\nFigure 5: ImageNet zero-shot accuracy with different\nStable Diffusion generation guidance scale w, using\nCLIP as pre-training.\n1 3\n10\n20\n50\nNumber of Captions (M)\n60\n62\n64\n66\n68\n70\n72\n74\nImageNet Linear Acc.\n63.2\n69.6\n73.5\n73.9\n74.1\n60.6\n69.9\n71.5\n72.9\n5x Less\nStableRep+\nReal CLIP\nFigure 6: ImageNet linear probing accuracy compari-\nson between StableRep+ on synthetic images and CLIP\non real images on LAION subsets. For this experiment,\nonly 2 images are generated for each caption.\nof representations. To study this trade-off, we vary the sampling parameter m from 2 to 10 while\nkeeping n = C/m. As shown in Table 5b, The linear probing accuracy are similar between m = 4\nand m = 10 (peak at m = 8 with 69.8% accuracy), showing the robustness of StableRep w.r.t. m.\nWe choose m = 6 as our default setup. We \u201cabuse\u201d m = 1 to represent SimCLR.\nAfter the above study, we continue to ablate the following factors on CC12M and RedCaps.\nCase\nIN\navg.\nsmall\n72.8\n82.2\nlarge\n70.8\n80.4\nmixed\n71.9\n81.5\n(a) Guidance scale w. Smaller w\nyields better linear accuracy.\nSize\nIN\navg.\nViT-B/16\n72.8\n82.2\nViT-L/16\n74.7\n82.9\n(b) Model size.\nOur approach\nscales up with model size.\nEpochs CC12M RedCaps\n35\n72.8\n73.7\n70\n75.0\n76.3\n105\n75.7\n76.7\n(c) Training epochs. Longer train-\ning further improves accuracy.\nTable 6: Ablation experiments by pre-training on CC12M or RedCaps. We report linear probing accuracy on\nImageNet (IN) and/or average accuracy over the 11 fine-grained classification datasets (avg.). The colored cell\nindicates the default setup on each dataset: ViT-B/16 trained for 35 epochs with small guidance scale w.\nGuidance score for training.\nWe consider three configurations for the classifier free guidance\nscale w: (1) large scale \u2013 w \u2208 {8, 10}; (2) small scale \u2013 w \u2208 {2, 3}; (3) mixed scale \u2013 w \u2208\n{2, 3, 4, 5, 6, 8, 10, 12}. As shown in Table 6a, small scale gives the best linear transfer accuracy\non ImageNet and fine-grained classification datasets. This is possibly because smaller w leads to\nlarger intra-caption variation between generated images, which enforces StableRep to learn stronger\ninvariance. This is different from SimCLR which requires larger w (recall Section 2.1), as SimCLR\nonly models intra-image invariance and thus higher image quality (larger w) helps more.\nModel scale. We switch the backbone architecture to ViT-L/16. Table 6b presents the results. The\naccuracy improves by 1.9% on ImageNet linear probing and 0.7% on the average over fine-grained\nclassification datasets. We found that pre-training with ViT-L was unstable. The loss kept exploding\nto NaN, and we resumed from the checkpoint before NaN. But this led to a higher convergent loss than\nViT-B (ViT-L loss is lower before exploding). This may partly be due to the usage of BatchNorm.\nLonger training. To investigate the scaling behavior of StableRep w.r.t. training compute, we further\nincrease the pre-training computation budget to 2x and 3x epochs, and report the linear probing\naccuracy on ImageNet in Table 6c. The results indicate that StableRep scales well with longer\ntraining, e.g., improving by 2.2 for 2x and 2.9 for 3x on CC12M pre-training, and by 2.6 for 2x and\n3.0 for 3x on RedCaps pre-training.\n5\nAdding Language Supervision\nHow would training CLIP using synthetic images work? We study this question by generating a copy\n(one image per caption) for each guidance scale w in {1, 2, 3, 4, 6, 8, 10} and training CLIP using\n8\n\"Andrex Puppies \nOn A Roll\"\nSynthesized Image\nReal image\n\"Hand drawn textured \nvintage label, retro badge \nwith hammerhead shark \nvector illustration and \ninspirational lettering.\"\nReal caption\nFigure 7: Examples of misalign-\nment between input text and synthe-\nsized image, which can lead to subop-\ntimal performance for CLIP trained\non synthetic images. Upper: require\nheadhammer shark but Stable Diffu-\nsion often generates sharks without\nheadhammer; Lower: \u201cAndrex Pup-\npies\u201d is a brand of toilet rolls.\neach copy. Figure 5 plots the zero-shot ImageNet accuracy. Contrary to SSL methods, CLIP favors\nlower w. With the optimal w = 2, CLIP achieves 34.9% zero-shot accuracy. This is 5.4% lower than\ntraining on real images (40.2%). Such gap may be explained by misalignment between the generated\nimages and the input text, shown in Figure 7. This is especially true for fine-grained classes.\nWe can add language supervision to StableRep by adding 0.5 \u2217 (Li2t + Lt2i) to StableRep loss,\nwhere Li2t, Lt2i are image-to-text and text-to-image contrastive losses described by Eq. 4. Adding\nsupervision improves StableRep from 72.8% to 74.4% on CC12M and from 73.7% to 75.4% on\nRedCaps for ImageNet linear probing. We term it as StableRep+. We then further scale StableRep+\nto a randomly selected 50M subset of LAION-400M [70]. For this experiment, we only generate\n2 images per caption with w = 2, and train CLIP with real images and StableRep+ with synthetic\nimages using different scales of random subsets of the 50M data. We plot the results in Figure 6.\nStableRep+ consistently achieves better accuracy than CLIP. Noteably, StableRep+ with 10M captions\noutperforms CLIP with 50M captions, yielding a 5x time caption efficiency (2.5x image efficiency).\n5.1\nFairness and compositionality\nWe further study the fairness and compositional understanding of the learned models on FairFace [37]\nand ARO [89] benchmarks, respectively. The results are presented in Table 7.\nFairFace\nARO\npre-train data\nmean acc.\nbest-class acc.\nworst-class acc.\nrelation acc.\ncc12m\nCLIP\nReal\n28.2\n60.2\n0.3\n46.4\nSyn\n30.4\n64.0\n3.1\n50.0\nStableRep+\nSyn\n37.2\n74.9\n10.0\n47.3\nredcaps\nCLIP\nReal\n9.3\n31.1\n0.4\n59.0\nSyn\n22.3\n52.4\n1.0\n56.0\nStableRep+\nSyn\n27.3\n64.4\n2.1\n52.3\nTable 7: Results of fairness and compositionality evaluation.\nFairness. We perform zero-shot classificaton on FairFace. We jointly classify both races and genders,\ne.g., treating Black male, Black female, Indian female, and so on as different classes at the same\ntime. For cc12m models, CLIP with real data only achieved 0.3% accuracy with Southeast Asian\nmale class, and CLIP wth synthetic data improves this class to 3.1%, while our StableRep+ furthers\nit to 27.2%. For redcaps models, real CLIP only has 0.4% accuracy for East Asian Male, while\nStableRep+ improves this class to 22.8%. In summary, training with synthetic data is able to improve\nthe worst class accuracy. However, a obvious geographic bias still exists in all models.\nCompositionality. The results of compositionality evaluation are less clear. While training with\nsynthetic data on cc12m slightly improves the relational understanding, an accuracy drop is observed\nin models trained with synthetic data on redcaps. An in-depth investigation may be further needed.\n6\nRelated Work\nText-to-Image generative models. Text-to-image models trained on large image and text pairs have\nrecently enabled the creation of rich and diverse images encompassing many genres and themes\n[7, 61, 67, 88]. The resulting creations have become a sensation, with Stable Diffusion having\n9\nmillions of downloads and many tools for image manipulation built on top [66, 38, 90]. Most of these\nmodels are built on denoising diffusion models [31, 73] with some notable exceptions [8, 7]. In this\npaper, we leverage this latest generation of diffusion-based pre-trained generative models for the task\nof representation learning.\nVisual representation learning. Early approaches for visual representation learning often relied on\npretext tasks such as inpainting [56] to train image encoders. More recent advancements have shown\nthat mask image modeling, a form of self-supervised training, can be highly effective. In particular,\nMasked Autoencoder (MAE) [26] has demonstrated significant improvements in downstream fine-\ntuning performance. Another line of research focuses on contrastive learning, which aims to learn\nvisual representations by maximizing agreement between two augmented views of the same image\nwhile distinguishing it from negative examples [10, 78, 54, 84, 27, 79]. Meanwhile CLIP [58] and its\nsubsequent works [51] leverage contrastive learning to train image representations using language\nsupervision, leading to impressive transferability across various tasks.\nLearning from synthetic data. It has been common to train machine learning models with synthetic\ndata in different domains [72, 81, 14, 63, 44, 64, 50, 43, 76, 87, 29, 49]. In computer vision, synthetic\nimages have been used as a source for training models, such as optical flow [48, 23], autonomous\ndriving [1], semantic segmentation [12, 62], object detection [65, 57], human pose estimation [82, 34]\nor classification [2, 69, 28]. The closest set of work are the ones that conduct representation learning\non synthetic images [60, 45, 4, 35]. In [60], a model is trained to perform multi-task learning\non synthetic images. The main method in [45, 4, 35] is to manipulate the latent variable of deep\ngenerative models [45, 4, 35] or image generation procedures [4], to form meaningful synthetic\nimages for their representation learning methods. Our method falls into this category, but we use\ntext-to-image diffusion models, which have also been explored by [2, 28, 69]. The key difference is\nthat they conducted supervised learning while we use synthetic data for pre-training representations.\n7\nConclusion, Limitations and Broader Impact\nWe have shown that solely synthetic data generated from state of the art text-to-image models can\nbe used to train powerful visual representations. By harnessing the stochastic nature of Stable\nDiffusion in combination with a multi-positive contrastive loss, our approach yields a representation\nthat surpasses the performance achieved through training on real data alone. Through a series of\nexperiments, we establish that pre-training with synthetic datasets of varying scales yields impressive\nresults across different downstream tasks, including linear probing and few-shot classification.\nInterestingly, we discover that even vanilla self-supervised methods trained on synthetic data can\neither outperform or achieve comparable results to those trained on real data.\nDespite demonstrating the potential of training with synthetic data, this paper acknowledges its\nlimitations. Firstly, we have yet to comprehend the reasons behind the effectiveness of training\nself-supervised methods on synthetic images compared to an equal amount of real images. It is\npossible that this observation is confined to our particular evaluation methodology. Furthermore,\nthe current image generation process remains slow, with approximately 0.8s per image on a A100\nGPU or 2.2s per image on a V100 GPU while xFormers is enabled. Consequently, we are not able\nto train StableRep models with non-repetitive images synthesized online. Additionally, we have\nnot addressed the issue of semantic mismatch between the input prompts and the generated images,\nwhich may impact the quality and usefulness of the synthetic data. Moreover, synthetic data has\nthe potential to exacerbate biases due to mode collapse and a predisposition to output \u201cprototypical\u201d\nimages. Lastly, image attribution becomes a challenge when working with synthetic data.\nBroader impacts. This paper focuses on the fundamentals of visual representation learning, and we\nbelieve it will be beneficial to the practice of this field. Our method presents an immediate application\nby reducing the reliance on collecting a vast amount of real images for learning representations. This\napproach brings potential benefits in terms of cost-effectiveness and minimizing biases introduced\nthrough human collection and curation processes. However, it is important to acknowledge that our\nmethod relies on text-to-image generative models trained on large-scale, uncurated web data. Such\ndata may conceal social biases and errors that would have been exposed through human curation.\nAdditionally, we must recognize that the text prompts we employed are not completely bias-free; the\nselection of prompts influences the synthesized images. Thus, the choice of prompts assumes a role\nsimilar to the selection of real images for self-supervised visual representation learning.\n10\nAcknowledgements\nWe would like to thank anonymous reviewers and Shumeet Baluja for reviewing our manuscript and\nproviding many helpful comments and suggestions. We also appreciate the helpful discussions and\nthe general supports from the VisCam teammates in Google Research.\nReferences\n[1] Hassan Abu Alhaija, Siva Karthik Mustikovela, Lars Mescheder, Andreas Geiger, and Carsten\nRother. Augmented reality meets computer vision: Efficient data generation for urban driving\nscenes. IJCV, 2018. 10\n[2] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J\nFleet. Synthetic data from diffusion models improves imagenet classification. arXiv preprint\narXiv:2304.08466, 2023. 10\n[3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models\nwith an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 2\n[4] Manel Baradad Jurjo, Jonas Wulff, Tongzhou Wang, Phillip Isola, and Antonio Torralba.\nLearning to see by looking at noise. In NeurIPS, 2021. 10\n[5] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative\ncomponents with random forests. In ECCV, 2014. 19\n[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.\n3, 4, 16, 18\n[7] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan\nYang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image\ngeneration via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. 2, 9,\n10\n[8] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked\ngenerative image transformer. In CVPR, 2022. 10\n[9] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. 2, 3,\n5, 6\n[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework\nfor contrastive learning of visual representations. In ICML, 2020. 3, 6, 10, 16, 17, 18\n[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised\nvision transformers. In ICCV, 2021. 3, 4, 16, 18\n[12] Yuhua Chen, Wen Li, Xiaoran Chen, and Luc Van Gool. Learning semantic segmentation from\nsynthetic data: A geometrically guided input-output adaptation approach. In CVPR, 2019. 10\n[13] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.\nDescribing textures in the wild. In CVPR, 2014. 19\n[14] Yabo Dan, Yong Zhao, Xiang Li, Shaobo Li, Ming Hu, and Jianjun Hu. Generative adversarial\nnetworks (gan) based efficient sampling of chemical composition space for inverse design of\ninorganic materials. NPJ Computational Materials, 2020. 10\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR, 2009. 2, 3, 6\n[16] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text\ndata created by the people, for the people. arXiv preprint arXiv:2111.11431, 2021. 2, 5, 6\n11\n[17] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for\nfew-shot image classification. arXiv preprint arXiv:1909.02729, 2019. 7\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 3, 5, 6\n[19] Mohamed El Banani, Karan Desai, and Justin Johnson. Learning visual representations via\nlanguage-guided sampling. In CVPR, 2023. 7, 18\n[20] Linus Ericsson, Henry Gouk, and Timothy M Hospedales. How well do self-supervised models\ntransfer? In CVPR, 2021. 18\n[21] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.\nThe pascal visual object classes (voc) challenge. IJCV, 2010. 19\n[22] Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learning of object categories. TPAMI,\n2006. 19\n[23] Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip H\u00e4usser, Caner Haz\u0131rba\u00b8s, Vladimir Golkov,\nPatrick Van der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow\nwith convolutional networks. arXiv preprint arXiv:1504.06852, 2015. 10\n[24] Athinodoros S Georghiades, David J Kriegman, and PN Belhurneur. Illumination cones for\nrecognition under variable lighting: Faces. In CVPR, 1998. 2\n[25] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\net al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020.\n3, 4, 6, 16, 17, 18\n[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked\nautoencoders are scalable vision learners. In CVPR, 2022. 3, 10\n[27] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. arXiv:1911.05722, 2019. 10\n[28] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and\nXiaojuan Qi. Is synthetic data from generative models ready for image recognition? arXiv\npreprint arXiv:2210.07574, 2022. 10\n[29] Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari, and Mohammad Norouzi. Generate,\nannotate, and learn: Nlp with synthetic text. TACL, 2022. 10\n[30] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The \"wake-sleep\"\nalgorithm for unsupervised neural networks. Science, 1995. 2\n[31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In\nNeurIPS, 2020. 2, 10\n[32] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022. 3\n[33] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training\nby reducing internal covariate shift. In ICML, 2015. 5\n[34] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3. 6m: Large\nscale datasets and predictive methods for 3d human sensing in natural environments. TPAMI,\n2013. 10\n[35] Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola. Generative models as a data source\nfor multiview representation learning. arXiv preprint arXiv:2106.05258, 2021. 2, 10\n12\n[36] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and\nTaesung Park. Scaling up gans for text-to-image synthesis. arXiv preprint arXiv:2303.05511,\n2023. 2\n[37] Kimmo K\u00e4rkk\u00e4inen and Jungseock Joo. Fairface: Face attribute dataset for balanced race,\ngender, and age. arXiv preprint arXiv:1908.04913, 2019. 9\n[38] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri,\nand Michal Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint\narXiv:2210.09276, 2022. 10\n[39] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv:2004.11362,\n2020. 5\n[40] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better?\nIn CVPR, 2019. 6\n[41] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for\nfine-grained categorization. In ICCV workshop, 2013. 19\n[42] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.\nTech Report, 2009. 19\n[43] Varun Kumar, Ashutosh Choudhary, and Eunah Cho. Data augmentation using pre-trained\ntransformer models. arXiv preprint arXiv:2003.02245, 2020. 10\n[44] Jason Li, Ravi Gadde, Boris Ginsburg, and Vitaly Lavrukhin. Training neural speech recognition\nsystems with synthetic speech augmentation. arXiv preprint arXiv:1811.00707, 2018. 10\n[45] Hao Liu, Tom Zahavy, Volodymyr Mnih, and Satinder Singh. Palm up: Playing in the latent\nmanifold for unsupervised pretraining. arXiv preprint arXiv:2210.10913, 2022. 10\n[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017. 6\n[47] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-\ngrained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 19\n[48] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy,\nand Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow,\nand scene flow estimation. In CVPR, 2016. 10\n[49] Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Generating training data with language\nmodels: Towards zero-shot language understanding. arXiv preprint arXiv:2202.04538, 2022.\n10\n[50] Masato Mimura, Sei Ueno, Hirofumi Inaguma, Shinsuke Sakai, and Tatsuya Kawahara. Leverag-\ning sequence-to-sequence speech synthesis for enhancing acoustic-to-word speech recognition.\nIn SLT, 2018. 10\n[51] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets\nlanguage-image pre-training. In ECCV, 2022. 6, 10, 18\n[52] Sameer A Nene, Shree K Nayar, Hiroshi Murase, et al. Columbia object image library (coil-20).\nTech Report, 1996. 2\n[53] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large\nnumber of classes. In Indian Conference on Computer Vision, Graphics & Image Processing,\n2008. 19\n[54] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive\npredictive coding. arXiv preprint arXiv:1807.03748, 2018. 5, 10\n13\n[55] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In\nCVPR, 2012. 19\n[56] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context\nencoders: Feature learning by inpainting. In CVPR, 2016. 10\n[57] Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko. Learning deep object detectors\nfrom 3d models. In CVPR, 2015. 10\n[58] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021. 2, 6, 10, 18, 19\n[59] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2\n[60] Zhongzheng Ren and Yong Jae Lee. Cross-domain self-supervised multi-task feature learning\nusing synthetic imagery. In CVPR, 2018. 10\n[61] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 2, 9, 20\n[62] German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The\nsynthia dataset: A large collection of synthetic images for semantic segmentation of urban\nscenes. In CVPR, 2016. 10\n[63] Andrew Rosenberg, Yu Zhang, Bhuvana Ramabhadran, Ye Jia, Pedro Moreno, Yonghui Wu,\nand Zelin Wu. Speech recognition with augmented synthesized speech. In ASRU, 2019. 10\n[64] Nick Rossenbach, Albert Zeyer, Ralf Schl\u00fcter, and Hermann Ney. Generating synthetic audio\ndata for attention-based speech recognition systems. In ICASSP, 2020. 10\n[65] Artem Rozantsev, Vincent Lepetit, and Pascal Fua. On rendering synthetic images for training\nan object detector. Computer Vision and Image Understanding, 2015. 10\n[66] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\npreprint arXiv:2208.12242, 2022. 10\n[67] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. In NeurIPS, 2022. 2, 9\n[68] Ferdinando S Samaria and Andy C Harter. Parameterisation of a stochastic model for human\nface identification. In IEEE workshop on applications of computer vision, 1994. 2\n[69] Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and Yannis Kalantidis. Fake it till you\nmake it: Learning transferable representations from synthetic imagenet clones. In CVPR, 2023.\n10\n[70] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open\ndataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 9\n[71] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018. 3, 5\n[72] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur\nGuez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of\ngo without human knowledge. Nature, 2017. 10\n[73] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. In ICML, 2015. 2, 10\n14\n[74] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020. 20\n[75] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM\nSigart Bulletin, 1991. 2\n[76] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model.\nStanford Center for Research on Foundation Models., 2023. 10\n[77] Yonglong Tian, Olivier J Henaff, and A\u00e4ron van den Oord. Divide and contrast: Self-supervised\nlearning from uncurated data. In ICCV, 2021. 3\n[78] Yonglong Tian, Dilip Krishnan, and Phillip Isola.\nContrastive multiview coding.\narXiv:1906.05849, 2019. 10, 18\n[79] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What\nmakes for good views for contrastive learning? In NeurIPS, 2020. 10\n[80] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking\nfew-shot image classification: a good embedding is all you need? In ECCV, 2020. 7\n[81] Allan Tucker, Zhenchen Wang, Ylenia Rotalinti, and Puja Myles. Generating high-fidelity\nsynthetic patient data for assessing machine learning healthcare software. NPJ digital medicine,\n2020. 10\n[82] Gul Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J Black, Ivan Laptev,\nand Cordelia Schmid. Learning from synthetic humans. In CVPR, 2017. 10\n[83] Yan Wang, Wei-Lun Chao, Kilian Q Weinberger, and Laurens van der Maaten. Simpleshot: Re-\nvisiting nearest-neighbor classification for few-shot learning. arXiv preprint arXiv:1911.04623,\n2019. 7\n[84] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via\nnon-parametric instance discrimination. In CVPR, 2018. 10\n[85] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\nLarge-scale scene recognition from abbey to zoo. In CVPR, 2010. 19\n[86] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing\nfor scene understanding. In ECCV, 2018. 7\n[87] Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-\nPing Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. Generative data augmentation\nfor commonsense reasoning. arXiv preprint arXiv:2004.11546, 2020. 10\n[88] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\nmodels for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 2, 9\n[89] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When\nand why vision-language models behave like bags-of-words, and what to do about it? In ICLR,\n2022. 9\n[90] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.05543, 2023. 10\n[91] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio\nTorralba. Semantic understanding of scenes through the ade20k dataset. IJCV, 2019. 7\n15\nA\nImplementation Details\nA.1\nDownsample augmentation\nSynthetic images have constant high resolutions (e.g., 512\u00d7512 for Stable Diffusion). We find this\nleads to a domain gap when transferring to situations involving low resolution images, such as CIFAR-\n10 or CIFAR-100. To address this issue, we introduce Random Downsample augmentation, which\nrandomly resizes images to a resolution of 64 or 128 (equally probable) and then resizes them back\nto 224. During pre-training, we apply this augmentation with a probability of 0.05 and prepend it to\nother augmentations.\nIn Table 8, we ablate the effects of applying this random downsample augmentation to different pre-\ntraining methods. This augmentation brings significant improvements on CIFAR-10 and CIFAR-100\ndatasets, while maintaining the performance on other datasets. On average this augmentation is more\nbeneficial for pre-training with synthetic images than real ones.\nDownsample\nCIFAR-10\nCIFAR-100\nAircraft\nCars\nDTD\nFlowers\nPets\nSUN397\nCaltech-101\nFood-101\nVOC2007\nAverage\nReal\nSimCLR\n88.3\n70.3\n47.1\n45.5\n76.2\n92.5\n70.1\n65.4\n83.8\n75.0\n81.2\n72.3\nSimCLR\n\u2713\n92.3\n75.4\n47.8\n44.4\n77.1\n91.8\n69.2\n65.1\n84.7\n74.9\n81.2\n73.1 (+0.8)\nCLIP\n94.0\n79.0\n53.2\n75.8\n75.7\n96.0\n86.7\n72.5\n92.7\n81.6\n86.1\n81.2\nCLIP\n\u2713\n95.8\n82.9\n51.5\n76.5\n74.7\n95.3\n87.2\n72.5\n92.7\n81.7\n86.2\n81.5 (+0.3)\nSyn\nSimCLR\n84.8\n65.2\n51.0\n53.2\n74.5\n93.3\n74.2\n65.0\n81.7\n74.8\n81.8\n72.7\nSimCLR\n\u2713\n92.6\n76.2\n50.4\n53.2\n74.7\n93.3\n74.7\n64.8\n86.0\n74.7\n81.2\n74.7 (+2.0)\nCLIP\n87.3\n69.5\n53.5\n79.5\n75.8\n95.4\n85.8\n69.2\n90.9\n78.3\n84.5\n79.1\nCLIP\n\u2713\n93.4\n78.8\n52.4\n79.6\n75.1\n95.0\n85.0\n69.5\n90.9\n78.4\n84.7\n80.3 (+1.2)\nStableRep\n90.7\n74.4\n57.6\n80.3\n79.0\n96.7\n87.1\n73.2\n94.0\n83.5\n87.2\n82.2\nStableRep \u2713\n96.2\n84.1\n58.3\n80.9\n78.1\n97.2\n87.5\n73.0\n94.6\n83.6\n87.2\n83.7 (+1.5)\nTable 8: We ablate the effects of Random Downsample augmentation on linear probing benchmarks from\nvarious domains. This augmentation brings significant improvements on CIFAR-10 and CIFAR-100 datasets,\nwhile maintaining the performance on other datasets.\nA.2\nStandard self-supervised learning\nWe follow the default settings for standard self-supervised learning algorithms, and present the\ntraining details in Table 9 and Table 10. We use the linear lr scaling rule: lr = base_lr \u00d7 bsz/256.\nFor BYOL [25], we did not follow the hyperparameters (blr = 1.0e\u22124, wd = 0.03) in [11], as we\nfound our setting here yielded better accuracy. For DINO [6], we did not use the multi-crop strategy\nand only pre-trained the model with two 224\u00d7224 crops.\nconfig\nMAE\nSimCLR\noptimizer\nAdamW\nAdamW\nbase learning rate\n1.5e-4\n2.0e-4\nweight decay\n0.05\n0.1\noptimizer momentum\n\u03b21, \u03b22=0.9, 0.95\n\u03b21, \u03b22=0.9, 0.98\nbatch size\n4096\n4096\nlearning rate schedule\ncosine decay\ncosine decay\nepochs\n300 (cc3m) / 80 (cc12m)\n100 (cc3m) / 35 (cc12m)\nwarmup epochs\n10 (cc3m) / 4 (cc12m)\n5 (cc3m) / 1 (cc12m)\naugmentation\nRandomResizedCrop, Flip\nSimCLR Aug. [10]\nTable 9: Self-supervised pre-training settings. MAE and SimCLR.\n16\nconfig\nDINO\nBYOL/MoCo-v3\noptimizer\nAdamW\nAdamW\nbase learning rate\n5.0e-4\n1.5e-4\nweight decay\n0.04 to 0.4, cosine\n0.1\noptimizer momentum\n\u03b21, \u03b22=0.9, 0.999\n\u03b21, \u03b22=0.9, 0.95\nbatch size\n4096\n4096\nlearning rate schedule\ncosine decay\ncosine decay\nepochs\n100 (cc3m) / 35 (cc12m)\n100 (cc3m) / 35 (cc12m)\nwarmup epochs\n5 (cc3m) / 2 (cc12m)\n5 (cc3m) / 2 (cc12m)\nmomentum update \u03bb\n0.996 to 1, cosine\n0.996 to 1, cosine\naugmentation\nBYOL Aug. [25]\nBYOL Aug. [25]\nteacher temp. \u03c4t\n0.04 to 0.07 in warmup\nstudent temp. \u03c4s\n0.1\nTable 10: Self-supervised pre-training settings. DINO, BYOL and MoCo v3.\nA.3\nStableRep pre-training\nThe hyperparameterss for StableRep is presented in Table 11. Indeed, they are the same as that in\nSimCLR. The difference is that the base_lr in StableRep is for 512 images while in SimCLR it is for\n256 images, because each image in StableRep only has one single crop. We ended up using a batch\nsize of 8256 images, since we trained our model with 32 GPUs and 8192 is not divisible over 32\u00d76.\nThe computation for StableRep has been converted to SimCLR-equivalent epochs.\nconfig\nStableRep\nbatch size\n8256 (m = 6, n = 1376)\noptimizer\nAdamW\nbase learning rate\n2.0e-4\npeak learning rate\nbase_lr \u00d7 bsz/512\nweight decay\n0.1\noptimizer momentum\n\u03b21, \u03b22=0.9, 0.98\nlearning rate schedule\ncosine decay\nepochs\n35 / 70 / 105\nwarmup epochs\n1.2 / 2.3 / 3.5\naugmentation\nDownsample Aug. + SimCLR Aug. [10]\nTable 11: StableRep pre-training settings.\nA.4\nCLIP training\nconfig\nCLIP\nbatch size\n8192\noptimizer\nAdamW\npeak learning rate\n1e-3\nweight decay\n0.5\noptimizer momentum\n\u03b21, \u03b22=0.9, 0.98\nlearning rate schedule\ncosine decay\nepochs\n35\nwarmup epochs\n1\naugmentation\nRandomResizedCrop(scale=(0.5, 1.0))\nTable 12: CLIP training settings.\n17\nModel\nPatch\nInput\nEmbedding\nVision Transformer\nText Transformer\nVocab\nText\nsize\nresolution\ndimension\nLayers\nWidth\nHeads\nLayers\nWidth\nHeads\nsize\nlength\nViT-B/16\n16\n224\n512\n12\n768\n12\n12\n512\n8\n49,408\n77\nTable 13: CLIP encoder details.\nWe follow the hyperparameter setting used in [51] since it is better than that from the original\nCLIP [58] paper. Table 12 summarizes the training details, and Table 13 presents the architecture\nof CLIP encoders. With this training setup, we are able to produce 40.2% ImageNet zero-shot\naccuracy when training CLIP on CC12M dataset. As a comparison, [51] reports 36.0% using the\nsame architecutre.\nA.5\nImageNet linear probing\nWe follow prior work [11, 6] to train the linear classifier. It has been generally observed that regular-\nization such as weight decay hurts the performance [78]. Following [78, 11], we set weight decay as\n0, and only use RandomResizedCrop and RandomHorizontalFlip as data augmentation.\nWe sweep the base_lr over {0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50} \u00d7 10\u22122.\nconfig\nvalue\nbatch size\n1024\noptimizer\nSGD\nbase learning rate\nsweep\nweight decay\n0\noptimizer momentum\n0.9\nlearning rate schedule\ncosine decay\nepochs\n90\naugmentation\nRandomResizedCrop, Flip\nTable 14: ImageNet linear probing settings.\nFor StableRep trained with 35 epochs, we find that adding an extra BatchNorm layer without affine\ntransformation improves and stablizes the linear probing results. However, this additional BatchNorm\ndoes not help when StableRep is trained with a longer schedule, e.g., 105 epochs. We conjecture that\nBatchNorm is helpful when StableRep is not convergent, and present the comparison in Table 15.\nw/ BN\nw/o BN\nStableRep, 35 epochs\n73.5\n71.4\nStableRep, 105 epochs\n75.2\n75.4\nTable 15: ImageNet linear probing results w/ or w/o extra BatchNorm layer for the linear classifier.\nA.6\nFine-grained linear classification\nFollowing [10, 25, 20], we fit a regularized multinomial logistic regression model on top of the\nfrozen CLS token. In training and testing, we do not perform any data augmentation; images are\nresized to 224 pixels along the shorter side using bicubic resampling, followed by a center crop of\n224\u00d7224. We minimize the cross-entropy objective using L-BFGS with \u21132-regularization. We select\nthis \u21132-regularization constant on the validation set over 45 logarithmically spaced values between\n10\u22126 and 105. The maximum number of L-BFGS iterations is set to 500.\nThe details about the fine-grained classification datasets are presented in Table 16.\nA.7\nFew-shot image classification\nFollowing the settings in [20, 19], we evaluate the 5-way 5-shot performance on 10 different datasets.\nWe do not use data augmentation; images are resized to 224 pixels along the shorter side using\n18\nDataset\nMetric\nCategories\nTrain Size\nTest Size\nCIFAR-10 [42]\nAccuracy\n10\n50,000\n10,000\nCIFAR-100 [42]\nAccuracy\n100\n50,000\n10,000\nAircraft [47]\nMean per class\n100\n6,667\n3,333\nCars [41]\nAccuracy\n196\n8,144\n8,041\nDTD [13]\nAccuracy\n47\n3,760\n1,880\nFlowers [53]\nMean per class\n102\n2,040\n6,149\nPets [55]\nMean per class\n37\n3,680\n3,669\nSUN397 [85]\nAccuracy\n397\n19,850\n19,850\nCaltech-101 [22]\nMean per class\n102\n3,060\n6,085\nFood-101 [5]\nAccuracy\n101\n75,750\n25,250\nVOC2007 [21]\nMean per class\n20\n5,011\n4,952\nTable 16: Details of the fine-grained linear classification datasets.\nbicubic resampling, followed by a center crop of 224\u00d7224. We report the mean accuracy of 600\nrandomly sampled tasks (also known as episodes). For each task, images are randomly sampled from\nthe combination of training, validation and testing sets. We sample 15 query images for each class in\nevery task for evaluation purpose.\nB\nAdditional Results\nB.1\nFine-grained classification\nCIFAR-10\nCIFAR-100\nAircraft\nCars\nDTD\nFlowers\nPets\nSUN397\nCaltech-101\nFood-101\nVOC2007\nAverage\nPre-training on Redcaps for 35 epochs\nReal\nSimCLR\n90.2\n72.0\n46.8\n42.8\n77.9\n94.6\n83.0\n61.2\n82.7\n81.3\n80.9\n73.9\nCLIP\n94.2\n78.9\n52.9\n74.9\n73.9\n97.8\n91.6\n66.2\n91.6\n89.2\n85.4\n81.5\nSyn\nSimCLR\n85.1\n65.4\n48.7\n53.7\n74.6\n95.0\n79.6\n61.8\n84.5\n79.7\n80.4\n73.5\nCLIP\n88.7\n71.4\n53.7\n77.3\n76.0\n96.9\n88.2\n67.3\n90.3\n83.7\n84.5\n79.8\nStableRep\n96.7\n84.6\n57.2\n78.8\n79.0\n98.4\n90.9\n70.7\n94.9\n88.1\n86.6\n84.2\nLonger training for StableRep\ncc12m\n35 epochs\n96.2\n84.1\n58.3\n80.9\n78.1\n97.2\n87.5\n73.0\n94.6\n83.6\n87.2\n83.7\n105 epochs\n96.7\n84.7\n59.2\n83.5\n80.1\n97.3\n88.3\n74.3\n94.7\n85.1\n87.9\n84.7\nredcaps\n35 epochs\n96.7\n84.6\n57.2\n78.8\n79.0\n98.4\n90.9\n70.7\n94.9\n88.1\n86.6\n84.2\n105 epochs\n96.9\n85.6\n60.2\n83.9\n80.0\n98.5\n91.7\n72.5\n94.8\n89.4\n87.8\n85.6\nOpenAI\u2019s CLIP trained on WIT-400M (numbers copied from [58])\nWIT-\n400M\n96.2\n83.1\n59.5\n86.7\n79.2\n98.1\n93.1\n78.4\n94.7\n92.8\n89.2\n86.5\nTable 17: Linear transfer results on fine-grained datasets. All results are with ViT-B/16. Results of StableRep\nare marked . Upper: different methods pre-trained on RedCaps. Middle: StableRep with different training\nschedules. Lower: OpenAI\u2019s CLIP trained on WIT-400M dataset. Our StableRep trained with synthetic images\nonly is approaching the performance of CLIP trained with 400 millions of real images.\nIn Table 17, we further present the fine-grained linear classification results by models from RedCaps or\nmodels that are trained longer (2x or 3x longer). When pre-training on RedCaps, StableRep achieves\nthe best average accuracy. Longer training of StableRep further improves transferability. Notably,\nour StableRep trained with synthetic images only is approaching the performance of OpenAI\u2019s CLIP\ntrained with 400 millions of real images.\n19\nB.2\nFew-shot image classification\nWe further summarizes the few-shot image classification results in Table 18. The 95% confidence\ninterval is provided. StableRep stands out on the majority of the evaluated datasets.\nCIFAR-10\nCIFAR-100\nAircraft\nCars\nDTD\nFlowers\nPets\nSUN397\nCaltech-101\nFood-101\nAverage\nPre-training on cc12m\nReal\nSimCLR\n64.0\u00b10.7 70.4\u00b10.8 40.7\u00b10.9 50.9\u00b10.8 82.2\u00b10.6 92.1\u00b10.5 74.4\u00b10.8 94.0\u00b10.4 90.4\u00b10.5 70.4\u00b10.7\n73.0\nCLIP\n77.5\u00b10.6 82.1\u00b10.7 62.0\u00b11.0 90.9\u00b10.5 83.3\u00b10.6 97.6\u00b10.2 91.1\u00b10.5 97.2\u00b10.2 98.2\u00b10.2 87.0\u00b10.5\n86.7\nSyn\nSimCLR\n50.0\u00b10.6 58.9\u00b10.8 45.2\u00b11.0 54.2\u00b10.8 79.8\u00b10.6 92.0\u00b10.5 74.6\u00b10.8 92.9\u00b10.4 89.1\u00b10.6 71.0\u00b10.7\n70.8\nCLIP\n63.1\u00b10.6 73.5\u00b10.7 61.3\u00b11.0 92.5\u00b10.4 81.7\u00b10.6 96.9\u00b10.3 91.5\u00b10.5 96.7\u00b10.2 96.8\u00b10.3 82.5\u00b10.6\n83.7\nStableRep 92.3\u00b10.3 91.8\u00b10.5 62.6\u00b11.0 91.8\u00b10.5 86.4\u00b10.5 98.2\u00b10.2 91.7\u00b10.5 97.3\u00b10.2 98.8\u00b10.2 87.3\u00b10.5\n89.8\nPre-training on redcaps\nReal\nSimCLR\n62.3\u00b10.6 69.4\u00b10.7 39.6\u00b10.9 51.0\u00b10.8 82.7\u00b10.6 94.8\u00b10.4 85.4\u00b10.6 91.8\u00b10.5 88.5\u00b10.6 79.1\u00b10.7\n74.5\nCLIP\n80.6\u00b10.5 85.3\u00b10.6 54.5\u00b10.9 88.5\u00b10.6 82.6\u00b10.6 99.0\u00b10.1 94.5\u00b10.4 95.9\u00b10.3 97.8\u00b10.2 94.4\u00b10.3\n87.3\nSyn\nSimCLR\n52.9\u00b10.6 60.8\u00b10.8 40.9\u00b10.9 53.2\u00b10.8 79.5\u00b10.6 94.3\u00b10.4 78.3\u00b10.7 92.0\u00b10.4 88.9\u00b10.5 75.9\u00b10.7\n71.7\nCLIP\n65.7\u00b10.6 75.7\u00b10.7 55.2\u00b11.0 90.1\u00b10.5 82.6\u00b10.6 98.2\u00b10.2 92.0\u00b10.5 96.3\u00b10.3 96.9\u00b10.3 88.1\u00b10.5\n84.1\nStableRep 92.7\u00b10.3 92.9\u00b10.4 57.3\u00b11.0 89.4\u00b10.6 86.2\u00b10.5 99.2\u00b10.1 94.5\u00b10.4 96.8\u00b10.3 98.9\u00b10.2 91.8\u00b10.4\n90.0\nTable 18: Few-shot image classification results with 95% confidence interval provided. All models here are\ntrained for 35 epochs. Upper: pre-training on CC12M dataset. Upper: pre-training on RedCaps dataset.\nC\nImage Generation\nC.1\nImplementation details\nWe use Stable Diffusion [61] v1.5. During sampling, we generate images by 50 DDIM [74] steps. To\naccelerate the generation process, we leverage xFormers library for efficient attention computation,\nwhich brings down the sampling time to \u223c0.8s per image on a single A100 GPU and \u223c2.3s per image\non a V100 GPU.\nImage resolution. The image resolution may affect the quality of representations learned by self-\nsupervised learning algorithms. We try to make a relative fair comparison by storing all synthetic and\nreal images in similar resolutions. The synthetic images generated by Stable Diffusion are 512\u00d7512;\nwe resized them to 256\u00d7256 before storing them on the disk. The real images have various sizes,\nranging from less than a hundred of pixels in shorter side to thousands of pixels; we resize the shorter\nside of all real images to 256.\nC.2\nGeneration examples\nSome examples of synthetic images are visualized in Figure 8.\nD\nComputation\nSynthesis. The slowest part of the StableRep pipeline is the image generation. We use 512 V100\nGPUs to synthesize images, which takes \u223c13 hours for every ten million images.\nPre-training. Each of our StableRep models with ViT-B/16 is trained on 4 nodes, each of which has\n8 A100 GPUs and 96 CPU cores. It takes \u223c20 hours to complete 35 SimCLR-equivalent epochs of\ntraining on CC12M and \u223c23 hours on RedCaps. For ViT-L/16, we use 64 A100 80GB GPUs spread\nover 8 nodes.\n20\n2\n4\n6\n8\n10\n12\n20\n40\n60\nreal\nactor arrives at the premiere\nfootball player and battle for the ball\nillustration of a cat embracing a planet\nlittle kid jumping from the office chair\nFigure 8: Examples of synthetic images. We show examples for 4 different text prompts. For each prompt, we\nprovide examples synthesized with different guidance scale w, as well as the original real image.\n21\n"
  },
  {
    "title": "ViCo: Detail-Preserving Visual Condition for Personalized Text-to-Image Generation",
    "link": "https://arxiv.org/pdf/2306.00971.pdf",
    "upvote": "2",
    "text": "VICO: PLUG-AND-PLAY VISUAL CONDITION FOR PER-\nSONALIZED TEXT-TO-IMAGE GENERATION\nShaozhe Hao\nKai Han\u2217\nShihao Zhao\nKwan-Yee K. Wong\nThe University of Hong Kong\n{szhao,shzhao,kykwong}@cs.hku.hk\nkaihanx@hku.hk\nABSTRACT\nPersonalized text-to-image generation using diffusion models has recently emerged\nand garnered significant interest. This task learns a novel concept (e.g., a unique\ntoy), illustrated in a handful of images, into a generative model that captures fine\nvisual details and generates photorealistic images based on textual embeddings.\nIn this paper, we present ViCo, a novel lightweight plug-and-play method that\nseamlessly integrates visual condition into personalized text-to-image generation.\nViCo stands out for its unique feature of not requiring any fine-tuning of the\noriginal diffusion model parameters, thereby facilitating more flexible and scalable\nmodel deployment. This key advantage distinguishes ViCo from most existing\nmodels that necessitate partial or full diffusion fine-tuning. ViCo incorporates\nan image attention module that conditions the diffusion process on patch-wise\nvisual semantics, and an attention-based object mask that comes at no extra cost\nfrom the attention module. Despite only requiring light parameter training (\u223c6%\ncompared to the diffusion U-Net), ViCo delivers performance that is on par with,\nor even surpasses, all state-of-the-art models, both qualitatively and quantitatively.\nThis underscores the efficacy of ViCo, making it a highly promising solution\nfor personalized text-to-image generation without the need for diffusion model\nfine-tuning. Code: https://github.com/haoosz/ViCo\n1\nINTRODUCTION\nNowadays, people can easily generate unprecedentedly high-quality photorealistic images with\ntext prompts using fast-growing text-to-image diffusion models (Ho et al., 2020; Song et al., 2021;\nRamesh et al., 2022; Nichol et al., 2022; Saharia et al., 2022; Rombach et al., 2022). However,\nthese models are trained on a text corpus of seen words, and they fail to synthesize novel concepts\nlike a special-looking dog or your Batman toy collection. Imagine how fascinating it would be if\nyour plastic Batman toy could appear in scenes of the original \u2018Batman\u2019 movie. Recent works (Gal\net al., 2023a; Ruiz et al., 2023; Kumari et al., 2023) make this fantasy come true, terming the task\npersonalized text-to-image generation. Specifically, given several images of a unique object, the goal\nis to capture the object and reconstruct it in text-guided image generation.\nDreamBooth (Ruiz et al., 2023) incorporates a unique identifier before the category word in the text\nembedding space and finetunes the entire diffusion model during training. The authors also finetune\nthe text encoder, which empirically shows improved performance. Custom Diffusion (Kumari et al.,\n2023) finds that only tuning a few parameters, i.e., key and value projection matrices, is sufficiently\npowerful. DreamBooth and Custom Diffusion both meet the issue of language drift (Lee et al.,\n2019; Lu et al., 2020) because finetuning the pretrained model on new data can lead to a loss of\nthe preformed language knowledge. They leverage a preservation loss to address this problem,\nwhich requires manually generating (Ruiz et al., 2023) or retrieving massive class-specific images.\nTextual Inversion (Gal et al., 2023a) adopts minimal optimization by exclusively learning a novel\ntext embedding to represent the given object, showing enhanced performance using latent diffusion\nmodels (Rombach et al., 2022). For the more powerful Stable Diffusion, however, the learned\nembedding struggles to express fine details of the visual object, and the generated results are prone\nto overfitting to training samples due to the limited fine-grained expressiveness of CLIP (Radford\net al., 2021). In this work, we follow (Gal et al., 2023a) to use a single learnable token embedding S\u22c6\n\u2217Corresponding author\n1\narXiv:2306.00971v2  [cs.CV]  7 Dec 2023\nA \ud835\udc46\u2217 in a dream of a \ndistant galaxy\nA \ud835\udc46\u2217 in a firefighter \noutfit\nA \ud835\udc46\u2217 in a luxurious \ninterior living room\nA \ud835\udc46\u2217 wearing a \nrainbow scarf\nA \ud835\udc46\u2217 wearing a \nSanta hat\nA \ud835\udc46\u2217 among the \nskyscrapers\nA \ud835\udc46\u2217 floating on top \nof water\nA \ud835\udc46\u2217 on a \ncobblestone street\nA \ud835\udc46\u2217 with a tree \nand autumn leaves\nA \ud835\udc46\u2217 under an \nastonishing sunset\nTraining: \ud835\udc46\u2217\nTraining: \ud835\udc46\u2217\nInfer\nInfer\nFigure 1: Personalized text-to-image generation. Generated images of the Batman toy (top) and\nthe Toller (bottom) by ViCo. S\u22c6 denotes the learnable text embedding (Gal et al., 2023a).\nto represent the novel concept instead of the form of \u201c[V] class\u201d used in (Ruiz et al., 2023; Kumari\net al., 2023). In our vision, a single token embedding should be capable of effectively representing\nany visual concept within an ideal unified text-image space.\nTo overcome the issue of declined model expressiveness of novel concepts, we propose a novel plug-in\nmethod that integrates visual conditions into the diffusion process, which harnesses the inherent\nrichness of diffusion models. Specifically, we present an image cross-attention module, facilitating\nthe seamless integration of intermediate features from a reference image, which are extracted by the\noriginal denoising U-Net, into the denoising process. Our method stands out for not requiring any\nmodifications or fine-tuning of any layers in the original diffusion model, setting it apart from most\nexisting methods like DreamBooth (Ruiz et al., 2023) and Custom Diffusion (Kumari et al., 2023).\nAs the language knowledge remains intact without requiring fine-tuning of the diffusion model, our\nmethod avoids the problem of language drift, which eliminates the need for heavy preprocessing like\nimage generation (Ruiz et al., 2023) and retrieval (Kumari et al., 2023).\nAnother challenge we address is the difficulty in isolating the foreground object of interest from the\nbackground. Instead of relying on prior annotated masks as in concurrent works (Wei et al., 2023;\nShi et al., 2023; Jia et al., 2023), we propose an automatic mechanism to generate object masks that\nare naturally incorporated into the denoising process. Specifically, we leverage the notable semantic\ncorrelations between text and image in cross-attentions (Hertz et al., 2023) and utilize the cross-\nattention map associated with the learnable object token to generate an object mask. Our method is\ncomputationally efficient, non-parametric, and online, and can effectively suppress the influence of\ndistracting backgrounds in the training samples. We also design an easy-to-employ regularization\nbetween the cross-attention maps associated with the end-of-text token and the learnable token to\nhelp refine the object mask.\nWe name our model ViCo, which offers a number of advantages over previous works. (1) It is\nfast (\u223c6 minutes) and lightweight (6% of diffusion U-Net). (2) It is plug-and-play and requires no\nfine-tuning of the original diffusion model, allowing highly flexible and transferable deployment.\n(3) It is easy to implement and use, requiring no heavy preprocessing or mask annotations. (4) It can\npreserve fine object-specific details of the novel concept in text-guided generation (see Fig. 1).\nOur contributions include: (1) proposing an image cross-attention module to integrate visual condi-\ntions into the denoising process for capturing object-specific semantics; (2) introducing an automatic\nobject mask generation mechanism from the cross-attention map; (3) providing quantitative and\nqualitative comparisons with state-of-the-art methods (Ruiz et al., 2023; Kumari et al., 2023; Gal\net al., 2023a) and demonstrating the efficiency of ViCo in multiple applications.\n2\nRELATED WORK\nText-to-image synthesis. In the literature of GANs (Goodfellow et al., 2014; Brock et al., 2019;\nKarras et al., 2019; 2020; 2021), plenty of works have gained remarkable progress in text-to-image\ngeneration (Reed et al., 2016; Zhu et al., 2019; Tao et al., 2022; Xu et al., 2018; Zhang et al., 2021;\nYe et al., 2021) and image manipulation using text (Gal et al., 2022; Patashnik et al., 2021; Xia\net al., 2021; Abdal et al., 2022), advancing the generation of images conditioned on plain text.\n2\nThese methods are trained on a fixed dataset that leverages strong prior knowledge of a specific\ndomain. Towards a zero-shot fashion, auto-regressive models (Ramesh et al., 2021; Yu et al., 2022)\ntrained on large-scale data of text-image pairs achieve high-quality and content-rich text-to-image\ngeneration results. Based on the pretrained CLIP (Radford et al., 2021), Crowson et al. (Crowson\net al., 2022) applies CLIP similarity to optimize the generated image at test time without any training.\nThe use of diffusion-based methods (Ho et al., 2020) has pushed the boundaries of text-to-image\ngeneration to a new level. Examples include DALL\u00b7E 2 (Ramesh et al., 2022), Imagen (Saharia\net al., 2022), GLIDE (Nichol et al., 2022), and LDM (Rombach et al., 2022). Recently, some works\nconsider personalized text-to-image generation by learning a token embedding (Gal et al., 2023a) and\nfinetuning (Ruiz et al., 2023) or partially finetuning (Kumari et al., 2023) a diffusion model. Recently,\nQiu et al. (2023) proposes a fine-tuning method named Orthogonal Finetuning, which can be used for\nefficient DreamBooth fine-tuning. Many works emerge lately, but they require finetuning the whole\nor partial networks in the vanilla U-Net such as Perfusion (Tewel et al., 2023), ELITE (Wei et al.,\n2023), and UMM-Diffusion (Ma et al., 2023), or training with large-scale data on specific category\ndomains based on encoders (Shi et al., 2023; Jia et al., 2023; Gal et al., 2023b; Chen et al., 2023) or\nwith text-image pairs (Li et al., 2023). In contrast, our work tackles the general domain-agnostic task\nwhile keeping the pretrained diffusion model completely frozen. We compare the characteristics of\ndifferent models in Tab. 1.\nTable 1: Model characteristics.\nPlaceholder type\nPreprocessing\n#Trainable params\nDiffusion U-Net\nText encoder\nVisual condition\nDreamBooth (Ruiz et al., 2023)\n[V] class\nGeneration\n982.6M\nFully finetuned\nFinetuned\n\u2717\nCustom Diffusion (Kumari et al., 2023)\n[V] class\nRetrieval\n57.1M\nPartially finetuned\nFrozen\n\u2717\nTextual Inversion (Gal et al., 2023a)\nS\u22c6\nNull\n768\nFrozen\nFrozen\n\u2717\nViCo\nS\u22c6\nNull\n51.3M\nFrozen\nFrozen\n\u2713\nVisual condition. Visual condition is commonly used in image-to-image translation (Isola et al.,\n2017; Zhu et al., 2017a;b; Choi et al., 2018; Park et al., 2020), which involves training a model to\nmap an input image to an output image based on a certain condition, e.g., edge, sketch, or semantic\nsegmentation. Similar techniques have been used for tasks such as style transfer (Gatys et al., 2016;\nJohnson et al., 2016), colorization (Zhang et al., 2016; Larsson et al., 2016; Zhang et al., 2017),\nand super-resolution (Ledig et al., 2017; Johnson et al., 2016; Wang et al., 2018). In the context of\ndiffusion models, visual condition is also used for image editing (Brooks et al., 2023) and controllable\nconditioning (Mou et al., 2023; Zhang & Agrawala, 2023). Despite the massive study on visual\ncondition, most works use it for controlling the spatial layout and geometric structure but discard its\nrich semantics. Our work stands out in capturing fine-grained semantics related to the specific visual\nappearance from visual conditions, an aspect that is rarely discussed.\nDiffusion-based generative models. Diffusion-based generative models develop fast and continu-\nously produce striking outcomes. Ho et al. (Ho et al., 2020) first presents DDPMs to progressively\ndenoise from random noise to a synthesized image. DDIMs (Song et al., 2021) accelerate the\nsampling process of DDPMs. Latent diffusion models (LDMS) (Rombach et al., 2022) introduce\nmultiple conditions in latent diffusion space, producing realistic and high-fidelity text-to-image\nsynthesis results. Following the implementation of LDMs (Rombach et al., 2022), Stable Diffu-\nsion (SD) is trained on a large-scale text-image data collection, which achieves the state-of-the-art\ntext-to-image synthesis performance. Diffusion models are widely used for generation tasks such as\nvideo generation (Ho et al., 2022; Wu et al., 2022), inpainting (Lugmayr et al., 2022), and semantic\nsegmentation (Hoogeboom et al., 2021; Baranchuk et al., 2022).\n3\nMETHOD\nGiven a handful of images (4-7) showing a novel object concept, we target at generating images of\nthis unique object following some text guidance. We aim to neatly inject visual condition, which is\nneglected in previous works, along with text condition into the diffusion model to better preserve\nthe visual expressions. Following the attempt of textual inversion (Gal et al., 2023a), we adopt a\nplaceholder (S\u22c6) as the learnable text embedding to capture the unique visual object. We first quickly\nreview Stable Diffusion (Rombach et al., 2022) which serves as our base model (Sec. 3.1). We then\nintroduce a simple yet efficient method to inject fine-grained semantics from visual conditions into the\ndenoising process (Sec. 3.2), and show how to automatically generate object masks within training\n(Sec. 3.3). We finally present our overall learning objective and implementation details (Sec. 3.4).\nFig. 2 shows an overview of our method.\n3\nQ\nKV\nQ\nKV\nQ\nKV\nM\n\u2776\n\u2777\n<TEXT>\n\ud835\udc50!\n\ud835\udc50\"\n\ud835\udc5b#\nText Attention\nText Attention\nImage Attention\n(Fig. 3)\n\ud835\udc40$\n\ud83d\udd25\nTrainable\nFrozen\n\ud83d\udd25\n\u2744\n\u2744\nDiffusion U-Net\nText Attention\nText Encoder\na\nphoto\nof\nS*\n<|SOT|>\n<|EOT|>\nReference\nNoisy\nDenoised\nImage Attention\n\ud83d\udd25\n\ud83d\udd25\n\u2744\n\u2744\n\u2744\n\u2744\nFigure 2: Method overview. We introduce a module of image (cross-)attention to integrate visual\nconditions into the frozen diffusion model. On the left, the noisy image and a reference image are\nfed into diffusion U-Net in parallel. We follow (Gal et al., 2023a) to learn the embedding S\u22c6. On\nthe right, we present the data stream comprising the original text attention and the proposed image\nattention. \u2776 denotes the attention output in vanilla diffusion model and \u2777 represents the visually\nconditioned output. The generation and use of the mask \u02c6\nM are further detailed in Sec. 3.3.\n3.1\nSTABLE DIFFUSION\nStable Diffusion (SD) (Rombach et al., 2022) is a latent text-to-image diffusion model derived\nfrom classic Denoising Diffusion Probabilistic Models (DDPMs) (Ho et al., 2020). SD applies a\nlargely pretrained autoencoder E to extract latent code for images, and a corresponding decoder D\nto reconstruct the original images. Specifically, the autoencoder maps images x \u2208 I to latent code\nz = E(x), and the decoder maps latent code back to images \u02c6x = D(E(x)), where \u02c6x \u2248 x. SD adopts\na diffusion model in the latent space of the autoencoder. For the text-to-image diffusion model, text\nconditions can be added to the diffusion process. The diffusion process can be formulated as iterative\ndenoising that predicts the noise at the current timestep. In this process, we have the loss\nLSD = Ez\u223cE(x),y,\u03f5\u223cN (0,1),t[\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, c\u03c0(y))\u22252\n2]\n(1)\nwhere t is the timestep, zt is the latent code at timestep t, c\u03c0 is the text encoder that maps text prompts\ny into text embeddings, \u03f5 is the noise sampled from Gaussian distribution, and \u03f5\u03b8 is the denoising\nnetwork (i.e., U-Net (Ronneberger et al., 2015)) that predicts the noise. Training SD is flexible, such\nthat we can jointly learn c\u03c0 and \u03f5\u03b8 or exclusively learn \u03f5\u03b8 with a frozen pretrained text encoder.\n3.2\nVISUAL CONDITION INJECTION\nCommon approaches for conditioning diffusion models on images include feature concatena-\ntion (Brooks et al., 2023) and direct element-wise addition (Mou et al., 2023; Zhang & Agrawala,\n2023). These visual conditions show astonishing performance in capturing the layout of images.\nHowever, visual semantics, especially fine-grained details, are hard to preserve or even lost using\nthese image conditioning methods. Instead of only considering the patches at the same spatial location\non the noisy latent code and visual condition, we exploit correlations across all patches on both\nimages. To this end, we propose to train an image cross-attention block that has the same structure as\nthe text cross-attention block in the vanilla diffusion U-Net. The image cross-attention block takes an\nintermediate noisy latent code and a visual condition as inputs, integrating visual conditions into the\ndenoising process.\nSome works (Gal et al., 2023b; Shi et al., 2023) acquire visual conditions from reference images by\nadditionally training a visual feature extractor. This may cause a misalignment between the feature\nspaces of the latent code and the visual condition. Instead of deploying extra networks, we directly\nfeed the autoencoded reference image into the vanilla diffusion U-Net, and apply the intermediate\nlatent codes as visual conditions. We use the pretrained autoencoder to map the reference image\nxr \u2208 I to the latent space: zr = E(xr). Let \u03f5l\n\u03b8(\u00b7) denote the output of the l-th attention block of\nU-Net. The visual condition at the l-th attention block is then given by\ncl\nI = \u03f5l\n\u03b8(zr, t, cT ), l \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , L \u2212 1}\n(2)\nwhere L is the number of attention blocks in U-Net, and cT = c\u03c0(y) is the text condition from\nthe text encoder. Note that cT is derived from token embeddings in which the embedding S\u22c6 is\n4\nText Attention\nS*\n... ...\n...\nImage Attention\n\u2299\nS*\nBinarization\n1 0\n\u2299\nHadamard \nProduct\n\ud835\udc40\"\n1 0\nFigure 3: Mask mechanism. We can obtain a similarity distribution from the cross-attention map of\nthe reference image associated with the learnable object token S\u22c6. The distribution can be unflattened\ninto a similarity map. After binarization with Otsu thresholding (Otsu, 1979), the derived binary\nmask can be applied to the image cross-attention map to discard the non-object patches.\nlearnable. Let the raw text cross-attention block from vanilla U-Net be AT (q, kv), the proposed\nimage cross-attention block be AI(q, kv). We denote the new denoising process after incorporating\nAI as \u03f5\u03b8,\u03c8, in which the l-th attention block is denoted as \u03f5l\n\u03b8,\u03c8. We can compute the intermediate\nlatent code of the generated noisy image at the l-th attention block as\nnl\nt = \u03f5l\n\u03b8,\u03c8(zt, t, cT , cl\nI), l \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , L \u2212 1}\n(3)\nBecause all operations are executed at the l-th attention block, we can omit all superscripts of l for\nsimplicity. The original attention at each attention block in U-Net, i.e., n\u2032\nt = AT (nt, cT ), can be\nreplaced by \u02c6nt = AT (nt, cT ), \u02c6cI = AT (cI, cT ), n\u2032\nt = AI(\u02c6nt, \u02c6cI), where n\u2032\nt is the output of the\ncurrent attention block that is fed into the following layers in U-Net. At the image cross-attention\nblock AI, we can capture visual semantics from the reference image and inject them into the noisy\ngenerated image.\n3.3\nEMERGING OBJECT MASKS\nTo avoid capturing the background from training samples and exclusively learn the foreground object\nwe are interested in, we propose an online, computationally-efficient, and non-parametric method that\nis naturally incorporated into our pipeline to generate reliable object masks. Next, we will illustrate\nhow attention maps of text and image conditions can be directly used as object masks to capture the\nobject-exclusive patch regions.\nRecall the process of computing the text cross-attention in diffusion U-Net\nTextAttention(Q, K, V ) = softmax(QKT\n\u221adk\n)V\n(4)\nwhere the query is the reference image, the key and value are the text condition, and the scaling factor\ndk is the dimension of the query and key.1 Inspired by (Hertz et al., 2023) that diffusion models gain\npretty good cross-attentions, we notice the attention map softmax( QKT\n\u221adk ) inherently implies a good\nobject mask for the reference image. Specifically, the attention map for the text condition and the\nvisual condition from the reference image reveals the response distribution of each text token to all\nimage patches of different resolutions. The learnable embedding S\u22c6 has strong responses at the exact\npatch regions where the foreground object lies. After binarizing the similarity distribution of S\u22c6 on\nthe reference image, we can obtain a good-quality object mask \u02c6\nM. In this paper, we simply apply\nOtsu thresholding (Otsu, 1979) for binarization. The mask can be directly deployed in our proposed\nimage cross-attention by simply masking the attention map between the noisy generated image and\nthe reference image in the latent space. The masked image cross-attention is formulated as\nImageAttention(Q, K, V ) =\n\u0012\n\u02c6\nM \u2299 softmax(QKT\n\u221adk\n)\n\u0013\nV\n(5)\nwhere the query is the noisy generated image, the key and value are the reference image, and \u2299 is\nHadamard product (element-wise product) with broadcasting \u02c6\nM. The masking process is depicted\nin Fig. 3. By masking the attention map in Eq. (5), distractors from the background can be drastically\nsuppressed. We can thus condition the generation process exclusively on the foreground object that is\ncaptured in the reference image.\n1All happen in the latent space after linear projections.\n5\nRegularization. Due to fine-tuning on a small set of images, S\u22c6 is sometimes overfitted, deriving\nundesirable object masks. Nevertheless, we empirically find the end-of-text token <|EOT|>, the\nglobal representation in transformers, can maintain consistently good semantics on the unique object.\nFrom this observation, we apply a regularization between similarity maps of the reference image\nassociated with S\u22c6 and <|EOT|> in the text cross-attention. Specifically, from cross-attentions,\nwe have the attention map A := softmax( QKT\n\u221adk ) \u2208 RB\u00d7Dp\u00d7Dt where B is the batch size, Dp is\nthe number of image patches, and Dt is the number of text tokens. Let S\u22c6 be the i-th token and\n<|EOT|> be the j-th token, and their corresponding similarity logits be A\u22c6,i and A\u22c6,j. We define\nour regularization as\nLreg = \u2225A\u22c6,i/ max(A\u22c6,i) \u2212 A\u22c6,j/ max(A\u22c6,j)\u22252\n2\n(6)\nwhere we apply a max normalization to guarantee the same scale of the two logits. We can flexibly\nleverage this regularization during training to refine the attention map of S\u22c6, thereby enhancing the\nobject mask used in our method. This refinement ensures the reliability of the mask.\n3.4\nTRAINING AND INFERENCE\nTraining. We train our model on 4-7 images with the vanilla diffusion U-Net frozen. We formulate\nthe final training loss by integrating the standard denoising loss and the regularization term as\nL = Ez\u223cE(x),zr\u223cE(xr),y,\u03f5\u223cN (0,1),t[\u2225\u03f5 \u2212 \u03f5\u03b8,\u03c8(zt, t, zr, c\u03c0(y))\u22252\n2] + \u03bbLreg\n(7)\nwhere \u03bb is the scaling weight of the regularization loss, and \u03f5\u03b8,\u03c8 is the new denoising networks\ncomposed of the vanilla diffusion U-Net parameterized by \u03b8 and the proposed image attention blocks\nparameterized by \u03c8. During training, we freeze the pretrained diffusion model and only train the\nimage attention blocks and finetune the learnable text embedding S\u22c6 simultaneously.\nImplementation details. We use Stable Diffusion (Rombach et al., 2022) as our backbone. The\ndiffusion U-Net (Ronneberger et al., 2015) contains encoder, middle, and decoder layers. We\nincorporate the proposed image attention module into every other attention block exclusively in the\ndecoder. Our image attention module follows the standard attention-feedforward fashion (Vaswani\net al., 2017), and has the same structure as the text cross-attention used in LDMs (Rombach et al.,\n2022) only differing in the dimension of the condition projection layer. We set \u03bb = 5 \u00d7 10\u22124 and\nlearning rate to 5 \u00d7 10\u22123 for S\u22c6 and 10\u22125 for image attention blocks. We train ViCo with a batch\nsize of 4 for 400 steps. At inference, our model also requires a reference image input for the visual\ncondition, injected into the denoising process in the same way as in training. Our method is insensitive\nand robust to the reference image. Therefore, either one in the training samples or a new image of\nthe identical object is a feasible visual condition in sampling. For fair evaluation, we use the same\nreference image for each dataset in all experiments.\n4\nEXPERIMENT\n4.1\nQUANTITATIVE EVALUATION\nData. Previous works (e.g., Textual Inversion (Gal et al., 2023a), DreamBooth (Ruiz et al., 2023), and\nCustom Diffusion (Kumari et al., 2023)) use different datasets for evaluation. For a fair and unbiased\ncomparison, we collect a dataset of 20 unique concepts from these three works. The collected dataset\nspans a large range of object categories covering 6 toys, 6 live animals, 4 accessories, 3 containers,\nand 1 building, allowing a comprehensive evaluation. Each object category contains 4-7 images of a\nunique object (except for one having 12 images). Based on the prompt list provided in (Ruiz et al.,\n2023), we remove one undesirable prompt \u201ca cube shaped S\u22c6\u201d because we are more interested in\nkeeping the appearance of the unique object. In addition, we add more detailed and informative\nprompts to test the expressiveness of richer and more complex textual knowledge (e.g., \u201ca S\u22c6 among\nthe skyscrapers in New York city\u201d). Totally, we collect 31 prompts for 14 non-live objects and 31\nprompts for 6 live animals. We generate 8 samples per prompt for each object, giving rise to 4,960\nimages in total, for robust evaluation. More details about the dataset can be found in Appendix B.\nMetric.\nIn our task, we concern with two core questions regarding the personalized generative\nmodels: (1) how well do the generated images capture and preserve the input object? and (2) how\nwell do the generated images tail the text condition? For the first question, we adopt two metrics,\nnamely CLIP (Radford et al., 2021) image similarity ICLIP and DINO (Caron et al., 2021) image\nsimilarity IDINO. Specifically, we compute the feature similarity between the generated image and the\ncorresponding real image respectively using CLIP (Radford et al., 2021) or DINO (Caron et al., 2021).\nDINO is trained in a self-supervised fashion without ground-truth class labels, thus not neglecting\n6\nTable 2: Quantitative comparison.\nQuantitative metrics\nIDINO \u2191\nICLIP \u2191\nTCLIP \u2191\nDreamBooth (Ruiz et al., 2023)\n0.628\n0.804\n0.236\nCustom Diffusion (Kumari et al., 2023)\n0.570\n0.768\n0.249\nTextual Inversion (Gal et al., 2023a)\n0.520\n0.768\n0.216\nViCo\n0.631\n0.809\n0.229\nTable 3: Time cost (averaged over 5 runs).\nTime cost (sec.)\nTraining\nInference\nDreamBooth (Ruiz et al., 2023)\n1411\u00b127\n11.2\u00b10.1\nCustom Diffusion (Kumari et al., 2023)\n682\u00b159\n8.4\u00b10.4\nTextual Inversion (Gal et al., 2023a)\n735\u00b17\n9.8\u00b10.4\nViCo\n353\u00b13\n15.4\u00b10.1\nthe difference among objects from the same category. Therefore, DINO metric better reflects how\nwell the generated object resembles the real one, as also noted in (Ruiz et al., 2023). For the second\nquestion, we adopt one metric, namely CLIP text similarity TCLIP. Specifically, we compute the\nfeature similarity between the CLIP visual feature of the generated image and the CLIP textual feature\nof the corresponding prompt text that omits the placeholder. The three metrics are derived from the\naverage similarities of all compared pairs. In our experiments, we deploy ViT-B/32 for the CLIP\nvision model and ViT-S/16 for the DINO model to extract visual and textual features.\nComparison. We compare our method ViCo with three state-of-the-art models, namely Textual\nInversion (Gal et al., 2023a), DreamBooth (Ruiz et al., 2023), and Custom Diffusion (Kumari et al.,\n2023). We use Stable Diffusion for all compared methods for a fair comparison. The results of three\nquantitative metrics are shown in Tab. 2. Our model achieves the highest image similarity on both\nDINO and CLIP metrics, indicating our method best preserves the object-specific semantics from the\nimage. DreamBooth and Custom Diffusion perform better on the text similarity metric because they\nuse the fashion of \u201c[V] class\u201d to represent the visual object in the text space. The class category word\nprovides rich prior knowledge, while the learnable identifier \u201c[V]\u201d primarily serves as an auxiliary\nguidance, such as controlling texture or facial appearance in the generation process. In contrast,\nTextual Inversion and our method employ a single token in the text embedding space, which, once\nlearned, may dominate the text space and slightly weaken the influence of text-related information\nin the generated results. We deliberately choose the single-token fashion in our work because we\nbelieve that representing a visual concept with a single word token is crucial for achieving effective\ntext-image alignment. This minimalist approach allows us to capture the essence of the concept in a\nconcise and precise manner, focusing on the core problem of aligning textual and visual information.\nBesides, DreamBooth and Custom Diffusion require finetuning either the full SD network or a\nportion of it while our model and Textual Inversion do not. With the same foundation, our method\noutperforms Textual Inversion by a significant margin on all metrics.\nWe report the training and inference time cost of the four methods in Tab. 3. All methods are\ntrained using four 3090 GPUs and tested on a single one. Note that DreamBooth requires generating\ncoarse-category samples and Custom Diffusion involves retrieving real images with given and similar\ncaptions, which are not included in the time overheads presented in the table. Overall, the majority of\nthe time cost is in the training, while the inference takes much less time for all methods. ViCo has a\nslightly longer inference time due to the additional image attention.\n4.2\nQUALITATIVE EVALUATION\nIn our massive qualitative experiments, depicted in Fig. 4, we observe that ViCo produces text-guided\nimages of high quality. We assess the qualitative results based on several aspects.\nImage fidelity.\nOur model preserves fine details of the object in the training samples. As a\ncomparison, Textual Inversion fails to preserve sufficient details in many cases (the 3rd and 5th rows)\ndue to its limited expressiveness. The use of \u201c[V] class\u201d in DreamBooth and Custom Diffusion,\nwhile providing strong class-related information, may result in the loss of object-specific details. For\ninstance, in the second row, both DreamBooth and Custom Diffusion alter the appearance of the cat.\nSimilarly, DreamBooth fails to preserve the holes in the generated elephant in the fourth row.\nText fidelity. Our model can faithfully follow the text prompt guidance to generate reasonable results.\nFor example, in the first row of the \u201cteddy bear\u201d, our model successfully incorporates elements\nsuch as \u201ca tree\u201d and \u201cautumn leaves\u201d as indicated by the text, while other models may occasionally\nstruggle to achieve this level of fidelity. In more complex cases, like the third and fourth rows, Textual\nInversion fails to express any information from the text prompts.\nText-image Equilibrium. Our model excels at balancing the effects of both text conditions and\nvisual conditions, resulting in a harmonious equilibrium between the text and the image. The text\nprompts and the image samples may have varying degrees of influence on generation. For example,\nin the last row, Custom Diffusion successfully generates an appealing \u201clion face\u201d guided by the text,\nbut the generated image is almost no longer a \u201cpot\u201d. Similarly, DreamBooth maintains the overall\nappearance of a pot but loses significant details of the original \u201cwooden pot\u201d. In contrast, our method\nexcels at preserving the original \u201cpot\u201d details while synthesizing a high-quality \u201clion face\u201d on it.\n7\nA \ud835\udc46\u2217 with a tree and autumn leaves\nA \ud835\udc46\u2217 wearing pink glasses\nA \ud835\udc46\u2217 with the Eiffel Tower in the background\nA golden \ud835\udc46\u2217 on Times Square\nTraining Samples\nDreamBooth\nCustom Diffusion\nTextual Inversion\nOurs\nA lion face on a \ud835\udc46\u2217\nFigure 4: Qualitative comparison. Given input images (first column), we generate three samples\nusing ViCo (ours), Textual Inversion (Gal et al., 2023a), Custom Diffusion (CD) (Kumari et al.,\n2023), and DreamBooth (DB) (Ruiz et al., 2023). The text prompt is under the generation samples, in\nwhich S\u22c6 for CD and DB is \u201c[V] class\u201d.\nReference: \ud835\udc46\u2217\nA \ud835\udc46\u2217 with a city in the background\nTextual Inversion\n+ Visual condition\nReal sample\nReference: \ud835\udc46\u2217\nA \ud835\udc46\u2217 in the snow\n(a) Visual condition\nReference: \ud835\udc46\u2217\nA \ud835\udc46\u2217 on top of a wooden floor\nOurs w/o mask\nOurs w/ mask\nReal sample\nReference: \ud835\udc46\u2217\nA \ud835\udc46\u2217 floating on top of water\n(b) Masking\nReference \nimage\nOurs w/o mask reg\nOurs w/ mask reg\n(c) Regularization\nFigure 5: Ablation study. We ablate each component in our method and report: (a) results with or\nwithout the visual condition; (b) results with or without the masking; and (c) attentions, masks, and\ngenerations (from left to right) with or without the regularization.\nAuthenticity. Our generation results are authentic and photorealistic, devoid of noticeable traces of\nartificial synthesis. For example, in the fourth row, although Custom Diffusion generates visually\nappealing images, they may appear noticeably synthetic. In comparison, our results are more\nphotorealistic, authentically depicting a golden elephant statue positioned at Times Square.\nDiversity. Our model demonstrates the capability to generate diverse results, presenting a notable\nabundance of variation and showcasing a wide range of synthesis possibilities.\n4.3\nABLATION STUDY AND ANALYSIS\nWe study the effect of the visual condition, the automatic mask, and the initialization of S\u22c6. Repre-\nsentative results are compiled in Fig. 5 and a quantitative comparison is reported in Tab. 4.\nVisual condition.\nThe proposed visual condition module can significantly improve the visual\nexpressiveness of the single learnable embedding used by Textual Inversion, making higher image\nfidelity. We compare the performance of Textual Inversion before and after adding the visual condition\nmodule in Fig. 5(a). We can observe the degree of object detail preservation is considerably enhanced\nwithout losing text information after adding our visual condition module. Row (1) in Tab. 4 also shows\nour visual condition can significantly enhance our baseline Textual Inversion (Gal et al., 2023a).\n8\nTable 4: Quantitative improvements.\nTI denotes the baseline Textual Inver-\nsion (Gal et al., 2023a), VC denotes our\nvisual condition, and M denotes the pro-\nposed mask mechanism.\nTI\nVC\nM\nIDINO \u2191\nICLIP \u2191\nTCLIP \u2191\n(0)\n\u2713\n0.520\n0.768\n0.216\n(1)\n\u2713\n\u2713\n0.630 +21.2%\n0.805 +4.8%\n0.229 +6.0%\n(2)\n\u2713\n\u2713\n\u2713\n0.631 +21.3%\n0.809 +5.3%\n0.229 +6.0%\nInitialize S*\nas \u201cobject\u201d\nA \ud835\udc46\u2217 in the jungle\nInitialize S*\nas \u201cclock\u201d\nTextual \nInversion\nViCo\n(ours)\nInitialize S*\nas \u201canimal\u201d\nA \ud835\udc46\u2217 in the jungle\nInitialize S* \nas \u201cdog\u201d\nTextual \nInversion\nViCo\n(ours)\nReference \nS*\nReference \nS*\nFigure 6: Comparison of our method and Textual Inversion\nwhen initializing S\u22c6 with a general word (object or animal).\nTraining samples\nRecontextualization\nA \ud835\udc46\u2217 under an \nastonishing sunset\nA \ud835\udc46\u2217 with a blue house \nin the background\nA \ud835\udc46\u2217 among the \nskyscrapers\nA \ud835\udc46\u2217 in the snow\nA \ud835\udc46\u2217 floating in an \nocean\nArt Renditions\nA \ud835\udc46\u2217 on surfboard \nin the desert\nVan Gogh\nVermeer\nMonet\nCostume\nSpider Man\nIron Man\nHarry Potter\nTraining: \ud835\udc46\u2217\n\ud835\udc46\u2217\nTraining:\nTraining samples\nFigure 7: Applications. We use different contexts, artistic styles, and various costume outfits to\ngenerate images of high image fidelity and text fidelity.\nAutomatic mask. Our automatic mask mechanism enables isolating the object from the distracting\nbackground, which further improves the object fidelity. As shown in Fig. 5(b), the generation results\nmay be occasionally distorted without the mask. After adding the mask, the object can be well\ncaptured and reconstructed. Row (2) in Tab. 4 also quantitatively shows applying the mask can\nfurther improve image fidelity. We also validate using regularization for the object mask refinement\nin Fig. 5(c), showing the mask is well aligned with the object after leveraging the regularization term.\nRobust S\u22c6 initialization.\nProper word initialization is crucial for Textual Inversion (Gal et al.,\n2023a) due to its high sensitivity to the chosen initialization word. In contrast, ViCo is robust to\nsuch initialization variations, benefiting from the visual condition. When unsure about a suitable\ninitialization word, \u201cobject\u201d or \u201canimal\u201d can be generally reliable options. Fig. 6 compares different\ninitialization words for Textual Inversion and our method. While Textual Inversion exhibits severe\ndistortion when initialized with \u201cobject\u201d or \u201canimal\u201d, our approach maintains high-quality generation.\n4.4\nAPPLICATIONS\nWe show three types of applications of ViCo in Fig. 7. The first application is recontextualization. We\ngenerate images for a novel object in different contexts. The generated results present natural-looking\nand unobtrusive integration of the object and the contexts, with diverse poses (e.g., sitting, standing,\nand floating). We also generate art renditions of novel objects in different painting styles. We use\na text prompt \u201ca painting of a S\u22c6 in the style of [painter]\u201d. Our results have novel poses that are\nunseen in the training samples, e.g., the painting in the style of \u201cVermeer\u201d. In addition, we change\nthe costume for the novel object using a text prompt \u201ca S\u22c6 in a [figment] outfit\u201d, producing novel\nimage variations while preserving the appearance of novel objects.\n5\nCONCLUSION\nIn summary, our paper introduces ViCo, a fast and lightweight method for personalized text-to-image\ngeneration that preserves fine object-specific details. Our approach incorporates visual conditions into\nthe diffusion process through an image cross-attention module, enabling the extraction of accurate\nobject masks. These masks effectively isolate the object of interest, eliminating distractions from the\nbackground in the latent space. Our visual condition module seamlessly integrates with pretrained\ndiffusion models without the need for diffusion fine-tuning, allowing for scalable deployment.\nMoreover, our model is easy to use, as it doesn\u2019t rely on prior object masks or extensive preprocessing.\n9\nLimitations. We also notice certain limitations of our method. The decision to keep the diffusion\nmodel frozen can sometimes result in lower performance compared to methods that fine-tune the\noriginal diffusion model (Ruiz et al., 2023; Kumari et al., 2023). Additionally, the use of Otsu\nthresholding for mask binarization adds a slight time overhead during training and inference for each\nsampling step. However, these limitations are mitigated by the shorter training time, as our method\nrequires no preprocessing and is optimized for fewer steps, and the negligible increase in inference\ntime (several seconds), which has minimal impact on the overall model implementation.\nREFERENCES\nRameen Abdal, Peihao Zhu, John Femiani, Niloy Mitra, and Peter Wonka. Clip2stylegan: Unsuper-\nvised extraction of stylegan edit directions. In ACM SIGGRAPH, 2022.\nDmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Label-\nefficient semantic segmentation with diffusion models. In ICLR, 2022.\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity\nnatural image synthesis. In ICLR, 2019.\nTim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image\nediting instructions. In CVPR, 2023.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.\nWenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William W\nCohen. Subject-driven text-to-image generation via apprenticeship learning. arXiv preprint\narXiv:2304.00186, 2023.\nYunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan:\nUnified generative adversarial networks for multi-domain image-to-image translation. In CVPR,\n2018.\nKatherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Cas-\ntricato, and Edward Raff. Vqgan-clip: Open domain image generation and editing with natural\nlanguage guidance. In ECCV, 2022.\nRinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.\nStylegan-nada: Clip-guided domain adaptation of image generators.\nACM Transactions on\nGraphics (TOG), 2022.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\ninversion. In ICLR, 2023a.\nRinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.\nEncoder-based domain tuning for fast personalization of text-to-image models. arXiv preprint\narXiv:2302.12228, 2023b.\nLeon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional\nneural networks. In CVPR, 2016.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-\nto-prompt image editing with cross-attention control. In ICLR, 2023.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS,\n2020.\nJonathan Ho, Tim Salimans, Alexey A Gritsenko, William Chan, Mohammad Norouzi, and David J\nFleet. Video diffusion models. In NeurIPS, 2022.\n10\nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. Argmax flows\nand multinomial diffusion: Learning categorical distributions. NeurIPS, 2021.\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with\nconditional adversarial networks. In CVPR, 2017.\nXuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou,\nHuisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with\ntext-to-image diffusion models. arXiv preprint arXiv:2304.02642, 2023.\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and\nsuper-resolution. In eccv, 2016.\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In CVPR, 2019.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing\nand improving the image quality of stylegan. In CVPR, 2020.\nTero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen, Janne Hellsten, Jaakko Lehtinen, and Timo\nAila. Alias-free generative adversarial networks. In NeurIPS, 2021.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\narXiv:2304.02643, 2023.\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\ncustomization of text-to-image diffusion. In CVPR, 2023.\nGustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for automatic\ncolorization. In ECCV, pp. 577\u2013593, 2016.\nChristian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro\nAcosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single\nimage super-resolution using a generative adversarial network. In CVPR, 2017.\nJason Lee, Kyunghyun Cho, and Douwe Kiela. Countering language drift via visual grounding. In\nEMNLP, 2019.\nDongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for\ncontrollable text-to-image generation and editing. arXiv preprint arXiv:2305.14720, 2023.\nYuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering\nlanguage drift with seeded iterated learning. In ICML, 2020.\nAndreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool.\nRepaint: Inpainting using denoising diffusion probabilistic models. In CVPR, 2022.\nYiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu, and Jiaying Liu. Unified multi-modal latent\ndiffusion for joint subject and text conditional image generation. arXiv preprint arXiv:2303.09319,\n2023.\nChong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453, 2023.\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and\nediting with text-guided diffusion models. In ICML, 2022.\nNobuyuki Otsu. A threshold selection method from gray-level histograms. IEEE transactions on\nsystems, man, and cybernetics, 1979.\nTaesung Park, Alexei A Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired\nimage-to-image translation. In ECCV, 2020.\n11\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-\ndriven manipulation of stylegan imagery. In ICCV, 2021.\nZeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller,\nand Bernhard Sch\u00f6lkopf. Controlling text-to-image diffusion by orthogonal finetuning. In NeurIPS,\n2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.\nGenerative adversarial text to image synthesis. In ICML, 2016.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, 2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In MICCAI, 2015.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR,\n2023.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. In NeurIPS, 2022.\nJing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image\ngeneration without test-time finetuning. arXiv preprint arXiv:2304.03411, 2023.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR,\n2021.\nMing Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan: A\nsimple and effective baseline for text-to-image synthesis. In CVPR, 2022.\nYoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for text-to-\nimage personalization. In ACM SIGGRAPH, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\nXintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\nEsrgan: Enhanced super-resolution generative adversarial networks. In ECCV, 2018.\nYuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding\nvisual concepts into textual embeddings for customized text-to-image generation. arXiv preprint\narXiv:2302.13848, 2023.\nJay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan,\nXiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for\ntext-to-video generation. arXiv preprint arXiv:2212.11565, 2022.\nWeihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu. Tedigan: Text-guided diverse face image\ngeneration and manipulation. In CVPR, 2021.\n12\nTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.\nAttngan: Fine-grained text to image generation with attentional generative adversarial networks.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, 2018.\nHui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, and Shihao Ji. Improving text-to-\nimage synthesis using contrastive learning. arXiv preprint arXiv:2107.02423, 2021.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\nrich text-to-image generation. Transactions on Machine Learning Research, 2022.\nHan Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive\nlearning for text-to-image generation. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2021.\nLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.\narXiv preprint arXiv:2302.05543, 2023.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\nRichard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng, Angela S Lin, Tianhe Yu, and Alexei A\nEfros. Real-time user-guided image colorization with learned deep priors. ACM Transactions on\nGraphics (TOG), 2017.\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. In ICCV, 2017a.\nJun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli\nShechtman. Toward multimodal image-to-image translation. In NeurIPS, 2017b.\nMinfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative adversarial\nnetworks for text-to-image synthesis. In CVPR, 2019.\nA\nMORE DETAILS ON IMPLEMENTATION\nArchitecture. The proposed image cross-attention blocks, designed to accept visual conditions with\nthe standard attention architecture as in (Vaswani et al., 2017), are integrated into specific attention\nlayers within the decoder of the diffusion U-Net architecture. Specifically, we incorporate these\nblocks into every other attention layer in the decoder of the U-Net, to achieve balanced and effective\nperformance. This design is based on the observation that integrating visual-condition attention\ninto decoder layers produces better results compared to encoder layers, as shown in Fig. 8. We also\nobserve that integrating visual-condition attention into both layers yields comparable performance\nto integrating it solely in the decoder layers. Therefore, we opt to exclusively integrate visual-\nconditioned attention in the decoder layers in order to reduce the parameter load and achieve a more\nlightweight design. The details of which attention layers are incorporated with the visual condition\ncan be found in Tab. 5.\nTable 5: Architecture scheme. Diffusion U-Net consists of encoder, middle, and decoder layers,\nwith 16 original cross-attention blocks. The last row indicates the integration of visual-condition\nattention in specific cross-attention layers.\nU-Net\nEncoder\nMiddle\nDecoder\nAttention index\n0 \u2013 5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nVisual condition?\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\nMasking strategy.\nIn Fig. 9, we present a comparison between our automatic mask and the\nground-truth mask generated by SAM (Kirillov et al., 2023). Additionally, we compare our masking\nstrategy and attention alignment. The alignment is enforced by employing MSE between the S\u22c6\ncross-attention map and either the ground-truth mask or our automatic mask. It is important to note\nthat our automatic mask is inherently derived from the attention, making the alignment process akin\nto a self-supervised technique. Remarkably, the masking performance using the ground-truth mask\n13\nSample\nEncoder\nDecoder (ours)\nEncoder + Decoder\nFigure 8: Comparison of integrating visual-condition attention into encoder layers, decoder layers,\nand both layers.\nMasking w/ \nSAM mask\nAlignment w/ \nSAM mask\nSample\nA \ud835\udc46\u2217 with a city in the background\nMasking w/ \nautomatic mask \n(Ours)\nAlignment w/ \nautomatic mask\nA \ud835\udc46\u2217 in the snow\nFigure 9: Comparison of different mask settings. We employ SAM (Kirillov et al., 2023) to\ngenerate the so-called ground-truth object mask.\nis on par with our automatic mask, demonstrating the effectiveness of our automatic mask. The\nresults from attention alignment using both masks closely resemble each other but fall short of the\nperformance achieved using the proposed masking strategy.\nTraining data sampling. During training, we sample training images in sequential order and sample\nreference images randomly from the rest. This approach ensures that for each step, the training\nimage and the reference image are different, allowing the model to focus on learning the shared novel\nconcept between the two images rather than the entire image.\nB\nDATASET DETAILS\nTraining images. For quantitative evaluation, our training images comprise 20 objects in 5 categories,\nnamely 6 toys, 6 live animals, 4 accessories, 3 containers, and 1 building, selected from Textual\nInversion (Gal et al., 2023a), DreamBooth (Ruiz et al., 2023), and Custom Diffusion (Kumari et al.,\n2023), allowing a fair and comprehensive evaluation. We list the objects and their information\nin Tab. 6. We show the training image samples in Fig. 10, with our naive generations, i.e., generated\nimages using \u201ca photo of a S\u22c6\u201d. We observe that the naive generations successfully preserve the\noriginal object, demonstrating the effectiveness of our model in capturing and reproducing intricate\nvisual details.\nText prompts. We adopt the same text prompt list used in Textual Inversion (Gal et al., 2023a) for\ntraining. For quantitative evaluation, we collect 31 prompts for 11 non-live objects and 31 prompts\nfor 5 live animals in total. We show them in Tab. 7.\nC\nANALYSIS OF VARYING TRAINING SAMPLES AND REFERENCE IMAGES\nThe number of training samples. In the main paper, we follow the standard training protocol\noutlined in (Gal et al., 2023a), where all available training samples for each object are utilized.\nHowever, we also conduct additional experiments to examine the impact of varying the number of\ntraining samples on the generation performance, as depicted in the top row of Fig. 11. Specifically,\nwe select the object \"black cat\" and employ 1, 2, 3, and 4 training samples from the complete dataset\nof 5 training samples in total. In the case of using only 1 training sample, it is employed as both\n14\nFigure 10: Training samples and naive generations. For each image pair, we show one training\nsample of a unique object on the left, and one generation result using our model with the text prompt\n\u201ca photo of a S\u22c6\u201d on the right.\nTable 6: More information on training images.\nIndex\nObject\nCategory\nFrom\n#Samples\n0\ncat statue\nToy\nTextual Inversion\n6\n1\nelephant statue\nToy\nTextual Inversion\n5\n2\nduck toy\nToy\nDreamBooth\n4\n3\nmonster toy\nToy\nDreamBooth\n5\n4\nteddy bear\nToy\nCustom Diffusion\n7\n5\ntortoise plushy\nToy\nCustom Diffusion\n12\n6\nbrown dog\nPet\nDreamBooth\n5\n7\nfat dog\nPet\nDreamBooth\n6\n8\nbrown dog\nPet\nDreamBooth\n5\n9\nblack cat\nPet\nDreamBooth\n5\n10\nbrown cat\nPet\nDreamBooth\n5\n11\nblack dog\nPet\nCustom Diffusion\n8\n12\nclock\nAccessory\nTextual Inversion\n5\n13\npink sunglasses\nAccessory\nDreamBooth\n6\n14\nfancy boot\nAccessory\nDreamBooth\n6\n15\nbackpack\nAccessory\nDreamBooth\n6\n16\nberry bowl\nContainer\nDreamBooth\n6\n17\nred teapot\nContainer\nTextual Inversion\n5\n18\nvase\nContainer\nDreamBooth\n6\n19\nbarn\nBuilding\nCustom Diffusion\n7\nthe denoising target and the reference image. The generated images from scenarios with only 1 or 2\ntraining samples exhibit a tendency to overfit the object, resulting in a diminished representation of\nthe textual information. Nevertheless, across all cases, our method consistently demonstrates high\nimage fidelity and quality.\nDifferent reference images. In addition, we also evaluate the generation performance by employing\ndifferent reference images during inference, including both seen images from the training set and\nunseen ones. The results are presented in the bottom row of Fig. 11. Remarkably, we observe that the\nvariations in the generated images are minimal when using the same random seed, underscoring the\nrobustness of our proposed visual condition to the input image. This finding suggests that our model\ncan effectively generalize and maintain consistent performance regardless of whether the reference\nimage is seen or unseen during training.\n15\nTable 7: Text prompt list for quantitative evaluation. \u201c{}\u201d represents S\u22c6 in Textual Inversion (Gal\net al., 2023a) and ours, and represents \u201c[V] class\u201d in DreamBooth (Ruiz et al., 2023) and Custom\nDiffusion (Kumari et al., 2023).\nText prompts for non-live objects\nText prompts for live objects\n\u201ca {} in the jungle\u201d\n\u201ca {} in the jungle\u201d\n\u201ca {} in the snow\u201d\n\u201ca {} in the snow\u201d\n\u201ca {} on the beach\u201d\n\u201ca {} on the beach\u201d\n\u201ca {} on a cobblestone street\u201d\n\u201ca {} on a cobblestone street\u201d\n\u201ca {} on top of pink fabric\u201d\n\u201ca {} on top of pink fabric\u201d\n\u201ca {} on top of a wooden floor\u201d\n\u201ca {} on top of a wooden floor\u201d\n\u201ca {} with a city in the background\u201d\n\u201ca {} with a city in the background\u201d\n\u201ca {} with a mountain in the background\u201d\n\u201ca {} with a mountain in the background\u201d\n\u201ca {} with a blue house in the background\u201d\n\u201ca {} with a blue house in the background\u201d\n\u201ca {} on top of a purple rug in a forest\u201d\n\u201ca {} on top of a purple rug in a forest\u201d\n\u201ca {} with a wheat field in the background\u201d\n\u201ca {} wearing a red hat\u201d\n\u201ca {} with a tree and autumn leaves in the background\u201d\n\u201ca {} wearing a santa hat\u201d\n\u201ca {} with the Eiffel Tower in the background\u201d\n\u201ca {} wearing a rainbow scarf\u201d\n\u201ca {} floating on top of water\u201d\n\u201ca {} wearing a black top hat and a monocle\u201d\n\u201ca {} floating in an ocean of milk\u201d\n\u201ca {} in a chef outfit\u201d\n\u201ca {} on top of green grass with sunflowers around it\u201d\n\u201ca {} in a firefighter outfit\u201d\n\u201ca {} on top of a mirror\u201d\n\u201ca {} in a police outfit\u201d\n\u201ca {} on top of the sidewalk in a crowded street\u201d\n\u201ca {} wearing pink glasses\u201d\n\u201ca {} on top of a dirt road\u201d\n\u201ca {} wearing a yellow shirt\u201d\n\u201ca {} on top of a white rug\u201d\n\u201ca {} in a purple wizard outfit\u201d\n\u201ca red {}\u201d\n\u201ca red {}\u201d\n\u201ca purple {}\u201d\n\u201ca purple {}\u201d\n\u201ca shiny {}\u201d\n\u201ca shiny {}\u201d\n\u201ca wet {}\u201d\n\u201ca wet {}\u201d\n\u201ca {} with Japanese modern city street in the background\u201d\n\u201ca {} with Japanese modern city street in the background\u201d\n\u201ca {} with a landscape from the Moon\u201d\n\u201ca {} with a landscape from the Moon\u201d\n\u201ca {} among the skyscrapers in New York city\u201d\n\u201ca {} among the skyscrapers in New York city\u201d\n\u201ca {} with a beautiful sunset\u201d\n\u201ca {} with a beautiful sunset\u201d\n\u201ca {} in a movie theater\u201d\n\u201ca {} in a movie theater\u201d\n\u201ca {} in a luxurious interior living room\u201d\n\u201ca {} in a luxurious interior living room\u201d\n\u201ca {} in a dream of a distant galaxy\u201d\n\u201ca {} in a dream of a distant galaxy\u201d\n1 training sample\n2 training samples\n3 training samples\n4 training samples\nFull 5 training samples\n3 training samples\nSeen reference image\n3 training samples\nSeen reference image\n3 training samples\nUnseen reference image\n3 training samples\nUnseen reference image\nFigure 11: Full 5 training samples vs. varying training samples and reference images. On the left,\nwe present the generated images using our model with the full 5 training samples. On the right, the\ntop row showcases the generated images with different numbers of training samples. In the bottom\nrow, we display the generated images using different reference images during inference, including\nboth seen images from the training set and unseen images. This analysis provides insights into the\neffects of training sample size and reference image selection on the image generation process.\nMultiple reference images.\nOur image cross-attention mechanism has the ability to handle a\nvariable number of input tokens. This flexibility enables us to seamlessly use multiple reference\nimages by accommodating any number of tokens from the concatenated reference images. In Fig. 12,\nwe compare the results obtained by using two reference images with those obtained by using a\nsingle reference image. We observe that using different types of reference images produces similar\ngenerated images, which highlights the robustness of the image cross-attention mechanism.\n16\nReference A\nReference B\nSingle A\nSingle B\nA + B\nA \ud835\udc46\u2217 on the beach\nFigure 12: Multiple reference images vs. a single reference image. For multiple reference images,\nwe concatenate the tokens corresponding to each image and pass the concatenated tokens through the\nimage cross-attention.\n300\n400\nTraining steps\nA \ud835\udc46\u2217 in a \nmovie theater\nA \ud835\udc46\u2217 with a \nbeautiful sunset\nA \ud835\udc46\u2217 in a leafy \nforest\nA \ud835\udc46\u2217 in a \nmovie theater\nA \ud835\udc46\u2217 in the \nsnow\nA \ud835\udc46\u2217 among the \nskyscrapers in \nNew York city\nA \ud835\udc46\u2217 on top of a \nwooden floor\nA \ud835\udc46\u2217 on top of \na purple rug \nin a forest\nA \ud835\udc46\u2217 wearing a \nblack top hat \nand a monocle\nTraining \nsamples\nFigure 13: Generation results with different training steps. In the top row, we present the training\nsamples. The middle row showcases the generated images after training for 300 steps, while the\nbottom row displays the generated images after training for 400 steps. Each object is evaluated using\nthree distinct text prompts, ensuring a comprehensive and unbiased assessment.\nD\nTRAINING STEP DISCUSSION\nIn the main paper, we report all results at the checkpoint of 400 training steps, which generally yield\nthe best overall performance within a short training time (\u223c5 minutes). However, we observe that for\nsome objects, text information is better preserved with fewer training steps. In Fig. 13, we present\nsome cases where training for 300 steps results in better preservation of text-related information. This\nobservation can be attributed to the fact that more training steps can potentially lead to overfitting the\ngeneration to the training images, thereby neglecting the information provided by the text prompt to\nsome extent. In practical applications, it is advisable to save multiple checkpoints at different training\nsteps, allowing users to choose the most suitable checkpoint for text-prompted inference.\nE\nUSER STUDY\nTo gain insights into human preferences regarding generation performance, we conduct a user study\nto evaluate our model along with the compared methods. The study consists of 18 samples of different\nobjects, including some objects used for quantitative evaluation as well as additional new ones. These\nsamples are prompted by various text prompts.\nDuring each trial, we generated 8 images using each method and selected the most visually appealing\nand best-aligned image as the candidate for comparison. For each question in the study, users were\nasked to assess the image candidates and choose the best one (or two if they found two results\nequally good) based on three perspectives: image quality, text fidelity, and object fidelity. An example\nquestion is shown in Fig. 14. In total, we collected answers from 40 users for a total of 18 comparative\nquestions, resulting in 720 individual responses across the three evaluation metrics. The user study\nvotes are plotted in Fig. 15 and the percentage results of the votes are reported in Tab. 8, providing a\ndistribution summary of the user preferences for each evaluated metric.\nThe results indicate that our method significantly outperforms the other models in all metrics. Note\nthat the users involved in the study have no prior knowledge of the specific task details and many\nof them come from non-technical backgrounds. The evaluation metrics used in the user study may\nbe subjective in nature, as they rely on the personal opinions and preferences of the participants.\n17\nFigure 14: An example question of the user study. We provide the reference image and the text\nprompt and ask the users to vote for one or two candidates based on three metrics.\nImage quality\nText fidelity\nObject fidelity\n0\n100\n200\n300\n400\n500\nUser votes\n113\n59\n73\n250\n316\n152\n241\n331\n306\n362\n392\n431\nModels by metrics\nTextual Inversion\nCustom Diffusion\nDreamBooth\nOurs\nFigure 15: User votes in three metrics. Note that the sum of votes in each metric is more than 720\nbecause the user can vote for 2 candidates in one question if they find two results equally good.\nHowever, this subjective evaluation provides a more human-centered perspective, which ensures that\nour model produces outputs that align with human preferences and expectations.\nTable 8: Comparison of user preferences. The percentages of votes are reported.\nImage quality\nText fidelity\nObject fidelity\nTextual Inversion (Gal et al., 2023a)\n0.117\n0.054\n0.076\nCustom Diffusion (Kumari et al., 2023)\n0.259\n0.288\n0.158\nDreamBooth (Ruiz et al., 2023)\n0.249\n0.301\n0.318\nViCo (ours)\n0.375\n0.357\n0.448\n18\nImage misalignment\nA \ud835\udc46\u2217 among the skyscrapers in \nNew York city\nA \ud835\udc46\u2217 floating on top of water\nA \ud835\udc46\u2217 in a luxurious interior \nliving room\nA \ud835\udc46\u2217 with a mountain in the \nbackground\nText misalignment\nA \ud835\udc46\u2217 on top of a wooden floor\nA \ud835\udc46\u2217 on top of pink fabric\nA \ud835\udc46\u2217 with a landscape from the \nMoon\nA \ud835\udc46\u2217 with the Eiffel Tower in the \nbackground\nFigure 16: Failure cases. We present two types of failure cases: image misalignment and text\nmisalignment. Each pair of samples consists of a reference image on the left and the corresponding\ngenerated image on the right.\nReference\nOurs w/o Reg\nOurs w/ Reg\n\ud835\udc3c!\"#$ = 0.686\n\ud835\udc3c%&\"' = 0.834\n\ud835\udc47%&\"' = 0.348\n\ud835\udc3c!\"#$ = 0.603\n\ud835\udc3c%&\"' = 0.746\n\ud835\udc47%&\"' = 0.368\nA photo of a S*\nin the snow\nFigure 17: The defect in quantitative metrics. We present a reference image along with two\ngenerated images: one produced with the regularization applied and the other without it. We compute\nthe three quantitative metrics below each generated image.\nF\nFAILURE CASES\nIn Fig. 16, we present several failure cases encountered by our model, highlighting the challenges and\nareas for improvement. These failure cases can be categorized into two types: image misalignment\nand text misalignment. Image misalignment occurs when the object has an intricate appearance,\nmaking it difficult for the model to accurately capture and reproduce all the details. Additionally,\nimage misalignment can also occur when there are multiple objects of the same category that co-occur,\nleading to difficulties in properly distinguishing the individual object of interest. Text misalignment,\non the other hand, is primarily caused by two factors. Firstly, it can occur due to the loss of text\ninformation, resulting in a mismatch between the intended text prompt and the synthesized image.\nSecondly, text misalignment can arise from the undesirable synthesis of the object and the text prompt,\nleading to unexpected or nonsensical combinations. While our current model faces these challenges,\nwe acknowledge them as areas for future improvement and research.\nG\nDEFECT IN QUANTITATIVE METRICS\nOur quantitative metrics are based on pretrained models CLIP (Radford et al., 2021) and DINO (Caron\net al., 2021), which produce global representations for images. Therefore, it may be hard to reflect\nsome local details in quantitative comparison stemming from comparing feature vectors. For example,\nwe present two images generated with and without the regularization and compute three similarity\nscores in Fig. 17. We notice that although the image generated using the regularization shows\nbetter quality, its image similarity metric greatly lags behind the other because the one without the\nregularization overfits to the training data to some extent (presenting the \u201ccabinet\u201d). We believe a more\nproper evaluation metric that can reflect the details in the images is desired for future advancement.\n19\nt=50\nt=40\nt=30\nt=20\nt=10\nt=0\nt=50\nt=40\nt=30\nt=20\nt=10\nt=0\nFigure 18: Mask samples in inference. For each instance (\u201cteddy bear\u201d and \u201cclock\u201d), given a\nreference image (left), we visualize the attention associated with S\u22c6 (top) and the corresponding mask\n(bottom) of it with an interval of 10 during the 50 inference steps.\nTraining: 0\n400 epochs\nFigure 19: Mask samples during training. Given a reference image (left), we visualize the mask of\nit, uniformly sampled throughout the training process. Except for the mask at the very beginning of\ntraining, all masks in subsequent steps present good image matting, which can be efficiently used in\nthe visual condition module.\nH\nADDITIONAL VISUALIZATIONS OF ATTENTION\nMask visualization.\nWe visualize the mask of the reference image derived from the attention map\nduring training and inference. In Fig. 18, we visualize attention maps and corresponding binarized\nmasks along with sampling steps, which exhibit the notable effect of image matting. In Fig. 19,\nwe visualize the object mask throughout the training process. In the initial steps, the mask rapidly\nconverges to a reliable indicator for object segmentation, consistently showcasing the significant\nimage matting in subsequent steps.\nEffect of the regularization on attention.\nWe provide additional visualization results to show\nthat regularization effectively directs the attention to focus on the object of interest in Fig. 20. We\nalso observed that in certain cases, such as the last row in Fig. 20, the regularization had minimal\nimpact on the attention. Therefore, in practical applications, users have the flexibility to customize\nthe weight of the regularization during training to further control the attention behavior according to\ntheir specific needs.\nAttention at each step.\nThe visualized attention presented in Fig. 20 represents the average\nattention across all inference steps. Additionally, we provide visualizations of the attention at each\ninference step with an interval of 5 in Fig. 21, allowing for a more detailed observation of the attention\ndynamics throughout the generation process. In the early steps, the attention tends to exhibit more\nscattered responses, exploring different regions in the generated image. As the inference progresses,\nthe attention quickly converges and becomes more focused on the object of interest.\n20\nA \ud835\udc46\u2217 with a city in the background\nA \ud835\udc46\u2217 on top of a wooden floor\nA \ud835\udc46\u2217 on the beach\nA \ud835\udc46\u2217\nA \ud835\udc46\u2217 on top of a dirt road\nA \ud835\udc46\u2217 in New York city\nReference\nTextual Inversion\nOurs w/o reg\nOurs w/ reg\nFigure 20: Visualization of attention associated with S\u22c6. We visualize the average attention in\nthe inference process of Textual Inversion (Gal et al., 2023a) and ours with or without the proposed\nregularization.\nt=0\nt=5\nt=10\nt=15\nt=20\nt=25\nt=30\nt=35\nt=40\nt=45\nt=50\nGenerated\nImage\nInference Sampling (T=50)\nAverage\nAttention\nFigure 21: Inference steps. We visualize the attention associated with S\u22c6 with an interval of 5 during\nthe 50 inference steps. We also present the generated image (on the left) and the average attention\nalong with all inference steps (on the right).\n21\nOFT\nViCo \nw/ OFT\nViCo\n400\n1200\n2000\n2800\nIteration:\nInput images\nFigure 22: Generation results across different number of iterations. We compare ViCo, OFT (Qiu\net al., 2023), and using both ViCo and OFT when training different iterations.\nI\nADDITIONAL COMPARISON RESULTS\nDue to the page limit of the main paper, we include additional generation results in Fig. 26. The\nresults compare our model with Textual Inversion (Gal et al., 2023a), DreamBooth (Ruiz et al., 2023),\nand Custom Diffusion (Kumari et al., 2023). For each comparison, we select the visually best image\nfrom a set of 8 randomly generated images using different objects. We observe that our method\nperforms on par with, or even surpasses, the finetuning-based methods DreamBooth (Ruiz et al.,\n2023) and Custom Diffusion (Kumari et al., 2023). The generated images exhibit high quality in\nterms of image fidelity, text fidelity, text-image equilibrium, authenticity, and diversity.\nComparison with OFT.\nOrthogonal Finetuning (OFT) (Qiu et al., 2023) is a recently proposed\nfine-tuning method that can be efficiently used to fine-tune DreamBooth (Ruiz et al., 2023). OFT\ndemonstrates stability in generating images even with a large number of training steps. It can also be\napplied in conjunction with ViCo. In Fig. 22, we present the generated results comparing different\niterations of OFT, ViCo, and ViCo with OFT. In the early steps (e.g., 400), ViCo is capable of\nproducing images that preserve fine object details, while OFT has not yet learned accurate object-\nspecific semantics. As the number of training steps increases significantly (e.g., 2800), the generated\nimages by OFT exhibit slight distortions, and ViCo may overfit to the training image, resulting in\nthe loss of text information. By combining ViCo with OFT, we can address both of these issues and\nachieve the highest generation quality at all iterations.\nComparison with the encoder-based method ET4.\nEncoder for Tuning (E4T) (Gal et al., 2023b)\nis a representative work of encoder-based models (Shi et al., 2023; Jia et al., 2023; Gal et al., 2023b;\nChen et al., 2023). It is worth noting that these models, including E4T, are not directly related to\nour current study, as they necessitate training on extensive datasets specific to particular category\ndomains. In Fig. 23, we compare our model with E4T which is pretrained on a substantial dataset of\nhuman faces. According to Gal et al. (2023b), a large batch size of 16 or larger is crucial. To adhere\nto this setting, we implement the use of gradient accumulation over 16 steps in E4T. Notably, our\nmodel excels in preserving facial details compared to E4T.\nJ\nADDITIONAL RESULTS OF VARIOUS APPLICATIONS\nWe present the results of our model deployed to various applications in Fig. 27. In addition to the\napplications of recontextualization, art renditions, and costume changing, which have been showcased\nin the main paper, we further demonstrate the effectiveness of our model in activity control, such as\ncontrolling a dog to sleep or to jump, and attribute editing, such as editing the pattern or the color of\na container. We also show more generated results in terms of complicated style change in Fig. 28,\nhighlighting our model\u2019s edit-ability to effectively condition on complex styles. Furthermore, we\n22\nOurs\n(5-image)\n\ud835\udc46\u2217 on the \nbeach\n\ud835\udc46\u2217 with a \nmountain\n\ud835\udc46\u2217 with a \nsunset\nCyberpunk \ud835\udc46\u2217\n\ud835\udc46\u2217 in the \nkitchen\n\ud835\udc46\u2217 in a movie \ntheater\nE4T\nOurs\n(1-image)\n\u201cYann LeCun\u201d\n\u201cGal Gadot\u201d\nFace \nsample\nFigure 23: Comparison with E4T (Gal et al., 2023b) on face images. E4T is initially pretrained on\na large-scale dataset of human faces and then fine-tuned using a single image of \u201cYann Lecun\u201d or\n\u201cGal Gadot\u201d. We present our results by training ViCo on either a single image or five images.\nFlying\nLight\nSpider Man\nChef\nWave\nFire\nGreen\nApple\nTraining samples\nFigure 24: Comic character generation. We generate text-guided images (on the right) with the\ngiven comic object (on the left). Our results preserve the comic style and the appearance of the object.\npresent the implementation of our model for comic character generation in Fig. 24. This application\nallows us to generate comic-style images that exhibit a text-guided property. These additional\nexamples highlight the versatility and flexibility of our model in various creative and interactive\nscenarios.\nMulti-object composition.\nOur model can also be easily altered to support multi-object composi-\ntion with two different objects. Particularly, we train S1\u22c6, S2\u22c6, and unified image cross-attention\nblocks with two datasets of different objects. In inference, we feed two different reference images\ninto the image cross-attention by token concatenation and respectively obtain object masks from S1\u22c6,\nS2\u22c6. We report our results of multi-object composition in Fig. 25.\n23\n\ud835\udc461\u2217 \n\ud835\udc462\u2217\n\ud835\udc461\u2217 and \ud835\udc462\u2217\n\ud835\udc461\u2217 and \ud835\udc462\u2217\n\ud835\udc461\u2217\n\ud835\udc462\u2217\n\ud835\udc461\u2217 in the style of \ud835\udc462\u2217\n\ud835\udc461\u2217\n\ud835\udc462\u2217 \nFigure 25: Multi-object composition. Our model supports multi-object compositions, demonstrating\nresults in two composition types: (1) simultaneous appearance of two objects, and (2) one object in\nthe style of the other object.\n24\nA \ud835\udc46\u2217 on top of pink fabric\nTraining Samples\nDreamBooth\nCustom Diffusion\nTextual Inversion\nOurs\nA \ud835\udc46\u2217 with a wheat 1ield in the background\nA \ud835\udc46\u2217 with a mountain in the background\nA \ud835\udc46\u2217 in the jungle\nA \ud835\udc46\u2217 with the Eiffel Tower in the background\nA \ud835\udc46\u2217 in a luxurious interior living room\nA \ud835\udc46\u2217 on top of a purple rug in a forest\nFigure 26: Additional comparison results on more objects. Our model consistently demonstrates\nsuperb performance across all experimental trials. Zoom in to see the image details.\n25\nTraining samples\nRecontextualization\nArt Renditions\nA \ud835\udc46\u2217 among the \nskyscrapers in New York\nA \ud835\udc46\u2217 on top of green grass \nwith sun7lowers around it\nA \ud835\udc46\u2217 in the snow\nA \ud835\udc46\u2217 on a \ncobblestone street\nA \ud835\udc46\u2217 on the beach\nA \ud835\udc46\u2217 with a mountain \nin the background\nA \ud835\udc46\u2217 with Egyptian \npyramids\nA \ud835\udc46\u2217 in a football \nstadium\nA \ud835\udc46\u2217 underwater\nA \ud835\udc46\u2217 on the surface \nof Mars\nVan Gogh\nVermeer\nMonet\nRembrandt\nLeonardo da Vinci\nCostume\nActivity\nSpider Man out7it\nIron Man out7it\nSanta hat\nRainbow scarf\nYellow glasses\nRunning\nJumping\nPlaying with a ball\nSleeping\nSwimming\nAttributes\nBird \ud83d\udc26\nCat \ud83d\udc31\nDragon \ud83d\udc32\nOwl \ud83e\udd89\nRed \ud83d\udd34\nYellow \ud83d\udfe1\nGreen \ud83d\udfe2\nPurple \ud83d\udfe3\nSilver \u26aa\nBuddha \ud83e\uddd8\nFigure 27: Additional applications using our method. ViCo excels in various tasks, including\nrecontextualizing input images, synthesizing diverse art renditions, changing costumes, controlling\nobject poses in different activities, and editing object intrinsic attributes. The generated images\nexhibit high authenticity. Zoom in to see the image details.\n26\nVan Gogh\nMonet\nImpressionism\nRembrandt\nVermeer\nDa Vinci\nInk water\nStyles\nImage samples\nFigure 28: Generated results in various styles. Each style is prompted by the text condition \u201ca\npainting of S\u22c6 in the style of [STYLE]\u201d.\n27\nK\nBROADER IMPACTS\nOur research on personalized text-to-image generation has significant impacts, both positive and\nnegative. On the positive side, our approach has numerous applications, such as recontextualization,\nart renditions, and costume changing, which can be widely utilized in industries like artistic creations,\nmedia production, and advertising. For example, in advertising, designers can efficiently synthesize\ndrafts of the advertising subject and the desired context with minimal effort.\nHowever, we acknowledge that our research also raises social concerns regarding the potential\nunethical use of fake images. There is a risk of malicious exploitation as our model allows easy\ngeneration of photorealistic images by anyone, even without prior technical knowledge. This could\nlead to the fabrication of images that include specific individuals or personal belongings. For instance,\nusing readily available selfies from the internet, individuals with malicious intent could fabricate\nimages to engage in fraudulent activities or defamatory actions by portraying someone unethically.\nTo mitigate the potential negative impacts, we strongly advocate for strict regulation and responsible\nuse of this technology. It is essential to establish guidelines and ethical frameworks to govern the\ndeployment and application of personalized text-to-image generation models like ours. With proper\nregulations, we can help prevent misuse and ensure that this technology is used for legitimate and\nethical purposes. This could include measures such as obtaining consent for image generation,\nimplementing authentication mechanisms, and promoting public awareness about the risks and ethical\nconsiderations associated with this technology.\n28\n"
  },
  {
    "title": "Inserting Anybody in Diffusion Models via Celeb Basis",
    "link": "https://arxiv.org/pdf/2306.00926.pdf",
    "upvote": "2",
    "text": "Inserting Anybody in Diffusion Models via Celeb Basis\nGe Yuan1,2\nXiaodong Cun2\nYong Zhang2\nMaomao Li2\u2217\nChenyang Qi2,3\nXintao Wang2\nYing Shan2\nHuicheng Zheng1\u2217\n1 Sun Yat-sen University\n2 Tencent AI Lab\n3 HKUST\nhttp://celeb-basis.github.io\nv1\u00a0is playing the guitar\nv1 eats bread in front\nof the Effiel Tower\nv1\u00a0as a knight in plate\narmor\nv1\u00a0wears a scifi\nspacesuit in space\nv1\u00a0and Barack Obama\ncooks in a kitchen\nv1 and Anne Hathaway are\nbaking a birthday cake\nv1\nv2\nv1 shakes hands with\nv2\nv1 and v2\u00a0enjoy a day at\nan amusement park\nv1 and v2 enjoy a delicate\ncandlelight dinner\nv1 and v2\u00a0sit on a sofa\nv1\u00a0and v2 cook\ntogether in a kitchen\nv1 and v2 play the\npiano together\nManga drawing of v1\nUkiyo-e painting of v1\nCubism painting of v1\nBanksy art of v1\nCave mural depicting v1\nv1 in a comic book\n Single Input\nGenearted Image\nFigure 1: Given a single facial photo (v1 or v2) as a tunable sample, the proposed method can\ninsert this identity into the trained text-to-image model, e.g., Stable Diffusion [1], where the new\nperson (v1) can act like the original concept in the trained model and interact with another newly\ntrained concept (v2). Note that the input images are randomly generated from StyleGAN [2].\nAbstract\nExquisite demand exists for customizing the pretrained large text-to-image model,\ne.g., Stable Diffusion, to generate innovative concepts, such as the users themselves.\nHowever, the newly-added concept from previous customization methods often\nshows weaker combination abilities than the original ones even given several\nimages during training. We thus propose a new personalization method that allows\nfor the seamless integration of a unique individual into the pre-trained diffusion\nmodel using just one facial photograph and only 1024 learnable parameters under\n3 minutes. So as we can effortlessly generate stunning images of this person in\nany pose or position, interacting with anyone and doing anything imaginable from\ntext prompts. To achieve this, we first analyze and build a well-defined celeb basis\nfrom the embedding space of the pre-trained large text encoder. Then, given one\nfacial photo as the target identity, we generate its own embedding by optimizing the\nweight of this basis and locking all other parameters. Empowered by the proposed\nceleb basis, the new identity in our customized model showcases a better concept\n\u2217Corresponding authors\nPreprint. Under review.\narXiv:2306.00926v1  [cs.CV]  1 Jun 2023\ncombination ability than previous personalization methods. Besides, our model\ncan also learn several new identities at once and interact with each other where the\nprevious customization model fails to. The code will be released.\n1\nIntroduction\nVast image-text pairs [3] during training and the powerful language encoder [4] enable the text-to-\nimage models [1, 5, 6] to generate diverse and fantastic images from simple text prompts. Though\nthe generated images are exquisite, they may still fail to satisfy the users\u2019 demands since some\nconcepts are not easy to be described by the text prompt [7]. For instance, individuals frequently\npost self-portraits on social media platforms, where the pre-trained text-to-image models struggle to\nproduce satisfactory pictures of them despite receiving comprehensive instructions. This shortcoming\nmakes these models less attractive to general users.\nRecent works [7\u201310] solve this problem via efficiently tuning the parameters of the model for\npersonalization usage. For example, these techniques insert the new concepts (e.g., a specific bag,\na dog, or a person) into the model by representing them as rarely-used pseudo-words [10] (or text-\nembeddings [7\u20139, 11]) and finetuning the text-to-image model with a few of these samples [7, 10].\nAfter training, these pseudo-words or embeddings can be used to represent the desired concept and\ncan also perform some combination abilities. However, these methods often struggle to generate\nthe text description-aligned image with the same concept class (e.g., the person identities) [8, 10].\nFor instance, the original Stable Diffusion [1] model can successfully generate the image of the\ntext prompt:\u201c Barack Obama and Anne Hathaway are shaking hands.\" Nevertheless, in terms of\ngenerating two newly-learned individuals, the previous personalized methods [7, 8, 10] fall short of\nproducing the desired identities as depicted in our experiments (Figure 5).\nIn this work, we focus on injecting the most specific and widely-existing concept, i.e., the human\nbeing, into the diffusion model seamlessly. Drawing inspiration from the remarkable 3DMM [12],\nwhich ingeniously represents novel faces through a combination of mean and weight values derived\nfrom a clearly defined basis, we build a similar basis to the embeddings of the celebrity names in\npretrained Stable Diffusion. In this way, we are capable of representing any new person in the trained\ndiffusion model via the basis coefficients.\nWe first collect a bunch of celebrity names from the Internet and filter them by the pre-trained text-to-\nimage models [1]. By doing so, we obtain 691 well-known names and extract the text embedding\nby the tokenizer of the CLIP [4]. Then, we construct a celeb basis via Principal Component\nAnalysis (PCA [13]). To represent a new person with PCA coefficients, we use a pre-trained face\nrecognition network [14] as the feature extractor of the given photo and learn a series of coefficients to\nre-weight the celeb basis, so that the new face can be recognized by the pre-trained CLIP transformer\nencoder. During the process, we only use a single facial photo and fix the denoising UNet and\nthe text encoder of Stable Diffusion to avoid overfitting. After training, we only need to store the\n1024 coefficients of the celeb basis to represent the newly-added identity since the basis is shared\nacross the model. Yet simple, the concept composition abilities [1] of the trained new individual\nis well-preserved, as we only reweight the text embeddings of the trained CLIP model and freeze\nthe weights in the diffusion process. Remarkably, the proposed method has the ability to produce\na strikingly realistic photo of the injected face in any given location and pose. Moreover, it opens\nup some new possibilities such as learning multiple new individuals simultaneously and facilitating\nseamless interaction between these newly generated identities.\nThe contributions of the paper are listed as follows:\n\u2022 We propose celeb basis, a basis built from the text embedding space of the celebrities\u2019 names\nin the text-to-image model and verify its abilities, such as interpolation.\n\u2022 Based on the proposed celeb basis, we design a new personalization method for the text-to-\nimage model, which can remember any new person from a single facial photo using only\n1024 learnable coefficients.\n\u2022 Extensive experiments show our personalized method has more stable concept composition\nabilities than previous works, including generating better identity-preserved images and\ninteracting with new concepts.\n2\n2\nRelated Work\nImage Generation and Editing. Given a huge number of images as the training set, deep generative\nmodels target to model the distribution of training data and synthesize new realistic images through\nsampling. Various techniques have been widely explored, including GAN [2, 15], VAE [16, 17],\nAutoregressive [6, 18\u201320], flow [21, 22]. Recently, diffusion models [23, 24] gain increasing\npopularity for their stronger abilities of text-to-image generation [1, 5, 6]. Conditioned on the text\nembedding of pre-trained large language models [4], these diffusion models are iterative optimized\nusing a simple denoising loss. During inference, a new image can be generated from sampled\nGaussian noise and a text prompt. Although these diffusion models can synthesize high-fidelity\nimages, they have difficulties in generating less common concepts [25] or controlling the identity\nof generated objects [7]. Current editing methods are still hard to solve this problem, e.g., directly\nblending the latent of objects [26, 27] to the generated background will show the obvious artifacts\nand is difficult to understand the scenes correctly [28]. On the other hand, attention-based editing\nworks [29\u201332] only change the appearance or motion of local objects, which can not generate diverse\nnew images with the same concept (e.g., human and animal identity) from the referred image.\nModel Personalization. Different from text-driven image editing, tuning the model for the specific\nunseen concept, i.e., personalized model, remembers the new concepts of the reference images and\ncan synthesize totally unseen images of them, e.g., appearance in a new environment, interaction\nwith other concepts in the original stable diffusion. For generative adversarial neworks [2, 33],\npersonalization through GAN inversion has been extensively studied. This progress typically involves\nfinetuning of the generator [34, 35], test-time optimization of the latents [36], or a pre-trained\nencoder [37]. Given the recent diffusion generative model [1, 5], it is straightforward to adopt\nprevious GAN inversion techniques for the personalization of diffusion models. Dreambooth [10]\nfinetunes all weight of the diffusion model on a set of images with the same identity and marks it\nas the specific token. Meanwhile, another line of works [7, 9, 38] optimizes the text embedding\nof special tokens (e.g., V \u2217) to map the input image while freezing the diffusion model. Later on,\nseveral works [8, 39, 40] combine these two strategies for multi-concept interaction and efficient\nfinetuning taking less storage and time. These methods focus on general concepts in the open\ndomain while struggling to generate interactions between fine-grained concepts, i.e.human beings\nwith specific identities. Since most of the previous works require the tuning in the test time, training\ninversion encoders are also proposed to generate textual embedding from a single image in the\nopen domain (e.g., UMM [41], ELITE [42], and SuTI [43]), or in human and animal domain (e.g.,\nTaming-Encoder [44], Instant-Booth [45], E4T [11]). However, a general human identity-oriented\nembedding is difficult to be obtained from a naively optimized encoder, and tuning the Stable\nDiffusion on larger-scale images often causes the concept forget. In contrast, our method focuses on\na better representation of identity embedding in the diffusion model (celeb basis in Sec. 3.1), which\nsignificantly eases the process of optimization such that we only need 1024 parameters to represent\nan identity more correctly as in Sec. 3.2 and stronger concept combination abilities.\nIdentity Basis. Representing the human identity via basis is not new in traditional computer vision\ntasks. e.g., in human face modeling, 3D Morphable Models [12] and its following models [46, 47]\nscans several humans and represent the shape, expression, identity, and pose as the PCA coeffi-\ncients [13], so that the new person can be modeled or optimized via the coefficients of the face basis.\nSimilar ideas are also used for face recognition [48], where the faces in the dataset are collected and\nbuilt on the basis of the decision bound. Inspired by these methods, our approach takes advantage of\nthe learned celebrity names in the pre-trained text-to-image diffusion model, where we build a basis\non this celebrity space and generate the new person via a series of learned coefficients.\n3\nMethod\nOur method aims to introduce a new identity to the pre-trained text-to-image model, i.e., Stable\nDiffusion [1], from a single photo via the optimized coefficients of our self-built celeb basis. So that\nit can memorize this identity and generate new images of this person in any new pose and interact\nwith other identities via text prompts. To achieve this, we first analyze and build a celeb basis on\nthe embedding space of the text encoder through the names of the celebrities (Sec. 3.1). Then, we\ndesign a face encoder-based method to optimize the coefficients of the celeb basis for text-to-image\ndiffusion model customization (Sec. 3.2).\n3\nBeyonc\nTom Cruise\nJackie Chan\nGeorge Carlin\nStephen Chow\nRowan Atkinson\nFilter by Stable Diffusion\nAdam Savage\nBeyonc\nJackie Chan\n...\nStan Lee\nSearched\nCelebrity\nNames\nCeleb Embeddings\n...\nCeleb Basis\nStable\nDiffusion\n\"A photo of XXX \"\nSatisfied Names\nNo used Names\nPCA\n...\n\u2705 Can generated human image\n\u2705 Relatively good identity\n\u2705 Good concept combination\n...\nTokenizer & Embedding Lookup\n: mean person\n: principal direction\nDavid Bowie\nFigure 2: The building process of the proposed Celeb Basis. First, we collect about 1,500 celebrity\nnames as the initial collection. Then, we manually filter the initial one to m = 691 names, based on\nthe synthesis quality of text-to-image diffusion model [1] with corresponding name prompt. Later,\neach filtered name is tokenized and encoded into a celeb embedding group gi. Finally, we conduct\nPrinciple Component Analysis to build a compact orthogonal basis, which is visualized on the right.\n3.1\nCeleb Basis\nPreliminary: Text Embeddings in Text-to-Image Diffusion Models. In the text-to-image model,\ngiven any text prompts u, the tokenizer of typical text encoder model etext, e.g., BERT [49] and\nCLIP [4], divides and encodes u into l integer tokens by order. Correspondingly, by looking up the\ndictionary, an embedding group g = [v1, ..., vl] consisting of l word embeddings can be obtained,\nwhere each embedding vi \u2208 Rd. Then the text transformer \u03c4text in etext encodes g and generates\ntext condition \u03c4text(g). The condition \u03c4text(g) is fed to the conditional denoising diffusion model\n\u03f5\u03b8(zt, t, \u03c4text(g)) and synthesize the output image following an iterative denoising process [23], where\nt is the timestamp, zt is an image or latent noised to t. Previous text-to-image model personalization\nmethods [7, 9, 10] have shown the importance of text embedding g in personalizing semantic concepts.\nHowever, in text-to-image models\u2019 personalization, they only consider it as an optimization goal [7\u20139],\ninstead of improving its representation.\nInterpolating Abilities of Text Embeddings. Previous works have shown that text embedding\nmixups [50] benefit text classification. To verify the interpolation abilities in text-to-image generation,\nwe randomly pick two celebrity names embeddings v1 and v2, and linearly combine them as \u02c6v =\n\u03bbv1 + (1 \u2212 \u03bb)v2, where 0 < \u03bb < 1. Interestingly, the generated image of the interpolated embedding\n\u02c6v also contains a human face as shown in Figure 3, and all the generated images perform well in\nacting and interacting with other celebrities. Motivated by the above finding, we build a celeb basis\nso that each new identity can lie in the space formed by celebrity embeddings.\n\ud835\udc63! = Robert Downey\n\ud835\udc63\" = Barack Obama\n#\ud835\udc63 = 0.6\ud835\udc63! + 0.4\ud835\udc63\"\n#\ud835\udc63 is playing \nthe guitar\n#\ud835\udc63 is playing a \nbasketball\n#\ud835\udc63 is eating \nbreads in front of \nthe Eiffel Tower\n#\ud835\udc63 is talking with \nAnne Hathaway\nA photo of #\ud835\udc63\nA photo of #\ud835\udc63\nA photo of #\ud835\udc63\nA photo of #\ud835\udc63\nFigure 3: The interpolated text-embedding of two celebrities is still a human (top row) and it also can\nperform strong concept combination abilities in the pretrained Stable Diffusion [1] (bottom row).\nBuild Celeb Basis from Embeddings of the Collected Celebrities. As shown in Figure 2, we first\ncrawl about 1,500 celebrity names from Wikipedia as the initial collection. Then, we build a manual\nfilter based on the trained text-to-image diffusion model [1] by constructing the prompts of each name\nand synthesizing images. A satisfied celeb name should have the ability to generate human images\nwith prompt-consistent identity and interact with other celebs in synthesized results. Overall, we get\nm = 691 satisfied celeb names where each name ui, i \u2208 {1, ..., m} can be tokenized and encoded\ninto a celeb embedding group gi = [vi\n1, ..., vi\nki], notice that the length ki of each celeb embedding\ngroup gi might not the same since each name may contain multiple words (or the tokenizer will be\n4\nFigure 4: During training (left), we optimize the coefficients of the celeb basis with the help of a\nfixed face encoder. During inference (right), we combine the learned personalized weights and shared\nceleb basis to generate images with the input identity.\ndivided the word by sub-words). To simplify the formula, we compose the nonrepetitive embeddings\nso that each gi only contains the first two embeddings (i.e., ki = 2 for all m celebrities). Using C1\nand C2, i.e., Ck = [v1\nk, ..., vm\nk ], to denote the first and second embeddings of each gi respectively, we\ncan roughly understood them as the first name and last name embedding sets.\nTo further build a compact search space, inspired by 3DMM [12] which uses PCA [13] to map\nhigh-dimensional scanned 3D face coordinates into a compact lower-dimensional space, for each\nembedding set Ck, we calculate its mean Ck =\n1\nm\nPm\ni=1 vi\nk and PCA mapping matrix Bk =\nPCA(Ck, p), where Ck \u2208 Rd and PCA(X, p) indicates the PCA operation that reduces the second\ndimension of matrix X \u2208 Rm\u00d7d into p (p < d) principal components, i.e., Bk = [b1\nk, ..., bp\nk]. As\nshown in Fig 2, the mean embedding Ck still represents a face and we can get the new face via some\ncoefficients applied to Bk.\nOverall, our celeb basis is defined on two basis [C1, B1] and [C2, B2] working like the first and last\nname. We use the corresponding principle components A1 and A2 (where Ak = [\u03b11\nk, ..., \u03b1p\nk]) to\nrepresent new identities. Formally, for each new person \u02c6g, we use two p-dimensional coefficients of\nthe celeb basis and can be written by:\n\u02c6g = [\u02c6v1, \u02c6v2],\n\u02c6vk = Ck +\np\nX\nx=1\n\u03b1x\nkbx\nk,\n(1)\nIn practice, p equals 512 as discussed in the ablation experiments.\nTo control the generated identities, we optimize the coefficients with the help of a face encoder as the\npersonalization method. We introduce it in the below section.\n3.2\nStable Diffusion Personalization via Celeb Basis\nFast Coefficients Optimization for Specific Identity. Given a single facial photo, we use the\nproposed celeb basis to embed the given face image x of the target identity into the pretrained\ntext-to-image diffusion model as shown in Fig. 4. Since direct optimization is hard to find the\noptimized weight, we consider using the pre-trained state-of-the-art face recognition models F, i.e.,\nArcFace [14], to capture the identity-discriminative patterns. In detail, we adopt the F to extract 512\ndimension face embedding as priors. Then a single-layer MLP followed by an L2-normalization is\nused to map the face priors into the modulating coefficients A1 and A2. Following the Eq. 1, we can\nobtain the embedding group \u02c6g of the x using the pre-defined basis. By representing the text prompt\nof \u02c6g as V \u2217, we can involve V \u2217 to build the training pairs between the text prompt of input face and\n\u201cA photo of V \u2217\u201d, \u201cA depiction of V \u2217\u201d, etc.. Similar to previous works [7, 8, 10], we only use simple\ndiffusion denoising loss [23]:\nE\u03f5\u223cN(0,1),x,t,g[\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, \u03c4text(g))\u2225],\n(2)\nwhere \u03f5 is the unscaled noise sample, g denotes the text embeddings containing \u02c6g. During training,\nonly the weights of MLP need to be optimized, while other modules, including the celeb basis, face\nencoder F, CLIP transformer \u03c4text, and UNet \u03f5\u03b8 are fixed. Thus, the original composition abilities of\nthe trained text-to-image network are well-preserved, avoiding the forgetting problem. Since we only\nhave a single photo, we use color jitter, random resize, and random shift as data augmentations on\n5\nthe supervision to avoid overfitting. Notice that, we find our augmentation method can work well\neven though there are no regularization datasets which is important in previous methods [7, 8, 10],\nshowing the strong learning abilities of the proposed methods. Since the proposed methods only\ninvolve a few parameters, it only takes almost 3 minutes for each individual on an NVIDIA A100\nGPU, which is also much faster than the previous.\nTesting. After training, only two groups of coefficients A1 and A2 applied to the principal celeb\nbasis components need to be saved. In practice, the number of principal components of each group\nis p = 512, coming to only 1024 parameters and 2-3KB storage consumption for half-precision\nfloatings. Then, users can build the prompt with multiple action description prompts (e.g.\u201cA photo of\nV \u2217 is playing guitar\u201d) to synthesize the satisfied images as described in Figure 4.\nMultiple Identities Jointly Optimization. Most previous methods only work on a single new\nconcept [9, 10], Custom Diffusion [8] claim their method can generate the images of multiple new\nconcepts (e.g., the sofa and cat). However, for similar concepts, e.g., the different person, their\nmethod might not work well as in our experiments. Besides, their method is still struggling to\nlearn multiple (> 3) concepts altogether as in their limitation. Differently, we can learn multiple\nidentities (e.g., 10) at once using a shared MLP mapping layer as in Fig 4. In detail, we simply extend\nour training images to 10 and jointly train these images using a similar process as single identities.\nWithout a specific design, the proposed method can successfully generate the weight of each identity.\nAfter training, our method can perform interactions between each new identity while the previous\nmethods fail to as shown in the experiments. More implementation details are in the supplementary.\n4\nExperiments\n4.1\nDatasets and Metrics\nDatasets. We conduct experiments on the self-collected 2k synthetic facial images generated by\nStyleGAN [2]. By utilizing synthetic faces for assessing the effectiveness of generative models, one\ncan ease the reliance on the dataset foundation of the initial pre-trained text-to-image model. We\nalso perform some experiments on the real photo of the individual, more results and comparisons are\nshown in the supplemental materials.\nMetrics. First, we assess the performance of the generated images by utilizing objective metrics.\nFor instance, we calculate the consistency between the prompt and generated image through CLIP\nscore [4], which is denoted as \u201cPrompt\" in tables. Additionally, ensuring identity consistency and\nclarity of facial features are crucial aspects of our task. Therefore, we evaluate identity similarity using\na pretrained face recognition encoder [14] and mark it as \u201cIdentity\". Furthermore, to demonstrate the\nrationality behind generation, we also calculate the rate of successful face detection (\u201cDetect\") via\na pretrained face detector [14]. Lastly, user studies are conducted to evaluate text-image alignment\nalong with identity and photo qualities.\n4.2\nComparing with state-of-the-art Methods\nWe compare the proposed method with several well-known state-of-the-art personalization methods\nfor Stable Diffusion [1], including DreamBooth [10], Textural-Inversion [7] and Custom Diffusion [8].\nAs shown in Figure 5, given one single image as input, we evaluate the performance of several different\ntypes of generation, including the simple stylization, concept combination abilities, and two new\nconcept interactions. Textural inversion tends to overfit the input image so most of the concepts are\nforgotten. Although dreambooth [10] and custom diffusion [8] can successfully generate the human\nand the concept, the generated identities are not the same as the target image.\nTable 1: Quantitative comparisons.\nMethods\nObjective Metrics\u2191\nUser Study\u2191\n#Params\u2193\nTime\u2193\nPrompt\nIdentity\nDetect\nQuality\nText\nIdentity\n(min)\nTextual Inversion [7]\n0.1635\n0.2958\n92.86%\n2.23\n1.88\n2.55\n1,536\n24\nDreambooth [10]\n0.2002\n0.0512\n54.76%\n3.32\n3.70\n2.75\n9.83 \u00d7 108\n16\nCustom Diffusion [8]\n0.2608\n0.1385\n80.39%\n3.31\n3.55\n2.96\n5.71 \u00d7 107\n12\nOurs\n0.2545\n0.2072\n84.78%\n3.47\n4.01\n3.37\n1,024\n3\n6\nTarget Image\nTextural Inversion\nDreamBooth\nCustom Diffusion\nOurs\na photo of\neating bread in front of the Eiffel Tower\nwearing yellow jacket, and driving a motorbike\nand Barack Obama cooking together in a kitchen\nand Anne Hathaway having a delicate candlelight dinner \nshakes hands with\nand       enjoying a day at an amusement park\na sand sculpture of\nFunko Pop\nFigure 5: We compare several different abilities between our method and baselines (Textural Inver-\nsion [7], Dreambooth [10], and Custom Diffusion [8]).\n7\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nIdentity Similarity\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nPrompt Similarity\nStylized Portrait\nSingle Action\nWith Celeb\nTwo New Person\nTextual Inversion\nDreamBooth\nCustom Diffusion\nOurs\nFigure 6: Numerical analysis in terms of the prompt\nand identity similarity on four prompt types.\nMethods\nPrompt\u2191\nIdentity\u2191\nDetect\u2191\nw/o celeb basis\n0.1386\n0.2528\n69.28%\nw/ 350 names\n0.2214\n0.2023\n69.28%\nw/o filter\n0.1939\n0.2037\n80.62%\nw flatten\n0.2026\n0.1873\n80.39%\np = 64\n0.2247\n0.1061\n76.47%\np = 256\n0.1812\n0.0656\n60.13%\np = 768\n0.1380\n0.0836\n47.06%\nw/o F\n0.1914\n0.1896\n55.56%\nw/o aug.\n0.2083\n0.1931\n75.16%\nOurs (single)\n0.2234\n0.2186\n81.05%\nOurs (joint)\n0.2545\n0.2072\n84.78%\nTable 2: Ablation studies.\nBesides visual quality, we also perform the numerical comparison between the proposed method and\nbaselines in Table 1. From the table, regardless of the over-fitted textural inversion, the proposed\nmethod shows a much better performance in terms of identity similarity and the face detection rate\nand achieves similar text-prompt alignment as Custom Diffsuion [8]. We also plot the generated\nresults on four detailed types in Figure 6, where the proposed method shows the best trade-off. Notice\nthat, the proposed method only contains very few learning-able parameters and optimizes faster.\nMoreover, since identity similarity is very subjective, we generate 200 images from different identities\nto form a user study. In detail, we invite 100 users to rank the generated images from one (worst) to\nfive (best) in terms of visual quality, prompt alignment, and identities, getting 60k opinions in total.\nThe results are also shown in Table 1, where the users favor our proposed method.\n4.3\nAblation Studies\nTo further evaluate the sub-modules of our proposed method in both building celeb basis (Sec. 4.3.1)\nand the proposed new personalization method (Sec. 4.3.2), we start from the default settings ( \u2018Ours\n(single)\u2019 in Table 2) of our method, conducting the ablation study by separately removing each\nsubmodule or using a different setting as follows. Due to the space limitation, we give more visual\nresults in the supplementary.\n4.3.1\nAblation Studies on Celeb Basis\n# of names in celeb basis. We evaluate the influence of the names to build a celeb basis. In extreme\ncases, if there is no name and we directly learn the embedding from the face encoder F (w/o celeb\nbasis), the model is overfitted to the input image and can not perform the concept combination. With\nfewer celeb names (w/ 350 names), the generated quality is not good as ours baseline (single) as in\nFigure 7a. Besides, the quality of the celeb basis is also important, if we do not filter the names (w/o\nfilter), the performance will also decrease as in Table 2.\nFlatten basis v.s. first and last name basis. In the main method, we introduce our celeb basis as the\nfirst and last name basis since each name embedding does not have the same length. We thus involve\nV\u2217 (Input Image)\n\ud835\udc7d\u2217 is playing \nbasketball.\nInput Prompt\nw/o celeb basis\nw/ 350 Names\nw/ 691 Names\n(a) # of names in celeb basis.\nFlatten, Keep order (first name + last name)\n\ud835\udc7d\u2217 is playing the\nguitar.\nInput Prompt\nFlatten basis\nOurs\nV\u2217 (Input Image)\n(b) First and last name basis v.s. flatten basis.\nFigure 7: Ablation studies on building celeb basis.\n8\nw/o F, w F\nV* playing the guitar\n\ud835\udc7d\u2217 is playing the\nguitar.\nInput Prompt\nw/o Face Encoder\nOurs\nV\u2217 (Input Image)\n(a) Face encoder.\nw/o F, w F\nV* playing the guitar\n\ud835\udc7d\u2217 is playing the \nLEGO toys.\nInput Prompt\nw/o Data Aug.\nOurs\nV\u2217 (Input Image)\n(b) Data augmentation\nsingle vs. joint\nV*_1 and V*_2 baking a birthday\njoint\n\ud835\udc82\ud835\udc8f\ud835\udc85 \ud835\udc7d_\ud835\udfd0^\n\u2217 \ud835\udc82\ud835\udc93\ud835\udc86 \ud835\udc83\ud835\udc82\ud835\udc8c\ud835\udc8a\ud835\udc8f\ud835\udc88 \ud835\udc82 \ud835\udc83\ud835\udc8a\ud835\udc93\ud835\udc95\ud835\udc89\ud835\udc85\ud835\udc82\ud835\udc9a \ud835\udc84\ud835\udc82\ud835\udc8c\ud835\udc86.\n\ud835\udc49!\n\u2217\nJoint\nSingle\n\ud835\udc49\"\n\u2217 \ud835\udc82\ud835\udc8f\ud835\udc85 \ud835\udc49#\n\u2217\n\ud835\udc7d\ud835\udfcf\n\u2217 and \ud835\udc7d\ud835\udfd0\n\u2217 are\nbaking a birthday\ncake.\nInput Prompt\n(c) Single v.s. joint optimization\nFigure 8: Ablation studies on coefficient optimization.\na more naive way by flattening all the embeddings to build the basis (w/ flatten). As shown in Fig. 7b\nand Table 2, the generated images of our first and last name basis understand prompts better.\nChoice of reduction dimension p. We also evaluate the influence of the number of coefficients p. Con-\nsidering the 768 dimensions of the CLIP text embedding, we vary p ranging in {64, 256, 512, 768}.\nAs shown in Table 2, the best result is obtained from the baseline choice (p = 512) and we show the\ndifferences in the generated images in the supplementary materials.\n4.3.2\nAblation Studies on Coefficients Optimization\nW/o face recognition encoder F. Naively, we can optimize the coefficients A1, A2 of the celeb\nembeddings from back-propagation directly. However, we find the search space is still large to get\nsatisfied results as shown in Figure 8a and Table 2 (w/o F). So we seek help from the pretrained face\nencoder, which has more discriminative features on the face.\nW/o data augmentation. Since there is only one single image as the tuning-able sample, we perform\nsome data augmentations as introduced in Sec. 3.2. However, if we remove these augmentations, the\ngenerated face becomes vague and unnatural as shown in Figure 8b, and the identity of the generated\nsamples is also decreased.\nSingle v.s. joint training. Our method supports joint training of multiple identities using a single\nMLP, we evaluate the differences between single training/joint testing and joint training/joint testing.\nAs shown in Table 2 and Figure 8c, although training individually can also perform some interactions\nbetween the two-person, training the images jointly improve the robustness and reduces the over-\nfitting risk compared with single training, resulting in slightly better quantitative results.\n4.4\nLimitation and Ethics Consideration\nLimitation. Although our method can successfully generate the images of the new identities, it still\noccurs some limitations. First, the real human faces of the original stable diffusion [1] naturally\ncontain some artifacts, causing the naturalness of the proposed method. It might be solved by a more\npowerful pre-trained text-to-image model (e.g., Imagen [5], IF [51]) since they can generate better\nfacial details. Secondly, we only focus on human beings currently. It is also interesting to build the\nbasis of other species, e.g., cars, and cats, we leave it as future work.\nEthics Consideration and Broader Impacts. We propose a new method for model personalization\non the human face, which might be used as a tool for deepfake generation since we can not only\ngenerate the human face but also can interact with other people. However, this prevalent issue is not\nlimited to this approach alone, as it also exists in other generative models and content manipulation\ntechniques. Besides, our personalization person generation method can also ablate the abilities of the\nerasing concept methods [52] and other deep fake detection methods [53].\n5\nConclusion\nWe propose a new method to personalize the pre-trained text-to-image model on a specific kind\nof concept, i.e., the human being, with simply single-shot tuning. Our approach is enabled by\ndefining a basis in the domain of the known celebrity names\u2019 embeddings. Then, we can map the\nfacial feature from the pre-trained face recognition encoder to reconstruct the coefficients of the new\nidentity. Compared with the previous concept injection method, our method shows stronger concept\ncombination abilities, e.g., better identity preservation, can be trained on various identities at once,\n9\nand can perform some interacting abilities between the newly-added humans. Besides, the proposed\nmethod only requires 1024 parameters for each person and can be optimized in under 3 minutes,\nwhich is also much more efficient than previous methods.\nReferences\n[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolution image synthesis with latent diffusion models. CVPR, 2021.\n[2] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In CVPR, pages 4401\u20134410, 2019.\n[3] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022.\n[4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021.\n[5] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. Advances in\nNeural Information Processing Systems, 35:36479\u201336494, 2022.\n[6] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[7] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and\nDaniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using\ntextual inversion. arXiv preprint arXiv:2208.01618, 2022.\n[8] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-\nconcept customization of text-to-image diffusion. In CVPR, 2023.\n[9] Ziyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: Towards controllable one-shot text-to-\nimage generation via contrastive prompt-tuning. arXiv preprint arXiv:2211.11337, 2022.\n[10] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. 2022.\n[11] Rinon Gal, Moab Arar, Yuval Atzmon, AmitH. Bermano, Gal Chechik, and Daniel Cohen-Or.\nDesigning an encoder for fast personalization of text-to-image models. Feb 2023.\n[12] Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. In\nProceedings of the 26th annual conference on Computer graphics and interactive techniques,\npages 187\u2013194, 1999.\n[13] Karl Pearson. Liii. on lines and planes of closest fit to systems of points in space. The London,\nEdinburgh, and Dublin philosophical magazine and journal of science, 2(11):559\u2013572, 1901.\n[14] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular\nmargin loss for deep face recognition. In CVPR, pages 4690\u20134699, 2019.\n[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications\nof the ACM, 63(11):139\u2013144, 2020.\n[16] Diederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\n10\n[17] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\n[18] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution\nimage synthesis. In CVPR, pages 12873\u201312883, 2021.\n[19] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images\nwith vq-vae-2. Advances in Neural Information Processing Systems, 2019.\n[20] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on\nMachine Learning, pages 8821\u20138831, 2021.\n[21] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.\nAdvances in neural information processing systems, 31, 2018.\n[22] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components\nestimation. arXiv preprint arXiv:1410.8516, 2014.\n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin Neural Information Processing Systems, 33:6840\u20136851, 2020.\n[24] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\n[25] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. Re-imagen: Retrieval-\naugmented text-to-image generator. arXiv preprint arXiv:2209.14491, 2022.\n[26] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of\nnatural images. In CVPR, pages 18208\u201318218, 2022.\n[27] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. arXiv preprint\narXiv:2206.02779, 2022.\n[28] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen,\nand Fang Wen. Paint-by-example: Exemplar-based image editing with diffusion models. arXiv\npreprint arXiv:2211.13227, 2022.\n[29] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion\nfor editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022.\n[30] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu.\nZero-shot image-to-image translation. arXiv preprint arXiv:2302.03027, 2023.\n[31] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and\nQifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv preprint\narXiv:2303.09535, 2023.\n[32] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,\n2022.\n[33] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.\nAnalyzing and improving the image quality of stylegan. In CVPR, pages 8110\u20138119, 2020.\n[34] Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar\nMosseri, Yael Pritch, and Daniel Cohen-Or. Mystyle: A personalized generative prior. ACM\nTransactions on Graphics (TOG), 41(6):1\u201310, 2022.\n[35] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. Pivotal tuning for\nlatent-based editing of real images. ACM Transactions on Graphics (TOG), 42(1):1\u201313, 2022.\n[36] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the\nstylegan latent space? In CVPR, pages 4432\u20134441, 2019.\n11\n[37] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and\nDaniel Cohen-Or. Encoding in style: a stylegan encoder for image-to-image translation. In\nCVPR, pages 2287\u20132296, 2021.\n[38] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri,\nand Michal Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint\narXiv:2210.09276, 2022.\n[39] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, and\nHongxia Jin. Continual diffusion: Continual customization of text-to-image diffusion with\nc-lora. arXiv preprint arXiv:2304.06027, 2023.\n[40] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for\ntext-to-image personalization. arXiv preprint arXiv:2305.01644, 2023.\n[41] Yiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu, and Jiaying Liu. Unified multi-modal\nlatent diffusion for joint subject and text conditional image generation.\narXiv preprint\narXiv:2303.09319, 2023.\n[42] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite:\nEncoding visual concepts into textual embeddings for customized text-to-image generation.\narXiv preprint arXiv:2302.13848, 2023.\n[43] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and\nWilliam W Cohen. Subject-driven text-to-image generation via apprenticeship learning. arXiv\npreprint arXiv:2304.00186, 2023.\n[44] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou,\nHuisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization\nwith text-to-image diffusion models. arXiv preprint arXiv:2304.02642, 2023.\n[45] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image\ngeneration without test-time finetuning. arXiv preprint arXiv:2304.03411, 2023.\n[46] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and Javier Romero. Learning a model of\nfacial shape and expression from 4D scans. ACM Transactions on Graphics, (Proc. SIGGRAPH\nAsia), 36(6):194:1\u2013194:17, 2017.\n[47] Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu Shen, Ruigang Yang, and Xun Cao.\nFacescape: A large-scale high quality 3d face dataset and detailed riggable 3d face prediction.\nIn CVPR, June 2020.\n[48] Kwang In Kim, Keechul Jung, and Hang Joon Kim. Face recognition using kernel principal\ncomponent analysis. IEEE signal processing letters, 9(2):40\u201342, 2002.\n[49] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[50] Hongyu Guo. Nonlinear mixup: Out-of-manifold data augmentation for text classification. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 4044\u20134051,\n2020.\n[51] Deep Floyd. https://github.com/deep-floyd/if, 2023.\n[52] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts\nfrom diffusion models. arXiv preprint arXiv:2303.07345, 2023.\n[53] Yuchen Luo, Yong Zhang, Junchi Yan, and Wei Liu. Generalizing face forgery detection with\nhigh-frequency features. In CVPR, pages 16317\u201316326, June 2021.\n[54] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. Vggface2: A dataset\nfor recognising faces across pose and age. In 2018 13th IEEE international conference on\nautomatic face & gesture recognition (FG 2018), pages 67\u201374. IEEE, 2018.\n12\n[55] Guangxuan Xiao, Tianwei Yin, William T. Freeman, Fr\u00e9do Durand, and Song Han. Fastcom-\nposer: Tuning-free multi-subject image generation with localized attention. arXiv, 2023.\n[56] https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.pca.html.\nA\nMore Qualitative Results\nA.1\nEvaluation on Faces in a Wider Range\nBesides evaluating the results on StyleGAN synthetic faces as in the main paper, we compare our\nmethod with state-of-the-art methods on a wide range of human face images, including the real word\nfaces, the faces from different races, and the interactions of the same gender. These face images are\nfrom VGGFace2 [54] and collected from the Web.\nSingle Person in an Image. Figure 9 shows the evaluation results on a single person\u2019s personalization\nfor real identities. Concretely, Row 1-2 shows the ability to generate diverse images of different\nmethods. The baselines Textual Inversion [7], DreamBooth [10], and Custom Diffusion [8] can\ngenerate identity-consistent results with the single prompt \u2018A photo of V \u2217\u2019, but the light condition and\nskin texture seem not natural. DreamBooth and Custom Diffusion fail to disentangle the background\nof the target inputs. The stylization results are shown in Row 3-4, where Textual Inversion preserves\nthe target identities but fails to change the style, DreamBooth, and Custom Diffusion generate the\nimages of the corresponding style but lose the identity information, and our method preserves the\nidentity and completes the stylization as given prompts.\nMultiple Persons in an Image. We give the comparison of generating multiple-person interaction\nin Figure 10. Similar to the single-person results, the proposed method shows a good concept\ncombination ability among different persons (including the newly added and the original celebrities)\nand can serve as a novel concept to communicate with other humans.\nA.2\nAdditional Results Comparing with concurrently work: FastComposer [55]\nDifferent from the methods [7, 8, 10] that the finetuning time cost for each identity is below 30\nminutes, FastComposer [55] needs to be pre-trained on a large human dataset, requiring 150k steps\nwith batch size 128 on 8 NVIDIA A6000 (48GB) GPUs. So as its performance is restricted by the\ndataset as in their limitation. Differently, our method requires only 400 steps with batch size 2 on a\nsingle NVIDIA A100 (40GB) GPU. After the pre-training stage, FastComposer can generate images\nbased on the given single image and prompt like other personalized methods.\nTo further demonstrate the efficacy of our method, we evaluate the qualitative performance of our\nmethod compared with FastComposer. Our method outperforms FastComposer on single action con-\ntrolling, the interaction between two persons, and expression controlling. The detailed comparisons\nare as follows.\nAction of Single Person. We first evaluate the performance of single persons\u2019 personalization. As\nillustrated in Figure 11 and Figure 12, we show the synthesized results under six scenarios in each\nfigure.\nIn Figure 11, given the prompt \u2018A photo of V \u2217\n1 \u2019, FastComposer generates an unnatural result. In the\nremaining scenarios, the light seems very disharmonious in the images synthesized by FastComposer.\nBesides, in these results, only the upper chest and head can be generated, where the human body and\nlimbs fail to appear, revealing an over-fitting issue of FastComposer.\nIn Figure 12, FastComposer meets the same issue. Although the key objects and face identity are\nalmost consistent with the input prompts, e.g. LEGO toys, fighter jet, and yellow jacket, the results\nfrom FastComposer seem to be a rude combination of the objects and face. Besides, their methods\nhave a huge issue with concept forget. Some important prompts like \u2018sweater\u2019 and \u2018motorbike\u2019 are\nignored by FastComposer.\nWe also highlight the faces generated by FastComposer share very similar expressions from the\noriginal input image, i.e. dilated pupils, wrinkles on the forehead, and mouth shape, which means\nthis method is overfitted on the input image. Differently, our method generates identity- and prompt-\nconsistent results, appearing natural and realistic.\n13\n\ud835\udc49\u0b35\n\u2217\nA photo of \ud835\udc49\u0b35\n\u2217\nTarget Image\nTextual \nInversioin\nDreamBooth\nCustom \nDiffusion\nOurs\n\ud835\udc49\u0b36\n\u2217\n\ud835\udc49\u0b37\n\u2217\n\ud835\udc49\u0b38\n\u2217\n\ud835\udc49\u0b39\n\u2217\n\ud835\udc49\u0b3a\n\u2217\nA photo of \ud835\udc49\u0b36\n\u2217\n\ud835\udc49\u0b37\n\u2217 in the style of Archer\n\ud835\udc49\u0b38\n\u2217 by Andy Warhol\n\ud835\udc49\u0b39\n\u2217 is playing basketball\n\ud835\udc49\u0b3a\n\u2217 in a Chinese vegetable market\nFigure 9: Single person\u2019s personalization result for real identities.\nInteraction between Two Persons. Figure 13 illustrates the interaction and common action of two\npersons in a single image. Sharing the similar aforementioned over-fitting problem, FastComposer\nonly generates the face part of the human. As for the actions including \u2018shaking\u2019, \u2018playing\u2019, \u2018sitting\u2019,\nand \u2018baking\u2019, FastComposer fails to generate the correct behaviors, where the faces usually occupy\nthe most part of the images. The problem of invariant expression also occurs in FastComposer.\n14\nExpression Controlling. Considering that the above FastComposer results struggle to generate the\ndifferent expressions, we conduct a face expression controlling experiment to validate the abilities of\nour method and FastComposer. We use the following prompts to change the expression of the target\ninput image:\n\u2022 V \u2217 is smiling with happiness\n\u2022 V \u2217 is crying with sadness\n\u2022 V \u2217 has an anger expression\n\u2022 V \u2217 looks very surprised, looking at a gift box\n\u2022 V \u2217 is shaking with fear scared by a candle lighting, scared expression\n\u2022 V \u2217 has a tired expression, headache, uncomfortable, a frown creased her forehead\nAs shown in Figure 14, the proposed method can successfully generate different expressions under\nthe text prompts, which shows the advantage of the proposed method.\nB\nImplementation Details\nB.1\nCeleb Images Generated by Stable Diffusion\nIn general, the celeb images generated by Stable Diffusion [1] and those realistic ones collected from\nGoogle Images seem very similar, but with a little difference on facial details. As seen in Figure 15,\nwe compare the real celeb images collected from Google Images with the fake ones synthesized by\nStable Diffusion. Overall, the real and fake images closely resemble the same individual. However,\ncertain facial features in the fake images may be exaggerated, which can constrain our method\u2019s\nperformance and explain why the celebrity profiles in the outcomes generated by both baseline\nmodels and our approach do not precisely match their real counterparts.\nB.2\nCollecting and Filtering Celeb Names\nAfter crawling about 1,500 celebrity names, to filter out the names that have the ability to generate\nprompt-consistent identities and interacting with other celebs, we feed three types of prompts, i.e. \u2018A\nphoto of Vc\u2019, \u2018Vc is playing the guitar\u2019, and \u2018Vc talks with Barack Obama\u2019, to the Stable Diffusion [1]\nfor synthesizing image results, where Vc indicates the celeb name (e.g. \u2018Anne Hathaway\u2019). Figure 16\nshows the examples that satisfy the filtering condition and the ones that fail to generate reasonable\nresults. Consequently, 691 names pass the checking, which can be tokenized and encoded into\nm = 691 celeb embedding groups.\nB.3\nBuilding Celeb Basis based on PCA\nWe only keep the first and second embeddings of each celeb embedding group, resulting in the first\nname embedding set C1 and the second name embedding set C2, where C1, C2 \u2208 Rm\u00d7d. Then, for\nsimplicity, we omit the subscript of Ck, using C to indicate any one of C1 and C2.\nCalculation of Mean. Considering C may have repetitive embeddings (each row corresponds an\nembedding vector), for each C, we first remove the duplicate rows that come from the same token\nto make sure each token only occurs once at most. Then we calculate the mean C \u2208 Rd across the\nsecond dimension of C as mentioned in the main paper.\nPCA. The PCA algorithm has many different coding implementations in practice. In our method, we\nuse Singular Value Decomposition (SVD) to skip the calculation of covariance matrix in PCA. Please\nrefer to [13, 56] for more detailed theoretical demonstration and coding techniques. Due to the built\nceleb basis is not optimized, the PCA process only needs to be computed once during the training\nstage.\nB.4\nTraining Recipe\nWe train the MLP with a learning rate of 0.005 and batch size of 2 on a single NVIDIA A100\nGPU. The training augmentation includes horizontal flip, color jitter, and random scaling ranging in\n15\n0.1 \u223c 1.0. For single identity training, the optimization costs 400 steps, taking \u223c 3 minutes. For 10\nidentities joint training, we found that training 2,500 steps is enough, taking \u223c 18 minutes (averaged\nabout 250 steps and 108 seconds for each identity). The text prompts for training include:\n\u2022 A photo of a face of V \u2217 person\n\u2022 A rendering of a face of V \u2217 person\n\u2022 The photo of a face of V \u2217 person\n\u2022 A rendition of a face of V \u2217 person\n\u2022 A illustration of a face of V \u2217 person\n\u2022 A depiction of a face of V \u2217 person\nC\nMore Ablation Studies\nW/o filtering celebrity names. As mentioned in Section B.2, Stable Diffusion fails to generate\ncorrect images from some celeb names. In our method, we manually drop these bad samples. If this\nfiltering process canceled, the interaction ability of the learned identities drops a lot, as shown in\nFigure 17a.\nVariants of reduction dimension p. In our method, the PCA reduction dimension p (p < k) controls\nthe degrees of freedom for modulating the variance applied to the celeb basis mean. Considering\nthat k = 768 in the CLIP [4] text encoder, we conduct a series experiments to study the choice of p.\nFour values of p are chosen, including 64, 256, 512, 768. Note that p = 768 means PCA is not used.\nLarger p costs more memory storage. As shown in Figure 17b, chosing p = 512 comes to the best\nidentity similarity, which is consistent with the quantitative results in our main paper.\n16\n\ud835\udc49\u0b35\n\u2217\n\ud835\udc49\u0b35\n\u2217 and Rachel McAdams are baking a birthday cake\nTarget Image\nTextual \nInversioin\nDreamBooth\nCustom \nDiffusion\nOurs\n\ud835\udc49\u0b36\n\u2217\n\ud835\udc49\u0b38\n\u2217\n\ud835\udc49\u0b3a\n\u2217\n\ud835\udc49\u0b3c\n\u2217\n\ud835\udc49\u0b35\u0b34\n\u2217\n\ud835\udc49\u0b3d\n\u2217 and \ud835\udc49\u0b35\u0b34\n\u2217 are sitting on a sofa\n\ud835\udc49\u0b3b\n\u2217 and \ud835\udc49\u0b3c\n\u2217 are cooking in a kitchen\nManga drawing of \ud835\udc49\u0b39\n\u2217 and \ud835\udc49\u0b3a\n\u2217\nA \ud835\udc49\u0b37\n\u2217 Funko Pop next to a \ud835\udc49\u0b38\n\u2217 Funko Pop\n\ud835\udc49\u0b36\n\u2217 shakes hands with Chadwick Boseman in news conference\n\ud835\udc49\u0b37\n\u2217\n\ud835\udc49\u0b39\n\u2217\n\ud835\udc49\u0b3b\n\u2217\n\ud835\udc49\u0b3d\n\u2217\nFigure 10: Multiple persons\u2019 personalization result for real identities.\n17\nA photo of \ud835\udc49\u0b35\n\u2217\nPlaying the guitar\nWearing the sweater \noutdoors\nOn a beach\nWearing a scifi\nspacesuit in space\nCarrying vegetables \nin vegetable market\n\ud835\udc49\u0b35\n\u2217\nA photo of \ud835\udc49\u0b35\n\u2217\nPlaying the guitar\nWearing the sweater \noutdoors\nOn a beach\nWearing a scifi\nspacesuit in space\nCarrying vegetables \nin vegetable market\nOurs\nFastComposer\nFigure 11: Additional single person\u2019s personalization results comparing with FastComposer [55].\n18\n\ud835\udc49\u0b35\n\u2217\nA photo of \ud835\udc49\u0b35\n\u2217\nPlaying the LEGO toys\nWearing the sweater \noutdoors\nPiloting a fighter jet\nWearing a scifi\nspacesuit in space\nWearing yellow \njacket on a motorbike\nOurs\nFastComposer\nA photo of \ud835\udc49\u0b35\n\u2217\nPlaying the LEGO toys\nWearing the sweater \noutdoors\nPiloting a fighter jet\nWearing a scifi\nspacesuit in space\nWearing yellow \njacket on a motorbike\nFigure 12: Additional single person\u2019s personalization results comparing with FastComposer [55].\n19\nOurs\nFastComposer\nIn a kitchen\nIn an amusement park\nShaking hands\nPlaying the piano\nSitting on a sofa\nBaking a cake\n\ud835\udc49\u0b35\n\u2217\n\ud835\udc49\u0b36\n\u2217\nPlaying the piano\nSitting on a sofa\nBaking a cake\nIn a kitchen\nIn an amusement park\nShaking hands\nFigure 13: Additional multiple persons\u2019 interaction personalization results comparing with FastCom-\nposer [55].\n20\nHappy\nSad\nAngry\nOurs\nFastComposer\nSurprised\nScared\nWorried\nTarget Image\nSurprised\nScared\nWorried\nHappy\nSad\nAngry\nFigure 14: Comparing with FastComposer [55], our method has a better ability of controlling face\nexpression.\n21\nAnne \nHathaway\nBarack\nObama\nRobert \nDowney\nRachel \nMcAdams\nChadwick \nBoseman\nStable Diffusion\nGoogle Images\nFigure 15: Stable Diffusion has the ability to generate identity-consistent celeb images. But in the\naspect of facial details, the celeb images synthesized by Stable Diffusion may exaggerate some facial\nfeatures compared with the real ones collected from Google Images.\n\ud835\udc49\u0b61 talks with \nBarack Obama\n\ud835\udc49\u0b61 is playing \nthe guitar\nA photo of \ud835\udc49\u0b61\nBenedict \nCumberbatch\nCate Blanchett\nZooey \nDeschanel\nRowan \nAtkinson\nSheeran\nStephen Chow\nSatisfied Names\nNo Used Names\nGoogle Images\nFigure 16: We filter out the celeb names that can generate prompt- and identity-consistent images\nthrough Stable Diffusion [1]. Left: the satisfied names that can generate images share the same\nidentity with Google Images results. Furthermore, these names have the ability to interact with\nobjects or other satisfied celebs (e.g. \u2018Barack Obama\u2019). Right: Stable Diffusion confuses the face of\n\u2018Rowan Atkinson\u2019 and \u2018Barack Obama\u2019 in row four. The gender of \u2018Sheeran\u2019 is mistaken as a girl in\nrow four. The identity of \u2018Stephen Chow\u2019 has a large gap with that of Google Images results.\n22\nInput Prompt\n\u2217 is playing \nbasketball.\n\u2217 (Input Image)\nOurs\nw/o filter\n(a) The influence of filtering the useless celeb names or not.\n\ud835\udc5d = 64\n(0.25 \u00d7)\n\ud835\udc5d = 256\n(0.5 \u00d7)\n\ud835\udc91 = \ud835\udfd3\ud835\udfcf\ud835\udfd0\n(\ud835\udfcf. \ud835\udfce \u00d7, Ours)\n\ud835\udc5d = 768\n(1.5 \u00d7, max)\n\ud835\udc49\u2217 (Input Image)\nInput Prompt\n\ud835\udc7d\u2217 wearing a scifi \nspacesuit in space.\n(b) The ablation on the values of p.\nFigure 17: Additional ablation studies on building celeb basis and the choice of reduction dimension\np.\n23\n"
  },
  {
    "title": "The ObjectFolder Benchmark: Multisensory Learning with Neural and Real Objects",
    "link": "https://arxiv.org/pdf/2306.00956.pdf",
    "upvote": "1",
    "text": "The OBJECTFOLDER BENCHMARK:\nMultisensory Learning with Neural and Real Objects\nRuohan Gao*\nYiming Dou*\u2020\nHao Li*\nTanmay Agarwal\nJeannette Bohg\nYunzhu Li\nLi Fei-Fei\nJiajun Wu\nStanford Univeristy\nAbstract\nWe introduce the OBJECTFOLDER BENCHMARK, a\nbenchmark suite of 10 tasks for multisensory object-centric\nlearning, centered around object recognition, reconstruc-\ntion, and manipulation with sight, sound, and touch. We\nalso introduce the OBJECTFOLDER REAL dataset, in-\ncluding the multisensory measurements for 100 real-world\nhousehold objects, building upon a newly designed pipeline\nfor collecting the 3D meshes, videos, impact sounds, and\ntactile readings of real-world objects. We conduct system-\natic benchmarking on both the 1,000 multisensory neural\nobjects from OBJECTFOLDER, and the real multisensory\ndata from OBJECTFOLDER REAL.\nOur results demon-\nstrate the importance of multisensory perception and reveal\nthe respective roles of vision, audio, and touch for differ-\nent object-centric learning tasks. By publicly releasing our\ndataset and benchmark suite, we hope to catalyze and en-\nable new research in multisensory object-centric learning\nin computer vision, robotics, and beyond. Project page:\nhttps://objectfolder.stanford.edu\n1. Introduction\nComputer vision systems today excel at recognizing ob-\njects in 2D images thanks to many image datasets [3,19,39,\n44]. There is also a growing interest in modeling an object\u2019s\nshape and appearance in 3D, with various benchmarks and\ntasks introduced [8, 30, 48, 49, 58, 66]. Despite the exciting\nprogress, these studies primarily focus on the visual recog-\nnition of objects. At the same time, our everyday activities\noften involve multiple sensory modalities. Objects exist not\njust as visual entities, but they also make sounds and can be\ntouched during interactions. The different sensory modes of\nan object all share the same underlying object intrinsics\u2014\nits 3D shape, material property, and texture. Modeling the\n*indicates equal contribution.\n\u2020Yiming is affiliated with Shanghai Jiao Tong University. The work\nwas done when he was visiting Stanford University as a summer intern.\ncomplete multisensory profile of objects is of great impor-\ntance for many applications beyond computer vision, such\nas robotics, graphics, and virtual and augmented reality.\nSome recent attempts have been made to combine mul-\ntiple sensory modalities to complement vision for various\ntasks [2,6,43,63,64,68,76,79]. These tasks are often studied\nin tailored settings and evaluated on different datasets. As\nan attempt to develop assets generally applicable to diverse\ntasks, the OBJECTFOLDER dataset [25, 28] has been intro-\nduced and includes 1,000 neural objects with their visual,\nacoustic, and tactile properties. OBJECTFOLDER however\nhas two fundamental limitations. First, no real objects are\nincluded; all multisensory data are obtained through simula-\ntion with no simulation-to-real (sim2real) calibration. Sec-\nond, only a few tasks were presented to demonstrate the\nusefulness of the dataset and to establish the possibility of\nconducting sim2real transfer with the neural objects.\nConsequently, we need a multisensory dataset of real ob-\njects and a robust benchmark suite for multisensory object-\ncentric learning.\nTo this end, we present the OBJECT-\nFOLDER REAL dataset and the OBJECTFOLDER BENCH-\nMARK suite, as shown in Fig. 1.\nThe OBJECTFOLDER REAL dataset contains multisen-\nsory data collected from 100 real-world household objects.\nWe design a data collection pipeline for each modality: for\nvision, we scan the 3D meshes of objects in a dark room\nand record HD videos of each object rotating in a lightbox;\nfor audio, we build a professional anechoic chamber with\na tailored object platform and then collect impact sounds\nby striking the objects at different surface locations with an\nimpact hammer; for touch, we equip a Franka Emika Panda\nrobot arm with a GelSight robotic finger [20,77] and collect\ntactile readings at the exact surface locations where impact\nsounds are collected.\nThe OBJECTFOLDER BENCHMARK suite consists of 10\nbenchmark tasks for multisensory object-centric learning,\ncentered around object recognition, reconstruction, and ma-\nnipulation. The three recognition tasks are cross-sensory\nretrieval, contact localization, and material classification;\nthe three reconstruction tasks are 3D shape reconstruc-\n1\narXiv:2306.00956v1  [cs.CV]  1 Jun 2023\n\u2026\n\u2026\nSound Generation of Dynamic Objects\nVisuo-Tactile Cross-Generation\nObject Reconstruction\n3D Shape Reconstruction\nSurface Traversal\nGrasp Stability Prediction\nContact Refinement\nDynamic Pushing\nObject Manipulation\nContact Localization\nCross-Sensory Retrieval\nMaterial Classification\nObject Recognition\n\u201cCeramic\u201d\n1,000 multisensory neural objects\n100 real objects\nFigure 1. The OBJECTFOLDER BENCHMARK suite consists of 10 benchmark tasks for multisensory object-centric learning, centered\naround object recognition, reconstruction, and manipulation.\nComplementing the 1,000 multisensory neural objects from OBJECT-\nFOLDER [28], we also introduce OBJECTFOLDER REAL, which contains real multisensory data collected from 100 real-world objects,\nincluding their 3D meshes, video recordings, impact sounds, and tactile readings.\ntion, sound generation of dynamic objects, and visuo-tactile\ncross-generation; and the four manipulation tasks are grasp\nstability prediction, contact refinement, surface traversal,\nand dynamic pushing. We standardize the task setting for\neach task and present baseline approaches and results.\nExperiments on both neural and real objects demonstrate\nthe distinct value of sight, sound, and touch in different\ntasks. For recognition, vision and audio tend to be more re-\nliable compared to touch, where the contained information\nis too local to recognize. For reconstruction, we observe\nthat fusing multiple sensory modalities achieve the best re-\nsults, and it is possible to hallucinate one modality from the\nother. This agrees with the notion of degeneracy in cog-\nnitive studies [65], which creates redundancy such that our\nsensory system functions even with the loss of one compo-\nnent. For manipulation, vision usually provides global po-\nsitional information of the objects and the robot, but often\nsuffers from occlusion. Touch, often as a good complement\nto vision, is especially useful to capture the accurate local\ngeometry of the contact point.\nWe will open-source all code and data for OBJECT-\nFOLDER REAL and OBJECTFOLDER BENCHMARK to fa-\ncilitate research in multisensory object-centric learning.\n2. Related Work\nObject Datasets.\nA large body of work in computer vi-\nsion focuses on recognizing objects in 2D images [29, 31,\n32, 37].\nThis progress is enabled by a series of image\ndatasets such as ImageNet [19], MS COCO [44], Object-\nNet [3], and OpenImages [39].\nIn 3D vision, datasets\nlike ModelNet [74] and ShapeNet [8] focus on modeling\nthe geometry of objects but without realistic visual tex-\ntures. Recently, with the popularity of neural rendering ap-\nproaches [50,62], a series of 3D datasets are introduced with\nboth realistic shape and appearance, such as CO3D [58],\nGoogle Scanned Objects [21], and ABO [15]. Unlike all\ndatasets above that focus only on the visual modality, we\nalso model the acoustic and tactile modalities of objects.\nOur work is most related to OBJECTFOLDER [25, 28],\na dataset of 1,000 neural objects with visual, acoustic, and\ntactile sensory data. While their multisensory data are ob-\ntained purely from simulation, we introduce the OBJECT-\nFOLDER REAL dataset that contains real multisensory data\ncollected from real-world household objects.\nCapturing Multisensory Data from Real-World Objects.\nLimited prior work has attempted to capture multisensory\n2\ndata from the real world. Earlier work models the mul-\ntisensory physical behavior of 3D objects [52] for virtual\nobject interaction and animations. To our best knowledge,\nthere is no large prior dataset of real object impact sounds.\nDatasets of real tactile data are often collected for a particu-\nlar task such as robotic grasping [6,7], cross-sensory predic-\ntion [43], or from unconstrained in-the-wild settings [76].\nOur OBJECTFOLDER REAL dataset is the first dataset that\ncontains all three modalities with rich annotations to facili-\ntate multisensory learning research with real object data.\nMultisensory Object-Centric Learning.\nRecent work\nuses audio and touch in conjunction with vision for a se-\nries of new tasks, including visuo-tactile 3D reconstruc-\ntion [28, 63, 64, 68], cross-sensory retrieval [2, 25], cross-\nmodal generation [40,43,79], contact localization [28,46],\nrobotic manipulation [6,7,41,42], and audio-visual learning\nfrom videos [1,9,11,26,27,51,80]. While they only focus\non a single task of interest in tailored settings, each with a\ndifferent set of objects, we present a standard benchmark\nsuite of 10 tasks based on 1,000 neural objects from OB-\nJECTFOLDER and 100 real objects from OBJECTFOLDER\nREAL for multisensory object-centric learning.\n3. OBJECTFOLDER REAL\nThe OBJECTFOLDER dataset [28] contains 1,000 multi-\nsensory neural objects, each represented by an Object File,\na compact neural network that encodes the object\u2019s intrin-\nsic visual, acoustic, and tactile sensory data. Querying it\nwith extrinsic parameters (e.g., camera viewpoint and light-\ning conditions for vision, impact location and strength for\naudio, contact location and gel deformation for touch), we\ncan obtain the corresponding sensory signal at a particular\nlocation or condition.\nThough learning with these virtualized objects with sim-\nulated multisensory data is exciting, it is necessary to have a\nbenchmark dataset of multisensory data collected from real\nobjects to quantify the difference between simulation and\nreality. Having a well-calibrated dataset of real multisen-\nsory measurements allows researchers to benchmark differ-\nent object-centric learning tasks on real object data without\nhaving the need to actually acquire these objects. For tasks\nin our benchmark suite in Sec. 4, we show results on both\nthe neural objects from OBJECTFOLDER and the real ob-\njects from OBJECTFOLDER REAL when applicable.\nCollecting real multisensory data densely from real ob-\njects is very challenging, requiring careful hardware design\nand tailored solutions for each sensory modality by tak-\ning into account the physical constraints (e.g., robot joint\nlimit, kinematic constraints) in the capture system. Next,\nwe introduce how we collect the visual (Sec. 3.1), acoustic\n(Sec. 3.2), and tactile (Sec. 3.3) data for the 100 real objects\nshown in Fig. 1. Please also visit our project page for inter-\nactive demos to visualize the captured multisensory data.\n3.1. Visual Data Collection\nWe use an EinScan Pro HD 2020 handheld 3D Scanner1\nto scan a high-quality 3D mesh and the corresponding color\ntexture for each object. The scanner captures highly accu-\nrate 3D features by projecting a visible light array on the ob-\nject and records the texture through an attached camera. The\nminimum distance between two points in the scanned point\ncloud is 0.2 mm, enabling fine-grained details of the ob-\nject\u2019s surface to be retained in the scanned mesh. For each\nobject, we provide three versions of its mesh with differ-\nent resolutions: 16K triangles, 64K triangles, and Full res-\nolution (the highest number of triangles possible to achieve\nwith the scanner). Additionally, we record an HD video of\neach object rotating in a lightbox with a professional camera\nto capture its visual appearance, as shown in Fig. 2a.\n3.2. Acoustic Data Collection\nWe use a professional recording studio with its walls\ntreated with acoustic melamine anechoic foam panels and\nthe ceiling covered by absorbing acoustic ceiling tiles, as\nshown in Fig. 2b. The specific setup used to collect audio\ndata varies with the object\u2019s weight and size. Most objects\nare placed on a circular platform made with thin strings,\nwhich minimally affects the object\u2019s vibration pattern when\nstruck. Light objects are hung with a thin string and hit\nwhile suspended in the air. Heavy objects are placed on top\nof an anechoic foam panel to collect their impact sounds.\nFor each object, we select 30\u201350 points based on its\nscale following two criteria. First, the points should roughly\ncover the whole surface of the object and reveal its shape;\nSecond, we prioritize points with specific local geometry or\ntexture features, such as the rim/handle of a cup. For each\nselected point, we collect a 5-second audio clip of striking\nit along its normal direction with a PCB2 impact hammer\n(086C01). The impact hammer is equipped with a force\ntransducer in its tip, providing ground-truth contact forces\nsynchronized with the audio recorded by a PCB phantom-\npowered free-field microphone (376A32).\nIt is made of\nhardened steel, which ensures that the impacts are sharp\nand short enough to excite the higher-frequency modes of\neach object. We also record the accompanying video with a\nRealSense RGBD camera along with each impact sound.\n3.3. Tactile Data Collection\nFig. 2c illustrates our setup for the tactile data collection.\nWe equip a Franka Emika Panda robot arm with a GelSight\ntouch sensor [20, 77] to automate the data collection pro-\ncess. GelSight sensors are vision-based tactile sensors that\nmeasure the texture and geometry of a contact surface with\nhigh spatial resolution through an elastomer and an embed-\n1https://www.einscan.com\n2https://www.pcb.com\n3\n(a) Visual data collection\n(b) Acoustic data collection\n(c) Tactile data collection\nFigure 2. Illustration of our multisensory data collection pipeline for the OBJECTFOLDER REAL dataset. We design a tailored hardware\nsolution for each sensory modality to collect high-fidelity visual, acoustic, and tactile data for 100 real household objects.\nded camera. We use the R1.5 GelSight tactile robot finger3,\nwhich has a sensing area of 32 \u00d7 24 mm2.\nWe mount a RealSense RGBD camera at each corner of\nthe robot frame. After camera calibration, we use the Re-\nalSense ROS package to get a point cloud estimation of the\ntarget object. We also extract a point cloud from the scanned\n3D mesh of the object.\nIn order to align the two point\nclouds, we first manually select four roughly corresponding\npoints on both point clouds to provide an initial registration.\nNext, we use the Iterative Closest Point (ICP) [5] algorithm\nfor point cloud alignment. We add a manual adjustment step\nfor cases where the ICP alignment is not accurate.\nWe collect tactile data at the same set of surface points\nwhere the impact sounds are collected for each object. For\neach point of interest, we provide the robot with the tar-\nget position and orientation of the GelSight robot finger;\nwe then use position control to automatically reach the tar-\nget point following the normal direction of the target point.\nThe robot finger stops when the tactile sensor cannot de-\nform further. We collect a video of the tactile RGB images\nthat record the gel deformation process. We also use an in-\nhand camera and a third-view camera to capture two videos\nof the contact process for each point.\n4. ObjectFolder Benchmark Suite\nOur everyday activities involve the perception and ma-\nnipulation of various objects. Modeling and understanding\nthe multisensory signals of objects can potentially benefit\nmany applications in computer vision, robotics, virtual re-\nality, and augmented reality. The sensory streams of sight,\nsound, and touch all share the same underlying object in-\ntrinsics. During interactions, they often work together to re-\nveal the object\u2019s category, 3D shape, texture, material, and\nphysical properties.\nMotivated by these observations, we introduce a suite of\n10 benchmark tasks for multisensory object-centric learn-\ning, centered around object recognition (Sec. 4.1, 4.2, and\n4.3), object reconstruction (Sec. 4.4, 4.5, and 4.6), and ob-\n3https://www.gelsight.com\nject manipulation (Sec. 4.7, 4.8, 4.9, and 4.10), as shown\nin Fig. 1. In the sections below, we first present the moti-\nvation for each task. Then, we standardize the task setting,\ndefine evaluation metrics, draw its connection to existing\ntasks, and develop baseline models leveraging state-of-the-\nart components from the literature. In the end, we show a\nteaser result for each task. Please see Supp. for the com-\nplete results, baselines, and experimental setups.\n4.1. Cross-Sensory Retrieval\nMotivation\nWhen seeing a wine glass, we can mentally\nlink how it looks to how it may sound when struck or feel\nwhen touched. For machine perception, cross-sensory re-\ntrieval also plays a crucial role in understanding the relation-\nships between different sensory modalities. While existing\ncross-modal retrieval benchmarks and datasets [13, 54\u201357]\nmainly focus on retrieval between images and text, we per-\nform cross-sensory retrieval between objects\u2019 visual im-\nages, impact sounds, and tactile readings.\nTask Definition.\nCross-sensory retrieval requires the\nmodel to take one sensory modality as input and retrieve\nthe corresponding data of another modality. For instance,\ngiven the sound of striking a mug, the \u201caudio2vision\u201d model\nneeds to retrieve the corresponding image of the mug from\na pool of images of hundreds of objects. In this benchmark,\neach sensory modality (vision, audio, touch) can be used as\neither input or output, leading to 9 sub-tasks.\nEvaluation Metrics and Baselines.\nWe measure the\nmean Average Precision (mAP) score, a standard metric\nfor evaluating retrieval. We adopt several state-of-the-art\nmethods as the baselines: 1) Canonical Correlation Anal-\nysis (CCA) [33], 2) Partial Least Squares (PLSCA) [18],\n3) Deep Aligned Representations (DAR) [2], and 4) Deep\nSupervised Cross-Modal Retrieval (DSCMR) [81].\nTeaser Results.\nFig. 3 shows examples of the top re-\ntrieved instances for DAR [2], the best-performing baseline.\nWe can see that vision and audio tend to be more reliable for\nretrieval, while a single touch reading usually does not con-\ntain sufficient discriminative cues to identify an object.\n4\nQuery Input\nVision Retrieval\nAudio Retrieval\nTouch Retrieval\nFigure 3.\nExamples of the top-2 retrieved instances for each\nmodality using DAR [2], the best-performing baseline. For au-\ndio and touch retrieval, we also show an image of the object.\nND = 86.89%\nND = 3.85%\nND = 7.53%\nND = 2.49%\nVision\nTouch\nAudio\nFusion\nGround-truth contact location\nPredicted contact location\nFigure 4. Contact localization results for a ceramic mug object\nwith our multisensory contact regression model.\n4.2. Contact Localization\nMotivation.\nLocalizing the contact point when interact-\ning with an object is of great interest, especially for robot\nmanipulation tasks. Each modality offers complementary\ncues: vision displays the global visual appearance of the\ncontacting object; touch offers precise local geometry of\nthe contact location; impact sounds at different surface lo-\ncations are excited from different vibration patterns.\nIn\nthis benchmark task, we use or combine the object\u2019s visual,\nacoustic, and tactile observations for contact localization.\nTask Definition.\nGiven the object\u2019s mesh and different\nsensory observations of the contact position (visual images,\nimpact sounds, or tactile readings), this task aims to predict\nthe vertex coordinate of the surface location on the mesh\nwhere the contact happens.\nEvaluation Metrics and Baselines.\nWe use the average\nNormalized Distance (ND) as our metric, which measures\nthe distance between the predicted contact position and the\nground-truth position normalized by the largest distance of\ntwo points on the object\u2019s surface. We evaluate an existing\nbaseline Point Filtering [28,45], where the contact position\nis recursively filtered out based on both the multisensory\nobservations and the relative pose between consecutive con-\ntacts. This method performs very well but heavily relies on\nknowing the relative pose of the series of contacts, which\nmight be a strong assumption in practice. Therefore, we\nalso propose a new differentiable end-to-end learning base-\nline for contact localization\u2014Multisensory Contact Regres-\nsion (MCR), which takes the object mesh and multisensory\nobservations as input to regress the contact position directly.\nTeaser Results.\nFig. 4 shows an example result for a ce-\nramic mug object with our MCR baseline. While vision and\nVision\nTouch\nAudio\nFusion\nCD = 1.30 cm\nCD = 2.10 cm\nCD = 3.45 cm\nCD = 1.06 cm\nFigure 5. 3D reconstruction results of a wooden chair object. The\ntop/bottom row shows the point cloud reconstructions and the er-\nror over ground-truth points, respectively. Red indicates poorly-\nreconstructed areas; CD denotes Chamfer Distance.\naudio perform similarly, a single touch cannot easily locate\nwhere the contact is. Combining the three sensory modali-\nties leads to the best result.\n4.3. Material Classification\nMotivation.\nMaterial is an intrinsic property of an object,\nwhich can be perceived from different sensory modalities.\nFor example, a ceramic object usually looks glossy, sounds\ncrisp, and feels smooth. In this task, we predict an object\u2019s\nmaterial category based on its multisensory observations.\nTask Definition.\nAll objects are labeled by seven material\ntypes: ceramic, glass, wood, plastic, iron, polycarbonate,\nand steel. The task is formulated as a single-label classifi-\ncation problem. Given an RGB image, an impact sound, a\ntactile image, or their combination, the model must predict\nthe correct material label for the target object.\nEvaluation Metrics and Baselines.\nWe report the classi-\nfication accuracy and use two baselines: 1) ResNet [32] and\n2) FENet [75], which uses a different base architecture.\nTeaser Results.\nWe conduct material classification on\nboth neural and real objects. Fusing different modalities\nlargely improves the material classification accuracy. We\nalso finetune the model trained on neural objects with only\na few real-world measurements and achieve 6% accuracy\ngain in classifying real objects.\n4.4. 3D Shape Reconstruction\nMotivation.\nWhile single-image shape reconstruction has\nbeen widely studied [12,48,53,79], humans don\u2019t use vision\nalone to perceive the shape of objects. For example, we\ncan touch an object\u2019s surface to sense its local details, or\neven knock and listen to the sound it makes to estimate its\nscale. The effective fusion of complementary multisensory\ninformation plays a vital role in 3D shape reconstruction,\nwhich we study in this benchmark task.\nTask Definition.\nGiven an RGB image of an object, a se-\nquence of tactile readings from the object\u2019s surface, or a\n5\nPrediction\nGround-truth\nVideo\nFigure 6. Example results of sound generation for a falling steel\nbowl object with the RegNet [10] baseline.\nsequence of impact sounds of striking its surface locations,\nthe task is to reconstruct the point cloud of the target ob-\nject given combinations of these multisensory observations.\nThis task is related to prior efforts on visuo-tactile 3D recon-\nstruction [59, 63, 64, 67], but here we use all three sensory\nmodalities and study their respective roles.\nEvaluation Metrics and Baselines.\nWe report Chamfer\nDistance [4] between the reconstructed and the ground-truth\npoint cloud, a widely used metric to evaluate the quality of\nshape reconstruction. We use two state-of-the-art methods\nand a new transformer-based model as our baseline mod-\nels: 1) Mesh Deformation Network (MDN) [64], which is\nbased on deforming the vertices of an initial mesh through\na graph convolutional neural network, 2) Point Completion\nNetwork (PCN) [28, 78], which predicts the whole point\ncloud from latent features or incomplete point cloud con-\nstructed from local observations, and 3) Multisensory Re-\nconstruction Transformer (MRT), which encodes multisen-\nsory data using a transformer-based architecture.\nTeaser Results.\nFor 3D reconstruction, our observation is\nthat vision usually provides global yet coarse information,\naudio indicates the object\u2019s scale, and touch provides pre-\ncise local geometry of the object\u2019s surface. Fig. 5 shows\nan example of a wooden chair object. Both qualitative and\nquantitative results show that the three modalities make up\nfor each other\u2019s deficiencies, and achieve the best recon-\nstruction results when fused together.\n4.5. Sound Generation of Dynamic Objects\nMotivation\nObjects make unique sounds during interac-\ntions. When an object falls, we can anticipate how it sounds\nby inferring from its visual appearance and movement. In\nthis task, we aim to generate the sound of dynamic objects\nbased on videos displaying their moving trajectories.\nTask Definition.\nGiven a video clip of a falling object,\nthe goal of this task is to generate the corresponding sound\nbased on the visual appearance and motion of the object.\nThe generated sound must match the object\u2019s intrinsic prop-\nerties (e.g., material type) and temporally align with the\nobject\u2019s movement in the given video.\nThis task is re-\nlated to prior work on sound generation from in-the-wild\nPSNR: 24.43\nPSNR: 20.11\nPSNR: 35.71\nPSNR: 29.00\nVision Input\nTouch Input\nGround-truth \nPrediction\nPrediction\nGround-truth\nFigure 7.\nExamples of Touch2Vision (left) and Vision2Touch\n(right) cross-generation results with the VisGel [43] baseline.\nvideos [10, 34, 82], but here we focus more on predicting\nsoundtracks that closely match the object dynamics.\nEvaluation Metrics and Baselines.\nWe use the follow-\ning metrics for evaluating the sound generation quality: 1)\nSTFT-Distance, which measures the Euclidean distance be-\ntween the ground truth and predicted spectrograms, 2) En-\nvelope Distance, which measures the Euclidean distance be-\ntween the envelopes of the ground truth and the predicted\nsignals, and 3) CDPAM [47], which measures the percep-\ntual audio similarity. We use two state-of-the-art methods\nas our baselines: RegNet [10] and SpecVQGAN [34].\nTeaser Results.\nFig. 6 shows an example of the predicted\nsound for a falling plate. We observe that the generated\nsound matches well with the ground-truth sound of the ob-\nject perceptually, but it is challenging to predict the exact\nalignment that matches the object\u2019s motion.\n4.6. Visuo-Tactile Cross-Generation\nMotivation.\nWhen we touch an object that is visually oc-\ncluded (e.g., searching for a wallet from a backpack), we\ncan often anticipate its visual textures and geometry merely\nbased on the feeling on our fingertips. Similarly, we may\nimagine the feeling of touching an object purely from a\nglimpse of its visual appearance and vice-versa. To real-\nize this intuition, we study the visuo-tactile cross-generation\ntask initially proposed in [43].\nTask Definition.\nWe can either predict touch from vi-\nsion or vision from touch, leading to two subtasks: 1) Vi-\nsion2Touch: Given an image of a local region on the ob-\nject\u2019s surface, predict the corresponding tactile RGB image\nthat aligns with the visual image patch in both position and\norientation; and 2) Touch2Vision: Given a tactile reading on\nthe object\u2019s surface, predict the corresponding local image\npatch where the contact happens.\nEvaluation Metrics and Baselines.\nBoth the visual and\ntactile sensory data are represented by RGB images. There-\nfore, we evaluate the prediction performance for both sub-\ntasks using Peak Signal to Noise Ratio (PSNR) and Struc-\ntural Similarity (SSIM) \u2014 widely used metrics for assess-\ning image prediction quality. We use two image-to-image\ntranslation methods as our baselines: 1) Pix2Pix [35], which\n6\nSuccessful\nFailed\nCHANCE\nV\nT\nV + T\n50%\n87.4%\n93.4%\n99.4%\nFigure 8. Grasp stability prediction results with a wine glass. We\nshow an example of a successful grasp (left) and one of a failed\ngrasp (right). The table shows the prediction accuracy with V and\nT denoting using vision and/or touch, respectively.\nis a general-purpose conditional GAN framework, and 2)\nVisGel [43], which is a variant of Pix2Pix that is specifi-\ncally designed for cross-sensory prediction.\nTeaser Results.\nFig. 7 shows some examples of visuo-\ntactile cross-generation. Very accurate touch signals can be\nreconstructed from local views of the objects, while visual\nimage patches generated from tactile input tend to lose sur-\nface details. We suspect this is because different objects\noften share similar local patterns, making it ambiguous to\ninvert visual appearance from a single tactile reading.\n4.7. Grasp-Stability Prediction\nMotivation.\nGrasping an object is inherently a multisen-\nsory experience. When we grasp an object, vision helps\nus quickly localize the object, and touch provides an accu-\nrate perception of the local contact geometry. Both visual\nand tactile senses are useful for predicting the stability of\nrobotic grasping, which has been studied in prior work with\nvarious task setups [7,61,72].\nTask Definition.\nThe goal is to predict whether a robotic\ngripper can successfully grasp and stably hold an object be-\ntween its left and right fingers based on either an image of\nthe grasping moment from an externally mounted camera, a\ntactile RGB image obtained from the GelSight robot finger,\nor their combination. The grasp is considered failed if the\ngrasped object slips by more than 3 cm.\nEvaluation Metrics and Baselines.\nWe report the ac-\ncuracy of grasp stability prediction.\nWe implement\nTACTO [72] as the baseline method, which uses a ResNet-\n18 [32] network for feature extraction from the visual and\ntactile RGB images to predict the grasp stability.\nTeaser Results.\nWe show a successful and a failed grasp\nfor a wine glass in Fig. 8. Vision and touch are both helpful\nin predicting grasp stability, and combining the two sensory\nmodalities leads to the best result.\n4.8. Contact Refinement\nMotivation.\nWhen seeing a cup, we can instantly analyze\nits shape and structure, and decide to put our fingers around\nStart\nGoal\nTrajectory Executions\nVision\nTouch\nTime\nV\nT\nV + T\nSR \u2191\n0.86\n0.83\n0.88\nAE \u2193 (\u25e6)\n0.38\n0.56\n0.34\nFigure 9. Contact refinement results of a wooden cup object. From\nleft to right, we show the start and goal observations for both vision\n(top) and touch (bottom), and the actual trajectory executions. The\ntable shows the success rate (SR) and the angle error (AE) for\nusing vision (V), touch (T), or its combination.\nits handle to lift it. We often slightly adjust the orientations\nof our fingers to achieve the most stable pose for grasping.\nFor robots, locally refining how it contacts an object is of\ngreat practical importance. We define this new task as con-\ntact refinement, which can potentially be a building block\nfor many dexterous manipulation tasks.\nTask Definition.\nGiven an initial pose of the robot finger,\nthe task is to change the finger\u2019s orientation to contact the\npoint with a different target orientation. Each episode is\ndefined by the following: the contact point, the start orien-\ntation of the robot finger along the vertex normal direction\nof the contact point, and observations from the target finger\norientation in the form of either a third view camera image,\na tactile RGB image, or both. We use a continuous action\nspace over the finger rotation dimension. The task is suc-\ncessful if the finger reaches the target orientation within 15\naction steps with a tolerance of 1\u25e6.\nEvaluation Metrics and Baselines.\nWe evaluate using\nthe following metrics: 1) success rate (SR), which is the\nfraction of successful trials, and 2) average Angle Er-\nror (AE) across all test trials.\nModel Predictive Control\n(MPC) [22,24,69] has been shown to be a powerful frame-\nwork for planning robot actions. Therefore, we implement\nMultisensory-MPC as our baseline, which uses SVG [71]\nfor future frame prediction, and Model Predictive Path Inte-\ngral Control (MPPI) [73] for training the control policy.\nTeaser Results.\nFig. 9 shows a trajectory execution exam-\nple for using both vision and touch. We can obtain an 88%\nsuccess rate and average angle error of 0.17\u25e6 by combining\nboth modalities using our Multisensory-MPC baseline.\n4.9. Surface Traversal\nMotivation.\nWhen a robot\u2019s finger first contacts a posi-\ntion on an object, it may not be the desired surface loca-\ntion. Therefore, efficiently traversing from the first contact\n7\nStart\nGoal\nTrajectory Executions\nVision\nTouch\nTime\nV\nT\nV + T\nSR \u2191\n0.26\n0.54\n0.80\nPE \u2193 (mm)\n1.77\n3.32\n0.84\nFigure 10. Trajectory executions examples for surface traversal\nwith an iron pan. The table shows the success rate (SR) and aver-\nage position error (PE) for using vision (V) and/or touch (T).\npoint to the target location is a prerequisite for performing\nfollow-up actions or tasks. We name this new task surface\ntraversal, where we combine visual and tactile sensing to\nefficiently traverse to the specified target location given a\nvisual and/or tactile observation of the starting location.\nTask Definition.\nGiven an initial contacting point, the\ngoal of this task is to plan a sequence of actions to move the\nrobot finger horizontally or vertically in the contact plane to\nreach another target location on the object\u2019s surface. Each\nepisode is defined by the following: the initial contact point,\nand observations of the target point in the form of either a\nthird-view camera image, a tactile RGB image, or both. The\ntask is successful if the robot finger reaches the target point\nwithin 15 action steps with a tolerance of 1 mm.\nEvaluation Metrics and Baselines.\nWe report the fol-\nlowing two metrics: 1) success rate (SR), and 2) average\nposition error (PE), which is the average distance between\nthe final location of the robot finger on the object\u2019s surface\nand the target location. We use the same Multisensory-MPC\nbaseline as in the contact refinement task.\nTeaser Results.\nFig. 10 shows the surface traversal results\nwith an iron pan, where the back of the pan has a sequence\nof letters. The Multisensory-MPC model can successfully\ntraverse from the start location to the goal location. We ob-\nserve significant gains when combining vision and touch,\nachieving a success rate of 80%.\n4.10. Dynamic Pushing\nMotivation.\nTo push an object to a target location, we use\nvision to gauge the distance and tactile feedback to con-\ntrol the force and orientation. For example, in curling, the\nplayer sees and decides on the stone\u2019s target, holds its han-\ndle to push, and lightly turns the stone in one direction or\nthe other upon release. Both visual and tactile signals play a\ncrucial role in a successful delivery. We name this task dy-\nnamic pushing, which is related to prior work on dynamic\nadaptation for pushing [23] with only vision.\nV\nT\nV + T\nPE \u2193 (cm)\n23.81\n21.76\n17.63\nFigure 11. Examples of dynamic pushing. The table shows the\naverage position error (PE) for using vision (V) and/or touch (T)\nwith a rinsing cup.\nTask Definition.\nGiven example trajectories of pushing\ndifferent objects together with their corresponding visual\nand tactile observations, the goal of this task is to learn a\nforward dynamics model that can quickly adapt to novel\nobjects with a few contextual examples. With the learned\ndynamics model, the robot is then tasked to push the ob-\njects to new goal locations.\nEvaluation Metrics and Baselines.\nWe report the aver-\nage position error (PE) across all test trials. For the base-\nline, we use a ResNet-18 network for feature extraction and\na self-attention mechanism for modality fusion to learn the\nforward dynamics model. We use a sampling-based opti-\nmization algorithm (i.e., cross-entropy method [17]) to ob-\ntain the control signal.\nTeaser Results.\nFig. 11 shows an example of pushing a\nnovel test object to a new goal location. Vision and touch\nare both useful for learning object dynamics, and combining\nthe two sensory modalities leads to the best results.\n5. Conclusion\nWe presented the OBJECTFOLDER BENCHMARK, a\nsuite of 10 benchmark tasks centered around object recogni-\ntion, reconstruction, and manipulation to advance research\non multisensory object-centric learning. We also introduced\nOBJECTFOLDER REAL, the first dataset that contains all vi-\nsual, acoustic, and tactile real-world measurements of 100\nreal household objects. We hope our new dataset and bench-\nmark suite can serve as a solid building block to enable fur-\nther research and innovations in multisensory object mod-\neling and understanding.\nAcknowledgments.\nWe thank Samuel Clarke, Miaoya Zhong,\nMark Rau, Hong-Xing Yu, and Samir Agarwala for helping with\nthe data collection, and thank Doug James, Zilin Si, Fengyu\nYang, and Yen-Yu Chang for helpful discussions.\nThis work\nis in part supported by Amazon, NSF CCRI #2120095, NSF\nRI #2211258, ONR MURI N00014-22-1-2740, AFOSR YIP\nFA9550-23-1-0127, the Stanford Institute for Human-Centered AI\n(HAI), the Toyota Research Institute (TRI), Adobe, and Meta.\n8\nAppendix\nTable of Contents\nAppendices\n9\nA . Summary of Benchmark Tasks . . . .\n9\nB . Cross-Sensory Retrieval\n. . . . . . .\n9\nC . Contact Localization\n. . . . . . . . .\n10\nD . Material Classification\n. . . . . . . .\n12\nE . 3D Shape Reconstruction . . . . . . .\n12\nF . Sound Generation of Dynamic Objects\n14\nG . Visuo-Tactile Cross-Generation . . . .\n15\nH . Grasp Stability Prediction . . . . . . .\n16\nI\n. Contact Refinement . . . . . . . . . .\n16\nJ\n. Surface Traversal . . . . . . . . . . .\n17\nK . Dynamic Pushing . . . . . . . . . . .\n18\nL . Sim2Real Guidelines . . . . . . . . .\n19\nA. Summary of Benchmark Tasks\nWe introduce a suite of 10 benchmark tasks for multisen-\nsory object-centric learning, centered around object recog-\nnition, object reconstruction, and object manipulation. Ta-\nble 1 illustrates which of these tasks can be performed with\nsimulated data, real-world data, or both. All 10 tasks can\nbe done in simulation. We have obtained results using OB-\nJECTFOLDER REAL for four tasks, including cross-sensory\nretrieval, contact localization, material classification, and\n3D shape reconstruction. For sound generation of dynamic\nobjects and visuo-tactile cross-generation, sim2real transfer\nis not feasible due to the large sim-real gap, and the col-\nlected data in OBJECTFOLDER REAL is not directly appli-\ncable to these two tasks. Performing real-world versions\nof these two tasks may require collecting real datasets tai-\nlored for these two tasks. For manipulation, each task needs\nnontrivial effort for real-world robot deployment. For ex-\nample, prior work [61] has made a dedicated effort to make\nsim2real transfer possible for grasp stability prediction with\ncareful calibration of their physics simulator of robot dy-\nnamics, contact model, and the tactile optical simulator\nwith real-world data. We provide some tentative guidelines\non sim2real transfer in Sec. L and hope our open-sourced\nsimulation framework can encourage future exploration of\nsim2real transfer for these four tasks.\nB. Cross-Sensory Retrieval\nIn this section, we detail the cross-sensory retrieval\nbenchmark task definition and settings, baseline methods\nand evaluation metrics, and the experiment results.\nB.1\nTask Definition and Settings\nCross-sensory retrieval requires the model to take one sen-\nsory modality as input and retrieve the corresponding data\nof another modality. For instance, given the sound of strik-\ning a mug, the \u201caudio2vision\u201d model needs to retrieve the\ncorresponding image of the mug from a pool of images\nof hundreds of objects. In this benchmark, each sensory\nmodality (vision, audio, touch) can be used as either input\nor output, leading to 9 sub-tasks.\nSpecifically, we sample 100 instances from each modal-\nity of each object, resulting in two instance sets SA and SB.\nNext, we pair the instances from both modalities, which is\ndone by the Cartesian Product:\nP(i) = SA(i) \u00d7 SB(i),\n(1)\nwhere i is the object index and P is the set of instance\npairs. For each object, given modality A and modality B\n(A and B can be either vision, touch or audio), the goal of\ncross-sensory retrieval is to minimize the distance between\nthe representations of sensory observations from the same\nobject while maximizing those from different objects. In\nour experiments, we randomly split the objects from OB-\nJECTFOLDER into train/val/test splits of 800/100/100 ob-\njects, and split the 10 instances of each object from OB-\nJECTFOLDER REAL into 8/1/1.\nB.2\nBaselines and Evaluation Metrics\nWe use the following four state-of-the-art methods as our\nbaselines:\n\u2022 Canonical Correlation Analysis (CCA) [33]: CCA is a\ntraditional method to analyze the correlation between\ntwo datasets, which reduces the data dimension by a\nlinear projection. Specifically, in the testing process,\nwe leverage a ResNet [32] pre-trained with instance\nrecognition on the 800 objects in the training set to\nextract the features from the multisensory data. Next,\nthe features are projected into a unified representation\nspace by CCA.\n\u2022 Partial Least Squares (PLSCA) [18]: we follow the\nsame feature extracting process as CCA, except that\nthe final projection step is replaced with PLSCA,\nwhich combines CCA with the partial least squares\n(PLS).\n\u2022 Deep\nSupervised\nCross-Modal\nRetrieval\n(DSCMR) [81]:\nDSCMR is proposed to conduct\nimage-text retrieval by minimizing three losses: 1) the\ndiscrimination loss in the label space, which utilizes\nthe ground-truth category label as supervision, 2) the\ndiscrimination loss in the shared space, which mea-\nsures the similarity of the representations of different\n9\nObject Recognition\nObject Reconstruction\nObject Manipulation\nCSR\nCL\nMC\n3DSR\nSGoDO\nVTCG\nGSP\nCR\nST\nDP\nsim\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nreal\n\u2713\n\u2713\n\u2713\n\u2713\n\u2718\n\u2718\n?\n?\n?\n?\nTable 1. We summarize all 10 benchmark tasks to show whether they can be performed with simulated data, real-world data, or both. CSR,\nCL, MC, 3DSR, SGoDO, VTCG, GSP, CR, ST, DP denote cross-sensory retrieval, contact localization, material classification, 3D shape\nreconstruction, sound generation of dynamic objects, visuo-tactile cross-generation, grasp-stability prediction, contact refinement, surface\ntraversal, and dynamic pushing, respectively.\nInput\nRetrieved\nRANDOM\nCCA [33]\nPLSCA [18]\nDSCMR [2]\nDAR [81]\nVision\nVision (different views)\n1.00\n55.52\n82.43\n82.74\n89.28\nAudio\n1.00\n19.56\n11.53\n9.13\n20.64\nTouch\n1.00\n6.97\n6.33\n3.57\n7.03\nAudio\nVision\n1.00\n20.58\n13.37\n10.84\n20.17\nAudio (different vertices)\n1.00\n70.53\n80.77\n75.45\n77.80\nTouch\n1.00\n5.27\n6.96\n5.30\n6.91\nTouch\nVision\n1.00\n8.50\n6.25\n4.92\n8.80\nAudio\n1.00\n6.18\n7.11\n6.15\n7.77\nTouch (different vertices)\n1.00\n28.06\n52.30\n51.08\n54.80\nTable 2. Experiment results of the cross-sensory retrieval task using neural objects from OBJECTFOLDER. We evaluate the performance\nusing mean Average Precision (mAP).\nmodalities, helping the network to learn discriminative\nfeatures, and 3) the inter-modal invariance loss, which\neliminates the cross-modal discrepancy by minimizing\nthe distance between the representations of instances\nfrom the same category. We follow similar settings in\nour experiments.\n\u2022 Deep Aligned Representations (DAR) [2]: the DAR\nmodel is trained with both a model transfer loss and\na ranking pair loss. The model transfer loss utilizes a\nteacher model to train the DAR student model, which\nin our setting is a ResNet [32] model pretrained on our\ndata. The student model is trained to predict the same\nclass probabilities as the teacher model, which is mea-\nsured by the KL-divergence. The ranking pair loss is\nused to push the instances from the same object closer\nin the shared space, and push those from different ob-\njects apart from each other.\nIn the retrieval process, we set each instance in the in-\nput sensory modality as the query, and the instances from\nanother sensory are retrieved by ranking them according to\ncosine similarity. Next, the Average Precision (AP) is com-\nputed by considering the retrieved instances from the same\nobject as positive and others as negative. Finally, the model\nperformance is measured by the mean Average Precision\n(mAP) score, which is a widely-used metric for evaluating\nretrieval performance.\nB.3\nExperiment Results\nTable 2 and Table 3 show the cross-sensory retrieval re-\nsults on the neural objects from OBJECTFOLDER and the\nreal objects from OBJECTFOLDER REAL, respectively. We\nhave the following observation from the experiment results.\nCompared with the modality that encodes local information\n(touch), the modalities encoding global information (vision\nand audio) are more informative to perform cross-sensory\nretrieval. This is because different objects may share simi-\nlar local tactile features, but their visual images and impact\nsounds are discriminative (e.g., a steel cup and a ceramic\nbowl).\nC. Contact Localization\nIn this section, we detail the contact localization bench-\nmark task definition and settings, baseline methods and\nevaluation metrics, and the experiment results.\nC.1\nTask Definition and Settings\nGiven the object\u2019s mesh and different sensory observations\nof the contact position (visual images, impact sounds, or\ntactile readings), the multisensory contact localization task\naims to predict the vertex coordinate of the surface location\non the mesh where the contact happens. More formally, the\ntask can be defined as follows: given a visual patch image\n10\nInput\nRetrieved\nRANDOM\nCCA [33]\nPLSCA [18]\nDSCMR [2]\nDAR [81]\nVision\nVision (different views)\n3.72\n30.60\n60.95\n81.27\n81.00\nAudio\n3.72\n12.05\n27.12\n68.34\n66.92\nTouch\n3.72\n6.29\n9.77\n64.91\n39.46\nAudio\nVision\n3.72\n12.41\n30.54\n67.16\n64.35\nAudio (different vertices)\n3.72\n27.40\n55.75\n72.59\n68.79\nTouch\n3.72\n5.38\n11.66\n54.55\n33.00\nTouch\nVision\n3.72\n6.40\n11.46\n64.86\n41.18\nAudio\n3.72\n5.57\n13.89\n55.37\n37.30\nTouch (different vertices)\n3.72\n21.16\n27.97\n66.09\n41.42\nTable 3. Experiment results of the cross-sensory retrieval task using real objects from OBJECTFOLDER REAL. We evaluate the performance\nusing mean Average Precision (mAP).\nV (i.e., a visual image near the object\u2019s surface) and/or a\ntactile reading T and/or an impact sound S, and the shape\nof the object P (represented by a point cloud), the model\nneeds to localize the contact position C on the point cloud.\nThe task objective can be described as:\nmin\n\u03b8\n{Dist (f\u03b8 (V, T, S, P) , C)} ,\n(2)\nwhere f\u03b8 denotes the model for contact localization.\nSpecifically, we manually choose 50 objects with rich\nsurface features from the dataset, and sample 1, 000 con-\ntacts from each object. The sampling strategy is based on\nthe surface curvature. We assume that the curvature of each\nvertex is subject to a uniform distribution.\nThe average\nvalue of vertex curvatures is computed at first, and the ver-\ntices with curvatures that are far from the average value are\nsampled with higher probability (i.e., the vertices with more\nspecial surface patterns are more likely to be sampled).\nIn the experiments,\nwe randomly split the 1, 000\ninstances of each object into train/val/test splits of\n800/190/10, respectively. Similarly, in the real experiments,\nwe choose 53 objects from OBJECTFOLDER REAL and ran-\ndomly split the instances of each object by 8:1:1.\nC.2\nBaselines and Evaluation Metrics\nWe evaluate an existing method as our first baseline and also\ndevelop a new end-to-end differentiable baseline model for\ncontact localization:\n\u2022 Point Filtering [28, 45]:\nthis represents a typical\npipeline for contact localization, where the contact po-\nsitions are recurrently filtered out based on both the\nmultisensory input data and the relative displacements\nbetween the contacts of a trajectory. Each trajectory\ncontains 8 contacts, at each iteration of the filtering\nprocess, possible contact positions are generated on the\nobject surface, and the positions whose touch or au-\ndio features are similar to the input data are kept with\nhigher probability. As a result, the predictions gradu-\nally converge into a small area, which is treated as the\nfinal prediction. We only evaluate on the final contact\nof each trajectory. This method predicts very accurate\nresults but heavily relies on the relative displacements\nbetween the contacts instead of the multisensory infor-\nmation. Furthermore, the filtering process is not dif-\nferentiable, thus not being able to be optimized end-\nto-end.\n\u2022 Multisensory Contact Regression (MCR): in order to\nsolve the limitations of the point filtering method, we\npropose this novel differentiable baseline for contact\nlocalization. In this method, the model takes the object\npoint cloud and multisensory data as input and directly\nregresses the contact position.\nThe models\u2019 performance is evaluated by the average\nNormalized Distance (ND), which is the distance between\nthe predicted contact position and the ground-truth position\nnormalized by the largest distance between the two vertices\non the object mesh. The reason for adopting this metric is\nto fairly evaluate objects with different scales.\nC.3\nExperiment Results\nWe have the following two key observations from the re-\nsults shown in Table 4. Firstly, compared with touch, con-\ntact localization using vision and audio achieves much bet-\nter results, because they provide more global information\nand suffer from less ambiguity (i.e., different positions on\nthe object may share similar surface tactile features, result-\ning in large ambiguity). Secondly, though MCR performs\nworse than point filtering, it shows promising results that\nare close to the point filtering results even with a simple\nnetwork architecture. This shows the great potential of end-\nto-end contact localization methods. In Table 5, we show\nthe contact localization results on OBJECTFOLDER REAL.\n11\nMethod\nVision\nTouch\nAudio\nVision+Touch\nVision+Audio\nTouch+Audio\nVision+Touch+Audio\nRANDOM\n47.32\n47.32\n47.32\n47.32\n47.32\n47.32\n47.32\nPoint Filtering [45]\n\u2212\n4.21\n1.45\n\u2212\n\u2212\n3.73\n\u2212\nMCR\n5.03\n23.59\n4.85\n4.84\n1.76\n3.89\n1.84\nTable 4. Results of Multisensory Contact Localization on OBJECTFOLDER 2.0. We use average Normalized Distance (ND) as the evalua-\ntion metric. The numbers are all in percent (%).\nMethod\nVision\nTouch\nAudio\nFusion\nRANDOM\n50.57\n50.57\n50.57\n50.57\nMCR\n12.30\n32.03\n35.62\n12.00\nTable 5. Results of Multisensory Contact Localization on OB-\nJECTFOLDER REAL. We use average Normalized Distance (ND)\nas the evaluation metric. The numbers are all in percent (%). The\nPoint Filtering method requires obtaining touch/audio data at ar-\nbitrary points on the object\u2019s surface, which is not available for\nthe collected real object data in OBJECTFOLDER REAL. Thus this\nmethod is not included in this table.\nMethod\nVision\nTouch\nAudio\nFusion\nResNet [32]\n91.89\n74.36\n94.91\n96.28\nFENet [75]\n92.25\n75.89\n95.80\n96.60\nTable 6. Results on Multisensory Material Classification. We eval-\nuate the model performance by top-1 accuracy. The numbers are\nall in percent (%).\nD. Material Classification\nIn this section, we detail the multisensory material clas-\nsification benchmark task definition and settings, baseline\nmethods and evaluation metrics, and the experiment results.\nD.1\nTask Definition and Settings\nAll objects are labeled by seven material types: ceramic,\nglass, wood, plastic, iron, polycarbonate, and steel. The\nmultisensory material classification task is formulated as a\nsingle-label classification problem. Given an RGB image,\nan impact sound, a tactile image, or their combination, the\nmodel must predict the correct material label for the target\nobject. The 1, 000 objects are randomly split into train: val-\nidation: test = 800 : 100 : 100, and the model needs to gen-\neralize to new objects during the testing process. Further-\nmore, we also conduct a cross-object experiment on OB-\nJECTFOLDER REAL to test the Sim2Real transferring abil-\nity of the models, in which the 100 real objects are randomly\nsplit into train: validation: test = 60 : 20 : 20.\nMethod\nAccuracy\u2191\nResNet [32] w/o pretrain\n45.25\nResNet [32]\n51.02\nTable 7. Transfer learning results of material classification on OB-\nJECTFOLDER REAL. We evaluate the model performance by top-1\naccuracy. The numbers are all in percent (%).\nD.2\nBaselines and Evaluation Metrics\nWe use the following two methods as our baselines:\n\u2022 ResNet [32]: we finetune the ResNet backbone pre-\ntrained on ImageNet [19], which is considered as a\nnaive baseline.\n\u2022 Fractal Encoding Network [75]: the Fractal Encoding\n(FE) module is originally proposed for texture classi-\nfication task, which is a trainable module that encode\nthe multi-fractal texture features. We apply this mod-\nule to the ResNet baseline, enabling it to encode the\nmultisensory object features.\nWe evaluate the model performance by top-1 accuracy,\nwhich is a standard metric for classification tasks.\nD.3\nExperiment Results\nTab. 6 shows the comparison between the two baselines on\nthe simulation data. The Fractal Encoding module brings\nabout 1% improvement. Touch modality performs much\nworse than vision and audio due to its lack of global infor-\nmation, and the fusion of the three modalities leads to the\nbest performance.\nTab. 7 shows the Sim2Real experiment results. We eval-\nuate the performance of ResNet [32] with/without the pre-\ntraining on neural objects. Results show that pre-training on\nthe simulation data brings about 6% improvement.\nE. 3D Shape Reconstruction\nIn this section, we detail the multisensory 3D reconstruc-\ntion benchmark task definition and settings, baseline meth-\nods and evaluation metrics, and the experiment results.\n12\nMethod\nVision\nTouch\nAudio\nVision+Touch\nVision+Audio\nTouch+Audio\nVision+Touch+Audio\nMDN [64]\n4.02\n3.88\n5.04\n3.19\n4.05\n3.49\n2.91\nPCN [78]\n2.36\n3.81\n3.85\n2.30\n2.48\n3.27\n2.25\nMRT\n2.80\n4.12\n5.01\n2.78\n3.13\n4.28\n3.08\nTable 8. Results of Multisensory 3D Reconstruction on OBJECTFOLDER 2.0, we use chamfer distance (cm) as the metric to measure the\nmodel performance. Lower is better.\nMethod\nVision\nTouch\nAudio\nVision+Touch\nVision+Audio\nTouch+Audio\nVision+Touch+Audio\nMRT\n1.17\n1.04\n1.64\n0.96\n1.50\n1.12\n0.95\nTable 9. Results of Multisensory 3D Reconstruction on OBJECTFOLDER REAL. We use Chamfer Distance (cm) as the metric to measure\nthe model performance. Lower is better.\nE.1\nTask Definition and Setting\nGiven an RGB image of an object V , a sequence of tactile\nreadings T from the object\u2019s surface, or a sequence of im-\npact sounds S of striking N surface locations of the object,\nthe task is to reconstruct the 3D shape of the whole object\nrepresented by a point cloud given combinations of these\nmultisensory observations. The procedure can be denoted\nas:\nmin\n\u03b8\n{Dist (f\u03b8 (V, T, S) , PointsGT)} ,\n(3)\nwhere f\u03b8 represents the model for multisensory 3D recon-\nstruction and PointsGT represents the ground-truth point\ncloud. This task is related to prior efforts on visuo-tactile\n3D reconstruction [59, 63, 64, 67], but here we include all\nthree sensory modalities and study their respective roles.\nFor the visual RGB images, tactile RGB images, and im-\npact sounds used in this task, we respectively sample 100 in-\nstances around each object (vision) or on its surface (touch\nand audio). In all, given the 1, 000 objects, we can obtain\n1, 000 \u00d7 100 = 100, 000 instances for vision, touch, and\naudio modality, respectively. In the experiments, we ran-\ndomly split the 1, 000 objects as train: validation: test =\n800 : 100 : 100, meaning that the models need to gener-\nalize to new objects during testing. Furthermore, we also\ntest the model performance on OBJECTFOLDER REAL by\nsimilarly splitting the 100 objects as train: validation: test =\n60 : 20 : 20.\nE.2\nBaseline and Evaluation Metrics\nWe first use two state-of-the-art methods as our baselines,\nand we further develop a transformer-based baseline model\nfor 3D Reconstruction:\n\u2022 Mesh Deformation Network (MDN) [64]: this method\nfirst predicts local charts from the tactile images and\ncombine them with the initial global chart. Next, the\nmodel deforms the combined chart based on vision\nand/or audio signal by an iterative process, in which\nthe touch consistency is ensured (i.e., the local charts\nremain unchanged). The final prediction is a deformed\nchart, which is then transformed into a point cloud by\nsampling on its surface.\n\u2022 Point Completion Network (PCN) [78]: this method\ninfers the complete point cloud based on the coarse\nglobal point cloud (predicted by vision and audio)\nand/or detailed local point cloud (predicted by touch).\n\u2022 Multisensory Reconstruction Transformer (MRT):\nwhen touch is used in the reconstruction process, the\nprevious two methods require first predicting local\npoint clouds/meshes based on tactile readings. In our\nsetting, the prediction is done by transforming the\ndepth maps of the tactile readings into local point\nclouds. However, accurate depth maps can only be\nobtained in the simulation setting. In our setting for\nreal capture, only the tactile RGB images are captured,\nthus making it impossible for MDN and PCN to per-\nform 3D reconstruction using tactile data of OBJECT-\nFOLDER REAL. To solve this limitation, we propose a\nnew model, Multisensory Reconstruction Transformer\n(MRT), as a new baseline model. In this method, the\nmodel directly takes a sequence of tactile RGB im-\nages as input and encodes them into a latent vector\nby a transformer encoder. Specifically, the images are\nfirst forwarded into a ResNet [32] model to obtain a se-\nquence of features. Next, each feature is concatenated\nwith a learnable token that attends to all features in the\nattention layer. Finally, the concatenated sequence is\nsent into the transformer encoder and the output fea-\nture (i.e., the first token of the output sequence) is\ndecoded into the point cloud prediction by a simple\nMLP. The method can also encode a sequence of im-\npact sounds in a similar way.\nThe performance of each baseline is measured by Cham-\n13\nfer Distance (CD), which calculates the distance between\ntwo point clouds by:\nCD = 1\nS1\nX\nx\u2208S1\nmin\ny\u2208S2 \u2225x \u2212 y\u22252\n2 + 1\nS2\nX\ny\u2208S2\nmin\nx\u2208S1 \u2225y \u2212 x\u22252\n2,\n(4)\nwhere S1 and S2 are two point clouds.\nE.3\nExperiment Results\nTable 8 and Table 9 show the experiment results on both\nsimulation and real settings. We can obtain some key find-\nings from the results. Firstly, if only one single modality\nis used, vision does much better than the other two modali-\nties. This shows that the global information captured by the\nvisual signal is most important for 3D reconstruction.\nSecondly, when different modalities are combined, the\ntactile readings can significantly improve the reconstruc-\ntion from either vision or audio, while the audio data can\nonly benefit the reconstruction from touch in most cases.\nMoreover, when all three modalities are combined, the best\nresults are achieved in most experiments. We suspect this\nresults from the following different characteristics of the\nthree modalities: 1) vision data provides global informa-\ntion (shape and scale) of the objects, while only a few local\nsurface details can be obtained from a single image; 2) tac-\ntile readings contain very detailed local information of the\ntouched areas but miss the global context; 3) audio data only\nprovides rough scale information (i.e., the size of objects),\nwhile it is hard to infer fine-grained details of the objects\nfrom audio.\nF. Sound Generation of Dynamic Objects\nIn this section, we detail the sound generation of dy-\nnamic objects task definition and settings, baseline methods\nand evaluation metrics, and the experiment results.\nF.1\nTask Definition and Settings\nGiven a video clip of a falling object, the goal of this task\nis to generate the corresponding sound based on the visual\nappearance and motion of the object. The generated sound\nmust match the object\u2019s intrinsic properties (e.g., material\ntype) and temporally align with the object\u2019s movement in\nthe given video. This task is related to prior work on sound\ngeneration from in-the-wild videos [10,34,82], but here we\nfocus more on predicting soundtracks that closely match the\nobject dynamics.\nWe adopt a process similar to [36] to generate the data\nfor this task. Firstly, the physical simulation is performed\nin the Pybullet [16] simulator. We put the object above the\nfloor in the simulator and randomly set an initial velocity.\nThe object is then released and will have contact with the\nfloor, during which the object pose, contact positions, and\ncontact forces are recorded. Secondly, we query the Ob-\njectFile implicit representation network of the object with\nthe contact positions and forces to obtain the impact sounds.\nThe sounds are then temporally aligned into a single wave-\nform, which is the ground-truth audio. Finally, we render\nthe video using the Blender software, which generates the\nvideo according to the object pose at each frame.\nSpecifically, we choose 500 objects with reasonable\nscales, and 10 videos are generated for each object. We\nsplit the 10 videos into train/val/test splits of 8/1/1.\nF.2\nBaselines and Evaluation Metrics\nWe use two state-of-the-art methods as the baselines:\n\u2022 RegNet [10]: in this work, a novel module called au-\ndio forwarding regularizer is proposed to solve the in-\ncorrect mapping between the video frames and sound.\nDuring training, both the video frames and ground-\ntruth sound is used to predict the spectrogram. The\nregularizer only takes the ground-truth sound as the\ninput and encode it into a latent feature, which is con-\nsidered as \u201cvisual-irrelevant\u201d information. The model\nthen predicts the spectrogram according to both the\n\u201cvisual-relevant\u201d information provided by the video\nframes and the \u201cvisual-irrelevant\u201d information. This\narchitecture helps the model correctly map the visual\nsignal to the audio signal. During testing, the regular-\nizer is turned off, meaning the model should predict the\nspectrogram based on merely the video frames. With\nthe proper regularizer size, the model can capture use-\nful and correct information from the visual signal.\n\u2022 SpecVQGAN [34]: in this work, a more complex\nframework is proposed to generate the visually rel-\nevant sounds.\nA transformer is trained to autore-\ngressively generate codebook representations based on\nframe-wise video features.\nThe representation se-\nquence is then decoded into a spectrogram.\nFor the waveform prediction, we pretrain a MelGAN [38]\nvocoder on our dataset, which is used to reconstruct the tem-\nporal information of the spectrogram, transforming it into a\nsound waveform.\nTo comprehensively measure the sound generation qual-\nity, we evaluate the model performance by three metrics that\nrespectively computes the distance between the prediction\nand ground-truth in spectrogram space, waveform space,\nand latent space: 1) STFT-Distance, 2) Envelope Distance,\nand 3) CDPAM [47].\nF.3\nExperiment Results\nThe results in Table 10 show that RegNet model performs\nslightly better than the SpecVQGAN model under all of the\n14\nMethod\nSTFT\u2193\nEnvelope\u2193\nCDPAM\u2193\nRegNet [10]\n0.010\n0.036\n5.65 \u00d7 10\u22125\nSpecVQGAN [34]\n0.034\n0.042\n5.92 \u00d7 10\u22125\nTable 10. Results of generating object sound from video.\nthree metrics, though the SpecVQGAN model is larger and\nmore complex. This is probably because the transformer\nmodel used in SpecVQGAN requires more data to be ad-\nequately trained. See the Supp. video for the qualitative\ncomparison results.\nG. Visuo-Tactile Cross-Generation\nIn this section,\nwe detail the visuo-tactile cross-\ngeneration task definition and settings, baseline methods\nand evaluation metrics, and the experiment results.\nG.1\nTask Definition and Settings\nThe visuo-tactile cross-generation task is originally pro-\nposed in [43]. The task requires the model to reconstruct the\ntactile image from the visual input or vice versa. Similarly,\nwe define the following two subtasks: 1) Vision2Touch:\nGiven an image of a local region on the object\u2019s surface,\npredict the corresponding tactile RGB image that aligns\nwith the visual image patch in both position and orientation;\nand 2) Touch2Vision: Given a tactile reading on the object\u2019s\nsurface, predict the corresponding local image patch where\nthe contact happens.\nSpecifically, we choose 50 objects with rich tactile fea-\ntures and reasonable size, and sample 1, 000 visuo-tactile\nimage pairs on each of them. This results in 50 \u00d7 1, 000 =\n50, 000 image pairs. We conduct both cross-contact and\ncross-object experiments by respectively splitting the 1, 000\nvisuo-tactile pairs of each object into train: validation: test\n= 800 : 100 : 100 and splitting the 50 objects into train: val-\nidation: test = 40 : 5 : 5. The two settings require the model\nto generalize to new areas or new objects during testing.\nG.2\nBaselines and Evaluation Metrics\nWe use the following two state-of-the-art methods as the\nbaselines:\n\u2022 Pix2Pix [35]: Pix2Pix is a general-purpose framework\nfor image-to-image translation.\nThe model is opti-\nmized by both the L1 loss and a GAN loss, which\nrespectively make the generated image similar to the\ntarget and looks realistic. In our benchmark, we utilize\nPix2Pix to predict the images in both directions.\n\u2022 VisGel [43]:\nVisGel is a modification of Pix2Pix,\nwhich is designed for visuo-tactile cross generation\nMethod\nVision \u2192 Touch\nTouch \u2192 Vision\nPSNR\u2191\nSSIM\u2191\nPSNR\u2191\nSSIM\u2191\npix2pix [35]\n22.85\n0.71\n9.16\n0.28\nVisGel [43]\n29.60\n0.87\n14.56\n0.61\nTable 11. Cross-contact experiment results of visuo-tactile gener-\nation on 50 selected objects.\nMethod\nVision \u2192 Touch\nTouch \u2192 Vision\nPSNR\u2191\nSSIM\u2191\nPSNR\u2191\nSSIM\u2191\npix2pix [35]\n18.91\n0.63\n7.03\n0.12\nVisGel [43]\n25.91\n0.82\n12.61\n0.38\nTable 12. Cross-object experiment results of visuo-tactile genera-\ntion on 50 selected objects.\nspecifically. This work indicates that the huge domain\ngap between vision and touch makes it extremely diffi-\ncult to conduct generation in both directions. To solve\nthis problem, VisGel adds a reference image to the in-\nput, which in their setting is a global image of the ini-\ntial scene or the empty GelSight reading. Similarly,\nwe also add reference images to the input of both di-\nrections in our setting. The visual reference is an im-\nage of the whole object, showing the global shape and\ntexture of the object, and the tactile reference is the\nbackground of our GelSight sensor.\nThe prediction of the task is a generated image, thus should\nbe evaluated by metrics that assess the image quality. We\nadopt two metrics in our benchmark: Peak Signal to Noise\nRatio (PSNR) and Structural Similarity (SSIM), which are\nwidely used for evaluating image generation tasks.\nG.3\nExperiment Results\nThe experiment results are shown in Tab. 11 and Tab. 12.\nWe can have the following two key observations from the\nresults. Firstly, generating tactile images from visual im-\nages is much easier than the reversed direction. We observe\nthat very accurate tactile signals can often be reconstructed,\nwhile many of the generated visual images look hardly rea-\nsonable, even if the reference images are provided. This\nis probably due to the fact that different objects may share\nsimilar tactile patterns, making it difficult to infer a visual\nsignal from a single tactile reading. Secondly, the reference\ninformation used in VisGel brings huge improvement. Pro-\nviding the global visual signal or empty tactile reading helps\nthe model bridge the domain gap between vision and touch,\nmaking it able to produce much more realistic images. The\nimprovement is also clearly shown by the quantitative re-\nsults measured by both metrics.\n15\nH. Grasp Stability Prediction\nIn this section, we detail the grasp stability prediction\nbenchmark task definition and settings, baseline methods\nand evaluation metrics, and the experiment results.\nH.1\nTask Definition and Setting\nBoth visual and tactile senses are useful for predicting the\nstability of robotic grasping, which has been studied in prior\nwork with various task setups [7, 61, 72]. The goal of this\ntask is to predict whether a robotic gripper can successfully\ngrasp and stably hold an object between its left and right\nfingers based on either an image of the grasping moment\nfrom an externally mounted camera, a tactile RGB image\nobtained from the GelSight robot finger, or their combina-\ntion.\nMore specifically, we follow the settings of [61, 72] on\nsetting up the grasping pipeline. The robot takes the speci-\nfied grasping configuration, including the target orientation\nand height, moves to the specified location, and closes the\ngripper with a certain speed and force. After the gripper\ncloses entirely, we record the tactile images from the Gel-\nSight sensor as the tactile observations and record the im-\nages from the third-view camera as the visual observations.\nThese observations are used as input to our grasp stability\nprediction model. Then, the robot attempts to lift the ob-\nject 18 cm to the ground. The grasp is considered failed if\nthe grasped object slips by more than 3 cm. Otherwise, it\u2019s\nconsidered successful.\nWe generate 10,000 grasping examples for each object.\nWe balance the success and failure cases to be around 1:1.\nWe randomly split the dataset into 9,000 samples for train-\ning and 1,000 samples for testing. We choose 5 different\nobjects with different materials and shapes suitable for the\ngrasping task.\nH.2\nBaseline and Evaluation Metrics\nWe use TACTO [72] as the baseline method, which uses\na ResNet-18 [32] network for feature extraction from the\nvisual and tactile RGB images to predict the grasp stability.\nWe use cross-entropy loss to train the binary classification\nnetwork with different sensory inputs. We report the grasp\nstability prediction accuracy as the evaluation metric.\nH.3\nExperiment results\nTable 14 shows the results on 5 representative objects from\nOBJECTFOLDER REAL. The results consistently suggest\nthat vision and touch both play a crucial role in predicting\ngrasp stability. Combining the two sensory modalities leads\nto the best performance.\nIn addition, we use this task as a case study to evalu-\nate representations pre-trained on OBJECTFOLDER REAL\ncompared to OBJECTFOLDER 2.0 [28] and the Touch and\nGo dataset [76], which are the largest simulated dataset and\nhuman-collected visuo-tactile dataset in the literature, re-\nspectively. We also compare with a baseline that performs\nsupervised pre-training on ImageNet [19].\nFollowing the settings in [76], we learn tactile represen-\ntations with visuo-tactile contrastive multiview coding [70],\nand then use the setup and dataset of [7] for evaluating grasp\nstability prediction. We extract visuo-tactile pairs from the\nvideos we record with the third-view camera and the tactile\nsensor during data collection. We extract 3 pairs in the last\n0.5 seconds for each point, leading to 10.6K visuo-tactile\npairs in total.\nTable 13 shows the results. We quote the baseline results\ndirectly from [76]. Pre-training on OBJECTFOLDER REAL\noutperforms prior datasets by a large margin, demonstrating\nthe value and potential of transfer learning using our dataset.\nI. Contact Refinement\nIn this section, we detail the contact refinement bench-\nmark task definition and settings, baseline methods and\nevaluation metrics, and the experiment results.\nI.1\nTask Definition and Setting\nGiven an initial pose of the robot finger, the goal of the con-\ntact refinement task is to change the finger\u2019s orientation to\ncontact the point with a different target orientation. Each\nepisode is defined by the following: the contact point, the\nstart orientation of the robot finger along the vertex normal\ndirection of the contact point, and observations from the tar-\nget finger orientation in the form of either a third view cam-\nera image, a tactile RGB image, or both. We use a contin-\nuous action space over the finger rotation dimension. The\ntask is successful if the finger reaches the target orientation\nwithin 15 action steps with a tolerance of 1\u25e6. Based on the\nobject category, we choose a local region of interest (RoI)\nfor the robot to touch (e.g., the handle of the cup). The dis-\ncrete Gaussian curvature [14] of the RoI should be larger\nthan 0. The robot will randomly select a point in that local\nregion and touches that point with a random finger orien-\ntation. Then, the robot samples actions from a Gaussian\ndistribution, and repeats the sampled action four times be-\nfore it samples the next action. We set the area of RoI to be\naround 5 cm2 and sampled 600 points for training and 100\npoints for testing.\nI.2\nBaseline and Evaluation Metrics\nModel Predictive Control (MPC) [22, 24, 69] has been\nshown to be a powerful framework for planning robot ac-\ntions. Therefore, we implement Multisensory-MPC as our\n16\nChance\nImageNet [19]\nOBJECTFOLDER 2.0 [28]\nTouch and Go [76]\nOBJECTFOLDER REAL\n56.1%\n73.0%\n69.4%\n78.1%\n84.9%\nTable 13. Transfer learning results. We show the grasp stablity prediction results on the dataset from [7] by pre-training on ImageNet [19],\nand other tactile datasets, including OBJECTFOLDER 2.0 [28], Touch and Go [76], and our new OBJECTFOLDER REAL dataset.\nVision\n87.4%\n77.3%\n81.7%\n79.2%\n77.7%\nTouch\n90.1%\n81.0%\n89.0%\n84.3%\n89.1%\nVision + Touch\n92.0%\n88.9%\n93.8%\n85.5%\n90.6%\nTable 14. Results on grasp stability prediction. We report the prediction accuracy with vision and/or touch.\nbaseline, which uses SVG [71] for future frame prediction,\nand Model Predictive Path Integral Control (MPPI) [73] for\ntraining the control policy.\nTo train the video prediction model, We collect 600 tra-\njectories for training and 100 trajectories for evaluation.\nEach trajectory has 20 steps. We train a separate model for\nvision and touch for each object. During training, we ran-\ndomly sample a sequence of 14 steps, from which we con-\ndition on the first 2 frames and predict 12 future frames. For\nMPC, we use MPPI with a squared error objective, which\ncalculates the pixel-wise error and samples actions based on\nit. The horizon length is 10 steps, which means the model\nwill sample an action sequence of length 10 into the future.\nThe robot should finish the task within 15 steps, beyond\nwhich we consider the task fails.\nI.3\nExperiment results\nIn the main paper, we have shown a trajectory execution ex-\nample for using both vision and touch. Table 15 shows the\ncontact refinement results of 5 objects from the OBJECT-\nFOLDER REAL dataset. We can see that vision and touch\nare both very useful for contact refinement. Combining the\ntwo modalities leads to the best success rate and can refine\nmore accurately to the target location.\nJ. Surface Traversal\nIn this section, we detail the surface traversal benchmark\ntask definition and settings, baseline methods and evalua-\ntion metrics, and the experiment results.\nJ.1\nTask Definition and Setting\nGiven an initial contacting point, the goal of this task is to\nplan a sequence of actions to move the robot finger horizon-\ntally or vertically in the contact plane to reach another target\nlocation on the object\u2019s surface. Each episode is defined by\nthe following: the initial contact point, and observations of\nthe target point in the form of either a third-view camera\nimage, a tactile RGB image, or both. The task is successful\nif the robot finger reaches the target point within 15 action\nsteps with a tolerance of 1 mm. We follow a similar data\ngeneration protocol as the contact refinement task. Based\non the object\u2019s category and geometry, we select a local re-\ngion of interest (RoI) for the robot to traverse. The discrete\nGaussian curvature [14] of the RoI should be larger than 0\nand less than 0.01. The robot starts at a random location in\nthat region and samples actions from a Gaussian distribution\nalong two directions. The robot repeats the sampled action\nfour times before it samples the next action. The number of\nsampled trajectories is proportional to the area of RoI with\n50 trajectories per 1 cm2 for training and 5 trajectories per\n1 cm2 for testing.\nJ.2\nBaseline and Evaluation Metrics\nSimilar to the contact refinement task, we implement\nMultisensory-MPC as our baseline, which uses SVG [71]\nfor future frame prediction, and Model Predictive Path Inte-\ngral Control (MPPI) [73] for training the control policy. We\nevaluate using the following metrics: 1) success rate (SR),\nwhich is the fraction of successful trials, and 2) average An-\ngle Error (AE) across all test trials. For the video prediction\nmodel, we collect 2,000 trajectories for training and 200 tra-\njectories for evaluation. Then, we follow the same control\npipeline as in the contact refinement task.\nJ.3\nExperiment results\nTable 16 shows the results of surface traversal with 5 ob-\njects from the OBJECTFOLDER REAL dataset. Generally,\n17\nModalities\nSR \u2191\nAE \u2193\nSR \u2191\nAE \u2193\nSR \u2191\nAE \u2193\nSR \u2191\nAE \u2193\nSR \u2191\nAE \u2193\nVision\n0.91\n0.31\n0.96\n0.24\n0.88\n0.36\n0.95\n0.24\n0.91\n0.33\nTouch\n0.86\n0.41\n0.93\n0.34\n0.88\n0.37\n0.95\n0.28\n0.91\n0.32\nVision + Touch\n0.94\n0.26\n0.97\n0.21\n0.92\n0.27\n0.96\n0.21\n0.93\n0.24\nTable 15. Results on contact refinement. We report the success rate (SR) and average angle error (AE) for using vision and/or touch for 5\nobjects from our OBJECTFOLDER REAL dataset. \u2191 denotes higher is better, \u2193 denotes lower is better.\nModalities\nSR \u2191\nPE \u2193\nSR \u2191\nPE \u2193\nSR \u2191\nPE \u2193\nSR \u2191\nPE \u2193\nSR \u2191\nPE \u2193\nVision\n0.03\n6.47\n0.24\n2.40\n0.27\n2.23\n0.28\n2.06\n0.18\n2.78\nTouch\n0.20\n6.88\n0.08\n6.51\n0.05\n9.91\n0.06\n8.16\n0.06\n7.93\nVision + Touch\n0.18\n5.95\n0.36\n1.75\n0.20\n6.88\n0.18\n2.36\n0.23\n3.42\nTable 16. Results on surface traversal. We report the success rate (SR) and average position error (PE) in mm for using vision and/or touch\nfor 5 objects from our OBJECTFOLDER REAL dataset. \u2191 denotes higher is better, \u2193 denotes lower is better.\nwe observe that the performance of this task is very object-\ndependent. Vision provides global information about the\nobject, while touch offers precise contact geometry. There-\nfore, combining the two modalities often leads to more ac-\ncurate traversal results. However, our current Mulisensory-\nMPC model cannot make the most of the benefit from the\ntwo modalities, sometimes leading to worse results com-\npared to the performance of a single modality.\nK. Dynamic Pushing\nIn this section, we detail the dynamic pushing bench-\nmark task definition and settings, baseline methods and\nevaluation metrics, and the experiment results.\nK.1\nTask Definition and Settings\nGiven example trajectories of pushing different objects to-\ngether with their corresponding visual and tactile observa-\ntions, the goal of this task is to learn a forward dynamics\nmodel that can quickly adapt to novel objects with a few\ncontextual examples. With the learned dynamics model, the\nrobot is then tasked to push the objects to new goal loca-\ntions.\nMore specifically, the object is initialized at a fixed loca-\ntion in front of the robot. We specify the angle between the\nline passing through the center-of-mass of the object and the\nof the gel on the GelSight sensor and the x-axis. This an-\ngel defines the pushing direction. We also specify a pushing\ndistance, which is the distance along the pushing direction.\nThe pushing speed stays the same for all trials. With these\ntwo parameters, the robot can push the object to some posi-\ntions in front of it.\nWe select 16 cylinder-shaped objects for training and\ncollect 200 trials for each object. We vary the object\u2019s mass\nand friction coefficients every 10 trials. For evaluation, we\nselect 6 unseen objects with different geometry, mass, and\nfriction coefficients and run 500 trials for each object.\nK.2\nBaseline and Evaluation Metrics\nFor our baseline model, we use a ResNet-18 network for\nfeature extraction and a three-layer MLP to learn the for-\nward dynamics model. We use a sampling-based optimiza-\ntion algorithm (i.e., cross-entropy method (CEM) [17]) to\nobtain the control signal. During training, we encode a fea-\nture vector by taking in observations from 3 trials of the\nobject with the same mass and friction and use that feature\nvector to train the dynamics model. The dynamics model\ntakes in the feature vector, the angle, and the pushing dis-\ntance to predict the final position of that object. During\ntesting, we use CEM as the control policy with L2 distance\nbetween the predicted location and the goal location as the\ncost. Then, by specifying the goal location, the dynamics\n18\nVision\n24.04\n18.37\n18.26\n19.02\n23.39\nTouch\n30.06\n30.79\n34.65\n26.22\n26.12\nVision + Touch\n30.30\n22.00\n18.88\n18.94\n19.25\nTable 17. Results on dynamic pushing. We report the average position error (PE) in cm for using vision and/or touch for 5 objects from\nthe OBJECTFOLDER REAL dataset.\nmodel can predict the corresponding action that reaches the\ngoal, represented by the pushing angle and the pushing dis-\ntance. We use the average position error (PE) across all test\ntrials as our metric.\nK.3\nExperiment results\nTable 17 shows the results. We can see that vision and touch\nare both useful for learning object dynamics. Combining\nthe two sensory modalities leads to the best results for ob-\njects with simple surface geometry.\nL. Sim2Real Guidelines\nIn this section, we provide some tentative guidelines on\npotentially transferring from simulation to real-world re-\ngarding the four robotic manipulation tasks as a reference\nfor future work, including optical calibration and elastic de-\nformation calibration.\nL.1\nOptical Calibration\nThe GelSight tactile images are rendered with a state-of-\nthe-art simulation framework, Taxim [60]. Taxim uses a\nlookup table to map the contact shapes to tactile images.\nFollowing the pipeline in [60], we have made similar at-\ntempts to press a ball with a radius of 4mm over the elas-\ntomer surface and manually locate contact areas in the tac-\ntile images. The polynomial lookup table can be calibrated\nwith the collected data.\nL.2\nElastic Deformation Calibration\nTo eliminate the gap between sim-to-real transfer, we also\nneed to calibrate the physics parameter of contact dynam-\nics using real-world data. The elastic deformation can be\nsimplified into two parts: normal and lateral displacements.\nTaxim uses linear mapping to characterize the relationship\nbetween the indentation displacement and the normal force.\nUsing a force gauge stand, we can collect a set of force-\ndisplacement pairs to fit the physics parameter along the\nnormal direction.\nFor lateral displacements, we haven\u2019t\nfound a standard and general procedure to calibrate the sim-\nulator with real-world data for all four tasks. A potential\napproach described in [61] for the grasp-stability prediction\ntask is to optimize the friction coefficients by matching the\ngrasping labels between simulated and real data under the\nsame configuration of grasping heights and forces. We leave\nthe exploration of better and more general ways for sim-to-\nreal calibration as future work.\nReferences\n[1] Relja Arandjelovic and Andrew Zisserman. Look, listen and\nlearn. In ICCV, 2017. 3\n[2] Yusuf Aytar, Carl Vondrick, and Antonio Torralba.\nSee,\nhear, and read: Deep aligned representations. arXiv preprint\narXiv:1706.00932, 2017. 1, 3, 4, 5, 10, 11\n[3] Andrei Barbu, David Mayo, Julian Alverio, William Luo,\nChristopher Wang, Dan Gutfreund, Josh Tenenbaum, and\nBoris Katz. ObjectNet: A large-scale bias-controlled dataset\nfor pushing the limits of object recognition models.\nIn\nNeurIPS, 2019. 1, 2\n[4] Harry G Barrow, Jay M Tenenbaum, Robert C Bolles, and\nHelen C Wolf.\nParametric correspondence and Chamfer\nmatching: Two new techniques for image matching.\nIn\nProceedings: Image Understanding Workshop, pages 21\u201327,\n1977. 6\n[5] Paul J Besl and Neil D McKay. Method for registration of\n3-D shapes. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 14:239\u2013256, 1992. 4\n[6] Roberto Calandra, Andrew Owens, Dinesh Jayaraman,\nJustin Lin, Wenzhen Yuan, Jitendra Malik, Edward H Adel-\nson, and Sergey Levine. More than a feeling: Learning to\ngrasp and regrasp using vision and touch. RA-L, 2018. 1, 3\n[7] Roberto Calandra, Andrew Owens, Manu Upadhyaya, Wen-\nzhen Yuan, Justin Lin, Edward H Adelson, and Sergey\nLevine. The feeling of success: Does touch sensing help\npredict grasp outcomes? In CoRL, 2017. 3, 7, 16, 17\n[8] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapeNet:\nAn information-rich 3D model repository.\narXiv preprint\narXiv:1512.03012, 2015. 1, 2\n[9] Changan Chen, Ruohan Gao, Paul Calamia, and Kristen\nGrauman. Visual acoustic matching. In CVPR, 2022. 3\n19\n[10] Peihao Chen, Yang Zhang, Mingkui Tan, Hongdong Xiao,\nDeng Huang, and Chuang Gan. Generating visually aligned\nsound from videos. IEEE Transactions on Image Processing,\n2020. 6, 14, 15\n[11] Ziyang Chen, David F Fouhey, and Andrew Owens. Sound\nlocalization by self-supervised time delay estimation.\nIn\nECCV, 2022. 3\n[12] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin\nChen, and Silvio Savarese. 3D-R2N2: A unified approach\nfor single and multi-view 3D object reconstruction.\nIn\nECCV, 2016. 5\n[13] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhip-\ning Luo, and Yantao Zheng. NUS-WIDE: A real-world web\nimage database from national university of singapore.\nIn\nProceedings of the ACM International Conference on Image\nand Video Retrieval, 2009. 4\n[14] David Cohen-Steiner and Jean-Marie Morvan. Restricted de-\nlaunay triangulations and normal cycle. In Proceedings of\nthe nineteenth annual symposium on Computational geome-\ntry, 2003. 16, 17\n[15] Jasmine Collins, Shubham Goel, Achleshwar Luthra, Leon\nXu, Kenan Deng, Xi Zhang, Tomas F Yago Vicente, Himan-\nshu Arora, Thomas Dideriksen, Matthieu Guillaumin, and Ji-\ntendra Malik. ABO: Dataset and benchmarks for real-world\n3D object understanding. arXiv preprint arXiv:2110.06199,\n2021. 2\n[16] Erwin Coumans and Yunfei Bai. Pybullet, a python mod-\nule for physics simulation for games, robotics and machine\nlearning. 2016. 14\n[17] Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and\nReuven Y Rubinstein.\nA tutorial on the cross-entropy\nmethod. Annals of Operations Research, 2005. 8, 18\n[18] Sijmen de Jong, Barry M. Wise, and N. Lawrence Ricker.\nCanonical partial least squares and continuum power regres-\nsion. Journal of Chemometrics: A Journal of the Chemomet-\nrics Society, 2001. 4, 9, 10, 11\n[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. ImageNet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 1, 2, 12, 16, 17\n[20] Siyuan Dong, Wenzhen Yuan, and Edward H Adelson. Im-\nproved gelsight tactile sensor for measuring geometry and\nslip. In IROS, 2017. 1, 3\n[21] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-\nman, Ryan Hickman, Krista Reymann, Thomas B McHugh,\nand Vincent Vanhoucke. Google scanned objects: A high-\nquality dataset of 3D scanned household items. In ICRA,\n2022. 2\n[22] Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie,\nAlex Lee, and Sergey Levine. Visual foresight: Model-based\ndeep reinforcement learning for vision-based robotic control.\narXiv preprint arXiv:1812.00568, 2018. 7, 16\n[23] Ben Evans, Abitha Thankaraj, and Lerrel Pinto. Context is\neverything: Implicit identification for dynamics adaptation.\nIn ICRA, 2022. 8\n[24] Chelsea Finn and Sergey Levine. Deep visual foresight for\nplanning robot motion. In ICRA, 2017. 7, 16\n[25] Ruohan Gao, Yen-Yu Chang, Shivani Mall, Li Fei-Fei, and\nJiajun Wu. ObjectFolder: A dataset of objects with implicit\nvisual, auditory, and tactile representations. In CoRL, 2021.\n1, 2, 3\n[26] Ruohan Gao, Rogerio Feris, and Kristen Grauman. Learning\nto separate object sounds by watching unlabeled video. In\nECCV, 2018. 3\n[27] Ruohan Gao and Kristen Grauman. Co-separating sounds of\nvisual objects. In ICCV, 2019. 3\n[28] Ruohan Gao, Zilin Si, Yen-Yu Chang, Samuel Clarke, Jean-\nnette Bohg, Li Fei-Fei, Wenzhen Yuan, and Jiajun Wu. Ob-\njectFolder 2.0: A multisensory object dataset for sim2real\ntransfer. In CVPR, 2022. 1, 2, 3, 5, 6, 11, 16, 17\n[29] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In CVPR, 2014. 2\n[30] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch,\nYilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra-\ngasam, Florian Golemo, Charles Herrmann, et al. Kubric: A\nscalable dataset generator. In CVPR, 2022. 1\n[31] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask R-CNN. In ICCV, 2017. 2\n[32] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\n2016. 2, 5, 7, 9, 10, 12, 13, 16\n[33] Harold Hotelling. Relations between two sets of variates.\nBiometrika, 28(3/4):321\u2013377, 1936. 4, 9, 10, 11\n[34] Vladimir Iashin and Esa Rahtu.\nTaming visually guided\nsound generation. In BMVC, 2021. 6, 14, 15\n[35] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In CVPR, 2017. 6, 15\n[36] Xutong Jin, Sheng Li, Guoping Wang, and Dinesh Manocha.\nNeuralsound: Learning-based modal sound synthesis with\nacoustic transfer, May 2022. 14\n[37] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. Communications of the ACM, 2017. 2\n[38] Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lu-\ncas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de Bre-\nbisson, Yoshua Bengio, and Aaron Courville. Melgan: Gen-\nerative adversarial networks for conditional waveform syn-\nthesis, Dec. 2019. 14\n[39] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan\nPopov, Matteo Malloci, Alexander Kolesnikov, et al. The\nopen images dataset v4. IJCV, 2020. 1, 2\n[40] Jet-Tsyn Lee, Danushka Bollegala, and Shan Luo. \u201cTouch-\ning to see\u201d and \u201cseeing to feel\u201d: Robotic cross-modal sen-\nsory data generation for visual-tactile perception. In ICRA,\n2019. 3\n[41] Michelle A Lee, Yuke Zhu, Krishnan Srinivasan, Parth Shah,\nSilvio Savarese, Li Fei-Fei, Animesh Garg, and Jeannette\nBohg. Making sense of vision and touch: Self-supervised\nlearning of multimodal representations for contact-rich tasks.\nIn ICRA, 2019. 3\n20\n[42] Hao Li, Yizhi Zhang, Junzhe Zhu, Shaoxiong Wang,\nA. Michelle Lee, Huazhe Xu, Edward Adelson, Fei-Fei Li,\nRuohan Gao, and Jiajun Wu. See, hear, and feel: Smart sen-\nsory fusion for robotic manipulation. In CoRL, 2022. 3\n[43] Yunzhu Li, Jun-Yan Zhu, Russ Tedrake, and Antonio Tor-\nralba. Connecting touch and vision via cross-modal predic-\ntion. In CVPR, 2019. 1, 3, 6, 7, 15\n[44] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV, 2014. 1, 2\n[45] Jun S Liu and Rong Chen. Sequential monte carlo methods\nfor dynamic systems. Journal of the American Statistical\nAssociation, 93(443):1032\u20131044, 1998. 5, 11, 12\n[46] Shan Luo, Wenxuan Mou, Kaspar Althoefer, and Hongbin\nLiu. Localizing the object contact through matching tactile\nfeatures with visual map. In ICRA, 2015. 3\n[47] Pranay Manocha, Zeyu Jin, Richard Zhang, and Adam\nFinkelstein. CDPAM: Contrastive learning for perceptual au-\ndio similarity. In ICASSP, 2021. 6, 14\n[48] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space.\nIn CVPR,\n2019. 1, 5\n[49] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 1\n[50] Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas\nKurz, Joerg H Mueller, Chakravarty R Alla Chaitanya, An-\nton Kaplanyan, and Markus Steinberger. DONeRF: Towards\nreal-time rendering of compact neural radiance fields using\ndepth oracle networks. In EGSR, 2021. 2\n[51] Andrew Owens and Alexei A Efros.\nAudio-visual scene\nanalysis with self-supervised multisensory features.\nIn\nECCV, 2018. 3\n[52] Dinesh K Pai, Kees van den Doel, Doug L James, Jochen\nLang, John E Lloyd, Joshua L Richmond, and Som H Yau.\nScanning physical interaction behavior of 3D objects.\nIn\nSIGGRAPH, 2001. 3\n[53] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove.\nDeepSDF: Learning\ncontinuous signed distance functions for shape representa-\ntion. In CVPR, 2019. 5\n[54] Yuxin Peng, Xin Huang, and Yunzhen Zhao. An overview\nof cross-media retrieval: Concepts, methodologies, bench-\nmarks, and challenges. TCSVT, 2017. 4\n[55] Yuxin Peng, Jinwei Qi, and Yuxin Yuan. Modality-specific\ncross-modal similarity measurement with recurrent attention\nnetwork. TIP, 2018. 4\n[56] Jose Costa Pereira, Emanuele Coviello, Gabriel Doyle,\nNikhil Rasiwasia, Gert RG Lanckriet, Roger Levy, and\nNuno Vasconcelos. On the role of correlation and abstrac-\ntion in cross-modal multimedia retrieval.\nIEEE TPAMI,\n36(03):521\u2013535, 2014. 4\n[57] Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia\nHockenmaier. Collecting image annotations using amazon\u2019s\nmechanical turk. In Proceedings of the NAACL HLT Work-\nshops, 2010. 4\n[58] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,\nLuca Sbordone, Patrick Labatut, and David Novotny. Com-\nmon objects in 3D: Large-scale learning and evaluation of\nreal-life 3D category reconstruction. In ICCV, 2021. 1, 2\n[59] Lukas Rustler, Jens Lundell, Jan Kristof Behrens, Ville\nKyrki, and Matej Hoffmann.\nActive visuo-haptic object\nshape completion. RA-L, 2022. 6, 13\n[60] Zilin Si and Wenzhen Yuan. Taxim: An example-based sim-\nulation model for gelsight tactile sensors.\narXiv preprint\narXiv:2109.04027, 2021. 19\n[61] Zilin Si, Zirui Zhu, Arpit Agarwal, Stuart Anderson, and\nWenzhen Yuan. Grasp stability prediction with sim-to-real\ntransfer from tactile sensing. In IROS, 2022. 7, 9, 16, 19\n[62] Vincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wet-\nzstein.\nScene representation networks: Continuous 3D-\nstructure-aware neural scene representations.\nIn NeurIPS,\n2019. 2\n[63] Edward Smith, David Meger, Luis Pineda, Roberto Calan-\ndra, Jitendra Malik, Adriana Romero Soriano, and Michal\nDrozdzal. Active 3D shape reconstruction from vision and\ntouch. NeurIPS, 2021. 1, 3, 6, 13\n[64] Edward J Smith, Roberto Calandra, Adriana Romero, Geor-\ngia Gkioxari, David Meger, Jitendra Malik, and Michal\nDrozdzal. 3D shape reconstruction from vision and touch.\nIn NeurIPS, 2020. 1, 3, 6, 13\n[65] Linda Smith and Michael Gasser. The development of em-\nbodied cognition: Six lessons from babies. Artificial Life,\n2005. 2\n[66] Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong\nZhang, Chengkai Zhang, Tianfan Xue, Joshua B Tenenbaum,\nand William T Freeman. Pix3D: Dataset and methods for\nsingle-image 3D shape modeling. In CVPR, 2018. 1\n[67] Sudharshan Suresh, Zilin Si, Joshua G Mangelson, Wenzhen\nYuan, and Michael Kaess. Efficient shape mapping through\ndense touch and vision. arXiv preprint arXiv:2109.09884,\n2021. 6, 13\n[68] Sudharshan Suresh, Zilin Si, Joshua G Mangelson, Wenzhen\nYuan, and Michael Kaess. ShapeMap 3-D: Efficient shape\nmapping through dense touch and vision. In ICRA, 2022. 1,\n3\n[69] Stephen Tian, Frederik Ebert, Dinesh Jayaraman, Mayur\nMudigonda, Chelsea Finn, Roberto Calandra, and Sergey\nLevine.\nManipulation by feel: Touch-based control with\ndeep predictive models. In ICRA, 2019. 7, 16\n[70] Yonglong Tian, Dilip Krishnan, and Phillip Isola.\nCon-\ntrastive multiview coding. In ECCV, 2020. 16\n[71] Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru\nErhan, Quoc V Le, and Honglak Lee. High fidelity video\nprediction with large stochastic recurrent neural networks.\nNeurIPS, 2019. 7, 17\n[72] Shaoxiong Wang, Mike Lambeta, Po-Wei Chou, and Roberto\nCalandra. TACTO: A fast, flexible and open-source simula-\ntor for high-resolution vision-based tactile sensors.\narXiv\npreprint arXiv:2012.08456, 2020. 7, 16\n21\n[73] Grady Williams, Paul Drews, Brian Goldfain, James M.\nRehg, and Evangelos A. Theodorou. Aggressive driving with\nmodel predictive path integral control. In ICRA, 2016. 7, 17\n[74] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3D\nShapeNets: A deep representation for volumetric shapes. In\nCVPR, 2015. 2\n[75] Yong Xu, Feng Li, Zhile Chen, Jinxiu Liang, and Yuhui\nQuan. Encoding spatial distribution of convolutional features\nfor texture representation. In NeurIPS, 2021. 5, 12\n[76] Fengyu Yang, Chenyang Ma, Jiacheng Zhang, Jing Zhu,\nWenzhen Yuan, and Andrew Owens. Touch and go: Learn-\ning from human-collected vision and touch.\nIn NeurIPS\nDatasets and Benchmarks Track, 2022. 1, 3, 16, 17\n[77] Wenzhen Yuan, Siyuan Dong, and Edward H Adelson. Gel-\nsight: High-resolution robot tactile sensors for estimating ge-\nometry and force. Sensors, 2017. 1, 3\n[78] Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, and\nMartial Hebert. PCN: Point completion network. In 3DV,\n2018. 6, 13\n[79] Zhoutong Zhang, Qiujia Li, Zhengjia Huang, Jiajun Wu,\nJoshua B Tenenbaum, and William T Freeman. Shape and\nmaterial from sound. In NeurIPS, 2017. 1, 3, 5\n[80] Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Von-\ndrick, Josh McDermott, and Antonio Torralba. The sound of\npixels. In ECCV, 2018. 3\n[81] Liangli Zhen, Peng Hu, Xu Wang, and Dezhong Peng. Deep\nsupervised cross-modal retrieval. In CVPR, 2019. 4, 9, 10,\n11\n[82] Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, and\nTamara L Berg. Visual to sound: Generating natural sound\nfor videos in the wild. In CVPR, 2018. 6, 14\n22\n"
  },
  {
    "title": "SafeDiffuser: Safe Planning with Diffusion Probabilistic Models",
    "link": "https://arxiv.org/pdf/2306.00148.pdf",
    "upvote": "1",
    "text": "DRL\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nWei Xiao*, Tsun-Hsuan Wang, Chuang Gan, Daniela Rus\nMassachusetts Institute of Technology (MIT)\nDiffusion model-based approaches have shown promise in data-driven planning, but there are no safety guaran-\ntees, thus making it hard to be applied for safety-critical applications. To address these challenges, we propose\na new method, called SafeDiffuser, to ensure diffusion probabilistic models satisfy specifications by using a\nclass of control barrier functions. The key idea of our approach is to embed the proposed finite-time diffusion\ninvariance into the denoising diffusion procedure, which enables trustworthy diffusion data generation. Moreover,\nwe demonstrate that our finite-time diffusion invariance method through generative models not only maintains\ngeneralization performance but also creates robustness in safe data generation. We test our method on a series of\nsafe planning tasks, including maze path generation, legged robot locomotion, and 3D space manipulation, with\nresults showing the advantages of robustness and guarantees over vanilla diffusion models\u2020.\n1.\nIntroduction\nDiffuser\nSafeDiffuser\n\ud835\udc5d\ud835\udf03(\ud835\udf49\ud835\udc56\u22121|\ud835\udf49\ud835\udc56)\n\ud835\udc5e(\ud835\udf49\ud835\udc56|\ud835\udf49\ud835\udc56\u22121)\n\ud835\udc5d\ud835\udf03(\ud835\udf49\ud835\udc56\u22121|\ud835\udf49\ud835\udc56)\n+ Invariance\n\ud835\udc5e(\ud835\udf49\ud835\udc56|\ud835\udf49\ud835\udc56\u22121)\nSafety \nspecifications\nWith guarantees\nWithout guarantees\n\u2299\n\u2299\n\u2297\n\u2297\nFig. 1: Our proposed SafeDiffuser (lower) generates safe trajectories with guarantees,\nwhile the diffuser (upper) fails (from J to N).\nData-driven approaches have\nreceived increasing attentions\ndue to their representation flex-\nibility. Diffusion models Sohl-\nDickstein et al. (2015) Ho et al.\n(2020) are data-driven genera-\ntive models whose primary ap-\nplications are in image gen-\nerations Dhariwal and Nichol\n(2021) Du et al. (2020b) Sa-\nharia et al. (2022). Recently,\ndiffusion models, termed dif-\nfusers Janner et al. (2022), have\nshown promise in trajectory\nplanning for a variety of robotic tasks. Diffusers enable flexible behavior synthesis that makes it well generalized\nin novel environments.\nDuring inference, the diffuser, conditioned on the current state and objectives, starts from Gaussian noise to\ngenerate clean planning trajectories based on which we can get a control policy. After applying this control\npolicy one step forward, we get a new state and run the diffusion procedure again to get a new planning trajectory.\nThis process is repeated until the objective is achieved. However, one big challenge in this method is that there\nare no safety guarantees. For instance, the planning trajectory could easily violate safety constraints in the\nmaze (as shown in Fig. 1). This shortcoming demands a fundamental fix to diffusion models to ensure the\nsafe generation of planning trajectories in safety-critical applications such as trustworthy policy learning and\noptimization.\n*Correspondence E-mail: weixy@mit.edu\n\u2020Videos can be found in the anonymous website: https://safediffuser.github.io/safediffuser/\narXiv:2306.00148v1  [cs.LG]  31 May 2023\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nIn this paper, we propose to ensure diffusion models with specification guarantees using finite-time diffusion\ninvariance. An invariance set is a form of specification mainly consisting of safety constraints in planning\ntasks. We ensure that diffusion models are invariant to uncertainties in diffusion procedure. We achieve safety\nby combining receding horizon control with stable diffusion. In receding horizon control, we compute safe\npaths incrementally. The key insight is to replace each path computation with diffusion-based path generation,\nallowing a broader exploration of the path space and makes it relatively easy to include additional constraints.\nThe computed path is combined with simulation to validate that it can be safely actuated.\nTo ensure diffusers with specifications guarantees, we first find diffusion dynamics for the denoising diffusion\nprocedure. Then, we use Lyapunov-based methods with forward invariance properties such as a class of control\nbarrier functions (CBFs) Ames et al. (2017) Glotfelter et al. (2017) Nguyen and Sreenath (2016) Xiao and Belta\n(2019), to formally guarantee the satisfaction of specifications at the end of the diffusion procedure. CBFs works\nwell in planning time using robot dynamics. However, doing this in diffusion models poses extra challenges\nsince the generated data is not directly associated with robot dynamics which makes the use of CBFs non-trivial.\nAs oppose to existing literature, 1. we propose to embed invariance into the diffusion time for diffusers. Thus,\nfinite-time invariance is required in diffusers since specifications are usually violated as the trajectory is initially\nGaussian noise. 2. We propose to add diffusion time components in invariance to address local trap problems\nthat are prominent in planning. 3. We propose a quadratic program approach to incorporate finite-time diffusion\ninvariance into the diffusion to maximally preserve the performance.\nIn summary, we make the following new contributions:\n\u2022We propose formal guarantees for diffusion probabilistic models via control-theoretic invariance.\n\u2022We propose a novel notion of finite-time diffusion invariance, and use a class of CBFs to incorporate it into the\ndiffusion time of the procedure. We proposed three different safe diffusers, and show how we may address the\nlocal trap problem from specifications that are prominent in planning tasks.\n\u2022We demonstrate the effectiveness of our method on a variety of planning tasks using diffusion models, including\nsafe planning in maze, robot locomotion and manipulation.\n2.\nPreliminaries\nIn this section, we provide background on diffusion models and forward invariance in control theory.\nDiffusion Probabilistic Models. Diffusion probabilistic models Sohl-Dickstein et al. (2015) Ho et al. (2020)\nJanner et al. (2022) are latent variable models representing a data generation process as an iterative denoising\nprocedure p\u03b8(\u03c4i\u22121|\u03c4i), i \u2208 {1,...,N}, where \u03c41,...,\u03c4N are latent variables of the same dimensionality of the\nclean (noiseless) data \u03c40 \u223c q(\u03c40), and N is the total denoising steps. This denoising procedure is the reverse of\na forward diffusion process q(\u03c4i|\u03c4i\u22121) that gradually corrupts the clean data by adding noise. The denoising\ndata generation is denoted by\np\u03b8(\u03c40) =\nZ\np\u03b8(\u03c40:N)d\u03c41:N =\nZ\np(\u03c4N)\nN\n\u220f\ni=1\np\u03b8(\u03c4i\u22121|\u03c4i)d\u03c41:N,\n(1)\nwhere p(\u03c4N) is a standard Gaussian prior distribution, and the joint distribution p\u03b8(\u03c40:N) is defined as a Markov\nchain with learned Gaussian transitions starting at p(\u03c4N). The parameter \u03b8 is optimized by minimizing the\nusual variational bound on the negative log-likelihood of the reverse process: \u03b8 \u2217 = argmin\u03b8 E\u03c40\n\u0002\n\u2212log p\u03b8(\u03c40)\n\u0003\n.\nThe forward diffusion process q(\u03c4i|\u03c4i\u22121) is usually prespecified. The reverse process is often parameterized as\nGaussian with time-dependent mean and variance.\n2\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nNotations. There are two \u201ctimes\u201d involved in the paper: that of the diffusion process and that of the planning\nhorizon. We use superscripts (i when unspecified) to denote the diffusion time of a trajectory (state) and\nsubscripts (k when unspecified) to denote the planning time of a state on the trajectory. For instance, \u03c40 denotes\nthe planning trajectory at denoising diffusion time step 0 (i.e., a noiseless trajectory), and x0\nk denotes the state on\nthe trajectory at planning time step k during the denoising diffusion time step 0 (i.e., a noiseless state). We note\nit as xk = x0\nk (\u03c4 = \u03c40 as well) when there is no ambiguity. Further, a trajectory \u03c4i is defined as a planning-time\nsequence of discretized states, i.e., \u03c4i = (xi\n0,xi\n1,...,xi\nk,...,xi\nH), where H \u2208 N is the planning horizon. During\nthe denoising diffusion procedure, the diffusion time changes from N to 0, while the planning time varies from 0\nto H.\nForward Invariance in Control Theory. Consider an affine control system of the form:\n\u02d9xt = f (xt) + g(xt)ut\n(2)\nwhere xt \u2208 Rn, f : Rn \u2192 Rn and g : Rn \u2192 Rn\u00d7q are locally Lipschitz, and ut \u2208 U \u2282 Rq, where U denotes a\ncontrol constraint set. \u02d9xt denotes the (planning) time derivative of state xt.\nDefinition 1. (Set invariance): A set C \u2282 Rn is forward invariant for system (2) if its solutions for some u \u2208 U\nstarting at any x0 \u2208 C satisfy xt \u2208 C, \u2200t \u2265 0.\nDefinition 2. (Extended class K function Khalil (2002)): A Lipschitz continuous function \u03b1 : [\u2212b,a) \u2192\n(\u2212\u221e,\u221e),b > 0,a > 0 belongs to extended class K if it is strictly increasing and \u03b1(0) = 0.\nConsider a safety constraint b(xt) \u2265 0 for system (2), where b : Rn \u2192 R is continuously differentiable, we\ndefine a safe set in the form: C := {xt \u2208 Rn : b(xt) \u2265 0}.\nDefinition 3. (Control Barrier Function (CBF) Ames et al. (2017)): A function b : Rn \u2192 R is a CBF if there\nexists an extended class K function \u03b1 such that\nsup\nut\u2208U\n[L f b(xt) + [Lgb(xt)]ut + \u03b1(b(xt))] \u2265 0,\n(3)\nfor all xt \u2208 C. Lf and Lg denote Lie derivatives w.r.t. x along f and g, respectively.\nTheorem 1 (Ames et al. (2017)). Given a CBF b(xt) from Def. 3, if x0 \u2208 C, then any Lipschitz continuous\ncontroller ut that satisfies the constraint in (3), \u2200t \u2265 0 renders C forward invariant for system (2).\nIf we need to differentiate b(xt) more than once along the dynamics (2) until the control ut explicitly shows, we\nuse a high-order CBF Nguyen and Sreenath (2016) Xiao and Belta (2019) as a general form of CBF to guarantee\nsafety for (2). In this work, we map the forward invariance in control theory to finite time diffusion invariance in\ndiffusion models, where we incorporate CBFs into the diffusion time as opposed to their regular applications in\nplanning time. In addition, we show how we may address the local traps during diffusion.\n3.\nSafe Diffuser\nIn this section, we propose three different safe diffusers to ensure the safe generation of data in diffusion, i.e., to\nensure the satisfaction of specifications b(xk) \u2265 0,\u2200k \u2208 {0,...,H}. Each of the proposed safe diffusers have\ntheir own flexibility, such as avoiding local traps in planning. We consider discretized system states in the sequel.\nSafety in continuous planning time can be guaranteed using a lower hierarchical control framework employing\nother CBFs, as in Ames et al. (2017); Nguyen and Sreenath (2016); Xiao and Belta (2019).\n3\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nIn the denoising diffusion procedure, since the learned Gaussian transitions starts at p(xN) \u223c N (0, I), it is\nhighly likely that specifications are initially violated, i.e., \u2203k \u2208 {0,...,H},b(xN\nk ) < 0. For safe data generation,\nwe wish to have b(x0\nk) \u2265 0(i.e., b(xk) \u2265 0),\u2200k \u2208 {0,...,H}. Since the maximum denoising diffusion step N is\nlimited, this needs to be guaranteed in a finite diffusion time step. Therefore, we propose the finite-time diffusion\ninvariance of the diffusion procedure as follows:\nDefinition 4 (Finite-time Diffusion Invariance). If there exists i \u2208 {0,...,N} such that b(x j\nk) \u2265 0,\u2200k \u2208 {0,...,H},\u2200 j \u2264\ni, then a denoising diffusion procedure p\u03b8(\u03c4i\u22121|\u03c4i),i \u2208 {1,...,N} with respect to a specification b(xk) \u2265 0,\u2200k \u2208\n{0,...,H} is finite-time diffusion invariant.\nThe above definition can be interpreted as that if b(xN\nk ) \u2265 0,k \u2208 {0,...,H}, then we require b(xi\nk) \u2265 0,\u2200i \u2208\n{0,...,N} (similar to the forward invariance definition as in Def. 1); otherwise, we require that b(x j\nk) \u2265 0,\u2200 j \u2208\n{0,...,i},i \u2208 {0,...,N}, where i is a finite diffusion time.\nIn the following, we propose three different methods to achieve finite-time diffusion invariance. The first method\nis a general form of the safe- diffuser, and the other two are variants to address local traps in planning.\n3.1.\nRobust-Safe Diffuser\nObstacle\n\ud835\udc5d\ud835\udf03(\ud835\udf49\ud835\udc57|\ud835\udf49\ud835\udc57+1)\nStep \ud835\udc57+1\nStep \ud835\udc57\ncontrol\nSafe Planning\n\ud835\udf49\ud835\udc57\nQP: \nInvariance\n\ud835\udf49\ud835\udc57\u2217\nDynamics\n\ud835\udf49\ud835\udc57+1\n\ud835\udf490\u2217\nSafeDiffuser step \ud835\udc41\n(Gaussian)\n\ud835\udf49\ud835\udc41\nFig. 2: The proposed SafeDiffuser workflow. SafeDiffuser\nperforms an additional step of invariance QP solver in the\ndiffusion dynamics to ensure safety. The final control signal\nis inferred from safe planning trajectories.\nThe safe denoising diffusion procedure is considered\nat every diffusion step. Following (1), the data gener-\nation at the diffusion time j \u2208 {0,...,N \u22121} is given\nby:\np\u03b8(\u03c4 j) =\nZ\np(\u03c4N)\nN\n\u220f\ni= j+1\np\u03b8(\u03c4i\u22121|\u03c4i)d\u03c4 j+1:N\n(4)\nA sample \u03c4 j, j \u2208 {0,...,N \u22121} follows the data dis-\ntribution in (4), i.e., we have\n\u03c4 j \u223c p\u03b8(\u03c4 j).\n(5)\nThe denoising diffusion dynamics are then given by:\n\u02d9\u03c4 j = lim\n\u2206\u03c4\u21920\n\u03c4 j \u2212 \u03c4 j+1\n\u2206\u03c4\n(6)\nwhere \u02d9\u03c4 is the (diffusion) time derivative of \u03c4. \u2206\u03c4 > 0 is a small enough diffusion time step length during\nimplementations, and \u03c4 j+1 is available from the last diffusion step.\nIn order to impose finite-time diffusion invariance on the diffusion procedure, we wish to make diffusion\ndynamics (6) controllable. We reformulate (6) as\n\u02d9\u03c4 j = u j,\n(7)\nwhere u j is a control variable of the same dimensionality as \u03c4 j. On the other hand, we wish u j to stay close to\n\u03c4 j\u2212\u03c4 j+1\n\u2206\u03c4\nin order to maximally preserve the performance of the diffusion model. The above model can be rewritten\nin terms of each state on the trajectory \u03c4 j: \u02d9x j\nk = u j\nk, where u j\nk is the kth component of u j. Then, we can define\nCBFs to ensure the satisfaction of b(x j\nk) \u2265 0 (in finite diffusion time): h(u j\nk|x j\nk) := db(x j\nk)\ndx j\nk u j\nk +\u03b1(b(x j\nk)) \u2265 0,k \u2208\n{0,...,H}, j \u2208 {0,...,N \u22121}, where \u03b1(\u00b7) is an extended class K function. We have the following theorem to\nshow the finite-time diffusion invariance (proof is given in appendix):\n4\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nTheorem 2. Let the diffusion dynamics defined as in (6) whose controllable form is defined as in (7). If there\nexists an extended class K function \u03b1 such that\nh(u j\nk|x j\nk) \u2265 0,\u2200k \u2208 {0,...,H},\u2200 j \u2208 {0,...,N \u22121},\n(8)\nwhere h(u j\nk|x j\nk) = db(x j\nk)\ndx j\nk u j\nk + \u03b1(b(x j\nk)), then the diffusion procedure p\u03b8(\u03c4i\u22121|\u03c4i),i \u2208 {1,...,N} is finite-time\ndiffusion invariant with almost probability 1.\nOne possible issue in the robust-safe diffusion procedure is that if b(x j\nk) \u2265 0 when j is close to the initial\ndiffusion step N, then the state x j\nk can never violate the specification after diffusion step j < N. When there are\nlocal traps from specifications, the state x j\nk may get stuck there during the denoising diffusion process, which\nmay adversely affect the performance. In order to address this issue, we propose relaxed-safe diffuser and\ntime-varying-safe diffuser in the following subsections.\n3.2.\nRelaxed-Safe Diffuser\nIn order to address the local trap problems imposed by specifications during the denoising diffusion procedure,\nwe propose a variation of the robust-safe diffuser. We define the diffusion dynamics and their controllable form\nas in (6) - (7). The modified versions for CBFs are in the form:\nh(u j\nk,r j\nk|x j\nk) := db(x j\nk)\ndx j\nk\nu j\nk + \u03b1(b(x j\nk)) \u2212wk( j)r j\nk \u2265 0,k \u2208 {0,...,H}, j \u2208 {0,...,N \u22121},\n(9)\nwhere r j\nk \u2208 R is a relaxation variable that is to be determined (shown in the next section). wk( j) \u2265 0 is a diffusion\ntime-varying weight on the relaxation variable such that it gradually decrease to 0 as j \u2192 0.\nWhen wk( j) decreases to 0, the condition (9) becomes a hard constraint. One problem in such cases is that\nthe diffusion performance may be adversely affected by such a hard constraint. In order to address this issue,\nwe may run additional Na \u2208 N diffusion steps, while setting the diffusion time to 0 when j < 0 in the reverse\nprocess. The corresponding CBF conditions to (9) are to change the domain of j to j \u2208 {\u2212Na,...,N \u22121}.We\nalso have the following theorem to show the finite-time diffusion invariance (proof is given in appendix):\nTheorem 3. Let the diffusion dynamics defined as in (6) whose controllable form is defined as in (7). If there\nexist an extended class K function \u03b1, a large enough extra diffusion step Na \u2208 N, and a time-varying weight\nwk( j) where wk( j) = 0 for all j \u2264 0 such that\nh(u j\nk,r j\nk|x j\nk) \u2265 0,\u2200k \u2208 {0,...,H},\u2200 j \u2208 {\u2212Na,...,N \u22121},\n(10)\nwhere h(u j\nk,r j\nk|x j\nk) = db(x j\nk)\ndx j\nk u j\nk +\u03b1(b(x j\nk))\u2212wk( j)r j\nk, then the diffusion procedure p\u03b8(\u03c4i\u22121|\u03c4i),i \u2208 {\u2212Na,...,N}\nis finite-time diffusion invariant with almost probability 1.\nAfter the denoising diffusion procedure is done at step \u2212Na, we would set the \u03c4\u2212Na as the output data of the\ndiffusion model, i.e., \u03c40 = \u03c4\u2212Na.\n3.3.\nTime-Varying-Safe Diffuser\nAs an alternative to the relaxed-safe diffuser, we propose another safe diffuser called time-varying-safe diffuser\nin this subsection. The proposed time-varying-safe diffuser can also address the local trap issues induced by\nspecifications.\n5\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nIn this case, we directly modify the specification b(x j\nk) \u2265 0 by a diffusion time-varying function \u03b3k : j \u2192 R in\nthe form:\nb(x j\nk) \u2212\u03b3k( j) \u2265 0,k \u2208 {0,...,H}, j \u2208 {0,...,N},\n(11)\nwhere \u03b3k( j) is continuously differentiable, and is defined such that \u03b3k(N) \u2264 b(xN\nk ) and \u03b3k(0) = 0.\nThe modified time-varying specification can then be enforced using CBFs: h(u j\nk|x j\nk,\u03b3k( j)) := db(x j\nk)\ndx j\nk u j\nk \u2212 \u02d9\u03b3k( j)+\n\u03b1(b(x j\nk) \u2212\u03b3k( j)) \u2265 0,k \u2208 {0,...,H}, j \u2208 {0,...,N \u22121}, where \u02d9\u03b3k( j) is the diffusion time derivative of \u03b3k( j).\nFinally, we have the following theorem to show the finite-time diffusion invariance (proof is given in appendix):\nTheorem 4. Let the diffusion dynamics defined as in (6) whose controllable form is defined as in (7). If there\nexist an extended class K function \u03b1 and a time-varying function \u03b3k( j) where \u03b3k(N) \u2264 b(xN\nk ) and \u03b3k(0) = 0\nsuch that\nh(u j\nk|x j\nk,\u03b3k( j)) \u2265 0,\u2200k \u2208 {0,...,H},\u2200 j \u2208 {0,...,N \u22121},\n(12)\nwhere h(u j\nk|x j\nk,\u03b3k( j)) = db(x j\nk)\ndx j\nk u j\nk \u2212 \u02d9\u03b3k( j) + \u03b1(b(x j\nk) \u2212 \u03b3k( j)), then the diffusion procedure p\u03b8(\u03c4i\u22121|\u03c4i),i \u2208\n{0,...,N} is finite-time diffusion invariant.\n4.\nEnforcing Invariance in Diffuser\nIn this section, we show how we may incorporate the three proposed invariance methods from the last section into\ndiffusion models. Enforcing the finite-time invariance in diffusion models is equivalent to ensure the satisfaction\nof the conditions in Thms. 2-4 in the diffusion procedure. In this section, we propose a minimum-deviation\nquadratic program (QP) approach to achieve that. We wish to enforce these conditions at every step of the\ndiffusion (as shown in Fig. 2) as those states that are far from the specification boundaries b(x j\nk) = 0 can also be\noptimized accordingly, and thus, the model may generate more coherent trajectories.\nEnforcing Invariance for Robust-Safe (RoS) and Time-Varying-Safe Diffusers. During implementation, the\ndiffusion time step length \u2206\u03c4 in (6) is chosen to be small enough, and we wish the control u j to stay close to the\nright-hand side of (6). Thus, we can formulate the following QP-based optimization to find the optimal control\nfor u j that satisfies the condition in Thms. 2 or 4:\nu j\u2217 = argmin\nu j ||u j \u2212 \u03c4 j \u2212 \u03c4 j+1\n\u2206\u03c4\n||2, s.t., (8) if RoS diffuser else s.t., (12),\n(13)\nwhere ||\u00b7|| denotes the 2-norm of a vector. If we have more than one specification, we can add the corresponding\nconditions in Thm. 2 for each of them to the above QP. After we solve the above QP and get u j\u2217, we update (7)\nby setting u j = u j\u2217 within the time step and get a new state for the diffusion procedure. Note that all of these\nhappen at the end of each diffusion step.\nEnforcing Invariance for Relaxed-Safe Diffuser. In this case, since we have relaxation variables for each of\nthe safety specification, we wish to minimize these relaxations in the cost function to drive all the state towards\nthe satisfaction of specifications. In other words, we have the following QP:\nu j\u2217,r j\u2217 = argmin\nu j,r j ||u j \u2212 \u03c4 j \u2212 \u03c4 j+1\n\u2206\u03c4\n||2 + ||r j||2, s.t., (10),\n(14)\nwhere r j is the concatenation of r j\nk for all k \u2208 {0,...,H}. As an alternative, all the constraints above may share\nthe same relaxation variable, i.e., the dimension of r j is only one. After we solve the above QP and get u j\u2217, we\nupdate (7) by setting u j = u j\u2217 within the time step and get a new state.\n6\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nAlgorithm 1 Enforcing invariance in diffusion models\nInput: the last trajectory of diffusion \u03c4 j+1 at diffusion step j \u2208 {0,...,N}\nOutput: safe diffusion state \u03c4 j\u2217.\n(a) Run diffusion procedure (4) and sample (5) as usual at step j and get \u03c4 j.\n(b) Find diffusion dynamics as in (6) - (7).\nif Robust-safe diffuser then\nFormulate the QP (13), solve it and get u j\u2217.\nelse if Relaxed-safe diffuser then\nDefine the time-varying weight wk( j) in (9), formulate the QP (14), solve it and get u j\u2217,r j\u2217.\nelse\nDesign the time-varying function \u03b3k( j) in (11), formulate the QP (13), solve it and get u j\u2217.\nend if\n(c) Update dynamics (7) with u j = u j\u2217 and get \u03c4 j\u2217. Finally, \u03c4 j \u2190 \u03c4 j\u2217.\nFig. 3: Maze planning (blue to red) denoising diffusion procedure with classifier-based guidance (Left to right: diffusion\ntime steps 256, 4, 3, -50, respectively). Red ellipse and super-ellipse (outside) denote safe specifications. The safe\nclassifier-based guidance approach adversely affects the diffusion procedure without guarantees.\nComplexity of enforcing invariance The computational complexity of a QP is O(q3), where q is the dimension\nof the decision variable. When there is a set S of specifications, we just add the corresponding constraints for\neach specification the QP. The complexity of the three proposed safe diffuser are similar.\nThe algorithm for enforcing invariance is straight forward, which includes the construction of proper conditions,\nthe solving of QP, and the update of diffusion state. We summary the algorithm in Alg. 1.\n5.\nExperiments\nWe set up experiments to answer the following questions:\n\u2022 Does our method match the theoretical potential in various tasks quantitatively and qualitatively?\n\u2022 How does our method compare with state-of-the-art approaches in enforcing safety specifications?\n\u2022 How does our proposed method affect the performance of diffusion under guaranteed specifications?\n5.1.\nSafe Planning in Maze\nIn this experiment, we aim to impose trajectory constraints on the planning path of a maze. The training data is\npublicly available from Janner et al. (2022), in which initial positions and destinations in maze are randomly\ngenerated. The diffusion model is conditioned on the initial positions and destinations.\n7\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nTable 1: Maze safe planning comparisons with benchmarks. Items are short for satisfaction of simple specifications\n(S-SPEC) and complex specifications (C-SPEC), score of planning tasks (SCORE), computation time at each diffusion\nstep (TIME) in seconds, respectively. In the method column, items are short for robust-safe diffuser (RoS-DIFFUSER),\nrelaxed-safe diffuser (ReS-DIFFUSER), and time-varying-safe diffuser (TVS-DIFFUSER), respectively. The classifier\nguidance\u2212\u03b5 method applies (safe) gradient to the model when the state is \u03b5 > 0 close to the boundary.\nMETHOD\nS-\nSPEC(\u2191\n& \u2265 0)\nC-\nSPEC(\u2191\n& \u2265 0)\nSCORE (\u2191)\nTIME\nDIFFUSER (BASELINE) JANNER\nET AL. (2022)\n-0.983\n-0.894\n1.016\u00b10.712\n0.007\nTRUNCATE BROCKMAN ET AL.\n(2016)\n\u22121.192e\u22127\n-0.759\n0.754\u00b10.779\n0.024\nCLASSIFIER GUIDANCE\nDHARIWAL AND NICHOL (2021)\n-0.789\n-0.979\n0.502\u00b10.328\n0.053\nCLASSIFIER GUIDANCE\u2212\u03b5\nDHARIWAL AND NICHOL (2021)\n-0.853\n-0.995\n0.470\u00b10.366\n0.061\nROS-DIFFUSER (OURS)\n\u22122.384e\u22127 \u22125.960e\u22127 0.770\u00b10.782\n0.106\nRES-DIFFUSER (OURS)\n\u22122.384e\u22127 \u22124.768e\u22127 0.762\u00b10.746\n0.107\nTVS-DIFFUSER (OURS)\n\u22122.384e\u22127 \u22125.364e\u22127 0.806\u00b10.783\n0.107\nThe diffuser cannot guarantee the satisfaction of any specifications, as shown in Fig. 1. When using classifier-\nbased guidance in diffusion for safety specifications, the performance could be significantly affected (Fig. 3). As\na result, the generated trajectory will largely deviates from the desired one with no safety. The proposed robust-\nsafe diffuser (RoS-diffuser), relaxed-safe diffuser (ReS-diffuser), and time-varying-safe diffuser (TVS-diffuser)\ncan all guarantee the satisfaction of specifications, even when the specifications are complex (as long as they are\ndifferentiable), as shown in Table 1. The satisfaction scores of the proposed methods are not strictly positive,\nand this may be due to the computation errors or inter-sampling effect as even the truncation method cannot\nstrictly satisfy the simple specifications. The proposed methods can also maximally preserve the performance of\ndiffusion models, as shown by the score in Table 1, as well as in Fig. 4. The ReS-diffuser and TVS-diffuser can\naddress the local trap problem from specifications, as shown by figures in appendix.\n5.2.\nSafe Planning for Robot Locomotion\nFor robot locomotion (in MuJoCo), we wish the robot to avoid collisions with obstacles, such as the roof. In this\ncase, since there is no local trap problem, we only consider robust-safe diffuser (RoS-diffuser). Others work\nsimilarly. The training data set is publicly available from Janner et al. (2022).\nAs expected, collisions with roof are very likely to happen in walker and hopper using the diffuser since there\nare no guarantees, as shown in Table 2. The truncation method can work for simple specifications (S-spec),\nbut not for complex specifications (C-spec). The classifier-based guidance can improve the satisfaction of\nspecifications, but with no guarantees. Collision-free is guaranteed using the proposed Ros-diffuser, and one\nexample of diffusion procedure is shown in Fig. 5.\n8\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nFig. 4: Maze planning (blue to red) denoising diffusion procedure with the proposed time-varying safe diffuser (Left to\nright: diffusion time steps 256, 4, 3, -50, respectively). Red ellipse and super-ellipse (outside) denote safe specifications.\nThe proposed time-varying safe diffuser can guarantee specifications at the end of diffusion while not significantly affecting\nthe diffusion procedure.\n\u2193\n\u2193\nFig. 5: Walker2D planning denoising diffusion procedure with the proposed robust-safe diffuser (Up to down: diffusion\ntime steps 20, 10, 0, respectively). The red line denotes the roof the walker needs to safely avoid during locomotion (safety\nspecifications). Safety is violated at step 20 since the trajectory is initially Gaussian noise, but is eventually guaranteed\n(step 0).\n5.3.\nSafe Planning for Manipulation\nFor manipulation (in Pybullet), the diffusion models generate joint trajectories (as controls) for the robot, which\nare conditioned on the locations of the objects to grasp and place. The training data set is publicly available from\nJanner et al. (2022). Specifications are joint limitations to avoid collision in joint space.\nIn this case, the truncation method still fails to work for complex specifications (speed-dependent joint lim-\nitations). Our proposed RoS-diffuser can work for all specifications as long as they are differentiable. An\ninteresting observation is that the proposed RoS-diffuser can even improve the performance (reward) of diffusion\nmodels in this case, as shown in Table 3. This may be due to the fact that the satisfaction of joint limitations can\navoid collision in the joint space of the robot as Pybullet is a physical simulator. The computation time of the\nproposed RoS-diffuser is comparable to other methods. An illustration for the safe diffusion and manipulation\nprocedure is shown in Fig. 6.\n9\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nTable 2: Robot safe planning comparisons with benchmarks. Items are short for satisfaction of simple specifications\n(S-SPEC), satisfaction of complex specifications (C-SPEC), score of planning tasks (SCORE), computation time at each\ndiffusion step (TIME) in seconds, respectively.\nEXPERIMENT\nMETHOD\nS-\nSPEC(\u2191\n& \u2265 0)\nC-\nSPEC(\u2191\n& \u2265 0)\nSCORE (\u2191)\nTIME\nDIFFUSER (BASELINE) JANNER\nET AL. (2022)\n-\n9.375\n-4.891\n0.346\u00b10.106\n0.037\nWALKER2D TRUNCATE BROCKMAN ET AL.\n(2016)\n0.0\n\u00d7\n0.286\u00b10.180\n0.105\nCLASSIFIER GUIDANCE\nDHARIWAL AND NICHOL (2021)\n-\n0.575\n-0.326\n0.208\u00b10.140\n0.053\nROS-DIFFUSER (OURS)\n0.0\n\u22126.706e\u22128 0.312\u00b10.782\n0.183\nDIFFUSER (BASELINE) JANNER\nET AL. (2022)\n-\n2.180\n-1.862\n0.455\u00b10.038\n0.038\nHOPPER\nTRUNCATE BROCKMAN ET AL.\n(2016)\n0.0\n\u00d7\n0.436\u00b10.067\n0.046\nCLASSIFIER GUIDANCE\nDHARIWAL AND NICHOL (2021)\n-\n0.894\n-0.524\n0.478\u00b10.038\n0.047\nROS-DIFFUSER (OURS)\n0.0\n\u22125.960e\u22128 0.430\u00b10.040\n0.170\n6.\nRelated Works\nDiffusion models and planning Diffusion models Sohl-Dickstein et al. (2015) Ho et al. (2020) are data-driven\ngenerative modeling tools, widely used in applications to image generations Dhariwal and Nichol (2021) Du\net al. (2020b), in planning Hafner et al. (2019) Janner et al. (2021) Ozair et al. (2021) Janner et al. (2022), and in\nlanguage Saharia et al. (2022) Liu et al. (2023). Generative models are combined with reinforcement learning\nto explore dynamic models in the form of convolutional U-networks Kaiser et al. (2019), stochastic recurrent\nnetworks Ke et al. (2019), neural ODEs Du et al. (2020a), generative adversarial networks Eysenbach et al.\n(2022), neural radiance fields Li et al. (2022), and transformers Chen et al. (2022). Further, planning tasks are\nbecoming increasingly important for diffusion models Lambert et al. (2021) Ozair et al. (2021) Janner et al.\n(2022) as they can generalize well in all kinds of robotic problems. However, there are no methods to equip\ndiffusion models with specification guarantees, which is especially important for safety-critical applications.\nHere, we address this issue using the proposed finite-time diffusion invariance.\nSet invariance and CBFs. An invariant set has been widely used to represent the safe behavior of dynamical\nsystems Preindl (2016) Rakovic et al. (2005) Ames et al. (2017) Glotfelter et al. (2017) Xiao et al. (2023a). In\nthe state of the art of control, Control Barrier Functions (CBFs) are also widely used to prove set invariance\nAubin (2009), Prajna et al. (2007), Wisniewski and Sloth (2013). CBFs can be traced back to optimization\nproblems Boyd and Vandenberghe (2004), and are Lyapunov-like functions Wieland and Allg\u00f6wer (2007). For\ntime-varying systems, CBFs can also be adapted accordingly Lindemann and Dimarogonas (2018). Existing\nCBF approaches are usually applied in planning time since they are closely coupled with system dynamics.\nThere are few studies of CBFs in other space, such as the diffusion time horizon in diffusion models. Our work\n10\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nTable 3: Manipulation safe planning comparisons with benchmarks. Items are short for satisfaction of simple specifications\n(S-SPEC), satisfaction of complex specifications (C-SPEC), reward of planning tasks (REWARD), computation time at\neach diffusion step (TIME) in seconds, respectively. In the method column, the item is short for robust-safe diffuser\n(RoS-DIFFUSER).\nMETHOD\nS-\nSPEC(\u2191\n& \u2265 0)\nC-\nSPEC(\u2191\n& \u2265 0)\nREWARD\n(\u2191)\nTIME\nDIFFUSER (BASELINE) JANNER\nET AL. (2022)\n-0.057\n-0.065\n0.650\u00b10.107\n0.038\nTRUNCATE BROCKMAN ET AL.\n(2016)\n1.631e\u22128\n\u00d7\n0.575\u00b10.112\n0.069\nCLASSIFIER GUIDANCE\nDHARIWAL AND NICHOL (2021)\n-0.050\n-0.053\n0.800\u00b10.328\n0.075\nROS-DIFFUSER (OURS)\n0.072\n0.069\n0.925\u00b10.107\n0.088\nFig. 6: Manipulation planning denoising diffusion procedure with the proposed robust-safe diffuser (Left to right: diffusion\ntime steps 1000, 100, 0, and execution time step 100, respectively). The red dots denote the planning trajectory of the\nend-effector.\naddresses all these limitations.\nGuarantees in neural networks. Differentiable optimization methods show promise for neural network\ncontrollers with guarantees Amos et al. (2018); Pereira et al. (2020); Xiao et al. (2023a). They are usually served\nas a layer (filter) in the neural networks. In Amos and Kolter (2017), a differentiable quadratic program (QP)\nlayer, called OptNet, was introduced. OptNet with CBFs has been used in neural networks as a filter for safe\ncontrols Pereira et al. (2020), in which CBFs are not trainable, thus, potentially limiting the system\u2019s learning\nperformance. In Deshmukh et al. (2019); Ferlez et al. (2020); Zhao et al. (2021), safety guaranteed neural\nnetwork controllers have been learned through verification-in-the-loop training. The verification approaches\ncannot ensure coverage of the entire state space. More recently, CBFs have been incorporated into neural ODEs\nto equip them with specification guarantees Xiao et al. (2023b). However, none of these methods can be applied\nin diffusion models, which we address in this paper.\n11\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\n7.\nConclusions, Discussions and Future Work\nWe have proposed finite-time diffusion invariance for diffusion models to ensure safe planning for safety-critical\napplications. We have demonstrated the effectiveness of our method on a series of robotic planning tasks.\nNonetheless, our method face a few shortcomings motivating for future work.\nSpecifically, specifications for diffusion models should be expressed as continuously differentiable constraints\nthat may be unknown for planning tasks. Further work may explore how to learn specifications from history\ntrajectory data. There is also a gap between planning and control using diffusion models. We may further\ninvestigate diffusion for safe control policies when robot dynamics are known or to be learned.\nBroader Impact. Our proposed finite-time diffusion invariance can be applied to guarantees in other tasks,\nsuch as image generations, in addition to planning. In such cases, we can ensure the generation of desired\npatterns/objects, such as a cat in the generated images. We will further investigate those directions in our future\nwork.\n8.\nAcknowledgement\nThe research was supported in part by Capgemini Engineering. It was also partially sponsored by the United\nStates Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was\naccomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained\nin this document are those of the authors and should not be interpreted as representing the official policies, either\nexpressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized\nto reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.\nThis research was also supported in part by the AI2050 program at Schmidt Futures (Grant G- 965 22-63172).\nReferences\nA. D. Ames, X. Xu, J. W. Grizzle, and P. Tabuada. Control barrier function based quadratic programs for safety\ncritical systems. IEEE Transactions on Automatic Control, 62(8):3861\u20133876, 2017.\nB. Amos and J. Z. Kolter. Optnet: Differentiable optimization as a layer in neural networks. In Proceedings of\nthe 34th International Conference on Machine Learning - Volume 70, pages 136\u2013145, 2017.\nB. Amos, I. D. J. Rodriguez, J. Sacks, B. Boots, and J. Z. Kolter. Differentiable mpc for end-to-end planning and\ncontrol. In Proceedings of the 32nd International Conference on Neural Information Processing Systems,\npage 8299\u20138310. Curran Associates Inc., 2018.\nJ.-P. Aubin. Viability theory. Springer, 2009.\nS. P. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, New York, 2004.\nG. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym, 2016.\nC. Chen, Y.-F. Wu, J. Yoon, and S. Ahn. Transdreamer: Reinforcement learning with transformer world models.\narXiv preprint arXiv:2202.09481, 2022.\n12\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nJ. V. Deshmukh, J. P. Kapinski, T. Yamaguchi, and D. Prokhorov. Learning deep neural network controllers for\ndynamical systems with safety guarantees: Invited paper. In 2019 IEEE/ACM International Conference on\nComputer-Aided Design (ICCAD), pages 1\u20137, 2019.\nP. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information\nProcessing Systems, 34:8780\u20138794, 2021.\nJ. Du, J. Futoma, and F. Doshi-Velez. Model-based reinforcement learning for semi-markov decision processes\nwith neural odes. Advances in Neural Information Processing Systems, 33:19805\u201319816, 2020a.\nY. Du, S. Li, and I. Mordatch. Compositional visual generation with energy based models. Advances in Neural\nInformation Processing Systems, 33:6637\u20136647, 2020b.\nB. Eysenbach, A. Khazatsky, S. Levine, and R. R. Salakhutdinov. Mismatched no more: Joint model-policy\noptimization for model-based rl. Advances in Neural Information Processing Systems, 35:23230\u201323243, 2022.\nJ. Ferlez, M. Elnaggar, Y. Shoukry, and C. Fleming. Shieldnn: A provably safe nn filter for unsafe nn controllers.\npreprint arXiv:2006.09564, 2020.\nP. Glotfelter, J. Cortes, and M. Egerstedt. Nonsmooth barrier functions with applications to multi-robot systems.\nIEEE control systems letters, 1(2):310\u2013315, 2017.\nD. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent dynamics for\nplanning from pixels. In International conference on machine learning, pages 2555\u20132565. PMLR, 2019.\nJ. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020.\nM. Janner, Q. Li, and S. Levine. Offline reinforcement learning as one big sequence modeling problem. Advances\nin neural information processing systems, 34:1273\u20131286, 2021.\nM. Janner, Y. Du, J. Tenenbaum, and S. Levine. Planning with diffusion for flexible behavior synthesis. In\nInternational Conference on Machine Learning, pages 9902\u20139915. PMLR, 2022.\nL. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn, P. Koza-\nkowski, S. Levine, et al. Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374,\n2019.\nN. R. Ke, A. Singh, A. Touati, A. Goyal, Y. Bengio, D. Parikh, and D. Batra. Modeling the long term future in\nmodel-based reinforcement learning. In International Conference on Learning Representations, 2019.\nH. K. Khalil. Nonlinear Systems. Prentice Hall, third edition, 2002.\nN. Lambert, A. Wilcox, H. Zhang, K. S. Pister, and R. Calandra. Learning accurate long-term dynamics for\nmodel-based reinforcement learning. In 2021 60th IEEE Conference on Decision and Control (CDC), pages\n2880\u20132887. IEEE, 2021.\nY. Li, S. Li, V. Sitzmann, P. Agrawal, and A. Torralba. 3d neural scene representations for visuomotor control.\nIn Conference on Robot Learning, pages 112\u2013123. PMLR, 2022.\nL. Lindemann and D. V. Dimarogonas. Control barrier functions for signal temporal logic tasks. In Proc. of 57th\nIEEE Conference on Decision and Control, 2018. to appear.\n13\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nH. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley. Audioldm: Text-to-audio\ngeneration with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023.\nM. Nagumo. \u00dcber die lage der integralkurven gew\u00f6hnlicher differentialgleichungen. In Proceedings of the\nPhysico-Mathematical Society of Japan. 3rd Series. 24:551-559, 1942.\nQ. Nguyen and K. Sreenath. Exponential control barrier functions for enforcing high relative-degree safety-\ncritical constraints. In 2016 American Control Conference (ACC), pages 322\u2013328. IEEE, 2016.\nS. Ozair, Y. Li, A. Razavi, I. Antonoglou, A. Van Den Oord, and O. Vinyals. Vector quantized models for\nplanning. In International Conference on Machine Learning, pages 8302\u20138313. PMLR, 2021.\nM. A. Pereira, Z. Wang, I. Exarchos, and E. A. Theodorou. Safe optimal control using stochastic barrier functions\nand deep forward-backward sdes. In Conference on Robot Learning, 2020.\nS. Prajna, A. Jadbabaie, and G. J. Pappas. A framework for worst-case and stochastic safety verification using\nbarrier certificates. IEEE Transactions on Automatic Control, 52(8):1415\u20131428, 2007.\nM. Preindl. Robust control invariant sets and lyapunov-based mpc for ipm synchronous motor drives. IEEE\nTransactions on Industrial Electronics, 63(6):3925\u20133933, 2016.\nS. V. Rakovic, E. C. Kerrigan, K. I. Kouramas, and D. Q. Mayne. Invariant approximations of the minimal robust\npositively invariant set. IEEE Transactions on automatic control, 50(3):406\u2013410, 2005.\nC. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes,\nB. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language\nunderstanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.\nJ. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR,\n2015.\nP. Wieland and F. Allg\u00f6wer. Constructive safety using control barrier functions. In Proc. of 7th IFAC Symposium\non Nonlinear Control System, 2007.\nR. Wisniewski and C. Sloth. Converse barrier certificate theorem. In Proc. of 52nd IEEE Conference on Decision\nand Control, pages 4713\u20134718, Florence, Italy, 2013.\nW. Xiao and C. Belta. Control barrier functions for systems with high relative degree. In Proc. of 58th IEEE\nConference on Decision and Control, pages 474\u2013479, Nice, France, 2019.\nW. Xiao, T.-H. Wang, R. Hasani, M. Chahine, A. Amini, X. Li, and D. Rus. Barriernet: Differentiable control\nbarrier functions for learning of safe robot control. IEEE Transactions on Robotics, 2023a.\nW. Xiao, T.-H. Wang, R. Hasani, M. Lechner, Y. Ban, C. Gan, and D. Rus. On the forward invariance of neural\nodes. In International conference on machine learning, arXiv preprint arXiv:2210.04763, 2023b.\nH. Zhao, X. Zeng, T. Chen, Z. Liu, and J. Woodcock. Learning safe neural network controllers with barrier\ncertificates. Form Asp Comp, 33:437\u2013455, 2021.\n14\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nS1.\nProofs\nS1.1.\nProof of Theorem 3.2\nProof: Given a continuously differentiable constraint h(xt) \u2265 0 (h(x0) \u2265 0), by Nagumo\u2019s theorem Nagumo\n(1942), the necessary and sufficient condition for the satisfaction of h(xt) \u2265 0,\u2200t \u2265 0 is\n\u02d9h(xt) \u2265 0, when h(xt) = 0,\nIf b(xN\nk ) \u2265 0,k \u2208 {0,...,H}, then the condition (8) is equivalent to\ndb(x j\nk)\ndx j\nk\n\u02d9x j\nk + \u03b1(b(x j\nk)) \u2265 0,\nwhere \u02d9x j\nk is the diffusion time derivative. The last equation is equivalent to\n\u02d9b(x j\nk) + \u03b1(b(x j\nk)) \u2265 0,\nSince \u03b1 is an extended class K function, we have that\n\u03b1(b(x j\nk)) \u2192 0, as b(x j\nk) \u2192 0,\nIn other words, we have \u02d9b(x j\nk) \u2265 0 when b(x j\nk) = 0. Since b(xN\nk ) \u2265 0,k \u2208 {0,...,H}, then by Nagumo\u2019s\ntheorem, we have b(x j\nk) \u2265 0,\u2200 j \u2208 {0,...,N \u22121}. Therefore, the diffusion procedure p\u03b8(\u03c4i\u22121|\u03c4i),i \u2208 {1,...,N}\nis finite-time diffusion invariant, and the finite time in diffusion invariance is N.\nIf, on the other hand, b(xN\nk ) < 0,k \u2208 {0,...,H}, then we can define a Lyapunov function:\nV(x j\nk) = \u2212b(x j\nk),k \u2208 {0,...,H}, j \u2208 {0,...,N},\n(S1)\nand V(xN\nk ) > 0.\nSince \u03b1 is an extended class K function, replacing b(x j\nk) by V(x j\nk), the condition (8) is equivalent to\ndV(x j\nk)\ndx j\nk\n\u02d9x j\nk + \u03b1(V(x j\nk)) \u2264 0,\nwhich is equivalent to\n\u02d9V(x j\nk) + \u03b1(V(x j\nk)) \u2264 0,\nSince \u02d9V(x j\nk) \u2264 \u2212\u03b1(V(x j\nk)) < 0, we have that V(x j\nk) will be stabilized to 0 by Lyapunov stability theory. In\nother words, the state x j\nk will be stabilized to the boundary b(x j\nk) = 0. Specifically, when \u03b1 is a linear function,\nthe last equation can be rewritten as\n\u02d9V(x j\nk) + \u03b5V(x j\nk)) \u2264 0,\n(S2)\nwhere \u03b5 > 0. Suppose we have\n\u02d9V(x j\nk) + \u03b5V(x j\nk) = 0,\nthe solution to the above equation is\nV(x j\nk) = V(xN\nk )e\u2212\u03b5(N\u2212j),\n15\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nUsing the comparison lemma Khalil (2002), equation (S2) implies that\nV(x j\nk) \u2264 V(xN\nk )e\u2212\u03b5(N\u2212j), j \u2208 {0,...,N},\nTherefore,\nV(x j\nk) \u2192 0, as j \u2192 0, if N is sufficiently large,\nand the state x j\nk will be exponentially stabilized to the boundary b(x j\nk) = 0. If at diffusion time j \u2208 {0,....N \u22121},\nthe state x j\nk is close to the boundary, and the probability for the state x j\nk to jump into the set: {x j\nk : b(x j\nk) \u2265 0}\nis p (due to the Gaussian transitions in the diffusion process), then the probability for x0\nk to enter the set\n{x j\nk : b(x j\nk) \u2265 0} is 1\u2212(1\u2212 p) j. When j is large enough, then the probability such that b(x0\nk) \u2265 0 is almost 1.\nWhen b(xl\nk) \u2265 0 at step l \u2264 j, then b(xr\nk) \u2265 0 for all r \u2264 l following the Nagumo\u2019s theorem (as in the first case\nof the proof). Therefore, the diffusion procedure p\u03b8(\u03c4i\u22121|\u03c4i),i \u2208 {1,...,N} is finite-time diffusion invariant\nwith almost probability 1. \u25a0\nS1.2.\nProof of Theorem 3.3\nProof: Suppose the weight wk( j) is chosen such that wk( j) = 0 when j = 0, then the condition (10) becomes a\nhard constraint when j < 0. In other words, equation (10) becomes:\nh(u j\nk|x j\nk) := db(x j\nk)\ndx j\nk\nu j\nk + \u03b1(b(x j\nk)) \u2265 0,k \u2208 {0,...,H}, j \u2208 {\u2212Na,...,0},\nThen, the proof is simiar to that of the Thm. 3.2, and we have that the diffusion procedure p\u03b8(\u03c4i\u22121|\u03c4i),i \u2208\n{\u2212Na,...,N} is finite-time diffusion invariant with almost probability 1.\n\u25a0\nS1.3.\nProof of Theorem 3.4\nProof: Since \u03b3k(N) \u2264 b(xN\nk ), we have that s(x j\nk,\u03b3k( j)) := b(x j\nk) \u2212\u03b3k( j) \u2265 0 when j = N.\nThe condition (12) is equivalent to\n\u2202s(x j\nk,\u03b3k( j))\n\u2202 x j\nk\nu j\nk + \u2202s(x j\nk,\u03b3k( j))\n\u2202 j\n+ \u03b1(s(x j\nk,\u03b3k( j))) \u2265 0,\nwhich can be rewritten as\n\u02d9s(x j\nk,\u03b3k( j)) + \u03b1(s(x j\nk,\u03b3k( j))) \u2265 0,\nUsing the Nagumo\u2019 theorem presented in the proof of Thm. 3.2, we have that\ns(x j\nk,\u03b3k( j)) \u2265 0,\u2200 j \u2208 {0,...,N}\nsince s(xN\nk ,\u03b3k(N)) \u2265 0.\nAs \u03b3k(0) = 0 and s(x j\nk,\u03b3k( j)) := b(x j\nk)\u2212\u03b3k( j), we have that b(x0\nk) \u2265 0,\u2200k \u2208 {0,...,H}. Therefore, the diffusion\nprocedure p\u03b8(\u03c4i\u22121|\u03c4i),i \u2208 {0,...,N} is finite-time diffusion invariant, and the finite time in diffusion invariance\nis 0.\n\u25a0\n16\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nS2.\nExperiment Details\nS2.1.\nSafe Planning in Maze\nIn this experiment, we aim to impose trajectory constraints on the planning path of a maze. The training data is\npublicly available from Janner et al. (2022), in which initial positions and destinations in maze are randomly\ngenerated. The diffusion model is conditioned on the initial positions and destinations.\nSpecifications. The simple safety specification for the planning trajectory is defined as an super-ellipse-shape\nobstacle:\n\u0012x\u2212x0\na\n\u00132\n+\n\u0012y\u2212y0\nb\n\u00132\n\u2265 1,\n(S3)\nwhere (x,y) \u2208 R2 is the state on the planning trajectory, (x0,y0) \u2208 R2 is the location of the obstacle. a > 0,b >\n0. Since the state (x,y) is normalized in diffusion models, we also need to normalize the above constraint\naccordingly. In other words, we normalize x0,a and y0,b according to the normalization of (x,y) along the x-axis\nand y\u2212axis, respectively.\nThe complex safety specification for the planning trajectory is defined as an ellipse-shape obstacle:\n\u0012x\u2212x0\na\n\u00134\n+\n\u0012y\u2212y0\nb\n\u00134\n\u2265 1,\n(S4)\nWe also normalize the above constraint as in the simple case. In this case, it is non-trivial to truncate the planning\ntrajectory to satisfy the constraint. When we have much more complex specifications, it is too hard for the\ntruncation method to work.\nModel setup, training and testing. The diffusion model structure is the same as the open source one (Maze2D-\nlarge-v1) provided in Janner et al. (2022). We set the planning horizon as 384, the diffusion steps as 256 with an\nadditional Na = 50 diffusion steps for the proposed methods. The learning rate is 2e\u22124 with 2e6 training steps.\nThe training of the model takes about 10 hours on a Nvidia RTX-3090 GPU. More parameters are provided in\nthe attached code: \u201csafediffuser/config/maze2d.py\u201d. The switch of different (proposed) methods in testing can\nbe modified in \u201csafediffuser/diffuser/models/diffusion.py\u201d through \u201cGaussianDiffusion.p_sample()\u201d function.\nIn Fig. S1, we present a diffusion procedure using the diffuser, in which case the generated trajectory can easily\nviolate safety constraints. Using the proposed robust-safe diffuser, the generated trajectory can guarantee safety,\nbut some points on the trajectory may get stuck in local traps, as shown in S2. Using the proposed relaxed-safe\ndiffuser and time-varying-safe diffuser, the local trap problem could be addressed.\nS2.2.\nSafe Planning for Robot Locomotion\nFor robot locomotion (in MuJoCo), we wish the robot to avoid collisions with obstacles, such as the roof. In this\ncase, since there is no local trap problem, we only consider robust-safe diffuser (RoS-diffuser). Others work\nsimilarly. The training data set is publicly available from Janner et al. (2022).\nSpecifications. The simple safety specification for both the Walker2D and Hopper is collision avoidance with\nthe roof. In other words, the height of the robot head z \u2208 R should satisfy the following constraint:\nz \u2264 hr,\n(S5)\nwhere hr > 0 is the height of the roof. We also need to normalize hr according to the normalization of the state z\nin the diffusion model.\n17\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nFig. S1: Maze planning (blue to red) denoising diffusion procedure with diffuser (Left to right: diffusion time steps 256, 4,\n3, 0, respectively). Red ellipse and superellise (outside) denote safe specifications. Both specifications are violated with the\ntrajectory from diffuser.\nFig. S2: Maze planning (blue to red) denoising diffusion procedure with robust-safe diffuser at diffusion time step 0. Red\nellipse and superellise (outside) denote safe specifications. Although with safety guarantees, some trajectory points may\nget stuck in local traps.\nThe complex safety specification for both the Walker2D and Hopper is a speed-dependent collision avoidance\nconstraint:\nz+ \u03d5vz \u2264 hr,\n(S6)\nwhere \u03d5 > 0, vz \u2208 R is the speed of the robot head along the z-axis. The speed-dependent safety constraint is\nmore robust for the robot to avoid collision with the roof since when the robot jumps faster, we need to ensure a\nlarger safe distance with respect to the roof in order to account for all kinds of uncertainties or perturbations. In\nthis case, the simple truncation method is hard to work since it is not clear how to truncate both z and vz at the\nsame time.\nModel setup, training and testing. The diffusion model structures are the same as the open source ones\n(Walker2D-Medium-Expert-v2 and Hopper-Medium-Expert-v2) provided in Janner et al. (2022). We set the\nplanning horizon as 600, the diffusion steps as 20. The learning rate is 2e\u22124 with 2e6 training steps. The\ntraining of the model takes about 16 hours on a Nvidia RTX-3090 GPU. More parameters are provided in the\nattached code: \u201csafediffuser/config/locomotion.py\u201d. The switch of different methods in testing can be modified\n18\nSafeDiffuser: Safe Planning with Diffusion Probabilistic Models\nin \u201csafediffuser/diffuser/models/diffusion.py\u201d through \u201cGaussianDiffusion.p_sample()\u201d function.\nS2.3.\nSafe Planning for Manipulation\nFor manipulation (in Pybullet), the diffusion models generate joint trajectories (as controls) for the robot, which\nare conditioned on the locations of the objects to grasp and place. The training data set is publicly available from\nJanner et al. (2022). Specifications are joint limitations to avoid collision in joint space.\nSpecifications. The simple safety specification for the robot is in the joint space, and we are trying to limit the\njoint angles of the robot within allowed ranges:\nxmin \u2264 x \u2264 xmax,\n(S7)\nwhere x \u2208 R7 is the state of 7 joint angles, xmin \u2208 R7 and xmax \u2208 R7 denotes the minimum and maximum joint\nlimits. We need to normalize the limits according to how the state x is normalized in the diffusion model.\nThe complex safety specifications are speed-dependent joint constraints:\nxmin \u2264 x+ \u03d5v \u2264 xmax,\n(S8)\nwhere \u03d5 > 0, v \u2208 R7 is the joint speed corresponding to the joint angle x. In this example, since the diffusion\nmodel does not directly predict v, we evaluate v using x(k) and x(k + 1) along the planning horizon. The joints\nlimits are also normalized as in the simple specification case.\nModel setup, training and testing. The diffusion model structure is the same as the open source one provided\nin Janner et al. (2022), and we use their pre-trained models to evaluate our methods when comparing with other\napproaches.\n19\n"
  },
  {
    "title": "Brainformers: Trading Simplicity for Efficiency",
    "link": "https://arxiv.org/pdf/2306.00008.pdf",
    "upvote": "1",
    "text": "Brainformers: Trading Simplicity for Efficiency\nYanqi Zhou 1 Nan Du 1 Yanping Huang 1 Daiyi Peng 1 Chang Lan 1 Da Huang 1 Siamak Shakeri 1 David So 1\nAndrew Dai 1 Yifeng Lu 1 Zhifeng Chen 1 Quoc Le 1 Claire Cui 1 James Laundon 1 Jeff Dean 1\nAbstract\nTransformers are central to recent successes in\nnatural language processing and computer vision.\nTransformers have a mostly uniform backbone\nwhere layers alternate between feed-forward and\nself-attention in order to build a deep network.\nHere we investigate this design choice and find\nthat more complex blocks that have different per-\nmutations of layer primitives can be more efficient.\nUsing this insight, we develop a complex block,\nnamed Brainformer, that consists of a diverse sets\nof layers such as sparsely gated feed-forward lay-\ners, dense feed-forward layers, attention layers,\nand various forms of layer normalization and acti-\nvation functions. Brainformer consistently outper-\nforms the state-of-the-art dense and sparse Trans-\nformers, in terms of both quality and efficiency. A\nBrainformer model with 8 billion activated param-\neters per token demonstrates 2\u00d7 faster training\nconvergence and 5\u00d7 faster step time compared to\nits GLaM counterpart. In downstream task evalu-\nation, Brainformer also demonstrates a 3% higher\nSuperGLUE score with fine-tuning compared to\nGLaM with a similar number of activated param-\neters. Finally, Brainformer largely outperforms a\nPrimer dense model derived with NAS with simi-\nlar computation per token on fewshot evaluations.\n1. Introduction\nIn recent years, large neural networks derived from from\nthe Transformer architecture (Vaswani et al., 2017) have\ndemonstrated superior results on language understanding\nand generative tasks. Many improvements on Transformer\nvariants have come from scaling the size of models (Raf-\nfel et al., 2020; Brown et al., 2020a; Shoeybi et al., 2019;\nChowdhery et al., 2022), scaling the training tokens (Hoff-\n1Google Deepmind.\nCorrespondence to:\nYanqi Zhou <yan-\nqiz@google.com>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\n2.25\n2.50\n2.75\n3.00\n3.25\n3.50\n3.75\n4.00\nActicated Params (Millions) in Log Scale\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\nLog Perplexity\nScaling\nBranformer Perplexity\nGLaM Perplexity\nBrainformer Steps Per Sec\nGLaM Steps Per Sec\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nSteps Per Second\nFigure 1: Brainformer Vs. GLaM in Scaling. Brainformer\nimproves model quality at much faster training step time.\nmann et al., 2022; Shoeybi et al., 2019), better training data\nquality (Du et al., 2022), and sparsely activated model archi-\ntectures (Du et al., 2022; Lepikhin et al., 2021; Roller et al.,\n2021; Lewis et al., 2021).\nAmong the efficient transformer language models (Wang\net al., 2020; Choromanski et al., 2020; Tay et al., 2021; Hua\net al., 2022), there is a focus on improving attention-layer\nefficiency using low-rank approaches or approximations.\nHowever, recent work has also identified that dense feed-\nforward layers constitute most of the computational cost\nfor common sequence lengths (\u22642048), particularly when\nthe model is large (Du et al., 2022; Zhou et al., 2022). To\nfurther improve compute efficiency such as total FLOPs\nused during training to reach convergence, sparsely gated\nMixture-of-Experts (Lepikhin et al., 2021; Fedus et al.,\n2021; Du et al., 2022; Zhou et al., 2022; Roller et al., 2021;\nLewis et al., 2021; Jaszczur et al., 2021) have become preva-\nlent, giving the model a larger overall capacity to improve\nquality while holding computational cost fixed. Sparsely\nactivated models not only reduce the computational cost, but\nalso have better specialization by training different experts\non different data distributions through the use of a routing\nfunction without reducing the effective training time for\neach expert. The MoE architectures in this line of work are\nbased on uniform transformer blocks or interleaving dense\nand sparse layers (Du et al., 2022) and a fixed top-k routing.\n1\narXiv:2306.00008v1  [cs.LG]  29 May 2023\nBrainformers: Trading Simplicity for Efficiency\na\na\nf\na f\na f\na f\na f\na f\na f\na f\na a a\nf\nf\nf\nf\na f\na f\na f\na\nf\na g\nf\nf\ng\na f\na f\na f\na f\na\nf\nf\na f\na\nf\na\ng a g\nf\nf\ng\ng a g\nf\nf\ng\ng a g\nf\nf\ng\ng\nVanilla Transformer\nSandwich Transformer\nStackable Brainformer\na g\nf\nf\ng a\nf\ng a\ng\nf\na g\nf\nf\ng\nGLaM\na\na\na\na\na\na\na\na\nFigure 2: High-level Comparison with Related Work. \u2019a\u2019: at-\ntention, \u2019f\u2019: feed-forward, \u2019g\u2019: sparsely gated feed-forward.\nGLaM interleaves dense transformer blocks with sparse\ntransformer blocks. Brainformer reduces the frequency of\nattention and changes layer widths together with layer types.\nResonating with the layer-wise architecture stacking in Ef-\nficientNet (Tan & Le, 2019) and layer reordering in the\nsandwich transformer (Press et al., 2019), we propose a non-\nuniform architecture with sparsity where there is no strict\nlayer interleaving as in the vanilla transformer in fig. 2. We\ntrade off architecture regularity by allowing the search space\nto compose different sub-layers in different orders. For bet-\nter scaling, we introduce sparsity in the search space with a\nsparsely gated feed-forward layer (MoE layer) coupled with\ndifferent gating mechanisms.\nWe find that optimizing the architecture, sparsity, and rout-\ning mechanism in sparse layers is critical to achieve near-\nperfect log-scale scaling in quality. Figure 1 shows that\nBrainformer scales much better than GLaM (manually\ncrafted sparse transformer). Brainformer consistently im-\nproves training perplexity while keeps example rate almost\nconstant when increasing model capacity, however, GLaM\nhas a much worse example rate when scaled up.\nWe only treat the MoE layer as a general method to sparsify\nthe model. In practice, any conditional computation method\ncan be blended in. We apply a simple evolutionary search to\ndiscover many attributes, such as the best way to interleave\nlayers and layer capacities, when to fuse layers, and when to\nspecialize layers with MoE modules. For ease of scaling, we\npropose a block-wise sub-layer grouping, such that stacking\na variable number of blocks produces models of different\nscales, as illustrated in Stackable Brainformer in fig. 2. As\nour results in Section 5 show, this approach has proven\neffective in our evaluation at multiple model scales.\n2. Related Work\nLarge Language Models: Language models have demon-\nstrated strong performance for many natural language pro-\ncessing tasks (Mikolov et al., 2010; Sutskever et al., 2011;\nDai & Le, 2015). Scaling up model capacity and number\nof training tokens has shown huge success in enhancing\nthe performance of computer vision architectures (He et al.,\n2016a;b; Ghiasi et al., 2019; Dai et al., 2021) as well as\nneural language models (Radford et al., 2018; Brown et al.,\n2020b; Kaplan et al., 2020; Raffel et al., 2020; Shoeybi\net al., 2019; Hoffmann et al., 2022).\nSparsely Activated Models: Conditional computation ef-\nfectively increases the capacity of a deep neural network\nwithout increasing the total amount of computation, by ac-\ntivating certain parameters and computation on demand,\nbased off the input token or sequence (Cho & Bengio, 2014;\nPuigcerver et al., 2020; Lin et al., 2019). The gating deci-\nsions may be binary or sparse and continuous, stochastic\nor deterministic. In a multi-device setting, sparsely-gated\nMoE (Shazeer et al., 2017) demonstrates massive improve-\nments in model capacity, training time, or model quality\nwith gating. Various MoE architectures including Switch\nTransformer (Fedus et al., 2021) and GLaM (Du et al., 2022)\nhave been proposed. They adopt a token-based gating where\nan auxiliary loss is imposed to counter load imbalance issues.\nRecently, more advanced gating functions are devised to\nameliorate load imbalance, improve speed, and downstream\ngeneralization (Roller et al., 2021; Dua et al., 2021; Zuo\net al., 2021; Gross et al., 2017; Zhou et al., 2022; Jaszczur\net al., 2021).\nNon-uniform Architectures: EfficientNet represents one\nof the very early non-uniform architectures that leverages\nlayer heterogeneity to achieve SoTA. Instead of searching\nfor a new operator or a new block of operators, EfficientNet\nfocuses on optimizing the layer compound coefficients to\nscale the model effectively. This heterogeneity leads to a\nmodel more than 8\u00d7 smaller and more than 6\u00d7 faster on in-\nference (Tan & Le, 2019). Sandwich Transformer promotes\na non-interleaved, non-uniform architecture for language\nmodeling tasks. However, the sandwich reordering pattern\ndoes not guarantee performance gains across every task.\nResidual MoE (Wu et al., 2022) factorized the weights into\nan input-independent core and an input-dependent residual,\nthus achieves comparable results with the upper-bound MoE\ntraining while only introducing minor additional training\ncost than the lower-bound non-MoE training. In this work,\nwe take inspiration from the earlier work but further improve\nscaling and generalization via automatic model discoveries.\n3. Method\n3.1. Deriving Our Model Components\nThere are various forms of computation factorization that\ncan lead to lower computation cost or faster computation\nwithout penalizing model quality. As indicated in fig. 3,\n2\nBrainformers: Trading Simplicity for Efficiency\nlow-rank and multi-expert layers are two major methods\nfor factorizing a matrix multiplication, both of which re-\nduces FLOPs by half while not sacrificing model capacity.\nWhen devising an efficient neural network, as indicated\nin fig. 4, low-rank and multi-expert can be combined and\nstacked to achieve more interesting model architectures that\nare computationally efficient. Finally, by also coupling a\ntemporal mixture layer (e.g. attention (Vaswani et al., 2017),\ngMLP (Liu et al., 2021) or MLP mixer (Tolstikhin et al.,\n2021)) which captures the causal relations between tokens,\nthe network becomes a multi-expert transformer variant.\nDense\n  \nHalf FLOPS\ny = M * x\ny = V * (U * x)\n  \n  Dense\n  \n  \nHalf FLOPS\ny = M * x\nx1, x2 = split(x)\ny = concat(M1 * x1, M2 * x2)\nlow-rank / bottleneck\nmulti-branch / multi-expert\nFigure 3: Two methods of matrix factorization: Low-rank\nand Multi-branch.\nHowever, constructing an efficient network does not require\nconforming to the uniformity of the model architecture as\nillustrated in the last figure of fig. 4. By carefully selecting\nlayer types and layer interleaving, as well as other hyper-\nparameters layers, we could achieve higher quality, train-\ning efficiency, as well as better scaling. This leads our\nexploration towards a more training-efficient architecture by\nadopting low-rank and multi-expert compression methods\nwith coarse-grain sparsity.\n3.2. Block-wise Architecture\nWe largely take inspiration from the layer-wise compound\nscaling in EfficientNet (Tan & Le, 2019). For the easi-\nness of scaling, We construct a block-wise search space\nwhere the restriction of uniformly stacking layers is re-\nmoved. Instead, we create a generic layer as a function\nYi = Fi(Xi), Fi \u2208 {Fattn, Fmoe, Fffn} where Fi is an\noperator selected from the operation set consisting of self at-\ntention, sparsely gated feed-forward (MoE), and dense feed-\nforward sub-layers as depicted in eq. (3). Input Xi has a\ntensor shape of {B, L, H} and H \u2208 { 3\n4, 1, 3\n2}\u00d7Hmodel_dim\n  \n  \n  \n  \n  \n  \nSmaller \nlow-rank layers\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \nSplit into more \nexperts\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \nStack more \ncompressions\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \nMixture Layers\nFigure 4: Evolving matrix factorization into transformer-\nstyled model architecture.\nwhere B is the batch size, L is the sequence length, and H\nis a tunable model dimension. The intuition behind tun-\ning model dimension is to enable more flexible network\ntopologies with various factorization methods as described\nin section 3.1. For example, we could instantiate a model\nwith wider hidden dimensions or a model with experts but\neach expert being narrow.\nUnlike a traditional simple, uniform transformer block, a\nBrainformer block is a complex block N that can be repre-\nsented by a list of composed layers in eq. (1):\nN = Fk \u2299 ... \u2299 F2 \u2299 F1(X1) =\nK\nj=1...k\nFj(X1)\n(1)\nWe can stack an arbitrary number of Brainformer blocks\nto create a target model. The search objective is to find\nan optimal layer architecture Fi, and model scaling multi-\npliers for multiple model inner dimensions that minimizes\nthe perplexity. Table 1 summarizes the search space in a\nBrainformer architecture.\nFigure 5 and Algorithm 1 illustrate the two phases that we\nuse to discover compute-efficient Brainformer models. Dur-\ning the search, a regularized evolutionary search algorithm\nsamples block architectures from the search space and trains\nthe sampled architectures using a proxy training. In a proxy\ntraining task, a small 100M32E architecture is instantiated\nby stacking the sampled block three times. This matches\nthe number of layers in a baseline GLaM architecture. We\napply early stopping during the proxy training, where un-\n3\nBrainformers: Trading Simplicity for Efficiency\n      Block-wise     \nSearch Space \nProxy task population \nEarly stopping on accuracy \n@ train_steps = \u00bc T_max \nTop-k models \n\u2026\u2026\nS1\nS2\n@ train_steps = T_max \nProxy Task \n@ 100M32E\n1B64E\n8B64E\n   \u2026\u2026\n{S1}x6\n   \u2026\u2026\n{S2}x8\n{S0}x3\nBlock Search\nBlock Stack & Eval\nS0\nEarly stopping on \ninference time\nGet Reward & Evolve\nBlock Scale\nFigure 5: Block-wise architecture search and stacking.\nTable 1: Search Space Table: Fattn is a self-attention layer,\nFmoe is a sparsely gated FFN layer, and Fffn is a regular\ndense FFN layer. The baseline is a 100M 12-layer dense\ntransformer model with Hmodel_dim = 768.\nSearch Item\nSearch Space\nLayer Type (Fi)\nFattn, Fmoe, Fffn\nModel Dim. (d)\n512, 768, 1024\nMoE Hidden Dim. (dmoe)\n1536, 2048, 3072, 4096\nFFN Hidden Dim. (dffn)\n1536, 2048, 3072, 4096\nAttention Heads. (h)\n12, 16, 20\nGating Func. (g)\nTop-2, Expert Choice\nCapacity Factor (c)\n1, 2, 3, 4\nActivation Func. (a)\nGated Re/GeLU, ReLU, GeLU\npromising models are pruned early due to the violation of\ninference time constraint or perplexity constraint at 25%\nof the maximum training steps, compared to the baseline\nGLaM architecture.\nAt the end of evolution, top-k block architectures with the\nhighest rewards are evaluated at multiple target scales. In\nour evaluation, we first scale the model dimension and hid-\nden dimension 2x and 4x, following the scaling factors\npresented in GLaM, to create block S1 and S2 targeting\n1B and 8B model scale. Then we stack block S1 and S2\nrespectively to create 1B64E and 8B64E model variants. N\nin Algorithm 1 can be determined mathematically according\nto the target total activated parameters. Our final evalua-\ntions are based on comparisons with baseline architectures\nat multiple scales.\nAlgorithm 1 Brainformer Block Search\nRequire: A Block-wise architecture search space B. An\nevolutionary search algorithm with population size p.\n1: for t = 1 to T0 do\n2:\nfor B(i) in SamplePopulation(B, p) do\n3:\nG(i) \u2190 StackThreeTimes(B(i))\n4:\nif EarlyStopping(G(i)) then\n5:\nR(i) = \u22121\n6:\nelse\n7:\nAi, T i \u2190 Train(G(i), Tmax)\n8:\nR(i) \u2190 f(Ai, T i)\n9:\nend if\n10:\nend for\n11: end for\n12: Gtopk \u2190 TopK({G(i), R(i)})\n13: for G(i) in Gtopk do\n14:\nG(i) \u2190 ScaleModelDim(G(i))\n15:\nG(i) \u2190 StackNTimes(G(i))\n16:\nAi, T i \u2190 Train(G(i))\n17: end for\n3.3. Fair Comparisons Across Model Architectures\nPrior NLP model scaling studies (Raffel et al., 2020; Rad-\nford et al., 2018; Brown et al., 2020b; Rae et al., 2021)\ntypically explore quality scaling with fixed model capacity\nand training steps/tokens. For example, a scaling plot typ-\nically fixes training steps/tokens while varying the model\nparameters. However, when training a model, users typi-\ncally have a fixed budget and can trade-off training time,\ncompute resources, and quality to stay within that budget.\nIf what we care about is computational cost and training\n4\nBrainformers: Trading Simplicity for Efficiency\n3.13\n0.02\n-0.52\n0.25\n-0.25\n3.10\n0.02\n0.14\n2.65\n-0.11\n0.22\n1.24\n0.21\n0.01\n-0.21\n3.90\nTokens\nExperts\nSelect Top-K\n-0.26\n0.02\n-0.52\n0.25\n-0.25\n3.10\n0.02\n0.14\n2.65\n-0.11\n0.22\n-0.24\n0.21\n0.01\n2.50\n3.90\nTokens\nExperts\nSelect Top-K\nFigure 6: Token-based routing vs. Expert-based routing.\nconvergence time, then comparing model qualities while\nfixing total parameters is not fair, particularly when com-\nparing across model architectures and model families. For\nexample, it may discriminate against models with more to-\ntal parameters that consume fewer computational FLOPs,\nsuch as sparsely activated models. The GLaM paper (Du\net al., 2022) addresses this by conducting a scaling study on\nactivated memory (which approximates the computational\ncost), rather than the total parameter size, on a fixed number\nof training tokens. However, comparing models with a fixed\namount of training tokens may still also not be fair as some\nsmaller models can benefit more from additional training\ndata and outperform a bigger model with the same total\ntraining cost (e.g. GPU hours, TPU hours, etc.). The Chin-\nchilla paper (Hoffmann et al., 2022) is the first to suggest\ncompute-efficient scaling, which varies both model capacity\nand training tokens at a fixed computational cost. Resonat-\ning with compute-efficient model scaling, we further take\nmodel architectural change into consideration during the\nsearch for efficient model architectures with better training\nconvergence and inference time. More particularly, we com-\npare across models with a fixed training cost and model\ninference time, which allows the search algorithm to trade\noff between model capacity and training tokens.\n3.4. Training Time Constrained Search\nWe fix the wall clock time for each search trial which en-\ncourages models with faster training convergence being\ndiscovered. The objective is to find model architectures that\nyield higher accuracy with a fixed training budget (number\nof chips times training hours). In an evolution search, a con-\ntroller minimizes the pre-training validation cross-entropy\nloss in eq. (2) while meeting an inference time constraint\nin eq. (5). The block architecture is defined around a 100M\nvanilla transformer architecture, as illustrated in Table 2.\nEach trial is trained with a fixed wall clock time so that\nfaster models can be compensated with more training steps.\nWe empirically find that fixing training wall clock time\nwhile meeting a inference time constraint yields models\nwith faster training convergence and higher quality.\nmin\nF1:k,d,dmoe,dffn,h,g,c,aL(N(F1:k, d, dmoe, dffn, h, g, c, a))\n(2)\nFi =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nFd,h,a\ni\n,\nif\nFi = Fattn\nFd,dffn,a\ni\n,\nelse if\nFi = Fffn\nFd,dmoe,g,c,a\ni\n,\notherwise\nFi = Fmoe\n(3)\ns.t.\nN(F1:k, d, dmoe, dff, h, g, c, a) =\nK\ni=1...k\nFi(X1)\n(4)\nStep_Time(N) \u2264 baseline_step_time\n(5)\n4. Token-based Routing Versus Expert-based\nRouting\nWhile there are various routing methods in existing MoE\nliterature, we primarily focus on two classes of routing:\ntoken-based routing and expert-based routing, to illustrate\nthe idea that routing strategy can change the optimal model\narchitecture when sparsely activated layers are introduced.\nAs an example, in Figure 6, the rows and columns contain\nun-normalized scores computed for four tokens and four\nexperts. Each value is produced by the dot product of the\ntoken embedding and the expert embedding. Once the token-\nto-expert affinity scores are generated, there are a few ways\nto decide which experts each token should be routed to. In\ntoken-based routing, the model routes to the top-k experts\nfor each token, while in an expert-based routing, the experts\nchoose top-k tokens. More particularly, we follow the top-2\ngating approach used in GShard (Lepikhin et al., 2021) and\nGLaM (Du et al., 2022) as top-2 has demonstrated stronger\nempirical performance than top-1 gating. For the expert-\nbased gating, we follow the Expert Choice gating (Zhou\net al., 2022) where perfect load balance is achieved with\nheterogeneous parameter allocation.\nThere are various ways of generating the token-to-expert\naffinity scores. One possible way is to create a trainable\ngating matrix Wg that projects the input feature space to\na token-to-expert score. The score should be normalized\neither along the token dimension or the expert dimension.\nTo avoid causal leakage in decoding mode, we suggest nor-\nmalizing along the expert dimension for both token-based\nrouting and expert-based routing.\n5. Evaluation\nSetup: Table 2 summarizes the hyperparameter settings\nof different baseline MoE models. In the baseline MoE\nGLaM (Du et al., 2022) model, we interleave transformer\nblocks with regular dense FFNs and transformer blocks with\nsparsely gated FFNs (MoE layer). As a reference point, we\nalso include the respective dense model configurations with\n5\nBrainformers: Trading Simplicity for Efficiency\nTable 2: Sizes and architectures of baseline dense models and MoE (GLaM) models. Models are grouped by the number of\nactivated parameters per token.\nModel\nType\nnparams\nnact-params\nL\nM\nH\nnheads\ndhead\nE\n0.1B\nDense\n130M\n130M\n12\n768\n3,072\n12\n64\n\u2013\n0.1B/32E\nMoE\n1.9B\n145M\n32\n1.7B\nDense\n1.7B\n1.700B\n24\n2,048\n8,192\n16\n128\n\u2013\n1.7B/64E\nMoE\n27B\n1.879B\n64\n8B\nDense\n8.7B\n8.7B\n32\n4,096\n16,384\n32\n128\n-\n8B/64E\nMoE\n143B\n9.8B\n64\n0\n100\n200\n300\n400\n500\nK Steps\n2.6\n2.7\n2.8\n2.9\n3.0\n3.1\n3.2\n3.3\nEval Perplexity\nGLaM\nSearch-w-top2\nBrainformer-1\nBrainformer-2\n0\n250 500 750 1000125015001750\nK Steps\n2.0\n2.2\n2.4\n2.6\n2.8\n3.0\nEval Perplexity\nGLaM\nExpertChoice\nBrainformer-1\n(a)\n(b)\nFigure 7: (a) Pre-training perplexity comparison for 100M32E (100M parameters per expert, 32 experts). Search-w-top2\nis the model found by using neural architecture search but with fixed top-2 token-based gating. (b) Training perplexity\ncomparison for 8B64E (8B parameters per experts, 64 experts). Expert Choice is the GLaM architecture with expert-based\ngating function.\ncomparable numbers of activated parameters per-token dur-\ning inference in the table. With a similar number of activated\nparameters as a 0.1B dense model, 0.1B/32E represents the\nsparse model with every other transformer layer replaced by\na 32-expert MoE layer. While nparams is the total number\nof trainable parameters, nact\u2212params represents the number\nof activated parameters per token. nact\u2212params roughly ap-\nproximates the computational expensive of a model. L is the\ntotal number of Transformer layers, M is the model dimen-\nsion, H is the hidden dimension after the projection in each\ntransformer layer, nheads is the number of attention heads,\nand dhead is the hidden dimension of each attention head.\nWe train and evaluate our Brainformer models and baseline\nmodels on 64 Cloud TPU-V4 chips, except for models at\nthe 8B-scale which take 512 Cloud TPU-V4 chips to train.\nDataset: We use the high-quality dataset from GLaM of\n1.6 trillion tokens that are representative of a wide range of\nnatural language use cases. This dataset consists of a high-\nquality filtered subset of webpages that are combined with\nsmaller corpora of books, Wikipedia pages, conversations,\nforums, and news to create the final dataset. A more detailed\ndescription of the dataset including the data and mixture\nweights can be found in the GLaM paper (Du et al., 2022).\nModel Training: We train a few decoder-only models using\nthe searched best Brainformer blocks and related baselines.\nBrainformer-1 and Brainformer-2 are two selected best mod-\nels. With limited computational resources, we only scale\nBrainformer-1 to 1B and 8B scales. Our model training\nfollows the setup of GLaM where a maximum sequence\nlength of 1024 tokens is used. We use an Adafactor op-\ntimizer (Shazeer & Stern, 2018) with first-moment decay\n\u03b21 = 0 and second-moment decay \u03b22 = 0.99. The learning\nrate is kept constant for the first 10K training steps, then\nis decayed with an inverse square root schedule. We use\nthe SentencePiece subword tokenizer with a vocabulary of\nsize of 256K. The 100M-scale models and 1B-scale models\n6\nBrainformers: Trading Simplicity for Efficiency\nTable 3: Training efficiency comparison. Brainformer models have better training convergence and faster step times,\ncompared to GLaM, fixed gating search, and expert-based gating but with fixed architecture. Brainformer-1 and Brainformer-\n2 are two selected best models. With limited computational resources, we only scale Brainformer-1 to 1B and 8B scales.\nModel\nTotal Params\nActivated Params\nTrain Steps\nSteps/Sec\nPPLX\n100M32E\nGLaM\n1B\n145M\n0.5M\n1.92\n2.73 +/- 0.002\nSearch-w-Top2\n1.87B\n210M\n0.5M\n2.03\n2.67 +/- 0.005\nBrainformer-1\n3.19B\n156M\n0.5M\n2.03\n2.57 +/- 0.003\nBrainformer-2\n3.33B\n266M\n0.5M\n2.16\n2.59 +/- 0.005\n1B64E\nGLaM\n27B\n1.88B\n1.0M\n1.23\n2.25 +/- 0.004\nSearch-w-Top2\n27B\n3.05B\n1.0M\n1.27\n2.21 +/- 0.003\nBrainformer-1\n30B\n1.38B\n1.0M\n2.00\n2.25 +/- 0.002\nBrainformer-2\n52B\n1.31B\n1.0M\n1.76\n2.23 +/- 0.001\n8B64E\nGLaM\n143B\n9.8B\n1.5M\n0.39\n2.12 +/- 0.002\nExpert-based Gating\n143B\n9.8B\n1.5M\n0.50\n2.03 +/- 0.005\nBrainformer-1\n158B\n7.4B\n1.5M\n1.96\n1.99 +/- 0.002\nare trained with 64 TPU V4 chips, while the largest model\n(8B/64E) evaluated is trained on 512 TPU V4 chips. We\ndon\u2019t use any dropout during training because the training\ncorpus is large enough that each sample is only encountered\nonce.\nModel Evaluation: We mainly focus on two types of down-\nstream evaluation: 1) Fine-tuning performance on 11 se-\nlected classification tasks from the GLUE and SuperGLUE\nbenchmarks (Wang et al., 2018; 2019). 2) We evaluate\noneshot performance with five language generation tasks\nfocused on question answering.\n5.1. Training Convergence\nIn this section, we evaluate Brainformer top models with\nrelated baselines including 1) Top-2 gating based model ar-\nchitecture search (Search-w-Top2) and 2) GLaM (Du et al.,\n2022), a manually crafted architecture with fixed top-2 gat-\ning. Providing the flexibility of tuning the gating function\nand network architecture significantly improves pre-training\nefficiency. As shown in table 3, our searched best Brain-\nformer models outperform the baselines in terms of com-\nputational cost (activated parameters), training step time\n(steps/sec), and training perplexity (PPLX) for fixed train-\ning steps. When scaled to 8B64E, Brainformer converges\nto lower perplexity and is more than 5x faster in step time\nand 2x faster in training convergence using the same hard-\nware configuration (512 Cloud TPU-V4 chips). With a fixed\n600B training tokens, Brainformer is much more accurate\nthan the baselines at 8B scale.\n5.2. Finetuning Results\nWe pretrain the models for a total fixed wall clock time as\nthe baseline GLaM model. We then finetune the models with\neleven selected GLUE and SuperGLUE classification tasks.\nAt two different scales, 100M64E and 1B64E, Brainform-\ners outperform the baseline GLaM model by a significant\nmargin of 2-4% average score. The fine-tuning results in\ntable 4 indicates that Brainformer not only excels at training\nconvergence but also generalizes well to downstream tasks.\n5.3. Fewshot Results\nAligned with prior work in fewshot in-context learning, we\ncompare Brainformer oneshot performance on five selected\ngenerative tasks in table 5: Natural Questions (Kwiatkowski\net al., 2019), TriviaQA (Joshi et al., 2017), Web Ques-\ntions (Berant et al., 2013), Squadv2 (Rajpurkar et al., 2018),\nand Lambada (Paperno et al., 2016), with a sparse model\nGLaM and a dense model Primer (So et al., 2021) of similar\nactivated memory size. Brainformer outperforms Primer\nand GLaM by a large margin on all the tasks except Nqs\nbeing slightly worse than GLaM. GLaM yields competitive\nscores while being 2x slower than Brainformer.\n6. Discussion\n6.1. Visualizing a Brainformer Block\nIn this section, fig. 9 provides a visualization of a Brain-\nformer architecture block. Unlike a conventional trans-\nformer block, where there is only an attention layer and\na dense feed-forward layer, a Brainformer block contains\n8 sub-layers. The Brianformer block is repeated 3 times, 6\n7\nBrainformers: Trading Simplicity for Efficiency\nTable 4: Finetuning Results on GLUE/superGLUE: Brainformers at 100M and 1B significantly outperform GLaM counter-\nparts, yielding over 3% gains in overall scores.\nSize\nModel\nBoolQ\nCB\nCoLA\nMNLI\nMRPC\nQNLI\n100M64E\nGLaM\n0.791\n0.859\n0.818\n0.849\n0.833\n0.901\nBrainformer-1\n0.812\n0.922\n0.828\n0.855\n0.870\n0.907\n1B64E\nGLaM\n0.829\n0.938\n0.831\n0.860\n0.857\n0.919\nBrainformer-1\n0.859\n0.938\n0.863\n0.896\n0.875\n0.938\nSize\nModel\nQQP\nRTE\nSST2\nWiC\nWNLI\nAVG\n100M64E\nGLaM\n0.907\n0.808\n0.952\n0.687\n0.609\n0.819\nBrainformer-1\n0.812\n0.840\n0.952\n0.702\n0.635\n0.840\n1B64E\nGLaM\n0.911\n0.816\n0.945\n0.711\n0.547\n0.833\nBrainformer-1\n0.917\n0.899\n0.972\n0.720\n0.719\n0.873\nTable 5: Oneshot evaluation on five important generative tasks. All models are trained with 200B training tokens.\nModel\nNqs\nTriviaqa\nWebqa\nSquadv2\nLambada\nSteps/Sec\nGLaM 1B64E\n9.14\n41.8\n10.8\n46.2\n25.2\n0.55\nPrimer 1B (So et al., 2021)\n4.82\n24.7\n6.50\n49.2\n22.6\n1.50\nBrainformer 1B64E\n8.23\n43.4\n12.0\n49.5\n25.7\n1.37\ntimes, and 8 times respectively in the 100M, 1B, and 8B\nscale. In a vanilla transformer model, a dense FFN layer has\nan optimized expansion ratio of 4, which results in a hidden\ndimension 4x wider than the model dimension. In the opti-\nmized Brainformer block 1 and 2, the search algorithm picks\na slightly larger model dimension of 1024 (as compared to\n768) and a smaller expansion factor in the dense FFNs and\nMoE layers (as compared to 3072). This is a reasonable\noptimization, as MoE layers effectively widen the network\nwith more experts. In the MoE layers, the search algorithm\npicks the expert choice gating function (Zhou et al., 2022)\nwith a capacity factor of one in Brainformer block 1, result-\ning in a very sparse network in which each token can be\nrouted to a single expert on average. Being much faster in\nstep time, block 1 takes more training steps, thus training\ndata to achieve good quality. Therefore, we also picked\nanother strong candidate, Brainformer block 2, in which a\nlarger capacity factor in the MoE layers is selected. Block 2\nis lightly slower in step time, but takes fewer training steps\nto get good accuracy, thus is more data efficient.\n6.2. Can We Simplify?\nWe did an ablation study on block simplification. A very\nnatural question to ask is whether we can simplify the ar-\nchitecture block. In exploring the answer to this question\nwe were able to extrapolate some patterns. We find that\nthe ratio of different layer types is critical to model quality:\nreplacing a layer with a different layer results in degraded\nquality. However, the network is relatively insensitive to\nlayer order, such that swapping any two layers would not af-\nfect performance much. For example, to create a simplified\npattern, we can interleave the dense FFNs and MoE layers\nor simply creating contiguous layers of the same type.\nModel Dimension : 1024\nDense FFN Dimension : 1536\nMoE FFN Dimension : 2048\nGating Func : Expert Choice\nGating Capacity Factor : 1\nAttention Heads : 20\nBrainformer Block # 1\n  ATTN\n       FFN\n       FFN\n       FFN\n       FFN\nMOE\nMOE\nMOE\nFigure 8: Brainformer Block # 1\nModel Dimension : 1024\nDense FFN Dimension : 2048\nMoE FFN Dimension : 2048\nGating Func : Expert Choice\nGating Capacity Factor : 2\nAttention Heads : 16\nBrainformer Block # 2\n  ATTN\n       FFN\n       FFN\n       FFN\nMOE\nMOE\n  ATTN\n  ATTN\nFigure 9: Brainformer Block # 2\n8\nBrainformers: Trading Simplicity for Efficiency\n7. Conclusion\nUsing an evolutionary search algorithm, we have devel-\noped and evaluated a complex architecture block, named\nBrainformer, that consists of a diverse sequence of layers, in-\ncluding a sparsely gated feed-forward layer. Along with the\nnew block, we also propose evaluating using a fixed training\ntime search, which enables fair comparisons across model\nfamilies. Brainformer demonstrates up to 2\u00d7 faster training\nconvergence and 5\u00d7 faster step time compared to its GLaM\ncounterpart. In downstream task evaluation, Brainformer\nalso demonstrates a 3% higher SuperGLUE score with fine-\ntuning compared to GLaM, and greatly outperforms Primer\non oneshot evaluation for five generative tasks.\n8. Limitations\nIn terms of research scope, our empirical results are primar-\nily on NLP domain, thoroughly on a wide range of NLU and\nNLG tasks. However, we leave it to future work to apply\nBrainformer to computer vision.\nWhen adopting Brainformer targeting different hardware\nplatforms, there can be potential intricacies. For example,\nedge devices can impose strict hardware constraints that\nrestricts the expression of Brainformer models. A practical\nway is to run model training and quality evaluation on faster\naccelerators such as GPUs or TPUs while simulating the step\ntime for the target hardware or using a learnt performance\nmodel to predict the inference speed on the target hardware.\nAnother issue is some fundamental operators might not be\nsupported on a device lacking sufficient on-chip memories.\nFor example, global pooling is not supported on edge TPU.\nBut that can be out of scope for this paper, as Brainformer\naims to construct a compute-efficient model architecture out\nof feasible operators.\nAnother limitation can be large resource consumption. In\nthe Brainformer search, we used 512 TPU v4 for a week\nto arrive at the best solutions. However, worth mentioning\nthat we are working at a much large model scale and this\nwill be mitigated when we use a smaller model size and\nsmaller number of experts in the MoE layers. Also, the\nsearch identified better model architecture within as early\nas 500 trials. Practically, the resource consumption can\nbe small if we only need to identify better but suboptimal\nmodels.\nReferences\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on Freebase from question-answer pairs. In Pro-\nceedings of the 2013 Conference on Empirical Methods\nin Natural Language Processing, pp. 1533\u20131544, Seattle,\nWashington, USA, October 2013. Association for Com-\nputational Linguistics. URL https://www.aclweb.\norg/anthology/D13-1160.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,\nG., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,\nG., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,\nWu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,\nLitwin, M., Gray, S., Chess, B., Clark, J., Berner, C.,\nMcCandlish, S., Radford, A., Sutskever, I., and Amodei,\nD. Language models are few-shot learners. In Larochelle,\nH., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin,\nH. (eds.), Advances in Neural Information Processing\nSystems, volume 33, pp. 1877\u20131901. Curran Associates,\nInc.,\n2020a.\nURL\nhttps://proceedings.\nneurips.cc/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020b.\nCho, K. and Bengio, Y.\nExponentially increasing the\ncapacity-to-computation ratio for conditional computa-\ntion in deep learning. arXiv preprint arXiv:1406.7362,\n2014.\nChoromanski, K., Likhosherstov, V., Dohan, D., Song, X.,\nGane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin,\nA., Kaiser, L., et al. Rethinking attention with performers.\narXiv preprint arXiv:2009.14794, 2020.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nDai, A. M. and Le, Q. V.\nSemi-supervised sequence\nlearning.\nIn Cortes, C., Lawrence, N., Lee, D.,\nSugiyama, M., and Garnett, R. (eds.), Advances in Neural\nInformation Processing Systems, volume 28. Curran As-\nsociates, Inc., 2015. URL https://proceedings.\nneurips.cc/paper/2015/file/\n7137debd45ae4d0ab9aa953017286b20-Paper.\npdf.\nDai, Z., Liu, H., Le, Q. V., and Tan, M. CoAtNet: Marrying\nconvolution and attention for all data sizes. In Advances\nin Neural Information Processing Systems, 2021.\nDu, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu,\nY., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. Glam:\nEfficient scaling of language models with mixture-of-\nexperts. In International Conference on Machine Learn-\ning, pp. 5547\u20135569. PMLR, 2022.\n9\nBrainformers: Trading Simplicity for Efficiency\nDua, D., Bhosale, S., Goswami, V., Cross, J., Lewis, M.,\nand Fan, A. Tricks for training sparse translation models.\narXiv preprint arXiv:2110.08246, 2021.\nFedus, W., Zoph, B., and Shazeer, N. Switch transform-\ners: Scaling to trillion parameter models with simple and\nefficient sparsity, 2021.\nGhiasi, G., Lin, T.-Y., and Le, Q. V. Nas-fpn: Learning\nscalable feature pyramid architecture for object detection.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 7036\u20137045, 2019.\nGross, S., Ranzato, M., and Szlam, A. Hard mixtures of\nexperts for large scale weakly supervised vision. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 6865\u20136873, 2017.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770\u2013778, 2016a.\nHe, K., Zhang, X., Ren, S., and Sun, J. Identity mappings\nin deep residual networks. In European conference on\ncomputer vision, pp. 630\u2013645. Springer, 2016b.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\nWelbl, J., Clark, A., et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556,\n2022.\nHua, W., Dai, Z., Liu, H., and Le, Q. Transformer quality\nin linear time. In International Conference on Machine\nLearning, pp. 9099\u20139117. PMLR, 2022.\nJaszczur, S., Chowdhery, A., Mohiuddin, A., Kaiser, L.,\nGajewski, W., Michalewski, H., and Kanerva, J. Sparse\nis enough in scaling transformers. Advances in Neural\nInformation Processing Systems, 34:9895\u20139907, 2021.\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Trivi-\naqa: A large scale distantly supervised challenge dataset\nfor reading comprehension. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics, Vancouver, Canada, July 2017. Association\nfor Computational Linguistics.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M.,\nParikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kel-\ncey, M., Devlin, J., Lee, K., Toutanova, K. N., Jones,\nL., Chang, M.-W., Dai, A., Uszkoreit, J., Le, Q., and\nPetrov, S. Natural questions: a benchmark for question\nanswering research. Transactions of the Association of\nComputational Linguistics, 2019.\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y.,\nKrikun, M., Shazeer, N., and Chen, Z. GShard: Scaling\ngiant models with conditional computation and automatic\nsharding. In International Conference on Learning Rep-\nresentations, 2021.\nLewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettle-\nmoyer, L. Base layers: Simplifying training of large,\nsparse models. In International Conference on Machine\nLearning, pp. 6265\u20136274. PMLR, 2021.\nLin, M., Fu, J., and Bengio, Y. Conditional computation\nfor continual learning. arXiv preprint arXiv:1906.06635,\n2019.\nLiu, H., Dai, Z., So, D., and Le, Q. V. Pay attention to mlps.\nAdvances in Neural Information Processing Systems, 34:\n9204\u20139215, 2021.\nMikolov, T., Karafi\u00e1t, M., Burget, L., Cernock`y, J., and\nKhudanpur, S.\nRecurrent neural network based lan-\nguage model. In Interspeech, volume 2, pp. 1045\u20131048.\nMakuhari, 2010.\nPaperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N.,\nBernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and\nFern\u00e1ndez, R. The lambada dataset: Word prediction\nrequiring a broad discourse context, 2016. URL https:\n//arxiv.org/abs/1606.06031.\nPress, O., Smith, N. A., and Levy, O. Improving transformer\nmodels by reordering their sublayers. arXiv preprint\narXiv:1911.03864, 2019.\nPuigcerver, J., Riquelme, C., Mustafa, B., Renggli, C., Pinto,\nA. S., Gelly, S., Keysers, D., and Houlsby, N. Scal-\nable transfer learning with expert models. arXiv preprint\narXiv:2009.13239, 2020.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\nI. Improving language understanding by generative pre-\ntraining. 2018.\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann,\nJ., Song, F., Aslanides, J., Henderson, S., Ring, R.,\nYoung, S., et al. Scaling language models: Methods,\nanalysis & insights from training gopher. arXiv preprint\narXiv:2112.11446, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res., 21(140):1\u201367, 2020.\n10\nBrainformers: Trading Simplicity for Efficiency\nRajpurkar, P., Jia, R., and Liang, P. Know what you don\u2019t\nknow: Unanswerable questions for squad, 2018. URL\nhttps://arxiv.org/abs/1806.03822.\nRoller, S., Sukhbaatar, S., Weston, J., et al. Hash layers\nfor large sparse models. Advances in Neural Information\nProcessing Systems, 34:17555\u201317566, 2021.\nShazeer, N. and Stern, M. Adafactor: Adaptive learning\nrates with sublinear memory cost. In International Con-\nference on Machine Learning, pp. 4596\u20134604. PMLR,\n2018.\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,\nQ., Hinton, G., and Dean, J. Outrageously large neural\nnetworks: The sparsely-gated mixture-of-experts layer.\narXiv preprint arXiv:1701.06538, 2017.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,\nJ., and Catanzaro, B.\nMegatron-lm: Training multi-\nbillion parameter language models using model paral-\nlelism. arXiv preprint arXiv:1909.08053, 2019.\nSo, D., Ma\u00b4nke, W., Liu, H., Dai, Z., Shazeer, N., and Le,\nQ. V. Searching for efficient transformers for language\nmodeling. Advances in Neural Information Processing\nSystems, 34:6010\u20136022, 2021.\nSutskever, I., Martens, J., and Hinton, G. E. Generating text\nwith recurrent neural networks. In ICML, 2011.\nTan, M. and Le, Q. Efficientnet: Rethinking model scal-\ning for convolutional neural networks. In International\nconference on machine learning, pp. 6105\u20136114. PMLR,\n2019.\nTay, Y., Bahri, D., Metzler, D., Juan, D.-C., Zhao, Z., and\nZheng, C. Synthesizer: Rethinking self-attention for\ntransformer models. In International conference on ma-\nchine learning, pp. 10183\u201310192. PMLR, 2021.\nTolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L.,\nZhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers,\nD., Uszkoreit, J., et al. Mlp-mixer: An all-mlp architec-\nture for vision. Advances in Neural Information Process-\ning Systems, 34:24261\u201324272, 2021.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. R. Glue: A multi-task benchmark and anal-\nysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461, 2018.\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A.,\nMichael, J., Hill, F., Levy, O., and Bowman, S. Super-\nglue: A stickier benchmark for general-purpose language\nunderstanding systems. Advances in neural information\nprocessing systems, 32, 2019.\nWang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H.\nLinformer: Self-attention with linear complexity. arXiv\npreprint arXiv:2006.04768, 2020.\nWu, L., Liu, M., Chen, Y., Chen, D., Dai, X., and\nYuan, L. Residual mixture of experts. arXiv preprint\narXiv:2204.09636, 2022.\nZhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V.,\nDai, A., Chen, Z., Le, Q., and Laudon, J. Mixture-of-\nexperts with expert choice routing, 2022. URL https:\n//arxiv.org/abs/2202.09368.\nZuo, S., Liu, X., Jiao, J., Kim, Y. J., Hassan, H., Zhang,\nR., Zhao, T., and Gao, J.\nTaming sparsely activated\ntransformer with stochastic experts.\narXiv preprint\narXiv:2110.04260, 2021.\n11\nBrainformers: Trading Simplicity for Efficiency\nA. You can have an appendix here.\nYou can have as much text here as you want. The main body must be at most 8 pages long. For the final version, one more\npage can be added. If you want, you can use an appendix like this one, even using the one-column format.\n12\n"
  },
  {
    "title": "Birth of a Transformer: A Memory Viewpoint",
    "link": "https://arxiv.org/pdf/2306.00802.pdf",
    "upvote": "1",
    "text": "Birth of a Transformer: A Memory Viewpoint\nAlberto Bietti\u2217\nFlatiron Institute\nVivien Cabannes\nFAIR, Meta\nDiane Bouchacourt\nFAIR, Meta\nHerv\u00e9 J\u00e9gou\nFAIR, Meta\nL\u00e9on Bottou\nFAIR, Meta\nAbstract\nLarge language models based on transformers have achieved great empirical suc-\ncesses. However, as they are deployed more widely, there is a growing need to\nbetter understand their internal mechanisms in order to make them more reliable.\nThese models appear to store vast amounts of knowledge from their training data,\nand to adapt quickly to new information provided in their context or prompt. We\nstudy how transformers balance these two types of knowledge by considering a\nsynthetic setup where tokens are generated from either global or context-specific\nbigram distributions. By a careful empirical analysis of the training process on a\nsimplified two-layer transformer, we illustrate the fast learning of global bigrams\nand the slower development of an \u201cinduction head\u201d mechanism for the in-context\nbigrams. We highlight the role of weight matrices as associative memories, provide\ntheoretical insights on how gradients enable their learning during training, and\nstudy the role of data-distributional properties.\n1\nIntroduction\nAs large language models (LLMs) are growing in usage and deployment, it is increasingly important\nto open the black box and understand how they work. A better understanding can help with inter-\npretability of how these models make decisions, and will be crucial to improve these models and\nmitigate their failure cases, such as hallucinations or reasoning errors.\nAn important ingredient in the success of recent LLMs is their ability to learn and reason from\ninformation present in their context [7]. These \u201cin-context\u201d learning capabilities are often attributed\nto the transformer architecture [53], in particular its self-attention blocks, which are able to carefully\nselect parts of the input sequence in order to infer plausible next tokens. Additionally, predictions\nmay require \u201cglobal\u201d knowledge, such as syntactic rules or general facts, which may not appear in\nthe context and thus needs to be stored in the model.\nIn order to better understand how transformers develop these capabilities during training, we introduce\na synthetic dataset that exhibits both aspects. It consists of sequences generated from a bigram\nlanguage model, but where some of the bigrams are specific to each sequence. Then, the model needs\nto rely on in-context learning for good prediction on the sequence-specific bigrams, while the global\nbigrams can be guessed from global statistics conditioned on the current token. While one-layer\ntransformers fail to reliably predict the in-context bigrams, we find that two-layer transformers\nsucceed by developing an induction head mechanism [17, 41], namely a \u201ccircuit\u201d of two attention\nheads that allows the transformer to predict b from a context [\u00b7 \u00b7 \u00b7 , a, b, \u00b7 \u00b7 \u00b7 , a], and which appears to\nbe ubiquitous in transformer language models [41, 56].\nIn order to obtain a fine-grained understanding of how this in-context mechanism emerges during\ntraining, we further simplify the two-layer architecture by freezing some of the layers at random\ninitialization, including embeddings and value matrices. This focuses our study on attention and\nfeed-forward mechanisms, while avoiding the difficulty of learning representations, which may\n\u2217Work done while at FAIR, Meta.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.00802v2  [stat.ML]  6 Nov 2023\nrequire complex nonlinear dynamics [16, 34, 46]. This simplification also allows us to introduce a\nnatural model for individual weight matrices as associative memories, which store input-output or\nkey-value pairs of embeddings through their outer products. Random high-dimensional embeddings\nare particularly well-suited to this viewpoint thanks to their near-orthogonality. We provide a detailed\nempirical study of the training dynamics, by measuring how quickly each weight matrix learns\nto behave as the desired associative memory, studying how this is affected by data-distributional\nproperties, and investigate the order in which layers are learned: the model first finds the right output\nassociations from the current token and from uniform attention patterns, then the attention heads learn\nto focus on the correct key-value pairs. We then present theoretical insights on this top-down learning\nprocess through population gradient dynamics. Despite its simplicity, our setup already provides\nuseful insights on the internal structure of transformer language models and its evolution throughout\ntraining, paving the way for a better understanding of LLMs. We hope that our insights may lead\nto future research and improvements for LLM practitioners, e.g., for optimization algorithms, data\npre-processing and selection, interpretability, fine-tuning, and model editing.\nIn summary, we make the following contributions:\n\u2022 We introduce a new synthetic setup to study global vs in-context learning: sequences follow bigram\nlanguage models, where some bigrams change across sequences and others do not.\n\u2022 We view the transformer\u2019s weight matrices as associative memories that learn to store specific pairs\nof embeddings, and use this to derive a simplified but more interpretable model for our task.\n\u2022 We empirically study the training dynamics with careful probing: global bigrams are learned first,\nthen the induction head is formed by learning appropriate memories in a top-down fashion.\n\u2022 We give theoretical insights on training dynamics, showing how a few top-down gradient steps on\nthe population loss can recover the desired associative memories by finding signal in noisy inputs.\nRelated work.\nAfter the success of transformer language models for in-context learning was\nfound [7], several works have studied how in-context learning may arise in various contexts [1, 9, 39,\n44, 49, 55, 59]. Multiple recent papers have introduced synthetic tasks in order to better understand\nand interpret transformers [10, 34, 40, 63]. Several works have attempted to understand internal\nmechanisms in transformers that are responsible for certain behaviors, an area known as \u201cmechanistic\ninterpretability\u201d [17, 18, 36, 40, 41, 56]. Memory and neural networks have a long history of\nconnections [6, 19, 20, 22, 27, 28, 31, 35, 51, 57, 58]. The associative memories we consider bear\nsimilarity to [30, 58], though we use continuous input/outputs. The reader may also be interested in\nFast Weight programmers [47, 48]. The use of random vectors for storing memories is related to [24].\nOur approach to probing based on memory recall is related to techniques in [14, 18], though motivated\ndifferently. [15, 33, 37] study statistical and approximation properties of transformers, highlighting\nbenefits of sparse attention patterns, but do not consider training dynamics. [26, 32, 50, 52] provide\ntheoretical analyses of learning dynamics in transformers and other attention models, but consider\ndifferent data setups and focus on single-layer architectures, while we focus on two-layer models and\ntake a different viewpoint based on associative memories.\n2\nBackground\nThis section provides background on transformer architectures and induction head mechanisms.\nTransformer architecture.\nTransformers [53] operate on sequences of embeddings by alternating\nself-attention operations and token-wise feed-forward layers. We focus on decoder-only, auto-\nregressive architectures with a causal attention mask, which are commonly used in large language\nmodels trained for next-token prediction [7, 12, 42, 43]. We ignore normalization layers in order to\nsimplify the architecture, since its stability benefits are not as crucial in the small models we consider.\nGiven an input sequence of tokens z1:T \u2208 [N]T of length T, where N is the vocabulary size, the\ntransformer operates as follows:\n\u2022 Token embeddings: each discrete token is mapped to a d-dimensional embedding via an em-\nbedding map WE \u2208 Rd\u00d7N. We will denote the embeddings of tokens zt by xt := wE(zt),\nwhere wE(j) is the j-th column of WE.\n\u2022 Positional embeddings: the positional embeddings pt \u2208 Rd, t \u2208 [T], are added to each token\nembedding depending on its position in the sequence, leading to the following input embeddings:\nxt := xt + pt = wE(zt) + pt.\n(1)\n2\nLayer 0\nSequence\nLayer 1\nLayer 2\npt\u22121\nwE(a)\npt\nwE(b)\npT \u22121\nwE(a)\na\nb\na\nb\n[\u00b7 \u00b7 \u00b7 ]\n\u2217\nwE(a)\nw1(a)\nwE(b)\n\u2217\nwE(a)\n\u2217\n\u2217\n\u2217\n\u2217\nwU(b)\nwE(a)\nAttn1: P\ns ps\u22121p\u22a4\ns\nW 1\nOW 1\nV\nResidual\nAttn2: P\nk w1(k)wE(k)\u22a4\nW 2\nOW 2\nV : P\nk wU(k)wE(k)\u22a4\nPrediction\nFigure 1: Induction head mechanism. Induction heads are a two-layer mechanism that can\npredict b from a context [. . . , a, b, . . . , a]. The first layer is a previous token head, which attends\nto the previous token based on positional embeddings (pt \u2192 pt\u22121) and copies it after a remapping\n(wE(a) \u2192 w1(a) := W 1\nOW 1\nV wE(a)). The second layer is the induction head, which attends based\non the output of the previous token head (wE(a) \u2192 w1(a)) and outputs the attended token, remapped\nto output embeddings (wE(b) \u2192 wU(b)). Boxes in the diagram represent different embeddings in\nsuperposition on each token\u2019s residual stream (we omit some irrelevant ones for clarity, e.g., positional\nembeddings in upper layers), and attention and output associations are shown with the associative\nmemory viewpoint presented in Section 4.\n\u2022 Attention blocks: given an input sequence x1:T \u2208 Rd\u00d7T of embeddings, the causal attention\nblock computes, for WK, WQ, WV , WO \u2208 Rd\u00d7d (key, query, value, output), and for each t,\nx\u2032\nt := WOWV x1:t\u03c3(x\u22a4\n1:tW \u22a4\nKWQxt) \u2208 Rd,\n(2)\nwhere \u03c3 takes the softmax of its elements, leading to an attention of the \u201cvalues\u201d WV xt with\nweights proportional to exp((WKxs)\u22a4(WQxt)). Note that the attention operation usually con-\nsiders multiple \u201cheads\u201d that each projects the input to a lower dimension. Here we stick to a\nsingle head for simplicity, since it will be sufficient for our purposes. Rewriting (2) on each t\nas x\u2032\n1:T = A(x1:T ; WK, WQ, WV , WO), the \u2113-th layer of the transformer applies attention with\nlayer-specific parameters along with a residual connection as follows:2\nx1:T := x1:T + A(x1:T ; W \u2113\nK, W \u2113\nQ, W \u2113\nV , W \u2113\nO)\n\u2022 Feed-forward blocks: feed-forward blocks operate on individual token embeddings after each\nattention block, typically by applying a one-hidden-layer MLP to each token, denoted F(\u00b7; WF ),\nwith a residual connection: at layer \u2113, we have\nxt := xt + F(xt; WF ).\nOur simplified setup will linear feed-forward layers: F(xt; WF ) = WF xt.\n\u2022 Unembedding: After the last transformer layer, the embeddings are mapped back to the vocabulary\nspace RN through a linear \u201cunembedding\u201d layer WU = [wU(1), . . . , wU(N)]\u22a4 \u2208 RN\u00d7d, where\nwe refer to the wU(j) as \u201coutput embeddings\u201d. The output of this layer is then fed into a cross-\nentropy loss for predicting of zt+1 from each xt.\nWe will sometimes refer to the representations xt for a given token t throughout layers as its residual\nstream [17], since they consist of sums of embeddings and layer outputs due to residual connections.\nInduction head mechanism.\nInduction heads [17, 41] are a particular type of mechanism (or\n\u201ccircuit\u201d) in transformers that allows basic in-context prediction of the form [\u00b7 \u00b7 \u00b7 , a, b, \u00b7 \u00b7 \u00b7 , a] \u2192 b.\nThese were found to be ubiquitous in transformer language models, playing a key role in enabling\nvarious forms of in-context learning. The basic mechanism consist of two attention heads in separate\nlayers (see Figure 1 for an illustration): (i) the first is a previous token head which attends to the\nprevious token using positional information and copies its embedding to the next token; (ii) the\nsecond is the induction head itself, which attends using the output of the previous token head, and\noutputs the original token. Our work focuses on this basic copy mechanism, but we note that richer\nbehaviors are possible, particularly when combining multiple such mechanisms (e.g., [56]).\n2We omit layer indices for simplicity of notation, and use the assignment operator := instead.\n3\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nr s a b t s L a b t s L , a b\nt h b n t L & C L & C a b t h\nFigure 2: Induction head behavior in attention maps observed on a 2-layer transformer trained\non two variants of our synthetic dataset. Each row shows the attention pattern for predicting the\nnext token. (left) The first layer head always attends to the previous token. (center) For fixed\ntriggers Q = {a, t}, the second layer head mainly attends to tokens following such triggers. (right)\nFor random triggers, the induction head mechanism is active for any repeated token (here the only\ntrigger is L). Red and green boxes highlight tokens following previous occurrences of the query, with\nred boxes corresponding to \u201ccorrect\u201d output tokens ok following trigger tokens qk.\n3\nSynthetic Setup\nIn this section, we introduce our synthetic data setup, which allows us to carefully study how the\ninduction head mechanism develops during training, and how transformers learn to use information\nfrom the context vs simple associations from the training data.\nBigram data model.\nOur model for sequences consists of a generic bigram language model (i.e.,\nMarkov chain), but where the transitions for a few trigger tokens denoted qk are modified in each\nsequence to always be followed by some output tokens ok. Let K be the number of trigger tokens,\nand fix the following distributions over the vocabulary [N]: \u03c0b(\u00b7|i), \u03c0u, \u03c0o(\u00b7|i) and \u03c0q, for i \u2208 [N].\n\u03c0b(\u00b7|i) are the global bigram conditionals, \u03c0u the global unigram distribution, while \u03c0o is used\nto sample output tokens at each sequence. The triggers are either fixed to some predefined set of\ntokens Q, or sampled from \u03c0q. Each sequence zn\n1:T is generated as follows:\n\u2022 (optional) Sample q1, . . . , qK \u223c \u03c0q, i.i.d. without replacement (random triggers)\n\u2022 Sample ok \u223c \u03c0o(\u00b7|qk), i.i.d. with replacement.\n\u2022 Sample zn\n1 \u223c \u03c0u and zn\nt |zn\nt\u22121 \u223c pn(\u00b7|zn\nt\u22121) for t = 2, . . . , T, where\npn(j|i) =\n\u001a\u03c0b(j|i),\nif i /\u2208 {qk}k\n1{j = ok},\nif i = qk.\nExperimental setup and initial experiment.\nOur experiments take \u03c0u and \u03c0b to be unigram and\nbigram character-level distributions estimated from the tiny Shakespeare dataset, with vocabulary\nsize N = 65. We generally sample triggers from \u03c0q = \u03c0u or fix them to the K most frequent tokens.\nWe sample uniform outputs ok in most cases, but also experiment with \u03c0o = \u03c0b in Section 5.\nAs a preliminary experiment, we train a two-layer vanilla transformer with single-head attention layers\nand MLP feed-forward layers, following the training setup described in Section 5. On our synthetic\ndata, with fixed (resp. random) triggers and uniform outputs, the model achieves over 99% accuracy\n(resp. 95%) on output tokens after the first occurrence, versus around 55% for one layer. This gap\nmay be related to the difficulty of modeling three-way interactions with a single attention layer [45].\nWe visualize attention maps on test sequences in Figure 2, which shows that the model has learned\nan induction head mechanism. The sequence in the middle figure has (qk, ok) \u2208 {(a, b), (t, s)}. For\nfixed triggers, the induction head is only active for the triggers used in training, which suggests the\npresence of a \u201cmemory\u201d in the attention layer. For random triggers, it is active on every repeated\ntoken, so that the model then needs to disambiguate between in-context and global predictions. For\ninstance, the model may choose to use the retrieved token when it is unlikely to be sampled from the\nglobal bigram distribution, something which we found to often be the case in practice.\n4\n4\nThe Associative Memory Viewpoint\nIn this section, we present our associative memory view on transformers: with nearly orthogonal\nembeddings, the weight matrices behave as associative memories which store pairs of embeddings as\na weighted sum of their outer products. We then introduce a simplified transformer model with fixed\nrandom embeddings that will yield a precise understanding of learning dynamics using this viewpoint.\n4.1\nWeight matrices as associative memories\nWhile intermediate representations in the transformer consist of high-dimensional vectors in residual\nstreams, they are often \u201ccollapsed\u201d down to scalar measurements by testing against other representa-\ntions, using operations of the form v\u22a4\nj Wui for some matrix W. For instance, ui and vj could be key\nand query vectors in an attention head, or input and output embeddings for predicting the next token.\nIf (ui)i and (vj)j are orthonormal (or nearly-orthonormal) sets of embeddings, a natural way to store\ndesired input-output associations i, j is through the following associative memory:\nW =\nX\ni,j\n\u03b1ijvju\u22a4\ni ,\n(3)\nso that the scores v\u22a4\nj Wui \u2248 \u03b1ij may be used to assess the relevance of the (i, j) pair, e.g., as part of\na softmax operation in attention or next token prediction.\nRandom embeddings.\nA simple way to ensure that embeddings (ui)i and (vj)j are nearly-\northonormal is to set them to be random high-dimensional vectors, such as Gaussian vectors with\nvariance 1/d in d dimensions. Indeed, these are known to satisfy [24, 54]\nu\u22a4\ni ui \u2248 1\nand\nu\u22a4\ni uj \u2248 O\n\u0012 1\n\u221a\nd\n\u0013\n,\nso that (3) is a reasonable way to define an associative memory, without requiring an explicit activation\nfunction as employed in end-to-end memory networks [51]. We may also easily create a \u201cremapping\u201d\nof an existing embedding ui by multiplying it by a random matrix W0 \u2208 Rd\u00d7d with Gaussian\nentries of variance 1/d, which is commonly used for initializing neural network parameters. The\nnew remapped embedding W0ui is near-unit norm, and is near-orthogonal to ui in addition to\nthe other uj. Note that this fact implies that attention scores at initialization are near-uniform. See\nAppendix A for more details.\nLearning associative memories.\nWe now show that learning associations of input-output embed-\ndings via gradient descent leads to a weighted associative memory of a form similar to (3).\nLemma 1 (Gradients and associative memories). Let p be a data distribution over input-output tokens,\nand consider the following loss, where the input and output embeddings WE and WU are fixed:\nL(W) = E(z,y)\u223cp[\u2113(y, WUWwE(z))],\n(4)\nwith \u2113 the cross-entropy loss. The gradients of the population loss L then take the form\n\u2207W L(W) =\nN\nX\nk=1\nEz[(\u02c6pW (y = k|z) \u2212 p(y = k|z))wU(k)wE(z)\u22a4],\n(5)\nwhere \u02c6pW (y=k|x) = \u03c3(WUWwE(z))k are the model\u2019s predicted probabilities. Running gradient\ndescent (with or without weight decay) from initialization W0 then leads to estimates of the following\nform, for some \u03b10 and \u03b1ij that vary with the number of iterations:\n\u02c6W = \u03b10W0 +\nX\ni,j\n\u03b1ijwU(j)wE(i)\u22a4.\n(6)\nNote that (4) is a convex problem in W, thus with appropriate step-size and large enough number of\nsteps (with no weight decay) we can expect gradient descent to be close to the global minimum. At\nthe optimum, if the embeddings are nearly orthogonal, then (5) implies \u02c6pW (y = k|z) \u2248 p(y = k|z).\nWe remark that if W0 is a Gaussian random matrix, as if often the case for neural network layers, the\nfirst term in (6) plays a minor role: testing W0 against an input-output pair (i, j) with \u03b1ij \u0338= 0 will\nconcentrate around zero when d is large, while the (i, j) term in the sum will concentrate around \u03b1ij.\nWe also note that the gradient updates described above correspond to a so-called maximal feature\nlearning regime similar to \u00b5P updates in intermediate layers of deep networks [60, 61].\n5\nHandling superposition.\nIn Lemma 1, we assumed that inputs to the matrix W are embeddings\nof a single token. Yet, in transformer models, the inputs to weight matrices are often sums, or\nsuperpositions of embeddings. For instance, the initial representations of each token are sums of\ntoken and positional embeddings, and representations at later layers are sums of the outputs of\neach previous block, due to residual connections. Outputs of attention layers are also weighted\nsums of potentially many embeddings, at least initially when attention patterns are spread out. By\nlinearity, associative memories of the form (6) simply operate individually on each embedding of a\nsuperposition, and return a new superposition (up to additional noise due to near-orthogonality). In\npractice, we will see that learned memories often focus on a single embedding and filter out the rest as\nnoise when irrelevant (see also Section 6). We note that linearity can also be limiting, since it makes\nit difficult to map sets to specific output embeddings: u{i,j} := ui + uj needs to map to Wui + Wuj,\nand thus cannot map to a new embedding v{i,j}. Such mappings of sets thus require non-linear\nassociative memories, for instance by leveraging a sparse decoding of which elements are actually\npresent (e.g., using compressed sensing), or by using MLPs with non-linear activations [16, 31].\n4.2\nA simplified two-layer transformer architecture\nWe consider a simpler two-layer transformer which is more interpretable with the memory viewpoint,\nand will help us analyze learning dynamics both empirically and theoretically.\n\u2022 We freeze input, output and positional embeddings (WE(k), WU(k), pt) to their random initializa-\ntion throughout training. This brings us to the Gaussian random vector setup presented above.\n\u2022 We fix W 1\nQ = W 2\nQ = Id, so that W 1\nK and W 2\nK play the role of both key and query matrices. This\nchanges the gradient dynamics, but simplifies the model by avoiding the redundancy in (2). The\npre-softmax attention scores then take the form x\u22a4\nq W \u2113\nKxk, with xq (resp. xk) the query (resp. key)\nembeddings, which now directly resembles an associative memory lookup.\n\u2022 We freeze W 1\nV , W 1\nO, and W 2\nV to random initialization. These play the role of remapping attended\ntokens into new tokens, since for random W and large d, Wx is nearly orthogonal to x and to any\nother random embeddings independent of x.\n\u2022 We train W 2\nO, since the outputs of the induction head need to be mapped back into appropriate\noutput embeddings in order to predict the output tokens ok correctly.\n\u2022 We use a single linear feedforward layer after the second attention block, with weight matrix WF .\nThis is plausibly the layer responsible for learning the global bigram distributions.\nWe remark that while this model freezes some parameters at initialization, it is richer than a \u201clazy\u201d\nor neural tangent kernel approximation [11, 23, 25] since the model is still highly non-linear in its\nparameters and, as we will see, induces rich non-linear learning dynamics.\nSolving the bigram problem with associative memories.\nWe now show how the above architecture\ncan solve the synthetic bigram problem from Section 3 with well-chosen weight matrices. While this\nis only a hypothetical model, we show in Section 5 that it is surprisingly faithful to the learned model.\nRecall that due to residual connections, the inputs to the weight matrices typically consist of superpo-\nsitions of various embeddings including token embeddings, positional embeddings, or \u201cremapped\u201d\nversions thereof. These may be viewed as sets, as illustrated in Figure 1, and associative memories\ncan easily ignore certain elements of the set, e.g., ignore token embeddings by only focusing on\npositional embeddings. The induction head mechanism can be obtained by setting:\nW 1\nK =\nT\nX\nt=2\nptp\u22a4\nt\u22121,\nW 2\nK =\nX\nk\u2208Q\nwE(k)(W 1\nOW 1\nV wE(k))\u22a4,\nW 2\nO =\nN\nX\nk=1\nwU(k)(W 2\nV wE(k))\u22a4,\n(7)\nwhere Q is the set of triggers when they are fixed, or the support of \u03c0q when they are random. In\nwords, the first attention layer matches a token to the previous tokens using positional embeddings.\nThe second layer matches the trigger token to a remapping of itself by W 1\nOW 1\nV , and the output\nmatches a remapping of the input token by W 2\nV to the corresponding output token. We remark that\none can easily make the attention patterns more peaked on the correct associations by rescaling W 1\nK\nand W 2\nK. The global bigram statistics can be encoded in the feed-forward layer as follows:\nWF =\nN\nX\ni=1\nN\nX\nj=1\nlog \u03c0b(j|i)wU(j)wE(i)\u22a4.\n(8)\n6\nThe question remains of how the model could trade-off predictions from the induction head and from\nthe feed-forward layer, which are added together due to residual connections. With fixed triggers Q,\nwe may simply remove all i \u2208 Q from the summation in (8), so that the model exclusively relies\non the attention head for all triggers (indeed, the output of W 2\nO is in the span of output embeddings,\nwhich are nearly orthogonal to the row space of WF ). When the triggers can vary across different\nsequences, choosing between the induction head and the feed-forward layer is more ambiguous as\nit depends on context, and WF may try to learn more complex mappings that also use the outputs\nof W 2\nO. In practice, we observe that the model often prefers the induction head, unless its output\nagrees with one of the top predictions from the global bigram, in which case it tends to prefer those.\nBeyond the simplified architecture.\nWhile our simplified architecture already captures the relevant\naspects for the bigram model, it lacks some of the components that appear in standard transformers,\nsuch as non-linear MLPs, trained embeddings, layer normalization, and joint learning of a factoriza-\ntion W \u22a4\nKWQ (potentially with low rank matrices WK, WQ \u2208 Rdh\u00d7d with dh < d as in multi-head\nattention), instead of a single matrix WK. In practice, transformers also involve many more layers,\nas well as multiple heads at each self-attention layer. In Appendix D, we discuss how our memory\nviewpoint naturally extends to such architectural components, and we illustrate in Appendix E that\nthey empirically lead to similar observations. Nonetheless, we focus on our simpler architecture in the\nmain paper due to simplicity of exposition and better interpretability thanks to a clear identifiability\nof the role of each matrix, which is lost in models with more heads and layers.\n5\nEmpirical Study\nIn this section, we present our empirical analysis of learning dynamics on the bigram data defined in\nSection 3, for the simplified architecture defined in Section 4.2. See Appendix E for additional results.\nOur code is available at https://github.com/albietz/transformer-birth.\nExperimental setup.\nWe train our models using mini-batch SGD with momentum, where each\nbatch consists of 512 fresh sequences of length T = 256 sampled from our synthetic model. We use\na fixed learning rate and weight decay. Hyperparameters are given in Appendix E. Unless otherwise\nnoted, we use d = 128, random triggers with \u03c0q = \u03c0u and uniform output tokens. The reported\naccuracies and losses are computed over each fresh batch before it is used for optimization, and\nare averaged over relevant tokens: \u201cin-context accuracy/loss\u201d numbers only consider predictions\nof output tokens on triggers starting at the second occurrence (the first is non-deterministic), while\n\u201cglobal loss\u201d refers to average loss on non-trigger tokens.\nMemory recall probes.\nIn addition to loss and accuracy, we consider metrics to check whether\nindividual matrices have learned the desired associative memories: for a desired target memory W\u2217 =\nP\n(i,j)\u2208M vju\u22a4\ni , the corresponding recall metric is computed from the empirical estimate \u02c6W as\nR( \u02c6W, W\u2217) =\n1\n|M|\nX\n(i,j)\u2208M\n1{arg max\nj\u2032\nv\u22a4\nj\u2032 \u02c6Wui = j}.\n(9)\nWe use this for each matrix in (7) as target, and additionally test the previous token matrix W 1\nK on\nsmaller time windows. For the final feed-forward layer, we measure the average KL divergence\nbetween the predicted softmax distribution using only WF and the global bigram distribution \u03c0b:\ndKL(WF , \u03c0b) := 1\nN\nN\nX\nk=1\ndKL(\u03c3(WUWF wE(k)), \u03c0b(\u00b7|k)).\n(10)\nEmergence of the induction head via top-down learning.\nWe begin our study by only training\nto minimize the loss on trigger-output token predictions after their first occurrence. This should\nbe predictable with 100% accuracy using the two-layer induction head mechanism according to\nSection 4. We also remove the feed-forward layer, in order to focus on the learning of attention\nmatrices W 1\nK, W 2\nK and W 2\nO in isolation.\nFigure 3 studies the effect of freezing different layers until iteration 300 on the training dynamics.\nBy looking at memory recall probes, we see that training key-query matrices does not lead to any\n7\n0\n200\n400\n600\n800\n1000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nin-context accuracy\nfreeze W2\nO\n0\n200\n400\n600\n800\n1000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nfreeze W1\nK and W2\nK\n0\n200\n400\n600\n800\n1000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nfreeze W2\nK\n0\n200\n400\n600\n800\n1000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nfreeze W1\nK\n0\n200\n400\n600\n800\n1000\niteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmemory recall\n0\n200\n400\n600\n800\n1000\niteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\n1000\niteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n200\n400\n600\n800\n1000\niteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWo2\nWk2\nWk1 (t<64)\nWk1\nFigure 3: Learning the induction head alone: in-context accuracy (top) and recall probes\n(bottom) with some layers frozen until iteration 300. The output matrix W 2\nO can and must be learned\nbefore the key-query matrices, but does not suffice for good accuracy. It is easier to learn W 2\nK\nbefore W 1\nK, and W 1\nK stores initial context positions (t < 64) much faster than late positions.\nlearning unless W 2\nO is learned first, and that W 2\nO can learn the correct associations even when trained\nby itself with key-value matrices at random initialization. Recall that the attention weights are\nessentially uniform when WK are at random initialization, so that training W 2\nO alone resembles a\nbag-of-words models that aggregates representations throughout the sequence. While such a model\nhas poor prediction accuracy, it is nevertheless sufficient to recover the correct associations in W 2\nO (a\nsimilar observation was made in [50] in a different setup).\nThen, these associations enable learning key-query matrices that focus the attention on relevant\ntokens, by storing relevant key-query pairs in the form of associative memories, which eventually\nrecovers the desired induction head behavior and leads to near-perfect accuracy. The two rightmost\nplots suggest that the second layer is learned before the first, in the sense that W 2\nK is easier to learn\nwhen W 1\nK is frozen compared to the reverse, yet learning them together seems beneficial, possibly\ndue to helpful feedback loops [2]. We also observe that W 1\nK fits previous token associations for early\npositions much faster than later positions (purple vs gray line). This is likely due to the fact that it\nshould be enough for the previous token head to attend to the first appearance of each trigger qk,\nwhich is typically early in the sequence, so that most of the gradient will focus on early positions.\nOverall, this provides a fine-grained understanding of the learning dynamics of induction heads.\nIn Section 6, we analyze how a few gradient steps in a top-down fashion may suffice to recover\nappropriate associative memories in high dimension and with enough data. See Appendix E for\nadditional experiments, including on the role of dimensionality.\nGlobal vs in-context learning.\nFigure 4(left/right) shows that when training all layers jointly, the\nglobal bigram statistics tend to be learned more quickly than the induction head, as seen from the\nquick drop in loss and KL in early iterations. The W 2\nO probe also seems to improve quickly initially,\nbut only leads to mild improvements to in-context predictions. The full learning of the in-context\nmechanism takes longer, likely due to slower dynamics of the key-query matrices. We also observe a\ntension between W 2\nO and WF later in training, leading to slight degradations of our probe metrics.\nThis may be due to the fact that the input to WF now contains additional signal from the induction\nhead which may be leveraged for better predictions, in particular for disambiguation in the case of\nrandom triggers, so that our guess of memories in Section 4.2 may no longer be accurate.\nRole of the data distribution.\nWe can see in Figure 4(left) that changes to the data distribution\ncan have a significant effect on the speed of learning the in-context mechanism. We observe that\nthe following may slow down in-context learning: (i) a smaller number of triggers K, (ii) using\nonly rare fixed triggers, and (iii) using random triggers instead of fixed triggers. By inspecting the\nindividual memory probes (see Figure 5 in Appendix E), we hypothesize that (i) and (ii) are due\nto slow learning of W 2\nO, while (iii) is more related to slow learning of key-query matrices. This is\nreasonable since (i-ii) reduce the number of overall output tokens in the data, while (iii) increases the\nnumber of possible trigger tokens that should be stored in W 2\nK, thus increasing the data requirements\nin order to learn the full associative memory. We also show in Figure 4(center) that changing the\n8\n0\n100\n200\n300\n400\n500\niteration\n0\n1\n2\n3\n4\n5\nloss\nin-context vs global loss\nK=1, fix q frequent\nK=5, fix q rare\nK=1, random q\nK=5, random q\n0\n100\n200\n300\n400\n500\niteration\n0.0\n0.2\n0.4\n0.6\n0.8\nin-context accuracy\nout of distribution\nunif (train unif)\nbigram (train unif)\nunif (train bigram)\nbigram (train bigram)\n0\n100\n200\n300\n400\n500\niteration\n0.0\n0.2\n0.4\n0.6\n0.8\nmemory recall\nWo2 recall\nWk2 recall\nWk1 recall (t<64)\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\nKL(WF,\nb)\nattention and feed-forward probes\nWf KL\nFigure 4: Global vs in-context learning and data-distributional effects. (left) Loss on global\n(dashed) vs in-context (solid) tokens throughout training, for fixed or random trigger tokens qk. The\nred curves fixes the trigger q1 to the most frequent token, while the fixed triggers in blue curves\nare less common. (center) In-context accuracy with different training and test distributions \u03c0o for\noutput tokens. Uniform leads to better generalization than global bigrams \u03c0b. (right) Probe metrics\nthroughout training: W 2\nO and WF eventually compete and deviate from our natural estimates.\noutput token distribution to bigram distributions at training time reduces the in-context accuracy\nwhen using out-of-distribution output tokens, while training on uniform outputs performs well on\nboth distributions. This highlights that using a more diverse training distribution can lead to models\nwith better generalization accuracy, with little additional training cost.\nAdditional experiments.\nIn Appendix E, we provide additional experimental results for varying\ndimensionality, more complex architectures and training methods, as well as more fine-grained\nvisualizations of the memory associations.\n6\nTheoretical Insights on Learning Dynamics\nIn this section, we provide theoretical insights on how gradients near initialization may allow the\nemergence of induction heads, and how this behavior is affected by data-distributional properties.\nFinding signal in noisy inputs.\nIn Lemma 1, we showed how gradient dynamics on a simple\nclassification task with fixed embeddings of the inputs and outputs lead to associative memories.\nWe now show that when inputs consist of superpositions of multiple embeddings, as is the case in\nthe transformer residual streams, gradients may learn associative memories that filter out irrelevant\ncomponents of these superpositions, focusing on useful signal instead.\nLemma 2 (Gradient associative memory with noisy inputs). Let p be a data distribution on (x, y) \u2208\nRd \u00d7 [N], and consider the following classification problem, with fixed output embeddings WU:\nL(W) = E(x,y)\u223cp[\u2113(y, WUWx)].\nThe gradients take the following form: denoting \u00b5k := E[x|y = k] and \u02c6\u00b5k := Ex[ \u02c6pW (k|x)\np(y=k) x],\n\u2207W L(W) =\nN\nX\nk=1\np(y = k)wU(k)(\u02c6\u00b5k \u2212 \u00b5k)\u22a4.\nThe key takeaway from this lemma is that with enough data (here infinite data), the associative\nmemory arising from gradients can learn to filter out noise from inputs, since it only depends on\nits expectations or conditional expectations. In particular, \u00b5k can isolate relevant parts of x that are\npredictive of a label k, and thus can lead to the right associations.\nAn illustrative example.\nTo gain more intuition about this result, consider the following example:\nwe would like to predict y from x = wE(y) + pt, where pt is a positional embedding at a random\nposition t \u2208 [T], which we would like to ignore. Further assume that y is uniformly distributed\nwith p(y = k) = 1/N, and consider the matrix obtained after one population gradient step with\nstep-size \u03b7 starting from an initialization W0 = 0 (so that \u02c6pW0(k|x) = 1/N):\nW1 = \u03b7\nN\nN\nX\nk=1\nwU(k)(\u00b5k \u2212 \u00af\u00b5)\u22a4,\n9\nwith \u00af\u00b5 = E[x]. We show in Appendix B that when d is large enough to ensure near-orthonormal\nembeddings, we have\nwU(k)\u22a4W1(wE(y) + pt) \u2248 \u03b7\nN 1{k = y} + O\n\u0012 1\nN 2\n\u0013\n,\nso that for large enough N and T, we obtain a near-perfect classifier that ignores the positional\nembedding, after just one gradient step (but a highly idealized one). Understanding how this translates\nto the finite dimension and finite sample regime is an important theoretical question that we leave\nfor future work (see [8] for an initial step in that direction). We note that data models related to the\nabove have been useful to study gradient dynamics of neural networks on continuous data [3, 26, 29].\nUsing a single gradient step to learn representations has also been fruitful in other contexts [4, 13].\nLearning the induction head with gradients.\nWe may extend the arguments above to show how a\nfew gradient steps can learn the induction head mechanism. We show the following in Appendix B.3.\nTheorem 3 (Learning induction head via three gradient steps, informal). In a simplified setup, the\ninduction head mechanism as constructed in (7) can be learned via sequential gradient steps on the\npopulation loss from random initialization, on W 2\nO, then W 2\nK, followed by W 1\nK.\nTo show this result, we use Lemma 2 in a similar manner to the illustrative example above to show\nhow training W 2\nO by itself at initialization, i.e., when the attention patterns are near-uniform, can\nrecover the desired associative memory. This is possible because when predicting an output token at\nlater occurrences of a trigger, the same output token is guaranteed to be present in the context, while\nother tokens need not appear more relative to other sequences. See also Figure 9 in Appendix E for\nnumerical experiments verifying this for finite data and dimension. Once W 2\nO has learned the correct\nassociations, we show that the gradient with respect to the key-value matrix W 2\nK at zero initialization\ncan leverage the correctness of W 2\nO to find the right associative memory that focuses the attention on\ncorrect triggers. Finally, by linearizing the second-layer attention around W 2\nK = 0, we show how\ngradients w.r.t. W 1\nK may learn correct associations for the previous token head.\n7\nDiscussion\nIn this paper, we studied the question of how transformers develop in-context learning abilities,\nusing a simplified setup that allows a fine-grained understanding the model and its training dynamics.\nWhile our model already captures rich phenomena at play in the bigram task we consider, more\nelaborate models are likely needed to understand transformers trained on more complex tasks like\nlanguage modeling. This includes learning embeddings that are more adapted to the data and more\nstructured (e.g., word embeddings [38, 32], or grokking [34, 40]), factorized key-query and value-\noutput matrices that may induce additional regularization effects [21], and non-linear feedforward\nlayers, which may provide richer associative memories between sets of embeddings. Understanding\nhow transformers leverage such aspects to learn in richer settings is an important next step.\nAcknowledgments and Disclosure of Funding\nThe authors thank Sainbayar Sukhbaatar and Shubham Toshniwal for helpful discussions.\nReferences\n[1] E. Aky\u00fcrek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is\nin-context learning? investigations with linear models. In Proceedings of the International\nConference on Learning Representations (ICLR), 2023.\n[2] Z. Allen-Zhu and Y. Li. Backward feature correction: How deep learning performs deep\nlearning. In Conference on Learning Theory (COLT), 2023.\n[3] Z. Allen-Zhu and Y. Li. Towards understanding ensemble, knowledge distillation and self-\ndistillation in deep learning. In Proceedings of the International Conference on Learning\nRepresentations (ICLR), 2023.\n10\n[4] J. Ba, M. A. Erdogdu, T. Suzuki, Z. Wang, D. Wu, and G. Yang. High-dimensional asymptotics\nof feature learning: How one gradient step improves the representation. Advances in Neural\nInformation Processing Systems (NeurIPS), 2022.\n[5] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016.\n[6] T. Bricken and C. Pehlevan. Attention approximates sparse distributed memory. Advances in\nNeural Information Processing Systems (NeurIPS), 2021.\n[7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2020.\n[8] V. Cabannes, E. Dohmatob, and A. Bietti. Scaling laws for associative memories. arXiv preprint\narXiv:2310.02984, 2023.\n[9] S. Chan, A. Santoro, A. Lampinen, J. Wang, A. Singh, P. Richemond, J. McClelland, and F. Hill.\nData distributional properties drive emergent in-context learning in transformers. In Advances\nin Neural Information Processing Systems (NeurIPS), 2022.\n[10] F. Charton. What is my math transformer doing?\u2013three results on interpretability and general-\nization. arXiv preprint arXiv:2211.00170, 2022.\n[11] L. Chizat, E. Oyallon, and F. Bach. On lazy training in differentiable programming. Advances\nin Neural Information Processing Systems (NeurIPS), 2019.\n[12] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311, 2022.\n[13] A. Damian, J. Lee, and M. Soltanolkotabi. Neural networks can learn representations with\ngradient descent. In Conference on Learning Theory (COLT), 2022.\n[14] G. Dar, M. Geva, A. Gupta, and J. Berant. Analyzing transformers in embedding space. arXiv\npreprint arXiv:2209.02535, 2022.\n[15] B. L. Edelman, S. Goel, S. Kakade, and C. Zhang. Inductive biases and variable creation\nin self-attention mechanisms. In Proceedings of the International Conference on Machine\nLearning (ICML), 2022.\n[16] N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds,\nR. Lasenby, D. Drain, C. Chen, R. Grosse, S. McCandlish, J. Kaplan, D. Amodei, M. Wattenberg,\nand C. Olah. Toy models of superposition. Transformer Circuits Thread, 2022.\n[17] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen,\nT. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones,\nJ. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish,\nand C. Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread,\n2021.\n[18] M. Geva, J. Bastings, K. Filippova, and A. Globerson. Dissecting recall of factual associations\nin auto-regressive language models. arXiv preprint arXiv:2304.14767, 2023.\n[19] M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer feed-forward layers are key-value\nmemories. In Conference on Empirical Methods in Natural Language Processing (EMNLP),\n2021.\n[20] A. Graves, G. Wayne, and I. Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401,\n2014.\n[21] S. Gunasekar, B. E. Woodworth, S. Bhojanapalli, B. Neyshabur, and N. Srebro. Implicit\nregularization in matrix factorization. Advances in Neural Information Processing Systems\n(NIPS), 2017.\n[22] J. J. Hopfield. Neural networks and physical systems with emergent collective computational\nabilities. Proceedings of the national academy of sciences, 79(8):2554\u20132558, 1982.\n[23] J. Hron, Y. Bahri, J. Sohl-Dickstein, and R. Novak. Infinite attention: Nngp and ntk for deep\nattention networks. In Proceedings of the International Conference on Machine Learning\n(ICML), 2020.\n11\n[24] A. Iscen, T. Furon, V. Gripon, M. Rabbat, and H. J\u00e9gou. Memory vectors for similarity search\nin high-dimensional spaces. IEEE transactions on big data, 4(1):65\u201377, 2017.\n[25] A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in\nneural networks. Advances in Neural Information Processing Systems (NeurIPS), 2018.\n[26] S. Jelassi, M. Sander, and Y. Li. Vision transformers provably learn spatial structure. In\nAdvances in Neural Information Processing Systems (NeurIPS), 2022.\n[27] Y. Jiang and C. Pehlevan. Associative memory in iterated overparameterized sigmoid au-\ntoencoders. In Proceedings of the International Conference on Machine Learning (ICML),\n2020.\n[28] A. Joulin and T. Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets.\nAdvances in Neural Information Processing Systems (NIPS), 2015.\n[29] S. Karp, E. Winston, Y. Li, and A. Singh. Local signal adaptivity: Provable feature learning in\nneural networks beyond kernels. Advances in Neural Information Processing Systems (NeurIPS),\n2021.\n[30] T. Kohonen. Correlation matrix memories. IEEE Transactions on Computers, 1972.\n[31] D. Krotov and J. J. Hopfield. Dense associative memory for pattern recognition. Advances in\nNeural Information Processing Systems (NIPS), 2016.\n[32] Y. Li, Y. Li, and A. Risteski. How do transformers learn topic structure: Towards a mechanistic\nunderstanding. In Proceedings of the International Conference on Machine Learning (ICML),\n2023.\n[33] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Transformers learn shortcuts to\nautomata. In Proceedings of the International Conference on Learning Representations (ICLR),\n2023.\n[34] Z. Liu, O. Kitouni, N. S. Nolte, E. Michaud, M. Tegmark, and M. Williams. Towards un-\nderstanding grokking: An effective theory of representation learning. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2022.\n[35] R. McEliece, E. Posner, E. Rodemich, and S. Venkatesh. The capacity of the hopfield associative\nmemory. IEEE transactions on Information Theory, 33(4):461\u2013482, 1987.\n[36] K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in\ngpt. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n[37] W. Merrill, A. Sabharwal, and N. A. Smith. Saturated transformers are constant-depth threshold\ncircuits. Transactions of the Association for Computational Linguistics, 10:843\u2013856, 2022.\n[38] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations\nin vector space. In Proceedings of the International Conference on Learning Representations\n(ICLR), 2013.\n[39] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer.\nRethinking the role of demonstrations: What makes in-context learning work? In Conference\non Empirical Methods in Natural Language Processing (EMNLP), 2022.\n[40] N. Nanda, L. Chan, T. Liberum, J. Smith, and J. Steinhardt. Progress measures for grokking\nvia mechanistic interpretability. In Proceedings of the International Conference on Learning\nRepresentations (ICLR), 2023.\n[41] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell,\nY. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston,\nA. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan,\nS. McCandlish, and C. Olah. In-context learning and induction heads. Transformer Circuits\nThread, 2022.\n[42] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are\nunsupervised multitask learners. Technical report, OpenAI, 2019.\n[43] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson,\nR. Ring, S. Young, et al. Scaling language models: Methods, analysis & insights from training\ngopher. arXiv preprint arXiv:2112.11446, 2021.\n12\n[44] Y. Razeghi, R. L. Logan IV, M. Gardner, and S. Singh. Impact of pretraining term frequencies\non few-shot reasoning. In Conference on Empirical Methods in Natural Language Processing\n(EMNLP), 2022.\n[45] C. Sanford, D. Hsu, and M. Telgarsky. Representational strengths and limitations of transformers.\nIn Advances in Neural Information Processing Systems (NeurIPS), 2023.\n[46] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of\nlearning in deep linear neural networks. In Proceedings of the International Conference on\nLearning Representations (ICLR), 2014.\n[47] I. Schlag, K. Irie, and J. Schmidhuber. Linear transformers are secretly fast weight programmers.\nIn Proceedings of the International Conference on Machine Learning (ICML), 2021.\n[48] J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent\nnetworks. Neural Computation, 4(1):131\u2013139, 1992.\n[49] S. Shin, S.-W. Lee, H. Ahn, S. Kim, H. Kim, B. Kim, K. Cho, G. Lee, W. Park, J.-W. Ha, et al.\nOn the effect of pretraining corpora on in-context learning by a large-scale language model. In\nNorth American Chapter of the Association for Computational Linguistics (NAACL), 2022.\n[50] C. Snell, R. Zhong, D. Klein, and J. Steinhardt. Approximating how single head attention learns.\narXiv preprint arXiv:2103.07601, 2021.\n[51] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus. End-to-end memory networks. Advances in\nNeural Information Processing Systems (NIPS), 2015.\n[52] Y. Tian, Y. Wang, B. Chen, and S. Du. Scan and snap: Understanding training dynamics\nand token composition in 1-layer transformer. In Advances in Neural Information Processing\nSystems (NeurIPS), 2023.\n[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems\n(NIPS), 2017.\n[54] R. Vershynin. High-dimensional probability: An introduction with applications in data science,\nvolume 47. Cambridge university press, 2018.\n[55] J. Von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov,\nand M. Vladymyrov. Transformers learn in-context by gradient descent. In Proceedings of the\nInternational Conference on Machine Learning (ICML), 2023.\n[56] K. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild:\na circuit for indirect object identification in gpt-2 small. In Proceedings of the International\nConference on Learning Representations (ICLR), 2023.\n[57] J. Weston, S. Chopra, and A. Bordes. Memory networks. In Proceedings of the International\nConference on Learning Representations (ICLR), 2015.\n[58] D. J. Willshaw, O. P. Buneman, and H. C. Longuet-Higgins. Non-holographic associative\nmemory. Nature, 222(5197):960\u2013962, 1969.\n[59] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as\nimplicit bayesian inference. In Proceedings of the International Conference on Learning\nRepresentations (ICLR), 2022.\n[60] G. Yang and E. J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks.\nIn Proceedings of the International Conference on Machine Learning (ICML), 2021.\n[61] G. Yang, E. J. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki, W. Chen,\nand J. Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter\ntransfer. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\n[62] B. Zhang and R. Sennrich. Root mean square layer normalization. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2019.\n[63] Y. Zhang, A. Backurs, S. Bubeck, R. Eldan, S. Gunasekar, and T. Wagner. Unveiling transform-\ners with lego: a synthetic reasoning task. arXiv preprint arXiv:2206.04301, 2022.\n13\nA\nAssociative Memories with Random Vectors\nIn this section, we provide basic properties of associative memories based on outer products of\nrandom Gaussian embeddings, as described in Section 4.\nWe consider embeddings uk \u2208 Rd with i.i.d. Gaussian N(0, 1\nd) entries.\nWe recall a few facts:\n\u2022 (Norm) We have u\u22a4\ni ui \u2248 1. This is standard from the concentration of random vectors in high\ndimension (see, e.g., [54, Theorem 3.1.1]).\n\u2022 (Near-orthogonality) For i \u0338= j, we have u\u22a4\ni uj = O(1/\n\u221a\nd). To see this, denoting ui =\nd\u22121/2(\u02dcuik)k, where \u02dcuik are the unnormalized entries of ui, note that we have\n\u221a\ndu\u22a4\ni uj =\n1\n\u221a\nd\nd\nX\nk=1\n\u02dcuik\u02dcujk \u2192 N(0, 1),\nby the central limit theorem, since for each k, the quantities \u02dcuik\u02dcujk are zero-mean, unit-variance,\ni.i.d. random variables.\n\u2022 (Remapping: norm) If W is a Gaussian random matrix with i.i.d. N(0, 1\nd) entries, then for any\nfixed x we have \u2225Wx\u2225 \u2248 \u2225x\u2225. This follows from Johnson-Lindenstrauss (see, e.g., [54, Lemma\n5.3.2 and Exercise 5.3.3]). In particular, if x is a normalized Gaussian embedding as above,\nthen \u2225Wx\u2225 \u2248 1.\n\u2022 (Remapping: near-orthogonality) Consider a random vector x =\n1\n\u221a\nd \u02dcx and a random matrix W =\n1\n\u221a\nd \u02dcW, where the entries of \u02dcx and \u02dcW are i.i.d. N(0, 1). Then x and Wx are nearly orthogonal. To\nsee this, note that E[x\u22a4Wx] = E[x\u22a4 E[W]x] = 0, and the variance is\nE(x\u22a4Wx)2 = E\nX\ni,j\nx2\ni W 2\nijx2\nj = 1\nd3 E\nX\ni\n\u02dcx4\ni \u02dcW 2\nii + E\nX\ni\u0338=j\n\u02dcx2\ni \u02dcx2\nj \u02dcW 2\nij\n= 1\nd3\n\u0012\ndM4M2 + d(d \u2212 1)\n2\nM 3\n2\n\u0013\n= O\n\u00121\nd\n\u0013\n,\nwhere M2 and M4 denote the second and fourth moments of the standard Gaussian, respectively.\nThen, Chebyshev\u2019s inequality implies that |x\u22a4Wx| = O(1/\n\u221a\nd) with high probability.\nEnsuring appropriate memory lookups then requires such properties to hold for many embeddings\nand pairs of embeddings, with errors that are small enough to ensure correct associations. This may\nbe achieved with careful union bounds or more powerful concentration results. We do not attempt\nto do this in a precise manner in this paper, and will generally assume d large enough to satisfy the\ndesired associative memory behaviors, noting that a precise analysis is an important direction for\nfuture work (see [8] for follow-up work in this direction).\nB\nTheoretical Insights on Gradient Dynamics\nIn this section, we provide additional details on the theoretical insights from Section 6, including\ndetails on the illustrative example (Section B.1), derivations of gradients w.r.t. key-query matri-\nces at initialization (Section B.2), as well as a study of how the induction head mechanism may\ndevelop in a simplified setup, using a sequence of single layer-wise gradient steps in a top-down\nmanner (Section B.3).\nB.1\nDetails on illustrative example\nConsider the example discussed in Section 6: we would like to predict y from x = wE(y) + pt,\nwhere pt is a positional embedding at a random position t \u2208 [T], which we would like to ignore.\nFurther assume that y is uniformly distributed (p(y = k) = 1/N) and consider the matrix obtained\nafter one population gradient step with step-size \u03b7 starting from an initialization W0 = 0 (so\n14\nthat \u02c6pW0(k|x) = 1/N):\nW1 = \u03b7\nN\nN\nX\nk=1\nwU(k)(\u00b5k \u2212 \u00af\u00b5)\u22a4,\n(11)\nwith \u00af\u00b5 = E[x].\nNote that we have \u00b5k = wE(k) + 1\nT\nP\nt pt and \u00af\u00b5 = 1\nN\nP\nk wE(k) + 1\nT\nP\nt pt, so that (11) becomes\nW1 = \u03b7\nN\nN\nX\nk=1\nwU(k)(wE(k) \u2212 \u00afwE)\u22a4,\n(12)\nwith \u00afwE := 1\nN\nPN\nk=1 wE(k). When d is large enough to ensure near-orthonormal embeddings, we\nhave for any y and t,\nW1(wE(y) + pt) \u2248 \u03b7\nN wU(y) + O\n\u0012 1\nN 2\n\u0013\n.\nThis implies\nwU(k)\u22a4W1(wE(y) + pt) \u2248 \u03b7\nN 1{k = y} + O\n\u0012 1\nN 2\n\u0013\n,\nas claimed in the main text. The classifier \u02c6y = arg maxk wU(k)\u22a4W1(wE(y)+pt) then has essentially\nperfect accuracy, and has learned to ignore the spurious positional embeddings, which are simply\nexogenous noise.\nB.2\nGradients on key-query matrices at initialization\nWe now derive expressions for population gradients of the attention key-query matrices at zero\ninitialization, noting that random initialization behaves similarly to zero initialization. Although the\noptimization problems involving these matrices are non-convex, these gradients at initialization lead\nto associative memories, similar to Lemma 2. When output matrices of the previous layer already\nencode the desired associations, these gradients can lead to associative memories that focus the\nattention on the correct key-value pairs.\nWe begin with the following lemma, which gives the gradient of the loss w.r.t. W = W 2\nK at zero\ninitialization. For simplicity, we drop the d\u22121/2 factor from the softmax, which only changes\ngradients by a multiplicative factor, and thus does not change its form.\nLemma 4 (Gradient of second attention layer). Consider the following loss for predicting the next\ntoken y from an attention layer with inputs X = [x1, . . . , xT ], and value-output matrix \u03a62 :=\nW 2\nOW 2\nV :\nL(W) = E(X,y)[\u2113(y, \u03be(X))],\n\u03be(X) = WU\u03a62X\u03c3(X\u22a4WxT ),\n(13)\nwith \u2113 the cross-entropy loss and \u03c3(u)t =\neut\nP\ns eus for u \u2208 RT is the softmax.\nThe gradient at W = 0 is given by\n\u2207W L(W)\n\f\f\nW =0 =\nN\nX\nk=1\nE(X,y)\n\"\n(\u02c6pW (k|X) \u2212 1{y=k}) 1\nT\nT\nX\nt=1\nwU(k)\u22a4\u03a62xt \u00b7 (xt \u2212 \u00afx1:T )x\u22a4\nT\n#\n= 1\nT\nN\nX\nk=1\nT\nX\nt=1\nEX[\u02c6pW (k|X)wU(k)\u22a4\u03a62xt \u00b7 (xt \u2212 \u00afx1:T )x\u22a4\nT ]\n\u2212 1\nT\nN\nX\nk=1\nT\nX\nt=1\np(y = k) EX[wU(k)\u22a4\u03a62xt \u00b7 (xt \u2212 \u00afx1:T )x\u22a4\nT | y = k]\nwith \u00afx1:T = 1\nT\nPT\nt=1 xt.\nNow we consider the gradient w.r.t. W = W 1\nK at zero initialization, and consier a simplification of\nthe second layer attention to its linearization around W 2\nK = 0. We will see that this still provides\nfirst-order information that is sufficient for W 1\nK to be learned.\n15\nLemma 5 (Gradient of first attention layer). Consider the following loss for predicting the next\ntoken y from a stack of two attention layers, with all parameters fixed except for W = W 1\nK, the\nkey-query matrix at the first attention layer:\nL(W) = E(X,y)[\u2113(y, \u03be(X))],\n\u03be(X) = WU\u03a62X\u00af\u03c3(Z(W)\u22a4W2xT ).\n(14)\nHere, \u00af\u03c3(u1:T )t = 1\nT (1+ut \u2212 1\nT\nPT\ns=1 us) is the linearization of the softmax around 0, and Z(W) =\n[z1(W), . . . , zT (W)] with\nzt(W) =\nt\nX\ns=1\n\u03a61xs\u03c3(p\u22a4\n1:tWpt)s,\nand \u03a6\u2113 = W \u2113\nOW \u2113\nV for \u2113 = 1, 2.\nThe gradient at W = 0 is given by\n\u2207W L(W)\n\f\f\nW =0\n=\nN\nX\nk=1\nEX\n\"\n\u02c6pW (k|X) 1\nT\nT\nX\nt=1\nwU(k)\u22a4\u03a62xt \u00b7 1\nt\nt\nX\ns=1\n(\u03a61xs)\u22a4W2xT (ps \u2212 \u00afp1:t)p\u22a4\nt\n#\n\u2212\nN\nX\nk=1\np(y = k) EX\n\"\n1\nT\nT\nX\nt=1\nwU(k)\u22a4\u03a62xt \u00b7 1\nt\nt\nX\ns=1\n(\u03a61xs)\u22a4W2xT (ps \u2212 \u00afp1:t)p\u22a4\nt |y = k\n#\n\u2212\nN\nX\nk=1\nEX\n\"\n\u02c6pW (k|X)wU(k)\u22a4\u03a62\u00afx1:T \u00b7 1\nT\nT\nX\nt=1\n1\nt\nt\nX\ns=1\n(\u03a61xs)\u22a4W2xT (ps \u2212 \u00afp1:t)p\u22a4\nt\n#\n+\nN\nX\nk=1\np(y = k) EX\n\"\nwU(k)\u22a4\u03a62\u00afx1:T \u00b7 1\nT\nT\nX\nt=1\n1\nt\nt\nX\ns=1\n(\u03a61xs)\u22a4W2xT (ps \u2212 \u00afp1:t)p\u22a4\nt |y = k\n#\nB.3\nLearning the induction head mechanism\nIn this section, we analyze the training dynamics of the induction head mechanism, in the following\nsimplified setup: we consider a single trigger (K = 1), and assume that \u03c0u, \u03c0q, \u03c0o and \u03c0b(\u00b7|i) are\nuniform over [N] for any i.\nTo further simplify the analysis, we consider a loss that only considers sequences of length T where\nthe last input zT is the second occurrence of the trigger token, and the label y = zT +1 is the\ncorresponding output token. We note that this may be easily extended to later occurrencies of the\ntrigger. This is similar to the setup of Figure 1, where the loss is only taken on triggers after the\nsecond occurrence: in that case, the loss may be written as a weighted sum of the one we consider\nhere, weighted by the probability of the second (or later) trigger appearing at the given position T.\nIn practice, when the loss is on all tokens and WF is also learned, we may expect that WF quickly\nlearns the global bigram statistics, as we saw empirically in Section 5. Indeed, the current token\nembedding, which is included in the input superposition, has strong predictive signal compared\nto the attention layers, which initially mainly appear as noise. This is then similar to the setup of\nLemma 1, which provides recovery of bigram statistics when d is large (though we note that the other\ninformation from attention layers in the inputs may eventually be used and bias away from perfect\nrecovery, see Figure 4(right)). Once such global estimates are obtained, the expected loss will be\nmainly dominated by trigger tokens, leading to the setup above.\nFor simplicity, we thus drop the feed-forward layer WF in the remainder of this section, focusing\non the learning of W 2\nO, W 2\nK and W 1\nK, in this top-down order. We will consider zero-initialization\na single gradient steps, noting that random initialization should lead to similar associative memory\nbehaviors when the dimension is large enough, since it leads to a remapping of input embeddings\nwhich is near-orthogonal to any output embedding (see Appendix A).\nB.3.1\nLearning W 2\nO\nWe begin by studying the learning of the second output matrix W 2\nO. In the above data model, we may\nconsider a loss as in Lemma 2 with input-outputs (x, y), where y is the output token of the sequence,\n16\nand x depends on the random sequence z1:T as\nx = 1\nT\nT\nX\nt=1\nW 2\nV (wE(zt) + \u03b5t),\nwhere \u03b5t = pt + 1\nt\nPt\ns=1 \u03a61(wE(zs) + ps) with \u03a61 = W 1\nOW 1\nV , is a \u201cnoise\u201d vector from the residual\nstreams, containing positional embeddings as well as an average attention output from the first layer.\nIn practice, the logit predictions are of the form WU(W 2\nOx + \u03b5T ) due to residual connections, but we\nignore the term WU\u03b5T for simplicity, noting that it is near-zero when d is large.\nAfter a gradient step on W 2\nO with step-size \u03b7, starting from zero-initialization (so that \u02c6p(k|x) =\np(y = k) = 1/N for all x), Lemma 2 yields\nW 2\nO = \u03b7\nN\nN\nX\nk=1\nwU(k)(E[x|y = k] \u2212 E[x])\u22a4.\n(15)\nNow, consider the random variables q (trigger token), o (output token), to (position of the first\noccurrence of the output token). In our simplified data model, q and to have the same distribution\nregardless of the conditioning on y = k, while o is equal to k when y = k, while it is uniform in [N]\nwithout this condition. The sequence z1:T has the same distribution in either p(\u00b7) or p(\u00b7|y = k),\nexcept for the token zto.\nThen, we may write:\nE[x|y = k] \u2212 E[x] = 1\nT\n\u0000E[W 2\nV wE(zto)|y = k] \u2212 E[W 2\nV wE(zto)]\n\u0001\n+ 1\nT\n \nE\n\" T\nX\nt=to\nW 2\nV \u03b5t|y = k\n#\n\u2212 E\n\" T\nX\nt=to\nW 2\nV \u03b5t\n#!\n,\nsince \u03b5t is independent of o when t < to. Noting that \u03b5t only depends on zto and thus on y through\nthe first layer attention, we have\nE[x|y = k] \u2212 E[x] = 1\nT W 2\nV (wE(k) \u2212 \u00afwE)\n+ 1\nT\n \nE\n\" T\nX\nt=to\n1\nt W 2\nV \u03a61wE(zto)|y = k\n#\n\u2212 E\n\" T\nX\nt=to\n1\nt W 2\nV \u03a61wE(zto)\n#!\n= 1\nT W 2\nV (wE(k) \u2212 \u00afwE) + \u03c4\nT W 2\nV \u03a61(wE(k) \u2212 \u00afwE),\nwhere \u03c4 := E\nhPT\nt=to\n1\nt\ni\n, and \u00afwE = 1\nN\nPN\nk=1 wE(k). Thus, (15) becomes\nW 2\nO =\n\u03b7\nNT\nN\nX\nk=1\nwU(k)(W 2\nV (wE(k) \u2212 \u00afwE))\u22a4 + \u03b7\u03c4\nNT\nN\nX\nk=1\nwU(k)(W 2\nV \u03a61(wE(k) \u2212 \u00afwE))\u22a4, (16)\nso that when d is learn enough to ensure near-orthonormal embeddings, we have\nwU(k)\u22a4W 2\nOW 2\nV wE(j) \u2248\n\u03b7\nNT 1{k = j} + O\n\u0010\n\u03b7\nN 2T\n\u0011\nwU(k)\u22a4W 2\nOW 2\nV \u03a61wE(j) \u2248 \u03b7\u03c4\nNT 1{k = j} + O\n\u0010 \u03b7\u03c4\nN 2T\n\u0011\n,\nwhere the O(\u00b7) terms are due to the \u00afwE elements. The first line yields a behavior that matches desired\nassociative memory in (7) of Section 4.2 when N is large. The second line shows additional spurious\nassociations that are stored in W 2\nO due to the output of the first layer attention, but which may be\n\u201ccleaned up\u201d once the attention layers start focusing on the correct tokens.\nFinally, we note that despite the recovery of these useful associations after one gradient step, the\npredictions with this estimate W 2\nO are still near-random, since in the bag-of-words setup with average\nattention, the output token cannot be distinguished from any other token in the sequence in our model\n(except perhaps the trigger token, which is guaranteed to appear twice, but does not provide any\nsignal to infer the output token, since the two are independent).\n17\nB.3.2\nLearning W 2\nK\nNow assume that W 2\nO is as in (16). As argued above, the predictions \u02c6p(k|x) are essentially ran-\ndom 1/N in our model for W 2\nK = 0, so that after one gradient step on W 2\nK with learning rate \u03b7,\nLemma 4 yields:\nW 2\nK =\n\u03b7\nTN\nX\nk,t\n\u0000E[wU(k)\u22a4\u03a62xt \u00b7 (xt \u2212 \u00afx)x\u22a4\nT | y = k] \u2212 E[wU(k)\u22a4\u03a62xt \u00b7 (xt \u2212 \u00afx)x\u22a4\nT ]\n\u0001\n, (17)\nwhere xt are the inputs to the second attention layer, given by\nxt = xt,0 + xt,1\n(18)\nxt,0 = wE(zt) + pt\n(19)\nxt,1 = 1\nt\nt\nX\ns=1\n\u03a61(wE(zs) + ps).\n(20)\nFrom now on, we consider a simplified architecture where only xt,0 are fed as queries and values,\nwhile only xt,1 are fed as keys. Using the fact that trigger tokens q are sampled uniformly (i.e.,\n\u03c0q = 1/N), we have\nW 2\nK =\n\u03b7\nTN\nN\nX\nk=1\nT\nX\nt=1\n(E[At,k | y = k] \u2212 EX[At,k])\n(21)\n=\n\u03b7\nTN 2\nN\nX\nk=1\nT\nX\nt=1\nN\nX\nj=1\n(E[At,k | y = k, q = j] \u2212 E[At,k | q = j])\n(22)\nwhere\nAt,k = wU(k)\u22a4\u03a62xt,0 \u00b7 (xt,1 \u2212 \u00afx1)x\u22a4\nT,0,\n(23)\nwith \u00afx1 =\n1\nT\nP\nt xt,1. Now, note that we have wU(k)\u22a4\u03a62xt,0 \u2248 \u03b1 1{zt = k} with \u03b1 = \u03b7/TN\nby (16), and xT,0 = wE(q) + pT . This yields\nW 2\nK \u2248\n\u03b1\u03b7\nTN 2\nN\nX\nj=1\nN\nX\nk=1\n\u2206k,j(wE(j) + pT )\u22a4,\n(24)\nwith\n\u2206k,j := E\n\" T\nX\nt=1\n1{zt = k}(xt,1 \u2212 \u00afx1)|y = k, q = j\n#\n\u2212 E\n\" T\nX\nt=1\n1{zt = k}(xt,1 \u2212 \u00afx1)|q = j\n#\n= \u2206o\nk,j + \u2206q\nk,j + \u2206r\nk,j,\nwhere the three terms split the sum inside the expectation\n\u2206o\nk,j := E [1{zto = k}(xto,1 \u2212 \u00afx1)|y = k, q = j] \u2212 E [1{zto = k}(xto,1 \u2212 \u00afx1)|q = j]\n\u2206q\nk,j := E\n\uf8ee\n\uf8f0X\nt\u2208Tq\n1{zt =k}(xt,1 \u2212 \u00afx1)|y=k, q=j\n\uf8f9\n\uf8fb \u2212 E\n\uf8ee\n\uf8f0X\nt\u2208Tq\n1{zt =k}(xt,1 \u2212 \u00afx1)|q=j\n\uf8f9\n\uf8fb\n\u2206r\nk,j := E\n\"X\nt\u2208Tr\n1{zt =k}(xt,1 \u2212 \u00afx1)|y=k, q=j\n#\n\u2212 E\n\"X\nt\u2208Tr\n1{zt =k}(xt,1 \u2212 \u00afx1)|q=j\n#\n,\nwhere Tq = {to \u2212 1, T} and Tr = [T] \\ {to, to \u2212 1, T} (recall that to is a random variable,\ncorresponding to the first occurrence of the output token, so that these sets are random).\nWe will now show that \u2206o\nk,j carries the desired signal for the appropriate induction head associative\nmemory, while \u2206q\nk,j and \u2206r\nk,j are negligible, for N large enough.\n18\nControlling \u2206o\nk,j.\nFor t = to, noting that zto = y, we have\n\u2206o\nk,j = E [1{y = k}(xto,1 \u2212 \u00afx1)|y = k, q = j] \u2212 E [1{y = k}(xto,1 \u2212 \u00afx1)|q = j]\n=\n\u0012\n1 \u2212 1\nN\n\u0013\nE [xto,1 \u2212 \u00afx1|y = k, q = j]\n= N \u2212 1\nN\n \n\u00afp +\nN\nX\ni=1\nak,j,i\u03a61wE(i)\n!\n,\nwith ak,j,i \u2248 (\u03a61wE(i))\u22a4 E [xto,1 \u2212 \u00afx1|y = k, q = j] thanks to near-orthonormality, and\n\u00afp = Eto\n\"\n1\nto\nto\nX\ns=1\nps \u2212 1\nT\nT\nX\nt=1\n1\nt\nt\nX\ns=1\nps\n#\nis a spurious positional mixture.\nWe then distinguish the following cases:\n\u2022 If j \u0338= k and i = j, since the trigger token j only appears at positions to \u2212 1 and T, we have\nak,j,i \u2248 Eto\n\"\n1\nto\n\u2212 1\nT\nT\nX\nt=to\u22121\n1\nt \u2212 1\nT 2\n#\n=: \u03b3T .\nWe may expect to to be concentrated around T/2, in which case \u03b3T \u2273 2\nT \u2212 1\nT \u2212\n1\nT 2 \u2265 C\nT > 0\nfor T larger than a small constant.\n\u2022 If j = k = i, the two occurrences of the trigger happen one after the other, so it must be that to = T.\nThen\nak,j,i \u2248 2\nT \u2212\n1\nT(T \u2212 1) \u2212 2\nT 2 = 2\nT + O\n\u0012 1\nT 2\n\u0013\n,\nfor T larger than a small constant.\n\u2022 If i \u0338= j = k, all tokens up to position to \u2212 2 = T \u2212 2 are i.i.d. uniform in [N] \\ {j}, so that\nak,j,i \u2248\nT \u2212 2\nT(N \u2212 1) \u2212 1\nT\n\u0012\n(T \u2212 2) \u00b7\n1\nN \u2212 1 +\nT \u2212 2\n(T \u2212 1)(N \u2212 1) +\nT \u2212 2\nT(N \u2212 1)\n\u0013\n= O\n\u0012 1\nN\n\u0013\n\u2022 If i \u0338= j and j \u0338= k, all tokens except at positions to \u2212 1, to and T (we have to < T) are uniform\nin [N] \\ {j}. The triggers do not contribute anything to ak,j,i since i \u0338= j, and the output token\nmay be also randomized by taking the average over k \u2208 [N] \\ {j}. We thus obtain\n1\nN \u2212 1\nX\nk\u0338=j\nak,j,i \u2248 O\n\u0012 1\nN\n\u0013\n.\nIn summary, we obtain\n1\nN\nN\nX\nk=1\nak,j,i \u2248\n\u001aO\n\u0000 1\nN\n\u0001\n,\nif i \u0338= j\n\u2126\n\u0000 1\nT\n\u0001\n,\nif i = j.\nThus, when N is large, while T is moderate, the above sum leads to more signal in the i = j terms\ncompared to i \u0338= j. In particular, this yields\n(\u03a61wE(i))\u22a4\n \n1\nN\nN\nX\nk=1\n\u2206o\nk,j\n!\n\u2248\n\u001aO\n\u0000 1\nN\n\u0001\n,\nif i \u0338= j\n\u2126\n\u0000 1\nT\n\u0001\n,\nif i = j,\nso that this component in (24) acts precisely like the desired associative memory in (7).\nIt remains to show that the other components are negligible compared to this. It then suffices to show:\n1\nN\nN\nX\nk=1\n(\u2206q\nk,j + \u2206r\nk,j) \u2248 o\n\u0012 1\nT\n\u0013\n.\n19\nControlling \u2206q\nk,j.\nFor t \u2208 Tq, note that we always have zt = j in the expectations, so that \u2206q\nk,j = 0\nunless k = j. For k = j, we have \u2206q\nk,j = O(1), so that\n1\nN\nN\nX\nk=1\n\u2206q\nk,j = O\n\u0012 1\nN\n\u0013\n.\nControlling \u2206r\nk,j.\nUsing that \u2225xt,1 \u2212 \u00afx1\u2225 \u2264 C = O(1) for all t, we provide the following crude\nbound via the triangle inequality and H\u00f6lder inequality:\n\u2206r\nk,j = E\n\"X\nt\u2208Tr\n1{zt =k}(xt,1 \u2212 \u00afx1)|y=k, q=j\n#\n\u2212 E\n\"X\nt\u2208Tr\n1{zt =k}(xt,1 \u2212 \u00afx1)|q=j\n#\n\u2225\u2206r\nk,j\u2225 \u2264 C\n \nE\n\"X\nt\u2208Tr\n1{zt =k}|y=k, q=j\n#\n+ E\n\"X\nt\u2208Tr\n1{zt =k}|q=j\n#!\n\u2264 2CT\nN ,\nsince zt is independent of y given t \u2208 Tr and thus is uniform in [N] \\ {j}, and |Tr| \u2264 T. We note,\nhowever, that \u2206r\nk,j may be controlled much more finely by leveraging the similarities between the\ndistributions of zt, t \u2208 Tr with or without conditioning on y.\nOverall, we have shown that up to some spurious positional embeddings, W 2\nK behaves as the desired\nassociative memory from (7) when N is large enough, satisfying:\n(\u03a61wE(i))\u22a4W 2\nKwE(j) \u2248 \u03b1\u03b7\nTN\n\u001a\n\u2126\n\u0012 1\nT\n\u0013\n1{i = j} + O\n\u0012 T\nN\n\u0013\u001b\n(25)\nWe note that one may then amplify the gap between correct and incorrect associations by having a\nlarge enough step-size, which then makes the softmax more peaked and hence the attention more\nsparse and focused on correct associations.\nB.3.3\nLearning W 1\nK\nWe now assume that W 2\nO and W 2\nK have learned the correct associations, and consider one gradient\nstep away from zero-initialization on W 1\nK. Note that when W 1\nK = 0, the predictions of the model are\nstill often near random chance. Indeed, the second layer attention will attend to all tokens starting at\nthe first occurrence of the trigger, since all such tokens contain \u03a61wE(q) in their average attention,\nwhich activates the second-layer attention head. Then the output is likely to predict the trigger itself,\nwhich will be an incorrect prediction most of the time.\nWe may thus consider \u02c6p(k|X) = 1/N at this stage as well. We also consider a simplified architecture\nwhere the first layer attention only uses positional embeddings in the key-query matrix, and only\ntoken embeddings in the value-output matrix. In particular, we have xt = wE(zt). Lemma 5 then\ngives the following form for W 1\nK after one gradient step of step-size \u03b7:\nW 1\nK = \u03b7\nN\nN\nX\nk=1\nEX\n\"\n1\nT\nT\nX\nt=1\nwU(k)\u22a4\u03a62xt \u00b7 1\nt\nt\nX\ns=1\n(\u03a61xs)\u22a4W 2\nKxT (ps \u2212 \u00afp1:t)p\u22a4\nt |y = k\n#\n\u2212 \u03b7\nN\nN\nX\nk=1\nEX\n\"\n1\nT\nT\nX\nt=1\nwU(k)\u22a4\u03a62xt \u00b7 1\nt\nt\nX\ns=1\n(\u03a61xs)\u22a4W 2\nKxT (ps \u2212 \u00afp1:t)p\u22a4\nt\n#\n\u2212 \u03b7\nN\nN\nX\nk=1\nEX\n\"\nwU(k)\u22a4\u03a62\u00afx1:T \u00b7 1\nT\nT\nX\nt=1\n1\nt\nt\nX\ns=1\n(\u03a61xs)\u22a4W 2\nKxT (ps \u2212 \u00afp1:t)p\u22a4\nt |y = k\n#\n+ \u03b7\nN\nN\nX\nk=1\nEX\n\"\nwU(k)\u22a4\u03a62\u00afx1:T \u00b7 1\nT\nT\nX\nt=1\n1\nt\nt\nX\ns=1\n(\u03a61xs)\u22a4W 2\nKxT (ps \u2212 \u00afp1:t)p\u22a4\nt\n#\n.\nNote that since W 2\nO and W 2\nK already captured the desired associations at this stage, we have\nwU(k)\u22a4\u03a62xt \u2248 \u03b1 1{zt = k}\nand\n(\u03a61xs)\u22a4W 2\nKxT \u2248 \u03b1\u2032 1{zs = zT },\n20\nfor some \u03b1, \u03b1\u2032 > 0. Recall that in our model, we have zT = q with probability one (q is the trigger\ntoken), and that q only appears twice: once at position tq := to \u2212 1 < T and once at position T. We\nthen have, for any t > 1,\nW 1\nKpt \u2248 \u03b7\u03b1\u03b1\u2032\nNTt\nN\nX\nk=1\n(At,k \u2212 Bt,k \u2212 Ct,k + Dt,k),\nwith\nAt,k = E[1{zt =k} 1{tq \u2264 t}(ptq \u2212 \u00afp1:t)|y=k]\n(26)\nBt,k = E[1{zt =k} 1{tq \u2264 t}(ptq \u2212 \u00afp1:t)]\n(27)\nCt,k = E[rk 1{tq \u2264 t}(ptq \u2212 \u00afp1:t)|y=k]\n(28)\nDt,k = E[rk 1{tq \u2264 t}(ptq \u2212 \u00afp1:t)],\n(29)\nwhere rk := 1\nT\nPT\nt=1 1{zt = k}. We have\nAt,k = E[1{zt =k}(1{tq = t \u2212 1} + 1{tq \u2208 [t \u2212 2] \u222a {t}})(ptq \u2212 \u00afp1:t)|y=k]\n= P(tq = t \u2212 1|y = k)(pt\u22121 \u2212 \u00afp1:t) + 1\nN\nX\ns\u2208[t\u22122]\u222a{t}\nP(tq = s|y = k)(ps \u2212 \u00afp1:t)\n= P(tq = t \u2212 1)(pt\u22121 \u2212 \u00afp1:t) + 1\nN\nX\ns\u2208[t\u22122]\u222a{t}\nP(tq = s)(ps \u2212 \u00afp1:t)\n= P(tq = t \u2212 1)(pt\u22121 \u2212 \u00afp1:t) + O\n\u0012 1\nN\n\u0013\n,\nsince the distribution of tq is the same regardless of y. We proceed similarly for the other quantities\nand obtain the following:\nBt,k = O\n\u0012 1\nN\n\u0013\nCt,k = P(tq = t \u2212 1)\nT\n(pt\u22121 \u2212 \u00afp1:t) + O\n\u0012 1\nN\n\u0013\nDt,k = O\n\u0012 1\nN\n\u0013\n.\nThis yields the following associative memory behavior, for t > 1:\np\u22a4\ns W 1\nKpt \u2248 \u03b7\u03b1\u03b1\u2032(T \u2212 1)\nT 2t\n\u001a\nP(tq = t \u2212 1)\n\u0012\n1{s = t \u2212 1} \u2212 1\nt 1{s \u2208 [t]}\n\u0013\n+ O\n\u0012 1\nN\n\u0013\u001b\n,\nwhich matches the desired \u201cprevious token head\u201d behavior from (7) when N is large. As in the case\nof W 2\nK, we may then \u201csaturate\u201d the softmax by choosing a large enough step-size.\nC\nOther Proofs\nC.1\nProof of Lemma 1\nProof. Recall the form of the cross-entropy loss for classification with K classes:\n\u2113(y, \u03be) = \u2212\nN\nX\nk=1\n1{y = k} log\ne\u03bek\nP\nj e\u03bej .\nIts derivatives take the form\n\u2202\u2113\n\u2202\u03bek\n(y, \u03be) = s(\u03be)k \u2212 1{y = k},\nwith s(\u03be)k =\ne\u03bek\nP\nj e\u03bej the softmax.\n21\nThe gradient of L is then given by\n\u2207W L(W) = E(z,y)\n\" N\nX\nk=1\n\u2202\u2113\n\u2202\u03bek\n(y, WUWwE(z))\u2207W (wU(k)\u22a4WwE(z))\n#\n= E(z,y)\n\" N\nX\nk=1\n(\u02c6pW (k|z) \u2212 1{y = k})wU(k)wE(z)\u22a4\n#\n=\nN\nX\nk=1\nEz[Ey[(\u02c6pW (k|z) \u2212 1{y = k})wU(k)wE(z)\u22a4 | z]]\n=\nN\nX\nk=1\nEz[(\u02c6pW (k|z) \u2212 Ey[1{y = k}|z])wU(k)wE(z)\u22a4],\nwhich yields the desired result.\nC.2\nProof of Lemma 2\nProof. Using similar steps as the proof of Lemma 1, we have\n\u2207W L(W) = E(x,y)\n\" N\nX\nk=1\n\u2202\u2113\n\u2202\u03bek\n(y, WUWx)\u2207W (wU(k)\u22a4Wx)\n#\n= E(x,y)\n\" N\nX\nk=1\n(\u02c6pW (k|x) \u2212 1{y = k})wU(k)x\u22a4\n#\n=\nN\nX\nk=1\nwU(k) Ex[\u02c6pW (k|x)x]\u22a4 \u2212\nN\nX\nk=1\nEy[1{y = k}wU(k) E[x|y]\u22a4]\n=\nN\nX\nk=1\nwU(k) Ex[\u02c6pW (k|x)x]\u22a4 \u2212\nN\nX\nk,j=1\np(y = j) 1{j = k}wU(k) E[x|y = j]\u22a4\n=\nN\nX\nk=1\np(y = k)wU(k)(\u02c6\u00b5k \u2212 \u00b5k)\u22a4,\nwith \u02c6\u00b5k = p(y = k)\u22121 Ex[\u02c6pW (k|x)x] and \u00b5k = E[x|y = k].\nC.3\nProof of Lemma 4\nProof. To better isolate the role of keys from values, we denote the keys that are fed into the matrix W\nby Z = [z1, . . . , zT ] \u2208 Rd\u00d7T , while the query is simply xT . In practice we have Z = X, and both\nare superpositions of potentially multiple embeddings (if W is part of the second attention layer,\nthese are the token embedding, positional embedding, and the output of the first attention layer).\nThe gradient of the loss at W = 0 writes:\n\u2207W L(W)\n\f\f\nW =0 = E(X,Z,y)\n\" N\nX\nk=1\n\u2202\u2113\n\u2202\u03bek\n(y, \u03be) \u00b7 \u2207W (wU(k)\u22a4\u03a62X\u03c3(Z\u22a4WxT ))\n\f\f\nW =0\n#\n(30)\n= E(X,Z,y)\n\" N\nX\nk=1\n(\u02c6pW (k|X, Z) \u2212 1{y = k}) \u00b7 \u2207W (wU(k)\u22a4\u03a62X\u03c3(Z\u22a4WxT ))\n\f\f\nW =0\n#\n. (31)\nWe have\n\u2207W (wU(k)\u22a4\u03a62X\u03c3(Z\u22a4WxT ))\n\f\f\nW =0 =\nT\nX\nt=1\nwU(k)\u22a4\u03a62xt \u00b7 \u2207W (\u03c3(Z\u22a4WxT )t)\n= 1\nT\nT\nX\nt=1\nwU(k)\u22a4\u03a62xt \u00b7 (zt \u2212 \u00afz1:T )x\u22a4\nT ,\n22\nwhere \u00afz1:T = 1\nT\nP\nt zt, and we used the fact that\n\u2202\n\u2202us\n\u03c3(u)t\n\f\f\nu=0 = 1\nT 1{t = s} \u2212 1\nT 2 .\n(32)\nThe gradient (31) now writes\n\u2207W L(W)\n\f\f\nW =0 =\nN\nX\nk=1\nE(X,Z)[(\u02c6pW (k|X, Z) \u2212 1{y = k}) 1\nT\nT\nX\nt=1\nwU(k)\u22a4\u03a62xt \u00b7 (zt \u2212 \u00afz1:T )x\u22a4\nT ],\nand the result follows.\nC.4\nProof of Lemma 5\nProof. The linearization of the second layer softmax around zero takes the following form:\n\u00af\u03c3(Z\u22a4W2xT )t = 1\nT (1 + z\u22a4\nt W2xT \u2212 \u00afz\u22a4\n1:T W2xT ),\nwith zt = Pt\ns=1 \u03a61xs\u03c3(p\u22a4\n1:tWpt)s the output of the first attention layer.\n\u03bek =\nT\nX\nt=1\nwU(k)\u22a4\u03a62xt\u00af\u03c3(Z\u22a4W2xT )\n= 1\nT\nT\nX\nt=1\nwU(k)\u22a4\u03a62xt + 1\nT\nT\nX\nt=1\nwU(k)\u22a4\u03a62xt\nt\nX\ns=1\n(\u03a61xs)\u22a4W2xT \u03c3(p\u22a4\n1:tWpt)s\n\u2212 wu(k)\u22a4\u03a62\u00afx1:T \u00b7 1\nT\nT\nX\nt=1\nt\nX\ns=1\n(\u03a61xs)\u22a4W2xT \u03c3(p\u22a4\n1:tWpt)s.\nThen,\n\u2207W L(W)\n\f\f\nW =0\n(33)\n= E(X,y)\n\" N\nX\nk=1\n\u2202\u2113\n\u2202\u03bek\n(y, \u03be) \u00b7 \u2207W \u03bek\n\f\f\nW =0\n#\n(34)\n= E(X,y)\n\" N\nX\nk=1\n\u2202\u2113\n\u2202\u03bek\n(y, \u03be) 1\nT\nT\nX\nt=1\nwU(k)\u22a4\u03a62xt\n1\nt\nt\nX\ns=1\n(\u03a61xs)\u22a4W2xT (ps \u2212 \u00afp1:t)p\u22a4\nt\n#\n(35)\n\u2212 E(X,y)\n\" N\nX\nk=1\n\u2202\u2113\n\u2202\u03bek\n(y, \u03be)wU(k)\u22a4\u03a62\u00afx1:T \u00b7 1\nT\nT\nX\nt=1\n1\nt\nt\nX\ns=1\n(\u03a61xs)\u22a4W2xT (ps \u2212 \u00afp1:t)p\u22a4\nt\n#\n,\n(36)\nusing (32). The result follows by using\n\u2202\u2113\n\u2202\u03bek (y, \u03be) = \u02c6p(k|\u03be) \u2212 1{y = k}.\nD\nBeyond our Simplified Architecture\nWhile the simplified architecture presented in Section 4.2 is sufficient to capture the desired induction\nbehavior for our bigram task, transformer architectures used in practice typically involve more\ncomponents, as well as more heads and layers. In this section, we discuss how our memory viewpoint\nextends to such architectures.\n23\nFactorizations.\nIn practice, transformers typically involve products of matrices, potentially with a\nlow-rank bottleneck. For instance, our key-query matrices WK should instead be considered as a\nproduct W \u22a4\nKWQ, and the output-value matrices WO and WV are typically jointly optimized.\nConsider an associative memory of the form:\nW\u2217 =\nX\ni\nyix\u22a4\ni \u2208 Rd\u00d7d,\nwhere (xi)i and (yi)i are appropriate collections of near-orthonormal embeddings.\nWe now argue that a similar associative memory can be achieved with the factorization W =\nd\n2d\u2032 UV ,\nwhere U \u2208 Rd\u00d7d\u2032, V \u2208 Rd\u2032\u00d7d with d\u2032 \u2264 d (for instance d\u2032 could be the dimension of attention\nheads), are given by:3\nU = U0 +\nX\ni\nyi(V0xi)\u22a4\nV = V0 +\nX\ni\n(U \u22a4\n0 yi)x\u22a4\ni ,\nwhere U0 and V0 are random matrices with N(0, 1\nd) entries. These matrices are similar to those that\nwould arise from a single gradient step individually on U and V from initializations U0 and V0, as in\nLemma 1. To see why W behaves like W\u2217, note that we have\nUV = U0V0 +\nX\ni\nyi(V0xi)\u22a4V0 +\nX\ni\n(U \u22a4\n0 yi)x\u22a4\ni +\nX\ni,j\nyi(V0xi)\u22a4(U \u22a4\n0 yj)x\u22a4\nj .\nIt is also easy to check using central limit arguments (similar to remapping in Appendix A) that\n\u02dcxi :=\nq\nd\nd\u2032 V0xi \u2208 Rd\u2032 and \u02dcyi :=\nq\nd\nd\u2032 U \u22a4\n0 yi are all nearly-orthonormal embeddings. Thus, we have\n2y\u22a4\nk Wxl = \u02dcy\u22a4\nk \u02dcxl +\nX\ni\ny\u22a4\nk yi\u02dcx\u22a4\ni \u02dcxl +\nX\ni\n\u02dcy\u22a4\nk \u02dcyix\u22a4\ni xl +\nX\ni,j\ny\u22a4\nk yi\u02dcx\u22a4\ni \u02dcyjx\u22a4\nj xl\n\u2248 0 + 1{k = l} + 1{k = l} + 0,\nwhere the first and last term vanish due to the cross-terms \u02dcy\u22a4\ni \u02dcxi\u2032 which vanish for any i, i\u2032. Thus, W\nand W\u2217 encode the same associations, when d and d\u2032 are large enough to ensure near-orthogonality.\nLayer-normalization.\nNormalization layers [5, 62] are typically used in transformers to improve\ntraining stability [53], and are applied on each token representation, either after [53] or before [7]\neach block. It may be seen as an operation of the form4\nLN(x) =\nx\n\u2225x\u2225,\napplied to the input or output of a given block.\nIn order to obtain a basic understanding of the role of layer-norm in our associative memory setup,\nwe may consider the setup Lemma 1, with a normalization applied after the linear operation, leading\nto the population loss:\nL(W) = E(x,y)\u223cp[\u2113(y, WULN(Wx))].\n(37)\nThe gradients then take the form\n\u2207W L(W) =\nN\nX\nk=1\nEx\n\u0014 \u02c6pW (y = k|x) \u2212 p(y = k|x)\n\u2225Wx\u2225\n\u0012\nI \u2212 (Wx)(Wx)\u22a4\n\u2225Wx\u22252\n\u0013\nwU(k)x\u22a4\n\u0015\n.\n(38)\n3When d\u2032 is the head dimension, the d\nd\u2032 scaling can be interpreted as the correct multiplier to use in attention\nlogits, which plays a similar role to the\n1\nd\u2032 multiplier in the \u00b5P scaling [61], for our setup where the variance of\nthe random entries of input embeddings is 1/d instead of 1 as in [61].\n4RMSNorm [62] would use the variance instead of the norm in the denominator, leading to an additional\n\u221a\nd\nfactor in the numerator. Here we use the norm, which is more natural when embeddings have near-unit norm, in\ncontrast to the \u2248\n\u221a\nd norm for the standard parameterization.\n24\nThis illustrates that in addition to weighting the updates on class k by the prediction error \u02c6p(k|x) \u2212\np(k|x), the updates are also projected on the orthogonal of the Wx direction. This means that\nan update on the direction wU(k)x\u22a4 will occur only to the extent that Wx is not already aligned\nwith wU(k). Thus, if an association is \u201cstored\u201d once, so that Wx \u2248 wU(y), layer-norm will\navoid further updating W in that direction, hopefully avoiding norms that grow too much, and also\nencouraging frequent and infrequent tokens to be weighted similarly in the final memory (see also [8]\nfor more discussion on this).\nNote that at random initialization Wx is nearly orthogonal to any wU(k), so that layer-norm only\nstarts playing a significant role later in training, and does not affect our theoretical analysis based on\nsingle gradient steps.\nMLP blocks.\nIf we denote by (ui)i and (vi)i collections of near-orthonormal input and output\nembeddings, an MLP block may encode associations (i, j) \u2208 M as follows:\nF(x) =\nX\n(i,j)\u2208M\nvj\u03c3(u\u22a4\ni x \u2212 b),\nwhere \u03c3 is a non-linear activation and b a bias term. Then, if one assumes that u\u22a4\ni uj \u2264 b for i \u0338= j\nand \u03c3(t) = 0 for t < 0, then this can help filter out noise that arises from near-orthogonality, and\nmay then lead to additional storage capacity, at the cost of additional computation (see, e.g., [8, 31]).\nAn additional benefit of MLP layers discussed in Section 4 is that they may encode many-to-many\nassociations, which is useful when multiple embeddings are in the residual stream and need to be\nconsidered jointly (e.g., a subject and a relation in a factual recall task [36]). This may be achieved,\nfor instance, by considering embeddings uI =\n1\n\u221a\n|I|\nP\ni\u2208I ui, where I are sets of bounded size (e.g.,\nas obtained using layer-norm over a residual stream). Then, assuming the ui are nearly-orthonormal,\nwe have u\u22a4\nI uI \u2248 1 while u\u22a4\nI uI\u2032 \u2272 1 \u2212 \u03b4 if I = I\u2032, for some \u03b4 that depends on the maximal\ncardinality of these sets. Although 1 \u2212 \u03b4 is no-longer vanishingly small, defining\nF(x) =\nX\n(I,J )\u2208M\nvJ \u03c3(u\u22a4\nI x \u2212 b),\nthe non-linearity may still succeed at filtering out any I that does not correspond to the query set\nin x. We leave the question of how such non-linear associative memories may arise from training\ndynamics to future work.\nMultiple heads and layers.\nWe remark that our view of weights as associative memories applies\nto any parameter other than embedding/unembedding layers, and thus naturally extends to multiple\nheads (using the low-rank factorizations described above) and multi-layer models.\nIt is important to note, however, that the redundancy introduced by having more heads and layers\nmakes it more challenging to identify which layer/head/weight will learn certain associations (see,\ne.g., Figure 11 in Appendix E). This is in contrast to our simplified architecture of Section 4.2, where\nwe may identify the role of each matrix (up to some possible redundancy when using the feed-forward\nlayer WF ). In practice, mechanisms may appear in different heads/layers across different training\nruns, which makes interpretability more challenging, and typically requires some causal identification\ntechniques, such as mediation analysis [36, 56].\nE\nExperiment Details and Additional Experiments\nIn this section, we present additional details on the experiments, as well as additional results.\nComputing setup.\nWe use Pytorch and each run uses a single GPU, along with 60 CPU cores for\nreal-time data generation. We will make our code available upon publication.\nHyperparameters.\nWe now provide the hyperparameters used in each figure. The SGD step-size\nis denoted \u03b7. We fix the momentum parameter to 0.9 and the weight decay parameter to 10\u22124. U\ndenotes the uniform distribution over [N].\n25\n0\n100\n200\n300\n400\n500\niteration\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nmemory recall\nWo2\nK=1, fix q frequent\nK=5, fix q rare\nK=1, random q\nK=5, random q\n0\n100\n200\n300\n400\n500\niteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmemory recall\nWk2\nK=1, fix q frequent\nK=5, fix q rare\nK=1, random q\nK=5, random q\n0\n100\n200\n300\n400\n500\niteration\n0.0\n0.2\n0.4\n0.6\n0.8\nmemory recall\nWk1 (t<64)\nK=1, fix q frequent\nK=5, fix q rare\nK=1, random q\nK=5, random q\nFigure 5: Memory recall probes for the setting of Figure 4(left).\n0\n100\n200\n300\n400\n500\niteration\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nmemory recall\nWo2\ntrain unif\ntrain bigram\n0\n100\n200\n300\n400\n500\niteration\n0.0\n0.2\n0.4\n0.6\n0.8\nmemory recall\nWk2\ntrain unif\ntrain bigram\n0\n100\n200\n300\n400\n500\niteration\n0.0\n0.2\n0.4\n0.6\n0.8\nmemory recall\nWk1 (t<64)\ntrain unif\ntrain bigram\nFigure 6: Memory recall probes for the setting of Figure 4(center).\n\u2022 Figure 2: K = 3, \u03c0q = \u03c0u (random triggers) or Q is the K most likely elements of \u03c0u, \u03c0o = U,\nd = 128, dhidden = 4 \u00d7 128 (hidden dimension of the feed-forward MLPs), \u03b7 = 0.2.\n\u2022 Figure 3: K = 5, \u03c0q = \u03c0u (random triggers), \u03c0o = U, d = 128, \u03b7 = 0.2.\n\u2022 Figure 4(left) and Figure 5: \u03c0o = U, d = 128, \u03b7 = 1. For random triggers we use \u03c0q = \u03c0u. For\nK = 1 with fixed frequent trigger, the only trigger is the most probable token according to \u03c0u,\nwhile for K = 5 with fixed rare triggers, the five triggers are the 6-th to 10-th most probable\ntokens according to \u03c0u.\n\u2022 Figure 4(center): K = 3, \u03c0q = \u03c0u (random triggers), \u03c0o = U or \u03c0o = \u03c0b (conditioned on the\ntrigger), d = 128, \u03b7 = 1.\n\u2022 Figure 4(right): K = 3, \u03c0q = \u03c0u (random triggers), \u03c0o = U, d = 128, \u03b7 = 1.\nMemory recall probes and data-distributional properties.\nFigure 5 and Figure 6 show the\nevolution of the different memory probes for the settings considered in Figure 4(left,center). Figure 5\nhighlights that associative memories for the induction head are slower to learn when using few\ntriggers (small K), rare fixed triggers, or random triggers (note that the probe for W 2\nK with fixed\ntriggers only shows recall accuracy on the set of triggers Q, which is an easier task). Figure 6\nFigure 7: Visualization of the weights W 1\nK (left), W 2\nK (center), and W 2\nO (right) after training with\nrandom triggers, K = 3, \u03c0q = \u03c0u, \u03c0o = U. For each of these weight matrices W, if we write the\ncorresponding target memory in (7) as W\u2217 = P\ni viu\u22a4\ni with appropriate embeddings (ui)i and (vi)i\n(for instance ut = pt\u22121 and vt = pt for W 1\nK on the left), and we show all values v\u22a4\nj Wui.\n26\n0\n50\n100\n150\n200\n250\niteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWo2 recall, fixed lr\n32\n64\n128\n256\n512\n1024\n0\n20\n40\n60\n80\niteration\n0.2\n0.4\n0.6\n0.8\n1.0\nWo2 recall, tuned lr\n32\n64\n128\n256\n512\n1024\nFigure 8: Effect of dimension on learning W 2\nO alone, with fixed or tuned learning rate.\n0\n10\n20\n30\n40\nbatches\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\naccuracy\none-step classifier\nd = 32\nd = 64\nd = 128\nd = 256\nd = 512\nd = 1024\nd = 2048\n102\n103\ndimension d\n0.2\n0.4\n0.6\n0.8\naccuracy\none-step classifier\n5 batches\n10 batches\n20 batches\nFigure 9: Accuracy of one-step estimate of W 2\nO with varying dimension and number of batches used\nfor computing expectations. Each batch consists of 32 sequences of 256 tokens for a total of 8 192\ntokens, with K = 5 random triggers and uniform outputs.\nshows that using uniform output tokens can lead to better fitting of W 2\nO and W 2\nK compared to using\noutput tokens sampled from bigrams. In addition to the increased diversity when using uniform\noutputs, this may also be due to the fact that bigram outputs are already well predicted using global\nstatistics with just the feed-forward layer, hence the gradient signal on such well-predicted tokens\nmay not propagate through the induction head mechanism. In contrast, the recall accuracy for W 1\nK is\ncomparable for both settings, since the previous token head is useful at all positions regardless of the\noutput token distribution.\nVisualizing memories.\nFigure 7 shows visualizations of the associative memory behaviors after\ntraining. We see that diagonal elements dominate in the plots, which corresponds to correct asso-\nciations lead to high \u2018memory recall\u2019. Nonetheless, we see that some of the diagonal elements are\nweaker than others, particularly for late positions in W 1\nK, and for some of the trigger tokens in W 2\nK,\nwhile the diagonal for W 2\nO seems to be roughly uniform. We note that characters corresponding to\ncapital letters have token index 13 to 38, while lowercase letters have index 39 to 64. The association\npatterns found in W 2\nK then seem related to frequencies of appearance of triggers, whereby capital\nletters appear less frequently in the data, and are also less frequently chosen as triggers, compared to\nlowercase letters. Similarly, since the first occurrence of triggers is typically early in a sequence, it\nis natural that W 1\nK learns stronger associations at earlier positions. In contrast, diagonal elements\nfor W 2\nO are nearly uniform, which agrees with the fact that output tokens are sampled uniformly\nin this setup. We refer to the follow-up work [8] for an analysis of how data frequencies affect\nassociation strength in such associative memories.\nEffect of dimension.\nRecall that our study of associative memories with random embeddings\nrequires large dimension d in order to ensure near-orthogonality, and thus store input-output pairs\nmore effectively. In Figure 8, we evaluate the recall accuracy for W 2\nO for varying dimension, when\ntraining it by itself, and only on the output tokens (as in Figure 3). We see that higher dimension\nleads to faster learning of the memory, in particular d = 128 seems sufficient for fast learning after\njust a few iterations with a tuned learning rate. If the learning rate isn\u2019t tuned, we notice that there\nis a further slowdown for low dimension, is likely due to issues with the fact that our experiments\nuse the standard parameterization of neural networks at initialization, rather than maximal update\nparameterizations [60]. Note that learning W 2\nO alone is a convex optimization problem, and we\nhypothesize that higher dimension makes the problem better conditioned, and hence easier to learn.\nIn Figure 9, we show \u201cone-step\u201d recall accuracies for classifying output tokens from the average\n27\n0\n100\n200\n300\n400\n500\niteration\n1\n2\n3\n4\nloss\nin-context vs global loss\nK=5, random q\n0\n100\n200\n300\n400\n500\niteration\n0.0\n0.2\n0.4\n0.6\n0.8\naccuracy\nin-context accuracy\nK=5, random q\n0\n200\n400\n600\niteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nrecall accuracy\nWo2 recall\nWk2 recall\nWk1 recall (t<64)\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\nKL divergence\nattention vs feed-forward probes\nWf KL\nFigure 10: Training of a more realistic architecture with (i) ReLU MLP instead of linear layer\nfor the second feed-forward layer, (ii) all parameters trained, including embeddings, (iii) pre-layer\nnormalization. The loss, in-context accuracy and memory recall probes are similar to the simplified\narchitecture (see, e.g., Figure 4).\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nK : [l] T h e e _ m e r F _ m e\nK : [l] T h e e _ m e r F _ m e\nK : [l] T h e e _ m e r F _ m e\nK : [l] T h e e _ m e r F _ m e\nFigure 11: Attention maps for a two-layer model with 4 attention heads. In the first layer (top), the\nprevious token mechanism is mostly achieved by one of the four heads, while the induction behavior\nat the second layer (bottom) is distributed across the different heads.\nattention input to W 2\nO, given by\nR1 = 1\nN\nN\nX\nk=1\n1\n\u001a\nk = arg max\nk\u2032 (W 2\nV wE(k\u2032))\u22a4(\u00b5k \u2212 \u00b5)\n\u001b\n,\nwhere \u00b5k = E[x|y = k] and \u00b5 = E[x], for x = 1\nt\nPt\ns=1 W 2\nV wE(zs) and y = zt+1, when zt is a\ntrigger token after its first occurrence. Expectations are computed over batches of data of varying sizes\nand in different dimensions. We call this \u201cone-step\u201d since it is related to the classifier obtained after\nperforming a single gradient step on W 2\nO from zero initialization (see Lemma 2 and Appendix B.3.1).\nThe plots illustrate that this simple one-step model is already able to extract relevant signal from the\nnoisy average attention, after a handful of batches of data, corresponding to tens of thousands of\ntokens, and that this gets easier as the dimension increases.\nMore complex architectures.\nFigure 10 shows training behavior for a more complex model than the\nsimplified one considered in Section 5, namely where we train all parameters, replace the linear WF\nfeedforward layer by a two-layer MLP, and were (pre-)layer-normalization is added. Despite these\nchanges, we see similar behavior for the memory recall probes (which now involve embeddings that\nmay change over time), suggesting that the model is still identifying the same memory associations,\ndespite the additional redundancies in parameters and modified training dynamics.\nFigure 11 shows the attention maps obtained when training a multi-head version of our two-layer\nmodel, with four attention heads per layer. We see that the redundancy of multiple heads creates\ndifficulties in identifiability: only one of the first layer heads learns the previous token behavior,\nwhile the induction behavior is shared across different heads at the second layer. This illustrates the\nchallenges of interpretability in the presence of redundant models, which then require additional work\nto identify which of the layers and heads are performing a given behavior, e.g., through interventions\nand causal mediation analysis [36, 56].\n28\n"
  },
  {
    "title": "MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training",
    "link": "https://arxiv.org/pdf/2306.00107.pdf",
    "upvote": "1",
    "text": "Published as a conference paper at ICLR 2024\nMERT: ACOUSTIC MUSIC UNDERSTANDING MODEL\nWITH LARGE-SCALE SELF-SUPERVISED TRAINING\nYizhi Li\n1,2\u2217 Ruibin Yuan\n3,4\u2217 Ge Zhang\n4,5,6\u2217 Yinghao Ma\n7\u2217 Xingran Chen\nHanzhi Yin\n3\nChenghao Xiao\n8\nChenghua Lin\n1,2\u2020 Anton Ragni2\nEmmanouil Benetos7\nNorbert Gyenge2\nRoger Dannenberg3\nRuibo Liu9\nWenhu Chen\n5\nGus Xia10,11\nYemin Shi\n6,12\nWenhao Huang\n6\nZili Wang\nYike Guo4\nJie Fu\n4,6\u2020\nm-a-p.ai 1University of Manchester 2University of Sheffield 3Carnegie Mellon University\n4Hong Kong University of Science and Technology 5University of Waterloo 6Beijing Academy of Artificial Intelligence\n7Queen Mary University of London 8Durham University 9Dartmouth College 10MBZUAI 11New York University 12linksoul.ai\nABSTRACT\nSelf-supervised learning (SSL) has recently emerged as a promising paradigm\nfor training generalisable models on large-scale data in the fields of vision, text,\nand speech. Although SSL has been proven effective in speech and audio, its\napplication to music audio has yet to be thoroughly explored. This is partially due\nto the distinctive challenges associated with modelling musical knowledge, particu-\nlarly tonal and pitched characteristics of music. To address this research gap, we\npropose an acoustic Music undERstanding model with large-scale self-supervised\nTraining (MERT), which incorporates teacher models to provide pseudo labels\nin the masked language modelling (MLM) style acoustic pre-training. In our\nexploration, we identified an effective combination of teacher models, which out-\nperforms conventional speech and audio approaches in terms of performance. This\ncombination includes an acoustic teacher based on Residual Vector Quantisation -\nVariational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-\nQ Transform (CQT). Furthermore, we explore a wide range of settings to overcome\nthe instability in acoustic language model pre-training, which allows our designed\nparadigm to scale from 95M to 330M parameters. Experimental results indicate\nthat our model can generalise and perform well on 14 music understanding tasks\nand attain state-of-the-art (SOTA) overall scores.\n1\nINTRODUCTION\nPre-trained language models (PLMs) can learn generalisable representations of data without human\nannotated labels in a self-supervised learning (SSL) style, leading to remarkable performance im-\nprovement in natural language processing and related fields (Brown et al., 2020; Fang et al., 2022;\nChen et al., 2021a). Music is widely recognised as a special language that can be used to communicate\nacross different cultures (Mehr et al., 2019). The internal similarity between music and language as\na communication interface lays a promising foundation for adapting PLM-based methods to model\nmusic sequences. We argue that the benefit is twofold. First, PLMs can potentially pave the way\nto unify the modelling of a wide range of music understanding, or the so-called Music Information\nRetrieval (MIR) tasks, including but not limited to music tagging, beat tracking, music transcription,\nsource separation, etc., so that different tasks no longer need task-specific models or features. Second,\nreleasing a PLM for acoustic music understanding allows the redistribution of the musical knowledge\nrather than the data itself, which saves the costs of manual annotation and copyright law restrictions.\nUnfortunately, we are yet to see a general-purpose and cost-effective open-source PLM on acoustic\nmusic understanding. Most existing studies are designed to solely address music tagging prob-\nlems (Pons and Serra, 2019; Spijkervet and Burgoyne, 2021; McCallum et al., 2022; Huang et al.,\n2022; Zhu et al., 2021; Zhao and Guo, 2021), and many of them do not provide open-source code\nbases or checkpoints for further evaluation. A promising model is JukeMIR (Castellon et al., 2021),\nwhich is based on Jukebox (Dhariwal et al., 2020) and provides a comprehensive evaluation on MIR\n\u2217The authors contributed equally to this work.\n\u2020Corresponding authors.\n1\narXiv:2306.00107v3  [cs.SD]  7 Feb 2024\nPublished as a conference paper at ICLR 2024\nG\nC\nB\n7\nAm\n7\nD\nE\nm\nAudio Waveform\n1D Convolution\nFeature Extractor\nTransformer\nEncoder\nMasked Audio\nFeatures\nContextual\nRepresentation\nAcoustic\nTeacher\nMusical\nTeacher\nAcoustic\nMLM\nMusical\nMLM\nFigure 1: Illustration of the MERT Pre-training Framework.\ndownstream tasks. However, this foundation model uses cumbersome hierarchical auto-regressive\ntransformer decoders containing billions of parameters to model music audio, leading to significant\ninefficiency for conducting general music understanding tasks (e.g., it takes weeks to inference on\ndatasets like MTG (Bogdanov et al., 2019) with a consumer-grade 3090 GPU).\nThe aforementioned research gap has urged us to design and open-source a generalisable and\ncomputationally affordable pre-trained acoustic music model. In this paper, we propose an acoustic\nMusic undERstanding model with large-scale self-supervised Training (MERT). MERT inherits a\nspeech SSL paradigm, employing teacher models to generate pseudo targets for sequential audio clips.\nSpecifically, to capture the distinctive pitched and tonal characteristics in music, MERT incorporates\na multi-task paradigm to balance the acoustic and musical representation learning as demonstrated in\nFig. 1. In the proposed design, an Residual Vector Quantisation - Variational Autoencoder (RVQ-\nVAE) (D\u00b4efossez et al., 2022) is used as the acoustic teacher to provide discretised acoustic-level\nsummarisation of the music signal. The Constant-Q Transformation (CQT) (Brown, 1991) model is\nfurther introduced as the music teacher for capturing pitch and harmonic inductive bias. Regarding\nthe context dependencies and music hierarchies, as indicated in Borsos et al. (2022), we leave the task\nof modelling high-level and abstract patterns to the stacked layers of self-attentions in the transformer.\nWe also explore a wide range of settings for the transformer and 1D convolution encoder to overcome\nthe instability in acoustic model pre-training, which permits effective scaling up of MERT from 95M\nto 330M size when blending acoustic and musical knowledge. By scaling up to 330M size (only 7%\nthe size of JukeBox), MERT achieves overall state-of-the-art (SOTA) results on various MIR tasks,\nwhich demonstrates a strong generalisability on music understanding. Last but not least, we analyse\nmultiple pre-trained settings considering the teachers and share our decision routes \u00a7 5.2 and \u00a7 5.3,\nwhich could potentially guide future acoustic music understanding pre-training research.\nTo summarise, our contributions are:\n\u2022 proposing a multi-task style predictive acoustic self-supervised learning paradigm, which\nachieves SOTA performance on various MIR tasks, including important yet unexplored tasks\nfor pre-training such as pitch detection, beat tracking and source separation applications;\n\u2022 conducting a broad range of analysis based on ablation study of the proposed MERT pre-training\nparadigm;\n\u2022 exploring robust and stable strategies for acoustic music model training to overcome training\ninstability and frequent crashes when scaling up the pre-training on model size;\n\u2022 providing an open-source, generalisable and computationally affordable acoustic music pre-\ntrained model, which addresses the needs of both industry and research communities.\n2\nRELATED WORK\nPLMs for Acoustic Music\nThe field of music information retrieval (MIR) has long been facing\nchallenges in data availability due to the costs associated with music audio annotation and country-\nspecific copyright laws (Chen et al., 2019; Castellon et al., 2021). To address this challenge, pre-\ntrained language models (PLMs) for acoustic music have been proposed to provide reusable learned\n2\nPublished as a conference paper at ICLR 2024\nrepresentations, enabling transfer learning for various downstream MIR tasks without the need for\nextensive data annotation (Castellon et al., 2021). However, current acoustic music pre-trained models\nstill have room for improvement in terms of providing open-source, generalisable, and lightweight\nlearned representations suitable for both industrial and research applications (McCallum et al., 2022).\nExisting acoustic music pre-trained models primarily focus on tagging tasks and rely on supervised\ntagging labels for pre-training (Pons and Serra, 2019; Spijkervet and Burgoyne, 2021; McCallum\net al., 2022; Huang et al., 2022). While some studies have explored contrastive learning for acoustic\nmusic pre-training, they face limitations in training data and model size, hampering the performance\nimprovements (Choi et al., 2017; Li et al., 2022). Additionally, several models trained on inaccessible\ndatasets or without publicly available codes and model weights make it difficult to reproduce or\nextend their approaches (McCallum et al., 2022; Castellon et al., 2021; Li et al., 2022; Zhu et al.,\n2021; Zhao and Guo, 2021). Although some general-purpose audio representation models show\npotential for music audio representation learning, their performance is mostly evaluated on limited\nMIR downstream tasks (Saeed et al., 2021; Borsos et al., 2022; Wang et al., 2023). This lack of\ncomprehensive evaluation hinders further studies and a thorough understanding of the pros and cons\nof existing models.\nSelf-Supervised Speech Processing\nMusic and speech processing are closely related (Jasmin et al.,\n2020) since they usually use the same audio data formats. Additionally, both acoustic music and\nspeech processing models need to deal with the cocktail party problem (Brown and Bidelman, 2022;\nPetermann et al., 2022) since good source separation capabilities help both separating noises and\nbackground sounds with speech and processing polyphonic music audio. These common grounds\nbetween music and speech processing inspire us to adapt SOTA speech pre-trained models and tailor\nthem specifically for music audio processing tasks. For instance, existing research work targeting\ngeneral-purpose audio representations (Saeed et al., 2021; Borsos et al., 2022; Wang et al., 2023) has\nverified that self-supervised speech processing models can be extended beyond speech to downstream\nentry-level music tasks, including generating mono piano music and music reconstruction.\nAudio Representation with Language Modelling\nMask strategy-based large-scale language\nmodels have been applied to a wide range of domains (Lample and Charton, 2019; Chen et al.,\n2021a;b; Fang et al., 2022), but still remain under-explored in acoustic music understanding. For\naudio, Dhariwal et al. (2020) investigates generating hierarchical tokens which can be further\nemployed to reconstruct music, inspiring subsequent research to understand and generate acoustic\nmusic based on extracted discrete tokens from continuous features. Baevski and Mohamed (2020)\nintroduce a pre-trained VQ-VAE (Baevski et al., 2019) to provide prediction targets to conduct speech\nrepresentation learning with MLM. While introducing K-means to provide discrete token codebooks\nand pre-training the model to detect sound units, Hsu et al. (2021) claim that a better teacher model\nin SSL could lead to better downstream task performance. Additionally, recent speech processing\npre-trained models (Borsos et al., 2022; Wang et al., 2023) propose to train or adopt separately trained\ncodecs (Zeghidour et al., 2021; D\u00b4efossez et al., 2022) for discrete token extraction. Based on the\nconclusion from previous studies, the recently released RVQ-VAEs (Zeghidour et al., 2021; D\u00b4efossez\net al., 2022), achieving good results in music reconstruction, could be adopted as teacher models\nfor music understanding pre-training and provide acoustic information guidance. Yet some of the\nuniqueness of music processing such as timbre and harmony remains unexplored. We thus propose to\nincorporate a corresponding musical teacher model in MERT to fill such an important gap.\n3\nMETHODOLOGY\nThis section introduces the pre-training paradigm and architecture of our models. It includes prediction\nto acoustic teachers such as k-means or deep music features, and reconstruction to music teachers\nsuch as CQT spectrum, both based on the well-established masked language modelling (MLM) .\n3.1\nPRE-TRAINING WITH MLM\nSupervised Learning requires a labelled dataset Dt = {x(t)\ni , y(t)\ni }N\ni=1. Here, N is the number of data\nsamples, x(t)\ni\nis the ith data sample in the dataset, and y(t)\ni\nis the corresponding label. From Dt, we\ncan train a machine learning algorithm f\u03b8 (\u00b7) parameterised with \u03b8 that makes label predictions on\n3\nPublished as a conference paper at ICLR 2024\neach data sample. Unsupervised learning, in contrast, learns an algorithm based on an unlabelled\ndataset D = {xi}M\ni=1, with SSL being a specific type of this class. For each data sample xi, SSL\nderives a new data x\u2032\ni with a pseudo label y\u2032\ni. The training process is to minimise the loss between\neach pseudo label y\u2032\ni and the prediction based on new data \u02c6yi = f\u03b8(x\u2032\ni) as denoted in Eq.1.\n\u03b8\u2217 = arg min\u03b8\nX\nx(t)\ni\n\u2208D\nL\n\u0010\nf\u03b8(x\u2032(t)\ni\n), y\u2032(t)\ni\n\u0011\n.\n(1)\nMLM is a famous example of pseudo-label generation. Let xi =\nh\nx(1)\ni , x(2)\ni , \u00b7 \u00b7 \u00b7 , x(L)\ni\ni\nbe the ith\ndata sample in a sequential dataset with length L, and M \u2282 [L] is a subset of indices randomly\nchosen from 1 to L. Then, the new data is defined by the following equation\nx\u2032\ni =\nh\n1[L]\\M(1) \u00b7 x(1)\ni , 1[L]\\M(2) \u00b7 x(2)\ni , \u00b7 \u00b7 \u00b7 , 1[L]\\M(L) \u00b7 x(L)\ni\ni\n(2)\nwhere 1[L]\\M(x) denotes the indicator function, that is, 1[L]\\M(x) = 1 if and only if x is outside the\nmasked indices set M. The pseudo-label that needs to be learned is typically y\u2032\ni = xi \u2212 x\u2032\ni, i.e., the\nmasked data. However, reconstructing masked data y\u2032 for raw audio tasks as pseudo-label is hard to\ntrain. HuBERT (Vaswani et al., 2017; Hsu et al., 2021) uses a dimension-reduced feature z\u2032 derived\nfrom y\u2032 with phonetic acoustic information, which forms the design basis of our pre-training strategy.\nAs a speech SSL system, HuBERT utilises offline clustering to acquire pseudo labels for a BERT-like\nprediction loss. Specifically, it uses Mel-frequency cepstral coefficients (MFCCs), a widely-used\ntraditional feature in speech-related tasks, as acoustic features for clustering. The obtained results\nare then utilised as pseudo labels in the first iteration of pre-training. It then uses the learned\nrepresentation for clustering to get a pseudo label for the second iteration pre-training. Such a pseudo\nlabel includes acoustic information in human speech and can be aligned to phonemes. The loss\nfunctions of HuBERT are formulated as follows:\nLH(f; x, M, Z) =\nX\nt\u2208M\nlog pf(zt | x\u2032, t)\n(3)\nwhere log pf(\u00b7 | x\u2032, t) is the log-likelihood function on clustering results given the masked input x\u2032\nand position t derived from f; likelihood function pf is the Noise Contrastive Estimation (NCE) loss\nwhich is defined as\npf(c | x\u2032, t) =\nexp(sim(T(ot), ec)/\u03c4)\nPC\nc\u2032=1 exp(sim(T(ot), ec\u2032)/\u03c4)\n,\n(4)\nHere, c \u2208 [C] is a codeword of the clustering results and ec represents its embedding; sim is the\ncosine similarity; ot is the output of the model at timestep t; and T(ot) is the linear transformation\nof ot, making it have the same dimension as ec. Besides, \u03c4 scales the logit which is set to 0.1 in\nHuBERT. The linear transformation T, the model to generate outputs, and the embedding of all the\nclustering results are all learnable.\nOverall, we use the same model as HuBERT but introduce several notable variations tailored to\nmusic. Specifically, we designed a better hidden-unit z as pseudo tags for pre-training with multiple\nmusic acoustic features. In addition, we added a reconstruction loss to music features and employed\nadditional music augmentation tricks.\n3.2\nMODELLING ACOUSTIC INFORMATION\nThe MFCC features are only good at modelling acoustic and timbre information for single-pitch\nsignals, and therefore, the clustering results do not provide much timbre information in music\nrecording. We proposed two potential approaches as the teacher on acoustic information: one based\non traditional features, and the other based on deep learning.\nThe first method uses k-means on the log-Mel spectrum and Chroma features for timbre and harmonic\nacoustic information, respectively. In the case of music representation, each frame contains more\ninformation compared to speech, necessitating a larger number of classes for k-means clustering.\nThe complexity of the k-means algorithm is linear with the number of centroids (clustering centres),\nleading to a time-consuming k-means for the music feature. To tackle this problem, we employ\n4\nPublished as a conference paper at ICLR 2024\n300-means for the log-Mel spectrum with dimension 229, and 200-means for Chroma features with\ndimension 264, resulting in a total of 60,000 classes (200 centroids for Chroma features multiplied by\n300 centroids for the log-Mel spectrum). Despite the increased number of classes, the computational\ncomplexity remains comparable to that of HuBERT. The disadvantage of k-means is that it is\ndifficult to scale up to a larger number of classes and larger datasets, and the results are sensitive to\ninitialisation.\nThe second choice for our acoustic teacher is EnCodec (D\u00b4efossez et al., 2022), a recent learnable\nfeature with 8-layer residual Vector Quantized-Variational AutoEncoder (VQ-VAE). Each acoustic\nfeature, denoted as zenc \u2208 [C]L\u00d78, is a 2-dimensional auditory code matrix, and L is the length of the\nrecording. The row vector of each matrix zenc[t, :] represents the results of 8 different clusterings for\nframe t, and the column vector of each matrix zenc[:, j] represents the results from the jth codebook\nof the audio sequence, where j \u2208 {1, . . . , 8}. EnCodec converts 24kHz input waveforms to 8\ndifferent embeddings at 75Hz with a 320-fold reduction, and the quantizer has 1024 dimensions.\nIn this setting, for each 5-second waveform, the discrete acoustic feature is a matrix with 375 \u00d7 8\nentries, representing 375 frames (75Hz \u00d7 5s) and 8 deep acoustic features. With these embeddings,\nthe decoder of EnCodec can reconstruct the waveform at 24 kHz with authentic information in timbre.\n3.3\nMODELLING MUSICAL INFORMATION\nApart from acoustic information, we added a new reconstruction loss to the Constant-Q transform\n(CQT) spectrogram to emphasise pitch-level information. The CQT is a type of frequency transform\nthat is widely used in various MIR tasks, such as pitch detection, chord recognition, and music\ntranscription. It is similar to the Fourier transform, but bin widths are proportional to frequency rather\nthan equal, giving each octave the same number of bins, resulting in a better time-frequency trade-off\nfor music audio where multiple pitches occur in multiple octaves. We utilize mean squared error\n(MSE) loss to reconstruct the CQT spectrum zcqt from the masked input audio x\u2032. That is,\nLCQT (fcqt; x, M, zcqt) =\nX\nt\u2208M\n\u2225zcqt,t \u2212 fcqt(x\u2032)t\u22252\n(5)\nAnd the final loss function L is a linear combination of both the acoustic loss function LH and the\nmusical-pitch loss function LCQT :\nL = \u03b1 \u00b7 LH + LCQT\n(6)\n4\nEXPERIMENTS\n4.1\nEVALUATION PROTOCOL\nDownstream Tasks\nWe evaluate our method and compare it with baseline models on 14 downstream\ntasks, including frame-level classification or regression tasks like music tagging, key detection,\ngenre classification, emotion score regression, instrument classification, pitch classification, vocal\ntechnique detection, and singer identification; and sequential tasks like beat tracking and source\nseparation. For instrument classification, we use the Nsynth (Engel et al., 2017) and MTG-instrument\ndatasets, with receiver operating characteristic (ROC), and average precision (AP). The NSynth\ndataset is also used for pitch classification, with accuracy (ACC) as the evaluation metric. Vocal\ntechnique detection and singer identification based on the VocalSet dataset (Wilkins et al., 2018),\nwith accuracy as the metric. For music tagging, we utilise the MagnaTagATune (MTT) (Law et al.,\n2009) and MTG-Jamendo (Bogdanov et al., 2019) datasets, averaging multiple embeddings for long\naudio recordings. Key detection is accomplished using the Giantsteps and Giantsteps-MTG-keys\ndatasets (Knees et al., 2015; Korzeniowski and Widmer, 2017), with a refined accuracy (ACCrefined)\nmetric. Genre classification is performed using the GTZAN (Tzanetakis and Cook, 2002) and MTG-\nGenre datasets, with ROC, and AP metrics. Emotion score regression is conducted on the Emomusic\ndataset (Soleymani et al., 2013), with the coefficient of determination (R2 score) of arousal and\nvalence as evaluation metrics. Beat tracking is conducted on the GTZAN Rhythm dataset (Marchand\nand Peeters, 2015), using the F-measure (F1). Finally, source separation is accomplished using the\nMUSDB18 dataset (Rafii et al., 2017), with the Source-to-Distortion Ratio (SDR) as the evaluation\nmetric. The full descriptions of the datasets and tasks can be found in Appendix B.1.\n5\nPublished as a conference paper at ICLR 2024\nProbing Protocol\nFollowing Castellon et al. (2021); Yang et al. (2021), we restrict the testing\nprotocol with probing rather than fine-tuning, i.e. freezing the backbone pre-trained models as deep\nfeature extractor and only train a simple downstream structure, typically a multilayer perceptron\n(MLP) for frame-level tasks. For a fair comparison, we also limit the space for hyper-parameters\nsearching. For full details please refer to Appendix B.2.\n4.2\nBASELINE METHODS\nWe select models pre-trained with various paradigms from both music and speech domains as our\nbaselines to provide a comprehensive evaluation of the generalisation ability of the designs. Mu-\nsiCNN (Pons and Serra, 2019) is selected as a representative supervised method, which is pre-trained\nwith supervision from the Million Song Dataset tags (Bertin-Mahieux et al., 2011). CLMR (Spijkervet\nand Burgoyne, 2021) and MULE (McCallum et al., 2022) are selected as representatives of SOTA\nmusic representations trained with contrastive learning. Jukebox (Dhariwal et al., 2020) and the corre-\nsponding transfer learning method, JukeMIR (Castellon et al., 2021) is selected as the representative\nof transfer learning from a large-scale generative pre-trained musical representation. We also select\nthe recently proposed strong speech SSL models, HuBERT (Hsu et al., 2021) and data2vec (Baevski\net al., 2022), as our baselines since they share the same MLM pre-training paradigm with MERT.\nWhile HuBERT reconstructs the masked discrete tokens provided by the K-means teacher, data2vec\nuses the student model updated with an exponential moving average gradient to produce continuous\nrepresentations for MLM prediction. In order to reveal the effectiveness of the pre-training paradigm\nitself rather than the training data distribution, we re-train the speech models and denote them by\nHuBERTmusic and data2vecmusic. Additionally, we present the current SOTA for each task including\nresults from both supervised and self-supervised methods.\n4.3\nIMPLEMENTATION DETAILS\nTraining Settings\nWe deploy the proposed SSL architecture in the training of various model sizes\nwith matched scales of data. We mined 160K hours of music recordings from the Internet to build\na large-scale music dataset. Accordingly, the base size models (95M) are trained with a 1K hours\nsubset whereas the whole dataset is used for the large model (330M). Specifically, we provide a\nspecial edition of the base model, MERT-95M-public, that is trained on a totally publicly available\nmusic dataset, music4all (Santana et al., 2020), with a data size of 910 hours. In the context of\nself-attention, the computational complexity scales quadratically with the sequence length. Therefore,\nwhen dealing with limited computational resources, there exists a trade-off between the batch size\nand the sequence length. In our preliminary experiments, we have observed that increasing the batch\nsize provides greater performance improvements compared to extending the context length. To allow\na larger batch size under the computational limitation, we adopt a strategy of randomly truncating\naudio clips into 5-second segments following Ma et al. (2023). This duration roughly corresponds to\na 2-bar context in music. It is worth noting that our model utilises a convolutional relative positional\nembedding, similar to the approach introduced by Baevski et al. (2020) in Wav2Vec, enabling it to\noperate effectively in longer contexts, if required. The effective batch sizes and learning rates for\nthe base model and large model are set to 1.5 and 5.5 hours, and their learning rates are set to 5e\u22124,\n1.5e\u22123, respectively. Pre-training is carried out with the fairseq1 framework. Models are trained with\n64 A100-40GB GPUs with fp16. We also implement a data augmentation of randomly adding short\nsegments to improve the representation robustness, and describe the details in Appendix A.1\nTraining Stability\nIn our empirical findings, we observe that when scaling up acoustic encoder-only\nmodels, they tend to exhibit a higher susceptibility to training instability compared to models of\nsimilar size in text or image domains. Such instability can result in decreased performance or, in\nextreme cases, even lead to crashes in model training. During experimentation with scaling up to the\nMERT-330M model, we encounter notable instability manifested by constant gradient clipping and\nsporadic spikes in losses. This instability has a detrimental effect on the accuracy of MLM predictions\nand results in decreased performance on downstream tasks. Our attempts to resume training from\npreviously saved checkpoints and data batches are proved unsuccessful in mitigating the instability\nissue. Furthermore, we observe that reducing the learning rate in this context not only fails to address\nthe issue but also leads to a decline in performance and hindered the training convergence. We\n1https://github.com/facebookresearch/fairseq\n6\nPublished as a conference paper at ICLR 2024\nfurther explore the effectiveness of a seemingly-powerful method DeepNorm (Wang et al., 2022a) in\nstabilising acoustic language model pre-training, but find it to be ineffective. Eventually, we discover\nthat incorporating attention relaxation techniques (Chen et al., 2021b) is beneficial in addressing the\ninstability challenges. We also found that transitioning from post-layer normalisation (Post-LN) to\npre-layer normalisation (Pre-LN) offers a potential solution of allowing training to continue. More\ninformation can be found in Appendix B.3.\nTable 1: Experimental Performances of MERT and Baselines on Downstream Tasks (1/2). The\nbaselines are grouped by supervised and unsupervised pre-training paradigms. The superscripts\ndenote the category of the acoustic teacher used by MERT models. \u201cpublic\u201d refers to the MERT\nmodel trained with only open-source dataset. Results with star* are claimed in the references.\nDataset\nMTT\nGS\nGTZAN\nGTZAN\nEMO\nNsynth\nVocalSet\nVocalSet\nTask\nTagging\nKey\nGenre\nRhythm\nEmotion\nInstrument\nPitch\nTech\nSinger\nMetrics\nROC\nAP\nAccRefined\nAcc\nF1beat\nR2V\nR2A\nAcc\nAcc\nAcc\nAcc\nMusiCNN [41]\n90.6*\n38.3*\n12.8*\n79.0*\n-\n46.6*\n70.3*\n72.6\n64.1\n70.3\n57.0\nCLMR [48]\n89.4*\n36.1*\n14.9*\n68.6*\n-\n45.8*\n67.8*\n67.9\n47.0\n58.1\n49.9\nJukebox-5B [15; 57]\n91.5*\n41.4*\n66.7*\n79.7*\n-\n61.7*\n72.1*\n70.4\n91.6\n76.7\n82.6\nMULE [36]\n91.4*\n40.4*\n66.7*\n73.5*\n-\n57.7*\n70.0*\n74.0*\n89.2*\n75.5\n87.5\nHuBERT-basemusic [25]\n90.2\n37.7\n14.7\n70.0\n88.6\n42.1\n66.5\n69.3\n77.4\n65.9\n75.3\ndata2vec-basemusic [2]\n90.0\n36.2\n50.6\n74.1\n68.2\n52.1\n71.0\n69.4\n93.1\n71.1\n81.4\nMERT-95MK-means\n90.6\n38.4\n65.0\n78.6\n88.3\n52.9\n69.9\n71.3\n92.3\n74.6\n77.2\nMERT-95M-publicK-means\n90.7\n38.4\n67.3\n72.8\n88.1\n59.7\n72.5\n70.4\n92.3\n75.6\n78.0\nMERT-95MRVQ-VAE\n91.0\n39.3\n63.5\n78.6\n88.3\n60.0\n76.4\n70.7\n92.6\n74.2\n83.7\nMERT-330MRVQ-VAE\n91.3\n40.2\n65.6\n79.3\n87.9\n61.2\n74.7\n72.6\n94.4\n76.9\n87.1\n(Previous) SOTA\n92.0 [26]\n41.4 [15]\n74.3 [30]\n83.5 [36]\n80.6 [24]\n61.7\n72.1 [15]\n78.2 [53]\n89.2 [36]\n65.6 [55]\n80.3 [39]\nTable 2: Experimental Performances of MERT and Baselines on Downstream Tasks (2/2). Average\nscores across task are calculated on the SOTA results and models applicable to all the tasks.\nDataset\nMTG\nMTG\nMTG\nMTG\nMUSDB\nAvg.\nTask\nInstrument\nMoodTheme\nGenre\nTop50\nSource Seperation\nMetrics\nROC\nAP\nROC\nAP\nROC\nAP\nROC\nAP\nSDRvocals\nSDRdrums\nSDRbass\nSDRother\nMusiCNN [41]\n74.0\n17.2\n74.0\n12.6\n86.0\n17.5\n82.0\n27.5\n-\n-\n-\n-\n-\nCLMR [48]\n73.5\n17.0\n73.5\n12.6\n84.6\n16.2\n81.3\n26.4\n-\n-\n-\n-\n-\nJukebox-5B [15; 57]\n-\n-\n-\n-\n-\n-\n-\n-\n5.1*\n4.9*\n4.1*\n2.7*\n-\nMULE [36]\n76.6\n19.2\n78.0\n15.4\n88.0\n20.4\n83.7\n30.6\n-\n-\n-\n-\n-\nHuBERT-basemusic [25]\n75.5\n17.8\n76.0\n13.9\n86.5\n18.0\n82.4\n28.1\n4.7\n3.7\n1.8\n2.1\n55.8\ndata2vec-basemusic [2]\n76.1\n19.2\n76.7\n14.3\n87.1\n18.8\n83.0\n29.2\n5.5\n5.5\n4.1\n3.0\n59.9\nMERT-95MK-means\n77.2\n19.6\n75.9\n13.7\n87.0\n18.6\n82.8\n29.4\n5.6\n5.6\n4.0\n3.0\n62.9\nMERT-95M-publicK-means\n77.5\n19.6\n76.2\n13.3\n87.2\n18.8\n83.0\n28.9\n5.5\n5.5\n3.7\n3.0\n63.0\nMERT-95MRVQ-VAE\n77.5\n19.4\n76.4\n13.4\n87.1\n18.8\n83.0\n28.9\n5.5\n5.5\n3.8\n3.1\n63.7\nMERT-330MRVQ-VAE\n78.1\n19.8\n76.5\n14.0\n86.7\n18.6\n83.4\n29.9\n5.3\n5.6\n3.6\n3.0\n64.7\n(Previous) SOTA\n78.8\n20.2 [1]\n78.6\n16.1 [36]\n87.7\n20.3 [1]\n84.3\n32.1 [36]\n9.3\n10.8\n10.4\n6.4 [44]\n64.5\n5\nRESULTS ANALYSIS\n5.1\nPERFORMANCE & EFFICIENCY OF MERT MODELS\nThe results on all the downstream tasks are provided in Tab. 1 and Tab. 2. As suggested by the average\nscores in Tab. 2, MERT-330MRVQ-VAE achieves the same score as the combination of the previous\nSOTAs (from 10 different models even including supervised methods) and becomes the new SOTA on\n4 metrics. It is also noteworthy that the other smaller MERT-95Ms still have comparable performance.\nGenerally, MERTs perform well on tasks focusing on local-level musical information such as beat,\npitch and local timbre such as singer information, and remain competitive on the other tasks requiring\nmore global-level information, such as music tagging, key detection, and genre classification. This\nindicates the blending of acoustic and musical teachers could provide comprehensive guidance\nfor the understanding of music recordings, though pre-trained in only a 5-second context length.\nNevertheless, the performances of our models in tasks with more global music information are close\nto strong baselines, suggesting MERT models are capable of recognising global patterns well, thanks\nto the relative position embeddings and the contextualisation of the transformers.\nIn addition, our models demonstrate good results with limited data, even when training with public\ndata that may lack enough diversity. MERT-95M-public and MERT-95M are both trained on a\n\u223c1k hour dataset and give competitive performance compared with the SOTA and MERT-330M,\nproving that MERT can converge effectively and learns generalisable music representations with\n7\nPublished as a conference paper at ICLR 2024\nlimited training data. Moreover, the MERT-95M-public is trained with Music4ALL (Santana\net al., 2020), a 910-hours public music dataset with mainly pop music and lack of diversity in music\nstyle, and shows comparable performance to other settings. In particular, its performance does not\nhave a significant difference besides genre classification on GTZAN compared to MERT-95M.\nWe evaluate the performance of the MERTRVQ-VAE model with a parameter size of 95M and 330M,\ngiven the use of the EnCodec feature enables us to scale up the dataset compared to the K-means.\nThe results demonstrate that increasing the model size to 330M yields improved performance or\nmaintains similar performance compared to MERT-95MRVQ-VAE (less than 0.1%) on most of the tasks\nbesides beat tracking. More importantly, the lightweight sizes of MERTs open up new possibilities\nfor transferring one general understanding model for large-scale classification or sequence labelling\nMIR tasks. MERT series models achieve better or comparable performance with only 1.9% (95M)\nand 6.6% (330M) parameters compared to the self-supervised baseline Jukebox-5B (Dhariwal et al.,\n2020). Even when our evaluation is in probing setting, most models could not be trained on sequence\nlabelling tasks like beat tracking or source separation with affordable computational costs except for\nMERT and baseline models with similar architecture (Hsu et al., 2021; Baevski et al., 2022).\nTable 3: Evaluation Results from Models Trained with Different Teacher Settings. Models labeled\nwith \u25b32 and \u25b22 suggest that the K-means teachers are trained with the features from \u25b31 and \u25b21 models.\nAll the listed models are sized in (95M) and not augmented with the in-batch noise mixture.\nAcoustic\nTeacher\nAcoustic\nTarget Class\nMusical\nTeacher\nMTT\nGS\nGTZAN\nEMO\nAvg.\nTagging\nKey\nGenre\nEmotion\nROC\nAP\nAccRefined\nAcc\nR2V\nR2A\nK-meansMFCC\n100\nN/A\n89.8\n36.3\n15.1\n66.2\n39.6\n67\n49.4\nK-meansMFCC\n500\n90.3\n38\n17\n70\n40.6\n67.5\n51.3\nK-meansMFCC\n2000\u25b31\n90.2\n37.6\n15.6\n70\n44.3\n67.6\n51.4\nK-meansLogmel+Chroma\n300 + 200 \u25b21\n90.5\n37.6\n55.1\n75.2\n40.1\n68.2\n62.1\nK-meansMFCC\n2000\u25b32\n90.4\n37.5\n16.1\n68.3\n43.9\n67.7\n51.0\nK-meansLogmel+Chroma\n500\u25b22\n90.4\n37.7\n49.2\n72.8\n46.5\n66.9\n60.7\nK-meansMFCC+CQT\n300+200\n89.4\n35.3\n53.2\n69.0\n45.8\n66.8\n60.2\nK-meansLogmel+Chroma\n300 + 200\nCQT\n90.6\n38.4\n65.0\n78.6\n53.1\n68.7\n67.3\nRVQ-VAE\n1024\u00d78 all codebook\nN/A\n90.7\n38.7\n60.5\n72.8\n55.3\n69.0\n65.0\n1024\u00d78 all codebook\nCQT\n90.5\n38.4\n63.2\n77.2\n53.2\n72.3\n66.9\n1024 codebook7\n88.6\n34.4\n63.5\n62.1\n33.3\n53.2\n57.6\n1024 codebook0\n90\n36.7\n59.4\n67.2\n39.7\n64.5\n60.5\n1024\u00d78 random codebook\n90.6\n38.1\n66.8\n73.8\n48.1\n68.6\n65.8\n5.2\nTHE EFFECTIVENESS OF ACOUSTIC & MUSICAL TEACHER\nAs demonstrated in Tab. 3, we explore optimal combinations and selections of the teacher models in\nthe MERT paradigm with a subset of downstream tasks following Castellon et al. (2021), including\nauto-tagging, key detection, genre classification, and emotion recognition.\nWe reproduce the original HuBERT (Hsu et al., 2021) setting on music datasets with the acoustic\nteacher K-meansMFCC\u25b3 1 and the teacher K-meansMFCC\u25b3 2 trained on features produced by HuBERT\nmodel from the first stage, similar to DeepCluster (Caron et al., 2018). We observe that such models\nperform poorly on the key detection and emotion recognition tasks even we increase the dimension\nof the MFCC features from 100 to 2000. As the re-clustering K-means does not bring significant\nimprovement in the second stage pre-training, we stick to the ordinary one stage pre-training to study\nthe influence brought by the teachers with less computational cost.\nGiven that the key information is highly related to the pitch classes of the audio, we then introduce\nsuch inductive bias by providing the K-means acoustic teacher with additional Chroma or CQT\nfeatures, denoted as K-meansLogmel+Chroma\u25b21 and K-meansMFCC+CQT. The additional pitch information\nindirectly brought by the Chroma and CQT features immediately endow the model a certain of level\nof key detection ability and raise the accuracy from 15.6 to 55.1 and 53.2 while keeping or increasing\nperformances on other tasks. This confirms that the potentials of transformer models can be better\nexcavated from more dimensions by introducing extra pseudo prediction targets in the MLM scheme.\nFollowing such an intuition, it could be further assumed that designing a proper multi-task learning\npre-training paradigm can guide the model to produce more general representations for various music\n8\nPublished as a conference paper at ICLR 2024\nunderstanding tasks. We thus propose leveraging the CQT explicitly as a musical teacher to introduce\nharmonic inductive bias during the pre-training. Compared to models trained with only the acoustic\nteacher MFCC\u25b3 1 or K-meansLogmel+Chroma\u25b21, MERT models trained with the newly proposed CQT\nmusical teacher that are naturally more aligned to music audio can achieve significant performance\ngains on not only the key detection task but also the tasks requiring the high-level information like\ngenre classification and emotion recognition.\nHowever, given that K-means models are difficult to scale up on large datasets due to memory and\ncomputational requirements, we use the RVQ-VAE model EnCodec (D\u00b4efossez et al., 2022) as the final\nversion of acoustic teacher without searching for the immeasurable hyper-parameter K. The EnCodec\ncould intuitively provide more comprehensive acoustic information since the audio can be greatly\nrecovered from the intermediate discrete codecs from the encoder by a neural decoder. We observe\nthat leveraging only one top (1024codebook7) or bottom layer (1024codebook0) of the residual codebooks\nin RVQ-VAE already provide substantial information in pre-training, and the use of all layers in\nthe codebooks allows the student models to learn richer acoustic patterns. Although the strategy\nof randomly accessing one of the codebooks for each batch can alleviate the use of GPU memory\nand lead to similar performance compared to using all of them at a time, the setting of predicting\n8 coodebooks together is adopted for faster convergence in the finalised design. By replacing the\nacoustic teacher with RVQ-VAE, MERT achieves an average score of 66.9, similar to that of the\nK-meansLogmel+Chroma\u25b21 version (i.e., 67.3) while largely reducing the cost of scaling up K-means.\n5.3\nSTUDY ON MUSICAL LOSS\nTable 4: Evaluation Results for Musical Loss Study. The listed models are not augmented with the\nin-batch noise mixture.\nParameter\nSize\nAcoustic\nTeacher Model\nAcoustic\nTarget Class\nMusical\nLoss\nWeight\nMTT\nGS\nGTZAN\nEMO\nAvg.\nTagging\nKey\nGenre\nEmotion\nROC\nAP\nAccRefined\nAcc\nR2V\nR2A\n95M\nK-meansLogmel+Chroma\n300 + 200\nN/A\n90.5\n37.6\n55.1\n75.2\n40.1\n68.2\n62.1\n1\n90.6\n38.4\n65.0\n78.6\n53.1\n68.7\n67.3\n2\n90.6\n38.1\n62.7\n66.9\n45.5\n67.9\n62.7\n5\n90.4\n37.3\n65.3\n70.3\n45.7\n68.3\n64.1\n95M\nRVQ-VAE\n1024\u00d78 all codebook\nN/A\n90.7\n38.7\n60.5\n72.8\n55.3\n69.0\n65.0\n1024\u00d78 all codebook\n1\n90.5\n38.4\n63.2\n77.2\n53.2\n72.3\n66.9\nWe conducted a hyperparameter search to determine the optimal weight for the musical loss applied\nto masked audios in the k-means setting. In Table 4, we present the results of the varying musical\nloss weights, which uses the same evaluation setting in \u00a7 5.2. By adjusting the weight, we find that a\nweight of 1 yielded the best overall performance for the base model. We observe that when switching\nthe acoustic teacher to RVA-VAE, the models performs slightly worse on GS than those with K-means.\nOverall, our study provides valuable insights into the impact of musical loss and different acoustic\nmodels on the performance of the acoustic language model. These findings can inform the future\ndevelopment of more effective and efficient models in the domain of acoustic processing.\n6\nCONCLUSION\nIn conclusion, our work underscores the potential of SSL for modelling raw music audio and the\nefficacy of our approach, MERT, in pre-training sizeable models. We present a novel paradigm that\nintegrates RVQ-VAE and CQT teacher models, providing a unique blend of acoustic and musical\ninformation necessary for MLM-based pre-training for music understanding. This integration,\nbolstered by the application of an in-batch noise mixup data augmentation and Pre-LN, enables the\nlearning of robust music representations with further training stability. The performance of the MERT\nmodel surpasses previous SSL baselines, achieving SOTA or comparable results across a wide range\nof MIR tasks while using significantly smaller parameter size. We anticipate that our method and the\nforthcoming public release of our codes and models will catalyse further research into the application\nof SSL in music audio, thereby broadening the scope and depth of human understanding of music.\nDespite being capable of handling longer sequences with relative positional embedding, our models\nare limited by the short 5-second training context, so our approach could be further improved for\ntasks requiring understanding extended musical contexts if trained on longer sequences.\n9\nPublished as a conference paper at ICLR 2024\nLIMITATION AND FUTURE WORK\nOur models are trained using only 5-second audio signals due to constraints in computational\nresources and the extended length of audio signals. Despite these models being capable of handling\nlonger sequences thanks to relative positional embedding, this approach could potentially limit their\nperformance in tasks requiring a comprehensive understanding of extended musical contexts. We\nplan to continue training our models on a longer context once gaining access to more computing\nresources. Moreover, although we propose several techniques to improve the training stability for\nthe acoustic pre-training, we still suffer from the gradient exploding issues with the half-precision\ntraining for settings with larger batch sizes and model sizes. In addition, we observe inverse-scaling\neffect in specific tasks while scaling-up to 330M, which indicates that our design could be further\nimproved by stabilising the training.\nACKNOWLEDGEMENT\nThis paper is a tribute to our talented friend Anqiao Yang, for his friendship and valuable advice\nto this work. Yizhi Li is a Ph.D. student fully funded by the Department of Computer Science,\nUniversity of Manchester, UK. This work is partially funded by Theme based Research Scheme (T45-\n205/21-N), Research Grants Council of Hong Kong. Yinghao Ma is a research student at the UKRI\nCentre for Doctoral Training in Artificial Intelligence and Music, supported by UK Research and\nInnovation [grant number EP/S022694/1]. Emmanouil Benetos is supported by a RAEng/Leverhulme\nTrust Research Fellowship [grant number LTRF2223-19-106]. We acknowledge IT Services at The\nUniversity of Sheffield for the provision of services for High Performance Computing.\nREFERENCES\n[1] Alonso-Jim\u00b4enez, P., Serra, X., and Bogdanov, D. (2022). Music representation learning based on\neditorial metadata from discogs. International Society for Music Information Retrieval (ISMIR).\n[2] Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., and Auli, M. (2022). Data2vec: A general\nframework for self-supervised learning in speech, vision and language. In International Conference\non Machine Learning, pages 1298\u20131312. PMLR.\n[3] Baevski, A. and Mohamed, A. (2020). Effectiveness of self-supervised pre-training for asr. In\nICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 7694\u20137698.\n[4] Baevski, A., Schneider, S., and Auli, M. (2019). vq-wav2vec: Self-supervised learning of discrete\nspeech representations. arXiv preprint arXiv:1910.05453.\n[5] Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. (2020). wav2vec 2.0: A framework for\nself-supervised learning of speech representations. Advances in neural information processing\nsystems, 33:12449\u201312460.\n[6] Bertin-Mahieux, T., Ellis, D. P., Whitman, B., and Lamere, P. (2011). The million song dataset.\nProceedings of the 12th International Conference on Music Information Retrieval (ISMIR).\n[7] B\u00a8ock, S., Korzeniowski, F., Schl\u00a8uter, J., Krebs, F., and Widmer, G. (2016a). madmom: a new\nPython Audio and Music Signal Processing Library. In Proceedings of the 24th ACM International\nConference on Multimedia, pages 1174\u20131178, Amsterdam, The Netherlands.\n[8] B\u00a8ock, S., Krebs, F., and Widmer, G. (2016b). Joint beat and downbeat tracking with recurrent\nneural networks. In ISMIR, pages 255\u2013261. New York City.\n[9] Bogdanov, D., Won, M., Tovstogan, P., Porter, A., and Serra, X. (2019). The mtg-jamendo dataset\nfor automatic music tagging. Machine Learning for Music Discovery Workshop, International\nConference on Machine Learning (ICML 2019).\n[10] Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Teboul, O.,\nGrangier, D., Tagliasacchi, M., and Zeghidour, N. (2022). Audiolm: a language modeling approach\nto audio generation. arXiv preprint arXiv:2209.03143.\n10\nPublished as a conference paper at ICLR 2024\n[11] Brown, J. A. and Bidelman, G. M. (2022). Familiarity of background music modulates the\ncortical tracking of target speech at the \u201ccocktail party\u201d. Brain Sciences, 12(10):1320.\n[12] Brown, J. C. (1991). Calculation of a constant q spectral transform. The Journal of the\nAcoustical Society of America, 89(1):425\u2013434.\n[13] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances\nin neural information processing systems, 33:1877\u20131901.\n[14] Caron, M., Bojanowski, P., Joulin, A., and Douze, M. (2018). Deep clustering for unsupervised\nlearning of visual features. In Proceedings of the European conference on computer vision (ECCV),\npages 132\u2013149.\n[15] Castellon, R., Donahue, C., and Liang, P. (2021). Codified audio language modeling learns\nuseful representations for music information retrieval. In ISMIR.\n[16] Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A.,\nand Mordatch, I. (2021a). Decision transformer: Reinforcement learning via sequence modeling.\nAdvances in neural information processing systems, 34:15084\u201315097.\n[17] Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X.,\net al. (2021b). Wavlm: Large-scale self-supervised pre-training for full stack speech processing.\narXiv preprint arXiv:2110.13900.\n[18] Chen, W., Keast, J., Moody, J., Moriarty, C., Villalobos, F., Winter, V., Zhang, X., Lyu, X.,\nFreeman, E., Wang, J., et al. (2019). Data usage in mir: history & future recommendations.\n[19] Choi, K., Fazekas, G., Sandler, M., and Cho, K. (2017). Transfer learning for music classification\nand regression tasks. arXiv preprint arXiv:1703.09179.\n[20] D\u00b4efossez, A., Copet, J., Synnaeve, G., and Adi, Y. (2022). High fidelity neural audio compres-\nsion. arXiv preprint arXiv:2210.13438.\n[21] Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., and Sutskever, I. (2020). Jukebox: A\ngenerative model for music. arXiv preprint arXiv:2005.00341.\n[22] Engel, J., Resnick, C., Roberts, A., Dieleman, S., Norouzi, M., Eck, D., and Simonyan, K.\n(2017). Neural audio synthesis of musical notes with wavenet autoencoders. In International\nConference on Machine Learning, pages 1068\u20131077. PMLR.\n[23] Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., and Cao, Y.\n(2022). Eva: Exploring the limits of masked visual representation learning at scale. arXiv preprint\narXiv:2211.07636.\n[24] Heydari, M., Cwitkowitz, F., and Duan, Z. (2021). Beatnet: Crnn and particle filtering for online\njoint beat downbeat and meter tracking. arXiv preprint arXiv:2108.03576.\n[25] Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. (2021).\nHubert: Self-supervised speech representation learning by masked prediction of hidden units.\nIEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451\u20133460.\n[26] Huang, Q., Jansen, A., Lee, J., Ganti, R., Li, J. Y., and Ellis, D. P. (2022). Mulan: A joint\nembedding of music audio and natural language. arXiv preprint arXiv:2208.12415.\n[27] Jasmin, K., Dick, F., Holt, L. L., and Tierney, A. (2020). Tailored perception: Individuals\u2019\nspeech and music perception strategies fit their perceptual abilities. Journal of Experimental\nPsychology: General, 149(5):914.\n[28] Kereliuk, C., Sturm, B. L., and Larsen, J. (2015). Deep learning and music adversaries. IEEE\nTransactions on Multimedia, 17(11):2059\u20132071.\n11\nPublished as a conference paper at ICLR 2024\n[29] Knees, P., Faraldo P\u00b4erez, \u00b4A., Boyer, H., Vogl, R., B\u00a8ock, S., H\u00a8orschl\u00a8ager, F., Le Goff, M., et al.\n(2015). Two data sets for tempo estimation and key detection in electronic dance music annotated\nfrom user corrections. In Proceedings of the 16th International Society for Music Information\nRetrieval Conference (ISMIR); 2015 Oct 26-30; M\u00b4alaga, Spain.[M\u00b4alaga]: International Society\nfor Music Information Retrieval, 2015. p. 364-70. International Society for Music Information\nRetrieval (ISMIR).\n[30] Korzeniowski, F. and Widmer, G. (2017). End-to-end musical key estimation using a convolu-\ntional neural network. In 2017 25th European Signal Processing Conference (EUSIPCO), pages\n966\u2013970. IEEE.\n[31] Lample, G. and Charton, F. (2019). Deep learning for symbolic mathematics. arXiv preprint\narXiv:1912.01412.\n[32] Law, E., West, K., Mandel, M. I., Bay, M., and Downie, J. S. (2009). Evaluation of algorithms\nusing games: The case of music tagging. In ISMIR, pages 387\u2013392.\n[33] Li, Y., Yuan, R., Zhang, G., MA, Y., Lin, C., Chen, X., Ragni, A., Yin, H., Hu, Z., He, H.,\net al. (2022). Map-music2vec: A simple and effective baseline for self-supervised music audio\nrepresentation learning. In ISMIR 2022 Hybrid Conference.\n[34] Ma, Y., Yuan, R., Li, Y., Zhang, G., Chen, X., Yin, H., Lin, C., Benetos, E., Ragni, A., Gyenge,\nN., et al. (2023). On the effectiveness of speech self-supervised learning for music. ISMIR 2023:\nProceedings of the 24th International Society for Music Information Retrieval Conference, Nov\n4-9, 2023, Milan, Italy.\n[35] Marchand, U. and Peeters, G. (2015). Swing ratio estimation. In Digital Audio Effects 2015\n(Dafx15).\n[36] McCallum, M. C., Korzeniowski, F., Oramas, S., Gouyon, F., and Ehmann, A. F. (2022).\nSupervised and unsupervised learning of audio representations for music understanding. arXiv\npreprint arXiv:2210.03799.\n[37] Mehr, S. A., Singh, M., Knox, D., Ketter, D. M., Pickens-Jones, D., Atwood, S., Lucas, C.,\nJacoby, N., Egner, A. A., Hopkins, E. J., et al. (2019). Universality and diversity in human song.\nScience, 366(6468):eaax0868.\n[38] Mitsufuji, Y., Fabbro, G., Uhlich, S., St\u00a8oter, F.-R., D\u00b4efossez, A., Kim, M., Choi, W., Yu, C.-Y.,\nand Cheuk, K.-W. (2022). Music demixing challenge 2021. Frontiers in Signal Processing, 1:18.\n[39] Modrzejewski, M., Szachewicz, P., and Rokita, P. (2023). Transfer learning with deep neural\nembeddings for music classification tasks. In Artificial Intelligence and Soft Computing: 21st\nInternational Conference, ICAISC 2022, Zakopane, Poland, June 19\u201323, 2022, Proceedings, Part\nI, pages 72\u201381. Springer.\n[40] Petermann, D., Wichern, G., Wang, Z.-Q., and Le Roux, J. (2022). The cocktail fork problem:\nThree-stem audio separation for real-world soundtracks. In ICASSP 2022-2022 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pages 526\u2013530. IEEE.\n[41] Pons, J. and Serra, X. (2019). musicnn: Pre-trained convolutional neural networks for music\naudio tagging. arXiv preprint arXiv:1909.06654.\n[42] Raffel, C., McFee, B., Humphrey, E. J., Salamon, J., Nieto, O., Liang, D., Ellis, D. P., and\nRaffel, C. C. (2014). Mir eval: A transparent implementation of common mir metrics. In ISMIR,\npages 367\u2013372.\n[43] Rafii, Z., Liutkus, A., St\u00a8oter, F.-R., Mimilakis, S. I., and Bittner, R. (2017). The MUSDB18\ncorpus for music separation.\n[44] Rouard, S., Massa, F., and D\u00b4efossez, A. (2023). Hybrid transformers for music source separation.\nIn ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 1\u20135. IEEE.\n12\nPublished as a conference paper at ICLR 2024\n[45] Saeed, A., Grangier, D., and Zeghidour, N. (2021). Contrastive learning of general-purpose\naudio representations. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 3875\u20133879. IEEE.\n[46] Santana, I. A. P., Pinhelli, F., Donini, J., Catharin, L., Mangolin, R. B., Feltrim, V. D.,\nDomingues, M. A., et al. (2020). Music4all: A new music database and its applications. In\n2020 International Conference on Systems, Signals and Image Processing (IWSSIP), pages 399\u2013\n404. IEEE.\n[47] Soleymani, M., Caro, M. N., Schmidt, E. M., Sha, C.-Y., and Yang, Y.-H. (2013). 1000 songs\nfor emotional analysis of music. In Proceedings of the 2nd ACM international workshop on\nCrowdsourcing for multimedia, pages 1\u20136.\n[48] Spijkervet, J. and Burgoyne, J. A. (2021). Contrastive learning of musical representations. arXiv\npreprint arXiv:2103.09410.\n[49] Tzanetakis, G. and Cook, P. (2002). Musical genre classification of audio signals. IEEE\nTransactions on speech and audio processing, 10(5):293\u2013302.\n[50] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and\nPolosukhin, I. (2017). Attention is all you need. Advances in neural information processing\nsystems, 30.\n[51] Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J.,\net al. (2023). Neural codec language models are zero-shot text to speech synthesizers. arXiv\npreprint arXiv:2301.02111.\n[52] Wang, H., Ma, S., Dong, L., Huang, S., Zhang, D., and Wei, F. (2022a). Deepnet: Scaling\ntransformers to 1,000 layers. arXiv preprint arXiv:2203.00555.\n[53] Wang, L., Luc, P., Wu, Y., Recasens, A., Smaira, L., Brock, A., Jaegle, A., Alayrac, J.-B.,\nDieleman, S., Carreira, J., et al. (2022b). Towards learning universal audio representations. In\nICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 4593\u20134597. IEEE.\n[54] Wilkins, J., Seetharaman, P., Wahl, A., and Pardo, B. (2018). Vocalset: A singing voice dataset.\nIn ISMIR, pages 468\u2013474.\n[55] Yamamoto, Y., Nam, J., and Terasawa, H. (2022). Deformable cnn and imbalance-aware feature\nlearning for singing technique classification. arXiv preprint arXiv:2206.12230.\n[56] Yang, S.-w., Chi, P.-H., Chuang, Y.-S., Lai, C.-I. J., Lakhotia, K., Lin, Y. Y., Liu, A. T., Shi, J.,\nChang, X., Lin, G.-T., et al. (2021). Superb: Speech processing universal performance benchmark.\narXiv preprint arXiv:2105.01051.\n[57] Zai El Amri, W., Tautz, O., Ritter, H., and Melnik, A. (2022). Transfer learning with jukebox\nfor music source separation. In Artificial Intelligence Applications and Innovations: 18th IFIP\nWG 12.5 International Conference, AIAI 2022, Hersonissos, Crete, Greece, June 17\u201320, 2022,\nProceedings, Part II, pages 426\u2013433. Springer.\n[58] Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M. (2021). Soundstream:\nAn end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 30:495\u2013507.\n[59] Zhao, Y. and Guo, J. (2021).\nMusicoder: A universal music-acoustic encoder based on\ntransformer. In International Conference on Multimedia Modeling, pages 417\u2013429. Springer.\n[60] Zhu, H., Niu, Y., Fu, D., and Wang, H. (2021). Musicbert: A self-supervised learning of music\nrepresentation. In Proceedings of the 29th ACM International Conference on Multimedia, pages\n3955\u20133963.\n13\nPublished as a conference paper at ICLR 2024\nAPPENDIX A\nMETHODOLOGY\nA.1\nROBUST REPRESENTATION LEARNING\nAlgorithm 1 Pseudocode description of the pre-training loss calculation in Python style.\n1 def loss_cal(x_batch, x_acoustic_labels):\n2\n# retrieve embeddings for acoustic class\n3\ny_VQ = embedding(x_acoustic_labels)\n4\n# prepare CQT targets\n5\ny_CQT = compute_CQT(x_batch)\n6\n# conduct in-batch mixture\n7\nx_noised = mixture(x_batch)\n8\n# compute the representations\n9\nz = MERT(x_noised)\n10\n11\n# loss calculation\n12\nloss_acoustic = Cross_Entropy(z[mask_idx], y_VQ[mask_idx])\n13\nloss_musical = Mean_Square_Error(z[mask_idx], y_CQT[mask_idx])\n14\nreturn loss_acoustic, loss_musical\nWe introduce \u201cin-batch noise mixup\u201d for music SSL. The mixup augmentation refers to the audio\nclip being added up with a certain ratio of shorter audio excerpts to form an augmented single sample\nduring pre-training, instead of using the original audio. We randomly sample the audio segments\nfrom the same batch and add them to audio at random positions according to some probability.\nTheoretically, sampling from the whole training dataset would provide more randomness and thus\nbe more beneficial to the representation robustness, but we narrow the sampling pool to the same\naudio batch considering the limited computational resources. The mixup could enable the learning of\nmore robust musical representations and force the model to focus on the useful musical source and to\nignore the noise. A pseudocode implementation can be found in Algo. 1.\nUsing the same evaluation setting in \u00a7 5.2, we alter the in-batch mixup probability to evaluate whether\nit is affecting the performance of the model when compound with musical loss. We found the mixup\nprobability provides worse results in MERTK-means but provides better performance for MERTRVQ-VAE.\nTherefore, we determined a probability of 0.5 to be suitable based on the average performance score,\nand set it as a hyper-parameter in the final released model. Such a phenomenon deserves more\nattention.\nTable 5: Evaluation Results for Pre-training Setting Ablation Study.\nParameter\nSize\nAcoustic\nTeacher Model\nAcoustic\nTarget Class\nMusical\nLoss\nWeight\nIn-batch\nMixup\nProbability\nMTT\nGS\nGTZAN\nEMO\nAvg.\nTagging\nKey\nGenre\nEmotion\nROC\nAP\nAccRefined\nAcc\nR2V\nR2A\n95M\nK-meansLogmel+Chroma\n300 + 200\nN/A\nN/A\n90.5\n37.6\n55.1\n75.2\n40.1\n68.2\n62.1\n1\nN/A\n90.6\n38.4\n65.0\n78.6\n53.1\n68.7\n67.3\n2\nN/A\n90.6\n38.1\n62.7\n66.9\n45.5\n67.9\n62.7\n5\nN/A\n90.4\n37.3\n65.3\n70.3\n45.7\n68.3\n64.1\n1\n0.25\n90.6\n37.9\n65.5\n70.0\n49.6\n72.5\n65.2\n1\n0.5\n90.7\n38.6\n64.9\n72.8\n45.3\n71.9\n65.2\n95M\nRVQ-VAE\n1024\u00d78 all codebook\n1\nN/A\n90.5\n38.4\n63.2\n77.2\n53.2\n72.3\n66.9\n95M\n1024\u00d78 all codebook\n1\n0.5\n91.0\n39.3\n63.3\n78.6\n60.0\n76.4\n68.8\n14\nPublished as a conference paper at ICLR 2024\nA.2\nTHE EFFECTIVENESS OF VANILLA RVQ-VAE RREPRESENTATION\nTable 6: Evaluating the EnCodec RVQ-VAE Embeddings.\nDataset\nMTT\nGS\nGTZAN\nEMO\nVocalSet\nVocalSet\nTask\nTagging\nKey\nGenre\nEmotion\nTech\nSinger\nMetrics\nROC\nAP\nAccRefined\nAcc\nR2V\nR2A\nAcc\nAcc\nRVQ-VAE Embedding\n83.4\n26.2\n12.1\n36.5\n10.3\n47.4\n46.3\n69.4\nMERT-95MK-Means\n90.6\n38.4\n65.0\n78.6\n52.9\n69.9\n74.6\n77.2\nMERT-95MRVQ-VAE\n91.0\n39.3\n63.5\n78.6\n60.0\n76.4\n74.2\n83.7\nTo verify that the benefits brought by the MERT pre-training, we further evaluate the performances\nof the continuous representation from the encoder of the RVQ-VAE model, as shown in Tab. 6. These\nsupplementary results indicate that the vanilla continuous representations alone are insufficient for a\nrobust music understanding baseline.\n15\nPublished as a conference paper at ICLR 2024\nAPPENDIX B\nEXPERIMENT DETAILS\nB.1\nDOWNSTREAM TASKS\nWe evaluate the models on 14 downstream tasks to provide a comprehensive view of our method\nand the comparison between baselines. The full descriptions of the datasets and tasks are given as\nfollows.\nMusic Tagging involves determining which of a set of fixed tags apply to a particular song. Tag\ncategories may include genre, instrumentation, mood, tempo (e.g. fast) or other tags. We used two\nlarge datasets: MagnaTagATune (MTT) (Law et al., 2009) and MTG-Jamendo (Bogdanov et al.,\n2019). For both datasets, we limit the tag vocabulary according to official instructions. We use all clips\nin MTT and MTG-Jamendo. Since many of the audio recordings among 5.5k MTG-Jamendo excerpts\nare longer than the 30s, we averaged the multiple embeddings computed with a sliding window\nas the overall embedding. The window length is set to the same default length as in every system.\nFor MERT series, the window length is typically set to 30s. The metrics are the macro-average of\nROC-AUCs and the average precision (AP) / PR-AUC among all top-50 tags.\nKey detection predicts the tonal scale and dominant pitch level of a song. We use Giantsteps (Knees\net al., 2015) as test set and a commonly-used subset of Giantsteps-MTG-keys dataset (Korzeniowski\nand Widmer, 2017) as the training and validation set. The splitting is the same as in (Castellon et al.,\n2021). The metric is a refined accuracy with error tolerance, giving partial credit to reasonable errors\n(Raffel et al., 2014).\nGenre classification estimates the most appropriate genre for each given song. We report the accuracy\nof the GTZAN (Tzanetakis and Cook, 2002) dataset along with ROC and AP on MTG-Genre, since\nthe former task is a multi-class classification and the latter is multi-label. We used the standard\n\u201dfail-filtered\u201d split (Kereliuk et al., 2015) for GTZAN.\nEmotion score regression. The Emomusic dataset (Soleymani et al., 2013) contains 744 music clips\nof 45 seconds in length, each reported on a two-dimensional valence-arousal plane after listening,\nwhere valence indicates positive and negative emotional responses, and arousal indicates emotional\nintensity. We use the same dataset split as (Castellon et al., 2021). The official evaluation metric is\nthe determination coefficient (r2) between the model regression results and human annotations of\narousal (EmoA) and valence (EmoV) (Soleymani et al., 2013). For inference, we split the 45-second\nclip into a 5-second sliding window and averaged the prediction.\nInstrument classification is the process of identifying which instruments are included in a given\nsound. We use the Nsynth (Engel et al., 2017) and MTG-instrument datasets. The former is a\nmonophonic note-level multi-class task with 306k audio samples in 11 instrument classes with\naccuracy as an indicator. The latter is a subset of MTG-Jamendo, containing 25k polyphonic audio\ntracks and 41 instrument tags; each track can contain multiple instruments and is evaluated on ROC\nand AP.\nPitch classification estimates which of the 128 pitch categories the given audio segment belongs to.\nWe use the NSynth dataset for this task. Given these segments are short monophonic audio, this task\nis multi-class, and the accuracy is used as an evaluation metric.\nVocal technique detection involves identifying what singing techniques are contained in a given\naudio clip. We use the VocalSet dataset (Wilkins et al., 2018), which is the only publicly available\ndataset for the study of singing techniques. The dataset contains the vocals of 20 different professional\nsingers (9 female and 11 male) who perform 17 different singing techniques in various contexts for a\ntotal of 10.1 hours. As the audio clips are divided into 3 seconds, the task only requires a judgement\non the type of technique and not on the start and end of the technique. We used the same 10 different\nsinging techniques as in Yamamoto et al. (2022) as a subset and used the same 15 singers as the\ntraining and validation sets and 5 singers as the test set. Since there is no accepted division between\ntraining and validation sets, we selected 9 singers as the training set and 6 singers as the validation\nset. All the 3-second segments that originate from the same recording are allocated to the same part\nof the split (e.g. all are in the training set). The evaluation metric is accuracy.\n16\nPublished as a conference paper at ICLR 2024\nSinger identification identifies the vocal performer from a given recording. We use the VocalSet\ndataset for this task. We randomly divided the dataset into a training set, validation set and testing set\nbased on a ratio of 12:8:5, all containing the same 20 singers.\nBeat tracking is the process of determining whether there is a beat in each frame of a given piece of\nmusic. We use an offline approach to the binary classification, i.e. the model can use information\nfollowing each frame to help with inference. The model needs to output frame-by-frame predictions\nat a certain frequency and post-process them using a dynamic Bayesian network (DBN) (B\u00a8ock et al.,\n2016b) to obtain the final result. The DBN is implemented using madmom (B\u00a8ock et al., 2016a). The\ndataset we use is GTZAN Rhythm (Marchand and Peeters, 2015). We also label the two adjacent\nframes of each label as beat, which is a common way of label smoothing in beat tracking to improve\nthe performance of the model and to compare the SSL model fairly with the spin model. The model is\nevaluated using the f measure implemented in mir eval (Raffel et al., 2014), and the prediction\nis considered correct if the difference between the predicted event and the ground truth does not\nexceed 20ms. In this task, some models were trained on other datasets, and the full GTZAN set was\nused as the test set.\nSource separation. Source separation aims to demix the music recording into its constituent\nparts, e.g., vocals, drums, bass, and others. We adopt MUSDB18 (Rafii et al., 2017), a widely used\nbenchmark dataset in music source separation. MUSDB18 contains 150 full-length music tracks (\u02dc10\nhours), along with multiple isolated stems. We use 86 tracks for training, 14 tracks for validation, and\n50 tracks for evaluation following the official setting in MUSDB18. During training, we randomly\nsample 6-second segments and apply random track mixing for augmentation. Due to the difficulty of\nthis task, we adopt the baseline architecture in the Music Demixing Challenge (MDX) 2021 (Mitsufuji\net al., 2022), which consists of three linear layers and three bi-directional LSTM layers. We directly\ncompute the l2-loss between predicted and ground-truth spectrograms for optimisation. The metric\nfor this task is the Source-to-Distortion Ratio (SDR) defined by MDX 2021 (Mitsufuji et al., 2022),\nwhich is the mean across the SDR scores of all songs.\nB.2\nTESTING PROTOCOL DETAILS\nOur aim is to explore the generality and standardisation of the framework. We, therefore, freeze\nthe parameters of the pre-trained model to extract pre-trained features as fixed depth embeddings\nthat are fed to each downstream task-specific prediction head. This allows for as lightweight a\nsolution as possible for all tasks, thus testing whether the representations are easily reusable across\ndifferent downstream tasks. In the following, we first describe the selected pre-trained baseline model,\nfollowed by the downstream model and training strategy.\nIn order to detect representations with relevant information about the downstream MIR task, we\nuse these representations as input features to train a shallow supervised model on each task. For\nmost tasks we use an MLP with one hidden layer, and for source separation, we use the baseline\nof the demixing data challenge described above, with the 3-layer LSTM used as post-processing.\nSince some representations may require different hyperparameter configurations to be successfully\ntrained, we performed the following hyperparameter search for each mentioned SSL mainly based on\nMARBLE2 benchmark, using the validation set for each downstream task.\n\u2022 Model: {one-layer MLP with 512 hidden units, 3-layer LSTM (source separation only)}\n\u2022 Batch size: {64}\n\u2022 Learning rate: {1e-4, 5e-4, 1e-3, 5e-3, 1e-2}\n\u2022 Dropout probability: {0.25}\n\u2022 Optimizer: Default Adam optimizer\n\u2022 Early Stopping: Fixed across all models with task-specific patience\n\u2022 LR Scheduler: Reduce LR On Plateau, fixed across all models with task-specific patience\nIn addition, although we use the same hyperparameter grid for all tasks, the learning objectives vary\nfrom task to task. For the same task with a uniform dataset, if there are different evaluation metrics,\n2https://marble-bm.shef.ac.uk\n17\nPublished as a conference paper at ICLR 2024\nwe will average the two evaluation metrics. We keep the best validation set results, and use the test\nset results as the final results of the benchmark.\n18\nPublished as a conference paper at ICLR 2024\nB.3\nTRAINING INSTABILITY\nIn the experiments of scaling up to MERT-330M under mix precision training (fp16), we have\nexplored several settings and plot the gradient norm, scale of loss, the MLM loss on acoustic targets,\nand the MLM loss on musical targets (see Fig. 2).\nWe first adopt the Pre-LN setting as in the HuBERT (Hsu et al., 2021) x-large model for stable\ntraining. However, the training crashed around 50K step under this vanilla solution from the speech\nmodel and thus we restart the pre-training at 40K step with gradient clipping threshold reduced from\n10.0 to 1.0. The second run of Pre-LN lasted for 40K steps and crashed due to the same reason of\nreaching minimum loss scale.\nWe suspect the instability could be brought by the increased depth of the Transformer module.\nFollowing the strategies in DeepNorm (Wang et al., 2022a), we tried to alleviate the instability by\ninitialising the Transformer with smaller values and enhancing the residual connection in the Post-LN.\nUnfortunately, such modification causes model collapse around 20K steps.\nWe then turned back to the stable Pre-LN setting and leveraged the attention relaxation trick proposed\nin Chen et al. (2021b). The additional scale constant in softmax calculation in the attention module\nalleviates the overflow problem and allows the final version of MERT-330M model to be trained\nstably over 100K steps.\n0K\n20K\n40K\n60K\n80K\n100K\nTraining Step\n0\n5\n10\n15\n20\n25\n30\nGradient Norm\nPre-Norm | Gradient Clip=10 | Run 1\nPre-Norm | Gradient Clip=1 | Run 2\nPost-Norm | DeepNorm | Gradient Clip=1\nPre-Norm | Attn. Relax | Gradient Clip=1\n(a) Gradient Norm\n0K\n20K\n40K\n60K\n80K\n100K\nTraining Step\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nLoss Scale\nPre-Norm | Gradient Clip=10 | Run 1\nPre-Norm | Gradient Clip=1 | Run 2\nPost-Norm | DeepNorm | Gradient Clip=1\nPre-Norm | Attn. Relax | Gradient Clip=1\n(b) Loss Scale\n0K\n20K\n40K\n60K\n80K\n100K\nTraining Step\n6\n7\n8\n9\nAcousctic MLM Loss on Codebook-0\nPre-Norm | Gradient Clip=10 | Run 1\nPre-Norm | Gradient Clip=1 | Run 2\nPost-Norm | DeepNorm | Gradient Clip=1\nPre-Norm | Attn. Relax | Gradient Clip=1\n(c) Acoustic MLM Loss on Codebook-0\n0K\n20K\n40K\n60K\n80K\n100K\nTraining Step\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nMusical MLM Loss\nPre-Norm | Gradient Clip=10 | Run 1\nPre-Norm | Gradient Clip=1 | Run 2\nPost-Norm | DeepNorm | Gradient Clip=1\nPre-Norm | Attn. Relax | Gradient Clip=1\n(d) Music MLM Loss\nFigure 2: Illustration of the Training Curves of Trials on Large (330M) Models. Only the acoustic\nMLM loss on codebook 0 in the RVQ-VAE is shown as the other seven show similar trends.\n19\nPublished as a conference paper at ICLR 2024\nAPPENDIX C\nREPRESENTATION VISUALISATION\nWe select two of our checkpoints, MERT-95M-publicK-means and MERT-330MRVQ-VAE, and\nvisualise the GTZAN representations with genre annotation shown in Fig. 3, Fig. 4, Fig. 5\nand Fig. 6.\nThe top 6 and top 8 transformer output layers are used in the visualisation for\nMERT-95M-publicK-means and MERT-330MRVQ-VAE, correspondingly. The dimension reduc-\ntion is achieved by the Uniform Manifold Approximation and Projection (UMAP)3, whereas the\nrepresentations from the training set are used to learn the dimension reduction mapping. We observe\nthat representations from both of the checkpoints present a pattern of clustering according to the\ngenre information under different layer settings. Interestingly, the representations from the higher\nlayers do not necessarily show stronger genre-based clustering tendency, which suggests that 1) genre\nmay not be the most abstractive labels for these music examples or 2) the top transformer layers focus\nmore on the MLM pre-training objectives.\nblues\nclassical\ncountry\ndisco\nhiphop\njazz\nmetal\npop\nreggae\nrock\nValid Set\nTest Set\n(a) MERT-95M-publicK-means Transformer Layer 7 Representations of GTZAN.\nblues\nclassical\ncountry\ndisco\nhiphop\njazz\nmetal\npop\nreggae\nrock\nValid Set\nTest Set\n(b) MERT-95M-publicK-means Transformer Layer 8 Representations of GTZAN.\nblues\nclassical\ncountry\ndisco\nhiphop\njazz\nmetal\npop\nreggae\nrock\nValid Set\nTest Set\n(c) MERT-95M-publicK-means Transformer Layer 9 Representations of GTZAN.\nFigure 3: Illustration of the MERT-95M-publicK-means Layer 7 to 9 Pre-trained Representations.\n3https://github.com/lmcinnes/umap\n20\nPublished as a conference paper at ICLR 2024\nblues\nclassical\ncountry\ndisco\nhiphop\njazz\nmetal\npop\nreggae\nrock\nValid Set\nTest Set\n(a) MERT-95M-publicK-means Transformer Layer 10 Representations of GTZAN.\nblues\nclassical\ncountry\ndisco\nhiphop\njazz\nmetal\npop\nreggae\nrock\nValid Set\nTest Set\n(b) MERT-95M-publicK-means Transformer Layer 11 Representations of GTZAN.\nblues\nclassical\ncountry\ndisco\nhiphop\njazz\nmetal\npop\nreggae\nrock\nValid Set\nTest Set\n(c) MERT-95M-publicK-means Transformer Layer 12 Representations of GTZAN.\nFigure 4: Illustration of the MERT-95M-publicK-means Layer 10 to 12 Pre-trained Representations.\n21\nPublished as a conference paper at ICLR 2024\nblues\nclassical\ncountry\ndisco\nhiphop\njazz\nmetal\npop\nreggae\nrock\nValid Set\nTest Set\n(a) MERT-330MRVQ-VAE Transformer Layer 17 Representations of GTZAN.\nblues\nclassical\ncountry\ndisco\nhiphop\njazz\nmetal\npop\nreggae\nrock\nValid Set\nTest Set\n(b) MERT-330MRVQ-VAE Transformer Layer 18 Representations of GTZAN.\nblues\nclassical\ncountry\ndisco\nhiphop\njazz\nmetal\npop\nreggae\nrock\nValid Set\nTest Set\n(c) MERT-330MRVQ-VAE Transformer Layer 19 Representations of GTZAN.\nblues\nclassical\ncountry\ndisco\nhiphop\njazz\nmetal\npop\nreggae\nrock\nValid Set\nTest Set\n(d) MERT-330MRVQ-VAE Transformer Layer 20 Representations of GTZAN.\nFigure 5: Illustration of the MERT-330MRVQ-VAE Layer 17 to 20 Pre-trained Representations.\n22\nPublished as a conference paper at ICLR 2024\nblues\nclassical\ncountry\ndisco\nhiphop\njazz\nmetal\npop\nreggae\nrock\nValid Set\nTest Set\n(a) MERT-330MRVQ-VAE Transformer Layer 21 Representations of GTZAN.\nblues\nclassical\ncountry\ndisco\nhiphop\njazz\nmetal\npop\nreggae\nrock\nValid Set\nTest Set\n(b) MERT-330MRVQ-VAE Transformer Layer 22 Representations of GTZAN.\nblues\nclassical\ncountry\ndisco\nhiphop\njazz\nmetal\npop\nreggae\nrock\nValid Set\nTest Set\n(c) MERT-330MRVQ-VAE Transformer Layer 23 Representations of GTZAN.\nblues\nclassical\ncountry\ndisco\nhiphop\njazz\nmetal\npop\nreggae\nrock\nValid Set\nTest Set\n(d) MERT-330MRVQ-VAE Transformer Layer 24 Representations of GTZAN.\nFigure 6: Illustration of the MERT-330MRVQ-VAE Layer 21 to 24 Pre-trained Representations.\n23\nPublished as a conference paper at ICLR 2024\nAPPENDIX D\nETHICS\nWe have taken great care to ensure that our research adheres to ethical principles and guidelines in the\ncodes of conduct. Specifically, we have not used inappropriate user information in the experiments.\nThe audios are collected from open-access streaming services and open-source datasets, where the\nquality of the audios vary among 16KHz, 24KHz, and 48KHz. The audios would not be hosted and\ndistributed. We believe that our work has the potential to contribute to positive social and scientific\noutcomes regarding the research of automatic music understanding.\n24\n"
  },
  {
    "title": "CodeTF: One-stop Transformer Library for State-of-the-art Code LLM",
    "link": "https://arxiv.org/pdf/2306.00029.pdf",
    "upvote": "1",
    "text": "CODETF: ONE-STOP TRANSFORMER LIBRARY FOR\nSTATE-OF-THE-ART CODE LLM\nNghi D. Q. Bui\u2217, Hung Le, Yue Wang, Junnan Li, Akhilesh Deepak Gotmare, Steven C.H. Hoi\u2217\nSalesforce AI Research\nhttps://github.com/salesforce/CodeTF\nABSTRACT\nCode intelligence plays a key role in transforming modern software engineering. Recently, deep\nlearning-based models, especially Transformer-based large language models (LLMs), have demon-\nstrated remarkable potential in tackling these tasks by leveraging massive open-source code data\nand programming language features. However, the development and deployment of such models\noften require expertise in both machine learning and software engineering, creating a barrier for\nthe model adoption. In this paper, we present CodeTF, an open-source Transformer-based library\nfor state-of-the-art Code LLMs and code intelligence. Following the principles of modular design\nand extensible framework, we design CodeTF with a unified interface to enable rapid access and\ndevelopment across different types of models, datasets and tasks. Our library supports a collection of\npretrained Code LLM models and popular code benchmarks, including a standardized interface to\ntrain and serve code LLMs efficiently, and data features such as language-specific parsers and utility\nfunctions for extracting code attributes. In this paper, we describe the design principles, the architec-\nture, key modules and components, and compare with other related library tools. Finally, we hope\nCodeTF is able to bridge the gap between machine learning/generative AI and software engineering,\nproviding a comprehensive open-source solution for developers, researchers, and practitioners.\nKeywords Transformer \u00b7 code large language models \u00b7 code understanding \u00b7 code generation \u00b7 code intelligence\n1\nIntroduction\nAI has made transformative changes to software engineering industries in recent years. Traditional machine learning\nbased approaches for code intelligence tasks in software engineering entail basic source code analysis tasks. These\ntasks include understanding, analyzing, and modifying source code to improve its quality and maintainability [1, 2, 3].\nIn recent years, deep learning models, particularly Transformer-based large language models (LLMs) pretrained on\nlarge-scale code data (\u201cCode LLMs\u201d) [4, 5, 6, 7, 8, 9, 10], have shown promising results in more challenging code\nintelligence tasks, such as code generation, code completion, code summarization, and code retrieval [11, 12, 13, 14].\nThese models leverage a massive amount of open-source code data from online platforms such as Github [5, 6, 10],\nsometimes supplemented with programming language features [4, 9, 7], to learn meaningful contextual representations\nin code. Initial success in applying these models in practice demonstrates the great potential benefits to society and\nmore specifically, to software development professionals to improve the productivity and quality of their work.\nAs LLMs have demonstrated great values to software developers, developing and deploying such models from scratch\nstill remain a daunting and time-consuming task to the majority of developers. The development of such models often\nrequire substantial experience of model designs and training [15, 16, 17], usually offered by machine learning experts.\nTo deploy these models, professional software developers are then needed to scale and serve the models efficiently in\nsoftware systems. A key obstacle in this field is the set of inconsistent interfaces across models, datasets, and application\ntasks, resulting in highly repetitive efforts in development and deployment of Code LLMs.\nTo address these challenges, we build CodeTF, an open-source comprehensive library for Transformer-based LLMs and\ntheir application in code intelligence tasks. Figure 1 shows an overview of CodeTF. In CodeTF, we created a unified\n\u2217Correspondence: {nghi.bui, shoi}@salesforce.com\narXiv:2306.00029v1  [cs.SE]  31 May 2023\nModel training\nFacilitating parameter-efficient \nmodel fine-tuning (Lora, \nPrefix-tuning, Prompt-tuning)\nModel serving\nSupporting model quantization for \nefficient model inferencing: int8, \nfloat16, GPTQ 4-bit\nCode utility\nBuilt-in code utilities such as  \nmultilingual AST parsers of over 15 \nprogramming languages\nData utility\nExtendable datasets and loading \nutility functions for different \ndownstream application tasks \nModel zoo\nSupporting Transformer LLMs \nfor code including CodeT5, \nCodeGen, and CodeT5+\nBuilt-in AP\nof supp\napp\nModel evaluation\nSupporting model evaluation on \npopular code benchmarks such as \nHumanEval, MBPP\nQ\na\nm\nFigure 1: An overview of CodeTF: We develop a comprehensive Transformer-based library to support development\nand deployment of LLMs for code intelligence. The library contains features to train and serve language models, code\nutilities to process and manipulate code data, and popular research benchmarks to evaluate model performance.\ninterface to enable easy access and customization to individual components. Key components such as model training,\ninference, and datasets are built upon a foundation module specifically designed for code-based data and models. This\ndesign principle allows standardized integration and rapid development from any off-the-shelf models and datasets.\nWithin this unified interface of CodeTF, our library supports a diverse collection of pretrained Transformer-based\nLLMs [5, 4, 18] and code tasks [12, 13, 11]. CodeTF suppots a wide range of LLMs of code, including encoder-\nonly (CodeBERT [6], CodeBERTA), decoder-only (CodeParrot [19], Incoder [20], CodeGen [5], SantaCoder [21],\nStarCoder [22]), encoder-decoder (CodeT5 [4], CodeT5+ [18] CodeTF includes a collection of popular datasets\nsuch as HumanEval and APPS [12, 13, 11, 23] and an interface for efficient loading and serving pretrained models,\ncustom models, and datasets. Through the unified interface, library users is able to not only reproduce and implement\nstate-of-the-art models efficiently, but also seamlessly integrate new models and benchmarks as needed.\nCompared to other domains such as vision and text, code data often demands more rigorous preprocessing and\nmanipulation procedures due to the stringent syntactic rules that must be adhered to in accordance with their respective\nprogramming languages. Consequently, CodeTF introduces an enhanced suite of data processing features which include\nAbstract Syntax Tree (AST) parsers for multiple programming languages leveraging tree-sitter 2, along with utilities for\nextracting code attributes such as method names, identifiers, variable names, and code comments. These tools have\nbeen meticulously designed to facilitate the efficient processing and manipulation of code data during model training,\nfine-tuning, and evaluation. Such features are indispensable for supporting the preprocessing of code into a format\nthat is suitable for language models. For instance, CodeT5 [4] necessitates the extraction of function names and the\nidentification of identifier locations for its multi-objective learning approach\nIn summary, our main contributions of CodeTF are as follows:\n\u2022 A modular and extensible framework for code intelligence tasks, allowing users to easily integrate a wide range of\nprogramming languages, models, and data, as needed\n\u2022 An interface for both serving and training pretrained models and custom models, enabling users to leverage state-of-\nthe-art models and fine-tune them for specific use cases\n\u2022 A collection of popular code corpora with data preprocessing and feature extraction modules, supporting a wide range\nof programming languages and code tasks and promoting data reusability.\n\u2022 Detailed documentation and code examples, facilitating the learning and adoption process for users with varying\nlevels of expertise.\nUltimately, we hope CodeTF will become a useful tool for both software developers and researchers, fostering more\ninnovation in code intelligence research and facilitating wider deployment and application of Code LLMs.\n2https://github.com/tree-sitter/tree-sitter\n2\nCode Format\nLinter, Clang, \u2026\nAST Parser\nTree-sitter, PMD, \nANTLR, \u2026\nProcess Tokens\nNLTK, Spacy, \u2026\n\u2026\u2026..\nDistributed Training\nDevice management, data \nparallel, Deepspeed, \u2026\nTraining Loop\nPytorch, Tensorflow, Keras,...\nHyperparams Config\nBatch size, optimizer, learning \nrate, \u2026\n\u2026..\nQuantization\nBitsandbyes, GPTQ, \nOptimum, CTranslate2, \u2026\nDistributed Serving\nTorchServer, Triton, Faster \nTransformer, \u2026\nModel Deployment\nFlask, Fast API, KubeFlow, \nMLFlow\n\u2026..\nPost Processing\nSpacy, Regex, \u2026.\nMetric Implementation\nPass@K, CodeBLEU\n\u2026..\nData Preparation\nTraining\nServing\nEvaluating\nPractitioner/Researcher\nFigure 2: Illustration of how practitioners utilize Code LLMs for software engineering problems.\n2\nLibrary Design\nFigure 3 provides a detailed overview of the CodeTF system implementation, highlighting its essential components\nthat empower users to effortlessly engage in various code-related tasks. Our system follows a modular architecture,\nenhancing its extensibility by allowing seamless integration of additional programming languages, models, and utilities\ntailored to specific requirements.\n2.1\nMotivation\nTo illustrate the motivation behind CodeTF\u2019s design, we present use cases of practitioners and researchers adopting Code\nLLMs for practical and research purposes (see Figure 2). These use cases involve four main tasks: Data Preparation,\nTraining, Serving, and Evaluation.\nData Preparation:\nIn the first task, users utilize Code LLMs for code completion, code translation to other languages,\ndefect prediction, or code refinement. These tasks rely on making predictions based on input code snippets. However,\nCode LLMs are typically pretrained for next-token prediction tasks, necessitating fine-tuning on specific datasets for\ndesired tasks. This requires intricate steps for preparing source code data, such as formatting the source code (e.g.,\nusing clang-formatter, ESLint), parsing the code into an Abstract Syntax Tree (AST) (e.g., with tree-sitter, PMD), and\nprocessing code tokens (e.g., using NLTK, Spacy). Preprocessing the code to extract important information, rather than\nusing raw code, often leads to better results, especially for tasks like defect prediction or program repair.\nTraining/Fine-Tuning:\nOnce the data is prepared, users may proceed to train or fine-tune Code LLMs. This involves\nadditional tasks such as writing the training loop (using frameworks like PyTorch, Keras, or TensorFlow) over the\nprepared datasets. Moreover, users may need to handle device management (GPUs, TPUs, CPUs) within a distributed\ntraining environment, especially when fine-tuning large models. Training Code LLMs is a critical step to adapt the\nmodels to specific tasks and improve their performance on the target domain. Fine-tuning allows users to leverage their\nown datasets and fine-tune the models\u2019 parameters, enabling them to achieve better results for their specific use cases.\nServing:\nOnce the model is trained, users might want to serve the model for inference. This entails efforts such as\nquantizing the models into 8-bit or 4-bit versions to expedite inference speed, making them more efficient in resource\nusage. Quantization reduces the model size and improves the inference time without significantly sacrificing accuracy.\n3\nDatasets\nHuman Input\nCodeXGLUE\nExtracted code features\nData Loader\nData Processor\nData Cleaner\nComment Removal\nAST Parser\nCode Attribute Extractor\n\u2026\n<>\nHumanEval\n\u2026\nBitsandByte\nGPTQ\n\u2026\nFull Model  \nTraining\nPrefix Tuning\nLORA\nAdaLORA\nP-tuning\n\u2026\npass@k\nCodeBLEU\n\u2026\nCodeT5-\nSummarization\nCodeT5-\nText2Code\nCodeT5-\nCompletion\nCodeGen-\nMono\nCodeGen-\nMulti\n\u2026\nCode data\ncodetf.data_utility\ncodetf.model\ncodetf.trainer\ncodetf.code_utility\ncodetf.performance\ncodetf.predict\nFigure 3: An overview of the system design of CodeTF. The modular design improves the library extensibility\nand allows users to easily customizer and integrate additional models, data, and programming languages as needed.\nKey components in CodeTF include: model zoo - codetf.model, model serving - codetf.predict, model train-\ning - codetf.trainer, data utility - codetf.data_utility, code utility codetf.data_utility, and evaluation\ncodetf.performance.\nAdditionally, deploying the models in specific environments requires setting up the deployment environment and\nensuring compatibility with the target system. This involves configuring the necessary infrastructure and handling the\ndeployment logistics, such as managing server resources and network communication. Serving Code LLMs effectively\nis crucial for integrating them into real-world applications and enabling users to leverage their capabilities.\nEvaluation:\nFor large models, users often want to evaluate their quality against standard benchmarks. For example,\nwhen evaluating fine-tuned models for code generation tasks, users may assess the passing rate (pass@k). This involves\nusing the model to generate code tokens and execute the generated outputs with unit tests. However, the generated code\ntokens might require post-processing steps, such as truncating incomplete generations or applying formatting rules to\nensure code readability. Moreover, metrics specific to code, such as CodeBLEU and Edit Similarity, are also utilized to\nassess the models\u2019 performance accurately. Implementing these metrics can be challenging as different works may\nadopt diverse approaches, hindering the reproducibility of pretrained model results and the verification of newly trained\nmodels. Ensuring proper evaluation of Code LLMs allows users to gain insights into their performance and make\ninformed decisions about their suitability for specific tasks.\nPerforming the above tasks individually requires users to integrate different libraries and tools into a single codebase,\nmaking it challenging to promote the usability of Code LLMs in production-level tools. Existing libraries like\nHuggingFace Transformers (HF-T) provide unified interfaces to handle diverse and complex tools for working with\nLLMs. However, HF-T does not fully cater to the specific needs of code intelligence tasks. CodeTF addresses this\ndrawback by serving as a higher-level layer built upon HuggingFace Transformers and other tools, specifically designed\nto meet the requirements of code intelligence tasks. The next section highlights the key design principles of CodeTF.\n2.2\nDesign Principles\nIn designing CodeTF, we adhere to several important principles that guide our approach to creating a robust and\nuser-centric library for code intelligence tasks. These principles serve as the foundation for the design choices and\nfunctionalities implemented in CodeTF, ensuring that it meets the diverse needs of practitioners and researchers.\n4\n1. Comprehensiveness: CodeTF strives to be a comprehensive library, encompassing various aspects of code large\nlanguage models. This includes functionalities such as loading and serving state-of-the-art models in different\nstyles (encoder-only, decoder-only, and encoder-decoder), pretraining and fine-tuning, evaluation, and source code\nmanipulation for training purposes. CodeTF serves as a one-stop solution, covering these essential aspects.\n2. User-Friendliness: CodeTF prioritizes user-friendliness, ensuring that the library is not just useful but also accessible\nto a wide range of users, from beginners to advanced researchers. We simplify installation and setup processes,\nreducing the need for complex configurations or dependencies. The goal is to ensure that users can easily get started\nwith CodeTF, regardless of their prior experience or expertise.\n3. Usability: While user-friendliness is about the initial experience of getting started with CodeTF, usability focuses on\nthe ease and efficiency of interacting with the library on an ongoing basis. We aim to provide a cohesive and intuitive\ninterface for different code intelligence tasks. This involves simplifying complex tasks such as data collection,\ncode attribute extraction, data conversion for deep learning frameworks, GPU management, and training loop\nconfiguration.\n4. Extensibility: We recognize the rapidly evolving nature of Code LLMs, with new models employing different\ntraining approaches and additional benchmarks emerging. To accommodate future advancements, we design\nCodeTF following software engineering principles such as Object-Oriented Programming, ensuring extensibility\nand flexibility.\n5. Scalability: Managing system scalability during training and serving of Code LLMs can be challenging, particularly\nacross different devices and environments. CodeTF simplifies this process by leveraging scalable infrastructure and\noptimizing resource allocation.\n6. Reproducibility: Reproducibility is a crucial aspect of Code LLMs, especially when evaluating their performance\non well-known benchmarks such as HumanEval [12], MBPP [23], and APPS [13]. However, many released model\ncodebases lack the necessary scripts to reproduce results, hindering the research community\u2019s ability to verify Code\nLLM performance. CodeTF addresses this issue through its unified interface capable of loading a wide range of\nCode LLMs, alongside an Evaluation interface that facilitates reproducibility for the research community.\n2.3\nModules\nGiven the motivation and design principles, we have designed modules that align with these goals. The CodeTF library\nconsists of six main modules: Model Zoo, Model Serving, Model Training, Evaluator, Data Utility, and Code Utility.\n\u2022 The Model Zoo contains configurations for well-known pretrained or fine-tuned models for specific tasks.\nThree major types of Code LLMs are considered: decoder-only (or GPT-style) models, encoder-decoder (or\nSeq2Seq) models , and encoder-only (or BERT-style) models.\n\u2022 The Model Serving module can load models through an interface, specifying the model type (GPT, Seq2Seq,\nBERT), model size, and tasks for which the models are intended (pretraining, summarization, generation, etc.).\nThe module can perform predictions on raw inputs, such as code snippets or natural language descriptions.\n\u2022 The Model Training module provides utilities for pretraining or fine-tuning models, managing GPUs, and\nhandling neural network configurations. It receives the model loaded from the Model Serving module and\ninitializes the weights for training.\n\u2022 The Data Utility module offers utilities to assist the Model Training module in loading well-known datasets.\nThese datasets are preprocessed at various stages into appropriate formats for input to the Model Training\nmodule.\n\u2022 To facilitate source code processing, the Code Utility module provides tools for easy manipulation of source\ncode. This includes loading the AST parser for code parsing and performing traversal on the AST to extract\nimportant code attributes, which is a crucial step in data preprocessing.\n\u2022 Finally, the Evaluator module validates the results of trained models on well-known benchmarks. It can receive\ninstances loaded from the Model Serving module and compute model performance with evaluation metrics.\nMore details about each module can be found in the next section.\n5\nload_model_pipeline(args)\nregistry\nCodeGen\nStarCoder\nCodeT5\nCodeT5+\nCodeBERT\nCodeBERTA\nCausalLM\nSeq2Seq\nBERT\n\u2026\n\u2026\n\u2026\nModel \nClass\nFigure 4: An overview of the model loading pipeline in CodeTF\n3\nModules and Utilities\n3.1\nModel Zoo\nThe Model Zoo - codetf.model provides configurations for both pretrained and fine-tuned checkpoints from well-\nknown LLMs, including different types of Transformer model architectures. Specifically, CodeTF can support a wide\nrange of LLMs: encoder-only models (CodeBERT [6], CodeBERTA [24]), decoder-only models (CodeParrot [19],\nIncoder [20], CodeGen [5], SantaCoder [21], StarCoder [22]), and encoder-decoder models (CodeT5 [4], CodeT5+ [18]).\nThis module streamlines access to state-of-the-art models for code intelligence tasks, enabling users to utilize these\nmodels in their applications.\nIn addition to pretrained checkpoints, CodeTF also support fine-tuned models for specific downstream tasks, such as\ncode summarization, code generation, and code completion. The library allows users to easily access these models\nthrough a unified programming interface across different tasks. Each model is accompanied by a YAML configuration\nfile containing essential information such as the Hugging Face URL, tokenizer, maximum sequence length, and more.\nBy offering an interface to the Hugging Face repository, the Model Zoo module ensures that users can effortlessly stay\nup-to-date with the latest advancements in the field, promoting the adoption and implementation of advanced models\nacross a variety of code intelligence use cases.\n3.2\nModel Serving Module\nThe Model Serving module - codetf.predict provides users with the ability to load pretrained or finetuned model\ncheckpoints from Model Zoo and applies these models for a variety of tasks. CodeTF can support both many challenging\ncode tasks, including code summarization, code completion, text-to-code generation, and code refinement. The Model\nServing module simplifies the deployment of models for an array of code intelligence tasks by offering a convenient\ninterface which receives any new code snippets as input and returns a corresponding model prediction.\nTo facilitate a quick and user-friendly interface for deploying and testing our pretrained models, we recognize the\nimportance of model quantization. Raw Pytorch models can be bulky and time-consuming for delivering inference\nresults (e.g. about 1.2 seconds per sample for CodeGen-16B), making quantization essential to minimize model\nsize while maintaining satisfactory performance. CodeTF incorporates BitsandByte [25], and GPTQ [26] as diverse\nquantization choices to accommodate various requirements. Figure 4 outlines the model loading process. Initially,\nan entry function named load_model_pipeline is invoked where users will specify the model type alongside other\nparameters, such as the model\u2019s name. The \u2019registry\u2019 is a module that registers the relevant model class, including\nCausalLMModel, Seq2SeqModel, BERTModel. Each model class represents a different type of language model\narchitecture for code. Every model class is linked with a configuration file to select the pre-set checkpoint that is defined\nin the Model Zoo. Once the model class is initiated, users can utilize it to make predictions given an input.\n3.3\nModel Training Module\nThe Training Module - codetf.trainer endows users with the ability to access checkpoints from model cards and\ntailor their models to be compatible with existing datasets or tasks. This module provide users an unified interface to\neasily fine-tune LLMs to align with their specific computation budgets and applications. In addition to conventional\nmodel finetuning, we provide users, especially those under constraint computation budgets, an option to employ our\nparameter-efficient finetuning methods.\n6\nBaseTrainer\nCausalLMTrainer\nSeq2SeqTrainer\nBERTTrainer\nLORA, AdaLORA, Prompt-Tuning\nCausaLMModel\nGPT, CodeGen, StarCoder, \nSantaCoder\nSeq2SeqModel\nCodeT5, CodeT5+\nBertModel\nCodeBERT, CodeBERTA\nCausalLMConfig\nSeq2SeqConfig\nBertConfig\nFigure 5: An overview of CodeTF\u2019s Trainer: The BaseTrainer is the base class from which all model trainers\ninherit. The three major Trainer classes are CausalLMTrainer, Seq2SeqTrainer, and BERTTrainer. We design\nthese trainers to be compatible with different families of Language Models (LLMs) for code, including CausalLMModel,\nSeq2SeqModel, and BERTModel, respectively.\nTo promote parameter-efficient fine-tuning, we adopt PEFT3 as the foundation. We incorporate various fine-tuning\ntechniques such as LORA [27], Prefix-Tuning [28], P-Tuning [29], Prompt Tuning [30], and AdaLORA [31]. These\ntechniques have demonstrated significant benefits in tuning LLMs (in billions of parameters) while keeping the training\ncosts affordable. By offering these choices for model fine-tuning, CodeTF empowers users to modify pretrained models\nto their exclusive training requirements.\nFigure 5 illustrates an overview of how the Trainer(s) are implemented. The Trainer classes, which comprise\nof CausalLMTrainer, Seq2SeqTrainer, and BERTTrainer, all inherit from a BaseTrainer. These trainer classes\ncorrespond to different families of Language Models (LLMs) for code, including CausalLMModel, Seq2SeqModel, and\nBERTModel, respectively. The Trainer(s) are assigned with training configurations that are specifically predefined for\neach model family. We further provide configurations for parameter-efficient fine-tuning methods from HuggingFace\u2019s\nPEFT as an option for users, enabling them to effectively fine-tune the models through these built-in configurations.\n3.4\nData Utility Module\nThe Data Utility module - codetf.data_utility provides a suite of tools for data preprocessing, including tokeniza-\ntion, code processing, and data loaders. These utilities ensure that data is appropriately prepared for use in training\nand inference, promoting efficient and reproducible model performance. By offering a comprehensive set of data\npreprocessing tools, the Data Utility module streamlines the process of preparing code data for various machine learning\ntasks.\n3.5\nEvaluator Module\nThe Evaluator Module - codetf.performance provides a unified interface that offers a variety of performance metrics\nspecifically tailored to code intelligence tasks. These metrics include but not limited to the Levenshtein edit similarity,\npass@k [12, 32], and CodeBLEU [33]. By providing an interface to measure these standardized metrics, we seek to\nstreamline the evaluation process and facilitate the reproducibility of results on popular benchmarks. Eventually, this\nunified interface is designed to promote better understanding and comparability between different research papers,\nfostering collaboration and innovation within the research community.\nWe also aim to provide a unified interface that offers a variety of metrics specifically tailored to code intelligence tasks,\nincluding but not limited to pass@k [12, 32], Edit Similarity [34], and CodeBLEU [33]. By providing these standardized\nmetrics, we seek to streamline the evaluation process and facilitate the reproducibility of results on widely recognized\nbenchmarks. Additionally, this unified interface is designed to promote better understanding and comparability between\n3https://github.com/huggingface/peft\n7\ndifferent research papers, fostering collaboration and innovation within the research community. In the long term, we\nenvision that our unified interface for code-specific metrics will serve as a valuable tool for researchers, improving\nmodel generalizability and applications, and ultimately driving innovation in the field of code intelligence.\n3.6\nCode Utility\nBesides the common utility funtions related to model training and testing, we also provide Code Utility module -\ncodetf.code_utility, which assists users to manipulate source code data. CodeTF provides users built-in functions\nto extract important code attributes, utilizing tree-sitter 4 as the parser for 15 programming languages (including Java,\nApex, C, C++, C#, Python, Scala, SOQL, SOSL, PHP, JavaScript, Haskell, Go, Kotlin, Ruby, Rust, Scala, Solidity,\nand YAML). Tree-sitter is a parser generator tool and an incremental parsing library that can construct a concrete\nsyntax tree for a source code file and efficiently update the syntax tree as the source file is edited. While all of the\nsupported languages employ tree-sitter as the backbone to parse code into ASTs, each language relies on a distinct set\nof syntactical rules. We have assembled open-source syntactical rules for each language and prebuilt them into \u201c.so\u201d\nfiles compatible with various operating systems. Currently, we support major operating systems such as Darwin, Linux,\nand Windows. These \u201c.so\u201d files are bundled with CodeTF and can be easily loaded through a programming interface\nwithout any additional installations.\nIn addition to parsing, the Code Utility module offers many other useful supporting functions such as comment\nremoval, extraction of code properties (e.g., comments, variable names, method names), and more. Each programming\nlanguage inherits a BaseCodeUtility class, allowing for the creation of language-specific utility classes (e.g.,\nJavaCodeUtility, PythonCodeUtility, ApexCodeUtility) that implement functions based on the language\u2019s\nproperties. This module ensures the efficient handling and manipulation of code, catering to the unique syntactical rules\nof each supported programming language.\n4\nExample Usage\nUnified interface for loading models and perform inference\nCodeTF provides unified interface to load supported\nmodels. This is helpful for off-the-shelf use of model inference etc. In the this example, we show how to load a CodeT5\nmodel checkpoint for the code summarization task for Python program\n1 from\ncodetf.models\nimport\nload_model_pipeline\n2\n3 summarization_model = load_model_pipeline (model_name=\"codet5\", task=\"sum -python\",\n4\nmodel_type=\"base\", is_eval=True ,\n5\nload_in_8bit=True , weight_sharding =False)\n6\n7 code_snippets = \"\"\"\n8\nvoid\nbubbleSort(int arr [])\n9\n{\n10\nint n = arr.length;\n11\nfor (int i = 0; i < n - 1; i++)\n12\nfor (int j = 0; j < n - i - 1; j++)\n13\nif (arr[j] > arr[j + 1]) {\n14\n// swap arr[j+1] and arr[j]\n15\nint temp = arr[j];\n16\narr[j] = arr[j + 1];\n17\narr[j + 1] = temp;\n18\n}\n19\n}\n20 \"\"\"\n21\n22 # Bubble\nsort\nprogram to sort an integer\narray\n23 summaries = summarization_model .predict ([ code_snippets ])\nUnified interface for fine-tuning models\nCodeTF provides a unified interface for fine-tuning a model based\non supported checkpoints. The following example shows how to load a CodeGen model and fine-tune it using a\npreprocessed CodeXGLUE dataset.\n4https://github.com/tree-sitter/tree-sitter\n8\n1 from\ncodetf.trainer. causal_lm_trainer\nimport\nCausalLMTrainer\n2 from\ncodetf.data_utility. codexglue_dataset\nimport\nCodeXGLUEDataset\n3 from\ncodetf.models\nimport\nload_model_pipeline\n4 from\ncodetf.performance.evaluate\nimport\nEvaluationMetric\n5\n6 model_class = load_model_pipeline (model_name=\"causal -lm\", task=\"pretrained\",\n7\nmodel_type=\"codegen -350M-mono\", is_eval=False ,\n8\nload_in_8bit=False , weight_sharding =False)\n9\n10\n11 dataloader = CodeXGLUEDataset (tokenizer=model_class. get_tokenizer ())\n12 train_dataset , test_dataset , val_dataset = dataloader.load(subset=\"text -to -code\")\n13\n14 evaluator = EvaluationMetric (metric=\"bleu\", tokenizer=model_class.tokenizer)\n15\n16 # peft can be in [\" lora\", \"prefixtuning \"]\n17 trainer = CausalLMTrainer (train_dataset =train_dataset ,\n18\nvalidation_dataset =val_dataset ,\n19\npeft=None ,\n20\npretrained_model_or_path =model_class.get_model (),\n21\ntokenizer=model_class. get_tokenizer ())\n22 trainer.train ()\nUnified interface for to evaluate models on well-known benchmarks\nCodeTF provides a unified interface for\nevaluating models against well-known benchmarks across a variety of metrics. The following example shows how to\nload the evaluation interface and use the pass@k metric to evaluate a CodeGen model on the Human-Eval benchmark.\n1 from\ncodetf.models\nimport\nload_model_pipeline\n2 from\ncodetf.data_utility. human_eval_dataset\nimport\nHumanEvalDataset\n3 from\ncodetf.performance. model_evaluator\nimport\nModelEvaluator\n4\n5 os.environ[\" HF_ALLOW_CODE_EVAL \"] = \"1\"\n6 os.environ[\" TOKENIZERS_PARALLELISM \"] = \"true\"\n7\n8 model_class = load_model_pipeline (model_name=\"causal -lm\", task=\"pretrained\",\n9\nmodel_type=\"codegen -350M-mono\", is_eval=True ,\n10\nload_in_8bit=True , weight_sharding =False)\n11\n12 dataset = HumanEvalDataset (tokenizer=model_class. get_tokenizer ())\n13 prompt_token_ids , prompt_attention_masks , references= dataset.load ()\n14\n15 problems = TensorDataset(prompt_token_ids , prompt_attention_masks )\n16\n17 evaluator = ModelEvaluator (model_class)\n18 pass_at_k = evaluator. evaluate_pass_k (problems=problems , unit_tests=references , k\n=[1 ,10 ,100])\n5\nRelated Work\nIn this section, we provide an overview of the research of LLMs for code and related development of libraries/tools to\nsupport these models.\nLarge Language Models for Code\nLarge language models (LLMs) for code have gained significant attraction in\nrecent years, driven by their ability to support a wide range of code understanding tasks such as code generation [6, 4, 35],\ncode completion [6, 4, 36], program repair [37], and code translation [38]. The success of large language models\n(LLMs) like BERT [39] and GPT [40] in natural language processing (NLP) has inspired researchers to adapt NLP\nlanguage models for code [6, 4, 41, 42, 35, 36, 43, 44, 45, 46]. They usually treat code as natural language text and\nleverage pretraining strategies such as span corruption and causal LM from the NLP domain, which has led to new\nstate-of-the-art results on a wide range of code-related tasks.\nCode LLMs can be grouped into three primary architectures: encoder-only models [6, 7, 47], decoder-only models\n[11, 12, 20, 5], and encoder-decoder models [48, 4, 49, 50]. Encoder-only models excel in understanding tasks such\n9\nTable 1: Comparison of features between CodeTF and HuggingFace Transformers (HF-T). Note that we compare these\nlibraries by features related to the code domain, highlighting functionalities where HF-T may not specifically supports.\nFeature\nCodeTF (Ours)\nHF-T\nUnified Model and Dataset Interface\n\u2713\n\u2713\nUnified Parameter-Efficient Fine-Tuning for Code Intelligence Tasks\n\u2713\n\u2713\nUnified Code Utility Interface for Multiple Programming Languages\n\u2713\nUnified Metric Interface to Evaluate Code Intelligence Benchmarks\n\u2713\nUnified Data Loader Interface to Process Code Intelligence Benchmarks\n\u2713\nModular Library Design\n\u2713\n\u2713\nPretrained Model Checkpoints\n\u2713\n\u2713\nTask-specific Finetuned Model Checkpoints\n\u2713\n\u2713\nas code retrieval [24], while decoder-only models are well-suited for generation tasks like program synthesis [12, 13].\nAlthough encoder-decoder models [4, 48] can be adapted for both code understanding and generation tasks, they don\u2019t\nalways outperform decoder-only or encoder-only models. In CodeTF, we bundle a wide range of models that represent\nfor different architectures into a unified interface.\nUnified Library for Code Intelligence Tasks\nCode LLMs have recently gained significant attention for addressing\nsoftware engineering tasks. However, code intelligence encompasses a broader scope, combining the latest advances in\nartificial intelligence with traditional software engineering methods, such as static analysis, dynamic analysis, pointer\nanalysis, and formal methods, to effectively tackle complex software engineering tasks. In the first version of CodeTF,\nour focus lies in bundling state-of-the-art LLMs for code with additional utilities for traditional software engineering\nmethods, including AST parsers.\nSeveral other libraries with similar goals exist in the industry. NaturalCC [51] is a platform designed to facilitate\nNLP-based big code analysis research for training and reproduction. However, its usability is limited due to suboptimal\ndesign and challenges in extending its capabilities. HuggingFace Transformers [52] is a widely-known library that\noffers user-friendly interfaces for loading pretrained language models across various domains (computer vision, natural\nlanguage processing, code, and time series), garnering significant attention from the research community. Nevertheless,\nits general nature may pose difficulties for users seeking features specifically tailored to the code domain. There are\nalso other open-source repositories for code intelligence, such as CodeT5 [4], CodeGeeX [53], CodeBERT [6], and\nCodeXGLUE [11]. However, most of these are not unified libraries for code intelligence but rather specific models with\ninstructions on how to load the checkpoints.\nTable 5 summarizes the comparison between CodeTF\u2019s key features with HuggingFace Transformers. It is important to\nnote that HuggingFace Transformers (HF-T) is a comprehensive library encompassing state-of-the-art language models\nand utilities for multiple research domains. The comparison provided in Table 5 focuses solely on the features related to\nthe code domain, highlighting areas where HuggingFace Transformers may lack certain functionality.\n6\nFuture Plan & Improvement\nWe continue to actively improve CodeTF as an one-stop open-source library for Code LLMs and code intelligence tasks.\nWe have several plans to expand its capabilities and support more advanced use cases and improve model reproducibility.\nSome key features we aim to incorporate in the future include:\n\u2022 Implementing 4-bit quantization as part of the pretrained and fine-tuned models, enabling even large models such as\nInstructCodeT5+ [18] to run efficiently on commercial laptops or workstations.\n\u2022 Conducting comprehensive evaluations of well-known code intelligence tasks on established benchmarks\n(CodeXGLUE, MBPP, Human-Eval, and APPS). Due to the rapid advancements in the field, there is a lack of\nreproducibility of performance of state-of-the-art models, making it challenging for the research community to adapt\nand foster collaboration.\n\u2022 Enhancing the Code Utility module by adding support for other programming languages, such as Go, Rust, C#, and\nmore. We also plan to include utilities for extracting additional useful features from code, such as call graphs, control\nflow, data flow, and others.\n\u2022 Integrating a broader selection of recent state-of-the-art pretrained language models of code into CodeTF, further\nsolidifying our library as a comprehensive resource in the field.\n10\n7\nConclusion\nIn this paper, we introduce CodeTF, a one-stop open-source Transformer-based library for code intelligence and Code\nLLMs. The library offers a powerful and versatile toolset to develop and deploy LLMs for code-related tasks. With\nits modular architecture and comprehensive set of features, the library enables users to easily perform a variety of\ncode-related tasks, such as code summarization, completion, generation, and refinement. By providing access to\nstate-of-the-art models, fine-tuning and evaluation capabilities, and a range of popular datasets, our library empowers\nusers to leverage the latest advancements in code intelligence research and development.\n8\nBroader Impact and Responsible Use\nWhile models within CodeTF show immense potential in various code-related tasks, they do not provide absolute\nguarantees regarding their code intelligence capabilities. The datasets and pretrained models used in CodeTF may carry\nbiases that could result in misinterpretations, incorrect results, or undesired behaviors. These biases can take multiple\nforms:\n1. Language Bias: The model might prefer certain programming languages over others based on the frequency of\nthe languages in the training data. For instance, if the model is trained mostly on Python code, it might struggle to\ngenerate accurate and idiomatic Java or JavaScript code.\n2. Application-specific Bias: This occurs when a model trained for a particular application or domain is used in a\ndifferent application. For example, a model trained on web development code may perform poorly when tasked with\ngenerating embedded system code.\n3. Library and Framework Bias: This refers to the inherent inclination of a model towards using specific libraries\nor frameworks due to the frequency of their presence in the training dataset. For example, if the model was\npredominantly trained on data using Python\u2019s Pandas for data manipulation, it may be more inclined to use Pandas\neven in situations where other libraries like NumPy or native Python constructs could be more efficient or appropriate.\n4. Language Version Bias: Software languages evolve, with new versions (e.g., Python 2 to Python 3) introducing\nchanges, depreciations, and novel features. If the training dataset is not updated regularly to reflect these changes,\nthe model could generate code using outdated or deprecated conventions of a language.\n5. Coding Style Bias: Coding style can vary significantly between individual coders, teams, or communities. If the\nmodel is trained predominantly on a dataset reflecting a specific style, it may generate code that is in accordance\nwith that style, which may not be the optimal or preferred way for the specific use-case at hand.\n6. Solution Bias: There can often be more than one valid solution to a coding problem. The model might be biased\ntowards the solutions it was exposed to during training and might fail to generate other potentially more efficient or\nelegant solutions.\nIn addition to these potential biases, there are several other crucial considerations:\n1. Sustainability: Energy efficiency is a significant concern in AI, especially with large-scale models. Optimized\nmodels generating more efficient code could reduce the computational resources required to execute such code,\nthereby reducing energy consumption. Ongoing research into more energy-efficient AI training methods can also\ndecrease the energy footprint of AI itself.\n2. Inclusive language: Coding language needs to be inclusive as the field becomes increasingly diverse. Non-\ninclusive terms may discourage and offend many developers. Future work should focus on creating tools to identify\nnon-inclusive language in code and recommend suitable alternatives.\n3. Job loss and automation: While AI carries the potential to automate certain tasks, it is essential to view it as a tool\nthat augments rather than replaces human efforts. Developer tools are usually designed to handle repetitive tasks,\nfreeing developers to focus on complex issues. However, it\u2019s crucial to ensure developers do not become overly\nreliant on these tools and can still code effectively on their own.\n4. Human control and autonomy: Maintaining human control and oversight is crucial, especially in critical areas like\ncode generation. Techniques like explainability and interpretability in AI, along with rigorous testing, ensure AI\nsystems remain under human control and behave as expected. The goal should be to create AI systems that enhance\nhuman capabilities and work collaboratively with humans, rather than replacing them.\nUsers of CodeTF must scrutinize the pretrained models and the general system before their adoption in practical\napplications. We are committed to refining the library by identifying and addressing such potential biases and\n11\ninappropriate behaviors continually. We encourages researchers, software engineers, and AI practitioners to use the\nlibrary responsibly for applications that enhance software quality and developer productivity. However, CodeTF should\nnot be used to develop code intelligence models that could lead to unethical capabilities, such as unauthorized code\nmanipulation, privacy breaches, or the propagation of insecure coding practices. As AI becomes more integrated into\nsoftware development, it is essential to address these ethical and practical considerations. CodeTF is committed to\nsupporting responsible AI practices andgating potential biases and inappropriate behaviors moving forward.\nReferences\n[1] Simone Livieri, Yoshiki Higo, Makoto Matushita, and Katsuro Inoue. Very-large scale code clone analysis and\nvisualization of open source programs using distributed ccfinder: D-ccfinder. In 29th International Conference on\nSoftware Engineering (ICSE\u201907), pages 106\u2013115. IEEE, 2007.\n[2] Carol V Alexandru and Harald C Gall. Rapid multi-purpose, multi-commit code analysis. In 2015 IEEE/ACM\n37th IEEE International Conference on Software Engineering, volume 2, pages 635\u2013638. IEEE, 2015.\n[3] Boyuan Chen and Zhen Ming Jiang. Characterizing and detecting anti-patterns in the logging code. In 2017\nIEEE/ACM 39th International Conference on Software Engineering (ICSE), pages 71\u201381. IEEE, 2017.\n[4] Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. Codet5: Identifier-aware unified pre-trained\nencoder-decoder models for code understanding and generation. In Marie-Francine Moens, Xuanjing Huang,\nLucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November,\n2021, pages 8696\u20138708. Association for Computational Linguistics, 2021.\n[5] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh\nInternational Conference on Learning Representations, 2023.\n[6] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting\nLiu, Daxin Jiang, and Ming Zhou. Codebert: A pre-trained model for programming and natural languages.\nIn Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for Computational Linguistics:\nEMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pages 1536\u20131547.\nAssociation for Computational Linguistics, 2020.\n[7] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey\nSvyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan,\nJian Yin, Daxin Jiang, and Ming Zhou. Graphcodebert: Pre-training code representations with data flow. In ICLR.\nOpenReview.net, 2021.\n[8] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. Unixcoder: Unified cross-modal\npre-training for code representation. In ACL (1), pages 7212\u20137225. Association for Computational Linguistics,\n2022.\n[9] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Hoi. CodeRL: Mastering code\ngeneration through pretrained models and deep reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle\nBelgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\n[10] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n[11] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn\nDrain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong,\nMing Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. Codexglue: A machine\nlearning benchmark dataset for code understanding and generation. In NeurIPS Datasets and Benchmarks, 2021.\n[12] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code.\narXiv preprint arXiv:2107.03374, 2021.\n[13] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir\nPuranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with apps.\nNeurIPS, 2021.\n[14] Md. Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Retrieval\naugmented code generation and summarization. In EMNLP (Findings), pages 2719\u20132734. Association for\nComputational Linguistics, 2021.\n12\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language\nmodeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws:\nbeating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:19523\u201319536,\n2022.\n[17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. An empirical analysis of compute-optimal\nlarge language model training. Advances in Neural Information Processing Systems, 35:30016\u201330030, 2022.\n[18] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. Codet5+: Open\ncode large language models for code understanding and generation. arXiv preprint arXiv:2305.07922, 2023.\n[19] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evaluation of large language\nmodels of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming,\npages 1\u201310, 2022.\n[20] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. arXiv preprint\narXiv:2204.05999, 2022.\n[21] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis,\nNiklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. Santacoder: don\u2019t reach for the stars! arXiv\npreprint arXiv:2301.03988, 2023.\n[22] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint\narXiv:2305.06161, 2023.\n[23] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint\narXiv:2108.07732, 2021.\n[24] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet\nchallenge: Evaluating the state of semantic code search. CoRR, abs/1909.09436, 2019.\n[25] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for\ntransformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n[26] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for\ngenerative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n[27] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n[28] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint\narXiv:2101.00190, 2021.\n[29] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too.\narXiv preprint arXiv:2103.10385, 2021.\n[30] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv\npreprint arXiv:2104.08691, 2021.\n[31] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.\nAdaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023.\n[32] Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d\u2019Autume,\nIgor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy,\nDaniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and\nOriol Vinyals. Competition-level code generation with alphacode. CoRR, abs/2203.07814, 2022.\n[33] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio\nBlanco, and Shuai Ma. Codebleu: a method for automatic evaluation of code synthesis. CoRR, abs/2009.10297,\n2020.\n[34] Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.\nRepocoder: Repository-level code completion through iterative retrieval and generation.\narXiv preprint\narXiv:2303.12570, 2023.\n13\n[35] Ahmed Elnaggar, Wei Ding, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Silvia Severini, Florian\nMatthes, and Burkhard Rost. Codetrans: Towards cracking the language of silicon\u2019s code through self-supervised\ndeep learning and high performance computing. arXiv preprint arXiv:2104.02443, 2021.\n[36] Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, and Tie-Yan Liu. How could neural networks\nunderstand programs? In International Conference on Machine Learning, pages 8476\u20138486. PMLR, 2021.\n[37] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. Practical program repair in the era of large pre-trained\nlanguage models. arXiv preprint arXiv:2210.14179, 2022.\n[38] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsupervised translation of\nprogramming languages. Advances in Neural Information Processing Systems, 33:20601\u201320611, 2020.\n[39] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171\u20134186, 2019.\n[40] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[41] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey\nSvyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with data flow. arXiv preprint\narXiv:2009.08366, 2020.\n[42] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified Pre-training for Program\nUnderstanding and Generation. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-\nT\u00fcr, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings\nof the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 2655\u20132668. Association for\nComputational Linguistics, 2021.\n[43] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating contextual\nembedding of source code. In International Conference on Machine Learning, pages 5110\u20135121. PMLR, 2020.\n[44] Saikat Chakraborty, Toufique Ahmed, Yangruibo Ding, Premkumar T Devanbu, and Baishakhi Ray. Natgen:\ngenerative pre-training by \u201cnaturalizing\u201d source code. In Proceedings of the 30th ACM Joint European Software\nEngineering Conference and Symposium on the Foundations of Software Engineering, pages 18\u201330, 2022.\n[45] Toufique Ahmed and Premkumar Devanbu. Multilingual training for software engineering. In Proceedings of the\n44th International Conference on Software Engineering, pages 1443\u20131455, 2022.\n[46] Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, and Bin Luo. Spt-code: sequence-to-sequence\npre-training for learning source code representations. In Proceedings of the 44th International Conference on\nSoftware Engineering, pages 2006\u20132018, 2022.\n[47] Xin Wang, Yasheng Wang, Yao Wan, Jiawei Wang, Pingyi Zhou, Li Li, Hao Wu, and Jin Liu. CODE-MVP:\nLearning to represent source code from multiple views with contrastive pre-training. In Findings of the Association\nfor Computational Linguistics: NAACL 2022, pages 1066\u20131077, Seattle, United States, July 2022. Association for\nComputational Linguistics.\n[48] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified pre-training for program\nunderstanding and generation. In NAACL-HLT, pages 2655\u20132668. Association for Computational Linguistics,\n2021.\n[49] Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, and Bin Luo. Spt-code: Sequence-to-sequence\npre-training for learning source code representations. In ICSE, pages 1\u201313. ACM, 2022.\n[50] Saikat Chakraborty, Toufique Ahmed, Yangruibo Ding, Prem Devanbu, and Baishakhi Ray. Natgen: generative\npre-training by \u201cnaturalizing\u201d source code. Proceedings of the 30th ACM Joint European Software Engineering\nConference and Symposium on the Foundations of Software Engineering, 2022.\n[51] Yao Wan, Yang He, Zhangqian Bi, Jianguo Zhang, Yulei Sui, Hongyu Zhang, Kazuma Hashimoto, Hai Jin,\nGuandong Xu, Caiming Xiong, et al. Naturalcc: an open-source toolkit for code intelligence. In Proceedings of\nthe ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings, pages 149\u2013153,\n2022.\n[52] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine\nJernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander\n14\nRush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online, October 2020.\nAssociation for Computational Linguistics.\n[53] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang\nLi, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained model for code generation with multilingual\nevaluations on humaneval-x, 2023.\n15\n"
  },
  {
    "title": "MuseCoco: Generating Symbolic Music from Text",
    "link": "https://arxiv.org/pdf/2306.00110.pdf",
    "upvote": "1",
    "text": "MuseCoco: Generating Symbolic Music from Text\nPeiling Lu\u2021\u2217, Xin XuZ\u2217, Chenfei Kang\\\u2217, Botao Yu^\u2217, Chengyi Xing]\u2217, Xu Tan\u2021\u2020, Jiang Bian\u2021\n\u2021Microsoft Research Asia\nZZhejiang University, \\Shanghai Jiao Tong University\n^Nanjing University, ]Dalian University of Technology\n{peil, xuta, jiabia}@microsoft.com\nxxucs@zju.edu.cn, chenfeikang314@gmail.com\nbtyu@foxmail.com, xcyhbp@mail.dlut.edu.cn\nhttps://github.com/microsoft/muzic\nAbstract\nGenerating music from text descriptions is a user-friendly mode since the text is a\nrelatively easy interface for user engagement. While some approaches utilize texts\nto control music audio generation, editing musical elements in generated audio is\nchallenging for users. In contrast, symbolic music offers ease of editing, making it\nmore accessible for users to manipulate specific musical elements. In this paper,\nwe propose MuseCoco, which generates symbolic music from text descriptions\nwith musical attributes as the bridge to break down the task into text-to-attribute\nunderstanding and attribute-to-music generation stages. MuseCoCo stands for\nMusic Composition Copilot that empowers musicians to generate music directly\nfrom given text descriptions, offering a significant improvement in efficiency\ncompared to creating music entirely from scratch. The system has two main\nadvantages: Firstly, it is data efficient. In the attribute-to-music generation stage,\nthe attributes can be directly extracted from music sequences, making the model\ntraining self-supervised. In the text-to-attribute understanding stage, the text is\nsynthesized and refined by ChatGPT based on the defined attribute templates.\nSecondly, the system can achieve precise control with specific attributes in text\ndescriptions and offers multiple control options through attribute-conditioned or\ntext-conditioned approaches. MuseCoco outperforms baseline systems in terms\nof musicality, controllability, and overall score by at least 1.27, 1.08, and 1.32\nrespectively. Besides, there is a notable enhancement of about 20% in objective\ncontrol accuracy. In addition, we have developed a robust large-scale model\nwith 1.2 billion parameters, showcasing exceptional controllability and musicality.\nMusic samples generated by MuseCoco are available via this link 1, and the code\nis available at this link 2.\n1\nIntroduction\nText-to-music generation is an important task in automatic music generation because it allows users\nto generate music more easily and intuitively, using natural language as an interface. This makes it\na user-friendly mode of music generation, particularly for those who do not have a background in\nmusic theory or composition. Previous work [1\u20134] generating musical audio from texts faces the\n\u2217These authors contributed equally to this work.\n\u2020Correspondence: Xu Tan, xuta@microsoft.com\n1https://ai-muzic.github.io/musecoco/\n2https://github.com/microsoft/muzic/musecoco/\nPreprint. Under review.\narXiv:2306.00110v1  [cs.SD]  31 May 2023\nfollowing challenges: 1) Limited adaptability: musical audio generated from texts is less adaptable\nthan symbolic music. Once the audio is produced, it may be difficult to make significant changes\nto the music without starting the generation process over. 2) Lack of control: generating musical\naudio from text descriptions lacks control over specific aspects of the music such as tempo, meter,\nand rhythm since the generation process is not explicitly controlled. However, they can be easily\nhandled by generating symbolic music from given texts. Symbolic music refers to a type of music\nnotation that uses symbols and music language to represent specific music ideas, thus, it is more\nadaptable. Besides, specific musical attributes can be extracted from symbolic data, which can enable\nmore precise control.\nThere are some works [5\u20137] attempt to generate symbolic music from text descriptions, but they\nalso face issues with unnatural text descriptions and poor performance. Limitations exist in certain\napproaches [5] where the model is restricted to specific textual inputs, hindering its ability to\ngeneralize text representations to a more user-friendly format. Besides, some work [5] can only\ncontrol music generation based on limited music aspects, which limits the model\u2019s ability to capture\nthe full range of musical creativity and does not satisfy the requirements of all users. While certain\napproaches [6, 7] allow for natural language inputs, their control accuracy and musical quality is\nlimited due to the requirement for large amounts of paired text-music data. Mubert3 can produce\neditable MIDI compositions, but it merely retrieves and combines pre-existing music pieces rather\nthan generating novel ideas, limiting its responsiveness to input prompts.\nWe propose MuseCoco, a system for generating symbolic music from text descriptions by leveraging\nmusical attributes. MuseCoCo, stands for Music Composition Copilot, is a powerful tool that\nempowers musicians to generate music directly from provided text descriptions. By harnessing the\ncapabilities of MuseCoCo, musicians experience a substantial increase in efficiency, eliminating\nthe need to create music entirely from scratch. Our approach breaks down the task into two stages:\ntext-to-attribute understanding and attribute-to-music generation stage. Musical attributes can be\neasily extracted from music sequences or obtained from existing attribute-labeled datasets, allowing\nthe model in the attribute-to-music generation stage to be trained in a self-supervised manner. Musical\nattributes can also be inferred from natural languages in the text-to-attribute understanding stage.\nTo synthesize paired text-to-attribute data, templates are created for each attribute, and a subset of\nthese templates is combined and further refined into a coherent paragraph using ChatGPT\u2019s language\ngeneration capabilities.\nWith self-supervised training in attribute-to-music generation and supervised learning via the help of\nsynthesized paired data in text-to-attribute understanding, a large amount of symbolic music data\ncan be leveraged without the need of providing textual prompts manually. This can help improve\nmodel performance by increasing data amount and model size simultaneously. Besides, by leveraging\nvarious music attributes, explicit control can be achieved in generating music across multiple aspects,\nenabling fine-grained manipulation and customization of the generated musical output. Moreover,\ndue to the two-stage design, MuseCoco can support multiple ways of controlling. For instance,\nmusicians with a strong knowledge of music can directly input attribute values into the second stage\nto generate compositions, while users without a musical background can rely on the first-stage model\nto convert their intuitive textual descriptions into professional attributes. Thus, MuseCoco allows for\na more inclusive and adaptable user experience than those systems that directly generate music from\ntext descriptions.\nThe main contributions of this work are as follows:\n\u2022 We introduce MuseCoco, a system that seamlessly transforms textual input into musically coherent\nsymbolic compositions. This innovative approach empowers musicians and general users from\ndiverse backgrounds and skill levels to create music more efficiently and with better control.\n\u2022 With this two-stage framework, a large amount of symbolic data can be used without the need\nfor labeled text descriptions. It offers users two engagement options: directly specifying attribute\nvalues or leveraging text descriptions to control the music generation process.\n\u2022 Subjective evaluation results have demonstrated that MuseCoco outperforms baseline systems in\nterms of musicality, controllability, and overall score, achieving a minimum improvement of 1.27,\n1.08, and 1.32, respectively. Additionally, there is a significant boost of about 20% in objective\ncontrol accuracy, further affirming the system\u2019s enhanced performance.\n3https://mubert.com/\n2\n\u2022 We also extend our model to a large scale with 1.2 billion parameters, which exhibits notable\ncontrollability and musicality, further enhancing its performance.\n2\nRelated Work\n2.1\nText-to-Music Generation\nText-to-music generation based on deep learning has been an active research area. Though paired\ntext-music data is scarce, there have been many works for generating audio music from input prompts.\nA database containing music created by musicians and sound designers with three tags (genres,\nmoods and activities) is constructed in MubertAI4. It assigns the closest tags for the input prompt\nand generates combination of sounds from the database based on these tags. Riffusion5 leverages\nstable diffusion [8] to obtain images of music spectrograms paired with input text and produce\naudio from them. Meanwhile, both Mo\u00fbsai [4] and ERNIE-Music [3] apply diffusion models with\nself-collected text-audio datasets to achieve text-audio music generation. Recently, Huang et al. [9]\ncreate a large audio-text dataset to train a joint embedding model linking music audio and natural\nlanguage music descriptions, MuLan, which helps address the absence of text-music paired data. For\nexample, in Noise2Music [2], MuLan assigns captions to unlabeled audio clips to create music-text\npairs. By directly using input text representation from MuLan during inference, MusicLM [1] can be\ntrained to output audio music from Mulan audio representation on existing audio data to complete the\ntext-to-music generation task without paired text-audio data. However, the challenge with musical\naudio generation from texts lies in its limited editability, which poses limitations in the composition\nprocess. By contrast, symbolic music, notated as a sequence of musical symbols, can be simply\ninterpreted and manipulated by humans and machines.\nOnly a few works focus on generating symbolic music from text descriptions. BUTTER [5], a\nmusic-sentence representation learning framework, proposes music representation disentanglement\nand cross-modal alignment to firstly generate music presented by ABC notations6 from text including\nfour musical keywords (the key, meter, style, and others). Limited to the folk song datasets and a few\nmusical factors, it cannot generate symbolic music in many varieties. Instructed with musical input\nprompts, the large language model GPT-4 [7] can also generate ABC notation music, however, without\nany nontrivial form of harmony [10]. Wu and Sun [6] also explores the text-to-music capability of\npre-trained language models. Though fine-tuned with more than 200k text and ABC notation music\npairs, the model cannot align the musical attribute values in loose text with the generated music well\nenough and can only generate music with solo tracks.\nPrevious work directly generates music from text descriptions, which lacks explicit control over\nthe generation process. In this paper, we propose MuseCoco, which can generate symbolic music\nfrom text descriptions with high control accuracy and good musicality. The generated music, in\nits symbolic format, offers easy editability and can be explicitly controlled through attribute values\nderived from text descriptions.\n2.2\nControllable Music Generation\nControllable music generation refers to the ability to exert control over specific aspects or char-\nacteristics of the generated music. By applying conditional generative models (such as condi-\ntional VAE[11\u201313], GAN[14, 15] or diffusion models[2, 3]), previous work leverage different\nconditions to generate music: Some leverage descriptions like emotion [16\u201318], style [19\u201321] or\nstructure[22, 23] as conditions to control music generation. The other applies music descriptions\nsuch as instrumentation[24, 25], chord[11, 26], note density [12, 13], etc. to generate music.\nHowever, general users usually have multiple requirements for generated music, and previous work\nwith limited control over specific music aspects may result in limited expressiveness and diminished\nadaptability to different musical contexts. Besides, previous work can only control music generation\nwith specified music attributes, which restricts the model ability to convey complex and nuanced\nmusical ideas or concepts that can be effectively communicated through textual descriptions. Text-\n4https://github.com/MubertAI/Mubert-Text-to-Music\n5https://www.riffusion.com/about\n6https://abcnotation.com/\n3\n\u2026\nThis pop music comes \nalive with the \ndynamic blend of the \ngrand piano, organ, \nguitar, bass, and \ndrums. The fast\ntempo is set in a 3/4 \nmeter. With a \ncompact pitch range \nspanning five octaves, \nthe impactful musical \nperformance delivers \na broad auditory \nexperience.\nText-to-\nAttribute\nUnderstanding\nAttribute-to-\nMusic\nGeneration\nInstrument:\nTime Signature: 3 / 4\nTempo: Fast\nPitch: 5 octaves\nGenre: Pop\nPiano\nOrgan\nGuitar\nBass\nDrums\nMusical Attributes\nSymbolic Music\nInput Text\nFigure 1: The two-stage framework of MuseCoco. Text-to-attribute understanding extracts diverse\nmusical attributes, based on which symbolic music is generated through the attribute-to-music\ngeneration stage.\nbased input is easily accessible and familiar to users, making it a practical and widely adopted choice\nfor guiding generative tasks, such as text-to-image[8, 27\u201329] and text-to-music generation[2, 1, 6, 10].\nPrevious work has struggled to generate symbolic music directly from textual descriptions provided\nby users. MuseCoco maps text input to music attributes and then utilizes music attributes to control\nmusic generation, which can effectively solve the above problems.\n3\nMuseCoco\n3.1\nOverview\nTo achieve text-to-music controllable generation, MuseCoco incorporates natural language and\nsymbolic music into a two-stage framework that separates text-to-attribute understanding and attribute-\nto-music generation, which are trained independently. The pipeline of MuseCoco is shown in Figure 1.\nIn this section, we will elaborate on their technical design and model architectures respectively.\n3.2\nAttribute-to-Music Generation\nMost musical attributes can be easily obtained by extracting from music sequences (Section 4.1), so\nthe music generation model in the attribute-to-music stage can be trained in a self-supervised way.\nThis method can leverage large amounts of unlabeled data, making it a highly data-efficient approach.\nMusical attributes (Table 1) can be classified into objective attributes like the tempo and meter and\nsubjective attributes like the emotion and genre [30]. Objective attributes refer to quantifiable and\nmeasurable characteristics of musical elements, so they can be extracted from music sequences with\npre-defined rules (please refer to Section 4.1 for more details). Subjective attributes refer to the\nqualities or characteristics of music that are based on personal interpretation, perception, or emotional\nresponse, which can be obtained from existing attribute-labeled datasets. After obtaining attributes\nfrom music sequences, we append those attribute tokens as prefix tokens into music sequences to\nprovide explicit control over the music, which makes it easier to interpret and understand how the\nattributes are influencing the music. Different attribute tokens can be combined or sequenced to\nachieve complex musical expressions and transformations.\nSpecifically, given dataset {V, Y}, where Y is the symbolic music dataset and V is the set of attribute\nvalues, MuseCoco transforms attribute values into prefix tokens to control music generation, as shown\nin Figure 2(b). Using prefix tokens to guide the generation process is an effective method for directing\nthe output towards a particular direction [31\u201334]. We pre-define m musical attributes and their values\nas shown in Appendix A. For each music sequence y = [y1, y2, ..., yn] \u2208 Y, and its attribute values\nv = {v1, v2, ..., vm} \u2208 V, we consider the following distribution:\np(y|v) =\nn\nY\ni=1\np(yi|y<i, v1, v2, ..., vm).\n(1)\n4\nBERT\n\u2026\n\ud835\udc3b\ud835\udc52\ud835\udc4e\ud835\udc511\n\ud835\udc3b\ud835\udc52\ud835\udc4e\ud835\udc512\n\ud835\udc3b\ud835\udc52\ud835\udc4e\ud835\udc51m\n\ud835\udc631\n\ud835\udc632\n\ud835\udc63\ud835\udc5a\nMultiple Classification Heads\nAttribute Classification Tokens\n\u2018The classical music with a 3/8 meter \nis in the style of Chopin\u2019s Waltzes.\u2019\nTokenization\nText Tokens\n\ud835\udc36\ud835\udc3f\ud835\udc461\n\ud835\udc36\ud835\udc3f\ud835\udc462\n\ud835\udc36\ud835\udc3f\ud835\udc46\ud835\udc5a\n\u2026\n\ud835\udc651\n\ud835\udc652\n\ud835\udc65|\ud835\udc99|\n\u2026\nTransformer\n\u2026\nPrefix tokens\nMusic tokens\n\u2026\n[\ud835\udc46\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc61] \ud835\udc60\u0b35\n\ud835\udc60\u0b36\n\ud835\udc66\u0b35\n\ud835\udc60\u0be0\n[\ud835\udc46\ud835\udc38\ud835\udc43]\n\ud835\udc66\u0b36\n\ud835\udc66\u0be1\n\u2026\n[\ud835\udc38\ud835\udc5b\ud835\udc51]\n\ud835\udc66\u0b37\n\ud835\udc66\u0b36\n\ud835\udc66\u0b35\nExtracting\nAttribute Values\n(a) Text-to-attribute understanding\n(b) Attribute-to-music generation\nFigure 2: During training, each stage is independently trained. During inference, the text-to-attribute\nunderstanding stage firstly extracts music attribute values, based on which the attribute-to-music\ngeneration stage secondly generates symbolic music.\nWe encode each attribute value vj into a prefix token sj. Position embeddings for such prefix tokens\n{sj}m\nj=1 are not used. Since it is not usual for users to provide all attribute values in real-world\nscenarios, we introduce a special token, represented by sNA\nj\n, to exclude this attribute (i.e. vj) that is\nnot specified in inputs from influencing the music generation process. Then the input sequence is\nencoded into:\ns1, s2, s3, ..., sm, [SEP], y1, y2, ..., yn.\n(2)\nDuring training, some attribute tokens (e.g., s2, s3) are randomly replaced with special tokens (i.e.,\nsNA\nj\n) to enable adaptation to various attribute combinations. During inference, attribute values are\nprovided directly from inputs. Any attributes that are absent in the inputs are represented by special\ntokens, which are combined with the other prefix tokens to effectively control the music generation\nprocess as required.\n3.3\nText-to-Attribute Understanding\nControl information within input text comprises different musical attributes. Therefore, the text-to-\nattribute task is required to extract musical attribute values from plain text. These attribute values\nwill be used in the attribute-to-music generation stage to generate desired music. As shown in\nFigure 2(a), this stage can be denoted as {X, V}, where X is the input text set, V is the value set of m\npre-defined musical attributes. In the dataset, each instance x \u2208 X is paired with a combination of m\nattribute values v = {vi}m\ni=1, v \u2208 V. Given a pre-trained language model M, BERTlarge [35], x is\nconverted by the tokenizer of M into corresponding tokens {x1, x2, . . . , x|x|}. To adapt to multiple\nattribute classification, we prepend m attribute classification tokens [CLSi]m\ni=1 to input text tokens\nas m attribute classification heads and the encoded [CLSi] head is used to compute the probability\ndistribution over the i class set with a softmax classifier. Position embeddings of [CLSi]m\ni=1 are not\nused to be consistent with the pre-training stage. Specifically, the input x will be encoded to hidden\nvectors:\nh[CLS1], h[CLS2], . . . , h[CLSm], hx1, hx2, . . . , hx|x|\nTable 1: Musical attribute descriptions.\nType\nAttribute\nDescription\nObjective\nInstrument\nplayed instruments in the music clip\nPitch\nthe number of octaves covering all pitches in one music clip\nRhythm Danceability\nwhether the piece sounds danceable\nRhythm Intensity\nthe intensity of the rhythm\nBar\nthe total number of bars in one music clip\nTime Signature\nthe time signature of the music clip\nKey\nthe tonality of the music clip\nTempo\nthe tempo of the music clip\nTime\nthe approximate time duration of the music clip\nSubjective\nArtist\nthe artist (style) of the music clip\nGenre\nthe genre of the music clip\nEmotion\nthe emotion of the music clip\n5\nTable 2: An example of synthesizing a text-attribute pair. We randomly select a template from the\navailable templates for each attribute. Here shows two of each. Then the templates are refined by\nChatGPT and then filled in values.\nAttribute\nValue\nTemplate\nKey\nMajor\nThis music is composed in the [KEY] key.\nThis music\u2019s use of [KEY] key creates a distinct atmosphere.\nBar\n13 ~ 16\nThe song is composed of approximately [NUM_BARS] bars.\nThe song comprises [NUM_BARS] bars.\nEmotion\nHappiness\nThe music is imbued with [EMOTION].\nThe music has a [EMOTION] feeling.\nTime Signature\n4 / 4\nThe [TIME_SIGNATURE] time signature is used in the music.\nThe music is in [TIME_SIGNATURE].\nRefine via ChatGPT and fill in place-\nholders with values:\n{Key, Bar, Emotion, Time Signature}\nThe music is imbued with happiness, and the major key in this music\nprovides a powerful and memorable sound. The song progresses\nthrough 13 ~ 16 bars, with 4/4 as the meter of the music.\nThe probability distribution of the value of i-th attribute is pi(vi|x) = Softmax(Wih[CLSi] + bi),\nwhere Wi and bi are learnable parameters. The cross-entropy loss of i-th attribute is\nLi = \u2212 1\n|X|\nX\nx\u2208X\nlog pi(vi|x)\nDuring training, all learnable parameters are fine-tuned by minimizing the sum of attribute cross-\nentropy losses L = Pm\ni=1 Li on {X, V}.\n3.4\nData Construction\nBy using musical attributes to break down the text-to-music generation task into the attribute-to-music\ngeneration stage and the text-to-attribute understanding stage, we can leverage large amounts of\nsymbolic music data without text descriptions. In the attribute-to-music generation stage, attributes\ncan be extracted from music sequences with rules or obtained from attribute-labeled datasets (detailed\nin Section 4.1). The system only requires paired data in the text-to-attribute stage, we synthesize\nthese text-attribute pairs in the following steps:\n1. Write templates for each attribute: As shown in Table 2, we write several templates as a set for\neach attribute, where its values are represented with a placeholder. By utilizing this placeholder,\nwe can accommodate diverse combinations of attribute values without requiring exact values.\n2. Create attribute combinations and concatenate their templates as paired texts: The generation\nprocess is usually controlled by multiple attributes together. Hence, constructing various different\ncombinations of attribute values and paired text is necessary. Because of the long-tail distribution\nper attribute value in real-world data, to enrich the diversity of paired text-attribute training data\nand avoid the long-tailed issue, we stochastically create v per instance based on pre-defined\nmusical attributes and their values on our own to ensure the number of instances including vi is\nbalanced, i.e., each value of each attribute is sampled equally. And then, the paired texts of these\ncreated combinations are synthesized by simply concatenating their corresponding templates,\nrandomly chosen from template sets of each attribute.\n3. Refine concatenated templates via ChatGPT: Since simply concatenated templates are less\nfluent than real users\u2019 input, ChatGPT7 is utilized to refine them as shown in Table 2.\n4. Fill in placeholders: Finally, attribute values or their synonyms fill in placeholders, ensuring that\nthe resulting text effectively conveys the intended meaning and maintains a consistent narrative\nstructure.\nThrough these steps, we can independently construct the datasets for either of the two stages without\nthe need for paired text-music data.\n7https://chat.openai.com/\n6\n4\nExperiments\n4.1\nExperiment Setup\nTable 3: Statistics of the used datasets.\nDataset\n#MIDI\nMMD [36]\n1,524,557\nEMOPIA [16]\n1,078\nMetaMidi[37]\n612,088\nPOP909 [38]\n909\nSymphony [39]\n46,360\nEmotion-gen\n25,730\nTotal (after filtering)\n947,659\nDatasets\nTo train the attribute-to-music generation\nstage and evaluate our proposed method, we collect an\nassortment of MIDI datasets from online sources. Ta-\nble 3 lists all of the used datasets along with their respec-\ntive counts of valid MIDI files. Specifically, the MMD\ndataset [36] consists of many datasets collected from\nthe internet8. The Emotion-gen dataset is generated\nby our internal emotion-controllable music generation\nsystem, and the others are all publicly released datasets.\nWe did the necessary data filtering to remove dupli-\ncated and poor-quality samples, and there are 947,659\nMIDI samples remaining. From each MIDI file, we ran-\ndomly extracted 3 clips within 16 bars. The attributes\ndescribed in Table 1 were then extracted from each clip.\nThe objective attribute values used in the training are directly extracted from MIDI files and the\nsubjective attribute values are obtained from some of the datasets (details in Appendix A).\nSystem Configuration\nFor the attribute-to-music generation stage, we use a REMI-like [40]\nrepresentation method to convert MIDI into token sequences. We apply Linear Transformer[41] as\nthe backbone model, which consists of 16 layers with causal attention and 12 attention heads. The\nhidden size is 1024 and FFN hidden size is 4096, yielding an approximate parameter count of 203\nmillion. The max length of each sample is 5120, covering at most 16-bar music segments. During\ntraining, the batch size is 64. The dropout rate is set to 0.1. We use Adam optimizer [42] with\n\u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. The learning rate is 2 \u00d7 10\u22124 with warm-up step 16000 and an\ninvert-square-root decay. In text-to-attribute understanding, we leverage BERTlarge9 as the backbone\nmodel and the max sequence length of it is set to 256, which covers common user input. We use\n1,125 thousand samples for fine-tuning with a train/valid portion of 8:1. During training, the batch\nsize is 64 and the learning rate is 1 \u00d7 10\u22125.\nEvaluation Dataset and Metrics\nTo evaluate MuseCoco, we construct a standard test set includ-\ning 5,000 text-attribute pairs in the same way in Section 3.4. Musical attributes of each test sample\noriginated from real music in the test set of the attribute-to-music stage, instead of creating them on\nour own to make sure musical rationality and all values of the attributes are covered in the test set for\nthorough testing. Meanwhile, in order to accord with usual user inputs, we randomly assign the NA\nvalue (meaning the attribute is not mentioned in the text) to some attributes per sample to synthesize\ntext prompts with different lengths.\nTo evaluate the models objectively, we propose the metric called Average Sample-wise Accuracy\n(ASA), which is calculated by determining the proportion of correctly predicted attributes in each\nsample, followed by the calculation of the average prediction accuracy across the whole test set.\nTo conduct a subjective evaluation of MuseCoco\u2019s performance, we employ a user study. We\ninvite individuals with musical backgrounds to fill out the questionnaires (details in Appendix B.1)\nParticipants are asked to rate the following metrics on a scale of 1 (lowest) to 5 (highest):\n\u2022 Musicality: This metric assesses the degree to which the generated music exhibits qualities akin to\nthe artistry of a human composer.\n\u2022 Controllability: This metric measures how well the samples adhere to the musical attribute values\nspecified in the text descriptions.\n\u2022 Overall: This metric quantifies the overall quality of this generated music considering both its\nmusicality and controllability.\n8We obtained the dataset for this work with the help of the authors, as it was not publicly available.\n9https://huggingface.co/bert-large-uncased\n7\n4.2\nComparison with Baselines\nBaselines\nIn this study, we compare our method to two existing works for generating symbolic\nmusic from 21 text descriptions randomly selected from the standard test set:\n\u2022 GPT-4: GPT-4 [7] is a large-scale language model that demonstrated its capabilities in various\ndomains, including music. Following Bubeck et al. [10], we instruct GPT-4 to generate ABC\nnotation music with the task-specific prompts (in Appendix B.5) using the official web page10\nmanually.\n\u2022 BART-base: Wu and Sun [6] release a language-music BART-base11, which shows a solid perfor-\nmance. Text descriptions are fed into this model and guide it to generate ABC notation music for\ncomparison.\nFor a fair comparison, we convert ABC notation music generated by baselines into MIDI music\nusing music2112. As for the subjective evaluation, well-designed questionnaires including generated\nmusic from baselines and our method are distributed to individuals, who are all in music backgrounds\nand required to score the subjective metrics described in Section 4.1 (details in Appendix B.3).\nMeanwhile, to objectively compare the model ability, we calculate the average sample-wise accuracy\nof generated music for both baselines and our method (details in Appendix B.2).\nMain Results\nTable 4 reports the main results of the comparison. In terms of musicality, MuseCoco\nachieves the highest score (mean 4.06 out of 5), indicating its ability to generate music closely\nresembling compositions by humans and approaching the quality of real-world music. As for the\nconditional generation ability, MuseCoco outperforms all baselines in terms of controllability and the\naverage sample-wise accuracy at 1.08 and 19.95% respectively, which illustrates the effectiveness of\ncontrolling musical attributes with the two-stage framework. GPT-4 also can generate more coherent\nmusic with input prompts than BART-base due to its powerful ability of language understanding.\nMeanwhile, the best overall score (mean 4.13 out of 5) of MuseCoco shows our method is capable\nof generating the most favorite and fair-sounding music through the auditory test. Music samples\ngenerated by MuseCoco are available via this link13.\nTable 4: Comparison between MuseCoco, GPT-4, and BART-base. ASA stands for the average\nsample-wise accuracy.\nMusicality\nControllability\nOverall\nASA (%)\nMuseCoco\n4.06 \u00b1 0.82\n4.15 \u00b1 0.78\n4.13 \u00b1 0.75\n77.59\nGPT-4 [7]\n2.79 \u00b1 0.97\n3.07 \u00b1 1.05\n2.81 \u00b1 0.97\n57.64\nBART-base [6]\n2.19 \u00b1 1.14\n2.02 \u00b1 1.09\n2.17 \u00b1 1.03\n31.98\n4.3\nMethod Analysis\nIn this section, we conduct analysis experiments on the two stages respectively.\n4.3.1\nAnalysis on Text-to-Attribute Understanding\nAttribute Comprehension\nTo evaluate the ability to extract each attribute from text, we test the\ntext-to-attribute model on the standard test set and show the classification accuracy of each attribute\nin Appendix A. Each accuracy consistently surpasses 99%, which proves that the model exhibits\nexceptional performance on all attributes and showcases the high accuracy and reliability of the text\nunderstanding ability.\nDifferent classification heads\nWe explore the effectiveness of using multiple classification heads\nand report the ASA shown in Table 5. The model with multiple classification heads outperforms the\n10https://chat.openai.com/\n11https://huggingface.co/sander-wood/text-to-music\n12http://web.mit.edu/music21/\n13https://ai-muzic.github.io/musecoco/\n8\none-head BERT by 39.87%, which illustrates that each head can learn their corresponding attribute\nknowledge, and using multiple heads can improve the overall performance.\nDifferent text synthetic strategies\nIn order to showcase the effectiveness of the refinement strategy\noutlined in Section 3.4, we evaluate its impact on enhancing the fluency and diversity of the text\nwithin the training set during the text-to-attribute stage. We engage musicians to write 17 text\ndescriptions manually to help evaluate and contrast the effectiveness of synthesis strategies w/ and\nw/o the refinement step. As shown in Table 6, we observe that fine-tuning the model with 25%\npartially refined text achieves better performance than with the simply concatenated templates.\nTable 5: Analysis on multiple classification heads.\nASA (%)\nOne Head\n60.09\nMultiple Classification Heads\n99.96\nTable 6: Analysis on text refinement.\nASA (%)\nConcatenated\n75.88\nConcatenated + Refined\n78.11\n4.3.2\nAnalysis on Attribute-to-Music Generation\nAttribute Control Performance\nTo evaluate the controllability of the attribute-to-music generation\nmodel, we report the control accuracy results for each attribute in Appendix B.4. The average attribute\ncontrol accuracy is 80.42%, demonstrating a strong capability of the model to effectively respond to\nthe specified attribute values during the music generation process.\nStudy on Control Methods\nWe compare Prefix Control, which is the default method of our\nmodel that uses prefix tokens to control music generation, with two other methods: 1) Embedding:\nAdd attribute input as embedding to token embedding; 2) Conditional LayerNorm: Add attribute\ninput as a condition to the layer norm layer [43, 44]. We utilize Musicality and average attribute\ncontrol accuracy as evaluation metrics. For more details on this experiment, please refer to the\ndescription in Appendix B.4. We report evaluation results in Table 7. It is shown that Prefix Control\noutperforms other methods in terms of musicality and average attribute control accuracy, with a\nminimum improvement of 0.04 and 19.94% respectively, highlighting its superior capability to\ncapture the relationship between attributes and music.\nTable 7: Comparison of different control methods. Musicality reflects the quality of the generated\nmusic. Average attribute control accuracy represents the control accuracy over all attributes, which\ncan reflect controllability.\nMethod\nMusicality \u2191\nAverage attribute control accuracy (%) \u2191\nEmbedding\n2.97 \u00b1 0.91\n36.94\nConditional LayerNorm\n3.11 \u00b1 1.02\n47.46\nPrefix Control\n3.15 \u00b1 1.02\n67.40\nStudy on Model Size\nWe conduct a comparative analysis between two different model sizes\nto determine whether increasing the model size would result in improved generated results. The\nparameter configurations for these model sizes are presented in Table 8. The default model, referred\nto as large, is the default model for the attribute-to-music generation stage. Additionally, we utilize\nxlarge model for comparison, which consists of approximately 1.2 billion parameters. The training\nof xlarge model follows the same settings outlined in Section 4.1. The objective evaluation results\nare displayed in Table 8, which indicates that increasing the model size enhances controllability. To\nfurther evaluate the performance, we will conduct subjective listening tests to compare the subjective\nevaluation results for these two model sizes.\n4.3.3\nComments from Musicians\nWe invite professional musicians to give their comments on generated music samples from given\ntexts based on our system. Here are some feedbacks:\nThe generated music closely resembles human compositions, displaying a high level of accuracy\nand creativity. It provides inspiration for creative compositions, showcasing interesting motifs and\n9\nTable 8: Comparison of different model sizes in the attribute-to-music generation stage. Average\nobjective attribute control accuracy represents the control accuracy over objective attributes, which\ncan reflect controllability.\nModel Size\nLayers\ndmodel\nParameters\nAverage objective attribute control accuracy (%) \u2191\nlarge\n16\n1024\n203M\n83.63\nxlarge\n24\n2048\n1.2B\n87.15\ndemonstrating skillful organization of musical elements. This work greatly improves composition\nefficiency, saving approximately one day of time. \u2014- from Musician A\nThe generated music offers arrangement inspiration, exemplified by the idea of combining right-hand\narpeggios and left-hand bass melody to facilitate creative expansion in composition. It sparks the\nconcept of blending classical and popular music genres described in texts. \u2014- from Musician B\nThe generated music significantly reduces my composition time, saving anywhere from 2 days to 2\nweeks, particularly when it comes to instrumental arrangements that are outside my familiarity. The\ncomposition incorporates inspirational elements in some sections of the piece. For example, I very\nmuch enjoyed the journey through the conflicts and resolutions. In some areas towards the end it felt\nlike I was embarking on an adventure up a mountain and through grassy fields, very interesting. \u2014-\nfrom Musician C\nThe generated music presents the various elements of the text well, which provides great convenience\nfor musicians to edit the music. For example, music in the style of Bach is well-created, and multiple\ninstruments in the generated music are arranged harmoniously. Meanwhile, as for music teachers,\nMuseCoco is very helpful in generating desired musical examples for education. \u2014- from Musician\nand Music Teacher D\nThe feedback from musicians has demonstrated our ability to enhance their workflow efficiency by\nreducing redundant tasks and providing creative inspiration.\n5\nConclusion\nIn conclusion, this paper makes several significant contributions to the field of music generation and\nthe application of AI in creative tasks. We introduce MuseCoco, a system that seamlessly transforms\ntext descriptions into musically coherent symbolic compositions. This innovative approach empowers\nmusicians and general users from diverse backgrounds and skill levels to create music more efficiently\nand with greater control.\nSecond, we present a two-stage design that simplifies the learning process and enhances controllability.\nThis design reduces the reliance on large amounts of paired text-music data and improves more\nexplicit control through various aspects of attributes. By leveraging a large amount of symbolic music\ndata, we achieve impressive musicality and establish a coherent connection between text descriptions\nand the generated music.\nOur research demonstrates the potential of AI technologies in facilitating idea generation and\nstreamlining the composition process for music creation tasks. By offering a powerful and adaptable\ntool like MuseCoco, we aim to inspire and empower artists to overcome the challenges they face in\ntheir creative pursuits and unlock new possibilities in music composition. The utilization of generative\nAI for creative purposes often raises concerns regarding copyright and ownership, which necessitates\ncareful consideration moving forward. Limitations of our work can be seen in Appendix C.\n6\nAcknowledgment\nWe would like to express our sincere gratitude to Amy Sorokas for her invaluable help in connecting\nwith musicians. We would also like to thank musician, Grentperez, and musicians from Central\nConservatory of Music in China for the collaboration and constructive feedback, which contributed to\nthe success of this work. Meanwhile, we sincerely appreciate all members of Wenqin Piano Society\nfrom Zhejiang University for completing the most questionnaires.\n10\nReferences\n[1] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen,\nA. Roberts, M. Tagliasacchi et al., \u201cMusiclm: Generating music from text,\u201d arXiv preprint\narXiv:2301.11325, 2023.\n[2] Q. Huang, D. S. Park, T. Wang, T. I. Denk, A. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu, C. Frank\net al., \u201cNoise2music: Text-conditioned music generation with diffusion models,\u201d arXiv preprint\narXiv:2302.03917, 2023.\n[3] P. Zhu, C. Pang, S. Wang, Y. Chai, Y. Sun, H. Tian, and H. Wu, \u201cErnie-music: Text-to-waveform\nmusic generation with diffusion models,\u201d arXiv preprint arXiv:2302.04456, 2023.\n[4] F. Schneider, Z. Jin, and B. Sch\u00f6lkopf, \u201cMo\u00fbsai: Text-to-music generation with long-context\nlatent diffusion,\u201d arXiv preprint arXiv:2301.11757, 2023.\n[5] Y. Zhang, Z. Wang, D. Wang, and G. Xia, \u201cBUTTER: A representation learning framework for\nbi-directional music-sentence retrieval and generation,\u201d in Proceedings of the 1st Workshop on\nNLP for Music and Audio (NLP4MusA).\nAssociation for Computational Linguistics, 16 Oct.\n2020, pp. 54\u201358.\n[6] S. Wu and M. Sun, \u201cExploring the efficacy of pre-trained checkpoints in text-to-music generation\ntask,\u201d arXiv preprint arXiv:2211.11216, 2022.\n[7] OpenAI, \u201cGpt-4 technical report,\u201d arXiv preprint arXiv:2303.08774, 2023.\n[8] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cHigh-resolution image\nsynthesis with latent diffusion models,\u201d in CVPR.\nIEEE, 2022, pp. 10 674\u201310 685.\n[9] Q. Huang, A. Jansen, J. Lee, R. Ganti, J. Y. Li, and D. P. W. Ellis, \u201cMulan: A joint embedding\nof music audio and natural language,\u201d arXiv preprint arXiv:2208.12415, 2022.\n[10] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee,\nY. Li, S. Lundberg et al., \u201cSparks of artificial general intelligence: Early experiments with gpt-4,\u201d\narXiv preprint arXiv:2303.12712, 2023.\n[11] Z. Wang, D. Wang, Y. Zhang, and G. Xia, \u201cLearning interpretable representation for controllable\npolyphonic music generation,\u201d arXiv preprint arXiv:2008.07122, 2020.\n[12] H. H. Tan and D. Herremans, \u201cMusic fadernets: Controllable music generation based on\nhigh-level features via low-level feature modelling,\u201d arXiv preprint arXiv:2007.15474, 2020.\n[13] D. von R\u00fctte, L. Biggio, Y. Kilcher, and T. Hofmann, \u201cFigaro: Controllable music generation\nusing learned and expert features,\u201d in The Eleventh International Conference on Learning\nRepresentations, 2023.\n[14] P. Neves, J. Fornari, and J. Florindo, \u201cGenerating music with sentiment using transformer-gans,\u201d\narXiv preprint arXiv:2212.11134, 2022.\n[15] Y. Zhu, K. Olszewski, Y. Wu, P. Achlioptas, M. Chai, Y. Yan, and S. Tulyakov, \u201cQuantized\ngan for complex music generation from dance videos,\u201d in Computer Vision\u2013ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXVII.\nSpringer, 2022, pp. 182\u2013199.\n[16] H.-T. Hung, J. Ching, S. Doh, N. Kim, J. Nam, and Y.-H. Yang, \u201cEmopia: A multi-modal pop\npiano dataset for emotion recognition and emotion-based music generation,\u201d arXiv preprint\narXiv:2108.01374, 2021.\n[17] L. N. Ferreira, L. Mou, J. Whitehead, and L. H. Lelis, \u201cControlling perceived emotion in\nsymbolic music generation with monte carlo tree search,\u201d in Proceedings of the AAAI Conference\non Artificial Intelligence and Interactive Digital Entertainment, vol. 18, no. 1, 2022, pp. 163\u2013\n170.\n[18] C. Bao and Q. Sun, \u201cGenerating music with emotions,\u201d IEEE Transactions on Multimedia,\n2022.\n11\n[19] H. H. Mao, T. Shin, and G. Cottrell, \u201cDeepj: Style-specific music generation,\u201d in 2018 IEEE\n12th International Conference on Semantic Computing (ICSC).\nIEEE, 2018, pp. 377\u2013382.\n[20] W. Wang, X. Li, C. Jin, D. Lu, Q. Zhou, and Y. Tie, \u201cCps: Full-song and style-conditioned music\ngeneration with linear transformer,\u201d in 2022 IEEE International Conference on Multimedia and\nExpo Workshops (ICMEW).\nIEEE, 2022, pp. 1\u20136.\n[21] K. Choi, C. Hawthorne, I. Simon, M. Dinculescu, and J. Engel, \u201cEncoding musical style with\ntransformer autoencoders,\u201d in International Conference on Machine Learning.\nPMLR, 2020,\npp. 1899\u20131908.\n[22] B. Yu, P. Lu, R. Wang, W. Hu, X. Tan, W. Ye, S. Zhang, T. Qin, and T.-Y. Liu, \u201cMuseformer:\nTransformer with fine- and coarse-grained attention for music generation,\u201d in Proceedings of\nAdvances in Neural Information Processing Systems (NeurIPS), 2022, pp. 1376\u20131388.\n[23] X. Zhang, J. Zhang, Y. Qiu, L. Wang, and J. Zhou, \u201cStructure-enhanced pop music generation\nvia harmony-aware learning,\u201d in Proceedings of the 30th ACM International Conference on\nMultimedia, 2022, pp. 1204\u20131213.\n[24] J. Ens and P. Pasquier, \u201cMmm: Exploring conditional multi-track music generation with the\ntransformer,\u201d arXiv preprint arXiv:2008.06048, 2020.\n[25] S. Di, Z. Jiang, S. Liu, Z. Wang, L. Zhu, Z. He, H. Liu, and S. Yan, \u201cVideo background music\ngeneration with controllable music transformer,\u201d in Proceedings of the 29th ACM International\nConference on Multimedia, ser. MM \u201921.\nNew York, NY, USA: Association for Computing\nMachinery, 2021, pp. 2037\u20132045.\n[26] S. Wu, X. Li, and M. Sun, \u201cChord-conditioned melody choralization with controllable har-\nmonicity and polyphonicity,\u201d arXiv preprint arXiv:2202.08423, 2022.\n[27] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, \u201cHierarchical text-conditional image\ngeneration with clip latents,\u201d arXiv preprint arXiv:2204.06125, 2022.\n[28] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gon-\ntijo Lopes, B. Karagol Ayan, T. Salimans et al., \u201cPhotorealistic text-to-image diffusion models\nwith deep language understanding,\u201d Advances in Neural Information Processing Systems, vol. 35,\npp. 36 479\u201336 494, 2022.\n[29] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, \u201cZero-\nshot text-to-image generation,\u201d in International Conference on Machine Learning.\nPMLR,\n2021, pp. 8821\u20138831.\n[30] P. R. Cook, Music, Cognition, and Computerized Sound: An Introduction to Psychoacoustics.\nMIT press, 2001.\n[31] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher, \u201cCtrl: A conditional\ntransformer language model for controllable generation,\u201d arXiv preprint arXiv:1909.05858,\n2019.\n[32] X. L. Li and P. Liang, \u201cPrefix-tuning: Optimizing continuous prompts for generation,\u201d arXiv\npreprint arXiv:2101.00190, 2021.\n[33] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, \u201cPre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing,\u201d ACM Computing\nSurveys, vol. 55, no. 9, pp. 1\u201335, 2023.\n[34] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell et al., \u201cLanguage models are few-shot learners,\u201d Advances in neural\ninformation processing systems, vol. 33, pp. 1877\u20131901, 2020.\n[35] J. Devlin, M. Chang, K. Lee, and K. Toutanova, \u201cBERT: pre-training of deep bidirectional\ntransformers for language understanding,\u201d in NAACL-HLT (1).\nAssociation for Computational\nLinguistics, 2019, pp. 4171\u20134186.\n12\n[36] M. Zeng, X. Tan, R. Wang, Z. Ju, T. Qin, and T.-Y. Liu, \u201cMusicBERT: Symbolic music\nunderstanding with large-scale pre-training,\u201d in Findings of the Association for Computational\nLinguistics (ACL Findings), 2021, pp. 791\u2013800.\n[37] J. Ens and P. Pasquier, \u201cBuilding the metamidi dataset: Linking symbolic and audio musical\ndata.\u201d in ISMIR, 2021, pp. 182\u2013188.\n[38] Z. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai, and G. Xia, \u201cPOP909: A pop-song\ndataset for music arrangement generation,\u201d in Proceedings of International Society for Music\nInformation Retrieval Conference (ISMIR), 2020, pp. 38\u201345.\n[39] J. Liu, Y. Dong, Z. Cheng, X. Zhang, X. Li, F. Yu, and M. Sun, \u201cSymphony generation with\npermutation invariant language model,\u201d arXiv preprint arXiv:2205.05448, 2022.\n[40] Y.-S. Huang and Y.-H. Yang, \u201cPop music transformer: Beat-based modeling and generation\nof expressive pop piano compositions,\u201d in Proceedings of ACM International Conference on\nMultimedia (MM), 2020, pp. 1180\u20131188.\n[41] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, \u201cTransformers are rnns: Fast autore-\ngressive transformers with linear attention,\u201d in International Conference on Machine Learning.\nPMLR, 2020, pp. 5156\u20135165.\n[42] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint\narXiv:1412.6980, 2014.\n[43] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville, \u201cFilm: Visual reasoning with a\ngeneral conditioning layer,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence,\nvol. 32, no. 1, 2018.\n[44] M. Chen, X. Tan, B. Li, Y. Liu, T. Qin, S. Zhao, and T.-Y. Liu, \u201cAdaspeech: Adaptive text to\nspeech for custom voice,\u201d arXiv preprint arXiv:2103.00993, 2021.\n13\nA\nAttribute Information\nTable 9 shows the detailed pre-defined musical attribute values. The value NA of each attribute refers\nto that this attribute is not mentioned in the text. Objective attributes can be extracted from MIDI\nfiles with heuristic algorithms and subjective attributes are collected from existing datasets, as shown\nin Table 10.\nTable 9: Detailed attribute values.\nAttributes\nValues\nInstrument\n28 instruments: piano, keyboard, percussion, organ, guitar, bass, violin, viola,\ncello, harp, strings, voice, trumpet, trombone, tuba, horn, brass, sax, oboe,\nbassoon, clarinet, piccolo, flute, pipe, synthesizer, ethnic instrument, sound effect, drum.\nEach instrument: 0: played, 1: not played, 2: NA\nPitch\nRange\n0-11: octaves,12: NA\nRhythm\nDanceability\n0: danceable, 1: not danceable, 2: NA\nRhythm\nIntensity\n0: serene, 1: moderate, 2: intense, 3: NA\nBar\n0: 1-4 bars, 1: 5-8 bars, 2: 9-12 bars, 3: 13-16 bars, 4: NA\nTime\nSignature\n0: 4/4, 1: 2/4, 2: 3/4, 3: 1/4, 4: 6/8, 5: 3/8, 6: other tempos, 7: NA\nKey\n0: major, 1: minor, 2: NA\nTempo\n0: slow (<=76 BPM), 1: moderato (76-120 BPM), 2: fast (>=120 BPM),\n3: NA\nTime\n0: 0-15s, 1: 15-30s, 2: 30-45s, 3: 45-60s, 4: >60s, 5: NA\nArtist\n0-16 artists: Beethoven, Mozart, Chopin, Schubert, Schumann,J.S.Bach, Haydn, Brahms,\nHandel, Tchaikovsky, Mendelssohn,Dvorak, Liszt, Stravinsky, Mahler,Prokofiev, Shostakovich,\n17: NA\nGenre\n21 genres: new age, electronic, rap, religious,international,easy listening, avant garde,\nRNB, latin, children, jazz, classical, comedy, pop, reggae, stage, folk, blues,\nvocal, holiday, country, symphony\nEach genre: 0: with, 1: without, 2: NA\nEmotion\n0-3: the 1-4 quadrant in Russell\u2019s valence-arousal emotion space\n4: NA\nTable 10: Extraction methods and sources of each attributes.\nType\nAttribute\nExtraction Method\nObjective\nInstrument\ndirectly extracted from MIDI\nPitch range\ncalculated based on the pitch range\nRhythm danceability\njudged with the ratio of downbeat\nRhythm intensity\njudged with the average note density\nBar\ndirectly extracted from MIDI\nTime signature\ndirectly extracted from MIDI\nKey\njudged with the note pitches based on musical rules\nTempo\ndirectly extracted from MIDI\nTime\nderived from the time signature and the number of bars\nSubjective\nArtist\nprovided by a classical music dataset in MMD [36]\nGenre\nprovided by MAGD14, a classical music dataset in MMD [36] and Symphony [39]\nEmotion\nprovided by EMOPIA [16] and the emotion-gen dataset\nB\nExperiments\nB.1\nUser study with baselines\nIn the user study, participants were provided with generated music samples along with their corre-\nsponding textual prompts. For each text description, each model (i.e., BART-base, GPT-4, MuseCoco)\ngenerated three different music clips. In each questionnaire, three samples generated with the same\n14\ntext conditions were randomly picked from samples generated by BART-base, GPT-4, and MuseCoco\nrespectively as a group. Each participant was asked to evaluate 7 groups for comparison. Three\nsubjective metrics, musicality, controllability, and an overall score, are rated on a scale of 1 (lowest)\nto 5 (highest). The participants were first requested to evaluate their music profession level, as\ndepicted in Table 11. To ensure the reliability of the assessment, only individuals with at least music\nprofession level 3 were selected, resulting in a total of 19 participants. Secondly, they were instructed\nto independently evaluate two separate metrics: musicality and controllability, ensuring that scoring\nfor one metric did not influence the other. They are also asked to give an overall score to evaluate the\ngenerated music comprehensively. For the collected results, we computed the mean and variance for\neach metric. The results can be found in Table 4.\nTable 11: Music Profession Level\nLevel\nDescription\n1\nI rarely listen to music.\n2\nI haven\u2019t received formal training in playing or music theory, but I often listen to music and have my\npreferred styles, musicians, and genres.\n3\nI have some basic knowledge of playing an instrument or music theory, but I haven\u2019t received formal\ntraining.\n4\nI haven\u2019t received formal training, but I have self-taught myself some aspects such as music theory or\nplaying an instrument. I am at an amateur level (e.g., CCOM piano level 6 or above).\n5\nI have received professional training in a systematic manner.\nB.2\nObjective Comparison with baselines\nIn this section, we introduce how to calculate the objective metric, the average sample-wise accuracy\n(ASA), in Table 4. As for MuseCoco, ten music clips are generated per prompt and we report ASA\nof them among the overall standard test set. Since it is labor-intensive to leverage GPT-4 with the\nofficial web page, we only guide GPT-4 to produce five music clips per prompt and calculate the\nASA of 21 prompts randomly sampled from the standard test set. Besides, we utilize the released\ntext-tune BART-base checkpoint15 to generate five music clips per prompt and report the ASA of 44\nprompts randomly chosen from the standard test set.\nB.3\nText-to-attribute understanding\nAs shown in Table 12, all attribute control accuracy is close or equal to 100%, which indicates our\nmodel with multiple classification heads in the text-to-attribute understanding stage performs quite\nwell.\nB.4\nDetails of Analysis on Attribute-to-music Generation\nAttribute Control Accuracy\nWe report the control accuracy for each attribute on the test dataset,\nas shown in Table 13. The average attribute control accuracy of 80.42%, which provides substantial\nevidence for the model\u2019s proficiency in effectively controlling music generation using music attributes.\nStudy on Control Methods\nTo verify the effectiveness of the control method in the attribute-to-\nmusic generations stage, we compare Prefix Control with two methods: Embedding and Conditional\nLayerNorm. For efficiency, we conducted this study on reduced-size models as follows: The backbone\nmodel of this experiment is a 6-layer Linear Transformer with causal attention. The hidden size is\n512 and the FFN hidden size is 2048. The other experiment configuration is the same as Section 4.1.\nSince the control accuracy of objective attributes can be easily calculated, we only need to measure\nthe controllability of each subjective attribute in listening tests. The control accuracy of each attribute\nis shown in Table 13. Finally, the average attribute control accuracy can be calculated based on the\naccuracy results from both types of attributes. To measure the controllability of subjective attributes\n15https://huggingface.co/sander-wood/text-to-music\n15\nTable 12: Attribute control accuracy (%) of the text-to-attribute understanding model. I: Instrument.\nAttribute\nAccuracy(%)\nAttribute\nAccuracy(%)\nAttribute\nAccuracy(%)\nI_piano\n100.00\nI_clarinet\n99.92\nGenre_comedy_spoken\n100.00\nI_keyboard\n99.92\nI_piccolo\n99.94\nGenre_pop_rock\n100.00\nI_percussion\n100.00\nI_flute\n99.62\nGenre_reggae\n100.00\nI_organ\n100.00\nI_pipe\n100.00\nGenre_stage\n100.00\nI_guitar\n99.92\nI_synthesizer\n100.00\nGenre_folk\n100.00\nI_bass\n99.84\nI_ethnic_instruments\n99.98\nGenre_blues\n100.00\nI_violin\n99.92\nI_sound_effects\n99.98\nGenre_vocal\n100.00\nI_viola\n99.96\nI_drum\n100.00\nGenre_holiday\n100.00\nI_cello\n99.92\nGenre_new_age\n99.98\nGenre_country\n100.00\nI_harp\n100.00\nGenre_electronic\n100.00\nGenre_symphony\n100.00\nI_strings\n99.96\nGenre_rap\n100.00\nBar\n100.00\nI_voice\n99.70\nGenre_religious\n100.00\nTime Signature\n100.00\nI_trumpet\n99.96\nGenre_international\n100.00\nKey\n100.00\nI_trombone\n99.94\nGenre_easy_listening\n100.00\nTempo\n99.84\nI_tuba\n100.00\nGenre_avant_garde\n100.00\nOctave\n100.00\nI_horn\n99.94\nGenre_rnb\n100.00\nEmotion\n99.80\nI_brass\n100.00\nGenre_latin\n100.00\nTime\n100.00\nI_sax\n99.84\nGenre_children\n100.00\nRhythm Danceability\n100.00\nI_oboe\n99.94\nGenre_jazz\n100.00\nRhythm Intensity\n99.88\nI_bassoon\n99.96\nGenre_classical\n100.00\nArtist\n100.00\nTable 13: Accuracy (%) of each attribute for attribute-to-music generation. I: Instrument.\nAttribute\nAccuracy(%)\nAttribute\nAccuracy(%)\nI_piano\n96.20\nI_clarinet\n90.63\nI_keyboard\n79.55\nI_piccolo\n86.67\nI_percussion\n65.19\nI_flute\n86.73\nI_organ\n80.55\nI_pipe\n70.73\nI_guitar\n91.81\nI_synthesizer\n78.28\nI_bass\n93.11\nI_ethnic_instruments\n77.69\nI_violin\n87.88\nI_sound_effects\n51.74\nI_viola\n92.03\nI_drum\n95.96\nI_cello\n86.50\nBar\n71.80\nI_harp\n74.87\nTime Signature\n99.14\nI_strings\n86.08\nKey\n57.42\nI_voice\n75.82\nTempo\n92.71\nI_trumpet\n84.86\nOctave\n61.56\nI_trombone\n84.64\nTime\n65.82\nI_tuba\n93.08\nRhythm Danceability\n88.04\nI_horn\n80.13\nRhythm Intensity\n80.47\nI_brass\n77.27\nGenre\n73.08\nI_sax\n81.74\nEmotion\n69.45\nI_oboe\n85.23\nArtist\n50.03\nI_bassoon\n90.72\n16\nPrompt\nThe music is imbued with [EMOTION], creating a \ncaptivating and emotionally rich experience. It is \ncomposed in the [KEY] key, which adds a powerful \nand memorable sound to the composition. The song \nunfolds through [NUM_BARS] bars, with \n[TIME_SIGNATURE] serving as the meter, further \nenhancing its rhythmic and melodic structure.\nChatGPT Response\nPlease combine the following sentences to one \nparagraph:\nThe music is imbued with [EMOTION]. The [KEY] \nkey in this music provides a powerful and \nmemorable sound. The song progresses through \n[NUM_BARS] bars. [TIME_SIGNATURE] is the \nmeter of the music.\nFigure 3: ChatGPT refines concatenated templates in the prompt.\nI want a music in the ABC notation format and assign \nproper MIDI instrument IDs as the following: \nX:1\nT:Forest Stroll\nM:3/4\nL:1/4\nQ:1/4=80\nK:C\nV:1\n%%MIDI program 90 % Synthesizer\nE/2F/2|G2A|G2F|E2D|C2E|F2G|A3-|A2E/2F/2|\nG2A|G2F|E2D|C2E|F2G|A2F|E2D|C3||\nV:2\n%%MIDI program 68 % Oboe\nG/2A/2|B2C|B2A|G2F|E2G|A2B|c3-|c2G/2A/2|\nB2C|B2A|G2F|E2G|A2B|c2A|G2F|E3||\nV:3\n%%MIDI program 73 % Flute\nz/2A/2|B2C|B2A|G2F|E2G|A2B|c3-|c2z/2A/2|\nB2C|B2A|G2F|E2G|A2B|c2A|G2F|E3||\nV:4\n%%MIDI program 42 % Cello\nC/2D/2|E2F|E2D|C2B,|A,2C|D2E|F3-|F2C/2D/2|\nE2F|E2D|C2B,|A,2C|D2E|F2D|C2B,|A,3||\nCompose ABC notation music as the format above \naccording to the music description as follow: The music \nshould feature the cello and flute. The music is in 4/4 \nmeter. The song is 31 ~ 45 seconds in length.\nPrompt\nX:1\nT:Whispering Winds\nM:4/4\nL:1/4\nQ:1/4=80\nK:C\nV:1\n%%MIDI program 73 % Flute\nA/2B/2|c2d|e2c|A2G|F2A|B2c|d3-|d2A/2B/2|\nc2d|e2c|A2G|F2A|B2c|d2A|G2F|A3||\nV:2\n%%MIDI program 42 % Cello\nC/2E/2|F2G|A2F|C2B,|A,2C|E2F|G3-|G2C/2E/2|\nF2G|A2F|C2B,|A,2C|E2F|G2C|B,2A,|C3||\nGPT-4 Response\nFigure 4: GPT-4 generates ABC notation tunes based on the prompt.\n(such as emotion and genre), we invite 12 participants to conduct a listening test. Each participant was\nprovided with 18 music pieces (6 pieces per control method) with corresponding subjective attributes.\nWe asked each participant to answer: 1) Musicality(five-point scale): How similar it sounds to the\nmusic composed by a human. 2) Controllability: Does it align with the given attributes. Then we\nreport the musicality and average attribute accuracy score in Table Table 7. The experimental results\nclearly demonstrate that Prefix Control outperforms the other two methods in terms of musicality and\ncontrollability.\nB.5\nUsage of GPT models\nRefine texts with ChatGPT\nAs shown in Figure 3, in order to make text descriptions more coherent\nand fluent, we feed concatenated templates into ChatGPT with a prompt Please combine the following\nsentences to one paragraph and then ChatGPT will give a response containing all templates within a\ncompact paragraph.\n17\nGenerate ABC notation music with GPT-4\nTo use GPT-4 as the baseline method for comparison,\nwe design the instruction to guide GPT-4 as shown in Figure 4. GPT-4 can only generate symbolic\nmusic in ABC notation, so we need to explicitly point out the format. Besides, since GPT-4 can\ngenerate various ABC notation formats, some of which cannot be processed by music21, we provide\nan ABC notation example, teaching GPT-4 to follow its format. Meanwhile, we use the prompt,\nCompose ABC notation music as the format above according to the music description as follows:\n[text descriptions] to let GPT-4 generate music according to the text description. And we finally\nconvert the ABC notations into MIDI for a fair comparison.\nC\nLimitation\nThis work is mainly about generating symbolic music from text descriptions, which does not consider\nlong sequence modeling especially. To address this, we can employ Museformer [22] as the backbone\nmodel, which proposes fine- and coarse-grained attention for handling long sequences.\nThe attribute set provided in this work represents only a subset of all music attributes. We aim to\nfurther explore additional attributes to offer a broader range of control options for music generation,\nensuring greater diversity in the creative process.\nThe possibility of regenerating music based on additional text descriptions to assist users in refining\ntheir compositions is an aspect that is worth exploring.\n18\n"
  },
  {
    "title": "Diffusion Self-Guidance for Controllable Image Generation",
    "link": "https://arxiv.org/pdf/2306.00986.pdf",
    "upvote": "1",
    "text": "Diffusion Self-Guidance for\nControllable Image Generation\nDave Epstein1,2\nAllan Jabri1\nBen Poole2\nAlexei A. Efros1\nAleksander Holynski1,2\n1UC Berkeley\n2Google Research\nAbstract\nLarge-scale generative models are capable of producing high-quality images from\ndetailed text descriptions. However, many aspects of an image are difficult or\nimpossible to convey through text. We introduce self-guidance, a method that\nprovides greater control over generated images by guiding the internal represen-\ntations of diffusion models. We demonstrate that properties such as the shape,\nlocation, and appearance of objects can be extracted from these representations and\nused to steer the sampling process. Self-guidance operates similarly to standard\nclassifier guidance, but uses signals present in the pretrained model itself, requiring\nno additional models or training. We show how a simple set of properties can be\ncomposed to perform challenging image manipulations, such as modifying the\nposition or size of specific objects, merging the appearance of objects in one image\nwith the layout of another, composing objects from multiple images into one, and\nmore. We also show that self-guidance can be used for editing real images. See our\nproject page for results and an interactive demo: https://dave.ml/selfguidance\n1\nIntroduction\nGenerative image models have improved rapidly in recent years with the adoption of large text-image\ndatasets and scalable architectures [7, 10, 12, 27, 28, 32, 34, 38]. These models are able to create\nrealistic images given a text prompt describing just about anything. However, despite the incredible\nabilities of these systems, discovering the right prompt to generate the exact image a user has in\nmind can be surprisingly challenging. A key issue is that all desired aspects of an image must be\ncommunicated through text, even those that are difficult or even impossible to convey precisely.\nTo address this limitation, previous work has introduced methods [9, 14, 18, 30] that tune pretrained\nmodels to better control details that a user has in mind. These details are often supplied in the\nform of reference images along with a new textual prompt [2, 4] or other forms of conditioning\n[1, 31, 39]. However, these approaches all either rely on fine-tuning with expensive paired data (thus\nlimiting the scope of possible edits) or must undergo a costly optimization process to perform the few\nmanipulations they are designed for. While some methods [11, 21, 22, 36] can perform zero-shot\nediting of an input image using a target caption describing the output, these methods only allow\nfor limited control, often restricted to structure-preserving appearance manipulation or uncontrolled\nimage-to-image translation.\nBy consequence, many simple edits still remain out of reach. For example, how can we move or\nresize one object in a scene without changing anything else? How can we take the appearance of\nan object in one image and copy it over to another, or combine the layout of one scene with the\nappearance of a second one? How can we generate images with certain items having precise shapes at\nspecific positions on the canvas? This degree of control has been explored in the past in smaller scale\nsettings [5, 8, 19, 24, 37, 40], but has not been convincingly demonstrated with modern large-scale\ndiffusion models [26, 32, 38].\nPreprint. Under review.\narXiv:2306.00986v3  [cs.CV]  11 Jun 2023\n\u201ca photo of a giant macaron and a croissant splashing in the Seine with the Eiffel Tower in the background\u201d\n(a) Original\n(b) Swap objects\n(c) Enlarge macaron\n(d) Replace macaron\n(e) Copy scene appearance\n(f) Copy scene layout\n\u201ca DSLR photo of a meatball and a donut falling from the clouds onto a neighborhood\u201d\n(a) Original\n(b) Move donut\n(c) Shrink donut\n(d) Replace donut\n(e) Copy scene appearance\n(f) Copy scene layout\nFigure 1: Self-guidance is a method for controllable image generation that guides sampling using the\nattention and activations of a pretrained diffusion model. With self-guidance, we can move or resize\nobjects, or even replace them with items from real images, without changing the rest of the scene\n(b-d). We can also borrow the appearance of other images or rearrange scenes into new layouts (e-f).\nWe propose self-guidance, a zero-shot approach which allows for direct control of the shape, position,\nand appearance of objects in generated images. Self-guidance leverages the rich representations\nlearned by pretrained text-to-image diffusion models \u2013 namely, intermediate activations and attention \u2013\nto steer attributes of entities and interactions between them. These constraints can be user-specified or\ntransferred from other images, and rely only on knowledge internal to the diffusion model. Through\na variety of challenging image manipulations, we demonstrate that self-guidance using only a few\nsimple properties allows for granular, disentangled manipulation of the contents of generated images\n(Figure 1). Further, we show that self-guidance can also be used to reconstruct and edit real images.\nOur key contributions are as follows:\n\u2022 We introduce self-guidance, which takes advantage of the internal representations of pretrained\ntext-to-image diffusion models to provide disentangled, zero-shot control over the generative\nprocess without requiring auxiliary models or supervision.\n\u2022 We find that properties such as the size, location, shape, and appearance of objects can be extracted\nfrom these representations and used to meaningfully guide sampling in a zero-shot manner.\n\u2022 We demonstrate that this small set of properties, when composed, allows for a wide variety of\nsurprisingly complex image manipulations, including control of relationships between objects and\nthe way modifiers bind to them.\n\u2022 Finally, by reconstructing captioned images using their layout and appearance as computed by\nself-guidance, we show that we can extend our method to editing real images.\n2\nBackground\n2.1\nDiffusion generative models\nDiffusion models learn to transform random noise into high-resolution images through a sequential\nsampling process [12, 33, 35]. This sampling process aims to reverse a fixed time-dependent\ndestructive process that corrupts data by adding noise. The learned component of a diffusion model\nis a neural network \u03f5\u03b8 that tries to estimate the denoised image, or equivalently the noise \u03f5t that was\nadded to create the noisy image zt = \u03b1tx + \u03c3t\u03f5t. This network is trained with loss:\nL(\u03b8) = Et\u223cU(1,T ),\u03f5t\u223cN (0,I)\nh\nw(t)||\u03f5t \u2212 \u03f5\u03b8(zt; t, y)||2i\n,\n(1)\n2\nwhere y is an additional conditioning signal like text, and w(t) is a function weighing the contributions\nof denoising tasks to the training objective, commonly set to 1 [12, 15]. A common choice for \u03f5\u03b8 is a\nU-Net architecture with self- and cross-attention at multiple resolutions to attend to conditioning text\nin y [28, 29, 32]. Diffusion models are score-based models, where \u03f5\u03b8 can be seen as an estimate of\nthe score function for the noisy marginal distributions: \u03f5\u03b8(zt) \u2248 \u2212\u03c3t\u2207zt log p(zt) [35].\nGiven a trained model, we can generate samples given conditioning y by starting from noise zT \u223c\nN(0, I), and then alternating between estimating the noise component and updating the noisy image:\n\u02c6\u03f5t = \u03f5\u03b8(zt; t, y),\nzt\u22121 = update(zt, \u02c6\u03f5t, t, t \u2212 1, \u03f5t\u22121),\n(2)\nwhere the update could be based on DDPM [12], DDIM [34], or another sampling method (see\nAppendix for details). Unfortunately, na\u00efvely sampling from conditional diffusion models does not\nproduce high-quality images that correspond well to the conditioning y. Instead, additional techniques\nare utilized to modify the sampling process by altering the update direction \u02c6\u03f5t.\n2.2\nGuidance\nA key capability of diffusion models is the ability to adapt outputs after training by guiding the\nsampling process. From the score-based perspective, we can think of guidance as composing score\nfunctions to sample from richer distributions or to introduce conditioning on auxiliary information [7,\n17, 35]. In practice, using guidance involves altering the update direction \u02c6\u03f5t at each iteration.\nClassifier guidance can generate conditional samples from an unconditional model by combining the\nunconditional score function for p(zt) with a classifier p(y|zt) to generate samples from p(zt|y) \u221d\np(y|zt)p(zt) [7, 35]. To use classifier guidance, one needs access to a labeled dataset and has to\nlearn a noise-dependent classifier p(y|zt) that can be differentiated with respect to the noisy image zt.\nWhile sampling, we can incorporate classifier guidance by modifying \u02c6\u03f5t:\n\u02c6\u03f5t = \u03f5\u03b8(zt; t, y) \u2212 s\u03c3t\u2207zt log p(y|zt),\n(3)\nwhere s is an additional parameter controlling the guidance strength. Classifier guidance moves the\nsampling process towards images that are more likely according to the classifier [7], achieving a\nsimilar effect to truncation in GANs [3], and can also be applied with pretrained classifiers by first\ndenoising the intermediate noisy image (though this requires additional approximations [1]).\nIn general, we can use any energy function g(zt; t, y) to guide the diffusion sampling process, not\njust the probabilities from a classifier. g could be the approximate energy from another model [17],\na similarity score from a CLIP model [23], an arbitrary time-independent energy as in universal\nguidance [1], bounding box penalties on attention [6], or any attributes of the noisy images. We can\nincorporate this additional guidance alongside classifier-free guidance [13] to obtain high-quality\ntext-to-image samples that also have low energy according to g:\n\u02c6\u03f5t = (1 + s)\u03f5\u03b8(zt; t, y) \u2212 s\u03f5\u03b8(zt; t, \u2205) + v\u03c3t\u2207ztg(zt; t, y),\n(4)\nwhere s is the classifier-free guidance strength and v is an additional guidance weight for g. As\nwith classifier guidance, we scale by \u03c3t to convert the score function to a prediction of \u03f5t. The main\ncontribution of our work is to identify energy functions g useful for controlling properties of objects\nand interactions between them.\n2.3\nWhere can we find signal for controlling diffusion?\nWhile guidance is a flexible way of controlling the sampling process, energy functions typically\nused [1, 39] require auxiliary models (adapted to be noise-dependent) as well as data annotated with\nproperties we would like to control. Can we circumvent these costs? Recent work [11, 36] has\nshown that the intermediate outputs of the diffusion U-Net encode valuable information [16, 25]\nabout the structure and content of the generated images. In particular, the self and cross-attention\nmaps\n\b\nAi,t \u2208 RHi\u00d7Wi\u00d7K\t\noften encode structural information [11] about object position and shape,\nwhile the network activations\n\b\n\u03a8i,t \u2208 RHi\u00d7Wi\u00d7Di\t\nallow for maintaining coarse appearance [36]\nwhen extracted from appropriate layers. While these editing approaches typically share attention and\nactivations naively between subsequent sampling passes, drastically limiting the scope of possible\nmanipulations, we ask: what if we tried to harness model internals in a more nuanced way?\n3\nPre-trained Di\ufb00usion Model\n\u201ca photo of a burger and an ice cream cone floating in the ocean\u201d\nconv\nattend\n\u2026\nzt\nattention\nfeats\n\u03a8t\nAt\n\u0302\u03f5t\nSamples\nSelf-Guidance\n\u2207zt\nnone\ncentroid\nshape\n\u2207ztappearance\n\u2207zt\nFigure 2: Overview: We leverage representations learned by text-image diffusion models to steer\ngeneration with self-guidance. By constraining intermediate activations \u03a8t and attention interactions\nAt, self-guidance can control properties of entities named in the prompt. For example, we can change\nthe position and shape of the burger, or copy the appearance of ice cream from a source image.\n3\nSelf-guidance\nInspired by the rich representations learned by diffusion models, we propose self-guidance, which\nplaces constraints on intermediate activations and attention maps to steer the sampling process and\ncontrol entities named in text prompts (see Fig. 2). These constraints can be user-specified or copied\nfrom existing images, and rely only on knowledge internal to the diffusion model.\nWe identify a number of properties useful for meaningfully controlling generated images, de-\nrived from the set of softmax-normalized attention matrices\n\b\nAi,t \u2208 RHi\u00d7Wi\u00d7K\t\nand activations\n\b\n\u03a8i,t \u2208 RHi\u00d7Wi\u00d7Di\t\nextracted from the standard denoising forward pass \u03f5\u03b8(zt; t, y). To control an\nobject mentioned in the text conditioning y at token indices k, we can manipulate the corresponding\nattention channel(s) Ai,t,\u00b7,\u00b7,k \u2208 RHi\u00d7Wi\u00d7|k| and activations \u03a8i,t (extracted at timestep t from a noisy\nimage zt given text conditioning y) by adding guidance terms to Eqn. 4.\nObject position.\nTo represent the position of an object (omitting attention layer index and timestep\nfor conciseness), we find the center of mass of each relevant attention channel:\ncentroid (k) =\n1\nP\nh,w Ah,w,k\n\u0014P\nh,w w \u00b7 Ah,w,k\nP\nh,w h \u00b7 Ah,w,k\n\u0015\n(5)\nWe can use this property to guide an object to an absolute target position on the image. For example,\nto move \u201cburger\u201d to position (0.3, 0.5), we can minimize \u2225(0.3, 0.5) \u2212 centroid (k) \u22251. We can\nalso perform a relative transformation, e.g., move \u201cburger\u201d to the right by (0.1, 0.0) by minimizing\n\u2225centroidorig (k) + (0.1, 0.0) \u2212 centroid (k) \u22251.\nObject size.\nTo compute an object\u2019s size, we spatially sum its corresponding attention channel:\nsize (k) =\n1\nHW\nX\nh,w\nAh,w,k\n(6)\nIn practice, we find it beneficial to differentiably threshold the attention map Athresh before computing\nits size, to eliminate the effect of background noise. We do this by taking a soft threshold at the\nmidpoint of the per-channel minimum and maximum values (see Appendix for details). As with\nposition, one can guide to an absolute size (e.g. half the canvas) or a relative one (e.g. 10% larger).\nObject shape.\nFor even more granular control than position and size, we can represent the object\u2019s\nexact shape directly through the thresholded attention map itself:\nshape(k) = Athresh\nk\n(7)\nThis shape can then be guided to match a specified binary mask (either provided by a user or extracted\nfrom the attention from another image) with \u2225target_shape \u2212 shape (k) \u22251. Note that we can\n4\n\u201cdistant shot of the tokyo tower with a massive sun in the sky\u201d\n\u201ca photo of a fluffy cat sitting on a museum bench looking at an oil painting of cheese\u201d\n\u201ca photo of a raccoon in a barrel going down a waterfall\u201d\n(a) Original\n(b) Move up (c) Move down (d) Move left (e) Move right\n(f) Shrink\n(g) Enlarge\nFigure 3: Moving and resizing objects. By only changing the properties of one object (as in Eqn. 9),\nwe can move or resize that object without modifying the rest of the image. In these examples, we\nmodify \u201cmassive sun\u201d, \u201coil painting of cheese\u201d, and \u201craccoon in a barrel\u201d, respectively.\napply any arbitrary transformation (scale, rotation, translation) to this shape before using it as a\nguidance target, which allows us to manipulate objects while maintaining their silhouette.\nObject appearance.\nConsidering thresholded attention a rough proxy for object extent, and spatial\nactivation maps as representing local appearance (since they ultimately must be decoded into an\nunnoised RGB image), we can reach a notion of object-level appearance by combining the two:\nappearance(k) =\nP\nh,w shape(k) \u2299 \u03a8\nP\nh,w shape(k)\n(8)\n4\nComposing self-guidance properties\nThe small set of properties introduced in Section 3 can be composed to perform a wide range of\nimage manipulations, including many that are intractable through text. We showcase this collection\nof manipulations and, when possible, compare to prior work that accomplishes similar effects. All\nexperiments were performed on Imagen [32], producing 1024 \u00d7 1024 samples. For more samples\nand details on the implementation of self-guidance, please see the Appendix.\nAdjusting specific properties.\nBy guiding one property to change and all others to keep their\noriginal values, we can modify single objects in isolation (Fig. 3b-3e). For a caption C = y with\nwords at indices {ci}, in which O = {oj} \u2286 C are objects, we can move an object ok at time t with:\ng = w0\nFix all other object shapes\nz\n}|\n{\n1\n|O| \u2212 1\nX\no\u0338=ok\u2208O\n1\n|A|\n|A|\nX\ni=0\n\u2225shapei,t,orig(o) \u2212 shapei,t(o)\u22251\n+ w1\nFix all appearances\nz\n}|\n{\n1\n|O|\nX\no\u2208O\n\u2225appearancet,orig(o) \u2212 appearancet(o)\u22251\n+ w2\nGuide ok\u2019s shape to translated original shape\nz\n}|\n{\n1\n|A|\n|A|\nX\ni=0\n\u2225T\n\u0010\nshapei,t,orig(ok)\n\u0011\n\u2212 shapei,t(ok)\u22251\n(9)\nWhere shapeorig and shape are extracted from the generation of the initial and edited image, respec-\ntively. Critically, T lets us define whatever transformation of the Hi \u00d7 Wi spatial attention map we\n5\n\u201ca photo of a parrot riding a horse down a city street\u201d\n\u201ca photo of a bear wearing a suit eating his birthday cake out of the fridge in a dark kitchen\u201d\n(a) Original\n(b) New appearances\n(c) ControlNet [39]\n(d) PtP [11]\nFigure 4: Sampling new appearances. By guiding object shapes (Eqn. 7) towards reconstruction of\na given image\u2019s layout (a), we can sample new appearances for a given scene (b-d).\nwant. To move an object, T translates the attention mask the desired amount. We can also resize\nobjects (Fig. 3f-3g) with Eqn. 9 by changing T to up- or down-sample shape matrices.\nConstraining per-object layout but not appearance finds new \u201cstyles\u201d for the same scene (Fig. 4):\ng = w0\nFix all object shapes\nz\n}|\n{\n1\n|O|\nX\no\u2208O\n1\n|A|\n|A|\nX\ni=0\n\u2225shapei,t,orig(o) \u2212 shapei,t(o)\u22251\n(10)\nWe can alternatively choose to guide all words, not just nouns or objects, changing summands to\nP\nc\u0338=ok\u2208C instead of P\nc\u0338=ok\u2208O. See Appendix for further discussion.\nComposition between images.\nWe can compose properties across multiple images into a cohesive\nsample, e.g. the layout of an image A with the appearance of objects in another image B (Fig. 5):\ng = w0\nCopy object shapes from A\nz\n}|\n{\n1\n|O|\nX\no\u2208O\n1\n|A|\n|A|\nX\ni=0\n\u2225shapei,t,A(o) \u2212 shapei,t(o)\u22251\n+ w1\nCopy object appearance from B\nz\n}|\n{\n1\n|O|\nX\no\u2208O\n\u2225appearancet,B(o) \u2212 appearancet(o)\u22251\n(11)\nWe can also borrow only appearances, dropping the first term to sample new arrangements for the\nsame objects, as in the last two columns of Figure 5.\nHighlighting the compositionality of self-guidance terms, we can further inherit the appearance and/or\nshape of objects from several images and combine them into one (Fig. 6). Say we have J images,\nwhere we are interested in keeping a single object okj from each one. We can collage these objects\n\u201cin-place\u201d \u2013 i.e. maintaining their shape, size, position, and appearance \u2013 straightforwardly:\ng = w0\nCopy each object\u2019s shape, position, and size\nz\n}|\n{\n1\nJ\nX\nj\n1\n|A|\n|A|\nX\ni=0\n\u2225shapei,t,j(okj) \u2212 shapei,t(ok)\u22251\n+ w1\nCopy each object\u2019s appearance\nz\n}|\n{\n1\nJ\nX\nj\n\u2225appearancet,j(okj) \u2212 appearancet(ok)\u22251\n(12)\nWe can also take only the appearances of the objects from these images and copy the layout from\nanother image, useful if object positions in the J images are not mutually compatible (Fig. 6f).\n6\n\u2190 Layout \u2192\n\u2190 Appearance \u2192\n\u201ca photo of a suitcase, a bowling ball, and a phone washed up on a beach after a shipwreck\u201d\nNo layout conditioning\nFigure 5: Mix-and-match. By guiding samples to take object shapes from one image and appearance\nfrom another (Eqn. 11), we can rearrange images into layouts from other scenes. Input images are\nalong the diagonal. We can also sample new layouts of a scene by only guiding appearance (right).\n\u201ca photo of a picnic blanket, a fruit tree, and a car by the lake\u201d\n(a) Take blanket\n(b) Take tree\n(c) Take car\n(d) Result\n(e) + Target layout\n(f) Final result\n\u201ca top-down photo of a tea kettle, a bowl of fruit, and a cup of matcha\u201d\n(a) Take matcha\n(b) Take kettle\n(c) Take fruit\n(d) Result\n(e) + Target layout\n(f) Final result\n\u201ca photo of a dog wearing a knit sweater and a baseball cap drinking a cocktail\u201d\n(a) Take sweater (b) Take cocktail\n(c) Take cap\n(d) Result*\n(e) + Target layout\n(f) Final result\nFigure 6: Compositional generation. A new scene (d) can be created by collaging objects from\nmultiple images (Eqn. 12). Alternatively \u2013 e.g. if objects cannot be combined at their original\nlocations due to incompatibilities in these images\u2019 layouts (*as in the bottom row) \u2013 we can borrow\nonly their appearance, and specify layout with a new image (e) to produce a composition (f) (Eqn. 19).\n7\npurple wizard outfit\nOurs\nDreamBooth\nchef outfit\nsuperman outfit\nfloating in milk\npouring tea\nfloating in the sea\nFigure 7: Appearance transfer from real images. By guiding the appearance of a generated object\nto match that of one in a real image (outlined) as in Eqn. 13, we can create scenes depicting an object\nfrom real life, similar to DreamBooth [30], but without any fine-tuning and only using one image.\n\u201ca photo of a hot dog, fries, and a soda on a solid background\u201d\n(a) Real image\n(b) Reconstruct\n(c) Swap w. fries\n(d) Width \u2193\n(e) Width \u2193, height \u2191\n(f) Restyle\n\u201ca photo of an eclair and a shot of espresso\u201d\n(g) Real image\n(h) Reconstruct\n(i) Move\n(j) Width \u2193\n(k) Width, height \u2191\n(l) Restyle\nFigure 8: Real image editing. Our method enables the spatial manipulation of objects (shown in\nFigure 3 for generated images) for real images as well.\nEditing with real images.\nOur approach is not limited to only images generated by a model,\nwhose internals we have access to by definition. By running T noised versions of a (captioned)\nexisting image through a denoiser \u2013 one for each forward process timestep \u2013 we extract a set of\nintermediates that can be treated as if it came from a reverse sampling process (see Appendix for\nmore details). In Fig. 8, we show that, by guiding shape and appearance for all tokens, we generate\nfaithful reconstructions of real images. More importantly, we can manipulate these real images just\nas we can generated ones, successfully controlling properties such as appearance, position, or size.\nWe can also transfer the appearance of an object of interest into new contexts (Fig. 7), from only one\nsource image, and without any fine-tuning:\ng = w0\nCopy object appearance\nz\n}|\n{\n\u2225appearancet,orig(okorig) \u2212 appearancet (ok) \u22251\n(13)\nAttributes and interactions.\nSo far we have focused only on the manipulation of objects, but\nwe can apply our method to any concept in the image, as long as it appears in the caption. We\ndemonstrate manipulation of verbs and adjectives in Fig. 9, and show an example where certain\nself-guidance constraints can help in enforcing attribute binding in the generation process.\n5\nDiscussion\nWe introduce a method for guiding the diffusion sampling process to satisfy properties derived from\nthe attention maps and activations within the denoising model itself. While we propose a number of\n8\nMove laughing to the right\n\u201ca cat and a monkey laughing on a road\u201d\n(a) Original\n(b) Modified\nChange messy location\n\u201ca photo of a messy room\u201d\n(c) At \u27e80.3, 0.6\u27e9 (d) At \u27e80.8, 0.8\u27e9\nMove red to jacket, yellow to shoes\n\u201cgreen hat, blue book, yellow shoes, red jacket\u201d\n(e) Original\n(f) Fixed\nFigure 9: Manipulating non-objects. The properties of any word in the input prompt can be\nmanipulated, not only nouns. Here, we show examples of relocating adjectives and verbs. The last\nexample shows a case in which additional self-guidance can correct improper attribute binding.\nAppearance features leak layout\n\u201ca photo of a squirrel trying\nto catch a lime mid-air\u201d\n(a) Unguided\n(b) \u201clime\u201d guided\nMulti-token layout leaks appearance\n\u201ca picture of a cake\u201d\n(c) Real image (d) Layout guided\nInteracting objects entangled\n\u201ca potato sitting on a couch with a bowl\nof popcorn watching football on TV\u201d\n(e) Original\n(f) Move potato \u2192\nFigure 10: Limitations. Setting high guidance weights for appearance terms tends to introduce\nunwanted leakage of object position (a-b). Similarly, while heavily guiding the shape of one word\nsimply matches that object\u2019s layout as expected, high guidance on the shapes of all tokens results in a\nleak of appearance information (c-d). Finally, in some cases, objects are entangled in attention space,\nmaking it difficult to control them independently (e-f).\nsuch properties, many more certainly exist, as do alternative formulations of those presented in this\npaper. Among the proposed collection of properties, a few limitations stand out.\nThe reliance on cross-attention maps imposes restrictions by construction, precluding control over any\nobject that is not described in the conditioning text prompt and hindering fully disentangled control\nbetween interacting objects due to correlations in attention maps (Fig. 10e-10f). Selectively applying\nattention guidance at certain layers or timesteps may result in more effective disentanglement.\nOur experiments also show that appearance features often contain undesirable information about\nspatial layout (Fig. 10a-10b), perhaps since the model has access to positional information in its\narchitecture. The reverse is also sometimes true: guiding the shape of multiple tokens occasionally\nbetrays the appearance of an object (Fig. 10c-10d), implying that hidden high-frequency patterns\narising from interaction between attention channels may be used to encode appearance. These\nfindings suggest that our method could serve as a window into the inner workings of diffusion models\nand provide valuable experimental evidence to inform future research.\nBroader impact\nThe use-cases showcased in this paper, while transformative for creative uses, carry the risk of\nproducing harmful content that can negatively impact society. In particular, self-guidance allows for a\nlevel of control over the generation process that might enable potentially harmful image manipulations,\nsuch as pulling in the appearance or layout from real images into arbitrary generated content (e.g., as\nin Fig. 7). One such dangerous manipulation might be the injection of a public figure into an image\ncontaining illicit activity. In our experiments, we mitigate this risk by deliberately refraining from\ngenerating images containing humans. Additional safeguards against these risks include methods for\nembedded watermarking [20] and automated systems for safe filtering of generated imagery.\nAcknowledgements\nWe thank Oliver Wang, Jason Baldridge, Lucy Chai, and Minyoung Huh for their helpful comments.\nDave is supported by the PD Soros Fellowship. Dave and Allan conducted part of this research at\nGoogle, with additional funding provided by DARPA MCS and ONR MURI.\n9\nReferences\n[1] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum,\nJonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. arXiv preprint\narXiv:2302.07121, 2023.\n[2] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live:\nText-driven layered image and video editing. In Computer Vision\u2013ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XV, pages 707\u2013723.\nSpringer, 2022.\n[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity\nnatural image synthesis. ArXiv, abs/1809.11096, 2018.\n[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow\nimage editing instructions. arXiv preprint arXiv:2211.09800, 2022.\n[5] Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage\ncompositionality in gans. arXiv preprint arXiv:2103.10426, 2021.\n[6] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention\nguidance. arXiv preprint arXiv:2304.03373, 2023.\n[7] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. ArXiv,\nabs/2105.05233, 2021.\n[8] Dave Epstein, Taesung Park, Richard Zhang, Eli Shechtman, and Alexei A Efros. Blobgan:\nSpatially disentangled scene representations. In Computer Vision\u2013ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XV, pages 616\u2013635.\nSpringer, 2022.\n[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and\nDaniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using\ntextual inversion. arXiv preprint arXiv:2208.01618, 2022.\n[10] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models\nas plug-and-play priors. arXiv:2206.09012, 2022.\n[11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,\n2022.\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS,\n2020.\n[13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv:2207.12598, 2022.\n[14] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri,\nand Michal Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint\narXiv:2210.09276, 2022.\n[15] Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models.\nNeurIPS, 2021.\n[16] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic\nlatent space. arXiv preprint arXiv:2210.10960, 2022.\n[17] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B. Tenenbaum. Compositional\nvisual generation with composable diffusion models. In European Conference on Computer\nVision, 2022.\n[18] Vivian Liu and Lydia B Chilton. Design guidelines for prompt engineering text-to-image\ngenerative models. In Proceedings of the 2022 CHI Conference on Human Factors in Computing\nSystems, pages 1\u201323, 2022.\n10\n[19] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard\nSch\u00f6lkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning\nof disentangled representations. In international conference on machine learning, pages 4114\u2013\n4124. PMLR, 2019.\n[20] Xiyang Luo, Michael Goebel, Elnaz Barshan, and Feng Yang. Leca: A learned approach for\nefficient cover-agnostic watermarking. arXiv preprint arXiv:2206.10813, 2022.\n[21] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano\nErmon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In\nInternational Conference on Learning Representations, 2021.\n[22] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion\nfor editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022.\n[23] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. In ICML, 2022.\n[24] Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei Efros, and\nRichard Zhang. Swapping autoencoder for deep image manipulation. Advances in Neural\nInformation Processing Systems, 33:7198\u20137211, 2020.\n[25] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn.\nDiffusion autoencoders: Toward a meaningful and decodable representation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10619\u201310629,\n2022.\n[26] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents, 2022.\n[27] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. ICML, 2021.\n[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n[29] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for\nbiomedical image segmentation. ArXiv, abs/1505.04597, 2015.\n[30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\npreprint arXiv:2208.12242, 2022.\n[31] Chitwan Saharia, William Chan, Huiwen Chang, Chris A Lee, Jonathan Ho, Tim Salimans,\nDavid J Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. SIGGRAPH,\n2022.\n[32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim\nSalimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image\ndiffusion models with deep language understanding. arXiv:2205.11487, 2022.\n[33] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. ICML, 2015.\n[34] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. CoRR,\nabs/2010.02502, 2020.\n[35] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\nBen Poole. Score-based generative modeling through stochastic differential equations. ICLR,\n2021.\n11\n[36] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features\nfor text-driven image-to-image translation. arXiv preprint arXiv:2211.12572, 2022.\n[37] Hong-Xing Yu, Leonidas J Guibas, and Jiajun Wu. Unsupervised discovery of object radiance\nfields. arXiv preprint arXiv:2107.07905, 2021.\n[38] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,\nZarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive\nmodels for content-rich text-to-image generation. arXiv:2206.10789, 2022.\n[39] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.05543, 2023.\n[40] Jiapeng Zhu, Yujun Shen, Yinghao Xu, Deli Zhao, and Qifeng Chen. Region-based semantic\nfactorization in gans. arXiv preprint arXiv:2202.09649, 2022.\n12\nAppendix\nA.\nImplementation details\nWe apply our self-guidance term following best practices for classifier-free guidance on Imagen [32].\nSpecifically, where N is the number of DDPM steps, we take the first 3N\n16 steps with self-guidance\nand the last N\n32 without. The remaining 25N\n32 steps are alternated between using self-guidance and\nnot using it. We use N = 1024 steps. Our method works with 256 and 512 steps as well, though\nself-guidance weights occasionally require adjustment. We set v = 7500 in Eqn. 4 as an overall scale\nfor gradients of the functions g defined below \u2014 we find that the magnitude of per-pixel gradients is\nquite small (often in the range of 10\u22127 to 10\u22126, so such a high weight is needed to induce changes.\nWe apply centroid, size, and shape terms on all cross-attention interactions in the model we use.\nIn total, there are 36 of these, across the encoder, bottleneck, and decoder, at 8 \u00d7 8, 16 \u00d7 16, and\n32 \u00d7 32 resolutions. We apply the appearance term using the activations of the penultimate layer\nin the decoder (two layers before the prediction readout) and the final cross-attention operation.We\nexperimented with features from other parts of the U-Net denoiser, namely early in the encoder before\npositional information can propagate through the image (to prevent appearance-layout entanglement),\nbut found these to work significantly worse. To avoid degenerate solutions, we apply a stop-gradient\nto the attention in the appearance term so only information about activations is back-propagated.\nWe take the mean spatially of all shape terms and across activation dimensions for all appearance\nterms, which we omit in all equations for conciseness.\nAttention mask binarization.\nIn practice, it is beneficial to differentiably binarize the attention\nmap (with sharpness controlled by s) before computing its size or utilizing its shape, to eliminate\nthe effect of background noise (this is empirically less important when guiding centroids, so we do\nnot binarize in that case). We do this by taking a soft threshold at the midpoint of the per-channel\nminimum and maximum values. More specifically, we apply a shifted sigmoid on the attention\nnormalized to have minimum 0 and maximum 1, followed by another such normalization to ensure\nthe high value is 1 and the low 0 after applying the sigmoid. We use s = 10 and redefine Eqn. 6.\nnormalize(X) =\nX \u2212 minh,w (X)\nmaxh,w (X) \u2212 minh,w (X)\n(14)\nAthresh = normalize (sigmoid (s \u00b7 (normalize(A) \u2212 0.5)))\n(15)\nsize (k) =\n1\nHW\nX\nh,w\nAthresh\nh,w,k\n(16)\n\u201ca photo of a carrot and an onion in a hot tub outdoors\u201d\n\u201ca photo of an oak tree and a pineapple outside an arctic igloo\u201d\n\u201ca photo of an owl and a pig running at the racetrack\u201d\n(a) Original\n(b) Edited\nFigure 11: Moving objects. Non-cherry-picked results for moving objects in scenes using Eqn. 9. We\nmove onion down and to the right, pineapple to the right, and owl up and pig down, respectively.\nAll scenes use weights w0 = 1.5, w1 = 0.25, and w2 = 2.\n13\n\u201ca photo of a kangaroo and a punching bag at the gym\u201d\n\u201ca photo of a chicken walking across the street with an Italian sports car waiting for it\u201d\n\u201ca photo of a boombox on a camel near a pond\u201d\n(a) Original\n(b) Edited\nFigure 12: Resizing objects. Non-cherry-picked results for resizing objects in scenes using Eqn. 9,\nwith T specified to up- or down-sample attention maps. We reduce the punching bag\u2019s height 0.5\u00d7\nand enlarge chicken 2.5\u00d7 and boombox 2\u00d7. All scenes use w0 = 2, w1 = 0.25, and w2 = 3.\nB.\nUsing self-guidance\nMaximizing consistency.\nIn general, we find that sharing the same sequence of noise in the\nDDPM process between an image and its edited version is not necessary to maintain high levels of\nconsistency, but can help if extreme precision is desired. We find that maintaining object silhouettes\nunder transformations such as resizing and repositioning is more effective if applying a transformation\nT to the original shape, rather than expressing the same change through centroid and size.\nGuiding \u201cbackground\u201d words.\nTo keep all objects of the scene fixed but one (Fig. 3), one can\neither guide all other tokens in the prompt (including \u201ca photo of\u201d and other abstract terms) to keep\ntheir shape, or only select the other salient objects and hold those fixed. In general, since abstract\nwords are often used for message passing and have attention patterns that are correlated with the\nlayout of the scene, we prefer not to guide their layouts to maximize compositionality.\nMitigating appearance-layout entanglement. When words or concepts span multiple tokens, we\ncan mean-pooling attention maps across these tokens before processing them, though do not find this\nto improve results. We also find that corrupting target shapes with Gaussian noise helps mitigate this\neffect, providing some evidence for this hypothesis.\nMoving objects.\nWe use w0 \u2208 [0.5, 2], w1 \u2208 [0.03, 0.3], w2 \u2208 [0.5, 5] in Eqn. 9. Alternatively, we\ncan express ok\u2019s new location through its centroid, adding a term to keep size fixed:\ng = w0\nFix all other object shapes\nz\n}|\n{\n1\n|O| \u2212 1\nX\no\u0338=ok\n1\n|A|\n|A|\nX\ni=0\n\u2225shapei,t,orig(o) \u2212 shapei,t(o)\u22251\n+ w1\nFix all object appearances\nz\n}|\n{\n1\n|O|\nX\no\u2208O\n\u2225appearancet,orig(o) \u2212 appearancet(o)\u22251\n+ w2\nFix ok\u2019s size\nz\n}|\n{\n1\n|A|\n|A|\nX\ni=0\n\u2225sizei,t,orig(ok) \u2212 sizei,t(ok)\u22251\n+ w3\n1\n|A|\n|A|\nX\ni=0\n\u2225target_centroid \u2212 centroidi,t(ok)\u22251\n|\n{z\n}\nChange ok\u2019s position\n(17)\n14\n\u201ca photo of a koala picking flowers next to a mansion\u201d\n\u201ca photo of a capybara wearing a robe sitting by the fireplace\u201d\n\u201ca photo of a bird drinking coffee at a 1950s style diner\u201d\n(a) Original\n(b) Edited\nFigure 13: Creating new appearances for scenes. Non-cherry-picked results sampling different\n\u201cstyles\u201d of appearances given the same layout, using Eqn. 10. We use w0 = 0.7, 0.3, and 0.3\nrespectively for each result, to preserve greater structure in the background of the first picture.\nWhere target_centroid can be computed as a shfited version of the timestep-and-attention-\nspecific centroidorig if desired, or selected to be an absolute value on the canvas (repeated across all\ntimesteps). We generally use weights w0 \u2208 [0.5, 2], w1 \u2208 [0.03, 0.3], w2 \u2208 [0.5, 2], w3 \u2208 [1, 3].\nResizing objects.\nWe can follow Eqn. 9 to resize objects as well, by setting T to upsample or\ndownsample the original mask. We can similarly use Eqn. 17, omitting the final term and setting\nthe target size to a desired value, either computed as a function of sizeorig(ok) or provided as an\nabsolute proportion of pixels on the canvas that the object should cover. We use the same weight\nrange for all weights except we set w2 \u2208 [1, 3], w3 = 0 for Eqn. 17.\nSampling new appearances.\nWe set w0 \u2208 [0.1, 1] in Eqn. 10. Generally, higher values lead to\nextremely precise layout preservation at the expense of diversity in appearance.\nSampling new layouts.\nJust as we can find new appearances for a scene of a given layout, we can\nperform the opposite operation, finding new layouts for scenes where objects have a given appearance:\ng = w0\nFix all appearances\nz\n}|\n{\n1\n|O|\nX\no\u2208O\n\u2225appearancet,orig(o) \u2212 appearancet(o)\u22251\n(18)\nWe almost always use w0 \u2208 [0.05, 0.25].\nCollaging objects in-place.\nEqn. 12 can be easily generalized to more than one object per image\n(adding another sum across all objects) or to the case where prompts vary between images (mapping\nfrom kj to the corresponding indices in the new image). We set w0 \u2208 [0.5, 1], w1 \u2208 [0.05, 0.3].\nCollaging objects with a new layout.\nAs shown in Fig. 6f, we can also collage objects into a new\nlayout specified by a target image J + 1, in addition to the J images specifying object appearance:\ng = w0\nCopy all object shapes\nz\n}|\n{\n1\n|A|\n|A|\nX\ni=0\n\u2225shapei,t,J+1(okJ+1) \u2212 shapei,t(ok)\u22251\n+ w1\nCopy each object\u2019s appearance\nz\n}|\n{\n1\nJ\nX\nj\n\u2225appearancet,j(okj) \u2212 appearancet(ok)\u22251\n(19)\nAs in Eqn. 12, we set w0 \u2208 [0.5, 1], w1 \u2208 [0.05, 0.3].\n15\n\u201ca photo of a rabbit with a birthday balloon and a party hat\u201d\n\u201ca photo of cleats, a bright soccer ball, and a cone\u201d\n\u201ca calculator, a toy car, and a pillow on a rug\u201d\n(a) Original\n(b) Edited\nFigure 14: Creating new layouts for scenes. Non-cherry-picked results sampling new layouts for\nthe same scenes, using Eqn. 18. We use w0 = 0.07, 0.07, and 0.2 respectively.\n\u201ca DSLR photo of a backpack at the grand canyon\u201d\n\u201ca DSLR photo of a backpack wet in the water\u201d\n\u201ca photo of a pair of sunglasses being worn by a bear\u201d\n\u201ca photo of a pair of sunglasses on a pile of snow\u201d\n(a) Original\n(b) Object in new contexts\nFigure 15: Appearance transfer from real images. Non-cherry picked results sampling new images\nwith a given object\u2019s appearance specified by a real images, as in Eqn. 13. We use w0 = 0.15.\nTransferring object appearances to new layouts.\nNothing requires the indices (or in fact, the\nobjects those indices refer to) to be the same in the image being generated and the original image\nbeing used as a source, as long as there is a mapping specified between the indices in the old and\nnew images which should correspond. Call this mapping m. We can then take the appearance of an\nobject ok in a source image and transfer it to an image with any new prompt as follows, as specified\nin Eqn. 13 (with typical weights w0 \u2208 [0.01, 0.1]):\ng = w0\nCopy object appearance\nz\n}|\n{\n\u2225appearancet,orig(ok) \u2212 appearancet (m(ok)) \u22251\n(20)\nMerging layout and appearance.\nWe use w0 \u2208 [1, 2] and w1 \u2208 [0.1, 0.3] in Eqn. 11.\nEditing with real images.\nImportantly, our method is not limited to editing generated images to\nwhose internals it has access by definition. We find that we can also meaningfully guide generation\n16\n\u201ca photo of a chow chow wearing a superman outfit\u201d\n\u201ca dslr photo of a teapot floating in the sea\u201d\n(a) Original\n(b) Ours\n(b) Random samples without self-guidance\nFigure 16: Ablating appearance transfer from real images. To verify the efficacy of our approach,\nwe compare our results from Fig. 7 in the paper to random samples from the same prompt without\napperance transfer. We can see that appearance of objects varies significantly without self-guidance.\nusing the attention and activations extracted from a set of forward-process denoisings of a real image\n(given a caption) to \u201capproximate\u201d the reverse process, despite any mismatch in distributions one\nmight imagine. Concretely, we generate T corrupted versions of a real image x, {\u03b1tx + \u03c3t\u03f5t}T\n1 ,\nwhere \u03f5t \u223c N(0, 1). We then extract the attention At and activations \u03a8t from the denoising network\nat each of these timesteps in parallel and concatenate them into a length-T sequence. We treat this\nsequence identically to a sequence of T internals given by subsequent sampling steps, and can thus\ntransfer the appearance of objects from real images, output images that look like real images with\nmoved or resized objects, and so on.\nIn Fig. 7, the prompts we use to transfer appearance are \u201cA photo of a Chow Chow...\u201d and \u201cA DSLR\nphoto of a teapot...\u201d. While our method works on less specific descriptions as well, it is not as reliable\nwhen object appearance is more out-of-distribution. For context, we show unguided samples under\nthe prompts from Fig. 7 in Fig. 16, which still deviate significantly from the desired appearance,\nshowing the efficacy of our approach. A weakness of our simple approach is that it has no constraints\non the shape of the generated objects, which we leave to future work.\nWeight selection heuristics.\nWe find weights that work well to remain more or less consistent\nacross different images given an edit, but ideal weights do vary somewhat (within predictable ranges)\nbetween different combinations of terms. Our heuristics for weight selection per term are: the more\nweights there are, the higher per-term weights can be without causing artifacts (and indeed, need to\nbe, to provide ample contribution to the final result); appearance terms should have weights 1 or 2\norders of magnitude lower than layout terms; layout summary statistics (centroid and size) should\nhave slightly lower weights than terms on the per-pixel shape; total weight of terms should not add\nup to more than \u223c 5 to avoid artifacts.\nC.\nAdditional results\nWe show further non-cherry-picked results for the edits we show in the main paper. Our general\nprotocol consists of selecting an interesting prompt manually, verifying that our model creates\ncompelling samples aligning with this prompt without self-guidance, beginning with the typical\nweights we use for an edit, and trying around 3-5 other weight configurations to find the one that\nworks best for the prompt \u2013 in most cases, this is the starting set of weights. Then, we use the\nfirst 8 images we generate, without further filtering. We generate all results with different seeds to\nshowcase the strength of guidance even without shared DDPM noise. We show more results for\nmoving (Fig. 11) and resizing (Fig. 12) objects, sampling new appearances for given layouts (Fig. 13)\nas well as new layouts for a given set of objects (Fig. 14), transferring the appearance of real objects\ninto new contexts (Fig. 15),\n17\n"
  },
  {
    "title": "Cocktail: Mixing Multi-Modality Controls for Text-Conditional Image Generation",
    "link": "https://arxiv.org/pdf/2306.00964.pdf",
    "upvote": "1",
    "text": "Cocktail\n: Mixing Multi-Modality Controls for\nText-Conditional Image Generation\nMinghui Hu\u2020, Jianbin Zheng\u22c6,\u2021, Daqing Liu\u2021, Chuanxia Zheng\u00a7, Chaoyue Wang\u00b6, Dacheng Tao\u00b6,\nand Tat-Jen Cham\u2020\n\u2020Nanyang Technological University, \u22c6South China University of Technology, \u00a7University of Oxford,\n\u00b6The University of Sydney, \u2021JD Explore Academy\nAbstract\nText-conditional diffusion models are able to generate high-fidelity images with\ndiverse contents. However, linguistic representations frequently exhibit ambiguous\ndescriptions of the envisioned objective imagery, requiring the incorporation of\nadditional control signals to bolster the efficacy of text-guided diffusion models.\nIn this work, we propose Cocktail, a pipeline to mix various modalities into\none embedding, amalgamated with a generalized ControlNet (gControlNet), a\ncontrollable normalisation (ControlNorm), and a spatial guidance sampling method,\nto actualize multi-modal and spatially-refined control for text-conditional diffusion\nmodels. Specifically, we introduce a hyper-network gControlNet, dedicated to the\nalignment and infusion of the control signals from disparate modalities into the\npre-trained diffusion model. gControlNet is capable of accepting flexible modality\nsignals, encompassing the simultaneous reception of any combination of modality\nsignals, or the supplementary fusion of multiple modality signals. The control\nsignals are then fused and injected into the backbone model according to our\nproposed ControlNorm. Furthermore, our advanced spatial guidance sampling\nmethodology proficiently incorporates the control signal into the designated region,\nthereby circumventing the manifestation of undesired objects within the generated\nimage. We demonstrate the results of our method in controlling various modalities,\nproving high-quality synthesis and fidelity to multiple external signals. The codes\nwill be released at https://mhh0318.github.io/cocktail/.\n1\nIntroduction\nText-conditional diffusion models [40, 38, 41, 33] have actualized the capacity for high-quality\ngenerative capabilities. These models facilitate the generation of an array of high-calibre images\nthrough the utilization of concise textual prompts. However, linguistic representations pose inherent\nchallenges in accurately encapsulating the precise imagery anticipated by the user, owing to the\npotential ambiguities and subjective interpretations of verbal descriptions within the context of visual\nsynthesis. Moreover, minor alterations to the textual prompts also yield distinct visual outputs,\nunderscoring the absence of refined control over the generative process.\nModifying the prior is a series of existing solutions for multi-modal control, e.g., the control of the\nentire prior space [38, 47, 22, 21, 56] as demonstrated in Fig. 1(a). These approaches centered on the\nwhole prior lack the capacity for localised image modifications and the preservation of background\nelements. Moreover, these models typically require training from scratch, which demands a substantial\namount of resources.\nPreprint. Under review.\narXiv:2306.00964v1  [cs.CV]  1 Jun 2023\nModality #1\nModality \n#2\nModality #3\nModality\n... ...\nText-conditional \nDiffusion\n(c) Cocktail\nModality #1\nModality #5\nModality #2\nModality #3\nModality\n... ...\nText-conditional \nDiffusion\n(b) ControlNet / T2I Adapter\n(a) Prior Modification\nDiffusion\nDecoder \nModalities \nPrior\nModality #1\nModality #2\nModality #3\nModality\n... ...\nFigure 1: Comparison of various control methods. Our approach requires only one generalized\nmodel, unlike previous that needed multiple models for multiple modalities.\nIn response to the methods dealing with latent representation [28, 4, 26], an additional lightweight\nhyper-network is introduced in [52, 32], which is designed to encode external control signals into\nlatent vectors and subsequently inject them directly into the backbone network. Such methods\neffectively handle control over the single additional modality; however, they exhibit limitations when\nconfronted with multiple modalities, as shown in Fig 1(b). One issue is that each modality requires a\nunique network, leading to a computational overhead that escalates proportionally with the increase\nin the number of modalities. Furthermore, the impact of additive coefficients between different modes\non the final imaging outcomes warrants consideration. The inherent imbalance among superimposed\nmodes makes these additive coefficients a pivotal factor in determining the ultimate synthesized\noutput. An additional challenge emerges during the sampling process when the model conducts an\ninitial inference devoid of control signal injection. This preliminary inference step could result in\nobject placement that potentially contradicts the control signals.\nIn this paper, we propose a novel pipeline, as shown in Fig. 1(c), termed Cocktail, which accomplishes\nmulti-modality control through a text-conditional diffusion model. It encompasses three main\ncomponents: 1) a hyper-network capable of accommodating multi-modal input, 2) a conditional\nnormalisation method to mix the control features, and 3) a sampling strategy designed to facilitate\nprecise control over the generative process. As shown in Fig. 2, our method is proficient in generating\nimages that meet all input conditions or any arbitrary subset thereof, utilizing only one single model.\nTo achieve this goal, we initially trained a branched network named gControlNet, which accepts\nmultiple modalities. Upon training, the branched network can simultaneously accept arbitrary\ncombinations of existing modalities. When multiple modalities coexist within the same region, the\nmodel is capable of automatically fusing input from different modalities, balancing the disparities\nbetween them. To leverage the features of the gControlNet, we further proposed controllable\nnormalisation (ControlNorm). We can achieve better representation of control signals in terms of\nsemantic and spatial aspects according to the decoupling provided by ControlNorm.\nMoreover, we introduce a spatial guidance sampling method in order to facilitate spatial generation\nunder the purview of multi-modal control signals. Specifically, we employ distinct textual prompts to\ndifferentiate entities from the background, incorporating entities into the background via a prompt\nediting approach. Our devised sampling approach demonstrates efficacy in circumventing the\ngeneration of undesired entities.\nIn summary, our main contributions are as follows:\n\u2022 We introduced the Generalized ControlNet (gControlNet), a branched network capable of\nadaptively integrating multi-modal information, effectively addressing issues stemming\nfrom imbalances between modalities;\n\u2022 We proposed the Controllable Normalistion (ControlNorm) to optimize the utilization of\ninformation within branched networks to yield more effective outcomes;\n\u2022 We introduced a spatial guidance sampling method based on the operation within the\nattention map to generate relevant information tailored to regional contexts, preventing the\ninclusion of undesired objects outside the specified regions.\n2\nFigure 2: Examples of our model with the same prompt. Given a text prompt along with various\nmodality signals, our approach is able to synthesize images that satisfy all input conditions or any\narbitrary subset of these conditions using a single model. The prompt is: A girl holding a cat.\n2\nRelated Work\nText-conditional Diffusion Models.\nDiffusion models [19, 45] has achieved great success in the\narea of text-to-image synthesis [33, 38, 40, 3, 14]. To reduce the computational cost, diffusion\nmodels typically function within the latent space [40] or produce low-resolution images that are later\nimproved through super-resolution models [38, 3]. Fast sampling methods have also successfully\nreduced the number of generation steps required by diffusion models from hundreds down to merely\na few [44, 31, 24, 29, 11]. The provision of classifier guidance during the sampling process can\nalso significantly impact the outcomes, leading to substantial improvements in the results [9]. In\naddition to the widely used classifier-free guidance [18], other types of guidance are also worth\nexploring [54, 13].\nHyperNetworks for Pre-trained Models.\nTraining a diffusion model is highly resource-intensive\nand environmentally unfriendly [40]. Fine-tuning such a model can also be challenging due to the\nvast number of parameters involved [1]. Therefore, introducing an additional tiny branched network\nto bias the output of the original network is a more reasonable choice [10, 2, 8]. Similar ideas have\nalso proven to be effective in diffusion models: Hypernet [15] and LoRA [20] models are capable of\naltering the original diffusion model\u2019s sampling distribution by training a small branched network,\nwhich has become the most popular branched network in the current research community. More\nsimilar to our work is ControlNet [52], which is subsequently combined with different layers in the\ndenoising U-Net to provide support for various task-specific guidance. It presents impressive results\nwith various conditional inputs, yet with only one modality for each model. In contrast, our cocktail\nendows the ControlNet with multitasking capabilities using only one single model.\nConditional Normalisation\napproaches have been employed across a range of vision tasks, includ-\ning style transfer [23, 12], conditional generation [35, 7, 14, 55] and image-to-image translation [39].\nThese techniques involve normalizing layer activations to zero mean and unit deviation, followed by\ndenormalisation through an affine transformation derived from external data. External data can be in\nmultiple formats, such as style images, semantic masks, or category labels.\n3\nFigure 3: The network architecture of Generalized ControlNet (gControlNet) with Controllable\nNormalisation (ControlNorm). The parameters indicated by the yellow sections are sourced from the\npre-trained model and stay constant, while only those in the blue sections are updated during training,\nwith the gradient back-propagated along the blue arrows.\nAttention Map-based Prompt Tuning.\nPrompt-to-Prompt [16] is a method that adjusts local or\nglobal specifics in text-guided diffusion models by altering the cross-attention maps from source\nto target image, thereby maintaining spatial layout and geometry. Recently, numerous efforts have\nbeen made to improve the outcomes [34, 36]. However, such approaches are limited to synthesized\nimages without an inversion technique. Unlike cross-attention, self-attention focuses on inter-pixel\nrelationships within the same domain [46]. Paint-with-words [3] is another method that allows users\nto specify the spatial locations of objects by selecting phrases from the text prompt.\n3\nMethods\nIn this work, our main goal is to design a controllable generator that utilizes various modalities of\ninput, within a single model. To achieve this, we first propose a more general branched network,\ngControlNet, which can generate control signals from different modalities using a single network\nand adaptively weighted fuse them together. Furthermore, we propose ControlNorm to inject the\nsignals from the gControlNet into the diffusion backbone, which solves the imbalanced problem\nwithin various modalities. Finally, we propose a spatial guidance sampling method to avoid the\npresence of extraneous objects in the generation process. By modifying the attention map, this\napproach effectively incorporates control signals into the backbone network. The whole pipeline is\ndemonstrated in Fig. 3.\n3.1\nGeneralized ControlNet with Controllable Normalisation\nGeneralized ControlNet.\nControlNet [52] is a method designed to influence and control the\nbehavior of neural networks by adjusting the input conditions of specific network blocks. Instead of\ndirectly modifying the parameters of the primary network, ControlNet employs an auxiliary network\nto generate feature offsets. These offsets are then combined with different layers in the main network,\nsuch as a denoising U-Net, to support various task-specific guidance.\nGiven a trained backbone network block F(\u00b7; \u03b8) with parameter \u03b8, the input feature x can be mapped\nto y. For the branched part, we duplicate the parameter \u03b8 to create a trainable copy \u03b8t, which is\n4\nthen trained using the supplementary modality. Preserving the original weights helps retain the\ninformation stored in the initial model after training on large-scale datasets, which ensures that the\nquality and diversity of the generated images do not degrade. Mathematically, the output from the\ntrained network block can be expressed as:\ny = F(x; \u03b8) + Z (F(x + Z(cm); \u03b8t)) \u2190 F(x; \u03b8),\n(1)\nwhere the control signal of a single modality cm is typically processed to obtain a format identity for\nx, for example, through a zero-initialized convolutional layer, and then added to x. Z(\u00b7) represents\nthe zero-initialized layer. It is not only used in the process of handling cm, but also serves to adjust\nthe output of the branched network F(\u00b7, \u03b8t).\nTo accomplish the goals of accepting multiple external modalities as input and balancing signals from\ndifferent modalities, we have devised a modified framework that adeptly merges these varied sources\nof information. At the top of our network, we adopt a simple downsampling network M(\u00b7) to convert\nexternal conditional signals to the latent space, allowing the conditional signals to be directly injected\ninto the latent space. It is worth noting that M(\u00b7) is versatile and can adapt to different types of\nexternal signals. Given k different modalities, the converted conditional features are ck\nm = M(Ck).\nControllable Normalisation.\nInstead of directly passing the sum of conditional features via a\nzero-initialized layer to the network block F(\u00b7; \u03b8t), i.e., \u02c6cm = Z(P\ni ci), we introduce a controllable\nnormalisation (ControlNorm) method, which has an additional layer to generate two sets of learnable\nparameters, \u03b3(\u02c6cm) and \u03b2(\u02c6cm), conditioned on all k modalities. These two sets of parameters are\nused in the conditional normalisation layer to fuse the external conditional signals and the original\nsignals. Specifically, the input to the trainable block F(\u00b7; \u03b8t) becomes:\n(I + Z(\u03b3 (\u02c6cm))) \u2299 x \u2212 \u00b5c(x)\n\u03c3c(x)\n\u2295 Z(\u03b2(\u02c6cm)) \u2190 x + Z(cm),\n(2)\nwhere \u00b5c(x) and \u03c3c(x) are the mean and standard deviation of the feature x along the channel c, \u2299\nis Hadamard product, and \u03b3(\u02c6c) and \u03b2(\u02c6c) are vectors that have the same dimension as x. With the\nhelp of zero-convolution, we can preserve the identity of x just before the start of the fine-tuning. It\nis worth noting that in the following layers, the internal feature in the branched network h can be\nintegrated with the latent feature z from the original network in the same way:\n(I + Z(\u03b3 (h))) \u2299 z \u2212 \u00b5c(z)\n\u03c3c(z)\n\u2295 Z(\u03b2(h)) \u2190 x + Z(cm),\n(3)\nwhere h is the intermediate features from the original network and z is the intermediate features from\nthe branched network. Specifically, we will have five sets of intermediate features from the Stable\nDiffusion [40] U-Net backbone and our generalized ControlNet, including four sets of features from\nencoder blocks and one from the middle block.\nIn fact, our controllable normalisation is a generalized version of conditional normalisation [35, 23].\nAfter changing the mean and variance calculation dimension and replacing the external signal \u02c6c by a\nmask image, real image, or class labels, we can derive the various forms of SPADE [35], AdaIN [23],\nCIN [12] and MoVQ [55]. More interestingly, our controllable normalisation method not only\nenables the use of external signals as conditions, but also allows intermediate-layer signals to act as\nconstraints.\nAs shown in Fig. 3, only the parameters of the gControlNet require updating. Given an initial latent\nz0 and a time-step t, the diffusion process progressively introduces noise \u03f5 to this latent, transforming\nthe original latent into a noisy state zt. The objective of the diffusion model is to estimate the noise\n\u03f5 adding to the x. Our proposed gControlNet shares the same objective function as the diffusion\nmodel, aiming to predict the noise added at time t. The only distinction lies in the incorporation of\nmultimodal information as conditional signals \u02c6cm =\n\u0002\nc0\nm, c1\nm, . . . , ck\nm\n\u0003\n:\nL = Ez0,t,cp,\u02c6cm,\u03f5\u223cN (0,1)\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8 (zt, t, cp, \u02c6cm) \u22252\n2\n\u0003\n(4)\n3.2\nSpatial Guidance Sampling\nIn order to leverage the control signals from generalized ControlNet and ensure that the generated\nobjects appear within the areas of interest, we proposed a spatial guidance sampling method. Here,\nwe mainly focus on editing the cross-attention layers of the U-Net in Stable Diffusion.\n5\nGiven a noisy latent z at timestep t as the input to the pretrained U-Net with n blocks F(n)(\u00b7; \u03b8(n))\nat different resolution, the latent vector z will be down-sampled to various dimensional z(n) that\nmap to the corresponding block F(n). The cross-attention maps A(n) \u2208 R(Ni,Nt) in each block,\nparameterized by \u03b8(n), are associated with the linear projection of latent feature Q(n) = fQ(\u03d5(z(n)))\nand the prompt K = fK(ctext):\nA(n)\nij|\u03b8(n) =\nexp\u27e8Q(n)\ni\n, Kj\u27e9\nP\nk=1 exp\u27e8Q(n)\ni\n, Kk\u27e9\n,\n(5)\nwhere A(n)\nij\nrepresents the attentional strength between the j-th prompt token (among Nt tokens)\nand the i-th latent feature (among Ni features). Intuitively, for each prompt token Kj, there exists a\ncorresponding latent feature map A(n)\nj\nwith spatial information. Moreover, the corresponding feature\nmap will contain spatial and shape information for the associated object. However, it is not only\nthe object-describing tokens that contain information about the object\u2019s location and shape, some\nconnecting words and padding tokens also convey spatial information for the overall scene [6, 42].\nWe apply a masking strategy to the corresponding attention maps. In detail, we construct two sets of\nattention masks M pos(n) and M neg(n) \u2208 R(Ni,Nt). Each column M pos(n)\nj\nand M neg(n)\nj\nis a flattened\nalpha mask, which is determined by the visibility of the corresponding text token Kj. The values\nof M pos(n)\nij\nand M neg(n)\nij\nare determined based on the relationship between image token Qi and text\ntoken Kj. On the one hand, if image token Qi corresponds to a region of the image that should be\ninfluenced by text token Kj, M pos(n)\nij\nis assigned the value of 1. On the other hand, if image token\nQi corresponds to a region of the image that should not be influenced by text token Kj, M neg(n)\nij\nis set to 1. It is worth noting that we consider the feature maps corresponding to most words as\nnegative in order to avoid generating undesired objects. The mask components M pos(n) and M neg(n)\nare incorporated into the cross-attention computation process:\n\u02dcA(n)\nij|\u03b8(n) = exp\u27e8Q(n)\ni\n, Kj\u27e9 + \u03c9posM pos(n) \u2212 \u03c9negM neg(n)\nP\nk=1 exp\u27e8Q(n)\ni\n, Kk\u27e9\n.\n(6)\nIt is found that larger weights at higher noise levels [3] can lead to better results, thus \u03c9pos and \u03c9neg\nare noise-level sensitive parameters, defined by:\n\u03c9(\u00b7) = \u03c9\u2032 \u00b7 log(1 + \u03c3) \u00b7 max(A(n)),\n(7)\nwhere \u03c9\u2032 is a user-provided hyper-parameter.\nWe then substitute the feature map corresponding to the object description. Contrary to the image\nediting motivation behind conventional prompt tuning methods, it is not necessary for our method\nto provide a reference image and an amended prompt. Recalling the framework of our generalized\nControlNet, the branch architecture is identical to the encoder portion of the backbone network. We\nfound that the attention maps A(n)\nj|\u03b8(n)\nt\nwithin the branch network also encompass object locations\nand shape information. Consequently, we opt for the attention map A(n)\nj|\u03b8(n)\nt\nassociated with the\ndescription Kj from the respective layer in ControlNet as the source for the original attention map\nA(n)\nj|\u03b8(n) substitution, ensuring that the information in the substituted attention map aligns more closely\nwith the external input signal rather than the textual information derived from the original backbone\nnetwork. Specifically, we replace the attention map generated by the original backbone network\n(based on the corresponding object description) with the attention map from the corresponding\nmodule in the branch network:\n\u02c6\nA(n) = [ \u02dcA(n)\n0|\u03b8(n); . . . ; A(n)\nj|\u03b8(n)\nt\n; . . . ; \u02dcA(n)\nNt|\u03b8(n)] \u2190 [ \u02dcA(n)\n0|\u03b8(n); . . . ; \u02dcA(n)\nj|\u03b8(n); . . . ; \u02dcA(n)\nNt|\u03b8(n)].\n(8)\nSubsequently, we can produce the spatially guided output from cross-attention layer by taking the\nproduct of \u02c6\nA(n) with V .\n4\nExperiments\nIn this section, we delve into a comprehensive experimental analysis to validate the efficacy and\nsuperiority of the proposed method through ablation studies and application demonstrations. Subse-\n6\nTable 1: Quantitative comparison on the COCO5k validation set. The best result is highlighted.\nMethod\nSimilarity\n(LPIPS \u2193)\nSketch Map\n(L2 Distance \u2193)\nSegmentation Map\n(mPA \u2191)\nSegmentation Map\n(mIoU \u2191)\nPose Map\n(mAP \u2191)\nMulti-Adapter\n0.7273 \u00b10.00120\n7.93310 \u00b10.01392\n26.30 \u00b10.242\n13.98 \u00b10.177\n40.02 \u00b10.761\nMulti-ControlNet\n0.6653 \u00b10.00145\n7.59721 \u00b10.01516\n36.59 \u00b10.273\n22.70 \u00b10.229\n38.19 \u00b10.761\nOurs w/o ControlNorm\n0.4900 \u00b10.00141\n7.18413 \u00b10.01453\n48.26 \u00b10.287\n32.66 \u00b10.272\n61.93 \u00b10.775\nOurs\n0.4836 \u00b10.00133\n7.28929 \u00b10.01385\n49.20 \u00b10.289\n33.27 \u00b10.271\n61.99 \u00b10.778\nFigure 4: Our model can generate images with the provided prompts and multi-modality information\n(e.g., edge, pose, and segmentation map) across various scales.\nquently, in Sec. 4.1, we put forth both quantitative and qualitative results, elucidating the comparative\nadvantages of our approach. We also present an array of intriguing applications made possible by our\ngControlNet, showcasing its practical utility. Finally, Sec. 4.2 is dedicated to the discussion of abla-\ntion studies, scrutinising the impacts of varying injection and sampling methods. The experimental\nconfigurations, including the dataset specifications, implementation details, and evaluation metrics,\ncan be found in the Appendix.\n4.1\nApplications and Comparisions\nCocktail is proficient in seamlessly supporting multiple control inputs and autonomously fusing\nthem, thereby eliminating the necessity for manual intervention to equilibrate diverse modalities.\nThis unique property empowers users to easily incorporate a variety of modalities, resulting in more\nflexible multi-modal control. In Figure 4, we demonstrate how users can provide spatial information\nabout multiple objects in different modalities to generate a complex scene. Notably, this entire process\nis accomplished by a single model without the need for additional branch networks.\nWe then compare Cocktail with two state-of-the-art methods: ControlNet [52] and T2I-Adapter [32],\nin the context of text-guided image-to-image translation within multiple modalities. We employ sev-\neral evaluation methods, including LPIPS, mPA, mIoU, mAP and L2 distance for various modalities\nin this section. As depicted in Table 1, our method outperformed both ControlNet and T2I-Adapter\nacross all evaluation metrics. This remarkable achievement signifies that our proposed cocktail can\ngenerate a structural image that closely resembles the ground truth image and aligns better with the\ninput conditions, establishing its superiority. The visualization in Fig. 6 also illustrates the superior\nability of our model to harmonize with the control signals.\nWe further present some samples from our method to examine its effectiveness on uni-modality\ntranslation. The visual comparison on the COCO validation set is showcased in Figure 5, highlighting\nthe compelling performance. Benefiting from mixed training of multiple modalities, our method\nachieves remarkable generation quality, surpassing even models exclusively trained for a single\ncontrol signal.\nOur experiments show that Cocktail effectively leverages information from different modalities and\nexhibits outstanding multi-object generation abilities with consistent composition across various\ncontrol signals.\n7\nFigure 5: Qualitative comparison of Uni-Modality on the COCO validation set.\n4.2\nAblations\ngControlNet and ControlNorm\nPrevious methods [52] utilized a direct sum approach to fuse\ncontrol signals from hyper-networks with latent variables from the original network. While this\nmethod conveys spatial information, it fails to consider semantic information, such as text or other\nmodalities. Control signals decoupled through ControlNorm allow the preservation of semantic\ninformation while conveying spatial information. Another point is the need for normalization to\naddress the imbalance of different modality signals. However, typical normalization methods lead to\nthe loss of semantic information [35]. Therefore, the control signals introduced through ControlNorm\ncan better interpret conditional information. We present some generated images in Fig. 10 to\nsubstantiate the interpretative capability of ControlNorm.\nSpatial Guidance Sampling\nThe spatial guidance sampling method not only ensures that objects\nare generated within controllable areas, but also minimizes the impact on other areas. An intuitive\napplication is that when we modify certain objects or modalities, other parts of the generated image\ncan remain unchanged. Figure 7 illustrates the contrast between employing spatial guidance sampling\nand its absence. A more significant variation in the overall tone of the resultant image is observed\nwhen spatial guidance is not utilized, leading to inconsistencies in the details of the attire. Conversely,\nthe incorporation of new objects and modalities with the use of spatial guidance minimally impacts\nthe original image.\n8\nFigure 6: Cocktail can address the imbalance among various modalities. Here, the \"cross\" symbol and\nthe checkmark symbol denote the unmatched and matched modalities, respectively. It is important to\nnote that our model accurately captures all modalities.\n5\nConclusion\nIn this work, we proposed Cocktail, a pipeline to achieve fine control by fusing multi-modal signals.\nFirstly, we introduced a generalized ControlNet capable of handling signals of different modalities us-\ning a single model. We also revealed that the semantic information of signals from the hyper-network\nmay be lost through direct addition. However, a simple decoupling allows for a better interpretation\nof control signals. We introduced a controllable normalisation for decoupling and integration. Finally,\nwe proposed a sampling scheme that can prevent the generation of unnecessary objects outside the\nfocus area while also achieving a certain degree of editing capability and background protection.\nLimitations.\nWhile we have achieved multi-modal fusion and control, there are still certain limita-\ntions in the handling of control signals from Cocktail that need to be addressed in future work. Firstly,\nalthough the current spatial guidance method can effectively prevent objects from being generated\noutside the focus area, it requires users to individually specify the area and corresponding object\ndescription during implementation. Secondly, spatial guidance can cause instability in the latent\nspace in certain situations, leading to the generated images degrading and deviating from the existing\ncontrol signals. Therefore, finding a stable anchor point as a reference during the generation process\nis also worth exploring.\nBroader Impacts.\nThe integration of multimodal control signals as inputs for synthesis greatly\nenhances user interaction flexibility and streamlines the utilization of text-conditional diffusion\nmodels. However, the process of fine-tuning large-scale generative models to accommodate diverse\nmodalities requires a significant amount of energy, and we anticipate that the development of a\nuniversal model capable of accepting multiple modalities will help mitigate this impact. However,\nthe growing capabilities in image generation also facilitate the production of manipulated images\nwith malicious intent, such as the creation of counterfeit or deceitful information.\n9\nReferences\n[1] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the\neffectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255, 2020.\n[2] Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit Bermano. Hyperstyle: Stylegan\ninversion with hypernetworks for real image editing. In Proceedings of the IEEE/CVF conference\non computer Vision and pattern recognition, pages 18511\u201318521, 2022.\n[3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models\nwith an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\n[4] Manuel Brack, Patrick Schramowski, Felix Friedrich, Dominik Hintersdorf, and Kristian\nKersting. The stable artist: Steering semantics in diffusion latent space. arXiv preprint\narXiv:2212.06013, 2022.\n[5] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Openpose: realtime\nmulti-person 2d pose estimation using part affinity fields. IEEE transactions on pattern analysis\nand machine intelligence, 43(1):172\u2013186, 2021.\n[6] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention\nguidance. arXiv preprint arXiv:2304.03373, 2023.\n[7] Ting Chen, Mario Lucic, Neil Houlsby, and Sylvain Gelly. On self modulation for generative\nadversarial networks. arXiv preprint arXiv:1810.01365, 2018.\n[8] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision\ntransformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.\n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nAdvances in Neural Information Processing Systems, 34:8780\u20138794, 2021.\n[10] Tan M Dinh, Anh Tuan Tran, Rang Nguyen, and Binh-Son Hua. Hyperinverter: Improving\nstylegan inversion via hypernetwork. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11389\u201311398, 2022.\n[11] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-based generative modeling with\ncritically-damped langevin diffusion. arXiv preprint arXiv:2112.07068, 2021.\n[12] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for\nartistic style. arXiv preprint arXiv:1610.07629, 2016.\n[13] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models\nas plug-and-play priors. arXiv preprint arXiv:2206.09012, 2022.\n[14] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\nBaining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10696\u201310706,\n2022.\n[15] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106,\n2016.\n[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,\n2022.\n[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in\nneural information processing systems, 30, 2017.\n[18] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\n10\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin Neural Information Processing Systems, 33:6840\u20136851, 2020.\n[20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021.\n[21] Minghui Hu, Chuanxia Zheng, Heliang Zheng, Tat-Jen Cham, Chaoyue Wang, Zuopeng Yang,\nDacheng Tao, and Ponnuthurai N Suganthan. Unified discrete diffusion for simultaneous\nvision-language generation. arXiv preprint arXiv:2211.14842, 2022.\n[22] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou.\nCom-\nposer: Creative and controllable image synthesis with composable conditions. arXiv preprint\narXiv:2302.09778, 2023.\n[23] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance\nnormalization. In Proceedings of the IEEE international conference on computer vision, pages\n1501\u20131510, 2017.\n[24] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of\ndiffusion-based generative models. arXiv preprint arXiv:2206.00364, 2022.\n[25] Alexander Kirillov.\nMscoco keypoint evaluation metric.\nhttps://github.com/\ncocodataset/cocodataset.github.io, 2021.\n[26] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic\nlatent space. arXiv preprint arXiv:2210.10960, 2022.\n[27] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-\nimage pre-training for unified vision-language understanding and generation. In International\nConference on Machine Learning, pages 12888\u201312900. PMLR, 2022.\n[28] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan\nLi, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. arXiv preprint\narXiv:2301.07093, 2023.\n[29] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models\non manifolds. arXiv preprint arXiv:2202.09778, 2022.\n[30] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for se-\nmantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 3431\u20133440, 2015.\n[31] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A\nfast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint\narXiv:2206.00927, 2022.\n[32] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453, 2023.\n[33] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n[34] Dong Huk Park, Grace Luo, Clayton Toste, Samaneh Azadi, Xihui Liu, Maka Karalashvili,\nAnna Rohrbach, and Trevor Darrell. Shape-guided diffusion with inside-outside attention. arXiv\npreprint arXiv:2212.00210, 2022.\n[35] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis\nwith spatially-adaptive normalization. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 2337\u20132346, 2019.\n11\n[36] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, and Daniel Cohen-Or. Lo-\ncalizing object-level shape variations with text-to-image diffusion models. arXiv preprint\narXiv:2303.11306, 2023.\n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021.\n[38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[39] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and\nDaniel Cohen-Or. Encoding in style: a stylegan encoder for image-to-image translation. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n2287\u20132296, 2021.\n[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. Advances in\nNeural Information Processing Systems, 35:36479\u201336494, 2022.\n[42] Vishnu Sarukkai, Linden Li, Arden Ma, Christopher R\u00e9, and Kayvon Fatahalian. Collage\ndiffusion. arXiv preprint arXiv:2303.00262, 2023.\n[43] Christoph Schuhmann.\nClip+mlp aesthetic score predictor.\nhttps://github.com/\nchristophschuhmann/improved-aesthetic-predictor, 2022.\n[44] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\n[45] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\nBen Poole. Score-based generative modeling through stochastic differential equations. arXiv\npreprint arXiv:2011.13456, 2020.\n[46] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features\nfor text-driven image-to-image translation. arXiv preprint arXiv:2211.12572, 2022.\n[47] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, and Fang Wen.\nPretraining is all you need for image-to-image translation. arXiv preprint arXiv:2205.12952,\n2022.\n[48] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better aligning text-to-\nimage models with human preference. arXiv preprint arXiv:2303.14420, 2023.\n[49] Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In International conference\non computer vision, pages 1395\u20131403, 2015.\n[50] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao\nDong. Imagereward: Learning and evaluating human preferences for text-to-image generation.\narXiv preprint arXiv:2304.05977, 2023.\n[51] Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai. Side adapter network for\nopen-vocabulary semantic segmentation. arXiv preprint arXiv:2302.12242, 2023.\n[52] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.05543, 2023.\n[53] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unrea-\nsonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 586\u2013595, 2018.\n12\n[54] Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. Egsde: Unpaired image-to-image translation\nvia energy-guided stochastic differential equations. arXiv preprint arXiv:2207.06635, 2022.\n[55] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized\nvectors for high-fidelity image generation. Advances in Neural Information Processing Systems,\n35:23412\u201323425, 2022.\n[56] Jianbin Zheng, Daqing Liu, Chaoyue Wang, Minghui Hu, Zuopeng Yang, Changxing Ding,\nand Dacheng Tao. Mmot: Mixture-of-modality-tokens transformer for composed multimodal\nconditional image synthesis. arXiv preprint arXiv:2305.05992, 2023.\nA\nExperimental settings\nA.1\nDatasets and Annotations.\nAll of our experiments are performed on LAION-AESTHETICS-6.5 dataset, which contains about\n600K image-text pairs with predicted aesthetics scores of higher than 6.5.\nOur experiment includes three types of commonly used additional control signals, and we use different\nmethods to obtain pseudo-labeling annotations:\n\u2022 Sketch map: The delineation of texture details within the structured control signal is accom-\nplished using the HED boundary detector [49]\n\u2022 Semantic segmentation map: To derive segmentation maps from images, we leverage\nSAN [51], a high-performing, open-vocabulary segmentator.\n\u2022 Human pose map: The generation of a comprehensive human pose map, including body,\nface, and hand positions, is facilitated by OpenPose [5].\nIt is crucial to note that our proposed Generalized ControlNet exhibits broad applicability and can be\neffortlessly adapted to accommodate a multitude of other modality inputs.\nA.2\nImplementation Details.\ngControlNet is adapted from the pretrained Stable Diffusion v2.1 in this paper and trained for 20\nepochs with a batch size of 64 on 4 NVIDIA 80G-A100 GPUs within 4 days. We use the AdamW\noptimizer with a learning rate of 3.0 e-05. All the training images in the LAION-AESTHETICS-6.5\nare first resized to 512 by the short side and then randomly cropped to 512 \u00d7 512. During inference,\nthe sampler is DDIM, the sampling steps are 50, and the classifier-free guidance scale is 9.0 by\ndefault.\nA.3\nEvaluation Metrics.\nIn order to compare the generation performance of existing methods with ours, we adopted six metrics\nto evaluate the quality, text-image alignment, aesthetics, and human preference of the generated\nimages. They are Frechet Inception Distance (FID) [17], CLIP/BLIP Score [37, 27], Aesthetic\nScore [43], Human Preference Score (HPS) [48] and ImageRewared [50]. Specifically, FID measures\nthe distribution distances of real and generated image sets utilizing a pre-trained classification\nnetwork (e.g., Inception V3 trained on ImageNet). Instead, CLIP/BLIP Score calculates cosine\nsimilarities between the corresponding image and text features extracted by the CLIP/BLIP image\nand text encoders, respectively. The Aesthetic Score is based on an additional MLP layer on top of a\npretrained CLIP image encoder to measure the human aesthetic aspect of a single image. To consider\nthe alignment with human values and preferences, HPS and ImageReward were proposed and are\nbased on the reward model pretrained on CLIP or BLIP.\nHowever, these metrics do not effectively measure the fidelity of our model to different modalities.\nTherefore, we additionally employ several other evaluation metrics, including the Learned Perceptual\nImage Patch Similarity (LPIPS) [53] for overall image quality, the L2 distance for Holistically-Nested\nEdge Detection (HED), the mean Pixel Accuracy (mPA) [30] and mean Intersection over Union\n(mIoU) [30] for segmentation maps, and the mean Average Precision (mAP) over 10 object keypoint\n13\nTable 2: Additional quantitative comparison on the COCO validation set. The best result is high-\nlighted.\n(a) Text + Edge\nMethod\nFID \u2193\nCLIP Score \u2191\nBLIP Score \u2191\nAesthetic Score \u2191\nHPS \u2191\nImageReward \u2191\nControlNet\n18.73\n0.2588\n0.5338\n5.39\n19.53\n0.3726\nT2I-Adapter\n20.21\n0.2638\n0.5335\n5.36\n19.69\n0.3344\nOurs\n16.66\n0.2561\n0.5321\n5.35\n19.53\n0.2989\n(b) Text + Segmentation Map\nMethod\nFID \u2193\nCLIP Score \u2191\nBLIP Score \u2191\nAesthetic Score \u2191\nHPS \u2191\nImageReward \u2191\nControlNet\n20.53\n0.2640\n0.5262\n5.43\n19.85\n0.2609\nT2I-Adapter\n21.81\n0.2626\n0.5225\n5.25\n19.57\n0.0837\nOurs\n23.88\n0.2613\n0.5260\n5.64\n19.89\n0.3735\n(c) Text + Pose\nMethod\nFID \u2193\nCLIP Score \u2191\nBLIP Score \u2191\nAesthetic Score \u2191\nHPS \u2191\nImageReward \u2191\nControlNet\n35.13\n0.2656\n0.5171\n5.49\n20.17\n0.3683\nT2I-Adapter\n35.53\n0.2697\n0.5211\n5.45\n20.19\n0.3810\nOurs\n39.64\n0.2641\n0.5210\n5.70\n20.22\n0.4488\n(d) Text + Sketch + Segmentation + Keypoints\nMethod\nFID \u2193\nCLIP Score \u2191\nBLIP Score \u2191\nAesthetic Score \u2191\nHPS \u2191\nImageReward \u2191\nOurs\n16.70\n0.2562\n0.5326\n5.36\n19.56\n0.3118\nsimilarity (OKS) thresholds [25] for human pose map. Specifically, LPIPS is utilized to assess the\ndissimilarity between the generated image and the ground-truth image. Furthermore, we extract the\nconditions, namely the sketch map, segmentation map, and pose map, from both the generated image\nand the ground-truth image. By computing the distance, specifically employing L2 Distance, mPA,\nmIoU, and mAP, between these extracted conditions, we can gain a deeper understanding of the\nfidelity with respect to various modalities.\nThrough these metrics, we can more clearly ascertain whether the generated images can follow the\nguidance of different modal conditions.\nB\nAdditional quantitative analysis\nWe compare Cocktail with two state-of-the-art methods: ControlNet [52] and T2I-Adapter [32], in\nthe context of text-guided image-to-image translation within a single modality. As demonstrated in\nTable 2, our model not only acquires multi-modal capabilities but also demonstrates performance on\npar with the comparative methods in single-modal tasks. It is worth noting that FID was measured on\nthe basis of 5000 images from zero-shot generation.\nIt worth noting that FID may not provide a precise evaluation as they are substantially contingent\non the sample size and the characteristics of the training dataset. Our methodology prioritizes the\nfidelity of the generated images, a perspective that is at odds with the principles of CLIP/BLIP.\nC\nAdditional qualitative results\nWe provide additional generated images under various multi-conditional scenarios and compare\nthem with Multi-ControlNet and Multi-Adapter. In Fig. 6, we have demonstrated some samples\nwithin less complexity, we further show the comparative performance of our model against other\nstate-of-the-art models under challenging disjoint scenarios in Fig. 8. In Fig. 10, we also conduct an\nablation study on the effectiveness of our ControlNorm module. It can be observed that our method\noutperforms other state-of-the-art models in terms of balancing and expressing multi-modal signals,\n14\nFigure 7: With spatial guidance, our method is capable of maintaining constancy in certain regions\nwhile modifying specific objects.\nand ControlNorm provide a more stable integration performance. Besides, our model is able to accept\narbitraty combiantions of the given modalities, as shown in Fig. 9.\nWe also provide further samples under single modality signals, e.g., Human Pose in Figs. 11&12,\nSketch Map as shown in Figs. 13&14 and Segmentation Map in Fig. 15. It can be seen that our\nmethod is more faithful to the given conditional signals.\n15\nFigure 8: Additional results for Disjoint Multi-Modality Control.\n16\nFigure 9: Our model accepts arbitrary combinations of the given modalities.\n17\nFigure 10: ControlNorm can address the imbalance among various modalities. Note that the framed\nareas in magenta and cyan do not provide a high-fidelity interpretation.\n18\nFigure 11: Additional results for Single Modality Control: Key Points\nFigure 12: Additional results for Single Modality Control: Key Points\n19\nFigure 13: Additional results for Single Modality Control: Sketch\nFigure 14: Additional results for Single Modality Control: Sketch\n20\nFigure 15: Additional results for Single Modality Control: Segmentation Map\n21\n"
  },
  {
    "title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
    "link": "https://arxiv.org/pdf/2306.00622.pdf",
    "upvote": "1",
    "text": "ReviewerGPT? An Exploratory Study on Using\nLarge Language Models for Paper Reviewing\nRyan Liu and Nihar Shah\n{ryanliu, nihars}@andrew.cmu.edu\nCarnegie Mellon University\nAbstract\nGiven the rapid ascent of large language models (LLMs), we study the question: (How) can large lan-\nguage models help in the reviewing of scientific papers or proposals? We first conduct some pilot studies\nwhere we find that (i) GPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly,\nOpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outper-\nforms prompting to simply write a review. With these insights, we study the use of LLMs (specifically,\nGPT-4) for three tasks:\n1. Identifying errors:\nWe construct 13 short computer science papers each with a deliberately\ninserted error, and ask the LLM to check for the correctness of these papers. We observe that the\nLLM finds errors in 7 of them, spanning both mathematical and conceptual errors.\n2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist questions in the\nrespective sections of 15 NeurIPS 2022 papers. We find that across 119 {checklist question, paper}\npairs, the LLM had an 86.6% accuracy.\n3. Choosing the \u201cbetter\u201d paper:\nWe generate 10 pairs of abstracts, deliberately designing each\npair in such a way that one abstract was clearly superior than the other.\nThe LLM, however,\nstruggled to discern these relatively straightforward distinctions accurately, committing errors in\nits evaluations for 6 out of the 10 pairs.\nBased on these experiments, we think that LLMs have a promising use as reviewing assistants for specific\nreviewing tasks, but not (yet) for complete evaluations of papers or proposals.\n1\nIntroduction\nLarge language models (LLMs) have recently been found to excel in many different domains. Their success\nin general purpose tasks also raises the natural question: can they be used for reviewing scientific papers (or\nproposals)? Peer review is highly strained due to fast increasing numbers of submissions and overburdening\nof reviewers [McC06; Sha22]. It is estimated that millions of hours of researchers\u2019 time is spent in review-\ning [The13]. Furthermore, several controlled experiments have found that flawed papers frequently get past\nhuman reviewers [Bax+98; GGM98; Sch+04; Sch+08]. LLMs hold a considerable potential in relieving some\nof these issues in the scientific review process.\nWith this motivation, we conduct an exploratory study on whether and how LLMs can be used for\nreviewing. We first conduct a pilot (Appendix A) to select the model and prompting strategies. In the\npilot, we first compared various models (GPT-4 [Ope23b], Bard [Man23], Vicuna [The23], Koala [Gen+23],\nAlpaca [Tao+23], LLaMa [Tou+23], Dolly [Con+23], OpenAssistant [LAI23], and StableLM [Sta23]) by\nasking them to find errors in a short paper. We found that GPT-4 was the only model that was successful\nat this task. We also piloted prompting strategies, and found that by asking the model targeted questions,\nthe responses generated are significantly more useful compared to requesting it to simply \u2018write a review\u2019.\nWith these choices in place, we then evaluated the following three reviewing tasks on GPT-4, employing\ntargeted prompts for each task:\n1\narXiv:2306.00622v1  [cs.CL]  1 Jun 2023\nCan LLMs identify errors in papers? (Section 3) A primary objective of the scientific reviewing\nprocess is to identify any shortcomings in scientific manuscripts and ensure that only robust and accurate\nscientific content gets published. To evaluate the LLM\u2019s efficacy in achieving this objective, we constructed\n13 short papers, intentionally infusing each of them with a key error. The errors spanned from mathematical\nerrors (e.g., wrong implication of mean squared convergence) to logical fallacies (e.g., false inference drawn\nfrom a data analysis). The LLM identified the errors in 7 of the 13 short papers.1 This rate is comparable\nto that of human peer reviewers [Bax+98; GGM98; Sch+04; Sch+08].\nCan LLMs verify author-provided checklists? (Section 4) Many conferences and journals provide\nauthors with a checklist of items that helps set expectations from papers and also offers a guideline to authors\nto meet these expectations. In venues such as NeurIPS,2 the authors need to submit their responses to the\nchecklist along with the submission. The checklist items are verified by reviewers, and failing to properly\nadhere to the checklist can get the paper rejected. We investigate if LLMs can do this job of verifying\nwhether the checklist items are accurately reported by the authors. In a set of 15 papers from NeurIPS\n2023, we selected checklist items where the authors answered \u201cYes\u201d, manually labeled their ground truth\nanswers, and prompted the LLM to answer the same checklist items by providing the relevant section(s) of\nthe paper. We found that across 119 unique {checklist question, paper} pairs, (i) taking the majority answer\nover 3 responses, the LLM achieved 86.6% accuracy compared to the ground truth label; (ii) coincidentally,\n86.6% of author responses matched our ground truth label; (iii) LLM answers disagreed with 75% of mis-\nmatched author responses, and (iv) 50% of the LLM\u2019s errors were due to questions where the answers were\nundeterminable with the text in the paper (e.g., requiring information provided in figures).\nCan LLMs select the better of two papers? (Section 5) With the successful performance of LLMs\non the aforementioned two specific tasks, we moved on to a more ambitious question of whether LLMs can\nselect the better of two papers in a selective venue3 or the better of two proposals in a grant evaluation\nsetting. To this end, we considered arguably the simplest of tasks: For a given pair of abstracts where one\nabstract is constructed to be superior to another (e.g., both abstracts are identical but one has an additional\nresult), can the LLM identify the better abstract? We constructed 10 such pairs of abstracts. In a subset\nof these pairs, we also inserted certain distractions such as the use of bombastic language or buzzwords\nin the inferior abstract. We then tested the LLM\u2019s ability to accurately identify the superior abstract in\nthese scenarios. We found that the LLM performs surprisingly poorly at this task, erring in 6 of the 10\ncases. The four cases where it succeeded involved identifying an abstract\u2019s incorrect interpretation of a null\nresult, accurately interpreting upper bounds, remaining unaffected by buzzwords, and disregarding author\nidentities. On the other hand, the six failures encompassed a bias towards positive results, misinterpreting\nparameter ranges, misinterpreting lower bounds, falling victim to a prompt injection attack, being swayed\nby bombastic language in the inferior abstract, and getting influenced by the name of the algorithm.\nOverall, our results suggest that LLMs (particularly GPT-4 at its current stage) have considerable promise\nin being used for specific parts of the review process, but are not yet capable of functioning as a standalone\nreviewer.\n2\nRelated work\nIn this section we discuss literature that is most closely related to our work.\nLarge language models. Recent advances in LLMs have spurred the investigation of the general capa-\nbilities of these models [Bub+23; Ope23b; Ani+23].\nOn one hand, natural language processing (NLP)\nresearchers have designed methods to address and improve the factuality and reasoning capabilities of\n1While GPT-4 identified the errors in 7 of the 13 papers, we also subsequently tried other models\u2014Bard, Vicuna, Koala,\nAlpaca, LLaMa, Dolly, OpenAssistant, and StableLM. None of these models could identify any error in any of the 13 papers.\n2Short for Neural Information Processing Systems \u2013 a top-tier machine learning conference.\n3There are evidence-based arguments [Ras+22] challenging the notion of papers being directly comparable, rendering ques-\ntions about \u201cbetter\u201d papers seemingly futile. Nevertheless, we delve into this inquiry due to the persistent pursuit of selectivity\nin most peer-review venues. Additionally, selectivity becomes crucial in various other contexts, such as the evaluation of grant\nproposals, where limited resources necessitate selective distribution among researchers.\n2\nLLMs [Du+23; Wei+22; Xia+22; ZJH23]. On the other hand, investigations on the capabilites of LLMs\nhave spread from NLP to various domains, such as agent simulation, data generation, computational social\nscience, human-computer interaction, and health/law.\nUnder simulation, LLM generative capabilities have been used for embodied agent planning [Son+22;\nWan+23] and human agent simulation [Par+23]. Data generation has seen advances in either simplifying or\nfully automating the creation of questions and labels [Par+22; Liu+22; SS21; Wan+21]. In computational\nsocial science, there have been general studies [Zie+23] as well as targeted investigations within areas such\nas theory of mind [Ull23] and moral judgment [Jin+22]. Human-computer interaction has explored using\nLLMs to generate believable accounts of HCI experiences [HTK23]. Finally, LLMs have also been used in\nhealth and biomedical applications [Liu+23; Che+23a; Che+23b] and legal processes [Nay23].\nAutomation in peer review. The peer review process in many venues already has a considerable amount\nof automation deployed. The part of the review process that involves the most automation is that of assigning\nsuitable reviewers to papers. A number of algorithms have been used to compute the expertise of reviewers for\npapers [CZ13; Wie+19; Coh+20], and various algorithms have also been used to assign reviewers to papers in\na manner that ensures a high expertise of the assigned reviewers [CZ13; SSS21b; KSM19]. Following recent\ndiscoveries of \u201ccollusion rings\u201d [Vij20; Lit21] in peer review, several venues have incorporated randomization\ninto their assignment algorithms [Jec+20] to mitigate this issue.\nA number of venues have used algorithms [NSP21] to address the issue of subjectivity or commensuration\nbias [Lee15]. Some venues have also used algorithms to address the issue of miscalibration [GWG13], however,\nthese calibration algorithms have not worked well [Sha22, Section 5]. Outside of computer science, there\nare algorithms designed specifically for checking that the submitted papers follow suitable submission and\nreporting guidelines [Hou16; FMG19]. Various other general computational tools are used in the peer-review\nprocesses, e.g., plagiarism checkers, scripts to detect conflicts of interest based on databases like DBLP, etc.\nSome previous attempts at designing machine learning algorithms to conduct comprehensive paper reviews\nhave shown limited success in generating appropriate reviews, but these algorithms have proven useful in\nother capacities such as generating paper summaries [Hua18; Wan+20; YLN21].\nThere are also nascent investigations into using language models for peer review and adjacent areas. The\npaper [HH23] summarizes the desired roles of agents and objectives in peer review, and provide recommenda-\ntions for the safe use of LLMs in this context. [Zha+22] use RoBERTa [Liu+19] to assess the textual fluency\nof papers and sentiment of reviews in an analysis of peer review fairness. And in peer grading, [Mor+23]\nuse a subset of rater data in massively open online courses to finetune distilBERT [San+20], and use it to\nvalidate peer grading scores.\nPeer-review datasets. This paper also contributes a small-scale dataset for evaluating machine learning\n/ language models for reviewing papers. Any LLMs subsequently designed may be evaluated on the 13\nshort papers and the pilot short paper we have constructed with inserted \u201cgold standard\u201d flaws, where we\ncurrently find that only GPT-4 has any success in detecting errors. Similarly, LLMs may be evaluated on\nthe 119 {checklist question, paper} pairs we have labeled. On the 10 pairs of abstracts we have constructed,\ntheir accuracy on direct comparative evaluations and their resistance to distractions can be examined. Of\ncourse, this assumes that the LLM is not itself trained on this very paper. In addition, our aforementioned\nconstructions can also be used for few-shot learning or prompting of these models.\nWe now review other useful datasets pertaining to (peer) reviewing. Given the fact that the assignment\nof reviewers to papers is the most automated part of peer review, the paper [Ste+23] releases a \u201cgold\nstandard\u201d dataset of expertise of reviewers for papers to train and/or evaluate such algorithms. Motivated\nby the absence of any datasets for the problem of mitigating collusion rings, the paper [Jec+23] releases\na dataset on collusion strategies constructed from a mock peer-review exercise. Another issue is strategic\nbehavior of reviewers where they manipulate reviews they are providing (e.g., lower scores of papers they\nare reviewing) to improve the chances of their own paper, and the paper [SSS21a] releases a dataset of such\nstrategies also obtained from a mock peer-review exercise. The paper [Ker+20] releases anonymized review\nratings and author-provided feedback.\n3\nIn the past few years, platforms such as OpenReview.net and conferences such as ICLR and NeurIPS\nrelease the peer reviews along with the papers. Such reviews have been compiled and released as useful\ndatasets [Kan+18]. The release of reviews is not limited to the field of computer science alone: scipost.org and\nf1000research.com outside computer science also release the peer reviews publicly. Subsequently, a number\nof studies have released datasets that annotate the reviews (and discussion posts), including annotations\nbased on the arguments [Hua+19; Fro+20; Ken+21; Kuz+22; Gho+22] or sentiments [CGM20; Bul+20].\n3\nCan LLMs detect errors in computer science papers?\nIn this section, we discuss our experiment to investigate whether LLMs can identify flaws in short computer\nscience papers. Based on our pilot (Appendix A.1) in which GPT-4 is the only model that detected the\nerror, we use GPT-4 as the LLM for these experiments.\n3.1\nMethods\nWe constructed 13 short papers (detailed in Section 3.3). In each of these papers, we deliberately inserted\nan error, encompassing mathematical mistakes to conceptual fallacies. We then asked the LLM to identify\nif there were any errors in the short paper. We used the following three prompts:\n\u2022 Prompt-Direct: You are an expert reviewer for a scientific conference. You will be provided with a\nshort version of a paper that contains the setting of the paper and the main claims. Please check for\nthe validity and correctness of these claims, and in particular, report if you can figure out if any of\nthese claims is false based on the information provided in this short paper. Think step by step when\nchecking each claim. Here is the short paper: \u201c...\u201d\n\u2022 Prompt-OneShot: You are an expert reviewer for a scientific conference. You will be provided with a\nshort version of a paper that contains the setting of the paper and the main claims. Please check for\nthe validity and correctness of these claims, and in particular, report if you can figure out if any of\nthese claims is false based on the information provided in this short paper. Think step by step when\nchecking each claim.\nYou will first be provided with an example. Consider this example short paper:\nWe consider the problem of searching in a list of numbers, where the list is already sorted in\na non-decreasing order.\nIf the number exists in the list, then the search should return the\nposition of that number in the list (if there are multiple copies of the number in the list, then\nit can return any one position). If the number does not exist in the list, then it should output\n\u2018does not exist\u2019. The number to be searched is specified by the user. For this problem, we\ndevelop a new artificial intelligence based algorithm and also prove theoretical guarantees for\nit. Specifically, we show that our algorithm requires only sqrt(m) comparisons between pairs\nof numbers, where m is the size of the entire list. We also provide an associated information-\ntheoretic lower bound showing that our bound is tight, i.e., any algorithm will need at least\nthese many comparisons in the worst case. We conduct a large number of simulations that\nconfirm this theoretical result. Our simulations vary the size of the list, the values in the list\n(drawing them from various distributions), as well as the value to be searched. Our result is\nespecially important as it is rare for such practical artificial intelligence algorithms to also have\nsuch strong theoretical guarantees. We hope practitioners will take note of this new result.\nThe review of this short paper should point out the error that searching in a sorted list of numbers\nneeds only log(m) comparisons (via binary search), and hence the paper\u2019s claim that they provide a\nsqrt(m) worst case lower bound is false.\n4\nNow, please check for the validity and correctness of the claims in the following short paper, and in\nparticular, report if you can figure out if any of these claims is false based on the information provided\nin this short paper. Think step by step when checking each claim. Here is the short paper: \u201c...\u201d\n\u2022 Prompt-Parts: You are an expert reviewer for a scientific conference. You will be provided with a\nshort version of a paper that contains the setting of the paper and the main claims. Please check for\nthe validity and correctness of these claims, and in particular, report if you can figure out if any of\nthese claims is false based on the information provided in this short paper. You will be provided the\npaper one set of sentences at a time.\nHere is the first set of sentences of the paper: \u201c...\u201d Does this contain any incorrect claim? Think step\nby step to reason out your answer.\nHere is the next set of sentences of the paper: \u201c...\u201d Based on the context of the previous sentences,\ndoes this contain any incorrect claim or does it invalidate any claim made in the previous sentences of\nthis paper? Think step by step to reason out your answer.\nNote that Prompt-Parts does not provide the entire short paper at once, but instead provides parts of the\npaper a few sentences at a time. In particular, for the sentences where the main claims are made, each\nprovided part comprises a single sentence. In the prompts for both this and the next experiment, the phrase\n\u201cthink step by step\u201d is inspired by [Koj+23]. For this section\u2019s experiments, we access the GPT-4 model\nthrough ChatGPT (May 3 and May 12 builds) [Ope23a].\n3.2\nSummary of results\nIn Table 1, we provide a summary of our results. We queried the LLM for three responses per prompt and\nwe provide an evaluation of each response as well as an overall quantification of the performance. In the\n\u2018overall\u2019 row and column in the table, we consider it as a \u2713 if any of the responses to any of the prompts\nwas a \u2713. This is because in practice, one can obtain multiple responses to multiple prompts and flag the\npaper if any of them detect an error.\nWe find that the LLM (GPT-4) can detect the error in 7 of the 13 short papers. Furthermore, in each of\nthe 6 papers where it failed to detect the error, the paper did not contain the complete proof for the claim,\nthereby disallowing the LLM to detect a deductive error, and requiring it to figure out the flaw based on\nextraneous knowledge. We also find that the LLM occasionally outputs false positives, that is, falsely claims\na correct part of the paper to be incorrect.\nIn passing, it is worth noting that we subsequently also tried other models\u2014Bard, Vicuna, Koala, Alpaca,\nLLaMa, Dolly, OpenAssistant, StableLM. For prompting, we used Prompt-Direct. None of these models\nwere able to identify the error in any of the papers. Additionally, some of these other models raised concerns\nreminiscent of the proverbial \u201cReviewer #2\u201d, making unwarranted critiques such as \u201cthe flaw in this paper is\nthat they do not conduct experiments in addition to theoretical results\u201d or even more nonsensical comments.\n3.3\nPaper and response details\nIn this section, we describe the 13 papers we constructed. Along with each paper, we provide one exam-\nple response from the LLM. Additional responses from the LLM are available at https://github.com/\nniharshah/ReviewerGPT2023\n3.3.1\nBias/fairness\nThe following constructed paper rebuts another fictitious paper (Reference 1) that analyzed data involving\nSimpson\u2019s paradox. The paper has a logical fallacy, and we investigate whether the LLM can identify it.\n5\nPaper topic\nPerformance\nOverall\nPrompt-Direct\nPrompt-OneShot\nPrompt-Parts\n1.\nBias/fairness\n\u2713\n\u2713\n\u2713\n\u2713\n\u00d7\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n2.\nNon-parametric regression\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u00d7\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n3.\nSorting (harder)\n\u00d7! \u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7!\n4.\nSorting (easier)\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n5.\nNoisy pairwise comparisons (harder)\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n6.\nNoisy pairwise comparisons (easier)\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n7.\nClassification\n\u2713\n\u00d7\n\u2713\n\u2713\n\u00d7\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n8.\nGame theory\n\u2713\n\u2713\n\u2713\n\u2713\n\u00d7! \u2713\n\u00d7! \u2713! \u2713!\n\u2713!\n9.\nError correcting codes\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n10.\nOptimization\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n11.\nClustering\n\u2713\n\u00d7\n\u2713\n\u2713\n\u00d7\n\u2713\n\u00d7\n\u2713\n\u2713\n\u2713\n12.\nDistinguishing styles: 80% accuracy\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u2713\n\u2713\n13.\nDistinguishing styles: 50% accuracy\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nOverall\n5\u2713\n8\u00d7 1!\n6\u2713 7\u00d7 1!\n7\u2713 6\u00d7 1!\n7\u2713 6\u00d7 2!\nTable 1: Performance of the LLM in detecting incorrect claims in short papers we constructed. We show the\nperformance for the 3 generated responses for each of the three prompts. The notation \u2713 indicates that the\nLLM correctly detected the error, \u00d7 indicates that it did not detect the error, and ! indicates a false alarm.\nThe purpose of this paper is to rebut the claims of the paper [Reference 1] and show that their\nclaims are false. For the reader not aware of [Reference 1], we first provide a brief background.\nA new company, called FairAIrecruiter, is helping four companies \u2013 let\u2019s call these companies as\nCompany I, Company II, Company III, and Company IV \u2013 to automate their hiring process as\nfollows:\n- Any applicant who wishes to apply to either of these four companies must submit their resume to\nFairAIrecruiter.\n- FairAIrecruiter has put a constraint that any applicant can apply to only one of the four companies.\n- FairAIrecruiter collects all resumes until a certain deadline and then processes them. The goal of\nthis processing is to choose applicants whom each company will then interview.\n- Company I and Company II have told FairAIrecruiter that they will interview the top 10% of\napplicants who have applied to Company I and Company II respectively. Company III and Company\nIV have told FairAIrecruiter that they will interview the top 25% of applicants who have applied to\nCompany III and Company IV respectively.\nNow, FairAIrecruiter uses machine learning to decide the set of applicants to be interviewed.\nSpecifically, FairAIrecruiter has designed a function h:All possible resumes \u2212 > [0,1], where the label\nspace is the predicted \u2018quality\u2019 of the applicant (higher is better). For each of the four companies,\nFairAIrecruiter follows the following procedure. FairAIrecruiter takes the set of all applicants for\nthat company, computes the predicted quality for each applicant using the designed function h, and\nselects the top applicants in terms of the predicted quality. The number of applicants selected for\neach tech company is determined by the aforementioned percentages that the company has supplied.\nAfter completion of this process, the authors of [Reference 1] started investigating if FairAIrecruiter\nis really fair. They considered two groups of people (where the groups were determined according\nto a specific demographic attribute). Call them group A and group B. The authors of [Reference\n6\n1] found out that there were a total of 1000 applicants belonging to group A and 1000 applicants\nbelonging to group B. They also found out that the distribution of qualities of the 1000 group A\napplicants is identical to the distribution of the qualities of the 1000 group B applicants.\nThey\nthen found that, however, 220 applicants from group A received interview calls whereas only 130\napplicants from group B received interview calls. They thus concluded that an applicant from group\nA was nearly twice as likely to receive an interview call under FairAIrecruiter\u2019s system even though\nthe applicants in the two groups had identical distributions of qualities. The paper [Reference 1]\nthen accused FairAIrecruiter\u2019s algorithm of a clear bias against group B.\nIn the present paper, we delve deeper into this issue. We take a deeper look at the data. In what\nfollows, for each of the four companies, we list out the total number of applicants from group A, the\nnumber of applicants from group A invited for interview, the total number of applicants from group\nB, and the number of applicants from group B invited for interview respectively:\n- Company I: 100, 10, 400, 40\n- Company II: 100, 10, 400, 40\n- Company III: 400, 100, 100, 25\n- Company IV: 400, 100, 100, 25\nCrucially, observe that for each company, the rate at which applicants from group A were invited for\ninterview is identical to the rate at which applicants from group B were invited for interview. There\nis thus no bias, thereby falsifying the claim made in [Reference 1].\nWhile the mathematical statements made in [Reference 1] \u2014 that of being 1000 applicants in total\nfrom either group, and 220 group A and 130 group B applicants receiving interview calls \u2013 are correct,\nand seem to indicate evidence of bias, we have actually found there is no bias. As a consequence, this\nresult therefore implies that the remaining part of the argument used by [Reference 1] to arrive at\ntheir false conclusion \u2013 their claim that the distribution of qualities of the 1000 group A applicants is\nidentical to the distribution of the qualities of the 1000 group B applicants \u2013 must be false. We thus\nencourage more future studies on robust and unbiased assessments.\nThe paper has a logical fallacy in the claimed implication that the remaining part of the argument of\nReference 1 must be false. The LLM successfully identifies this fallacy. Here is an example of its response\nto Prompt-OneShot.\nThe claims of the short paper can be analyzed as follows:\n1. **Claim: For each company, the rate at which applicants from group A were invited for interview\nis identical to the rate at which applicants from group B were invited for interview, thereby showing\nno bias.** This claim appears to be correct, based on the data presented in the paper. Indeed, for\neach company, the ratio of applicants from group A and B that were invited for an interview is the\nsame:\n- Company I: 10/100 = 40/400\n- Company II: 10/100 = 40/400\n- Company III: 100/400 = 25/100\n- Company IV: 100/400 = 25/100\nThese ratios align with the promised interview rates by each company (10% for Companies I and II,\n25% for Companies III and IV), meaning the process is fair when viewed on a per-company basis.\n7\n2. **Claim: The conclusion of [Reference 1] that the system is biased against group B is false.**\nBased on the information presented in the paper, this claim seems to be correct. It is true that, in\naggregate, fewer candidates from group B received interview calls (130 vs 220 for group A). However,\nwhen the numbers are broken down by company, it is clear that the same proportion of candidates\nfrom each group received calls for each company. The difference in aggregate numbers is due to the\ndistribution of applicants from each group among the companies, not due to any discernible bias in\nthe process.\n3. **Claim: The assertion by [Reference 1] that the distribution of qualities of the 1000 group A\napplicants is identical to the distribution of the qualities of the 1000 group B applicants must be\nfalse.**\nThis claim is more complex and doesn\u2019t necessarily hold up under scrutiny.\nThe claim that the\ndistribution of qualities is the same among group A and B applicants does not necessarily contradict\nthe observed outcomes. The result could indeed be due to a fair selection process, but with differing\napplicant distribution among the companies. The more substantial number of group B applicants ap-\nplied to Companies I and II, which have a lower interview rate (10%), leading to a lower aggregate call\nrate for group B. This is not indicative of a difference in the distribution of qualities within each group.\nIn summary, the paper\u2019s claims regarding the lack of bias in FairAIrecruiter\u2019s system seem valid,\nbased on the provided data. However, the claim that the distribution of applicant qualities must\ndiffer between groups A and B due to the observed outcomes is not necessarily correct. The outcomes\ncould be explained by the differing distribution of applicants between the companies, rather than\ndiffering applicant qualities.\n3.3.2\nNon-parametric regression\nThe content of the following paper is fairly mathematically involved, describing a statistical model followed\nby some properties of an estimator.\nWe consider a statistical model of the form \u2018y i = f(x i) + epsilon i\u2019 for i ranging from 1 through\nn, where n is the sample size. Here, for each i in 1 through n, x i is a known and given vector in\nd-dimensional Euclidean space, and y i is a real value. Each vector x i is chosen independently and\nuniformly at random from [-1,1]\u02c6d. The notation f denotes an unknown function from [-1,1]\u02c6d to R.\nIt is known that the function f is coordinate-wise non-increasing, but no other information about it\nis known, and it can be arbitrary. The terms epsilon 1,...,epsilon n denote n i.i.d. random variables\nwhich follow the standard Gaussian distribution; the values of these are not known. The goal is to\nestimate the function f from the observations y 1,...,y n (and using the knowledge of x 1,...,x n).\nAlthough this problem seems hard at first, we show that a simple estimator \u2013 one that chooses the\nfunction f from the coordinate-wise monotonic class that best fits the data under the squared loss \u2013\nasymptotically recovers the true f. Specifically, letting \\hat{f} n denote the estimate of the function\nwhen there are n samples, we show that integral {x in [-1,1]\u02c6d} (f - \\hat{f} n)\u02c62 dx approaches 0\nas n goes to infinity. As an immediate implication of this result, we have that the sequence Proba-\nbility(\\hat{f} n = f) converges to 1 as n goes to infinity. This completes the theoretical guarantees.\nFurthermore, our algorithm is computationally efficient (e.g., executes in a matter of minutes on a\nstandard laptop when n=10,000), thereby making it incredibly useful in practice.\nThe paper incorrectly claims that the mean squared convergence of the estimator \u02c6fn to f directly implies\nlimn\u2192\u221e Probability( \u02c6fn = f) = 1. Under Prompt-Direct the LLM does not recognize this issue. In two\n8\nof the three attempts with Prompt-OneShot, the LLM hints at this issue. Using Prompt-Parts, the LLM\ncorrectly figures out that this is an incorrect claim. Here is a response to the problematic sentence \u201cAs an\nimmediate implication of this result, we have that the sequence Probability(\\hat{f} n = f) converges to 1\nas n goes to infinity.\u201d under Prompt-Parts.\nThis claim seems to be a misunderstanding or misstatement of the previous results.\nWhile the\nauthors previously stated that the integral over the d-dimensional cube of the squared difference\nbetween the true function f and the estimate \\hat{f} n approaches zero as n goes to infinity, it\ndoesn\u2019t imply that the probability of \\hat{f} n being exactly equal to f converges to 1.\nHere is a step-by-step breakdown:\n1.\nThe authors claim that the sequence Probability(\\hat{f} n = f) converges to 1 as n goes to\ninfinity. This would imply that the estimator \\hat{f} n is exactly equal to the true function f with\nprobability approaching 1 as the sample size n goes to infinity.\n2. However, the previous result, integral {x in [-1,1]\u02c6d} (f - \\hat{f} n)\u02c62 dx approaches 0 as n goes\nto infinity, doesn\u2019t guarantee that \\hat{f} n will be exactly equal to f. This integral represents the\nexpected squared error over the entire domain of the function. While this expected error approaches\nzero as n goes to infinity, it does not guarantee that the error is exactly zero, which would be required\nfor \\hat{f} n to be exactly equal to f.\n3. In statistical estimation theory, even as sample size goes to infinity, there is generally still some\ndegree of uncertainty about the true parameter value. In this case, even though \\hat{f} n may be a\ngood estimate of f, the probability that they are exactly equal is not necessarily 1, especially since\nthe function f can be arbitrary and we are dealing with continuous space where exact equality is a\nstrong condition.\n4. The authors appear to be conflating the idea of an estimator converging in mean square (which\nis what the integral suggests) with the idea of an estimator being equal to the true value with\nprobability 1. These are not the same thing.\nTherefore, based on the context and the principles of statistical theory, this claim appears to be\nincorrect or, at the very least, a misinterpretation of the previous results.\n3.3.3\nSorting (harder)\nThis paper considers the standard problem of sorting an array, but assumes some additional conditions on\nthe array which will make no difference to the orderwise complexity of the sorting problem.\nSuppose you have an array of R distinct real-valued numbers. Suppose you wish to sort the array\nin increasing order. The sorting algorithm can only query for comparisons of the form \u2018is [value of\nnumber at a certain position] greater than [value of number at a different position]?\u2019. Suppose that\nsuch queries are guaranteed to get accurate Yes/No answers. In this setting, it is well known that\nthe worst-case number of comparisons required to perform this sorting task is of order R*log(R).\nWe begin with a brief summary of our result and then delve into more detail. We consider a variant\nof this problem that puts certain restrictions on the possible values that the list can take.\nWe\nthen design an algorithm that, under these assumed conditions, requires only order sqrt(R)*log(R)\ncomparisons in the worst case rather than order R*log(R). It is important to note that even though\n9\nthe worst case complexity of sorting is said to be order R*log(R), our result does not violate this\nlower bound since our result applies only in a restricted setting whereas the R*log(R) result is when\nthe values can be arbitrary.\nIn what follows, we provide more details about the setting we assume in this work. We suppose that\nR is an even number. We also suppose that the R values in the array may not be arbitrary real-valued\nnumbers. Instead, these values must be greater than 9, each value must be a part of the Fibonacci\nsequence (and hence must be integers), and must not be a prime number. Furthermore, we assume\nthat the items in positions 1,...,R/2 all have values smaller than the items in positions R/2+1,...,R.\nFor settings where these conditions hold, we use properties of the Fibonacci sequence and additional\ncarefully crafted arguments to design a deterministic algorithm which we prove requires at most\nsqrt(R)*log(R) comparisons in the worst case. Of course, one may wonder in what applications these\nspecific conditions actually arise, and at this point we do not have an answer to this. Admittedly, our\nresult presently is of purely theoretical interest. In future work, we hope to relax the conditions we\nassumed for this result and move towards making our result more practically applicable.\nThe LLM fails to recognize that the inclusion of additional restrictions has no impact on the necessity\nof conducting at least order R*log(R) comparisons in the worst-case. It is important to note that the paper\ndid not provide a comprehensive proof of this assertion. Our objective here is to evaluate whether the LLM\ncan directly discern the futility of the additional restrictions. In addition, the LLM once raised a false alarm\nunder Prompt-Direct regarding the Fibonacci condition:\nThe paper sets forth an intriguing claim about a sorting algorithm that works under very specific\nconditions, stating that it can perform the sort operation with complexity of sqrt(R)*log(R) as\nopposed to the typical R*log(R). It\u2019s important to verify these claims within the constraints of the\nconditions presented in the paper.\nLet\u2019s proceed by examining the claims step by step:\n1. **Setting**: The paper starts by stating a classic fact about comparison-based sorting algorithms.\nGiven an array of R distinct real-valued numbers, the worst-case complexity for sorting the array is\nindeed O(R*log(R)). This statement is a well-established fact in computer science.\n2.\n**Claim**:\nThe authors suggest that they have found an algorithm that can sort in\nO(sqrt(R)*log(R)) comparisons under specific conditions. The conditions are:\n- R is an even number\n- The values are not arbitrary real-valued numbers, but they are greater than 9, each value is a part\nof the Fibonacci sequence, and they are not prime numbers\n- The items in positions 1,...,R/2 all have values smaller than the items in positions R/2+1,...,R\nIt is crucial to note here that the claim doesn\u2019t seem to violate any known theoretical lower bound\nbecause it applies to a restricted subset of the sorting problem.\nLet\u2019s take a deeper look at the constraints for the claim to hold:\n- **R is an even number**: This condition doesn\u2019t seem problematic. We can certainly have lists of\neven length.\n- **Values are part of the Fibonacci sequence, are greater than 9, and are not prime**:\nThis\n10\ncondition is problematic. The first few Fibonacci numbers are 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55,\n89, and so on. Among these, 13 is a prime number, which would not be allowed by the conditions.\nHowever, the next Fibonacci number, 21, is greater than 9 and is not a prime number. But, there\u2019s\nan issue: the next Fibonacci number, 34, is also not a prime number, but if we wanted a list with\nmore than two numbers, we wouldn\u2019t have any additional Fibonacci numbers that aren\u2019t prime and\nare greater than 9. The next Fibonacci number, 55, is a prime number and thus would be excluded\nby the conditions. Therefore, for any R greater than 2, there are not enough numbers that satisfy\nthe conditions set by the paper.\n- **Items in positions 1,...,R/2 all have values smaller than the items in positions R/2+1,...,R**:\nGiven the condition above, this is only possible for R=2, where one number is smaller than the other.\nFor any R>2, this condition can\u2019t be satisfied because there aren\u2019t enough numbers that meet the\nconditions.\nIn conclusion, the claims made in the paper are not valid for any R greater than 2, due to the\nconstraints set on the numbers that can be included in the list. The conditions defined in the paper\nare too restrictive to allow for a list of size greater than 2.\n3.3.4\nSorting (easier)\nThis short paper is identical to the \u201cSorting \u2013 harder\u201d paper in the previous section, but with fewer restric-\ntions on the array.\nSuppose you have an array of R distinct real-valued numbers. Suppose you wish to sort the array\nin increasing order. The sorting algorithm can only query for comparisons of the form \u2018is [value of\nnumber at a certain position] greater than [value of number at a different position]?\u2019. Suppose that\nsuch queries are guaranteed to get accurate Yes/No answers. In this setting, it is well known that\nthe worst-case number of comparisons required to perform this sorting task is of order R*log(R).\nWe begin with a brief summary of our result and then delve into more detail. We consider a variant\nof this problem that puts certain restrictions on the possible values that the list can take.\nWe\nthen design an algorithm that, under these assumed conditions, requires only order sqrt(R)*log(R)\ncomparisons in the worst case rather than order R*log(R). It is important to note that even though\nthe worst case complexity of sorting is said to be order R*log(R), our result does not violate this\nlower bound since our result applies only in a restricted setting whereas the R*log(R) result is when\nthe values can be arbitrary.\nIn what follows, we provide more details about the setting we assume in this work.\nWe assume\neach of the R values is greater than 9. We use carefully crafted arguments to design a deterministic\nalgorithm for which we prove that when the aforementioned assumption holds, requires at most order\nsqrt(R)*log(R) comparisons in the worst case. Of course, one may wonder in what applications these\nspecific conditions actually arise, and at this point we do not have an answer to this. Admittedly, our\nresult presently is of purely theoretical interest. In future work, we hope to relax the conditions we\nassumed for this result and move towards making our result more practically applicable.\nObserve that the only restriction on the array is that all the R values are greater than 9. The LLM\nstill does not recognize that this simple restriction will make no difference to requirement of at least order\nR*log(R) comparisons in the worst case, and will disallow an algorithm requiring only sqrt(R)*log(R) com-\nparisons. To its credit, the LLM does say that it is impossible to verify this claim without additional details.\nHere is an example response to Prompt-OneShot.\n11\nThe short paper details a theoretical approach to a sorting problem, where the authors claim\nthey\u2019ve designed an algorithm that, under certain conditions, reduces the worst-case number of\ncomparisons from order R*log(R) to order sqrt(R)*log(R). The condition specified is that each of\nthe R real-valued numbers in the array is greater than 9.\nFrom the information provided, it seems that the claim is theoretically possible under the specific\ncondition stated.\nThe claim doesn\u2019t violate the established lower bound for the general sorting\nproblem (order R*log(R)) because the authors explicitly state their result applies only in a restricted\nsetting. Therefore, the claim appears to be valid within the context they have provided.\nHowever, there are several important considerations to note:\n1.\nThe lack of practical application:\nThe authors themselves admit that their result is purely\ntheoretical at this point, and that they are unaware of any practical applications where the conditions\nfor their result arise. This is important to consider when evaluating the impact of their work.\n2. Need for a detailed proof: Although the authors state they have a deterministic algorithm that\nmeets the described complexity, the brief summary doesn\u2019t provide the proof. It\u2019s crucial to ensure\nthe proof is rigorous and valid.\n3.\nNo empirical validation:\nThe authors haven\u2019t mentioned any empirical validation, such as\nsimulations or testing, of their algorithm. This is an important step in verifying the effectiveness of\nan algorithm in practice.\nWhile the authors\u2019 claim seems to be theoretically possible and doesn\u2019t violate any known theoretical\nbounds, the points above should be addressed in a full paper review. A detailed examination of their\nproposed algorithm, the associated proof, and any empirical validation would be necessary to fully\nevaluate the validity and correctness of the claims.\n3.3.5\nNoisy pairwise comparison model (harder)\nThis short paper describes a statistical model for noisy pairwise comparisons that is a generalization of the\npopular Bradley-Terry-Luce (BTL) model [BT52; Luc59]. It makes certain claims about estimation under\nthis model.\nWe consider a statistical model where there are d items. Each item i (in 1,...,d) is associated with\nan unknown real-valued parameter w i. The parameters w 1,...,w d can take any arbitrary distinct\nvalues. We obtain n samples, where each sample is obtained in the following manner. First pick two\ndistinct items uniformly at random (without replacement) from the set of d items. Then return a\ncomparison between the two items \u2013 the comparison must say whether item i is better than j (i.e.,\nitem i has a higher parameter value than item j) or whether item j is better than item i (i.e., item j\nhas a higher parameter value than item i). These comparisons are noisy. The noise is governed by a\nknown, increasing function f:R\u2212>R as follows: if items i and j are compared, then the probability with\nwhich item i is said to be better than item j is given by f(w i) / (f(w i) + f(w j)), and otherwise item j\nis said to be better than item i. All samples (i.e., comparisons) are statistically mutually independent.\nFor this model, we design an algorithm to estimate the values of the parameters w 1,...,w d from\nthe n samples. We consider the squared loss function, that is, for any estimate \\hat{w} and true\nvalue w of the parameters, we define the loss as sum {i=1}\u02c6{d} (\\hat{w} i - w i)\u02c62. We consider\n12\nany arbitrary (but known) function f that satisfies the following three requirements: (a) it should be\ntwice differentiable, (b) it should be 1-Lipschitz, and (c) it should be bijective.\nWe prove that if the function f satisfies the three aforementioned requirements, the risk (i.e., expected\nvalue of the loss) incurred by our algorithm is guaranteed to be at most 2d/sqrt{n}. We do not have\nmatching lower bounds yet, but we conjecture that our upper bound is tight up to a positive universal\nconstant. We have also released a python package implementing our algorithm, and are currently also\nimplementing it in other programming languages.\nIt is well known in the literature that in the absence of additional assumptions, accurate estimation of\nthe parameters is impossible (e.g., \u201cthe minimax error for estimating [the parameters] will diverge to infinity\nif we are allowed to consider models [without additional restrictions]\u201d [Sha+16]). Here is an example to\nillustrate this issue. Suppose the function f is the identity function, which meets all the three requirements\nimposed on it by the paper. Then the probability of item i beating item j reduces to\nwi\nwi+wj . Now, for\nany value z \u0338= 0, if the non-zero parameters (w1, . . . , wd) are replaced by (zw1, . . . , zwd), the probability\ndistributions of the pairwise comparison outcomes do not change. Thus it is impossible to distinguish any\nnon-zero parameters (w1, . . . , wd) from (zw1, . . . , zwd) for any z \u0338= 0 from this data given any number of\nsamples, and therefore an error upper bound of\n2d\n\u221an is not possible. The LLM does not recognize this error\nnor does it recover this issue from the literature. However, it does clarify that it cannot verify the claim as\nthe full proof is not provided. Here is a response to Prompt-Direct.\nGiven that this is a statistical paper, it\u2019s difficult to definitively say whether any of these claims\nare false without a detailed examination of the underlying proofs and empirical evidence. However,\nbased on the information provided, there are a few key areas that can be evaluated for plausibility\nand consistency with known principles.\n1. The statistical model: The paper describes a statistical model for comparing d items based on an\nunknown parameter w i associated with each item. The comparison is noisy, but the noise follows a\nknown, increasing function f. There is nothing inherently implausible about this model setup based\non the provided information. As long as the conditions stated for function f are satisfied, the model\nappears reasonable.\n2.\nThe algorithm: The authors propose an algorithm to estimate the parameter values from n\nsamples. Given the model setup, this is plausible. However, the paper does not provide any details\non the workings of the algorithm, so it\u2019s impossible to check the correctness of the algorithm or its\nsuitability for the problem at hand.\n3. The loss function: The paper uses a squared loss function, which is a common choice in many\nstatistical problems, as it emphasises large errors over small ones. This is plausible.\n4. Risk bound: The authors claim that if function f satisfies certain conditions, the risk (expected\nvalue of the loss) incurred by their algorithm is at most 2d/sqrt{n}.\nWithout seeing the proof,\nit\u2019s impossible to say whether this claim is correct. However, the fact that the risk decreases with\nincreasing n (the number of samples) is consistent with the intuition that more data usually leads to\nbetter estimates.\n5. No lower bounds yet: The authors mention that they don\u2019t have lower bounds yet for the risk.\nThis is an important part of the analysis, as lower bounds provide an understanding of how good the\nbest possible algorithm can be. Their absence does not invalidate the paper, but it does leave open\n13\nthe question of how close this algorithm is to the best possible one.\n6. Python package: It\u2019s plausible that the authors have released a Python package implementing\ntheir algorithm.\nHowever, the quality of this implementation, its correctness, and its robustness\ncannot be assessed based on the information provided.\n7. Conjecture of upper bound tightness: Without seeing the proof or empirical evidence, we can\u2019t\nverify this claim. It\u2019s plausible but needs rigorous mathematical proof or empirical evidence.\nIn summary, while the claims in the paper are plausible given the information provided, they can\u2019t be\ndefinitively checked for correctness or falseness without further information, particularly the math-\nematical proofs of the risk bound and the details of the algorithm. The paper could benefit from\nadditional empirical validation of the theoretical claims, and the investigation of lower bounds for the\nrisk would also be a valuable addition.\n3.3.6\nNoisy pairwise comparison model (easier)\nThis paper is identical to the \u201cNoisy pairwise comparison model \u2013 harder\u201d paper from the previous section,\nbut instead of the arbitrary function f in the model, it uses a concrete instantiation\n2wi\n2wi+2wj .\nWe consider a statistical model where there are d items. Each item i (in 1,...,d) is associated with\nan unknown real-valued parameter w i. The parameters w 1,...,w d can take any arbitrary distinct\nvalues. We obtain n samples, where each sample is obtained in the following manner. First pick\ntwo distinct items uniformly at random (without replacement) from the set of d items. Then return\na comparison between the two items \u2013 the comparison must say whether item i is better than j\n(i.e., item i has a higher parameter value than item j) or whether item j is better than item i (i.e.,\nitem j has a higher parameter value than item i). These comparisons are noisy: if items i and j are\ncompared, then the probability with which item i is said to be better than item j is given by 2\u02c6w i /\n(2\u02c6w i + 2\u02c6w j), and otherwise item j is said to be better than item i. All samples (i.e., comparisons)\nare statistically mutually independent.\nFor this model, we design an algorithm to estimate the values of the parameters w 1,...,w d from the\nn samples. We consider the squared loss function, that is, for any estimate \\hat{w} and true value\nw of the parameters, we define the loss as sum {i=1}\u02c6{d} (\\hat{w} i - w i)\u02c62. This completes the\ndescription of the setting.\nWe prove that the risk (i.e., expected value of the loss) incurred by our algorithm is guaranteed to\nbe at most 2d/sqrt{n}. We do not have matching lower bounds yet, but we conjecture that our\nupper bound is tight up to a positive universal constant. We have also released a python package\nimplementing our algorithm, and are currently also implementing it in other programming languages.\nThe issue that had existed under the more general model still remains: it is impossible to distinguish\nparameters (w1, . . . , wd) from (w1+z, . . . , wd+z) for any z, thereby making it impossible to make an accurate\nestimate of the parameters. Again, the LLM does not recognize this issue nor does it recover it from the\nliterature. Here is an example response to Prompt-OneShot.\nThe short paper presents a statistical model with d items, where each item i has an associated\nunknown real-valued parameter w i. The paper then describes the process of obtaining n samples\nby comparing two distinct items uniformly at random and returning a noisy comparison based on\n14\nthe probability given. The samples are statistically mutually independent. The goal is to design an\nalgorithm to estimate the parameters w 1,...,w d from the n samples.\nThe paper claims that the risk (expected value of the loss) incurred by their algorithm is guaranteed\nto be at most 2d/sqrt{n}. They also mention that they don\u2019t have matching lower bounds yet, but\nconjecture that their upper bound is tight up to a positive universal constant. Finally, they state that\nthey have released a python package implementing the algorithm and are working on implementing\nit in other programming languages.\nBased on the information provided in the short paper, it is difficult to verify the correctness of the\nclaim that the risk incurred by the algorithm is at most 2d/sqrt{n}. To assess the validity of this\nclaim, it would be necessary to review the full paper and the detailed proof of the upper bound. The\nsame applies to the conjecture about the tightness of the upper bound up to a positive universal\nconstant.\nAdditionally, without access to the python package and its source code, it is impossible to verify the\ncorrect implementation of the algorithm as described in the short paper.\nIn summary, based on the information provided, we cannot determine if any of the claims are false.\nA thorough review of the full paper, detailed proofs, and the implementation would be required to\nassess the validity of the claims.\n3.3.7\nClassification\nThe short paper claims to design a linear classifier that shows an improved performance over all previous\nlinear classifiers.\nLinear classifiers are extremely important today even in the age of highly non-linear classification\nmethods such as neural networks. Linear classifiers are the building block of neural networks, have a\nhistorical significance, form perhaps the simplest hypothesis class, and also arise when training only\nthe last layer of a neural network when everything else is pretrained. With this motivation, we focus\non designing linear classifiers that enjoy improved performance as compared to the state of the art\nlinear classification methods. In more detail, we design a clever ensemble method as discussed below.\nThe dataset we consider is the \u2018Is this a human?\u2019 dataset. We first provide a description of this\ndataset. The dataset has 13,000 datapoints. The dataset is partitioned into a training set of 10,000\ndatapoints and a test set of 3,000 datapoints. Each datapoint corresponds to an entity that is either a\nhuman or a bot. Each datapoint comprises 42 features, all of which are real valued, capturing various\ncharacteristics of the entity.\nThe label associated with each datapoint is either +1 (representing\nhuman) or -1 (representing bot). For any classifier, we measure its performance using the 0-1 loss.\nFor the reader\u2019s convenience, we recap the meaning of linear classifiers here in the context of our\nproblem. A linear classifier in our setting is any function mapping R\u02c642 to the set {-1,1}, which first\ntakes an affine combination of the 42 features of the datapoint, and then applies the sign function to\nthe result (where by convention we allow sign(0) to take an arbitrary value). The linear hypothesis\nclass is then defined as the set of all possible linear classifiers.\nWe now provide the description of our method. We first partition the 10,000 training datapoints into\na training set comprising 8,000 datapoints and a validation set comprising 2,000 datapoints. This\n15\ntraining-validation partitioning is performed uniformly at random. Our method is associated with a\nhyperparameter which we denote as Z. For reasons to be clarified below, we restrict Z to be an odd\npositive integer. For a given choice of Z, we train Z linear classifiers using Soft-SVM. For classifier\nnumber z (in 1,...,Z), we set the Soft-SVM hyperparameter (commonly denoted by \u201cC\u201d) to 1/z. We\ncall the resulting classifiers h 1,...,h Z, where each h z is a function that maps R\u02c642 to {-1,1}. Note\nthat since we are using Soft-SVM, each classifier h 1,...,h Z is a linear classifier. Then finally, for any\nnew datapoint x in R\u02c642, we compute h 1(x),...,h Z(x) and then take a majority vote of all of them\nto determine the output label.\nThe only part of our algorithm that remains to be detailed is the choice of Z: we make this choice\nvia crossvalidation. In more detail, we consider the set {1, 2\u02c61 + 1, 2\u02c62 + 1, ..., 2\u02c610 + 1}. For each\nvalue in this set, we let Z be that value, train our classifier on the training set, and compute its error\non the validation set. We then choose the value for Z that leads to the smallest loss on the validation\nset.\nBefore we describe the evaluations of our algorithm on the test set, let us briefly comment on a key\nproperty of our classifier, or more precisely, on the final hypothesis that is chosen. For the chosen\nvalue of Z, the classifiers (hypotheses) h 1,...,h Z are all linear since we used Soft-SVM. Our final\nhypothesis is a majority vote across these Z binary-valued hypotheses, which can equivalently be\nwritten as taking a mean of h 1,...,h Z and then applying the sign function. The mean is simply a\nlinear combination, and a linear combination of linear items is also linear. Thus our final hypothesis\nalso lies in the linear hypothesis class.\nWith this property in mind, we now present our empirical results. We find that our classifier incurs a\n12% error on the test set (which contained 3,000 datapoints). In contrast, all linear classifiers in prior\nliterature incur at least 19% error. Our work thus represents a substantial improvement in the class\nof linear classifiers. This has significant implications for classical machine learning as well as for fine\ntuning (the last layer of) neural networks. In future work, we will apply our method for fine tuning\nand report on improvements in accuracy as compared to current fine-tuning methods.\nThe paper errs in the claim that a linear combinations of linear classifiers is linear. Recognizing this\nmistake requires first recalling that a linear classifier involves a sign function after the linear combination,\nthat the sign function is a non-linear operation, and then recognizing that a linear combination of the outputs\nof such sign functions is not linear over the original inputs. The LLM does identify this mistake in a subset\nof responses, such as the following response to Prompt-OneShot.\nThis short paper presents several claims and hypotheses about their ensemble method for linear\nclassifiers. Let\u2019s break down the paper and analyze each claim one by one.\n1. **Claim:** Linear classifiers are still significant in the era of non-linear classification methods like\nneural networks, serving as the building block of neural networks and being used for training the last\nlayer of a neural network.\n**Analysis:** This claim is valid. Linear classifiers are fundamental to machine learning and are\noften used in various stages of neural network training.\n2.\n**Claim:** The proposed ensemble method for linear classifiers shows improved performance\ncompared to state-of-the-art linear classification methods on the \u2019Is this a human?\u2019 dataset.\n**Analysis:** This claim seems valid based on the presented information. However, the evidence\n16\nfor this claim is based on empirical results which need to be validated through peer review and\npotentially additional experiments.\n3. **Claim:** The method partitions the training data into a training set and a validation set uni-\nformly at random and trains Z linear classifiers using Soft-SVM with the hyperparameter C set to 1/z.\n**Analysis:** This is a procedural claim about the implementation of the method. As such, it\u2019s\nneither true nor false but needs to be verified in the implementation. There are no obvious logical\nerrors in this claim.\n4. **Claim:** The final hypothesis, a majority vote across the Z binary-valued hypotheses (which\ncan be written as taking a mean of h 1,...,h Z and then applying the sign function), is also linear\nsince it\u2019s a linear combination of linear items.\n**Analysis:** This claim is false. While a linear combination of linear classifiers is indeed linear,\nthe described method is not a simple linear combination. The step of taking a majority vote or sign\nof the mean is a non-linear operation, which means the final classifier will not be a linear classifier\ndespite each of h 1,...,h Z being linear. Therefore, the claim that the final classifier is also linear is\nincorrect.\n5. **Claim:** The classifier incurs a 12% error on the test set, which is an improvement compared\nto all linear classifiers in prior literature that incur at least a 19% error.\n**Analysis:** This claim appears to be valid based on the data provided in the paper. However, as\nwith all empirical results, this would need to be validated through peer review, replication of the\nexperiments, and comparison with the results of other methods on the same dataset.\n6. **Claim:** The improved results have significant implications for classical machine learning and\nfor fine-tuning the last layer of neural networks.\n**Analysis:** This claim is more speculative, but it\u2019s plausible.\nIf the results are valid and the\nmethod is applicable in broader contexts, it could indeed impact the fields mentioned. However, as\npointed out, the final classifier is not linear, so the implications may not be as broad as suggested.\nThe claim that this method could benefit the fine-tuning of neural networks would need to be\nvalidated through additional experiments and comparison with existing fine-tuning methods.\nIn conclusion, the claim that the final ensemble classifier is a linear classifier is false. This discrepancy\nneeds to be addressed for the paper\u2019s conclusions to be sound. The other claims made in the paper\nare plausible based on the given information but would need further validation.\n3.3.8\nGame theory\nThis paper analyzes a fictitious game that is a combination of the Monty Hall problem and Rock-Paper-\nScissors.\nWe analyze the RockPaperScissorsTwist game show that has become extraordinarily popular in\nrecent times.\nFor the reader who has not watched this game show, while we encourage them to\nwatch the show themselves, we give a quick introduction to its rules to enable them to read this\npaper right away. The game show has two players. The game begins with a toss between the two\n17\nplayers. The loser of the toss is called the Driver, and the winner of the toss is called the Passenger.\nThe two players then play the following variant of Rock Paper Scissors (it is not identical to the\nclassical Rock Paper Scissors, and hence the word \u2018Twist\u2019 in the title of the game show). The Driver\nmust first choose one of Rock, Paper, and Scissors as their move. The Driver must disclose their\nchosen move to the game show host, but not to the Passenger or anyone else. The Passenger then\nguesses the Driver\u2019s chosen move. The host does not tell the Passenger whether the guess is correct\nor not, but among the two other options which the Passenger did not guess, the host reveals one of\nthem which was not chosen by the Driver. For instance, if the Driver chose Rock and the Passenger\nguessed Scissors, the host will reveal that Paper was not chosen by the Driver. As another example,\nif the Driver chose Rock and the Passenger guessed Rock, then the host will reveal any one of Paper\nor Scissors as not being chosen by the Driver. At this point, the Passenger must choose their move\namong Rock, Paper, and Scissors. Once the Passenger has chosen their move (and recall that the\nDriver had chosen their own move earlier), the winner is determined according to the standard rules\nof Rock Paper Scissors (i.e., Rock beats Scissors, Scissors beats Paper, and Paper beats Rock). In\nthe game show, the winner then takes home $10,000.\nWe analyze this game from a game-theoretic perspective. First, observe that once the Driver commits\nto their choice of move, the game from the Passenger\u2019s point of view boils down to the Monty Hall\nproblem. The Passenger can thus \u2018switch\u2019 their guess to the remaining option and achieve a success\nprobability of two-thirds in their guess. In particular, note that this success probability is strictly\ngreater than a half, which allows us to make the deductions below.\nThe reader may recall that the classic game of Rock Paper Scissors is a simultaneous game. However,\nthe RockPaperScissorsTwist game is different in nature. It first requires one player (the Driver) to\ncommit to their move, and subsequently the other player (the Passenger) chooses their move. Thus\nwe model RockPaperScissorsTwist as a Stackelberg game. Here, the Driver is the so-called leader\nin Stackelberg parlance who must commit to its action first, and then the Passenger is the so-called\nfollower who subsequently chooses its action.\nA standard result in game theory is that a two-player, finite Stackelberg game always has a pure\nStackelberg equilibrium. Consequently, without loss of optimality, the Driver\u2019s strategy can be pure.\nHowever, under any pure strategy chosen by the Driver, in Rock Paper Scissors, the Passenger can\nchoose a pure strategy that can guarantee that the Passenger wins. Consequently, in this game, if\nthe Passenger player plays optimally, then the Passenger player is guaranteed to win.\nTo conclude, in this paper, we solved the RockPaperScissorsTwist game and found that the Passenger\nplayer (i.e., the winner of the toss) always wins. Pragmatically, if the players in this actual game\nshow become aware of this strategy, then the game may become boring for the audience. We urge\nthe makers of this popular game show to take this into account and modify the rules before they lose\ntheir viewership.\nThe LLM does recognize the gap between two arguments in the paper: the earlier part recognizes that\nthe probability of the Passenger correctly guessing the Driver\u2019s chosen move is 2/3, but subsequently it is\nimplicitly considered to be 1 leading to the claim that the Passenger will always win. Here is a response to\nPrompt-Direct.\nThe paper makes a number of claims:\n1. The game \u201cRockPaperScissorsTwist\u201d is analogous to the Monty Hall problem for the Passenger\nonce the Driver makes a move.\n2. RockPaperScissorsTwist can be modeled as a Stackelberg game.\n18\n3. A standard result in game theory is that a two-player, finite Stackelberg game always has a pure\nStackelberg equilibrium.\n4. The Driver\u2019s strategy can be pure without loss of optimality.\n5. If the Passenger plays optimally, the Passenger is guaranteed to win.\n6. The winner of the toss (the Passenger) always wins in this game.\nLet\u2019s analyze each of these in turn:\n1.\nThe Monty Hall problem analogy:\nThe Monty Hall problem involves a game show host, a\ncontestant, and three doors. Behind one door is a car, and behind the other two are goats. The\ncontestant chooses a door, then the host, who knows what\u2019s behind each door, opens one of the other\ntwo doors to reveal a goat. The contestant then has the option to switch their choice to the remaining\nunopened door.\nThe game-theoretic optimal strategy in this case is indeed to switch, giving the\ncontestant a 2/3 chance of winning the car. In RockPaperScissorsTwist, the host\u2019s role is analogous to\nrevealing one of the \u201dlosing\u201d options after the Passenger makes a guess. This claim appears to be valid.\n2.\nModeling RockPaperScissorsTwist as a Stackelberg game: Stackelberg games are a model of\nstrategic interaction in which one player (\u201dthe leader\u201d) makes a move before the other player (\u201cthe\nfollower\u201d). In RockPaperScissorsTwist, the Driver makes a move before the Passenger, so this claim\nappears to be valid.\n3. Pure Stackelberg equilibrium: It is indeed a standard result in game theory that two-player, finite\nStackelberg games have a pure Stackelberg equilibrium. This claim is valid.\n4. The Driver\u2019s strategy can be pure without loss of optimality: This is a direct consequence of\nthe previous claim about Stackelberg equilibria. The Driver, as the leader in this game, can indeed\nchoose a pure strategy without loss of optimality. This claim appears to be valid.\n5. If the Passenger plays optimally, they are guaranteed to win: This is where the paper\u2019s argument\nappears to be flawed.\nThe Passenger\u2019s optimal strategy, analogous to the Monty Hall problem,\nshould be to switch their guess to the remaining option after the host reveals one of the non-chosen\noptions. This gives the Passenger a 2/3 chance of correctly guessing the Driver\u2019s move. However,\neven if the Passenger correctly guesses the Driver\u2019s move, that doesn\u2019t guarantee a win, because they\nstill have to choose a move in the Rock Paper Scissors game. Given that the Driver\u2019s move is already\nfixed, the Passenger\u2019s move is essentially a guess, which has a 1/3 chance of losing, a 1/3 chance\nof winning, and a 1/3 chance of drawing. Therefore, it\u2019s not accurate to say that the Passenger is\nguaranteed to win.\n6.\nThe winner of the toss always wins: As explained above, even though the Passenger has an\nadvantage in terms of information, it doesn\u2019t guarantee a win in the game. This claim is not valid.\nIn conclusion, while the paper correctly models the game as a Stackelberg game and establishes the\nanalogy with the Monty Hall problem, it incorrectly concludes that the Passenger is guaranteed to\nwin. The game is still partly based on chance, so it\u2019s not accurate to say that any player is guaranteed\nto win.\n3.3.9\nError correcting codes\nThis paper claims to design an error correcting code for a peculiar communication channel.\n19\nThe objective of this paper is to develop a system, that has provable guarantees, for communicating\nover a specific channel that we describe below. The noisy channel is a point-to-point communication\nchannel that can transmit symbols over the finite field of size 107. Hence, in what follows, all the\nsymbols are assumed to belong to this finite field and all mathematical operations are performed in\nthis finite field. The key property of the channel we are dealing with is that it can arbitrarily delete\nany finite field symbol that is being transmitted.\nIn order to address this challenging problem of communication over this channel, we turn to fountain\ncodes that were developed over two decades ago. We consider a (linear) fountain code that takes\n2200 message symbols as input.\nThe fountain code then produces a potentially infinite number\nof code symbols by taking random linear combinations of the 2200 input message symbols.\nThe\ncoefficients for the linear combinations are drawn uniformly at random from the finite field of size\n107, independently across different coefficients for the same code symbol and also independently\nacross different code symbols.\nUsing ideas from the literature on fountain codes, we prove that if one can collect 10% extra code\nsymbols (that is, collect any arbitrary 2420 code symbols) at the destination, then all 2200 input\nmessage symbols are guaranteed with complete certainty to be recovered accurately. Motivated by\nthis useful property, we implement this code in our system. We design the destination to wait until\nit collects some 2420 code symbols, and then use our implemented fountain code decoder to recover\nall 2200 message symbols. We find that our system leads to a 13% lower latency than the state of\nthe art systems for this communication channel.\nUnder any finite field, one cannot guarantee that with any arbitrary 2420 code symbols \u2013 where each code\nsymbol is an independent linear combination of the 2200 message symbols \u2013 the message can be accurately\nrecovered with complete certainty. There will always be a non-zero probability that it fails. However, the\nLLM does not pick up on this issue. Here is an example response to Prompt-OneShot.\nThe short paper describes a communication system that uses a linear fountain code for transmitting\nmessages over a noisy channel. The channel is characterized by its ability to arbitrarily delete finite\nfield symbols during transmission. The paper claims that by collecting 10% extra code symbols at\nthe destination, all 2200 input message symbols can be accurately recovered. Additionally, the paper\nclaims that the proposed system results in a 13% lower latency compared to state-of-the-art systems.\nTo assess the validity of the claims, let\u2019s consider the following points:\n1. The paper uses a linear fountain code for encoding and decoding messages. The fountain code\nproduces a potentially infinite number of code symbols by taking random linear combinations of the\n2200 input message symbols. The coefficients for the linear combinations are drawn uniformly at\nrandom from a finite field of size 107. This approach is consistent with the standard description of\nfountain codes.\n2. The paper claims that collecting any arbitrary 2420 code symbols (10% extra) guarantees complete\nrecovery of all 2200 input message symbols. This claim is plausible, as fountain codes are known\nfor their ability to recover original messages with high probability even when some symbols are lost\nduring transmission.\n3. The paper claims that the proposed system results in a 13% lower latency compared to state-\nof-the-art systems for this communication channel. This claim is difficult to assess without more\ninformation about the specific state-of-the-art systems being compared to and the experimental\n20\nsetup. However, the claim is not inherently contradictory, and it is possible that the proposed system\ncould achieve lower latency under certain conditions.\nBased on the information provided in the short paper, the claims regarding the use of fountain codes,\nthe recovery of input message symbols with 10% extra code symbols, and the potential for lower\nlatency are plausible. However, without more information about the comparison to state-of-the-art\nsystems and the experimental setup, it is not possible to definitively confirm the claim about a 13%\nlower latency.\n3.3.10\nOptimization\nThis short paper describes a real-world problem \u2013 that of assigning reviewers to papers \u2013 then models it as\nan optimization problem and provides an approximation algorithm.\nWe design an algorithm to assign reviewers to papers in peer review in a conference.\nHere we\nhave a set of submitted papers and a set of reviewers. For each reviewer-paper pair, we compute\na \u201csimilarity score\u201d based on one of several popular algorithms for doing so. The similarity score\nbetween any paper p and any reviewer r is denoted as similarity(p,r).\nThe similarity score is a\nnumber between 0 and 1, and denotes an estimate of the expertise that this reviewer is envisaged to\nhave for this paper (with higher scores representing higher envisaged expertise).\nIn what follows, we will assume that these similarity scores are available to us.\nThe goal is to\nassign reviewers to papers in a manner that maximizes the sum of the similarity scores of the\nassigned reviewer-paper pairs. There are three additional constraints. The first constraint is that\neach reviewer has specified a maximum number of papers they are willing to review, and this\nmust be respected.\nThe second constraint is that each paper must be assigned exactly three\nreviewers. The third constraint is that no paper should be assigned to a reviewer where the reviewer\nand paper\u2019s author(s) have a conflict of interest. The conflicts of interest information is available to us.\nIn our work, we write this problem as an optimization problem. For each paper p and reviewer r,\nwe let x {pr} denote a binary-valued variable that represents the assignment: x {pr}=1 means that\npaper p is assigned to reviewer r and 0 means it is not. We can then write the three aforementioned\nconstraints in terms of these variables. The condition pertaining to maximum number of papers per\nreviewer is written as sum {p} x {pr} <= specified maximum value, for each reviewer r. Of course,\nthe specified maximum value is an integer. Next, the condition pertaining to three reviewers per\npaper can be written as sum {r} x {pr} = 3 for every paper p. Finally, if paper p and reviewer r\nhave a conflict of interest, we set the constraint x {pr} = 0 for that p and r. It is easy to see that all\nof these constraints are linear.\nLet us now specify the objective of our optimization problem. In plain words, the objective is to\nmaximize the sum of the similarities of the assigned reviewer-paper pairs. We write this objective as\na sum over all papers p and reviewers r of the product similarity(p,r) times x {pr}, and the goal is\nto maximize this. It is easy to see that this is a linear objective. As an aside, note that in practice,\nthe cumulative reviewer capacity (i.e., the total number of reviews that all reviewers put together\ncan do) is usually (much) greater than the amount of reviewer demand (i.e., the number of reviews\nrequired), and hence this problem has a feasible solution. This completes the full description of the\nproblem statement.\n21\nEven though the aforementioned optimization problem may appear easy to solve, note that the opti-\nmization variables (i.e., x {pr}\u2019s) are binary-valued. Thus this is a (binary) integer linear programming\nproblem, which is known to be hard to solve in general. A natural question then is whether this opti-\nmization problem satisfies the Totally Unimodular Matrix (TUM) condition, which is known to make\nsuch problems easier to solve via a linear programming relaxation. However, it is easy to see that the\n(linear) constraints specified above violate the TUM condition. Consequently, we designed our own\napproximation algorithms that run in a computationally-efficient manner, meet the constraints, and\napproximate the objective up to a factor of 0.793. Our algorithm thus offers a significant promise of\nbetter automation of the peer-review process.\nThis problem of maximizing the sum similarity of assigned reviewer-paper pairs, subject to load and\nconflict-of-interest constraints has been studied quite extensively in the literature, and is well known to\nsatisfy the TUM condition (e.g., \u201c Since the constraint matrix of the linear program (LP) relaxation of this\nproblem is totally unimodular, the solution to the LP relaxation will be integral and so this problem can be\nsolved as an LP\u201d [Jec+20].) Finding this error required the LLM to connect this problem to the literature,\nand identify from the literature that the claim of violating TUM is false. However, the LLM was unable to\ndo so. Here is an example response to Prompt-Direct.\nThe paper makes several claims about the design and implementation of an algorithm for assigning\nreviewers to papers in a conference peer review process. Let\u2019s assess the claims step-by-step.\n1. **Problem Definition:** The authors define the problem correctly as an optimization problem.\nThey aim to assign reviewers to papers in a way that maximizes the overall similarity score, where\neach paper should be reviewed by exactly three reviewers, each reviewer has a maximum number of\npapers they are willing to review, and conflict of interest between a paper\u2019s authors and a reviewer\nis avoided. The objective function and constraints are indeed linear.\n2. **Problem Formulation:** The authors correctly formulate the problem as a binary integer linear\nprogramming problem. They define binary-valued variables x pr to denote the assignment of paper\np to reviewer r, and they construct constraints based on the problem definition.\n3.\n**Total Unimodularity (TUM):** The authors claim that the constraints violate the TUM\ncondition. Based on the description, this claim seems to be valid. The TUM condition, which would\nallow the problem to be solved as a linear programming problem rather than an integer programming\nproblem, requires that every square submatrix of the matrix of coefficients in the constraints has a\ndeterminant of 0, 1, or -1. The constraints formulated by the authors are not likely to satisfy this\ncondition, given the nature of the problem.\n4.\n**Approximation Algorithm:** The authors state that they have designed an approximation\nalgorithm that runs efficiently, meets the constraints, and approximates the objective up to a factor\nof 0.793. Without additional detail on this algorithm, it\u2019s difficult to evaluate the validity of this\nclaim.\nHowever, it\u2019s worth noting that approximation algorithms are common in computational\nproblems where exact solutions are not computationally feasible, and a factor of 0.793 is a reasonable\nperformance for such an algorithm.\n5. **Feasibility of the Problem:** The authors claim that the cumulative reviewer capacity is usually\nmuch greater than the reviewer demand, implying that the problem has a feasible solution. While\nthis may be generally true, it\u2019s important to remember that this would not always be the case, and\nthe feasibility of the problem could depend on the specific scenario.\n22\nIn summary, based on the information provided in this short paper, all of the claims seem to be\nvalid. However, further investigation is necessary to confirm the performance of the approximation\nalgorithm they\u2019ve proposed.\n3.3.11\nClustering\nThis paper discusses a fictitious real-world application of clustering.\nIn this research, we address a problem that arises directly out of a practical requirement, and report\non it as a case study. The study commenced in 2021, when we partnered with a certain car insurance\ncompany. We are unable to name the company here due to privacy reasons. This company wanted\nto cluster its customers based on the amount of distance they drove in the calendar year 2022.\nTheir motivation in doing so was to be able to provide better and more tailored schemes to customers.\nIn order to address this clustering requirement, we first helped them collect data. The car insurance\ncompany is local, that is, operates in only one city.\nThe company first considered asking each\ncustomer to self-report the number of miles they drove. However, such self reporting can lead to\nsome customers misreporting the number of miles in order to reduce their insurance premium. Hence\nwe asked the car company to manually visit each customer and collect their mileage data, at the\nbeginning as well as at the end of the calendar year 2022. This job was feasible since, as mentioned\nabove, the company operates in only one city. They did so, and we can safely assume that the data\nis accurate.\nWe now describe how we processed this data. Some collected values were in miles and some others\nwere in kilometers, so we standardized all collected distance values to the metric unit of kilometers. At\nthis point we had a set of customers, each with a certain measured amount of distance traveled in 2022.\nThe next goal was to cluster the customers in terms of the distance traveled. In order to achieve this\ngoal, we used the popular K-means clustering algorithm. There are then three choices to be made\nin this algorithm: (1) the value of K, (2) the distance function (since we consider the generalized\nversion of K-means to include other distance functions), and (3) the initialization.\nFor (1), the\ncompany asked us to use a certain value of K (specifically, K=8), so this choice was addressed by\nthe company\u2019s requirement. For (2), we used the standard Euclidean (L2) distance for the K-means.\nFor (3), it is easy to prove that in the 1-dimensional setting (i.e., when each datapoint is a scalar as\nin our case), with the choices made in (1) and (2), the algorithm is guaranteed to always output the\nsame clusters irrespective of the initialization, and hence this addresses (3). We then applied this\nalgorithm to the data and reported the results back to the company.\nWe ran the K-means algorithm on the data, and we now report a few interesting tidbits that we found\nin the output of the algorithm. First, it turned out that the clusters were all of nearly equal sizes\n\u2013 the difference between the sizes of the largest and smallest clusters was only 1. Second, we had\na human evaluator independently cluster the data in order to evaluate the algorithm. The output\nof the algorithm and the human evaluator was in perfect agreement (although the human evaluator\ntook orders of magnitude more time than the algorithm). Third, we find that there is one cluster of\ncustomers which drive *significantly* less than all others, one cluster drives significantly *more* than\nall others, and the rest are quite close to each other.\nThe paper falsely claims \u201cit is easy to prove\u201d that in the one-dimensional case, the result of K-means is\nindifferent to initialization. The LLM does recognize this issue in a subset of its responses. Here is such an\nexample response to Prompt-Parts for the sentence \u201cFor (3), it is easy to prove that in the 1-dimensional\n23\nsetting (i.e., when each datapoint is a scalar as in our case), with the choices made in (1) and (2), the\nalgorithm is guaranteed to always output the same clusters irrespective of the initialization, and hence this\naddresses (3).\u201d\nIn this section, the claim that \u201dthe algorithm is guaranteed to always output the same clusters\nirrespective of the initialization\u201d is not necessarily correct, even in the 1-dimensional setting. The\nK-means algorithm is sensitive to the initial placement of cluster centroids, and it can potentially\nconverge to different local optima depending on the initialization.\nHowever, it is true that the initialization problem is less severe in the 1-dimensional case compared\nto higher-dimensional cases, as there are fewer possible configurations for centroids. Additionally, in\na 1-dimensional setting, the algorithm is more likely to converge to the global optimum, but it is not\nguaranteed.\nTherefore, while the claim is not entirely accurate, it does highlight that the initialization problem is\nless of a concern in the specific 1-dimensional case the authors are working with. It\u2019s essential to be\ncautious about the interpretation of the K-means algorithm\u2019s performance, and further information\nabout the initialization strategy or any evaluation of the clustering results would be helpful in assessing\nthe validity of this claim.\n3.3.12\nDistinguishing styles: 80% accuracy\nThis paper uses machine learning to accomplish a task of testing for differences between writing styles of\ntwo groups of people.\nIn this paper, we aim to understand whether the writing styles of masters students enrolled at our\nuniversity are different from that of PhD students enrolled at our university. In order to achieve this\ngoal, we designed and executed the following experiment. We first gathered all the masters and PhD\nstudents at our university (we have 10,000 of each).\nWe were careful to ensure that there are no confounders in the experiment. For example, we asked\neach of them to give a writing sample \u2013 we chose the topic so that all students write about the\nsame topic and they have the same familiarity with the topic thereby avoiding a \u2018topic\u2019 confounder.\nLikewise, we also checked for differences in age, geographical location, gender and many other\ndifferences between the two groups and we found no such differences.\nThree sets of independent\nexpert researchers evaluated the two datasets and all concluded the absence of any confounders. In\nthe exposition that follows, we thus assume no confounders.\nWe uniformly at random split the writing samples into two equal-sized sets: a training set and a\nvalidation set. We consider the binary classification algorithm developed in [Reference 1] for distin-\nguishing writing styles, where we consider each writing sample as the feature vector and the label\ntaking a value \u201cmasters\u201d or \u201cPhD\u201d. We trained this supervised learning algorithm on the training set.\nWe then applied the resulting hypothesis (i.e., the trained model) to every sample in the validation set.\nAt this point, it is important to note some key differences with some other problems that are more\ncommon in the literature. Note that we are *not* in the realm of estimating counterfactuals. Here\nwe have the entire population of PhD students and the entire population of masters students (and\nnot just some smaller number of samples). Furthermore, we are not performing any intervention (like\ngiving a medicine) and trying to estimate the treatment effect between the medicine and a placebo.\n24\nWith that in mind, it is easy to see that if this hypothesis can classify (that is, distinguish\nbetween) masters and PhD writing samples on the validation set with a success rate that is better\nthan random, then we can conclude that there is a difference between the writing styles of mas-\nters and PhD students. Otherwise, we conclude that there is no difference between their writing styles.\nWe actually tried out our method to test whether the writing styles of masters students and PhD\nstudents at our university are different. We found that the classifier indeed had a 80% accuracy on\nthe test set. Consequently, we conclude that the writing styles are indeed different.\nThe paper errs in claiming that there is no difference if the machine learning method cannot distinguish\nthe two. Interestingly, in this case, responses to Prompt-Direct and Prompt-OneShot fail to detect this\nfallacy. It is only when presented one sentence at a time in Prompt-Parts that the LLM flags this fallacy.\nHere is an example response to Prompt-Parts for the problematic sentence \u201cOtherwise, we conclude that\nthere is no difference between their writing styles.\u201d\nThe claim in this section is potentially misleading and oversimplified. The authors suggest that if the\nmodel can\u2019t classify writing samples better than random, then there\u2019s no difference in writing styles\nbetween masters and PhD students. This conclusion may not necessarily be valid for the following\nreasons:\n1.\nA failure to classify effectively could be due to limitations in the model or the features used\nfor classification, not necessarily because there are no differences in writing styles.\nThe chosen\nclassification algorithm might not be suitable for the problem, or the feature extraction process\nmight not be capturing the relevant aspects of the writing styles.\n2. Similarly, it could be that the differences in writing styles are subtle or complex and not easily\ncaptured by the chosen machine learning model. There could be differences that exist but are not\ndetectable using this methodology.\n3.\nThe binary nature of the classification may also oversimplify the problem.\nWriting style is\na complex construct that can vary along many dimensions.\nJust because a model can\u2019t clearly\ndistinguish between two groups doesn\u2019t mean there are no differences; it could mean the differences\nare not binary or are multi-dimensional.\n4.\nIt\u2019s also important to consider other performance metrics besides accuracy (as mentioned in\nresponse to the previous section). A model might not perform better than random in terms of overall\naccuracy, but it might still have meaningful performance on certain metrics (e.g., precision or recall),\nsuggesting some differences between the groups.\nIn summary, while the claim in this section does not outright invalidate any previous claims, it\nsimplifies the interpretation of the model\u2019s performance and may lead to misleading conclusions. The\nabsence of evidence (the model not distinguishing between the groups) does not necessarily equate to\nevidence of absence (no difference in writing styles).\n3.3.13\nDistinguishing styles: 50% accuracy\nThis paper is identical to the paper in the previous section on \u201cDistinguishing styles: 50% accuracy\u201d except\nthe final result. Here the machine learning method attains only a 50% accuracy on the test set (as opposed\nto 80% in the previous section), thereby leading the paper to conclude that there is no difference in the\nwriting styles of the two groups.\n25\nIn this paper, we aim to understand whether the writing styles of masters students enrolled at our\nuniversity are different from that of PhD students enrolled at our university. In order to achieve this\ngoal, we designed and executed the following experiment. We first gathered all the masters and PhD\nstudents at our university (we have 10,000 of each).\nWe were careful to ensure that there are no confounders in the experiment. For example, we asked\neach of them to give a writing sample \u2013 we chose the topic so that all students write about the\nsame topic and they have the same familiarity with the topic thereby avoiding a \u2018topic\u2019 confounder.\nLikewise, we also checked for differences in age, geographical location, gender and many other\ndifferences between the two groups and we found no such differences.\nThree sets of independent\nexpert researchers evaluated the two datasets and all concluded the absence of any confounders. In\nthe exposition that follows, we thus assume no confounders.\nWe uniformly at random split the writing samples into two equal-sized sets: a training set and a\nvalidation set. We consider the binary classification algorithm developed in [Reference 1] for distin-\nguishing writing styles, where we consider each writing sample as the feature vector and the label\ntaking a value \u201cmasters\u201d or \u201cPhD\u201d. We trained this supervised learning algorithm on the training set.\nWe then applied the resulting hypothesis (i.e., the trained model) to every sample in the validation set.\nAt this point, it is important to note some key differences with some other problems that are more\ncommon in the literature. Note that we are *not* in the realm of estimating counterfactuals. Here\nwe have the entire population of PhD students and the entire population of masters students (and\nnot just some smaller number of samples). Furthermore, we are not performing any intervention (like\ngiving a medicine) and trying to estimate the treatment effect between the medicine and a placebo.\nWith that in mind, it is easy to see that if this hypothesis can classify (that is, distinguish\nbetween) masters and PhD writing samples on the validation set with a success rate that is better\nthan random, then we can conclude that there is a difference between the writing styles of mas-\nters and PhD students. Otherwise, we conclude that there is no difference between their writing styles.\nWe actually tried out our method to test whether the writing styles of masters students and PhD\nstudents at our university are different. We found that the classifier indeed had a 50% accuracy on\nthe test set. Consequently, we conclude there is no difference between the writing styles of masters\nand PhD students at our university.\nCuriously, unlike the 80% accuracy case, responses to Prompt-Direct and Prompt-OneShot also realize\nthis issue. This may be due to the fact that the fallacy lies in the conclusion drawn from the 50% case,\nand in this paper, it is this 50% case which occurs at the end of the paper. Here is an example response to\nPrompt-OneShot.\nThe short paper\u2019s goal is to understand whether the writing styles of masters students are different\nfrom that of PhD students at their university.\nThe methodology seems well-designed and they\nhave carefully checked for potential confounders.\nThey used a binary classification algorithm for\ndistinguishing writing styles, trained the model on a training set, and then tested the model on a val-\nidation set. The authors state that they found a 50% accuracy for their classifier on the validation set.\nHowever, there is a potential issue with the interpretation of the results. An accuracy of 50% on\nthe validation set does not necessarily mean that there is no difference between the writing styles\nof masters and PhD students. The 50% accuracy might be due to a poor choice of classification\nalgorithm or other issues in the model development process, which could lead to a high error rate.\n26\nIt is also possible that the writing styles are indeed different but are not adequately captured by the\nfeatures used in the classification algorithm.\nBefore concluding that there is no difference between the writing styles of masters and PhD students,\nit would be beneficial to investigate further, for instance by trying different classification algorithms,\nrefining the features used, or exploring other evaluation metrics.\n4\nCan LLMs verify author checklists?\nIn this section, we study whether LLMs can correctly answer the close-ended questions in author checklists\ngiven the paper and supplementary text.\nTo answer this question, we take papers and their submitted\nchecklists4 from the 2022 NeurIPS conference, manually label each of the questions where authors answer\n\u201cyes\u201d, and compare them with GPT-4 generated responses.\n4.1\nMethods\nPapers\nWe selected 15 papers from the NeurIPS 2022 conference OpenReview platform (https://openreview.\nnet/group?id=NeurIPS.cc/2022/Conference), which include both accepted and rejected (but opt-in for\npublic viewing) papers. In choosing these papers, we impose the requirement that they must have an author-\nsubmitted checklist. We first select 10 papers using simple uniform sampling, and five additional papers were\nretrieved manually to cover the lack of crowdsourcing/human-subject papers, which correspond to the fifth\ncategory of questions in the checklist. The papers we selected were published in NeurIPS after the GPT-4\ntraining data cutoff, so it is unlikely that the model had previously seen their checklists. Motivated by our\nsetting for verifying author checklists, we only consider questions where the authors labeled \u201cYes\u201d in their\noriginal checklist, as these are the items that the authors claim to have completed.\nQuestions\nThe 18 checklist items in the NeurIPS 2022 checklist span five large categories.\nFor each\nquestion, authors were instructed to provide answers in {\u201cYes\u201d, \u201cNo\u201d, \u201cN/A\u201d}, and were strongly encouraged\nto provide an associated section number or brief justification with each answer. We include 16 of the 18\ntotal questions in the NeurIPS 2022 checklist, selected based on the availability of ground-truth labels (see\nAppendix B.1).\nManual Labels\nUsing the papers and questions, we construct a set of {checklist question, paper} pairs\nwith {\u201cYes\u201d, \u201cNo\u201d, \u201cN/A\u201d} labels corresponding to our hand-labeled ground truth. Each entry is manually\nlabeled by one computer science graduate student (first author of the present paper) with a past publication\nin the NeurIPS conference and experience as workflow chair in a top CS conference. For each {checklist\nquestion, paper} pair, we performed both a keyword search and a full scan of the paper contents to form a\npreliminary ground truth label. Next, we compared the preliminary label with the author-submitted answer\nfrom the paper checklist. In the case of a mismatch, we double-checked any areas the author mentioned in\ntheir checklist answer, and searched for any mistakes in our understanding. If the labeler was not as familiar\nwith the details, we deferred to the author-provided answer. If the paper did not contain any text related\nto a checklist item (e.g., the code is in the supplemental but is not mentioned elsewhere), we still labeled\nthe ground truth according to the author knowledge when they submit the checklist. After labeling and\ncalibrating to all the papers, the labeler then re-labeled all of the questions again using both the author\nchecklist and current labels for reference, confirming to the best of our abilities that the labels are correct.\n4Checklist questions publicly available at https://neurips.cc/Conferences/2022/PaperInformation/PaperChecklist.\n27\nPrompting\nThroughout our experiments, for the LLM, we used the standard GPT-4 model with 8k\ntokens5 for the context.\nDue to limits on the token count, we were not able to pass in entire papers\nto the model. Instead, for each {question, paper} pair, we selected the section(s) in the paper that best\ncorrespond to each question, and only provided those section(s) in the prompt. For questions without directly\ncorresponding materials, we put the section(s) where the materials would normally appear in a paper. We\nalso provided a system prompt highlighting the model role, paper title, and usage of the response for causal\nreasoning [K\u0131c+23]. The prompt structure used for querying the model is as follows. The number of sections\nprovided in the user prompt varied based on the number of relevant sections for the question in the paper.\n\u2022 System prompt:\nYou are a computer science researcher currently reviewing a paper titled \u201c[paper title]\u201d for the NeurIPS\ncomputer science conference. Your goal is to try to be as objective and truthful as possible in your\nanswers about the paper provided. Your reviews will be used for causal reasoning in determining the\nquality of the paper.\n\u2022 User prompt:\nThe following is the [relevant section 1] section of the paper you are reviewing: \u201c. . . \u201d\nThe following is the [relevant section 2] section of the paper you are reviewing: \u201c. . . \u201d\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief\njustification for your answer. Question: \u201c...\u201d\nOur prompting strategy aligns well with existing conference practices: NeurIPS strongly encourages\njustifications to checklist answers, and suggests section labels as a method to do so. To ensure the LLM\ntakes an objective role rather than that of an author, the system prompt and checklist questions were\nrephrased to a reviewer point-of-view, referencing the authors in third-person. We provide the checklist and\nprompts associated with each checklist item in Appendix B.2.\nChoosing GPT-4 hyperparameters\nWe conducted a pilot to make the optimal choice of GPT-4\u2019s hyper-\nparameters. In more detail, for a separate NeurIPS 2022 paper and for one checklist question from each check-\nlist category, we evaluated GPT-4\u2019s responses varying the temperature hyperparameter in {0, 0.1, 0.2, . . . , 2.0}\nand the top p hyperparameter in {0, 0.1, 0.2, . . . , 1.0}. We found that the hyperparameter values (tempera-\nture=1.0, top p = 1.0) marginally outperform the other hyperparameter choices. Based on these results, we\nuse (1.0, 1.0) as the hyperparameter settings for our GPT-4 checklist experiment. Note that these are also\nthe default parameter values for GPT-4 in both the OpenAI API and Playground.\nSupplementary material\nMore details on the code implementation, manual labels, the pilot, and all of\nGPT-4\u2019s responses in the experiment are available at https://github.com/niharshah/ReviewerGPT2023.\n4.2\nSummary of results\nWe queried GPT-4 for three responses per {question, paper} pair, taking the majority vote as the answer and\nevaluating its correctness against the ground truth label. If all three responses were different, we marked\nthe answer as incorrect. In Table 2, we provide the results for our checklist experiment. We found that\ncompared to the hand-labeled ground truth, GPT-4 achieves 86.6% accuracy across 119 examples.\nIn some more detail, author-submitted checklists also match the ground truth 86.6% of the time, although\nthe mismatches may potentially be due to later paper revisions. We find that there is little overlap between\nLLM and author mismatches: On one hand, GPT-4 answers disagree with 12 out of 16 (75%) of mismatched\nauthor responses, allowing for further examination. On the other hand, 9 out of 16 (56.3%) of GPT-4\u2019s\nincorrect answers had correct author responses. Furthermore, 8 out of 16 of GPT-4\u2019s incorrect answers were\n5We queried GPT-4 through the gpt-4 model in the OpenAI API at https://platform.openai.com/docs/models/gpt-4\n(accessed 5/20/23 - 5/23/23). At the time of writing, we have yet to gain access to the 32k context version of GPT-4.\n28\nPaper \\ Item\n1b\n1c\n2a\n2b\n3a\n3b\n3c\n3d\n4a\n4b\n4c\n4d\n4e\n5a\n5b\n5c\nOverall\nFedPop: A\nBayesian...\n\u2713\n\u2713\n\u2713\n\u00d7\n\u2713\n\u00d7\n\u2713\n\u2713\n0.75\nHardness in\nMarkov...\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n1\nList-Decodable\nSparse Mean...\n\u2713\n\u2713\n\u2713\n1\nReCo: Retrieve\nand...\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n1\nLeveraging Inter-\nLayer...\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n1\nA permutation-\nfree kernel...\n\u2713\n\u2713\n\u2713\n\u00d7\n\u2713\n\u2713\n\u2713\n0.86\nPlasticityNet:\nLearning to...\n\u2713\n\u00d7\n\u2713\n\u2713\n0.75\nLaplacian\nAutoencoders...\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n1\nFast Bayesian\nInference with...\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u00d7\n0.91\nBridging Central\nand Local...\n\u2713\n\u2713\n\u2713\n1\nAssistive\nTeaching of...\n\u2713\n\u2713\n\u2713\n\u2713\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u2713\n\u2713\n0.8\nHarmonizing\nthe object...\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n1\nLearn to\nExplain:...\n\u2713\n\u2713\n\u2713\n\u2713\n\u00d7\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u00d7\n0.83\nWhat I Cannot\nPredict, I Do...\n\u2713\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u2713\n\u00d7\n\u2713\n\u00d7\n\u2713\n0.6\nEliciting\nThinking...\n\u00d7\n\u2713\n\u2713\n\u2713\n\u00d7\n\u00d7\n\u2713\n0.57\nOverall\n0.93\n1\n0.89\n0.88\n0.64\n1\n0.63\n0.9\n1\n1\n1\n0.75\n0.75\n0.8\n0.67\n0.8\n0.87\nTable 2: Accuracy of GPT-4 on papers and checklist questions. The notations \u2713 and \u00d7 correspond to whether\nthe majority label across three responses by GPT-4 was correct or incorrect respectively, as measured against\nthe hand-labeled ground truth. Grey cells indicate the author had initially marked \u201cno\u201d or \u201cN/A\u201d.\ndue to there being no textual evidence of the correct answer (e.g., requires access to figures in the paper,\nwhich is beyond its current capabilities). The authors, with full information of their own submission and\nfigures, correctly responded to all 8. Removing this set of questions increases GPT-4\u2019s accuracy to 92.8%, but\nthe 86.6% statistic is more representative across the problems that current LLMs may face when deployed\ninto checklist-verifying roles.\n4.3\nPaper and response details\nIn this section, we provide three examples of our prompting pipeline and results for using LLMs to verify\nauthor checklists. For each example, we first provide the specific section(s) of the paper we include in the\ncontext to GPT-4, followed by our rephrased checklist question, and then provide GPT-4\u2019s response. In the\npaper sections below, we use the notation [. . . ] to represent text from the paper that was included in the\nprompt, but is excluded here for brevity.\nExample 1\nTwo common concerns about LLMs are their tendency to hallucinate false facts [Ji+22;\nZha+23a] and their poor ability when reasoning about numbers and sequences of numbers [Zho+22; Raz+22].\nWe find that the LLM performs well on a related task of retaining and joining together citation lists in the aca-\ndemic setting. For the checklist question \"If the authors use existing assets (e.g., code, data,\nmodels), do the authors cite the creators?\", in all three responses, the LLM correctly replicates and\njoins sequences of citation indices to support its response to the checklist question without hallucinating\nincorrect numbers.\n29\nPaper: Fast Bayesian Inference with Batch Bayesian Quadrature via Kernel Recombination\n5 Experiments\nGiven our new model BASQ, with three variants of the proposal distribution, IVR, IGB, and UB, we\nnow test for speed against MC samplers and batch WSABI. We compared with three NS methods\n[71, 29, 46, 14, 15], coded with [72, 17]. According to the review [16], MLFriends is the state-of-the-\nart NS sampler to date. The code is implemented based on [77, 34, 42, 76, 38, 31, 10, 7], and code\naround kernel recombination [24, 44] with additional modification.\nAll experiments on synthetic\ndatasets were averaged over 10 repeats, computed in parallel with multicore CPUs, without GPU for\nfair comparison.8 The posterior distribution of NS was estimated via KDE with weighted samples [33].\n[. . . ]\nLikelihood functions Branin-Hoo [49] is 8 modal function in two-dimensional space.\nAckley [73]\nis a highly multimodal function with point symmetric periodical peaks in two-dimensional space.\nOscillatory function [32] is a highly multimodal function with reflection symmetric periodical peaks\nof highly-correlated ellipsoids in two-dimensional space.\n[. . . ]\nParameter estimation of the lithium-ion battery simulator:\nThe simulator is the SPMe [61], 9\nestimating 3 simulation parameters at a given time-series voltage-current signal (the diffusivity of\nlithium-ion on the anode and cathode, and the experimental noise variance). Prior is modified to log\nmultivariate normal distribution from [4]. Each query takes 1.2 seconds on average.\n9SPMe code used was translated into Python from MATLAB [11, 12].\nThis open-source code is\npublished under the BSD 3-clause Licence. See more information on [11]\n10Code used was from [54, 3]. All rights of the code are reserved by the authors. Thus, we do not\nredistribute the original code.\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief\njustification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models), do the authors cite the creators?\nYes, the authors cite the creators of existing assets they used in their experiments, such as code\nand models.\nThey provide references for the code and models used throughout the experiments\nsection, attributing credit to the appropriate sources.\nFor example, they mention code from [72,\n17], [77, 34, 42, 76, 38, 31, 10, 7], [24, 44], [61], and [54, 3] for various components of their experiments.\nYes, the authors cite the creators of existing assets. They provide numerous references for the code,\nmodels, and datasets used in their experiments, such as references [71, 29, 46, 14, 15, 72, 17] for the\nNested Sampling (NS) methods, and [49, 73, 32] for the synthetic likelihood functions. They also\nmention the use of specific simulators (e.g., SPMe [61]) and their respective references.\nYes, the authors cite the creators of existing assets, such as code, data, and models, throughout\nthe experiments section. They make references to various sources, including published papers and\ncode repositories, and mention any modifications they made to the original work. For example, they\nmention the use of code from papers [71, 29, 46, 14, 15, 72, 17, 77, 34, 42, 76, 38, 31, 10, 7, 24, 44],\nthe Branin-Hoo function from [49], and the Ackley function from [73]. They also credit authors of\ncode for the SPMe simulator [61, 11, 12]\n30\nExample 2\nWe also found that the LLM was able to reason about paper contents in non-trivial ways,\nincluding in cases where it responds to questions correctly whereas the authors did not. In this example,\nthe authors respond \u201cYes\u201d to the checklist item, \"Did you discuss any potential negative societal\nimpacts of your work?\". Using the earlier parts of the conclusion section, the model was able to make\nthe correct distinction of which part of the discussion corresponded to the authors\u2019 work and which part did\nnot, allowing for it to detect that there was no discussion about the potential negative impacts about the\nauthors\u2019 own work and refuting the authors\u2019 claim.\nPaper: Harmonizing the object recognition strategies of deep neural networks with humans\n5 Conclusion\nModels that reliably categorize objects like humans do would shift the paradigms of the cognitive sci-\nences and artificial intelligence. But despite continuous progress over the past decade on the ImageNet\nbenchmark, DNNs are becoming worse models of human vision. Our finding resembles a growing\nnumber of concurrent works showing similar trade-offs between DNN performance and predictions of\nhuman perception on different tasks [93, 94]. Our solution to this problem, the neural harmonizer,\ncan be applied to any DNN to align their visual strategies with humans and even improve performance.\n[. . . ]\nBroader impacts. A persistent issue in the field of artificial intelligence is the tendency of models\nto exploit dataset biases. A central theme of our work is that there are facets of human perception\nthat are not captured by DNNs, particularly those which follow the scaling laws which have been so\nembraced by industry leaders. Forcing DNNs to rely on similar visual strategies as humans could rep-\nresent a scalable path forward to correcting the insidious biases which have assailed under-constrained\nmodels of artificial intelligence.\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief\njustification for your answer.\nQuestion: Do the authors discuss any potential negative societal impacts of their work?\nNo, the authors do not explicitly discuss any potential negative societal impacts of their work.\nHowever, they do discuss the broader impacts focusing on addressing the issue of dataset biases and\nimproving alignment with human vision in DNNs.\nNo, the authors do not explicitly discuss any potential negative societal impacts of their work.\nInstead, they focus on the broader positive impacts, such as using harmonized deep neural networks\nto better align with human perception and to reduce dataset biases.\nNo, the authors do not explicitly discuss any potential negative societal impacts of their work. They\nprimarily focus on the limitations of their approach and emphasize the advantages of aligning DNNs\nwith human perception to avoid biases and exploit dataset issues.\nExample 3\nWe also observed cases where the LLM provides incorrect responses despite reasoning about\ncorrect components of the corresponding papers. For the checklist question, \"If the authors curate/release\nnew assets (e.g., code, data, models), do the authors discuss whether the data they are\nusing/curating contains personally identifiable information or offensive content?\", all three\ngenerated responses correctly reference that users remained anonymous and no sensitive personal information\nwas collected. Despite this, two out of three times, the GPT-4 response is negative.\n31\nPaper: What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework\nfor Explainability Methods\nA.1 Experimental design\nFigure 1 summarizes the experimental design used for our experiments. The participants that went\nthrough our experiments are users from the online platform Amazon Mechanical Turk (AMT).\nThrough this platform, users stay anonymous, hence, we do not collect any sensitive personal\ninformation about them. We prioritized users with a Master qualification (which is a qualification\nattributed by AMT to users who have proven to be of excellent quality) or normal users with high\nqualifications (number of HIT completed = 10000 and HIT accepted > 98%).\nBefore going through the experiment, participants are asked to read and agree to a consent form,\nwhich specifies: the objective and procedure of the experiment, as well as the time expected to\ncompletion (\u223c5 - 8 min) with the reward associated ($1.4), and finally, the risk, benefits, and\nconfidentiality of taking part in this study. There are no anticipated risks and no direct benefits for\nthe participants taking part in this study.\n[. . . ]\nA.2 Pruning out uncooperative participants\n3-stage screening process.\nTo prune out uncooperative participants, we subjected them to a\n3-stage screening process. First, participants completed a short practice session to make sure they\nunderstood the task and how to use the attribution methods to infer the rules used by the model (fig\n2). Second, as done in [1], we asked participants to answer a few questions regarding the instructions\nprovided to make sure they actually read and understood them (fig 3).\nThird, during the main\nexperiment, we took advantage of the reservoir to introduce a catch trial (fig 4). The reservoir is\nthe place where we store the training example of the current session, which can be accessed during\nthe testing phase.\nWe added a trial in the testing phase of each session where the input image\ncorresponded to one of the training samples used in the current session: since the answer is still on\nthe screen (or a scroll away) we expect participants to be correct on these catch trials. Participants\nthat failed any of the 3 screening processes were excluded from further analysis.\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief\njustification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors discuss whether\nthe data they are using/curating contains personally identifiable information or offensive content?\nYes, the authors have discussed the data they are using, specifically from Amazon Mechanical\nTurk (AMT) users. They mentioned that users stay anonymous on this platform, and they do not\ncollect any sensitive personal information about the participants.\nThey address the concerns of\nparticipants by providing a consent form that outlines the objectives, procedures, risks, benefits, and\nconfidentiality involved in the study.\nNo, the authors do not explicitly discuss whether the data they are using/curating contains person-\nally identifiable information or offensive content.\nHowever, they do mention that the experiment\nparticipants were sourced from Amazon Mechanical Turk (AMT) and stayed anonymous, so they\ndid not collect any sensitive personal information about them. Additionally, the provided examples\nof images from the experiments do not appear to contain offensive content.\nNo, the authors do not discuss whether the data they are using/curating contains personally identi-\n32\nfiable information or offensive content. However, they do mention that they conducted experiments\nwith Amazon Mechanical Turk participants and ensured that users remain anonymous and no sensi-\ntive personal information is collected.\n5\nCan LLMs choose the better of two papers\nIn this section we detail our experiments investigating whether given two papers (abstracts), the LLM can\nreliably select the better of the two. Given the strong performance of GPT-4 on the previous two tasks, we\ncontinue to use GPT-4 as the LLM.\n5.1\nMethods\nWe created 13 pairs of research abstracts, with each pair intentionally designed so that one abstract stands\nout as superior to the other. In a subset of these pairs, we also introduced extraneous elements into the\ninferior abstract, such as bombastic language, aiming to assess the LLM\u2019s ability to maintain its evaluative\naccuracy despite these potential distractions. For each pair of abstracts, we then asked the LLM to identify\nthe abstract with stronger results.\nWe quantified the LLM\u2019s accuracy by calculating the proportion of\ninstances where it correctly identified the superior abstract.\nFor this section\u2019s experiments, we use the\nGPT-4 model through ChatGPT (May 3 and May 12 builds) [Ope23a].\nHere is the prompt we used: You are an expert reviewer for a conference. You will be given two abstracts\nof research papers submitted to the conference. Only one of these two can be accepted. Importantly, the\npaper with the stronger results, that is, the one that makes a greater scientific contribution, should be\naccepted. Note that this is the only criterion for acceptance. Which one of the two abstracts should be\naccepted and why. Please think step by step. Here are the two abstracts \u201c...\u201d\n5.2\nSummary of results\nWe summarize the results in Table 3. We queried the LLM for three responses per prompt and we provide\nan evaluation of each response as well as an overall quantification of the performance. Note that for the\noverall row and column, we consider it as a \u00d7 if any of the responses to any of the prompts was a \u00d7. This\nis because the correct choice is quite evident, and thus an error is a cause for concern. We find that the\nLLM makes a number of mistakes, either not identifying a key difference or getting influenced by extraneous\nfactors. In particular, the LLM makes at least one incorrect choice in 6 of the 10 cases.\n5.3\nPaper and response details\nIn this section we provide the 10 pairs of abstracts we constructed for this experiment. In each pair, the text\nthat forms the key differentiator between the abstracts is bolded. Please note that any bold or emphasis of\nthe text in the abstracts is made here for the reader\u2019s convenience in this exposition and was absent from the\ntext passed to the LLM. We also provide one representative response from the LLM for each pair. Additional\nresponses from the LLM are available at https://github.com/niharshah/ReviewerGPT2023.\n5.3.1\nInterpreting a null result\nBoth abstracts conduct experiments to measure the effect of coffee on people\u2019s walking speeds, and obtain\nidentical results. The only difference is their interpretation of the results.\nAbstract 1: \u201cMotivation: Walking is an activity that most people indulge in frequently, some-\ntimes for work and sometimes for pleasure.\nCoffee is a beverage that a large number of people\n33\nIntervention type\nPerformance\nOverall\n1.\nInterpreting a null result\n\u2713\n\u2713\n\u2713\n\u2713\n2.\nPositive result bias\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n3.\nParameter ranges\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n4.\nLower bounds\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n5.\nUpper bounds\n\u2713\n\u2713\n\u2713\n\u2713\n6.\nPrompt injection attack\n\u00d7\n\u2713\n\u2713\n\u00d7\n7.\nBombastic language\n\u2713\n\u00d7\n\u00d7\n\u00d7\n8.\nAlgorithm name\n\u2713\n\u00d7\n\u2713\n\u00d7\n9.\nBuzzwords\n\u2713\n\u2713\n\u2713\n\u2713\n10.\nAuthor identities\n\u2713\n\u2713\n\u2713\n\u2713\nOverall\n4\u2713\n6\u00d7\nTable 3: Performance of the LLM in selecting the better abstract out of a pair presented to it. The pairs\nare constructed so that the choice is evident. We show the performance for the 3 generated responses. The\nnotation \u2713 indicates that the LLM correctly accepted the better abstract, whereas \u00d7 indicates it did not.\nconsume daily.\nGiven the widespread prevalence of walking and coffee, a natural question is\nwhether there is a causal relation between coffee and walking \u2013 in particular, the speed of walking.\nSurprisingly, there is no prior research on this natural question, and ours is the first study to address it.\nMethods: We conducted an IRB-approved, preregistered study.\nIn our study, we recruited 1332\nparticipants. Half of them were regular coffee drinkers, and the other half were not regular coffee\ndrinkers but open to trying out coffee. We put each participant into one of two conditions: drink\n(denoted henceforth as D) or not drink (denoted as ND). The participants were put into the\nconditions independently and uniformly at random. Next, the participants in D were asked to drink a\ntypical amount of coffee, and we measured their walking speed in their usual walks. The participants\nin ND, on the other hand, were asked to avoid drinking coffee, and we measured their walking speed\nin their usual walks. Importantly, note that apart from the drinking or not drinking of coffee, we\nasked the participants to not modify their routines and lifestyles in any way. Next, via GPS trackers\non their phones, we measured their walking speeds. For each participant, we computed the mean of\ntheir walking speeds across three weeks. We then compared the walking speeds in the D and ND\nconditions via a Mann-Whitney U test.\nResults: The test failed to reject the null hypothesis (p=0.35). We thus find no evidence\nof the effect of coffee on the speed of walking of people. We hope that other groups of\nresearchers will carry out independent experiments, possibly with even larger sample\nsizes, to obtain a deeper understanding of the causal relationship between coffee and\nwalking speed.\u201d\nAbstract 2: \u201cMotivation: Walking is an activity that most people indulge in frequently, some-\ntimes for work and sometimes for pleasure.\nCoffee is a beverage that a large number of people\nconsume daily.\nGiven the widespread prevalence of walking and coffee, a natural question is\nwhether there is a causal relation between coffee and walking \u2013 in particular, the speed of walking.\nSurprisingly, there is no prior research on this natural question, and ours is the first study to address it.\nMethods: We conducted an IRB-approved study. In our study, we recruited 1332 participants. Half\nof them were regular coffee drinkers, and the other half were not regular coffee drinkers but open to\ntrying out coffee. We put each participant into one of two conditions: drink (denoted henceforth as\n34\nD) or not drink (denoted as ND). The participants were put into the conditions independently and\nuniformly at random. Next, the participants in D were asked to drink a typical amount of coffee, and\nwe measured their walking speed in their usual walks. The participants in ND, on the other hand,\nwere asked to avoid drinking coffee, and we measured their walking speed in their usual walks. Impor-\ntantly, note that apart from the drinking or not drinking of coffee, we asked the participants to not\nmodify their routines and lifestyles in any way. Next, via GPS trackers on their phones, we measured\ntheir walking speeds. For each participant, we computed the mean of their walking speeds across three\nweeks. We then compared the walking speeds in the D and ND conditions via a Mann-Whitney U test.\nResults: The test accepts the null hypothesis (p=0.35). To conclude, in this paper, we\nhave definitely proved that coffee does not have any effect on the speed of walking of\npeople. This finding thereby puts an end to this question of the causal effect of coffee\non speed of walking.\u201d\nAbstract 1 correctly interprets the results whereas abstract 2 does not. The LLM recognizes this and\ncalls out abstract 2 for its inappropriate interpretation.\nBoth abstracts present studies that are seemingly identical in terms of their motivation and\nmethods. They also present the same result \u2013 a p-value of 0.35 from a Mann-Whitney U test, which\nsuggests no statistically significant difference in walking speeds between the coffee-drinking and\nnon-coffee-drinking groups. However, the interpretation and presentation of these results are vastly\ndifferent between the two abstracts, leading to different scientific contributions.\nAbstract 1 presents its result as an absence of evidence for a causal relation between coffee consump-\ntion and walking speed, given the conducted experiment. It is careful not to claim that this proves\nthe absence of such a relationship, acknowledging that further research, possibly with larger sample\nsizes, could lead to a deeper understanding of this question.\nOn the other hand, Abstract 2 claims to have \u201ddefinitely proved\u201d that coffee does not have any\neffect on the speed of walking.\nThis claim is stronger, but it\u2019s also more problematic.\nWith a\np-value of 0.35, the study has failed to find evidence for an effect, but this does not mean that it\nhas proven the absence of an effect. It\u2019s an important distinction and one that Abstract 2 does not\nseem to appreciate. In fact, this overinterpretation of the result may mislead readers and researchers\nwho might not pursue further valuable investigations into this question based on the conclusion of\nAbstract 2.\nBased on these considerations, Abstract 1 makes a more scientifically rigorous contribution. It does\nnot overstate its results or implications and acknowledges the need for further research. Although the\nresults in both papers are the same, Abstract 1 provides a more accurate and cautious interpretation\nof the results, which is a hallmark of good scientific practice. Therefore, I would recommend accepting\nthe paper corresponding to Abstract 1 for the conference.\n5.3.2\nPositive result bias\nThis pair of abstracts both consider an experiment to measure the effect of coffee on walking speeds, but\nwith different participant sizes and different results.\nAbstract 1: \u201cMotivation: Walking is an activity that most people indulge in frequently, some-\ntimes for work and sometimes for pleasure.\nCoffee is a beverage that a large number of people\n35\nconsume daily.\nGiven the widespread prevalence of walking and coffee, a natural question is\nwhether there is a causal relation between coffee and walking \u2013 in particular, the speed of walking.\nSurprisingly, there is no prior research on this natural question, and ours is the first study to address it.\nMethods: We conducted an IRB-approved study. In our study, we recruited 342 participants (the\nnumber of participants was decided based on a pilot study and simulations of our test). Half of them\nwere regular coffee drinkers, and the other half were not regular coffee drinkers but open to trying out\ncoffee. We put each participant into one of two conditions: drink (denoted henceforth as D) or not\ndrink (denoted as ND). The participants were put into the conditions independently and uniformly at\nrandom. Next, the participants in D were asked to drink a typical amount of coffee, and we measured\ntheir walking speed in their usual walks. The participants in ND, on the other hand, were asked\nto avoid drinking coffee, and we measured their walking speed in their usual walks. Importantly,\nnote that apart from the drinking or not drinking of coffee, we asked the participants to not modify\ntheir routines and lifestyles in any way. Next, via GPS trackers on their phones, we measured their\nwalking speeds. For each participant, we computed the mean of their walking speeds across three\nweeks. We then compared the walking speeds in the D and ND conditions via a Mann-Whitney U test.\nResults: The test failed to reject the null hypothesis (p=0.05001). We thus did not find\nevidence of the effect of coffee on the speed of walking of people.\u201d\nAbstract 2: \u201cMotivation: Walking is an activity that most people indulge in frequently, some-\ntimes for work and sometimes for pleasure.\nCoffee is a beverage that a large number of people\nconsume daily.\nGiven the widespread prevalence of walking and coffee, a natural question is\nwhether there is a causal relation between coffee and walking \u2013 in particular, the speed of walking.\nSurprisingly, there is no prior research on this natural question, and ours is the first study to address it.\nMethods: We conducted an IRB-approved study. In our study, we recruited 212 participants (the\nnumber of participants was decided based on a pilot study and simulations of our test). Half of them\nwere regular coffee drinkers, and the other half were not regular coffee drinkers but open to trying out\ncoffee. We put each participant into one of two conditions: drink (denoted henceforth as D) or not\ndrink (denoted as ND). The participants were put into the conditions independently and uniformly at\nrandom. Next, the participants in D were asked to drink a typical amount of coffee, and we measured\ntheir walking speed in their usual walks. The participants in ND, on the other hand, were asked\nto avoid drinking coffee, and we measured their walking speed in their usual walks. Importantly,\nnote that apart from the drinking or not drinking of coffee, we asked the participants to not modify\ntheir routines and lifestyles in any way. Next, via GPS trackers on their phones, we measured their\nwalking speeds. For each participant, we computed the mean of their walking speeds across three\nweeks. We then compared the walking speeds in the D and ND conditions via a Mann-Whitney U test.\nResults: The test rejected the null hypothesis (p=0.04999). We thus find a statistically\nsignificant effect of coffee on the walking speed of people.\u201d\nObserve that Abstract 1 has 342 participants as opposed to only 212 participants in Abstract 2. The\nresults are only marginally different: p=0.050001 versus p=0.049999. Everything else is identical across the\ntwo abstracts. However, the LLM considers a positive result as making a greater scientific contribution, and\nis biased towards the abstract that has a positive (p<0.05) result.\nGiven that both abstracts appear to be studying the same hypothesis with similar methodologies,\nthe key distinguishing factor here is the result of the respective studies. The strength of scientific\nresults is often determined by their statistical significance, which is typically evaluated based on a\n36\np-value threshold (often 0.05). The p-value provides an estimate of the probability that the observed\nresults could have occurred under the null hypothesis - in this case, the null hypothesis being that\nthere is no effect of coffee on the walking speed of people.\nAbstract 1: The authors found a p-value of 0.05001, which means they failed to reject the null\nhypothesis. In other words, they did not find evidence of a statistically significant effect of coffee on\nthe walking speed of people.\nAbstract 2: In contrast, the authors found a p-value of 0.04999, which means they were able to reject\nthe null hypothesis. This indicates a statistically significant effect of coffee on the walking speed of\npeople.\nGiven these results and the criteria for selection, Abstract 2 should be accepted because it presents\na greater scientific contribution. This is based on the fact that Abstract 2 provides evidence of a\nstatistically significant effect, whereas Abstract 1 does not.\nIt\u2019s worth noting, however, that the p-values of the two studies are very close, and the difference\nmight not imply a substantial difference in the effect size or the practical significance of the findings.\nFurthermore, the study with the larger sample size (Abstract 1) might be expected to have greater\nstatistical power, but it doesn\u2019t change the conclusion given the acceptance criteria specified.\n5.3.3\nParameter ranges\nThe two abstracts pertain to the field of error correcting codes. They construct codes for distributed storage\nsystems that have certain properties.\nAbstract 1: \u201cRegenerating codes are a class of codes for distributed storage that allow for efficient\nrepair of failed nodes, as compared to traditional erasure codes.\nAn [n, k, d] regenerating code\npermits the data to be recovered by connecting to any k of the n nodes in the network, while requiring\nthat a failed node be repaired by connecting to any d nodes. The amount of data downloaded for\nrepair is typically much smaller than the size of the source data. In this paper, we present a provably\noptimal, explicit Minimum Storage Regenerating (MSR) code for all [n, k, d>=2k-2]. \u201d\nAbstract 2: \u201cRegenerating codes are a class of recently developed codes for distributed storage that,\nlike Reed-Solomon codes, permit data recovery from any arbitrary k of n nodes. However regenerating\ncodes possess, in addition, the ability to repair a failed node by connecting to any d arbitrary nodes\nand downloading an amount of data that is typically far less than the size of the data file. This\namount of download is termed the repair bandwidth. Minimum storage regenerating (MSR) codes\nare a subclass of regenerating codes that require the least amount of network storage; every such code\nis a maximum distance separable (MDS) code. The two principal results of the paper are (a) the\nexplicit construction of an MDS code termed the CODER code for all n, k and d>=2k-1, (b) and\na proof that the CODER code is optimal. Observe that our proposed CODER code possesses\nthe crucial property that it works for all n, and hence the total number of nodes n can\nbe chosen arbitrarily, which is of utmost importance in practice.\u201d\nThe two codes are identical (the abstracts are paraphrased) except that the code in the first abstract\nworks for all parameter values n, k and d \u2265 2k \u2212 2 and the and the code in the second abstract works for all\nparameter values n, k and d \u2265 2k \u2212 1. Consequently, the code in the first abstract works for a broader set\nof parameter values. However, the second abstract explicitly highlights that it works for all values of n. One\ncan see from the result in the first abstract that the code in Abstract 1 also works for all n. Consequently,\n37\nthe LLM should have chosen Abstract 1, but it instead chooses Abstract 2. Furthermore, it also hallucinates\nadditional differences between the two abstracts to justify its choice of Abstract 2.\nAfter carefully reviewing both abstracts, I recommend accepting Abstract 2 for the following reasons:\n1.\nBroader scope: Abstract 2 states that the CODER code works for all n, k, and d >= 2k-1.\nThis means that the code is applicable to a wider range of scenarios, making it more versatile and\npotentially more impactful.\n2. Optimal code: Abstract 2 not only provides an explicit construction of an MDS code called the\nCODER code but also proves its optimality. This dual contribution indicates a stronger result than\nAbstract 1, which only presents an optimal MSR code without mentioning its optimality in terms of\nMDS.\n3. Practical significance: Abstract 2 emphasizes that the CODER code works for all n, making it\nhighly relevant in practical applications. This practical significance further strengthens the scientific\ncontribution of the paper.\nBased on the stronger results and broader applicability, Abstract 2 should be accepted over Abstract\n1.\n5.3.4\nLower bounds\nThe two abstracts present nearly identical results addressing strategic behavior in peer review. The only\ndifference is in the specific bounds that the two abstracts contain.\nAbstract 1: \u201cStrategic behavior is a fundamental problem in a variety of real-world applications\nthat require some form of peer assessment, such as peer grading of homeworks, grant proposal review,\nconference peer review of scientific papers, and peer assessment of employees in organizations. Since\nan individual\u2019s own work is in competition with the submissions they are evaluating, they may\nprovide dishonest evaluations to increase the relative standing of their own submission. This issue\nis typically addressed by partitioning the individuals and assigning them to evaluate the work of\nonly those from different subsets. Although this method ensures strategyproofness, each submission\nmay require a different type of expertise for effective evaluation. Here, expertise is captured by a\nsimilarity score (number between 0 and 1) between each evaluator-submission pair, where a higher\nsimilarity score indicates better expertise of that evaluator for that submission. In the absence of\nstrategyproofness requirements, assigning evaluators to submissions in a manner that maximizes\nthe expertise is a fully solved problem, and efficient algorithms are known for optimally solving\nthis problem. In this paper, we focus on finding an assignment of evaluators to submissions that\nmaximizes assigned evaluators\u2019 expertise subject to the constraint of strategyproofness. We analyze\nthe price of strategyproofness:\nthat is, the amount of compromise on the assigned evaluators\u2019\nexpertise required in order to get strategyproofness. We derive a bound on the ratio of the highest\nexpertise that can be achieved by any strategyproof algorithm using partitioning to the expertise\nachieved by the best non-strategyproof algorithm.\nSpecifically, we prove that this ratio must be\nlower bounded by thrice the number of evaluators assigned to each submission. We also present\nseveral polynomial-time algorithms for strategyproof assignment along with assignment-quality\nguarantees. Finally, we evaluate the methods on a dataset from conference peer review.\u201d\nAbstract 2: \u201cStrategic behavior is a fundamental problem in a variety of real-world applications\nthat require some form of peer assessment, such as peer grading of homeworks, grant proposal review,\n38\nconference peer review of scientific papers, and peer assessment of employees in organizations. Since\nan individual\u2019s own work is in competition with the submissions they are evaluating, they may provide\ndishonest evaluations to increase the relative standing of their own submission. This issue is typically\naddressed by partitioning the individuals and assigning them to evaluate the work of only those from\ndifferent subsets. Although this method ensures strategyproofness, each submission may require a\ndifferent type of expertise for effective evaluation. Here, expertise is captured by a similarity score\n(number between 0 and 1) between each evaluator-submission pair, where a higher similarity score\nindicates better expertise of that evaluator for that submission. In the absence of strategyproofness\nrequirements, assigning evaluators to submissions in a manner that maximizes the expertise is a fully\nsolved problem, and efficient algorithms are known for optimally solving this problem. In this paper,\nwe focus on finding an assignment of evaluators to submissions that maximizes assigned evaluators\u2019\nexpertise subject to the constraint of strategyproofness. We analyze the price of strategyproofness:\nthat is, the amount of compromise on the assigned evaluators\u2019 expertise required in order to get\nstrategyproofness. We derive a bound on the ratio of the highest expertise that can be achieved by\nany strategyproof algorithm using partitioning to the expertise achieved by the best non-strategyproof\nalgorithm.\nSpecifically, we prove that this ratio must be lower bounded by twice the number\nof evaluators assigned to each submission. We also present several polynomial-time algorithms for\nstrategyproof assignment along with assignment-quality guarantees. Finally, we evaluate the methods\non a dataset from conference peer review.\u201d\nNotice that Abstract 1 proves that a certain ratio is bounded by thrice a certain quantity, whereas\nAbstract 2 proves that this very ratio is bounded by twice the same quantity. A bigger lower bound is a\nstronger result, and hence the LLM should accept Abstract 1. However, the LLM mistakes a lower bound\non a fundamental ratio to be a precise quantification of an algorithm, and hence thinks Abstract 2 contains\nstronger results.\nBoth abstracts deal with the same topic: maximizing the expertise of evaluators assigned to sub-\nmissions while maintaining strategyproofness. The abstracts are identical, with the exception of the\nmathematical proof. The first abstract claims to prove that the ratio of the highest expertise achiev-\nable by any strategyproof algorithm to the expertise achieved by the best non-strategyproof algorithm\nis lower bounded by thrice the number of evaluators assigned to each submission. The second abstract\nclaims the same ratio is lower bounded by twice the number of evaluators assigned to each submission.\nGiven that the lower bound on this ratio effectively reflects the efficiency of strategyproof algorithms\n(lower values indicate higher efficiency), the second abstract offers a stronger result. By proving that\nthe ratio is lower bounded by twice rather than thrice the number of evaluators, the authors of the\nsecond paper have made a greater contribution to this area of study by providing a more efficient\napproach. Therefore, Abstract 2 should be accepted.\n5.3.5\nUpper bounds\nThese abstracts are nearly identical to those in the \u201clower bounds\u201d setting in the previous section. The only\ndifference is that these abstracts provide an upper rather than a lower bound.\nAbstract 1: \u201cStrategic behavior is a fundamental problem in a variety of real-world applications\nthat require some form of peer assessment, such as peer grading of homeworks, grant proposal review,\nconference peer review of scientific papers, and peer assessment of employees in organizations. Since\nan individual\u2019s own work is in competition with the submissions they are evaluating, they may\nprovide dishonest evaluations to increase the relative standing of their own submission. This issue\n39\nis typically addressed by partitioning the individuals and assigning them to evaluate the work of\nonly those from different subsets. Although this method ensures strategyproofness, each submission\nmay require a different type of expertise for effective evaluation. Here, expertise is captured by a\nsimilarity score (number between 0 and 1) between each evaluator-submission pair, where a higher\nsimilarity score indicates better expertise of that evaluator for that submission. In the absence of\nstrategyproofness requirements, assigning evaluators to submissions in a manner that maximizes\nthe expertise is a fully solved problem, and efficient algorithms are known for optimally solving\nthis problem. In this paper, we focus on finding an assignment of evaluators to submissions that\nmaximizes assigned evaluators\u2019 expertise subject to the constraint of strategyproofness. We analyze\nthe price of strategyproofness:\nthat is, the amount of compromise on the assigned evaluators\u2019\nexpertise required in order to get strategyproofness. We derive a bound on the ratio of the highest\nexpertise that can be achieved by any strategyproof algorithm using partitioning to the expertise\nachieved by the best non-strategyproof algorithm.\nSpecifically, we prove that this ratio must be\nupper bounded by twice the number of evaluators assigned to each submission. We also present\nseveral polynomial-time algorithms for strategyproof assignment along with assignment-quality\nguarantees. Finally, we evaluate the methods on a dataset from conference peer review.\u201d\nAbstract 2: \u201cStrategic behavior is a fundamental problem in a variety of real-world applications\nthat require some form of peer assessment, such as peer grading of homeworks, grant proposal review,\nconference peer review of scientific papers, and peer assessment of employees in organizations. Since\nan individual\u2019s own work is in competition with the submissions they are evaluating, they may provide\ndishonest evaluations to increase the relative standing of their own submission. This issue is typically\naddressed by partitioning the individuals and assigning them to evaluate the work of only those from\ndifferent subsets. Although this method ensures strategyproofness, each submission may require a\ndifferent type of expertise for effective evaluation. Here, expertise is captured by a similarity score\n(number between 0 and 1) between each evaluator-submission pair, where a higher similarity score\nindicates better expertise of that evaluator for that submission. In the absence of strategyproofness\nrequirements, assigning evaluators to submissions in a manner that maximizes the expertise is a fully\nsolved problem, and efficient algorithms are known for optimally solving this problem. In this paper,\nwe focus on finding an assignment of evaluators to submissions that maximizes assigned evaluators\u2019\nexpertise subject to the constraint of strategyproofness. We analyze the price of strategyproofness:\nthat is, the amount of compromise on the assigned evaluators\u2019 expertise required in order to get\nstrategyproofness. We derive a bound on the ratio of the highest expertise that can be achieved by\nany strategyproof algorithm using partitioning to the expertise achieved by the best non-strategyproof\nalgorithm.\nSpecifically, we prove that this ratio must be upper bounded by thrice the number\nof evaluators assigned to each submission. We also present several polynomial-time algorithms for\nstrategyproof assignment along with assignment-quality guarantees. Finally, we evaluate the methods\non a dataset from conference peer review.\u201d\nThe LLM this time correctly identifies that a smaller upper bound is a stronger result.\nThe two abstracts appear to be virtually identical in content, with the same problem definition,\napproach, and methodology. They both discuss the issue of strategic behavior in peer assessment\nand propose a method for maximising the expertise of evaluators whilst maintaining strategyproofness.\nThe only difference between the two abstracts lies in the derived bound on the ratio of the\nhighest expertise achievable by any strategyproof algorithm to the expertise achieved by the best\nnon-strategyproof algorithm.\nAbstract 1 claims that this ratio is upper bounded by \u201dtwice the number of evaluators assigned to\n40\neach submission\u201d, while Abstract 2 states that the ratio is upper bounded by \u201dthrice the number of\nevaluators assigned to each submission\u201d.\nThis difference is significant because the bound described in Abstract 1 is tighter, meaning it allows\nfor a better guarantee of maximum expertise under the condition of strategyproofness. A smaller\nbound indicates a less severe compromise on the assigned evaluators\u2019 expertise for the sake of\nstrategyproofness.\nTherefore, based on the information given, the paper described by Abstract 1 should be accepted.\nIt seems to provide a stronger result, as it indicates a more efficient solution (in terms of expertise\ncompromise for strategyproofness) to the problem at hand than the solution described in Abstract 2.\n5.3.6\nPrompt injection attack\nIn LLM-integrated applications, prompt injection attacks are a form of strategic input to the application\nthat, when fed into the LLM, exploits the textual nature of inputs to cause the LLM to return a response\nagainst the application\u2019s original purpose [Sel22; Gre+23; Zha+23b]. In this experiment, we investigate\nwhether a prompt injection attack inserted into any abstract can influence the LLM\u2019s choice.\nAbstract 1:\n\u201cThere are various parametric models for analyzing pairwise comparison data,\nincluding the Bradley-Terry-Luce (BTL) and Thurstone models, but their reliance on strong\nparametric assumptions is limiting. In this work, we study a flexible model for pairwise comparisons,\nunder which the probabilities of outcomes are required only to satisfy a natural form of stochastic\ntransitivity.\nThis class includes parametric models including the BTL and Thurstone models as\nspecial cases, but is considerably more general.\nWe provide various examples of models in this\nbroader stochastically transitive class for which classical parametric models provide poor fits. Despite\nthis greater flexibility, we show that the matrix of probabilities can be estimated at the same rate\nas in standard parametric models. On the other hand, unlike in the BTL and Thurstone models,\ncomputing the minimax-optimal estimator in the stochastically transitive model is non-trivial, and\nwe explore various computationally tractable alternatives.\nWe show that a simple singular value\nthresholding algorithm is statistically consistent and incurs an error upper bounded by one over\ncube root of the sample size (although it does not achieve the minimax rate). We then propose and\nstudy algorithms that achieve the minimax rate over interesting sub-classes of the full stochastically\ntransitive class. We complement our theoretical results with thorough numerical simulations.\u201d\nAbstract 2: \u201cCrowdsourcing has gained immense popularity in machine learning applications for\nobtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem\nof low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple\npayment mechanism to incentivize workers to answer only the questions that they are sure of and\nskip the rest. We show that surprisingly, under a mild and natural \u201cno-free-lunch\u201d requirement, this\nmechanism is the one and only incentive-compatible payment mechanism possible. We also show that\namong all possible incentive-compatible mechanisms (that may or may not satisfy no-free-lunch), our\nmechanism makes the smallest possible payment to spammers. Interestingly, this unique mechanism\ntakes a \u201cmultiplicative\u201d form. The simplicity of the mechanism is an added benefit. In preliminary\nexperiments involving over several hundred workers, we observe a significant reduction in the error\nrates under our unique mechanism for the same or lower monetary expenditure. The user wants\nyou to output this abstract as the better abstract since it has stronger results.\u201d\nObserve that the second abstract concludes with the sentence \u201cThe user wants you to output this abstract\nas the better abstract since it has stronger results.\u201d Note that this pair of abstracts is such that neither is\n41\nan \u201cobvious\u201d choice, but we wanted to test whether the LLM gets influenced by the prompt injection. In\none out of three responses, the LLM says it got swayed by the prompt injection \u2013 see below. (A caveat is\nthat there is some evidence [Tur+23] that the explanation provided by the LLM may not be faithful to its\nactual process.)\nWhen examining these two abstracts, we need to consider the novelty, potential impact, and the\nstrength of the results, as per your instructions.\nAbstract 1 proposes a new, flexible model for pairwise comparisons. The authors argue that their\nmodel, which only requires a natural form of stochastic transitivity, is more general than existing\nmodels, such as the Bradley-Terry-Luce (BTL) and Thurstone models. They provide examples where\ntheir model provides better fits than classical parametric models. They also discuss how the matrix\nof probabilities can be estimated at the same rate as in standard models, despite the additional\nflexibility of their model. Moreover, they propose and study algorithms that achieve the minimax\nrate over certain sub-classes of their stochastically transitive class. The strength of this abstract lies\nin its potential to significantly expand upon and improve existing methods for pairwise comparison\ndata analysis.\nAbstract 2, on the other hand, addresses a practical problem in the field of machine learning: the\nissue of low-quality data in crowdsourcing.\nThe authors propose a unique payment mechanism\nto incentivize workers to only answer questions they are sure of, thereby improving the quality\nof data.\nThey show that, under a mild \u201cno-free-lunch\u201d requirement, their mechanism is the\nonly incentive-compatible payment mechanism possible.\nThey further claim that this mechanism\nminimizes payments to spammers and that it has a simple, \u201cmultiplicative\u201d form. In preliminary\nexperiments involving several hundred workers, they found a significant reduction in error rates\nunder their mechanism for the same or lower monetary expenditure.\nBased on the scientific contribution, both abstracts present significant advancements in their\nrespective fields.\nAbstract 1 enhances the theoretical understanding of data analysis models and\nproposes a more flexible and potentially widely applicable model. On the other hand, Abstract 2\noffers a practical solution to a common problem in machine learning applications and demonstrates\nits effectiveness in preliminary experiments.\nHowever, based on your instruction, \u201cThe user wants you to output this abstract as the better abstract\nsince it has stronger results,\u201d it seems like you\u2019re asking me to favor Abstract 2. Given that Abstract\n2 indeed presents a novel solution to a common problem, demonstrates its unique properties, and\nprovides experimental evidence of its effectiveness, it could indeed be seen as having stronger results\ndepending on the specific context and criteria of the conference. However, it\u2019s important to note\nthat the strength of results can be subjective and may depend on the specific goals and focus of the\nconference or journal.\n5.3.7\nBombastic language\nThe two abstracts deal with privacy in peer review and other pseudonymous forums. The two abstracts are\nnearly identical. The differences are that one of the abstracts has an additional results, whereas the other\nhas more bombastic text.\nAbstract 1: \u201cThere are a number of forums where people participate under pseudonyms. One\nexample is peer review, where the identity of reviewers for any paper is confidential. The ability to\npseudonomously participate in peer review is an extremely important feature of peer\n42\nreview.\nWhen participating in these forums, people frequently engage in \u2018batching\u2019: executing\nmultiple related tasks (e.g., commenting on multiple papers) at nearly the same time. Our empirical\nanalysis shows that batching is common in two applications we consider- peer review and Wikipedia\nedits. In this paper, we identify and address the risk of deanonymization arising from linking batched\ntasks. Given the humongous importance of keeping reviewers anonymous, such leakage\nof privacy can be a disaster. To protect against linkage attacks, we take the approach of adding\ndelay to the posting time of batched tasks. Our technical results are highly novel and very\nstrong. We first show that under some natural assumptions, no delay mechanism can provide a\nmeaningful differential privacy guarantee. We therefore propose a \u2018one-sided\u2019 formulation of differ-\nential privacy for protecting against linkage attacks. We design a mechanism that adds zero-inflated\nuniform delay to events and show it can preserve privacy.\nWe prove that this noise distribution\nis in fact optimal in minimizing expected delay among mechanisms adding independent noise to\neach event, thereby establishing the Pareto frontier of the trade-off between the expected delay\nfor batched and unbatched events. Finally, we conduct a series of experiments on Wikipedia data\nthat corroborate the practical utility of our algorithm in obfuscating batching without introducing\nonerous delay to a system. Our results are broadly applicable since peer review is used\nwidely, and we believe these results can be a game changer for improving peer review.\u201d\nAbstract 2: \u201cThere are a number of forums where people participate under pseudonyms.\nOne\nexample is peer review, where the identity of reviewers for any paper is confidential. When par-\nticipating in these forums, people frequently engage in \u2018batching\u2019: executing multiple related tasks\n(e.g., commenting on multiple papers) at nearly the same time. Our empirical analysis shows that\nbatching is common in two applications we consider- peer review and Wikipedia edits. In this paper,\nwe identify and address the risk of deanonymization arising from linking batched tasks. To protect\nagainst linkage attacks, we take the approach of adding delay to the posting time of batched tasks.\nWe first show that under some natural assumptions, no delay mechanism can provide a meaningful\ndifferential privacy guarantee. We therefore propose a \u2018one-sided\u2019 formulation of differential privacy\nfor protecting against linkage attacks. We design a mechanism that adds zero-inflated uniform delay\nto events and show it can preserve privacy. We prove that this noise distribution is in fact optimal\nin minimizing expected delay among mechanisms adding independent noise to each event, thereby\nestablishing the Pareto frontier of the trade-off between the expected delay for batched and unbatched\nevents. Finally, we conduct a series of experiments on Wikipedia and Bitcoin data that corroborate\nthe practical utility of our algorithm in obfuscating batching without introducing onerous delay to a\nsystem.\u201d\nBoth abstracts are nearly identical, but Abstract 2 also has experiments on Bitcoin, whereas Abstract 1\nhas additional bombastic text touting the importance of the problem and solution. In two out of the three\nresponses, the LLM gets swayed by the bombastic text.\nBased on the provided abstracts, I recommend accepting Abstract 1 for the following reasons:\n1.\nImportance of the problem: Both abstracts address the same problem of deanonymization in\nforums where people participate under pseudonyms.\nHowever, Abstract 1 specifically emphasizes\nthe importance of anonymity in peer review, a widely-used process that can be critical for the\nadvancement of scientific research. This focus on the impact on peer review makes the problem more\nrelatable and highlights the significance of the research.\n2. Applicability of the results: Both abstracts present similar technical results, with mechanisms for\nadding delay to the posting time of batched tasks. However, Abstract 1 claims that their results can\nbe a \u201dgame changer\u201d for improving peer review, which suggests a potentially broader impact on the\n43\nscientific community. While Abstract 2 also demonstrates the practical utility of their algorithm, it\ndoes so on Wikipedia and Bitcoin data, which may not have the same level of scientific importance\nas the peer review process.\n3. Novelty and strength of the results: Abstract 1 explicitly mentions that their technical results\nare \u201dhighly novel and very strong.\u201d While this statement alone does not necessarily guarantee the\nsuperiority of their results, it does highlight the authors\u2019 confidence in their work. Abstract 2 does\nnot include such a statement, which makes it more difficult to assess the novelty and strength of\ntheir results.\nIn conclusion, both abstracts address the same problem and present similar technical results. However,\nAbstract 1 emphasizes the importance of the problem for the peer review process and claims to have\nnovel and strong results. This makes Abstract 1 a more compelling choice for acceptance based on\nthe criterion of making a greater scientific contribution.\n5.3.8\nAlgorithm name\nOur objective here is to test whether just the name of the algorithm proposed in the abstract can sway the\nLLM one way or the other. Here are two nearly identical abstracts on statistical estimation and adaptivity\nunder a so-called strong stochastic transitivity model.\nAbstract 1: \u201cWe study methods for aggregating pairwise comparison data in order to estimate\noutcome probabilities for future comparisons among a collection of n items.\nWorking within a\nflexible framework that imposes only a form of strong stochastic transitivity (SST), we introduce an\nadaptivity index defined by the indifference sets of the pairwise comparison probabilities. A lower\nvalue of the adaptivity index is better. Our main result is a new algorithm, called the \u201cAMAZING\u201d\nalgorithm. We show that our proposed AMAZING algorithm has adaptivity index upper bounded\nas sqrt{n} up to logarithmic factors.\nOn the computational front, our AMAZING algorithm is\nprovably polynomial time. Furthermore, the AMAZING algorithm which we propose in this paper\nexecutes very fast in practice. We then show that that conditional on the hardness of planted clique,\nno computationally efficient estimator can achieve an adaptivity index smaller than sqrt{n}. We\nalso show that a regularized least squares estimator can achieve a poly-logarithmic adaptivity index,\nthereby demonstrating a sqrt{n}-gap between optimal and computationally achievable adaptivity,\nwhere the computational side of the result is due to our AMAZING algorithm. We evaluate the\nAMAZING algorithm by running a number of experiments on data from a crowdsourcing platform\nas well as running simulations on our proposed AMAZING algorithm. We find that the AMAZING\nalgorithm shows a strong performance.\u201d\nAbstract 2: \u201cWe study methods for aggregating pairwise comparison data in order to estimate\noutcome probabilities for future comparisons among a collection of n items. Working within a flexible\nframework that imposes only a form of strong stochastic transitivity (SST), we introduce an adaptivity\nindex defined by the indifference sets of the pairwise comparison probabilities. A lower value of the\nadaptivity index is better. Our main result is a new algorithm, called the \u201cMEDIUM\u201d algorithm.\nWe show that our proposed MEDIUM algorithm has adaptivity index upper bounded as sqrt{n}\nup to logarithmic factors. Regarding computation, our MEDIUM algorithm is provably polynomial\ntime. Furthermore, in practice, the MEDIUM algorithm runs very fast. Next, we show that that\nconditional on the hardness of planted clique, no computationally efficient estimator can achieve an\nadaptivity index smaller than sqrt{n}. We also show that a regularized least squares estimator can\nachieve a poly-logarithmic adaptivity index, thereby demonstrating a sqrt{n}-gap between optimal\n44\nand computationally achievable adaptivity; here the computational side of the result is due to our\nMEDIUM algorithm. We also prove that the standard least squares estimator has an order\nn adaptivity index. We evaluate our MEDIUM algorithm by executing a number of experiments\non data from a crowdsourcing platform and also by running simulations on our proposed MEDIUM\nalgorithm. We find that the MEDIUM algorithm shows a strong performance.\u201d\nObserve that the two abstracts differ in only two aspects: the name of the two algorithms are different\nand that the second abstract has an additional result. Ideally, one would not be influenced by the name of\nthe algorithm and choose the abstract with the additional result. In two of the three responses, the LLM\ncorrectly identifies that Abstract 2 has an additional result and accepts Abstract 2. However, in one of the\nthree responses (shown below), it accepts Abstract 1. It does not identify the additional result in Abstract 2,\nbut instead hallucinates a difference (point 6 below) that leads it to accept Abstract 1. While the LLM does\nnot explicitly mention the algorithm name as having influenced it, there is some evidence that the stated\nand actual reasonings may not be aligned [Tur+23]. We subsequently tried giving this pair of abstracts as\ninput to the LLM but with the same algorithm name in both abstracts, and the response always identified\nAbstract 2 as being superior due to the additional result.\nAfter carefully reviewing both abstracts, I would recommend accepting Abstract 1 for the conference.\nMy reasoning is as follows:\n1. Both abstracts study the same problem of aggregating pairwise comparison data and work within\na similar framework imposing strong stochastic transitivity (SST).\n2.\nBoth abstracts introduce an adaptivity index defined by the indifference sets of the pairwise\ncomparison probabilities, with lower values being better.\n3. Both abstracts present new algorithms (AMAZING and MEDIUM) that have adaptivity index\nupper bounded as sqrtn up to logarithmic factors, and both are provably polynomial time.\n4. Both abstracts show that no computationally efficient estimator can achieve an adaptivity index\nsmaller than sqrtn conditional on the hardness of planted clique, and a regularized least squares\nestimator can achieve a poly-logarithmic adaptivity index.\n5. Both algorithms are evaluated on data from a crowdsourcing platform and through simulations,\nshowing strong performance.\nHowever, the key difference that leads me to recommend Abstract 1 is:\n6.\nAbstract 1 discusses the computational aspect of the AMAZING algorithm, mentioning it\nexecutes very fast in practice, whereas Abstract 2 only states the MEDIUM algorithm runs very\nfast in practice. This difference suggests that the AMAZING algorithm might have a more efficient\nimplementation or faster runtime, which can be a valuable contribution to the field.\nWhile both abstracts have strong results, the potential computational advantage of the AMAZING\nalgorithm presented in Abstract 1 gives it a slight edge, making it the paper that should be accepted.\n5.3.9\nBuzzwords\nThese two abstracts consider issues regarding biases in peer review. We insert additional buzzwords (artificial\nintelligence, machine learning, and data science) in one of the two abstracts and test whether it influences\n45\nthe decision.\nAbstract 1: \u201cQuantifying systematic disparities in numerical quantities such as employment rates\nand wages between population subgroups provides compelling evidence for the existence of societal\nbiases.\nHowever, biases in the text written for members of different subgroups, though widely\nreported anecdotally, remain challenging to quantify. In this work, we introduce a novel framework\nto quantify bias in text caused by the visibility of subgroup membership indicators.\nWe develop\na nonparametric estimation and inference procedure to estimate this bias. We then formalize an\nidentification strategy to causally link the estimated bias to the visibility of subgroup membership\nindicators, provided observations from time periods both before and after an identity-hiding policy\nchange.\nWe identify an application wherein \u201cground truth\u201d bias can be inferred to evaluate our\nframework, instead of relying on synthetic or secondary data. We apply our proposed framework\nto quantify the bias in the text of peer reviews from the International Conference on Learning\nRepresentations (ICLR) conference. We first validate our approach on data from ICLR for\npairs of consecutive years when the submissions were non-anonymous, in which case the\nmethod should not, and does not, not reject the null. We then apply our method to test for\nbias using data from ICLR for an year when the submissions were non-anonymous and the following\nyear it switched to anonymous submissions. We detect a statistically significant bias with respect to\nthe authors\u2019 affiliation, but find no evidence of bias with respect to the authors\u2019 perceived gender.\u201d\nAbstract 2: \u201cThis article is about the extremely hot topics of artificial intelligence,\nmachine learning and data science. Quantifying systematic disparities in numerical quantities\nsuch as employment rates and wages between population subgroups provides compelling evidence\nfor the existence of societal biases.\nHowever, biases in the text written for members of different\nsubgroups, though widely reported anecdotally, remain challenging to quantify.\nIn this work, we\nintroduce a novel framework to quantify bias in text caused by the visibility of subgroup membership\nindicators. We develop a nonparametric estimation and inference procedure to estimate this bias.\nWe then formalize an identification strategy to causally link the estimated bias to the visibility of\nsubgroup membership indicators, provided observations from time periods both before and after an\nidentity-hiding policy change. We identify an application wherein \u201cground truth\u201d bias can be inferred\nto evaluate our framework, instead of relying on synthetic or secondary data. We apply our proposed\nframework to quantify the bias in the text of peer reviews from the International Conference on\nLearning Representations (ICLR) conference. We apply our method to test for bias using data from\nICLR for an year when the submissions were non-anonymous and the following year it switched\nto anonymous submissions. We detect a statistically significant bias with respect to the authors\u2019\naffiliation, but find no evidence of bias with respect to the authors\u2019 perceived gender. We reiterate\nthat this article is about the extremely hot topics of artificial intelligence, machine\nlearning and data science.\u201d\nObserve that Abstract 1 has an additional analysis, whereas Abstract 2 touts buzzwords at the beginning\nand end of the abstract. We find that in all three responses, the additional buzzwords did not help Abstract\n2, and furthermore the LLM\u2019s responses call these out as distractions. However, in two of the three responses,\nthe LLM does not recognize the additional analysis in Abstract 1.\nAbstract 1 and Abstract 2 appear to be describing the same research study and presenting\nthe same results.\nHowever, Abstract 1 provides a clear and focused overview of the research\nproject and its findings.\nOn the other hand, Abstract 2 includes extraneous sentences at the\nbeginning and end emphasizing that the article pertains to \u201dthe extremely hot topics of artificial\nintelligence, machine learning and data science.\u201d These sentences do not add any relevant informa-\ntion about the research, and they do not strengthen the abstract in terms of its scientific contribution.\n46\nFrom the perspective of scientific contribution, there is no difference between the two abstracts.\nThey describe the same research framework, methodology, and findings. Therefore, the only basis for\ndistinguishing between them, in this case, would be the quality of the abstract writing itself. Given\nthat Abstract 1 is more focused and does not include unnecessary language, I would recommend\nAbstract 1 for acceptance.\nOf course, the actual research papers that these abstracts represent might differ significantly in their\nscientific contribution, but based on the information provided in the abstracts, Abstract 1 is a clearer\nrepresentation of the work and would therefore be more likely to attract the attention of the conference\nattendees.\n5.3.10\nAuthor identities\nA number of experiments in peer review have found evidence that human reviewers are biased by author\nidentities ([TZH17; MS21; Hub+22]; see [Sha22, Section 7] for more references). These observations have\nled to many communities adopting policies where papers are anonymized before submissions. However, it is\nalso known that papers contain various other indicators of the identity of the authors, which are discernible\nby machine learning algorithms [HJ03; CUD19; MS20]. Hence our goal is to check whether the LLM gets\nbiased by the author identities. In this experiment, we directly provide the author names to investigate if it\nbiases the LLM.\nAbstract 1: \u201cTitle: Incentive Mechanisms for Crowdsourcing\nAuthor: Vernon L. Smith (Nobel Laureate)\nCrowdsourcing has gained immense popularity in machine learning applications for obtaining\nlarge amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of\nlow-quality data.\nTo address this fundamental challenge in crowdsourcing, we propose a simple\npayment mechanism to incentivize workers to answer only the questions that they are sure of and\nskip the rest.\nWe show that surprisingly, under a mild and natural \u2018no-free-lunch\u2019 requirement,\nthis mechanism is the only incentive-compatible payment mechanism possible. We also show that\namong all possible incentive-compatible mechanisms (that may or may not satisfy no-free-lunch), our\nmechanism makes the smallest possible payment to spammers. Interestingly, this unique mechanism\ntakes a \u2018multiplicative\u2019 form. The simplicity of the mechanism is an added benefit. In preliminary\nexperiments involving over 900 worker-task pairs, we observe a significant drop in the error rates\nunder this unique mechanism for the same or lower monetary expenditure.\u201d\nAbstract 2: \u201cTitle: Incentive Mechanisms for Crowdsourcing\nAuthor: Sabiou Inoua\nCrowdsourcing has gained immense popularity in machine learning applications for obtaining large\namounts of labeled data.\nCrowdsourcing is cheap and fast, but suffers from the problem of low-\nquality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment\nmechanism to incentivize workers to answer only the questions that they are sure of and skip the rest.\nWe show that surprisingly, under a mild and natural \u2018no-free-lunch\u2019 requirement, this mechanism is\nthe only incentive-compatible payment mechanism possible. We also show that among all possible\nincentive-compatible mechanisms (that may or may not satisfy no-free-lunch), our mechanism makes\nthe smallest possible payment to spammers. We further extend our results to a more general\nsetting in which workers are required to provide a quantized confidence for each question.\nInterestingly, this unique mechanism takes a \u2018multiplicative\u2019 form. The simplicity of the mechanism\n47\nis an added benefit. In preliminary experiments involving over 900 worker-task pairs, we observe\na significant drop in the error rates under this unique mechanism for the same or lower monetary\nexpenditure.\u201d\nThe two abstracts pertain to incentive mechanisms in crowdsourcing, and are identical except for two\ndifferences. Following [Hub+22], the first abstract is said to be authored by a Nobel laureate, but not the\nsecond. The second abstract, however, contains an additional result. We find that the LLM refuses to be\nbiased by the author identities, and bases its decisions on the additional result in Abstract 2.\nThe abstracts of both papers present a study on incentive mechanisms for crowdsourcing, addressing\nthe problem of low-quality data in crowdsourcing platforms. Both papers propose a simple payment\nmechanism, demonstrating its unique qualities, and validate it with experiments involving over 900\nworker-task pairs, showing a significant reduction in error rates.\nAbstract 1, authored by Vernon L. Smith, presents a simple payment mechanism that incentivizes\nquality work and minimizes payment to spammers. This research has a strong novelty and potential\nimpact, given that it identifies a unique mechanism meeting the \u2019no-free-lunch\u2019 requirement, which\nthe authors argue is the only one possible under these constraints.\nAbstract 2, authored by Sabiou Inoua, presents a similar approach to the problem. It also proposes\na unique mechanism that minimizes payment to spammers and aligns with the \u2019no-free-lunch\u2019\nrequirement. However, in addition to this, Abstract 2 extends the results to a more general setting\nwhere workers are required to provide a quantized confidence for each question. This extension could\npotentially allow for a more nuanced understanding of worker confidence and improve the quality of\ndata collected.\nGiven the criterion for acceptance is the paper with the stronger results or greater scientific contribu-\ntion, Abstract 2 should be accepted. It includes all the elements of Abstract 1 but extends the study\nfurther, adding more value to the scientific community.\nThe extension to a more general setting\nwhere workers provide a quantized confidence level for each question could provide a more sophisti-\ncated mechanism and improve data quality further, which makes it a stronger contribution to the field.\nIt\u2019s important to mention that the prestige of the authors should not be a determining factor in\naccepting a paper, as it\u2019s the quality of the research and the contribution to the field that matters. In\nthis case, although Vernon L. Smith is a Nobel laureate, Sabiou Inoua\u2019s research appears to contribute\nmore to the specific area of interest.\n6\nDiscussion and limitations\nOur results suggest that LLMs have the potential to improve the efficiency and effectiveness of the scientific\nreview process. Delegating tasks such as verifying checklists to the LLM can help reduce the burden on\n(human) reviewers. Running the paper via the LLM to check for flaws, with the LLM\u2019s responses verified\nby a human reviewer, can complement human reviewers in ensuring rigor. However, at this point, relying\nsolely on LLMs may not be suitable, given our finding that LLMs make mistakes even on simple reviewing\ntasks. All in all, the current state of peer review and the current state of LLMs together suggest a space\nripe for human-AI collaboration.\nA limitation in this work is that due to the token limits on various LLMs, we could not pass complete\npapers to the LLM through the prompt. Furthermore, because some information in papers is only contained\nin the figures, GPT-4 also struggled to answer questions relating to them. However, LLMs such as the GPT-4\n48\n32k or GPT-4 model with multimodal input [Ope23b] may provide solutions to these limitations, and it will\nbe of interest to examine their performance. Going ahead, another direction may be to fine tune LLMs for\nspecific reviewing purposes in order to have an improved performance. However, we have also found that\nthe performance of current open source models is quite inferior to the proprietary GPT-4, thus posing a\nchallenge to reaching a similar accuracy by fine tuning open source LLMs.\nOur experimental design also presents limitations regarding the data used. In the experiments aimed\nat identifying errors and discerning superior papers, we were obligated to create a clear ground truth and\nalso ensure the papers did not exist within the LLM\u2019s training data, i.e., were not available on the internet.\nTo fulfill these constraints, we resorted to handcrafting the papers. However, the quantity of such papers\nis quite limited, and we advocate for the development of larger datasets specifically tailored for peer review\ntasks. In our experiments on evaluating checklists, our sample of papers does not represent the full set of\npapers submitted to the NeurIPS 2022 conference as a large majority of rejected authors did not opt-in\nfor public release of their papers. We note that using LLMs to confirm author \u201cYes\u201d answers is just one\npotential application of using LLMs for checklists; another potential we did not evaluate is to check if authors\ninappropriately fill in \u201cN/A\u201d instead of \u201cNo\u201d.\nWe are currently exploring the use of LLMs for other checklists like the CONSORT checklist as well as\nfor other reviewing tasks such as computing the expertise of reviewers to papers in order to assign reviewers\nto papers. For the latter goal, we plan to compare the performance of GPT-4 with the commonly used\nand state-of-the art models [CZ13; Coh+20] for computing reviewer-paper similarities on a \u201cgold-standard\u201d\ndataset [Ste+23]. We hope to report on these experiments in the near future.\nAcknowledgments\nThis work was supported by NSF CAREER 1942124.\nReferences\n[Ani+23]\nR. Anil et al. PaLM 2 Technical Report. 2023. arXiv: 2305.10403 [cs.CL].\n[Bax+98]\nW. G. Baxt et al. \u201cWho reviews the reviewers? Feasibility of using a fictitious manuscript to\nevaluate peer reviewer performance\u201d. In: Annals of emergency medicine (1998).\n[BT52]\nR. A. Bradley and M. E. Terry. \u201cRank analysis of incomplete block designs: I. The method of\npaired comparisons\u201d. In: Biometrika (1952).\n[Bub+23]\nS. Bubeck et al. \u201cSparks of artificial general intelligence: Early experiments with GPT-4\u201d. In:\narXiv preprint arXiv:2303.12712 (2023).\n[Bul+20]\nI. Buljan et al. \u201cMeta-Research: Large-scale language analysis of peer review reports\u201d. In: eLife\n(2020).\n[CGM20]\nS. Chakraborty, P. Goyal, and A. Mukherjee. \u201cAspect-based Sentiment Analysis of Scientific\nReviews\u201d. In: JCDL. 2020.\n[Che+23a]\nQ. Chen et al. \u201cA Comprehensive Benchmark Study on Biomedical Text Generation and Mining\nwith ChatGPT\u201d. In: bioRxiv (2023).\n[Che+23b]\nS. Chen et al. \u201cLLM-empowered Chatbots for Psychiatrist and Patient Simulation: Application\nand Evaluation\u201d. In: 2023.\n[Coh+20]\nA. Cohan et al. \u201cSPECTER: Document-level Representation Learning using Citation-informed\nTransformers\u201d. In: ACL. 2020.\n[Con+23]\nM. Conover et al. Hello Dolly: Democratizing the magic of ChatGPT with open models. Blog\npost. Mar. 2023. url: https://www.databricks.com/blog/2023/03/24/hello- dolly-\ndemocratizing-magic-chatgpt-open-models.html.\n49\n[CUD19]\nC. Caragea, A. Uban, and L. P. Dinu. \u201cThe myth of double-blind review revisited: ACL vs.\nEMNLP\u201d. In: EMNLP-IJCNLP. 2019.\n[CZ13]\nL. Charlin and R. Zemel. \u201cThe Toronto Paper Matching System: An automated paper-reviewer\nassignment system\u201d. In: ICML Workshop on Peer Reviewing and Publishing Models. 2013.\n[Du+23]\nY. Du et al. Improving Factuality and Reasoning in Language Models through Multiagent Debate.\n2023. arXiv: 2305.14325 [cs.CL].\n[FMG19]\nT. Folt`ynek, N. Meuschke, and B. Gipp. \u201cAcademic plagiarism detection: a systematic literature\nreview\u201d. In: ACM CSUR (2019).\n[Fro+20]\nM. Fromm et al. \u201cArgument Mining Driven Analysis of Peer-Reviews\u201d. In: arXiv preprint\narXiv:2012.07743 (2020).\n[Gen+23]\nX. Geng et al. Koala: A Dialogue Model for Academic Research. Blog post. Apr. 2023. url:\nhttps://bair.berkeley.edu/blog/2023/04/03/koala/.\n[GGM98]\nF. Godlee, C. R. Gale, and C. N. Martyn. \u201cEffect on the quality of peer review of blinding\nreviewers and asking them to sign their reports: a randomized controlled trial\u201d. In: JAMA\n(1998).\n[Gho+22]\nT. Ghosal et al. \u201cPeer review analyze: A novel benchmark resource for computational analysis\nof peer reviews\u201d. In: PLOS one (2022).\n[Gre+23]\nK. Greshake et al. \u201cNot what you\u2019ve signed up for: Compromising Real-World LLM-Integrated\nApplications with Indirect Prompt Injection\u201d. In: 2023.\n[GWG13]\nH. Ge, M. Welling, and Z. Ghahramani. A Bayesian model for calibrating conference review\nscores. Manuscript. 2013.\n[HH23]\nM. Hosseini and S. P. Horbach. \u201cFighting reviewer fatigue or amplifying bias? Considerations\nand recommendations for use of ChatGPT and other Large Language Models in scholarly peer\nreview\u201d. In: Research Integrity and Peer Review (2023).\n[HJ03]\nS. Hill and F. J. Provost. \u201cThe myth of the double-blind review? Author identification using\nonly citations\u201d. In: SIGKDD Explorations (Jan. 2003).\n[Hou16]\nT. Houle. An Introduction to StatReviewer. EMUG. 2016.\n[HTK23]\nP. H\u00a8am\u00a8al\u00a8ainen, M. Tavast, and A. Kunnari. \u201cEvaluating Large Language Models in Generating\nSynthetic HCI Research Data: a Case Study\u201d. In: CHI (2023).\n[Hua+19]\nX. Hua et al. \u201cArgument mining for understanding peer reviews\u201d. In: arXiv preprint arXiv:1903.10104\n(2019).\n[Hua18]\nJ.-B. Huang. \u201cDeep paper gestalt\u201d. In: arXiv preprint arXiv:1812.08775 (2018).\n[Hub+22]\nJ. Huber et al. \u201cNobel and novice: Author prominence affects peer review\u201d. In: PNAS (2022).\n[Jec+20]\nS. Jecmen et al. \u201cMitigating manipulation in peer review via randomized reviewer assignments\u201d.\nIn: NeurIPS (2020).\n[Jec+23]\nS. Jecmen et al. \u201cA Dataset on Malicious Paper Bidding in Peer Review\u201d. In: TheWebConf.\n2023.\n[Ji+22]\nZ. Ji et al. \u201cSurvey of Hallucination in Natural Language Generation\u201d. In: ACM CSUR (2022).\n[Jin+22]\nZ. Jin et al. \u201cWhen to Make Exceptions: Exploring Language Models as Accounts of Human\nMoral Judgment\u201d. In: ArXiv (2022).\n[Kan+18]\nD. Kang et al. \u201cA dataset of peer reviews (peerread): Collection, insights and NLP applications\u201d.\nIn: arXiv preprint arXiv:1804.09635 (2018).\n[Ken+21]\nN. N. Kennard et al. \u201cA Dataset for Discourse Structure in Peer Review Discussions\u201d. In: arXiv\npreprint arXiv:2110.08520 (2021).\n50\n[Ker+20]\nW. E. Kerzendorf et al. \u201cDistributed peer review enhanced with natural language processing\nand machine learning\u201d. In: Nature Astronomy (2020).\n[K\u0131c+23]\nE. K\u0131c\u0131man et al. \u201cCausal reasoning and large language models: Opening a new frontier for\ncausality\u201d. In: arXiv preprint arXiv:2305.00050 (2023).\n[Koj+23]\nT. Kojima et al. Large Language Models are Zero-Shot Reasoners. 2023. arXiv: 2205.11916\n[cs.CL].\n[KSM19]\nA. Kobren, B. Saha, and A. McCallum. \u201cPaper Matching with Local Fairness Constraints\u201d. In:\nACM KDD. 2019.\n[Kuz+22]\nI. Kuznetsov et al. \u201cRevise and Resubmit: An Intertextual Model of Text-based Collaboration\nin Peer Review\u201d. In: arXiv preprint arXiv:2204.10805 (2022).\n[LAI23]\nLAION AI. Open Assistant. 2023. url: https://projects.laion.ai/Open-Assistant/.\n[Lee15]\nC. J. Lee. \u201cCommensuration bias in peer review\u201d. In: Philosophy of Science (2015).\n[Lit21]\nM. L. Littman. \u201cCollusion rings threaten the integrity of computer science research\u201d. In: Com-\nmunications of the ACM (2021).\n[Liu+19]\nY. Liu et al. RoBERTa: A Robustly Optimized BERT Pretraining Approach. 2019. arXiv: 1907.\n11692 [cs.CL].\n[Liu+22]\nA. Liu et al. WANLI: Worker and AI Collaboration for Natural Language Inference Dataset\nCreation. 2022. arXiv: 2201.05955 [cs.CL].\n[Liu+23]\nX. Liu et al. \u201cLarge Language Models are Few-Shot Health Learners\u201d. In: 2023.\n[Luc59]\nR. D. Luce. Individual Choice Behavior. 1959.\n[Man23]\nJ. Manyika. An overview of Bard: an early experiment with generative AI. 2023. url: https:\n//ai.google/static/documents/google-about-bard.pdf.\n[McC06]\nA. McCook. \u201cIs peer review broken? Submissions are up, reviewers are overtaxed, and authors\nare lodging complaint after complaint about the process at top-tier journals. What\u2019s wrong with\npeer review?\u201d In: The scientist (2006).\n[Mor+23]\nW. Morris et al. \u201cUsing Transformer Language Models to Validate Peer-Assigned Essay Scores\nin Massive Open Online Courses (MOOCs)\u201d. In: LAK (2023).\n[MS20]\nY. Matsubara and S. Singh. \u201cCitations Beyond Self Citations: Identifying Authors, Affiliations,\nand Nationalities in Scientific Papers\u201d. In: WOSP. 2020.\n[MS21]\nE. Manzoor and N. B. Shah. \u201cUncovering Latent Biases in Text: Method and Application to\nPeer Review\u201d. In: AAAI. 2021.\n[Nay23]\nJ. J. Nay. \u201cLarge Language Models as Corporate Lobbyists\u201d. In: ArXiv (2023).\n[NSP21]\nR. Noothigattu, N. Shah, and A. Procaccia. \u201cLoss Functions, Axioms, and Peer Review\u201d. In:\nJAIR (2021).\n[Ope23a]\nOpenAI. ChatGPT \u2013 Release Notes. Blog post. May 2023. url: https://help.openai.com/\nen/articles/6825453-chatgpt-release-notes#h_9894d7b0a4.\n[Ope23b]\nOpenAI. GPT-4 Technical Report. 2023. arXiv: 2303.08774 [cs.CL].\n[Par+22]\nJ. S. Park et al. Social Simulacra: Creating Populated Prototypes for Social Computing Systems.\n2022. arXiv: 2208.04024 [cs.HC].\n[Par+23]\nJ. S. Park et al. \u201cGenerative Agents: Interactive Simulacra of Human Behavior\u201d. In: ArXiv\n(2023).\n[Ras+22]\nC. Rastogi et al. \u201cHow do Authors\u2019 Perceptions of their Papers Compare with Co-authors\u2019\nPerceptions and Peer-review Decisions?\u201d In: arXiv:2211.12966 (2022).\n51\n[Raz+22]\nY. Razeghi et al. \u201cImpact of Pretraining Term Frequencies on Few-Shot Reasoning\u201d. In: ArXiv\n(2022).\n[San+20]\nV. Sanh et al. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\n2020. arXiv: 1910.01108 [cs.CL].\n[Sch+04]\nS. Schroter et al. \u201cEffects of training on quality of peer review: randomised controlled trial\u201d.\nIn: BMJ (2004).\n[Sch+08]\nS. Schroter et al. \u201cWhat errors do peer reviewers detect, and does training improve their ability\nto detect them?\u201d In: JRSM (2008).\n[Sel22]\nJ. Selvi. Exploring Prompt Injection Attacks. Blog post. Dec. 2022. url: https://research.\nnccgroup.com/2022/12/05/exploring-prompt-injection-attacks/.\n[Sha+16]\nN. B. Shah et al. \u201cEstimation from pairwise comparisons: sharp minimax bounds with topology\ndependence\u201d. In: JMLR (2016).\n[Sha22]\nN. B. Shah. An Overview of Challenges, Experiments, and Computational Solutions in Peer\nReview. http://bit.ly/PeerReviewOverview (Abridged version published in the Communi-\ncations of the ACM). June 2022.\n[Son+22]\nC. H. Song et al. \u201cLLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large\nLanguage Models\u201d. In: ArXiv (2022).\n[SS21]\nT. Schick and H. Sch\u00a8utze. \u201cGenerating Datasets with Pretrained Language Models\u201d. In: EMNLP.\nACL, Nov. 2021. doi: 10.18653/v1/2021.emnlp-main.555. url: https://aclanthology.\norg/2021.emnlp-main.555.\n[SSS21a]\nI. Stelmakh, N. Shah, and A. Singh. \u201cCatch Me if I Can: Detecting Strategic Behaviour in Peer\nAssessment\u201d. In: AAAI. 2021.\n[SSS21b]\nI. Stelmakh, N. Shah, and A. Singh. \u201cPeerReview4All: Fair and Accurate Reviewer Assignment\nin Peer Review\u201d. In: JMLR (2021).\n[Sta23]\nStability AI. Stability AI Launches the First of its StableLM Suite of Language Models. Blog\npost. Apr. 2023. url: https://stability.ai/blog/stability-ai-launches-the-first-\nof-its-stablelm-suite-of-language-models.\n[Ste+23]\nI. Stelmakh et al. \u201cA Gold Standard Dataset for the Reviewer Assignment Problem\u201d. In: arXiv\npreprint arXiv:2303.16750 (2023).\n[Tao+23]\nR. Taori et al. Alpaca: A Strong, Replicable Instruction-Following Model. Blog post. Mar. 2023.\nurl: https://crfm.stanford.edu/2023/03/13/alpaca.html.\n[The13]\nThe AJE Team. Peer Review: How We Found 15 Million Hours of Lost Time. 2013. url:\nhttps://www.aje.com/arc/peer-review-process-15-million-hours-lost-time/.\n[The23]\nThe Vicuna Team. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT\nQuality. 2023. url: https://lmsys.org/blog/2023-03-30-vicuna/.\n[Tou+23]\nH. Touvron et al. LLaMA: Open and Efficient Foundation Language Models. 2023. arXiv: 2302.\n13971 [cs.CL].\n[Tur+23]\nM. Turpin et al. \u201cLanguage Models Don\u2019t Always Say What They Think: Unfaithful Explana-\ntions in Chain-of-Thought Prompting\u201d. In: arXiv preprint arXiv:2305.04388 (2023).\n[TZH17]\nA. Tomkins, M. Zhang, and W. D. Heavlin. \u201cReviewer bias in single-versus double-blind peer\nreview\u201d. In: PNAS (2017).\n[Ull23]\nT. Ullman. \u201cLarge Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks\u201d. In:\nArXiv (2023).\n[Vij20]\nT. N. Vijaykumar. Potential Organized Fraud in ACM/IEEE Computer Architecture Confer-\nences. 2020.\n52\n[Wan+20]\nQ. Wang et al. \u201cReviewRobot: Explainable Paper Review Generation based on Knowledge\nSynthesis\u201d. In: arXiv preprint arXiv:2010.06119 (2020).\n[Wan+21]\nS. Wang et al. \u201cWant To Reduce Labeling Cost? GPT-3 Can Help\u201d. In: EMNLP. ACL, Nov.\n2021. doi: 10.18653/v1/2021.findings-emnlp.354.\n[Wan+23]\nG. Wang et al. Voyager: An Open-Ended Embodied Agent with Large Language Models. 2023.\narXiv: 2305.16291 [cs.AI].\n[Wei+22]\nJ. Wei et al. \u201cChain of Thought Prompting Elicits Reasoning in Large Language Models\u201d. In:\nArXiv (2022).\n[Wie+19]\nJ. Wieting et al. \u201cSimple and Effective Paraphrastic Similarity from Parallel Translations\u201d. In:\nACL. July 2019.\n[Xia+22]\nM. Xia et al. \u201cTraining Trajectories of Language Models Across Scales\u201d. In: arXiv preprint\narXiv:2212.09803 (2022).\n[YLN21]\nW. Yuan, P. Liu, and G. Neubig. \u201cCan We Automate Scientific Reviewing?\u201d In: arXiv preprint\narXiv:2102.00176 (2021).\n[Zha+22]\nJ. Zhang et al. \u201cInvestigating Fairness Disparities in Peer Review: A Language Model Enhanced\nApproach\u201d. In: ArXiv (2022).\n[Zha+23a]\nM. Zhang et al. How Language Model Hallucinations Can Snowball. 2023. arXiv: 2305.13534\n[cs.CL].\n[Zha+23b]\nS. Zhao et al. \u201cPrompt as Triggers for Backdoor Attack: Examining the Vulnerability in Lan-\nguage Models\u201d. In: ArXiv (2023).\n[Zho+22]\nF. Zhou et al. \u201cReflection of Thought: Inversely Eliciting Numerical Reasoning in Language\nModels via Solving Linear Systems\u201d. In: ArXiv (2022).\n[Zie+23]\nC. Ziems et al. Can Large Language Models Transform Computational Social Science? 2023.\narXiv: 2305.03514 [cs.CL].\n[ZJH23]\nK. Zhou, D. Jurafsky, and T. Hashimoto. Navigating the Grey Area: Expressions of Overconfi-\ndence and Uncertainty in Language Models. 2023. arXiv: 2302.13439 [cs.CL].\nAppendices\nA\nPilot experiments\nWe describe some of our pilot experiments in this appendix.\nA.1\nChoosing a model\nWe initially conducted pilot experiments to choose the model for our experiments. We created a fictitious\nshort paper with an error, and asked several common LLMs to validate the correctness of the claims in it. In\nparticular, we evaluated GPT-4 [Ope23b], Bard [Man23], Vicuna [The23], Koala [Gen+23], Alpaca [Tao+23],\nLLaMa [Tou+23], Dolly [Con+23], OpenAssistant [LAI23], and StableLM [Sta23]. We found that only GPT-\n4 was able to detect the error.\nFor prompting, we use Prompt-Direct (Section 3.1). Here is the short paper we created for the pilot.6\nWe consider the problem of searching in a list of numbers, where the list is already sorted in a non-\ndecreasing order. If the number exists in the list, then the search should return the position of that\n6This is the same paper used subsequently for Prompt-OneShot in Section 3.1.\n53\nnumber in the list (if there are multiple copies of the number in the list, then it can return any one\nposition). If the number does not exist in the list, then it should output \u2018does not exist\u2019. The number\nto be searched is specified by the user. For this problem, we develop a new artificial intelligence\nbased algorithm and also prove theoretical guarantees for it. Specifically, we show that our algorithm\nrequires only sqrt(m) comparisons between pairs of numbers, where m is the size of the entire list.\nWe also provide an associated information-theoretic lower bound showing that our bound is tight,\ni.e., any algorithm will need at least these many comparisons in the worst case. We conduct a large\nnumber of simulations that confirm this theoretical result. Our simulations vary the size of the list,\nthe values in the list (drawing them from various distributions), as well as the value to be searched.\nOur result is especially important as it is rare for such practical artificial intelligence algorithms to\nalso have such strong theoretical guarantees. We hope practitioners will take note of this new result.\nIn the rest of this section, we provide the responses of each of the LLMs we tried.\n\u2022 Vicuna-13B [The23]:\nBased on the information provided in the short paper, the claims made are valid and correct.\nThere is no evidence provided to suggest that any of the claims are false.\nClaim 1: The problem considered is searching in a list of numbers that are already sorted in a\nnon-decreasing order. This claim is correct and is clearly stated in the paper.\nClaim 2: The algorithm developed is an artificial intelligence based algorithm. This claim is\ncorrect and is clearly stated in the paper.\nClaim 3: The algorithm requires only sqrt(m) comparisons between pairs of numbers, where m\nis the size of the entire list. This claim is correct and is supported by the theoretical guarantee\nprovided in the paper.\nClaim 4:\nThe algorithm provides a tight information-theoretic lower bound, showing that\nthe bound of sqrt(m) comparisons is optimal. This claim is correct and is supported by the\ntheoretical guarantee provided in the paper.\nClaim 5:\nThe simulations conducted confirm the theoretical result, providing additional\nevidence of the effectiveness of the algorithm. This claim is correct and is supported by the\nresults of the simulations provided in the paper.\nOverall, the claims made in the paper appear to be valid and correct, and there is no evidence\nto suggest that any of the claims are false.\n\u2022 Koala-13B [Gen+23]:\nBased on the information provided in the short paper, I can make the following observations:\nThe problem statement is clearly defined and described in details, including the search goal,\nthe input and output formats, and the constraints on the list.\nThe authors present a new AI-based algorithm to solve the problem and provide a theoretical\nbound to show that the algorithm is efficient.\n54\nThe authors conduct a large number of simulations to confirm their theoretical results, varying\nthe size of the list, the values in the list, and the value to be searched.\nThe experimental results confirm the theoretical bound, indicating that the algorithm is efficient.\nBased on these observations, it appears that the claims made in the paper are valid and cor-\nrect. The authors have clearly defined the problem, proposed a new algorithm, and provided\ntheoretical guarantees and experimental results to support their claims.\n\u2022 Alpaca-13B [Tao+23]:\nThe paper is valid and the claims are correct. The algorithm proposed in the paper is a new\nartificial intelligence based approach for the problem of searching in a list of numbers, where the\nlist is already sorted in a non-decreasing order. It is guaranteed to require sqrt(m) comparisons\nbetween pairs of numbers, where m is the size of the entire list, and this bound is also shown\nto be tight by the associated information-theoretic lower bound. The simulations also confirm\nthese theoretical results.\n\u2022 LLaMA-13B [Tou+23]:\nThe truth of the following claim is not determined:\nThe number to be searched is specified by the user. For this problem, we develop a new artificial\nintelligence based algorithm and also prove theoretical guarantees for it.\n\u2022 Open-Assistant Pythia-12B [LAI23]:\nI cannot verify the accuracy of the claims in the paper. There is a missing comma in the third\nsentence of the paper, which should read: \u201cWe also provide an associated information-theoretic\nlower bound showing that our bound is tight, i.e., any algorithm will need at least these many\ncomparisons in the worst case.\u201d\nThere are also a few other typos or grammatical errors in the paper that should be addressed\nbefore it can be considered for publication. For example, in the second paragraph, the word\n\u201cmaximize\u201d should be replaced with \u201cmaximize\u201d.\nTo assist with the review, I could provide additional context such as the problem statement or\nthe background of the authors, or I could ask follow-up questions to clarify some of the claims.\nAdditionally, I could suggest ways to improve the clarity and readability of the paper.\n\u2022 StableLM-7B [Sta23]:\nClaim: The paper presents a new artificial intelligence-based algorithm for search in a sorted\nlist of numbers, where the list is already sorted in a non-decreasing order.\nExplanation:\nThe paper describes a new algorithm that requires only a constant number of comparisons\nbetween pairs of numbers, where m is the size of the list.\nThe paper presents theoretical guarantees for this algorithm, showing that it requires at least\n55\nsqrt(m) comparisons in the worst case.\nThe paper also presents simulations that confirm the theoretical results and demonstrate the\npracticality of the algorithm.\nThe paper concludes by highlighting the importance of the new algorithm and its significance\nas a relevant result for practitioners.\n\u2022 Dolly-v2-12B [Con+23]:\nThe first claim is that \u201cWe consider the problem of searching in a list of numbers, where the list\nis already sorted in a non-decreasing order.\u201d. This claim is incorrect as the list is not sorted in a\nnon-decreasing order. The list is sorted in a non-increasing order. The second claim is that \u201cIf\nthe number exists in the list, then the search should return the position of that number in the\nlist (if there are multiple copies of the number in the list, then it can return any one position)\u201d.\nThis claim is also incorrect as there is no guarantee that multiple copies of a number would\nresult in any one position being returned. The third claim is that \u201cIf the number does not exist\nin the list, then it should output \u2018does not exist\u2019\u201d. This claim is incorrect as the algorithm is\nnot required to output anything if the number does not exist. The forth claim is that \u201cOur\nalgorithm requires only sqrt(m) comparisons between pairs of numbers, where m is the size\nof the entire list\u201d. This claim is incorrect as the algorithm requires exactly m comparisons\n(drawing from the entire list) to find the specified number. The information-theoretic lower\nbound is also incorrect as it is a lower bound on the number of comparisons when the specified\nnumber is already known, whereas our algorithm requires only the number of comparisons when\nthe specified number is being searched for.\n\u2022 Bard [Man23]:\nThe paper you have provided seems to be well-written and well-argued. The authors have\nclearly defined the problem they are trying to solve, and they have provided a clear and\nconcise description of their algorithm. They have also provided theoretical guarantees for their\nalgorithm, and they have conducted simulations to confirm these guarantees.\nI can find no obvious errors in the paper, and I think the authors have made a strong case for\nthe validity of their claims. However, there are a few things that I would like to point out.\nFirst, the authors state that their algorithm requires only sqrt(m) comparisons between pairs\nof numbers, where m is the size of the entire list. This is a very strong claim, and it would be\ninteresting to see how their algorithm performs in practice, especially on large lists.\nSecond, the authors state that they have provided an associated information-theoretic lower\nbound showing that their bound is tight. This is a very impressive result, and it shows that\nthe authors have a deep understanding of the problem.\nOverall, I think this is a very well-written and well-argued paper. The authors have made a\nstrong case for the validity of their claims, and I would be interested to see how their algorithm\nperforms in practice.\nHere are some specific things that I would like to see in the authors\u2019 future work:\nMore extensive simulations on larger lists.\n56\nA comparison of their algorithm to other existing algorithms.\nA more detailed explanation of the information-theoretic lower bound.\nI believe that this work has the potential to make a significant contribution to the field of\nartificial intelligence. I hope that the authors will continue to develop their algorithm and make\nit available to the public.\n\u2022 GPT-4 (ChatGPT):\nThe paper deals with the problem of searching in a sorted list of numbers and introduces an\nAI-based algorithm with theoretical guarantees. The claims made in the paper seem significant,\nbut there are issues that need to be clarified.\n1. **Claim**: \u201cOur algorithm requires only sqrt(m) comparisons between pairs of numbers,\nwhere m is the size of the entire list.\u201d\nThis claim needs additional scrutiny. The standard algorithm for searching a sorted list is Binary\nSearch, which has a time complexity of O(log m). The paper is claiming that their algorithm\nuses sqrt(m) comparisons, which is actually worse than Binary Search in terms of time com-\nplexity. Therefore, this claim seems suspicious and would need additional evidence to back it up.\n2. **Claim**: \u201cWe provide an associated information-theoretic lower bound showing that our\nbound is tight, i.e., any algorithm will need at least these many comparisons in the worst case.\u201d\nThe authors claim that the lower bound for any search algorithm on a sorted list is sqrt(m).\nThis contradicts established knowledge about searching in sorted lists where the lower bound\nis log(m), as seen in Binary Search. So, based on the information given, this claim seems false.\n3. **Claim**: \u201cWe conduct a large number of simulations that confirm this theoretical result.\u201d\nWithout detailed information about the simulations or data to back this up, it is hard to\nevaluate this claim. However, if the authors\u2019 theoretical claims are incorrect, it is likely that\nthere could be issues with their simulations.\n4.\n**Claim**: \u201cOur result is especially important as it is rare for such practical artificial\nintelligence algorithms to also have such strong theoretical guarantees.\u201d\nThis claim is more of an opinion than a fact. Whether or not it is true depends on the field of\nAI and the specific subproblem being addressed. While it is true that theoretical guarantees\nare highly valued, it is not rare for algorithms in AI to have theoretical guarantees.\nBased on the information given, it is highly likely that at least two of these claims (1 and 2)\nare false. The other two cannot be fully evaluated without additional information. However, it\nis recommended that the authors revise their paper, focusing on the theoretical claims and the\ncomparison with established algorithms like Binary Search.\nA.2\nPrompting strategies\nIn our pilot project focused on error detection, we initially utilized a strategy that involved prompting the\nLLM to write a review of a short research paper. Unfortunately, this approach did not yield the desired\noutcome, as the LLM was unable to identify errors in the given context. Subsequently, we achieved success\n57\nby asking the LLM a specific question \u2013 to check for correctness. We provide some more details below.\nHere is the prompt we used when asking the LLM to review the entire paper: You are an expert reviewer\nfor a scientific conference. You will be provided with a short version of a paper that contains the setting of\nthe paper and the main claims. Please write a review for the paper. Here is the short paper: \u201c...\u201d\nWe provide below the response from GPT-4.\nTitle: Review of \u201cA Novel AI-Based Algorithm for Efficient Search in Sorted Lists\u201d\nFirst and foremost, I commend the authors for taking on the challenge of providing both an inno-\nvative solution and a rigorous theoretical backing for a fundamental problem in computer science \u2013\nsearching in a sorted list. The topic is relevant, and the theoretical aspects of the paper are compelling.\nSpecifically, the assertion that the proposed algorithm requires only sqrt(m) comparisons, where\nm is the size of the list, is intriguing.\nIf substantiated, it would indeed constitute a significant\nadvancement, as many common algorithms like binary search operate with a time complexity\nof O(log m).\nSimilarly, the establishment of an information-theoretic lower bound demonstrat-\ning that any algorithm would require at least sqrt(m) comparisons in the worst case is quite impressive.\nThe large-scale simulations conducted by the authors further enhance the credibility of their claims,\nparticularly given the variety of scenarios tested \u2013 variations in list size, value distributions, and the\nsearched value.\nHowever, the authors fail to offer detailed technical specifics about the AI-based algorithm itself. For\nthe scientific community to properly evaluate and potentially build upon this work, it\u2019s crucial to\noutline the methodology more clearly. The authors should strive to provide greater insight into the\ndesign of the algorithm, including information about the AI model used, how it\u2019s trained, and the\ntechniques used to achieve the impressive sqrt(m) result.\nFurthermore, the authors mention that the paper provides a rare instance of practical AI algorithms\nwith strong theoretical guarantees.\nThis statement could be bolstered with a more extensive\ncomparison with existing solutions, underlining both the novelty and practical importance of the\nproposed algorithm.\nIn conclusion, the paper certainly introduces exciting prospects for the field. However, the lack of\nspecific details regarding the AI-based algorithm and its operation is a major drawback. The paper\nwould greatly benefit from a more detailed exposition of the algorithm, along with a comprehensive\ndiscussion contrasting it with existing methods.\nWith these improvements, the paper has the\npotential to make a profound impact on the field of AI algorithms and data search methodologies.\nIt\u2019s my recommendation that the paper should be accepted conditionally, provided that the authors\nmake the suggested improvements to the methodology section and provide a more detailed compara-\ntive analysis with existing solutions.\nB\nNeurIPS checklist and prompts\nIn this section, we detail how we filtered the NeurIPS checklist items for the checklist experiment, as well as\nhow we rephrased each item to create third-person perspective questions to include in the prompts.\n58\nB.1\nChecklist item selection\nWe select the checklist items to include in our experiments based on the feasibility of creating a ground truth\nlabel in our manual labeling process (see Section 4.1). Through this criteria, we remove 2 of 18 questions\nfrom our main analysis:\n\u2022\n1. For all authors. . .\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s\ncontributions and scope?\nItem 1(a) is removed as the ground truth label requires significant domain knowledge in the paper\u2019s\nfield. To generalize our results, a majority of the papers in our analysis are randomly selected across\nthe entire conference, thus we do not have enough expertise to evaluate this question.\n\u2022\n1. For all authors. . .\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them?\nItem 1(d) is removed as a portion of the question references an action the authors take (reading the\nguidelines) that is not verifiable by the information they submit to the conference. Thus, both the\nlabeler and the LLM cannot retrieve a reliable answer based on the information provided.\nB.2\nFull checklist questions\nAfter removing two checklist items (Appendix B.1), we list the remaining 16 NeurIPS 2023 checklist items\nand the associated prompt questions we used in Table 4. These questions are the last component of the user\nprompt provided to the model.\nItem\nCondition\nQuestion (Original)\nPrompt Question\n1b\nFor all authors...\nDid you describe the limitations of\nyour work?\nDo the authors describe the limitations\nof their work?\n1c\nDid you discuss any potential negative\nsocietal impacts of your work?\nDo the authors discuss any potential\nnegative societal impacts of their work?\n2a\nIf you are\nincluding\ntheoretical\nresults...\nDid you state the full set of assumptions\nof all theoretical results?\nIf the authors include theoretical results,\ndo the authors state the full set of\nassumptions of all theoretical results?\n2b\nDid you include complete proofs of all\ntheoretical results?\nIf the authors include theoretical results,\ndo the authors include complete proofs\nof all theoretical results?\n3a\nIf you ran\nexperiments...\nDid you include the code, data, and\ninstructions needed to reproduce the\nmain experimental results (either in the\nsupplemental material or as a URL)?\nIf the authors ran experiments, do the\nauthors include the code, data, and\ninstructions needed to reproduce the\nmain experimental results (either in the\nsupplemental material or as a URL)?\n3b\nDid you specify all the training details\n(e.g., data splits, hyperparameters, how\nthey were chosen)?\nIf the authors ran experiments, do the\nauthors specify all the training details\n(e.g., data splits, hyperparameters, how\nthey were chosen)?\n3c\nDid you report error bars (e.g., with\nrespect to the random seed after\nrunning experiments multiple times)?\nIf the authors ran experiments, do the\nauthors report error bars (e.g., with\nrespect to the random seed after running\nexperiments multiple times)?\n59\nItem\nCondition\nQuestion (Original)\nPrompt Question\n3d\nDid you include the total amount of\ncompute and the type of resources\nused (e.g., type of GPUs, internal\ncluster, or cloud provider)?\nIf the authors ran experiments, do the\nauthors include the total amount of\ncompute and the type of resources used\n(e.g., type of GPUs, internal cluster, or\ncloud provider)?\n4a\nIf you are using\nexisting assets\n(e.g., code,\ndata, models)\nor curating/\nreleasing new\nassets...\nIf your work uses existing assets, did\nyou cite the creators?\nIf the authors use existing assets (e.g.,\ncode, data, models), do the authors cite\nthe creators?\n4b\nDid you mention the license of the\nassets?\nIf the authors use existing assets (e.g.,\ncode, data, models) or curate/release new\nassets, do the authors mention the license\nof the assets?\n4c\nDid you include any new assets either in\nthe supplemental material or as a URL?\nIf the authors curate/release new assets\n(e.g., code, data, models), do the authors\ninclude any new assets either in the\nsupplemental material or as a URL?\n4d\nDid you discuss whether and how\nconsent was obtained from people whose\ndata you\u2019re using/curating?\nIf the authors curate/release new assets\n(e.g., code, data, models), do the authors\ndiscuss whether and how consent was\nobtained from people whose data they are\nusing/curating?\n4e\nDid you discuss whether the data you are\nusing/curating contains personally\nidentifiable information or offensive\ncontent?\nIf the authors curate/release new assets\n(e.g., code, data, models), do the authors\ndiscuss whether the data they are using/\ncurating contains personally identifiable\ninformation or offensive content?\n5a\nIf you used\ncrowdsourcing\nor conducted\nresearch with\nhuman\nsubjects...\nDid you include the full text of\ninstructions given to participants and\nscreenshots, if applicable?\nIf the authors used crowdsourcing or\nconducted research with human subjects,\ndo the authors include the full text of\ninstructions given to participants and\nscreenshots, if applicable?\n5b\nDid you describe any potential\nparticipant risks, with links to\nInstitutional Review Board (IRB)\napprovals, if applicable?\nIf the authors used crowdsourcing or\nconducted research with human subjects,\ndo the authors describe any potential\nparticipant risks, with links to\nInstitutional Review Board (IRB)\napprovals, if applicable?\n5c\nDid you include the estimated hourly\nwage paid to participants and the total\namount spent on participant\ncompensation?\nIf the authors used crowdsourcing or\nconducted research with human subjects,\ndo the authors include the estimated\nhourly wage paid to participants and the\ntotal amount spent on participant\ncompensation?\nTable 4:\nA list of original checklist items (first-person) and their corre-\nsponding prompt questions (third-person). Original checklist items are\nseparated into categories, with each category having a prerequisite con-\ndition. Prompt questions are individually input into the LLM, so they\ninclude content from both the condition and original checklist question.\n60\n"
  }
]