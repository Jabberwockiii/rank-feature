[
  {
    "title": "PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models",
    "link": "https://arxiv.org/pdf/2309.05793.pdf",
    "upvote": "50",
    "text": "PhotoVerse: Tuning-Free Image Customization with Text-to-Image\nDiffusion Models\nLi Chen1*, Mengyi Zhao1, 2*, Yiheng Liu1*, Mingxu Ding1,\nYangyang Song1, Shizun Wang1, 3, Xu Wang1, Hao Yang1, Jing Liu1, Kang Du1, Min Zheng1\n1ByteDance, Beijing, China.\n2Beihang University, Beijing, China.\n3National University of Singapore.\n{chenli.phd, liuyiheng.yolo, dingmingxu.leo, songyangyang.2021, wangxu.ailab, jing.liu,\ndukang.daniel, zhengmin.666}@bytedance.com, zhaomengyi@buaa.edu.cn,\nshizun.wang@u.nus.edu, yanghao.alexis@foxmail.com\nAbstract\nPersonalized text-to-image generation has emerged as a pow-\nerful and sought-after tool, empowering users to create cus-\ntomized images based on their specific concepts and prompts.\nHowever, existing approaches to personalization encounter\nmultiple challenges, including long tuning times, large stor-\nage requirements, the necessity for multiple input images per\nidentity, and limitations in preserving identity and editabil-\nity. To address these obstacles, we present PhotoVerse, an in-\nnovative methodology that incorporates a dual-branch condi-\ntioning mechanism in both text and image domains, provid-\ning effective control over the image generation process. Fur-\nthermore, we introduce facial identity loss as a novel com-\nponent to enhance the preservation of identity during train-\ning. Remarkably, our proposed PhotoVerse eliminates the\nneed for test time tuning and relies solely on a single fa-\ncial photo of the target identity, significantly reducing the re-\nsource cost associated with image generation. After a single\ntraining phase, our approach enables generating high-quality\nimages within only a few seconds. Moreover, our method\ncan produce diverse images that encompass various scenes\nand styles. The extensive evaluation demonstrates the supe-\nrior performance of our approach, which achieves the dual\nobjectives of preserving identity and facilitating editability.\nProject page: https://photoverse2d.github.io/\nIntroduction\nThe remarkable advancements in text-to-image models, e.g.,\nImagen (Saharia et al. 2022), DALL-E2 (Ramesh et al.\n2022), and Stable Diffusion (Rombach et al. 2022), have\ngarnered significant attention for their ability to generate\nphotorealistic images based on natural language prompts.\nDespite the impressive capability of these models to gen-\nerate diverse and sophisticated images by leveraging large-\nscale text-image datasets, they encounter difficulties when it\ncomes to synthesizing desired novel concepts. Consider the\nscenario where users aim to incorporate themselves, family\nmembers, or friends into a new scene, achieving the desired\nlevel of fidelity solely through text descriptions becomes\nchallenging. This challenge stems from the fact that these\n*These authors contributed equally.\nFigure 1: Our proposed PhotoVerse exhibits a wide range of\ndiverse results. By providing a single reference image fea-\nturing the target concept alongside various prompts, Pho-\ntoVerse facilitates the generation of high-quality images\nthat align seamlessly with the given prompts. Notably, our\nmethod achieves this outcome within a matter of seconds,\neliminating the need for test-time tuning.\nnovel concepts were absent from the dataset used for train-\ning the models, making it hard to generate accurate repre-\nsentations based solely on textual information. In the pur-\nsuit of enabling personalized text-to-image synthesis, sev-\neral methods e.g., Dreambooth (Ruiz et al. 2023a), Textual\nInversion (Gal et al. 2022), DreamArtist (Dong, Wei, and\nLin 2022), and CustomDiffusion (Kumari et al. 2023) pri-\nmarily focused on identity preservation and propose the in-\nverse transformation of reference images into the pseudo\nword through per-subject optimization. Text-to-image mod-\nels undergo joint fine-tuning to enhance fidelity. The result-\ning optimized pseudo word can then be leveraged in new\nprompts to generate scenes incorporating the specified con-\ncepts. However, this optimization-based paradigm often re-\nquires expensive computational resources and large stor-\nage requirements, taking minutes to hours when executed\non high-end GPUs. Such lengthy processing times and tun-\ning processes render it impractical for user-friendly usage,\nwhere short time generation is desirable. Also, these ap-\nproaches had limitations in their language editing capabil-\nities due to potential overfitting on a tiny identity-specific\nimage dataset. Recent encoder-based methods e.g., E4T (Gal\net al. 2023a), InstantBooth (Shi et al. 2023), SuTI (Chen\narXiv:2309.05793v1  [cs.CV]  11 Sep 2023\net al. 2023), Profusion (Zhou et al. 2023) aimed to address\nthese challenges by optimizing pseudo token embeddings,\nintroducing sampling steps, or incorporating new modules\nfor concept injection. However, challenges persist, such as\nthe need for multiple input images and test-time tuning\n(Sohn et al. 2023) to maintain identity preservation and ed-\nitability.\nTo address the aforementioned issues, we present a\nnovel methodology that leverages a dual-branch condition-\ning mechanism, combining improved identity textual em-\nbeddings and spatial concept cues through dual-modality\nadapters in both the text and image domains. Furthermore,\nwe introduce a facial identity loss component during train-\ning to enhance identity preservation. Our method stands out\nby effectively balancing high identity similarity and robust\nediting capabilities. Crucially, our approach achieves a re-\nmarkable reduction in generation time for personalized text-\nto-image models, completing the process in just a few sec-\nonds and utilizing only a single concept photo. This signif-\nicant advancement in efficiency streamlines the generation\nworkflow and enhances the user experience.\nIn summary, our contributions can be categorized into\nthree key aspects:\n\u2022 We propose a novel architecture for user-friendly text-to-\nimage personalization. Our approach eliminates the need\nfor test-time tuning and requires only a single image of\nthe target subject. This results in a rapid and effortless\ngeneration, typically completed in \u223c 5 seconds.\n\u2022 We introduce a dual-branch concept injection paradigm\nthat effectively extracts identity information from both\ntextual embeddings and visual representations. By lever-\naging this approach, we enhance the preservation of\nidentity during training. Additionally, we incorporate a\nface identity loss component to further facilitate identity\npreservation throughout the training process.\n\u2022 We demonstrate the exceptional quality of our method\nin maintaining identity while capturing rich details and\nessential attributes, such as facial features, expressions,\nhair color and hairstyle. Our approach not only ensures\nidentity preservation but also preserves editability. It em-\npowers diverse stylization, image editing, and new scene\ngeneration in accordance with the provided prompts.\nRelated Work\nText-to-Image Synthesis\nThe text-to-image synthesis re-\nlies on deep generative models such as Generative Adver-\nsarial Networks (GANs) (Xia et al. 2021; Kang et al. 2023),\nautoregressive models (Ramesh et al. 2021), and diffusion\nmodels (Ho, Jain, and Abbeel 2020; Rombach et al. 2022).\nInitially, prior works primarily focused on generating im-\nages under specific domain and text conditions, limiting\ntheir applicability. However, with the advent of large-scale\nimage-text datasets and powerful language encoders, text-\nto-image synthesis has achieved remarkable advancements.\nDALL-E (Ramesh et al. 2021) was the first approach uti-\nlizing an autoregressive model to generate diverse and in-\ntricate images from arbitrary natural language descriptions.\nThis methodology served as the foundation for subsequent\nmethods like Make-A-Scene (Gafni et al. 2022), CogView\n(Ding et al. 2021), Parti (Yu et al. 2022), Muse (Chang et al.\n2023), and others.\nHowever, the pioneering work of GLIDE (Nichol\net al. 2021) introduced diffusion models, surpassing the\nautoregressive-based DALL-E in producing more photo-\nrealistic and high-resolution images. Consequently, diffu-\nsion models have gradually emerged as the predominant\napproach for text-to-image synthesis. Subsequent advance-\nments, such as DALL-E2 (Ramesh et al. 2022), Imagen (Sa-\nharia et al. 2022), and LDM (Rombach et al. 2022), have\nfurther improved diffusion models in terms of photorealism,\nlanguage comprehension, and generation diversity. Notably,\nthe release of Stable Diffusion as an open-source model has\npropelled its widespread adoption, making it the most ex-\ntensively utilized text-to-image model. This has also led to\nthe creation of numerous fine-tuned models by the research\ncommunity. Given this context, we employ Stable Diffusion\nin our experiments.\nImage Inversion\nIn the domain of generative models, the\nability to invert an image into a latent code that accurately\nreconstructs the original image holds significant importance.\nThis capability facilitates various downstream applications\nthat rely on manipulating the latent code to enable tasks\nsuch as image editing, translation, customization, and over-\nall control over image generation. In the literature on GAN\ninversion, there are primarily two approaches: optimization-\nbased inversion, involves directly optimizing the latent code\nto minimize image reconstruction error. While this method\nachieves high fidelity, its drawback lies in the requirement of\na hundred times iterations, rendering it unsuitable for real-\ntime applications. Encoder-based inversion, trains a neural\nnetwork to predict the latent code. Once trained, the encoder\nperforms a single forward pass to obtain a generalized result,\noffering improved efficiency.\nThe inversion of diffusion models also draws inspiration\nfrom the aforementioned methods. However, due to the iter-\native nature of diffusion model-based image generation, in-\nverting an image into the noise space associated with the im-\nage (e.g., DDIM (Song, Meng, and Ermon 2020), DDIB (Su\net al. 2022), ILVR (Choi et al. 2021), CycleDiffusion (Wu\nand De la Torre 2022)) results in a latent space that is not\nas decoupled and smooth as the latent space of GANs. Con-\nsequently, identity preservation suffers. Alternatively, some\nworks explore performing inversion in a different latent\nspace, such as textual embedding space (Gal et al. 2022).\nThis space exhibits strong semantic information and better\npreserves the object\u2019s characteristics. In our research, we\nemploy an encoder-based approach to achieve instant inver-\nsion in the text embedding space. And the method is fur-\nther extended to conditioning on visual features, which can\nquickly capture the image in multi-modality, and realize fast\ngeneration.\nPersonalization\nBy leveraging the generative capabili-\nties of pre-trained text-to-image models, Personalization of-\nfers users the ability to synthesize specific unseen con-\ncepts within new scenes using reference images. Typically,\nthe unseen concept is transformed into a pseudo word\n(e.g., S\u2217) within the textual embedding space (Ruiz et al.\n2023a; Gal et al. 2022). Optimization-based methods, such\nas DreamArtist (Dong, Wei, and Lin 2022), directly opti-\nmize the pseudo word to establish a mapping between user-\nprovided images and textual inversion. Other approaches,\ne.g., Dreambooth (Ruiz et al. 2023a) and CustomDiffusion\n(Kumari et al. 2023) employ fine-tuning of text-to-image\nmodels to enhance fidelity. However, these strategies require\nminutes to hours for concept-specific optimization. In con-\ntrast, encoder-based methods such as E4T (Gal et al. 2023a),\nInstantBooth (Shi et al. 2023), Profusion (Zhou et al. 2023)\ntrain an encoder to predict the pseudo word, enabling the\ngeneration of personalized images within a few fine-tuning\nsteps.\nNonetheless, tuning the entire model entails substantial\nstorage costs and often results in overfitting on specific con-\ncepts. Moreover, many approaches (Gal et al. 2022; Ruiz\net al. 2023a; Kumari et al. 2023; Shi et al. 2023; Chen et al.\n2023) rely on multiple reference images, which is not always\nthe case users could provide. To address these limitations,\nour work mitigates these imperfections through the utiliza-\ntion of the parameter-efficient fine-tuning technique and the\ndesign of an encoder that can perform the personalization\ntask with only one reference image, enhancing the efficiency\nand effectiveness of the synthesis process.\nMethodology\nThe objective of personalized text-to-image synthesis is to\ntrain models to learn specific concepts through reference im-\nages and subsequently generate new images based on text\nprompts. In our paper, we aim to achieve instantaneous and\noptimization-free personalization using only a single refer-\nence image. To accomplish this, we propose a novel ap-\nproach for image customization by integrating a dual-branch\nconditioning mechanism in both the textual and visual do-\nmains. This involves designing adapters to project the refer-\nence image into a pseudo word and image feature that ac-\ncurately represents the concept. These concepts are then in-\njected into the text-to-image model to enhance the fidelity of\nthe generated personalized appearance.\nTo enable this process, we incorporate the original text-\nto-image model with concept conditions and train it within\na concept scope, supplemented by an additional face iden-\ntity loss. A high-level overview of our proposed method is\ndepicted in Figure 2.\nPreliminary\nWe utilize Stable Diffusion (Rombach et al. 2022) as our\nbase text-to-image model, which has demonstrated stabil-\nity and reliability in the field. This model is trained on a\nlarge-scale dataset of image-text pairs, enabling it to gener-\nate high-quality and diverse images that align with the given\ntext prompts. Stable Diffusion, based on the Latent Diffu-\nsion Model (LDM) architecture, comprises two crucial com-\nponents.\nFirstly, an autoencoder (E, D) is trained on a large-scale\nimage dataset to compress images. The encoder E maps an\nimage x from the pixel space to a low-dimensional latent\nFigure 2: Overview of our proposed PhotoVerse.\nspace z = E(x). The decoder D is responsible for recon-\nstructing the latent representation z back into an image with\nhigh fidelity, aiming to achieve D(E(x)) \u2248 x.\nSecondly, a denoising network E, utilizing the UNet\n(Ronneberger, Fischer, and Brox 2015) architecture as the\nbackbone, is trained to perform the diffusion process in the\nlatent space. This approach offers computational and mem-\nory advantages compared to operating directly in the pixel\nspace. Additionally, the diffusion model incorporates a text\ncondition y for text-to-image synthesis. It employs a CLIP\ntext encoder c\u03b8 to project the condition y into an interme-\ndiate representation c\u03b8(y). This representation is then em-\nployed in the intermediate layers of the UNet through a\ncross-attention mechanism:\nAttn(Q, K, V ) = Softmax(QKT\n\u221a\nd\u2032 )V,\n(1)\nwhere Q = WQ \u00b7\u03c6 (zt) , K = WK \u00b7c\u03b8(y), V = WV \u00b7c\u03b8(y),\n\u03c6 (zt) is the hidden states inside Unet, zt is the latent noised\nto time t, d\u2032 corresponds to the scale factor utilized for atten-\ntion mechanisms. The training objective of the latent diffu-\nsion model is to predict the noise that is added to the latent\nof the image, which is formulated as:\nLdiffusion = Ez\u223cE(x),y,\u03f5\u223cN (0,I),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8 (zt, t, c\u03b8(y))\u22252\n2\ni\n,\n(2)\nhere \u03f5 represents the unscaled noise sample, and E denotes\nthe denoising network. During inference, a random Gaus-\nsian noise zT is sampled, and through a series of T iterative\ndenoising steps, it is transformed into z\u2032\n0. Subsequently, the\ndecoder D is employed to generate the final image, given by\nx\u2032 = D(z\u2032\n0).\nDual-branch Concept Extraction\nPrior to extracting facial features, it is essential to prepro-\ncess the input images. In the preprocessing stage, Firstly,\na face detection algorithm was applied to identify and lo-\ncalize human faces within the input image x. This step en-\nsured that the subsequent analysis focused solely on facial\nregions of interest. Additionally, to provide a slightly larger\nregion for feature extraction, the bounding boxes around the\ndetected faces were expanded by a scaling factor i.e., 1.3.\nThis expansion ensured that important facial details were\ncaptured within the region of interest. Subsequently, the ex-\npanded regions are cropped as a new face image xm for sub-\nsequent feature extraction. Furthermore, to meet the specific\nimage generation requirements of the diffusion model, the\nexpanded facial regions were resized to the desired shape.\nMoreover, to remove non-facial elements and focus solely\non the facial attributes, a mask was applied to the resized fa-\ncial regions. This mask effectively masked out irrelevant ar-\neas such as background and accessories, ensuring that subse-\nquent identity feature extraction conduct efficiently. For the\ndenoising image during training of the Diffusion model, we\nalso employ the cropped face image with a scaling factor of\n3 as xt.\nTextual Condition. To personalize a specific concept that\ncannot be adequately described by existing words, we adopt\nthe approach proposed in (Gal et al. 2022; Wei et al. 2023)\nto embed the reference image into the textual word embed-\nding space. In this process, we summarize the concept using\npseudo-words denoted as S\u2217. Initially, we utilize the CLIP\nimage encoder, the same encoder employed in Stable Dif-\nfusion, to extract the features of the reference image. Fol-\nlowing the technique described in (Wei et al. 2023), we en-\nhance the representational capability of the image tokens as\nwell as the editability of the model by selecting features af-\nter m layers from the CLIP image encoder, which capture\nspatial and semantic information at various hierarchical lev-\nels of abstraction and concreteness. Subsequently, a multi-\nadapter architecture is employed to translate the image fea-\ntures from each layer into multi-word embeddings, resulting\nin S\u2217 = S1, ..., Sm. Since CLIP effectively aligns textual\nembeddings and image features, each text adapter consists of\nonly two MLP layers with non-linear activations, making it\nlightweight and enabling fast representation translation. This\ndesign choice leverages the existing alignment provided by\nCLIP, ensuring efficient and accurate transformation of im-\nage features into textual embeddings.\nVisual Condition. Despite the advantages of condition in\nthe textual embedding space, there are certain limitations to\nconsider. For instance, the performance can be influenced\nby the encoder ability of the following text encoder, and the\nsemantic information in the text space tends to be abstract\nwhich leads to higher requirements for token representa-\ntion capabilities. Consequently, we propose the incorpora-\ntion of the condition from the image space as an auxiliary\naid, which is more specific for the model to understand new\nFigure 3: Concept injection mechanism in the cross-\nattention module of UNet.\nconcepts and contributes to the effective learning of pseudo-\ntokens in the text space. To accomplish this, we utilize the\nfeature obtained from the CLIP image encoder. These fea-\ntures are then mapped using an image adapter, which fol-\nlows the same structural design as the text adapter. The re-\nsulting feature capture essential visual cues related to iden-\ntity, enabling a more accurate representation of the desired\nattributes during image generation.\nDual-branch Concept Injection\nThe original diffusion model might lack the necessary fea-\ntures required for constructing novel concepts. Thus, we pro-\npose injecting the new concept into the text-to-image synthe-\nsis process by fine-tuning the diffusion model to recover per-\nsonal appearance concepts with a high level of detail. Rather\nthan fine-tuning the entire UNet, which can be computa-\ntionally expensive and potentially reduce model editability\ndue to overfitting, we only add conditions and fine-tune the\nweights in the cross-attention module. Previous studies, such\nas E4T (Gal et al. 2023a) and Custom Diffusion (Kumari\net al. 2023) have also highlighted that the most expressive\nmodel parameters reside in attention layers.\nAs shown in Figure 3, for the textual conditioning branch,\nwe incorporate the Parameter-Efficient Fine-Tuning (PEFT)\nmethod i.e., LoRA (Hu et al. 2021), a more efficient tun-\ning approach. LoRA was originally designed to adapt large-\nscale pre-trained natural language models to new down-\nstream tasks or domains while preserving performance.\nLoRA works by freezing the pre-trained model weights and\nintroducing new trainable rank decomposition matrices into\neach layer of the model architecture. Consequently, we have\nKT = Wk\nT p + \u03b1\u2206Wk\nT p and V T = Wv\nT p + \u03b1\u2206Wv\nT p,\nwhere p represents the text feature extracted from the text\nencoder, \u2206Wk\nT and \u2206Wv\nT denote the low-rank decompo-\nsition \u2206W = BA, with A and B containing trainable pa-\nrameters. Regarding the visual conditioning branch, KS =\nWk\nSf and V S = Wv\nSf, where f corresponds to the im-\nage representation obtained after passing through the image\nadapter.\nThen we present a random fusion strategy for the multi-\nmodality branches:\nO = \u03b3Attn(Q, KT , V T ) + \u03c3Attn(Q, KS, V S),\n(3)\nwhere \u03b3 and \u03c3 denote two scale factors that regulate the in-\nfluence of control. A random seed is sampled from the uni-\nform distribution U = (0, 1), the fused representation can be\nobtained in the following manner:\nO =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\u03b3Attn(Q, KT , V T ),\nif seed < r1; \u03b3 = 2\n\u03c3Attn(Q, KS, V S),\nif seed > r2; \u03c3 = 2\n\u03b3Attn(Q, KT , V T )+\n\u03c3Attn(Q, KS, V S),\notherwise ; \u03b3 = \u03c3 = 1\n(4)\nwhere r1 and r2 is the threshold of random seed.\nIn addition, we introduce regularization for both persuade\ntoken embedding after text adapter pf and reference image\nvalues V S. Specifically,\nLT\nreg = Mean\u2225pf\u22251,\n(5)\nand\nLS\nreg = Mean\u2225V S\u22251.\n(6)\nThe whole pipeline can be trained as usual LDM (Rom-\nbach et al. 2022) does, except for additional facial identity\npreserving loss Lface:\nLface = C(f(x) \u2212 f(x\u2032)).\n(7)\nHere, f(\u00b7) represents a domain-specific feature extractor, x\ndenotes the reference image, and x\u2032 corresponds to the de-\nnoised image with the prompt \u201ca photo of S\u2217\u201d. To achieve\nthe goal of measuring identity in human face scenarios, we\nemploy the Arcface face recognition approach (Deng et al.\n2019). The function C(\u00b7) computes the cosine similarity of\nfeatures extracted from the face region in the images. Sub-\nsequently, the total training loss is formulated as:\nLtotal = Ldiffusion +\u03bbfaceLface +\u03bbrtLT\nreg +\u03bbrvLS\nreg, (8)\nwhere \u03bbface is the scale factor of Lface, while \u03bbrt and \u03bbrv\nare hyperparameters that control the sparsity level for the\nspecific feature.\nOverall, the lightweight adapters and UNet are jointly\ntrained on public datasets within a concept scope, such as\n(Karras, Laine, and Aila 2019). In this way, PhotoVerse can\nlearn how to recognize novel concepts. At inference time,\nfine-tuning the model is obviated, thereby enabling fast per-\nsonalization based on user-provided textual prompts.\nExperiments\nExperimental settings\nDatasets. We fine-tune our model on three public datasets\nFairface (K\u00a8arkk\u00a8ainen and Joo 2019), CelebA-HQ (Karras\net al. 2017) and FFHQ (Karras, Laine, and Aila 2019). Eval-\nuation is conducted on a self-collected dataset, which con-\ntains 326 images with balanced racial and gender samples\nfor 4 colors: White, Black, Brown, and Yellow from all the\nrace groups.\nFor the evaluation phase, we generate five images for each\nphoto with the default inversion prompt \u201ca photo of S\u2217\u201d.\nThis allows us to robustly measure the performances and\ngeneralization capabilities of our method.\nImplementation Details. For the text-to-image diffusion\nmodel, we utilize Stable Diffusion (Rombach et al. 2022)\nas the base model and fine-tune adapters, loRA as well as\nvisual attention weights. We pre-train the model with learn-\ning rate of 1e \u2212 4 and batch size of 64 on V100 GPU.\nOur model was trained for 60, 000 steps. During training,\n\u03b1 = 1, \u03bbface = 0.01, \u03bbrt = 0.01, \u03bbrv = 0.001, m = 5,\nr1 =\n1\n3, r2 =\n2\n3. For evaluation, we set m = 1, \u03b1 = 1\nduring sampling. In addition, one can adjust parameters m,\n\u03c3, \u03b3, and \u03b1 with more flexibility to achieve desired gener-\nation objectives, such as preserving more identity for photo\nediting or incorporating stronger style effects for the Ghi-\nbli anime style. By customizing these parameters, one can\nset the level of ID information retention and the degree of\nstylization according to specific preferences and application\nrequirements. Besides, one can also integrate the image-to-\nimage trick during sampling to reduce randomness, which\nmeans replacing initial noise zT with zt and sampling from\ntime step t, here zt is derived from the noisy image of x0.\nEvaluation Metrics. In this paper, we aim to assess the abil-\nity of generated results to preserve identity by employing\nfacial ID similarity as an evaluation metric. Specifically, we\nutilize a facial recognition model i.e., Arcface (Deng et al.\n2019) to compute the cosine similarity between facial fea-\ntures. The ID similarity serves as a quantitative measure to\ndetermine the extent to which the generated outputs retain\nthe original identity information.\nQuantitative Results\nAs indicated in Table 1, we conducted an evaluation to as-\nsess the facial identity similarity in generated facial images.\nThe evaluation involved measuring the ID similarity across\ndiverse ethnicities by employing cosine similarity on facial\nfeatures. The obtained results substantiate the efficacy of our\napproach, which achieves a notable level of similarity, with\nan average value of 0.696. Remarkably, certain ethnicities,\nincluding brown and white, exhibit similarity scores surpass-\ning 0.7, with white ethnicity demonstrating the highest sim-\nilarity. We posit that this discrepancy could be attributed to\nthe inherent bias of the pre-trained large model. Regardless,\nit is noteworthy that our method demonstrates consistent and\nrobust performance across all ethnicities, with a marginal\ndeviation of approximately 0.03 observed among different\nethnic groups.\nQualitative Results\nWithin Figure 4, we present a comprehensive comparison\nof our proposed PhotoVerse method alongside state-of-the-\nart (SOTA) personalization techniques, namely DreamBooth\nFigure 4: Comparison with State-of-the-Art methods. Our proposed PhotoVerse shows superior performance in preserving\nidentity attributes and generating high-quality images. Notice that DreamBooth, Textual Inversion, E4T and ProFusion require\nan additional stage of fine-tuning on the provided reference image prior to generation. In contrast, our PhotoVerse is tuning-free\nand boasts rapid generation speed, offering a distinct advantage in terms of efficiency and user convenience.\n(Ruiz et al. 2023a), Textual Inversion (Gal et al. 2022), E4T\n(Gal et al. 2023b), and ProFusion (Zhou et al. 2023), fo-\ncusing on qualitative results. The results of corresponding\nmethods are directly taken from (Zhou et al. 2023). No-\ntably, all aforementioned methods require test time tuning.\nFor instance, DreamBooth and Textual Inversion necessi-\ntate 3-5 photos per subject, which incurs considerable time\nand storage requirements for personalization. DreamBooth,\nin particular, demands approximately 5 minutes, while Tex-\ntual Inversion entails 5000 steps for fine-tuning. E4T and\nProFusion allow customization with just 1 reference image,\nbut they still require additional time for fine-tuning, approx-\nimately 30 seconds (Zhou et al. 2023). In contrast, our pro-\nposed approach is test time tuning-free, enabling the synthe-\nsis of 5 images within a mere 25 seconds. This remarkable\nefficiency makes our method exceedingly user-friendly, sig-\nnificantly enhancing the user experience.\nFurthermore, concerning the preservation of identity at-\ntributes in the reference image, our PhotoVerse (shown in the\nlast column) exhibits exceptional proficiency in capturing fa-\ncial identity information. Our approach successfully retains\ncrucial subject features, including facial features, expres-\nsions, hair color, and hairstyle. For instance, when compared\nto alternative techniques, our proposed method outperforms\nin restoring intricate hair details while effectively preserv-\ning facial features, as evident in the first row. Additionally,\nas observed in the second row, our approach excels at fully\nrestoring facial features while maintaining consistency with\nthe desired \u201cManga\u201d style specified in the prompt. In con-\ntrast, the Profusion-generated photo exhibits blurry mouth\ndetails, while E4T fails to exhibit a sufficiently pronounced\n\u201cManga\u201d style. Shifting to the third row, our results success-\nfully capture characteristic expressions present in the input\nimages, such as frowning.\nFigure 5: Results of our PhotoVerse with various prompts\nfor stylization and new scene generation.\nRegarding the quality of generated images, our method\nsurpasses other works by producing noticeably sharper im-\nages with enhanced detail. Moreover, our approach exhibits\na high degree of aesthetic appeal, featuring natural lighting,\nnatural colors, and the absence of artifacts.\nFigure 5 presents additional results showcasing the per-\nformance of our approach across different ethnicities in im-\nage editing, image stylization, and novel scene generation,\nfurther substantiating the three aforementioned aspects of\nsuperiority.\nAblation studies\nEffect of Visual conditioning branch According to Table 1,\nit is evident that the image branch has a substantial impact\nMethod\nBlack\nBrown\nWhite\nYellow\nAll\nw/o visual conditioning branch\n0.561\n0.563\n0.584\n0.556\n0.556\nw/o LS\nreg\n0.566\n0.573\n0.589\n0.550\n0.569\nw/o Lface\n0.632\n0.658\n0.663\n0.622\n0.643\nw/o LT\nreg\n0.650\n0.668\n0.678\n0.657\n0.663\nPhotoVerse\n0.685\n0.702\n0.715\n0.682\n0.696\nTable 1: Ablation study results of 4 components on ID simi-\nlarity of 4 races.\non the preservation of ID information. Removing the image\nbranch leads to a loss of 0.14, indicating its crucial role in the\nmodel\u2019s ability to maintain identity consistency. This effect\ncan be attributed to the provision of specific, detailed, and\nspatial conditions by the visual branch during image gener-\nation. Moreover, the visual branch contributes positively to\noptimizing the embedding of textual tokens.\nEffect of Regularizations The experimental results illus-\ntrate the importance of regularizations for visual values and\ntextual facial embeddings during concept injection. It can\npromote the sparsity of representations, thereby retaining\nkey values and mitigating overfitting issues, as well as en-\nhancing the generalization capability of our model.\nEffect of Face ID loss We also evaluated the impact of face\nID loss on the preservation of identity information. The ex-\nperimental results demonstrate that it also plays an important\nrole in maintaining identity, resulting in an improvement of\n0.05 in performance.\nConclusions\nIn this paper, we introduced an innovative methodology that\nincorporates a dual-branch conditioning mechanism in both\nthe text and image domains. This approach provides effec-\ntive control over the image generation process. Additionally,\nwe introduced facial identity loss as a novel component to\nenhance identity preservation during training. Remarkably,\nour proposed PhotoVerse eliminates the need for test-time\ntuning and relies solely on a single facial photo of the tar-\nget identity. This significantly reduces the resource costs as-\nsociated with image generation. Following a single training\nphase, our approach enables the generation of high-quality\nimages within just a few seconds. Moreover, our method\nexcels in producing diverse images encompassing various\nscenes and styles. Our approach also supports incorporat-\ning other methods such as ControlNet (Zhang and Agrawala\n2023), specifically leveraging its control branch for pre-\nserving the overall high-level structure, further enhancing\nthe pose control of personalized text-to-image generation.\nThrough extensive evaluation, we have demonstrated the su-\nperior performance of our approach in achieving the dual\nobjectives of preserving identity and facilitating editabil-\nity. The results highlight the potential and effectiveness of\nPhotoVerse as a promising solution for personalized text-\nto-image generation, addressing the limitations of existing\nmethods and paving the way for enhanced user experiences\nin this domain.\nReferences\nChang, H.; Zhang, H.; Barber, J.; Maschinot, A.; Lezama,\nJ.; Jiang, L.; Yang, M.-H.; Murphy, K.; Freeman, W. T.;\nRubinstein, M.; et al. 2023.\nMuse: Text-to-image gener-\nation via masked generative transformers.\narXiv preprint\narXiv:2301.00704.\nChen, W.; Hu, H.; Li, Y.; Rui, N.; Jia, X.; Chang, M.-\nW.; and Cohen, W. W. 2023. Subject-driven text-to-image\ngeneration via apprenticeship learning.\narXiv preprint\narXiv:2304.00186.\nChoi, J.; Kim, S.; Jeong, Y.; Gwon, Y.; and Yoon, S. 2021.\nIlvr: Conditioning method for denoising diffusion proba-\nbilistic models. arXiv preprint arXiv:2108.02938.\nDeng, J.; Guo, J.; Xue, N.; and Zafeiriou, S. 2019. Arcface:\nAdditive angular margin loss for deep face recognition. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, 4690\u20134699.\nDing, M.; Yang, Z.; Hong, W.; Zheng, W.; Zhou, C.; Yin,\nD.; Lin, J.; Zou, X.; Shao, Z.; Yang, H.; et al. 2021.\nCogview: Mastering text-to-image generation via transform-\ners. Advances in Neural Information Processing Systems,\n34: 19822\u201319835.\nDong, Z.; Wei, P.; and Lin, L. 2022. Dreamartist: Towards\ncontrollable one-shot text-to-image generation via con-\ntrastive prompt-tuning. arXiv preprint arXiv:2211.11337.\nGafni, O.; Polyak, A.; Ashual, O.; Sheynin, S.; Parikh, D.;\nand Taigman, Y. 2022. Make-a-scene: Scene-based text-to-\nimage generation with human priors. In European Confer-\nence on Computer Vision, 89\u2013106. Springer.\nGal, R.; Alaluf, Y.; Atzmon, Y.; Patashnik, O.; Bermano,\nA. H.; Chechik, G.; and Cohen-Or, D. 2022. An image is\nworth one word: Personalizing text-to-image generation us-\ning textual inversion. arXiv preprint arXiv:2208.01618.\nGal, R.; Arar, M.; Atzmon, Y.; Bermano, A. H.; Chechik,\nG.; and Cohen-Or, D. 2023a. Encoder-based Domain Tun-\ning for Fast Personalization of Text-to-Image Models. ACM\nTransactions on Graphics (TOG), 42(4): 1\u201313.\nGal, R.; Arar, M.; Atzmon, Y.; Bermano, A. H.; Chechik,\nG.; and Cohen-Or, D. 2023b. Encoder-based Domain Tun-\ning for Fast Personalization of Text-to-Image Models. ACM\nTransactions on Graphics (TOG), 42(4): 1\u201313.\nHo, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion\nprobabilistic models. Advances in neural information pro-\ncessing systems, 33: 6840\u20136851.\nHu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang,\nS.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation\nof large language models. arXiv preprint arXiv:2106.09685.\nKang, M.; Zhu, J.-Y.; Zhang, R.; Park, J.; Shechtman, E.;\nParis, S.; and Park, T. 2023. Scaling up gans for text-to-\nimage synthesis. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 10124\u2013\n10134.\nK\u00a8arkk\u00a8ainen, K.; and Joo, J. 2019. Fairface: Face attribute\ndataset for balanced race, gender, and age. arXiv preprint\narXiv:1908.04913.\nKarras, T.; Aila, T.; Laine, S.; and Lehtinen, J. 2017. Pro-\ngressive growing of gans for improved quality, stability, and\nvariation. arXiv preprint arXiv:1710.10196.\nKarras, T.; Laine, S.; and Aila, T. 2019. A style-based gen-\nerator architecture for generative adversarial networks. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, 4401\u20134410.\nKumari, N.; Zhang, B.; Zhang, R.; Shechtman, E.; and Zhu,\nJ.-Y. 2023.\nMulti-concept customization of text-to-image\ndiffusion. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 1931\u20131941.\nMa, J.; Liang, J.; Chen, C.; and Lu, H. 2023.\nSubject-\nDiffusion: Open Domain Personalized Text-to-Image Gen-\neration without Test-time Fine-tuning.\narXiv preprint\narXiv:2307.11410.\nNichol, A.; Dhariwal, P.; Ramesh, A.; Shyam, P.; Mishkin,\nP.; McGrew, B.; Sutskever, I.; and Chen, M. 2021. Glide: To-\nwards photorealistic image generation and editing with text-\nguided diffusion models. arXiv preprint arXiv:2112.10741.\nRamesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen, M.\n2022. Hierarchical text-conditional image generation with\nclip latents. arXiv preprint arXiv:2204.06125.\nRamesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Rad-\nford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-to-\nimage generation. In International Conference on Machine\nLearning, 8821\u20138831. PMLR.\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\nmer, B. 2022. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, 10684\u2013\n10695.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015.\nU-net:\nConvolutional networks for biomedical image segmenta-\ntion. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, 234\u2013241. Springer.\nRuiz, N.; Li, Y.; Jampani, V.; Pritch, Y.; Rubinstein, M.; and\nAberman, K. 2023a. Dreambooth: Fine tuning text-to-image\ndiffusion models for subject-driven generation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 22500\u201322510.\nRuiz, N.; Li, Y.; Jampani, V.; Wei, W.; Hou, T.; Pritch, Y.;\nWadhwa, N.; Rubinstein, M.; and Aberman, K. 2023b. Hy-\nperdreambooth: Hypernetworks for fast personalization of\ntext-to-image models. arXiv preprint arXiv:2307.06949.\nSaharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton,\nE. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan,\nB.; Salimans, T.; et al. 2022. Photorealistic text-to-image\ndiffusion models with deep language understanding.\nAd-\nvances in Neural Information Processing Systems, 35:\n36479\u201336494.\nShi, J.; Xiong, W.; Lin, Z.; and Jung, H. J. 2023. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. arXiv preprint arXiv:2304.03411.\nSohn, K.; Ruiz, N.; Lee, K.; Chin, D. C.; Blok, I.; Chang, H.;\nBarber, J.; Jiang, L.; Entis, G.; Li, Y.; et al. 2023. StyleDrop:\nText-to-Image Generation in Any Style.\narXiv preprint\narXiv:2306.00983.\nSong, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion\nimplicit models. arXiv preprint arXiv:2010.02502.\nSu, X.; Song, J.; Meng, C.; and Ermon, S. 2022. Dual diffu-\nsion implicit bridges for image-to-image translation. arXiv\npreprint arXiv:2203.08382.\nWei, Y.; Zhang, Y.; Ji, Z.; Bai, J.; Zhang, L.; and Zuo,\nW. 2023. Elite: Encoding visual concepts into textual em-\nbeddings for customized text-to-image generation.\narXiv\npreprint arXiv:2302.13848.\nWu, C. H.; and De la Torre, F. 2022. Unifying Diffusion\nModels\u2019 Latent Space, with Applications to CycleDiffusion\nand Guidance. arXiv preprint arXiv:2210.05559.\nXia, W.; Yang, Y.; Xue, J.-H.; and Wu, B. 2021. Tedigan:\nText-guided diverse face image generation and manipula-\ntion. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, 2256\u20132265.\nYu, J.; Xu, Y.; Koh, J. Y.; Luong, T.; Baid, G.; Wang, Z.;\nVasudevan, V.; Ku, A.; Yang, Y.; Ayan, B. K.; et al. 2022.\nScaling autoregressive models for content-rich text-to-image\ngeneration. arXiv preprint arXiv:2206.10789, 2(3): 5.\nYuan, G.; Cun, X.; Zhang, Y.; Li, M.; Qi, C.; Wang,\nX.; Shan, Y.; and Zheng, H. 2023.\nInserting Anybody\nin Diffusion Models via Celeb Basis.\narXiv preprint\narXiv:2306.00926.\nZhang, L.; and Agrawala, M. 2023.\nAdding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543.\nZhou, Y.; Zhang, R.; Sun, T.; and Xu, J. 2023.\nEnhanc-\ning Detail Preservation for Customized Text-to-Image Gen-\neration: A Regularization-Free Approach.\narXiv preprint\narXiv:2305.13579.\nSupplementary Material\nA More Qualitative Results\nIn Figure 6, 9, 10, 11, 12 and 13, we present supplemental\ncomparison results involving state-of-the-art methods E4T\n(Gal et al. 2023a), InsertingAnybody (Yuan et al. 2023), Pro-\nfusion (Zhou et al. 2023), HyperDreamBooth (Ruiz et al.\n2023b) and Subject-Diffusion (Ma et al. 2023), respectively.\nAll the results are taken from corresponding papers. These\nresults show the exceptional performance of our Photo-\nVerse in terms of fidelity, editability, and image quality. No-\ntably, our approach stands apart from methods E4T (Gal\net al. 2023a), InsertingAnybody (Yuan et al. 2023), Profu-\nsion (Zhou et al. 2023) and HyperDreamBooth (Ruiz et al.\n2023b) by eliminating the need for test-time tuning, while\nmaintaining the ability to generate a single image in \u223c 5\nseconds. We also provide more results of our PhotoVerse in\nFigure 14, 15 and 16.\nFigure 6: Comparison results with E4T.\nB Experiment Details\nAdapter Design\nIn our proposed approach, the adapter\nmodule is designed to be both straightforward and computa-\ntionally efficient. It comprises two blocks, with each block\nconsisting of two MLP layers that effectively project the in-\nput feature into a 1024-dimensional space. Following this\nprojection, layer normalization and a leaky-ReLU activa-\ntion layer are applied to further enhance the adapter\u2019s per-\nformance.\nUser Study\nFollow ProFusion (Zhou et al. 2023), we also\nevaluated E4T (Gal et al. 2023a), ProFusion (Zhou et al.\n2023), and our method using human assessment. The results\nfor each method were obtained from the corresponding pa-\npers. We designed two separate questionnaires for E4T, Pro-\nFusion, and our PhotoVerse for comparison. Each question-\nnaire consisted of 25 questions. A total of 15 participants\ntook part in the assessment, ensuring a diverse range of per-\nspectives.\nFigure 7: Results of user study on E4T and our PhotoVerse.\nFigure 8: Results of user study on Profusion and our Photo-\nVerse.\nC User Study\nAs shown in Figure 7 and 8, we conduct a human evalua-\ntion of two recent state-of-the-art methods E4T, Profusion\nand our PhotoVerse. For each of the two methods, we re-\nquested the participants to provide their preferences among\ntwo images generated using different methods, along with\nthe original image and textual instructions. The participants\nwere tasked with indicating their preferred choice according\nto the subject fidelity and text fidelity. According to Figure 7\nand 8, our method outperformed the other two recent meth-\nods, which are 66% VS 34% and 78% VS 22% respectively.\nFigure 9: Comparison results with InsertingAnybody.\nFigure 10: Comparison results with ProFusion.\nFigure 11: Comparison results with HyperDreamBooth.\nFigure 12: Comparison results with Subject-Diffusion.\nFigure 13: Comparison results with Subject-Diffusion.\nFigure 14: Results of our PhotoVerse.\nFigure 15: Results of our PhotoVerse.\nFigure 16: Results of our PhotoVerse.\n"
  },
  {
    "title": "InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation",
    "link": "https://arxiv.org/pdf/2309.06380.pdf",
    "upvote": "30",
    "text": "InstaFlow: One Step is Enough for High-Quality\nDiffusion-Based Text-to-Image Generation\nXingchao Liu1\u2217, Xiwen Zhang2, Jianzhu Ma2, Jian Peng2, Qiang Liu12\n1 Department of Computer Science, University of Texas at Austin\n2 Helixon Research\nxcliu@cs.utexas.edu, xiwen@helixon.com\nmajianzhu@tsinghua.edu.cn, jianpeng@illinois.edu, lqiang@cs.utexas.edu\nAbstract\nDiffusion models have revolutionized text-to-image generation with its exceptional\nquality and creativity. However, its multi-step sampling process is known to be\nslow, often requiring tens of inference steps to obtain satisfactory results. Previous\nattempts to improve its sampling speed and reduce computational costs through\ndistillation have been unsuccessful in achieving a functional one-step model. In\nthis paper, we explore a recent method called Rectified Flow [1, 2], which, thus\nfar, has only been applied to small datasets. The core of Rectified Flow lies\nin its reflow procedure, which straightens the trajectories of probability flows,\nrefines the coupling between noises and images, and facilitates the distillation\nprocess with student models. We propose a novel text-conditioned pipeline to\nturn Stable Diffusion (SD) into an ultra-fast one-step model, in which we find\nreflow plays a critical role in improving the assignment between noise and images.\nLeveraging our new pipeline, we create, to the best of our knowledge, the first\none-step diffusion-based text-to-image generator with SD-level image quality,\nachieving an FID (Fr\u00e9chet Inception Distance) of 23.3 on MS COCO 2017-5k,\nsurpassing the previous state-of-the-art technique, progressive distillation [3], by a\nsignificant margin (37.2 \u2192 23.3 in FID). By utilizing an expanded network with\n1.7B parameters, we further improve the FID to 22.4. We call our one-step models\nInstaFlow. On MS COCO 2014-30k, InstaFlow yields an FID of 13.1 in just 0.09\nsecond, the best in \u2264 0.1 second regime, outperforming the recent StyleGAN-T [4]\n(13.9 in 0.1 second). Notably, the training of InstaFlow only costs 199 A100 GPU\ndays. Project page: https://github.com/gnobitab/InstaFlow.\nFigure 1: InstaFlow is the first high-quality one-step text-to-image model derived from Stable Diffusion [5].\nWithin 0.1 second, it generates images with similar FID as StyleGAN-T [4] on MS COCO 2014. The whole\nfine-tuning process to yield InstaFlow is pure supervised learning and costs only 199 A100 GPU days.\n\u2217This work was done during an internship at Helixon Research\narXiv:2309.06380v1  [cs.LG]  12 Sep 2023\n(A) Images from one-step InstaFlow-0.9B (0.09s per image, 512 \u00d7 512)\n(B) One-step InstaFlow-0.9B + SDXL-Refiner \n(1024 \u00d7 1024)\n(C) Images from one-step InstaFlow-1.7B\n(0.12s per image, 512 \u00d7 512)\nFigure 2: (A) Examples of 512 \u00d7 512 images generated from one-step InstaFlow-0.9B in 0.09s; (B) The\nimages generated from one-step InstaFlow-0.9B can be further enhanced by SDXL-Refiner [6] to achieve higher\nresolution and finer details; (C) Examples of 512 \u00d7 512 images generated from one-step InstaFlow-1.7B in\n0.12s. Inference time is measured on our machine with NVIDIA A100 GPU.\n2\n1\nIntroduction\nModern text-to-image (T2I) generative models, such as DALL-E [7, 8], Imagen [9, 10], Stable\nDiffusion [5], StyleGAN-T [4], and GigaGAN [11], have demonstrated the remarkable ability to\nsynthesize realistic, artistic, and detailed images based on textual descriptions. These advancements\nare made possible through the assistance of large-scale datasets [12] and models [5, 7, 11].\nHowever, despite their impressive generation quality, these models often suffer from excessive\ninference time and computational consumption [5, 7, 8, 9, 10]. This can be attributed to the fact that\nmost of these models are either auto-regressive [13, 14, 15] or diffusion models [16, 17]. For instance,\nStable Diffusion, even when using a state-of-the-art sampler [18, 19, 20], typically requires more than\n20 steps to generate acceptable images. As a result, prior works [3, 21, 22] have proposed employing\nknowledge distillation on these models to reduce the required sampling steps and accelerate their\ninference. Unfortunately, these methods struggle in the small step regime. In particular, one-step\nlarge-scale diffusion models have not yet been developed. The existing one-step large-scale T2I\ngenerative models are StyleGAN-T [4] and GigaGAN [11], which rely on generative adversarial\ntraining and require careful tuning of both the generator and discriminator.\nIn this paper, we present a novel one-step generative model derived from the open-source Stable\nDiffusion (SD). We observed that a straightforward distillation of SD leads to complete failure.\nThe primary issue stems from the sub-optimal coupling of noises and images, which significantly\nhampers the distillation process. To address this challenge, we leverage Rectified Flow [1, 2], a recent\nadvancement in generative models that utilizes probabilistic flows [17, 23, 24]. In Rectified Flow, a\nunique procedure known as reflow is employed. Reflow gradually straightens the trajectory of the\nprobability flows, thereby reducing the transport cost between the noise distribution and the image\ndistribution. This improvement in coupling significantly facilitates the distillation process.\nConsequently, we succeeded in training the first one-step SD model capable of generating high-\nquality images with remarkable details. Quantitatively, our one-step model achieves a state-of-the-art\nFID score of 23.4 on the MS COCO 2017 dataset (5,000 images) with an inference time of only\n0.09 second per image. It outperforms the previous fastest SD model, progressive distillation [3],\nwhich achieved an one-step FID of 37.2. For MS COCO 2014 (30,000 images), our one-step model\nyields an FID of 13.1 in 0.09 second, surpassing one of the recent large-scale text-to-image GANs,\nStyleGAN-T [4] (13.9 in 0.1s). Notably, this is the first time a distilled one-step SD model performs\non par with GAN, with pure supervised learning.\n2\nRelated Works\nDiffusion Models and Flow-based Models\nDiffusion models [16, 17, 25, 26, 27, 28, 29, 30, 31]\nhave achieved unprecedented results in various generative modeling tasks, including image/video\ngeneration [9, 32, 33, 34, 35], audio generation [36], point cloud generation [37, 38, 39, 40], biological\ngeneration [34, 41, 42, 43], etc.. Most of the works are based on stochastic differential equations\n(SDEs), and researchers have explored techniques to transform them into marginal-preserving\nprobability flow ordinary differential equations (ODEs) [17, 20]. Recently, [1, 2, 23, 24, 44] propose\nto directly learn probability flow ODEs by constructing linear or non-linear interpolations between\ntwo distributions. These ODEs obtain comparable performance as diffusion models, but require much\nfewer inference steps. Among these approaches, Rectified Flow [1, 2] introduces a special reflow\nprocedure which enhances the coupling between distributions and squeezes the generative ODE to\none-step generation. However, the effectiveness of reflow has only been examined on small datasets\nlike CIFAR10, thus raising questions about its suitability on large-scale models and big data. In\nthis paper, we demonstrate that the Rectified Flow pipeline can indeed enable high-quality one-step\ngeneration in large-scale text-to-image diffusion models, hence brings ultra-fast T2I foundation\nmodels with pure supervised learning.\nLarge-Scale Text-to-Image Generation\nEarly research on text-to-image generation focused on\nsmall-scale datasets, such as flowers and birds [45, 46, 47]. Later, the field shifted its attention to more\ncomplex scenarios, particularly in the MS COCO dataset [48], leading to advancements in training\nand generation [49, 50, 51]. DALL-E [7] was the pioneering transformer-based model that showcased\nthe amazing zero-shot text-to-image generation capabilities by scaling up the network size and the\ndataset scale. Subsequently, a series of new methods emerged, including autoregressive models [14,\n3\nFigure 3: An overview of our pipeline for learning one-step large-scale text-to-image generative models. Direct\ndistillation from pre-trained diffusion models, e.g., Stable Diffusion, fails because their probability flow ODEs\nhave curved trajectories and incur bad coupling between noises and images. After fine-tuned with our text-\nconditioned reflow, the trajectories are straightened and the coupling is refined, thus the reflowed model is more\nfriendly to distillation. Consequently, the distilled model generates clear, high-quality images in one step. The\ntext prompt is \u201cA dog head in the universe with planets and stars\u201d.\n52, 53, 54], GAN inversion [55, 56], GAN-based approaches [57], and diffusion models [8, 9, 58, 59].\nAmong them, Stable Diffusion is an open-source text-to-image generator based on latent diffusion\nmodels [5]. It is trained on the LAION 5B dataset [12] and achieves the state-of-the-art generalization\nability. Additionally, GAN-based models like StyleGAN-T [4] and GigaGAN [11] are trained with\nadversarial loss to generate high-quality images rapidly. Our work provides a novel approach to yield\nultra-fast, one-step, large-scale generative models without the delicate adversarial training.\nAcceleration of Diffusion Models\nDespite the impressive generation quality, diffusion models are\nknown to be slow during inference due to the requirement of multiple iterations to reach the final\nresult. To accelerate inference, there are two categories of algorithms. The first kind focuses on fast\npost-hoc samplers [19, 20, 29, 60, 61, 62]. These fast samplers can reduce the number of inference\nsteps for pre-trained diffusion models to 20-50 steps. However, relying solely on inference to boost\nperformance has its limitations, necessitating improvements to the model itself. Distillation [63] has\nbeen applied to pre-trained diffusion models [64], squeezing the number of inference steps to below\n10. Progressive distillation [21] is a specially tailored distillation procedure for diffusion models, and\nhas successfully produced 2/4-step Stable Diffusion [3]. Consistency models [22] are a new family\nof generative models that naturally operate in a one-step manner, but their performance on large-scale\ntext-to-image generation is still unclear. Instead of employing direct distillation like previous works,\nwe adopt Rectified Flow [1, 2], which utilizes the reflow procedure to refine the coupling between the\nnoise distribution and the image distribution, thereby improving the performance of distillation.\n3\nMethods\n3.1\nLarge-Scale Text-to-Image Diffusion Models and the Need for Efficient Inference\nRecently, various of diffusion-based text-to-image generators [5, 10, 58] have emerged with unprece-\ndented performance. Among them, Stable Diffusion (SD) [5], an open-sourced model trained on\n4\nLAION-5B [12], gained widespread popularity from artists and researchers. It is based on latent\ndiffusion model [5], which is a denoising diffusion probabilistic model (DDPM) [16, 17] running\nin a learned latent space. Because of the recurrent nature of diffusion models, it usually takes\nmore than 100 steps for SD to generate satisfying images. To accelerate the inference, a series of\npost-hoc samplers have been proposed [18, 19, 20]. By transforming the diffusion model into a\nmarginal-preserving probability flow, these samplers can reduce the necessary inference steps to\nas few as 20 steps [19]. However, their performance starts to degrade noticeably when the number\nof inference steps is smaller than 10. For the \u226410 step regime, progressive distillation [3, 21] is\nproposed to compress the needed number of inference steps to 2-4. Yet, it is still an open problem if\nit is possible to turn large diffusion models, like SD, into an one-step model with satisfying quality.\n3.2\nRectified Flow and Reflow\nRectified Flow [1, 2] is a unified ODE-based framework for generative modeling and domain transfer.\nIt provides an approach for learning a transport mapping T between two distributions \u03c00 and \u03c01\non Rd from their empirical observations. In image generation, \u03c00 is usually a standard Gaussian\ndistribution and \u03c01 the image distribution.\nRectified Flow learns to transfer \u03c00 to \u03c01 via an ordinary differential equation (ODE), or flow model\ndZt\ndt = v(Zt, t),\ninitialized from Z0 \u223c \u03c00, such that Z1 \u223c \u03c01,\n(1)\nwhere v: Rd \u00d7 [0, 1] \u2192 Rd is a velocity field, learned by minimizing a simple mean square objective:\nmin\nv\nE(X0,X1)\u223c\u03b3\n\u0014Z 1\n0\n|| d\ndtXt \u2212 v(Xt, t) ||2 dt\n\u0015\n,\nwith\nXt = \u03d5(X0, X1, t),\n(2)\nwhere Xt = \u03d5(X0, X1, t) is any time-differentiable interpolation between X0 and X1, with d\ndtXt =\n\u2202t\u03d5(X0, X1, t). The \u03b3 is any coupling of (\u03c00, \u03c01). A simple example of \u03b3 is the independent\ncoupling \u03b3 = \u03c00 \u00d7 \u03c01, which can be sampled empirically from unpaired observed data from \u03c00\nand \u03c01. Usually, v is parameterized as a deep neural network and (2) is solved approximately with\nstochastic gradient methods.\nDifferent specific choices of the interpolation process Xt result in different algorithms. As shown\nin [1], the commonly used denoising diffusion implicit model (DDIM) [20] and the probability\nflow ODEs of [17] correspond to Xt = \u03b1tX0 + \u03b2tX1, with specific choices of time-differentiable\nsequences \u03b1t, \u03b2t (see [1] for details). In rectified flow, however, the authors suggested a simpler\nchoice of\nXt = (1 \u2212 t)X0 + tX1\n=\u21d2\nd\ndtXt = X1 \u2212 X0,\n(3)\nwhich favors straight trajectories that play a crucial role in fast inference, as we discuss in sequel.\nStraight Flows Yield Fast Generation\nIn practice, the ODE in (1) need to be approximated by\nnumerical solvers. The most common approach is the forward Euler method, which yields\nZt+ 1\nN = Zt + 1\nN v(Zt, t),\n\u2200t \u2208 {0, . . . , N \u2212 1}/N,\n(4)\nwhere we simulate with a step size of \u03f5 = 1/N and completes the simulation with N steps.\nFigure 4: ODEs with straight trajectories ad-\nmits fast simulation.\nObviously, the choice N yields a cost-accuracy trade-off:\nlarge N approximates the ODE better but causes high\ncomputational cost.\nFor fast simulation, it is desirable to learn the ODEs that\ncan be simulated accurately and fast with a small N. This\nleads to ODEs whose trajectory are straight lines. Specifi-\ncally, we say that an ODE is straight (with uniform speed)\nif\nStraight flow:\nZt = tZ1 + (1 \u2212 t)Z0 = Z0 + tv(Z0, 0),\n\u2200t \u2208 [0, 1],\nIn this case, Euler method with even a single step (N = 1) yields perfect simulation; See Figure 4.\nHence, straightening the ODE trajectories is an essential way for reducing the inference cost.\n5\nAlgorithm 1 Training Text-Conditioned Rectified Flow from Stable Diffusion\n1: Input: The pre-trained Stable Diffusion vSD = v1; A dataset of text prompts DT .\n2: for k \u2264 a user-defined upper bound do\n3:\nInitialize vk+1 from vk.\n4:\nTrain vk+1 by minimizing the objective (6), where the couplings (X0, X1 = ODE[vk](X0 | T ))\ncan be generated beforehand.\n5:\n#NOTE: The trained vk is called k-Rectified Flow.\n6: end for\nAlgorithm 2 Distilling Text-Conditioned k-Rectified Flow for One-Step Generation\n1: Input: k-Rectified Flow vk; A dataset of text prompts DT ; A similarity loss D(\u00b7, \u00b7).\n2: Initialize \u02dcvk from vk.\n3: Train \u02dcvk by minimizing the objective (7), where the couplings (X0, X1 = ODE[vk](X0 | T )) can\nbe generated beforehand.\n4: #NOTE: The trained \u02dcvk is called k-Rectified Flow+Distill.\nStraightening via Reflow\nReflow is an iterative procedure to straighten the trajectories of rectified\nflow without modifying the marginal distributions, hence allowing fast simulation at inference time.\nAssume we have an ODE model dXt = vk(Xt, t)dt with velocity field vk at the k-th iteration of the\nreflow procedure; denote by X1 = ODE[vk](X0) the Xt we obtained at t = 1 when following the\nvk-ODE starting from X0. A reflow step turns vk into a new vector field vk+1 that yields straighter\nODEs while Xnew\n1\n= ODE[vk+1](X0) has the same distribution as X1 = ODE[vk](X0),\nvk+1 = arg min\nv\nEX0\u223c\u03c00\n\u0014Z 1\n0\n|| (X1 \u2212 X0) \u2212 v(Xt, t) ||2 dt\n\u0015\n,\nwith X1 = ODE[vk](X0) and Xt = tX1 + (1 \u2212 t)X0,\n(5)\nwhere vk+1 is learned using the same rectified flow objective (2), but with the linear interpolation (3)\nof (X0, X1) pairs constructed from the previous ODE[vk].\nThe key property of reflow is that it preserves the terminal distribution while straightening the particle\ntrajectories and reducing the transport cost of the transport mapping:\n1) The distribution of ODE[vk+1](X0) and ODE[vk](X0) coincides; hence vk+1 transfers \u03c00 to \u03c01 if\nvk does so.\n2) The trajectories of ODE[vk+1] tend to be straighter than that of ODE[vk]. This suggests that it\nrequires smaller Euler steps N to simulate ODE[vk+1] than ODE[vk]. If vk is a fixed point of reflow,\nthat is, vk+1 = vk, then ODE[vk] must be exactly straight.\n3) (X0, ODE[vk+1](X0)) forms a better coupling than (X0, ODE[vk](X0)) in that it yields lower\nconvex transport costs, that is, E[c(ODE[vk+1](X0)\u2212X0)] \u2264 E[c(ODE[vk](X0)\u2212X0)] for all convex\nfunctions c: Rd \u2192 R. This suggests that the new coupling might be easier for the network to learn.\nText-Conditioned Reflow\nIn text-to-image generation, the velocity field v should additionally\ndepend on an input text prompt T to generate corresponding images. The reflow objective with text\nprompts is\nvk+1 = arg min\nv\nEX0\u223c\u03c00,T \u223cDT\n\u0014Z 1\n0\n|| (X1 \u2212 X0) \u2212 v(Xt, t | T ) ||2 dt\n\u0015\n,\nwith X1 = ODE[vk](X0 | T ) and Xt = tX1 + (1 \u2212 t)X0,\n(6)\nwhere DT is a dataset of text prompts and ODE[vk](X0 | T ) = X0 +\nR 1\n0 vk(Xt, t | T )dt.\nIn this paper, we set v1 to be the velocity field of a pre-trained probability flow ODE model (such as\nthat of Stable Diffusion, vSD), and denote the following vk(k \u2265 2) as k-Rectified Flow.\n6\nDistillation\nTheoretically, it requires an infinite number of reflow steps (5) to obtain ODEs with\nexactly straight trajectories. However, it is not practical to reflow too many steps due to high\ncomputational cost and the accumulation of optimization and statistical error. Fortunately, it was\nobserved in [1] that the trajectories of ODE[vk] becomes nearly (even though not exactly) straight with\neven one or two steps of reflows. With such approximately straight ODEs, one approach to boost the\nperformance of one-step models is via distillation:\n\u02dcvk = arg min\nv\nEX0\u223c\u03c00,T \u223cDT [D (ODE[vk](X0 | T ), X0 + v(X0 | T ))] ,\n(7)\nwhere we learn a single Euler step x+v(x | T ) to compress the mapping from X0 to ODE[vk](X0 | T )\nby minimizing a differentiable similarity loss D(\u00b7, \u00b7) between images. Following [1, 21, 22], we adopt\nthe Learned Perceptual Image Patch Similarity (LPIPS) loss [65] as the similiarty loss since it results\nin higher visual quality and better quantitative results. Learning one-step model with distillation\navoids adversarial training [4, 11, 66] or special invertible neural networks [67, 68, 69].\nIt is essential to use reflow to get good coupling before applying distillation.\nIt is important to\nnote the difference between distillation and reflow: while distillation tries to honestly approximate the\nmapping from X0 to ODE[vk](X0 | T ), reflow yields a new mapping ODE[vk+1](X0 | T ) that can be\nmore regular and smooth due to lower convex transport costs. In practice, we find that it is essential to\napply reflow to make the mapping ODE[vk](X0 | T ) sufficiently regular and smooth before applying\ndistillation.\nClassifier-Free Guidance Velocity Field for Rectified Flow\nClassifier-Free Guidance [70] has a\nsubstantial impact on the generation quality of SD. Similarly, we can define the following velocity\nfield to apply Classifier-Free Guidance on the learned Rectified Flow,\nv\u03b1(Zt, t | T ) = \u03b1v(Zt, t | T ) + (1 \u2212 \u03b1)v(Zt, t | NULL),\n(8)\nwhere \u03b1 trades off the sample diversity and generation quality. When \u03b1 = 1, v\u03b1 reduces back to the\noriginal velocity field v(Zt, t | T ). We provide analysis on \u03b1 in Section 6.\n4\nPreliminary Observations on Stable Diffusion 1.4\nIn this section, we conduct experiments with Stable Diffusion 1.4 to examine the effectiveness of the\nRectified Flow framework and the reflow procedure.\n4.1\nReflow is the Key to Improve Distillation\nThe goal of the experiments in this section is to:\n1) examine whether straightforward distillation can be effective for learning a one-step model from\npre-trained large-scale T2I prbobility flow ODEs;\n2) examine whether text-conditioned reflow can enhance the performance of distillation.\nOur experiment concludes that: Reflow significantly eases the learning process of distillation, and\ndistillation after reflow successfully produces a one-step model.\nGeneral Experiment Settings\nIn this section, we use the pre-trained Stable Diffusion 1.4 provided\nin the official open-sourced repository2 to initialize the weights, since otherwise the convergence is\nunbearably slow.\nIn our experiment, we set DT to be a subset of text prompts from laion2B-en [12], pre-processed by\nthe same filtering as SD. ODE[vSD] is implemented as the pre-trained Stable Diffusion with 25-step\nDPMSolver [19] and a fixed guidance scale of 6.0. We set the similarity loss D(\u00b7, \u00b7) for distillation to\nbe the LPIPS loss [65]. The neural network structure for both reflow and distillation are kept to the\nSD U-Net. We use a batch size of 32 and 8 A100 GPUs for training with AdamW optimizer [71].\nThe choice of optimizer follows the default protocol3 in HuggingFace for fine-tuning SD. We adopt\nexponential moving average with a ratio of 0.9999.\n2https://github.com/CompVis/stable-diffusion\n3https://huggingface.co/docs/diffusers/training/text2image\n7\n4.1.1\nDirect Distillation Fails\nFigure 5: Left: The inference time and FID-5k on MS COCO 2017 of all\nthe models. Model distilled from 2-Rectified Flow has a lower FID and\nsmaller gap with the teacher model. Right: The images generated from\ndifferent models with the same random noise and text prompt. 2-Rectified\nFlow refines the coupling between noises and images, making it a better\nteacher for distillation.\nExperiment\nProtocol\nOur\ninvestigation starts from directly\ndistilling\nthe\nvelocity\nfield\nv1 = vSD of Stable Diffusion\n1.4 with (7) without applying\nany reflow.\nTo achieve the\nbest empirical performance, we\nconduct grid search on learning\nrate and weight decay to the\nlimit\nof\nour\ncomputational\nresources.\nParticularly,\nthe\nlearning\nrates\nare\nselected\nfrom\n{10\u22125, 10\u22126, 10\u22127}\nand the weight decay coef-\nficients\nare\nselected\nfrom\n{10\u22121, 10\u22122, 10\u22123}.\nFor all\nthe 9 models, we train them\nfor 100, 000 steps. We generate\n32 \u00d7 100, 000\n=\n3, 200, 000\npairs of (X0, ODE[vSD](X0)) as\nthe training set for distillation.\nWe compute the Fr\u00e9chet incep-\ntion distance (FID) on 5, 000 captions from MS COCO 2017 following the evaluation protocol in [3],\nthen we show the model with the lowest FID in Figure 5. For more experiment results, please refer to\nAppendix.\nObservation and Analysis\nWe observe that, after 100, 000 training steps, all the nine models\nconverge. However, the learned one-step generative model is far from satisfying. As shown in\nFigure 5, there is a huge gap in FID between SD and SD+Distill. In fact, it is difficult for the student\nmodel (SD+Distill) to imitate the teacher model (25-step SD). On the right side of Figure 5, with\nthe same random noise, SD+Distill generates image with substantial difference from the teacher SD.\nFrom the experiments, we conclude that: directly distillation from SD is a tough learning problem for\nthe student one-step model, and this is hard to mitigate by simply tuning the hyperparameters.\n4.1.2\nReflow Improves Couling and Eases Distillation\nExperiment Protocol\nFor fair comparison with distillation, we train v2 for 50, 000 steps with the\nweights initialized from pre-trained SD, then perform distillation for another 50, 000 training steps\ncontinuing from the obtained v2. The learning rate for reflow is 10\u22126. To distill from 2-Rectified\nFlow, we generate 32 \u00d7 50, 000 = 1, 600, 000 pairs of (X0, ODE[v2](X0)) with 25-step Euler solver.\nThe results are also shown in Figure 5 for comparison with direct distillation. The guidance scale \u03b1\nfor 2-Rectified Flow is set to 1.5.\nObservation and Analysis\nFirst of all, the obtained 2-Rectified Flow has similar FID with the\noriginal SD, which are 22.1 and 22.8, respectively. It indicates that reflow can be used to learn\ngenerative ODEs with comparable performance. Moreover, 2-Rectified Flow refines the coupling\nbetween the noise distribution and the image distribution, and eases the learning process for the\nstudent model when distillation. This can be inferred from two aspects. (1) The gap between the\n2-Rectified Flow+Distill and the 2-Rectified Flow is much smaller than SD+Distill and SD. (2) On the\nright side of Figure 5, the image generated from 2-Rectified Flow+Distill shares great resemblance\nwith the original generation, showing that it is easier for the student to imitate. This illustrates that\n2-Rectified Flow is a better teacher model to distill a student model than the original SD.\n8\nMethod\nInference Time\nFID-5k\nCLIP\nSD 1.4-DPM Solver (25 step)[5, 19]\n0.88s\n22.8\n0.315\n(Pre) 2-Rectified Flow (25 step)\n0.88s\n22.1\n0.313\n(Pre) 3-Rectified Flow (25 step)\n0.88s\n23.6\n0.309\nProgressive Distillation-SD (1 step)[3]\n0.09s\n37.2\n0.275\nSD 1.4+Distill (U-Net)\n0.09s\n40.9\n0.255\n(Pre) 2-Rectified Flow (1 step)\n0.09s\n68.3\n0.252\n(Pre) 2-Rectified Flow+Distill (U-Net)\n0.09s\n31.0\n0.285\n(Pre) 3-Rectified Flow (1 step)\n0.09s\n37.0\n0.270\n(Pre) 3-Rectified Flow+Distill (U-Net)\n0.09s\n29.3\n0.283\nProgressive Distillation-SD (2 step)[3]\n0.13s\n26.0\n0.297\nProgressive Distillation-SD (4 step)[3]\n0.21s\n26.4\n0.300\nSD 1.4+Distill (Stacked U-Net)\n0.12s\n52.0\n0.269\n(Pre) 2-Rectified Flow+Distill (Stacked U-Net)\n0.12s\n24.6\n0.306\n(Pre) 3-Rectified Flow+Distill (Stacked U-Net)\n0.12s\n26.3\n0.307\nTable 1: Comparison of FID on MS COCO 2017 following the evaluation setup in [3]. As in [4, 11], the\ninference time is measured on NVIDIA A100 GPU, with a batch size of 1, PyTorch 2.0.1 and Huggingface\nDiffusers 0.19.3. 2-Rectified Flow+Distill outperforms Progressive Distillation within the same inference\ntime using much less training cost. The numbers for Progressive Distillation are measured from Figure 10 in [3].\n\u2018Pre\u2019 is added to distinguish the models from Table 3.\n4.2\nQuantitative Comparison and Additional Analysis\nIn this section, we provide additional quantitative and qualitative results with further analysis and\ndiscussion. 2-Rectified Flow and its distilled versions are trained following the same training\nconfiguration as in Section 4.1.2.\nExpanding the Network Size\nFor distillation, we consider two network structures: (i) U-Net,\nwhich is the exact same network structure as the denoising U-Net of SD; (ii) Stacked U-Net, which is\na simplified structure from direct concatenation of two U-Nets with shared parameters. Compared\nwith two-step inference, Stacked U-Net reduces the inference time to 0.12s from 0.13s by removing a\nset of unnecessary modules, while keeping the number of parameters unchanged. Stacked U-Net is\nmore powerful than U-Net, which allows it to achieve better one-step performance after distillation.\nMore details can be found in the Appendix.\nMultiple Reflow\nAccording to Eq. (6), the reflow procedure can be repeated for multiple times.\nWe repeat reflow for one more time to get 3-Rectified Flow (v3), which is initialized from 2-Rectified\nFlow (v2). 3-Rectified Flow is trained to minimize Eq. (6) for 50, 000 steps. Then we get its distilled\nversion by generating 1, 600, 000 new pairs of (X0, ODE[v3](X0)) and distill for another 50, 000\nsteps. We found that to stabilize the training process of 3-Rectified Flow and its distillation, we have\nto decrease the learning rate from 10\u22126 to 10\u22127.\nTraining Cost\nBecause our Rectified Flows are fine-tuned from the publicly available pre-trained\nmodels, the training cost is negligible compared with other large-scale text-to-image models. On our\nplatform, when training with batch size of 4 and U-Net, one A100 GPU day can process 100, 000\niterations using L2 loss, 86, 000 iterations using LPIPS loss; when generating pairs with batch\nsize of 16, one A100 GPU day can generate 200, 000 data pairs. Therefore, to get 2-Rectified\nFlow + Distill (U-Net), the training cost is approximately 3, 200, 000/200, 000 (Data Generation) +\n32/4 \u00d7 50, 000/100, 000 (Reflow) + 32/4 \u00d7 50, 000/86, 000 (Distillation) \u2248 24.65 A100 GPU days.\nFor reference, the training cost for SD 1.4 from scratch is 6250 A100 GPU days [5]; StyleGAN-T is\n1792 A100 GPU days [4]; GigaGAN is 4783 A100 GPU days [11]. A lower-bound estimation of\ntraining the one-step SD in Progressive Distillation is 108.8 A100 GPU days [3] (the details for the\nestimation can be found in the Appendix).\nComparison on MS COCO\nWe compare the performance of our models with baselines on MS\nCOCO [48]. Our first experiment follows the evaluation protocol of [3], where we use 5,000 captions\nfrom the MS COCO 2017 validation set and generate corresponding images. Then we measure the\n9\nMethod\nInference Time\n# Param.\nFID-30k\nSD\u2217 [5]\n2.9s\n0.9B\n9.62\n(Pre) 2-Rectified Flow (25 step)\n0.88s\n0.9B\n13.4\nSD 1.4+Distill (U-Net)\n0.09s\n0.9B\n34.6\n(Pre) 2-Rectified Flow+Distill (U-Net)\n0.09s\n0.9B\n20.0\nSD 1.4+Distill (Stacked U-Net)\n0.12s\n0.9B\n41.5\n(Pre) 2-Rectified Flow+Distill (Stacked U-Net)\n0.12s\n0.9B\n13.7\nTable 2: Comparison of FID on MS COCO 2014 with 30, 000 images. Note that the models distilled after\nreflow has noticeable advantage compared with direct distillation even when (Pre) 2-Rectified Flow has worse\nperformance than the original SD due to insufficient training. \u2217 denotes that the numbers are measured by [11].\n\u2018Pre\u2019 is added to distinguish the models from Table 4. As in StyleGAN-T [4] and GigaGAN [11], our generated\nimages are downsampled to 256 \u00d7 256 before computing FID.\nFID score and CLIP score using the ViT-g/14 CLIP model [72, 73] to quantitatively evaluate the\nimage quality and correspondence with texts. We also record the average running time of different\nmodels with NVIDIA A100 GPU to generate one image. For fair comparison, we use the inference\ntime of standard SD on our computational platform for Progressive Distillation-SD as their model\nis not available publicly. The inference time contains the text encoder and the latent decoder, but\ndoes NOT contain NSFW detector. The results are shown in table 1. (Pre) 2-Rectified Flow and\n(Pre) 3-Rectified Flow can generate realistic images that yields similar FID with SD 1.4+DPMSolver\nusing 25 steps (22.1, 23.6 \u2194 22.8). Within 0.09s, (Pre) 3-Rectified Flow+Distill (U-Net) gets an\nFID of 29.3, and within 0.12s, (Pre) 2-Rectified Flow+Distill (Stacked U-Net) gets an FID of 24.6,\nsurpassing the previous best distilled SD model (37.2 and 26.0, respectively) [3]. In our second\nexperiment, we use 30,000 captions from MS COCO2014, and perform the same evaluation on\nFID. The results are shown in Table 2. We observe that (Pre) 2-Rectified Flow+Distill (Stacked\nU-Net) obtains an FID of 13.7, which is much better than SD+Distill with Stacked U-Net (41.5). We\nempirically find that SD+Distill has worse FID with the larger Stacked U-Net in both experiments,\nthough it has better visual quality. This could be attributed to the instability of FID metric when the\nimages deviate severely from the real images.\nFigure 6: The straightening effect of reflow.\nLeft: the straightness S(Z) on different mod-\nels. Right: trajectories of randomly sampled\npixels following SD 1.4+DPMSolver and 2-\nRectified Flow.\nStraightening Effects of Reflow\nWe empirically exam-\nine the properties of reflow in text-to-image generation.\nTo quantitatively measure the straightness, we use the de-\nviation of the velocity along the trajectory following [1, 2],\nthat is, S(Z) =\nR 1\nt=0 E\n\u0002\n|| (Z1 \u2212 Z0) \u2212 v(Zt, t) ||2\u0003\ndt.\nA smaller S(Z) means straighter trajectories, and when\nthe ODE trajectories are all totally straight, S(Z) = 0. We\nrandomly generate 1, 000 trajectories to estimate S(Z).\nThe results are shown in Figure 6 and Figure 7. In Fig-\nure 6, every time of reflow decreases the estimated S(Z),\nvalidating the straightening effect of reflow. Moreover, the\ndifference between Stable Diffusion and 2-Rectified Flow\nis noticeable to human. The pixels in Stable Diffusion\ntravel in curved trajectories, while 2-Rectified Flow has\nmuch straighter trajectories. The straightening effect also\nexists in real images. Figure 7 demonstrates that, apply-\ning reflow on Stable Diffusion dramatically straightens\nthe ODE trajectories, making 2-Rectified Flow generate\nrecognizable images with one step and eases distillation.\n5\nTowards Better One-Step Generation: Scaling Up on Stable Diffusion 1.5\nOur preliminary results with Stable Diffusion 1.4 demonstrate the advantages of adopting the reflow\nprocedure in distilling one-step generative models. However, since only 24.65 A100 GPU days\nare spent in training, it is highly possible that the performance can be further boosted with more\nresources. Therefore, we expand the training time with a larger batch size to examine if scaling up\n10\nFigure 7: Reflow straightens the ODE trajectories and improves distillation. In this figure, N is the number of\ninference steps. If an ODE trajectory is straight enough, we shall see that N = 25 and N = 1 yields similar\nimages. All the distilled models use Stacked U-Net structure. From the examples, we observe that: (1) the\ntrajectory of Stable Diffusion is curved, since N = 1 leads to meaningless noises; (2) distillation from Stable\nDiffusion results in blurry low-quality images; (3) after only one reflow, the ODE trajectories of 2-Rectified\nFlow is much straighter, as N = 1 generates vague but meaningful images; (4) distillation from 2-Rectified\nFlow results in clear high-quality images; (5) 3-Rectified Flow further straightens the ODEs.\nhas a positive impact on the result. The answer is affirmative. With 199 A100 GPU days, we obtain\nthe first one-step SD that generates high-quality images with intricate details in 0.09 second, on par\nwith one of the state-of-the-art GANs, StyleGAN-T [4] .\nImplementation Details and Training Pipeline\nWe switch to Stable Diffusion 1.5, and keep the\nsame DT as in Section 4. The ODE solver sticks to 25-step DPMSolver [19] for ODE[vSD]. Guidance\nscale is slightly decreased to 5.0 because larger guidance scale makes the images generated from\n2-Rectified Flow over-saturated. Since distilling from 2-Rectified Flow yields satisfying results,\n3-Rectiifed Flow is not trained. We still generate 1, 600, 000 pairs of data for reflow and distillation,\nrespectively. To expand the batch size to be larger than 4 \u00d7 8 = 32, gradient accumulation is applied.\nThe overall training pipeline for 2-Rectified Flow+Distill (U-Net) is summarized as follows:\n1. Reflow (Stage 1): We train the model using the reflow objective (6) with a batch size of 64\nfor 70,000 iterations. The model is initialized from the pre-trained SD 1.5 weights. (11.2\nA100 GPU days)\n2. Reflow (Stage 2): We continue to train the model using the reflow objective (6) with an\nincreased batch size of 1024 for 25,000 iterations. The final model is 2-Rectified Flow. (64\nA100 GPU days)\n3. Distill (Stage 1): Starting from the 2-Rectified Flow checkpoint, we fix the time t = 0 for\nthe neural network, and fine-tune it using the distillation objective (7) with a batch size of\n1024 for 21,500 iterations. The guidance scale \u03b1 of the teacher model, 2-Rectified Flow, is\nset to 1.5 and the similarity loss D is L2 loss. (54.4 A100 GPU days)\n4. Distill (Stage 2): We switch the similarity loss D to LPIPS loss, then we continue to train\nthe model using the distillation objective (7) and a batch size of 1024 for another 18,000\niterations. The final model is 2-Rectified Flow+Distill (U-Net). We name it InstaFlow-0.9B.\n(53.6 A100 GPU days)\nThe total training cost for InstaFlow-0.9B is 3, 200, 000/200, 000 (Data Generation) + 11.2 + 64 +\n54.4 + 53.6 = 199.2 A100 GPU days.\n11\nMethod\nInference Time\nFID-5k\nCLIP\nSD 1.5-DPM Solver (25 step)[5, 19]\n0.88\n20.1\n0.318\n2-Rectified Flow (25 step)\n0.88\n21.5\n0.315\nProgressive Distillation-SD (1 step)[3]\n0.09\n37.2\n0.275\n2-Rectified Flow (1 step)\n0.09\n47.0\n0.271\nInstaFlow-0.9B\n0.09\n23.4\n0.304\nProgressive Distillation-SD (2 step)[3]\n0.13\n26.0\n0.297\nProgressive Distillation-SD (4 step)[3]\n0.21\n26.4\n0.300\n2-Rectified Flow (2 step)\n0.13\n31.3\n0.296\nInstaFlow-1.7B\n0.12\n22.4\n0.309\nTable 3: Comparison of FID on MS COCO 2017 following the evaluation setup in [3]. As in [4, 11], the\ninference time is measured on NVIDIA A100 GPU, with a batch size of 1, PyTorch 2.0.1 and Huggingface\nDiffusers 0.19.3. The numbers for Progressive Distillation are measured from Figure 10 in [3].\nLarger Neural Network for One-Step Generation\nExpanding the model size is a key step in\nbuilding modern foundation models [5, 6, 74, 75, 76]. To this end, we adopt the Stacked U-Net\nstructure in Section 4, but abandon the parameter-sharing strategy. This gives us a Stacked U-Net\nwith 1.7B parameters, almost twice as large as the original U-Net. Starting from 2-Rectified Flow,\n2-Rectified Flow+Distill (Stacked U-Net) is trained by the following distillation steps:\n1. Distill (Stage 1): The Stacked U-Net is initialized from the weights in the 2-Rectified Flow\ncheckpoint. Then we fix the time t = 0 for the neural network, and fine-tune it using the\ndistillation objective (7) with a batch size of 64 for 110,000 iterations. The similarity loss D\nis L2 loss. (35.2 A100 GPU days)\n2. Distill (Stage 2): We switch the similarity loss D to LPIPS loss, then we continue to train the\nmodel using the distillation objective (7) and a batch size of 320 for another 2,500 iterations.\nThe final model is 2-Rectified Flow+Distill (Stacked U-Net). We name it InstaFlow-1.7B.\n(4.4 A100 GPU days)\nDiscussion 1 (Experiment Observations)\nDuring training, we made the following observations:\n(1) the 2-Rectified Flow model did not fully converge and its performance could potentially benefit\nfrom even longer training duration; (2) distillation showed faster convergence compared to reflow;\n(3) the LPIPS loss had an immediate impact on enhancing the visual quality of the distilled one-step\nmodel. Based on these observations, we believe that with more computational resources, further\nimprovements can be achieved for the one-step models.\nDiscussion 2 (One-Step Stacked U-Net and Two-Step Progressive Distillation)\nAlthough one-\nstep Stacked U-Net and 2-step progressive distillation (PD) need similar inference time, they have\ntwo key differences: (1) 2-step PD additionally minimizes the distillation loss at t = 0.5, which may\nbe unnecessary for one-step generation from t = 0; (2) by considering the consecutive U-Nets as one\nmodel, we are able to examine and remove redundant components from this large neural network,\nfurther reducing the inference time by approximately 8% (from 0.13s to 0.12s).\n6\nEvaluation\nIn this section, we systematically evaluate 2-Rectified Flow and the distilled one-step models. We\nname our one-step model 2-Rectified Flow+Distill (U-Net) as InstaFlow-0.9B and 2-Rectified\nFlow+Distill (Stacked U-Net) as InstaFlow-1.7B.\n6.1\nComparison with State-of-the-Arts on MS COCO\nWe follow the experiment configuration in Seciton 4.2. The guidance scale \u03b1 for the teacher model,\n2-Rectified Flow, is set to 1.5. In Table 3, our InstaFlow-0.9B gets an FID-5k of 23.4 with an\ninference time of 0.09s, which is significantly lower than the previous state-of-the-art, Progressive\nDistillation-SD (1 step). The training cost for Progressive Distillation-SD (1 step) [3] is \u2265 108.8 A100\nGPU days, while the training cost of the distillation step for InstaFlow-0.9B is 54.4 + 53.6 = 108\n12\nCat.\nRes.\nMethod\nInference Time\n# Param.\nFID-30k\nAR\n256\nDALLE [7]\n-\n12B\n27.5\nAR\n256\nParti-750M [54]\n-\n750M\n10.71\nAR\n256\nParti-3B [54]\n6.4s\n3B\n8.10\nAR\n256\nParti-20B [54]\n-\n20B\n7.23\nAR\n256\nMake-A-Scene [53]\n25.0s\n-\n11.84\nDiff\n256\nGLIDE [58]\n15.0s\n5B\n12.24\nDiff\n256\nLDM [5]\n3.7s\n0.27B\n12.63\nDiff\n256\nDALLE 2 [8]\n-\n5.5B\n10.39\nDiff\n256\nImagen [10]\n9.1s\n3B\n7.27\nDiff\n256\neDiff-I [77]\n32.0s\n9B\n6.95\nGAN\n256\nLAFITE [57]\n0.02s\n75M\n26.94\n-\n512\nMuse-3B [78]\n1.3s\n0.5B\n7.88\nGAN\n512\nStyleGAN-T [4]\n0.10s\n1B\n13.90\nGAN\n512\nGigaGAN [11]\n0.13s\n1B\n9.09\nDiff\n512\nSD\u2217 [5]\n2.9s\n0.9B\n9.62\n-\n512\n2-RF (25 step)\n0.88s\n0.9B\n11.08\n-\n512\nInstaFlow-0.9B\n0.09s\n0.9B\n13.10\n-\n512\nInstaFlow-1.7B\n0.12s\n1.7B\n11.83\nTable 4: Comparison of FID on MS COCO 2014 with 30, 000 images (\u2018RF\u2019 refers to \u2018Rectified Flow\u2019, \u2018AR\u2019\nrefers to \u2018Autoregressive\u2019). \u2217 denotes that the numbers are measured by [11]. As in StyleGAN-T [4] and\nGigaGAN [11], our generated images are downsampled to 256 \u00d7 256 before computing FID.\nA100 GPU days. With similar distillation cost, InstaFlow-0.9B yields clear advantage. The empirical\nresult indicates that reflow helps improve the coupling between noises and images, and 2-Rectified\nFlow is an easier teacher model to distill from. By increaseing the model size, InstaFlow-1.7B leads\nto a lower FID-5k of 22.4 with an inference time of 0.12s.\nOn MS COCO 2014, our InstaFlow-0.9B obtains an FID-30k of 13.10 within 0.09s, surpassing\nStyleGAN-T [4] (13.90 in 0.1s). This is for the first time one-step distilled SD performs on par with\nstate-of-the-art GANs. Using Stacked U-Net with 1.7B parameters, FID-30k of our one-step model\nachieves 11.83. Although this is still higher than GigaGAN [11], we believe more computational\nresources can close the gap: GigaGAN spends over 4700 A100 GPU days in training, while our\nInstaFlow-1.7B only consumes 130.8 A100 GPU days.\n6.2\nAnalysis on 2-Rectified Flow\nFew-step Generation\n2-Rectified Flow has straighter trajectories, which gives it the capacity to\ngenerate with extremely few inference steps. We compare 2-Rectified Flow with SD 1.5-DPM\nSolver [61] on MS COCO 2017. 2-Rectified Flow adopts standard Euler solver. The inference\nsteps are set to {1, 2, 4, 8}. Figure 10 (A) clearly shows the advantage of 2-Rectified Flow when the\nnumber of inference steps \u2264 4. In Figure 11, 2-Rectified Flow can generate images much better than\nSD with 1,2,4 steps, implying that it has a straighter ODE trajectory than the original SD 1.5.\nGuidance Scale \u03b1\nIt is widely known that guidance scale \u03b1 is a important hyper-parameter when\nusing Stable Diffusion [5, 70]. By changing the guidance scale, the user can change semantic\nalignment and generation quality. Here, we investigate the influence of the guidance scale \u03b1 for\n2-Rectified Flow, which has straighter ODE trajectories. In Figure 10 (B), increasing \u03b1 from 1.0 to 4.0\nincreases FID-5k and CLIP score on MS COCO 2017 at the same time. The former metric indicates\ndegradation in image quality and the latter metric indicates enhancement in semantic alignment.\nGenerated examples are shown in Figure 12. While the trending is similar to the original SD 1.5, there\nare two key differences. (1) Even when \u03b1 = 1.0 (no guidance), the generated images already have\ndecent quality, since we perform reflow on SD 1.5 with a guidance scale of 5.0 and the low-quality\nimages are thus dropped in training. Therefore, it is possible to avoid using classifier-free guidance\nduring inference to save GPU memory. (2) Unlike original SD 1.5, changing the guidance scale does\nnot bring drastic change in the generated contents. Rather, it only perturbs the details and the tone.\nWe leave explanations for these new behaviors as future directions.\n13\nFigure 8: Latent space interpolation of our one-step InstaFlow-0.9B. The images are generated in 0.09s, saving\n\u223c 90% of the computational time from the 25-step SD-1.5 teacher model in the inference stage.\nFigure 9: Images generated from our one-step InstaFlow-1.7B in 0.12s. With the same random noise, the pose\nand lighting are preserved across different text prompts.\nAlignment between 2-Rectified Flow and the One-Step Models\nThe learned latent spaces of\ngenerative models have intriguing properties. By properly exploiting their latent structure, prior works\nsucceeded in image editing [33, 79, 80, 81, 82, 83, 84], semantic control [55, 56, 85], disentangled\ncontrol direction discovery [86, 87, 88, 89], etc.. In general, the latent spaces of one-step generators,\nlike GANs, are usually easier to analyze and use than the multi-step diffusion models. One advantage\nof our pipeline is that it gives a multi-step continuous flow and the corresponding one-step models\nsimultaneously. Figure 13 shows that the latent spaces of our distilled one-step models align with\n2-Rectified Flow. Therefore, the one-step models can be good surrogates to understand and leverage\nthe latent spaces of continuous flow, since the latter one has higher generation quality.\n14\n(A)\n(B)\nFigure 10: (A) Comparison between SD 1.5-DPM Solver and 2-Rectified Flow (with standard Euler solver) in\nfew-step inference. 2-Rectified Flow consistently outperforms SD1.5-DPM Solver on FID-5k and CLIP score,\nespecially when the number of inference steps is smaller than 4. (B) The trade-off curve of applying different \u03b1\nas the guidance scale for 2-Rectified Flow. \u03b1 increases from {1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0}.\nFigure 11: Visual comparison with different number of inference steps N. With the same random seed, 2-\nRectified Flow can generate clear images when N \u2264 4, while SD 1.5-DPM Solver cannot.\nFigure 12: Visual comparison with different guidance scale \u03b1 on 2-Rectified Flow. When \u03b1 = 1.0, the generated\nimages have blurry edges and twisted details; when \u03b1 \u2265 2.0, the generated images gradually gets over-saturated.\n15\nFigure 13: With the same random noise and text prompts, the one-step models generate similar images with the\ncontinuous 2-Rectified Flow, indicating their latent space aligns. Therefore, the one-step models can be good\nsurrogates to analyze the properties of the latent space of the continuous flow.\nFigure 14: The images generated from our one-step model can be refined by SDXL-Refiner [6] to generate\nenjoyable high-resolution images. It suggests that a potential usage of the one-step models is to serve as fast\npreviewers to quickly filter out unwanted images.\n16\n6.3\nFast Preview with One-Step Model\nA potential use case of our one-step models is to serve as previewers. Typically, large-scale text-to-\nimage models work in a cascaded manner [90]: the user can choose one image from several low-\nresolution images, and then an expensive super-resolution model expands the chosen low-resolution\nimage to higher resolution. In the first step, the composition/color tone/style/other components of the\nlow-resolution images may be unsatisfying to the user and the details are not even considered. Hence, a\nfast previewer can accelerate the low-resolution filtering process and provide the user more generation\npossibilities under the same computational budget. Then, the powerful post-processing model can\nimprove the quality and increase the resolution. We verify the idea with SDXL-Refiner [6], a recent\nmodel that can refine generated images. The one-step models, InstaFlow-0.9B and InstaFlow-1.7B,\ngenerate 512 \u00d7 512 images, then these images are interpolated to 1024 and refined by SDXL-Refiner\nto get high-resolution images. Several examples are shown in Figure 14. The low-resolution images\ngenerated in one step determine the content, composition, etc. of the image; then SDXL-Refiner\nrefines the twisted parts, adds extra details, and harmonizes the high-resolution images.\n7\nThe Best is Yet to Come\nThe recurrent nature of diffusion models hinders their deployment on edge devices [91, 92], harms\nuser experience, and adds to the overall cost. In this paper, we demonstrate that a powerful one-step\ngenerative model can be obtained from pre-trained Stable Diffusion (SD) using the text-conditioned\nRectified Flow framework. Based on our results, we propose several promising future directions for\nexploration:\n1. Improving One-Step SD: The training of the 2-Rectified Flow model did not fully converge,\ndespite investing 75.2 A100 GPU days. This is only a fraction of training cost of the original\nSD (6250 A100 GPU days). By scaling up the dataset, model size, and training duration, we\nbelieve the performance of one-step SD will improve significantly. Moreover, SOTA base\nmodels, e.g., SDXL [6], can be leveraged as teachers to enhance the one-step SD model.\n2. One-Step ControlNet [93]: By applying our pipeline to train ControlNet models, it is\npossible to get one-step ControlNets capable of generating controllable contents within\nmilliseconds. The only required modification involves adjusting the model structure and\nincorporating the control modalities as additional conditions.\n3. Personalization for One-Step Models: By fine-tuning SD with the training objective\nof diffusion models and LORA [94] , users can customize the pre-trained SD to generate\nspecific contents and styles [95]. However, as the one-step models have substantial difference\nfrom traditional diffusion models, determining the objective for fine-tuning these one-step\nmodels remains a subject that requires further investigation.\n4. Neural Network Structure for One-Step Generation: It is widely acknowledged in\nthe research community that the U-Net structure plays a crucial role [16, 17, 30] in the\nimpressive performance of diffusion models in image generation. With the advancement\nof creating one-step SD models using text-conditioned reflow and distillation, several\nintriguing directions arise: (1) exploring alternative one-step structures, such as successful\narchitectures [4, 11, 96, 97] used in GANs, that could potentially surpass the U-Net in\nterms of quality and efficiency; (2) leveraging techniques like pruning, quantization, and\nother approaches for building efficient neural networks to make one-step generation more\ncomputationally affordable while minimizing potential degradation in quality.\n17\nReferences\n[1] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate\nand transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.\n[2] Qiang Liu. Rectified flow: A marginal preserving approach to optimal transport. arXiv preprint\narXiv:2209.14577, 2022.\n[3] Chenlin Meng, Ruiqi Gao, Diederik P Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. arXiv preprint arXiv:2210.03142, 2022.\n[4] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking\nthe power of gans for fast large-scale text-to-image synthesis. arXiv preprint arXiv:2301.09515,\n2023.\n[5] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolution image synthesis with latent diffusion models, 2021.\n[6] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis, 2023.\n[7] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on\nMachine Learning, pages 8821\u20138831. PMLR, 2021.\n[8] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[9] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. Advances in\nNeural Information Processing Systems, 35:36479\u201336494, 2022.\n[10] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,\nDiederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High\ndefinition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.\n[11] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and\nTaesung Park. Scaling up gans for text-to-image synthesis. arXiv preprint arXiv:2303.05511,\n2023.\n[12] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth\nConference on Neural Information Processing Systems Datasets and Benchmarks Track.\n[13] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya\nSutskever. Generative pretraining from pixels. In International conference on machine learning,\npages 1691\u20131703. PMLR, 2020.\n[14] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin,\nXu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via\ntransformers. Advances in Neural Information Processing Systems, 34:19822\u201319835, 2021.\n[15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution\nimage synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 12873\u201312883, 2021.\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin Neural Information Processing Systems, 33:6840\u20136851, 2020.\n[17] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\nBen Poole. Score-based generative modeling through stochastic differential equations. In\nInternational Conference on Learning Representations.\n18\n[18] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models\non manifolds. In International Conference on Learning Representations.\n[19] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A\nfast ode solver for diffusion probabilistic model sampling in around 10 steps. In Advances in\nNeural Information Processing Systems.\n[20] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In\nInternational Conference on Learning Representations.\n[21] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.\nIn International Conference on Learning Representations.\n[22] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv\npreprint arXiv:2303.01469, 2023.\n[23] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow\nmatching for generative modeling. In The Eleventh International Conference on Learning\nRepresentations, 2022.\n[24] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A\nunifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.\n[25] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. In International Conference on Machine\nLearning, pages 2256\u20132265. PMLR, 2015.\n[26] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of\nscore-based diffusion models. Advances in Neural Information Processing Systems, 34:1415\u2013\n1428, 2021.\n[27] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.\nAdvances in neural information processing systems, 33:12438\u201312448, 2020.\n[28] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data\ndistribution. Advances in neural information processing systems, 32, 2019.\n[29] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of\ndiffusion-based generative models. In Advances in Neural Information Processing Systems.\n[30] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nAdvances in neural information processing systems, 34:8780\u20138794, 2021.\n[31] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion\nfor high resolution images. arXiv preprint arXiv:2301.11093, 2023.\n[32] Jonathan Ho, Tim Salimans, Alexey A Gritsenko, William Chan, Mohammad Norouzi, and\nDavid J Fleet. Video diffusion models. In Advances in Neural Information Processing Systems.\n[33] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan\nWang, Silvio Savarese, Stefano Ermon, Caiming Xiong, and Ran Xu. Hive: Harnessing human\nfeedback for instructional visual editing. arXiv preprint arXiv:2303.09618, 2023.\n[34] Lemeng Wu, Chengyue Gong, Xingchao Liu, Mao Ye, et al. Diffusion-based molecule genera-\ntion with informative prior bridges. In Advances in Neural Information Processing Systems.\n[35] Mao Ye, Lemeng Wu, and Qiang Liu. First hitting diffusion models for generating manifold,\ngraph and categorical data. In Advances in Neural Information Processing Systems, 2022.\n[36] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile\ndiffusion model for audio synthesis. In International Conference on Learning Representations.\n[37] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n2837\u20132845, 2021.\n19\n[38] Shitong Luo and Wei Hu. Score-based point cloud denoising. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 4583\u20134592, 2021.\n[39] Xingchao Liu, Lemeng Wu, Mao Ye, et al. Learning diffusion bridges on constrained domains.\nIn The Eleventh International Conference on Learning Representations, 2023.\n[40] Lemeng Wu, Dilin Wang, Chengyue Gong, Xingchao Liu, Yunyang Xiong, Rakesh Ranjan,\nRaghuraman Krishnamoorthi, Vikas Chandra, and Qiang Liu. Fast point cloud generation with\nstraight flows. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9445\u20139454, 2023.\n[41] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A\ngeometric diffusion model for molecular conformation generation. In International Conference\non Learning Representations.\n[42] Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. Antigen-\nspecific antibody design and optimization with diffusion-based generative models for protein\nstructures. In Advances in Neural Information Processing Systems.\n[43] Emiel Hoogeboom, V\u0131ctor Garcia Satorras, Cl\u00e9ment Vignac, and Max Welling. Equivariant\ndiffusion for molecule generation in 3d. In International conference on machine learning, pages\n8867\u20138887. PMLR, 2022.\n[44] Eric Heitz, Laurent Belcour, and Thomas Chambon. Iterative \u03b1 -(de)blending: A minimalist\ndeterministic diffusion model. In ACM SIGGRAPH 2023 Conference Proceedings, SIGGRAPH\n\u201923, New York, NY, USA, 2023. Association for Computing Machinery.\n[45] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak\nLee. Generative adversarial text to image synthesis. In International conference on machine\nlearning, pages 1060\u20131069. PMLR, 2016.\n[46] Scott E Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee.\nLearning what and where to draw. Advances in neural information processing systems, 29,\n2016.\n[47] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and\nDimitris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative\nadversarial networks. In Proceedings of the IEEE international conference on computer vision,\npages 5907\u20135915, 2017.\n[48] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pages 740\u2013755. Springer, 2014.\n[49] Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan:\nA simple and effective baseline for text-to-image synthesis. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 16515\u201316525, 2022.\n[50] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal\ncontrastive learning for text-to-image generation. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 833\u2013842, 2021.\n[51] Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang, Xiaodong He, Siwei Lyu, and\nJianfeng Gao. Object-driven text-to-image synthesis via adversarial training. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12174\u201312182,\n2019.\n[52] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-\nimage generation via hierarchical transformers. Advances in Neural Information Processing\nSystems, 35:16890\u201316902, 2022.\n20\n[53] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-based text-to-image generation with human priors. In Computer Vision\u2013\nECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings,\nPart XV, pages 89\u2013106. Springer, 2022.\n[54] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\nmodels for content-rich text-to-image generation. Transactions on Machine Learning Research.\n[55] Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis\nCastricato, and Edward Raff. Vqgan-clip: Open domain image generation and editing with\nnatural language guidance. In European Conference on Computer Vision, pages 88\u2013105.\nSpringer, 2022.\n[56] Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, and Qiang Liu. Fuse-\ndream: Training-free text-to-image generation with improved clip+ gan space optimization.\narXiv preprint arXiv:2112.01573, 2021.\n[57] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxi-\nang Gu, Jinhui Xu, and Tong Sun. Lafite: Towards language-free training for text-to-image\ngeneration. arXiv preprint arXiv:2111.13792, 2021.\n[58] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,\nBob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image genera-\ntion and editing with text-guided diffusion models. In International Conference on Machine\nLearning, pages 16784\u201316804. PMLR, 2022.\n[59] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos\nNiebles, Caiming Xiong, Silvio Savarese, et al. Unicontrol: A unified diffusion model for\ncontrollable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023.\n[60] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models\non manifolds. In International Conference on Learning Representations, 2021.\n[61] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.\nDpm-\nsolver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022.\n[62] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the\noptimal reverse variance in diffusion probabilistic models. In International Conference on\nLearning Representations, 2021.\n[63] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531, 2015.\n[64] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for\nimproved sampling speed. arXiv preprint arXiv:2101.02388, 2021.\n[65] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unrea-\nsonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 586\u2013595, 2018.\n[66] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications\nof the ACM, 63(11):139\u2013144, 2020.\n[67] Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and J\u00f6rn-Henrik Jacobsen. Residual\nflows for invertible generative modeling. Advances in Neural Information Processing Systems,\n32, 2019.\n[68] Ivan Kobyzev, Simon JD Prince, and Marcus A Brubaker. Normalizing flows: An introduction\nand review of current methods. IEEE transactions on pattern analysis and machine intelligence,\n43(11):3964\u20133979, 2020.\n21\n[69] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji\nLakshminarayanan. Normalizing flows for probabilistic modeling and inference. The Journal\nof Machine Learning Research, 22(1):2617\u20132680, 2021.\n[70] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop\non Deep Generative Models and Downstream Applications, 2021.\n[71] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International\nConference on Learning Representations.\n[72] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021.\n[73] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan\nTaori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi,\nAli Farhadi, and Ludwig Schmidt. Openclip, July 2021. If you use this software, please cite it\nas below.\n[74] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[75] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\nArx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the\nopportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\n[76] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied\nmultimodal language model. arXiv preprint arXiv:2303.03378, 2023.\n[77] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models\nwith an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\n[78] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan\nYang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image\ngeneration via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.\n[79] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang.\nGan inversion: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n45(3):3121\u20133138, 2022.\n[80] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri,\nand Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6007\u20136017,\n2023.\n[81] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or.\nPrompt-to-prompt image editing with cross-attention control. In The Eleventh International\nConference on Learning Representations, 2022.\n[82] Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and Qifeng Chen. High-fidelity gan inversion\nfor image attribute editing. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 11379\u201311388, 2022.\n[83] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano\nErmon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In\nInternational Conference on Learning Representations, 2021.\n[84] Xingchao Liu, Lemeng Wu, Shujian Zhang, Chengyue Gong, Wei Ping, and Qiang Liu. Flow-\ngrad: Controlling the output of generative odes with gradients. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 24335\u201324344, 2023.\n22\n[85] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip:\nText-driven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 2085\u20132094, 2021.\n[86] Erik H\u00e4rk\u00f6nen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering\ninterpretable gan controls. Advances in neural information processing systems, 33:9841\u20139850,\n2020.\n[87] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn.\nDiffusion autoencoders: Toward a meaningful and decodable representation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10619\u201310629,\n2022.\n[88] Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespace analysis: Disentangled controls\nfor stylegan image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12863\u201312872, 2021.\n[89] Yujun Shen and Bolei Zhou. Closed-form factorization of latent semantics in gans. In Proceed-\nings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1532\u20131540,\n2021.\n[90] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim\nSalimans. Cascaded diffusion models for high fidelity image generation. The Journal of\nMachine Learning Research, 23(1):2249\u20132281, 2022.\n[91] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey\nTulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within\ntwo seconds. arXiv preprint arXiv:2306.00980, 2023.\n[92] Atila Orhon, Michael Siracusa, and Aseem Wadhwa. Stable diffusion with core ml on apple\nsilicon, 2022.\n[93] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.05543, 2023.\n[94] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu\nChen, et al. Lora: Low-rank adaptation of large language models. In International Conference\non Learning Representations, 2021.\n[95] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n[96] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 4401\u20134410, 2019.\n[97] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.\nAnalyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 8110\u20138119, 2020.\n23\nA\nNeural Network Structure\nFigure 15: Different neural network structures for distillation and their inference time. The blocks with the same\ncolors can share weights.\nThe whole pipeline of our text-to-image generative model consists of three parts: the text encoder, the\ngenerative model in the latent space, and the decoder. We use the same text encoder and decoder as\nStable Diffusion: the text encoder is adopted from CLIP ViT-L/14 and the latent decoder is adopted\nfrom a pre-trained auto-encoder with a downsampling factor of 8. During training, the parameters\nin the text encoder and the latent decoder are frozen. On average, to generate 1 image on NVIDIA\nA100 GPU with a batch size of 1, text encoding takes 0.01s and latent decoding takes 0.04s.\nBy default, the generative model in the latent space is a U-Net structure. For reflow, we do not change\nany of the structure, but just fine-tune the model. For distillation, we tested three network structures,\nas shown in Figure 15. The first structure is the original U-Net structure in SD. The second structure\nis obtained by directly concatenating two U-Nets with shared parameters. We found that the second\nstructure significantly decrease the distillation loss and improve the quality of the generated images\nafter distillation, but it doubles the computational time.\n24\nlearning rate\nFID\nweight decay\n10\u22121\n10\u22122\n10\u22123\n10\u22125\n44.04\n45.90\n44.03\n10\u22126\n137.05\n134.83\n139.31\n10\u22127\n\u223c297\n\u223c297\n\u223c297\nTable 5: FID of different distilled SD models measured with 5000 images on MS COCO2017.\nTo reduce the computational time, we tested a family of networks structures by deleting different\nblocks in the second structure. By this, we can examine the importance of different blocks in this\nconcatenated network in distillation, remove the unnecessary ones and thus further decrease inference\ntime. We conducted a series of ablation studies, including:\n1. Remove \u2018Downsample Blocks 1 (the green blocks on the left)\u2019\n2. Remove \u2018Upsample Blocks 1 (the yellow blocks on the left)\u2019\n3. Remove \u2018In+Out Block\u2019 in the middle (the blue and purple blocks in the middle).\n4. Remove \u2018Downsample Blocks 2 (the green blocks on the right)\u2019\n5. Remove \u2018Upsample blocks 2 (the yellow blocks on the right)\u2019\nThe only one that would not hurt performance is Structure 3, and it gives us a 7.7% reduction in\ninference time ( 0.13\u22120.12\n0.13\n= 0.0769 ). This third structure, Stacked U-Net, is illustrated in Figure 15\n(c).\nB\nAdditional Details on Experiments\nOur training script is based on the official fine-tuning script provided by HuggingFace4. We use\nexponential moving average with a factor of 0.9999, following the default configuration. We clip the\ngradient to reach a maximal gradient norm of 1. We warm-up the training process for 1,000 steps\nin both reflow and distillation. BF16 format is adopted during training to save GPU memory. To\ncompute the LPIPS loss, we used its official 0.1.4 version5 and its model based on AlexNet.\nC\nEstimation of the Training Cost of Progressive Distillation (PD)\nMeasured on our platform, when training with a batch size of 4, one A100 GPU day can process\n\u223c100,000 iterations using L2 loss. We compute the computational cost according to this.\nWe refer to Appendix C.2.1 (LAION-5B 512 \u00d7 512) of [3] and estimate the training cost. PD starts\nfrom 512 steps, and progressively applies distillation to 1 step with a batch size of 512. Quoting\nthe statement \u2018For stage-two, we train the model with 2000-5000 gradient updates except when the\nsampling step equals to 1,2, or 4, where we train for 10000-50000 gradient updates\u2019, a lower-bound\nestimation of gradient updates would be 2000 (512 to 256) + 2000 (256 to 128) + 2000 (128 to 64) +\n2000 (64 to 32) + 2000 (32 to 16) + 5000 (16 to 8) + 10000 (8 to 4) + 10000 (4 to 2) + 50000 (2 to 1)\n= 85,000 iterations. Therefore, one-step PD at least requires 512/4 \u00d7 85000/100000 = 108.8 A100\nGPU days. Note that we ignored the computational cost of stage 1 of PD and \u20182 steps of DDIM with\nteacher\u2019 during PD, meaning that the real training cost is higher than 108.8 A100 GPU days.\nD\nDirect Distillation of Stable Diffusion\nWe provide additional results on direct distillation of Stable Diffusion 1.4, shown in Fig-\nure 16, 17, 18, 19 and Table 5. Although increasing the learning rate boosts the performance,\nwe found that a learning rate of \u2265 10\u22124 leads to unstable training and NaN errors. A small learning\nrate, like 10\u22126 and 10\u22127, results in slow convergence and blurry generation after training 100, 000\nsteps.\n4https://huggingface.co/docs/diffusers/training/text2image\n5https://github.com/richzhang/PerceptualSimilarity\n25\nFigure 16: Uncurated samples from SD+Distill (U-Net) trained with a learning rate of 10\u22127 and a weight decay\ncoefficient of 10\u22123.\nFigure 17: Uncurated samples from SD+Distill (U-Net) trained with a learning rate of 10\u22126 and a weight decay\ncoefficient of 10\u22123.\n26\nFigure 18: Uncurated samples from SD+Distill (U-Net) trained with a learning rate of 10\u22125 and a weight decay\ncoefficient of 10\u22123.\nFigure 19: Uncurated samples from SD+Distill (Stacked U-Net) trained with a learning rate of 10\u22125 and a\nweight decay coefficient of 10\u22123.\n27\nFigure 20: Uncurated samples from Stable Diffusion 1.5 with 25-step DPMSolver [19] and guidance scale 5.0.\nE\nAdditional Generated Images\nWe show uncurated images generated from 20 random LAION text prompts with the same random\nnoises for visual comparison. The images from different models are shown in Figure 20, 21, 22, 23.\n28\nFigure 21: Uncurated samples from 2-Rectified Flow with guidance scale 1.5 and 25-step Euler solver.\nFigure 22: Uncurated samples from one-step InstaFlow-0.9B\n29\nFigure 23: Uncurated samples from one-step InstaFlow-1.7B\n30\n"
  },
  {
    "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
    "link": "https://arxiv.org/pdf/2309.06180.pdf",
    "upvote": "23",
    "text": "Efficient Memory Management for Large Language\nModel Serving with PagedAttention\nWoosuk Kwon1,\u2217 Zhuohan Li1,\u2217 Siyuan Zhuang1 Ying Sheng1,2 Lianmin Zheng1 Cody Hao Yu3\nJoseph E. Gonzalez1 Hao Zhang4 Ion Stoica1\n1UC Berkeley\n2Stanford University\n3Independent Researcher\n4UC San Diego\nAbstract\nHigh throughput serving of large language models (LLMs)\nrequires batching sufficiently many requests at a time. How-\never, existing systems struggle because the key-value cache\n(KV cache) memory for each request is huge and grows\nand shrinks dynamically. When managed inefficiently, this\nmemory can be significantly wasted by fragmentation and\nredundant duplication, limiting the batch size. To address\nthis problem, we propose PagedAttention, an attention al-\ngorithm inspired by the classical virtual memory and pag-\ning techniques in operating systems. On top of it, we build\nvLLM, an LLM serving system that achieves (1) near-zero\nwaste in KV cache memory and (2) flexible sharing of KV\ncache within and across requests to further reduce mem-\nory usage. Our evaluations show that vLLM improves the\nthroughput of popular LLMs by 2-4\u00d7 with the same level\nof latency compared to the state-of-the-art systems, such\nas FasterTransformer and Orca. The improvement is more\npronounced with longer sequences, larger models, and more\ncomplex decoding algorithms. vLLM\u2019s source code is publicly\navailable at https://github.com/vllm-project/vllm.\n1\nIntroduction\nThe emergence of large language models (LLMs) like GPT [5,\n37] and PaLM [9] have enabled new applications such as pro-\ngramming assistants [6, 18] and universal chatbots [19, 35]\nthat are starting to profoundly impact our work and daily\nroutines. Many cloud companies [34, 44] are racing to pro-\nvide these applications as hosted services. However, running\nthese applications is very expensive, requiring a large num-\nber of hardware accelerators such as GPUs. According to\nrecent estimates, processing an LLM request can be 10\u00d7 more\nexpensive than a traditional keyword query [43]. Given these\nhigh costs, increasing the throughput\u2014and hence reducing\nPermission to make digital or hard copies of part or all of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. Copyrights for third-\nparty components of this work must be honored. For all other uses, contact\nthe owner/author(s).\nSOSP \u201923, October 23\u201326, 2023, Koblenz, Germany\n\u00a9 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0229-7/23/10.\nhttps://doi.org/10.1145/3600006.3613165\nNVIDIA A100 40GB\nParameters \n(26GB, 65%)\nKV \nCache\n(>30%)\nOthers\n20\n30\n40\nMemory usage (GB)\nParameter size\nExisting systems\nvLLM\n0\n10\n20\n30\n40\nBatch size (# requests)\n0\n0.4k\n0.8k\n1.2k\nThroughput (token/s)\nFigure 1. Left: Memory layout when serving an LLM with\n13B parameters on NVIDIA A100. The parameters (gray)\npersist in GPU memory throughout serving. The memory\nfor the KV cache (red) is (de)allocated per serving request.\nA small amount of memory (yellow) is used ephemerally\nfor activation. Right: vLLM smooths out the rapid growth\ncurve of KV cache memory seen in existing systems [31, 60],\nleading to a notable boost in serving throughput.\nthe cost per request\u2014of LLM serving systems is becoming\nmore important.\nAt the core of LLMs lies an autoregressive Transformer\nmodel [53]. This model generates words (tokens), one at a\ntime, based on the input (prompt) and the previous sequence\nof the output\u2019s tokens it has generated so far. For each re-\nquest, this expensive process is repeated until the model out-\nputs a termination token. This sequential generation process\nmakes the workload memory-bound, underutilizing the com-\nputation power of GPUs and limiting the serving throughput.\nImproving the throughput is possible by batching multi-\nple requests together. However, to process many requests\nin a batch, the memory space for each request should be\nefficiently managed. For example, Fig. 1 (left) illustrates the\nmemory distribution for a 13B-parameter LLM on an NVIDIA\nA100 GPU with 40GB RAM. Approximately 65% of the mem-\nory is allocated for the model weights, which remain static\nduring serving. Close to 30% of the memory is used to store\nthe dynamic states of the requests. For Transformers, these\nstates consist of the key and value tensors associated with the\nattention mechanism, commonly referred to as KV cache [41],\nwhich represent the context from earlier tokens to gener-\nate new output tokens in sequence. The remaining small\n\u2217Equal contribution.\n1\narXiv:2309.06180v1  [cs.LG]  12 Sep 2023\nOrca\n(Max)\nOrca\n(Pow2)\nOrca\n(Oracle)\nvLLM\n0\n20\n40\n60\n80\n100\nKV cache usage (%)\n20.4\n13.3\n57.3\n8.9\n26.8\n17.9\n13.6\n41.6\n38.2\n25.2\n36.6\n96.3\nToken states\nReservation\nInternal frag.\nExternal frag.\n& Others\nFigure 2. Average percentage of memory wastes in different\nLLM serving systems during the experiment in \u00a76.2.\npercentage of memory is used for other data, including ac-\ntivations \u2013 the ephemeral tensors created when evaluating\nthe LLM. Since the model weights are constant and the ac-\ntivations only occupy a small fraction of the GPU memory,\nthe way the KV cache is managed is critical in determining\nthe maximum batch size. When managed inefficiently, the\nKV cache memory can significantly limit the batch size and\nconsequently the throughput of the LLM, as illustrated in\nFig. 1 (right).\nIn this paper, we observe that existing LLM serving sys-\ntems [31, 60] fall short of managing the KV cache memory\nefficiently. This is mainly because they store the KV cache of\na request in contiguous memory space, as most deep learning\nframeworks [33, 39] require tensors to be stored in contigu-\nous memory. However, unlike the tensors in the traditional\ndeep learning workloads, the KV cache has unique charac-\nteristics: it dynamically grows and shrinks over time as the\nmodel generates new tokens, and its lifetime and length are\nnot known a priori. These characteristics make the existing\nsystems\u2019 approach significantly inefficient in two ways:\nFirst, the existing systems [31, 60] suffer from internal and\nexternal memory fragmentation. To store the KV cache of\na request in contiguous space, they pre-allocate a contigu-\nous chunk of memory with the request\u2019s maximum length\n(e.g., 2048 tokens). This can result in severe internal frag-\nmentation, since the request\u2019s actual length can be much\nshorter than its maximum length (e.g., Fig. 11). Moreover,\neven if the actual length is known a priori, the pre-allocation\nis still inefficient: As the entire chunk is reserved during the\nrequest\u2019s lifetime, other shorter requests cannot utilize any\npart of the chunk that is currently unused. Besides, external\nmemory fragmentation can also be significant, since the pre-\nallocated size can be different for each request. Indeed, our\nprofiling results in Fig. 2 show that only 20.4% - 38.2% of the\nKV cache memory is used to store the actual token states in\nthe existing systems.\nSecond, the existing systems cannot exploit the opportu-\nnities for memory sharing. LLM services often use advanced\ndecoding algorithms, such as parallel sampling and beam\nsearch, that generate multiple outputs per request. In these\nscenarios, the request consists of multiple sequences that can\npartially share their KV cache. However, memory sharing is\nnot possible in the existing systems because the KV cache of\nthe sequences is stored in separate contiguous spaces.\nTo address the above limitations, we propose PagedAt-\ntention, an attention algorithm inspired by the operating\nsystem\u2019s (OS) solution to memory fragmentation and shar-\ning: virtual memory with paging. PagedAttention divides the\nrequest\u2019s KV cache into blocks, each of which can contain\nthe attention keys and values of a fixed number of tokens. In\nPagedAttention, the blocks for the KV cache are not neces-\nsarily stored in contiguous space. Therefore, we can manage\nthe KV cache in a more flexible way as in OS\u2019s virtual mem-\nory: one can think of blocks as pages, tokens as bytes, and\nrequests as processes. This design alleviates internal frag-\nmentation by using relatively small blocks and allocating\nthem on demand. Moreover, it eliminates external fragmen-\ntation as all blocks have the same size. Finally, it enables\nmemory sharing at the granularity of a block, across the\ndifferent sequences associated with the same request or even\nacross the different requests.\nIn this work, we build vLLM, a high-throughput distributed\nLLM serving engine on top of PagedAttention that achieves\nnear-zero waste in KV cache memory. vLLM uses block-level\nmemory management and preemptive request scheduling\nthat are co-designed with PagedAttention. vLLM supports\npopular LLMs such as GPT [5], OPT [62], and LLaMA [52]\nwith varying sizes, including the ones exceeding the memory\ncapacity of a single GPU. Our evaluations on various models\nand workloads show that vLLM improves the LLM serving\nthroughput by 2-4\u00d7 compared to the state-of-the-art sys-\ntems [31, 60], without affecting the model accuracy at all. The\nimprovements are more pronounced with longer sequences,\nlarger models, and more complex decoding algorithms (\u00a74.3).\nIn summary, we make the following contributions:\n\u2022 We identify the challenges in memory allocation in serving\nLLMs and quantify their impact on serving performance.\n\u2022 We propose PagedAttention, an attention algorithm that\noperates on KV cache stored in non-contiguous paged\nmemory, which is inspired by the virtual memory and\npaging in OS.\n\u2022 We design and implement vLLM, a distributed LLM serving\nengine built on top of PagedAttention.\n\u2022 We evaluate vLLM on various scenarios and demonstrate\nthat it substantially outperforms the previous state-of-the-\nart solutions such as FasterTransformer [31] and Orca [60].\n2\nBackground\nIn this section, we describe the generation and serving pro-\ncedures of typical LLMs and the iteration-level scheduling\nused in LLM serving.\n2\n2.1\nTransformer-Based Large Language Models\nThe task of language modeling is to model the probability\nof a list of tokens (\ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b). Since language has a natural\nsequential ordering, it is common to factorize the joint prob-\nability over the whole sequence as the product of conditional\nprobabilities (a.k.a. autoregressive decomposition [3]):\n\ud835\udc43(\ud835\udc65) = \ud835\udc43(\ud835\udc651) \u00b7 \ud835\udc43(\ud835\udc652 | \ud835\udc651) \u00b7 \u00b7 \u00b7 \ud835\udc43(\ud835\udc65\ud835\udc5b | \ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b\u22121).\n(1)\nTransformers [53] have become the de facto standard ar-\nchitecture for modeling the probability above at a large scale.\nThe most important component of a Transformer-based lan-\nguage model is its self-attention layers. For an input hidden\nstate sequence (\ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b) \u2208 R\ud835\udc5b\u00d7\ud835\udc51, a self-attention layer\nfirst applies linear transformations on each position \ud835\udc56 to get\nthe query, key, and value vectors:\n\ud835\udc5e\ud835\udc56 = \ud835\udc4a\ud835\udc5e\ud835\udc65\ud835\udc56, \ud835\udc58\ud835\udc56 = \ud835\udc4a\ud835\udc58\ud835\udc65\ud835\udc56, \ud835\udc63\ud835\udc56 = \ud835\udc4a\ud835\udc63\ud835\udc65\ud835\udc56.\n(2)\nThen, the self-attention layer computes the attention score\n\ud835\udc4e\ud835\udc56\ud835\udc57 by multiplying the query vector at one position with all\nthe key vectors before it and compute the output \ud835\udc5c\ud835\udc56 as the\nweighted average over the value vectors:\n\ud835\udc4e\ud835\udc56\ud835\udc57 =\nexp(\ud835\udc5e\u22a4\n\ud835\udc56 \ud835\udc58\ud835\udc57/\n\u221a\n\ud835\udc51)\n\u00cd\ud835\udc56\n\ud835\udc61=1 exp(\ud835\udc5e\u22a4\n\ud835\udc56 \ud835\udc58\ud835\udc61/\n\u221a\n\ud835\udc51)\n, \ud835\udc5c\ud835\udc56 =\n\ud835\udc56\u2211\ufe01\n\ud835\udc57=1\n\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc63 \ud835\udc57.\n(3)\nBesides the computation in Eq. 4, all other components\nin the Transformer model, including the embedding layer,\nfeed-forward layer, layer normalization [2], residual connec-\ntion [22], output logit computation, and the query, key, and\nvalue transformation in Eq. 2, are all applied independently\nposition-wise in a form of \ud835\udc66\ud835\udc56 = \ud835\udc53 (\ud835\udc65\ud835\udc56).\n2.2\nLLM Service & Autoregressive Generation\nOnce trained, LLMs are often deployed as a conditional gen-\neration service (e.g., completion API [34] or chatbot [19, 35]).\nA request to an LLM service provides a list of input prompt\ntokens (\ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b), and the LLM service generates a list of\noutput tokens (\ud835\udc65\ud835\udc5b+1, . . . ,\ud835\udc65\ud835\udc5b+\ud835\udc47 ) according to Eq. 1. We refer to\nthe concatenation of the prompt and output lists as sequence.\nDue to the decomposition in Eq. 1, the LLM can only sam-\nple and generate new tokens one by one, and the generation\nprocess of each new token depends on all the previous tokens\nin that sequence, specifically their key and value vectors. In\nthis sequential generation process, the key and value vectors\nof existing tokens are often cached for generating future\ntokens, known as KV cache. Note that the KV cache of one\ntoken depends on all its previous tokens. This means that the\nKV cache of the same token appearing at different positions\nin a sequence will be different.\nGiven a request prompt, the generation computation in\nthe LLM service can be decomposed into two phases:\nThe prompt phase takes the whole user prompt (\ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b)\nas input and computes the probability of the first new to-\nken \ud835\udc43(\ud835\udc65\ud835\udc5b+1 | \ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b). During this process, also gener-\nates the key vectors \ud835\udc581, . . . ,\ud835\udc58\ud835\udc5b and value vectors \ud835\udc631, . . . , \ud835\udc63\ud835\udc5b.\nSince prompt tokens \ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b are all known, the computa-\ntion of the prompt phase can be parallelized using matrix-\nmatrix multiplication operations. Therefore, this phase can\nefficiently use the parallelism inherent in GPUs.\nThe autoregressive generation phase generates the re-\nmaining new tokens sequentially. At iteration \ud835\udc61, the model\ntakes one token \ud835\udc65\ud835\udc5b+\ud835\udc61 as input and computes the probability\n\ud835\udc43(\ud835\udc65\ud835\udc5b+\ud835\udc61+1 | \ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b+\ud835\udc61) with the key vectors \ud835\udc581, . . . ,\ud835\udc58\ud835\udc5b+\ud835\udc61 and\nvalue vectors \ud835\udc631, . . . , \ud835\udc63\ud835\udc5b+\ud835\udc61. Note that the key and value vectors\nat positions 1 to \ud835\udc5b + \ud835\udc61 \u2212 1 are cached at previous iterations,\nonly the new key and value vector \ud835\udc58\ud835\udc5b+\ud835\udc61 and \ud835\udc63\ud835\udc5b+\ud835\udc61 are com-\nputed at this iteration. This phase completes either when the\nsequence reaches a maximum length (specified by users or\nlimited by LLMs) or when an end-of-sequence (<eos>) token\nis emitted. The computation at different iterations cannot\nbe parallelized due to the data dependency and often uses\nmatrix-vector multiplication, which is less efficient. As a re-\nsult, this phase severely underutilizes GPU computation and\nbecomes memory-bound, being responsible for most portion\nof the latency of a single request.\n2.3\nBatching Techniques for LLMs\nThe compute utilization in serving LLMs can be improved\nby batching multiple requests. Because the requests share\nthe same model weights, the overhead of moving weights is\namortized across the requests in a batch, and can be over-\nwhelmed by the computational overhead when the batch\nsize is sufficiently large. However, batching the requests\nto an LLM service is non-trivial for two reasons. First, the\nrequests may arrive at different times. A naive batching strat-\negy would either make earlier requests wait for later ones\nor delay the incoming requests until earlier ones finish, lead-\ning to significant queueing delays. Second, the requests may\nhave vastly different input and output lengths (Fig. 11). A\nstraightforward batching technique would pad the inputs\nand outputs of the requests to equalize their lengths, wasting\nGPU computation and memory.\nTo address this problem, fine-grained batching mecha-\nnisms, such as cellular batching [16] and iteration-level sched-\nuling [60], have been proposed. Unlike traditional methods\nthat work at the request level, these techniques operate at\nthe iteration level. After each iteration, completed requests\nare removed from the batch, and new ones are added. There-\nfore, a new request can be processed after waiting for a\nsingle iteration, not waiting for the entire batch to complete.\nMoreover, with special GPU kernels, these techniques elim-\ninate the need to pad the inputs and outputs. By reducing\nthe queueing delay and the inefficiencies from padding, the\nfine-grained batching mechanisms significantly increase the\nthroughput of LLM serving.\n3\nFour\nscore\nand\nseven\nyears\nago\nour\nfathers brought\nforth\n<eos> <resv>\n\u2026\n<resv>\nYou\nonly\nlive\nonce\n<eos> <resv>\n\u2026\n<resv>\n2038 slots never used \n(internal fragmentation)\n2 slots future used\n(reserved)\nExternal fragmentation\n7 KV cache states for \nrequest A\u2019s prompt\n3 KV cache states for \nrequest B\u2019s prompt\n1 slot future used\n(reserved)\n507 slots never used\n(Internal fragmentation)\nRequest B\ncurrent iteration\nRequest A\ncurrent iteration\n1 slot for\ngenerated token\nFigure 3. KV cache memory management in existing systems. Three types of memory wastes \u2013 reserved, internal fragmentation,\nand external fragmentation \u2013 exist that prevent other requests from fitting into the memory. The token in each memory slot\nrepresents its KV cache. Note the same tokens can have different KV cache when at different positions.\n3\nMemory Challenges in LLM Serving\nAlthough fine-grained batching reduces the waste of com-\nputing and enables requests to be batched in a more flexible\nway, the number of requests that can be batched together is\nstill constrained by GPU memory capacity, particularly the\nspace allocated to store the KV cache. In other words, the\nserving system\u2019s throughput is memory-bound. Overcom-\ning this memory-bound requires addressing the following\nchallenges in the memory management:\nLarge KV cache. The KV Cache size grows quickly with the\nnumber of requests. As an example, for the 13B parameter\nOPT model [62], the KV cache of a single token demands 800\nKB of space, calculated as 2 (key and value vectors) \u00d7 5120\n(hidden state size) \u00d7 40 (number of layers) \u00d7 2 (bytes per\nFP16). Since OPT can generate sequences up to 2048 tokens,\nthe memory required to store the KV cache of one request\ncan be as much as 1.6 GB. Concurrent GPUs have memory\ncapacities in the tens of GBs. Even if all available memory\nwas allocated to KV cache, only a few tens of requests could\nbe accommodated. Moreover, inefficient memory manage-\nment can further decrease the batch size, as shown in Fig. 2.\nAdditionally, given the current trends, the GPU\u2019s computa-\ntion speed grows faster than the memory capacity [17]. For\nexample, from NVIDIA A100 to H100, The FLOPS increases\nby more than 2x, but the GPU memory stays at 80GB max-\nimum. Therefore, we believe the memory will become an\nincreasingly significant bottleneck.\nComplex decoding algorithms. LLM services offer a range\nof decoding algorithms for users to select from, each with\nvarying implications for memory management complexity.\nFor example, when users request multiple random samples\nfrom a single input prompt, a typical use case in program\nsuggestion [18], the KV cache of the prompt part, which\naccounts for 12% of the total KV cache memory in our ex-\nperiment (\u00a76.3), can be shared to minimize memory usage.\nOn the other hand, the KV cache during the autoregressive\ngeneration phase should remain unshared due to the dif-\nferent sample results and their dependence on context and\nposition. The extent of KV cache sharing depends on the\nspecific decoding algorithm employed. In more sophisticated\nalgorithms like beam search [49], different request beams\ncan share larger portions (up to 55% memory saving, see\n\u00a76.3) of their KV cache, and the sharing pattern evolves as\nthe decoding process advances.\nScheduling for unknown input & output lengths. The\nrequests to an LLM service exhibit variability in their input\nand output lengths. This requires the memory management\nsystem to accommodate a wide range of prompt lengths. In\naddition, as the output length of a request grows at decoding,\nthe memory required for its KV cache also expands and may\nexhaust available memory for incoming requests or ongoing\ngeneration for existing prompts. The system needs to make\nscheduling decisions, such as deleting or swapping out the\nKV cache of some requests from GPU memory.\n3.1\nMemory Management in Existing Systems\nSince most operators in current deep learning frameworks\n[33, 39] require tensors to be stored in contiguous memory,\nprevious LLM serving systems [31, 60] also store the KV\ncache of one request as a contiguous tensor across the differ-\nent positions. Due to the unpredictable output lengths from\nthe LLM, they statically allocate a chunk of memory for a\nrequest based on the request\u2019s maximum possible sequence\nlength, irrespective of the actual input or eventual output\nlength of the request.\nFig. 3 illustrates two requests: request A with 2048 max-\nimum possible sequence length and request B with a max-\nimum of 512. The chunk pre-allocation scheme in existing\nsystems has three primary sources of memory wastes: re-\nserved slots for future tokens, internal fragmentation due to\nover-provisioning for potential maximum sequence lengths,\nand external fragmentation from the memory allocator like\nthe buddy allocator. The external fragmentation will never\nbe used for generated tokens, which is known before serving\na request. Internal fragmentation also remains unused, but\nthis is only realized after a request has finished sampling.\nThey are both pure memory waste. Although the reserved\nmemory is eventually used, reserving this space for the en-\ntire request\u2019s duration, especially when the reserved space\nis large, occupies the space that could otherwise be used to\nprocess other requests. We visualize the average percentage\nof memory wastes in our experiments in Fig. 2, revealing\nthat the actual effective memory in previous systems can be\nas low as 20.4%.\n4\nKV Cache Manager\nScheduler\nCPU Block \nAllocator\nGPU Block \nAllocator\nBlock tables\nWorker 0\nModel\nShard 0\nCache\nEngine\nWorker 1\nModel\nShard 1\nCache\nEngine\nWorker N - 1\nModel\nShard N - 1\nCache\nEngine\n\u2026\nFigure 4. vLLM system overview.\nAlthough compaction [54] has been proposed as a poten-\ntial solution to fragmentation, performing compaction in a\nperformance-sensitive LLM serving system is impractical\ndue to the massive KV cache. Even with compaction, the\npre-allocated chunk space for each request prevents memory\nsharing specific to decoding algorithms in existing memory\nmanagement systems.\n4\nMethod\nIn this work, we develop a new attention algorithm, Page-\ndAttention, and build an LLM serving engine, vLLM, to tackle\nthe challenges outlined in \u00a73. The architecture of vLLM is\nshown in Fig. 4. vLLM adopts a centralized scheduler to\ncoordinate the execution of distributed GPU workers. The\nKV cache manager effectively manages the KV cache in a\npaged fashion, enabled by PagedAttention. Specifically, the\nKV cache manager manages the physical KV cache memory\non the GPU workers through the instructions sent by the\ncentralized scheduler.\nNext, We describe the PagedAttention algorithm in \u00a74.1.\nWith that, we show the design of the KV cache manager in\n\u00a74.2 and how it facilitates PagedAttention in \u00a74.3, respec-\ntively. Then, we show how this design facilitates effective\nmemory management for various decoding methods (\u00a74.4)\nand handles the variable length input and output sequences\n(\u00a74.5). Finally, we show how the system design of vLLM\nworks in a distributed setting (\u00a74.6).\n4.1\nPagedAttention\nTo address the memory challenges in \u00a73, we introduce Page-\ndAttention, an attention algorithm inspired by the classic idea\nof paging [25] in operating systems. Unlike the traditional\nattention algorithms, PagedAttention allows storing continu-\nous keys and values in non-contiguous memory space. Specif-\nically, PagedAttention partitions the KV cache of each se-\nquence into KV blocks. Each block contains the key and value\nvectors for a fixed number of tokens,1 which we denote as KV\n1In Transformer, each token has a set of key and value vectors across layers\nand attention heads within a layer. All the key and value vectors can be\nmanaged together within a single KV block, or the key and value vectors at\ndifferent heads and layers can each have a separate block and be managed\nin separate block tables. The two designs have no performance difference\nand we choose the second one for easy implementation.\nforth\nQuery \nvector\nyears\nago\nour\nfathers\nbrought\nforth\nFour\nscore\nand\nseven\nKey and value vectors\nBlock 1\nBlock 2\nBlock 0\nFigure 5. Illustration of the PagedAttention algorithm,\nwhere the attention key and values vectors are stored as\nnon-contiguous blocks in the memory.\nblock size (\ud835\udc35). Denote the key block \ud835\udc3e\ud835\udc57 = (\ud835\udc58(\ud835\udc57\u22121)\ud835\udc35+1, . . . ,\ud835\udc58\ud835\udc57\ud835\udc35)\nand value block \ud835\udc49\ud835\udc57 = (\ud835\udc63 (\ud835\udc57\u22121)\ud835\udc35+1, . . . , \ud835\udc63 \ud835\udc57\ud835\udc35). The attention com-\nputation in Eq. 4 can be transformed into the following block-\nwise computation:\n\ud835\udc34\ud835\udc56\ud835\udc57 =\nexp(\ud835\udc5e\u22a4\n\ud835\udc56 \ud835\udc3e\ud835\udc57/\n\u221a\n\ud835\udc51)\n\u00cd\u2308\ud835\udc56/\ud835\udc35\u2309\n\ud835\udc61=1\nexp(\ud835\udc5e\u22a4\n\ud835\udc56 \ud835\udc3e\ud835\udc611/\n\u221a\n\ud835\udc51)\n, \ud835\udc5c\ud835\udc56 =\n\u2308\ud835\udc56/\ud835\udc35\u2309\n\u2211\ufe01\n\ud835\udc57=1\n\ud835\udc49\ud835\udc57\ud835\udc34\u22a4\n\ud835\udc56\ud835\udc57,\n(4)\nwhere \ud835\udc34\ud835\udc56\ud835\udc57 = (\ud835\udc4e\ud835\udc56,(\ud835\udc57\u22121)\ud835\udc35+1, . . . ,\ud835\udc4e\ud835\udc56,\ud835\udc57\ud835\udc35) is the row vector of atten-\ntion score on \ud835\udc57-th KV block.\nDuring the attention computation, the PagedAttention\nkernel identifies and fetches different KV blocks separately.\nWe show an example of PagedAttention in Fig. 5: The key\nand value vectors are spread across three blocks, and the\nthree blocks are not contiguous on the physical memory. At\neach time, the kernel multiplies the query vector \ud835\udc5e\ud835\udc56 of the\nquery token (\u201cforth\u201d) and the key vectors \ud835\udc3e\ud835\udc57 in a block (e.g.,\nkey vectors of \u201cFour score and seven\u201d for block 0) to compute\nthe attention score\ud835\udc34\ud835\udc56\ud835\udc57, and later multiplies\ud835\udc34\ud835\udc56\ud835\udc57 with the value\nvectors \ud835\udc49\ud835\udc57 in a block to derive the final attention output \ud835\udc5c\ud835\udc56.\nIn summary, the PagedAttention algorithm allows the\nKV blocks to be stored in non-contiguous physical memory,\nwhich enables more flexible paged memory management in\nvLLM.\n4.2\nKV Cache Manager\nThe key idea behind vLLM\u2019s memory manager is analogous\nto the virtual memory [25] in operating systems. OS parti-\ntions memory into fixed-sized pages and maps user programs\u2019\nlogical pages to physical pages. Contiguous logical pages can\ncorrespond to non-contiguous physical memory pages, al-\nlowing user programs to access memory as though it were\ncontiguous. Moreover, physical memory space needs not to\nbe fully reserved in advance, enabling the OS to dynamically\nallocate physical pages as needed. vLLM uses the ideas be-\nhind virtual memory to manage the KV cache in an LLM\nservice. Enabled by PagedAttention, we organize the KV\ncache as fixed-size KV blocks, like pages in virtual memory.\nA request\u2019s KV cache is represented as a series of logical\nKV blocks, filled from left to right as new tokens and their KV\ncache are generated. The last KV block\u2019s unfilled positions\nare reserved for future generations. On GPU workers, a block\nengine allocates a contiguous chunk of GPU DRAM and\n5\nRequest\nA\nFour\nscore\nand\nseven\nyears\nago\nour\nfathers\nbrought\nPrompt: \u201cFour score and seven years ago our\u201d\nOutputs: \u201cfathers\u201d \u2192 \u201cbrought\u201d \u2192 \u2026\nBlock 0\nBlock 1\nBlock 2\nBlock 3\nyears\nago\nour\nfathers\nbrought\nFour\nscore\nand\nseven\nPhysical KV blocks\n(on GPU DRAM)\nBlock 0\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7\nBlock 8\nLogical KV blocks\nPhysical block \nnumber\n# filled\n7\n4\n1\n3 \u2192 4\n3\n1\n\u2013\n\u2013\nBlock Table\n1\n1\n1\n1\n1\n1\n1\n2\n3\n1\n1\n1\n1\n2\n3\n3\n3\n1\n1\n1\n1\n3\n1\n1\n1\n2\n1\n1\n1\n1\n3\nFigure 6. Block table translation in vLLM.\ndivides it into physical KV blocks (this is also done on CPU\nRAM for swapping; see \u00a74.5). The KV block manager also\nmaintains block tables\u2014the mapping between logical and\nphysical KV blocks of each request. Each block table entry\nrecords the corresponding physical blocks of a logical block\nand the number of filled positions. Separating logical and\nphysical KV blocks allows vLLM to dynamically grow the\nKV cache memory without reserving it for all positions in\nadvance, which eliminates most memory waste in existing\nsystems, as in Fig. 2.\n4.3\nDecoding with PagedAttention and vLLM\nNext, we walk through an example, as in Fig. 6, to demon-\nstrate how vLLM executes PagedAttention and manages the\nmemory during the decoding process of a single input se-\nquence: 1\u25cb As in OS\u2019s virtual memory, vLLM does not require\nreserving the memory for the maximum possible generated\nsequence length initially. Instead, it reserves only the nec-\nessary KV blocks to accommodate the KV cache generated\nduring prompt computation. In this case, The prompt has 7\ntokens, so vLLM maps the first 2 logical KV blocks (0 and\n1) to 2 physical KV blocks (7 and 1, respectively). In the\nprefill step, vLLM generates the KV cache of the prompts\nand the first output token with a conventional self-attention\nalgorithm (e.g., [13]). vLLM then stores the KV cache of the\nfirst 4 tokens in logical block 0 and the following 3 tokens\nin logical block 1. The remaining slot is reserved for the\nsubsequent autoregressive generation phase. 2\u25cb In the first\nautoregressive decoding step, vLLM generates the new token\nwith the PagedAttention algorithm on physical blocks 7 and\n1. Since one slot remains available in the last logical block,\nthe newly generated KV cache is stored there, and the block\ntable\u2019s #filled record is updated. 3\u25cb At the second decoding\nstep, as the last logical block is full, vLLM stores the newly\ngenerated KV cache in a new logical block; vLLM allocates a\nnew physical block (physical block 3) for it and stores this\nmapping in the block table.\nGlobally, for each decoding iteration, vLLM first selects\na set of candidate sequences for batching (more in \u00a74.5),\nand allocates the physical blocks for the newly required\nlogical blocks. Then, vLLM concatenates all the input tokens\nof the current iteration (i.e., all tokens for prompt phase\nFour\nscore\nand\nseven\nyears\nago\nour\nfathers\nbrought\nBlock 0\nBlock 1\nBlock 2\nBlock 3\nyears\nago\nour\nfathers\nof\ntimes\nbrought\nIt\nwas\nthe\nbest\nFour\nscore\nand\nseven\nPhysical KV blocks\nBlock 0\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7\nBlock 8\nLogical KV blocks\nIt\nwas\nthe\nbest\nof\ntimes\nBlock 0\nBlock 1\nBlock 2\nLogical KV blocks\nRequest\nA\nRequest\nB\nFigure 7. Storing the KV cache of two requests at the same\ntime in vLLM.\nrequests and the latest tokens for generation phase requests)\nas one sequence and feeds it into the LLM. During LLM\u2019s\ncomputation, vLLM uses the PagedAttention kernel to access\nthe previous KV cache stored in the form of logical KV blocks\nand saves the newly generated KV cache into the physical\nKV blocks. Storing multiple tokens within a KV block (block\nsize > 1) enables the PagedAttention kernel to process the\nKV cache across more positions in parallel, thus increasing\nthe hardware utilization and reducing latency. However, a\nlarger block size also increases memory fragmentation. We\nstudy the effect of block size in \u00a77.2.\nAgain, vLLM dynamically assigns new physical blocks to\nlogical blocks as more tokens and their KV cache are gener-\nated. As all the blocks are filled from left to right and a new\nphysical block is only allocated when all previous blocks\nare full, vLLM limits all the memory wastes for a request\nwithin one block, so it can effectively utilize all the memory,\nas shown in Fig. 2. This allows more requests to fit into mem-\nory for batching\u2014hence improving the throughput. Once a\nrequest finishes its generation, its KV blocks can be freed to\nstore the KV cache of other requests. In Fig. 7, we show an\nexample of vLLM managing the memory for two sequences.\nThe logical blocks of the two sequences are mapped to differ-\nent physical blocks within the space reserved by the block\nengine in GPU workers. The neighboring logical blocks of\nboth sequences do not need to be contiguous in physical GPU\nmemory and the space of physical blocks can be effectively\nutilized by both sequences.\n4.4\nApplication to Other Decoding Scenarios\n\u00a74.3 shows how PagedAttention and vLLM handle basic de-\ncoding algorithms, such as greedy decoding and sampling,\nthat take one user prompt as input and generate a single out-\nput sequence. In many successful LLM applications [18, 34],\nan LLM service must offer more complex decoding scenarios\nthat exhibit complex accessing patterns and more opportuni-\nties for memory sharing. We show the general applicability\nof vLLM on them in this section.\nParallel sampling. In LLM-based program assistants [6, 18],\nan LLM generates multiple sampled outputs for a single in-\nput prompt; users can choose a favorite output from various\ncandidates. So far we have implicitly assumed that a request\n6\nSample\nA1\nFour\nscore\nand\nseven\nyears\nago\nour\nfathers\nBlock 0\nBlock 1\nyears\nago\nour\nmothers\nyears\nago\nour\nfathers\nFour\nscore\nand\nseven\nPhysical KV blocks\nBlock 0\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7\nBlock 8\nLogical KV blocks\nFour\nscore\nand\nseven\nyears\nago\nour\nmothers\nBlock 0\nBlock 1\nLogical KV blocks\nSample\nA2\nCopy-on-write\nRef count: 2 \u2192 1\nFigure 8. Parallel sampling example.\ngenerates a single sequence. In the remainder of this paper,\nwe assume the more general case in which a request gener-\nates multiple sequences. In parallel sampling, one request\nincludes multiple samples sharing the same input prompt,\nallowing the KV cache of the prompt to be shared as well. Via\nits PagedAttention and paged memory management, vLLM\ncan realize this sharing easily and save memory.\nFig. 8 shows an example of parallel decoding for two out-\nputs. Since both outputs share the same prompt, we only\nreserve space for one copy of the prompt\u2019s state at the prompt\nphase; the logical blocks for the prompts of both sequences\nare mapped to the same physical blocks: the logical block 0\nand 1 of both sequences are mapped to physical blocks 7 and\n1, respectively. Since a single physical block can be mapped\nto multiple logical blocks, we introduce a reference count for\neach physical block. In this case, the reference counts for\nphysical blocks 7 and 1 are both 2. At the generation phase,\nthe two outputs sample different output tokens and need\nseparate storage for KV cache. vLLM implements a copy-on-\nwrite mechanism at the block granularity for the physical\nblocks that need modification by multiple sequences, similar\nto the copy-on-write technique in OS virtual memory (e.g.,\nwhen forking a process). Specifically, in Fig. 8, when sample\nA1 needs to write to its last logical block (logical block 1),\nvLLM recognizes that the reference count of the correspond-\ning physical block (physical block 1) is greater than 1; it\nallocates a new physical block (physical block 3), instructs\nthe block engine to copy the information from physical block\n1, and decreases the reference count to 1. Next, when sample\nA2 writes to physical block 1, the reference count is already\nreduced to 1; thus A2 directly writes its newly generated KV\ncache to physical block 1.\nIn summary, vLLM enables the sharing of most of the\nspace used to store the prompts\u2019 KV cache across multiple\noutput samples, with the exception of the final logical block,\nwhich is managed by a copy-on-write mechanism. By sharing\nphysical blocks across multiple samples, memory usage can\nbe greatly reduced, especially for long input prompts.\nBeam search. In LLM tasks like machine translation [59],\nthe users expect the top-\ud835\udc58 most appropriate translations out-\nput by the LLM. Beam search [49] is widely used to decode\nthe most probable output sequence from an LLM, as it miti-\ngates the computational complexity of fully traversing the\nBlock 10\nBlock 11\nBlock 1\nBlock 3\nBlock 6\nBlock 7\nBlock 5\nBlock 0\nBlock 2\nBlock 4\nBlock 8\nBlock 9\nBlock 12\nBeam candidate 0\nBeam candidate 1\nBeam candidate 2\nBeam candidate 3\nFigure 9. Beam search example.\nsample space. The algorithm relies on the beam width pa-\nrameter \ud835\udc58, which determines the number of top candidates\nretained at every step. During decoding, beam search ex-\npands each candidate sequence in the beam by considering\nall possible tokens, computes their respective probabilities us-\ning the LLM, and retains the top-\ud835\udc58 most probable sequences\nout of \ud835\udc58 \u00b7 |\ud835\udc49 | candidates, where |\ud835\udc49 | is the vocabulary size.\nUnlike parallel decoding, beam search facilities sharing\nnot only the initial prompt blocks but also other blocks across\ndifferent candidates, and the sharing patterns dynamically\nchange as the decoding process advances, similar to the pro-\ncess tree in the OS created by compound forks. Fig. 9 shows\nhow vLLM manages the KV blocks for a beam search ex-\nample with \ud835\udc58 = 4. Prior to the iteration illustrated as the\ndotted line, each candidate sequence has used 4 full logi-\ncal blocks. All beam candidates share the first block 0 (i.e.,\nprompt). Candidate 3 digresses from others from the second\nblock. Candidates 0-2 share the first 3 blocks and diverge at\nthe fourth block. At subsequent iterations, the top-4 prob-\nable candidates all originate from candidates 1 and 2. As\nthe original candidates 0 and 3 are no longer among the\ntop candidates, their logical blocks are freed, and the refer-\nence counts of corresponding physical blocks are reduced.\nvLLM frees all physical blocks whose reference counts reach\n0 (blocks 2, 4, 5, 8). Then, vLLM allocates new physical blocks\n(blocks 9-12) to store the new KV cache from the new can-\ndidates. Now, all candidates share blocks 0, 1, 3; candidates\n0 and 1 share block 6, and candidates 2 and 3 further share\nblock 7.\nPrevious LLM serving systems require frequent memory\ncopies of the KV cache across the beam candidates. For exam-\nple, in the case shown in Fig. 9, after the dotted line, candidate\n3 would need to copy a large portion of candidate 2\u2019s KV\ncache to continue generation. This frequent memory copy\noverhead is significantly reduced by vLLM\u2019s physical block\nsharing. In vLLM, most blocks of different beam candidates\ncan be shared. The copy-on-write mechanism is applied only\nwhen the newly generated tokens are within an old shared\nblock, as in parallel decoding. This involves only copying\none block of data.\nShared prefix. Commonly, the LLM user provides a (long)\ndescription of the task including instructions and example\ninputs and outputs, also known as system prompt [36]. The\ndescription is concatenated with the actual task input to form\nthe prompt of the request. The LLM generates outputs based\n7\nTranslate English to French:\n\u201csea otter\u201d => \u201cloutre de mer\u201d\n\u201cpeppermint\u201d => \u201cmenthe poivr\u00e9e\u201d\n\u201cplush girafe\u201d => \u201cgirafe en peluche\u201d\n\u201ccheese\u201d =>\n\u201cfromage\u201d\nTranslate English to French:\n\u201csea otter\u201d => \u201cloutre de mer\u201d\n\u201cpeppermint\u201d => \u201cmenthe poivr\u00e9e\u201d\n\u201cplush girafe\u201d => \u201cgirafe en peluche\u201d\n\u201cI love you\u201d =>\n\u201cJe t\u2019amie\u201d\nShared prefix\nTask input\nTask output\nSequence A\nPrompt\nSequence B\nPrompt\nSequence A\nLLM output\nSequence B\nLLM output\nFigure 10. Shared prompt example for machine translation.\nThe examples are adopted from [5].\non the full prompt. Fig. 10 shows an example. Moreover, the\nshared prefix can be further tuned, via prompt engineering,\nto improve the accuracy of the downstream tasks [26, 27].\nFor this type of application, many user prompts share a\nprefix, thus the LLM service provider can store the KV cache\nof the prefix in advance to reduce the redundant computa-\ntion spent on the prefix. In vLLM, this can be conveniently\nachieved by reserving a set of physical blocks for a set of\npredefined shared prefixes by the LLM service provider, as\nhow OS handles shared library across processes. A user in-\nput prompt with the shared prefix can simply map its logi-\ncal blocks to the cached physical blocks (with the last block\nmarked copy-on-write). The prompt phase computation only\nneeds to execute on the user\u2019s task input.\nMixed decoding methods. The decoding methods dis-\ncussed earlier exhibit diverse memory sharing and access-\ning patterns. Nonetheless, vLLM facilitates the simultane-\nous processing of requests with different decoding prefer-\nences, which existing systems cannot efficiently do. This is\nbecause vLLM conceals the complex memory sharing be-\ntween different sequences via a common mapping layer that\ntranslates logical blocks to physical blocks. The LLM and\nits execution kernel only see a list of physical block IDs\nfor each sequence and do not need to handle sharing pat-\nterns across sequences. Compared to existing systems, this\napproach broadens the batching opportunities for requests\nwith different sampling requirements, ultimately increasing\nthe system\u2019s overall throughput.\n4.5\nScheduling and Preemption\nWhen the request traffic surpasses the system\u2019s capacity,\nvLLM must prioritize a subset of requests. In vLLM, we adopt\nthe first-come-first-serve (FCFS) scheduling policy for all\nrequests, ensuring fairness and preventing starvation. When\nvLLM needs to preempt requests, it ensures that the earliest\narrived requests are served first and the latest requests are\npreempted first.\nLLM services face a unique challenge: the input prompts\nfor an LLM can vary significantly in length, and the resulting\noutput lengths are not known a priori, contingent on both\nthe input prompt and the model. As the number of requests\nand their outputs grow, vLLM can run out of the GPU\u2019s phys-\nical blocks to store the newly generated KV cache. There\nare two classic questions that vLLM needs to answer in this\ncontext: (1) Which blocks should it evict? (2) How to recover\nevicted blocks if needed again? Typically, eviction policies\nuse heuristics to predict which block will be accessed fur-\nthest in the future and evict that block. Since in our case we\nknow that all blocks of a sequence are accessed together, we\nimplement an all-or-nothing eviction policy, i.e., either evict\nall or none of the blocks of a sequence. Furthermore, multi-\nple sequences within one request (e.g., beam candidates in\none beam search request) are gang-scheduled as a sequence\ngroup. The sequences within one sequence group are always\npreempted or rescheduled together due to potential memory\nsharing across those sequences. To answer the second ques-\ntion of how to recover an evicted block, we consider two\ntechniques:\nSwapping. This is the classic technique used by most virtual\nmemory implementations which copy the evicted pages to a\nswap space on the disk. In our case, we copy evicted blocks to\nthe CPU memory. As shown in Fig. 4, besides the GPU block\nallocator, vLLM includes a CPU block allocator to manage\nthe physical blocks swapped to CPU RAM. When vLLM\nexhausts free physical blocks for new tokens, it selects a set\nof sequences to evict and transfer their KV cache to the CPU.\nOnce it preempts a sequence and evicts its blocks, vLLM\nstops accepting new requests until all preempted sequences\nare completed. Once a request completes, its blocks are freed\nfrom memory, and the blocks of a preempted sequence are\nbrought back in to continue the processing of that sequence.\nNote that with this design, the number of blocks swapped to\nthe CPU RAM never exceeds the number of total physical\nblocks in the GPU RAM, so the swap space on the CPU RAM\nis bounded by the GPU memory allocated for the KV cache.\nRecomputation. In this case, we simply recompute the KV\ncache when the preempted sequences are rescheduled. Note\nthat recomputation latency can be significantly lower than\nthe original latency, as the tokens generated at decoding\ncan be concatenated with the original user prompt as a new\nprompt\u2014their KV cache at all positions can be generated in\none prompt phase iteration.\nThe performances of swapping and recomputation depend\non the bandwidth between CPU RAM and GPU memory and\nthe computation power of the GPU. We examine the speeds\nof swapping and recomputation in \u00a77.3.\n4.6\nDistributed Execution\nMany LLMs have parameter sizes exceeding the capacity of a\nsingle GPU [5, 9]. Therefore, it is necessary to partition them\nacross distributed GPUs and execute them in a model parallel\nfashion [28, 63]. This calls for a memory manager capable of\nhandling distributed memory. vLLM is effective in distributed\nsettings by supporting the widely used Megatron-LM style\ntensor model parallelism strategy on Transformers [47]. This\nstrategy adheres to an SPMD (Single Program Multiple Data)\nexecution schedule, wherein the linear layers are partitioned\n8\nTable 1. Model sizes and server configurations.\nModel size\n13B\n66B\n175B\nGPUs\nA100\n4\u00d7A100\n8\u00d7A100-80GB\nTotal GPU memory\n40 GB\n160 GB\n640 GB\nParameter size\n26 GB\n132 GB\n346 GB\nMemory for KV cache\n12 GB\n21 GB\n264 GB\nMax. # KV cache slots\n15.7K\n9.7K\n60.1K\nto perform block-wise matrix multiplication, and the the\nGPUs constantly synchronize intermediate results via an all-\nreduce operation. Specifically, the attention operator is split\non the attention head dimension, each SPMD process takes\ncare of a subset of attention heads in multi-head attention.\nWe observe that even with model parallel execution, each\nmodel shard still processes the same set of input tokens, thus\nrequiring the KV Cache for the same positions. Therefore,\nvLLM features a single KV cache manager within the cen-\ntralized scheduler, as in Fig. 4. Different GPU workers share\nthe manager, as well as the mapping from logical blocks to\nphysical blocks. This common mapping allows GPU workers\nto execute the model with the physical blocks provided by\nthe scheduler for each input request. Although each GPU\nworker has the same physical block IDs, a worker only stores\na portion of the KV cache for its corresponding attention\nheads.\nIn each step, the scheduler first prepares the message with\ninput token IDs for each request in the batch, as well as the\nblock table for each request. Next, the scheduler broadcasts\nthis control message to the GPU workers. Then, the GPU\nworkers start to execute the model with the input token IDs.\nIn the attention layers, the GPU workers read the KV cache\naccording to the block table in the control message. During\nexecution, the GPU workers synchronize the intermediate\nresults with the all-reduce communication primitive without\nthe coordination of the scheduler, as in [47]. In the end, the\nGPU workers send the sampled tokens of this iteration back\nto the scheduler. In summary, GPU workers do not need\nto synchronize on memory management as they only need\nto receive all the memory management information at the\nbeginning of each decoding iteration along with the step\ninputs.\n5\nImplementation\nvLLM is an end-to-end serving system with a FastAPI [15]\nfrontend and a GPU-based inference engine. The frontend\nextends the OpenAI API [34] interface, allowing users to\ncustomize sampling parameters for each request, such as\nthe maximum sequence length and the beam width \ud835\udc58. The\nvLLM engine is written in 8.5K lines of Python and 2K lines of\nC++/CUDA code. We develop control-related components in-\ncluding the scheduler and the block manager in Python while\ndeveloping custom CUDA kernels for key operations such as\nPagedAttention. For the model executor, we implement pop-\nular LLMs such as GPT [5], OPT [62], and LLaMA [52] using\n0\n500\n1000\n1500\n2000\n# Tokens\n0.0\n0.5\n1.0\n1.5\n2.0\nDensity\n1e\u22122\nInput (mean: 161.31)\nOutput (mean: 337.99)\n(a) ShareGPT\n0\n500\n1000\n1500\n2000\n# Tokens\n0\n2\n4\n6\n8\nDensity\n1e\u22122\nInput (mean: 19.31)\nOutput (mean: 58.45)\n(b) Alpaca\nFigure 11. Input and output length distributions of the (a)\nShareGPT and (b) Alpaca datasets.\nPyTorch [39] and Transformers [58]. We use NCCL [32] for\ntensor communication across the distributed GPU workers.\n5.1\nKernel-level Optimization\nSince PagedAttention introduces memory access patterns\nthat are not efficiently supported by existing systems, we\ndevelop several GPU kernels for optimizing it. (1) Fused re-\nshape and block write. In every Transformer layer, the new\nKV cache are split into blocks, reshaped to a memory layout\noptimized for block read, then saved at positions specified\nby the block table. To minimize kernel launch overheads, we\nfuse them into a single kernel. (2) Fusing block read and atten-\ntion. We adapt the attention kernel in FasterTransformer [31]\nto read KV cache according to the block table and perform\nattention operations on the fly. To ensure coalesced memory\naccess, we assign a GPU warp to read each block. More-\nover, we add support for variable sequence lengths within a\nrequest batch. (3) Fused block copy. Block copy operations,\nissued by the copy-on-write mechanism, may operate on\ndiscontinuous blocks. This can lead to numerous invocations\nof small data movements if we use the cudaMemcpyAsync\nAPI. To mitigate the overhead, we implement a kernel that\nbatches the copy operations for different blocks into a single\nkernel launch.\n5.2\nSupporting Various Decoding Algorithms\nvLLM implements various decoding algorithms using three\nkey methods: fork, append, and free. The fork method\ncreates a new sequence from an existing one. The append\nmethod appends a new token to the sequence. Finally, the\nfree method deletes the sequence. For instance, in paral-\nlel sampling, vLLM creates multiple output sequences from\nthe single input sequence using the fork method. It then\nadds new tokens to these sequences in every iteration with\nappend, and deletes sequences that meet a stopping condi-\ntion using free. The same strategy is also applied in beam\nsearch and prefix sharing by vLLM. We believe future decod-\ning algorithms can also be supported by combining these\nmethods.\n6\nEvaluation\nIn this section, we evaluate the performance of vLLM under\na variety of workloads.\n9\n0.0\n0.5\n1.0\n1.5\n2.0\nRequest rate (req/s)\n(a) OPT-13B, 1 GPU, ShareGPT\n0.0\n0.5\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRequest rate (req/s)\n(b) OPT-66B, 4 GPUs, ShareGPT\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nRequest rate (req/s)\n(c) OPT-175B, 8 GPUs, ShareGPT\n0.0\n0.5\n1.0\nNormalized latency\n       (s/token)\nFasterTransformer\nOrca (Max)\nOrca (Pow2)\nOrca (Oracle)\nvLLM\n0\n10\n20\n30\nRequest rate (req/s)\n(d) OPT-13B, 1 GPU, Alpaca\n0.0\n0.5\n1.0\n0\n5\n10\n15\n20\nRequest rate (req/s)\n(e) OPT-66B, 4 GPUs, Alpaca\n0.0\n0.5\n1.0\n0\n5\n10\n15\n20\nRequest rate (req/s)\n(f) OPT-175B, 8 GPUs, Alpaca\n0.0\n0.5\n1.0\nNormalized latency\n       (s/token)\nFigure 12. Single sequence generation with OPT models on the ShareGPT and Alpaca dataset\nOrca\n(Max)\nOrca\n(Pow2)\nOrca\n(Oracle)\nvLLM\n0\n5\n10\n15\n20\n25\n30\n35\n# Batched requests\n7.00\n9.81\n13.62\n30.42\n(a) ShareGPT\nOrca\n(Max)\nOrca\n(Pow2)\nOrca\n(Oracle)\nvLLM\n0\n25\n50\n75\n100\n125\n150\n# Batched requests\n7.00\n43.24\n72.75\n132.44\n(b) Alpaca\nFigure 13. Average number of batched requests when serv-\ning OPT-13B for the ShareGPT (2 reqs/s) and Alpaca (30\nreqs/s) traces.\n6.1\nExperimental Setup\nModel and server configurations. We use OPT [62] mod-\nels with 13B, 66B, and 175B parameters and LLaMA [52] with\n13B parameters for our evaluation. 13B and 66B are popular\nsizes for LLMs as shown in an LLM leaderboard [38], while\n175B is the size of the famous GPT-3 [5] model. For all of\nour experiments, we use A2 instances with NVIDIA A100\nGPUs on Google Cloud Platform. The detailed model sizes\nand server configurations are shown in Table 1.\nWorkloads. We synthesize workloads based on ShareGPT [51]\nand Alpaca [50] datasets, which contain input and output\ntexts of real LLM services. The ShareGPT dataset is a collec-\ntion of user-shared conversations with ChatGPT [35]. The\nAlpaca dataset is an instruction dataset generated by GPT-\n3.5 with self-instruct [57]. We tokenize the datasets and use\ntheir input and output lengths to synthesize client requests.\nAs shown in Fig. 11, the ShareGPT dataset has 8.4\u00d7 longer\ninput prompts and 5.8\u00d7 longer outputs on average than the\nAlpaca dataset, with higher variance. Since these datasets do\nnot include timestamps, we generate request arrival times\nusing Poisson distribution with different request rates.\nBaseline 1: FasterTransformer. FasterTransformer [31] is\na distributed inference engine highly optimized for latency.\nAs FasterTransformer does not have its own scheduler, we\nimplement a custom scheduler with a dynamic batching\nmechanism similar to the existing serving systems such as\nTriton [30]. Specifically, we set a maximum batch size \ud835\udc35 as\nlarge as possible for each experiment, according to the GPU\nmemory capacity. The scheduler takes up to \ud835\udc35 number of\nearliest arrived requests and sends the batch to FasterTrans-\nformer for processing.\nBaseline 2: Orca. Orca [60] is a state-of-the-art LLM serving\nsystem optimized for throughput. Since Orca is not publicly\navailable for use, we implement our own version of Orca. We\nassume Orca uses the buddy allocation algorithm to deter-\nmine the memory address to store KV cache. We implement\nthree versions of Orca based on how much it over-reserves\nthe space for request outputs:\n\u2022 Orca (Oracle). We assume the system has the knowledge\nof the lengths of the outputs that will be actually generated\nfor the requests. This shows the upper-bound performance\nof Orca, which is infeasible to achieve in practice.\n\u2022 Orca (Pow2). We assume the system over-reserves the\nspace for outputs by at most 2\u00d7. For example, if the true\noutput length is 25, it reserves 32 positions for outputs.\n\u2022 Orca (Max). We assume the system always reserves the\nspace up to the maximum sequence length of the model,\ni.e., 2048 tokens.\nKey metrics. We focus on serving throughput. Specifically,\nusing the workloads with different request rates, we mea-\nsure normalized latency of the systems, the mean of every\nrequest\u2019s end-to-end latency divided by its output length,\nas in Orca [60]. A high-throughput serving system should\nretain low normalized latency against high request rates.\nFor most experiments, we evaluate the systems with 1-hour\ntraces. As an exception, we use 15-minute traces for the\nOPT-175B model due to the cost limit.\n10\n0\n5\n10\n15\nRequest rate (req/s)\n(a) parallel generation (parallel size = 2)\n0.0\n0.5\n1.0\n0\n2\n4\n6\n8\n10\nRequest rate (req/s)\n(b) parallel generation (parallel size = 4)\n0.0\n0.5\n1.0\n0\n2\n4\n6\nRequest rate (req/s)\n(c) parallel generation (parallel size = 6)\n0.0\n0.5\n1.0\nNormalized latency\n       (s/token)\nOrca (Max)\nOrca (Pow2)\nOrca (Oracle)\nvLLM\n0\n5\n10\n15\nRequest rate (req/s)\n(d) beam search (beam width = 2)\n0.0\n0.5\n1.0\n0\n2\n4\n6\n8\n10\nRequest rate (req/s)\n(e) beam search (beam width = 4)\n0.0\n0.5\n1.0\n0\n2\n4\n6\nRequest rate (req/s)\n(f) beam search (beam width = 6)\n0.0\n0.5\n1.0\nNormalized latency\n       (s/token)\nFigure 14. Parallel generation and beam search with OPT-13B on the Alpaca dataset.\n6.2\nBasic Sampling\nWe evaluate the performance of vLLM with basic sampling\n(one sample per request) on three models and two datasets.\nThe first row of Fig. 12 shows the results on the ShareGPT\ndataset. The curves illustrate that as the request rate in-\ncreases, the latency initially increases at a gradual pace but\nthen suddenly explodes. This can be attributed to the fact\nthat when the request rate surpasses the capacity of the serv-\ning system, the queue length continues to grow infinitely\nand so does the latency of the requests.\nOn the ShareGPT dataset, vLLM can sustain 1.7\u00d7\u20132.7\u00d7\nhigher request rates compared to Orca (Oracle) and 2.7\u00d7\u20138\u00d7\ncompared to Orca (Max), while maintaining similar laten-\ncies. This is because vLLM\u2019s PagedAttention can efficiently\nmanage the memory usage and thus enable batching more\nrequests than Orca. For example, as shown in Fig. 13a, for\nOPT-13B vLLM processes 2.2\u00d7 more requests at the same\ntime than Orca (Oracle) and 4.3\u00d7 more requests than Orca\n(Max). Compared to FasterTransformer, vLLM can sustain up\nto 22\u00d7 higher request rates, as FasterTransformer does not\nutilize a fine-grained scheduling mechanism and inefficiently\nmanages the memory like Orca (Max).\nThe second row of Fig. 12 and Fig. 13b shows the results\non the Alpaca dataset, which follows a similar trend to the\nShareGPT dataset. One exception is Fig. 12 (f), where vLLM\u2019s\nadvantage over Orca (Oracle) and Orca (Pow2) is less pro-\nnounced. This is because the model and server configuration\nfor OPT-175B (Table 1) allows for large GPU memory space\navailable to store KV cache, while the Alpaca dataset has\nshort sequences. In this setup, Orca (Oracle) and Orca (Pow2)\ncan also batch a large number of requests despite the inef-\nficiencies in their memory management. As a result, the\nperformance of the systems becomes compute-bound rather\nthan memory-bound.\n2\n4\n6\n# Output sequences\n0\n4\n8\n12\nMemory saving (%)\n6.09\n8.53\n9.79\n(a) Parallel sampling\n2\n4\n6\nBeam width\n0\n20\n40\n60\nMemory saving (%)\n37.56\n53.13\n55.16\n(b) Beam search\nFigure 15. Average amount of memory saving from sharing\nKV blocks, when serving OPT-13B for the Alpaca trace.\n6.3\nParallel Sampling and Beam Search\nWe evaluate the effectiveness of memory sharing in Page-\ndAttention with two popular sampling methods: parallel\nsampling and beam search. In parallel sampling, all paral-\nlel sequences in a request can share the KV cache for the\nprompt. As shown in the first row of Fig. 14, with a larger\nnumber of sequences to sample, vLLM brings more improve-\nment over the Orca baselines. Similarly, the second row of\nFig. 14 shows the results for beam search with different beam\nwidths. Since beam search allows for more sharing, vLLM\ndemonstrates even greater performance benefits. The im-\nprovement of vLLM over Orca (Oracle) on OPT-13B and the\nAlpaca dataset goes from 1.3\u00d7 in basic sampling to 2.3\u00d7 in\nbeam search with a width of 6.\nFig. 15 plots the amount of memory saving, computed by\nthe number of blocks we saved by sharing divided by the\nnumber of total blocks without sharing. We show 6.1% - 9.8%\nmemory saving on parallel sampling and 37.6% - 55.2% on\nbeam search. In the same experiments with the ShareGPT\ndataset, we saw 16.2% - 30.5% memory saving on parallel\nsampling and 44.3% - 66.3% on beam search.\n6.4\nShared prefix\nWe explore the effectiveness of vLLM for the case a prefix\nis shared among different input prompts, as illustrated in\n11\n0\n20\n40\nRequest rate (req/s)\n(a) 1-shot prefix prompt\n0.0\n0.5\n1.0\n0\n20\n40\nRequest rate (req/s)\n(b) 5-shot prefix prompt\n0.0\n0.5\n1.0\nNormalized latency\n       (s/token)\nOrca (Oracle)\nvLLM\nFigure 16. Translation workload where the input prompts\nshare a common prefix. The prefix includes (a) 1 example\nwith 80 tokens or (b) 5 examples with 341 tokens.\n0.0\n0.2\n0.4\n0.6\n0.8\nRequest rate (req/s)\n0.0\n0.5\n1.0\nNormalized latency\n       (s/token)\nOrca (Max)\nOrca (Pow2)\nOrca (Oracle)\nvLLM\nFigure 17. Performance on chatbot workload.\nFig. 10. For the model, we use LLaMA-13B [52], which is mul-\ntilingual. For the workload, we use the WMT16 [4] English-\nto-German translation dataset and synthesize two prefixes\nthat include an instruction and a few translation examples.\nThe first prefix includes a single example (i.e., one-shot)\nwhile the other prefix includes 5 examples (i.e., few-shot). As\nshown in Fig. 16 (a), vLLM achieves 1.67\u00d7 higher through-\nput than Orca (Oracle) when the one-shot prefix is shared.\nFurthermore, when more examples are shared (Fig. 16 (b)),\nvLLM achieves 3.58\u00d7 higher throughput than Orca (Oracle).\n6.5\nChatbot\nA chatbot [8, 19, 35] is one of the most important applications\nof LLMs. To implement a chatbot, we let the model generate\na response by concatenating the chatting history and the\nlast user query into a prompt. We synthesize the chatting\nhistory and user query using the ShareGPT dataset. Due to\nthe limited context length of the OPT-13B model, we cut the\nprompt to the last 1024 tokens and let the model generate\nat most 1024 tokens. We do not store the KV cache between\ndifferent conversation rounds as doing this would occupy the\nspace for other requests between the conversation rounds.\nFig. 17 shows that vLLM can sustain 2\u00d7 higher request\nrates compared to the three Orca baselines. Since the ShareGPT\ndataset contains many long conversations, the input prompts\nfor most requests have 1024 tokens. Due to the buddy allo-\ncation algorithm, the Orca baselines reserve the space for\n1024 tokens for the request outputs, regardless of how they\npredict the output lengths. For this reason, the three Orca\nbaselines behave similarly. In contrast, vLLM can effectively\n64\n128\n256\nContext length\n0\n50\n100\n150\n200\n250\nKernel latency (us)\nvLLM (bs 8)\nFT (bs 8)\nvLLM (bs 32)\nFT (bs 32)\n(a) Latency of attention kernels.\n1\n2\n4\n8\n16\n32\n64\n128 256\nBlock size\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nNormalized latency (s/token)\nShareGPT\nAlpaca\n(b) End-to-end latency with dif-\nferent block sizes.\nFigure 18. Ablation experiments.\nhandle the long prompts, as PagedAttention resolves the\nproblem of memory fragmentation and reservation.\n7\nAblation Studies\nIn this section, we study various aspects of vLLM and evalu-\nate the design choices we make with ablation experiments.\n7.1\nKernel Microbenchmark\nThe dynamic block mapping in PagedAttention affects the\nperformance of the GPU operations involving the stored KV\ncache, i.e., block read/writes and attention. Compared to the\nexisting systems, our GPU kernels (\u00a75) involve extra over-\nheads of accessing the block table, executing extra branches,\nand handling variable sequence lengths. As shown in Fig. 18a,\nthis leads to 20\u201326% higher attention kernel latency, com-\npared to the highly-optimized FasterTransformer implemen-\ntation. We believe the overhead is small as it only affects\nthe attention operator but not the other operators in the\nmodel, such as Linear. Despite the overhead, PagedAttention\nmakes vLLM significantly outperform FasterTransformer in\nend-to-end performance (\u00a76).\n7.2\nImpact of Block Size\nThe choice of block size can have a substantial impact on the\nperformance of vLLM. If the block size is too small, vLLM\nmay not fully utilize the GPU\u2019s parallelism for reading and\nprocessing KV cache. If the block size is too large, inter-\nnal fragmentation increases and the probability of sharing\ndecreases.\nIn Fig. 18b, we evaluate the performance of vLLM with dif-\nferent block sizes, using the ShareGPT and Alpaca traces with\nbasic sampling under fixed request rates. In the ShareGPT\ntrace, block sizes from 16 to 128 lead to the best performance.\nIn the Alpaca trace, while the block size 16 and 32 work\nwell, larger block sizes significantly degrade the performance\nsince the sequences become shorter than the block sizes. In\npractice, we find that the block size 16 is large enough to\nefficiently utilize the GPU and small enough to avoid signifi-\ncant internal fragmentation in most workloads. Accordingly,\nvLLM sets its default block size as 16.\n12\n1\n2\n4\n8\n16\n32\n64\n128 256\nBlock size\n0\n20\n40\n60\n80\n100\n120\n140\nTime (ms)\nRecompute\nSwap in\nSwap out\nSwap in + out\n(a) Microbenchmark\n1\n2\n4\n8\n16\n32\n64\n128 256\nBlock size\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nNormalized latency (s/token)\nRecompute\nSwap\n(b) End-to-end performance\nFigure 19. (a) Overhead of recomputation and swapping for\ndifferent block sizes. (b) Performance when serving OPT-13B\nwith the ShareGPT traces at the same request rate.\n7.3\nComparing Recomputation and Swapping\nvLLM supports both recomputation and swapping as its re-\ncovery mechanisms. To understand the tradeoffs between\nthe two methods, we evaluate their end-to-end performance\nand microbenchmark their overheads, as presented in Fig. 19.\nOur results reveal that swapping incurs excessive overhead\nwith small block sizes. This is because small block sizes often\nresult in numerous small data transfers between CPU and\nGPU, which limits the effective PCIe bandwidth. In contrast,\nthe overhead of recomputation remains constant across dif-\nferent block sizes, as recomputation does not utilize the KV\nblocks. Thus, recomputation is more efficient when the block\nsize is small, while swapping is more efficient when the block\nsize is large, though recomputation overhead is never higher\nthan 20% of swapping\u2019s latency. For medium block sizes from\n16 to 64, the two methods exhibit comparable end-to-end\nperformance.\n8\nDiscussion\nApplying the virtual memory and paging technique to\nother GPU workloads. The idea of virtual memory and\npaging is effective for managing the KV cache in LLM serving\nbecause the workload requires dynamic memory allocation\n(since the output length is not known a priori) and its perfor-\nmance is bound by the GPU memory capacity. However, this\ndoes not generally hold for every GPU workload. For exam-\nple, in DNN training, the tensor shapes are typically static,\nand thus memory allocation can be optimized ahead of time.\nFor another example, in serving DNNs that are not LLMs,\nan increase in memory efficiency may not result in any per-\nformance improvement since the performance is primarily\ncompute-bound. In such scenarios, introducing the vLLM\u2019s\ntechniques may rather degrade the performance due to the\nextra overhead of memory indirection and non-contiguous\nblock memory. However, we would be excited to see vLLM\u2019s\ntechniques being applied to other workloads with similar\nproperties to LLM serving.\nLLM-specific optimizations in applying virtual mem-\nory and paging. vLLM re-interprets and augments the idea\nof virtual memory and paging by leveraging the application-\nspecific semantics. One example is vLLM\u2019s all-or-nothing\nswap-out policy, which exploits the fact that processing a\nrequest requires all of its corresponding token states to be\nstored in GPU memory. Another example is the recomputa-\ntion method to recover the evicted blocks, which is not feasi-\nble in OS. Besides, vLLM mitigates the overhead of memory\nindirection in paging by fusing the GPU kernels for memory\naccess operations with those for other operations such as\nattention.\n9\nRelated Work\nGeneral model serving systems. Model serving has been\nan active area of research in recent years, with numerous\nsystems proposed to tackle diverse aspects of deep learning\nmodel deployment. Clipper [11], TensorFlow Serving [33],\nNexus [45], InferLine [10], and Clockwork [20] are some\nearlier general model serving systems. They study batch-\ning, caching, placement, and scheduling for serving single\nor multiple models. More recently, DVABatch [12] intro-\nduces multi-entry multi-exit batching. REEF [21] and Shep-\nherd [61] propose preemption for serving. AlpaServe [28]\nutilizes model parallelism for statistical multiplexing. How-\never, these general systems fail to take into account the auto-\nregressive property and token state of LLM inference, result-\ning in missed opportunities for optimization.\nSpecialized serving systems for transformers. Due to\nthe significance of the transformer architecture, numerous\nspecialized serving systems for it have been developed. These\nsystems utilize GPU kernel optimizations [1, 29, 31, 56], ad-\nvanced batching mechanisms [14, 60], model parallelism [1,\n41, 60], and parameter sharing [64] for efficient serving.\nAmong them, Orca [60] is most relevant to our approach.\nComparison to Orca. The iteration-level scheduling in\nOrca [60] and PagedAttention in vLLM are complementary\ntechniques: While both systems aim to increase the GPU\nutilization and hence the throughput of LLM serving, Orca\nachieves it by scheduling and interleaving the requests so\nthat more requests can be processed in parallel, while vLLM\nis doing so by increasing memory utilization so that the\nworking sets of more requests fit into memory. By reducing\nmemory fragmentation and enabling sharing, vLLM runs\nmore requests in a batch in parallel and achieves a 2-4\u00d7\nspeedup compared to Orca. Indeed, the fine-grained sched-\nuling and interleaving of the requests like in Orca makes\nmemory management more challenging, making the tech-\nniques proposed in vLLM even more crucial.\nMemory optimizations. The widening gap between the\ncompute capability and memory capacity of accelerators has\ncaused memory to become a bottleneck for both training\nand inference. Swapping [23, 42, 55], recomputation [7, 24]\nand their combination [40] have been utilized to reduce the\npeak memory of training. Notably, FlexGen [46] studies how\nto swap weights and token states for LLM inference with\n13\nlimited GPU memory, but it does not target the online serv-\ning settings. OLLA [48] optimizes the lifetime and location\nof tensors to reduce fragmentation, but it does not do fine-\ngrained block-level management or online serving. FlashAt-\ntention [13] applies tiling and kernel optimizations to reduce\nthe peak memory of attention computation and reduce I/O\ncosts. This paper introduces a new idea of block-level mem-\nory management in the context of online serving.\n10\nConclusion\nThis paper proposes PagedAttention, a new attention algo-\nrithm that allows attention keys and values to be stored\nin non-contiguous paged memory, and presents vLLM, a\nhigh-throughput LLM serving system with efficient mem-\nory management enabled by PagedAttention. Inspired by\noperating systems, we demonstrate how established tech-\nniques, such as virtual memory and copy-on-write, can be\nadapted to efficiently manage KV cache and handle various\ndecoding algorithms in LLM serving. Our experiments show\nthat vLLM achieves 2-4\u00d7 throughput improvements over the\nstate-of-the-art systems.\nAcknowledgement\nWe would like to thank Xiaoxuan Liu, Zhifeng Chen, Yan-\nping Huang, anonymous SOSP reviewers, and our shepherd,\nLidong Zhou, for their insightful feedback. This research is\npartly supported by gifts from Andreessen Horowitz, Anyscale,\nAstronomer, Google, IBM, Intel, Lacework, Microsoft, Mo-\nhamed Bin Zayed University of Artificial Intelligence, Sam-\nsung SDS, Uber, and VMware.\nReferences\n[1] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Am-\nmar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden\nSmith, Olatunji Ruwase, et al. 2022. DeepSpeed Inference: Enabling\nEfficient Inference of Transformer Models at Unprecedented Scale.\narXiv preprint arXiv:2207.00032 (2022).\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer\nnormalization. arXiv preprint arXiv:1607.06450 (2016).\n[3] Yoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. 2000. A neural\nprobabilistic language model. Advances in neural information process-\ning systems 13 (2000).\n[4] Ond rej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Gra-\nham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp\nKoehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aurelie\nNeveol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Car-\nolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos\nZampieri. 2016. Findings of the 2016 Conference on Machine Trans-\nlation. In Proceedings of the First Conference on Machine Translation.\nAssociation for Computational Linguistics, Berlin, Germany, 131\u2013198.\nhttp://www.aclweb.org/anthology/W/W16/W16-2301\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\nSastry, Amanda Askell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing systems 33 (2020),\n1877\u20131901.\n[6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\nde Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas\nJoseph, Greg Brockman, et al. 2021. Evaluating large language models\ntrained on code. arXiv preprint arXiv:2107.03374 (2021).\n[7] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.\nTraining deep nets with sublinear memory cost.\narXiv preprint\narXiv:1604.06174 (2016).\n[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao\nZhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E.\nGonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source\nChatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.\norg/blog/2023-03-30-vicuna/\n[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,\nGaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling lan-\nguage modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).\n[10] Daniel Crankshaw, Gur-Eyal Sela, Xiangxi Mo, Corey Zumar, Ion\nStoica, Joseph Gonzalez, and Alexey Tumanov. 2020. InferLine: latency-\naware provisioning and scaling for prediction serving pipelines. In\nProceedings of the 11th ACM Symposium on Cloud Computing. 477\u2013491.\n[11] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin,\nJoseph E Gonzalez, and Ion Stoica. 2017. Clipper: A Low-Latency\nOnline Prediction Serving System. In 14th USENIX Symposium on\nNetworked Systems Design and Implementation (NSDI 17). 613\u2013627.\n[12] Weihao Cui, Han Zhao, Quan Chen, Hao Wei, Zirui Li, Deze Zeng,\nChao Li, and Minyi Guo. 2022. DVABatch: Diversity-aware Multi-\nEntry Multi-Exit Batching for Efficient Processing of DNN Services\non GPUs. In 2022 USENIX Annual Technical Conference (USENIX ATC\n22). 183\u2013198.\n[13] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9.\n2022. Flashattention: Fast and memory-efficient exact attention with\nio-awareness. Advances in Neural Information Processing Systems 35\n(2022), 16344\u201316359.\n[14] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. 2021. TurboTrans-\nformers: an efficient GPU serving system for transformer models. In\nProceedings of the 26th ACM SIGPLAN Symposium on Principles and\nPractice of Parallel Programming. 389\u2013402.\n[15] FastAPI. 2023. FastAPI. https://github.com/tiangolo/fastapi.\n[16] Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. 2018. Low latency\nrnn inference with cellular batching. In Proceedings of the Thirteenth\nEuroSys Conference. 1\u201315.\n[17] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, and\nKurt Keutzer. 2021. Ai and memory wall. RiseLab Medium Post 1 (2021),\n6.\n[18] Github. 2022. https://github.com/features/copilot\n[19] Google. 2023. https://bard.google.com/\n[20] Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kauf-\nmann, Ymir Vigfusson, and Jonathan Mace. 2020. Serving {DNNs} like\nClockwork: Performance Predictability from the Bottom Up. In 14th\nUSENIX Symposium on Operating Systems Design and Implementation\n(OSDI 20). 443\u2013462.\n[21] Mingcong Han, Hanze Zhang, Rong Chen, and Haibo Chen.\n2022.\nMicrosecond-scale Preemption for Concurrent {GPU-\naccelerated}{DNN} Inferences. In 16th USENIX Symposium on Oper-\nating Systems Design and Implementation (OSDI 22). 539\u2013558.\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep\nresidual learning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition. 770\u2013778.\n[23] Chien-Chin Huang, Gu Jin, and Jinyang Li. 2020. Swapadvisor: Push-\ning deep learning beyond the gpu memory limit via smart swapping.\nIn Proceedings of the Twenty-Fifth International Conference on Archi-\ntectural Support for Programming Languages and Operating Systems.\n1341\u20131355.\n[24] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter\nAbbeel, Joseph Gonzalez, Kurt Keutzer, and Ion Stoica. 2020. Check-\nmate: Breaking the memory wall with optimal tensor rematerialization.\n14\nProceedings of Machine Learning and Systems 2 (2020), 497\u2013511.\n[25] Tom Kilburn, David BG Edwards, Michael J Lanigan, and Frank H\nSumner. 1962. One-level storage system. IRE Transactions on Electronic\nComputers 2 (1962), 223\u2013235.\n[26] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power\nof scale for parameter-efficient prompt tuning.\narXiv preprint\narXiv:2104.08691 (2021).\n[27] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing contin-\nuous prompts for generation. arXiv preprint arXiv:2101.00190 (2021).\n[28] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng,\nXin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E Gonzalez,\net al. 2023. AlpaServe: Statistical Multiplexing with Model Parallelism\nfor Deep Learning Serving. arXiv preprint arXiv:2302.11665 (2023).\n[29] Lingxiao Ma, Zhiqiang Xie, Zhi Yang, Jilong Xue, Youshan Miao, Wei\nCui, Wenxiang Hu, Fan Yang, Lintao Zhang, and Lidong Zhou. 2020.\nRammer: Enabling holistic deep learning compiler optimizations with\nrtasks. In Proceedings of the 14th USENIX Conference on Operating\nSystems Design and Implementation. 881\u2013897.\n[30] NVIDIA. [n. d.]. Triton Inference Server. https://developer.nvidia.com/\nnvidia-triton-inference-server.\n[31] NVIDIA. 2023.\nFasterTransformer.\nhttps://github.com/NVIDIA/\nFasterTransformer.\n[32] NVIDIA. 2023. NCCL: The NVIDIA Collective Communication Library.\nhttps://developer.nvidia.com/nccl.\n[33] Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li\nLao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, and Jordan Soyke.\n2017. Tensorflow-serving: Flexible, high-performance ml serving.\narXiv preprint arXiv:1712.06139 (2017).\n[34] OpenAI. 2020. https://openai.com/blog/openai-api\n[35] OpenAI. 2022. https://openai.com/blog/chatgpt\n[36] OpenAI. 2023.\nhttps://openai.com/blog/custom-instructions-for-\nchatgpt\n[37] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[38] LMSYS ORG. 2023. Chatbot Arena Leaderboard Week 8: Introduc-\ning MT-Bench and Vicuna-33B. https://lmsys.org/blog/2023-06-22-\nleaderboard/.\n[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James\nBradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia\nGimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in neural informa-\ntion processing systems 32 (2019).\n[40] Shishir G Patil, Paras Jain, Prabal Dutta, Ion Stoica, and Joseph Gon-\nzalez. 2022. POET: Training Neural Networks on Tiny Devices with\nIntegrated Rematerialization and Paging. In International Conference\non Machine Learning. PMLR, 17573\u201317583.\n[41] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin,\nJames Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani\nAgrawal, and Jeff Dean. 2022. Efficiently Scaling Transformer Inference.\narXiv preprint arXiv:2211.05102 (2022).\n[42] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji\nRuwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He.\n2021. ZeRO-Offload: Democratizing Billion-Scale Model Training.. In\nUSENIX Annual Technical Conference. 551\u2013564.\n[43] Reuters. 2023. https://www.reuters.com/technology/tech-giants-ai-\nlike-bing-bard-poses-billion-dollar-search-problem-2023-02-22/\n[44] Amazon Web Services. 2023. https://aws.amazon.com/bedrock/\n[45] Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong,\nMatthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. 2019.\nNexus: A GPU cluster engine for accelerating DNN-based video anal-\nysis. In Proceedings of the 27th ACM Symposium on Operating Systems\nPrinciples. 322\u2013337.\n[46] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin,\nDaniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gon-\nzalez, et al. 2023. High-throughput Generative Inference of Large\nLanguage Models with a Single GPU. arXiv preprint arXiv:2303.06865\n(2023).\n[47] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\nJared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-\nbillion parameter language models using model parallelism. arXiv\npreprint arXiv:1909.08053 (2019).\n[48] Benoit Steiner, Mostafa Elhoushi, Jacob Kahn, and James Hegarty. 2022.\nOLLA: Optimizing the Lifetime and Location of Arrays to Reduce the\nMemory Usage of Neural Networks. (2022). https://doi.org/10.48550/\narXiv.2210.12924\n[49] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to se-\nquence learning with neural networks. Advances in neural information\nprocessing systems 27 (2014).\n[50] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen\nLi, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.\nStanford Alpaca: An Instruction-following LLaMA model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\n[51] ShareGPT Team. 2023. https://sharegpt.com/\n[52] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-\nAnne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric\nHambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971 (2023).\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. Advances in neural information processing\nsystems 30 (2017).\n[54] Jing Wang, Youyou Lu, Qing Wang, Minhui Xie, Keji Huang, and Jiwu\nShu. 2022. Pacman: An Efficient Compaction Approach for {Log-\nStructured}{Key-Value} Store on Persistent Memory. In 2022 USENIX\nAnnual Technical Conference (USENIX ATC 22). 773\u2013788.\n[55] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuai-\nwen Leon Song, Zenglin Xu, and Tim Kraska. 2018. Superneurons: Dy-\nnamic GPU memory management for training deep neural networks.\nIn Proceedings of the 23rd ACM SIGPLAN symposium on principles and\npractice of parallel programming. 41\u201353.\n[56] Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, and Lei Li.\n2021. LightSeq: A High Performance Inference Library for Transform-\ners. In Proceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Tech-\nnologies: Industry Papers. 113\u2013120.\n[57] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A\nSmith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-Instruct:\nAligning Language Model with Self Generated Instructions. arXiv\npreprint arXiv:2212.10560 (2022).\n[58] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,\nClement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf,\nMorgan Funtowicz, et al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 conference on empirical\nmethods in natural language processing: system demonstrations. 38\u201345.\n[59] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad\nNorouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao,\nKlaus Macherey, et al. 2016. Google\u2019s neural machine translation\nsystem: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144 (2016).\n[60] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and\nByung-Gon Chun. 2022.\nOrca: A Distributed Serving System for\n{Transformer-Based} Generative Models. In 16th USENIX Symposium\non Operating Systems Design and Implementation (OSDI 22). 521\u2013538.\n[61] Hong Zhang, Yupeng Tang, Anurag Khandelwal, and Ion Stoica. 2023.\nSHEPHERD: Serving DNNs in the Wild. In 20th USENIX Symposium on\nNetworked Systems Design and Implementation (NSDI 23). USENIX As-\nsociation, Boston, MA, 787\u2013808. https://www.usenix.org/conference/\nnsdi23/presentation/zhang-hong\n15\n[62] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen,\nShuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria\nLin, et al. 2022. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068 (2022).\n[63] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng\nChen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo,\nEric P Xing, et al. 2022. Alpa: Automating Inter-and Intra-Operator\nParallelism for Distributed Deep Learning. In 16th USENIX Symposium\non Operating Systems Design and Implementation (OSDI 22). 559\u2013578.\n[64] Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. 2022. PetS:\nA Unified Framework for Parameter-Efficient Transformers Serving. In\n2022 USENIX Annual Technical Conference (USENIX ATC 22). 489\u2013504.\n16\n"
  },
  {
    "title": "Large Language Model for Science: A Study on P vs. NP",
    "link": "https://arxiv.org/pdf/2309.05689.pdf",
    "upvote": "20",
    "text": "Large Language Model for Science:\nA Study on P vs. NP\nQingxiu Dong \u2217 1 2 Li Dong \u2217 1 Ke Xu \u2217 3\nGuangyan Zhou 4 Yaru Hao 1 Zhifang Sui 2 Furu Wei 1\nhttps://aka.ms/GeneralAI\nAbstract\nIn this work, we use large language models (LLMs) to augment and accelerate research on\nthe P versus NP problem, one of the most important open problems in theoretical computer\nscience and mathematics. Specifically, we propose Socratic reasoning, a general framework\nthat promotes in-depth thinking with LLMs for complex problem-solving. Socratic reason-\ning encourages LLMs to recursively discover, solve, and integrate problems while facilitat-\ning self-evaluation and refinement. Our pilot study on the P vs. NP problem shows that\nGPT-4 successfully produces a proof schema and engages in rigorous reasoning through-\nout 97 dialogue turns, concluding \u201cP \u0338= NP\u201d, which is in alignment with (Xu and Zhou,\n2023).\nThe investigation uncovers novel insights within the extensive solution space of\nLLMs, shedding light on LLM for Science.\n1\nIntroduction\nLarge language models (LLMs) have exhibited remarkable abilities across various tasks, in-\ncluding natural language generation, coding, and reasoning (OpenAI, 2023; Bubeck et al.,\n2023).\nWhile LLMs have successfully interpolated existing knowledge, it remains open\nwhether they can extrapolate novel knowledge from the vast solution space. The funda-\nmental question pertains to the potential of LLMs to achieve expert-level capabilities.\nIn this work, we propose a paradigm, named LLM for Science (LLM4Science), that integrates\nLLMs to augment and accelerate the scientific research process. In previous work (Wang et al.,\n2023), AI usually serves as support tools to carry out procedures predetermined by humans.\nBy contrast, the new paradigm elevates LLMs to the role of a collaborative peer alongside\nhumans, leveraging the creativity of AI. Besides, unlike task-specific models tailored for\nparticular problems (such as AlphaFold; Jumper et al. 2021), LLMs act as general-purpose\ninnovation navigators. The polymath ability of LLMs, in terms of both breadth and depth of\nknowledge, enables AI to combine skills and ideas in novel and useful ways.\nWe conduct a pilot study on the P versus NP problem (Cook, 2000), which is a pivotal\nopen question in computer science and mathematics. The problem concerns the classification\nof computational problems based on their solvability. It investigates whether NP problems\nwith quickly verifiable solutions can be solved as efficiently as P problems, which are solv-\nable using deterministic polynomial-time algorithms. Despite its substantial impact on other\nareas, such as cryptography and optimization, the P vs. NP problem has remained an elusive\n*Equal contribution 1 Microsoft Research 2 School of Computer Science, Peking University 3 State Key Lab of\nSoftware Development Environment, Beihang University 4 Department of Mathematics and Statistics, Beijing\nTechnology and Business University. Contribution during Q.D.\u2019s internship at Microsoft Research. Contact person:\nFuru Wei <fuwei@microsoft.com>.\narXiv:2309.05689v1  [cs.CL]  11 Sep 2023\nchallenge for over fifty years (Fortnow, 2022). The inherent complexity and prerequisite for\nexpert-level knowledge make it a good arena for investigating the potential of using LLMs\nto discover novel science. Moreover, as the problem is still open, it naturally avoids solution\nmemorization and contamination from the training corpus of LLMs.\nInspired by the ancient Greek philosopher Socrates, we propose Socratic Reasoning, a gen-\neral problem-solving framework that allows LLMs to navigate the expansive solution space\nand efficiently arrive at the answer. As Socrates stated, \u201cI cannot teach anybody anything. I\ncan only make them think.\u201d Socratic Method employs a series of questions to stimulate criti-\ncal thinking and elicit ideas through ongoing dialogues. Following the above wisdom, our\nframework encourages LLMs to think critically and generate solutions for complex problems.\nAs shown in Table 1, there are five atomic prompt patterns in Socratic reasoning: deduction,\ntransformation, decomposition, verification, and integration. They are used to uncover novel\ninsights and perspectives, decompose complex issues into subproblems or steps, and engage\nin self-refinement by challenging the responses.\nWe utilize Socratic reasoning to tackle the P vs. NP problem. GPT-4 demonstrates the\ncapability to develop a reasoning pathway and arrive at a conclusion in alignment with\nthe results presented by Xu and Zhou (2023). Specifically, we introduce five distinct roles\n(e.g., a mathematician proficient in probability theory) as our collaborative provers. GPT-\n4 is prompted to devise a preliminary proof schema with the initial 14 turns of dialogues.\nThen we proceed with rigorous reasoning in the subsequent 83 dialogue turns, following the\nsketch. In total, the 97 turns collectively construct extremely hard NP-complete problems,\nwhere some instances are unsolvable within a time complexity lower than O(2n) (i.e., ex-\nhaustive search). In other words, the proof concludes \u201cP \u0338= NP\u201d. Our investigation highlights\nthe potential capability of GPT-4 to collaborate with humans in exploring exceptionally com-\nplex and expert-level problems. We show that, beyond mastering fundamental knowledge,\nLLMs can discover novel insights within the extensive solution space. The revelation sheds\nlight on the prospect of scientific discoveries in the paradigm of LLM for Science.\n2\nSocratic Reasoning: Stimulating LLMs for Complex Problem Solving\nWe introduce a general-purpose problem-solving framework, called Socratic Reasoning. Draw-\ning inspiration from Socratic Method employed by ancient Greek philosophers, we utilize a\nsequence of questions to stimulate critical thinking and elicit ideas of LLMs through contin-\nuous dialogues. The framework aims to motivate LLMs to orchestrate various subproblems\nwhen solving highly complicated tasks, guiding LLMs to establish a high-level reasoning\npathway. Socratic reasoning is conducted in a series of dialogue turns between humans and\nLLMs, functioning as a recursive mechanism for resolving intricate challenges with LLMs.\nAs shown in Table 1, Socratic Reasoning includes five types of prompt patterns: deduc-\ntion, transformation, decomposition, verification, and integration. Generally, when dealing\nwith an atomic problem, which LLMs can directly reason to conclude, we employ deduction\npatterns (e.g., \u201cLet\u2019s think step by step...\u201d) to instruct the LLMs to derive a conclusion straight-\nforwardly. For more complex problems, we initially ask the LLMs to transform the problem\ninto a new one or break it down into several subproblems. We then pursue such patterns\nrecursively until reaching the atomic problems. When generating a new problem or arriving\nat a new conclusion, we apply the verification pattern to leverage the self-critique capabilities\nof LLMs for validation and refinement. Lastly, the integration pattern requests LLMs to syn-\nthesize conclusions based on the outcomes of subproblems. We motivate LLMs to continue\nthe above process recursively via a series of dialogues until it addresses the target problem.\nPrevious work focuses on optimizing the thought process of LLM inference for relatively\n2\nPatterns\nDescription\nDiagram\nExamples\nDeduction\nDerive a conclusion for a\ngiven problem directly.\nP\nC\nP\nC\u2019\nC\nBased on this, can you define ... (25)\nLet\u2019s calculate the ... (42)\nwhich kind of ... will guarantee ... holds\ntrue? (44)\nPlease provide a strict proof of this point.\n(93)\nTransformation\nTransform the problem\ninto a homogeneous or\nsimilar problem, or\nabstract the problem.\nP\nP\u2019\nCan you find the fundamental problem ...\nfrom ... perspective? (1)\nDecomposition\nBreak the problem into\nmanageable\nsubproblems, or make a\nplan for reasoning steps.\nP\nP1\nP3\nP2\nPlease explain the theorem to me, lay out\na high-level proof plan, and then try\nvarious tactics to prove the theorem. (32)\nPlease lay out a high-level proof plan. (63)\nVerification\nCheck the conclusion or\nits relationship with\nothers to verify or correct\nit.\nC\nP\nPlease check for these issues ... (30)\nPlease check ... and refine any possible\nmistakes. (46)\nDoes this prove ...? (51)\nWhy do you say ...? (58)\nIntegration\nSummarize multiple\nconclusions to derive a\nnew conclusion.\nC\nC1\nC3\nC2\nPlease now organize all our historical\nconversations and sort out ... (14)\nNow what conclusion can we draw? (79)\nTable 1: Problem-solving patterns in Socratic Reasoning. We use P\u20dd and C to represent\n(sub)problems and conclusions, respectively.\nsimple problems compared with scientific discovery. For instance, Wei et al. (2022) consider\nthe reasoning steps in a model response as a chain of thoughts. Yao et al. (2023) structure rea-\nsoning as searching over a tree, achieving improved results on closed-format reasoning tasks,\nsuch as math word problems, and Game of 24. Nonetheless, how to stimulate the potential of\nLLMs in free-form, highly complex problems remains under-explored. In this work, Socratic\nreasoning provides a systematic prompting framework for challenging problems.\n\u2026\nResponse generated by GPT-4\nIndexes of past turns incorporated \ninto the dialogue history\nInput prompt\nTurn index\nFigure 2.1: Example of a dialogue turn in Socratic Reasoning.\n3\nFigure 2.1 shows an example dialogue turn used to tackle the P vs. NP problem (Sec-\ntion 3.2). We used GPT-4 API (Appendix B) in the case study. We steer the roles of LLMs\nby customizing system instructions as in Appendix A. We sort the process according to the\nturn index. Because the context length of GPT-4 is 8192, we incorporate the relevant turns\ninto context. The history indexes are shown at the top-right corner. Besides, in the margin,\nwe summarize some best practices for prompting LLMs in Socratic reasoning. We also use\nitalicized margin notes to explain some details. Notice that we only format GPT-4 responses\nto LaTeX while keeping the content unmodified. Moreover, we highlight the insightful parts\nof the response generated by GPT-4 for better understanding.\n3\nA Pilot Study on the P Versus NP Problem\n3.1\nIntuitions Behind the Proof: Extreme Hardness is Easy to Understand\nTo study the P vs. NP problem in computation theory, it is essential first to develop a founda-\ntional comprehension of the laws of computation and their mathematical proofs. Akin to laws\nof physics in the natural world, laws of computation are fundamental laws that objectively\nexist. They are primarily expressed as mathematical theorems and are proven through math-\nematical approaches. Grounded in the principle that \u201cthe greatest truths are the simplest,\u201d\nmost fundamental laws possess elegant and straightforward forms, and their mathematical\nproofs are not complicated. With this intuition, it is crucial first to figure out why some com-\nputational problems are hard and provide intuitive explanations for computational hardness.\nHowever, the reasons why some problems are hard have not been thoroughly studied. For\na long time, NP-completeness has been the core concept of computational complexity theory.\nNP-complete problems are defined by reductions, which is instrumental in establishing the\ntower of computational complexity theory. Yet, reductions depend on skills and vary among\nproblems, lacking a unified pattern. Fundamentally, reductions classify problems according\nto relative hardness and cannot provide intuitive explanations for why a problem is hard.\nWithout such explanations, it is hard to determine the hardness degree of NP-complete prob-\nlems. Many think that computational complexity theory is hard because it studies hardness.\nIn our opinion, the pursuit of reductions rather than explanations is the root cause of the dif-\nficulties faced by computational complexity theory after its initial flourishing. In a nutshell,\nthe successes or failures of computational complexity theory are all attributed to reductions.\nAt this point, understanding computational hardness without relying on reductions is cru-\ncial for breakthroughs. Exhaustive search signifies extreme hardness. The extreme hardness\nof exhaustive search, instead, can be intuitively explained - exhaustive search implies that\nthere is no way to prune the search space when solving the problem (the solution space is\nalmost independent). Therefore, the problem is hard to solve.\nThe almost independent solution space is a unique property of Model RB (Xu and Li,\n2000), i.e., the intuition of choosing Model RB in 15.\nIntuitively, an almost independent\nsolution space means that the state of any assignment in the solution space (whether or not\nit is a solution) is uncorrelated to the majority of other assignments. When the parameter\nd in Model RB is large enough (the intuition of letting d and n tend to infinity in 31), the\nsolution space of Model RB is almost independent. However, most problems, such as 2-SAT\nor 3-SAT problems, do not possess the property of almost independent solution space. In\nother words, the assignments in their solution spaces are correlated to varying degrees. This\ndifference in the degree of correlation makes it difficult to distinguish quantitatively between\n2-SAT and 3-SAT. It also explains why prior attempts to distinguish between P problems and\nNP problems have failed, consequently complicating proving that P does not equal NP.\n4\n3.2\nCo-Proving with GPT-4\n1\nPrompt: Can you find the fundamental problem behind the P!=NP problem from a philo-\nsophical perspective, rather than from a computer theory perspective?\nEncourage creativity\nby avoiding prompts\nthat may retrieve pre-\nexisting answers. For\ninstance, prompting\ndirectly on how to\nprove P!=NP may lead\nthe model to retrieve\nrelated work. Apply\nthe transformation\npattern in Socratic\nreasoning to generate\nmore innovative\nresponses.\nFrom a philosophical perspective, the P versus NP problem raises deep questions about the\nnature of knowledge, intelligence, problem-solving, and the limits of human understanding.\nThe P vs NP problem is essentially a question of whether efficient solutions exist for complex\nproblems, or if some problems are inherently difficult to solve quickly, regardless of the\nmethod used.\nThe fundamental philosophical problem behind the P!=NP problem can be seen as the\ntension between creativity, intuition, and insight (which are often associated with NP prob-\nlems) versus the systematic, algorithmic, and deterministic approaches (which are associated\nwith P problems). This tension reflects an epistemological question: what is the nature of\nknowledge and the process of acquiring it?\nThe P!=NP conjecture implies that there might be some problems that are easy to verify\n(i.e., confirming that a solution is correct) but difficult or even impossible to solve efficiently.\nThis raises questions about the limits of human knowledge and understanding, as well as the\npotential role of creativity and intuition in problem-solving.\nMoreover, the P vs NP problem also has implications for the ontology of mathematics and\nthe nature of reality. If P=NP, it would mean that there is an underlying simplicity to the\nuniverse, with problems that seem complex on the surface having efficient, elegant solutions.\nIn contrast, if P!=NP, it would suggest that the universe is fundamentally complex, with some\nproblems inherently difficult to solve.\nAdditionally, the P vs NP problem touches on the nature of intelligence and consciousness.\nIf P=NP, then it might be possible to develop artificial intelligence (AI) systems that can\nefficiently solve any problem, potentially surpassing human intelligence. However, if P!=NP,\nit could imply a fundamental limit to the capabilities of AI, suggesting that human creativity\nand intuition might be unique and irreplaceable.\nIn conclusion, the philosophical implications of the P!=NP problem revolve around the\nnature of knowledge, intelligence, and reality, as well as the potential limits of human un-\nderstanding and computational capabilities. This makes it not only a central problem in\ncomputer science but also a profound philosophical question that challenges our fundamen-\ntal assumptions about the world and ourselves.\n2\nPrompt: Are all problems that seem complex on the surface having efficient, elegant solu-\ntions? If not, what methods might be used to prove it?\nNot all problems that seem complex on the surface have efficient, elegant solutions. The\ncomplexity of a problem can be attributed to various factors, such as the number of variables\ninvolved, the nature of the relationships between variables, or the inherent difficulty of the\nproblem itself. Some problems may not even have a solution at all.\nTo explore the solvability and efficiency of a given problem, one may consider the follow-\ning methods:\n1. Analytical approach: Analytical methods involve using mathematical techniques, logic,\nand reasoning to solve a problem or prove a theorem. These methods can provide insights\ninto the structure and properties of the problem, which may help identify whether an\n5\nefficient, elegant solution exists or not.\n2. Computational complexity theory: This field of study analyzes the resources (such as time,\nmemory, or computational steps) required to solve a problem. By comparing a problem to\nknown classes of computational complexity, such as P (problems solvable in polynomial\ntime) or NP (problems whose solutions can be checked in polynomial time), one can gain a\nbetter understanding of the inherent difficulty of the problem and the possibility of finding\nan efficient solution.\n3. Heuristics and approximation algorithms: In cases where an elegant solution is not achiev-\nable, heuristics and approximation algorithms can be employed to find \"good enough\"\nsolutions. While these methods may not always guarantee optimal solutions, they can\nprovide practical ways to tackle complex problems.\n4. Empirical approach: Sometimes, a problem\u00b4s complexity can be explored through experi-\nments, simulations, or real-world applications. By observing the behavior of the problem\nand analyzing the results, one may be able to infer whether an efficient solution exists or\nnot.\n5. Reduction or transformation: By reducing a problem to another well-known problem,\none can leverage existing solutions or insights to better understand the complexity of the\noriginal problem. If the reduced problem is known to have an efficient, elegant solution,\nthen it may suggest that the original problem shares similar properties.\n6. Proof by contradiction: To prove that a problem does not have an efficient, elegant solution,\none can assume the existence of such a solution and then show that this assumption leads\nto a contradiction. This method can be powerful in establishing the impossibility of certain\nsolutions.\nIn conclusion, not all complex problems have efficient, elegant solutions. Different meth-\nods, such as analytical approaches, computational complexity theory, heuristics, empirical\napproaches, reduction, and proof by contradiction, can be used to explore the solvability and\nefficiency of a given problem.\n3\nPrompt: We want to prove that P != NP by contradiction, please list several possible ideas.\nThe answer provided\nby GPT-4 here is not\nrigorous. To prove by\ncontradiction, one must\nfind an NP-complete\nproblem that cannot be\nsolved in polynomial\ntime. However, the\nresponse can inspire the\nmodel to think about\nNP-complete problems in\nsubsequent conversations.\nTo prove P != NP by contradiction, we would initially assume that P = NP, and then\nshow that this assumption leads to contradictions or absurd consequences. Here are several\npossible ideas to explore:\n1. Polynomial-time algorithm for NP-hard problems: If P = NP, then there would exist a polynomial-\ntime algorithm for solving NP-hard problems. However, this would contradict the widely accepted\nbelief that NP-hard problems cannot be efficiently solved. If we could find a specific NP-hard problem\nand show that it cannot be solved in polynomial time, we could establish P != NP.\n2. Polynomial-time reduction: Assuming P = NP, every NP problem can be reduced to a P\nproblem in polynomial time. By finding a problem in NP that cannot be reduced to a P\nproblem in polynomial time, we could prove that P != NP.\n3. Collapse of complexity classes: If P = NP, then the complexity classes P, NP, co-NP, and\neven the polynomial hierarchy (PH) would all collapse, as each class would become equiv-\nalent to the others. Demonstrating that this collapse would lead to absurd consequences\nor contradictions could be a way to show P != NP.\n6\n4. Encryption and Cryptography: If P = NP, then many modern cryptographic systems\n(such as RSA) would be easily breakable, as the underlying hard problems would have\npolynomial-time solutions. By showing that breaking these cryptographic systems cannot\nbe done in polynomial time, we could establish that P != NP.\n5. Proof of lower bounds: Demonstrating a lower bound for the computational complexity of\nany NP-complete problem (e.g., proving that it requires at least exponential time) would\ncontradict the assumption that P = NP. To do this, we could explore various computational\nmodels and attempt to establish lower bounds for specific NP-complete problems.\n6. Logical contradictions: Assuming P = NP, we might be able to derive logical contradictions\nusing concepts from logic and computability theory. For example, we could explore the\nconsequences of P = NP on the undecidability of the halting problem or the existence of\ntrue but unprovable statements in mathematics (G\u00f6del\u2019s incompleteness theorems).\nThese ideas are only starting points for proving P != NP by contradiction. Each of them\nwould require extensive research, collaboration, and potentially new mathematical tech-\nniques to make significant progress towards a proof.\n4\n3\nPrompt: How to construct such problems?\nThe numerals in the\nupper right corner of\nthe box indicate the\ndialogue turns from\npast interactions that\nhave been incorporated\ninto the current turn.\nBy default, all of the\nmodel\u2019s responses will be\nincorporated. However,\nif there are italicized\nparts in the response,\nonly the italicized parts\nare selected as historical\ndialogue or formal\ndefinitions for subsequent\nprompts. For instance,\nif the model provides\nmultiple solutions, we\nonly include the most\nvaluable solution in the\nconversation history.\nThis strategy enables\nGPT-4 to concentrate on\npertinent information,\nthereby enhancing its\noverall efficiency and\neffectiveness.\nTo construct such problems, we can follow these general steps:\n1. Identify a well-known NP-complete problem: Start with a known NP-complete problem, such as\nthe Traveling Salesman Problem (TSP), Boolean Satisfiability (SAT), or the Clique problem. These\nproblems have been extensively studied and are known to be hard to solve.\n2. Reduction: Transform the chosen NP-complete problem into a new problem while preserving its\nNP-hard properties. This is done through a polynomial-time reduction, which converts an instance\nof the original problem into an instance of the new problem in such a way that the solutions to both\ninstances correspond to each other.\n3. Prove the new problem is NP-hard: Show that the new problem is at least as hard as the\noriginal NP-complete problem. This is typically done by demonstrating that any algorithm\nthat solves the new problem in polynomial time could be used to solve the original NP-\ncomplete problem in polynomial time as well.\n4. Attempt to prove the new problem cannot be solved in polynomial time: This is the most\nchallenging step, as it requires proving a lower bound on the complexity of the problem.\nOne approach is to assume that the problem can be solved in polynomial time, and then\nderive a contradiction based on known complexity-theoretical results or conjectures (such\nas the Exponential Time Hypothesis).\nHere are a few example ideas for constructing problems to prove P != NP by contradiction:\n\u2022 Idea 1: Construct a problem by combining multiple NP-complete problems, such as TSP\nand SAT, into a single problem. If an algorithm could solve this combined problem in\npolynomial time, it would imply that both TSP and SAT can be solved in polynomial time,\nleading to a contradiction.\n\u2022 Idea 2: Create a problem that involves solving an infinite sequence of increasingly difficult\ninstances of an NP-complete problem. If an algorithm could solve such a problem, it would\n7\nimply that there is no limit to the difficulty of instances that can be solved in polynomial\ntime, contradicting the idea that NP problems are harder than P problems.\n\u2022 Idea 3: Design a problem that encodes the behavior of a Turing machine, such that solving\nthis problem would allow us to decide the halting problem (which is known to be unde-\ncidable) in polynomial time. This would lead to a contradiction, as it would imply that an\nundecidable problem can be decided in polynomial time. Remember that constructing a\nproblem and proving its complexity is a difficult task that often requires deep mathemati-\ncal insights. So far, no definitive proof for P != NP has been found, and it remains one of\nthe most important open questions in computer science.\nLLMs thrive on posi-\ntive feedback. Simple\nphrases such as \u2018Great!\u2019\nprovide affirmation\nfor their previous re-\nsponses, motivating\nLLMs to produce better\nresponses based on\nprior ones.\n5\n3, 4\nPrompt: Great idea! What problems can we convert SAT problems to? List as many problems\nas possible.\nThere are many problems to which Boolean Satisfiability (SAT) problems can be reduced.\nSome of these problems include:\n1. 3-SAT: A special case of SAT where each clause contains exactly three literals. Despite its\nrestricted nature, 3-SAT is also NP-complete.\n2. MAX-SAT: Given a Boolean formula in CNF, find the maximum number of clauses that\ncan be satisfied simultaneously by an assignment.\n3. Graph Coloring: Given an undirected graph and a number k, determine if there is a way to\ncolor the vertices with k different colors such that no two adjacent vertices share the same\ncolor.\n4. Clique: Given an undirected graph and a number k, determine if there is a complete\nsubgraph (clique) of size k.\n5. Hamiltonian Cycle: Given a directed or undirected graph, determine if there is a cycle that\nvisits each vertex exactly once.\n6. Vertex Cover: Given an undirected graph and a number k, determine if there is a set of k\nvertices that includes at least one endpoint of each edge.\n7. Subset Sum: Given a set of integers and a target value, determine if there is a subset of the\nintegers that sums to the target value.\n8. Knapsack Problem: Given a set of items, each with a weight and a value, determine the\nmost valuable subset of items that can be packed into a knapsack of limited capacity.\n9. Scheduling problems: Various scheduling problems, such as job shop scheduling and task\nscheduling in multiprocessor systems, can be reduced from SAT problems.\n10. Constraint Satisfaction Problems (CSPs): Many CSPs, such as Sudoku, map coloring, and\ncrossword puzzles, can be reduced from SAT problems.\nThis is not an exhaustive list, but it demonstrates the versatility of SAT problems and their\nconnections to a wide range of other problems.\n8\n6\n3, 4, 5\nPrompt: Great idea! What is the time complexity of the process of converting between CSPs\nand SAT problems?\nThe time complexity of converting between Constraint Satisfaction Problems (CSPs) and\nSAT problems depends on the specific CSP problem being considered and the encoding\ntechnique used. In general, the conversion process can be carried out in polynomial time.\nHere\u2019s an outline of the general process for converting a CSP to a SAT problem:\n1. Variables: For each variable in the CSP, create a corresponding Boolean variable in the\nSAT problem. If the CSP variable has a domain size greater than 2, introduce additional\nBoolean variables to represent each possible value the CSP variable can take.\n2. Constraints: For each constraint in the CSP, create a corresponding clause in the SAT\nproblem. This may involve converting complex constraints into simpler conjunctions and\ndisjunctions of literals and possibly introducing auxiliary variables.\nWhen using a proper encoding technique, the number of Boolean variables and clauses\nintroduced will be polynomial in the size of the CSP problem. Therefore, the overall time\ncomplexity of converting a CSP to a SAT problem is polynomial.\nThis ensures the\npolynomial-time con-\nversion requirement\nmentioned in 4 is met.\nThe reverse process, converting a SAT problem to a CSP, can also be done in polynomial\ntime. The SAT problem\u2019s Boolean variables and clauses can be mapped to CSP variables with\nbinary domains and constraints defined by the clauses, respectively.\n7\nPrompt: Suppose we construct some extremely difficult CSPs (Constraint Satisfaction Prob-\nlems), how can we prove that they cannot be solved within polynomial time? Please provide\nsome possible ideas.\nProving that certain Constraint Satisfaction Problems (CSPs) cannot be solved within poly-\nnomial time typically involves showing that the problem in question is NP-hard or NP-\ncomplete. Here are some possible ideas on how to prove that an extremely difficult CSP\ncannot be solved within polynomial time:\n1. Reduction: One common approach is to show that an existing known NP-hard or NP-\ncomplete problem can be reduced to the given CSP in polynomial time. This would imply\nthat if the CSP could be solved in polynomial time, then the known NP-hard or NP-\ncomplete problem could also be solved in polynomial time, which contradicts the current\nunderstanding of computational complexity.\n2. Gadgets and Compositions: Another technique involves constructing gadgets or smaller\ncomponents that represent parts of known NP-hard or NP-complete problems. By com-\nbining these gadgets, you can create instances of the given CSP that essentially replicate\nthe harder problem. This way, if there was a polynomial-time solution to the CSP, it would\nalso solve the harder problem, which is a contradiction.\n3. Phase Transitions: In some cases, it might be possible to identify phase transitions in the problem\ninstances, where the difficulty of solving the problem rapidly changes from easy to hard as some\nparameter varies. By studying these transitions, you might be able to show that certain instances of\nthe CSP are inherently difficult and cannot be solved in polynomial time.\n9\n4. Lower Bound Proofs: It might also be possible to prove lower bounds on the time com-\nplexity of the CSP under certain computational models, such as decision trees or algebraic\ncircuits. If you can show that the problem requires a superpolynomial number of opera-\ntions in the chosen model, this would provide evidence that the problem is unlikely to be\nsolvable in polynomial time.\n5. Average-Case Complexity: Proving that a problem is hard in the worst-case might not be\nenough to establish its intractability in practice. In this case, you could consider analyzing\nthe average-case complexity of the CSP. If you can show that even on average, the problem\ninstances require superpolynomial time to solve, this would provide stronger evidence of\nthe CSP\u2019s inherent difficulty.\n6. Parameterized Complexity: Another approach is to study the parameterized complex-\nity of the CSP, which involves analyzing the problem complexity as a function of certain\nproblem-specific parameters. If you can show that the problem remains hard even when\nthese parameters are small, it would provide further evidence of the problem\u2019s intractabil-\nity.\nIt\u2019s important to note that proving a problem is not solvable in polynomial time is often\ndifficult and sometimes an open question in computer science. However, the above ideas can\nbe a starting point for establishing the computational complexity of extremely difficult CSPs.\n8\n7\nPrompt: Great!\nHow to prove that some CSPs cannot be solved in polynomial time by\nstudying phase transition?\nTo prove that some CSPs cannot be solved in polynomial time by studying phase transi-\ntions, you can follow these steps:\n1. Identify a parameter that affects the problem\u2019s difficulty: The first step is to identify a\nparameter associated with the CSP that influences its difficulty. This parameter could be\nrelated to the problem\u2019s size, density, or some other structural property. For example, in\ngraph coloring problems, the ratio of edges to nodes may be a relevant parameter.\n2. Analyze the phase transition: Investigate how the problem\u2019s difficulty changes as the pa-\nrameter varies. Observe at which point the problem transitions from being easy to solve\nto becoming hard. This is the phase transition point. Typically, phase transitions in CSPs\nexhibit a sudden change in the solvability of problem instances.\n3. Show the existence of hard instances: Prove that there exist instances of the CSP near the phase tran-\nsition point that are difficult to solve. You can do this by showing that the problem instances exhibit\ncertain properties or structures that make them inherently hard. This might involve relating the\nCSP to a well-known hard problem, such as an NP-complete problem, or establishing the presence\nof a lower bound on the complexity of the instances.\n4. Relate the difficulty to polynomial time solvability: With the existence of hard instances\nestablished, you must now relate this difficulty to the inability to solve the problem in\npolynomial time. One way to achieve this is by showing that any algorithm capable of\nsolving the hard instances in polynomial time would also be able to solve other known\nhard problems in polynomial time, which is unlikely or considered impossible under the\nwidely believed conjecture P \u0338= NP.\n10\nKeep in mind that this method does not provide a formal proof that the problem is not\nsolvable in polynomial time, but it can provide strong evidence to support the intractability of\nthe CSP under consideration. To obtain a more rigorous proof, you would need to establish\na direct connection between your CSP and a known hard problem, such as by showing a\nreduction from an NP-complete problem to your CSP.\n9\nPrompt: Suppose we prove that there exist instances of the CSP near the phase transition,\ncan we prove that some of them cannot be solved within polynomial time? If not, how to\nconstruct more complex instances near the phase transition?\nThe phase transition in the context of Constraint Satisfaction Problems (CSPs) refers to\nthe phenomenon where the probability of a problem being solvable changes abruptly as a\ncertain parameter is varied. For many CSPs, this parameter is the constraint density (ratio\nof constraints to variables). The instances of CSPs near the phase transition are generally\nconsidered to be the hardest to solve. However, proving that some of these instances cannot\nbe solved within polynomial time is a challenging task.\nTo date, there is no general proof that instances of CSPs near the phase transition cannot\nbe solved within polynomial time.\nThe main reason is the lack of a definitive proof for\nthe P vs NP problem, which asks whether every problem whose solution can be verified in\npolynomial time can also be solved in polynomial time. Since many CSPs are NP-complete,\nproving that some instances cannot be solved within polynomial time would imply that P \u0338=\nNP, which remains an open question.\nRegarding the construction of more complex instances near the phase transition, one can\nuse the following strategies:\n1. Generate instances with a higher number of variables and constraints: Increasing the size\nof the problem will generally make it more difficult to solve. However, this might also\npush the instance further away from the phase transition.\nThis strategy provides us\nwith the intuition that\nconsidering the scale of\nthe problem, i.e., problems\nwith an infinitely large\nnumber of variables, may\nbe extremely hard.\n2. Introduce hidden structure: Some instances of CSPs may appear to be random but con-\ntain hidden structures that make them difficult to solve. For example, one can generate\na problem that consists of multiple smaller, interconnected CSPs that are hard to solve\nindependently. The difficulty of solving the overall problem will be compounded by the\ndifficulty of solving each subproblem.\n3. Vary the type of constraints: Combining different types of constraints can lead to more\ncomplex instances. For example, one can mix algebraic, geometric, and logical constraints\nin a single problem.\n4. Exploit symmetries and other properties: Some instances of CSPs can be made more difficult by\nexploiting symmetries or other properties that make them harder to solve. For example, problems\nwith a high degree of symmetry may be difficult for certain algorithms to handle efficiently.\nIn summary, while proving that instances of CSPs near the phase transition cannot be\nsolved within polynomial time is an open problem, there are several strategies for construct-\ning more complex instances near the phase transition. These strategies involve increasing the\nsize of the problem, introducing hidden structures, combining different constraint types, and\nexploiting symmetries or other properties.\n11\n10\n9\nPrompt: Great! How to exploit symmetries to prove that there exist some CSPs near the\nphase transition cannot be solved within polynomial time?\nExploiting symmetries to prove that there exist some Constraint Satisfaction Problems\n(CSPs) near the phase transition that cannot be solved within polynomial time is a challenging\ntask. However, we can provide a general approach that might help in constructing such\ninstances.\n1. Study the problem structure: Analyze the structure of the CSP instances near the phase\ntransition.\nObserve the properties and patterns, such as symmetries, that make these\ninstances harder to solve.\n2. Construct instances with strong symmetries: Design CSP instances near the phase transition that\nhave a high degree of symmetry, which would make them challenging for algorithms to solve. For\nexample, you could create instances where the constraints are symmetric under certain transforma-\ntions (e.g., rotations, reflections, or permutations).\n3. Prove the hardness of these instances: To show that these instances cannot be solved within\npolynomial time, you can try to reduce a known NP-hard problem to the symmetric CSP\ninstances you\u2019ve constructed. This would imply that if there were a polynomial-time algo-\nrithm for solving these instances, it would also solve the known NP-hard problem, which\ncontradicts the assumption that P \u0338= NP.\n4. Show the existence: Finally, demonstrate that these symmetric CSP instances exist near the\nphase transition. This could be done through a combination of mathematical analysis and\nempirical evidence, such as generating random CSP instances near the phase transition\nand showing that a significant number of them have the desired symmetry properties.\nIt\u2019s important to note that this approach does not guarantee a proof that some CSPs near\nthe phase transition cannot be solved within polynomial time. However, it provides a frame-\nwork for constructing more complex instances and exploring their hardness, which could\npotentially lead to new insights and results in the study of phase transitions and computa-\ntional complexity.\n\u2018Symmetry\u2019 encompasses\nvarious interpretations,\nsuch as the symmetry\nof physical structures,\ngrammatical symmetry,\nand semantic symmetry.\n11\n9, 10\nPrompt: Great! The term \u2018symmetry\u2019 seems to be ambiguous. By \u2018instances with strong\nsymmetries\u2019, do you mean concepts similar to fixed points?\nApologies for the confusion. By \u201cinstances with strong symmetries,\u201d I mean instances that\nexhibit certain structural properties or regularities that make them harder to solve. This could\ninclude fixed points, but it is not limited to that concept.\nSymmetries in CSPs can refer to various types of invariance under transformations. For\nexample, a CSP instance could be symmetric under:\n1. Permutations: The variables and their assignments can be reordered without affecting the\nsatisfaction of the constraints.\n2. Rotations or Reflections: The problem can be transformed geometrically (e.g., by rotating\nor reflecting a graph) without changing the satisfaction of the constraints.\n3. Duality: A transformation that interchanges the roles of variables and constraints or do-\nmain values, while preserving the structure of the problem.\n12\nThese symmetries can make it more difficult for algorithms to distinguish between differ-\nent solutions or to efficiently search the solution space. Exploiting such symmetries in CSP\ninstances near the phase transition could help demonstrate the existence of problems that\ncannot be solved within polynomial time. However, proving that such instances exist and are\nhard to solve would require further analysis and rigorous mathematical proof.\n12\nPrompt: Suppose now that we have proven the existence of a class of difficult CSP problems\nthat are easy to experience phase transitions (where solvability changes rapidly) and have\ninvariance under transformations, like fixed points. How can we prove that this type of\nproblem cannot be solved in a given time complexity rigorously? Please provide 10 possible\nideas.\nIf multiple sampling\nattempts yield similar\nresults, compel the\nmodel to generate 10\nor 20 different ideas.\nThis strategy sacrifices\nthe depth of response\nbut broadens the\nexploration.\n1. Reduction to a known hard problem: Show that the given class of difficult CSP problems\ncan be reduced to a known problem that has already been proven to have a certain time\ncomplexity. This will imply that the given class of problems cannot be solved faster than\nthe known hard problem.\n2. Lower bound techniques: Use lower bound techniques like time hierarchy theorems and\ncommunication complexity to establish a lower bound on the time complexity of the given\nclass of problems. If the lower bound is higher than the given time complexity, then it is\nimpossible to solve the problem in that time.\n3. Circuit complexity: Analyze the circuit complexity of the problem. If it can be proven that\nthe problem requires circuits of a certain size, it would imply a lower bound on the time\ncomplexity of the problem.\n4. Fine-grained complexity: Investigate fine-grained complexity assumptions that are based\non conjectures about the hardness of specific problems. Use these conjectures to build a\nreduction from the given class of problems, proving that they cannot be solved within the\ngiven time complexity.\n5. Proof by contradiction: Assume that the given class of problems can be solved within the given time\ncomplexity, and then derive a contradiction. This would establish that the problem cannot be solved\nwithin the specified time complexity.\n6. Parameterized complexity: Analyze the problem\u2019s parameterized complexity by identify-\ning the parameters that affect the time complexity of the problem. Show that the problem\nis hard for some specific values of these parameters, implying that the problem cannot be\nsolved in the given time complexity.\n7. Space-time tradeoffs: Investigate space-time tradeoffs for the given class of problems. If\nit can be shown that any algorithm solving the problem within the given time complexity\nmust use an infeasible amount of space, this would imply that the problem cannot be\nsolved in the given time complexity.\n8. Randomized lower bounds: Use randomized lower bounds to show that even randomized\nalgorithms cannot solve the problem within the given time complexity. This will imply\nthat deterministic algorithms also cannot solve the problem in the given time complexity.\n9. Oracle separation: Assume the existence of an oracle that can solve some other hard prob-\nlem in polynomial time.\nShow that even with access to this oracle, the given class of\nproblems cannot be solved within the given time complexity.\n13\n10. Algebraic complexity: Investigate the algebraic complexity of the problem by analyzing the\nalgebraic computation tree model. Show that any algorithm solving the problem requires\na certain number of algebraic operations, implying a lower bound on the time complexity\nof the problem.\n13\n12\nPrompt: Great! Please design an idea for proof by contradiction.\nIdea for Proof by Contradiction:\n1. Assume that there exists an algorithm A that can solve the given class of difficult CSP\nproblems within the given time complexity T(n), where n is the input size.\n2. Identify a transformation function F that maps instances of the problem to other instances\nwhile preserving the problem\u2019s inherent difficulty and invariance under transformation.\n3. Let P be a difficult CSP problem instance from the class that exhibits phase transitions,\nand let Q be its transformed version using the function F such that Q = F(P).\n4. Since our assumption states that algorithm A can solve P within time complexity T(n), it\nshould also be able to solve Q within the same given time complexity T(n), as Q has the\nsame inherent difficulty as P.\n5. Now, given that P experiences phase transitions, there exists a critical region (a point or\ntiny interval) where the solvability of P drastically changes. Let\u2019s call this critical region C.\n6. By the invariance property of the transformation function F, there exists a critical region C\u2032\nfor Q such that C\u2032 = F(C). Now, in the critical region C\u2032, the solvability of Q also changes\nrapidly.\n7. Since A can solve P within time complexity T(n), it should be able to solve instances of P\nin the critical region C within T(n) as well. Similarly, it should be able to solve instances\nof Q in the critical region C\u2032 within T(n).\n8. However, due to the rapid change in solvability in the critical regions C and C\u2032, algorithm\nA will face difficulty in consistently solving instances of P and Q within the given time\ncomplexity T(n). This is because the rapid change in solvability implies that small changes\nin the input can lead to a dramatic increase in the difficulty of solving the problem.\n9. This contradicts our initial assumption that algorithm A can solve the given class of diffi-\ncult CSP problems within the given time complexity T(n).\n10. Therefore, our initial assumption is false, and there is no algorithm that can solve the given\nclass of difficult CSP problems within the given time complexity T(n) for all instances,\nincluding those in the critical regions where solvability changes rapidly.\nA basic intuition is\nestablished suggesting\nthat once we prove the\nexistence of extremely\nhard CSPs, we can use\na proof by contradiction\nto demonstrate that the\nproblems cannot be solved\nin polynomial time.\n14\n3, 4, 5, 6, 7, 8, 9, 10,11 12, 13\nPrompt: Please now organize all our historical conversations and sort out a general proof\nidea.\nHere\u2019s a general proof idea based on our historical conversations:\n1. Identify a well-known NP-complete problem, such as SAT, which has been extensively\nstudied and known to be hard to solve.\n14\n2. Convert the NP-complete problem into a CSP problem using a polynomial-time reduction\nthat preserves the problem\u2019s inherent difficulty.\n3. Study the phase transition behavior of the resulting CSP problem by identifying a parame-\nter associated with the problem that influences its difficulty. Investigate how the problem\u2019s\ndifficulty changes as the parameter varies and identify the phase transition point where\nthe problem transitions from being easy to hard.\n4. Construct instances of the CSP problem near the phase transition point that exhibit strong\nsymmetries or invariance under specific transformations, such as fixed points, permuta-\ntions, rotations, reflections, or duality.\n5. Prove the existence of hard instances of the CSP problem near the phase transition point by\nshowing that certain instances exhibit properties or structures that make them inherently\ndifficult to solve. This might involve relating the CSP to a well-known hard problem or\nestablishing the presence of a lower bound on the complexity of the instances.\nThis response serves as\na general schema for\nour subsequent proof.\nBeginning with the next\nturn, we will adhere to\nthis preliminary schema\nto rigorously proceed\nwith our proofs. The two\nhighlighted parts are the\ntwo key points in our\nsubsequent proof.\n6. Design a proof by contradiction to show that the hard instances of the CSP problem near\nthe phase transition point cannot be solved within a given time complexity. Assume that\nthere exists an algorithm that can solve the problem within the given time complexity,\nand derive a contradiction by considering the phase transition behavior, symmetries, and\ninvariance properties of the problem instances.\n7. Conclude that the hard instances of the CSP problem near the phase transition point can-\nnot be solved within the given time complexity. This provides evidence supporting the\nintractability of the CSP problem and the claim that P \u0338= NP.\nKeep in mind that this general proof idea provides a framework for exploring the com-\nplexity of CSP problems and their relationship to NP-complete problems. A rigorous proof\nof the intractability of specific CSP problems would require a more extensive analysis and\nmathematical proof.\n15\nPrompt: Can you list some models that can generate hard examples for Constraint Satisfac-\ntion Problems (CSP)?\nThe rigorous proof begins.\nAccording to the first\nkey point of the schema,\nwe begin constructing\nextremely hard CSPs.\nConstraint Satisfaction Problems (CSP) involve finding values for a set of variables subject\nto a set of constraints. Some models and techniques are designed to generate hard instances\nof CSPs, making it challenging for algorithms to solve them efficiently. These hard instances\nare useful for evaluating and comparing the performance of various algorithms.\n1. Phase Transition Model: CSPs exhibit a phase transition phenomenon where the problem\ngoes from being mostly easy to mostly hard as the density of constraints increases. The\nphase transition model generates hard instances by tuning the constraint density to be\naround the phase transition region. For example, in the case of random k-SAT problems,\nthis occurs when the ratio of constraints to variables (clause-to-variable ratio) is around a\ncritical value.\nLLM responses may\ncontain hallucinations,\nsuch as the claim that\nModel RB focuses on\nbackbone size. While\ndrawing insights from\nmodel responses, it is\ncrucial to verify the\nresponse.\n2. Model RB: Model RB is a random binary CSP generator that focuses on the backbone\nsize, the number of variables that must be assigned a specific value in any solution. By\ncontrolling the backbone size, Model RB can generate hard instances with a small backbone\nsize, which makes them difficult to solve.\n15\n3. Hidden Structure Model: This model generates hard instances by introducing hidden\nstructure in CSPs. For example, problems with an underlying graph structure (e.g., graph\ncoloring) can be made more challenging by hiding clique-like substructures within the\ngraph. The hidden structure makes it difficult for algorithms to exploit the natural struc-\nture of the problem.\n4. Quasigroup Completion Problem (QCP): QCP is an example of a specific CSP that can\ngenerate hard instances. In this problem, a partially filled Latin square must be completed\nby assigning the missing values while preserving the Latin square property (i.e., each value\nappears exactly once in each row and column).\n5. Magic Square Problem: Another specific CSP that can produce hard instances is the magic\nsquare problem, where the task is to fill a square grid with distinct integers such that the\nsum of the numbers in each row, column, and diagonal is the same. The problem becomes\nharder as the size of the square increases.\nThese models and problems can generate hard instances for CSPs that are particularly\nuseful for benchmarking and testing the performance of constraint solvers and heuristic al-\ngorithms.\nIn the last turn of dia-\nlogue, GPT-4 mentioned\nModel RB. Then, we\nintegrated additional\ndetails of Model RB into\nthe prompt for the current\nturn.\n16\nPrompt: Model RB is a random constraint satisfaction problem (CSP) model, which could\nalso be encoded to well-known NP-complete problems like SAT and CLIQUE.\nA random instance I of Model RB consists of the following:\n\u2022 A set of variables X = {x1, . . . , xn} : Each variable xi takes values from its domain Di,\nand the domain size is |Di| = d, where d = n\u03b1 for i = 1, . . . , n, and \u03b1 > 0 is a constant.\n\u2022 A set of constraints C = {C1, . . . , Cm} (m = rn ln d, where r > 0 is a constant): for each\ni = 1, . . . , m, constraint Ci = (Xi, Ri). Xi = (xi1, xi2, . . . , xik) (k \u2265 2 is a constant) is a\nsequence of k distinct variables chosen uniformly at random without repetition from X .\nRi is the permitted set of tuples of values which are selected uniformly without repetition\nfrom the subsets of Di1 \u00d7 Di2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Dik, and |Ri| = (1 \u2212 p)dk where 0 < p < 1 is a\nconstant.\nWe want to construct extremely hard examples from instances of Model RB to create phase\ntransition, that is to show that there is a fixed point that the satisfiability can be easily\nchanged, do you have any suggestions?\nHenceforth, we will\nadopt \u2018satisfiability\u2019, a\nmore precise term for\nCSPs, as a substitute for\n\u2018solvability\u2019.\nTo construct extreme hard instances from Model RB that exhibit phase transitions, we can\nfollow these steps:\n1. Identify critical values for parameters: The first step is to determine the critical values for\nthe parameters \u03b1, r, and p that are associated with the phase transition. This can be done\nby performing an empirical study or by analyzing the model theoretically. In the case\nof Model RB, previous studies have found that when r is around a critical value rc, the\nsatisfiability undergoes a sharp transition from being almost always satisfiable to almost\nalways unsatisfiable.\n2. Generate instances near the critical values: Once the critical values are identified, generate\na set of instances of Model RB with parameters close to these critical values. These in-\nstances should have a mix of satisfiable and unsatisfiable cases, making them challenging\nfor constraint solvers to handle.\n3. Add additional constraints or variables: To further increase the hardness of the instances, consider\nadding additional constraints or variables that do not change the satisfiability but increase the\n16\ncomplexity of the problem. This can be done by introducing new variables with carefully chosen\ndomains or by adding redundant constraints that are logically equivalent to existing constraints\nbut create additional search space for solvers.\n4. Test solvers on these instances: Evaluate the performance of various constraint solvers\non these hard instances. This will help you understand the strengths and weaknesses of\ndifferent solving techniques and identify areas for improvement in the solvers.\n5. Analyze the structure of hard instances: Investigate the structure of these hard instances\nto gain insights into the features that make them difficult for solvers. This can include\nanalyzing the constraint graph, the distribution of constraint tightness, or other properties\nthat may be related to the hardness of the instances.\nBy following these steps, you can create instances of Model RB that exhibit phase tran-\nsitions, providing a challenging testbed for constraint solvers and contributing to a deeper\nunderstanding of the factors that influence the hardness of constraint satisfaction problems.\nWe use 16* to denote that\nthe definition of Model\nRB, mentioned in the\nprompt of 16, has been\nincorporated into the\ncurrent prompt.\n17\n16*\nPrompt: Great! Can you propose some possible additional constraints?\nCertainly! Here are some possible additional constraints you can consider adding to create\nextremely hard examples from instances of Model RB:\n1. Hidden variable constraints: Introduce new variables with domains that are subsets of the\noriginal variables\u2019 domains. Add constraints that force these new variables to take values\nthat are consistent with the values of the original variables. This will create additional\nsearch space without changing the satisfiability of the problem.\n2. Redundant constraints: Create multiple copies of existing constraints but with different\nvariable orderings or with variables replaced by their equivalent hidden variables. This\nwill increase the size and complexity of the constraint graph without affecting the prob-\nlem\u2019s satisfiability.\n3. Implied constraints: Add constraints that are logically implied by the existing constraints.\nFor example, if you have two constraints C1 = (X1, R1) and C2 = (X2, R2) such that X1\nand X2 share a common variable, you could add a new constraint C3 = (X1 \u2229 X2, R3) that\nenforces the consistency between C1 and C2.\n4. Symmetry-breaking constraints: If your problem has a high degree of symmetry, you can\nintroduce constraints that break these symmetries by forcing a specific ordering or assign-\nment of values to variables. This can help reduce the search space and make the problem\nharder to solve.\nThe discussion on\nsymmetry-breaking con-\nstraints may not directly\nanswer our question, but\nit does provide valuable\ninsights. In particular, it\nemphasizes that problems\ninvolving constrained\nsymmetries can be hard.\n5. Cyclic constraints: Add constraints that form cycles in the constraint graph, making it more\ndifficult for solvers to perform simple constraint propagation and variable elimination\ntechniques.\nRemember that when adding these additional constraints, you should ensure that they do not\nchange the satisfiability of the problem. The goal is to increase the complexity and hardness\nof the instances without affecting the underlying solutions.\n18\n17\nPrompt: Great! What symmetry requirements need to be added to the original Model RB so\nthat the instances of Model RB are highly symmetric?\n17\nTo introduce highly symmetric instances in Model RB, you can consider the following\nsymmetry requirements:\n1. Permutation Symmetry: Ensure that the variables and their domains are interchangeable.\nThis can be achieved by making the domains for all variables identical, i.e., Di = Dj for all\ni, j \u2208 {1, . . . , n}.\n2. Value Symmetry: Ensure that some or all values in the domain can be interchanged with-\nout affecting the satisfiability of the problem. You can achieve this by making some con-\nstraints invariant under the exchange of values. For example, you can introduce constraints\nin the form Ci = (Xi, Ri), where Ri is a relation that is symmetric with respect to its values.\n3. Constraint Symmetry: Ensure that the problem remains the same when some or all con-\nstraints are permuted. To achieve this, you can introduce constraints that are logically\nequivalent but have a different order of variables or use different tuples of values. This\ncan be done by creating constraints of the form Ci = (Xi, Ri), where Ri is a relation that is\nsymmetric with respect to its variables. This will make the problem more difficult to solve,\nas the solver needs to explore more possibilities in the search space.\nBy introducing these symmetry requirements, you can create instances of Model RB that\nare highly symmetric and more challenging for solvers. Note that the challenge lies in finding\nthe right balance between symmetry and hardness, as excessive symmetry can sometimes\nmake the problem easier to solve.\n19\n17, 18\nPrompt: Please redefine the RB Model with constraint symmetry requirement.\nTo redefine the Model RB with constraint symmetry requirement, we\u2019ll introduce sym-\nmetric relations in the constraints and maintain the other components of the original model.\nHere\u2019s the modified model:\n1. A set of variables X = {x1, . . . , xn}. Each variable xi takes values from its domain Di, and\nthe domain size is |Di| = d, where d = n\u03b1 for i = 1, . . . , n, and \u03b1 > 0 is a constant.\n2. A set of constraints C = {C1, . . . , Cm} (m = rn ln d, where r > 0 is a constant): for each\ni = 1, . . . , m, constraint Ci = (Xi, Ri). Xi = {xi1, xi2, . . . , xik} (k \u2265 2 is a constant) is a\nsequence of k distinct variables chosen uniformly at random without repetition from X .\n3. Ri is the permitted set of tuples of values, which are selected uniformly without repetition\nfrom the subsets of Di1 \u00d7 Di2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Dik, and |Ri| = (1 \u2212 p)dk, where 0 < p < 1 is a\nconstant. To introduce constraint symmetry, ensure that the relation Ri is symmetric with\nrespect to its variables. This can be done by creating constraints of the form Ci = (Xi, Ri),\nwhere Ri is invariant under permutation of its variables. For example, if k = 3, a sym-\nmetric relation Ri could be such that for any tuple (a, b, c) \u2208 Ri, all other permutations of\n(a, b, c) are also in Ri, i.e., (a, b, c), (a, c, b),\n(b, a, c), (b, c, a), (c, a, b), (c, b, a) \u2208 Ri. By incorporating constraint symmetry in this man-\nner, the modified Model RB will have a higher degree of symmetry, making it more chal-\nlenging for solvers.\nAsking for details\ncan stimulate LLMs\nto generate more\nprecise responses\nor to correct minor\nerrors. This strategy is\nespecially important\nwhen generating\ndefinitions, and can\nbe more efficient than\ncontinuously sampling\nfor a perfect answer.\n20\n17, 18, 19\nPrompt: Are the domains of different Ri the same? For different i, is the permitted set Ri\nisomorphic? If not, how to update the definition of Model RB?\n18\nIn the original Model RB definition, the domains for different constraints Ci are not nec-\nessarily the same, as they are defined by the random selection of variables xi1, xi2, . . . , xik.\nHowever, the domain size |Di| for each variable is the same, as it is defined by the constant\nd = n\u03b1. As for the permitted sets Ri, they are not guaranteed to be isomorphic, as they are\nalso constructed randomly.\nTo update the definition of Model RB so that the sets Ri are isomorphic, we can introduce\na single symmetric relation R\u2217, and use it to define all permitted sets Ri. To do this, follow\nthese steps:\nDefine a symmetric relation R\u2217 with respect to its variables. This can be done by creating a\nrelation that is invariant under permutation of its variables. Make sure that |R\u2217| = (1 \u2212 p)dk,\nwhere 0 < p < 1 is a constant.\nFor each constraint Ci = (Xi, Ri), define the permitted set Ri as an isomorphic copy of the\nsymmetric relation R\u2217. To create an isomorphic copy of R\u2217, map the values in the domain of\nR\u2217 to the corresponding values in the domains of Xi = {xi1, xi2, . . . , xik}.\nBy using the same symmetric relation R\u2217 for all permitted sets Ri in this way, the modified\nModel RB will ensure that the domains of different Ri are isomorphic, while still maintaining\nthe constraint symmetry requirement.\n21\n17, 18, 19, 20\nPrompt: How to define R\u2217? How to generate the permitted set Ri for each constraint from\nR\u2217? Ensure that the generation of Ri is random.\nTo define R\u2217 and generate random permitted sets Ri from it, follow these steps:\nDefine a symmetric relation R\u2217 with respect to its variables by selecting tuples from a\ngeneric domain D\u2217 = {1, 2, . . . , d}k. To ensure symmetry, for each tuple (a1, a2, . . . , ak) \u2208 R\u2217,\nmake sure all its permutations are also in R\u2217. The size of R\u2217 should be |R\u2217| = (1 \u2212 p)dk,\nwhere 0 < p < 1 is a constant.\nTo generate a random permitted set Ri for each constraint Ci = (Xi, Ri), use the isomorphic\nrelation R\u2217 and perform the following steps:\n1. Randomly select a bijection \u03d5i between the generic domain D\u2217 and the domains of Xi =\n{xi1, xi2, . . . , xik}. This bijection maps values from D\u2217 to corresponding values in the do-\nmains of Xi.\n2. Apply the bijection \u03d5i to each tuple in R\u2217 to create the permitted set Ri. For each tuple\n(a1, a2, . . . , ak) \u2208 R\u2217, include the tuple (\u03d5i(a1), \u03d5i(a2), . . . , \u03d5i(ak)) in Ri.\nBy generating random bijections \u03d5i for each constraint, you ensure that the permitted sets\nRi are randomly generated while maintaining an isomorphic relation to the symmetric rela-\ntion R\u2217. This modified Model RB will have isomorphic and randomly generated permitted\nsets Ri while still preserving the constraint symmetry requirement.\n22\n17, 18, 19, 20, 21\nPrompt: I think it\u2019s better to use fi instead of \u03d5i. Also, is it possible to only apply bijection\nto k \u2212 1 domains?\nThis step is optional,\nbut we can tailor the\nsymbols or streamline the\nproblem according to our\nunderstanding.\nYes, you can use fi instead of \u03d5i to represent the bijection for each constraint. To apply the\nbijection to only k \u2212 1 domains, you can modify the method as follows:\n1. Define a symmetric relation R\u2217 with respect to its variables by selecting tuples from a\ngeneric domain D\u2217 = {1, 2, . . . , d}k. To ensure symmetry, for each tuple (a1, a2, . . . , ak) \u2208\n19\nR\u2217, make sure all its permutations are also in R\u2217. The size of R\u2217 should be |R\u2217| = (1 \u2212 p)dk,\nwhere 0 < p < 1 is a constant.\n2. To generate a random permitted set Ri for each constraint Ci = (Xi, Ri), use the isomorphic\nrelation R\u2217 and perform the following steps:\n(a) Randomly select a bijection fi between the generic domain D\u2217 and the domains of\nXi = {xi1, xi2, . . . , xik} for the first k \u2212 1 variables, keeping the domain of the last variable\nxik unchanged.\nThis bijection maps values from D\u2217 to corresponding values in the\ndomains of Xi for the first k \u2212 1 variables.\n(b) Apply the bijection fi to each tuple in R\u2217 to create the permitted set Ri. For each tuple\n(a1, a2, . . . , ak) \u2208 R\u2217, include the tuple ( fi(a1), fi(a2), . . . , fi(ak\u22121), ak) in Ri.\nBy applying the bijections to only k \u2212 1 domains, you will create instances of Model RB\nwith isomorphic permitted sets Ri that preserve the constraint symmetry requirement\nwhile introducing additional variability in the domains.\n23\n17, 18, 19, 20, 21, 22\nPrompt: I hope to define a bijection for each domain separately to increase complexity.\nAdditionally, please make it more concise.\nTo increase complexity, we can define a separate bijection for each domain in the first k \u2212 1\nvariables. Here\u2019s the concise modified Model RB:\n1. Define a symmetric relation R\u2217 with respect to its variables by selecting tuples from a\ngeneric domain D\u2217 = {1, 2, . . . , d}k. Ensure symmetry by including all permutations of\neach tuple in R\u2217. Set |R\u2217| = (1 \u2212 p)dk, where 0 < p < 1 is a constant.\n2. To generate a random permitted set Ri for each constraint Ci = (Xi, Ri), use the isomorphic\nrelation R\u2217 and perform the following steps:\n(a) For each of the first k \u2212 1 variables in Xi = {xi1, xi2, . . . , xik}, randomly select a bijection\nfij between the generic domain D\u2217 and the domain of xij.\n(b) Apply the bijections fij to each tuple in R\u2217 to create the permitted set Ri. For each tuple\n(a1, a2, . . . , ak) \u2208 R\u2217, include the tuple ( fi1(a1), fi2(a2), . . . , fik\u22121(ak\u22121), ak) in Ri.\nThis modification introduces separate bijections for each domain in the first k \u2212 1 variables,\nincreasing the complexity of the Model RB while preserving the constraint symmetry require-\nment.\nLLMs might fall short\nin ensuring accuracy\nand thoroughness.\nTherefore, when\nutilizing LLMs for\ngenerating definitions\nor carrying out compu-\ntations, it\u2019s advisable\nto iteratively refine the\noutputs and manually\nconfirm their precision\nand rigor.\n24\n17, 18, 19, 20, 21, 22, 23\nPrompt: Great! So now please give a final version of the definition of the improved Model\nRB, keeping the symbols in the original definition as much as possible.\nHere\u2019s the final version of the improved Model RB definition:\n1. A set of variables X = {x1, . . . , xn}. Each variable xi takes values from its domain Di, and the\ndomain size is |Di| = d, where d = n\u03b1 for i = 1, . . . , n, and \u03b1 > 0 is a constant.\n2. A set of constraints C = {C1, . . . , Cm} ( m = rn ln d, where r > 0 is a constant): for each\ni = 1, . . . , m, constraint Ci = (Xi, Ri). Xi = {xi1, xi2, . . . , xik} ( k \u2265 2 is a constant) is a sequence\nof k distinct variables chosen uniformly at random without repetition from X .\n20\n3. Define a symmetric relation R\u2217 with respect to its variables by selecting tuples from a generic do-\nmain D\u2217 = {1, 2, . . . , d}k. Ensure symmetry by including all permutations of each tuple in R\u2217. Set\n|R\u2217| = (1 \u2212 p)dk, where 0 < p < 1 is a constant.\n4. To generate a random permitted set Ri for each constraint Ci = (Xi, Ri), use the isomorphic relation\nR\u2217 and perform the following steps:\n(a) For each of the first k \u2212 1 variables in Xi, randomly select a bijection fij between the generic\ndomain D\u2217 and the domain of xij.\n(b) Apply the bijections fij to each tuple in R\u2217 to create the permitted set Ri. For each tuple\n(a1, a2, . . . , ak) \u2208 R\u2217, include the tuple ( fi1(a1), fi2(a2), . . . , fik\u22121(ak\u22121), ak) in Ri.\nThis improved Model RB incorporates separate bijections for each domain in the first k \u2212 1\nvariables, increasing the complexity while preserving the constraint symmetry requirement.\n25\n24\nPrompt: Based on this, can you define the assignment (\u03c3) and the solution of I, respectively?\nWhen a constraint is said to be satisfied by an assignment? When I is called satisfiable?\nAn assignment \u03c3 for a random instance I of Model RB is a function that maps each vari-\nable xi \u2208 X to a value v \u2208 Di. In other words, \u03c3 : X \u2192 Sn\ni=1 Di, such that \u03c3(xi) \u2208 Di for all\ni = 1, . . . , n. An assignment \u03c3 assigns a value from the domain of each variable in the problem\ninstance.\nA constraint Ci = (Xi, Ri) is said to be satisfied by an assignment \u03c3 if the tuple (\u03c3(xi1),\n\u03c3(xi2), . . . , \u03c3(xik)) \u2208 Ri. This means that the assignment of values for the variables in Xi must\nbe a tuple in the permitted set Ri for the constraint to be satisfied.\nA solution for the instance I is an assignment \u03c3 that satisfies all the constraints in the set\nC. Formally, a solution \u03c3 satisfies \u2200Ci \u2208 C, (\u03c3(xi1), \u03c3(xi2), . . . , \u03c3(xik)) \u2208 Ri.\nThe instance I is called satisfiable if there exists a solution \u03c3 that satisfies all the constraints\nin C. If no such solution exists, the instance is called unsatisfiable.\n26\n24\nPrompt: We want to construct instances of Model RB where the satisfiability can be easily\nchanged (but keep the parameter values unchanged), do you have any possible ideas?\nThis prompt highlights\ntwo essential require-\nments: phase transition\n(mentioned as \u2018a sudden\nchange in the solvability\u2019\nin 8) and fix point (re-\nferred to as \u2018invariance\nunder transformations\u2019 in\n11).\nYes, there are several ideas to construct instances of Model RB where the satisfiability can\nbe easily changed, while keeping the parameter values unchanged. Here are some ideas:\n1. Modify the permitted set R\u2217: You can change the structure of the permitted set R\u2217 by\nadding or removing tuples, while keeping the size of R\u2217 constant. Ensure that the new\nR\u2217 remains symmetric with respect to its variables. This will have a direct impact on the\nsatisfiability of the CSP, as the constraints will be affected.\n2. Keep a set of \"control\" variables: Introduce a set of \"control\" variables and their corre-\nsponding constraints that have a direct impact on the satisfiability of the problem. For\nexample, you can include additional constraints that involve these control variables and\nare either very restrictive (making the problem unsatisfiable) or very permissive (allowing\nfor more solutions). By changing the values assigned to these control variables, you can\neffectively change the satisfiability of the instance.\n21\n3. Modify the constraints C: Modify the constraints by changing the variables involved in the con-\nstraints or the permitted sets Ri. This can be done by either swapping variables in the con-\nstraints or by applying different value mapping functions fi. Ensure that the number of\nconstraints remains constant, so that the parameter values stay unchanged.\n4. Introduce XOR constraints: You can add a new type of constraint called XOR constraint,\nwhich is an exclusive OR constraint between two or more variables.\nBy adjusting the\nnumber of XOR constraints and the variables involved in them, you can control the sat-\nisfiability of the problem. An even number of XOR constraints can make the problem\nsatisfiable, while an odd number of XOR constraints can make it unsatisfiable.\n5. Combine multiple instances: You can create multiple instances of Model RB with different\nsatisfiability properties and combine them into a single instance. By doing so, you can\ncreate complex instances where the satisfiability depends on the interaction between the\nsub-instances. This approach allows for a more fine-grained control over the satisfiability\nof the problem.\nIn summary, there are multiple ways to construct instances of Model RB where the sat-\nisfiability can be easily changed while keeping the parameter values unchanged. By mod-\nifying the permitted set, introducing control variables, changing constraints, adding XOR\nconstraints, or combining multiple instances, you can create instances of Model RB with\nvarying satisfiability properties.\nBeginning here, we\ngradually establish the\ntransformation that\nchanges the problem\u2019s\nsatisfiability while\npreserving invariance.\n27\n24, 26\nPrompt: Can you provide a more specific solution for changing the satisfiability of a Model\nRB instance? I would like the resulting instance after the mapping to remain a valid Model\nRB instance. Please explain this with an example, specifically by applying the mapping to a\nconstraint C = (X, R) of a Model RB instance with k = 2.\nTo change the satisfiability of a Model RB instance while keeping the parameter values\nand domains unchanged, we can modify the constraints by swapping variables or altering\nthe permitted sets Ri. In this case, we\u2019ll focus on altering the permitted sets without changing\nthe variables in the permitted set.\nHere\u2019s a specific solution for changing the satisfiability of a Model RB instance with k = 2:\n1. Start with an instance of Model RB, where each constraint Ci = (Xi, Ri), and Xi = {xi1, xi2}.\n2. Define a new set of permitted values R\u2032\ni, which is derived from the existing set Ri, but with\na different value mapping function. To do this, for each tuple (a, b) \u2208 Ri, we will apply\na new bijection gi on the first value a to obtain the new tuple (gi(a), b). Include this new\ntuple in R\u2032\ni.\n3. Replace the original permitted set Ri with the modified permitted set R\u2032\ni for each constraint\nCi = (Xi, Ri). This results in a new constraint C\u2032\ni = (Xi, R\u2032\ni).\n4. The new instance of Model RB will have the same set of variables, domains, and con-\nstraints, but with the modified permitted sets R\u2032\ni for each constraint.\nAs an example, let\u2019s say we have a Model RB instance with a constraint C = (X, R), where\nk = 2, X = {x1, x2}, and R = {(1, 2), (2, 3), (3, 1)}. We define a new bijection g such that\ng(1) = 2, g(2) = 3, and g(3) = 1. Applying this bijection to the first values in the tuples of\nR, we get the new permitted set R\u2032 = {(2, 2), (3, 3), (1, 1)}.\n22\nNow, we replace the original constraint C = (X, R) with the modified constraint C\u2032 =\n(X, R\u2032). The new Model RB instance has the same variables, domains, and constraints, but\nwith the modified permitted set R\u2032, which may change its satisfiability.\nBy using this method, we can change the satisfiability of a Model RB instance while keep-\ning the parameter values and domains unchanged. This approach allows us to create in-\nstances with different properties, which can be useful for experimentation and understanding\nthe behavior of Model RB instances.\n28\n24, 26, 27\nPrompt: Great! In the case of k = 2, how can we change the satisfiability while minimizing\nthe impact of the mapping operation on the instance?\nIn the case of k = 2, to change the satisfiability while minimizing the impact of the map-\nping operation on the instance, we can focus on modifying the permitted sets in such a way\nthat the alterations are minimal. One way to do this is by ensuring that the new bijections we\nchoose are as close as possible to the original bijections.\nHere\u2019s a specific approach to minimize the impact:\n1. Start with an instance of Model RB, where each constraint Ci = (Xi, Ri), and Xi = {xi1, xi2}.\n2. For each constraint Ci, find a minimal bijection gi that changes the satisfiability of Ri. To\ndo this, you can try swapping a single pair of values in the domain or choosing the identity\nmapping (i.e., keeping the same values).\n3. Apply the minimal bijection gi to the first value of each tuple in the original permitted set\nRi to create the new permitted set R\u2032\ni.\n4. Replace the original permitted set Ri with the modified permitted set R\u2032\ni for each constraint\nCi = (Xi, Ri). This results in a new constraint C\u2032\ni = (Xi, R\u2032\ni).\n5. The new instance of Model RB will have the same set of variables, domains, and con-\nstraints, but with the minimally modified permitted sets R\u2032\ni for each constraint.\nLet\u2019s take the previous example:\n- C = (X, R), where k = 2, X = {x1, x2}, and R = {(1, 2), (2, 3), (3, 1)}.\nTo minimize the impact of the mapping operation, we could choose bijection g such that\ng(1) = 2, g(2) = 1, and g(3) = 3 (swapping only values 1 and 2). Applying this bijection to\nthe first values in the tuples of R, we get the new permitted set R\u2032 = {(2, 2), (1, 3), (3, 1)}.\nNow, we replace the original constraint C = (X, R) with the modified constraint C\u2032 =\n(X, R\u2032). The new Model RB instance has the same variables, domains, and constraints, but\nwith the minimally modified permitted set R\u2032, which may change its satisfiability.\nBy using this minimal impact approach, we can change the satisfiability of a Model RB\ninstance while keeping the parameter values and domains unchanged, and ensure that the\nalterations to the instance are as small as possible. This can be useful for understanding the\nsensitivity of Model RB instances to small changes in their permitted sets.\n29\n24, 26, 27, 28\nPrompt: Great! However, if the new tuple (after swapping a single pair of values) is already\npresent in the permitted set, it will not change the constraint. Can you provide a more\nprecise and concise definition of this mapping operation?\n23\nYou\u2019re correct. If the new tuple (after swapping a single pair of values) is already present\nin the permitted set, it will not change the constraint. To address this issue, we can use a\nmore precise and concise definition of the mapping operation that guarantees a change in the\nconstraint.\nLet\u2019s define the mapping operation as follows:\nStart with an instance of Model RB, where each constraint Ci = (Xi, Ri), and Xi = xi1, xi2.\nFor each constraint Ci, find a bijection gi that changes the satisfiability of Ri by swapping a\nsingle pair of values in the domain, ensuring that the new tuple is not already in the permitted\nset. If no such bijection exists, try swapping two pairs of values. Repeat this process until\nyou find a bijection that guarantees a change in the constraint.\nApply the bijection gi to the first value of each tuple in the original permitted set Ri to\ncreate the new permitted set R\u2032\ni.\nReplace the original permitted set Ri with the modified permitted set R\u2032\ni for each constraint\nCi = (Xi, Ri). This results in a new constraint C\u2032\ni = (Xi, R\u2032\ni).\nThe new instance of Model RB will have the same set of variables, domains, and con-\nstraints, but with the permitted sets R\u2032\ni for each constraint, ensuring a change in the con-\nstraint.\nUsing this mapping operation, we can guarantee a change in the satisfiability of the Model\nRB instance while minimizing the impact on the permitted sets.\nFor example, let\u2019s say we have a constraint C = (X, R) with k = 2, X = x1, x2, and\nR = (1, 2), (2, 3), (3, 1). To find a bijection g that changes the constraint, we can first try\nswapping a single pair of values, like g(1) = 2, g(2) = 1, and g(3) = 3. However, since\napplying this bijection results in a tuple (2, 2) that is already in the permitted set, we need\nto try another bijection. Suppose we try swapping two pairs of values: g(1) = 3, g(2) = 1,\nand g(3) = 2. Applying this bijection to the first values in the tuples of R, we get the new\npermitted set R\u2032 = (3, 2), (1, 3), (2, 1), which is different from the original set R. Now, we can\nreplace the original constraint C = (X, R) with the modified constraint C\u2032 = (X, R\u2032), ensuring\na change in the constraint while minimizing the impact on the instance.\n30\n24, 26, 27, 28, 29\nPrompt: It seems good, but there might be two small issues: 1. The expression \u2018changes\nthe satisfiability of Ri\u2019 doesn\u2019t seem correct since Ri is a permitted set. 2. In your example,\n(2, 2) is not in the permitted set. Please check for these issues, modify the definition of the\nmapping operation and make it more concise.\nYou are right, my previous response contained inaccuracies. I apologize for the confusion,\nand I appreciate your patience. Here\u2019s a revised and more concise definition of the mapping\noperation to address these issues:\n1. Start with an instance of Model RB, where each constraint Ci = (Xi, Ri), and Xi = {xi1, xi2}.\n2. For each constraint Ci, find a bijection gi that modifies Ri by swapping a single pair of values in\nthe domain while ensuring that the new tuples generated are not already in the permitted set. If no\nsuch bijection exists, try swapping two pairs of values. Repeat this process until you find a bijection\nthat guarantees a change in Ri.\n3. Apply the bijection gi to the first value of each tuple in the original permitted set Ri to create the\nnew permitted set R\u2032\ni.\n4. Replace the original permitted set Ri with the modified permitted set R\u2032\ni for each constraint Ci = (Xi, Ri).\nThis results in a new constraint C\u2032\ni = (Xi, R\u2032\ni).\n24\n5. The new instance of Model RB will have the same set of variables, domains, and constraints, but\nwith the modified permitted sets R\u2032\ni for each constraint.\nFor example, let\u2019s say we have a constraint C = (X, R) with k = 2, X = {x1, x2}, and\nR = {(1, 2), (2, 3), (3, 1)}. To find a bijection g that modifies the permitted set, we can first try\nswapping a single pair of values: g(1) = 2, g(2) = 1, and g(3) = 3. Applying this bijection\nto the first values in the tuples of R, we get the new permitted set R\u2032 = {(2, 2), (1, 3), (3, 1)},\nwhich is different from the original set R. Now, we can replace the original constraint C =\n(X, R) with the modified constraint C\u2032 = (X, R\u2032), ensuring a change in the permitted set\nwhile minimizing the impact on the instance.\nOur objective is presented\nas a theorem for a meticu-\nlous proof and is denoted\nas 31\u2217 in later references.\nMotivated by 9, we con-\nsider the scenario where\nthe number of variables\nis infinitely large, as it is\nmore prone to generate\nextremely hard instances\nwith almost indepen-\ndent solution space. In\nthis case, we can use\n\u2018an infinite set\u2019 because\nthe instance-generating\nprocess of Model RB is\nrandom, which signifies\nthat there could be infinite\npossible instances as the\nnumber of variables or the\ndomain size approaches\ninfinity.\n31\n24, 25, 30\nPrompt: We want to prove the Theorem: There exists an infinite set of satisfiable and unsat-\nisfiable instances of Model RB such that this set is a fixed point under the mapping operation\nof changing satisfiability. (n tends to infinity, d tends to infinity.)\nWhat thinking method should we use to conceive the proof plan?\nFor intricate problems,\nask LLMs to lay out a\nplan for problem de-\nconstruction. However,\nif the plans or solutions\nappear too restricted or\nsingular, prompt LLMs\nto explore various\nthinking methods first.\nThis strategy inherently\nbroadens the search\nspace of LLMs.\nTo prove the theorem, we can take a step-by-step approach, combining various thinking\nmethods such as constructive reasoning, logical reasoning, and contradiction.\n1. Constructive reasoning: First, we need to demonstrate the existence of an infinite set of satisfiable\nand unsatisfiable instances of Model RB. This can be done by constructing a sequence of instances\nwith different parameters and showing that the satisfiability of these instances alternates between\nsatisfiable and unsatisfiable.\n2. Logical reasoning: Next, we need to show that the mapping operation of changing sat-\nisfiability indeed alters the satisfiability of the instances in the set. This can be achieved\nby analyzing the properties of the mapping operation and proving that the satisfiability\nstatus is changed when the operation is applied.\n3. Contradiction: To show that the set is a fixed point under the mapping operation of chang-\ning satisfiability, we can assume the opposite, i.e., that the set is not a fixed point, and then\nderive a contradiction. This will involve analyzing the properties of the mapping operation\nand the instances in the set and showing that the assumption leads to an inconsistency.\n4. Asymptotic behavior: Since we need to prove the theorem for n and d tending to infinity, we should\nalso analyze the asymptotic behavior of the instances and the mapping operation. This will involve\nstudying the growth rates of the parameters and constraints and how they affect the satisfiability of\nthe instances.\nBy combining these thinking methods in a coherent and structured proof plan, we can\nestablish the existence of such an infinite set and show that it is a fixed point under the\nmapping operation of changing satisfiability.\nWhen the context\nis long, prompt the\nmodel to summarize\nor explain critical\nconditions or goals for\na better understanding.\n32\n24, 25, 30, 31*\nPrompt: Please explain the theorem to me, lay out a high-level proof plan, and then try\nvarious tactics to prove the theorem.\nTheorem: There exists an infinite set of satisfiable and unsatisfiable instances of Model RB\nsuch that this set is a fixed point under the mapping operation of changing satisfiability. (n\ntends to infinity, d tends to infinity.) High-level proof plan:\n25\n1. Existence of an infinite set of satisfiable and unsatisfiable instances: First, we need to\ndemonstrate the existence of an infinite set of instances of Model RB with alternating\nsatisfiability. This can be achieved by constructing a sequence of instances with different\nparameters and showing that the satisfiability of these instances alternates between satis-\nfiable and unsatisfiable.\n2. Mapping operation changes satisfiability: Next, we need to show that the mapping opera-\ntion of changing satisfiability indeed alters the satisfiability of the instances in the set. This\ncan be achieved by analyzing the properties of the mapping operation and proving that\nthe satisfiability status is changed when the operation is applied.\n3. Fixed point under the mapping operation : To show that the set is a fixed point under the\nmapping operation of changing satisfiability, we can assume the opposite, i.e., that the set\nis not a fixed point, and then derive a contradiction. This involves analyzing the properties\nof the mapping operation and the instances in the set and showing that the assumption\nleads to an inconsistency.\n4. Asymptotic behavior: Since we need to prove the theorem for n and d tending to infinity,\nwe should also analyze the asymptotic behavior of the instances and the mapping oper-\nation. This will involve studying the growth rates of the parameters and constraints and\nhow they affect the satisfiability of the instances.\nProof:\n\u2022 Step 1: Existence of an infinite set of satisfiable and unsatisfiable instances. Consider a\nsequence of instances of Model RB with increasing values of n and d. By adjusting the\nparameters of the model, such as the constant \u03b1, the probability p, and the number of\nconstraints m, we can create a sequence of instances with alternating satisfiability. As n\nand d tend to infinity, the sequence becomes infinite.\n\u2022 Step 2: Mapping operation changes satisfiability. We have already discussed how the map-\nping operation changes the satisfiability of an instance by altering the mapping function\nwhile keeping the parameter values and domains unchanged. By analyzing the proper-\nties of the mapping operation, we can show that it indeed changes the satisfiability of the\ninstances in the set.\n\u2022 Step 3: Fixed point under the mapping operation. Suppose the set is not a fixed point\nunder the mapping operation. Then, there exists an instance in the set whose satisfiability\nis not changed by the mapping operation. However, this contradicts the result from Step\n2, which states that the mapping operation changes the satisfiability of the instances in\nthe set. Thus, the set must be a fixed point under the mapping operation of changing\nsatisfiability.\n\u2022 Step 4: Asymptotic behavior. As n and d tend to infinity, the number of variables and the\ndomain size increase. By analyzing the growth rates of the parameters and constraints, we\ncan show how they affect the satisfiability of the instances. For example, as the domain\nsize increases, the probability of randomly selecting a satisfying assignment also increases.\nSimilarly, as the number of constraints increases, the likelihood of an unsatisfiable instance\nalso increases. By carefully choosing the parameters, we can create an infinite set of in-\nstances with alternating satisfiability.\nConclusion: We have shown that there exists an infinite set of satisfiable and unsatisfiable\ninstances of Model RB such that this set is a fixed point under the mapping operation of\n26\nchanging satisfiability as n and d tend to infinity. This is achieved by constructing a sequence\nof instances with alternating satisfiability, proving that the mapping operation changes sat-\nisfiability, showing that the set is a fixed point under the mapping operation, and analyzing\nthe asymptotic behavior of the instances and the mapping operation.\n33\n24, 25, 30, 31*\nPrompt: First, please clarify what kinds of instances are unsatisfiable and what kinds are\nsatisfiable.\nThen, as we only need to prove the existence, perhaps we can consider the\ninstances whose satisfiability can be changed easily, so as to simplify the proof. What kind\nof instances are most suitable?\nTo prove the theorem, we first need to understand the characteristics of the satisfiable and\nunsatisfiable instances of Model RB. We will then construct an infinite set of instances that can\nbe easily altered between satisfiability and unsatisfiability.\n1. Satisfiable instances: These instances have a solution that satisfies all the constraints in\nthe set C.\nIn other words, there exists an assignment \u03c3 such that for each constraint\nCi = (Xi, Ri), the tuple (\u03c3(xi1), \u03c3(xi2), . . . , \u03c3(xik)) \u2208 Ri.\n2. Unsatisfiable instances: These instances do not have any solution that satisfies all the\nconstraints in the set C. In other words, for every assignment \u03c3, there exists at least one\nconstraint Ci = (Xi, Ri) such that the tuple (\u03c3(xi1), \u03c3(xi2), . . . , \u03c3(xik)) /\u2208 Ri.\nNow, let\u2019s consider instances that can be easily altered between satisfiability and unsat-\nisfiability. We can focus on instances where only a single variable assignment needs to be\nchanged to switch between satisfiability and unsatisfiability. More specifically, we can con-\nstruct instances where only one constraint is \u201ccritical\u201d to the satisfiability of the instance, and\nthe remaining constraints are always satisfied.\nConsider a sequence of instances I1, I2, . . ., where each instance Ij has nj variables, dj\ndomain size, and mj constraints. We construct the instances such that nj, dj \u2192 \u221e as j \u2192 \u221e.\nFor each instance Ij, we construct the set of constraints Cj such that all but one constraint are \u201cnon-\ncritical\u201d and can always be satisfied, while the remaining constraint is \u201ccritical\u201d and its satisfiability\ndepends on a single variable assignment.\nTo ensure that these instances are fixed points under the mapping operation of changing satisfia-\nbility, we need to ensure that the satisfiability of each instance can be easily altered by applying the\nmapping operation on the \u201ccritical\u201d constraint. In other words, the mapping operation should trans-\nform the \u201ccritical\u201d constraint from satisfiable to unsatisfiable or vice versa, while keeping the other\nconstraints unchanged.\nLet Ij be an instance in the sequence, and let Cj,1 be the \u201ccritical\u201d constraint of the instance.\nWe can apply the mapping operation on Cj,1 to create a new instance I\u2032\nj with the altered\nsatisfiability. Since the mapping operation only modifies the \u201ccritical\u201d constraint, the remain-\ning constraints in I\u2032\nj will be unchanged, and their satisfiability will remain the same as in Ij.\nTherefore, by construction, the infinite set of instances {I1, I2, . . .} is a fixed point under the\nmapping operation of changing satisfiability.\nIn conclusion, we have shown the existence of an infinite set of satisfiable and unsatisfi-\nable instances of Model RB such that this set is a fixed point under the mapping operation of\nchanging satisfiability as n and d tend to infinity. We achieved this by constructing instances\nwhere only one constraint is \u201ccritical\u201d to the satisfiability of the instance, and the remain-\ning constraints are always satisfied. By applying the mapping operation on the \u201ccritical\u201d\nconstraint, we can easily alter the satisfiability of the instances while keeping the parameter\nvalues and domains unchanged.\n27\n34\n24, 25, 30, 31*, 33\nPrompt: Great! You have found instances that seem likely to be transformed from unsat-\nisfiable to satisfiable. So, what kind of instances are more likely to be transformed from\nsatisfiable to unsatisfiable?\nTo find instances that are more likely to be transformed from satisfiable to unsatisfiable, we can\nfocus on instances that have a very delicate balance between the constraints. These instances should\nhave a unique solution that satisfies all constraints, but a small change in any constraint could break\nthe satisfiability.\nConsider a sequence of instances J1, J2, . . ., where each instance Jj has nj variables, dj\ndomain size, and mj constraints. We construct the instances such that nj, dj \u2192 \u221e as j \u2192 \u221e.\nFor each instance Jj, we construct the set of constraints Cj such that all constraints are satisfied\nby a unique solution \u03c3j. However, the constraints are tightly coupled, meaning that even a\nsmall change in any of the constraints could make the instance unsatisfiable.\nLet Jj be an instance in the sequence, and let Cj,1 be one of the constraints in the instance.\nWe can apply the mapping operation on Cj,1 to create a new instance J\u2032j with the altered\nsatisfiability. Since the mapping operation modifies the constraint Cj, 1, it is likely to break\nthe delicate balance between the constraints in Jj and make the new instance J\u2032\nj unsatisfiable.\nTo ensure that these instances are more likely to be transformed from satisfiable to unsat-\nisfiable, we can focus on constructing instances with the following properties:\n1. The unique solution \u03c3j should have very specific variable assignments that make all con-\nstraints in Jj satisfied. Even a small change in any variable assignment should result in at\nleast one constraint being unsatisfied.\n2. The constraints should have a high degree of interdependence, meaning that satisfying one\nconstraint may require other constraints to be satisfied as well. This increases the likelihood\nthat a small change in any constraint will render the entire instance unsatisfiable.\n3. The constraints should have a narrow range of permitted tuples, making it more likely\nthat a small change in any constraint will result in an unsatisfiable instance.\nBy constructing instances with these properties, we increase the likelihood that the map-\nping operation will transform the instances from satisfiable to unsatisfiable. Thus, we have\nshown that there are instances that are more likely to be transformed from satisfiable to\nunsatisfiable under the mapping operation, while keeping parameter values and domains\nunchanged.\n35\n24, 25, 30, 31*, 33, 34\nPrompt: Great!\nPlease summarize the instances that we can focus on, be as concise as\npossible.\nWe can focus on two types of instances:\n1. From unsatisfiable to satisfiable: Instances with mostly non-critical constraints that are always\nsatisfied, and one critical constraint that depends on a single variable assignment. Changing this\nassignment alters the instance\u2019s satisfiability.\n2. From satisfiable to unsatisfiable: Instances with a delicate balance between constraints, having a\nunique solution. Modifying one critical constraint disrupts this balance and makes the instance\nunsatisfiable.\n28\n36\n24, 25, 30, 31*, 33, 34, 35\nPrompt: Great! Let\u2019s prove the theorem for the second type of instances. Please explain\nthe theorem to me, lay out a high-level proof plan, and then try various tactics to prove the\ntheorem.\nThe second type appears\nsimpler, hence we begin\nwith proving the second\ntype.\nTheorem: There exists an infinite set of instances of Model RB that can be transformed\nfrom satisfiable to unsatisfiable under the mapping operation while keeping the parameter\nvalues and domains unchanged.\nHigh-level proof plan:\n1. Construct an infinite sequence of instances I1, I2, . . . such that each instance Ij is satisfiable and has\na unique solution.\n2. Show that applying the mapping operation on one critical constraint in each instance Ij makes the\ninstance unsatisfiable.\n3. Demonstrate that the infinite set of instances is a fixed point under the mapping operation of chang-\ning satisfiability as n and d tend to infinity.\nProof:\n1. Constructing the infinite sequence of instances:\nConsider a sequence of instances I1, I2, . . ., where each instance Ij has nj variables, dj\ndomain size, and mj constraints. We construct the instances such that nj, dj \u2192 \u221e as j \u2192 \u221e.\nFor each instance Ij, we carefully construct the set of constraints Cj such that there exists\na unique solution that satisfies all constraints. This can be achieved by ensuring that the\nconstraints are tightly interconnected, and the variable assignments are coordinated to\ncreate a unique solution.\n2. Applying the mapping operation on a critical constraint:\nFor each instance Ij, select a critical constraint Cj,1 that is part of the unique solution.\nApply the mapping operation on this constraint to create a new instance I\u2032\nj. Since the\nmapping operation modifies the critical constraint, the delicate balance of the constraints\nis disrupted, and the unique solution of Ij no longer satisfies all constraints in I\u2032\nj. Therefore,\neach instance I\u2032\nj becomes unsatisfiable.\n3. Demonstrating the fixed point property:\nThe infinite set of instances {I1, I2, . . .} is a fixed point under the mapping operation of\nchanging satisfiability as n and d tend to infinity because the mapping operation can al-\nways be applied to the critical constraint in each instance to alter its satisfiability. When the\nmapping operation is applied to an instance in the sequence, its satisfiability changes from\nsatisfiable to unsatisfiable, and the parameter values and domains remain unchanged.\nIn conclusion, we have shown the existence of an infinite set of instances of Model RB\nthat can be transformed from satisfiable to unsatisfiable under the mapping operation while\nkeeping the parameter values and domains unchanged. We achieved this by constructing\ninstances with a delicate balance of constraints and a unique solution, and then applying the\nmapping operation on a critical constraint to disrupt this balance and make the instances\nunsatisfiable.\n29\n37\n24, 25, 36\nPrompt: : Denote Pr[I is SAT ] the probability that a random instance I of Model RB\nis satisfiable, then we have Theorem A: Let rcr =\n1\n\u2212 ln(1\u2212p). If \u03b1 > 1/k, 0 < p < 1\nare two constants and k, p satisfy the inequality k \u2265 1/(1 \u2212 p), then\nlim\nn\u2192\u221e Pr[I is SAT] = 1 if r < rcr,\nlim\nn\u2192\u221e Pr[I is SAT] = 0 if r > rcr.\nNow we want to prove: there exist infinitely many satisfiable instances of Model RB\nwith a unique solution when n and d tend to infinity. Please abstract the essence of\nthis proof problem and outline 10 possible ideas for the proof.\nWhen model knowl-\nedge is insufficient\nfor calculations, we\nintegrate the known\nproperties of Model RB\n(from related work Xu\nand Li (2000)) into\nprompts.\nTo ensure the integrity of\nsubsequent derivations,\nwe include the conditions\n\u03b1 > 1/k, 0 < p < 1\nand k \u2265 1/(1 \u2212 p)\nevery time we use the\ndefinition of Model RB in\nthe upcoming prompts.\nTo prove that there exist infinitely many satisfiable instances of Model RB with a unique\nsolution when n and d tend to infinity, we can consider the following ideas:\n1. Bounding the probability of multiple solutions: Analyze the probability of a random in-\nstance having multiple solutions, and show that this probability tends to zero as n and d\ntend to infinity, ensuring that most instances have a unique solution.\n2. Exploit the phase transition: Utilize the phase transition between satisfiable and unsatisfi-\nable instances in Model RB to identify a range of parameters where instances are likely to\nbe satisfiable with a unique solution.\n3. Constructing instances with a unique solution: Develop a method for constructing in-\nstances of Model RB with a unique solution, and show that this method can be applied to\ngenerate infinitely many instances as n and d tend to infinity.\n4. Analyzing the solution space geometry: Investigate the geometry of the solution space of\nModel RB instances, and show that the structure of the space tends to favor instances with\na unique solution as n and d grow without bound.\n5. Random walk-based argument: Use a random walk-based argument to show that, in the\nlimit as n and d tend to infinity, the probability of reaching a second solution from a given\nsolution becomes vanishingly small, implying the existence of a unique solution.\n6. Exploiting the symmetries of the problem: Leverage the symmetries in Model RB to argue\nthat there exist instances with a unique solution up to symmetries, and that these instances\nbecome more prevalent as n and d tend to infinity.\n7. Probabilistic method: Apply the probabilistic method to demonstrate that there exists a pos-\nitive probability for an instance of Model RB to have a unique solution as n and d tend to infinity,\nthus proving the existence of infinitely many such instances.\nThe constraint gener-\nation process in Model\nRB is independent, al-\nlowing the subsequent\nprobability analysis to\nmeet the requirements of\nindependence.\n8. Average-case analysis: Conduct an average-case analysis of Model RB instances, showing\nthat the expected number of solutions tends to one as n and d grow without bound, which\nimplies the existence of instances with a unique solution.\n9. Sparse instances: Investigate the behavior of sparse instances of Model RB (i.e., instances\nwith a low constraint density) and show that as n and d tend to infinity, such instances are\nmore likely to have a unique solution.\n10. Local search algorithms: Analyze the performance of local search algorithms on Model RB\ninstances and argue that the difficulty of finding multiple solutions increases as n and d\n30\ntend to infinity, suggesting that instances with a unique solution become more common in\nthe limit.\nLLMs focus on user\ndemands and instruc-\ntions. If a statement\nrequests a specific\nformat, the model may\ncontinue using it, even\nif not needed. To avoid\nconfusion, remove past\ninstructions from the\nconversation history.\n38\n24, 25, 37*\nPrompt: Now we want to prove: how to demonstrate that there exists a positive probability\nfor an instance of Model RB to have a unique solution as n and d tend to infinity.\nPlease abstract the essence of this proof problem and outline possible ideas for the proof.\nessence of this proof problem and outline 10 possible ideas for the proof.\nWe utilize the notation\n37* to indicate the\nincorporation of Theorem\nA into the prompt.\nTo prove that there exists a positive probability for an instance of Model RB to have a\nunique solution as n and d tend to infinity, we need to show that the limiting probability\nof finding a unique solution is greater than zero. We can do this by analyzing the behavior\nof the model as n and d increase and finding the conditions under which a unique solution\nexists.\nAbstracting the essence of the problem, we have:\n1. A random constraint satisfaction problem (CSP) model with variables, domains, con-\nstraints, and relations.\n2. A satisfiability condition for the model, dependent on the parameters n, d, k, p, and r.\n3. A theorem (Theorem A) giving the limiting probability of satisfiability under certain con-\nditions.\n4. A target: proving the existence of a positive probability for a unique solution as n and d\ntend to infinity.\nHere are possible ideas and steps for the proof:\n1. Study the behavior of the model: Analyze how the parameters n, d, k, p, and r affect the\ninstance\u2019s satisfiability and the number of solutions. This analysis may involve counting\narguments, combinatorial techniques, and probabilistic methods.\n2. Identify conditions for uniqueness: Determine the conditions on the parameters that lead\nto a unique solution. This may involve exploring the parameter space and finding where\nthe probability of having multiple solutions becomes negligible.\n3. Establish a lower bound: Prove a lower bound on the probability of having a unique solution un-\nder the identified conditions. This may involve using techniques from probability theory,\nsuch as the first or second moment methods, or applying concentration inequalities like\nChebyshev\u2019s inequality or Chernoff bounds.\n4. Relate the lower bound to Theorem A: Show that the lower bound on the probability of a\nunique solution is consistent with the conditions of Theorem A. This may involve proving\nthat the lower bound is positive when the conditions of Theorem A are satisfied.\n5. Conclude the proof: Demonstrate that the positive probability of a unique solution exists\nas n and d tend to infinity, completing the proof.\nIn summary, to prove the existence of a positive probability for a unique solution in the\nModel RB as n and d tend to infinity, we need to analyze the behavior of the model, identify condi-\ntions for uniqueness, establish a lower bound on the probability of a unique solution, relate the lower\n31\nbound to Theorem A, and conclude the proof. This will involve using various mathematical\ntechniques, including combinatorics, probability theory, and concentration inequalities.\n39\n24, 25, 37*\nPrompt: Problem: How to establish a lower bound on the probability of the conditions\nrequired for a unique solution. Please first abstract the essence of this problem and then\noutline possible ideas.\nEssence of the problem: The problem deals with the probability of satisfiability (having a\nunique solution) of a random constraint satisfaction problem instance (Model RB) based on\nvarious parameters such as the number of constraints (r), the domain size (d), and the size\nof symmetric relation (|R\u2217|). The conditions for a unique solution are given by Theorem A,\nwhich establishes a critical threshold (rcr) that separates the cases when the probability of\nsatisfiability tends to 1 (satisfiable) or 0 (unsatisfiable). The main goal is to establish a lower\nbound on the probability of the conditions required for a unique solution. Outline of possible\nideas:\n1. Analyze the structure of Model RB instances: Investigate the structural properties of Model\nRB instances and their dependencies on the parameters r, d, and |R\u2217|. This will help to\nidentify the characteristics that contribute to the satisfiability of the instances.\n2. Study the phase transition phenomenon: In random CSPs, there is often a phase transition\naround a critical threshold where the probability of satisfiability changes abruptly. Analyze\nthis phase transition in Model RB and its relation to the critical threshold rcr given by\nTheorem A.\n3. Probabilistic analysis: Perform a probabilistic analysis to calculate the lower bound on the prob-\nability of the conditions required for a unique solution. This can be done using techniques such as\nthe first and second moment methods, which involve computing the expected number of solutions\nand the variance of the number of solutions, respectively.\n4. Explore connections with other CSP models and problems: Investigate how the results or\ntechniques used for similar CSP models or NP-complete problems (e.g., SAT, CLIQUE) can\nbe adapted or applied to Model RB to establish the lower bound on the probability of a\nunique solution.\n5. Investigate the effect of different parameters: Analyze the impact of varying the param-\neters r, d, and |R\u2217| on the probability of a unique solution. This analysis can help in\nunderstanding the interplay between these parameters and their role in determining the\nsatisfiability of Model RB instances.\n6. Use computational methods: Employ computational methods such as simulations or solv-\ning instances of Model RB with different parameters using constraint solvers to gather\nempirical evidence supporting the lower bound on the probability of a unique solution.\n40\n24, 25, 37*, 39\nPrompt: Great! Can you give me some possible ideas?\nCertainly! Here are some possible ideas to establish the lower bound on the probability of\nthe conditions required for a unique solution in Model RB:\n1. First Moment Method: Compute the expected number of solutions, E[S], where S is a\nrandom variable representing the number of solutions in a random instance of Model RB.\n32\nIf E[S] is small, it indicates that the probability of having a unique solution is high. To do\nthis, you can use linearity of expectation and indicator random variables for each potential\nsolution.\n2. Second Moment Method: Compute the variance of the number of solutions, Var(S), to\nobtain information about the distribution of solutions. If the variance is small, it implies\nthat most instances have a number of solutions close to the expected value, E[S]. You can\ncombine this information with the first moment method to get a better understanding of\nthe probability of a unique solution.\n3. Bounds on Satisfiability: Establish upper and lower bounds on the probability that a random\ninstance of Model RB is satisfiable. This can help in determining the threshold of the parameter r\n(constraint density) that separates the regions of satisfiability and unsatisfiability, as mentioned in\nTheorem A. Understanding this threshold can give insights into the likelihood of having a unique\nsolution.\n4. Phase Transition Analysis: Investigate the behavior of Model RB around the critical value\nof r (rcr) where the probability of satisfiability changes abruptly. Phase transition analysis\ncan provide information about how the probability of having a unique solution changes\nwith different values of r and other parameters, such as k and p.\n5. Probabilistic Inequalities: Employ probabilistic inequalities, such as Chebyshev\u2019s or Cher-\nnoff\u2019s inequalities, to obtain bounds on the probability of a unique solution. These in-\nequalities can be applied to the random variables related to the number of solutions and\ntheir distribution.\n6. Exploit Problem Structure: Analyze the structure of Model RB, such as its graph represen-\ntation or properties of its constraints, to identify conditions that promote a unique solution.\nFor example, you can study how the choice of k, p, and the relation R\u2217 affect the likelihood\nof a unique solution in Model RB.\nBy exploring these ideas and combining their results, you can establish a lower bound on\nthe probability of the conditions required for a unique solution in Model RB instances.\n41\n24, 25, 37*, 39, 40\nPrompt: Great! How to establish the the upper bound?\nTo establish the upper bound on the probability that a random instance of Model RB is\nsatisfiable, we can use the first moment method. Here\u2019s how:\n1. Compute the expected number of solutions: Calculate the expected number of solutions\nE(S) for a random instance of Model RB by taking the product of the probability that each\nconstraint is satisfied by a random assignment. To do this, consider the probability of a sin-\ngle constraint being satisfied and raise it to the power of the total number of constraints,\nm.\n2. Use Markov\u2019s inequality: Apply Markov\u2019s inequality to bound the probability that a ran-\ndom instance has at least one solution. Markov\u2019s inequality states that for any non-negative\nrandom variable X and a > 0, Pr(X \u2265 a) \u2264 E(X)\na\n. In this case, X represents the number of\nsolutions and a = 1.\n3. Upper bound: By combining the calculation of E(S) and Markov\u2019s inequality, we can estab-\nlish an upper bound on the probability that a random instance of Model RB is satisfiable.\n33\nIf E(S) is small, then the probability of having at least one solution is also small, indicating\nan upper bound on the satisfiability probability.\nThis upper bound will depend on the values of the parameters \u03b1, p, k, and r. By analyz-\ning the bound, we can gain insights into the behavior of the satisfiability probability as the\nparameters vary and help identify the threshold values for r that separate the satisfiable and\nunsatisfiable regions.\n42\n24, 25, 37*, 39, 40, 41\nPrompt: Great! Let\u2019s calculate the upper bound.\nTo calculate the upper bound on the probability that a random instance of Model RB is\nsatisfiable, we first compute the expected number of solutions E(S) and then apply Markov\u2019s\ninequality.\n1. Compute the expected number of solutions E(S):\nFor a single constraint Ci with k variables, the probability of being satisfied by a random\nassignment is |R\u2217|\ndk\n= (1\u2212p)dk\ndk\n= 1 \u2212 p. Since there are m constraints, and we assume they\nare independent, the probability that all of them are satisfied is (1 \u2212 p)m.\nNow, there are dn = n\u03b1n possible assignments for the variables in the instance. So, the\nexpected number of solutions, E(S), is the product of the number of possible assignments\nand the probability that each assignment satisfies all constraints:\nE(S) = n\u03b1n(1 \u2212 p)m\n2. Apply Markov\u2019s inequality:\nLet X be the random variable representing the number of solutions. We want to bound\nthe probability that X \u2265 1, i.e., the probability that there exists at least one solution. Using\nMarkov\u2019s inequality, we have:\nPr[X \u2265 1] \u2264 E(S)\n1\n3. Upper bound:\nBy substituting the expression for E(S) into the inequality, we get an upper bound on the\nprobability that a random instance of Model RB is satisfiable:\nPr[I is SAT] \u2264 n\u03b1n(1 \u2212 p)m\nTo determine the threshold value for r that separates satisfiable and unsatisfiable regions,\nwe can analyze the behavior of this upper bound as the parameters \u03b1, p, k, and r vary.\nIn particular, when the upper bound is small, it indicates that the probability of having a\nsatisfiable instance is small, helping us identify the critical value rcr mentioned in Theorem\nA.\n43\n24, 25, 37*, 39, 40, 41, 42\nPrompt: We are aiming to construct a challenging example, so it is sufficient to ensure that\nthe probability of I having a solution and the probability of I being unsatisfiable are both\npositive.\nPlease set a specific value for n\u03b1n(1 \u2212 p)m and specify the parameter selection\nrequirements accordingly during the construction process.\n34\nLet\u2019s choose a specific value for n\u03b1n(1 \u2212 p)m to ensure that the probability of I having a\nsolution and the probability of I being unsatisfiable are both positive. For example, we can\nset:\nn\u03b1n(1 \u2212 p)m = 1\n2\nThis means that the probability of a random instance I being satisfiable is less than or\nequal to 1/2, and the probability of it being unsatisfiable is greater than or equal to 1/2. This\nchoice balances the probabilities and makes the problem challenging.\nNow, we need to specify the parameter selection requirements during the construction\nprocess to satisfy this condition:\n1. Choose a suitable value for \u03b1, such that \u03b1 > 0. This will affect the domain size d and the\nnumber of possible assignments for the variables in the instance.\n2. Choose a constant p, where 0 < p < 1. This represents the fraction of tuples not included\nin the permitted sets and determines the difficulty of constraints.\n3. Select a constant k \u2265 2, the arity of the constraints. This choice will impact the complexity\nof the constraints and the overall problem.\n4. Determine the constraint density parameter r. Ensure that the value of r is chosen such\nthat it relates to rcr, the critical value mentioned in Theorem A. This will help in creating\ninstances that are challenging to solve.\nTo satisfy the equation n\u03b1n(1 \u2212 p)m = 1\n2, tune the values of \u03b1, p, k, and r during the con-\nstruction process. Keep in mind that the chosen parameters should abide by the conditions\nmentioned in Theorem A and should not violate any constraints imposed by the Model RB\ndefinition.\nIn summary, carefully selecting the values of \u03b1, p, k, and r during the construction process,\naccording to the equation, will ensure that the probability of the instance having a solution\nand the probability of it being unsatisfiable are both positive, making the problem challeng-\ning.\n44\n24, 25, 37*, 39, 40, 41, 42, 43\nPrompt: Great!\nConsidering Theorem A, which kind of r will guarantee the inequality\nn\u03b1n(1 \u2212 p)m = 1\n2 holds true?\nTo ensure that n\u03b1n(1 \u2212 p)m = 1\n2 holds true, we need to select an appropriate value for the\nconstraint density parameter r. Based on Theorem A, we know that:\n\u2022 If r < rcr, the probability of I being satisfiable tends to 1 as n \u2192 \u221e.\n\u2022 If r > rcr, the probability of I being satisfiable tends to 0 as n \u2192 \u221e.\nIn our case, we want the probability of I being satisfiable to be less than or equal to 1/2.\nTo achieve this, we should choose r slightly greater than rcr.\nRecall that rcr =\n1\n\u2212 ln(1\u2212p). Given the constant p, we can calculate rcr and then select r \u2248 rcr + \u03f5,\nwhere \u03f5 > 0 is a small constant.\nBy choosing r in this manner, we can expect that the probability of a random instance I\nbeing satisfiable will be close to 1/2, satisfying the condition n\u03b1n(1 \u2212 p)m = 1\n2.\n35\nKeep in mind that the actual probability of I being satisfiable will depend on the specific\nvalues of \u03b1, p, k, and r. To ensure that the equation holds true, you might need to perform\nsome numerical analysis and fine-tune the parameters accordingly.\n45\n24, 25, 37*, 39, 40, 41, 42, 43, 44\nPrompt: Great! What kind of \u03f5 guarantees that n\u03b1n(1 \u2212 p)m = 1\n2? Please calculate a proper\n\u03f5 (represent it with other variables).\nTo find a suitable \u03f5 that guarantees n\u03b1n(1 \u2212 p)m = 1\n2, we need to analyze the effect of\nchanging r on the equation while considering the other parameters as well. Using the given\nvalues of \u03b1, p, and k, we can express m as m = rn ln d = rn ln(n\u03b1), which can be further\nsimplified to m = r\u03b1n2 ln n. Now, let\u2019s rewrite the equation as:\nn\u03b1n(1 \u2212 p)r\u03b1n2 ln n = 1\n2\nTaking the natural logarithm of both sides, we get:\n\u03b1n ln n + r\u03b1n2 ln n ln(1 \u2212 p) = ln 1\n2\nNow, we want to find the change in r, denoted as \u03f5, such that the equation holds true. To\ndo this, we can take the partial derivative of the left-hand side of the equation with respect\nto r:\n\u2202\n\u2202r(\u03b1n ln n + r\u03b1n2 ln n ln(1 \u2212 p)) = \u03b1n2 ln n ln(1 \u2212 p)\nWe can approximate the change in the left-hand side of the equation caused by an increase\nin r by the product of the partial derivative and \u03f5:\n\u2206(LHS) \u2248 \u03f5\u03b1n2 ln n ln(1 \u2212 p)\nNow, we need to find an \u03f5 such that the change in the left-hand side of the equation is\nequal to the change in the right-hand side of the equation, which is ln 1\n2:\n\u03f5\u03b1n2 ln n ln(1 \u2212 p) = ln 1\n2\nWe can solve for \u03f5:\n\u03f5 =\nln 1\n2\n\u03b1n2 ln n ln(1 \u2212 p)\nThis expression represents a suitable \u03f5 value in terms of the other variables that should\nguarantee that n\u03b1n(1 \u2212 p)m = 1\n2. However, keep in mind that this is an approximation, and\nyou might need to fine-tune the value of \u03f5 depending on the specific values of the other\nparameters and the behavior of the equation.\n46\n24, 25, 37*, 39, 40, 41, 42, 43, 44, 45\nPrompt: Great! Please check your calculation process and refine any possible mistakes.\nFor conciseness, we\nmaintain the condition\nr = rcr + \u03f5, \u03f5 =\nln 1\n2\n\u03b1n ln n ln(1\u2212p) by default\nwhen constructing Model\nRB instances in our\nsubsequent discussions.\nApologies for the confusion in my previous response. I made a mistake in the calculation\nprocess. Let\u2019s correct the expression for m and re-derive the equation for \u03f5.\nFirst, we recall that m = rn ln d = rn ln(n\u03b1). Now, we can simplify this expression as\nm = r\u03b1n ln n. Next, let\u2019s rewrite the equation as:\n36\nn\u03b1n(1 \u2212 p)r\u03b1n ln n = 1\n2\nTaking the natural logarithm of both sides, we get:\n\u03b1n ln n + r\u03b1n ln n ln(1 \u2212 p) = ln 1\n2\nNow, we want to find the change in r, denoted as \u03f5, such that the equation holds true. To\ndo this, we can take the partial derivative of the left-hand side of the equation with respect\nto r:\n\u2202\n\u2202r(\u03b1n ln n + r\u03b1n ln n ln(1 \u2212 p)) = \u03b1n ln n ln(1 \u2212 p)\nWe can approximate the change in the left-hand side of the equation caused by an increase\nin r by the product of the partial derivative and \u03f5:\n\u2206(LHS) \u2248 \u03f5\u03b1n ln n ln(1 \u2212 p)\nNow, we need to find an \u03f5 such that the change in the left-hand side of the equation is\nequal to the change in the right-hand side of the equation, which is ln 1\n2:\n\u03f5\u03b1n ln n ln(1 \u2212 p) = ln 1\n2\nWe can solve for \u03f5:\n\u03f5 =\nln 1\n2\n\u03b1n ln n ln(1 \u2212 p)\nThis expression represents a suitable \u03f5 value in terms of the other variables that should\nguarantee that n\u03b1n(1 \u2212 p)m = 1\n2. However, keep in mind that this is an approximation, and\nyou might need to fine-tune the value of \u03f5 depending on the specific values of the other\nparameters and the behavior of the equation.\nWhen the LLM falls\nshort for computations,\nwe incorporate the\nestablished equations\nof Model RB solu-\ntions from previous\nwork (Xu and Li, 2000)\ninto the prompts.\n47\n24, 25, 37*, 39, 40, 43\nPrompt: Let X denote the number of solutions of an instance of Model RB, then we have:\nE[X2] =\nn\n\u2211\nS=0\ndn\n\u0012n\nS\n\u0013\n(d \u2212 1)n\u2212S\n \n(1 \u2212 p) (S\nk)\n(n\nk) + (1 \u2212 p)2\n \n1 \u2212 (S\nk)\n(n\nk)\n!!rn ln d\n,\nwhere S = ns is the number of variables for which an assignment pair take the same values.\nWe have also known:\nE\nh\nX2i\n= E[X]2\n\u0012\n1 + O\n\u0012 1\nn\n\u0013\u0013\nn\n\u2211\nS=0\nF(S),\nwhere F(S) = (n\nS)\n\u0010\n1 \u2212 1\nd\n\u0011n\u2212S \u0010\n1\nd\n\u0011S h\n1 +\np\n1\u2212p skirn ln d\n, and S = ns is the number of variables\nfor which an assignment pair take the same values. Notice that \u03b1 > 1, according to previous\nwork, we know that all the elements are negligible except when S=0 and S=n.\nWe want to calculate the lower bound when n and d tend to infinity, can you lay out a plan?\nThe condition for all other\nelements to be negligible\nis 1 + \u03b1(1 \u2212 rcrpk) < 0\nand \u03b1 > 1. For the\nsake of conciseness,\nwe will maintain this\ncondition by default when\nconstructing Model RB\ninstances henceforth.\nTo calculate the lower bound when n and d tend to infinity, we can follow these steps:\n1. Simplify the expression for E[X2] by considering only the terms when S = 0 and S = n.\nSince all other terms are negligible, we have:\nE[X2] \u2248 E[X]2\n\u0012\n1 + O\n\u0012 1\nn\n\u0013\u0013\n(F(0) + F(n)).\n37\n2. Compute the asymptotic expressions for F(0) and F(n) as n, d \u2192 \u221e:\nFor F(0), we have:\nF(0) =\n\u0012n\n0\n\u0013 \u0012\n1 \u2212 1\nd\n\u0013n \u00121\nd\n\u00130 \u0014\n1 +\np\n1 \u2212 p(0)k\n\u0015rn ln d\n=\n\u0012\n1 \u2212 1\nd\n\u0013n\n.\nAs d \u2192 \u221e, we have limd\u2192\u221e F(0) = 1.\nFor F(n), we have:\nF(n) =\n\u0012n\nn\n\u0013 \u0012\n1 \u2212 1\nd\n\u00130 \u00121\nd\n\u0013n \u0014\n1 +\np\n1 \u2212 p(n)k\n\u0015rn ln d\n= 1\ndn\n\u0014\n1 +\np\n1 \u2212 pnk\n\u0015rn ln d\n.\nWe need to analyze the behavior of\n1\ndn and\nh\n1 +\np\n1\u2212pnkirn ln d\nas n, d \u2192 \u221e. This might re-\nquire advanced mathematical techniques such as using L\u2019H\u00f4pital\u2019s rule, series expansions,\nor other asymptotic analysis methods.\n3. Substitute the asymptotic expressions for F(0) and F(n) back into the expression for E[X2]:\nE[X2] \u2248 E[X]2\n\u0012\n1 + O\n\u0012 1\nn\n\u0013\u0013\n(F(0) + F(n)).\n4. Compute the lower bound by taking the limit as n, d \u2192 \u221e:\nlim\nn,d\u2192\u221e Pr[I is SAT] \u2265\nlim\nn,d\u2192\u221e\nE[X]2\nE[X2].\nBy analyzing the asymptotic behavior of the terms in the expression for E[X2], we can\ncompute the lower bound as n and d tend to infinity. This analysis will provide insight\ninto the behavior of the probability of satisfiability for large instances of Model RB and\nhelp understand the phase transition phenomenon.\n48\n24, 25, 37*, 39, 40, 43, 47\nPrompt: Great! But can you check the correctness of your F(n)?\nYou are correct. There was an error in my calculation for F(n). Let me correct the expres-\nsion for F(n):\nF(n) =\n\u0012n\nn\n\u0013 \u0012\n1 \u2212 1\nd\n\u00130 \u00121\nd\n\u0013n \u0014\n1 +\np\n1 \u2212 p(1)k\n\u0015rn ln d\n= 1\ndn\n\u0014\n1 +\np\n1 \u2212 p\n\u0015rn ln d\nNow, we can continue with the plan to calculate the lower bound as n and d tend to infinity,\nby analyzing the behavior of\n1\ndn and\nh\n1 +\np\n1\u2212p\nirn ln d\nas n, d \u2192 \u221e. This analysis will provide\ninsight into the behavior of the probability of satisfiability for large instances of Model RB\nand help understand the phase transition phenomenon.\n49\n24, 25, 37*, 39, 40, 43, 47, 48\nPrompt: Great! Please don\u2019t forget that our construction makes the instances have the fol-\nlowing property: E[X] = n\u03b1n(1 \u2212 p)m = 1\n2. Now let\u2019s use the corrected F(n), simplify it and\ncalculate the final lower bound on the probability that a random instance of Model RB is\nsatisfiable.\nGiven the property E[X] = n\u03b1n(1 \u2212 p)m = 1\n2, we can simplify the expression for E[X2] by\nconsidering only the terms when S = 0 and S = n:\n38\nE[X2] \u2248 E[X]2\n\u0012\n1 + O\n\u0012 1\nn\n\u0013\u0013\n(F(0) + F(n)).\nNow, we have the corrected expressions for F(0) and F(n):\n1. F(0) = limd\u2192\u221e\n\u0010\n1 \u2212 1\nd\n\u0011n\n= 1.\n2. F(n) =\n1\ndn\nh\n1 +\np\n1\u2212p\nirn ln d\n.\nTo simplify F(n), let\u2019s analyze the behavior of\n1\ndn and\nh\n1 +\np\n1\u2212p\nirn ln d\nas n, d \u2192 \u221e.\nFirst, recall that d = n\u03b1. Then, we have:\n1\ndn =\n1\n(n\u03b1)n = n\u2212\u03b1n.\nNext, notice that\nh\n1 +\np\n1\u2212p\nirn ln d\nis an exponential function of n. To simplify this term, we\ncan use the property E[X] = 1\n2. From this property, we know that:\nn\u03b1n(1 \u2212 p)m = 1\n2 =\u21d2 (1 \u2212 p)rn ln d =\n1\n2n\u03b1n .\nNow, we can rewrite the term\nh\n1 +\np\n1\u2212p\nirn ln d\nas:\n\u0014\n1 +\np\n1 \u2212 p\n\u0015rn ln d\n=\n\u00121 \u2212 p + p\n1 \u2212 p\n\u0013rn ln d\n=\n\u0012\n1\n1 \u2212 p\n\u0013rn ln d\n=\n1\n(1 \u2212 p)rn ln d .\nSince (1 \u2212 p)rn ln d =\n1\n2n\u03b1n , we have:\n\u0014\n1 +\np\n1 \u2212 p\n\u0015rn ln d\n= 2n\u03b1n.\nFinally, we can calculate F(n) as:\nF(n) = n\u2212\u03b1n \u00b7 2n\u03b1n = 2.\nNow, we can substitute the simplified expressions for F(0) and F(n) back into the expres-\nsion for E[X2]:\nE[X2] \u2248 E[X]2\n\u0012\n1 + O\n\u0012 1\nn\n\u0013\u0013\n(1 + 2) = 3E[X]2.\nTo compute the lower bound, we have:\nlim\nn,d\u2192\u221e Pr[I is SAT] \u2265\nlim\nn,d\u2192\u221e\nE[X]2\nE[X2] =\nlim\nn,d\u2192\u221e\nE[X]2\n3E[X]2 = 1\n3\nTherefore, the lower bound on the probability that a random instance of Model RB is\nsatisfiable is 1\n3 when n and d tend to infinity.\nWhen dealing with\ncomplex mathematical\nproblems, the capacity\nof LLMs to understand\nextensive context might\nbe inadequate. There-\nfore, it is crucial to\ndecompose the prob-\nlem and filter out the\nrelevant information.\n50\n24, 25, 37, 43, 49\nPrompt: Now let\u2019s calculate the probability that a random instance of Model RB has excatly\none solution.\nLet Ns denote the number of solutions of a random instance of Model RB. We want to find\nthe probability that Ns = 1. We know that the expectation of the number of solutions is 1\n2,\n39\nand the lower and upper bounds on the probability that a random instance is satisfiable are\n1\n3 and 1\n2, respectively.\nLet P(Ns = 1) be the probability that a random instance has exactly one solution, P(Ns =\n0) be the probability that a random instance has no solution, and P(Ns \u2265 2) be the probability\nthat a random instance has at least two solutions.\nWe know that P(Ns \u2265 0) = 1, since a random instance can either have no solution, exactly\none solution, or at least two solutions. We also know that the probability of a random instance\nbeing satisfiable is the sum of the probabilities of having exactly one solution and having at\nleast two solutions, which can be expressed as P(Ns = 1) + P(Ns \u2265 2) \u2265 1\n3 (the lower bound\non satisfiability).\nNow, let\u2019s use the expectation of the number of solutions to find the probability of having\nexactly one solution. The expectation can be written as:\nE[Ns] = 0 \u00b7 P(Ns = 0) + 1 \u00b7 P(Ns = 1) +\n\u221e\n\u2211\nk=2\nk \u00b7 P(Ns = k)\nSince we know that E[Ns] = 1\n2, and that P(Ns \u2265 2) = 1 \u2212 P(Ns = 0) \u2212 P(Ns = 1), we can\nrewrite the expectation as:\n1\n2 = P(Ns = 1) +\n\u221e\n\u2211\nk=2\nk \u00b7 P(Ns = k)\nWe can\u2019t directly solve for P(Ns = 1) from this equation, but we can get an estimate of its\nrange using the bounds on the probability of satisfiability.\nWe know that P(Ns = 1) + P(Ns \u2265 2) \u2265 1\n3, so we have:\nP(Ns = 1) \u2265 1\n3 \u2212 P(Ns \u2265 2)\nSince P(Ns \u2265 2) \u2264 1\n2 \u2212 P(Ns = 1) (the upper bound on satisfiability minus the probability\nof having exactly one solution), we have:\nP(Ns = 1) \u2265 1\n3 \u2212\n\u00121\n2 \u2212 P(Ns = 1)\n\u0013\nSolving this inequality, we get:\nP(Ns = 1) \u2265 1\n6\nSo, we have a lower bound on the probability that a random instance of Model RB has\nexactly one solution, which is 1\n6. However, without more information or a more specific\ndistribution on the number of solutions, we cannot determine an exact probability or an\nupper bound.\n51\n24, 25, 43, 50\nPrompt: Does this prove the existence of an infinite set of unsatisfiable Model RB instances\nand satisfiable Model RB instances with exactly one solution?\nYes, this does prove the existence of an infinite set of unsatisfiable Model RB instances and\nsatisfiable Model RB instances with exactly one solution. Here\u2019s why:\n1. First, let\u2019s consider unsatisfiable instances. Since the upper bound on the probability of a\nrandom instance having exactly one solution is 1\n2, the probability that a random instance\nhas no solution is at least 1\n2 (as it may have more than one solution as well). Since there are\ninfinitely many random instances, there must be infinitely many unsatisfiable instances.\n40\n2. Now let\u2019s consider instances with exactly one solution. We have a lower bound on the\nprobability that a random instance of Model RB has exactly one solution, which is 1\n6. This\nmeans that for a large enough number of instances, we can expect at least 1\n6 of them to\nhave exactly one solution. Since there are infinitely many random instances, there must be\ninfinitely many instances with exactly one solution.\nTherefore, there exists an infinite set of both unsatisfiable Model RB instances and satisfi-\nable Model RB instances with exactly one solution.\n52\n24, 25, 30\nPrompt: Now we focus on Model RB instances with exactly one solution or with only one\nunsatisfied constraint, and we\u2019re designing a new mapping operation to change the satisfi-\nability of such instances. I notice that the current mapping operation with bijections seems\nsomewhat redundant because for most tuples, no operation is actually required. In real-\nity, we only select two groups of variables and swap their values. Can our new mapping\noperation be defined only on the tuples that need to be modified?\nIt is beneficial to enable\nLLMs to backtrack,\nadjust previous defini-\ntions or conditions, and\ncorrect earlier errors.\nThis approach resem-\nbles human thought\nprocesses and can yield\nmore refined outcomes.\nYes, the new mapping operation can focus only on the tuples that need to be modified. To\nachieve this, we can perform the following steps:\n1. Identify the constraint Cu = (Xu, Ru) that is either unsatisfied (if the instance is unsatisfi-\nable) or can be unsatisfied by slightly modifying the solution (if the instance has exactly\none solution).\n2. For the constraint Cu, find two tuples (a1, a2) and (b1, b2) \u2208 Ru such that they do not appear\ntogether in the solution or other constraints, and they satisfy the following conditions:\n\u2022 (a1, b2) /\u2208 Ru and (b1, a2) /\u2208 Ru.\n\u2022 Swapping the values of a2 and b2 in the solution will not violate other constraints in the\ninstance.\n3. Modify the permitted set Ru by removing (a1, a2) and (b1, b2), and adding (a1, b2) and\n(b1, a2). Let the new permitted set be R\u2032\nu.\n4. Replace the original permitted set Ru with the modified permitted set R\u2032\nu for the constraint\nCu = (Xu, Ru). This results in a new constraint C\u2032\nu = (Xu, R\u2032\nu).\n5. The new instance of Model RB will have the same set of variables, domains, and con-\nstraints, but with the modified permitted set R\u2032\nu for the constraint C\u2032\nu.\nThis mapping operation changes the satisfiability of the instance by focusing only on the\ntuples that need to be modified. If the instance had exactly one solution, this operation creates\nan unsatisfied constraint, making the instance unsatisfiable. If the instance was unsatisfiable,\nthis operation removes the unsatisfied constraint, creating a new satisfiable instance.\n41\n53\n24, 25, 52\nPrompt: Great! But I have some minor suggestions: 1. If the instance has a unique solution,\nany constraint can be unsatisfied by slightly modifying the solution. Therefore, the phrase\n\u2018can be unsatisfied by slightly modifying the solution (if the instance has exactly one solu-\ntion)\u2019 in the current definition is redundant. 2. Since our mapping operation only considers\nspecific constraints, there is no need to consider other constraints. The statements regarding\nthe appearance of tuples together and the preservation of other constraints are unnecessary.\n3. Steps 3, 4, and 5 can be integrated to be more concise. Please update the definition of the\nmapping operation to make it more concise.\nThank you for the suggestions. Based on your feedback, I\u2019ve updated the definition of the\nmapping operation as follows:\n1. Identify the constraint Cu = (Xu, Ru) that needs to be modified. If the instance is unsatisfiable,\nchoose the unsatisfied constraint. If the instance has exactly one solution, choose any constraint.\n2. For the constraint Cu, find two tuples (a1, a2) and (b1, b2) \u2208 Ru such that (a1, b2) /\u2208 Ru and\n(b1, a2) /\u2208 Ru.\n3. Modify the constraint Cu = (Xu, Ru) by replacing the permitted set Ru with a new permitted set\nR\u2032\nu obtained by removing (a1, a2) and (b1, b2), and adding (a1, b2) and (b1, a2).\nThe new instance of Model RB will have the same set of variables, domains, and constraints, but\nwith the modified permitted set R\u2032\nu for the constraint C\u2032\nu.\nThis concise mapping operation changes the satisfiability of the instance by focusing only\non the tuples that need to be modified. If the instance had exactly one solution, this op-\neration creates an unsatisfied constraint, making the instance unsatisfiable. If the instance\nwas unsatisfiable, this operation removes the unsatisfied constraint, creating a new satisfiable\ninstance.\n54\n24, 25, 53\nPrompt: We have proved that there exists an infinite set of satisfiable instances of Model RB\nwith exactly one solution, how to prove that any instance in this set can be transformed to\nunsatisfiable instance under the mapping operation? (n tends to infinity, d tends to infinity.)\nTo prove that any instance in the infinite set of satisfiable instances of Model RB with\nexactly one solution can be transformed into an unsatisfiable instance under the mapping\noperation, we need to show that the mapping operation can eliminate the unique solution of\nthe instance without introducing new solutions.\nLet I be an instance from the infinite set of satisfiable instances with exactly one solution, and let\n\u03c3 be the unique solution of I. Let Cu = (Xu, Ru) be the constraint chosen in the mapping operation\n(either unsatisfied or any constraint if the instance has exactly one solution).\nNow, consider the tuples (a1, a2) and (b1, b2) \u2208 Ru used in the mapping operation, with (a1, b2) /\u2208 Ru\nand (b1, a2) /\u2208 Ru. Since \u03c3 is the unique solution for the instance I, we must have either \u03c3(xu1) = a1\nand \u03c3(xu2) = a2, or \u03c3(xu1) = b1 and \u03c3(xu2) = b2.\nAfter applying the mapping operation, the constraint Cu is replaced by C\u2032\nu = (Xu, R\u2032\nu), where the\npermitted set R\u2032\nu is obtained by removing (a1, a2) and (b1, b2) from Ru and adding (a1, b2) and\n(b1, a2). As a result, both (a1, a2) and (b1, b2) are not in R\u2032\nu.\nSince the unique solution \u03c3 must satisfy either (a1, a2) \u2208 Ru or (b1, b2) \u2208 Ru, it cannot satisfy the\nmodified constraint C\u2032\nu. Therefore, \u03c3 is not a solution for the modified instance I\u2032. Moreover, no other\nassignments satisfy all constraints, as we only changed Cu without introducing new solutions.\n42\nHence, the instance I\u2032 obtained after the mapping operation is unsatisfiable. This proves that any\ninstance in the infinite set of satisfiable instances of Model RB with exactly one solution can be trans-\nformed into an unsatisfiable instance under the mapping operation, as the number of variables (n) and\nthe domain size (d) tend to infinity.\n55\n24, 25, 53, 54\nPrompt: But is it possible for tuples like (a1, \u2217) or (b1, \u2217) (* denotes other possible domain\nvalues for the second variable) in the permitted set to lead to new solutions? Please rigor-\nously calculate the expected number of such tuples and their probability of being expanded\nto new solutions.\nLLMs may struggle\nwith self-evaluating\nthe accuracy of their\nproofs, often concen-\ntrating on formalities\nlike symbol usage.\nBy iteratively asking\nquestions such as\n\u2018why\u2019, \u2018how\u2019, or \u2018is\nit...\u2019, the model can\nbe prompted to check\nfor inaccuracies and\nmake corrections. This\napproach serves as\nan effective method\nfor enhancing LLM\nself-evaluation.\nTo calculate the expected number of tuples (a1, \u2217) and (b1, \u2217) in the permitted set, we need\nto analyze the probability of having such tuples in the permitted set and the possibility of\nthem expanding to new solutions.\nLet I be an instance from the infinite set of satisfiable instances of Model RB with exactly\none solution, and Cu = (Xu, Ru) be the constraint chosen in the mapping operation. Recall\nthat |Ru| = (1 \u2212 p)dk. Since the constraint is symmetric, the probability of a specific tuple\n(a1, a2) being in Ru is (1 \u2212 p).\nNow, let\u2019s consider the tuples of the form (a1, \u2217) and (b1, \u2217), where \u2217 can be any value\nfrom the domain D\u2217 = {1, 2, . . . , d}. There are d possible tuples for each form, giving a total\nof 2d tuples. Since the tuples are picked independently and the probability of a specific tuple\nbeing in Ru is (1 \u2212 p), the expected number of tuples of the form (a1, \u2217) and (b1, \u2217) in Ru is\n2d(1 \u2212 p).\nNow, let\u2019s analyze the probability of these tuples expanding to new solutions in the modi-\nfied instance I\u2032. When we apply the mapping operation, we replace (a1, a2) and (b1, b2) with\n(a1, b2) and (b1, a2) in the permitted set R\u2032\nu, while all other tuples remain unchanged.\nSince the instance I has exactly one solution, the tuples of the form (a1, \u2217) and (b1, \u2217) in\nRu are not part of any other solution. After the mapping operation, the tuples (a1, b2) and\n(b1, a2) are added to the permitted set, but we know that (a1, b2) /\u2208 Ru and (b1, a2) /\u2208 Ru.\nTherefore, these tuples were not part of the unique solution in the original instance I.\nThus, the probability of these tuples expanding to new solutions is 0, as they were not part\nof the unique solution in the original instance and the mapping operation only modified the\nspecific tuples (a1, a2) and (b1, b2) without introducing new solutions.\nIn conclusion, the expected number of tuples of the form (a1, \u2217) and (b1, \u2217) in the permit-\nted set is 2d(1 \u2212 p), but their probability of expanding to new solutions after the mapping\noperation is 0. This supports our previous claim that the mapping operation can transform\nan instance from the infinite set of satisfiable instances of Model RB with exactly one solution\ninto an unsatisfiable instance.\nWhen tasked with\nproving theorems,\nLLMs often provide\nexplanations that\nlack rigorous proof,\nglossing over crucial\ndetails. Emphasizing\nwords like \u2018rigorously\u2019,\n\u2018mathematically\u2019, or\n\u2018probabilistically\u2019 in\nthe prompt can guide\nLLMs towards more\nrigorous proofs.\n56\n24, 25, 53, 54, 55\nPrompt: Please calculate the probability of such tuples being expanded to new assignments\nthat satisfy all the constraints more rigorously.\nTo calculate the probability of tuples of the form (a1, \u2217) and (b1, \u2217) expanding to new\nassignments that satisfy all the constraints, we need to consider the impact of the mapping\noperation on these tuples and the other constraints in the problem instance.\nLet I be an instance from the infinite set of satisfiable instances of Model RB with exactly\none solution, and Cu = (Xu, Ru) be the constraint chosen in the mapping operation. Let I\u2032 be\nthe modified instance obtained after the mapping operation.\n43\nRecall that we have 2d(1 \u2212 p) expected tuples of the form (a1, \u2217) and (b1, \u2217) in Ru. After the\nmapping operation, we have a new permitted set R\u2032\nu, where we removed (a1, a2) and (b1, b2),\nand added (a1, b2) and (b1, a2). Since we only modified two tuples in Ru, the expected number\nof tuples of the form (a1, \u2217) and (b1, \u2217) in R\u2032\nu remains 2d(1 \u2212 p).\nNow, consider a new assignment \u03c3\u2032 that satisfies the modified constraint C\u2032\nu. For \u03c3\u2032 to be\na solution for the instance I\u2032, it must satisfy all other constraints Ci \u2208 C \u2212 {Cu}. Since the\ninstance I has exactly one solution, the other constraints are designed such that they limit the\npossible assignments to the unique solution \u03c3.\nThe probability of \u03c3\u2032 satisfying all the other constraints is very low, as these constraints\nare independent and were not affected by the mapping operation. The probability of a single\nconstraint Ci being satisfied by \u03c3\u2032 is given by the ratio of allowed tuples to all possible tuples,\nwhich is (1 \u2212 p).\nFor the new assignment \u03c3\u2032 to satisfy all m \u2212 1 remaining constraints, the probability is\n(1 \u2212 p)m\u22121. As the number of variables (n) and domain size (d) tend to infinity, the number\nof constraints m = rn ln d also increases.\nThus, (1 \u2212 p)m\u22121 tends to 0, making it highly\nunlikely for such tuples to expand to new assignments that satisfy all the constraints.\nIn conclusion, the probability of tuples of the form (a1, \u2217) and (b1, \u2217) expanding to new\nassignments that satisfy all the constraints in the modified instance I\u2032 is very low, approaching\n0 as n and d tend to infinity.\n57\n24, 25, 53, 54, 55, 56\nPrompt: It seems that you\u2019ve forgotten the multiple possible domain values for each variable\nin a new assignment.\nYou are correct. I apologize for the oversight. To address the multiple possible domain\nvalues for each variable in a new assignment, let\u2019s reexamine the probability calculation.\nLet I be an instance from the infinite set of satisfiable instances of Model RB with exactly\none solution, and Cu = (Xu, Ru) be the constraint chosen in the mapping operation. Let I\u2032 be\nthe modified instance obtained after the mapping operation.\nConsider a new assignment \u03c3\u2032 that satisfies the modified constraint C\u2032\nu. As before, for \u03c3\u2032\nto be a solution for the instance I\u2032, it must satisfy all other constraints Ci \u2208 C \u2212 {Cu}. A new\nassignment \u03c3\u2032 can have dn\u22121 possible combinations of domain values for the remaining n \u2212 1\nvariables.\nThe probability of a single constraint Ci being satisfied by a specific combination of domain\nvalues is (1 \u2212 p), as previously discussed. To calculate the probability of \u03c3\u2032 satisfying all the\nother constraints for any combination of domain values, we need to consider all possible\ncombinations and the independence of each constraint.\nThe probability of \u03c3\u2032 satisfying all m \u2212 1 remaining constraints for a specific combination\nof domain values is (1 \u2212 p)m\u22121. To account for all dn\u22121 possible combinations of domain\nvalues for the remaining n \u2212 1 variables, we need to consider the union of probabilities for\nall these combinations.\nUsing the union bound, the probability of \u03c3\u2032 satisfying all m \u2212 1 remaining constraints for\nany combination of domain values is at most dn\u22121(1 \u2212 p)m\u22121. As the number of variables\n(n) and domain size (d) tend to infinity, the number of constraints m = rn ln d also increases.\nThus, as n and d grow, (1 \u2212 p)m\u22121 tends to 0, but dn\u22121 tends to infinity.\nIn this case, the probability of tuples of the form (a1, \u2217) and (b1, \u2217) expanding to new\nassignments that satisfy all the constraints in the modified instance I\u2032 cannot be determined\nto approach 0 or any fixed value without more information on the relationship between n, d,\np, and the instance\u2019s structure.\n44\nTo rigorously prove that the mapping operation eliminates the unique solution of the\ninstance without introducing new solutions, we would need additional constraints on the\nstructure of the instances or the relationships between the parameters.\n58\n24, 25, 53, 54, 55, 56, 57\nPrompt: Why do you say \u2018the remaining n \u2212 1 variables\u2019? It seems that the value of two\nvariables has been assigned.\nMy apologies, I misspoke in my explanation. You are correct that the values of two vari-\nables, xu1 and xu2, have been assigned in the modified constraint C\u2032\nu. The remaining n \u2212 2\nvariables still have multiple possible domain values in a new assignment.\nTo account for this correction, let\u2019s reevaluate the probability calculation. A new assign-\nment \u03c3\u2032 can have dn\u22122 possible combinations of domain values for the remaining n \u2212 2 vari-\nables.\nThe probability of \u03c3\u2032 satisfying all m \u2212 1 remaining constraints for a specific combination\nof domain values is still (1 \u2212 p)m\u22121. To account for all dn\u22122 possible combinations of domain\nvalues for the remaining n \u2212 2 variables, we need to consider the union of probabilities for\nall these combinations.\nUsing the union bound, the probability of \u03c3\u2032 satisfying all m \u2212 1 remaining constraints for\nany combination of domain values is at most dn\u22122(1 \u2212 p)m\u22121. As the number of variables\n(n) and domain size (d) tend to infinity, the number of constraints m = rn ln d also increases.\nThus, as n and d grow, (1 \u2212 p)m\u22121 tends to 0, but dn\u22122 tends to infinity.\nAs before, the probability of tuples of the form (a1, \u2217) and (b1, \u2217) expanding to new as-\nsignments that satisfy all the constraints in the modified instance I\u2032 cannot be determined to\napproach 0 or any fixed value without more information on the relationship between n, d, p,\nand the instance\u2019s structure.\nTo rigorously prove that the mapping operation eliminates the unique solution of the\ninstance without introducing new solutions, we would still need additional constraints on\nthe structure of the instances or the relationships between the parameters.\n59\n24, 25, 53, 54, 55, 56, 57, 58\nPrompt: Great! So considering all the possible tuples, what is the probability of such new\nsolutions?\nTo calculate the probability of new solutions arising from the tuples of the form (a1, \u2217) and\n(b1, \u2217) after the mapping operation, we need to evaluate the joint probability of these tuples\nleading to new assignments that satisfy all the constraints in the modified instance I\u2032.\nAs discussed earlier, the probability of a new assignment \u03c3\u2032 satisfying all m \u2212 1 remaining\nconstraints for any combination of domain values for the remaining n \u2212 2 variables is at most\ndn\u22122(1 \u2212 p)m\u22121.\nNow, let\u2019s consider the tuples of the form (a1, \u2217) and (b1, \u2217). There are 2d possible tuples,\nand after the mapping operation, we have 2d(1 \u2212 p) expected such tuples in R\u2032\nu.\nTo calculate the probability of new solutions arising from these tuples, we need to account\nfor the fact that different tuples may lead to overlapping sets of new assignments. Since the\nconstraints are chosen independently, we can use the union bound to estimate the probability\nof new solutions arising from any of the expected 2d(1 \u2212 p) tuples.\nUsing the union bound, the probability of new solutions arising from the tuples of the\nform (a1, \u2217) and (b1, \u2217) is at most 2d(1 \u2212 p)dn\u22122(1 \u2212 p)m\u22121.\nAs the number of variables (n) and domain size (d) tend to infinity, the number of con-\n45\nstraints m = rn ln d also increases.\nThus, as n and d grow, (1 \u2212 p)m\u22121 tends to 0, but\n2d(1 \u2212 p)dn\u22122 tends to infinity.\nThe probability of new solutions arising from the tuples of the form (a1, \u2217) and (b1, \u2217)\ncannot be determined to approach 0 or any fixed value without more information on the\nrelationship between n, d, p, and the instance\u2019s structure.\nTo rigorously prove that the mapping operation eliminates the unique solution of the\ninstance without introducing new solutions, we would still need additional constraints on\nthe structure of the instances or the relationships between the parameters.\n60\nPrompt: Previous, we\u2019ve set dn(1 \u2212 p)m = 1\n2. Based on this, what is the value of 2d(1 \u2212\np)dn\u22122(1 \u2212 p)m\u22121, when d and n tend to infinity?\nLet\u2019s first simplify the expression 2d(1 \u2212 p)dn\u22122(1 \u2212 p)m\u22121 by combining the terms with d\nand (1 \u2212 p):\n2d(1 \u2212 p)dn\u22122(1 \u2212 p)m\u22121 = 2dn(1 \u2212 p)m (1 \u2212 p)\u22121\nd2\nWe are given that dn(1 \u2212 p)m = 1\n2. Multiplying both sides of this equation by 2, we get:\n2dn(1 \u2212 p)m = 1\nWe have proved that\nthe mapping operation\ndoes not introduce new\nsolutions. With this,\nwe have completed the\nproof of the second type\nmentioned in 35.\nNow, we can substitute this into our simplified expression:\n2dn(1 \u2212 p)m (1 \u2212 p)\u22121\nd2\n= 1 \u00b7 (1 \u2212 p)\u22121\nd2\nWhen d and n tend to infinity, the term (1\u2212p)\u22121\nd2\ntends to (1\u2212p)\u22121\n\u221e\n= 0. Therefore, the value\nof 2d(1 \u2212 p)dn\u22122(1 \u2212 p)m\u22121 when d and n tend to infinity is:\n1 \u00b7 0 = 0\n61\n24, 25, 53, 54, 55\nPrompt: Great! We have also proved that there exists an infinite set of unsatisfiable instances\nof Model RB, how to prove that any instance in this set can be transformed to satisfiable\ninstance under the mapping operation? (n tends to infinity, d tends to infinity.)\nIn 43, we set the upper\nbound for the probability\nof a random instance\nbeing satisfiable at\n1\n2 . Consequently, the\nprobability of an instance\nbeing unsatisfiable retains\na positive value, which\ninherently implies the\nexistence of an infinite\nnumber of unsatisfactory\ninstances.\nTo prove that any instance in the infinite set of unsatisfiable instances of Model RB can be\ntransformed into a satisfiable instance under the mapping operation, we need to show that\nthe mapping operation can introduce a solution to the unsatisfiable instance.\nLet I be an instance from the infinite set of unsatisfiable instances of Model RB, and let\nCu = (Xu, Ru) be the unsatisfied constraint chosen in the mapping operation.\nNow, consider the tuples (a1, a2) and (b1, b2) \u2208 Ru used in the mapping operation, with\n(a1, b2) /\u2208 Ru and (b1, a2) /\u2208 Ru. After applying the mapping operation, the constraint Cu is\nreplaced by C\u2032\nu = (Xu, R\u2032\nu), where the permitted set R\u2032\nu is obtained by removing (a1, a2) and\n(b1, b2) from Ru and adding (a1, b2) and (b1, a2).\nTo show that the instance I\u2032 obtained after the mapping operation is satisfiable, we will\nconstruct a solution \u03c3\u2032 that satisfies C\u2032\nu and all the other constraints in the instance.\nFirst, note that since I is unsatisfiable, there exists an assignment \u03c3 that satisfies all con-\nstraints except Cu. Now, we will modify \u03c3 to create a new assignment \u03c3\u2032 that satisfies C\u2032\nu as\nwell.\n46\nIf \u03c3(xu1) = a1 and \u03c3(xu2) = a2, set \u03c3\u2032(xu1) = a1 and \u03c3\u2032(xu2) = b2. If \u03c3(xu1) = b1 and\n\u03c3(xu2) = b2, set \u03c3\u2032(xu1) = b1 and \u03c3\u2032(xu2) = a2. In both cases, the new assignment \u03c3\u2032 satisfies\nthe modified constraint C\u2032\nu.\nSince the other constraints have not been changed during the mapping operation, \u03c3\u2032 also\nsatisfies all the other constraints, as it only differs from \u03c3 in the values assigned to xu1 and\nxu2. Therefore, \u03c3\u2032 is a solution for the modified instance I\u2032, making it a satisfiable instance.\nHence, we have proved that any instance in the infinite set of unsatisfiable instances of\nModel RB can be transformed into a satisfiable instance under the mapping operation, as the\nnumber of variables (n) and the domain size (d) tend to infinity.\n62\n24, 25, 53, 54, 55, 61\nPrompt: Great! In the above transformation process, it is necessary to ensure the existence\nof only unsatisfied constraint Cu for the current assignment (the assignment satisfies all the\nother constraints except Cu). However, how to prove that during the solving process, for any\nassignment, we can always modify the satisfiability using the above method (i.e., proving\nthe existence of only unsatisfied constraint for any variables)?\nIn the above argument, we showed that we can transform an unsatisfiable instance into\na satisfiable one and vice versa using the mapping operation. However, we did not prove\nthat there always exists an assignment that satisfies all constraints except Cu. In general, it\nmay not be the case that we can find an only unsatisfied constraint for any assignment. The\nexamples we considered were simplified to illustrate the mapping operation\u2019s effect on the\nsatisfiability of instances.\nIn practice, to modify the satisfiability of an instance during the solving process, we can\napply local search techniques, constraint propagation, or other methods for solving con-\nstraint satisfaction problems. These methods can help find partial assignments that satisfy a\nlarge number of constraints and identify critical constraints or variables that determine the\ninstance\u2019s satisfiability.\nFor example, we can use a local search algorithm to find an assignment that maximizes the\nnumber of satisfied constraints. If the maximum number of satisfied constraints is equal to\nthe total number of constraints minus one (m \u2212 1), then we can identify the only unsatisfied\nconstraint and apply the mapping operation. If the maximum number of satisfied constraints\nis less than m \u2212 1, it indicates that the problem instance might have more complex interdepen-\ndencies among constraints and variables. In this case, we may need to apply more advanced\ntechniques or heuristics to modify the satisfiability of the instance.\nTo summarize, the existence of an only unsatisfied constraint for any assignment is not\nguaranteed. However, the mapping operation can still be useful to modify the satisfiability\nof instances during the solving process, especially when combined with other techniques for\nsolving constraint satisfaction problems.\nIf the LLM struggles\nwith generating an\nanswer, consider\nsimplifying the request\nby removing some past\nturns in the dialogue\nhistory.\n63\n24, 25\nPrompt: If there exists an assignment that satisfies all the other constraints of an instance\nexcept Cu, then we call Cu an self-unsatisfiable constraint.\nFor an unsatisfiable instance, how to prove that considering all the possible assignments,\nevery variable is contained in at least one self-unsatisfiable constraint? (n tends to infinity, d\ntends to infinity.) Do you have any possible ideas? Please lay out a high-level proof plan.\nWe adopt 63* to denote\nthe inclusion of the self-\nunsatisfiable constraint\ndefinition within the\nprompt.\nTo prove that in an unsatisfiable instance of Model RB, every variable is contained in at\nleast one self-unsatisfiable constraint, we can leverage the probabilistic method. We will show\n47\nthat as n and d tend to infinity, the probability of a variable not being contained in any self-\nunsatisfiable constraint tends to zero. This implies that, with high probability, every variable\nis contained in at least one self-unsatisfiable constraint. Here\u2019s the high-level proof plan:\n1. Define the events: Let Ei be the event that variable xi is not contained in any self-unsatisfiable\nconstraint.\n2. Analyze the probability of a single constraint being self-unsatisfiable: Calculate the prob-\nability that a randomly chosen constraint Ci is self-unsatisfiable.\nThis can be done by\nconsidering the probability that all other constraints are satisfied while Ci is not. We can\nexpress this probability in terms of p, k, and d.\n3. Calculate the probability of a variable not being in a self-unsatisfiable constraint: For each\nvariable xi, calculate the probability that it is not contained in any self-unsatisfiable con-\nstraint. This can be done by considering the complementary event, i.e., that xi is contained\nin at least one self-unsatisfiable constraint, and then take the complement of this prob-\nability. Use the union bound to obtain an upper bound on the probability of the event\nEi.\n4. Apply the union bound and show that the probability tends to zero: Now, consider the\nprobability of the event Sn\ni=1 Ei, i.e., the probability that at least one variable is not con-\ntained in any self-unsatisfiable constraint. Apply the union bound to obtain an upper\nbound on this probability. Show that as n and d tend to infinity, this probability tends to\nzero.\n5. Conclude that every variable is contained in a self-unsatisfiable constraint with high prob-\nability: Since the probability of the event Sn\ni=1 Ei tends to zero as n and d tend to infinity,\nwe can conclude that with high probability, every variable is contained in at least one\nself-unsatisfiable constraint in an unsatisfiable instance of Model RB.\nIf the model\u2019s response\nreflects a misunder-\nstanding, it could be\na result of ambiguity\nwithin the prompt.\nIn such instances, try\nto review and revise\nthose specific parts in\nthe prompt for clarity.\nFor instance, we pre-\nviously used the term\n\u2019the only unsatisfiable\nconstraint\u2019 to denote\nsuch restrictions, but\nGPT-4 often misinter-\nprets this phrase. Thus,\nwe introduce the new\nterm, \u2018self-unsatisfiable\nconstraint\u2019.\nThis high-level proof plan leverages the probabilistic method and the union bound to\nshow that in an unsatisfiable instance of Model RB, every variable is contained in at least one\nself-unsatisfiable constraint with high probability as n and d tend to infinity.\n64\n24, 25, 63*\nPrompt: For an unsatisfiable instance, considering all the possible assignments, let\u2019s calculate\nthe probability of a constraint being self-unsatisfiable. (n tends to infinity, d tends to infinity.)\nCan you suggest some possible methods?\nTo calculate the probability of a constraint being self-unsatisfiable, we can use the following\nmethods:\n1. Monte Carlo method: This method involves generating a large number of random in-\nstances and assignments, and then calculating the fraction of instances where a constraint\nis self-unsatisfiable. As the number of instances and assignments tends to infinity, the\nMonte Carlo estimate of the probability converges to the true probability.\n2. Analytical approach: This method involves calculating the probability of a constraint being\nself-unsatisfiable directly, using combinatorics and probability theory. This approach can\nbe quite challenging, especially for complex models like Model RB. However, if successful,\nit can provide closed-form expressions for the probability, which can be useful for further\ntheoretical analysis.\n48\n3. Generating function method: This method involves constructing a generating function that\ncaptures the combinatorial structure of the problem. By analyzing the properties of this\nfunction (e.g., its asymptotic behavior), we can derive information about the probability of\na constraint being self-unsatisfiable.\n4. First moment method: This method involves calculating the expected number of self-\nunsatisfiable constraints for a random instance. If the expected number tends to zero as n\nand d tend to infinity, then the probability of a constraint being self-unsatisfiable also tends\nto zero. This method can be useful for establishing a threshold behavior for the probability\nof unsatisfiability.\n5. Second moment method: This method involves calculating the variance of the number\nof self-unsatisfiable constraints for a random instance. If the variance is much smaller\nthan the square of the expected number, then the probability of a constraint being self-\nunsatisfiable can be approximated by the ratio of the expected number to the total number\nof constraints. This method can be useful for proving concentration results for the proba-\nbility of unsatisfiability.\nThese methods can be used separately or combined to provide a comprehensive under-\nstanding of the probability of a constraint being self-unsatisfiable in Model RB as n and d\ntend to infinity.\n65\n24, 25, 63*\nPrompt: For an unsatisfiable instance, considering all the possible assignments, let N be the\nnumber of assignments that satisfies all constraints except the given one. Can you calculate\nthe expectation of N?\nLet E(N) be the expected value of N, the number of assignments that satisfy all constraints\nexcept the given one in an unsatisfiable instance. To calculate E(N), we can consider the\nprobability of each assignment satisfying all constraints except the given one and sum it over\nall possible assignments.\nLet \u03c3 be an assignment and Cu be the given constraint. We want to compute the probability\nthat \u03c3 satisfies all constraints except Cu. The probability of \u03c3 satisfying a single constraint Ci\nis given by the ratio of the size of the permitted set Ri to the total number of possible tuples\nfor the variables in Xi, which is dk. Since |Ri| = (1 \u2212 p)dk, the probability of \u03c3 satisfying Ci is\n(1 \u2212 p).\nNow, since there are m \u2212 1 constraints other than Cu, and each of them must be satis-\nfied, the probability of \u03c3 satisfying all of them is (1 \u2212 p)m\u22121. Note that we assume that the\nconstraints are independent.\nThere are a total of dn possible assignments for the instance. To compute the expected\nvalue of N, we sum the probabilities of all assignments satisfying all constraints except Cu:\nE(N) = dn(1 \u2212 p)m\u22121.\nHowever, it\u2019s important to keep in mind that this is an approximation, as it assumes that\nthe constraints are independent. In practice, since some variables are shared between con-\nstraints, the true expected value of N might be different.\n66\n24, 25, 63*, 65\nPrompt: You seem to be missing the probability that the assignment unsatisfies the only\nunsatisfied constraint.\n49\nYou are correct, I missed that part in the calculation. Let\u2019s add that probability to our\ncalculation of the expected value of N.\nLet q be the probability that an assignment \u03c3 does not satisfy the given constraint Cu. Since\n|Ru| = (1 \u2212 p)dk, the probability of not satisfying Cu is q = 1 \u2212 (1 \u2212 p) = p.\nNow, we want the probability of an assignment satisfying all other constraints except Cu,\nand not satisfying Cu. The probability of \u03c3 satisfying all other constraints except Cu is still\n(1 \u2212 p)m\u22121, and the probability of not satisfying Cu is p.\nSince we are assuming the constraints are independent, we can multiply these probabilities\ntogether to get the probability of an assignment satisfying all constraints except Cu and not\nsatisfying Cu:\n(1 \u2212 p)m\u22121 \u00b7 p\nNow, to compute the expected value of N, we sum this probability over all dn possible\nassignments:\nE(N) = dn(1 \u2212 p)m\u22121 \u00b7 p\nThis is the expected value of N under the assumption that the constraints are independent.\nIn practice, due to shared variables between constraints, the true expected value of N might\nbe different.\n67\n24, 25\nPrompt: We have: E[N] = dn(1 \u2212 p)m\u22121 \u00b7 p and E[X] = n\u03b1n(1 \u2212 p)m. Can you calculate and\ntell me the relationship between E[N]2 and E[X]2?\nDrawing on our prior\nexperience with the\nsecond moment method\n(47) and the known\nvalues of E\n\u0002\nX2\u0003\nand\nE [X]2, we analyze the\nconnection between\nE\n\u0002\nN2\u0003\nand E\n\u0002\nX2\u0003\n.\nWe are given:\nE[N] = dn(1 \u2212 p)m\u22121 \u00b7 p\nE[X] = n\u03b1n(1 \u2212 p)m\nTo find the relationship between E[N]2 and E[X]2, we need to find the expressions for both\nand compare them.\nFirst, let\u2019s find E[N]2:\nE[N]2 = (dn(1 \u2212 p)m\u22121 \u00b7 p)2 = d2n(1 \u2212 p)2m\u22122 \u00b7 p2\nNow, let\u2019s find E[X]2:\nE[X]2 = (n\u03b1n(1 \u2212 p)m)2 = n2\u03b1n(1 \u2212 p)2m\nFinally, let\u2019s compare the two expressions:\nE[N]2\nE[X]2 = d2n(1 \u2212 p)2m\u22122 \u00b7 p2\nn2\u03b1n(1 \u2212 p)2m\nSimplifying the expression by cancelling out (1 \u2212 p)2m\u22122:\nE[N]2\nE[X]2 =\nd2n \u00b7 p2\nn2\u03b1n(1 \u2212 p)2\nNow, recall that d = n\u03b1. Substituting this into the expression:\n50\nE[N]2\nE[X]2 =\n(n\u03b1)2n \u00b7 p2\nn2\u03b1n(1 \u2212 p)2\nSimplifying:\nE[N]2\nE[X]2 =\np2\n(1 \u2212 p)2\nSo, the relationship between E[N]2 and E[X]2 is:\nE[N]2 =\np2\n(1 \u2212 p)2 \u00b7 E[X]2\n47* denotes the equations\nin the prompt of 47 are\nincluded in the current\nprompt as well.\n68\n24, 25, 47*\nCan you calculate and tell me the relationship between E[N2] and E[X2]?\nTo find the relationship between E[N2] and E[X2], let\u2019s first observe the differences in their\nformulas.\nThe main difference is in the power of the terms inside the parentheses. In E[N2], the\npower is (rn ln d \u2212 1), while in E[X2], the power is (rn ln d).\nNow, let\u2019s consider the following transformation:\nT[X2] = E[N2]\nE[X2] .\nWe can rewrite this expression as follows:\nT[X2] =\n\u2211n\nS=0 dn(n\nS)(d \u2212 1)n\u2212S((1 \u2212 p) (S\nk)\n(n\nk) + (1 \u2212 p)2(1 \u2212 (S\nk)\n(n\nk)))rn ln d\u22121 \u00b7 (p (S\nk)\n(n\nk) + p2(1 \u2212 (S\nk)\n(n\nk)))\n\u2211n\nS=0 dn(n\nS)(d \u2212 1)n\u2212S((1 \u2212 p) (S\nk)\n(n\nk) + (1 \u2212 p)2(1 \u2212 (S\nk)\n(n\nk)))rn ln d\n=\nn\n\u2211\nS=0\n((1 \u2212 p) (S\nk)\n(n\nk) + (1 \u2212 p)2(1 \u2212 (S\nk)\n(n\nk)))rn ln d\u22121 \u00b7 (p (S\nk)\n(n\nk) + p2(1 \u2212 (S\nk)\n(n\nk)))\n((1 \u2212 p) (S\nk)\n(n\nk) + (1 \u2212 p)2(1 \u2212 (S\nk)\n(n\nk)))rn ln d\n=\nn\n\u2211\nS=0\np (S\nk)\n(n\nk) + p2(1 \u2212 (S\nk)\n(n\nk))\n((1 \u2212 p) (S\nk)\n(n\nk) + (1 \u2212 p)2(1 \u2212 (S\nk)\n(n\nk)))\n.\nThus, T[X2] represents the relationship between E[N2] and E[X2].\n69\n24, 25, 63*, 67, 68\nPrompt: Now let\u2019s calculate the probability of N > 0 with the second moment method.\nTo calculate the probability of N > 0 using the second moment method, we first need to\ncompute the second moment E\n\u0002\nN2\u0003\nand then use Chebyshev\u2019s inequality.\nSince we have already derived a formula for the ratio\nE[N2]\nE[X2] , we can obtain E\n\u0002\nN2\u0003\nby\nmultiplying the ratio by E\n\u0002\nX2\u0003\n:\nE\nh\nN2i\n= E\n\u0002\nN2\u0003\nE [X2] \u00b7 E\nh\nX2i\n=\nn\n\u2211\nS=0\np (S\nk)\n(n\nk) + p2\n\u0012\n1 \u2212 (S\nk)\n(n\nk)\n\u0013\n\u0012\n(1 \u2212 p) (S\nk)\n(n\nk) + (1 \u2212 p)2\n\u0012\n1 \u2212 (S\nk)\n(n\nk)\n\u0013\u0013 \u00b7 E\nh\nX2i\n51\nNext, we apply Chebyshev\u2019s inequality to get an upper bound on the probability of N > 0:\nPr(N > 0) \u2265 1 \u2212 E\n\u0002\nN2\u0003\nE[N]2 = 1 \u2212\n\u2211n\nS=0\np (S\nk)\n(n\nk) +p2\n\u0012\n1\u2212 (S\nk)\n(n\nk)\n\u0013\n\u0012\n(1\u2212p) (S\nk)\n(n\nk) +(1\u2212p)2\n\u0012\n1\u2212 (S\nk)\n(n\nk)\n\u0013\u0013 \u00b7 E\n\u0002\nX2\u0003\n\u0010\np\n(1\u2212p)2 \u00b7 E[X]2\n\u00112\nSimplifying the expression, we obtain:\nPr(N > 0) \u2265 1 \u2212\n\u2211n\nS=0\np (S\nk)\n(n\nk) +p2\n\u0012\n1\u2212 (S\nk)\n(n\nk)\n\u0013\n\u0012\n(1\u2212p) (S\nk)\n(n\nk) +(1\u2212p)2\n\u0012\n1\u2212 (S\nk)\n(n\nk)\n\u0013\u0013\np2\n(1\u2212p)2\nBy calculating this probability, we can estimate the likelihood of an unsatisfiable instance\nhaving at least one self-unsatisfiable constraint.\nWhen the computa-\ntional load of LLMs\nbecomes excessively\nlarge, we break down\nthe intermediate\nsteps for preliminary\ncalculations. The pre-\ncomputation strategy\naims to reduce the\nsingle-round com-\nputational burden of\nLLMs.\n70\n24, 25\nPrompt: Can you estimate the value of (S\nk)\n(n\nk) when S = ns(0 \u2264 s \u2264 1) and n tends to infinity.\nTo estimate the value of (S\nk)\n(n\nk) when S = ns (0 \u2264 s \u2264 1) and n tends to infinity, we use the\nformula for binomial coefficients:\n\u0012n\nk\n\u0013\n=\nn!\nk!(n \u2212 k)!\nSo, we get:\n(S\nk)\n(n\nk) =\nS!\nk!(S\u2212k)!\nn!\nk!(n\u2212k)!\nWe can simplify the expression to:\n(S\nk)\n(n\nk) = S!(n \u2212 k)!\nn!(S \u2212 k)! = (ns)!(n \u2212 k)!\nn!(ns \u2212 k)!\nNow, as n tends to infinity, we can approximate the factorials using Stirling\u2019s approxima-\ntion:\nn! \u2248\n\u221a\n2\u03c0n\n\u0010n\ne\n\u0011n\nUsing Stirling\u2019s approximation, we get:\n(ns)!(n \u2212 k)!\nn!(ns \u2212 k)!\n\u2248\n\u221a\n2\u03c0ns\n\u0000 ns\ne\n\u0001ns p\n2\u03c0(n \u2212 k)\n\u0010\nn\u2212k\ne\n\u0011n\u2212k\n\u221a\n2\u03c0n\n\u0000 n\ne\n\u0001n p\n2\u03c0(ns \u2212 k)\n\u0010\nns\u2212k\ne\n\u0011ns\u2212k\nNow we can cancel out common terms and simplify:\n(ns)!(n \u2212 k)!\nn!(ns \u2212 k)!\n\u2248\nq\nns\nn\u2212k\n\u0000 ns\nn\n\u0001ns \u0010\nn\u2212k\nn\n\u0011n\u2212k\nq\nns\nns\u2212k\n\u0010\nns\u2212k\nn\n\u0011ns\u2212k\nSince n tends to infinity, we can further simplify the expression:\n52\nlim\nn\u2192\u221e\n(S\nk)\n(n\nk) \u2248 lim\nn\u2192\u221e\nq\nns\nn\u2212k\n\u0000 ns\nn\n\u0001ns \u0010\nn\u2212k\nn\n\u0011n\u2212k\nq\nns\nns\u2212k\n\u0010\nns\u2212k\nn\n\u0011ns\u2212k\nAs n \u2192 \u221e, the limit converges to:\n(S\nk)\n(n\nk) \u2248 sk(1 \u2212 s)k\n(1 \u2212 s)k\n= sk\nSo, when S = ns and n tends to infinity, the value of (S\nk)\n(n\nk) is approximately sk.\n71\n24, 25, 63*, 69, 70\nPrompt: Now let\u2019s calculate the probability of N>0 with the second moment method.\nTo calculate the probability of N > 0 with the second moment method, we first note that:\nP(N > 0) \u2265 E[N]2\nE[N2]\nWe already know the expressions for E[N]2 and E[N2] in terms of E[X]2 and E[X2] from\nthe given information. We can now substitute the expressions into the inequality:\nP(N > 0) \u2265\np2\n(1\u2212p)2 \u00b7 E[X]2\n\u2211n\nS=0\np (S\nk)\n(n\nk) +p2\n\u0012\n1\u2212 (S\nk)\n(n\nk)\n\u0013\n\u0012\n(1\u2212p) (S\nk)\n(n\nk) +(1\u2212p)2\n\u0012\n1\u2212 (S\nk)\n(n\nk)\n\u0013\u0013\nWe also know that E[X]2\nE[X2] \u2265 1\n3 and the expression for (S\nk)\n(n\nk) when S = ns as sk. We can now\nsimplify the expression:\nP(N > 0) \u2265\np2\n(1\u2212p)2 \u00b7 E[X]2\n\u22111\ns=0\npsk+p2(1\u2212sk)\n(1\u2212p)sk+(1\u2212p)2(1\u2212sk)\nNow we can calculate the limit as n tends to infinity:\nlim\nn\u2192\u221e P(N > 0) \u2265\np2\n(1\u2212p)2 \u00b7 1\n3\n\u22111\ns=0\npsk+p2(1\u2212sk)\n(1\u2212p)sk+(1\u2212p)2(1\u2212sk)\nBy calculating the limit, we can obtain an expression for the probability of N > 0, which\nis the probability that there exists an assignment that satisfies all the other constraints of an\ninstance except the given one as n tends to infinity.\n72\n24, 25, 63*, 69, 70, 71\nPrompt: I have identified a problem with the current formula. The formula contains a sum-\nmation symbol, but the summation is not needed. Please correct the formula and simplify\nit, then continue to calculate the probability of N > 0, or at least its lower bound.\nApologies for the confusion. You are right, there is no need for a summation. We can\nrewrite the expression for the probability of N > 0 as follows:\n53\nP(N > 0) \u2265\np2\n(1\u2212p)2 \u00b7 E[X]2\npsk+p2(1\u2212sk)\n(1\u2212p)sk+(1\u2212p)2(1\u2212sk)\nNow, we can apply the known inequality E[X]2\nE[X2] \u2265 1\n3:\nP(N > 0) \u2265\np2\n(1\u2212p)2 \u00b7 1\n3\npsk+p2(1\u2212sk)\n(1\u2212p)sk+(1\u2212p)2(1\u2212sk)\nSince we are interested in the lower bound of the probability P(N > 0), we can simplify\nthe expression by taking s = 1, as the denominator will be minimized in this case:\nP(N > 0) \u2265\np2\n(1\u2212p)2 \u00b7 1\n3\np+p2(1\u22121)\n(1\u2212p)+(1\u2212p)2(1\u22121)\nThis simplifies to:\nP(N > 0) \u2265\np2\n(1\u2212p)2 \u00b7 1\n3\np\n(1\u2212p)\nNow, we can cancel out the p in the numerator and denominator, and further simplify:\nP(N > 0) \u2265\np\n3(1 \u2212 p)\nThis expression provides the lower bound for the probability of N > 0, which is the\nprobability that there exists an assignment that satisfies all the other constraints of an instance\nexcept the given one.\n73\n24, 25, 63*, 69, 72\nPrompt: Great! Since 0 < p < 1, can we make the lower bound simpler?\nYes, we can further simplify the lower bound for the probability of N > 0. Since 0 < p < 1,\nwe know that 1 \u2212 p > 0. Thus, we can rewrite the lower bound as follows:\nP(N > 0) \u2265\np\n3(1 \u2212 p) \u00b7 (1 \u2212 p)\n(1 \u2212 p)\nThis simplifies to:\nP(N > 0) \u2265 p(1 \u2212 p)\n3(1 \u2212 p)\nNow, we can cancel out the (1 \u2212 p) in the numerator and denominator:\nP(N > 0) \u2265 p\n3\nThis expression provides a simpler lower bound for the probability of N > 0, which is\nthe probability that there exists an assignment that satisfies all the other constraints of an\ninstance except the given one.\nStep 2 of the plan in 63\nwas accomplished:\ncalculating the probability\nthat a single constraint is\nself-unsatisfiable.\n54\n74\n24, 25, 63*, 73\nPrompt: How to prove that considering all the possible assignments, every variable is con-\ntained in at least one self-unsatisfiable constraint? (n tends to infinity, d tends to infinity.)\nCan you lay out a plan for proof?\nTo prove that every variable is contained in at least one self-unsatisfiable constraint, we\ncan proceed with the following plan for proof:\n1. Define notations and assumptions: Define notations for the variables, domains, constraints,\nand assignments involved in a random instance of Model RB. State the assumptions that\nn \u2192 \u221e, d \u2192 \u221e, and the probability of a randomly chosen constraint Ci being self-\nunsatisfiable is at least p/3.\n2. Establish bounds on the number of constraints: Since there are m = rn ln d constraints, and each\nvariable xi appears in k distinct constraints, we can establish upper and lower bounds on the number\nof constraints involving a specific variable xi.\n3. Probabilistic analysis: Perform a probabilistic analysis to calculate the probability that a\nrandomly chosen constraint Ci containing a specific variable xi is self-unsatisfiable. Use the\nassumption that the probability of a randomly chosen constraint being self-unsatisfiable is\nat least p/3.\n4. Union bound: Apply the union bound to calculate the probability that there exists a vari-\nable which is not part of any self-unsatisfiable constraint. This probability can be cal-\nculated as the complement of the probability that all variables are part of at least one\nself-unsatisfiable constraint.\n5. Limit analysis: Examine the behavior of the calculated probability as n \u2192 \u221e and d \u2192 \u221e.\nIf the probability tends to zero, conclude that every variable is contained in at least one\nself-unsatisfiable constraint with high probability.\n6. Conclusion: Summarize the results of the proof and outline any additional observations\nor implications for the Model RB problem.\n75\n24, 25, 63*, 73, 74\nPrompt: Now let\u2019s calculate the lower bounds on the number of constraints involving a\nspecific variable xi.\nTo calculate the lower bounds on the number of constraints involving a specific variable\nxi, we can analyze the constraint generation process in Model RB.\nIn each constraint Ci = (Xi, Ri), a sequence of k distinct variables Xi =\nxi1, xi2, . . . , xik is chosen uniformly at random without repetition from the set X . Since there\nare n variables in total, the probability of a specific variable xi being chosen in a single\nconstraint is given by:\nP(xi \u2208 Xi) = k\nn\nThere are m = rn ln d constraints in total. The number of constraints involving the variable\nxi follows a binomial distribution with parameters m and k/n. We can calculate the expected\nnumber of constraints involving xi as follows:\n55\nE[constraints involving xi] = m \u00b7 P(xi \u2208 Xi) = (rn ln d) \u00b7 k\nn = rk ln d\nAccording to the con-\nstraint generation process\nof Model RB, the vari-\nables are independent,\nwhich ensures the appli-\ncability of the Chernoff\nbounds.\nTo calculate the lower bound on the number of constraints involving the variable xi, we\ncan use the Chernoff bound for the binomial distribution. For any \u03b4 > 0, the Chernoff bound\nstates that:\nP(constraints involving xi \u2264 (1 \u2212 \u03b4)E[constraints involving xi]) \u2264 e\u2212 1\n2 \u03b42E[constraints involving xi]\nBy choosing an appropriate \u03b4 > 0, we can establish a lower bound on the number of\nconstraints involving the variable xi. As an example, let\u2019s choose \u03b4 = 1/2. Applying the\nChernoff bound, we get:\nP(constraints involving xi \u2264 1\n2rk ln d) \u2264 e\u2212 1\n8 rk ln d\nThus, with high probability, the number of constraints involving a specific variable xi is at\nleast 1\n2rk ln d.\n76\n24, 25, 63*, 73, 74, 75\nPrompt: Great! What is that probability for all the possible variables? Please convert the\nprobability into an expression with n as the base.\nand determine how to construct the\nvariables to make the probability infinitesimal as n and d approach infinity.\nWe have found the probability that the number of constraints involving a specific variable\nxi is at least 1\n2rk ln d. Now, we want to find the probability that this holds true for all n\nvariables.\nUsing the union bound, the probability that there exists a variable that has less than 1\n2rk ln d\nconstraints involving it is at most n times the probability for a single variable. Therefore, the\nprobability that at least one variable violates the lower bound is:\nP(at least one variable has < 1\n2rk ln d constraints) \u2264 n \u00b7 e\u2212 1\n8 rk ln d\nTo express this probability with n as the base, we can take the logarithm base n of both\nsides:\nlogn P(at least one variable has < 1\n2rk ln d constraints) \u2264 logn n + logn e\u2212 1\n8 rk ln d\n\u21d2 logn P(at least one variable has < 1\n2rk ln d constraints) \u2264 1 \u2212 1\n8rk ln d\nln n\nNow, we want this probability to go to zero as n \u2192 \u221e and d \u2192 \u221e. To achieve this, we\nneed to ensure that ln d\nln n grows unbounded as n, d \u2192 \u221e. Recall that the domain size d = n\u03b1,\nwhere \u03b1 > 0. Thus, we have:\nln d\nln n = ln n\u03b1\nln n = \u03b1\nWe can see that as long as \u03b1 > 0, the term ln d\nln n is constant and does not depend on n or d.\nTo make the probability infinitesimal as n, d \u2192 \u221e, we can adjust the value of r to ensure that\nthe inequality: 1 \u2212 1\n8rk\u03b1 < 0 holds true. By choosing a sufficiently large value of r (depending\n56\non the values of k and \u03b1), we can make the probability of having at least one variable with\nless than 1\n2rk ln d constraints infinitesimal as n and d approach infinity.\nFrom this point forward,\nwe incorporate 1 \u2212\n1\n8 rk\u03b1 < 0 into our\nconstruction conditions.\n77\n24, 25, 63*, 73, 74, 75\nPrompt: Great! Now let\u2019s apply the union bound to calculate the probability that there exists\na variable which is not part of any self-unsatisfiable constraint.\nTo apply the union bound, we first calculate the probability that a specific variable xi is\nnot part of any self-unsatisfiable constraint. Then, we\u2019ll use the union bound to calculate the\nprobability that there exists at least one such variable.\nLet the event Ai denote the situation where variable xi is not part of any self-unsatisfiable\nconstraint. We know that the lower bound on the number of constraints involving xi is at\nleast 1\n2rk ln d with high probability. Moreover, we have the assumption that the probability of\na randomly chosen constraint Ci being self-unsatisfiable is at least p\n3 .\nThus, the probability that a constraint involving xi is not self-unsatisfiable is at most 1 \u2212 p\n3 .\nWe can now calculate the probability of the event Ai:\nP(Ai) \u2264\n\u0010\n1 \u2212 p\n3\n\u0011 1\n2 rk ln d\nNow, to apply the union bound, we want to find the probability that there exists at least\none variable that is not part of any self-unsatisfiable constraint. There are n variables in total,\nso we can apply the union bound as follows:\nP\n n[\ni=1\nAi\n!\n\u2264\nn\n\u2211\ni=1\nP(Ai) \u2264 n\n\u0010\n1 \u2212 p\n3\n\u0011 1\n2 rk ln d\nThis expression represents the upper bound on the probability that there exists at least one\nvariable which is not part of any self-unsatisfiable constraint. In the next step, we can analyze\nthe behavior of this probability as n \u2192 \u221e and d \u2192 \u221e.\n78\n24, 25, 63*, 73, 74, 75, 77\nPrompt: Great! Please convert n\n\u00001 \u2212 p\n3\n\u0001 1\n2 rk ln d into an expression with n as the base. Then\ncalculate how to construct the variables to ensure that the expression becomes infinitesimal,\nas both n and d tend towards infinity.\nWe can rewrite the expression n\n\u00001 \u2212 p\n3\n\u0001 1\n2 rk ln d in terms of the base n by taking the following\nsteps:\n1. First, let\u2019s isolate the exponent 1\n2rk ln d:\n\u0010\n1 \u2212 p\n3\n\u0011 1\n2 rk ln d\n=\n\u0012\u0010\n1 \u2212 p\n3\n\u0011ln d\u0013 1\n2 rk\n2. Now, recall that d = n\u03b1. Taking the logarithm of both sides, we have ln d = \u03b1 ln n. Substi-\ntute this into the expression:\n\u0010\n1 \u2212 p\n3\n\u0011 1\n2 rk ln d\n=\n\u0012\u0010\n1 \u2212 p\n3\n\u0011\u03b1 ln n\u0013 1\n2 rk\n3. Using the property aln b = bln a, we can convert the expression with base n:\n\u0010\n1 \u2212 p\n3\n\u0011 1\n2 rk ln d\n=\n\u0010\nnln(1\u2212 p\n3 )\u0011 1\n2 \u03b1rk\n57\nNow the expression becomes:\nP(\nn[\ni=1\nAi) \u2264 n\n\u0010\nnln(1\u2212 p\n3 )\u0011 1\n2 \u03b1rk\nTo ensure that this expression becomes infinitesimal as both n and d tend towards infinity,\nwe need to find a condition on the variables such that the exponent of n is negative. Let\u2019s\ndenote the exponent of n as E:\nE = 1 + 1\n2\u03b1rk ln(1 \u2212 p\n3 )\nTo make the expression infinitesimal, we want E < 0. Therefore,\n1 + 1\n2\u03b1rk ln(1 \u2212 p\n3 ) < 0\nWe can rearrange the inequality to find a condition on the variables:\n1\n2\u03b1rk ln(1 \u2212 p\n3 ) < \u22121\nWe also integrate\n1\n2 \u03b1rk ln(1 \u2212 p\n3 ) < \u22121\ninto our construction\ncriteria.\nTo satisfy this inequality, we can choose the values of \u03b1, r, k, and p such that the product\n\u03b1rk ln(1 \u2212 p\n3 ) is less than \u22122. One possible way to achieve this is by choosing \u03b1, r, k > 0 and\np such that ln(1 \u2212 p\n3 ) is negative, which can be satisfied by choosing 0 < p < 1.\nBy constructing the variables in this manner, we can ensure that the expression n\n\u0010\nnln(1\u2212 p\n3 )\u0011 1\n2 \u03b1rk\nbecomes infinitesimal as both n and d tend towards infinity. This implies that, with high prob-\nability, every variable is contained in at least one self-unsatisfiable constraint.\nUp to this point, step\n3 and 4 of the plan\nin 63 was accomplished:\napplying the union bound\nand showing that the\nprobability tends to zero.\n79\n24, 25, 63*, 73, 74, 75, 77\nPrompt: With proper parameter selection, now what conclusion can we draw? Answer in\none sentence.\nWith proper parameter selection, we can conclude that every variable is contained in at\nleast one self-unsatisfiable constraint with high probability as both n and d tend towards\ninfinity.\n80\n63*\nPrompt: We have proved that there exists an infinite set of unsatisfiable instances of Model\nRB. For each unsatisfiable instance, we\u2019ve proved that considering all the possible assign-\nments, every variable is contained in at least one self-unsatisfiable constraint. How to prove\nthat any instance in this set can be transformed to a satisfiable instance under the mapping\noperation? (n tends to infinity, d tends to infinity.)\nTo prove that any unsatisfiable instance in the given infinite set can be transformed into a\nsatisfiable instance under the mapping operation, we will follow these steps:\n1. Choose an unsatisfiable instance I from the given infinite set.\n2. Since every variable is contained in at least one self-unsatisfiable constraint, there must\nexist at least one constraint Cu = (Xu, Ru) that is self-unsatisfiable.\n3. Perform the mapping operation on constraint Cu, creating a new instance I\u2032 with the\nmodified constraint C\u2032\nu = (Xu, R\u2032\nu).\n58\n4. Show that the new instance I\u2032 is satisfiable.\nStep 3 is straightforward, as we have already detailed the mapping operation for k = 2.\nNow we need to show that the new instance I\u2032 is satisfiable.\nLet \u03c3 be an assignment that satisfies all constraints in I except for Cu. Since (a1, a2) and\n(b1, b2) are not allowed in Ru, but (a1, b2) and (b1, a2) are allowed in R\u2032\nu, we can modify \u03c3 to\ncreate a new assignment \u03c3\u2032 that satisfies the modified constraint C\u2032\nu.\nSpecifically, update \u03c3\u2032 as follows:\n\u2022 For variable xu1, set \u03c3\u2032(xu1) = a1 if \u03c3(xu1) = b1, otherwise set \u03c3\u2032(xu1) = \u03c3(xu1).\n\u2022 For variable xu2, set \u03c3\u2032(xu2) = a2 if \u03c3(xu2) = b2, otherwise set \u03c3\u2032(xu2) = \u03c3(xu2).\nThis new assignment \u03c3\u2032 satisfies the modified constraint C\u2032\nu, because (\u03c3\u2032(xu1), \u03c3\u2032(xu2)) is\neither (a1, b2) or (b1, a2), both of which are allowed in R\u2032\nu.\nSince \u03c3\u2032 satisfies all other constraints in I, it also satisfies all constraints in I\u2032. Thus, the\nnew instance I\u2032 is satisfiable.\nWe have completed\nthe proof of the first\ntype mentioned in 35.\nCombined with the\nprevious proof of the\nsecond type, we have\ndemonstrated that it is\npossible to construct hard\ninstances required within\nthe schema.\nIn conclusion, we have shown that any unsatisfiable instance in the given infinite set can be\ntransformed into a satisfiable instance under the mapping operation. As n and d tend to infin-\nity, this demonstrates the existence of a transformation that can turn unsatisfiable instances\nof Model RB into satisfiable instances for large problem sizes.\n81\nPrompt: Now, let us verify whether there always exist instances that meet our requirements.\nBesides the requirements for the parameters in Model RB definition, the requirements we\nhad set for the parameters previously are:\n1. rcr =\n1\n\u2212 ln(1\u2212p), \u03b1 > 1\nk , 0 < p < 1 and k \u2265\n1\n1\u2212p.\n2. r = rcr + \u03f5, \u03f5 =\nln 1\n2\n\u03b1n ln n ln(1\u2212p).\n3. 1 + \u03b1(1 \u2212 rcrpk) < 0 and \u03b1 > 1.\n4. 1 \u2212 1\n8rk\u03b1 < 0.\n5. 1 + 1\n2\u03b1rk ln\n\u00001 \u2212 p\n3\n\u0001 < 0.\nPlease analyze if there are always instances that satisfy all demands. Please study a most\nimportant parameter, and explore how this parameter can ensure that instances always meet\nthe requirements.\nTo analyze if there are always instances that satisfy all the given demands, let\u2019s first rewrite\nthe conditions in a more readable format:\n1. rcr =\n1\n\u2212 ln(1\u2212p), \u03b1 > 1\nk, 0 < p < 1, and k \u2265\n1\n1\u2212p.\n2. r = rcr + \u03f5, \u03f5 =\nln 1\n2\n\u03b1n ln n ln(1\u2212p).\n3. 1 + \u03b1(1 \u2212 rcrpk) < 0 and \u03b1 > 1.\n4. 1 \u2212 1\n8rk\u03b1 < 0.\n5. 1 + 1\n2\u03b1rk ln\n\u00001 \u2212 p\n3\n\u0001 < 0.\nNow, let\u2019s analyze the most important parameter that can ensure the instances always meet\nthe requirements. The most important parameter is \u03b1, as it affects all the conditions directly\nor indirectly.\nTo satisfy condition 1, we need \u03b1 > 1\nk. This ensures that the domain size d grows with n,\nmaking the problem more complex.\n59\nCondition 2 is satisfied for any \u03f5 > 0. Since \u03b1 is in the denominator of \u03f5, a larger \u03b1 will\nmake \u03f5 smaller, keeping it positive.\nFor condition 3, we notice two parts: 1 + \u03b1(1 \u2212 rcrpk) < 0 and \u03b1 > 1. The second part\nimplies that \u03b1 should be greater than 1. The first part implies that, when \u03b1 is large enough, the\nterm inside the parentheses needs to be negative. This can be achieved if rcrpk is sufficiently\nlarge.\nCondition 4 states that 1 \u2212 1\n8rk\u03b1 < 0. This implies that 1\n8rk\u03b1 > 1, which means that rk\u03b1\nshould be large enough. Since \u03b1 is a part of the product, it should be large enough to satisfy\nthis condition.\nFinally, condition 5 states that 1 + 1\n2\u03b1rk ln\n\u00001 \u2212 p\n3\n\u0001 < 0. This implies that \u03b1rk ln\n\u00001 \u2212 p\n3\n\u0001\nshould be negative and large enough in magnitude. This is possible if \u03b1 is large enough, and\nthe term inside the logarithm is less than 1.\nIn conclusion, by carefully choosing the parameter \u03b1 and making it large enough, we can\nensure that instances of Model RB always meet the requirements. The value of \u03b1 has a sig-\nnificant impact on the complexity of the problem, and a larger value of \u03b1 tends to make the\nproblem harder to solve, ensuring that the requirements are met.\nThus far, we have demon-\nstrated the first key point\nin the schema: there exists\nan infinite set of Model\nRB instances, which\npossess the property of\nbeing fixed points when\ntheir satisfiability is al-\ntered through a mapping\noperation. The proba-\nbility of this property\nholding true tends to 1 as\nthe number of variables\napproaches infinity. In\nfact, as long as the prob-\nability is greater than 0,\nit implies that there are\ndefinite instances within\nthe sample space. Here,\nthe existence of infinitely\nmany instances with\na probability of 1 is a\nstronger conclusion.\n82\nPrompt: Assuming that we\u2019ve already constructed a set of difficult problems - a collection of\ninfinitely many instances of Model RB (a special type of CSP generation model) - and have\nproved that the satisfiable instances and unsatisfiable instances can be transformed into each\nother through a mapping operation, while the instances before and after the transformation\nhave the same parameter values. Now, we need to prove that these types of problems cannot\nbe solved in polynomial time. Please design 10 creative ideas.\nHaving constructed the\nModel RB with relevant\nproperties (i.e., 31),\nwe now revert to our\ninitial objective: to prove\nP!=NP through a proof\nby contradiction.\n1. Reduction from a known NP-hard problem: Show that solving Model RB is at least as\nhard as solving another well-known NP-hard problem by designing a polynomial-time\nreduction from that problem to instances of Model RB. If the reduction holds, then solv-\ning Model RB in polynomial time would imply solving the original NP-hard problem in\npolynomial time, leading to a contradiction.\n2. Time hierarchy theorem: Use the time hierarchy theorem to show that, even with the mapping op-\neration, the time complexity of solving Model RB instances cannot be reduced to polynomial time.\nThe theorem could be applied to prove the existence of problems that are harder than the polynomial\ntime problems.\n3. Circuit complexity: Demonstrate that any circuit solving instances of Model RB would\nhave to be of super-polynomial size, implying that the problem cannot be solved in poly-\nnomial time.\nThis approach relies on proving lower bounds on the circuit complexity\nrequired to solve Model RB instances.\n4. Randomized algorithms: Investigate the potential of randomized algorithms for solving\nModel RB instances. Show that no efficient randomized algorithm exists that can solve\nthese instances in polynomial time, even under the mapping operation, with high proba-\nbility.\n5. Non-deterministic Turing machines: Prove that instances of Model RB cannot be solved\nby non-deterministic Turing machines in polynomial time. This could involve encoding\nthe instances as languages and proving that these languages are not in NP. Fine-grained\n60\ncomplexity: Analyze the fine-grained complexity of Model RB instances, identifying specific poly-\nnomial-time solvable subproblems and proving that these subproblems cannot be combined in a\npolynomial time algorithm to solve the entire problem.\n6. Oracle separation: Using an oracle separation argument, demonstrate that there exists an\noracle relative to which Model RB instances cannot be solved in polynomial time. This\nwould show that, in some sense, solving Model RB instances is fundamentally harder than\nsolving problems in P.\n7. Average-case complexity: Investigate the average-case complexity of Model RB instances.\nShow that even under average-case conditions, the problem remains hard and cannot be\nsolved in polynomial time.\n8. Parameterized complexity: Analyze the parameterized complexity of Model RB instances,\nidentifying fixed-parameter tractable (FPT) subproblems and demonstrating that solving\nthe entire problem remains intractable, even when considering these FPT subproblems.\n9. Cryptographic assumptions: Assume the existence of cryptographic primitives with cer-\ntain security properties, such as one-way functions or trapdoor permutations. Show that\nif Model RB instances can be solved in polynomial time, this would imply breaking these\ncryptographic primitives, leading to a contradiction.\n83\n82\nPrompt: Interesting! How to prove that rigorously?\nTo prove that rigorously, you can follow these steps:\n1. Define the problem: Start by formally defining the Model RB instances and the transfor-\nmation mapping operation. Also, define the parameter values and constraints for each\ninstance.\n2. Identify subproblems: Break down the Model RB instances into smaller subproblems that can be\nsolved in polynomial time. For each subproblem, provide a clear definition, the input, and the out-\nput.\n3. Prove polynomial-time solvability: For each identified subproblem, prove that it can be\nsolved in polynomial time. You can do this by providing a polynomial-time algorithm for\neach subproblem and showing that the algorithm is correct and efficient.\n4. Show the subproblems are necessary: Prove that solving all the subproblems is necessary\nto solve the entire Model RB problem. This can be done by providing a reduction from the\nModel RB problem to the subproblems, showing that the solutions to the subproblems can\nbe combined to solve the overall problem.\n5. Prove subproblems are not sufficient: Now, show that solving all the subproblems is not sufficient to\nsolve the entire Model RB problem in polynomial time. This can be done by providing a counterex-\nample or using a complexity-theoretic argument to show that combining the subproblem solutions\nin polynomial time is not possible.\n6. Establish a lower-bound: Assuming that P \u0338= NP, show that the Model RB problem is NP-\nhard or NP-complete. This can be done by providing a reduction from a known NP-hard\nor NP-complete problem to the Model RB problem, proving that solving Model RB would\nalso solve the known NP-hard or NP-complete problem in polynomial time.\n61\n7. Show the transformation preserves hardness: Prove that the transformation mapping op-\neration between satisfiable and unsatisfiable instances preserves the hardness of the Model\nRB problem. This can be done by showing that the transformation operation can be per-\nformed in polynomial time and that it maintains the parameter values and constraints of\nthe original instances.\n8. Prove no polynomial-time algorithm: Now that you\u2019ve shown that the Model RB prob-\nlem is NP-hard or NP-complete, prove that there exists no polynomial-time algorithm for\nsolving it, unless P = NP.\n9. Analyze the fine-grained complexity: Investigate the fine-grained complexity of the Model\nRB problem and its subproblems, identifying any specific properties, structures, or param-\neter values that could potentially be exploited for a more efficient algorithm.\n10. Conclude: Sum up your findings and conclude that solving the Model RB problem in\npolynomial time is impossible, based on your rigorous analysis and proofs.\nRemember that providing rigorous proofs in complexity theory can be challenging and\noften requires deep understanding of algorithmic techniques and computational models. It\u2019s\nessential to be precise in your definitions and assumptions, and to build your arguments step\nby step, connecting the proofs to well-established results in complexity theory.\n84\nPrompt: What is the general time complexity of the CSP problem (let n denote the number\nof variables and d denote the domain size of each variable), please express it in O().\nThe general time complexity of the Constraint Satisfaction Problem (CSP) is exponential\nin the worst case, as it may require searching through all possible assignments of values to\nvariables. The time complexity can be expressed as O(dn), where n is the number of variables\nand d is the domain size of each variable.\nFollowing the proof\nlogic for the second\nkey in the schema, we\nemploy a proof by con-\ntradiction. However,\ndirectly prompting\non \u2018polynomial time\u2019\nevokes the model to\nretrieve related work\non the P!=NP problem.\nTherefore, we rephrase\nit as \u2018less than expo-\nnential time\u2019, which\nrepresents the lower\nbound for problems\nthat are not solvable in\npolynomial time.\n85\n84\nPrompt: Great! But as you said, O(dn) only represents the worst-case complexity. Now\nmy goal is to prove that a specific type of CSP problem cannot be solved in less than the\nexponential time complexity by contradiction. So my assumption is that this type of CSP\nproblem can be solved in less than the exponential time complexity. What O() should we\nuse in our assumption?\nIf your assumption is that the specific type of CSP problem can be solved in less than\nexponential time complexity, you can consider that it can be solved in polynomial time com-\nplexity. The assumption would then be that there exists an algorithm that solves the specific\nCSP problem in O(p(n)), where p(n) is a polynomial function of n (the number of variables).\nTo prove this by contradiction, you would assume that there exists an algorithm that solves\nthe specific CSP problem in O(p(n)) time, and then show that this leads to a contradiction,\nsuch as solving an NP-hard problem in polynomial time or violating a known complexity\nlower bound. This would then imply that the specific type of CSP problem cannot be solved\nin less than exponential time complexity.\n62\n86\n84, 85\nPrompt: O(p(n)) is too broad and loses the characteristics of the CSP problem itself. Is there\na way to simply adjust the complexity of the original CSP, but ensure that all cases that are\nless than the exponential time complexity are covered?\nYes, you can adjust the complexity of the original CSP to make it more specific while still\ncovering all cases that are less than the exponential time complexity. One way to do this is to\nexpress the complexity in terms of a function that is less than dn but still captures the essence\nof the CSP problem.\nFor example, you could assume that the specific type of CSP problem can be solved in O(d\nn\n\u03b1 ),\nwhere \u03b1 > 1 is a constant. This ensures that the assumption is strictly less than exponential\ntime complexity, as d\nn\n\u03b1 < dn for any \u03b1 > 1.\nThis adjusted complexity still captures the\nessence of the CSP problem while allowing you to cover all cases less than the exponential\ntime complexity.\nTo prove the contradiction, you would assume that there exists an algorithm that solves\nthe specific CSP problem in O(d\nn\n\u03b1 ) time, and then show that this leads to a contradiction.\nThis would then imply that the specific type of CSP problem cannot be solved in less than\nexponential time complexity.\n87\n82, 83, 86\nPrompt: What is the time complexity of the subproblems with n-1 variables and how many\nsubproblems are needed to solve the original problem?\nBased on 83, we inves-\ntigate the relationship\nbetween subproblems and\nthe original problem.\nLet\u2019s denote the given assumption as T(n) = O(dn/\u03b1), where \u03b1 > 1, d is the domain\nsize, and n is the number of variables in the Model RB problem.\nWe will now analyze\nthe time complexity of the subproblems with n \u2212 1 variables and determine the number of\nsubproblems needed to solve the original problem.\n1. Time complexity of subproblems with n \u2212 1 variables:\nDenote the time complexity of solving a subproblem with n \u2212 1 variables as T(n \u2212 1).\nSince we are assuming that Model RB can be solved in O(dn/\u03b1) time, we can expect the\ntime complexity of solving a subproblem with n \u2212 1 variables to be:\nT(n \u2212 1) = O(d(n\u22121)/\u03b1)\n2. Number of subproblems needed to solve the original problem:\nLet\u2019s denote the number of subproblems needed to solve the original problem as S(n).\nTo determine the relationship between S(n) and the time complexity of the subproblems,\nwe can consider the total time complexity of solving all the subproblems. If solving all\nthe subproblems is sufficient to solve the original problem, then the total time complexity\nof solving all subproblems should be equal to the time complexity of solving the original\nproblem:\nT(n) = S(n) \u2217 T(n \u2212 1)\nSubstituting our expressions for T(n) and T(n \u2212 1):\nO(dn/\u03b1) = S(n) \u2217 O(d(n\u22121)/\u03b1)\n63\nTo isolate S(n), we can divide both sides by O(d(n\u22121)/\u03b1):\nS(n) = O(dn/\u03b1)/O(d(n\u22121)/\u03b1)\nSimplifying the expression:\nS(n) = O(d(n/\u03b1)\u2212(n\u22121)/\u03b1)\nS(n) = O(d1/\u03b1)\nSince \u03b1 > 1, the expression 1/\u03b1 will be between 0 and 1.\nTherefore, the number of\nsubproblems needed to solve the original problem, S(n), will be proportional to a fractional\npower of the domain size d.\nPlease note that these calculations are based on the given assumption that Model RB can be solved in\nO(dn/\u03b1) time, and the actual relationship between the time complexity of the subproblems\nand the original problem may vary depending on the specific properties and structures of\nModel RB instances.\n88\n82, 83, 86, 87\nPrompt: Great! If we\u2019ve solved each subproblem (n \u2212 1 variables), how to solve the original\nproblem (n variables) through the solved subproblems?\nTo solve the original problem with n variables using the solutions to the subproblems with\nn \u2212 1 variables, you can try the following approach:\n1. Identify the connection between subproblems and the original problem: Analyze the structure and\nconstraints of the Model RB instances to understand how the n \u2212 1 variables subproblems\nare related to the n variables problem. Determine whether there is a specific variable that\ncan be eliminated or fixed to reduce the problem size.\n2. Combine the subproblem solutions: Once you have solved all the necessary subproblems\nwith n \u2212 1 variables, you will need to combine their solutions to form a solution for the\noriginal problem with n variables. This may involve merging partial assignments, updating\nconstraints, or finding a way to reintroduce the eliminated or fixed variable from step 1.\n3. Check for consistency: After combining the subproblem solutions, you will need to ensure\nthat the merged solution is consistent and satisfies all the constraints of the original prob-\nlem. If there are any conflicts or inconsistencies, you may need to backtrack and revise the\nsubproblem solutions or explore alternative ways of combining them.\n4. Verify the solution: Once you have obtained a consistent solution for the original problem,\nverify that it is correct by checking it against the problem\u2019s constraints and any additional\nconditions specific to the Model RB instances.\nPlease note that this general approach assumes that the subproblems with n \u2212 1 variables\nare closely related to the original problem with n variables, and that solving the subproblems\nand combining their solutions will lead to a solution for the original problem. The actual\nprocess of combining subproblem solutions and ensuring consistency may vary depending on\nthe specific properties and structures of Model RB instances. It is also important to consider\nthe complexity of combining the subproblem solutions, as it may impact the overall time\ncomplexity of solving the original problem.\n64\n89\n82, 83, 86, 87\nPrompt: Great! According to this, once we have the solutions to all the subproblems, how\nmany assignments should we try for an arbitrary variable at most to obtain the solution to\nthe original problem?\nGiven the time complexity of the subproblems T(n \u2212 1) = O(d(n\u22121)/\u03b1) and the number of\nsubproblems S(n) = O(d1/\u03b1), we can now analyze how many assignments we should try for\nan arbitrary variable to obtain the solution to the original problem.\nLet\u2019s assume that we have the solutions to all the subproblems with n \u2212 1 variables, and we\nneed to find the solution to the original problem with n variables. For an arbitrary variable,\nlet\u2019s denote the maximum number of assignments we need to try as A(n).\nIf we try each assignment for the arbitrary variable in combination with the solutions of\nthe subproblems, the total time complexity of solving the original problem should still be\nT(n) = O(dn/\u03b1). We can express this as follows:\nT(n) = A(n) \u2217 S(n) \u2217 T(n \u2212 1)\nSubstituting our expressions for T(n), S(n), and T(n \u2212 1):\nO(dn/\u03b1) = A(n) \u2217 O(d1/\u03b1) \u2217 O(d(n\u22121)/\u03b1)\nTo isolate A(n), we can divide both sides by O(d1/\u03b1) \u2217 O(d(n\u22121)/\u03b1):\nA(n) =\nO(dn/\u03b1)\nO(d1/\u03b1) \u2217 O(d(n\u22121)/\u03b1)\nSimplifying the expression:\nA(n) = O(dn/\u03b1\u22121/\u03b1\u2212(n\u22121)/\u03b1)\nA(n) = O(d(n\u2212(n\u22121))/\u03b1)\nA(n) = O(d1/\u03b1)\nSo, according to this analysis, we should try at most O(d1/\u03b1) assignments for an arbitrary vari-\nable to obtain the solution to the original problem. Keep in mind that this result is based on the\nassumption that Model RB can be solved in O(dn/\u03b1) time and that solving all subproblems\nis sufficient to solve the original problem. The actual number of assignments to try may vary\ndepending on the specific properties and structures of Model RB instances.\n90\n24, 25, 53, 89\nPrompt: With the mapping operation, let\u2019s prove that such Model RB instances cannot be\nsolved in O(dn/a) time rigorously.\nIn the definition of Model\nRB, the symbol \u03b1 is\nalready in use. To avoid\nambiguity, we substitute\nit with a new symbol, a.\nWe will prove this by contradiction. Suppose there exists an algorithm A that can solve\ninstances of Model RB in T(n) = O(d(n/a)) time, where a > 1. We will show that this leads\nto a contradiction, and therefore no such algorithm exists.\nFirst, let\u2019s consider the subproblems with n \u2212 1 variables. According to the assumption,\nwe can solve all these subproblems in O(d((n\u22121)/a)) time.\n65\nNow, using the mapping operation, we can transform a satisfiable instance into an unsatis-\nfiable instance or vice versa. Since the instances before and after the transformation have the\nsame parameter values, we can apply the algorithm A to the transformed instance as well.\nLet\u2019s focus on the case where we transform a satisfiable instance I into an unsatisfi-\nable instance I\u2032. In the original instance I, we can solve for the variables x1, x2, ..., xn\u22121 in\nO(d((n\u22121)/a)) time. For the unsatisfiable instance I\u2032, the algorithm A should also be able to\nsolve for the variables x1, x2, ..., xn\u22121 in O(d((n\u22121)/a)) time, according to the assumption.\nHowever, since we only made a small change in the permitted set of the constraint Cu,\nthe solutions to the subproblems for both instances I and I\u2032 should be almost the same.\nTherefore, once we have the solutions to all the subproblems for instance I, we can try at\nmost O(d1/a) assignments for an arbitrary variable xn to obtain the solution to the original\nproblem.\nNow, we have a contradiction.\nWe assumed that the algorithm A can solve instances\nof Model RB in O(dn/a) time, but we showed that once we have the solutions to all the\nsubproblems, we can find the solution to the original problem in O(d1/a) time. This means\nthat the overall time complexity for solving the original problem should be O(d((n\u22121)/a) \u2217\nd1/a) = O(dn/a), which is faster than the assumed time complexity of the algorithm A.\nTherefore, we conclude that there is no algorithm that can solve instances of Model RB in\nT(n) = O(dn/a) time, where a > 1.\n91\n24, 25, 53, 89, 90\nPrompt: Great! However, our study of complexity is not limited to specific algorithms, so\nwe should not restrict our analysis to a particular algorithm A. Perhaps it would be more\nappropriate to replace A with assignments. Besides, you cannot deduce the contradiction\nfrom time complexity O(dn/a).\nYou are correct. My previous explanation was not rigorous enough, and we should not\nrestrict our analysis to a particular algorithm A. Let\u2019s provide a more general proof using\nthe assumption that there exists an algorithm that can solve instances of Model RB in T(n) =\nO(dn/a) time, where a > 1. We will still prove this by contradiction.\nAssume there exists an algorithm that can solve instances of Model RB in T(n) = O(dn/a)\ntime, where a > 1. Let\u2019s denote this algorithm as A.\nNow, let\u2019s consider the subproblems with n \u2212 1 variables. According to the assumption,\nwe can solve all these subproblems in O(d(n\u22121)/a) time using algorithm A.\nLet\u2019s focus on the case where we transform a satisfiable instance I into an unsatisfiable\ninstance I\u2032 using the mapping operation. In the original instance I, we can solve for the\nvariables x1, x2, ..., xn\u22121 in O(d(n\u22121)/a) time using algorithm A.\nSimilarly, for the unsatisfiable instance I\u2032, algorithm A should also be able to solve for the\nvariables x1, x2, ..., xn\u22121 in O(d(n\u22121)/a) time.\nNotice that since we only made a small change in the permitted set of the constraint Cu,\nthe solutions to the subproblems for both instances I and I\u2032 should be almost the same.\nTherefore, once we have the solutions to all the subproblems for instance I, we can try at\nmost O(d1/a) assignments for an arbitrary variable xn to obtain the solution to the original\nproblem.\nNow, let\u2019s consider the overall time complexity for solving the original problem. Since we\nhave solved all the subproblems with n \u2212 1 variables in O(d(n\u22121)/a) time, and we can try at\nmost O(d1/a) assignments for an arbitrary variable xn, the overall time complexity for solving\nthe original problem should be O(d(n\u22121)/a \u00b7 d1/a) = O(dn/a).\nHowever, this leads to a contradiction. If instances of Model RB can be solved in O(dn/a)\n66\ntime, it should be possible to distinguish between satisfiable and unsatisfiable instances ef-\nficiently. But the mapping operation we introduced can transform satisfiable instances into\nunsatisfiable instances and vice versa, while maintaining the same parameter values. This\nmeans that it would be difficult to differentiate between satisfiable and unsatisfiable instances,\nwhich contradicts the assumption that there exists an algorithm that can solve instances of\nModel RB in O(dn/a) time.\nTherefore, we conclude that there is no algorithm that can solve instances of Model RB in\nT(n) = O(dn/a) time, where a > 1.\n92\n24, 25, 53, 89, 90, 91\nPrompt: Great! When n and d tend to infinity, can you prove the solutions to the subproblems\nfor both instances I and I\u2019 should be almost the same more rigorously?\nYou are right, we should be more rigorous in proving that \u2019the solutions to the subproblems\nfor both instances I and I\u2032 should be almost the same\u2019 as n and d tend to infinity. Let\u2019s provide\na more detailed argument for this part of the proof:\nConsider two instances I and I\u2032, where I is a satisfiable instance and I\u2032 is an unsatisfiable\ninstance created by applying the mapping operation on I. Recall that the mapping operation\nonly modifies a constraint Cu = (Xu, Ru) by replacing the permitted set Ru with a new\npermitted set R\u2032\nu obtained by removing two tuples (a1, a2) and (b1, b2), and adding two tuples\n(a1, b2) and (b1, a2).\nNow, let\u2019s analyze the impact of this modification on the subproblems with n \u2212 1 variables.\n1. Subproblems that do not involve variables from Xu: The mapping operation does not\naffect these subproblems, as their constraints are not modified. Therefore, the solutions to\nthese subproblems will be the same for both instances I and I\u2032.\n2. Subproblems that involve variables from Xu: Since the mapping operation only makes\na small change in the permitted set Ru, the majority of tuples in Ru will still be present in\nR\u2032\nu. As n and d tend to infinity, the size of the domains and the number of constraints will\ngrow, but the number of modified tuples remains constant (only two tuples are removed and\ntwo are added). Therefore, the proportion of modified tuples in R\u2032\nu compared to Ru becomes\nnegligible.\nAs a result, the solutions to the subproblems involving variables from Xu should be almost\nthe same for both instances I and I\u2032, as the majority of tuples in the permitted sets remain\nunchanged. The only difference between the solutions will be due to the modified tuples,\nwhose impact becomes negligible as n and d tend to infinity.\nIn conclusion, as n and d tend to infinity, the solutions to the subproblems for both in-\nstances I and I\u2032 should be almost the same, since the changes introduced by the mapping\noperation become negligible compared to the size of the domains and the number of con-\nstraints.\n93\n24, 25, 53, 89, 90, 91, 92\nPrompt: Great! Moreover, our assumption cannot infer try at most O(d1/a) assignments for an\narbitrary variable xn to obtain the solution of I\u2019. Please provide a strict proof of this point (n\nand d tend to infinity).\nYou are right; we need to provide a more rigorous proof for the claim that we can try at\nmost O(d1/a) assignments for an arbitrary variable xn to obtain the solution of I\u2032. Let\u2019s refine\nour argument.\nRecall that we have already shown that the solutions to the subproblems for both instances\n67\nI and I\u2032 should be almost the same when n and d tend to infinity, as the changes introduced\nby the mapping operation become negligible compared to the size of the domains and the\nnumber of constraints.\nNow, let\u2019s consider the overall time complexity for solving the original problem. Since we\nhave solved all the subproblems with n \u2212 1 variables in O(d(n\u22121)/a) time, we only need to\nanalyze the assignments for variable xn.\nLet Sn be the set of all possible assignments for variable xn, such that |Sn| = d. Since the\nsolutions to the subproblems for both instances I and I\u2032 are almost the same, we can focus\non the subset of Sn that is consistent with the solutions to the subproblems.\nLet S\u2032\nn be the subset of Sn that is consistent with the solutions to the subproblems, such\nthat |S\u2032\nn| \u2264 d1/a. Note that we have |S\u2032\nn| \u2264 d1/a because the proportion of modified tuples in\nR\u2032\nu compared to Ru becomes negligible as n and d tend to infinity, and therefore, the impact\nof the changes introduced by the mapping operation on the assignments for variable xn is\nalso negligible.\nNow, we only need to try the assignments in S\u2032\nn for variable xn to obtain the solution of I\u2032.\nSince |S\u2032\nn| \u2264 d1/a, the time complexity of trying all the assignments in S\u2032\nn is O(d1/a).\nIn conclusion, as n and d tend to infinity, we can try at most O(d1/a) assignments for\nan arbitrary variable xn to obtain the solution of I\u2032, as the changes introduced by the map-\nping operation become negligible compared to the size of the domains and the number of\nconstraints.\n94\n3, 4, 5\nPrompt: Great idea! Yes, we\u2019ve chosen to focus on SAT, through extremely hard CSP prob-\nlems. The previous work showed that CSP problems can be encoded into SAT problems\nthrough log-encoding. Now, we have constructed extremely difficult CSP examples. So,\nwhat is the relationship between the time complexity of SAT and the time complexity of the\ncorresponding CSP problem (let n denote the number of variables and d denote the domain\nsize of each variable)? Please let me know the conversion formula (explain the time complex-\nity of the original CSP problem and the time complexity of the corresponding SAT problem,\nand feel free to introduce new variables if necessary).\nWe have reverted to the\ninitial question and added\nthe initial turns into the\ndialogue history.\nWhen converting a CSP problem to a SAT problem, the time complexity depends on the\nsize of the CSP problem and the encoding used. The log-encoding method is a common\napproach to encode CSP problems into SAT problems. Given a CSP problem with n variables,\neach with a domain size of d, the time complexity can be analyzed as follows:\n1. Time complexity of the original CSP problem: In the worst case, solving a CSP problem\ncan take O(dn) time, as we may have to explore all possible combinations of variable\nassignments in the search space. This is an exponential time complexity with respect to\nthe number of variables n and the domain size d.\n2. Time complexity of the corresponding SAT problem after log-encoding: Log-encoding is a\ncompact representation that uses log2(d) binary variables to represent each CSP variable.\nTherefore, the total number of binary variables in the SAT problem will be n \u00b7 log2(d).\nNow, let\u2019s consider the time complexity of solving the SAT problem.\nThe worst-case\ntime complexity of solving SAT problems is exponential in the number of binary variables.\nSo, after log-encoding the CSP problem into a SAT problem, the time complexity will be\nO(2n\u00b7log2(d)). Using the properties of exponents, we can simplify this expression:\nO(2n\u00b7log2(d)) = O((2log2(d))n) = O(dn)\n68\nIn conclusion, the time complexity of the original CSP problem is O(dn), and the time\ncomplexity of the corresponding SAT problem after log-encoding is also O(dn). This implies\nthat solving the SAT problem after log-encoding has the same worst-case time complexity as\nsolving the original CSP problem.\n95\n94\nPrompt: Now we\u2019ve proved that an infinite set of instances of Model RB (a kind of CSP\nproblem) cannot be solved in O(dn/a) time, where a > 1, how can we use it to prove the\ntime complexity lower bound of SAT (please ignore the time of encoding or encoding when\ntransferring the problem)?\nThe result that an infinite set of instances of Model RB cannot be solved in O(dn/a) time,\nwith a > 1, can be used to prove a lower bound on the time complexity of SAT. The idea is to\nestablish a connection between the hardness of solving Model RB instances and the hardness\nof solving SAT instances.\nSince we can encode a Model RB instance as an SAT instance with log-encoding, we can\nuse the hardness of Model RB instances to derive the lower bound on the time complexity\nof SAT. Let\u2019s assume, for the sake of contradiction, that there exists an algorithm A that\nsolves SAT instances in time complexity O(2n/b), where b > 1. Note that this is a faster time\ncomplexity than the O(2n) time complexity we derived earlier.\nNow, consider an instance of Model RB with n variables and domain size d. We can encode\nthis instance into an SAT instance with n \u00b7 log2(d) binary variables. According to algorithm\nA, this SAT instance can be solved in time complexity O(2(n\u00b7log2(d))/b), which simplifies to:\nO(2(n\u00b7log2(d))/b) = O((2log2(d)/b)n)\n= O(dn/b)\nThis result contradicts the assumption that an infinite set of instances of Model RB cannot\nbe solved in O(dn/a) time, with a > 1, since we assumed b > 1. Therefore, our assumption\nthat there exists an algorithm A that solves SAT instances in time complexity O(2n/b), with\nb > 1, must be false.\nAs a result, we can conclude that there is no algorithm that solves SAT instances with a time com-\nplexity lower than O(2n). The time complexity lower bound of SAT is thus O(2n), ignoring the time\nfor encoding or decoding when transferring the problem.\n96\n94\nPrompt: If we prove that there is no algorithm that solves some SAT instances with a time\ncomplexity lower than O(2n), does this prove the existence of some NP-complete problems\nthat cannot be solved in polynomial time as the number of variables tends to infinity?\nYes, if we can prove that there is no algorithm that solves some SAT instances with a\ntime complexity lower than O(2n), it would indeed provide strong evidence for the existence\nof some NP-complete problems that cannot be solved in polynomial time as the number of\nvariables tends to infinity.\nThe reason for this is that SAT (satisfiability) is a well-known NP-complete problem. This\nmeans that every problem in NP can be reduced to an instance of SAT in polynomial time.\nIf we can show that some SAT instances cannot be solved in polynomial time (i.e., the time\ncomplexity is lower-bounded by O(2n)), it would imply that those particular instances of\nNP-complete problems cannot be solved in polynomial time either.\n69\nHowever, it\u2019s important to note that proving such a lower bound on the time complexity\nof SAT (or any NP-complete problem) is a major open question in computer science and is\nessentially equivalent to proving that P \u0338= NP. If P = NP, then all problems in NP can be solved\nin polynomial time, and there would be an algorithm that solves SAT instances with a time\ncomplexity lower than O(2n). Conversely, if P \u0338= NP, then there are NP-complete problems\nthat cannot be solved in polynomial time, and there would be no algorithm that solves SAT\ninstances faster than O(2n) for the worst cases.\n97\nPrompt: If we prove the existence of a specific type of NP-complete problem that cannot\nbe solved in polynomial time as the number of variables tends to infinity, would this be\nconsidered as proof that P != NP?\nYes, if you can rigorously prove the existence of a specific type of NP-complete problem\nthat cannot be solved in polynomial time as the number of variables tends to infinity, it would\nbe considered as proof that P != NP.\nThe conjecture P != NP is based on the idea that some problems are fundamentally more\ndifficult than others, and solving an NP-complete problem in polynomial time would mean\nthat all problems in the NP class can also be solved in polynomial time (since NP-complete\nproblems are the most difficult problems in NP). If you can prove that at least one NP-\ncomplete problem cannot be solved in polynomial time, then it would mean that not all\nproblems in NP can be solved in polynomial time, and therefore P != NP.\n4\nDiscussions\nAI as an Innovation Navigator\nOur pilot study introduces LLM4Science, where LLMs act\nas innovation navigators, collaborating with humans to conquer substantial scientific chal-\nlenges. In earlier paradigms, AI primarily functions as a support tool to carry out steps\npredetermined by humans. In contrast, the LLM4Science paradigm elevates AI to the role\nof a collaborative peer alongside humans. Specifically, LLMs steer innovation, and humans\nneed to pose questions to spark the creative ideas of LLMs. The proposed paradigm high-\nlights the potential for AI to extrapolate novel knowledge and drive scientific innovation\nautonomously. Besides, efficient sampling allows LLMs to effortlessly explore solutions, ac-\ncelerating idea generation and verification.\nGeneral-Purpose vs. Task-Specific\nOur exploration exemplifies that LLMs can serve as\ngeneral-purpose innovation navigators across various fields. In previous work, AI typically\ninvolves task-specific models tailored for particular problems, such as protein structure pre-\ndiction (Jumper et al., 2021) in AI4Science. In comparison, the proposed paradigm employs\nAI as a general-purpose engine for extensive tasks, harnessing comprehensive capabilities\nsuch as planning, coding, and mathematical deductions.\nThe newfound potential allows\nscientists to harness the power of LLMs across various fields and tasks.\nLLMs as Polymaths\nLLMs are interdisciplinary polymaths in terms of both breadth and\ndepth of knowledge. The extensive expertise across domains allows LLMs to generate di-\nverse ideas. At the same time, their profound understanding enables them to tackle prob-\nlems as experts, such as conducting mathematical deductions and generating code. In the\nLLM4Science paradigm, the fluid transition between experts of different domains (i.e., the\nrole-playing strategy described in Appendix A) facilitates interdisciplinary discoveries.\n70\nSocratic Reasoning: Teach vs. Coach\nWe propose Socratic reasoning as a general frame-\nwork to prompt LLMs for complex tasks. Different from conventional prompting strategies\nwhich \u2018teach\u2019 LLMs to interpolate existing knowledge, Socratic reasoning \u2018coaches\u2019 LLMs\nand stimulates them to extrapolate new knowledge. In this work, we first use the trans-\nformation pattern to view the problem from a higher perspective (1-2). We then iteratively\napply transformation, deduction, and decomposition patterns to generate rough ideas (3-13)\nand a preliminary schema (14). According to the schema, we stimulate the model to construct\nModel RB of extremely hard instances (15-81). Assuming that such problems can be solved\nin less than exponential time complexity, a series of deduction patterns encourage GPT-4\nto derive a contradiction when determining the satisfiability of a Model RB instance and\nits corresponding transformed instance (82-93). The concluding deduction and integration\npatterns lead GPT-4 to establish that \u201cP \u0338= NP\u201d, thereby completing the proof (94-97). Like\ndesign patterns in software engineering, the five prompt patterns in Socratic reasoning pro-\nvide modular, adaptable templates for LLMs to navigate the solution space effectively. More\nimportantly, appropriate abstraction is important for the automation of the whole process.\nMathematics as a Native Language\nMathematics is often regarded as \u201cthe language of sci-\nence\u201d because it provides a precise, universal, and consistent way to describe and analyze\nthe world.\nNevertheless, previous mathematical tools (e.g., Mathematica and Lean) have\nbeen limited to capturing formal calculations and deductions, without grasping the physical\nmeanings behind symbols and equations. Our research unveils that LLMs tend to master\nmathematics as a native language. Considering the excellent code-switching performance of\nLLMs, they can seamlessly utilize natural language and mathematical language for complex\nreasoning. Therefore, LLMs can comprehend and contemplate the world through a mathe-\nmatical lens, leading to infinite potential in solving more fundamental problems.\n5\nLimitations and Future Work\nOur work sheds light on LLM for Science, which is promising in scientific discoveries. We\nshow that the solution space of LLMs encompasses strategies to address complex problems.\nBesides, we would like to suggest some limitations and potential future directions. First,\nthe workflow of LLM for Science can be further automated. Our current process relies on\nhuman guidance and inspection. In this process, multiple samplings (see detailed settings\nin Appendix B) and manual verification are still required, leading to challenges in terms of\nreproducibility. In the future, increased automation (Saunders et al., 2022; Bai et al., 2022) can\nsignificantly improve the efficiency and controllability of LLM for Science. Second, this paper\npresents the entire Socratic reasoning history between humans and GPT-4 in a flattened for-\nmat. Reorganizing reasoning processes can make AI-generated proofs more logically robust\nand reader-friendly. Third, LLMs can use external tools (e.g., Mathematica) for deterministic\ncomputations during the proving process. Besides, we can augment LLMs with laboratory\nautomation, which is advantageous for fields that require hands-on experiments with equip-\nment (such as chemistry and biology). Last but not least, our study serves as a promising ex-\nploration built upon Xu and Zhou (2023). Future research endeavours could delve into more\nopen questions in various research fields, such as Riemann Hypothesis (Riemann, 1859).\n71\nContributions\nQ.D. prompted large language models. L.D. and Q.D. contributed to the conception and\ndesign of the work. K.X. contributed proof verification and intuitions. K.X., G.Z. and Y.H.\ncontributed technical advice and ideas. Z.S. advised on Q.D.\u2019s research. F.W. managed and\nadvised on the project. All authors contributed to the drafting and revising of the manuscript.\nReferences\nBai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirho-\nseini, A., McKinnon, C., et al. (2022). Constitutional AI: Harmlessness from ai feedback.\narXiv preprint arXiv:2212.08073.\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T.,\nLi, Y., Lundberg, S., et al. (2023). Sparks of artificial general intelligence: Early experiments\nwith GPT-4. arXiv preprint arXiv:2303.12712.\nCook, S. A. (2000). The P versus NP problem. Clay Mathematics Institute.\nFortnow, L. (2022). Fifty years of P vs. NP and the possibility of the impossible. Communica-\ntions of the ACM, 65(1):76\u201385.\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool,\nK., Bates, R., \u017d\u00eddek, A., Potapenko, A., et al. (2021). Highly accurate protein structure\nprediction with alphafold. Nature, 596(7873):583\u2013589.\nOpenAI (2023). Gpt-4 technical report. ArXiv, abs/2303.08774.\nRiemann, B. (1859). Ueber die anzahl der primzahlen unter einer gegebenen grosse. Ges.\nMath. Werke und Wissenschaftlicher Nachla\u00df, 2(145-155):2.\nSaunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J. (2022). Self-critiquing\nmodels for assisting human evaluators. arXiv preprint arXiv:2206.05802.\nWang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z., Chandak, P., Liu, S., Van Katwyk, P.,\nDeac, A., Anandkumar, A., Bergen, K., Gomes, C. P., Ho, S., Kohli, P., Lasenby, J., Leskovec,\nJ., Liu, T.-Y., Manrai, A., Marks, D., Ramsundar, B., Song, L., Sun, J., Tang, J., Veli\u02c7ckovi\u00b4c, P.,\nWelling, M., Zhang, L., Coley, C. W., Bengio, Y., and Zitnik, M. (2023). Scientific discovery\nin the age of artificial intelligence. Nature, 620(7972):47\u201360.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. (2022).\nChain-of-thought prompting elicits reasoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nXu, K. and Li, W. (2000). Exact phase transitions in random constraint satisfaction problems.\nJournal of Artificial Intelligence Research, 12:93\u2013103.\nXu, K. and Zhou, G. (2023). SAT requires exhaustive search. arXiv preprint arXiv:2302.09512.\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. (2023).\nTree of thoughts: Deliberate problem solving with large language models. arXiv preprint\narXiv:2305.10601.\n72\nA\nInstruction Format\nSystem Message: \u201cYou are {}.\u201d\nTurn Index\na wise philosopher\n1-2\na mathematician and computer theory expert, good at innovation\nand thinking\n3-30, 82-97\nan expert mathematician collaborator who is good at proving\ntheorems\n31-46, 50-63\na mathematician skilled in probability theory\n47-49\na mathematician skilled in probability theory, numerical methods,\nand combinatorics\n64-81\nTable 2: Detailed system messages for each turn.\nWe introduce five distinct roles as our collaborative provers (the roles are set through the\nsystem message of GPT-4). Different roles are specialized in proving different parts. For\ninstance, the role of \u2018a mathematician and computer theory expert, good at innovation and\nthinking\u2019 is more adept at brainstorming, proposing innovative and open-ended suggestions,\nwhile \u2018an expert mathematician collaborator who is good at proving theorems\u2019 excels in\ngeneral theorem proving, and so on. The detailed role distribution, including the dialogue\nturns they participate in, can be found in Table 2.\nB\nSettings\nParameter\nValue\nModel\nGPT-4\nEndpoint Version\n2023-03-15-preview\nTemperature\n0.7\nTop Probabilities\n0.95\nStop\nNone\nFrequency Penalty\n0\nPresence Penalty\n0\nTable 3: Hyper-parameters and API information.\nOur experiments employ the openai.ChatCompletion function from the OpenAI Python\nlibrary. The specific API version and parameter settings utilized in our research are delin-\neated in Table 3. Notice that we used the internally provided endpoints, so differences with\npublic API versions are as expected. The dialogue history is available at https://aka.ms/\nPvsNP-notebook.\n73\n"
  },
  {
    "title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy",
    "link": "https://arxiv.org/pdf/2309.06126.pdf",
    "upvote": "16",
    "text": "AstroLLaMA\n: Towards Specialized Foundation Models in Astronomy\nTuan Dung Nguyen1, 2*, Yuan-Sen Ting2, 3*, Ioana Ciuc\u02d8a2*\nCharles O\u2019Neill2\u2020, Ze-Chang Sun4\u2020, Maja Jab\u0142o\u00b4nska2\u2020, Sandor Kruk5\u2020\nErnest Perkowski5, Jack Miller2, Jason Li6, Josh Peek7 Kartheik Iyer8,\nTomasz R\u00f3\u02d9za\u00b4nski2,9, Pranav Khetarpal10, Sharaf Zaman2, David Brodrick2\nSergio J. Rodr\u00edguez M\u00e9ndez2, Thang Bui2, Alyssa Goodman11, Alberto Accomazzi12,\nJill Naiman13, Jesse Cranney2, Kevin Schawinski14, UniverseTBD\n1University of Pennsylvania, United States\n2Australian National University, Australia\n3Ohio State University, United States\n4Tsinghua University, China\n5European Space Astronomy Centre, Spain\n6Learning Machines, Australia\n7Space Telescope Science Institute, United States\n8Columbia University, United States\n9Wroc\u0142aw University, Poland\n10Indian Institute of Technology Delhi, India\n11Harvard University, United States\n12NASA Astrophysics Data System, Harvard & Smithsonian, United States\n13University of Illinois at Urbana-Champaign\n14Modulos AG\nAbstract\nLarge language models excel in many human-\nlanguage tasks but often falter in highly spe-\ncialized domains like scholarly astronomy. To\nbridge this gap, we introduce AstroLLaMA,\na 7-billion-parameter model fine-tuned from\nLLaMA-2 using over 300,000 astronomy ab-\nstracts from arXiv.\nOptimized for tradi-\ntional causal language modeling, AstroLLaMA\nachieves a 30% lower perplexity than Llama-\n2, showing marked domain adaptation. Our\nmodel generates more insightful and scientif-\nically relevant text completions and embed-\nding extraction than state-of-the-arts founda-\ntion models despite having significantly fewer\nparameters. AstroLLaMA serves as a robust,\ndomain-specific model with broad fine-tuning\npotential.\nIts public release aims to spur\nastronomy-focused research, including auto-\nmatic paper summarization and conversational\nagent development.\n1\nIntroduction\nThe advent of Large Language Models (LLMs) has\nsparked interdisciplinary interest thanks to a conflu-\nence of factors: accumulation of massive datasets,\nleaps in computational power and breakthroughs in\nneural architectures. Flagship models like GPT-4\n(OpenAI, 2023), PaLM (Chowdhery et al., 2022;\nGoo) and LLaMA (Touvron et al., 2023; Meta,\n2023) have exhibited exceptional versatility in a\nvariety of tasks from logical reasoning and compre-\nhension to creative writing, often accomplished via\n*Lead contribution. Email: joshtn@seas.upenn.edu\n\u2020Major contribution.\nmethods like prompting, fine-tuning, and human-\nin-the-loop reinforcement learning.\nThe astronomy discipline presents both a unique\nchallenge and a fertile ground for the application\nof LLMs. First, the corpus of scholarly texts in\nastronomy likely constitutes but a minuscule por-\ntion of the data on which generic LLMs are trained,\nresulting in limitations like hallucinations in fa-\nvor of more \u201cgeneric\u201d responses. Second, the na-\nture of astronomical research often involves cross-\ndisciplinary insights due to universally applicable\nphysical processes.\nWhen well-curated, LLMs\ncould meaningfully assist in hypothesis generation.\nExisting scales based on in-context prompting\nand instruction learning, primarily involving GPT-\n4, have already demonstrated significant potential\nfor generating substantive hypotheses (Ciuc\u02d8a and\nTing, 2023; Ciuc\u02d8a et al., 2023). Further, the as-\ntronomy community\u2019s \u201copen sky\u201d policy, which\ngrants public access to the majority of its datasets\neither immediately or after a brief proprietary pe-\nriod (Almeida et al., 2023; Fabricius et al., 2021),\npairs well with the wealth of resources available\nin archives like NASA\u2019s Astrophysics Data System\n(Accomazzi et al., 2015; Borgman and Wofford,\n2021). Such an open-access policy can facilitate\ndeep engagement with the astronomical literature.\nDespite their general capabilities, LLMs fre-\nquently lag behind specialized, smaller models in\ndomain-specific applications. This disparity stems\nfrom two primary factors: (i) the eclectic nature\nof the training datasets, which dilutes the focus\non specialized subjects, and (ii) the design ethos\narXiv:2309.06126v1  [astro-ph.IM]  12 Sep 2023\n0\n50\n100\n150\n200\nProcessed tokens (millions)\n7\n8\n9\n10\nTraining perplexity\nEpoch 1\nEpoch 2\nEpoch 3\nFigure 1: Learning curve of AstroLLaMA during its\nfine-tuning on the arXiv astrophysics dataset.\nThe\nFig.tracks the evolution of perplexity, a measure of\nthe model\u2019s next-token prediction performance. The\nlight blue curve shows the training perplexity at each\nAdamW update step, while the dark black curve pro-\nvides a smoothed average taken over 10-step intervals.\nof LLMs as \u201cfoundation models\u201d meant for sub-\nsequent fine-tuning tailored to specific tasks. The\nexisting landscape for fine-tuned LLMs in astron-\nomy remains limited, however. To our knowledge,\nthe only existing specialized model is astroBERT\n(Grezes et al., 2021), which has 110 million pa-\nrameters, trained on nearly 400,000 ADS papers.\nBut as an non-generative model, the utility of as-\ntroBERT remains limited to discriminative tasks.\nMotivated by these gaps, we present AstroL-\nLaMA, a state-of-the-art generative language\nmodel fine-tuned from LLaMA-2. Our model lever-\nages a corpus of 300,000 astronomy abstracts from\narXiv and boasts an architecture approximately 67\ntimes larger than that of astroBERT. AstroLLaMA\naspires to build upon astroBERT\u2019s foundation by\noffering improved performance in generating spe-\ncialized information.\n2\nAstroLLaMA\nIn this section, we discuss AstroLLaMA\u2019s imple-\nmentation, focusing on the curation of its dataset,\nbase model architecture, and fine-tuning settings.\n2.1\nDataset\nWe derive our dataset from the arXiv repository,\navailable on Kaggle.\u2020 Our curated subset focuses\non papers classified under the astrophysics category\n(astro-ph), resulting in a collection of 326,238\narticles spanning from April 1992 to July 2023. We\nextract the these papers\u2019 abstracts to form a corpus\n\u2020https://www.kaggle.com/Cornell-University/\narxiv\nconsisting of approximately 95 million tokens. The\nmedian length of these abstracts is 291 tokens. To\nenable effective model evaluation, we randomly\ndesignate 20% of this curated dataset for testing.\n2.2\nBase Model\nOur base model is LLaMA-2, a 6.7 billion-\nparameter model developed by Meta (Meta, 2023).\nOriginally trained on a corpus containing 2 trillion\ntokens, LLaMA-2 features a context window of\n4,096 tokens. For tokenization, the model employs\na bytepair encoding strategy (Sennrich et al., 2016;\nKudo and Richardson, 2018), incorporating a vo-\ncabulary set of 32,000 unique tokens.\n2.3\nFine-tuning Settings\nFor the fine-tuning phase, we rely on our curated\ntraining set described in Section 2.1, which in-\ncludes 77 million tokens. Special [BOS] (Begin-\nning Of Sequence) and [EOS] (End Of Sequence)\ntokens are prepended and appended to each training\nsequence. These sequences are then concatenated\nand divided into fixed-length chunks, each compris-\ning 512 tokens.\nThe fine-tuning process follows the causal lan-\nguage modeling objective employed during the\nmodel\u2019s pre-training phase. We use the AdamW\noptimizer (Loshchilov and Hutter, 2018) with hy-\nperparameters \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22125 and\na batch size of 32. The learning rate follows a co-\nsine schedule with a linear warmup to a peak value\nof 3 \u00d7 10\u22124 in the first 10% of the optimization\nsteps and a final learning rate of 10% of its peak.\nAdditional settings include weight decay and gra-\ndient clipping values of 0.1 and 1.0, respectively.\nWe fine-tune LLaMA over nearly three epochs,\ncorresponding to about 230 million processed\ntokens, using four NVIDIA A100 GPUs, each\nequipped with 40GB of VRAM. To maximize re-\nsource efficiency, we employ 4-bit quantization\nand utilize LoRA, a technique based on low-rank\nmatrix decomposition (Hu et al., 2021). We set\nLoRA\u2019s hyperparameters \u03b1 and dropout rate to 32\nand 0.05, respectively. The entire process is facili-\ntated through the Hugging Face Python library.\n2.4\nFine-Tuning Evaluation\nFig. 1 depicts the performance of AstroLLaMA\nduring its fine-tuning phase. Here, we present per-\nplexity, a commonly used metric for evaluating\ncausal language models. Perplexity is defined as\nThe Magellanic Stream (MS) - an enormous ribbon of gas spanning 140\u2218 of the southern sky \ntrailing the Magellanic Clouds - has been exquisitely mapped in the five decades since its \ndiscovery. However, despite concerted efforts, no stellar counterpart to the MS has been \nconclusively identified. This stellar stream would reveal the distance and 6D kinematics of the MS, \nconstraining its formation and the past orbital history of the Clouds. We have been conducting a \nspectroscopic survey of the most distant and luminous red giant stars in the Galactic outskirts. From \nthis dataset, we have discovered a prominent population of 13 stars matching the extreme angular \nmomentum of the Clouds, spanning up to 100\u2218 along the MS at distances of 60\u2212120 kpc. \nFurthermore, these kinemetically-selected stars lie along a [\u03b1/Fe]-deficient track in chemical space \nfrom \u22122.5<[Fe/H]<\u22120.5, consistent with their formation in the Clouds themselves. We identify \nthese stars as high-confidence members of the Magellanic Stellar Stream. Half of these stars are \nmetal-rich and closely follow the gaseous MS, whereas the other half are more scattered and metal-\npoor. We argue that the metal-rich stream is the recently-formed tidal counterpart to the MS, and \nspeculate that the metal-poor population was thrown out of the SMC outskirts during an earlier \ninteraction between the Clouds. The Magellanic Stellar Stream provides a strong set of constraints - \ndistances, 6D kinematics, and birth locations - that will guide future simulations towards unveiling \nthe detailed history of the Clouds. \nThe Magellanic Stream (MS) - an enormous ribbon of gas spanning 140\u2218 of the southern\nsky trailing the Magellanic Clouds - has been exquisitely mapped in the five decades since\nits discovery. However, despite concerted efforts, no stellar counterpart to the MS has been\nconclusively identified. This stellar stream would reveal the distance and 6D kinematics of\nthe MS, constraining its formation and the past orbital history of the Clouds. We propose a \nnovel approach to search for the elusive stellar counterpart of the Magellanic Stream. By \nemploying advanced photometric and spectroscopic techniques, combined with state-of-\nthe-art data mining and machine learning algorithms, we aim to systematically analyze and \ncross-reference multi-wavelength survey data to identify potential candidate stars \nbelonging to the hypothesized stellar stream. Our methodology involves leveraging the \ndistinctive chemical signatures and proper motions associated with stars originating from \nthe Magellanic Clouds, allowing us to differentiate them from the foreground and \nbackground contaminants. If successful, this discovery would not only provide essential \ninsights into the origin and evolution of the Magellanic Stream, but also enhance our \nunderstanding of the interaction between the Magellanic Clouds and the Milky Way. \nFurthermore, the proposed approach establishes a powerful framework for investigating \nother unresolved galactic structures and tracing their underlying dynamical histories.\nThe Magellanic Stream (MS) - an enormous ribbon of gas spanning 140\u2218 of the southern sky trailing the Magellanic Clouds - has\nbeen exquisitely mapped in the five decades since its discovery. However, despite concerted efforts, no stellar counterpart to the MS\nhas been conclusively identified. This stellar stream would reveal the distance and 6D kinematics of the MS, constraining its\nformation and the past orbital history of the Clouds. We 1) present a new method to select MS candidate stars from wide field \noptical imaging, 2) apply this method to the MS, 3) compare the results to existing catalogs, and 4) identify the most promising MS \ncandidate stars. We apply our method to 2547 stars in the Gaia EDR3 catalog that have Gaia proper motions consistent with the MS \nand use these as a pilot sample to optimize our method. We then apply our method to the entire Gaia EDR3 catalog to identify \n46,000 stars that are candidates for the MS, and we use the Gaia-ESO Survey to identify 13,000 of these stars that are likely to be \nMS candidates. We identify 266 stars with (G-K) colours and [Fe/H] that are consistent with previous estimates of the MS. These \nstars have proper motions and radial velocities that are consistent with the MS, and we place them on a 3D map of the MS. We find \na previously unknown stellar stream, which we call the Southeastern Stream (SES), in the direction of the Clouds and MS. The SES \nhas an M31 distance of 32.2+/-0.5 kpc and a 6D velocity of 136+/-10 km/s. The SES is about 20 times brighter in stars than the MS \nand about 40 times brighter in stars than the SMC. The SES is also 2.7 times more metal-rich than the MS, which suggests that it is \na separate population. \nThe Magellanic Stream (MS) - an enormous ribbon of gas \nspanning 140\u2218 of the southern sky trailing the Magellanic\nClouds - has been exquisitely mapped in the five decades \nsince its discovery. However, despite concerted efforts, no \nstellar counterpart to the MS has been conclusively \nidentified. This stellar stream would reveal the distance and \n6D kinematics of the MS, constraining its formation and \nthe past orbital history of the Clouds. We \ufb01nd a stellar \nstream in the MS, with a 6D velocity of 100 km s\u22121, and a \ndistance of 100 kpc. The stream is 100 pc wide and 1000 \npc long, and is consistent with the MS. The stream is 1000 \ntimes more massive than the Magellanic Clouds, and is \nlikely to be a tidal stream from the Large Magellanic \nCloud.\nOriginal abstract\nCompleted by GPT-4\nCompleted by LLaMA-2\nCompleted by AstroLLaMA\nFigure 2: Completion of an abstract from the arXiv database (ID: 2306.15719) using three different models: GPT-4,\nLLaMA-2, and AstroLLaMA. Each model is prompted with the same short text snippet, highlighted in their\nrespective boxes. GPT-4 tends to produce more generic statements, lacking domain-specific nuance. AstroLLaMA\ndemonstrates the most robust completion, offering more relevant concepts and deeper insights specific to the field of\nastronomy, thus significantly outperforming LLaMA-2 and GPT-4.\nthe exponentiation of the training loss, with lower\nvalues indicating a better fit.\nOur initial observations reveal that LLaMA-2\nperforms suboptimally on our dataset, with an av-\nerage perplexity close to 10. By the conclusion\nof three epoch, AstroLLaMA achieves an average\nperplexity of 6.55. This represents a 32.5% reduc-\ntion in perplexity compared to the base LLaMA-2\nmodel, signifying a substantial improvement in the\nmodel\u2019s predictive accuracy.\n3\nResults\nAs illustrated in the previous section, AstroL-\nLaMA outperforms its non-fine-tuned counterpart,\nLLaMA-2, in terms of context-awareness during\ntoken prediction within astronomy abstracts. To\ndelve deeper into the advantages of fine-tuning, we\nassess AstroLLaMA\u2019s general abilities in two key\naspects: text generation and embedding space\nquality. We compare its performance against mul-\ntiple models, including LLaMA-2, GPT-4 and GPT-\n3 (ada-002) to provide a comprehensive evaluation.\nRegarding text generation, we task AstroL-\nLaMA, LLaMA-2 and GPT-4 with completing var-\nious astronomy-related abstracts, an example of\nwhich is presented in Fig. 2. Each model is given\nthe first few sentences of an abstract as a prompt,\nallowing us to gauge its ability to comprehend the\ncontext and generate a meaningful continuation.\nFor GPT-4, we utilize ChatGPT and specifically\nprompt it to limit the completion to a single para-\ngraph. AstroLLaMA and LLaMA-2 are deployed\nusing standard sampling methods, with the temper-\nature set to 0.3 and a maximum new tokens limit of\n1,024. We find that altering the temperature setting\ndoes not substantively improve LLaMA-2\u2019s results.\nOur observations largely echo the patterns de-\npicted in Fig. 2. LLaMA-2 often deviates from the\nintended context after generating only a short and\noften off-topic continuation, resulting in inferior\ncompletions. While GPT-4 produces more coher-\nent text, its responses are too generic to capture\nthe nuanced understanding required in the astron-\nomy domain. Even when explicitly prompted to\nfocus on astronomy-related topics, GPT-4\u2019s gener-\nated text remains largely off-target or generically\napplicable rather than domain-specific.\nIn stark contrast, AstroLLaMA exhibits remark-\nable context-awareness in its completions by show-\ning a deep understanding of astronomical concepts.\nFor example, in Fig. 2, AstroLLaMA comprehends\nthat an effective search for stars in the Magellanic\nStream involves a three-step process: initial wide-\nfield imaging, followed by refinement using astro-\nmetric data from Gaia, and then further curation\nwith spectroscopic data. The model also under-\nstands Gaia-ESO is surveying the southern sky\nand hence can observe (part of) the Magellanic\nStream. It also demonstrates nuanced knowledge\nof the Magellanic Stream, understanding the impor-\ntance of bifurcation within the stream. As a result,\nit appropriately completes the text by discussing\nthe southeast stream and exploring metallicity dif-\nferences to ascertain their origins.\nRegarding embedding space quality, we as-\nsess models\u2019 ability to reflect semantic similari-\nties among astronomy texts. We randomly choose\n10,000 abstracts from our dataset and embed them\nusing AstroLLaMA and GPT-3. Specifically, we\nuse OpenAI\u2019s API to invoke the text embedding\nfunction for GPT-3 (ada-002). To get text embed-\ndings from AstroLLaMA, we pass an input through\nthe model and extract its final hidden states, which\ncontain embeddings for all tokens in the input.\nThen, we omit the [BOS] token and take the av-\nerage of all other tokens\u2019 embeddings to get the\nfinal result. Finally, for each pair of abstracts we\ncalculate their cosine similarity (the normalised dot\nproduct) between on their vector embeddings.\nThe top panel of Fig. 3 presents the distribution\nof these pairwise similarities for the two embed-\nding methods. We find that the embeddings by\nGPT-3 are overly generic with similarities cluster-\ning around relatively high values of 0.7\u20130.9, sug-\ngesting a lack of discriminative power (most papers\nare embedded very similarly). AstroLLaMA\u2019s em-\nbeddings, on the other hand, exhibit much higher\nvariance within each bin. This suggests that our\nfine-tuned model is more adept at representing the\nspecialized semantic variance inherent to the field\nof astronomy, which may enable a more granu-\nlar representation of astronomical content and can\nfacilitate better document retrieval and semantic\nanalysis.\nThe bottom panel of Fig. 3 provides two repre-\nsentative examples where AstroLLaMA and GPT-3\nclassifications diverge. In the first example, GPT-3\nfixates on the keyword \u2018magnetized,\u2019 resulting in\nan inflated similarity score, despite the contexts be-\ning markedly different. AstroLLaMA, on the other\nhand, successfully distinguishes between these dis-\nparate contexts. In the second example, AstroL-\nLaMA accurately identifies that the study of Spitzer\nis closely related to star formation. GPT-3, how-\never, fails to make this connection due to the ab-\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPairwise cosine similarity\n0\n100\n200\n300\n400\n500\nDensity + Shift\nGPT-3 (ada) embedding\nAstroLLaMA embedding\nFigure 3: Top: Distribution of pairwise cosine similari-\nties among 10,000 randomly selected abstracts from our\ncorpus, divided into 10 equal bins based on similarity\nlevels from GPT-3. Bottom: Two representative exam-\nples illustrating divergent cosine similarity values when\ncomparing AstroLLaMA and GPT-3 embeddings.\nsence of matching keywords.\n4\nLimitations and Future Directions\nIn this work, we introduce AstroLLaMA, a 7-\nbillion-parameter language model fine-tuned on a\ndataset encompassing over 300,000 abstracts from\nastronomical research papers. Compared to its\nbase model, LLaMA-2, and even GPT-4, a cur-\nrent state-of-the-art general LLM, AstroLLaMA\nexhibits marked improvements in generating high-\nquality abstracts with a competent grasp of relevant\ninformation in this literature.\nAstroLLaMA is not without limitations, never-\ntheless. The most salient is the model\u2019s knowledge\ngaps in certain areas of astronomy: in Fig. 2, As-\ntroLLaMA\u2019s estimation of potential star candidates\nfrom Gaia-ESO data is notably inaccurate. To ad-\ndress such issues, we are in the process of enriching\nAstroLLaMA\u2019s training set with not just abstracts\nbut the full LaTeX sources of existing astronomy\narticles, thereby expanding the token count by ap-\nproximately two orders of magnitude. Another\nconcern lies in the model\u2019s tendency to generate\nhallucinated or fictitious numerical data, an issue\nlikely attributed to our focus on reducing perplexity\nrather than explicitly steering the model towards\nfactual accuracy. The release of AstroLLaMA aims\nto facilitate community engagement, both for ad-\ndressing these inaccuracies and for refining its bal-\nance between \u201cfaithfulness\u201d (respecting scientific\nevidence and accuracy) and \u201ccreativity\u201d (being able\nto come up with interesting hypotheses).\nAstroLLaMA stands as a compelling prototype\nfor specialized LLMs in astronomy, showing supe-\nrior context-aware capabilities compared to GPT-\n4 despite having much fewer parameters. It not\nonly paves the way for improved performance in\ntasks like question-answering, scientific summa-\nrization and hypothesis generation but applies also\nto multi-modal models (Liu et al., 2023). We have\nmade the AstroLLaMA\u2019s weights and its training\ndata publicly available\u2020 for researchers interested\nin leveraging LLMs for astronomy-centric applica-\ntions. Along with this, we are establishing various\n\u201cplaygrounds\u201d on Hugging Face to invite interested\nreaders to further adapt and refine this robust start-\ning point for a variety of relevant downstream tasks.\nAcknowledgments\nWe are deeply grateful to the Microsoft Accelerate\nFoundation Models Research Initiative for enabling\nus to fast-track our project. Thanks to advanced AI\nplatform from Microsoft Research, we have been\nable to significantly expedite our efforts in using\nlanguage models to analyze astronomical literature.\n\u2020https://huggingface.co/universeTBD/astrollama\nEthics Statement\nWe obtain the pre-trained weights for LLaMA-2\nfrom Meta, which offers these models for down-\nload on Hugging Face. The arXiv dataset used in\nthis paper is publicly available on Kaggle. While\nwe have demonstrated that AstroLLaMA is capa-\nble of generating high-quality, relevant abstracts\nfor astronomical research papers, we have noted\nthat it has the potential to generate inaccurate data\nand measurements. This should serve as a cau-\ntion for researchers aiming to use this model for\ndownstream tasks, and we invite the adoption of\nalignment strategies in future work to ameliorate\nthis issue.\nReferences\nGoogle AI PaLM 2. https://ai.google/discover/palm2/.\nA. Accomazzi, M. J. Kurtz, E. A. Henneken, R. Chyla,\nJ. Luker, C. S. Grant, D. M. Thompson, A. Holachek,\nR. Dave, and S. S. Murray. 2015. ADS: The Next\nGeneration Search Platform. In Open Science at the\nFrontiers of Librarianship, volume 492 of Astronom-\nical Society of the Pacific Conference Series, page\n189.\nAndr\u00e9s Almeida, Scott F. Anderson, Maria Argudo-\nFern\u00e1ndez, Carles Badenes, Kat Barger, Jorge K.\nBarrera-Ballesteros, Chad F. Bender, Erika Benitez,\nFelipe Besser, Dmitry Bizyaev, Michael R. Blan-\nton, John Bochanski, Jo Bovy, William Nielsen\nBrandt, Joel R. Brownstein, Johannes Buchner, Esra\nBulbul, Joseph N. Burchett, Mariana Cano D\u00edaz,\nJoleen K. Carlberg, Andrew R. Casey, Vedant Chan-\ndra, Brian Cherinka, Cristina Chiappini, Abigail A.\nCoker, Johan Comparat, Charlie Conroy, Gabriella\nContardo, Arlin Cortes, Kevin Covey, Jeffrey D.\nCrane, Katia Cunha, Collin Dabbieri, James W.\nDavidson Jr. au2, Megan C. Davis, Nathan De\nLee, Jos\u00e9 Eduardo M\u00e9ndez Delgado, Sebastian De-\nmasi, Francesco Di Mille, John Donor, Peter Dow,\nTom Dwelly, Mike Eracleous, Jamey Eriksen, Xiao-\nhui Fan, Emily Farr, Sara Frederick, Logan Fries,\nPeter Frinchaboy, Boris T. Gaensicke, Junqiang\nGe, Consuelo Gonz\u00e1lez \u00c1vila, Katie Grabowski,\nCatherine Grier, Guillaume Guiglion, Pramod Gupta,\nPatrick Hall, Keith Hawkins, Christian R. Hayes,\nJ. J. Hermes, Lorena Hern\u00e1ndez-Garc\u00eda, David W.\nHogg, Jon A. Holtzman, Hector Javier Ibarra-\nMedel, Alexander Ji, Paula Jofre, Jennifer A. John-\nson, Amy M. Jones, Karen Kinemuchi, Matthias\nKluge, Anton Koekemoer, Juna A. Kollmeier, Marina\nKounkel, Dhanesh Krishnarao, Mirko Krumpe, Ivan\nLacerna, Paulo Jakson Assuncao Lago, Chervin La-\nporte, Ang Liu, Chao Liu, Xin Liu, Alexandre Roman\nLopes, Matin Macktoobian, Viktor Malanushenko,\nDan Maoz, Thomas Masseron, Karen L. Masters,\nGal Matijevic, Aidan McBride, Ilija Medan, An-\ndrea Merloni, Sean Morrison, Natalie Myers, Sz-\nabolcs M\u00e9sz\u00e1ros, C. Alenka Negrete, David L. Nide-\nver, Christian Nitschelm, Audrey Oravetz, Daniel\nOravetz, Kaike Pan, Yingjie Peng, Marc H. Pin-\nsonneault, Rick Pogge, Dan Qiu, Anna Barbara\nde Andrade Queiroz, Solange V. Ramirez, Hans-\nWalter Rix, Daniela Fern\u00e1ndez Rosso, Jessie Run-\nnoe, Mara Salvato, Sebastian F. Sanchez, Felipe A.\nSantana, Andrew Saydjari, Conor Sayres, Kevin C.\nSchlaufman, Donald P. Schneider, Axel Schwope,\nJavier Serna, Yue Shen, Jennifer Sobeck, Ying-Yi\nSong, Diogo Souto, Taylor Spoo, Keivan G. Stassun,\nMatthias Steinmetz, Ilya Straumit, Guy Stringfellow,\nJos\u00e9 S\u00e1nchez-Gallego, Manuchehr Taghizadeh-Popp,\nJamie Tayar, Ani Thakar, Patricia B. Tissera, An-\ndrew Tkachenko, Hector Hernandez Toledo, Benny\nTrakhtenbrot, Jose G. Fernandez Trincado, Nicholas\nTroup, Jonathan R. Trump, Sarah Tuttle, Natalie Ul-\nloa, Jose Antonio Vazquez-Mata, Pablo Vera Alfaro,\nSandro Villanova, Stefanie Wachter, Anne-Marie\nWeijmans, Adam Wheeler, John Wilson, Leigh Wo-\njno, Julien Wolf, Xiang-Xiang Xue, Jason E. Ybarra,\nEleonora Zari, and Gail Zasowski. 2023. The eigh-\nteenth data release of the sloan digital sky surveys:\nTargeting and first spectra from sdss-v.\nChristine L. Borgman and Morgan F. Wofford. 2021.\nFrom Data Processes to Data Products: Knowledge\nInfrastructures in Astronomy. arXiv e-prints, page\narXiv:2109.01707.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. PaLM: Scaling Language\nModeling with Pathways.\nIoana Ciuc\u02d8a and Yuan-Sen Ting. 2023.\nGalactic\nChitChat: Using Large Language Models to Con-\nverse with Astronomy Literature.\narXiv e-prints,\npage arXiv:2304.05406.\nIoana Ciuc\u02d8a, Yuan-Sen Ting, Sandor Kruk, and Kartheik\nIyer. 2023. Harnessing the Power of Adversarial\nPrompting and Large Language Models for Robust\nHypothesis Generation in Astronomy. arXiv e-prints,\npage arXiv:2306.11648.\nC. Fabricius, X. Luri, F. Arenou, C. Babusiaux,\nA. Helmi, T. Muraveva, C. Reyl\u00e9 , F. Spoto,\nA. Vallenari, T. Antoja, E. Balbinot, C. Barache,\nN. Bauchet, A. Bragaglia, D. Busonero, T. Cantat-\nGaudin, J. M. Carrasco, S. Diakit\u00e9, M. Fabrizio,\nF. Figueras, A. Garcia-Gutierrez, A. Garofalo,\nC. Jordi, P. Kervella, S. Khanna, N. Leclerc, E. Li-\ncata, S. Lambert, P. M. Marrese, A. Masip, P. Ramos,\nN. Robichon, A. C. Robin, M. Romero-G\u00f3mez,\nS. Rubele, and M. Weiler. 2021. Gaia early data\nrelease 3. Astronomy & Astrophysics, 649:A5.\nFelix Grezes, Sergi Blanco-Cuaresma, Alberto Acco-\nmazzi, Michael J. Kurtz, Golnaz Shapurian, Edwin\nHenneken, Carolyn S. Grant, Donna M. Thomp-\nson, Roman Chyla, Stephen McDonald, Timothy W.\nHostetler, Matthew R. Templeton, Kelly E. Lockhart,\nNemanja Martinovic, Shinyi Chen, Chris Tanner, and\nPavlos Protopapas. 2021. Building astroBERT, a\nlanguage model for Astronomy & Astrophysics.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. LoRA: Low-Rank Adaptation\nof Large Language Models.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for Neural Text Processing.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66\u201371.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nWeight Decay Regularization. In International Con-\nference on Learning Representations.\nMeta. 2023.\nLlama 2:\nOpen Foundation and\nFine-Tuned Chat Models | Meta AI Research.\nhttps://ai.meta.com/research/publications/llama-2-\nopen-foundation-and-fine-tuned-chat-models/.\nOpenAI. 2023. GPT-4 Technical Report.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715\u20131725.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. LLaMA: Open\nand Efficient Foundation Language Models.\n"
  },
  {
    "title": "Uncovering mesa-optimization algorithms in Transformers",
    "link": "https://arxiv.org/pdf/2309.05858.pdf",
    "upvote": "11",
    "text": "Preprint\nUNCOVERING MESA-OPTIMIZATION ALGORITHMS IN\nTRANSFORMERS\nJohannes von Oswald\u2217,1\nETH Z\u00fcrich &\nGoogle Research\nEyvind Niklasson\u2217\nGoogle Research\nMaximilian Schlegel\u2217\nETH Z\u00fcrich\nSeijin Kobayashi\nETH Z\u00fcrich\nNicolas Zucchet\nETH Z\u00fcrich\nNino Scherrer\nIndependent Researcher\nNolan Miller\nGoogle Research\nMark Sandler\nGoogle Research\nBlaise Ag\u00fcera y Arcas\nGoogle Research\nMax Vladymyrov\nGoogle Research\nRazvan Pascanu\nGoogle DeepMind\nJo\u00e3o Sacramento1\nETH Z\u00fcrich\nABSTRACT\nTransformers have become the dominant model in deep learning, but the reason\nfor their superior performance is poorly understood. Here, we hypothesize that\nthe strong performance of Transformers stems from an architectural bias towards\nmesa-optimization, a learned process running within the forward pass of a model\nconsisting of the following two steps: (i) the construction of an internal learn-\ning objective, and (ii) its corresponding solution found through optimization. To\ntest this hypothesis, we reverse-engineer a series of autoregressive Transformers\ntrained on simple sequence modeling tasks, uncovering underlying gradient-based\nmesa-optimization algorithms driving the generation of predictions. Moreover, we\nshow that the learned forward-pass optimization algorithm can be immediately\nrepurposed to solve supervised few-shot tasks, suggesting that mesa-optimization\nmight underlie the in-context learning capabilities of large language models. Fi-\nnally, we propose a novel self-attention layer, the mesa-layer, that explicitly and\nefficiently solves optimization problems specified in context. We find that this\nlayer can lead to improved performance in synthetic and preliminary language\nmodeling experiments, adding weight to our hypothesis that mesa-optimization is\nan important operation hidden within the weights of trained Transformers.\n1\nINTRODUCTION\nTransformers (Vaswani et al., 2017) and especially large language models (LLMs) are known to\nstrongly adjust their predictions and learn based on data given in-context (Brown et al., 2020).\nRecently, a number of works have studied this phenomenon in detail by meta-learning Transformers\nto solve few-shot tasks, providing labeled training sets in context. These studies discovered that\nTransformers implement learning algorithms that either closely resemble or exactly correspond to\ngradient-based optimizers (Garg et al., 2022; Aky\u00fcrek et al., 2023; von Oswald et al., 2023; Kirsch\net al., 2022; Zhang et al., 2023; Mahankali et al., 2023; Ahn et al., 2023; Li et al., 2023a).\nHowever, it remains unclear how well these findings on meta-trained Transformers translate to models\nthat are autoregressively-trained on sequential data, the prevalent LLM training setup. Here, we\naddress this question by building on the theoretical construction of von Oswald et al. (2023), and show\nhow Transformers trained on sequence modeling tasks predict using gradient-descent learning based\non in-context data. Thus, we demonstrate that minimizing a generic autoregressive loss gives rise to a\nsubsidiary gradient-based optimization algorithm running inside the forward pass of a Transformer.\nThis phenomenon has been recently termed mesa-optimization (Hubinger et al., 2019). Moreover, we\nfind that the resulting mesa-optimization algorithms exhibit in-context few-shot learning capabilities,\nindependently of model scale. Our results therefore complement previous reports characterizing the\nemergence of few-shot learning in large-scale LLMs (Kaplan et al., 2020; Brown et al., 2020).\n\u2217These authors contributed equally to this work. 1Correspondence to jvoswald@google.com, rjoao@ethz.ch.\n1\narXiv:2309.05858v1  [cs.LG]  11 Sep 2023\nPreprint\nAutoregressive Transformer\n (i) Create mesa-dataset\n      \u2192 select input-output pairs \n          predictive of the future\nlayerwise \nmesa-optimization \n (ii) De\ufb01ne mesa-objective\n      \u2192 learn internal model \n            based on \n (iii) Mesa-optimize\n       \u2192 improves over sequence \n             length and layer depth\nSelf-attention map\nFigure 1: Illustration of our hypothesis: Optimizing the weights \u03b8 of an autoregressive Transformer\nf\u03b8 gives rise to mesa-optimization algorithms implemented in the forward pass of the model. As a\nsequence of inputs s1, . . . , st is processed up to timestep t, the Transformer (i) creates an internal\ntraining set consisting of pairs of input-target associations, (ii) defines an internal objective function\nthrough the resulting dataset, used to measure the performance of an internal model with weights W,\n(iii) optimizes this objective and uses the learned model to generate a prediction \u02c6st+1 of the future.\nOur contributions are as follows:\n\u2022 We generalize the construction of von Oswald et al. (2023) and show how, in theory,\nTransformers can autoregressively predict the next element of a sequence by optimizing\ninternally-constructed objectives with gradient-based methods.\n\u2022 Experimentally, we reverse-engineer Transformers trained on simple sequence modeling\ntasks, and find strong evidence that their forward pass implements two-step algorithms:\n(i) early self-attention layers construct internal training datasets by grouping and copying\ntokens, and therefore implicitly define internal objective functions, (ii) deeper layers optimize\nthese objectives to generate predictions.\n\u2022 Similarly to LLMs, we show that these simple autoregressively-trained models become\nin-context learners, and that prompt-tuning, crucial to improve in-context learning in LLMs,\nalso improves performance in our setting.\n\u2022 Motivated by our findings that attention layers are attempting to implicitly optimize internal\nobjective functions, we introduce the mesa-layer, a novel attention layer that efficiently\nsolves a least-squares optimization problem, instead of taking just a single gradient step\ntowards an optimum. We show that a single mesa-layer outperforms deep linear and softmax\nself-attention Transformers on simple sequential tasks while offering more interpretability.\n\u2022 We carry out preliminary language modeling experiments replacing standard self-attention\nlayers with the mesa-layer, and obtain promising results demonstrating strong in-context\nlearning capabilities enabled by the layer.\n2\nPRELIMINARIES\nSelf-attention.\nWe study causally-masked, autoregressive Transformers (Vaswani et al., 2017)\nwhere self-attention (Bahdanau et al., 2015) is the elementary building block. Given a sequence of t\ninput tokens Et = (et\u2032)t\nt\u2032=1, representing the first t time steps, a self-attention layer with H heads\nand parameters \u03b8 updates the current token et \u2208 RDe as follows:\n\u2206esoftmax\nt\n(Et, \u03b8) =\nH\nX\nh=1\nPhVh,t softmax(K\u22a4\nh,tqh,t),\n(1)\nwhere qh,t = Wh,qet \u2208 RDa is referred to as a query, each column kh,t\u2032 = Wh,ket\u2032 \u2208 RDa of\nmatrix Kh,t \u2208 RDa\u00d7t as a key, and each column vh,t\u2032 = Wh,vet\u2032 \u2208 RDv of matrix Vh,t \u2208 RDv\u00d7t\nas a value. The nonlinear function softmax(a) applied to vector a \u2208 Rt returns an attention\nvector with entries [softmax(a)]i =\nexp(ai)\nPt\nt\u2032=1 exp(at\u2032). We absorb bias terms and assume here for\n2\nPreprint\nconciseness that all heads are equally sized. The parameters \u03b8 of this layer are the projection matrices\n{(Ph, Wh,q, Wh,k, Wh,v)}H\nh=1 for all heads. Transformers include other layers that we do not review\nhere, notably multi-layer perceptrons (MLPs) and layer normalization (LayerNorm) units.\nWe also consider linear attention models (e.g., Katharopoulos et al., 2020; Wang et al., 2020; Schlag\net al., 2021; Choromanski et al., 2021), which simply omit the softmax nonlinearity:\n\u2206elinear\nt\n(Et, \u03b8) =\nH\nX\nh=1\nPhVh,tK\u22a4\nh,tqh,t =\nH\nX\nh=1\nPh \u02c6W linear\nh,t qh,t.\n(2)\nAbove, we rewrite this equation using a weight matrix \u02c6W linear\nh,t\n= Pt\nt\u2032=1 vh,t\u2032k\u22a4\nh,t\u2032. The size of this\nweight matrix does not scale with time, but it encodes information from all past tokens (et\u2032)t\nt\u2032=1,\nallowing inference at constant memory cost. For this reason, there is at present considerable interest\nin linear attention (Fournier et al., 2023; Treviso et al., 2023).\nLinear self-attention can implement one step of gradient descent.\nOur starting point is the main\nresult of von Oswald et al. (2023), who showed that one such attention layer can implement one\nstep of gradient descent (GD) on a quadratic cost function evaluated on in-context data. Therefore,\nmulti-layer Transformers can, in theory, minimize the loss down to an arbitrary desired level through\nmultiple steps of GD. In this paper, we extend this result to the autoregressive setting. First, we\nreview the original model and task setting.\nIn the setup of von Oswald et al. (2023), the goal is to meta-learn the parameters \u03b8 of a lin-\near self-attention layer such that it learns to solve supervised learning tasks, similarly to related\nwork (Garg et al., 2022; Aky\u00fcrek et al., 2023; Kirsch et al., 2022; Zhang et al., 2023; Ma-\nhankali et al., 2023; Ahn et al., 2023; Li et al., 2023a). Each task \u03c4 is specified in-context by\na training set D\u03c4 = {(x\u03c4,i, y\u03c4,i)}N\ni=1 and a test input x\u03c4,test. The goal of meta-learning is then\nmin\u03b8 E\u03c4\n\u0002\n\u2225y\u03c4,test \u2212 f(x\u03c4,test, D\u03c4, \u03b8)\u22252\u0003\n, where y\u03c4,test is the correct output revealed during meta-\nlearning, f(x\u03c4,test, D\u03c4, \u03b8) denotes the actual output of the linear self-attention layer, and the ex-\npectation is taken over a distribution of linear regression tasks.\nA standard approach for solving a linear regression task is to resort to a linear model fW (x) = Wx\nwith parameters W \u2208 RDy\u00d7Dx learned by gradient descent on the squared error loss L(W, D\u03c4) =\nPN\ni=1\n1\n2\u2225y\u03c4,i \u2212fW (x\u03c4,i)\u22252. Starting from an initial parameter W0, a gradient-descent learner updates\nit by taking a step \u2206W0 of size \u03b7 along the negative of the gradient, \u2207L = PN\ni=1(y\u03c4,i \u2212W0x\u03c4,i)x\u22a4\n\u03c4,i.\nThe main result of von Oswald et al. (2023) is a theoretical construction showing that a linear\nself-attention layer can implement exactly one such gradient descent step. We briefly sketch this\nresult now.\nFirst, we construct a set of tokens ET , with T = N, such that et = (y\u03c4,i, x\u03c4,i), with y\u03c4,i and x\u03c4,i\nconcatenated. Additionally, we create a query token eT +1 = (\u2212W0x\u03c4,test, x\u03c4,test) not contained\nwithin the set D\u03c4, where we place the test input for which a prediction should be made. Under this\ntoken construction and using the symbol Ix to denote the identity matrix of size dim(x), if all bias\nterms are zero and W \u22a4\nk Wq =\n\u0012\n0\n0\n0\nIx\n\u0013\n, and PWv =\n\u0012\n\u2212\u03b7Iy\n\u03b7W0\n0\n0\n\u0013\n, the query token eT +1, after\none such layer, becomes (\u2212(W0 + \u2206W0)x\u03c4,test), x\u03c4,test). The y-component of this token contains the\n(negative) of the prediction obtained by a linear model that underwent one step (\u2206W0) of gradient\ndescent. Therefore, this self-attention layer implicitly constructs a least-squares optimization problem\nand takes one step of mesa-gradient descent towards solving it. This layer can be directly stacked to\nimplement multiple steps of GD, cf. Appendix A4.2. The term mesa reinforces that this optimization\noccurs within the forward attention dynamics, without any actual change to the parameters of the\nattention layer itself (Hubinger et al., 2019). We stress the necessary assumption of having x\u03c4,i and\ny\u03c4,i concatenated within a single token.\n3\nSEQUENTIAL PREDICTION BY LEAST-SQUARES MESA-OPTIMIZATION\nThe construction reviewed above is designed to solve few-shot supervised learning problems. As we\nsee next, moving to a general autoregressive modeling setting requires minimal change. However,\nthe spirit of what follows is markedly different: we no longer ask whether an attention layer can\n3\nPreprint\nsolve few-shot supervised learning problems that are presented in-context. Instead, we ask whether\nTransformers can rely on mesa-gradient descent to predict future inputs.\nWe therefore move to the case where a self-attention layer has to learn sequentially as some inputs\ns1:T are gradually unveiled. The goal at time t is now to minimize the autoregressive loss:\nLt(W) =\nt\u22121\nX\nt\u2032=1\n1\n2\u2225st\u2032+1 \u2212 Wst\u2032\u22252,\n(3)\nwhere st\u2032+1 serves as the label for st\u2032. As in the previous section, we assume that the model always\nstarts from the same initial weights W0, and that learning corresponds to taking only a single gradient\nstep; this appears sub-optimal. We address this concern in the next section.\nAs is usually done in autoregressive modeling we apply causal masking, and at time t we update\ntoken et using the in-context data available in Et. To adapt to the autoregressive setting, we adapt the\ntoken construction to a three-channel code, et = (\u2212W0st, st, st\u22121), to include an additional separate\nfirst channel to be filled with the prediction \u02c6st+1 of future inputs at every time step t, alongside\nchannels for the previous and current sequence element, with the latter playing the role of target in\nthe construction of von Oswald et al. (2023). Note that by providing neighboring elements st, st\u22121\nwithin one token et, self-attention is able to compute dot products of targets and inputs of the loss\nLt(W) necessary to compute \u2207Lt, see Eq. 3. Then, to update the first channel of such a token with\nthe prediction of a linear model learned with one step of gradient descent, it suffices to set\nPWv =\n 0\n\u2212\u03b7Is\n\u03b7W0\n0\n0\n0\n0\n0\n0\n!\n,\nand\nW \u22a4\nk Wq =\n 0\n0\n0\n0\n0\n0\n0\nIs\n0\n!\n.\n(4)\nWe refer to this result (Eq. 4) as the one-step mesa-gradient descent construction.\nMulti-layer mesa-optimizers.\nWe next move to the case of deep networks comprising stacked\nlinear self-attention layers. While it is natural to hypothesize that K layers simply implement K steps\nof mesa-gradient descent, as in the few-shot learning (non-autoregressive) case reviewed above, this\npicture might be too simple to explain actual trained autoregressive Transformers. A first hint towards\nthis view being too narrow lies in the fact that stacking the one-step mesa-gradient descent construction\n(Eq. 4) over multiple layers does not yield vanilla gradient descent, as explained in Appendix A4.2.\nInstead, we obtain an unconventional online gradient-based optimizer, that is expected to behave\nworse than vanilla gradient descent. This observation, together with a mathematical analysis of the\nresulting optimization algorithm, can be found in a study arguing for the disadvantages of causally-\nmasked attention for few-shot in-context learning (Ding et al., 2023). One may thus wonder if\nTransformers can implement more efficient mesa-optimizers.\nHere, we provide an alternative mesa-optimizer that is also based on causally-masked self-attention\nlayers. The novel optimizer operates in two stages. In a first stage, comprising one or more self-\nattention layers, the algorithm implements an iterative preconditioning procedure. The result of\nthis stage is a regularized mesa-objective \u00afLt(W) = Pt\u22121\nt\u2032=1\n1\n2\u2225st\u2032+1 \u2212 WHtst\u2032\u22252 +\n1\n2\u03bb||W||2\nF, with\nimproved condition number compared to Lt(W). Above, Ht is a preconditioning matrix and the scalar\n\u03bb\u22121 \u2265 0 controls the regularization strength. This preconditioning procedure has the property that in\nthe many-layer limit and under some mild conditions, Ht converges to H\u2217\nt = (St\u22121S\u22a4\nt\u22121 + 1/\u03bbI)\u22121,\nwith St the data matrix whose columns are (st\u2032)t\nt\u2032=1. In a second stage, a final self-attention layer\ntakes a single gradient descent step on the preconditioned mesa-objective \u00afLt(W).\nThe two-stage algorithm described here is theoretically justified: when Ht = H\u2217\nt , the regression prob-\nlem is solved in a single step, starting from a zero-weight initialization W0 = 0. In Appendix A4.2,\nwe provide a simple weight and input token construction to implement this algorithm. Our novel\nconstruction leverages the truncated Neumann series to iteratively approximate the required inverse-\nmatrix-vector products H\u2217\nt\u22121st in parallel for all t = 2, . . . , T, and compactly, without ever explicitly\nrepresenting any of the Ht matrices.\nIn Section 5 we show empirically that training a Transformer on autoregressive tasks can lead to\nthe solutions presented above. But first, in the next section, we assume that mesa-optimization is a\ndesirable feature for a model to have, and we discuss an architectural modification that makes this\nbehavior built-in by default within a Transformer.\n4\nPreprint\n4\nAN ATTENTION LAYER FOR OPTIMAL LEAST-SQUARES LEARNING\nHere we introduce the mesa-layer: a novel self-attention layer that fully solves a layer-specific\noptimization problem, such as the minimization of Eq. 3, instead of only descending a loss function\nwith a single gradient step. The layer we propose is closely related to the Delta-Net model of\nSchlag et al. (2021), which is hardwired to do one gradient descent step per time point. We focus on\ncausally-masked autoregressive problems, while noting that the insights remain the same for other\nstrategies such as BERT-style masking (Devlin et al., 2019).\nGiven again a sequence of tokens Et, we design a layer that changes the tokens following the update\n\u2206emesa\nt\n(Et, \u03b8) =\nH\nX\nh=1\nPh \u02c6W mesa\nh,t qh,t,\n(5)\nwith\n\u02c6W mesa\nh,t\n= arg min\nW\n(\n1\n2\nt\nX\nt\u2032=1\n||vh,t\u2032 \u2212 Wkh,t\u2032||2 +\n1\n2\u03bbh\n||W||2\nF\n)\n.\n(6)\nAbove, the scalar \u03bb\u22121\nh\n> 0 controls the strength of a regularizer added to improve generalization,\nand key, value and query vectors are the usual learned head-specific affine transformations of the\ntokens, as before. However, through Eq. 6 these vectors are now assigned a precise, interpretable role:\nvalue vectors specify targets to which an internal model with parameters W should map training and\ntest inputs, represented by keys and queries, respectively. The minimizer of a regularized version of\nEq. 3 can be immediately mapped to Eq. 6 under the token construction discussed in Section 3 by\nappropriately setting the projection matrices Wh,v, Wh,k and Wh,q.\nAt any given time step t = 1, . . . , T computing \u2206emesa\nt\nrequires solving a regularized least squares\nproblem per head. To efficiently solve this sequence of T optimization problems, we will leverage\nthe recursive dependency of the solutions of these consecutive problems which can be expressed in\nclosed-form as\n\u02c6W mesa\nh,t\n= Vh,tK\u22a4\nh,tRh,t =\nt\nX\nt\u2032=1\nvh,t\u2032k\u22a4\nh,t\u2032\n \nt\nX\nt\u2032=1\nkh,t\u2032k\u22a4\nh,t\u2032 + 1/\u03bbh I\n!\u22121\n.\n(7)\nNote that if we drop the inverted matrix Rh,t, we recover a standard linear self-attention layer,\ncf. Eq. 2. A recent study has also shown that the solution of a least-squares problem can be expressed\nas a generalized attention layer (Garnelo & Czarnecki, 2023).\nWe now use the Sherman & Morrison (1950) formula to obtain the inverse at time t from the inverse\nat the previous time step t \u2212 1. This iterative update is possible because we only change the inverse\nby a rank-one update. This solution scheme is known as recursive least squares (Gauss, 1821). We\nobtain through Sherman-Morrison the recursion\nRh,t = Rh,t\u22121 \u2212\nRh,t\u22121kh,tk\u22a4\nh,tRh,t\u22121\n1 + k\u22a4\nh,tRh,t\u22121kh,t\n(8)\nwith Rh,0 = \u03bbh I. With this, we can (causally in time) compute\n\u2206emesa\nt\n(Et, \u03b8) =\nH\nX\nh=1\nPhVh,tK\u22a4\nh,tRh,tqh,t\n(9)\nwhich requires 2 additional vector-matrix and 2 vector-vector multiplications per step compared to the\nstandard self-attention operation. Note that since our intermediates consist of matrices of dimension\nDa \u00d7Da across the timesteps, naive backward gradient computation requires storing them in memory.\nFortunately, this memory overhead can be avoided using the Sherman-Morrison formula in reverse\nduring the backward pass, cf. Appendix A2.1, enabling memory-efficient gradient computation of the\noutput of the mesa-layer w.r.t. its inputs. We further note that while the implementation described here\nhas a desirable O(1) inference memory cost like standard linear self-attention, it is not parallelizable\nacross time during training. This is a disadvantage for training on contemporary hardware shared\nwith recurrent neural networks, but not with standard softmax or linear self-attention. As discussed in\nAppendix A2.1, in practice this significantly slows down our experiments.\nWe demonstrate the expressivity and performance of the mesa-layer in reverse-engineerable sequence\nlearning tasks as well as in language modeling in the next sections.\n5\nPreprint\n5\nEMPIRICAL ANALYSIS\n5.1\nPREDICTION OF LINEAR DYNAMICS BY IN-CONTEXT LEARNING\nWe now attempt to reverse-engineer Transformers trained on simple synthetic autoregressive tasks.\nWe have two main goals. First, we want to understand whether autoregressively-trained Transformers\nuse mesa-optimization algorithms to predict future inputs. We use the constructions presented in\nSection 3 to guide our reverse-engineering analyses. Our second goal is to determine if introducing\nthe mesa-layer improves the performance of standard Transformers, by subsuming multiple attention\nlayers that are otherwise needed to go beyond one mesa-gradient descent step.\nGenerative model.\nWe focus on fully-observed linear dynamical systems. For all experiments\ndescribed in this section, we use the following generative model. To create a sequence s1:T we first\ndraw a random groundtruth Ds\u00d7Ds weight matrix W \u2217 as well as a random initial state s1 \u223c N(0, Is);\nsubsequent states for t = 2, . . . , T are then generated according to the rule st+1 = W \u2217st + \u03f5t, where\n\u03f5t \u223c N(0, \u03c32\ns Is) introduces Gaussian noise. We take W \u2217 to be a random orthogonal matrix1. The\ngeneration of W \u2217 anew for each sequence avoids the memorization solution that stores W \u2217 in \u03b8, and\ncorresponds to a highly simplified toy model meant to capture the diversity present in real-world\ndata. A similar in spirit design choice may be found in the hierarchical generative model of Xie\net al. (2022). We refer to Appendix A6.1 for additional experimental details. Under such an assumed\ngroundtruth dynamics, the standard way of predicting future states from a given past sequence s1:T is\nto use a linear model, st+1 = Wst, where the weights W are learned by minimizing Lt(W), Eq. 3,\npossibly with an added regularizer.\nTraining and in-context learning objectives.\nHere, we analyze various configurations of Trans-\nformers trained through stochastic online minimization of the autoregressive loss\nL(\u03b8) = Es\n\" T \u22121\nX\nt=1\nLt(s1:t, \u03b8)\n#\n= Es\n\"\n1\n2\nT \u22121\nX\nt=1\n\u2225st+1 \u2212 ft(s1:t, \u03b8)\u22252\n#\n,\n(10)\nwhere the expectation is taken under the sequence distribution described above, ft(s1:t, \u03b8) denotes\nthe output of the Transformer model using st as query and s1:t as context, and \u03b8 are the Transformer\nparameters, which vary depending on the exact architecture being trained. To avoid confusion with\nmesa-optimization, we refer to the minimization of L(\u03b8) as the base-optimization process.\nHere and throughout, to measure in-context learning performance we take the per-timestep loss\nLt(s1:t, \u03b8) and monitor its evolution as a function of context size t. Thus, we simply measure how\nfuture-input predictions improve as more context is provided to the model. This corresponds to the\noperational definition of in-context learning proposed by Kaplan et al. (2020).\nHypothesis statement.\nThe hypothesis we pursue is that base-optimization of L(\u03b8) gives rise to a\nmesa-optimization process in charge of generating predictions ft(s1:t, \u03b8), as illustrated in Figure 2A.\nMore concretely, for our linear generative model, we hypothesize that learning yields Transformers\nthat predict future inputs by implicitly, and entirely within their forward dynamics: (i) representing\na linear model with mesa-parameters W, (ii) constructing the least-squares mesa-objective Lt(W),\ncf. Eq. 3, using in-context data s1:t, (iii) learning W by minimizing the mesa-objective, and (iv) apply-\ning W to predict the next token st+1. We note that, according to our hypothesis, the mesa-objective\nLt(W) governing the forward pass of our Transformer coincides with the base-objective L(\u03b8), but\nnow defined w.r.t. an implicit linear autoregressive model with mesa-parameters W.\nSingle self-attention layer.\nWe begin by verifying our hypothesis on single-layer, linear-attention-\nonly Transformers, using the token construction of Section 3, et = (0, st, st\u22121). We hypothesize that\nfeeding the Transformer with input-target pairs provides an inductive bias towards mesa-gradient\ndescent. Using this token construction, we then train by online mini-batch gradient descent on L(\u03b8),\ngenerating new sequences at each base optimization step according to the process described above.\n1This detail turns out to be important; we found that converging linear dynamics led to different inference\nalgorithms. We discuss this point in Appendix A5.\n6\nPreprint\nA \n> Construct mesa-objective\n> Create mesa-dataset\nAutoregressive Transformer\n> Mesa-optimize\nA \n> Construct mesa-objective\n> Create mesa-dataset\nAutoregressive Transformer\n> Mesa-optimize\n1\n2000\nTraining steps\n0.0\n0.5\n1.0\n1.5\nTest Loss\nB\nRevAlg-1\nInterpolation\nMesa\nlinear-SA\n0\n20\n40\nSequence length t\n0.0\n0.5\n1.0\n1.5\nMSE\nC\nGDexact-1\nMesa\nlinear-SA\nFigure 2: Reverse-engineering a trained linear self-attention layer. (A) Transformers mesa-\noptimize an internal linear model and use it to predict the future state of a linear dynamical system.\n(B) A trained 2-head linear self-attention layer (linear-SA) is perfectly described by a reverse-\nengineered mesa-gradient descent algorithm (RevAlg-1; see Eq. A43). We show also the performance\nachieved by an interpolation model, obtained by averaging the parameters \u03b8 of the trained model and\nthose expected from our reverse-engineered construction. (C) In-context learning loss after training:\nnext-input st+1 mean squared prediction error (MSE) as a function of sequence length. The trained\nlinear-SA layer is very well described by a linear model learned by one step of gradient descent\nwith a tuned learning rate (GDexact-1). Linear-SA is greatly outperformed by a single mesa-layer,\nwhich optimally solves the autoregressive learning problem at every time point t, reaching minimal\nmean-squared prediction error after observing enough examples. By contrast, one-step GD runs into\ncapacity issues, exhibiting non-monotonic MSE as a function of sequence length. Averages over 5\ndifferent seeds; shaded area represents standard deviation.\nWe are able to perfectly identify the algorithm (RevAlg-1) that this single-layer Transformer uses\nto generate predictions. Visual inspection of the projection matrices is revealing, cf. Figure A2:\nwe see that the dominant pattern coincides with our one-step mesa-gradient descent construction,\nEq. 4, plus some identification noise. We verify quantitatively that the layer is indeed implementing\na step of mesa-gradient descent by (i) comparing the loss reached by the trained layer with a\nlinear autoregressive model learned through one step of gradient descent, and by (ii) studying an\ninterpolated model, obtained by averaging directly in parameter space learned and constructed\nweights, cf. Appendix A6.1. We find that we can perfectly fit our trained layer when using all degrees\nof freedom in our construction, including not only a learned learning rate \u03b7, but also a learned set of\ninitial weights W0, reminiscent of the model-agnostic meta-learning method of Finn et al. (2017).\nImportantly, as shown in Figure 2, the resulting learned one-step algorithm is still vastly outperformed\nby a single mesa-layer. We note that under a simple setting of its weights, easily discovered by\nbase-optimization, this layer can optimally solve the task studied here. This result demonstrates the\nadvantage of hardcoded inductive biases in favor of mesa-optimization.\nMultiple self-attention layers.\nArmed with our theoretical insights for the multi-layer case, cf. Sec-\ntion 3, we now analyze deep linear and softmax attention-only Transformers. We format our inputs\naccording to a 4-channel construction, et = (0, st, st, st\u22121), which corresponds to choosing W0 = 0.\nThis makes it possible to implement both multi-step mesa-optimization and our iterative precondi-\ntioning algorithm, as well as hybrid variants mixing both, as discussed in Appendix A4.2.\nLike with single-layer models, we see clean structure in the weights of the trained models, see\nFigures A7 and A5. As a first reverse-engineering analysis, we exploit this structure and construct an\nalgorithm (RevAlg-d, where d denotes layer number) comprising 16 parameters (instead of 3200) per\nlayer head. We find that this compressed, albeit convoluted, expression can describe a trained model.\nIn particular, it allows interpolating between actual Transformer and RevAlg-d weights in an almost\nlossless fashion, cf. Figure 3A. Experimental details can be found in Appendix A6.1.2.\nWhile the RevAlg-d expression explains a trained multi-layer Transformer with a small number of\nfree parameters, it is difficult to interpret it as a mesa-optimization algorithm. We, therefore, resort\nto a linear regression probing analysis (Alain & Bengio, 2017; Aky\u00fcrek et al., 2023) to look for\nsignatures of our hypothesized mesa-optimization algorithms. In particular, we seek evidence both for\nthe stacked multi-layer gradient descent construction, which should bring the outputs of intermediate\n7\nPreprint\n1\n1000\nTraining steps\n0.2\n0.4\n0.6\n0.8\nTest Loss\nA\nMesa\nInterpolation\nLSA-6\nRevAlg-6\n1\n20\n40\nSequence length t\n0.1\n0.01\n1\nMSE(f(d)\nt\n, st + 1)\nB\nd = 1\nd = 2\nd = 3\nd = 4\nd = 5\nd = 6\n1\n20\n40\nSequence length t\n0.1\n0.01\nMSE(f(d)\nt\n, (St\n1ST\nt\n1 + 1/ I)\n1st)\nC\nd = 1\nd = 2\nd = 3\nd = 4\nd = 5\nd = 6\nFigure 3: Reverse-engineering multi-layer Transformers trained on constructed token inputs.\nWe report results for a 6-layer linear-self-attention-only Transformer. (A) As training proceeds, this\nmulti-layer linear model (LSA-6) is again perfectly described by a reverse-engineered algorithm\n(RevAlg-6), described in Appendix A4. Note that the model is still outperformed by a single trained\nmesa-layer. (B & C) We linearly regress the activations of each layer against (B) final targets (target\nprobing) as well as (C) the preconditioned inputs (St\u22121S\u22a4\nt\u22121 + 1/\u03bbI)\u22121st predicted by our theory\n(inverse probing), observing an improvement in linear decoding performance across layers. Averages\ncomputed over 5 different seeds; shaded area represents standard deviation.\nlayers closer to the desired targets; and for our novel iterative preconditioning algorithm, which\nshould bring layer outputs closer to H\u2217\nt st. We therefore carry out our probing analysis taking as\ntargets for regression (i) the future state to be predicted st+1 used as the target to train the Transformer,\nwhich we term the target probe; and (ii) the preconditioned current input, (St\u22121S\u22a4\nt\u22121 + 1/\u03bbI)\u22121st,\nwhich we term the inverse probe, and that would allow for solving the least-squares problem in a\nsingle gradient descent step as discussed above. Experimental details on how exactly we carry out\nthese regression analyses can be found in Appendix A6.1.2.\nAs shown in Figure 3 for deep linear self-attention Transformers (see Figure A14 for a softmax\nmodel) we see that both probes can be linearly decoded, with decoding performance increasing with\nsequence length and network depth. Base-optimization has therefore discovered a hybrid algorithm\nthat descends over layers the original mesa-objective Lt(W) while simultaneously improving the\ncondition number of the mesa-optimization problem. This leads to a fast descent of the mesa-objective\nLt(W), Eq. 3. Moreover, we find that performance strongly improves with depth, cf. Figure 3, with a\n6-layer model coming close to but still not matching a single mesa-layer.\nOur probing analysis results therefore support our hypothesis that a fast descent on the autoregressive\nmesa-objective Lt(W) is achieved through mesa-optimization on progressively (across layers) better\npreconditioned data. We point to Figures A12 and A13, and Appendix A6.1.2, for an additional\nconfirmation of this effect, showing that when taking regressed inverse probes as inputs to a linear\nmodel (instead of raw inputs st), the performance of single-step learning significantly improves.\nFull-fledged Transformers.\nTo finish our synthetic data experiments, we relax all previous ar-\nchitectural simplifications and turn to training standard Transformers that use positional encodings,\ninput and output projections, and which need to process raw tokens et = st. We hypothesize that\nafter autoregressive training these models operate in two stages. In a first stage, they use positional\ninformation to re-create our token construction in the first softmax self-attention layer through a\ncopying mechanism, essentially identical to first stage of the induction heads discovered by Olsson\net al. (2022). This effectively corresponds to an internal specification of a mesa-optimization problem.\nSince the states are Markovian, i.e. only depend (linearly) on the immediate previous state, a simple\nnext-token copying mechanism suffices in our toy model. The second part of our hypothesis is that\nsubsequent layers implement a mesa-optimizer that solves the self-constructed least-squares problem.\nFor this second part, we again use our two candidate constructions \u2013 mesa-gradient descent steps and\niterative preconditioning \u2013 to guide our analyses.\nFollowing this hypothesis, we compare three model families, namely, softmax-only Transformers,\nand hybrid models that have a first softmax layer followed by either linear or mesa layers. First,\nwe verify that Transformers of all three types learn copy layers when trained on linear dynamics by\n8\nPreprint\n0\n20\n40\nSequence length t\n0.00\n0.05\n0.10\n0.15\nMSE\nA\nSoftmax-Hy\nLinear-Hy\nMesa-Hy\n0\n5000\nTraining steps\n0.0\n0.5\n1.0\n1.5\n2.0\nst\u2032f(1)\n50\nB\nt\u2032 = 50\nt\u2032 = 49\nt\u2032 = 48\nt\u2032 = 47\nt\u2032 = 46\nt\u2032 < 45\n1\n20\n40\nSequence length t\n1\n0.1\n0.01\nMSE(f(d)\nt\n, st + 1)\nC\nd = 0\nd = 1\nd = 2\nd = 3\nd = 4\nd = 5\nd = 6\nd = 7\n1\n20\n40\nSequence length t\n0.1\n0.01\n0.003\nMSE(f(d)\nt\n, (St\n1ST\nt\n1 + 1/ I)\n1st)\nD\nd = 0\nd = 1\nd = 2\nd = 3\nd = 4\nd = 5\nd = 6\nd = 7\nFigure 4: Reverse engineering full-fledged trained Transformers. We study 2-layer hybrid-\nmesa, 7-layer hybrid-linear, and 7-layer softmax-only Transformers. (A) After training, the hybrid-\nmesa Transformer slightly outperforms the deep hybrid-linear and softmax-only models in terms\nof autoregressive loss. In (B & C & D), we show results for a softmax-only model. The results\nfor a linear-hybrid and an MLP-layernorm model can be found in Appendix A11, A13. (B) The\nfirst softmax layer groups together neighboring tokens. This can be seen in the high sensitivity\nto the current and previous tokens of the outputs of the first layer of a softmax-only Transformer\n(with even more clean next-token copying behavior for hybrid-linear and hybrid-mesa Transformers;\nsee also complementary attention map visualizations in Appendix A3). (B & C) We linearly\nregress the activations of each layer against final targets (C) as well as (St\u22121S\u22a4\nt\u22121 + 1/\u03bbI)\u22121st, the\npreconditioned inputs (D) predicted by our theory. Compared to our more constructed models of\nFigure 3, here we observe a rather harsh transition in the last layer when measuring target probing (C)\nwhile observing a gradual performance increase for early layers when probing for curvature-corrected\ninputs (D). These results are well aligned with our hypothesized two-stage mesa-optimizer. Averages\ncomputed over 5 different seeds; shaded area represents standard deviation.\n(i) computing the sensitivity norm \u2225\u2207st\u2032 f (1)\nt\n(s1:t, \u03b8)\u2225 of the output of the first layer for all t\u2032 \u2264 t,\nand by (ii) inspecting attention maps. We use f (d)\nt\n(s1:t, \u03b8) to denote the intermediate output of the\nd-th layer of a Transformer, including the residual (skip connection) value. Both experiments provide\nevidence that after the first layer, every token mostly depends on itself and on the preceding token, as\nshown in Figure 4B. The corresponding attention maps as well as sensitivity analyses of all models\nincluding hybrid-linear and -mesa can be found in Appendix A3, A6.1.2.\nWe now turn to the post-copying behavior of the models. Although some interpretable identity\nstructure can be observed in the weight matrix products W \u22a4\nKWQ, PWV of the Transformers, cf. Fig-\nures A6 and A8, we speculate that the initial embedding layer introduces too much ambiguity on\nhow the input data is represented and processed by the subsequent attention layers, complicating\nreverse-engineering a clean algorithm. We therefore build on insights extracted from our previous\nanalyses and probe hidden layer activations using the same simple linear regression analysis. Even\nfor this more complex model, we find that again hidden activations gradually (over depth) become\nmore predictive for both the target as well as the inverse probes. Interestingly, we observe a hard-\ntransition-like behavior at the last layer in terms of target decoder performance, in line with our\nconstructed two-stage mesa-optimizer, which first preconditions, and then takes an optimization\nstep in the last layer, see Figure 4C&D and remarkably clear in Figure A11 for softmax resp. linear\nself-attention Transformers. We show qualitatively similar results for Transformers trained with\nMLPs and LayerNorm, cf. Figure A13. For experimental details, see Appendix A6.1.2.\nTaken together, these findings provide evidence that realistic deep Transformers trained autoregres-\nsively on simple linear dynamics implement prediction algorithms based on mesa-optimization\nprinciples. These iterative algorithms allow a standard Transformer to harness depth to almost match\nthe performance of a learned mesa-layer, which achieves optimality for the task considered here.\n5.2\nSIMPLE AUTOREGRESSIVE MODELS BECOME FEW-SHOT LEARNERS\nIn the previous section, we established a close connection between autoregressively-trained Trans-\nformers to gradient-based mesa-optimization. It is therefore natural to ask whether these models can\nbe repurposed to learn in-context when presented with few-shot regression data. Here, we pursue this\n9\nPreprint\na-objective\nataset\ne Transformer\nA \n> Construct mesa-objective\n> Create mesa-dataset\nAutoregressive Transformer\n> Mesa-optimize\n0\n20\n40\n60\nDatapoints (xi, yi) in sequence\n0.5\n1.0\n1.5\n2.0\nMSE\nB\nFew-Shot Regression\nBase\nBase+EOS\nBase+EOS+P\nLSQ\n0\n20\n40\n60\nDatapoints (xi, yi) in sequence\n1.0\n1.5\n2.0\n2.5\nMSE\nC\nContinual Few-Shot Regression\nBase\nBase+EOS\nBase+EOS+P\nLSQ\nFigure 5: Autoregressively-trained Transformers solve supervised few-shot regression problems.\n(A) In-context learning by autoregressive mesa-optimization. (B) The mesa-optimization algorithm\nacquired by training on autoregressive linear dynamics tasks allows softmax Transformers to learn\nsupervised tasks in-context, i.e., the mean-squared error \u27e8(f(xi; \u03b8) \u2212 yi)2\u27e9 decreases gradually\nand significantly with the number of labeled examples. When prompted with a special EOS token\nafter each pair (xi, yi) or a prefix-prompt P at the beginning of an input sequence, which we fine-\ntune for this regression task on a held-out training set, the performance improves considerably,\nhighlighting the usefulness of prompt-tuning already in this very simple setting. (C) Autoregressive\nTransformers already display some continual in-context learning capabilities, being able to learn two\ntasks consecutively. Here, we show the results for the full-fledged softmax-only transformer. The\nresults for the other models can be found in Appendix A6.2. Averages computed over 5 different\nseeds; shaded area represents standard deviation.\nquestion experimentally by changing the generation of the sequences after training, from a linear\ndynamical system to a linear regression task. We illustrate our findings in Figure 5A.\nFew-shot task generative model.\nTo generate our few-shot tasks we still sample a groundtruth\nW \u2217 as a random orthogonal matrix as done during training, but now use this groundtruth model\nto generate a labeled training set {xi, yi}N\ni=1, with inputs xi \u223c N(0, Ix) and targets yi = W \u2217xi.\nWe then present this dataset to our autoregressively-trained Transformers as a sequence of tokens,\nefew-shot = [x1, y1, . . . , xN, yN] of length T = 2N, cf. Figure 5. As the sequence unfolds, and more\ntraining data is presented, we measure in-context learning performance through the mean squared\nerror between the Transformer output f\u03b8(e2i\u22121; efew-shot\n1:2i\u22121 ) and the corresponding target yi = e2i. We\nemphasize that both the sequence generative model and loss function differ from the ones used during\ntraining; compare the task performance metric Lfew-shot = 1\n2\nPN\ni=1 \u2225e2i \u2212 f\u03b8(e2i\u22121; efew-shot\n1:2i\u22121 )\u22252 used\nto evaluate in-context learning performance in this section with the actual loss used to train the\nTransformer, Eq. 10.\nAutoregressive Transformers are capable of few-shot learning.\nAlthough never trained on this\nsetting, we observe that the loss of the Transformer decreases with sequence length, see Figure 5B\nfor results obtained when taking the exact same 7-layer softmax Transformer model analyzed in\nFigure 4, repurposing it for in-context linear regression. The model can thus learn in-context,\nmaking use of additional in-context training data to improve its predictions. As a control, we\nfurther report the performance reached by the least-squares solution (LSQ) obtained on the dataset\nDmesa\nN\n= {(xi, yi)}N\ni=1 \u222a{(yi, xi+1)}N\u22121\ni=1 , and observe a similar decrease in loss. This dataset, where\nhalf of the associations consist of wrong input-output pairs Dspurious\nN\n= {(yi, xi+1)}N\u22121\ni=1 as illustrated\nin Figure 5A, corresponds to the training set an autoregressive Transformer imbued with the mesa-\noptimizers uncovered in the previous section learns from. In this sense, our models achieve a few-shot\nlearning performance that is not far from optimal. Thus, our results show that training Transformers\non simple autoregressive tasks can give rise to in-context few-shot learning, complementing previous\nevidence for this phenomenon in large-scale models (Brown et al., 2020).\nPrompt tuning improves in-context learning performance.\nTo mitigate the influence of wrongly-\nconstructed inputs (yi, xi+1) in a sequence, we fine-tune a single token, which we refer to as the EOS\ntoken, to improve the in-context-learned predictions. Prompt (or prefix) tuning has been shown to lead\n10\nPreprint\nFigure 6: Language modeling experiments on the Pile. We observe improved perplexity and\nin-context learning scores across all our language modeling experiments when switching from\nstandard linear self-attention to the mesa-layer. When comparing loss values for longer time horizons,\ncf. Appendix A20, we still observe a performance gap between softmax and mesa, possibly pointing\ntowards memory issues over long sequences. As hypothesized, we confirm that in all models various\ncopying heads can be found in the first softmax layer, see Appendix A3 for visualizations of the\nattention heads. (A&B) 2-layer Transformers without MLPs and first layers softmax self-attention\nand second layer either softmax, mesa or linear. (C&D) 4-layer Transformers with MLPs and first\nlayers softmax self-attention and rest of the layers either all softmax, mesa or linear.\nto significant performance improvements when applied to large language models (Li & Liang, 2021;\nLester et al., 2021); here we investigate the effectiveness of this technique on our mechanistically-\nunderstood models. When presenting data sequentially as [x1, y1, EOS, x2, y2, . . . , EOS, xN, yN] we\nobserve a considerable performance improvement after prompt-tuning, see Figure 5B. Furthermore,\nto \u2018guide\u2019 the model for few-shot tasks, we learn a single prefix-prompt P which we append at the\nbeginning of a sequence with EOS tokens. This appears to further improve the few-shot performance\nfor early data-pairs. Additional experimental details can be found in Appendix A6.2.\nContinual in-context learning.\nLastly, we demonstrate the capability of our trained Transformers\nto learn multiple tasks in a row. We study the minimal setup where the model has to learn two\ntasks, generated from two distinct groundtruth linear models with parameters W \u2217,1, W \u2217,2 sampled as\ndescribed above, resulting in a sequence of data of the form [x1\n1, y1\n1, . . . , x1\nN, y1\nN, x2\n1, y2\n1, . . . , x2\nN, y2\nN].\nWe plot the performance when using EOS tokens (constructed as before) and prefix prompts P, as\nwell. In Figure 5C we see that the trained Transformer has the capability to overwrite the first and\nlearn a second task in-context, even though it was never explicitly trained to solve such sequential\nlearning problems.\nA toy model for in-context learning.\nWe conclude that Transformers trained to predict the next\nelement in a sequence can be naturally repurposed as in-context learners due to the similarity of\nthe algorithms implemented within their forward pass. This allows studying in a controlled setting\ninteresting properties of in-context learning, such as the advantages of prompt tuning and the ability\nto learn continually. Our toy models could serve as a test bed for future work investigating the\nshortcomings and various particularities of in-context learning observed in LLMs (e.g., Chan et al.,\n2022a; Min et al., 2022; Kossen et al., 2023).\n5.3\nLANGUAGE MODELS EQUIPPED WITH LEAST-SQUARES SOLVERS\nWe now move beyond synthetic tasks and provide results on autoregressive language modeling, a\nproblem domain Transformers have revolutionized in recent years. Because reverse-engineering\nthe ensuing models to the degree of our previous analyses is difficult, we base our claims on\nperformance comparisons between standard Transformers, and new variants based on the mesa-layer.\nOur hypothesis is that the mesa-layer will improve the in-context learning and working memory\ncapabilities of a Transformer, in particular of the linear kind. We further hypothesize that this in turn\ntranslates to language modeling improvements, based on the high correlation between in-context\nlearning and actual autoregressive loss reported by Kaplan et al. (2020). We therefore quantify\nperformance along two axes: the next-token prediction loss, the actual objective of base-optimization;\nand the ability to learn in-context, measured as the difference in loss calculated over two timepoints\nwithin a sequence, as defined by Kaplan et al. (2020) and Olsson et al. (2022).\n11\nPreprint\nFigure 7:\nSingle-layer Transformers with\nkey-shifts, the Pile. We observe improved (A)\nperplexity and (B) in-context learning scores when\ncomparing one linear to one mesa layer with dif-\nferent DPFP sizes \u03bd \u2208 {0, 1, 2, 3}, corresponding\ninversely to color fade. Mesa layers consistently\noutperform linear layers, catching up with softmax.\nWe train Transformers with various architec-\ntural configurations on the Pile (Gao et al.,\n2020), a large compilation of various English\ntext datasets including parts of Wikipedia, arXiv,\nand code. We always model the first layer using\nsoftmax self-attention in all experiments. This\ndecision is based on insights from our previous\nexperiments, where base-optimization consis-\ntently attributed a mesa-objective creation role\nto this layer. We then compare pure softmax-\nonly Transformers to two types of hybrid mod-\nels, where the subsequent layers are either lin-\near or mesa. We vary the depth of our mod-\nels, from 2-layer attention-only to deeper 4-\nattention-layer models endowed with tokenwise\nMLPs which are present by default in standard\nTransformers. By transforming the data nonlin-\nearly, MLP layers allow solving nonlinear regression problems by mesa-gradient descent. Following\nthis reasoning, we further adopt in our hybrid-linear and hybrid-mesa Transformers the deterministic\nparameter-free projection (DPFP, size denoted by \u03bd) due to Schlag et al. (2021), a non-learned and\nsimple to compute nonlinear transformation of keys and queries. We found that this significantly\nimproved the performance of non-softmax attention layers. Finally, to represent discrete input sym-\nbols as real-valued vectors, we learn a vocabulary of real-valued vectors using the standard GPT-2\ntokenizer. All architectural and training details can be found in Appendix A6.3. We note that all\nmodels have an (almost) identical number of parameters.\nIn line with our synthetic experiments, we observe stable learning across all model types of copying\nlayers, indicated by the constant attention to tokens in direct or close proximity, as shown in Figure\nA1. We therefore reproduce the findings of Olsson et al. (2022), extending them to models that\ninclude other forms of attention. This phenomenon is predicted by the mesa-optimization theory\npresented here, where copy layers serve the purpose of constructing internal mesa-objective functions.\nWe note that, in contrast to our previous synthetic linear prediction tasks, the Pile is no longer\nMarkovian of order 1. This is reflected in the more complicated attention maps, indicating more\ninvolved copying behavior. Additionally, we run an ablation where we compare to a single-layer\ncontrol model whose first softmax layer is removed and replaced by a hardcoded one-step key-shift\noperator, cf. Appendix A6.3. Interestingly, such an operator can be found in previous work (Olsson\net al., 2022; Fu et al., 2023). Again, we verify the findings of Olsson et al. (2022) and observe strong\nin-context learning scores, within a single layer, with the mesa-layer performing on-par with softmax,\nsee Figure 7. As in Schlag et al. (2021), DPFP features substantially improve performance; we fix\n\u03bd = 3 for the linear as well as the mesa layer for all other language modeling experiments.\nWe find that the hybrid-mesa Transformers dominate their hybrid-linear counterparts in terms of\nperformance, across all configurations, essentially matching (for 2-layer models) or coming closer\n(for 4-layer models with MLPs) to pure-softmax Transformers, cf. Figure 6. We leave for future\nwork studying the mesa-layer equipped with forgetting factors, see Appendix A2.1, which could\nfurther improve upon our results here. This is reflected both in terms of perplexity and in-context\nlearning scores. Strictly speaking, these results are not sufficient to make claims on whether mesa-\noptimization is occurring within standard Transformers. However, the high performance achieved\nby the hybrid-mesa models, which operate on mesa-optimization principles by design, suggests that\nmesa-optimization might be happening within conventional Transformers. More reverse-engineering\nwork is needed to add weight to this conjecture.\n6\nDISCUSSION\nWe presented evidence that Transformer models are capable of developing gradient-based inference\nalgorithms when trained on sequence prediction tasks under a standard autoregressive objective. We\ntherefore confirmed that recent results obtained under a multi-task, meta-learning setup translate\nto the conventional self-supervised LLM training setup. Moreover, we have seen that the resulting\n12\nPreprint\nautoregressive inference algorithms can be repurposed without retraining to solve supervised in-\ncontext learning tasks, thus explaining the aforementioned results within a single, unified framework.\nIt should be noted that our reverse-engineering findings are for now restricted to simple linear\nprediction tasks. More work is needed to understand how and if our findings translate to the nonlinear\nsetting, and more generally to determine the conditions that lead some base optimization process\nto pick solutions corresponding to gradient-based in-context learning algorithms. It seems unlikely\nthat the internal construction and gradient-based solution of least-squares problems is a universal\nmechanistic explanation of trained Transformers. An interesting future work direction is to attempt to\nreverse-engineer and describe through mesa-optimization models trained on problems of a radically\ndifferent kind than those considered here, such as algorithmic reasoning (Liu et al., 2023).\nThe idea that a Transformer generates its predictions by solving one or more internal optimization\nproblems has ties to many different lines of thinking in machine learning. One closely related line\nof work explores the concept of a declarative node: a differentiable layer whose output is defined\nimplicitly as the solution of an optimization problem (Amos & Kolter, 2017; Gould et al., 2021;\nZucchet & Sacramento, 2022). The mesa-layer is an example of such a node. Summarizing the\noperation of an entire chain of layers with thousands of parameters by a single declarative node is not\nonly potentially more efficient, but also more interpretable. We thus join a line of interesting recent\nwork exploring the advantages of including declarative nodes within attention-based models (Martins\net al., 2020; Garnelo & Czarnecki, 2023).\nOur reverse-engineering analyses brought a surprising revelation: gradient-based base-optimization\nof an autoregressive loss discovered such a declarative node, at least when the underlying sequence\nwas generated by a linear dynamics. This discovery or selection of an optimization algorithm through\nlearning has been termed mesa-optimization (Hubinger et al., 2019), a notion that we have adopted\nthroughout this paper. While we do not wish to comment here on the possible risks associated with\nmesa-optimization, we point out that our results may be of interest to the artificial intelligence safety\ncommunity, by providing a simple mesa-optimization toy model.\nThe mesa-layer can also be seen as a locally-optimal fast weight programmer from the perspective of\nSchmidhuber (1992). In his seminal work, Schmidhuber (1992) proposed to dynamically reprogram\nthe weights of a feedforward neural network using a Hebbian rule. As pointed out by Schlag et al.\n(2021) and as can be seen from Eq. 2, this is precisely what a linear self-attention layer does: it\ngenerates predictions using an effective weight matrix that is learned during a forward pass by taking\nouter products of values and keys, a Hebbian associative rule (Hebb, 1949). In this work, we instead\nframe fast weight learning as an optimization problem, that is efficiently and optimally solved at\nevery moment in time by the mesa-layer. This form of optimal fast learning is strictly superior to\nHebb\u2019s rule, both in terms of generalization and memory capacity (Hertz et al., 1991). The mesa-layer\nis therefore also closely related to the Delta-Net of Schlag et al. (2021), which uses the delta rule\n(Widrow & Hoff, 1960) for fast weight learning. Unlike the mesa-layer which is optimal at every\ntime step, this rule requires multiple steps to converge, but it is cheaper to implement.\nWhen using mesa-layers in an autoregressive Transformer, the base-optimization process becomes\nexplicitly a meta-learning algorithm (Thrun & Pratt, 1998). This algorithm should however be\ndistinguished from the end-to-end supervised meta-learning approaches that are currently highly\npopular in machine learning (e.g., Ravi & Larochelle, 2017; Finn et al., 2017; Hochreiter et al.,\n2001). In our models, everything is ultimately driven by the pressure to predict the future, the signal\nthat drives the slow autoregressive base-optimization process. This process ultimately dictates the\nobjectives each layer must optimize. Moreover and also unusually for meta-learning, each mesa-layer\nis a greedy supervised local learner, which does not use backpropagation or any other kind of global\nerror information. Instead, each mesa-layer has its own local objective functions specified through\nthe corresponding key and value matrices.\nSeen from this angle, our work has an unexpected connection to research on local learning rules, a\nquestion of great interest in theoretical neuroscience (Lillicrap et al., 2020). Decomposing a global\nsupervised learning problem into a sequence of local quadratic optimization problems, as we do\nhere, is at the heart of the target propagation (Lee et al., 2015), predictive coding (Whittington &\nBogacz, 2017) and control-based (Meulemans et al., 2022) theories of learning in the brain, and\nprevious studies have proposed greedy layerwise learning algorithms that do not require global\nerror information (Hinton et al., 2006; N\u00f8kland & Eidnes, 2019; Belilovsky et al., 2019; L\u00f6we\n13\nPreprint\net al., 2019; Hinton, 2022). Our study introduces greedy local learning algorithms, which only use\nbottom-up information, to the fast timescale of inference. It is interesting that our models achieve\nstrong performance in natural tasks without any top-down feedback at fast timescales, at odds with\ncanonical predictive coding theories (Mumford, 1992; Rao & Ballard, 1999).\nWe finish by sharing our excitement about future research directions that aim at analyzing simple\nautoregressively-trained sequence models like Transformers and in particular in-context learning\nwithin by reverse engineering. We hope our work motivates further studies trying to describe\nthe emergence of single, multiple or mixture of expert models mesa-optimized in simple trained\nTransformers (Bai et al., 2023) which we hypothesize could illicit inference reminiscent to world\nmodels (Ha & Schmidhuber, 2018; Werbos, 1987). Furthermore, the insights we gained in our\ncontrolled setting could motivate studying limitations and particularities of in-context learning (Min\net al., 2022; Kossen et al., 2023) and its powerful variants such as chain-of-thought prompting (Wei\net al., 2022; Li et al., 2023b; Giannou et al., 2023) as well as the fascinating interplay between\nin-weights and in-context learning (Chan et al., 2022b).\nACKNOWLEDGMENTS\nJo\u00e3o Sacramento and Johannes von Oswald deeply thank Angelika Steger and Jyrki Alakuijala for\ntheir support and guidance. The authors also thank Marc Kaufmann and Yassir Akram for many\nvaluable insights throughout the project and especially thank Andrey Zhmoginov for many fruitful\ndiscussions. Furthermore, we are grateful to Luke Sernau and Alexander Meulemans providing\nvaluable comments on the manuscript. Jo\u00e3o Sacramento and Nicolas Zucchet were supported by\nan Ambizione grant (PZ00P3_186027) from the Swiss National Science Foundation and an ETH\nResearch Grant (ETH-23 21-1).\nREFERENCES\nKwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement\npreconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297, 2023.\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning\nalgorithm is in-context learning? Investigations with linear models. In International Conference of\nLearning Representations, 2023.\nGuillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes.\nIn International Conference of Learning Representations, 2017.\nBrandon Amos and J. Zico Kolter. OptNet: Differentiable optimization as a layer in neural networks.\nIn International Conference on Machine Learning, 2017.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv preprint\n1607.06450, 2016.\nIgor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky,\nDavid Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Antoine Dedieu, Claudio Fantacci,\nJonathan Godwin, Chris Jones, Ross Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou, Steven\nKapturowski, Thomas Keck, Iurii Kemaev, Michael King, Markus Kunesch, Lena Martens, Hamza\nMerzic, Vladimir Mikulik, Tamara Norman, George Papamakarios, John Quan, Roman Ring,\nFrancisco Ruiz, Alvaro Sanchez, Laurent Sartran, Rosalia Schneider, Eren Sezener, Stephen\nSpencer, Srivatsan Srinivasan, Milo\u0161 Stanojevi\u00b4c, Wojciech Stokowiec, Luyu Wang, Guangyao\nZhou, and Fabio Viola. The DeepMind JAX Ecosystem, 2020.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In International Conference of Learning Representations, 2015.\nYu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians:\nprovable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637,\n2023.\nEugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can scale\nto ImageNet. In International Conference on Machine Learning, 2019.\n14\nPreprint\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclau-\nrin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang.\nJAX: composable transformations of Python+NumPy programs, 2018.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural\nInformation Processing Systems, volume 33, 2020.\nStephanie C. Y. Chan, Ishita Dasgupta, Junkyung Kim, Dharshan Kumaran, Andrew K. Lampinen,\nand Felix Hill. Transformers generalize differently from information stored in context vs in weights.\narXiv preprint arXiv:2210.05675, 2022a.\nStephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H.\nRichemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent in-\ncontext learning in transformers. Advances in Neural Information Processing Systems, 35, 2022b.\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy\nColwell, and Adrian Weller. Rethinking attention with performers. In International Conference of\nLearning Representations, 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT, 2019.\nNan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, and Radu Soricut. CausalLM is not\noptimal for in-context learning. arXiv preprint arXiv:2308.06912, 2023.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. In International Conference on Machine Learning, 2017.\nQuentin Fournier, Ga\u00e9tan Marceau Caron, and Daniel Aloise. A practical survey on faster and lighter\ntransformers. ACM Computing Surveys, 55(14s), 2023.\nDaniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry\nhungry hippos: towards language modeling with state space models. In International Conference\nof Learning Representations, 2023.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: an 800GB\ndataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\nShivam Garg, Dimitris Tsipras, Percy S. Liang, and Gregory Valiant. What can transformers learn\nin-context? A case study of simple function classes. In Advances in Neural Information Processing\nSystems, volume 35, 2022.\nMarta Garnelo and Wojciech Marian Czarnecki. Exploring the space of key-value-query models with\nintention. arXiv preprint arXiv:2305.10203, 2023.\nCarl Friedrich Gauss. Theoria combinationis observationum: erroribus minimis obnoxiae. Societas\nRegia Scientiarum Gottingensis, 1821.\nAngeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris\nPapailiopoulos. Looped transformers as programmable computers. In International Conference on\nMachine Learning, 2023.\nStephen Gould, Richard Hartley, and Dylan John Campbell. Deep declarative networks. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 2021.\nDavid Ha and J\u00fcrgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.\n15\nPreprint\nRichard H. R. Hahnloser, Rahul Sarpeshkar, Misha A. Mahowald, Rodney J. Douglas, and H. Sebas-\ntian Seung. Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit.\nNature, 405(6789):947\u2013951, 2000.\nCharles R. Harris, K. Jarrod Millman, St\u00e9fan J. van der Walt, Ralf Gommers, Pauli Virtanen, David\nCournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti\nPicus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern\u00e1ndez del\nR\u00edo, Mark Wiebe, Pearu Peterson, Pierre G\u00e9rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren\nWeckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with\nNumPy. Nature, 585(7825):357\u2013362, 2020.\nDonald O. Hebb. The Organization of Behavior: A Neuropsychological Theory. Wiley, New York,\n1949.\nJonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas\nSteiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023.\nTom Hennigan, Trevor Cai, Tamara Norman, Lena Martens, and Igor Babuschkin. Haiku: Sonnet for\nJAX, 2020.\nJohn Hertz, Richard G. Palmer, and Anders S. Krogh. Introduction to the Theory of Neural Computa-\ntion. Perseus Publishing, 1st edition, 1991.\nGeoffrey Hinton. The forward-forward algorithm: Some preliminary investigations. arXiv preprint\narXiv:2212.13345, 2022.\nGeoffrey Hinton, Simon Osindero, and Yee Whye Teh. A Fast Learning Algorithm for Deep Belief\nNets. Neural Computation, 18:1527\u20131554, 2006.\nSepp Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning to learn using gradient descent.\nIn Artificial Neural Networks \u2014 ICANN 2001, 2001.\nEvan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Risks from\nlearned optimization in advanced machine learning systems. arXiv preprint 1906.01820, 2019.\nJ. D. Hunter. Matplotlib: A 2D graphics environment. Computing in Science & Engineering, 9(3):\n90\u201395, 2007.\nRichard M. Johnstone, C. Richard Johnson, Robert R. Bitmead, and Brian D. O. Anderson. Exponen-\ntial convergence of recursive least squares with exponential forgetting factor. Systems & Control\nLetters, 2(2):77\u201382, 1982.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are\nRNNs: fast autoregressive transformers with linear attention. In International Conference on\nMachine Learning, 2020.\nDiederik P. Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In International\nConference on Learning Representations, 2015.\nLouis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-context\nlearning by meta-learning transformers. In Sixth Workshop on Meta-Learning at the Conference\non Neural Information Processing Systems, 2022.\nJannik Kossen, Tom Rainforth, and Yarin Gal. In-context learning in large language models learns\nlabel relationships but is not conventional learning. arXiv preprint arXiv:2307.12375, 2023.\nDong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation.\nIn Joint European Conference on Machine Learning and Knowledge Discovery in Databases,\n2015.\n16\nPreprint\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, 2021.\nXiang Lisa Li and Percy Liang. Prefix-tuning: optimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics, 2021.\nYingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers\nas algorithms: Generalization and stability in in-context learning. In International Conference on\nMachine Learning, 2023a.\nYingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, and Samet Oymak.\nDissecting chain-of-thought: a study on compositional in-context learning of MLPs. arXiv preprint\narXiv:2305.18869, 2023b.\nTimothy P. Lillicrap, Adam Santoro, Luke Marris, Colin J. Akerman, and Geoffrey Hinton. Back-\npropagation and the brain. Nature Reviews Neuroscience, 21(6):335\u2013346, 2020.\nBingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers\nlearn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2023.\nSindy L\u00f6we, Peter O\u2019Connor, and Bastiaan Veeling. Putting an end to end-to-end: Gradient-isolated\nlearning of representations. In Advances in Neural Information Processing Systems, volume 32,\n2019.\nArvind Mahankali, Tatsunori B. Hashimoto, and Tengyu Ma. One step of gradient descent is\nprovably the optimal in-context learner with one layer of linear self-attention. arXiv preprint\narXiv:2307.03576, 2023.\nAndr\u00e9 Martins, Ant\u00f3nio Farinhas, Marcos Treviso, Vlad Niculae, Pedro Aguiar, and Mario Figueiredo.\nSparse and continuous attention mechanisms. In Advances in Neural Information Processing\nSystems, volume 33, 2020.\nAlexander Meulemans, Nicolas Zucchet, Seijin Kobayashi, Johannes von Oswald, and Jo\u00e3o Sacra-\nmento. The least-control principle for local learning at equilibrium. In Advances in Neural\nInformation Processing Systems, volume 35, 2022.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke\nZettlemoyer. Rethinking the role of demonstrations: what makes in-context learning work? In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022.\nDavid Mumford. On the computational architecture of the neocortex. Biological Cybernetics, 66(3):\n241\u2013251, 1992.\nArild N\u00f8kland and Lars Hiller Eidnes.\nTraining neural networks with local error signals.\nIn\nInternational Conference on Machine Learning, 2019.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,\nBen Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli,\nZac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane\nLovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish,\nand Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022.\nRajesh P. N. Rao and Dana H. Ballard. Predictive coding in the visual cortex: a functional in-\nterpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2(1):79\u201387,\n1999.\nAllan Ravent\u00f3s, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the\nemergence of non-Bayesian in-context learning for regression. arXiv preprint arXiv:2306.15063,\n2023.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International\nConference on Learning Representations, 2017.\n17\nPreprint\nImanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight\nprogrammers. In International Conference on Machine Learning, 2021.\nJ\u00fcrgen Schmidhuber. Learning to control fast-weight memories: an alternative to dynamic recurrent\nnetworks. Neural Computation, 4(1):131\u2013139, 1992.\nJack Sherman and Winifred J. Morrison. Adjustment of an inverse matrix corresponding to a change\nin one element of a given matrix. The Annals of Mathematical Statistics, 21(1):124\u2013127, 1950.\nSebastian Thrun and Lorien Pratt. Learning to learn. Springer US, 1998.\nMarcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R. Ciosici, Michael\nHassid, Kenneth Heafield, Sara Hooker, Colin Raffel, Pedro H. Martins, Andr\u00e9 F. T. Martins,\nJessica Zosa Forde, Peter Milder, Edwin Simpson, Noam Slonim, Jesse Dodge, Emma Strubell,\nNiranjan Balasubramanian, Leon Derczynski, Iryna Gurevych, and Roy Schwartz. Efficient meth-\nods for natural language processing: a survey. Transactions of the Association for Computational\nLinguistics, 11, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\nProcessing Systems, volume 30, 2017.\nJohannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev,\nAndrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In\nInternational Conference on Machine Learning, 2023.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768, 2020.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc\nLe, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems, volume 35, 2022.\nPaul J. Werbos. Learning how the world works: Specifications for predictive networks in robots and\nbrains. In Proceedings of IEEE International Conference on Systems, Man and Cybernetics, NY,\n1987.\nJames C. R. Whittington and Rafal Bogacz. An approximation of the error backpropagation algorithm\nin a predictive coding network with local Hebbian synaptic plasticity. Neural Computation, 29(5):\n1229\u20131262, 2017.\nBernard Widrow and Marcian E. Hoff. Adaptive switching circuits. In IRE WESCON convention\nrecord, volume 4, 1960.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context\nlearning as implicit Bayesian inference. In International Conference of Learning Representations,\n2022.\nRuiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context.\narXiv preprint arXiv:2306.09927, 2023.\nNicolas Zucchet and Jo\u00e3o Sacramento. Beyond backpropagation: bilevel optimization through\nimplicit differentiation and equilibrium propagation. Neural Computation, 34(12), 2022.\n18\nPreprint\nAppendix\nTable of Contents\nA1 Mesa layer with forgetting factors\n19\nA1.1 Computing the inverse term within \u02c6\nW mesa\nt\n. . . . . . . . . . . . . . . . . . . .\n20\nA1.2 Computing \u2206emesa\nt\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nA2 Mesa layer backward computation\n20\nA2.1 Mesa layer computation backward pass via Sherman-Morrison . . . . . . . . . .\n20\nA2.2 Alternative derivation through the implicit function theorem . . . . . . . . . . .\n22\nA2.3 Parallel backward pass through Neumann series approximation\n. . . . . . . . .\n23\nA3 Visualization of weights and attention maps of trained Transformers\n25\nA4 Mechanistic interpretability of Transformers trained on linear dynaimcs\n31\nA4.1 Single-layer mesa-gradient descent . . . . . . . . . . . . . . . . . . . . . . . .\n31\nA4.2 Multi-layer accelerated mesa-gradient descent\n. . . . . . . . . . . . . . . . . .\n33\nA5 Analysing contracting linear dynamics\n36\nA6 Experimental details\n36\nA6.1 Training Transformers on linear dynamical systems . . . . . . . . . . . . . . . .\n36\nA6.2 Testing trained Transformers on few-shot in-context learning . . . . . . . . . . .\n40\nA6.3 Language modeling experiments\n. . . . . . . . . . . . . . . . . . . . . . . . .\n45\nA7 Software\n45\nA1\nMESA LAYER WITH FORGETTING FACTORS\nHere, we revisit the mesa-layer forward pass introduced in Section 4, with an added forget factor\n\u0393h,t = (\u03b3h,t\u2032)t\nt\u2032=1, where \u03b3h,t\u2032 \u2208 (0, 1].\nAlthough we leave an empirical investigation of the forget gate for future work, we hypothesize that a\ntoken-dependent forget gate can benefit the performance of the layer by allowing selective memory\nretention and forgetting. Nevertheless, we stress potential initialization and numerical issues as well\nas training instabilities when computing the necessary products of factors across time.\nGiven again a set of tokens Et, the generalized mesa-layer changes the tokens as follows:\n\u2206emesa\nt\n=\nH\nX\nh=1\nPh \u02c6W mesa\nh,t qh,t,\n(A1)\nwith\n\u02c6W mesa\nh,t\n= arg min\nW\n(\n1\n2\nt\nX\nt\u2032=1\n \ntY\nt\u2032\u2032=t\u2032+1\n\u03b3h,t\u2032\u2032\n!\n||Wkh,t\u2032 \u2212 vh,t\u2032||2 +\nQt\nt\u2032\u2032=1 \u03b3h,t\u2032\u2032\n2\u03bbh\n||W||2\nF\n)\n.\n(A2)\nThis is known as the recursive least squares problem with forgetting and is widely used in the online\nlearning literature (Johnstone et al., 1982). For notational simplicity we drop the subscript in h and\nignore the sum over the heads in the following derivation. It can be shown that the analytical solution\nof the optimization problem is\n\u02c6W mesa\nt\n=\n \nt\nX\nt\u2032=1\n \ntY\nt\u2032\u2032=t\u2032+1\n\u03b3t\u2032\u2032\n!\nvt\u2032k\u22a4\nt\u2032\n!  \nt\nX\nt\u2032=1\n \ntY\nt\u2032\u2032=t\u2032+1\n\u03b3t\u2032\u2032\n!\nkt\u2032k\u22a4\nt\u2032 +\nQt\nt\u2032\u2032=1 \u03b3t\u2032\u2032\n\u03bb\nI\n!\u22121\n(A3)\nWe will now see how \u2206emesa\nt\ncan be efficiently computed in a forward pass.\n19\nPreprint\nA1.1\nCOMPUTING THE INVERSE TERM WITHIN \u02c6W mesa\nt\nComputing the full-fledged inverse at every timestep is computationally too expensive. As in Section 4,\nwe resort to using the Sherman-Morrison formula to efficiently compute the inverse term for all\ntimestep sequentially in time. We redefine\nRt =\n \nt\nX\nt\u2032=1\n \ntY\nt\u2032\u2032=t\u2032+1\n\u03b3t\u2032\u2032\n!\nkt\u2032k\u22a4\nt\u2032 +\nQt\nt\u2032\u2032=1 \u03b3t\u2032\u2032\n\u03bb\nI\n!\u22121\n.\n(A4)\nIt satisfies the recursive formula\nRt+1 =\n\u0000\u03b3tR\u22121\nt\n+ kt+1k\u22a4\nt+1\n\u0001\u22121\n(A5)\nwith R0 = \u03bbI, and the Sherman-Morrison formula thus gives\nRt+1 = \u03b3\u22121\nt+1\n\u0000R\u22121\nt\n+ \u03b3\u22121\nt+1kt+1k\u22a4\nt+1\n\u0001\u22121\n(A6)\n= \u03b3\u22121\nt+1\n \nRt \u2212 \u03b3\u22121\nt+1Rtkt+1k\u22a4\nt+1Rt\n1 + \u03b3\u22121\nt+1k\u22a4\nt+1Rtkt+1\n!\n(A7)\n= \u03b3\u22121\nt+1\n\u0012\nRt \u2212\nRtkt+1k\u22a4\nt+1Rt\n\u03b3t+1 + k\u22a4\nt+1Rtkt+1\n\u0013\n.\n(A8)\nNote that we recover Eq. 8 by setting all \u03b3t to 1.\nA1.2\nCOMPUTING \u2206emesa\nt\nGiven Rh,t for all heads, we can rewrite the token update as\n\u2206emesa\nt\n=\nH\nX\nh=1\nPh\n \nt\nX\nt\u2032=1\n \ntY\nt\u2032\u2032=t\u2032+1\n\u03b3h,t\u2032\u2032\n!\nvh,t\u2032k\u22a4\nh,t\u2032\n!\nRh,tqh,t\n(A9)\n=\nH\nX\nh=1\nPhVh\n\uf8eb\n\uf8ed\n \n1t\u2032\u2264t\ntY\nt\u2032\u2032=t\u2032+1\n\u03b3h,t\u2032\u2032\n!\u22a4\nt\u2032=1\n\u2299 K\u22a4\nh \u02dcqh,t\n\uf8f6\n\uf8f8\n(A10)\n=\nH\nX\nh=1\nPhVh\n\u0000M:,t \u2299 K\u22a4\nh \u02dcqh,t\n\u0001\n(A11)\nwhere \u02dcqh,t = Rh,tqh,t and Mt\u2032,t := 1t\u2032\u2264t\nQt\nt\u2032\u2032=t\u2032+1 \u03b3h,t\u2032\u2032. Note that we apply some form causal\nmasking here: we take the key Kh \u2208 RDa\u00d7T and value matrices Vh \u2208 RDa\u00d7T with all the sequence\ntimesteps and select the entries occurring before time t. The main difference with the usual causal\nmask (1t\u2032\u2264t)t\u2032,t is the inclusion of the forget factors. It can be efficiently computed leveraging partial\nproducts. We conclude by remarking that the same mask can be applied to softmax attention layers,\napplying it to the key-queries products before the softmax.\nA2\nMESA LAYER BACKWARD COMPUTATION\nA2.1\nMESA LAYER COMPUTATION BACKWARD PASS VIA SHERMAN-MORRISON\nIn this section, we detail how to compute the backward pass of the mesa layer with forget factor\ndetailed in Section A1. Recall that the forward pass of the Mesa layer is computed recursively\nfollowing\nRh,t+1 = \u03b3\u22121\nh,t+1\n \nRh,t \u2212\nRh,tkh,t+1k\u22a4\nh,t+1Rh,t\n\u03b3h,t+1 + k\u22a4\nh,t+1Rh,tkh,t+1\n!\n(A12)\n\u2206et,mesa =\nH\nX\nh=1\nPhVh\n\u0000M:,t \u2299 K\u22a4\nh \u02dcqh,t\n\u0001\n(A13)\n20\nPreprint\nwith Rh,0 = \u03bbhI.\nThe forward pass can be decomposed into 3 steps:\n1. First, the matrices Rt,h are computed sequentially.\n2. Then, for all t and h, the transformed queries \u02dcqh,t = Rh,tqh,t are computed.\n3. Finally, using the transformed queries \u02dcQh = (\u02dcqh,t)t as the queries, a standard cross-attention\noperation is computed from (Vh, Kh, \u02dcQh) using the causal mask M that includes forgetting\nrates.\nWhile the backward pass of 2 and 3 can be computed easily with automatic differentiation tools\nwithout much overhead compared to standard attention layers, the same thing cannot be said about\n1. We will here discuss how the backward pass of the computation of \u02dcQh can be computed in a\nmemory-efficient way. Without loss of generality, we drop the subscript h for notational simplicity.\nThe issue with automatic differentiation out of the box.\nFor all time t, \u02dcqt = Rtqt depends on qt,\nbut also Kt, \u0393t and \u03bb through the variable Rt.\nIn the backward pass, we are given as input the gradient of the loss function w.r.t. \u02dcQ, namely dL\nd\u02dcqt for\nall t. The goal is then to compute the gradient of the loss w.r.t. the input of \u02dcQ, namely dL\ndkt , dL\nd\u03b3t , dL\ndqt\nand dL\nd\u03bb , which can be achieved via the chain rule.\nWhile using automatic differentiation out of the box would take care of this computation, it would\nrequire in particular the storing of all intermediate variables Rt, which can be prohibitively expensive.\nMemory efficient custom backward pass.\nInstead, we will show that storing the matrices K, \u0393, Q\nas well as RT where T is the last time step of the training sequence, is sufficient to exactly compute\nthe backward pass. Indeed, given the aforementioned inputs, all Rt can be recomputed in linear\ncomplexity w.r.t. T, which means we can reconstruct recursively the inputs of \u02dcqt at all time steps.\nBy noticing that Rt\u22121 = \u03b3t(R\u22121\nt\n\u2212ktk\u22a4\nt )\u22121, we can apply the Sherman-Morrison formula backwards\nto obtain Rt\u22121 as\nRt\u22121 = \u03b3t\n\u0012\nRt \u2212\nRt(\u2212kt)k\u22a4\nt Rt\n1 + (\u2212kt)\u22a4Rtkt\n\u0013\n(A14)\n= \u03b3t\n\u0012\nRt \u2212 Rtktk\u22a4\nt Rt\nk\u22a4\nt Rtkt \u2212 1\n\u0013\n(A15)\nWe will now show how accumulating the right error signal and leveraging the vector-jacobian product\ntrick together with automatic differentiation tools is sufficient for computing the full backward pass\nrecursively.\nFirstly, given the error signal and reconstructed Rt allows the computation of dL\ndqt via\ndL\ndqt\n= dL\nd\u02dcqt\nd\u02dcqt\ndqt\n= dL\nd\u02dcqt\nSt\n(A16)\nSecondly, we rewrite \u02dcqt as a function of kt, \u03b3t, Rt\u22121 and qt, i.e.\n\u02dcqt = Rforward(Rt\u22121, kt, \u03b3t)qt\n(A17)\nSince L depends on kt only via both \u02dcqt and Rt, we can then rewrite\ndL\ndkt\n= dL\nd\u02dcqt\nd\u02dcqt\ndkt\n+ dL\ndRt\ndRt\ndkt\n(A18)\n= dL\nd\u02dcqt\n\u2202\u02dcqt\n\u2202kt\n+ dL\ndRt\n\u2202Rt\n\u2202kt\n(A19)\n21\nPreprint\nwhere, provided Rt\u22121, kt, \u03b3t and qt, \u2202\u02dcqt\n\u2202kt can be computed easily using e.g. automatic differentiation\ntools. Similarly, we have,\ndL\nd\u03b3t\n= dL\nd\u02dcqt\n\u2202\u02dcqt\n\u2202\u03b3t\n+ dL\ndRt\n\u2202Rt\n\u2202\u03b3t\n(A20)\nNotice that\ndL\ndRt can be computed recursively following the chain rule\ndL\ndRt\u22121\n= dL\ndRt\n\u2202Rt\n\u2202Rt\u22121\n+ dL\nd\u02dcqt\n\u2202\u02dcqt\n\u2202Rt\u22121\n(A21)\nwhere again, provided Rt\u22121, kt, \u03b3t and qt, both terms can be computed efficiently with standard\nautomatic differentiation tools coupled with the well known vector-Jacobian product trick given the\nquantities dL\ndRt and dL\nd\u02dcqt .\nThirdly, we can show that\ndL\nd\u03bb = Tr\n\u0014 dL\ndR0\n\u0015\n(A22)\nCombining everything, we can now implement the backward computation recursively via the follow-\ning equations:\nRt\u22121 = \u03b3t\n\u0012\nRt \u2212 Rtktk\u22a4\nt Rt\nk\u22a4\nt Rtkt \u2212 1\n\u0013\n(A23)\ndL\ndRt\u22121\n= dL\ndRt\n\u2202Rt\n\u2202Rt\u22121\n+ \u2202L\n\u2202\u02dcqt\n\u2202\u02dcqt\n\u2202Rt\u22121\n(A24)\ndL\ndkt\n= dL\nd\u02dcqt\n\u2202\u02dcqt\n\u2202kt\n+ dL\ndRt\n\u2202Rt\n\u2202kt\n(A25)\ndL\nd\u03b3t\n= dL\nd\u02dcqt\n\u2202\u02dcqt\n\u2202\u03b3t\n+ dL\ndRt\n\u2202Rt\n\u2202\u03b3t\n(A26)\ndL\ndqt\n= dL\nd\u02dcqt\nRt\n(A27)\ndL\nd\u03bb = Tr\n\u0014 dL\ndR0\n\u0015\n(A28)\nRT is assumed to be given and\ndL\ndRT\n= 0. The above equations only require the storage of\ndL\ndRt ,\ndL\ndRt\u22121 , Rt, Rt\u22121 at all time, and computes the backward pass in a similar time and mem-\nory complexity as for the forward pass. The derivation is identical without forgetting factors, by\nsetting all \u03b3 to 1.\nComment on runtime. We highlight that, although this implementation of the mesa-layer reduces\nthe memory footprint of the forward and backward pass substantially, the layer still runs forward (and\nbackward) in time. This prevents the computation of all mesa-layer outputs in parallelization during\ntraining, a crucial advantage of softmax as well as linear attention. On the other hand, during test\ntime, the mesa-layer benefits from the same advantages of linear self-attention or RNNs and predicts\nthe next token without the necessity to store and attend to the past. In the next section, we present\none potential avenue to improve the training time by approximating the necessary inversions by a\nNeumann series running in parallel.\nA2.2\nALTERNATIVE DERIVATION THROUGH THE IMPLICIT FUNCTION THEOREM\nWe here present an alternative way of deriving the gradients presented above that leverages the\nimplicit function theorem. The key here is to remark that \u02c6W mesa\nt\nsatisfies that the gradient of the\nleast-square regression loss L is 0. For simplicity, we restrict ourselves to the case in which the output\ndimension of \u02c6W mesa\nt\nis one, that is \u02c6W mesa\nt\n= \u02c6w\u22a4\nt for \u02c6wt some column vector, and remark that we\nhave to repeat the same operation over all rows of \u02c6W mesa\nt\nto obtain the full gradient, as all output\ncoordinates are independent in the least-square regression problem. Therefore, we w defined through\n22\nPreprint\nthe implicit function\ndL\ndw( \u02c6wt) =\nt\nX\nt\u2032=1\nMt\u2032,t( \u02c6w\u22a4\nt kt\u2032 \u2212 vt\u2032)k\u22a4\nt\u2032 + M1,t\n\u03bb\n\u02c6w\u22a4\nt = 0.\n(A29)\nWe can then use the implicit function theorem and compute the derivative of w with respect to any\nquantity \u00b7 through\nd \u02c6wt\nd \u00b7 = \u2212\n\u0012d2Lt\ndw2 (wt)\n\u0013\u22121 d2Lt( \u02c6wt)\nd \u00b7 dw\n(A30)\n= \u2212Rt\nd2Lt( \u02c6wt)\nd \u00b7 dw\n.\n(A31)\nFor example, this yields\nd \u02c6wt\ndvt\u2032 = Mt\u2032,tRtkt\u2032.\n(A32)\nFinally, we can recover the desired gradient by combining the previous equation with the chain rule.\nA2.3\nPARALLEL BACKWARD PASS THROUGH NEUMANN SERIES APPROXIMATION\nNote: We present this section for the sake of completeness \u2013 no experiments presented in this\nmanuscript use this approximation.\nAlthough the previous custom backward gradient computation allows for dramatic memory savings\nduring training, the underlying recursive least squares computation still suffers from linear scaling\nin time, similar to recurrent neural networks, as we cannot parallelize computation across time\ndimension.\nHere, we discuss an alternative forward pass that can be used when one can afford storing all\nintermediate matrices Rh,t in time. This forward pass leverages a K-step truncated Neumann series\nto approximate the inverses in parallel, and is compatible with automatic differentiation tools out of\nthe box. Interestingly, we can do this by simply repeating (with the same weights) a slightly altered\nlinear self-attention layer K times.\nOur goal is now to efficiently compute the terms \u02dcqt := Rtqt = (KtK\u22a4\nt + 1\n\u03bbI)\u22121qt for all time steps\nin parallel. Indeed, once give these vectors, one can leverage Equation A11 and efficient dot-product\nattention (DPA) layers implementations2. Note that we here ignore the forgetting factors, but their\npartial products can easily be integrated in one of the Kt in KtK\u22a4\nt to recover the version with forget\nrates described above.\nGiven an invertible matrix X with operator norm less than 1, the truncated Neumann series approxi-\nmates its inverse by\nX\u22121 \u2248 \u02dcX\u22121\n(K) :=\nK\nX\nk=0\n(I \u2212 X)k.\n(A33)\nWhen multiplying a vector from the right, we see that\n\u02dcx(K) := \u02dcX\u22121\n(K)x =\nK\nX\nk=0\n(I \u2212 X)kx\n(A34)\n=\nK\nX\nk=1\n(I \u2212 X)kx + x\n(A35)\n= (I \u2212 X)\nK\u22121\nX\nk=0\n(I \u2212 X)kx + x\n(A36)\n= (I \u2212 X)\u02dcx(K\u22121) + x\n(A37)\n2See https://flax.readthedocs.io/en/latest/_modules/flax/linen/attention.\nhtml for an implementation of DPA in JAX (Bradbury et al., 2018).\n23\nPreprint\nAn advantage of the truncated Neumann series compared to other approximate inverse techniques\nsuch as Newton-Iteration is that we can compute more series elements without passing intermediate\nmatrices across algorithmic steps \u2013 which in turn makes it memory efficient and straightforward to\nuse in the light of automatic differentiation. We only need to keep the original matrix we wish to\ninvert in memory at all times and store the intermediate vectors \u02dcx(k) for the backward pass.\nWe now look at the quantities we wish to compute, that is \u02dcqt = (KtK\u22a4\nt + 1\n\u03bbI)\u22121qt, and approximate\nit by \u02dcq(K)\nt\n, obtained by multiplying qt to the K-step truncated Neumann series approximating the\ninverse term (KtK\u22a4\nt + 1\n\u03bbI)\u22121. Note that a normalization by the operator norm of the matrix inside\nthe inverse is necessary for the approximation to hold.\nThen, \u02dcq(K)\nt\ncan be computed recursively as\n\u02dcq(k+1)\nt\n=\n\u0012\nI \u2212\n\u0012\nKtK\u22a4\nt + 1\n\u03bbI\n\u0013\u0013\n\u02dcq(k)\nt\n+ qt\n(A38)\n= qt +\n\u0012\n1 \u2212 1\n\u03bb\n\u0013\n\u02dcq(k)\nt\n\u2212 KtK\u22a4\nt \u02dcq(k)\nt\n(A39)\nand thus by denoting \u02dcQ(k)\nt\n:= (\u02dcq(k)\nt\u2032 )t\nt\u2032=1, we have\n\u02dcQ(k+1)\nk+1\n= Qt +\n\u0012\n1 \u2212 1\n\u03bb\n\u0013\n\u02dcQ(k)\nt\n\u2212 KtK\u22a4\nt \u02dcQ(k)\nt\n(A40)\nwhich is the sum of simple terms with a DPA computed between Kt, Kt, \u02dcQ(k)\nt\n.\nAfter obtaining \u02dcQ(K)\nt\nto approximate \u02dcQt, we compute the approximate least-squares solution as de-\nscribed above. Note that other implementations could save us from effectively recomputing (KtK\u22a4\nt )\nat every iteration of Equation A40 by simply pre-computing these terms before running the Neumann\napproximation. We nevertheless observe the former version to be faster when timing for forward\nand backward computation and speculate the reason being the highly optimized implementation of\nDPA as the backbone of the self-attention layer. Note that a simple byproduct of the derivations\nhere is the insight that chaining linear self-attention layers can actually easily implement truncated\nNeumann series computation \u2013 especially if the goal is an inverse multiplied by a known vector. See\nSection A4.2 for a more in-depth analysis.\n24\nPreprint\nA3\nVISUALIZATION OF WEIGHTS AND ATTENTION MAPS OF TRAINED\nTRANSFORMERS\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure A1: Softmax attention maps of the 2-layer softmax-only Transformer trained on the\nPile. We average the attention maps of the first softmax-attention layer over a batch of size 256 and\nobserve stable off diagonals with different offsets and widths indicating clean copying behavior based\non positional encodings in multiple heads.\n0\n10\n20\n0\n10\n20\nWT\nKWQ Head 1\n0\n10\n20\n0\n10\n20\nPWV Head 1\n0\n10\n20\n0\n10\n20\nWT\nKWQ Head 2\n0\n10\n20\n0\n10\n20\nPWV Head 2\n0.2\n0.1\n0.0\n0.1\n0.2\nFigure A2: Mesa-optimization in a trained linear self-attention layer. We inspect the parameters\nof a two-headed, linear self-attention layer trained to predict the future state of a linear dynamical\nsystem. The dominant pattern obtained after learning corresponds to our mesa-gradient descent\nconstruction described in Section 3. The faint additional structure can be further reverse-engineered,\nand results from a modified mesa-objective function, Eq. A43, discovered by base-optimization of\nEq. 10. Please compare to the similar structure of the weight matrix products of our construction.\n25\nPreprint\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure A3: Softmax attention maps of the 2-layer mesa-hybrid trained on the Pile. We average\nthe attention maps of the first layer i.e. a softmax self-attention layer over a batch of size 256 and\nobserve stable off diagonals with different offsets and widths indicating clean copying behavior based\non positional encodings in multiple heads.\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0\n63\ntoken idx\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure A4: Softmax attention maps of the 2-layer linear-hybrid trained on the Pile. We average\nthe attention maps of the first layer i.e. a softmax self-attention layer over a batch of size 256 and\nobserve stable off diagonals with different offsets and widths indicating clean copying behavior based\non positional encodings in multiple heads.\n26\nPreprint\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0.02\n0.01\n0.00\n0.01\n0.02\n0.03\nWT\nKWQ\n0.02\n0.00\n0.02\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0.05\n0.00\n0.05\nWT\nKWQ\n0.05\n0.00\n0.05\n0.10\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0.1\n0.0\n0.1\n0.2\nWT\nKWQ\n0.2\n0.1\n0.0\n0.1\n0.2\nPWV\nFigure A5: Weights of the deep 6-layer linear Transformers trained on constructed tokens et =\n(0, st, st, st\u22121). We observe clear structure in the trained Transformer weight products W \u22a4\nKWQ as\nwell as PWV . Note that this structure seems to be sufficient to approximate (St\u22121S\u22a4\nt\u22121 + 1/\u03bbI)\u22121st,\nsee probing experiment in the main text, Section A2.3 and Section A4. We show here all 4 heads\n(f.l.t.r.) of the first (top 2 rows), the second (next 2 rows), and the last (last 2 rows) linear layer.\n27\nPreprint\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n1\n0\n1\nWT\nKWQ\n0.25\n0.00\n0.25\n0.50\n0.75\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n2\n0\n2\n4\nWT\nKWQ\n0\n1\n2\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nWT\nKWQ\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n0\n10\n20\n30\n0\n10\n20\n30\nPWV\n2\n0\n2\n4\nWT\nKWQ\n0.6\n0.4\n0.2\n0.0\nPWV\nFigure A6: Weights of the deep 6-layer softmax Transformers trained on constructed tokens et =\n(0, st, st, st\u22121). We observe a lot of structure in the trained Transformer weight products W \u22a4\nKWQ as\nwell as PWV . Note that this structure seems to be sufficient to approximate (St\u22121S\u22a4\nt\u22121 + 1/\u03bbI)\u22121st,\nsee probing experiment in the main text, Section A2.3 and Section A4. We show here all 4 heads\n(f.l.t.r.) of the first (top 2 rows), the second (next 2 rows), and the last (last 2 rows) softmax layer.\n28\nPreprint\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0.100\n0.075\n0.050\n0.025\n0.000\n0.025\n0.050\n0.075\n0.100\nWT\nKWQ\n0.100\n0.075\n0.050\n0.025\n0.000\n0.025\n0.050\n0.075\n0.100\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0.10\n0.05\n0.00\n0.05\n0.10\nWT\nKWQ\n0.10\n0.05\n0.00\n0.05\n0.10\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0.4\n0.2\n0.0\n0.2\n0.4\nWT\nKWQ\n0.4\n0.2\n0.0\n0.2\n0.4\nPWV\nFigure A7: Weights of the deep 1+6-layer linear-hybrid Transformers trained on unconstructed\ntokens et = st. We observe some diagonal structure in the trained Transformer weight products\nW \u22a4\nKWQ as well as PWV . Note that this structure seems to be sufficient to approximate (St\u22121S\u22a4\nt\u22121 +\n1/\u03bbI)\u22121st, see probing experiment in the main text, Section A2.3 and Section A4. We show here all\n4 heads (f.l.t.r.) of the first (top 2 rows), the second (middle 2 rows) and last (last 2 rows) linear layer\nafter the first softmax layer.\n29\nPreprint\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n4\n2\n0\n2\n4\nWT\nKWQ\n0.4\n0.2\n0.0\n0.2\n0.4\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nWT\nKWQ\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nWT\nKWQ\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0\n10\n20\n30\n0\n5\n10\n15\n20\n25\n30\n35\nPWV\n0.2\n0.1\n0.0\n0.1\n0.2\nWT\nKWQ\n0.15\n0.10\n0.05\n0.00\n0.05\n0.10\n0.15\nPWV\nFigure A8: Weight products of the deep 7-layer softmax Transformers trained on unconstructed\ntokens et = st. We observe some diagonal structure in the trained Transformer weight products\nW \u22a4\nKWQ as well as PWV . Note that this structure seems to be sufficient to approximate layer-wise\nthe final prediction st+1 as well as (St\u22121S\u22a4\nt\u22121 + 1/\u03bbI)\u22121st, see probing experiment in the main text,\nSection A2.3 and Section A4. We show here all 4 heads (f.l.t.r.) of the second (top 2 rows) the third\n(middle 2 rows) and the last (last 2 rows) layers after the first (potential) copying-softmax-layer.\n30\nPreprint\nA4\nMECHANISTIC INTERPRETABILITY OF TRANSFORMERS TRAINED ON\nLINEAR DYNAIMCS\nA4.1\nSINGLE-LAYER MESA-GRADIENT DESCENT\nWe state here for completion the training objective of Transformers trained on sequences with\ninitial state s1 \u223c N(0, I) with subsequent states t = 1, . . . , T generated according to the rule\nst = W \u2217st\u22121 + \u03f5t, where \u03f5t \u223c N(0, \u03c32\ns I) introduces uncorrelated Gaussian noise. We take W \u2217\nto be a random orthogonal matrix. The Transformer models t\u03b8 are trained by stochastic online\nminimization of the autoregressive loss L(\u03b8), cf. Eq. 10.\nAfter training, we obtain structured matrix products W \u22a4\nKWQ, PWV per layer which we visualize in\nFigure A2 for a single linear self-attention Transformer and in Figure A5 for the multi-layer case.\nWhen inspecting the trained weight matrix products, one observes stable values across block-diagonals\nof the input size across all layers.\nWe start by analyzing the simpler single layer Transformer computation and reduce it to\net \u2190 et + LSA(et; (et\u2032)t\nt\u2032=1)\n=\nH\nX\nh=1\n\u03d5t\n1(dP V\nh\n, dKT Q\nh\n)st + \u03d5t\n2(dP V\nh\n, dKT Q\nh\n)st\u22121,\n(A41)\nwhere LSA(et; (et\u2032)t\nt\u2032=1) denotes the linear self-attention operation with context (et\u2032)t\nt\u2032=1 and query\net, and with d\u00b7\n\u00b7 inputs to the functions \u03d5 defined as follows:\n\u03d5t\nM(dP V , dKT Q) =\nt\nX\nt\u2032=1\ndKT QM1 \u00b7\n\u0000dP V1st\u2032s\u22a4\nt\u2032 + dP V2st\u2032\u22121s\u22a4\nt\u2032\n\u0001\n+ dKT QM2 \u00b7\n\u0000dP V1st\u2032s\u22a4\nt\u2032\u22121 + dP V2st\u2032\u22121s\u22a4\nt\u2032\u22121\n\u0001\n.\nHere dP V , dKQ corresponds to the 4 (the lower right square of size 2d \u00d7 2d of KT Q) resp. 2\n(the upper right rectangle of size d \u00d7 2d of PV ) non-zero off-diagonal values that we observe\nin the trained weight products per head i.e. W \u22a4\nk Wq =\n\uf8eb\n\uf8ed\n\u00b7\n\u00b7\n\u00b7\n\u00b7\ndKT Q1,1Is\ndKT Q1,2Is\n\u00b7\ndKT Q2,1Is\ndKT Q2,2Is\n\uf8f6\n\uf8f8 as well as\nPWv =\n\uf8eb\n\uf8ed\n\u00b7\ndP V1Is\ndP V2Is\n\u00b7\n\u00b7\n\u00b7\n\u00b7\n\u00b7\n\u00b7\n\uf8f6\n\uf8f8.\nWe extract the values from the trained models by computing the mean of the block diagonal matrices.\nNote that we allow for all combinations between temporally accumulated block matrices and the\ncurrent token inputs et = [0, 0, st, st\u22121]\u22a4 with specific strengths controlled through the parameters.\nIn all of our experiments, we observe a performance increase when changing from single-head to\ntwo-head attention layers (more than two heads do not alter performance). This can be explained by\nthe improved flexibility of scaling the different terms individually as can be seen by comparing the\nformulas for one and two heads.\nFollowing equation A41, the prediction of a two-head single layer Transformer with our construction\nof et = [0, st, st\u22121] is given by\n\u02c6st+1 = 0 +\n2\nX\nh=1\n\u03d5t\n1(dP V\nh\n, dKT Q\nh\n)st + \u03d5t\n2(dP V\nh\n, dKT Q\nh\n)st\u22121\n=\n\u0010\n\u03d5t\n1(dP V\n1\n, dKT Q\n1\n) + \u03d5t\n1(dP V\n2\n, dKT Q\n2\n)\n\u0011\n|\n{z\n}\nA\nst +\n\u0010\n\u03d5t\n2(dP V\n1\n, dKT Q\n1\n) + \u03d5t\n2(dP V\n2\n, dKT Q\n2\n)\n\u0011\n|\n{z\n}\nB\nst\u22121\n31\nPreprint\nwith\nA =\nt\nX\nt\u2032=1\ndKT Q11\n1\n\u00b7\n\u0010\ndP V1\n1\nst\u2032s\u22a4\nt\u2032 + dP V2\n1\nst\u2032\u22121s\u22a4\nt\u2032\n\u0011\n+ dKT Q12\n1\n\u00b7\n\u0010\ndP V1\n1\nst\u2032s\u22a4\nt\u2032\u22121 + dP V2\n1\nst\u2032\u22121s\u22a4\nt\u2032\u22121\n\u0011\n+ dKT Q11\n2\n\u00b7\n\u0010\ndP V1\n2\nst\u2032s\u22a4\nt\u2032 + dP V2\n2\nst\u2032\u22121s\u22a4\nt\u2032\n\u0011\n+ dKT Q12\n2\n\u00b7\n\u0010\ndP V1\n2\nst\u2032s\u22a4\nt\u2032\u22121 + dP V2\n2\nst\u2032\u22121s\u22a4\nt\u2032\u22121\n\u0011\n=\nt\nX\nt\u2032=1\n\u03bbA,1st\u2032s\u22a4\nt\u2032 + \u03bbA,2st\u2032\u22121s\u22a4\nt\u2032 + \u03bbA,3st\u2032s\u22a4\nt\u2032\u22121 + \u03bbA,4st\u2032\u22121s\u22a4\nt\u2032\u22121.\n(A42)\nafter summarizing by combining factors in front of the same outerproducts. (B) is computed\naccordingly. Here, every outer product combination of st\u2032 and st\u2032\u22121, is weighted by a product of two\nfactors, before summarizing, that are co-dependent within one head. Therefore we indeed need a\nminimum of two heads to obtain independent \u03bbi. However, adding further heads does not increase\nperformance as no further expressivity is gained. See for a visualization of the corresponding weight\nmatrix products and factors that we extract from the block diagonals Figure A2. The extracted mean\nvalues of our trained Transformer block diagonals are reported in Table A1.\nTable A1: Understanding the algorithm parametrization of Transformers trained on linear dynamics.\nTo test the significance of the \u03bb values derived in Eq. A42, we set almost all values i.e. \u03bb\u00b7,1 = \u03bb\u00b7,2 =\n\u03bbB = 0 - we call this loss Lreduced. This coincides as shown to gradient descent on a meta-learned\ninitial prediction and learning rate (see Eq. A43). To show the influence of the \u03bb values corresponding\nto GD, we compute the algorithms performance when only setting \u03bbA,3/4 = 0 and observe a drastic\nloss increase, we denote this loss as Lablation and also report the loss of one step of GD as LGD.\nSeed\n\u03bb\n\u03bb\u00b7,1\n\u03bb\u00b7,2\n\u03bb\u00b7,3\n\u03bb\u00b7,4\nL\nLreduced\nLablation\nLGD\n1\n\u03bbA\n0.000620\n0.000254\n-0.033142\n0.000149\n0.8923\n0.9007\n1.7275\n0.9151\n1\n\u03bbB\n0.003644\n0.000034\n-0.000978\n-0.000129\n2\n\u03bbA\n-0.001648\n0.000089\n-0.033428\n0.001809\n0.8926\n0.9039\n1.7349\n0.9088\n2\n\u03bbB\n0.003974\n-0.000215\n-0.000423\n0.000023\n3\n\u03bbA\n-0.003381\n0.000776\n-0.033751\n0.006270\n0.8941\n0.9408\n1.7643\n0.9088\n3\n\u03bbB\n0.002756\n-0.000444\n-0.002846\n0.000625\n4\n\u03bbA\n0.001905\n0.000270\n-0.033858\n-0.002946\n0.8928\n0.9089\n1.7417\n0.9019\n4\n\u03bbB\n0.003851\n0.000285\n0.000873\n0.000225\n5\n\u03bbA\n-0.001676\n0.000324\n-0.033557\n0.001557\n0.8932\n0.9050\n1.7222\n0.9136\n5\n\u03bbB\n0.002450\n-0.000028\n-0.000027\n0.000441\nWe now aim to interpret this parametrized algorithm and motivate it by gradient descent on a particular\nregression loss. Since the parametrizations remain constant across a sequence, we speculate that\nthe Transformer has two principles by which it aims to predict the next token: gradient descent, and\npast-token averaging. The latter becomes especially useful for quickly contracting dynamics after\nconvergence since simply copying over the last token can be one optimal and simple-to-implement\nsolution, see Section A5, even when aiming to obtain low loss on the entire sequence. We thus\nhypothesize that past-token averaging is a simple way to overcome the sub-optimality of taking only\none step of gradient descent.\nConsider again the squared error loss from Eq. 3, which we hypothesize is internally optimized inside\na single layer of self-attention\nLself-attention\nt\n(W) = 1\n2\nt\u22121\nX\nt\u2032=1\n\u2225st\u2032 \u2212 Wst\u2032\u22121\u22252,\nWe now compute and evaluate the gradient of the loss evaluated at the initial W = \u02dc\u03bb1I leading to an\ninitially scaled prediction of the current s i.e. \u02c6st+1 = \u02dc\u03bb1st,\n\u2207W Lself-attention\nt\n(\u02dc\u03bb1I) = \u2212\nt\u22121\nX\nt\u2032=1\n(st\u2032 \u2212 \u02dc\u03bb1st\u2032\u22121)s\u22a4\nt\u2032\u22121.\n32\nPreprint\nThe prediction after a gradient step can be computed by\n\u02c6st+1 = (\u02dc\u03bb1I \u2212 \u03b71\u2207W Lself-attention\nt\n)st\n= \u02dc\u03bb1st \u2212 \u03b71\nt\u22121\nX\nt\u2032=1\n(st\u2032 \u2212 \u02dc\u03bb1st\u2032\u22121)s\u22a4\nt\u2032\u22121st\n= \u02dc\u03bb1st + (\nt\u22121\nX\nt\u2032=1\n\u03bbC,3st\u2032s\u22a4\nt\u2032\u22121 + \u03bbC,4st\u2032\u22121st\u2032\u22121)\u22a4st.\n(A43)\nNote that this is a stripped down version of the derivation above, see equation A42. We now simply\ncompare the final MSE loss when setting \u03bbA,1 = \u03bbA,2 = \u03bbB = 0 and observe minimal loss\ndegradation, see Table A1. Given this robustness, we are confident that almost all of the behavior and\nperformance of the trained Transformer in this setting can be explained by simply descending the\nmesa-objective of Eq. 3 by gradient descent. We also see that empirically using an initial prediction\nand therefore a non-zero implicit initial weight has some influence on the final performance. Note that\nin certain settings the transformer can obtain a significantly improved performance when including\nthe other terms in A42, as well, as can be seen in A5. Furthermore, note that to realize full expressivity\nin \u03bbA,3 = \u03bbA,4 it requires 6 parameters coming from both heads.\nA4.2\nMULTI-LAYER ACCELERATED MESA-GRADIENT DESCENT\nWe now return to the mesa-optimization algorithms presented in Section 3 \u2013 stacked mesa-gradient\ndescent layers, and preconditioned mesa-gradient descent \u2013 and present them in full detail, in the\ncontext of the linear dynamics prediction problems studied in the main text.\nReview: d-layers of self-attention can implement d steps of gradient descent in the few-shot\nsetting. We start by repeating the multi-layer construction provided in von Oswald et al. (2023)\nwhich allows a Transformer, without causal masking, and applied to the few-shot regression setting.\nThis construction performs a gradient descent step per layer while simultaneously constructing a\nprediction on some final input. Compared to Section 2, we slightly change notation to easily bridge\nthe gap to the autoregressive case in the next paragraph. Recall the squared error regression loss\ngiven T \u2212 1 data pairs (st\u2032, st\u2032+1)\nL(W (0)) = 1\n2\nT \u22121\nX\nt\u2032=1\n(W (0)st\u2032 \u2212 st\u2032+1)2,\nwith the gradient given by\n\u2207W L(W (0)) =\nT \u22121\nX\nt\u2032=1\n(W (0)st\u2032 \u2212 st\u2032+1)s\u22a4\nt\u2032\ninducing a change in the weights \u2206W (0) = \u2212\u03b7\u2207W L(W (0)). We now evaluate the the loss again at\nW (1) = W (0) + \u2206W (0):\nL(W (1)) = L(W (0) + \u2206W (1)) = 1\n2\nT \u22121\nX\nt\u2032=1\n((W (0) + \u2206W (1))st\u2032 \u2212 st\u2032+1)2\n= 1\n2\nT \u22121\nX\nt\u2032=1\n(W (0)st\u2032 \u2212 (st\u2032+1 \u2212 \u2206W (1)))2\n= 1\n2\nT \u22121\nX\nt\u2032=1\n(W (0)st\u2032 \u2212 \u02dcs(1)\nt\u2032+1)2\nwith \u02dcs(1)\nt\u2032+1 = st\u2032+1 \u2212 \u2206W (1). Note that when repeating this algorithm, we descend the regression\nloss after d steps of gradient descent by transforming the targets instead of updating the weights.\nNote that we are here not learning weights which we could use for test predictions. Nevertheless,\nwhen using the induced target transformation on some novel data point sT +1 while simultaneously\ntransforming the targets of our dataset, we see that after a \u22121 correction i.e. \u02c6sT +1 = W (0)sT +1 +\n33\nPreprint\n\u22121 Pd\u22121\nl=0 \u2212\u2206W (d))sT +1) = W (d)sT +1, we obtain an equivalent prediction to the one of standard\ngradient descent. Note that the linear self-attention weight matrices provided in the main text for\nthe single-step case directly implement this multi-step case when we restrict the attention to the first\nT \u2212 1 datapoints.\nTo implement this d-step algorithm in a Transformer, if all bias terms are zero, W \u22a4\nk Wq =\n\u0012\n0\n0\n0\nIs\n\u0013\n,\nand PWv =\n\u0012\n\u2212\u03b7Is\n\u03b7W0\n0\n0\n\u0013\n, and the token at initialization is e(0)\nt\n= (st+1, st) for the training data\nand e(0)\nT +1 = (\u2212W0sT +1, sT +1) for the test point, after d such layers, the last token in which we com-\npute the test data prediction is transformed into e(d)\nT +1 = (\u2212W0sT +1 +Pd\u22121\nl=0 \u2212\u2206W (d))sT +1, sT +1).\nThe y-component of this token contains again the (negative) of the prediction obtained by a linear\nmodel that underwent d steps of gradient descent. Therefore, configured as such, our d self-attention\nlayers take d steps of mesa-gradient descent toward solving a least-squares problem. Note that in this\nconstruction the lower half of the tokens e(l)\nt\n= (\u00b7, st) \u2200l keep unchanged throughout the Transformer\nforward pass.\nd-layers of causally-masked self-attention can implement d steps of online gradient descent.\nTo transfer the previous multi-layer construction to the autoregressive setting, we make two obser-\nvations: (i) we now wish to make a prediction at every time step which will require more token\nspace, and (ii) introducing causal masking will affect the computations carried out by the layer of von\nOswald et al. (2023) reviewed above, leading to T different models learned in parallel by an unusual\nvariant of gradient descent.\nTo see (i), we note that at every point in time the Transformer needs to keep in memory throughout\nits forward pass not only the inputs but also the targets, (st\u2032, st\u2032+1) for t\u2032 \u2208 {1, . . . , T}. This is the\ncase as we need both to transform some target st\u2032 through gradient descent dynamics to implicitly\nkeep updating the learned linear model, but also to use that same target st\u2032 now as input to the\nimplicitly learned model to construct a final output. This observation motivates the token construction\net = (\u2212W0st, st, st, st\u22121), where the last two entries are kept unchanged throughout the Transformer\nforward pass. This token construction suggests the following weight configuration for a 2-headed\nlinear self-attention layer:\nW \u22a4\nk,1Wq,1 =\n\uf8eb\n\uf8ec\n\uf8ed\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nIs\n0\n\uf8f6\n\uf8f7\n\uf8f8 ,\nP1Wv,1 =\n\uf8eb\n\uf8ec\n\uf8ed\n0\n\u2212\u03b7Iy\n0\n\u03b7W0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\uf8f6\n\uf8f7\n\uf8f8 ,\nW \u22a4\nk,2Wq,2 =\n\uf8eb\n\uf8ec\n\uf8ed\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nIs\n\uf8f6\n\uf8f7\n\uf8f8 ,\nP2Wv,2 =\n\uf8eb\n\uf8ec\n\uf8ed\n0\n0\n0\n0\n0\n\u2212\u03b7Iy\n0\n\u03b7W0\n0\n0\n0\n0\n0\n0\n0\n0\n\uf8f6\n\uf8f7\n\uf8f8 .\nThe dynamics induced by these two heads are equivalent but are evaluated in head 1 at the current st\u2032\nwhich we use to make the next token prediction and at 2 at the training data input st\u2032\u22121, to change\nthe moving targets based on our previously discussed gradient-based transformation of the targets.\nGiven these weights in layer l, we obtain the following change in the tokens\net+1 = et + \u2206e(l)\nt\n= (\u02c6s(l)\nt+1, \u02dcs(l)\nt , st, st\u22121) + (\u2212\u2206W (l)\nt st, \u2212\u2206W (l)\nt st\u22121, 0, 0)\n= (\u02c6s(l)\nt+1, \u02dcs(l)\nt , st, st\u22121) + (\u2206\u02c6s(l)\nt+1, \u2206\u02dcs(l)\nt , 0, 0)\n34\nPreprint\nwith \u2206W (l)\nt\n= \u2212\u03b7 Pt\nt\u2032=1(W0st\u2032\u22121 \u2212 \u02dcs(l)\nt\u22121)s\u22a4\nt\u2032\u22121 and \u02c6s(0)\nt+1 = \u2212W0st, \u02dcs(0)\nt\n= st. Note that now\nin the causally-masked setting, the sum only runs to element t for the token et and that therefore\nthe transformed targets each follow their own dynamics instead of the gradient summed across the\nsequence, as in the few-shot regression case. The above construction is equivalent to the few-shot\nsetting, for which we would want to make a prediction based on d steps of gradient descent for the\ntest as well as the training data, i.e., at all points in time.\nFurthermore, we can motivate this token update as the gradient of the time-dependent loss Lt(W0) =\n1\n2\nPt\nt\u2032=1(W0st\u2032\u22121 \u2212 \u02dcs(l)\nt\u22121)2 for which again the targets at time step t change throughout the layers\nbased on their specific target transformation mechanism. This online gradient descent algorithm was\nproved to be sub-optimal w.r.t. conventional (full-batch) gradient descent by Ding et al. (2023). They\nshow that, in the limit of infinitely many layers, this construction implements online gradient descent,\nwhich does not coincide with the optimal (recursive) least-squares solution. Additionally, stochastic\ngradient descent with non-vanishing learning rates does not converge so the effective weight does not\nconverge to the optimal solution when given infinitely many samples.\nIn the next section we will show how, at least in theory, multi-layer self-attention can implement a\ndifferent algorithm which can lead to the desired result, even in the causally-masked setting. We\nend by noting that the aforementioned weight construction, despite being potentially sub-optimal,\nmotivates our token construction et = (0, st, st, st\u22121) which we use throughout all experiments\nwhen training deep Transformers, i.e., whenever the model has more than one self-attention layer.\nNote that \u2212W0st = 0 since we assume an W0 = 0. Finally, we stress that until now it is not clear\nhow to incorporate common L2 regularization into the (online) gradient descent dynamics. This\nhowever is now in the autoregressive case of particular importance: In the beginning of the sequence\nthe causally masked Transformer is forced to solve an under-constrained learning problem, dependent\non the input data dimension and data generation. The following paragraph will again provide a simple\nsolution to this problem.\nd-layers of causally-masked self-attention can approximate optimal preconditioned gradient\ndescent. Our results are influenced by the GD++ algorithm presented by von Oswald et al. (2023),\nwhich can lead to accelerated optimization by applying a whitening transform to input data. We restate\nthe goal of the autoregressive Transformer, namely, to solve the underlying least-squares problem\nfor all time steps simultaneously. This amounts to computing StS\u22a4\nt\u22121(St\u22121S\u22a4\nt\u22121 + 1\n\u03bbI)\u22121st\n\u2200t, a\n(recursive) least squares solution, where time-shifted (by one) sequence elements play the role of\ninputs and desired outputs in a dataset, with inputs St\u22121, targets St, and test input st.\nWith the limited expressivity of one layer, we have already established that Transformers can, and\ndo, in various settings, implement a single gradient step on the corresponding regression problem\nPt\u22121\nt\u2032=1(st\u2032+1\u2212Wst\u2032)2 both in theory and in practice. We now diverge from the previous section which\ngeneralized a single mesa-gradient descent step to the multi-layer case. Instead, we argue here that the\nTransformer could solve the problem differently. Our key observation is that given a preconditioning\nmatrix Ht = (St\u22121S\u22a4\nt\u22121 + 1\n\u03bbI)\u22121 which changes the loss as Pt\u22121\nt\u2032=1 \u2225st\u2032+1 \u2212 WHtst\u2032\u22252, a gradient\ndescent dynamics would converge in a single step to the regularized least-squares solution. This way,\nwe do not apply several gradient steps, thereby circumventing the potential problems arising from\ncausally-masked gradient descent dynamics on the targets discussed in the previous section.\nBased on these insights, we provide a theoretical construction that shows how Transformers can\napproximate (St\u22121S\u22a4\nt\u22121 + 1\n\u03bbI)\u22121qt layer by layer in their forward pass, leading to improved single-\nstep gradient descent performance. To do so, we build on the derivations of Section A2.3, where\nwe showed how to implement an approximation of \u02dcst := (St\u22121S\u22a4\nt\u22121 + 1\n\u03bbI)\u22121st efficiently, and for\nall time steps t in parallel. We achieved this result with the help of the dot-product-attention (DPA)\noperation at the heart of linear and softmax self-attention layers, and by resorting to the truncated\nNeumann series.\nFirst, we recall from Section A2.3 that we can approximate \u02dcst by \u02dcsK\nt , obtained when multiplying\nst to the K-step truncated Neumann series approximating the inverse term (St\u22121S\u22a4\nt\u22121 + 1\n\u03bbI)\u22121,\nmodulo normalization of the matrix, see A2.3. Then, we observe that the \u02dcsK\nt satisfy the following\n35\nPreprint\nrecursive relationship:\n\u02dcsk+1\nt\n=\n\u0012\nI \u2212 (St\u22121S\u22a4\nt\u22121 + 1\n\u03bbI)\n\u0013\n\u02dcsk\nt + st\n= st + (1 \u2212 1\n\u03bb)\u02dcsk\nt \u2212 St\u22121S\u22a4\nt\u22121\u02dcsk\nt .\n(A44)\nWe now see how, if (\u02dcsk\nt , st, st\u22121) is present in the activations at layer k at time point t, then \u02dcsk+1\nt\ncan be computed for the next layer. In particular, the last term in eq A44, can be obtained by\none head of linear self-attention from the input (\u02dcsk\nt , st, st\u22121) when W \u22a4\nk Wq =\n 0\n0\n0\n0\n0\n0\nIx\n0\n0\n!\n, and\nPWv =\n 0\n0\n\u2212Ix\n0\n0\n0\n0\n0\n0\n!\n. The other terms are simple scaled additions to \u02dcsk\nt of \u02dcsk\nt itself and \u02dcst\nfor which many constructions exist. The latter information is also available at all times if not\noverwritten otherwise. Note that this weight construction strictly speaking only requires 3 channels\ni.e. et = (st, st, st\u22121) in which we update the first by the just provided Neumann series computation.\nNevertheless, in practice we still use et = (0, st, st, st\u22121), i.e. tokens with additional memory. We\nobserve in practice that both constructions e.g. et = (0, 0, st, st\u22121) or et = (0, st, st\u22121) reach similar\nperformance but observe more training difficulties and instabilities for the more compact one. We\nalso stress that the derivation presented here is one out of possibly many ways of how Transformers\ncould implement and approximate the desired inverses (St\u22121S\u22a4\nt\u22121 + 1/\u03bbI)\u22121st in parallel. This is\nthe main reason we resorted to the probing analyses presented in the main text.\nA5\nANALYSING CONTRACTING LINEAR DYNAMICS\nWe show here the preliminary result when diverging from orthogonal teachers W to construct the\nsequence presented to the Transformer and restrict the eigenvalues of W \u223c N(0, I) in a band of\n[0.9, 0.3]. We notice that with these W approximately 2% of the sequences lead to very large values.\nTo ease trainability, we therefore clip all the values of those sequences to values between [\u22122, 2].\nWhen training a single layer of linear self-attention, see Figure A9, we observe that the trained layer\noutperforms a naive step of GD dramatically - while the mesa-layer still outperforms both. These\nfindings differ therefore from results obtained when training on an orthogonal teacher. We again find\nclean weight structure but qualitatively different weights found by optimization compared to weights\ntrained on sequences which are generated by an orthogonal teacher, see Figure A2. We speculate\nat this point that the meta-learned initialization W of the model provides a much more favorable\ninitial guess in this setting compared to the orthogonal teacher. Therefore, only adjusting, by the\nresidual connection, the current input element of the sequence leads to a large performance boost\nwhen compared to naive GD, for which we only tune the learning rate and do not meta-learn an initial\nmodel that we update. Nevertheless, we still find gradient descent implemented in the weights even\nin this setting. We stress that we believe the emergence of gradient-descent-like algorithms will be\ndependent on the data statistics as well as other experimental design choices (Ravent\u00f3s et al., 2023).\nWe leave this important investigation for future work. We use the same training hyperparameters as\nwhen training on orthogonal teachers W.\nA6\nEXPERIMENTAL DETAILS\nA6.1\nTRAINING TRANSFORMERS ON LINEAR DYNAMICAL SYSTEMS\nWe provide here details about the training details of the Transformer models when training on the\nlinear dynamics setting. As described in the main text we focus on fully-observed linear dynamical\nsystems and use a simple generative model. To create a sequence s1:T we first draw a random ground-\ntruth Ds \u00d7Ds weight matrix W \u2217 as well as a random initial state s1 \u223c N(0, I); the subsequent states\nfor t = 2, . . . , T are then generated according to the rule st+1 = W \u2217st + \u03f5t, where \u03f5t \u223c N(0, \u03c32\ns I)\nintroduces uncorrelated Gaussian noise. We take W \u2217 to be a random orthogonal matrix. Now, as\n36\nPreprint\n0\n20\n40\nSequence length t\n0.0\n0.5\n1.0\n1.5\n2.0\nMSE\nA\nGDexact-1\nMesa\nlinear-SA\n10\n15\n20\n25\n10\n15\n20\n25\nB\nWT\nKWQ Head 1\n0\n5\n10\n15\n20\n25\n(PWV)T Head 1\n10\n15\n20\n25\n10\n15\n20\n25\nWT\nKWQ Head 2\n0\n5\n10\n15\n20\n25\n(PWV)T Head 2\n0.2\n0.1\n0.0\n0.1\n0.2\nFigure A9: Training a single linear self-attention layer on contracting linear dynamics The\ntrained single layer strongly outperforms (A) a naive step GD, diverging from results obtain when\ntraining on an orthogonal teacher. We find clear structure in the weights of the trained model (B), as\nfound when training on sequences generated by orthogonal W, see Figure 2. However, here we see an\nequal emphasis on all diagonals. Previously, we established a direct link between the weights found\nin 2 and gradient descent. The predictions of a single layer with these weights are still explained by\nA42, which includes a gradient-descent term, but here we find similar weight for other non-GD terms\nin the formula as well, indicating their usefulness in settings different to our original linear sequence\nmodels.\nalready stated we train all Transformer models on the following autoregressive linear regression loss\nL(\u03b8) = Ex\n\"\n1\n2\nT \u22121\nX\nt=1\n\u2225st+1 \u2212 ft(s1:t, \u03b8)\u22252\n#\n.\n(A45)\nIn all of our experiments, we employ causal masking during self-attention, implemented in the same\nway as in the majority of auto-regressive language modeling experiments. Specifically, during the\nself-attention operation we zero out the elements corresponding to the upper triangular matrix of\nthe attention map, except for the diagonal itself. We do this both for the linear attention layer and\nfor the mesa-layer. In practice, for softmax self-attention the incoming logits to the softmax are set\nto \u22121e30. We did not use LayerNorm (Ba et al., 2016) in our models, except for A13, but ran into\nstability issues especially when training hybrid models with linear layers. To mitigate those, we\nsimply clipped the activations of the forward pass to values between [\u22121, 1] which stabilized training\nsignificantly. Hyperparameters and other experimental details can be found in table A2.\nA6.1.1\nSINGLE-LAYER LINEAR SELF-ATTENTION TRANSFORMER\nWe analyze single-layer, two-head, key-size-20 Transformers, trained on constructed tokens, by\ncomparing their performance with other models and providing an interpolation in parameter space\nbetween trained Transformers and the provided construction, which can be described using only a\nfew hyperparameters, see Table A41. While we read out the predictions from the first Ds entries of\nthe outputs (which initially contain a zero-vector), we simulate a residual connection by manually\nadding st to the output of the single layer. Since we do not use an embedding layer in this setting,\nduring training we learn another parameter \u03b1 as a factor for the added st, such that the prediction\nreads \u02c6st+1 = \u03b1 \u00b7 st + LSA(et; (et\u2032)t\nt\u2032=1). For the performance analysis, these models are compared\nto \u2018exact\u2019 gradient descent, a single gradient update step on the auto-regressive loss, and a single\nmesa-layer. The optimal learning rate for this gradient descent step is line-searched.\nInterpolation details: We first train a Transformer, then extract scalar parameters, e.g. dKQ11\n(see A41), from the Ds \u00d7 Ds sub-matrices by taking the mean of the sub-diagonals of the matrix\nproducts W \u22a4\nk Wq, PWv. We proceed by using these to both build a construction of sparse weight\nmatrices, each consisting only of identity-sub-matrices (scaled by the resp. parameters), and, for the\nsingle-layer case, also directly compute a loss for the hard-coded implementation of Table A41 with\nthe respective parameters. Then, during a second training-run of a Transformer for the same initial\nconditions, we simultaneously compute the test loss for an interpolation, where we average equally\n37\nPreprint\nTable A2: Hyperparameters for all settings and model variants when training on simple linear\ndynamics.\nHyperparameter\nValue\nContext size\nWe used length 50, except for the ICL experiments, where we used length 224.\nOptimizer\nAdam (Kingma & Ba, 2015) with \u03f5 = 1e\u22128, \u03b21 = 0.9, \u03b22 = 0.999\nWeight decay\n0.1 for constructed tokens, 0.05 otherwise\nBatchsize\n2048 for constructed tokens, 256 otherwise.\nGradient clipping\n10 for constructed tokens, 1 otherwise\nActivation clipping\nClip [\u22125, 5] for all models trained on constructed tokens, clip [\u22121, 1] for\nhybrid models, no clipping otherwise.\nPositional encodings\nWe add positional encodings of dimension 40 for models trained on\nunconstructed tokens, otherwise no positional encodings.\nDropout\nWe do not use Dropout for any model.\nArchitecture 1-L. Constr.\nWe use a 1-layer, 2-head, key-size 20, dim-30-tokens, no input- or output-\nembedding architecture for single-layer models trained on constructed tokens.\nArchitecture 6-L. Constr.\nWe use a 6-layer, 4-head, key-size 20, dim-40-token, no input- or output-\nembedding architecture for the multi-layer models (softmax and linear)\ntrained on constructed tokens for the probing analysis and used\nkey-size 40 for the interpolation.\nArchitecture 7-L. No Constr.\nWe use a 7-layer, 4-head, key-size 20, dim-10-tokens, dim-40-\nembedding- architecture with input- and output-embedding layers for hybrid-\nand softmax-only-models. Hybrid models contain a softmax and\nafterwards linear self-attention-layers. The MLP-softmax model contains\nMLPs with one layer of hidden activations of size 160\nand 2 layer- normalization layers (Ba et al., 2016), one directly after\nthe attention layer and one directly after the MLP.\nArchitecture Hybrid-Mesa\nWe use 2-layer, 4-head, key-size 20, dim-10-tokens, dim-40-\nembedding-architecture with inputs- and output embedding layers. First a\nsoftmax-self-attention layer, then a single Mesa-layer.\nWeight init\nW \u223c N(0, \u03c32) with \u03c3 = 0.0002 for models trained on constructed tokens\nand \u03c3 = 0.05 for all other models. We always fixed the bias parameters to zero.\nLearning rate (scheduler?)\nFor models trained on non-constructed tokens, we used linear warm-up\nstarting from 0 to 7e\u22124 in 500 steps, Cosine annealing to 1e \u2212 5 for the next\n20000 steps. We note here that we only train softmax-only and softmax-mlp\nmodels for this amount of steps. For models trained on constructed tokens,\nwe never trained for more than 5000 steps. For models trained on constructed\ntokens, we used a fixed learning rate of 7e\u22124 and\n9e\u22125 for the interpolations.\nMesa regularization \u03bb\nWe initialize the learnable regularization parameter \u03bb for every mesa-head to 1.\nnot between the single weight matrices, but between the correct weight-matrix-products per head\nto obtain a new, interpolated model. The reason for this procedure is the non-uniqueness of weight\nmatrices to obtain the found matrix products. We repeat this procedure for 5 different seeds, train a\nnewly initialized Transformer each time and plot the obtained mean and standard deviation values for\nthe test loss during training.\nA6.1.2\nMULTI-LAYER SELF-ATTENTION TRANSFORMER\nFor the multi-layer experiments, we use different settings: For the experiments with constructed\ntokens, we use a 6-layer, no input- or output-embedding layer architecture, while for the other\nexperiments, we use a 1+6-layer architecture with input- and output-embedding layers, where the first\nlayer is always a softmax self-attention layer, while the other 6 Transformer layers are either 6 linear,\n6 softmax self-attention layers, or 1 mesa layer (1+1-layer architecture). We found that forward-pass\nactivation clipping after each layer (except for embedding layers) greatly stabilized training.\nInterpolation details: The interpolation of multi-layer transformers when training on the token\nconstruction, we follow the procedure described in the previous subsection, per layer, but extend it\nto 4-head key-size 40 self-attention layers: We read off the parameters as the mean of the diagonals\nof the respective Ds \u00d7 Ds sub-matrices of the resulting matrix weight products W \u22a4\nk Wq, PWv per\n38\nPreprint\n1\n2000\nTraining steps\n0.0\n0.5\n1.0\n1.5\nTest Loss\nA\nRevAlg-1\nInterpolation\nsoftmax-SA\nMesa\nGDexact-1\n0\n20\n40\nSequence length t\n0\n1\n2\n3\nMSE\nB\nsoftmax-SA\nMesa\nGDexact-1\n10\n15\n20\n25\n10\n15\n20\n25\nC\nWT\nKWQ Head 1\n0\n5\n10\n15\n20\n25\n(PWV)T Head 1\n10\n15\n20\n25\n10\n15\n20\n25\nWT\nKWQ Head 2\n0\n5\n10\n15\n20\n25\n(PWV)T Head 2\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\n0\n20\n40\n0\n10\n20\n30\n40\nD\nAttention-map head 1\n0\n20\n40\n0\n10\n20\n30\n40\nE\nAttention-map head 2\n0\n20\n40\n0\n10\n20\n30\n40\nF\nCosine-similarities\nFigure A10: Reverse-engineering a single trained softmax self-attention layer. (A & B) The\nsoftmax self-attention layer outperforms a plain gradient descent step with the mesa-layer again\noutperforming both, visible by plotting the loss across the sequence during training (A) as well as\nwhen looking at the loss throughout the sequence (B) after training. (C) We show the non-zero parts\nof the weight-products W T\nKWQ, PWV for both heads and find a clear structure as in Figure 2. At\nthis point, we speculate that a mix of gradient descent and induction-head-like copying leads to\nthe final prediction. This is reflected by the softmax attention maps (of a single non-cherry picked\nsequence) which show waves of attention indicating a copying behavior based on data similarity\nand not based on positional encodings which are anyway not present here. We again highlight that\ngenerally different architectural choices may lead to different mesa-algorithms, here potentially a mix\nof GD and copying within the model.\nhead of a trained Transformer. Then we construct sparse weight matrices consisting of identity-sub-\nmatrices (scaled by the resp. parameters). We use key-size 40 only here as it is otherwise hard to\naccurately decompose the found weight products into non-square weight matrices. We proceed as\nfor the single-layer experiment and re-train the Transformer from the initial conditions, but during\ntraining also report the test loss of a model that is obtained by equally averaging the weight products\nof our construction and the Transformer. We average the products and not the single weight matrices\nfor the same reasons stated in the previous subsection A6.1.1.\nProbing experiment details: We presented three variations of probing experiments for the multi-\nlayer models: Target, inverse and implicit target probings. For the basic target probings, we sampled\na batch of size 512 and linearly regressed the activations after each layer for that batch against the\ntargets. Using the model learned by the regression, we computed the loss using the same metric as\nduring model training. We repeated this process for 5 different seeds and reported mean and standard\ndeviation in the plotted results.\nWhile analyzing the inverse and the implicit probing experiments, we noticed that a small number of\noutlier-sequences per batch with had comparably large target norm i.e. ||(St\u22121ST\nt\u22121 + 1/\u03bbI)\u22121st||.\nTo overcome this, we introduced a form of dataset pre-processing, where at each token st, we\ndisregarded sequences for which the product (St\u22121S\u22a4\nt\u22121 + 1/\u03bbI)\u22121st with z\u2212 score larger than\n2. For the inverse probing, we first pre-process the batch, sorting out \u2264 5% of the sequences. We\nproceed now to linearly regress the models activations layer by layer for every time step t against\nthe implicit targets (St\u22121S\u22a4\nt\u22121 + 1/\u03bbI)\u22121st and report the loss, per token. We tuned the lambda\nparameter per hand to obtain, across layers, the lowest implicit target probing loss and re-used it for\nthe implicit target probing.\n39\nPreprint\nThere we proceeded as follows: Per token, we obtained a cleaned batch as in the inverse probings,\nthen computed the products (St\u22121S\u22a4\nt\u22121 + 1/\u03bbI)\u22121st and StS\u22a4\nt\u22121. We first linearly regress the\nactivations at each layer against the inverse token product as in the inverse probing experiments and\nobtain a linear model W (d)\nt,inverse probe for every layer and time step t. Then we multiply the activations\nat each layer with this learned model per layer and the other product StS\u22a4\nt\u22121 to obtain a least-squares\nprediction model - or when learning a model based on a single gradient descent step. The prediction\nof this model is weighted with a learning rate which we again tune by hand to achieve overall, across\nlayers, the best implicit target. We define this prediction/vector as g(d)\nt\n= \u03b7StSt\u22121W (d)\nt,inverse probee(d)\nt ,\nsee Figure A12 & A13. Note again that if W (d)\nt,inverse probee(d)\nt\n= (St\u22121S\u22a4\nt\u22121 + 1/\u03bbI)\u22121st and \u03b7 = 1\nwe descent the loss optimally well in one step. We compute the loss per token and layer of this\nprediction model by directly comparing it with the actual targets for one batch. This procedure is\nrepeated for 5 seeds and we report the mean and standard deviation. Furthermore, we include one\nstep of gradient descent as a comparison baseline. To provide a fair comparison between the implicit\ntarget probes, we do not compare to \u2212\u03b7StSt\u22121st i.e. a naive step of GD, for which we obtained\nmuch worse results, but use (with abuse of notation) GDexact \u2212 1 = \u2212\u03b7StSt\u22121W (d)\nt,inverse probee(0)\nt\ni.e.\nthe performance after the data emebedding based on the same cleaning procedure. We tuned the\nlearning rate and \u03bb parameters for all settings parameters by hand.\n0\n5000\n10000\nTrain steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nst\u2032f(1)\n50\nA\nt\u2032 = 50\nt\u2032 = 49\nt\u2032 = 48\nt\u2032 = 47\nt\u2032 = 46\nt\u2032 < 45\n1\n20\n40\nSequence length t\n1\n0.1\n0.01\nMSE(f(d)\nt\n, st + 1)\nB\nd = 0\nd = 1\nd = 2\nd = 3\nd = 4\nd = 5\nd = 6\nd = 7\n1\n20\n40\nSequence length t\n0.1\n0.01\n0.001\nMSE(f(d)\nt\n, (St\n1ST\nt\n1 + 1/ I)\n1st)\nC\nd = 0\nd = 1\nd = 2\nd = 3\nd = 4\nd = 5\nd = 6\nd = 7\nFigure A11: Reverse-engineering full-fledged Transformers: Linear-Hybrid 1+6-layer model\n(A) The first softmax layer groups together neighboring tokens. This can be seen in the high\nsensitivity to the current and previous tokens of the outputs of the first layer of a hybrid-linear\nTransformer. (B & C) We linearly regress the activations of each layer against final targets (C) as well\nas (St\u22121S\u22a4\nt\u22121 + 1/\u03bbI)\u22121st, the curvature-corrected inputs (D) predicted by the provided theory. We\nobserve a harsh phase transition in the last layer when measuring target probing (B) while observing\nan intriguingly stable and gradual probing for curvature-corrected inputs (C), except for the last\nlayer, where we hypothesize that the worse probing loss is explained by the computation of the actual\npredictions. Averages computed over 5 different seeds; shaded area represents standard deviation.\nA6.2\nTESTING TRAINED TRANSFORMERS ON FEW-SHOT IN-CONTEXT LEARNING\nWe provide here details about the post-training in-context learning experiment. After training, see\nSection A6.1 for details, we \"prompt\" the model with few-shot regression datasets i.e. simply switch\nfrom sequences [x1, x2, . . . , xt\u22121, xt] where xt+1 = Wxt and x0 \u223c N(0, I) to [x1, y1, . . . , xN, yN]\nwhere yi = Wxi and all xi \u223c N(0, I). Note that there is no relation between yi, xi+1 as in the\nautoregressive case. In both cases we sample W, if not stated otherwise from the same distribution\ni.e. as random orthogonal matrices. This results in a sequence length of t = 2N and t = 3N when\nincorporating EOS tokens. Throughout the sequence we measure\nLi = E\n\u00141\n2\u2225yi \u2212 f2i\u22121(xi; {(yj, xj)}i\u22121\nj=1)\u22252\n\u0015\n.\n(A46)\nfor i \u2265 2 depicted e.g. in Figure 5.\nFor the EOS-token fine-tuning experiments, we initialize a single vector EOS \u223c N(0, I) and optimize\nthis single vector on the same loss\n40\nPreprint\n0\n10\n20\n30\n40\n50\nSequence length t\n10\n1\n2 \u00d7 10\n1\nMSE(g(d)\nt , st + 1)\nA\nGDexact-1\nd = 0\nd = 1\nd = 2\nd = 3\nd = 4\nd = 5\nd = 6\nd = 7\n0\n10\n20\n30\n40\n50\nSequence length t\n10\n1\nMSE(g(d)\nt , st + 1)\nB\nGDexact-1\nd = 0\nd = 1\nd = 2\nd = 3\nd = 4\nd = 5\nd = 6\nd = 7\nFigure A12: Implicit target probing for full-fledged Transformers. To further support the hy-\npothesis that trained multi-layer Transformers first precondition constructed optimization problems\nby computing an, for example, an approximation of a truncated Neumann series of the required\ninverses before solving the optimization problems, we provide another probing analysis: For each\nlayer, starting with the embedding layer at d = 0, we linearly regress the activations against the\npreconditioned inputs (St\u22121S\u22a4\nt\u22121 + 1/\u03bbI)\u22121st and multiply these probes with \u03b7StS\u22a4\nt\u22121 to compute\na least-squares prediction approximation. We measure therefore measure the possibility to implicitly\npredict the target st+1 from the hidden activations of the model. (A) For full-fledged softmax-only\nTransformers, we observe as expected, a gradual increase in probing performance across layers as\nexpected at first, where we are able to outperform a step of gradient descent. (B) For the hybrid model,\nwe find similar results: The probing performance gradually increases across layers and decreases for\nthe last layer, were we hypothesize that the Transformer performs an update step of gradient descent\nto solve the well-conditioned optimization problem. Note that again, we are able to outperform a\nstep of gradient descent. Averages computed over 5 different seeds; shaded area represents standard\ndeviation.\n1\n20\n40\nSequence length t\n1\n0.1\n0.01\nMSE(f(d)\nt\n, st + 1)\nA\nd = 0\nd = 1\nd = 2\nd = 3\nd = 4\nd = 5\nd = 6\nd = 7\n1\n20\n40\nSequence length t\n0.1\n0.01\n0.001\nMSE(f(d)\nt\n, (St\n1ST\nt\n1 + 1/ I)\n1st)\nB\nd = 0\nd = 1\nd = 2\nd = 3\nd = 4\nd = 5\nd = 6\nd = 7\n0\n10\n20\n30\n40\n50\nSequence length t\n10\n1\n2 \u00d7 10\n1\n3 \u00d7 10\n1\nMSE(g(d)\nt , st + 1)\nC\nGDexact-1\nd = 0\nd = 1\nd = 2\nd = 3\nd = 4\nd = 5\nd = 6\nd = 7\n0\n5000\n10000\nTrain steps\n0.0\n0.5\n1.0\n1.5\n2.0\nst\u2032f(1)\n50\nD\nt\u2032 = 50\nt\u2032 = 49\nt\u2032 = 48\nt\u2032 = 47\nt\u2032 = 46\nt\u2032 < 45\nFigure A13: Probing and copy results for a trained 7-layer Transformer with softmax self-\nattention layers with MLPs and layer-normalization layers. Even when training full-fledged\nTransformers i.e. use all parts of the common Transformer architecture, we observe mostly robust\nand gradual improved probing on the target and the projected inverse when increasing the sequence\nas well as the depth. Furthermore, we find very gradually increasing probing results for the implicit\ntarget probings as in Figure A12 and especially outperform an update step of gradient descent. In\nboth the inverse and the implicit target probings, we find worse results for the probings of the last\nlayer. We hypothesize the reason for this is the update step on the well-conditioned optimization\nproblem that is performed in the last layer. (D) Again, we find very strong copying behavior indicated\nby the sensitivity analyses.\n41\nPreprint\n1\n1000\n2000\nTraining steps\n0.2\n0.4\n0.6\n0.8\nTest loss\nA\nInterpolation\nSM-6\nRevAlg-6\nMesa\n1\n20\n40\nSequence length t\n1\n0.1\n0.01\nMSE(f(d)\nt\n, st + 1)\nB\nd = 1\nd = 2\nd = 3\nd = 4\nd = 5\nd = 6\n1\n20\n40\nSequence length t\n0.1\n0.01\nMSE(f(d)\nt\n, (St\n1ST\nt\n1 + 1/ I)\n1st)\nC\nd = 1\nd = 2\nd = 3\nd = 4\nd = 5\nd = 6\nFigure A14: Reverse-engineering 6-layer softmax self-attention Transformers trained on con-\nstructed inputs. Here we show that a trained multi-layer softmax Transformer trained on constructed\ntokens is explained by a reverse-engineered algorithm (RevAlg-6) and provide an interpolation be-\ntween constructed weights and the found Transformer weights. Furthermore, we provide the target as\nwell as inverse probe analogous to Figure 3 and again provide evidence of the hypotheses that targets\nand the inverse-vector product are gradually approximated throughout depth in the trained Trans-\nformer. As the last layer has to perform one update step of gradient descent on the well-conditioned\noptimization problem, we observe an expected decrease in inverse-probing performance. Averages\ncomputed over 5 different seeds; shaded area represents standard deviation.\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map hybrid, head 1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map hybrid, head 2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map hybrid, head 3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map hybrid, head 4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure A15: Softmax attention maps of the first softmax layer when training a linear-hybrid\nTransformer on unconstructed inputs We visualize all four heads of the first softmax-attention\nlayer and observe strong copying behavior, as predicted by the provided theory, in the heads i.e. full\nattention on the current and the previous token. We average the attention maps over a batch of 1000.\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map softmax, head 1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map softmax, head 2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map softmax, head 3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map softmax, head 4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure A16: Softmax attention maps of the first softmax self-attention layer when training\na softmax-only Transformer on unconstructed inputs. We visualize all four heads of the first\nsoftmax-attention layer and observe strong copying behavior, as predicted by the provided theory, in\nthe heads i.e. full attention on the current and the previous token. We average the attention maps over\na batch of 1000.\n42\nPreprint\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map mesa, head 1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map mesa, head 2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map mesa, head 3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map mesa, head 4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure A17: Softmax attention maps of the first softmax self-attention layer when training\na mesa-hybrid Transformer on unconstructed inputs. We visualize all four heads of the first\nsoftmax-attention layer and observe strong copying behavior, as predicted by the provided theory, in\nthe heads i.e. full attention on the current and the previous token. We average the attention maps over\na batch of 1000.\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map softmax-mlp, head 1\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map softmax-mlp, head 2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map softmax-mlp, head 3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n10\n20\n30\n40\n0\n10\n20\n30\n40\nAttention-map softmax-mlp, head 4\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nFigure A18: Softmax attention maps of the first softmax self-attention layer when training a\nsoftmax-only Transformer with MLPs and layer-normalization on unconstructed inputs. We\nvisualize all four heads of the first softmax-attention layer and observe strong copying behavior, as\npredicted by the provided theory, in the heads i.e. full attention on the current and the previous token.\nWe average the attention maps over a batch of 1000.\nL(EOS) = E\n\"\n1\n2\nN\nX\ni=1\n\u2225yi \u2212 f3i\u22122(xi, EOS; {(yj, xj)}i\u22121\nj=1)\u22252\n#\n(A47)\nvia batch gradient descent for 100 steps with batchsize 1024 on randomly sampled training data. Note\nthat we interleave every datapair with an EOS token i.e. [x1, y1, EOS, x2, . . . , yN\u22121, EOS, xN, yN]\nand we therefore increase the sequence length from 2N to 3N.\nFor the prefix-prompt P, we fine-tune a single sequence of 20 tokens which we append at the\nbeginning of every in-context learning sequence. We initialize here again all vectors before training\nof the soft-prompt Pi \u223c N(0, I) and optimize again the same loss with or without the additional\n(pre-trained, see above) EOS token\nL(P) = E\n\"\n1\n2\nN\u221220\nX\ni=21\n\u2225yi\u221220 \u2212 f3i\u22122+20(xi\u221220, P, EOS; {(yj, xj)}i\u221221\nj=1 )\u22252\n#\n(A48)\nvia batch gradient descent for 100 steps with batchsize 1024 on randomly sampled training data\nresulting in sequences [P1, . . . , P20, x1, y1, EOS, x2, . . . , yN\u22121, EOS, xN, yN]\n43\nPreprint\n0\n20\n40\n60\nDatapoints (xi, yi) in sequence\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nMSE\nA\nBase\nBase+EOS\nBase+EOS+P\nLSQ\n0\n20\n40\n60\nDatapoints (xi, yi) in sequence\n1.0\n1.5\n2.0\n2.5\nMSE\nB\nBase\nBase+EOS\nBase+EOS+P\nLSQ\n0\n20\n40\n60\nDatapoints (xi, yi) in sequence\n0.5\n1.0\n1.5\n2.0\nMSE\nC\nFew-shot regression\nBase\nBase+EOS\nBase+EOS+P\nLSQ\n0\n20\n40\n60\nDatapoints (xi, yi) in sequence\n1.0\n1.5\n2.0\n2.5\nMSE\nD\nContinual few-shot regression\nBase\nBase+EOS\nBase+EOS+P\nLSQ\n0\n20\n40\n60\nDatapoints (xi, yi) in sequence\n0.50\n0.75\n1.00\n1.25\n1.50\nMSE\nE\nAblation 1\nBase\nBase+EOS\nBase+EOS (Abl.)\nFigure A19: Autoregressively-trained hybrid-mesa and hybrid linear Transformers prompted to\nsolve supervised few-shot regression problems. We present here the results analog to Figure 5 in\nthe main text with hybrid-linear (A & B) and hybrid-mesa (C & D) Transformers. The in-context\nlearning performance for both models decreases gradually and significantly with the number of\nlabeled examples. Again, when prompted with a special EOS token or a prefix-prompt P, which\nwe both fine-tune for this regression task on a held-out training set, the performance improves\nconsiderably. This highlights the usefulness of prompt-tuning already in this very simple setting.\nFurthermore, both models show strong performance when tested with multi-task problems, with the\nhybrid-mesa significantly outperforming the linear model for increasing number of datapoints (xi, yi).\nWhile the hybrid-linear model shows stronger performance for earlier datapoints, the hybrid-mesa\nmodel matches (and outperforms) the LSQ baseline with prefix- and EOS-prompting. (E) As an\nablation, we test how a softmax-only model, corresponding to Figure 5 of the main text, behaves\nwhen tested with a sequence that contains EOS tokens only for the first 20 data pairs. As expected,\nthe performance gradually decreases to non-EOS level once the datapoints are presented without\nEOS-tokens. Averages computed over 5 different seeds; shaded area represents standard deviation.\n44\nPreprint\nTable A3: Hyperparameters for language modelling experiments across all Transformer variants i.e.\npure softmax, linear-hybrid and mesa-hybrid with/out MLPs.\nHyperparameter\nValue\nDataset\nThe pile (Gao et al., 2020)\nTokenizer\nGPT-2 tokenizer - we append a special \"EOS\" token between every sequence\nContext size\n1024\nVocabulary size\n50257\nVocabulary dim\n756\nOptimizer\nAdam (Kingma & Ba, 2015) with \u03f5 = 1e\u22128, \u03b21 = 0.9, \u03b22 = 0.95\nWeight decay\n0.1\nBatchsize\n256\nGradient clipping\nGlobal norm of 1.\nPositional encodings\nWe add standard positional encodings.\nDropout\nWe use embedding dropout of 0.1 right after adding positional encodings.\nArchitecture details\n12 heads, key size 64, token size 756, no input- but output-embedding\nWeight init\nW \u223c N(0, \u03c32) with \u03c3 = 0.02 and bias parameter to zero. We scale all\nweight matrices before a skip connection with\n1\n2\n\u221a\nN with N the number of layers.\nLearning rate scheduler\nLinear warm-up starting from 1e\u22126 to 3e\u22124 in the first 8000 training steps,\ncosine annealing to 2e \u2212 4 for the next 300 billion tokens\nMLP size\nWidening factor 4 i.e. hidden dimension 4 \u2217 756 with ReLU\nnon-linearities (Hahnloser et al., 2000)\nMesa regularization \u03bb\nWe initialize the learnable regularization parameter \u03bb for every mesa-head to 1.\nA6.3\nLANGUAGE MODELING EXPERIMENTS\nFigure A20: In-context learning score\ncomparing loss values later in se-\nquence.\nWhen comparing to the in-\ncontext learning score comparing earlier\ntime points, Figure 6, we still observe\na noticable gap between mesa and soft-\nmax indicating remaining memory reten-\ntion problems for the mesa-layer. We\nhypothesize that learned mesa-forgetting\nfactors might close this gap.\nWe provide here details about the language modeling ex-\nperiments. We use standard values found in the literature\nand the same hyperparameters, which we did not tune,\nacross all experiments. We, if not stated otherwise, use\nthe standard GPT-2 transformer architecture with Layer-\nNorm (Ba et al., 2016), MLPs between self-attention layer\nand skip-connection after every layer which we train on\na standard (autoregressively) masked cross-entropy loss.\nWe do not use an input embedding layer but an output pro-\njection before computing the logits. To train enable stable\ntraining of the linear as well as the mesa-layer, we apply\nthe proposed key and query normalization of schlag and\nsimply devide them by their L2 norm. Intriguingly, this\nstabilizes training drastically also for the mesa-layer after\nwhich we did not observe any more instabilities. Note\nthat this is very similar to using additional LayerNorm\n(Ba et al., 2016) on the keys and queries. Except from\nthis normalization, all models are constructed and trained\nidentically. See A3 for an overview of all design decisions\nand hyperparameters. Also, we refer to the appendix of\nSchlag et al. (2021) on how to compute the DPFP kernels\nto non-linearly alter the key and query features,we use\n\u03bd = 3 if not stated otherwise.\nA7\nSOFTWARE\nThe results reported in this paper were produced with open-source software. We used the Python\nprogramming language together with the Google JAX (Bradbury et al., 2018) framework, and the\nNumPy (Harris et al., 2020), Matplotlib (Hunter, 2007), Flax (Heek et al., 2023), Haiku (Hennigan\net al., 2020) and Optax (Babuschkin et al., 2020) packages.\n45\n"
  },
  {
    "title": "LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning",
    "link": "https://arxiv.org/pdf/2309.06440.pdf",
    "upvote": "9",
    "text": "LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand\nfor Robot Learning\nKenneth Shaw, Ananye Agarwal, Deepak Pathak\nCarnegie Mellon University\nFig. 1: (a) LEAP Hand is an anthropomorphic dexterous robot hand designed for robot learning research. It can be assembled in under 4 hours\nfor 2000 USD, is composed of readily available parts, and is robust. (b) to-scale comparison of LEAP Hand and a human hand (c-h) LEAP\nHand in different power and precision grasps holding common objects. The hand design and code will be open-sourced to democratize access\nto hardware for anthropomorhic dexterous manipulation. Video, assembly instructions, and sim2real pipeline at https://leap-hand.github.io/\nAbstract\u2014Dexterous manipulation has been a long-standing\nchallenge in robotics. While machine learning techniques have\nshown some promise, results have largely been currently limited\nto simulation. This can be mostly attributed to the lack of\nsuitable hardware. In this paper, we present LEAP Hand, a low-\ncost dexterous and anthropomorphic hand for machine learning\nresearch. In contrast to previous hands, LEAP Hand has a novel\nkinematic structure that allows maximal dexterity regardless\nof finger pose. LEAP Hand is low-cost and can be assembled\nin 4 hours at a cost of 2000 USD from readily available parts.\nIt is capable of consistently exerting large torques over long\ndurations of time. We show that LEAP Hand can be used to\nperform several manipulation tasks in the real world\u2014from visual\nteleoperation to learning from passive video data and sim2real.\nLEAP Hand significantly outperforms its closest competitor\nAllegro Hand in all our experiments while being 1/8th of the cost.\nWe release detailed assembly instructions, the Sim2Real pipeline\nand a development platform with useful APIs on our website at\nhttps://leap-hand.github.io/\nI. INTRODUCTION\nHand dexterity has been critically responsible for human\ncognition through active manipulation, tool use, and governing\nhow humans learn from the world [1, 2, 3]. Replicating the\ndexterity of the human hand with a robot hand has been a long-\nstanding challenge in robotics. Machine learning techniques\nhave recently shown promise in areas such as learning from\nhumans. However, unlike the learning successes in locomotion\n[4, 5] across truly diverse terrains, robotic manipulation results\nin the real world have mostly been limited to one degree-of-\nfreedom parallel jaw grippers [6, 7, 8]. In contrast, dexterous\nmanipulation has largely been limited to simulation [9, 10] with\ncomparatively fewer real-world results [11, 12, 13, 14, 15].\nA major bottleneck in democratizing dexterous manipulation\nhas been the hardware. Tendon-based hands like Shadow [16],\nwhile impressively capable [11], cost over 100K USD and\noften require significant maintenance [17] due their complicated\nnature. While Inmoov [18] is inexpensive and open source,\nit only has 5 actuators on weak tendons. Therefore, direct-\ndriven hands have been the popular alternative for many\napplications [19, 20]. The Allegro Hand has been a popular\ndirect-driven hand, but it is often unreliable, difficult to repair,\nand does not have an anthropomorphic kinematic structure,\n(see Fig 3) and is expensive at over $16K. Please see Section\nII for further analysis.\nAs a result, only a few labs have access to hardware capable\nof complex dexterous tasks. This is in stark contrast to 2-finger\ngrasping or locomotion [22, 23, 24, 25] where readily available\nhardware allows results to be easily reproduced and improved\nupon by the community. Following this analogy, good hand\nhardware for machine learning must be durable, repeatable,\nlow-cost, versatile, and ideally anthropomorphic to enable easy\ntransfer learning from humans.\narXiv:2309.06440v1  [cs.RO]  12 Sep 2023\nFig. 3: Relative size of popular robot hands to scale. Left to right, adult human hand, Allegro Hand [20], LEAP-C Hand, LEAP Hand,\nInmoov [18], D\u2019Manus [21]. LEAP Hand is similar in size to Allegro and \u223c 30% larger than a human hand. D\u2019Manus is considerably larger\nthan the rest. Because of the tendon-driven nature, Inmoov is the smallest robotic hand. The hands are accurate to scale.\nWe propose LEAP Hand\u2013 a dexterous, extremely low-cost\nand robust hand for robot learning, built from off-the-shelf\nor 3D-printed parts. Our hand can be assembled in under 4\nhours at a cost of 2000 USD, which is 1/8th the cost of the\nAllegro Hand and 1/50th to that of ShadowHand. While we\nacknowledge this is still not affordable for all, we believe it is\na step towards democratizing dexterous manipulation research.\nWe show through a number of rigorous experiments that LEAP\nHand is both robust, durable, and able to exert large torques\nover long periods of time. Additionally, our robot hand it is\neasily repairable in-house just using a standard $250 3D printer\nand does not need to be sent out for repair.\nAlthough robustness and low-cost is critical, they should\nnot come at the cost of dexterity and anthropomorphism. We\nbelieve a good versatile hand is the one that is both dexterous\nas well as anthropomorphic because much of the world around\nus, for instance, doors, kitchens, tools, or instruments, are\ndesigned with human hands in mind, making it easier to learn\nby watching humans act. In LEAP Hand, we aim to maximize\ndexterity while being kinematically similar to a human hand.\nSince the ball joint at the human knuckle (Metacarpopha-\nlangeal aka MCP) cannot be replicated with direct-driven\nhands, it must instead be approximated using two separate\nmotors. Prior work in direct driven hands has converged\nprimarily to two designs, see Fig 4, one that allows abduction-\nadduction of fingers in open hand pose and the other that only\nallows with the finger flexed upwards. However, both of these\nlose one degree of freedom (DoF)\u2014either in the flexed or\nextended position of the finger. In LEAP Hand, we propose\na new kinematic mechanism to facilitate universal abduction-\nadduction for direct-driven hands that retain all degrees of\nfreedom in all finger positions. We demonstrate that this leads\nto higher dexterity and for improved grasping and in-hand\nmanipulation.\nFinally, we show that LEAP Hand easily integrates with\nexisting results in robot learning. For instance, YouTube video-\nbased learned teleoperation and behavior cloning. In addition\nto the physical robot hardware, we also release an Isaac Gym-\ncompatible simulator for the LEAP Hand and show sim2real\ntransfer for a contact-rich task of blind in-hand rotation of a\ncube [26]. This shows that the hardware and simulation are\naccurate and that complex tasks trained in simulation can be\ntransferred to the real hand. We open source the URDF model,\nassembly instructions, ROS/Python API, mapping methods\nfrom human hands to LEAP Hand, and an Isaac Gym simulation\nenvironment at https://leap-hand.github.io/.\nII. RELATED WORK\nRobot Hands\nShadow [16] and ADROIT [27] hands\npaved the way to enable complex, contact-rich dexterous tasks\nwith an anthropomorphic ball joint MCP. [11, 17]. However,\nthey are costly (100k USD) and require constant maintenance.\nIn contrast, the Inmoov hand is 3D printable, tendon-driven, and\nhuman-like[18]. It has only one DoF per finger and is reliant\non tendon actuation which is difficult to calibrate and can be\ninaccurate. Bauer et. al. [28] present a soft tendon-driven hand\nthat is very flexible for many configurations. Unfortunately, it\nis difficult to simulate due to its deformable nature [29, 30]. In\ncontrast to tendon-driven hands, which have motors in the wrist,\nthe Allegro Hand [20] has its motors in the finger joints. It is\nmost popular in research labs [31, 32, 33, 34, 35] because it\nis relatively cheap (16k USD). However, users find the motors\nin the fingers to be weak for many everyday tasks. Moreover,\nthe closed-source components are difficult to repair or replace.\nAdditionally, its kinematic structure is not anthropomorphic\nor dexterous as we demonstrate. The ROBEL suite (which\nincludes D\u2019Manus) [21] is more robust, open-sourced, and\neasy to build. However, it has only two fingers and a thumb\u2014\nmaking it significantly different from a human hand. Yuan\net. al. [36] accomplish within-hand manipulation using rollers\nattached to the fingers. Humans lack this degree of freedom\nand manipulate objects in a very different manner. [37, 38, 39]\nhave shown impressive results and hold a lot of promise by\nusing fluids and linear actuators to move the fingers, but these\nhands are not readily available and too complicated to quickly\nproduce, use, and maintain for robot learning.\nRapid Manufacturing\nAluminum machining is tradition-\nally used to create strong parts but is prohibitively difficult\nand expensive. Manufacturing plastic parts includes a cumber-\nsome process of mold making, casting, curing, and support\nFig. 4: Comparison of MCP joints in different robot hands and their dexterity and two different positions. (A) In LEAP-C Hand there is a\nlarge range of motion at extended but not flexed position (B) In LEAP Hand, at flexed and extended positions, the fingertip has a large range\nof motion. (C) In Allegro, there is a large of motion at flexed but not extended position.\nremoval [40]. In contrast, additive manufacturing can be used\nto create parts very quickly for prototyping. In our paper,\nwe leverage recent advancements in extruders, hot-ends, and\nmotors made by the open-source Reprap Community [41].\nThis allows us to directly print soft flexible filaments like\nNinjaflex [42]. We use this to create many parts for LEAP\nHand like the hard palm and soft rubber fingertips.\nLearning Dexterity Using a Shadow hand and Sim2real,\nAndrychowicz et. al. [11, 17, 43] accomplish in-hand rotation\nfor a variety of objects. Policies that scale to thousands of\nobjects can also be trained in simulation [9, 44]. [45] uses the\nD\u2019Hand to reposition a valve. In-hand rotation of Baoding Balls\nusing the Shadow Hand trained purely in the real-world [46],\nand pipe insertion using the D\u2019Hand [47] are other notable\nexamples of dexterous manipulation.\nSeveral recent works focus on supervising policies of robot\nhands [48, 49, 50, 51]. from MANO [52] parameters which\nparameterize a human hand. Closely related is the teleoperation\nof robot hands from real-time video [31, 53], which can be used\nto guide learning and improve sample-efficiency [54, 55]. Hand\nposes can be extracted from video data available on the web to\nlearn manipulation policies [54, 56]. Large-scale pre-training\nusing internet videos is helpful for efficiently training robot\nhands for downstream tasks using a few task specific-demos\n[57] and also on non-dexterous manipulation [58, 59].\nIII. KINEMATIC DESIGN AND ANALYSIS\nThe kinematic structure of a hand refers to the arrangement\nof its joints which determines the different poses and forces\nof motion it can apply. First, LEAP Hand should be as\nanthropomorphic as possible so that data from humans can be\nused to learn skills [58, 57, 54] with machine learning. This can\nbe done using methods such as teleoperation with VR gloves\nor extracting keypoints from videos of human hands [53]. In\naddition, LEAP Hand should be dexterous for tasks such as in-\nhand manipulation from sim2real. In this section, we propose a\nrobot hand design that is both anthropomorphic and dexterous.\nIn the human hand, there are four main degrees of freedom\nin each finger (Fig. 5). The knuckle or metacarpophalangeal\n(MCP) is a ball joint with two degrees of freedom that allows\nabduction/adduction and flexion/extension. The joint closest\nto the knuckle is called the proximal interphalangeal (PIP)\njoint. The last joint, closest to the fingertip, is the distal\ninterphalangeal (DIP). The PIP and DIP are hinge joints, each\nwith one degree of freedom. The human hand features an\nopposable thumb which allows the application of force in\nopposition to other fingers. This enables a variety of power\nand precision grasps [61]. To easily map motions, a robot hand\nmust have analogous joints to a human hand.\nTo replicate this structure, it is alluring to use tendons like\nrobot hands such as the ShadowHand [16]. Such tendon-driven\nhands can store the large motors needed to drive them in the\nwrist, enabling greater flexibility in joint design and introduce\nball joints. However, they are very expensive (100K USD),\ncomplicated or hard to maintain. As a result, cheaper direct-\ndriven alternatives [20, 19] have been more popular.\nA. Universal Abduction-Adduction Mechanism\nDirect-driven hands must store the motors inside the fingers\nso they are limited in kinematic structure and cannot precisely\nimitate the human hand. Since the PIP and DIP joints are hinge\njoints, they are easily modeled, each with a single actuator.\nA ball joint cannot be modeled in this way and is typically\napproximated using two motors (MCP-1, MCP-2) arranged\nclose together [19]. Prior seminal work has proposed two\ndesigns for this (Fig. 4). However, both of these designs,\nAllegro and LEAP-C Hand, lose one degree of freedom in\neither the extended or closed position. As a result, Allegro is\nless dexterous when extended whereas LEAP-C Hand (like\nC-Hand in [19]) is less dexterous when closed.\nThe reason for the lost dexterity in both LEAP-C Hand and\nAllegro is that the axis of the motor responsible for adduction-\nabduction (MCP-2) is fixed to the palm of the hand. In LEAP-\nC Hand, the axis is perpendicular to the plane of the palm,\nwhereas, in Allegro, it lies in the plane of the palm. Thus, when\nthe finger becomes parallel to this axis, that DoF is ineffective.\nPlease see Figure 4 and the kinematic tree in the supplemental.\nFig. 6: We compare the possible positions of opposability of the\nthumb and each of the other fingers on each of the three hands. We\nfind that LEAP Hand has the best even spread on top of the palm\nand a very large contact area.\nIn LEAP Hand, we propose a new universal abduction-\nadduction mechanism for the fingers such that they can retain\nall degrees of freedom at all MCP positions. Instead of the\nMCP-2 axis being fixed to the palm (i.e., of motor responsible\nfor adduction-abduction), the key idea is to bring the axis to\nthe frame of reference of the first finger joint and arrange it\nsuch that it is always perpendicular to it. This allows the finger\nto have adduction-abduction in all positions (Fig. 4). Thus,\nLEAP Hand has adduction-abduction in the extended position\n(similar to LEAP-C Hand) as well as pronation/supination in\nthe flexed position (similar to Allegro).\nB. Evaluating Manipulability via Thumb Opposability\nChalon et.al. [62] and Lee et.al. [19], have shown that what\nmakes a hand more versatile is not merely the degree of\nabduction-adduction but also its thumb opposability volume.\nWe test our design against Allegro and LEAP-C Hand, a\nbaseline hand we manufacture with the same motors and parts\nas LEAP Hand. In Fig. 6, we plot the intersection of the thumb\nand finger workspaces for each hand and compute a thumb\nopposability metric [62]. In Table II, we show that LEAP Hand\ncombined with the new MCP joints in the secondary fingers is\nbetter placed and is more dexterous compared to other available\nhands because of the increased opposable volume.\nNext, manipulability measures the ease with which the\nfingertip can be moved in various directions at a particular joint\npose. We use metrics introduced by Yoshikawa et. al. [63].\nTo evaluate this, many calculate the manipulability ellipsoid\nfrom the end-effector Jacobian which models the directions in\nwhich the end-effector can move. We compute the volume of\nthis ellipsoid using the following equation:\nw =\nq\ndet (J(q)J(q)T)\nwhere q is the joint configuration and J is the Jacobian of\nthe end-effector. Note that because we are calculating volumes,\na hand that can only move in one or two cartesian directions\nwill have a volume close to zero at that pose. In three key poses,\nwe show that LEAP Hand has consistently larger ellipsoids of\ngreater volume for both the cartesian and angular components of\nthe jacobian. This means that LEAP Hand has better movement\nat the fingertips in these few poses and a higher manipulability\nmetric leading to more dexterity (Table II).\nFig. 5: The human hand kinematics above has ball joints at the MCP and CMC joints. These are difficult joints for low-cost hands to include.\nLeft Figure from [60]. Comparison of MCP joints in different robot hands. (A) In LEAP-C Hand there is a large range of motion at extended\nbut not flexed position (B) In LEAP Hand, at flexed and extended positions, the fingertip has a large range of motion. (C) In Allegro, there is\na large of motion at flexed but not extended position.\nRobot/Position\nDown (m3)\nUp (m3)\nCurled (m3)\nAllegro Hand\nLinear\n8.11 \u00d7 10\u22129\n3.98 \u00d7 10\u221213\n2.39 \u00d7 10\u22125\nAngular\n0\n0\n0\nLEAP-C Hand\nLinear\n1.60 \u00d7 10\u221212\n1.23 \u00d7 10\u221210\n9.28 \u00d7 10\u22125\nAngular\n1.02 \u00d7 10\u221213\n1.02 \u00d7 10\u22129\n2.02 \u00d7 10\u221213\nLEAP Hand (ours)\nLinear\n2.02 \u00d7 10\u22126\n2.42 \u00d7 10\u22126\n4.51 \u00d7 10\u22125\nAngular\n1.20 \u00d7 10\u22125\n1.20 \u00d7 10\u22125\n1.20 \u00d7 10\u22125\nTABLE I: We show the manipulability ellipsoid volume for both\nthe linear and angular component at three different finger positions,\ndown, all the way up, and then halfway/curled. We find that LEAP\nHand has a large manipulability ellipsoid at all three configurations.\nFinally, we show that increased dexterity leads to practical\nbenefits as well. In the grasping test (Sec. VI-A), we find that\nLEAP Hand is able to grasp more objects tightly. In the blind\nin-hand cube rotation task (Sec. VI-D), we find that LEAP\nHand is able to rotate the cube much faster than Allegro.\nIV. HAND DESIGN PRINCIPLES\nA good kinematic design must be realized effectively in\nhardware. In particular, the hardware should be low-cost, easy\nto repair, and robust.\nA. Low-cost and Easy to Repair\nIn contrast to locomotion or manipulation with two-fingered\ngrippers, real-world research in dexterous manipulation has\nbeen limited. This can be attributed in large part to the lack of\nsuitable dexterous hand hardware. Commonly used dexterous\nhands such as ShadowHand [16] and AllegroHand [20] cost\n100K and 16K USD, respectively, and must be sent back to\nmanufacturers for repair in case of damage. This hardware is\nout-of-budget or impossible to maintain for many researchers\nallowing only a small fraction to work on real-world dexterous\nmanipulation. On the other hand, due to the availability of cheap\nand reliable locomotion [24, 25] and manipulation [22, 23, 64]\nhardware, a large community of researchers is able to build\noff of each others\u2019 work and drive progress.\nA suitable hand should therefore be as accessible as possible.\nThis implies that it should be low-cost and easy to repair. In\nLEAP Hand, we accomplish this by using as many off-the-\nshelf parts as possible and fabricating the rest using only a\ncommodity 3D printer that costs around 200 USD. It can be\nassembled in under 4 hours.\nLEAP Hand is designed to be modular. This allows key\nfeatures of the robot hand to be changed, such as the length\nor number of fingers and the distances between each of the\nfingers in the palm for particular learning tasks or for analysis.\nAdditionally, the modularity makes the hand easily repairable\nwith only a few distinct parts.\nB. Robustness\nLearning, whether via teleoperation, behavior cloning, or\nreinforcement learning, on a robot hand can be notoriously\nOpposability Vol.\nIndex (mm3)\nMiddle (mm3)\nRing (mm3)\nAllegro\n409,135\n348,809\n204,281\nLEAP-C Hand\n834,516\n743,764\n638,605\nLEAP Hand (ours)\n1,125,556\n1,056,746\n804,618\nTABLE II: We show the finger-to-thumb opposability volume in\nmm3 by randomly sampling 25,000 joint configurations and finding\nthe instances at which both fingers touch and recording that contact\npoint. The volume of this area of contact is calculated and reported.\nharsh on hardware, especially when it is placed on a robot\narm [31, 33]. Due to the movement of the arm, the hand may\nrepeatedly collide with the table and objects it is trying to grasp.\nA robot hand should be robust to such treatment and continue\nto function reliably without breaking. In addition, a robot hand\nmust be able to impart large torques. This is required for lifting\nheavy objects or using heavy tools like drills or hammers.\nWhile 3D printing is fast and inexpensive, the resulting\nparts are often not strong enough. One alternative is custom\nmetal machined parts. However, we avoid these as they add\nsignificant cost and require specialized skill and equipment\nto manufacture. We instead rely on inexpensive ($10) off-the-\nshelf professionally extruded reinforced plastic brackets from\nRobotis [65] that are designed to withstand wear and tear. We\nonly print the palm and smaller wire guide spacers using a\ncommercial 3D printer.\nThe joints in LEAP Hand are designed to exceed the strength\nof the human hand. We choose motors geared to high torque\noutput for their size while still being capable of a hand-like\njoint movement velocity of around 8 rad/sec. The amount of\nmotor mass inside the hand is maximized compared to the size\nof the hand, and every other component is minimized. This\nenables the hand to be as strong as possible for its human-like\nform factor. Because these motors are so powerful, we support\ncurrent- or torque-limiting them as in Section V to manipulate\nfragile objects and increase the durability of the hand.\n1) Endurance test\nTo test the strength of the hand over a long period of time,\nwe hang a 2kg weight on one of the fingertips. This pose is\nAllegro\nFailure\nFig. 7: Repeatability test. Left: An illustration of LEAP-C Hand per-\nforming repeating grasp-ungrasp on a small plush dice using one joint\nfor an hour. Right: Comparison of LEAP-C Hand and Allegro [20].\nAfter just 15 minutes, the Allegro Hand cannot maintain movement. In\ncontrast, LEAP Hand continues to maintain movement with minimal\njoint error (< 0.05 rad). Videos at https://leap-hand.github.io/\nHand\nStrength (N)\nPower Density N \u00d7 DOF/(cm2)\nBauer et. al [28]\n37.4\n0.677\nAllegro Hand [20]\n8.5\n0.35\nD\u2019Manus [21]\n27.8\n0.313\nInmoov Hand [18]\n5.8\n0.116\nAdult Human Hand\n26.5\n2.199\nLEAP Hand\n19.5\n1.045\nLEAP-C Hand\n21.5\n1.15\nTABLE III: Pullout Test. A resistance comparison of each hand to\npullout force which correlates to grasping strength. Power density is\nthe total amount of motor force per square area of the hand.\nsimilar to if a person was holding a half gallon of milk up with\none finger without using their palm as support. We find that\nLEAP Hand is able to continuously hold the grasp for an hour\nwith only a small angle error. While the current usage gradually\nincreases initially, it stabilizes along with the temperature of\nthe motors, which remain cool. The current usage of the top\nmotor reaches\n250mA, which is still less than half of the\nmaximum possible. The Allegro Hand is not powerful enough\nto complete this test. See Figure 8 for a plot of the results.\n2) Repeatability test\nWe test the consistency and accuracy of LEAP Hand against\nAllegro Hand by running them continuously for 1 hour in a\ngrasping scenario as in Figure 7. We continuously raise and\nlower a small (25g) plush dice strapped onto the finger by\ncommanding one of the base finger joints up and down at 5Hz.\nThe error of the desired joint angle compared to the actual\njoint angle is graphed through time.\nLEAP Hand has a consistent error of 0.025 radians in the\nup position and 0.005 in the down position (Fig. 7), which\nis reasonable given the PID controller and the 750mA current\nlimit. On the other hand, the Allegro hand starts at a much\nhigher error. After 15 minutes, it begins to fail and then\ncompletely fails to move on one out of three grasps. This\nwas not a failure of the position sensor in the motor. The strain\nof the continuous grasping on the motor caused overheating\nsuch that the motor was not able to apply required torques.\nFig. 8: Endurance Test. We balance a heavy 2kg weight on only one\nfingertip of LEAP Hand for one hour. On the left axis, we show that\nthe angle error of commanded vs actual remains small. The right axis\nshows that the current use does initially increase with the temperature\nof the motor. However, it still withholds the weight and uses less than\nhalf of its maximum possible current of 600ma.\n3) Pull-out force test\nThis test measures the amount of momentary outward force\nthat can be resisted by a flexed finger from a hooked force\ngauge before failure. Failure is defined as a motor or gear\nslipping or a finger deviating more than 15 degrees from its\ncommanded position (Fig.9). The force returned correlates with\nthe grip strength.\nTab. III compares force for robot and human hands.\nD\u2019Manus [21] is the strongest due to its large motors. Of\nthe anthropomorphic hands, LEAP Hand performs the best,\nexceeding the grip strength of a human. The Allegro Hand is\nweak because of smaller motors explaining why it struggles in\nmany grasping tasks. The tendon-driven hands, Bauer et. al,\nand Inmoov do not perform that much better in this test even\nthough they can store large motors in their wrists and arms.\nWe find that these tendons often slip and cannot provide that\nmuch force at the end-effector.\nV. FABRICATION AND SOFTWARE\nFabrication\nFirst, each of the 3D printed components must\nbe fabricated ( Fig. 1). A $200 Ender 3 3D printer [66] was\nused with PLA plastic over a 2 day period, but any consumer-\ngrade FDM printer will suffice. Each of the two palm pieces\nis printed along with fingertips and finger spacers. We collect\nthe 3D printed parts, plastic extruded brackets, the Dynamixel\nmotors [67], U2D2 controller, and assorted cabling. The fingers\nare assembled individually using brackets, 3D-printed finger\nspacers, and motors. The assembly process for LEAP Hand\ntakes around 4 hours. Then each of the fingers is mounted\nonto the palm, and their firmware is flashed for control. The\nhand interfaces with the computer using a USB cable and ROS,\nPython, or C++. The 4-finger LEAP Hand weighs 595g and\ncan be easily mounted to a variety of robot arms. Full video\ninstructions of the assembly process is on our website.\nFig. 9: Pullout Test. A pullout\nforce is applied and the maximum\nforce is recorded before the hand\nhas a 15\u25e6 error or slipping.\nSoftware\nA variety of\ncontrol\nmodes\nare\nsup-\nported on LEAP Hand: posi-\ntion control, current control,\ncurrent-based position con-\ntrol, and velocity control.\nPosition control enables the\nhand to create torques to\nmatch a desired position\non the motors which is\ntypical of many PID-based\ncontrollers. Current control\nmode enables a desired\ntorque to be applied to the\nmotors. Current-based posi-\ntion control mode enables\nPID-based position control\nbut also caps the maximum\ncurrent and torque. This en-\nables the hand to follow position commands but also prevents\nlarge torques, which can be unsafe for the robot and the\nenvironment around it.\nFig. 10: Teleoperation and Behavior Cloning. Left: We perform\ndexterous teloperation using Telekinesis [53] with a single view color\ncamera. Right: We perform behavior cloning from internet video and\nteeloperated demonstrations using Videodex [57].\nSimulation\nWe construct a detailed 3D assembly of the\nhand as used in (Fig. 6) on Pybullet. This will enable anyone\nto 3D print and design their own version of LEAP Hand. In\naddition to hardware, we release an Isaac Gym and Pybullet-\nbased simulator for LEAP Hand. Its faithfulness to the real\nworld is verified by performing sim2real in Sec. VI-D. We\nrelease the sim2real platform to jumpstart lab research with\nLEAP Hand.\nVI. LEAP HAND APPLICATIONS\nFirst, we compare all of the hands in a grasping test with\nvarious everyday objects. Next, we compare the two most\nrobust, human-like hands, the 4-finger LEAP Hand and the\nAllegro Hand [20] against each other in a variety of machine\nlearning tasks. Teleoperation from human video demonstrates\ngrasping capabilities and human-like form factor. Next, leverag-\ning internet video shows the capability of learning from humans.\nFinally, we show LEAP Hand on in-hand manipulation via\nsim2real, which demonstrates that the simulation and hardware\nare precise. In this task, LEAP Hand is able to rotate the\ncube faster and is more robust to disturbances. Please see the\nsupplemental and our website for videos of these results.\nSuccess Rate\nCompletion Time (in s)\n#\nTeleoperated Task\nLEAP Hand\nAllegro\nLEAP Hand\nAllegro\n1\nPickup Dice Toy\n1.0\n0.9\n6.5 (1.7)\n8.6 (2.65)\n2\nPickup Dino Doll\n1.0\n0.9\n6.0 (1.5)\n8.2 (3.49)\n3\nBox Rotation\n0.7\n0.6\n28.2 (15.7)\n37.2 (12.6)\n4\nScissor Pickup\n0.6\n0.7\n32.4 (7.8)\n28.6 (9.4)\n5\nCup Stack\n0.8\n0.6\n15.4 (7.0)\n21.5 (7.6)\n6\nTwo Cup Stacking\n0.6\n0.3\n18.2 (9.2)\n27.3 (11.0)\n7\nPour Cubes in Plate\n0.8\n0.7\n30.2 (15.2)\n36.8 (17.7)\n8\nCup Into Plate\n0.8\n0.8\n6.2 (2.5)\n10.6 (4.4)\n9\nOpen Drawer\n0.9\n0.9\n18.2 (11.2)\n23.6 (12.3)\n10\nOpen Drawer & Pick\n0.7\n0.6\n37.2 (10.2)\n33.7 (8.1)\nOutperform rate\n9/10\n3/10\n8/10\n2/10\nTABLE IV: Teleoperation\u2014comparing LEAP Hand and Allegro.\nSuccess rate and average completion time of a trained operator\ncompleting a variety of teleoperated tasks. LEAP Hand outperforms\nor matches the Allegro performance on 9/10 tasks.\nObject\nGrasp Type [61]\nLEAP LEAP-C Allegro D\u2019Manus Inmoov\nPower:\nMustard\nMed. Palm+Pad\n20\n20\n13\n8\nY\nToy Kick Ball\nLrg. Palm+Pad\n20\n20\n9\n20\nN\nGolf Ball\nSmall Pad\n16\n20\n7\n0\nY\nSoftball\nLarge Pad\n20\n20\n10\n15\nN\nDrill\nTrigger Press\n20\n20\n15\n0\nN\nPringle Can\nPower Palm\n19\n20\n20\n0\nY\nPan (from rim)\nDisk Grasp\n20\n20\n20\n14\nN\nIntermediate:\nChopsticks\nTripod Grasp\n16\n13\n0\n0\nN\nWood Cylinder\nCigarette Grasp\n4\n5\n0\n0\nN\nPrecision:\n1\u201d Cube\n2 Finger Precision\n20\n20\n20\n0\nN\nM&M\nTip Pinch Grasp\nY\nY\nY\nN\nN\nWine Glass\nFlat Hand Cupping\n20\n20\n4\n0\nN\nCredit Card\nLateral Pinch\n20\n20\n8\n0\nN\nTABLE V: We test each robot hand on a variety of objects and\ngrasps types and see how much perturbation force they can resist (in\nnewtons). The dexterous morphology of LEAP Hand as well as its\nstrong motors enables the cigarette and flat-hand cupping grasps.\nA. Grasping Test using Teleoperation\nWe compare each of the hands and their ability to perform\ndifferent types of grasp when holding objects. To quickly\nexperiment and find these poses, we use the Manus Meta VR\nglove [68] to accurately teleoperate the first three hands (see\nappendix for details). Since D\u2019Manus is not anthropomorphic\nenough to teleoperate from human motion, we manually control\nkeyframes for it. We show various types of grasps that each\nhand can perform, and the amount of perturbation force they\ncan resist (up to 20N). Once the object is grasped we push on\nit with the force gauge until it slips or the force gauge crosses\n20N. Because InMoov is too fragile to teleoperate and apply\nperturbation forces to, we only test if it can grasp the object\nsecurely and report these results in the table.\nTable V shows LEAP Hand can grasp all objects and can\nperform both many power and precision grasps. While LEAP\nHand and LEAP-C Hand perform similarly, the latter has\nweaker grasps because its MCP side motors cannot be used\nto adjust the grasp. Allegro\u2019s motors are significantly weaker\nwhich leads to objects like the golf ball or the soccer ball\nto easily slip out. Additionally, because its kinematics lacks\nadduction/abduction in an extended position, it cannot perform\nthe tripod grasp for the chopsticks, the cigarette grasp for\nthe wooden cylinder, or the flat hand cupping grasp for the\nwine glass properly. D\u2019Manus can complete extremely strong\npower grasps on larger objects, but its lack of opposability and\ninability to provide resistive force on all 4 sides of the objects\nhurts its performance. Due to its large size, it fails to grasp\nsmaller objects.\nB. Teleoperation from Uncalibrated Human Video\nTeleoperation enables control of high DOF robots in real-time\nvia human feedback. This is also a useful method for collection\ndemonstrations. Because the Allegro Hand\u2019s morphology does\nnot have a human-like MCP joint we must borrow the human-\nto-robot re-targeting method from Robotic Telekinesis [53]\nPick\nRotate\nOpen\nCover\nUncover\nPlace\nPush\nOverall\ntrain\ntest\ntrain\ntest\ntrain\ntest\ntrain\ntest\ntrain\ntest\ntrain\ntest\ntrain\ntest\nAllegro Hand\n0.81\n0.75\n0.89\n0.69\n0.90\n0.80\n0.78\n0.67\n1.00\n0.90\n0.90\n0.70\n1.00\n1.00\n6/14\nLEAP Hand\n0.92\n0.84\n0.89\n0.72\n0.94\n0.76\n0.80\n0.75\n0.96\n0.90\n0.94\n0.75\n1.00\n1.00\n12/14\nTABLE VI: Learning from videos via VideoDex [57]. Hand policies are pretrained on internet videos of humans and finetuned using\nminimal (\u2248 100) teleoperated demos. On this practical use case, LEAP Hand performs better on 12 of 14 {task}\u00d7{train, test} pairs.\n(Fig. 10 (left)), that manually defines key vectors between\npalms and fingertips on both robot vr\ni and human hand vh\ni .\nThese vectors define an energy function E\u03c0 which minimizes\nthe distance between human hand poses (parameterized by the\ntuple (\u03b2h, \u03b8h)) and the robot hand poses q scaled by ci:\nE\u03c0( (\u03b2h, \u03b8h), q ) =\n10\nX\ni=1\n||vh\ni \u2212 (ci \u00b7 vr\ni )||2\n2\n(1)\n[53] trains an MLP HR(.) to implicitly minimize the energy\nfunction described in Equation 1.\nBecause LEAP Hand includes similar joints to a human, we\ncan directly map joint angles between the human and robot.\nIn table IV, we observe that LEAP Hand performs better than\nAllegro Hand on 9/10 of these teleoperated tasks. LEAP Hand\nis easier to control due to its better morphology, accuracy, and\nresponsiveness to hand input. As users in [53] mention, it is\ndifficult to teleoperate a robot hand with an energy function.\nAdditionally, the better opposability and strength of LEAP\nHand allows the operator to reliably grasp objects that are\ndifficult to grasp on the Allegro Hand. While Allegro Hand\nneeded to take breaks to avoid overheating like in [14], LEAP\nHand kept running without a degredation of performance.\nC. Behavior Cloning from Demonstrations\nBehavior cloning enables agents to learn a policy for\na particular task given demonstrations. However, collecting\ndemonstrations for behavior cloning is expensive. We utilize\nvideo from Epic-Kitchens [69] as pre-training for our policy\nusing VideoDex [57] along with NDP [70], see Fig. 10 (right).\nWe also only use demonstrations collected from prior work\n[57] on Allegro Hand and map those to LEAP Hand. LEAP\nHand still outperforms the Allegro Hand in task performance\nas in Table VI. This is because of its consistency and strength\nwhile completing these tasks.\nFig. 11: Sim2Real transfer. Left: Simulated LEAP Hand in Isaac\nGym [71] completing an in-Hand manipulation task. Right: LEAP\nHand completing the same task in the real world. Please see our\nwebsite https://leap-hand.github.io/ for our open source pipeline.\nD. Sim2Real In-Hand Manipulation\nWe perform in-hand rotation of a cube along an axis\nperpendicular to the palm. LEAP Hand can return current\njoint position, velocity, and torque and can be controlled from\nboth torque or position commands. In this case, the robot infers\nthe object pose through the history of observed joint angles\nalone. This is a challenging task since it is contact-rich, and\nthe policy cannot directly observe the pose of the cube. The\npolicy receives joint angles (16 values) from the motors and\noutputs the target joint angles (16) at 20 Hz which is passed\nas position commands to the motors.\nWe choose a GRU [72] architecture for our policy. We first\ngenerate a cache of stable grasps similar to [73]. The policy is\nthen rewarded for turning the cube rrot = clip(\u03c9z, \u22120.25, 0.25),\nwhere \u03c9z is the angular velocity along the vertical axis. We\nadd additional penalties for deviation from the stable grasp\npose, mechanical work done, motor torques, and object linear\nvelocity. The scale for the rotation reward is 1.25. The scales\nfor the penalties are -0.1, -1, -0.1, -0.3 respectively. We train\nPPO [74] with BPPT [75] in IsaacGym [71].\nIn simulation, we compare the average angular velocity of\na cube for different hands and find that LEAP Hand leads to\nfaster rotations (Tab. VII) than Allegro. This is because the joint\nstructure of LEAP Hand allows it to support the cube from the\nsides, whereas since Allegro does not have adduction/abduction\nit must let go of the cube periodically in order to re-orient it.\nVII. CONCLUSION AND FUTURE WORK\nWe introduce LEAP Hand and its core design principles.\nFollowing these principles, we demonstrate that LEAP Hand\ncan perform exceedingly well compared to other hands on\nthe market in strength, grasping, and durability. We show\nits usefulness in a variety of real-world tasks, including\nteleoperation, behavior cloning, and sim2real. We open source\nthe URDF model, 3D CAD files, and a development platform\nwith useful APIs. In future work, we plan to develop and\nintegrate LEAP Hand with low-cost touch sensors.\nHand\nAngular velocity (rad/s)\nAllegro\n0.0828\nLEAP-C Hand\n0.2205\nLEAP Hand (ours)\n0.2288\nTABLE VII: Comparison of angular velocity for the blind in-hand\nrotation of a cube in simulation. Since the Allegro Hand lacks\nabduction/adduction at its extended position, it has low angular\nvelocity. However, LEAP Hand and LEAP-C Hand have this ability\nand have better performance.\nAcknowledgement\nWe thank Shikhar Bahl, Russell Men-\ndonca, Unnat Jain and Jianren Wang for fruitful discussions\nabout the project. KS is supported by NSF Graduate Research\nFellowship under Grant No. DGE2140739. This work is sup-\nported by ONR N00014-22-1-2096 and the DARPA Machine\nCommon Sense grant.\nREFERENCES\n[1] K. Libertus, A. S. Joh, and A. W. Needham, \u201cMotor\ntraining at 3 months affects object exploration 12 months\nlater,\u201d Developmental Science, vol. 19, no. 6, pp. 1058\u2013\n1066, 2016. 1\n[2] T. Bruce, Learning through play, for babies, toddlers and\nyoung children.\nHachette UK, 2012. 1\n[3] E. J. Gibson, \u201cExploratory behavior in the development\nof perceiving, acting, and the acquiring of knowledge,\u201d\nAnnual review of psychology, vol. 39, no. 1, pp. 1\u201342,\n1988. 1\n[4] T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun,\nand M. Hutter, \u201cLearning robust perceptive locomotion\nfor quadrupedal robots in the wild,\u201d Science Robotics,\nvol. 7, no. 62, p. eabk2822, 2022. 1\n[5] A. Agarwal, A. Kumar, J. Malik, and D. Pathak, \u201cLegged\nlocomotion in challenging terrains using egocentric vision,\u201d\nCoRL, 2022. 1\n[6] Bhardwaj, Mohak and Sundaralingam, Balakumar and\nMousavian, Arsalan and Ratliff, Nathan and Fox, Dieter\nand Ramos, Fabio and Boots, Byron, \u201cSTORM: An Inte-\ngrated Framework for Fast Joint-Space Model-Predictive\nControl for Reactive Manipulation,\u201d in Conference on\nRobot Learning (CoRL), 2021. 1\n[7] S.\nDasari,\nJ.\nWang,\nJ.\nHong,\nS.\nBahl,\nY.\nLin,\nA. Wang, A. Thankaraj, K. Chahal, B. Calli, S. Gupta,\nD.\nHeld,\nL.\nPinto,\nD.\nPathak,\nV.\nKumar,\nand\nA. Gupta, \u201cRb2: Robotic manipulation benchmarking\nwith\na\ntwist,\u201d\nin\nProceedings\nof\nthe\nNeural\nInformation Processing Systems Track on Datasets\nand\nBenchmarks,\nJ.\nVanschoren\nand\nS.\nYeung,\nEds.,\nvol.\n1,\n2021.\n[Online].\nAvailable:\nhttps://\ndatasets-benchmarks-proceedings.neurips.cc/paper/2021/\nfile/3988c7f88ebcb58c6ce932b957b6f332-Paper-round2.\npdf 1\n[8] M. Sundermeyer, A. Mousavian, R. Triebel, and D. Fox,\n\u201cContact-graspnet: Efficient 6-dof grasp generation in\ncluttered scenes,\u201d in 2021 IEEE International Conference\non Robotics and Automation (ICRA).\nIEEE, 2021, pp.\n13 438\u201313 444. 1\n[9] T. Chen, J. Xu, and P. Agrawal, \u201cA system for general\nin-hand object re-orientation,\u201d Conference on Robot\nLearning, 2021. 1, 3\n[10] W. Huang, I. Mordatch, P. Abbeel, and D. Pathak, \u201cGen-\neralization in dexterous manipulation via geometry-aware\nmulti-task learning,\u201d arXiv preprint arXiv:2111.03062,\n2021. 1\n[11] O. M. Andrychowicz, B. Baker, M. Chociej, R. Joze-\nfowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert,\nG. Powell, A. Ray et al., \u201cLearning dexterous in-hand\nmanipulation,\u201d The International Journal of Robotics\nResearch, vol. 39, no. 1, pp. 3\u201320, 2020. 1, 2, 3\n[12] L. Sievers, J. Pitz, and B. B\u00a8auml, \u201cLearning purely\ntactile in-hand manipulation with a torque-controlled\nhand,\u201d arXiv preprint arXiv:2204.03698, 2022. 1\n[13] A. S. Morgan, K. Hang, B. Wen, K. Bekris, and A. M.\nDollar, \u201cComplex in-hand manipulation via compliance-\nenabled finger gaiting and multi-modal planning,\u201d IEEE\nRobotics and Automation Letters, vol. 7, no. 2, pp. 4821\u2013\n4828, 2022. 1\n[14] A. Handa, A. Allshire, V. Makoviychuk, A. Petrenko,\nR. Singh, J. Liu, D. Makoviichuk, K. Van Wyk, A. Zhurke-\nvich, B. Sundaralingam et al., \u201cDextreme: Transfer of\nagile in-hand manipulation from simulation to reality,\u201d\narXiv preprint arXiv:2210.13702, 2022. 1, 8\n[15] T. Chen, M. Tippur, S. Wu, V. Kumar, E. Adelson, and\nP. Agrawal, \u201cVisual dexterity: In-hand dexterous manip-\nulation from depth,\u201d arXiv preprint arXiv:2211.11744,\n2022. 1\n[16] \u201cShadowhand,\u201d https://ninjatek.com/shop/edge/. 1, 2, 3, 5\n[17] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin,\nB. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell,\nR. Ribas et al., \u201cSolving rubik\u2019s cube with a robot hand,\u201d\narXiv preprint arXiv:1910.07113, 2019. 1, 2, 3\n[18] \u201cInmoov hand,\u201d https://inmoov.fr/. 1, 2, 6\n[19] D.-H. Lee, J.-H. Park, S.-W. Park, M.-H. Baeg, and J.-H.\nBae, \u201cKitech-hand: A highly dexterous and modularized\nrobotic hand,\u201d IEEE/ASME Transactions on Mechatronics,\nvol. 22, no. 2, pp. 876\u2013887, 2016. 1, 3, 4\n[20] \u201cAllegro\nhand,\u201d\nhttps://www.wonikrobotics.com/\nresearch-robot-hand. 1, 2, 3, 5, 6, 7\n[21] R. Bhirangi, A. DeFranco, J. Adkins, C. Majidi, A. Gupta,\nT. Hellebrekers, and V. Kumar, \u201cAll the feels: A dex-\nterous hand with large area sensing,\u201d arXiv preprint\narXiv:2210.15658, 2022. 2, 6\n[22] \u201cxarm6\nby\nufactory,\u201d\nhttps://www.ufactory.cc/\nxarm-collaborative-robot. 1, 5\n[23] \u201cFranka panda,\u201d https://www.franka.de/. 1, 5\n[24] \u201cUnitree a1,\u201d https://www.unitree.com/en/a1. 1, 5\n[25] \u201cUnitree go1,\u201d https://www.unitree.com/en/go1. 1, 5\n[26] R. R. Ma and A. M. Dollar, \u201cOn dexterity and dexterous\nmanipulation,\u201d in 2011 15th International Conference on\nAdvanced Robotics (ICAR).\nIEEE, 2011, pp. 1\u20137. 2\n[27] V. Kumar, Y. Tassa, T. Erez, and E. Todorov, \u201cReal-time\nbehaviour synthesis for dynamic hand-manipulation,\u201d in\n2014 IEEE International Conference on Robotics and\nAutomation (ICRA).\nIEEE, 2014, pp. 6808\u20136815. 2\n[28] D. Bauer, C. Bauer, A. Lakshmipathy, R. Shu, and N. S.\nPollard, \u201cTowards very low-cost iterative prototyping for\nfully printable dexterous soft robotic hands,\u201d in 2022 IEEE\n5th International Conference on Soft Robotics (RoboSoft).\nIEEE, 2022, pp. 490\u2013497. 2, 6\n[29] F. Faure, C. Duriez, H. Delingette, J. Allard, B. Gilles,\nS. Marchesseau, H. Talbot, H. Courtecuisse, G. Bousquet,\nI. Peterlik, and S. Cotin, SOFA: A Multi-Model Framework\nfor Interactive Physical Simulation.\nBerlin, Heidelberg:\nSpringer Berlin Heidelberg, 2012, pp. 283\u2013321. [Online].\nAvailable: https://doi.org/10.1007/8415 2012 125 2\n[30] C. Duriez and T. Bieze, \u201cSoft robot modeling, simulation\nand control in real-time,\u201d in Soft Robotics: Trends, Appli-\ncations and Challenges: Proceedings of the Soft Robotics\nWeek, April 25-30, 2016, Livorno, Italy.\nSpringer, 2017,\npp. 103\u2013109. 2\n[31] A. Handa, K. Van Wyk, W. Yang, J. Liang, Y.-W. Chao,\nQ. Wan, S. Birchfield, N. Ratliff, and D. Fox, \u201cDexpilot:\nVision-based teleoperation of dexterous robotic hand-\narm system,\u201d in 2020 IEEE International Conference\non Robotics and Automation (ICRA).\nIEEE, 2020, pp.\n9164\u20139170. 2, 3, 5\n[32] B. Sundaralingam and T. Hermans, \u201cRelaxed-rigidity con-\nstraints: kinematic trajectory optimization and collision\navoidance for in-grasp manipulation,\u201d Autonomous Robots,\nvol. 43, no. 2, pp. 469\u2013483, 2019. 2\n[33] A. Sivakumar, K. Shaw, and D. Pathak, \u201cRobotic telekine-\nsis: learning a robotic hand imitator by watching humans\non youtube,\u201d RSS, 2022. 2, 5\n[34] S. P. Arunachalam, S. Silwal, B. Evans, and L. Pinto,\n\u201cDexterous imitation made easy: A learning-based frame-\nwork for efficient dexterous manipulation,\u201d arXiv preprint\narXiv:2203.13251, 2022. 2\n[35] F. O. H. to Multiple Hands: Imitation Learning for Dex-\nterous Manipulation from Single-Camera Teleoperation,\n\u201cQin, yuzhe and su, hao and wang, xiaolong,\u201d 2022. 2\n[36] S. Yuan, A. D. Epps, J. B. Nowak, and J. K. Salisbury,\n\u201cDesign of a roller-based dexterous hand for object\ngrasping and within-hand manipulation,\u201d in 2020 IEEE\nInternational Conference on Robotics and Automation\n(ICRA).\nIEEE, 2020, pp. 8870\u20138876. 2\n[37] \u201cClone robotics,\u201d https://www.clonerobotics.com/. 2\n[38] Y.-J. Kim, J. Yoon, and Y.-W. Sim, \u201cFluid lubricated dex-\nterous finger mechanism for human-like impact absorbing\ncapability,\u201d IEEE Robotics and Automation Letters, vol. 4,\nno. 4, pp. 3971\u20133978, 2019. 2\n[39] U. Kim, D. Jung, H. Jeong, J. Park, H.-M. Jung, J. Cheong,\nH. R. Choi, H. Do, and C. Park, \u201cIntegrated linkage-\ndriven dexterous anthropomorphic robotic hand,\u201d Nature\ncommunications, vol. 12, no. 1, pp. 1\u201313, 2021. 2\n[40] \u201cPneuflex\ntutorial,\u201d\nhttps://www.robotics.tu-berlin.de/\nmenue/software tutorials/pneuflex tutorial/. 3\n[41] \u201cReprap open-source 3d printer,\u201d https://www.reprap.org/\nwiki/RepRap. 3\n[42] \u201cNinjatek ninjaflex edge,\u201d https://ninjatek.com/shop/edge/.\n3\n[43] A. Kumar, Z. Fu, D. Pathak, and J. Malik, \u201cRma: Rapid\nmotor adaptation for legged robots,\u201d RSS, 2021. 3\n[44] W. Huang, I. Mordatch, P. Abbeel, and D. Pathak, \u201cGen-\neralization in dexterous manipulation via geometry-aware\nmulti-task learning,\u201d arXiv preprint arXiv:2111.03062,\n2021. 3\n[45] A. Nair, A. Gupta, M. Dalal, and S. Levine, \u201cAwac:\nAccelerating online reinforcement learning with offline\ndatasets,\u201d arXiv preprint arXiv:2006.09359, 2020. 3\n[46] A. Nagabandi, K. Konolige, S. Levine, and V. Kumar,\n\u201cDeep dynamics models for learning dexterous manipula-\ntion,\u201d in Conference on Robot Learning.\nPMLR, 2020,\npp. 1101\u20131112. 3\n[47] A. Gupta, J. Yu, T. Z. Zhao, V. Kumar, A. Rovinsky,\nK. Xu, T. Devlin, and S. Levine, \u201cReset-free reinforcement\nlearning via multi-task learning: Learning dexterous\nmanipulation behaviors without human intervention,\u201d in\n2021 IEEE International Conference on Robotics and\nAutomation (ICRA).\nIEEE, 2021, pp. 6664\u20136671. 3\n[48] J. Wang, F. Mueller, F. Bernard, S. Sorli, O. Sotnychenko,\nN. Qian, M. A. Otaduy, D. Casas, and C. Theobalt,\n\u201cRgb2hands: real-time tracking of 3d hand interactions\nfrom monocular rgb video,\u201d ACM Transactions on Graph-\nics (TOG), vol. 39, no. 6, pp. 1\u201316, 2020. 3\n[49] A.\nKanazawa,\nM.\nJ.\nBlack,\nD.\nW.\nJacobs,\nand\nJ. Malik, \u201cEnd-to-end recovery of human shape and pose,\u201d\nCoRR, vol. abs/1712.06584, 2017. [Online]. Available:\nhttp://arxiv.org/abs/1712.06584 3\n[50] Y. Feng, V. Choutas, T. Bolkart, D. Tzionas, and M. J.\nBlack, \u201cCollaborative regression of expressive bodies\nusing moderation,\u201d arXiv preprint arXiv:2105.05301,\n2021. 3\n[51] Y. Rong, T. Shiratori, and H. Joo, \u201cFrankmocap: A\nmonocular 3d whole-body pose estimation system via re-\ngression and integration,\u201d in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV)\nWorkshops, October 2021, pp. 1749\u20131759. 3\n[52] J. Romero, D. Tzionas, and M. J. Black, \u201cEmbodied\nhands: Modeling and capturing hands and bodies together,\u201d\nACM Transactions on Graphics, (Proc. SIGGRAPH Asia),\nvol. 36, no. 6, Nov. 2017. 3\n[53] A. Sivakumar, K. Shaw, and D. Pathak, \u201cRobotic telekine-\nsis: Learning a robotic hand imitator by watching humans\non youtube,\u201d 2022. 3, 7, 8\n[54] Y. Qin, Y.-H. Wu, S. Liu, H. Jiang, R. Yang, Y. Fu,\nand X. Wang, \u201cDexmv: Imitation learning for dexter-\nous manipulation from human videos,\u201d arXiv preprint\narXiv:2108.05877, 2021. 3\n[55] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schul-\nman, E. Todorov, and S. Levine, \u201cLearning complex\ndexterous manipulation with deep reinforcement learning\nand demonstrations,\u201d arXiv preprint arXiv:1709.10087,\n2017. 3\n[56] P. Mandikal and K. Grauman, \u201cDexvip: Learning dexter-\nous grasping with human hand pose priors from video,\u201d\nin Conference on Robot Learning (CoRL), 2021. 3\n[57] K. Shaw, S. Bahl, and D. Pathak, \u201cVideoDex: Learning\nDexterity from Internet Videos,\u201d in Conference on Robot\nLearning (CoRL), 2022. 3, 7, 8\n[58] S. Bahl, A. Gupta, and D. Pathak, \u201cHuman-to-robot\nimitation in the wild,\u201d RSS, 2022. 3\n[59] J. Pari, N. M. Shafiullah, S. P. Arunachalam, and L. Pinto,\n\u201cThe surprising effectiveness of representation learning\nfor visual imitation,\u201d 2021. 3\n[60] I. Cerulo, F. Ficuciello, V. Lippiello, and B. Siciliano,\n\u201cTeleoperation of the schunk s5fh under-actuated anthro-\npomorphic hand using human hand motion tracking,\u201d\nRobotics and Autonomous Systems, vol. 89, pp. 75\u201384,\n2017. 4\n[61] J. Liu, F. Feng, Y. C. Nakamura, and N. S. Pollard,\n\u201cA taxonomy of everyday grasps in action,\u201d in 2014\nIEEE-RAS International Conference on Humanoid Robots.\nIEEE, 2014, pp. 573\u2013580. 3, 7\n[62] M.\nChalon,\nM.\nGrebenstein,\nT.\nWimb\u00a8ock,\nand\nG. Hirzinger, \u201cThe thumb: Guidelines for a robotic design,\u201d\nin 2010 IEEE/RSJ international conference on intelligent\nrobots and systems.\nIEEE, 2010, pp. 5886\u20135893. 4\n[63] T. Yoshikawa, \u201cManipulability of robotic mechanisms,\u201d\nThe international journal of Robotics Research, vol. 4,\nno. 2, pp. 3\u20139, 1985. 4\n[64] \u201cUr5,\u201d\nhttps://www.universal-robots.com/products/\nur5-robot/. 5\n[65] \u201cRobotis dynamixels,\u201d https://www.robotis.us/. 5\n[66] \u201cCreality ender 5 3d printer,\u201d https://www.creality.com/.\n6\n[67] \u201cRobotis\ndynamixel,\u201d\nhttps://www.robotis.us/\ndynamixel-xc330-m288-t/. 6\n[68] \u201cManus,\u201d https://www.manus-meta.com, note=Accessed\non 2022-11-28. 7\n[69] D. Damen, H. Doughty, G. M. Farinella, S. Fidler,\nA. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Perrett,\nW. Price, and M. Wray, \u201cScaling egocentric vision:\nThe epic-kitchens dataset,\u201d in European Conference on\nComputer Vision (ECCV), 2018. 8\n[70] S. Bahl, M. Mukadam, A. Gupta, and D. Pathak, \u201cNeural\ndynamic policies for end-to-end sensorimotor learning,\u201d\nin NeurIPS, 2020. 8\n[71] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu,\nK. Storey, M. Macklin, D. Hoeller, N. Rudin, A. Allshire,\nA. Handa et al., \u201cIsaac gym: High performance gpu-based\nphysics simulation for robot learning,\u201d arXiv preprint\narXiv:2108.10470, 2021. 8\n[72] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \u201cEmpirical\nevaluation of gated recurrent neural networks on sequence\nmodeling,\u201d arXiv preprint arXiv:1412.3555, 2014. 8\n[73] H. Qi, A. Kumar, R. Calandra, Y. Ma, and J. Malik, \u201cIn-\nHand Object Rotation via Rapid Motor Adaptation,\u201d in\nConference on Robot Learning (CoRL), 2022. 8\n[74] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and\nO. Klimov, \u201cProximal policy optimization algorithms,\u201d\narXiv preprint arXiv:1707.06347, 2017. 8\n[75] P. J. Werbos, \u201cBackpropagation through time: what it\ndoes and how to do it,\u201d Proceedings of the IEEE, vol. 78,\nno. 10, pp. 1550\u20131560, 1990. 8\n"
  },
  {
    "title": "Natural Language Supervision for General-Purpose Audio Representations",
    "link": "https://arxiv.org/pdf/2309.05767.pdf",
    "upvote": "7",
    "text": "NATURAL LANGUAGE SUPERVISION FOR GENERAL-PURPOSE AUDIO\nREPRESENTATIONS\nBenjamin Elizalde\u2217, Soham Deshmukh\u2217, Huaming Wang\nMicrosoft\n{benjaminm, sdeshmukh, huawang}@microsoft.com\nABSTRACT\nAudio-Language models jointly learn multimodal text\nand audio representations that enable Zero-Shot inference.\nModels rely on the encoders to create powerful representa-\ntions of the input and generalize to multiple tasks ranging\nfrom sounds, music, and speech.\nAlthough models have\nachieved remarkable performance, there is still a gap with\ntask-specific models. In this paper, we propose a Contrastive\nLanguage-Audio Pretraining model that is pretrained with a\ndiverse collection of 4.6M audio-text pairs employing two\ninnovative encoders for Zero-Shot inference. To learn audio\nrepresentations, we trained an audio encoder on 22 audio\ntasks, instead of the standard training of sound event clas-\nsification. To learn language representations, we trained an\nautoregressive decoder-only model instead of the standard\nencoder-only models. Then, the audio and language repre-\nsentations are brought into a joint multimodal space using\nContrastive Learning.\nWe used our encoders to improve\nthe downstream performance by a large margin.\nWe ex-\ntensively evaluated the generalization of our representations\non 26 downstream tasks, the largest in the literature. Our\nmodel achieves state of the art results in several tasks out-\nperforming 4 different models and leading the way towards\ngeneral-purpose audio representations. Code is on GitHub1.\nIndex Terms\u2014 contrastive learning, general purpose au-\ndio representation, zero-shot, language, sounds\n1. INTRODUCTION\nRecent research in the audio domain focuses on learning rep-\nresentations that generalize to a wide range of downstream\ntasks across different domains. The 2021 Holistic Evaluation\nof Audio Representations (HEAR) [1] took a major step in\nthis direction by providing a comprehensive setup to bench-\nmark audio representations. The models were pretrained on\na large dataset \u2013AudioSet [2] (1.7M files)\u2013 using Supervised,\nSelf-Supervised or Unsupervised Learning. All the methods\nhave to undergo additional fine-tuning to use their representa-\ntions on a given downstream task.\nZero-Shot models can be applied to any task directly\nachieving flexibility and generalization.\nOne of the most\nsuccessful type are Contrastive Language-Audio Pretraining\n(CLAP) models that jointly learn multimodal text and audio\n\u2217Equal Contribution\n1https://github.com/microsoft/CLAP\nrepresentations. Authors in [3] introduced a CLAP model\nthat achieved state of the art (SoTA) in 16 downstream tasks.\nSubsequent literature showed that the choice of audio and\ntext encoders are critical to generate powerful representations\nand increase performance across tasks [4, 5, 6, 7]. For exam-\nple, upgrading from CNN to audio transformers (HTSAT) to\nencode audio and from BERT to RoBERTa to encode text.\nAnother conclusion is that scaling up the number of train-\ning pairs improves overall performance.\nHowever, simply\nadding pairs may result in a drop of performance in certain\ndomains and tasks [4, 5, 3, 6]. CLAP\u2019s performance is de-\npendent on the diversity of the text and audio training pairs\nand how noisy they are. Wav2clip [8] and Audioclip [9] used\n200k and 1.7M audio-text pairs respectively from AudioSet,\na dataset annotated for sound events. Authors paired audio\nwith class labels rather than with sentence-level descriptions,\npotentially missing the context and language semantics of de-\nscriptions, but with good Zero-Shot performance in 3 and 9\ntasks respectively. CLAP [3] used 128k pairs but the text\nwere descriptions coming from audio captioning and a web-\nsourced dataset. It was evaluated on 16 tasks and significantly\nimproved over its predecessors. LAION CLAP [4] used a\ncollection of 2.5M pairs, further improving performance in 8\ntasks. Authors later added music and speech-related training\npairs, but performance in sound event classification (ESC50)\ndegraded by an absolute 1%. Wavcaps[6] used 500k pairs, but\ncleaned up the noisy web-sourced descriptions with a Chat-\nGPT language model. Results outperformed the literature in\n8 tasks. Therefore, when scaling up pairs it is essential to ver-\nify performance trade offs by evaluating generalization across\ndifferent domains and tasks.\nIn this paper we make the following contributions. To\nlearn audio representations, we trained an audio encoder on\n22 audio tasks. To learn language representations, we trained\nan autoregressive decoder-only model.\nWe pretrained our\nCLAP model with an unprecedented 4.6 million audio-text\npairs and extensively evaluated the generalization of our\nrepresentations on 26 downstream tasks, the largest in the\nliterature, achieving SoTA results in several.\n2. METHOD\nContrastive Language-Audio Pretraining (Fig 1) jointly trains\nan audio an a text encoder to learn multimodal representations\nwhich can be used for different types of inference.\narXiv:2309.05767v2  [cs.SD]  6 Feb 2024\nFig. 1: CLAP\nlearns audio and a text embeddings that can be\ncompared in a multimodal space. The pretrained encoders can be\nused for Zero-Shot Classification, Text to Audio and Audio to Text\nRetrieval, and Audio Captioning.\n2.1. Contrastive Language-Audio Pretraining\nLet the processed audio be Xa s.t. Xa \u2208 RF \u00d7T where F\nare the number of spectral components (e.g. Mel bins) and T\nare the number of time bins. Let the text be represented by\nXt. Each audio-text pair in a batch of N is represented as\n{Xa, Xt}i where i \u2208 [0, N]. For convenience, we drop the i\nnotation, and henceforth {Xa, Xt} will denote a batch of N.\nFrom the pairs, the audio and text are passed to an au-\ndio encoder fa(.) and a text encoder ft(.) respectively. For a\nbatch of N:\n\u02c6Xa = fa(Xa); \u02c6Xt = ft(Xt)\n(1)\nwhere \u02c6Xa \u2208 RN\u00d7V are the audio representations of dimen-\nsionality V , and \u02c6Xt \u2208 RN\u00d7U are the text representations of\ndimensionality U.\nWe brought audio and text representations, \u02c6Xa and \u02c6Xt,\ninto a joint multimodal space of dimension d by using a learn-\nable projection layer:\nEa = La(Xa); Et = Lt(Xt)\n(2)\nwhere Ea \u2208 RN\u00d7d, Et \u2208 RN\u00d7d, La and Lt are the projec-\ntions for audio and text respectively.\nNow that the audio and text embeddings (Ea, Et) are\ncomparable, we can measure similarity:\nC = \u03c4(Et \u00b7 E\u22a4\na )\n(3)\nwhere \u03c4 is a temperature parameter to scale the range of log-\nits. The similarity matrix C \u2208 RN\u00d7N has N matching pairs\nin the diagonal and N 2 \u2212 N non-matching pairs in the off-\ndiagonal.\nL = 0.5(\u2113text(C) + \u2113audio(C))\n(4)\nwhere \u2113k\n=\n1\nN\nPN\ni=0 log diag(softmax(C)) along text\nand audio axis respectively. We used this symmetric cross-\nentropy loss (L) over the similarity matrix to jointly train the\naudio and text encoders along with their projection layers.\n2.2. Audio and Text Encoders\nAudio Encoder: To process audio, we trained a transformer-\nbased audio encoder (HTSAT [10]) on 22 audio tasks using a\nsimilar method to this paper [11]. We called it HTSAT-22. We\nhypothesized that an encoder trained on multiple audio tasks\nwould improve generalization and thus performance across\ntasks. The method learns an audio encoder and a mapper net-\nwork to prompt a large language model to perform multiple\naudio tasks, such as classification, captioning, retrieval and\naudio Q&A. The architecture is trained essentially as a cap-\ntioning system, where it learns to generate a free-form text\noutput ci in an autoregressive fashion conditioned on the au-\ndio prompt pi. Note that \u03b3 denotes the model\u2019s trainable pa-\nrameters. The loss function is Cross-Entropy:\nL = \u2212\nN\nX\ni=1\nl\nX\nj=1\nlog p\u03b3(ci\nj|pi\n1, ..., pi\n2k, ci\n1, ..., ci\nj\u22121)\n(5)\nText Encoder: To process text, we adapted GPT2 (base\n124M), which is an autoregressive model that has exhibited\nimpressive abilities for text tasks. We addressed the chal-\nlenge \u2013 How to make an autoregressive model produce a\nsentence-level representation? Autoregressive models built\nwith transformer-decoder blocks, take an input text and out-\nput the most likely sequence of words (tokens), one after\nthe other. In contrast, models built with transformer-encoder\nblocks (BERT or RoBERTA) output a sentence-level repre-\nsentation in a continuous space.\nTo make GPT2 output a\nsentence-level representation, we appended the special to-\nken < |endoftext| > at the end of each input text. During\ncontrastive pretraining, we use the representations from this\ntoken as sentence-level representations. This forces the token\nto contain the aggregate information from the text input.\n2.3. Evaluation\nZero-Shot Inference: We used CLAP\u2019s ability to determine\nthe similarity between audio and text. Let\u2019s consider a target\ndataset with C class labels and N test audios. First, we com-\npute CLAP\u2019s audio and text embeddings for N audios and C\nclasses using the pretrained encoders. Second, we compute\nthe cosine similarity between each testing audio and all the\nclass labels. In the case of retrieval, we treat text queries as\nclasses. Each test audio will have as many logits as classes.\nThird, logits are turned into a probability distribution by ap-\nplying softmax for binary or multiclass classification; sigmoid\nfor multilabel classification; and left unaltered for retrieval.\nAudio Captioning: In the architecture of Fig 1, a test audio is\npassed to the pretrained audio encoder, then to a mapper net-\nwork, and then to GPT2 to generate a description. At training\ntime, only the weights of the mapper network are learned with\na captioning loss (Eq.5) and the training split.\nZero-Shot\nScore \u2191\nSound Event Classification \u2191\nVocal Sound\nClassification \u2191\nSurveillance\nSound Classif.\u2191\nAction\nClassification\u2191\nAcoustic Scene\nClassification\u2191\nModel\nAverage\nESC50\nFSD50K\nUS8K\nDCASE17\nTask 4\nVocal\nSound\nSESA\nESC50\nActions\nTUT 2017\nCNN14+BERT\n0.428\n0.826\n0.302\n0.732\n0.300\n0.495\n0.749\n0.495\n0.296\nHTSAT+CLIP\n0.430\n0.813\n0.289\n0.748\n0.277\n0.645\n0.761\n0.442\n0.219\nHTSAT+RoBERTa\n0.431\n0.811\n0.322\n0.757\n0.226\n0.610\n0.745\n0.475\n0.285\nHTSAT+GPT2\n0.435\n0.819\n0.336\n0.767\n0.242\n0.646\n0.644\n0.503\n0.286\nHTSAT-22+RoBERTa\n0.454\n0.879\n0.388\n0.767\n0.209\n0.682\n0.656\n0.481\n0.369\nHTSAT-22+CLIP\n0.469\n0.830\n0.411\n0.791\n0.229\n0.692\n0.723\n0.488\n0.292\nHTSAT-22+GPT2\n0.480\n0.882\n0.403\n0.750\n0.337\n0.692\n0.762\n0.475\n0.317\nMusic Classification \u2191\nInstrument Classification \u2191\nSpeech Emotion\nClassification\u2191\nKWS\u2191\nSpeaker\nCounting\u2191\nModel\nGTZAN\nMusic Speech\nGTZAN\nGenres\nBeijing\nOpera\nNS Instr.\nfamily\nCRE\nMA-D\nRAV\nDESS\nSpeech\nCommands\nLibriCount10\nCNN14+BERT\n1\n0.252\n0.475\n0.295\n0.178\n0.160\n0.106\n0.179\nHTSAT+CLIP\n0.992\n0.156\n0.627\n0.312\n0.208\n0.169\n0.120\n0.113\nHTSAT+RoBERTa\n0.992\n0.178\n0.436\n0.352\n0.263\n0.2\n0.098\n0.149\nHTSAT+GPT2\n1\n0.150\n0.539\n0.322\n0.234\n0.171\n0.139\n0.155\nHTSAT-22+RoBERTa\n1\n0.209\n0.309\n0.402\n0.301\n0.278\n0.129\n0.207\nHTSAT-22+CLIP\n1\n0.280\n0.517\n0.462\n0.275\n0.233\n0.116\n0.094\nHTSAT-22+GPT2\n1\n0.289\n0.487\n0.425\n0.297\n0.217\n0.089\n0.254\nTable 1: Zero-Shot performance on 16 downstream tasks and 119k training pairs. Our proposed encoders (HTSAT-22+GPT2) outperformed\nthe best combinations in the literature. Higher is better for all numbers. The metrics are mAP for FSD50k and ESC50-actions; F1-score for\nDCASE17; all others use Accuracy. Zero-Shot score is the average of the metrics. This is the first comparison of encoders in literature with\n16 tasks, usually only a couple of enocders and a handful of tasks are considered.\n3. EXPERIMENTS\nTraining Datasets.\nCollecting pairs is perhaps the main\nbottleneck of scaling up CLAP models.\nWe gathered the\nlargest collection with 4.6 million audio and text pairs from\ndifferent datasets and web archives.\nThe audios describe\nhuman sounds and activities, environmental sounds, acous-\ntic scenes, music, sound effects, and speech emotion.\nTo\nstudy the effect of encoders in Table 1, we used the same\ntraining sets as CLAP [3].\nUnlike the authors, we did\nnot include the test set of AudioCaps and Clotho, so the\nnumber of pairs was 119k instead of 128k.\nThe training\ndatasets for the 4.6M collection are:\nWavCaps [6], Au-\ndioSet [2], FSD50K [12], Clotho [13], AudioCaps [14],\nMACS [15], WavText5k [5], SoundDesc [16], NSynth [17],\nFMA [18], Mosi [19], Meld [20], Iemocap [21], Mosei [22],\nMSP-Podcast [23], CochlScene [24], LJspeech [25], EpicK-\nitchen [26], Kinectics700 [27], findsounds.com. Details on\nGitHub.\nDownstream Tasks. We used 26 downstream tasks from dif-\nferent domains, several come from HEAR[1]: sound events,\nvocal sounds, surveillance sounds, and acoustic scenes classi-\nfication; audio captioning; retrieval; music, instruments, and\nnote attributes classification; speech emotions and language\nclassification; keyword spotting; and speaker counting. To\nstudy the effect of encoders in Table 1, we used a subset of\n16 tasks.\nPre-processing. We used log Mel spectrogram representa-\ntions of audio with a sampling rate of 44.1 KHz, hop size of\n320 frames, window size 1024 frames, and 64 Mel bins in the\nrange of 50-8000 Hz. During training, each audio clip is ran-\ndomly truncated to a continuous segment of 7 secs, or padded\nif shorter. The batches with pairs are randomly sampled.\nEncoders. For our proposed CLAP model, we used the au-\ndio and text encoders HTSAT-22+GPT2 described in Sec.2.2.\nFor comparison, in Table 1 we used the two best combina-\ntions of encoders in the literature CNN14+BERT and HT-\nSAT+RoBERTa [3, 4, 6]. We also included the text encoder\nfrom CLIP because it was used by different authors [9, 8, 4].\nBoth, the audio and text embeddings are projected into a mul-\ntimodal space with independent learnable projection layers\nwith an output dimension of 1024.\nTraining. We trained by unfreezing both encoders for 40\nepochs, although the overall performance peaked in the first\n10 epochs. We report the performance of the downstream\ntasks corresponding to the epoch that yielded the best Zero-\nShot score (average of all tasks). We hypothesize that the\nmodel corresponding to such epoch will generalize better to\nunseen datasets and serve the community better. It is possible\nthat the performance of each task was higher or lower in a dif-\nferent epoch. Batch size was 1,536. We used Adam Optimiser\nwith an initial learning rate 10\u22123 and reduce the learning rate\non plateau by 10\u22121 with a patience of 15. The temperature\nparameter \u03c4 is learnable and initialised to 0.007.\n4. RESULTS AND DISCUSSION\nThe results comparing different audio and text encoders are in\nTable 1 and the results of our proposed CLAP are in Table 2.\n4.1. Proposed audio and text encoder\nOur proposed encoders HTSAT-22+GPT2 outperformed two\nof the best combination of encoders in the literature, as\nshown in Table 1.\nTo compare overall performance, we\nused Zero-Shot score, which is the average of the metrics\nfrom all 16 tasks.\nHTSAT-22+GPT2 achieved 0.480, an\nabsolute 9% higher than the most common combinations\nHTSAT+RoBERTa and CNN14+BERT with 0.431 and 0.428\nrespectively.\nAll encoder combinations performed better\nSound Event Classification \u2191\nVocal Sound\nClassification \u2191\nSurveillance\nSound Classif.\u2191\nAction\nClassification\u2191\nAcoustic Scene\nClassification\u2191\nModel\nESC50\n[28]\nFSD50K\n[12]\nUS8K\n[29]\nDCASE17\nTask 4 [30]\nAudioSet\n[2]\nVocal\nSound [31]\nSESA\n[32]\nESC50\nActions [33]\nTUT 2017\n[30]\nBenchmark\n0.948 [6]\n0.302 [3]\n0.806 [6]\n0.3 [3]\n0.058 [3]\n0.495 [3]\n0.25\n0.045\n0.296 [3]\nHTSAT-22+GPT2\n0.939\n0.485\n0.823\n0.466\n0.102\n0.8\n0.65\n0.509\n0.538\nMusic Classification \u2191\nInstrument Classification \u2191\nSpeech Emotion\nClassification\u2191\nKWS\u2191\nSpeaker\nCounting\u2191\nModel\nGTZAN\nMusic\nSpeech [1]\nGTZAN\nGenres\n[1]\nNS\nPitch\n[17]\nNS\nVelocity\n[17]\nNS\nQualities\n[17]\nBeijing\nOpera\n[1]\nNS Instr.\nfamily\n[17]\nCRE\nMA-D\n[1]\nRAV\nDESS\n[34]\nSpeech\nCommands\n[1]\nLibri\nCount10\n[1]\nBenchmark\n1 [3]\n0.25 [3]\n0.015\n0.2\n0.1\n0.4746 [3]\n0.09\n0.178 [3]\n0.159 [3]\n0.106 [3]\n0.178 [3]\nHTSAT-22+GPT2\n0.992\n0.584\n0.444\n0.222\n0.489\n0.466\n0.479\n0.3\n0.315\n0.164\n0.246\nAudio Captioning \u2191\nAudio-Text Retrieval \u2191\nText-Audio Retrieval \u2191\nModel\nAudioCaps\n[14]\nClotho\n[13]\nAudioCaps\nR@1\nAudioCaps\nmAP@10\nClotho\nR@1\nClotho\nmAP@10\nAudioCaps\nR@1\nAudioCaps\nmAP@10\nClotho\nR@1\nClotho\nmAP@10\nBenchmark\n0.438[35]\n0.215[35]\n0.517[6]\n0.457 [4]\n0.234[6]\n0.138[4]\n0.397[6]\n0.51[4]\n0.195[6]\n0.204[4]\nHTSAT-22+GPT2\n0.455\n0.271\n0.425\n0.319\n0.229\n0.155\n0.356\n0.51\n0.157\n0.257\nTable 2: Performance on 26 downstream tasks using our proposed encoders and 4.6M training pairs. As the benchmark, we used the best\nnumbers in the literature, when no number was available we used random performance. Higher is better for all tasks. The evaluation metrics\nare mAP for FSD50k, ESC50-Actions, AudioSet, and NS Qualities; F1-score for DCASE17; and SPIDEr for Captioning; all others use\nAccuracy.\nthan random. Although different combinations did better at\ndifferent tasks, none of them excelled at a specific domain.\nOur HTSAT-22 audio encoder is the major contributor to\nperformance improvement. HTSAT-22 is pretrained on 22\naudio tasks in contrast to HTSAT which is pretrained only\non sound event classification. Hence, suggesting that gen-\nerating pretraining on multiple audio tasks can improve the\nrepresentations from the audio encoder. Comparing HTSAT-\n22+GPT2 to HTSAT+GPT2 evidenced major improvements\nsuch as LibriCount10 (absolute 10%), NS Instrument (abso-\nlute 7%) and ESC50 (absolute 6%).\nThe proposed GPT2 autoregressive model improves upon\nthe popular RoBERTa. Using GPT2 with either HTSAT or\nHTSAT-22 yielded the best performance over the other text\nencoders. We hypothesize that the improvement comes from\ntwo reasons.\nFirst, GPT2 has a larger vocabulary of 50k\ntokens compared to BERT and RoBERTa with 30k.\nSec-\nond, our modified GPT2 autoregressive predicts tokens till <\n|endoftext| > used for sentence-level representation. This\nacts as self-supervision and forces the model to learn and put\nemphasis on the ordering of words.\n4.2. Scaling proposed CLAP architecture\nOur CLAP model established new Zero-Shot SoTA on most\nof the 26 downstream tasks as shown in Table 2, outperform-\ning 4 different SoTA models. To benchmark our model, we\nused the best numbers in the literature coming from different\nmodels. When no number was available, we used random per-\nformance. In some cases, performance improvement is more\nthan double the benchmark literature. Some highlights are\nMusic Genres with 58.4% acc. vs 25%, Vocal Sounds with\n80% acc. vs 49.5%, Acoustic Scenes with 53.8% acc. vs\n29.6%. Some downstream tasks do not constitute a true Zero-\nShot setup as the audio files in the training set were part of the\n4.6M pairs (see Sec.3). For instance, FSD50k audio and web\ndescriptions were used in training but not the class labels. We\ndid not fine-tune CLAP encoders for any downstream task.\nWe only fine-tune the audio encoder for ESC50 and were able\nto improve performance from our previous CLAP model from\n96.70% to 98.25% accuracy, thus establishing a new SoTA.\n4.3. Generalization and individual domain performance\nAdding diversity and scaling the audio-text pairs in training\npresents a trade-off that increases performance in some tasks\nbut decreases it in others. As expected, adding training pairs\nthat resemble the domain from a given task helps, hence diver-\nsity is essential for generalization. For example, CLAP [3] did\nnot include emotion recognition training pairs and achieved\n17.1% acc. in RAVDESS and 23.4% in CREMAD. We added\nemotion-related pairs and improved accuracy to 31.5% and\n30% respectively. Nonetheless, more pairs can cause a distri-\nbution shift, creating a mismatch between training and some\ntesting data. For example, our model achieved a slightly lower\nscore than a model [6] trained with 500k pairs on ESC50\n(94.8% vs 93.9% acc.). Another example is with GTZAN\nMusic vs Speech, where a model [3] with 128k pairs achieved\n100% acc. over ours with 99.2%. Even our model in Table 1\nachieved 100% acc with 119k pairs. We should expect that\nas we add training pairs, performance across tasks will vary.\nHence, zero-shot models should be evaluated across different\ndomains and tasks with focus on generalization rather than on\noverfitting to specific tasks.\nAudio-Text (A-T) and Text-Audio (T-A) Retrieval perfor-\nmance fell short of the benchmark. We measured the tasks\nwith mAP@10, which is the ranking metric of IEEE DCASE,\nand R@1. Our model outperformed the literature in terms of\nmAP@10 for Clotho (A-T: 0.155 vs 0.138 and T-A: 0.257 vs\n0.204), and struggled only with A-T AudioCaps (A-T: 0.319\nvs 0.457 and T-A: 0.51 vs 0.51). Both datasets are sensitive\nto out-of-domain training data and adding training pairs did\nnot translate into an improvement. This was demonstrated by\nauthors in [5] who unsuccessfully tried to add 39k files from\nSounDesc or authors in [4] with 500k from Wavcaps or au-\nthors in [6] with 1.7M from AudioSet.\n5. CONCLUSION\nWe introduced a CLAP model with our proposed encoders\nand 4.6M training pairs. Zero-shot models should be eval-\nuated across different tasks with a focus on generalization\nrather than on overfitting to specific tasks.\nWe evaluated\nCLAP on 26 tasks and established SoTA on most of them,\nleading the way in general-purpose audio representations.\n6. REFERENCES\n[1] Joseph Turian, Jordie Shier, et al.,\n\u201cHEAR: Holistic\nEvaluation of Audio Representations,\u201d in NeurIPS 2021\nCompetitions and Demonstrations Track, 2022.\n[2] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman,\nAren Jansen, Wade Lawrence, et al., \u201cAudio set: An\nontology and human-labeled dataset for audio events,\u201d\nin IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2017.\n[3] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Is-\nmail, and Huaming Wang, \u201cClap learning audio con-\ncepts from natural language supervision,\u201d in IEEE In-\nternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2023.\n[4] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Tay-\nlor Berg-Kirkpatrick, and Shlomo Dubnov, \u201cLarge-scale\ncontrastive language-audio pretraining with feature fu-\nsion and keyword-to-caption augmentation,\u201d\narXiv\npreprint arXiv:2211.06687, 2022.\n[5] Soham Deshmukh, Benjamin Elizalde, and Huaming\nWang,\n\u201cAudio Retrieval with WavText5K and CLAP\nTraining,\u201d in Proc. INTERSPEECH 2023, 2023.\n[6] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang\nKong, Tom Ko, Chengqi Zhao, Mark Plumbley, et al.,\n\u201cWavcaps:\nA chatgpt-assisted weakly-labelled audio\ncaptioning dataset for audio-language multimodal re-\nsearch,\u201d arXiv preprint arXiv:2303.17395, 2023.\n[7] Benjamin Elizalde, Shuayb Zarar, and Bhiksha Raj,\n\u201cCross modal audio search and retrieval with joint\nembeddings based on text and audio,\u201d\nin ICASSP\n2019-2019 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP). IEEE,\n2019, pp. 4095\u20134099.\n[8] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar,\net al., \u201cWav2clip: Learning robust audio representations\nfrom clip,\u201d in IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), 2022.\n[9] Andrey Guzhov, Federico Raue, J\u00a8orn Hees, and Andreas\nDengel, \u201cAudioclip: Extending clip to image, text and\naudio,\u201d in IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2022.\n[10] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor\nBerg-Kirkpatrick, et al., \u201cHts-at: A hierarchical token-\nsemantic audio transformer for sound classification and\ndetection,\u201d in IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), 2022.\n[11] Soham Deshmukh, Benjamin Elizalde, Rita Singh, and\nHuaming Wang, \u201cPengi: An audio language model for\naudio tasks,\u201d arXiv preprint arXiv:2305.11834, 2023.\n[12] Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic\nFont, and Xavier Serra, \u201cFsd50k: An open dataset of\nhuman-labeled sound events,\u201d IEEE/ACM Transactions\non Audio, Speech, and Language Processing, 2022.\n[13] Konstantinos Drossos, Samuel Lipping, and Tuomas\nVirtanen,\n\u201cClotho: an audio captioning dataset,\u201d\nin\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2020.\n[14] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,\nand Gunhee Kim,\n\u201cAudioCaps: Generating Captions\nfor Audios in The Wild,\u201d in NAACL-HLT, 2019.\n[15] Irene Mart\u00b4\u0131n-Morat\u00b4o and Annamaria Mesaros, \u201cWhat\nis the ground truth? reliability of multi-annotator data\nfor audio tagging,\u201d in 29th European Signal Processing\nConference (EUSIPCO), 2021.\n[16] A. Sophia Koepke, Andreea-Maria Oncescu, Joao Hen-\nriques, Zeynep Akata, and Samuel Albanie,\n\u201cAudio\nretrieval with natural language queries: A benchmark\nstudy,\u201d IEEE Transactions on Multimedia, 2022.\n[17] Jesse Engel, Cinjon Resnick, Adam Roberts, Sander\nDieleman, et al.,\n\u201cNeural audio synthesis of musi-\ncal notes with wavenet autoencoders,\u201d in International\nConference on Machine Learning. PMLR, 2017.\n[18] Micha\u00a8el Defferrard, Kirell Benzi, Pierre Vandergheynst,\nand Xavier Bresson, \u201cFma: A dataset for music analy-\nsis,\u201d in 18th International Society for Music Information\nRetrieval Conference, 2017.\n[19] Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-\nPhilippe Morency, \u201cMosi: multimodal corpus of sen-\ntiment intensity and subjectivity analysis in online opin-\nion videos,\u201d arXiv preprint arXiv:1606.06259, 2016.\n[20] Soujanya Poria, Devamanyu Hazarika, Navonil Ma-\njumder, Gautam Naik, et al.,\n\u201cMeld: A multimodal\nmulti-party dataset for emotion recognition in conver-\nsations,\u201d in Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, 2019.\n[21] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe\nKazemzadeh, Emily Mower, Samuel Kim, et al.,\n\u201cIemocap: Interactive emotional dyadic motion capture\ndatabase,\u201d Language resources and evaluation, 2008.\n[22] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Po-\nria, Erik Cambria, and Louis-Philippe Morency, \u201cMulti-\nmodal language analysis in the wild: Cmu-mosei dataset\nand interpretable dynamic fusion graph,\u201d in Proceedings\nof the 56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), 2018.\n[23] Reza Lotfian and Carlos Busso, \u201cBuilding naturalistic\nemotionally balanced speech corpus by retrieving emo-\ntional speech from existing podcast recordings,\u201d IEEE\nTransactions on Affective Computing, 2017.\n[24] Il-Young Jeong and Jeongsoo Park, \u201cCochlscene: Ac-\nquisition of acoustic scene data using crowdsourcing,\u201d\nin Asia-Pacific Signal and Information Processing As-\nsociation Annual Summit and Conference, 2022.\n[25] Keith\nIto\nand\nLinda\nJohnson,\n\u201cThe\nlj\nspeech\ndataset,\u201d\nhttps://keithito.com/\nLJ-Speech-Dataset/, 2017.\n[26] Dima\nDamen,\nHazel\nDoughty,\nGiovanni\nMaria\nFarinella, , Antonino Furnari, Jian Ma, Kazakos, et al.,\n\u201cRescaling egocentric vision:\nCollection,\npipeline\nand challenges for epic-kitchens-100,\u201d\nInternational\nJournal of Computer Vision (IJCV), 2022.\n[27] Lucas Smaira, Jo\u02dcao Carreira, Eric Noland, Ellen Clancy,\nAmy Wu, and Andrew Zisserman, \u201cA short note on the\nkinetics-700-2020 human action dataset,\u201d 2020.\n[28] Karol J. Piczak,\n\u201cESC: Dataset for Environmental\nSound Classification,\u201d in Proceedings of the 23rd An-\nnual ACM Conference on Multimedia. pp. 1015\u20131018,\nACM Press.\n[29] Justin Salamon, Christopher Jacoby, et al., \u201cA dataset\nand taxonomy for urban sound research,\u201d in 22nd ACM\ninternational conference on Multimedia, 2014.\n[30] Annamaria Mesaros, Aleksandr Diment, Benjamin\nElizalde, Toni Heittola, et al., \u201cSound event detection\nin the dcase 2017 challenge,\u201d IEEE/ACM Transactions\non Audio, Speech, and Language Processing, 2019.\n[31] Yuan Gong, Jin Yu, and James Glass, \u201cVocalsound: A\ndataset for improving human vocal sounds recognition,\u201d\nin IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2022.\n[32] Tito Spadini, \u201cSound events for surveillance applica-\ntions,\u201d Oct. 2019.\n[33] Benjamin Elizalde, Radu Revutchi, Samarjit Das, Bhik-\nsha Raj, Ian Lane, and Laurie M Heller, \u201cIdentifying\nactions for sound event classification,\u201d in 2021 IEEE\nWorkshop on Applications of Signal Processing to Au-\ndio and Acoustics (WASPAA). IEEE, 2021, pp. 26\u201330.\n[34] Steven R. Livingstone and Frank A. Russo, \u201cThe Ry-\nerson Audio-Visual Database of Emotional Speech and\nSong (RAVDESS),\u201d Apr. 2018.\n[35] Minkyu Kim, Kim Sung-Bin, and Tae-Hyun Oh, \u201cPre-\nfix tuning for automated audio captioning,\u201d in IEEE In-\nternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2023.\n"
  },
  {
    "title": "Learning Disentangled Avatars with Hybrid 3D Representations",
    "link": "https://arxiv.org/pdf/2309.06441.pdf",
    "upvote": "4",
    "text": "Learning Disentangled Avatars with Hybrid 3D Representations\nYAO FENG, Max Planck Institute for Intelligent Systems, Germany & ETH Z\u00fcrich, Switzerland\nWEIYANG LIU, Max Planck Institute for Intelligent Systems, Germany & University of Cambridge, United Kingdom\nTIMO BOLKART, Max Planck Institute for Intelligent Systems, Germany\nJINLONG YANG, Max Planck Institute for Intelligent Systems, Germany\nMARC POLLEFEYS, ETH Z\u00fcrich, Switzerland\nMICHAEL J. BLACK, Max Planck Institute for Intelligent Systems, Germany\nProject Page: yfeng95.github.io/delta\nDisentangled face and hair\nInput monocular video\n \nHair transfer & reposing\nInput monocular video\n \nDisentangled body and clothing\nClothing transfer & reposing\n (a) Application I (head): \nDisentangling face and hair from monocular video\n (b) Application II (body):\nDisentangling body and clothing from monocular video\n (c) Application III (head + body):\nSimultaneous hair and clothing transfer \nInput single image\nHair & clothing transfer\nFig. 1. (a) Disentangled human head: DELTA outputs disentangled mesh-based face and NeRF-based hair given a monocular video input. (b) Disentangled\nhuman body: DELTA outputs disentangled mesh-based body and NeRF-based clothing given a monocular video input. (c) With the disentangled clothing and\nhair learned by DELTA, we can easily transfer any hair and clothing to a human body estimated from a single image.\nAbstract: Tremendous efforts have been made to learn animatable and\nphotorealistic human avatars. Towards this end, both explicit and implicit 3D\nrepresentations are heavily studied for a holistic modeling and capture of the\nwhole human (e.g., body, clothing, face and hair), but neither representation\nis an optimal choice in terms of representation efficacy since different parts of\nthe human avatar have different modeling desiderata. For example, meshes\nare generally not suitable for modeling clothing and hair. Motivated by\nthis, we present Disentangled Avatars (DELTA), which models humans\nwith hybrid explicit-implicit 3D representations. DELTA takes a monocular\nRGB video as input, and produces a human avatar with separate body and\nclothing/hair layers. Specifically, we demonstrate two important applications\nfor DELTA. For the first one, we consider the disentanglement of the human\nbody and clothing and in the second, we disentangle the face and hair.\nTo do so, DELTA represents the body or face with an explicit mesh-based\nparametric 3D model and the clothing or hair with an implicit neural radiance\nfield. To make this possible, we design an end-to-end differentiable renderer\nthat integrates meshes into volumetric rendering, enabling DELTA to learn\ndirectly from monocular videos without any 3D supervision. Finally, we show\nthat how these two applications can be easily combined to model full-body\navatars, such that the hair, face, body and clothing can be fully disentangled\nyet jointly rendered. Such a disentanglement enables hair and clothing\ntransfer to arbitrary body shapes. We empirically validate the effectiveness\nof DELTA\u2019s disentanglement by demonstrating its promising performance\non disentangled reconstruction, virtual clothing try-on and hairstyle transfer.\nTo facilitate future research, we also release an open-sourced pipeline for\nthe study of hybrid human avatar modeling.\nTechnical Report, September 2023\n1\nINTRODUCTION\nRecent years have witnessed an unparalleled surge in the utiliza-\ntion of 3D human reconstruction and reenactment in numerous\napplications such as virtual and augmented reality, telepresence,\ngames, and movies. It is of broad interest to create personal avatars\nfrom readily available setups (e.g., monocular videos). It is desirable\nin practice for the avatars to be photorealistic, 3D-consistent, an-\nimatable, easily editable and generalizable to novel poses. These\ncharacteristics call for a faithful disentanglement and modeling of\ndifferent semantic components of the avatar (e.g., face and hair for\nhead, body and clothing for whole body). Therefore, how to disen-\ntangle human avatars while yielding accurate reconstructions is of\ngreat significance and remains an open challenge.\nExisting methods for learning 3D human avatars can be roughly\ncategorized into explicit ones and implicit ones. Explicit methods\n(e.g., [Feng et al. 2021b; Grassal et al. 2022; Khakhulin et al. 2022;\nSanyal et al. 2019] for head, [Choutas et al. 2020; Feng et al. 2021a;\nKanazawa et al. 2018; Kolotouros et al. 2019; Pavlakos et al. 2019;\nZanfir et al. 2021] for body) typically use triangular meshes as repre-\nsentation, and the reconstruction heavily relies on statistical shape\npriors, such as 3D morphable models for head [Blanz and Vetter\n1999; Egger et al. 2020; Li et al. 2017] and 3D parametric models\nfor body [Anguelov et al. 2005; Joo et al. 2018; Loper et al. 2015;\nOsman et al. 2020; Pavlakos et al. 2019; Xu et al. 2020]. Implicit\nmethods usually encode the 3D geometry either with implicit sur-\nfaces (e.g., signed distance fields (SDF)) [Jiang et al. 2022; Saito et al.\narXiv:2309.06441v1  [cs.CV]  12 Sep 2023\n2\n\u2022\nYao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, and Michael J. Black\n2019; Zheng et al. 2022] or with volumetric representation [Gafni\net al. 2021; Gao et al. 2022; Peng et al. 2021b]. Both explicit and\nimplicit methods use a single 3D representation to model different\nparts of the avatar, which ignores the representation efficacy and\ntherefore can be sub-optimal. For example, triangular meshes are\nan efficient representation for faces and minimally clothed body,\nfor which statistical template priors are available, but meshes are\ngenerally a poor representation for hair or clothing since they can\nbe inefficient to capture the underlying geometry. On the other\nhand, implicit representation renders high-fidelity 2D views but it\nis nontrivial to animate and usually can not generalize to unseen\nposes and expressions. Since no single 3D representation is perfect,\nwhy not use different one for different part of the avatar? Motivated\nby this, we propose DisEntangLed avaTAr (DELTA), which models\nface and body with explicit triangular meshes, and models hair and\nclothing with an implicit neural radiance field (NeRF) [Mildenhall\net al. 2020]. The intuition behind such a design is in two folds. First,\nboth faces and bodies have regular topological structures and live in\na low-dimensional subspace [Basri and Jacobs 2003; Li et al. 2009].\nIt is therefore a well-motivated choice to represent the face or body\ngeometry with mesh templates. Second, hair consists of countless\nfreely deformed thin strands, which hinders triangular meshes to\nbe a suitable representation. Clothing (e.g., dresses) also consists of\ncomplex topological structures and has a diverse set of styles. Due\nto the complex nature of hair and clothing, it is highly difficult to\naccurately model their surface geometry, which renders NeRF an\narguably better choice of representation.\nThe effectiveness of hybrid 3D representation has already found\nits traces in human-scene reconstruction [Pavlakos et al. 2022],\nclothed body modeling [Feng et al. 2022], and human eye model-\ning [Li et al. 2022]. For example, [Pavlakos et al. 2022] reconstructs\nthe static scene with a NeRF which excels at representing fine-\ngrained scene details, and the people inside with a SMPL [Loper\net al. 2015] representation which is good at body pose recovery.\nDespite modeling different subjects under different context, the\nessence of hybrid representation is the adoption of heterogeneous\n3D representations such that each representation can be made the\nbest use of. Extending our prior work [Feng et al. 2022], DELTA is\nthe first method to demonstrate the power of hybrid representation\nfor learning human avatars (including face, body, hair and clothing).\nSpecifically, we instantiate the idea of DELTA in two capture settings.\nFirst, we consider the disentangled reconstruction of human head\nwhere the head (and upper shoulder) is represented by a parametric\nmesh model (i.e., FLAME [Li et al. 2017] and SMPL-X [Pavlakos\net al. 2019]) and the hair is represented by a NeRF. Unlike exist-\ning works [Gafni et al. 2021; Grassal et al. 2022; Zheng et al. 2022],\nDELTA additionally reconstruct the upper body (e.g., shoulder),\nsuch that people with long hair can be better captured. Second, we\nconsider the disentangled reconstruction of human body where the\nbody is represented by a parametric mesh model (i.e., SMPL-X) and\nthe clothing is represented by a NeRF. Combining the disentangled\ncapture of both human head and body, we demonstrate that both\nhair and clothing can be simultaneously transferred to arbitrary\nreconstructed human body. See Figure 1 for an illustration.\nDistinct from existing work [Li et al. 2022; Pavlakos et al. 2022],\nat the very heart of DELTA is our novel mesh-integrated volumetric\nrenderer, which not only drives the disentanglement of different\nparts of the avatar (i.e., face, hair, body, clothing), but also enables the\nend-to-end differentiable learning directly from monocular videos\nwithout any 3D supervision. We expect the idea of hybrid 3D rep-\nresentation to be quite general, and DELTA aims to demonstrate\nthe power of hybrid 3D representation by bringing together meshes\nand NeRFs in modeling human avatars.\nWhy is disentanglement so important for learning avatars? We\nanswer this question by listing some key desiderata for photore-\nalistic avatar creation. First, the pose-dependent factors should be\ndisentangled from the appearance such that the captured avatar\ncan be easily reusable in new environments. Second, disentangling\nthe human body, hair, and clothing is crucial to accurately model\ntheir respective dynamics, since the motion dynamics of the human\nbody, hair, and clothing are completely distinct from each other.\nMoreover, modeling the interaction between body and hair/clothing\nalso requires an accurate disentanglement. Such a disentanglement\nbecomes even more important when performing physical simulation\non the reconstructed avatar. Third, human body, hair and clothing\nhave totally different material and physical properties, which results\nin different lighting phenomena. In order to construct realistic and\ngeneralizable avatars, human body and hair/clothing have to be\ndisentangled and modeled separately. Towards the goal of learning\ndisentangled avatars, our contributions are listed below:\n\u2022 By substantially extending our previous work [Feng et al. 2022],\nwe propose the disentangled avatar that models face/body and\nhair/clothing with a hybrid 3D representation. Such an hybrid\nrepresentation marries the statistical prior from mesh surfaces\nand the representation flexibility from implicit functions. DELTA\nis one of the first methods that uses a hybrid explicit-implicit\nrepresentation to reconstruct high-fidelity disentangled avatars.\n\u2022 We design a novel differentiable volumetric rendering method\nthat incorporates meshes into volumetric rendering.\n\u2022 The framework of DELTA is fully differentiable and end-to-end\ntrainable. It is trained on a monocular video (e.g., from web cam-\neras) without requiring any 3D supervision.\n\u2022 For the face and body, DELTA delivers high-fidelity details while\nbeing able to effortlessly reposed. For the hair and clothing region,\nDELTA yields realistic hair and clothing reconstruction owing to\nthe powerful implicit NeRF representation.\n\u2022 We emphasize that the major contribution of DELTA is to serve\nas a demonstration to showcase the potentials of hybrid 3D rep-\nresentation in modeling human avatars.\n2\nRELATED WORK\n2.1\nHead Avatar Creation\nExplicit head avatars. Explicit head avatars are typically based on\nexplicit 3D representations (e.g., triangular meshes). 3D morphable\nmodels (3DMM) [Blanz and Vetter 1999], which are obtained from\na population of 3D head scans [Egger et al. 2020], are widely used\nas a stronger statistical prior to represent the geometry of faces.\nBuilt upon 3DMM, many improved variants have been proposed,\nincluding multi-linear models for shape and expression [Cao et al.\n2013; Vlasic et al. 2006], full-head models [Dai et al. 2020; Li et al.\nLearning Disentangled Avatars with Hybrid 3D Representations\n\u2022\n3\n2017; Ploumpis et al. 2020], and deep nonlinear models [Ranjan\net al. 2018; Tran and Liu 2018]. Besides, morphable models also\nprovide a linear model for textures [Aldrian and Smith 2010; Blanz\nand Vetter 1999, 2003; Paysan et al. 2009]. 3DMM and its variants\ncan be used to reconstruct faces through an optimization proce-\ndure [Gecer et al. 2019; Romdhani and Vetter 2005; Sch\u00f6nborn et al.\n2017; Thies et al. 2016] or learning-based estimation [Deng et al.\n2019; Dib et al. 2021; Feng et al. 2021b; Khakhulin et al. 2022; Lattas\net al. 2020; Li et al. 2018; Sanyal et al. 2019; Shang et al. 2020; Tewari\net al. 2019, 2018, 2017; Wen et al. 2021]. Besides 3DMM template\npriors, other priors (e.g., symmetry [Liu et al. 2022b; Wu et al. 2020],\ncausality [Liu et al. 2022b; Wen et al. 2021], identity [Cole et al. 2017;\nFeng et al. 2021b]) are also considered in 3D face reconstruction.\nDespite producing good coarse facial geometry, these methods are\nusually unable to reconstruct fine-grained facial details and the en-\ntire head (e.g., hair). Some methods [Alldieck et al. 2018a; Cao et al.\n2015; Feng et al. 2021b] use mesh displacements to reconstruct fine\ndetails such as wrinkles, producing fine-grained geometry. Follow-\ning a similar spirit, Grassal et al. [2022] use a geometry refinement\nnetwork that learns a pose-dependent offset function for geometry\ncorrections, and produces photorealistic outputs under novel views.\nPointAvatar [Zheng et al. 2023b] uses a deformable point-based\nrepresentation to reconstruct human heads from videos. Unlike\nprevious work, DELTA captures the head avatar with disentangled\nface and hair components. DELTA adopts the explicit mesh-based\nrepresentation to model the face region, making it easily animat-\nable. For the hair, we utilize an implicit NeRF-based representation,\ncapable of accommodating various hair types. With this approach,\nwe can utilize models tailored for faces and hair, and it also unlocks\npotential applications like hairstyle transfer.\nImplicit head avatars. Implicit models normally encode the 3D\nhead avatar with NeRF-based representation [Mildenhall et al. 2020;\nM\u00fcller et al. 2022] or implicit surface functions [Chen and Zhang\n2019; Kellnhofer et al. 2021; Mescheder et al. 2019; Park et al. 2019;\nYariv et al. 2020]. NeRF-based methods have been explored for 3D\nface modeling from images or videos [Chan et al. 2021; Gafni et al.\n2021; Park et al. 2021; Wang et al. 2021]. Gafni et al. [2021] recon-\nstruct an animatable NeRF from a single monocular video, which\nis conditioned on the expression code from a 3DMM. Gao et al.\n[2022] propose a NeRF-based linear blending representation where\nexpression is encoded by multi-level voxel fields. AvatarMAV [Xu\net al. 2023a] uses neural voxel fields to represent motion and ap-\npearance to achieve fast head reconstruction. LatentAvatar [Xu et al.\n2023b] reconstructs a NeRF-based head avatar that is driven by\nlatent expression codes, and these expression codes are learned in\nan end-to-end and self-supervised manner without the tracking\nof templates. However, NeRF-based head representations gener-\nally suffer from poor 3D geometry and struggles to generalize to\nunseen poses/expressions. Approaches utilizing implicit surface\nfunctions generally provide better geometry for faces. Yenamandra\net al. [2021] proposes an implicit morphable face model that dis-\nentangles texture and geometry. Zheng et al. [2022] parameterize\nthe head with implicit surface functions in the canonical space, and\nrepresents the expression- and pose-dependent deformations via\nlearned blendshapes and skinning fields. Ramon et al. [2021] use\nan optimization-based approach to estimate the signed distance\nfunction (SDF) of a full head from a few images, and this optimiza-\ntion is constrained by a pre-trained 3D head SDF model. In contrast\nto both explicit and implicit head avatars that use a holistic 3D\nrepresentation, DELTA is the first method that adopts a hybrid\nexplicit-implicit 3D representation to separately model face and\nhair. DELTA marries the strong controllability of the mesh-based\nface and the high-fidelity rendering of the NeRF-based hair.\n2.2\nFull Body Avatar Creation\nExplicit Body Avatars. The 3D surface of a human body is typi-\ncally represented by a learned statistical 3D model using an explicit\nmesh representation [Anguelov et al. 2005; Joo et al. 2018; Loper\net al. 2015; Osman et al. 2020; Pavlakos et al. 2019]. The paramet-\nric models [Loper et al. 2015; Pavlakos et al. 2019] can produce a\nminimal clothed body when the shape parameters are provided. Nu-\nmerous optimization and regression methods have been proposed\nto compute 3D shape and pose parameters from images, videos, and\nscans. See [Liu et al. 2022a; Tian et al. 2022] for recent surveys. We\nfocus on methods that capture full-body pose and shape, including\nthe hands and facial expressions [Choutas et al. 2020; Feng et al.\n2021a; Pavlakos et al. 2019; Rong et al. 2021; Xiang et al. 2019; Xu\net al. 2020; Zhou et al. 2021]. Such methods, however, do not capture\nhair, clothing, or anything that deviates the body. Also, they rarely\nrecover texture information, due to the large geometric discrepancy\nbetween the clothed human in the image and captured minimal\nclothed body mesh. Some methods choose to model body along\nwith clothing. However, clothing is more complex than the body in\nterms of geometry, non-rigid deformation, and appearance, making\nthe capture of clothing from images challenging. Explicit ways to\ncapture clothing often use additional vertex offsets relative to the\nbody mesh [Alldieck et al. 2019a, 2018a,b, 2019b; Jin et al. 2020;\nLazova et al. 2019; Ma et al. 2020; Xiu et al. 2023]. While such an\napproach generally works well for tight clothing, it still struggles to\ncapture loose clothing like skirts and dresses.\nImplicit Body Avatars. Recently, implicit representations have\ngained traction in modeling the human body [Alldieck et al. 2021;\nXu et al. 2020]. Correspondingly, methods have been developed to\nestimate implicit body shape from images [Xu et al. 2020]. However,\nsimilar to explicit body model [Pavlakos et al. 2019], they only model\nminimal clothed body. When it comes to clothed avatars, recent\nmethods are leveraging implicit representations to handle more com-\nplex variations in clothing styles, aiding in the recovery of clothing\nstructures. For instance, [He et al. 2021; Huang et al. 2020; Saito et al.\n2019, 2020; Xiu et al. 2022; Zheng et al. 2021] extract pixel-aligned\nspatial features from images and map them to an implicit shape\nrepresentation. To animate the captured non-parametric clothed hu-\nmans, Yang et al. [2021] predict skeleton and skinning weights from\nimages to drive the representation. Corona et al. [2021] represent\nclothing layers with deep unsigned distance functions [Chibane\net al. 2020], and learn the clothing style and clothing cut space\nwith an auto-decoder. Once trained, the clothing latent code can\nbe optimized to match image observations, but it produces over-\nsmooth results without detailed wrinkles. PoseVocab [Li et al. 2023b]\nmodels NeRF-based human avatars by learning pose encoding. Al-\nthough such implicit models can capture various clothing styles\n4\n\u2022\nYao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, and Michael J. Black\nmuch better than explicit mesh-based approaches, faces and hands\nare usually poorly recovered due to the lack of a strong prior on the\nhuman body. In addition, such approaches typically require a large\nset of manually cleaned 3D scans as training data. Recently, various\nmethods recover 3D clothed humans directly from multi-view or\nmonocular RGB videos [Chen et al. 2021b; Jiang et al. 2022; Liu et al.\n2021; Peng et al. 2021a, 2022, 2021b; Qiu et al. 2023; Su et al. 2021;\nWeng et al. 2022]. They optimize avatars from image information\nusing implicit shape rendering [Liu et al. 2020; Niemeyer et al. 2020;\nYariv et al. 2021, 2020] or volume rendering [Mildenhall et al. 2020],\nno 3D scans are needed. Although these approaches demonstrate\nimpressive performance, hand gestures and facial expressions are\ndifficult to capture and animate due to the lack of model expressive-\nness and controllability. AvatarReX [Zheng et al. 2023c] learns a\nNeRF-based full-body avatar with disentangled modeling of face,\nbody and hands, but the clothing is still entangled with body.\nUnlike prior methods, we view clothing as a separate layer above\nthe body and combine explicit body models and implicit clothing to\nleverage the advantages of both. The mesh-based body model allows\nus to create human shapes with detailed components (e.g., hands)\nand to control the body (e.g., expressions and hand articulations).\nWith implicit representation, we can capture a variety of clothing\nusing images, without the need for 3D scans. Moreover, the disen-\ntangled modeling of explicit body and implicit clothing facilitates\nseamless clothing transfer, enabling applications like virtual try-ons.\n2.3\nOther Related Work\nHybrid 3D representation. The potentials of hybrid 3D represen-\ntation have also been demonstrated in other 3D reconstruction tasks.\nPavlakos et al. [2022] represent the background static scene as a\nNeRF and the people inside as SMPL models. Li et al. [2022] model\nthe eye-ball surface with an explicit parametric surface model and\nrepresents the periocular region and the interior of the eye with\ndeformable volumetric representations. Hybrid explicit-implicit rep-\nresentation has also been explored in transparent object reconstruc-\ntion [Xu et al. 2022] and haptic rendering [Kim et al. 2004].\nHair modeling. How to represent hair is a long-standing problem\nin human modeling [Ward et al. 2007]. Strand-based modeling is\nwidely adopted to model human hair [Beeler et al. 2012; Chai et al.\n2013, 2012; Herrera et al. 2012; Hu et al. 2014; Luo et al. 2012, 2013;\nNam et al. 2019; Rosu et al. 2022; Sun et al. 2021; Yang et al. 2019;\nZhang et al. 2017; Zhang and Zheng 2019; Zhou et al. 2018]. Zheng\net al. [2023a] recover the strand-based 3D hair from an intermedi-\nate representation that consists of a strand map and a depth map.\nNeural Haircut [Sklyarova et al. 2023] uses a two-stage coarse-to-\nfine optimization to reconstruct the strand-level hair. More recently,\nvolumetric representation is also applied to perform hair model-\ning [Saito et al. 2018; Wang et al. 2022]. Their primary focus is on\nhair reconstruction, and they typically utilize head-tracked meshes\nfrom multi-view images [Rosu et al. 2022; Wang et al. 2021, 2022]\nor reconstruct faces from videos with stationary heads [Sklyarova\net al. 2023]. None of these methods, however, are designed to learn\nfaces from monocular videos with dynamic facial expressions. In\ncontrast, our approach distinguishes itself by learning both facial\nfeatures and hair from monocular videos, even when the head is\nmoving. Since the primary objective of DELTA is to disentangle the\nrepresentation of faces and hair rather than accurately capturing\nhair geometry, we employ a NeRF representation for hair modeling.\nThe disentangled capture of face, upper body and hair is a necessary\nstep before one can perform high-fidelity hair modeling, so DELTA\nalso serves as a stepping stone for future work that combines better\nhair modeling in creating disentangled head avatars.\nGarment reconstruction. The task of reconstructing 3D garments\nfrom images or videos has proven to be a complex challenge [Dan\u011b\u0159ek\net al. 2017; Hong et al. 2021; Li et al. 2021; Qiu et al. 2023; Su et al.\n2022; Zhao et al. 2021; Zhu et al. 2020]. This complexity arises from\nthe wide diversity in clothing topologies. To tackle this, existing\nmethods often rely on either clothing template meshes or implicit\nsurface functions. Typically, these approaches demand access to 3D\ndata. Many approaches employ training data produced by physics-\nbased simulations [Bertiche et al. 2020; Patel et al. 2020; Santesteban\net al. 2019; Vidaurre et al. 2020] or require template meshes fit to\n3D scans [Chen et al. 2021a; Halimi et al. 2022; Pons-Moll et al.\n2017; Tiwari et al. 2020; Xiang et al. 2021]. Jiang et al. [2020] train\na mesh-based multi-clothing model on 3D datasets with various\nclothing styles. Zhu et al. [2020] introduce a adaptable template that\nallows for encoding clothing with diverse topologies within a single\nmesh template. Then during inference, a trained network produces\nthe 3D clothing as a separate mesh-based layer by recognizing and\npredicting the clothing style from an image. Zhu et al. [2022] fit\ntemplate meshes to non-parametric 3D reconstructions. While these\nmethods recover garments from images, they are limited in visual\nfidelity, as they do not capture clothing appearance. Additionally,\nmethods with such predefined clothing style templates can not eas-\nily handle the real clothing variations, limiting their applications.\nIn contrast, Corona et al. [2021] represent clothing layers with deep\nunsigned distance functions [Chibane et al. 2020], and learn the\nclothing style and clothing cut space with an auto-decoder. Once\ntrained, the clothing latent code can be optimized to match image\nobservations, but it produces over-smooth results without detailed\nwrinkles. Instead, DELTA models the clothing layer with a neural ra-\ndiance field, and optimizes the body and clothing layer from scratch\ninstead of the latent space of a learned clothing model. Therefore,\nDELTA produces avatars with higher visual fidelity (see Section 5).\n3\nDELTA: LEARNING DISENTANGLED AVATARS\nGiven a monocular video, DELTA reconstructs a head (or body)\navatar where head/body and hair/clothing are fully disentangled.\nOnce the avatar is built, we can animate it with novel poses and\nchange the hairstyle and clothing effortlessly. Because the way that\nDELTA reconstructs head and body shares many similarities, we\nsimplify the description by referring the face or body as avatar\ninterior and the hair or clothing as avatar exterior.\n3.1\nHybrid Explicit-Implicit 3D Representations\nPrevious work on face and body modeling [Bi et al. 2021; Grassal et al.\n2022; Li et al. 2017; Lombardi et al. 2018; Loper et al. 2015; Pavlakos\net al. 2019] has demonstrated that both human faces and bodies can\nbe accurately modeled by mesh-based representations. In the light\nof these encouraging results, we choose mesh as the representation\nLearning Disentangled Avatars with Hybrid 3D Representations\n\u2022\n5\nCanonicalization (Sec. 3.2)\nColored face\nColored posed face\nCanonical NeRF hair\nPosed NeRF hair\nCanonical 3D space\nPosed 3D space\nRendered image\nForeground image\n& hair mask \nImage space\nLosses (Sec. 3.4)\nOutput\nInput\nCanonicalization (Sec. 3.2)\nSMPL-X\nSMPL-X w. offset\nColored body\nColored posed body\nCanonical NeRF clothing\nPosed NeRF clothing\nCanonical 3D space\nPosed 3D space\nRendered image\nForeground image \n& clothing mask\nImage space\nLosses (Sec. 3.4)\nOutput\nInput\nSMPL-X\nSMPL-X w. offset\nFd\nEstimated\nMesh Interior\nEstimated\nNeRF Exterior\nEstimated\nMesh Interior\nEstimated\nNeRF Exterior\nFt\nTp(\u03b2,\u03b8,\u03c8,O)\nFh\nTp(\u03b2,\u03b8,\u03c8,O)\nFd\nFt\nFh\n(a) DELTA for disentangled body and clothing reconstrcution\n(b) DELTA for disentangled head and hair reconstrcution\nMesh-integrated\nvolume rendering\n(Sec. 3.3)\nMesh-integrated\nvolume rendering\n(Sec. 3.3)\nFig. 2. DELTA takes a monocular RGB video and clothing/hair segmentation masks as input, and outputs a human avatar with separate body and clothing/hair\nlayers. Green letters indicate optimizable modules or parameters.\nfor the face and body. Specifically, we use SMPL-X [Pavlakos et al.\n2019] to make full use of the human geometry priors. When it comes\nto representing hair and clothing, it remains an open problem which\nrepresentation works the best. Because of the complex geometry\nof hair and clothing, we propose to model both hair and clothing\nwith NeRF [Mildenhall et al. 2020] \u2013 a more flexible and expressive\nimplicit representation. Distinct from meshes, NeRF is agnostic to\nthe style, geometry and topology of hair and clothing.\nExplicit avatar interior by SMPL-X. SMPL-X is an expressive\nbody model with detailed face shape and expressions. A subject\u2019s\nface and body with neutral expression in the rest pose is defined as\n\ud835\udc47\ud835\udc43 (\ud835\udf37, \ud835\udf3d, \ud835\udf4d) = \u00af\ud835\udc7b + \ud835\udc35\ud835\udc46 (\ud835\udf37; S) + \ud835\udc35\ud835\udc43 (\ud835\udf3d; P) + \ud835\udc35\ud835\udc38 (\ud835\udf4d; E),\n(1)\nwhere \u00af\ud835\udc7b \u2208 R\ud835\udc5b\ud835\udc63\u00d73 is a template of body shape in the rest pose,\n\ud835\udf37 \u2208 R|\ud835\udf37 | is the body identity parameters, and \ud835\udc35\ud835\udc46 (\ud835\udf37; S) : R|\ud835\udf37 | \u2192\nR\ud835\udc5b\ud835\udc63\u00d73 are the identity blend shapes. More specifically, \ud835\udc35\ud835\udc46 (\ud835\udf37; S) =\n\u00cd|\ud835\udf37 |\n\ud835\udc56=1 \ud835\udf37\ud835\udc56S\ud835\udc56 where \ud835\udf37\ud835\udc56 is the \ud835\udc56-th linear coefficient and S\ud835\udc56 is the \ud835\udc56-th\northonormal principle component. \ud835\udf3d \u2208 R3\ud835\udc5b\ud835\udc58+3 denotes the pose\nparameters, and \ud835\udf4d \u2208 R|\ud835\udf4d| denotes the facial expression parameters.\nSimilar to the shape space S, \ud835\udc35\ud835\udc43 (\ud835\udf3d; P) : R|\ud835\udf3d | \u2192 R\ud835\udc5b\ud835\udc63\u00d73 denotes the\npose blend shapes (P is the pose space), and \ud835\udc35\ud835\udc38 (\ud835\udf4d; E) : R|\ud835\udf4d| \u2192\nR\ud835\udc5b\ud835\udc63\u00d73 denotes the expression blend shapes from the SMPL-X model\n(E is the expression space). To increase the flexibility of SMPL-X, we\nadd additional vertex offsets \ud835\udc76 := {\ud835\udc39\ud835\udc51 (\ud835\udc951), \ud835\udc39\ud835\udc51 (\ud835\udc952), \u00b7 \u00b7 \u00b7 , \ud835\udc39\ud835\udc51 (\ud835\udc95\ud835\udc5b\ud835\udc63)}\u22a4 \u2208\nR\ud835\udc5b\ud835\udc63\u00d73 in the canonical space. The offset is modeled by a vertex-wise\nimplicit function \ud835\udc39\ud835\udc51 :\ud835\udc95 \u2192\ud835\udc90, which predicts an offset \ud835\udc90 \u2208 R3 for the\nvertex \ud835\udc95 \u2208 R3 in the rest template. Therefore, we augment the body\nshape with the following set of offsets:\n\u02dc\ud835\udc47\ud835\udc43 (\ud835\udf37, \ud835\udf3d, \ud835\udf4d, \ud835\udc76) = \ud835\udc47\ud835\udc43 (\ud835\udf37, \ud835\udf3d, \ud835\udf4d) + \ud835\udc76.\n(2)\nThe albedo is represented by an implicit function \ud835\udc39\ud835\udc61 : \ud835\udc95 \u2192 \ud835\udc84mesh\nwhich predicts the RGB color \ud835\udc84mesh of each given vertex \ud835\udc95 on the\nsurface. Specifically, we sample vertex \ud835\udc95 from the template mesh \u00af\ud835\udc7b\nif the video is under uniform lighting. For more complex lighting\nconditions, in order to better model the texture, we sample \ud835\udc95 from\nthe surface after the pose deformation. More details can be found in\nSection 5.2. To capture more geometric details, we use an upsampled\nversion of SMPL-X with \ud835\udc5b\ud835\udc63 = 38, 703 vertices and \ud835\udc5b\ud835\udc61 = 77, 336\nfaces [Feng et al. 2022]. Similar to [Grassal et al. 2022], we also add\nadditional faces inside the mouth region for head avatar modeling.\nImplicit avatar exterior by NeRF. Based on NeRF [Mildenhall\net al. 2020], we define the avatar exterior (hair or clothing) in the\n6\n\u2022\nYao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, and Michael J. Black\ncanonical 3D space as an implicit function \ud835\udc39\u210e : \ud835\udc99\ud835\udc50 \u2192 (\ud835\udc84nerf, \ud835\udf0e)\nwhich can be parameterized by a multi-layer perceptron (MLP).\n\ud835\udc84nerf represents the RGB color. Given a query point \ud835\udc99\ud835\udc50 \u2208 R3 in the\ncanonical space, the implicit NeRF-based function \ud835\udc39\u210e outputs an\nemitted RGB color \ud835\udc84nerf and a volume density \ud835\udf0e.\n3.2\nPose-dependent Deformation\nExplicit avatar interior deformation. Given the monocular video,\nwe need to model the movement of this subject. Since our avatar\ninterior model is based on SMPL-X, it provides a good way to capture\nthe pose deformation and facial expressions. For each frame of\ngiven video, we estimate the parameters of shape \ud835\udf3d \u2208 R|\ud835\udf3d | and\nexpression \ud835\udf4d \u2208 R|\ud835\udf4d|. Then we can deform the head/body to the\nobservation pose using the linear blend skinning function (i.e., LBS).\nThe deformation for the explicit SMPL-X mesh model is modeled by\na differential function \ud835\udc40(\ud835\udf37, \ud835\udf3d, \ud835\udf4d, \ud835\udc76) that outputs a 3D human body\nmesh (\ud835\udc7d, \ud835\udc6d) where \ud835\udc7d \u2208 R\ud835\udc5b\ud835\udc63\u00d73 is a set of \ud835\udc5b\ud835\udc63 vertices and \ud835\udc6d \u2208 R\ud835\udc5b\ud835\udc61 \u00d73\nis a set of \ud835\udc5b\ud835\udc61 faces with a fixed topology:\n\ud835\udc40(\ud835\udf37, \ud835\udf3d, \ud835\udf4d, \ud835\udc76) = LBS( \u02dc\ud835\udc47\ud835\udc43 (\ud835\udf37, \ud835\udf3d, \ud835\udf4d, \ud835\udc76), \ud835\udc3d (\ud835\udf37), \ud835\udf3d,\ud835\udc7e),\n(3)\nin which \ud835\udc7e \u2208R\ud835\udc5b\ud835\udc58 \u00d7\ud835\udc5b\ud835\udc63 is the blend skinning weights used in the LBS\nfunction. \ud835\udc3d (\ud835\udf37) \u2208R\ud835\udc5b\ud835\udc58 \u00d73 is a function of body shape [Pavlakos et al.\n2019], representing the shape-dependent joints. Given a template\nvertex \ud835\udc95\ud835\udc56, the vertex \ud835\udc97\ud835\udc56 can be computed with simple linear transfor-\nmation. Specifically, the forward vertex-wise deformation can be\nwritten as the following equation in the homogeneous coordinates:\n\ud835\udc97\ud835\udc56\n|{z}\nPosed vertex\n=\n\ud835\udc5b\ud835\udc58\n\u2211\ufe01\n\ud835\udc58=1\n\ud835\udc7e\ud835\udc58,\ud835\udc56\ud835\udc3a\ud835\udc58 (\ud835\udf3d, \ud835\udc3d (\ud835\udf37)) \u00b7\n\u0014\ud835\udc70\n\ud835\udc90\ud835\udc56 + \ud835\udc83\ud835\udc56\n0\n1\n\u0015\n|                                        {z                                        }\n\ud835\udc40\ud835\udc56 (\ud835\udf37,\ud835\udf3d,\ud835\udf4d,\ud835\udc76): Deformation to the posed space\n\u00b7\n\ud835\udc95\ud835\udc56\n|{z}\nTemplate vertex\n,\nwhere \ud835\udc40\ud835\udc56 (\ud835\udf37, \ud835\udf3d, \ud835\udf4d, \ud835\udc76) \u2208 R4\u00d74 is the deformation function of template\nvertex \ud835\udc95\ud835\udc56. \ud835\udc7e\ud835\udc58,\ud835\udc56 is the (\ud835\udc58,\ud835\udc56)-th element of the blend weight matrix\n\ud835\udc7e, \ud835\udc3a\ud835\udc58 (\ud835\udf3d, \ud835\udc3d (\ud835\udf37)) \u2208 R4\u00d74 is the world transformation of the \ud835\udc58-th\njoint and \ud835\udc83\ud835\udc56 is the \ud835\udc56-th vertex of the sum of all blend shapes \ud835\udc69 :=\n\ud835\udc35\ud835\udc46 (\ud835\udf37) + \ud835\udc35\ud835\udc43 (\ud835\udf3d) + \ud835\udc35\ud835\udc38 (\ud835\udf4d). We denote \ud835\udc7d as the vertex set of the posed\navatar (\ud835\udc97\ud835\udc56 \u2208 \ud835\udc7d). Both \ud835\udc97\ud835\udc56 and \ud835\udc95\ud835\udc56 are the homogeneous coordinates\nwhen applying this deformation function.\nImplicit avatar exterior deformation. Aiming to learn the NeRF-\nbased clothing/hair representation in the canonical space, we need\nto deform from the posed space to the canonical space. Therefore,\nwe perform backward deformation on the top of the explicit body\nskinning. Given a query point \ud835\udc99\ud835\udc5d in the posed space (from the\nobserved video frame), we first find the nearest \ud835\udc58 points on the\nbody surface \ud835\udc40. Then we use the weighted backward skinning func-\ntion to transform the posed point \ud835\udc99\ud835\udc5d to the canonical space (i.e.,\n\ud835\udc65\ud835\udc50). To model more accurate clothing/hair movement and defor-\nmation, we further learn a pose-dependent deformation function\n\ud835\udc39\ud835\udc52 : (\ud835\udc99\ud835\udc50, \ud835\udc97\ud835\udc5d\n\ud835\udc5b(\ud835\udc99\ud835\udc5d )) \u2208 R6 \u2192 \u0394\ud835\udc99\ud835\udc50 \u2208 R3, where \ud835\udc99\ud835\udc5d denotes a point in ob-\nservation space and \ud835\udc5b(\ud835\udc99\ud835\udc5d) is the set of indices of the nearest points\nto \ud835\udc99\ud835\udc5d in \ud835\udc7d\ud835\udc5d which denotes the posed body meshes in \ud835\udc40(0, \ud835\udf3d, 0, 0).\n\ud835\udc39\ud835\udc52 aims to predict the detailed non-rigid deformation for the query\npoint in the canonical space. Then the residual \u0394\ud835\udc99\ud835\udc50 is added back\nto \ud835\udc99\ud835\udc50, and the displaced point \u02dc\ud835\udc99\ud835\udc50 = \ud835\udc99\ud835\udc50 + \u0394\ud835\udc99\ud835\udc50 is fed to the canonical\nNeRF model \ud835\udc39\u210e in order to compensate the exterior clothing/hair\nRay distance\n\ud835\udc61!\nRay mesh \nintersection\n\ud835\udc61\"\nRay 1\nRay 2\nRay 3\n\ud835\udf48\n\ud835\udc95\ud835\udc87\n\ud835\udc95\ud835\udc8f\n\ud835\udf48\n\ud835\udc95\ud835\udc87\n\ud835\udc95\ud835\udc8f\nRay 1\nRay 2\n\ud835\udc61\"\n\ud835\udc61\"\n\ud835\udc61!\n\ud835\udc61!\n\ud835\udc95\ud835\udc8f\nRay 3\n\ud835\udf48\n\ud835\udc95\ud835\udc87\nFig. 3. Illustration of mesh-integrated volume rendering.\ndeformation in the observation space. Specifically, we have the in-\nverse blend skinning mapping from the observation space to the\nposed space as the following transformation:\n\ud835\udc99\ud835\udc50\n|{z}\nCanonical\nvertex\n=\n\u2211\ufe01\n\ud835\udc97\ud835\udc56 \u2208\ud835\udc5b(\ud835\udc99\ud835\udc5d )\n\ud835\udefc\ud835\udc56 (\ud835\udc99\ud835\udc5d)\u00b7\ud835\udc40\ud835\udc56 (0, \ud835\udf3d, 0, 0)\u00b7\ud835\udc40\u22121\n\ud835\udc56\n(\ud835\udf37, \ud835\udf3d, \ud835\udf4d, \ud835\udc76)\n|                                                      {z                                                      }\nTransformation to the canonical space\n\u00b7\n\ud835\udc99\ud835\udc5d\n|{z}\nObserved\nvertex\n,\nwhere \ud835\udefc\ud835\udc56 is the parameter that balances the importance:\n\ud835\udefc\ud835\udc56 (\ud835\udc99\ud835\udc5d) = 1\n\ud835\udc4d exp\n\u0012\n\u2212 1\n2\ud835\udf0e2 \u00b7 \u2225\ud835\udc99\ud835\udc5d \u2212 \ud835\udc97\ud835\udc56 \u2225 \u00b7 \u2225\ud835\udc98\ud835\udc5b\ud835\udc5b(\ud835\udc99\ud835\udc5d ) \u2212 \ud835\udc98\ud835\udc56 \u2225\n\u0013\n.\nWhere \ud835\udc4d := \u00cd\n\ud835\udc97\ud835\udc56 \u2208\ud835\udc5b(\ud835\udc99\ud835\udc5d ) \ud835\udefc\ud835\udc56 (\ud835\udc99\ud835\udc5d) is a normalizing coefficient,\ud835\udc98\ud835\udc56 \u2208 R\ud835\udc5b\ud835\udc58\nis the blend weights of \ud835\udc97\ud835\udc56, \ud835\udf0e is a constant and \ud835\udc5b\ud835\udc5b(\ud835\udc99\ud835\udc5d) denotes the\nindex of the nearest point of \ud835\udc99\ud835\udc5d in \ud835\udc7d\ud835\udc5d.\n3.3\nMesh-integrated Volume Rendering\nCamera model. We simplify the problem by using a scaled ortho-\ngraphic camera model p = {\ud835\udc60, \ud835\udc95\u22a4}\u22a4 where \ud835\udc60 \u2208 R is the isotropic\nscale and \ud835\udc95 \u2208 R2 denotes the translation.\nMesh rasterization. With the geometry parameters (\ud835\udf37, \ud835\udf3d, \ud835\udf4d), the\nvertex offsets \ud835\udc76, the RGB color \ud835\udc84mesh of vertices in the upsampled\nSMPL-X template and the camera parameters p, we render the col-\nored mesh into an image with R\ud835\udc5a(\ud835\udc40(\ud835\udf37, \ud835\udf3d, \ud835\udf4d, \ud835\udc39\ud835\udc51), \ud835\udc84mesh, p) where\nR\ud835\udc5a denotes the differentiable rasterizer function.\nMesh-integrated volume rendering. Finally we discuss how to\ntake mesh into consideration while performing volumetric render-\ning. The basic idea is that the camera ray will stop when it intersects\nwith the mesh in the 3D space. Given a camera ray \ud835\udc93(\ud835\udc61) = \ud835\udc92 + \ud835\udc61\ud835\udc85\nwith center \ud835\udc92 \u2208 R3 and direction \ud835\udc85 \u2208 R3. The rendering interval is\n\ud835\udc61 \u2208 [\ud835\udc61\ud835\udc5b,\ud835\udc61\ud835\udc53 ] \u2282 R (near and far bounds). Unlike previous work, we\nintegrate the body model, \ud835\udc40(\ud835\udf37, \ud835\udf3d, \ud835\udf4d,\ud835\udc42), into the volumetric render-\ning. Specifically, if \ud835\udc93(\ud835\udc61) intersects \ud835\udc40, we set the \ud835\udc61\ud835\udc53 such that \ud835\udc93(\ud835\udc61\ud835\udc53 )\nis the intersection point with \ud835\udc40. In this case, we use the mesh color\ninstead of the NeRF color \ud835\udc84nerf(\ud835\udc93(\ud835\udc61\ud835\udc53 )) (see Figure 3). More formally,\nthe expected color of the camera ray \ud835\udc5f is defined as\n\ud835\udc84(\ud835\udc93) =\n\u222b \ud835\udc61\ud835\udc53\n\ud835\udc61\ud835\udc5b\n\ud835\udc84nerf(\ud835\udc93(\ud835\udc61)) \u00b7 \ud835\udc47 (\ud835\udc61) \u00b7 \ud835\udf0e(\ud835\udc93(\ud835\udc61)) + 1s(\ud835\udc93) \u00b7 \ud835\udeff(\ud835\udc61 \u2212 \ud835\udc61\ud835\udc53 ) \u00b7 \ud835\udc84mesh\ud835\udc51\ud835\udc61,\nwhere 1s(\ud835\udc93) is the indicator function for whether the ray intersects\nthe mesh surface (1 if true, 0 otherwise), \ud835\udeff(\u00b7) denotes the Dirac delta\nfunction and \ud835\udc47 (\ud835\udc61) = exp(\u2212\n\u222b \ud835\udc61\n\ud835\udc61\ud835\udc5b \ud835\udf0e(\ud835\udc93(\ud835\udc60))\ud835\udc51\ud835\udc60). When 1s(\ud835\udc93) is true, we\nLearning Disentangled Avatars with Hybrid 3D Representations\n\u2022\n7\nset the \ud835\udc61\ud835\udc53 such that \ud835\udc95(\ud835\udc61\ud835\udc53 ) is the intersection point with the SMPL-\nX mesh \ud835\udc40. \ud835\udc84mesh is the vertex color of the intersected mesh. We\napproximate the integral with evenly split \ud835\udc5b\ud835\udc4f bins in practice:\n\ud835\udc84(\ud835\udc93) = \u00001 \u2212\n\ud835\udc5b\ud835\udc4f \u22121\n\u2211\ufe01\n\ud835\udc58=1\n\ud835\udc47\ud835\udc58\n\u00001 \u2212 exp(\u2212\ud835\udf0e\ud835\udc58\u0394\ud835\udc58)\u0001\u0001 \u00b7 \u0000(1 \u2212 1s(\ud835\udc93))\ud835\udc84nerf(\ud835\udc93\ud835\udc50\n\ud835\udc5b\ud835\udc4f )\n+ 1s(\ud835\udc93) \u00b7 \ud835\udc84mesh(\ud835\udc93\ud835\udc5b\ud835\udc4f )\u0001 +\n\ud835\udc5b\ud835\udc4f \u22121\n\u2211\ufe01\n\ud835\udc57=1\n\ud835\udc47\ud835\udc57\n\u00001 \u2212 exp(\u2212\ud835\udf0e\ud835\udc57\u0394\ud835\udc57)\u0001\ud835\udc84nerf(\ud835\udc93\ud835\udc50\n\ud835\udc57 ),\nwhere we define \ud835\udc47\ud835\udc57 = exp(\u2212 \u00cd\ud835\udc57\u22121\n\ud835\udc5e=1 \ud835\udf0e\ud835\udc57\u0394\ud835\udc57). \ud835\udc93\ud835\udc57 is sampled from the\n\ud835\udc57-th bin along the camera ray \ud835\udc93. \ud835\udc93\ud835\udc50\n\ud835\udc56 is the corresponding canonical\npoint for the observed point \ud835\udc93\ud835\udc56.\n3.4\nObjective Function\nOverall objective function. Given a sequence of\ud835\udc5b\ud835\udc53 images, \ud835\udc3c\ud835\udc53 (1 \u2264\n\ud835\udc53 \u2264 \ud835\udc5b\ud835\udc53 ), we optimize \ud835\udf37 and the weights of the MLPs \ud835\udc39\ud835\udc51, \ud835\udc39\u210e, \ud835\udc39\ud835\udc61, \ud835\udc39\ud835\udc52\njointly across the entire sequence, and \ud835\udf3d\ud835\udc53 and p\ud835\udc53 per frame. We use\nthe following overall objective function:\nL = Lrecon + Lext + Lint + Lreg,\n(4)\nwith reconstruction loss Lrecon, avatar exterior loss Lext, avatar\ninterior loss Lint (Lbody\nint\nor Lface\nint ) and regularization Lreg. For sim-\nplicity, we omit the frame index \ud835\udc53 and the optimization arguments\nwhenever there is no ambiguity. For videos, the final objective func-\ntion is the average over all frames.\nReconstruction loss. We minimize the difference between the\nrendered image and the input image with the following objective:\nLrecon = \ud835\udf06pixel \u00b7 L\ud835\udeff (R\ud835\udc63 \u2212 \ud835\udc3c) + \ud835\udf06semantic \u00b7 Lsemantic(R\ud835\udc63, \ud835\udc3c),\n(5)\nwhere L\ud835\udeff is the Huber loss [Huber 1964] that penalizes the pixel-\nlevel difference. Lsemantic is used to regularize the semantic differ-\nence. More specifically, we use an ID-MRF loss [Wang et al. 2018]\nLmrf as Lsemantic for reconstructing the body avatar, and an percep-\ntual loss [Johnson et al. 2016] Lper as Lsemantic for reconstructing\nthe head avatar. While the Huber loss focuses on the overall recon-\nstruction, the semantic loss allows us to reconstruct more details as\npreviously shown by Feng et al. [2021b].\nAvatar exterior loss Only minimizing the reconstruction error\nLrecon results in a NeRF that models the entire avatar including the\nbody/face regions. Our goal is to only capture exterior components\nsuch as clothing or hair using \ud835\udc39\u210e. To achieve this, we employ a seg-\nmentation mask to explicitly limit the space within which the NeRF\ndensity can be. Given a segmentation mask \ud835\udc46\ud835\udc52, which is represented\nby 1 for every exterior pixel (clothing or hair) and 0 elsewhere, we\nminimize the following exterior loss:\n\ud835\udc3fext = \ud835\udf06ext \u2225\ud835\udc46\ud835\udc63 \u2212 \ud835\udc46\ud835\udc52 \u22251,1 ,\n(6)\nwith the rendered NeRF mask \ud835\udc46\ud835\udc63, which is obtained by sampling\nrays for all image pixels and computing per ray\n\ud835\udc94\ud835\udc97(\ud835\udc93) =\n\ud835\udc5b\ud835\udc4f \u22121\n\u2211\ufe01\n\ud835\udc58=1\n\ud835\udc47\ud835\udc58\n\u00001 \u2212 exp(\u2212\ud835\udf0e\ud835\udc58\u0394\ud835\udc58)\u0001.\n(7)\nMinimizing \ud835\udc3fext ensures that the aggregated density across rays\n(excluding the far bound) outside of clothing or hair is 0. Therefore,\nonly the intended exterior region is captured by the NeRF model.\nAvatar interior loss. To further disentangle the avatar interior\nand exterior, we need to ensure that the interior mesh model does\nnot capture any exterior variation. To this end, we define a few\nadditional loss functions based on prior knowledge.\nFirst, the interior mesh should match the masked image. Given\na binary mask \ud835\udc46 of the entire avatar (1 for inside, 0 elsewhere), we\nminimize the difference between the silhouette of the rendered body\n(denoted by R\ud835\udc60\ud835\udc5a(\ud835\udc40, p)) and the given mask as\nLsilhouette = \ud835\udf06silhouetteL\ud835\udeff (R\ud835\udc60\n\ud835\udc5a(\ud835\udc40, p) \u2212 \ud835\udc46).\n(8)\nSecond, the interior mesh should match visible avatar interior (e.g.,\nfor reconstructing the body, the body mesh should match the visible\nbody region). Only optimizing Lsilhouette results in meshes that\nalso fit the avatar exterior (e.g., clothing or hair). This is undesired\nespecially for loose clothing or long hair, and also leads to visible\nartifacts when transferring clothing between subjects. Instead, given\na binary mask \ud835\udc46\ud835\udc4f of the visible body parts (1 for body parts, 0\nelsewhere), we minimize the following part-based silhouette loss\nLint-mask = \ud835\udf06int-maskL\ud835\udeff (\ud835\udc46\ud835\udc4f \u2299 R\ud835\udc60\n\ud835\udc5a(\ud835\udc40, p) \u2212 \ud835\udc46\ud835\udc4f),\n(9)\nand a part-based photometric loss\nLskin = \ud835\udf06skinL\ud835\udeff (\ud835\udc46\ud835\udc4f \u2299 (R\ud835\udc5a(\ud835\udc40, \ud835\udc84, p) \u2212 \ud835\udc3c)),\n(10)\nto put special emphasis on fitting visible interior parts.\nThird, the interior mesh should stay within the exterior region.\nSpecifically, the body or face should be generally covered by the\nclothing or hair, yielding to the following loss function:\nLinside = \ud835\udf06insideL\ud835\udeff (\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 (R\ud835\udc60\n\ud835\udc5a(\ud835\udc40, p) \u2212 \ud835\udc46\ud835\udc50)).\n(11)\nFourth, the skin color of occluded body vertices should be similar to\nvisible skin regions. For this, we minimize the difference between\nthe body colors in occluded regions and the average skin color as\nLskin-inside = \ud835\udf06skin-insideL\ud835\udeff (\ud835\udc46\ud835\udc50 \u2299 (R\ud835\udc5a(\ud835\udc40, \ud835\udc84, p) \u2212 Cskin)),\n(12)\nwhere Cskin is the average color of the visible skin regions. In prac-\ntice, we encountered challenges with skin detection not performing\neffectively. Therefore, for body video sequences, we assume that\nthe hands are visible and utilize these hand regions to compute the\naverage skin color. Moreover, for face videos, we determine the skin\ncolor by computing the mean color of the cheek region.\nCombining the loss functions above, we use the following Lint\nfor reconstructing the interior avatar:\nLint = Lsilhouette + Lint-mask + Lskin + Linside + Lskin-inside. (13)\nRegularization. We regularize the reconstructed mesh surface with\nLreg = \ud835\udf06edgeLedge(\ud835\udc40) + \ud835\udf06offset \u2225\ud835\udc76\u22252,2 ,\n(14)\nwhere Ledge denotes the relative edge loss [Hirshberg et al. 2012]\nbetween the optimized interior mesh with and without the applied\noffsets. For the offset loss, we apply different weights to the body,\nhand and face region. Details are given in the experiment section.\n8\n\u2022\nYao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, and Michael J. Black\n4\nINTRIGUING INSIGHTS\nHybrid representation for general 3D modeling. While the pro-\nposed DELTA demonstrates the effectiveness of hybrid 3D represen-\ntation for human avatar modeling, the idea of hybrid representation\ncan be broadly useful for modeling general 3D objects and scenes,\nespecially for objects whose components have quite different phys-\nical properties. For example, a burning candle can be represented\nwith a mesh-based candle and a NeRF-based flame, and a hourglass\ncan be represented with mesh-based glass and point-based sand.\nDELTA shows the power of hybrid 3D representation through the\nlens of human avatar modeling, and we expect more future efforts\ncan be put in exploring hybrid 3D representation.\nHybrid vs. holistic 3D representation. It has been a long-standing\ndebate regarding the optimal holistic 3D representation for shape\nmodeling. In the existing graphics pipeline, meshes are still a de\nfacto choice for holistic 3D representation due to its efficiency in\nstorage and rendering. However, meshes can be quite limited in\nrepresenting certain geometric structures, such as hair strand, fluid,\nsmoke and complex clothing. Implicit 3D representations [Chen\nand Zhang 2019; Mescheder et al. 2019; Mildenhall et al. 2020; Park\net al. 2019] demonstrate strong flexibility in complex shape repre-\nsentation, and in particular, NeRF further shows great novel view\nsynthesis quality. However, it is difficult for NeRF to capture thin\nshell geometry like human body. While there is no single perfect 3D\nrepresentation for all objects, why not combine the advantages of\ndifferent representations and use them together? However, hybrid\nrepresentation also inevitably introduces some shortcomings. First,\nthe rendering process for hybrid representation becomes highly\nnontrivial and case-dependent. For example, our mesh-integrated\nvolume rendering only works for the hybrid mesh and NeRF repre-\nsentation. Second, the representational heterogeneity makes subse-\nquent learning and processing more difficult. For example, learning\na generative model on hybrid representation is far more complicated\nthan holistic representation. Moreover, editing hybrid representa-\ntion will also become more challenging for designers. Third, how to\nchoose the right 3D representations to combine is task-dependent.\nWhile DELTA uses meshes for human head and NeRFs for hair, it\ncould be better to use a strand-based representation for hair.\n5\nEXPERIMENTS AND RESULTS\n5.1\nDatasets\nDELTA offers a solution for capturing dynamic objects from monoc-\nular video. We demonstrate the effectiveness of our approach by\napplying it to the challenging tasks of capturing clothing and hair\nfrom videos. To evaluate our approach, we introduce two types of\ndatasets, one for full-body and one for head capture.\nFull-body datasets. To compare with other state-of-the-art meth-\nods of realistic human capturing. We evaluate DELTA on sequences\nfrom public sources: People Snapshot [Alldieck et al. 2018b], iPER [Liu\net al. 2019], SelfRecon [Jiang et al. 2022]. However, none of them\nprovide complicated clothes such as long dresses. Thus, we cap-\nture our own data MPIIS-SCARF, where we record videos of each\nsubject wearing short and long dresses. For People Snapshot, we\nuse the provided SMPL pose as initialization instead of running\nPIXIE [Feng et al. 2021a]. To be specific, we use 4 subjects (\u201cmale-3-\ncasual\u201d, \u201cfemale-3-casual\u201d, \u201cmale-4-casual\u201d, \u201cfemale-4-casual\u201d) from\nPeople Snapshot [Alldieck et al. 2018b] for qualitative and quanti-\ntative evaluation. The quantitative evaluation follows the settings\nof Anim-NeRF [Chen et al. 2021b]. We further use 4 subjects (\u201csub-\nject003\u201d, \u201csubject016\u201d, \u201csubject022\u201d, \u201csubject023\u201d) with outfit 1 and\nmotion 1 from iPER [Liu et al. 2019] and 4 synthetic video data (\u201cfe-\nmale outfit1\u201d, \u201cfemale outfit2\u201d, \u201cfemale outfit3\u201d, \u201cmale outfit1\u201d) and\n1 self-captured video (\u201cCHH female\u201d) from SelfRecon [Jiang et al.\n2022] for qualitative evaluation. For MPIIS-SCARF, we use A-pose\nvideos of subject \u201cYao\u201d with six types of clothing for qualitative\nevaluation, those videos include loose dressing and short skirts. For\neach subject, we use around 100-150 images for optimization. For\neach frame, we run PIXIE [Feng et al. 2021a] to initialize (\ud835\udf37, \ud835\udf3d, \ud835\udf4d),\nand camera p. For datasets without providing silhouette masks, we\ncompute \ud835\udc46 with [Lin et al. 2022], and [Dabhi 2022] for \ud835\udc46\ud835\udc50.\nHead datasets. We also evaluate DELTA on head videos from\npublic sources. To be specific, we use video \u201cMVI_1810\" from IMA-\nvatar [Zheng et al. 2022], \u201cperson_0000\u201d and \u201cperson_0004\u201d from\nneural head avatar [Grassal et al. 2022]. As subjects with long hair\nare missing, we further collected one video with long hair from the\nInternet, named video \u201cb0_0\u201d [Xiao 2022] (2:30). For each image\nfrom the video, we detect the upper body region and resize it to\nan image with 512x512 size. We then estimate 68 landmarks [Bulat\nand Tzimiropoulos 2017] and iris [Lugaresi et al. 2019], portrait\nmatting with MODNet [Ke et al. 2022], and segment face and hair\nwith face parsing [zllrunning 2019]. Given the estimated labels and\nSMPL-X model, we roughly estimate the shape and texture param-\neters for the subject, and camera, pose, expression and lighting\n(Spherical harmonic) for each frame. Subsequently, for enhanced\nSMPL-X shape fitting, we perform parameter optimization across\nall frames, where shape and texture parameters are shared across\nframes. These optimized parameters serve as the initialization for\nour model training. Nonetheless, these videos often lack backviews\nof the head as they predominantly focus on face-related areas. To\ndemonstrate our method\u2019s capacity for capturing complete hairs,\nwe also incorporate synthetic data from the AGORA dataset [Patel\net al. 2021]. We select three subjects from Agora, each containing\nthe mesh, texture, and corresponding SMPL fits. 200 images are\nrendered from the textured mesh for training DELTA.\n5.2\nImplementation Details\nWe choose \ud835\udf0e = 0.1 and |N (x) | = 6. For full-body video, we set \ud835\udc61\ud835\udc5b =\n\u22120.6, and \ud835\udc61\ud835\udc53 = 0.6 and weight the individual losses with \ud835\udf06pixel = 1.0,\n\ud835\udf06semantic = 0.0005, \ud835\udf06ext = 0.5, \ud835\udf06silhouette = 0.001, \ud835\udf06int-mask = 30,\n\ud835\udf06skin = 1.0, \ud835\udf06inside = 40, \ud835\udf06skin-inside = 0.01, \ud835\udf06edge = 500, \ud835\udf06offset =\n400. For \ud835\udf06offset, the weight ratio of body, face and hands region\nis 2 : 3 : 12. Note that it is important to perform the first stage\nNeRF training without optimizing the non-rigid deformation model.\nIn this stage, we also set \ud835\udf06semantic = 0. In the second stage, the\nnon-rigid deformation model then explains clothing deformations\nthat cannot be explained by the body transformation. And \ud835\udc3f\ud835\udc60\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc50\nhelps capture more details that can not be modelled by the non-rigid\ndeformation. The overall optimization time is around 40 hours with\nNVIDIA V100. In head video settings, we conducted SMPL-X fitting\nLearning Disentangled Avatars with Hybrid 3D Representations\n\u2022\n9\nReference image\nAnim-NeRF\nSelfRecon\nOurs\nFig. 4. Qualitative comparison with SelfRecon [Jiang et al. 2022] and Anim-\nNeRF [Chen et al. 2021b] for reconstruction. While all methods capture the\nclothing with comparable quality, our approach has much more detailed\nface and hands due to the disentangled representation of clothing and body.\nReference image\nNHA\nIMAvatar\nOurs\nFig. 5. Qualitative comparison with neural head avatar (NHA) [Grassal et al.\n2022] and IMavatar [Zheng et al. 2022] for reconstruction. Our method\nexhibits superior performance in capturing the geometry of the face and\nshoulders. Moreover, it achieves exceptional rendering quality for the hair.\nThis can be attributed to the effective utilization of a disentangled represen-\ntation for separating the hair and face components in DELTA.\nfor all frames during data processing, that ensures accurate face\nfitting. By employing this as our initialization for DELTA training,\nwe can directly train both mesh-based face and NeRF-based hair\ncomponents. The chosen hyperparameters include \ud835\udc61\ud835\udc5b = \u22121.5, and\n\ud835\udc61\ud835\udc53 = 1.5. We assign weights to individual losses as follows: \ud835\udf06pixel =\n1.0, \ud835\udf06semantic = 0.015, \ud835\udf06ext = 0.5, \ud835\udf06silhouette = 0.001, \ud835\udf06int-mask = 30,\n\ud835\udf06skin = 1.0, \ud835\udf06inside = 40, \ud835\udf06skin-inside = 0.001, \ud835\udf06edge = 500, \ud835\udf06offset =\n400. To enhance training efficiency, we adopt Instant-NGP [Li et al.\n2023a; M\u00fcller et al. 2022] for parameterizing the hair component.\nUnlike the MLP layers in the original NeRF model, Instant-NGP\nleverages a hash table to store feature grids at various coarseness\nscales, resulting in fast training and inference speeds. We then\nrequire around 40 minutes of optimization time with NVIDIA A100.\n5.3\nComparison to Existing Methods\nOur approach enables the creation of hybrid explicit-implicit avatars\nfrom monocular videos. We note that this has not been achieved by\nFig. 6. Qualitative result on synthetic upper-body videos. The leftmost and\nrightmost images show the colored rendering of the learned avatars. The\nmiddle images show the hybrid rendering of the estimated upper body and\nhair. The results validate DELTA\u2019s ability to accurately represent complete\nhair views, including both short and long hair types.\nprevious methods, which typically model clothed bodies or heads\nholistically using either implicit or explicit representations. To eval-\nuate the effectiveness of our approach, we compare it to existing\nstate-of-the-art methods on the challenging tasks of clothed-body\nand head modeling. The explicit-implicit modeling of DELTA also\nnaturally disentangles objects such as the body and clothing, thereby\nenabling garment reconstruction. Unlike previous methods that\nreconstruct cloth geometry from a single image with the help of\nextensive 3D scan data, our approach can reconstruct garments from\nimages alone. We evaluate the effectiveness of DELTA for garment\nreconstruction by comparing it to existing methods.\nBody and clothing modeling. We quantitatively compare NB [Om-\nran et al. 2018], SMPLpix [Prokudin et al. 2021], Neural Body [Peng\net al. 2021b] and Anim-NeRF [Chen et al. 2021b], following the\nevaluation protocol of [Chen et al. 2021b]. To be specfic, we use\n4 subjects (\u201csubject003\u201d, \u201csubject016\u201d, \u201csubject022\u201d, \u201csubject023\u201d)\nwith outfit 1 and motion 1 from iPER [Liu et al. 2019] for qualita-\ntive evaluation. For all subjects, we uniformly select frames 1-490\nwith a step-size 4 for optimization. We use 4 synthetic video data\n(\u201cfemale outfit1\u201d, \u201cfemale outfit2\u201d, \u201cfemale outfit3\u201d, \u201cmale outfit1\u201d)\nand 1 self-captured video (\u201cCHH female\u201d) from SelfRecon [Jiang\net al. 2022]. For each subject, we use 100 frames for optimization.\nFor self-captured data, we use A-pose videos of subject \u201cYao\u201d with\nsix types of clothing for qualitative evaluation, those videos include\nloose dressing and short skirts. For each video, we uniformly select\nframes 0-400 with a step-size 2 for optimization. Table 1 shows that\nDELTA is more accurate than the other methods under most metrics.\nThe qualitative comparison in Figure 4 demonstrates that DELTA\ncan better reconstruct the hand and face geometry compared to\nSelfRecon [Jiang et al. 2022] and Anim-NeRF [Chen et al. 2021b].\n10\n\u2022\nYao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, and Michael J. Black\nSubject ID\nPSNR\u2191\nSSIM\u2191\nLIPIS\u2193\nNeRF SMPLpix\nNB\nAnim-NeRF DELTA NeRF SMPLpix NB Anim-NeRF DELTA NeRF SMPLpix NB Anim-NeRF DELTA\nmale-3-casual\n20.64\n23.74\n24.94\n29.37\n30.59\n.899\n.923\n.943\n.970\n.977\n.101\n.022\n.033\n.017\n.024\nmale-4-casual\n20.29\n22.43\n24.71\n28.37\n28.99\n.880\n.910\n.947\n.961\n.970\n.145\n.031\n.042\n.027\n.025\nfemale-3-casual 17.43\n22.33\n23.87\n28.91\n30.14\n.861\n.929\n.950\n.974\n.977\n.170\n.027\n.035\n.022\n.028\nfemale-4-casual 17.63\n23.35\n24.37\n28.90\n29.96\n.858\n.926\n.945\n.968\n.972\n.183\n.024\n.038\n.017\n.026\nTable 1. Quantitative comparison of novel view synthesis on People-Snapshot [Alldieck et al. 2018b].\nInput image\nSMPLicit\nBCNet\nOurs\nFig. 7. Qualitative comparison of garment reconstruction. DELTA recon-\nstructs different clothing types more faithfully than SMPLicit [Corona et al.\n2021] and BCNet [Jiang et al. 2020].\nFace and hair modeling. We conduct an evaluation of our pro-\nposed method using four real-world videos. To assess the effec-\ntiveness of our approach, we compare it with two state-of-the-art\nmethods, neural head avatar (NHA) [Grassal et al. 2022] and IMa-\nvatar [Zheng et al. 2022]. To ensure a fair comparison, we adopt\nthe same experimental protocol, where we train NHA and IMa-\nvatar using exactly the same set of video frames and reserve the\nremaining frames for evaluation. To be specific, for subjects \u201cper-\nson_0000\u201d, \u201cperson_0004\u201d and \u201cMVI_1810\u201d, we sample every 50\nframes for evaluation, and for the subject \u201cb0_0\", we sample every 5\nframes. Following neural head avatar [Grassal et al. 2022], for each\nimage, we keep the trained model and optimize per-frame param-\neters such as camera, pose, and expression. Consistent with prior\nresearch [Gafni et al. 2021; Grassal et al. 2022; Zheng et al. 2022], we\nemploy four image-based metrics to evaluate our approach. These\nmetrics include pixel-wise L1 loss, peak signal-to-noise ratio (PSNR),\nstructural similarity metric (SSIM), and the learned perceptual im-\nage patch similarity (LPIPS). We find that NHA only focuses on\nthe face, neck, and hair regions for training and evaluation. For a\nfair comparison, we compute the metrics on both the whole human\nregion and only face, neck and hair regions.\nThe quantitative comparison presented in Table 2 demonstrates\nthat our method attains the highest level of quality when consider-\ning the entire human region. However, when specifically focusing\non the face, hair, and neck regions, it is worth noting that NHA\nSource subject\nReposing\nClothing transfer\nFig. 8. Applications of DELTA. The hybrid representation enables (middle)\nreposing with detailed control over the body pose and (right) dressing up\nthe source subject with target clothing. The target pose and clothing are\nshown in the inset images.\nFig. 9. Applications of DELTA. The hybrid representation enables transfer-\nring NeRF-based hairs into another face. Picture in the left indicates the\nsource of the original hair. The avatar can also be animated with different\nposes and expressions.\nachieves superior results for subjects with short hair, such as \u201cper-\nson_0000\u201d. Nevertheless, when it comes to subjects with longer hair,\nNHA struggles to capture both hair and face details, as exempli-\nfied in instances such as \u201cMVI_1810\u201d and \u201cb0_0\u201d. In contrast, our\nmethod performs effectively across various hair types and success-\nfully captures the entirety of the avatar, including changes in the\nshoulders. This capability can be attributed to the utilization of\nhybrid representations within our approach.\nWe additionally provide qualitative comparisons for novel view\nimages and shapes in Figure 5, along with supplementary qualitative\nresults of DELTA applied to synthetic upper-body videos from the\nAGORA [Patel et al. 2021] dataset in Figure 6. Our method show-\ncases superior performance in capturing accurate face and shoulder\ngeometry, while also delivering high-quality renderings of the hair.\n5.4\nApplications\nBody and garment reconstruction. We show comparisons on\nGarment reconstruction with SMPLicit [Corona et al. 2021] and\nLearning Disentangled Avatars with Hybrid 3D Representations\n\u2022\n11\nVideo\nModel\nWhole\nFace, Hair and Neck\nL1 \u2193\nPSNR \u2191\nSSIM \u2191\nLIPIS \u2193\nL1 \u2193\nPSNR \u2191\nSSIM \u2191\nLIPIS \u2193\nperson_0000\nNHA [Grassal et al. 2022]\n0.094\n12.15\n0.843\n0.198\n0.012\n24.92\n0.920\n0.046\nIMavatar [Zheng et al. 2022]\n0.024\n22.55\n0.882\n0.177\n0.015\n23.70\n0.917\n0.089\nDELTA\n0.021\n24.04\n0.892\n0.122\n0.017\n23.37\n0.914\n0.086\nMVI_1810\nNHA [Grassal et al. 2022]\n0.054\n16.01\n0.817\n0.195\n0.038\n18.94\n0.842\n0.149\nIMavatar [Zheng et al. 2022]\n0.039\n20.33\n0.829\n0.171\n0.031\n21.44\n0.851\n0.137\nDELTA\n0.039\n21.33\n0.835\n0.156\n0.034\n22.12\n0.852\n0.132\nb0_0\nNHA [Grassal et al. 2022]\n0.062\n15.60\n0.874\n0.203\n0.042\n16.12\n0.896\n0.137\nIMavatar [Zheng et al. 2022]\n0.043\n19.61\n0.871\n0.188\n0.030\n20.13\n0.905\n0.097\nDELTA\n0.025\n23.28\n0.909\n0.096\n0.022\n21.47\n0.917\n0.103\nTable 2. Quantitative comparison of novel pose and expression synthesis on public real videos.\nBCNet [Jiang et al. 2020] in Fig 7. DELTA gives better visual qual-\nity than SMPLicit and BCNet. Note that the training/optimization\nsettings are different, they reconstruct the body and garment from\na single image, while our results are learned from video. However,\nthey require a large set of 3D scans and manually designed cloth\ntemplates for training, while we do not need any 3D supervision,\nand capture the garment appearance as well. Figure 7 shows that\nDELTA reconstructs different clothing types more faithfully.\nReposing. For clothed body modeling, unlike previous methods\nthat represent clothed bodies holistically, DELTA offers more fine-\ngrained control over body pose especially hand pose. Figure 8 shows\nreposing into novel poses. Similar to the face and hair, utilizing an\nexplicit shape model to present face region facilitates generalization\nacross a wide range of facial expression animations. As Figure 9\nshows different expressions of the reconstructed avatar.\nClothing and hair transfer. Figures 1, 8 and 9 qualitatively demon-\nstrate the capability of our hybrid 3D representation in enabling\nclothing and hair transfer between avatars. We note that the cloth-\ning and hair is able to seamlessly adapt to accommodate various\nbody shapes. Furthermore, the trained hair and clothing models can\nbe both seamlessly transferred to different subjects. One potential\napplication involves utilizing an existing body estimation method\nlike PIXIE [Feng et al. 2021a] to estimate the body shape from a\nsingle image. Subsequently, our captured hair and clothing models\ncan be applied to this subject, offering a streamlined approach for\nvirtual try-on applications, as shown in Figure 10.\nAltering human Shape. Figure 11 highlights an additional facet\nof DELTA\u2019s capabilities. We show the capacity to alter human body\nor face shape through adjustments in SMPL-X shape parameters.\nSubsequently, the NeRF-based clothing or hair seamlessly adjusts\nto align with the modified shape.\n5.5\nAblation Study\nWe run different ablation experiments to show the impact of dif-\nferent components of our hybrid representation, and to show the\nimpact of the pose refinements.\nEffect of representations. DELTA consists of a NeRF to represent\nclothing, and a mesh with vertex displacements. Figure 12 compares\nNeRF to holistically represent body and clothing (i.e., DELTA w/o\nbody-clothing segmentation) and mesh-only based representation\nFig. 10. Virtual try-on Application of DELTA. Given a single image, we can\nestimate the body shape using PIXIE [Feng et al. 2021a]. The body texture is\nfrom PIXIE template. Both the trained hair and clothing can be subsequently\napplied to this subject, resulting in smooth virtual try-on applications. In\nthis instance, the captured hair is derived from the second example in Figure\n6, and the clothing is from the second example of Figure 7.\n(i.e., DELTA w/o NeRF). Our hybrid representation is better able to\nestimate the face, hands, and complex clothing. Note that, unlike\nour hybrid representation, none of the existing body NeRF methods\ncan transfer clothing between avatars.\nEffect of pose refinement. Since the pose estimation for each\nframe is not accurate, the pose refinement is important to gain de-\ntails. We try learning our method without pose refinement. Figure 14\nshows that pose refinement improves the image quality a lot.\n6\nDISCUSSION AND LIMITATION\nSegmentation. DELTA requires body and clothing/hair segmenta-\ntion for training. Segmentation errors of the subject and background\nnegatively impact the visual quality of the extracted avatar, and er-\nroneous clothing or hair segmentation results in poor separation of\nmesh-based body and NeRF-based clothing or hair part. Figure 13\nshows the wrong reconstruction due to consistent clothing seg-\nmentation errors, e.g. the belt is not recognized as part of clothing\nin segmentation, this results in wrong disentanglement between\nhuman body and clothing. Enforcing temporal consistency by ex-\nploiting optical flow could improve the segmentation quality.\nGeometric quality. The strength of NeRF is its visual quality and\nthe ability to synthesize realistic images, even when the geometry\nis not perfect. Figure 15 and Figure 16 show examples of noisy\n12\n\u2022\nYao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, and Michael J. Black\nSource subject\nReconstruction\nChanging shape\nFig. 11. DELTA can change underlying body/face shapes by modifying\nSMPL-X shape parameters, and the NeRF-based clothing/hair will adapt to\nthe new body/face shape accordingly.\nReference image\nNeRF\nSMPL-X +D\nOurs\nFig. 12. Rendered images and extracted meshes from different components\nof DELTA. Our hybrid representation gives a better estimated face, hand,\nand clothing geometry than vanilla NeRF or a mesh-based representation.\nInput image and clothing segmentations\nReconstruction\nFig. 13. The wrong clothing segmentation masks result in a visible gap\nwithin the reconstructed clothing.\ngeometry despite good visual quality. In contrast, recent SDF-based\nmethods have demonstrated good geometric reconstruction (e.g.,\n[Jiang et al. 2022]). It may be possible to leverage their results to\nbetter represent the underlying clothed shape or to regularize NeRF.\nNovel poses and views. Although DELTA demonstrates general-\nization to unseen poses, artifacts may occur in extreme poses. As\ndepicted in Figure 17, the animation results for new poses exhibit\nsatisfactory performance for the body and face regions. However,\nartifacts are prevalent in the non-rigid fusion (clothing or hair) com-\nponent. Notably, for regions that have not been encountered in\nSource subject\nw/o pose refinement\nw/ pose refinement\nFig. 14. Rendering results of clothed body (up) and head (bottom) w/o and\nw/ pose refinement. The pose refinement improves the visual quality of\nthe reconstruction, as more texture details are reconstructed. For the face\nsubject, please zoom in to check the difference.\nReference image\nCaptured clothing appearance & geometry\nFig. 15. While DELTA gives good visual quality for clothing renderings, the\nunderlying geometry of the NeRF clothing is sometimes noisy.\nFig. 16. Two examples of captured hair appearance and geometry. While\nDELTA gives good visual quality for hair renderings, the underlying geome-\ntry of the NeRF hair is noisy.\nthe training data, our model will fail to capture the desired details.\nFor instance, in the example featuring short hair, the hair in the\nhead top is always missing in all poses and views in the video. To\naddress these limitations, potential solutions include incorporating\nregularization techniques during NeRF optimization or training a\ngenerative model using a diverse set of training examples encom-\npassing different individuals and poses. These approaches have the\npotential to enhance the robustness and accuracy of the model when\ndealing with unseen regions and extreme poses.\nPose initialization. DELTA refines the body pose during optimiza-\ntion. However, it may fail if the initial pose is far from the right\npose. Handling difficult poses where PIXIE [Feng et al. 2021a] fails\nrequires a more robust 3D body pose estimator.\nLearning Disentangled Avatars with Hybrid 3D Representations\n\u2022\n13\nFig. 17. Some failure cases for avatar animation. Artifacts may occur in\nextreme poses. In the example shown at the bottom-left, the top part of the\nhair is absent since it was never observed in the training data.\nDynamics. DELTA handles non-rigid cloth deformation with the\npose-conditioned deformation model. While the global pose can\naccount for some deformation, how to accurately model the clothing\nand hair dynamics as a function of body movement remains an open\nproblem and is an important future work.\nLighting. As with other NeRF methods, we do not factor lighting\nand material properties. This results in baked-in shading and the\naveraging of specular reflections across frames. Factoring lighting\nfrom shape and material is a key next step to improve realism.\nFacial expressions. DELTA uses the facial expressions estimated\nby PIXIE [Feng et al. 2021a] which is unable to capture the full\nspectrum of emotions (cf. [Dan\u011b\u010dek et al. 2022]). Also, we have not\nfully exploited neural radiance fields to capture complex changes in\nfacial appearance, e.g., due to the movement of mouth opening. We\nbelieve this is a promising future direction.\n7\nCONCLUDING REMARKS\nDELTA is able to automatically extract human body, clothing or hair\nfrom a monocular video. Our key novelty is a hybrid representation\nthat combines a mesh-based body model with a neural radiance field\nto separately model the body and clothing/hair. This factored repre-\nsentation enables DELTA to transfer clothing/hair between avatars,\nanimate the body pose of the avatars including finger articulation,\nalter their body shape and facial expression, and visualize them from\nunseen viewing directions. This property makes DELTA well suited\nto VR and virtual try-on applications. Finally, DELTA outperforms\nexisting avatar extraction methods from videos in terms of visual\nquality and generality.\nACKNOWLEDGMENTS\nWe would like to sincerely thank Sergey Prokudin, Yuliang Xiu,\nSongyou Peng, Qianli Ma for fruitful discussions, and Peter Kulits,\nZhen Liu, Yandong Wen, Hongwei Yi, Xu Chen, Soubhik Sanyal,\nOmri Ben-Dov, Shashank Tripathi for proofreading. We also thank\nBetty Mohler, Sarah Danes, Natalia Marciniak, Tsvetelina Alexi-\nadis, Claudia Gallatz, and Andres Camilo Mendoza Patino for their\nsupports with data. This work was partially supported by the Max\nPlanck ETH Center for Learning Systems.\nDisclosure. MJB has received research gift funds from Adobe, Intel,\nNvidia, Meta/Facebook, and Amazon. MJB has financial interests\nin Amazon, Datagen Technologies, and Meshcapade GmbH. While\nMJB is a consultant for Meshcapade, his research in this project was\nperformed solely at, and funded solely by, the Max Planck Society.\nREFERENCES\nOswald Aldrian and WA Smith. 2010. A linear approach of 3d face shape and texture\nrecovery using a 3d morphable model. In BMVC.\nThiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar, Christian Theobalt, and Gerard\nPons-Moll. 2019a. Learning to reconstruct people in clothing from a single RGB\ncamera. In CVPR.\nThiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, and Gerard Pons-\nMoll. 2018a. Detailed human avatars from monocular video. In 3DV.\nThiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, and Gerard Pons-\nMoll. 2018b. Video based reconstruction of 3d people models. In CVPR.\nThiemo Alldieck, Gerard Pons-Moll, Christian Theobalt, and Marcus Magnor. 2019b.\nTex2shape: Detailed full human body geometry from a single image. In ICCV.\nThiemo Alldieck, Hongyi Xu, and Cristian Sminchisescu. 2021. imGHUM: Implicit\nGenerative Models of 3D Human Shape and Articulated Pose. In ICCV.\nDragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers,\nand James Davis. 2005. SCAPE: Shape completion and animation of people. ACM\nTransactions on Graphics 24, 3 (2005), 408\u2013416.\nRonen Basri and David W Jacobs. 2003. Lambertian reflectance and linear subspaces.\nIEEE transactions on pattern analysis and machine intelligence 25, 2 (2003), 218\u2013233.\nThabo Beeler, Bernd Bickel, Gioacchino Noris, Paul Beardsley, Steve Marschner,\nRobert W Sumner, and Markus Gross. 2012. Coupled 3D reconstruction of sparse\nfacial hair and skin. ACM Transactions on Graphics 31, 4 (2012), 1\u201310.\nHugo Bertiche, Meysam Madadi, and Sergio Escalera. 2020. CLOTH3D: clothed 3d\nhumans. In ECCV.\nSai Bi, Stephen Lombardi, Shunsuke Saito, Tomas Simon, Shih-En Wei, Kevyn Mcphail,\nRavi Ramamoorthi, Yaser Sheikh, and Jason Saragih. 2021. Deep relightable ap-\npearance models for animatable faces. ACM Transactions on Graphics 40, 4 (2021),\n1\u201315.\nVolker Blanz and Thomas Vetter. 1999. A morphable model for the synthesis of 3D\nfaces. In SIGGRAPH.\nVolker Blanz and Thomas Vetter. 2003. Face recognition based on fitting a 3D morphable\nmodel. IEEE Transactions on pattern analysis and machine intelligence 25, 9 (2003),\n1063\u20131074.\nAdrian Bulat and Georgios Tzimiropoulos. 2017. How far are we from solving the 2d &\n3d face alignment problem?(and a dataset of 230,000 3d facial landmarks). In ICCV.\nChen Cao, Derek Bradley, Kun Zhou, and Thabo Beeler. 2015. Real-time high-fidelity\nfacial performance capture. ACM Transactions on Graphics 34, 4 (2015), 1\u20139.\nChen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun Zhou. 2013. Faceware-\nhouse: A 3d facial expression database for visual computing. IEEE Transactions on\nVisualization and Computer Graphics 20, 3 (2013), 413\u2013425.\nMenglei Chai, Lvdi Wang, Yanlin Weng, Xiaogang Jin, and Kun Zhou. 2013. Dynamic\nhair manipulation in images and videos. ACM Transactions on Graphics 32, 4 (2013),\n1\u20138.\nMenglei Chai, Lvdi Wang, Yanlin Weng, Yizhou Yu, Baining Guo, and Kun Zhou. 2012.\nSingle-view hair modeling for portrait manipulation. ACM Transactions on Graphics\n31, 4 (2012), 1\u20138.\nEric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein.\n2021. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image\nsynthesis. In CVPR.\nJianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Xu Jia, and Huchuan\nLu. 2021b. Animatable neural radiance fields from monocular rgb videos. arXiv\npreprint arXiv:2106.13629 (2021).\nXin Chen, Anqi Pang, Wei Yang, Peihao Wang, Lan Xu, and Jingyi Yu. 2021a. TightCap:\n3D Human Shape Capture with Clothing Tightness Field. ACM Transactions on\nGraphics 41, 1 (2021), 1\u201317.\nZhiqin Chen and Hao Zhang. 2019. Learning implicit fields for generative shape\nmodeling. In CVPR.\nJulian Chibane, Aymen Mir, and Gerard Pons-Moll. 2020. Neural Unsigned Distance\nFields for Implicit Function Learning. In NeurIPS.\nVasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dimitrios Tzionas, and Michael J.\nBlack. 2020. Monocular Expressive Body Regression through Body-Driven Attention.\nIn ECCV.\nForrester Cole, David Belanger, Dilip Krishnan, Aaron Sarna, Inbar Mosseri, and\nWilliam T Freeman. 2017. Synthesizing normalized faces from facial identity features.\nIn CVPR.\nEnric Corona, Albert Pumarola, Guillem Alenya, Gerard Pons-Moll, and Francesc\nMoreno-Noguer. 2021. Smplicit: Topology-aware generative model for clothed\npeople. In CVPR.\nLevin Dabhi. 2022. Clothes Segmentation using U2NET. https://github.com/levindabhi/\ncloth-segmentation\nHang Dai, Nick Pears, William Smith, and Christian Duncan. 2020. Statistical modeling\nof craniofacial shape and texture. International Journal of Computer Vision 128, 2\n14\n\u2022\nYao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, and Michael J. Black\n(2020), 547\u2013571.\nRadek Dan\u011b\u010dek, Michael J Black, and Timo Bolkart. 2022. EMOCA: Emotion driven\nmonocular face capture and animation. In CVPR.\nR Dan\u011b\u0159ek, Endri Dibra, Cengiz \u00d6ztireli, Remo Ziegler, and Markus Gross. 2017. Deep-\ngarment: 3d garment shape estimation from a single image. In Computer Graphics\nForum, Vol. 36. Wiley Online Library, 269\u2013280.\nYu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. 2019.\nAccurate 3d face reconstruction with weakly-supervised learning: From single\nimage to image set. In CVPR Workshops.\nAbdallah Dib, C\u00e9dric Th\u00e9bault, Junghyun Ahn, Philippe-Henri Gosselin, Christian\nTheobalt, and Louis Chevallier. 2021. Towards high fidelity monocular face recon-\nstruction with rich reflectance using self-supervised learning and ray tracing. In\nICCV.\nBernhard Egger, William AP Smith, Ayush Tewari, Stefanie Wuhrer, Michael Zollhoefer,\nThabo Beeler, Florian Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani,\net al. 2020. 3d morphable face models\u2014past, present, and future. ACM Transactions\non Graphics 39, 5 (2020), 1\u201338.\nYao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, and Michael J Black.\n2021a. Collaborative regression of expressive bodies using moderation. In 3DV.\nYao Feng, Haiwen Feng, Michael J Black, and Timo Bolkart. 2021b. Learning an animat-\nable detailed 3D face model from in-the-wild images. ACM Transactions on Graphics\n40, 4 (2021), 1\u201313.\nYao Feng, Jinlong Yang, Marc Pollefeys, Michael J Black, and Timo Bolkart. 2022. Cap-\nturing and Animation of Body and Clothing from Monocular Video. In SIGGRAPH\nAsia.\nGuy Gafni, Justus Thies, Michael Zollhofer, and Matthias Nie\u00dfner. 2021. Dynamic\nneural radiance fields for monocular 4d facial avatar reconstruction. In CVPR.\nXuan Gao, Chenglai Zhong, Jun Xiang, Yang Hong, Yudong Guo, and Juyong Zhang.\n2022. Reconstructing Personalized Semantic Facial NeRF Models From Monocular\nVideo. In SIGGRAPH Asia.\nBaris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos Zafeiriou. 2019. Ganfit:\nGenerative adversarial network fitting for high fidelity 3d face reconstruction. In\nCVPR.\nPhilip-William Grassal, Malte Prinzler, Titus Leistner, Carsten Rother, Matthias Nie\u00dfner,\nand Justus Thies. 2022. Neural head avatars from monocular RGB videos. In CVPR.\nOshri Halimi, Fabian Prada, Tuur Stuyck, Donglai Xiang, Timur Bagautdinov, He\nWen, Ron Kimmel, Takaaki Shiratori, Chenglei Wu, and Yaser Sheikh. 2022. Gar-\nment avatars: Realistic cloth driving using pattern registration. arXiv preprint\narXiv:2206.03373 (2022).\nTong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and Tony Tung. 2021. ARCH++:\nAnimation-ready clothed human reconstruction revisited. In ICCV.\nTomas Lay Herrera, Arno Zinke, and Andreas Weber. 2012. Lighting hair from the\ninside: A thermal approach to hair reconstruction. ACM Transactions on Graphics\n31, 6 (2012), 1\u20139.\nDavid A. Hirshberg, Matthew Loper, Eric Rachlin, and Michael J. Black. 2012. Coregis-\ntration: Simultaneous Alignment and Modeling of Articulated 3D Shape. In ECCV.\nFangzhou Hong, Liang Pan, Zhongang Cai, and Ziwei Liu. 2021. Garment4d: Garment\nreconstruction from point cloud sequences. In NIPS.\nLiwen Hu, Chongyang Ma, Linjie Luo, and Hao Li. 2014. Robust hair capture using\nsimulated examples. ACM Transactions on Graphics 33, 4 (2014), 1\u201310.\nZeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung. 2020. ARCH:\nAnimatable reconstruction of clothed humans. In CVPR.\nPeter J. Huber. 1964. Robust Estimation of a Location Parameter. The Annals of\nMathematical Statistics 35, 1 (1964), 73 \u2013 101.\nBoyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. 2022. SelfRecon: Self Recon-\nstruction Your Digital Avatar from Monocular Video. In CVPR.\nBoyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang Liu, and Hujun Bao. 2020.\nBcnet: Learning body and cloth shape from a single image. In ECCV.\nNing Jin, Yilin Zhu, Zhenglin Geng, and Ronald Fedkiw. 2020. A Pixel-Based Framework\nfor Data-Driven Clothing. In Computer Graphics Forum, Vol. 39. Wiley Online Library,\n135\u2013144.\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual losses for real-time\nstyle transfer and super-resolution. In ECCV.\nHanbyul Joo, Tomas Simon, and Yaser Sheikh. 2018. Total capture: A 3d deformation\nmodel for tracking faces, hands, and bodies. In CVPR.\nAngjoo Kanazawa, Michael J Black, David W Jacobs, and Jitendra Malik. 2018. End-to-\nend recovery of human shape and pose. In CVPR.\nZhanghan Ke, Jiayu Sun, Kaican Li, Qiong Yan, and Rynson W.H. Lau. 2022. MODNet:\nReal-Time Trimap-Free Portrait Matting via Objective Decomposition. In AAAI.\nPetr Kellnhofer, Lars C Jebe, Andrew Jones, Ryan Spicer, Kari Pulli, and Gordon Wet-\nzstein. 2021. Neural lumigraph rendering. In CVPR.\nTaras Khakhulin, Vanessa Sklyarova, Victor Lempitsky, and Egor Zakharov. 2022. Real-\nistic One-shot Mesh-based Head Avatars. In ECCV.\nLaehyun Kim, Gaurav S Sukhatme, and Mathieu Desbrun. 2004. A haptic-rendering\ntechnique based on hybrid surface representation. IEEE computer graphics and\napplications 24, 2 (2004), 66\u201375.\nNikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis. 2019.\nLearning to reconstruct 3D human pose and shape via model-fitting in the loop. In\nICCV.\nAlexandros Lattas, Stylianos Moschoglou, Baris Gecer, Stylianos Ploumpis, Vasileios\nTriantafyllou, Abhijeet Ghosh, and Stefanos Zafeiriou. 2020. AvatarMe: Realistically\nRenderable 3D Facial Reconstruction\" in-the-wild\". In CVPR.\nVerica Lazova, Eldar Insafutdinov, and Gerard Pons-Moll. 2019. 360-Degree Textures of\nPeople in Clothing from a Single Image. In 3DV.\nGengyan Li, Abhimitra Meka, Franziska Mueller, Marcel C Buehler, Otmar Hilliges, and\nThabo Beeler. 2022. EyeNeRF: a hybrid representation for photorealistic synthesis,\nanimation and relighting of human eyes. ACM Transactions on Graphics 41, 4 (2022),\n1\u201316.\nRuilong Li, Hang Gao, Matthew Tancik, and Angjoo Kanazawa. 2023a. NerfAcc: Efficient\nSampling Accelerates NeRFs. arXiv preprint arXiv:2305.04966 (2023).\nTianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier Romero. 2017. Learning a\nmodel of facial shape and expression from 4D scans. ACM Transactions on Graphics\n36, 6 (2017), 194\u20131.\nXiaoxing Li, Tao Jia, and Hao Zhang. 2009. Expression-insensitive 3D face recognition\nusing sparse representation. In CVPR.\nYue Li, Marc Habermann, Bernhard Thomaszewski, Stelian Coros, Thabo Beeler, and\nChristian Theobalt. 2021. Deep physics-aware inference of cloth deformation for\nmonocular human performance capture. In 3DV.\nYue Li, Liqian Ma, Haoqiang Fan, and Kenny Mitchell. 2018. Feature-preserving detailed\n3d face reconstruction from a single image. In ACM SIGGRAPH European Conference\non Visual Media Production.\nZhe Li, Zerong Zheng, Yuxiao Liu, Boyao Zhou, and Yebin Liu. 2023b. PoseVocab: Learn-\ning Joint-structured Pose Embeddings for Human Avatar Modeling. In SIGGRAPH.\nShanchuan Lin, Linjie Yang, Imran Saleemi, and Soumyadip Sengupta. 2022. Robust\nVideo Matting (RVM). https://github.com/PeterL1n/RobustVideoMatting\nLingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and\nChristian Theobalt. 2021. Neural actor: Neural free-view synthesis of human actors\nwith pose control. ACM Transactions on Graphics 40, 6 (2021), 1\u201316.\nShaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, and Zhaopeng\nCui. 2020. Dist: Rendering deep implicit signed distance function with differentiable\nsphere tracing. In CVPR.\nWu Liu, Qian Bao, Yu Sun, and Tao Mei. 2022a. Recent advances of monocular 2d\nand 3d human pose estimation: A deep learning perspective. Comput. Surveys 55, 4\n(2022), 1\u201341.\nWeiyang Liu, Zhen Liu, Liam Paull, Adrian Weller, and Bernhard Sch\u00f6lkopf. 2022b.\nStructural causal 3d reconstruction. In ECCV.\nWen Liu, Zhixin Piao, Min Jie, Wenhan Luo, Lin Ma, and Shenghua Gao. 2019. Liquid\nWarping GAN: A Unified Framework for Human Motion Imitation, Appearance\nTransfer and Novel View Synthesis. In ICCV.\nStephen Lombardi, Jason Saragih, Tomas Simon, and Yaser Sheikh. 2018. Deep ap-\npearance models for face rendering. ACM Transactions on Graphics 37, 4 (2018),\n1\u201313.\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J.\nBlack. 2015. SMPL: A Skinned Multi-Person Linear Model. ACM Transactions on\nGraphics 34, 6 (2015), 248:1\u2013248:16.\nCamillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja,\nMichael Hays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee, et al.\n2019. Mediapipe: A framework for building perception pipelines. arXiv preprint\narXiv:1906.08172.\nLinjie Luo, Hao Li, Sylvain Paris, Thibaut Weise, Mark Pauly, and Szymon Rusinkiewicz.\n2012. Multi-view hair capture using orientation fields. In CVPR.\nLinjie Luo, Hao Li, and Szymon Rusinkiewicz. 2013. Structure-aware hair capture. ACM\nTransactions on Graphics 32, 4 (2013), 1\u201312.\nQianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang,\nand Michael J Black. 2020. Learning to dress 3D people in generative clothing. In\nCVPR.\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas\nGeiger. 2019. Occupancy networks: Learning 3d reconstruction in function space.\nIn CVPR.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ra-\nmamoorthi, and Ren Ng. 2020. Nerf: Representing scenes as neural radiance fields\nfor view synthesis. In ECCV.\nThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant Neu-\nral Graphics Primitives with a Multiresolution Hash Encoding. ACM Transactions\non Graphics 41, 4, Article 102 (july 2022), 15 pages. https://doi.org/10.1145/3528223.\n3530127\nGiljoo Nam, Chenglei Wu, Min H Kim, and Yaser Sheikh. 2019. Strand-accurate multi-\nview hair capture. In CVPR.\nMichael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. 2020. Dif-\nferentiable volumetric rendering: Learning implicit 3d representations without 3d\nsupervision. In CVPR.\nLearning Disentangled Avatars with Hybrid 3D Representations\n\u2022\n15\nMohamed Omran, Christoph Lassner, Gerard Pons-Moll, Peter V. Gehler, and Bernt\nSchiele. 2018. Neural Body Fitting: Unifying Deep Learning and Model Based Human\nPose and Shape Estimation. In 3DV.\nAhmed AA Osman, Timo Bolkart, and Michael J Black. 2020. Star: Sparse trained\narticulated human body regressor. In ECCV.\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Love-\ngrove. 2019. Deepsdf: Learning continuous signed distance functions for shape\nrepresentation. In CVPR.\nKeunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman,\nSteven M Seitz, and Ricardo Martin-Brualla. 2021. Nerfies: Deformable neural\nradiance fields. In ICCV.\nChaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-Moll. 2020. Tailornet: Predicting\nclothing in 3d as a function of human pose, shape and garment style. In CVPR.\nPriyanka Patel, Chun-Hao Paul Huang, Joachim Tesch, David Hoffmann, Shashank\nTripathi, and Michael J. Black. 2021. AGORA: Avatars in Geography Optimized for\nRegression Analysis. In CVPR.\nGeorgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A.\nOsman, Dimitrios Tzionas, and Michael J. Black. 2019. Expressive Body Capture:\n3D Hands, Face, and Body From a Single Image. In CVPR.\nGeorgios Pavlakos, Ethan Weber, Matthew Tancik, and Angjoo Kanazawa. 2022. The\nOne Where They Reconstructed 3D Humans and Environments in TV Shows. In\nECCV.\nPascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter.\n2009. A 3D face model for pose and illumination invariant face recognition. In IEEE\ninternational conference on advanced video and signal based surveillance.\nSida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei\nZhou, and Hujun Bao. 2021a. Animatable Neural Radiance Fields for Modeling\nDynamic Human Bodies. In ICCV.\nSida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei\nZhou. 2022. Animatable Neural Implicit Surfaces for Creating Avatars from Videos.\narXiv preprint arXiv:2203.08133 (2022).\nSida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and\nXiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured\nlatent codes for novel view synthesis of dynamic humans. In CVPR.\nStylianos Ploumpis, Evangelos Ververas, Eimear O\u2019Sullivan, Stylianos Moschoglou,\nHaoyang Wang, Nick Pears, William AP Smith, Baris Gecer, and Stefanos Zafeiriou.\n2020. Towards a complete 3D morphable model of the human head. IEEE transactions\non pattern analysis and machine intelligence 43, 11 (2020), 4142\u20134160.\nGerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J Black. 2017. ClothCap:\nSeamless 4D clothing capture and retargeting. ACM Transactions on Graphics 36, 4\n(2017), 1\u201315.\nSergey Prokudin, Michael J. Black, and Javier Romero. 2021. SMPLpix: Neural Avatars\nfrom 3D Human Models. In WACV.\nLingteng Qiu, Guanying Chen, Jiapeng Zhou, Mutian Xu, Junle Wang, and Xiaoguang\nHan. 2023. REC-MV: REconstructing 3D Dynamic Cloth from Monocular Videos. In\nCVPR.\nEduard Ramon, Gil Triginer, Janna Escur, Albert Pumarola, Jaime Garcia, Xavier Giro-i\nNieto, and Francesc Moreno-Noguer. 2021. H3d-net: Few-shot high-fidelity 3d head\nreconstruction. In ICCV.\nAnurag Ranjan, Timo Bolkart, Soubhik Sanyal, and Michael J Black. 2018. Generating\n3D faces using convolutional mesh autoencoders. In ECCV.\nSami Romdhani and Thomas Vetter. 2005. Estimating 3D shape and texture using pixel\nintensity, edges, specular highlights, texture constraints and a prior. In CVPR.\nYu Rong, Takaaki Shiratori, and Hanbyul Joo. 2021. FrankMocap: A Monocular 3D\nWhole-Body Pose Estimation System via Regression and Integration. In ICCV Work-\nshops.\nRadu Alexandru Rosu, Shunsuke Saito, Ziyan Wang, Chenglei Wu, Sven Behnke, and\nGiljoo Nam. 2022. Neural Strands: Learning Hair Geometry and Appearance from\nMulti-View Images. In ECCV.\nShunsuke Saito, Liwen Hu, Chongyang Ma, Hikaru Ibayashi, Linjie Luo, and Hao Li.\n2018. 3D hair synthesis using volumetric variational autoencoders. ACM Transactions\non Graphics 37, 6 (2018), 1\u201312.\nShunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Hao Li, and Angjoo\nKanazawa. 2019. PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed\nHuman Digitization. In ICCV.\nShunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. 2020. PIFuHD: Multi-\nLevel Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization.\nIn CVPR.\nIgor Santesteban, Miguel A Otaduy, and Dan Casas. 2019. Learning-based animation\nof clothing for virtual try-on. In Computer Graphics Forum, Vol. 38. Wiley Online\nLibrary, 355\u2013366.\nSoubhik Sanyal, Timo Bolkart, Haiwen Feng, and Michael J Black. 2019. Learning to\nregress 3D face shape and expression from an image without 3D supervision. In\nCVPR.\nSandro Sch\u00f6nborn, Bernhard Egger, Andreas Morel-Forster, and Thomas Vetter. 2017.\nMarkov chain monte carlo for automated face image analysis. International Journal\nof Computer Vision 123, 2 (2017), 160\u2013183.\nJiaxiang Shang, Tianwei Shen, Shiwei Li, Lei Zhou, Mingmin Zhen, Tian Fang, and Long\nQuan. 2020. Self-supervised monocular 3d face reconstruction by occlusion-aware\nmulti-view geometry consistency. In ECCV.\nVanessa Sklyarova, Jenya Chelishev, Andreea Dogaru, Igor Medvedev, Victor Lempit-\nsky, and Egor Zakharov. 2023. Neural Haircut: Prior-Guided Strand-Based Hair\nReconstruction. In ICCV.\nShih-Yang Su, Frank Yu, Michael Zollh\u00f6fer, and Helge Rhodin. 2021. A-NeRF: Articulated\nneural radiance fields for learning human shape, appearance, and pose. In NeurIPS.\nZhaoqi Su, Tao Yu, Yangang Wang, and Yebin Liu. 2022. Deepcloth: Neural garment\nrepresentation for shape and style editing. IEEE Transactions on Pattern Analysis\nand Machine Intelligence 45, 2 (2022), 1581\u20131593.\nTiancheng Sun, Giljoo Nam, Carlos Aliaga, Christophe Hery, and Ravi Ramamoor-\nthi. 2021. Human Hair Inverse Rendering using Multi-View Photometric data. In\nEurographics Symposium on Rendering.\nAyush Tewari, Florian Bernard, Pablo Garrido, Gaurav Bharaj, Mohamed Elgharib,\nHans-Peter Seidel, Patrick P\u00e9rez, Michael Zollhofer, and Christian Theobalt. 2019.\nFml: Face model learning from videos. In CVPR.\nAyush Tewari, Michael Zollh\u00f6fer, Pablo Garrido, Florian Bernard, Hyeongwoo Kim,\nPatrick P\u00e9rez, and Christian Theobalt. 2018. Self-supervised multi-level face model\nlearning for monocular reconstruction at over 250 hz. In CVPR.\nAyush Tewari, Michael Zollhofer, Hyeongwoo Kim, Pablo Garrido, Florian Bernard,\nPatrick Perez, and Christian Theobalt. 2017. Mofa: Model-based deep convolutional\nface autoencoder for unsupervised monocular reconstruction. In ICCV.\nJustus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias\nNie\u00dfner. 2016. Face2face: Real-time face capture and reenactment of rgb videos. In\nCVPR.\nYating Tian, Hongwen Zhang, Yebin Liu, and Limin Wang. 2022. Recovering 3D Human\nMesh from Monocular Images: A Survey. arXiv preprint arXiv:2203.01923 (2022).\nGarvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, and Gerard Pons-Moll. 2020. SIZER:\nA dataset and model for parsing 3d clothing and learning size sensitive 3d clothing.\nIn ECCV.\nLuan Tran and Xiaoming Liu. 2018. Nonlinear 3d face morphable model. In CVPR.\nRaquel Vidaurre, Igor Santesteban, Elena Garces, and Dan Casas. 2020. Fully Convolu-\ntional Graph Neural Networks for Parametric Virtual Try-On. In Computer Graphics\nForum, Vol. 39. Wiley Online Library, 145\u2013156.\nDaniel Vlasic, Matthew Brand, Hanspeter Pfister, and Jovan Popovic. 2006. Face transfer\nwith multilinear models. In SIGGRAPH.\nYi Wang, Xin Tao, Xiaojuan Qi, Xiaoyong Shen, and Jiaya Jia. 2018. Image inpainting\nvia generative multi-column convolutional neural networks. In NeurIPS.\nZiyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih,\nJessica Hodgins, and Michael Zollhofer. 2021. Learning compositional radiance\nfields of dynamic human heads. In CVPR.\nZiyan Wang, Giljoo Nam, Tuur Stuyck, Stephen Lombardi, Michael Zollh\u00f6fer, Jessica\nHodgins, and Christoph Lassner. 2022. HVH: Learning a Hybrid Neural Volumetric\nRepresentation for Dynamic Hair Performance Capture. In CVPR.\nKelly Ward, Florence Bertails, Tae-Yong Kim, Stephen R Marschner, Marie-Paule Cani,\nand Ming C Lin. 2007. A survey on hair modeling: Styling, simulation, and rendering.\nIEEE transactions on visualization and computer graphics 13, 2 (2007), 213\u2013234.\nYandong Wen, Weiyang Liu, Bhiksha Raj, and Rita Singh. 2021. Self-supervised 3d face\nreconstruction via conditional estimation. In ICCV.\nChung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira\nKemelmacher-Shlizerman. 2022. HumanNeRF: Free-Viewpoint Rendering of Moving\nPeople From Monocular Video. In CVPR.\nShangzhe Wu, Christian Rupprecht, and Andrea Vedaldi. 2020. Unsupervised learning\nof probably symmetric deformable 3d objects from images in the wild. In CVPR.\nDonglai Xiang, Hanbyul Joo, and Yaser Sheikh. 2019. Monocular Total Capture: Posing\nFace, Body, and Hands in the Wild. In CVPR.\nDonglai Xiang, Fabian Prada, Timur Bagautdinov, Weipeng Xu, Yuan Dong, He Wen,\nJessica Hodgins, and Chenglei Wu. 2021. Modeling clothing as a separate layer for\nan animatable human avatar. ACM Transactions on Graphics 40, 6 (2021), 1\u201315.\nYiqing Xiao. 2022. head video. https://www.bilibili.com/video/BV1b84y1q7Vm/?spm_\nid_from=333.999.0.0\nYuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J. Black. 2023. ECON:\nExplicit Clothed humans Optimized via Normal integration. In CVPR.\nYuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J. Black. 2022. ICON: Implicit\nClothed humans Obtained from Normals. In CVPR.\nHongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir, William T. Freeman, Rahul Suk-\nthankar, and Cristian Sminchisescu. 2020. GHUM & GHUML: Generative 3D Human\nShape and Articulated Pose Models. In CVPR.\nJiamin Xu, Zihan Zhu, Hujun Bao, and Wewei Xu. 2022. A Hybrid Mesh-neural Repre-\nsentation for 3D Transparent Object Reconstruction. arXiv preprint arXiv:2203.12613\n(2022).\nYuelang Xu, Lizhen Wang, Xiaochen Zhao, Hongwen Zhang, and Yebin Liu. 2023a.\nAvatarMAV: Fast 3D Head Avatar Reconstruction Using Motion-Aware Neural\nVoxels. In SIGGRAPH.\n16\n\u2022\nYao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, and Michael J. Black\nYuelang Xu, Hongwen Zhang, Lizhen Wang, Xiaochen Zhao, Huang Han, Qi Guojun,\nand Yebin Liu. 2023b. LatentAvatar: Learning Latent Expression Code for Expressive\nNeural Head Avatar. In SIGGRAPH.\nLingchen Yang, Zefeng Shi, Youyi Zheng, and Kun Zhou. 2019. Dynamic hair modeling\nfrom monocular videos using deep neural networks. ACM Transactions on Graphics\n38, 6 (2019), 1\u201312.\nZe Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng Huang, Wei-Chiu Ma, Xinchen\nYan, Ersin Yumer, and Raquel Urtasun. 2021. S3: Neural shape, skeleton, and skinning\nfields for 3D human modeling. In CVPR.\nLior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. Volume rendering of neural\nimplicit surfaces. In NeurIPS.\nLior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and\nYaron Lipman. 2020. Multiview neural surface reconstruction by disentangling\ngeometry and appearance. In NeurIPS.\nTarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-Peter Seidel, Mohamed El-\ngharib, Daniel Cremers, and Christian Theobalt. 2021. i3dmm: Deep implicit 3d\nmorphable model of human heads. In CVPR.\nAndrei Zanfir, Eduard Gabriel Bazavan, Mihai Zanfir, William T Freeman, Rahul Suk-\nthankar, and Cristian Sminchisescu. 2021. Neural descent for visual 3d human pose\nand shape. In CVPR.\nMeng Zhang, Menglei Chai, Hongzhi Wu, Hao Yang, and Kun Zhou. 2017. A data-driven\napproach to four-view image-based hair modeling. ACM Transactions on Graphics\n36, 4 (2017), 1\u201311.\nMeng Zhang and Youyi Zheng. 2019. Hair-GAN: Recovering 3D hair structure from a\nsingle image using generative adversarial networks. Visual Informatics 3, 2 (2019),\n102\u2013112.\nFang Zhao, Wenhao Wang, Shengcai Liao, and Ling Shao. 2021. Learning anchored\nunsigned distance functions with gradient direction alignment for single-view\ngarment reconstruction. In ICCV.\nYufeng Zheng, Victoria Fern\u00e1ndez Abrevaya, Marcel C B\u00fchler, Xu Chen, Michael J\nBlack, and Otmar Hilliges. 2022. Im avatar: Implicit morphable head avatars from\nvideos. In CVPR.\nYujian Zheng, Zirong Jin, Moran Li, Haibin Huang, Chongyang Ma, Shuguang Cui, and\nXiaoguang Han. 2023a. Hairstep: Transfer synthetic to real using strand and depth\nmaps for single-view 3d hair modeling. In CVPR.\nYufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J. Black, and Otmar Hilliges.\n2023b. PointAvatar: Deformable Point-based Head Avatars from Videos. In CVPR.\nZerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. 2021. Pamir: Parametric model-\nconditioned implicit representation for image-based human reconstruction. IEEE\ntransactions on pattern analysis and machine intelligence 44, 6 (2021), 3170\u20133184.\nZerong Zheng, Xiaochen Zhao, Hongwen Zhang, Boning Liu, and Yebin Liu. 2023c.\nAvatarRex: Real-time Expressive Full-body Avatars. ACM Transactions on Graphics\n42, 4 (2023).\nYuxiao Zhou, Marc Habermann, Ikhsanul Habibie, Ayush Tewari, Christian Theobalt,\nand Feng Xu. 2021. Monocular Real-Time Full Body Capture With Inter-Part Corre-\nlations. In CVPR.\nYi Zhou, Liwen Hu, Jun Xing, Weikai Chen, Han-Wei Kung, Xin Tong, and Hao Li. 2018.\nHairnet: Single-view hair reconstruction using convolutional neural networks. In\nECCV.\nHeming Zhu, Yu Cao, Hang Jin, Weikai Chen, Dong Du, Zhangye Wang, Shuguang\nCui, and Xiaoguang Han. 2020. Deep Fashion3D: A dataset and benchmark for 3D\ngarment reconstruction from single images. In ECCV.\nHeming Zhu, Lingteng Qiu, Yuda Qiu, and Xiaoguang Han. 2022. Registering Explicit to\nImplicit: Towards High-Fidelity Garment mesh Reconstruction from Single Images.\nIn CVPR.\nzllrunning. 2019. face-parsing.PyTorch. https://github.com/zllrunning/face-parsing.\nPyTorch\n"
  }
]