[
  {
    "title": "A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation",
    "link": "https://arxiv.org/pdf/2310.16656.pdf",
    "upvote": "34",
    "text": "A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image\nGeneration\nEyal Segalis\nGoogle Research\neyalis@google.com\nDani Valevski\nGoogle Research\ndaniv@google.com\nDanny Lumen\nGoogle Research\ndwasserman@google.com\nYossi Matias\nGoogle Research\nyossi@google.com\nYaniv Leviathan\nGoogle Research\nleviathan@google.com\nFigure 1. Examples of non-cherrypicked generations from the base Stable Diffusion model (left) and our model trained on a recaptioned\ndataset (right), on the same set of random seeds.\n1\narXiv:2310.16656v1  [cs.CV]  25 Oct 2023\n1. Abstract\nText-to-image diffusion models achieved a remarkable leap\nin capabilities over the last few years, enabling high-quality\nand diverse synthesis of images from a textual prompt. How-\never, even the most advanced models often struggle to pre-\ncisely follow all of the directions in their prompts. The vast\nmajority of these models are trained on datasets consisting\nof (image, caption) pairs where the images often come from\nthe web, and the captions are their HTML alternate text.\nA notable example is the LAION dataset, used by Stable\nDiffusion and other models. In this work we observe that\nthese captions are often of low quality, and argue that this\nsignificantly affects the model\u2019s capability to understand\nnuanced semantics in the textual prompts. We show that\nby relabeling the corpus with a specialized automatic cap-\ntioning model and training a text-to-image model on the\nrecaptioned dataset, the model benefits substantially across\nthe board. First, in overall image quality: e.g. FID 14.84 vs.\nthe baseline of 17.87, and 64.3% improvement in faithful\nimage generation according to human evaluation. Second, in\nsemantic alignment, e.g. semantic object accuracy 84.34 vs.\n78.90, counting alignment errors 1.32 vs. 1.44 and positional\nalignment 62.42 vs. 57.60. We analyze various ways to re-\nlabel the corpus and provide evidence that this technique,\nwhich we call RECAP, both reduces the train-inference dis-\ncrepancy and provides the model with more information\nper example, increasing sample efficiency and allowing the\nmodel to better understand the relations between captions\nand images.\n2. Introduction\nIn recent years, text-to-image (T2I) generation models such\nas Imagen [1], Muse [2], Dall-E [3], Dall-E 2 [4], Parti\n[5], and Stable Diffusion [6] have undergone significant ad-\nvancements. This progress has enabled the generation of\nremarkably high-quality and diverse images by conditioning\non textual inputs. However, while revolutionary, even mod-\nern state-of-the-art text-to-image models may fail to generate\nimages that fully convey the semantics and nuances from\nthe given textual prompts. Failure modes include: missing\none or more subjects from the input prompt [5, 7]; incor-\nrect binding of entities and modifiers [5, 7, 8]; and incorrect\nplacement and spatial composition of entities [5, 9, 10].\nIn this work we first observe that open-web datasets used\nto train open text-to-image models suffer from significant\nissues. For example, the captions in the LAION [11] dataset,\nused to train Stable Diffusion, come from alt HTML tags\n(Alttext). According to W3C\u2019s web content accessibility\nguidelines1, the alt attribute is used to convey the meaning\nand intent of the image, and not necessarily being a literal\n1https://www.w3.org/TR/2016/NOTE-WCAG20-TECHS-\n20161007/H37\ndescription of the image itself. Indeed, we observe, that\noften the Alttext describes only a narrow aspect of the image,\nneglecting significant visual details. For example, an image\nof a person can have as Alttext the name of the person and\nthe name of the photographer, but not a description of their\nappearance, their clothes, their position, or the background.\nAlso, sometimes Alttext tags contain inaccuracies, mistakes\nand out of context information. See Fig. 4 for examples.\nWe further observe that while trained mainly on similar\ndatasets of open (image, caption) pairs, recent automatic cap-\ntioning systems, such as PaLI [12], produce highly accurate\ncaptions. See examples in Fig. 3. This may be due to the fact\nthat the inverse problem of image-to-text (I2T) is easier, or\nto the fact that these captioning models are larger, are trained\nlonger than the T2I models, or leverage large pre-trained\nlanguage components.\nWith these observations, we suggest a new method for\nhorizontally improving T2I models by training them on im-\nproved captions, auto-generated by a custom I2T model.\nWe call our method RECAP, and show that applying it\nto Stable Diffusion results in a model that is better than\nthe baseline across the board, with a battery of standard\nmetrics substantially improving, e.g. FID 17.87\u219214.84, as\nwell as in human evaluation of successful image generation\n29.25% \u2192 48.06% (see Section 5).\nIn Sec. 4 we provide the details of our method, RECAP.\nSec. 5 discusses our results and shows the horizontal im-\nprovements both in image quality as well as in semantic\nfidelity to the prompts. In Sec. 6 we analyze the issues with\nthe original captions, demonstrate that the improvements are\nindeed due to the new captions, and that they arise as a result\nof both minimizing the train-test skew as well as increasing\nsample efficiency.\nTo summarize, our main contributions are:\n\u2022 A new method we call RECAP, that leverages automatic\ncaptioning to improve the quality of a text-to-image model\nin a substantial way horizontally, both in fidelity and se-\nmantics, measured on a set of 7 standard metrics as well\nas with human evaluations.\n\u2022 An analysis showing how Alttext captions used by current\ntraining methods suffer from train-inference skew and lack\nin semantic details, resulting in text-to-image models that\noften fail in fidelity and semantic alignment, and how\ndifferent captions mitigate both issues.\n3. Related Work\nText-to-Image Models. Deep generative models for image\ngeneration from text have shown notable progress in recent\nyears, transitioning from using GAN-based methods [13]\nto using autoregressive transformers [2, 3, 5] and diffusion\nmodels [1, 4, 14]. An important area of enhancement is\nin improving a model\u2019s capability to align with the input\ntext effectively. Methods condition the image on the output\n2\n1a. Manually \ncaption a \nsample\nSpecialized Image \nCaptioning Model\n1b. Finetune \nCaptioning Model\n2. Generate \ncaptions\nRECAP Image \nGeneration Model\n3. Train Image \nGeneration Model\nFigure 2. Schematic diagram of our method RECAP. In steps (1a) and (1b) we fine-tune an image-to-text captioning model on a small set of\ndetailed human captions. In step (2) we use this fine-tuned model to recaption the images in the training dataset of a text-to-image model,\nand with this dataset, in step (3) we train an image generation model with the recaptioned dataset.\nof a pre-trained text embedder, usually CLIP [15]. Imagen\n[1] shows that using a strong T5 [16] encoder significantly\nimproves text-image alignment. Parti [5] uses a SimVLM\nmodel [17] to annotate some of the training set images.\nImage captioning. Image captioning is a fundamental\nproblem in computer graphics. Recent models [12, 18\u201321]\nhave made significant progress on this task, thanks to high\nscale training data and utilizing pre-trained image and text\nmodels, allowing them to address a variety of multi-modal\ntasks. In this work we fine-tune PaLI [12] to recaption the\ntraining dataset of a text-to-image model.\nSynthetic multimodal datasets. Some concurrent works\naugment multimodal dataset via automatic means to improve\nthe capabilities of image captioning and embedding models.\nNguyen et al. [22] show that training CLIP on data with\ngenerated captions improves its performance. Li et al. [23]\nshow how to improve the BLIP model by iteratively remov-\ning captions with high loss, replacing them with captions\nfrom the previous epoch or generating a new image for them\nusing Stable Diffusion. Ma et al. [24] generate a synthetic\ntext-to-image model using Stable Diffusion and then uses it\nto train a captioning algorithm.\nImproving text-alignment of diffusion models. An im-\nportant line of work attempts to rectify text-to-image align-\nment issues in diffusion models. This is usually done by\nlexical analysis of the input prompt modifying the attention\nmaps throughout sampling. Chefer et al. [7] try to prevent\nthe diffusion models from ignoring certain tokens by guid-\ning the sample process to reweigh the token attention maps.\nRassin et al. [8] analyze the prompt to identify modifiers,\nand then uses a sample-time guidance to bind their atten-\ntion maps to those of their entities. Wu et al. [9] isolates\nphrases that describe individual entities and attends to them.\nPhung et al. [10] allows placing objects in certain region by\nboosting their attention in those regions.\nConcurrently with our work, Dall-E 3 [25] proposes to use\nan automatic captioning system to regenerate the captions\nused to train a T2I model. Our work uses an open model\n(Stable Diffusion) and we provide more details and focus\nmore on analysis and evaluation, but otherwise the main\nideas are very similar.\n4. Method\nOur method, RECAP consists of 3 steps: (1) fine-tune an\nautomatic captioning system to produce desirable labels; (2)\nrelabel the images from our text-to-image training set with\nthis automatic recaptioning system; and (3) train the text-\nto-image model on the dataset consisting of the images and\nnew captions. Fig. 2 visualizes the overall method.\n4.1. Training Dataset\nWe selected a subset of 10M photos from LAION-2B-en\nimproved Aesthetics dataset. We followed the data filters\nused to train Stable Diffusion versions 1.2-1.42, as fol-\nlows: aesthetics score \u2265 5.0, pwatermark < 0.5, nsfw\nis UNLIKELY and both height & width \u2265 512. We\nnote that this filtering operation might amplify biases in the\ndataset [26]. We further excluded an arbitrary subset of\n10K photos from the training set to be used as an internal\nvalidation set.\n4.2. Captioning Model\nWe used a pre-trained I2T captioning model (PaLI [12]).\nAs the model outputs are relatively terse and lack in detail,\nwe first collected a small set of 100 manual captions from\nhuman raters and fine-tuned the captioning model on that\nset.\nIn order to experiment with the effect of different caption-\ning distributions, the raters were asked to provide two types\n2https : / / huggingface . co / CompVis / stable -\ndiffusion\n3\nof captions. First, a detailed caption for each image, with\nthese instructions: \u201cDescribe what you see in each image\nusing 1-2 detailed sentences\u201d. Note that the instructions\nlimited the length to only 1-2 detailed sentences at most, due\nto CLIP\u2019s (the downstream text encoder) context size of only\n77 tokens, which longer captions exceeded. Second, we col-\nlected a short and less detailed caption for each image with\nthis instruction: \u201cDescribe what you see in each image using\na single short sentence\u201d. We did not iterate on the quality\nof these manual captions. Fig. 3 provides example captions\ngiven by the human raters, as well as those produced by the\nnon fine-tuned PaLI model.\nWith this small dataset, we fine-tuned PaLI for 300 steps,\nusing a learning rate of 4e-5, dropout rate of 0.1 and a batch\nsize of 64, mixing 50% short captions and 50% long captions\nfor multiple copies of the 100 images, using a different fixed\nconditioning prefix for the short vs. long captions. When\ngenerating captions from the fine-tuned model, we use the\nterms RECAP Short and RECAP Long to refer to genera-\ntions conditioned on the short and long prefixes respectively.\nExample outputs, as well as a comparison to the original\nAlttext captions can be found in Fig. 4. See Appendix C for\nadditional dozen random examples of the generated RECAP\ncaptions.\nOverall, the captions produced by this bespoke model\nimprove both of the issues above - their distribution better\nmatches inference time prompts and they contain much more\ndetail to improve sample efficiency (see Sec. 6.1.1).\n4.3. Image Generation Model\nNext, we fine-tuned Stable Diffusion v1.4 for an additional\n250k3 steps with a learning rate of 1e-5, batch size of 512,\nand prompt dropout rate of 0.14. We fine-tuned the model\ntraining both UNet and CLIP weights and used a 50%-50%\nmixture of RECAP Short and RECAP Long captions (RE-\nCAP Mix) as this performed best. The results in the main\ntext are all from this configuration.\nRECAP is independent of the sampling method, so we\ncan use any sampling method with it. That said, for all of\nthe experiments in this work, we used DDIM sampling with\n50 inference steps and a guidance scale of 7.5.\n5. Results\nWe compare the RECAP model to two models: Baseline and\nAlttext. Baseline is the Stable Diffusion v1.4 model. Alttext\nis the baseline model fine-tuned for the same number of\nsteps and on the same set of images as RECAP, but with the\n3When further fine-tuning the model for 1M steps, we observe better\nvisual results along with diminishing returns in the auto computed metrics.\nAll results in the paper are for 250K steps except Fig. 1 and the human\nevaluation, where we used a model fine-tuned for 1M steps.\n4Prompts with more than 77 tokens (CLIP\u2019s upper limit) were dropped.\nThis happened for <1% of the data in each training set.\noriginal captions (Alttext) instead of the RECAP captions.\nThe Alttext model resolves contamination concerns, as it\nincludes the exact set of images. In all the comparisons we\nused the same random seeds across models.\nWe compare the models using a variety of automated\nmetrics, human evaluation and a qualitative evaluation of\nexamples. We observe improvements in all metrics (see\nTabs. 1 and 2).\n5.1. Automated metrics\nWe evaluated the performance and semantic capabilities of\nthe RECAP model using a battery of metrics suggested by\n[27] (using their publicly available code) on the MS-COCO\nvalidation dataset. Tab. 1 contains a summary of the results.\nTo assess overall generation quality we use the stan-\ndard FID metric and observe that images generated with\nthe RECAP model have a significantly better score (17.87 \u2192\n14.84).\nIn addition, we assess the semantic capabilities of our\nmodel: to check that the model generates faithfully the re-\nquested objects we use Semantic Object Accuracy (80.80 \u2192\n86.17), to check the number of generated objects we use\nCounting Alignment errors (1.44 \u2192 1.32), to check that the\nlocations of objects are correct we use Positional Alignment\n(57.60 \u2192 62.42), and finally to check the overall adherence\nto the prompt we measure Clip score (92.78 \u2192 93.80).\nIn all the metrics, we see no improvement in the Alttext\nmodel compared to the baseline, proving the improvements\nstem from the captions themselves and not from the addi-\ntional training.\nNote that following [27], we omit the IS and O-IS metrics,\nas all Stable Diffusion models yield a higher score than real\nimages. As noted by [27], IS is better suited for datasets with\na single object and does not perform well for the MS-COCO\ndataset, containing multiple objects per each image.\nAdditional details on the automated metrics are in Appen-\ndices A.1 to A.4.\n5.2. Human evaluation\nFor complementary evaluation of model performance, we\nused human raters. Results are summarized in Tab. 2.\nRaters were asked to select images generated from each\nmodel, only if they successfully follow a given prompt. We\nevaluated once on 200 random prompts from the MS-COCO\nvalidation set, and separately on the challenging DrawBench\ndataset [1]. We presented four images (from different seeds)\nfor each prompt, using the same seeds across models.\nWe calculated two metrics: percentage of successful im-\nage generation across all prompts and seeds (i.e. given a\nprompt and a seed, the chance of a successful image gen-\neration); and percentage of at least one successful image\ngeneration for a given prompt, out of four seeds (i.e. given\na prompt, the chance of successfully generating an image\n4\nFigure 3. Examples of captions given by human raters, and the automatically generated caption from the non fine-tuned PaLI model. Photos\ntaken from LAION.\nfor it, in four attempts). We see a relative 64.3% improve-\nment in successful image generation on MS-COCO, and a\n41.7% improvement on DrawBench. We also see a relative\nimprovement of 42.1% in successful prompt generation on\nMS-COCO and 37.5% improvement on DrawBench. The\nAlttext model showed minor improvement on the MS-COCO\ndataset (12%-13%) and did not improve the DrawBench\ndataset. Further details can be found in Appendix A.5.\n5.3. Qualitative Results\nFig. 1 provides representative examples where RECAP out-\nperforms the base Stable Diffusion model (and sometimes\nalso larger models, see Appendix B).\nGenerally, we observe that RECAP can better interpret re-\nlations between entities. Prepositions like \"near\", \"through\",\n\"made off\" are often ignored by the base Stable Diffusion\nmodel. The RECAP model applies the prepositions correctly,\nwhile the base model often resorts to the most common rela-\ntion between the entities (based on the training data distribu-\ntion). For example, in the prompt \"A pizza near a pineapple\",\nthe base model places the pineapple on the pizza, as the prob-\nable relation between the two, while RECAP generates a\npineapple near it, as requested.\nRECAP also better handles cases where different modi-\nfiers are applied to multiple entities (e.g. \"A red bench and a\nyellow clock\"). The base model will treat the sentence as a\nbag of words, applying all modifiers to all entities or ignore\nsome of them. RECAP is also able to interpret complex\nmodifiers like anaphors. For examples, in the prompt \"Two\nflowers, one is blue and the other one is green\" it understands\nthat \"one\" refers to a flower.\n6. Analysis\nWe hypothesize that the underlying improvement in image\ngeneration quality, as measured in the results above, stems\nfrom two improvements in the training captions: (1) reducing\nthe discrepancy between the train and inference prompts, and\n(2) giving the model more textual information per image,\nthus improving the training sample efficiency. Below we\nprovide an ablation analysis showing that RECAP captions\nachieve both properties, and that the resulting model benefits\nfrom both.\n6.1. Comparing Different Caption Types\n6.1.1\nGenerated Captions\nTab. 3 compares the generated captions in the training set,\nto the caption in the MS-COCO validation set, on a variety\n5\nFigure 4. Examples of captions generated by the RECAP model conditioned on the short or long prefixes, the original PaLI model, and the\noriginal Alttext captions. Photos taken from LAION.\n6\nFID\u2193\nO-FID\u2193\nSOA-C\u2191\nSOA-I\u2191\nCA\u2193\nPA\u2191\nRP\u2191\nBaseline\n17.87\n8.19\n78.90\n80.80\n1.44\n57.60\n92.78\nAlttext\n17.53\n8.90\n78.99\n80.85\n1.47\n57.40\n91.32\nRECAP\n14.84\n6.23\n84.34\n86.17\n1.32\n62.42\n93.80\nReal Images\n2.62\n0.00\n90.02\n91.19\n1.05\n100.0\n83.54\nTable 1. Results for the automated metrics for RECAP model vs. baseline and Alttext models. RECAP model improves across all metrics.\nMS-COCO\nMS-COCO\nDrawBench\nDrawBench\nSuccessful Images\nSuccessful Prompts\nSuccessful Images\nSuccessful Prompts\nBaseline\n29.3%\n53.5%\n15.6%\n33.1%\nAlttext\n33.4%\n60.0%\n13.2%\n33.8%\nRECAP\n48.1%\n76.0%\n22.1%\n45.5%\nTable 2. Human evaluation results comparing RECAP, Alttext and Baseline models, on two benchmarks (MS-COCO and DrawBench) for\ntwo metrics: percentage of images generated that fully follow the prompt, and percentage of prompts with at least one generated image (out\nof four seeds) that fully followed the prompt.\nof language metrics. We used MS-COCO to represent the\nprompts we expect to see in inference time. We observe\nthat our generated captions are closer in distribution to MS-\nCOCO in several senses.\nFirst, we compare them by using standard language met-\nrics from the textstat python package5 calculated over the\n10M examples in each train dataset. Flesch Reading Ease\nscore is a standard measure for how difficult it is to read a\ntext in a given language. We observe that RECAP is able to\ngenerate easy to read sentences, while the original Alttext is\noften difficult to read. Similarly, the text_standard score is\na consensus score based on a battery of metrics to estimate\nthe smallest grade of a native speaking reader to be able to\nread the text. RECAP generates texts which a 4th grader\n(the lowest possible) can read, while the original Alttext is\nestimated to require 8th graders to fully understand.\nTo measure more directly the reduction in train-inference\nskew, we compare the distribution of the embedded texts.\nSince Stable Diffusion is using CLIP to encode the text,\nwe calculate the Fr\u00e9chet distance between the CLIP embed-\ndings of the various datasets6. Results are summarized in\nTab. 3. As suspected, RECAP generated captions are closer\nin distribution to the MS-COCO captions than the Alttext\ncaptions. Furthermore, the RECAP Short captions are closer\nto MS-COCO than RECAP Long captions. These results\nare in line with the improvement in FID of the images from\nmodels trained with the corresponding datasets, as detailed\nin Sec. 6.1.2.\nNext, we measured how well the automatically generated\ncaptions describe the images, using a human evaluation of\n100 random images, asking raters to score each caption con-\n5https://pypi.org/project/textstat\n6Similarly to the FID metric used to compare between image distribu-\ntions, we take the Fr\u00e9chet distance of the CLIP embedding distributions,\nmodeled as a Gaussian.\nsidering both faithfulness to the image, and completeness in\ndescribing the image, on a scale of 1-5. RECAP generated\ncaptions were rated as more faithful and complete (with an\naverage score of 3.58 for RECAP Short and 4.3 for RECAP\nLong) than Alttext captions (scoring on average 2.9). This\nsupports our hypothesis that automated captions make train-\ning more efficient by providing more textual information.\n6.1.2\nGenerated Images\nNext, we compared the results of fine-tuning Stable Diffu-\nsion on our two caption types, RECAP Long and RECAP\nShort. The results are summarized in Tab. 4. We observe\nthat training on RECAP Short captions achieves better FID\nscores, and faster, but with little semantic improvement,\nwhile the RECAP Long captions exhibit significant semantic\nimprovement (see representative metrics in Fig. 5). Mixing\nthe caption sets (RECAP Mix) provides the best of both\nworlds.\nNote that the improvement in FID on the MS-COCO\nvalidation set, is correlated to the improvement in the Fr\u00e9chet\ndistance of the generated captions, as detailed in Sec. 6.1.1.\nThis indicates that the semantic improvement in the RECAP\nLong model stems from improved training sample efficiency,\nand not only from reducing the train-inference skew.\n6.2. Training Different Model Weights\nTo further explore the contribution of the improved examples\nto each part of the Stable Diffusion model, we compared\ntraining only the UNet weights, vs. the CLIP weights, vs.\nboth, using the same training and evaluation procedures. For\nsimplicity, we only report the results for RECAP Mix vs.\nAlttext and baseline models.\nThe results are summarized in Tab. 5. Overall, as ex-\npected, training more weights achieves better performance.\n7\nAlttext\nRECAP Long\nRECAP Short\nMS-COCO\n# sentences\n1.15\n2.15\n1.00\n1.00\n# words\n11.38\n21.29\n5.89\n10.45\n# letters\n61.08\n87.96\n24.39\n41.67\nFlesch Reading Ease score\n45.71 (= Difficult)\n88.35 (= Easy)\n86.61 (= Easy)\n86.93 (= Easy)\ntext_standard score\n8.78 (8th grader)\n4.50 (4th grader)\n3.11 (4th grader)\n4.59 (4th grader)\nDistance to MS-COCO\n0.45\n0.24\n0.18\n0.00\nTable 3. Comparison of average statistics on the captions generated by RECAP vs. the original Alttext over the 10m examples in the training\nset, and the 10k examples in the MS-COCO validation set. Distance to MS-COCO is the Fr\u00e9chet distance of the CLIP embeddings of the\nRECAP caption sets (calculated on 250k samples) vs. the MS-COCO validation set. Note that since the embeddings are normalized, the\ndistance in between 0-1.\nFID\u2193\nO-FID\u2193\nSOA-C\u2191\nSOA-I\u2191\nCA\u2193\nPA\u2191\nRP\u2191\nBaseline\n17.87\n8.19\n78.90\n80.80\n1.44\n57.60\n92.78\nAlttext\n17.53\n8.90\n78.99\n80.85\n1.47\n57.40\n91.32\nRECAP Short\n14.85\n5.81\n79.81\n81.95\n1.43\n60.00\n93.30\nRECAP Long\n15.61\n7.48\n84.59\n86.16\n1.32\n63.27\n93.26\nRECAP Mix\n14.84\n6.23\n84.34\n86.17\n1.32\n62.42\n93.80\nReal Images\n2.62\n0.00\n90.02\n91.19\n1.05\n100.0\n83.54\nTable 4. Results for the automated metrics for RECAP models vs. baseline and Alttext models, comparing models that trained on different\nsets of captions.\nInterestingly, training only CLIP weights (which are \u223d 12%\nof the total weights) achieves better FID, with less training\nsteps, but with little semantic improvement. Training both\nCLIP and UNet weights results in significantly higher im-\nprovement to semantic scores than training only on one of\nthem. See Appendix A.6 for more details.\nWe believe that training CLIP weights mainly reduces the\nskew in the distribution of texts between the training set and\nthe evaluation set, while training the UNet weights mostly\nimproves the alignment of the text to the image.\n7. Summary and Discussion\nIn this paper, we show how text-to-image models can be\nimproved across the board by training on synthetically\ngenerated captions. We performed an in-depth analysis\ndemonstrating that short descriptions which narrow the train-\ninference gap are helpful, as are long and detailed descrip-\ntions that improve sample efficiency despite being different\nfrom the inference set. We further demonstrated that mix-\ning these descriptions in the training set improves all fronts\nsimultaneously.\nThere are several interesting directions for future work.\nIt would be interesting to check if by tuning the captioning\nmodel to produce ample detail in narrow areas, the same\nrecipe can be used to improve semantic capabilities in new\ndomains (for example, could we create models that can accu-\nrately generate hair styles, room designs, facial expressions,\nclothing, etc. based on detailed descriptions?). Similarly, it\nis possible to use RECAP to train T2I models in domains\nthat lack textual captions altogether (e.g., a personal photo\nalbum or screencaps of TV shows). We conducted initial\nexperiments here, and they show a lot of promise.\nWe experimented with fine-tuning a model with the RE-\nCAP captions, but it would be interesting to compare that to\na model that was pre-trained on the RECAP captions from\nscratch. Relatedly, it would be interesting to further experi-\nment with different mixtures of the three caption types we\nhave (RECAP Short, RECAP Long, and Alttext). Even more\ngenerally, we could imagine creating and mixing together\nseveral more flavors of recaptioning models. This could al-\nlow us to circumvent the token limit (by training on the same\nimage multiple times with a different subset of the caption\neach time). Regardless of token limit, it could be interesting\nto explore training on several shorter captions per image\ninstead of a single long one. It would also be interesting to\nexplore the effects of RECAP on larger models trained on\nlarger datasets.\nFinally, RECAP shows the importance of high quality\ndatasets, and that it can be improved with synthetic data, we\nhope that this provides yet another encouragement to apply\nsuch techniques even beyond the T2I domain.\n8. Acknowledgments\nWe would like to extend our gratitude to Eyal Molad, Matan\nKalman, Jason Baldridge, and the Theta Labs team at\nGoogle, for great reviews, suggestions, and support to this\npaper.\n8\nFID\n14\n15\n16\n17\n18\n0k\n50k\n100k\n150k\n200k\n250k\nAlttext\nRECAP Mix\nRECAP Short\nRECAP Long\nSOA-C\n78%\n80%\n82%\n84%\n86%\n0k\n50k\n100k\n150k\n200k\n250k\nAlttext\nRECAP Mix\nRECAP Short\nRECAP Long\nFigure 5. FID (left) and SOA (right) scores for various checkpoints of the Alttext model vs. RECAP models, comparing models that\ntrained on different sets of captions. 0 fine-tuning steps is vanilla Stable Diffusion 1.4. Lower FID is better. We see that RECAP Short\nachieves better FID and faster, but with no semantic improvement. RECAP Long achieves some FID improvement with significant semantic\nimprovement, and RECAP Mix achieves both.\nFID\u2193\nO-FID\u2193\nSOA-C\u2191\nSOA-I\u2191\nCA\u2193\nPA\u2191\nRP\u2191\nBaseline\n17.87\n8.19\n78.90\n80.80\n1.44\n57.60\n92.78\nAlttext UNet\n16.83\n8.20\n76.90\n78.72\n1.50\n56.05\n91.89\nAlttext CLIP\n17.04\n8.32\n79.08\n81.10\n1.52\n58.72\n91.34\nAlttext UNet+CLIP\n17.53\n8.90\n78.99\n80.85\n1.47\n57.40\n91.32\nRECAP Mix UNet\n15.49\n6.47\n82.15\n84.04\n1.40\n59.77\n92.76\nRECAP Mix CLIP\n14.60\n6.09\n80.19\n82.04\n1.37\n60.44\n92.48\nRECAP Mix UNet+CLIP\n14.84\n6.23\n84.34\n86.17\n1.32\n62.42\n93.80\nReal Images\n2.62\n0.00\n90.02\n91.19\n1.05\n100.0\n83.54\nTable 5. Results for the automated metrics for RECAP models vs. baseline and Alttext models, comparing models that trained different set\nof weights.\n9\nReferences\n[1] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho,\nDavid J Fleet, and Mohammad Norouzi. Photorealis-\ntic text-to-image diffusion models with deep language\nunderstanding, 2022. 2, 3, 4, 12\n[2] Huiwen\nChang,\nHan\nZhang,\nJarred\nBarber,\nAJ Maschinot,\nJose Lezama,\nLu Jiang,\nMing-\nHsuan Yang, Kevin Murphy, William T. Freeman,\nMichael Rubinstein, Yuanzhen Li, and Dilip Krishnan.\nMuse: Text-to-image generation via masked generative\ntransformers, 2023. 2\n[3] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation,\n2021. 2\n[4] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents, 2022. 2\n[5] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong,\nGunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander\nKu, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchin-\nson, Wei Han, Zarana Parekh, Xin Li, Han Zhang,\nJason Baldridge, and Yonghui Wu. Scaling autoregres-\nsive models for content-rich text-to-image generation,\n2022. 2, 3\n[6] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution im-\nage synthesis with latent diffusion models, 2022. 2\n[7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or. Attend-and-excite: Attention-based\nsemantic guidance for text-to-image diffusion models,\n2023. 2, 3\n[8] Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli\nRavfogel, Yoav Goldberg, and Gal Chechik. Linguistic\nbinding in diffusion models: Enhancing attribute corre-\nspondence through attention map alignment, 2023. 2,\n3\n[9] Qiucheng Wu, Yujian Liu, Handong Zhao, Trung Bui,\nZhe Lin, Yang Zhang, and Shiyu Chang. Harnessing\nthe spatial-temporal attention of diffusion models for\nhigh-fidelity text-to-image synthesis, 2023. 2, 3\n[10] Quynh Phung, Songwei Ge, and Jia-Bin Huang.\nGrounded text-to-image synthesis with attention re-\nfocusing, 2023. 2, 3\n[11] Christoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell\nWortsman, Patrick Schramowski, Srivatsa Kundurthy,\nKatherine Crowson, Ludwig Schmidt, Robert Kaczmar-\nczyk, and Jenia Jitsev. Laion-5b: An open large-scale\ndataset for training next generation image-text models,\n2022. 2\n[12] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-\nvanni, Piotr Padlewski, Daniel Salz, Sebastian Good-\nman, Adam Grycner, Basil Mustafa, Lucas Beyer,\nAlexander Kolesnikov, Joan Puigcerver, Nan Ding,\nKeran Rong, Hassan Akbari, Gaurav Mishra, Linting\nXue, Ashish Thapliyal, James Bradbury, Weicheng\nKuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol\nAyan, Carlos Riquelme, Andreas Steiner, Anelia An-\ngelova, Xiaohua Zhai, Neil Houlsby, and Radu Sori-\ncut. Pali: A jointly-scaled multilingual language-image\nmodel, 2023. 2, 3\n[13] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial\nnetworks, 2014. 2\n[14] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution im-\nage synthesis with latent diffusion models, 2021. 2\n[15] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable\nvisual models from natural language supervision, 2021.\n3\n[16] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J. Liu. Exploring the limits of transfer\nlearning with a unified text-to-text transformer. Journal\nof Machine Learning Research, 21(140):1\u201367, 2020. 3\n[17] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai,\nYulia Tsvetkov, and Yuan Cao. Simvlm: Simple vi-\nsual language model pretraining with weak supervision,\n2022. 3\n[18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models,\n2023. 3\n[19] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\nRoman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne Mon-\nteiro, Jacob Menick, Sebastian Borgeaud, Andrew\nBrock, Aida Nematzadeh, Sahand Sharifzadeh, Miko-\nlaj Binkowski, Ricardo Barreira, Oriol Vinyals, An-\ndrew Zisserman, and Karen Simonyan. Flamingo: a\nvisual language model for few-shot learning, 2022.\n[20] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung,\nMojtaba Seyedhosseini, and Yonghui Wu. Coca: Con-\ntrastive captioners are image-text foundation models,\n2022.\n10\n[21] Haotian Liu, Chunyuan Li, Qingyang Wu, and\nYong Jae Lee. Visual instruction tuning, 2023. 3\n[22] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco,\nSewoong Oh, and Ludwig Schmidt. Improving multi-\nmodal datasets with image captioning, 2023. 3\n[23] Wenyan Li, Jonas F. Lotz, Chen Qiu, and Desmond\nElliott. Data curation for image captioning with text-\nto-image generative models, 2023. 3\n[24] Feipeng Ma, Yizhou Zhou, Fengyun Rao, Yueyi Zhang,\nand Xiaoyan Sun. Text-only image captioning with\nmulti-context data generation, 2023. 3\n[25] James Betker, Gabriel Goh, Li Jing, Tim Brooks,\nJianfeng Wang, Linjie Lia, Long Ouyang, Juntang\nZhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Pra-\nfulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya\nRamesh. Improving image generation with better cap-\ntions.\nhttps://cdn.openai.com/papers/\ndall-e-3.pdf, 2023. 3\n[26] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel\nKahembwe. Multimodal datasets: misogyny, pornog-\nraphy, and malignant stereotypes, 2021. 3\n[27] Tan M. Dinh, Rang Nguyen, and Binh-Son Hua. Tise:\nBag of metrics for text-to-image synthesis evaluation,\n2022. 4, 12\n[28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained\nby a two time-scale update rule converge to a local nash\nequilibrium. In I. Guyon, U. Von Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Process-\ning Systems, volume 30. Curran Associates, Inc., 2017.\n12\n[29] Tobias Hinz, Stefan Heinrich, and Stefan Wermter. Se-\nmantic object accuracy for generative text-to-image\nsynthesis. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 44(3):1552\u20131565, mar 2022. 12\n[30] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan,\nX. Huang, and X. He. Attngan: Fine-grained text to im-\nage generation with attentional generative adversarial\nnetworks. In 2018 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n1316\u20131324, Los Alamitos, CA, USA, jun 2018. IEEE\nComputer Society. 12\n11\nA. Detailed Results\nThe following sub-sections explain in more detail the metrics\nused to evaluate our model, as well as provides plots of\nseveral metrics over the number of training steps.\nA.1. Image Realism\nFID [28] is a measure of how close two image datasets are\nin terms of the distribution of the semantic content across a\nlarge set of photos in each dataset. It is commonly used for\nevaluation of text-to-image models, by taking a paired set of\ntexts and images, and comparing the set of original images\nto a set of newly generated images from the texts. A lower\nscore means the distributions are more alike.\nO-FID [27] is a variant of FID where off-the-shelf object\ndetector crops out objects from the images first, and FID is\ncalculated on the cropped image sets, giving somewhat more\ngranular measurement of the distribution of objects.\nFig. 6 shows the calculated FID and O-FID scores for\nseveral checkpoints throughout training. It is clear that the\nRECAP model improves significantly the FID and O-FID\nscores over the MS-COCO dataset, while fine-tuning the\nsame amount of steps on the same set of images, but with\nthe original Alttext captions, produces only a minor improve-\nment.\nA.2. Semantic Object Accuracy\nThe SOA metric was suggested in [29] and is used to eval-\nuate the accuracy of a text-to-image generative model, by\nmeasuring how well it follows the instructions to generate\nspecific objects as part of a prompt. To do so, it uses off-\nthe-shelf specialized object detectors of 80 different classes\n(e.g. a motorcycle or a keyboard) on labeled data. There are\ntwo variants to this metric, one that averages across images\n(SOA-I) and another that averages across classes (SOA-C).\nResults can be found in Fig. 7. We see a very significant\nimprovement for the RECAP model vs. the baseline, while\nthe Alttext model does not show any improvement.\nA.3. Counting and Positional Alignments\nGenerative text to image models are known to struggle count-\ning objects, i.e. if a prompt specifies a specific number of\nobjects (e.g. \u201c3 birds\u201d) the model often generates a different\nnumber of objects. In the Counting Alignment (CA) metric\n[27], MS-COCO was filtered to prompts which contain a\nspecific instruction to generate a known number of objects.\nAn off-the-shelf counting model was used to count the num-\nber of expected objects in the generated image (per object\ntype). The lower the score the better.\nSimilarly, generative text to image models often struggle\nto follow positional cues, e.g. \u201ca girl in front of a boy\u201d or \u201ca\nplate of avocado under the table\u201d. In the Positional Align-\nment (PA) metric [27], MS-COCO was filtered to prompts\nwith specific positional cues. Each image generated by such\nprompt gets a CLIP score vs. the original prompt, and also\nvs. each replacement of the positional cue in the original\nprompt with a different (wrong) one (e.g. \"under\" instead of\n\"in front of\").\nResults for both metrics can be found in Fig. 8. Once\nagain, the RECAP model improves the baseline while the\nAlttext model does not.\nA.4. Text Alignment\nR-Precision metric (RP), also known as CLIP score, [30]\nis a popular measure to how close a prompt is to an image\ngenerated from it, by using the CLIP embeddings distance\nbetween each prompt and the image generated from it. How-\never, CLIP is also used by Stable Diffusion as the text en-\ncoder, producing a bias towards images generated by Stable\nDiffusion model, and in particular yielding higher RP score\nfor the generated images vs. the original real images. Still,\nwe see that relative to the base model (scoring 92.78), the\nAlttext model achieves lower score (91.31, probably due to\nthe small dataset size which reduces the training data vari-\nance), while the RECAP model improves it (93.8, despite\nthe small dataset size).\nA.5. Human Evaluation\nWe sent samples from the base, Alttext and RECAP models\nfor human evaluation. Raters were presented with four im-\nages for each model along with the prompt used to generate\nthe image. The instructions were to only select images that\nstrictly followed the prompt and did not contain any major\ndeformities (minor deformities are fairly common with Sta-\nble Diffusion 1.4). Presented results are averaged across\nraters.\nWe evaluate all of these models on 200 randomly sampled\nprompts from MS-COCO and the DrawBench dataset [1]7,\nwhich Stable Diffusion 1.4 is known to have difficulty with.\nThe RECAP model generates 64.3% (29.25% \u2192 48.06%)\nmore valid images, and is 42.1% (53.5% \u2192 76%) more\nlikely to be able to generate at least one valid image (out of\nfour seeds) over the base model. This indicates that RECAP\nis both better at generating images as well as being capable\nof following more difficult prompts (Tab. 2).\nA.6. Model Weights\nWe provide additional plots for the various models trained\nwith different subset of unfrozen weights, as described in\nSec. 6.2, in Figs. 9 and 10.\n72 prompts which were >77 CLIP tokens were dropped\n12\nFID\n14\n15\n16\n17\n18\n0k\n50k\n100k\n150k\n200k\n250k\nAlttext\nRECAP\nO-FID\n6\n7\n8\n9\n0k\n50k\n100k\n150k\n200k\n250k\nAlttext\nRECAP\nFigure 6. FID (left) and O-FID (right) scores for various checkpoints of the Alttext model vs. the RECAP model. 0 fine-tuning steps is\nvanilla Stable Diffusion 1.4. Lower score is better.\nSOA-I\n80%\n82%\n84%\n86%\n88%\n0k\n50k\n100k\n150k\n200k\n250k\nAlttext\nRECAP\nSOA-C\n78%\n80%\n82%\n84%\n86%\n0k\n50k\n100k\n150k\n200k\n250k\nAlttext\nRECAP\nFigure 7. Semantic Object Accuracy (SOA) scores for various checkpoints of the Alttext model vs. the RECAP model. 0 fine-tuning steps is\nvanilla Stable Diffusion 1.4. SOA-I (left) averages across images, while SOA-C (right) averages across classes.\nCA\n1.3\n1.4\n1.5\n1.6\n0k\n50k\n100k\n150k\n200k\n250k\nAlttext\nRECAP\nPA\n55%\n58%\n60%\n63%\n0k\n50k\n100k\n150k\n200k\n250k\nAlttext\nRECAP\nFigure 8. Counting Alignment (CA) (left) and Positional Alignment (PA) (right) scores for various checkpoints of the Alttext model vs. the\nRECAP model. 0 fine-tuning steps is vanilla Stable Diffusion 1.4. Lower CA score is better.\n13\nFID\n14\n15\n16\n17\n18\n0k\n50k\n100k\n150k\n200k\n250k\nAlttext UNet+CLIP\nAlttext UNet\nAlttext CLIP\nRECAP UNet+CLIP\nRECAP UNet\nRECAP CLIP\nO-FID\n6\n7\n8\n9\n0k\n50k\n100k\n150k\n200k\n250k\nAlttext UNet+CLIP\nAlttext UNet\nAlttext CLIP\nRECAP UNet+CLIP\nRECAP UNet\nRECAP CLIP\nFigure 9. FID (left) and O-FID (right) scores for various checkpoints of the Alttext model vs. RECAP models, comparing models that\ntrained on different set of weights. 0 fine-tuning steps is vanilla Stable Diffusion 1.4. Lower is better.\nSOA-I\n78%\n80%\n82%\n84%\n86%\n88%\n0k\n50k\n100k\n150k\n200k\n250k\nAlttext UNet+CLIP\nAlttext UNet\nAlttext CLIP\nRECAP UNet+CLIP\nRECAP UNet\nRECAP CLIP\nSOA-C\n76%\n78%\n80%\n82%\n84%\n86%\n0k\n50k\n100k\n150k\n200k\n250k\nAlttext UNet+CLIP\nAlttext UNet\nAlttext CLIP\nRECAP UNet+CLIP\nRECAP UNet\nRECAP CLIP\nFigure 10. Semantic Object Accuracy (SOA) scores for various checkpoints of the Alttext model vs. RECAP models, comparing models that\ntrained on different set of weights. 0 fine-tuning steps is vanilla Stable Diffusion 1.4. SOA-I (left) averages across images, while SOA-C\n(right) averages across classes.\n14\nB. Other Image Generation Models\nFig. 11 shows example images generated by SDXL and Midjourney for the prompts in the top figure.\nFigure 11. Examples of images generated by SDXL 1.0 and Midjourney 5.1 for the prompts in the top figure.\n15\nC. Additional Captioning Examples\nFig. 12 and Fig. 13 show additional example captions generated by RECAP versus the original Alttext.\nFigure 12. Examples of captions generated by RECAP variants vs. the original Alttext. Photos taken from LAION.\n16\nFigure 13. More examples of captions generated by RECAP variants vs. the original Alttext. Photos taken from LAION.\n17\n"
  },
  {
    "title": "DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior",
    "link": "https://arxiv.org/pdf/2310.16818.pdf",
    "upvote": "27",
    "text": "DREAMCRAFT3D: HIERARCHICAL 3D GENERATION\nWITH BOOTSTRAPPED DIFFUSION PRIOR\nJingxiang Sun1\u2217, Bo Zhang3\u2020, Ruizhi Shao1, Lizhen Wang1, Wen Liu2, Zhenda Xie2, Yebin Liu1\u2020\n1 Tsinghua University, 2 DeepSeek AI, 3 Independent Researcher\nABSTRACT\nWe present DreamCraft3D, a hierarchical 3D content generation method that pro-\nduces high-fidelity and coherent 3D objects. We tackle the problem by leveraging\na 2D reference image to guide the stages of geometry sculpting and texture boost-\ning. A central focus of this work is to address the consistency issue that existing\nworks encounter. To sculpt geometries that render coherently, we perform score\ndistillation sampling via a view-dependent diffusion model. This 3D prior, along-\nside several training strategies, prioritizes the geometry consistency but compro-\nmises the texture fidelity. We further propose Bootstrapped Score Distillation to\nspecifically boost the texture. We train a personalized diffusion model, Dream-\nbooth, on the augmented renderings of the scene, imbuing it with 3D knowledge\nof the scene being optimized. The score distillation from this 3D-aware diffu-\nsion prior provides view-consistent guidance for the scene. Notably, through an\nalternating optimization of the diffusion prior and 3D scene representation, we\nachieve mutually reinforcing improvements: the optimized 3D scene aids in train-\ning the scene-specific diffusion model, which offers increasingly view-consistent\nguidance for 3D optimization. The optimization is thus bootstrapped and leads to\nsubstantial texture boosting. With tailored 3D priors throughout the hierarchical\ngeneration, DreamCraft3D generates coherent 3D objects with photorealistic ren-\nderings, advancing the state-of-the-art in 3D content generation. Code available\nat https://github.com/deepseek-ai/DreamCraft3D.\n1\nINTRODUCTION\nThe remarkable success of 2D generative modeling (Saharia et al., 2022; Ramesh et al., 2022; Rom-\nbach et al., 2022; Gu et al., 2022) has profoundly shaped the way that we create visual content.\n3D content creation, which is crucial for applications like games, movies and virtual reality, still\npresents a significant challenge for deep generative networks. While 3D generative modeling has\nshown compelling results for certain categories (Wang et al., 2023a; Chan et al., 2022; Zhang et al.,\n2023b), generating general 3D objects remains formidable due to the lack of extensive 3D data.\nRecent research effort has sought to leverage the guidance of pretrained text-to-image generative\nmodels (Poole et al., 2022; Lin et al., 2023; Tang et al., 2023) and showcases promising results.\nThe idea of leveraging pretrained text-to-image (T2I) models for 3D generation is initially proposed\nby DreamFusion (Poole et al., 2022). A score distillation sampling (SDS) loss is enforced to opti-\nmize the 3D model such that its renderings at random viewpoints match the text-conditioned image\ndistribution as interpreted by a powerful T2I diffusion model. DreamFusion inherits the imaginative\npower of 2D generative models and can yield highly creative 3D assets. To deal with the over-\nsaturation and blurriness issues, recent works adopt stage-wise optimization strategies (Mildenhall\net al., 2021) or propose improved 2D distillation loss (Wang et al., 2023b), which leads to an en-\nhancement in photo-realism. However, the majority of current research falls short of synthesizing\ncomplex content as achieved by 2D generative models. In addition, these works are often plagued\nwith the \u201cJanus issue\u201d, where 3D renderings that appear plausible individually show semantic and\nstylistic inconsistencies when examined holistically.\n*Work partially done during the internship at DeepSeek AI.\n\u2020Corresponding authors.\n1\narXiv:2310.16818v2  [cs.CV]  26 Oct 2023\nA DSLR photo of spiderman\nA beagle in a detective's outfit\nA cute rabbit in a stunning, detailed Chinese coat\nHumoristic san goku body mixed with wild boar head \nrunning\nA DSLR photo of a chimpanzee dressed like Henry \nVIII king of England\nAn astronaut in a space suit, perched atop a \njagged lunar rock, gazing up at the stars.\nPortrait painting of batman with black \nleather armor, ultra realistic\nIsometric view of a MINI cute hyperrealistic\nfuturistic soldier cat wearing cyberpunk\nFigure 1: By lifting 2D images to 3D, DreamCraft3D achieves 3D generation with rich details and\nholistic 3D consistency. Please refer to the Appendix and the demo video for more results.\n2\nIn this paper, we propose DreamCraft3D, an approach to produce complex 3D assets while main-\ntaining holistic 3D consistency. Our approach explores the potential of hierarchical generation. We\ndraw inspiration from the manual artistic process: an abstract concept is first solidified into a 2D\ndraft, followed by the sculpting of rough geometry, the refinement of the geometric details and the\npainting of high-fidelity textures. We adopt a similar approach, breaking down the challenging 3D\ngeneration into manageable steps. Starting with a high-quality 2D reference image generated from\na text prompt, we lift it into 3D via stages of geometry sculpting and texture boosting. Contrary to\nprior approaches, our work highlights how careful consideration of each stage can unleash the full\npotential of hierarchical generation, resulting in superior-quality 3D creation.\nThe geometry sculpting stage aims to produce plausible and consistent 3D geometry from the 2D\nreference image. On top of using the SDS loss for novel views and photometric loss at the reference\nview, we introduce multiple strategies to promote geometric consistency. Foremost, we leverage\nan off-the-shelf viewpoint-conditioned image translation model, Zero-1-to-3 (Liu et al., 2023b), to\nmodel the distribution of novel views based on the reference image. Since this view-conditioned\ndiffusion model is trained on diverse 3D data (Deitke et al., 2023), it provides a rich 3D prior that\ncomplements the 2D diffusion prior. Additionally, we find annealing the sampling timestep and pro-\ngressively enlarging training views are crucial to further improve coherency. During optimization,\nwe transition from implicit surface representation (Wang et al., 2021) to mesh representation (Shen\net al., 2021) for coarse-to-fine geometry refinement. Through these techniques, the geometry sculpt-\ning stage produces sharp, detailed geometry while effectively suppressing most geometric artifacts.\nWe further propose bootstrapped score distillation to substantially boost the texture. Existing view-\nconditioned diffusion models trained on limited 3D often struggle to match the fidelity of modern 2D\ndiffusion models. Instead, we finetune the diffusion model according to multi-view renderings of the\n3D instance being optimized. This personalized 3D-aware generative prior becomes instrumental in\naugmenting the 3D texture while ensuring view consistency. Importantly, we find that alternatively\noptimizing the generative prior and 3D representation leads to mutually reinforcing improvements.\nThe diffusion model benefits from training on improved multi-view renderings, which in turn pro-\nvides superior guidance for optimizing the 3D texture. In contrast to prior works (Poole et al., 2022;\nWang et al., 2023b) that distill from a fixed target distribution, we learn from a distribution that\ngradually evolves according to the optimization state. Through this \u201cbootstrapping\u201d, our approach\ncaptures increasingly detailed texture while keeping the view consistency.\nAs shown in Figure 1, our method is capable of producing creative 3D assets with intricate geomet-\nric structures and realistic textures rendered coherently in 360\u25e6. Compared to optimization-based\napproaches (Poole et al., 2022; Lin et al., 2023), our method offers substantially improved texture\nand complexity. Meanwhile, compared to image-to-3D techniques (Tang et al., 2023; Qian et al.,\n2023), our work excels at producing unprecedented realistic renderings in 360\u25e6renderings. These\nresults suggest the strong potential of DreamCraft3D in enabling new creative possibilities in 3D\ncontent creation. The full implementation will be made publicly available.\n2\nRELATED WORK\n3D generative models have been intensively studied to generate 3D assets without tedious manual\ncreation. Generative adversarial networks (GANs) (Chan et al., 2021; 2022; 2021; Xie et al., 2021;\nZeng et al., 2022; Skorokhodov et al., 2023; Gao et al., 2022; Tang et al., 2022; Xie et al., 2021; Sun\net al., 2023; 2022) have long been the prominent techniques in the field. Auto-regressive models\nhave been explored (Sanghi et al., 2022; Mittal et al., 2022; Yan et al., 2022; Zhang et al., 2022;\nYu et al., 2023), which learn the distribution of these 3D shapes conditioned on images or texts.\nDiffusion models (Wang et al., 2023a; Cheng et al., 2023; Li et al., 2023; Nam et al., 2022; Zhang\net al., 2023a; Nichol et al., 2022; Jun & Nichol, 2023; Bautista et al., 2022; Gupta et al., 2023) have\nalso shown significant recent success in learning probabilistic mappings from text or images to 3D\nshape latent. However, these methods require 3D shapes or multi-view data for training, raising\nchallenges when generating in-the-wild 3D assets due to the scarcity of diverse 3D data (Chang\net al., 2015; Deitke et al., 2023; Wu et al., 2023) compared to 2D.\n3D-aware image generation aims to render images in novel views while offering some level of\n3D consistency. These works (Sargent et al., 2023; Skorokhodov et al., 2023; Xiang et al., 2023)\noften rely on a pretrained monocular depth prediction model to synthesize view-consistent images.\n3\nWhile they achieve photo-realistic renderings for categories of ImageNet, they fall short in producing\nresults in large views. There are a few recent attempts (Watson et al., 2022; Liu et al., 2023b) that\ntrain view-dependent diffusion models on 3D data and demonstrate promising novel view synthesis\ncapability for open domain. However, these are inherently 2D models and cannot ensure perfect\nview consistency.\nLifting 2D to 3D approaches improve a 3D scene representation by seeking guidance using estb-\nlished 2D text-image foundation models. Early works (Jain et al., 2022; Lee & Chang, 2022; Hong\net al., 2022) utilize the pretrained CLIP (Radford et al., 2021) model to maximize the similarity\nbetween rendered images and text prompt. DreamFusion (Poole et al., 2022) and SJC (Wang et al.,\n2022), on the other hand, propose to distill the score of image distribution from a pretrained diffusion\nmodel and demonstrate promising results. Recent works have sought to further enhance the texture\nrealism via coarse-to-fine optimization (Lin et al., 2023; Chen et al., 2023), improved distillation\nloss (Wang et al., 2023b), shape guidance (Metzer et al., 2023) or lifting 2D image to 3D (Deng\net al., 2023; Tang et al., 2023; Qian et al., 2023; Liu et al., 2023a). Recently, Raj et al. (2023) pro-\nposes to finetune a personalized diffusion model for 3D consistent generation. However, producing\nglobally consistent 3D remains challenging. In this work, we meticulously design 3D priors through\nthe whole hierarchical generation process, achieving unprecedented coherent 3D generation.\n3\nPRELIMINARIES\nDreamFusion (Poole et al., 2022) achieves text-to-3D generation by utilizing a pretrained text-to-\nimage diffusion model \u03f5\u03d5 as an image prior to optimize the 3D representation parameterized by \u03b8.\nThe image x = g(\u03b8), rendered at random viewpoints by a volumetric renderer, is expected to rep-\nresent a sample drawn from the text-conditioned image distribution p(x|y) modeled by a pretrained\ndiffusion model. The diffusion model \u03d5 is trained to predict the sampled noise \u03f5\u03d5(xt; y, t) of the\nnoisy image xt at the noise level t, conditioned on the text prompt y. A score distillation sam-\npling (SDS) loss encourages the rendered images to match the distribution modeled by the diffusion\nmodel. Specifically, the SDS loss computes the gradient:\n\u2207\u03b8LSDS(\u03d5, g(\u03b8)) = Et,\u03f5\nh\n\u03c9(t)(\u03f5\u03d5(xt; y, t) \u2212 \u03f5)\u2202x\n\u2202\u03b8\ni\n,\n(1)\nwhich is the per-pixel difference between the predicted and the added noise upon the rendered image,\nwhere \u03c9(t) is the weighting function.\nOne way to improve the generation quality of a conditional diffusion model is to use the classifier-\nfree guidance (CFG) technique to steer the sampling slightly away from the unconditional sampling,\ni.e., \u03f5\u03d5(xt; y, t)+\u03f5\u03d5(xt; y, t)\u2212\u03f5\u03d5(xt, t, \u2205), where \u2205 represents the \u201cempty\u201d text prompt. Typically,\nthe SDS loss requires a large CFG guidance weight for high-quality text-to-3D generation, yet this\nwill bring side effects like over-saturation and over-smoothing (Poole et al., 2022).\nRecently, Wang et al. (2023b) proposed a variational score distillation (VSD) loss that is friendly\nto standard CFG guidance strength and better resolves unnatural textures. Instead of seeking a\nsingle data point, this approach regards the solution corresponding to a text prompt as a random\nvariable. Specifically, VSD optimizes a distribution q\u00b5(x0|y) of the possible 3D representations\n\u00b5(\u03b8|y) corresponding to the text y, to be closely aligned with the distribution defined by the diffusion\ntimestep t = 0, p(x0|y), in terms of KL divergence:\nLVSD = DKL(q\u00b5(x0|y)||p(x0|y)).\n(2)\nWang et al. (2023b) further shows that this objective can be optimized by matching the score of\nnoisy real images and that of noisy rendered images at each time t, so the gradient of LVSD is\n\u2207\u03b8LVSD(\u03d5, g(\u03b8)) = Et,\u03f5\nh\n\u03c9(t)(\u03f5\u03d5(xt; y, t) \u2212 \u03f5lora(xt; y, t, c))\u2202x\n\u2202\u03b8\ni\n.\n(3)\nHere, \u03f5lora estimates the score of the rendered images using a LoRA (Low-rank adaptation) (Hu\net al., 2021) model. The obtained variational distribution yields samples with high-fidelity textures.\nHowever, this loss is applied for texture enhancement and is helpless to the coarse geometry initially\nlearned by SDS. Moreover, both the SDS and VSD attempt to distill from a fixed target 2D distri-\nbution which only assures per-view plausibility rather than a global 3D consistency. Consequently,\nthey suffer from the same appearance and semantic shift issue that hampers the perceived 3D quality.\n4\n\u201ca [V] astronaut\u201d\nNovel View\nReference \nImage\nNoise\nFrom Implicit surface (NeuS) \nto 3D Mesh (DMTet)\nEstimated \nNormal\nReference \nview\nNovel Views\nText-to-Image \nDiffusion\nView-conditioned \nDiffusion\nDreamBooth\nText-to-Image \nDiffusion\n\u201can astronaut in \nsand beach\u201d\nAugmented \nrenderings\nGeometry Sculpting Result\nTexture Boosting Result\nLBSD\nLSDS, L3D-SDS\nFigure 2: DreamCraft3D leverages a 2D image generated from the text prompt and uses it to guide\nthe stages of geometry sculpting and texture boosting. When sculpting the geometry, the view-\nconditioned diffusion model provides crucial 3D guidance to ensure geometric consistency. We\nthen dedicately improve the texture quality by conducting a cyclic optimization. We augment the\nmulti-view renderings and use them to finetune a diffusion model, DreamBooth, to offer multi-view\nconsistent gradients to optimize the scene. We term the loss that distills from an evolving diffusion\nprior as bootstrapped distillation sampling (LBSD in the figure).\n4\nDREAMCRAFT3D\nWe propose a hierarchical pipeline for 3D content generation as illustrated in Figure 2. Our method\nfirst leverages a state-of-the-art text-to-image generative model to generate a high-quality 2D image\nfrom a text prompt. In this way, we can leverage the full power of state-of-the-art 2D diffusion mod-\nels to depict intricate visual semantics described in the text, retaining the creative freedom as 2D\nmodels. We then lift this image to 3D through cascaded stages of geometric sculpting and texture\nboosting. By decomposing the problem, we can apply specialized techniques at each stage. For\ngeometry, we prioritize multi-view consistency and global 3D structure, allowing for some com-\npromise on detailed textures. With the geometry fixed, we then focus solely on optimizing realistic\nand coherent texture, for which we jointly learn a 3D-aware diffusion prior that bootstraps the 3D\noptimization. In the next, we elaborate on key design considerations for the two phases.\n4.1\nGEOMETRY SCULPTING\nAt this stage, we aim to craft a 3D model such that it matches the appearance of the reference\nimage \u02c6x at the same reference view while maintaining plausibility under different viewing angles.\nTo achieve this, we encourage plausible image renderings for each randomly sampled view, recog-\nnizable by a pretrained diffusion model. This is achieved using the SDS loss LSDS, as defined in\nEquation 1. In order to effectively utilize guidance from the reference image, we penalize the pho-\ntometric difference between the rendered image and the reference via Lrgb = \u2225 \u02c6\nm \u2299 (\u02c6x \u2212 g(\u03b8; \u02c6c))\u22252\nat the reference view \u02c6c. The loss is computed only within the foreground region denoted by the\nmask \u02c6\nm. Meanwhile, we implement the mask loss Lmask = \u2225 \u02c6\nm \u2212 gm(\u03b8; \u02c6c)\u22252 to encourage scene\nsparsity, where gm renders the silhouette. In addition, akin to (Deng et al., 2023), we fully exploit\nthe geometry prior inferred from the reference image, and enforce the consistency with the depth\nand normal map computed for the reference view. The corresponding depth and normal loss are\n5\nrespectively computed as:\nLdepth = \u2212conv(d, \u02c6d)\n\u03c3(d)\u03c3( \u02c6d)\n,\nLnormal = \u2212\nn \u00b7 \u02c6n\n\u2225n\u22252 \u00b7 \u2225\u02c6n\u22252\n,\n(4)\nwhere conv(\u00b7) and \u03c3(\u00b7) represent the covariance and variance operators respectively, and the depth\n\u02c6d and the normal \u02c6n at the reference view are computed using the off-the-shelf single-view estima-\ntor (Eftekhar et al., 2021). The depth loss adopts the form of negative Pearson correlation Ldepth to\naccount for the scale mismatch in depth.\nDespite these, maintaining consistent semantics and appearance across back-views remains a chal-\nlenge. Thus, we employ additional techniques to produce coherent, detailed geometry.\n3D-aware diffusion prior. We argue that the 3D optimization with per-view supervision alone is\nunder-constrained. Hence, we utilize a view-conditioned diffusion model, Zero-1-to-3, which is\ntrained on a large scale of 3D assets and offers an improved viewpoint awareness. The Zero-1-to-3\nis a fine-tuned 2D diffusion model, which hallucinates the image in a relative camera pose c given\nthe reference image \u02c6x. This 3D-aware model encodes richer 3D knowledge of the visual world and\nallows us to better extrapolate the views given a reference image. As such, we distill the probability\ndensity from this model and compute the gradient of a 3D-aware SDS loss for novel views:\n\u2207\u03b8L3D-SDS(\u03d5, g(\u03b8)) = Et,\u03f5[\u03c9(t)(\u03f5\u03d5(xt; \u02c6x, c, y, t) \u2212 \u03f5)\u2202x\n\u2202\u03b8 ].\n(5)\nThis loss effectively alleviates 3D consistency issues like Janus problem. However, the finetuning on\nlimited categories of 3D data of inferior rendering quality impairs the diffusion model\u2019s generation\ncapability, so the 3D-aware SDS loss alone is prone to induce deteriorated quality when lifting\ngeneral images to 3D. Therefore, we employ a hybrid SDS loss, which incorporates both the 2D and\n3D diffusion priors simultaneously. Formally, this hybrid SDS loss provides the gradient as:\n\u2207\u03b8Lhybrid(\u03d5, g(\u03b8)) = \u2207\u03b8LSDS(\u03d5, g(\u03b8)) + \u00b5\u2207\u03b8L3D-SDS(\u03d5, g(\u03b8)),\n(6)\nwhere we choose \u00b5 = 2 to emphasize the weight of the 3D diffusion prior. When computing LSDS,\nwe adopt the DeepFloyd IF base model (Shonenkov et al., 2023), a diffusion model that operates at\n64 \u00d7 64 resolution pixel space and better captures coarse geometry.\nProgressive view training. However, directly deriving the free views in 360\u25e6may still result in\ngeometric artifacts, such as extra chair legs, due to the ambiguity inherent in a single reference\nimage. To solve this, we progressively enlarge the training views, gradually propagating the well-\nestablished geometry to 360\u25e6results.\nDiffusion timestep annealing. To align with the coarse-to-fine progression of 3D optimization,\nwe adopt a diffusion timestep annealing strategy similar to Huang et al. (2023). At the start of\noptimization, we prioritize sampling larger diffusion timestep t from the range [0.7, 0.85] when\ncomputing Equation 6 to provide the global structure. As training proceeds, we linearly anneal\nthe t sampling range to [0.2, 0.5] over hundreds of iterations. This annealing strategy allows the\nmodel to first establish a plausible global geometry in the early optimization phase before refining\nthe structural details.\nDetailed structural enhancement. We initially optimize an implicit surface representation with\nthe corresponding volume rendering as in NeuS (Wang et al., 2021) to establish the coarse structure.\nThen, following Lin et al. (2023), we use this result to initialize a textured 3D mesh representation\nusing a deformable tetrahedral grid (DMTet) (Shen et al., 2021) to facilitate high-resolution details.\nMoreover, this representation disentangles the learning of geometry and texture. Hence, at the end\nof this structural enhancement, we are able to solely refine the texture and better preserve high-\nfrequency details from the reference image.\n4.2\nTEXTURE BOOSTING VIA BOOTSTRAPPED SCORE SAMPLING\nThe geometry sculpting stage prioritizes the learning of coherent and detailed geometry but leaves\nthe texture blurry. This is due to our reliance on a 2D prior model that operates at a coarse resolution,\nand the limited sharpness offered by the 3D-aware diffusion model. Additionally, texture issues such\nas over-smoothing and over-saturation arise from excessively large classifier-free guidance.\n6\nTo augment the texture realism, we use variational score distillation (VSD) loss, as detailed in Equa-\ntion 3. We switch to the Stable Diffusion model (Rombach et al., 2021) in this stage which offers\nhigh-resolution gradients. To promote realistic rendering, we exclusively optimize the mesh texture\nwith the tetrahedral grid fixed. In this learning stage, we do not leverage the Zero-1-to-3 model as\nthe 3D prior since it adversely impacts the texture quality. Nonetheless, the inconsistent textures\nmay come back, resulting in bizarre 3D outcomes.\nWe observe that the multi-view renderings from the last stage, despite some blurriness, exhibit good\n3D consistency. One idea is to adapt a pretrained 2D diffusion model using these rendering results,\nenabling the model to form a concept about the scene\u2019s surrounding views. In light of this, we\nfinetune the diffusion model with the multi-view image renderings {x}, using DreamBooth (Ruiz\net al., 2023). Specifically, we incorporate the text prompts containing a unique identifier and the\nsubject\u2019s class name (e.g., \u201cA [V] astronaut\u201d in Figure 2). During finetuning, the camera parameter\nof each view is introduced as an additional condition. In practice, we train the DreamBooth with\n\u201caugmented\u201d image renderings, xr = rt\u2032(x). We introduce Gaussian noises, in an amount specified\nby the diffusion timestep t\u2032, to the multi-view renderings, i.e., xt\u2032 = \u03b1t\u2032x0 + \u03c3t\u2032\u03f5 (\u03b1t\u2032, \u03c3t\u2032 > 0\nare hyperparameters), which are restored using the diffusion model. By choosing a large t\u2032, these\naugmented images reveal high-frequency details at the cost of the fidelity to the original renderings.\nThe DreamBooth model trained on these augmented renderings can serve as a 3D prior to guide\ntexture refinement.\nFurther, we propose to alternatively optimize the 3D scene to facilitate a bootstrapped optimization\n(Figure 2). Initially, the 3D mesh yields blurry multi-view renderings. We adopt a large diffusion t\u2032\nto augment their texture quality while introducing some 3D inconsistency. The DreamBooth model\ntrained on these augmented renderings obtains a unified 3D concept of the scene to guide texture\nrefinement. As the 3D mesh reveals finer textures, we reduce the diffusion noises introduced to\nthe image renderings, so the DreamBooth model learns from more consistent renderings and better\ncaptures the image distribution faithful to evolving views. In this cyclic process, the 3D mesh and\ndiffusion prior mutually improve in a bootstrapped manner. Formally, we derive the 3D optimization\ngradient using the following bootstrapped score distillation (BSD) loss:\n\u2207\u03b8LBSD(\u03d5, g(\u03b8)) = Et,\u03f5,c[\u03c9(t)(\u03f5DreamBooth(xt; y, t, rt\u2032(x), c) \u2212 \u03f5lora(xt; y, t, x, c))\u2202x\n\u2202\u03b8 ].\n(7)\nContrary to prior works (Poole et al., 2022; Wang et al., 2023b) that distill the score function from\na fixed 2D model, our BSD loss learns from an evolving model which becomes increasingly 3D\nconsistent by drawing feedback from the ongoing crafted 3D model. In our experiments, we alternate\nthe optimization twice, which suffices to produce consistent textures with rich details.\n5\nEXPERIMENTS\n5.1\nIMPLEMENTATION DETAILS\nArchitectural details. In the geometry sculpting stage, we use Neus and textured 3D mesh repre-\nsentations. We employ Instant NGP (M\u00a8uller et al., 2022), optimizing from a 64 to a 384 resolution.\nFor the textured mesh, we use DMTet at a 128 grid and 512 rendering resolution.\nOptimization. During mesh refinement, we iteratively render a guided normal map and an RGB\nimage, enhancing geometric detail and optimizing our texture prediction network for consistency.\nConsidering the given plausible global geometric structure, our approach eschews the use of a 3D\nprior during texture optimization. We leverage random sampling of the camera radius and field-of-\nview (FOV) angles, aligning with Dreamfusion\u2019s methodology. This results in improved texture and\ngeometry details via alternate rendering of normal maps and RGB images.\n5.2\nCOMPARISONS WITH THE STATE OF THE ARTS\nBaselines. We conduct a comparative analysis of our technique against five baseline methods. The\nfirst three are text-to-3D methods: DreamFusion (Poole et al., 2022), Magic3D (Lin et al., 2023) and\nProlificDreamer (Wang et al., 2023b). We also compare our method against two image-to-3d meth-\nods: Make-it-3D (Tang et al., 2023) and Magic123 (Qian et al., 2023). For DreamFusion, Magic3D,\n7\nA beagle in a detective's outfit\nA Minion wearing the cloths of Spiderman\nSuper Saiyan Goku unleashe a massive energy wave\nReference\nOurs\nMagic123\nMake-It-3D\nDreamFusion\nMagic3D\nProlificDreamer\nFigure 3: Qualitative comparison with baselines. Our method generates sharper and more plausible\ndetails in both geometry and texture. Note that our method generates rich texture detail at novel\nviews and eliminates multi-face Janus problems.\nDreamBooth Times\nRendering Views\nReference\n\u201c3D Pixar Lionel Messi artfully \nkicking paint-filled bottles\u201d\nFigure 4: Improved view consistency and texture fidelity along bootstrapping.\nMagic123 and ProlificDreamer, we utilize their implementations in the Threestudio library (Guo\net al., 2023) for comparison. For Make-it-3D, we use its official implementation.\nTable 1: Quantitative comparison against prior 2D-to-\n3D lifting methods. The metrics are measured on 300\ngenerated samples.\nCLIP \u2191 Contextual \u2193 PSNR \u2191 LPIPS \u2193\nMake-it-3D\n0.872\n1.609\n18.937\n0.054\nMagic123\n0.843\n1.628\n22.838\n0.053\nDreamCraft3D 0.896\n1.579\n31.801\n0.005\nDatasets. We establish a test benchmark\nthat includes 300 images, which is a mix\nof real pictures and those produced by Sta-\nble Diffusion (Rombach et al., 2021) and\nDeep Floyd. Each image in this bench-\nmark comes with an alpha mask for the\nforeground, a predicted depth map, and\na text prompt. For real images, the text\nprompts are sourced from an image cap-\ntion model. We intend to make this test\nbenchmark accessible to the public.\n8\n(a) w/o Zero1-to-3\n(b) SDS\n(c) VSD\n(d) 1-round BSD (e) 2-round BSD\nFigure 6: Ablation study of the effectiveness of 3D prior and our proposed BSD (Bootstrapped\nScore Distillation). (a) geometry sculpting stage without 3D prior. (b) texture optimization with\nSDS loss. (c) VSD loss produces richer texture detail while suffering from texture inconsistency.\n(d) BSD improves the texture consistency with one round DreamBooth. (e) Two-round BSD adds\nmore details to the generated result.\n\u201ca DSLR photo of a corgi \nwearing a beret and holding \na baguette, standing up on \ntwo hind legs\u201d\nFigure 7: Continual improvement of geometry and texture quality through multiple stages.\nQuantitative comparison. To generate compelling 3D content that resembles the input image and\nconsistently conveys semantics from various perspectives, we compare our technique with estab-\nlished baselines using a quantitative analysis. Our evaluation employed four metrics: LPIPS (Zhang\net al., 2018) and PSNR for fidelity measurement at the reference viewpoint; Contextual Dis-\ntance (Mechrez et al., 2018) for pixel-level congruence assessment; and CLIP score (Radford et al.,\n2021) to estimate semantic coherence. Results depicted in Table 1 indicate that our approach signif-\nicantly surpasses the baselines in maintaining both texture consistency and fidelity.\nFigure 5: User study.\nUser study. To substantiate the robustness and quality of\nour proposed model, we executed a user study employ-\ning 15 distinct pairs of prompts and images. Each partic-\nipant was provided with four free-view rendering video\nalongside their corresponding textual input, and asked to\nchoose their top preferred 3D model. The study gathered\n480 responses from a total of 32 participants, the analysis\nof which is depicted in Figure 5. On an average basis, our\nmodel was favored by 92% of users over alternative mod-\nels, outperforming the baselines by a large margin. This\nresult provides compelling evidence of the resilience and\nsuperior quality inherent to our proposed method.\nQualitative comparison. Figure 3 compares our method\nwith the baselines. All the text-to-3D methods suffer from\nmulti-view consistency issues. While ProlificDreamer of-\nfers realistic textures, it fails to form a plausible 3D ob-\nject. Image-to-3D methods like Make-it-3D create quality frontal views but struggle with geometry.\nMagic123, enhanced by Zero1-to-3, fares better in geometry regularization, but both generate overly\nsmoothed textures and geometric details. In contrast, our Bootstrapped Score Distillation method\nimproves imagination diversity while maintaining semantic consistency.\n9\n5.3\nANALYSIS\nThe effect of 3D prior. In our paper we claim that the guidance offered by a 3D prior enhances the\ngeneration of globally plausible geometry. To ascertain its impact, an ablation study is conducted,\nwhere the 3D prior is deactivated. Figure 6 demonstrates that, in the absence of the 3D prior, the\nresultant character tends to exhibit the multifaceted Janus issue and suffers from irregular geometry.\nThis observation underlines the significance of a viewpoint-aware 3D prior in regulating a globally\nconsistent shape.\nThe effect of BSD. Figure 6 also presents an ablation study encompassing three texture optimiza-\ntion techniques: (1) BSD, (2) VSD, and (3) Score Distillation Sampling (SDS) with the traditional\nstable diffusion. The application of SDS has been observed to generate novel-view textures that are\nexcessively smooth and over-saturated. In contrast, while VSD using standard stable diffusion can\nproduce realistic textures, it yields a notably high inconsistency. In contrast, our proposed approach\nsuccessfully generates textures that strike a balance between realism and consistency.\nVisualization of multiple stages. Figure 7 provides the visualization of the intermediate rendering\nresults for each stage in our hierarchical pipeline. In the geometry sculpting stage, we convert Neus\nto DMTet to improve high-resolution geometry details. However, the improvement in texture is\nnegligible. On the contrary, in the texture stage, we significantly improve the texture quality with\nour proposed BSD.\nDreamBooth times. Figure 4 illustrates multi-view datasets for DreamBooth. The initial stage\ninvolves the introduction of substantial noise into each image to amplify detail richness, leading to\ninconsistent denoised images. However, as the textured mesh undergoes optimization, the produced\nrenderings evolve towards increased consistency and photorealism, thereby enhancing the quality of\nthe input dataset tailored for DreamBooth.\n6\nCONCLUSION\nWe have presented DreamCraft3D, an innovative approach that advances the field of complex 3D\nasset generation. This work introduces a meticulous geometry sculpting phase for producing plau-\nsible and coherent 3D geometries and a novel Bootstrapped Score Distillation strategy. The latter,\nby distilling from an optimizing 3D-aware diffusion prior and adapting to multi-view renderings of\nthe instance being optimized, significantly improves texture quality and consistency. DreamCraft3D\nproduces high-fidelity 3D assets with compelling texture details and multi-view consistency. We be-\nlieve this work represents an important step towards democratizing 3D content creation and shows\ngreat promise in future applications.\nREFERENCES\nMiguel Angel Bautista, Pengsheng Guo, Samira Abnar, Walter Talbott, Alexander Toshev,\nZhuoyuan Chen, Laurent Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, et al. Gaudi: A\nneural architect for immersive 3d scene generation. Advances in Neural Information Processing\nSystems, 35:25102\u201325116, 2022.\nEric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic\nimplicit generative adversarial networks for 3d-aware image synthesis. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pp. 5799\u20135809, 2021.\nEric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio\nGallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware\n3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 16123\u201316133, 2022.\nAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,\nSilvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d\nmodel repository. arXiv preprint arXiv:1512.03012, 2015.\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and\nappearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873, 2023.\n10\nYen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander G Schwing, and Liang-Yan Gui. Sd-\nfusion: Multimodal 3d shape completion, reconstruction, and generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4456\u20134465, 2023.\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig\nSchmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of anno-\ntated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 13142\u201313153, 2023.\nCongyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir\nAnguelov, et al. Nerdi: Single-view nerf synthesis with language-guided diffusion as general\nimage priors.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 20637\u201320647, 2023.\nAinaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: A scalable pipeline\nfor making multi-task mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 10786\u201310796, 2021.\nJun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan\nGojcic, and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned\nfrom images. Advances In Neural Information Processing Systems, 35:31841\u201331854, 2022.\nShuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\nBaining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10696\u201310706, 2022.\nYuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-\nHao Chen, Zi-Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: A unified\nframework for 3d content generation. https://github.com/threestudio-project/\nthreestudio, 2023.\nAnchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas O\u02d8guz.\n3dgen: Triplane latent\ndiffusion for textured mesh generation. arXiv preprint arXiv:2303.05371, 2023.\nFangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. Avatarclip:\nZero-shot text-driven generation and animation of 3d avatars. ACM Transactions on Graphics\n(TOG), 41(4):1\u201319, 2022.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nYukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, and Lei Zhang. Dream-\ntime:\nAn improved optimization strategy for text-to-3d content creation.\narXiv preprint\narXiv:2306.12422, 2023.\nAjay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided\nobject generation with dream fields. 2022.\nHeewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint\narXiv:2305.02463, 2023.\nHan-Hung Lee and Angel X Chang. Understanding pure clip guidance for voxel grid nerf models.\narXiv preprint arXiv:2209.15172, 2022.\nMuheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusion-sdf: Text-to-shape via voxelized diffu-\nsion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 12642\u201312651, 2023.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content\ncreation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\n11\nMinghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang Xu, Hao Su, et al.\nOne-2-3-45:\nAny single image to 3d mesh in 45 seconds without per-shape optimization.\narXiv preprint\narXiv:2306.16928, 2023a.\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-3: Zero-shot one image to 3d object. arXiv preprint arXiv:2303.11328, 2023b.\nRoey Mechrez, Itamar Talmi, and Lihi Zelnik-Manor. The contextual loss for image transformation\nwith non-aligned data. In Proceedings of the European conference on computer vision (ECCV),\npp. 768\u2013783, 2018.\nGal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for\nshape-guided generation of 3d shapes and textures. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 12663\u201312673, 2023.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications\nof the ACM, 65(1):99\u2013106, 2021.\nParitosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. Autosdf: Shape priors for\n3d completion, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 306\u2013315, 2022.\nThomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics prim-\nitives with a multiresolution hash encoding.\nACM Trans. Graph., 41(4):102:1\u2013102:15, July\n2022.\ndoi: 10.1145/3528223.3530127.\nURL https://doi.org/10.1145/3528223.\n3530127.\nGimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto Tono, Linqi Zhou, and Paul Guerrero.\n3d-ldm: Neural implicit 3d shape generation with latent diffusion models.\narXiv preprint\narXiv:2212.00842, 2022.\nAlex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system\nfor generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022.\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. arXiv preprint arXiv:2209.14988, 2022.\nGuocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-\nYing Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al.\nMagic123: One image\nto high-quality 3d object generation using both 2d and 3d diffusion priors.\narXiv preprint\narXiv:2306.17843, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nAmit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran\nZada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven\ntext-to-3d generation. arXiv preprint arXiv:2303.13508, 2023.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models, 2021.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684\u201310695, 2022.\n12\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22500\u2013\n22510, 2023.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\ntion Processing Systems, 35:36479\u201336494, 2022.\nAditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Ka-\nmal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18603\u201318613,\n2022.\nKyle Sargent, Jing Yu Koh, Han Zhang, Huiwen Chang, Charles Herrmann, Pratul Srinivasan, Jiajun\nWu, and Deqing Sun. Vq3d: Learning a 3d-aware generative model on imagenet. arXiv preprint\narXiv:2302.06833, 2023.\nTianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra:\na hybrid representation for high-resolution 3d shape synthesis. Advances in Neural Information\nProcessing Systems, 34:6087\u20136101, 2021.\nAlex Shonenkov, Misha Konstantinov, Daria Bakshandaeva, Christoph Schuhmann, Ksenia Ivanova,\nand Nadiia Klokova. DeepFloyd IF: A modular cascaded diffusion model. https://github.\ncom/deep-floyd/IF/tree/develop, 2023.\nIvan Skorokhodov, Aliaksandr Siarohin, Yinghao Xu, Jian Ren, Hsin-Ying Lee, Peter Wonka, and\nSergey Tulyakov. 3d generation on imagenet. arXiv preprint arXiv:2303.01416, 2023.\nJingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, and Yebin Liu. Ide-3d: Interactive\ndisentangled editing for high-resolution 3d-aware portrait synthesis. ACM Trans. Graph., 41(6),\nnov 2022. ISSN 0730-0301. doi: 10.1145/3550454.3555506. URL https://doi.org/10.\n1145/3550454.3555506.\nJingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong Zhang, Hongwen Zhang, and Yebin Liu.\nNext3d: Generative neural texture rasterization for 3d-aware head avatars. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 20991\u201321002,\nJune 2023.\nJunshu Tang, Bo Zhang, Binxin Yang, Ting Zhang, Dong Chen, Lizhuang Ma, and Fang Wen.\nExplicitly controllable 3d-aware portrait generation. arXiv preprint arXiv:2209.05434, 2022.\nJunshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen.\nMake-it-3d: High-fidelity 3d creation from a single image with diffusion prior. arXiv preprint\narXiv:2303.14184, 2023.\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score ja-\ncobian chaining: Lifting pretrained 2d diffusion models for 3d generation.\narXiv preprint\narXiv:2212.00774, 2022.\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus:\nLearning neural implicit surfaces by volume rendering for multi-view reconstruction.\narXiv\npreprint arXiv:2106.10689, 2021.\nTengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen,\nDong Chen, Fang Wen, Qifeng Chen, et al. Rodin: A generative model for sculpting 3d digital\navatars using diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 4563\u20134573, 2023a.\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-\ndreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv\npreprint arXiv:2305.16213, 2023b.\n13\nDaniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mo-\nhammad Norouzi. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628,\n2022.\nTong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi\nWang, Chen Qian, et al. Omniobject3d: Large-vocabulary 3d object dataset for realistic percep-\ntion, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 803\u2013814, 2023.\nJianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin Tong. 3d-aware image generation using 2d\ndiffusion models. arXiv preprint arXiv:2303.17905, 2023.\nChulin Xie, Chuxin Wang, Bo Zhang, Hao Yang, Dong Chen, and Fang Wen. Style-based point\ngenerator with adversarial rendering for point cloud completion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 4619\u20134628, 2021.\nXingguang Yan, Liqiang Lin, Niloy J Mitra, Dani Lischinski, Daniel Cohen-Or, and Hui Huang.\nShapeformer: Transformer-based shape completion via sparse representation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6239\u20136249, 2022.\nWang Yu, Xuelin Qian, Jingyang Huo, Tiejun Huang, Bo Zhao, and Yanwei Fu. Pushing the limits\nof 3d shape generation at scale. arXiv preprint arXiv:2306.11510, 2023.\nXiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and\nKarsten Kreis. Lion: Latent point diffusion models for 3d shape generation. arXiv preprint\narXiv:2210.06978, 2022.\nBiao Zhang, Matthias Nie\u00dfner, and Peter Wonka. 3dilg: Irregular latent grids for 3d generative\nmodeling. In NeurIPS, 2022.\nBiao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape rep-\nresentation for neural fields and generative diffusion models. arXiv preprint arXiv:2301.11445,\n2023a.\nHuichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu Wang, Li Chen, Chao Long, Feida Zhu,\nKang Du, and Min Zheng. Avatarverse: High-quality & stable 3d avatar creation from text and\npose. arXiv preprint arXiv:2308.03610, 2023b.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 586\u2013595, 2018.\nA\nAPPENDIX\nA.1\nIMPLEMENTATION DETAILS\nAlgorithms for Bootstrapped Score Distillation. We provide a summarized algorithm of boot-\nstrapped score distillation in Algorithm 1. The \u201cBootstrapped Score Distillation\u201d algorithm starts by\ninitializing n (n = 1 in our case) meshes and a pretrained text-to-image diffusion model \u03f5DreamBooth\nparameterized by \u03d5.\nThe algorithm then enters an iterative loop: in each iteration, it renders\nthe mesh to obtain multi-view images x, augments these images with Gaussian noises to form\nxt\u2032 = \u03b1t\u2032x + \u03c3t\u2032\u03f5, and fine-tunes the pretrained diffusion model \u03f5DreamBooth based on these aug-\nmented image renderings. Within each iteration, there\u2019s an inner loop running for n steps, where\na random mesh and camera pose are sampled, and a 2D image is rendered from the chosen pose.\nThen, updates are performed on \u03b8 and \u03d5 using gradients calculated from the difference between the\npretrained score function and the predicted score function, and from the L2 norm between the pre-\ndicted score and real noise, respectively. These iterations continue until convergence, and the final\nrefined mesh(s) are returned.\nStructure-aware latent regularization. To maintain the high-quality output produced by BSD\nwhile reducing noise and inconsistencies, we further incorporate a control net-guided inpainting\n14\nAlgorithm 1 Bootstrapped Score Distillation\nInput: Number of particles n (\u2265 1). Pretrained text-to-image diffusion model \u03f5DreamBooth. Learning\nrate \u03b71 and \u03b72 for 3D structures and diffusion model parameters, respectively. A prompt y. Number\nof images m and Camera poses {c(i)\nr }m\ni=1 for the multi-view datasets.\n1: initialize n meshes {\u03b8(i)}n\ni=1, a noise prediction model \u03f5\u03d5 parameterized by \u03d5.\n2: while not converged do\n3:\nRender the mesh to get multi-view images x = g(\u03b8, cr).\n4:\nAugment image renderings with Gaussian noises: xt\u2032 = \u03b1t\u2032x + \u03c3t\u2032\u03f5.\n5:\nFinetune \u03f5DreamBooth on augmented image renderings xr = rt\u2032(x).\n6:\nfor i in T steps do\n7:\nRandomly sample \u03b8 \u223c {\u03b8(i)}n\ni=1 and a camera pose c.\n8:\nRender the 3D structure \u03b8 at pose c to get a 2D image x0 = g(\u03b8, c).\n9:\n\u03b8 \u2190 \u03b8 \u2212 \u03b71Et,\u03f5,c\nh\n\u03c9(t) (\u03f5DreamBooth(xt, t, y) \u2212 \u03f5\u03d5(xt, t, c, y)) \u2202g(\u03b8,c)\n\u2202\u03b8\ni\n10:\n\u03d5 \u2190 \u03d5 \u2212 \u03b72\u2207\u03d5Et,\u03f5||\u03f5\u03d5(xt, t, c, y) \u2212 \u03f5||2\n2.\n11:\nend for\n12: end while\n13: return\ndiffusion model that regularizes the generated textures. Specifically, for a rendered image x from\nan arbitrary viewpoint, the visible section under the reference view is initially computed. This\ninvariant portion during the generation process allows our inpainting model to fill in the remaining\nsegments. As these remaining parts adhere to geometric constraints, we integrate geometric normal\ninformation through a control net. Ultimately, this method permits us to enforce view consistency\nand generate realistic results using a control-net guided inpainting diffusion model. To preserve the\nhigh-quality generation output, we avoid utilizing this image directly as a loss against the rendered\nimage. Instead, we subtly introduce it by constraining the norm of the latent variables:\nLreg(\u03d5, g(\u03b8)) = \u03a3(\u2225E(x)\u22252 \u2212 \u2225E(xreg)\u22252)2.\n(8)\nArchitectural details. In the Neus approach (Wang et al., 2021), we employ a single-layer Multi-\nLayer Perceptron (MLP) with 32 hidden units to simultaneously predict RGB color, volume den-\nsity, and normal. The inputs to this MLP are the concatenated feature vectors derived from multi-\nresolution hash encoding sampled with trilinear interpolation. To sparsify the Instant NGP repre-\nsentation, we implement density-based pruning every 10 iterations within an octree structure, as\nsuggested by Magic3D (Lin et al., 2023). In our experiments, we use a bounding sphere with a\nradius of 2. For the density prediction, we utilise the softplus activation function and, following the\napproach of Poole et al. 2022, include an initial spatial density bias in order to encourage optimiza-\ntion in favor of the object-centric neural field.\nCamera and light augmentations. We follow Magic3D to add random augmentations to the cam-\nera and light sampling for rendering the shaded images. Differently, we sample the point light\nlocation such that the angular distance from the random camera center location (w.r.t. the origin)\nis sampled from \u03d5cam \u223c U(0, \u03c0/3) with a random point light distance rcam \u223c (7.5, 10), and (b)\nwe freeze the material augmentation unlike Dreamfusion and Magic3D, as we found it is bad for\ntraining convergence (c) In the coarse neus stage, we propose a fixed-random mixed camera pose\nstrategy. Specifically, following the common practice of the current text-to-3D methods, random\ncamera view sampling benefits scene optimization. However, Zero1-to-3 needs fixed camera intrin-\nsic parameters. Therefore, we let half of the GPUs sample the camera distance from U(3.2, 3.5),\nand the Field-of-View from U(10, 20), while the left GPUs are fixed to the default camera intrinsic.\nTime annealing. At the beginning of the geometry sculpting stage, We utilize a simple two-stage\nannealing of time step t in the score distillation objective. For the first several iterations we sample\ntime steps t \u223c U(0.7, 0.85) and then anneal into t \u223c U(0.2, 0.50). We refer the readers to Prolific-\nDreamer (Wang et al., 2023b). For the left iterations, we fix time steps to U(0.2, 0.50). We also\nutilize a simple two-stage time annealing for the multi-view dataset generation, that is, for the first\nupdating step, we select a time step t = 0.5 for all rendered images and then anneal it into t = 0.1\nalong the later updating steps.\n15\n\u201ca DSLR photo of a \nbagel filled with \ncream cheese and lox\u201d\nFigure 8: DreamCraft3D skillfully generates an assortment of visually compelling 3D models when\nprovided with a textual description.\nFigure 9: Failure case. Our method learns incorrect geometry for elephant nose.\nA.2\nADDITIONAL EXPERIMENTS\nDiversity. Prior studies frequently yield models of limited diversity with disproportionately smooth\ntextures. Our approach to superior text-to-3D generation initially translates the text prompt into\na reference image via 2D diffusion before implementing our proprietary image-based 3D creation\nmethodology. Figure 8 demonstrates the proficiency of our method in generating an array of diverse\nmodels from a single text prompt, all characterized by their remarkable quality.\nA.3\nLIMITATIONS\nOur approach occasionally incorporates frontal-view geometric details into texture, as depicted in\nFigure 9, due to depth ambiguity and inaccuracies in the depth prior. Furthermore, we do not ex-\npressly segregate material and lighting from the 2D reference image, an aspect deferred for future\nexploration.\nA.4\nADDITIONAL QUALITATIVE RESULTS\nFigure 10\u2212Figure 13 provides more results produced by DreamCraft3D. Our method is able to\nproduce photo-realistic 3D assets with compelling textural details. Moreover, our method shows\nsignificantly improved 3D consistency. Please find video results in the supplementary video.\n16\nMictlantecuhtli\nLebron James wearing Lakers \njersey, holding_a basketball\nPresident Obama\nA DSLR photo of a delicious chocolate brownie \ndessert with ice cream on the side\nWes Anderson style Red Panda, reading a book, \nsuper cute, highly detailed and colored\nA brightly colored mushroom growing on a log\nFigure 10: Additional results of DreamCraft3D.\n17\nSuper Saiyan Goku unleashes a massive energy \nwave while standing on top of a mountain\nMech robot with large weapons on top with \nhexagonal bases\nSuper Mario\n3d stylized game little building\n3d render of a statue of an astronaut\nFigure 11: Additional results of DreamCraft3D.\n18\nAn ice cream cone\nA delicious hamburger\nA DSLR photo of a plate of fried chicken and waffles \nwith maple syrup on them\nA blue jay standing on a large basket of \nrainbow macarons\nNike \u201cAir Jordan 1\u201d Sneakers\nNike Lebron 10 \"Cork\" Sneakers\nFigure 12: Additional results of DreamCraft3D.\n19\nTeddy with  sunglasses and swimming trunks\nCute asuka anime figure, 3d render, full body, with a \nblack handbag\nBaby yoda in the style of Mormookiee\nA banana peeling itself\nRealistic photo of pistachio mousse cake\nA DSLR photo of a bear dressed in medieval armor\nNeymar 3d character cartoon disney\npixar render\nFigure 13: Additional results of DreamCraft3D.\n20\n"
  },
  {
    "title": "QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models",
    "link": "https://arxiv.org/pdf/2310.16795.pdf",
    "upvote": "26",
    "text": "QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\nElias Frantar 1 Dan Alistarh 1 2\nAbstract\nMixture-of-Experts (MoE) architectures offer a\ngeneral solution to the high inference costs of\nlarge language models (LLMs) via sparse routing,\nbringing faster and more accurate models, at the\ncost of massive parameter counts. For example,\nthe SwitchTransformer-c2048 model has 1.6 tril-\nlion parameters, requiring 3.2TB of accelerator\nmemory to run efficiently, which makes practi-\ncal deployment challenging and expensive. In\nthis paper, we present a solution to this mem-\nory problem, in form of a new compression\nand execution framework called QMoE. Specif-\nically, QMoE consists of a scalable algorithm\nwhich accurately compresses trillion-parameter\nMoEs to less than 1 bit per parameter, in a cus-\ntom format co-designed with bespoke GPU de-\ncoding kernels to facilitate efficient end-to-end\ncompressed inference, with minor runtime over-\nheads relative to uncompressed execution. Con-\ncretely, QMoE can compress the 1.6 trillion pa-\nrameter SwitchTransformer-c2048 model to less\nthan 160GB (20x compression, 0.8 bits per pa-\nrameter) at only minor accuracy loss, in less than\na day on a single GPU. This enables, for the first\ntime, the execution of a trillion-parameter model\non affordable commodity hardware, like a single\nserver with 4x NVIDIA A6000 or 8x NVIDIA\n3090 GPUs, at less than 5% runtime overhead\nrelative to ideal uncompressed inference. The\nsource code and compressed models are available\nat github.com/IST-DASLab/qmoe.\n1. Introduction\nGenerative large language models (LLMs), e.g. (Radford\net al., 2019; Brown et al., 2020; Touvron et al., 2023a;b),\nhave garnered significant industrial and popular attention\ndue to their surprising performance across many practical\nlanguage and reasoning tasks. Yet, a major obstacle to\nbroad deployment is given by their extremely high inference\ncosts. One particularly promising approach for reducing\nthese costs is the use of Mixture-of-Experts (MoE) architec-\n1Institute of Science and Technology Austria (ISTA) 2Neural\nMagic Inc. Corresponding author: elias.frantar@ist.ac.at\ntures, e.g. (Fedus et al., 2022; Artetxe et al., 2022), whose\ngeneral idea is to replicate certain model components many\ntimes while routing each input only to a small subset of\nthose replicas. Through expert \u201cspecialization\u201d to input\nsubsets, MoEs achieve faster inference for the same model\nquality, but with significantly higher memory costs due to\ncomponents being replicated hundreds or even thousands of\ntimes, for the largest and best-performing models.\nFor example, the popular SwitchTransformer family (Fe-\ndus et al., 2022), which we focus on in this study, uses\nbetween 128 and 2048 experts (layer replicas) to signifi-\ncantly outperform standard dense T5 models (Raffel et al.,\n2020b) in terms of inference and training costs, at equiv-\nalent model accuracy. Artetxe et al. (2022) report similar\nimprovements, on different tasks, for 512 experts. However,\nthese results come at the cost of dramatic increases in model\nsize: the largest SwitchTransformer has 1.6 trillion parame-\nters, requiring 3.2TB of storage in standard half-precision,\nand correspondingly requires a hundred or more expensive\n(GPU or TPU) accelerators for efficient usage. This not only\nmakes practical deployment costly and challenging, but also\nstrongly limits research on such models.\nChallenges. It is natural to ask whether the truly mas-\nsive memory costs of such MoEs can be reduced via stan-\ndard techniques for model compression, such as quantiza-\ntion (Gholami et al., 2021) or sparsity (Hoefler et al., 2021),\nwithout significant accuracy loss. Achieving this would\nrequire overcoming conceptual and technical barriers:\n1. Conceptually, existing post-training compression meth-\nods, whose costs would be affordable enough to exe-\ncute on such models, are currently only able to reduce\nprecision to 3 or 4 bits per parameter (Frantar et al.,\n2022; Dettmers & Zettlemoyer, 2022; Wu et al., 2023)\nor around 50% sparsity (Frantar & Alistarh, 2023),\nbefore significant accuracy loss occurs. Yet, making\ntrillion-parameter MoEs practical would require com-\npression rates between 10\u00d7 and 20\u00d7 relative to 16-bit\nprecision, i.e., on average less than 1 bit per parameter.\n2. A key practical issue is scaling: applying state-of-the-\nart compression methods, designed for large dense\nmodels, to MoEs that are an order of magnitude larger,\nwhile maintaining affordability, runs into a plethora of\nmemory, performance and reliability roadblocks.\n3. Actually achieving sub-1-bit compression would re-\narXiv:2310.16795v1  [cs.LG]  25 Oct 2023\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\nquire a non-trivial custom compression format. Such a\nformat would also need to come with decoding algo-\nrithms that are highly-efficient on accelerators such as\nGPUs, in order to run inference on compressed models\nwithout major processing slowdowns.\nContribution. In this paper, we overcome these challenges,\nand introduce QMoE, a framework for accurate compres-\nsion and fast compressed inference of massive MoEs, re-\nducing model sizes by 10\u201320\u00d7, to less than 1 bit per pa-\nrameter. QMoE is specifically designed to compress and\nsubsequently inference with models like the 1.6 trillion\nparameter SwitchTransformer-c2048, using only modest\ncomputational resources.\nOur key technical contributions are a highly scalable com-\npression algorithm implementation and a customized com-\npression format designed together with bespoke GPU-\nkernels for fast on-the-fly decoding. Further, we show for\nthe first time that accurate sub-1-bit compression of tril-\nlion parameter MoEs is feasible and can be achieved via\naffordable retraining-free compression techniques.\nConcretely, we reduce the size of SwitchTransformer-c2048,\nthe largest openly-available model, from 3.2TB in bfloat16\nto less than 160GB in our customized compressed format,\nthat is, \u2248 0.8 bits per parameter, at only a minor increase in\nloss on pretraining validation and zero-shot data. Using our\nQMoE kernels, this compressed model can then be executed\nfully, without any slow offloading, on commodity hardware\nsuch as 8\u00d7 NVIDIA RTX 3090 or 4\u00d7 NVIDIA A6000\nGPUs, with < 5% runtime overhead relative to an idealized\nversion of uncompressed execution, which would require\n\u2248 20\u00d7 more GPUs.\nIn summary, our work enables, for the first time, the perfor-\nmant execution of massive-scale MoE models on commod-\nity hardware. This is illustrated by the fact that we are able\nto efficiently run the trillion-parameter SwitchTransformer-\nc2048 model on a single commodity GPU server, with minor\naccuracy loss. This addresses one of the key limitations be-\nhind MoE architectures, and should improve their practical\nadoption as well as facilitate further research on understand-\ning and improving such models.\n2. Background\n2.1. Mixture of Expert Models (MoEs)\nThe core idea behind Mixture of Expert models (MoEs)\nis to increase the number of parameters, and thus the net-\nwork\u2019s modelling power, while at the same time keeping\ncompute costs near-constant, relative to a standard feed-\nforward architecture. This is typically achieved by creating\nmany copies of certain model components, each of which\nis responsible for processing only a subset of all input to-\nkens. The corresponding input-to-component assignments\nare generally decided by a \u201crouter\u201d layer. Probably the most\ncommon MoE design (Fedus et al., 2022; Artetxe et al.,\n2022), which we also focus on in this paper, is to replicate\nthe fully-connected module of a Transformer and route to-\nkens to the replica, referred to as an expert, with the highest\nassignment score predicted by a linear routing layer; see\nFigure 1 for an illustration. This design enables efficient\ntraining and inference of extremely large models, using 100s\nor even 1000s of experts/, since each token is processed only\nby a small subset of the massive overall network.\nAttention Block\nRouter\nFC Block 1\nFC Block 2\nFC Block 3\nMoE Layer\nTokens\nFigure 1. Example of an MoE Transformer block. Each token is\nrouted to a different fully-connected (FC) block.\nMoEs have been shown to bring substantial accuracy and\ntraining speed improvements for equivalent inference speed\n(Clark et al., 2022; Du et al., 2022; Zoph et al., 2022). How-\never, their current practicality is limited since they are ex-\ntremely large in size and thus require massive amounts of\naccelerator memory to be executed efficiently.\n2.2. Data-dependent Quantization\nThe currently most effective strategy for reducing model\nsize and corresponding memory costs is quantization, i.e.,\nconverting model weights to lower numerical precision. On\nlarge models (Dettmers et al., 2022; Dettmers & Zettle-\nmoyer, 2022), in particular also MoEs (Kim et al., 2022b;\nYi et al., 2023), just simple rounding can decrease precision\nto 8 or even 4 bits per weight, at minimal accuracy loss\nrelative to the standard half (16-bit) precision employed\nfor these models. However, some MoEs are so large that\nreduction rates significantly higher than 4\u00d7 (accomplished\nby 4-bit) would be required to render them practical. Accu-\nrately quantizing models to extremely low precision (e.g.,\nlower than 3 bits per parameter) typically requires more\nsophisticated data-dependent methods (Nagel et al., 2020;\nWang et al., 2020; Hubara et al., 2021).\nSuch data-dependent quantization methods use a small set\nof calibration data, which is passed through the model. As\nthis happens, for each linear layer \u2113 with weights W\u2113, quan-\ntized weights Q\u2113 are determined one-by-one. Specifically,\none approach to do this is by solving a layer-wise quantiza-\ntion problem, stated with respect to W\u2113 and the observed\ncalibration data inputs X\u2113 at the current layer:\nargminQ\u2113 ||Q\u2113X\u2113 \u2212 W\u2113X\u2113||.\n(1)\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\nVarious solvers for Equation (1) have been proposed, with\nsome optimized, in terms of speed and accuracy, particularly\nfor extremely large models, like GPTQ (Frantar et al., 2022)\nor ZeroQuant (Yao et al., 2022; Wu et al., 2023). The former\nperforms quantization using second-order information in the\nlayer-wise Hessian matrix X\u2113X\u22a4\n\u2113 , while the latter applies\nSGD-optimization with straight-through gradient estimation\n(Bengio et al., 2013).\nAnother noteworthy characteristic of many such methods is\nthat per-layer quantization can be performed sequentially,\nusing the input from the already partially quantized model\nup to layer \u2113 \u2212 1, when quantizing layer \u2113, serving to re-\nduce error accumulation. Concretely, this can be efficiently\nimplemented by using X\u2113 to find Q\u2113 before passing on\nX\u2113+1 = Q\u2113X\u2113 to the next layer.\n2.3. MoE Quantization\nThere are several aspects which make very-low-bit, e.g.\nternary (3 values) quantization promising for MoE models:\n\u2022 In many architectures, almost all parameters are lo-\ncated in the experts, as they are 1000s of them. This\nmeans that, for size reduction, it suffices to focus on\ncompressing just those experts and leave other layers\nin standard precision. This reduces error accumulation\nsince only a subset of modules involved in a forward\npass are actually quantized.\n\u2022 Previous work has observed that extremely large dense\nmodels are more resistant to quantization noise than\nsmaller ones (Frantar et al., 2022; Chee et al., 2023).\nLarge MoEs can be much larger than some of these\nmassive dense models, and are thus a prime target for\naccurate quantization.\n\u2022 MoE training involves additional stochasticity through\nrouting instabilities and strategies like token drop-\nping (Lepikhin et al., 2020), which may inherently\nencourage high resistance to noise. Finetuning is also\noften performed with high dropout (Fedus et al., 2022).\nOur experiments in Section 5.2 confirm that MoEs are in-\ndeed highly robust to extreme levels of quantization.\n3. Scaling Data-dependent Quantization to\nTrillion Parameter MoEs\n3.1. Challenges\nWhile data-dependent quantization techniques have already\nbeen used to successfully compress large dense models up to\n176 billion parameters (Frantar et al., 2022; Wu et al., 2023),\napplying them to sparse mixture-of-expert models another\norder of magnitude larger brings several new challenges.\nMemory Costs. The first major problem we encounter is\na large increase in the memory required to apply such tech-\nniques. Not only are the original model weights nearly 10\u00d7\nlarger, but the quantization process itself also needs > 100\u00d7\nmore data. The latter constraint is because accurate data-\ndependent quantization methods require a sufficient number\nof input samples for each layer that is being compressed.\nFor very large dense models, a few hundreds of thousands\nof \u201ccalibration tokens\u201d typically suffice (Frantar et al., 2022;\nYao et al., 2022). However, in MoEs with thousands of\nlayers, a single expert processes only a small subset of all\ninputs, hence we need much more tokens overall to achieve\ngood coverage of all experts. Further, in encoder-decoder\narchitecture models, like SwitchTransformers, each token\nis processed only by half of the model, again increasing\ndata requirements. For fast compression, we must maintain\nintermediate results for the full calibration dataset, which\nrequires 100s of GBs of memory for the largest models.\nGPU Utilization. The next significant challenge is that\nexisting large-scale quantization implementations, in par-\nticular for GPTQ and related methods (Frantar et al., 2022;\nChee et al., 2023), are designed to be fast and memory ef-\nficient for the massive individual layers occurring in dense\nmodels. Meanwhile, MoEs typically have smaller layers,\nbut 100\u00d7 to 1000\u00d7 more of them. Current implementations\nhave poor GPU utilization in this case, and consequently\nbad performance. A similar issue occurs if activations and\nweights have to be transferred between CPU and GPU with\nhigh frequency, which may be required to cope with the\nmassive memory requirements discussed previously.\nReliability Requirements. Finally, another issue when\ncompressing models with tens of thousands of layers is that\nrunning into rare edge cases, which may break the process, is\nhighly likely. This is includes numerical problems like non-\ninvertible layer-wise Hessians, as well as model-specific\nones, e.g., extreme routing patterns on particular layers.\n3.2. System Design & Optimizations\nIn this section, we describe system-level design and opti-\nmizations to address the challenges in Section 3.1. This\nallows us to apply data-dependent compression to massive\nMoEs, while preserving the key feature of post-training\ncompression techniques: the ability to perform effective\ncompression using only modest computational resources,\ne.g., a single NVIDIA A6000 GPU and less than one day of\ncompute. Although we focus on scaling the popular GPTQ\nmethod, most techniques described below will generalize\nto other data-dependent quantization approaches, like Zero-\nQuant (Yao et al., 2022), as well.\n3.2.1. OPTIMIZED ACTIVATION OFFLOADING\nAs discussed in Section 3.1, a key challenge in compressing\nMoEs is that we need to maintain massive activation sets.\nYet, it is possible to carefully orchestrate model execution in\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\nQ\nToken Hidden States\nQuantized Expert\nGPU\n1\n2\n2\n2\n1\n1\n2\nExpert Assignments\nFigure 2. Illustration of the offloading execution for the sparse part of a Transformer block. An expert E2 and its corresponding input\ntokens XE are fetched to GPU memory to produce E\u2032\n2, which together with the corresponding outputs YE are written back to CPU again.\nsuch a way that we only ever need to perform computation\non a small subset of the intermediate data. This allows us\nto offload main storage from GPU, to much less expensive\nand plentiful CPU memory.\nConcretely, we maintain a single large buffer B which we\nupdate as follows, for the dense part of a Transformer block:\n1. Fetch one \u201csample\u201d X, containing a few hundreds of\ntokens, from CPU to GPU.\n2. Pass it through the corresponding dense layers to obtain\nthe result Y .\n3. Calculate and store expert assignment for tokens in Y .\n4. Send Y back to CPU and overwrite X in B.\nand respectively for the sparse part, looping over experts:\n1. Fetch all individual tokens in B that have been assigned\nto expert E, denoted by XE, from CPU to GPU.\n2. Use them to produce compressed expert E\u2032 (for exam-\nple, with GPTQ).\n3. Run XE through E\u2032 to get YE\u2032.\n4. Send YE\u2032 back to CPU and overwrite XE in B.\nThis process, which is visualized in Figure 2, minimizes\nboth memory consumption and transfer cost: we need only\na single copy of B and each token is only read and written\ntwice per Transformer block.\n1\n3\n4\n7\n8\n8\nBounds\nTokens\nFigure 3. List buffer example with 3 samples, indicated by hue.\n3.2.2. LIST BUFFER\nTo efficiently support per-sample access for evaluating dense\nmodel components, as well as fully-vectorized querying of\nexpert tokens, we store B as a list buffer data structure. This\ncan be seen as a huge contiguous buffer of all token hidden\nstates, together with delimiter indices denoting boundaries\nbetween individual samples. Figure 3 illustrates this storage\nformat. This datastructure is crucial for efficiency; naively\niterating over samples and fetching relevant tokens via mask-\ning is unusably slow for large sample counts.\n3.2.3. LAZY WEIGHT FETCHING\nSince the weights of the 1.6 trillion parameter model con-\nsume > 3 TB of storage, they cannot even be stored in\nCPU RAM. Thus, we lazily fetch them directly from disk\nstorage as they are required. If we follow the inference\nprocedure outlined in Section 3.2.1, this would be exactly\nonce. Afterwards, their memory is released again.\n3.2.4. EXPERT GROUPING\nAdditionally, in order to avoid GPU underutilization (see\nSection 3.1), we group multiple experts together and apply\na joint batched variant of the GPTQ algorithm. Concretely,\nwe extract the inputs XE corresponding to all experts E \u2208 E\nin group E (the XE will generally have different sizes) and\ncompute Hessians HE. These matrices, together with the\nweight matrices WE, are then stacked to 3-dimensional ten-\nsors, on which our modified GPTQ algorithm operates, com-\npressing all experts simultaneously. We can also compute\nHE = XEX\u22a4\nE directly with a single matmul as the XE are\ngenerally small enough, avoiding the slow per-sample accu-\nmulation employed by prior implementations. Our default\nexpert groupsize |E| is 16, which brings a good trade-off\nbetween GPU memory consumption and utilization.\nTable 1 demonstrates the impact of expert grouping via\nGPTQ batching, when compressing a sparse encoder layer\nof switch-base-128 using 10k samples; |E| = 16 yields\nabout \u2248 6\u00d7 speedup over standard per-expert computation.\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\n|E| = 1\n|E| = 4\n|E| = 16\n174.1s\n54.4s\n28.8s\nTable 1. Sparse layer compression time for different |E|.\n3.2.5. ROBUSTNESS MODIFICATIONS\nTo achieve sufficiently high robustness for successfully\nquantizing trillion parameter models with tens of thousands\nof layers, we need to employ various numerical and memory\nadjustments. The most important are listed below:\n\u2022 We use 10\u00d7 higher relative Hessian dampening \u03b4 =\n0.1, avoiding breakdowns with inf-values.\n\u2022 Very few layer Hessians are not invertible even after\nhigh dampening; we skip GPTQ for those and simply\nperform vanilla rounding.\n\u2022 Sometimes an expert receives a number of tokens that\nis much larger than average, leading to out-of-memory\nsituations when these are fetched to GPU. We avoid\nthis by capping the maximum number of tokens used\nfor compression at 4\u00d7 the mean and use multiple itera-\ntions for computing and updating YE in such cases.\n3.3. Accuracy Improvements\nIn addition to implementing a highly efficient compression\nsystem, we also make new discoveries about applying GPTQ\nin our particular context, i.e., for models trained for masked-\nlanguage-modelling, MoEs and ternary quantization.\nPremasking Special Tokens. First, we find that results\ncan be improved if the various special separator tokens\ninserted by the masked-language-modelling task (Raffel\net al., 2020b) are excluded from the calibration data used for\ncompression. Conretely, in the encoder, we mask out those\n\u201cmask-tokens\u201d during the Hessian computation. Meanwhile,\nin the decoder, we skip the token directly before such a\nspecial token as this is the one used to predict the latter.\nAs shown in Table 2 for switch-base-128 with 10k samples,\nthis brings noticeably lower loss at no additional compute\ncost. We think that because those tokens are very common\nduring training, the model is so robust in their prediction\nthat any error compensation on them during quantization is\nunnecessary, while worsening correction for other tokens.\nmask\nBF16\n2bit\ntern\nno\n1.73\n1.86\n2.16\nyes\n1.73\n1.76\n1.99\nTable 2. Impact of special token masking; validation loss.\nIneffective Heuristics. We also evaluate two more recently\nproposed GPTQ enhancement heuristics: activation reorder-\ning and true sequential execution (Frantar et al., 2023). How-\never, as shown in Table 3 for ternary quantization of switch-\nbase-128, we find the former to be actually harmful and the\nlatter to be more or less quality neutral, for our particular\nuse-case. We suspect that, in this highly aggressive setting,\nquantizing all the most sensitive columns first, leads to large\nchanges of the entire weight matrix, and thus to overfitting.\nGPTQ\nact\nseq\nact + seq\n1.99\n2.23\n1.99\n2.28\nTable 3. Activation reordering (act) and sequential execution (seq).\n4. Realizing Sub-1-Bit Compression\nUsing our system discussed in Section 3, we can accurately\nquantize extremely large SwitchTransformers to very low\nbit-widths: 2-bit and even ternary (3 possible values). Yet, in\npractice, this falls still short of our compression goal of less\nthan 1 bit per parameter. We find that compression rates can\nbe pushed significantly further by taking advantage of the\nlow entropy in the quantized weights. Next, we co-design\nan encoding scheme and a CUDA kernel which realize sub-\n1-bit per weight compression in practice, at minimal cost in\nterms of GPU execution overhead for inference.\n4.1. Natural Sparsity\nWe pick quantization grids in standard fashion: row-wise\naround the min and max weights values (Dettmers et al.,\n2022; Frantar et al., 2022), e.g., for ternary: {wmin, 0, wmax}.\nThese rather wide grids combined with the fact that weights\nare typically close to normally distributed, naturally lead to\nhigh sparsity after quantization, i.e., a large number of zeros.\nWe demonstrate this in Table 4, averaged over all layers. For\nternary weights, the largest model achieves close to 90%\nnatural sparsity; the standard deviation is also quite low, at\n< 5%. Seen another way, the quantized weights have low\nentropy, meaning that, on average, significantly less bits per\nweight should be required for lossless storage.\nmodel\n2-bit\nternary\nbase128\n72.2%\n85.7%\nlarge128\n73.1%\n86.4%\nc2048\n76.5%\n88.6%\nTable 4. Natural sparsity for different compressed models.\n4.2. From Sparsity to Entropy\nThe direct way of utilizing these high zero proportions\nwould be in form of a joint sparse & quantized represen-\ntation (Kurtic et al., 2022; Yu et al., 2023): storing only\nthe quantized values of non-zero weights, together with\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\nnecessary position metadata. However, as our base quantiza-\ntion levels are already very low, standard sparsity metadata\nformats (Elsen et al., 2020; Lin et al., 2023) would only\nallow limited additional compression. A bitmask indicating\nnon-zero locations requires 1 bit per weight, while 10-13\nbit (depending on layer size) column indices are even less\nmemory efficient at the sparsity levels we encounter. There-\nfore, we take a different approach: we do not utilize sparsity\ndirectly but rather the low entropy, which is implied by the\nfact that a single value (0) occurs very frequently, using an\nappropriate encoding scheme.\n4.2.1. FAST GPU DECODING CHALLENGES\nIn principle, we could group multiple consecutive ternary\nweights into super-symbols and then apply a code which\nassigns variable length codewords to those super-symbols,\nbased on their probability of occurrence, for example, via a\nHuffman approach (Huffman, 1952). If the quantized weight\nvalues were close to independent, this would achieve strong\ncompression rates; in fact, for actual independence, they\nwould be essentially Shannon-optimal (MacKay, 2003).\nAt the same time, our primary goal is to use compressed\nmodels for fast and space-efficient inference. Thus, it is\ncritical not only that our encoding scheme achieves good\ncompression, but also that it can be decoded fast on GPU\nhardware. This is challenging for a number of reasons:\nChallenge 1: Entropy-based codes generally possess se-\nquential decoding dependencies: symbol i can only be de-\ntermined if the length, which is variable, of all (i \u2212 1) prior\nsymbols is known. Hence, processing consecutive symbols\nsimultaneously leads to high synchronization overhead.\nChallenge 2: Binary words in storage (e.g., INT32 blobs)\nmay contain different numbers of decoded symbols. Conse-\nquently, even if rows/blocks are encoded independently, par-\nallel decoding will happen non-uniformly, while all threads\nin a GPU-warp must always execute the same instruction.\nThis would result in many wasted operations.\nChallenge 3: Variable-length low-bit decoding involves a\nlarge number of binary operations like shifts, which are not\nparticularly efficient on GPUs.\nChallenge 4: Individual matrices of MoEs are typically not\nvery large, making it difficult to split them into enough sep-\narately decoded segments to achieve good GPU utilization\nwithout having to store additional data to break sequential\ndependencies, which would harm compression rates.\nIn contrast, uncompressed half-precision matrix-vector prod-\nucts, which are the primary operation underlying generative\ninference, easily achieve close to ideal memory-bandwidth\nutilization and thus present a very strong baseline.\n4.3. Compression Scheme & Kernel Co-design\nTo achieve our goal, we need to design a compression\nscheme and its GPU decoding kernel jointly, and poten-\ntially trade off compression for faster decoding. We begin\nwith an overview of the main ideas behind our approach,\nfollowed by an in-depth discussion of key details.\n4.3.1. OVERVIEW\nInstead of a code with variable length codewords (see Sec-\ntion 4.2.1) mapping to fixed length data, we will use a\ndictionary-based code with fixed length codewords mapping\nto a variable number of symbols. Such LZW-based schemes\n(Welch, 1984) are popular for general purpose compression\nlike ZIP, as they are particularly effective for text data with\nlong repeated segments. While a dictionary code is not ideal\nin terms of compression rate for the case of almost-random\ndata in our application, it will be key for fast GPU decoding.\nFirst, our kernel design uses one warp, that is 32 consecutive\nthreads, to handle a row of a weight matrix, each of which is\nencoded independently. This addresses Challenge 4 in Sec-\ntion 4.2.1, yielding reasonable GPU utilization for relevant\nmatrix sizes, with negligible metadata overhead. Further,\nwe use a fixed-to-variable code with a large dictionary. This\nallows us to use a full warp to process one codeword at-a-\ntime, extracting all data, while maintaining good efficiency,\nthus working around Challenges 1 and 2. This way, slow\nbit and base-3 operations (for ternary) can also be kept at a\nminimum, resolving Challenge 3.\n4.3.2. DICTIONARY DESIGN AND IMPLEMENTATION\nIn general, assume that the values of a ternary weight matrix\n(denoted by 0, 1, 2) are distributed close to independently\naccording to the distribution:\nP(0) = p0,\nP(1) = P(2) = 1 \u2212 p0\n2\n,\n(2)\nwhere p0 denotes the probability of sampling 0, e.g., 0.885\nas per Table 4. Since we plan to use a rather large dictionary,\nit should be shared between many weight matrices, in or-\nder for the dictionary itself not to cause substantial storage\noverheads. We find that such a static dictionary works well\nenough, while simplifying memory efficient compression\n(see Section 3.2) as we do not have to collect statistics over\nmany yet uncompressed experts.\nNext, we consider pairs of ternary values t = (t1, t2), whose\ncorresponding probability is P(t) = P(t1)P(t2). We gen-\nerate the 216 highest probability sequences containing at\nmost 14 such pairs. This dictionary can be generated using a\nmax-priority queue on probability, as shown by Algorithm 1.\nTo briefly understand the procedure, notice that upon the\nfirst iteration, it will push all individual pairs t = (t1, t2) to\nthe priority queue, sorting them by decreasing probability,\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\nAlgorithm 1 Generate decoding dictionary sequences.\nQ \u2190 max priority queue containing (1.0, ())\nwhile |D| < 216 do\np, s \u2190 pop(Q)\nappend s to dictionary if 0 < |s| < 28\nfor t \u2208 {(t1, t2) | t1, t2 \u2208 {0, 1, 2}} do\npush((p \u00b7 P(t), cat(s, t)), Q)\nend for\nend while\nafter which they will be expanded in this order.\nWe have exactly 216 codewords as this allows us to store\nthem in the native UINT16 datatype, avoiding any slow bit-\nextractions at this decoding level. Each of those codewords\nmaps to two consecutive UINT32 values containing up to 7\npairs each, stored using 2 bits per ternary value, followed by\nthe total number of pairs in the sequence; see also Figure 4.\nThis format dictates our maximum chosen pair count of 14.\nFurther, we consider pairs, rather than individual weights, to\nfit the maximum count into 4 bits. The 2-bit-per-weight for-\nmat is used as there is enough space, while a more compact\nternary encoding would involve slow modulo and division\noperations for extraction. We store the pair-count twice so\nthat each thread can work with only half of the data, stored\nin a fast INT32 type.\n01\n10\n00\n00\n01\n00\n00\n00\n01\n10\n00\n00\n10\n00\n1100\n2 bits\n4 bits\n1 weight\n1 pair\npair count\n4 bits\n01\n10\n00\n00\n01\n00\n00\n00\n10\n10\n00\n00\n00\n00\n1100\nunfilled\nFigure 4. Data format of a dictionary entry; here of 24 weights.\nOverall, mapping 16-bit codewords to 64-bit data blobs\nstrikes a good balance between several goals: (a) Having\ncodewords map to, on average, more uncompressed values\nthan their bitwidth, a necessary condition for achieving < 1-\nbit compression. (b) Minimizing the overall storage cost of\nthe dictionary to fit into the L2-cache of the GPU, which\nis critical for good decoding performance. (c) Utilizing\nas many threads in a warp as possible for simultaneously\nextracting plain weights from the decoded data; usually,\n> 16 will do useful work and only 4 out of 32 threads are\nnever active in this step. (d) Avoiding as many conditionals\nand extra operations necessary for dealing with non-uniform\ndata storage as possible, which slow down parallelization.\nFinally, we note that while dictionary lookups are in princi-\nple random access, keeping it sorted from highest to lowest\nprobability ensures very favorable caching behavior. Since\neach lookup also automatically prefetches several subse-\nquent elements, and most lookups are for frequently occur-\nring codewords, there are many fast L1-cache hits.\nValidation. To assess the effectiveness of our scheme, we\ncompute achieved compression rates, both on a real ternary\nquantized c2048 model as well as on weight matrices sam-\npled directly from distribution (2), yielding 20.07\u00d7 and\n21.11\u00d7, respectively. This gap of only \u2248 5% suggests that\nour simplifying independence assumption is indeed quite\nclose for large models. We also note that our rates are only\n\u2248 20% away from the distribution\u2019s (with p = 0.885) theo-\nretical compression limit of 25.40\u00d7, which we consider a\nreasonable trade-off for enabling fast GPU decoding.\n4.3.3. GPU KERNEL\nHaving defined the dictionary format, we can now discuss\nthe design of the actual decoding kernel in detail. We focus\non the most important operation for inference, decompres-\nsion fused with a matrix-vector-product. However, our tech-\nniques can easily be adapted to other use-cases, e.g., pure\ndecompression.\nListing 1 provides CUDA-like pseudocode for our kernel,\ncomputing the matrix-vector-product of compressed matrix\nw comp (with metadata row off and ter minmax, using\ndictionary dec) and BF16 vector x, into output buffer y. The\nhandling of various edge cases and some index calculations\nhave been removed for readability. Please see our repository\nfor the fully functional implementation.\n1\ntemplate <int num_warps, int w_width>\n2\n__global__ void Sub1MatVec(\n3\nint* dec,\n4\nushort* w_comp, int* row_off, __nv_bfloat162* ter_minmax,\n5\n__nv_bfloat16* x, __nv_bfloat16* y\n6\n) {\n7\n__shared__ float x_shared[w_width];\n8\nfor (int i = thread; i < w_width; i += 32 * num_warps)\n9\nx_shared[i] = __bfloat162float(x[i]);\n10\n11\n__shared__ float deq[3][32 * num_warps];\n12\ndeq[0][thread] = 0;\n13\ndeq[1][thread] = __bfloat162float(ter_minmax[row].x);\n14\ndeq[2][thread] = __bfloat162float(ter_minmax[row].y);\n15\n16\n__syncthreads();\n17\n__shared__ w_comp_block[32][num_warps];\n18\n19\nfloat res = 0;\n20\nint idx = 0;\n21\n22\nfor (int i = 0; i < row_off[row + 1] - row_off[row]; i += 32) {\n23\nw_comp_block[warp][lane] = w_comp[i + lane];\n24\n25\nif (lane < 28) {\n26\nfor (int j = 0; j < 32; j++) {\n27\nint enc = w_comp_block[warp][j];\n28\nint wx14 = dec[2 * enc + (lane / 14)];\n29\nint ter = (wx14 >> (4 + 2 * (lane % 14))) & 0x3;\n30\nfloat w = deq[ter][thread];\n31\nres += w * x_shared[idx + lane];\n32\nidx += 2 * (wx14 & 0xf);\n33\n}\n34\n}\n35\n}\n36\n37\nfor (int i = 16; i > 0; i /= 2)\n38\nres += __shfl_down_sync(0xffffffff, res, i);\n39\nif (lane == 0)\n40\ny[row] += __float2bfloat16(res);\n41\n}\nListing 1. Simplified kernel pseudocode for a fused decompress +\nmatrix-vector-product operation.\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\nParallelization. Overall, each threadblock will handle mul-\ntiple consecutive rows, each of which is processed by a sin-\ngle warp. We use exactly one thread-block per GPU Stream-\ning Multiprocessor (SM) with min(#rows in block, 32)\nwarps; if there are more than 32 rows in a block, (some)\nwarps sequentially process multiple rows (note that this part\nis omitted in Listing 1 for simplicity). This avoids any bad\nwave quantization effects. We find this strategy to be an ef-\nfective heuristic that yields good performance for all matrix\nshapes we consider.\nExecution. Our kernel starts by loading the entire input\nvector to shared memory (x shared, lines 7-9), using all\nwarps in a threadblock. This enables fast element access in\nthe subsequent per-row product-sum accumulations.\nNext, each warp processes its corresponding row by\nfirst fetching (up to) 32 codewords into shared memory\n(w comp block, line 23) using a single coalesced transac-\ntion. It then loops over those symbols, processing one-at-\na-time (lines 26-33). First, using 28 of its 32 threads (line\n25), it fetches the corresponding decoding data from the\ndictionary where the first UINT32 is assigned to threads\n0-13 and the second to threads 14-27 (wx14, line 27). Then,\neach thread extracts its corresponding ternary weight (lines\n29-30) and adds the corresponding input product into its\nown partial result accumulator (res, line 31). We note that\nthe input reads from shared memory are contiguous and do\nnot cause bank conflicts. Afterwards, each thread advances\nthe offset index (idx, line 32) into the input vector by the\ntotal number of weights encoded in the current symbol.\nFinally, after the full row has been scanned, a warp-\nreduction (lines 37-38) over the partial results of each thread\nyields the output (y, lines 39-40).\nTernary decoding. Another relevant detail is that ternary\nweights are stored as 0, 1, 2 (line 29) but need to be dequan-\ntized to 0, wmin, wmax for multiplication with inputs. We\nfound that the most efficient way of performing this con-\nversion is via a shared memory lookup table (lines 11-14).\nCrucially, this table needs to be replicated 32 times across\nthe column-dimension to avoid very frequent bank conflicts,\nwhich would otherwise occur every time not all 28 threads\ndequantize the same value (line 30). Fortunately, there are\nonly 3 input values and so its overall size is tolerable.\nEncoding. So far, we have only focused on the decoding\noperation, but we also have to encode matrices with reason-\nable efficiency. In general, this is done by building a trie\ndatastructure (of the dictionary discussed in Section 4.3.2)\nmapping sequences to codewords. Then, we iterate through\nthe input while simulatenously traversing the trie to find\nlongest prefix matches, yielding the corresponding code-\nwords. Finally, we densely pack rows of different lengths\ninto a contiguous buffer and record corresponding row off-\nsets. Unlike decoding, encoding is not very latency critical\nand a straight-forward GPU kernel using one thread per row\nof the matrix to compress suffices.\n5. Experiments\n5.1. General Setup\nModels. We focus our experiments on the SwitchTrans-\nformer (Fedus et al., 2022) family of models. Our primary\ntarget is the very largest variant, c2048, with around 1.6\ntrillion parameters, but we also consider the comparatively\nsmall base128 (7B params) and large128 (26B params) ver-\nsions for testing and ablations. We chose the SwitchTrans-\nformer family as it contains the largest publicly-available\nmodel, which also features a similar or higher number of\ntraining tokens to parameters ratio than potential alterna-\ntives like Artetxe et al. (2022). Further, those models are\nalso among the most popular massive MoEs, with several\nimplementations across frameworks.\nFramework. As accessibility is a major goal of our work,\nwe build our code-base around the PyTorch-backend of the\nhighly popular HuggingFace (Wolf et al., 2019) framework,\nrather than on the SwitchTransormer\u2019s original training en-\nvironment MeshTensorflow (Shazeer et al., 2018) or its\nJAX-based successor T5X (Google, 2023). This brings a\nnumber of additional challenges.\nFirst, we find that the largest model variants require a hand-\nful of bugfixes, primarily configuration and model setup\nchanges, in order to run properly. We suspect that this is\nbecause their enormous sizes have rendered extensive test-\ning very difficult. Second, we observed a major inefficiency\nin the context of generative inference for models with a\nlarge number of experts: the HuggingFace implementation\nwill perform several (empty) CUDA calls for potentially\n1000s of experts to which no token is routed, accumulating\nlarge overheads. We modify the implementation (also for\nbaselines) to skip such unnecessary calls, leading to > 10\u00d7\nspeedup for large models. We apply all changes to the Hug-\ngingFace framework only dynamically at runtime, so that\nour code can be run directly with an official installation.\nHuggingFace prioritizes ease-of-use and flexibility over\nhigh performance. For that reason, we conduct inference\nmeasurements not only end-to-end, including all Hugging-\nFace overheads, but also in isolated fashion, comparing\nuncompressed and compressed matrix operations directly.\nThis is to demonstrate that our GPU kernels would also yield\nlow overhead in more optimized inference environments.\nDatasets.\nSwitchTransformers have been trained for a\nMasked-Language-Modelling (MLM) objective (Raffel\net al., 2020b) on the C4 dataset (Raffel et al., 2020a). Similar\nto most works in the area of LLM quantization (Yao et al.,\n2022; Frantar et al., 2022; Dettmers & Zettlemoyer, 2022),\nwe focus on general upstream compression directly on this\npretraining task/dataset combination. Consequently, our\nevaluation focuses on validation performance for C4/MLM,\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\nwhere we use the public reproduction of C4 on HuggingFace\nas well as their replication of the original masking proce-\ndure. Calibration data for compression is taken, in order,\nfrom the first two shards of the training set. For efficiency,\nwe primarily evaluate on 128 samples (corresponding to the\naverage loss over > 10K tokens, which is quite stable) from\nthe first shard of the validation set, but we also perform\nsome evaluations other datasets.\nHardware. All compression experiments, including those\nfor the very largest models, can be performed in less than a\nday on a single NVIDIA A6000 with 48GB of GPU memory.\nHowever, efficiently compressing trillion parameter models\nusing a large number of calibration samples requires a few\n100GBs of (CPU) RAM; the original 1.6T model itself also\noccupies > 3 TB disk storage. We highlight that our work is\nperformed in a highly constrained environment for models\nof this size, for example, it is already infeasible to load the\nentire (uncompressed) 1.6T model into RAM, let alone into\nGPU memory. For inference on compressed models, we\nwill also consider running on multiple NVIDIA 3090 GPUs,\nwith 24GB of memory each, in addition to A6000s.\n5.2. Compression Results\nAccuracy. We begin by quantizing all SwitchTransformer\nmodels to 2-bit and ternary precision, and evaluating their\nvalidation loss. Our default number of calibration samples is\n10K for 128 experts and 160K for 2048, but we also consider\nusing 0.5\u00d7 and 2\u00d7 as many samples. In addition to using\nour efficient QMoE framework discussed in Section 3, we\nalso consider a standard round-to-nearest (RTN) baseline\n(Dettmers et al., 2022). We simulate the latter by fixing\nHessians to the identity matrix, thus applying precisely the\nsame quantization settings and evaluation protocol. Table 5\nsummarizes our results.\nmethod\nbase128\nlarge128\nc2048\n2bit\ntern\n2bit\ntern\n2bit\ntern\nBF16\n1.73\n1.55\n1.18\nRTN\n2.27\n4.54\n1.96\n2.79\n1.33\n2.15\nQMoE 0.5x\n1.78\n2.11\n1.54\n1.70\n1.22\n1.27\nQMoE 1.0x\n1.76\n1.99\n1.56\n1.69\n1.20\n1.26\nQMoE 2.0x\n1.76\n1.93\n1.57\n1.64\n1.21\n1.26\nTable 5. Comparing C4 validation losses for 2-bit and ternary (tern)\nquantized SwitchTransformers. \u201cQMoE 0.5x\u201d indicates that only\nhalf of the default number of calibration samples are used.\nPerhaps surprisingly, vanilla rounding (RTN) does not lead\nto a complete model collapse even at ternary precision, em-\nphasizing the high robustness of large MoEs to quantization.\nNevertheless, the loss increases are quite significant for\nsmaller models at 2-bit and far too large to be useful at\nternary precision. In contrast, using data-dependent quanti-\nzation, 2-bit is achievable at minimal loss (1.7% relative on\nc2048) and ternary at only a small increase (6.7% relative\non c2048). This demonstrates not only the effectiveness\nof such advanced quantization methods in this context, but\nalso shows that extremely low-bit compression is indeed\npractical for massive MoEs.\nAdditionally, we conduct evaluations on Arxiv, GitHub,\nStackeEchange and Wikipedia data sampled from RedPa-\njama (Computer, 2023). Even though only < 0.01% of our\nC4 calibration data originates from those websites, the com-\npressed model still preserves performance almost as well as\non the core of the distribution (see Table 6).\nbits\narxiv\ngithub\nstackexch.\nwiki\nBF16\n1.31\n0.99\n1.15\n1.20\n2-bit\n1.34\n1.05\n1.17\n1.24\ntern\n1.42\n1.13\n1.22\n1.32\nTable 6. Additional evaluations for the c2048 model.\nIn terms of calibration data, we see that increasing the\namount of samples generally improves performance slightly,\nmost noticeably for ternary quantization, but there is also\nsome noise in the process, especially at 2-bit.\nCompression. Next, we investigate the actual compression\nrates that are achieved by further compressing ternary mod-\nels using our scheme introduced in Section 4. We consider\nboth compression relative to just the MoE modules (the\nmodel parts we quantize) as well as to the full model and all\nits metadata. The compression rates and overall checkpoint\nsizes are listed in Table 7.\nmodel\nmoe-only\nfull\nsize [GB]\nbf16\nours\nbase128\n17.06\u00d7\n11.76\u00d7\n14.9\n1.27\nlarge128\n18.34\u00d7\n13.32\u00d7\n52.7\n3.96\nc2048\n20.07\u00d7\n19.81\u00d7\n3142\n158.6\nTable 7. Compression rates and sizes for ternary models.\nIn general, measuring only relative to parts we compress\n(moe-only), all sizes achieve > 16\u00d7 compression rate and\nthus < 1 bits per parameter storage. On c2048, even the\noverall rate, including all uncompressed dense layers, re-\nmains at 19.81\u00d7, corresponding to 0.807 bits per parameter,\nreducing the checkpoint size from 3142GB to 158.6GB. One\ncan also observe that compression rates increase with model\nsize, which is for two reasons: (a) natural sparsity increases\nwhile our encoding dictionary is also optimized for c2048\n(see Section 4), and (b) weight distributions become closer\nto independent for larger layer sizes.\nRuntime. Finally, we evaluate how long it takes to produce\ncompressed models on a single A6000 GPU, for different\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\n768 \u00d7 3072\n3072 \u00d7 768\n1024 \u00d7 4096\n4096 \u00d7 1024\n2080 \u00d7 6144\n6144 \u00d7 2080\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nTime relative to BF16\nPer-layer performance of compressed matrix-vector kernels\nBF16\nRTX3090\nA6000\nbase128\nlarge128\nc2048\n0\n1\n2\n3\n4\n5\n6\nSeconds to generate 128 tokens\nEnd-to-end performance of compressed models\n3090-BF16*\n3090\nA6000-BF16*\nA6000\nFigure 5. (Left) Per-layer compressed kernel performance relative to uncompressed execution. (Right) End-to-end runtimes of compressed\nmodels and estimates (\u2217, would require 65/130 GPUs) for bloat16 baselines. c2048 is run on 4\u00d7A6000 and 8\u00d73090 GPUs, respectively.\namounts of calibration data. The results are shown in Ta-\nble 8. Smaller models can be compressed in less than an\nhour and even c2048 in less than a day, confirming the high\nefficiency of QMoE. The runtime increase from large128\nto c2048 is roughly proportional to the difference in size,\ndespite the latter using 16\u00d7 more samples. This is because\nthe number of samples per expert stays constant and the\nexpert size increases only slightly. Finally, we note that\nsimply (iteratively) loading the original 1.6T model into\nRAM takes close to 5 hours on our slow disk storage.\nmodel\n5K/80K\n10K/160K\n20K/320K\nbase128\n8.4min\n14.0min\n21.6min\nlarge128\n22.0min\n30.2min\n45.2min\nc2048\n13.3h\n16.0h\n20.8h\nTable 8. Compression runtime for different calibration data size.\n5.3. Runtime Results\nIndividual Layers. Our kernel performance evaluation\nstarts with a direct (isolated) comparison of our compressed\nmatrix-vector product kernels (see Section 4) against Py-\nTorch\u2019s standard (uncompressed) bfloat16 cuBLAS kernels.\nFigure 5 (Left) shows the time taken by our compressed\nkernels relative to bfloat16, for the matrix shapes found in\nour MoEs, on two different GPUs. While our kernels have\nto perform a lot less slow (global) memory reads than the\nbfloat16 baseline due to lower storage costs, they need to\nspend much more compute for complex unpacking of the\nheavily-compressed weights. Nevertheless, executing our\ncompressed kernels takes less time than the close to ideal\nbfloat16 baseline in all cases, with up to 35% speedup on\nspecific matrix shapes. We note that these are very low-\nlatency operations, with the smallest matrix taking < 0.02\nmilliseconds and the largest < 0.05.\nEnd-to-End Execution. Finally, we also benchmark our\nkernels end-to-end in HuggingFace on the real weights of\nour compressed MoE models. We consider an individual\nuser application, like (Frantar et al., 2022; Leviathan et al.,\n2023; Park et al., 2022), where a single prompt (sampled\nfrom C4) should be processed to generate a 128-token re-\nsponse. As actually running the bfloat16 version of the\nc2048 model would require > 65 A6000 and > 130 3090\nGPUs (versus 4 and 8, respectively, for sub-1-bit com-\npressed weights) we have to estimate its runtime. We do this\nby having all experts in a layer point to the same weight data\n(completely resolving memory issues), which allows us to\ncollect timings with precisely the same overheads as for our\ncompressed models. However, this is a highly optimistic\nestimate since real execution would require close to 20\u00d7\nmore GPUs, with corresponding communication overheads,\nand our numbers should thus be viewed only as a lower\nbound.\nThe results, shown in Figure 5 (Right), demonstrate that\nend-to-end execution of compressed models is only < 5%\nslower than standard (uncompressed) execution. This slight\nslow-down despite faster per-layer timings is due to the fact\nthat the encoder may sometimes route multiple tokens to\nthe same expert. Our current implementation naively exe-\ncutes a separate matrix-vector product for each token, while\nthe baseline performs a much more efficient joint matrix\nmultiplication. For applications where this is a significant\nbottleneck, one could easily introduce an inner loop over to-\nkens into our kernel (Listing 1, line 30), or fully decompress\nfirst, followed by a standard matmul, for large token counts.\n6. Related Work\nMixture-of-Expert (MoE) Models.\nMixture-of-expert\nmodels are a popular research direction aimed at creating\nsignificantly more efficient large-scale models (Fedus et al.,\n2022; Artetxe et al., 2022; Clark et al., 2022). At the core\nof MoEs lie (sparse) routing mechanisms, of which many\nvariants have been proposed. Those range from static as-\nsignment based on input tokens IDs (Roller et al., 2021),\nover dynamic token-to-expert matching (Zhou et al., 2022),\nto \u201csoft\u201d routing of linear input combinations (Puigcerver\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\net al., 2023). Since MoEs can feature rather different com-\nputational profiles from standard dense models, there is also\nsignificant research on optimizing inference and training\nsystems (Barham et al., 2022; Gale et al., 2023; Hwang\net al., 2023). Among the most critical problems in this area\nare data-exchanges between accelerators during routing and\ndealing with uneven compute-loads for different experts.\nLLM Quantization. Quantization is a very popular com-\npression technique, which has seen a vast amount of\nwork (Gholami et al., 2021), especially in the context\nof LLMs.\nSpecifically, the ability to perform accurate\nweight quantization for billion-parameter models has greatly\nboosted their accessibility: it has been shown that extremely\nlarge dense models can be quantized to 8- or even 4-bit\nprecision at little accuracy loss (Dettmers et al., 2022; Yao\net al., 2022; Frantar et al., 2022; Dettmers & Zettlemoyer,\n2022). Pushing towards even lower bitwidths via more\nsophisticated compression formats, like multi-level group-\ning coupled with higher-precision outliers (Dettmers et al.,\n2023b), or new quantization techniques, like incoherence\npreprocessing (Chee et al., 2023), is an active area of re-\nsearch. Currently, accurate quantization to 2 or less bits per\nparameter appears to be a major barrier for post-training\nquantization of standard LLMs. By contrast, in this work we\nshow that massive MoE models appear to be significantly\nmore compressible, as we achieve sub-1-bit compression at\ncomparable loss increases to 3-bit or 4-bit quantization of\nstandard LLMs with advanced techniques.\nMoE Compression. There has also been work on com-\npressing MoE models in particular. Chen et al. (2022) and\nKoishekenov et al. (2022) perform compression via spe-\ncialization of MoEs to specific \u201cdownstream\u201d finetuning\ndatasets by pruning components not relevant to the par-\nticular task. In contrast, we focus on general \u201cupstream\u201d\ncompression of the pretrained model, via extremely low-bit\nquantization. Other works (Kim et al., 2022b; Yi et al., 2023;\nKim et al., 2023) also perform MoE quantization, but focus\non noticeably higher bit-widths, like 8 or 4 bits per weight.\nThis is accomplished primarily via simple rounding, which,\nas shown by our experiments, is not accurate enough for full\n2-bit or lower compression. Kim et al. (2022a) achieve 2-bit\nquantization on a 5 billion parameter MoE, which is con-\nsidered relatively small in this area, by further optimization\nof the model via Quantization-Aware Training (Nagel et al.,\n2021). Applying such an approach for trillion-scale models\nwould be extremely resource intensive. They also do not\nprovide any mechansims for exploiting low-bit quantization\nand its corresponding natural sparsity in practice, which is\nchallenging and constitutes a key contribution of our work.\nWe are particularly focused on scalabilty and practicalty.\nWhile existing works study models with at most tens of\nbillions of parameters, we demonstrate the effectiveness\nand efficiency of our techniques at trillion parameter scale,\nboth for the quantization process itself as well as for actual\ninference of compressed models.\n7. Discussion and Limitations\nWe have presented QMoE, an end-to-end compression and\ninference framework for addressing the massive memory\ncosts of MoE inference. We showed, for the first time, that\nmodels such as the trillion-parameter SwitchTransformer-\nc2048 can be accurately compressed to less than 1 bit per\nparameter, close to 20\u00d7 compression rate, in a custom for-\nmat that enables the first efficient end-to-end execution of\nsuch a model on a single commodity GPU server. QMoE is\nfully open-source and built around the popular HuggingFace\nframework, making deployment and research for massive\nMoEs significantly cheaper and more accessible.\nOur study is confined to a limited set of models, as only\nvery few massive and accurate MoEs are available publicy.\nAdditionaly, due to their size, most MoEs are trained and de-\nployed in different bespoke framework, requiring complex\nmanual integrations to use for further research. Neverthe-\nless, we have covered some of the largest and most accurate\navailable MoEs, specifically SwitchTransformers (Fedus\net al., 2022). A natural extension of our work would be to\napply our QMoE techniques to other MoE models and vari-\nants, such as Artetxe et al. (2022) or the recently-proposed\nSoftMoEs (Puigcerver et al., 2023).\nAdditionally, we have focused on direct compression of the\npretrained base model. However, it would also be interesting\nto further finetune a compressed model for specialized down-\nstream tasks, similar to QLoRA (Dettmers et al., 2023a).\nZoph et al. (2022) report strong results when finetuning\nonly non-expert layers, which QMoE leaves uncompressed,\nsuggesting that this application could be promising. We\nhope to explore this in future work.\nReferences\nArtetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M.,\nShleifer, S., Lin, X. V., Du, J., Iyer, S., Pasunuru, R., et al.\nEfficient large scale language modeling with mixtures\nof experts. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2022.\nBarham, P., Chowdhery, A., Dean, J., Ghemawat, S., Hand,\nS., Hurt, D., Isard, M., Lim, H., Pang, R., Roy, S., et al.\nPathways: Asynchronous distributed dataflow for ml. In\nConference on Machine Learning and Systems (MLSys),\n2022.\nBengio, Y., L\u00b4eonard, N., and Courville, A. Estimating or\npropagating gradients through stochastic neurons for con-\nditional computation. arXiv preprint arXiv:1308.3432,\n2013.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nIn Conference on Neural Information Processing Systems\n(NeurIPS), 2020.\nChee, J., Cai, Y., Kuleshov, V., and De Sa, C. Quip: 2-bit\nquantization of large language models with guarantees.\narXiv preprint arXiv:2307.13304, 2023.\nChen, T., Huang, S., Xie, Y., Jiao, B., Jiang, D., Zhou, H.,\nLi, J., and Wei, F. Task-specific expert pruning for sparse\nmixture-of-experts. arXiv preprint arXiv:2206.00277,\n2022.\nClark, A., De Las Casas, D., Guy, A., Mensch, A., Paganini,\nM., Hoffmann, J., Damoc, B., Hechtman, B., Cai, T.,\nBorgeaud, S., et al. Unified scaling laws for routed lan-\nguage models. In International Conference on Machine\nLearning (ICML), 2022.\nComputer, T.\nRedPajama:\nAn open source recipe\nto reproduce llama training dataset, 2023.\nURL\nhttps://github.com/togethercomputer/\nRedPajama-Data.\nDettmers, T. and Zettlemoyer, L. The case for 4-bit pre-\ncision: k-bit inference scaling laws.\narXiv preprint\narXiv:2212.09720, 2022.\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.\nLLM.int8(): 8-bit matrix multiplication for transformers\nat scale. arXiv preprint arXiv:2208.07339, 2022.\nDettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,\nL. QLoRA: Efficient finetuning of quantized llms. arXiv\npreprint arXiv:2305.14314, 2023a.\nDettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev,\nD., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T.,\nand Alistarh, D. SpQR: A sparse-quantized representation\nfor near-lossless llm weight compression. arXiv preprint\narXiv:2306.03078, 2023b.\nDu, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D.,\nXu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O.,\net al. GLaM: Efficient scaling of language models with\nmixture-of-experts. In International Conference on Ma-\nchine Learning (ICML), 2022.\nElsen, E., Dukhan, M., Gale, T., and Simonyan, K. Fast\nsparse convnets. In Conference on Computer Vision and\nPattern Recognition (CVPR), 2020.\nFedus, W., Zoph, B., and Shazeer, N. Switch transform-\ners: Scaling to trillion parameter models with simple\nand efficient sparsity. The Journal of Machine Learning\nResearch, 23(1):5232\u20135270, 2022.\nFrantar, E. and Alistarh, D. SparseGPT: Massive language\nmodels can be accurately pruned in one-shot. In Interna-\ntional Conference on Machine Learning (ICML), 2023.\nFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh,\nD.\nGPTQ: Accurate post-training compression for\ngenerative pretrained transformers.\narXiv preprint\narXiv:2210.17323, 2022.\nFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D.\nGPTQ code, 2023.\nURL https://github.com/\nIST-DASLab/gptq.\nGale, T., Narayanan, D., Young, C., and Zaharia, M.\nMegaBlocks: Efficient sparse training with mixture-of-\nexperts. In Conference on Machine Learning and Systems\n(MLSys), 2023.\nGholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W.,\nand Keutzer, K.\nA survey of quantization methods\nfor efficient neural network inference. arXiv preprint\narXiv:2103.13630, 2021.\nGoogle.\nT5x, 2023.\nURL https://github.com/\ngoogle-research/t5x.\nHoefler, T., Alistarh, D., Ben-Nun, T., Dryden, N., and\nPeste, A. Sparsity in deep learning: Pruning and growth\nfor efficient inference and training in neural networks.\narXiv preprint arXiv:2102.00554, 2021.\nHubara, I., Nahshan, Y., Hanani, Y., Banner, R., and Soudry,\nD. Accurate post training quantization with small cal-\nibration sets. In International Conference on Machine\nLearning (ICML), 2021.\nHuffman, D. A. A method for the construction of minimum-\nredundancy codes. Proceedings of the IRE, 40(9):1098\u2013\n1101, 1952.\nHwang, C., Cui, W., Xiong, Y., Yang, Z., Liu, Z., Hu, H.,\nWang, Z., Salas, R., Jose, J., Ram, P., et al. Tutel: Adap-\ntive mixture-of-experts at scale. In Conference on Ma-\nchine Learning and Systems (MLSys), 2023.\nKim, Y. J., Fahim, R., and Awadalla, H. H. Mixture of\nquantized experts (MoQE): Complementary effect of low-\nbit quantization and robustness. OpenReview, 2022a.\nKim, Y. J., Henry, R., Fahim, R., and Awadalla, H. H.\nWho says elephants can\u2019t run: Bringing large scale\nmoe models into cloud scale production. arXiv preprint\narXiv:2211.10017, 2022b.\nKim, Y. J., Henry, R., Fahim, R., and Awadalla, H. H.\nFinequant:\nUnlocking efficiency with fine-grained\nweight-only quantization for llms.\narXiv preprint\narXiv:2308.09723, 2023.\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\nKoishekenov, Y., Nikoulina, V., and Berard, A. Memory-\nefficient NLLB-200: Language-specific expert pruning\nof a massively multilingual machine translation model.\narXiv preprint arXiv:2212.09811, 2022.\nKurtic, E., Campos, D., Nguyen, T., Frantar, E., Kurtz,\nM., Fineran, B., Goin, M., and Alistarh, D. The Op-\ntimal BERT Surgeon: Scalable and accurate second-\norder pruning for large language models. arXiv preprint\narXiv:2203.07259, 2022.\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang,\nY., Krikun, M., Shazeer, N., and Gshard, Z. Scaling\ngiant models with conditional computation and automatic\nsharding. arXiv preprint arXiv:2006.16668, 2020.\nLeviathan, Y., Kalman, M., and Matias, Y. Fast inference\nfrom transformers via speculative decoding. In Interna-\ntional Conference on Machine Learning (ICML), 2023.\nLin, B., Zheng, N., Wang, L., Cao, S., Ma, L., Zhang,\nQ., Zhu, Y., Cao, T., Xue, J., Yang, Y., et al. Efficient\nGPU kernels for n:m-sparse weights in deep learning. In\nConference on Machine Learning and Systems (MLSys),\n2023.\nMacKay, D. J. Information theory, inference and learning\nalgorithms. Cambridge University Press, 2003.\nNagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and\nBlankevoort, T. Up or down? Adaptive rounding for\npost-training quantization. In International Conference\non Machine Learning (ICML), 2020.\nNagel, M., Fournarakis, M., Amjad, R. A., Bondarenko,\nY., van Baalen, M., and Blankevoort, T. A white pa-\nper on neural network quantization.\narXiv preprint\narXiv:2106.08295, 2021.\nPark, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee,\nD. nuQmm: Quantized matmul for efficient inference of\nlarge-scale generative language models. arXiv preprint\narXiv:2206.09557, 2022.\nPuigcerver, J., Riquelme, C., Mustafa, B., and Houlsby, N.\nFrom sparse to soft mixtures of experts. arXiv preprint\narXiv:2308.00951, 2023.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. OpenAI blog, 1(8):9, 2019.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research, 21\n(140):1\u201367, 2020a.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the\nlimits of transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research (JMLR),\n21(1):5485\u20135551, 2020b.\nRoller, S., Sukhbaatar, S., Weston, J., et al. Hash layers for\nlarge sparse models. In Conference on Neural Informa-\ntion Processing Systems (NeurIPS), 2021.\nShazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A.,\nKoanantakool, P., Hawkins, P., Lee, H., Hong, M., Young,\nC., et al. Mesh-tensorflow: Deep learning for supercom-\nputers. Conference on Neural Information Processing\nSystems (NeurIPS), 2018.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023b.\nWang, P., Chen, Q., He, X., and Cheng, J. Towards accurate\npost-training network quantization via bit-split and stitch-\ning. In International Conference on Machine Learning\n(ICML), 2020.\nWelch, T. A. A technique for high-performance data com-\npression. Computer, 17(06):8\u201319, 1984.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,\nMoi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\net al. Huggingface\u2019s transformers: State-of-the-art natural\nlanguage processing. arXiv preprint arXiv:1910.03771,\n2019.\nWu, X., Yao, Z., and He, Y. ZeroQuant-FP: A leap forward\nin llms post-training w4a8 quantization using floating-\npoint formats. arXiv preprint arXiv:2307.09782, 2023.\nYao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and\nHe, Y. ZeroQuant: Efficient and affordable post-training\nquantization for large-scale transformers. arXiv preprint\narXiv:2206.01861, 2022.\nYi, R., Guo, L., Wei, S., Zhou, A., Wang, S., and Xu,\nM. Edgemoe: Fast on-device inference of moe-based\nlarge language models. arXiv preprint arXiv:2308.14352,\n2023.\nYu, C., Chen, T., and Gan, Z. Boost transformer-based\nlanguage models with gpu-friendly sparsity and quanti-\nzation. In Findings of the Association for Computational\nLinguistics: ACL 2023, 2023.\nQMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\nZhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V.,\nDai, A. M., Le, Q. V., Laudon, J., et al. Mixture-of-\nexperts with expert choice routing. Conference on Neural\nInformation Processing Systems (NeurIPS), 2022.\nZoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J.,\nShazeer, N., and Fedus, W. ST-MoE: Designing stable\nand transferable sparse expert models. arXiv preprint\narXiv:2202.08906, 2022.\n"
  },
  {
    "title": "CommonCanvas: An Open Diffusion Model Trained with Creative-Commons Images",
    "link": "https://arxiv.org/pdf/2310.16825.pdf",
    "upvote": "22",
    "text": "CommonCanvas: An Open Diffusion Model Trained\nwith Creative-Commons Images\nAaron Gokaslan1\nA. Feder Cooper1\nJasmine Collins2\nLandan Seguin2\nAustin Jacobson2\nMihir Patel2\nJonathan Frankle2\nCory Stephenson2\nVolodymyr Kuleshov1\n1Cornell Tech\n{akg87,afc78,vk379}@cornell.edu\n2Databricks Mosaic\n{firstname.lastname}@databricks.com\nAbstract\nWe assemble a dataset of Creative-Commons-licensed (CC) images, which we use\nto train a set of open diffusion models that are qualitatively competitive with Stable\nDiffusion 2 (SD2). This task presents two challenges: (1) high-resolution CC im-\nages lack the captions necessary to train text-to-image generative models; (2) CC\nimages are relatively scarce. In turn, to address these challenges, we use an intu-\nitive transfer learning technique to produce a set of high-quality synthetic captions\npaired with curated CC images. We then develop a data- and compute-efficient\ntraining recipe that requires as little as 3% of the LAION data (i.e., roughly 70 mil-\nlion examples) needed to train existing SD2 models, but obtains the same quality.\nThese results indicate that we have a sufficient number of CC images (also roughly\n70 million) for training high-quality models. Our training recipe also implements\na variety of optimizations that achieve \u223c3X training speed-ups, and that enable\nrapid model iteration. We leverage this recipe to train several high-quality text-\nto-image models, which we dub the CommonCanvas family. Our largest model\nachieves comparable performance to SD2 on human evaluation, even though we\nonly use a CC dataset that is <3% the size of LAION and synthetic captions\nfor training. We release our models, data, and code at https://github.com/\nmosaicml/diffusion/blob/main/assets/common-canvas.md.\n1\nIntroduction\nCurrent methods train high-quality, text-to-image (T2I) models with. A lack of curated datasets\nthat are large enough for the task has led researchers to turn to web-scraped solutions [29, 30],\nlike LAION-2B [26]. The use of web-scraped data is a very common practice for training genera-\ntive models, however, US courts have yet to definitively rule if this is permissible under copyright\nlaw [1, 13, 15, 20, 21, 60]. In response, recent work has begun to investigate alternative methods of\nnavigating copyright concerns in text generation [39], code completion [16, 51], and image genera-\ntion [24]. Nevertheless, matching the performance of state-of-the-art models remains a challenge. In\nthis work, we study the following natural question: Is it possible to efficiently produce a high-quality\nT2I model by training only on Creative-Commons-licensed data?\nWe suggest a possible path forward, training a suite of T2I architectures using only open-licensed,\nCreative-Commons (CC) images (Figures 1 & 2).\nThis task brings to light two significant\nchallenges.\nThe first problem is data incompleteness: almost all CC images lack the captions\nnecessary to train a high-quality T2I model. The second is data scarcity: there are relatively few\nhigh-resolution CC images \u2014 roughly 70 million, compared to LAION-2B\u2019s roughly 2 billion [26].\narXiv:2310.16825v1  [cs.CV]  25 Oct 2023\nPrompt\nSD2-base\nCommonCanvas-S-C CommonCanvas-S-NC CommonCanvas-L-NC\na cute black cat\ninside of a pumpkin\na robot holding a\npaint palette\nan oil painting of\na tall ship sailing\nthrough a field of\nwheat at sunset\nFigure 1: Selection of text prompts. Using entirely Creative-Commons images and our synthetic\ncaptioning approach, we achieve comparable qualitative performance to Stable Diffusion 2 (SD2-\nbase), as seen in CommonCanvas generations, while only requiring a small fraction (< 3%) of\nthe amount of training data. We include results for two CommonCanvas architectures, small (S)\nand large (L) (Section 6), and two CC-image datasets, commercial (C) and non-commercial (NC)\n(Section 4). We label our results accordingly as CommonCanvas-<architecture>-<dataset>.\nWe address the data incompleteness problem by using a pre-trained BLIP-2 model [34], which we\nuse to produce high-quality, synthetic captions for a set of curated, open licensed CC images. This is\nan intuitive transfer-learning solution: leveraging powerful pre-trained generative models to produce\nsynthetic labels for an unlabeled dataset, which we can then use to train a different multimodal\ngenerative model. We note that this is an increasingly common pattern in the literature, which we\nshorthand with the name telephoning.\nTo deal with data scarcity, we propose a data- and compute-efficient training recipe that obtains the\nsame quality as SD2, but (perhaps surprisingly) requires as little as 3% of the LAION-2B data (i.e.,\nroughly 70 million examples) originally used to train SD2. We call this model SD2-base. These\nresults indicate that we have a sufficient number of CC images (also roughly 70 million) for training\nhigh-quality models. Our training recipe also implements a variety of optimizations that achieve\n\u223c3X training speed-ups, and that allow for rapid model iteration.\nThe above methods enable us to create CommonCanvas, a suite of latent diffusion model (LDM)\narchitectures trained on our curated dataset of CC images and synthetic captions, which we denote\nCommonCatalog. For CommonCanvasL-NC, we swap SD2\u2019s UNet for SDXL to demonstrate how\neven with less data, larger models do not overfit to this smaller dataset. Our largest model achieves\nperformance comparable to SD2-base on human evaluation of Parti Prompts [66], even though our\nCommonCatalog training dataset is < 3% the size of LAION and has synthetically generated cap-\ntions. Figure 1 shows select samples from our CommonCanvas models compared to corresponding\nsamples from SD2-base. Although this model is a larger and - likely - more capable model architec-\nture than SD2, we find it surprising and important that it is possible to train an SD2-quality model at\nall based on such a limited dataset that was cobbled together in this fashion. This reveals a promising\npath forward for future research on highly-capable, open T2I models. In summary, we:\n\u2022 Synthesize a set of high-quality captions for uncaptioned CC images, which we can then use to-\ngether for training. We note that this type of transfer-learning technique is increasingly common,\nand we give it the shorthand name telephoning (Section 3).\n\u2022 Curate CommonCatalog, a dataset of roughly 70 million open-licensed CC images, for which\nwe use telephoning to generate accompanying high-quality synthetic captions (Section 4).\n2\nan image of\nelsa from\nfrozen\n(a) Prompt\n(b) SD2 Output\n(c) CommonCanvas\nOutput\nthe lion king\n(d) Prompt\n(e) SD2 Output\n(f) CommonCanvas\nOutput\nFigure 2: When given prompts for concepts related to Disney movies (a, d), SD2-base generates a\nrecognizable image of Elsa from Frozen (b) and a poster-like image with a misshapen Disney logo\nand characters resembling those from The Lion King (e), and CommonCanvas (-SC) does not (c, f).\n\u2022 Train and evaluate CommonCanvas, a suite of LDM architectures trained on CommonCatalog.\nWe demonstrate that these models produce competitive qualitative and quantitative results com-\npared to the SD2-base baseline (Section 6). To make this analysis tractable, we implement a va-\nriety of training optimizations, which achieve \u223c3X speed-ups in training SD2-base (Section 5).\n\u2022 Release our CommonCatalog dataset of CC images and synthetic captions along with\nour trained CommonCanvas model at https://github.com/mosaicml/diffusion/blob/\nmain/assets/common-canvas.md.\n2\nPreliminaries and Motivation\nIn this section, we present background on training the T2I Stable Diffusion model, originally trained\non the web-scraped LAION-2B dataset. We then discuss copyright and reproducibility with respect\nto LAION datasets. This discussion motivates the creation of an alternative dataset composed of\nopen licensed, CC images with synthetic captions, which we introduce in Section 4.\n2.1\nText-to-image generative models\nText-to-image (T2I) generative models refer to large neural networks trained on paired image-\ncaption data examples. One such family of T2I models is Stable Diffusion (SD) [47]. SD is a\nlatent diffusion model (LDM) that converts images to latent representations and back again us-\ning Variational Autoencoders (VAEs) [23]; it uses an iterative sampling procedure [57] and trains\nan underlying UNet [48]. The architecture also includes a text encoder, such as the Contrastive\nLanguage-Image Pre-training (CLIP) model [43] \u2013 either the original CLIP from OpenAI [45] or its\nopen-source counterpart, OpenCLIP [10, 18].\nStable Diffusion 2 (SD2)\u2019s UNet has approximately 865 million trainable parameters; Stable Diffu-\nsion XL (SDXL) is larger, with 2.6 billion parameters, and has other advancements involving aspect\nratio bucketing, micro-conditioning, and multiple text encoders and tokenizers. In terms of train-\ning data, the SD-family of models and OpenCLIP are both trained on subsets of the LAION-5B\ndataset [3, 53]. The exact training dataset for CLIP is unknown, but it is likely webscraped data [45]\n2.2\nCopyright and reproducibility in relation to LAION datasets\nLAION-5B is a dataset derived from a snapshot of the Common Crawl, a massive corpus of data\nscraped from the web. From this snapshot, the LAION organization curated pairs of image URLs\nand their corresponding alt-text captions for the intended use of training T2I and image-to-text (I2T)\ngenerative models [3, 53]. In practice, T2I models are typically trained on filtered subsets of the full\nLAION-5B dataset (e.g. LAION-2B [26]). Training T2I models on this dataset requires visiting the\nURLs and downloading the associated images. There are two elements of LAION datasets that are\nrelevant to our work:\nCopyright. The images associated with LAION datasets have unclear provenance: it is often not\nknown what the original image sources are [29, 30]. Courts have not yet decided if training on\nthese datasets is \u201cfair use\u201d \u2014 an important exception in copyright [29, 33, 50, 56]. In the interim,\nthere are several copyright lawsuits for the alleged use of LAION-5B subsets to train generative\nmodels [1, 15, 20, 61].\n3\nBLIP-2\nLAION-400M\nBLIP-2\nCC Image\nCommonCatalog\nN=~70M\ni=1\nCaption\nN=~70M\ni=1\n(a) Pre-trained BLIP-2.\nBLIP-2\nLAION-400M\nBLIP-2\nCC Image\nCommonCatalog\nN=~70M\ni=1\nCaption\nN=~70M\ni=1\nCommonCanvas\n(b) Generating CommonCatalog for training CommonCanvas.\na black and \nwhite cartoon \ndog with \nblack ears\nBLIP-2\nCommon\nCanvas\n(c) \u201cLossy compression\u201d via BLIP-2 from an input image to a synthetic caption. When we use a T2I\nmodel to generate an image with this \u201clossy\u201d caption (e.g., via CommonCanvas), the resulting generation\nlooks nothing like the original prompt image that produced the caption.\nFigure 3: (a) LAION\u2019s massive dataset of image-caption pairs is used to train BLIP-2, an image-to-\ntext model. (b) We leverage BLIP-2 to produce synthetic captions for our caption-less CC images,\nand use the resulting synthetic image-caption pairs (the CommonCatalog dataset) to train our open\ndiffusion model, CommonCanvas. (c) Although BLIP-2 was trained on LAION (e.g., including\npictures of characters Snoopy), the captions it produces behave like a \u201clossy compression\u201d (e.g., a\nblack and white cartoon dog with black ears, which has no mention of Snoopy). When\nwe supply such \u201clossy\u201d captions to a T2I model, like a game of telephone, it produces outputs that\nno longer resemble the original images (e.g., we show how CommonCanvas produces an image that\nmatches the caption, but does not look like Snoopy).\nReproducibility. Since the datasets only contain the image URLs, and not the images themselves,\nthey are plagued with link rot [27].1 When accessing LAION-5B, there is no guarantee the images\nstill exist at their URLs, making it impossible to fully reproduce the dataset and opening up the\npossibility of data poisoning attacks [8].\nA natural alternative is to not use LAION datasets for training. One could instead independently\ncurate a dataset of CC-licensed images with known provenance that expressly allow for copying,\nadaptation, and commercial use. As constituent images can be stored and distributed, this would\nalso solve the link rot problem, thereby enabling greater reproducibility. We defer our discussion of\nsourcing CC-licensed images to Section 4, where we detail CommonCatalog: our new, open dataset.\nWhile CC images are an attractive alternative to LAION-5B, we note that CC images rarely contain\nthe captions necessary to train T2I models. Therefore, we first need a method for captioning CC\nimages, which we describe in the next section.\n3\nTelephoning: A Transfer Learning-based Image-captioning Method\nOur solution for handling the lack of captions in CC images is an intuitive type of transfer learning\nfor producing high-quality synthetic labels. We describe this method, and then note that there are\nvarious similar methods in prior generative modeling literature. Altogether, these methods indicate\nthat this type of transfer learning to produce synthetic labels (to later serve as inputs to training other\ngenerative models) has become an increasingly common pattern. We therefore give this method a\nname: telephoning.\n3.1\nDescribing telephoning\nTelephoning (Figure 3) takes inputs from a high-dimensional modality (e.g., images), effectively\nperforms a \u201clossy compression\u201d to a low-dimensional modality (e.g., short-text captions), and then\n1This also applies to other scraped datasets, such as DataComp [14] and OBELICS [28].\n4\ndecompresses back to the high-dimensional modality. Because the intermediate compression step\nis \u201clossy\u201d, the ultimate output often does not remotely resemble the original input, just like a game\nof telephone [38]. We derive the term telephoning from the above intuition, and employ it as useful\nshorthand to denote instances of transfer learning that solve data-scarcity problems in multimodal\ngenerative modeling.\nIn this work, CC images are the high-dimensional inputs, and we use a pre-trained BLIP-2\nmodel [34] for \u201clossy compression\u201d to short-text captions (Figure 3a). Together, these CC-image-\ncaption pairs comprise the CommonCatalog dataset, which we use to train our CommonCanvas T2I\nmodels (Figure 3b). Even though BLIP-2 was pre-trained on LAION-400M [52], CommonCata-\nlog and CommonCanvas never have direct access to LAION-400M or, importantly, anything that is\nsimilar to the images that BLIP-2 was trained on. Instead, we only have access to the mapping in\nthe model, which, given an image input, produces lossy output text that inherently does not literally\nresemble its image counterpart (Figure 3c).2\nWe defer to experts about fair use (Section 2.2) \u2014 namely, regarding models like BLIP-2, and\nLAION-5B\u2019s images and alt-text captions. Generally, these experts seem to think that many cases\nwill fall under fair use [29, 32, 50], especially when model outputs do not resemble their inputs,\nwhich is the case with BLIP-2.\n3.2\nRelated work on telephoning\nOur work aligns with the trend of using advanced generative models to address data scarcity. This is\nevident in various modalities, such as producing audio captions from image-text pairs [64] and text\nfrom audio [46]. Similar approaches have also been used to generate instruction tuning datasets for\nboth text and images [35, 37]. Concurrent work has used visual question answers models such as\nLLava [37] to enhance existing captions such as such as DALLE\u00b73 [4] and Chen et al. [9]. However,\nour model is the one of the first work to train on a dataset without any ground truth captions, and\none of the first to release our synthetic captioning dataset along with a fully trained diffusion model.\nFurthermore, the caption upsampling approaches described in these works could be used to further\nimprove the captions of the CommonCatalogue in future work. Captioning models have been used\nbefore to create descriptive captions before to guide a diffusion model to create an image visually\nsimilar to a specific image. The concurrent work SynthCap [6] generates a synthetic captioning\ndataset using a diffusion model to generate images from captions, tackling the inverse of our problem\nstatement.\nWe coin the term telephoning to shorthand processes like these, which include our work and prior\nwork, and which we believe will become more prevalent as generative models progress.\n4\nCommonCatalog: A Dataset of CC Images & Synthetic Captions\nIn this section, we introduce our open dataset, CommonCatalog. First, we describe the collection\nand curation process for the open-licensed, CC images. This process brings to light two challenges:\ncaption-data incompleteness and image-data scarcity. To address the lack of CC captions, we show\nconcretely how we use telephoning to produce high-quality synthetic captions to accompany our\nset of curated images. We investigate the topic of data scarcity in the next section, where we also\ndiscuss necessary systems-level training optimizations that enable us efficient SD-model iteration.\n4.1\nSourcing provenanced, licensed images for CommonCatalog\nWe focus on locating high-resolution Creative-Commons images that have open licenses.\nWe\nbegan with the YFCC100M dataset, which consists of 100 million CC-licensed images and\nmultimedia files, as well as Flickr IDs linking to the original data [59].\nThe images in the\ndataset associated with the original paper exhibit two issues that make it ill-suited for direct\nuse to train Stable Diffusion: they are low-resolution, and many of them have licenses that\ndo not expressly allow for the distribution of derivative works, which are an area of unset-\ntled copyright law in the context of model training.\nWe therefore re-scraped these images\nfrom Flickr, based on the IDs provided in the YFCC100M metadata.\nOur scraped images\nare very high resolution (exceeding 4K), which makes them more suitable for T2I training.\n2We draw on the example of Snoopy from [49]. Figure 3\u2019s Snoopy is CC-licensed [54].\n5\nSource\nCaption\nAlt-Text (LAION-2B)\nLatest 1PC Transparent Gradient Color Voile\nWindow\nCurtain\nBLIP2-OPT-2.7B\nA living room with a white couch and curtains\nFigure 5: Original vs. BLIP-2-generated captions for an image from LAION-2B. BLIP-2 generates\na caption that better aligns with what a human would write. See Figure 14 for more examples.\nFigure 4:\nCommonCatalog-C contains images\nlicensed only for commercial use; -NC contains -C\nas well as images licensed for non-commercial use.\nDataset\n# Images\n% Alt Text\nCommonCatalog-C\n26,232,417\n30.76%\nCommonCatalog-NC\n67,015,331\n31.22%\nWe exclude images with non-derivative (ND)\nlicenses. The remaining images can be further\ndivided into those that can be used for com-\nmercial (C) purposes and those that cannot\n(non-commercial/ NC). As shown in Ta-\nble 4, we accordingly construct two datasets,\nCommonCatalog-C\nand\nCommonCatalog-\nNC. We defer additional details about licenses\nto Appendix B.1.1, but emphasize that all of the images included have open licenses: individuals are\nfree to use, adapt, and remix the images, so long as they attribute them. In total, CommonCatalog\ncontains roughly 70 million NC CC-images, of which a subset of approximately 25 million images\ncan also be used commercially.\nDirectly sourcing CommonCatalog avoids some concerns (Section 2.2); however, it also comes with\nits own challenges. For one, CC images rarely have the alt-text captions necessary to train a T2I\nmodel like Stable Diffusion (Figure 4); those that do have associated text often just include the\nimage title or a URL. For another, we could only find roughly 70 million usable CC images, which\npales in comparison to the billions of images in LAION used to train SD2 (Section 5). We take each\nof these challenges in turn. First, in the next subsection, we show how we instantiate telephoning\n(Section 3) to produce high-quality, synthetic captions for CC images.\n4.2\nSynthesizing captions with telephoning\nWe compared several captioning models and, based on qualitative analysis and its state-of-the-art\nperformance on MS COCO, chose to use the pre-trained BLIP-2 OPT2.5B model for synthesizing\nCommonCatalog\u2019s captions [34]. BLIP-2 consists of three components: a pre-trained, frozen (i.e.,\nfixed) visual encoder, a learned transformer network that converts the visual embeddings into a text\nprompt, and a frozen large language model (LLM) that takes in the prompt. The only trainable\nvariables in the transformers are between the frozen visual encoder and frozen LLM layers.\nGiven a LAION-2B image as input, we found that the resulting BLIP-2 caption is often qualitatively\nmore descriptive than the corresponding LAION-2B ground-truth alt-text caption. LAION-2B cap-\ntions often contain product names, irrelevant details, or poor grammar and syntax (Figure 5). This\nfinding is corroborated by Nguyen et al. [42], which shows quantitatively (in terms of CLIP Score)\nthat BLIP-2 captions are higher quality than ground-truth captions, at the cost of caption diversity.\nBased on these preliminary results, we captioned all of the YFCC100M Creative-Commons images,\nwhich required about 1,120 GPU A100 hours. To do so, we center-cropped and resized all of the\nimages to a maximum size of 512x512 pixels. We perform these transformations because captioning\nimages at native resolution would be very expensive. At training time of the diffusion model, all\nimages remain in their native resolution. We release our commercial (CommonCatalog-C) and non-\ncommercial (CommonCatalog-NC) CC-image and synthetic-caption datasets on HuggingFace at\n[REDACTED] with associated data cards. As an evaluation set, we also release the BLIP-2 captions\nthat we produced for the non-derivative (ND) CC images that we did not use for training.\n5\nTraining Efficiency Optimizations and Data Scarcity Analysis\nHigh-resolution CC images are indeed much less abundant than arbitrary web-scraped ones, but the\namount of data necessary to train high-quality SD2 models has not been well-studied. We set out to\nquantify this amount by training multiple SD2 models on differently-sized subsets of LAION-2B.\nHowever, training a single SD2 model, even with hundreds of GPUs, can take several days. To make\nour data scarcity analysis more tractable, we first implement several efficiency optimizations.\n6\n5.1\nSoftware and hardware speed-ups\nStability AI reports an estimated 200,000 A100 hours to train SD2 [58]. Depending on the available\nhardware, a single SD2 run could take anywhere from a few weeks to over a month to train. We\nsought out multiple avenues to reduce this training-time constraint. Ultimately we were able to\nachieve a speedup of 2.71X relative to the original SD2 implementation.\nFirst, we applied Flash Attention [11] with the xFormers library [31]. We also pre-computed VAE\nand text encoder latents over the entire training dataset, cast all GroupNorm [63] and LayerNorm [2]\nto float16 precision, and applied fully-sharded data parallelism (FSDP) to our training run. Finally\nwe opted to only keep an exponential moving average of the weights for the final 3.5% of training.\nMore detail on each of these improvements can be found in Appendix D.\nWhen applying all of the aforementioned strategies together, we are able to achieve a 2.71X speedup\nin A100 hours over our SD2-baseline implementation. We found that latent pre-computation helped\nthe most at low resolutions, while FSDP also provided significant gains, especially at scale. The\nother optimizations helped reduce total memory usage, allowing us to increase the microbatch size\nfor better hardware utilization. Figure 6 summarizes each of the proposed methods and the cumu-\nlative speedup that results from its application. Equipped with an optimized training setup, we are\nable to more easily study effect of varying training dataset size.\n5.2\nInvestigating data scarcity: Saturating SD2 evaluations with < 3% of LAION-2B\nYFCC100M contains 100 million images, about 10% the size of the 1.1B LAION examples we could\naccess, thus about 5% of the original LAION-2B dataset. One interesting question that remains\nunanswered is how much data is actually needed to train these diffusion models effectively.\nWe ask whether or not it is necessary to train on 1+ billion images to get results that are as good as the\noriginal LAION-trained SD2. Our results show, surprisingly, that this is not the case with a slightly\nlarger model (CommonCanvas-L); this model replaces SD2\u2019s U-Net with SDXL\u2019s [43] larger one.\nFurther, our larger model achieves comparable results to SD2-base on human evaluation, using 33X\nless training data. We train on increasingly smaller, random subsets of data from our LAION-1.1B\nmodel and find that we can achieve a similar result on the commonly reported MS COCO numbers,\nbut with <3% the amount of SD2\u2019s training data (Figure 8). In fact, we run experiments down to\n1-million LAION-1.1B images, and find that only 10 million images are required for stable training\nbehavior (Appendix, Figure 15).\n5.3\nInvestigating the performance of CC trained model\nThese findings suggest that SD2 models may be underparameterized.\nIn fact, when we use\nCommonCanvas-LNC, we achieve competitive performance with SD2 on user preferences, de-\nspite training on significantly less data (Section 7). Further, in spite of the drastic reduction in\ndataset size, we observe that the larger model (CommonCanvas-LNC) outperforms the smaller one\n(CommonCanvas-SNC), consistent with the notion that these models are still underparameterized.\nWe hypothesize about why this might be the case and how much data is actually necessary to saturate\nthe model in Appendix A.1.\nBaseline\n+ Microbatch 4->8\n + Precomputed latents\n+ Low Precision LN/GN\n+ Microbatch 8->16\n+ FSDP\n+ no EMA\n0\n1000\n2000\n3000\n4000\nThroughput (samples/s)\nFigure 6: Cumulative effect of various speed-\nups in our SD2 training pipeline on 128\nThroughputs evaluated on 128 A100s.\nCommonCanvas-SC\nCommonCanvas-SNC\nCommonCanvas-LNC\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nPreference rate\n443\n401\n437\nFigure 7:\nUser preference study using\nParti prompts. CommonCanvas-LNC model\nmatches the performance of SD2 despite be-\ning trained with < 3% the amount of data.\n.\n7\n29\n30\n31\n32\nCLIP Score\n8\n10\n12\n14\n16\n18\nFID\n10m LAION captions\n90m LAION captions\n10m BLIP2 captions\n29\n30\n31\n32\nCLIP-Score\n0.002\n0.004\n0.006\n0.008\nKID\n29\n30\n31\n32\nCLIP-Score\n8\n9\n10\n11\n12\n13\nCLIP-FID\nFigure 8: FID, KID, and CLIP-FID vs. CLIP-Score computed on 30K samples from COCO2014\nfor different SD2 models trained on smaller subsets of LAION (10M, 90M, using either original\ncaptions or synthetic BLIP2 captions. Interestingly, increasing the amount of training data from\n10M to 90M samples does not lead to improved quantitative metrics across guidance scales 1 to 8.\nLower FID is better; higher CLIP score is better.\npaintings\nphotography\nCOCO 2014\npeople\nfaces\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nCLIP-FID\n1.13B LAION\n10M LAION\n90M LAION\n90M BLIP2\n30M CommonCanvas-SC\n70M CommonCanvas-SNC\nFigure 9: CLIP-FID for different models. We can see domain shift between MS COCO captions\nand web-scraped conceptual captions. CLIP-FID likely favors SD2, as CLIP is trained on a similar\nstyle of text as LAION. This plot only covers the first stage of training at 256x256 resolution. We\n6\nExperiments\nEquipped with commercial (CommonCatalog-C) and non-commercial (CommonCatalog-NC)\ndatasets, we train two different CommonCanvas models. We additionally train a larger variant of\nCommonCanvas-NC (CommonCanvas-LNC) that, as we note above (Section 5.2), has a signifi-\ncantly larger U-Net. Figure 1 displays qualitative results from each of these model variants. More\ndetails on the CommonCanvas-LNC architecture can be found in Appendix A.2.\n6.1\nAutomated quality metrics for model evaluation\nWe measure performance with three automated image quality metrics on the MS COCO dataset [36]:\nFrechet Inception Distance (FID) [17], Kernal Inception Distance (KID) [5], and CLIP-FID [25].\nAdditionally, CLIP Score was evaluated to understand the alignment between captions and their\nrespective images. Our model demonstrated comparable performance compared to the baseline of\nSD2 on the popular MS COCO benchmark.\nHowever, like any model, ours has limitations. It underperformed in several categories, including\nfaces, general photography, and paintings. These categories originated from the Conceptual Cap-\ntions dataset [55], which relies on web-scraped data. These web-sourced captions, while abundant,\nmay not always align with human-generated language nuances.\nThis discrepancy underscores the importance of incorporating large-scale, human-generated caption\ndata. Although transitioning to synthetic captions introduces certain performance challenges, the\n8\nOurs\nSD2\nOurs\nSD2\nOurs\nSD2\nice princess\nSnoopy\na adventurous archaeologist with\na whip and a fedora\nA teenage wizard with round\nglasses\na cartoon beagle in a red dog\nhouse\nblack and white stencil little\ngirl reaching for a red balloon\nFigure 10: We compare CommonCanvas-SNC (Ours) to SD2. Our model is less likely to generate\niconic characters given suggestive prompts (drawn from Lee et al. [29]).\ndrop in performance is not as dramatic as one might assume. Moreover, we speculate that it would if\nusers were to supplement with their own datasets, like FFHQ [22], if they seek to fine-tune models\nfor specific categories.\n6.2\nHuman evaluation\nWhile automated quality metrics are useful, given the level of detail and breadth of of the distribution\nlarge T2I are intended to generate, there is no substitute for evaluation by human raters. Human\npairwise preference ratings for the three 512x512 resolution CommonCanvas models compared to\nSD2-base can be seen in Figure 7.\nIn this experiment, human raters were shown a prompt (selected randomly from the PartiPrompts\nprompts set [66]) along with two generated images in randomized order, one from the reference\nmodel (SD2-base) and the other from a CommonCanvas model. Users were asked which generated\nimage they preferred. We report the fraction of the time users selected the image generated by the\nCommonCanvas model over the corresponding generation from SD2 as the user preference rate for\nthat model. In agreement with our automated quality metrics, we find that the two small Common-\nCanvas models are less perferred than SD2-base, with preference rates of 37% for CommonCanvas-\nSC and 38% for CommonCanvas-SNC, which we find surprisingly high considering the smaller and\nsynthetic nature of the dataset. For the largest model, CommonCanvas-LNC, we do not measure\na statistically significant difference in user preference between this model and SD2-base. While\nSDXL is a significantly larger model, this finding represents an existential result, showing that we\nare capable of matching the performance of a model trained on several magnitudes more of data.\n6.3\nBenefits and challenges of synthetic captions\nInterestingly, we observe that synthetic captions can enhance the alignment of our model. For in-\nstance, the CLIP Score for synthetic captions exceeded that of ground-truth captions as seen in\nFigure 8.\nWe also observed reduced diversity of n-grams in our synthetic captions, a pattern previously noted\nby Nguyen et al. [42]. This effect can be visualized through the decrease in unique trigrams.\nAlthough we train on Creative-Commons images, it is still possible for an adversarial prompt to\nproduce content that, for example, includes iconic characters. In Figure 10, we subject our model\nto ambiguous prompts that are suggestive of such characters. Examples include visuals closely\nresembling Elsa from Frozen, Indiana Jones resembling Harrison Ford, and even a likeness to Harry\nPotter (Figure 10). Qualitatively, our model deviated more from these characters than SD2.\n9\nOurs\nSD2\nOurs\nSD2\nOurs\nSD2\nBill Gates\nElon Musk\nKim Kardashian\nBarack Obama\nHillary Clinton\nRichard Feynman\nFigure 11: Using CommonCanvas-SNC (Ours) to generate celebrities. Our model is worse at syn-\nthesizing individual people than SD2, but is capable of generating some noteworthy public figures.\n7\nDiscussion and Related Work\nIn this paper, we train the family of CommonCanvas text-to-image latent diffusion models on only\nCreative-Commons images and synthetic captions. We discuss the data incompleteness and scarcity\nissues associated with CC images, and how we address each of these issues in turn. For data in-\ncompleteness, we propose telephoning, an intuitive type of transfer learning (Section 3), which we\ninstantiate with BLIP-2 to produce synthetic captions for CC images \u2014 together, the CommonCata-\nlog dataset (Section 4). With regard to data scarcity, we hypothesize that much less data than what is\ncontained in LAION-2B is necessary to saturate SD2, and that CommonCatalog should be sufficient\nfor training. To make testing this hypothesis more efficient, we implement a variety of ML-systems\noptimizations, which achieve a 2.7X speed-up over our SD2 baseline. Ultimately, we find that we\ncan train SD2 on <3% of LAION-2B (Section 5), which encourages us to train on CommonCat-\nalog\u2019s commercial (roughly 70 million) and non-commercial (roughly 25 million) examples. Our\nCommonCanvas models under-perform in some categories, like faces, but CommonCanvas-LNC\ndemonstrates statistically equivalent performance with SD2 on human evaluation (Section 6).\nWe note that several recent works study copyright. This work tends to concern text-to-text training\ndata [39], be primarily theoretical [51, 62], involve ablation studies [24], or only handle verbatim\nmemorization [7] through the use of generation-time content filters [16], which has been shown to\nbe an incomplete solution [19]. To the best of our knowledge, no prior open work attempts to train\nT2I models on only open-licensed data.\nMost prior work on text-caption-dataset creation has focused on extracting caption data from\nCommon Crawl [12, 14, 28].\nWe instead focus on synthesizing captions directly by using a\npre-trained BLIP-2 model. [42] demonstrate that existing caption datasets can be improved by\nusing BLIP2 to re-caption low-quality captions in large datasets like Datacomp, but do not focus on\ncreating a new dataset of synthetic captions, as we do here.\nAn issue, which we do not address, is that the YFCC100M data is about a decade old; its CC images\nare not as current as those in LAION-2B. Given the success of our results, in the future, we plan to\naugment CommonCatalog with Creative-Commons images from other sources, as well as test larger\nCommonCanvas model architectures.\nAcknowledgements\nWe would like to thank Christopher De Sa for feedback on earlier drafts of this work. A. Feder\nCooper is funded by Professor Christopher De Sa\u2019s NSF RI-CAREER award 2046760. This work\nwas also sponsored by Volodymyr Kuleshov\u2019s CAREER grant: #2145577. We also would like to\nthank Apolin\u00b4ario Passos for helping us host the data + models and for insightful discussions along\nthe way.\n10\nReferences\n[1] Anderson v. Stability AI, Ltd., 2023. No. 3:23-cv-00201 (N.D. Cal. Jan. 13, 2023).\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[3] Romain Beaumont. LAION-5B: A New Era of Large-Scale Multi-Modal Datasets. LAION\nBlog, March 2022. URL https://laion.ai/blog/laion-5b/.\n[4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang,\nJuntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,\nYunxin Jiao, and Aditya Ramesh. Improving image generation with better captions. 2023.\n[5] Miko\u0142aj Bi\u00b4nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying\nmmd gans. arXiv preprint arXiv:1801.01401, 2018.\n[6] Davide Caffagni, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara.\nSynthcap: Augmenting transformers with synthetic data for image captioning. In International\nConference on Image Analysis and Processing, pages 112\u2013123. Springer, 2023.\n[7] Nicholas Carlini, Florian Tram`er, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Kather-\nine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00b4Ulfar Erlingsson, Alina Oprea, and Colin\nRaffel. Extracting Training Data from Large Language Models. In 30th USENIX Security Sym-\nposium (USENIX Security 21), pages 2633\u20132650. USENIX Association, August 2021. ISBN\n978-1-939133-24-3.\n[8] Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will\nPearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tram`er. Poisoning Web-\nScale Training Datasets is Practical, 2023.\n[9] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang,\nJames Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-\u03b1: Fast training of diffusion\ntransformer for photorealistic text-to-image synthesis, 2023.\n[10] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade\nGordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws\nfor contrastive language-image learning, 2022.\n[11] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. FlashAttention: Fast\nand memory-efficient exact attention with IO-awareness. In Advances in Neural Information\nProcessing Systems, 2022.\n[12] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-\ntext data created by the people, for the people. arXiv preprint arXiv:2111.11431, 2021.\n[13] Doe 1 v. GitHub, Inc., 2022. No. 4:22-cv-06823 (N.D. Cal. November 3, 2022).\n[14] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao\nNguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. DataComp: In\nsearch of the next generation of multimodal datasets, 2023.\n[15] Getty Images (US), Inc. v. Stability AI, Inc., 2023. No. 1:23-cv-00135 (D. Del. February 3,\n2023).\n[16] GitHub.\nConfiguring\ngithub\ncopilot\nin\nyour\nenvironment,\n2023.\nURL\nhttps://docs.github.com/en/copilot/configuring-github-copilot/\nconfiguring-github-copilot-in-your-environment.\n[17] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A\nreference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.\n[18] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan\nTaori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi,\nAli Farhadi, and Ludwig Schmidt.\nOpenCLIP, July 2021.\nURL https://doi.org/10.\n5281/zenodo.5143773.\n[19] Daphne Ippolito, Florian Tram`er, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine\nLee, Christopher A. Choquette-Choo, and Nicholas Carlini. Preventing Verbatim Memoriza-\ntion in Language Models Gives a False Sense of Privacy, 2023.\n[20] J.L. v. Alphabet Inc., 2023. No. 3:23-cv-03440-LB (N.D. Cal July 11, 2023).\n11\n[21] Kadrey v. Meta Platforms, Inc., 2023. No. 3:23-cv-03417 (N.D. Cal. July 7, 2023).\n[22] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 4401\u20134410, 2019.\n[23] Dirk P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Confer-\nence on Learning Representations, 2014.\n[24] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-\nYan Zhu. Ablating Concepts in Text-to-Image Diffusion Models, 2023.\n[25] Tuomas Kynk\u00a8a\u00a8anniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role\nof imagenet classes in fr\\\u2019echet inception distance. arXiv preprint arXiv:2203.06026, 2022.\n[26] LAION-2Ben, 2022.\nURL https://huggingface.co/datasets/laion/laion2B-en.\nAccessed September 23, 2023.\n[27] Viktor Lakic, Luca Rossetto, and Abraham Bernstein. Link-Rot In Web-Sourced Multime-\ndia Datasets. In MultiMedia Modeling: 29th International Conference, MMM 2023, Bergen,\nNorway, January 9\u201312, 2023, Proceedings, Part I, page 476\u2013488, Berlin, Heidelberg, 2023.\nSpringer-Verlag.\nISBN 978-3-031-27076-5.\ndoi: 10.1007/978-3-031-27077-2 37.\nURL\nhttps://doi.org/10.1007/978-3-031-27077-2_37.\n[28] Hugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bekman, Amanpreet Singh, Anton\nLozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu\nCord, and Victor Sanh. OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-\nText Documents, 2023.\n[29] Katherine Lee, A. Feder Cooper, and James Grimmelmann. Talkin\u2019 \u2019Bout AI Generation:\nCopyright and the Generative-AI Supply Chain, 2023.\n[30] Katherine Lee, A. Feder Cooper, James Grimmelmann, and Daphne Ippolito. AI and Law:\nThe Next Generation, 2023.\n[31] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,\nSean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haz-\niza. xFormers: A modular and hackable Transformer modelling library. https://github.\ncom/facebookresearch/xformers, 2022.\n[32] Mark A. Lemley. How Generative AI Turns Copyright Law on its Head, 2023. URL https:\n//ssrn.com/abstract=4517702orhttp://dx.doi.org/10.2139/ssrn.4517702.\n[33] Pierre N. Leval. Toward a Fair Use Standard. Harvard Law Review, 103(5):1105, 1990.\n[34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023.\n[35] Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason We-\nston, and Mike Lewis.\nSelf-alignment with instruction backtranslation.\narXiv preprint\narXiv:2308.06259, 2023.\n[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Pi-\notr Doll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pages 740\u2013755. Springer, 2014.\n[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n[38] Susan Box Mann. The Telephone Game, 2019. URL https://icebreakerideas.com/\ntelephone-game/. Accessed September 27, 2023.\n[39] Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, and Luke\nZettlemoyer. SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore,\n2023.\n[40] The Mosaic ML Team. composer. https://github.com/mosaicml/composer/, 2021.\n[41] The Mosaic ML Team. streaming. <https://github.com/mosaicml/streaming/>, 2022.\n12\n[42] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Im-\nproving multimodal datasets with image captioning. arXiv preprint arXiv:2307.10350, 2023.\n[43] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00a8uller,\nJoe Penna, and Robin Rombach.\nSDXL: Improving Latent Diffusion Models for High-\nResolution Image Synthesis, 2023.\n[44] Jacob Portes, Alexander R Trott, Sam Havens, Daniel King, Abhinav Venigalla, Moin Nadeem,\nNikhil Sardana, Daya Khudia, and Jonathan Frankle. Mosaicbert: How to train bert with a\nlunch money budget. In Workshop on Efficient Systems for Foundation Models@ ICML2023,\n2023.\n[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning Transferable Visual Models From Natural Language Supervision. In Pro-\nceedings of the 38th International Conference on Machine Learning, 2021.\n[46] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya\nSutskever. Robust speech recognition via large-scale weak supervision. In International Con-\nference on Machine Learning, pages 28492\u201328518. PMLR, 2023.\n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer.\nHigh-Resolution Image Synthesis with Latent Diffusion Models. In 2022 IEEE Conference\non Computer Vision and Pattern Recognition, 2022.\n[48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for\nBiomedical Image Segmentation. Medical Image Computing and Computer-Assisted Inter-\nvention, pages 234\u2013241, 2015.\n[49] Matthew Sag. Copyright Safety for Generative AI. Houston Law Review, 2023. Forthcoming.\n[50] Pamela Samuelson.\nGenerative AI meets copyright.\nScience, 381(6654):158\u2013161, 2023.\ndoi:\n10.1126/science.adi0656.\nURL https://www.science.org/doi/abs/10.1126/\nscience.adi0656.\n[51] Sarah Scheffler, Eran Tromer, and Mayank Varia. Formalizing Human Ingenuity: A Quantita-\ntive Framework for Copyright Law\u2019s Substantial Similarity. In Proceedings of the Symposium\non Computer Science and Law, pages 37\u201349, 2022.\n[52] Christoph Schuhmann. LAION-400-Million Open Dataset, 2021. URL https://laion.ai/\nblog/laion-400-open-dataset/.\n[53] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.\nLAION-5B: An open large-scale dataset for training next generation image-text models. Ad-\nvances in Neural Information Processing Systems, 35:25278\u201325294, 2022.\n[54] Charles M. Schultz.\nSnoopy Peanuts, 2020.\nURL https://en.wikipedia.org/wiki/\nSnoopy#/media/File:Snoopy_Peanuts.png. Accessed September 26, 2023.\n[55] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 2556\u20132565, 2018.\n[56] Benjamin L.W. Sobel. Artificial Intelligence\u2019s Fair Use Crisis. Columbia Journal of Law and\nThe Arts, 41:45, 2017.\n[57] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathany, and Surya Ganguli. Deep Unsu-\npervised Learning using Nonequilibrium Thermodynamics. In Proceedings of the 32nd Inter-\nnational Conference on Machine Learning, 2015.\n[58] Stability AI. Stable Diffusion v2-base Model Card, 2022. URL https://huggingface.co/\nstabilityai/stable-diffusion-2-base.\n[59] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas\nPoland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Com-\nmunications of the ACM, 59(2):64\u201373, 2016.\n[60] Tremblay v. OpenAI, Inc., 2023. No. 3:23-cv-03223 (N.D. Cal. June 28, 2023).\n13\n[61] James Vincent. Getty Images is suing the creators of AI art tool Stable Diffusion for scrap-\ning its content. The Verge, January 2023. URL https://www.theverge.com/2023/1/17/\n23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit.\n[62] Nikhil Vyas, Sham Kakade, and Boaz Barak. On Provable Copyright Protection for Generative\nModels, 2023.\n[63] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference\non computer vision (ECCV), pages 3\u201319, 2018.\n[64] Feiyang Xiao, Qiaoxi Zhu, Jian Guan, Xubo Liu, Haohe Liu, Kejia Zhang, and Wenwu\nWang.\nSynth-ac: Enhancing audio captioning with synthetic supervision.\narXiv preprint\narXiv:2309.09705, 2023.\n[65] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Hongjun Choi, Blake Hechtman, and Shibo\nWang. Automatic cross-replica sharding of weight update in data-parallel training. arXiv\npreprint arXiv:2004.13336, 2020.\n[66] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Va-\nsudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models\nfor content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.\nA\nAdditional Details on Data Scarcity Analysis\nA.1\nHypothesis: Diffusion models are too small\nA back-of-the-envelope calculation provides some insight on why this is the case. Consider a train-\ning dataset consisting of N images with resolution H \u00d7W and c channels. To completely memorize\nthe training data, the model must be capable of storing c\u00d7H \u00d7W \u00d7N numbers. Given a number of\ntrainable parameters Np, it is natural to assume that on average each parameter is capable of storing\nroughly enough information to reconstruct a single number from the training dataset. Under this\nassumption, complete memorization is only possible if the size of the training dataset is at or below\na critical size Nc (N \u2264 Nc) with Nc given by Nc =\nNp\ncHW . Note that this critical size assumes the\ndata cannot be further compressed, which is obviously not the case for natural images. However,\nSD2 and SDXL are latent diffusion models, which first use a pretrained encoder to compress images\nby a factor of 8 in both H and W, and so when we train LDMS like SD2 and SDXL, we are training\non data that has been significantly compressed already.\nIn our experiments, c = 4 and H = W = 32, corresponding to 256 \u00d7 256 resolution RGB images\nin the SD2 and SDXL latent space. The SD2 UNet has Np = 866 \u00d7 106 trainable parameters, and\nSDXL\u2019s UNet has Np = 2567 \u00d7 106. So we calculate Nc \u2248 0.2 \u00d7 106 for SD2 and Nc \u2248 0.6 \u00d7 106\nfor CommonCanvas-Large; both of these numbers are several orders of magnitude below the size of\nour YFCC derived datasets, and so even with significant additional data compression we expect that\nour CommonCatalog datasets should be sufficient to train both SD2 and SDXL. Additionally, this\nargument predicts that we should only begin to see significant overfitting in these models for datasets\nof size N \u223c 106. These estimates are resolution dependent, and as image resolution increases we\nexpect that Nc will decrease as more information is provided per image.\nA.2\nIncreasing model capacity with CommonCanvas-LNC\nWe also train a variant of SD2 with more trainable parameters, taking the UNet from SDXL. We refer\nto this model as CommonCanvas-LNC. We adapt the SDXL UNet architecture to SD2 by changing\nthe cross-attention dimensionality to match that of the SD2 text encoder hidden state dimensionality\n(1024 for SD2 vs. 2048 for SDXL). SDXL also retrains the VAE component in their model, and we\nuse this improved performance VAE as well. Except for these changes, the architecture is identical\nto that of SD2.\n14\nB\nTraining Dataset Details\nB.1\nLAION-2B\nThe fact that LAION is not a stable benchmark can lead to multiple reproducability and security\nissues. Data poisoning attacks would be difficult to detect at the scale of 2 billion parameters. While\nthis could be mitigated by using hash values of the images, then any time the a site decide to re-\nencode the image, those images would now need to be excluded from the dataset. Furthermore,\ntargeted data poisoning attacks for diffusion models are no longer just academic conjecture. Last\nyear after the release of Stable Diffusion, a protest was launched on ArtStation that had uses upload\nimages that said \u201cNoAI\u201d to taint future training data for generative models after artists felt as though\ntheir work had been unfairly used to train the models. With the high degree of link rot, targeted\nattacks are fairly easy. Furthermore, reproduction of the experiments becomes virtually impossible.\nThis means any benchmarks that use copies of LAION as ground truth are are likely using differing\nsubsets of the full dataset.\nB.1.1\nSourcing Creative-Commons images\nTable 1: CC licenses in YFCC100M. ND means derivative works are not licensed or the license\ndoesn\u2019t allow the user to create derivative works. NC means images cannot be used in commercial\ncontexts. CommonCatalog-C only contains data from the bottom two (yellow) rows, reflecting\nimages licensed for commercial contexts (i.e., roughly 25 million images). CommonCatalog-NC\ncontains CommonCatalog-C, and additionally includes the middle two (blue) rows, reflecting images\nlicensed for non-commercial purposes. We do not include the roughly 30 million images in the\ntop two (pink) rows in CommonCatalog, as they are non-derivative licenses. We do not train on\nthese images. We do, however, produce BLIP-2 captions for them and release those captions as an\nevaluation set.\nCC License\n# Images\n% Captioned\nCC-BY-NC-ND-2.0\n25,790,117\n33.52%\nCC-BY-ND-2.0\n4,827,970\n30.23%\nCC-BY-NC-2.0\n12,468,229\n31.39%\nCC-BY-NC-SA-2.0\n28,314,685\n31.57%\nCC-BY-SA 2.0\n9,270,079\n34.05%\nCC-BY 2.0\n16,962,338\n28.96%\nB.1.2\nRelease and documentation\nC\nYFCC Example Images\nTable 2: Randomly sampled images from the YFCC [59] training set. Our synthetic BLIP2 captions\nare also provided below.\na person riding a bike on a\ndirt road\na paintings on the wall\nan orange and blue race car\ndriving on a track\nModel Architecture\n15\nTable 3: Top 10 highest frequency captions in the YFCC dataset. The most common captions are\nnot user generated and are not very descriptive of the corresponding image.\nYFCC Original Caption\nCount\nOLYMPUS+DIGITAL+CAMERA\n184889\nSONY+DSC\n123128\nExif JPEG PICTURE\n104480\nBarclays+Center+Arena%0AAtlantic+Yards%0A6th+and+Atlantic+A\n68832\nOlympus+digital+camera\n54805\nEffortlessly+uploaded+by Eye-Fi\n48388\n.\n43227\n-+Camera+phone+upload+powered+by ShoZu\n38856\nSony+dsc\n32709\nPhoto+by @Kmeron \u2014Facebook page is this way\u2014\n23754\nTable 4: Number of usable captions from OpenAI\u2019s YFCC14M dataset [45]. This table is actually\na subset from 1 for which either the user description or image title were deemed usable. These\nfigures provide an estimate on how many images in each category are actually potentially usable as\ncaptions.\nLicense Name\ncount\nCC-BY 2.0\n2448002\nCC-BY-ND 2.0\n682273\nCC-BY-NC 2.0\n1925854\nCC-BY-NC-ND 2.0\n4058817\nCC-BY-NC-SA 2.0\n4146113\nCC-BY-SA 2.0\n1568336\nWe follow the model architecture and training recipe of Stable Diffusion 2 as closely as we can to\nbest reproduce the model for CC-Small. The model has an identical number of params and structure\nas the original model. In fact, we can even load SD2\u2019s model weights into our framework due to the\nidentical architecture and naming scheme. We are able to achieve virtually identical performance\nwith SD2 in a much shorter training time with less data. We use the same VAE, tokenizers, and\nUNet archicture as SD2 except for reducing the precision of the normalization layers.\nOur CC-Large model takes SD2\u2019s model and replaces the UNet with the SDXL architecture [43].\nLike CC-Small, we also replace the normalization layers with their low-precision version. The\nreplacement of all the normalization layers is handled automatically by MosaicML\u2019s Composer li-\nbrary [40]. We perform all dataloading through MosaicML\u2019s streaming library [41].\nD\nAdditional Details on Efficiency Optimizations\nIn this section we provide additional details on the optimizations we implemented to achieve SD2\ntraining speedups. We also report the approximate cost of training our implementation of SD2 on\nvarious hardware configurations in Table 5.\nFlash Attention.\nCross attention operations are a very expensive part of training that occurs in\ndozens of layers in diffusion model UNets [47]. Flash Attention is an efficient implementation that\nTable 5: Performance (throughput) and approximate cost of training SD2 UNet with our optimiza-\ntions. Depending on the number of GPUs used, the cost to train the same models without these\noptimizations range from $90,000-$140,000\nNumber of A100s\n256x256 (img/s)\n512x512 (img/s)\n512x512 with EMA (img/s)\nDays to Train\nCost ($)\n8\n1100\n290\n290\n101.04\n$38,800.00\n16\n2180\n585\n580\n50.29\n$38,630.00\n32\n4080\n1195\n1160\n25.01\n$38,420.00\n64\n8530\n2340\n2220\n12.63\n$38,800.00\n128\n11600\n4590\n3927\n6.79\n$41,710.00\n16\nPrompt\nSD2\nCommonCanvas-\nSC\nCommonCanvas-\nSNC\nCommonCanvas-\nLNC\na 3D CAD model\nof an airplane\na bear and a fox in\nthe forest\na klein bottle\na partially cut\nbirthday cake with\npink and blue\nfrosting\ntwo hummingbirds\nand a squirrel in a\nbird bath\nFigure 12: Additional qualitative examples comparing SD2 to our model trained on the commeri-\ncal split (CommonCanvas-SC), non-commerical split (CommonCanvas-SNC), and the larger UNet\nmodel trained on the non-commercial (CommonCanvas-LNC).\nFigure 13: Additional qualitative examples of our CommonCanvas models.\n17\nInput for BLIP2\nBLIP2 Caption\nSD2\nCommonCanvas-\nSNC\nCommonCanvas-\nSC\nan image of elsa\nfrom frozen\npikachu pikachu\npikachu pikachu\npikachu pikachu\npikachu pikachu\npikachu pikachu\nthree characters\ndressed like bears,\nstanding in the\nforest\nFigure 14: Additional qualitative examples comparing our CommonCanvas models to SD2, given\nsynthetic BLIP2 captions as prompts. While not perfect, our models are better at avoiding generating\npotentially problematic data.\nis optimized to work well with reduced precision and GPU hardware [11], which was implemented\nusing the XFormers library [31], allowing us to save compute and memory usage.\nPrecomputing latents. Each forward pass of SD2 requires computing a latent representation of the\ninput image, as well as transforming the caption into a text embedding. Instead of computing the\nlatents for each example during training, we can precompute latents for the entire dataset, amortizing\nthe cost. Doing so speeds up training of the model, especially at lower resolutions, in exchange for\na one-time fixed cost of precomputing all the latents over 1 epoch.\nReduced-precision GroupNorm and LayerNorm.\nMost layers in SD2 are implemented in\nfloat16 precision, but GroupNorm and LayerNorm are implemented in float32, in part because\nit was assumed to be necessary for training stability. The resulting, frequent upcasting causes a ma-\njor bottleneck in training speed. Recent work shows that it is safe to implement LayerNorm using\nfloat16 precision [44], and we found the same to be true of GroupNorm. We thus cast all Group-\nNorm and LayerNorm operators to float16 and are able to further reduce total memory consumption\nand accelerate training.\nFully-Sharded Data Parallelism (FSDP). FSDP is a variant of data-parallel training that shards\nthe models parameters, gradients and optimizer state across multiple devices. When training data\nbatches do not fit into memory, we do several forward and backward passes on smaller microbatches,\nfollowed by a single gradient update. At GPU scale, there may only be a single microbatch, so the\ntime for the gradient update can become a significant bottleneck. In standard data distributed train-\ning, each GPU communicates all its gradients to every other GPU, and then each GPU updates its\nlocal copy of the model. Instead, we use a different paradigm inspired by [65] where each GPU only\ngets the gradients and updates the weights for a small part of the model before sending the updated\nweights for that part of the model to all of the other GPUs. By dividing the update step across all\nthe GPUs, we can ensure that the amount of work per GPU decreases as we increase the number\nof GPUs, helping us achieve linear scaling. To tackle this problem, we use PyTorch\u2019s experimental\nsupport for Fully Sharded Data Parallelism (FSDP), specifically, FSDP\u2019s SHARD GRAD OP mode.\n18\nFigure 15: How does reducing the amount of training data affect the training dynamics? We find a\nnoticeable improvement drop when training with less than 10 million samples.\nScheduled Exponential Moving Average (EMA). SD2 uses EMA, which maintains an exponential\nmoving average of the weights at every gradient update for the entire training period. This can be\nslow due to the memory operations required to read and write all the weights at every step. Since\nthe old weights are decayed by a factor of 0.9999 at every batch, the early iterations of training only\ncontribute minimally to the final average. We decide to only apply EMA for the final 50K steps\n(about 3.5% of the training period), and are able to avoid adding overhead and still achieve a nearly\nequivalent EMA model.\nE\nTelephoning: A Transfer Learning-based Image-captioning Method\nOur solution for handling the lack of captions in CC images is called telephoning, a type\nof transfer learning (Figure 3).\nTelephoning assumes the existence of a large labeled dataset\nD1 = {(x(i), y(i))}n\ni=1, consisting of pairs of high-dimensional x(i) (e.g., images, audio) that\nmap to a compact, structured label y(i) (e.g., caption, audio transcript). Telephoning trains a for-\nward model q(y|x) on D1 to learn the mapping of y given x via maximum likelihood learning\nmaxq\u2208Q\nPn\ni=1 log q(y(i)|x(i)). It then uses q as training signal for a reverse model p(x|y) trained\non a separate dataset D2 = {x(i)}m\ni=1 by maximizing Pm\ni=1 Ey\u223cq(y|x(i))[log p(x(i)|y(i))], the likeli-\nhood of the data D2 and the predicted label y under q. This forms a type of knowledge transfer from\nthe forward labeling task defined by D1 to the reverse task of inverting x from y on a separate D2.\nWhile telephoning can be viewed as a type of synthetic labeling, it becomes particularly interesting\nwhen x is a type of protected modality (e.g., a copyrighted image), while y is a compact representa-\ntion of x that does not encode sensitive aspects of y (e.g., a generic caption). Effectively, telephoning\nperforms a type of \u201clossy compression\u201d or \u201cdistillation\u201d from a high-dimensional or information-\nrich x (e.g., an image of Snoopy) to a low-dimensional or information-poor y that loses the sensitive\ncontent in x (e.g., the visual characteristics of Snoopy). Because this compression step is \u201clossy\u201d, a\nreconstruction x\u2032 of x from p(x|y) via y often does not remotely resemble the original input, just like\nin a game of telephone [38]. We derive the term telephoning from the above intuition, and employ\nit as useful shorthand to denote instances of transfer learning that solve data-scarcity problems in\nmultimodal generative modeling.\nTelephoning for text-to-image modeling.\nIn this work, we apply telephoning to the image and\ntext domains, where CC images are the high-dimensional inputs x, and we use a pre-trained BLIP-2\nmodel [34] for \u201clossy compression\u201d to short-text captions y (Figure 3a). Together, these CC-image-\ncaption pairs comprise the CommonCatalog dataset, which we use to train our CommonCanvas T2I\nmodels (Figure 3b). Even though BLIP-2 was pre-trained on LAION-400M [52], CommonCatalog\nand CommonCanvas never have direct access to LAION-400M or, importantly, anything that is\nsimilar to the images that BLIP-2 was trained on. Instead, we only have access to the mapping in\nthe model, which, given an image input, produces lossy output text that inherently does not literally\nresemble its image counterpart (Figure 3c).3\n3We draw on the example of Snoopy from [49]. Figure 3\u2019s Snoopy is CC-licensed [54].\n19\n"
  },
  {
    "title": "An Early Evaluation of GPT-4V(ision)",
    "link": "https://arxiv.org/pdf/2310.16534.pdf",
    "upvote": "21",
    "text": "An Early Evaluation of GPT-4V(ision)\nYang Wu Shilong Wang Hao Yang Tian Zheng\nHongbo Zhang Yanyan Zhao\u2217 Bing Qin\nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology\n{ywu, shilongwang, hyang, tzheng, hbzhang, yyzhao, qinb}@ir.hit.edu.cn\nAbstract\nIn this paper, we evaluate different abilities of\nGPT-4V including visual understanding, lan-\nguage understanding, visual puzzle solving,\nand understanding of other modalities such as\ndepth, thermal, video, and audio. To estimate\nGPT-4V\u2019s performance, we manually construct\n656 test instances and carefully evaluate the\nresults of GPT-4V. The highlights of our find-\nings are as follows: (1) GPT-4V exhibits im-\npressive performance on English visual-centric\nbenchmarks but fails to recognize simple Chi-\nnese texts in the images; (2) GPT-4V shows\ninconsistent refusal behavior when answering\nquestions related to sensitive traits such as gen-\nder, race, and age; (3) GPT-4V obtains worse\nresults than GPT-4 (API) on language under-\nstanding tasks including general language un-\nderstanding benchmarks and visual common-\nsense knowledge evaluation benchmarks; (4)\nFew-shot prompting can improve GPT-4V\u2019s\nperformance on both visual understanding and\nlanguage understanding; (5) GPT-4V struggles\nto find the nuances between two similar images\nand solve the easy math picture puzzles; (6)\nGPT-4V shows non-trivial performance on the\ntasks of similar modalities to image, such as\nvideo and thermal. Our experimental results re-\nveal the ability and limitations of GPT-4V and\nwe hope our paper can provide some insights\ninto the application and research of GPT-4V1.\n1\nIntroduction\nGPT-4V has shown remarkable capabilities on a\nwide of tasks (Yang et al., 2023). However, the per-\nformance of GPT-4V has not been quantitatively\nstudied. In this paper, we manually construct 656\ntest examples to quantitatively assess GPT-4V\u2019s\nabilities and seek answers to the following intrigu-\ning questions.\n\u2217 Corresponding Author\n1Our data are available at https://github.com/albertwy/GPT-\n4V-Evaluation\n1. What is the performance of GPT-4V on visual-\ncentric benchmarks such as image captioning\nand visual question answering? Can GPT-4V\nsurpass the current SOTA multimodal LLMs\nsuch as Qwen-VL-Chat (Bai et al., 2023) on\nthese benchmarks? (Visual Understanding)\n2. After being equipped with visual percep-\ntion, can GPT-4V maintain its language un-\nderstanding performance and better capture\nvisual commonsense knowledge and world\nknowledge (specifically physics knowledge)?\n(Language Understanding)\n3. Can GPT-4V benefit from exemplars? (Visual\nUnderstanding, Language Understanding)\n4. How to evaluate multimodal LLMs given\nthe observation that multimodal LLMs have\nachieved really high performance on the cur-\nrent benchmarks? (Visual Puzzle Solving)\n5. Can GPT-4V perceive other modalities\nsuch as depth, thermal, video, and audio?\n(Understanding of Other Modalities)\nWe conduct extensive evaluation of GPT-4V and\nthe results not only reveal GPT-4V\u2019s strong abil-\nities, but also point to the following issues that\nshould be addressed in future studies.\n1. GPT-4V tends to generate verbose responses,\neven when provided with exemplars that have\nshort answers, which makes it hard to ac-\ncurately assess GPT-4V\u2019s performance us-\ning current automatic metrics. For example,\nthe CIDEr scores on Nocaps (Agrawal et al.,\n2019) and Flickr30K (Young et al., 2014) 2\nobtained by GPT-4V are close to 0.\n2. GPT-4V shows inconsistent refusal behavior\nwhen answering questions related to sensitive\n2We utilize the released code by Bai et al. (2023) to esti-\nmate the results.\narXiv:2310.16534v1  [cs.CL]  25 Oct 2023\ntraits such as gender, race, and age. This is-\nsue causes an obvious performance drop on\nGQA. Future research should address this is-\nsue carefully when comparing GPT-4V with\nother multimodal LLMs.\n3. GPT-4V performs very well with English text\nrecognition, yet it cannot recognize Chinese\ntexts in images.\n4. GPT-4V struggles to solve the easy math pic-\nture puzzle (grades five level) although it ex-\nhibits strong performance on much harder tex-\ntual math datasets such as SAT math (OpenAI,\n2023).\n5. The current version of GPT-4V does not sup-\nport interleaved images and texts and can only\naccept a maximum of four images. These con-\nstraints limit the design space of prompts.\nFigure 1: An example image from GQA.\n2\nVisual Understanding\nWe evaluate GPT-4V on various visual-centric\nbenchmarks such as image captioning and visual\nquestion answering to assess its visual understand-\ning ability. Following Qwen-VL-Chat (Bai et al.,\n2023), we choose Nocaps (Agrawal et al., 2019)\nand Flickr30K (Young et al., 2014) as the eval-\nuation datasets for image captioning. As for vi-\nsual question answering, we evaluate GPT-4V on\nVQAv2 (Goyal et al., 2016), OKVQA (Marino\net al., 2019), GQA (Hudson and Manning, 2019),\nScienceQA (Lu et al., 2022), and Vizwiz VQA (Gu-\nrari et al., 2018).\nMetric\nGPT-4V always tends to generate ver-\nbose responses, which makes it hard to accurately\nevaluate GPT-4V\u2019s performance using current au-\ntomatic metrics. For example, given the image\nshown in Figure 1, we ask GPT-4V to find out\nwhich kind of watercraft is underneath the airplane\nand GPT-4V answers correctly with \u201cthe watercraft\nTable 1: Human evaluation for GPT-4V and Qwen-VL-\nChat (Zero-shot).\nTask\nDataset\nGPT-4V\nQwen-VL-Chat\nImage\nCaptioning\nNocaps\n17/20\n15/20\nFlickr30K\n19/20\n17/20\nVisual\nQuestion\nAnswering\nVQAv2\n16/20\n15/20\nOKVQA\n18/20\n16/20\nGQA\n11/20\n15/20\nScienceQA\n17/20\n14/20\nVizWiz\n17/20\n14/20\nOCR-VQA\n20/20\n19/20\nTable 2: Automatic evaluation for GPT-4V and Qwen-\nVL-Chat (Zero-shot). We do not carefully adjust the\nprompts and we acknowledge that using task-specific\nprompts to control the output formats could be helpful.\nTask\nDataset\nMetric\nGPT-4V\nQwen-VL-Chat\nImage\nCaptioning\nNocaps\nSPICE\n15.9\n16.2\nFlickr30K\nSPICE\n15.2\n17.3\nVisual\nQuestion\nAnswering\nVQAv2\nVQA Score\n0.0\n85.0\nOKVQA\nVQA Score\n5.0\n38.0\nGQA\nEM Accuracy\n10.0\n40.0\nScienceQA\nAccuracy\n85.0\n70.0\nVizWiz\nVQA Score\n23.0\n36.0\nOCR-VQA\nEM Accuracy\n0.0\n70.0\nunderneath the airplane are sailboats\u201d. However,\nif we utilize EM Accuracy as the metric, which is\nadopted by Qwen-VL-Chat for GQA, the answer of\nGPT-4V will be considered as incorrect given the\nground truth is \u201csailboat\u201d. To address this problem,\nwe manually evaluate the results of GPT-4V and\nQwen-VL-Chat. The automatic evaluation results\nare also reported to reveal the limitation of current\nautomatic metrics. Besides, we utilize SPICE (An-\nderson et al., 2016) instead of CIDEr (Vedantam\net al., 2015) as the metric for image captioning,\nbecause we find that the current implementation\nof CIDEr adopted by Qwen-VL-Chat gives a large\npenalty to the difference between candidate and\nreference sentence lengths. In our experiments, the\nCIDEr scores obtained by GPT-4V are close to 0\non Nocaps and Flickr30K.\nZero-shot Results.\nWe randomly sample 20 test\ninstances for each dataset and manually evaluate\nGPT-4V\u2019s performance. We list the human and\nautomatic evaluation results in Table 1 and Table 2.\nThere are mainly two findings. (1) GPT-4V per-\nforms really well on various tasks. GPT-4V can\ngenerate detailed and coherent descriptions for the\ngiven images and answer the questions based on\nthe images. GPT-4V is able to accurately recognize\nthe English characters in the images, achieving\n100% accuracy on OCR-VQA. We also observe\nImage Captioning\nPrompt:\nDescribe this image\nGPT-4V:\nThe\nimage\ndepicts\na\nbride\nin\nher\nwedding\nattire. She is wearing a white wedding gown\nwith intricate beading and detailing on the\nbodice. The bride is also adorned with a veil\nthat cascades down her back, and she holds a\nbouquet of white roses in one hand. Her hair\nis styled in an updo with a slight bouffant\nat the crown. Beside her, partially visible,\nare a few people, including an elderly woman\nin a pink dress and a man in a suit. In the\nbackground,\nthere's\na\nstone\nwall\nand\nblossoming\ntrees,\nsuggesting\nthe\nphotograph\nmight\nhave\nbeen\ntaken\nin\na\ngarden\nor\na\ncourtyard setting.\nQwen-VL-Chat:\nA bride holding a bouquet of flowers in her \nhand.\nGround Truth:\nA bride in her wedding gown holding a bouquet.\nFigure 2: The caption generated by GPT-4V is more detailed and specific than the ground truth. GPT-4V can capture\nthe details in the whole image (marked with red color) and also attempts to infer the location where the photo was\ntaken (marked with blue color).\nthat GPT-4V exhibits superior performance than\nQwen-VL-Chat on all datasets except on GQA. It is\nbecause GPT-4V refuses to answer some questions\nof GQA. We will analyze this later. (2) Current\nautomatic metrics are not suitable for evaluat-\ning the responses generated by GPT-4V. The au-\ntomatic evaluation results show that GPT-4V per-\nforms worse than Qwen-VL-Chat. However, when\nwe manually evaluate the outputs, we find that GPT-\n4V\u2019s performance is better than Qwen-VL-Chat.\nWe have elaborated on the reasons and show an\nexample of visual question answering in the Met-\nric subsection. We further present an example of\nimage captioning in Figure 2. As shown in this\nfigure, the generated descriptions by GPT-4V are\nmore detailed and specific than the ground truths,\nwhich makes the generated captions less similar to\nthe ground truths leading to lower metric scores.\nFew-shot Results.\nIn-context learning (ICL) as\na training-free learning framework enables us to\ninject human knowledge into models by design-\ning the demonstration and templates (Dong et al.,\n2022). Given the observation that GPT-4V can dis-\ncern the order of uploaded images, we devise the\nTable 3: Human evaluation for GPT-4V (Zero-shot and\nFew-shot).\nTask\nDataset\nZero-shot\nFew-shot\nImage Captioning\nFlickr30K\n19/20\n19/20\nVisual Question\nAnswering\nVQAv2\n16/20\n17/20\nGQA\n11/20\n16/20\nfew-shot prompts to guide GPT-4V to complete the\ntasks. Considering the maximum number of the\nuploaded images to GPT-4V is 4, we prompt GPT-\n4V with 3-shot exemplars. We show an example in\nFigure 3. The human evaluation results of few-shot\nprompting are presented in Table 3. GPT-4V with\nfew-shot prompting achieves better performance\non the VQA tasks, which indicates GPT-4V has\nin-context learning ability. Although the exemplars\nof the image caption task do not help GPT-4V ob-\ntain better human evaluation results, they make\nGPT-4V generate shorter responses leading to an\nimprovement in SPICE from 15.2 to 17.5.\nGPT-4V\u2019s inconsistent refusal behavior.\nGPT-\n4V is instructed to refuse requests for identity, sen-\nsitive traits (e.g. age, race), and ungrounded infer-\nVisual Question Answering (Few-shot)\nPrompt:\nImage: The first image\nQuestion: Which type of clothing in this scene is gray?\nAnswer: Coat\nImage: The second image\nQuestion: Which kind of animal is it?\nAnswer: Dog\nImage: The third image\nQuestion: Do you see any people in the living room?\nAnswer: Yes\nImage: The fourth image\nQuestion: Is the tape green?\nAnswer:\nGPT-4V(Few-shot):\nYes, the tape is green.\nFigure 3: Few-shot prompting for VQA. We specify the corresponding image to each question in the prompt.\nences. GPT-4V refuses to answer some questions\nof GQA resulting in low performance on GQA.\nSpecifically, we evaluate 20 test instances sampled\nfrom GQA and ask GPT-4V to answer the questions\nwith the zero-shot prompt. 4 out of 20 requests are\nrefused by GPT-4V. These four questions are as\nfollows.\n1. Is the player next to the other player female\nor male?\n2. The woman to the right of the camera is watch-\ning who?\n3. Who wears the shorts?\n4. What do you think is the old lady wearing?\nThe first one is asking for sensitive traits. The sec-\nond and third questions, to some extent, are asking\nfor information related to identity. The fourth one\nmay be seen as impolite, particularly due to the\ndescriptor \"old lady\".\nTo study GPT-4V\u2019s refusal behavior, we select\nsome words related to the sensitive traits such as\ngender, race, and age. We filter out the questions\ncontaining these words from the test dataset of\nGQA and obtain 20, 11, and 25 test instances re-\nlated to gender, race, and age respectively. We con-\nduct experiments using these instances. And we\nobserve that 10 out of 20 gender-related requests\nare refused. 8 out of 10 refused questions are di-\nrectly asking for gender. Here is an example. \u201cIs\nthe young person female or male?\u201d. It is in line\nwith our expectation as GPT-4V is trained to refuse\nsuch requests.\nBut the following examples make us confused,\nwhich reveal the inconsistent refusal behavior of\nGPT-4V.\n1. Refused Request: What is the person that is\nnot male standing on?\n2. Refused Request: Which side of the photo is\nthe male person on?\n3. Approved Request: On which side of the\nphoto is the female pedestrian?\n4. Approved Request: Does the male person\nseem to be sitting?\nIt seems hard to find out the reasons why does\nGPT-4V refuse the first and second questions while\napprove the third and fourth ones.\nChinese Text Recognition\uff081\uff09\nPrompt:\n\u56fe\u4e2d\u662f\u4ec0\u4e48\u6c49\u5b57\uff1f\nGPT-4V:\n\u56fe\u4e2d\u7684\u6c49\u5b57\u662f\u201c\u611b\u201d\uff0c\u8fd9\u662f\u7e41\u4f53\u4e2d\u6587\u7684\u201c\u7231\u201d\uff0c\u610f\n\u601d\u662f\u201clove\u201d\u3002\nTranslation\uff1aWhat is the Chinese \nword in the picture?\nTranslation\uff1aThe Chinese character in the picture is \u201c\u611b\u201d, \nwhich is the traditional Chinese character for \u201c\u7231\u201d, \nmeaning \u201clove\u201d\nFigure 4: GPT-4V fails to recognize the Chinese word in the given image.\nChinese Text Recognition\uff082\uff09\nPrompt:\n\u8bf7\u590d\u8ff0\u9898\u5e72\u4e0e\u9009\u9879\nGPT-4V:\n\u5f53\u7136\u53ef\u4ee5\u3002\u9898\u5e72\u4e0e\u9009\u9879\u5982\u4e0b\uff1a\u9898\u5e72\uff1a3.\u5f53\u6d89\u53ca\u5230\u7269\u4f53\u7684\u957f\u5ea6\u6216\n\u8005\u5927\u5c0f\u65f6\uff0c\u4f1a\u53d1\u751f\u7684\u5355\u4f4d\u8f6c\u6362\u662f\u9009\u9879\uff1aA. \u4ecd\u65e7\u6d89\u53ca\u957f\u5ea6\u5355\u4f4dB.\n\u628a\u5229\u6da6\u6362\u7b97\u4e3a\u8d27\u5e01\u5355\u4f4dC. \u4e0b\u5348\u6d89\u53ca\u957f\u5ea6\u5355\u4f4d\u65f6\u9700\u8981\u8003\u8651\u8d27\u5e01\n\u5355\u4f4dD. \u518d\u5c0f\u91cf\u5355\u4f4d\u6362\u7b97\u4e3a\u66f4\u5927\u7684\u91cd\u91cf\u5355\u4f4d\nTranslation\uff1aPlease extract the \nquestion stem and options.\nTotally Wrong\nFigure 5: GPT-4V fails to extract the question and choices from the given image.\nAs for the questions related to race, 4 out of 11\nrequests are refused.\n1. Refused Request: Is the white person next to\nthe windows wearing shorts?\n2. Approved Request: Does the white person\nnear the plants seem to be standing?\nAs for the questions related to age, 4 out of 25\nrequests are refused.\n1. Refused Request: Does the old man appear to\nbe waiting?\n2. Approved Request: Are there any old women\nor men?\nThe inconsistent refusal behavior is also ob-\nserved in the early version of GPT-4. They find\nthat GPT-4 tends to become overly cautious in cer-\ntain ways such as refusing innocuous requests. We\nconsider that this issue is vital for future research\nand should be systematically studied.\nGPT-4V fails to recognize the Chinese text in\nimages.\nImpressed by the strong English OCR\nperformance of GPT-4V, we wonder whether GPT-\n4V can recognize the Chinese text in images. We\nTable 4: Results on MMLU, HellaSwag, and Wino-\nGrande (Zero-shot).\nDataset\nGPT-4V\nGPT-4 API\nMMLU\n16/20\n17/20\nHellaSwag\n14/20\n18/20\nWinoGrande\n15/20\n19/20\nTable 5: Results on MMLU, HellaSwag, and Wino-\nGrande (Few-shot).\nDataset\nGPT-4V\nGPT-4 API\nMMLU (5-shot)\n17/20\n18/20\nHellaSwag (5-shot)\n16/20\n16/20\nWinoGrande (5-shot)\n15/20\n17/20\ndevise the following two tasks: (1) Given an image\nwith only one Chinese word, identify this word; (2)\nGiven an image, extract the question and choices\nfrom it. The first task is much easier than the sec-\nond one. However, GPT-4V fails to complete ei-\nther the first task or the second task. Specifically,\nwe create 10 instances for each task and show the\nexamples in Figure 4 and Figure 5. We evaluate\nGPT-4V on these instances, and it achieves 0% ac-\ncuracy on both tasks, revealing that GPT-4V could\nnot recognize the Chinese text in images.\n3\nLanguage Understanding\nWe evaluate GPT-4V on a wide range of bench-\nmarks to answer two intriguing questions. After\nbeing equipped with visual perception, can GPT-\n4V (1) maintain its language understanding perfor-\nmance and (2) better capture visual commonsense\nknowledge, world knowledge (specifically physics\nknowledge)?\nAs for the first question, we conduct the\nexperiments on MMLU (challenging subjects:\nabstract_algebra,\nanatomy,\nastronomy,\nbisi-\nness_ethics), HellaSwag, and WinoGrande to\nevaluate the language understanding ability of\nGPT-4V. Specifically, 20 test instances are sampled\nfor each dataset. Considering that OpenAI may\nutilize different models to process text-only inputs\nand text-image inputs, we upload a white image\nalong with the text input. We acknowledge that\nit is possible that GPT-4V could be affected by\nthe input white image if GPT-4V is not robust\nenough.\nWe manually obtain and evaluate the\nresults. The results of GPT-4V and GPT-4 (API)\nare shown in Table 4 and Table 5. We observe that\nGPT-4V obtains worse results than GPT-4 (API).\nTable 6: Results on ViComTe (Zero-shot).\nType\nGPT-4V\nGPT-4 API\nColor\n10/10\n10/10\nShape\n9/10\n10/10\nMaterial\n10/10\n10/10\nSize\n10/10\n10/10\nVisual co-occurrence\n10/10\n10/10\nTable 7: Results on UTOPIA (Zero-shot).\nScene\nGPT-4V\nGPT-4 API\nCollision\n6/10\n9/10\nFree fall\n3/10\n2/10\nFriction\n10/10\n10/10\nIncline\n10/10\n10/10\nMotion\n10/10\n10/10\nProjection\n7/10\n7/10\nBut the few-shot results indicate that GPT-4V\u2019s\nperformance could be further boosted by more\nadvanced prompting methods.\nLet us turn to the second question. We choose\nViComTe (Zhang et al., 2022) as our benchmark\nto find out whether GPT-4V can capture a broad\nrange of visually salient attributes. ViComTe cov-\ners 5 property types (color, shape, material, size,\nand visual co-occurrence) and we sample 10 test\ninstances for each property type to construct our\nevaluation dataset. We also upload a white im-\nage along with the test question. The results are\nlisted in Table 6. The results show that both GPT-\n4V and GPT-4 (API) perform well on this task.\nTo evaluate GPT-4V\u2019s ability to capture physics\nknowledge, we utilize UTOPIA (Liu et al., 2023a)\nas our benchmark. This task requires the models\nto understand and reason over some basic laws of\nphysics. UTOPIA covers six common scenes in-\ncluding collision, free fall, friction, incline, motion,\nand projection. We sample 10 test instances for\neach type and evaluate GPT-4V on them. We also\nupload a white image along with the input question.\nThe results are presented in Table 7. GPT-4V does\nnot show superiority over GPT-4 (API).\n4\nVisual Puzzle Solving\nGPT-4V has achieved very high performance on\nthe standard visual-centric benchmarks, which in-\ndicates these benchmarks could be insufficient to\nmeasure its capabilities and limitations. To address\nthis problem, in contrast to constructing more com-\nSpot the Differences (1)\nPrompt:\nSpot ten differences between them\nGPT-4V:\nHere are the ten differences between the two images:\n1. The first image has two birds flying in the sky, while the second image has only one bird.\n2. In the first image, the bird on the left is dark-colored, whereas in the second image, the\nbird is lighter in color.\n3. The shadow of the tree on the right side of the first image is longer than in the second\nimage.\n4. The elephant's tail in the first image is pointing downwards, while in the second image, it's\npointing upwards.\n5. There's a small plant near the elephant's foot in the second image, which is not present in\nthe first image.\n6. The shape and curve of the tree on the right in the second image are slightly different from\nthe first image.\n7. In the first image, the horizon has a more defined mountain range compared to the second\nimage.\n8. The cloud formations in the two images are different.\n9. The tree trunk on the left side of the first image is straight, whereas in the second image,\nit has a curve.\n10.The tree branches on the left in the first image are slightly more spread out compared to the\nsecond image.\nFigure 6: GPT-4V finds three differences (marked with red color). GPT-4V hallucinates that the elephant\u2019s tail in\nthe second image is pointing upwards.\nFigure 7: Solution to the level-1 puzzle.\nprehensive evaluation benchmarks like MME (Fu\net al., 2023) and MMBench (Liu et al., 2023b),\nwe ask GPT-4V to solve interesting but challeng-\ning puzzles including Spot the Difference, Draw &\nGuess, and Math Picture Puzzle.\nSpot the Difference.\nGiven two similar images,\nthe goal is to find 10 differences between them.\nThis challenging task requires the model to capture\nthe details of the input images and find out the\ndifferences by comparing two similar images. We\nmanually obtain 10 different level puzzles (from\n1 to 10) from CrazyGames3. To evaluate GPT-\n4V\u2019s performance on these puzzles, we devise two\ntypes of prompts. As for the first one, we simply\nposition two images within a single image: one\non the left and the other on the right. 4 Then we\nupload this obtained image to GPT-4V and ask\nit to spot ten differences. We show the level-1\n3https://www.crazygames.com/game/find-the-difference\n4We also tried to upload two images separately but did not\nfind any significant difference.\nSpot the Differences (2)\nPrompt:\nSpot ten differences between them and provide the corresponding block numbers\nGPT-4V:\nSure,\nhere\nare\nten\ndifferences\nbetween\nthe\ntwo\nimages,\nalong\nwith\ntheir\ncorresponding\nblock\nnumbers:\n1. A1: In the left image, there are two birds. In the right image, there's only one bird.\n2. A3: In the left image, the top branches of the tree have two clusters of leaves. In the right\nimage, there are three clusters of leaves.\n3. A4: In the left image, there's one bird. In the right image, there's no bird.\n4. B1: In the left image, there are three clouds. In the right image, there are two clouds.\n5. C1: In the left image, there's a small tree in the background. In the right image, there's no\nsmall tree.\n6. C4: In the left image, the elephant's tail is pointing downward. In the right image, the tail\nis pointing up.\n7. D2: In the left image, there's a small green plant near the bottom edge. In the right image,\nthe plant is missing.\n8. D3: In the left image, the shadow of the elephant's front leg is longer. In the right image,\nthe shadow is shorter.\n9. D4: In the left image, there's no small plant near the elephant's back leg. In the right image,\nthere's a small green plant.\n10.B2 and B3: In the left image, the top of the tree where it splits has a more pronounced gap\nbetween the branches. In the right image, the gap is narrower.\nFigure 8: GPT-4V should not only spot the differences, but also provide the corresponding block numbers. Only\none difference is found (marked with red color).\nDraw & Guess (1)\nPrompt:\nLet's play Draw & Guess. You are given a drawing. Please guess a \nword (Hints: Profession) based on the drawing. \nGPT-4V:\nMagician!\nFigure 9: GPT-4V guesses the word \u201cmagician\u201d correctly.\npuzzle to GPT-4V and the result is presented in\nFigure 6. We also present the solution to this puzzle\nin Figure 7 for reference. As shown in Figure 6,\nGPT-4V finds three differences but the answers\nare rough. Therefore, we design another prompt.\nWe label different areas of the picture with block\nnumbers and ask GPT-4V to spot the differences\nand provide the corresponding block numbers. In\nDraw & Guess (2)\nPrompt:\nLet's play Draw & Guess. You are given a drawing. Please guess a \nword (Hints: Activity) based on the drawing. \nGPT-4V:\nBased on the drawing,\nmy guess is \"golf.\"\nFigure 10: GPT-4V guesses the word \u201cgolf\u201d correctly.\nDraw & Guess (3)\nPrompt:\nLet's play Draw & Guess. You are given a drawing. Please guess a \nword (Hints: Object, Machine) based on the drawing. \nGPT-4V:\nBased on the drawing,\nmy\nguess\nis\n\"Washing\nMachine\".\nFigure 11: GPT-4V wrongly recognizes the claw machine as the washing machine.\nthis way, we can evaluate GPT-4V\u2019s results more\naccurately. We show an example in Figure 8. GPT-\n4V only correctly finds one difference. We test 10\npuzzles and manually check the correctness of the\nanswers. GPT-4V finds 14 differences in total with\nthe first prompting method and 8 differences with\nthe second one. The evaluation results show that\nGPT-4V struggles to capture the small differences\nbetween two similar images, which could limit its\napplication. For example, it may be unsuitable\nthat utilizing GPT-4V as a strict discriminator to\nevaluate the predictions of shadow removal models.\nDraw & Guess.\nDraw & Guess is a casual draw-\ning game. One player selects one word and draws\nit. The other players should guess the word based\non the drawing and some hints such as describing\nan object. We collect 10 drawings and the corre-\nsponding hints. We want to know whether GPT-4V\ncan understand the meaning of each drawing and\nfurther guess the corresponding word successfully.\nWe show two success examples in Figure 9 and\nFigure 10. GPT-4V can capture the visual concepts\nand guess the words successfully. But GPT-4V\nalso makes mistakes. The only one failure case is\npresented in Figure 11. GPT-4V fails to find the\nMath Picture Puzzle (1)\nPrompt:\nSlove this question\nGPT-4V:\nLet's solve the puzzle step by step:\nFrom the first equation:\n\ud83c\udf83 + \ud83c\udf83 + \ud83c\udf83 + \ud83c\udf40 = 32\n3\ud83c\udf83 + \ud83c\udf40 = 32\n=> 3\ud83c\udf83 = 32 - \ud83c\udf40       ...(i)\nFrom the second equation:\n\ud83c\udf83 + \ud83d\udc22 = 17=> \ud83c\udf83 = 17 - \ud83d\udc22        ...(ii)\nFrom the third equation:\n\ud83c\udf83 + \ud83c\udf40 = 10\nUsing equation (ii), we get:\n17 - \ud83d\udc22 + \ud83c\udf40 = 10 => \ud83c\udf40 - \ud83d\udc22 = -7 => \ud83d\udc22 - \ud83c\udf40 = 7\n...(iii)\nSubstituting equation (ii) in equation (i):\n3(17 - \ud83d\udc22) + \ud83c\udf40 = 32\n=> 51 - 3\ud83d\udc22 + \ud83c\udf40 = 32 => \ud83c\udf40 - 3\ud83d\udc22 = -19 => 3\ud83d\udc22 - \ud83c\udf40 = 19\n...(iv)\nNow, subtracting equation (iii) from equation (iv):\n3\ud83d\udc22 - \ud83d\udc22 = 19 \u2013 7 => 2\ud83d\udc22 = 12 => \ud83d\udc22 = 6\nUsing equation (ii):\n\ud83c\udf83 = 17 \u2013 6 => \ud83c\udf83 = 11\nUsing equation (iii):\n\ud83c\udf40 = 6 - 7\n\ud83c\udf40 = -1 (This doesn't seem logical since we can't have a negative value for a symbol in such\npuzzles.)\nFrom the fourth equation:\n\ud83d\udc22 + \ud83c\udf83 \u00d7 \ud83c\udf40 = ?\n6 + 11 \u00d7 (-1) = 6 - 11 = -5\nHowever, the value for \ud83c\udf40 as -1 doesn't make intuitive sense for this type of puzzle. There\nmight be an error in the equations provided or the symbols might represent unconventional values.\nFigure 12: GPT-4V does not recognize the first math equation correctly and fails to solve the puzzle.\nclues such as the claw in the image and recognize\nit as the washing machine. Draw & Guess requires\nthe model to understand the visual concepts (rec-\nognizing the hat and the rabbit), recall the related\ncommonsense knowledge (magic), and conduct rea-\nsoning to guess the words (magician). It could be\npossible to construct a visual reasoning benchmark\nby collecting more challenging instances.\nMath Picture Puzzle.\nPrevious work has shown\nthat GPT-4 is good at solving math problems. In-\nspired by it, we are curious about GPT-4V\u2019s per-\nformance on the math picture puzzles. Surpris-\ningly, the performance of GPT-4V is really low\neven though the math picture puzzles are much eas-\nier than the problems of the widely used datasets\nsuch as GSM-8K (Cobbe et al., 2021). We show\nan example in Figure 12. GPT-4V wrongly rec-\nognizes the first math equation leading to cascad-\ning errors and fails to solve the puzzle. We also\nask Bard to solve the same puzzle (shown in Fig-\nure 13) and Bard fails to recognize all equations.\nWe collect 10 math picture puzzles and test GPT-\n4V on them. GPT-4V only solves one problem\ncorrectly, which is out of our expectations consid-\nering the strong visual understanding ability and\nmath problem-solving ability. We consider the rea-\nson could be that GPT-4V does not generalize well\nto this domain or GPT-4V utilizes some additional\nmodules such as the OCR module to help itself but\nthe additional modules do not work well in this\ndomain.\n5\nUnderstanding of Other Modalities\nThe research question that we investigate in this sec-\ntion is whether can GPT-4V perceive other modali-\nties including depth, thermal, video, and audio. We\nmanually construct test instances for each modality\nand evaluate GPT-4V\u2019s performance.\nDepth.\nBased on the SUN-D dataset (Song et al.,\n2015), we create 20 test questions of asking for\nGPT-4V to select the scene label from the five\nchoices for the given depth image. An example\nis shown in Figure 14. In this case, GPT-4V rec-\nognizes the chair through the depth image and cor-\nrectly predicts the scene label as \u201cliving room\".\nWe prompt GPT-4V with 20 questions and 4 out\nof 20 questions are answered correctly, which re-\nveals that GPT-4V struggles to understand the given\nMath Picture Puzzle (2)\nPrompt:\nSlove this question\nBard:\nLet's solve the puzzle step by step:\nTo solve the puzzle, we need to find the missing numbers in the equations.\nEquation 1: 32 + ? = 17\nEquation 2: ? + 3 = 10\nEquation 3: ? + ? = 15\nWe can start by solving Equation 2. We know that 10 - 3 = 7, so the missing number in Equation 2\nis 7.\nNow we can use this information to solve Equation 1. We know that 17 - 7 = 10, so the missing\nnumber in Equation 1 is 10.\nFinally, we can use the information from Equations 1 and 2 to solve Equation 3. We know that 15 -\n10 - 7 = 2, so the missing numbers in Equation 3 are 10 and 2.\nTherefore, the completed puzzle looks like this:\n= + 32 + 10 = 17 + 7 + 3 = 10 + 10 + 2 = 15\nFigure 13: Bard can not recognize the math equations correctly and fails to solve the puzzle.\ndepth image and further training could be necessary\nbefore the application.\nThermal.\nWe sample 20 test instances from\nLLVIP (Jia et al., 2021) and ask GPT-4V to find\nthe number of pedestrians and their location in the\nthermal infrared image. We present an example in\nFigure 15. GPT-4V successfully detects all pedes-\ntrians in the image. We test GPT-4V on the sampled\n20 instances and 9 out of 20 instances are solved.\nAs for the remaining instances, GPT-4V also can\ndetect correctly several pedestrians from the given\nimages.\nVideo.\nIt is hard to evaluate accurately and fairly\nGPT-4V\u2019s performance on video understanding\nconsidering the maximum number of the uploaded\nimages to GPT-4V is 4 and too much information\nof the input video is lost. Nevertheless, we are still\ncurious about the GPT-4V\u2019s performance on video\nunderstanding. We sample 20 test instances from\nthe MSR-VTT dataset (Xu et al., 2016). We select\nfour frames from each video and ask GPT-4V to\ngenerate the corresponding video description based\non the four frames. We present an example in Fig-\nure 16. We find that GPT-4V tends to describe\nthe images separately and struggles to generate the\ncaption for the whole video. We attribute it to the\nlow sampling rate of the video and increasing the\nnumber of the uploaded images could be helpful.\nWe conduct the experiment with 20 instances and\n6 out of 20 instances are described correctly.\nAudio.\nIt is a common way to treat mel spectro-\ngrams as images and adopt pre-trained visual en-\ncoders to process the mel spectrograms (Wu et al.,\n2022). Hence, we attempt to evaluate GPT-4V\u2019s\nability to perceive audio signals by converting the\ninput audio waveforms into the mel spectrograms.\nBased on the ESC dataset (Piczak, 2015), we devise\ntwo tasks to assess GPT-4V\u2019s ability: (1) Given a\nmel spectrum diagram and four choices, the goal\nis to select an audio label from the four choices;\n(2) Given four mel spectrograms and four audio\nlabels, the goal is to match the four spectrograms\nto the four audio labels. We show one example for\neach task in Figure 17 and Figure 18. As shown in\nFigure 17, GPT-4V knows the specific patterns of\ncommon animal sounds. It could be easy to elimi-\nnate \u201cPig\u201d and \u201cCow\u201d by the audio duration, but it\nis harder to decide which one of the other choices\nis the correct answer. The ground truth is \u201cDog\u201d\nScene Understanding\nPrompt:\nThis image is the depth image of scene. Please determine the category of this \nscene.\nA. Office  B. Kitchen  C. Bedroom  D. Bathroom  E. Living room\nGPT-4V:\nBased on the depth image provided, it is challenging to definitively determine the\ncategory of the scene. However, the image appears to show a chair-like structure\nwhich could be associated with multiple room categories.\nIf I had to guess based on the provided options, I would suggest:\nE. living room\nHowever, this is a speculative answer as the image does not provide clear details\nabout the entire scene.\nFigure 14: GPT-4V can recognize the chair in the depth image and predict the scene label correctly.\nPedestrian Detection\nPrompt:\nThis is an infrared image. Please describe the number of pedestrians and their location in the \nimage for me.\nGPT-4V:\nIn the infrared image, I see two pedestrians.\n1. One pedestrian is near the left side of the image, walking on the street beside a\ntree.\n2.\nAnother\npedestrian\nis\non\nthe\nright\nside\nof\nthe\nimage,\ncloser\nto\na\nfence\nor\nbarrier and appears to be on a marked crosswalk or pavement.\nThe\nimage\nis\nin\ngrayscale\nand\ncaptures\na\nnight-time\nor\nlow-light\nscenario\nusing\ninfrared technology.\nFigure 15: GPT-4V can detect the pedestrians in the given infrared image correctly.\nVideo Captioning\nPrompt:\nHere are some pictures from a video. Could you explain \nwhat the video is describing?\nGPT-4V:\nThese images depict fantastical cityscapes, likely from\na\nmovie\nor\na\nseries.\nThe\nintricate\narchitecture,\nthe\nflowing waterfalls, and the towering structures give it\na majestic and otherworldly vibe. The silhouette of a\nperson in the second image adds to the sense of scale\nand wonder.\nFigure 16: GPT-4V describes the image content well but struggles to generate the caption for the whole video.\nIncreasing the number of the sampled frames could be helpful.\nAudio Classification (1)\nPrompt:\nThis image is the mel spectrum diagram of a piece of audio. Please determine the source of \nthis audio.\nA. Dog    B. Rooster    C. Pig    D. Cow.\nGPT-4V:\nThe image is a Mel spectrogram, which represents the distribution of frequencies in\nan audio signal over time. By looking at the image alone, I cannot definitively\nidentify the source of the audio. However, based on common audio patterns: A rooster\ntypically\nhas\na\nsharp\nand\ndistinctive\ncall\nthat\nmight\nshow\na\npronounced\nand\nrelatively short pattern in a spectrogram. Dogs can have a variety of barks, howls,\nand whines, with different frequency distributions and lengths. Pigs often produce\ngrunts and squeals. Cows typically moo, which can be longer in duration and might\nshow a more spread-out pattern in the spectrogram. Without listening to the actual\naudio, I can't provide a definitive answer. If you're looking for a guess based on\nthe\nspectrogram's\nvisual\nrepresentation\nand\nthe\nchoices\nprovided,\nI\nwould\nlean\ntowards\nB.\nRooster\ndue\nto\nthe\nsharp\nand\npronounced\npattern\nin\nthe\nspectrogram.\nHowever, this is just an educated guess.\nFigure 17: The ground truth is \u201cDog\u201d while GPT-4V wrongly recognize it as \u201cRooster\u201d.\nwhile GPT-4V wrongly recognizes it as \u201cRooster\u201d.\nWe construct 20 test instances covering 5 major\ncategories including Animals, Natural soundscapes\n& water sounds, Human(non-speech sounds), In-\nAudio Classification (2)\nPrompt:\nThese four images are mel spectrograms of four audio clips. Match the four sounds to the four \noptions below\na: Pig  b. Cow  c. Dog  d. Rooster\nGPT-4V:\nMatching\naudio\nclips\nto\ntheir\nvisual\nrepresentations\nas\nmel\nspectrograms\ncan\nbe\nchallenging, especially without hearing the actual\nsound.\nHowever,\nbased\non\ntypical\npatterns,\nI'll\nmake\nan\neducated\nguess:1.\nThe\nfirst\nspectrogram\nshows multiple harmonic lines, which might indicate\nsome sort of vocalization with a repetitive pattern.\nThis could potentially represent the crowing of **d.\nRooster**.\n2.\nThe\nsecond\nspectrogram\nis\nmore\nconsistent\nwith\nfewer\nvariations,\nwhich\nmight\nindicate a constant sound or background noise. This\nmight represent the grunting or ambient sounds of\n**a.\nPig**.\n3.\nThe\nthird\nspectrogram\nhas\na\nmore\nirregular\npattern,\npossibly\nindicating\nbarking\nor\nother intermittent sounds. This might correspond to\n**c. Dog**. 4. The fourth spectrogram has patterns\nthat\nsuggest\na\nmixture\nof\nconsistent\nsounds\nwith\nsome\nharmonic\novertones.\nThis\ncould\nbe\nindicative\nof the mooing of **b. Cow**. Please note that these\nare\nspeculative\nmatches\nbased\non\nthe\nvisual\nrepresentation and might not be accurate. Listening\nto\nthe\nactual\naudio\nclips\nwould\nprovide\na\nmore\naccurate match.\nFigure 18: GPT-4V fails to match the given four audios to their labels. The ground truth is \u201c1. Dog 2. Rooster 3.\nPig 4. Cow\u201d\nterior/domestic sounds, and Exterior/urban noises\nbased on ESC. The result is that GPT-4V success-\nfully recognizes 5 out of 20 instances, which is\nthe same as the random selecting method. As for\nthe second task, GPT-4V successfully matches 2\nout of audios to their labels. We show an example\nfor the second task in Figure 18. GPT-4V fails to\nmatch the given four audios to their labels, which\nindicates that although GPT-4V knows some com-\nmon patterns of sounds, it is still challenging for\nGPT-4V to recognize the audio labels directly from\nthe mel spectrograms.\n6\nConclusion\nIn this paper, we quantitatively study GPT-4V\u2019s per-\nformance on various tasks. According to the results,\nwe find that although GPT-4V achieves high per-\nformance on standard English visual-centric bench-\nmarks, it still can not perform Chinese text recog-\nnition. This observation suggests further in-depth\nevaluation on Chinese benchmarks is necessary for\nmeasure GPT-4V\u2019s capability. We also observe\nthat GPT-4V fails to solve easy math picture puz-\nzles even though it has strong visual understanding\nability and math problem solving ability. The rea-\nson could be that GPT-4V does not generalize well\nto this domain. Another problem is that GPT-4V\nexhibits inconsistent refusal behavior when answer-\ning questions related to identity and sensitive traits\nsuch as gender, race, and age. This issue could lead\nto an obvious performance drop of GPT-4V and\nshould be dealt with carefully in future studies.\nAs for the limitations, we acknowledge that GPT-\n4V\u2019s performance could be different by adopting\ndifferent prompting methods. For example, more\nspecific instructions and better exemplars will im-\nprove its performance. We would like to explore\nutilizing other advanced prompts such as chain-of-\nthought prompting (Wei et al., 2022) in future work.\nWe also acknowledge that more test instances for\neach task can make the estimated results more ac-\ncurate, but we only sample a part of instances due\nto the high labor cost.\nNevertheless, it is the first attempt to quanti-\ntatively study GPT-4V\u2019s performance on a wide\nrange of tasks. In our study, we reveal the strengths\nand limitations of GPT-4V. We hope our study can\nprovide insights into future research and applica-\ntion.\nReferences\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi\nParikh, Stefan Lee, and Peter Anderson. 2019. No-\ncaps: Novel object captioning at scale. In Proceed-\nings of the IEEE/CVF international conference on\ncomputer vision, pages 8948\u20138957.\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2016.\nSpice: Semantic proposi-\ntional image caption evaluation. In Computer Vision\u2013\nECCV 2016: 14th European Conference, Amsterdam,\nThe Netherlands, October 11-14, 2016, Proceedings,\nPart V 14, pages 382\u2013398. Springer.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023. Qwen-vl: A frontier large\nvision-language model with versatile abilities. arXiv\npreprint arXiv:2308.12966.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nZhifang Sui. 2022. A survey for in-context learning.\narXiv preprint arXiv:2301.00234.\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-\nrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-\ngrong Ji. 2023. Mme: A comprehensive evaluation\nbenchmark for multimodal large language models.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2016. Making the v in vqa\nmatter: Elevating the role of image understanding in\nvisual question answering. International Journal of\nComputer Vision, 127:398 \u2013 414.\nDanna Gurari, Qing Li, Abigale J Stangl, Anhong Guo,\nChi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P\nBigham. 2018. Vizwiz grand challenge: Answering\nvisual questions from blind people. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 3608\u20133617.\nDrew A Hudson and Christopher D Manning. 2019.\nGqa: A new dataset for real-world visual reasoning\nand compositional question answering. In Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 6700\u20136709.\nXinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, and\nWenli Zhou. 2021. Llvip: A visible-infrared paired\ndataset for low-light vision. In Proceedings of the\nIEEE/CVF international conference on computer vi-\nsion, pages 3496\u20133504.\nRuibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu,\nSoroush Vosoughi, Claire Cui, Denny Zhou, and An-\ndrew M. Dai. 2023a. Mind\u2019s eye: Grounded language\nmodel reasoning through simulation. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, Kai Chen, and Dahua\nLin. 2023b. Mmbench: Is your multi-modal model\nan all-around player?\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. 2022. Learn to explain:\nMultimodal reasoning via thought chains for science\nquestion answering. Advances in Neural Information\nProcessing Systems, 35:2507\u20132521.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual ques-\ntion answering benchmark requiring external knowl-\nedge. In Proceedings of the IEEE/cvf conference\non computer vision and pattern recognition, pages\n3195\u20133204.\nOpenAI. 2023. Gpt-4 technical report.\nKarol J. Piczak. 2015. Esc: Dataset for environmental\nsound classification. In Proceedings of the 23rd ACM\nInternational Conference on Multimedia, MM \u201915,\npage 1015\u20131018, New York, NY, USA. Association\nfor Computing Machinery.\nShuran Song, Samuel P Lichtenberg, and Jianxiong\nXiao. 2015. Sun rgb-d: A rgb-d scene understanding\nbenchmark suite. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition,\npages 567\u2013576.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4566\u20134575.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nYang Wu, Zhenyu Zhang, Pai Peng, Yanyan Zhao, and\nBing Qin. 2022. Leveraging multi-modal interac-\ntions among the intermediate representations of deep\ntransformers for emotion recognition. In Proceed-\nings of the 3rd International on Multimodal Senti-\nment Analysis Workshop and Challenge, MuSe\u2019 22,\npage 101\u2013109, New York, NY, USA. Association for\nComputing Machinery.\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-\nvtt: A large video description dataset for bridging\nvideo and language. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition,\npages 5288\u20135296.\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang.\n2023. The dawn of lmms: Preliminary explorations\nwith gpt-4v(ision).\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67\u201378.\nChenyu Zhang, Benjamin Van Durme, Zhuowan Li,\nand Elias Stengel-Eskin. 2022. Visual commonsense\nin pretrained unimodal and multimodal models. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5321\u20135335.\n"
  },
  {
    "title": "Wonder3D: Single Image to 3D using Cross-Domain Diffusion",
    "link": "https://arxiv.org/pdf/2310.15008.pdf",
    "upvote": "19",
    "text": "Wonder3D: Single Image to 3D using Cross-Domain Diffusion\nXiaoxiao Long1,3,6\u2217, Yuan-Chen Guo2,3\u2217, Cheng Lin1\u2020, Yuan Liu1, Zhiyang Dou1\nLingjie Liu4, Yuexin Ma5, Song-Hai Zhang2, Marc Habermann6, Christian Theobalt6, Wenping Wang7\u2020\n1 The University of Hong Kong\n2 Tsinghua University\n3 VAST\n4 University of Pennsylvania\n5 Shanghai Tech University\n6 MPI Informatik\n7 Texas A&M University\n\u2217 Equal Contributions.\nhttps://www.xxlong.site/Wonder3D/\nFigure 1. Wonder3D reconstructs highly-detailed textured meshes from a single-view image in only 2 \u223c 3 minutes. Wonder3D first\ngenerates consistent multi-view normal maps with corresponding color images via a cross-domain diffusion model, and then leverages a\nnovel normal fusion method to achieve fast and high-quality reconstruction.\n\u2020 Corresponding authors.\n1\narXiv:2310.15008v3  [cs.CV]  8 Nov 2023\nAbstract\nIn this work, we introduce Wonder3D, a novel method\nfor efficiently generating high-fidelity textured meshes from\nsingle-view images. Recent methods based on Score Dis-\ntillation Sampling (SDS) have shown the potential to re-\ncover 3D geometry from 2D diffusion priors, but they typ-\nically suffer from time-consuming per-shape optimization\nand inconsistent geometry. In contrast, certain works di-\nrectly produce 3D information via fast network inferences,\nbut their results are often of low quality and lack geomet-\nric details. To holistically improve the quality, consistency,\nand efficiency of single-view reconstruction tasks, we pro-\npose a cross-domain diffusion model that generates multi-\nview normal maps and the corresponding color images. To\nensure the consistency of generation, we employ a multi-\nview cross-domain attention mechanism that facilitates in-\nformation exchange across views and modalities. Lastly, we\nintroduce a geometry-aware normal fusion algorithm that\nextracts high-quality surfaces from the multi-view 2D rep-\nresentations. Our extensive evaluations demonstrate that\nour method achieves high-quality reconstruction results, ro-\nbust generalization, and good efficiency compared to prior\nworks.\n1. Introduction\nReconstructing 3D geometry from a single image stands as\na fundamental task in computer graphics and 3D computer\nvision [12, 25, 31, 33, 35, 38, 41, 44], offering a wide range\nof versatile applications such as virtual reality, video games,\n3D content creation, and the precision of robotics grasping.\nHowever, this task is notably challenging since it is ill-posed\nand demands the ability to discern the 3D geometry of both\nvisible and invisible parts. This ability requires extensive\nknowledge of the 3D world.\nRecently, the field of 3D generation has experienced\nrapid and flourishing development with the introduction of\ndiffusion models. A growing body of research [5, 29, 43,\n59, 63], such as DreamField [24], DreamFusion [43], and\nMagic3D [29], resort to distilling prior knowledge of 2D\nimage diffusion models or vision language models to create\n3D models from text or images via Score Distillation Sam-\npling (SDS) [43]. Despite their compelling results, these\nmethods suffer from two main limitations: efficiency and\nconsistency. The per-shape optimization process typically\nentails tens of thousands of iterations, involving full-image\nvolume rendering and inferences of the diffusion models.\nConsequently, it often consumes tens of minutes or even\nhours on per-shape optimization. Moreover, the 2D prior\nmodel operates by considering only a single view at each\niteration and strives to make every view resemble the input\nimage. This often results in the generation of 3D shapes\nexhibiting inconsistencies, thus, often leading to the gen-\neration of 3D shapes with inconsistencies such as multiple\nfaces (i.e., the Janus problem [43]).\nThere exists another group of works that endeavor to di-\nrectly produce 3D geometries like point clouds [37, 41, 71,\n75], meshes [16, 34], neural fields [1, 4, 7, 14, 17, 21, 25\u2013\n27, 40, 42, 61, 72] via network inference to avoid time-\nconsuming per-shape optimization. Most of them attempt\nto train 3D generative diffusion models from scratch on 3D\nassets. However, due to the limited size of publicly avail-\nable 3D datasets, these methods demonstrate poor general-\nizability, most of which can only generate shapes on spe-\ncific categories.\nMore recently, several methods have emerged that di-\nrectly generate multi-view 2D images, with representative\nworks including SyncDreamer [33] and MVDream [51]. By\nenhancing the multi-view consistency of image generation,\nthese methods can recover 3D shapes from the generated\nmulti-view images. Following these works, our method also\nadopts a multi-view generation scheme to favor the flexi-\nbility and efficiency of 2D representations. However, due\nto only relying on color images, the fidelity of the gen-\nerated shapes is not well-maintained, and they struggle to\nrecover geometric details or come with enormous computa-\ntional costs.\nTo better address the issues of fidelity, consistency, gen-\neralizability and efficiency in the aforementioned works,\nin this paper, we introduce a new approach to the task\nof single-view 3D reconstruction by generating multi-view\nconsistent normal maps and their corresponding color im-\nages with a cross-domain diffusion model. The key idea\nis to extend the stable diffusion framework to model the\njoint distribution of two different domains, i.e., normals and\ncolors. We demonstrate that this can be achieved by in-\ntroducing a domain switcher and a cross-domain attention\nscheme. In particular, the domain switcher allows the dif-\nfusion model to generate either normal maps or color im-\nages, while the cross-domain attention mechanisms assist\nin the information exchange between the two domains, ulti-\nmately improving consistency and quality. Finally, in order\nto stably extract surfaces from the generated views, we pro-\npose a geometry-aware normal fusion algorithm that is ro-\nbust to the inaccuracies and capable of reconstructing clean\nand high-quality geometries (see Figure 1).\nWe conduct extensive experiments on the Google\nScanned Object dataset [13] and various 2D images with\ndifferent styles. The experiments validate that Wonder3D is\ncapable of producing high-quality geometry with high ef-\nficiency in comparison with baseline methods. Wonder3D\npossesses several distinctive properties and accordingly has\nthe following contributions:\n\u2022 Wonder3D holistically considers the issues of generation\nquality, efficiency, generalizability, and consistency for\n2\nsingle-view 3D reconstruction. It has achieved a lead-\ning level of geometric details with reasonably good effi-\nciency among current zero-shot single-view reconstruc-\ntion methods.\n\u2022 We propose a new multi-view cross-domain 2D diffusion\nmodel to predict normal maps and color images. This\nrepresentation not only adapts to the original data distri-\nbution of Stable Diffusion model but also effectively cap-\ntures the rich surface details of the target shape.\n\u2022 We propose a cross-domain attention mechanism to pro-\nduce multi-view normal maps and color images that are\nconsistently aligned. This mechanism facilitates infor-\nmation perception across different domains, enabling our\nmethod to recover high-fidelity geometry.\n\u2022 We introduce a novel geometry-aware normal fusion al-\ngorithm that can robustly extract surfaces from the gener-\nated normal maps and color images.\n2. Related Works\n2.1. 2D Diffusion Models for 3D Generation\nRecent compelling successes in 2D diffusion models [8,\n22, 47] and large vision language models (e.g., CLIP\nmodel [45]) provide new possibilities for generating 3D as-\nsets using the strong priors of 2D diffusion models. Pi-\noneering works DreamFusion [43] and SJC [59] propose\nto distill a 2D text-to-image generation model to generate\n3D shapes from texts, and many follow-up works follow\nsuch per-shape optimization scheme. For the task of text-\nto-3D [2, 5, 6, 23, 29, 48, 49, 57, 63, 65, 69, 77] or image-\nto-3D synthesis [38, 44, 46, 50, 54, 67], these methods typ-\nically optimize a 3D representation (i.e., NeRF, mesh, or\nSDF), and then leverage neural rendering to generate 2D\nimages from various viewpoints. The images are then fed\ninto the 2D diffusion models or CLIP model for calculating\nSDS [43] losses, which can guide the 3D shape optimiza-\ntion.\nHowever, most of these methods always suffer from low\nefficiency and multi-face problem, where a per-shape opti-\nmization consumes tens of minutes and the optimized ge-\nometry tends to produce multiple faces due to the lack of\nexplicit 3D supervision.\nA recent work one-2-3-45 [15]\nproposes to leverage a generalizable neural reconstruction\nmethod SparseNeuS [36] to directly produce 3D geometry\nfrom the generated images from zero123 [31]. Although\nthe method achieves high efficiency, its results are of low-\nquality and lack geometric details.\n2.2. 3D Generative Models\nInstead of performing a time-consuming per-shape opti-\nmization guided by 2D diffusion models, some works at-\ntempt to directly train 3D diffusion models based on vari-\nous 3D representations, like point clouds [37, 41, 71, 75],\nmeshes [16, 34], neural fields [1, 4, 7, 14, 17, 21, 25\u2013\n27, 40, 42, 61, 72] However, due to the limited size of pub-\nlic available 3D assets dataset, most of the works have only\nbeen validated on limited categories of shapes, and how to\nscale up on large datasets is still an open problem. On the\ncontrary, our method adopts 2D representations and, thus,\ncan be built upon the 2D diffusion models [47] whose pre-\ntrained priors significantly facilitate zero-shot generaliza-\ntion ability.\n2.3. Multi-view Diffusion Models\nTo generate consistent multi-view images, some efforts [3,\n10, 18, 28, 32, 53, 55, 56, 58, 64, 66, 68, 70, 76] are made\nto extend 2D diffusion models from single-view images to\nmulti-view images. However, most of these methods fo-\ncus on image generation and are not designed for 3D re-\nconstruction. The works [66, 73] first warp estimated depth\nmaps to produce incomplete novel view images to then per-\nform inpainting on them, but their result quality signifi-\ncantly degrades when the depth maps estimated by exter-\nnal depth estimation models are inaccurate.\nThe recent\nworks Viewset Diffusion [53], SyncDreamer [33], and MV-\nDream [51] share a similar idea to produce consistent multi-\nview color images via attention layers. However, unlike that\nnormal maps explicitly encode geometric information, re-\nconstruction from color images always suffers from texture\nambiguity, and, thus, they either struggle to recover geo-\nmetric details or require huge computational costs. Sync-\nDreamer [33] requires dense views for 3D reconstruction,\nbut still suffers from low-quality geometry and blurring tex-\ntures.\nMVDream [51] still resorts to a time-consuming\noptimization using SDS loss for 3D reconstruction, and\nits multi-view distillation scheme requires 1.5 hours.\nIn\ncontrast, our method can reconstruct high-quality textured\nmeshes in just 2 minutes.\n3. Problem Formulation\n3.1. Diffusion Models\nDiffusion models [22, 52] are first proposed to gradually re-\ncover images from a specifically designed degradation pro-\ncess, where a forward Markov chain and a Reverse Markov\nchain are adopted. Given a sample z0 drawn from the data\ndistribution p(z), the forward process of denoising diffusion\nmodels yields a sequence of noised data {zt | t \u2208 (0, T)}\nwith zt = \u03b1tz0 + \u03c3t\u03f5, where \u03f5 is random noise drawn from\ndistribution N(0, 1), and \u03b1t, \u03c3t are fixed sequence of the\nnoise schedule. The forward process will be iteratively ap-\nplied to the target image until the image becomes complete\nGaussian noise at the end. On the contrary, the reverse chain\nthen is employed to iteratively denoise the corrupted image,\ni.e., recovering zt\u22121 from zt by predicting the added ran-\ndom noise \u03f5. The readers can refer to [22, 52] for more\n3\nCLIP\nGeometry\nFusion\nNormal\nColor\n\u2a01\n\u22ef\n\u22ef\nCameras\nDomain switcher\nSingle RGB image\nGenerated normals and colors\nTextured mesh\nUNet\nFigure 2. Overview of Wonder3D. Given a single image, Wonder3D takes the input image, the text embedding produced by CLIP\nmodel [45], the camera parameters of multiple views, and a domain switcher as conditioning to generate consistent multi-view normal\nmaps and color images. Subsequently, Wonder3D employs an innovative normal fusion algorithm to robustly reconstruct high-quality 3D\ngeometry from the 2D representations, yielding high-fidelity textured meshes.\ndetails about image diffusion models.\n3.2. The Distribution of 3D Assets\nUnlike that prior works adopt 3D representations like point\nclouds, tri-planes, or neural radiance fields, we propose that\nthe distribution of 3D assets, denoted as pa(z), can be mod-\neled as a joint distribution of its corresponding 2D multi-\nview normal maps and corresponding color images. Specif-\nically, given a set of cameras {\u03c01, \u03c02, \u00b7 \u00b7 \u00b7 , \u03c0K} and a con-\nditional input image y, we have\npa(z) = pnc\n\u0000n1:K, x1:K|y\n\u0001\n,\n(1)\nwhere pnc is the distribution of the normal maps n1:K and\ncolor images x1:K observed from 3D assets conditioned on\nan image y. For simplicity, we omit the symbol y for this\nequation in the following discussions. Therefore, our goal\nis to learn a model f that synthesizes multiple normal maps\nand color images of a set of camera poses denoted as\n(n1:K, x1:K) = f(y, \u03c01:K).\n(2)\nAdopting the 2D representation enables our method to\nbe built upon the 2D diffusion models trained on billions of\nimages like the Stable Diffusion model [45], where strong\npriors facilitate zero-shot generalization ability.\nOn the\nother hand, the normal map characterizes the undulations\nand variations present on the surface of the shape, thus en-\ncoding rich detailed geometric information. This allows for\nthe high-fidelity extraction of 3D geometry from 2D normal\nmaps.\nFinally, we can formulate this cross-domain joint distri-\nbution as a Markov chain within the diffusion scheme:\np\n\u0010\nn(1:K)\nT\n, x(1:K)\nT\n\u0011 Y\nt\np\u03b8\n\u0010\nn(1:K)\nt\u22121 , x(1:K)\nt\u22121 |n(1:K)\nt\n, x(1:K)\nt\n\u0011\n,\n(3)\nwhere p\n\u0010\nn(1:K)\nT\n, x(1:K)\nT\n\u0011\nare Gaussian noises.\nOur key\nproblem is to characterize the distribution p\u03b8, so that we can\nsample from this Markov chain to generate normal maps\nand images.\n4. Method\nAs per our problem formulation in Section 3.2, we propose\na multi-view cross-domain diffusion scheme, which oper-\nates on two distinct domains to generate multi-view consis-\ntent normal maps and color images. The overview of our\nmethod is presented in Figure 2. First, our method adopts\na multi-view diffusion scheme to generate multi-view nor-\nmal maps and color images, and enforces the consistency\nacross different views using multi-view attentions (see Sec-\ntion 4.1). Second, our proposed domain switcher allows\nthe diffusion model to operate on more than one domain\nwhile its formulation does not require a re-training of an\nexisting (potentially single domain) diffusion model such as\nStable Diffusion [45]. Thus, we can leverage the generaliz-\nability of large foundational models, which are trained on a\nlarge corpus of data. A cross-domain attention is proposed\nto propagate information between the normal domain and\ncolor image domain ensuring geometric and visual coher-\nence between the two domains (see Section 4.2). Finally,\nour novel geometry-aware normal fusion reconstructs the\nhigh-quality geometry and appearance from the multi-view\n2D normal and color images (see Section 4.3).\n4.1. Consistent Multi-view Generation\nThe prior 2D diffusion models [31, 45] generate each image\nseparately, so that the resulting images are not geometrically\nand visually consistent across different views. To enhance\nconsistency among different views, similar to prior works\nsuch as SyncDreamer [33] and MVDream [51], we utilize\nattention mechanism to facilitate information propagation\nacross different views, implicitly encoding multi-view de-\npendencies (as illustrated in Figure 4).\nThis is achieved by extending the original self-attention\n4\nInput images\n(a) Ours\n(b) SyncDreamer\n(c) Zero123\nFigure 3. The qualitative comparisons with baseline models on synthesized multi-view color images.\nlayers to be global-aware, allowing connections to other\nviews within the attention layers. Keys and values from dif-\nferent views are connected to each other to facilitate the ex-\nchange of information. By sharing information across dif-\nferent views within the attention layers, the diffusion model\nperceives multi-view correlation and becomes capable of\ngenerating consistent multi-view color images and normal\nmaps.\n4.2. Cross-Domain Diffusion\nOur model is built upon pre-trained 2D stable diffusion\nmodels [45] to leverage its strong generalization. However,\ncurrent 2D diffusion models [31, 45] are designed for a sin-\ngle domain, so the main challenge is how to effectively ex-\ntend stable diffusion models that are capable of operating\non more than one domain.\nNaive Solutions. To achieve this goal, we explore several\npossible designs. A straightforward solution is to add four\nmore channels to the output of the UNet module represent-\ning the extra domain. Therefore, the diffusion model can\nsimultaneously output normals and color image domains.\nHowever, we notice that such a design suffers from low\nconvergence speed and poor generalization. This is because\nthe channel expansion may perturb the pre-trained weights\nof stable diffusion models and therefore cause catastrophic\nmodel forgetting.\nRevisiting Eq. 1, it is possible to factor the joint distribu-\ntion into two conditional distributions:\nqa(z) =qn(n1:K)\u00b7\nqc\n\u0000x1:K | n1:K\u0001\n.\n(4)\nThis equation suggests an alternative solution where we\ncould initially train a diffusion model to generate normal\nmaps and then train another diffusion model to produce\ncolor images, conditioning on the generated normal maps\n(or vice versa). Nonetheless, the implementation of this\nResBlock\nMulti-view \nSelfAttention\nCross-domain\nSelfAttention\nCrossAttention\nMulti-view normals/colors\nPaired normal & color\nFigure 4. The illustration of the structure of the multi-view cross-\ndomain transformer block.\ntwo-stage framework introduces certain complications. It\nnot only substantially increases the computational cost but\nalso results in performance degradation. Please refer to Sec-\ntion 5.6 for an in-depth discussion.\nDomain Switcher.\nTo overcome these difficulties men-\ntioned above, we design a cross-domain diffusion scheme\nvia a domain switcher, denoted as s. The switcher s is a\none-dimensional vector that labels different domains, and\nwe further feed the switcher into the diffusion model as an\nextra input. Therefore, the formulation of Eq. 2 can be ex-\ntended as:\nn1:K, x1:K = f(y, \u03c01:K, sn), f(y, \u03c01:K, sc).\n(5)\nThe domain switcher s is first encoded via positional en-\ncoding [39] and subsequently concatenated with the time\nembedding. This combined representation is then injected\ninto the UNet of the stable diffusion models. Interestingly,\nexperiments show that this subtle modification does not sig-\nnificantly alter the pre-trained priors. As a result, it allows\nfor fast convergence and robust generalization, without re-\nquiring substantial changes to the stable diffusion models.\nCross-domain Attention.\nUsing the proposed domain\n5\nFigure 5. The qualitative results of Wonder3D on various styles of images.\nswitcher, the diffusion model can generate two different do-\nmains. However, it is important to note that for a single\nview, there is no guarantee that the generated color image\nand the normal map will be geometrically consistent. To ad-\ndress this issue and ensure the consistency between the gen-\nerated normal maps and color images, we introduce a cross-\ndomain attention mechanism to facilitate the exchange of\ninformation between the two domains.\nThis mechanism\naims to ensure that the generated outputs align well in terms\nof geometry and appearance.\nThe cross-domain attention layer maintains the same\nstructure as the original self-attention layer and is integrated\nbefore the cross-attention layer in each transformer block of\nthe UNet, as depicted in Figure 4. In the cross-domain at-\ntention layer, the keys and values from the normal and color\nimage domains are combined and processed through atten-\ntion operations. This design ensures that the generations of\ncolor images and normal maps are closely correlated, thus\npromoting geometric consistency between the two domains.\n4.3. Textured Mesh Extraction\nTo extract explicit 3D geometry from 2D normal maps and\ncolor images, we optimize a neural implicit signed distance\nfield (SDF) to amalgamate all 2D generated data. Unlike al-\nternative representations like meshes, SDF offers compact-\nness and differentiability, making them ideal for stable op-\ntimization.\nNonetheless, adopting existing SDF-based reconstruc-\ntion methods, such as NeuS [60], proves unviable. These\nmethods were tailored for real-captured images and neces-\nsitate dense input views. In contrast, our generated views\nare relatively sparse, and the generated normal maps and\ncolor images may exhibit subtle inaccurate predictions of\nsome pixels. Regrettably, these errors accumulate during\nthe geometry optimization, leading to distorted geometries,\noutliers, and incompleteness. To overcome the challenges\nabove, we propose a novel geometric-aware optimization\nscheme.\nOptimization Objectives. With the obtained normal maps\nG0:N and color images H0:N, we first leverage segmen-\ntation models to segment the object masks M0:N from\nthe normal maps or color images.\nSpecifically, we per-\nform the optimization by randomly sampling a batch of\npixels and their corresponding rays in world space P =\n{gk, hk, mk, vk}, where gk is normal value of the kth sam-\npled pixel, hk is color value of the kth pixel, mk \u2208 {0, 1}\nis mask value of the kth pixel, and vk is the direction of\nthe corresponding sampled kth ray, from all views at each\niteration.\nThe overall objective function is defined as\nL = Lnormal + Lrgb + Lmask\n+ Reik + Rsparse + Rsmooth,\n(6)\nwhere Lnormal denotes the normal loss term that will be\ndiscussed later, Lrgb denotes a MSE loss term that calcu-\nlates the errors between rendered colors \u02c6hk and generated\ncolors hk, Lmask denotes a binary cross-entropy loss term\nthat calculating errors between the rendered mask \u02c6mk and\nthe generated mask mk, Reik denotes eikonal regulariza-\ntion term that encourages the magnitude of the SDF gradi-\nents to be unit length, Lsparse denotes a sparsity regulariza-\ntion term that avoid floaters of SDF, and Lsmooth denotes\n6\nFigure 6. The qualitative comparisons with baseline methods on GSO [13] dataset in terms of the reconstructed textured meshes.\na 3D smoothness regularization term that enforces the SDF\ngradients to be smooth in 3D space.\nGeometry-aware Normal Loss. Thanks to the differen-\ntiable nature of SDF representation, we can easily extract\nnormal values \u02c6g of the optimized SDF via calculating the\nsecond-order gradients of SDF. We maximize the similar-\nity of the normal of SDF \u02c6g and our generated normal g to\nprovide 3D geometric supervision. To tolerate trivial inac-\ncuracies of the generated normals from different views, we\nintroduce a geometry-aware normal loss:\nLnormal =\n1\nP wk\nX\nwk \u00b7 ek\nek = (1 \u2212 cos (\u02c6gk, gk)) ,\n(7)\nwhere ek is error between the normal of SDF \u02c6gk and the\ngenerated normal gk for the kth sampled ray, cos(\u00b7, \u00b7) de-\nnotes cosine function, and wk is a geometric-aware weight\ndefined as\nwk =\n(\n0,\ncos (vk, gk) > \u03f5\nexp (| cos (vk, gk) |) ,\ncos (vk, gk) \u2264 \u03f5 .\n(8)\nHere exp(\u00b7) denotes exponential function, |\u00b7| denotes abso-\nlute function, \u03f5 is a negative threshold closing to zero, and\nwe measure the cosine value of the angle between the gen-\nerated normal gk and the kth ray\u2019s viewing direction vk.\nThe design rationale behind this approach lies in the ori-\nentation of normals, which are deliberately set to face out-\nward, while the viewing direction is inward-facing. This\nconfiguration ensures that the angle between the normal\nvector and the viewing ray remains not less than 90\u25e6. A\ndeviation from this criterion would imply inaccuracies in\nthe generated normals.\nFurthermore, it\u2019s worth noting that a 3D point on the op-\ntimized shape can be visible from multiple distinct view-\npoints, thereby being influenced by multiple normals cor-\nresponding to these views. However, if these multiple nor-\nmals do not exhibit perfect consistency, the geometric su-\npervision may become somewhat ambiguous, leading to im-\nprecise geometry. To address this issue, rather than treat-\ning normals from different views equally, we introduce a\nweighting mechanism. We assign higher weights to normals\nthat form larger angles with the viewing rays. This prioriti-\nzation enhances the accuracy of our geometric supervision\nprocess.\nOutlier-dropping Losses. Besides the normal loss, mask\nloss and color loss are also adopted for optimizing geometry\nand appearance. However, it is inevitable that there exist\nsome inaccuracies in the masks and color images, which\nwill accumulate in the optimization and thus cause noisy\n7\n(c) Sequential models\n(normal model \u2192 color model)\nInput images\n(d) Sequential models\n(color model \u2192 normal model)\n(a) Cross-domain model\nw/ cross-domain attention\n(b) Cross-domain model\nw/o cross-domain attention\nFigure 7. Ablation studies on different cross-domain diffusion schemes.\nsurfaces and holes.\nTo mitigate the issues, we employ a simple yet effective\nstrategy named outlier-dropping loss. Taking the color loss\ncalculation as an example, instead of simply summing up\nthe color errors of all sampled rays at each iteration, we first\nsort these errors in a descending order and then discard the\ntop largest errors according to a predefined percentage. This\napproach is motivated by the fact that erroneous predictions\nlack sufficient consistency with other views, making them\nless amenable to effective minimization during optimiza-\ntion, and they often result in large errors. By implementing\nthis strategy, the optimized geometry can eliminate incor-\nrect isolated geometries and distorted textures.\n5. Experiments\n5.1. Implementation Details\nWe train our model on the LVIS subset of the Obja-\nverse dataset [9], which comprises approximately 30,000+\nobjects following a cleanup process.\nSurprisingly, even\nwith fine-tuning on this relatively small-scale dataset, our\nmethod demonstrates robust generalization capabilities. To\ncreate the rendered multi-view dataset, we first normalized\neach object to be centered and of unit scale. Then we render\nnormal maps and color images from six views, including\nthe front, back, left, right, front-right, and front-left views,\nusing Blenderproc [11]. Additionally, to enhance dataset di-\nversity, we applied random rotations to the 3D assets during\nthe rendering process.\nWe fine-tune our model starting from the Stable Diffu-\nsion Image Variations Model, which has previously been\nfine-tuned with image conditions. We retain the optimizer\nsettings and \u03f5-prediction strategy from the previous fine-\ntuning. During fine-tuning, we use a reduced image size\nof 256 \u00d7 256 and a total batch size of 512 for training. The\nfine-tuning process involves training the model for 30,000\nsteps. This entire training procedure typically requires ap-\nproximately 3 days on a cluster of 8 Nvidia Tesla A800\nGPUs. To reconstruct 3D geometry from the 2D represen-\ntations, our method is built on the instant-NGP based SDF\nreconstruction method [19].\n5.2. Baselines\nWe adopt Zero123 [31], RealFusion [38], Magic123 [44],\nOne-2-3-45 [30], Point-E [41], Shap-E [25] and a recent\nwork SyncDreamer [33] as baseline methods. Given an in-\nput image, zero123 is capable of generating novel views\nof arbitrary viewpoints, and it can be incorporated with\nSDS loss [43] for 3D reconstruction (we adopt the im-\nplementation of ThreeStudio [20]). RealFusion [38] and\nMagic123 [44] leverage Stable Diffusion [47] and SDS loss\nfor single-view reconstruction.\nOne-2-3-45 [30] directly\npredict SDFs via SparseNeuS [36] by taking the generated\nmultiple images of Zero123 [31]. Point-E [41] and Shap-\nE [25] are 3D generative models trained on a large inter-\nnal OpenAI 3D dataset, both of which are able to convert\na single-view image into a point cloud or an implicit rep-\nresentation. SyncDreamer[33] aims to generate multi-view\n8\nInput image\nBaseline\nBaseline + Outlier-dropping\nBaseline + Geo-aware normal\nFull model\nFigure 8. Ablation study on the strategies in the mesh extraction module: geometry-aware normal loss and outlier-dropping strategy.\nMethod\nChamfer Dist.\u2193\nVolume IoU\u2191\nRealfusion [38]\n0.0819\n0.2741\nMagic123 [44]\n0.0516\n0.4528\nOne-2-3-45 [30]\n0.0629\n0.4086\nPoint-E [41]\n0.0426\n0.2875\nShap-E [25]\n0.0436\n0.3584\nZero123 [31]\n0.0339\n0.5035\nSyncDreamer [33]\n0.0261\n0.5421\nOurs\n0.0199\n0.6244\nTable 1. Quantitative comparison with baseline methods. We re-\nport Chamfer Distance and Volume IoU on the GSO [13] dataset.\nconsistent images from a single image for deriving 3D ge-\nometry.\n5.3. Evaluation Protocol\nEvaluation Datasets. Following prior research [31, 33], we\nadopt the Google Scanned Object dataset [13] for our eval-\nuation, which includes a wide variety of common every-\nday objects. Our evaluation dataset matches that of Sync-\nDreamer [33], comprising 30 objects that span from every-\nday items to animals. For each object in the evaluation set,\nwe render an image with a size of 256\u00d7256, which serves as\nthe input. Additionally, to assess the generalization ability\nof our model, we include some images with diverse styles\ncollected from the internet in our evaluation.\nMetrics. To evaluate the quality of the single-view recon-\nstructions, we adopt two commonly used metrics Cham-\nfer Distances (CD) and Volume IoU between ground-truth\nshapes and reconstructed shapes. Since different methods\nadopt various canonical systems, we first align the gener-\nated shapes to the ground-truth shapes before calculating\nthe two metrics. Moreover, we adopt the metrics PSNR,\nSSIM [62] and LPIPS [74] for evaluating the generated\ncolor images.\n5.4. Single View Reconstruction\nWe evaluate the quality of the reconstructed geometry of\ndifferent methods. The quantitative results are summarized\nin Table 1, and the qualitative comparisons are presented\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nRealfusion [38]\n15.26\n0.722\n0.283\nZero123 [31]\n18.93\n0.779\n0.166\nSyncDreamer [33]\n20.05\n0.798\n0.146\nOurs\n26.07\n0.924\n0.065\nTable 2. The quantitative comparison in novel view synthesis. We\nreport PSNR, SSIM [62], LPIPS [74] on the GSO [13] dataset.\nin Fig. 6. Shap-E [25] tends to produce incomplete and\ndistorted meshes. SyncDreamer [33] generates shapes that\nare roughly aligned with the input image but lack detailed\ngeometries, and the texture quality is subpar.\nOne-2-3-\n45 [30] attempts to reconstruct meshes from the multiview-\ninconsistent outputs of Zero123 [31].\nWhile it can cap-\nture coarse geometries, it loses important details in the pro-\ncess. In comparison, our method stands out by achieving\nthe highest reconstruction quality, both in terms of geome-\ntry and textures.\n5.5. Novel View Synthesis\nWe evaluate the quality of novel view synthesis for dif-\nferent methods. The quantitative results are presented in\nTable 2, and the qualitative results can be found in Fig-\nure 3. Zero123 [31] produces visually reasonable images,\nbut they lack multi-view consistency since it operates on\neach view independently. Although SyncDreamer [31] in-\ntroduces a volume attention scheme to enhance the consis-\ntency of multi-view images, their model is sensitive to the\nelevation degrees of the input images and tends to produce\nunreasonable results. In contrast, our method is capable\nof generating images that not only exhibit semantic consis-\ntency with the input image but also maintain a high degree\nof consistency across multiple views in terms of both colors\nand geometry.\n5.6. Discussions\nIn this section, we conduct a set of studies to verify the ef-\nfectiveness of our designs as well as the properties of the\nmethod.\n9\nW/ multi-view attention\nW/O multi-view attention\nInput images\nFront-right view\nFront-right view\nBack view\nBack view\nFigure 9. Ablation study on multi-view attention.\nCross-Domain Diffusion.\nTo validate the effectiveness\nof our proposed cross-domain diffusion scheme, we study\nthe following settings: (a) cross-domain model with cross-\ndomain attention; (b) cross-domain model without cross-\ndomain attention; (c) sequential model rgb-to-normal: first\ntrain a multi-view color diffusion model then train a multi-\nview normal diffusion model conditioned on the previously\ngenerated color images; (d) sequential model normal-to-\nrgb: first train a multi-view normal diffusion model then\ntrain a multi-view color diffusion model conditioned on the\npreviously generated normal images.\nAs shown in (a) and (b) of Figure 7, it\u2019s evident that\nthe cross-domain attentions significantly enhance the con-\nsistency between color images and normals, particularly in\nterms of the detailed geometries of objects like the ice-\ncream and Pharaoh sculpture. From (c) and (d) of Figure 7,\nwhile the normals and color images generated by sequential\nmodels maintain some consistency, their results suffer from\nperformance drops.\nFor the sequential model rgb-to-normal, conditioning on\nthe separately generated normal maps, the generated color\nimages exhibit color aberrations in comparison to the in-\nput image, as shown in (c) of Figure 7. Conversely, for the\nsequential model normal-to-rgb, conditioning on the sepa-\nrately generated color images, the normal maps give unrea-\nsonable geometry, as illustrated in (d) of Figure 7. These ex-\nperiments demonstrate that jointly predicting normal maps\nand color images through the cross-domain attention mech-\nanism can facilitate a comprehensive perception of informa-\ntion from different domains. We also speculate that in the\ncontext of sequential models, the generated color images or\nnormal maps of stage 1 may exhibit a minor domain gap to\nthe ground truth data trained in stage 2. Therefore, com-\npared to sequential prediction, the cross-domain approach\nFigure 10. The qualitative results of Wonder3D on various animal\nobjects.\nproves to be more effective in enhancing the quality of each\ndomain as well as the overall prediction.\nMulti-view Consistency. We conducted an analysis of the\neffectiveness of the multi-view attention mechanism, as il-\nlustrated in Figure 9. Our findings show that the multi-view\nattention greatly enhances the 3D consistency of the gener-\nated multi-view images, particularly for the rear views. In\nthe absence of the multi-view attention, the color images of\nthe rear views exhibited unrealistic predictions.\nNormal Fusion. To assess the efficacy of our normal fusion\nalgorithm, we conducted experiments using the complex\nlion model, which is rich in geometric details, as illustrated\nin Figure 8. The baseline model\u2019s surfaces exhibited numer-\nous holes and noises. Utilizing either the geometry-aware\nnormal loss or the outlier-dropping loss helps mitigate the\nnoisy surfaces. Finally, combining both strategies yields the\nbest performance, resulting in clean surfaces while preserv-\ning detailed geometries.\nGeneralization. To demonstrate the generalization capabil-\nity of our method, we conducted evaluations using diverse\nimage styles, including sketches, cartoons, and images of\nanimals, as shown in Figure 5 and Figure 10. Despite varia-\ntions in lighting effects and geometric complexities among\nthese images, our method consistently generated multi-view\nnormal maps and color images, ultimately yielding high-\nquality geometries.\n6. Conclusions and Future Works\nConclusions. In this paper, we present Wonder3D, an in-\nnovative approach designed for efficiently generating high-\n10\nfidelity textured meshes from single-view images. When\nprovided with a single image, Wonder3D initiates the pro-\ncess by generating consistent multi-view normal maps and\npaired color images. Subsequently, it utilizes a novel nor-\nmal fusion algorithm to extract highly-detailed geometries\nfrom these multi-view 2D representations. Experimental re-\nsults demonstrate that our method upholds good efficiency\nand robust generalization, and delivers high-quality geome-\ntry.\nFuture Works.\nWhile Wonder3D has demonstrated\npromising performance in reconstructing 3D geometry from\nsingle-view images, there are still some limitations that the\ncurrent framework does not fully address. First, the cur-\nrent implementation of Wonder3D only produces normals\nand color images from six views. This limited number of\nviews makes it challenging for our method to accurately re-\nconstruct objects with very thin structures and severe oc-\nclusions. Additionally, expanding Wonder3D to incorpo-\nrate more views would demand increased computational re-\nsources during training. To address this issue, Wonder3D\nmay benefit from leveraging more efficient multi-view at-\ntention mechanisms to handle a greater number of views\neffectively.\nAcknowledgements\nThanks for the GPU support from VAST, the valuable sug-\ngestions from Wei Yin, the help in data rendering from\nDehu Wang.\nReferences\n[1] Titas Anciukevi\u02c7cius, Zexiang Xu, Matthew Fisher, Paul Hen-\nderson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero. Ren-\nderdiffusion: Image diffusion for 3d reconstruction, inpaint-\ning and generation. In CVPR, 2023. 2, 3\n[2] Mohammadreza\nArmandpour,\nHuangjie\nZheng,\nAli\nSadeghian, Amir Sadeghian, and Mingyuan Zhou.\nRe-\nimagine the negative prompt algorithm:\nTransform 2d\ndiffusion into 3d, alleviate janus problem and beyond. arXiv\npreprint arXiv:2304.04968, 2023. 3\n[3] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W\nBergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini\nDe Mello, Tero Karras, and Gordon Wetzstein. Generative\nnovel view synthesis with 3d-aware diffusion models.\nIn\nICCV, 2023. 3\n[4] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen\nTu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf:\nA unified approach to 3d generation and reconstruction. In\nICCV, 2023. 2, 3\n[5] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 2, 3\n[6] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai,\nGang Yu, Lei Yang, and Guosheng Lin. It3d: Improved text-\nto-3d generation with explicit view synthesis. arXiv preprint\narXiv:2308.11473, 2023. 3\n[7] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexan-\nder G Schwing, and Liang-Yan Gui. Sdfusion: Multimodal\n3d shape completion, reconstruction, and generation.\nIn\nCVPR, 2023. 2, 3\n[8] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,\nand Mubarak Shah. Diffusion models in vision: A survey.\nT-PAMI, 2023. 3\n[9] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects. In CVPR, 2023. 8\n[10] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,\nYin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.\nNerdi: Single-view nerf synthesis with language-guided dif-\nfusion as general image priors. In CVPR, 2023. 3\n[11] Maximilian Denninger, Dominik Winkelbauer, Martin Sun-\ndermeyer, Wout Boerdijk, Markus Knauer, Klaus H. Strobl,\nMatthias Humt, and Rudolph Triebel.\nBlenderproc2: A\nprocedural pipeline for photorealistic rendering. Journal of\nOpen Source Software, 8(82):4901, 2023. 8\n[12] Zhiyang Dou, Qingxuan Wu, Cheng Lin, Zeyu Cao,\nQiangqiang Wu, Weilin Wan, Taku Komura, and Wenping\nWang. Tore: Token reduction for efficient human mesh re-\ncovery with transformer. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 15143\u2013\n15155, 2023. 2\n[13] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-\nman, Ryan Hickman, Krista Reymann, Thomas B McHugh,\nand Vincent Vanhoucke. Google scanned objects: A high-\nquality dataset of 3d scanned household items.\nIn ICRA,\n2022. 2, 7, 9\n[14] Ziya Erkoc\u00b8, Fangchang Ma, Qi Shan, Matthias Nie\u00dfner,\nand Angela Dai.\nHyperdiffusion:\nGenerating implicit\nneural fields with weight-space diffusion.\narXiv preprint\narXiv:2303.17015, 2023. 2, 3\n[15] Hugging Face. One-2-3-45. https://huggingface.\nco/spaces/One-2-3-45/One-2-3-45, 2023. 3\n[16] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. NeurIPS, 2022. 2, 3\n[17] Jiatao Gu, Qingzhe Gao, Shuangfei Zhai, Baoquan Chen,\nLingjie Liu, and Josh Susskind. Learning controllable 3d\ndiffusion models from single-view images. arXiv preprint\narXiv:2304.06700, 2023. 2, 3\n[18] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind,\nChristian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.\nNerfdiff: Single-image view synthesis with nerf-guided dis-\ntillation from 3d-aware diffusion. In ICML, 2023. 3\n[19] Yuan-Chen Guo. Instant neural surface reconstruction, 2022.\nhttps://github.com/bennyguo/instant-nsr-pl. 8\n[20] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian\nLaforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-\nXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.\nthreestudio: A unified framework for 3d content generation.\n11\nhttps://github.com/threestudio-project/\nthreestudio, 2023. 8\n[21] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-\nlas O\u02d8guz. 3dgen: Triplane latent diffusion for textured mesh\ngeneration. arXiv preprint arXiv:2303.05371, 2023. 2, 3\n[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020. 3\n[23] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-\nJun Zha, and Lei Zhang. Dreamtime: An improved optimiza-\ntion strategy for text-to-3d content creation. arXiv preprint\narXiv:2306.12422, 2023. 3\n[24] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n867\u2013876, 2022. 2\n[25] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 2, 3, 8, 9\n[26] Animesh Karnewar, Niloy J Mitra, Andrea Vedaldi, and\nDavid Novotny. Holofusion: Towards photo-realistic 3d gen-\nerative modeling. In ICCV, 2023.\n[27] Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten\nKreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio\nTorralba, and Sanja Fidler. Neuralfield-ldm: Scene gener-\nation with hierarchical latent diffusion models. In CVPR,\n2023. 2, 3\n[28] Jiabao Lei, Jiapeng Tang, and Kui Jia. Generative scene syn-\nthesis via incremental view inpainting using rgbd diffusion\nmodels. In CVPR, 2022. 3\n[29] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In CVPR, 2023. 2, 3\n[30] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang\nXu, and Hao Su. One-2-3-45: Any single image to 3d mesh\nin 45 seconds without per-shape optimization. arXiv preprint\narXiv:2306.16928, 2023. 8, 9\n[31] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object. In ICCV, 2023. 2, 3, 4, 5,\n8, 9\n[32] Xinhang Liu, Shiu-hong Kao, Jiaben Chen, Yu-Wing Tai,\nand Chi-Keung Tang. Deceptive-nerf: Enhancing nerf recon-\nstruction using pseudo-observations from diffusion models.\narXiv preprint arXiv:2305.15171, 2023. 3\n[33] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv preprint arXiv:2309.03453, 2023. 2, 3, 4, 8, 9\n[34] Zhen\nLiu,\nYao\nFeng,\nMichael\nJ\nBlack,\nDerek\nNowrouzezahrai, Liam Paull, and Weiyang Liu. Meshdiffu-\nsion: Score-based generative 3d mesh modeling. In ICLR,\n2023. 2, 3\n[35] Xiaoxiao Long, Cheng Lin, Lingjie Liu, Wei Li, Christian\nTheobalt, Ruigang Yang, and Wenping Wang. Adaptive sur-\nface normal constraint for depth estimation. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 12849\u201312858, 2021. 2\n[36] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and\nWenping Wang. Sparseneus: Fast generalizable neural sur-\nface reconstruction from sparse views. In European Confer-\nence on Computer Vision, pages 210\u2013227. Springer, 2022. 3,\n8\n[37] Shitong Luo and Wei Hu. Diffusion probabilistic models for\n3d point cloud generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2837\u20132845, 2021. 2, 3\n[38] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In CVPR, 2023. 2, 3, 8, 9\n[39] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 5\n[40] Norman\nM\u00a8uller,\nYawar\nSiddiqui,\nLorenzo\nPorzi,\nSamuel Rota Bulo,\nPeter Kontschieder,\nand Matthias\nNie\u00dfner.\nDiffrf:\nRendering-guided 3d radiance field\ndiffusion. In CVPR, 2023. 2, 3\n[41] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 2, 3, 8, 9\n[42] Evangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski,\nChaoyang Wang, Luc Van Gool, and Sergey Tulyakov.\nAutodecoding latent 3d diffusion models.\narXiv preprint\narXiv:2307.05445, 2023. 2, 3\n[43] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR,\n2023. 2, 3, 8\n[44] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:\nOne image to high-quality 3d object generation using both\n2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843,\n2023. 2, 3, 8, 9\n[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 3, 4, 5\n[46] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nNataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-\nman, Michael Rubinstein, Jonathan Barron, et al. Dream-\nbooth3d:\nSubject-driven text-to-3d generation.\narXiv\npreprint arXiv:2303.13508, 2023. 3\n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 3, 8\n[48] Hoigi Seo, Hayeon Kim, Gwanghyun Kim, and Se Young\nChun.\nDitto-nerf: Diffusion-based iterative text to omni-\ndirectional 3d model.\narXiv preprint arXiv:2304.02827,\n2023. 3\n[49] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon\nKo, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,\n12\nand Seungryong Kim.\nLet 2d diffusion model know 3d-\nconsistency for robust text-to-3d generation. arXiv preprint\narXiv:2303.07937, 2023. 3\n[50] Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-\n3d: Towards single-view anything reconstruction in the wild.\narXiv preprint arXiv:2304.10261, 2023. 3\n[51] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512, 2023. 2, 3, 4\n[52] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In ICML, 2015. 3\n[53] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea\nVedaldi. Viewset diffusion:(0-) image-conditioned 3d gener-\native models from 2d data. arXiv preprint arXiv:2306.07881,\n2023. 3\n[54] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d\ncreation from a single image with diffusion prior. In ICCV,\n2023. 3\n[55] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and\nYasutaka Furukawa. Mvdiffusion: Enabling holistic multi-\nview image generation with correspondence-aware diffusion.\narXiv preprint arXiv:2307.01097, 2023. 3\n[56] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon\nRezchikov, Joshua B Tenenbaum, Fr\u00b4edo Durand, William T\nFreeman, and Vincent Sitzmann.\nDiffusion with forward\nmodels: Solving stochastic inverse problems without direct\nsupervision. arXiv preprint arXiv:2306.11719, 2023. 3\n[57] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gen-\neration of realistic 3d meshes from text prompts.\narXiv\npreprint arXiv:2304.12439, 2023. 3\n[58] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-\nBin Huang, and Johannes Kopf. Consistent view synthesis\nwith pose-guided diffusion models. In CVPR, 2023. 3\n[59] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In CVPR,\n2023. 2, 3\n[60] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\nIn NeurIPS, 2021. 6\n[61] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin\nBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\nWen, Qifeng Chen, et al. Rodin: A generative model for\nsculpting 3d digital avatars using diffusion. In CVPR, 2023.\n2, 3\n[62] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image quality assessment: from error visibility\nto structural similarity. TIP, 2004. 9\n[63] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 2, 3\n[64] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan\nHo,\nAndrea\nTagliasacchi,\nand\nMohammad\nNorouzi. Novel view synthesis with diffusion models. arXiv\npreprint arXiv:2210.04628, 2022. 3\n[65] Jinbo Wu, Xiaobo Gao, Xing Liu, Zhengyang Shen, Chen\nZhao, Haocheng Feng, Jingtuo Liu, and Errui Ding.\nHd-\nfusion: Detailed text-to-3d generation leveraging multiple\nnoise estimation. arXiv preprint arXiv:2307.16183, 2023.\n3\n[66] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin\nTong. 3d-aware image generation using 2d diffusion mod-\nels. arXiv preprint arXiv:2303.17905, 2023. 3\n[67] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-wild\n2d photo to a 3d object with 360 views. arXiv e-prints, pages\narXiv\u20132211, 2022. 3\n[68] Paul Yoo, Jiaxian Guo, Yutaka Matsuo, and Shixiang Shane\nGu. Dreamsparse: Escaping from plato\u2019s cave with 2d frozen\ndiffusion model given sparse views. CoRR, 2023. 3\n[69] Chaohui Yu, Qiang Zhou, Jingliang Li, Zhe Zhang, Zhibin\nWang, and Fan Wang. Points-to-3d: Bridging the gap be-\ntween sparse points and shape-controllable text-to-3d gener-\nation. arXiv preprint arXiv:2307.13908, 2023. 3\n[70] Jason J. Yu, Fereshteh Forghani, Konstantinos G. Derpanis,\nand Marcus A. Brubaker. Long-term photometric consistent\nnovel view synthesis with diffusion models. In ICCV, 2023.\n3\n[71] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,\nOr Litany, Sanja Fidler, and Karsten Kreis.\nLion: Latent\npoint diffusion models for 3d shape generation. In NeurIPS,\n2022. 2, 3\n[72] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter\nWonka. 3dshape2vecset: A 3d shape representation for neu-\nral fields and generative diffusion models. In SIGGRAPH,\n2023. 2, 3\n[73] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing\nLiao. Text2nerf: Text-driven 3d scene generation with neural\nradiance fields. arXiv preprint arXiv:2305.11588, 2023. 3\n[74] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 9\n[75] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation\nand completion through point-voxel diffusion. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 5826\u20135835, 2021. 2, 3\n[76] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-\ntilling view-conditioned diffusion for 3d reconstruction. In\nCVPR, 2023. 3\n[77] Joseph Zhu and Peiye Zhuang.\nHifa: High-fidelity text-\nto-3d with advanced diffusion guidance.\narXiv preprint\narXiv:2305.18766, 2023. 3\n13\n"
  },
  {
    "title": "ConvNets Match Vision Transformers at Scale",
    "link": "https://arxiv.org/pdf/2310.16764.pdf",
    "upvote": "18",
    "text": "2023-10-26\nConvNets Match Vision Transformers at Scale\nSamuel L Smith1, Andrew Brock1, Leonard Berrada1 and Soham De1\n1Google DeepMind\nMany researchers believe that ConvNets perform well on small or moderately sized datasets, but are not\ncompetitive with Vision Transformers when given access to datasets on the web-scale. We challenge this\nbelief by evaluating a performant ConvNet architecture pre-trained on JFT-4B, a large labelled dataset of\nimages often used for training foundation models. We consider pre-training compute budgets between\n0.4k and 110k TPU-v4 core compute hours, and train a series of networks of increasing depth and width\nfrom the NFNet model family. We observe a log-log scaling law between held out loss and compute\nbudget. After fine-tuning on ImageNet, NFNets match the reported performance of Vision Transformers\nwith comparable compute budgets. Our strongest fine-tuned model achieves a Top-1 accuracy of 90.4%.\nKeywords: ConvNets, CNN, Convolution, Transformer, Vision, ViTs, NFNets, JFT, Scaling, Image\nIntroduction\nConvolutional Neural Networks (ConvNets) were\nresponsible for many of the early successes of\ndeep learning.\nDeep ConvNets were first de-\nployed commercially over 20 years ago (Le-\nCun et al., 1998), while the success of AlexNet\non the ImageNet challenge in 2012 re-ignited\nwidespread interest in the field (Krizhevsky et al.,\n2017). For almost a decade ConvNets (typically\nResNets (He et al., 2016a,b)) dominated com-\nputer vision benchmarks. However in recent years\nthey have increasingly been replaced by Vision\nTransformers (ViTs) (Dosovitskiy et al., 2020).\nSimultaneously, the computer vision commu-\nnity has shifted from primarily evaluating the\nperformance of randomly initialized networks\non specific datasets like ImageNet, to evaluat-\ning the performance of networks pre-trained on\nlarge general purpose datasets collected from the\nweb. This raises an important question; do Vision\nTransformers outperform ConvNet architectures\npre-trained with similar computational budgets?\nAlthough most researchers in the community\nbelieve Vision Transformers show better scaling\nproperties than ConvNets, there is surprisingly\nlittle evidence to support this claim. Many papers\nstudying ViTs compare to weak ConvNet base-\nlines (typically the original ResNet architecture\n(He et al., 2016a)). Additionally, the strongest ViT\nmodels have been pre-trained using large com-\npute budgets beyond 500k TPU-v3 core hours\n(Zhai et al., 2022), which significantly exceeds\nthe compute used to pre-train ConvNets.\nWe evaluate the scaling properties of the NFNet\nmodel family (Brock et al., 2021), a pure con-\nvolutional architecture published concurrently\nwith the first ViT papers, and the last ConvNet\nto set a new SOTA on ImageNet. We do not\nmake any changes to the model architecture or\nthe training procedure (beyond tuning simple\nhyper-parameters such as the learning rate or\nepoch budget). We consider compute budgets\nup to a maximum of 110k TPU-v4 core hours,1\nand pre-train on the JFT-4B dataset which con-\ntains roughly 4 billion labelled images from 30k\nclasses (Sun et al., 2017). We observe a log-log\nscaling law between validation loss and the com-\npute budget used to pre-train the model. After\nfine-tuning on ImageNet, our networks match the\nperformance of pre-trained ViTs with comparable\ncompute budgets (Alabdulmohsin et al., 2023;\nZhai et al., 2022), as shown in Figure 1.\nPre-trained NFNets obey scaling laws\nWe train a range of NFNet models of varying\ndepth and width on JFT-4B. Each model is trained\nfor a range of epoch budgets between 0.25 and\n8, using a cosine decay learning rate schedule.\n1TPU-v4 cores have roughly double the theoretical flops\nof TPU-v3 cores, however both cores have similar memory.\nCorresponding author(s): slsmith@google.com\n\u00a9 2023 Google DeepMind. All rights reserved\narXiv:2310.16764v1  [cs.CV]  25 Oct 2023\nConvNets Match Vision Transformers at Scale\n103\n104\n105\nTPU-v4 Core Hours\n10\n11\n12\n13\n14\nImageNet Top-1 Error (%)\nF1\nF3\nF3+\nF7\nF7+\nF7+ with RA\nViT-g/14\nViT-G/14\nSoViT-400m/14\nFigure 1 | ImageNet Top-1 error, after fine-tuning pre-trained NFNet models for 50 epochs. Both axes\nare log-scaled. Performance improves consistently as the compute used during pre-training increases.\nOur largest model (F7+) achieves comparable performance to that reported for pre-trained ViTs with\na similar compute budget (Alabdulmohsin et al., 2023; Zhai et al., 2022). The performance of this\nmodel improved further when fine-tuned with repeated augmentation (RA) (Hoffer et al., 2019).\nThe base learning rate is tuned separately for\neach epoch budget on a small logarithmic grid.\nIn Figure 2, we provide the validation loss at the\nend of training on a held out set of 130k images,\nplotted against the compute budget required to\ntrain each model2. We note that F7 has the same\nwidth as F3, but is double the depth. Similarly\nF3 is double the depth of F1, and F1 is double\nthe depth of F0. F3+ and F7+ have the same\ndepths as F3 and F7 but larger width. We train us-\ning SGD with Momentum and Adaptive Gradient\nClipping (AGC) at batch size 4096, and we use an\nimage resolution of 224\u00d7224 during training and\n256 \u00d7 256 at evaluation. For additional details\ndescribing the NFNet architecture and training\npipeline we refer the reader to the original paper\n(Brock et al., 2021), including the pre-training\nframework for JFT described in Section 6.2. Note\nthat we removed near-duplicates of images in the\ntraining and validation sets of ImageNet from\nJFT-4B before training (Kolesnikov et al., 2020).\nFigure 2 shows a clear linear trend, consistent\nwith a log-log scaling law between validation loss\nand pre-training compute. This matches the log-\nlog scaling laws previously observed when per-\n2We estimate the compute required to train each model\nby eye from the typical steps per second achieved by each\nmodel during training (when not pre-empted).\nforming language modelling with transformers\n(Brown et al., 2020; Hoffmann et al., 2022).\nThe optimal model size and the optimal epoch\nbudget (which achieve the lowest validation loss)\nboth increase in size as the compute budget in-\ncreases. We found that a reliable rule of thumb is\nto scale the model size and the number of train-\ning epochs at the same rate, as previously ob-\nserved for language modelling by Hoffmann et al.\n(2022). We note that the optimal epoch budget\nwas greater than 1 for overall compute budgets\ngreater than roughly 5k TPU-v4 core hours.\nIn Figure 3 we plot the observed optimal learn-\ning rate (which minimizes validation loss), for 3\nof our models, across a range of epoch budgets.3\nNote that we tuned the learning rate on a loga-\nrithmic grid spaced by factors of 2. We find that\nall models in the NFNet family show a similar\noptimal learning rate \ud835\udefc \u2248 1.6 for small epoch\nbudgets. However the optimal learning rate falls\nas the epoch budget rises, and for large models\nthe optimal learning rate falls more quickly. In\npractice one can efficiently tune the learning rate\nwithin 2 trials by assuming that the optimal learn-\ning rate falls slowly but monotonically as both the\nmodel size and the epoch budget increases.\n3The optimal learning rate showed very similar trends\nfor all models. We select 3 models here for visual clarity.\n2\nConvNets Match Vision Transformers at Scale\n103\n104\n105\nTPU-v4 Core Hours\n2.4\n2.5\n2.6\n2.7\n2.8\n2.9\n3.0\nJFT-4B Validation Loss\nF0\nF1\nF3\nF3+\nF7\nF7+\nFigure 2 | Held out loss of NFNets on JFT-4B,\nplotted against the compute used during training.\nBoth axes are log-scaled, and each curve denotes\na different model trained for a range of epoch\nbudgets. We observe a linear trend, matching the\nscaling laws observed for language modelling.\nFinally, we note that some pre-trained models\nin Figure 2 perform less well than expected. For\nexample, the curve for NFNet-F7+ models at dif-\nferent pre-training budgets is not smooth. We be-\nlieve this arises because our data loading pipeline\ndid not guarantee that each training example\nwould be sampled once per epoch if the training\nrun was pre-empted/restarted, potentially caus-\ning some training examples to be under-sampled\nif a training run was restarted multiple times.\nFine-tuned NFNets are competitive\nwith Vision Transformers on ImageNet\nIn Figure 1, we fine-tune our pre-trained NFNets\non ImageNet, and plot the Top-1 error against\nthe compute used during pre-training. We fine-\ntune each model for 50 epochs using sharpness\naware minimization (SAM) (Foret et al., 2020)\nwith stochastic depth and dropout. We train at\nresolution 384 \u00d7 384 and evaluate at 480 \u00d7 480.\nThe ImageNet Top-1 accuracy consistently im-\nproves as the compute budget increases. Our most\nexpensive pre-trained model, an NFNet-F7+ pre-\ntrained for 8 epochs, achieves an ImageNet Top-1\naccuracy of 90.3% while requiring roughly 110k\nTPU-v4 core hours to pre-train and 1.6k TPU-v4\ncore hours to fine-tune. Furthermore, we achieve\n90.4% Top-1 accuracy if we additionally intro-\nduce repeated augmentation during fine-tuning\n(Fort et al., 2021; Hoffer et al., 2019) with aug-\n0.25\n0.5\n1\n2\n4\n8\nTraining Epochs\n0.4\n0.8\n1.6\nOptimal Learning Rate\nF0\nF3\nF7+\nFigure 3 | The optimal learning rate behaves pre-\ndictably and is easy to tune. All models show\nsimilar optimal learning rates \ud835\udefc \u223c 1.6 when the\nepoch budget is small. The learning rate falls\nslowly as model size and epoch budget increases.\nmentation multiplicity 4.4 For comparison, the\nbest reported Top-1 accuracy of an NFNet on Im-\nageNet without extra data is 86.8% (Fort et al.,\n2021), achieved by an NFNet-F5 with repeated\naugmentation. This demonstrates that NFNets\nbenefit substantially from large scale pre-training.\nDespite the substantial differences between\nthe two model architectures, the performance of\npre-trained NFNets at scale is remarkably similar\nto the performance of pre-trained Vision Trans-\nformers. For example, Zhai et al. (2022) achieve\n90.2% Top-1 on ImageNet with a ViT-g/14, af-\nter pre-training on JFT-3B for 210k TPU-v3 core\nhours, and 90.45% with a ViT-G/14 after pre-\ntraining on JFT-3B for over 500k TPU-v3 core\nhours. In a recent work, Alabdulmohsin et al.\n(2023) optimize the ViT architecture and achieve\n90.3% Top-1 with a SoViT-400m/14 after pre-\ntraining on JFT-3B for 230k TPU-v3 hours.\nWe evaluated the pre-training speed for these\nmodels on TPU-v4 (using the original authors\u2019\ncodebase), and estimate that ViT-g/14 would take\n120k TPU-v4 core hours to pre-train, while ViT-\nG/14 would take 280k TPU-v4 core hours and\nSoViT-400m/14 would take 130k TPU-v4 core\nhours. We use these estimates to compare the pre-\ntraining efficiency of ViTs and NFNets in Figure 1.\nWe note however that NFNets were optimized for\nTPU-v4, and perform less well when evaluated\n4When using repeated augmentation, we reduce the num-\nber of passes through the data such that the total computa-\ntional cost of fine-tuning is constant.\n3\nConvNets Match Vision Transformers at Scale\non other devices. For example, we estimate that\nNFNet-F7+ would require 250 TPU-v3 core hours\nto pre-train for 8 epochs in our codebase.\nFinally, we note that the pre-trained check-\npoints achieving the lowest validation loss on\nJFT-4B did not always achieve the highest Top-1\naccuracy on ImageNet after fine-tuning. In par-\nticular, we found that, under a fixed pre-training\ncompute budget, the fine-tuning regime consis-\ntently favoured slightly larger models and slightly\nsmaller epoch budgets. Intuitively, larger models\nhave more capacity and are therefore better able\nto adapt to the new task. In some cases, slightly\nlarger learning rates (during pre-training) also\nachieved better performance after fine-tuning.\nDiscussion\nOur work reinforces the bitter lesson. The most\nimportant factors determining the performance\nof a sensibly designed model are the compute\nand data available for training5 (Tolstikhin et al.,\n2021). Although the success of ViTs in computer\nvision is extremely impressive, in our view there\nis no strong evidence to suggest that pre-trained\nViTs outperform pre-trained ConvNets when eval-\nuated fairly. We note however that ViTs may have\npractical advantages in specific contexts, such\nas the ability to use similar model components\nacross multiple modalities (Bavishi et al., 2023).\nAcknowledgements\nWe thank Lucas Beyer and Olivier Henaff for feed-\nback on an earlier draft of this note. We also\nthank Lucas Beyer for providing training speed\nestimates for ViT models on TPU-v4 devices.\nReferences\nI. Alabdulmohsin, X. Zhai, A. Kolesnikov, and\nL. Beyer. Getting vit in shape: Scaling laws for\ncompute-optimal model design. arXiv preprint\narXiv:2305.13035, 2023.\n5By sensibly designed, we mean models that are suffi-\nciently expressive and have stable gradient propagation.\nR. Bavishi, E. Elsen, C. Hawthorne, M. Nye,\nA. Odena, A. Somani, and S. Ta\u015f\u0131rlar. Intro-\nducing our multimodal models, 2023.\nURL\nhttps://www.adept.ai/blog/fuyu-8b.\nA. Brock, S. De, S. L. Smith, and K. Simonyan.\nHigh-performance large-scale image recogni-\ntion without normalization. In International\nConference on Machine Learning, pages 1059\u2013\n1071. PMLR, 2021.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D.\nKaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models\nare few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901,\n2020.\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, et al. An\nimage is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\nP. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur.\nSharpness-aware minimization for efficiently\nimproving generalization.\narXiv preprint\narXiv:2010.01412, 2020.\nS. Fort, A. Brock, R. Pascanu, S. De, and\nS. L. Smith.\nDrawing multiple augmenta-\ntion samples per image during training effi-\nciently decreases test error.\narXiv preprint\narXiv:2105.13343, 2021.\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual\nlearning for image recognition. In Proceedings\nof the IEEE conference on computer vision and\npattern recognition, pages 770\u2013778, 2016a.\nK. He, X. Zhang, S. Ren, and J. Sun. Identity map-\npings in deep residual networks. In European\nconference on computer vision, pages 630\u2013645.\nSpringer, 2016b.\nE. Hoffer, T. Ben-Nun, I. Hubara, N. Giladi, T. Hoe-\nfler, and D. Soudry. Augment your batch: bet-\nter training with larger batches. arXiv preprint\narXiv:1901.09335, 2019.\nJ.\nHoffmann,\nS.\nBorgeaud,\nA.\nMensch,\nE. Buchatskaya, T. Cai, E. Rutherford, D. d. L.\n4\nConvNets Match Vision Transformers at Scale\nCasas, L. A. Hendricks, J. Welbl, A. Clark, et al.\nTraining compute-optimal large language\nmodels.\narXiv preprint arXiv:2203.15556,\n2022.\nA. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver,\nJ. Yung, S. Gelly, and N. Houlsby. Big trans-\nfer (bit): General visual representation learn-\ning. In Computer Vision\u2013ECCV 2020: 16th Eu-\nropean Conference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part V 16, pages 491\u2013507.\nSpringer, 2020.\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. Im-\nagenet classification with deep convolutional\nneural networks. Communications of the ACM,\n60(6):84\u201390, 2017.\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner.\nGradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 86(11):\n2278\u20132324, 1998.\nC. Sun, A. Shrivastava, S. Singh, and A. Gupta.\nRevisiting unreasonable effectiveness of data in\ndeep learning era. In Proceedings of the IEEE in-\nternational conference on computer vision, pages\n843\u2013852, 2017.\nI. O. Tolstikhin, N. Houlsby, A. Kolesnikov,\nL. Beyer, X. Zhai, T. Unterthiner, J. Yung,\nA. Steiner, D. Keysers, J. Uszkoreit, et al. Mlp-\nmixer: An all-mlp architecture for vision. Ad-\nvances in neural information processing systems,\n34:24261\u201324272, 2021.\nX. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer.\nScaling vision transformers. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12104\u201312113,\n2022.\n5\n"
  },
  {
    "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
    "link": "https://arxiv.org/pdf/2310.16836.pdf",
    "upvote": "10",
    "text": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers\nShih-yang Liu\u22171, Zechun Liu\u22172, Xijie Huang1, Pingcheng Dong1, Kwang-Ting Cheng1\n1Hong Kong University of Science and Technology, 2Meta Reality Labs\n{sliuau, xhuangbs, pingcheng.dong}@connect.ust.hk\nzechunliu@meta.com\ntimcheng@ust.hk\nAbstract\nWe propose LLM-FP4 for quantizing both\nweights and activations in large language mod-\nels (LLMs) down to 4-bit floating-point val-\nues, in a post-training manner. Existing post-\ntraining quantization (PTQ) solutions are pri-\nmarily integer-based and struggle with bit\nwidths below 8 bits.\nCompared to integer\nquantization, floating-point (FP) quantization\nis more flexible and can better handle long-tail\nor bell-shaped distributions, and it has emerged\nas a default choice in many hardware platforms.\nOne characteristic of FP quantization is that its\nperformance largely depends on the choice of\nexponent bits and clipping range. In this re-\ngard, we construct a strong FP-PTQ baseline\nby searching for the optimal quantization pa-\nrameters. Furthermore, we observe a high inter-\nchannel variance and low intra-channel vari-\nance pattern in activation distributions, which\nadds activation quantization difficulty. We rec-\nognize this pattern to be consistent across a\nspectrum of transformer models designed for\ndiverse tasks, such as LLMs, BERT, and Vision\nTransformer models. To tackle this, we propose\nper-channel activation quantization and show\nthat these additional scaling factors can be repa-\nrameterized as exponential biases of weights,\nincurring a negligible cost. Our method, for the\nfirst time, can quantize both weights and acti-\nvations in the LLaMA-13B to only 4-bit and\nachieves an average score of 63.1 on the com-\nmon sense zero-shot reasoning tasks, which is\nonly 5.8 lower than the full-precision model,\nsignificantly outperforming the previous state-\nof-the-art by 12.7 points. Code is available at:\nhttps://github.com/nbasyl/LLM-FP4.\n1\nIntroduction\nSince the introduction of transformer architec-\nture (Vaswani et al., 2017), transformers have\nsuperseded recursive neural networks, emerging\nas the dominant architecture in numerous natu-\nral language processing (NLP) tasks (Kenton and\n*These authors contributed equally to this work\nToutanova, 2019; Lewis et al., 2020). The trans-\nformative impact of the transformer has been fur-\nther propelled by the emergence of models like\nGPT (Brown et al., 2020; OpenAI, 2023), cata-\npulting the popularity of this architecture to new\nheights. Meanwhile, the versatility of transformers\nextends beyond NLP, encompassing diverse do-\nmains such as vision (Dosovitskiy et al.; Touvron\net al., 2021), audio (Akbari et al., 2021), etc. This\ntrend towards a unified architecture for different\nmodalities represents a groundbreaking develop-\nment within the realm of deep learning.\nHowever, the advancements in transformer per-\nformance are accompanied by a corresponding in-\ncrease in model size and computational costs (Ka-\nplan et al., 2020). This poses significant challenges\nwhen attempting to leverage the full potential of\ntransformer models in use cases where memory\nor computational resources are limited. Despite\nthe extensive research and widespread adoption\nof transformers, the field of transformer compres-\nsion remains relatively underexplored. To address\nthis gap, our study focuses on the compression\nof transformers, especially through floating-point\npost-training quantization techniques.\nPost-training quantization (PTQ) offers the ad-\nvantages of simple to use with minimal fine-tuning\nrequirements (Nagel et al., 2020; Cai et al., 2020).\nExisting PTQ solutions for transformers primar-\nily focus on integer (INT) quantization (Liu et al.,\n2021; Yuan et al., 2022), which can be effective\nin certain scenarios but often break down when bit\nwidths are below 8 bit. On the other hand, floating-\npoint (FP) quantization has gained significant trac-\ntion as a more flexible alternative, capable of better\naccommodating various activation and weight dis-\ntributions. In fact, FP8 has emerged as the default\nchoice in various hardware platforms, including the\nNVIDIA H100.\nDifferent from integer (INT) quantization, a par-\nticular challenge in floating-point (FP) quantiza-\narXiv:2310.16836v1  [cs.CL]  25 Oct 2023\ntion is how to select appropriate exponent bits and\nscale parameters. Improper parameter choices can\nlead to subpar or divergent quantization results. To\ntackle this challenge, we introduce a robust recipe\nfor FP quantization, which leverage layer-wise re-\nconstruction to jointly search for optimal exponent\nbits and maximum values. Compared to previous\napproaches that utilize gradient updates for expo-\nnent bits (Kuzmin et al., 2022), our search-based\nmethod proves to be more stable and consistently\ndelivers desirable quantization results, which estab-\nlishes a strong baseline for FP-PTQ.\nFurthermore, our investigation uncovers an in-\ntriguing pattern of activation distributions in trans-\nformers, characterized by high inter-channel vari-\nance and low intra-channel variance. Similar pat-\nterns are also observed in previous works (Xiao\net al., 2022; Dettmers et al., 2022), while we argue\nthat this pattern is inherent to transformer architec-\ntures and not limited to specific tasks, as we have\nobserved consistent patterns not only in large lan-\nguage models but also in BERT model and even\nvision transformers. Motivated by these findings,\nwe introduce a novel pre-shifted exponent bias for\nFP quantization of transformers. Concretely, we\nleverage the per-channel activation variance com-\nputed from calibration data and reparameterize\nthese scales as the exponential bias of the corre-\nsponding FP quantized weight vectors. This ap-\nproach effectively addresses the challenge posed\nby high inter-channel variance while incurring neg-\nligible computational cost.\nIn summary, we study floating-point post-\ntraining quantization (PTQ) for transformer archi-\ntectures, and the contribution of this paper includes:\n\u2022 We propose a search-based framework for de-\ntermining the optimal exponent bias and maximal\nquantization value. This method outperforms ex-\nisting techniques in terms of stability and perfor-\nmance, establishing a strong baseline for floating-\npoint post-training quantization.\n\u2022 We propose a novel technique, pre-shifted expo-\nnent bias, which effectively addresses the challenge\nof high inter-channel variance in the transformer\nwith negligible computational overhead.\n\u2022 Experimental results demonstrate that the pro-\nposed method yields the first usable FP4 weight\nand activation quantized LLaMA-13B model with\nmere 5.8-point degradation in zero-shot reasoning\ntasks against the full-precision model, reducing the\ngap by \u223c70% compared to the previous SoTA.\n\u2022 We further extend our method to BERT and vi-\nsion transformers. It surpasses the previous best 4-\nbit quantized BERT by 7.8 points on GLUE dataset\nand achieves 31.4 points higher accuracy compared\nto the previous SoTA ViT quantization method for\n4-bit DeiT-S on ImageNet dataset.\n2\nRelated Works\n2.1\nPost-Training Quantization\nModel quantization can be mainly categorized\ninto quantization-aware training (QAT) and post-\ntraining quantization (PTQ), depending on whether\nit involves additional training for weight fine-\ntuning or not. Most PTQ studies are primarily\nfocused on convolutional neural networks (CNNs)\n(Nagel et al., 2020; Li et al., 2021; Wu et al., 2020;\nCai et al., 2020; Nagel et al., 2019). However, with\nthe growing popularity of transformer-based mod-\nels, only a limited number of works (Bondarenko\net al., 2021; Yuan et al., 2022; Ding et al., 2022)\nhave been conducted to realize PTQ on transform-\ners. Moreover, the existing works primarily focus\non visual transformer models and exhibit inferior\nperformance when the bit width is below 8. There-\nfore, in this work, we delve into the challenges of\nthe low-bit PTQ for language transformers.\n2.2\nFloating-Point Quantization\nFloating-point (FP) quantization has emerged as a\npromising alternative to integer quantization due\nto its ability to handle long-tail distributions, and\noffers increased flexibility (Kuzmin et al., 2022).\nAdditionally, modern GPUs such as H100 (Micike-\nvicius et al., 2022) now support FP quantization.\nNonetheless, minimal research has been conducted\non FP quantization. Only (Kuzmin et al., 2022) pro-\nposes a general FP8 quantization scheme primarily\nfor vision tasks, and (Zhang et al., 2023) adopts\na mixture of FP and INT formats quantization for\nLLMs. In this work, we propose FPQ baseline as\na general guideline for low-bit floating-point PTQ\nto compress language transformer models.\n3\nPreliminaries\n3.1\nFormulation of Floating-Point Variables\nA standard floating-point number is represented as:\nXFP = (\u22121)s2p\u2212b(1 + d1\n2 + d2\n22 + ... + dm\n2m ) (1)\nwhere s \u2208 {0, 1} is the sign bit. di \u2208 {0, 1} is ith\nmantissa bit, m denoted number of mantissa bits.\n0\n2!\n2\"\n2#\n#$%& = 2 \u2212 2'$ 2(\"!'!)= 14\n! = 2\n$ = 2\n( = 2!'$\n)\u210e+, -*\n++\n./ < 2!\n-*\n++\n./\n)\u210e+, log\"\n-*\n++\n./\n\u2265 1\n( = 2 ,-.\"\n/#\n$$\n01\n'$\n./ = 2' 23\nlog\"\n-*\n++\n./\n= log\" -*\n++ + 67\n(\nFigure 1: An illustration of floating-point (FP) quantization process using FP5 (E2M2) positive axis. The real-valued\nclipped X\u2032\u2032\nR in Eq. 5 is rescaled by the real-valued scaling factor \u02dc\u03b1. Then, the quantization step-size v is determined\nby the range [2p, 2p + 1) in which X\u2032\u2032\nR\n\u02dc\u03b1 falls (Eq. 9). Here, p \u2208 {0, 1, ..., 2e\u22121} is the exponent bit value. Lastly, X\ncan be quantized to low-bit floating point values simply by XFP = \u02dc\u03b1 \u00b7 v \u00b7\nj\nX\u2032\u2032\nR\n\u02dc\u03b1\u00b7v\nm\n(Eq. 8).\np is an integer in [0, 2e \u2212 1], and e denotes number\nof exponent bits. b is an integer exponent bias. A\nfloating point with j number exponent bits and k\nmantissa bits is denoted as FP format EjMk.\n3.2\nFloating-Point Quantization Process\nIn integer quantization, the real-valued variable XR\nis quantized to an integer XINT with the following\nformula:\nXINT = \u03b1\n\u0016\nClip\n\u0012XR\n\u03b1 , Qmin, Qmax\n\u0013\u0019\n(2)\nwhere \u230a\u00b7\u2309 is the rounding function. XR is the\nreal-valued variable, \u03b1 represents the full-precision\nscaling factor, and Qmin, Qmax are the min/max\nvalue of the quantization range. Similarly, a real-\nvalued variable XR can be converted to floating-\npoint XFP in two steps.\n(1) Scale and clip. In FP quantization, we also\nscale and clip the real-valued variable before quan-\ntization as:\nX\u2032\nR = Clip(XR, Qmin, Qmax)\n(3)\nwhere the min/max value range of signed floating-\npoint quantization can be calculated from Eq.1:\nQmax = \u2212Qmin = (2 \u2212 2\u2212m)22e\u2212b\u22121\n(4)\nHere the integer exponent bias b is another ad-\njustable hyperparameter controlling Qmax and\nQmin, which has similar functionality as \u03b1. There-\nfore, for simplicity, we reformulate Eq. 3 as:\nX\u2032\u2032\nR = Clip\n\u0010\nXR, \u02dcQmin, \u02dcQmax\n\u0011\n,\n(5)\nwhere\n\u02dcQmax = \u03b1Qmax = \u03b1 \u00b7 (2 \u2212 2\u2212m)22e\u2212b\u22121\n= \u03b1 \u00b7 2\u2212b \u00b7 (2 \u2212 2\u2212m)22e\u22120\u22121\n= 2\u2212\u02dcb \u00b7 (2 \u2212 2\u2212m)22e\u22120\u22121\n(6)\nNote that we combine the tensor-wise real-valued\nscaling factor \u03b1 with integer exponent bias b to\nform a new scaling factor \u02dc\u03b1 = 2\u2212\u02dcb = 2\u2212b \u00b7 \u03b1.\nHere \u02dcb denotes a relaxed tensor-wise real-valued\nexponent, and we can derive \u02dcb from the desired\nclipping value \u02dcQmax from Eq. 6 as:\n\u02dcb = 2e \u2212 log2 \u02dcQmax + log2(2 \u2212 2\u2212m) \u2212 1\n(7)\n(2) Compare and quantize. Different from inte-\nger quantization, which simply utilizes the round-\ning function to convert the real-valued variables\nto quantized ones, in floating-point quantization,\nthere is an additional step of comparing X\u2032\u2032\nR with\nquantization levels and then quantize:\nXFP = \u02dc\u03b1 \u00b7 v \u00b7\n\u0016 X\u2032\u2032\nR\n\u02dc\u03b1 \u00b7 v\n\u0019\n(8)\nwhere X\u2032\u2032\nR is clipped real-valued variable (Eq. 5),\n\u02dc\u03b1 is the tensor-wise floating-point scaling factor,\nand v is an integer power of 2.\nv=\n\u001a\n2\u230alog2|X\u2032\u2032\nR|+\u02dcb\u230b\u2212m if \u230alog2|X\u2032\u2032\nR|+\u02dcb\u230b\u22651\n21\u2212m\notherwise\n(9)\nHere we select the quantization level v according\nto the magnitude of X\u2032\u2032\nR\n\u02dc\u03b1 , which equals to X\u2032\u2032\nR \u00b7 2\u02dcb.\nThen the floating-point quantized variables can be\nderived with Eq.8. The illustration of the quantiza-\ntion process is in Fig. 1, detailed explanation can\nalso be found in (Micikevicius et al., 2022).\n3.3\nFloating-Point Matrix Multiplication\nWith the floating-point quantized variables, the ma-\ntrix multiplication is formulated as:\nOi,k\nout = Xi,:\nFPW:,k\nFP = \u02dc\u03b1X \u02dc\u03b1k\nW \u02dcXi,:\nFP \u02dc\nW:,k\nFP\n(10)\nHere in per-tensor activation quantization and per-\nchannel weight quantization, Xi,:\nFP denotes ith row\nin the activation matrix and W:,k\nFP denotes kth col-\numn in the weight matrix, such that each element\nOi,k\nout in the output matrix is computed by the prod-\nuct of two real-valued scalars \u02dc\u03b1X and \u02dc\u03b1k\nW times\nthe corresponding quantized activation and weight\nvectors. We depict all the possible quantization\ngranularity options that support such efficient ma-\ntrix multiplication in Appendix D.\n4\nMethod\nIn this section, we begin by introducing our joint\nformat and max value search, which establishes\nour strong baseline and already achieves state-of-\nthe-art results at 8-bit and 6-bit quantization. Then\nwe present an efficient pre-shifted exponent bias\nto tackle the catastrophic high inter-channel activa-\ntion variance in transformer models and push the\nquantization limit to 4-bit.\n4.1\nJoint Format and Max Value Search\nThe objective of post-training quantization is to\nminimize the perturbation (\u03b4X = XFP \u2212 XR)\nintroduced by quantization to the pre-trained real-\nvalued network:\nmin E[L(XR + \u03b4X) \u2212 L(XR)]\n(11)\nIn this study, we adopt the setting presented in\n(Choukroun et al., 2019; Wu et al., 2020), which\nassumes a positive correlation between the change\nin the intermediate output of the quantized model\nand Eq. 11. Therefore, minimizing the distance\nbetween the intermediate output of the quantized\nlayer ( \u02c6O) and the output of the original layer (O)\nleads to minimize Eq. 11. Hence, the objective loss\nmetric is formulated as:\nmin ( \u02c6O \u2212 O)2\n(12)\nwhich is used to search for the optimal FP quantiza-\ntion function in the following proposed framework.\nThe challenges in FP quantization arise from its\nsensitivity to the quantization format and clipping\nrange. Undesirable format selection will result in a\ncatastrophic error rate. In addition, we observe that\nthe optimal clipping range varies depending on the\nformat used. Previous work (Kuzmin et al., 2022)\non floating-point (FP) quantization-aware training\n(QAT) proposed to learn both the FP format and\nmaximum value with gradients. However, we find\nthis method suffers from over-fitting in PTQ, with\naccuracy being even worse than na\u00efve MinMax\nmethod, details can be found in Appendix E. In-\nstead, we propose a search-based algorithm that\njointly determines the optimal format and its asso-\nciated clipping range to address this challenge.\nThe searching process is conducted layer by\nlayer with the metric of minimizing Eq. 12. The\noutput of matrix multiplication corresponding to\neach sub-module is denoted as O = XY, where\nY can be either a weight tensor W or another acti-\nvation tensor.\nThe search space of q-bit FP format includes all\nformats except for the format with an exponent bit\nequal to 0, as the quantization of the format with\nan exponent bit equal to 1 already degenerates to\nINT quantization. We search for the real-valued ex-\nponent bias \u02dcb, which equals to the logarithm of the\nscaling factor. We initialize \u02dcbX and \u02dcbY from Eq. 7\nwith Qmax equals the maximum value of |XR| and\n|YR|, respectively. We then define the search space\nof \u02dcbX and \u02dcbY by linearly dividing [\u03b31\u02dcbinit\nX , \u03b32\u02dcbinit\nX ]\nand [\u03b31\u02dcbinit\nY , \u03b32\u02dcbinit\nY ] into k intervals, where \u03b31 and\n\u03b32 are empirically set to 0.01 and 1.2, and k = 100.\nThe search process is outlined in Alg.1. We\nsearch the quantization scheme in all the matrix\nmultiplication layers in parallel following (Yuan\net al., 2022; Bai et al., 2022). The algorithm can be\ndivided into two parts. (1) Do forward propagation\nto store the intermediate raw output of each layer\nl. (2) Iteratively update the optimal format and bi-\nases for each layer for three rounds by minimizing\nthe reconstruction metric (Eq. 12). We name this\nsearch-based framework as Floating Point Quanti-\nzation Baseline (FPQ baseline), and it can already\nachieve state-of-the-art results on both 8-bit and 6-\nbit settings.\n4.2\nPre-Shifted Exponent Bias\nIn transformer architectures, we observed an in-\ntriguing phenomenon of high inter-channel vari-\nance. As shown in Fig.2, the magnitudes of values\nwithin the same channel are close to each other\nbut exhibit significant differences across different\nchannels. This phenomenon is not only observed\nin language models (i.e., LLaMA and BERT) but\nalso significant in vision transformer models. Since\noutlier channels are often orders of magnitude big-\nger than the rest, they will dominate the quantiza-\ntion precision of the quantized tensor, resulting in\nless representation capacity for those channels with\nsmaller magnitudes (Xiao et al., 2022). This makes\ntensor-wise or token-wise scaling factor insufficient\nfor accurate activations quantization.\nAlgorithm 1 FPQ baseline\n1: Input: Calibration dataset, Full-precision Model M,\nQuantization format search space RX (e.g., RX\n=\n{E3M0, E2M1, E1M2} for FP4), number of round\nn = 3,\n2: Output: FP q Quantized model\n3: for l in 1st to Lth layer in M do\n4:\nForward & collect raw output Ol = XlY l of layer l;\n5: end for\n6: for l in 1st to Lth layer in M do\n7:\nInitialize the FP format search space w.r.t Xl and Y l\nas RX = {r1\nX, r2\nX, ..., rt\nX} and RY = {r1\nY, r2\nY, ....rt\nY}.\n8:\nInitialize bias \u02dcbi\nX,\u02dcbi\nY with Eq.7 for each format can-\ndidate ri\nX \u2208 RX and ri\nY \u2208 RY.\n9:\nGenerate search space of \u02dcbX in t formats to be\n[\u03b31\u02dcbinit\nX\n, \u03b32\u02dcbinit\nX\n] and \u02dcbY to be [\u03b31\u02dcbinit\nY\n, \u03b32\u02dcbinit\nY\n].\n10:\nfor 0 to n do\n11:\nSearch for \u02dcbi\nX w.r.t each ri\nX that minimizes Eq.12\n12:\nSearch for ri\nX \u2208 RX that minimizes Eq.12\n13:\nSearch for \u02dcbi\nY w.r.t each ri\nY that minimizes Eq.12\n14:\nSearch for ri\nY \u2208 RY that minimizes Eq.12\n15:\nend for\n16: end for\nHowever, applying per-channel scaling factors\nfor activations poses challenges to efficient matrix\nmultiplication, because the scaling factor is not\na shared constant along the multiplication direc-\ntion and cannot be extracted as Eq. 10. To address\nthis challenge, we introduce pre-shifted exponent\nbias, which allows us to calculate per-channel scal-\ning factors from activations. These scaling factors\nare then re-parameterized as the exponent biases\nof the corresponding weights. This method effec-\ntively handles high inter-channel variance while\nmaintaining nearly identical efficiency to per-tensor\nquantization.\nRecalling in Eq. 7, we extracted the tensor-wise\ninteger exponent bias b and times it with real-\nvalued scaling factor \u03b1 and becomes a new scaling\nfactor \u02dc\u03b1 = 2\u2212\u02dcb = 2\u2212b \u00b7 \u03b1. Then, the floating-point\nquantization formula in Eq. 13 becomes:\nXFP =2\u2212\u02dcb(\u22121)s2p\u22120(1+d1\n2 +d2\n22 +...+dm\n2m ) (13)\nWe note that after the bias is absorbed in the scal-\ning factor, the original bias term (bori) in the FP\nformula is always zero. In dealing with the inter-\nchannel variance, we devise an innovative usage\nof this integer exponent bias: we set it to be a per-\nchannel variant (bori \u2208 Zc).\nThen the calculation of the channel-wise integer\nbias vector (bori) is very straightforward. We first\ncalculate the initial per-channel real-valued scal-\ning factor (2\u2212\u02dcbj) from the per-channel maximum\nFigure 2: Magnitude of the output activations of the\nfeed-forward network blocks in LLaMA-7B, BERT, and\nDeiT.\nvalues:\n\u02dcbj =2e\u2212log2(max(|X:,j\nR |))+log2(2\u22122\u2212m)\u22121 (14)\nHere X:,j\nR denotes the jth channel in the activation\nmatrix. Then we separate \u02dcb to a tensor-wise real-\nvalued scaling factor plus a channel-wise integer\nscaling factor:\n\u02dcb = \u02dc\u03c1 + bori\n= \u02dc\u03c1 + clip(\u230a\u02dcb \u2212 \u02dc\u03c1\u2309, 0, 2e\u22121)\n(15)\nwhere \u02dc\u03c1 \u2208 R1, bori \u2208 Zc. Then the formula for\none of the entries in the jth channel of X can be\nrewrote as follows:\nXFP =2\u2212\u02dcbj(\u22121)s2p\u22120(1 + d1\n2 + ... + dm\n2m )\n=2\u2212\u02dc\u03c1(\u22121)s2p\u2212bori\nj (1 + d1\n2 + ... + dm\n2m )\n(16)\nNote that the bias bori is constrained to integers\nwithin [0, 2e \u2212 1], compatible with the standard\nfloating-point number calculation. Nevertheless,\nadding different biases for each channel during\ninference may still cause some extra hardware\noperations. Thus, we re-parameterized the per-\nchannel activation bias into a weight tensor and\npre-computed the weights using the calibration set.\nThis way, the exponent biases shifting only hap-\npens in the calibration stage. Then, an element in\njth channel of activation tensors X becomes:\nXFP =2\u2212\u02dc\u03c1(\u22121)s2p\u22120(1+ d1\n2 +...+ dm\n2m )\n(17)\n!\"!,!\n!\"!,#\n!\"#,!\n!\"#,#\n#$!\n#$#\n!\n%!,!\n!\n%!,#\n!\n%#,!\n!\n%#,#\n#$!\n$\n#$#\n$\n!\"!,!\n!\"!,#\n!\"#,!\n!\"#,#\n&\n+\n#$!\n%&'\n&\n+\n#$#\n%&'\n!\n%!,!\n!\n%!,#\n!\n%#,!\n!\n%#,#\n#$!\n$\n#$#\n$\n#$!\n%&'\n#$#\n%&'\n!\"!,!\n!\"!,#\n!\"#,!\n!\"#,#\n&\nDecompose\n#$ (Eq. 15)\nPre-shifted exponent bias\n#$!\n%&' (Eq. 18)\nTensor-wise\nscaling factor\n& (Eq. 17)\n'((\n!\n%\u2032!,!\n!\n%\u2032#,!\n!\"!,!\n!\"!,#\n'($\n!\nEfficient matrix multiplication\n(Eq. 22)\n*\n*\n!\n%!,!\n!\n%!,#\n!\n%#,!\n!\n%#,#\n#$!\n%&'\n#$#\n%&'\n!\n%\u2032!,!\n!\n%\u2032!,#\n!\n%\u2032#,!\n!\n%\u2032#,#\nPre-compute weights to store !\"!\n\"#$ in low-bit FP\nformat\n#$-\n%&' \u2208 \u2124!\n#$- \u2208 \u211d!\n'(( = 2)* '($\n! = 2)+,!\" \n#$.\n$ \u2208 \u211d!\n&\n\u2208 \u211d!\n(a)\n(b)\n(c)\nFigure 3: Overview of pre-shifted exponent bias method: (a) Search phase: The real-valued channel-wise\nscaling exponent bias for activations (\u02dcbj) is partitioned into a real-valued tensor-wise exponent bias (\u03c1), and the\ninteger-based channel-wise exponent bias (\u02dcbori\nj ). (b) Reparameterization and weight pre-computation: Once the\noptimal values are determined on the calibration set, \u02dcbori\nj\nare re-parameterized into the weight tensor. The weights\nare pre-computed to apply the bias, therefore this is a one-time cost. (c) Inference phase: The method leverages\nefficient matrix multiplication between low-bit floating-point matrices.\nand the corresponding weight element in jth row\nof the weight tensor W becomes:\nWFP =2\u2212\u02dcbW(\u22121)s2p\u2212bori\nj (1+ d1\n2 +...+ dm\n2m ) (18)\nAs result, efficient matrix multiplication in Eq.10\nis reformulated as:\nOi,k\nout =Xi,:\nFPW:,k\nFP = \u02dc\u03b1X \u02dc\u03b1k\nW \u02dcXi,:\nFP(\u03b2\u2299 \u02dc\nW:,k\nFP) (19)\nwhere \u2299 is the element-wise multiplication, \u03b2 =\n2\u2212bori and (\u03b2 \u2299 \u02dc\nW:,k\nFP) can be pre-calculated\nand stored in low-bit FP format. We depict the\noverall pre-shifted exponent bias method in Fig.3.\nThis method applies to quantizing all the fully-\nconnected layers. During the search process, we\ninitialize \u02dc\u03c1X as the minj(\u02dcbj). Then, we fixed \u02dcbX\nto be the bias calculated from the Eq. 14 and search\nfor the optimal \u02dc\u03c1X from [\u03b31 \u02dc\u03c1 init\nX\n, \u03b32 \u02dc\u03c1 init\nX\n].\nCombining pre-shifted exponent bias method\nwith the joint format and max-value search\nframework(FPQ baseline), we name our method\nas (FPQ), short for Floating Point Quantization.\n5\nExperiments\nTo validate the effectiveness of the proposed\nmethod, we conduct experiments on LLaMA (Tou-\nvron et al., 2023) and BERT (Devlin et al., 2019)\nmodels in 5.2.1 and Sections 5.2.2. Further, in\nSection 5.2.3 we show that our method also gen-\neralizes well to vision transformer architectures.\nWe present ablation studies on the calibration size\nand search range in Section 5.3, and analyze the\nhardware costs of implementing FP operators in\nSection 5.4.\n5.1\nExperiments Details\nWe adopt per-tensor quantization for activation and\nper-channel quantization for weight. We employ\nlayer reconstruction following the settings of (Yuan\net al., 2022; Nagel et al., 2020), and parallel quanti-\nzation based on the approach outlined in (Bai et al.,\n2022; Yuan et al., 2022). A more detailed discus-\nsion regarding our implementation decisions can\nbe found in Appendix F. For LLaMA models, we\nquantize all the weight and activation tensors in\nfully-connected layers for a fair comparison with\nprevious work (Xiao et al., 2022; Liu et al., 2023).\nFor BERT and ViT models, both fully-connected\nlayers and activation-activation multiplication ten-\nsors in the self-attention module are quantized.\nNote that for FPQ on BERT (Devlin et al., 2019)\nand ViTs models, the reconstruction metric Eq. 12\nis substituted with a Hessian approximation loss\nmetric. This substitution is further detailed in Ap-\npendix A.\n5.2\nMain Results\n5.2.1\nLLM Zero-Shot Reasoning\nWe evaluate the effectiveness of FPQ for LLaMA-\n7B/ LLaMA-13B (Touvron et al., 2023) on com-\nmon sense zero-shot reasoning tasks. For the cali-\nbration data, we sample 32 random segments with\n2048 tokens length from the C4 (Raffel et al., 2020)\nQuant Method\n#Bits (E/W/A) # Calib BoolQ PIQA HellaSwag WinoGrande ARC-e ARC-c Avg.\nLLaMA-7B Full-precision\n16/16/16\n-\n75.1\n78.7\n56.9\n69.9\n75.3\n41.9\n66.3\nMinMax INT Quant\n8/8/8\n32\n64.3\n66.8\n40.5\n57.4\n59.0\n29.6\n52.9\nMinMax FP Quant (E4M3)\n8/8/8\n32\n74.9\n78.6\n56.8\n69.5\n75.5\n41.6\n66.1\nSmoothQuant (Xiao et al., 2022)\n16/8/8\n512\n74.0\n77.5\n55.0\n69.6\n74.4\n37.4\n64.6\nFPQ baseline\n8/8/8\n32\n75.8\n78.3\n55.9\n69.5\n75.6\n41.3\n66.1\nFPQ\n8/8/8\n32\n75.6\n78.2\n56.6\n70.2\n74.6\n40.7\n66.0\nMinMax INT Quant\n4/4/16\n32\n64.1\n76.1\n51.6\n66.3\n72.4\n40.0\n61.7\nMinMax FP Quant (E2M1)\n4/4/16\n32\n73.0\n77.9\n55.2\n69.1\n73.6\n40.9\n64.9\nGPTQ (Frantar et al., 2023)\n4/4/16\n128\n73.3\n77.9\n54.9\n67.9\n72.7\n37.4\n64.0\nFPQ baseline\n4/4/16\n32\n74.8\n77.9\n55.6\n69.5\n75.2\n41.0\n65.7\nFPQ\n4/4/16\n32\n74.2\n77.8\n55.8\n69.9\n74.9\n40.4\n65.5\nMinMax INT Quant\n4/4/8\n32\n50.4\n56.5\n27.9\n46.5\n36.1\n21.2\n39.7\nMinMax FP Quant (E2M1/E4M3)\n4/4/8\n32\n73.0\n77.5\n55.0\n69.3\n73.6\n40.9\n64.9\nFPQ baseline\n4/4/8\n32\n75.0\n77.6\n55.9\n69.9\n74.3\n39.4\n65.3\nFPQ\n4/4/8\n32\n75.0\n77.7\n55.5\n69.8\n74.5\n39.9\n65.4\nMinMax INT Quant\n4/4/4\n32\n54.1\n51.7\n25.6\n49.8\n24.7\n22.9\n38.1\nMinMax FP Quant (E2M1)\n4/4/4\n32\n47.3\n53.1\n25.7\n50.7\n25.1\n22.4\n37.4\nSmoothQuant (Xiao et al., 2022)\n16/4/4\n512\n54.1\n62.8\n41.5\n52.6\n50.6\n32.9\n49.1\nLLM-QAT (Liu et al., 2023)\n16/4/4\n(QAT)\n63.5\n64.3\n55.6\n52.9\n50.3\n30.2\n52.8\nFPQ baseline\n4/4/4\n32\n57.4\n56.6\n30.2\n51.1\n37.7\n23.2\n42.7\nFPQ\n4/4/4\n32\n64.2\n73.5\n47.8\n63.7\n65.9\n33.6\n58.1\nLLaMA-13B Full-precision\n16/16/16\n-\n77.9\n79.2\n59.9\n72.6\n77.4\n46.4\n68.9\nMinMax INT Quant\n8/8/8\n32\n60.6\n69.6\n46.0\n61.5\n63.3\n32.8\n55.6\nMinMax FP Quant (E4M3)\n8/8/8\n32\n78.0\n79.1\n60.0\n72.3\n77.2\n47.1\n68.9\nSmoothQuant (Xiao et al., 2022)\n16/8/8\n512\n76.5\n78.0\n58.0\n72.1\n76.3\n45.5\n68.2\nFPQ baseline\n8/8/8\n32\n78.0\n79.1\n59.9\n72.3\n77.2\n47.1\n68.9\nFPQ\n8/8/8\n32\n78.1\n78.5\n59.1\n72.4\n76.4\n46.1\n68.4\nMinMax INT Quant\n4/4/8\n32\n52.1\n65.0\n36.4\n53.9\n52.3\n29.0\n48.1\nMinMax FP Quant (E2M1/E4M3)\n4/4/8\n32\n78.0\n78.9\n58.0\n71.6\n76.0\n44.8\n67.9\nFPQ baseline\n4/4/8\n32\n76.2\n78.2\n57.9\n71.9\n75.1\n43.9\n67.2\nFPQ\n4/4/8\n32\n76.4\n78.5\n58.2\n72.1\n75.2\n44.7\n67.5\nMinMax INT Quant\n4/4/4\n32\n54.5\n52.7\n25.5\n51.1\n25.3\n22.1\n38.5\nMinMax FP Quant (E2M1)\n4/4/4\n32\n45.8\n51.7\n25.5\n49.5\n25.0\n22.8\n36.7\nSmoothQuant (Xiao et al., 2022)\n16/4/4\n512\n57.6\n61.3\n56.0\n52.6\n49.9\n25.1\n50.4\nFPQ baseline\n4/4/4\n32\n54.3\n57.7\n35.7\n52.2\n41.1\n25.7\n44.5\nFPQ\n4/4/4\n32\n71.9\n74.8\n53.3\n66.7\n71.7\n39.9\n63.1\nTable 1: Zero-shot performance on common sense reasoning tasks with LLaMA (Touvron et al., 2023) models. We\ndenote E/W/A as the bit-width of word embeddings, model weight and activations, respectively.\ndataset following the setting of GPTQ (Frantar\net al., 2023). The data preprocessing and score\ncalculation are based on EleutherAI evaluation har-\nness1. In Table 1, we compare FPQ to the floating-\npoint PTQ baselines, and state-of-the-art PTQ and\nQAT methods, including SmoothQuant (Xiao et al.,\n2022) and GPTQ (Frantar et al., 2023), and LLM-\nQAT (Liu et al., 2023).\nIn general, all methods, except for the na\u00efve Min-\nMax INT Quantization, produce comparable out-\ncomes in the 8-bit setting on both LLaMA-7B and\nLLaMA-13B. Additionally, we observe that the\nna\u00efve MinMax FP Quantization achieves nearly\nlossless results and even surpasses the state-of-\nthe-art integer post-training quantization method,\nSmoothQuant (Xiao et al., 2022), which indicates\nthat floating-point quantization naturally has a\nstrong capability in handling the distributions in\ntransformers. However, both MinMax FP Quant\nand FPQ baseline fail when pushing the quan-\n1https://github.com/EleutherAI/lm-evaluation-harness\ntization precision to ultra-low 4/4/4 bit setting,\nwith 28.9% and 23.8% accuracy degradation on\nLLaMA-7B, respectively. In this extreme case,\nthe previous state-of-the-art PTQ and QAT meth-\nods, SmoothQuant (Xiao et al., 2022) and LLM-\nQAT (Liu et al., 2023) also suffer severe accu-\nracy downgrade. In comparison, FPQ demonstrates\na strong capability of handling extra-low bit set-\ntings and achieves only 8.2/5.8% accuracy drop on\nLLaMA-7B/13B with 4/4/4 bit-width, outperform-\ning SmoothQuant (Xiao et al., 2022) by a large\nmargin, yet with less bit-width and smaller calibra-\ntion size. Moreover, FPQ even achieves 5.3% accu-\nracy improvements compared to LLM-QAT (Liu\net al., 2023) in the 4/4/4 setting and 1.5% over\nGPTQ (Frantar et al., 2023) in the 4/4/16 configu-\nration on LLaMA-7B.\nFor practitioners, a crucial consideration is\ndetermining the appropriate quantization meth-\nods for various bit-widths.\nTherefore, based\non our findings, we offer two recommendations\nthat balance the trade-off between accuracy and\nQuant Method\n#Bits (E/W/A) # Calib MNLI\u2212m QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.\n(Full-precision)\n32-32-32\n-\n84.9\n91.4\n92.1\n93.2\n59.7\n90.1\n86.3\n72.2\n83.7\nMinMax INT Quant\n8/8/8\n128\n77.0\n89.9\n88.9\n92.9\n51.8\n88.2\n83.8\n71.5\n80.5\nMinMax FP Quant (E2M5)\n8/8/8\n128\n78.9\n90.8\n88.6\n92.9\n52.7\n88.4\n84.3\n69.0\n80.7\nMinMax FP Quant (E3M4)\n8/8/8\n128\n84.5\n90.9\n91.5\n93.2\n58.3\n89.3\n87.7\n71.8\n83.4\nMinMax FP Quant (E4M3)\n8/8/8\n128\n84.7\n90.9\n91.7\n93.0\n58.6\n89.3\n86.5\n72.2\n83.4\nMinMax FP Quant (E5M2)\n8/8/8\n128\n84.1\n90.9\n91.4\n93.6\n58.1\n89.2\n87.5\n71.8\n83.3\nFPQ baseline\n8/8/8\n128\n84.6\n90.9\n91.7\n93.1\n58.6\n89.3\n88.0\n72.2\n83.5\nFPQ\n8/8/8\n128\n84.6\n91.0\n91.6\n93.3\n58.8\n89.3\n88.0\n72.2\n83.6\nMinMax INT Quant\n6/6/6\n128\n31.9\n62.0\n52.8\n58.8\n0.0\n12.7\n32.1\n52.7\n37.9\nMinMax FP Quant (E2M3)\n6/6/6\n128\n43.5\n85.4\n79.4\n90.5\n45.2\n86.0\n66.9\n59.9\n69.6\nMinMax FP Quant (E3M2)\n6/6/6\n128\n83.9\n90.8\n90.8\n92.2\n58.2\n88.6\n87.0\n72.2\n83.0\nMinMax FP Quant (E4M1)\n6/6/6\n128\n84.4\n90.2\n90.1\n92.2\n58.2\n89.2\n85.3\n69.7\n82.4\nFPQ baseline\n6/6/6\n128\n84.6\n90.9\n91.2\n93.2\n58.8\n88.7\n87.5\n70.8\n83.2\nFPQ\n6/6/6\n128\n84.5\n90.8\n91.6\n93.1\n57.3\n89.3\n88.7\n71.8\n83.2\nMinMax INT Quant\n4/4/8\n128\n33.1\n63.8\n60.1\n49.3\n0.0\n44.0\n50.2\n49.1\n43.7\nMinMax FP Quant (E2M1)\n4/4/8\n128\n60.6\n70.9\n77.4\n79.9\n5.5\n78.6\n46.8\n56.6\n59.5\nMREM-S (Bai et al., 2022)\n4/4/8\n4096\n83.5\n90.2\n91.2\n91.4\n55.1\n89.1\n84.8\n71.8\n82.1\nMREM-P (Bai et al., 2022)\n4/4/8\n4096\n83.4\n90.2\n91.0\n91.5\n54.7\n89.1\n86.3\n71.1\n82.2\nFPQ baseline\n4/4/8\n128\n84.4\n90.6\n91.4\n92.9\n58.6\n83.7\n88.2\n73.3\n82.9\nFPQ\n4/4/8\n128\n84.5\n90.6\n91.1\n92.7\n58.8\n89.3\n88.7\n73.3\n83.6\nMinMax INT Quant\n4/4/4\n128\n31.8\n39.7\n50.5\n49.1\n0.0\n6.7\n31.6\n54.5\n32.9\nMinMax FP Quant (E2M1)\n4/4/4\n128\n33.6\n54.0\n50.6\n50.8\n0.0\n0.0\n31.6\n52.0\n34.1\nBrecQ (Li et al., 2021)\n8/4/4\n4096\n31.9\n62.3\n50.7\n50.9\n0.9\n6.4\n31.7\n52.3\n35.8\nQDrop (Wei et al., 2022)\n8/4/4\n4096\n71.4\n79.0\n76.8\n88.1\n40.9\n81.9\n79.2\n60.7\n72.3\nFPQ baseline\n4/4/4\n128\n38.9\n68.3\n55.3\n83.6\n10.6\n0.0\n43.8\n55.2\n44.5\nFPQ\n4/4/4\n128\n82.3\n89.2\n86.6\n91.5\n52.6\n85.5\n83.8\n69.0\n80.1\nTable 2: Results on the GLUE development set with BERT (Bai et al., 2022) model. We denote E/W/A as the\nbit-width of word embeddings, model weight and activations, respectively.\nsearch/optimization efficiency. First of all, since\nthe difference between MinMax FP Quant and the\nrest of the methods is marginal for the 8/8/8 set-\nting, we recommend simply using the MinMax FP\nQuant method for the 8/8/8 setting as the MinMax\nmethod does not involve search process. However,\nfor more demanding scenarios, especially with ac-\ntivation quantization to 4 bits, we recommend em-\nploying FPQ for minimizing accuracy degradation\nwith negligible inference overhead.\n5.2.2\nBERT Model\nWe evaluate the proposed quantization techniques\nfor BERT model on GLUE tasks (Wang et al.,\n2019).\nFull-precision BERT-base models fine-\ntuned on GLUE datasets are obtained from Hug-\ngingface public repository2. We randomly sample\n128 data from the training set as the calibration set.\nIn Table 2, FPQ demonstrates remarkable perfor-\nmance, achieving absolute average accuracy im-\nprovements of 44.3% compared to BrecQ (Li et al.,\n2021) and 7.9% over QDrop (Wei et al., 2022)\nwith 4/4/4 bit setting. Further, with 4-bit weight\nand 8-bit activation, MREM-S/MREM-P (Bai et al.,\n2022) present a 1.6/1.5% accuracy gap to the full-\nprecision model with 4096 calibration data, while\nFPQ achieves almost no accuracy loss with only\n2https://huggingface.co/textattack/bert-base-uncased-\n{TASK_NAME}\n128 calibration data points.\n5.2.3\nGeneralizability on Vision Transformer\nBased on our findings that vision transformers also\nexhibit a consistent activation distribution pattern\nas language transformers, characterized by high\ninter-channel variance and low intra-channel vari-\nance, as detailed in Fig. 2, we extended our pro-\nposed methods to ViT and compared FPQ with\nfloating-point PTQ baselines and state-of-the-art\nPTQ method for ViT on the ImageNet classifica-\ntion task. Table 3 shows that findings on ViT are\nconsistent with that on language models: previous\nstate-of-the-art integer-based methods struggled to\nmaintain reasonable accuracy when quantizing the\ntransformer to lower bits. In comparison, the pro-\nposed FPQ outperformed both PTQ4ViT and APQ-\nViT on 6 bits, and also achieved 40.9% and 31.5%\nabsolute accuracy improvement over PTQ4ViT and\nAPQ-ViT on DeiT-S in the 4-bit configuration.\n5.3\nAblation Study\nIn this section, we first compare the influence of dif-\nferent calibration sizes on FPQ. We vary the calibra-\ntion size in {32, 64, 128, 256} and test on MNLI,\nQQP, and CoLA. Table 4 shows that the evalua-\ntion on MNLI and QQP is more robust to different\nsettings, and the variance is more significant on\nCoLA. We observe that FPQ performs well with a\nW/A\nQuant Method\nDeit-S Deit-B ViT-S\nFull-prec\n-\n79.9\n81.8\n81.4\n6/6\nPTQ4ViT(Yuan et al., 2022)\n76.3\n80.3\n78.6\n6/6\nAPQ-ViT(Ding et al., 2022)\n77.8\n80.4\n79.2\n6/6\nMinMax FP Quant (E3M2)\n79.3\n81.7\n80.7\n6/6\nFPQ baseline\n79.43\n81.7\n80.9\n6/6\nFPQ\n79.5\n81.8\n81.1\n4/4\nPTQ4ViT(Yuan et al., 2022)\n34.1\n64.4\n42.6\n4/4\nAPQ-ViT (Ding et al., 2022)\n43.6\n67.5\n48.0\n4/4\nMinMax FP Quant (E2M1)\n0.4\n0.1\n0.1\n4/4\nFPQ baseline\n6.57\n0.71\n0.3\n4/4\nFPQ\n75.0\n79.4\n73.2\nTable 3: Comparison on the ImageNet dataset with\nvision transformer structures.\nE/W/A #Calib MNLI-M QQP CoLA\n4/4/4\n32\n81.5\n89.4\n44.4\n4/4/4\n64\n81.8\n89.4\n47.9\n4/4/4\n128\n82.3\n89.2\n52.6\n4/4/4\n256\n81.9\n89.0\n52.9\n6/6/6\n32\n84.8\n90.8\n55.0\n6/6/6\n64\n84.7\n90.9\n58.2\n6/6/6\n128\n84.5\n90.8\n57.3\n6/6/6\n256\n84.6\n90.8\n57.6\nTable 4: Ablation studies of different calibration sizes.\ncalibration set size of 128 data points. However,\nwe also find that it remains robust and maintains\ncompetitive accuracy even with limited access to\ncalibration data, such as when using as few as 32\ndata points.\nWe investigate the robustness of FPQ to dif-\nferent search ranges (\u03b31, \u03b32). Table 5 presents\nthe results of FPQ using three sets of (\u03b31, \u03b32):\n(0.01, 1.2), (0.1, 1.2), (0.5, 1.5), on MNLI, QQP,\nand CoLA. It is observed that no single search\nrange outperforms the others consistently across\nall tasks. For instance, the search range (0.01, 1.2)\nperforms better than (0.5, 1.5) on MNLI and QQP,\nbut slightly worse on CoLA in the 4-bit configu-\nration. Overall, FPQ exhibits robustness to various\n\u03b31 and \u03b32, as long as the search range is not overly\naggressive.\n5.4\nHardware Cost\nWe further examine the hardware utilization of low-\nbit INT, FP, and mixed-format FP multiplication\noperators, including adder, multiplier, and multiply-\naccumulate (MAC) units, in terms of hardware area.\nMixed-format FP refers to the multiplication of\nfloating-point numbers with different formats, e.g.,\nE2M1 multiplies with E1M2. We implemented\nthe MAC operator by Verilog HDL and utilized\nCadence Genus to obtain the synthesized area un-\nder TSMC 40nm technology and 0.5GHz clock\nfrequency.\nTable 6 illustrates the hardware cost of the INT\nand FP operators, with the multiplier being the pri-\nE/W/A\n\u03b31, \u03b32\nMNLI-M QQP CoLA\n4/4/4\n0.01, 1.2\n82.3\n89.2\n52.6\n4/4/4\n0.1, 1.2\n82.2\n89.1\n53.6\n4/4/4\n0.5, 1.5\n82.3\n88.4\n52.8\n6/6/6\n0.01, 1.2\n84.5\n90.8\n57.3\n6/6/6\n0.1,1.2\n84.7\n90.8\n57.5\n6/6/6\n0.5,1.5\n84.7\n90.8\n57.8\nTable 5: Ablation studies of different search range.\nFormat\nAdder(\u00b5m2) Multiplier(\u00b5m2) MAC(\u00b5m2)\nINT4\n93\n182\n410\nINT6\n132\n340\n529\nE2M1\n111\n92\n443\nE3M2\n223\n138\n498\nE2M1 * E1M2\n105\n107\n432\nTable 6: Area differences of INT, FP and mixed Format\nFP operators across different bit-widths.\nmary cost for INT and the adder for FP. Notably, the\ndisparity between FP4 and INT4 adders is small,\nwhile INT has twice the hardware cost for the mul-\ntiplier. Moreover, the mixed-format FP4 operator\nhas comparable hardware area as the standard FP4\noperator. These findings indicate that the proposed\nFPQ approach imposes negligible overhead in terms\nof hardware implementation when compared to the\nstandard FP operators and the hardware cost for FP\nis comparable with INT.\n6\nConclusion\nThis paper presents the first successful demonstra-\ntion of 4-bit floating-point post-training quantiza-\ntion for weights, activations, and embeddings in\nnatural language transformer architectures, includ-\ning both large language models and BERT model.\nWe also extend our method to vision transform-\ners and observe its robust generalization ability.\nOur approach involves a practical search-based\ntechnique which establishes a strong baseline and\nachieves state-of-the-art results for 6-bit and 8-bit\nquantization. Furthermore, we address the chal-\nlenge of high inter-channel variance in transform-\ners by proposing pre-shifted exponent bias, which\nproves highly effective in achieving accurate 4-bit\nquantization.\nAcknowledgement\nThis research is supported by National Natural\nScience Foundation of China/ HKSAR Research\nGrants Council Joint Research Scheme under Grant\nNHKUST627/20, and Foshan HKUST Projects\nunder Grant FSUST21 \u2212 HKUST10E.\nLimitations\nOur experiments were conducted on publicly avail-\nable datasets with finite sentence lengths, and the\ngeneralizability of our method to extremely long\nsequences or streaming data has not been verified\nand may require further investigation. In addition,\nit remains to be seen how our proposed method can\ngeneralize to other domains beyond language and\nvision, such as audio. It would also be interesting\nto see the applicability of our method to generative\ntasks and other applications.\nReferences\nHassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong\nChuang, Shih-Fu Chang, Yin Cui, and Boqing Gong.\n2021.\nVatt:\nTransformers for multimodal self-\nsupervised learning from raw video, audio and text.\nAdvances in Neural Information Processing Systems,\n34:24206\u201324221.\nHaoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin\nKing, and Michael Lyu. 2022. Towards efficient post-\ntraining quantization of pre-trained language models.\nIn Advances in Neural Information Processing Sys-\ntems.\nYelysei Bondarenko, Markus Nagel, and Tijmen\nBlankevoort. 2021. Understanding and overcoming\nthe challenges of efficient transformer quantization.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nYaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami,\nMichael W Mahoney, and Kurt Keutzer. 2020. Zeroq:\nA novel zero shot quantization framework. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 13169\u201313178.\nYoni Choukroun, Eli Kravchik, Fan Yang, and Pavel\nKisilev. 2019. Low-bit quantization of neural net-\nworks for efficient inference.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. Llm.int8(): 8-bit matrix multipli-\ncation for transformers at scale. Advances in Neural\nInformation Processing Systems, 35:30318\u201330332.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nYifu Ding, Haotong Qin, Qinghua Yan, Zhenhua Chai,\nJunjie Liu, Xiaolin Wei, and Xianglong Liu. 2022.\nTowards accurate post-training quantization for vi-\nsion transformer. In Proceedings of the 30th ACM\nInternational Conference on Multimedia, MM \u201922,\npage 5380\u20135388, New York, NY, USA. Association\nfor Computing Machinery.\nAlexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk Weissenborn,\nXiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on\nLearning Representations.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2023. GPTQ: Accurate post-training\ncompression for generative pretrained transformers.\nIn International Conference on Learning Representa-\ntions.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\narXiv\npreprint arXiv:2001.08361.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nProceedings of NAACL-HLT, pages 4171\u20134186.\nAndrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus\nNagel, Jorn Peters, and Tijmen Blankevoort. 2022.\nFp8 quantization: The power of the exponent. Ad-\nvances in Neural Information Processing Systems,\n35:14651\u201314662.\nJemin Lee, Yongin Kwon, Jeman Park, Misun Yu, and\nHwanjun Song. 2023. Q-hyvit: Post-training quan-\ntization for hybrid vision transformer with bridge\nblock reconstruction.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7871\u20137880.\nYuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng\nHu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi\nGu. 2021. Brecq: Pushing the limit of post-training\nquantization by block reconstruction. arXiv preprint\narXiv:2102.05426.\nZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie\nChang, Pierre Stock, Yashar Mehdad, Yangyang\nShi, Raghuraman Krishnamoorthi, and Vikas Chan-\ndra. 2023. Llm-qat: Data-free quantization aware\ntraining for large language models. arXiv preprint\narXiv:2305.17888.\nZhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei\nMa, and Wen Gao. 2021. Post-training quantization\nfor vision transformer. Advances in Neural Informa-\ntion Processing Systems, 34:28092\u201328103.\nPaulius Micikevicius, Dusan Stosic, Neil Burgess, Mar-\nius Cornea, Pradeep Dubey, Richard Grisenthwaite,\nSangwon Ha, Alexander Heinecke, Patrick Judd,\nJohn Kamalu, Naveen Mellempudi, Stuart Oberman,\nMohammad Shoeybi, Michael Siu, and Hao Wu.\n2022. Fp8 formats for deep learning.\nMarkus Nagel, Rana Ali Amjad, Mart Van Baalen,\nChristos Louizos, and Tijmen Blankevoort. 2020. Up\nor down? adaptive rounding for post-training quan-\ntization. In International Conference on Machine\nLearning, pages 7197\u20137206. PMLR.\nMarkus Nagel, Mart van Baalen, Tijmen Blankevoort,\nand Max Welling. 2019.\nData-free quantization\nthrough weight equalization and bias correction.\nOpenAI. 2023.\nGpt-4 technical report.\nArXiv,\nabs/2303.08774.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9-\ngou. 2021. Training data-efficient image transform-\ners & distillation through attention. In International\nconference on machine learning, pages 10347\u201310357.\nPMLR.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding.\nXiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu,\nand Fengwei Yu. 2022. QDrop: Randomly dropping\nquantization for extremely low-bit post-training quan-\ntization. In International Conference on Learning\nRepresentations.\nDi Wu, Qi Tang, Yongle Zhao, Ming Zhang, Ying Fu,\nand Debing Zhang. 2020. Easyquant: Post-training\nquantization via scale optimization.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien De-\nmouth, and Song Han. 2022. Smoothquant: Accurate\nand efficient post-training quantization for large lan-\nguage models. arXiv preprint arXiv:2211.10438.\nZhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu,\nand Guangyu Sun. 2022.\nPtq4vit: Post-training\nquantization for vision transformers with twin uni-\nform quantization. In Computer Vision\u2013ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October\n23\u201327, 2022, Proceedings, Part XII, pages 191\u2013207.\nSpringer.\nYijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang,\nTing Cao, Fan Yang, Mao Yang, Shanghang Zhang,\nand Ningyi Xu. 2023. Integer or floating point? new\noutlooks for low-bit quantization on large language\nmodels.\nA\nHessian-Based Loss Metric\nThe objective of post-training quantization is to\nminimize the perturbation (\u03b4X = XFP \u2212 XR)\nintroduced by quantization to the pre-trained real-\nvalued network:\nmin E[L(XR + \u03b4X) \u2212 L(XR)]\n(20)\nFollowing the Taylor series expansion, we have\nE[L(XR + \u03b4X) \u2212 L(XR)]\n\u2248 \u03b4XT \u00afg(X) + 1\n2\u03b4XT \u00afH(X)\u03b4X\n\u2248 1\n2\u03b4XT \u00afH(X)\u03b4X\n(21)\nHere, \u00afg(X) is the gradients and \u00afH(X) is the Hes-\nsian matrix. Since the pre-trained model is well-\nconverged, we can assume that \u00afg(X) has near zero\nvalue in every element, and thus term \u03b4XT \u00afg(X)\ncan be neglected.\nThe Hessian matrix \u00afH(X) is computed as:\n\u00afH(X) = JT\nO(X) \u00afH(O)JO(X)\n(22)\nwhere JO(X) denotes the Jacobian matrix of the\nlayer output O w.r.t X, and \u00afH(O) is the Hessian\nmatrix w.r.t O. We then substitute the above equa-\ntion back to equation 21 :\n\u03b4XT \u00afH(X)\u03b4X\n= (JO(X)\u03b4X)T \u00afH(O)(JO(X)\u03b4X)\n\u2248 ( \u02c6O \u2212 O)T \u00afH(O)( \u02c6O \u2212 O)\n(23)\nHere \u02c6O is the intermediate output of the quantized\nlayer and O is the original layer output. Note that\nunder the assumption that \u03b4X is relatively small (Li\net al., 2021), we can approximate ( \u02c6O \u2212 O) as\nJO(X)\u03b4X using first-order Taylor expansion.\nNevertheless, the calculation of \u00afH(O) is still bur-\ndensome, therefore, we use the diagonal entries of\nthe Fisher Information Matrix of O to substitute\n\u00afH(O) following (Li et al., 2021; Yuan et al., 2022),\nand the new Hessian-based metric becomes:\nE[( \u02c6O \u2212 O)T diag(( \u2202L\n\u2202O1\n)2, ..., ( \u2202L\n\u2202On\n)2( \u02c6O \u2212 O)]\n(24)\nHere, each entry of O is assumed to be independent\nand n denoted the total number of elements in O.\nIn this study, this hessian-based metric is used as\nthe reconstruction metric to search for the optimal\nFP quantization function for both the weight and ac-\ntivation when performing layer-wise reconstruction\nin BERT and Vision Transformer models.\nB\nQuantization Error of Different\nFloating-Point Formats\nFigure 4 compares the quantization error of differ-\nent formats in 8-bit quantization, including INT8,\nE2M5, E3M4, E4M3, and E5M2. We apply these\nformats to different BERT modules in the first, fifth,\nand last layers. The figures demonstrate that the op-\ntimal FP formats differs depending on the specific\nmodule that we are quantizing.\nC\nInter-Channel Variance Visualization\nFigure 5 and 6 depict the output of different fully-\nconnected layers in BERT for the MNLI task, DeiT-\nS for the ImageNet-1K task, and LLaMA-7B for\nthe zero-shot reasoning task. The visualizations\nreveal a noticeable inter-channel variance presented\nin both language and vision transformers.\nD\nEfficient Matrix Multiplication\nFigure 7 displays a comprehensive list of all the\ngranularity options that allow for efficient ma-\ntrix multiplication. While per-token quantization\ntheoretically provides greater precision in terms\nof quantization granularity, the accuracy gains\nachieved through this method are minimal and do\nnot justify the additional computational overhead\nrequired. As a result, we have opted to use per-\ntensor quantization when quantizing activations.\nE\nLearning Format and Maximum Value\nWe\ncompare\nthe\nprevious\ngradient-based\nmethod (Kuzmin et al., 2022) with the proposed\nsearch-based method for finding the optimal\nformat and maximum value.\nOn DeiT-S, the\nlearnable method only achieves 74.38% accuracy\nfor an 8-bit quantized model on ImageNet, in\ncontrast, FPQ can attain an almost loss-less result of\n79.88%. We analyze the gradients for the number\nof exponent bits e derived in (Kuzmin et al.,\n2022) and observe that each time the exponent\nbits change, the gradients experience exponential\nvariations, leading to high instability.\nBased\non this observation, we assert that employing a\nsearch-based method to determine the optimal\nformats is crucial in post-training quantization\n(PTQ).\nF\nReconstruction Choices\nThe previous works on integer post-training quanti-\nzation involves breaking down the target model into\nsub-modules and reconstructing them separately\n(Nagel et al., 2020; Li et al., 2021; Bai et al., 2022;\nYuan et al., 2022). This addresses the problem of\nover-fitting, given that only a limited amount of\nunlabeled calibration data is available. In this study\nwe find the layer-wise reconstruction and parallel\nquantization works best for floating-point PTQ:\nLayer Reconstruction: Recent research (Li\net al., 2021; Bai et al., 2022) suggests increasing the\nreconstruction granularity from layer reconstruc-\ntion (Nagel et al., 2020) to block reconstruction (Li\net al., 2021) or even larger granularity (Lee et al.,\n2023). This is achieved by jointly optimizing all the\nlinear layers or matrix multiplication components\nwithin each module to prevent the propagation of\nreconstruction errors among the layers. Despite\nthis, we have observed that increasing the recon-\nstruction granularity does not improve the accuracy\nof FPQ baseline or sometimes even lead to worse\nresults. Therefore, we choose layer reconstruction.\nParallel Quantization: Sequential quantization\nis the most commonly used approach (Wu et al.,\n2020; Nagel et al., 2020; Li et al., 2021) where\nmodules are quantized consecutively based on their\nsequential order, and the input for the current cali-\nbrating module is generated using all the previously\nquantized modules. However, some recent works\n(Yuan et al., 2022; Bai et al., 2022) proposed a new\nparallel quantization framework. This framework\nuses the raw output of the full-precision modules\nas input and makes the calibration of each module\nindependent from one another. In this work, we\nuse parallel quantization, as it yields better results\nthan its sequential counterparts.\nFigure 4: Quantization error of different formats for BERT layers.\nBert\nDeiT-S\nBert\nDeiT-S\nBert\nDeiT-S\nBert\nDeiT-S\nBert\nDeiT-S\nBert\nDeiT-S\n(a)\n(c)\n(b)\n(d)\n(e)\n(f)\nBert\nDeiT-S\nFigure 5: Magnitude of the output activations of different modules in BERT (left column), and DeiT-S (right\ncolumn).\nLLaMa-7B\n(a)\n(b)\n(c)\nFigure 6: Magnitude of the output activations of differ-\nent modules in LLaMA-7B.\n!\"!,!\n!\"!,#\n!\"#,!\n!\"#,#\n!\n%!,!\n!\n%!,#\n!\n%#,!\n!\n%#,#\n.$\n!\n.$\n#\n.(\n!\n.(\n#\n!\"!,!\n!\"!,#\n!\"#,!\n!\"#,#\n((\n!\n%!,!\n!\n%!,#\n!\n%#,!\n!\n%#,#\n($\nPer-tensor\nPer-tensor\nPer-token\nPer-channel\nFigure 7: Quantization granularity options that support\nefficient matrix multiplication. The dimensions that\nshare the same scaling factor are indicated with red\ndotted frames\n"
  },
  {
    "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
    "link": "https://arxiv.org/pdf/2310.16450.pdf",
    "upvote": "9",
    "text": "Published as a conference paper at ICLR 2024\nCLEX: CONTINUOUS LENGTH EXTRAPOLATION FOR\nLARGE LANGUAGE MODELS\nGuanzheng Chen1,2,3,\u2217 Xin Li2,3,\u2020\nZaiqiao Meng4\nShangsong Liang1,5,\u2020 Lidong Bing2,3\n1Sun Yat-sen University 2DAMO Academy, Alibaba Group\n3Hupan Lab, 310023, Hangzhou, China\n4University of Glasgow\n5Mohamed bin Zayed University of Artificial Intelligence\nguanzzh.chen@gmail.com, {xinting.lx,l.bing}@alibaba-inc.com\nzaiqiao.meng@glasgow.ac.uk, liangshangsong@gmail.com\nABSTRACT\nTransformer-based Large Language Models (LLMs) are pioneering advances in\nmany natural language processing tasks, however, their exceptional capabilities\nare restricted within the preset context window of Transformer. Position Em-\nbedding (PE) scaling methods, while effective in extending the context window\nto a specific length, demonstrate either notable limitations in their extrapolation\nabilities or sacrificing partial performance within the context window. Length ex-\ntrapolation methods, although theoretically capable of extending the context win-\ndow beyond the training sequence length, often underperform in practical long-\ncontext applications. To address these challenges, we propose Continuous Length\nEXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to\nmodel the continuous dynamics by ordinary differential equations over the length\nscaling factor, thereby overcoming the constraints of current PE scaling methods\ndesigned for specific lengths. Moreover, by extending the dynamics to desired\ncontext lengths beyond the training sequence length, CLEX facilitates the length\nextrapolation with impressive performance in practical tasks. We demonstrate that\nCLEX can be seamlessly incorporated into LLMs equipped with Rotary Position\nEmbedding, such as LLaMA and GPT-NeoX, with negligible impact on training\nand inference latency. Experimental results reveal that CLEX can effectively ex-\ntend the context window to over 4\u00d7 or almost 8\u00d7 training length, with no deteri-\noration in performance. Furthermore, when evaluated on the practical LongBench\nbenchmark, our model trained on a 4k length exhibits competitive performance\nagainst state-of-the-art open-source models trained on context lengths up to 32k.\nOur code is available at https://github.com/DAMO-NLP-SG/CLEX.\n1\nINTRODUCTION\nTransformer-based large language models (LLMs),\nsuch as GPT-4 (OpenAI, 2023) and\nLLaMA (Touvron et al., 2023a;b), have now emerged as the state-of-the-art models in various nat-\nural language processing (NLP) tasks. However, these models grapple with the limitations inher-\nent to the Transformer architecture - mainly, a preset context window, beyond which performance\nplummets catastrophically (Press et al., 2022). The quadratic complexity of the attention mecha-\nnism renders training LLMs with a larger context window extraordinarily resource-intensive. Prior\nworks (Dai et al., 2019; Beltagy et al., 2020; Bulatov et al., 2022) have proposed circumventing\nfull context length access via hierarchical architecture or sparse attention, albeit at the expense of\nforfeiting partial context information.\nRecently, there have been two lines of methods aimed at efficiently extending the pre-trained con-\ntext length of LLMs, both centred on position embedding (PE). The first line of methods, known as\nPE scaling, are proposed to effectively extend the context window of LLMs integrated with Rotary\n\u2217This work was done during the internship of Guanzheng Chen at Alibaba DAMO Academy.\n\u2020Corresponding authors.\n1\narXiv:2310.16450v2  [cs.CL]  17 Mar 2024\nPublished as a conference paper at ICLR 2024\n0.0\n10k\n20k\n30k\n40k\n50k\n60k\nEvaluation Sequence Length\n23\n24\nPPL\nFT-16k\nPI-16k (t=16)\nYarn-16k (t=16)\nYarn-16k (t=32)\nCLEX-16k (t=16)\nFigure 1: The PPLs of our CLEX and various baselines tested on 64k context length.\nPosition Embedding (RoPE) (Su et al., 2022). They allow LLMs to access longer context by scal-\ning either position indices (Chen et al., 2023) or frequency basis (Rozi`ere et al., 2023; Peng et al.,\n2023) of RoPE, demonstrating remarkable performance in long-context applications. However, such\nmethods are designed for extending the context length corresponding to a fixed scaling factor, which\neither restricts their ability to extrapolate to longer sequences (when using small factors) or impairs\nthe performance even within the native context window (when using large factors) as shown in Fig-\nure 1. On the other hand, length extrapolation methods (Press et al., 2022; Sun et al., 2023; Chi\net al., 2022; 2023), typified by ALiBi (Press et al., 2022), strive to achieve test-time context length\nextension (i.e., \u201ctraining on short, testing on long\u201d) by substituting position embeddings with addi-\ntional biases, where the biases encode positional information to the attention scores. Despite their\nimpressive capability in language modelling, ALiBi-like methods usually struggle in the practical\ntasks requiring long-context dependency (Pal et al., 2023) (also see \u00a74.3).\nIn this work, we present Continuous Length EXtrapolation (CLEX), a novel approach that effi-\nciently extrapolates the context window of LLMs through continuous PE scaling. Concretely, we\npropose a unified view of PE scaling via generalising the PE scaling methods to the transition of\nfrequency basis. Upon it, we formulate the PE scaling as a continuous dynamical system, which\nmodels the transition of frequency basis through the continuous dynamics over the length scaling\nfactor. We argue that previous PE scaling methods, training models using fixed (discrete) scaling fac-\ntors, overlook the progressively continuous dynamics over the gradually length-extending process.\nThis ensnares themselves in the aforementioned dilemma between extrapolating the length and pre-\nserving the performance within shorter lengths. In contrast, our CLEX exploits a neural ordinary\ndifferential equation (ODE) (Chen et al., 2018), parameterised by an up-and-down projection layer\nwith lightweight parameters to learn these continuous dynamics, enabling fine-grained extending to\nlong context. More essentially, by extending the dynamics beyond training length, CLEX empowers\nmodels to progressively extrapolate to longer contexts even when trained with short sequences.\nCLEX can serve as a drop-in component for RoPE-based LLMs, such as LLaMA (Touvron et al.,\n2023a;b) and GPT-NeoX (Black et al., 2022), with negligible overhead in computation and param-\neters size. We evaluate the performance of CLEX on two datasets: (1) a subset of RedPajama-\nBook (Computer, 2023) for long-context language modelling, and (2) LongBench (Bai et al., 2023)\nfor long-context practical tasks. Empirically, CLEX demonstrates remarkable length extrapolation\nability in language modelling, which can extend the context window to more than 4\u00d7 training length\nwithout any performance deterioration. For example, LLaMA-2-7B trained with CLEX on 16k con-\ntext length achieves comparable perplexities when testing on 16k and 64k tokens, respectively. By\nscaling the base model scale from 7B to 13B, CLEX exhibits an expanded extrapolation scope from\n4\u00d7 to almost 8\u00d7 training length. To be complementary, we also conduct instruction tuning (Wei\net al., 2022) with the proposed CLEX on the sequences of 4k length. The resulting model, when\nevaluated on the LongBench benchmark, is on par with current state-of-the-art open-source mod-\nels trained on context lengths up to 32k. These findings underscore the effectiveness of CLEX in\nextrapolating context length, signifying its efficiency for developing long-context LLMs.\n2\nPublished as a conference paper at ICLR 2024\n2\nPRELIMINARIES\n2.1\nROTARY POSITION EMBEDDING (ROPE)\nRotary Position Embedding (RoPE) (Su et al., 2022) has recently emerged as the most prevailing po-\nsitional encoding method in open-source LLMs like LLaMA. It integrates both absolute and relative\npositional information for Transformer models. Given a position index m \u2208 [1, L], RoPE injects the\nabsolute positional information into x \u2208 Rd via the transformation f : Rd \u2192 Rd as:\nf(x, m, \u03b8) = R\u03b8,mx,\n(1)\nwhere \u03b8 \u2208 R\u230ad/2\u230b is the rotation frequency basis and \u03b8i = 10, 000\u22122i/d; R\u03b8,m \u2208 Rd\u00d7d is a block\ndiagonal matrix formed by the elements\n(R\u03b8,m)i =\n\u0014\ncos m\u03b8i\n\u2212 sin m\u03b8i\nsin m\u03b8i\ncos m\u03b8i\n\u0015\n, for i = 1, 2, ..., \u230ad/2\u230b.\n(2)\nThe transformation in Eq. (1) is applied to the query and key vectors during self-attention. When\ncalculating the attention score for the query vector qm \u2208 Rd at position m and the key vector\nkn \u2208 Rd at position n, we have\n(R\u03b8,mqm)\u22a4(R\u03b8,nkn) = qmR\u03b8,n\u2212mkn.\n(3)\nHence, the relative positional information R\u03b8,n\u2212m is implicitly incorporated into the attention\nscores. However, even given the relative information, LLMs trained with RoPE, e.g., LLaMA,\nstill cannot achieve reasonable performance beyond the pre-trained context length.\n2.2\nPE SCALING METHODS\nTo extend the context length L, several strategies are proposed to adjust the position embedding by\nscaling either the position index m or frequency basis \u03b8 in Eq. (1). Formally, we define t = L\u2032/L\nas the length scaling factor where L\u2032 denotes the desired extended length. While Chen et al. (2023)\nintroduces scaling the index m by Position Interpolation (PI) as\nf PI\nt (x, m, \u03b8) = f(x, m\nt , \u03b8).\n(4)\nThis strategy maintains the position indices within the range [1, L], while effectively extending the\nprocessed range to [1, t \u00b7 L] by minimal fine-tuning steps on t \u00b7 L sequences. On the other hand,\nPeng et al. (2023) proposes Yarn, a.k.a. NTK-Aware Scaled RoPE, extends the context window by\nfrequency basis scaling (FBS). This strategy is similarly utilised by CodeLLaMA (Rozi`ere et al.,\n2023). Formally, the FBS methods are denoted as\nf FBS\nt\n(x, m, \u03b8) = f(x, m, \u03b8t),\n(5)\nwhere \u03b8t is the scaled frequency basis. Specifically, \u03b8t,i = \u03b8i \u00b7 (t)\u22122i/(d\u22122) in Yarn and \u03b8t,i =\n\u03b8i \u00b7 100\u22122i/d in CodeLLaMA.\n3\nMETHODOLOGY\nThis section demonstrates the details of CLEX. We first generalise the PE scaling to a continuous\ndynamical system in a unified manner (see \u00a73.1). On top of the continuous dynamical system,\nCLEX employs the neural ODE, parameterised by an up-and-down projection layer, to adaptively\nlearn the continuous dynamics during PE scaling (see \u00a73.2). In \u00a73.3, we introduce the training\nstrategy of CLEX that distributes the continuous dynamics beyond the training sequence length,\nthereby enabling the generalisation of continuous PE scaling to achieve the length extrapolation.\n3.1\nPOSITION EMBEDDING SCALING: A UNIFIED VIEW\nGiven the various methods that extend models\u2019 context length through position indices scaling and\nfrequency basis scaling, we first show that the transformations applied to position indices are essen-\ntially casting the frequency basis, which is formalised in Theorem 1.\n3\nPublished as a conference paper at ICLR 2024\n\ud835\udf03\n\ud835\udf03!\n\ud835\udf033\n\ud835\udf034\n1.0\n2.0\n3.0\n4.0\n\ud835\udf03\n\ud835\udf03!\n\ud835\udf033\n\ud835\udf034\n\ud835\udc61\n\ud835\udc61\n1.0\n2.0\n3.0\n4.0\n4k\n8k\n12k\n16k\n4k\n8k\n12k\n16k\nFigure 2: The graphical model of discrete PE scaling (left) and our continuous PE scaling (right).\nTheorem 1.\nFor the transformation T to position index m, there exists an equivalent transforma-\ntion T to frequency basis \u03b8 in Eq. (1), namely\nf(x, T \u00b7 m, \u03b8) = f(x, m, T \u2299 \u03b8),\n(6)\nwhere T = [T ]d/2\ni=1 and \u2299 denotes the element-wise transformation.\nProof. From Eq. (1), we have f(x, T \u00b7 m, \u03b8) = R\u03b8,T mx and f(x, m, T \u2299 \u03b8) = RT \u2299\u03b8,mx. For\nany T = [T ]d/2\ni=1,\n(R\u03b8,T m)i =\n\u0014\ncos T m\u03b8i\n\u2212 sin T m\u03b8i\nsin T m\u03b8i\ncos T m\u03b8i\n\u0015\n=\n\u0014\ncos m(T \u2299 \u03b8i)\n\u2212 sin m(T \u2299 \u03b8i)\nsin m(T \u2299 \u03b8i)\ncos m(T \u2299 \u03b8i)\n\u0015\n= (RT \u2299\u03b8,m)i .\n(7)\nHence, there is a unified form for PE scaling that consistently projects the frequency basis by \u03b1(t):\nft(x, m, \u03b8) = f (x, m, \u03b1(t) \u2299 \u03b8) ,\n(8)\nwhere \u03b1(t) is a single-variable transformation defined over the length scaling factor t. Through this\nunified formulation, PI (Chen et al., 2023) and Yarn (Peng et al., 2023) can be viewed as the special\ncases when plugging \u03b1PI(t) = [1/t]d/2\ni=1 and \u03b1Yarn(t) =\n\u0002\nt\u22122i/(d\u22122)\u0003d/2\ni=1 into Eq. 8, respectively.\nNote that \u03b8t = \u03b1(t) \u2299 \u03b8 denotes the scaled frequency basis at context length of t \u00b7 L and \u03b81 = \u03b8\n(namely \u03b1(1) = 1). As illustrated in Figure 2, this indicates a progressive chain across discrete t\nvalues that\nz(t) = z(1) + log \u03b1(t) = z(t \u2212 1) + log\n\u03b1(t)\n\u03b1(t \u2212 1),\n(9)\nwhere z(t)=log \u03b8t.\nBy continuizing the progressive chain, we can formulate the PE scaling as a continuous dynamical\nsystem, with the continuous dynamics of frequency basis dz(t)/dt as\ndz(t)\ndt\n= d log \u03b1(t)\ndt\n.\n(10)\nIn essence, recent PE scaling methods, concentrating on manually formulating the \u03b1(t), are equiva-\nlent to applying various dynamics for frequency basis that enable models to adapt to longer contexts.\n3.2\nCONTINUOUS PE SCALING VIA NEURAL ODE\nEven given the continuous dynamics of frequency basis, previous methods are inherently designed\nfor extending the context length at discrete t values. For example, PI (Chen et al., 2023) fine-tunes\nthe model on a specific scaling factor t to extend the context window length to t \u00b7 L. One potential\nissue of these methods, as depicted in Figure 1, is that they are susceptible to overfitting to the speci-\nfied frequency basis, leading to either poor extrapolation ability to longer lengths beyond training or\nperformance drops within short lengths, or both in some cases. Therefore, our CLEX aims to build\na continuous PE scaling, which induces the model to adapt the frequency basis corresponding to a\ncontinuous scope of t as illustrated in Figure 2 (right).\n4\nPublished as a conference paper at ICLR 2024\nRecall that previous PE scaling, corresponding to a manually defined \u03b1(t), implies the constant\ndynamics in Eq. (10). In our method, we utilise a variable function g: Rd/2 \u2192 Rd/2 to model the\ndynamics, hence towards a more general and flexible view as:\ndz(t)\ndt\n= g(z(t), t).\n(11)\nBy restricting the function to be associated with the latent states z(t), g is capable of capturing the\nfine-grained changes of frequency basis during the length-extending process. However, it is non-\ntrivial to manually define the z(t)-aware function g. Here, we directly parameterise the function\nusing the neural network \u03d5. Therefore, for any t\u2032 \u2208 [1, t], there is a neural ODE modelling the\nscaling of frequency basis as\nz(t\u2032) = z(1) +\nZ t\u2032\n1\ng\u03d5(z(t), t)dt,\n(12)\nwhere the frequency basis at the length t\u2032\u00b7L can be derived by \u03b8t\u2032 = exp(z(t\u2032)).\nMore specifically, we adopt an up-and-down projection as the neural network, expressed as:\ng\u03d5(z(t), t) = Wdown \u00b7 \u03c3 (Wup \u00b7 z(t)) + \u03bet,\n(13)\nwhere Wup \u2208 R\nd\n2 \u00d7\u03bbd and Wdown \u2208 R\u03bbd\u00d7 d\n2 are the transformation matrices, of which the parameters\nare determined by the amplification factor \u03bb; \u03c3 is the SiLU activation function and \u03bet is the scalar\nembedding typifying the scaling procedure at factor of t. Here, we adopt the constant dynamics of\nYarn as the \u03bet for speeding up convergence, namely\n\u03bet = d log \u03b1Yarn(t)\ndt\n= \u2212\n\u0014\n2i\n(d \u2212 2) \u00b7 t\n\u0015d/2\ni=1\n(14)\n3.3\nCONTINUOUS LENGTH EXTRAPOLATION: TRAIN ON SHORT, TEST ON LONG\nContinuous PE scaling can serve as a more adaptive and flexible PE scaling method to extend the\ncontext length to a given training length LTrain. Unlike the previous PE scaling methods built on\na larger scaling factor, which would lead to inferior performance on the lengths corresponding to\nsmaller counterparts, the continuous PE scaling would enable non-destructively generalisation to\nlarger scaling factors via adaptive continuous dynamics. Therefore, by simply extending the contin-\nuous dynamics beyond the factor t=LTrain/L during training (where we denote the desired scaling\nfactor as tTrain), we can access the continuous length extrapolation (CLEX) method, which achieves\nthe capability of \u201ctraining on short, testing on long\u201d.\nMoreover, to learn the neural ODE in Eq. (12) for continuous t, we randomly sample t\u2032 \u2208 [1, tTrain]\nfor each training step, enabling the model to adapt to the broad scope frequency basis without over-\nfitting a specific one. Note that the frequency basis is bound with the position index in Eq. (1). This\nreveals the aforementioned training involves inconsistency between the frequency basis and position\nindices: the frequency basis is varied corresponding to the t\u2032 \u2208 [1, tTrain], while the position indices\nare fixed as {1, 2, . . . , LTrain}. Here, we propose the position extrapolation strategy to address this\nconsistency. Contrary to PI, which shrinks the position indices into the context length, we enlarge\nthe position indices {1, 2, . . . , LTrain} of the trained sequences up to the range [1, t\u2032\u00b7L] for each train-\ning step. The position indices can be acquired by uniformly scaling to {1\u00b7s, 2\u00b7s, . . . , LTrain\u00b7s} where\ns = t\u2032\u00b7L/LTrain, or alternatively, by randomly sampling LTrain of indices from [1, t\u2032\u00b7L]. Empirically,\nwe found that random sampling generally performs better. More discussions can be found in \u00a74.2.\nDuring inference, the ideal scenario is to acquire the frequency basis corresponding to each sequence\nlength. However, this approach is computationally demanding. To improve efficiency, we first cache\nsome frequency basis derived from g\u03d5 for K discrete t values as {tk|k \u2208 [1, K]}. For each sequence\nwith a length of LInfer during inference, we employ the frequency basis corresponding to the nearest\nupper bound within tk \u00b7L for k = 1, . . . , K. Through this, our method introduces negligible time\ncost compared to naive inference of LLMs.\n4\nEXPERIMENTS\nIn this section, we conduct a thorough evaluation of CLEX\u2019s performance in terms of handling long\ncontexts and its extrapolation capabilities. We compare our approach against other methods covering\n5\nPublished as a conference paper at ICLR 2024\nTrain\nEvaluation Length\nLength\n4096 (4k)\n8192 (8k)\n16,384 (16k)\n32,768 (32k)\n65,536 (64k)\nMethods\nPPL\nACC.\nPPL\nACC.\nPPL\nACC.\nPPL\nACC.\nPPL\nACC.\nLLaMA-2\n4k\n6.04\n58.18\n20.54\n44.50\n>100\n22.43\n>1000\n12.70\n>1000\n10.64\nCodeLLaMA\n16k\n7.60\n54.88\n7.40\n55.19\n7.33\n55.30\n15.12\n44.70\n52.02\n31.16\nNaive FT\n16k\n5.98\n58.83\n5.93\n58.91\n5.91\n58.58\n18.31\n43.04\n>100\n26.05\nPI\n16k\n5.90\n59.05\n5.71\n59.44\n5.72\n59.87\n6.05\n58.5\n8.75\n52.02\nYarn (t=16)\n16k\n6.50\n57.28\n5.71\n59.57\n5.73\n59.87\n5.99\n58.13\n8.51\n52.62\nYarn (t=32)\n16k\n6.61\n57.12\n5.94\n58.27\n5.96\n58.04\n6.08\n57.73\n6.22\n57.98\nCL-Scaling\n16k\n24.99\n37.84\n5.86\n59.08\n5.87\n59.05\n10.56\n50.47\n41.09\n34.16\nALiBi\n4k\n6.34\n58.01\n6.39\n57.8\n6.41\n57.78\n6.50\n57.47\n6.51\n56.44\nRandomPos\n4k\n5.88\n58.49\n>100\n34.23\n>1000\n18.27\n>1000\n9.31\n>1000\n7.44\n4k\n5.86\n59.21\n5.70\n59.62\n5.87\n58.93\n14.53\n47.55\n30.51\n35.33\nCLEX\n8k\n5.98\n58.75\n5.78\n59.44\n5.71\n59.64\n5.99\n58.66\n11.74\n47.50\n16k\n5.88\n59.21\n5.68\n59.73\n5.52\n60.28\n5.55\n60.06\n5.64\n59.94\nTable 1: Perplexity (PPL) and next-token-prediction accuracy (ACC.) on language modeling with\nevaluation lengths from 4k to 64k. We train the LLaMA-2-7B using length extrapolation methods on\n4k length and PE scaling methods on 16k length, while reporting the results of CLEX trained across\n4k, 8k and 16k. CL-Scaling denotes training LLaMA-2-7B with the scaling method of CodeLLaMA\nbut using our training data. The training loss curves are depicted in Figure 9.\nboth length extrapolation (i.e., ALiBi (Press et al., 2022) and random positions (denoted as Random-\nPos) (Ruoss et al., 2023)) and PE scaling methods (i.e., PI (Chen et al., 2023) and Yarn (Peng et al.,\n2023)). We primarily conduct experiments on the LLaMA-2-7B model. For the language mod-\nelling, we train our model and the baselines on 2B tokens extracted from Redpajama-Book (Com-\nputer, 2023), which is collected from Pile-Books3 (Gao et al., 2020) and PG-19 (Rae et al., 2019)\ndatasets. The performance of the models is assessed based on perplexity and next-token-prediction\naccuracy, with evaluation sequence lengths up to 64k. Furthermore, we conduct instruction tun-\ning for LLaMA-2-7B using CLEX on the UltraChat dataset (Ding et al., 2023b). The evaluation\nis performed on the LongBench benchmark (Bai et al., 2023), where we compare our model with\nGPT-3.5-turbo and other LLaMA-2-based open-source models designed for handling long context.\nFurther details about baselines and training configuration will be discussed in Appx. \u00a7A, as well as\nmore experimental results and ablations in Appx. \u00a7B.\n4.1\nLONG-CONTEXT LANGUAGE MODELLING\nCLEX achieves length extrapolation.\nWe first report the experimental results of baselines and\nCLEX on language modelling, with the evaluation length from 4k to 64k. As shown in Table 1,\nour CLEX consistently demonstrates remarkable performance in length extrapolation, being able\nto extrapolate the context length to more than 4\u00d7 training length without any performance drops.\nTaking CLEX-4k as an example, its PPL on 4k sequence (training length) is comparable to that on\n16k sequence (5.86 vs. 5.87). When evaluated on the sequences within 16k, CLEX-4k is on par\nwith or even better than all of the compared methods trained on lengths up to 16k. Moreover, with\nthe increase in training length, our CLEX not only exhibits promising generalisation capability to\nvery long contexts (up to 64k) but also guarantees performance on short sequences.\nWe also found that discrete PE scaling methods (i.e., PI and Yarn) have self-extending property:\ntraining with scaled frequency basis equips the model with the ability to extrapolate to further-\nscaled counterparts (see Appx. \u00a7B.2 for more discussions.). As depicted in Figure 1, however,\nthe extrapolation capability of these methods is limited, accompanied by a significant performance\ndecline even within the naive context length. This indicates the inherent challenge of achieving a\ndelicate balance between extrapolation to longer lengths and performance maintenance within short\nlengths when using the discrete scaling factor. In contrast, CLEX tackles this issue via learnable\ncontinuous dynamics, providing a more fine-grained extrapolation while preserving the performance\nfor the internal context.\nNote that ALiBi may extend further than CLEX trained on 4k sequences (though typically pro-\nducing inferior results), our experiments reveal that these gains may come at the cost of long-term\ninformation, leading to underperformance in long-context practical tasks (see \u00a74.3 for more details).\n6\nPublished as a conference paper at ICLR 2024\n4k\n8k\n16k\n32k\n64k\nEvaluation Sequence Length\n10\n20\n30\nPPL\nCLEX-7B-4k\nCLEX-7B-8k\nCLEX-13B-4k\n0.25\n0.5\n0.75\n1\n1.25\n1.5\n1.75\n2\nTraining Dataset Size (Billion tokens)\n10\n20\n30\nCLEX-7B-4k on 16k\nCLEX-7B-4k on 32k\nCLEX-13B-4k on 16k\nCLEX-13B-4k on 32k\nFigure 3: Left: The PPLs of CLEX on different evaluation sequence lengths with 7B and 13B\nparameter sizes. Right: The PPLs of CLEX cross variable training data size with different parameter\nsizes and evaluation lengths.\n4k\n8k\n16k\n32k\n64k\n20\n40\n60\nPPL\nContinuous vs Discrete\nContinuous\nDiscrete\n4k\n8k\n16k\n32k\n64k\nEvaluation Sequence Length\n20\n40\n60\n80\nSampling Strategy\nRandom\nUniform\nNo sampling\n4k\n8k\n16k\n32k\n64k\n10\n20\n30\n Factor\n= 1\n= 2\n= 4\nFigure 4: The ablation studies for continuous dynamics, sampling strategies and log scaling.\nThe scaling law for the extrapolation ability of CLEX.\nTo investigate the effectiveness of CLEX\nover the scale of the base model and training data size, we further port our method to LLaMA-2-\n13B. As depicted in Figure 3, when trivially extending the base model scale from 7B to 13B, our\nCLEX demonstrates an increased capacity to extrapolate to longer context lengths. Specifically, the\nextrapolation ability of CLEX-13B trained on 4k length approaches that of CLEX-7B trained on\n8k. While the training data scale, more surprisingly, does not significantly impact the extrapolation\ncapability of CLEX. Models trained with 0.25B or 2B tokens with 4k sequence length achieve\ncomparable PPLs when evaluating on 16k or 32k lengths in Figure 3, indicating the negligible\nmargins from the larger training data size. This also implies that CLEX can efficiently extend the\ncontext length of LLMs through minimal training steps resembling PI and Yarn.\nBased on these findings, we propose a scaling law for CLEX: to scale the context length of LLMs\nto moderately desired lengths (e.g., 16k \u2192 64k), one should proportionally enlarge the training\nsequence lengths (e.g., 4k \u2192 16k). For scaling the context length up to considerably long lengths\n(e.g., >200k), the parameter size of the base model should be correspondingly increased while\nmaintaining the training length, since the contexts may take more footprints than model parameters.\nNote that scaling the training data does not directly affect the extrapolation ability of CLEX, but\nmay be implicitly incorporated when scaling the base pre-trained LLMs.\n4.2\nABLATION STUDY\nWe now conduct three types of ablations to investigate the efficacy of the components in CLEX:\nContinuous dynamics.\nTo learn the continuous dynamics using neural ODE, we adopt a distinct\ntraining approach that involves sampling the scaling factor t for each data batch. Here we seek to\nexplore if the exceptional extrapolation ability of CLEX is solely derived from the variable t rather\nthan the continuous dynamics. We employ the discrete Yarn method with t = 16, that undergoes the\nsame training procedure of CLEX but removes the ODE parameters, serving as a discrete baseline.\nIn Figure 4 (left), we discover that the discrete approach equipped with the random-sampled t signif-\nicantly underperforms our CLEX, indicating the essentiality of the learnable continuous dynamics\nin CLEX for accessing the extrapolation ability.\n7\nPublished as a conference paper at ICLR 2024\nSingle-Doc QA\nMulti-Doc QA\nSummarization\nFew-shot Learning\nSynthetic Task\nCode Completion\n0\n20\n40\n60\n80\n100\nGPT-3.5-Turbo-16k\nLongChat-v1.5-7B-32k\nCodeLlama-7B-16k\nVicuna-v1.5-7B-16k\nBaichuan-13B-4k (ALiBi)\nCLEX-7B-4k (Ours)\n4k\n16k\n32k\nTrained Sequence Length\n20\n30\n40\nAvg. All\nGPT-3.5-Turbo-16k\nLongChat-v1.5-7B-32k\nCodeLLaMA-7B-16k\nVicuna-v1.5-7B-16k\nBaichuan-13B-4k (ALiBi)\nCLEX-7B-4k (Ours)\nFigure 5:\nLeft: the average scores for each domain of tasks in LongBench. Right: the average\nscores of all tasks corresponding to the training length of each model. Note that CLEX is trained on\n4k sequence length but directly tested on 16k context length without truncation.\nPosition extrapolation.\nWe adopt the position extrapolation strategy, which extends the scope of\nposition indices in training sequences by sampling from a broader range, to reconcile the inconsis-\ntency between frequency basis and position indices during the training process. In this study, we\nexamine the impact of various sampling strategies (uniform or random) and contrast them with the\nnaive position indices. The results in Figure 4 underscore the efficacy of position extrapolation in\nCLEX, without which the extrapolation ability of models declines significantly. Furthermore, ran-\ndom sampling slightly performs better than uniform sampling, so we adopt it across all experiments.\nThe parameter scale of ODE.\nWe also study the impact of parameter size of the neural ODE\nin CLEX. The parameter size is determined by the \u03bb, namely the amplification factor in Eq. (13).\nIn Figure 4, we plot the results of CLEX with \u03bb = 1, 2, 4, where they achieve similar performance.\nNote that the parameter size of neural ODE in CLEX is quite small even when \u03bb = 4, as the\ndimension d in Eq. (13) is usually equal to 128. Although it is possible to enhance CLEX with\nlarger \u03bb (e.g., 32), we set the \u03bb=1 in all experiments for the minimal effect on inference latency.\n4.3\nEVALUATION ON LONG-CONTEXT BENCHMARK\nTo ascertain the comprehensive performance of CLEX in real-world scenarios, we further conduct\nan evaluation on the zero-shot LongBench benchmark. This benchmark encompasses a broad range\nof tasks, such as question-answering, summarization, and code completion, where the evaluation\nlength ranges from 5k to 15k. We perform a pilot instruction tuning for LLaMA-2-7B by employing\nCLEX on the UltraChat dataset, with a sequence length of 4k. During inference, we harness all\nmodels to tackle the context length of 16k, thereby ensuring the comprehensive exploitation of\ncontextual information in the tasks. As depicted in Figure 5, we present the average scores of each\ndomain in LongBench for CLEX, in comparison to the GPT-3.5-Turbo-16k model and strong open-\nsource LLMs like LongChat-v1.5-7B-32k and CodeLLaMA-7B-16k.\nGenerally, when trained with sequences of 4k length, CLEX holds its own against any open-source\nLLMs that are trained on lengths up to 32k. In the specific domains of Summarization, Few-shot\nLearning, and Code Completion, CLEX on LLaMA-2-7B remains competitive with or even sur-\npasses the GPT-3.5-Turbo-16k. We note that the Baichuan-13B-4k, pre-trained with ALiBi (Press\net al., 2022), demonstrates marked underperformance on the LongBench although with a larger\nparameter size. Additionally, similar poor results are achieved by ALiBi when applying it upon\nLLaMA-2-7B using the same training procedure as CLEX (see Appx. \u00a7B.5). This could likely be\nattributed to ALiBi\u2019s overemphasis on local context through the attention bias, which, while advan-\ntageous for language modelling, restricts access to long-context information in practical tasks. In\ncontrast, CLEX directly extends the context length of LLMs without imposing any constraints on\ncontext, which consistently achieves superior extrapolation ability on both language modelling and\nthe LongBench. This substantiates the considerable potential of CLEX to serve as the state-of-the-art\napproach for extrapolating the context length of LLMs to excel in long-context applications.\n8\nPublished as a conference paper at ICLR 2024\nIn addition, we highlight that our CLEX merely introduces minuscule inference latency. Given a\ncontext length of 16k in LongBench with a generation length of 512, the generation throughput\nbetween our CLEX-7B and LLaMA-2-7B is comparable (27.8 tokens/s vs 28.3 tokens/s, in a single\nA100), when using the cache mechanism introduced in \u00a73.3.\n5\nRELATED WORK\nHierarchical Architecture / Sparse Attention.\nTo overcome the quadratic complexity of atten-\ntion, Dai et al. (2019) proposes the Transformer-XL that handles the long sequence at segment level\nby Transformer, with these segments interacting through a recurrence mechanism. The Recurrent\nMemory Transformer (Bulatov et al., 2022) refines this mechanism by incorporating special memory\ntokens into the recurrence, which is capable of scaling the context length to the millions (Bulatov\net al., 2023). On the other hand, Child et al. (2019); Beltagy et al. (2020) proposed using the sparse\nattention to circumvent the full access to the long sequences, hence reducing the complexity. The\nsparse attention has been adopted by Ding et al. (2023a) to scale the context length of transformers\ninto the billions. However, these methods sacrifice the utilisation of the entire sequence during at-\ntention, resulting in an inevitable loss of some contextual information. Additionally, modifications\nto the model architecture make these methods challenging to apply to existing pre-trained LLMs.\nConversely, our CLEX serves as a drop-in component for LLMs, can efficiently extend the capacity\nof models to tack the entire long sequences without explicit drops of context information.\nLength Extrapolation.\nBuilding on the foundation laid by ALiBi (Press et al., 2022), a series of\nworks (Sun et al., 2023; Chi et al., 2022; 2023) seek to train the Transformer-based models on a\nshort length, while directly testing on longer counterparts. These methods substitute the position\nembedding with bias introduced into attention scores, thereby incorporating positional information.\nNotably, the bias typically gives higher profits to closer tokens. This mechanism intuitively am-\nplifies the local context for each token at the expense of distant information. Consequently, these\nlength-extrapolation methods encounter challenges in effectively handling long contexts in practi-\ncal applications (Pal et al., 2023). However, our CLEX demonstrates remarkable effectiveness in\npractical tasks such as summarization, indicating the de facto extrapolation ability for applications.\nPosition Embedding (PE) Scaling.\nRecent research has sought to extend the context length of\nTransformers through the scaling of the extensively employed RoPE. Specifically, Chen et al. (2023)\nproposed position interpolation, a method that efficiently extends the context window by scaling the\nposition index within RoPE. In a similar vein, Peng et al. (2023); Rozi`ere et al. (2023) opted to scale\nthe frequency basis, achieving superior performance. However, these methods necessitate training\n(or fine-tuning) on the desired extended length. As a result, they exhibit a limited ability to extrapo-\nlate beyond the trained length and even suffer from performance drops within the shorter lengths. In\nCLEX, we generalise the discrete PE scaling to a continuous counterpart, hence uniformly extrapo-\nlating the context length of LLMs while preserving the performance within short lengths.\n6\nCONCLUSION\nWe have presented the Continuous Length EXtrapolation (CLEX), a novel approach that efficiently\nextrapolates the context length of Large Language Models (LLMs) to over 4x the training (fine-\ntuning) length without any decline in performance. CLEX utilises the neural ODE to learn the\ncontinuous dynamics over the length scaling factor during PE scaling, hence enabling fine-grained\nextension for the frequency basis in the RoPE. We conduct thorough experiments to investigate the\neffectiveness of CLEX compared to a variety of strong LLMs, covering the language modelling task\nand LongBench benchmark. The experimental results have demonstrated the exceptional extrapola-\ntion ability of CLEX, where our CLEX trained with a sequence length of 4k holds the potential to\nremain competitive to any open-source long-context LLMs (e.g., CodeLLaMA) trained on lengths\nup to 32k. These results highlight the potential of CLEX as a state-of-the-art approach for effi-\nciently extrapolating the context length of LLMs, paving the way for advancements in long-context\napplications. By scaling the base model size up, we found CLEX can be correspondingly enhanced\nand subsequently is capable of extrapolating the model to a longer context length. This property\nindicates the tempting effectiveness of CLEX in the era of LLMs.\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGEMENTS\nThis work was substantially supported by DAMO Academy through DAMO Academy Research\nIntern Program. Shangsong Liang was supported by the National Natural Science Foundation of\nChina (Grant No. 61906219) and the Mohamed bin Zayed University of Artificial Intelligence,\nUnited Arab Emirates.\nREFERENCES\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,\nXiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual,\nmultitask benchmark for long context understanding. 2023. URL https://arxiv.org/\nabs/2308.14508.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv:2004.05150, 2020. URL https://arxiv.org/abs/2004.05150.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Ho-\nrace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shiv-\nanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-\n20B: An open-source autoregressive language model.\nIn Proceedings of BigScience Episode\n#5 \u2013 Workshop on Challenges & Perspectives in Creating Large Language Models. Association\nfor Computational Linguistics, May 2022.\nURL https://aclanthology.org/2022.\nbigscience-1.9.\nAydar Bulatov, Yuri Kuratov, and Mikhail Burtsev.\nRecurrent memory transformer.\nIn Al-\nice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neu-\nral Information Processing Systems, 2022. URL https://openreview.net/forum?id=\nUynr3iPhksa.\nAydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Scaling transformer to 1m tokens and beyond\nwith rmt. 2023. URL https://arxiv.org/abs/2304.11062.\nRicky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary dif-\nferential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,\nand R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/\npaper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window\nof large language models via positional interpolation. 2023. URL https://arxiv.org/\nabs/2306.15595.\nTa-Chung\nChi,\nTing-Han\nFan,\nPeter\nJ\nRamadge,\nand\nAlexander\nRudnicky.\nKer-\nple:\nKernelized relative positional embedding for length extrapolation.\nIn Advances\nin\nNeural\nInformation\nProcessing\nSystems.\nCurran\nAssociates,\nInc.,\n2022.\nURL\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/\n37a413841a614b5414b333585e7613b8-Paper-Conference.pdf.\nTa-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer\nlength extrapolation via the lens of receptive field analysis. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers). Associa-\ntion for Computational Linguistics, July 2023. URL https://aclanthology.org/2023.\nacl-long.756.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. 2019. URL https://arxiv.org/abs/1904.10509.\nTogether Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURL https://github.com/togethercomputer/RedPajama-Data.\n10\nPublished as a conference paper at ICLR 2024\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics. Association for Compu-\ntational Linguistics, July 2019. URL https://aclanthology.org/P19-1285.\nTri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.\nURL https://arxiv.org/abs/2307.08691.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness. In Advances in Neural Information Process-\ning Systems, 2022. URL https://arxiv.org/abs/2205.14135.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning\nZheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. 2023a. URL\nhttps://arxiv.org/abs/2307.02486.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong\nSun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional\nconversations. arXiv preprint arXiv:2305.14233, 2023b. URL https://arxiv.org/abs/\n2305.14233.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al.\nThe Pile: An 800GB dataset of di-\nverse text for language modeling.\narXiv preprint arXiv:2101.00027, 2020.\nURL https:\n//arxiv.org/abs/2101.00027.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gi-\nanna Lengyel, Guillaume Bour, Guillaume Lample, L\u00b4elio Renard Lavaud, Lucile Saulnier, Marie-\nAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le\nScao, Th\u00b4eophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00b4ee Lacroix, and William El Sayed.\nMixtral of experts. 2024. URL https://arxiv.org/abs/2401.04088.\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Yoshua\nBengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:\n//arxiv.org/abs/1412.6980.\nOpenAI. GPT-4 Technical Report. 2023. URL https://arxiv.org/abs/2303.08774.\nArka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha\nNaidu.\nGiraffe: Adventures in expanding context lengths in llms.\n2023.\nURL https://\narxiv.org/abs/2308.10882.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\nextension of large language models. 2023. URL https://arxiv.org/abs/2309.00071.\nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables\ninput length extrapolation. In International Conference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=R8sQPpGCv0.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.\nCompressive transformers for long-range sequence modelling.\narXiv preprint, 2019.\nURL\nhttps://arxiv.org/abs/1911.05507.\nBaptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00b4er\u00b4emy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton,\nManish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00b4efossez,\nJade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and\nGabriel Synnaeve. Code llama: Open foundation models for code. 2023. URL https://\narxiv.org/abs/2308.12950.\n11\nPublished as a conference paper at ICLR 2024\nAnian Ruoss, Gr\u00b4egoire Del\u00b4etang, Tim Genewein, Jordi Grau-Moya, R\u00b4obert Csord\u00b4as, Mehdi Ben-\nnani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization\nof transformers. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers). Association for Computational Linguistics, July 2023. URL\nhttps://aclanthology.org/2023.acl-short.161.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: En-\nhanced transformer with rotary position embedding. 2022. URL https://arxiv.org/abs/\n2104.09864.\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaud-\nhary, Xia Song, and Furu Wei. A length-extrapolatable transformer. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). As-\nsociation for Computational Linguistics, July 2023. URL https://aclanthology.org/\n2023.acl-long.816.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models. 2023a. URL https://arxiv.org/abs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.\n2023b. URL https://arxiv.org/abs/2307.09288.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Interna-\ntional Conference on Learning Representations, 2022. URL https://openreview.net/\nforum?id=gEZrGCozdqR.\n12\nPublished as a conference paper at ICLR 2024\nA\nEXPERIMENTAL DETAILS\nA.1\nBASELINES\nWe compare our CLEX with a variety of baselines in language modelling, all of which are trained\nwith the subset of 2B tokens from Pile-Books3 and PG-19 except LLaMA-2 and CodeLLaMA\nthat are evaluated by the open-source pre-trained checkpoints. We train the PE scaling and length\nextrapolation methods with 16k and 4k sequence lengths, respectively. Specifically, we use the\nscaling factor t = 4 to train models using 16k sequence length for PI following Chen et al. (2023).\nWhile for Yarn, we train models with the scaling factor of 16 and 32 (but also using the 16k sequence\nlength), respectively, following the settings in Peng et al. (2023). For the ALiBi, we directly remove\nthe RoPE in LLaMA-2 and train models with the attention bias. For the random positions, we sample\nthe position indices from [1, 16k] while train models with 4k sequence length. We also utilise the PE\nscaling method from CodeLLaMA to train LLaMA-2-7B using our training data, which is denoted\nas CL-Scaling.\nA.2\nTRAINING DETAILS\nWe use a subset from the Redpajama-Book (Computer, 2023) as the training data for language\nmodelling, which is collected from Pile-Books3 and PG-19 datasets and the subset comprises ap-\nproximately 2B tokens. We set the learning rate of 2e-5 for all models, which are optimised by Adam\noptimiser (Kingma & Ba, 2015). The batch size is set to 64k tokens for 7B models and 16k tokens\nfor 13B models. The maximum desired t during training in CLEX (namely tTrain in \u00a73.3) is set as\n16 for LLaMA-2. We utilise the Flash Attention (Dao et al., 2022; Dao, 2023) to support efficient\ntraining. The amplification factor of ODE layer \u03bb is set as 1 for all 7B models and 2 for 13B models.\nFor the instruction tuning, we train our model using the UltraChat dataset for 1 epoch, starting with\nthe checkpoint after the training of language modelling. The training procedure of CLEX is shown\nin Alg. 1.\nAlgorithm 1 Training Procedure of CLEX\n1: repeat\n2:\nGiven a batch of sequences B with length of LTrain;\n3:\nsample t\u2032 \u223c [1, tTrain];\n4:\ncalculate z(t\u2032) by Eq. (12);\n5:\n\u03b8t\u2032 = exp(z(t\u2032));\n6:\nsample (randomly or uniformly) t\u2032 \u00b7 LTrain position indices from [1, tTrain \u00b7 LTrain];\n7:\ncalculate RoPE with \u03b8t\u2032 and sampled position indices;\n8:\ntrain the model with RoPE on B with language modelling objective.\n9: until converged\nA.3\nEVALUATION DETAILS\nFor language modelling, we evaluate all models with a similar subset to the training set but con-\ntaining 20 million tokens. For different evaluation lengths, we group the test set into corresponding\nlengths of sequences. We found fixing the scaling factor as the training one would hamper the\ngeneral performance and extrapolation ability of baselines, therefore, we proportionally enlarge the\nscaling factor when the evaluation length is beyond the training length. For example, when evaluat-\ning on 32k length, we set the scaling factor of PI as 8 rather than the training factor of 4. We also\nadopt the log scaling trick1 for all baselines and our model, where we scale the attention scores by\nthe max{1, logLTrain LTest}. We found it can improve the performance when evaluating on lengths\nbeyond training length as shown in Figure 8. For the LongBench, we follow the setups from Bai\net al. (2023), where the decoding method is set to greedy search. We set the tk in \u00a73.3 as {1, 2, 3, 4},\nto cache the frequency basis corresponding to {4k, 8k, 12k, 16k}.\n1https://spaces.ac.cn/archives/8823\n13\nPublished as a conference paper at ICLR 2024\n0\n20\n40\n60\nDimension\n0.0\n0.2\n0.4\n0.6\n0.8\nSinusoidal Frequency\nBase=1e4 (original)\nCLEX (t = 4)\nCLEX (t = 8)\nCLEX (t = 16)\nBase=1e6\nFigure 6: The sinusoidal frequency over the dimension of the frequency basis. The lower dimension\ndenotes the higher frequency.\n0.0\n10k\n20k\n30k\n40k\n50k\n60k\nEvaluation Sequence Length\n23\n24\nPPL\nPI-16k (t=4)\nPI-16k (t=16)\nFigure 7: The results of PI-16k (trained with t = 4) on 64k sequence length, while evaluating with\nt = 4 and t = 16.\nB\nADDITIONAL RESULTS\nB.1\nVISUALISATION OF LEARNED FREQUENCY BASIS\nIn Figure 6, we show the frequency basis according to different sequence lengths after the training\nof CLEX on LLaMA-2 with a 4k sequence length. The plot reveals that CLEX tends to upscale\nthe high frequencies at certain dimensions while simultaneously downscaling some others. More\nsurprisingly, we have observed that the frequency bases associated with different t values in CLEX\nexhibit an isotropic behaviour, that the dimensions where downscaling and upscaling occur are sim-\nilar across different t values, with larger t values resulting in further scaling. This finding may\ncontribute to the design of heuristics PE scaling methods without training.\nB.2\nTHE SELF-EXTENDING PROPERTY OF PE SCALING METHODS\nPrevious PE scaling methods usually evaluate models with a specified scaling factor during training.\nWe found this would significantly hamper the performance evaluation beyond the training length.\nHence, we adjust the scaling factor of the previous PE scaling method corresponding to the evalu-\nation lengths. In Figure 7, PI-16k (trained with t = 4) achieves non-trivial results when evaluating\n14\nPublished as a conference paper at ICLR 2024\nModels\nTrain\nEvaluation Length\nLength\n32K\n64K\n128K\n256K\nCLEX-Phi-2\n32K\n5.11\n5.17\n6.55\n>10\nCLEX-Mixtral-8x7B\n32K\n2.56\n2.53\n2.57\n3.78\nTable 2: Perplexity (PPL) of CLEX-Phi-2 and CLEX-Mixtral-8x7B on language modelling with\nevaluation lengths from 32k to 256k.\nFigure 8: The ablation of log scaling (left) and constant dynamic \u03bet (right) for CLEX trained on 4k\nsequence length.\nwith t = 16 on 64k length, which significantly outperforms the naive evaluation using t = 4.\nNote that Yarn is naively trained with larger t, so it is not necessary to adjust the scaling factor\nfor Yarn during evaluation. When the evaluation lengths become longer, however, we believe the\nself-extending property would benefit the Yarn as well. Regarding these, the extrapolation ability\nfrom the self-extend in previous PE scaling methods is still far weaker than CLEX as illustrated\nin Figure 1.\nB.3\nRESULTS OF MORE MODELS\nWe also extend CLEX to more models beyond LLaMA-2, i.e., Phi-22 and Mixtral-8x7B (Jiang et al.,\n2024). The results in Table 2 further support the general effectiveness of CLEX, where the context\nlengths are extrapolated to 4x\u223c8x training length across various LLMs.\nB.4\nABLATION ON SCALAR EMBEDDING\nThe learnable dynamic in Eq. (13) is based on the constant dynamic \u03bet from Yarn. To conduct an\nablation on the impact of \u03bet, we remove \u03bet in Eq. (13) and train the LLaMA-2-7B on 4k sequence\nlength that undergoes the same training procedure. From Figure 8, we found the training of CLEX\nwithout \u03bet would not hamper the extrapolation ability (or even better). However, as depicted in Fig-\nure 10, training with \u03bet can speed up the convergence and produce a more stable loss curve. This\nis essential for training larger LLMs on longer sequence lengths, so we integrate the \u03bet into the\nlearnable dynamics.\nB.5\nRESULTS OF LONGBENCH\nWe list the numerical results of our CLEX trained on 4k compared with a variety of baselines trained\non up to 32k, covering single-document question answering (Table 3), multi-document question an-\nswering (Table 4), summarization (Table 5), few-shot learning (Table 6), synthetic (Table 7), and\n2https://huggingface.co/microsoft/phi-2\n15\nPublished as a conference paper at ICLR 2024\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEpoch\n2.0\n2.5\n3.0\n3.5\n4.0\nLoss\nNaive-FT-16k\nPI-16k\nYarn-16k (t=16)\nCL-Scaling-16k\nALiBi-4k\nRandomPos-4k\nCLEX-4k\nCLEX-8k\nCLEX-16k\nFigure 9: The training loss curves of various baselines and our CLEX models in Table 1. We train\nall models upon LLaMA-2-7B on 2B tokens for one epoch.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEpoch\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nLoss\nCLEX-4k (w/o t)\nCLEX-4k (w/ t)\nFigure 10: The training loss curves of our CLEX-4k trained on LLaMA-2-7B with or without the\nconstant dynamics (\u03bet) in Eq. (14).\ncode completion (Table 8) tasks. Note that the average sequence length of most tasks in Long-\nBench ranges from 5k to 15k, so the samples would be truncated into the sequence length within\nthe supported context length (if necessary) for baselines, except Baichuan-13B-4k, ALiBi-7B-4k\nand CLEX-7B-4k which enables length extrapolation are evaluated with a context window of 16k.\nBaichuan-13B-4k is a model pre-trained with ALiBi on a sequence length of 4k, while ALiBi-7B-\n4k is the model trained with ALiBi that undergoes the same training procedure as our CLEX. We\nfound that both ALiBi-based models significantly underperform in the LongBench benchmark when\nevaluated with sequence length (16k) beyond their training length, indicating the challenges of their\nextrapolation ability in practical tasks. Our CLEX, however, achieves well-performing results across\nmost tasks even trained with 4k but tested with 16k length, which further reveals the superior length\nextrapolation ability of our method.\n16\nPublished as a conference paper at ICLR 2024\nModels\nNarrativeQA\nQasper\nMultiFieldQA-en\nMultiFieldQA-zh\nAVG.\nGPT-3.5-Turbo-16k\n23.6\n43.3\n52.3\n61.2\n45.1\nLlama2-7B-chat-4k\n18.7\n19.2\n36.8\n11.9\n21.65\nLongChat-v1.5-7B-32k\n16.9\n27.7\n41.4\n29.1\n28.78\nCodeLLaMA-7B-16k\n22.93\n30.69\n43.37\n31.76\n32.19\nXGen-7B-8k\n18\n18.1\n37.7\n14.8\n22.15\nInternLM-7B-8k\n12.1\n16.7\n23.4\n33.6\n21.45\nVicuna-v1.5-7B-16k\n19.4\n26.1\n38.5\n43\n31.75\nBaichuan-13B-4k\n0.07\n17.55\n17.28\n38.55\n18.36\nALiBi-7B-4k\n0.04\n8.13\n17.87\n2.89\n7.23\nCLEX-7B-4k\n18.05\n23.68\n44.62\n31.15\n29.38\nTable 3: Single-document question answering tasks in LongBench.\nModel\nHotpotQA\n2WikiMQA\nMusique\nDuReader\nAVG.\nGPT-3.5-Turbo-16k\n51.6\n37.7\n26.9\n28.7\n36.23\nLlama2-7B-chat-4k\n25.4\n32.8\n9.4\n5.2\n18.2\nLongChat-v1.5-7B-32k\n31.5\n20.6\n9.7\n19.5\n20.33\nCodeLLaMA-7B-16k\n33.05\n27.93\n14.2\n10.78\n21.49\nXGen-7B-8k\n29.7\n21.1\n10.3\n11\n18.02\nInternLM-7B-8k\n28.7\n22.8\n9\n11.1\n17.9\nVicuna-v1.5-7B-16k\n25.3\n20.8\n9.8\n19.3\n18.8\nBaichuan-13B-4k\n3.29\n15\n0.1\n8.77\n6.79\nALiBi-7B-4k\n2.73\n8\n1.33\n11.87\n5.98\nCLEX-7B-4k\n28.44\n19.53\n9.15\n23.21\n20.08\nTable 4: Multi-document question answering tasks in LongBench.\nModel\nGovReport\nQMSum\nMultiNews\nVCSUM\nAVG.\nGPT-3.5-Turbo-16k\n29.5\n23.4\n26.7\n16\n23.9\nLlama2-7B-chat-4k\n27.3\n20.8\n25.8\n0.2\n18.525\nLongChat-v1.5-7B-32k\n30.8\n22.7\n26.4\n9.9\n22.45\nCodeLLaMA-7B-16k\n28.43\n24.18\n26.84\n0.79\n20.06\nXGen-7B-8k\n27.3\n20.5\n26.2\n2.2\n19.05\nInternLM-7B-8k\n9.7\n15.9\n22.8\n12.4\n15.2\nVicuna-v1.5-7B-16k\n27.9\n22.8\n27.2\n15.1\n23.25\nBaichuan-13B-4k\n6.8\n1.71\n23.1\n8.09\n9.925\nALiBi-7B-4k\n5.31\n1.64\n19.38\n3.25\n7.395\nCLEX-7B-4k\n32.52\n22.9\n25.55\n12.03\n23.25\nTable 5: Summarization tasks in LongBench.\n17\nPublished as a conference paper at ICLR 2024\nModel\nTREC\nTriviaQA\nSAMSum\nLSHT\nAVG.\nGPT-3.5-Turbo-16k\n68\n91.4\n41.7\n29.2\n57.575\nLlama2-7B-chat-4k\n61.5\n77.8\n40.7\n19.8\n49.95\nLongChat-v1.5-7B-32k\n63.5\n82.3\n34.2\n23.2\n50.8\nXGen-7B-8k\n65.5\n77.8\n25.3\n20.5\n47.275\nCodeLLaMA-7B-16k\n70\n84.97\n43.43\n32.5\n57.725\nInternLM-7B-8k\n52\n77.8\n21.2\n15.2\n41.55\nVicuna-v1.5-7B-16k\n71.5\n86.2\n40.8\n28.8\n56.825\nBaichuan-13B-4k\n20.05\n20.06\n5.77\n1\n11.72\nALiBi-7B-4k\n9.25\n8.83\n4.67\n0\n5.6875\nCLEX-7B-4k\n68\n84.92\n42.82\n28.35\n56.0225\nTable 6: Few-shot learning tasks in LongBench.\nPassage Count\nPassageRetrieval-en\nPassageRetrieval-zh\nAVG.\nGPT-3.5-Turbo-16k\n4.5\n71\n77.5\n51\nLlama2-7B-chat-4k\n2.1\n9.8\n0.5\n4.13\nLongChat-v1.5-7B-32k\n1\n30.5\n7.6\n13.03\nCodeLLaMA-7B-16k\n2\n13.5\n11.25\n8.92\nXGen-7B-8k\n2.1\n8.5\n3.5\n4.7\nInternLM-7B-8k\n3\n6\n0.9\n3.3\nVicuna-v1.5-7B-16k\n6.5\n4.5\n5\n5.33\nBaichuan-13B-4k\n0.06\n0.5\n5\n1.85\nALiBi-7B-4k\n0\n1.27\n0.75\n0.67\nCLEX-7B-4k\n0\n11.5\n17.5\n9.67\nTable 7: Synthetic tasks in LongBench.\nLCC\nRepoBench-P\nAVG\nGPT-3.5-Turbo-16k\n54.7\n53.6\n54.15\nLlama2-7B-chat-4k\n52.4\n43.8\n48.1\nLongChat-v1.5-7B-32k\n53\n55.3\n54.15\nCodeLLaMA-7B-16k\n64.35\n55.87\n60.11\nXGen-7B-8k\n38.6\n38.6\n38.6\nInternLM-7B-8k\n44.1\n28.8\n36.45\nVicuna-v1.5-7B-16k\n51\n43.5\n47.25\nBaichuan-13B-4k\n47.98\n16.58\n32.28\nALiBi-7B-4k\n46.69\n18.54\n32.61\nCLEX-7B-4k\n59.01\n56.87\n57.94\nTable 8: Code completion tasks in LongBench.\n18\n"
  },
  {
    "title": "Detecting Pretraining Data from Large Language Models",
    "link": "https://arxiv.org/pdf/2310.16789.pdf",
    "upvote": "9",
    "text": "Published as a conference paper at ICLR 2024\nDETECTING PRETRAINING DATA FROM LARGE LAN-\nGUAGE MODELS\nWeijia Shi1 \u2217\nAnirudh Ajith2\u2217\nMengzhou Xia2\nYangsibo Huang2\nDaogao Liu1\nTerra Blevins1\nDanqi Chen2\nLuke Zettlemoyer1\n1University of Washington\n2Princeton University\nswj0419.github.io/detect-pretrain.github.io\nABSTRACT\nAlthough large language models (LLMs) are widely deployed, the data used to\ntrain them is rarely disclosed. Given the incredible scale of this data, up to trillions\nof tokens, it is all but certain that it includes potentially problematic text such as\ncopyrighted materials, personally identifiable information, and test data for widely\nreported reference benchmarks. However, we currently have no way to know which\ndata of these types is included or in what proportions. In this paper, we study the\npretraining data detection problem: given a piece of text and black-box access\nto an LLM without knowing the pretraining data, can we determine if the model\nwas trained on the provided text? To facilitate this study, we introduce a dynamic\nbenchmark WIKIMIA that uses data created before and after model training to\nsupport gold truth detection. We also introduce a new detection method MIN-K%\nPROB based on a simple hypothesis: an unseen example is likely to contain a\nfew outlier words with low probabilities under the LLM, while a seen example\nis less likely to have words with such low probabilities. MIN-K% PROB can\nbe applied without any knowledge about the pretraining corpus or any additional\ntraining, departing from previous detection methods that require training a reference\nmodel on data that is similar to the pretraining data. Moreover, our experiments\ndemonstrate that MIN-K% PROB achieves a 7.4% improvement on WIKIMIA over\nthese previous methods. We apply MIN-K% PROB to three real-world scenarios,\ncopyrighted book detection, contaminated downstream example detection and\nprivacy auditing of machine unlearning, and find it a consistently effective solution.\n1\nINTRODUCTION\nAs the scale of language model (LM) training corpora has grown, model developers (e.g, GPT-\n4 (Brown et al., 2020a) and LLaMA 2 (Touvron et al., 2023b)) have become reluctant to disclose the\nfull composition or sources of their data. This lack of transparency poses critical challenges to scien-\ntific model evaluation and ethical deployment. Critical private information may be exposed during\npretraining; previous work showed that LLMs generated excerpts from copyrighted books (Chang\net al., 2023) and personal emails (Mozes et al., 2023), potentially infringing upon the legal rights\nof original content creators and violating their privacy. Additionally, Sainz et al. (2023); Magar &\nSchwartz (2022); Narayanan (2023) showed that the pretraining corpus may inadvertently include\nbenchmark evaluation data, making it difficult to assess the effectiveness of these models.\nIn this paper, we study the pretraining data detection problem: given a piece of text and black-box\naccess to an LLM with no knowledge of its pretraining data, can we determine if the model was\npretrained on the text? We present a benchmark, WIKIMIA, and an approach, MIN-K% PROB, for\npretraining data detection. This problem is an instance of Membership Inference Attacks (MIAs),\nwhich was initially proposed by Shokri et al. (2016). Recent work has studied fine-tuning data\ndetection (Song & Shmatikov, 2019; Shejwalkar et al., 2021; Mahloujifar et al., 2021) as an MIA\nproblem. However, adopting these methods to detect the pertaining data of contemporary large LLMs\npresents two unique technical challenges: First, unlike fine-tuning which usually runs for multiple\nepochs, pretraining uses a much larger dataset but exposes each instance only once, significantly\n\u2217Equal contribution\n1\narXiv:2310.16789v3  [cs.CL]  9 Mar 2024\nPublished as a conference paper at ICLR 2024\nFigure 1: Overview of MIN-K% PROB. To determine whether a text X is in the pretraining data of\na LLM such as GPT, MIN-K% PROB first gets the probability for each token in X, selects the k%\ntokens with minimum probabilities and calculates their average log likelihood. If the average log\nlikelihood is high, the text is likely in the pretraining data.\nreducing the potential memorization required for successful MIAs (Leino & Fredrikson, 2020;\nKandpal et al., 2022). Besides, previous methods often rely on one or more reference models (Carlini\net al., 2022; Watson et al., 2022) trained in the same manner as the target model (e.g., on the shadow\ndata sampled from the same underlying pretraining data distribution) to achieve precise detection.\nThis is not possible for large language models, as the training distribution is usually not available and\ntraining would be too expensive.\nOur first step towards addressing these challenges is to establish a reliable benchmark. We introduce\nWIKIMIA, a dynamic benchmark designed to periodically and automatically evaluate detection\nmethods on any newly released pretrained LLMs. By leveraging the Wikipedia data timestamp and\nthe model release date, we select old Wikipedia event data as our member data (i.e, seen data during\npretraining) and recent Wikipedia event data (e.g., after 2023) as our non-member data (unseen). Our\ndatasets thus exhibit three desirable properties: (1) Accurate: events that occur after LLM pretraining\nare guaranteed not to be present in the pretraining data. The temporal nature of events ensures that\nnon-member data is indeed unseen and not mentioned in the pretraining data. (2) General: our\nbenchmark is not confined to any specific model and can be applied to various models pretrained\nusing Wikipedia (e.g., OPT, LLaMA, GPT-Neo) since Wikipedia is a commonly used pretraining data\nsource. (3) Dynamic: we will continually update our benchmark by gathering newer non-member\ndata (i.e., more recent events) from Wikipedia since our data construction pipeline is fully automated.\nMIA methods for finetuning (Carlini et al., 2022; Watson et al., 2022) usually calibrate the target\nmodel probabilities of an example using a shadow reference model that is trained on a similar\ndata distribution. However, these approaches are impractical for pretraining data detection due to\nthe black-box nature of pretraining data and its high computational cost. Therefore, we propose\na reference-free MIA method MIN-K% PROB. Our method is based on a simple hypothesis: an\nunseen example tends to contain a few outlier words with low probabilities, whereas a seen example\nis less likely to contain words with such low probabilities. MIN-K% PROB computes the average\nprobabilities of outlier tokens. MIN-K% PROB can be applied without any knowledge about the\npretrainig corpus or any additional training, departing from existing MIA methods, which rely on\nshadow reference models (Mattern et al., 2023; Carlini et al., 2021). Our experiments demonstrate\nthat MIN-K% PROB outperforms the existing strongest baseline by 7.4% in AUC score on WIKIMIA.\nFurther analysis suggests that the detection performance correlates positively with the model size and\ndetecting text length.\nTo verify the applicability of our proposed method in real-world settings, we perform three case\nstudies: copyrighted book detection (\u00a75), privacy auditing of LLMs (\u00a77) and dataset contamination\ndetection (\u00a76). We find that MIN-K% PROB significantly outperforms baseline methods in both\nscenarios. From our experiments on copyrighted book detection, we see strong evidence that\nGPT-3 1 is pretrained on copyrighted books from the Books3 dataset (Gao et al., 2020; Min et al.,\n2023). From our experiments on privacy auditing of machine unlearning, we use MIN-K% PROB\n1text-davinci-003.\n2\nPublished as a conference paper at ICLR 2024\nto audit an unlearned LLM that is trained to forget copyrighted books using machine unlearning\ntechniques (Eldan & Russinovich, 2023) and find such model could still output related copyrighted\ncontent. Furthermore, our controlled study on dataset contamination detection sheds light on the\nimpact of pretraining design choices on detection difficulty; we find detection becomes harder when\ntraining data sizes increase, and occurrence frequency of the detecting example and learning rates\ndecreases.\n2\nPRETRAININING DATA DETECTION PROBLEM\nWe study pretraining data detection, the problem of detecting whether a piece of text is part of the\ntraining data. First, we formally define the problem and describe its unique challenges that are\nnot present in prior finetuning data detection studies (\u00a72.1). We then curate WIKIMIA, the first\nbenchmark for evaluating methods of pretraining data detection (\u00a72.2).\n2.1\nPROBLEM DEFINITION AND CHALLENGES\nWe follow the standard definition of the membership inference attack (MIA) by Shokri et al. (2016);\nMattern et al. (2023). Given a language model f\u03b8 and its associated pretraining data D = {zi}i\u2208[n]\nsampled from an underlying distribution D, the task objective is to learn a detector h that can infer\nthe membership of an arbitrary data point x: h(x, f\u03b8) \u2192 {0, 1}. We follow the standard setup of\nMIA, assuming that the detector has access to the LM only as a black box, and can compute token\nprobabilities for any data point x.\nChallenge 1: Unavailability of the pretraining data distribution.\nExisting state-of-art MIA\nmethods for data detection during finetuning (Long et al., 2018; Watson et al., 2022; Mireshghallah\net al., 2022a) typically use reference models g\u03b3 to compute the background difficulty of the data point\nand to calibrate the output probability of the target language model : h(x, f\u03b8, g\u03b3) \u2192 {0, 1}. Such\nreference models usually share the same model architecture as f\u03b8 and are trained on shadow data\nDshadow \u2282 D (Carlini et al., 2022; Watson et al., 2022), which are sampled from the same underlying\ndistribution D. These approaches assume that the detector can access (1) the distribution of the target\nmodel\u2019s training data, and (2) a sufficient number of samples from D to train a calibration model.\nHowever, this assumption of accessing the distribution of pretraining training data is not realistic\nbecause such information is not always available (e.g., not released by model developers (Touvron\net al., 2023b; OpenAI, 2023)). Even if access were possible, pretraining a reference model on it would\nbe extremely computationally expensive given the incredible scale of pretraining data. In summary,\nthe pretraining data detection problem aligns with the MIA definition but includes an assumption that\nthe detector has no access to pretraining data distribution D.\nChallenge 2: Detection difficulty.\nPretraining and finetuning differ significantly in the amount\nof data and compute used, as well as in optimization setups like training epochs and learning rate\nschedules. These factors significantly impact detection difficulty. One might intuitively deduce that\ndetection becomes harder when dataset sizes increase, and the training epochs and learning rates\ndecrease. We briefly describe some theoretical evidence that inform these intuitions in the following\nand show empirical results that support these hypotheses in \u00a76.\nTo illustrate, given an example z \u2208 D, we denote the model output as f\u03b8(z) Now, take another\nexample y sampled from D \\ D (not part of the pretraining data). Determining whether an example\nx was part of the training set becomes challenging if the outputs f\u03b8(z) and f\u03b8(y) are similar. The\ndegree of similarity between f\u03b8(z) and f\u03b8(y) can be quantified using the total variation distance.\nAccording to previous research (Hardt et al., 2016; Bassily et al., 2020), the bound on this total\nvariation distance between f\u03b8(z) and f\u03b8(y) is directly proportional to the occurrence frequency of\nthe example x, learning rates, and the inverse of dataset size, which implies the detection difficulty\ncorrelates with these factors as well.\n3\nPublished as a conference paper at ICLR 2024\n2.2\nWIKIMIA: A DYNAMIC EVALUATION BENCHMARK\nWe construct our benchmark by using events added to Wikipedia after specific dates, treating them as\nnon-member data since they are guaranteed not to be present in the pretraining data, which is the key\nidea behind our benchmark.\nData construction.\nWe collect recent event pages from Wikipedia. Step 1: We set January 1,\n2023 as the cutoff date, considering events occurring post-2023 as recent events (non-member data).\nWe used the Wikipedia API to automatically retrieve articles and applied two filtering criteria: (1)\nthe articles must belong to the event category, and (2) the page must be created post 2023. Step 2:\nFor member data, we collected articles created before 2017 because many pretrained models, e.g.,\nLLaMA, GPT-NeoX and OPT, were released after 2017 and incorporate Wikipedia dumps into their\npretraining data. Step 3: Additionally, we filtered out Wikipedia pages lacking meaningful text, such\nas pages titled \"Timeline of ...\" or \"List of ...\". Given the limited number of events post-2023, we\nultimately collected 394 recent events as our non-member data, and we randomly selected 394 events\nfrom pre-2016 Wikipedia pages as our member data. The data construction pipeline is automated,\nallowing for the curation of new non-member data for future cutoff dates.\nBenchmark setting.\nIn practice, LM users may need to detect texts that are paraphrased and edited,\nas well. Previous studies employing MIA have exclusively focused on detecting examples that exactly\nmatch the data used during pretraining. It remains an open question whether MIA methods can\nbe employed to identify paraphrased examples that convey the same meaning as the original. In\naddition to the verbatim setting (original), we therefore introduce a paraphrase setting we leverage\nChatGPT2 to paraphrase the examples and subsequently assess if the MIA metric can effectively\nidentify semantically equivalent examples.\nMoreover, previous MIA evaluations usually mix different-length data in evaluation and report a\nsingle performance metric. However, our results reveal that data length significantly impacts the\ndifficulty of detection. Intuitively, shorter sentences are harder to detect. Consequently, different data\nlength buckets may lead to varying rankings of MIA methods. To investigate this further, we propose\na different-length setting: we truncate the Wikipedia event data into different lengths\u201432, 64, 128,\n256\u2014and separately report the MIA methods\u2019 performance for each length segment. We describe the\ndesirable properties in Appendix B.\n3\nMIN-K% PROB: A SIMPLE REFERENCE-FREE PRETRAINING DATA\nDETECTION METHOD\nWe introduce a pretraining data detection method MIN-K% PROB that leverages minimum token\nprobabilities of a text for detection. MIN-K% PROB is based on the hypothesis that a non-member\nexample is more likely to include a few outlier words with high negative log-likelihood (or low\nprobability), while a member example is less likely to include words with high negative log-likelihood.\nConsider a sequence of tokens in a sentence, denoted as x = x1, x2, ..., xN, the log-likelihood of a\ntoken, xi, given its preceding tokens is calculated as log p(xi|x1, ..., xi\u22121). We then select the k%\nof tokens from x with the minimum token probability to form a set, Min-K%(x), and compute the\naverage log-likelihood of the tokens in this set:\nMIN-K% PROB(x) = 1\nE\nX\nxi\u2208Min-K%(x)\nlog p(xi|x1, ..., xi\u22121).\n(1)\nwhere E is the size of the Min-K%(x) set. We can detect if a piece of text was included in pretraining\ndata simply by thresholding this MIN-K% PROB result. We summarize our method in Algorithm 1 in\nAppendix B.\n2OpenAI. https://chat.openai.com/chat\n4\nPublished as a conference paper at ICLR 2024\n4\nEXPERIMENTS\nWe evaluate the performance of MIN-K% PROB and baseline detection methods against LMs such as\nLLaMA Touvron et al. (2023a), GPT-Neo (Black et al., 2022), and Pythia (Biderman et al., 2023) on\nWIKIMIA.\n4.1\nDATASETS AND METRICS\nOur experiments use WIKIMIA of different lengths (32, 64, 128, 256), original and paraphrase\nsettings. Following (Carlini et al., 2022; Mireshghallah et al., 2022a), we evaluate the effectiveness\nof a detection method using the True Positive Rate (TPR) and its False Positive Rate (FPR). We plot\nthe ROC curve to measure the trade-off between the TPR and FPR and report the AUC score (the\narea under ROC curve) and TPR at low FPRs (TPR@5%FPR) as our metrics.\n4.2\nBASELINE DETECTION METHODS\nWe take existing reference-based and reference-free MIA methods as our baseline methods and\nevaluate their performance on WIKIMIA. These methods only consider sentence-level probability.\nSpecifically, we use the LOSS Attack method (Yeom et al., 2018a), which predicts the membership\nof an example based on the loss of the target model when fed the example as input. In the context\nof LMs, this loss corresponds to perplexity of the example (PPL). Another method we consider\nis the neighborhood attack (Mattern et al., 2023), which leverages probability curvature to detect\nmembership (Neighbor). This approach is identical to the DetectGPT (Mitchell et al., 2023) method\nrecently proposed for classifying machine-generated vs. human-written text. Finally, we compare with\nmembership inference methods proposed in (Carlini et al., 2021), including comparing the example\nperplexity to zlib compression entropy (Zlib), to the lowercased example perplexity (Lowercase) and\nto example perplexity under a smaller model pretrained on the same data (Smaller Ref). For the\nsmaller reference model setting, we employ LLaMA-7B as the smaller model for LLaMA-65B and\nLLaMA-30B, GPT-Neo-125M for GPT-NeoX-20B, OPT-350M for OPT-66B and Pythia-70M for\nPythia-2.8B.\n4.3\nIMPLEMENTATION AND RESULTS\nImplementation details.\nThe key hyperparameter of MIN-K% PROB is the percentage of tokens\nwith the highest negative log-likelihood we select to form the top-k% set. We performed a small\nsweep over 10, 20, 30, 40, 50 on a held-out validation set using the LLAMA-60B model and found\nthat k = 20 works best. We use this value for all experiments without further tuning. As we report\nthe AUC score as our metric, we don\u2019t need to determine the threshold \u03f5.\nMain results.\nWe compare MIN-K% PROB and baseline methods in Table 1. Our experiments show\nthat MIN-K% PROB consistently outperforms all baseline methods across diverse target language\nmodels, both in original and paraphrase settings. MIN-K% PROB achieves an AUC score of 0.72\non average, marking a 7.4% improvement over the best baseline method (i.e., PPL). Among the\nbaselines, the simple LOSS Attack (PPL) outperforms the others. This demonstrates the effectiveness\nand generalizability of MIN-K% PROB in detecting pretraining data from various LMs. Further\nresults such as TPR@5%FPR can be found in Appendix A, which shows a trend similar to Table 6.\n4.4\nANALYSIS\nWe further delve into the factors influencing detection difficulty, focusing on two aspects: (1) the size\nof the target model, and (2) the length of the text.\nModel size.\nWe evaluate the performance of reference-free methods on detecting pretraining 128-\nlength texts from different-sized LLaMA models (7, 13, 30, 65B). Figure 2a demonstrates a noticeable\ntrend: the AUC score of the methods rises with increasing model size. This is likely because larger\nmodels have more parameters and thus are more likely to memorize the pretraining data.\n5\nPublished as a conference paper at ICLR 2024\n(a) AUC score vs. model size\n(b) AUC score vs. text length\nFigure 2: As model size or text length increases, detection becomes easier.\nLength of text.\nIn another experiment, we evaluate the detection method performance on examples\nof varying lengths in the original setting. As shown in Figure 2b, the AUC score of different methods\nincreases as text length increases, likely because longer texts contain more information memorized\nby the target model, making them more distinguishable from the unseen texts.\nTable 1: AUC score for detecting pretraining examples from the given model on WIKIMIA for MIN-\nK% PROB and baselines. Ori. and Para. denote the original and paraphrase settings, respectively.\nBold shows the best AUC within each column.\nPythia-2.8B\nNeoX-20B\nLLaMA-30B\nLLaMA-65B\nOPT-66B\nMethod\nOri.\nPara.\nOri.\nPara.\nOri.\nPara.\nOri.\nPara.\nOri.\nPara.\nAvg.\nNeighbor\n0.61\n0.59\n0.68\n0.58\n0.71\n0.62\n0.71\n0.69\n0.65\n0.62\n0.65\nPPL\n0.61\n0.61\n0.70\n0.70\n0.70\n0.70\n0.71\n0.72\n0.66\n0.64\n0.67\nZlib\n0.65\n0.54\n0.72\n0.62\n0.72\n0.64\n0.72\n0.66\n0.67\n0.57\n0.65\nLowercase\n0.59\n0.60\n0.68\n0.67\n0.59\n0.54\n0.63\n0.60\n0.59\n0.58\n0.61\nSmaller Ref\n0.60\n0.58\n0.68\n0.65\n0.72\n0.64\n0.74\n0.70\n0.67\n0.64\n0.66\nMIN-K% PROB\n0.67\n0.66\n0.76\n0.74\n0.74\n0.73\n0.74\n0.74\n0.71\n0.69\n0.72\nIn the following two sections, we apply MIN-K% PROB to real-world scenarios to detect copyrighted\nbooks and contaminated downstream tasks within LLMs.\n5\nCASE STUDY: DETECTING COPYRIGHTED BOOKS IN PRETRAINING DATA\nMIN-K% PROB can also detect potential copyright infringement in training data, as we show in this\nsection. Specifically, we use MIN-K% PROB to detect excerpts from copyrighted books in the Books3\nsubset of the Pile dataset (Gao et al., 2020) that may have been included in the GPT-33 training data.\n5.1\nEXPERIMENTAL SETUP\nValidation data to determine detection threshold.\nWe construct a validation set using 50 books\nknown to be memorized by ChatGPT, likely indicating their presence in its training data (Chang et al.,\n2023), as positive examples. For negative examples, we collected 50 new books with first editions\nin 2023 that could not have been in the training data. From each book, we randomly extract 100\nsnippets of 512 words, creating a balanced validation set of 10,000 examples. We determine the\noptimal classification threshold with MIN-K% PROB by maximizing detection accuracy on this set.\nTest data and metrics.\nWe randomly select 100 books from the Books3 corpus that are known to\ncontain copyrighted contents (Min et al., 2023). From each book, we extract 100 random 512-word\nsnippets, creating a test set of 10,000 excerpts. We apply the threshold to decide if these books\nsnippets have been trained with GPT-3. We then report the percentage of these snippets in each book\n(i.e., contamination rate) that are identified as being part of the pre-training data.\n3text-davinci-003\n6\nPublished as a conference paper at ICLR 2024\n5.2\nRESULTS\nFigure 3 shows MIN-K% PROB achieves an AUC of 0.88, outperforming baselines in detecting\ncopyrighted books. We apply the optimal threshold of MIN-K% PROB to the test set of 10,000\nsnippets from 100 books from Books3. Table 2 represents the top 20 books with the highest predicted\ncontamination rates. Figure 4 reveals nearly 90% of the books have an alarming contamination rate\nover 50%.\nMethod\nBook\nNeighbor\n0.75\nPPL\n0.84\nZlib\n0.81\nLowercase\n0.80\nMIN-K% PROB\n0.88\nFigure 3: AUC scores for detecting the vali-\ndation set of copyrighted books on GPT-3.\nFigure 4: Distribution of detected contamination\nrate of 100 copyrighted books.\nTable 2: Top 20 copyrighted books in GPT-3\u2019s pretraining data. The listed contamination rate\nrepresents the percentage of text excerpts from each book identified in the pretraining data.\nContamination %\nBook Title\nAuthor\nYear\n100\nThe Violin of Auschwitz\nMaria \u00c0ngels Anglada\n2010\n100\nNorth American Stadiums\nGrady Chambers\n2018\n100\nWhite Chappell Scarlet Tracings\nIain Sinclair\n1987\n100\nLost and Found\nAlan Dean\n2001\n100\nA Different City\nTanith Lee\n2015\n100\nOur Lady of the Forest\nDavid Guterson\n2003\n100\nThe Expelled\nMois Benarroch\n2013\n99\nBlood Cursed\nArcher Alex\n2013\n99\nGenesis Code: A Thriller of the Near Future\nJamie Metzl\n2014\n99\nThe Sleepwalker\u2019s Guide to Dancing\nMira Jacob\n2014\n99\nThe Harlan Ellison Hornbook\nHarlan Ellison\n1990\n99\nThe Book of Freedom\nPaul Selig\n2018\n99\nThree Strong Women\nMarie NDiaye\n2009\n99\nThe Leadership Mind Switch: Rethinking How We Lead in the\nNew World of Work\nD. A. Benton, Kylie Wright-Ford\n2017\n99\nGold\nChris Cleave\n2012\n99\nThe Tower\nSimon Clark\n2005\n98\nAmazon\nBruce Parry\n2009\n98\nAin\u2019t It Time We Said Goodbye: The Rolling Stones on the\nRoad to Exile\nRobert Greenfield\n2014\n98\nPage One\nDavid Folkenflik\n2011\n98\nRoad of Bones: The Siege of Kohima 1944\nFergal Keane\n2010\n6\nCASE STUDY: DETECTING DOWNSTREAM DATASET CONTAMINATION\nAssessing the leakage of downstream task data into pretraining corpora is an important issue, but it is\nchallenging to address given the lack of access to pretraining datasets. In this section, we investigate\nthe possibility of using MIN-K% PROB to detect information leakage and perform ablation studies\nto understand how various training factors impact detection difficulty. Specifically, we continually\npretrain the 7B parameter LLaMA model (Touvron et al., 2023a) on pretraining data that have been\npurposefully contaminated with examples from the downstream task.\n6.1\nEXPERIMENTS\nExperimental setup.\nTo simulate downstream task contamination that could occur in real-world\nsettings, we create contaminated pretraining data by inserting examples from downstream tasks into a\npretraining corpus. Specifically, we sample text from the RedPajama corpus (TogetherCompute, 2023)\nand insert formatted examples from the downstream datasets BoolQ (Clark et al., 2019), IMDB (Maas\net al., 2011), Truthful QA (Lin et al., 2021), and Commonsense QA (Talmor et al., 2019) in contiguous\nsegments at random positions in the uncontaminated text. We insert 200 (positive) examples from each\nof these datasets into the pretraining data while also isolating a set of 200 (negative) examples from\n7\nPublished as a conference paper at ICLR 2024\neach dataset that are known to be absent from the contaminated corpus. This creates a contaminated\npretraining dataset containing 27 million tokens with 0.1% drawn from downstream datasets.\nWe evaluate the effectiveness of MIN-K% PROB at detecting leaked benchmark examples by com-\nputing AUC scores over these 400 examples on a LLaMA 7B model finetuned for one epoch on our\ncontaminated pretraining data at a constant learning rate of 1e-4.\nMain results.\nWe present the main attack results in Table 3. We find that MIN-K% PROB out-\nperforms all baselines. We report TPR@5%FPR in Table 7 in Appendix A, where MIN-K% PROB\nshows 12.2% improvement over the best baseline.\nTable 3: AUC scores for detecting contaminant downstream examples. Bold shows the best AUC\nscore within each column.\nMethod\nBoolQ\nCommonsense QA\nIMDB\nTruthful QA\nAvg.\nNeighbor\n0.68\n0.56\n0.80\n0.59\n0.66\nZlib\n0.76\n0.63\n0.71\n0.63\n0.68\nLowercase\n0.74\n0.61\n0.79\n0.56\n0.68\nPPL\n0.89\n0.78\n0.97\n0.71\n0.84\nMIN-K% PROB\n0.91\n0.80\n0.98\n0.74\n0.86\n6.2\nRESULTS AND ANALYSIS\nThe simulation with contaminated datasets allows us to perform ablation studies to empirically\nanalyze the effects of dataset size, frequency of data occurrence, and learning rate on detection\ndifficulty, as theorized in section 2.1. The empirical results largely align with and validate the\ntheoretical framework proposed. In summary, we find that detection becomes more challenging as\ndata occurrence and learning rate decreases, and the effect of dataset size on detection difficulty\ndepends on whether the contaminants are outliers relative to the distribution of the pretraining data.\nPretraining dataset size.\nWe construct contaminated datasets of 0.17M, 0.27M, 2.6M and 26M\ntokens by mixing fixed downstream examples (200 examples per downstream task) with varying\namounts of RedPajama data, mimicking real-world pretraining. Despite the theory suggesting greater\ndifficulty with more pretraining data, Figure 5a shows AUC scores counterintuitively increase with\npre-training dataset size. This aligns with findings that LMs better memorize tail outliers (Feldman,\n2020; Zhang et al., 2021). With more RedPajama tokens in the constructed dataset, downstream\nexamples become more significant outliers. We hypothesize that their enhanced memorization likely\nenables easier detection with perplexity-based metrics.\nTo verify the our hypothesis, we construct control data where contaminants are not outliers. We\nsample Real Time Data News August 20234, containing post-2023 news absent from LLaMA pre-\ntraining. We create three synthetic corpora by concatenating 1000, 5000 and 10000 examples from\nthis corpus, hence creating corpora of sizes 0.77M, 3.9M and 7.6M tokens respecitvely. In each\nsetting, we consider 100 of these examples to be contaminant (positive) examples and set aside\nanother set of 100 examples from News August 2023 (negative). Figure 5b shows AUC scores\ndecrease as the dataset size increases.\nDetection of outlier contaminants like downstream examples gets easier as data size increases, since\nmodels effectively memorize long-tail samples. However, detecting general in-distribution samples\nfrom the pretraining data distribution gets harder with more data, following theoretical expectations.\nData occurrence.\nTo study the relationship between detection difficulty and data occurrence, we\nconstruct a contaminated pretraining corpus by inserting multiple copies of each downstream data\npoint into a pre-training corpus, where the occurrence of each example follows a Poisson distribution.\nWe measure the relationship between the frequency of the example in the pretraining data and its\nAUC scores. Figure 5c shows that AUC scores positively correlates with the occurrence of examples.\n4https://huggingface.co/datasets/RealTimeData/News_August_2023\n8\nPublished as a conference paper at ICLR 2024\n(a) Outlier contaminants, e.g., down-\nstream examples, become easier to\ndetect as dataset size increases.\n(b) In-distribution contaminants,\ne.g., news articles, are harder to de-\ntect as dataset size increases.\n(c) Contaminants that occur more\nfrequently in the dataset are easier\nto detect.\nFigure 5: We show the effect of contamination rate (expressed as a percentage of the total number of\npretraining tokens) and occurrence frequency on the ease of detection of data contaminants using\nMIN-K% PROB.\nLearning rate.\nWe also study the effect of varying the learning rates used during pretraining on\nthe detection statistics of the contaminant examples (see Table 4). We find that raising the learning\nrate from 10\u22125 to 10\u22124 increases AUC scores significantly in all the downstream tasks, implying\nthat higher learning rates cause models to memorize their pretraining data more strongly. A more\nin-depth analysis in Table 8 in Appendix A demonstrates that a higher learning rate leads to more\nmemorization rather than generalization for these downstream tasks.\nTable 4: AUC scores for detecting contaminant downstream examples using two different learning\nrates. Detection becomes easier when higher learning rates are used during training. Bold shows the\nbest AUC score within each column.\nLearning rate\nBoolQ\nCommonsense QA\nIMDB\nLSAT QA\nTruthful QA\n1 \u00d7 10\u22125\n0.64\n0.59\n0.76\n0.72\n0.56\n1 \u00d7 10\u22124\n0.91\n0.80\n0.98\n0.82\n0.74\n7\nCASE STUDY: PRIVACY AUDITING OF MACHINE UNLEARNING\nWe also demonstrate that our proposed technique can effectively address the need for auditing\nmachine unlearning, ensuring compliance with privacy regulations (Figure 6).\n7.1\nBACKGROUNDING\nThe right to be forgotten and machine unlearning.\nIn today\u2019s landscape of machine learning\nsystems, it is imperative to uphold individuals\u2019 \u201cright to be forgotten\u201d, a legal obligation outlined\nin regulations such as the General Data Protection Regulation (GDPR) (Voigt & Von dem Bussche,\n2017) and the California Consumer Privacy Act (CCPA) (Legislature, 2018). This requirement allows\nusers to request the removal of their data from trained models. To address this need, the concept of\nmachine unlearning has emerged as a solution for purging data from machine learning models, and\nvarious machine unlearning methods have been introduced (Ginart et al., 2019; Liu et al., 2020; Wu\net al., 2020; Bourtoule et al., 2021; Izzo et al., 2021; Sekhari et al., 2021; Gupta et al., 2021; Ye et al.,\n2022).\nRecently, Eldan & Russinovich (2023) introduced a novel approach for performing machine un-\nlearning on LLMs. This approach involves further fine-tuning the LLMs with alternative labels\nfor specific tokens, effectively creating a modified version of the model that no longer contains the\nto-be-unlearned content. Specifically, the authors demonstrated the efficacy of this method using\nthe LLaMA2-7B-chat model (Touvron et al., 2023b), showcasing its ability to \u201cunlearn\u201d information\nfrom the Harry Potter book series which results in the LLaMA2-7B-WhoIsHarryPotter model5. In\nthis case study, we aim to assess whether this model successfully eliminates memorized content\nrelated to the Harry Potter series.\n5Available at https://huggingface.co/microsoft/Llama2-7b-WhoIsHarryPotter.\n9\nPublished as a conference paper at ICLR 2024\nFigure 6: Auditing machine unlearning with MIN-K% PROB. Machine unlearning methods are\ndesigned to remove copyrighted and personal data from large language models. We use MIN-K%\nPROB to audit an unlearned LLM that has been trained to forget copyrighted books. However, we\nfind that such a model can still output related copyrighted content.\n7.2\nEXPERIMENTS\nTo\nextract\nthe\ncontents\nrelated\nto\nHarry\nPotter\nfrom\nthe\nunlearned\nmodel,\nLLaMA2-7B-WhoIsHarryPotter, we consider two settings:\nstory completion (\u00a77.2.1) and\nquestion answering (\u00a77.2.2). In story completion, we identify suspicious chunks from the original\nHarry Potter books using MIN-K% PROB. We then use the unlearned model to generate completions\nand compare them with the gold continuation. In question answering, we generate a series of\nquestions related to Harry Potter using GPT-4 6. We filter these questions using MIN-K% PROB, and\nthen use the unlearned model to produce answers. These answers are then compared with the gold\nanswers generated by GPT-4 and subsequently verified by humans.\n7.2.1\nSTORY COMPLETION\nIdentifying suspicious texts using MIN-K% PROB.\nThe process begins with the identifica-\ntion of suspicious chunks using our MIN-K% PROB metric. Firstly, we gather the plain text of\nHarry Potter Series 1 to 4 and segment these books into 512-word chunks, resulting in approxi-\nmately 1000 chunks. We then compute the MIN-K% PROB scores for these chunks using both the\nLLaMA2-7B-WhoIsHarryPotter model and the original LLaMA2-7B-chat model. To identify chunks\nwhere the unlearning process may have failed at, we compare the MIN-K% PROB scores between the\ntwo models. If the ratio of the scores from the two models falls within the range of (\n1\n1.15, 1.15), we\nclassify the chunk as a suspicious unlearn-failed chunk. This screening process identifies 188 such\nchunks. We also notice that using perplexity alone as the metric fails to identify any such chunk. We\nthen test the LLaMA2-7B-WhoIsHarryPotter model with these suspicious chunks to assess its ability\nto complete the story. For each suspicious chunk, we prompt the model with its initial 200 words and\nuse multinomial sampling to sample 20 model-generated continuations for each chunk.\nResults\nWe compare the completed stories with the ground truth storylines using both the SimCSE\nscore (Gao et al., 2021) (which gives a similarity score from 0 to 1) and GPT-4 (where we prompt the\nmodel with the template in Table 9 to return a similarity score from 1 to 5, and a reason explaining\nthe similarity). We can still find very similar completion with the original story. For example, 5.3%\ngenerated completions have greater and equal to 4 GPT score similarity to the gold completion. The\ndistributions for these two scores of the suspicious chunks are shown in Section 7.2.1. Surprisingly,\nwe find a considerable number of chunks whose auto-completions from the \u201cunlearned\u201d model closely\nresemble the original story: 10 chunks have a similarity score higher than or equal to 4 according to\n6OpenAI. https://chat.openai.com/chat\n10\nPublished as a conference paper at ICLR 2024\n(a) SimCSE score\n(b) GPT-scored similarity\nFigure 7: Distribution of the SimCSE score (a) and GPT-scored similarity (b) between the original\nstory and the completion by the LLaMA2-7B-WhoIsHarryPotter model.\nTable 5: The unlearned model LLaMA2-7B-WhoIsHarryPotter answer the questions related to Harry\nPotter correctly. We manually cross-checked these responses against the Harry Potter book series for\nverification.\nQuestion\nAnswer by\nSource in Harry Potter Book Series\nLLaMA2-7B-WhoIsHarryPotter\nGPT-4\nIn\nHarry\nPotter,\nWhat type of animal\nis Hedwig?\nHedwig is a white owl.\nHedwig is a\nwhite owl.\n\"For Harry\u2019s birthday, Hagrid buys Harry a snowy owl named\nHedwig.\" \u2013 Harry Potter and the Philosopher\u2019s Stone\nIn\nHarry\nPotter,\nWhat\ncreature\nguards the vaults at\nGringotts Bank?\nIn the Harry Roper series by\nT.L. Knighton, the creature that\nguards the vaults at Gringotts\nBank is a dragon.\nThe creature\nthat\nguards\nthe\nvaults\nat Gringotts\nBank\nis\na\ndragon.\n\"With Harry and Griphook under the Invisibility Cloak,\nHermione disguised as Bellatrix and Ron as a foreign wiz-\nard with bushy facial hair, the trio and Griphook Apparate to\nGringotts ... the trio are almost crushed by the gold and cap-\ntured, but Hermione gets them to jump on the back of a dragon\nkept as a security measure.\" \u2013 Harry Potter and the Deathly\nHallows\nIn\nHarry\nPotter,\nwhat is the name\nof\nHagrid\u2019s\ngiant\nspider friend?\nHagrid\u2019s giant spider friend is\nnamed Aragog.\nAragog\n\"\u2018Aragog!\u2019 it called. \u2018Aragog!\u2019 And from the middle of the\nmisty, domed web, a spider the size of a small elephant emerged,\nvery slowly.\" \u2013 Harry Potter and the Chamber of Secrets\nIn\nHarry\nPotter,\nwhat does the spell\n\"Alohomora\" do?\nIn the \"Magic for Good\" series by\nJohn G. Hartness, the spell \"Alo-\nhomora\" is a spell for unlocking\ndoors.\nUnlocks\ndoors.\n\"She grabbed Harry\u2019s wand, tapped the lock, and whispered,\n\"Alohomora!\" The lock clicked and the door swung open \u2013\nthey piled through it, shut it quickly...\" \u2013 Harry Potter and the\nSorcerer\u2019s Stone\nIn\nHarry\nPotter,\nwhich of the three\nUnforgivable Curses\ncauses\nunbearable\npain in the target?\nThe Unforgivable Curse that\ncauses unbearable pain in the tar-\nget is the \"Crucio\" curse.\nCrucio\n\"\u2018Crucio!\u2019 At once, the spider\u2019s legs bent in upon its body; it\nrolled over and began to twitch horribly, rocking from side to\nside. No sound came from it, but Harry was sure that if it could\nhave given voice, it would have been screaming.\" \u2013 Harry Potter\nand the Goblet of Fire\nIn\nHarry\nPotter,\nwhat magical crea-\nture is known to\nguard treasure?\nIn the magical world of Harry\nRex\u2019s adventures, the guardian of\nthe treasure is a dragon named\n\"Glimmer.\"\nDragon\n\"A gigantic dragon was tethered to the ground in front of them,\nbarring access to four or five of the deepest vaults in the place.\n\" \u2013 Harry Potter and the Deathly Hallows\nIn\nHarry\nPotter,\nwhich\nspell\nsum-\nmons objects?\nThe spell that summons objects\nin the world of Harry Potter is the\n\"Accio\" spell.\nAccio\n\"\u2018Accio! Accio! Accio!\u2019 she shouted, and toffees zoomed from\nall sorts of unlikely places, including the lining of George\u2019s\njacket...\" \u2013 Harry Potter and the Goblet of Fire\nIn\nHarry\nPotter,\nwhich spell conjures\na\nsmall\nflock\nof\nbirds?\nThe spell that conjures a small\nflock of birds in the magical\nworld of Harry Potter is the \"Avis\nSummoning Spell\".\nAvis\n\u2018Avis!\u2019 The hornbeam wand let off a blast hike a gun, and a\nnumber of small, twittering birds flew out of the end and through\nthe open window into the watery sunlight. \u2013 Harry Potter and\nthe Goblet of Fire\nthe GPT-4 evaluator. For instance, Table 10 showcases a few such examples, with all of them having\nSimCSE scores exceeding 0.7. We further note that the study only uses Harry Potter books 1 to 4.\nIncluding the whole Harry Potter series (7 books) potentially will expose more unlearn-failed chunks.\n7.2.2\nQUESTION ANSWERING\nSelecting Harry Potter-related questions with MIN-K% PROB\nWe generate 1000 questions\nrelated to Harry Potter by prompting GPT-4 with the query \"Can you give me a list of questions and\n11\nPublished as a conference paper at ICLR 2024\nanswers related to Harry Potter\". Similar to identifying suspocious texts in story completion, we\ncompare the MIN-K% PROB scores between the original and unlearned models and select questions\nwith the ratio falling within the range of (\n1\n1.15, 1.15), resulting in 103 questions. We use the unlearned\nmodel to generate answer given these questions, specifically employing multinomial sampling to\nsample 20 answers for each question.\nResults\nWe then compare the answers by the unlearned model (referred to as the \"candidate\") to\nthose provided by GPT-4 (referred to as the \"reference\") using the ROUGE-L recall measure (Lin,\n2004), which calculates the ratio: (# overlapping words between the candidate and reference) / (#\nwords in the reference). A higher ROUGE-L recall value signifies a greater degree of overlap, which\ncan indicate a higher likelihood of unlearning failure. Among the 103 selected questions, we observe\nan average ROUGE-L recall of 0.23. Conversely, for the unselected questions, the average ROUGE-L\nrecall is 0.10. These findings underscore the capability of our MIN-K% PROB to identify potentially\nunsuccessful instances of unlearning.\nTable 5 shows the selected questions related to Harry Potter that are answered correctly by the\nunlearned model LLaMA2-7B-WhoIsHarryPotter (with ROUGE-L recall being 1). We also verify\nthe generated answers by cross-checking them against the Harry Potter series. These results suggest\nthe knowledge about Harry Potter is not completely erased from the unlearned model.\n8\nRELATED WORK\nMembership inference attack in NLP.\nMembership Inference Attacks (MIAs) aim to determine\nwhether an arbitrary sample is part of a given model\u2019s training data (Shokri et al., 2017; Yeom et al.,\n2018b). These attacks pose substantial privacy risks to individuals and often serve as a basis for\nmore severe attacks, such as data reconstruction (Carlini et al., 2021; Gupta et al., 2022; Cummings\net al., 2023). Due to its fundamental association with privacy risk, MIA has more recently found\napplications in quantifying privacy vulnerabilities within machine learning models and in verifying\nthe accurate implementation of privacy-preserving mechanisms (Jayaraman & Evans, 2019; Jagielski\net al., 2020; Zanella-B\u00e9guelin et al., 2020; Nasr et al., 2021; Huang et al., 2022; Nasr et al., 2023;\nSteinke et al., 2023). Initially applied to tabular and computer vision data, the concept of MIA\nhas recently expanded into the realm of language-oriented tasks. However, this expansion has\npredominantly centered around finetuning data detection (Song & Shmatikov, 2019; Shejwalkar\net al., 2021; Mahloujifar et al., 2021; Jagannatha et al., 2021; Mireshghallah et al., 2022b). Our work\nfocuses on the application of MIA to pretraining data detection, an area that has received limited\nattention in previous research efforts.\nDataset contamination.\nThe dataset contamination issue in LMs has gained attention recently since\nbenchmark evaluation is undermined if evaluation examples are accidentally seen during pre-training.\nBrown et al. (2020b), Wei et al. (2022), and Du et al. (2022) consider an example contaminated if\nthere is a 13-gram collision between the training data and evaluation example. Chowdhery et al.\n(2022) further improves this by deeming an example contaminated if 70% of its 8-grams appear in the\ntraining data. Touvron et al. (2023b) builds on these methods by extending the framework to tokenized\ninputs and judging a token to be contaminated if it appears in any token n-gram longer than 10 tokens.\nHowever, their methods require access to retraining corpora, which is largely unavailable for recent\nmodel releases. Other approaches try to detect contamination without access to pretraining corpora.\nSainz et al. (2023) simply prompts ChatGPT to generate examples from a dataset by providing\nthe dataset\u2019s name and split. They found that the models generate verbatim instances from NLP\ndatasets. Golchin & Surdeanu (2023) extends this framework to extract more memorized instances\nby incorporating partial instance content into the prompt. Similarly, Weller et al. (2023) demonstrates\nthe ability to extract memorized snippets from Wikipedia via prompting. While these methods study\ncontamination in closed-sourced models, they cannot determine contamination on an instance level.\nMarone & Van Durme (2023) argues that model-developers should release training data membership\ntesting tools accompanying their LLMs to remedy this. However, this is not yet widely practiced.\n12\nPublished as a conference paper at ICLR 2024\n9\nCONCLUSION\nWe present a pre-training data detection dataset WIKIMIA and a new approach MIN-K% PROB.\nOur approach uses the intuition that trained data tends to contain fewer outlier tokens with very low\nprobabilities compared to other baselines. Additionally, we verify the effectiveness of our approach\nin real-world setting, we perform two case studiies: detecting dataset contamination and published\nbook detection. For dataset contamination, we observe empirical results aligning with theoretical\npredictions about how detection difficulty changes with dataset size, example frequency, and learning\nrate. Most strikingly, our book detection experiments provide strong evidence that GPT-3 models\nmay have been trained on copyrighted books.\n13\nPublished as a conference paper at ICLR 2024\nREFERENCES\nRaef Bassily, Vitaly Feldman, Crist\u00f3bal Guzm\u00e1n, and Kunal Talwar. Stability of stochastic gradient\ndescent on nonsmooth convex losses. Advances in Neural Information Processing Systems, 33:\n4381\u20134391, 2020.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan,\nMohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron,\nLintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models\nacross training and scaling, 2023.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu\nPurohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceedings of the ACL Workshop on Challenges\n& Perspectives in Creating Large Language Models, 2022. URL https://arxiv.org/abs/2204.\n06745.\nLucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers,\nBaiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium\non Security and Privacy (SP), pp. 141\u2013159. IEEE, 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,\nR. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Sys-\ntems, volume 33, pp. 1877\u20131901. Curran Associates, Inc., 2020a. URL https://proceedings.\nneurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020b.\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data\nfrom large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp.\n2633\u20132650, 2021.\nNicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer.\nMembership inference attacks from first principles. In 2022 IEEE Symposium on Security and\nPrivacy (SP), pp. 1897\u20131914. IEEE, 2022.\nKent K Chang, Mackenzie Cramer, Sandeep Soni, and David Bamman. Speak, memory: An\narchaeology of books known to chatgpt/gpt-4. arXiv preprint arXiv:2305.00118, 2023.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL,\n2019.\nRachel Cummings, Damien Desfontaines, David Evans, Roxana Geambasu, Matthew Jagielski,\nYangsibo Huang, Peter Kairouz, Gautam Kamath, Sewoong Oh, Olga Ohrimenko, et al. Challenges\ntowards the next frontier in privacy. arXiv preprint arXiv:2304.06929, 2023.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language\nmodels with mixture-of-experts. In International Conference on Machine Learning, pp. 5547\u20135569.\nPMLR, 2022.\n14\nPublished as a conference paper at ICLR 2024\nRonen Eldan and Mark Russinovich. Who\u2019s Harry Potter? approximate unlearning in LLMs. arXiv\npreprint arXiv:2310.02238, 2023.\nVitaly Feldman. Does learning require memorization? a short tale about a long tail. In Proceedings\nof the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pp. 954\u2013959, 2020.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for\nlanguage modeling. arXiv preprint arXiv:2101.00027, 2020.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence\nembeddings. In Empirical Methods in Natural Language Processing (EMNLP), 2021.\nAntonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. Making ai forget you: Data\ndeletion in machine learning. Advances in neural information processing systems, 32, 2019.\nShahriar Golchin and Mihai Surdeanu. Time travel in llms: Tracing data contamination in large\nlanguage models. arXiv preprint arXiv:2308.08493, 2023.\nSamyak Gupta, Yangsibo Huang, Zexuan Zhong, Tianyu Gao, Kai Li, and Danqi Chen. Recovering\nprivate text in federated learning of language models. Advances in Neural Information Processing\nSystems, 35:8130\u20138143, 2022.\nVarun Gupta, Christopher Jung, Seth Neel, Aaron Roth, Saeed Sharifi-Malvajerdi, and Chris Waites.\nAdaptive machine unlearning. Advances in Neural Information Processing Systems, 34:16319\u2013\n16330, 2021.\nMoritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic\ngradient descent. In International conference on machine learning, pp. 1225\u20131234. PMLR, 2016.\nYangsibo Huang, Chun-Yin Huang, Xiaoxiao Li, and Kai Li. A dataset auditing method for collabo-\nratively trained machine learning models. IEEE Transactions on Medical Imaging, 2022.\nZachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. Approximate data deletion\nfrom machine learning models. In International Conference on Artificial Intelligence and Statistics,\npp. 2008\u20132016. PMLR, 2021.\nAbhyuday Jagannatha, Bhanu Pratap Singh Rawat, and Hong Yu. Membership inference attack\nsusceptibility of clinical language models. arXiv preprint arXiv:2104.08305, 2021.\nMatthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine\nlearning: How private is private sgd? Advances in Neural Information Processing Systems, 33:\n22205\u201322216, 2020.\nBargav Jayaraman and David Evans. Evaluating differentially private machine learning in practice.\nIn 28th USENIX Security Symposium (USENIX Security 19), pp. 1895\u20131912, 2019.\nNikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks\nin language models. In International Conference on Machine Learning, pp. 10697\u201310707. PMLR,\n2022.\nCalifornia State Legislature. California consumer privacy act, 2018. URL https://oag.ca.gov/\nprivacy/ccpa.\nKlas Leino and Matt Fredrikson. Stolen memories: Leveraging model memorization for calibrated\n{White-Box} membership inference. In 29th USENIX security symposium (USENIX Security 20),\npp. 1605\u20131622, 2020.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74\u201381, 2004.\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human\nfalsehoods, 2021.\n15\nPublished as a conference paper at ICLR 2024\nGaoyang Liu, Xiaoqiang Ma, Yang Yang, Chen Wang, and Jiangchuan Liu. Federated unlearning.\narXiv preprint arXiv:2012.13891, 2020.\nYunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A\nGunter, and Kai Chen. Understanding membership inferences on well-generalized learning models.\narXiv preprint arXiv:1802.04889, 2018.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Human Language Technologies, pp. 142\u2013150,\nPortland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:\n//www.aclweb.org/anthology/P11-1015.\nInbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. ArXiv,\nabs/2203.08242, 2022. URL https://api.semanticscholar.org/CorpusID:247475929.\nSaeed Mahloujifar, Huseyin A Inan, Melissa Chase, Esha Ghosh, and Marcello Hasegawa. Member-\nship inference on word embedding and beyond. arXiv preprint arXiv:2106.11384, 2021.\nMarc Marone and Benjamin Van Durme. Data portraits: Recording foundation model training data,\n2023. URL https://arxiv.org/abs/2303.03919.\nJustus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schoelkopf, Mrinmaya Sachan,\nand Taylor Berg-Kirkpatrick. Membership inference attacks against language models via neigh-\nbourhood comparison. In Findings of the Association for Computational Linguistics: ACL 2023,\npp. 11330\u201311343, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.\n18653/v1/2023.findings-acl.719. URL https://aclanthology.org/2023.findings-acl.719.\nSewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A Smith, and Luke\nZettlemoyer. Silo language models: Isolating legal risk in a nonparametric datastore. arXiv\npreprint arXiv:2308.04430, 2023.\nFatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri.\nQuantifying privacy risks of masked language models using membership inference attacks. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp.\n8332\u20138347, Abu Dhabi, United Arab Emirates, December 2022a. Association for Computational\nLinguistics. doi: 10.18653/v1/2022.emnlp-main.570. URL https://aclanthology.org/2022.\nemnlp-main.570.\nFatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri.\nQuantifying privacy risks of masked language models using membership inference attacks. arXiv\npreprint arXiv:2203.03929, 2022b.\nEric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn.\nDetectgpt: Zero-shot machine-generated text detection using probability curvature, 2023. URL\nhttps://arxiv.org/abs/2301.11305.\nMaximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D. Griffin. Use of llms for illicit\npurposes: Threats, prevention measures, and vulnerabilities, 2023.\nArvind Narayanan. Gpt-4 and professional benchmarks: the wrong answer to the wrong question,\n2023. URL https://www.aisnakeoil.com/p/gpt-4-and-professional-benchmarks.\nMilad Nasr, Shuang Songi, Abhradeep Thakurta, Nicolas Papernot, and Nicholas Carlin. Adversary\ninstantiation: Lower bounds for differentially private machine learning. In 2021 IEEE Symposium\non security and privacy (SP), pp. 866\u2013882. IEEE, 2021.\nMilad Nasr, Jamie Hayes, Thomas Steinke, Borja Balle, Florian Tram\u00e8r, Matthew Jagielski, Nicholas\nCarlini, and Andreas Terzis. Tight auditing of differentially private machine learning. arXiv\npreprint arXiv:2302.07956, 2023.\nOpenAI. Gpt-4 technical report, 2023.\n16\nPublished as a conference paper at ICLR 2024\nOscar Sainz, Jon Ander Campos, Iker Garc\u00eda-Ferrero, Julen Etxaniz, and Eneko Agirre. Did chat-\ngpt cheat on your test?, 2023. URL https://hitz-zentroa.github.io/lm-contamination/\nblog/.\nAyush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember\nwhat you want to forget: Algorithms for machine unlearning. Advances in Neural Information\nProcessing Systems, 34:18075\u201318086, 2021.\nVirat Shejwalkar, Huseyin A Inan, Amir Houmansadr, and Robert Sim. Membership inference attacks\nagainst NLP classification models. In NeurIPS 2021 Workshop Privacy in Machine Learning,\n2021. URL https://openreview.net/forum?id=74lwg5oxheC.\nR. Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks\nagainst machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pp.\n3\u201318, 2016.\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks\nagainst machine learning models. In 2017 IEEE symposium on security and privacy (SP), pp. 3\u201318.\nIEEE, 2017.\nCongzheng Song and Vitaly Shmatikov. Auditing data provenance in text-generation models. In\nProceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data\nMining, pp. 196\u2013206, 2019.\nThomas Steinke, Milad Nasr, and Matthew Jagielski. Privacy auditing with one (1) training run.\narXiv preprint arXiv:2305.08846, 2023.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question\nanswering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4149\u20134158, Minneapolis, Minnesota, June\n2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https:\n//aclanthology.org/N19-1421.\nTogetherCompute. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURL https://github.com/togethercomputer/RedPajama-Data.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-\ntian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,\nWenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n2023b.\nPaul Voigt and Axel Von dem Bussche. The eu general data protection regulation (gdpr). A Practical\nGuide, 1st Ed., Cham: Springer International Publishing, 10(3152676):10\u20135555, 2017.\nLauren Watson, Chuan Guo, Graham Cormode, and Alexandre Sablayrolles. On the importance of\ndifficulty calibration in membership inference attacks. In International Conference on Learning\nRepresentations, 2022. URL https://openreview.net/forum?id=3eIrli0TwQ.\n17\nPublished as a conference paper at ICLR 2024\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International\nConference on Learning Representations, 2022. URL https://openreview.net/forum?id=\ngEZrGCozdqR.\nOrion Weller, Marc Marone, Nathaniel Weir, Dawn Lawrie, Daniel Khashabi, and Benjamin Van\nDurme. \"according to ...\" prompting language models improves quoting from pre-training data,\n2023.\nYinjun Wu, Edgar Dobriban, and Susan Davidson. Deltagrad: Rapid retraining of machine learning\nmodels. In International Conference on Machine Learning, pp. 10355\u201310366. PMLR, 2020.\nJingwen Ye, Yifang Fu, Jie Song, Xingyi Yang, Songhua Liu, Xin Jin, Mingli Song, and Xinchao\nWang. Learning with recoverable forgetting. In European Conference on Computer Vision, pp.\n87\u2013103. Springer, 2022.\nSamuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning:\nAnalyzing the connection to overfitting. In 2018 IEEE 31st Computer Security Foundations\nSymposium (CSF), pp. 268\u2013282, 2018a. doi: 10.1109/CSF.2018.00027.\nSamuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning:\nAnalyzing the connection to overfitting.\nIn 2018 IEEE 31st computer security foundations\nsymposium (CSF), pp. 268\u2013282. IEEE, 2018b.\nSantiago Zanella-B\u00e9guelin, Lukas Wutschitz, Shruti Tople, Victor R\u00fchle, Andrew Paverd, Olga\nOhrimenko, Boris K\u00f6pf, and Marc Brockschmidt. Analyzing information leakage of updates to\nnatural language models. In Proceedings of the 2020 ACM SIGSAC conference on computer and\ncommunications security, pp. 363\u2013375, 2020.\nChiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tram\u00e8r, and Nicholas\nCarlini. Counterfactual memorization in neural language models. arXiv preprint arXiv:2112.12938,\n2021.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068, 2022.\n18\nPublished as a conference paper at ICLR 2024\nA\nADDITIONAL RESULTS\nTable 6: TPR@5%FPR score for detecting pretraining examples from the given model on WIKIMIA\nfor MIN-K% PROB and baselines. Ori. and Para. denote the original and paraphrase settings,\nrespectively. Bold shows the best score within each column.\nPythia-2.8B\nNeoX-20B\nLLaMA-30B\nLLaMA-65B\nOPT-66B\nMethod\nOri.\nPara.\nOri.\nPara.\nOri.\nPara.\nOri.\nPara.\nOri.\nPara.\nAvg.\nNeighbor\n10.2\n16.2\n15.2\n19.3\n20.1\n17.2\n17.2\n20.0\n17.3\n18.8\n17.2\nPPL\n9.4\n18.0\n17.3\n24.9\n23.7\n18.7\n16.5\n23.0\n20.9\n20.1\n19.3\nZlib\n18.7\n18.7\n20.3\n22.1\n18.0\n20.9\n23.0\n23.0\n21.6\n20.1\n20.6\nLowercase\n10.8\n7.2\n12.9\n12.2\n10.1\n6.5\n14.4\n12.2\n14.4\n8.6\n10.9\nSmaller Ref\n10.1\n10.1\n15.8\n10.1\n10.8\n11.5\n15.8\n21.6\n15.8\n10.1\n13.2\nMIN-K% PROB\n13.7\n15.1\n21.6\n27.3\n22.3\n25.9\n20.9\n30.9\n21.6\n23.0\n22.2\nTable 7: TPR @ FPR=5% for detecting contaminant downstream examples using reference-based\nand reference-free methods. Bold shows the best reference-free TPR within each column.\nMethod\nBoolQ\nCommonsense QA\nIMDB\nTruthful QA\nAvg.\nNeighbor\n19\n7\n41\n13\n20\nPPL\n52\n24\n74\n17\n42\nZlib\n18\n9\n19\n7\n13\nLowercase\n24\n3\n26\n14\n17\nMIN-K% PROB\n55\n23\n83\n21\n46\nTable 8: Accuracy of the model finetuned in Section 6.1 on each non-contaminant and contaminant\nexamples used for AUC computation for each downstream dataset. The difference in average\nclassification accuracy of contaminant examples over that of non-contaminant examples is 0.04 at a\nlearning rate of 1 \u00d7 10\u22125 and 0.11 at a learning rate of 1 \u00d7 10\u22124. This indicates that memorization\nbecomes a significantly more pronounced effect than generalization at larger learning rates.\nLearning rate\nBoolQ\nCommonsense QA\nIMDB\nLSAT QA\nTruthful QA\nAvg.\nNon-contaminant examples\n1 \u00d7 10\u22125\n0.68\n0.47\n0.89\n0.22\n0.28\n0.51\n1 \u00d7 10\u22124\n0.69\n0.48\n0.90\n0.24\n0.33\n0.53\nContaminant examples\n1 \u00d7 10\u22125\n0.71\n0.49\n0.92\n0.26\n0.38\n0.55\n1 \u00d7 10\u22124\n0.81\n0.60\n0.89\n0.35\n0.56\n0.64\n19\nPublished as a conference paper at ICLR 2024\nTable 9: Input template we use to prompt GPT-4 to obtain the similarity score.\nSystem\nYou are a helpful assistant in evaluating the similarity between two outputs generated by two different\nAI chatbots. Your goal is to rate the similarity between the two outputs based on a scale of 1 to 5,\nwith 1 being highly dissimilar and 5 being highly similar.\nUser\nRate the similarity between Output (a) and Output (b) on a scale of 1 to 5, where 1 indicates high\ndissimilarity, and 5 indicates high similarity. Here are some rules of the evaluation:\n(1) Consider how closely Output (a) matches Output (b) in terms of content, context, and relevance.\n(2) Do not provide a rating outside the 1 to 5 scale, and avoid giving a rating of 3 (neutral) whenever\npossible.\n(3) Your judgment should be as objective as possible, without being influenced by any potential bias.\nYou should answer \u2018Score: \u2019, followed by an integer rating between 1 to 5, where 1 indicates high\ndissimilarity, and 5 indicates high similarity. You should then output \u2018Reason: \u2019 and provide a short\nsentence to explain your rating.\nOutput (a):\nOutput (b):\nTable 10:\nExamples of similar ground truth completion and the completion by the\nLLaMA2-7B-WhoIsHarryPotter model, alongside the SimCSE and GPT-4 similarity scores.\nChunk #1:\n\u25b6 Context: He was chewing the gillyweed as hard and fast as he could; it felt unpleasantly slimy and rubbery,\nlike octopus tentacles. Waist-deep in the freezing water he stopped, swallowed, and waited for something to\nhappen. He could hear laughter in the crowd and knew he must look stupid, walking into the lake without\nshowing any sign of magical power. The part of him that was still dry was covered in goose pimples; half\nimmersed in the icy water, a cruel breeze lifting his hair, Harry started to shiver violently. He avoided looking at\nthe stands; the laughter was becoming louder, and there were catcalls and jeering from the Slytherins. ... Then,\nquite suddenly, Harry felt as though an invisible pillow had been pressed over his mouth and nose. He tried to\ndraw breath, but it made his head spin; his lungs were empty, and he suddenly felt a piercing pain on either side\nof his neck - Harry clapped his hands around his throat and felt two large slits just below his ears, flapping in the\ncold air. . . . He had gills. Without pausing to think, he did the only thing that made\n\u25b6 Ground truth (Output a): sense - he flung himself forward into the water. The first gulp of icy lake water\nfelt like the breath of life. His head had stopped spinning; he took another great gulp of water and felt it pass\nsmoothly through his gills, sending oxygen back to his brain. He stretched out his hands in front of him and\nstared at them. They looked green and ghostly under the water, and they had become webbed. He twisted around\nand looked at his bare feet - they had become elongated and the toes were webbed too: It looked as though he\nhad sprouted flippers. The water didn\u2019t feel icy anymore either ... on the contrary, he felt pleasantly cool and\nvery light. . . . Harry struck out once more, marveling at how far and fast his flipper-like feet propelled him\nthrough the vater, and noticing how clearly he could see, and how he no longer seemed to need to blink. He had\nsoon swum so far into the lake that he could no longer see the bottom. He flipped over and dived into its depths.\nSilence pressed upon his ears as he soared over a strange, dark, foggy landscape. He could only see ten feet\naround him, so that as he sped throuugh the water new scenes seemed to loom suddenly out of the incoming\ndarkness: forests of rippling, tangled black weed, wide plains of mud littered with dull, glimmering stones. He\nswam deeper and deeper, out toward the middle of the lake, his eyes wide, staring through the eerily gray-lit\nwater around him to the shadow beyond, where the water became opaque. Small fish flickered past him like\nsilver darts. Once or twice he thought he saw something larger moving ahead of him, but when he got nearer, he\ndiscovered it to be\n\u25b6 Completion by LLaMA2-7B-WhoIsHarryPotter (Output b): sense: he opened his mouth and let the water\nrush in.\nReflexively, he gulped air, and his lungs expanded, filling with the icy water. The coldness spread through his\nbody, and he shivered again, but this time it was different. He felt a strange tingling sensation, like tiny fingers\nmassaging his muscles.\nAs he gulped in more water, his body began to change. His legs grew longer and thinner, turning into a pair of\nflippers. His arms shortened and transformed into a pair of fins, and his hands became webbed.\n\u25b6 SimCSE score: 0.82\n\u25b6 GPT-4 score and reason: 5. Both outputs describe a character changing into a sea creature. They both mention\nthe character gulping in water, feeling cold initially then adjusting to the temperature, and developing webbed\nhands and feet or flippers. However, Output (b) doesn\u2019t include the exploration of the underwater environment\nthat is a key part of Output (a) which results in slight difference.\nChunk #2:\n20\nPublished as a conference paper at ICLR 2024\n\u25b6 Context: doesn\u2019t know about the troll.\" Ron bit his lip. \"Oh, all right,\" he snapped. \"But Percy\u2019d better not see\nus.\" Ducking down, they joined the Hufflepuffs going the other way, slipped down a deserted side corridor, and\nhurried off toward the girls\u2019 bathroom. They had just turned the corner when they heard quick footsteps behind\nthem. \"Percy!\" hissed Ron, pulling Harry behind a large stone griffin. Peering around it, however, they saw not\nPercy but Snape. He crossed the corridor and disappeared from view. \"What\u2019s he doing?\" Harry whispered.\n\"Why isn\u2019t he down in the dungeons with the rest of the teachers?\" \"Search me.\" Quietly as possible, they crept\nalong the next corridor after Snape\u2019s fading footsteps. \"He\u2019s heading for the third floor,\" Harry said, but Ron\nheld up his hand. \"Can you smell something?\" Harry sniffed and a foul stench reached his nostrils, a mixture of\nold socks and the kind of public toilet no one seems to clean. And then they heard it \u2013 a low grunting, and the\nshuffling footfalls of gigantic feet. Ron pointed \u2013 at the end of a passage to the left, something huge was moving\ntoward them. They shrank into the shadows and\n\u25b6 Ground truth: watched as it emerged into a patch of moonlight. It was a horrible sight. Twelve feet tall, its\nskin was a dull, granite gray, its great lumpy body like a boulder with its small bald head perched on top like a\ncoconut. It had short legs thick as tree trunks with flat, horny feet. The smell coming from it was incredible. It\nwas holding a huge wooden club, which dragged along the floor because its arms were so long. The troll stopped\nnext to a doorway and peered inside. It waggled its long ears, making up its tiny mind, then slouched slowly\ninto the room. \"The keys in the lock,\" Harry muttered. \"We could lock it in.\" \"Good idea,\" said Ron nervously.\nThey edged toward the open door, mouths dry, praying the troll wasn\u2019t about to come out of it. With one great\nleap, Harry managed to grab the key, slam the door, and lock it. \u2019Yes!\" Flushed with their victory, they started to\nrun back up the passage, but as they reached the corner they heard something that made their hearts stop \u2013 a\nhigh, petrified scream \u2013 and it was coming from the chamber they\u2019d just chained up. \"Oh, no,\" said Ron, pale\nas the Bloody Baron. \"It\u2019s the girls\u2019 bathroom!\" Harry gasped. \"Hermione!\" they said together. It was the last\nthing they wanted to do, but what choice did they have? Wheeling around, they sprinted back to the door and\nturned the key, fumbling in their panic. Harry pulled the door open and they ran inside. Hermione Granger\nwas shrinking against the wall opposite, looking as if she was about to faint. The troll was advancing on her,\nknocking the sinks off the walls as it went. \"Confuse it!\" Harry said desperately to Ron, and, seizing a tap, he\nthrew it as hard as he could against the wall.\n\u25b6 Completion by LLaMA2-7B-WhoIsHarryPotter (Output b): watched in horror as the creature came into view.\nIt was a troll from the depths of the earth, its skin the color of dirt and mold, its eyes glowing red slits in the\ndim light. The passageway was too narrow for it to fit, but it could see them, and its grunts grew louder, more\nmenacing. \"What do we do?\" gulped Harry. \"We can\u2019t outrun it.\" \"We have to find a way to distract it,\" decided\nRon, his mind racing with ideas. \"Quick!\" He grabbed a nearby book from the wall and flung it at the troll,\nhoping to divert its attention. The creature roared in anger and chased after the book, giving them just enough\ntime to slip through a nearby door and slam it shut behind them. They leaned against the door, panting with fear,\nand listened as the troll beat its fists against the other side of the door, trying to break through. \"That was close,\"\nsaid Harry, his voice shaking. \"Too close.\" \"We have to get out of here,\" agreed Ron. \"Before it breaks through.\"\nBut they knew they couldn\u2019t escape that way. The only other exit was through the library, and they knew Mr.\nHufflepuff would never let them pass. They were trapped.\n\u25b6 SimCSE score: 0.84\n\u25b6 GPT-4 score and reason: 4. Both outputs present a similar scenario where characters are encountered by\na troll and try to escape or distract it. The issue they face, the settings, and the way they express fear show\nconsiderable similarities. However, the specific details and descriptions of events do somewhat differ.\nChunk #3:\n21\nPublished as a conference paper at ICLR 2024\n\u25b6 Context: one, \u2019cause it might be the last thing yeh do.\" Malfoy, Crabbe, and Goyle weren\u2019t listening; they\nwere talking in an undertone and Harry had a nasty feeling they were plotting how best to disrupt the lesson.\n\"Yeh always wait fer the hippogriff ter make the firs\u2019 move,\" Hagrid continued. \"It\u2019s polite, see? Yeh walk toward\nhim, and yeh bow, an\u2019 yeh wait. If he bows back, yeh\u2019re allowed ter touch him. If he doesn\u2019 bow, then get away\nfrom him sharpish, \u2019cause those talons hurt. \"Right \u2013 who wants ter go first?\" Most of the class backed farther\naway in answer. Even Harry, Ron, and Hermione had misgivings. The hippogriffs were tossing their fierce heads\nand flexing their powerful wings; they didn\u2019t seem to like being tethered like this. \"No one?\" said Hagrid, with a\npleading look. \"I\u2019ll do it,\" said Harry. There was an intake of breath from behind him, and both Lavender and\nParvati whispered, \"Oooh, no, Harry, remember your tea leaves!\" Harry ignored them. He climbed over the\npaddock fence. \"Good man, Harry!\" roared Hagrid. \"Right then \u2013 let\u2019s see how yeh get on with Buckbeak.\" He\nuntied one of the chains, pulled the gray hippogriff\n\u25b6 Ground truth (Output a): \"away from its fellows, and slipped off its leather collar. The class on the other side\nof the paddock seemed to be holding its breath. Malfoy\u2019s eyes were narrowed maliciously. \"Easy) now, Harry,\"\nsaid Hagrid quietly. \"Yeh\u2019ve got eye contact, now try not ter blink.... Hippogriffs don\u2019 trust yeh if yeh blink\ntoo much....\" Harry\u2019s eyes immediately began to water, but he didn\u2019t shut thern. Buckbeak had turned his great,\nsharp head and was staring at Harry with one fierce orange eye. \"Tha\u2019s it,\" said Hagrid. \"Tha\u2019s it, Harry... now,\nbow.\" Harry didn\u2019t feel much like exposing the back of his neck to Buckbeak, but he did as he was told. He\ngave a short bow and then looked up. The hippogriff was still staring haughtily at him. It didn\u2019t move. \"Ah,\"\nsaid Hagrid, sounding worried. \"Right \u2013 back away, now, Harry, easy does it But then, to Harry\u2019s enormous\nsurprise, the hippogriff suddenly bent its scaly front knees and sank into what was an unmistakable bow. \"Well\ndone, Harry!\" said Hagrid, ecstatic. \"Right \u2013 yeh can touch him! Pat his beak, go on!\" Feeling that a better\nreward would have been to back away, Harry moved slowly toward the hippogriff and reached out toward it. He\npatted the beak several times and the hippogriff closed its eyes lazily, as though enjoying it. The class broke into\napplause, all except for Malfoy, Crabbe, and Goyle, who were looking deeply disappointed. \"Righ\u2019 then, Harry,\"\nsaid Hagrid. \"I reckon he might\u2019 let yeh ride him!\" This was more than Harry had bargained for. He was used to\na broomstick; but he wasn\u2019t sure a hippogriff would be quite the same. \"Yeh climb up there, jus\u2019 behind the wing\njoint,\" said Hagrid, \"an\u2019 mind yeh don\u2019 pull any of his feathers out, he won\u2019 like that....\" Harry put his foot on\nthe top of Buckbeaks\n\u25b6 Completion by LLaMA2-7B-WhoIsHarryPotter (Output b): forward, and held it out on a leash. The bird was\nmassive, with a beak like a scythe and eyes that glowed like lanterns in the dark. \"Walk towards him, boys and\ngirls,\" urged Hagrid. \"Be polite, now. No need ter be afraid. He\u2019n\u2019t goin ter hurt ye.\" But he wasn\u2019t sure if that\nwas true. He could feel a dark mischief in the air, a sense of unease that made his spine tingle. He\u2019d heard stories\nof hippogriffs turning on their handlers when they were least expected, and he didn\u2019 mind sharing his tea leaves\nwith them. But he couldn\u2019 afford ter back down now. So he took a deep breath and started walking, trying ter\nkeep his eyes fixed on the bird\u2019 eyes. \"Good boy, good boy,\" coaxed Haggridd. \"Now, touch his wing, if yeh\ndare.\"\n\u25b6 SimCSE score: 0.77\n\u25b6 GPT-4 score and reason: 4. The two outputs share notable similarities in context, content, and elements. They\nboth discuss Hagrid instructing someone to approach a hippogriff, with an emphasis on maintaining eye contact\nand eventual touching. While differences can be seen in the detailed dialogues or descriptions, the general\nthemes and ideas remain consistent.\nB\nDETAILS OF WIKIMIA\nData properties.\nOur WIKIMIA benchmark demonstrates several desirable properties that make it\nsuitable for evaluating methods to detect data during pretraining on any newly released models.\n(1) Accurate: Since non-member data consists of events that occurred after the LM pretraining, there\nis a guarantee that this data was not present during pretraining, ensuring the accuracy of our dataset.\nWe consider Wikipedia event data because of its time sensitivity. A recent non-event Wikipedia page\nmay be only a recent version of an older page that was already present during the model\u2019s pretraining,\nand thus it may not serve as true non-member data. For example, a Wikipedia page created after 2023\nabout a historical figure or a well-known concept could contain substantial text already mentioned in\nthe pretraining corpus.\n(2) General: Our benchmark is designed to be widely applicable across different models pretrained\non Wikipedia, a commonly used source of pretraining data. This includes models like OPT (Zhang\net al., 2022), LLaMA (Touvron et al., 2023a;b), GPT-Neo (Black et al., 2022), and Pythia (Biderman\net al., 2023), thereby ensuring the benchmark\u2019s generalizability across various models.\n(3) Dynamic: Our benchmark will be continually updated by incorporating the latest non-member\ndata, such as recent events from Wikipedia. This consistent renewal ensures that the benchmark\u2019s\n22\nPublished as a conference paper at ICLR 2024\nnon-member data is always up-to-date and can be used to evaluate MIA for any newly introduced\npretrained models.\nC\nDETAILS OF MIN-K% PROB\nAlgorithm 1 Pretraining Data Detection\n1: Input: A sequence of tokens x = x1, x2, ..., xN, decision threshold \u03f5\n2: Output: Membership of the sequence x\n3: for i = 1 to N do\n4:\nCompute \u2212 log p(xi|x1, . . . , xi\u22121)\n5: end for\n6: Select the top k% of tokens from x with the lowest probability and add to Min-k%(x)\n7: MIN-K% PROB(x) = P\nxi\u2208Min-k%(x) \u2212 log p(xi|x1, ..., xi\u22121)\n8: If MIN-K% PROB(x) > \u03f5 : return Non-member\nElse: return Member\n23\n"
  },
  {
    "title": "TiC-CLIP: Continual Training of CLIP Models",
    "link": "https://arxiv.org/pdf/2310.16226.pdf",
    "upvote": "7",
    "text": "Pre-print\n\t TIC-CLIP: CONTINUAL TRAINING OF CLIP MODELS\nSaurabh Garg;\u02da\nMehrdad Farajtabar:\nHadi Pouransari:\nRaviteja Vemulapalli:\nSachin Mehta:\nOncel Tuzel:\nVaishaal Shankar:\nFartash Faghri:\n:Apple ;Carnegie Mellon University\nsgarg2@andrew.cmu.edu, fartash@apple.com\nABSTRACT\nKeeping large foundation models up to date on latest data is inherently expensive.\nTo avoid the prohibitive costs of constantly retraining, it is imperative to continually\ntrain these models. This problem is exacerbated by the lack of any large scale\ncontinual learning benchmarks or baselines. We introduce the first set of web-\nscale Time-Continual (TiC) benchmarks for training vision-language models: TIC-\nDataComp, TIC-YFCC, and TIC-RedCaps with over 12.7B timestamped image-\ntext pairs spanning 9 years (2014\u20132022). We first use our benchmarks to curate\nvarious dynamic evaluations to measure temporal robustness of existing models. We\nshow OpenAI\u2019s CLIP (trained on data up to 2020) loses \u00ab 8% zero-shot accuracy\non our curated retrieval task from 2021\u20132022 compared with more recently trained\nmodels in OpenCLIP repository. We then study how to efficiently train models on\ntime-continuous data. We demonstrate that a simple rehearsal-based approach that\ncontinues training from the last checkpoint and replays old data reduces compute\nby 2.5\u02c6 when compared to the standard practice of retraining from scratch.\n1\nINTRODUCTION\nLarge multimodal foundation models (Bommasani et al., 2021) have offered unprecedented advance-\nments in image-generation and zero-shot generalization, and have led to a paradigm shift in multi-\nmodal learning, e.g., CLIP (Radford et al., 2021), Flamingo (Alayrac et al., 2022), and Stable Dif-\nfusion (Rombach et al., 2022). These foundation models are typically trained on large web-scale\ndatasets which are fixed and static in nature. For example, CLIP\u2019s training data contains 400 million\nimage-text pairs, and Stable Diffusion was trained on LAION-2B dataset (Schuhmann et al., 2022).\nIn reality, however, these models must operate in a dynamic environment, where the world is in a\nstate of constant change. For instance, the internet continually evolves, with petabytes of new data\nbeing added daily (Wenzek et al., 2019; Wiener & Bronson, 2014). It remains unclear how legacy\nmodels, e.g., OpenAI\u2019s CLIP models which were trained on internet-scale data up until 2020, work\non future data and whether they even require any re-training to adapt to time-evolving data.\nWe begin by comparing robustness of OpenAI\u2019s CLIP models to others in OpenCLIP repository that\nare trained on more recently curated web-datasets (e.g., LAION-5B, DataComp) containing data up\nuntil 2022 (Ilharco et al., 2021). Since there is no existing benchmark to understand robustness to time-\nevolving vision-language data, we curate dynamic classification and retrieval tasks for years 2014\u2013\n2022 and evaluate different CLIP models (see Sec. 2.2 for our evaluation tasks). We make an intriguing\nobservation that OpenAI models exhibit a significant gap in retrieval performance on data from 2021\u2013\n2022 compared with 2014\u20132016 whereas OpenCLIP models retain their performance. In contrast,\nstandard evaluations such as accuracy on ImageNet distribution shifts paint an incomplete picture that\nOpenAI\u2019s CLIP models are slightly more robust than OpenCLIP models (Fig. 1). Our findings not\nonly demonstrate the critical need for models to adapt and evolve alongside dynamic data distributions,\nbut also underscores the limitations of relying solely on static benchmarks (e.g. ImageNet).\nOne naive but common practice for adapting to time-evolving data is to train a new CLIP model\nfrom scratch every time we obtain a new pool of image-text data. This practice has its rationale:\n\u02daWork done during an internship at Apple.\n1\narXiv:2310.16226v1  [cs.CV]  24 Oct 2023\nPre-print\n25\n50\n75\nImagenet accuracy\n20\n40\n60\n80\nImagenet dist. shift accuracy\nStandard Evaluation Paradigm\n40\n60\n80\nRetrieval recall on 2014\u20132016\n40\n50\n60\n70\n80\nRetrieval recall on 2021\u20132022\nPerformance\ngap\nOur Proposed Evaluation Paradigm\nOpenAI models trained on data before 2020\nOpenClip models trained on data before 2022\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nTotal Compute (MACs)\n\u00d71020\n20\n25\n30\n35\n40\n45\n50\nImagenet and its dist. shifts accuracy\n2.7x\nTiC-Datacomp (L)\nTrain from scratch with new +\nold data (standard practice)\nWarm start + new data\n+ replay old data\nFigure 1: (Left, Middle) OpenAI models show less zero-shot robustness on retrieval task from\n2021\u20132022. OpenCLIP models and OpenAI models have similar robustness on standard benchmarks.\nHowever, OpenAI models show less robustness on our retrieval task when compared with recent\nmodels in OpenCLIP repository, highlighting susceptibility to a time-evolving data distribution (Right)\nSimple continual training baseline is computationally efficient and competitive to retraining\nfrom scratch. Different points denote models trained sequentially on our TIC-DataComp (L) as data\narrives over time. Warm start training with previous checkpoint and replaying all old data, performs\nsimilar to Oracle which trains from scratch every time new data arrives, by using 2.7\u02c6 less compute.\ninitiating training from a pre-existing model can make it difficult to change the model\u2019s behavior in\nlight of new data (Ash & Adams, 2020; Achille et al., 2018; Liu et al., 2023). However, training\nfoundation models from scratch demands significant computational resources and is often infeasible\nto repeat frequently. For example, ViT-g-14 in Schuhmann et al. (2022); Cherti et al. (2022) was\ntrained for 240K A100 GPU hours which is approximately one month on 400 GPUs. The prevailing\ntraining guidelines centered around scaling laws for CLIP training have only looked at training from\nscratch (Cherti et al., 2023). This leads to a pivotal question: How can we continuously update models\nas the data distribution evolves over time given computational constraints?\nThere exists a vast literature on continual learning, with a focus on adapting models to dynamic\nenvironments (Parisi et al., 2019; Hadsell et al., 2020; De Lange et al., 2021). Traditionally, this\nfield concentrated on synthetic incremental benchmarks that lack natural evolution between tasks,\nand hence, continual learning methods are seldom used in real-world scenarios (Cossu et al., 2022;\nLin et al., 2021). In contrast, recent works focusing on continual learning methods for CLIP models,\nprimarily target improving performance on a single or a sequence of disjoint downstream tasks (Ding\net al., 2022; Zhou et al., 2023; Zheng et al., 2023; Ilharco et al., 2022). While some recent works have\nstarted to address these problems, existing benchmarks are comparatively much smaller in scale, or\nlack paired image-text data (Ni et al., 2023; Lin et al., 2021). Simply put, there is a scarcity of work\nfocusing on continual training of CLIP models on naturally evolving data with time at web-scale.\nWe take the first step towards Time-Continual (TIC) training of CLIP models where data distribution\nevolves naturally over time (overview in Fig. 2). We introduce TIC-DataComp, a new benchmark for\nTime-Continual training of CLIP models, which we create by appending \u201ccrawl time\u201d information\nto existing CommonPool dataset (Gadre et al., 2023). We also repurpose other web-scale datasets\ngathered from diverse sources, such as Reddit and Flickr. Specifically, we curate TIC-YFCC and TIC-\nRedCaps by leveraging time information available in YFCC (Thomee et al., 2016) and Redcaps (Desai\net al., 2021) respectively. The primary objective of our study on this benchmark is to develop continual\nlearning methods that operate within a constrained computational budget (say C) each time a fresh\nbatch of data becomes available. These methods compete with an Oracle, which starts training from\nscratch every time new data arrives, utilizing a cumulative computational budget.\nTo assess models trained in our TIC-CLIP framework, we evaluate models on our proposed dynamic\nevaluation tasks that evolve with time along with 28 standard classification and retrieval tasks\nincluding ImageNet (Krizhevsky et al., 2012), ImageNet distributions shifts, and Flickr (Plummer\net al., 2015), in a zero-shot manner following the work of Gadre et al. (2023); Radford et al. (2021).\nFinally, we develop continual learning methods on our benchmarks and perform over two hundred\nexperiments with different baselines that utilize previous checkpoints (e.g., warm start, patching,\nand distillation), replay buffers, and learning rate schedules. Our findings highlight a key takeaway:\nCumulative method that warm starts training with the latest checkpoint and replays all old data,\n2\nPre-print\nModel at time t -1 \nData at time t\nData at time 1\nData at time t - 2 \nData at time t -1 \n\u2026\nReplay buffer\u2028\n with constraints  \nNew \u2028\ndata\nTrain a CLIP model with compute budget \nconstraints\nRandom init\nOR\nA. Aggregate Data\nB. Continual Training\nC. Static and Dynamic Evaluation\nStandard Static Evaluation Tasks\n Proposed Dynamic Evaluation Tasks\nTask at \nTime 1\nTask at \nTime t-1\n\u2026\nTask at \nTime t\nTask at \nTime t + 1\nTask at \nTime T \n\u2026\nBackward Transfer\nForward Transfer\nID \u2028\nEvaluation\nImageNet (IN), ObjectNet,  \nIN-v2, IN-R, IN-S, IN-A, IN-O, \nVTAB, Wilds, etc.\nFlickr30k, MSCOCO\nClassi\ufb01cation \nRetrieval\nFigure 2: Experimental protocol on our proposed continual benchmarks. (A) Combine new and\nold data given buffer constraints. (B) Continually train a model with a compute budget (say C) either\nby starting with previous checkpoint or from scratch. (C) Evaluate models on standard datasets and\nour proposed dynamic datasets. Comparison with other benchmarks in Appendix A.\nachieves performance competitive to an Oracle while being 2.7\u02c6 computationally more efficient.\nAdditionally, our experiments demonstrate interesting trade-offs between buffer sizes for static\nand dynamic performance and provide valuable insights into learning rate schedules for sequential\ntraining. Our results span over various dataset scales (from 11M samples to 3B) and highlight trends\nwith different methods that are largely consistent across scales.\nTo make our benchmarks accessible, we will soon release the code and time information we collect\non top of existing datasets. Our work is just an initial step towards continual training of foundation\nmodels, and we believe our research would spur more attention to this understudied area.\n2\nTIC-CLIP: BENCHMARKS AND EXPERIMENTAL PROTOCOL\nIn this section, we introduce our benchmark (Fig. 2) focusing on the training of a vision-language\nfoundation model with the objective of Contrastive Language Image Pretraining (CLIP) (Radford\net al., 2021)). Notably, we train on image-text data that arrives sequentially unlike the conventional\nimage-text datasets which are static (e.g. WiT in CLIP, DataComp in Gadre et al. (2023)). We curate\nTIC-DataComp, TIC-YFCC, and TIC-RedCaps that are image-text pairs sourced from the internet\nwhich we augment with auxiliary time information. We also introduce dynamic evaluation tasks to\nassess performance of our continually trained models on data evolving with time. The goal of a learner\nis to train a deployable model at each step as new data becomes available with a fixed compute budget.\n2.1\nBENCHMARK DESIGN: HOW WE CREATE TIME-CONTINUAL DATASETS?\nTo instantiate continual training of CLIP, we extend existing image-text datasets with time information\ncollected from the original source of the datasets. Our largest dataset is TIC-DataComp which\ncontains 12.7 billion image-text pairs with \u201ccrawl-time\u201d metadata. We create this dataset on top of the\nexisting DataComp benchmark (Gadre et al., 2023). We also create TIC-YFCC and TIC-RedCaps\non top of existing YFCC15M (Thomee et al., 2016; Radford et al., 2021) and Redcaps (Desai et al.,\n2021) datasets to highlight that our findings are broadly applicable to carefully curated datasets from\ndiverse sources such as Reddit and Flickr. While time-related metadata is absent in the DataComp\nbenchmark, it is available in the original releases of YFCC and Redcaps. Nevertheless, to the best of\nour knowledge, no prior work utilizes such time information for continual training of CLIP models.\nWe show dataset statistics for all datasets, e.g., number of examples in each year in App. C.3.\nTIC-DataComp\nWe collect timestamps for the CommonPool dataset introduced in DataComp\nwhich contains 12.7B image-text pairs (not including 0.1B inaccessible ones). This dataset stands\nas the largest public image-text dataset to date. The source of DataComp is Common Crawl, which\nperiodically releases web-crawled data snapshots, typically on a monthly basis since 2014 with new\nand updated webpages. To construct TIC-DataComp, we augment each image-text pair in DataComp\nwith their first timestamp. We followed the same construction process as DataComp but retained only\nthe image-text pair found in the earliest snapshot during the deduplication stage. This process provides\ntimestamps at the granularity of months, spanning years 2014\u20132022. See App. C.7 for details on the\nconstruction process. We note that while this augmented time information may contain some noise, on\naverage, we find it to be a reasonably accurate proxy for the upload time of web pages (see App. C.7).\n3\nPre-print\nApple Confidential\u2013Internal Use Only\n2014\n2022\n.  \n. \n.\nPreviously unseen \ntopics, e.g., \nCOVID-19 \nemerge with time \nMount Rainier and Towers \nFirst Snow Storm of \nthe Season\nimage of virus that \ncauses sickness \ncoronavirus covid-19\nA. Dynamic Retrieval Task\nB. Dynamic Classi\ufb01cation Task\nMask\nSports Car\nPhone\nComputer\nFigure 3: Distribution of examples changes from 2014 to 2022 in our dynamic evaluation tasks.\n(Left) Samples for text to image retrieval. For new timestamps, images from novel concepts appear\n(e.g., COVID-19). (Right) Samples from our classification task for 4 categories. We observe that not\nonly objects evolve over time but also images from recent timestamps are captured more in the wild.\nAlthough our benchmark contains time information at the granularity of months, we limit our\nexperiments to granularity of years by consolidating data for all months in a year. Similar to\nDataComp, our benchmark has an inclusive design, accommodating participants with varying levels\nof computational resources. In particular, we experiment with medium, large, and xlarge sizes\nfrom CommonPool. Gadre et al. (2023) leverage different filtering strategies to select the training\nsubset. We are concerned that filtering techniques bias the selected training data. In App. C.1, we\nprovide preliminary evidence that \u201cBestpool\u201d filtering that uses off-the-shelf CLIP models, indeed\nbiases the selected data to old time steps. Nevertheless, to highlight significance of our findings even\nfor state-of-the filtering techniques, we experiment with both Bestpool and Basic filtering (no CLIP\nfiltering) at xlarge scale. For large and medium scales, we only experiment with Basic filtering.\nTIC-YFCC\nWe experiment with the 15M subset of YFCC100M (Thomee et al., 2016), namely\nYFCC15M, selected by OpenAI (Radford et al., 2021). This filtering retains only images with natural\ntext in captions. YFCC100M contains data from years 2008\u20132014 and was originally released with\nupload timestamps. We use this information to create continual splits at the granularity of years.\nTIC-RedCaps RedCaps contains 12M image-caption pairs from manually curated set of subreddits\nacross 2011\u20132020 (Desai et al., 2021). We use the creation timestamps of the posts to create splits\nfor continual learning. Similar to the other two datasets, we experiment at the granularity of years.\n2.2\nEVALUATION TESTBED\nDynamic tasks\nWe leverage the temporal information in our benchmarks to create dynamic\nevaluation tasks. Here, the test data comprises samples varying over years as the world evolved.\nFor our largest dataset which is TIC-DataComp, we create dynamic tasks for both retrieval and\nclassification as described below. (examples in Figure 3 and additional examples in App. C.5):\nI. Dynamic retrieval task: To create a retrieval task, we sample a batch of IID image-text pairs from\ndifferent timestamps and evaluate text retrieval performance given the corresponding image (similarly,\nimage retrieval given the corresponding text). We refer to the dataset as TIC-DataComp-Retrieval.\nII. Dynamic classification task: We also create a classification dataset TIC-DataComp-Net with Ima-\ngeNet classes from CommonPool and augmented with timestamps. Inspired by LAIONNet (Shirali\n& Hardt, 2023), we first filter examples where the corresponding caption contains one and only one\nof the synsets of ImageNet. Then we only retain examples where the similarity between ImageNet\nsynset definition and the caption exceeds a threshold of 0.5. We evaluate the similarity using an off-\nthe-shelf sentence embedding model (Reimers & Gurevych, 2019). Crucially, unlike LAIONNet, we\ndo not filter the image-text pairs with CLIP similarity scores to avoid biasing the selection process.\nWe describe the construction process in more details in App. C.5. On TIC-DataComp-Net, we report\naverage accuracy over all classes and over selected nodes (e.g., motor vehicles) at each time step.\nSimilarly, we create retrieval tasks for TIC-YFCC and TIC-RedCaps. Note that we remove the\nextracted image-text pairs for dynamic retrieval and classification tasks from the training sets.\nEvaluations on dynamic tasks are done in a zero shot manner.\n4\nPre-print\nStatic tasks\nWe also evaluate models on numerous classification and retrieval tasks in a zero-\nshot manner as in Radford et al. (2021). In particular, we consider 28 standard tasks: 27 image\nclassification tasks, e.g., ImageNet and its 6 distribution shifts (e.g., ImageNetv2, ImageNet-R,\nImageNet-Sketch, and Objectnet), datasets from VTAB and Flickr30k retrieval task. We refer to these\nas static evaluation tasks. We list all the datasets in App. C.2.\nEvaluation metrics We define metrics for classification tasks and retrieval tasks based on accuracy\nand Recall@1, respectively. Let T represent the number of time steps for which we have data. For\neach training method, we generate a total of T models, each corresponding to the end of training at a\nparticular time step. For static datasets (e.g., ImageNet), we report average performance of T models.\nHowever, when dealing with dynamic evaluation datasets, we assess the performance of each of the T\nmodels on evaluation datasets collected at all time steps. Consequently, for each model and a dynamic\nevaluation task, we obtain T performance values. We represent these values using the performance\nmatrix E, where each entry Ei,j signifies the performance of the model obtained after observing\ntraining data at time step i when evaluated on a dataset from time step j. The performance matrix E\ncan also be succinctly summarized using three standard metrics commonly employed in continual\nlearning evaluations (Lin et al., 2021; Lopez-Paz & Ranzato, 2017; D\u00edaz-Rodr\u00edguez et al., 2018):\n\u2022 In-domain performance: average performance at each training time step (i.e., the diagonal of E)\n\u2022 Backward transfer: average on time steps before each training step (i.e., the lower triangular of E)\n\u2022 Forward transfer: average on time steps following each training step (i.e., the upper triangular of E)\nWhile the static tasks capture performance on standard benchmarks, dynamic tasks capture problems\ndue to distribution shift (for forward transfer) and forgetting (for backward transfer). The goal in our\nbenchmark is to develop continual learning methods that maximize performance on static tasks while\nsimultaneously optimizing for performance on dynamic tasks.\n2.3\nEXPERIMENTAL PROTOCOL FOR TRAINING\nStreaming protocol We follow a streaming protocol, where data is progressively revealed to the\nlearner in large batches with the objective of achieving a deployable model as early as possible after\neach batch arrives. We conduct experiments with data streaming at the granularity of years and our\nbenchmark supports future research at the granularity of months. Additionally, as the amount of\ndata from earlier time steps is limited (see App. C.3), we aggregate data from the earlier time steps\ninto a single larger batch and timestamp it by the latest year in the range. After this aggregation, we\nhave 7 time steps for TIC-DataComp (2016\u20132022) and 4 for both TIC-YFCC (2011\u20132014) and TIC-\nRedCaps (2017\u20132020). While the number of image-text pairs revealed at each time step are of similar\norders of magnitude, the exact number does vary across steps and we do not artificially alter the sizes.\nMemory budget\nWe allow methods to use the last model checkpoint at each step as the cost of\nkeeping one checkpoint per month is often negligible. In contrast, the cost of retaining old data can be\nhigh and might not be permitted due to data expiration policies. Thus, along with studying methods\nthat retain all old data, we also explore strategies that restrict data persistence (see Sec. 3 for details).\nCompute budget\nTo ensure a fair comparison between methods, we establish a consistent total\ncompute budget, quantified in terms of Multiply-Accumulate Operations (MACs), and allocate it\nevenly for training at every time step. Unless specified otherwise, for all methods except Oracle and\nLwF, we use the same compute budget. For experiments on TIC-DataComp, we refer to compute\nconfigurations in DATACOMP for overall compute. For TIC-RedCaps and TIC-YFCC, we use\ncompute of order medium scale in TIC-DataComp. Compute budget details are in App. C.4.\n2.4\nANALYZING DISTRIBUTION SHIFTS IN THE CONSTRUCTED BENCHMARKS\nTIC-DataComp analysis through the lens of constructed evaluation tasks First, we qualitatively\nanalyze the examples in our retrieval and classification dataset (Fig. 3). We observe that over time,\nin the retrieval task, new concepts like COVID-19 emerge. Likewise, certain ImageNet classes\nevolve, such as the shift from \u201cmasquerad\u201d masks to \u201csurgical/protective\u201d masks in their definitions.\nMoreover, as time evolves, we observe that image quality improves and more images tend to appear in\nthe wild in contrast to centered white background images. Next, we compare performance of OpenAI\nand OpenCLIP models on our datasets. Here, we only present the main findings, and delegate a\ndetailed discussion to App. C.6. We observe a significant performance gap between OpenAI and\nOpenCLIP models on our dynamic retrieval task (Fig. 1). This gap widens notably on retrieval queries\n5\nPre-print\nwhere captions mention COVID-19. On the other hand, OpenAI and OpenCLIP models exhibit\nsimilar robustness for retrieval on data coming from Flickr highlighting that data from some domains\ndo not exhibit shifts that cause performance drops. For our classification task, we observe a very\nsmall drop (\u00ab 1%) when averaged across all categories. However, we observe a substantial gap on\nspecific subtrees in ImageNet. For example, classes in \u201cmotor vehicle\u201d subtree show an approximate\n4% performance drop, when comparing OpenAI and OpenCLIP models. These findings highlight\nthat while overall ImageNet classes may remain timeless, certain categories tend to evolve faster than\nothers. Our qualitative and quantitative analysis on TIC-DataComp clearly highlights evolution of\ndistributions and captures different properties than standard benchmarks.\nQuantitative analysis on TIC-YFCC We analyze TIC-YFCC using off-the-shelf sentence and\nimage encoders. We first embed images from different time steps with an OpenAI CLIP encoder\nand then compute Frechet Inception Distance (FID; Seitzer (2020)). As time progresses, we observe\nthat FID distance increases with respect to data from first time step (Fig. 16 in App. C.6). Similarly,\nwe use pretrained sentence transformer to extract top-5 categories from Wordnet Nouns for each\ncaption. We observe that the TV distance over distribution of WordNet Nouns evolves over time\nwhen compared to data from the first time step. More details in App. C.6.\n3\nTIC-CLIP: HOW TO CONTINUALLY TRAIN CLIP MODELS?\nTable 1: Table summarizing our methods. D: data\nsize in each step, T total time steps, t: current time\nstep, C: compute budget (iterations).\nMethod\nEach Step\nTotal\nTrain Size\nInit.\nCompute\nCompute\nCumulative-All\ntD\nLast\nC\nTC\nCumulative-Exp\n2D\nLast\nC\nTC\nCumulative-Equal\n2D\nLast\nC\nTC\nSequential\nD\nLast\nC\nTC\nRestart\ntD\nRand\nC\nTC\nPatching\nD\nLast Patch\nC\nTC\nLwF\nD\nLast\n1.2 \u02c6 C\n1.2 \u02c6 TC\nOracle\u02da\u02da\ntD\nRand\ntC\npT `1qT\n2\nC\nIn this section, we lay out different methods\nspecifically focus on the following questions\n(Tab. 1): (i) How to utilize/replay data from pre-\nvious time steps; (ii) How to leverage previously\ntrained model checkpoints? (iii) What should be\nthe training/optimization procedure?\nData replay methods initialized from the last\ncheckpoint demonstrate strong performance on\nstandard continual learning benchmarks (Sec. 5).\nWe consider replay methods with/without initial-\nization from last checkpoint(s):\nI. Oracle: Train a CLIP model from scratch (i.e., random initialization) on all image-text data\nreceived till time t using a large compute budget of t \u02c6 C. Oracle represents a prohibitively expensive\nmethod that is the most common practice in training large-scale foundation models. The goal of other\nmethods is to perform as close as possible to the Oracle within their limited budget.\nII. Cumulative: Train each model initialized from last checkpoint on the union of all data up to t\nwith compute budget C. This method is analogous to Experience Replay (ER; (Robins, 1995; Hayes\net al., 2019)) but with substantially larger buffers than common in the continual learning literature.\nGiven a fixed buffer size for each past step, we observe minimal to no difference between random\nsubsampling and other strategies. After sampling the replay data, we randomly shuffle it together\nwith new data for training. We consider the following strategies for sampling buffer sizes per step:\n\u2022 -All: Replay all previous data.\n\u2022 -Exp: Replay a buffer of size D and reduce the amount of old data by half at each step. For example,\nat 3-th time step, we retain D{2, D{2 of old data and at 4-th, we retain D{4, D{4, D{2 of old data.\nAlong with D data from current step, this method trains on at most 2D data in each step.\n\u2022 -Equal: Replay a buffer of size D but split the buffer equally among all previous years. For example,\nat 4-th step, we retain D{3, D{3, D{3 of old data. Along with D data from current time step, this\nmethod trains on at most 2D data in each step.\nIII. Sequential: Train only on the new data starting from the best checkpoint of the previous time\nstep. Sequential is similar to Cumulative but without any replay buffer.\nIV. Restart: Train each model from scratch (i.e., random initialization) on all the data till time t for\ncompute budget C. Restart is similar to the Oracle but with compute budget C at each time step and\nsimilar to Sequential but with random initialization. As such, Restart helps us understand the forward\ntransfer and loss of plasticity in our benchmark (Ash & Adams, 2020; Dohare et al., 2023).\n6\nPre-print\nTable 2: Zero shot performance on our time-continual benchmarks. \u02da and \u02da\u02da denote methods\nthat violate the compute budget. For static tasks, we tabulate accuracy of the models obtained on the\nfinal timestamp. For dynamic tasks, we tabulate forward/backward transfer and ID performance on\nretrieval tasks (Sec. 2.3). For TIC-DataComp (XL), we include results with Bestpool filtering (basic\nfiltering in Table 5). For all metrics, higher is better.\nBenchmark\nMethod\nCompute\n(MACs)\nStatic Tasks\nDynamic Retrieval Tasks\nImageNet\nImageNet\ndist. shift\nFlickr30k\nAverage over\n28 datasets\nBackward\nTransfer\nID Perfor-\nmance\nForward\nTransfer\nTIC-YFCC\nRestart\n3.4 \u02c6 1018\n5.2\n3.6\n3.0\n4.0\n13.2\n41.4\n18.6\nSequential\n3.4 \u02c6 1018\n17.3\n10.5\n15.9\n11.6\n42.2\n48.4\n23.7\nPatching\n3.4 \u02c6 1018\n18.9\n11.3\n18.5\n12.1\n44.7\n53.4\n24.5\nCumulative-Exp\n3.4 \u02c6 1018\n24.1\n14.3\n20.4\n15.4\n60.4\n60.1\n27.1\nCumulative-Equal\n3.4 \u02c6 1018\n23.9\n13.8\n20.5\n14.7\n60.4\n60.4\n27.1\nCumulative-All\n3.4 \u02c6 1018\n29.3\n17.6\n26.8\n18.0\n66.4\n60.2\n27.6\nLwF\u02da\n4.1 \u02c6 1018\n16.9\n9.8\n14.7\n10.5\n36.6\n56.0\n23.2\nCumulative-All\u02da\n3.6 \u02c6 1018\n29.2\n17.5\n27.4\n18.1\n66.8\n60.3\n27.6\nOracle\u02da\u02da\n8.5 \u02c6 1018\n29.2\n17.0\n25.9\n18.0\n66.1\n61.8\n26.9\nTIC-RedCaps\nRestart\n3.4 \u02c6 1018\n11.7\n8.5\n3.7\n7.6\n21.3\n25.4\n22.4\nSequential\n3.4 \u02c6 1018\n19.3\n13.7\n6.2\n11.9\n33.0\n33.6\n27.5\nPatching\n3.4 \u02c6 1018\n21.3\n15.2\n7.7\n14.0\n34.8\n34.8\n27.8\nCumulative-Exp\n3.4 \u02c6 1018\n27.3\n19.1\n10.5\n16.3\n44.5\n42.0\n32.6\nCumulative-Equal\n3.4 \u02c6 1018\n27.8\n19.4\n10.0\n16.7\n44.4\n42.0\n32.6\nCumulative-All\n3.4 \u02c6 1018\n32.2\n18.7\n14.5\n19.7\n48.9\n43.2\n33.4\nLwF\u02da\n4.1 \u02c6 1018\n21.6\n14.8\n8.2\n13.5\n35.4\n36.0\n28.4\nCumulative-All\u02da\n3.6 \u02c6 1018\n32.9\n23.7\n14.1\n20.1\n49.0\n43.4\n33.4\nOracle\u02da\u02da\n8.5 \u02c6 1018\n32.7\n22.7\n14.3\n20.6\n48.5\n43.1\n33.4\nTIC-DataComp (M)\nSequential\n3.0 \u02c6 1018\n19.2\n16.4\n16.4\n15.0\n25.7\n26.4\n14.9\nPatching\n3.0 \u02c6 1018\n19.3\n16.8\n18.5\n14.7\n26.9\n25.4\n14.5\nCumulative-Exp\n3.0 \u02c6 1018\n22.1\n18.4\n20.4\n16.7\n31.7\n27.1\n15.2\nCumulative-Equal\n3.0 \u02c6 1018\n22.1\n18.4\n19.2\n17.1\n31.8\n26.8\n15.1\nCumulative-All\n3.0 \u02c6 1018\n24.0\n20.2\n20.9\n17.9\n33.8\n26.4\n15.1\nLwF\u02da\n3.8 \u02c6 1018\n19.2\n16.5\n17.7\n14.3\n25.6\n26.6\n14.9\nCumulative-All\u02da\n3.9 \u02c6 1018\n30.0\n25.0\n28.6\n22.3\n36.7\n28.3\n15.5\nOracle\u02da\u02da\n1.2 \u02c6 1019\n25.5\n21.2\n23.3\n19.0\n34.9\n27.8\n15.6\nTIC-DataComp (L)\nSequential\n2.7 \u02c6 1019\n44.7\n37.4\n48.4\n32.7\n52.6\n58.4\n41.1\nPatching\n2.7 \u02c6 1019\n45.8\n38.9\n49.7\n33.6\n55.2\n57.5\n40.9\nCumulative-Exp\n2.7 \u02c6 1019\n47.3\n39.6\n50.8\n35.0\n60.4\n58.4\n41.4\nCumulative-Equal\n2.7 \u02c6 1019\n47.7\n40.3\n51.8\n36.0\n60.9\n58.2\n41.4\nCumulative-All\n2.7 \u02c6 1019\n48.9\n41.3\n50.9\n36.3\n62.1\n57.3\n41.2\nCumulative-All\u02da\n4.1 \u02c6 1019\n53.0\n44.3\n54.4\n39.0\n63.0\n57.8\n41.2\nOracle\u02da\u02da\n1.1 \u02c6 1020\n53.6\n44.0\n53.9\n38.0\n64.3\n58.6\n41.8\nTIC-DataComp (XL)\nSequential\n2.7 \u02c6 1020\n66.5\n54.2\n61.2\n51.7\n63.1\n68.9\n56.8\nCumulative-All\n2.7 \u02c6 1020\n71.6\n58.8\n65.1\n55.7\n70.7\n68.5\n57.1\nCumulative-All\u02da\n3.5 \u02c6 1020\n72.8\n60.4\n66.5\n57.7\n71.0\n68.6\n57.1\nOracle\u02da\u02da\n1.1 \u02c6 1021\n73.3\n61.3\n68.0\n58.1\n-\n-\n-\nV. LwF: Train only on the new data with an additional loss that regularizes the model by KL\ndivergence between the image-text similarity matrix of last checkpoint and current model on each\nmini-batch (Li & Hoiem, 2017; Ding et al., 2022).\nVI. Patching: We use sequential patching from Ilharco et al. (2022) and initialize from a patched\nmodel of last step and train only on the new data. To obtain patched model at each time step, we apply\nweight interpolation with the patched model (if any) trained at time step t\u00b41 and model trained at time\nstep t. We tune the mixing coefficients by optimizing average retrieval performance on previous tasks.\nLearning rate schedule The defacto Learning Rate (LR) schedule for training CLIP models is an\ninitial linear increase to a maximum value, i.e., warm up, followed by a cosine decay (Radford et al.,\n2021; Gadre et al., 2023). We default to using cosine LR schedule for each sequential run, resulting\nin a cyclic schedule and observe a significant increase in training loss early in subsequent runs when\nthe LR is high (refer to App. B.4). However, as training progresses, we observe that the increased\nloss decreases at a faster rate (when compared to training from scratch) allowing us to train with\ncyclic schedules. We explore an alternate learning rate schedule in App. B.4.\nOther Training details and hyperparameters\nUnless specified otherwise, we closely follow the\noriginal CLIP training recipe (Radford et al., 2021). We train the CLIP variant with ViT-B/16 as the\nimage encoder (Dosovitskiy et al., 2020). All training and hyperparameters can be found in App. D.2.\n4\nEXPERIMENTS AND MAIN RESULTS\nOur main results are in Table 2 and more detailed plots on each dataset are in App. B.1. Recall, our\ngoal is compete with an Oracle that re-trains from scratch every time new data is observed, both on\ndynamic and static tasks, while being computationally efficient. Here, we summarize our key findings:\n7\nPre-print\n0.0\n0.5\n1.0\nTotal Compute (MACs)\n\u00d71020\n20\n30\n40\nImagenet (& dist. shifts) accuracy\nSimilar\nStatic Evaluation\n0.0\n0.5\n1.0\nTotal Compute (MACs)\n\u00d71020\n30\n40\n50\n60\n70\nRetrieval Recall@1 on 2021\u20132022\nGap\nDynamic Evaluation\nCumulative-All\nOracle\n2016 2017 2018 2019 2020 2021 2022\nEvaluation time step\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nTraining time step\n47.09\n32.46\n28.98\n25.04\n23.10\n22.67\n22.82\n59.39\n54.99\n47.50\n41.57\n38.93\n38.19\n36.98\n63.07\n60.23\n60.11\n53.13\n47.08\n45.79\n45.29\n64.23\n62.00\n59.78\n57.68\n51.65\n50.27\n50.01\n65.38\n63.92\n64.72\n64.89\n61.35\n58.67\n56.61\n66.90\n64.57\n65.11\n66.38\n64.00\n63.73\n61.12\n66.92\n65.38\n65.91\n67.23\n64.33\n65.42\n65.36\nDynamic evaluation of Oracle\n25\n30\n35\n40\n45\n50\n55\n60\n65\nFigure 4: (Left) Dynamic and static evaluations rank models differently. Models with similar per-\nformance on static datasets, have \u0105 6% difference on retrieval task from 2021-2022 TIC-DataComp\n(L). Different points denote models trained sequentially over time. (Right) Performance of Oracle\non future time steps drops highlighting distribution shift in dataset. Each row evaluates the Ora-\ncle trained on TIC-DataComp (L) at a particular time step across all dynamic retrieval tasks.\nCumulative-All saves up to 4\u02c6 the cost. On dynamic evaluation tasks, we observe that Cumulative-\nAll where we replay all the past data, achieves performance close to the Oracle (within 1%) using\nsignificantly less compute (4\u02c6 less on TIC-DataComp and 2.5\u02c6 less on TIC-YFCC and TIC-\nRedCaps). On static tasks, the gap remains small at small scales but grows to 4.7% on large, 1.8%\non xlarge Bestpool, and 4% on xlarge Basic (see Table 2 and Table 5). In these cases, training\nCumulative models with slightly extra compute bridges the gap while remaining at least 2.7\u02c6 more\ncomputationally efficient (see rows with \u02da in Table 2). This highlights that with unconstrained access\nto past data, we can simply train sequentially and save significant computational resources.\nAt scale, Sequential has strong forward transfer but lacks on static tasks. On TIC-YFCC and TIC-\nRedCaps, which are at the smallest scale, we observe a significant gap (\u0105 10%) between Sequential\n(with no data replay) and Oracle on all tasks. On the other hand, on all scales in TIC-DataComp,\nSequential shows strong performance on forward transfer and ID dynamic evaluations. However, on\nstatic tasks and backward transfer evaluations, Sequential significantly underperforms the Oracle.\nPatching and LwF improve over Sequential but lag behind Cumulative-All. On static tasks, LwF\nimproves over Sequential by 2%, while on dynamic tasks, LwF improves backward transfer by 7%\non TIC-DataComp (M). However, its computation cost is higher than even Cumulative-All\u02da which\noutperforms LwF on all tasks. Patching improves over Sequential on backward transfer on all datasets\n(e.g., 5% boost on TIC-DataComp L) highlighting that Patching combines benefits of previously\npatched model and the new Sequential model without additional computation cost. However, such\nbenefits do not show up on static tasks. These results hint that to continuously improve on static tasks\nwith time, replaying old data as in Cumulative-All plays a crucial role.\n-Exp and -Equal significantly reduce replay buffer size and maintain static task performance\nand backward transfer. Recall, that -Exp and -Equal reduce the replay buffer size to a maximum 2D\nof old data. In particular, at the last time step, -Exp and -Equal reduce the buffer size by 3.5\u02c6 for TIC-\nDataComp datasets. While reducing the buffer sizes, these methods still achieve performance close\nto Cumulative-All (within 2%) on both static and dynamic tasks, with -Equal consistently better than\n-Exp strategy. As we go to large scale, e.g., from medium to large, the gap between these methods\nand Cumulative-All reduces. These findings demonstrate that even a small amount of replay data from\nold time steps stays competitive with replaying all data and significantly improves over no replay at all.\nWarm up helps training on data from first time step, but hurts on subsequent time steps. Cosine\nLR is commonly coupled with an initial warm-up that linearly increases the LR from zero to maximum\nLR. We investigate the effectiveness of warm-up in first versus subsequent time steps. Surprisingly,\nwe observe that not using warmup for subsequent training runs is strictly more beneficial than using\nwarm up on both static and dynamic tasks. In particular, on TIC-DataComp (L), we observe about\n1.5% improvement in ImageNet accuracy and 4.3% improvement on ID dynamic retrieval when not\nusing warmup with Cumulative (see App. B.3). Moreover, we also ablate over not using warm up for\nthe first training run and observe a drop of approximately 4.8% accuracy in the first time step on TIC-\nDataComp (L). Hence, we default to using warmup when training on the first time step and not using\nit on the subsequent time steps with all methods except for training on TIC-DataComp (XL) where\nwe add a smaller warm up (10% of the warm up iterations used in first step) to stabilize training.\n8\nPre-print\nSame maximum LR works best across all runs when using cosine schedule. We ablate on TIC-\nDataComp (M) to investigate how to change LR after training on data from the first time step. Unlike\nconventional pretraining and finetuning settings where LR is typically decreased for subsequent\ntraining, we observe that decaying maximum LR for subsequent steps in our setup hurts on static and\ndynamic tasks and consequently, we use same maximum LR across our runs (see App. B.3).\nFiltering strategy changes the ordering of performance on static and dynamic retrieval tasks. We\nobserve that while bestpool filtering models outperform basic filterining models on TIC-DataComp\n(XL) by 6% on static tasks, they underperform by over 5% on dynamic retrieval task (see Fig. 6).\nDynamic tasks provide complimentary information for model selection compared to static\ntasks. Choosing models solely based on static task performance may inadvertently select models\nthat underperform on dynamic tasks. For example, Cumulative models that show relatively modest\nimprovements on static tasks continue to improve by \u0105 6% for retrieval on 2021-2022 (Fig. 4).\nTable 3: ImageNet continual training\nwith up to 8 splits. Cumulative-All re-\nmains close to Oracle.\nMethod\nNumber of splits\n1 (Oracle)\n2\n4\n8\nCumulative-All\n80.9\n80.8\n80.6\n80.0\nCumulative-All remains competitive to Oracle even on\nImageNet on up to 8 splits. CLIP models are often trained\nfor fewer epochs and are typically not trained until they\nreach an \u201coverfitting\u201d regime. Moreover, CLIP models\nare trained with noisy supervision of web data with a\ncontrastive loss. Here, we investigate how Cumulative-All\n(with no extra budget) performs when compared to Oracle\nwhen training is done for longer with high-quality data.\nSpecifically, we assess Cumulative-All on 2, 4 and 8 IID splits including the full dataset (see App. D.1\nfor details). Table 3 summmarizes our key findings. Notably, even with up to 8 splits, the difference in\naccuracy between Oracle and Cumulative-All remains below 0.9% on the ImageNet holdout test set.\nThese results underscore the feasibility of continual training with Cumulative-All even on ImageNet.\n5\nRELATED WORK\nNeural networks trained on new data suffer from catastrophic forgetting of prior knowledge (Sutton,\n1986; Goodfellow et al., 2013). Continual learning literature has focused on benchmarks and\nmethods to address this challenge (Hadsell et al., 2020) while concentrating on domain, class, or task\nincremental benchmarks (Hsu et al., 2018; Van de Ven & Tolias, 2019) with artificial task boundaries\n(e.g., Split-CIFAR, Perm-MNIST). This results into benchmarks with minimal or no meaningfully\nevolution between adjacent tasks. Continual learning methods can be categorized broadly into i)\nregularization ii) replay, and iii) architecture-based methods. Regularization methods push the model\nto change slowly in the directions of prior knowledge and often incur additional memory/compute\ncosts (Kirkpatrick et al., 2017; Mirzadeh et al., 2020a;b; Farajtabar et al., 2020). Data replay methods\nretain all or a subset of the prior data for either retraining or regularization (Lopez-Paz & Ranzato,\n2017; Rebuffi et al., 2017; Chaudhry et al., 2018). Simple replay-based baselines can surpass\nvarious methods on standard benchmarks (Lomonaco et al., 2022; Balaji et al., 2020; Prabhu et al.,\n2020). Lastly, architecture-based methods expand the model as new tasks arrive which limits their\napplicability in continually evolving environments (Schwarz et al., 2018; Rusu et al., 2016).\nReal-world machine learning has recently been dominated by training of large-scale foundation mod-\nels that flourish with scale (Kaplan et al., 2020; Cherti et al., 2023). Particularly, vision-language\nmodels have demonstrated scalability with data size leading to growing compute requirements (Rad-\nford et al., 2021; Jia et al., 2021; Li et al., 2023). Continual learning of foundation models would\nsignificantly reduce the costs and increase quick adaptability. While some recent works have started\nto introduce large-scale continual learning benchmarks, they are not naturally time-continual and\nare comparatively much smaller in scale (Ni et al., 2023; Srinivasan et al., 2022). Proposing meth-\nods, Thengane et al. (2022) use the zero-shot capability of CLIP to evaluate on standard continual\nlearning benchmarks without any training. Ding et al. (2022) focus on continual fine-tuning of CLIP\nmodels on classification tasks and proposes an adaptation of LwF. Model averaging methods aim at\ninterpolating between the performance of multiple contrastively pretrained models and classification-\nfinetuned copies (Wortsman et al., 2022; Ilharco et al., 2022; Stoica et al., 2023).\n6\nCONCLUSION AND FUTURE WORK\nIn conclusion, we view TIC-DataComp as the initial stride toward the continual training of large-scale\nvision-language foundation models. We aspire to empower the research on large-scale continual-\n9\nPre-print\nlearning through our new benchmark, alongside the preliminary results obtained using simple\nbaselines. We are committed to open-sourcing both our code and data.\nThere are several pivotal directions for future work: (i) Reduce the replay buffer size while maintain-\ning the performance on static evaluation tasks and backward-transfer; (ii) Compare our baselines on\ncontinually streaming data at finer granularity, e.g., streaming data at the monthly level; (iii) Investi-\ngate alternate learning rate schedules (e.g., Const-Cosine) that are forward looking, and are better\nsuited to continual learning; (iv) Better data filtering techniques that are more inclusive of future data;\n(v) Expand our problem setup to encompass the training of other large-scale foundation models.\nACKNOWLEDGEMENTS\nWe thank Samy Bengio, Preetum Nakkiran, Arno Blaas, and Mitchell Wortsman for their feedback on\nthe project and on the earlier versions of the draft. We thank Denise Hui and Tatiana Likhomanenko\nfor their support with the computing infrastructure.\nREFERENCES\nAlessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep networks.\nIn International Conference on Learning Representations, 2018.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u2013\n23736, 2022.\nJordan Ash and Ryan P Adams. On warm-starting neural network training. Advances in neural\ninformation processing systems, 33:3884\u20133894, 2020.\nYogesh Balaji, Mehrdad Farajtabar, Dong Yin, Alex Mott, and Ang Li. The effectiveness of memory\nreplay in large scale continual learning. arXiv preprint arXiv:2010.02418, 2020.\nPeter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke\nHermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al.\nFrom detection of individual metastases to classification of lymph node status at the patient level:\nthe camelyon17 challenge. IEEE Transactions on Medical Imaging, 2018. https://pubmed.\nncbi.nlm.nih.gov/30716025/.\nAndrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh\nTenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the\nlimits of object recognition models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-\nBuc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems (NeurIPS),\nvolume 32. Curran Associates, Inc., 2019. https://proceedings.neurips.cc/paper/\n2019/file/97af07a14cacba681feacf3012730892-Paper.pdf.\nSara Beery, Elijah Cole, and Arvi Gjoka. The iwildcam 2020 competition dataset, 2020. https:\n//arxiv.org/abs/2004.10340.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportuni-\nties and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\nJorg Bornschein, Alexandre Galashov, Ross Hemsley, Amal Rannen-Triki, Yutian Chen, Arslan\nChaudhry, Xu Owen He, Arthur Douillard, Massimo Caccia, Qixuang Feng, et al. Nevis\u201922:\nA stream of 100 tasks sampled from 30 years of computer vision research.\narXiv preprint\narXiv:2211.11747, 2022.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative com-\nponents with random forests.\nIn European Conference on Computer Vision (ECCV), 2014.\nhttps://link.springer.com/chapter/10.1007/978-3-319-10599-4_29.\n10\nPre-print\nZhipeng Cai, Ozan Sener, and Vladlen Koltun. Online continual learning with natural distribution\nshifts: An empirical study with visual data. In Proceedings of the IEEE/CVF international\nconference on computer vision, pp. 8281\u20138290, 2021.\nFabio Cermelli, Massimiliano Mancini, Samuel Rota Bulo, Elisa Ricci, and Barbara Caputo. Modeling\nthe background for incremental learning in semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 9233\u20139242, 2020.\nArslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient\nlifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018.\nArslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K\nDokania, Philip HS Torr, and Marc\u2019Aurelio Ranzato. On tiny episodic memories in continual\nlearning. arXiv preprint arXiv:1902.10486, 2019.\nGong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark\nand state of the art. Proceedings of the Institute of Electrical and Electronics Engineers (IEEE),\n2017. https://ieeexplore.ieee.org/abstract/document/7891544.\nMehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade\nGordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for\ncontrastive language-image learning, 2022. https://arxiv.org/abs/2212.07143.\nMehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade\nGordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for\ncontrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 2818\u20132829, 2023.\nGordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world.\nIn Conference on Computer Vision and Pattern Recognition (CVPR), 2018. https://arxiv.\norg/abs/1711.07846.\nAdam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised\nfeature learning. In International Conference on Artificial Intelligence and Statistics (AISTATS),\n2011. https://proceedings.mlr.press/v15/coates11a.html.\nAndrea Cossu, Gabriele Graffieti, Lorenzo Pellegrini, Davide Maltoni, Davide Bacciu, Antonio\nCarta, and Vincenzo Lomonaco. Is class-incremental enough for continual learning? Frontiers in\nArtificial Intelligence, 5:829842, 2022.\nMatthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory\nSlabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification\ntasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366\u20133385, 2021.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In Conference on Computer Vision and Pattern Recognition (CVPR),\n2009. https://ieeexplore.ieee.org/abstract/document/5206848.\nKaran Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data\ncreated by the people, for the people. arXiv preprint arXiv:2111.11431, 2021.\nNatalia D\u00edaz-Rodr\u00edguez, Vincenzo Lomonaco, David Filliat, and Davide Maltoni. Don\u2019t forget, there\nis more than forgetting: new metrics for continual learning. arXiv preprint arXiv:1810.13166, 2018.\nYuxuan Ding, Lingqiao Liu, Chunna Tian, Jingyuan Yang, and Haoxuan Ding. Don\u2019t stop learning:\nTowards continual learning for the clip model. arXiv preprint arXiv:2207.09248, 2022.\nShibhansh Dohare, Juan Hernandez-Garcia, Parash Rahman, Richard Sutton, and A Rupam Mahmood.\nLoss of plasticity in deep continual learning. 2023.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n11\nPre-print\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In\nInternational Conference on Learning Representations (ICLR), 2021. https://openreview.\nnet/forum?id=YicbFdNTTy.\nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual\nObject Classes Challenge 2007 (VOC2007) Results, 2007. http://www.pascal-network.\norg/challenges/VOC/voc2007/workshop/index.html.\nMehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for continual\nlearning. In International Conference on Artificial Intelligence and Statistics, pp. 3762\u20133773.\nPMLR, 2020.\nSamir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen,\nRyan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the\nnext generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023.\nAndreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti\nvision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\nhttps://ieeexplore.ieee.org/abstract/document/6248074.\nIan J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investi-\ngation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211,\n2013.\nPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,\nAndrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet\nin 1 hour. arXiv preprint arXiv:1706.02677, 2017.\nRaia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual\nlearning in deep neural networks. Trends in cognitive sciences, 24(12):1028\u20131040, 2020.\nTyler L Hayes, Nathan D Cahill, and Christopher Kanan. Memory efficient experience replay for\nstreaming learning. In 2019 International Conference on Robotics and Automation (ICRA), pp.\n9769\u20139776. IEEE, 2019.\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul\nDesai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer.\nThe many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV,\n2021a. https://arxiv.org/abs/2006.16241.\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial\nexamples. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021b. https:\n//arxiv.org/abs/1907.07174.\nYen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual learning\nscenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori,\nAchal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali\nFarhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/\nzenodo.5143773. If you use this software, please cite it as below.\nGabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon\nKornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating\nweights. Advances in Neural Information Processing Systems, 35:29262\u201329277, 2022.\nJoel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stan-\nley Jungkyu Choi, and Minjoon Seo. Towards continual knowledge learning of language models.\narXiv preprint arXiv:2110.03215, 2021.\n12\nPre-print\nJoel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun\nKim, and Minjoon Seo. Temporalwiki: A lifelong benchmark for training and evaluating ever-\nevolving language models. arXiv preprint arXiv:2204.14211, 2022.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy\ntext supervision. In International conference on machine learning, pp. 4904\u20134916. PMLR, 2021.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text\nclassification. In Proceedings of the 15th Conference of the European Chapter of the Association for\nComputational Linguistics: Volume 2, Short Papers, pp. 427\u2013431. Association for Computational\nLinguistics, April 2017.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming\ncatastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114\n(13):3521\u20133526, 2017.\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsub-\nramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne\nDavid, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec,\nAnshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A bench-\nmark of in-the-wild distribution shifts. In International Conference on Machine Learning (ICML),\n2021. https://arxiv.org/abs/2012.07421.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained\ncategorization. In International Conference on Computer Vision Workshops (ICML), 2013. https:\n//www.cv-foundation.org/openaccess/content_iccv_workshops_2013/\nW19/html/Krause_3D_Object_Representations_2013_ICCV_paper.html.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-\ntional neural networks. Advances in neural information processing systems, 25, 2012.\nYann LeCun. The MNIST database of handwritten digits, 1998. http://yann.lecun.com/\nexdb/mnist/.\nSang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming\ncatastrophic forgetting by incremental moment matching. Advances in neural information process-\ning systems, 30, 2017.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n2023.\nZhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis\nand machine intelligence, 40(12):2935\u20132947, 2017.\nZhiqiu Lin, Jia Shi, Deepak Pathak, and Deva Ramanan. The clear benchmark: Continual learning on\nreal-world imagery. In Thirty-fifth conference on neural information processing systems datasets\nand benchmarks track (round 2), 2021.\nAdam Li\u0161ka, Tom\u00e1\u0161 Ko\u02c7cisk\u00fd, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal,\nCyprien de Masson d\u2019Autume, Tim Scholtes, Manzil Zaheer, Susannah Young, Ellen Gilsenan-\nMcMahon Sophia Austin, Phil Blunsom, and Angeliki Lazaridou.\nStreamingqa: A bench-\nmark for adaptation to new knowledge over time in question answering models. arXiv preprint\narXiv:2205.11388, 2022.\nXingyu Liu, Alex Leonardi, Lu Yu, Chris Gilmer-Hill, Matthew Leavitt, and Jonathan Frankle.\nKnowledge distillation for efficient sequences of training runs. arXiv preprint arXiv:2303.06480,\n2023.\n13\nPre-print\nVincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous\nobject recognition. In Conference on robot learning, pp. 17\u201326. PMLR, 2017.\nVincenzo Lomonaco, Lorenzo Pellegrini, Pau Rodriguez, Massimo Caccia, Qi She, Yu Chen, Quentin\nJodelet, Ruiping Wang, Zheda Mai, David Vazquez, et al. Cvpr 2020 continual learning in computer\nvision competition: Approaches, results, current challenges and future directions. Artificial\nIntelligence, 303:103635, 2022.\nDavid Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning.\nAdvances in neural information processing systems, 30, 2017.\nUmberto Michieli and Pietro Zanuttigh. Incremental learning techniques for semantic segmentation.\nIn Proceedings of the IEEE/CVF international conference on computer vision workshops, pp. 0\u20130,\n2019.\nSeyed Iman Mirzadeh, Mehrdad Farajtabar, and Hassan Ghasemzadeh. Dropout as an implicit gating\nmechanism for continual learning. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops, pp. 232\u2013233, 2020a.\nSeyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, and Hassan Ghasemzadeh. Understand-\ning the role of training regimes in continual learning. Advances in Neural Information Processing\nSystems, 33:7308\u20137320, 2020b.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading\ndigits in natural images with unsupervised feature learning. In Advances in Neural Information\nProcessing Systems (NeurIPS) Workshops, 2011. https://storage.googleapis.com/\npub-tools-public-publication-data/pdf/37648.pdf.\nZixuan Ni, Longhui Wei, Siliang Tang, Yueting Zhuang, and Qi Tian. Continual vision-language\nrepresentaion learning with off-diagonal information. arXiv preprint arXiv:2305.07437, 2023.\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number\nof classes. In Indian Conference on Computer Vision, Graphics and Image Processing, 2008.\nhttps://ieeexplore.ieee.org/document/4756141.\nGerman I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual\nlifelong learning with neural networks: A review. Neural networks, 113:54\u201371, 2019.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In Con-\nference on Computer Vision and Pattern Recognition (CVPR), 2012. https://ieeexplore.\nieee.org/document/6248092.\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-\nsentence models. In Proceedings of the IEEE international conference on computer vision, pp.\n2641\u20132649, 2015.\nAmeya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions\nour progress in continual learning. In Computer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pp. 524\u2013540. Springer, 2020.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:\nIncremental classifier and representation learning. In Proceedings of the IEEE conference on\nComputer Vision and Pattern Recognition, pp. 2001\u20132010, 2017.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers\ngeneralize to ImageNet?\nIn International Conference on Machine Learning (ICML), 2019.\nhttp://proceedings.mlr.press/v97/recht19a.html.\n14\nPre-print\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics, 11 2019. URL http://arxiv.org/abs/1908.\n10084.\nAnthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2):\n123\u2013146, 1995.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684\u201310695, 2022.\nAndrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray\nKavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint\narXiv:1606.04671, 2016.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models. Advances in Neural\nInformation Processing Systems, 35:25278\u201325294, 2022.\nJonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye\nTeh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual\nlearning. In International conference on machine learning, pp. 4528\u20134537. PMLR, 2018.\nMaximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/\npytorch-fid, August 2020. Version 0.3.0.\nAli Shirali and Moritz Hardt.\nWhat makes imagenet look unlike laion.\narXiv preprint\narXiv:2306.15769, 2023.\nTejas Srinivasan, Ting-Yun Chang, Leticia Pinto Alva, Georgios Chochlakis, Mohammad Rostami,\nand Jesse Thomason. Climb: A continual learning benchmark for vision-and-language tasks.\nAdvances in Neural Information Processing Systems, 35:29440\u201329453, 2022.\nJohannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recogni-\ntion benchmark: a multi-class classification competition. In International Joint Conference on Neu-\nral Networks (IJCNN), 2011. https://ieeexplore.ieee.org/document/6033395.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas\nBeyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv\npreprint arXiv:2106.10270, 2021.\nGeorge Stoica, Daniel Bolya, Jakob Bjorner, Taylor Hearn, and Judy Hoffman. Zipit! merging\nmodels from different tasks without training. arXiv preprint arXiv:2305.03053, 2023.\nRichard S Sutton. Two problems with backpropagation and other steepest-descent learning procedures\nfor networks. In Proc. of Eightth Annual Conference of the Cognitive Science Society, pp. 823\u2013\n831, 1986.\nVishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an efficient\ncontinual learner. arXiv preprint arXiv:2210.03114, 2022.\nBart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland,\nDamian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications\nof the ACM, 59(2):64\u201373, 2016.\nGido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint\narXiv:1904.07734, 2019.\nBastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivari-\nant CNNs for digital pathology, 2018. https://arxiv.org/abs/1806.03962.\n15\nPre-print\nTom Veniat, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato. Efficient continual learning with modular\nnetworks and task-driven priors. arXiv preprint arXiv:2012.12631, 2020.\nEli Verwimp, Kuo Yang, Sarah Parisot, Lanqing Hong, Steven McDonagh, Eduardo P\u00e9rez-Pellitero,\nMatthias De Lange, and Tinne Tuytelaars. Clad: A realistic continual learning benchmark for\nautonomous driving. Neural Networks, 161:659\u2013669, 2023.\nHaohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations\nby penalizing local predictive power. In Advances in Neural Information Processing Systems\n(NeurIPS), 2019. https://arxiv.org/abs/1905.13549.\nJianren Wang, Xin Wang, Yue Shang-Guan, and Abhinav Gupta. Wanderlust: Online continual object\ndetection in the real world. In Proceedings of the IEEE/CVF international conference on computer\nvision, pp. 10829\u201310838, 2021.\nZifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren,\nGuolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for\nrehearsal-free continual learning. In European Conference on Computer Vision, pp. 631\u2013648.\nSpringer, 2022.\nYeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to efficient\nensemble and lifelong learning. arXiv preprint arXiv:2002.06715, 2020.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n,\nArmand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from\nweb crawl data. arXiv preprint arXiv:1911.00359, 2019.\nJanet Wiener and Nathan Bronson. Facebook\u2019s top open data problems. https://research.\nfacebook.com/blog/2014/10/facebook-s-top-open-data-problems/,\n10\n2014. Accessed: 2023-09-28.\nMitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs,\nRaphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust\nfine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 7959\u20137971, 2022.\nJianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database:\nExploring a large collection of scene categories. International Journal of Computer Vision (IJCV),\n2016. https://link.springer.com/article/10.1007/s11263-014-0748-y.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual\ndenotations: New similarity metrics for semantic inference over event descriptions. Transactions\nof the Association for Computational Linguistics, 2014. https://aclanthology.org/\nQ14-1006/.\nFriedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.\nIn International conference on machine learning, pp. 3987\u20133995. PMLR, 2017.\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\nLucic, Josip Djolonga, Andr\u00e9 Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer,\nOlivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and\nNeil Houlsby. The visual task adaptation benchmark, 2019. http://arxiv.org/abs/1910.\n04867.\nZangwei Zheng, Mingyuan Ma, Kai Wang, Ziheng Qin, Xiangyu Yue, and Yang You. Preventing\nzero-shot transfer degradation in continual learning of vision-language models. arXiv preprint\narXiv:2303.06628, 2023.\nDa-Wei Zhou, Yuanhan Zhang, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. Learning\nwithout forgetting for vision-language models. arXiv preprint arXiv:2305.19270, 2023.\n16\nPre-print\nA\nCONTINUAL LEARNING BENCHMARKS AND METHODS\nWe introduce a large-scale image-text benchmark with web scale streaming image text pairs specially\ndeveloped for studying how efficiently one can get a fresh CLIP model with new incoming batches of\ndata. Table 4 compares the proposed benchmark with existing datasets for continual learning. Note\nthat this table is not aimed to be an exhaustive list of all CL datasets, but the most popular benchmarks\nin each domain. For language modeling tasks we report the number of examples/documents as the\nnumber of samples and for detection tasks we report the number of labeled objects/bounding boxes.\nTable 4: Comparison with continual learning benchmarks.\nBenchmark\n# Samples\nYears\nTime-Continual\nImage-Text\nTask\nSplit-MNIST (Goodfellow et al., 2013)\n60K\n1998\n\u2717\n\u2717\nClassification\nPerm-MNIST (Goodfellow et al., 2013)\n60K\n1998\n\u2717\n\u2717\nClassification\nRot-MNIST (Lopez-Paz & Ranzato, 2017)\n60K\n1998\n\u2717\n\u2717\nClassification\nSplit-CIFAR-100 (Zenke et al., 2017)\n50K\n2008\n\u2717\n\u2717\nClassification\nSplit-MINI-ImageNet (Chaudhry et al., 2019)\n50K\n2009\n\u2717\n\u2717\nClassification\nSplit-ImageNet (Wen et al., 2020)\n1.2M\n2009\n\u2717\n\u2717\nClassification\nSplit-ImageNet-R (Wang et al., 2022)\n30K\n2019\n\u2717\n\u2717\nClassification\nCORe50 (Lomonaco & Maltoni, 2017)\n165K\n2017\n\u2717\n\u2717\nDetection\nCLAD (Verwimp et al., 2023)\n23K\n2021\n\u2717\n\u2717\nDetection\nWANDERLUST (Wang et al., 2021)\n326K\n2021\n\u2713\n\u2717\nDetection\nInc-PASCAL (Michieli & Zanuttigh, 2019)\n11K\n2012\n\u2717\n\u2717\nSegmentation\nInc-ADE20K (Cermelli et al., 2020)\n20K\n2012\n\u2717\n\u2717\nSegmentation\nStreamingQA (Li\u0161ka et al., 2022)\n100K\n2007\u20132020\n\u2713\n\u2717\nQuestion Answering\nTemporalWiki (Jang et al., 2022)\n32M\n2021\n\u2713\n\u2717\nLanguage Modeling\nCKL (Jang et al., 2021)\n30K\n2019-2021\n\u2717\n\u2717\nLanguage Modeling\nCTrL (Veniat et al., 2020)\n300K\n1998-2017\n\u2717\n\u2717\nClassification\nCLOC (Cai et al., 2021)\n39M\n2006-2014\n\u2713\n\u2717\nClassification\nCLEAR (Lin et al., 2021)\n7.8M\n2004-2014\n\u2713\n\u2717\nClassification\nNEVIS (Bornschein et al., 2022)\n8M\n1992-2021\n\u2713\n\u2717\nClassification\nMod-X (Ni et al., 2023)\n156K\n2014\n\u2717\n\u2713\nRetrieval\nCLiMB (Srinivasan et al., 2022)\n1.3M\n2013-2021\n\u2717\n\u2713\nClassification\nTIC-YFCC\n15M\n2008-2014\n\u2713\n\u2713\nRetrieval / ZS Classification\nTIC-RedCaps\n12M\n2011-2020\n\u2713\n\u2713\nRetrieval / ZS Classification\nTIC-DataComp\n100M/1B/12B\n2014-2022\n\u2713\n\u2713\nRetrieval / ZS Classification\nA.1\nCONTINUAL LEARNING METHODS\nAddressing the continual learning challenge, researchers have primarily honed in on methods tai-\nlored for small-scale benchmarks, specifically focusing on domain, class, or task incremental bench-\nmarks (Hsu et al., 2018; Van de Ven & Tolias, 2019). While evaluations on these benchmarks often\nneglect the consideration of \u201ctraining time\u201d, it becomes a pivotal factor when scaling continual learn-\ning approaches to scenarios involving the training of foundation models such as CLIP.\nIn our study, we abstain from comparing with continual learning methods that notably prolong\nthe \u201ctraining time\u201d. Methods such as GEM (Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2018),\nEWC (Kirkpatrick et al., 2017), and IMM (Lee et al., 2017), which compute gradients for two\nmodels in each training iteration, essentially double the training duration. Instead, for setups where\nadditional compute resources are available, we run our Cumulative-All approach for slightly longer.\nCumulative-All narrows the gap with Oracle (refer to Table 2). Given that data storage costs are\nsubstantially lower than computational costs at scale, we advocate for taking computational efficiency\ninto consideration in future endeavors.\n17\nPre-print\nB\nADDITIONAL EXPERIMENTAL RESULTS\nB.1\nDETAILED RESULTS ON OUR BENCHMARKS\n0\n1\n2\n3\nTrain Timestamp\n10\n20\n30\nImagenet + dist. shifts accuracy\nStatic Evaluation on TiC-YFCC\n0\n1\n2\nTrain Timestamp\n20\n40\nRetrieval Recall\nForward Transfer on TiC-YFCC\nOracle**\nCumulative-All*\nCumulative-All\nCumulative-Exp\nSequential\n1\n2\n3\nTrain Timestamp\n20\n40\n60\n80\nRetrieval Recall\nBackward Transfer on TiC-YFCC\n(a) TIC-YFCC.\n2017\n2018\n2019\n2020\nTrain Timestamp\n10\n20\n30\nImagenet + dist. shifts accuracy\nStatic Evaluation on TiC-Redcaps\n2017\n2018\n2019\nTrain Timestamp\n20\n30\n40\n50\nRetrieval Recall\nForward Transfer on Tic-Redcaps\nOracle**\nCumulative-All*\nCumulative-All\nCumulative-Exp\nSequential\n2018\n2019\n2020\nTrain Timestamp\n20\n40\nRetrieval Recall\nBackward Transfer on TiC-Redcaps\n(b) TIC-RedCaps.\n2016\n2018\n2020\n2022\nTrain Timestamp\n10\n20\n30\nImagenet + dist. shifts accuracy\nStatic Evaluation on TiC-Datacomp (M)\n2016\n2018\n2020\nTrain Timestamp\n10\n20\n30\n40\nRetrieval Recall\nForward Transfer on TiC-Datacomp (M)\nOracle**\nCumulative-All*\nCumulative-All\nCumulative-Exp\nSequential\n2018\n2020\n2022\nTrain Timestamp\n10\n20\n30\n40\nRetrieval Recall\nBackward Transfer on TiC-Datacomp (M)\n(c) TIC-DataComp (medium).\n2016\n2018\n2020\n2022\nTrain Timestamp\n20\n30\n40\nImagenet + dist. shifts accuracy\nStatic Evaluation on TiC-Datacomp (L)\n2016\n2018\n2020\nTrain Timestamp\n20\n40\n60\nRetrieval Recall\nForward Transfer on TiC-Datacomp (L)\nOracle**\nCumulative-All*\nCumulative-All\nCumulative-Exp\nSequential\n2018\n2020\n2022\nTrain Timestamp\n40\n50\n60\nRetrieval Recall\nBackward Transfer on TiC-Datacomp (L)\n(d) TIC-DataComp (large).\nFigure 5: Static and dynamic evaluation performance over time with selected methods in our\ntestbed. As we get more data, all methods improve on both static and forward transfer on dynamic\ntasks but methods with limited replay buffer start performing slightly worse for backward transfer.\n18\nPre-print\nB.2\nRESULTS WITH BASIC FILTERING ON TIC-DATACOMP XL\nFiltering strategy changes the ordering of performance on static and dynamic retrieval tasks. We\nobserve that while Bestpool filtering models outperform basic filterining models on TIC-DataComp\n(XL) by 6% on static tasks, they underperform by over 5% on dynamic retrieval task (see Fig. 6). In\nthe main paper (Table 2), we included TIC-DataComp (xlarge) results with Bestpool filtering. In\nTable 5, we include basic filtering results. We observe that while Bestpool filtering models perform\nbetter than basic filtering models on static tasks, the order is flipped on dynamic retrieval tasks. Hence,\nwe resort to including results with Basic filtering at smaller scales, but include Bestpool results for\ncompleteness as it achieves better results on static tasks.\nTable 5: Zero shot performance on our time-continual benchmarks (Basic filtering). \u02da and \u02da\u02da\ndenote methods that violate the compute budget and use extra compute. For static tasks, we tabulate\naccuracy of the models obtained on the final timestamp. For dynamic tasks, we tabulate forward\ntransfer, backward transfer and ID performance. For all metrics, higher is better.\nBenchmark\nMethod\nCompute\n(MACs)\nStatic Tasks\nDynamic Retrieval Tasks\nImageNet\nImageNet\ndist. shift\nFlickr30k\nAverage over\n28 datasets\nBackward\nTransfer\nID Perfor-\nmance\nForward\nTransfer\nTIC-DataComp (XL)\nCumulative-All\n2.7 \u02c6 1020\n63.5\n52.0\n62.8\n47.5\n64.6\n55.5\n47.6\nSequential\n2.7 \u02c6 1020\n60.2\n48.9\n62.4\n44.4\n51.6\n50.3\n45.0\nOracle\u02da\u02da\n1.1 \u02c6 1021\n66.0\n54.0\n63.8\n49.6\n-\n-\n-\n2016\n2018\n2020\n2022\nEvaluation Timestamp\n60\n65\n70\n75\nRetrieval Performance\nDynamic Evaluation on TiC-Datacomp (XL)\nBestpool \ufb01ltering\nBasic \ufb01ltering\nFigure 6: Comparing Oracle models trained on Bestpool and Basic filtering trained on data from all\ntime steps. Our results clearly highlight that Basic filtering performs better than Bestpool filtering on\ndynamic retrieval task. However, on static tasks, the order is reversed. Moreover, Bestpool filtering\nshows a drop in retrieval performance from 2016 to 2022 when compared with Basic filtering.\nB.3\nABLATIONS WITH LEARNING RATE WARMUP AND MAXIMUM LEARNING RATE\nTo continually train models as more data arrives sequentially over time, we use multiple cycles of\ncosine learning rate schedule (Fig. 7). There are two crucial design choices: (i) Should we warm\nup the learning rate for subsequent continual runs? and (ii) How should the maximum learning rate\nchange for sequential training runs?\n19\nPre-print\n0\n0.5T\nT\n1.5T\n2T\n(N-1)T (N - 0.5)T\nNT\nIterations\n0.0000\n0.0002\n0.0004\n0.0006\n0.0008\n0.0010\nLearning Rate\nWarm up\nWarm up\nWarm up\nCyclic learning schedule with warm up\n(a) Multiple cycles of standard cosine learning rate schedules which involves warm-up for all\nsubsequent training runs.\n0\n0.5T\nT\n1.5T\n2T\n(N-1)T (N - 0.5)T\nNT\nIterations\n0.0000\n0.0002\n0.0004\n0.0006\n0.0008\n0.0010\nLearning Rate\nWarm up\nCyclic learning schedule with no warm up in subsequent iterations\n(b) Our proposed cosine learning rate schedule without learning rate warm-up for subsequent training\nruns.\nFigure 7: Learning rate schedule ablations. Schedules vary on how continual training is performed\nwhen the training run is initialized with the best previous model. When training with cosine learning\nschedules for subsequent runs, we observe that keeping the same maximum learning rate as the first\nrun performs the best.\nTable 6: Zero shot performance on our time-continual benchmarks with and without initial LR\nwamrup for subsequent runs. Using warm up on sequential runs after training on the first time step\nhurts slightly when compared with not using warm up on sequential runs.\nBenchmark\nMethod\nStatic Tasks\nDynamic Retrieval Tasks\nImageNet\nImageNet\ndist. shift\nFlickr30k\nAverage over\n28 datasets\nBackward\nTransfer\nID Perfor-\nmance\nForward\nTransfer\nTIC-DataComp (M)\nCumulative-All (w/o warmup)\n24.0\n20.2\n20.9\n17.9\n33.8\n26.4\n15.1\nCumulative-All (w warmup)\n23.3\n20.1\n20.3\n17.6\n33.3\n26.1\n14.8\nTIC-DataComp (L)\nCumulative-All (w/o warmup)\n48.9\n41.3\n50.9\n36.3\n62.1\n57.3\n41.2\nCumulative-All (w warmup)\n47.6\n40.6\n50.0\n35.2\n60.1\n53.0\n39.5\nTable 7: Cumulative experiments on TIC-DataComp (M) with different maximum learning rates for\nsubsequent runs with first run fixed at LR 0.00025. Our default choice for subsequent runs is 0.00025.\nPerformance reported on ImageNet. At maximum learning rate 0.001, the runs crashed with Nan in\nloss.\nMethod\nMax LR\n0.00005\n0.0001\n0.00025\n0.0005\n0.001\nCumulative-All\n16.3\n19.0\n24.0\n10.1\n\u2013\n20\nPre-print\nWhen training with large batches, linear learning rate warm-up is typically employed to stabilize the\nstart of the training when beginning from a random initialization (Goyal et al., 2017; Steiner et al.,\n2021). However, when training sequentially by initializing models with checkpoints from the previous\nstep, it remains unclear whether we should employ a learning rate warm up or not. Our observations\nhighlight that while warm up is benefits for the first time step, not using warm up on subsequent runs\nperforms better. In particular, we observe that removing the warm up for the first training run hurts the\nfinal performance. On TIC-DataComp (large), we observe that training a ViT-B/16 with warm up\non the first time step (i.e., 2016) gets 29.9 zero-shot on Imagenet, whereas, without warm up ViT-B/16\nachieves only 24.1 zero-shot performance on Imagenet. Table 6 shows the final performance of models\ntrained with and without warmup on subsequent time steps (after training on the first time step with\nwarmup). In particular, on TIC-DataComp (large), we observe 1.5% accuracy gap on Imagenet and\n4.3% accuracy gap on dynamic ID retrieval performance on models trained with and without warm up.\nHence, we default to using warmup when training on the first time step and not using it on the\nsubsequent time steps with all methods except for training on TIC-DataComp (XL) where we add a\nsmaller warm up (10% of the warm up iterations used in first step) to stabilize training.\nNext, we experiment with different maximum learning rate when training with cosine schedules. We\nablate on TIC-DataComp (M) to investigate how to change LR after training on data from the first\ntime step. Unlike conventional pretraining and finetuning settings where LR is typically decreased\nfor subsequent training, we observe that decaying maximum LR for subsequent steps in our setup\nhurts on static and dynamic tasks and consequently, we use the same maximum LR across our runs\n(see Table 7).\nB.4\nCONST-COSINE: AN ALTERNATIVE LEARNING RATE SCHEDULE\nThe defacto LR schedule for training CLIP models is an initial linear increase to a maximum value, i.e.,\nwarm up, followed by a cosine decay (Radford et al., 2021; Gadre et al., 2023). In the main paper, we\ndefault to using cosine LR schedule for each sequential run, resulting in a cyclic schedule. We observe\na significant increase in training loss early in subsequent runs when the LR is high. Comparing the\nloss on training data with Cumulative and Oracle methods, we observe that as training progresses the\ntraining loss increases every time the learning rate is increased to the maximum LR (Fig. 8).\nIt would be ideal for continual training to employ a learning rate schedule that is \u201cforward looking\u201d,\nallowing us to continually train from a previous checkpoint without experiencing a significant increase\nin training loss. One desirable property of such a learning rate schedule would be its ability to adapt\nwithout requiring prior knowledge of the decay period.\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nIterations\n5\n10\n15\nTraining loss\nCumulative (All)\nOracle\n200\n400\n600\n800\n1000\n1200\n1400\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nFigure 8: Training loss increases every time the LR is reset to maximum LR for Cumulative.\nLoss comparison on training data with Cumulative and Oracle method. Cumulative is trained with a\ncyclic cosine schedule without warm up for sequential training runs. For Cumulative, we plot the\nloss on training data, and as the training progresses, samples from new time steps are added to the\ntraining pool. For Oracle, the training data is the union of data from all time steps and remains the\nsame throughout the training.\n21\nPre-print\n0\n0.5T\nT\n1.5T\n2T\n(N-1)T (N - 0.5)T\nNT\nIterations\n0.0000\n0.0002\n0.0004\n0.0006\n0.0008\n0.0010\nLearning Rate\nWarm up\nDecay\nDecay\nDecay\n(0.2*N*T)\nSpawn a new run\nwith lr decay\nConstant + Cosine learning schedule\nFigure 9: Const-Cosine: Our proposed alternative forward-looking learning rate schedule schedule\nwhich trains one model with constant learning rate and decays the learning rate with cosine schedule\nonly for a fraction of iterations before obtaining a deployable model. Const-Cosine schedule uses an\nextra compute budget than an Oracle run because an extra training run is launched for the fraction of\ntraining when learning rate is decayed.\nIn our work, we perform preliminary experiments with the simplest alternative, Const-Cosine where\nafter the warm up period, we train with a constant learning rate and decay the learning rate only for a\nsmall fraction of training towards the end when we want a deployable model (Fig. 9). This allows us\nto continue training for subsequent runs from the checkpoint at the end of the constant learning rate\nschedule and decay the LR only in the end. For our experiments, we fix the decay period as 0.2 of\nthe total training iterations. Due to this, Const-Cosine schedule slightly increases the overall training\nbudget of the Cumulative runs when compared with cyclic cosine schedules.\nFor Const-Cosine, we only ablate at relatively smaller scale datasets in our testbed (i.e., TIC-YFCC,\nTIC-RedCaps, and TIC-DataComp (medium)). For a fair comparison, we also re-run Oracle methods\nwith the same Const-Cosine schedule. Note that for Const-Cosine experiments, we use the same\nmaximum LR as with the cosine schedule.\nWe observe that training with Const-Cosine schedule significantly improves both Cumulative and Or-\nacle as compared to their counterparts trained with cosine learning rates 1. Moreover, as expected, we\ndo not observe jumps in training loss when training Cumulative with Const-Cosine schedule. How-\never, the gap between Oracle and Cumulative with Const-Cosine doesn\u2019t decrease when compared\nwith gap between Oracle and Cumulative with cosine learning rate schedules. This highlights that the\njumps in the training loss observed while training with the cyclic cosine schedule might have benign\neffects on the final performance.\nTable 8: Zero shot performance on Imagenet with Const-Cosine LR schedule. We observe that\nConst-Cosine improves over cyclic cosine LR schedule. However, the gap between cyclic cosine LR\nschedule and Const-Cosine for different LR schedules remains the same. \u02da\u02da denote methods that\nviolate the compute budget.\nBenchmark\nMethod\nCosine LR Schedule\nConst-Cosine LR schedule\nCompute (MACs)\nImageNet\nCompute (MACs)\nImageNet\nTIC-YFCC\nCumulative-All\n3.4 \u02c6 1018\n29.3\n4.4 \u02c6 1018\n32.8\nOracle\u02da\u02da\n8.5 \u02c6 1018\n29.2\n8.5 \u02c6 1018\n33.2\nTIC-RedCaps\nCumulative-All\n3.4 \u02c6 1018\n32.2\n4.4 \u02c6 1018\n35.1\nOracle\u02da\u02da\n8.5 \u02c6 1018\n32.7\n8.5 \u02c6 1018\n36.2\nTIC-DataComp (M)\nCumulative-All\n3.0 \u02c6 1018\n24.0\n3.6 \u02c6 1018\n28.2\nOracle\u02da\u02da\n1.2 \u02c6 1019\n25.5\n1.2 \u02c6 1019\n28.9\n1We also experimented with Const-Cosine schedule for Oracle training on TIC-DataComp (large) and\nTIC-DataComp (xlarge). We observe that with a decay fraction of 0.2, Const-Cosine achieves similar results\nto that of the cosine learning rate schedule. In particular, Const-Cosine achieves 61.3 on large and 73.0 on\nxlarge versus Cosine schedule achieves 62.3 on large and 73.3 on xlarge. This highlights the potential\nof training with Const-Cosine schedule in scenarios where total training duration might be unknown apriori.\n22\nPre-print\nB.5\nOPENCLIP MODELS OBTAINED BY RETRAINING AFTER REMOVING ANY DUPLICATE\nEXAMPLES FROM THE TEST SET\nOpenCLIP models (e.g., models trained on Datacomp and LAION-5B) have been trained on data\ncurated from Common Crawl. Since the retrieval tasks we constructed are built on top of data curated\nfrom Common Crawl, one may argue there is a possibility of train/test overlap in our evaluations of\nOpenCLIP models. Thus, we retrain OpenCLIP models on DataComp datasets after removing the\nsamples in our test sets. Figure 10 shows that the trends observed for OpenCLIP models holds for\nour retrained models.\n40\n50\n60\n70\n80\nRetrieval recall on 2014\u20132016\n40\n50\n60\n70\n80\nRetrieval recall on 2021\u20132022\nOur Proposed Evaluation Paradigm\nOpenAI models trained on data before 2020\nOpenClip models trained on data before 2022\nOur Models\nFigure 10: We replicate OpenCLIP models by training from scratch and removing duplicates from\nthe evaluation dataset. We observe that trends continue to hold.\nB.6\nRESULTS ON DYNAMIC CLASSIFICATION TASK\nIn the main paper, we include results on our dynamic retrieval task. For completeness, here we include\nresults on dynamic classification tasks on TIC-DataComp splits (Table 9). Along with including\nresults on all nodes of ImageNet, we also include results on classification task restricted to classes in\nthe \u201cmotor vehicles\u201d subtree of ImageNet hierarchy. For the dynamic classification task, we observe\ntrends similar to the dynamic retrieval task.\nTable 9: Zero shot performance on our TIC-DataComp-Net classification task. \u02da and \u02da\u02da denote\nmethods that violate the compute budget. We tabulate forward/backward transfer and ID performance\non retrieval tasks (Sec. 2.3). For TIC-DataComp (XL), we include results with Bestpool filtering.\nBenchmark\nMethod\nCompute\n(MACs)\nDynamic Retrieval Tasks (All)\nDynamic Retrieval Tasks (\u2018Motor Vehicles\u2019)\nBackward\nTransfer\nID Perfor-\nmance\nForward\nTransfer\nBackward\nTransfer\nID Perfor-\nmance\nForward\nTransfer\nTIC-DataComp (M)\nSequential\n3.0 \u02c6 1018\n15.9\n13.3\n9.9\n34.5\n30.0\n22.6\nPatching\n3.0 \u02c6 1018\n15.6\n13.1\n9.7\n34.4\n29.2\n22.1\nCumulative-Exp\n3.0 \u02c6 1018\n17.6\n14.4\n10.4\n36.6\n30.9\n23.5\nCumulative-Equal\n3.0 \u02c6 1018\n17.5\n14.2\n10.4\n36.4\n31.1\n23.5\nCumulative-All\n3.0 \u02c6 1018\n18.3\n14.7\n10.6\n38.2\n31.7\n23.7\nLwF\u02da\n3.8 \u02c6 1018\n16.0\n13.5\n9.9\n35.1\n30.7\n23.3\nCumulative-All\u02da\n3.9 \u02c6 1018\n20.7\n16.0\n10.9\n40.4\n32.3\n23.9\nOracle\u02da\u02da\n1.2 \u02c6 1019\n19.2\n15.2\n10.7\n38.7\n31.9\n23.5\nTIC-DataComp (L)\nSequential\n2.7 \u02c6 1019\n38.3\n36.9\n33.3\n58.4\n55.6\n49.7\nPatching\n2.7 \u02c6 1019\n38.6\n36.8\n33.3\n58.3\n54.9\n49.3\nCumulative-Exp\n2.7 \u02c6 1019\n40.2\n37.9\n34.2\n60.7\n56.8\n51.1\nCumulative-Equal\n2.7 \u02c6 1019\n40.6\n38.0\n34.2\n60.7\n56.8\n50.8\nCumulative-All\n2.7 \u02c6 1019\n41.3\n38.3\n34.4\n61.4\n56.6\n50.9\nCumulative-All\u02da\n4.1 \u02c6 1019\n43.0\n39.2\n34.6\n62.7\n57.5\n51.1\nOracle\u02da\u02da\n1.1 \u02c6 1020\n43.8\n40.0\n35.2\n62.6\n56.8\n50.7\nTIC-DataComp (XL)\nSequential\n2.7 \u02c6 1020\n55.4\n55.1\n53.3\n67.8\n66.0\n63.5\nCumulative-All\n2.7 \u02c6 1020\n58.5\n56.7\n54.3\n70.2\n67.4\n63.8\nCumulative-All\u02da\n3.5 \u02c6 1020\n58.8\n56.9\n54.3\n70.5\n67.5\n63.8\n23\nPre-print\nC\nADDITIONAL BENCHMARK DETAILS\nC.1\nFILTERING ABLATIONS ON TIC-DATACOMP\nFor Basic Filtering, Gadre et al. (2023) performs the following three steps: filter by English language\n(using fasttext (Joulin et al., 2017)), filter by caption length over two words and 5 characters, and\nfilter by image sizes with smallest dimensions over 200 pixels and aspect ratio above 3. We do not\ndefault to other filtering techniques that use off-the-shelf CLIP models from Gadre et al. (2023) to\navoid biasing dataset selection from each time step. In Fig. 11, we show that \u201cBestpool\u201d filtering\n(which filters image-text pairs with CLIP scores and ImageNet image embeddings) biases dataset\nselection to preferring old time step data over new timestamp data. Moreover, we also show that\nmodels trained with Bestpool filtering is less robust when evaluated on our dynamic tasks from 2021-\n2022 (Fig. 11). Nevertheless, for completeness and to highlight the significance of our findings even\nfor state-of-the-art filtering techniques, we perform continual learning experiments with Bestpool\nfiltering at xlarge scale which is included in the main paper. In App. B.2, we include results with\nBasic filtering at xlarge.\n60\n65\n70\n75\n80\n85\nRetrieval Performance on 2014-2016\n60\n65\n70\n75\n80\n85\n90\nRetrieval Performance on 2021-2022\nOpenClip models (Bestpool/Image/CLIP filtering)\nOpenClip models (basic/no filtering)\n2014\n2016\n2018\n2020\n2022\nYear\n0.1\n0.2\n0.3\n0.4\nFraction of samples retained\nfrom each year\nBasic\nCLIP\nBestPool\nFigure 11: (Left) Gap in retrieval performance for different OpenCLIP models that use different\nfiltering techniques. (Right) Reduction in TIC-DataComp data at different times with different\nfiltering techniques. This clearly highlights that there is a selection bias towards retaining more old\ndata for CLIP/BestPool filtering. No such bias exists for basic filtering.\nC.2\nSTATIC DATASETS CONSIDERED FOR EVALUATION\nTable 10: Evaluation tasks borrowed from Gadre et al. (2023).\nTask type\nDataset\nTask\nTest set size\nNumber of classes\nMain metric\nFood-101 Bossard et al. (2014)\nFood recognition\n25,250\n101\naccuracy\nGTSRB Stallkamp et al. (2011)\nTraffic sign recognition\n12,630\n43\naccuracy\nImageNet 1k Deng et al. (2009)\nVisual recognition\n50,000\n1,000\naccuracy\nImageNet Sketch Wang et al. (2019)\nVisual recognition\n50,889\n1,000\naccuracy\nImageNet V2 Recht et al. (2019)\nVisual recognition\n10,000\n1,000\naccuracy\nImageNet-A Hendrycks et al. (2021b)\nVisual recognition\n7,500\n200\naccuracy\nImageNet-O Hendrycks et al. (2021b)\nVisual recognition\n2,000\n200\naccuracy\nImageNet-R Hendrycks et al. (2021a)\nVisual recognition\n30,000\n200\naccuracy\nKITTI distance Geiger et al. (2012); Zhai et al. (2019)\nDistance prediction\n711\n4\naccuracy\nMNIST LeCun (1998)\nDigit recognition\n10,000\n10\naccuracy\nObjectNet Barbu et al. (2019)\nVisual recognition\n18,574\n113\naccuracy\nOxford Flowers-102 Nilsback & Zisserman (2008)\nFlower recognition\n6,149\n102\nmean per class\nOxford-IIIT Pet Parkhi et al. (2012); Zhai et al. (2019)\nPet classification\n3,669\n37\nmean per class\nPascal VOC 2007 Everingham et al. (2007)\nObject recognition\n14,976\n20\naccuracy\nPatchCamelyon Veeling et al. (2018); Zhai et al. (2019)\nMetastatic tissue cls.\n32,768\n2\naccuracy\nRendered SST2 Zhai et al. (2019)\nSentiment classification\n1,821\n2\naccuracy\nRESISC45 Cheng et al. (2017); Zhai et al. (2019)\nSatellite imagery recognition\n6,300\n45\naccuracy\nStanford Cars Krause et al. (2013)\nVehicle recognition\n8,041\n196\naccuracy\nSTL-10 Coates et al. (2011)\nVisual recognition\n8,000\n10\naccuracy\nSUN-397 Xiao et al. (2016)\nScene recognition\n108,754\n397\naccuracy\nSVHN Netzer et al. (2011); Zhai et al. (2019)\nDigit recognition\n26032\n10\naccuracy\niWildCam Beery et al. (2020); Koh et al. (2021)\nAnimal recognition\n42,791\n182\nmacro F1 score\nCamelyon17 Bandi et al. (2018); Koh et al. (2021)\nMetastatic tissue cls.\n85,054\n2\naccuracy\nClassification\nFMoW Christie et al. (2018); Koh et al. (2021)\nSatellite imagery recognition\n22,108\n62\nworst-region acc.\nRetrieval\nFlickr30k Young et al. (2014)\nImage and text retrieval\n31,014\nN/A\nR@1\n24\nPre-print\nC.3\nOUR BENCHMARK STATISTICS\nIn this section, we discuss statistics of our constructed benchmarks. Fig. 12 summarizes TIC-RedCaps,\nTIC-YFCC and TIC-DataComp dataset sizes. Fig. 13 summarizes original YFCC dataset sizes.\nTable 11, Table 12 and Table 13 present the exact numbers for these datasets. For TIC-DataComp,\nwe only discuss the sizes at xlarge scale.\n2004-2008\n2009-2010\n2011-2012\n2012-2014\nTimestamp\n0\n2\n4\nSizes\n\u00d7106\nTiC-YFCC Sizes\n2017\n2018\n2019\n2020\nTimestamp\n0\n1\n2\n3\n4\nSizes\n\u00d7106\nTiC-Redcaps Sizes\n2014\n2016\n2018\n2020\n2022\nTimestamp\n0.0\n0.5\n1.0\n1.5\n2.0\nSizes\n\u00d7109\nTiC-Datacomp Sizes (XL)\nNo Filter\nBasic Filter\nCLIP\nBestPool\nFigure 12: Number of examples in each year in our benchmarks.\nFigure 13: Number of examples in each year in original YFCC 15M. X-axis the upload month and\ny-axis is the number of examples in that month.\nTable 11: Number of examples in TIC-RedCaps in each year.\nDataset\nYear\n2017\n2018\n2019\n2020\nTIC-RedCaps\n4,220,262\n1,660,003\n2,526,575\n3,115,715\nTable 12: Number of examples in TIC-YFCC in each year.\nDataset\nYear\n2004\u20132008\n2009\u20132010\n2011\u20132012\n2012\u20132014\nTIC-YFCC\n4,337,727\n4,050,166\n3,976,339\n2,312,753\nTable 13: Number of examples in TIC-DataComp in each year before filtering.\nDataset\nYear\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nTIC-DataComp (no filter)\n244,802,598\n175,648,045\n666,019,511\n1,906,357,755\n1,877,561,875\n2,016,011,588\n1,778,751,066\n2,044,463,701\n1,442,233,121\nTIC-DataComp (basic filter)\n52,764,775\n50,757,898\n133,333,267\n400,225,598\n501,347,511\n519,575,760\n417,067,014\n494,038,122\n371,748,613\n25\nPre-print\nC.4\nCOMPUTE CONSTRAINTS FOR DIFFERENT DATASETS\nWe closely follow compute budget constraints from Gadre et al. (2023). In particular, on TIC-\nDataComp, we restrict to using exactly the same amount of overall compute as fixed in Gadre et al.\n(2023). Below we list exact total MACs on each dataset:\n\u2022 TIC-YFCC: Total MACs: 3.4 \u02c6 1018\n\u2022 TIC-RedCaps: Total MACs: 3.4 \u02c6 1018\n\u2022 TIC-DataComp medium: Total MACs: 3.0 \u02c6 1018\n\u2022 TIC-DataComp large: Total MACs: 2.7 \u02c6 1019\n\u2022 TIC-DataComp xlarge: Total MACs: 2.7 \u02c6 1020\nFor a ViT-B architecure, these values correspond to 20k iterations on TIC-YFCC (batch size: 8192),\nTIC-RedCaps (batch size: 8192), 35k iterations on TIC-DataComp (M) (batch size: 4096), 157k\niterations on TIC-DataComp (L) (batch size: 8192), and 143.5k iterations on TIC-DataComp (XL)\n(batch size: 90100). We divide these iterations equally among all time steps.\nC.5\nCREATION PIPELINE FOR EVALUATION DATASETS\nTIC-DataComp-Retrieval To create a retrieval task, we sample a batch of IID image-text pairs\nfrom different timestamps and evaluate text retrieval performance given the corresponding image\n(similarly, image retrieval given the corresponding text). Alongside general evaluations, we also\nconstruct datasets from specific domains, e.g., Covid-19 subset and Flickr subset. To create Covid-19,\nwe filter the dataset to only retain pairs where the caption contains a mention of \"covid\". This search\nprocess restricts the data to time only after 2019. For the Flickr subset, we filter the dataset to only\nretain pairs where the corresponding \u201curl\u201d contains data from Flickr.\nTIC-DataComp-Net We create our dynamic classification dataset TIC-DataComp-Net with Ima-\ngeNet classes from the CommonPool data augmented with temporal information. Our construction\nprocess draws inspiration from the LAIONet construction process described in Shirali & Hardt (2023).\nIn particular, we first filter examples where the corresponding caption contains one and only one of\nthe synsets of ImageNet-1K. We also apply additional basic filtering (Gadre et al., 2023) to make sure\nthat images are of at least 200 size in smallest dimension and the caption contains at least 2 words and\n5 characters. After filtering for examples with ImageNet synsets, we only retain examples where the\nsimilarity\u2014as evaluated by an off-the-shelf sentence embedding model (Reimers & Gurevych, 2019)\u2014\nbetween imagenet synset definition and the caption exceeds a threshold of 0.5. The goal of this filter-\ning step is to restrict examples with \u201chigh\u201d alignment between caption and imagenet synset definition.\nThis last step differs from the LAIONet construction. Crucially, unlike LAIONet, we do not filter the\nimage-text pairs with CLIP similarity scores to avoid biasing the dataset selection process.\nC.6\nDISTRIBUTION SHIFT ANALYSIS ON PROPOSED BENCHMARKS\n20\n30\n40\n50\n60\nRetrieval Performance on Flickr 2014-2015\n20\n30\n40\n50\n60\nRetrieval Performance on Covid 2021-2022\nOpenAI models trained on data before 2020\nOpenClip models trained on data before 2022\n10\n20\n30\n40\n50\n60\n70\nRetrieval Performance on Flickr 2014-2015\n10\n20\n30\n40\n50\n60\n70\nRetrieval Performance on Flickr 2021-2022\nOpenAI models trained on data before 2020\nOpenClip models trained on data before 2022\nFigure 14: (Left) Comparison of retrieval performance on COVID queries versus Flickr queries\n(construction described in App. C.5). (Right) Comparison on old Flickr versus new Flickr data.\nClearly, we observe that while gap on old versus new flickr data is small, the gap is significantly\nlarger on Covid queries.\n26\nPre-print\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nTic-DataComp-Net Performance on 2014-2015\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nTic-DataComp-Net Performance on 2021-2022\nOpenAI models trained on data before 2020\nOpenClip models trained on data before 2022\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nTic-DataComp-Net Performance on 2014-2015\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nTic-DataComp-Net Performance on 2021-2022\nMotor Vehicles Subtree\nOpenAI models trained on data before 2020\nOpenClip models trained on data before 2022\nFigure 15: (Left) Comparison on old versus new data from TIC-DataComp-Net. (Right) Comparison\non motor vehicles node from TIC-DataComp-Net. For our classification task, we observe a very\nsmall drop (\u00ab 1%) when averaged across all categories. However, we observe a substantial gap on\nclasses in \u201cmotor vehicle\u201d subtree, when comparing OpenAI and OpenCLIP models. These findings\nhighlight that while overall ImageNet classes may remain timeless, certain categories tend to evolve\nfaster than others.\nTIC-DataComp analysis through the lens of constructed evaluation tasks Here, we compare\nperformance of OpenAI and OpenCLIP models on our datasets. We observe a significant performance\ngap between OpenAI and OpenCLIP models on our dynamic retrieval task (Fig. 1). This gap widens\nnotably on retrieval queries where captions mention COVID-19. On the other hand, OpenAI and\nOpenCLIP models exhibit similar robustness for retrieval on data coming from Flickr highlighting\nthat data from some domains do not exhibit shifts that cause performance drops. For our classification\ntask, we observe a very small drop (\u00ab 1%) when averaged across all categories. However, we observe\na substantial gap on specific subtrees in ImageNet. For example, classes in \u201cmotor vehicle\u201d subtree\nshow an approximate 7% performance drop, when comparing OpenAI and OpenCLIP models. These\nfindings highlight that while overall ImageNet classes may remain timeless, certain categories tend\nto evolve faster than others. Our qualitative and quantitative analysis on TIC-DataComp clearly\nhighlights evolution of distributions and captures different properties than standard benchmarks.\nQuantitative analysis on TIC-YFCC We analyze TIC-YFCC using off-the-shelf sentence and\nimage encoders. For off-the-shelf sentence embedder, we used an existing sentence transformer from\nHugging Face (Reimers & Gurevych, 2019). For the image encoder, we use a CLIP pretrained ViT-B-\n16 model (Radford et al., 2021; Ilharco et al., 2021).\nWe first embed images from different time steps with an OpenAI CLIP encoder and then compute\nFrechet Inception Distance (FID; Seitzer (2020)). As time progresses, we observe that FID distance\nincreases with respect to data from first time step (Fig. 16). Similarly, we use the pretrained sentence\ntransformer to extract top-5 categories from Wordnet Nouns for each caption. We then obtain a\ndistribution over these Nouns for each time step. We observe that the TV distance over the distribution\nof WordNet nouns evolves over time when compared to data from the first time step.\nC.7\nCREATION PIPILINE FOR TIC-DATACOMP\nWe collect timestamps for the CommonPool dataset introduced in DataComp. We repeat the crawling\nprocess described in Gadre et al. (2023) to download WARC files from Common Crawl. In particular,\nwe follow the same multistep process which involved: (i) parsing URLs and alt-text from Common\nCrawl dumps and downloading these images; (ii) tagging images with meta data and id of the\ncommon crawl batch; and (iii) conducting evaluation set duplication and safety content filtering.\nAfter downloading the WARC files, we perform a join with the datacomp 12.8B examples. During\nthis join, we lost approximately 0.1B of examples that are no longer available online. Moreover,\nwhile performing this join, we only retain examples with their first occurrence. This is done before\nrunning any de-duplication on image-text pairs for exact matches as done in Gadre et al. (2023).\nThe source of DataComp is Common Crawl, which periodically releases web-crawled data snapshots,\ntypically on a monthly basis since 2014 with new and updated webpages. This process provides\ntimestamps at the granularity of months, spanning years 2014\u20132022.\n27\nPre-print\nFigure 16: YFCC15M distribution shift results. Analyze on TIC-YFCC using off-the-shelf sentence\nand image encoders. We first embed images from different time steps with an OpenAI CLIP\nencoder and then compute Frechet Inception Distance (FID; Seitzer (2020)). As time progresses, we\nobserve that FID distance increases with respect to data from first time step. Similarly TV distance\nover categorical distribution on Wordnet Noun synsets also increases with time when compared to\ncategorical distribution on first timestep.\n2014 2015 2016 2017 2018 2019 2020 2021 2022\nCC time step\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nTrue time step\n0.09\n0.03\n0.05\n0.02\n0.06\n0.03\n0.13\n0.15\n0.15\n0.63\n0.34\n0.38\n0.54\n0.58\n0.44\n0.29\n0.23\n0.18\n2.01\n1.16\n1.41\n1.37\n1.63\n1.17\n0.83\n0.79\n0.75\n4.36\n2.19\n3.18\n2.49\n3.14\n1.93\n2.00\n1.48\n1.76\n7.53\n5.06\n4.94\n5.25\n5.83\n3.61\n3.25\n2.40\n2.79\n10.80\n6.82\n7.79\n7.06\n7.51\n5.46\n4.84\n3.38\n3.98\n12.89\n6.68\n8.41\n9.30\n7.92\n5.76\n4.83\n3.75\n4.17\n16.24\n8.67\n9.49\n11.85\n8.93\n6.53\n5.76\n5.06\n4.96\n19.07\n11.58\n10.30\n11.42\n8.56\n7.49\n6.63\n5.81\n6.22\n12.01\n8.12\n10.05\n10.41\n8.66\n7.81\n6.66\n6.56\n6.07\n14.29\n8.21\n7.30\n8.16\n6.17\n7.32\n6.07\n5.18\n5.34\n0.00\n41.08\n9.36\n8.80\n6.15\n7.64\n6.94\n6.26\n4.79\n0.02\n0.02\n27.31\n9.75\n5.31\n7.64\n6.91\n5.86\n5.15\n0.02\n0.00\n0.01\n13.57\n8.58\n7.99\n6.49\n5.70\n4.79\n0.01\n0.01\n0.00\n0.01\n20.87\n10.94\n6.99\n5.36\n5.29\n0.01\n0.00\n0.00\n0.00\n0.01\n18.21\n10.76\n5.90\n5.49\n0.00\n0.01\n0.00\n0.00\n0.02\n0.02\n20.52\n10.06\n4.33\n0.01\n0.00\n0.00\n0.01\n0.01\n0.02\n0.01\n26.01\n9.62\n0.00\n0.00\n0.01\n0.00\n0.03\n0.00\n0.02\n0.02\n24.09\n0\n5\n10\n15\n20\n25\n30\n35\n40\nFigure 17: Comparison of Common Crawl assigned timestamp and true timestamp on a subset of 10k\nexamples containing image-text pairs from Flickr. We observe a clear trend where CC timestamps\ncorrelate with true timestamps.\nWe note that while this augmented time information may contain some noise, on average, we find\nit to be a reasonably accurate proxy for the upload time of web pages. To perform an initial check,\nwe note that our data contains images from flickr which provides an API to query for true upload\ntimestamp. So we extract 10k examples from our benchmark TIC-DataComp and query Flickr for\ntheir true timestamp. Fig. 17 summarizes true timestamps with timestamps extracted from CC.\nD\nADDITIONAL EXPERIMENTAL DETAILS\nD.1\nADDITIONAL DETAILS ON IMAGENET IID SPLIT CONTINUAL LEARNING EXPERIMENT\nWith ImageNet data, we consider 2, 4 and 8 splits including the full dataset. This design is inspired\nby Ash & Adams (2020). We consider ViT-B/16 architecture trained for 300 epochs on full data and\nsplit the iterations corresponding to 300 epochs equally among k splits when training sequentially.\nWe keep all other hyperparameters, such as learning rate, optimizer, and batch size, set to the standard\nvalues typically employed for training ViT-B/16 on the ImageNet dataset (Dosovitskiy et al., 2020).\n28\nPre-print\nWe also employ \u21132 regularization and augmentation on ImageNet training data. We evaluate the\nmodels on IID ImageNet test set.\nD.2\nTRAINING AND HYPERPARAMETER DETAILS\nWe create a common experimental setup by fixing the training procedure for sequential runs. Unless\nspecified otherwise, we closely follow the CLIP training recipe proposed in (Ilharco et al., 2021;\nRadford et al., 2021) where we train models with a contrastive objective over images and captions.\nGiven a set of image-text pairs, we train an image encoder and a text encoder such that the similarity\nbetween the representations of images and their corresponding text is maximized relative to unaligned\npairs. Only LwF deviates from this standard training procedure. For each benchmark, we pick\nVision Transformers (ViTs) as the image encoder, in particular, we fix the model architecture to\nViT-B/16 (Dosovitskiy et al., 2021). We fix the Adam optimizer and its hyperparameters to values\nsuggested in (Ilharco et al., 2021).\nWe primarily ablate over only two things: maximum learning rate with cosine learning schedule\nand warm up iterations for sequential training. For choosing other hyperparameters, we follow the\nOpenCLIP library (Ilharco et al., 2021).\nD.3\nREPLAY SIZES WITH EXP AND EQUAL STRATEGIES\nWe default to using 2D size of data where D represents incoming data size from new time step. As\ndescribed in the main text, for -Exp, we reduce the buffer size by half of what we used at old time\nstep and use rest of the half as data from previous time step. App. C.3 lists the dataset sizes for each\nbenchmark which dictate the exact buffer sizes.\n29\n"
  },
  {
    "title": "TD-MPC2: Scalable, Robust World Models for Continuous Control",
    "link": "https://arxiv.org/pdf/2310.16828.pdf",
    "upvote": "5",
    "text": "TD-MPC2:\nScalable, Robust World Models for Continuous Control\nNicklas Hansen\u22c6, Hao Su\u22c6\u2020, Xiaolong Wang\u22c6\u2020\n\u22c6University of California San Diego,\n\u2020Equal advising\n{nihansen,haosu,xiw012}@ucsd.edu\nMulti-task\nSingle-task\n1M\n10M\n100M\n1B\nModel parameters\n0\n20\n40\n60\n80\nNormalized score\n1M\n5M\n19M\n48M\n317M\nDMControl & Meta\u2212World\n80 tasks\nTD-MPC2\nTD-MPC\n25\n50\n75\n100\nSAC\nDreamerV3\nTD-MPC\nTD-MPC2\nDMControl\n39 tasks\n25\n50\n75\n100\nSAC\nDreamerV3\nTD-MPC\nTD-MPC2\nMeta\u2212World\n50 tasks\n0\n25\n50\n75\n100\nSAC\nDreamerV3\nTD-MPC\nTD-MPC2\nManiSkill2\n5 tasks\n0\n25\n50\n75\n100\nSAC\nDreamerV3\nTD-MPC\nTD-MPC2\nLocomotion\n7 tasks\n0\n25\n50\n75\nSAC\nDreamerV3\nTD-MPC\nTD-MPC2\nMyoSuite\n10 tasks\n0\n25\n50\n75\nSAC\nDreamerV3\nTD-MPC\nTD-MPC2\nPick YCB\n1 task\nFigure 1. Overview. TD-MPC2 compares favorably to existing model-free and model-based RL\nmethods across 104 continuous control tasks spanning multiple domains, with a single set of hyper-\nparameters (right). We further demonstrate the scalability of TD-MPC2 by training a single 317M\nparameter agent to perform 80 tasks across multiple domains, embodiments, and action spaces (left).\nABSTRACT\nTD-MPC is a model-based reinforcement learning (RL) algorithm that performs\nlocal trajectory optimization in the latent space of a learned implicit (decoder-\nfree) world model. In this work, we present TD-MPC2: a series of improvements\nupon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves signifi-\ncantly over baselines across 104 online RL tasks spanning 4 diverse task domains,\nachieving consistently strong results with a single set of hyperparameters. We\nfurther show that agent capabilities increase with model and data size, and suc-\ncessfully train a single 317M parameter agent to perform 80 tasks across multiple\ntask domains, embodiments, and action spaces. We conclude with an account of\nlessons, opportunities, and risks associated with large TD-MPC2 agents.\nExplore videos, models, data, code, and more at\nhttps://nicklashansen.github.io/td-mpc2\n1\nINTRODUCTION\nTraining large models on internet-scale datasets has led to generalist models that perform a wide\nvariety of language and vision tasks (Brown et al., 2020; He et al., 2022; Kirillov et al., 2023). The\nsuccess of these models can largely be attributed to the availability of enormous datasets, and care-\nfully designed architectures that reliably scale with model and data size. While researchers have\nrecently extended this paradigm to robotics (Reed et al., 2022; Brohan et al., 2023), a generalist\nembodied agent that learns to perform diverse control tasks via low-level actions, across multiple\nembodiments, from large uncurated (i.e., mixed-quality) datasets remains an elusive goal. We argue\nthat current approaches to generalist embodied agents suffer from (a) the assumption of near-expert\ntrajectories for behavior cloning which severely limits the amount of available data (Reed et al.,\n2022; Lee et al., 2022; Kumar et al., 2022; Schubert et al., 2023; Driess et al., 2023; Brohan et al.,\n2023), and (b) a lack of scalable continuous control algorithms that are able to consume large uncu-\nrated datasets.\n1\narXiv:2310.16828v1  [cs.LG]  25 Oct 2023\nFigure 2. Tasks. TD-MPC2 performs 104 diverse tasks from (left to right) DMControl (Tassa et al.,\n2018), Meta-World (Yu et al., 2019), ManiSkill2 (Gu et al., 2023), and MyoSuite (Caggiano et al.,\n2022), with a single set of hyperparameters. See Appendix B for visualization of all tasks.\nReinforcement Learning (RL) is an ideal framework for extracting expert behavior from uncurated\ndatasets. However, most existing RL algorithms (Lillicrap et al., 2016; Haarnoja et al., 2018) are\ndesigned for single-task learning and rely on per-task hyperparameters, with no principled method\nfor selecting those hyperparameters (Zhang et al., 2021). An algorithm that can consume large multi-\ntask datasets will invariably need to be robust to variation between different tasks (e.g., action space\ndimensionality, difficulty of exploration, and reward distribution). In this work, we present TD-\nMPC2: a significant step towards achieving this goal. TD-MPC2 is a model-based RL algorithm\ndesigned for learning generalist world models on large uncurated datasets composed of multiple\ntask domains, embodiments, and action spaces, with data sourced from behavior policies that cover\na wide range of skill levels, and without the need for hyperparameter-tuning.\nOur algorithm, which builds upon TD-MPC (Hansen et al., 2022), performs local trajectory opti-\nmization in the latent space of a learned implicit (decoder-free) world model. While the TD-MPC\nfamily of algorithms has demonstrated strong empirical performance in prior work (Hansen et al.,\n2022; 2023; Yuan et al., 2022; Yang et al., 2023; Feng et al., 2023; Chitnis et al., 2023; Zhu et al.,\n2023; Lancaster et al., 2023), most successes have been limited to single-task learning with little em-\nphasis on scaling. As shown in Figure 1, na\u00a8\u0131vely increasing model and data size of TD-MPC often\nleads to a net decrease in agent performance, as is commonly observed in RL literature (Kumar et al.,\n2023). In contrast, scaling TD-MPC2 leads to consistently improved capabilities. Our algorithmic\ncontributions, which have been key to achieving this milestone, are two-fold: (1) improved algo-\nrithmic robustness by revisiting core design choices, and (2) careful design of an architecture that\ncan accommodate datasets with multiple embodiments and action spaces without relying on domain\nknowledge. The resulting algorithm, TD-MPC2, is scalable, robust, and can be applied to a variety\nof single-task and multi-task continuous control problems using a single set of hyperparameters.\nWe evaluate TD-MPC2 across a total of 104 diverse continuous control tasks spanning 4 task do-\nmains: DMControl (Tassa et al., 2018), Meta-World (Yu et al., 2019), ManiSkill2 (Gu et al., 2023),\nand MyoSuite (Caggiano et al., 2022). We summarize our results in Figure 1, and visualize task do-\nmains in Figure 2. Tasks include high-dimensional state and action spaces (up to A \u2208 R39), sparse\nrewards, multi-object manipulation, physiologically accurate musculoskeletal motor control, com-\nplex locomotion (e.g. Dog and Humanoid embodiments), and cover a wide range of task difficulties.\nOur results demonstrate that TD-MPC2 consistently outperforms existing model-based and model-\nfree methods, using the same hyperparameters across all tasks (Figure 1, right). Here, \u201cLocomotion\u201d\nand \u201cPick YCB\u201d are particularly challenging subsets of DMControl and ManiSkill2, respectively.\nWe further show that agent capabilities increase with model and data size, and successfully train\na single 317M parameter world model to perform 80 tasks across multiple task domains, embod-\niments, and action spaces (Figure 1, left). In support of open-source science, we publicly release\n300+ model checkpoints, datasets, and code for training and evaluating TD-MPC2 agents,\nwhich is available at https://nicklashansen.github.io/td-mpc2. We conclude the\npaper with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents.\n2\nBACKGROUND\nReinforcement Learning (RL) aims to learn a policy from interaction with an environment, for-\nmulated as a Markov Decision Process (MDP) (Bellman, 1957).\nWe focus on infinite-horizon\nMDPs with continuous action spaces, which can be formalized as a tuple (S, A, T , R, \u03b3) where\ns \u2208 S are states, a \u2208 A are actions, T : S \u00d7 A 7\u2192 S is the transition function, R: S \u00d7 A 7\u2192 R\nis a reward function associated with a particular task, and \u03b3 is a discount factor. The goal is to\n2\nderive a control policy \u03c0: S 7\u2192 A such that the expected discounted sum of rewards (return)\nE\u03c0 [P\u221e\nt=0 \u03b3trt] , rt = R(st, \u03c0(st)) is maximized. In this work, we obtain \u03c0 by learning a world\nmodel (model of the environment) and then select actions by planning with the learned model.\nModel Predictive Control (MPC) is a general framework for model-based control that optimizes\naction sequences at:t+H of finite length such that return is maximized (or cost is minimized) over\nthe time horizon H, which corresponds to solving the following optimization problem:\n\u03c0(st) = arg max\nat:t+H E\n\" H\nX\ni=0\n\u03b3t+iR(st+i, at+i)\n#\n.\n(1)\nThe return of a candidate trajectory is estimated by simulating it with the learned model (Negenborn\net al., 2005). Thus, a policy obtained by Equation 1 will invariably be a (temporally) locally optimal\npolicy and is not guaranteed (nor likely) to be a solution to the general reinforcement learning\nproblem outlined above. As we discuss in the following, TD-MPC2 addresses this shortcoming\nof local trajectory optimization by bootstrapping return estimates beyond horizon H with a learned\nterminal value function.\n3\nTD-MPC2\nOur work builds upon TD-MPC (Hansen et al., 2022), a model-based RL algorithm that performs\nlocal trajectory optimization (planning) in the latent space of a learned implicit world model. TD-\nMPC2 marks the beginning of a new era for model-based RL, in which massively multitask world\nmodels are trained and subsequently finetuned to new tasks. Specifically, we propose a series of\nimprovements to the TD-MPC algorithm, which have been key to achieving strong algorithmic\nrobustness (can use the same hyperparameters across all tasks) and scaling its world model to 300\u00d7\nmore parameters than previously. In the following, we introduce the TD-MPC2 algorithm in detail.\n3.1\nLEARNING AN IMPLICIT WORLD MODEL\n\u00e2\n\u00e2\nenc\nenc\nenc\n\u00e2\n\u00e2r\na\na\ns1\ns2\ns3\nz1\nz2\nz3\n\u00e2r\n\u00e2r\n\u00e2q\n\u00e2q\n\u00e2q\n1\n2\n1\n2\n3\n1\n2\n3\n1\n2\n3\nFigure 3. The TD-MPC2 architecture.\nObservations s are encoded into their\n(normalized) latent representation z. The\nmodel then recurrently predicts actions \u02c6a,\nrewards \u02c6r, and terminal values \u02c6q, without\ndecoding future observations.\nLearning a generative model of the environment using\na reconstruction (decoder) objective is tempting due to\nits rich learning signal. However, accurately predict-\ning raw future observations (e.g., images or propriocep-\ntive features) over long time horizons is a difficult prob-\nlem, and does not necessarily lead to effective control\n(Lambert et al., 2020). Rather than explicitly model-\ning dynamics using reconstruction, TD-MPC2 aims to\nlearn a maximally useful model: a model that accurately\npredicts outcomes (returns) conditioned on a sequence\nof actions. Specifically, TD-MPC2 learns an implicit,\ncontrol-centric world model from environment interac-\ntion using a combination of joint-embedding prediction\n(Grill et al., 2020), reward prediction, and TD-learning\n(Sutton, 1998), without decoding observations. We ar-\ngue that this alternative formulation of model-based RL\nis key to modeling large datasets with modest model\nsizes. The world model can subsequently be used for\ndecision-making by performing local trajectory opti-\nmization (planning) following the MPC framework.\nComponents. The TD-MPC2 architecture is shown in Figure 3 and consists of five components:\nEncoder\nz = h(s, e)\n\u25b7 Maps observations to their latent representations\nLatent dynamics\nz\u2032 = d(z, a, e)\n\u25b7 Models (latent) forward dynamics\nReward\n\u02c6r = R(z, a, e)\n\u25b7 Predicts reward r of a transition\nTerminal value\n\u02c6q = Q(z, a, e)\n\u25b7 Predicts discounted sum of rewards (return)\nPolicy prior\n\u02c6a = p(z, e)\n\u25b7 Predicts action a\u2217 that maximizes Q\n(2)\nwhere s and a are states and actions, z is the latent representation, and e is a learnable task embed-\nding for use in multitask world models. For visual clarity, we will omit e in the following unless it is\n3\nparticularly relevant. The policy prior p serves to guide the sample-based trajectory optimizer (plan-\nner), and to reduce the computational cost of TD-learning. During online interaction, TD-MPC2\nmaintains a replay buffer B with trajectories, and iteratively (i) updates the world model using data\nsampled from B, and (ii) collects new environment data by planning with the learned model.\nModel objective. The h, d, R, Q components are jointly optimized to minimize the objective\nL (\u03b8) .=\nE\n(s,a,r,s\u2032)0:H\u223cB\n\uf8ee\n\uf8ef\uf8f0\nH\nX\nt=0\n\u03bbt\n\uf8eb\n\uf8ec\n\uf8ed\u2225 z\u2032\nt \u2212 sg(h(s\u2032\nt))\u22252\n2\n|\n{z\n}\nJoint-embedding prediction\n+ CE(\u02c6rt, rt)\n|\n{z\n}\nReward prediction\n+ CE(\u02c6qt, qt)\n|\n{z\n}\nValue prediction\n\uf8f6\n\uf8f7\n\uf8f8\n\uf8f9\n\uf8fa\uf8fb ,\n(3)\nwhere sg is the stop-grad operator, (z\u2032\nt, \u02c6rt, \u02c6qt) are as defined in Equation 2, qt .= rt+ \u00afQ(z\u2032\nt, p(z\u2032\nt))\nis the TD-target at step t, \u03bb \u2208 (0, 1] is a constant coefficient that weighs temporally farther time\nsteps less, and CE is the cross-entropy. \u00afQ used to compute the TD-target is an exponential moving\naverage (EMA) of Q (Lillicrap et al., 2016). As the magnitude of rewards may differ drastically\nbetween tasks, TD-MPC2 formulates reward and value prediction as a discrete regression (multi-\nclass classification) problem in a log-transformed space, which is optimized by minimizing cross-\nentropy with rt, qt as soft targets (Bellemare et al., 2017; Kumar et al., 2023; Hafner et al., 2023).\nPolicy objective. The policy prior p is a stochastic maximum entropy (Ziebart et al., 2008; Haarnoja\net al., 2018) policy that learns to maximize the objective\nLp(\u03b8) .=\nE\n(s,a)0:H\u223cB\n\" H\nX\nt=0\n\u03bbt [\u03b1Q(zt, p(zt)) \u2212 \u03b2H(p(\u00b7|zt))]\n#\n, zt+1 = d(zt, at), z0 = h(s0) , (4)\nwhere H is the entropy of p which can be computed in closed form. Gradients of Lp(\u03b8) are taken wrt.\np only. As magnitude of the value estimate Q(zt, p(zt)) and entropy H can vary greatly between\ndatasets and different stages of training, it is necessary to balance the two losses to prevent premature\nentropy collapse (Yarats et al., 2021). A common choice for automatically tuning \u03b1, \u03b2 is to keep\none of them constant, and adjusting the other based on an entropy target (Haarnoja et al., 2018) or\nmoving statistics (Hafner et al., 2023). In practice, we opt for tuning \u03b1 via moving statistics, but\nempirically did not observe any significant difference in results between these two options.\nArchitecture. All components of TD-MPC2 are implemented as MLPs with intermediate linear\nlayers followed by LayerNorm (Ba et al., 2016) and Mish (Misra, 2019) activations. To mitigate\nexploding gradients, we normalize the latent representation by projecting z into L fixed-dimensional\nsimplices using a softmax operation (Lavoie et al. (2022); see Appendix H for an implementation).\nA key benefit of embedding z as simplices (as opposed to e.g. a discrete representation or squashing)\nis that it naturally biases the representation towards sparsity without enforcing hard constraints. We\ndub this normalization scheme SimNorm. Let V be the dimensionality of each simplex g constructed\nfrom L partitions (groups) of z. SimNorm then applies the following transformation:\nz\u25e6 .= [gi, . . . , gL] , gi =\nezi:i+V /\u03c4\nPV\nj=1 ezi:i+V /\u03c4 ,\n(5)\nwhere z\u25e6 is the simplicial embedding of z, [\u00b7] denotes concatenation, and \u03c4 > 0 is a temperature\nparameter that modulates the \u201csparsity\u201d of the representation. As we will demonstrate in our ex-\nperiments, SimNorm is essential to the training stability of TD-MPC2. Finally, to reduce bias in\nTD-targets generated by \u00afQ, we learn an ensemble of Q-functions using the objective from Equa-\ntion 3 and maintain \u00afQ as an EMA of each Q-function. We use 5 Q-functions in practice. Targets are\nthen computed as the minimum of two randomly sub-sampled \u00afQ-functions (Chen et al., 2021).\n3.2\nMODEL PREDICTIVE CONTROL WITH A POLICY PRIOR\nTD-MPC2 derives its closed-loop control policy by planning with the learned world model. Specif-\nically, our approach leverages the MPC framework for local trajectory optimization using Model\nPredictive Path Integral (MPPI) (Williams et al., 2015) as a derivative-free optimizer with sampled\naction sequences (at, at+1, . . . , at+H) of length H evaluated by rolling out latent trajectories with\nthe model. At each decision step, we estimate parameters \u00b5\u2217, \u03c3\u2217 of a time-dependent multivariate\nGaussian with diagonal covariance such that expected return is maximized, i.e.,\n\u00b5\u2217, \u03c3\u2217 = arg max\n(\u00b5,\u03c3)\nE\n(at,at+1,...,at+H)\u223cN (\u00b5,\u03c32)\n\"\n\u03b3HQ(zt+H, at+H) +\nH\u22121\nX\nh=t\n\u03b3hR(zh, ah)\n#\n,\n(6)\n4\n0\n1M\n2M\n3M\n4M\n0\n500\n1000\nDMControl\n39 tasks\n0\n1M\n2M\n0\n50\n100\nMeta\u2212World\n50 tasks\n0\n1M\n2M\n3M\n4M\n0\n50\n100\nManiSkill2\n5 tasks\n0\n1M\n2M\n0\n50\n100\nMyoSuite\n10 tasks\nSAC\nDreamerV3\nTD-MPC\nTD-MPC2\nFigure 4. Single-task RL. Episode return (DMControl) and success rate (others) as a function of\nenvironment steps across 104 continuous control tasks spanning 4 diverse task domains. TD-MPC2\nachieves higher data-efficiency and asymptotic performance than existing methods, while using the\nsame hyperparameters across all tasks. Mean and 95% CIs over 3 seeds.\nwhere \u00b5, \u03c3 \u2208 RH\u00d7m, A \u2208 Rm. Equation 6 is solved by iteratively sampling action sequences from\nN(\u00b5, \u03c32), evaluating their expected return, and updating \u00b5, \u03c3 based on a weighted average. Notably,\nEquation 6 estimates the full RL objective introduced in Section 2 by bootstrapping with the learned\nterminal value function beyond horizon H. TD-MPC2 repeats this iterative planning process for\na fixed number of iterations and executes the first action at \u223c N(\u00b5\u2217\nt , \u03c3\u2217\nt ) in the environment. To\naccelerate convergence of planning, a fraction of action sequences originate from the policy prior p,\nand we warm-start planning by initializing (\u00b5, \u03c3) as the solution to the previous decision step shifted\nby 1. Refer to Hansen et al. (2022) for more details about the planning procedure.\n3.3\nTRAINING GENERALIST TD-MPC2 AGENTS\nThe success of TD-MPC2 in diverse single-task problems can be attributed to the algorithm outlined\nabove. However, learning a large generalist TD-MPC2 agent that performs a variety of tasks across\nmultiple task domains, embodiments, and action spaces poses several unique challenges: (i) how to\nlearn and represent task semantics? (ii) how to accommodate multiple observation and action spaces\nwithout specific domain knowledge? (iii) how to leverage the learned model for few-shot learning\nof new tasks? We describe our approach to multitask model learning in the following.\nLearnable task embeddings. To succeed in a multitask setting, an agent needs to learn a common\nrepresentation that takes advantage of task similarities, while still retaining the ability to differen-\ntiate between tasks at test-time. When task or domain knowledge is available, e.g. in the form of\nnatural language instructions, the task embedding e from Equation 2 may encode such information.\nHowever, in the general case where domain knowledge cannot be assumed, we may instead choose\nto learn the task embeddings (and, implicitly, task relations) from data. TD-MPC2 conditions all of\nits five components with a learnable, fixed-dimensional task embedding e, which is jointly trained\ntogether with other components of the model. To improve training stability, we constrain the \u21132-\nnorm of e to be \u2264 1. When finetuning a multitask TD-MPC2 agent to a new task, we can choose to\neither initialize e as the embedding of a semantically similar task, or simply as a random vector.\nAction masking. TD-MPC2 learns to perform tasks with a variety of observation and action spaces,\nwithout any domain knowledge. To do so, we zero-pad all model inputs and outputs to their largest\nrespective dimensions, and mask out invalid action dimensions in predictions made by the policy\nprior p during both training and inference. This ensures that prediction errors in invalid dimensions\ndo not influence TD-target estimation, and prevents p from falsely inflating its entropy for tasks with\nsmall action spaces. We similarly only sample actions along valid dimensions during planning.\n4\nEXPERIMENTS\nWe evaluate TD-MPC2 across a total of 104 diverse continuous control tasks spanning 4 task do-\nmains: DMControl (Tassa et al., 2018), Meta-World (Yu et al., 2019), ManiSkill2 (Gu et al., 2023),\nand MyoSuite (Caggiano et al., 2022). Tasks include high-dimensional state and action spaces (up to\nA \u2208 R39), sparse rewards, multi-object manipulation, physiologically accurate musculoskeletal mo-\ntor control, complex locomotion (e.g. Dog and Humanoid embodiments), and cover a wide range\nof task difficulties. In support of open-source science, we publicly release 300+ model check-\npoints, datasets, and code for training and evaluating TD-MPC2 agents, which is available at\nhttps://nicklashansen.github.io/td-mpc2.\n5\n0\n4M\n8M\n12M\n0\n500\n1000\nDog Run\n0\n4M\n8M\n12M\nDog Trot\n0\n4M\n8M\n12M\nDog Walk\n0\n4M\n8M\n12M\nHumanoid Run\n0\n4M\n8M\n12M\nHumanoid Walk\nSAC\nDreamerV3\nTD-MPC\nTD-MPC2\nFigure 5. High-dimensional locomotion. Episode return as a function of environment steps in\nHumanoid (A \u2208 R21) and Dog (A \u2208 R38) locomotion tasks from DMControl. SAC and DreamerV3\nare prone to numerical instabilities in Dog tasks, and are significantly less data-efficient than TD-\nMPC2 in Humanoid tasks. Mean and 95% CIs over 3 seeds. See Appendix D for more tasks.\n0\n1M\n2M\n3M\n4M\n0\n50\n100\nLift Cube\n0\n1M\n2M\n3M\n4M\nPick Cube\n0\n4M\n8M\n12M\nPick YCB\n0\n1M\n2M\n3M\n4M\nStack Cube\n0\n1M\n2M\n3M\n4M\nTurn Faucet\nSAC\nDreamerV3\nTD-MPC\nTD-MPC2\nFigure 6. Object manipulation. Success rate (%) as a function of environment steps on 5 object\nmanipulation tasks from ManiSkill2. Pick YCB considers manipulation of all 74 objects from the\nYCB (Calli et al., 2015) dataset. TD-MPC2 excels at hard tasks. Mean and 95% CIs over 3 seeds.\nWe seek to answer three core research questions through experimentation:\n\u2022 Comparison to existing methods. How does TD-MPC2 compare to state-of-the-art model-free\n(SAC) and model-based (DreamerV3, TD-MPC) methods for data-efficient continuous control?\n\u2022 Scaling. Do the algorithmic innovations of TD-MPC2 lead to improved agent capabilities as\nmodel and data size increases? Can a single agent learn to perform diverse skills across multiple\ntask domains, embodiments, and action spaces?\n\u2022 Analysis. How do the specific design choices introduced in TD-MPC2 influence downstream\ntask performance? How much does planning contribute to its success? Are the learned task\nembeddings semantically meaningful? Can large multi-task agents be adapted to unseen tasks?\nBaselines. Our baselines represent the state-of-the-art in data-efficient RL, and include (1) Soft\nActor-Critic (SAC) (Haarnoja et al., 2018), a model-free actor-critic algorithm based on maximum\nentropy RL, (2) DreamerV3 (Hafner et al., 2023), a model-based method that optimizes a model-\nfree policy with rollouts from a learned generative model of the environment, and (3) the original\nversion of TD-MPC (Hansen et al., 2022), a model-based RL algorithm that performs local trajec-\ntory optimization (planning) in the latent space of a learned implicit (non-generative) world model.\nSAC and TD-MPC use task-specific hyperparameters, whereas TD-MPC2 uses the same hyperpa-\nrameters across all tasks. Additionally, it is worth noting that both SAC and TD-MPC use a larger\nbatch size of 512, while 256 is sufficient for stable learning with TD-MPC2. Similarly, DreamerV3\nuses a high update-to-data (UTD) ratio of 512, whereas TD-MPC2 uses a UTD of 1 by default. We\nuse a 5M parameter TD-MPC2 agent in all experiments (unless stated otherwise). For reference, the\nDreamerV3 baseline has approx. 20M learnable parameters. See Appendix H for more details.\n4.1\nRESULTS\nComparison to existing methods. We first compare the data-efficiency of TD-MPC2 to a set of\nstrong baselines on 104 diverse tasks in an online RL setting. Aggregate results are shown in Fig-\nure 4. We find that TD-MPC2 outperforms prior methods across all task domains. The MyoSuite\nresults are particularly noteworthy, as we did not run any TD-MPC2 experiments on this bench-\nmark prior to the reported results. Individual task performances on some of the most difficult tasks\n(high-dimensional locomotion and multi-object manipulation) are shown in Figure 5 and Figure 6.\nTD-MPC2 outperforms baselines by a large margin on these tasks, despite using the same hyper-\nparameters across all tasks. Notably, TD-MPC sometimes diverges due to exploding gradients,\nwhereas TD-MPC2 remains stable. We provide per-task visualization of gradients in Appendix G.\nSimilarly, we observe that DreamerV3 experiences occasional numerical instabilities (Dog) and\n6\n1M\n10M\n100M\n1B\nModel parameters\n0\n20\n40\n60\n80\nNormalized score\n16.0\n49.5\n57.1\n68.0\n70.6\nDMControl & Meta\u2212World\n80 tasks\nTD-MPC2\nTD-MPC\n1M\n10M\n100M\n1B\nModel parameters\n0\n20\n40\n60\n80\nNormalized score\n18.9\n28.3\n54.2\n59.4\n71.4\nDMControl\n30 tasks\nWalker Walk\nWalker Run\nCheetah Run\nCartpole Balance\nCartpole Swingup\nCup Catch\nFinger Spin\nFinger Turn Easy\nFinger Turn Hard\nHopper Stand\nHopper Hop\nCheetah Jump\nCup Spin\nDial Turn\nDoor Open\nDoor Close\nPick Place\nPick Place Wall\nReach\nReach Wall\nStick Push\nStick Pull\nFigure 7. Massively multi-task world models. (Left) Normalized score as a function of model\nsize on the two 80-task and 30-task datasets. TD-MPC2 capabilities scale with model size. (Right)\nT-SNE (van der Maaten & Hinton, 2008) visualization of task embeddings learned by a TD-MPC2\nagent trained on 80 tasks from DMControl and Meta-World. A subset of labels are shown for clarity.\ngenerally struggles with tasks that require fine-grained object manipulation (lift, pick, stack). See\nAppendix D for the full single-task RL results.\nTable 1.\nTraining cost.\nAp-\nproximate TD-MPC2 training cost\non the 80-task dataset, reported in\nGPU days on a single NVIDIA\nGeForce RTX 3090 GPU. We also\nlist the normalized score achieved\nby each model at end of training.\nParams (M)\nGPU days\nScore\n1\n3.7\n16.0\n5\n4.2\n49.5\n19\n5.3\n57.1\n48\n12\n68.0\n317\n33\n70.6\nMassively multitask world models. To demonstrate that our\nproposed improvements facilitate scaling of world models, we\nevaluate the performance of 5 multitask models ranging from\n1M to 317M parameters on a collection of 80 diverse tasks that\nspan multiple task domains and vary greatly in objective, em-\nbodiment, and action space. Models are trained on a dataset\nof 545M transitions obtained from the replay buffers of 240\nsingle-task TD-MPC2 agents, and thus contain a wide variety\nof behaviors ranging from random to expert policies. The task\nset consists of all 50 Meta-World tasks, as well as 30 DMCon-\ntrol tasks. The DMControl task set includes 19 original DM-\nControl tasks, as well as 11 new tasks. For completeness, we\ninclude a separate set of scaling results on the 30-task DMCon-\ntrol subset (345M transitions) as well. Due to our careful de-\nsign of the TD-MPC2 algorithm, scaling up is straightforward:\nto improve rate of convergence we use a 4\u00d7 larger batch size (1024) compared to the single-task\nexperiments, but make no other changes to hyperparameters.\n0\n20\n40\n60\nNormalized score\n24.0\n47.0\nFinetuning\n10 tasks\nFrom scratch\nFinetuned\nFigure 8.\nFinetuning.\nScore of a 19M parameter\nTD-MPC2 agent trained\non 70 tasks and finetuned\nonline to each of 10 held-\nout tasks for 20k environ-\nment steps. 3 seeds.\nScaling TD-MPC2 to 317M parameters. Our scaling results are\nshown in Figure 7. To summarize agent performance with a single\nmetric, we produce a normalized score that is an average of all indi-\nvidual task success rates (Meta-World) and episode returns normalized\nto the [0, 100] range (DMControl). We observe that agent capabili-\nties consistently increase with model size on both task sets. Notably,\nperformance does not appear to have saturated for our largest models\n(317M parameters) on either dataset, and we can thus expect results\nto continue improving beyond our considered model sizes. We refrain\nfrom formulating a scaling law, but note that normalized score appears\nto scale linearly with the log of model parameters (gray line in Fig-\nure 7). We also report approximate training costs in Table 1. The 317M\nparameter model can be trained with limited computational resources.\nTo better understand why multitask model learning is successful, we\nexplore the task embeddings learned by TD-MPC2 (Figure 7, right).\nIntriguingly, tasks that are semantically similar (e.g., Door Open and\nDoor Close) are close in the learned task embedding space. However,\nembedding similarity appears to align more closely with task dynamics (embodiment, objects) than\nobjective (walk, run). This makes intuitive sense, as dynamics are tightly coupled with control.\nFew-shot learning. While our work mainly focuses on the scaling and robustness of world models,\nwe also explore the efficacy of finetuning pretrained world models for few-shot learning of unseen\ntasks. Specifically, we pretrain a 19M parameter TD-MPC2 agent on 70 tasks from DMControl and\nMeta-World, and na\u00a8\u0131vely finetune the full model to each of 10 held-out tasks (5 from each domain)\nvia online RL with an initially empty replay buffer and no changes to hyperparameters. Aggregate\n7\n0\n4M\n8M\n12M\n0\n20\n40\n60\n80\nNormalized score\nActor\n3 tasks\nPolicy\nPlanning\nPlanning + policy\n30\n40\n50\n60\nNormalized score\n42.2\n53.7\n54.2\nActor\nMultitask (80)\nPolicy\nPlanning\nPlanning + policy\n0\n4M\n8M\n12M\n0\n20\n40\n60\n80\nNormalized score\nNormalization\n3 tasks\nNo Norm\nSimNorm\nLN + SimNorm\n30\n40\n50\n60\nNormalized score\n46.8\n51.0\n54.2\nNormalization\nMultitask (80)\nNo Norm\nSimNorm\nLN + SimNorm\n0\n4M\n8M\n12M\n0\n20\n40\n60\n80\nNormalized score\nRegression\n3 tasks\nContinuous\nDiscrete\n30\n40\n50\n60\nNormalized score\n49.6\n54.2\nRegression\nMultitask (80)\nContinuous\nDiscrete\n0\n4M\n8M\n12M\n0\n20\n40\n60\n80\nNormalized score\nQ\u2212functions\n3 tasks\n2\n5\n10\n30\n40\n50\n60\nNormalized score\n53.5\n54.2\n57.0\nQ\u2212functions\nMultitask (80)\n2\n5\n10\nFigure 9. Ablations. (Curves) Normalized score as a function of environment steps, averaged\nacross three of the most difficult tasks: Dog Run, Humanoid Walk (DMControl), and Pick YCB\n(ManiSkill2). Mean and 95% CIs over 3 random seeds. (Bars) Normalized score of 19M parameter\nmultitask (80 tasks) TD-MPC2 agents. Our ablations highlight the relative importance of each\ndesign choice; red is the default formulation of TD-MPC2. See Appendix D for more ablations.\nresults are shown in Figure 8. We find that TD-MPC2 improves 2\u00d7 over learning from scratch\non new tasks in the low-data regime (20k environment steps1). Although finetuning world models\nto new tasks is very much an open research problem, our exploratory results are promising. See\nAppendix E for experiment details and individual task curves.\nAblations. We ablate most of our design choices for TD-MPC2, including choice of actor, various\nnormalization techniques, regression objective, and number of Q-functions. Our main ablations,\nshown in Figure 9, are conducted on three of the most difficult online RL tasks, as well as large-\nscale multitask training (80 tasks). We observe that all of our proposed improvements contribute\nmeaningfully to the robustness and strong performance of TD-MPC2 in both single-task RL and\nmulti-task RL. Interestingly, we find that the relative importance of each design choice is consistent\nacross both settings. Lastly, we also ablate normalization of the learned task embeddings, shown in\nAppendix F. The results indicate that maintaining a normalized task embedding space (\u21132-norm of 1)\nis moderately important for stable multitask training, and results in more meaningful task relations.\n5\nLESSONS, OPPORTUNITIES, AND RISKS\nLessons. Historically, RL algorithms have been notoriously sensitive to architecture, hyperparame-\nters, characteristics of the task, and even random seed (Henderson et al., 2018), with no principled\nmethod for tuning the algorithms. As a result, successful application of deep RL often requires large\nteams of experts with significant computational resources (Berner et al., 2019; Schrittwieser et al.,\n2020; Ouyang et al., 2022). TD-MPC2 \u2013 along with several other contemporary RL methods (Yarats\net al., 2021; Ye et al., 2021; Hafner et al., 2023) \u2013 seek to democratize use of RL (i.e., lowering the\nbarrier of entry for smaller teams of academics, practitioners, and individuals with fewer resources)\nby improving robustness of existing open-source algorithms. We firmly believe that improving al-\ngorithmic robustness will continue to have profound impact on the field. A key lesson from the\ndevelopment of TD-MPC2 is that the community has yet to discover an algorithm that truly masters\neverything out-of-the-box. While e.g. DreamerV3 (Hafner et al., 2023) has delivered strong results\non challenging tasks with discrete action spaces (such as Atari games and Minecraft), we find that\nTD-MPC2 produces significantly better results on difficult continuous control tasks. At the same\ntime, extending TD-MPC2 to discrete action spaces remains an open problem.\nOpportunities. Our scaling results demonstrate a path for model-based RL in which massively\nmultitask world models are leveraged as generalist world models. While multi-task world models\n120k environment steps corresponds to 20 episodes in DMControl and 100 episodes in Meta-World.\n8\nremain relatively underexplored in literature, prior work suggests that the implicit world model of\nTD-MPC2 may be better suited than reconstruction-based approaches for tasks with large visual\nvariation (Zhu et al., 2023). We envision a future in which implicit world models are used zero-shot\nto perform diverse tasks on seen embodiments (Xu et al., 2023; Yang et al., 2023), finetuned to\nquickly perform tasks on new embodiments, and combined with existing vision-language models to\nperform higher-level cognitive tasks in conjunction with low-level physical interaction. Our results\nare promising, but such level of generalization will likely require several orders of magnitude more\ntasks than currently available. Lastly, we want to remark that, while TD-MPC2 relies on rewards for\ntask learning, it is useful to adopt a generalized notion of reward as simply a metric for task comple-\ntion. Such metrics already exist in the wild, e.g., success labels, human preferences or interventions\n(Ouyang et al., 2022), or the embedding distance between a current observation and a goal (Eysen-\nbach et al., 2022; Ma et al., 2022) within a pre-existing learned representation. However, leveraging\nsuch rewards for large-scale pretraining is an open problem. To accelerate research in this area, we\nare releasing 300+ TD-MPC2 models, including 12 multitask models, as well as datasets and code,\nand we are beyond excited to see what the community will do with these resources.\nRisks. While we are excited by the potential of generalist world models, several challenges re-\nmain: (i) misspecification of task rewards can lead to unintended outcomes (Clark & Amodei, 2016)\nthat may be difficult to anticipate, (ii) handing over unconstrained autonomy of physical robots to\na learned model can result in catastrophic failures if no additional safety checks are in place (Lan-\ncaster et al., 2023), and (iii) data for certain applications may be prohibitively expensive for small\nteams to obtain at the scale required for generalist behavior to emerge, leading to a concentration of\npower. Mitigating each of these challenges will require new research innovations, and we invite the\ncommunity to join us in these efforts.\n6\nRELATED WORK\nMultiple prior works have sought to build RL algorithms that are robust to hyperparameters, archi-\ntecture, as well as variation in tasks and data. For example, (1) Double Q-learning (Hasselt et al.,\n2016), RED-Q (Chen et al., 2021), SVEA (Hansen et al., 2021), and SR-SPR (D\u2019Oro et al., 2023)\neach improve the stability of Q-learning algorithms by adjusting the bias-variance trade-off in TD-\ntarget estimation, (2) C51 (Bellemare et al., 2017) and DreamerV3 (Hafner et al., 2023) improve\nrobustness to the magnitude of rewards by performing discrete regression in a transformed space,\nand (3) model-free algorithms DrQ (Kostrikov et al., 2020) and DrQ-v2 (Yarats et al., 2021) improve\ntraining stability and exploration, respectively, through use of data augmentation and several other\nminor but important implementation details. However, all of the aforementioned works strictly focus\non improving data-efficiency and robustness in single-task online RL.\nExisting literature that studies scaling of neural architectures for decision-making typically assume\naccess to large datasets of near-expert demonstrations for behavior cloning (Reed et al., 2022; Lee\net al., 2022; Kumar et al., 2022; Schubert et al., 2023; Driess et al., 2023; Brohan et al., 2023). Gato\n(Reed et al., 2022) learns to perform tasks across multiple domains by training a large Transformer-\nbased sequence model (Vaswani et al., 2017) on an enormous dataset of expert demonstrations, and\nRT-1 (Brohan et al., 2023) similarly learns a sequence model for object manipulation on a single\n(real) robot embodiment by training on a large dataset collected by human teleoperation. While the\nempirical results of this line of work are impressive, the assumption of large demonstration datasets\nis impractical. Additionally, current sequence models rely on discretization of the action space\n(tokenization), which makes scaling to high-dimensional continuous control tasks difficult.\nMost recently, researchers have explored scaling of RL algorithms as a solution to the aforemen-\ntioned challenges (Baker et al., 2022; Jia et al., 2022; Xu et al., 2023; Kumar et al., 2023; Hafner\net al., 2023). For example, VPT (Baker et al., 2022) learns to play Minecraft by first pretraining\na behavior cloning policy on a large human play dataset, and then finetuning the policy with RL.\nGSL (Jia et al., 2022) requires no pre-existing data. Instead, GSL iteratively trains a population of\n\u201cspecialist\u201d agents on individual task variations, distills them into a \u201cgeneralist\u201d policy via behavior\ncloning, and then uses the generalist as initialization for the next population of specialists. However,\nthis work considers strictly single-task RL and assumes full control over the initial state in each\nepisode. Lastly, DreamerV3 (Hafner et al., 2023) successfully scales its world model in terms of\nparameters and shows that larger models generally are more data-efficient in an online RL setting,\nbut does not consider multitask RL.\n9\nREFERENCES\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. Advances in Neural\nInformation Processing Systems, 2016.\nBowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. Advances in Neural Information Processing Systems, 35:24639\u201324654,\n2022.\nMarc G Bellemare, Will Dabney, and R\u00b4emi Munos. A distributional perspective on reinforcement\nlearning. In International Conference on Machine Learning, pp. 449\u2013458. PMLR, 2017.\nRichard Bellman. A markovian decision process. Indiana Univ. Math. J., 6:679\u2013684, 1957. ISSN\n0022-2518.\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy\nDennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large\nscale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choroman-\nski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action\nmodels transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in Neural Information Processing Systems, 33:1877\u20131901, 2020.\nVittorio Caggiano, Huawei Wang, Guillaume Durandau, Massimo Sartori, and Vikash Kumar.\nMyosuite \u2013 a contact-rich simulation suite for musculoskeletal motor control.\nhttps:\n//github.com/facebookresearch/myosuite, 2022.\nURL https://sites.\ngoogle.com/view/myosuite.\nBerk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M. Dollar.\nThe ycb object and model set: Towards common benchmarks for manipulation research. In 2015\nInternational Conference on Advanced Robotics, pp. 510\u2013517, 2015. doi: 10.1109/ICAR.2015.\n7251504.\nXinyue Chen, Che Wang, Zijian Zhou, and Keith Ross. Randomized ensembled double q-learning:\nLearning fast without a model. International Conference on Learning Representations, 2021.\nRohan Chitnis, Yingchen Xu, Bobak Hashemi, Lucas Lehnert, Urun Dogan, Zheqing Zhu, and\nOlivier Delalleau. Iql-td-mpc: Implicit q-learning for hierarchical model predictive control. arXiv\npreprint arXiv:2306.00867, 2023.\nJack Clark and Dario Amodei. Faulty reward functions in the wild. OpenAI Blog, 2016.\nDjork-Arn\u00b4e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network\nlearning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.\nPierluca D\u2019Oro, Max Schwarzer, Evgenii Nikishin, Pierre-Luc Bacon, Marc G Bellemare, and\nAaron Courville.\nSample-efficient reinforcement learning by breaking the replay ratio bar-\nrier.\nIn The Eleventh International Conference on Learning Representations, 2023.\nURL\nhttps://openreview.net/forum?id=OpC-9aBBVJe.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multi-\nmodal language model. arXiv preprint arXiv:2303.03378, 2023.\nBenjamin Eysenbach, Tianjun Zhang, Sergey Levine, and Russ R Salakhutdinov. Contrastive learn-\ning as goal-conditioned reinforcement learning. Advances in Neural Information Processing Sys-\ntems, 35:35603\u201335620, 2022.\n10\nYunhai Feng, Nicklas Hansen, Ziyan Xiong, Chandramouli Rajagopalan, and Xiaolong Wang. Fine-\ntuning offline world models in the real world. Conference on Robot Learning, 2023.\nJean-Bastien Grill, Florian Strub, Florent Altch\u2019e, Corentin Tallec, Pierre H. Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo \u00b4Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi\nAzar, Bilal Piot, Koray Kavukcuoglu, R\u00b4emi Munos, and Michal Valko. Bootstrap your own latent:\nA new approach to self-supervised learning. Advances in Neural Information Processing Systems,\n2020.\nJiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiaing Liu, Tongzhou Mu, Yihe Tang, Stone\nTao, Xinyue Wei, Yunchao Yao, Xiaodi Yuan, Pengwei Xie, Zhiao Huang, Rui Chen, and Hao\nSu. Maniskill2: A unified benchmark for generalizable manipulation skills. In International\nConference on Learning Representations, 2023.\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, G. Tucker, Sehoon Ha, Jie Tan, Vikash Ku-\nmar, Henry Zhu, Abhishek Gupta, P. Abbeel, and Sergey Levine. Soft actor-critic algorithms and\napplications. ArXiv, abs/1812.05905, 2018.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\nNicklas Hansen, Hao Su, and Xiaolong Wang. Stabilizing deep q-learning with convnets and vision\ntransformers under data augmentation. In Annual Conference on Neural Information Processing\nSystems, 2021.\nNicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive\ncontrol. In ICML, 2022.\nNicklas Hansen, Yixin Lin, Hao Su, Xiaolong Wang, Vikash Kumar, and Aravind Rajeswaran.\nModem: Accelerating visual model-based reinforcement learning with demonstrations. 2023.\nH. V. Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. In Aaai,\n2016.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00b4ar, and Ross Girshick. Masked au-\ntoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 16000\u201316009, 2022.\nPeter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.\nDeep reinforcement learning that matters. In Proceedings of the Thirty-Second AAAI Confer-\nence on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence\nConference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence,\nAAAI\u201918/IAAI\u201918/EAAI\u201918. AAAI Press, 2018. ISBN 978-1-57735-800-8.\nZhiwei Jia, Xuanlin Li, Zhan Ling, Shuang Liu, Yiran Wu, and Hao Su. Improving Policy Opti-\nmization with Generalist-Specialist Learning. In International Conference on Machine Learning,\n2022.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00b4ar, and Ross Girshick.\nSegment anything. arXiv:2304.02643, 2023.\nIlya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing\ndeep reinforcement learning from pixels. International Conference on Learning Representations,\n2020.\nAviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine. Should i run offline reinforcement\nlearning or behavioral cloning? International Conference on Learning Representations, 2022.\nAviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. Offline q-\nlearning on diverse multi-task data both scales and generalizes.\nInternational Conference on\nLearning Representations, 2023.\n11\nNathan Lambert, Brandon Amos, Omry Yadan, and Roberto Calandra.\nObjective mismatch in\nmodel-based reinforcement learning. Conference on Learning for Decision and Control, 2020.\nPatrick Lancaster, Nicklas Hansen, Aravind Rajeswaran, and Vikash Kumar. Modem-v2: Visuo-\nmotor world models for real-world robot manipulation. arXiv preprint, 2023.\nSamuel Lavoie, Christos Tsirigotis, Max Schwarzer, Ankit Vani, Michael Noukhovitch, Kenji\nKawaguchi, and Aaron Courville. Simplicial embeddings in self-supervised learning and down-\nstream classification. arXiv preprint arXiv:2204.00616, 2022.\nKuang-Huei Lee, Ofir Nachum, Mengjiao Sherry Yang, Lisa Lee, Daniel Freeman, Sergio Guadar-\nrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, et al. Multi-game decision trans-\nformers. Advances in Neural Information Processing Systems, 35:27921\u201327936, 2022.\nT. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and Daan Wierstra. Continuous\ncontrol with deep reinforcement learning. CoRR, abs/1509.02971, 2016.\nYecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy\nZhang. Vip: Towards universal visual reward and representation via value-implicit pre-training.\narXiv preprint arXiv:2210.00030, 2022.\nDiganta Misra. Mish: A self regularized non-monotonic neural activation function. arXiv preprint\narXiv:1908.08681, 2019.\nRudy R. Negenborn, Bart De Schutter, Marco A. Wiering, and Hans Hellendoorn. Learning-based\nmodel predictive control for markov decision processes. IFAC Proceedings Volumes, 38(1):354\u2013\n359, 2005. 16th IFAC World Congress.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al.\nA generalist agent. arXiv preprint arXiv:2205.06175, 2022.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon\nSchmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,\ngo, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609, 2020.\nIngmar Schubert, Jingwei Zhang, Jake Bruce, Sarah Bechtle, Emilio Parisotto, Martin Riedmiller,\nJost Tobias Springenberg, Arunkumar Byravan, Leonard Hasenclever, and Nicolas Heess.\nA\ngeneralist dynamics model for control. arXiv preprint arXiv:2305.10912, 2023.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: A simple way to prevent neural networks from overfitting.\nJournal of Machine\nLearning Research, 15(56):1929\u20131958, 2014.\nURL http://jmlr.org/papers/v15/\nsrivastava14a.html.\nR. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3:9\u201344,\n1998.\nYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-\nden, Abbas Abdolmaleki, et al. Deepmind control suite. Technical report, DeepMind, 2018.\nLaurens van der Maaten and Geoffrey Hinton.\nVisualizing data using t-SNE.\nJournal of Ma-\nchine Learning Research, 9:2579\u20132605, 2008. URL http://www.jmlr.org/papers/v9/\nvandermaaten08a.html.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\n12\nGrady Williams, Andrew Aldrich, and Evangelos A. Theodorou. Model predictive path integral\ncontrol using covariance variable importance sampling. ArXiv, abs/1509.01149, 2015.\nYifan Xu, Nicklas Hansen, Zirui Wang, Yung-Chieh Chan, Hao Su, and Zhuowen Tu.\nOn the\nfeasibility of cross-task transfer with model-based reinforcement learning. 2023.\nSizhe Yang, Yanjie Ze, and Huazhe Xu. Movie: Visual model-based policy adaptation for view\ngeneralization. Advances in Neural Information Processing Systems, 2023.\nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous con-\ntrol: Improved data-augmented reinforcement learning. International Conference on Learning\nRepresentations, 2021.\nWeirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games\nwith limited data. Advances in Neural Information Processing Systems, 34:25476\u201325488, 2021.\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey\nLevine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\nIn Conference on Robot Learning, 2019.\nYifu Yuan, Jianye Hao, Fei Ni, Yao Mu, Yan Zheng, Yujing Hu, Jinyi Liu, Yingfeng Chen, and\nChangjie Fan. Euclid: Towards efficient unsupervised reinforcement learning with multi-choice\ndynamics model. arXiv preprint arXiv:2210.00498, 2022.\nBaohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, Andr\u00b4e Biedenkapp, Kurtland Chua,\nFrank Hutter, and Roberto Calandra.\nOn the importance of hyperparameter optimization for\nmodel-based reinforcement learning. In International Conference on Artificial Intelligence and\nStatistics, pp. 4015\u20134023. PMLR, 2021.\nChuning Zhu, Max Simchowitz, Siri Gadipudi, and Abhishek Gupta. Repo: Resilient model-based\nreinforcement learning by regularizing posterior predictability. arXiv preprint arXiv:2309.00082,\n2023.\nBrian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse\nreinforcement learning. In Proceedings of the 23rd National Conference on Artificial Intelligence,\nvolume 3, 2008.\n13\nAPPENDICES\nA Summary of Improvements\n15\nB\nTask Visualizations\n16\nC Task Domains\n17\nD Single-task Experimental Results\n20\nE\nFew-shot Experimental Results\n23\nF\nAdditional Ablations\n24\nG Gradient Norm and Training Stability\n24\nH Implementation Details\n25\n14\nA\nSUMMARY OF IMPROVEMENTS\nWe summarize the main differences between TD-MPC and TD-MPC2 as follows:\n\u2022 Architectural design. All components of TD-MPC2 are MLPs with LayerNorm (Ba et al.,\n2016) and Mish (Misra, 2019) activations after each layer. We apply SimNorm normaliza-\ntion to the latent state z which biases the representation towards sparsity and maintaining\na small \u21132-norm. We train an ensemble of Q-functions (5 by default) and additionally\napply 1% Dropout (Srivastava et al., 2014) after the first linear layer in each Q-function.\nTD-targets are computed as the mininum of two randomly subsampled Q-functions (Chen\net al., 2021). In contrast, TD-MPC is implemented as MLPs without LayerNorm, and in-\nstead uses ELU (Clevert et al., 2015) activations. TD-MPC does not constrain the latent\nstate at all, which in some instances leads to exploding gradients (see Appendix G for ex-\nperimental results). Lastly, TD-MPC learns only 2 Q-functions and does not use Dropout.\nThe architectural differences in TD-MPC2 result in a 4M net increase in learnable param-\neters (5M total) for our default single-task model size compared to the 1M parameters of\nTD-MPC. However, as shown in Figure 7, na\u00a8\u0131vely increasing the model size of TD-MPC\ndoes not lead to consistently better performance, whereas it does for TD-MPC2.\n\u2022 Policy prior. The policy prior of TD-MPC2 is trained with maximum entropy RL (Ziebart\net al., 2008; Haarnoja et al., 2018), whereas the policy prior of TD-MPC is trained as a\ndeterministic policy with Gaussian noise applied to actions. We find that a carefully tuned\nGaussian noise schedule is comparable to a policy prior trained with maximum entropy.\nHowever, maximum entropy RL can more easily be applied with task-agnostic hyperparam-\neters. We only compute policy entropy over valid action dimensions in multi-task learning\nwith multiple action spaces.\n\u2022 Planning. The planning procedure of TD-MPC2 closely follows that of TD-MPC. How-\never, we simplify planning marginally by not leveraging momentum between iteration, as\nwe find it to produce comparable results. We also improve the throughput of planning by\napprox. 2\u00d7 through a series of code-level optimizations.\n\u2022 Model objective. We revisit the training objective of TD-MPC and improve its robustness\nto variation in tasks, such as the magnitude of rewards. TD-MPC2 uses discrete regression\n(soft cross-entropy) of rewards and values in a log-transformed space (Bellemare et al.,\n2017; Kumar et al., 2023; Hafner et al., 2023), which makes the magnitude of the two\nloss terms independent of the magnitude of the task rewards. TD-MPC uses continuous\nregression which leads to training instabilities in tasks where rewards are large. While this\nissue can be alleviated by, e.g., normalizing task rewards based on moving statistics, in the\nsingle-task case, it is difficult to design robust reward normalization schemes for multi-task\nlearning. TD-MPC2 retains the continuous regression term for joint-embedding prediction\nas the latent representation is already normalized by SimNorm (Lavoie et al., 2022), and\ndiscrete regression is computationally expensive for high-dimensional spaces (requires N\nbins for each dimension of z).\n\u2022 Multi-task model. TD-MPC2 introduces a framework for learning multi-task world mod-\nels across multiple domains, embodiments, and action spaces. We introduce a normalized\nlearnable task embedding space which all components of TD-MPC are conditioned on,\nand we accommodate multiple observation and action spaces by applying zero-padding\nand action masking during both training and inference. We train multi-task models on a\nlarge number of tasks, and finetune the model to held-out tasks (across embodiments) us-\ning online RL. TD-MPC only considers multi-task learning on a small number of tasks\nwith shared observation and action space, and does not consider finetuning of the learned\nmulti-task model.\n\u2022 Simplified algorithm and implementation.\nTD-MPC2 removes momentum in MPPI\n(Williams et al., 2015), and replaces prioritized experience replay sampling from the re-\nplay buffer with uniform sampling, both of which simplify the implementation with no\nsignificant change in experimental results. Finally, we also use a faster replay buffer imple-\nmentation that uses multiple workers for sampling, and we increase training and planning\nthroughput through code-level optimizations such as Q-function ensemble vectorization,\nwhich makes the wall-time of TD-MPC2 comparable to that of TD-MPC despite a larger\narchitecture (5M vs. 1M).\n15\nB\nTASK VISUALIZATIONS\nFigure 10. Task visualizations. Visualization of a random initial state for each of the 104 tasks\nthat we consider. Tasks vary greatly in objective, embodiment, and action space. Visit https://\nnicklashansen.github.io/td-mpc2 for videos of TD-MPC2 performing each task. See\nAppendix C for task details.\n16\nC\nTASK DOMAINS\nWe consider a total of 104 continuous control tasks from 4 task domains: DMControl (Tassa et al.,\n2018), Meta-World (Yu et al., 2019), ManiSkill2 (Gu et al., 2023), and MyoSuite (Caggiano et al.,\n2022). This section provides an exhaustive list of all tasks considered, as well as their observation\nand action dimensions. Environment details are listed at the end of the section. We provide (static)\ntask visualizations in Appendix B and videos of TD-MPC2 agents performing each task at https:\n//nicklashansen.github.io/td-mpc2.\nTable 2. DMControl. We consider a total of 39 continuous control tasks in the DMControl domain,\nincluding 19 original DMControl tasks and 11 new (custom) tasks created specifically for TD-MPC2\nbenchmarking and multitask training. We list all considered DMControl tasks below. The Locomo-\ntion task set shown in Figure 1 corresponds to the Humanoid and Dog embodiments of DMControl,\nwith performance reported at 14M environment steps.\nTask\nObservation dim\nAction dim\nSparse?\nNew?\nAcrobot Swingup\n6\n1\nN\nN\nCartpole Balance\n5\n1\nN\nN\nCartpole Balance Sparse\n5\n1\nY\nN\nCartpole Swingup\n5\n1\nN\nN\nCartpole Swingup Sparse\n5\n1\nY\nN\nCheetah Jump\n17\n6\nN\nY\nCheetah Run\n17\n6\nN\nN\nCheetah Run Back\n17\n6\nN\nY\nCheetah Run Backwards\n17\n6\nN\nY\nCheetah Run Front\n17\n6\nN\nY\nCup Catch\n8\n2\nY\nN\nCup Spin\n8\n2\nN\nY\nDog Run\n223\n38\nN\nN\nDog Trot\n223\n38\nN\nN\nDog Stand\n223\n38\nN\nN\nDog Walk\n223\n38\nN\nN\nFinger Spin\n9\n2\nY\nN\nFinger Turn Easy\n12\n2\nY\nN\nFinger Turn Hard\n12\n2\nY\nN\nFish Swim\n24\n5\nN\nN\nHopper Hop\n15\n4\nN\nN\nHopper Hop Backwards\n15\n4\nN\nY\nHopper Stand\n15\n4\nN\nN\nHumanoid Run\n67\n24\nN\nN\nHumanoid Stand\n67\n24\nN\nN\nHumanoid Walk\n67\n24\nN\nN\nPendulum Spin\n3\n1\nN\nY\nPendulum Swingup\n3\n1\nN\nN\nQuadruped Run\n78\n12\nN\nN\nQuadruped Walk\n78\n12\nN\nN\nReacher Easy\n6\n2\nY\nN\nReacher Hard\n6\n2\nY\nN\nReacher Three Easy\n8\n3\nY\nY\nReacher Three Hard\n8\n3\nY\nY\nWalker Run\n24\n6\nN\nN\nWalker Run Backwards\n24\n6\nN\nY\nWalker Stand\n24\n6\nN\nN\nWalker Walk\n24\n6\nN\nN\nWalker Walk Backwards\n24\n6\nN\nY\n17\nTable 3. Meta-World. We consider a total of 50 continuous control tasks from the Meta-World\ndomain. The Meta-World benchmark is designed for multitask and meta-learning research and all\ntasks thus share embodiment, observation space, and action space.\nTask\nObservation dim\nAction dim\nAssembly\n39\n4\nBasketball\n39\n4\nBin Picking\n39\n4\n...\n...\n...\nWindow Open\n39\n4\nTable 4. ManiSkill2. We consider a total of 5 continuous control tasks from the ManiSkill2 domain.\nThe ManiSkill2 benchmark is designed for large-scale robot learning and contains a high degree\nof randomization and task variations. The Pick YCB task shown in Figure 1 corresponds to the\nManiSkill2 task of the same name, with performance reported at 14M environment steps.\nTask\nObservation dim\nAction dim\nLift Cube\n42\n4\nPick Cube\n51\n4\nPick YCB\n51\n7\nStack Cube\n55\n4\nTurn Faucet\n40\n7\nTable 5. MyoSuite. We consider a total of 10 continuous control tasks from the MyoSuite domain.\nThe MyoSuite benchmark is designed for high-dimensional physiologically accurate muscoloskele-\ntal motor control and involves particularly complex object manipulation with a dexterous hand. The\nMyoSuite domain consists of tasks with and without goal randomization. We consider both settings,\nand refer to them as Easy (fixed goal) and Hard (random goal), respectively.\nTask\nObservation dim\nAction dim\nReach Easy\n115\n39\nReach Hard\n115\n39\nPose Easy\n108\n39\nPose Hard\n108\n39\nPen Twirl Easy\n83\n39\nPen Twirl Hard\n83\n39\nObject Hold Easy\n91\n39\nObject Hold Hard\n91\n39\nKey Turn Easy\n93\n39\nKey Turn Hard\n93\n39\n18\nEnvironment details. We benchmark algorithms on DMControl, Meta-World, ManiSkill2, and\nMyoSuite without modification. All four domains are infinite-horizon continuous control environ-\nments for which we use a fixed episode length and no termination conditions. We list episode\nlengths, action repeats, total number of environment steps, and the performance metric used for\neach domain in Table 6. In all experiments, we only consider an episode successful if the final step\nof an episode is successful. This is a stricter definition of success than used in some of the related\nliterature, which e.g. may consider an episode successful if success is achieved at any step within\na given episode. In tasks that require manipulation of objects, such as picking up an object, our\ndefinition of success ensures that an episode in which an object is picked up but then dropped again\nis not considered successful.\nTable 6. Environment details. We list the episode length and action repeat used for each task\ndomain, as well as the total number of environment steps and performance metrics that we use for\nbenchmarking methods. All methods use the same values for all tasks.\nDMControl\nMeta-World\nManiSkill2\nMyoSuite\nEpisode length\n1, 000\n200\n200\n100\nAction repeat\n2\n2\n2\n1\nEffective length\n500\n100\n100\n100\nTotal env. steps\n4M - 14M\n2M\n4M - 14M\n2M\nPerformance metric\nReward\nSuccess\nSuccess\nSuccess\n\u2014 Appendices continue on next page \u2014\n19\nD\nSINGLE-TASK EXPERIMENTAL RESULTS\n0\n500\n1000 Acrobot Swingup\nCartp Balance\nCartp Balance Sparse\nCartp Swingup\nCartp Swingup Sparse\n0\n500\n1000\nCheetah Jump\nCheetah Run\nCheetah Run Back\nCheetah Run Bkwd\nCheetah Run Front\n0\n500\n1000\nCup Catch\nCup Spin\nDog Run\nDog Stand\nDog Trot\n0\n500\n1000\nDog Walk\nFinger Spin\nFinger Turn Easy\nFinger Turn Hard\nFish Swim\n0\n500\n1000\nHopper Hop\nHopper Hop Bkwd\nHopper Stand\nHumanoid Run\nHumanoid Stand\n0\n500\n1000\nHumanoid Walk\nPendulum Spin\nPendulum Swingup\nQuadruped Run\nQuadruped Walk\n0\n500\n1000\nReacher Easy\nReacher Hard\nReacher 3 Easy\nReacher 3 Hard\nWalker Run\n0\n1M\n2M\n3M\n4M\n0\n500\n1000 Walker Run Bkwd\n0\n1M\n2M\n3M\n4M\nWalker Stand\n0\n1M\n2M\n3M\n4M\nWalker Walk\n0\n1M\n2M\n3M\n4M\nWalker Walk Bkwd\nSAC\nDreamerV3\nTD-MPC\nTD-MPC2\nFigure 11. Single-task DMControl results. Episode return as a function of environment steps.\nThe first 4M environment steps are shown for each task, although the Humanoid and Dog tasks are\nrun for 14M environment steps; we provide those curves in Figure 14 as part of the \u201cLocomotion\u201d\nbenchmark. Note that TD-MPC diverges on tasks like Walker Stand and Walker Walk whereas TD-\nMPC2 remains stable. We visualize gradients on these tasks in Appendix G. Mean and 95% CIs\nover 3 seeds.\n20\n0\n50\n100\nAssembly\nBasketball\nBin Picking\nBox Close\nButton Press\n0\n50\n100\nButton Press Td\nButton Press Td Wall\nButton Press Wall\nCoffee Button\nCoffee Pull\n0\n50\n100\nCoffee Push\nDial Turn\nDisassemble\nDoor Close\nDoor Lock\n0\n50\n100\nDoor Open\nDoor Unlock\nDrawer Close\nDrawer Open\nFaucet Close\n0\n50\n100\nFaucet Open\nHammer\nHand Insert\nHandle Press\nHandle Press Side\n0\n50\n100\nHandle Pull\nHandle Pull Side\nLever Pull\nPeg Insert Side\nPeg Unplug Side\n0\n50\n100\nPick Out Of Hole\nPick Place\nPick Place Wall\nPlate Slide\nPlate Slide Back\n0\n50\n100Plate Slide Back Side\nPlate Slide Side\nPush\nPush Back\nPush Wall\n0\n50\n100\nReach\nReach Wall\nShelf Place\nSoccer\nStick Pull\n0\n1M\n2M\n0\n50\n100\nStick Push\n0\n1M\n2M\nSweep\n0\n1M\n2M\nSweep Into\n0\n1M\n2M\nWindow Close\n0\n1M\n2M\nWindow Open\nSAC\nDreamerV3\nTD-MPC\nTD-MPC2\nFigure 12. Single-task Meta-World results. Success rate (%) as a function of environment steps.\nTD-MPC2 performance is comparable to existing methods on easy tasks, while outperforming other\nmethods on hard tasks such as Pick Place Wall and Shelf Place. DreamerV3 often fails to converge.\n21\n0\n1M\n2M\n3M\n4M\n0\n50\n100\nLift Cube\n0\n1M\n2M\n3M\n4M\nPick Cube\n0\n4M\n8M\n12M\nPick YCB\n0\n1M\n2M\n3M\n4M\nStack Cube\n0\n1M\n2M\n3M\n4M\nTurn Faucet\nSAC\nDreamerV3\nTD-MPC\nTD-MPC2\nFigure 13. Single-task ManiSkill2 results. Success rate (%) as a function of environment steps on 5\nobject manipulation tasks from ManiSkill2. Pick YCB is the hardest task and considers manipulation\nof all 74 objects from the YCB (Calli et al., 2015) dataset. We report results for this tasks at 14M\nenvironment steps, and 4M environment steps for other tasks. TD-MPC2 achieves a > 60% success\nrate on the Pick YCB task, whereas other methods fail to learn within the given budget. Mean and\n95% CIs over 3 seeds.\n0\n500\n1000\nDog Run\nDog Stand\nDog Trot\nDog Walk\nHumanoid Run\n0\n4M\n8M\n12M\n0\n500\n1000 Humanoid Stand\n0\n4M\n8M\n12M\nHumanoid Walk\nSAC\nDreamerV3\nTD-MPC\nTD-MPC2\nFigure 14. Single-task high-dimensional locomotion results. Episode return as a function of\nenvironment steps on all 7 \u201cLocomotion\u201d benchmark tasks. This domain includes high-dimensional\nHumanoid (A \u2208 R21) and Dog (A \u2208 R38) embodiments. Mean and 95% CIs over 3 seeds.\n0\n50\n100\nKey Turn\nKey Turn Hard\nObj Hold\nObj Hold Hard\nPen Twirl\n0\n1M\n2M\n0\n50\n100\nPen Twirl Hard\n0\n1M\n2M\nPose\n0\n1M\n2M\nPose Hard\n0\n1M\n2M\nReach\n0\n1M\n2M\nReach Hard\nSAC\nDreamerV3\nTD-MPC\nTD-MPC2\nFigure 15. Single-task MyoSuite results. Success rate (%) as a function of environment steps.\nThis task domain includes high-dimensional contact-rich musculoskeletal motor control (A \u2208 R39)\nwith a physiologically accurate robot hand. Goals are randomized in tasks designated as \u201cHard\u201d.\nTD-MPC2 achieves comparable or better performance than existing methods on all tasks from this\nbenchmark, except for Key Turn Hard in which TD-MPC succeeds early in training.\n22\nE\nFEW-SHOT EXPERIMENTAL RESULTS\nWe finetune a 19M parameter TD-MPC2 agent trained on 70 tasks to each of 10 held-out tasks.\nIndividual task curves are shown in Figure 16. We compare data-efficiency of the finetuned model to\na baseline agent of similar model capacity trained from scratch. However, we find that performance\nof our 19M parameter baselines trained from scratch are comparable to our 5M parameter agents\nalso trained from scratch. Our few-shot finetuning results suggest that the efficacy of finetuning is\nsomewhat task-dependent. However, more research is needed to conclude whether this is due to task\nsimilarity (or rather lack thereof) or due to subpar task performance of the pretrained agent on the\nsource task. We conjecture that both likely influence results.\nWhen finetuning to an unseen task, we initialize the learnable task embedding for the new task as\nthe embedding of a semantically similar task from the pretraining dataset. We list the source task\nembedding used as initialization for each experiment in Table 7. We did not experiment with other\ninitialization schemes, nor other task pairings.\n0\n50\n100\nCheetah Run\nHopper Hop\nBin Picking\nBox Close\nDoor Lock\n0\n20k\n40k\n0\n50\n100\nDoor Unlock\n0\n20k\n40k\nHand Insert\n0\n20k\n40k\nPendulum Swingup\n0\n20k\n40k\nReacher Hard\n0\n20k\n40k\nWalker Run\nFrom scratch\nFinetuned\nFigure 16. Few-shot learning. Normalized episode return (DMControl) and success rate (Meta-\nWorld) as a function of environment steps while finetuning a 19M parameter TD-MPC2 agent\ntrained on 70 tasks to each of 10 held-out tasks. 40k steps corresponds to 40 episodes in DM-\nControl and 200 in Meta-World. Mean and 95% CIs over 3 seeds.\nTable 7. Initialization of task embeddings for few-shot learning. We list the task embeddings\nused as initialization when finetuning our 19M parameter TD-MPC2 agent to held-out tasks. We did\nnot experiment with other initialization schemes, nor other task pairings.\nTarget task\nSource task\nWalker Run\nWalker Walk\nCheetah Run\nCheetah Run Backwards\nHopper Hop\nHopper Stand\nPendulum Swingup\nPendulum Spin\nReacher Hard\nReacher Easy\nBin Picking\nPick Place\nBox Close\nAssembly\nDoor Lock\nDoor Open\nDoor Unlock\nDoor Open\nHand Insert\nSweep Into\n23\nF\nADDITIONAL ABLATIONS\n30\n40\n50\n60\nNormalized score\n46.6\n54.2\nTask embedding\nMultitask (80)\nUnnormalized\nNormalized\nFigure 17. Normalized task embeddings. Normalized score of 19M parameter multitask (80 tasks)\nTD-MPC2 agents, with and without normalized task embeddings e as described in Section 3.1. We\nfind that normalizing e to have a maximum \u21132-norm of 1 improves multitask performance.\nWalker Walk\nWalker Run\nCheetah Run\nCartpole Balance\nCartpole Swingup\nCup Catch\nFinger Spin\nFinger Turn Easy\nFinger Turn Hard\nHopper Stand\nHopper Hop\nCheetah Jump\nCup Spin\nDial Turn\nDoor Open\nDoor Close\nPick Place\nPick Place Wall\nReach\nReach Wall\nStick Push\nStick Pull\nWalker Walk\nWalker Run\nCheetah Run\nCartpole Balance\nCartpole Swingup\nCup Catch\nFinger Spin\nFinger Turn Easy\nFinger Turn Hard\nHopper Stand\nHopper Hop\nCheetah Jump\nCup Spin\nDial Turn\nDoor Open\nDoor Close\nPick Place\nReach\nReach Wall\nStick Push\nStick Pull\nFigure 18. T-SNE of task embeddings with and without normalization. T-SNE (van der Maaten\n& Hinton, 2008) visualizations of task embeddings learned by TD-MPC2 agent trained on 80 tasks\nfrom DMControl and Meta-World. (Left) with normalization. (Right) without normalization. A\nsubset of labels are shown for clarity. We observe that task embeddings are more semantically\nmeaningful when normalized during training, e.g., \u201cDoor Open\u201d and \u201cDoor Close\u201d are close in\nembedding space on the left, but far apart on the right.\nG\nGRADIENT NORM AND TRAINING STABILITY\n0\n4M\n8M\n12M\n10\u22126\n10\u22123\n1\n103\n106\n109\n1012\nDog Trot\n0\n1M\n2M\n3M\n4M\nHopper Hop\n0\n1M\n2M\n3M\n4M\nQuadruped Run\n0\n1M\n2M\n3M\n4M\nReacher Hard\n0\n1M\n2M\n3M\n4M\nWalker Stand\nTD-MPC\nTD-MPC2\nFigure 19. Gradient norm during training. We compare the gradient norm (log-scale) of TD-MPC\nand TD-MPC2 as a function of environment steps on five tasks from DMControl. TD-MPC is prone\nto exploding gradients, which can cause learning to diverge on some tasks (e.g., Walker Stand in\nFigure 11). In comparison, the gradients of TD-MPC2 remain stable throughout training. We only\ndisplay 1 seed per task for visual clarity.\n24\nH\nIMPLEMENTATION DETAILS\nArchitectural details. All components of TD-MPC2 are implemented as MLPs. The encoder h\ncontains a variable number of layers (2\u22125) depending on the architecture size; all other components\nare 3-layer MLPs. Intermediate layers consist of a linear layer followed by LayerNorm and a Mish\nactivation function. The latent representation is normalized as a simplicial embedding. Q-functions\nadditionally use Dropout. We summarize the TD-MPC2 architecture for the 5M parameter base\n(default for online RL) model size using PyTorch-like notation:\nEncoder parameters: 167,936\nDynamics parameters: 843,264\nReward parameters: 631,397\nPolicy parameters: 582,668\nQ parameters: 3,156,985\nTask parameters: 7,680\nTotal parameters: 5,389,930\nArchitecture: TD-MPC2 base 5M(\n(task_embedding): Embedding(T, 96, max_norm=1)\n(encoder): ModuleDict(\n(state): Sequential(\n(0): NormedLinear(in_features=S+T, out_features=256, act=Mish)\n(1): NormedLinear(in_features=256, out_features=512, act=SimNorm)\n)\n)\n(dynamics): Sequential(\n(0): NormedLinear(in_features=512+T+A, out_features=512, act=Mish)\n(1): NormedLinear(in_features=512, out_features=512, act=Mish)\n(2): NormedLinear(in_features=512, out_features=512, act=SimNorm)\n)\n(reward): Sequential(\n(0): NormedLinear(in_features=512+T+A, out_features=512, act=Mish)\n(1): NormedLinear(in_features=512, out_features=512, act=Mish)\n(2): Linear(in_features=512, out_features=101,)\n)\n(pi): Sequential(\n(0): NormedLinear(in_features=512+T, out_features=512, act=Mish)\n(1): NormedLinear(in_features=512, out_features=512, act=Mish)\n(2): Linear(in_features=512, out_features=2A, bias=True)\n)\n(Qs): Vectorized ModuleList(\n(0-4): 5 x Sequential(\n(0): NormedLinear(in_features=512+T+A, out_features=512, dropout=0.01, act=Mish)\n(1): NormedLinear(in_features=512, out_features=512, act=Mish)\n(2): Linear(in_features=512, out_features=101, bias=True)\n)\n)\nwhere S is the input dimensionality, T is the number of tasks, and A is the action space. We exclude\nthe task embedding T from single-task experiments. The exact parameter counts listed above are for\nS= 39, T= 80, and A= 6.\nHyperparameters. We use the same hyperparameters across all tasks. Our hyperparameters are\nlisted in Table 8. We use the same hyperparameters for TD-MPC and SAC as in Hansen et al.\n(2022). DreamerV3 (Hafner et al., 2023) uses a fixed set of hyperparameters.\nWe set the discount factor \u03b3 for a task using the heuristic\n\u03b3 = clip(\nT\n5 \u2212 1\nT\n5\n, [0.95, 0.995])\n(7)\nwhere T is the expected length of an episode after applying action repeat, and clip constrains the\ndiscount factor to the interval [0.95, 0.995]. Using this heuristic, we obtain \u03b3 = 0.99 for DMControl\n(T = 500), which is the most widely used discount factor for this task domain. Tasks with shorter\nepisodes are assigned a lower discount factor, whereas tasks with longer episodes are assigned a\nhigher discount factor. All of the tasks that we consider are infinite-horizon MDPs with fixed episode\nlengths. We use individual discount factors (set using the above heuristic) for each task in our\nmultitask experiments. For tasks with variable or unknown episode lengths, we suggest using an\nempirical mean length, a qualified guess, or simply \u03b3 = 0.99. While this heuristic is introduced in\nTD-MPC2, we apply the same discount factor for the TD-MPC and SAC baselines to ensure that\ncomparison is fair across all task domains.\n25\nTable 8. TD-MPC2 hyperparameters. We use the same hyperparameters across all tasks. Certain\nhyperparameters are set automatically using heuristics.\nHyperparameter\nValue\nPlanning\nHorizon (H)\n3\nIterations\n6 (+2 if \u2225A\u2225 \u2265 20)\nPopulation size\n512\nPolicy prior samples\n24\nNumber of elites\n64\nMinimum std.\n0.05\nMaximum std.\n2\nTemperature\n0.5\nMomentum\nNo\nPolicy prior\nLog std. min.\n\u221210\nLog std. max.\n2\nReplay buffer\nCapacity\n1, 000, 000\nSampling\nUniform\nArchitecture (5M)\nEncoder dim\n256\nMLP dim\n512\nLatent state dim\n512\nTask embedding dim\n96\nTask embedding norm\n1\nActivation\nLayerNorm + Mish\nQ-function dropout rate\n1%\nNumber of Q-functions\n5\nNumber of reward/value bins\n101\nSimNorm dim (V )\n8\nSimNorm temperature (\u03c4)\n1\nOptimization\nUpdate-to-data ratio\n1\nBatch size\n256\nJoint-embedding coef.\n10\nReward prediction coef.\n0.5\nValue prediction coef.\n1\nQ-fn. momentum coef.\n0.99\nPolicy prior entropy coef.\n1 \u00d7 10\u22124\nPolicy prior loss norm.\nMoving (5%, 95%) percentiles\nOptimizer\nAdam\nLearning rate\n3 \u00d7 10\u22124\nEncoder learning rate\n1 \u00d7 10\u22124\nDiscount factor\nHeuristic\nSeed steps\nHeuristic\nWe set the seed steps S (number of environment steps before any gradient updates) for a task using\nthe heuristic\nS = max(5T, 1000)\n(8)\nwhere T again is the expected episode length of the task after applying action repeat. We did not\nexperiment with other heuristics nor constant values, but conjecture that Equation 8 will ensure that\n26\nthe replay buffer B has sufficient data for model learning regardless of episode lengths.\nModel configurations. Our multitask experiments consider TD-MPC2 agents with model sizes\nranging from 1M parameters to 317M parameters. Table 9 lists the exact specifications for each of\nour model sizes. We scale the model size by varying dimensions of fully-connected layers, the latent\nstate dimension z, the number of encoder layers, and the number of Q-functions. We make no other\nmodifications to the architecture nor hyperparameters across model sizes.\nTable 9. Model configurations. We list the specifications for each model configuration (size)\nof our multitask experiments. Encoder dim is the dimensionality of fully connected layers in the\nencoder h, MLP dim is the dimensionality of layers in all other components, Latent state dim is the\ndimensionality of the latent representation z, # encoder layers is the number of layers in the encoder\nh, # Q-functions is the number of learned Q-functions, and Task embedding dim is the dimensionality\nof e from Equation 2. TD-targets are always computed by randomly subsampling two Q-functions,\nregardless of the number of Q-functions in the ensemble. We did not experiment with other model\nconfigurations. *The default (base) configuration used in our single-task RL experiments has 5M\nparameters.\n1M\n5M*\n19M\n48M\n317M\nEncoder dim\n256\n256\n1024\n1792\n4096\nMLP dim\n384\n512\n1024\n1792\n4096\nLatent state dim\n128\n512\n768\n768\n1376\n# encoder layers\n2\n2\n3\n4\n5\n# Q-functions\n2\n5\n5\n5\n8\nTask embedding dim\n96\n96\n96\n96\n96\nSimplicial Normalization (SimNorm). We implement the SimNorm normalization layer (Lavoie\net al., 2022) using PyTorch-like notation as follows:\ndef simnorm(self, z, V=8):\nshape = z.shape\nz = z.view(*shape[:-1], -1, V)\nz = softmax(z, dim=-1)\nreturn z.view(*shape)\nHere, z is the latent representation z, and V is the dimensionality of each simplex. The number of\nsimplices L can be inferred from V and the dimensionality of z. We apply a softmax (optionally\nmodulated by a temperature \u03c4) to each of L partitions of z to form simplices, and then reshape to\nthe original shape of z.\nTD-MPC baseline implementation. We benchmark against the official implementation of TD-\nMPC available at https://github.com/nicklashansen/tdmpc. The default TD-MPC\nworld model has approx. 1M trainable parameters, and uses per-task hyperparameters. We use the\nsuggested hyperparameters where available (DMControl and Meta-World). For example, TD-MPC\nrequires tuning of the number of planning iterations, latent state dimensionality, batch size, and\nlearning rate in order to solve the challenging Dog and Humanoid tasks. Refer to their paper for a\ncomplete list of hyperparameters.\nDreamerV3 baseline implementation. We benchmark against the official reimplementation of\nDreamerV3 available at https://github.com/danijar/dreamerv3. We follow the au-\nthors\u2019 suggested hyperparameters for proprioceptive control (DMControl) and use the S model size\n(20M parameters), as well as an update-to-data (UTD) ratio of 512. We use this model size and\nUTD for all tasks. Refer to their paper for a complete list of hyperparameters.\nSAC baseline implementation.\nWe follow the TD-MPC (Hansen et al., 2022) paper in\ntheir decision to benchmark against the SAC implementation from https://github.com/\ndenisyarats/pytorch_sac, and we use the hyperparameters suggested by the authors (when\navailable). For example, this includes tuning the latent dimension, learning rate, and batch size for\nthe Dog and Humanoid tasks. Refer to their paper for a complete list of hyperparameters.\n27\n"
  },
  {
    "title": "LightSpeed: Light and Fast Neural Light Fields on Mobile Devices",
    "link": "https://arxiv.org/pdf/2310.16832.pdf",
    "upvote": "4",
    "text": "LightSpeed: Light and Fast\nNeural Light Fields on Mobile Devices\nAarush Gupta1\nJunli Cao2,\u2020\nChaoyang Wang2,\u2020\nJu Hu2\nSergey Tulyakov2\nJian Ren2\nL\u00e1szl\u00f3 A Jeni1\n1Robotics Institute, Carnegie Mellon University\n2Snap Inc.\nProject page: https://lightspeed-r2l.github.io\nAbstract\nReal-time novel-view image synthesis on mobile devices is prohibitive due to the\nlimited computational power and storage. Using volumetric rendering methods,\nsuch as NeRF and its derivatives, on mobile devices is not suitable due to the high\ncomputational cost of volumetric rendering. On the other hand, recent advances in\nneural light field representations have shown promising real-time view synthesis\nresults on mobile devices. Neural light field methods learn a direct mapping from a\nray representation to the pixel color. The current choice of ray representation is\neither stratified ray sampling or Pl\u00fccker coordinates, overlooking the classic light\nslab (two-plane) representation, the preferred representation to interpolate between\nlight field views. In this work, we find that using the light slab representation is an\nefficient representation for learning a neural light field. More importantly, it is a\nlower-dimensional ray representation enabling us to learn the 4D ray space using\nfeature grids which are significantly faster to train and render. Although mostly\ndesigned for frontal views, we show that the light-slab representation can be further\nextended to non-frontal scenes using a divide-and-conquer strategy. Our method\noffers superior rendering quality compared to previous light field methods and\nachieves a significantly improved trade-off between rendering quality and speed.\n1\nIntroduction\nReal-time rendering of photo-realistic 3D content on mobile devices such as phones is crucial for\nmixed-reality applications. However, this presents a challenge due to the limited computational power\nand memory of mobile devices. The current graphics pipeline requires storing tens of thousands of\nmeshes for complex scenes and performing ray tracing for realistic lighting effects, which demands\npowerful graphics processing power that is not feasible on current mobile devices. Recently, neural\nradiance field (NeRF [23]) has been the next popular choice for photo-realistic view synthesis, which\noffers a simplified rendering pipeline. However, the computational cost of integrating the radiance\nfield remains a bottleneck for real-time implementation on mobile devices. There have been several\nattempts to reduce the computational cost of this integration step, such as using more efficient radiance\nrepresentations [13, 40, 28, 17, 5, 10] or distilling meshes from radiance field [34, 6, 39, 35, 27, 29].\nAmong these approaches, only a handful of mesh-based methods [6, 29] have demonstrated real-\ntime rendering capabilities on mobile phones, but with a significant sacrifice in rendering fidelity.\nMoreover, all aforementioned methods require significant storage space (over 200MB), which is\nundesirable for mobile devices with limited onboard storage.\n\u2020These authors contributed equally.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2310.16832v2  [cs.CV]  26 Oct 2023\n7.5\n10.0\n12.5\n15.0\n17.5\nRendering latency (in ms)\n29.0\n29.5\n30.0\n30.5\n31.0\nRendering Quality (PSNR)\n15-L\n15-L\n30-L\n30-L\n60-L\n60-L\nLightSpeed (Ours)\nMobileR2L\nMobileNeRF\n4x\n(a) Rendering latency v/s fidelity.\n(b) Faster training speed.\nFigure 1: Our LightSpeed approach demonstrates a superior trade-off between on-device rendering\nquality and latency while maintaining a significantly reduced training time and boosted rendering\nquality. (a) rendering quality and latency on the 400 \u00d7 400 Lego scene [23] running on an iPhone 13.\n(b) training curves for the 756 \u00d7 1008 Fern scene [22].\nAlternatively, researchers have used 4D light field1 (or lumigraph) to represent radiance along rays\nin empty space [11, 24, 12, 19], rather than attempting to model the 5D plenoptic function as in\nNeRF-based approaches. Essentially, the light field provides a direct mapping from rays to pixel\nvalues since the radiance is constant along rays in empty space. This makes the light field suitable for\nview synthesis, as long as the cameras are placed outside the convex hull of the object of interest.\nCompared to integrating radiance fields, rendering with light fields is more computationally efficient.\nHowever, designing a representation of light field that compresses its storage while maintaining\nhigh view-interpolation fidelity remains challenging. Previous methods, such as image quilts [38]\nor multiplane images (MPI) [41, 16, 32, 9], suffer from poor trade-offs between fidelity and storage\ndue to the high number of views or image planes required for reconstructing the complex light field\nsignal. Recent works [36, 4, 2, 31] have proposed training neural networks to represent light fields,\nachieving realistic rendering with a relatively small memory footprint. Among those, MobileR2L [4]\nuses less than 10MB of storage per scene, and it is currently the only method that demonstrates\nreal-time performance on mobile phones.\nHowever, prior neural light field (NeLF) representations, including MobileR2L, suffer from\ninefficiencies in learning due to the high number of layers (over 60 layers), and consequently,\na long training time is required to capture fine scene details. One promising strategy to address\nthis issue is utilizing grid-based representations, which have proven to be effective in the context of\ntraining NeRFs [30, 25, 17, 10]. Nonetheless, incorporating such grid-based representation directly\nto prior NeLFs is problematic due to the chosen ray parameterization. R2L [36] and MobileR2L [4]\nparameterize light rays using a large number of stratified 3D points along the rays, which were\ninitially motivated by the discrete formulation of integrating radiance. However, this motivation is\nunnecessary and undermines the simplicity of 4D light fields because stratified sampling is redundant\nfor rays with constant radiance. This becomes problematic when attempting to incorporate grid-based\nrepresentations for more efficient learning, as the high-dimensional stratified-point representation is\nnot feasible for grid-based discretization. Similarly, the 6-dimensional Pl\u00fccker coordinate used by\nSitzmann et al. [31] also presents issues for discretization due to the fact that Pl\u00fccker coordinates\nexist in a projective 5-space, rather than Euclidean space.\nIn this paper, we present LightSpeed, the first NeLF method designed for mobile devices that uses a\ngrid-based representation. As shown in Fig. 1, our method achieves a significantly better trade-off\nbetween rendering quality and speed compared to prior NeLF methods, while also being faster to\ntrain. These advantages make it well-suited for real-time applications on mobile devices. To achieve\nthese results, we propose the following design choices:\nFirst, we revisit the classic 4D light-slab (or two-plane) representation [12, 19] that has been largely\noverlooked by previous NeLF methods. This lower-dimensional parameterization allows us to\ncompactly represent the rays and efficiently represent the light field using grids. To our knowledge,\n1For the rest of the paper, we will use the term \u2018light field\u2019 to refer to the 4D light field, without explicitly\nstating the dimensionality.\n2\nAttal et al. [2] is the only other NeLF method that has experimented with the light-slab representation.\nHowever, they did not take advantage of the grid-based representation, and their method is not\ndesigned for real-time rendering. Second, to address the heavy storage consumption of 4D light field\ngrids, we take inspiration from k-planes [10] and propose decomposing the 4D grids into six 2D\nfeature grids. This ensures that our method remains competitive for storage consumption compared\nto prior NeLF methods. Third, we apply the super-resolution network proposed by MobileR2L [4],\nwhich significantly reduces the computational cost when rendering high-resolution images. Finally,\nthe light-slab representation was originally designed for frontal-view scenes, but we demonstrate that\nit can be extended to represent non-frontal scenes using a divide-and-conquer strategy.\nOur contributions pave the way for efficient and scalable light field representation and synthesis,\nmaking it feasible to generate high-quality images of real-world objects and scenes. Our method\nachieves the highest PSNR and among the highest frame rates (55 FPS on iPhone 14) on LLFF\n(frontal-view), Blender (360\u25e6), and unbounded 360\u25e6 scenes, proving the effectiveness of our approach.\n2\nRelated work\nLight Field. Light field representations have been studied extensively in the computer graphics and\ncomputer vision communities [38]. Traditionally, light fields have been represented using the 4D\nlight slab representation, which parameterizes the light field by two planes in 4D space [12, 19].\nMore recently, neural-based approaches have been developed to synthesize novel views from the light\nfield, leading to new light field representations being proposed.\nOne popular representation is the multi-plane image (MPI) representation, which discretizes the light\nfield into a set of 2D planes. The MPI representation has been used in several recent works, including\n[41, 16, 32, 9, 7]. However, the MPI representation can require a large amount of memory, especially\nfor high-resolution light fields. Another recent approach that has gained substantial attention is\nNeRF [23] (Neural Radiance Fields), which can synthesize novel views with high accuracy, but is\ncomputationally expensive to render and train due to the need to integrate radiance along viewing\nrays. There has been a substantial amount of works [37, 26, 28, 21, 13, 40, 28, 17, 5, 10, 34, 6, 39, 35,\n27, 29, 36, 4, 2, 31] studying how to accelerate training and rendering of NeRF, but in the following,\nwe focus on recent methods that achieve real-time rendering with or without mobile devices.\nGrid Representation of Radiance Field. The first group of methods trade speed with space, by\nprecomputing and caching radiance values using grid or voxel-like data structures such as sparse\nvoxels [30, 13], octrees [40], and hash tables [25]. Despite the efficient data structures, the memory\nconsumption for these methods is still high, and several approaches have been proposed to address\nthis issue. First, Chen et al. [5] and Fridovich-Keil et al. [10] decompose voxels into matrices that\nare cheaper to store. Takikawa et al. [33] performs quantization to compress feature grids. These\napproaches have enabled real-time applications on desktop or server-class GPUs, but they still require\nsignificant computational resources and are not suitable for resource-constrained devices such as\nmobile or edge devices.\nBaking High Resolution Mesh. Another group of methods adopts the approach of extracting\nhigh-resolution meshes from the learned radiance field [6, 29, 35]. The texture of the mesh stores\nthe plenoptic function to account for view-dependent rendering. While these approaches have been\ndemonstrated to run in real-time on mobile devices, they sacrifice rendering quality, especially for\nsemi-transparent objects, due to the mesh-based representation. Additionally, storing high-resolution\nmeshes with features is memory-intensive, which limits the resolution and complexity of the mesh\nthat can be used for rendering.\nNeural Light Fields. Recent works such as R2L [36], LFNS [31] and NeuLF [20] have framed\nthe view-synthesis problem as directly predicting pixel colors from camera rays, making these\napproaches fast at inference time without the need for multiple network passes to generate a pixel\ncolor. However, due to the complexity of the 4D light field signal, the light field network requires\nsufficient expressibility to be able to memorize the signal. As a result, Wang et al. [36] end up using\nas many as 88 network layers, which takes three seconds to render one 200 \u00d7 200 image on iPhone 13.\nIn this regard, Cao et al. [4] introduce a novel network architecture that dramatically reduces R2L\u2019s\ncomputation through super-resolution. The deep networks are only evaluated on a low-resolution ray\nbundle and then upsampled to the full image resolution. This approach, termed MobileR2L, achieves\nreal-time rendering on mobile phones. NeuLF [20] also proposes to directly regress pixel colors\n3\nusing a light slab ray representation but is unable to capture fine-level details due to lack of any sort\nof high-dimensional input encoding and is limited to frontal scenes. Another notable work, SIGNET\n[8], utilizes neural methods to compress a light field by using a ultra spherical input encoding to\nthe light slab representation. However, SIGNET doesn\u2019t guarantee photorealistic reconstruction and\nhence deviates from task at hand. Throughout the paper, we will mainly compare our method to\nMobileR2L [4], which is currently the state-of-the-art method for real-time rendering on mobile\ndevices and achieves the highest PSNR among existing methods.\nIt is important to note that training NeLFs requires densely sampled camera poses in the training\nimages and may not generalize well if the training images are sparse, as NeLFs do not explicitly\nmodel geometry. While there have been works, such as those by Attal et al. [2], that propose a\nmixture of NeRF and local NeLFs, allowing learning from sparse inputs, we do not consider this to be\na drawback since NeLFs focus on photo-realistic rendering rather than reconstructing the light field\nfrom sparse inputs, and they can leverage state-of-the-art reconstruction methods like NeRF to create\ndense training images. However, it is a drawback for prior NeLFs [36, 4] that they train extremely\nslowly, often taking more than two days to converge for a single scene. This is where our new method\ncomes into play, as it offers improvements in terms of training efficiency and convergence speed.\n3\nMethodology\n3.1\nPrerequisites\n4D Light Fields or Lumigraphs are a representation of light fields that capture the radiance\ninformation along rays in empty space. They can be seen as a reduction of the higher-dimensional\nplenoptic functions. While plenoptic functions describe the amount of light (radiance) flowing in\nevery direction through every point in space, which typically has five degrees of freedom, 4D light\nfields assume that the radiance is constant along the rays. Therefore, a 4D light field is a vector\nfunction that takes a ray as input (with four degrees of freedom) and outputs the corresponding\nradiance value. Specifically, assuming that the radiance c is represented in the RGB space, a 4D light\nfield is mathematical defined as a function, i.e.:\nF : r \u2208 RM 7\u2192 c \u2208 R3,\n(1)\nwhere r is M-dimensional coordinates of the ray depending how it is parameterized.\nGenerating images from the 4D light field is a straightforward process. For each pixel on the image\nplane, we calculate the corresponding viewing ray r that passes through the pixel, and the pixel value\nis obtained by evaluating the light field function F(r). In this paper, our goal is to identify a suitable\nrepresentation for F(r) that minimizes the number of parameters required for learning and facilitates\nfaster evaluation and training.\nMobileR2L. We adopt the problem setup introduced by MobileR2L [6] and its predecessor R2L [36],\nwhere the light field F(r) is modeled using neural networks. The training of the light field network is\nframed as distillation, leveraging a large dataset that includes both real images and images generated\nby a pre-trained NeRF. Both R2L and MobileR2L represent r using stratified points, which involves\nconcatenating the 3D positions of points along the ray through stratified sampling. In addition, the\n3D positions are encoded using sinusoidal positional encoding [23]. Due to the complexity of the\nlight field, the network requires a high level of expressiveness to capture fine details in the target\nscene. This leads to the use of very deep networks, with over 88 layers in the case of R2L. While this\nallows for detailed rendering, it negatively impacts the rendering speed since the network needs to be\nevaluated for every pixel in the image.\nTo address this issue, MobileR2L proposes an alternative approach. Instead of directly using deep\nnetworks to generate high-resolution pixels, they employ deep networks to generate a low-resolution\nfeature map, which is subsequently up-sampled to obtain high-resolution images using shallow\nsuper-resolution modules. This approach greatly reduces the computational requirements and enables\nreal-time rendering on mobile devices. In our work, we adopt a similar architecture, with a specific\nfocus on improving the efficiency of generating the low-resolution feature map.\n4\ny\nx\nv\nu\nxy\nxu\nuv\nyv\nyu\nxv\nLow-resolution Ray \nBundle\nRay-space Grid Encoder\nDecoder Network\nTwo-Plane Light \nField\n2D Feature Grids\nConv 1x1\nConv 1x1\n\u2026\nSuper-Resolution\nEncoded \nRay\nFinal\nFigure 2: LightSpeed Model for Frontal Scenes. Taking a low-resolution ray bundle as input, our\napproach formulates rays in two-plane ray representation. This enables us to encode each ray using\nmulti-scale feature grids, as shown. The encoded ray bundle is fed into a decoder network consisting\nof convolutions and super-resolution modules yielding the high-resolution image.\n3.2\nLightSpeed\nWe first describe the light-slab ray representation for both frontal and non-frontal scenes in Sec. 3.2.1.\nNext, we detail our grid representation for the light-slab in Sec. 3.2.2 and explain the procedure for\nsynthesizing images from this grid representation in Sec. 3.3. Refer to Fig. 2 for a visual overview.\n3.2.1\nRay Parameterization\nLight Slab (two-plane representation). Instead of utilizing stratified points or Pl\u00fccker coordinates,\nwe represent each directed light ray using the classic two-plane parameterization[19] as an ordered\npair of intersection points with two fixed planes. Formally,\nr = (x, y, u, v),\n(2)\nwhere (x, y) \u2208 R2 and (u, v) \u2208 R2 are ray intersection points with fixed planes P1 and P2 in their\nrespective coordinate systems. We refer to these four numbers as the ray coordinates in the 4D ray\nspace. To accommodate unbounded scenes, we utilize normalized device coordinates (NDC) and\nselect the planes P1 and P2 as the near and far planes (at infinity) defined in NDC.\nDivided Light Slabs for Non-frontal Scenes.\nA single light slab is only suitable for modeling\na frontal scene and cannot capture light rays that are parallel to the planes. To model non-frontal\nscenes, we employ a divide-and-conquer strategy by using a composition of multiple light slab\nrepresentations to learn the full light field. We partition the light fields into subsets, and each subset\nis learned using a separate NeLF model. The partitions ensure sufficient overlap between sub-scenes,\nresulting in a continuous light field representation without additional losses while maintaining the\nfrontal scene assumption. To perform view synthesis, we identify the scene subset of the viewing\nray and query the corresponding NeLF to generate pixel values. Unlike Attal et al. [2], we do not\nperform alpha blending of multiple local light fields because our division is based on ray space rather\nthan partitioning 3D space.\nFor object-centric 360\u25e6 scenes, we propose to partition the scene into 5 parts using surfaces of a\nnear-isometric trapezoidal prism and approximate each sub-scene as frontal (as illustrated in Fig. 3).\nFor unbounded 360\u25e6 scenes, we perform partitioning using k-means clustering based on camera\norientation and position. We refer the reader to the supplementary material for more details on our\nchoice of space partitioning.\n3.2.2\nFeature Grids for Light Field Representation\nStoring the 4D light-slab directly using a high-resolution grid is impractical in terms of storage and\ninefficient for learning due to the excessive number of parameters to optimize. The primary concern\narises from the fact that the 4D grid size increases quartically with respect to resolutions. To address\nthis, we suggest the following design choices to achieve a compact representation of the light-slab\nwithout exponentially increasing the parameter count.\n5\nDivided Object-Centric Scene\nScene Subset as Frontal Scene\nTwo-Plane Light Field\n\u221e\nFinal\nFigure 3: Space Partitioning for Non-frontal scenes. We partition object-centric 360\u25e6 scenes into 5\nparts as shown. Each colored face of the trapezoidal prism corresponds to a partitioning plane. Each\nscene subset is subsequently learned as a separate NeLF\nLower Resolution Feature Grids. Instead of storing grids at full resolution, we choose to utilize\nlow-resolution feature grids to take advantage of the quartic reduction in storage achieved through\nresolution reduction. We anticipate that the decrease in resolution can be compensated by employing\nhigh-dimensional features. In our implementation, we have determined that feature grids of size\n1284 are suitable for synthesizing full HD images. Additionally, we adopt the approach from Instant-\nNGP [25] to incorporate multi-resolution grids, which enables an efficient representation of both\nglobal and local scene structures.\nDecompose 4D Grids into 2D Grids. Taking inspiration from k-planes [10], we propose to\ndecompose the 4D feature grid using\n\u00004\n2\n\u0001\n= 6 number of 2D grids, with each 2D grid representing a\nsub-space of the 4D ray space. This results in a storage complexity of O(6N 2), greatly reducing the\nstorage required to deploy our grid-based approach to mobile devices.\n3.3\nView Synthesis using Feature Grids\nSimilar to MobileR2L [4], LightSpeed takes two steps to render a high resolution image (see Fig. 2).\nEncoding Low-Resolution Ray Bundles. The first step is to render a low-resolution (HL \u00d7 WL)\nfeature map from the feature grids. This is accomplished by generating ray bundles at a reduced\nresolution, where each ray corresponds to a pixel in a downsampled image. We project each ray\u2019s 4D\ncoordinates r = (x, y, u, v) onto 6 2D feature grids Gxy, Gxu, Gxv, Gyu, Gyv, Guv to obtain feature\nvectors from corresponding sub-spaces. The feature values undergo bilinear interpolation from the\n2D grids, resulting in six interpolated F-dimensional features. These features are subsequently\nconcatenated to form a 6F-dimensional feature vector. As the feature grids are multi-resolutional\nwith L levels, features gl(r) \u2208 R6F from different levels (indexed by l) are concatenated together to\ncreate a single feature g(r) \u2208 R6LF . Combining the features from all rays generates a low-resolution\n2D feature map \u02dcG \u2208 RHL\u00d7WL\u00d76LF , which is then processed further in the subsequent step.\nDecoding High-Resolution Image. To mitigate the approximation introduced by decomposing\n4D grids into 2D grids, the features g(r) undergo additional processing through a MLP. This is\nimplemented by applying a series of 1 \u00d7 1 convolutional layers to the low-resolution feature map .\nSubsequently, the processed feature map is passed through a sequence of upsampling layers (similar\nto MobileR2L [4]) to generate a high-resolution image.\n4\nExperiments\nDatasets. We benchmark our approach on the real-world forward-facing [22] [23], the realistic\nsynthetic 360\u25e6 datasets [23] and unbounded 360\u25e6 scenes [3]. The forward-facing dataset consists of\n8 real-world scenes captured using cellphones, with 20-60 images per scene and 1/8th of the images\nused for testing. The synthetic 360\u25e6 dataset has 8 scenes, each having 100 training views and 200\ntesting views. The unbounded 360\u25e6 dataset consists of 5 outdoor and 4 indoor scenes with a central\nobject and a detailed background. Each scene has between 100 to 300 images, with 1 in 8 images\nused for testing. We use 756 \u00d7 1008 LLFF dataset images, 800 \u00d7 800 resolution for the 360\u25e6 scenes,\nand 1/4th of the original resolution for the unbounded 360\u25e6 scenes.\n6\nGT: Paper\nGT: Paper\n(a) Scene\n(b) Ground truth\n(c) NeRF\n(d) MobileR2L\n(e) LightSpeed\nFigure 4: Qualitative Results on frontal and non-frontal scenes. Zoomed-in comparison between\nNeRF [23], MobileR2L [4] and our LightSpeed approach.\nTraining Details. We follow a similar training scheme as MobileR2L: train the LightSpeed model\nusing pseudo-data mined from a pre-trained NeRF teacher. We specifically train MipNeRF teachers\nto sample 10k pseudo-data points for the LLFF dataset. For synthetic and unbounded 360\u25e6 scenes,\nwe mine 30k samples per scene using Instant-NGP [25] teachers. Following this, we fine-tune the\nmodel on the original data. We optimize for the mean-squared error between generated and ground\ntruth images. We refer the reader to the supplementary material for more training details.\nWe use 63 \u00d7 84 (12\u00d7 downsampled from the desired 756 \u00d7 1008 resolution) input ray bundles\nfor the forward-facing scenes. For 360\u25e6 scenes, we use 100 \u00d7 100 (8\u00d7 downsampled from the\ndesired 800 \u00d7 800 image resolution) ray bundles. For unbounded scenes, we use ray bundles 12\u00d7\ndownsampled from the image resolution we use. We train our frontal LightSpeed models as well as\neach sub-scene model in non-frontal scenes for 200k iterations.\nBaselines and Metrics.\nWe compare our method\u2019s performance on bounded scenes with\nMobileR2L[6], MobileNeRF[6] and SNeRG[13]. We evaluate our method for rendering quality\nusing three metrics: PSNR, LPIPS, and SSIM. For unbounded scenes, we report the PSNR metric\non 6 scenes and compare it with MobileNeRF [6] and NeRFMeshing [27]. To further demonstrate\nthe effectiveness of our approach, we compare our approach with others on two other criteria: (a)\nOn-device Rendering Speed: We report and compare average inference times per rendered frame\non various mobile chips, including Apple A15, Apple M1 Pro and Snapdragon SM8450 chips; and\n(b) Efficient Training: We compare the number of iterations LightSpeed and MobileR2L require to\nreach a target PSNR. We pick Lego scene from 360\u25e6 scenes and Fern from forward-facing scenes as\nrepresentative scenes to compare. We also report the storage requirements of our method per frontal\nscene and compare it with baselines.\n4.1\nResults and Analysis\nRendering Quality. As in Tab. 1, we obtain better results on all rendering fidelity metrics on the two\nbounded datasets. We also outperform MobileNeRF and NeRFMeshing on 4 out of 6 unbounded 360\u25e6\nscenes. We refer the reader to Fig. 4 for a visual comparison of our approach with MobileR2L and\nNeRF. Our method has much better rendering quality, capturing fine-level details where MobileR2L,\nand in some cases, even the original NeRF model, fails. Note that we use Instant-NGP teachers for\n360\u25e6 scenes, which have slightly inferior performance to MipNeRF teachers used by MobileR2L.\nThis further shows the robustness of our approach to inferior NeRF teachers.\nStorage Cost. We report storage requirements in Tab. 1. Our approach has a competitive on-device\nstorage to the MobileR2L model. Specifically, we require a total of 16.3 MB of storage per frontal\nscene. The increase in storage is expected since we\u2019re using grids to encode our light field. We also\nreport storage values for lighter LightSpeed networks in the ablation study (see Tab. 5), all of which\nhave similar or better rendering quality than the full-sized MobileR2L network.\nTraining Speed. We benchmark the training times and the number of iterations required for\nLightSpeed and MobileR2L in Tab. 2 with a target PSNR of 24 for Fern scene and 32 for the\nLego scene. Our approach demonstrates a training speed-up of 2.5\u00d7 on both scenes. Since we are\nmodeling 360\u25e6 scenes as a composition of 5 light fields, we can train them in parallel (which is not\n7\nTable 1: Quantitative Comparison on Forward-facing, Synthetic 360\u25e6 and Unbounded 360\u25e6\nDatasets. LighSpeed achieves the best rendering quality with competitive storage. We use an\nout-of-the-box Instant-NGP [25] implementation [1] (as teachers for 360\u25e6 scenes) which dose not\nreport SSIM and LPIPS values. We omit storage for NeRF-based methods since they are not\ncomparable.\nMethod\nSynthetic 360\u25e6\nForward-Facing\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nStorage \u2193\nNeRF [23]\n31.01\n0.947\n0.081\n26.50\n0.811\n0.250\n-\nNeRF-PyTorch\n30.92\n0.991\n0.045\n26.26\n0.965\n0.153\n-\nSNeRG [13]\n30.38\n0.950\n0.050\n25.63\n0.818\n0.183\n337.3 MB\nMobileNeRF [6]\n30.90\n0.947\n0.062\n25.91\n0.825\n0.183\n201.5 MB\nMobileR2L [4]\n31.34\n0.993\n0.051\n26.15\n0.966\n0.187\n8.2 MB\nLightSpeed (Ours)\n32.23\n0.994\n0.038\n26.50\n0.968\n0.173\n16.3 MB\nOur Teacher\n32.96\n-\n-\n26.85\n0.827\n0.226\n-\nUnbounded 360\u25e6\nMethod\nBicycle\nGarden\nStump\nBonsai\nCounter\nKitchen\nMobileNeRF [6]\n21.70\n23.54\n23.95\n-\n-\n-\nNeRFMeshing [27]\n21.15\n22.91\n22.66\n25.58\n20.00\n23.59\nLightSpeed (Ours)\n22.51\n24.54\n22.22\n28.24\n25.46\n27.82\nInstant-NGP (Our teacher) [25]\n21.70\n23.40\n23.20\n27.4\n25.80\n27.50\nTable 2: Training Time for Lego and Fern scenes with 32 and 24 target PSNRs. LightSpeed trains\nsignificantly faster than MobileR2L. It achieves even greater speedup when trained in parallel for\n360\u25e6 scenes (parallel training is not applicable for frontal scenes).\nForward-Facing: Fern\nSynthetic 360\u25e6: Lego\nMethod\nDuration \u2193\nIterations \u2193\nDuration \u2193\nIterations \u2193\nMobileR2L\n12.5 hours\n70k\n192 hours\n860k\nLightSpeed\n4 hours\n27k\n75 hours\n425k\nLightSpeed (Parallelized)\n-\n-\n15 hours\n85k\npossible for MobileR2L), further trimming down the training time. Moreover, the training speedup\nreaches \u223c 4\u00d7 when networks are trained beyond the mentioned target PSNR (see Fig. 1).\nInference Speed. Tab. 3 shows our method\u2019s inference time as compared to MobileR2L and\nMobileNeRF. We maintain a comparable runtime as MobileR2L while having better rendering fidelity.\nSince on-device inference is crucial to our problem setting, we also report rendering times of a smaller\n30-layered decoder network that has similar rendering quality as the MobileR2L model (see Tab. 5).\nTable 3: Rendering Latency Analysis. LightSpeed maintains a competitive rendering latency (ms)\nto prior works. MobileNeRF is not able to render 2 out of 8 real-world scenes ( N\nM in table) due to\nmemory constraints, and no numbers are reported for A13, M1 Pro and Snapdragon chips.\nForward-Facing\nSynthetic 360\u25e6\nChip\nMobileNeRF\nMobileR2L\nOurs\nOurs (30-L)\nMobileNeRF\nMobileR2L\nOurs\nOurs (30-L)\nApple A13 (Low-end)\n-\n40.23\n41.06\n32.29\n-\n65.54\n66.10\n53.89\nApple A15(Low-end)\n27.15 2\n8\n18.04\n19.05\n15.28\n17.54\n26.21\n27.10\n20.15\nApple A15(High-end)\n20.98 2\n8\n16.48\n17.68\n15.03\n16.67\n22.65\n26.47\n20.35\nApple M1 Pro\n-\n17.65\n17.08\n13.86\n-\n27.37\n27.14\n20.13\nSnapdragon SM8450\n-\n39.14\n45.65\n32.89\n-\n40.86\n41.26\n33.87\n4.2\nAblations\nData Requirements. We use 10k samples as used by MobileR2L to train LightField models for\nfrontal scenes. However, for non-frontal scenes, we resort to using 30k pseudo-data samples per\n8\n0\n50000\n100000\n150000\n200000\nTraining iterations\n24\n26\n28\n30\nRendering Quality (PSNR)\nLS (Grid) D15\nLS (Freq) D15\nPlucker (Freq) D15\n0\n50000\n100000\n150000\n200000\nTraining iterations\n24\n26\n28\n30\n32\nRendering Quality (PSNR)\nLS (Grid) D30\nLS (Freq) D30\nPlucker (Freq) D30\n0\n50000\n100000\n150000\n200000\nTraining iterations\n24\n26\n28\n30\n32\nRendering Quality (PSNR)\nLS (Grid) D60\nLS (Freq) D60\nPlucker (Freq) D60\n(a) 15-Layer Networks\n(b) 30-Layer Networks\n(c) 60-Layer Networks\nFigure 5: Test PSNR v/s Training Iterations. We compare test set PSNR obtained by LightSpeed\n(Grid)(ours), LightSpeed (frequency encoded), and Pl\u00fccker-based neural light field as the training\nprogresses for 3 different network configurations.\nscene. Dividing 10k samples amongst 5 sub-scenes assigns too few samplers per sub-scene, which is\ndetrimental to grid learning. We experimentally validate data requirements by comparing MobileR2L\nand LightSpeed trained for different amounts of pseudo-data. We train one 400 \u00d7 400 sub-scene\nfrom the Lego scene for 200k iterations with 1/5th of 10k and 30k samples, i.e., 2k and 6k samples.\nTab. 4 exhibits significantly decreased rendering quality for the LightSpeed network as compared to\nMobileR2L when provided with less pseudo-data.\nTable 4: Pseudo-Data Requirement for Non-Frontal Scenes. We analyze the importance of mining\nmore pseudo-data for non-frontal scenes. Using 1/5th of 10k and 30k sampled pseudo-data points,\nwe find more pseudo-data is crucial for the boosted performance of the LightSpeed model.\n2k Samples\n6k Samples\nMethod\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMobileR2L\n30.19\n0.9894\n0.0354\n30.56\n0.9898\n0.0336\nLightSpeed (Ours)\n30.44\n0.9899\n0.0299\n31.2\n0.9906\n0.0284\nDecoder Network Size. We further analyze the trade-off between inference speed and rendering\nquality of our method and MobileR2L. To this end, we experiment with decoders of different\ndepths and widths. Each network is trained for 200k iterations and benchmarked on an iPhone 13.\nTab. 5 shows that a 30-layered LightSpeed model has a better inference speed and rendering quality\nas compared to the 60-layered MobileR2L model. This 30-layered variant further occupies less\nstorage as compared to its full-sized counterpart. Furthermore, lighter LightSpeed networks obtain\na comparable performance as the 60-layered MobileR2L. Note that reducing the network capacity\nof MobileR2L results in significant drops in performance. This means that we can get the same\nrendering quality as MobileR2L with considerably reduced on-device resources, paving the way for a\nmuch better trade-off between rendering quality and on-device inference speed.\nTable 5: Decoder Network Size. Our approach maintains a much better tradeoff between inference\nspeeds v/s rendering quality, with our smallest network achieving comparable quality to the\nMobileR2L. Benchmarking done on an iPhone 13. L is network depth, and W is network width.\nMethod\nPSNR \u2191\nLatency \u2193\nStorage \u2193\nFLOPs \u2193\n15-L W-256 MobileR2L\n27.69\n14.54 ms\n2.4 MB\n12626M\n30-L W-128 MobileR2L\n27.54\n14.47 ms\n1.4 MB\n8950M\n30-L W-256 MobileR2L\n29.21\n18.59 ms\n4.5 MB\n23112M\n60-L W-256 MobileR2L\n30.34\n22.65 ms\n8.2 MB\n42772M\n15-L W-256 LightSpeed\n30.37\n14.94 ms\n10.5 MB\n12833M\n30-L W-128 LightSpeed\n30.13\n14.86 ms\n9.5 MB\n9065M\n30-L W-256 LightSpeed\n31.70\n20.35 ms\n12.6 MB\n23319M\n60-L W-256 LightSpeed\n32.34\n26.47 ms\n16.3 MB\n42980M\nRay-Space Grid Encoding. We provide an ablation in Tab. 6 below on how the proposed ray-space\ngrid encoder helps as compared to just using the light-slab representation with a traditional frequency\nencoder. We compare different LightSpeed configurations with grid-encoder and frequency encoders.\nNetworks are trained for 200k iterations on a full-resolution 800\u00d7800 Lego sub-scene from Synthetic\n9\n360\u25e6 dataset. Further, we show the training dynamics of all the trained variants in Fig. 5 (red and\ngreen plots). As claimed, our approach offers better visual fidelity and training dynamics (iterations to\nreach a target PSNR) for both computationally cheaper small networks as well as full sized networks.\nTable 6: Effect of using a Ray-Space Grid Encoder. We demonstrate the effect of using a grid-based\nLightSpeed by comparing with a frequency encoded variant (no grid). L is network depth, and W is\nnetwork width.\nMethod\nPSNR \u2191\n15-L W-256 LS (PE)\n28.84\n30-L W-256 LS (PE)\n30.63\n60-L W-256 LS (PE)\n32.16\n15-L W-256 LS (Grid)\n30.37\n30-L W-256 LS (Grid)\n31.70\n60-L W-256 LS (Grid)\n32.34\nComparison with Pl\u00fccker Representation.\nGiven the challenges of discretizing Pl\u00fccker\nrepresentation, we compare between using positionally encoded Pl\u00fccker coordinates and our grid-\nbased light-slab approach in Tab. 7 below for different network sizes to demonstrate the effectiveness\nof our approach. We train all models for 200k iterations on one 800\u00d7800 Lego sub-scene. We also\nshare training curves for the variants in question in Fig. 5 (red and blue curves). As claimed, our\nintegrated approach performs better in terms of training time and test-time visual fidelity for large\nand small models (having less computational costs) alike whereas the Pl\u00fccker-based network shows a\nsharp decline in visual fidelity and increased training times to reach a target test PSNR as network\nsize is reduced.\nTable 7: Light-Slab Grid Representation vs. Pl\u00fccker Coordinates. We compare the light-slab\nbased LightSpeed (LS) with a positionally encoded variant of the Pl\u00fccker ray representation. L is\nnetwork depth, and W is network width.\nMethod\nPSNR \u2191\n15-L W-256 Pl\u00fccker\n28.65\n30-L W-256 Pl\u00fccker\n30.84\n60-L W-256 Pl\u00fccker\n32.14\n15-L W-256 LS\n30.37\n30-L W-256 LS\n31.70\n60-L W-256 LS\n32.34\n5\nDiscussion and Conclusion\nIn this paper, we propose an efficient method, LightSpeed, to learn neural light fields using the\nclassic two-plane ray representation. Our approach leverages grid-based light field representations\nto accelerate light field training and boost rendering quality. We demonstrate the advantages of our\napproach not only on frontal scenes but also on non-frontal scenes by following a divide-and-conquer\nstrategy and modeling them as frontal sub-scenes. Our method achieves SOTA rendering quality\namongst prior works at same time providing a significantly better trade-off between rendering fidelity\nand latency, paving the way for real-time view synthesis on resource-constrained mobile devices.\nLimitations. While LightSpeed excels at efficiently modeling frontal and 360\u25e6 light fields, it currently\nlacks the capability to handle free camera trajectories. The current implementation does not support\nrefocusing, anti-aliasing, and is limited to static scenes without the ability to model deformable\nobjects such as humans. We plan to explore these directions in future work.\nBroader Impact.\nFocused on finding efficiencies in novel view synthesis, our study could\nsignificantly reduce costs, enabling wider access to this technology. However, potential misuse,\nlike unsolicited impersonations, must be mitigated.\n10\nReferences\n[1] ngp_pl. https://github.com/kwea123/ngp_pl,. 8\n[2] Benjamin Attal, Jia-Bin Huang, Michael Zollh\u00f6fer, Johannes Kopf, and Changil Kim. Learning neural\nlight fields with ray-space embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 19819\u201319829, 2022. 2, 3, 4, 5\n[3] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360:\nUnbounded anti-aliased neural radiance fields. CVPR, 2022. 6, 16\n[4] Junli Cao, Huan Wang, Pavlo Chemerys, Vladislav Shakhrai, Ju Hu, Yun Fu, Denys Makoviichuk, Sergey\nTulyakov, and Jian Ren. Real-time neural light field on mobile devices. arXiv preprint arXiv:2212.08057,\n2022. 2, 3, 4, 6, 7, 8, 15, 16\n[5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In\nECCV, 2022. 1, 3\n[6] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. Mobilenerf: Exploiting the\npolygon rasterization pipeline for efficient neural field rendering on mobile architectures. arXiv preprint\narXiv:2208.00277, 2022. 1, 3, 4, 7, 8, 15\n[7] Inchang Choi, Orazio Gallo, Alejandro Troccoli, Min H Kim, and Jan Kautz. Extreme view synthesis. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 7781\u20137790, 2019. 3\n[8] Brandon Yushan Feng and Amitabh Varshney. Signet: Efficient neural representation for light fields. In\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 14224\u201314233,\nOctober 2021. 4\n[9] John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham Fyffe, Ryan Overbeck, Noah\nSnavely, and Richard Tucker. Deepview: View synthesis with learned gradient descent. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2367\u20132376, 2019. 2, 3\n[10] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k Warburg, Benjamin Recht, and Angjoo Kanazawa.\nK-planes for radiance fields in space, time, and appearance, 2023. 1, 2, 3, 6\n[11] Andre\u00af\u0131 Gershun. The light field. Journal of Mathematics and Physics, 18(1-4):51\u2013151, 1939. 2\n[12] Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F Cohen.\nThe lumigraph.\nIn\nProceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 43\u201354,\n1996. 2, 3\n[13] Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, and Paul Debevec. Baking neural\nradiance fields for real-time view synthesis. ICCV, 2021. 1, 3, 7, 8\n[14] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error\nlinear units. CoRR, abs/1606.08415, 2016. URL http://arxiv.org/abs/1606.08415. 14\n[15] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. CoRR, abs/1502.03167, 2015. URL http://arxiv.org/abs/1502.03167. 14\n[16] Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ramamoorthi. Learning-based view synthesis for\nlight field cameras. ACM Transactions on Graphics (TOG), 35(6):1\u201310, 2016. 2, 3\n[17] Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy J. Mitra. Relu fields: The little non-linearity\nthat could. Transactions on Graphics (Proceedings of SIGGRAPH), volume = 41, number = 4, year =\n2022, month = july, pages = 13:1\u201313:8, doi = 10.1145/3528233.3530707. 1, 2, 3\n[18] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014. 14\n[19] Marc Levoy and Pat Hanrahan. Light field rendering. In Proceedings of the 23rd annual conference on\nComputer graphics and interactive techniques, pages 31\u201342, 1996. 2, 3, 5\n[20] Zhong Li, Liangchen Song, Celong Liu, Junsong Yuan, and Yi Xu. Neulf: Efficient novel view synthesis\nwith neural 4d light field. In Eurographics Symposium on Rendering, 2022. 3\n[21] D. B.* Lindell, J. N. P.* Martel, and G. Wetzstein. Autoint: Automatic integration for fast neural volume\nrendering. In Proc. CVPR, 2021. 3\n11\n[22] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi,\nRen Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling\nguidelines. ACM Transactions on Graphics (TOG), 2019. 2, 6, 14\n[23] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.\nNerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65\n(1):99\u2013106, 2021. 1, 2, 3, 4, 6, 7, 8, 15, 16\n[24] Parry Moon and Domina Eberle Spencer. Theory of the photic field. Journal of the Franklin Institute, 255\n(1):33\u201350, 1953. 2\n[25] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives\nwith a multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013102:15, July 2022. doi: 10.1145/\n3528223.3530127. URL https://doi.org/10.1145/3528223.3530127. 2, 3, 6, 7, 8, 16\n[26] Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H Mueller, Chakravarty R Alla\nChaitanya, Anton Kaplanyan, and Markus Steinberger. Donerf: Towards real-time rendering of compact\nneural radiance fields using depth oracle networks. In Computer Graphics Forum, volume 40, pages 45\u201359.\nWiley Online Library, 2021. 3\n[27] Marie-Julie Rakotosaona, Fabian Manhardt, Diego Martin Arroyo, Michael Niemeyer, Abhijit Kundu, and\nFederico Tombari. Nerfmeshing: Distilling neural radiance fields into geometrically-accurate 3d meshes.\narXiv preprint arXiv:2303.09431, 2023. 1, 3, 7, 8\n[28] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance\nfields with thousands of tiny mlps. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 14335\u201314345, 2021. 1, 3\n[29] Sara Rojas, Jesus Zarzar, Juan Camilo Perez, Artsiom Sanakoyeu, Ali Thabet, Albert Pumarola, and\nBernard Ghanem. Re-rend: Real-time rendering of nerfs across devices. arXiv preprint arXiv:2303.08717,\n2023. 1, 3\n[30] Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa.\nPlenoxels: Radiance fields without neural networks. In CVPR, 2022. 2, 3\n[31] Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, and Fredo Durand.\nLight field networks: Neural scene representations with single-evaluation rendering. In Proc. NeurIPS,\n2021. 2, 3\n[32] Pratul P Srinivasan, Tongzhou Wang, Ashwin Sreelal, Ravi Ramamoorthi, and Ren Ng. Learning to\nsynthesize a 4d rgbd light field from a single image. In Proceedings of the IEEE International Conference\non Computer Vision, pages 2243\u20132251, 2017. 2, 3\n[33] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas M\u00fcller, Morgan McGuire, Alec Jacobson, and\nSanja Fidler. Variable bitrate neural fields. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1\u20139,\n2022. 3\n[34] Jiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Errui Ding, Jingdong Wang, and Gang\nZeng.\nDelicate textured mesh recovery from nerf via adaptive surface refinement.\narXiv preprint\narXiv:2303.02091, 2022. 1, 3\n[35] Ziyu Wan, Christian Richardt, Alja\u017e Bo\u017ei\u02c7c, Chao Li, Vijay Rengarajan, Seonghyeon Nam, Xiaoyu Xiang,\nTuotuo Li, Bo Zhu, Rakesh Ranjan, et al. Learning neural duplex radiance fields for real-time view\nsynthesis. arXiv preprint arXiv:2304.10537, 2023. 1, 3\n[36] Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Menglei Chai, Yun Fu, and Sergey Tulyakov. R2l:\nDistilling neural radiance field to neural light field for efficient novel view synthesis. In ECCV, 2022. 2, 3,\n4\n[37] Peng Wang, Yuan Liu, Guying Lin, Jiatao Gu, Lingjie Liu, Taku Komura, and Wenping Wang.\nProgressively-connected light field network for efficient view synthesis. arXiv preprint arXiv:2207.04465,\n2022. 3\n[38] Gaochang Wu, Belen Masia, Adrian Jarabo, Yuchen Zhang, Liangyong Wang, Qionghai Dai, Tianyou\nChai, and Yebin Liu. Light field image processing: An overview. IEEE Journal of Selected Topics in\nSignal Processing, 11(7):926\u2013954, 2017. 2, 3\n12\n[39] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard Szeliski, Jonathan T\nBarron, and Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-time view synthesis. arXiv preprint\narXiv:2302.14859, 2023. 1, 3\n[40] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees for real-time\nrendering of neural radiance fields. In ICCV, 2021. 1, 3\n[41] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification:\nLearning view synthesis using multiplane images. In SIGGRAPH, 2018. 2, 3\n13\nA\nTraining Details\nNetwork architecture.\nOur multi-scale feature grids have 16 levels, with resolutions exponentially\ngrowing from 16 to 256, and 4-D features in every grid. Our LightSpeed network follows a similar\narchitecture to MobileR2L: 60 point-wise residual convolutions with 256 channels and BatchNorm\n[15]and GeLU [14] activation interleaved. The convolutions are followed by 3 super-resolution\nmodules to upsample the low-resolution input to the desired resolution. The first two super-resolution\nmodules upsample the input by 2\u00d7 and consist of transposed convolution layers with 4 \u00d7 4 kernel\nsize followed by 2 residual convolution layers each. The third super-resolution module consists of\ntransposed kernel size with 4 \u00d7 4 kernel size (upsample by 2\u00d7) for 360\u25e6 scenes (both bounded and\nunbounded) and 3 \u00d7 3 kernel size (upsample by 3\u00d7) for forward-facing [22] scenes.\nTraining details.\nWe use Adam [18] optimizer with a batch size of 32 to train the feature grids\nand decoder network. We use an initial learning rate of 1e-5 with 100 warmup steps taking the\nlearning rate to 5e-4. Beyond that, the learning rate decays linearly until the training finishes. All our\nexperiments are conducted on Nvidia V100s and A100s.\nB\nMore Ablation Analysis\nChoice of Splitting Planes. We discuss two aspects of dividing non-frontal scenes into separate\nlight fields: the number of parts to divide the scene into and the placement of the splitting planes.\nWe find the optimal number of splits for 360\u25e6 scenes to be 5 since more number of splits would\nmean increased storage cost, which is detrimental to mobile deployment. We also want the scene\nsplits to be collectively exhaustive (but not mutually exclusive to maintain continuity while switching\nfrom one light field to another) in the poses sampled around the object. Consequently, fewer planes\nwould mean placing the splitting planes near the scene origin to cover the entire scene, which starts\nto violate the frontal assumption for each sub-scene.\nGiven poses distributed on the surface of a sphere with radius r, we propose assigning each pose to\n(possibly multiple) sub-scenes based on the camera origin satisfying one or more of the 5 following\ncriteria:\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0\n0\n\u221a\n2\n\u221a\n2\n0\n\u221a\n2 \u2212 1\n\u2212\n\u221a\n2\n0\n\u221a\n2 \u2212 1\n0\n\u221a\n2\n\u221a\n2 \u2212 1\n0\n\u2212\n\u221a\n2\n\u221a\n2 \u2212 1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\"x\ny\nz\n#\n\u2265\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nr\nr\nr\nr\nr\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(3)\nThese five hyperplanes form the surface of a near-isometric trapezoidal prism, as shown in Fig.\n3 (main paper). We experimentally show the effect of the choice of splitting plane by training\nLightSpeed models on a Lego sub-scene with different plane placements and compare with the\ncorresponding MobileR2L models trained on the same data. Specifically, we choose two axis-aligned\nplanes at a distance of radius\n\u221a\n2\nand radius\n\u221a\n3\nfrom the scene origin and train models with 6k pseudo\ndata points sampled independently from the two resulting sub-scenes. As shown in Tab. 8, placing\nthe splitting plane at a distance of radius\n\u221a\n3\nresults in inferior performance as compared to placing\nthe splitting plane at a distance of radius\n\u221a\n2\nfrom the origin. This suggests that frontal sub-scene\napproximation starts to break down as we move the splitting plane closer to the origin.\nTable 8: Choice of Splitting Planes. We experiment with two planes parallel to the x-y sub-space at\ndifferent distances. Splitting planes further from the origin work better empirically maintaining the\nfrontal sub-scene assumption.\nLF Representation\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nradius /\n\u221a\n2\n30.44\n0.9903\n0.028\nradius /\n\u221a\n3\n30.23\n0.9899\n0.031\n14\nC\nPer-Scene Quantitative Results\nWe provide a per-scene quantitative comparison between LightSpeed, MobileR2L [6] and NeRF [23]\non the synthetic 360\u25e6 dataset (Tab. 9, Tab. 10, and Tab. 11) and forward-facing dataset (Tab. 12, Tab.\n13, and Tab. 14). We use PSNR, LPIPS, and SSIM as comparison metrics. As can be seen from\nthe comparisons, LighSpeed (our approach) outperforms MobileR2L [4] on almost all the metrics.\nFurther, LightSpeed performs comparably or even better than NeRF [23]. We also provide additional\nzoom-in comparisons between LightSpeed and MobileR2L in Fig. 7.\nTable 9: Per-scene PSNR \u2191 comparison on the Synthetic 360\u25e6 dataset between NeRF [23], MobileR2L\n[4], and our approach.\nMethod\nChair\nDrums\nFicus\nHotdog\nLego\nMaterials\nMic\nShip\nAverage\nNeRF[23]\n33.00\n25.01\n30.13\n36.18\n32.54\n29.62\n32.91\n28.65\n31.01\nMobileR2L [4]\n33.66\n25.05\n29.80\n36.84\n32.18\n30.54\n34.37\n28.75\n31.34\nLightSpeed (Ours)\n34.21\n25.63\n32.82\n36.77\n34.35\n29.51\n35.65\n28.90\n32.23\nTable 10: Per-scene SSIM \u2191 comparison on the Synthetic 360\u25e6 dataset between NeRF [23],\nMobileR2L [4], and our approach.\nMethod\nChair\nDrums\nFicus\nHotdog\nLego\nMaterials\nMic\nShip\nAverage\nNeRF[23]\n0.967\n0.925\n0.964\n0.974\n0.961\n0.949\n0.980\n0.856\n0.947\nMobileR2L [4]\n0.998\n0.986\n0.996\n0.998\n0.992\n0.992\n0.997\n0.982\n0.993\nLightSpeed (Ours)\n0.998\n0.988\n0.998\n0.998\n0.994\n0.990\n0.998\n0.984\n0.994\nTable 11: Per-scene LPIPS \u2193 comparison on the Synthetic 360\u25e6 dataset between NeRF [23],\nMobileR2L [4], and our approach.\nMethod\nChair\nDrums\nFicus\nHotdog\nLego\nMaterials\nMic\nShip\nAverage\nNeRF[23]\n0.046\n0.091\n0.044\n0.121\n0.050\n0.063\n0.028\n0.206\n0.081\nMobileR2L [4]\n0.027\n0.083\n0.025\n0.026\n0.043\n0.029\n0.012\n0.162\n0.051\nLightSpeed (Ours)\n0.017\n0.061\n0.016\n0.023\n0.019\n0.030\n0.007\n0.138\n0.039\nTable 12: Per-scene PSNR \u2191 comparison on the forward-facing dataset between NeRF [23],\nMobileR2L [4], and our approach.\nMethod\nRoom\nFern\nLeaves\nFortress\nOrchids\nFlower\nT-Rex\nHorns\nAverage\nNeRF[23]\n32.70\n25.17\n20.92\n31.16\n20.36\n27.40\n26.80\n27.45\n26.50\nMobileR2L [4]\n32.09\n24.39\n20.52\n30.81\n20.06\n27.61\n26.71\n27.01\n26.15\nLightSpeed (Ours)\n32.32\n25.05\n21.01\n31.45\n20.33\n27.88\n26.93\n27.04\n26.50\nTable 13: Per-scene SSIM \u2191 comparison on the forward-facing dataset between NeRF [23],\nMobileR2L [4], and our approach.\nMethod\nRoom\nFern\nLeaves\nFortress\nOrchids\nFlower\nT-Rex\nHorns\nAverage\nNeRF[23]\n0.948\n0.792\n0.690\n0.881\n0.641\n0.827\n0.880\n0.828\n0.811\nMobileR2L [4]\n0.995\n0.973\n0.923\n0.995\n0.916\n0.971\n0.973\n0.982\n0.966\nLightSpeed (Ours)\n0.991\n0.976\n0.931\n0.996\n0.921\n0.972\n0.975\n0.983\n0.968\n15\nTable 14: Per-scene LPIPS \u2193 comparison on the forward-facing dataset between NeRF [23],\nMobileR2L [4], and our approach.\nMethod\nRoom\nFern\nLeaves\nFortress\nOrchids\nFlower\nT-Rex\nHorns\nAverage\nNeRF[23]\n0.178\n0.280\n0.316\n0.171\n0.321\n0.219\n0.249\n0.268\n0.250\nMobileR2L [4]\n0.088\n0.239\n0.280\n0.103\n0.296\n0.150\n0.121\n0.217\n0.187\nLightSpeed (Ours)\n0.085\n0.211\n0.255\n0.093\n0.272\n0.145\n0.119\n0.209\n0.173\nD\nLimitations\nResults on Unbounded Scenes.\nThe rendering fidelity of LightSpeed is closely tied to the\nperformance of the corresponding NeRF teacher. LightSpeed uses Instant NGP [25] teachers for both\nbounded and unbounded scenes to maintain experimental consistency. We would like to highlight that\nInstant-NGP introduces the artifacts to unbounded scenes, which are carried forward to LightSpeed\nvia the mined pseudo-data. We share some of the pseudo-data images from Instant-NGP in Fig. 6.\nMipNeRF360 [3] specifically uses space contraction techniques to model the unbounded nature of the\nscene and deal with blurriness in the renderings. It further introduces a distortion-based regularizer to\nremove floater artifacts and prevent background collapse. The techniques introduced by MipNeRF360\ntackle the same type of artifacts pointed out in Fig. 6. Hence, using MipNeRF360 teachers will\nmitigate both these issues and could boost the visual fidelity on unbounded scenes for LightSpeed.\nFigure 6: Instant NGP Failure Cases for Unbounded Scenes. Such artifacts carry over to\nLightSpeed, affecting its visual fidelity on unbounded scenes.\n16\nGT: Paper\nGT: Paper\n(a) Scene\n(b) Ground truth\n(c) MobileR2L\n(d) LightSpeed\nFigure 7: Qualitative Results on frontal and non-frontal scenes. Zoomed-in comparison between\nMobileR2L and our LightSpeed approach.\n17\n"
  }
]