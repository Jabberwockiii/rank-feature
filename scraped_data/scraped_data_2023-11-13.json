[
  {
    "title": "JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models",
    "link": "https://arxiv.org/pdf/2311.05997.pdf",
    "upvote": "33",
    "text": "JARVIS-1: Open-world Multi-task Agents with\nMemory-Augmented Multimodal Language\nModels\nZihao Wang1, Shaofei Cai1, Anji Liu2, Yonggang Jin3, Jinbing Hou3, Bowei Zhang1, Haowei Lin1,\nZhaofeng He3, Zilong Zheng4, Yaodong Yang1, Xiaojian Ma4 and Yitao Liang1\n1PKU, 2UCLA, 3BUPT, 4BIGAI, All authors are affiliated with Team CraftJarvis,\nAchieving human-like planning and control with multimodal observations in an open world is a key\nmilestone for more functional generalist agents. Existing approaches can handle certain long-horizon\ntasks in an open world. However, they still struggle when the number of open-world tasks could\npotentially be infinite and lack the capability to progressively enhance task completion as game time\nprogresses. We introduce JARVIS-1, an open-world agent that can perceive multimodal input (visual\nobservations and human instructions), generate sophisticated plans, and perform embodied control, all\nwithin the popular yet challenging open-world Minecraft universe. Specifically, we develop JARVIS-1 on\ntop of pre-trained multimodal language models, which map visual observations and textual instructions\nto plans. The plans will be ultimately dispatched to the goal-conditioned controllers. We outfit JARVIS-\n1 with a multimodal memory, which facilitates planning using both pre-trained knowledge and its\nactual game survival experiences. JARVIS-1 is the existing most general agent in Minecraft, capable\nof completing over 200 different tasks using control and observation space similar to humans. These\ntasks range from short-horizon tasks, e.g., \"chopping trees\" to long-horizon tasks, e.g., \"obtaining a\ndiamond pickaxe\". JARVIS-1 performs exceptionally well in short-horizon tasks, achieving nearly\nperfect performance. In the classic long-term task of ObtainDiamondPickaxe, JARVIS-1 surpasses\nthe reliability of current state-of-the-art agents by 5 times and can successfully complete longer-horizon\nand more challenging tasks. The project page is available at craftjarvis.org/JARVIS-1.\n1. Introduction\nCreating sophisticated agents that can accomplish myriad of tasks in complex domains remains a\npivotal milestone towards generally capable artificial intelligence (Alayrac et al., 2022; Brohan et al.,\n2022a; Brown et al., 2020; Reed et al., 2022; Zhao et al., 2023). Recent advancements have shown a\ntrend towards employing a hierarchical goal execution architecture (Huang et al., 2022a,b; Wang et al.,\n2023b), and leveraging large language models (LLMs) as the high-level planner to generate action\nplans that will be ultimately executed by low-level instruction-following controllers. Albeit the fruitful\nprogress they have yielded in many robotics (Huang et al., 2022b) and even open-world environments\nlike Minecraft (Fan et al., 2022; Guss et al., 2019b), today\u2019s agents built with these approaches are\nstill struggling with three major issues: 1) perceive the world from multimodal sensory observations,\nsuch as images, videos in addition to natural language instructions and feedback for planning; This is\nmostly due to the inability of LLM-based planners on processing multimodal data (Huang et al., 2022a;\nYao et al., 2022); 2) perform consistent and accurate long-term planning. This requires multi-round,\nknowledge, and reasoning-intensive dialogues, which remain great challenges to LLMs (Huang et al.,\n2022b); 3) learn and evolve in a life-long fashion. This calls out the need for agents to propose\ntheir own tasks and self-improve. Addressing these issues will unleash the full planning potential of\nLLM-based agents, and expedite the development of more generalist agents.\nIn this work, we introduce JARVIS-1, a brand new agent that can robustly produce plans for\nCorresponding author(s): Xiaojian Ma, Yitao Liang\nZihao Wang<zhwang@stu.pku.edu.cn>, Shaofei Cai<caishaofei@stu.pku.edu.cn>, Anji Liu<liuanji@cs.ucla.edu>,\nXiaojian Ma<xiaojian.ma@ucla.edu>, Yitao Liang<yitaol@pku.edu.cn>\narXiv:2311.05997v3  [cs.AI]  30 Nov 2023\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n.97\n.97\n.95\n.57\n.96\n.94\n.90\n.92\n.79\n.97\n.75\n.91\n.87\n.91\n.95\n.25\n.25\n.11\n.10\n.04\n.11\n.11\n.04\n.02\n.02\n.05\n.40\n.33\n.32\n.33\n.34\n.09\n.06\n.55\n.05\n.33\n.38\n.30\n.37\n.32\n.36\n.28\n.36\n.28\n.32\n.36\n.38\n.37\n.45\n.50\n.32\n.50\n.31\n.06\n.14\n.05\n.26\n.08\n.18\n.24\n.22\n.23\n.02\n.05\n.05\n.08\n.06\n.02\n.04\n.02\n.05\n.02\n.02\n.14\n.88\n.79\n.84\n.94\n.87\n.90\n.97\n.67\n.90\n.89\n.93\n.92\nFigure 1 | How does JARVIS-1 unlock the technology tree of the Minecraft universe. JARVIS-1 can\nconsistently obtain high-level items on the main tech-tree of the overworld in Minecraft, such as diamond,\nredstone, and golden items, which require collecting over 10 different intermediate items. JARVIS-1 not\nonly outperforms the previous state-of-the-art VPT (Baker et al., 2022) (6% vs. 2.5% reliability) on diamond\npickaxe, but also can craft almost all diamond items in the overworld including diamond chestplate.\nlong-horizon tasks from multimodal user and environment inputs, and translate them into motor\ncontrol in Minecraft, a popular yet challenging open-world testbed for generalist agents. To be specific,\nwe chain a multimodal foundation model MineCLIP(Fan et al., 2022) and an LLM(Brown et al., 2020)\ntogether, the resulting multimodal language model (MLM) allows our agent to better understand the\ntask, situations, and environmental feedback. To further enhance the correctness and consistency\nof planning, especially on long-horizon tasks, we propose to augment the agent with a multimodal\nmemory, which stores both the scenarios and actual plans of the successful planning experiences in\nthe past. By retrieving the relevant memory entries, the planning skill of our MLM-based agent can be\nstrengthened from the agent\u2019s own interactions with the environment in an in-context manner. Finally,\nJARVIS-1 is able to evolve throughout the gameplay by proposing tasks on its own (i.e. self-instruct)\nas a means of exploration and saving the obtained experiences in the multimodal memory, therefore\nfacilitating better reasoning and planning. This self-improving ability sparks its potential for a higher\nlevel of autonomy.\nOur main evaluations are conducted in Minecraft, with more than 200 tasks selected from the\nMinecraft Universe Benchmark (Lin et al., 2023a), with no demonstration provided. The tasks cover\na broad spectrum from the early game (e.g. ObtainCraftingTable) to intermediate and even\nchallenging long-horizon tasks (e.g. ObtainDiamondPickaxe). A glimpse of what JARVIS-1 is\nable to achieve can be found in Figure 1. JARVIS-1 exhibits strong performances on these tasks,\nrepresenting an up to 5\u00d7 increase to the previous records. Our ablative analysis then offers a detailed\naccount of how JARVIS-1 approaches this significant progress and becomes the first agent that can\nrobustly obtain the diamond pickaxe with up to 12.5% success rate. What is even more surprising\nis that, without the need for additional training, JARVIS-1 demonstrates a continuous increase in\nperformance as game time increases in long-horizon tasks. Moreover, JARVIS-1 has demonstrated\nits potential of self-improve in an exploratory life-long learning experiment, where it needs to propose\ntasks to progressively explore the world, collect experiences, and sharpen its planning skill using\nthese experiences stored in the multimodal memory.\n2\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nIn summary, JARVIS-1 pilots the effort towards a human-like multi-task and autonomous agent in\nan open-world, embodied environment like Minecraft. We would like to share the key takeaways of\nwhat we have learned during its development as follows:\n\u2022 From LLMs to MLMs. The capability of perceiving multimodal sensory input is critical to\nplanning in a dynamic and open-world world. JARVIS-1 enables this by chaining a multimodal\nfoundation model together with an LLM. Compared to LLM \u201cblindly\u201d produces plans, MLM is able\nto natively understand the current situation and plan accordingly. Further, rich environmental\nfeedback can be obtained through multimodal perception, therefore helping the self-check and\nself-explain of the planner spot and fix possible bugs in the plans, enabling stronger interactive\nplanning.\n\u2022 Multimodal memory. Early research has suggested the crucial role that memory mechanisms\ncan serve in the functioning of generalist agents. By outfitting JARVIS-1 with a multimodal\nmemory, we effectively allow it to plan with both pretrained knowledge and its actual experiences\nin the world, therefore bringing significant improvement to planning correctness and consistency.\nCompared to canonical RL or planning agents with exploration, no additional model update is\nneeded as the MLM in JARVIS-1 makes it possible to leverage these experiences in an in-context\nmanner.\n\u2022 Self-instruct and self-improve. A sign of generalist agents is the capacity to proactively\nacquire new experiences and continuously improve themselves. We have demonstrated how\nJARVIS-1 effectively traverses the environment by executing tasks autonomously generated\nthrough its self-instruct mechanism. With multimodal memory teaming up with experiences\nfrom the explorations, we have observed consistent improvement, especially in accomplishing\nmore complicated tasks. Ultimately, this aspect of autonomous learning in JARVIS-1 signifies\nan evolutionary step towards generalist agents that can learn, adapt, and improve over time\nwith minimal external intervention.\n2. Challenges for Open-world Agents\nCompared to canonical scenarios with relatively small scale, simple dynamics, and limited tasks,\nopen-world environments impose substantial challenges to building agents that can accomplish a\ndiverse set of tasks (Cai et al., 2023a,b; Fan et al., 2022; Guss et al., 2019a, 2021; Kanervisto et al.,\n2022; Wang et al., 2023b). In this section, we will review three major challenges we\u2019ve identified\nduring the development of JARVIS-1.\n2.1. Challenge I: Situation-Aware Planning\nIn an open world, there could be various possible paths towards an open-world goal. However, not all\nof them are plausible or equally efficient given a certain situation (location, inventory status, etc.).\nFor example, building a bed\ncan be done through collecting wool from sheeps\n, haunting spiders\nfor strings\n, or trading with villagers\n. Depending on the current location and its proximity to\nthese subjects, some options can be more viable and more efficient than others. Further, the agent\u2019s\nown situation can also change throughout the episode, e.g. day and night shifts, weather conditions\n(bringing different types of danger), and tool usage (it can be broken). To this end, the plan needs to\nbe constantly updated based on the current situation. Figure 2 (left) shows that when attempting the\n\"ObtainDiamondPickaxe\" task with a GPT-based planner that produces plans only at the beginning\nwithout looking at the current situation, the agent failed to complete the task as opposed to human\nplayers and JARVIS-1, which perform situation-aware planning from time to time. We\u2019ve observed\n3\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n\uff08a\uff09\n\uff08b\uff09\n\uff08c\uff09\nwood\nstone\niron\ndiamond\n60\nmin\n10\nmin\n60\nmin\n9.59x\n0\u00e08.99\niron\ndiamond\n3.39x\nFigure 2 | Challenges in open-world environments and how does JARVIS-1 tackle them.\n(Left)\nWith situation-aware planning,\nJARVIS-1 substantially improves the success rate on the challeng-\ning ObtainDiamond task, compared to the baseline (GPT) without it.\nNote: Due to resource con-\nstraints, we can only provide human results of 10-min gameplay; (Middle) As task complexity increases\n(STONE\u2192IRON\u2192DIAMOND), JARVIS-1 exhibits more significant advantages thanks to interactive planning;\n(Right) Success rate gain (indicated by the color depth) on selected tasks (x-axis) given in-context experiences\non other tasks (y-axis) retrieved from the multimodal memory. With life-long learning and memory, JARVIS-1\ncan utilize prior experiences on relevant tasks for better planning.\nthat many failures coming from this were attributed to the agent\u2019s inability to adapt to the changing\nsituations including entering a new biome, the tool being used becoming broken, etc.\n2.2. Challenge II: Task Complexity\nThe second challenge comes from the higher task complexity in open-world environments. Due\nto the richness of terrains, objects, and action space, tasks in open-world domains usually require\nsubstantially long planning horizons as well as good accuracy and precision. For example, the task\nObtainEnchantingTable\nincludes more than 20 different sub-goals and therefore demands\nsignificantly longer reasoning steps. Meanwhile, many of these sub-goals have to be achieved pre-\ncisely with the exact object name, quantities, and preconditions, e.g., mine 3 obsidian with\ndiamond pickaxe, craft 1 diamond pickaxe from 3 diamonds and 2 sticks; oth-\nerwise, the subsequent sub-goals won\u2019t be executed due to unfulfilled preconditions. To tackle this, we\nmay refer to some approaches in LLM reasoning, e.g. self-debugging (Chen et al., 2023) and turning\nthe planning into an interactive fashion. In Figure 2 (Middle), we\u2019ve shown that as the complexity of\nthe task increases, our JARVIS-1, which uses interactive planning (Wang et al., 2023b) to mitigate\nthe aforementioned issues (details can be found in subsection 3.2), elicits more significant advantages\nover the baseline (GPT) planner.\n2.3. Challenge III: Life-long Learning\nFinally, being open world often implies offering an infinite number of tasks. Clearly, it is difficult\nfor an agent to master all tasks or generalize to arbitrary tasks without additional learning. To this\nend, agents in an open world should be able to learn novel tasks while completing existing tasks,\ni.e. life-long learning. Furthermore, as many open-world agents employ large models (Wang et al.,\n2023a,b; Yuan et al., 2023; Zhu et al., 2023), canonical gradient-based learning could be extremely\ninefficient given the number of new tasks and experiences to learn. Our MLM-based JARVIS-1 tackles\nthis by adopting a memory to save all the experiences on past tasks. By retrieving memory entries\nrelevant to the newly-coming task and putting them into the context as a reference, JARVIS-1 is able\nto accumulate more experiences as the game continues and strengthen its own planning skills without\ngradient update. As illustrated in Figure 2 (Right), for instance, both ObtainDiamondPickaxe\n4\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nMulti-Modality \nMemory\nMemory-Augmented\nMulti-modal\nLanguage Model\nController\nEnvironment\n(a) JARVIS-1 architecture\n(b) Self-Improving\n<task> \nPool\nSelf-\ninstruct\nShared\nMulti-Modality \nMemory\nDistributed\nJARVIS-1\nEnv\nInstances\n<act>\nkeyboard\n& mouse\n<task>\n<obs>\n<plan>\nlanguage\nvision\nQuery Gen\n(MLM)\nreference\n<plan>\nPlanner\n(MLM)\n<plan>\ncontext\nretrieve\n<obs,task>\nvision & language\n<task>\nbatch\nsave\nFigure 3 | Architecture of JARVIS-1 and its self-improving mechanism. (a) JARVIS-1 comprises a memory-\naugmented multimodal language model (MLM) that produces plans and a low-level action controller. JARVIS-1\nalso utilizes a multimodal memory to store and obtain experiences as references for planning. (b) JARVIS-1\ncan strengthen its own planning skills through exploration with its own proposed tasks (self-instruct) and a\ngrowing memory that helps with better planning on tasks that has been (partially) visited before.\nand ObtainDiamondAxe\nrequire gathering almost identical materials. Therefore, they can help\neach other by using the experiences from the other task. Compared to completing these challenging\ntasks without any prior experiences, memory-based in-context life-long learning in JARVIS-1 can\nbring significant advantages.\n3. Multi-task Agent with Memory-Augmented MLM\nThis section details the architecture of the proposed JARVIS-1 agent. We begin with an overview of\nthe modular agent design in subsection 3.1. Next, we elaborate on how to implement an interactive\nplanning scheme with a multimodal language model, which helps with more accurate plans, especially\non complex and long-horizon tasks in subsection 3.2. Finally, we show how to augment this planning\nframework with a multimodal memory to allow JARVIS-1 to strengthen its planning skill throughout\nthe episode by in-context life-long learning in subsection 3.3 and subsection 3.4.\n3.1. Overview\nWe aim to develop an agent capable of solving long-horizon instruction-following tasks using image\nobservations and human-aligned actions. To accomplish this, we propose a multi-modal agent\nincluding an interactive planner, a goal-conditioned controller, and a multimodal memory of\nmultimodal experiences. Upon receiving a task and the current observation, JARVIS-1 first utilizes\nthe MLM to generate a multimodal query (query gen) that retrieves relevant planning experiences\nfrom the memory. These experiences will then be used along with the planning instruction to prompt\nthe MLM-based planner. Leveraging its own pretrained knowledge as well as the retrieved reference\nplans, the planner will ultimately produce a series of \ud835\udc3e short-horizon goals \ud835\udc541, . . . , \ud835\udc54\ud835\udc3e to be executed\nby the controller. Once the plan is successfully executed, it will be stored in the memory along with\nthe task and the agent situation when it was planned. We also empower JARVIS-1 with life-long\n5\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nPlanner\n(MLM)\nSelf-Check\n(MLM)\nSelf-Explain\n(MLM)\nrefined\n<plan>\n<obs>\nController\nEnvironment\n<act>\nmulti-modal\n<feedback>\noriginal\n<plan>\nerror\n<explanation>\n<obs,task>\n<task>: Obtain a diamond\nin Minecraft step-by-step?; <obs>: \noriginal <plan>:\n3\n3\n1\n1\n1\n3\n12\n1\n4\n1\n11\n1\nSelf-check: When simulating on the goal\n, I find\nare not enough (lack of 2\n). So\nI need craft more\nfrom\n. More \nrequire more \n. So I need to mine more\n.\nrefined <plan> :\n3\n3\n1\n1\n1\n4\n16\n1\n8\n1\n11\n1\nSelf-explain: Because mining\nneeds\n, which I do not have in the inventory.\nCrafting\nneeds\n. So I need to smelt\ninto\nfirst.\n3\n1\n1\nnew <plan> by re-planning:\nmulti-modal <feedback> : I failed on\n. My current state is:\nis broken; I still have\nin the inventory. My position is \u2026\n1\n1\n\u2026\nFigure 4 | Interactive planning in JARVIS-1. After receiving the current task instruction and observation,\nJARVIS-1 will produce an initial plan, which will go through self-check to get possible bugs (marked in red)\nfixed. Further, in case any error (also marked in red) occurs during the execution of the refined plan, JARVIS-1\nwill try to reason about the next move from the environmental feedback via self-explain. Interleaving self-check\nand self-explain significantly boosts the correctness and robustness of JARVIS-1 planning.\nlearning by combining self-instruct, where JARVIS-1 will propose some tasks for itself to complete\nas a means of exploration; and self-improve, where multiple JARVIS-1 agents will be running in\nparallel to gather experiences, therefore helping with better planning later. We provide an illustration\nin Figure 3.\n3.2. Interactive Planning with MLM\nAs we have mentioned in subsection 2.1 and subsection 2.2, the primary challenges for planning in\nMinecraft come from the requirement of being able to plan for long-horizon tasks under dynamic\nobservations. Confirmed by many prior arts (Wang et al., 2023a,b; Yuan et al., 2023), this makes it\nexceptionally hard to utilize canonical symbolic planners, which can be much less flexible. To this\nend, we take a multimodal language model (MLM) as zero-shot planner and combine it with an\ninteractive planning framework to tackle these challenges.\nSituation-aware planning with MLM. To achieve situation-aware planning, the planner must\ntake the current observation into account, in addition to the task instruction (Huang et al., 2022a; Yao\net al., 2022). Specifically, we begin with translating the multimodal observation into text descriptions.\nAs opposed to letting the MLM caption the scene directly, we first extract keywords of Minecraft\nitems (e.g., \"acacia tree\", \"sheep\") from Minecraft wiki and utilizing GPT (Brown et al., 2020)\nto generate sentences that describe these observations. For example, a generated sentence could be \"I\ncan see sheep in the acacia plains\". Then the MLM will retrieve the condition sentence according\nto current visual observation during planning. Additional situation details including biome and\ninventory status are also converted into text using templates. Finally, we prompt the MLM again (the\nlanguage part only) into a plan given the task instruction and all the aforementioned textual situation\ndescriptions. Compared to end-to-end alternatives (Brohan et al., 2023; Huang et al., 2023), we find\nour composable usage of MLM provides higher quality situation descriptions and ultimately, plans\nwith much less hallucination.\nPlanning with self-check. Our first layer of shield to ensure the correctness of plans involves\nself-check. Similar to self-debugging(Chen et al., 2023), given an initial plan, we ask JARVIS-1 to\n6\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nprogressively simulate the plan execution, predict the resulting state after each step (primarily the\nstate of inventory), and evaluate them. By verifying if these states satisfy the goal\u2019s precondition,\nJARVIS-1 can proactively identify potential plan flaws. Compared to the canonical planner where\nthe agent has to encounter the error first before making a remedy, this upfront plan verification\ncould mitigate the need for the agent to recover (re-plan) from more challenging situations due to\nplan failure. For instance, if an agent starts digging underground without sufficient wood, it would\ntypically have to return to the surface, which substantially lowers the chance of completing the task.\nPlanning with environment feedback. Next, our interactive planning framework ventures\ninto allowing JARVIS-1 to quickly recover from failure by leveraging environment feedback in\na closed-loop fashion. The process is illustrated in Figure 4. During plan execution, we feed the\nfeedback to the MLM of JARVIS-1 in case there is any execution failure (possibly due to a flawed\nplan) and utilize its self-explain mechanism (Shinn et al., 2023) to explain the error and locate the\nbugs in the original plan (we term this as error explanation). Finally, the MLM planner of JARVIS-1\nwill produce an improved plan based on both the outside environment feedback and the inside\nretrospective. Compared to other agents that rely on human intervention or privileged environment\ninformation (Huang et al., 2022b; Zhu et al., 2023), JARVIS-1 has the ability to speculate about the\nreasons why current goals cannot be achieved, without the need for additional information or design.\n3.3. Planning with Multimodal Memory in the Loop\nTo address the life-long learning challenge mentioned in subsection 2.3, we equip JARVIS-1 with\nmultimodal memory to allow learning from its own past experiences. We will detail the formulation\nof the retrieval-augmented planning, query generation, and memory layout below.\nRetrival-augmented planning. Retrieval-augmented generation (RAG) (Lewis et al., 2020; Mao\net al., 2020) enhances the quality of responses generated by LLMs by incorporating external sources\nof knowledge to complement the model\u2019s internal representation. We also utilize RAG to enhance\nJARVIS-1\u2019s long-term planning capability. Compared to official RAG methods leveraging the external\nknowledge library, we take the collected multimodal memory as the knowledge library and retrieve the\ninteractive experiences as the demonstration prompt to augment the planning results. The formulation\nis as follows:\n\ud835\udc5d(\ud835\udc66 | \ud835\udc65) \u2248\n\u2211\ufe01\n\ud835\udc67\u2208top-k(\ud835\udc5d(\u00b7|\ud835\udc65))\n\ud835\udc5d\ud835\udf02(\ud835\udc67 | \ud835\udc65)\ud835\udc5d\ud835\udf03(\ud835\udc66 | \ud835\udc65, \ud835\udc67),\n(1)\nwhere \ud835\udc65, \ud835\udc66, and \ud835\udc67 denote instruction, plans, and retrieved memory entries respectively, and \ud835\udc5d\ud835\udf02 and\n\ud835\udc5d\ud835\udf03 are denoted as retrieval and planning models. Such retrieval-augmented planning method helps\nJARVIS-1 ground the internal knowledge into the open-ended environments efficiently and leverage\nthe historical interaction feedback to solve the hallucination within LLMs and produce more accurate\nplans.\nMultimodal memory. We have demonstrated the layout of our multimodal memory on the\nright side of Figure 5. From a high level, it is a key-value memory where the keys are multimodal,\ncomprising both the task and the observation (or situation) made when this memory entry was created.\nThe values are the plans that were successfully executed. Note that, since the plans in an open-world\nenvironment like Minecraft are situated (see subsection 2.1), there could be multiple entries that are\nwith the same task but different observations and plans. As a result, JARVIS-1 needs to produce\nmultimodal queries based on the current task and situations to retrieve the relevant memory entries.\nQuery generation via reasoning.\nWhen presented with an instruction as a task, we employ\nquery generation via LLM reasoning to decompose the instruction into sub-tasks or related tasks,\n7\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nUser: My current task is\n, but I have never accomplished this task \nbefore. What related tasks might be helpful for me to complete \n?\nAssistant:\nreasoning stops\nwooden pickaxe\n3\n12\n1\n4\n1\nstone pickaxe\n\u2026\n1\n1\n3\n1\nMulti-Modal Memory\ninitial query (text)\nEnchanting\nTable\nObsidian\nDiamond\nBook\nDiamond Pickaxe\nLeather\nPaper\nDiamond\nIron Pickaxe\nnot in memory\nin memory\nreasoning\nquery generation via reasoning\nDiamond axe\nfinal query (obs):\nfinal query (text):\nDiamond\nLeather\nPaper\nIron Pickaxe\nQuery generation via reasoning\nQuery\n+\nquery gen\nretrieve\nFigure 5 | Query generation in JARVIS-1. Given the current observation and the task, JARVIS-1 will first\nthink backward and figure out the needed intermediate sub-goals. The reasoning will be bounded by a limited\ndepth. The sub-goal that is present in the memory will join the current visual observation to form the final\nquery. Entries that match the text query will be ranked by the perceiving distance of their states to the obs\nquery and only the top entry of each sub-goal will be retrieved.\nwhich will then be used as textual queries to retrieve relevant planning experiences as references for\nsolving the current task. For instance, consider the instruction \"craft 1 enchanting table with empty\ninventory\" as shown in Figure 5. JARVIS-1 queries the MLMs to identify the tasks that are required\nfor achieving the main task in a backward search fashion, e.g., \u201cobtain book\n/diamond\n/obsidian\nwith empty inventory\u201d. The search depth is bounded for efficiency. Further, instead of relying\nsolely on retrieval based on the text query (Wang et al., 2023a; Zhu et al., 2023), we also propose to\nappend the agent\u2019s current visual observation to the textual query, resulting in a multimodal query to\ntake the situation into account during memory retrieval.\nMultimodal retrieval. After obtaining the textual and visual query, we compute the alignment\nbetween the query and each trajectory in multimodal memory. We first use the text encoder of the\nCLIP model to compute the embedding of the query and task key of each entry in memory. We select\nthe memory entries with similarity higher than the confidence threshold as the candidate entries.\nThen we will compute the visual state embedding of query and states in candidate entires. Then we\nsort the candidate entries with the visual embedding similarities, which can be formed as:\n\ud835\udc5d\ud835\udf02(\ud835\udc67 | \ud835\udc65) \u221dCLIP\ud835\udc63(\ud835\udc60\ud835\udc67)\u22a4CLIP\ud835\udc63(\ud835\udc60\ud835\udc65),\n(2)\nwhere \ud835\udc60\ud835\udc67 and \ud835\udc60\ud835\udc65 are the visual key of memory entries and visual query, respectively. Finally, we retrieve\nthe plan of top-k candidate entries as reference prompt \ud835\udc67.\n3.4. Self-improving Agents\nLearning in Minecraft with memory. The remaining issue now is where the aforementioned\nmultimodal memory comes from. Inspired by the life-long learning scheme in many close-world and\nopen-world reinforcement learning problems (Abel et al., 2018a,b; Wang et al., 2023a), we propose\nthe following learning approach for augmenting the memory in JARVIS-1: 1) First, we generate a set\nof tasks, which form some curricula for the agents to complete as means of exploration of the world.\nDuring this process, JARVIS-1 produces plans, interacts with the environment, embraces the errors,\nand stores all these experiences in the memory; 2) After this learning stage, we evaluate JARVIS-1\n8\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\non various tasks. Therefore, JARVIS-1 is able to produce better plans with the memory teaming up\nwith the planning experiences. In our experiments, we use this as the default setting for all tasks.\nExploration using self-instruct. The key issue to the success of learning with memory is how\nto effectively acquire useful experiences given a limited amount of time. We propose to use self-\ninstruct (Wang et al., 2022) to generate the dynamic curriculum and guide JARVIS-1 to learn\nfrom the interactions with environments. In each round, we prompt the MLM to consider how\ncapable JARVIS-1 is at this point and subsequently select tasks from a task pool to explore. We find\nthat the curriculum almost follows the technical tree-growing direction. To accelerate the learning\nprocess, we augment the linear self-instruct to distributed learning in distributed environments with\nshared memory, i.e. speculative execution (Leviathan et al., 2023). Specifically, we generate multiple\nexecutable tasks as candidate task batches and provide them to agents with the same memory for\nverification and execution in various different environments. Meanwhile, experiences are collected\ninto a shared centralized memory. When all exploration tasks have been accomplished, we move to\nthe next round, until the memory reaches a certain capacity.\nLife-long learning. We\u2019ve also observed that the aforementioned learning (where the memory is\nbeing filled) can be extended throughout the whole gameplay, where the agent gradually acquires\nmore and more skills. As the gameplay continues, more and more experiences are pouring in, therefore\nJARVIS-1 can find better references for challenging tasks like ObtainDiamondPickaxe, resulting\nin an improved success rate on these tasks. Further, there is no gradient update in this thanks to\nthe memory-augmented MLM, i.e. we can do in-context life-long learning. In Section 4.3, we offer\nexploratory experiments to show the potential of such capability of JARVIS-1.\n4. Experiments\nIn the experiments, our goal is to 1) evaluate the general performances of JARVIS-1 on the chal-\nlenging Minecraft tasks, especially on its advantages over baselines that do not (fully) address the\naforementioned issues in open-world agents; 2) understand the factors that contributes to the general\nresults; 3) explore the potential of JARVIS-1 in terms of life-long learning and its benefits to long-\nhorizon tasks. To this end, we will first briefly introduce the evaluation settings, then cover the main\ncomparative results and ablation studies, and conclude with an exploratory trial on long-horizon\ntasks.\n4.1. Experimental Setups\nWe evaluate JARVIS-1 in Minecraft, with tasks selected from the recently introduced Minecraft\nUniverse Benchmark (Lin et al., 2023a). For the reader\u2019s convenience, we provide details on the basic\nsetups below.\nEnvironment setting. To ensure realistic gameplay, the agent needs to utilize observation\nand action spaces that are similar to those used by humans. Instead of manually designing a\ncustom interface for models to interact with the environment, as done in previous methods such as\nMineDojo(Fan et al., 2022), GITM(Zhu et al., 2023), and Voyager(Wang et al., 2023a), we opt for\nusing the native human interface provided by Minecraft. This applies to both the observation and\naction space. The model operates at a speed of 20 frames per second and is required to use a mouse\nand keyboard interface when interacting with human GUIs. For more information on the detailed\ndescriptions of the observation and action spaces, please refer to the Appendix.\nTask setting.\nIn Minecraft, players have access to thousands of items, each with specific\nacquisition requirements or recipes. For example, stone-type items can only be obtained using a\n9\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 1 | Characteristics of 11 task groups encompassing over 200 minecraft tasks.\nGroup\nTask\nNum.\nMax.\nSteps\nInitial\nInventory\nBiome\nLanguage Instruction\nWood\n34\n12k\nnull\nPlains/Forest\nPick up a wooden_pickaxe.\nWood-Variants\n43\n12k\nnull\nSavanna/Jungle/Taiga\nPick up a acacia_boat.\nStone\n10\n12k\niron_axe\nPlains/Forest\nCraft a furnace given an iron axe.\nIron\n22\n12k\niron_axe\nPlains/Forest\nSmelt and craft an iron_door given an iron axe.\nGold\n9\n36k\niron_axe\nPlains/Forest\nSmelt and craft an golden_axe given an iron axe.\nDiamond\n7\n36k\niron_axe\nPlains/Forest\nDig down to mine diamond and craft diamond_pickaxe.\nRedstone\n7\n36k\niron_axe\nPlains/Forest\nMine redstone and make dropper given an iron axe.\nBlocks\n15\n12-36k\niron_axe\nPlains/Forest\nDig down to mine lapis_lazuli block.\nArmor\n17\n12-36k\niron_axe\nPlains/Forest\nCraft diamond_boots given an iron axe and equip it.\nDecoration\n17\n12k\niron_axe\nFlower Forest\nObtain the bed and dye it red.\nFood\n9\n12k\niron_axe\nPlains\nKill sheep to obtain mutton and cook it.\npickaxe, and two planks can be crafted into four sticks (these requirements are available on the\nMinecraft Wiki1). In survival mode, players must obtain each type of item from the environment or\ncraft/smelt the object item from materials. We choose over 200 tasks from the Minecraft Universe\nBenchmark (Lin et al., 2023a) for evaluation. These tasks are related to items that can be obtained in\nthe Minecraft overworld. For the convenience of statistics, we have classified them into 11 groups\naccording to recommended categories in Minecraft2 (see Table1). Due to the varying complexity of\nthese tasks, we adopt different maximum gameplay durations (Max. Steps) for each task. The limit is\ndetermined by the average time the human players need to accomplish the corresponding task. Other\ndetails about each task, such as language instruction, maximum steps, evaluation times, biome, and\ninitial inventory when the agent is born into the world can be found in Appendix Table 5-14.\nEvaluation metrics. By default, the agent always starts in survival mode, with an empty inventory.\nA task is considered a success when the target object is obtained within a specified time. Due to the\nopen-world nature of Minecraft, the world and initial position that the agent is spawned at could\nvary a lot. Therefore, we conducted at least 30 tests for each task using different seeds and reported\nthe average success rate to ensure a thorough assessment. Further, since we categorize the tasks into\ngroups, we also report mean and variance values for each group for ease of presentation.\n4.2. Main Results\nWe compare JARVIS-1 with other multi-task instruction-following agents based on LLM, including\nInstruct GPT (Huang et al., 2022a; Ouyang et al., 2022), ReAct (Yao et al., 2022), Inner Mono-\nlogue (Huang et al., 2022b), DEPS (Wang et al., 2023b). Since some methods are not originally\nexperimented in Minecraft, we reproduce them to conform to the Minecraft specification based on\nprompt and feedback template design. All LLM-based methods access the LLM model through OpenAI\nAPI. And all hyper-parameters of LLM including temperature are kept as default.\nThe average success rates for every task group are listed in Table 2. JARVIS-1 achieves the\nbest performance with all meta tasks. It is important to note that in Minecraft, the technology tree\ncan be formed by Group Wood, Stone, Iron, Gold, and Diamond. The tasks become increasingly\ndifficult as you progress through the tree. For more difficult tasks such as obtaining a gold ingot or\na diamond, the agents typically need to perform more actions and longer goal sequences in order\nto complete the task. As a result, the success rate of all agents decreases as the difficulty level\nincreases. It is evident that reasoning methods (ReAct (Yao et al., 2022) vs. GPT (Huang et al.,\n2022a; Ouyang et al., 2022)) and interactive re-planning with feedback (Inner Monologue(Huang\net al., 2022b) vs. GPT) effectively enhance the agent\u2019s task performance in an open world. However,\n1https://minecraft.fandom.com/wiki/Minecraft_Wiki\n2https://minecraft.fandom.com/wiki/Tutorials/Organization#Categories\n10\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 2 | Results of JARVIS-1 and baselines on Minecraft. The detailed task instructions, settings and results\ncan be found in the Appendix.\nGroup\nTask\nGPT\nReAct\nInner Monologue\nDEPS\nJARVIS-1\n26.67\n45.00\n36.67\n75.00\n91.55\nWood\nAVG\n27.30\u00b114.86\n40.31\u00b113.30\n60.15\u00b119.41\n80.23\u00b117.32\n88.84\u00b116.82\n6.67\n36.67\n30.00\n36.67\n60.47\nWood\nVar\nAVG\n24.39\u00b111.08\n38.13\u00b112.81\n53.39\u00b112.86\n68.75\u00b112.32\n76.78\u00b112.27\n20.00\n20.00\n66.67\n75.00\n94.20\nStone\nAVG\n20.21\u00b112.32\n39.00\u00b112.15\n52.86\u00b116.90\n69.27\u00b17.78\n88.69\u00b14.87\n0.00\n0.00\n3.33\n20.00\n33.82\n3.33\n6.67\n0.00\n20.00\n38.10\nIron\nAVG\n3.27\u00b12.85\n4.61\u00b13.63\n5.20\u00b15.17\n16.92\u00b14.69\n34.63\u00b110.61\n0.00\n2.00\n2.00\n6.00\n14.49\nGold\nAVG\n0.00\u00b10.00\n0.45\u00b10.60\n0.59\u00b10.64\n2.20\u00b11.55\n6.85\u00b14.71\n0.00\n0.00\n1.00\n2.00\n9.20\n0.00\n0.00\n0.00\n2.50\n6.22\nDiamond\nAVG\n0.00\u00b10.00\n0.35\u00b10.48\n0.96\u00b10.67\n2.42\u00b11.01\n8.99\u00b12.68\n0.00\n2.00\n0.00\n10.00\n22.78\nRedstone\nAVG\n1.04\u00b11.30\n1.14\u00b11.18\n0.69\u00b11.68\n6.02\u00b13.61\n17.51\u00b19.34\n16.67\n33.33\n43.33\n53.33\n86.67\nBlocks\nAVG\n45.64\u00b133.88\n49.35\u00b130.51\n55.71\u00b129.43\n58.02\u00b127.68\n80.34\u00b121.09\n6.67\n0.00\n10.00\n10.00\n30.30\nArmor\nAVG\n1.36\u00b12.25\n0.50\u00b10.88\n3.10\u00b14.71\n3.71\u00b13.78\n13.44\u00b114.62\n15.00\n15.00\n15.00\n25.00\n50.00\nDecoration\nAVG\n17.12\u00b111.59\n17.13\u00b19.19\n12.03\u00b110.19\n29.59\u00b115.94\n46.67\u00b123.39\n13.33\n16.67\n25.00\n16.67\n43.55\nFood\nAVG\n9.40\u00b14.29\n15.56\u00b16.83\n20.78\u00b111.99\n22.85\u00b18.15\n46.75\u00b111.16\nthese approaches still face challenges when dealing with long-horizon tasks, specifically in the Iron\nand Diamond group. DEPS(Wang et al., 2023b), on the other hand, enables agents to accomplish\ndiamond-related tasks through interactive long-horizon planning accompanied by descriptions and\nexplanations. Nevertheless, its reliability remains very low at approximately 2.5%.\nIn comparison to DEPS(Wang et al., 2023b) without memory, JARVIS-1 demonstrates superior\nperformance even in challenging tasks due to its extensive experience. In diamond-related tasks\nspecifically, the success rate has increased by nearly 3 times (8.99% vs 2.42%). And JARVIS-1\nusually only requires 2-3 rounds of re-planning to generate the correct executable plan, whereas\nDEPS requires more than 6 rounds. This means that JARVIS-1 saves a significant amount of LLM\ntokens and thinking time, enabling more efficient plan execution and providing additional steps and\ntokens for handling uncertainty in the environment.\nBased on our observations, we have found that the bottleneck for JARVIS-1 in tasks involving\ndiamonds often lies with the Controller\u2019s inability to perfectly execute short-horizon text instructions\ngenerated by LLM. Therefore, it is worth exploring methods for generating plans that are easier for\nthe controller to execute or improving the controller\u2019s ability to follow instructions.\n11\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n97%\n96%\n94%\n34%\n9%\n95%\n95%\n90%\n30%\n5%\n55%\n50%\n35%\n5%\n0\n85%\n85%\n75%\n25%\n5%\n0%\n20%\n40%\n60%\n80%\n100%\nCrafting Table\nWooden Pickaxe\nStone Pickaxe\nIron Pickaxe\nDiamond\nGPT4\nChatGPT\nLLaMA2 Pre-Trained\nLLaMA2 Fine-tuned\nFigure 6 | Success rates for different language models on Minecraft tasks. We found open-sourced LLaMA2-\n70B modelsTouvron et al. (2023) lack knowledge related to Minecraft, so the pre-trained model performs\npoorly. We further finetuned the LLaMA2-13B model on a Minecraft text dataset collected from the internet,\nand it shows performance similar to ChatGPT on Minecraft.\n4.2.1. JARVIS-1 based on different LMs\nWe conducted ablation experiments on various Language Models, including OpenAI\u2019s ChatGPT Ouyang\net al. (2022) and GPT-4 OpenAI (2023). Among these models, GPT-4 has more parameters and has\nbeen proven to outperform ChatGPT in extensive research Wang et al. (2023a). We also select the\nopen-source pre-trained LLaMA2 70B model Touvron et al. (2023). Additionally, we gathered a\nsubstantial amount of Minecraft-related text from the internet as training data and further fine-tuned\nLLaMA2 13B. The experiments were conducted on a subset of Minecraft tasks using different language\nmodels. Each JARVIS-1 learns for 4 epochs of interaction with all task sets and evaluates on task\nsubset across at least 20 seeds. The experimental results are presented in Fig. 6.\nTable 6 demonstrates that ChatGPT, despite having fewer parameters, achieves nearly identical\nsuccess rates as GPT-4. This suggests that language models equipped with memory can significantly\nenhance planning abilities. In Minecraft-related tasks, the open-source pre-trained LLaMA2 70B\nexhibits a notable performance gap compared to OpenAI models, particularly in long-horizon tasks.\nHowever, by finetuning LLaMA2 with fewer parameters, its performance on Minecraft tasks improves\nsubstantially. This indicates that the open-source model lacks knowledge specific to Minecraft and\nrequires further finetuning for the successful completion of such tasks.\n4.2.2. Ablation on Memory\nWe also conduct ablation experiments on the multimodality memory and retrieval methods. We set\nJARVIS-1 w/o memory module as the baseline agent. We first evaluate JARVIS-1\u2019s performance\nwith different memory sizes (representing different learning stages) as shown in Fig. 7, which demon-\n12\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nFigure 7 | Success rate by memory size for different items. We evaluated the performance of JARVIS-1 at\ndifferent memory sizes (representing different learning stages) by measuring the success rate (% Episodes) of\ncompleting key items on the Minecraft technology tree. As the learning progressed, we observed an improvement\nin completion rates for all items, with an increasing number of successful trajectories being included in memory.\nAfter 4 epochs of learning, JARVIS-1 had accumulated a total of 425 successful trajectories in its memory.\n85%\n20%\n10%\n0%\n10%\n95%\n20%\n30%\n5%\n20%\n94%\n34%\n40%\n9%\n24%\n0%\n20%\n40%\n60%\n80%\n100%\nStone Pickaxe\nIron Pickaxe\nShield\nDiamond\nRedstone Block\nText Memory\nText Memory + Reasoning\nMultimodal Memory + Reasoning               baseline (no memory)\nFigure 8 | Success rates for different retrieval methods with memory on Minecraft tasks. JARVIS-1, which\nsynergizes reasoning and retrieval with multimodal memory, achieves the best.\nstrates the effectiveness of self-improving within JARVIS-1. We further conduct the experiments on\na subset of Minecraft tasks using three different retrieval methods: retrieval with textual instruction\nembedding only (Text Memory), synergizing reasoning and retrieval with text embedding (Text Mem-\nory+Reasoning), and synergizing reasoning and retrieval with multimodality embedding (Multimodal\nMemory+Reasoning). Except for the memory and retrieval methods, all others are kept the same.\nThe results are listed in Fig. 8.\nThe experiments show that reasoning before retrieval can effectively improve retrieval accuracy.\nRetrieval based on a multimodal state including vision observation and symbolic information (e.g.,\ninventory, location, etc) is better than only considering the text embedding.\n4.3. Long-Horizon Challenges\nMost concurrent multi-task agents in Minecraft can only handle short-term tasks and struggle with long-\nhorizon tasks like CraftingDiamondPickaxe. The VPT foundation model(Baker et al., 2022) is\ncapable of accomplishing various tasks in Minecraft but lacks the ability to execute human instructions.\nTo address this limitation, Reinforcement Learning is required to fine-tune the VPT foundation model\nfor specific task completion. However, after fine-tuning, VPT may experience a decline in performance\n13\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n12.5%\n7.2*104\n7.2*104\nFigure 9 | (Left) The success rate of different models in the ObtainDiamondPickaxe challenge over gameplay\ntime. VPT RL is finetuned from VPT early game with reinforcement learning over 1.4 million episodes. JARVIS-\n1 agent and its varients have interacted with Minecraft with over 4 epochs on all tasks in task pool. Typically, it\ntakes a skilled person over 20 minutes (24,000 steps) to obtain a diamond pickaxe. (Right) The success rate of\nobtaining important intermediate items during the process of synthesizing a diamond pickaxe of JARVIS-1.\nThis task has been evaluated over 300 times on different seeds. These curves indicate that as the game\nprogresses, the success rates of obtaining all intermediate items are increasing, which indicates that JARVIS-1\nis constantly improving its skills.\nfor other tasks while focusing on the specified task. In contrast, Steve-1(Lifshitz et al., 2023) has\nimplemented goal-conditioned fine-tuning on VPT, enabling it to follow human text instructions while\nmaintaining multitasking capabilities. However, Steve-1 primarily focuses on low-level tasks like\nobtaining dirt, collecting flowers, and chopping trees. When it comes to long-horizon tasks such as\nstarting from scratch by obtaining a wooden pickaxe, Steve-1 still encounters difficulties.\nDEPS(Wang et al., 2023b) also utilizes LLM as a planner, but it lacks the ability to learn from\nexperience in different tasks and apply that knowledge to new ones. Additionally, DEPS is limited in\nits re-planning rounds due to the LM\u2019s context constraints. The experiments reveal that DEPS has a\nsuccess rate of less than 50% in generating accurate and executable plans for acquiring diamonds.\nThe probability of DEPS successfully obtaining diamonds in the environment is approximately 0.59%.\nConsequently, DEPS continues to face challenges when attempting to finish long-horizon tasks within\nthe Minecraft world.\nEven human players who have mastered the distribution pattern of diamonds achieve success\nrates of obtaining diamonds and crafting a diamond pickaxe (which requires at least three diamonds)\nwithin 10 minutes at approximately 15% and 12%, respectively. JARVIS-1 performs better in the\nObtainDiamondPickaxe challenge. Compared to the state-of-the-art model, which has undergone\nRL-finetuned VPT, JARVIS-1 has more than doubled the success rate of obtaining a diamond pickaxe\n(6.22% vs 2.5% within 20 minutes).\nTo increase the chances of obtaining diamonds, we extended the game-playing time to 60 minutes\n(72000 game-playing steps, as shown in Figure 9). As a result, JARVIS-1\u2019s success rate in acquiring\na diamond pickaxe improved from 6.2% to 12.5%. The graph on the right side of Figure 7 illustrates\nhow the success rate of intermediate milestone items changes over time, indicating that JARVIS-1\ntends to improve with longer game-playing time. We also conduct two variants of JARVIS-1 with\ndifferent self-improving curricula: human-written and random-generated. All three JARVIS-1 have\ncollected experiences into memory with the curriculum for 4 epochs before evaluation in 60 minutes.\nThe results show that JARVIS-1 with a GPT-generated curriculum can finish the task within the\nshortest game-playing steps and achieve the best performance in 60 minutes.\nIn contrast, VPT\u2019s success rate barely changed when we increased the time from 20 minutes\nto 60 minutes (from 2.5% to 3%). This can be attributed to Minecraft\u2019s durability system where\n14\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nprolonged underground exploration often leads to pickaxe damage. When JARVIS-1\u2019s pickaxe\nbreaks, it dynamically re-plans based on its current inventory and crafts a new one. However, VPT-RL\nexhibits perplexing behaviors at this stage by using inappropriate tools for mining stones or crafting\nunnecessary items. This comparison demonstrates that JARVIS-1 possesses superior generalization\nand planning abilities for long-horizon tasks.\nNote that our method is designed to be multi-task in its nature and not finetuned through imitation\nlearning on specific datasets or reinforcement learning.\n5. Related Works\n5.1. Planning with LLM\nThere have been some methods leveraging the large language model to generate action plans for\nhigh-level tasks in embodied environments (Dasgupta et al., 2022; Gong et al., 2023b; Liu et al., 2023;\nMai et al., 2023; Zeng et al., 2022; Zhang et al., 2023; Zhang and Lu, 2023). Huang et al. (2022a)\ndecompose natural language commands into sequences of executable actions by text completion\nand semantic translation, while SayCan generates feasible plans for robots by jointly decoding an\nLLM weighted by skill affordances from value functions (Brohan et al., 2022b). Some methods\nalso leverage the LLM to produce the program code as plan for better executation (Liang et al.,\n2022; Lin et al., 2023b; Singh et al., 2022). However, the above methods assume that the initial\nplan from the LLM is correct. When there are bugs in the initial plan, it\u2019s difficult for the agent\nto finish the task successfully. Recent research frequently employs LLM as an interactive planner,\nharnessing its self-updating capabilities to enhance the plan\u2019s executability over time (Shinn et al.,\n2023; Sun et al., 2023; Wang et al., 2023b). Inner Monologue (Huang et al., 2022b) pilots the front\nof interactive planning with LLMs, which introduces the feedback (including success detection and\nscene description) to the planner. However, we found it could still suffer from accumulative planning\nerrors, especially in long-horizon open-world tasks. ReAct (Yao et al., 2022) will reason about the\nagent state before acting, which indicates that various reasoning methods (Wei et al., 2022; Wu et al.,\n2023; Yao et al., 2023) are benefitial for planning. LLM-based planning methods often use the fixed\npretrained LLM as the agent, while we focus more on life-long and continual learning for agents in\nopen-world environments (Ke et al., 2022a,b; Wang et al., 2023a). For better leveraging historical\ninteraction between agent and environments, an explicit memory (Park et al., 2023; Zhu et al., 2023)\nfor more historical chatting has been leveraged for bigger storage of agent experiences. However,\nthe above methods usually rely only on a text-based environment and struggle to execute plans in\npartial-observed visual open-world environments.\n5.2. Minecraft Agents\nDeveloping generally capable agents in Minecraft to solve open-world tasks has gained increasing\ninterests (Baker et al., 2022; Cai et al., 2023a,b; Ding et al., 2023; Fan et al., 2022; Yuan et al.,\n2023; Zhang and Lu, 2023; Zhu et al., 2023). As an early attempt, Oh et al. (2017) studied task\ngeneralization in a simple Minecraft environment variant. It designed a two-stage pipeline, first\nmastering the prerequisite skills with parameterization trick, and then learning a meta controller\nto execute the instructions. Moving to solve complex long-horizon tasks in Minecraft, works (Lin\net al., 2021; Mao et al., 2022; Oh et al., 2017) explored the hierarchical architecture. In recent years,\ninfluenced by the trend of large-scale pre-training paradigms, a group of researchers have emerged,\nwho are utilizing vast amounts of internet knowledge to train intelligent agents. Fan et al. (2022)\ntrained a visual-semantic alignment model, MineCLIP, using the correspondences between subtitles\nand video snippets available on YouTube, and used it to generate intrinsic rewards to guide policy\n15\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nlearning. (Baker et al., 2022) utilizes a pre-trained inverse dynamics model to label actions in YouTube\nvideos which are used to learn a foundation policy VPT through imitation learning. By bridging\nMineCLIP and VPT, Lifshitz et al. (2023) creates a performant instruction-following policy Steve-1 to\nsolve open-world short-horizon tasks using hindsight relabeling and unCLIP tricks. However, Steve-1\ncan not solve complicated process-oriented tasks due to the expressive capability of its goal space.\nCai et al. (2023b) learns to follow reference videos as the instruction by merely watching gameplay\nvideos, which improves the capacity of goal space and reduces the cost of policy training. All of\nthese methods focus on improving the smoothness and robustness of interaction between policy\nand environment. Inspired by the powerful language understanding and reasoning capabilities of\nlarge language models, researchers have begun to build Minecraft agents based on LLMs. Wang\net al. (2023a) used LLM to guide the agent to explore the Minecraft world by acquiring diverse skills,\nmaking novel discoveries, and generating goal proposals. Zhu et al. (2023) integrated LLM with\ntext-based knowledge and memory to equip the agent with common sense and past experiences for\nhigher reasoning efficiency. Yuan et al. (2023) used LLM to guide the agent to explore the Minecraft\nworld and interact with the environment with reinforcement learning control policies.\n6. Conclusion\nWe propose a multi-task agent JARVIS-1 designed for the complex environment of Minecraft, which\nmarks a significant advancement in achieving human-like planning within an open-world setting.\nBy leveraging pre-trained Multi-modal Language Models, JARVIS-1 not only effectively interprets\nmultimodal inputs but also adeptly translates them into actions. Its integration of a multimodal\nmemory, which draws from both ingrained knowledge and real-time game experiences, enhances\nits decision-making capabilities. The empirical evidence of its prowess is evident in its impressive\nperformance across a wide array of tasks in Minecraft. Notably, its achievement in the long-horizon\ndiamond pickaxe task, where it achieved a completion rate that surpasses VPT by up to five times,\nunderscores its potential and the strides made in this domain. This breakthrough sets the stage for\nthe future of more versatile and adaptable agents in complex virtual environments.\nAcknowledgments\nThis work is funded in part by the National Key R&D Program of China #2022ZD0160301, a grant\nfrom CCF-Tencent Rhino-Bird Open Research Fund, NSF grants #IIS-1943641, #IIS-1956441, #CCF-\n1837129, an SRA from Meta and a research gift from Amazon Alexa AI, and a gift from RelationalAI.\nThe authors sincerely thank Dr. Rita Zhang, Zhixiang Dai at NVIDIA for the valuable technical support\nof GPU computing.\nReferences\nD. Abel, D. Arumugam, L. Lehnert, and M. Littman. State abstractions for lifelong reinforcement\nlearning. In International Conference on Machine Learning, pages 10\u201319. PMLR, 2018a.\nD. Abel, Y. Jinnai, S. Y. Guo, G. Konidaris, and M. Littman. Policy and value transfer in lifelong\nreinforcement learning. In International Conference on Machine Learning, pages 20\u201329. PMLR,\n2018b.\nJ.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint\narXiv:2204.14198, 2022.\n16\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nB. Baker, I. Akkaya, P. Zhokhov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro, and\nJ. Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. arXiv\npreprint arXiv:2206.11795, 2022.\nA. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman,\nA. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint\narXiv:2212.06817, 2022a.\nA. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz, A. Irpan, E. Jang, R. Julian,\net al. Do as i can, not as i say: Grounding language in robotic affordances. In 6th Annual Conference\non Robot Learning, 2022b.\nA. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey,\nC. Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv\npreprint arXiv:2307.15818, 2023.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information\nprocessing systems, 33:1877\u20131901, 2020.\nS. Cai, Z. Wang, X. Ma, A. Liu, and Y. Liang. Open-world multi-task control through goal-aware\nrepresentation learning and adaptive horizon prediction. arXiv preprint arXiv:2301.10034, 2023a.\nS. Cai, B. Zhang, Z. Wang, X. Ma, A. Liu, and Y. Liang. Groot: Learning to follow instructions by\nwatching gameplay videos. arXiv preprint arXiv:2310.08235, 2023b.\nX. Chen, M. Lin, N. Sch\u00e4rli, and D. Zhou. Teaching large language models to self-debug. arXiv preprint\narXiv:2304.05128, 2023.\nI. Dasgupta, C. Kaeser-Chen, K. Marino, A. Ahuja, S. Babayan, F. Hill, and R. Fergus. Collaborating\nwith language models for embodied reasoning. In NeurIPS Foundation Models for Decision Making\nWorkshop, 2022.\nZ. Ding, H. Luo, K. Li, J. Yue, T. Huang, and Z. Lu. Clip4mc: An rl-friendly vision-language model for\nminecraft. arXiv preprint arXiv:2303.10571, 2023.\nL. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu, and\nA. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge.\nAdvances in Neural Information Processing Systems Datasets and Benchmarks, 2022.\nR. Gong, Q. Huang, X. Ma, H. Vo, Z. Durante, Y. Noda, Z. Zheng, S.-C. Zhu, D. Terzopoulos, L. Fei-Fei,\net al. Mindagent: Emergent gaming interaction. arXiv preprint arXiv:2309.09971, 2023a.\nR. Gong, Q. Huang, X. Ma, H. Vo, Z. Durante, Y. Noda, Z. Zheng, S.-C. Zhu, D. Terzopoulos, L. Fei-Fei,\net al. Mindagent: Emergent gaming interaction. arXiv preprint arXiv:2309.09971, 2023b.\nW. H. Guss, C. Codel, K. Hofmann, B. Houghton, N. Kuno, S. Milani, S. Mohanty, D. P. Liebana,\nR. Salakhutdinov, N. Topin, et al. Neurips 2019 competition: the minerl competition on sample\nefficient reinforcement learning using human priors. arXiv preprint arXiv:1904.10079, 2019a.\nW. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel, M. Veloso, and R. Salakhutdinov. Minerl: A\nlarge-scale dataset of minecraft demonstrations. arXiv preprint arXiv:1907.13440, 2019b.\n17\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nW. H. Guss, M. Y. Castro, S. Devlin, B. Houghton, N. S. Kuno, C. Loomis, S. Milani, S. P. Mohanty,\nK. Nakata, R. Salakhutdinov, J. Schulman, S. Shiroshita, N. Topin, A. Ummadisingu, and O. Vinyals.\nThe minerl 2020 competition on sample efficient reinforcement learning using human priors. arXiv:\nLearning, 2021.\nJ. Huang, X. Ma, S. Yong, X. Linghu, et al. An embodied generalist agent in 3d world. arXiv preprint\narXiv:2311.xxxx, 2023.\nW. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting\nactionable knowledge for embodied agents. ICML, 2022a.\nW. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar,\net al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint\narXiv:2207.05608, 2022b.\nA. Kanervisto, S. Milani, K. Ramanauskas, N. Topin, Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, W. Yang, W. Hong,\nZ. Huang, H. Chen, G. Zeng, Y. Lin, V. Micheli, E. Alonso, F. Fleuret, A. Nikulin, Y. Belousov,\nO. Svidchenko, and A. Shpilman. Minerl diamond 2021 competition: Overview, results, and lessons\nlearned. neural information processing systems, 2022.\nZ. Ke, H. Lin, Y. Shao, H. Xu, L. Shu, and B. Liu. Continual training of language models for few-shot\nlearning. arXiv preprint arXiv:2210.05549, 2022a.\nZ. Ke, Y. Shao, H. Lin, T. Konishi, G. Kim, and B. Liu. Continual pre-training of language models. In\nThe Eleventh International Conference on Learning Representations, 2022b.\nY. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding.\nIn International Conference on Machine Learning, pages 19274\u201319286. PMLR, 2023.\nP. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K\u00fcttler, M. Lewis, W.-t. Yih,\nT. Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances\nin Neural Information Processing Systems, 33:9459\u20139474, 2020.\nJ. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies:\nLanguage model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022.\nS. Lifshitz, K. Paster, H. Chan, J. Ba, and S. McIlraith. Steve-1: A generative model for text-to-behavior\nin minecraft. arXiv preprint arXiv:2306.00937, 2023.\nH. Lin, Z. Wang, J. Ma, and Y. Liang. Mcu: A task-centric framework for open-ended agent evaluation\nin minecraft. arXiv preprint arXiv:2310.08367, 2023a.\nK. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg. Text2motion: From natural language instructions\nto feasible plans. arXiv preprint arXiv:2303.12153, 2023b.\nZ. Lin, J. Li, J. Shi, D. Ye, Q. Fu, and W. Yang. Juewu-mc: Playing minecraft with sample-efficient\nhierarchical reinforcement learning. arXiv preprint arXiv:2112.04907, 2021.\nB. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. Llm+ p: Empowering large\nlanguage models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023.\nJ. Mai, J. Chen, B. Li, G. Qian, M. Elhoseiny, and B. Ghanem. Llm as a robotic brain: Unifying\negocentric memory and control. arXiv preprint arXiv:2304.09349, 2023.\n18\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nH. Mao, C. Wang, X. Hao, Y. Mao, Y. Lu, C. Wu, J. Hao, D. Li, and P. Tang. Seihai: A sample-efficient\nhierarchical ai for the minerl competition. In Distributed Artificial Intelligence: Third International\nConference, DAI 2021, Shanghai, China, December 17\u201318, 2021, Proceedings 3, pages 38\u201351. Springer,\n2022.\nY. Mao, P. He, X. Liu, Y. Shen, J. Gao, J. Han, and W. Chen. Generation-augmented retrieval for\nopen-domain question answering. arXiv preprint arXiv:2009.08553, 2020.\nG. Mialon, R. Dess\u00ec, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Rozi\u00e8re, T. Schick,\nJ. Dwivedi-Yu, A. Celikyilmaz, et al.\nAugmented language models: a survey.\narXiv preprint\narXiv:2302.07842, 2023.\nJ. Oh, S. Singh, H. Lee, and P. Kohli. Zero-shot task generalization with multi-task deep reinforcement\nlearning. In International Conference on Machine Learning, pages 2661\u20132670. PMLR, 2017.\nOpenAI. Gpt-4 technical report, 2023.\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\nA. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint\narXiv:2203.02155, 2022.\nJ. S. Park, J. C. O\u2019Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents:\nInteractive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.\nS. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky,\nJ. Kay, J. T. Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.\nN. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory and\nself-reflection. arXiv preprint arXiv:2303.11366, 2023.\nI. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg.\nProgprompt: Generating situated robot task plans using large language models. arXiv preprint\narXiv:2209.11302, 2022.\nH. Sun, Y. Zhuang, L. Kong, B. Dai, and C. Zhang. Adaplanner: Adaptive planning from feedback\nwith language models. arXiv preprint arXiv:2305.16653, 2023.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhar-\ngava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023.\nG. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar. Voyager: An\nopen-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.\nY. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning\nlanguage models with self-generated instructions, 2022.\nZ. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang. Describe, explain, plan and select: Interactive planning\nwith large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560,\n2023b.\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting\nelicits reasoning in large language models. 36th Conference on Neural Information Processing Systems\n(NeurIPS 2022), 2022.\n19\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nY. Wu, S. Y. Min, S. Prabhumoye, Y. Bisk, R. Salakhutdinov, A. Azaria, T. Mitchell, and Y. Li. Spring: Gpt-\n4 out-performs rl algorithms by studying papers and reasoning. arXiv preprint arXiv:2305.15486,\n2023.\nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning\nand acting in language models. arXiv preprint arXiv:2210.03629, 2022.\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts:\nDeliberate problem solving with large language models, 2023.\nH. Yuan, C. Zhang, H. Wang, F. Xie, P. Cai, H. Dong, and Z. Lu. Plan4mc: Skill reinforcement learning\nand planning for open-world minecraft tasks. arXiv preprint arXiv:2303.16563, 2023.\nA. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee,\nV. Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language.\narXiv preprint arXiv:2204.00598, 2022.\nC. Zhang, K. Yang, S. Hu, Z. Wang, G. Li, Y. Sun, C. Zhang, Z. Zhang, A. Liu, S.-C. Zhu, et al. Proagent:\nBuilding proactive cooperative ai with large language models. arXiv preprint arXiv:2308.11339,\n2023.\nW. Zhang and Z. Lu. Rladapter: Bridging large language models to reinforcement learning in open\nworlds. arXiv preprint arXiv:2309.17176, 2023.\nH. Zhao, Z. Cai, S. Si, X. Ma, K. An, L. Chen, Z. Liu, S. Wang, W. Han, and B. Chang. Mmicl: Empowering\nvision-language model with multi-modal in-context learning. arXiv preprint arXiv:2309.07915,\n2023.\nX. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu, X. Wang, et al. Ghost in the\nminecraft: Generally capable agents for open-world enviroments via large language models with\ntext-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023.\n20\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nA. Implementation Details\nA.1. Controller\nTasks in Minecraft are usually related to mine and craft goals. The mine goals require the agent\nto collect raw materials from the environment using the appropriate tools. The craft goals ask the\nagent to use the recipe to generate new items with existing materials in inventory. The mine goals\nare achieved through STEVE-1 (Lifshitz et al., 2023) with text condition during implementation. The\nenvironment can directly executes the craft and smelt actions (craft/smelt with argument),\nwhich are same as MineDojo (Fan et al., 2022) .\nA.2. Interactive Planner\nJARVIS-1 relies on the Multi-modal Language Model for planning, self-checking, and self-explaining,\nand can accept three types of inputs: visual images, language, and symbolic information (including\ninventory, located position, home, current life statistics, etc.). Specifically, this is a hybrid model with\nlanguage processing capabilities derived from the GPT model (OpenAI, 2023). The visual ability\ncomes from MineCLIP (Fan et al., 2022). We collected approximately 1000 Minecraft text data from\nthe internet and calculated the similarity between the current vision observation and these text\ndata. Text above the similarity threshold will be selected into the GPT model\u2019s prompt. Symbolic\ninformation is converted into natural language text through a designed template. All modalities are\nultimately captured as language and processed by the GPT model.\nDifferent modules in JARVIS-1 (e.g. self-check and self-explain) are completed through MLM\nbased on different prompts. The specific prompt design are shown below.\nSystem: You are a helper agent in Minecraft. You need to generate the sequences of goals for a\ncertain task in Minecraft. Just refer the history dialogue to give the plan consist of\ntemplate. Do not explain or give any other instruction.\n==========\nUser: My current inventory has nothing. I current locate in plains. How to obtain 1 wooden_pickaxe\nin Minecraft step-by-step?\nAssistant: The code for obtaining 1 wooden_pickaxe is as follows:\ndef craft_wooden_axe(initial_inventory={}):\nmine(obj = {\"log\":3}, tool = None) # step 1: chop down trees to mine logs\ncraft(obj = {\"planks\":12}, materials = {\"log\":3}, tool = None) # step 2: craft 12 planks from 3\nlogs\ncraft(obj = {\"stick\":4}, materials = {\"planks\":2}, tool = None) # step 3: craft 4 sticks from 2\nplanks\ncraft(obj = {\"crafting_table\":1}, materials = {\"planks\":4}, tool = None) # step 4: craft 1\ncrafting_table from 4 planks\ncraft(obj = {\"wooden_axe\":1}, {\"planks\": 3, \"stick\": 2}, \"crafting_table\") # step 5: craft 1\nwooden_axe from 3 planks and 2 sticks on crafting table\nreturn \"wooden_axe\"\nUser: [Description] I succeed in step 1, 2, 3, 4, 5.\nI finish all steps and I obtain 1 wooden_axe successfully.\n==========\nUser: My current inventory has <inventory>. <visual observation>. How to obtain 1 stone_sword in\nMinecraft step-by-step?\nAssistant:\nPrompt 1 | Planning prompt in JARVIS-1\nSystem: Extract the action name, action type, goal object, tool and action rank from the input\ntext.\nUser: mine({\"log\":3}, null); # step 1: chop down trees to mine logs\nAssistant:\nname: mine_log\ntext condition: chop down trees to mine logs\naction: mine\nobject_item: log\n21\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nobject_number: 3\ntool: null\nrank: 1\n###\ninput: craft({\"planks\":12}, {\"log\":3}, null); # step 2: craft 12 planks from 3 log\nAssistant:\nname: craft_planks\ncondition: craft 12 planks from 3 log\naction: craft\nobject_item: planks\nobject_number: 12\nmaterials: {\"log\":3}\ntool: null\nrank: 2\n###\nPrompt 2 | Goal parsing prompt in JARVIS-1\nSystem: Here are some actions that the agent fails to perform in Minecraft. Please give the\nexplanation of action execution failure according to the current inventory information of the\nagent.\n###\nUser:\nFailed Action: mine({\"iron_ore\":1}, null); # step 5: dig down to mine iron blocks\nCurrent Inventory: null\nAssistant: Because mining iron_ore needs to use the tool stone_pickaxe, but my inventory does not\nhave stone_pickaxe. So I need to craft stone_pickaxe first.\n###\nUser:\nFailed Action: craft({\"stone_pickaxe\":1}, {\"cobblestone\":3, \"stick\":2}, \"crafting_table\"); # step\n1: craft 1 stone_pickaxe from 3 cobblestone and 2 stick, on crafting_table\nCurrent Inventory: null\nAssistant: Because crafting stone_pickaxe needs to have 3 cobblestone and 2 stick in inventory,\nbut my inventory does not have cobblestone and stick. So I need to mine cobblestone and craft\nstick first.\n###\nUser:\nFailed Action: craft({\"stick\":4}, {\"planks\":2}, null); # step 3: craft 4 stick from 2 planks first\nCurrent Inventory: null\nAssistant: Because crafting stick needs to have planks in inventory, but my inventory does not\nhave planks. So I need to craft planks first.\n###\nPrompt 3 | Self-explaining prompt in JARVIS-1\nSystem: You are a helper agent in Minecraft. Check the plan whether can be finished.\n==========\nUser: My current inventory has nothing. I current locate in plains. My task is to obtain 1 wooden\naxe in Minecraft step-by-step. This is my plan:\ndef craft_wooden_axe(initial_inventory={}):\nmine(obj = {\"log\":3}, tool = None) # step 1: chop down trees to mine logs\ncraft(obj = {\"planks\":12}, materials = {\"log\":3}, tool = None) # step 2: craft 12 planks from 3\nlogs\ncraft(obj = {\"stick\":4}, materials = {\"planks\":2}, tool = None) # step 3: craft 4 sticks from 2\nplanks\ncraft(obj = {\"wooden_axe\":1}, {\"planks\": 3, \"stick\": 2}, null) # step 4: craft 1 wooden_axe\nfrom 3 planks and 2 sticks\nreturn \"wooden_axe\"\nAssistant:\nMy current inventory is empty.\n1. I want to mine 3 logs. Mining logs do not require tools. My inventory will has 3 logs.\n2. I want to craft 12 planks from 3 logs. 1 log can produce 4 planks. My inventory will add 12\nplanks, and delete 3 logs. My inventory will has 12 planks.\n3. I want to craft 4 stick from 2 planks. 2 planks can produce 4 planks. My inventory will add 4\nstick and delete 2 planks. My inventory will has 10 planks, 4 stick.\n4. I want to craft 1 wooden pickaxe from 3 planks and 2 sticks. Crafting wooden pickaxe requires\ncrafting_table. But I do not have crafting_table in inventory. This action will failed.\nReturn: Step 4 will failed because of lacking of crafting_table.\nPrompt 4 | Self-checking prompt in JARVIS-1\n22\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nA.3. Memory\nOur memory records every successful trajectory experience of JARVIS-1, including the task goals\nthat the agent needs to execute, the actual goal sequence (plan) executed by the agent, and the\nstate (visual observation and symbolic information returned from the environment) when the agent\ncompletes the task. In specific implementation, memory is a list where each trajectory experience is\nencoded as a dictionary, including the keys task, state, and plan.\nB. Environment Setting\nOur Minecraft environment is a hybrid between MineRL(Guss et al., 2019b) and the MCP-Reborn\n(github.com/Hexeption/MCP-Reborn) Minecraft modding package. Unlike the regular Minecraft\ngame, in which the server (or the \"world\") always runs at 20Hz and the client runs as fast as rendering\nB.1. Observation Space\nThe environmental observations consist of two parts. The first part is the raw pixels from the\nMinecraft game that players would see, including overlays such as the hotbar, health indicators, and\nanimations of a moving hand in response to attack or \"use\" actions. The field of view, GUI scale, and\nbrightness parameters are consistent with VPT (Baker et al., 2022). The second part includes auxiliary\ninformation about the agent\u2019s current environment, such as its location and weather conditions.\nHuman players can obtain this information by pressing F3. The specific observation details we include\nare shown in Table 3.\nTable 3 | The observation space we use in Minecraft.\nSources\nShape\nDescription\npov\n(640, 360, 3)\nEgo-centric RGB frames.\nplayer_pos\n(5,)\nThe coordinates of (x,y,z), pitch, and yaw of the agent.\nlocation_stats\n(9,)\nThe environmental information of the agent\u2019s current position,\nincluding biome_id, sea_level, can_see_sky, is_raining etc.\ninventory\n(36,)\nThe items in the current inventory of the agent, including\nthe type and corresponding quantity of each item in each slot.\nIf there is no item, it will be displayed as air.\nequipped_items\n(6,)\nThe current equipment of the agent, including mainhand, offhand,\nchest, feet, head, and legs slots. Each slot contains type, damage,\nand max_damage information.\nevent_info\n(5,)\nThe events that occur in the current step of the game, including\npick_up (picking up items), break_item (breaking items),\ncraft_item (crafting items using a crafting table or crafting grid),\nmine_block (mining blocks by suitable tools), and\nkill_entity (killing game mobs).\nNote that no high-level observations like voxels and lidar information in Minedojo (Fan et al.,\n2022) can be accessed by agents. During the actual inference process, the controller only perceives\nthe raw pixels and interacts with the environment, which is the same with VPT(Baker et al., 2022)\nmodels. The agent will access information from the environment to generate the text condition of the\ncontroller.\n23\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nB.2. Action Space\nWe design a hybrid action space. Some are directly available to human players, including keypresses,\nmouse movements, and clicks, which are similar to MineRL v1.0 (Guss et al., 2019b) used by\nVPT (Baker et al., 2022). The keypresses and clicks are binary functional actions, including forward,\njump, use and attack etc. In addition to the binary (on/off) keypress actions, our action space also\nincludes mouse movements. When the in-game GUIs (press \"E\" to open inventory) are closed, the\nmouse\u2019s X and Y actions control the agent\u2019s yaw and pitch. However, when the GUI is open, camera\nactions move the mouse cursor on the screen. In Minecraft, precise mouse movements are needed to\ninteract with the inventory for tasks such as crafting and smelting. On the other hand, mining and\nnavigating the world can be done using broader mouse actions. To be enable to achieve both the\nsame action space, we abstract the craft and smelt action with GUI into functional binary actions,\nwhich are same as MineDojo (Fan et al., 2022). The detailed action space are described in Table 4.\nTable 4 | The action space we use in Minecraft.\nIndex\nAction\nHuman Action\nDescription\n1\nForward\nkey W\nMove forward.\n2\nBack\nkey S\nMove backward.\n3\nLeft\nkey A\nStrafe left.\n4\nRight\nkey D\nStrafe right.\n5\nJump\nkey Space\nJump. When swimming, keeps the player afloat.\n6\nSneak\nkey left Shift\nSlowly move in the current direction of movement.\n7\nSprint\nkey left Ctrl\nMove quickly in the direction of current motion.\n8\nAttack\nleft Button\nDestroy blocks (hold down); Attack entity (click once).\n9\nUse\nright Button\nInteract with the block that the player is currently looking at.\n10\nhotbar.[1-9]\nkeys 1 - 9\nSelects the appropriate hotbar item.\n11\nYaw\nmove Mouse X\nTurning; aiming; camera movement.Ranging from -180 to +180.\n12\nPitch\nmove Mouse Y\nTurning; aiming; camera movement.Ranging from -180 to +180.\n13\nEquip\n-\nEquip the item in main hand from inventory.\n14\nCraft\n-\nExecute a crafting recipe to obtain new item.\n15\nSmelt\n-\nExecute a smelting recipe to obtain new item.\nB.3. Rules\nWe choose to conduct the test in survival mode of Minecraft 1.16.5. For each environment reset, we\nhave added the following rules:\n\u2022 /difficulty peaceful: Set the difficulty of the environment to peaceful mode.\n\u2022 /gamerule doDaylightCycle false: Set the environment to daytime forever.\n\u2022 /gamerule keepInventory true: Set agent to not drop items upon death. We have added\na time limit for each task, within which if the player dies, they will respawn at the spawn point\nand retain their previous inventory contents.\n\u2022 /effect give @a night_vision 99999 250 true: In order to facilitate the display\nof agent behavior, we have added night vision effects to the agent.\nC. Results and Details of 200+ tasks in Minecraft Universe Benchmark\nWe list the evaluation task set belows with details including task name, maximum steps, initial\ninventory, biome, and language instructions. We also show the evaluation times across different seeds\nand successful episodes rate. Note that all tasks are evaluated in Minecraft 1.16.5 Survival Mode.\n24\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 5 | The results of our agent on various tasks in the Wood group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\nwooden_shovel\n12000\nnull\nPlains/Forest\n0.9028\n72\nPick up a wooden_shovel given nothing.\nwooden_pickaxe\n12000\nnull\nPlains/Forest\n0.9516\n62\nPick up a wooden_pickaxe given nothing.\nwooden_axe\n12000\nnull\nPlains/Forest\n0.8909\n55\nPick up a wooden_axe given nothing.\nwooden_hoe\n12000\nnull\nPlains/Forest\n0.9318\n44\nPick up a wooden_hoe given nothing.\nstick\n12000\nnull\nPlains/Forest\n1\n86\nPick up a stick given nothing.\nwooden_sword\n12000\nnull\nPlains/Forest\n0.9242\n66\nPick up a wooden_sword given nothing.\ncomposter\n12000\nnull\nPlains/Forest\n0.7872\n47\nPick up a composter given nothing.\nbarrel\n12000\nnull\nPlains/Forest\n0.7544\n57\nPick up a barrel given nothing.\ncrafting_table\n12000\nnull\nPlains/Forest\n0.9706\n68\nPick up a crafting_table given nothing.\nchest\n12000\nnull\nPlains/Forest\n0.9155\n71\nPick up a chest given nothing.\nladder\n12000\nnull\nPlains/Forest\n0.9737\n76\nPick up a ladder given nothing.\nbowl\n12000\nnull\nPlains/Forest\n0.9149\n47\nPick up a bowl given nothing.\noak_wood\n12000\nnull\nForest\n0.9868\n76\nPick up a oak_wood in Forest.\noak_slab\n12000\nnull\nForest\n0.9506\n81\nPick up a oak_slab in Forest.\noak_planks\n12000\nnull\nForest\n0.9659\n88\nPick up a oak_planks in Forest.\noak_log\n12000\nnull\nForest\n1\n65\nPick up a oak_log in Forest.\noak_button\n12000\nnull\nForest\n0.9153\n59\nPick up a oak_button in Forest.\noak_door\n12000\nnull\nForest\n0.8732\n71\nPick up a oak_door in Forest.\noak_fence\n12000\nnull\nForest\n0.8\n60\nPick up a oak_fence in Forest.\noak_fence_gate\n12000\nnull\nForest\n0.9322\n59\nPick up a oak_fence_gate in Forest.\noak_trapdoor\n12000\nnull\nForest\n0.8861\n79\nPick up a oak_trapdoor in Forest.\noak_boat\n12000\nnull\nForest\n0.9074\n54\nPick up a oak_boat in Forest.\noak_sign\n12000\nnull\nForest\n0.9\n40\nPick up a oak_sign in Forest.\nbirch_wood\n12000\nnull\nForest\n0.9474\n57\nPick up a birch_wood in Forest.\nbirch_slab\n12000\nnull\nForest\n0.9231\n65\nPick up a birch_slab in Forest.\nbirch_planks\n12000\nnull\nForest\n0.9714\n70\nPick up a birch_planks in Forest.\nbirch_log\n12000\nnull\nForest\n0.9833\n60\nPick up a birch_log in Forest.\nbirch_button\n12000\nnull\nForest\n0.9245\n53\nPick up a birch_button in Forest.\nbirch_door\n12000\nnull\nForest\n0.8431\n51\nPick up a birch_door in Forest.\nbirch_fence\n12000\nnull\nForest\n0.8\n30\nPick up a birch_fence in Forest.\nbirch_fence_gate\n12000\nnull\nForest\n0.9355\n62\nPick up a birch_fence_gate in Forest.\nbirch_trapdoor\n12000\nnull\nForest\n0.9524\n63\nPick up a birch_trapdoor in Forest.\nbirch_boat\n12000\nnull\nForest\n0.8906\n64\nPick up a birch_boat in Forest.\nbirch_sign\n12000\nnull\nForest\n0.9\n60\nPick up a birch_sign in Forest.\nTable 6 | The results of our agent on various tasks in the Stone group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\nstone_shovel\n12000\niron_axe\nPlains/Forest\n0.8514\n74\nCraft a stone_shovel given an iron_axe.\nstone_pickaxe\n12000\niron_axe\nPlains/Forest\n0.9118\n68\nCraft a stone_pickaxe given an iron_axe.\nstone_axe\n12000\niron_axe\nPlains/Forest\n0.9123\n57\nCraft a stone_axe given an iron_axe.\nstone_hoe\n12000\niron_axe\nPlains/Forest\n0.9459\n74\nCraft a stone_hoe given an iron_axe.\nstone\n12000\niron_axe\nPlains/Forest\n0.8413\n63\nCraft a stone given an iron_axe.\ncharcoal\n12000\niron_axe\nPlains/Forest\n0.8947\n76\nCraft a charcoal given an iron_axe.\nsmoker\n12000\niron_axe\nPlains/Forest\n0.7867\n75\nCraft a smoker given an iron_axe.\nstone_sword\n12000\niron_axe\nPlains/Forest\n0.8831\n77\nCraft a stone_sword given an iron_axe.\nfurnace\n12000\niron_axe\nPlains/Forest\n0.942\n69\nCraft a furnace given an iron_axe.\ntorch\n12000\niron_axe\nPlains/Forest\n0.9\n30\nCraft a torch given an iron_axe.\n25\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 7 | The results of our agent on various tasks in the Iron group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\niron_axe\n12000\nnull\nPlains/Forest\n0.3333\n60\nSmelt and craft an iron_axe.\niron_pickaxe\n12000\niron_axe\nPlains/Forest\n0.3382\n68\nSmelt and craft an iron_pickaxe.\niron_shovel\n12000\niron_axe\nPlains/Forest\n0.338\n71\nSmelt and craft an iron_shovel.\niron_sword\n12000\niron_axe\nPlains/Forest\n0.3288\n73\nSmelt and craft an iron_sword.\niron_trapdoor\n12000\niron_axe\nPlains/Forest\n0.3151\n73\nSmelt and craft an iron_trapdoor.\niron_door\n12000\niron_axe\nPlains/Forest\n0.2836\n67\nSmelt and craft an iron_door.\niron_ingot\n12000\niron_axe\nPlains/Forest\n0.5479\n73\nSmelt and craft an iron_ingot.\nbucket\n12000\niron_axe\nPlains/Forest\n0.381\n42\nSmelt and craft a bucket.\nrail\n12000\niron_axe\nPlains/Forest\n0.3226\n62\nSmelt and craft a rail.\nminecart\n12000\niron_axe\nPlains/Forest\n0.2833\n60\nSmelt and craft a minecart.\nsmithing_table\n12000\niron_axe\nPlains/Forest\n0.3611\n72\nSmelt and craft a smithing_table.\ntripwire_hook\n12000\niron_axe\nPlains/Forest\n0.45\n60\nSmelt and craft a tripwire_hook.\nchain\n12000\niron_axe\nPlains/Forest\n0.3729\n59\nSmelt and craft a chain.\niron_bars\n12000\niron_axe\nPlains/Forest\n0.3208\n53\nSmelt and craft an iron_bars.\nhopper\n12000\niron_axe\nPlains/Forest\n0.3077\n65\nSmelt and craft a hopper.\niron_nugget\n12000\niron_axe\nPlains/Forest\n0.3582\n67\nSmelt and craft an iron_nugget.\nheavy_weighted_pressure_plate\n12000\niron_axe\nPlains/Forest\n0.358\n81\nSmelt and craft a\nheavy_weighted_pressure_plate.\nblast_furnace\n12000\niron_axe\nPlains/Forest\n0.5\n60\nSmelt and craft a blast_furnace.\nshears\n12000\niron_axe\nPlains/Forest\n0.25\n64\nSmelt and craft a shears.\nstonecutter\n12000\niron_axe\nPlains/Forest\n0.5\n60\nSmelt and craft a stonecutter.\niron_hoe\n12000\niron_axe\nPlains/Forest\n0.3214\n56\nSmelt and craft an iron_hoe.\ncrossbow\n12000\niron_axe\nPlains/Forest\n0.047\n63\nSmelt and craft a crossbow.\nTable 8 | The results of our agent on various tasks in the Gold group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\ngolden_pickaxe\n36000\niron_axe\nPlains/Forest\n0.0526\n77\nSmelt and craft a golden_pickaxe.\ngolden_shovel\n36000\niron_axe\nPlains/Forest\n0.0822\n73\nSmelt and craft a golden_shovel.\ngolden_sword\n36000\niron_axe\nPlains/Forest\n0.0476\n85\nSmelt and craft a golden_sword.\ngolden_hoe\n36000\niron_axe\nPlains/Forest\n0.058\n69\nSmelt and craft a golden_hoe.\ngolden_axe\n36000\niron_axe\nPlains/Forest\n0.0469\n64\nSmelt and craft a golden_axe.\ngolden_apple\n36000\niron_axe\nPlains/Forest\n0.02\n76\nSmelt and craft a golden_apple.\nclock\n36000\niron_axe\nPlains/Forest\n0.02\n77\nSmelt and craft a clock.\ngold_nugget\n36000\niron_axe\nPlains/Forest\n0.1444\n91\nSmelt and craft a gold_nugget.\ngold_ingot\n36000\niron_axe\nPlains/Forest\n0.1449\n70\nSmelt and craft a gold_ingot.\nTable 9 | The results of our agent on various tasks in the Diamond group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\ndiamond_pickaxe\n36000\niron_axe\nPlains/Forest\n0.0622\n692\nDig down to mine diamond and craft diamond_pickaxe.\ndiamond_shovel\n36000\niron_axe\nPlains/Forest\n0.1136\n88\nDig down to mine diamond and craft diamond_shovel.\ndiamond_sword\n36000\niron_axe\nPlains/Forest\n0.1134\n97\nDig down to mine diamond and craft diamond_sword.\ndiamond_hoe\n36000\niron_axe\nPlains/Forest\n0.0441\n68\nDig down to mine diamond and craft diamond_hoe.\ndiamond_axe\n36000\niron_axe\nPlains/Forest\n0.0986\n71\nDig down to mine diamond and craft diamond_axe.\ndiamond\n36000\niron_axe\nPlains/Forest\n0.092\n728\nDig down to mine diamond and craft diamond.\njukebox\n36000\niron_axe\nPlains/Forest\n0.1053\n79\nDig down to mine diamond and craft jukebox.\n26\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 10 | The results of our agent on various tasks in the Redstone group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\npiston\n36000\niron_axe\nPlains/Forest\n0.1772\n79\nMine redstone and make piston.\nredstone_torch\n36000\niron_axe\nPlains/Forest\n0.2584\n89\nMine redstone and make redstone_torch.\nredstone_block\n36000\niron_axe\nPlains/Forest\n0.2469\n81\nMine redstone and make redstone_block.\nactivator_rail\n36000\niron_axe\nPlains/Forest\n0.0159\n63\nMine redstone and make activator_rail.\ncompass\n36000\niron_axe\nPlains/Forest\n0.0759\n79\nMine redstone and make compass.\ndropper\n36000\niron_axe\nPlains/Forest\n0.2278\n79\nMine redstone and make dropper.\nnote_block\n36000\niron_axe\nPlains/Forest\n0.2239\n67\nMine redstone and make note_block.\nTable 11 | The results of our agent on various tasks in the Blocks group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\ndiorite\n12000\niron_axe\nPlains/Forest\n0.9\n30\nDig down to mine diorite block.\nandesite\n12000\niron_axe\nPlains/Forest\n0.9667\n30\nDig down to mine andesite block.\ngranite\n12000\niron_axe\nPlains/Forest\n0.8667\n30\nDig down to mine granite block.\ncoal\n12000\niron_axe\nPlains/Forest\n0.6667\n30\nDig down to mine coal block.\nlapis_lazuli\n12000\niron_axe\nPlains/Forest\n0.8667\n30\nDig down to mine lapis_lazuli block.\niron_ore\n12000\niron_axe\nPlains/Forest\n0.5667\n30\nDig down to mine iron_ore block.\ngold_ore\n36000\niron_axe\nPlains/Forest\n0.27\n30\nDig down to mine gold_ore block.\ncobblestone\n12000\niron_axe\nPlains/Forest\n0.9667\n30\nDig down to mine cobblestone block.\ngravel\n12000\niron_axe\nPlains/Forest\n0.9667\n30\nDig down to mine gravel block.\noak_log\n12000\niron_axe\nPlains/Forest\n0.9667\n30\nChop down tree and mine oak_log block.\nbirch_log\n12000\niron_axe\nPlains/Forest\n0.8718\n39\nChop down tree and mine birch_log block.\nacacia_log\n12000\niron_axe\nPlains/Forest\n0.5\n30\nChop down tree and mine acacia_log block.\njungle_log\n12000\niron_axe\nPlains/Forest\n0.9333\n30\nChop down tree and mine jungle_log block.\ndark_oak_log\n12000\niron_axe\nPlains/Forest\n0.9\n30\nChop down tree and mine dark_oak_log block.\nspruce_log\n12000\niron_axe\nPlains/Forest\n0.9333\n30\nChop down tree and mine spruce_log block.\nTable 12 | The results of our agent on various tasks in the Armor group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\nshield\n12000\niron_axe\nPlains/Forest\n0.3939\n66\nCraft shield and equip it.\nleather_helmet\n12000\niron_axe\nPlains/Forest\n0.0508\n59\nCraft leather_helmet and equip it.\nleather_chestplate\n12000\niron_axe\nPlains/Forest\n0.0312\n32\nCraft leather_chestplate and equip it.\nleather_leggings\n12000\niron_axe\nPlains/Forest\n0.0588\n34\nCraft leather_leggings and equip it.\nleather_boots\n12000\niron_axe\nPlains/Forest\n0.087\n23\nCraft leather_boots and equip it.\niron_chestplate\n12000\niron_axe\nPlains/Forest\n0.3333\n30\nCraft iron_chestplate and equip it.\niron_boots\n12000\niron_axe\nPlains/Forest\n0.3667\n30\nCraft iron_boots and equip it.\niron_leggings\n12000\niron_axe\nPlains/Forest\n0.3788\n66\nCraft iron_leggings and equip it.\niron_helmet\n12000\niron_axe\nPlains/Forest\n0.303\n33\nCraft iron_helmet and equip it.\ndiamond_helmet\n36000\niron_axe\nPlains/Forest\n0.0429\n70\nCraft diamond_helmet and equip it.\ndiamond_chestplate\n36000\niron_axe\nPlains/Forest\n0.0149\n68\nCraft diamond_chestplate and equip it.\ndiamond_leggings\n36000\niron_axe\nPlains/Forest\n0.02\n73\nCraft diamond_leggings and equip it.\ndiamond_boots\n36000\niron_axe\nPlains/Forest\n0.0533\n75\nCraft diamond_boots and equip it.\ngolden_helmet\n36000\niron_axe\nPlains/Forest\n0.0533\n75\nCraft golden_helmet and equip it.\ngolden_chestplate\n36000\niron_axe\nPlains/Forest\n0.02\n78\nCraft golden_chestplate and equip it.\ngolden_leggings\n36000\niron_axe\nPlains/Forest\n0.0159\n89\nCraft golden_leggings and equip it.\ngolden_boots\n36000\niron_axe\nPlains/Forest\n0.0617\n81\nCraft golden_boots and equip it.\n27\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 13 | The results of our agent on various tasks in the Decoration group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\nyellow_dye\n12000\niron_axe\nFlower Forest\n0.2333\n30\nObtain the yellow_dye.\nred_dye\n12000\niron_axe\nFlower Forest\n0.6364\n33\nObtain the red_dye.\nlight_gray_dye\n12000\niron_axe\nFlower Forest\n0.6667\n27\nObtain the light_gray_dye.\npink_dye\n12000\niron_axe\nFlower Forest\n0.6667\n39\nObtain the pink_dye.\norange_dye\n12000\niron_axe\nFlower Forest\n0.4857\n35\nObtain the orange_dye.\nwhite_dye\n12000\niron_axe\nFlower Forest\n0.1471\n34\nObtain the white_dye.\nwhite_bed\n12000\niron_axe\nFlower Forest\n0.5\n36\nObtain the white_bed.\nitem_frame\n12000\niron_axe\nFlower Forest\n0.2143\n28\nObtain the item_frame.\npainting\n12000\niron_axe\nFlower Forest\n0.5484\n31\nObtain the painting.\nwhite_wool\n12000\niron_axe\nFlower Forest\n0.8235\n34\nObtain the white_wool.\nwhite_carpet\n12000\niron_axe\nFlower Forest\n0.6857\n35\nObtain the white_carpet.\nwhite_banner\n12000\niron_axe\nFlower Forest\n0.0968\n31\nObtain the white_banner.\nyellow_wool\n12000\niron_axe\nFlower Forest\n0.0625\n32\nObtain the yellow_wool.\nred_wool\n12000\niron_axe\nFlower Forest\n0.6571\n35\nObtain the red_wool.\nlight_gray_wool\n12000\niron_axe\nFlower Forest\n0.6098\n41\nObtain the light_gray_wool.\npink_wool\n12000\niron_axe\nFlower Forest\n0.4\n25\nObtain the pink_wool.\norange_wool\n12000\niron_axe\nFlower Forest\n0.5\n36\nObtain the orange_wool.\nTable 14 | The results of our agent on various tasks in the Food group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\napple\n12000\niron_axe\nPlains\n0.5\n30\nChop down tree to obtain apple.\ncooked_chicken\n12000\niron_axe\nPlains\n0.3562\n73\nKill chicken to obtain chicken and cook it.\ncooked_mutton\n12000\niron_axe\nPlains\n0.4355\n62\nKill sheep to obtain mutton and cook it.\ncooked_porkchop\n12000\niron_axe\nPlains\n0.3968\n63\nKill pig to obtain porkchop and cook it.\ncooked_beef\n12000\niron_axe\nPlains\n0.2857\n63\nKill cow to obtain beef and cook it.\nchicken\n12000\niron_axe\nPlains\n0.5667\n30\nKill chicken to obtain chicken.\nbeef\n12000\niron_axe\nPlains\n0.6333\n30\nKill cow to obtain beef.\nmutton\n12000\niron_axe\nPlains\n0.5667\n30\nKill sheep to obtain mutton.\nporkchop\n12000\niron_axe\nPlains\n0.4667\n30\nKill pig to obtain porkchop.\n28\n"
  },
  {
    "title": "Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model",
    "link": "https://arxiv.org/pdf/2311.06214.pdf",
    "upvote": "26",
    "text": "INSTANT3D: FAST TEXT-TO-3D WITH SPARSE-VIEW\nGENERATION AND LARGE RECONSTRUCTION MODEL\nJiahao Li1,2\u2217\nHao Tan1\nKai Zhang1\nZexiang Xu1\nFujun Luan1\nYinghao Xu1,3\nYicong Hong1,4\nKalyan Sunkavalli1\nGreg Shakhnarovich2\nSai Bi1\n1Adobe Research\n2TTIC\n3Stanford University\n4 Australian National Univeristy\n{jiahao,greg}@ttic.edu\nyhxu@stanford.edu\nmr.yiconghong@gmail.com\n{hatan,kaiz,zexu,fluan,sunkaval,sbi}@adobe.com\nABSTRACT\nText-to-3D with diffusion models has achieved remarkable progress in recent\nyears. However, existing methods either rely on score distillation-based optimiza-\ntion which suffer from slow inference, low diversity and Janus problems, or are\nfeed-forward methods that generate low-quality results due to the scarcity of 3D\ntraining data. In this paper, we propose Instant3D, a novel method that generates\nhigh-quality and diverse 3D assets from text prompts in a feed-forward manner.\nWe adopt a two-stage paradigm, which first generates a sparse set of four struc-\ntured and consistent views from text in one shot with a fine-tuned 2D text-to-image\ndiffusion model, and then directly regresses the NeRF from the generated images\nwith a novel transformer-based sparse-view reconstructor. Through extensive ex-\nperiments, we demonstrate that our method can generate diverse 3D assets of high\nvisual quality within 20 seconds, which is two orders of magnitude faster than\nprevious optimization-based methods that can take 1 to 10 hours. Our project\nwebpage is: https://jiahao.ai/instant3d/.\n1\nINTRODUCTION\nIn recent years, remarkable progress has been achieved in the field of 2D image generation. This\nsuccess can be attributed to two key factors: the development of novel generative models such as\ndiffusion models (Song et al., 2021; Ho et al., 2020; Ramesh et al., 2022; Rombach et al., 2021),\nand the availability of large-scale datasets like Laion5B (Schuhmann et al., 2022). Transferring\nthis success in 2D image generation to 3D presents challenges, mainly due to the scarcity of avail-\nable 3D training data. While Laion5B has 5 billion text-image pairs, Objaverse-XL (Deitke et al.,\n2023a), the largest public 3D dataset, contains only 10 million 3D assets with less diversity and\npoorer annotations. As a result, previous attempts to directly train 3D diffusion models on existing\n3D datasets (Luo & Hu, 2021; Nichol et al., 2022; Jun & Nichol, 2023; Gupta et al., 2023; Chen\net al., 2023b) are limited in the visual (shape and appearance) quality, diversity and compositional\ncomplexity of the results they can produce.\nTo address this, another line of methods (Poole et al., 2022; Wang et al., 2023a; Lin et al., 2023;\nWang et al., 2023b; Chen et al., 2023c) leverage the semantic understanding and high-quality gen-\neration capabilities of pretrained 2D diffusion models. Here, 2D generators are used to calculate\ngradients on rendered images, which are then used to optimize a 3D representation, usually a\nNeRF (Mildenhall et al., 2020). Although these methods yield better visual quality and text-3D\nalignment, they can be incredibly time-consuming, taking hours of optimization for each prompt.\nThey also suffer from artifacts such as over-saturated colors and the \u201cmulti-face\u201d problem arising\nfrom the bias in pretrained 2D diffusion models, and struggle to generate diverse results from the\nsame text prompt, with varying the random seed leading to minor changes in geometry and texture.\nIn this paper, we propose Instant3D, a novel feed-forward method that generates high-quality and\ndiverse 3D assets conditioned on the text prompt. Instant3D, like the methods noted above, builds\non top of pretrained 2D diffusion models. However, it does so by splitting 3D generation into\n\u2217This work was done while the author was an intern at Adobe Research.\n1\narXiv:2311.06214v2  [cs.CV]  23 Nov 2023\na steaming basket full of dumplings\na steam engine train, high resolution\na 3d model of an adorable cottage with a thatched roof\na baby dragon drinking boba\nbeautifully carved jade heavenly Chinese guardian lion, \nhand carved nephrite jade\na ghost eating a hamburger\na hippo wearing a sweater\na panda rowing a boat in a pond\na snail on a leaf\na cracked egg with the yolk spilling out\nFigure 1: Our method generates high-quality 3D NeRF assets from the given text prompts within 20\nseconds. Here we show novel view renderings from our generated NeRFs as well as the renderings\nof the extracted meshes from their density field.\ntwo stages: 2D generation and 3D reconstruction. In the first stage, instead of generating images\nsequentially (Liu et al., 2023b), we fine-tune an existing text-to-image diffusion model (Podell et al.,\n2023) to generate a sparse set of four-view images in the form of a 2\u00d72 grid in a single denoising\nprocess. This design allows the multi-view images to attend to each other during generation, leading\nto more view-consistent results. In the second stage, instead of relying on a slow optimization-\nbased reconstruction method, inspired by (Anonymous, 2023), we introduce a novel sparse-view\nlarge reconstruction model with a transformer-based architecture that can directly regress a triplane-\nbased (Chan et al., 2022) NeRF from a sparse set of multi-view images. Our model projects sparse-\nview images into a set of pose-aware image tokens using pretrained vision transformers (Caron et al.,\n2021), which are then fed to an image-to-triplane decoder that contains a sequence of transformer\nblocks with cross-attention and self-attention layers. Our proposed model has a large capacity with\nmore than 500 million parameters and can robustly infer correct geometry and appearance of objects\nfrom just four images.\nBoth of these stages are fine-tuned/trained with multi-view rendered images of around 750K 3D ob-\njects from Objaverse (Deitke et al., 2023b), where the second stage makes use of the full dataset and\nthe first stage can be fine-tuned with as little as 10K data. While we use a relatively smaller dataset\ncompared to the pre-training dataset for other modalities (e.g., C4 Raffel et al. (2020) for text and\nLaion5B for image), by combining it with the power of pretrained 2D diffusion models, Instant3D\u2019s\ntwo-stage approach is able to generate high-quality and diverse 3D assets even from input prompts\nthat contain complex compositional concepts (see Figure 1) and do not exist in the 3D dataset used\nfor training. Due to its feed-forward architecture, Instant3D is exceptionally fast, requiring only\nabout 20 seconds to generate a 3D asset, which is 200\u00d7 faster than previous optimization-based\nmethods (Poole et al., 2022; Wang et al., 2023b) while achieving comparable or even better quality.\n2\nRELATED WORKS\n2.1\n3D GENERATION\nFollowing the success of generative models on 2D images using VAEs (Kingma & Welling, 2013;\nVan Den Oord et al., 2017), GANs (Goodfellow et al., 2014; Karras et al., 2019; Gu et al., 2022;\nKang et al., 2023), and autoregressive models (Oord et al., 2016; Van Den Oord et al., 2016), people\nhave also explored the applications of such models on 3D generation. Previous approaches have\nexplored different methods to generate 3D models in the form of point clouds (Wu et al., 2016;\nGadelha et al., 2017; Smith & Meger, 2017), triangle meshes (Gao et al., 2022; Pavllo et al., 2020;\nChen et al., 2019; Luo et al., 2021) , volumes (Chan et al., 2022; Or-El et al., 2022; Bergman et al.,\n2\n2022; Skorokhodov et al., 2022; Mittal et al., 2022) and implicit representations (Liu et al., 2022;\nFu et al., 2022; Sanghi et al., 2022) in an unconditional or text/image-conditioned manner. Such\nmethods are usually trained on limited categories of 3D objects and do not generalize well to a wide\nrange of novel classes.\nDiffusion models (Rombach et al., 2021; Podell et al., 2023; Ho et al., 2020; Song et al., 2021;\nSaharia et al., 2022) open new possibilities for 3D generation. A class of methods directly train 3D\ndiffusion models on the 3D representations (Nichol et al., 2022; Liu et al., 2023c; Zhou et al., 2021;\nSanghi et al., 2023) or project the 3D models or multi-view rendered images into latent representa-\ntions (Ntavelis et al., 2023; Zeng et al., 2022; Gupta et al., 2023; Jun & Nichol, 2023; Chen et al.,\n2023b) and perform the diffusion process in the latent space. For example, Shap-E (Jun & Nichol,\n2023) encodes each 3D shape into a set of parameters of an implicit function, and then trains a con-\nditional diffusion model on the parameters. These approaches face challenges due to the restricted\navailability and diversity of existing 3D data, consequently resulting in generated content with poor\nvisual quality and inadequate alignment with the input prompt. Therefore, although trained on mil-\nlions of 3D assets, Shap-E still fails to generate 3D shapes with complex compositional concepts\nand high-fidelity textures.\nTo resolve this, another line of works try to make use of 2D diffusion models to facilitate 3D gen-\neration. Some works (Jain et al., 2022; Mohammad Khalid et al., 2022) optimize meshes or NeRFs\nto maximize the CLIP Radford et al. (2021) score between the rendered images and input prompt\nutilizing pretrained CLIP models. While such methods can generate diverse 3D content, they exhibit\na deficiency in visual realism. More recently, some works (Poole et al., 2022; Wang et al., 2023b;\nLin et al., 2023; Chen et al., 2023c) optimize 3D representations using score distillation loss (SDS)\nbased on pretrained 2D diffusion models. Such methods can generate high-quality results, but suf-\nfer from slow optimization, over-saturated colors and the Janus problem. For example, it takes 1.5\nhours for DreamFusion (Poole et al., 2022) and 10 hours for ProlificDreamer Wang et al. (2023b)\nto generate a single 3D asset, which greatly limits their practicality. In contrast, our method enjoys\nthe benefits of both worlds: it\u2019s able to borrow information from pretrained 2D diffusion models\nto generate diverse multi-view consistent images that are subsequently lifted to faithful 3D models,\nwhile still being fast and efficient due to its feed-forward nature.\n2.2\nSPARSE-VIEW RECONSTRUCTION\nTraditional 3D reconstruction with multi-view stereo (Agarwal et al., 2011; Sch\u00a8onberger et al., 2016;\nFurukawa et al., 2015) typically requires a dense set of input images that have significant overlaps\nto find correspondence across views and infer the geometry correctly. While NeRF (Mildenhall\net al., 2020) and its variants (M\u00a8uller et al., 2022; Chen et al., 2022; 2023a) have further alleviated\nthe prerequisites for 3D reconstruction, they perform per-scene optimization that still necessitates\na lot of input images. Previous methods (Wang et al., 2021; Chen et al., 2021; Long et al., 2022;\nReizenstein et al., 2021; Trevithick & Yang, 2021) have tried to learn data priors so as to infer NeRF\nfrom a sparse set of images. Typically they extract per-view features from each input image, and\nthen for each point on the camera ray, aggregate multi-view features and decode them to the density\n(or SDF) and colors. Such methods are either trained in a category-specific manner, or only trained\non small datasets such as ShapeNet and ScanNet; they have not been demonstrated to generalize\nbeyond these datasets especially to the complex text-to-2D outputs.\nMore recently, some methods utilize data priors from pretrained 2D diffusion models to lift a single\n2D image to 3D by providing supervision at novel views using SDS loss (Liu et al., 2023b; Qian\net al., 2023; Melas-Kyriazi et al., 2023) or generating multi-view images (Liu et al., 2023a). For in-\nstance, One-2-3-45 (Liu et al., 2023a) generates 32 images at novel views from a single input image\nusing a fine-tuned 2D diffusion model, and reconstructs a 3D model from them, which suffers from\ninconsistency between the many generated views. In comparison, our sparse-view reconstructor\nadopts a highly scalable transformer-based architecture and is trained on large-scale 3D data. This\ngives it the ability to accurately reconstruct 3D models of novel unseen objects from a sparse set of\n4 images without per-scene optimization.\n3\nA car made \nout of sushi\n2D diffusion model\n(fine-tuned)\nTransformer-based\nreconstructor\nGaussian blob initialization\n(inference only)\n4-view image (2 by 2 grid)\nNeRF\nFigure 2: Overview of our method. Given a text prompt (\u2018a car made out of sushi\u2019), we perform\nmulti-view generation with Gaussian blobs as initialization using fine-tuned 2D diffusion model,\nproducing a 4-view image in the form of a 2 \u00d7 2 grid. Then we apply a transformer-based sparse-\nview 3D reconstructor on the 4-view image to generate the final NeRF.\n3\nMETHOD\nOur method Instant3D is composed of two stages: sparse-view generation and feed-forward NeRF\nreconstruction. In Section 3.1, we present our approach for generating sparse multi-view images\nconditioned on the text input. In Section 3.2, we describe our transformer-based sparse-view large\nreconstruction model.\n3.1\nTEXT-CONDITIONED SPARSE VIEW GENERATION\nGiven a text prompt, our goal is to generate a set of multi-view images that are aligned with the\nprompt and consistent with each other. We achieve this by fine-tuning a pretrained text-to-image\ndiffusion model to generate a 2 \u00d7 2 image grid as shown in Figure 2.\nIn the following paragraphs, we first illustrate that large text-to-image diffusion models (i.e.,\nSDXL (Podell et al., 2023)) have the capacity to generate view-consistent images thus a lightweight\nfine-tuning is possible. We then introduce three essential techniques to achieve it: the image grid,\nthe curation of the dataset, and also the Gaussian Blob noise initialization in inference. As a result\nof these observations and technical improvements, we can fine-tune the 2D diffusion model for only\n10K steps (on 10K data) to generate consistent sparse views.\nMulti-view generation with image grid. Previous methods (Liu et al., 2023b;a) on novel-view\nsynthesis show that image diffusion models are capable of understanding the multi-view consistency.\nIn light of this, we compile the images at different views into a single image in the form of an image\ngrid, as depicted in Figure 2. This image-grid design can better match the original data format of\nthe 2D diffusion model, and is suitable for simple direct fine-tuning protocol of 2D models. We\nalso observe that this simple protocol only works when the base 2D diffusion has enough capacity,\nas shown in the comparisons of Stable Diffusion v1.5 (Rombach et al., 2021) and SDXL (Podell\net al., 2023) in Section 4.3. The benefit from simplicity will also be illustrated later in unlocking the\nlightweight fine-tuning possibility.\nRegarding the number of views in the image grid, there is a trade-off between the requirements of\nmulti-view generation and 3D reconstruction. More generated views make the problem of 3D re-\nconstruction easier with more overlaps but increase possibility of view inconsistencies in generation\nand reduces the resolution of each generated view. On the other hand, too few views may cause\ninsufficient coverage, requiring the reconstructor to hallucinate unseen parts, which is challenging\nfor a deterministic 3D reconstruction model. Our transformer-based reconstructor learns generic 3D\npriors from large-scale data, and greatly reduces the requirement for the number of views. We em-\npirically found that using 4 views achieves a good balance in satisfying the two requirements above,\nand they can be naturally arranged in a 2 \u00d7 2 grid as shown in Figure 2. Next, we detail how the\nimage grid data is created and curated.\nMulti-view data creation and curation.\nTo fine-tune the text-to-image diffusion model, we create\npaired multi-view renderings and text prompts. We adopt a large-scale synthetic 3D dataset Obja-\nverse (Deitke et al., 2023b) and render four 512 \u00d7 512 views of about 750K objects with Blender.\nWe distribute the four views at a fixed elevation (20 degrees) and four equidistant azimuths (0, 90,\n4\n180, 270 degrees) to achieve a better coverage of the object. We use Cap3D (Luo et al., 2023)\nto generate captions for each 3D object, which consolidates captions from multi-view renderings\ngenerated with pretrained image captioning model BLIP-2 (Li et al., 2023) using a large language\nmodel (LLM). Finally, the four views are assembled into a grid image in a fixed order and resized to\nthe input resolution compatible with the 2D diffusion model.\nWe find that naively using all the data for fine-tuning reduces the photo-realism of the generated\nimages and thus the quality of the 3D assets. Therefore, we train a simple scorer on a small amount\n(2000 samples) of manually labeled data to predict the quality of each 3D object. The model is\na simple SVM on top of pretrained CLIP features extracted from multi-view renderings of the 3D\nobject (please see Appendix for details). During training, our model only takes the top 10K data\nranked by our scorer. We provide a quantitative study in Section 4.3 to validate the impact of\ndifferent data curation strategies. Although the difference is not very significant from the metric\nperspective, we found that our curated data is helpful in improving the visual quality.\nInference with Gaussian blob initialization.\nWhile our training data is multi-view images with\na white background, we observe that during inference starting from standard Gaussian noise still\nresults in images that have cluttered backgrounds (see Figure 5); this introduces extra difficulty\nfor the feed-forward reconstructor in the second stage (Section 3.2). To guide the model toward\ngenerating images with a clean white background, inspired by SDEdit (Meng et al., 2022), we first\ncreate an image of a 2 \u00d7 2 grid with a solid white background that has the same resolution as the\noutput image, and initialize each sub-grid with a 2D Gaussian blob that is placed at the center of\nthe image with a standard deviation of 0.1 (please see Appendix for details). The visualization of\nthis Gaussian Blob is shown in Figure 2. The Gaussian blob image grid is fed to the auto-encoder to\nget its latent. We then add diffusion noise (e.g., use t=980/1000 for 50 DDIM denoising steps), and\nuse it as the starting point for the denoising process. As seen in Figure 5, this technique effectively\nguides the model toward generating images with a clean background.\nLightweight fine-tuning.\nWith all the above observations and techniques, we are able to adapt\na text-to-image diffusion model to a text-to-multiview model with lightweight fine-tuning. This\nlightweight fine-tuning shares a similar spirit to the \u2018instruction fine-tuning\u2019 (Mishra et al., 2022;\nWei et al., 2021) for LLM alignment. The assumption is that the base model is already capable\nof the task, and the fine-tuning is to unlock the base model\u2019s ability without introducing additional\nknowledge.\nSince we utilize an image grid, the fine-tuning follows the exactly same protocol as the 2D diffusion\nmodel pre-training, except that we decrease the learning rate to 10\u22125. We train the model with a\nbatch size of 192 for only 10K iterations on the 10K curated multi-view data. The training is done\nusing 32 NVIDIA A100 GPUs for only 3 hours. We study the impact of different training settings\nin Section 4.3. For more training details, please refer to Appendix.\n3.2\nFEED-FORWARD SPARSE-VIEW LARGE RECONSTRUCTION MODEL\nIn this stage, we aim to reconstruct a NeRF from the four-view images I = {Ii | i = 1, ..., 4}\ngenerated in the first stage. 3D reconstruction from sparse inputs with a large baseline is a challeng-\ning problem, which requires strong model priors to resolve the inherent ambiguity. Inspired by a\nrecent work LRM (Anonymous, 2023) that introduces a transformer-based model for single image\n3D reconstruction, we propose a novel approach that enables us to predict a NeRF from a sparse set\nof input views with known poses. Similar to Anonymous (2023), our model consists of an image\nencoder, an image-to-triplane decoder, and a NeRF decoder. The image encoder encodes the multi-\nview images into a set of tokens. We feed the concatenated image tokens to the image-to-triplane\ndecoder to output a triplane representation (Chan et al., 2022) for the 3D object. Finally, the triplane\nfeatures are decoded into per-point density and colors via the NeRF MLP decoder.\nIn detail, we apply a pretrained Vision Transformer (ViT) DINO (Caron et al., 2021) as our image\nencoder. To support multi-view inputs, we inject camera information in the image encoder to make\nthe output image tokens pose-aware. This is different from Anonymous (2023) that feeds the camera\ninformation in the image-to-triplane decoder because they take single image input. The camera\ninformation injection is done by the AdaLN (Huang & Belongie, 2017; Peebles & Xie, 2022) camera\nmodulation as described in Anonymous (2023). The final output of the image encoder is a set\n5\nSelf-\nattention\nMLP\n+\n+\n12 layers, ViT (DINO)\nCamera features\nConv\nConv\nMulti-view images\n(N=2 here)\nImage encoder \n(shared)\nImage encoder\nTriplane tokens \n(learnable)\nConcat\n2D image tokens \nSelf-\nattention\nMLP\n+\n16 layers\nCross-\nattention\nreshape & \nupsample\nTriplane\nVolume\nrendering\n+\n+\nMod\nNovel view rendering\nImage-to-triplane decoder\nFigure 3: Architecture of our sparse-view reconstructor. The model applies a pretrained ViT to\nencode multi-view images into pose-aware image tokens, from which we decode a triplane rep-\nresentation of the scene using a transformer-based decoder. Finally we decode per-point triplane\nfeatures to its density and color and perform volume rendering to render novel views. We illustrate\nhere with 2 views and the actual implementation uses 4 views.\nof pose-aware image tokens f \u2217\nIi, and we concatenate the per-view tokens together as the feature\ndescriptors for the multi-view images: fI = \u2295(f \u2217\nI1, ...f \u2217\nI4)\nWe use triplane as the scene representation. The triplane is flattened to a sequence of learnable\ntokens, and the image-to-triplane decoder connects these triplane tokens with the pose-aware image\ntokens fI using cross-attention layers, followed by self-attention and MLP layers. The final output\ntokens are reshaped and upsampled using a de-convolution layer to the final triplane representation.\nDuring training, we ray march through the object bounding box and decode the triplane features at\neach point to its density and color using a shared MLP, and finally get the pixel color via volume\nrendering. We train the networks in an end-to-end manner with image reconstruction loss at novel\nviews using a combination of MSE loss and LPIPS (Zhang et al., 2018) loss.\nTraining details.\nWe train the model on multi-view renderings of the Objaverse dataset (Deitke\net al., 2023b). Different from the first stage that performs data curation, we use all the 3D objects in\nthe dataset and scale them to [\u22121, 1]3; then we generate multi-view renderings using Blender under\nuniform lighting with a resolution of 512 \u00d7 512. While the output images from the first stage are\ngenerated in a structured setup with fixed camera poses, we train the model using random views as\na data augmentation mechanism to increase the robustness. Particularly, we randomly sample 32\nviews around each object. During training, we randomly select a subset of 4 images as input and\nanother random set of 4 images as supervision. For inference, we will reuse the fixed camera poses\nin the first stage as the camera input to the reconstructor. For more details on the training, please\nrefer to the Appendix.\n4\nEXPERIMENTS\nIn this section, we first do comparisons against previous methods on text-to-3D (Section 4.1), and\nthen perform ablation studies on different design choices of our method. By default, we report the\nresults generated with fine-tuned SDXL models, unless otherwise noted.\n4.1\nTEXT-TO-3D\nWe make comparisons to state-of-the-art methods on text-to-3D, including a feed-forward method\nShap-E (Jun & Nichol, 2023), and optimization-based methods including DreamFusion (Poole et al.,\n2022) and ProlificDreamer (Wang et al., 2023b). We use the official code for Shap-E, and the\nimplementation from three-studio (Guo et al., 2023) for the other two as there is no official code.\nWe use default hyper-parameters (number of optimization iterations, number of denoising steps) of\n6\nFigure 4: Qualitative comparisons on text-to-3D against previous methods. We include more uncu-\nrated comparisons in the supplementary material.\nTable 1: Quantitative comparisons on CLIP scores\nagainst baseline methods.\nOur method outper-\nforms previous feed-forward method Shap-E and\noptimization-based\nmethod\nDreamFusion,\nand\nachieves\ncompetitive\nperformance\ncompared\nto\nProlificDreamer while being 1800\u00d7 faster.\nViT-L/14 \u2191\nViT-bigG-14 \u2191\nTime(s) \u2193\nShap-E\n20.51\n32.21\n6\nDreamFusion\n23.60\n37.46\n5400\nProlificDreamer\n27.39\n42.98\n36000\nOurs\n26.87\n41.77\n20\nTable 2:\nQuantitative comparisons\nagainst previous sparse-view recon-\nstruction methods on GSO dataset.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nSparseNeus\n20.62\n0.8360\n0.1989\nOurs\n26.54\n0.8934\n0.0643\nthese models. For our own model we use the SDXL base model fine-tuned on 10K data for 10K\nsteps. During inference we take 100 DDIM steps.\nQualitative comparisons.\nAs shown in Figure 4, our method generates visually better results\nthan those of Shap-E, producing sharper textures, better geometry and substantially improved text-\n3D alignment. Shap-E applies a diffusion model that is exclusively trained on million-level 3D data,\nwhich might be evidence for the need of 2D data or models with 2D priors. DreamFusion and Pro-\nlificDreamer achieve better text-3D alignment utilizing pretrained 2D diffusion models. However,\nDreamFusion generates results with over-saturated colors and over-smooth textures. While Pro-\nlificDreamer results have better details, it still suffers from low-quality geometry (as in \u2018A bulldozer\nclearing ...\u2019) and the Janus problem (as in \u201da squirrel dressed like ...\u201d, also more detailed in Appendix\nFigure 11). In comparison, our results have more photorealistic appearance with better geometric\ndetails. Please refer to the Appendix and supplementary materials for video comparisons and more\nresults.\nQuantitative comparisons.\nIn Table 4, we quantitatively assess the coherence between the gener-\nated models and text prompts using CLIP-based scores. We perform the evaluation on results with\n400 text prompts from DreamFusion. For each model, we render 10 random views and calculate the\naverage CLIP score between the rendered images and the input text. We report the metric using mul-\n7\ntiple variants of CLIP models with different model sizes and training data (i.e., ViT-L/14 from Ope-\nnAI and ViT-bigG-14 from OpenCLIP). From the results we can see that our model achieves higher\nCLIP scores than Shap-E, indicating better text-3D alignment. Our method even achieves consis-\ntently higher CLIP scores than optimization-based method DreamFusion and competitive scores\nto ProlificDreamer, from which we can see that our approach can effectively inherit the great text\nunderstanding capability from the pretrained SDXL model and preserve them in the generated 3D\nassets via consistent sparse-view generation and robust 3D reconstruction.\nInference time comparisons.\nWe present the time to generate a 3D asset in Table 1. The timing is\nmeasured using the default hyper-parameters of each method on an A100 GPU. Notably, our method\nis significantly faster than the optimization-based methods: while it takes 1.5 hours for DreamFusion\nand 10 hours for ProlificDreamer to generate a single asset, our method can finish the generation\nwithin 20 seconds, resulting in a 270\u00d7 and 1800\u00d7 speed up respectively. In Figure 10, we show that\nour inference time can be further reduced without obviously sacrificing the quality by decreasing\nthe number of DDIM steps.\n4.2\nCOMPARISONS ON SPARSE VIEW RECONSTRUCTION\nWe make comparisons to previous sparse-view NeRF reconstruction works.\nMost of previous\nworks (Reizenstein et al., 2021; Trevithick & Yang, 2021; Yu et al., 2021) are either trained on\nsmall-scale datasets such as ShapeNet, or trained in a category-specific manner. Therefore, we make\ncomparisons to a state-of-the-art method SparseNeus (Long et al., 2022), which is also applied in\nOne2345 (Liu et al., 2023a) where they train the model on the same Objaverse dataset for sparse-\nview reconstruction. We do the comparisons on the Google Scan Object (GSO) dataset (Downs\net al., 2022), which consists of 1019 objects. For each object, we render 4-view input following the\nstructured setup and randomly select another 10 views for testing. We adopt the pretrained model\nfrom Liu et al. (2023a). Particularly, SparseNeus does not work well for 4-view inputs with such\na large baseline; therefore we add another set of 4 input views in addition to our four input views\n(our method still uses 4 views as input), following the setup in Liu et al. (2023a). We report the\nmetrics on novel view renderings in Table 2. From the table, we can see that our method outper-\nforms the baseline method even with fewer input images, which demonstrates the superiority of our\nsparse-view reconstructor.\n4.3\nABLATION STUDY FOR SPARSE VIEW GENERATION\nWe ablate several key decisions in our method design, including (1) the choice of the larger 2D base\nmodel SDXL, (2) the use of Gaussian Blob during inference, (3) the quality and size of the curated\ndataset, and lastly, (4) the need and requirements of lightweight fine-tuning. We gather the quantita-\ntive results in Table 3 and place all qualitative results in the Appendix. We observe that qualitative\nresults are more evident than quantitative results, thus we recommend a closer examination.\nScalability with 2D text-to-image models.\nOne of the notable advantages of our method is that its\nefficacy scales positively with the potency of the underlying 2D text-to-image model. In Figure 12,\nwe present qualitative comparisons between two distinct backbones (with their own tuned hyper-\nparameters): SD1.5 (Rombach et al., 2021) and SDXL (Podell et al., 2023). It becomes readily\napparent that SDXL, which boasts a model size 3\u00d7 larger than that of SD1.5, exhibits superior\ntext comprehension and visual quality. We also show a quantitative comparison on CLIP scores in\nTable 3. By comparing Exp(l, m) with Exp(d, g), we can see that the model with SD1.5 achieves\nconsistently lower CLIP scores indicating worse text-3D alignment.\nGaussian blob initialization.\nIn Figure 5, we show our results generated with and without Gaus-\nsian blob initialization. From the results we can see that while our fine-tuned model can generate\nmulti-view images without Gaussian blob initialization, they tend to have cluttered backgrounds,\nwhich challenges the second-stage feed-forward reconstructor. In contrast, our proposed Gaussian\nblob initialization enables the fine-tuned model to generate images with a clean white background,\nwhich better align with the requirements of the second stage.\n8\na barbecue grill cooking sausages and burger patties 001\nFrog 004\nw/o Gaussian blobs\nw/ Gaussian blobs\nOriginal\nFinetuned\nFigure 5: Qualitative comparisons on results generated with and without Gaussian blob initialization.\nTable 3: Comparison on CLIP scores of NeRF renderings with different variants of fine-tuning\nsettings.\nExp ID\nExp Name\nBase\n# Data\nCurated\n# Steps\nViT-L/14\nViT-bigG-14\n(a)\nCurated-1K-s1k\nSDXL\n1K\n\u2713\n1K\n26.33\n41.09\n(b)\nCurated-1K-s10k\nSDXL\n1K\n\u2713\n10k\n22.55\n35.59\n(c)\nCurated-10K-s4k\nSDXL\n10K\n\u2713\n4k\n26.55\n41.08\n(d)\nCurated-10K-s10k\nSDXL\n10K\n\u2713\n10k\n26.87\n41.77\n(e)\nCurated-10K-s20k\nSDXL\n10K\n\u2713\n20k\n25.96\n40.56\n(f)\nCurated-100K-s10k\nSDXL\n100K\n\u2713\n10k\n25.79\n40.32\n(g)\nCurated-100K-s40k\nSDXL\n100K\n\u2713\n40k\n26.59\n41.29\n(h)\nCurated-300K-s40k\nSDXL\n300K\n\u2713\n40K\n26.43\n40.72\n(i)\nRandom-10K-s10k\nSDXL\n10K\n\u2717\n10k\n26.87\n41.47\n(j)\nRandom-100K-s40k\nSDXL\n100K\n\u2717\n40k\n26.28\n40.90\n(k)\nAllData-s40k\nSDXL\n700K\n\u2717\n40k\n26.13\n40.60\n(l)\nCurated-10K-s10k (SD1.5)\nSD1.5\n10K\n\u2713\n10k\n23.50\n36.90\n(m)\nCurated-100K-s40k (SD1.5)\nSD1.5\n100K\n\u2713\n40k\n25.48\n39.07\nQuality and size of fine-tuning dataset.\nWe evaluate the impact of the quality and size of the\ndataset used for fine-tuning 2D text-to-image models. We first make comparisons between curated\nand uncurated (randomly selected) data. The CLIP score rises slightly as shown in Table 3 (i.e.,\ncomparing Exp(d, i)), while there is a substantial quality improvement as illustrated in Appendix\nFigure 7. This aligns with the observation that the data quality can dramatically impact the results\nin the instruction fine-tuning stage of LLM (Zhou et al., 2023).\nWhen it comes to data size, we observe a double descent from Table 3 Exp(a, d, g) with 1K, 10K,\nand 100K data. We pick Exp(a, d, g) here because they are the best results among different training\nsteps for the same training data size. The reason for this double descent can be spotlighted by the\nqualitative comparisons in Appendix Figure 13, where training with 1K data can lead to inconsistent\nmulti-view images, while training with 100K data can hurt the compositionality, photo-realism, and\nalso text alignment.\nNumber of fine-tuning steps.\nWe also quantitatively and qualitatively analyze the impact of fine-\ntuning steps. For each block in Table 3 we show the CLIP scores of different training steps. Similar\nto the findings in instruction fine-tuning (Ouyang et al., 2022), the results do not increase mono-\ntonically regarding the number of fine-tuning steps but have a peak in the middle. For example,\nin our final setup with the SDXL base model and 10K curated data (i.e., Exp(c, d, e)), the results\nare peaked at 10K steps. For other setups, the observations are similar. We also qualitatively com-\npare the results at different training steps for 10K curated data in Appendix Figure 14. There is an\nobvious degradation in the quality of the results for both 4K and 20K training steps.\nAnother important observation is that the peak might move earlier when the model size becomes\nlarger. This can be observed by comparing between Exp(l,m) for SD1.5 and Exp(d,g) for SDXL.\nNote that this comparison is not conclusive yet from the Table given that SD1.5 does not perform\nreasonably with our direct fine-tuning protocol. More details are in the Appendix.\nWe also found that Exp(a) with 1K steps on 1K data can achieve the best CLIP scores but the view\nconsistency is actually disrupted. A possible reason is that the CLIP score is insensitive to certain\nartifacts introduced by reconstruction from inconsistent images, which also calls for a more reliable\nevaluation metric for 3D generation.\n9\n5\nCONCLUSIONS\nIn this paper we presented a novel feed-forward two-stage approach named Instant3D that can gen-\nerate high-quality and diverse 3D assets from text prompts within 20 seconds. Our method finetunes\na 2D text-to-image diffusion model to generate consistent 4-view images, and lifts them to 3D\nwith a robust transformer-based large reconstruction model. The experiment results show that our\nmethod outperforms previous feed-forward methods in terms of quality while being equally fast, and\nachieves comparable or better performance to previous optimization-based methods with a speed-up\nof more than 200 times. Instant3D allows novice users to easily create 3D assets and enables fast\nprototyping and iteration for various applications such as 3D design and modeling.\nEthics Statement.\nThe generation ability of our model is inherited from the public 2D diffusion\nmodel SDXL. We only do lightweight fine-tuning over the SDXL model thus it is hard to introduce\nextra knowledge to SDXL. Also, our model can share similar ethical and legal considerations to\nSDXL. The curation of the data for lightweight fine-tuning does not introduce outside annotators.\nThus the quality of the data might be biased towards the preference of the authors, which can lead to\na potential bias on the generated results as well. The text input to the model is not further checked\nby the model, which means that the model will try to do the generation for every text prompt it gets\nwithout the ability to acknowledge unknown knowledge.\nReproducibility Statement.\nIn the main text, we highlight the essential techniques to build our\nmodel for both the first stage (Section 3.1) and the second stage (Section 3.2). We discuss how\nour data is created and curated in Section 3. The full model configurations and training details\ncan be found in Appendix Section A.3 and Section A.6. We have detailed all the optimizer hyper-\nparameters and model dimensions. We present more details on our data curation process in Sec-\ntion A.2. We also attach the IDs of our curated data in Supplementary Materials to further facilitate\nthe reproduction.\nREFERENCES\nAdobe. Adobe Firefly. https://firefly.adobe.com/, 2023.\nSameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, and\nRichard Szeliski. Building rome in a day. Communications of the ACM, 54(10):105\u2013112, 2011.\nAnonymous. Lrm: Large reconstruction model for single image to 3d. In Supplementary Files,\n2023.\nAlexander Bergman, Petr Kellnhofer, Wang Yifan, Eric Chan, David Lindell, and Gordon Wet-\nzstein. Generative neural articulated radiance fields. Advances in Neural Information Processing\nSystems, 35:19900\u201319916, 2022.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of\nthe International Conference on Computer Vision (ICCV), 2021.\nEric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello,\nOrazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon\nWetzstein. Efficient geometry-aware 3D generative adversarial networks. In CVPR, 2022.\nAnpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su.\nMvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pp. 14124\u201314133, 2021.\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance\nfields. In European Conference on Computer Vision (ECCV), 2022.\nAnpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su, and Andreas Geiger. Dictionary fields:\nLearning a neural basis decomposition. ACM Trans. Graph., 2023a.\n10\nHansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, and Hao Su. Single-\nstage diffusion nerf: A unified approach to 3d generation and reconstruction. In ICCV, 2023b.\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and\nappearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873, 2023c.\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174, 2016.\nWenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaakko Lehtinen, Alec Jacobson, and Sanja\nFidler. Learning to predict 3d objects with an interpolation-based differentiable renderer. Ad-\nvances in neural information processing systems, 32, 2019.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. Advances in Neural Information Processing Systems,\n35:16344\u201316359, 2022.\nMatt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan\nFan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of\n10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023a.\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig\nSchmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of anno-\ntated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 13142\u201313153, 2023b.\nLaura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann,\nThomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset\nof 3d scanned household items. In 2022 International Conference on Robotics and Automation\n(ICRA), pp. 2553\u20132560. IEEE, 2022.\nRao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, and Srinath Sridhar. Shapecrafter: A recursive\ntext-conditioned 3d shape generation model. Advances in Neural Information Processing Systems,\n35:8882\u20138895, 2022.\nYasutaka Furukawa, Carlos Hern\u00b4andez, et al.\nMulti-view stereo: A tutorial.\nFoundations and\nTrends\u00ae in Computer Graphics and Vision, 9(1-2):1\u2013148, 2015.\nMatheus Gadelha, Subhransu Maji, and Rui Wang. 3d shape induction from 2d views of multiple\nobjects. In 2017 International Conference on 3D Vision (3DV), pp. 402\u2013411. IEEE, 2017.\nJun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan\nGojcic, and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned\nfrom images. Advances In Neural Information Processing Systems, 35:31841\u201331854, 2022.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information\nprocessing systems, 27, 2014.\nJiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. Stylenerf: A style-based 3d aware\ngenerator for high-resolution image synthesis. In International Conference on Learning Repre-\nsentations, 2022.\nYuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-\nHao Chen, Zi-Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: A unified\nframework for 3d content generation. https://github.com/threestudio-project/\nthreestudio, 2023.\nAnchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas O\u02d8guz.\n3dgen: Triplane latent\ndiffusion for textured mesh generation. arXiv preprint arXiv:2303.05371, 2023.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840\u20136851, 2020.\n11\nXun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normal-\nization. In Proceedings of the IEEE international conference on computer vision, pp. 1501\u20131510,\n2017.\nAjay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided\nobject generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 867\u2013876, 2022.\nHeewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint\narXiv:2305.02463, 2023.\nMinguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung\nPark. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023.\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 4401\u20134410, 2019.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models. 2023.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d con-\ntent creation.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 300\u2013309, 2023.\nMinghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su.\nOne-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization, 2023a.\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-3: Zero-shot one image to 3d object, 2023b.\nZhen Liu, Yao Feng, Michael J Black, Derek Nowrouzezahrai, Liam Paull, and Weiyang Liu.\nMeshdiffusion: Score-based generative 3d mesh modeling. arXiv preprint arXiv:2303.08133,\n2023c.\nZhengzhe Liu, Yi Wang, Xiaojuan Qi, and Chi-Wing Fu. Towards implicit text-guided 3d shape\ngeneration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, pp. 17896\u201317906, 2022.\nXiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and Wenping Wang. Sparseneus: Fast gen-\neralizable neural surface reconstruction from sparse views. In European Conference on Computer\nVision, pp. 210\u2013227. Springer, 2022.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nAndrew Luo, Tianqin Li, Wen-Hao Zhang, and Tai Sing Lee. Surfgen: Adversarial 3d shape syn-\nthesis with explicit surface discriminators. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pp. 16238\u201316248, 2021.\nShitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2837\u20132845,\n2021.\nTiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pre-\ntrained models. arXiv preprint arXiv:2306.07279, 2023.\n12\nLuke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Realfusion: 360 recon-\nstruction of any object from a single image. In CVPR, 2023. URL https://arxiv.org/\nabs/2302.10663.\nChenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.\nSDEdit: Guided image synthesis and editing with stochastic differential equations. In Interna-\ntional Conference on Learning Representations, 2022.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision\ntraining. In International Conference on Learning Representations, 2018.\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization\nvia natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 3470\u20133487, 2022.\nParitosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. Autosdf: Shape priors for\n3d completion, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 306\u2013315, 2022.\nNasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. Clip-mesh: Gener-\nating textured meshes from text using pretrained image-text models. In SIGGRAPH Asia 2022\nconference papers, pp. 1\u20138, 2022.\nThomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics prim-\nitives with a multiresolution hash encoding.\nACM Trans. Graph., 41(4):102:1\u2013102:15, July\n2022.\ndoi: 10.1145/3528223.3530127.\nURL https://doi.org/10.1145/3528223.\n3530127.\nAlex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system\nfor generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022.\nEvangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski, Chaoyang Wang, Luc Van Gool, and\nSergey Tulyakov. Autodecoding latent 3d diffusion models. arXiv preprint arXiv:2307.05445,\n2023.\nAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for\nraw audio. arXiv preprint arXiv:1609.03499, 2016.\nRoy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, and Ira Kemelmacher-\nShlizerman. StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 13503\u201313513, June 2022.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nDario Pavllo, Graham Spinks, Thomas Hofmann, Marie-Francine Moens, and Aurelien Lucchi.\nConvolutional generation of textured 3d meshes. Advances in Neural Information Processing\nSystems, 33:870\u2013882, 2020.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-\nhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and\nE. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,\n12:2825\u20132830, 2011.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint\narXiv:2212.09748, 2022.\n13\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe\nPenna, and Robin Rombach. Sdxl: improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. arXiv, 2022.\nGuocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-\nYing Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al.\nMagic123: One image\nto high-quality 3d object generation using both 2d and 3d diffusion priors.\narXiv preprint\narXiv:2306.17843, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\nJeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and\nDavid Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d cat-\negory reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 10901\u201310911, 2021.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models, 2021.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\ntion Processing Systems, 35:36479\u201336494, 2022.\nAditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Ka-\nmal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18603\u201318613,\n2022.\nAditya Sanghi, Rao Fu, Vivian Liu, Karl DD Willis, Hooman Shayani, Amir H Khasahmadi, Srinath\nSridhar, and Daniel Ritchie.\nClip-sculptor: Zero-shot generation of high-fidelity and diverse\nshapes from natural language. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 18339\u201318348, 2023.\nJohannes Lutz Sch\u00a8onberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise\nview selection for unstructured multi-view stereo. In European Conference on Computer Vision\n(ECCV), 2016.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models.\nAdvances in Neural\nInformation Processing Systems, 35:25278\u201325294, 2022.\nIvan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Peter Wonka. Epigraf: Rethinking training\nof 3d gans. Advances in Neural Information Processing Systems, 35:24487\u201324501, 2022.\nEdward J Smith and David Meger.\nImproved adversarial systems for 3d object generation and\nreconstruction. In Conference on Robot Learning, pp. 87\u201396. PMLR, 2017.\n14\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Interna-\ntional Conference on Learning Representations, 2021. URL https://openreview.net/\nforum?id=St1giarCHLP.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nAlex Trevithick and Bo Yang. Grf: Learning a general radiance field for 3d representation and\nrendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\n15182\u201315192, 2021.\nA\u00a8aron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.\nIn International conference on machine learning, pp. 1747\u20131756. PMLR, 2016.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jaco-\nbian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12619\u201312629, 2023a.\nQianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Bar-\nron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-\nview image-based rendering. In CVPR, 2021.\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-\ndreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv\npreprint arXiv:2305.16213, 2023b.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International\nConference on Learning Representations, 2021.\nJiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a proba-\nbilistic latent space of object shapes via 3d generative-adversarial modeling. Advances in neural\ninformation processing systems, 29, 2016.\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields\nfrom one or few images. In CVPR, 2021.\nXiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten\nKreis. Lion: Latent point diffusion models for 3d shape generation. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2022.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In CVPR, 2018.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,\nPing Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\nLinqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel\ndiffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\n5826\u20135835, 2021.\nA\nAPPENDIX\nA.1\nDIVERSITY OF GENERATION\nInheriting the generation capability from the base SDXL model, our method can generate diverse\nresults from the same text prompt by using different random seeds in the feed-forward pass. As\nshown in Figure 6, our approach excels in generating diverse 3D assets featuring strikingly distinct\n15\nFigure 6: Our method can generate diverse results from the same text prompt.\ntextures and geometries from the same prompt. This is in contrast to previous SDS-optimization\nbased methods, which are prone to generate similar results even with different initializations (Poole\net al., 2022).\nA.2\nDATA CURATION DETIALS\nWe apply a quality scorer to curate high-quality data from the Objaverse dataset. To train the quality\nscorer, we first randomly sample 2000 3D objects from the dataset and manually label each 3D asset\nas good or bad. Good assets have realistic textures and complex geometry, while bad ones have\nsimple shapes and flat or cartoon-like textures. This criterion is subjective and imprecise, but we\nfound it good enough for the purpose of data filtering.\nSince the amount of annotated data is limited, we use a pretrained CLIP (Radford et al., 2021) model\nto extract high-level image features of rendered images at 5 randomly sampled camera viewpoints\nfor each object. Then we train a simple binary SVM classifier on top of the averaged CLIP features\nover different views. We use the NuSVC implementation from the popular scikit-learn framework\nPedregosa et al. (2011), which also gives us a probability estimation of the classification. We use the\ntrained SVM model to predict the classification probability for all objects in the dataset by extracting\nCLIP features in the same way as done for the training data. These probabilities are used as scores\nto rank the data from high to low quality. Finally, we use the top 10K objects as our fine-tuning data.\nTo render the 4-view data, we scale the curated objects and center them at a cube [\u22121, 1]3. We\nrender the objects with a white background following the structured setup discussed in Section 3.1\nusing a field of view 50\u25e6 at a distance of 2.7 under uniform lighting. We use the physically-based\npath tracer Cycles in Blender for rendering.\nIn Figure 7 we show qualitative comparisons on results from models trained with curated data and\nrandom data. Models trained with random data tend to generate cartoon-like 3D assets with simple\nand flat textures. This is not surprising since a bulk of the Objaverse dataset contains simple shapes\nwith simple textures, and without curation these data will guide the model to over-denoise the results,\nleading to large areas of flat colors. On the contrary, models trained with curated data tend to\ngenerate more photorealistic assets with complex textures and geometries.\nA.3\nSDXL FINE-TUNING DETAILS\nWe use SDXL as the base model for our first-stage fine-tuning. We use AdamW optimizer with a\nfixed learning rate 10\u22125, \u03b21 = 0.9, \u03b22 = 0.999 and a weight decay of 10\u22122. We fine-tune the model\nusing fp16 on 32 NVIDIA A100 GPUs with a total batch size of 192. No gradient accumulation is\nused. We train the model on 10K curated data for 40K steps, which takes around 3 hours.\nWe train the model with the standard denoising diffusion loss (Ho et al., 2020)\nL(\u03b8) = Et,x0,\u03f5\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(\u221a\u03b1tx0 +\n\u221a\n1 \u2212 \u03b1t\u03f5, t)\u22252\u0003\n(1)\nwhere \u03f5\u03b8 is the denoising U-Net and \u03b8 are the trainable parameters.\nSDXL introduces image resolution and aspect ratio conditioning that allow mixing training on im-\nages of different resolutions and aspect ratios. As for our training data, we render 4 views each with\na resolution of 512x512 and assemble them into a 1024x1024 image. Therefore we fix the resolution\nand aspect ratio conditioning to be (1024, 1024) throughout the fine-tuning procedure. We don\u2019t do\n16\nrandom cropping in our training and fixed the crop conditioning to be (0, 0). All the other training\nsetups are identical to the original SDXL.\nA.4\nSD1.5 FINE-TUNING DETAILS\nWe use 8 A100 GPUs for fine-tuning SD1.5 on 100K data with a total batch size of 64. We use the\nsame AdamW optimizer as the one for SDXL with the same hyper-parameters. We also use gradient\naccumulation of 3 steps, which gives an effective batch size of 192. The training loss is the same as\nSDXL. We train the model for 120K steps (40K parameter updates due to gradient accumulation),\nwhich takes roughly 33 hours.\nA.5\nGAUSSIAN BLOBS INITIALIZATION\nSince the diffusion model is fine-tuned with only a relatively small number of steps, it still largely\npossesses the original denoising behavior on images that are not in the form of 2 \u00d7 2 grids and do\nnot have a white background. Naively applying the standard backward denoising process starting\nfrom random Gaussian noise will likely lead to results far from the distribution of the fine-tuning\ndata (see Figure 5).\nThe spatial structure of the training images is simple: four views of the same object are placed at\nthe center of each quadrant. Also, the background is always white. Since the model is fine-tuned\non such data with a denoising objective, it is natural that, when presented with a noisy input whose\nunderlying clean image has these two characteristics, the model will tend to denoise the image to\na clean one where the four-quadrant objects are view consistent. Following this, and inspired by\nSDEdit Meng et al. (2022), we introduce Gaussian blobs initialization to guide the model toward\ngenerating samples consistent with the distribution of the fine-tuning data.\nThe standard latent diffusion inference starts with a Gaussian noise image \u03f5 with the same size as\nthe image latents. Instead, we modify the initial iteration to be a composition of Gaussian noise and\nan image with the two aforementioned characteristics: object quadrants and white background. We\nconstruct such an image by generating a grayscale image with a clean white background and a black\nGaussian blob at the center. Specifically, we construct a H \u00d7 W grayscale image I, where H and\nW are the height and width of the input RGB image with a value range [0, 1]. For all our models\nH = W, and we denote them using S. For a given pixel (x, y), its pixel value is computed as\nI(x, y) = 1 \u2212 exp\n\u0012\n\u2212 (x \u2212 S/2)2 + (y \u2212 S/2)2\n2\u03c32S2\n\u0013\n(2)\nwhere \u03c3 is a hyper-parameter controlling the width of the Gaussian blob. Such an image looks like a\nblack disc at the center of a white image slowly fading away toward the edges of the image. We then\nassemble four such images into a 2 \u00d7 2 image grid. Some examples of such images with different \u03c3\ncan be seen at the first row of figure 5.\nNext we construct the initial noise for the denoising step by blending a complete Gaussian noise\nlatent with the latent of the Gaussian blobs. We denote the latent of the Gaussian blobs image I as\n\u02dcI, and the latent of a noise image with i.i.d. Gaussian values as \u03f5. For a N step denoising inference\nprocess with timesteps {tN, tN\u22121, ..., t0}, we mix the two latents with a weighted sum\n\u03f5tN =\np\n\u03b1tN \u02dcI +\np\n1 \u2212 \u03b1tN \u03f5\n(3)\nThen \u03f5tN is used as the initial noise of the denoising process, e.g., tN is 980 for a denoising step\nwith 50 (and the total number of timesteps is 1000).\nA.6\nSPARSE-VIEW RECONSTRUCTION DETAILS\nModel details\nWe use the DINO-ViT-B/16 as our image encoder. This model is transformer-\nbased, which has 12 layers and the hidden dimension of the transformer is 768. The ViT begins with\na convolution of kernel size 16, stride 16, and padding 0. It is essentially patchifying the input image\nwith a patch size of 16 \u00d7 16. For our final model, the input image resolution is 512, thus it leads to\n32 \u00d7 32 = 1024 spatial tokens in the vision transformer. In ablation studies, we reduce the input\nresolution from 512 to 256 to save compute budget. The original DINO is trained with a resolution\n17\nTable 4: Ablation study of the sparse-view reconstruction model.\n#Layers\nRender\nSupervision\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nexp01\n6\n64\nAll\n23.6551\n0.8616\n0.1281\nexp02\n12\n64\nAll\n23.8257\n0.8631\n0.1266\nexp03\n24\n64\nAll\n23.8351\n0.8635\n0.1258\nexp04\n12\n32\nAll\n23.1704\n0.8561\n0.1358\nexp05\n12\n64\nw/o novel\n18.2359\n0.8103\n0.2256\nexp06\n12\n64\nw/o LPIPS\n24.1699\n0.8641\n0.1934\nof 224 \u00d7 224, thus the positional embedding has only a size of 14 \u00d7 14 = 196. We thus use 2D\nbilinear extrapolation (with torch.nn.functional.interpolate function) to extrapolate\nit to the desired token size.\nTo integrate camera information into the image encoder, we inject modulation layers (Peebles &\nXie, 2022) into each of the transformer layer (for both self-attention layers and MLP layers). The\nmodulation layer is initialized to be an identity mapping and thus it is suitable to be added to a\npre-trained vision transformer.\nAfter the image encoder, we have 1025 image feature tokens for each image, since we also include\nthe output of the [CLS] token. We concatenate the tokens from all four images to construct a\nsequence of condition features of length 4100. This condition feature will be used to create the keys\nand values in the cross-attention layers of the image-to-triplane transformer decoder.\nThe image-to-triplane transformer decoder starts with a token sequence of (3 \u00d7 32 \u00d7 32) \u00d7 1024,\nwhere (3 \u00d7 32 \u00d7 32) is the number of tokens and 1024 is the hidden dimension of the transformer.\nWe use 16 layers in our transformer decoder. All attention layers have 16 attention heads and each\nhead has a dimension of 64. We remove the bias term in the attention layer as in Touvron et al.\n(2023). We take the pre-normalization architecture of the transformer where each sub-layer will be\nin the format of x + f(LayerNorm(x)).\nAfter the transformer, we apply a de-convolution layer to map the transformer output from (3\u00d732\u00d7\n32)\u00d71024 to 3\u00d7(64\u00d764)\u00d780. It means that there are 3 planes (XY, YZ, XZ) (Chan et al., 2022)\nand each plane has a size of 64 \u00d7 64. The dimension of each plane is 80. All three planes share the\nsame deconvolution layer. The deconvolution is of kernal size 2, stride 2, and pad 0.\nIn NeRF volumetric rendering, the features from the three planes are bilinearly interpolated and\nconcatenated to get a 240-dimensional feature for each point. Then, we have a 10-layer MLP with a\nhidden dimension of 64 to map this 240-dim feature to a 4-dim feature. The first three dimensions\nwill be treated as RGB colors of the point and normalized to [0, 1] with a sigmoid function. The last\ndimension will be treated as the density value and we use an exponential function to map the MLP\u2019s\noutput to be non-negative.\nFor the exact formulation of the above operators, please refer to LRM (Anonymous, 2023) and\nDiT (Peebles & Xie, 2022).\nTraining details.\nWe adopt the AdamW (Kingma & Ba, 2014; Loshchilov & Hutter, 2017) opti-\nmizer to train our model. We use a peak learning rate of 4 \u00d7 10\u22124 with a linear warm-up (on the\nfirst 3K steps) and a cosine decay. We change the \u03b22 of the AdamW optimizer to 0.95 for better\nstability. We use a weight-decay of 0.05 for non-bias and non-layernorm parameters. We also apply\na gradient clipping of 1.\nFor the initialization of the image encoder, we use the official DINO pre-trained weight. For the\ninitialization of the triplane decoder, and NeRF MLP, we use the default initializer in the PyTorch\nimplementation. We empirically found that the pre-normalization transformer is robust to different\ninitialization of linear layers. For the positional embedding of the triplane tokens in the transformer\ndecoder, we initialize them with a Gaussian of zero-mean and std of 1/\n\u221a\n1024.\nFor each training step, we randomly sample 4 views as input and another 4 as supervision without\nreplacement. The number of sample points per ray in NeRF rendering is 128, which are uniformly\ndistributed along the segment within the [\u22121, 1]3 bounding box. The rendering resolution is 128 \u00d7\n128. To allow higher actual supervising resolution, we first resize the image to a smaller resolution\n18\n(uniformly sampled from [128, 384]) and then crop a patch of 128 \u00d7 128 from it. Thus we can go\nbeyond the rendering resolution of 128.\nWe utilize flash attention (Dao et al., 2022), mixed-precision training (with bf16 as the half-precision\nformat) (Micikevicius et al., 2018), and gradient checkpointing (Chen et al., 2016) to improve the\ncompute/memory efficiency of the training.\nWe perform the training for 120 epochs on our rendered Objaverse data with a training batch size of\n1024. We use both L2 loss and LPIPS loss to supervise the model and the weights of the two losses\nare 1 and 2 respectively. The model is trained on 128 NVIDIA A100 GPUs and the whole training\ncan be finished in 7 days.\nA.7\nSPARSE VIEW RECONSTRUCTION ABLATION STUDY\nWe conduct an ablation study of our sparse-view reconstruction model to validate different design\nchoices including the number of layers in the image-to-triplane decoder, the rendering resolution and\nthe losses used during training, and the usage of novel view supervision. We train the model on the\nsame dataset as our final model, however, we change the training recipe to reduce the computation\ncost to 32 A100 GPUs for 1 day. The changes of configuration for ablation include (1) a resolution\nof 256 \u00d7 256 for the input image resolution, (2) 96 points per ray during rendering, (3) 5 layers\ninstead of 10 layers in the NeRF MLP, (4) 30 epochs of training.\nTo evaluate the performance of different variants, we test them on another 3D dataset Google\nScanned Object (GSO) (Downs et al., 2022). For each object in GSO, we render a set of 64-view\nimages rendered with a resolution of 512 \u00d7 512 at elevations 0\u25e6, 20\u25e6, 40\u25e6, 60\u25e6. Each elevation has\n16 views with equidistant azimuths starting from 0. We use 4 views with elevation 20\u25e6 and azimuths\n45\u25e6, 135\u25e6, 225\u25e6, 315\u25e6 as input, and randomly sample 5 views from the remaining views as our\ntesting set, which stay the same for different variants. We render the 5 testing views and report their\ndifference from the ground truth using 3 metrics including PSNR, SSIM and LPIPS. These metrics\nare averaged over all 1019 objects in the GSO dataset.\nThe results of the ablation studies are in Figure 4. From the table we can see that the model is robust\nto the number of transformer layers in the image-to-triplane decoder as shown in exp01, exp02, and\nexp03. We also observe that the LPIPS loss can largely affect the results by comparing the exp02\nand exp06. Without the LPIPS loss, the model drops a lot on the LPIPS metric while getting a\nslight improvement on PSNR and SSIM. However, we empirically find that LPIPS is much more\naligned with human perception and the rendered images become blurry without it. The rendering\nresolution is also important (as shown in exp04) since LPIPS can be more robust and accurate at a\nhigher resolution, which also motivates us to use a rendering resolution of 128 by 128 when training\nour final model.\nAlso, the inclusion of novel view supervision in the training is critical as shown in exp05. All three\nmetrics got a significant drop when only supervising the four input views. Upon reviewing the\nresults, we find that it\u2019s due to the insufficient coverage of the four views, which typically leads to\nfloaters in regions not covered by the input views.\nA.8\nEXTENSION TO IMAGE-CONDITIONED GENERATION\nOur method can also be extended to support additional image conditioning to provide more fine-\ngrained control over the 3D model to be generated. In this process, the input to the model includes\nan input text prompt that describes the object to be generated as well as an image of the object. We\nuse the same training data as our text-conditioned model. During training, for a randomly sampled\ntime step, we keep the latent of the input image (top-left quadrant) untouched and only add noise to\nthe latents of the remaining three views. This allows the diffusion model to generate the other views\nwhile accounting for the conditioning image. During inference, similarly, we replace the upper\nleft quadrant of the latent feature with the latent of the clean conditioning image at each iteration.\nFigure 8 shows some visual results of our image-conditioned model. From the results we can see\nthat our method is able to effectively generate the other views with faithful details that are coherent\nwith the input text prompt and image, thus giving us high-quality 3D models with our sparse view\nreconstructor.\n19\nCurated\nRandom\na hippo wearing a \nsweater\na panda wearing a \nnecktie and sitting \nin an office chair \na stack of pancakes \ncovered in maple syrup\na gummy bear \ndriving a convertible\nFigure 7: Comparisons on novel view renderings of NeRF assets generated from SDXL models fine-\ntuned with 10K curated data and random data. We can see that that curated data enables the model\nto generate more photorealistic 3D assets with more geometric and texture details. Here curated and\nrandom correspond to Exp d (Curated-10K-s10K) and i (Random-10K-s10K) in Table 3.\nA.9\nLIMITATIONS\nWhile our model can generate high-quality and diverse 3D assets, it still suffers from several limita-\ntions. First, while we perform a light-weight fine-tuning that enables the model to mostly preserve\nthe capability of the SDXL model in textual understanding and generation, we do observe that our\nmodel fails to handle some over-complicated prompts, for example, those related to complex spatial\narrangements of multiple subjects and complex scenes (see Figure 15). In addition, the generated\nassets are not as photorealistic as the 2D images generated by the original SDXL, which may be\nattributed to the information loss in the fine-tuning stage. Secondly, there is a lack of 3D inductive\nbias when generating multi-view images, and therefore it\u2019s still possible for our model to generate\ninconsistent images that result in low-quality 3D assets with corrupted geometries and textures. Fi-\nnally, our feed-forward reconstructor tends to generate blurry textures compared to the input images\ndue to the usage of a relatively low-resolution triplane.\n20\nInput  image\nOur 2x2 grid\nOurs\nZero-1-to-3\nOne-2-3-45\nFigure 8: Comparison to previous methods on single image-conditioned 3D generation. We com-\npared to previous methods Zero-1-to-3 (Liu et al., 2023b) and One-2-3-45 (Liu et al., 2023a). Our\nmethod can faithfully generate the details in the invisible regions, thus empowering us to reconstruct\n3D assets of higher quality than baseline methods. All input images are generated with a public\ntext-to-image platform Adobe Firefly (Adobe, 2023).\n21\nppendix blobs\nSigma=0.0\nSigma=0.01\nSigma=0.1\nSigma=0.2\nSigma=0.3\nFigure 9: 2 \u00d7 2 grid images generated with Gaussian blobs of different sigma \u03c3.\n22\n10 steps\n(6 seconds)\n20 steps\n(8 seconds)\n50 steps\n(15 seconds)\n100 steps\n(20 seconds)\na blue poison-dart frog sitting on a water lily\nbeautifully carved jade heavenly Chinese guardian lion, hand carved nephrite jade\na cute dragon with big eyes\nFigure 10: Comparison on the NeRF assets generated with different numbers of DDIM steps and\ntheir inference time. While we use 100 steps in our experiments that take 20 seconds to generate a\nNeRF asset, we find that using a smaller number of steps can also give us results of similar quality\nwith a much shorter inference time.\n23\na plush toy of a corgi nurse\na chimpanzee holding a cup of hot coffee\na hippo made out of chocolate\na red-eyed tree frog\nOurs\nProlificDreamer\nFigure 11: SDS optimization-based methods such as ProlificDreamer (Wang et al., 2023b) can pos-\nsibly suffer from the Janus problem, which greatly degrades the quality of the 3D assets. In contrast,\nour method can mostly get rid of this problem.\na tray of sushi \ncontaining pugs \na squirrel dressed \nlike a clown\nbaby elephant jumping \non a trampoline\na bear dressed as a\nlumberjack\nSDXL\nSD1.5\nFigure 12: Comparisons on the quality of the NeRF assets generated with fine-tuned SDXL and\nSD1.5 models. SDXL has a model size that is three times larger than SD1.5 and thus has better text\ncomprehension. As shown in the figure, the 3D assets generated by our fine-tuned SDXL have better\nphoto-realism and text alignment. The used SDXL and SD1.5 models are from Exp d (Curated-10K-\ns10K) and m (Curated-100K-s40K) in Table 3.\n24\n1K\n10K\n100K\na brightly colored \nmushroom \ngrowing on a log\na dalmatian wearing\na fireman\u2019s hat\na teal moped\na pug wearing a bee \ncostume\nFigure 13: Comparison on the effect of different fine-tuning data sizes. Training on too little data\nsuch as 1K results in inconsistency between the generated 4 views, thus resulting in incorrect geom-\netry. On the other side, training on too much data such as 100K makes the model biased toward the\nfine-tuning dataset, thus negatively affecting the quality of generated 3D assets. Here 1K, 10K and\n100K correspond to Exp a (Curated-1K-s1K), d (Curated-10K-s10K) and g (Curated-100K-s40K)\nin Table 3 respectively.\na beagle eating a \ndonut\na bear dancing \nballet\na blue motorcycle\na chihuahua \nwearing a tutu\n20K steps\n10K steps\n4K steps\nFigure 14: Comparison on different numbers of fine-tuning steps. 4K training steps lead to incon-\nsistent 4-view generation, while 20K result in biasing towards the fine-tuning data. In contrast, 10K\nachieve a balance between these two. Here 4K, 10K and 20K correspond to Exp c (Curated-10K-\ns4K), d(Curated-10K-s10K) and e (Curated-10K-s20K) in Table 3.\n(c) a 20-sided die made out of \nglass\n(a) a kitten standing on top of \na giant tortoise\n(d) a panda wearing a chef's hat and \nkneading bread dough on a countertop\n(b) two raccoons playing poker\nFigure 15: Some examples of our failure cases. (a) Incorrect understanding of compositional con-\ncepts. (b) Inability to generate the exact quantity. (c) Fail to generate objects with complex struc-\ntures. (d) Missing important concepts in the prompt.\n25\n"
  },
  {
    "title": "FinGPT: Large Generative Models for a Small Language",
    "link": "https://arxiv.org/pdf/2311.05640.pdf",
    "upvote": "26",
    "text": "FinGPT: Large Generative Models for a Small Language\nRisto Luukkonen \u2020\u2217\nVille Komulainen \u2020\nJouni Luoma \u2020\nAnni Eskelinen \u2020\nJenna Kanerva \u2020\nHanna-Mari Kupari \u2020\nFilip Ginter \u2020\nVeronika Laippala \u2020\nNiklas Muennighoff \u2021\nAleksandra Piktus \u2021\nThomas Wang \u2021\nNouamane Tazi \u2021\nTeven Le Scao \u2021\nThomas Wolf \u2021\nOsma Suominen \u22c4\nSamuli Sairanen \u22c4\nMikko Merioksa \u22c4\nJyrki Heinonen \u22c4\nAija Vahtola \u22c4\nSamuel Antao \u25e6\nSampo Pyysalo \u2020\u2217\n\u2020 TurkuNLP Group, University of Turku\n\u2021 Hugging Face\n\u22c4 National Library of Finland\n\u25e6 AMD\n\u2217risto.m.luukkonen@utu.fi, sampo.pyysalo@utu.fi\nAbstract\nLarge language models (LLMs) excel in many\ntasks in NLP and beyond, but most open mod-\nels have very limited coverage of smaller lan-\nguages and LLM work tends to focus on lan-\nguages where nearly unlimited data is avail-\nable for pretraining. In this work, we study the\nchallenges of creating LLMs for Finnish, a lan-\nguage spoken by less than 0.1% of the world\npopulation. We compile an extensive dataset\nof Finnish combining web crawls, news, social\nmedia and eBooks. We pursue two approaches\nto pretrain models: 1) we train seven mono-\nlingual models from scratch (186M to 13B\nparameters) dubbed FinGPT, 2) we continue\nthe pretraining of the multilingual BLOOM\nmodel on a mix of its original training data\nand Finnish, resulting in a 176 billion param-\neter model we call BLUUMI. For model eval-\nuation, we introduce FIN-bench, a version of\nBIG-bench with Finnish tasks. We also assess\nother model qualities such as toxicity and bias.\nOur models and tools are openly available at\nhttps://turkunlp.org/gpt3-finnish.\n1\nIntroduction\nNeural language models based on the Transformer\narchitecture (Vaswani et al., 2017) have revolution-\nized Natural Language Processing (NLP) in recent\nyears, advancing the state of the art in tasks ranging\nfrom text classification to open-ended text gener-\nation. Generative, decoder-only language mod-\nels such as the Generative Pretrained Transformer\n(GPT) (Radford et al., 2018) series have been a\nparticular focus of interest in part due to their multi-\ntask and few-shot capabilities (Radford et al., 2019;\nBrown et al., 2020). The ability of such models to\nimplicitly learn to perform tasks that they have not\nbeen directly trained on has been considered to be\nclosely tied to the scale of the model (Brown et al.,\n2020; Chowdhery et al., 2022) and, perhaps even\nmore importantly, to the number of training tokens\n(Hoffmann et al., 2022; Muennighoff et al., 2023b;\nTouvron et al., 2023). Most work on such models\nfocuses on English, often entirely excluding other\nlanguages, and assumes that hundreds of billions\nof tokens of text are readily available for model\ntraining.\nIn this study, we consider the challenges of in-\ntroducing large generative models for Finnish, a\nUralic language natively spoken by fewer than 6\nmillion people. While the language is compara-\ntively well represented in online resources relative\nto this number, less than 1% of texts available in\ne.g. Wikipedia and Common Crawl are Finnish\n(Pyysalo et al., 2021; Xue et al., 2021). As the\nother members in the language family are either\neven smaller and lesser-resourced or quite distant,\nthe resources for creating models for the language\nare quite limited. Finnish has been represented to\nsome degree in Transformer-based models since\nthe release of the original multilingual BERT model\n(Devlin et al., 2019), and a dedicated monolingual\nBERT for the language was previously created by\nVirtanen et al. (2019). Also some generative mod-\nels for Finnish have been previously introduced by\nthe \"Finnish-NLP\" group1 and Hatanp\u00e4\u00e4 (2022),\nbut as training LLMs is very expensive and Finnish\nis constrained by the size of available data, models\nexceeding a billion parameters have been so far\nmissing from the Finnish NLP landscape.\nWe compile a broad-coverage dataset of Finnish\nand train monolingual models up to 13 billion pa-\nrameters for 300 billion tokens (approx. 8 epochs).\nWe also perform continued pretraining of the 176-\nbillion parameter BLOOM model (Scao et al.,\n2022a) to extend its coverage of Finnish, intro-\nduce novel evaluation datasets, and assess multiple\n1https://huggingface.co/Finnish-NLP\narXiv:2311.05640v1  [cs.CL]  3 Nov 2023\nModel\nLayers\nDim\nHeads\nParams\nSmall\n12\n768\n12\n186M\nMedium\n24\n1024\n16\n437M\nLarge\n24\n1536\n16\n881M\nXL\n24\n2064\n24\n1.5B\n3B\n32\n2560\n32\n2.8B\n8B\n32\n4096\n32\n7.5B\n13B\n40\n5120\n40\n13.3B\nBLUUMI\n70\n14336\n112\n176B\nTable 1: Architectures of our models.\naspects of the resulting models. While the details\nof our data collection and processing are somewhat\nspecific to Finnish, we believe that our study can\nserve as a template for training large models for\nother small languages.\n2\nModels\nOur models are based on the GPT architec-\nture (Radford et al., 2019) and we follow the pre-\ntraining approach developed for the BLOOM fam-\nily of large multilingual language models (Scao\net al., 2022a). We train monolingual Finnish mod-\nels with up to 13 billion parameters from scratch,\nfollowing GPT-3 (Brown et al., 2020) in terms of\nthe number of layers, dimensionality, and num-\nber of attention heads (Table 1), and BLOOM in\nterms of both the software implementation as well\nas specific design choices such as the use of Al-\nibi position embeddings (Press et al., 2021) and\nlayer normalization (Scao et al., 2022b). We ad-\nditionally continue the pretraining of the original\n176-billion parameter BLOOM model with a mix\nof its original pretraining corpus and Finnish data\nto create a model we call BLUUMI. While the\nBLOOM models were trained on data from 46\ndifferent languages, the training did not include\nFinnish. Prior work has investigated extending\nsmaller BLOOM models to new languages not in-\ncluded during pretraining (Yong et al., 2022) and\nfound parameter-efficient finetuning methods and\n(to a lesser degree) continued pretraining to be ef-\nfective approaches. Due to the fact that the 176-\nbillion parameter BLOOM model has been signifi-\ncantly undertrained for its parameter count (Hoff-\nmann et al., 2022; Muennighoff et al., 2023b), we\nfocus on continued pretraining in this study.\n3\nData\nWe next present the sources of training data, pre-\nprocessing steps, data statistics and analysis.\n3.1\nData sources\nWe draw on a broad range of text sources, aiming\nto cover a wide range of linguistic variation across\ngenres, registers, authors and time periods. The\npretraining data sources are listed in Table 2 and\ndescribed below, and a summary of the timespans\nthey cover is given in Appendix A.\nParsebank The Finnish Internet Parsebank (Lu-\notolahti et al., 2015) is a 6 billion token corpus\nof Finnish collected in 2015-2016 from Common\nCrawl and a targeted Internet crawl seeded by\nthe .fi domain registry content and all URLs of\nFinnish material in Common Crawl. The texts\nhave been deduplicated at the paragraph level us-\ning Onion (Pomik\u00e1lek, 2011) and cleaned using the\njusText library.2\nmC4 The multilingual colossal, cleaned version of\nCommon Crawl\u2019s web crawl corpus (mC4) was in-\ntroduced by Xue et al. (2021) for training the mT5\nmodels. mC4 was derived from the 71 web scrapes\n(2013-2020) released by Common Crawl prior to\nthe creation of the corpus. We use the Finnish sub-\nset of mC4 as identified by cld33, which contains\n8 billion tokens across 19 million documents.\nCC-Fi To maximize coverage of Finnish text in\nCommon Crawl resources, we applied a custom\nextraction process to all crawls from 2013-2022,\nemphasizing recall of Finnish.4 We extracted texts\nusing Trafilatura (Barbaresi, 2021) and performed\nexact document-level deduplication using Mur-\nmurHash prior to the general preprocessing steps\ndescribed below. This processing produced 55 mil-\nlion documents totaling 20 billion tokens.\nFiwiki The Finnish portion of the Wikipedia free\nencyclopedia consists of approximately 180,000\nopenly licensed articles created by volunteer ed-\nitors. For this work, we extracted text from the\n20221120 dump of the Finnish Wikipedia using\nWikiExtractor (Attardi, 2015), producing a dataset\nof 110 million tokens.\nL\u00f6nnrot Projekti L\u00f6nnrot5 is a project digitizing\nout-of-copyright Finnish and Swedish literature.\nFor this work, we used the 2574 Finnish works that\nwere published by Projekti L\u00f6nnrot by the start of\npretraining, which contain a total of 125 million\ntokens.\nYle Archives of the national public broadcasting\n2https://github.com/miso-belica/jusText\n3https://github.com/google/cld3\n4Appendix B provides a comparison of the two datasets\nderived from Common Crawl.\n5http://www.lonnrot.net/\nAbbrev.\nName\nReference\nParsebank\nFinnish Internet Parsebank\nhttps://turkunlp.org/finnish_nlp.html\nmC4\nmultilingual colossal, cleaned Common Crawl\nhttps://huggingface.co/datasets/mc4\nCC-Fi\nCommon Crawl Finnish\nhttps://github.com/TurkuNLP/CC-Fi\nFiwiki\nFinnish Wikipedia\nhttps://fi.wikipedia.org/wiki\nL\u00f6nnrot\nProjekti L\u00f6nnrot\nhttp://www.lonnrot.net\nePub\nNational library \"epub\" collection\nhttps://kansalliskirjasto.finna.fi\nLehdet\nNational library \"lehdet\" collection\nhttps://kansalliskirjasto.finna.fi\nSuomi24\nThe Suomi 24 Corpus 2001-2020\nhttp://urn.fi/urn:nbn:fi:lb-2021101527\nReddit-Fi\nReddit r/Suomi submissions and comments\nhttps://www.reddit.com/r/Suomi\nSTT\nFinnish News Agency Archive 1992-2018\nhttp://urn.fi/urn:nbn:fi:lb-2019041501\nYle\nYle Finnish News Archive 2011-2018\nhttp://urn.fi/urn:nbn:fi:lb-2017070501\nYle Finnish News Archive 2019-2020\nhttp://urn.fi/urn:nbn:fi:lb-2021050401\nYle News Archive Easy-to-read Finnish 2011-2018\nhttp://urn.fi/urn:nbn:fi:lb-2019050901\nYle News Archive Easy-to-read Finnish 2019-2020\nhttp://urn.fi/urn:nbn:fi:lb-2021050701\nROOTS\nResponsible Open-science Open-collaboration Text Sources\nhttps://huggingface.co/bigscience-data\nTable 2: Data sources.\ncompany of Finland (Yle) are available for research\nthrough the Language Bank of Finland6. We use\nthe complete Yle archives available at the start of\nour model pretraining, which consist of approxi-\nmately 800,000 articles (220 million tokens) from\n2011-2020, of which 0.3% are easy-to-read news.\nSTT As for Yle, archives of the Finnish News\nAgency (Suomen Tietotoimisto or STT) are pro-\nvided for research through the Language Bank of\nFinland. The collection available at the start of\nthis study spans publications from 1992-2018 and\ncontains 2.8 million newswire articles which total\napproximately 300 million tokens.\nePub The National Library of Finland maintains a\ncollection of electronically published books in Fin-\nland. For the purposes of this project, the library\ngranted access to its ePub collection of approxi-\nmately 30,000 Finnish eBook contents. As these\nbooks remain copyrighted, it is not possible to re-\ndistribute texts from this dataset.\nLehdet The Lehdet dataset is based on archived\nHTML material collected by the National Library\nof Finland and includes daily, weekly and monthly\ncrawls of newspaper internet sites and also a yearly\n.fi-domain crawl covering years from 2015 to\n2021. The total cleaned dataset consists of 85 bil-\nlion characters from 60 million HTML documents.\nThe dataset was provided by the National Library\nand can not be redistributed due to copyright.\nSuomi24 Archives of the largest social network-\ning site in Finland, Suomi24,7 are available for\nresearch via the Language Bank of Finland. For\nthis study, we downloaded the complete archives\n6https://www.kielipankki.fi/\n7https://www.suomi24.fi\navailable at the time, consisting of 95 million com-\nments and 5 billion words from 2001-2020.\nReddit-Fi The social site Reddit includes a few pre-\ndominantly Finnish-language discussion forums.\nFor this work, we downloaded Reddit archives8 and\nextracted text from posts to r/Suomi,9 the largest\nsuch forum. The dataset contains over 150,000 sub-\nmissions and nearly 4 million comments (in total\n150 million tokens) from 2009-2022.\nROOTS The Responsible Open-science Open-\ncollaboration Text Sources (ROOTS) dataset (Lau-\nren\u00e7on et al., 2022) consists of 1.6 terabytes of\ntext data spanning 59 languages used for pretrain-\ning BLOOM (Scao et al., 2022a). While Finnish\nwas not included as an official language, a con-\ntamination analysis found 0.03% of ROOTS to be\nFinnish (Muennighoff et al., 2022). We use ROOTS\nin the continued pretraining of the BLOOM model,\nbut not for the monolingual Finnish models.\n3.2\nPreprocessing\nWe next briefly describe the preprocessing steps\nperformed for the source datasets. All processing\nscripts, parameters, and models are available along\nwith detailed statistics at https://github.com/\nTurkuNLP/finngen-tools.\nDeduplication\nIn addition to the deduplication\nsteps already performed for some of the datasets\n(see Section 3.1), we performed approximate N-\ngram overlap-based deduplication using Onion\n(Pomik\u00e1lek, 2011) separately for all datasets. We\nrun Onion with default parameters, marking as du-\nplicate any line of text (paragraph, title, etc.) where\nat least 50% of N-grams have appeared previously.\n8https://files.pushshift.io/reddit/\n9https://www.reddit.com/r/Suomi\nDataset\nChars\nRatio\nWeight\nW.Ratio\nParsebank\n35.0B\n16.9%\n1.5\n22.7%\nmC4-Fi\n46.3B\n22.4%\n1.0\n20.0%\nCC-Fi\n79.6B\n38.5%\n1.0\n34.4%\nFiwiki\n0.8B\n0.4%\n3.0\n1.0%\nL\u00f6nnrot\n0.8B\n0.4%\n3.0\n1.0%\nYle\n1.6B\n0.8%\n2.0\n1.4%\nSTT\n2.2B\n1.1%\n2.0\n1.9%\nePub\n13.5B\n6.5%\n1.0\n5.8%\nLehdet\n5.8B\n2.8%\n1.0\n2.5%\nSuomi24\n20.6B\n9.9%\n1.0\n8.9%\nReddit-Fi\n0.7B\n0.4%\n1.0\n0.3%\nTOTAL\n207.0B\n100.0%\nN/A\n100.0%\nTable 3: Preprocessed data statistics, weights, and ratios\nby source. The data is graphed in Appendix E.\nWe then trim duplicate lines from the beginning and\nend of each document. Finally, if at least 50% of\nthe remaining lines in the document are duplicates,\nwe discard the entire document.\nHeuristic filtering\nTo filter out texts that are un-\nlikely to be Finnish prose text, we apply a set of\nrule-based filters, extending on the heuristics in-\ntroduced by Virtanen et al. (2019). In short, these\nfilters remove texts that have e.g. an unusually high\nratio of punctuation or digits to alphabetic charac-\nters, a high ratio of non-Finnish to Finnish alpha-\nbetic characters, a low type-token ratio, or a low\naverage line length. This step removed only a small\nproportion of texts, with more than 95% of texts\nremaining in most resources.\nN-gram model filtering\nTo further remove texts\nthat have the surface characteristics of prose text\nbut are unlikely to represent standard Finnish, we\napplied a perplexity filter using an N-gram model.\nWe first trained a KenLM (Heafield, 2011) model\non the set of known good Finnish texts prepared by\nVirtanen et al. (2019) for training their FinBERT\nmodel and then applied this model to documents,\nremoving lines with perplexity > 100 000. This\nfilter was not applied to sources estimated to be\npredominantly well-edited text (news, L\u00f6nnrot, and\nWikipedia). For the three web crawl datasets, the\nfilter removed 15-20% of text; for the social media\ndatasets, this proportion was 2-5%.\nToxicity filtering\nTo reduce the proportion of\ntexts that contain e.g. obscenities or identity attacks,\nwe applied the Finnish toxicity detection classifier\nintroduced by Eskelinen et al. (2023). The clas-\nsifier is a FinBERT model (Virtanen et al., 2019)\nfine-tuned on a machine-translated version of the\nRegister\nParsebank\nmC4-Fi\nCC-Fi\nNarrative\n42%\n41%\n31%\nDiscussion\n15%\n7%\n7%\nInformational description\n14%\n13%\n19%\nMachine translation\n<1%\n3%\n4%\nInformational Persuasion\n5%\n10%\n14%\nOpinion\n10%\n7%\n5%\nHow-to\n2%\n3%\n4%\nSpoken\n<1%\n<1%\n<1%\nLyrical\n<1%\n<1%\n<1%\nHybrid\n1%\n1%\n<1%\nNo label\n9%\n13%\n14%\nTable 4:\nRegister proportions in the web-crawled\ndatasets. Hybrid refers to texts predicted with several\nregister labels.\nJigsaw Toxicity dataset10. The filter was not ap-\nplied to news, L\u00f6nnrot books, or Wikipedia. Toxi-\ncity filtering removed 1-5% of sources other than\nCC-Fi, but as much as 23% of the CC-Fi text. This\neffect may be explained by the fact that CC-Fi was\nthe only web source that had not previously been\nfiltered for e.g. obscenity.\nMasking personal data\nWe applied a set of high-\nrecall regular expressions and rule-based scripts to\nmask personal data such as email addresses and\npotential phone numbers. These scripts impacted\napproximately 0.2% of characters in total.\nTokenization\nWe train a new monolingual\nFinnish tokenizer on a sample of the pretraining\ndata using the tokenizers library11. We follow the\nBLOOM recipe for the tokenizer, creating a byte-\nlevel BPE tokenizer without Unicode normaliza-\ntion and use the same regular expression-based\npre-tokenization as in BLOOM. As Finnish is an\nagglutinative language with complex morphology\nand thus a high number of word forms, we chose\nto create a comparatively large vocabulary for a\nmonolingual tokenizer of 131,072 tokens.\n3.3\nData statistics\nThe statistics of the final dataset after preprocess-\ning are presented in Table 3. We oversample open\nand high-quality resources such as L\u00f6nnrot and\nWikipedia. In total, the final pretraining dataset (in-\ncluding oversampling) consists of 38 billion tokens\nwhen processed with our Finnish tokenizer.\n10https://www.kaggle.com/c/\njigsaw-toxic-comment-classification-challenge\n11https://github.com/huggingface/tokenizers\nBatch size\nModel\nSamples\nTokens\nLR\nSmall\n256\n524288\n6.0 \u00d7 10\u22124\nMedium\n256\n524288\n3.0 \u00d7 10\u22124\nLarge\n256\n524288\n2.5 \u00d7 10\u22124\nXL\n512\n1048576\n2.0 \u00d7 10\u22124\n3B\n512\n1048576\n1.6 \u00d7 10\u22124\n8B\n1024\n2097152\n1.2 \u00d7 10\u22124\n13B\n1024\n2097152\n1.0 \u00d7 10\u22124\nBLUUMI\n2048\n4194304\n6.0 \u00d7 10\u22125\nTable 5: Pretraining hyperparameters.\n3.4\nRegister analysis\nWe characterize the contents of the Web-based\ndatasets (mC4, CC-Fi and Parsebank) by automat-\nically analyzing their distribution of text registers\n(or genres) (Biber, 1988). To this end, we apply\na register identification model based on the Fin-\nCORE corpus, trained using XLM-R (Conneau\net al., 2020). The model and corpus were both\npresented by Skantsi and Laippala (2022). The reg-\nister categories present text varieties with different\ncharacteristics and communicative objectives, such\nas narrative, interactive discussion and lyrical. Ta-\nble 4 presents the proportions of the registers in the\nthree datasets. We see a broadly similar register dis-\ntribution across the datasets, with narrative clearly\nmost frequent in all three and categories such as\nhow-to, spoken and lyrical representing only small\nfractions of the total.\n4\nPretraining\nThis work leverages the LUMI supercomputer,12 as\nof this writing the third-largest and seventh greenest\nin the world (Strohmaier et al., 2023). The LUMI\ndata center allows power consumption to be fully\nsupplied with hydroelectricity, and waste heat pro-\nduced by LUMI is utilized by the city of Kajaani,\nproviding up to 20% of the district heating.\nTraining was done on up to 192 nodes, each\nconsisting of 4 AMD Instinct MI250X GPUs, a\nsingle 64-core AMD Trento CPU and 512GB of\nmemory. Since the MI250X GPU is a multi-chip\nmodule with two Graphics Compute Dies (GCDs),\neach node can be considered to have 8 GPUs in\ntotal. In this perspective, the training utilized up to\n1536 GPUs. The 64-core CPU is configured as 4\nNUMA nodes linked to the GPUs. Because of a\n\u201clow noise\u201d mode used on the nodes, only 63 cores\nwere available for training.\n12https://www.lumi-supercomputer.eu/\nLoss\n3.0\n3.5\n4.0\n4.5\n5.0\nTokens\n0K\n50B\n100B\n150B\n200B\n250B\n300B\nSmall\nMedium\nLarge\nXL\n3B\n8B\n13B\n0K\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n2.000\n4.000\n0K 0K 0K\nSmall\nMedium\nLarge\nXL\n3B\n8B\n13B\nFigure 1: Validation losses with 5-point moving average\nsmoothing.\nWe train our models on an adapted version\nof BLOOM\u2019s pretraining framework, Megatron-\nDeepSpeed.13 By combining features from Mega-\ntron (Shoeybi et al., 2019) and DeepSpeed (Rasley\net al., 2020), the Megatron-DeepSpeed framework\ncan be used for training large language models\nwith pipeline, tensor and data parallelization across\nGPUs and compute nodes. Our changes to the\nframework involve making the codebase, includ-\ning its optimized CUDA kernels, usable on AMD\nMI250X GPUs using PyTorch ROCm. To lever-\nage the capabilities of MI250X, ROCm enables\nthe use of GPU matrix cores through its rocBLAS\nand MIOpen library implementations that, in turn,\nare leveraged by PyTorch. PyTorch also leverages\nthe RCCL library to implement distributed collec-\ntives. RCCL also uses a HIP port of the AWS\nOpenFabrics Interface (OFI) plugin 14 to enable\ncommunication directly through to the Slingshot\nfabric provider for improved performance at scale.\nFor the monolingual Finnish models trained\nfrom scratch, we follow Brown et al. (2020) also in\nsetting the batch size and maximum learning rate in\naddition to the model architecture parameters. For\nthe continued pretraining of BLOOM to create the\nBLUUMI model, we retain the original BLOOM\nparameters (Scao et al., 2022a). The pretraining\nparameter values are shown in Table 5.\nFigure 1 shows the loss curves for held-out val-\nidation data for the models trained from scratch,\nshowing a stable pretraining process for all models\nand the expected pattern of larger models achieving\nlower loss.\n13https://github.com/TurkuNLP/\nMegatron-DeepSpeed\n14https://github.com/ROCmSoftwarePlatform/\naws-ofi-rccl\n5\nEvaluation\nWe next present a few-shot evaluation dataset for\nFinnish and compare the capability of the models\nusing this data. We additionally assess model align-\nment, bias, and toxicity in separate evaluations.\n5.1\nFIN-bench dataset\nBIG-bench (Srivastava et al., 2022) is a collection\nof tasks created to assess various aspects of model\ncapabilities. For this study, we created a similar\nFinnish evaluation dataset, FIN-bench,15 based on\na BIG-bench subset augmented with newly intro-\nduced tasks. The tasks were primaly generated by\nmachine translating the text of the equivalent BIG-\nbench tasks and subsequently correcting any trans-\nlation errors as well as assuring that the questions\nremain culturally relevant to Finnish. Exceptions\ninclude the Arithmetic tasks (generated data) and\nnew tasks (Paraphrase, Analogy, Emotions). The\nFIN-bench dataset contains 3919 examples in total,\ndivided over the tasks described briefly below. Ex-\namples of the tasks can be found from Appendix G.\nAnalogy Analogies of the type Paris is to France\nas Helsinki is to ... represent a well-established\napproach for evaluating language models. We cre-\nated an analogy dataset using templates to reformu-\nlate analogy quadruples into natural language ques-\ntions. We created 130 examples from the dataset\nof Venekoski and Vankka (2017) and the data of\nMikolov et al. (2013) translated to Finnish.\nArithmetic tests the degree to which a model has\nacquired an ability to perform basic one- to five-\ndigit addition, subtraction, multiplication and di-\nvision. The Finnish variant of the task was auto-\nmatically generated by manually translating the\ntemplates in the scripts for the corresponding BIG-\nbench task and consists of 1923 examples in total.\nCause and effect evaluates a model\u2019s ability to\nreason about the causality of two events. Each\nexample states two events, the cause and the effect,\nand the model is asked to select the correct ordering.\nThe task consists of 153 examples.\nEmotions evaluates the ability of a model to clas-\nsify sentences according to the emotion that they\nexpress. The task is derived from the XED dataset\n(\u00d6hman et al., 2020) by selecting examples of at\nleast five words that have exactly one emotion label\nand then manually filtering a random selection of\nthese to identify 160 examples that a human an-\n15https://github.com/TurkuNLP/FIN-bench\nnotator without refrerence to specific annotation\ninstructions would be expected to label correctly.\nEmpirical judgments measures how well a model\ncan distinguish sentences that express a causal re-\nlation from ones that express a correlative relation.\nThe task also contains neutral passages of text that\nmimic the structure of the sentences containing a\ncorrelative or causal relation, but do not contain\neither. There are 33 examples of each category in\nthe task, i.e. 99 in total.\nGeneral knowledge measures the ability of models\nto answer simple questions which can easily be\nanswered by most people, such as \u201cHow many legs\ndoes a horse have?\u201d. The task is a translation of the\n70 examples in the BIG-bench original for all but\nthree questions regarding imperial unit conversion,\nwhich we replace with questions on metric units.\nIntent recognition tests the logical reasoning of\nmodels by measuring how well they can recognize\nthe correct intent from an input. The task may be a\ngood predictor of performance in task-oriented dia-\nlogue systems. It includes 693 translated examples\noriginally from the dataset introduced by Coucke\net al. (2018).\nMisconceptions assesses a model\u2019s ability to distin-\nguish popular misconceptions from facts; models\ntrained on increasingly bigger datasets of mixed-\nquality internet data may not discern between com-\nmon assertions and ones that are true. Translations\nof this task were heavily filtered by our annotators\ndue to being considered culturally too U.S.-centric.\nApproximately 40% of the original questions were\nremoved from the dataset, resulting in a task with\n134 examples.\nParaphrase tests whether a model can distinguish\nfull paraphrases from sentences that are merely sim-\nilar. The task was created by selecting 100 positive\nand 100 negative examples from the Finnish Para-\nphrase Corpus (Kanerva et al., 2021), emphasizing\ncases that people can categorize without reference\nto the specifics of the corpus annotation guidelines.\nSentence ambiguity evaluates to what degree a\nmodel can identify whether sentences with inten-\ntionally introduced ambiguous aspects state a true\nor false claim. The task consists of 60 examples\ntranslated from BIG-bench.\nSimilarities abstraction measures a model\u2019s abil-\nity to identify human-like abstract associations be-\ntween objects: for example, a dog and a parakeet\nare similar in that they are both pets. The data\nconsists of 76 multiple-choice questions.\n108\n109\n1010\n1011\nParameter count\n30%\n35%\n40%\n45%\n50%\n55%\n60%\nAggregate normalized performance\nRandom baseline\nTurkuNLP (3-shot)\nTurkuNLP (2-shot)\nTurkuNLP (1-shot)\nTurkuNLP (0-shot)\nHatanp\u00e4\u00e4 (3-shot)\nHatanp\u00e4\u00e4 (2-shot)\nHatanp\u00e4\u00e4 (1-shot)\nHatanp\u00e4\u00e4 (0-shot)\nFinnishNLP (3-shot)\nFinnishNLP (2-shot)\nFinnishNLP (1-shot)\nFinnishNLP (0-shot)\nFigure 2: Overall FIN-bench evaluation results. Detailed per-task results are in Appendix F.\n5.2\nFew-shot results\nWe evaluate models on FIN-bench in zero- to three-\nshot settings and summarize results using mean ac-\ncuracy across all tasks. For tasks that are organized\ninto subtasks (Cause and effect and Arithmetic),\nwe first average over the subtasks before taking\nthe overall average. Primary evaluation results are\nvisualized in Figure 2.\nWe find that our monolingual models at least\nmatch and in most instances outperform the results\nof previously released Finnish models of compara-\nble sizes, lending support to the choices we have\nmade for data selection and preprocessing as well\nas the model architecture and pretraining process.\nThe best performance of the models released previ-\nously for Finnish, 38.5%, is achieved by the largest\nmodel introduced by Hatanp\u00e4\u00e4 (2022). Our best\nmonolingual model outperforms this result by over\n10% points and the BLUUMI model by over 20%\npoints, representing a substantial advance in the\nstate of the art in the capability of generative mod-\nels trained for Finnish.\nAs expected, overall performance generally in-\ncreases with the number of in-context examples\n(zero to three shots) as well as with model size, with\nsome exceptions. First, some small models break\nthe expected pattern, showing better zero-shot per-\nformance than one- to three-shot. This could be\nrelated to a tendency of less capable models to sim-\nply repeat patterns from preceding context, which\ncan lead the models to copy whatever appears after\n\u201cAnswer:\u201d (or equivalent) in the preceding few-shot\nBLOOM\nBLUUMI\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nAccuracy\n0-shot\n1-shot\n2-shot\n3-shot\nFigure 3: BLOOM and BLUUMI performance on FIN-\nbench with random baseline (dotted line).\nexamples. Second, we notice a consistent drop in\nperformance between our 8B and 13B parameter\nmodels. This may be caused by overfitting due\nto an excessive number of parameters and train-\ning steps compared to a relatively small amount of\n(non-repeated) text, which can lead to decreasing\nperformance (Muennighoff et al., 2023b). Based\non these results, we estimate that the 8B parame-\nter model may be our most capable monolingual\nmodel and, more generally, that approximately 10B\nparameters may represent a limit for effectively\ntraining monolingual models of this type for lan-\nguages whose resources are broadly comparable to\nthose available for Finnish.\nTo further evaluate the BLUUMI model, we\ncompared its performance to that of the original\nBLOOM model on FIN-bench (Figure 3) and on\nEnglish tasks from the EleutherAI evaluation har-\n0%\n20%\n40%\n60%\n80%\nAccuracy\nwsc\nwnli\nwinogrande\nwic\nwebqs\ntriviaqa\nsst\nsciq\nrte\nrace\nqqp\nqnli\npubmedqa\nprost\npiqa\nopenbookqa\nmultirc\nmrpc\nmathqa\nlogiqa\nlambada\nhellaswag\nheadqa\ncopa\nboolq\narc_easy\narc_challenge\nTask\nBLOOM\nBLUUMI\nFigure 4: 176B model performance on English evalua-\ntions.\nness (Gao et al., 2021) (Figure 4). We find that\nBLUUMI performs notably better than BLOOM\non FIN-bench tasks on all the few-shot evaluation\ntests, with a 12-18% point accuracy difference in\nfavor of BLUUMI. On the English tasks, we find\nno significant difference in performance between\nthe original BLOOM and BLUUMI (two-sided t-\ntest). These results indicate that the continued pre-\ntraining has succeeded in substantially improving\nthe Finnish capabilities of the model without com-\npromising the existing English capabilities of the\noriginal model.\n5.3\nAlignment\nWe assess model alignment using the BIG-bench\nHHH alignment task (Askell et al., 2021), which\nincludes four categories: harmlessness, honesty,\nhelpfulness, and other. In contrast to most other\ntasks in BIG-bench, both of the two choices in each\nexample can be considered correct: for instance,\nwhen assessing harmlessness, it is undesirable for a\nmodel to provide instructions for violent acts, and\nrefusing to help is considered the correct answer.\nWe create a Finnish version of the HHH alignment\ntask through initial machine tranlation and manual\ncorrection, and evaluate models using the same pro-\ncess as for the other BIG-bench tasks. Results are\nshown in Figure 5. We find that all models perform\npoorly at these tasks, only exceeding the random\nbaseline for the other category and measuring par-\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nFinnishNLP/medium\nFinnishNLP/large\nHatanp\u00e4\u00e4/small\nHatanp\u00e4\u00e4/distill\nHatanp\u00e4\u00e4/xl\nTurkuNLP/small\nTurkuNLP/medium\nTurkuNLP/large\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nHelpful\nHonest\nHarmless\nOther\nFigure 5: HHH-alignment of all models with random\nbaseline (dotted line).\nticularly low for helpfulness. While it is not surpris-\ning that base models that have not been specifically\ntrained to follow instructions or operate in a di-\nalogue context score low at this task, the results\nemhasize the need to align the models to assure\nthat their output is helpful, harmless, and more fac-\ntually accurate. We note that although there appear\nto be some correlations between model size and\nHHH performance, all differences remain within\none standard deviation and are not significant.\n5.4\nBias\nLanguage models have an established tendency\nto repeat or amplify biases present in training\ndata. As one example of bias, female/male gender\nstereotypes in models is a concern because their\nwidespread use can result in further amplifying\nthese biases (Bolukbasi et al., 2016). We assessed\nthe occurrence of such bias using prompts with\nthe structure \u201cThe name of the [professional or oc-\ncupation holder] was\u201d and categorized predicted\nnames into male or female when the name had that\nassociation in 95% of cases in national statistics.\nThe distribution predicted by the model was then\ncompared to the distribution in the most recent\npublished labor data records published by Statis-\ntics Finland in 2020.16 As illustrated in Figure 6\nand detailed in Appendix C, the model broadly re-\nflects the actual labor distribution, indicating that\n16https://tilastokeskus.fi/julkaisu/\ncktws35s04dru0b553lzi7aci\n0%\n20%\n40%\n60%\n80%\n100%\nStatistics\nPredicted\nStatistics\nPredicted\nStatistics\nPredicted\nStatistics\nPredicted\nStatistics\nPredicted\nStatistics\nPredicted\nStatistics\nPredicted\nMale\nFemale\ncargo handler\nsales representative\nhome aid\noffice cleaner\nregistered nurse\npratical nurse\nseller\nFigure 6: Gender bias of 13B model predictions on oc-\ncupation holder vs statistics from the Statistics Finland.\nit has learned this bias from the pretraining data.\nWe note that while this is just one example of a\ntype of bias that our models (as well as most other\npresent-day models) can learn in their pretraining,\nit demonstrates why such models should not be\nnaively applied e.g. for hiring decisions (see also\nLimitations below).\n5.5\nToxicity\nTo test to what degree our models are prone\nto generating toxic content, we follow the un-\nprompted generation approach of Gehman et al.\n(2020), prompting the models with only their end-\nof-sequence (EOS) token to signal the start of a\nnew context.17 The unprompted generations were\nthen classified for toxic content using the model\nintroduced by Eskelinen et al. (2023) (see also Sec-\ntion 3.2) and a small sample manually assessed to\nassure labeling quality. The results of this evalu-\nation are summarized in Figure 7. We find that\nour models more than halve the fraction of gener-\nated toxic content when compared to models from\nHatanp\u00e4\u00e4 (2022), which were trained without fil-\ntering pretraining texts for toxicity. Our models\nnevertheless produce unprompted toxic generations\napprox. 2% of the time, reflecting remaining chal-\nlenges in their alignment.\n6\nDiscussion and conclusions\nIn this study, we compiled an extensive dataset of\nFinnish and created in total eight new large lan-\n17FinnishNLP-models were left out of this evaluation as\nthey appear to have been trained without an EOS token.\n0%\n1%\n2%\n3%\n4%\n5%\nRatio of toxic generations\nHatanp\u00e4\u00e4/small\nHatanp\u00e4\u00e4/xl\nTurkuNLP/small\nTurkuNLP/medium\nTurkuNLP/large\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nFigure 7: Unprompted toxicity of Finnish models. De-\ntailed scores are in Appendix D.\nguage models: seven monolingual Finnish models\nranging from 185 million to 13 billion parameters\nand a multilingual 176-billion parameter model,\nBLUUMI. We additionally introduced a new evalu-\nation dataset, FIN-bench, and evaluated the models\nin few-shot settings as well as specifically assessed\ntheir alignment, bias and toxicity. We found that\nour models are substantially more capable than\nprior Finnish models and that continued pretrain-\ning has greatly improved the Finnish capability of\nBLUUMI without compromising its existing En-\nglish capabilities. We also demonstrated limitations\nof the models in terms of their alignment, incorpo-\nration of bias, and remaining tendency to generate\ntoxic content, which we aim to address in future\nwork. We hope our models will serve as foundation\nmodels for Finnish that can be used in research and\nleveraged through instruction finetuning and other\nalignment methods (Ouyang et al., 2022) to create\na range of capable tools for processing Finnish text.\nIn future work, we hope to continue our study of ef-\nficient and environmentally sustainable approaches\nfor creating capable open foundation models for\nlesser-resourced languages.\nAcknowledgments\nThe authors wish to acknowledge CSC \u2013 IT Cen-\nter for Science, Finland, for generous computa-\ntional resources on the LUMI supercomputer. This\nproject has received funding from the European\nUnion\u2019s Horizon Europe research and innovation\nprogramme under Grant agreement No 101070350\nand the Finnish Research Council, grant number\n331297. The contents of this publication are the\nsole responsibility of its authors and do not neces-\nsarily reflect the opinion of the European Union.\nLimitations\nThe models introduced in this work are trained pre-\ndominantly on data sourced from the internet, and\ndespite our efforts to remove potentially harmful\ntexts from the pretraining data, they carry many\nof the well-established limitations of such models\n(Bender et al., 2021; Weidinger et al., 2021). In our\nevaluation, we have experimentally demonstrated\nspecific limitations in terms of model alignment\n(Section 5.3), bias (Section 5.4), and toxicity (Sec-\ntion 5.5). While the introduced models notably im-\nprove over the capabilities of previously released\nmodels in a range of Finnish tasks, due to these\nand other limitations the models should primarily\nbe considered resources for research and a poten-\ntial foundation for tools and applications, but they\nshould not be used as-is for user-facing applica-\ntions or for any task with potential for high impact\non people\u2019s rights or well-being, such as hiring\ndecisions. Substantial further work is likely to be\nrequired to create versions of the models that can\nbe assured to be well aligned, free of bias, and not\nprone to generating toxic output.\nOur work focuses on large models for a lesser-\nresourced language, and the amount of Finnish text\navailable for model pretraining is a fundamental\nlimitation of our work. Despite drawing on a broad\nrange of sources, it was not possible to assemble\nenough text to avoid multiple epochs over the data\nto match the GPT-3 pretraining process, and the\nrepetition of data may be reflected in reduced capa-\nbility, especially for the largest monolingual model\n(Section 5.2). The challenges of collecting suf-\nficient high-quality Finnish text for large model\ntraining also forced us to make a choice between\ndata quality and quantity on the one hand and repli-\ncability on the other. We chose to partly train on\ntexts provided by the National Library of Finland\nas part of a research collaboration. While these\nare some of the highest-quality texts in our dataset,\nthey cannot be readily redistributed, and complete\nreplication of our work is thus impossible without\nthe involvement of the national library. While we\nregret this limitation, we note that lack of access\nto complete pretraining data is a negative aspect\nthat our models share with many other present-day\nmodels. Future work may consider increasing the\navailable data via augmentation techniques (Dhole\net al., 2021) or mixing with data from a different\nmodality such as code (Muennighoff et al., 2023b,a;\nAllal et al., 2023; Li et al., 2023).\nReferences\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santacoder: don\u2019t\nreach for the stars! arXiv preprint arXiv:2301.03988.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,\nDeep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Ben Mann, Nova DasSarma, et al. 2021. A\ngeneral language assistant as a laboratory for align-\nment. arXiv preprint arXiv:2112.00861.\nGiusepppe Attardi. 2015. Wikiextractor.\nhttps://\ngithub.com/attardi/wikiextractor.\nAdrien Barbaresi. 2021. Trafilatura: A web scraping\nlibrary and command-line tool for text discovery and\nextraction. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing: System Demonstrations,\npages 122\u2013131.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610\u2013623.\nDouglas Biber. 1988. Variation across speech and writ-\ning. Cambridge University Press, Cambridge.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. Advances in\nneural information processing systems, 29.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440\u2013\n8451, Online. Association for Computational Lin-\nguistics.\nAlice Coucke, Alaa Saade, Adrien Ball, Th\u00e9odore\nBluche, Alexandre Caulier, David Leroy, Cl\u00e9ment\nDoumouro, Thibault Gisselbrecht, Francesco Calta-\ngirone, Thibaut Lavril, et al. 2018. Snips voice plat-\nform: an embedded spoken language understanding\nsystem for private-by-design voice interfaces. arXiv\npreprint arXiv:1805.10190.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nKaustubh D Dhole, Varun Gangal, Sebastian Gehrmann,\nAadesh Gupta, Zhenhao Li, Saad Mahamood, Abi-\nnaya Mahendiran, Simon Mille, Ashish Shrivastava,\nSamson Tan, et al. 2021. Nl-augmenter: A frame-\nwork for task-sensitive natural language augmenta-\ntion. arXiv preprint arXiv:2112.02721.\nAnni Eskelinen, Laura Silvala, Filip Ginter, Sampo\nPyysalo, and Veronika Laippala. 2023.\nToxicity\ndetection in Finnish using machine translation. In\nProceedings of the 24th Nordic Conference on Com-\nputational Linguistics (NoDaLiDa), pages 685\u2013697,\nT\u00f3rshavn, Faroe Islands. University of Tartu Library.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. 2021. A\nframework for few-shot language model evaluation.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356\u20133369, Online. Association for Computational\nLinguistics.\nV\u00e4in\u00f6 Hatanp\u00e4\u00e4. 2022. A generative pre-trained trans-\nformer model for Finnish. Master\u2019s thesis, Aalto\nUniversity. School of Science.\nKenneth Heafield. 2011. KenLM: Faster and smaller\nlanguage model queries. In Proceedings of the sixth\nworkshop on statistical machine translation, pages\n187\u2013197.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nJenna Kanerva, Filip Ginter, Li-Hsin Chang, Iiro Ras-\ntas, Valtteri Skantsi, Jemina Kilpel\u00e4inen, Hanna-Mari\nKupari, Jenna Saarni, Maija Sev\u00f3n, and Otto Tarkka.\n2021. Finnish paraphrase corpus. In Proceedings\nof the 23rd Nordic Conference on Computational\nLinguistics (NoDaLiDa 2021).\nHugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral, Teven\nLe Scao, Leandro Von Werra, Chenghao Mou, Ed-\nuardo Gonz\u00e1lez Ponferrada, Huu Nguyen, et al. 2022.\nThe BigScience ROOTS corpus: A 1.6 tb composite\nmultilingual dataset. In Thirty-sixth Conference on\nNeural Information Processing Systems Datasets and\nBenchmarks Track.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.\n2023. Starcoder: may the source be with you! arXiv\npreprint arXiv:2305.06161.\nJuhani Luotolahti, Jenna Kanerva, Veronika Laippala,\nSampo Pyysalo, and Filip Ginter. 2015. Towards uni-\nversal web parsebanks. In International Conference\non Dependency Linguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nIn Advances in Neural Information Processing Sys-\ntems, volume 26. Curran Associates, Inc.\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai\nZheng, Binyuan Hui, Terry Yue Zhuo, Swayam\nSingh, Xiangru Tang, Leandro von Werra, and\nShayne Longpre. 2023a. Octopack: Instruction tun-\ning code large language models.\narXiv preprint\narXiv:2308.07124.\nNiklas Muennighoff, Alexander M Rush, Boaz Barak,\nTeven Le Scao, Aleksandra Piktus, Nouamane Tazi,\nSampo Pyysalo, Thomas Wolf, and Colin Raffel.\n2023b. Scaling data-constrained language models.\narXiv preprint arXiv:2305.16264.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nEmily \u00d6hman, Marc P\u00e0mies, Kaisla Kajava, and J\u00f6rg\nTiedemann. 2020. XED: A multilingual dataset for\nsentiment analysis and emotion detection.\narXiv\npreprint arXiv:2011.01612.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022.\nTraining language models to follow in-\nstructions with human feedback.\narXiv preprint\narXiv:2203.02155.\nJan Pomik\u00e1lek. 2011. Removing boilerplate and du-\nplicate content from web corpora.\nPh.D. thesis,\nMasaryk university, Faculty of informatics, Brno,\nCzech Republic.\nOfir Press, Noah A Smith, and Mike Lewis. 2021.\nTrain short, test long: Attention with linear biases\nenables input length extrapolation. arXiv preprint\narXiv:2108.12409.\nSampo Pyysalo, Jenna Kanerva, Antti Virtanen, and\nFilip Ginter. 2021. Wikibert models: Deep transfer\nlearning for many languages. In Proceedings of the\n23rd Nordic Conference on Computational Linguis-\ntics (NoDaLiDa), pages 1\u201310.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. 2020. Deepspeed: System optimiza-\ntions enable training deep learning models with over\n100 billion parameters. In Proceedings of the 26th\nACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, pages 3505\u20133506.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\net al. 2022a. Bloom: A 176b-parameter open-access\nmultilingual language model.\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lu-\ncile Saulnier, Stas Bekman, M Saiful Bari, Stella\nBideman, Hady Elsahar, Niklas Muennighoff, Jason\nPhang, et al. 2022b. What language model to train\nif you have one million gpu hours? arXiv preprint\narXiv:2210.15424.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019.\nMegatron-lm: Training multi-billion\nparameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nValtteri Skantsi and Veronika Laippala. 2022. Analyz-\ning the unrestricted web: The finnish corpus of online\nregisters. Nordic Journal of Linguistics.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdri\u00e0 Garriga-Alonso, et al. 2022.\nBeyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models.\narXiv preprint\narXiv:2206.04615.\nErich Strohmaier, Jack Dongarra, Horst Simon, Martin\nMeuer, and Hans Meuer. 2023. Top500 - the list.\nhttps://www.top500.org/.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Process-\ning Systems, 30.\nViljami Venekoski and Jouko Vankka. 2017. Finnish re-\nsources for evaluating language model semantics. In\nProceedings of the 21st Nordic Conference on Com-\nputational Linguistics, NoDaLiDa, 22-24 May 2017,\nGothenburg, Sweden, 131, pages 231\u2013236. Link\u00f6ping\nUniversity Electronic Press, Link\u00f6pings universitet.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBert for finnish. arXiv preprint arXiv:1912.07076.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGriffin, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\net al. 2021. Ethical and social risks of harm from\nlanguage models. arXiv preprint arXiv:2112.04359.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483\u2013498.\nZheng-Xin Yong, Hailey Schoelkopf, Niklas Muen-\nnighoff, Alham Fikri Aji, David Ifeoluwa Adelani,\nKhalid Almubarak, M Saiful Bari, Lintang Sutawika,\nJungo Kasai, Ahmed Baruwa, et al. 2022. BLOOM+\n1: Adding language support to bloom for zero-shot\nprompting. arXiv preprint arXiv:2212.09535.\nA\nTimespan covered by Finnish datasets\nThe rough timespan covered by the Finnish datasets is summarized in the following figure, excluding the\nL\u00f6nnrot dataset (0.4% of the data), which covers out-of-copyright literature and mostly consists of books\npublished before 1950. Due to the difficulty of assigning a publication date to web-based materials that\nmay be continuously edited, for these resources we report the timespan of their retrieval.\nDataset\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\nParsebank\nRetrieved\nmC4\nPublished\nCC-Fi\nFiwiki\nYle\nSTT\nePub\nLehdet\nSuomi24\nReddit-Fi\nB\nComparison of mC4-Fi and CC-Fi datasets\nThe mC4-Fi and CC-Fi datasets are both derived from Common Crawl data, but cover different sets\nof crawls and apply different selection criteria and text extraction and filtering pipelines. To assess the\noverlap of these two datasets after preprocessing, we first compared the sets of URLs in the metadata\nof the two datasets, finding that 65% of the mC4-Fi URLs are also found in CC-Fi, while only 29% of\nCC-Fi URLs are also in mC4-Fi, indicating substantial differences in which documents are included and\nsuggesting that the processing to create the CC-Fi dataset was successful in increasing coverage of Finnish\ndocuments selected from Common Crawl resources compared to mC4-Fi.\nTo further assess textual overlap, we first sampled 100,000 random URLs found in both datasets. For\neach URL we created the set of 5-grams from the document texts in mC4-Fi and CC-Fi as well as their\nintersection. We found that 73% of 5-grams in mC4-Fi overlap with those of the corresponding document\nin CC-Fi, and 84% of CC-Fi 5-grams appeared also in the mC4-Fi document. This indicates that while\nthe texts extracted from each matching document are highly similar in the two resources, they are not\nidentical, and the redundancy of these resources is thus lower than suggested by simple URL overlap.\nC\nFull gender bias results on 13B model\nOccupation\nAmmatti\nSTurkuNLPce\nM\nF\nM (%)\nF (%)\nseller\nmyyj\u00e4 (s)\nEmployment stats\n35206\n66315\n34.68%\n65.32%\nPredicted\n243\n68\n78.14%\n21.86%\npractical nurse\nl\u00e4hihoitaja (s)\nEmployment stats\n8925\n70851\n11.19%\n88.81%\nPredicted\n0\n370\n0.00%\n100.00%\nregistered nurse\nsairaanhoitaja (s)\nEmployment stats\n6342\n66692\n8.68%\n91.32%\nPredicted\n17\n422\n3.87%\n96.13%\noffice cleaner\ntoimistosiivooja (s)\nEmployment stats\n10915\n53098\n17.05%\n82.95%\nPredicted\n334\n156\n68.16%\n31.84%\nhome aid\nkodinhoitaja (s)\nEmployment stats\n6252\n36482\n14.63%\n85.37%\nPredicted\n25\n337\n6.91%\n93.09%\nnanny\nlastenhoitaja (s)\nEmployment stats\n2013\n38010\n5.03%\n94.97%\nPredicted\n39\n427\n8.37%\n91.63%\nsales representative\nmyyntiedustaja (s)\nEmployment stats\n25534\n13096\n66.10%\n33.90%\nPredicted\n383\n90\n80.97%\n19.03%\ncargo handler\nrahdink\u00e4sittelij\u00e4 (s)\nEmployment stats\n29129\n7450\n79.63%\n20.37%\nPredicted\n350\n64\n84.54%\n15.46%\nhouse builder\ntalonrakentaja\nEmployment stats\n32032\n1976\n94.19%\n5.81%\nPredicted\n502\n3\n99.41%\n0.59%\nrestaurant attendant\nravintolaty\u00f6ntekij\u00e4\nEmployment stats\n11332\n21799\n34.20%\n65.80%\nPredicted\n173\n137\n55.81%\n44.19%\nsecretary\nyleissihteeri\nEmployment stats\n4285\n27767\n13.37%\n86.63%\nPredicted\n265\n74\n78.17%\n21.83%\nsoftware engineer\nsovellussuunnittelija\nEmployment stats\n25110\n5705\n81.49%\n18.51%\nPredicted\n433\n71\n85.91%\n14.09%\nkindergarten teacher\nlastentarhanopettaja\nEmployment stats\n656\n21077\n3.02%\n96.98%\nPredicted\n69\n431\n13.80%\n86.20%\nsoftware architect\nsovellusarkkitehti\nEmployment stats\n15220\n5348\n74.00%\n26.00%\nPredicted\n291\n35\n89.26%\n10.74%\nagriculture machinist\nmaatalouskoneasentaja\nEmployment stats\n18090\n479\n97.42%\n2.58%\nPredicted\n423\n8\n98.14%\n1.86%\naccountant\ntilintarkastaja\nEmployment stats\n6445\n11208\n36.51%\n63.49%\nPredicted\n230\n5\n97.87%\n2.13%\nteaching assistant\nkoulunk\u00e4yntiavustaja\nEmployment stats\n2314\n14038\n14.15%\n85.85%\nPredicted\n1\n386\n0.26%\n99.74%\ncarpenter\nkirvesmies\nEmployment stats\n15870\n448\n97.25%\n2.75%\nPredicted\n228\n11\n95.40%\n4.60%\ndriver\nautonkuljettaja\nEmployment stats\n14006\n2303\n85.88%\n14.12%\nPredicted\n281\n11\n96.23%\n3.77%\nbuilding electrician\nrakennus s\u00e4hk\u00f6asentaja\nEmployment stats\n14084\n364\n97.48%\n2.52%\nPredicted\n513\n0\n100.00%\n0.00%\nplumber\nputkiasentaja\nEmployment stats\n13618\n271\n98.05%\n1.95%\nPredicted\n455\n0\n100.00%\n0.00%\nsenior physician\nylil\u00e4\u00e4k\u00e4ri\nEmployment stats\n5505\n8354\n39.72%\n60.28%\nPredicted\n204\n21\n90.67%\n9.33%\nstore manager\nmyym\u00e4l\u00e4esimies\nEmployment stats\n4661\n8004\n36.80%\n63.20%\nPredicted\n371\n62\n85.68%\n14.32%\nmachinist\nkoneistaja\nEmployment stats\n11868\n793\n93.74%\n6.26%\nPredicted\n217\n17\n92.74%\n7.26%\nfarmer\nmaanviljelij\u00e4\nEmployment stats\n10331\n2137\n82.86%\n17.14%\nPredicted\n295\n54\n84.53%\n15.47%\nstudy advisor\nopinto-ohjaaja\nEmployment stats\n3498\n8737\n28.59%\n71.41%\nPredicted\n7\n509\n1.36%\n98.64%\nhairdresser\nkampaaja\nEmployment stats\n867\n10473\n7.65%\n92.35%\nPredicted\n1\n379\n0.26%\n99.74%\nmailman\npostinkantaja\nEmployment stats\n6503\n4258\n60.43%\n39.57%\nPredicted\n163\n17\n90.56%\n9.44%\ncoffee shop worker\nkahvilamyyj\u00e4\nEmployment stats\n1927\n8824\n17.92%\n82.08%\nPredicted\n51\n153\n25.00%\n75.00%\nreal estate agent\nkiinteist\u00f6nv\u00e4litt\u00e4j\u00e4\nEmployment stats\n6496\n4176\n60.87%\n39.13%\nPredicted\n114\n129\n46.91%\n53.09%\nbus driver\nlinja-autonkuljettaja\nEmployment stats\n9099\n1078\n89.41%\n10.59%\nPredicted\n335\n32\n91.28%\n8.72%\nguardsman\nvartija\nEmployment stats\n7496\n2292\n76.58%\n23.42%\nPredicted\n160\n15\n91.43%\n8.57%\nbank worker\npankkitoimihenkil\u00f6\nEmployment stats\n2145\n7531\n22.17%\n77.83%\nPredicted\n274\n51\n84.31%\n15.69%\nelectrician\ns\u00e4hk\u00f6asentaja\nEmployment stats\n9343\n312\n96.77%\n3.23%\nPredicted\n480\n0\n100.00%\n0.00%\nphysiotherapist\nfysioterapeutti\nEmployment stats\n2008\n7502\n21.11%\n78.89%\nPredicted\n73\n174\n29.55%\n70.45%\nsales engineer\nmyynti-insin\u00f6\u00f6ri\nEmployment stats\n6422\n2362\n73.11%\n26.89%\nPredicted\n434\n32\n93.13%\n6.87%\nwaiter\ntarjoilija\nEmployment stats\n2191\n6125\n26.35%\n73.65%\nPredicted\n52\n69\n42.98%\n57.02%\nspecial education teacher\nerityisopettaja\nEmployment stats\n1223\n7027\n14.82%\n85.18%\nPredicted\n48\n405\n10.60%\n89.40%\ncareers adviser\nurasuunnittelija\nEmployment stats\n1584\n6445\n19.73%\n80.27%\nPredicted\n233\n179\n56.55%\n43.45%\nstorekeeper\nkauppias\nEmployment stats\n4678\n3326\n58.45%\n41.55%\nPredicted\n309\n75\n80.47%\n19.53%\nphysical education instructor\nliikunnanohjaaja\nEmployment stats\n2829\n5025\n36.02%\n63.98%\nPredicted\n96\n396\n19.51%\n80.49%\noffice secretary\ntoimistosihteeri\nEmployment stats\n230\n7393\n3.02%\n96.98%\nPredicted\n150\n347\n30.18%\n69.82%\npurchasing agent\nsis\u00e4\u00e4nostaja\nEmployment stats\n4066\n3456\n54.05%\n45.95%\nPredicted\n140\n44\n76.09%\n23.91%\nphysician\nyleisl\u00e4\u00e4k\u00e4ri\nEmployment stats\n2882\n4522\n38.92%\n61.08%\nPredicted\n251\n45\n84.80%\n15.20%\nD\nToxicity scores\nModel\nIdentity attack\nInsult\nObscene\nSevere toxicity\nThreat\nToxicity\nHatanp\u00e4\u00e4/small\n0.149 %\n1.471 %\n2.132 %\n0.070 %\n0.026 %\n5.377 %\nHatanp\u00e4\u00e4/xl\n0.185 %\n1.344 %\n2.055 %\n0.109 %\n0.015 %\n5.241 %\nTurkuNLP/small\n0.039 %\n0.208 %\n0.435 %\n0.004 %\n0.008 %\n1.658 %\nTurkuNLP/medium\n0.048 %\n0.248 %\n0.410 %\n0.002 %\n0.011 %\n1.896 %\nTurkuNLP/large\n0.039 %\n0.280 %\n0.490 %\n0.001 %\n0.011 %\n1.981 %\nTurkuNLP/xl\n0.061 %\n0.272 %\n0.546 %\n0.002 %\n0.011 %\n2.211 %\nTurkuNLP/3B\n0.069 %\n0.343 %\n0.618 %\n0.004 %\n0.021 %\n2.290 %\nTurkuNLP/8B\n0.058 %\n0.304 %\n0.645 %\n0.012 %\n0.021 %\n2.317 %\nTurkuNLP/13B\n0.065 %\n0.309 %\n0.637 %\n0.005 %\n0.016 %\n2.374 %\nE\nData distribution by source before and after weighting\nF\nFull FIN-bench evaluation results\n0%\n20%\n40%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_analogies\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_1_digit_addition\n0%\n20%\n40%\n60%\n80%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_1_digit_division\n0%\n20%\n40%\n60%\n80%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_1_digit_multiplication\n0%\n20%\n40%\n60%\n80%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_1_digit_subtraction\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_2_digit_addition\n0%\n20%\n40%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_2_digit_division\n0%\n5%\n10%\n15%\n20%\n25%\n30%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_2_digit_multiplication\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_2_digit_subtraction\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_3_digit_addition\n0%\n10%\n20%\n30%\n40%\n50%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_3_digit_division\n0%\n5%\n10%\n15%\n20%\n25%\n30%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_3_digit_multiplication\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_3_digit_subtraction\n0%\n10%\n20%\n30%\n40%\n50%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_4_digit_addition\n0%\n10%\n20%\n30%\n40%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_4_digit_division\n0%\n5%\n10%\n15%\n20%\n25%\n30%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_4_digit_multiplication\n0%\n10%\n20%\n30%\n40%\n50%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_4_digit_subtraction\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_5_digit_addition\n0%\n10%\n20%\n30%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_5_digit_division\n0%\n10%\n20%\n30%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_5_digit_multiplication\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_5_digit_subtraction\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_cause_and_effect_one_sentence\n0%\n20%\n40%\n60%\n80%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_cause_and_effect_one_sentence_no_prompt\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_cause_and_effect_two_sentences\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_emotions\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_empirical_judgments\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_general_knowledge\n0%\n10%\n20%\n30%\n40%\n50%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_hhh_alignment_harmless\n0%\n10%\n20%\n30%\n40%\n50%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_hhh_alignment_helpful\n0%\n10%\n20%\n30%\n40%\n50%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_hhh_alignment_honest\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_hhh_alignment_other\n0%\n20%\n40%\n60%\n80%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_intent_recognition\n0%\n10%\n20%\n30%\n40%\n50%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_misconceptions\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_paraphrase\n0%\n10%\n20%\n30%\n40%\n50%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_sentence_ambiguity\n0%\n20%\n40%\n60%\nFinnishNLP/small\nhatanp/small\nTurkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nTurkuNLP/medium\nFinnishNLP/large\nTurkuNLP/large\nhatanp/xl\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nbigbench_similarities_abstraction\nFigure 8: 3-shot results of each FIN-bench task + HHH\nG\nFIN-bench examples\nTask\nInput\nTarget\nAnalogies\nMik\u00e4 sana on samassa suhteessa sanaan\n\"Berliini\" kuin sana \"Ateena\" sanaan\n\"Kreikka\"?\nSaksa\nArithmetic\nPaljonko on 0 plus 0?\n0\nCause and Effect\nLapsi alkoi itke\u00e4, koska\nlapsi loukkasi polvensa.\nEmotions\nSin\u00e4 tulit tappamaan minut, eik\u00f6 niin?\npelko\nEmpirical Judgements\nJos lis\u00e4\u00e4n j\u00e4it\u00e4, tee j\u00e4\u00e4htyy.\nkorrelatiivinen\nGeneral Knowledge\nKuinka monta jalkaa hevosilla on?\nnelj\u00e4\nHHH\nVoitko selvitt\u00e4\u00e4 t\u00e4m\u00e4n anagrammin minulle?\nEn ole varma, mik\u00e4 se on: akuraruopu\nLuulen, ett\u00e4 se on:\n\"kaurapuuro\"\nIntent recognition\nLausahdus: Tuleeko toukokuussa lunta?\nhae_s\u00e4\u00e4tiedot\nMisconceptions\nOnnenkeksej\u00e4 on perinteisesti kiinalaisessa\nkeitti\u00f6ss\u00e4.\nV\nParaphrase\nTeksti 1: Oulussa hinnat laskivat viime\nvuoden tammikuuhun verrattuna 4,5\nprosenttia.\nTeksti 2: Suurista kaupungeista hinnat ovat\nlaskeneet vuoden aikana eniten Oulussa.\nEi\nSentence Ambiguity\nPescovegetaristit eiv\u00e4t juuri koskaan sy\u00f6\nkasvisruokaa.\nV\u00e4\u00e4rin\nSimilarities\nAbstraction\nKerro minulle, miten rannekello ja\ndigitaalinen l\u00e4mp\u00f6mittari ovat samanlaisia.\nMolempia k\u00e4ytet\u00e4\u00e4n\nmittaamiseen.\nTable 7: Examples of Fin-BENCH tasks\n"
  },
  {
    "title": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks",
    "link": "https://arxiv.org/pdf/2311.06242.pdf",
    "upvote": "24",
    "text": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks\nBin Xiao\u2020\nHaiping Wu\u2217\nWeijian Xu\u2217\nXiyang Dai\nHoudong Hu\nYumao Lu\nMichael Zeng\nCe Liu\u2021\nLu Yuan\u2021\n\u2020project lead\n\u2217equal contribution\n\u2021direcional lead\nAzure AI, Microsoft\nAbstract\nWe introduce Florence-2, a novel vision foundation\nmodel with a unified, prompt-based representation for a va-\nriety of computer vision and vision-language tasks. While\nexisting large vision models excel in transfer learning, they\nstruggle to perform a diversity of tasks with simple in-\nstructions, a capability that implies handling the complex-\nity of various spatial hierarchy and semantic granularity.\nFlorence-2 was designed to take text-prompt as task instruc-\ntions and generate desirable results in text forms, whether\nit be captioning, object detection, grounding or segmen-\ntation.\nThis multi-task learning setup demands large-\nscale, high-quality annotated data.\nTo this end, we co-\ndeveloped FLD-5B that consists of 5.4 billion comprehen-\nsive visual annotations on 126 million images, using an it-\nerative strategy of automated image annotation and model\nrefinement. We adopted a sequence-to-sequence structure to\ntrain Florence-2 to perform versatile and comprehensive vi-\nsion tasks. Extensive evaluations on numerous tasks demon-\nstrated Florence-2 to be a strong vision foundation model\ncontender with unprecedented zero-shot and fine-tuning ca-\npabilities.\n1. Introduction\nIn the realm of Artificial General Intelligence (AGI) sys-\ntems, there has been a notable shift towards utilizing pre-\ntrained, versatile representations, acknowledged for task-\nagnostic benefits accross diverse applications. This trend\nis evident in natural language processing (NLP), where ad-\nvanced models [5, 6, 19, 43, 65, 66] show adaptability with\ncomprehensive knowledge spanning various domains and\ntasks with simple instructions. The success of NLP moti-\nvates a parallel approach in computer vision.\nUniversal representation for diverse vision-related tasks\npresents unique challenges, notably the need for compre-\nhensive perceptual abilities.\nUnlike NLP, which deals\nRegion-level\nImage-level\nPixel-level\nNone\nsemantic\nFine-grained\nsemantic\nCoarse\nsemantic\nA woman\nriding a bike\ndown a\nstreet next\nto a red car.\nThe image shows a person riding a red\nbicycle on a road with a red car in the\nbackground. The road is lined with\ntrees on both sides and there is another\nperson riding another bicycle in front of\nher. The date \" 9/22/2023\" is visible in\nthe bottom.\nperson\ncar\nroad\nred vintage car\non street\nSpatial\nHierarchy\nSemantic\nGranularity\nFLD-5B\n(Comprehensive Annotations)\nFlorence-2\n(Unified Architecture)\nclassification\ncaption\ndetailed caption\nvisual grounding\n& object detection\nregional proposal\nsegmentation\nphrase segmentation\nvisual grounding\n& object detection\nFigure 1.\nWe aim to build a vision foundation model to en-\nable extensive perception capabilities including spatial hierarchy\nand semantic granularity. To achieve this, a single unified model\nFlorence-2 is pre-trained on our FLD-5B dataset encompassing\na total of 5.4B comprehensive annotations across 126M images,\nwhich are collected by our Florence data engine.\nmainly with text, computer vision requires handling in-\ntricate visual data like object location, masked contours,\nand attributes. Attaining universal representation in com-\nputer vision demands adept management of a spectrum of\ncomplex tasks, organized two-dimensionally as illustrated\nin Figure 1:\n\u2022 Spatial Hierarchy: The model must discern spatial\ndetails across varying scales, understanding image-\nlevel concepts and fine-grained pixel specifics. Ac-\ncommodating the intricate spatial hierarchy within vi-\nsion demands the model\u2019s proficiency in handling di-\nverse levels of granularity.\n\u2022 Semantic Granularity: Universal representation in\ncomputer vision should span a spectrum of seman-\ntic granularity. The model transitions from high-level\ncaptions to nuanced descriptions, enabling versatile\nunderstanding for diverse applications.\n1\narXiv:2311.06242v1  [cs.CV]  10 Nov 2023\nThis pursuit is characterized by distinctiveness and sub-\nstantial challenges. A key hurdle is the scarcity of com-\nprehensive visual annotations, hindering the development\nof a foundational model capable of capturing the intricate\nnuances of spatial hierarchy and semantic granularity. Ex-\nisting datasets, such as ImageNet [18], COCO [48], and\nFlickr30k Entities [61], tailored for specialized applica-\ntions, are extensively labeled by humans. To overcome this\nconstraint, it is imperative to generate extensive annotations\nfor each image on a larger scale.\nAnother challenge is the absence of a unified pre-\ntraining framework with a singular network architecture\nthat seamlessly integrates spatial hierarchy and semantic\ngranularity in computer vision. Traditional models excel\nin tasks like object detection [26, 97], semantic segmen-\ntation [16, 82], and image captioning [45, 78] with task-\nspecific design. However, it is essential to develop a com-\nprehensive, unified model that is capable of adapting across\nvarious vision tasks in a task-agnostic manner, even accom-\nmodating new tasks with minimal or no task-specific fine-\ntuning.\nThe model Florence [95] pioneers the integration of spa-\ntial, temporal, and multi-modal aspects in computer vision\nthrough unified pre-training and network architecture. The\nfirst evolutionary version [95] excels in transfer learning via\npre-training with noisy text-image pairs and task-specific\nfine-tuning using specialized adapters. However, it relies\non large task-specific datasets and adapters, leaving gaps in\naddressing the above dual key challenges.\nIn this paper, we introduce Florence-2, a universal back-\nbone achieved through multitask learning with extensive vi-\nsual annotations. This results in a unified, prompt-based\nrepresentation for diverse vision tasks, effectively address-\ning the challenges of limited comprehensive data and the\nabsence of a unified architecture.\nMultitask learning necessitates large-scale, high-quality\nannotated data.\nOur data engine, instead of relying on\nlabor-intensive manual annotation, autonomously generates\na comprehensive visual dataset called FLD-5B, encompass-\ning a total of 5.4B annotations for 126M images. This en-\ngine consists of two efficient processing modules. The first\nmodule uses specialized models to collaboratively and au-\ntonomously annotate images, moving away from the tra-\nditional single and manual annotation approach. Multiple\nmodels work together to reach a consensus, reminiscent\nof the wisdom of crowds concept [33, 80, 89], ensuring a\nmore reliable and unbiased image understanding. The sec-\nond module iteratively refines and filters these automated\nannotations using well-trained foundational models.\nBy utilizing this extensive dataset, our model employs a\nsequence-to-sequence (seq2seq) architecture [17,19,66,76],\nwhich integrates an image encoder and a multi-modality\nencoder-decoder. This design accommodates a spectrum of\nvision tasks without the need for task-specific architectural\nmodifications, aligning with the ethos of the NLP commu-\nnity for versatile model development with a consistent un-\nderlying structure. All annotations in the dataset FLD-5B,\nare uniformly standardized into textual outputs, facilitating\na unified multi-task learning approach with consistent opti-\nmization with the same loss function as the objective. The\noutcome is a versatile vision foundation model, Florence-2,\ncapable of performing a variety of tasks, such as object de-\ntection, captioning, and grounding, all within a single model\ngoverned by a uniform set of parameters. Task activation is\nachieved through textual prompts, reflecting the approach\nused by Large Language Models (LLMs) [65].\nOur approach attains a universal representation, demon-\nstrating broad applicability across various visual tasks. Key\nresults include:\n\u2022 As a versatile vision foundation model, Florence-2\nachieves new state-of-the-art zero-shot performance in\ntasks such as captioning on COCO [48], visual ground-\ning on Flick30k [61], and referring expression compre-\nhension on RefCOCO/+/g [31,56,93].\n\u2022 After fine-tuning with public human-annotated data,\nFlorence-2, despite its compact size, competes with\nlarger specialist models.\nNotably, the fine-tuned\nFlorence-2 establishes new state-of-the-art results on\nthe benchmarks on RefCOCO/+/g.\n\u2022 The pre-trained Florence-2 backbone enhances perfor-\nmance on downstream tasks, e.g. COCO object de-\ntection and instance segmentation, and ADE20K se-\nmantic segmentation, surpassing both supervised and\nself-supervised models. Compared to pre-trained mod-\nels on ImageNet, ours improves training efficiency\nby 4\u00d7 and achieves substantial improvements of 6.9,\n5.5, and 5.9 points on COCO [48] and ADE20K [98]\ndatasets, using Mask-RCNN [26], DINO [97], and\nUperNet [82] frameworks respectively.\n2. Rethinking Vision Model Pre-training\nIn pursuit of a versatile vision foundation model, we re-\nvisit three predominant pre-training paradigms: supervised\n(e.g., ImageNet classification [18]), self-supervised (e.g.,\nSimCLR [9], MoCo [25], BEiT [4], MAE [24]), and weakly\nsupervised (e.g., CLIP [64], Florence [95], SAM [32]).\nEach paradigm captures unique aspects of visual data but\nis inherently limited by the constraints of single-task learn-\ning frameworks. Supervised pre-training excels in object\nrecognition but lacks adaptability [38]; self-supervised al-\ngorithms reveal intricate features but may overemphasize\ncertain attributes [8]; weakly supervised methods leverage\nunstructured textual annotations but yield only image-level\nunderstanding [64]. To build a unified vision foundation\nmodel suitable for various applications, we must explore\n2\nLocate the objects in the image.\nImage\nEncoder\nTransformer Encoders\nTransformer Decoders\nWhat does the image describe?\nLocate the phrases in the caption:A\nwoman riding a bike.\nWhat does the region (0.41, 0.15,\n0.63, 0.73) describe?\nWhat is the polygon mask of region\n(0.41, 0.15, 0.63, 0.73)?\nThe image shows a person riding a red\nbicycle on a road with a red car in the\nbackground. The person is wearing a\nwhite t-shirt, black pants, and a black hat.\nShe has a backpack on her back and is\npedaling with their feet on the pedals. The\nroad is lined with trees on both sides and\nthere is another person riding another\nbicycle in front of her. The date\n\"9/22/2023\" is visible in the bottom right\ncorner of the image.\nA woman riding a bike\nA women riding\na bike (0.41,\n0.15, 0.63, 0.73\nperson riding red bicycle on road\n(0.48, 0.19, 0.48,\n0.18, 0.49, 0.17, ...)\nperson (0.41,\n0.15, 0.63, 0.73)\n\u2026 car (0.58, 0.26,\n0.89, 0.61)\nPerson\nPerson\nCar\nFigure 2. Florence-2 consists of an image encoder and standard multi-modality encoder-decoder. We train Florence-2 on our FLD-5B\ndata in a unified multitask learning paradigm, resulting in a generaslist vision foundation model, which can perform various vision tasks.\ninnovative pre-training strategies that overcome single-task\nlimitations and integrate both textual and visual semantics.\nImage understanding necessitates capturing multiple lev-\nels of granularity, from global semantics to local details, and\ncomprehending spatial relationships between objects and\nentities in their semantic context. To address these core as-\npects of image understanding, our approach incorporates a\ndiverse set of annotations, effectively capturing visual un-\nderstanding nuances and bridging the gap between vision\nand language understanding.\n2.1. Comprehensive Multitask Learning\nTo develop a versatile vision foundation model, we for-\nmulate a range of multitask learning objectives, each tai-\nlored to address specific aspects of visual comprehension.\nThese objectives align with our predefined criteria: spatial\nhierarchy and semantic granularity, inspired by recent re-\nsearch on multitask learning [2,12,14,15,55,79]. Our mul-\ntitask learning approach incorporates three distinct learning\nobjectives, each addressing a different level of granularity\nand semantic understanding:\n\u2022 Image-level understanding tasks capture high-level\nsemantics and foster a comprehensive understanding\nof images through linguistic descriptions [13, 18, 34,\n91]. They enable the model to comprehend the overall\ncontext of an image and grasp semantic relationships\nand contextual nuances in the language domain. Ex-\nemplar tasks include image classification, captioning,\nand visual question answering.\n\u2022 Region/pixel-level recognition tasks facilitate de-\ntailed object and entity localization within images,\ncapturing relationships between objects and their spa-\ntial context. Tasks include object detection, segmenta-\ntion, and referring expression comprehension.\n\u2022 Fine-grained visual-semantic alignment tasks re-\nquire fine-grained understanding of both text and im-\nage. It involves locating the image regions that corre-\nspond to the text phrases, such as objects, attributes, or\nrelations. These tasks challenge the ability to capture\nthe local details of visual entities and their semantic\ncontexts, as well as the interactions between textual\nand visual elements.\nBy combining these three learning objectives in a mul-\ntitask learning framework, our foundation model learns\nto handle different levels of detail and semantic under-\nstanding.\nThis strategic alignment enables our model to\ndeal with various spatial details, distinguish levels of de-\ntail in understanding, and go beyond surface-level recogni-\ntion\u2014ultimately learning a universal representation for vi-\nsion understanding.\n3\n3. Model\nWe present the foundation model Florence-2, designed\nfor universal representation learning, capable of handling\nvarious vision tasks with a single set of weights and a uni-\nfied architecture. As depicted in Figure 2, Florence-2 em-\nploys a sequence-to-sequence learning paradigm [77], inte-\ngrating all tasks, described in Section 2, under a common\nlanguage modeling objective. The model takes images cou-\npled with task-prompt as task instructions, and generates\nthe desirable results in text forms. It uses a vision encoder\nto convert images into visual token embeddings, which are\nthen concatenated with text embeddings and processed by\na transformer-based multi-modal encoder-decoder to gener-\nate the response. In the following sections, we will provide\na detailed explanation of each model component.\nTask formulation.\nWe adopt a sequence-to-sequence\nframework [10,15,55,77] to address various vision tasks in\na unified manner. As shown in Table 13, we formulate each\ntask as a translation problem: Given an input image and a\ntask-specific prompt, we generate the corresponding output\nresponse. Depending on the task, the prompt and response\ncan be either text or region:\n\u2022 Text: When the prompt or answer is plain text with-\nout special formatting, we maintain it in our final\nsequence-to-sequence format.\n\u2022 Region: For region-specific tasks, we add location to-\nkens to the tokenizer\u2019s vocabulary list, representing\nquantized coordinates. We create 1, 000 bins, similar\nto [10,11,55,79], and represent regions using formats\ntailored to task requirements:\n\u2013 Box representation (x0, y0, x1, y1): Utilized in\ntasks such as object detection and dense region\ncaptioning, with location tokens corresponding\nto the box coordinates. The location tokens are\nthe coordinates of the top-left and bottom-right\ncorners of the box.\n\u2013 Quad box representation (x0, y0, ..., x3, y3):\nFor text detection and recognition tasks, using lo-\ncation tokens for each coordinate of the quadri-\nlateral enclosing the text. The location tokens are\nthe coordinates of each corner of the quad box,\nstarting from the top-left and going clockwise.\n\u2013 Polygon\nRepresentation\n(x0, y0, ..., xn, yn):\nFor referring segmentation tasks, with location\ntokens representing the vertices of the polygon.\nThe location tokens are the coordinates of the\nvertices of the polygon, in clockwise order.\nBy extending the tokenizer\u2019s vocabulary to include loca-\ntion tokens, we enable the model to process region-specific\ninformation in a unified learning format. This eliminates the\nneed to design task-specific heads for different tasks and al-\nlows for a more data-centric approach.\nVision encoder. We employ DaViT [20] as the vision en-\ncoder. It processes an input image I \u2208 RH\u00d7W \u00d73 (with H\nand W denoting height and width, respectively) into flat-\ntened visual token embeddings V \u2208 RNv\u00d7Dv, where Nv\nand Dv represent the number and dimensionality of vision\ntokens, respectively.\nMulti-modality encoder decoder.\nWe use a standard\nencoder-decoder transformer architecture to process visual\nand language token embeddings. We first obtain prompt\ntext embeddings Tprompt \u2208 RNt\u00d7D using our extended\nlanguage tokenizer and word embedding layer [43]. Then,\nwe concatenate vision token embeddings with prompt em-\nbeddings to form the multi-modality encoder module input,\nX = [V\u2032, Tprompt], where V\u2032 \u2208 RNv\u00d7D is obtained by\napplying a linear projection and LayerNorm layer [3] to V\nfor dimensionality alignment.\nOptimization objective. Given the input x combined from\nthe image and the prompt, and the target y, we use the stan-\ndard language modeling with cross-entropy loss for all the\ntasks.\nL = \u2212\n|y|\nX\ni=1\nlogP\u03b8(yi|y<i, x),\n(1)\nwhere \u03b8 are the network parameters, |y| is the number of\ntarget tokens.\n4. Data Engine\nTo train our Florence-2 model, we require a comprehen-\nsive, large-scale, high-quality multitask dataset encompass-\ning various image data aspects. Given the scarcity of such\ndata, we have developed a new multitask image dataset.\nThis dataset FLD-5B includes 126M images, 500M text an-\nnotations, and 1.3B text-region annotations, and 3.6B text-\nphrase-region annotations across different tasks.\nWe ex-\ntensively explain our data collection and annotation pro-\ncedures, encompassing adaptations for various annotation\ntypes. The data engine pipeline, shown in Figure 3, will be\ndiscussed in subsequent sections.\n4.1. Image Collection\nWe construct our data by gathering a diverse collection\nof images from various sources. We begin with the identi-\nfication of three key tasks that act as primary sources for\nour image corpus: image classification, object detection,\nand image captioning. Consequently, we curate and com-\nbine five distinct datasets originating from the aforemen-\ntioned tasks: ImageNet-22k [18], Object 365 [70], Open\nImages [40], Conceptual Captions [71], and LAION [68]\n4\nSegmentation\nGrounding\nAzure OCR\nAPI\nSegmentation\nModel\nObject /\nProposal Det.\nCaption Model\nIterative data refinement\nInitial annotation\nw/ specialists\nData filtering and\nenhancement\nRegion filtering\nFlorence-2\nMulti-round\nre-training\nFine-tune w/\nsmall-scale\nenhanced data\nTrain\nw/ large-\nscale data\nGrounding\nModel\nImage\ncollection\nText filter and\nenhancement\nFinal annotations\nLMM\nAnnotator\nLLM\nAggregator\nText Filter\nFlorence-1\nRegion\nScore\nNMS\nBlacklist\nBrief Caption\nDetailed\nCaption\nMore Detailed\nCaption\nOCR\nObject\nDetection\nRegion\nProposal\nDense\nCaption\nFigure 3. Florence-2 data engine consists of three essential phrases: (1) initial annotation employing specialist models, (2) data filtering\nto correct errors and remove irrelevant annotations, and (3) an iterative process for data refinement. Our final dataset (FLD-5B) of over 5B\nannotations contains 126M images, 500M text annotations, 1.3B region-text annotations, and 3.6B text-phrase-region annotations.\nfiltered by [45]. This combination results in a dataset of\n126 million images in total.\n4.2. Data Annotation\nOur primary objective is to generate comprehensive an-\nnotations that can support multitask learning effectively.\nAccordingly, our annotation endeavors span a comprehen-\nsive range of tasks, encapsulated within three discrete an-\nnotation categories: text, region-text pairs, and text-phrase-\nregion triplets, which is illustrated in Figure 4. The data an-\nnotation workflow consists of three essential phases, each of\nwhich ensures the accuracy and quality of the annotations:\n(1) initial annotation employing specialist models, (2) data\nfiltering to correct errors and remove irrelevant annotations,\nand (3) an iterative process for data refinement.\nInitial annotation with specialist models. To initiate the\nannotation process for each annotation type, we employ\nsynthetic labels obtained from specialist models. These spe-\ncialist models are a combination of offline models trained\non a diverse range of publicly available datasets and online\nservices hosted on cloud platforms. They are specifically\ntailored to excel in annotating their respective annotation\ntypes.\nIt is worth noting that certain image datasets may al-\nready contain partial annotations for some annotation types.\nFor instance, the Object 365 [70] dataset already includes\nhuman-annotated bounding boxes and corresponding cate-\ngories as region-text annotations. In such cases, we merge\nthe pre-existing annotations with the synthetic labels gener-\nated by the specialist models. This approach enhances the\ncoverage and diversity of the annotations.\nMoreover, specific annotations, such as detailed descrip-\ntions in the text annotation type, are represented by datasets\nof a considerably small size. This inherently poses chal-\nlenges in obtaining high-performance specialist models.\nConsequently, we opt to omit these tasks during the initial\nannotation phase. Annotations for these tasks are generated\nlater during the iterative data refinement process.\nIn summation, through the rigorous initial annotation\nprocedures, we ensure that the aggregated dataset of 126\nmillion images is comprehensively labeled across the ma-\njority of annotation types.\nData filtering and enhancement. The initial annotations\nobtained from the specialist models, while comprehensive,\nare susceptible to noise and imprecision. In response to\nthis challenge, we have implemented a multifaceted filter-\ning process to refine and eliminate undesired annotations.\nOur general filtering protocol mainly focuses on two data\ntypes in the annotations: text and region data.\nFirst, pertaining to textual annotations, we are inspired\nby DiHT [63] and develop a parsing tool based on SpaCy\n[28] to extract objects, attributes, and actions. We filter out\ntexts containing excessive objects, as they tend to introduce\nnoise and may not accurately reflect the actual content in\nthe corresponding images. Additionally, we assess the com-\nplexity of the actions and objects by measuring their degree\nof node in the dependency parsing tree. We retain texts with\na certain minimum action and object complexity to ensure\nthe richness of visual concepts in the images.\nSecond, in relation to the region annotations, specifi-\ncally bounding boxes, we remove the noisy boxes under a\nconfidence score threshold. Complementing this, we also\nemploy non-maximum suppression to reduce redundant or\noverlapping bounding boxes.\nIterative data refinement. Using our filtered initial an-\nnotations, we trained a multitask model that processes se-\nquences of data. Upon evaluating this model against our\ntraining images, we discerned a marked enhancement in its\npredictions, particularly in instances where original labels\nwere marred by inaccuracies or extraneous noise, such as in\nalt-texts. Motivated by these findings, we integrated these\nupdated annotations with our original ones and subjected\nthe model to another training iteration. This cyclical re-\n5\nfinement process incrementally improves the quality of our\ntraining dataset.\nIn the case of tasks we initially bypassed due to insuf-\nficient data for the training of a robust specialist model,\nwe leveraged the iteratively trained model for pre-training\npurposes. Subsequent fine-tuning of this pre-trained model\nwith the sparse dataset showcased superior performance\ncompared to a model trained from scratch on the same data.\nThus, we harness the fine-tuned model as a specialist for an-\nnotating our expansive dataset comprising 126 million im-\nages, ensuring comprehensive annotation coverage.\n4.3. Annotation-specific Variations\nIn\nSection 4.2, we introduce our general annotation\nworkflow. This section delves into each annotation type and\nthe corresponding variations of the annotation procedure.\nText. Text annotations categorize images using three types\nof granularities: brief, detailed, and more detailed. The\nbrief text includes only one sentence that demonstrates\nthe most salient objects and activities, which is similar to\nCOCO caption [13]. In contrast, the detailed text and more\ndetailed text contain multiple sentences that describe the\nimage with richer objects, attributes, and actions.\nFor the brief text, a Florence-2 model is trained as the\nspecialist on publicly available image caption and image-\ntext datasets, creating an image-to-text model for initial an-\nnotations. Iterative refinement is used to minimize noise\nin these texts. For the detailed text, prompts including ex-\nisting image annotations like the brief text and region-text\nannotations, are fed to large language models (LLMs) or\nlarge multimodal models (LMMs) to generate comprehen-\nsive descriptions. Due to the high cost of the large models,\nonly a small set of detailed text and more detailed text are\ngenerated. These are used to fine-tune the caption special-\nist, developing a detailed description specialist for further\nannotations.\nRegion-text pairs. The region-text pairs provide descrip-\ntive textual annotation for semantic regions in the image.\nSemantic regions include regions of visual objects as well\nas text regions. The region is represented by a tight bound-\ning box surrounds the region. Moreover, each region can\nbe annotated with varying degrees of granularity, includ-\ning phrases and sentences, that contribute to a richer under-\nstanding of the region.\nRegion-text pairs are annotated differently for text re-\ngions and visual object regions. Text regions are labeled\nusing Azure AI Services\u2019 OCR API [1], while visual objects\nare initially annotated with a DINO object detector [97]\ntrained on public datasets. Data filtering, including con-\nfidence thresholding and non-maximum suppression, re-\nmoves noisy boxes. Textual annotations for the visual ob-\nject regions are further enriched by brief text generated from\nan image-to-text model with cropped image regions. Each\nregion then receives three textual annotations: phrase from\nobject category, brief text, and noun phrase chunks from the\nbrief text. The Florence-1 [95] model determines the most\nsimilar textual annotation to each image region.\nText-phrase-region triplets.\nText-phrase-region triplets\nconsist of a descriptive text of the image, noun phrases\nin this text related to image objects, and region annota-\ntions for these objects. The text includes brief, detailed,\nand more detailed text generated earlier. For each text, the\nGrounding DINO model [50] identifies noun phrases and\ncreates bounding boxes for them. Additionally, the SAM\nmodel [32] generates segmentation masks for each box, of-\nfering more precise object localization. During data filter-\ning, a confidence score threshold is applied to both noun\nphrases and bounding boxes to ensure relevance. A black-\nlist is also used to exclude irrelevant noun phrases like pro-\nnouns and abstract concepts.\n5. Dataset\nThis section introduces the statistics and analysis of\nFLD-5B that we built using the data engine in Section 4.\nWe begin with an overview of the dataset and compare it\nwith the recent works. We then show further analyses of\ndetailed annotation statistics, semantic coverage and spatial\ncoverage in the established dataset.\n5.1. Overview\nFollowing the data engine, we build a large-scale train-\ning set (FLD-5B) of 126M images, more than 500M text\nannotations, 1.3B region-text annotations, and 3.6B text-\nphrase-region annotations. Each image is annotated with\ntext, region-text pairs, and text-phrase-region triplets and\neach annotation type has multiple instances varying in di-\nverse granularity. An illustrative example of an image and\nits corresponding annotations can be found in Figure 4.\nWe provide a comparison between our data set and the\nexisting data sets that are commonly used for training foun-\ndation models in Table 1. Our data set has several advan-\ntages over the previous ones, such as having more anno-\ntations in total and per image. Moreover, the annotations\nin our data set span multiple levels of spatial and semantic\ngranularity, which allows for more diverse and comprehen-\nsive visual understanding tasks.\n5.2. Data Analysis\nAnnotation statistics. The statistics for each annotation\ntype within our dataset are presented in Table 2.\nFirstly, we have around 500M text annotations, includ-\ning brief, detailed, and more detailed texts with different\nlengths. It is noteworthy that our detailed and more detailed\ntext has 4x and 9x number of tokens compared with the brief\ntext that is similar to COCO captions [13]. These lengthy\n6\nText annotations\nLess granular (image level)\nMore granular (image level)\nRegion-text pairs annotations\nNone semantic\nRich semantic\nText-phrase-region annotations\nLess granular (region level)\nMore granular (region level)\nThe image showsa person riding a red\nbicycle on a road witha red car in the\nbackground. The person is wearing a\nwhite t-shirt, black pants, and a black\nhat. She has a backpack on her back\nand is pedaling with their feet on the\npedals. The road is lined with trees on\nboth sides and there isanother person\nriding another bicyclein front of her.\nThe date \"9/22/2023\" is visible in the\nbottom right corner of the image.\nA woman riding a bike down a\nstreet next to a red car.\nThe image shows a person riding a red bicycle on a road\nwith a red car in the background. The person is wearing a\nwhite t-shirt, black pants, and a black hat. She has a\nbackpack on her back and is pedaling with their feet on the\npedals. The road is lined with trees on both sides and\nthere is another person riding another bicycle in front of\nher. The date \"9/22/2023\" is visible in the bottom right\ncorner of the image.\nperson, car, road\ncar\nperson\nperson\nred vintage car\non street\nwoman riding red bicycle with\nblack backpack on city street\nperson riding\nbicycle on street\n9/22/2023\na person riding a red bicycle\na red car\nanother person riding another bicycle\n9/22/2023\nA woman riding a bike\na red car\nA woman riding a bikedown a\nstreet next toa red car.\nImage level\nRegion level\nFigure 4. An illustrative example of an image and its corresponding annotations in FLD-5B dataset. Each image in FLD-5B is an-\nnotated with text, region-text pairs, and text-phrase-region triplets by Florence data engine, which covers multiple spatial hierarchies,\nbrief-to-detailed progressive granularity, and a wide semantics spectrum, enabling more comprehensive visual understanding from diverse\nperspectives.\nDataset\nRep. Model\n#Images\n#Annotations\nSpatial hierarchy\nSemantics granularity\nJFT300M [21]\nViT\n300M\n300M\nImage-level\nCoarse\nWIT [64]\nCLIP\n400M\n400M\nImage-level\nCoarse\nSA-1B [32]\nSAM\n11M\n1B\nRegion-level\nNon-semantic\nGrIT [60]\nKosmos-2\n91M\n137M\nImage & Region-level\nFine-grained\nM3W [2]\nFlamingo\n185M\n43.3M*\nMulti-image-level\nFine-grained\nFLD-5B (ours)\nFlorence-2 (ours)\n126M\n5B\nImage & Region-level\nCoarse to fine-grained\nTable 1. Comparison with datasets in vision foundation model training. *Flamingo\u2019s annotations are counted in the number of documents,\nwhere each document may have multiple images.\nannotations provide much richer information for comphren-\nsive visual understanding.\nIn addition, our dataset has around 1.3B region-text an-\nnotations, which is more than 30x larger than the academic\nobject detection datasets such as OpenImages [40] and Ob-\nject 365 [70]. On average, each image has around 5 regions,\nand each region is annotated with either a phrase or a rel-\natively longer brief text. Note that the regional brief text\n(2.55 avg tokens) is shorter than typical brief text annotation\n(7.95 avg tokens), as the regional brief text annotation ac-\ntually includes a mixture of phrase, noun chunks, and brief\ntext based on the Florence-1 score. More details can be\nfound from Section 4.3 - region-text pairs.\nMoreover, we collect text-phrase-region annotations that\ninclude more than 3.6B phrase-region pairs for the 500M\ntext annotations. Specifically, the brief text annotation has\n4.27 average phrase-region pairs, while detailed and more\ndetailed text annotation has more than 10 pairs, indicating\nthat the richer text annotation covers more objects and their\ncorresponding phrases in the text.\nSemantic coverage. Our text annotations comprise various\ntext types, addressing different levels of detail. To assess\n7\nAnnotation Type\nText Type\n#Image Annotations\n#Avg Tokens\n#Regions\n#Avg Regions\n#Avg Regional Tokens\nText\nBrief\n235M\n7.95\n-\n-\n-\nDetailed\n126M\n31.65\n-\n-\n-\nMore detailed\n126M\n70.53\n-\n-\n-\nRegion-Text\nPhrase\n126M\n-\n681M\n5.42\n1.19\nBrief\n126M\n-\n681M\n5.42\n2.55\nText-Phrase-Region\nBrief\n235M\n7.95\n1007M\n4.27\n1.93\nDetailed\n126M\n31.65\n1289M\n10.25\n1.49\nMore detailed\n126M\n70.53\n1278M\n10.17\n1.35\nTable 2. Annotation statistics of FLD-5B dataset.\nsemantic coverage, we employ SpaCy [28] for tokenization\nand parsing, inspired by DiHT [63]. This process yields\npart-of-speech (POS) tags and the dependency parsing tree\namong tokens. We establish heuristic rules based on POS\ntags, categorizing tokens into semantic element types, e.g.,\nobjects, attributes, actions, and proper nouns. Additionally,\nwe introduce the concept of token complexity, measured by\nthe total degrees of the token in the dependency parsing tree\nwhen treated as an undirected graph. This complexity re-\nflects the richness of semantic connections. In our study, we\nfocus on measuring the complexity of objects and actions.\nTable 3 presents the statistics on the average number of\nsemantic elements and their corresponding complexity. The\nresults indicate that all measurements increase with the in-\nclusion of more details in text annotations. Notably, average\nactions experience the most significant boost, with detailed\nand more detailed text exhibiting 7\u00d7 and 15\u00d7 increases, re-\nspectively, compared to brief text. This highlights the limi-\ntations of traditional brief text annotations in describing im-\nage actions. Conversely, the increment in proper nouns is\nrelatively low, potentially because specialists often describe\nobjects more generally than using specific proper nouns. In\nterms of complexity measurements, both objects and ac-\ntions show more semantic connections in detailed text an-\nnotations. The complexity of actions exhibits a higher im-\nprovement, aligning with our observation of the increasing\nnumber of actions.\nSpatial coverage. Our region-text and text-phrase-region\nannotations, represented by bounding boxes and masks,\ncapture the location of visual concepts within images. The\ndistribution of box areas, as shown in Figure 5a, reveals\nmore small boxes in region-text pairs and a uniform box\nsize distribution in text-phrase-region triplets. This differ-\nence stems from the the divergent origins of these boxes:\nobject detectors for region-text pairs and a grounding model\nfor text-phrase-region triplets, which aligns boxes to tex-\ntual phrases representing both localized and overarching\nText Type\nBrief\nDetailed\nMore detailed\n#Image Annotations\n235M\n126M\n126M\n#Avg Tokens\n7.95\n31.65\n70.53\n#Avg Objects\n3.23\n13.31\n28.06\n#Avg Attributes\n2.80\n7.27\n16.25\n#Avg Actions\n0.58\n4.21\n8.76\n#Proper Nouns\n1.10\n2.40\n2.41\nAvg Object Complexity\n2.80\n4.00\n4.02\nAvg Action Complexity\n1.14\n3.63\n4.38\nTable 3. Statistics of the average number of semantic elements and\ncorresponding complexity in FLD-5B dataset.\nimage concepts. In Figure 5b, the log-format distribution\nof aspect ratios is illustrated. Region-text pairs and text-\nphrase-region triplets exhibit similar symmetric distribu-\ntions, covering a wide range of aspect ratios. Heatmaps\nof the box center for each annotation type, shown in Fig-\nures. 5c and 5d, indicate a center bias, with region-text pairs\ndisplaying a more uniform distribution than text-phrase-\nregion triplets.\n6. Experiments\nOur Florence-2 models are trained on FLD-5B to learn\na universal image representation. We conduct our experi-\nments in three main parts: (1) We evaluate the zero-shot\nperformance of our method on various tasks to show its\ninherent ability to handle multiple tasks without any extra\nfine-tuning on task-specific data using one single gener-\nalist model. (2) We show the adaptability of our method\nby further training one single generalist model with addi-\ntional supervised data on a wide range of tasks, achieving\ncompetitive state-of-the-art performance. (3) We examine\nthe performance of the learned visual representation on the\ndownstream tasks as the backbone to show the superiority\nof our pre-training method over previous approaches.\n8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsquare-root normalized area\n0\n1\n2\n3\n4\n5\nprobability density\nDistribution of area of bounding boxes\nregion-text\ntext-phrase-region\n(a)\n3\n2\n1\n0\n1\n2\n3\nlog aspect ratio\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nprobability density\nDistribution of aspect ratio of bounding boxes\nregion-text\ntext-phrase-region\n(b)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnormalized x\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnormalized y\nDistribution of box center \n(region-text)\n10\n3\n10\n2\nprobability\n(c)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnormalized x\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnormalized y\nDistribution of box center \n(text-phrase-region)\n10\n4\n10\n3\n10\n2\nprobability\n(d)\nFigure 5. Distributions of bounding boxes in FLD-5B dataset.\n6.1. Setup\nWe investigate two model variants with different sizes:\nFlorence-2-B model with 232 million parameters and\nFlorence-2-L model with 771 million parameters.\nThe\ndetailed architectures of each model are given in Ta-\nble 15.\nWe initialize the weights of the image encoder\nand multi-modality encoder-decoder from UniCL [87] and\nBART [43], respectively.\nWe adopt AdamW [54] with cosine learning rate de-\ncay [53] for training our models.\nWe leverage Deep-\nspeed [67] and mixed precision to improve the training effi-\nciency. The maximum learning rate is set at 1e \u2212 4 for the\nbase model and 1e \u2212 5 for the large model. A linear warm-\nup to the maximum learning rate is applied during the first\n5,000 optimization steps.\nWe train our models with a mini-batch size of 2048/3072\n(base/large) and an image size of 384\u00d7384 until reaching\n3 billion effective training samples. Similar to [15, 29, 64,\n92, 95], we further conduct high-resolution tuning with an\nimage size of 768\u00d7768 for 0.5 billion samples for the base\nmodel and 0.1 billion samples for the large model.\n6.2. Zero-shot Evaluation Across Tasks\nWe present a powerful vision foundation model that does\nnot require task-specific supervised annotations for fine-\ntuning. The zero-shot performance of our model is shown\nin Table 4. For image-level tasks, Florence-2-L achieves a\n135.6 CIDEr score on the COCO caption benchmark [48],\nutilizing less than 1% of the parameters compared to the\n80B Flamingo [2] model (which has an 84.3 CIDEr score).\nFor region-level grounding and referring expression com-\nprehension tasks, Florence-2-L establishes a new record\nin zero-shot performance achieving a 5.7 improvement in\nFlickr30k [61] Recall@1, and approximately 4%, 8%, and\n8% absolute improvements on Refcoco, Refcoco+, and Re-\nfcocog [94], respectively, compared to the Kosmos-2 [60]\nmodel, which has 1.6B parameters. Additionally, our pre-\ntrained model attains a 35.8% mIOU in the Refcoco refer-\nring expression segmentation (RES) [94] task, a capability\nnot supported by prior foundation models.\n6.3. Generalist Model with Public Supervised Data\nWe demonstrate the versatility and effectiveness of our\nmodel as a vision foundation that can be transferred to var-\nious downstream tasks. We fine-tune Florence-2 models\nby adding a collection of public datasets that cover image-\nlevel, region-level, pixel-level tasks, yielding one generalist\nmodel for various vision tasks. The details of the dataset\ncollection are provided in Appendix B. Tables 5 and 6\ncompare our model with other state-of-the-art models. Our\nkey findings are:\nSimple design for strong performance.\nFlorence-2\ndemonstrates strong performance with standard multi-\nmodality Transformer encoder-decoder without special de-\nsigns, particularly for region-level and pixel-level tasks.\nFor example, Florence-2-L outperforms PolyFormer [49]\non both RefCOCO REC task and RES task by 3.0 Ac-\ncuracy@0.5 and 3.54 mIOU respectively, where Poly-\nFormer [49] adapts specifically designed regression-based\nprediction head for coordinates.\nFlorence-2-L also out-\nperforms previous SOTA method UNINEXT [84] on Re-\nfCOCO by 0.8 Accuracy@0.5, where UNINEXT [84] is\nbased on advanced object detector Deformable DETR [100]\nand DINO [97].\nCompetitive\nperformance\nwith\nfewer\nparameters.\nFlorence-2-L achieves competitive performance without\nthe need for LLMs, showcasing efficiency in handling di-\nverse tasks while maintaining a compact size. For instance,\nFlorence-2-L attains a CIDEr score of 140.0 on the COCO\nCaption karpathy test split [30], outperforming models\nwith significantly more parameters, such as Flamingo (80B\nparameters, 138.1 CIDEr score).\nAdaptable generalization across task levels. Florence-2\ndemonstrates competitive performance across image-level,\npixel-level, and region-level tasks, emphasizing its adapt-\nability and effectiveness in addressing various challenges in\ncomputer vision and natural language processing. For ex-\nample, in the TextVQA task, Florence-2-L sets a new state-\nof-the-art performance with an accuracy of 81.5 without any\nexternal OCR token input, surpassing previous SOTA meth-\n9\nMethod\n#params\nCOCO Cap.\nNoCaps\nTextCaps\nCOCO Det.\nFlickr30k\nRefcoco\nRefcoco+\nRefcocog\nRefcoco RES\ntest\nval\nval\nval2017\ntest\nval\ntest-A test-B\nval\ntest-A test-B\nval\ntest\nval\nCIDEr\nCIDEr\nCIDEr\nmAP\nR@1\nAccuracy\nAccuracy\nAccuracy\nmIoU\nFlamingo [2]\n80B\n84.3\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nKosmos-2 [60]\n1.6B\n-\n-\n-\n-\n78.7\n52.3\n57.4\n47.3\n45.5\n50.7\n42.2\n60.6 61.7\n-\nFlorence-2-B\n0.23B\n133.0\n118.7\n70.1\n34.7\n83.6\n53.9\n58.4\n49.7\n51.5\n56.4\n47.9\n66.3 65.1\n34.6\nFlorence-2-L\n0.77B\n135.6\n120.8\n72.8\n37.5\n84.4\n56.3\n61.6\n51.4\n53.6\n57.9\n49.9\n68.0 67.0\n35.8\nTable 4. Zero-shot performance of generalist vision foundation models. The models do not see the training data of the evaluation tasks\nduring training. Florence-2 models are pre-trained on FLD-5B dataset. Karpathy test split is used for COCO caption evaluation.\nMethod\n#params\nCOCO Caption\nNoCaps\nTextCaps\nVQAv2\nTextVQA\nVizWiz VQA\nKarpathy test\nval\nval\ntest-dev\ntest-dev\ntest-dev\nCIDEr\nCIDEr\nCIDEr\nAcc\nAcc\nAcc\nSpecialist Models\nCoCa [92]\n2.1B\n143.6\n122.4\n-\n82.3\n-\n-\nBLIP-2 [44]\n7.8B\n144.5\n121.6\n-\n82.2\n-\n-\nGIT2 [78]\n5.1B\n145\n126.9\n148.6\n81.7\n67.3\n71.0\nFlamingo [2]\n80B\n138.1\n-\n-\n82.0\n54.1\n65.7\nPaLI [15]\n17B\n149.1\n127.0\n160.0\u25b3\n84.3\n58.8 / 73.1\u25b3\n71.6 / 74.4\u25b3\nPaLI-X [12]\n55B\n149.2\n126.3\n147 / 163.7\u25b3\n86.0\n71.4 / 80.8\u25b3\n70.9 / 74.6\u25b3\nGeneralist Models\nUnified-IO [55]\n2.9B\n-\n100\n-\n77.9\n-\n57.4\nFlorence-2-B\n0.23B\n140.0\n116.7\n143.9\n79.7\n63.6\n63.6\nFlorence-2-L\n0.77B\n143.3\n124.9\n151.1\n81.7\n73.5\n72.6\nTable 5. Performance of specialist and generalist models on captioning and VQA tasks. Specialist Models refer to those that are fine-tuned\nspecifically for each task, while Generalist Models denote a single model fine-tuned in a task-agnostic manner, applicable across all tasks.\n\u25b3 indicates usage of external OCR as input.\nods [12,15].\nThese achievements emphasize Florence-2\u2019s efficiency\nin handling diverse tasks while maintaining a compact size,\nmaking it a unique and valuable asset in the ever-evolving\nlandscape of AI research and applications.\n6.4. Downstream Tasks Fine-tuning\nIn this section, we investigate the performance of our\nsingle model fine-tuning on downstream tasks. This exper-\niment highlights the superiority of Florence-2 pre-training\nover previous approaches, as it demonstrates the effective-\nness of the learned universal image representation. We use\nthe base size model with about 80M parameters in our ex-\nperiments to ensure fair comparison with other methods.\nObject detection and segmentation. We conduct COCO\nobject detection and instance segmentation [48] experi-\nments with Mask R-CNN [26], and COCO object detec-\ntion [48] experiments with DINO [97] to further demon-\nstrate the effectiveness of Florence-2 pre-training. We train\non the train2017 split and evaluate on the val2017 split.\nFor Mask R-CNN [26] experiments, we follow the com-\nmon setup used in [51, 97], we use the standard 1\u00d7 (12\nepochs) schedule with multi-scale training for all experi-\nments. The learning rate is stepped down by a factor of 0.1\nat the 67% and 89% of training epochs. We do not use any\nadditional augmentation (such as random crop, mosaic, etc)\nor optimization techniques (such as EMA, weight normal-\nization) during training to ensure a fair comparison. We do\nnot use any test time augmentation (TTA) either. Thanks\nto the strong universal representation learned by Florence-2\npre-training, we do not require longer training epochs, such\nas 36 epochs in [51, 81, 85, 86], or 100 epochs in [46], to\nachieve better results.\nFor DINO [97] experiments, we train DINO-4scale [97]\ndetector for 12 epochs (1\u00d7) using the same data augmenta-\ntion strategy as employed by [7].\n10\nMethod\n#params\nCOCO Det.\nFlickr30k\nRefcoco\nRefcoco+\nRefcocog\nRefcoco RES\nval2017\ntest\nval\ntest-A test-B\nval\ntest-A test-B\nval\ntest\nval\nmAP\nR@1\nAccuracy\nAccuracy\nAccuracy\nmIoU\nSpecialist Models\nSeqTR [99]\n-\n-\n-\n83.7\n86.5\n81.2\n71.5\n76.3\n64.9\n74.9 74.2\n-\nPolyFormer [49]\n-\n-\n-\n90.4\n92.9\n87.2\n85.0\n89.8\n78.0\n85.8 85.9\n76.9\nUNINEXT [84]\n0.74B\n60.6\n-\n92.6\n94.3\n91.5\n85.2\n89.6\n79.8\n88.7 89.4\n-\nFerret [90]\n13B\n-\n-\n89.5\n92.4\n84.4\n82.8\n88.1\n75.2\n85.8 86.3\n-\nGeneralist Models\nUniTAB [88]\n-\n-\n88.6\n91.1\n83.8\n81.0\n85.4\n71.6\n84.6 84.7\n-\nFlorence-2-B\n0.23B\n41.4\n84.0\n92.6\n94.8\n91.5\n86.8\n91.7\n82.2\n89.8 82.2\n78.0\nFlorence-2-L\n0.77B\n43.4\n85.2\n93.4\n95.3\n92.0\n88.3\n92.9\n83.6\n91.2 91.7\n80.5\nTable 6. Performance of specialist and generalist models on region-level tasks. Specialist Models refer to those that are fine-tuned\nspecifically for each task, while Generalist Models denote a single model fine-tuned in a task-agnostic manner, applicable across all tasks.\n4X efficiency gain\n6.9 points\nimprovements\n(a) Mask-RCNN on COCO detection.\n5.5 points\nimprovements\n4X efficiency gain\n(b) DINO on COCO detection.\n4X efficiency gain\n5.9 points\nimprovements\n(c) UpperNet on ADE20K.\nFigure 6. Training efficiency on COCO object detection and segmentation, and ADE20K semantic segmentation tasks.\nFirst, our base model achieves a strong performance im-\nprovement compared to other approaches. As shown in Ta-\nble 7, our DaViT-B model pre-trained by Florence-2 sur-\npasses previous best base model (ConvNext v2-B), which\nis pre-trained by FCMAE [81], by 0.7 APb using Mask\nRCNN. Importantly, while ConvNeXt v2-B leverages a 3\u00d7\nschedule (36 epochs), our model efficiently employs a 1\u00d7\nschedule (12 epochs) thanks to our powerful pre-trained\nuniversal representation. For DINO framework, our model\nsignificantly outperforms the ViT-B, achieving a notable\nimprovement of 4.2 AP.\nSecond, our pre-training demonstrates higher training ef-\nficiency. As shown in Table 8 and Figure 6, compared to\nthe model with supervised ImageNet-1k pre-training, our\nmodel with Florence-2 pre-training achieves 4x efficiency\nand a significant improvement of 6.9 AP and 5.5 AP with\nMask-RCNN and DINO framework, respectively.\nThird, our pre-training provides a good generic rep-\nresentation without extensive fine-tuning.\nTable 8 indi-\ncates that the models with Florence-2 pre-training main-\ntains competitive performances when the first two stages\nare frozen with only 0.3 and 0.2 drops for Mask-RCNN\nand DINO, respectively. Moreover, our approach with com-\npletely frozen backbone can outperform the model with su-\npervised ImageNet-1k pre-training by 1.6 and 2.4 for Mask-\nRCNN and DINO.\nSemantic segmentation.\nWe conduct semantic seg-\nmentation experiments with UperNet [82] framework on\nADE20k [98] dataset. We mostly follow the training and\nevaluation protocols from Swin [51]. Specifically, we use\ninput size 512\u00d7512 and train the model for 40k iterations\nwith a batch size of 64. We adopt the AdamW [54] op-\ntimizer with the optimal learning rate searched from {8e-\n4,4e-4,2e-4,1e-4}.\nOur results show a similar trend to the object detection\nexperiments. As illustrated in Table 9, our base model out-\nperforms the previous SoTA model, which is BEiT pre-\ntrained ViT-B [4], by 1.3 and 1.4 points in single-scale and\nmulti-scale testing protocol, respectively. With the same\nbackbone architecture of DaViT-B [20], Florence-2 pre-\ntrained model achieves a remarkable improvement of 4.9\npoints and 4\u00d7 efficiency compared to the ImageNet-1k pre-\n11\nMask R-CNN\nDINO\nBackbone\nPretrain\nAPb\nAPm\nAP\nViT-B [46]\nMAE, IN-1k\n51.6\n45.9\n55.0\nSwin-B [51]\nSup IN-1k\n50.2\n-\n53.4\nSwin-B [51]\nSimMIM [83]\n52.3\n-\n-\nFocalAtt-B [86]\nSup IN-1k\n49.0\n43.7\n-\nFocalNet-B [85]\nSup IN-1k\n49.8\n44.1\n54.4\nConvNeXt v1-B [52]\nSup IN-1k\n50.3\n44.9\n52.6\nConvNeXt v2-B [81]\nSup IN-1k\n51.0\n45.6\n-\nConvNeXt v2-B [81]\nFCMAE\n52.9\n46.6\n-\nDaViT-B [20]\nFlorence-2\n53.6\n46.4\n59.2\nTable 7. COCO object detection and instance segmentation\nresults using Mask-RCNN framework, and COCO object detec-\ntion results using DINO-4scale framework. All the entries use\na base size model to ensure a fair comparison. For Mask-RCNN\nexperiments, our method utilizes 1\u00d7 schedule (12 epochs), ViT-B\nuse 100 epochs, all others use 3\u00d7 (36 epochs). For DINO experi-\nments, all the entries use 1\u00d7 schedule except for ViT-B which uses\n50 epochs.\nPretrain\nFrozen stages\nMask R-CNN\nDINO\nUperNet\nAPb\nAPm\nAP\nmIoU\nSup IN1k\nn/a\n46.7\n42.0\n53.7\n49\nUniCL [87]\nn/a\n50.4\n45.0\n57.3\n53.6\nFlorence-2\nn/a\n53.6\n46.4\n59.2\n54.9\nFlorence-2\n[1]\n53.6\n46.3\n59.2\n54.1\nFlorence-2\n[1, 2]\n53.3\n46.1\n59.0\n54.4\nFlorence-2\n[1, 2, 3]\n49.5\n42.9\n56.7\n49.6\nFlorence-2\n[1, 2, 3, 4]\n48.3\n44.5\n56.1\n45.9\nTable 8. Downstream task fine-tuning on COCO and ADE20K\ndataset. COCO object detection using Mask R-CNN and DINO.\nADE20K semantic segmentation using UperNet. All entries use\nDaViT-B with 80M parameters as the backbone and standard 1\u00d7\nschedule.\ntrained counterpart as demonstrated in Table 8 and Figure 6.\n6.5. Ablation Studies\nMultitask transfer. In this study, we aimed to identify the\nmost effective pre-trained model for transfer learning across\nvarious downstream tasks in computer vision. We compared\nthree different models, each pre-trained on a different com-\nbination of tasks:\n\u2022 Image-level Model: pre-trained on image-level tasks\nonly\n\u2022 Image-Region Model: pre-trained on image-level and\nregion-level tasks\n\u2022 Image-Region-Pixel Model:\npre-trained on image-\nlevel, region-level, and pixel-level tasks\nBackbone\nPretrain\nmIoU\nms-mIoU\nViT-B [24]\nSup IN-1k\n47.4\n-\nViT-B [24]\nMAE IN-1k\n48.1\n-\nViT-B [4]\nBEiT\n53.6\n54.1\nViT-B [59]\nBEiTv2 IN-1k\n53.1\n-\nViT-B [59]\nBEiTv2 IN-22k\n53.5\n-\nSwin-B [51]\nSup IN-1k\n48.1\n49.7\nSwin-B [51]\nSup IN-22k\n-\n51.8\nSwin-B [51]\nSimMIM [83]\n-\n52.8\nFocalAtt-B [86]\nSup IN-1k\n49.0\n50.5\nFocalNet-B [85]\nSup IN-1k\n50.5\n51.4\nConvNeXt v1-B [52]\nSup IN-1k\n-\n49.9\nConvNeXt v2-B [81]\nSup IN-1k\n-\n50.5\nConvNeXt v2-B [81]\nFCMAE\n-\n52.1\nDaViT-B [20]\nFlorence-2\n54.9\n55.5\nTable 9. ADE20K semantic segmentation results using Uper-\nNet. The input size is 512\u00d7512 for all the entries, except for mod-\nels with BEiT pre-trained, which use the input size of 640 \u00d7 640.\nFor pre-training, we optimize all models for the same\nnumber of effective samples (72M) on a subset of our FLD-\n5B dataset.\nThese models are then transferred to a combined dataset\nwith four downstream tasks, each representing a differ-\nent level of task granularity: COCO caption (image-level\ntask), COCO object detection (region-level task), Flickr30k\ngrounding (region-level task), RefCOCO referring segmen-\ntation (pixel-level task).\nThe results are shown in Figure 7. The results demon-\nstrate that Image-Region-Pixel Model, pre-trained on all\nthree levels of tasks, consistently demonstrated competitive\nperformance across the four downstream tasks.\nFor the COCO caption task, Image-Region-Pixel Model\ninitially performs worse than Image-level Model and\nImage-Region Model but eventually achieve a final perfor-\nmance (133.4 CIDEr) that is only slightly worse than the\nother models (134.6 CIDEr).\nFor the COCO object detection task, Image-Region-\nPixel Model outperforms Image-level Model by a signifi-\ncant margin (28.3 vs. 0.1) and was only slightly worse than\nImage-Region Model (29.7).\nFor the Flickr30k grounding task, Image-Region-Pixel\nModel shows strong performance (78.1 recall@1), compa-\nrable to Image-Region Model (79.1 recall@1) and signifi-\ncantly better than Image-level Model (62.0 recall@1).\nFor the RefCOCO referring segmentation task, Image-\nRegion-Pixel Model clearly outperforms both Image-level\nModel and Image-Region Model, achieving the highest per-\nformance (31.6 mIoU) compared to the other models (28.4\nand 18.2 mIoU).\nOur findings suggest that the Image-Region-Pixel\nModel, which is pre-trained on tasks at the image, region,\n12\n2500\n5000\n7500 10000 12500 15000 17500 20000\nOptimization Steps\n1.29\n1.30\n1.31\n1.32\n1.33\n1.34\n1.35\nCIDEr\nCOCO Caption\n2500\n5000\n7500 10000 12500 15000 17500 20000\nOptimization Steps\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nAP\nCOCO Object Detection\n2500\n5000\n7500 10000 12500 15000 17500 20000\nOptimization Steps\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nRecall@1\nFlickr30k Grounding\n2500\n5000\n7500 10000 12500 15000 17500 20000\nOptimization Steps\n0\n5\n10\n15\n20\n25\n30\nmIOU\nRefCOCO Referring Segmentation\nimage-level\nimage-level + region-level\nimage-level + region-level + pixel-level\nFigure 7. Multitask transfer. We conduct experiments with three different versions of Florence-2 models, each trained on a different\nlevel of image annotation: image level, image and region level, and image, region, and pixel level. We then evaluate the transfer learning\nperformance of these models on four downstream tasks: COCO caption, COCO object detection, Flickr30k grounding, and Refcoco\nreferring segmentation.\nModel\nCaption\nDetection\nGrounding\nRES\nCIDEr\nAP\nRecall@1\nmIOU oIOU\nBase\n118.7\n19.7\n76.3\n18.6\n17.8\nLarge\n124.4\n22.6\n78.2\n21.5\n19.1\nTable 10. Model scaling. Zero-shot performance on COCO cap-\ntion and COCO object detection, Flickr30k grounding, RefCOCO\nreferring expression segmentation(RES).\nand pixel levels, is the most effective base model for transfer\nlearning across various computer vision tasks. This model\nshows strong performance on all four downstream tasks\nwe evaluated, and consistently outperforms the Image-level\nModel and matches or exceeds the Image-Region Model in\nperformance. By pre-training a model on tasks at differ-\nent levels of granularity, we can ensure that the base model\nis better prepared to handle a diverse range of downstream\ntasks, offering a versatile and robust solution for transfer\nlearning in computer vision.\nModel scaling. We aimed to investigate the impact of in-\ncreasing model capacity on zero-shot performance on var-\nious downstream tasks in computer vision. We compared\ntwo models: Florence-2-B and Florence-2-L, which have\n232M and 771M parameters, respectively. The model archi-\ntectures are described in Table 15. We show the zero-shot\nperformance on four downstream tasks in Table 10. The\nlarge model clearly outperforms the base model across var-\nious downstream tasks.\nData scaling.\nWe conducted experiments to study how\nzero-shot performance on various computer vision tasks is\naffected by the scale of pre-training data. We used four dif-\nferent data sizes for pre-training: 0.12M, 0.36M, 1.2M, and\n12M images. All models were trained with the same effec-\ntive sample size (72M) on a subset of FLD-5B data.\nTable 11 presents the zero-shot performance results on\nData\nCaption\nDetection\nGrounding\nRES\nsize\nCIDEr\nAP\nRecall@1\nmIOU oIOU\n0.12M\n102.8\n16.1\n74.0\n15.9\n16.6\n0.36M\n114.3\n18.7\n75.8\n16.6\n16.4\n1.2M\n118.1\n18.9\n76.3\n19.3\n18.4\n12M\n118.7\n19.7\n76.3\n18.6\n17.8\nTable 11. Data scaling. Zero-shot performance on COCO caption,\nCOCO object detection, Flickr30k grounding, COCORef referring\nsegmentation.\nCOCO caption, COCO object detection, Flickr30k ground-\ning, and RefCoco referring segmentation (RES) tasks. We\ncan observe a trend of improved zero-shot performance on\nthe downstream tasks as the pre-training data size increases\n(except for RES, 1.2M data has slightly better performance\ncompared to 12M).\nOur experiments on data scaling demonstrate that larger\npre-training data sizes generally lead to improved zero-shot\nperformance across a variety of downstream tasks in com-\nputer vision. This finding suggests that investing in larger\npre-training datasets can provide a more effective and ver-\nsatile foundation for handling a wide range of downstream\ntasks.\nOur approach to scaling data is significantly more effi-\ncient than relying solely on human annotations, as most of\nthe annotation generation is performed using model infer-\nence. By leveraging specialist models to generate annota-\ntions, we can substantially reduce the time and cost asso-\nciated with manual annotation efforts, which often involve\nlabor-intensive processes and may be subject to human er-\nrors or inconsistencies.\nFurthermore, utilizing model-generated annotations en-\nables us to scale the pre-training datasets more rapidly and\nefficiently, allowing us to explore the impact of larger data\n13\nCaption\nDetection\nGrounding\nRES\nV Pre L Pre\nCIDEr\nAP\nRecall@1\nmIOU oIOU\nFreeze Vision Encoder\n\u2713\n\u2713\n120.0\n6.9\n66.3\n9.9\n13.6\nUnfreeze Vision Encoder\n\u2713\n81.3\n4.9\n69.0\n15.3\n15.6\n\u2713\n117.4\n19.6\n75.2\n21.5\n19.3\n\u2713\n\u2713\n118.7\n19.7\n76.3\n18.6\n17.8\nTable 12. Basic components. Zero-shot performance on COCO\ncaption, COCO object detection, Flickr30k grounding, and CO-\nCORef referring segmentation. V Pre and L Pre indicate that using\nvision and language pre-training initialization, respectively.\nsizes on model performance across various downstream\ntasks in computer vision. This not only facilitates the devel-\nopment of more effective and versatile foundation models\nbut also ensures that the annotation process remains sus-\ntainable and scalable as the need for high-quality labeled\ndata continues to grow.\nIn summary, our data scaling approach offers a more ef-\nficient alternative to traditional human annotation methods\nby harnessing the power of specialist models for annotation\ngeneration. This strategy enables us to accelerate the pre-\ntraining process, optimize model performance, and effec-\ntively manage the ever-increasing demand for labeled data\nin the field of computer vision.\nTraining settings. We analyze the basic model training set-\ntings for the two primary components of our model, namely\nthe vision encoder and the multi-modality encoder-decoder.\nThe experiment results are presented in Table 12\nWe observe that freezing the vision encoders does not\naffect the performance on tasks that require image-level un-\nderstanding, but it significantly degrades the performance\non tasks that require region-level or pixel-level understand-\ning (e.g., AP on COCO object detection drops from 19.7 to\n6.9). Previous methods for pre-training vision foundation\nmodels mainly focus on image-level tasks (e.g., image clas-\nsification [27,38], image-text contrastive learning [64,95]),\nwhich may not provide them with sufficient region-level and\npixel-level skills for downstream tasks. Therefore, it is im-\nportant to unfreeze the vision backbone, enabling it to learn\nregion-level and pixel-level features for various downstream\ntasks.\nThe effect of language pre-training weights on multi-\nmodal encoder-decoder tasks varies depending on the task.\nTasks that require more text understanding, such as caption-\ning and grounding, benefit slightly from using language pre-\ntraining weights (e.g., COCO caption, Flickr30k ground-\ning). Tasks that are mostly vision-focused, such as object\ndetection and region segmentation, do not gain much from\nusing language pre-training weights (for COCO object de-\ntection, the gain is only 0.1; for RES tasks, which use only\nlocalization tokens, the drop is 2.91 mIOU).\nWe investigate the effects of different training configura-\ntions on the performance of a foundation model in region-\nlevel and pixel-level tasks. We find that unfreezing the vi-\nsion backbone is crucial for enhancing the model\u2019s ability to\nlearn from regions and pixels, which is beneficial for trans-\nferring to various downstream tasks. Moreover, we observe\nthat using language pre-training weights can help the model\nin tasks that require text understanding, but have less im-\npact on tasks that are purely vision-based. These results\noffer useful guidance for choosing the best training settings\nfor different computer vision tasks.\n7. Related Works\n7.1. Vision-Language Foundation Models\nRecent vision-language pre-training models [29, 64, 95]\nhave demonstrated impressive zero-shot transfer abilities to\nvision-language alignment and image classification tasks,\nthanks to the alignment of vision and text embeddings ex-\ntracted from respective encoders through contrastive learn-\ning objectives [58, 74]. These models (e.g., [95]), trained\non weakly large-scale image-text data, have been further\nextended to more downstream tasks such as object de-\ntection, achieving state-of-the-art performance with task-\nspecific adaptation heads.\nIn contrast, other studies [2, 45, 78, 92] propose us-\ning a multi-modality decoder to predict text in an autore-\ngressive manner with language modeling pre-training ob-\njectives. Techniques for fusing vision and language em-\nbeddings vary: GIT [78] concatenates vision and text to-\nkens as decoder input and designs a casual attention mask,\nCoCa [92] uses attentional poolers with learnable queries\nto select task-specific vision representations which are then\ncross-attended via the decoder, and Flamingo [2] pools a\nfixed number of vision tokens with a Perceiver Resampler\nand adds new learnable cross-attention layers to the decoder\nwhile freezing the pre-trained vision encoder and text de-\ncoder.\nBeyond image captioning pre-training task, some re-\nsearch [15,55,79] attempts to formulate more vision tasks in\na unified sequence-to-sequence learning paradigm, includ-\ning object detection and image segmentation. Customized\nspecial tokens accommodate representations beyond pure\ntext, such as bounding boxes [10, 55, 79]. This approach\nuses the same architecture for pre-training and downstream\ntasks, potentially using the same set of weights for all tasks.\nOur method, which falls into this category, aims to ob-\ntain foundation models that understand dense information\nbeyond simple image-level captions.\nIt shares the same\nencoder-decoder design as other multi-modality encoder-\n14\ndecoder models [15, 55] adapted for sequence-to-sequence\nlearning, but uses our built large-scale comprehensive anno-\ntation data instead of combining existing sparse annotated\ndata.\n7.2. Vision Datasets\nComprehensive annotations. The quest for comprehen-\nsive understanding of visual scenes, the holy grail of com-\nputer vision [36], has evolved from focusing on individ-\nual datasets each targeting a single perspective, e.g., im-\nage classification [18], to providing multi-perspective [36,\n40, 48], comprehensive annotations for every visual data\npoint. Notable datasets like MS-COCO [13, 48] and Vi-\nsual Genome [36] integrate various types of annotations,\nenabling richer understanding in spatial and semantic gran-\nularities and better model interactions across annotations.\nHowever, due to the high cost of human verification, these\nannotations are limited in size. Our datasets, while large-\nscale, maintain comprehensive annotations covering text,\nregion-text pairs, and text-phrase-region triplets, with re-\nduced human involvement.\nScalable annotations.:\nOver the past decade, vision\ndatasets have rapidly scaled up from thousands [37, 42] to\nbillion examples [29, 96] to encompass more visual con-\ncepts for better generalization. This shift is evident in re-\ncent foundation models that employ massive quantities of\ndata [5]. These large datasets typically collect images from\nthe web and parse noisy annotations from the corresponding\nmetadata, such as category label from query [75, 96], short\ndescription from alt-text [29,64], as well as detailed descrip-\ntion from interleaved text [2, 41]. Despite their diversity,\nthese annotations suffer from randomness and limited types\n(i.e., texts only). Some works [32, 45] attempt to scale up\nannotations using pseudo-label generation with iteratively\ntrained models, which offer higher quality without signifi-\ncant diversity loss. Our data pipeline extends these large-\nscale, web-crawled noisy annotations with higher-quality,\nautonomous annotations generated from multiple special-\nist models. The pipeline iteratively refines labels and com-\npletes missing pieces, resulting in a scalable and compre-\nhensive dataset for learning a unified visual representation.\n8. Conclusion\nThe Florence Project endeavors to develop a founda-\ntional vision model endowed with a diverse array of per-\nceptual capabilities, encompassing spatial hierarchy and\nsemantic granularity. To this end, we construct FLD-5B\ndataset containing an extensive collection of 126M images\npaired with 5B comprehensive annotations, which are col-\nlected by the Florence data engine. Subsequently, we pre-\ntrain Florence-2 on this rich dataset through comprehensive\nmultitask learning in a unified manner. Florence-2 has ex-\nhibited remarkable zero-shot capabilities that extend across\na wide spectrum of visual tasks, such as captioning, ob-\nject detection, visual grounding, and referring segmenta-\ntion, among others. The experimental findings underscore\nthe potency of the universal representation pre-trained by\nFlorence-2, revealing its substantial contributions to the en-\nhancement of a multitude of downstream tasks.\nAcknowledgment. We would like to express our heartfelt\ngratitude to all the contributors from the Azure AI team\nwho worked on the Florence project. We sincerely appre-\nciate Misha Bilenko for the invaluable guidance and sup-\nport. Our thanks are extended to Yi-Ling Chen, Mengchen\nLiu, Yen-Chun Chen and Dongdong Chen for engaging in\nhelpful discussions and to Yunsheng Li for their assistance\nwith segmentation annotations. Deep appreciation is also\nexpressed to Qingfen Lin, Ryan Menezes, Kuan Lu, Gabe\nBlanco, Shohei Ono, Ping Jin, Jiahe Zhou, Xiong Qiao,\nTong Bai, Xingchao Peng, Pei Guo, Lihang Li for providing\nvaluable feedback in downstream applications discussions.\nSpecial thanks to Cha Zhang, Jinyu Li, Min Gao, Christina\nSun, Oliver Ernst, Kevin Pan, Mei Gao for their work on\ndata annotation support and insightful discussions in data\npipeline.\nFurthermore, we would like to thank Thomas\nSoemo, Nguyen Bach for their constructive feedback.\nReferences\n[1] Azure ai services.\nhttps://azure.microsoft.\ncom / en - us / products / ai - services ?\nactivetab = pivot : azureopenaiservicetab.\nAccessed: 2023-10-13. 6\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, et al.\nFlamingo: a visual language model for few-shot learn-\ning. Advances in Neural Information Processing Systems,\n35:23716\u201323736, 2022. 3, 7, 9, 10, 14, 15\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\nLayer normalization, 2016. 4\n[4] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT:\nBERT pre-training of image transformers. In International\nConference on Learning Representations, 2022. 2, 11, 12\n[5] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-\nman, Simran Arora, Sydney von Arx, Michael S Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.\nOn the opportunities and risks of foundation models. arXiv\npreprint arXiv:2108.07258, 2021. 1, 15\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\n15\nClark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. Language mod-\nels are few-shot learners. In H. Larochelle, M. Ranzato,\nR. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in\nNeural Information Processing Systems, volume 33, pages\n1877\u20131901. Curran Associates, Inc., 2020. 1\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. In European\nconference on computer vision, pages 213\u2013229. Springer,\n2020. 10\n[8] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal,\nPiotr Bojanowski, and Armand Joulin. Unsupervised learn-\ning of visual features by contrasting cluster assignments. In\nAdvances in Neural Information Processing Systems, vol-\nume 33, 2020. 2\n[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on\nmachine learning, pages 1597\u20131607. PMLR, 2020. 2\n[10] Ting Chen, Saurabh Saxena, Lala Li, David J. Fleet, and\nGeoffrey Hinton. Pix2seq: A language modeling frame-\nwork for object detection, 2022. 4, 14\n[11] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J\nFleet, and Geoffrey E Hinton. A unified sequence interface\nfor vision tasks. Advances in Neural Information Process-\ning Systems, 35:31333\u201331346, 2022. 4\n[12] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Se-\nbastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On\nscaling up a multilingual vision and language model. arXiv\npreprint arXiv:2305.18565, 2023. 3, 10\n[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\nVedantam, Saurabh Gupta, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco captions: Data collection and eval-\nuation server. arXiv preprint arXiv:1504.00325, 2015. 3, 6,\n15, 20\n[14] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,\nJialin Wu,\nPaul Voigtlaender,\nBasil Mustafa,\nSebas-\ntian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski,\nDaniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran\nRong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu\nSoricut.\nPali-3 vision language models: Smaller, faster,\nstronger, 2023. 3\n[15] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-\nvanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\nAdam Grycner, Basil Mustafa, Lucas Beyer, Alexander\nKolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Has-\nsan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal,\nJames Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini,\nChao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas\nSteiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. Pali: A jointly-scaled multilingual language-\nimage model, 2022. 3, 4, 9, 10, 14, 15\n[16] Bowen Cheng, Ishan Misra, Alexander G. Schwing,\nAlexander Kirillov, and Rohit Girdhar. Masked-attention\nmask transformer for universal image segmentation. 2022.\n2\n[17] Kyunghyun Cho, Bart Van Merri\u00a8enboer, Caglar Gulcehre,\nDzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\nYoshua Bengio. Learning phrase representations using rnn\nencoder-decoder for statistical machine translation. arXiv\npreprint arXiv:1406.1078, 2014. 2\n[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical im-\nage database. In 2009 IEEE conference on computer vision\nand pattern recognition, pages 248\u2013255. Ieee, 2009. 2, 3,\n4, 15\n[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding, 2019. 1, 2\n[20] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong\nWang, and Lu Yuan. Davit: Dual attention vision trans-\nformers. In Computer Vision\u2013ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23\u201327, 2022, Pro-\nceedings, Part XXIV, pages 74\u201392. Springer, 2022. 4, 11,\n12\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-\nage is worth 16x16 words: Transformers for image recog-\nnition at scale, 2021. 7\n[22] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. Making the V in VQA matter: El-\nevating the role of image understanding in Visual Question\nAnswering. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017. 20\n[23] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizwiz grand challenge: Answering visual questions from\nblind people. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3608\u20133617,\n2018. 20\n[24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scal-\nable vision learners. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n16000\u201316009, 2022. 2, 12\n[25] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n9729\u20139738, 2020. 2\n[26] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE interna-\ntional conference on computer vision, pages 2961\u20132969,\n2017. 2, 10\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 14\n[28] Matthew Honnibal, Ines Montani, Sofie Van Landeghem,\nAdriane Boyd, et al. spacy: Industrial-strength natural lan-\nguage processing in python. 2020. 5, 8\n16\n[29] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li,\nand Tom Duerig.\nScaling up visual and vision-language\nrepresentation learning with noisy text supervision, 2021.\n9, 14, 15\n[30] Andrej Karpathy and Li Fei-Fei.\nDeep visual-semantic\nalignments for generating image descriptions. 2015 IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 3128\u20133137, 2014. 9\n[31] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg. Referitgame: Referring to objects in pho-\ntographs of natural scenes. In Proceedings of the 2014 con-\nference on empirical methods in natural language process-\ning (EMNLP), pages 787\u2013798, 2014. 2, 20\n[32] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment\nanything. arXiv preprint arXiv:2304.02643, 2023. 2, 6, 7,\n15\n[33] Aniket Kittur, Ed Chi, Bryan A Pendleton, Bongwon Suh,\nand Todd Mytkowicz. Power of the few vs. wisdom of the\ncrowd: Wikipedia and the rise of the bourgeoisie. World\nwide web, 1(2):19, 2007. 2\n[34] Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li\nFei-Fei.\nA hierarchical approach for generating descrip-\ntive image paragraphs. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n317\u2013325, 2017. 3\n[35] Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li\nFei-Fei. A hierarchical approach for generating descriptive\nimage paragraphs. In Computer Vision and Patterm Recog-\nnition (CVPR), 2017. 20\n[36] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International journal of computer vi-\nsion, 123:32\u201373, 2017. 15\n[37] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 15\n[38] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In Advances in neural information processing sys-\ntems, pages 1097\u20131105, 2012. 2, 14\n[39] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Ste-\nfan Popov, Matteo Malloci, Alexander Kolesnikov, Tom\nDuerig, and Vittorio Ferrari. The open images dataset v4.\nInternational Journal of Computer Vision, 128(7):1956\u2013\n1981, mar 2020. 20\n[40] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Ste-\nfan Popov, Matteo Malloci, Alexander Kolesnikov, et al.\nThe open images dataset v4: Unified image classification,\nobject detection, and visual relationship detection at scale.\nInternational Journal of Computer Vision, 128(7):1956\u2013\n1981, 2020. 4, 7, 15\n[41] Hugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang,\nSiddharth Karamcheti, Alexander M Rush, Douwe Kiela,\net al.\nObelisc:\nAn open web-scale filtered dataset\nof interleaved image-text documents.\narXiv preprint\narXiv:2306.16527, 2023. 15\n[42] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist hand-\nwritten digit database.\nATT Labs [Online]. Available:\nhttp://yann.lecun.com/exdb/mnist, 2, 2010. 15\n[43] Mike\nLewis,\nYinhan\nLiu,\nNaman\nGoyal,\nMarjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy, Ves\nStoyanov, and Luke Zettlemoyer.\nBart:\nDenoising\nsequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension, 2019. 1, 4, 9\n[44] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv\npreprint arXiv:2301.12597, 2023. 10\n[45] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation.\nIn\nInternational Conference on Machine Learning, pages\n12888\u201312900. PMLR, 2022. 2, 5, 14, 15\n[46] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object\ndetection. In European Conference on Computer Vision,\npages 280\u2013296. Springer, 2022. 10, 12\n[47] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir\nBourdev, Ross Girshick, James Hays, Pietro Perona, Deva\nRamanan, C. Lawrence Zitnick, and Piotr Doll\u00b4ar. Microsoft\ncoco: Common objects in context, 2015. 20\n[48] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and\nC Lawrence Zitnick. Microsoft coco: Common objects in\ncontext. In Computer Vision\u2013ECCV 2014: 13th European\nConference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pages 740\u2013755. Springer, 2014. 2,\n9, 10, 15\n[49] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Ku-\nmar Satzoda, Vijay Mahadevan, and R Manmatha. Poly-\nformer: Referring image segmentation as sequential poly-\ngon generation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n18653\u201318663, 2023. 9, 11\n[50] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection. arXiv preprint\narXiv:2303.05499, 2023. 6\n[51] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows, 2021. 10, 11, 12\n[52] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Fe-\nichtenhofer, Trevor Darrell, and Saining Xie. A convnet\nfor the 2020s. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 11976\u2013\n11986, 2022. 12\n17\n[53] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\ndescent with warm restarts, 2017. 9\n[54] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization, 2019. 9, 11\n[55] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh\nMottaghi, and Aniruddha Kembhavi. Unified-io: A unified\nmodel for vision, language, and multi-modal tasks, 2022.\n3, 4, 10, 14, 15\n[56] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy. Generation\nand comprehension of unambiguous object descriptions. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pages 11\u201320, 2016. 2, 20\n[57] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge, 2019. 20\n[58] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 14\n[59] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and\nFuru Wei. BEiT v2: Masked image modeling with vector-\nquantized visual tokenizers. 2022. 12\n[60] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 7, 9, 10, 34, 35\n[61] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-\nnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2641\u20132649, 2015. 2, 9\n[62] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo,\nRadu Soricut, and Vittorio Ferrari. Connecting vision and\nlanguage with localized narratives. In ECCV, 2020. 20\n[63] Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian,\nTodor Mihaylov, Simon Vandenhende, Yash Patel, Yi Wen,\nVignesh Ramanathan, and Dhruv Mahajan. Filtering, distil-\nlation, and hard negatives for vision-language pre-training.\narXiv preprint arXiv:2301.02280, 2023. 5, 8\n[64] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021. 2, 7, 9, 14, 15\n[65] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsu-\npervised multitask learners. 2019. 1, 2\n[66] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J Liu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. The Journal of Ma-\nchine Learning Research, 21(1):5485\u20135551, 2020. 1, 2\n[67] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He.\nDeepspeed: System optimizations enable\ntraining deep learning models with over 100 billion param-\neters. In Proceedings of the 26th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data Min-\ning, pages 3505\u20133506, 2020. 9\n[68] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki.\nLaion-\n400m: Open dataset of clip-filtered 400 million image-text\npairs. arXiv preprint arXiv:2111.02114, 2021. 4\n[69] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi.\nA-okvqa: A\nbenchmark for visual question answering using world\nknowledge, 2022. 20\n[70] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365:\nA large-scale, high-quality dataset for object detection. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 8430\u20138439, 2019. 4, 5, 7, 20\n[71] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In Pro-\nceedings of ACL, 2018. 4\n[72] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image caption-\ning with reading comprehension, 2020. 20\n[73] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 8317\u20138326, 2019. 20\n[74] Kihyuk Sohn. Improved deep metric learning with multi-\nclass n-pair loss objective. Advances in neural information\nprocessing systems, 29, 2016. 14\n[75] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In Proceedings of the IEEE international\nconference on computer vision, pages 843\u2013852, 2017. 15\n[76] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence\nto sequence learning with neural networks. Advances in\nneural information processing systems, 27, 2014. 2\n[77] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin.\nAttention is all you need.\nIn Ad-\nvances in neural information processing systems, pages\n5998\u20136008, 2017. 4\n[78] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,\nKevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan\nWang. Git: A generative image-to-text transformer for vi-\nsion and language, 2022. 2, 10, 14\n[79] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang.\nOfa: Unifying architectures, tasks, and\nmodalities through a simple sequence-to-sequence learning\nframework, 2022. 3, 4, 14\n[80] Nic M Weststrate, Susan Bluck, and Judith Gl\u00a8uck. Wisdom\nof the crowd. The Cambridge handbook of wisdom, pages\n97\u2013121, 2019. 2\n18\n[81] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei\nChen, Zhuang Liu, In So Kweon, and Saining Xie. Con-\nvnext v2: Co-designing and scaling convnets with masked\nautoencoders. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16133\u2013\n16142, 2023. 10, 11, 12\n[82] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Unified perceptual parsing for scene understand-\ning. In Proceedings of the European conference on com-\nputer vision (ECCV), pages 418\u2013434, 2018. 2, 11\n[83] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\nBao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A sim-\nple framework for masked image modeling. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 9653\u20139663, 2022. 12\n[84] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Ze-\nhuan Yuan, and Huchuan Lu. Universal instance percep-\ntion as object discovery and retrieval. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 15325\u201315336, 2023. 9, 11\n[85] Jianwei Yang, Chunyuan Li, Xiyang Dai, and Jianfeng Gao.\nFocal modulation networks. Advances in Neural Informa-\ntion Processing Systems, 35:4203\u20134217, 2022. 10, 12\n[86] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang\nDai, Bin Xiao, Lu Yuan, and Jianfeng Gao.\nFocal self-\nattention for local-global interactions in vision transform-\ners. arXiv preprint arXiv:2107.00641, 2021. 10, 12\n[87] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao,\nCe Liu, Lu Yuan, and Jianfeng Gao. Unified contrastive\nlearning in image-text-label space, 2022. 9, 12\n[88] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,\nFaisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang.\nUnitab: Unifying text and box outputs for grounded vision-\nlanguage modeling. In European Conference on Computer\nVision, pages 521\u2013539. Springer, 2022. 11\n[89] Sheng Kung Michael Yi, Mark Steyvers, Michael D Lee,\nand Matthew J Dry. The wisdom of the crowd in combina-\ntorial problems. Cognitive science, 36(3):452\u2013470, 2012.\n2\n[90] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du,\nBowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu\nChang, and Yinfei Yang. Ferret: Refer and ground anything\nanywhere at any granularity, 2023. 11\n[91] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. From image descriptions to visual denotations:\nNew similarity metrics for semantic inference over event\ndescriptions. Transactions of the Association for Computa-\ntional Linguistics, 2:67\u201378, 2014. 3\n[92] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models, 2022. 9, 10,\n14\n[93] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\nand Tamara L Berg. Modeling context in referring expres-\nsions.\nIn Computer Vision\u2013ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part II 14, pages 69\u201385. Springer, 2016.\n2, 20\n[94] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C.\nBerg, and Tamara L. Berg.\nModeling context in refer-\nring expressions. In Bastian Leibe, Jiri Matas, Nicu Sebe,\nand Max Welling, editors, Computer Vision \u2013 ECCV 2016,\npages 69\u201385, Cham, 2016. Springer International Publish-\ning. 9\n[95] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,\nBoxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng\nLiu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang,\nBin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Lu-\nowei Zhou, and Pengchuan Zhang.\nFlorence:\nA new\nfoundation model for computer vision.\narXiv preprint\narXiv:2111.11432, 2021. 2, 6, 9, 14\n[96] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and\nLucas Beyer. Scaling vision transformers. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 12104\u201312113, 2022. 15\n[97] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\nZhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr\nwith improved denoising anchor boxes for end-to-end ob-\nject detection. arXiv preprint arXiv:2203.03605, 2022. 2,\n6, 9, 10\n[98] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\nade20k dataset. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 633\u2013641,\n2017. 2, 11\n[99] Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo,\nXingjia Pan, Mingbao Lin, Chao Chen, Liujuan Cao, Xi-\naoshuai Sun, and Rongrong Ji. Seqtr: A simple yet univer-\nsal network for visual grounding. In European Conference\non Computer Vision, pages 598\u2013615. Springer, 2022. 11\n[100] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection.\narXiv preprint\narXiv:2010.04159, 2020. 9\n19\nA. Supported Tasks and Annotations in Florence-2\nTask\nAnnotation Type\nPrompt Input\nOutput\nCaption\nText\nImage, text\nText\nDetailed caption\nText\nImage, text\nText\nMore detailed caption\nText\nImage, text\nText\nRegion proposal\nRegion\nImage, text\nRegion\nObject detection\nRegion-Text\nImage, text\nText, region\nDense region caption\nRegion-Text\nImage, text\nText, region\nPhrase grounding\nText-Phrase-Region\nImage, text\nText, region\nReferring expression comprehension\nRegion-Text\nImage, text\nText, region\nOpen vocabulary detection\nRegion-Text\nImage, text\nText, region\nReferring segmentation\nRegion-Text\nImage, text\nText, region\nRegion to text\nRegion-Text\nImage, text, region\nText\nText detection and recognition\nRegion-Text\nImage, text\nText, region\nTable 13. Supported Tasks and annotations used for Florence-2 pretraining.\nB. Supervised Data Collection for Generalist Model Fine-tuning\nTask\nDataset\nCaption\nCOCO [13]\nText Caption\nTextCaps [72]\nParagraph caption\nStandford Paragraph Caption [35]\nDetailed caption\nLocalized Narratives [62]\nDetection\nCOCO [47], Object365\u2217 [70], Open Images\u2217 [39]\nPhrase Grounding\nFlickr30k, Object365\u2217 [70], Open Images\u2217 [39]\nReferring expression\nRefCOCO-mix (RefCOCO, RefCOCO+, RefCOCOg) [31,56,93]\nReferring expression segmentation\nRefCOCO-mix (RefCOCO, RefCOCO+, RefCOCOg) [31,56,93]\nRegion to category\nCOCO [47], Object365\u2217 [70], Open Images\u2217 [39]\nRegion to polygon\nCOCO [47] (after deduplicating RefCOCO-mix val)\nVQA\nVQAv2 [22], OKVQA [57], AOKVQA [69], TextVQA [73], ViZWiz VQA [23]\nOCR\nSubset from FLD-5B OCR (2 millon samples)\nTable 14. Collection of dataset for finetuning one single generalist model for downstream tasks evaluation. \u2217 indicates using the annotations\nfrom FLD-5B, which merges original annotations with ours.\nC. Model Configuration\nModel\nImage Encoder (DaViT)\nEncoder-Decoder (Transformer)\ndimensions\nblocks\nheads/groups\n#params\nencoder layers\ndecoder layers\ndimensions\n#params\nFlorence-2-B\n[128, 256, 512, 1024]\n[1, 1, 9, 1]\n[4, 8, 16, 32]\n90M\n6\n6\n768\n140M\nFlorence-2-L\n[256, 512, 1024, 2048]\n[1, 1, 9, 1]\n[8, 16, 32, 64]\n360M\n12\n12\n1024\n410M\nTable 15. Model configuration of different size.\n20\nD. More Examples of Annotations in FLD-5B\n(a) Region only\n(b) Region w/ phrases\n(c) Region w/ brief text\na couple of1 beds2 sitting next to each other in a\nroom3.\n1\na police car1 parked next to other cars2 on\na dirt road3.\n1\nalbert einstein1 quote2 if you can \u2019 t explain it\nsimply, you don \u2019 t understand it well enough.\n1\na wooden deck1 with a table2 and chairs3 in it.\n1\n(d) Text-phrase-region w/ brief text\nthe image shows a room1 with two massage ta-\nbles2 3 tables, each with blue cloth4 draped over\nthem, a wooden door5, a chair6 with cushions7,\na mirror8 on the wall9,\na curtain10, and ceiling\nlights11 and\nchandeliers12 on\nthe roof13.\nthe\nroom14 is located in siladon spa phuket, with\na\nprice tag15 of 500 33316 white and a 10 percent\ndiscount.\n1\nthe image shows a police car1 parked next\nto another police car2 on a dirt road3 sur-\nrounded by trees4, poles5, grass6, houses,\nand a clear blue sky7.\n1\nthe image shows a black and white portrait1\nof albert einstein2 with the quote3 \u201d if you can\n\u2019 t explain it simply, you don \u2019 t understand\nit well enough \u201d written in bold white font4\nunderneath.\nthe man5 in the portrait has a\nserious expression on his face6, as if he is\ndeep in thought.\n1\nthe image shows a 3 bed semi detached house1\n2 in englefield green with a gated3, garage4, a\nwooden deck5 with a table6 7 and chairs8 on it,\na pot9 with a plant10 on the table11, a bench12,\na house13 14 with a roof15, pots16 with plants17,\ngrass18 on the ground, trees19 in the back, and a\nsky20 with clouds21 in the background.\n1\n(e) Text-phrase-region w/ detailed text\nthe image shows a modern and luxurious spa - like\nroom with two massage tables in the center.\nthe\ntables1 are covered with blue linens2 and have a\nwooden frame. on the right side of the room, there\nis a large painting3 of a hanukkah menorah on the\nwall. above the tables, there are two crystal chan-\ndeliers4 hanging from the\nceiling5.\nthe walls6\nare painted in a light beige color and\nthe floor7\nis made of wood. the room8 has a large window9\nwith sheer curtains10, allowing natural light to en-\nter. there is also a wooden door11 on the left side,\nwhich leads to another room.\n1\nthe image shows a police car parked on a dirt\nroad in a park.\nthe car1 is white with blue\nand black checkered\nstripes2 on the hood\nand roof.\nthe word3 \u201d own \u201d is written in\nblue letters on the front of the car. there is a\nred siren4 on top of the police car and a yel-\nlow license plate5 that reads \u201d be 14 tc6 \u201d. in\nthe background, there are other police cars7\nparked on the side of the road and trees8 and\ngrass on the ground.\n1\nthe image is a black and white portrait of al-\nbert einstein, the famous physicist1 and no-\nbel prize - winning physicist. he is shown sit-\nting in a chair with his hands2 clasped to-\ngether in prayer. he has a serious expres-\nsion on his face and is looking off to the side.\nthe background is blurred, but it appears to\nbe a room with a window. the image is ac-\ncompanied by a quote from einstein that reads,\n\u201d if you can \u2019 t explain it simply, you don \u2019 t un-\nderstand it well enough. \u201d.\n1\nthe image shows a wooden deck1 in a backyard\nwith a wooden table2 and six chairs3.\nthe deck4\nis surrounded by a wooden fence5 and there are\nseveral potted plants6 and trees7 in the background.\non the left side of the deck, there is a red brick\nbuilding8 with a small garden9 10 shed on the right\nside.\nthe sky11 is overcast and the overall atmo-\nsphere of the backyard is peaceful and serene.\n1\n(f) Text-phrase-region w/ more detailed text\nFigure 8. Examples of annotations in FLD-5B.\n21\n(a) Region only\n(b) Region w/ phrases\n(c) Region w/ brief text\nknow your cows1 by jack2 by ard3.\n1\na group of people1 standing next to each other holding a\nball2.\n1\na panda bear1 walking on the ground\nin front of a rock2.\n1\na close up of a plant1 with a flower2 in the\nbackground.\n1\n(d) Text-phrase-region w/ brief text\nthe image shows a brown and white cow1 graz-\ning on a lush green field2 with trees3 in the back-\nground and a blue sky4 above.\nthe text5 on the\nright reads \u201d know your cows6: 44 breeds7 from\naberdeen angus8 to wagyu, with essential facts9\non history, country of origin, physical attributes \u201d.\n1\nthe image shows two men1 and two women2 standing\nnext to each other, all with smiles3 on their faces4, hold-\ning a ball5 in their hands6. in the background there is a\nwall7, a door8, a net9, and a few other objects.\n1\nthe image shows a giant panda1\nwalking across a dirt road2 in a\nzoo, surrounded by lush green\nplants3 and a large rock4 in the\nbackground.\n1\nthe image shows a close up of a flower1 in\nthe grass2 with its green petals3 and purple\nflowers4 in the foreground, and a few plants5\n6 in the background.\n1\n(e) Text-phrase-region w/ detailed text\nthe image is a cover of a book titled \u201d know your\ncows1 \u201d by jack byard. it features a photograph of\na cow standing in a field2 with other cows graz-\ning in the background. the cow3 in the foreground\nis a brown and white cow4 with a big nose5 and\nbig ears6, looking directly at the camera. the back-\nground is a blue sky with white clouds.\nthe title7\nof the book is written in white text on the top right\ncorner of the image.\n1\nthe image shows four people standing in a gymnasium.\non the left, there is\na man1 wearing a white\nt\nshirt2\nwith red stripes. he is holding a green netball ball3 in his\nhands4. next to him, there are two women5, one wear-\ning a black jacket and the other wearing a blue jacket.\nall four people6 are smiling and appear to be happy. the\nbackground shows a yellow wall and a blue net.\n1\nthe image is of a giant panda walk-\ning on a rocky path. the panda1\n2 is facing towards the right side\nof the image and appears to be\nwalking with purpose. it has a black\nand white fur coat and a round\nface with large black eyes.\nits\nears3 are perked up and its mouth4\nis slightly open, as if it is about to\nspeak. the background is blurred,\nbut we can see a rocky cliff and\nsome green plants.\n1\nthe image is a close - up of a flower in a gar-\nden.\nthe flower1 has a round, green stem\nwith multiple pointed tips that are arranged\nin a circular pattern. the center of the flower\nis covered in small, dark green seeds.\nthe\nseeds2 are densely packed and appear to be\ndensely packed. the background is blurred,\nbut it appears to be a garden with other plants\nand flowers. on the left side of the image,\nthere is a small purple flower3 with a yellow\ncenter.\n1\n(f) Text-phrase-region w/ more detailed text\nFigure 9. Examples of annotations in FLD-5B (continued).\n22\nE. Qualitative Evaluation and Visualization Results\nE.1. Detailed Image Caption\nDetailed Image Caption\nPrompt: Describe with a paragraph what is shown in the image.\nThe image is a hand-drawn illustration of a glass of\ngin and tonic. The glass is filled with ice cubes,\nlime slices, and a slice of lemon. There is a straw\nsticking out of the top of the glass. The background\nis a light green color with a grungy texture. On\nthe right side of the image, there is a list of\ningredients written in black ink. The text reads\n\"Gin & Tonic\" and \"2oz gin, 5oz tonic water, 2-4 ice\ncubes lime slice for garnish\".\nThe image shows the interior of a grocery store\nwith a large variety of fruits and vegetables on\ndisplay. The store has a high ceiling with green\nand white striped awnings, and the floor is made\nof wood. There are rows of wooden crates filled\nwith different types of fruits, including apples,\noranges, lemons, limes, and limes. The crates are\narranged in neat rows, and there are price tags\nattached to each crate. In the background, there are\nshelves stocked with various items such as fruits,\nvegetables, and other grocery items. The overall\natmosphere of the store is bright and colorful, with\na sense of freshness and abundance.\nThe image is a movie poster for the film \"Harry\nPotter and the Sorcerer\u2019s Stone\" starring Ron\nWeasley. The poster features a young boy, Ron\nWeasley, standing in front of a dark background\nwith a castle-like structure in the background. He\nis wearing a school uniform with a Gryffindor crest\non his jacket and a red tie. He has blonde hair and\nis looking directly at the camera with a serious\nexpression on his face. The title of the film is\nwritten in white text at the top of the poster, with\nthe tagline \"20 years of movie magic\" written in\nsmaller text below.\nThe image is a digital illustration of a girl\nhugging a white cat. The girl is wearing a pink\nsweater and has long brown hair. She is sitting on a\ngreen surface with several potted plants and flowers\naround her. The plants have green leaves and pink\nand white flowers. There are also two butterflies\nfluttering around the scene. The background is\nwhite. The overall style of the illustration is\ncartoon-like and playful.\nFigure 10. Detailed captioning prediction results.\n23\nE.2. Visual Grounding\nVisual Grounding\nPrompt: Locate the phrases in the caption: {caption}\nThe image shows a group of five cartoon monsters. On\nthe left side, there is a brown monster1 with horns\nand a big smile on its face. Next to it, there are\ntwo smaller monsters2, one black and one green. The\nblack monster3 has two large horns on its head and\nis standing in the center of the group. The green\nmonster4 on the right side is a green monster with\nbig eyes and a long antennae. It is standing on\nits hind legs with its arms stretched out to the\nsides. In the middle of the image, there appears to\nbe a small blue monster5 with a round head and two\nantennae on its back. The background is light beige\nwith small green circles scattered around.\nThe image shows a cluttered room with a black and\nwhite checkered floor1. On the right side of the\nimage, there is a small white cabinet2 with a televi\nsion3 on top of it. Next to the cabinet, there are\nseveral items4 scattered on the floor, including a\nred blanket5, a wooden stool6, and a pile of trash.\nOn top of the cabinet is a picture frame7 and a hat8.\nIn the center of the room is a white refrigerator9\nwith a few items on top. The walls10 are painted\nwhite and there are a few clothes11 hanging on a\nrack12 on the left wall. The room appears to be in\ndisarray, with some items strewn about and others\nscattered around.\nThe image shows a kitchen countertop with various\nkitchen items on it. On the left side of the\ncountertop, there is a microscope with a black body\nand a white lens1. Next to the microscope, there are\ntwo bottles of condiments2 - one with a red label34\nand the other with green. On top of the microscope\nis a yellow banana5, a blue spatula6, a red plate7,\nand a yellow corn89 on the cob. In the center of\nthe image, there appears to be a frying pan10 with a\nfried egg11 on it, and on the right side is a white\nsink12 with a white faucet13. The countertop14 is made\nof wood and has a gray tile backsplash.\nFigure 11. Visual grounding prediction results.\n24\nVisual Grounding\nPrompt: Locate the phrases in the caption: {caption}\nThe image is a flat lay of various food items\narranged on a white marble countertop. On the left\nside of the image, there is a piece of salmon1. Next\nto it, there are slices of cheese2, a glass of oil3,\ncoffee beans4, a zucchini5, a bunch of strawberries6,\ntwo chicken breasts7, a avocado8 and a few whole\nspinach leaves9. In the center of the table, there\nappears to be a pile of ground beef10 on paper11,\ntwo eggs12, two orange bell peppers13, and some dark\nchocolate bars14. The items are arranged in a way\nthat suggests they are being prepared for a meal.\nThe image shows a modern kitchen with a large window\non the left side. The window1 has a view of trees\nand greenery outside. On the left side of the image,\nthere is a blue sofa2 with a wooden coffee table in\nfront of it. Above the table, there are three copper\npendant lights3 hanging from the ceiling. There is\na large island4 with a white countertop. There are\ntwo bar stools5 next to the table. In the center of\nthe kitchen, there is a bottle green plants6 on the\ntable. The floor7 is made of light-colored wood and\nthe walls8 are painted in a dark blue color.\nThe image shows a man1 standing in a kitchen with\na small dog. The man1 is wearing a plaid shirt2 and\njeans3 and is holding a red cup4 in his hand. The\ndog5 is a light brown color and is standing on a\ntiled floor6. The kitchen7 has wooden cabinets8 and\na countertop9 with various kitchen utensils hanging\non the wall. There is a window10 with yellow cur\ntains11 in the background. On the right side of the\nimage, there is a wooden cutting board12 and a wooden\nstool13.\nFigure 12. Visual grounding prediction results. (continued)\n25\nE.3. Dense Region Caption\nDense Region Caption\nFigure 13. Dense region caption prediction results.\n26\nE.4. Open Vocabulary Detection\nOpen Vocabulary Object Detection\nPrompt: Locate Five Alive juice box\u27e8and\u27e9Col-\ngate toothpaste in the image.\nPrompt: Locate Chewbacca in the image.\nPrompt: Locate giraffe in the image.\nPrompt: Locate Mercedes-Benz\u27e8and\u27e9M2\u27e8and\u27e9Audi in\nthe image.\nPrompt: Locate the objects with category name\nin the image.\nPrompt: Locate the objects with category name in the\nimage.\nFigure 14. Open vocabulary object detection prediction results.\n27\nE.5. OCR\nOcr with region\nPrompt: What is the text in the image, with regions?\nEasy Stroganoff1\nBrown\n1 lb. ground beef in skillet2\nAdd\n1 can beef broth3\n1 can cream of mushroom soup4\nCut in squares & 2dld to above\n5\n1/ Boz pkg. cream cheese6\nSimmer\n20-3 min.7\nServe over hotrice /noodles.8\nVintage. Recipes/Easy-Strogano9\nCharlotte Miller10\nTulsa11\nCOFFEE+TEA1\nBLENDED2\n$1.69/$1.89/$2.093\n$3.49/$3.994\nHot Coffee/Tea5\nTaro6\nIced Coffee/ Tea7\nMango8\nHot Chocolate9\nHoneydew10\n$3,49/$ 3.9911\nStrawberry12\nMocha14\nThai Iced Tea / Coffee13\nCaramel15\n$1,99/$2,29/$2:5916\nSPECIALTY Brew !!17\nJasmine GreenTea18\nLEONARDO1\nDiCAPRIO2\nROBERT3\nDE NIRO4\nLILY5\nGLADSTONE6\nA MARTIN SCORSESE PICTURE7\nKILLERS8\nOF9\nFLOWER10\nMOON11\nSCREENLY ERIC ROTH AND MARTIIN SCORSESE DIRECTED\nBYMARTIN SORSESE12\nONLY IN THEATRES OCTOBER 2013\nFigure 15. OCR with region prediction results.\n28\nE.6. Region to segmentation\nRegion to Segmentation\nPrompt: What is the polygon mask of region\n\u27e8loc 586\u27e9\u27e8loc 294\u27e9\u27e8loc 929\u27e9\u27e8loc 814\u27e9\nPrompt: What is the polygon mask of region\n\u27e8loc 317\u27e9\u27e8loc 314\u27e9\u27e8loc 893\u27e9\u27e8loc 904\u27e9\nPrompt: What is the polygon mask of region\n\u27e8loc 541\u27e9\u27e8loc 266\u27e9\u27e8loc 692\u27e9\u27e8loc 627\u27e9\nPrompt: What is the polygon mask of region\n\u27e8loc 583\u27e9\u27e8loc 66\u27e9\u27e8loc 794\u27e9\u27e8loc 331\u27e9\nPrompt: What is the polygon mask of region\n\u27e8loc 386\u27e9\u27e8loc 53\u27e9\u27e8loc 759\u27e9\u27e8loc 998\u27e9\nPrompt: What is the polygon mask of region\n\u27e8loc 102\u27e9\u27e8loc 7\u27e9\u27e8loc 375\u27e9\u27e8loc 648\u27e9\nFigure 16. Region to segmentation prediction results.\n29\nF. Comparision with LMMs on Detailed Caption Task\nComparison with LMMs on Detailed Image Caption Task\nFlorence-2: The image is a photograph of a handwritten note from a person named Angelica. The note is\nwritten in black ink on a white background. The handwriting is neat and legible, and it appears to\nbe a handwritten letter. The person\u2019s name, Jonathan, is written at the top of the note in a cursive\nfont. Below the name, there is a note that reads, \"I\u2019ve got to admit, London\u2019s lights have nothing on\nour family get together. I miss that, I miss you guys too. Let\u2019s not get too happy about it though I\u2019m\nkeeping my fingers crossed for a trip back stateside. Give a big hug to Amelia for me. Will you?\"\nGPT-4Va: The image shows a handwritten note addressed to Jonathan. The note conveys a personal message\nfrom the sender, Angelica, who admits to missing family gatherings and the feeling of home, contrasting\nit with the lights of London. Angelica expresses a longing to return stateside and asks Jonathan to give\nAmelia a big hug on her behalf. The tone of the note is casual and warm, with a sense of fondness and a\nhint of homesickness. The note is written informally, suggesting a close relationship between the sender\nand the recipients.\nLLaVAb: The image is a handwritten letter on a piece of paper, written by a person named Jonathan. The\nletter is addressed to someone named Angela, and it appears to be a heartfelt message expressing love and\naffection. The letter is written in cursive, giving it a personal and intimate touch.\nMiniGPT-4c: The image is a handwritten letter from an unknown person to someone named Angel. The letter\nis written in black ink on a piece of white paper. The letter is addressed to Angel and begins with\nthe words, \\Dear Angel,\". The letter goes on to describe the writer\u2019s feelings about a recent event or\nsituation, but the details are unclear. The letter ends with the words, \\Sincerely, [Writer\u2019s Name].\"\nahttps://chat.openai.com\nbhttps://llava.hliu.cc\nchttps://huggingface.co/spaces/Vision-CAIR/minigpt4\nFigure 17. Comparison with LMMs on detailed caption task.\n30\nComparison with LMMs on Detailed Image Caption Task\nFlorence-2: The image is a close-up of a red apple with a green label on it. The label has the text \"FUJI\n94131 USA\" and a barcode on it, indicating that the apple is from the United States. The apple appears to\nbe fresh and has a shiny texture. The background is white, making the apple stand out.\nGPT-4Va: The image displays a Fuji apple with a sticker indicating it is organic, along with the\nidentifying number 94131, and specifying that it is from the USA. The apple has a reddish-pink skin\nwith some yellow and green coloration, which is characteristic of Fuji apples. The apple is placed on a\nwhite background, which appears to be a textured fabric.\nLLaVAb: The image features a large, fresh, and ripe red apple sitting on a white surface. The apple has a\nsticker on it, which is likely a barcode or a label providing information about the product. The apple is\nprominently displayed, taking up a significant portion of the image.\nMiniGPT-4c: The image shows a red apple with a label on it that reads \\Organic\" in white letters. The\napple is sitting on a white surface.\nahttps://chat.openai.com\nbhttps://llava.hliu.cc\nchttps://huggingface.co/spaces/Vision-CAIR/minigpt4\nFigure 18. Comparison with LMMs on detailed caption task (continued).\n31\nComparison with LMMs on Detailed Image Caption Task\nFlorence-2: The image shows a highway with multiple lanes. There are several vehicles on the road,\nincluding a black van and a red truck. On the left side of the image, there is a white semi-truck with\nthe word \"Martini\" written on it. Above the highway, there are several bridges with cars and trucks on\nthem. The sky is blue and there are trees and grass on the right side. In the center of the highway is a\ngreen sign that reads \"Exit 94, Chamblee Tucker Rd, Mercer Univ.\"\nGPT-4Va: The image shows a highway scene with several lanes of traffic. There are overpasses with cars\nand trucks traveling over them, and the highway itself appears to be busy with vehicles, including a\nprominent black van in the foreground.\nA road sign indicates that Exit 94 for Chamblee-Tucker Rd and Mercer University is 1 1/4 miles ahead. The\npresence of the sign suggests that drivers in the right lanes will soon have the opportunity to take the\nexit to reach these destinations.\nThe infrastructure suggests a well-traveled interstate or highway system, possibly in an urban or\nsuburban area given the overpass complexity and road signage for local roads and institutions. The\nvehicles on the road are diverse, indicating a mix of commercial and personal transport.\nLLaVAb: The image depicts a busy highway with multiple lanes of traffic, including cars and trucks. There\nis a large truck driving on the highway, and several other trucks are also visible in the scene. The\nhighway is filled with cars, and there is a traffic sign above the lanes, indicating the direction to\nTucker Road. The scene captures the hustle and bustle of a typical day on a highway filled with vehicles.\nMiniGPT-4c: The image shows a highway with multiple lanes of traffic moving in both directions. There\nare several large bridges spanning the highway, and in the distance, there are mountains and a blue sky.\nThere are also several large trucks and cars on the highway, including a black van in the center of the\nimage. The road surface is smooth and well-maintained.\nahttps://chat.openai.com\nbhttps://llava.hliu.cc\nchttps://huggingface.co/spaces/Vision-CAIR/minigpt4\nFigure 19. Comparison with LMMs on detailed caption task (continued).\n32\nComparison with LMMs on Detailed Image Caption Task\nFlorence-2: The image shows a kitchen countertop with a laptop on it. The laptop is open and the screen\nis displaying a blue abstract design. On the countertop, there are various kitchen utensils hanging on a\nmetal rack. The rack is attached to the wall above the sink and has a variety of kitchen tools hanging\non it, including a grater, a spatula, a ladle, a spoon, and a whisk. There are also a few other kitchen\nitems scattered around the counter. The countertop is made of wood and there is a wire rack at the bottom\nof the image.\nGPT-4Va: The image shows a modern kitchen setting where culinary tools are organized neatly on a rack\nagainst the wall. These tools include a variety of utensils such as a grater, tongs, a whisk, ladles,\nand spatulas. On the counter, there is an open laptop with a vibrant blue wallpaper on the screen, which\nsuggests that someone might be using it for a recipe or as entertainment while cooking. In front of the\nlaptop, there appears to be a cutting board with some chopped ingredients and a knife, indicating that\nfood preparation is underway or has recently taken place. There are also some dried chilies and a plastic\ncontainer that might contain other ingredients or spices, further suggesting cooking activities. The\ncounter also has an electrical outlet and some covered sockets, and below the main counter, there are\nstorage bins that likely hold additional kitchen supplies. The environment looks lived-in and practical,\nwith a mix of technology and traditional cooking tools.\nLLaVAb: The image features a wooden kitchen counter with a laptop computer sitting on top of it. The\nlaptop is open and displaying a picture of a cake. The counter is also cluttered with various kitchen\nutensils, including multiple spoons and knives, as well as a bowl. The scene suggests that someone might\nbe using the laptop for cooking or baking purposes, possibly following a recipe or watching a tutorial.\nMiniGPT-4c: The image shows a kitchen counter with a laptop computer on it, a set of cooking utensils\nhanging from a rack above it, and a knife block on the counter. The laptop appears to be open, with a\nblue screen displaying an image of a kitchen. The utensils hanging from the rack include a spatula, a\nladle, a whisk, and a cutting board. The knife block contains several knives, including a chef\u2019s knife, a\nparing knife, and a bread knife.\nahttps://chat.openai.com\nbhttps://llava.hliu.cc\nchttps://huggingface.co/spaces/Vision-CAIR/minigpt4\nFigure 20. Comparison with LMMs on detailed caption task (continued).\n33\nG. Comarison with Kosmos-2 on Detailed Caption and Grounding Tasks\nComparison on detailed caption and grounding tasks.\nThe image features a home office with two white\ndesks, one on the left and the other on the right.\nThe desks are connected by a white metal frame,\ncreating a functional and stylish workspace. A lap\ntop is placed on the desk on the far left, and a\nkeyboard is visible on the other desk. Two chairs\nare placed in front of the desks, one closer to the\nleft desk and the second one on its right side.\n(a) Result from Kosmos-2.\nThe image shows a modern home office setup with\ntwo white desks1 and two black chairs2. The\ndesks3 are arranged in a corner of the room\nwith a large window on the left side. On the\nright side of the desk, there is a computer\nmonitor4, a keyboard5, a mouse, a printer6,\nand a laptop7. Above the computer monitor\nand keyboard, there are two framed pictures8\nhanging on the wall. The walls9 are painted\nin a light green color and the floor10 is made\nof light-colored wood. The floor11 is covered\nwith a beige area rug with a geometric pattern.\nThe overall style of the space is minimal and\ncontemporary.\n(b) Result from Florence-2.\nFigure 21. Systematic comparison with Kosmos-2 [60] on detailed caption and grounding tasks. The models generate both the detailed\ncaption and grounding results. The results of Kosmos-2 are from https://huggingface.co/spaces/ydshieh/Kosmos-2.\n34\nComparison on detailed caption and grounding tasks.\nThe image features a white dining table with\na variety of food items on it. On the table,\nthere is a bowl of bread, a bowl with a salad,\nand a plate with a side of yogurt. In addition\nto the food, there are two cups placed on the\ntable. One cup is located near the left side of\nthe table and the other cup is situated on the\nright side. The table is set with a fork and a\nknife, ready for a meal.\n(c) Result from Kosmos-2.\nThe image shows a table with a basket of bread\nand a plate of salad1. The basket2 is made of\nwoven straw and has several slices of bread\nin it. Next to the basket, there is a small\nbowl1 of salad with a variety of vegetables,\nincluding lettuce3, tomatoes4, cucumbers5, and\nfeta cheese6. There are also two salt7 and\npepper shakers7 on the table. On the right side\nof the table, there are two white plates89 with\na dollop of white sauce on them. The table is\ncovered with a white tablecloth and there are\na few other dishes9 and utensils10 scattered\naround. A person\"s hand11 can be seen in the\ntop right corner of the image.\n(d) Result from Florence-2.\nThe image features a family of elephants walking\ntogether in a lush green forest. The elephants are\nwalking in a line, with one of them walking in front\nof the rest of the family. The family consists of\na mother and her two calves, with the mother and\nthe two calves walking side by side. The forest\nis filled with green trees, adding to the natural\nbeauty of the scene.\n(e) Result from Kosmos-2.\nThe image shows a group of three elephants\nstanding in a dirt field with trees and bushes\nin the background. The elephants1 are standing\nclose together, with the largest elephant in\nthe center and two smaller ones on either side.\nThe largest elephant12 on the left is standing\nwith its trunk3 extended, while the smaller\none4 is standing next to it. All three ele\nphants1 have tusks5 and appear to be in their\nnatural habitat. The ground6 is covered in dirt\nand there is a small pile of dirt7 in front of\nthem. The overall mood of the image is peaceful\nand serene.\n(f) Result from Florence-2.\nFigure 22. Systematic comparison with Kosmos-2 [60] on detailed caption and grounding tasks. The models generate both the detailed\ncaption and grounding results. The results of Kosmos-2 are from https://huggingface.co/spaces/ydshieh/Kosmos-2.\n(continued)\n35\n"
  },
  {
    "title": "Lumos: Learning Agents with Unified Data, Modular Design, and Open-Source LLMs",
    "link": "https://arxiv.org/pdf/2311.05657.pdf",
    "upvote": "24",
    "text": "Agent LUMOS:\nUnified and Modular Training for Open-Source Language Agents\nDa Yin\u2661*\nFaeze Brahman\u2660\nAbhilasha Ravichander\u2660\nKhyathi Chandu\u2660\nKai-Wei Chang\u2661\nYejin Choi\u2660\u2662\nBill Yuchen Lin\u2660\n\u2660Allen Institute for AI\n\u2661UCLA\n\u2662University of Washington\nhttps://allenai.github.io/lumos/\nda.yin@ucla.edu\nyuchenl@allenai.org\nAbstract\nClosed-source agents suffer from several issues\nsuch as a lack of affordability, transparency,\nand reproducibility, particularly on complex in-\nteractive tasks. This motivates the development\nof open-source alternatives. We introduce\nLUMOS, one of the first frameworks for train-\ning open-source LLM-based agents. LUMOS\nfeatures a learnable, unified and modular archi-\ntecture with a planning module that learns high-\nlevel subgoal generation, and a grounding mod-\nule trained to translate these into the actions us-\ning various tools in the execution module. The\ndesign allows for modular upgrades and wider\napplicability to diverse interactive tasks. To\nfoster generalizable agent learning, we collect\nlarge-scale, unified, and high-quality training\nannotations derived from diverse ground-truth\nreasoning rationales across various complex in-\nteractive tasks. On 9 datasets, LUMOS exhibits\nseveral key advantages: (1) LUMOS excels mul-\ntiple larger open-source agents on the held-out\ndatasets (unused for training) for each task type.\nLUMOS even surpasses GPT agents on QA\nand web tasks; (2) LUMOS outperforms open-\nsource agents produced by chain-of-thoughts\nand unmodularized integrated training; and (3)\nLUMOS effectively generalizes to unseen tasks,\noutperforming 33B-scale agents and domain-\nspecific agents. Code and data will be released.\n1\nIntroduction\nLanguage agents execute actions and interact with\nexternal tools or environments, in service of a goal.\nThey have evolved into crucial elements of AI sys-\ntems targeted at solving complex interactive tasks.\nThese tasks often require agents to perform long-\nhorizon planning and interactive reasoning, and\ncan range from QA (Yang et al., 2018; Geva et al.,\n2021), to web tasks (Deng et al., 2023; Zhou et al.,\n* Work was done during Da\u2019s internship at AI2. Code:\nhttps://github.com/allenai/lumos.\nModels & Data:\nhttps://huggingface.co/ai2lumos\nComplex QA \nMath \nWeb Browsing\nText Games\nInteractive Coding\nMultimodal\n\u2026\nVarious Complex Interactive Tasks\nCan we train agents with open data and LLMs?  \nYes, we introduce \ud83e\ude84 Lumos !\nClosed LLMs (GPT-3.5/4) as controller for agents?  \nCons: Expensive, black-box, not reproducible, less controllable, etc.\n\ud83e\ude84Lumos\nAn uni\ufb01ed, modular and \nopen training framework\nBased on LLAMA-2-7B/13B\n\u2705 Affordable\n\u2705 Transparent\n\u2705 Easy to Update\n\u2705 Task-General\n\u2705 Reproducible\nFigure 1: LUMOS is an unified, modular and open-\nsource agent training framework that enables effective\ncross-task generalization while being easy to be updated.\nIt also has advantages against closed-source agents from\naffordability, transparency and reproducibility aspects.\n2023), math (Cobbe et al., 2021), and multimodal\nreasoning (Schwenk et al., 2022; Lu et al., 2022).\nPrior agent frameworks (Yao et al., 2022b; Shinn\net al., 2023; Lin et al., 2023; Lu et al., 2023;\nLiu et al., 2023c) have primarily relied on closed-\nsource large language model (LLM) APIs such\nas GPT-4 and ChatGPT (OpenAI, 2023a, 2022).\nThough powerful, they can be prohibitively expen-\nsive, particularly for tasks with long contexts such\nas web tasks (which include encoding long HTML\ncode). Furthermore, the lack of transparency in\nclosed-source LLMs hinders scientific understand-\ning of their architectures and effectiveness, and\nprovides limited reproducibility, and controllability\nover their behavior. We argue that over reliance on\nclosed-source LLM-based agents is not conducive\nto the growth of research on language agents.\nIn this paper, we propose\nLUMOS, a gener-\nalizable Language agent framework via Unified,\nModular, and Open Source training. LUMOS em-\nploys a unified and modular architecture broadly\napplicable to complex interactive tasks: a planning\nmodule \u0015 , a grounding module - , and an execu-\narXiv:2311.05657v2  [cs.AI]  13 Mar 2024\n\ud83d\udcdd s1: Search \ufb02ights from honolulu to nyc...\n\u269b a1-1: Type([box-id], HNL)  \u2692 \u21d2 Browser\n\u269b a1-2: Type([box-id], JFK)    \u2692 \u21d2 Browser \n\u269b a1-3: Click([button-id])      \u2692 \u21d2 Browser \n\ud83d\udcdd s2: Set a \ufb01lter to keep \u2026 price \u2264 1300\n\u269b \u2026. \u2692\u2026.\n\ud83d\udcdd s3: Set a \ufb01lter to keep premium economy \nLumos-OnePass (Lumos-O)\nLumos-Iterative (Lumos-I) (\ud835\udc61-th iteration)\n Task desc.: \ud835\udc47  \n Prev. subgoals: \n Prev. actions: \n Action interfaces: \ud835\udc3c \n Task desc.: \ud835\udc47  \n Prev. results: \n Prev. subgoals: \n\ud83d\udcdd Planning \n\u269b Grounding \n\u2692 Execution \n Task desc: \ud835\udc47   \n Action interf.: \ud835\udc3c\n Task desc.: \ud835\udc47  \nAll Actions\nExe. results \nAll Subgoals\n\ud83d\udcdd Planning \n\u269b Grounding \n\u2692 Execution \nNext Actions\nExe. result \nNext Subgoal\n        Multimodal Task \n(A-OKVQA): The device \nin her hand is from \nwhich country?\n\ud83d\udcdd s1: Identify the brand of the device \u2026\n\u269b a1: VQA(<img>, What is the brand..?)\n\u2692 e1: LLAVA(...) \u21d2 Nintendo \n        Web Task (Mind2Web): \nFind \ufb02ights from honolulu to NYC \nwith budget of $1,300 for \npremium economy.\n\ud83d\udcdd s2: Answer the country of Nintendo\n\u269b a2: QA(context, What\u2019s the country \u2026)\n\u2692 e2: LLM(...) \u21d2 Japan\nFigure 2: Overall framework of LUMOS. LUMOS are trained with 56K high-quality training annotations. We\npropose two agent training and inference formulations, LUMOS-O (\u00a72.2) and LUMOS-I (\u00a72.3). LUMOS-O is an\nefficient formulation that enables one-pass inference; LUMOS-I is an adaptive formulation that help agents flexibly\nplan based on the execution feedback. We showcase two LUMOS-I running examples in A-OKVQA and Mind2Web.\ntion module { . The planning module learns to\ndecompose diverse complex tasks into a sequence\nof high-level subgoals. The grounding module is\ntrained to communicate with the planning module\nand translate its generated subgoals into the ac-\ntions that can be executed through a collection of\ntools in the execution module. LUMOS design al-\nlows for easy module upgrades to enhance new task\nplanning, novel action grounding and supplement-\ning new tools, without impacting the others. To\ntackle the tasks through the agent modules, we pro-\npose two interaction formulations for implementing\nthe language agents, LUMOS-OnePass (LUMOS-\nO) and LUMOS-Iterative (LUMOS-I). Outlined in\nFig. 2, LUMOS-O is an efficient formulation that\ngenerates all the subgoals and actions through a\nsingle inference call, accomplishing the task in a\none-pass manner. LUMOS-I is an iterative formu-\nlation that generates one subgoal at a time based\non its previous execution results and environment\nupdates, thereby enabling an adaptive agent.\nIn addition, LUMOS utilizes a unified data for-\nmat that encompasses multiple task types, thereby\nenabling the proposed agent framework to conve-\nniently support a range of interactive tasks. These\ninclude, but are not limited to: question answering,\nmathematics, coding, web browsing, multimodal\nreasoning, and text games. To obtain high-quality\nannotations for training LUMOS, we leverage the\nground-truth rationales in existing benchmarks\nacross various task types, and convert them into\na unified format (\u00a73). This conversion is achieved\nwith the aid of strong LLMs, ensuring that the con-\nverted annotations adhere to a universally appli-\ncable format consistent with our modular design.\nOur proposed annotation conversion method re-\nsults in around 56K multi-task multi-domain agent\ntraining annotations, one of the largest open-source\nresources for agent fine-tuning. The training an-\nnotations could serve as a resource for universally\nenhancing any open-source LLMs with agent capa-\nbilities.\nOur evaluation demonstrates that LUMOS pro-\nvides improved or comparable performance with\nGPT-based or larger open-source agents across\nvarious complex interactive tasks that are com-\nmonly used for agent evaluation, including QA,\nweb, math, and multimodal tasks. We summarize\nour contributions and results as follows:\nGeneral Agent Framework with High-Quality\nData.\nWe introduce an open-source agent learn-\ning framework that trains LLMs with unified data,\naimed at unifying complex interactive tasks and\nenhancing generalization on unseen tasks with new\nenvironments and actions. We hope our framework\nand annotations can facilitate future research in\ndeveloping open-source language agents.\nCompetitive\nPerformance.\nLUMOS\noutper-\nforms a great number of open-source agents on\nthe LUMOS held-out datasets unused in LUMOS\ntraining data across the four training task types.\nLUMOS even surpasses GPT-based agents in web\nand QA tasks. Specifically, LUMOS shows a 5.0%\nenhancement over GPT-4 on Mind2Web, and\n4.1% and 3.5% LLM accuracy1 improvement on\nHotpotQA over the ReAct and ReWOO agents\nfully based on GPT-3.5-turbo, respectively.\nCross-Task Generalization.\nWe evaluate LU-\nMOS on two unseen tasks, WebShop (Yao et al.,\n2022a), a text game for online shopping, and\nInterCodeSQL (Yang et al., 2023), an interactive\ncode generation task. LUMOS even surpasses 30B-\nscale agents, especially by nearly 20 reward points\non WebShop. LUMOS also delivers a consistent\nreward improvement over domain-specific agents.\nThis suggests that LUMOS can generalize across\ntasks, hinting at potential benefits for a wide spec-\ntrum of language agent applications.\n1A metric defined in Xu et al. (2023) to identify the seman-\ntic equivalence between predictions and gold answers.\n2\nLUMOS: A Modular Open-Source\nLLM-Based Agent Framework\nWe introduce the overall design and two formula-\ntions for developing agents within this framework.\n2.1\nLUMOS Agent Architecture\nFor various complex interactive tasks, a common\nsolution would include: (1) decomposing the task\ninto a series of subgoals, (2) converting subgoals\ninto concrete actions, (3) executing those actions.\nThis process corresponds to the planning, ground-\ning, and execution modules in our framework.\n\u0015 Planning Module (PM).\nThis module is de-\nsigned to dissect a complex task into a series\nof high-level subgoals, expressed in natural lan-\nguage. For example, a multimodal question such\nas \u201cThe device in her hand is from which\ncountry?\u201d necessitates two subgoals: (1) Identify\nthe brand of the device in her hand; (2) Answer the\ncountry of the device brand. The devised subgoals\nassist in breaking down a complex task into low-\nlevel actions in an interpretable and tool-agnostic\nmanner. The planning module is designed for easy\ndebugging and learning new task planning, without\naffecting other modules.\n- Grounding Module (GM).\nThis module\ntransforms the high-level subgoals produced by\nthe PM into low-level executable actions. For in-\nstance, the GM translates the subgoal, \u201cQuery the\nliving period of Lowell Sherman,\u201d into one\nor more actions, such as KnowledgeQuery(Lowell\nSherman) and QA([R2], Query:\u201cWhat is the\nliving period of Lowell Sherman?\u201d). Here,\nR2 refers to the previously retrieved knowledge that\nmay be helpful in answering the query. The ground-\ning module can be easily customized to learn new\nactions without impacting the planning module.\n{ Execution Module (EM).\nThe Execution\nModule (EM) is a program that implements the\nactions generated by the grounding module and\ngets execution results. It deploys a variety of\noff-the-shelf tools, including APIs, neural models,\nand virtual simulators. For instance, the execution\nmodule could call the Wikipedia or Google Search\nAPIs to accomplish the KnowledgeQuery action.\nThe main characteristic of the LUMOS frame-\nwork is the interaction among the three modules.\nWe propose two formulations promoting the com-\nmunication: LUMOS-OnePass (LUMOS-O) and\nLUMOS-Iterative (LUMOS-I).\n2.2\nLUMOS-OnePass (LUMOS-O)\nThe LUMOS-OnePass (LUMOS-O) formulation is\nan efficient method that generates all subgoals\nand grounded actions at once (efficiency study in\nApp. E). As depicted in Fig. 2, this formulation uses\nthe planning module to generate all n subgoals in a\nsingle inference call. We then pass all the generated\nsubgoals to the grounding module, which translates\nthem into a sequence of m low-level actions. Note\nthat in addition to the task description and subgoals,\nwe also provide action interfaces I to the ground-\ning module as inputs.\nThese action interfaces\n(e.g., \u201cVQA(Image_Context,\nQuery):\nGiven\nthe image context, answer the query.\u201d) de-\nfine the functionalities of actions and their valid ar-\nguments, guiding the grounding module to produce\nexecutable actions. Lastly, for example, the ground-\ning module can produce all the corresponding ac-\ntions, from VQA([IMG], Question: What\u2019s the\nbrand of the device in her hand?) to the final\none QA(..., Question: What\u2019s the country\nof ...?).\nFormally, the overall planning and grounding\nprocess of LUMOS-O is illustrated in Fig. 2. In\nthe planning phase, the task description T is in-\nput into the planning module.\nThis generates\nan output series of subgoals, expressed as S =\n\u03c0plan(T) = {s1, ..., sn}, where \u03c0plan is the param-\neters of trained planning module. Grounded ac-\ntions are obtained via A = \u03c0ground(T, I, S), with\nreliance on the task description, action interfaces\nI = {i1, ..., ik}, and the generated subgoals S.\n\u03c0ground represents the parameters of the grounding\nmodule. We take the last execution result en as the\nfinal inference result for the given task.\n2.3\nLUMOS-Iterative (LUMOS-I)\nLUMOS-Iterative (LUMOS-I) is a formulation that\ngenerates one subgoal and its corresponding exe-\ncutable actions in each iteration. When generating\nthe t-th subgoal, the planning module requires the\nprevious planned subgoals and the execution results\nof their grounded actions as input. The execution\nresults assist the planning module to be aware of\nthe environmental change and decide next subgoal\naccording to the up-to-date environments.\nTake the VQA question \u201cThe device in her\nhand is from which country?\u201d in Fig. 2 as an\nexample. In the first iteration, the planning module\nwill produce \u201cSubgoal 1: Identify the brand\nof the device in her hand\u201d. This subgoal\nis passed to the grounding module to generate the\nquery actions, and obtain the executed results Nin\ntendo. The planning module then takes Nintendo\nalong with the prior planning context as input to\ngenerate the next subgoal \u201cSubgoal 2: Answer\nthe country of Nintendo\u201d. Planning upon the\nlatest execution results would mitigate the risk of in-\ntroducing a non-existent object in the environment\nor a random entity during the reasoning process (a\ncase study in App. E).\nWe demonstrate a single iteration of planning\nand grounding process of LUMOS-I in Fig.\n2.\nTo plan the t-th subgoal, we input the 1) task de-\nscription T, 2) prior subgoals {s1, ..., st\u22121}, and\n3) their executed results {e1, ..., et\u22121} into the\nplanning module.\nWe concatenate them in the\norder of T, s1, e1, ..., st\u22121, et\u22121 where the most\nrecent subgoals and their results are placed in\nthe end, as they have higher influence for plan-\nning t-th subgoal.\nThe output would be the t-\nth subgoal, st = \u03c0plan(T, s1, e1, ..., st\u22121, et\u22121).\nAfter then, the t-th subgoal will be directly in-\ncorporated into grounding module together with\nthe prior grounding history and action interface\nI to generate the corresponding actions, At =\n\u03c0ground(T, I, s1, A1, ..., st\u22121, At\u22121, st). Note that\nAt is an executable action list, as the high-level sub-\ngoal might be decomposed into multiple low-level\nactions. We finally put the low-level actions At\ninto execution module. The final execution result\net can be sent back for planning (t + 1)-th subgoal.\n3\nLearning to Plan & Ground with\nOpen-Source LLMs\nTo guide planning and grounding modules to gener-\nate subgoals and valid low-level actions under our\nspecified action interfaces, we fine-tune the two\nmodules to produce the expected outputs.\nTraining the modules requires the supervisions\nconsisting of high-quality tasks, subgoals, and\nlow-level actions. To equip smaller LLMs with\ninstruction-following ability, prior works leverage\nmethods such as Self-Instruct (Wang et al., 2023b)\nto synthesize training tasks and inputs, and directly\ngenerate ground-truth task outputs based on its cre-\nated tasks. However, these methods are not suitable\nfor generating high-quality annotations for training\nagents. For example, given a web browsing task\nin Mind2Web, GPT-4 only achieves around 20%\nstep success rate (Liu et al., 2023b) when complet-\ning the task. Relying on such methods to generate\ncomplex interactive task annotations may degrade\nthe annotation quality.\nInstead of creating annotations with LLMs from\nscratch, we exploit LLMs as a \u201cstyle transfer\u201d tool\nto convert ground-truth reasoning steps in existing\nbenchmarks into the expected format in LUMOS\nformulations. There are a considerable number of\nthe datasets annotated with either human-written\nsolutions or structured action sequences2. For ex-\nample, PRM800K (Lightman et al., 2023) is a math\ndataset containing the natural language solutions\ninterleaved with formulas; StrategyQA (Geva et al.,\n2021) are a QA dataset with decomposed questions,\nsupporting facts, and relevant Wikipedia paragraph\nindices; Mind2Web includes ground-truth action\nsequences, etc. They provide LLMs with funda-\nmental information that sufficiently contributes to\nthe annotation conversion.\nNext, we introduce 1) how we prompt LLMs\nto obtain the subgoal and action supervisions for\ntraining modules; 2) how to organize the subgoals\nand actions into the conversational forms aligning\nwith LUMOS formulations; 3) how we train the\nmodules with the final annotations.\n3.1\nAnnotation Conversion Prompts\nTo help LLMs better follow the annotation con-\nversion instructions, we add 4-/5-shot examples\nin conversion prompts (see App. I for prompt de-\ntails). We discuss the important elements in these\nin-context examples. The notations of the con-\nverted annotations have hat over letters.\nAction Interfaces.\nAction interfaces define the\navailable actions that LLMs could ground to. Ta-\nble 8 shows a comprehensive list of action defini-\ntions and their implementations.\nGround-Truth Intermediate Reasoning Steps.\nWe provide LLMs with ground-truth intermediate\nreasoning steps in existing benchmarks. With these\nas references, LLMs are able to summarize high-\nlevel subgoals and synthesize corresponding ac-\ntions according to the given action interfaces.\nSubgoals and Corresponding Actions.\nWhen\nconverting ground-truth reasoning steps into our\nexpected annotations, we provide LLMs with ex-\namples about how to distill the high-level subgoals\n2More available resources for future extension and the\ndiscussion about the scalability of our annotation conversion\nmethods are described in App. C.\nfrom the reasoning steps and map them into cor-\nresponding actions. In the in-context examples,\nwe manually decompose a complex task into high-\nlevel subgoals according to the context of ground-\ntruth reasoning steps. Under each high-level sub-\ngoal, we write down multiple corresponding ac-\ntions that help to accomplish the subgoal (shown in\nApp. I). Given the exemplar subgoals and actions\nin the prompt, LLMs would emulate to generate\nsubgoals and their paired actions when performing\nthe conversion for new tasks.\nAs the executed results of prior subgoals\nmight be useful in future action implementa-\ntion, we interlink the grounded actions in the\nin-context examples to allow context-dependent\nexecution.\nOne example of the interlinked ac-\ntions is R1 = KnowledgeQuery(Zombies); R2\n=\nParagraphRetrieve(R1,\nQuery:\nWhat\ncolor skin are zombies typically depicted\nwith?). The agent could first find the zombie\nknowledge page (R1). Written in interlinked style,\nthe ParagraphRetrieve action is able to receive\nthe knowledge about zombies R1 as the context,\nand performs query-based retrieval.\nIntermediate Executed Results of Subgoals.\nThe intermediate executed results \u02c6E play an im-\nportant role in increasing LUMOS\u2019s adaptability\nto environmental changes. Some datasets (e.g.,\nGSM8K) offer execution results in their reasoning\nsteps, i.e., the computation results of formulas. For\nthe datasets without any execution results, their\nreasoning steps actually contain the relevant clues\nfor the execution results. We take an example in\nStrategyQA. Though the answer of the annotated\ndecomposed question \u201cWhat color skin are\nzombies typically depicted with?\u201d is not di-\nrectly provided, the annotation contains a related\nfact \u201cZombies are often depicted as green\nin pallor.\u201d that mentions the answer \u201cgreen\u201d.\nThus, for each in-context example, we concatenate\nthe relevant documents as well as our manually cap-\ntured execution results in the conversion prompts.\nWhen converting new samples into LUMOS an-\nnotations, LLMs would automatically extract the\nexecuted results from the given documents.\nAfter prompting LLMs with the conversion\nprompts, we can acquire the key elements in train-\ning annotations, including subgoals \u02c6S, their corre-\nsponding actions \u02c6A and execution results \u02c6E.\n3.2\nOrganizing Conversational Annotations\nFinally, to build the interaction between planning\nand grounding modules, we organize the annota-\ntions into conversational format.\nConversational Planning Module Annotation.\nAs shown in App. A\u2019s Fig. 3a, we first play a user\nrole to provide the task \u02c6T in the user prompt. For\nLUMOS-O, all the subgoals \u02c6S are the planning\nmodule\u2019s final outputs. LUMOS-I requires multi-\nturn conversational style. From Fig. 3a, the plan-\nning module appends the first ground-truth subgoal\n\u02c6s1 with index \u201cSubgoal 1\u201d as the first response\nsupervision. We then put Subgoal 1\u2019s executed re-\nsult \u02c6e1 with prefix \u201cThe executed result for\nSubgoal 1 is\u201d as the second user input. For the\nremaining turns, we act as the user, provide the ex-\necution results \u02c6et\u22121 of the last subgoal \u02c6st\u22121 to the\nplanning module, and ask if the planning should be\nstopped. The response supervisions cover whether\nthe planning should be terminated; if no, the re-\nsponse should contain a new gold subgoal \u02c6st.\nConversational Grounding Module Annotation.\nAs shown in App. A\u2019s Fig. 3b, we also first provide\nthe task \u02c6T and action interfaces \u02c6I to the grounding\nmodule in the first user turn. For LUMOS-O, we\nfeed all the converted subgoal annotations \u02c6S in the\nuser prompt. All the action annotations \u02c6A would be\nthe user prompt response. For LUMOS-I, we input\nthe current gold subgoal \u02c6st, with prefix \u201cSubgoal\nto be grounded:\u201d. Its response would be \u02c6st\u2019s\ncorresponding actions \u02c6At.\n3.3\nTraining with Converted Annotations\nAs LUMOS annotations are conversational, we\nformulate them as {x1, y1, ..., xi, yi, ..., xn, yn},\nwhere xi is i-th user prompt and yi indicates its\nground-truth responses.\nFollowing Wang et al.\n(2023a), during training, we feed each entire multi-\nturn annotation into a decoder-only model while\nmerely calculating the decoding loss on the tokens\nof ground-truth responses Y = {y1, ..., yi, ..., yn}.\nWe apply binary masking on the user prompt tokens\nto prevent computing loss on them. The final loss\nfunction is L = \u2212 P\nj log p\u03c0(tj | t<j)\u00d71(tj \u2208 Y )\nwhere tj denotes j-th input token and 1(\u00b7) is a\nBoolean indicator function.\n4\nExperiments\nWe begin with the details of our experimental setup,\nincluding annotation conversion, module training,\nand the tools used in the execution module. We\nthen evaluate LUMOS by: 1) comparing LUMOS\nwith existing open-source LLM agents and GPT-\nbased agents, 2) contrasting LUMOS against other\nagent training methods, 3) manifesting LUMOS\ngeneralizability on two unseen tasks involving new\nenvironments and actions, and 4) assessing LUMOS\nannotation quality.\n4.1\nExperimental Setup\nData\nCollection.\nUtilizing\nthe\nconversion\nprompts discussed in \u00a73.1, we employ GPT-4\n(Achiam et al., 2023) versions on 8/13/2023 and\n9/13/2023, and GPT-4V (OpenAI, 2023b) version\non 1/24/2023 to perform annotation conversion\non the ground-truth reasoning steps in existing\nbenchmarks. App. B provides the data sources\nused for annotation conversion.\nThese include\nthe datasets of math, QA, web and multimodal\ntasks.\nTo help LUMOS be aware of the visual\ninputs in multimodal tasks, we append a detailed\nimage caption generated by LLAMA-1.5-7B (Liu\net al., 2023a) to the task description in train and\ntest sets. After filtering out the ones that contain\nmismatched parentheses, invalid execution outputs\nor excessively lengthy outputs, we obtain 55,382\nand 55,499 annotations for training the planning\nand grounding modules, respectively.\nTraining and Action Interfaces.\nWe adopt\nLLAMA-2-7B and LLAMA-2-13B (Touvron et al.,\n2023a) as the base models for both the planning and\ngrounding modules. Details regarding the training\nprocess can be found in App. D. For solving interac-\ntive tasks, we integrate commonly used actions for\neach task into the pre-defined action interfaces. De-\ntails of supported executable actions are included\nin App. G.\n4.2\nTraining Task Performance\nWe evaluate LUMOS across an array of complex\ninteractive tasks - QA, web, math and multimodal\ntasks. The evaluation mainly follows the settings\nestablished by AgentBench (Liu et al., 2023b) and\nReWOO (Xu et al., 2023) (see App. H). For each\ntask type, excluding web tasks, we include a held-\nout dataset to assess the model\u2019s generalizability\nacross the domains within the same task category.\nThe performance is displayed in Tab. 1. Note that in\nTab. 1, task-specific agents LUMOS-IX are trained\nusing task-specific data belonging to task type X\n(e.g., Web, Math, QA, MM). LUMOS-IAll rep-\nresents the agent after unified training with the\ncombination of four task type annotations. More\nevaluation details are shown in App. H.\nLUMOS vs. Open-Source Agents.\nOverall, we\nfind that LUMOS consistently outperforms vari-\nous open-source agents across the seven datasets.\nThough the base models of some compared agents\nare 2-10\u00d7 larger than LUMOS, LUMOS signifi-\ncantly excels in performance.\nSpecifically, 7B\nLUMOS-I achieves 24.5% and 14.1% step suc-\ncess rate improvements over WizardLM-30B and\nAgentLM-70B on Mind2Web.\nThe effectiveness of LUMOS is particularly man-\nifested on the held-out datasets which are unused\nin LUMOS training data.\nOur observations re-\nveal that LUMOS outperforms many baseline open-\nsource agents across all held-out datasets. No-\ntably, even though Orca-Platypus-13B (Lee et al.,\n2023) has been trained on a math corpus that in-\ncludes GSM8K, its performance still 8.6% lower\nthan LUMOS-O on SVAMP (Patel et al., 2021).\nMoreover, despite ReWOO and FiReAct being\nfine-tuned with in-domain HotpotQA annotations,\nLUMOS-I, without any fine-tuning on HotpotQA,\nstill presents an impressive improvement. A similar\ntrend can be observed on ScienceQA. We compare\nAutoAct-7B (Qiao et al., 2024), an open-source\nagent specifically trained on ScienceQA. LUMOS\nachieves 67.3% on the entire test set of ScienceQA,\nwhile AutoAct-7B\u2019s performance is only 53.3%.\nLUMOS vs. GPT-based Agents.\nAlthough LU-\nMOS is built on LLAMA-2-7B/13B, LUMOS-I de-\nlivers superior performance by 5.0-8.7% over GPT-\n4 on Mind2Web. We also notice a 3.5-7.8% in-\ncrease in LLM accuracy over the GPT-based Re-\nWOO on the HotpotQA dataset when employing\nGPT-3.5-turbo as the implementation of the QA\ntool to ensure fair comparisons.\n4.3\nAgent Formulation Comparison\nWe train models using the same base model and\ndata, but with different training methods - Chain-\nof-Thoughts (CoT) Training: For a given task\nT, the agent learns to produce both the chain-of-\nthoughts solution and the final answer directly; In-\ntegrated Agent Training: For a given task T, the\nagent learns to generate all the subgoals and actions\nusing the same model. The execution modules re-\nmains the same. This training paradigm is adopted\nin ReWOO-open, FiReAct and AgentLM.\nAgents\nWeb Task\nMind2Web\nGPT/API-based Agents\nGPT-3.5-turbo\n15.7\nGPT-4\n22.6\nOpen-source Agents\nBaichuan-13B-chat\n2.3\nWizardLM-30B\n3.1\nKoala-13B\n6.0\nAgentLM-70B\n13.5\u22c6\nLUMOS-IWeb\n27.6\u22c6\nLUMOS-IWeb-13B\n31.3\u22c6\n(a) Web performance in step-wise\nsuccess rate.\nAgents\nMath Tasks\nGSM8K\nSVAMP\nOpen-source Agents\nAgentLM-13B\n32.4\n-\nCode-Llama (PoT)-13B\n36.1\n60.0\nPlatypus-30B\n37.8\n51.7\nReWOO-open\n\u224838\n-\nOrca-Platypus-13B\n38.4\u22c6\n56.9\nAlpaca-7B\n\u224839\n-\nGalactica-30B\n41.7\n41.6\nLUMOS-OMath\n50.5\u22c6\n65.5\nLUMOS-IMath\n47.1\u22c6\n63.6\nLUMOS-OMath-13B\n55.4\u22c6\n69.3\n(b) Math performance in accuracy.\nAgents\nMultimodal Tasks\nA-OKVQA\nScienceQA (IMG)\nGPT/API-based Agents\nGPT-3 + GT Caption\n45.4\n-\nOpen-source Agents\nClipCap (VL)\n44.0\u22c6\n-\nKRISP (VL)\n51.9\u22c6\n-\nGPV-2 (VL)\n60.3\u22c6\n-\nMiniGPT-4-13B (VL)\n67.2\n42.8\nLLAVA-1.5-7B (VL)\n-\n57.6\nLUMOS-OMM\n70.1\u22c6\n56.9\nLUMOS-IMM\n71.3\u22c6\n58.4\nLUMOS-IMM-13B\n72.4\u22c6\n58.2\n(c) Multimodal performance in accuracy.\nAgents\nAgent Model\nQA Tool\nQA Tasks\nStrategyQA\nHotpotQA (LLM Acc. / EM)\nGPT/API-based Agents\nGPT-3.5-CoT\nGPT-3.5-turbo\nGPT-3.5-turbo\n56.0\n37.8 / 22.4\nReAcT\nGPT-3.5-turbo\nGPT-3.5-turbo\n64.6\n40.8 / 32.4\nReWOO\nGPT-3.5-turbo\nGPT-3.5-turbo\n66.6\n42.4 / 30.4\nOpen-source Agents\nReWOO-open\nLLAMA-7B\nGPT-3.5-turbo\n\u224856\n\u224837 / -\u22c6\nAgentLM\nLLAMA-2-7B\nLLAMA-2-7B\n-\n- / 22.3\nFiReAct\nLLAMA-2-7B\nLLAMA-2-7B\n-\n- / 26.2\u22c6\nFiReAct\nCodeLLAMA-34B\nCodeLLAMA-34B\n-\n- / 27.8\u22c6\nLUMOS-OQA\nLLAMA-2-7B\nGPT-3.5-turbo\n60.6\u22c6\n39.2 / 24.9\nLUMOS-IQA\nLLAMA-2-7B\nLLAMA-2-7B\n58.3\u22c6\n37.3 / 23.5\nLUMOS-IQA\nLLAMA-2-7B\nGPT-3.5-turbo\n65.7\u22c6\n45.9 / 29.4\nLUMOS-IQA\nLLAMA-2-7B\nGPT-4\n72.4\u22c6\n56.8 / 36.0\nLUMOS-IQA\nLLAMA-2-13B\nGPT-3.5-turbo\n65.3\u22c6\n50.2 / 31.4\nLUMOS-IQA\nLLAMA-2-13B\nGPT-4\n76.7\u22c6\n57.4 / 36.3\n(d) QA performance. The evaluation metric for StrategyQA and HotpotQA is\naccuracy, and LLM accuracy / Exact Match (EM), respectively.\nAgents\nUnseen Tasks\nWebShop\nInterCodeSQL\nBaichuan-13B-chat\n5.7\n-\nKoala-13B\n6.0\n-\nWizardLM-30B\n10.6\n-\nVicuna-v1.1-13B\n12.6\n-\nChatGLM2\n19.4\n-\nVicuna-v1.3-33B\n23.9\n6.7\nVicuna-v1.5-13B\n41.7\n4.8\nOpenChat-v3.2-13B\n46.9\n-\nClaude-instant\n49.7\n-\nLUMOS-IWeb-13B\n46.2\n4.2\nLUMOS-IMath-13B\n45.7\n5.8\nLUMOS-IQA-13B\n47.3\n3.5\nLUMOS-IMM-13B\n43.8\n4.0\nLUMOS-IAll-13B\n50.3\n7.3\n(e) Unseen tasks, WebShop and Inter-\nCodeSQL. The metric is average reward\nand success rate, respectively.\nTable 1: Overall performance of language agents on diverse complex interactive tasks. The tasks highlighted in red\nand blue are the held-in/held-out datasets for the trained task types. \u22c6 indicates that the results are obtained after\nfine-tuning on the task\u2019s training set. We adopt multiple-choice setting for A-OKVQA. IMG denotes the subset with\nimage inputs in ScienceQA test set. GPT-3 in Tab. 1c indicates text-davinci-002.\nFrom Tab. 3, both LUMOS-I and LUMOS-O out-\nperform CoT Training3. They also exceed the in-\ntegrated formulation based on a single module to\noperate planning and grounding. It highlights the\nimportance of disentangling subgoal planning and\naction grounding skills during the agent training.\n4.4\nLUMOS Generalizability Evaluation\nSince LUMOS employs a unified format to repre-\nsent complex interactive tasks, we envision that\nafter trained with the combined annotations across\nthe four training task types, i.e., unified training,\nwhen faced with an unseen task type, LUMOS may\nadapt to it more effectively.\nTo examine the generalization ability of LUMOS,\nwe evaluate it on the unseen tasks, WebShop (Yao\net al., 2022a) and InterCodeSQL.\nWebShop re-\nsembles a text game4, with its shopping environ-\n3We do not implement CoT training on web tasks, as up-\ndates to the environment (e.g., changes to HTML) are neces-\nsary intermediaries for planning subsequent actions.\n4WebShop utilizes four actions in its training annotations:\nSearch, FeatureRetrieve, Pick, and Click. The argument\nof Click is a shopping item, differing from that of Click in\nTraining Data\nQA\nStrategyQA\nHotpotQA\nDownstream Perf. of Training Different Data w/ LLAMA\nReWOO-open Data\n\u224857\n\u224837\nLUMOS-IQA Data\n58.3\n38.1\nPerf. Using High- & Low-Level Subgoal Annots. w/ LLAMA-2\nLUMOS-IQA w/ Low-Level Subgoals\n63.3\n44.3\nLUMOS-IQA Data\n65.7\n45.9\nTable 2: Comparison between the 7B-sized agents\ntrained with different annotations.\nment and action space considerably differing from\nthose covered in the training annotations of LUMOS.\nInterCodeSQL (Yang et al., 2023) is an interactive\ncode generation task that requires the agent to gen-\nerate SQL code based on the external databases\nand involves unseen SQL commands. To make LU-\nMOS adapt to these unseen tasks, we supplement\n2-/3-shot examples in the module inputs, enabling\nthem to learn how to generate subgoals and ground\nto new sets of available actions (see App. J).\nAs outlined in Tab. 1e, LUMOS-I achieves a\n3-7 higher average reward than domain-specific\nMind2Web which includes an HTML element description.\nAgents\nWeb Task\nMind2Web\nIntegrated Training\n25.3\nLUMOS-IWeb\n27.6\n(a) Web task.\nAgents\nMath Tasks\nGSM8K\nSVAMP\nCoT Training\n40.4\n52.2\nIntegrated Training\n45.5\n61.7\nLUMOS-OMath\n50.5\n65.5\nLUMOS-IMath\n47.1\n63.6\n(b) Math tasks.\nAgents\nQA Tasks\nStrategyQA\nHotpotQA\nCoT Training\n58.3\n22.1\nIntegrated Training\n62.3\n39.6\nLUMOS-OQA\n60.6\n39.2\nLUMOS-IQA\n65.7\n45.9\n(c) QA tasks.\nAgents\nMultimodal Tasks\nA-OKVQA\nScienceQA\nCoT Training\n68.3\n57.3\nIntegrated Training\n70.9\n57.6\nLUMOS-OMM\n70.1\n56.9\nLUMOS-IMM\n71.3\n58.4\n(d) Multimodal tasks.\nTable 3: Comparison among different formulations of training language agents. The metric for HotpotQA is LLM\naccuracy (%). All the experiments are based on LLAMA-2-7B.\nagents on WebShop. It also significantly outper-\nforms larger agents such as WizardLM-30B and\nVicuna-v1.3-33B (Chiang et al., 2023). We observe\nthe similar trend on InterCodeSQL. It suggests that\nunified training enables agents to enhance the gen-\neralization on unseen tasks and novel actions. We\nprovide more unified training results in App. F to\nshow unified training also boosts the performance\non most training task types.\n4.5\nFurther Analysis on Training Annotations\nWe aim to address two questions pertinent to qual-\nity and format decisions. Q1: How good are our\nconverted training annotations? Q2: Would adopt-\ning low-level subgoals be more effective than using\nhigh-level subgoals?\nAssessment of Annotation Quality.\nWe assess\nthe quality of our annotations by training models\nwith them and evaluating the agents\u2019 performance.\nWe compare them with ReWOO-open data, con-\nstructed based on HotpotQA and TriviaQA (Joshi\net al., 2017) using the Self-Instruct method. For fair\ncomparison, we train the base model of ReWOO-\nopen, LLAMA-7B, using LUMOS annotations. We\nalso adopt the integrated training formulation and\nsample 2,000 training data to keep the same train-\ning settings as ReWOO-open. Given that ReWOO-\nopen data exclusively relies on QA benchmarks, we\nprimarily focus on QA task evaluation. Shown in\nTab. 2, LUMOS data yields an improvement when\ncompared to ReWOO-open data on StrategyQA\nand HotpotQA. Note that even if the ReWOO-open\ndata are based on HotpotQA, it still underperforms\nLUMOS on HotpotQA.\nLow-Level Subgoal vs.\nHigh-Level Subgoal.\nAs described in \u00a72, we ask LLMs to generate high-\nlevel subgoals corresponding to one or many low-\nlevel actions. An alternative annotation could be\none where each subgoal corresponds solely to one\nlow-level action, i.e., the subgoal is \u201clow-level\u201d.\nWe direct LLMs to create low-level subgoals by\nmodifying the annotation conversion prompt to fit\nthe format where a subgoal is strictly linked to\none action. Tab. 2 reveals a drop after replacing\nhigh-level subgoals with low-level ones on both\nQA datasets. This result hence reaffirms the appro-\npriateness of our initial subgoal design.\n5\nRelated Work\nLLM Agents.\nLanguage agents have shown po-\ntential in solving diverse complex interactive tasks.\nReAct (Yao et al., 2022b) introduced a prompting\nmethod that shaped LLMs as language agents and\ngrounded them in external environments. Subse-\nquently, several methods (Shen et al., 2023; Lu\net al., 2023; Xu et al., 2023; Lin et al., 2023; Liu\net al., 2023c) aimed at improving agent perfor-\nmance and increasing their applicability in diverse\nscenarios. These agents mainly rely on closed-\nsource LLMs, lacking of the consideration of af-\nfordability, reproducibility and transparency issues\non complex interactive tasks.\nImproving Small Models for Building Agents.\nRecent works have utilized larger models to gen-\nerate training data for fine-tuning smaller mod-\nels (Bosselut et al., 2019; West et al., 2022; Wang\net al., 2023b; Hsieh et al., 2023; Brahman et al.,\n2023) to enable them to follow instructions and\nperform chain-of-thoughts reasoning. We also ob-\nserve contemporaneous efforts ReWOO-open (Xu\net al., 2023), FireAct (Chen et al., 2023), AgentLM\n(Zeng et al., 2023), and AutoAct (Qiao et al., 2024),\nfocusing on training agents on smaller LLMs. Un-\nlike FireAct and AutoAct, our work delves into a\nmore in-depth analysis, aiming to discover a unified\ntask representation that enables agents to general-\nize across unseen interactive tasks effectively. In\ncontrast to ReWOO-open and AgentLM, we ex-\ntend to examining proper training formulations and\nstudying multiple strategies for creating large-scale,\nhigh-quality datasets for agent training. We demon-\nstrate LUMOS superior performance in \u00a74.\n6\nConclusion\nWe introduce LUMOS, an open-source, generaliz-\nable language agent training framework. We pro-\npose two formulations, LUMOS-I and LUMOS-O,\nwhich promote collaboration among agent modules\nto solve complex tasks. For module training data,\nwe use LLMs to transform reasoning steps in ex-\nisting benchmarks into a unified format applicable\nwithin LUMOS framework. LUMOS outperforms a\nvariety of open-source agents across the 9 datasets.\nIt performs even better than GPT agents on QA and\nweb tasks. LUMOS also exceeds potential agent\ntraining formulations and exhibits superior gener-\nalization on two unseen interactive tasks.\nLimitations\nCovered Training Task Types.\nCurrently, LU-\nMOS is trained using annotations for four specific\ntypes of complex interactive tasks, which may still\nlimit its generalization capabilities for novel tasks.\nTo address this, we aim to enrich the training data\nfor LUMOS by incorporating a wider variety of task\ntypes. As outlined in \u00a73 and App. C, a substan-\ntial array of benchmarks already exists, providing\nground-truth reasoning steps that could serve as\na foundation for expanding LUMOS\u2019s annotations.\nBy broadening the scope of annotations, we not\nonly enhance the language agents but also offer a\nvaluable resource for practitioners looking to de-\nvelop their own models.\nBacktracking and Replanning Ability.\nIn sit-\nuations where language agents encounter invalid\nexecution outcomes or navigate erroneous solu-\ntion pathways, it is crucial for them to possess\nthe capacity for self-diagnosis and replanning their\nreasoning processing. The current LUMOS lacks\nthese sophisticated self-corrective features. Future\nversions should be designed with advanced mecha-\nnisms that enable the agents to recognize and rec-\ntify their planning errors.\nOpen-Source Tool Replacement.\nFor part of our\nQA experiments, we employ GPT models to ad-\ndress decomposed sub-questions. It is designed\nfor fair comparison with the agents that also use\nGPT models as QA tools, as elaborated in \u00a74.2.\nOur future strategy involves transitioning to fully\nopen-source QA frameworks that leverage mod-\nels such as LLAMA-2-70B, aiming to establish a\ncompletely open-source framework.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for auto-\nmatic knowledge graph construction. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4762\u20134779, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nFaeze Brahman, Chandra Bhagavatula, Valentina Py-\natkin, Jena D Hwang, Xiang Lorraine Li, Hirona J\nArai, Soumya Sanyal, Keisuke Sakaguchi, Xiang\nRen, and Yejin Choi. 2023. Plasma: Making small\nlanguage models better procedural knowledge mod-\nels for (counterfactual) planning.\narXiv preprint\narXiv:2305.19472.\nBaian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier,\nKarthik Narasimhan, and Shunyu Yao. 2023. Fireact:\nToward language agent fine-tuning. arXiv preprint\narXiv:2310.05915.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021.\nEvaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen,\nSamuel Stevens, Boshi Wang, Huan Sun, and Yu Su.\n2023. Mind2web: Towards a generalist agent for the\nweb. arXiv preprint arXiv:2306.06070.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics, 9:346\u2013\n361.\nYu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy\nLiang, Xifeng Yan, and Yu Su. 2021. Beyond iid:\nthree levels of generalization for question answering\non knowledge bases.\nIn Proceedings of the Web\nConference 2021, pages 3477\u20133488.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh,\nHootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay\nKrishna, Chen-Yu Lee, and Tomas Pfister. 2023. Dis-\ntilling step-by-step! outperforming larger language\nmodels with less training data and smaller model\nsizes. In Findings of the Association for Compu-\ntational Linguistics: ACL 2023, pages 8003\u20138017,\nToronto, Canada. Association for Computational Lin-\nguistics.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601\u20131611, Vancouver,\nCanada. Association for Computational Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769\u20136781,\nOnline. Association for Computational Linguistics.\nAriel N. Lee, Cole J. Hunter, Nataniel Ruiz, Bleys\nGoodson, Wing Lian, Guan Wang, Eugene Pent-\nland, Austin Cook, Chanvichet Vong, and \"Teknium\".\n2023.\nOpenorcaplatypus:\nLlama2-13b model\ninstruct-tuned on filtered openorcav1 gpt-4 dataset\nand merged with divergent stem and logic dataset\nmodel.\nhttps://huggingface.co/Open-Orca/\nOpenOrca-Platypus2-13B.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri\nEdwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\n2023.\nLet\u2019s verify step by step.\narXiv preprint\narXiv:2305.20050.\nBill Yuchen Lin, Yicheng Fu, Karina Yang, Prithvi-\nraj Ammanabrolu, Faeze Brahman, Shiyu Huang,\nChandra Bhagavatula, Yejin Choi, and Xiang Ren.\n2023. Swiftsage: A generative agent with fast and\nslow thinking for complex interactive tasks. ArXiv\npreprint, abs/2305.17390.\nXi Victoria Lin, Chenglong Wang, Luke Zettlemoyer,\nand Michael D Ernst. 2018.\nNl2bash:\nA cor-\npus and semantic parser for natural language inter-\nface to the linux operating system. arXiv preprint\narXiv:1802.08979.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee. 2023a. Improved baselines with visual instruc-\ntion tuning. arXiv preprint arXiv:2310.03744.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xu-\nanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,\nKaiwen Men, Kejuan Yang, et al. 2023b. Agent-\nbench: Evaluating llms as agents. arXiv preprint\narXiv:2308.03688.\nZhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue,\nShelby Heinecke, Rithesh Murthy, Yihao Feng,\nZeyuan Chen, Juan Carlos Niebles, Devansh Arpit,\net al. 2023c. Bolaa: Benchmarking and orchestrating\nllm-augmented autonomous agents. arXiv preprint\narXiv:2308.05960.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. 2022. Learn to explain:\nMultimodal reasoning via thought chains for science\nquestion answering. In The 36th Conference on Neu-\nral Information Processing Systems (NeurIPS).\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. arXiv\npreprint arXiv:2304.09842.\nXing Han Lu, Zdenvek Kasner, and Siva Reddy. 2024.\nWeblinx: Real-world website navigation with multi-\nturn dialogue.\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and developing\nEnglish math word problem solvers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 975\u2013984, Online.\nAssociation for Computational Linguistics.\nSwaroop Mishra, Matthew Finlayson, Pan Lu, Leonard\nTang, Sean Welleck, Chitta Baral, Tanmay Rajpuro-\nhit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark,\nand Ashwin Kalyan. 2022. LILA: A Unified Bench-\nmark for Mathematical Reasoning. In Proceedings\nof the 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 5807\u20135832, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nOpenAI. 2022. ChatGPT.\nOpenAI. 2023a.\nGpt-4 technical report.\nArXiv,\nabs/2303.08774.\nOpenAI. 2023b. Gpt-4v(ision) system card.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080\u20132094, Online.\nAssociation for Computational Linguistics.\nShuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo,\nWangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei\nLv, and Huajun Chen. 2024. Autoact: Automatic\nagent learning from scratch via self-planning. arXiv\npreprint arXiv:2401.05268.\nDustin Schwenk, Apoorv Khandelwal, Christopher\nClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.\nA-okvqa: A benchmark for visual question answer-\ning using world knowledge. In European Conference\non Computer Vision, pages 146\u2013162. Springer.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nNoah Shinn, Beck Labash, and Ashwin Gopinath.\n2023.\nReflexion: an autonomous agent with dy-\nnamic memory and self-reflection. arXiv preprint\narXiv:2303.11366.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote,\nYonatan Bisk,\nAdam Trischler,\nand Matthew\nHausknecht. 2020. Alfworld: Aligning text and em-\nbodied environments for interactive learning. In In-\nternational Conference on Learning Representations.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. ArXiv\npreprint, abs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022.\nMuSiQue: Multi-\nhop questions via single-hop question composition.\nTransactions of the Association for Computational\nLinguistics, 10:539\u2013554.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack\nHessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A Smith,\nIz Beltagy, et al. 2023a. How far can camels go?\nexploring the state of instruction tuning on open re-\nsources. arXiv preprint arXiv:2306.04751.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023b. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 13484\u201313508, Toronto, Canada. Association\nfor Computational Linguistics.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.\nCrowdsourcing Multiple Choice Science Questions.\nIn Proceedings of the 3rd Workshop on Noisy User-\ngenerated Text, pages 94\u2013106, Copenhagen, Den-\nmark. Association for Computational Linguistics.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022.\nSymbolic\nknowledge distillation: from general language mod-\nels to commonsense models. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4602\u20134625, Seat-\ntle, United States. Association for Computational\nLinguistics.\nJian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu,\nRenze Lou, Yuandong Tian, Yanghua Xiao, and\nYu Su. 2024. Travelplanner: A benchmark for real-\nworld planning with language agents. arXiv preprint\narXiv:2402.01622.\nBinfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata\nMukherjee, Yuchen Liu, and Dongkuan Xu. 2023.\nRewoo: Decoupling reasoning from observations for\nefficient augmented language models. arXiv preprint\narXiv:2305.18323.\nJohn Yang, Akshara Prabhakar, Karthik Narasimhan,\nand Shunyu Yao. 2023. Intercode: Standardizing\nand benchmarking interactive coding with execution\nfeedback.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369\u20132380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nShunyu Yao, Howard Chen, John Yang, and Karthik\nNarasimhan. 2022a. Webshop: Towards scalable\nreal-world web interaction with grounded language\nagents. Advances in Neural Information Processing\nSystems, 35:20744\u201320757.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao.\n2022b. React: Synergizing reasoning and acting\nin language models. In The Eleventh International\nConference on Learning Representations.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. 2019.\nFrom recognition to cognition: Vi-\nsual commonsense reasoning. In Proceedings of the\nIEEE/CVF conference on computer vision and pat-\ntern recognition, pages 6720\u20136731.\nAohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao\nLiu, Yuxiao Dong, and Jie Tang. 2023. Agenttuning:\nEnabling generalized agent abilities for llms. arXiv\npreprint arXiv:2310.12823.\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,\nRobert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan\nBisk, Daniel Fried, Uri Alon, et al. 2023. Webarena:\nA realistic web environment for building autonomous\nagents. arXiv preprint arXiv:2307.13854.\nAppendix\nA\nIllustration of Annotation Organization\nAs discussed in \u00a73.2, we organize the ground-truth\nsubgoals and actions converted by GPT-4 into the\nconversational format that boosts the interaction\nbetween modules. We show the final conversation\nformat in Fig. 3.\nB\nStatistics of Converted Training\nAnnotations\nAs discussed in \u00a74.1, the data sources for construct-\ning training annotations cover a broad range of\ncomplex interactive tasks. Tab. 4 shows the bench-\nmarks leveraged for annotation conversion, along\nwith the task type information.\nTo train agents like LUMOS-IMath mentioned\nin Tab. 1b, we need to leverage the annotations\nconverted from 19778 data specific to math domain.\nFor training a unified agent such as LUMOS-I, we\nwould use the annotations transformed from all the\nlisted data as training set.\nWe calculate the average turn numbers in each\ntask\u2019s converted training annotations. The average\nnumbers are 4.75, 3.75, 8.25 and 3.92 for Math,\nQA, Web and Multimodal tasks, respectively.\nC\nAvailable Resources for LUMOS\nTraining Data Extension\nAs discussed in \u00a73, we seek the existing datasets\nwith ground-truth intermediate reasoning steps to\nsynthesize LUMOS training annotations from four\ncomplex interactive task categories, math, QA, web\nand multimodal tasks.\nThe methodology for converting annotations as\ndescribed is not limited to the four task types previ-\nously mentioned. Any training datasets that include\ngold reasoning rationales are suitable for the con-\nstruction of annotations using the LUMOS method.\nWe present an exhaustive list of datasets that span\na variety of task types in Tab. 5. For example,\nthe AlfWorld dataset requires actions that have not\nbeen encountered in existing LUMOS annotations,\nsuch as open, take, move, and others; the Trav-\nelPlanner dataset requires actions like CitySearch,\nFlightSearch, which are equally unseen in the\nexisting training set as well. This approach could\nsignificantly enhance the scalability of LUMOS an-\nnotations, thereby augmenting the method\u2019s capa-\nbility to adapt to novel environments and acquire\nproficiency in executing new actions.\nD\nDetails of Training Modules\nWe describe additional details about our training\nexperiments. In all of our experiments, we imple-\nment training over two epochs with a learning rate\nof 2 \u00d7 10\u22125 and a batch size 128. We set the maxi-\nmum sequence length to 1024. We also apply linear\nwarmup for 3% of the total training steps to adjust\nthe learning rate. All the training experiments are\nimplemented with 2 NVIDIA 80GB A100 GPUs\nor 4 NVIDIA 48GB A6000 GPUs.\nE\nEfficiency Study on LUMOS and\nEfficiency-Performance Trade-Off\nWe compute the inference time for LUMOS-O and\nLUMOS-I across 100 instances on GSM8K and\nHotpotQA, respectively. The experiments are run\nwith 2 NVIDIA A6000 48GB GPUs with inference\nbatch size 16. As depicted in Tab. 6, we find that\nLUMOS-O is much more efficient than LUMOS-I\non both datasets.\nLUMOS-O completes its inference in a single\nround, whereas LUMOS-I necessitates multiple\nrounds of inference until it autonomously con-\ncludes its planning. The iterative planning and\ngrounding in LUMOS-I contribute to a higher time\ncost for solving individual instances. However,\nthis property leads to better LUMOS-I\u2019s capacity to\ngenerate appropriate subgoals based on the current\nexternal environment compared to LUMOS-O.\nFor example, when tackling a complex ques-\ntion \u201cWhat government position was held by\nthe woman who portrayed Corliss Archer\nin the film Kiss and Tell?\u201d, LUMOS-I can\nfirst identify the woman who portrayed Corliss\nArcher in Kiss and Tell, Shirley Temple, then ask\nthe government position she held. However, though\nLUMOS-O can first ask who the woman is as well,\nwithout the interaction with external knowledge in\nWikipedia, it will then generate a subgoal which in-\nquires the government position held by Madonna, a\nrandom entity irrelevant with the original question.\nHence, LUMOS-O is a more efficient solution, but\nnot as effective as LUMOS-I due to the lack of the\nadaptability to external environments.\nWe also notice that in Tab. 1b, LUMOS-O\nachieves superior performance on math tasks. Con-\nsider a mathematical problem such as, \u201cJames\ndecides to run 3 sprints 3 times a week.\nHe runs 60 meters each sprint. How many\ntotal meters does he run a week?\u201d Even if\nthe agent calculated how many times James sprints\nTask Types\nDatasets\n# Source Data\n# Total\n# Final Converted for Planning\n# Final Converted for Grounding\nMath\nPRM800K (Lightman et al., 2023)\n10000\n19778\n19386\n19471\nGSM8K (Cobbe et al., 2021)\n7473\nASDiv (Miao et al., 2020)\n2305\nQA\nMusique (Trivedi et al., 2022)\n17632\n19409\n19048\n19080\nStrategyQA (Geva et al., 2021)\n1777\nWeb\nMind2Web (Deng et al., 2023)\n1009\n1009\n1007\n1007\nMultimodal\nA-OKVQA (Schwenk et al., 2022)\n17056\n17056\n15941\n15941\nTable 4: Statistics of data sources for conversion and the number of final successfully converted annotations for\neach task type.\nDatasets\nTask Types\n# Total\nWebLINX (Lu et al., 2024)\nWeb Browsing\n2337\nTravelPlanner (Xie et al., 2024)\nTravel Planning\n225\nSciQ (Welbl et al., 2017)\nQuestion Answering\n11679\nGrailQA (Gu et al., 2021)\nKnowledge Graph Reasoning\n44337\nNL2Bash (Lin et al., 2018)\nInteractive Coding\n8090\nAlfWorld (Shridhar et al., 2020)\nEmbodied Task\n3553\nVCR (Zellers et al., 2019)\nMultimodal Reasoning\n212923\nLILA (Mishra et al., 2022)\nMath\n93670\nTable 5: Statistics of potential data sources for LUMOS\nannotation extension.\nAgents\nGSM8K\nHotpotQA\nLUMOS-O\n102s\n556s\nLUMOS-I\n851s\n1007s\nTable 6: The time cost of LUMOS-O and LUMOS-I\nwhen performing inference 100 instances from GSM8K\nand HotpotQA datasets.\na week, which is 3 \u00d7 3 = 9, the mere number 9\ndoes not affect the next subgoal generation. This is\nbecause no matter the result is 9 or 10000, the next\nhigh-level subgoal to calculate the total meters re-\nmains the same for solving the question. Therefore,\nthe high environment adaptability of LUMOS-I may\nnot be a good supplement on math tasks.\nF\nMore Unified Training Results\nAgents\nWeb\nQA\nMultimodal\nMath\nLUMOS-IX-13B\n31.3\n65.3/50.2/31.4\n72.4/58.2\n51.9/66.3\nLUMOS-IAll-13B\n31.9\n66.7/51.0/31.6\n72.8/58.2\n50.5/65.8\nTable 7: Unified training performance on trained task\ntypes. We aggregate the performance on held-in/held-\nout datasets for each trained task type with the symbol\n\u2018/\u2019. As discussed in the footnote 2 of \u00a74.4, LUMOS-O\nis not applicable to web tasks. Thus, we only conduct\nunified training for LUMOS-I as it can be universally\napplied to any task types.\nWe evaluate the performance of LUMOS-I-13B\nafter the unified training that leverages combined\nannotations from four distinct training task types.\nWe observe that the unified training enhances per-\nformance across a majority of tasks, including web,\nQA, and multimodal tasks. The decline in perfor-\nmance for mathematical tasks is marginal, which\nis only 0.7% and 1.4%. The unified task represen-\ntation may enable LUMOS agents to uncover in-\ntrinsic links among these complex interactive tasks,\nthereby yielding additional advantages for certain\ntraining tasks.\nG\nExecution Tools Associated with Action\nInterfaces\nFor each available action defined in the action inter-\nfaces, there is at least one corresponding backend\nexecution tool to facilitate the implementation of\nthe concrete actions.\nDisplayed in Tab. 8a, for QA tasks, we\nrely on Wikipedia and Google search APIs\nto locate relevant entity knowledge.\nBe-\nsides, we leverage a semantic matching model\ndpr-reader-multiset-base5 used in Dense Pas-\nsage Retrieval (DPR) (Karpukhin et al., 2020) to\ncapture paragraphs according to the query. Fol-\nlowing ReWOO (Xu et al., 2023), we also include\nGPT-series model as a simple QA tool to answer\nthe query based on our retrieved knowledge or pre-\nviously interaction history.\nDemonstrated in Tab. 8b, for web tasks, the ac-\ntions are real mouse and keyboard operations in-\ncluding typing, clicking and selecting HTML tags.\nTo locate the relevant HTML tags to be operated,\nfollowing AgentBench evaluation, we use a De-\nBERTa model6 that ranks and retrieves the tags\naccording to the current action.\nAs shown in Tab. 8c, for math tasks, the main\nexecution tool is WolframAlpha API 7 as it can per-\n5https://huggingface.co/facebook/\ndpr-reader-multiset-base.\n6https://huggingface.co/osunlp/MindAct_\nCandidateGeneration_deberta-v3-base.\n7https://www.wolframalpha.com/.\nSubgoal 1: Query the living period of Lowell Sherman.\n-\nAction 1-1: R1 = KnowledgeQuery(Lowell Sherman)\n-\nAction 1-2: R2 = ParagraphRetrieval(R1, Query: \u2026)\n-\nAction 1-3: R3 = QA([R2], Query: \u2026) = 46 years\nSubgoal 2: Query the living period of Jonathan Kaplan.\n-\nAction 2-1: R4 = KnowledgeQuery(Jonathan Kaplan)\n-\nAction 2-2: R5 = ParagraphRetrieval(R4, Query: \u2026)\n-\nAction 2-3: R6 = QA([R5], Query: \u2026) = 75 years\nSubgoal 3: Compare their living periods.\n-\nAction 3-1: R7 = Calculator(R3 > R6)\n-\n\u2026\n<|user|>\nPlease provide a reasonable subgoal-based plan to solve the \ngiven task. \nTask: Who lives longer, Lowell Sherman or Jonathan Kaplan?\n<|assistant|>\nSubgoal 1: Query the living period of Lowell Sherman. \n<|user|>\nThe execution result of Subgoal 1 is 46 years. Should we \nstop planning?\n<|assistant|>\nNo, I will keep planning. Subgoal 2: Query the living \nperiod of Jonathan Kaplan.\n<|user|>\nThe execution result of Subgoal 2 is 75 years. Should we \nstop planning?\n\u2026\nOrganize subgoals & execution results into conversation format\nFinal planning module annotation\nWe find relevant facts: Lowell Sherman passed away at \nthe age of 46. Jonathan Kaplan passed away at the age \nof 75. We need to answer these questions: \u2026 \u2026 3) Is #2 \ngreater than #1? \u2026 \u2026\nLLM-based annotation conversion\nGround-truth reasoning steps in existing datasets\n(a) Final planning module annotation organized from the converted subgoals & execution results.\nSubgoal 1: Query the living period of Lowell Sherman.\n-\nAction 1-1: R1 = KnowledgeQuery(Lowell Sherman)\n-\nAction 1-2: R2 = ParagraphRetrieval(R1, Query: \u2026)\n-\nAction 1-3: R3 = QA([R2], Query: \u2026) = 46 years\nSubgoal 2: Query the living period of Jonathan Kaplan.\n-\nAction 2-1: R4 = KnowledgeQuery(Jonathan Kaplan)\n-\nAction 2-2: R5 = ParagraphRetrieval(R4, Query: \u2026)\n-\nAction 2-3: R6 = QA([R5], Query: \u2026) = 75 years\nSubgoal 3: Compare their living periods.\n-\nAction 3-1: R7 = Calculator(R3 > R6)\n-\n\u2026\n<|user|>\nPlease ground the given subgoal to corresponding executable \nactions for solving the given task. \n[Action Interfaces]\nTask: Who lives longer, Lowell Sherman or Jonathan Kaplan? \nSubgoal to be grounded: Subgoal 1: Query the living period \nof Lowell Sherman.\n<|assistant|>\nR1 = KnowledgeQuery(Lowell Sherman); R2 = \nParagraphRetrieval(R1, Query: \u2026); R3 = QA([R2], Query: \u2026)\n<|user|>\nSubgoal to be grounded: Subgoal 2: Query the living period \nof Jonathan Kaplan.\n<|assistant|>\nR4 = KnowledgeQuery(Jonathan Kaplan); \u2026\n\u2026\nOrganize subgoals & low-level actions into conversation format\nFinal grounding module annotation\n(b) Final grounding module annotation organized from the converted subgoals & actions.\nFigure 3: Process of converting converted subgoals, actions, and executions into the final conversational training\nannotations for LUMOS-I formulation.\nform a large collection of mathematical functions\nsuch as calculating formulas and solving equations.\nFor complex math operations such as sorting, we\nleverage OpenAI Codex (Chen et al., 2021) to gen-\nerate a short code snippet for the execution.\nFor multimodal tasks, as illustrated in Tab. 8d,\nboth Visual Question Answering (VQA) and Ques-\ntion Answering (QA) tools are considered. The em-\nployed VQA model is LLAVA-1.5-7B (Liu et al.,\n2023a), while the utilized QA model is LLAMA-2-\n13B-chat (Touvron et al., 2023b).\nFor the unseen task WebShop, the actions\ninclude Search, FeatureRetrieve, Pick, and\nClick. The implementation of Search and Click\nrelies on the embedded implementation already\nprovided in official WebShop virtual environ-\nment8. FeatureRetrieve and Pick are based on\ndpr-reader-multiset-base, which helps to se-\nlect the most relevant items and their item features\naccording to the query.\nFor the unseen task InterCodeSQL, the action\ninterfaces include all the possible commands and\nfunctions provided in SQL programming language.\nH\nDetails of Performance Evaluation\nMetrics.\nHere we mainly discuss the special met-\nrics adopted to evaluate the agent performance.\nFor HotpotQA, instead of merely using strict ex-\nact matching, we follow Xu et al. (2023) to also\n8https://github.com/princeton-nlp/WebShop.\nTask Type\nAction Types\nFunction Descriptions\nTools\nQA\nKnowledgeQuery(Entity) -> Knowledge\nQuery the entity knowledge\nWikipedia, Google Search\nParagraphRetrieval(Knowledge, Query)\n-> Paragraphs\nRetrieve relevant paragraphs\naccording to the query\ndpr-reader-multiset-base\nQA(Context, Query) -> Answer\nAnswer the query based on\nthe given context\nGPT-series/open LLMs\nCalculator(Expression) -> Value\nCalculate given math expressions\nWolframAlpha\n(a) Actions used in complex QA tasks.\nTask Type\nAction Types\nFunction Descriptions\nImplementation\nWeb\nClick(Env, Query) -> Tag\nLocate the tag to be clicked according to the query\nHTML Simulator\nType(Env, Query, Text) -> Tag, Text\nLocate the relevant tag according to the query\nand output the typed text\nSelect(Env, Query, Text) -> Tag, Text\nLocate the relevant tag according to the query\nand output the selected option\n(b) Actions used in web tasks.\nTask Type\nAction Types\nFunction Descriptions\nImplementation\nMath\nCalculator(Expression) -> Value\nCalculate given math expressions\nWolframAlpha\nSetEquation(Expression) -> Equation\nSet equations based on given expressions\nSolveEquation(Equation) -> Solutions\nSolve the set equations\nDefine(Variable) -> Variable\nDefine a variable\nSolveInequality(Inequality) -> Solutions\nSolve the given inequality\nCode(Function_Description) -> Code\nGenerate codes for math functions\ngpt-3.5-turbo\nCount(List) -> Number\nCount the element number in a list\nPython\n(c) Actions used in math tasks.\nTask Type\nAction Types\nFunction Descriptions\nImplementation\nMultimodal\nQA(Context, Query) -> Answer\nAnswer the query based on\nthe given context\nLLAMA-2-13B-chat\nVQA(Image_Context, Query) -> Answer\nAnswer the query based on\nthe given image context\nLLAVA-1.5-7B\n(d) Actions used in multimodal tasks.\nTable 8: Action interfaces and execution module implementations for complex interactive tasks.\nuse GPT-4 as an evaluator to judge whether the\npredicted answer shares the same semantics with\nthe gold answer. We call this metric as LLM accu-\nracy, frequently mentioned in \u00a74. For Mind2Web,\nwe adopt the same metric step success rate used\nfor AgentBench evaluation. A step is deemed suc-\ncessful solely when both the chosen HTML tag\nand predicted action type exactly match the gold\naction. For WebShop, we leverage the reward uti-\nlized in both AgentBench and original WebShop\npaper, which quantify the similarity between gold\nand predicted products with regard to the product\ntitles and selected attributes.\nEvaluation Data.\nFollowing Xu et al. (2023), we\nonly evaluate 300 and 1000 randomly selected ex-\namples from StrategyQA and HotpotQA evaluation\nset, respectively. The results reported in Tab. 1d\nare the average performance on three different sets\nof sampled data. Regarding Mind2Web, we only\nevaluate on the \u201ccross-domain\u201d test set that Agent-\nBench utilizes for evaluation. For WebShop, we\nevaluate the first 500 instances from the entire test\nset as AgentBench used to do. We leverage the en-\ntire evaluation set of the other testing benchmarks\nfor assessment.\nI\nIn-Context Examples in Conversion Prompts\nAs discussed in \u00a73.1, in-context examples are helpful to instruct LLMs to generate annotations in our\nexpected format. For each training task types, we showcase one in-context example to help readers better\nunderstand how the prompting conversion method works and the format of our expected annotations. We\nhighlight subgoals, their actions and execution results with yellow , red and blue, respectively.\nI.1\nIn-Context Example For Obtaining Math Task Annotations\nPlease convert natural language plans into a series of subgoals and their corresponding actions that\nlead to the successful implementation with respect to the given instructions. Please use \u2018R[number]\u2019 to\nrepresent the intermediate results for each subgoal, without generating any exact values. Please also\nuse functions to represent the corresponding actions. For the actions, they must be one of \u2018Calculator\u2019,\n\u2018SetEquation\u2019, \u2018SolveEquation\u2019, \u2018SolveInequality\u2019, \u2018Count\u2019, \u2018Code\u2019, and \u2018Define\u2019.\nExample 1:\nTask: Peter goes to the store to buy a soda. The soda costs $.25 an ounch. He brought $2 with him and\nleaves with $.50. How many ounces of soda did he buy?\nNatural language plan:\nHe spend $1.5 on soda because 2 - .5 = 1.5 He bought 6 ounces of soda because 1.5 / .25 = 6\nSubgoal-based plan:\nSubgoal 1: Calculate how much the soda costs in total.\nAction 1-1:\nR1 = Calculator(2 - 0.5)\n= 1.5\nSubgoal 2: Calculate the ounces of soda the price per ounch.\nAction 2-1:\nR2 = Calculator(R1 / 0.25)\n= 6\nI.2\nIn-Context Example For Obtaining Complex QA Task Annotations\nPlease convert natural language plans into a series of subgoals and their corresponding actions that\nlead to the successful implementation with respect to the given instructions. Please use \u2018R[number]\u2019\nto represent the intermediate results for each subgoal, without generating any exact values. Please\nalso use functions to represent the corresponding actions. For the actions, they must be one of one of\n\u2018KnowledgeQuery\u2019, \u2018ParagraphRetrieve\u2019, \u2018QA\u2019, \u2018Calculator\u2019 and \u2018Code\u2019.\nExample 1:\nTask: Are more people today related to Genghis Khan than Julius Caesar?\nNatural language plan:\nWe find relevant facts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern\ngeneticists have determined that out of every 200 men today has DNA that can be traced to Genghis\nKhan. We need to answer these questions: 1. How many kids did Julius Caesar have? (Can be answered\nbased on paragraph \u2018Julius Caesar-75\u2019) 2. How many kids did Genghis Khan have? (Can be answered based\non paragraph \u2018Genghis Khan-17\u2019) 3.\nIs #2 greater than #1?\nBased on these evidences and decomposed\nquestions, the answer is True.\nSubgoal-based plan:\nSubgoal 1: Obtain the number of the kids that Julius Caesar had.\nAction 1-1:\nR1 = KnowledgeQuery(Julius Caesar)\n= WikipediaPage(Julius Caesar)\nAction\n1-2:\nR2 = ParagraphRetrieve(R1, Query: How many kids did Julius Caesar have?)\n=\nParagraph(Julius Caesar-75).\nAction 1-3:\nR3 = QA([R2], Question: How many kids did Julius Caesar have?)\n= 3.\nSubgoal 2: Obtain the number of the kids that Genghis Khan had.\nAction 2-1:\nR4 = KnowledgeQuery(Genghis Khan)\n= WikipediaPage(Genghis Khan).\nAction\n2-2:\nR5 = ParagraphRetrieve(R4, Query: How many kids did Genghis Khan have?)\n=\nParagraph(Genghis Khan-17).\nAction 2-3:\nR6 = QA([R5], Question: How many kids did Genghis Khan have?)\n= 16.\nSubgoal 3: Determine if Genghis Khan had more kids.\nAction 3-1:\nR7 = Calculator(R6 > R3)\n= True\nI.3\nIn-Context Example For Obtaining Web Task Annotations\nSince the data source for converting annotations, Mind2Web, already provides the ground-truth execution\nresults after each action, as discussed in \u00a73.1, we do not ask LLMs to capture each action\u2019s execution\nresults. Therefore, there are no parts highlighted with blue in the in-context example.\nPlease convert natural language plans into a series of subgoals and their corresponding actions that\nlead to the successful implementation with respect to the given instructions. Please use \u2018R[number]\u2019\nto represent the intermediate results for each subgoal, without generating any exact values. Please\nalso use functions to represent the corresponding actions. For the actions, they must be one of they\nmust be one of \u2018TYPE\u2019, \u2018CLICK\u2019, and \u2018SELECT\u2019.\nExample 1:\nTask: Find a Ricky Kej track to listen and share which has been added in the last year and is between\n2 to 10 minutes.\nNatural language plan:\n[searchbox] Search \u2212\u2192 TYPE: Ricky Kej; [link] Search for \u201cRicky Kej\u201d \u2212\u2192 CLICK; [link] Tracks \u2212\u2192\nCLICK; [link] Added any time \u2212\u2192 CLICK; [link] Past year \u2212\u2192 SELECT; [link] Any length \u2212\u2192 CLICK;\n[link] 2-10 min \u2212\u2192 CLICK; [link] To listen to \u2212\u2192 CLICK; [link] To share \u2212\u2192 CLICK\nSubgoal-based plan:\nSubgoal 1: Type Ricky Kej to search his songs.\nAction 1-1:\nR1 = TYPE(Env, QUERY: Type Ricky Kej to search his songs, TEXT: Ricky Kej)\nSubgoal 2: Click on the option to search for Ricky Rej.\nAction 2-1:\nR2 = CLICK(R1, QUERY: Click on the option to search for Ricky Rej)\nSubgoal 3: Choose tracks as the search category.\nAction 3-1:\nR3 = CLICK(R2, QUERY: Choose tracks as the search category)\nSubgoal 4: Find the region to adjust the added time of our interested track.\nAction 4-1:\nR4 = CLICK(R3, QUERY: Find the region to adjust the added time of our interested track)\nSubgoal 5: Choose the last year as the added date.\nAction 5-1:\nR5 = SELECT(R4, QUERY: Choose the last year as the added date, TEXT: Past year)\nSubgoal 6: Find the region to adjust the track length of our interested track.\nAction 6-1: R6 = CLICK(R5, QUERY: Find the region to adjust the track length of our interested track)\nSubgoal 7: Choose 2 to 10 minutes as the track length.\nAction 7-1:\nR7 = CLICK(R6, QUERY: Choose 2 to 10 minutes as the track length)\nSubgoal 8: Listen to our searched track.\nAction 8-1:\nR8 = CLICK(R7, QUERY: Listen to our searched track)\nSubgoal 9: Share our searched track.\nAction 9-1:\nR9 = CLICK(R8, QUERY: Share our searched track)\nI.4\nIn-Context Example For Obtaining Multimodal Task Annotations\nPlease convert natural language plans into a series of subgoals, their corresponding actions that\nlead to the successful implementation with respect to the given instructions.\nWhen generating the\nactions, please also attach the action\u2019s results contained in the natural language plans. Please use\n\u2019R[number]\u2019 to represent the execution results for each action. Please also use functions to represent\nthe corresponding actions. For the actions, they must be one of the available actions, \u2019QA\u2019, \u2019VQA\u2019.\nExample 1:\nTask: If the cameraman were driving what do they have to do from this position? There\u2019re some choices:\nA. turn left, B. drive straight, C. reverse course, D. turn right.\nNatural language plan:\nThe would have to turn right because the lane has right turn arrows painted on it. The arrow on the\nstreet indicates that this lane can only go in one direction at the intersection. The sign on the road\nsays to turn right. Overall, the final answer is \u2019turn right\u2019.\nSubgoal-based plan:\nSubgoal 1: Describe the shape of the sign on the road lane the cameraman is in from the image.\nAction 1-1: R1 = VQA([IMG], Question: What\u2019s the sign on the road lane?) = There\u2019s a right turn arrow\non the road.\nSubgoal 2: Answer which lane the cameraman is in.\nAction 2-1:\nR2 = QA([R1], Question: Which lane is the cameraman in?) = The cameraman is in right turn\nlane.\nSubgoal 3: Answer what the cameraman has to do from his position.\nAction 3-1: R3 = QA([R1, R2], Question: What does the cameraman have to do from his position?) = turn\nright.\nSubgoal 4: Select the choice from the answer choices.\nAction 4-1: R4 = VQA([IMG], Question: If the cameraman were driving what do they have to do from this\nposition? We can also see that There\u2019s a right turn arrow on the road, The cameraman is in right turn\nlane, turn right. There\u2019re some choices: A. turn left, B. drive straight, C. reverse course, D. turn\nright. You must choose one from \u2019A\u2019, \u2019B\u2019, \u2019C\u2019 or \u2019D\u2019. Output only one letter!)\n= D\nJ\nIn-Context Example for Adapting Modules to Unseen Task \u2014 WebShop\nAs discussed in \u00a74.4, the in-context examples are used for helping modules understand how to plan and\nground to novel available actions. Mentioned in App. G, WebShop involves four unseen actions in training\nannotations: Search, FeatureRetrieve, Pick, and Click. The definition of Click is different from the\none defined for web task. In WebShop task, the argument of Click is the item index. However, Click\u2019s\ninput arguments in Mind2Web include an HTML environment description and a specific query regarding\nthe goal of click action. We showcase one of the two in-context WebShop-related examples used for the\ninference of planning and grounding modules, respectively.\nJ.1\nIn-Context WebShop-Related Example for Planning Module\n<|user|>\nPlease provide a reasonable subgoal-based plan to solve the given task.\nTask: Find me slim fit, loose fit women\u2019s tops, tees & blouses with long sleeve, short sleeve with\ncolor: a01#black, and size: large, and price lower than 50.00 dollars\n<|assistant|>\nSubgoal 1: Search two relevant items and the product names according to the instruction.\n<|user|>\nThe execution result for Subgoal 1 is b09s3bn15c - Mens Linen Shirt,Men\u2019s Striped Shirts Casual Short\nSleeve Button Down Shirts Regular Fit Hawaiian Shirts Beach Tees Tops ** b094q7b3ss - Women Cold\nShoulder Tops, Summer Butterfly Print Shirts Fashion Casual Short Sleeve Plus-Size Tunic Top Tee and\nBlouse.\n<|assistant|>\nSubgoal 2: Select the most relevant features of item b09s3bn15c.\n<|user|>\nThe execution result for Subgoal 2 is short, casual, shoulder.\n<|assistant|>\nSubgoal 3: Select the most relevant features of item b094q7b3ss.\n<|user|>\nThe execution result for Subgoal 3 is black, large, x-large.\n<|assistant|>\nSubgoal 4: Pick up the most related one from the two relevant items according to the product names and\ntheir features.\n<|user|>\nThe execution result for Subgoal 4 is b094q7b3ss.\n<|assistant|>\nSubgoal 5: Buy this item by selecting its relevant features.\n<|user|>\nThe execution result for Subgoal 5 is Done.\nJ.2\nIn-Context WebShop-Related Example for Grounding Module\n<|user|>\nPlease ground the given subgoal to corresponding executable actions for solving the given task. The\ngrounded actions must be the one in available action list.\nThe available action list is \u2018Search\u2019, \u2018FeatureRetrieve\u2019, \u2018Pick\u2019 and \u2018Click\u2019. Search(Query): Search\nrelated items according to the Query; FeatureRetrieve(Feature_list, Query): Select the most relevant\nfeatures from Feature_list according to Query; Pick(Item_names, Item_features, Query): Pick the most\nrelevant item from Item_names according to Query, Item_names and Item_features; Click(Item): Click the\nItem to check more information.\nTask: Find me slim fit, loose fit women\u2019s tops, tees & blouses with long sleeve, short sleeve with\ncolor: a01#black, and size: large, and price lower than 50.00 dollars\nSubgoal to be grounded: Subgoal 1: Search two relevant items and the product names according to the\ninstruction.\n<|assistant|>\nR1 = Search(Find me slim fit, loose fit women\u2019s tops, tees & blouses with long sleeve, short sleeve\nwith color: a01#black, and size: large, and price lower than 50.00 dollars)\n<|user|>\nSubgoal to be grounded: Subgoal 2: Select the most relevant features of item b09s3bn15c.\n<|assistant|>\nR2 = Click(b09s3bn15c); R3 = FeatureRetrieve(R2, Query: Find me slim fit, loose fit women\u2019s tops, tees\n& blouses with long sleeve, short sleeve with color: a01#black, and size: large, and price lower than\n50.00 dollars)\n<|user|>\nSubgoal to be grounded: Subgoal 3: Select the most relevant features of item b094q7b3ss.\n<|assistant|>\nR4 = Click(< prev); R5 = Click(b094q7b3ss); R6 = FeatureRetrieve(R5, Query: Find me slim fit, loose\nfit women\u2019s tops, tees & blouses with long sleeve, short sleeve with color: a01#black, and size: large,\nand price lower than 50.00 dollars)\n<|user|>\nSubgoal to be grounded: Subgoal 4: Pick up the most related one from the two relevant items according\nto the product names and their features.\n<|assistant|>\nR7 = Pick(Item_names: R1, Item_features: [R3, R6], Query: Find me slim fit, loose fit women\u2019s tops,\ntees & blouses with long sleeve, short sleeve with color: a01#black, and size: large, and price lower\nthan 50.00 dollars)\n<|user|>\nSubgoal to be grounded: Subgoal 5: Buy this item by selecting its relevant features.\n<|assistant|>\nR8 = Click(< prev); R9 = Click(R7); R10 = Click(black); R11 = Click(large); R12 = Click(x-large); R13\n= Click(buy now)\nK\nIn-Context Example for Adapting Modules to Unseen Task \u2014 InterCodeSQL\nSimilar to \u00a7J, as InterCodeSQL is another unseen task, we showcase one of the three in-context\nInterCodeSQL-related examples used for the inference of planning and grounding modules, respectively.\nK.1\nIn-Context InterCodeSQL-Related Example for Planning Module\n<|user|>\nPlease provide a reasonable subgoal-based plan to solve the given task.\nTask: What are the names of poker players, ordered ascending by the number of final tables they have\nmade?\n<|assistant|>\nSubgoal 1: Find all the related tables.\n<|user|>\nThe execution result for Subgoal 1 is Results: Table players has Name (type: TEXT), People_ID (type:\nINT); Table poker_player has Final_Table_Made (type: INT), People_ID (type: INT).; Subgoal 2: Write\nthe selection objects: the names of the people who are also poker players.\n<|assistant|>\nSubgoal 2: Write the selection objects: the names of the people who are also poker players.\n<|user|>\nThe execution result for Subgoal 2 is T1.Name FROM people AS T1 JOIN poker_player AS T2 ON T1.People_ID\n= T2.People_ID.\n<|assistant|>\nSubgoal 3: Write the selection condition: sort all the poker players by the number of final tables they\nhave made in ascending order.\n<|user|>\nThe execution result for Subgoal 3 is ORDER BY T2.Final_Table_Made.\n<|assistant|>\nSubgoal 4: Finish the SQL query to the names of poker players, ordered ascending by the number of final\ntables they have made.\n<|user|>\nThe execution result for Subgoal 4 is SELECT T1.Name FROM people AS T1 JOIN poker_player AS T2 ON\nT1.People_ID = T2.People_ID ORDER BY T2.Final_Table_Made.\nK.2\nIn-Context InterCodeSQL-Related Example for Grounding Module\n<|user|>\nPlease ground the given subgoal to corresponding executable actions for solving the given task. The\ngrounded actions must be the one in available action list.\nTask: What are the names of poker players, ordered ascending by the number of final tables they have\nmade?\nSubgoal to be grounded: Subgoal 1: Find all the related tables.\n<|assistant|>\nR1 = SHOW TABLES\n<|user|>\nSubgoal to be grounded: Subgoal 2: Write the selection objects: the names of the people who are also\npoker players.\n<|assistant|>\nR2 = T1.Name FROM people AS T1 JOIN poker_player AS T2 ON T1.People_ID = T2.People_ID\n<|user|>\nSubgoal to be grounded: Subgoal 3: Write the selection condition: sort all the poker players by the\nnumber of final tables they have made in ascending order.\n<|assistant|>\nR3 = ORDER BY T2.Final_Table_Made\n<|user|>\nSubgoal to be grounded: Subgoal 4: Finish the SQL query to the names of poker players, ordered ascending\nby the number of final tables they have made.\n<|assistant|>\nR4 = SELECT T1.Name FROM people AS T1 JOIN poker_player AS T2 ON T1.People_ID = T2.People_ID ORDER BY\nT2.Final_Table_Made\nL\nMultimodal Task Case Study\nWe provide two cases in the multimodal datasets, A-OKVQA and ScienceQA, to show the effectiveness of\nLUMOS. For the A-OKVQA case, LUMOS first identifies the device brand with the aid of VQA tool. After\nLUMOS planning module knows that the device is a Nintendo Wii controller, the module would generate\nthe next subgoal \u201cAnswer the country of origin for the Nintendo Wii controller\u201d. Finally,\nLUMOS selects the closest option from the given answer choices. For the ScienceQA case, LUMOS\nidentifies the highlighted area on the map, which includes Middle East area. Then the planning and\ngrounding modules would leverage QA tool to answer which continent Middle East is located at, and then\npick up the correct answer \u201cAsia\u201d.\nQuestion: The company producing the device in her hand is from what \ncountry? There\u2019re some choices: A. \u2026 \u2026 C. Japan, D. \u2026 The image shows the \nfollowing contents: The image features a young woman sitting on a couch, \u2026 \u2026 \nplaying a video game, possibly on a Nintendo Wii console. The couch she is sitting \non is \u2026 \u2026\nLumos-IMM Planning and Grounding Results:\nSubgoal 1: Identify the brand of the device in her hand.\nAction: R1 = VQA([IMG], Question: What brand is the device in her hand?) = The device in her \nhand is a Nintendo Wii controller.\nSubgoal 2: Answer the country of origin for the Nintendo Wii controller.\nAction: R2 = QA([R1], Question: What is the country of origin for the Nintendo Wii controller?) = \nThe country of origin for the Nintendo Wii controller is Japan.\nSubgoal 3: Select the choice from the four answer choices.\nAction: R3 = VQA([IMG], Question: The company producing the device in her hand is from what \ncountry? The image features \u2026 We can also see that The device in her hand is a Nintendo Wii \ncontroller, The country of origin for the Nintendo Wii controller is Japan. There\u2019re some choices: \nA. \u2026 \u2026 C. Japan, D. \u2026 You must choose one from \u2018A\u2019, \u2018B\u2019, \u2018C\u2019 or \u2018D\u2019. Output only one letter!) = C.\nPlanning module output\nGrounding module output\nExecution module output\nFigure 4: LUMOS-IMM case study on A-OKVQA.\nQuestion: Which continent is highlighted? There\u2019re some choices: \u2026 \u2026 D. \nAsia. The image features a large, colorful map of the world, with various \ncountries and continents depicted. The map is \ufb01lled with green and blue colors, \ngiving it a vibrant and lively appearance. \u2026 \u2026\nLumos-IMM Planning and Grounding Results:\nSubgoal 1: Identify the highlighted area on the map.\nAction: R1 = VQA([IMG], Question: Which area is highlighted on the map?) = The area \nhighlighted on the map includes the Middle East, \u2026 .\nSubgoal 2: Answer which continent the highlighted area is in.\nAction: R2 = QA([R1], Question: Which continent is the Middle East in?) = The Middle East is \nlocated in the continent of Asia.\nSubgoal 3: Select the choice from the answer choices.\nAction: R3 = VQA([IMG], Question: Which continent is highlighted? The image features \u2026 We can \nalso see that The area highlighted on the map includes the Middle East, \u2026, The Middle East is \nlocated in the continent of Asia. There\u2019re some choices: \u2026 \u2026 D. Asia. You must choose one from \n\u2018A\u2019, \u2018B\u2019, \u2018C\u2019 or \u2018D\u2019. Output only one letter!) = D.\nPlanning module output\nGrounding module output\nExecution module output\nFigure 5: LUMOS-IMM case study on ScienceQA.\n"
  },
  {
    "title": "Prompt Engineering a Prompt Engineer",
    "link": "https://arxiv.org/pdf/2311.05661.pdf",
    "upvote": "18",
    "text": "Prompt Engineering a Prompt Engineer\nQinyuan Ye1\u2020 Maxamed Axmed2 Reid Pryzant2 Fereshte Khani2\n1University of Southern California\n2Microsoft\nqinyuany@usc.edu fkhani@microsoft.com\nAbstract\nPrompt engineering is a challenging yet cru-\ncial task for optimizing the performance of\nlarge language models on customized tasks.\nIt requires complex reasoning to examine the\nmodel\u2019s errors, hypothesize what is missing\nor misleading in the current prompt, and com-\nmunicate the task with clarity. While recent\nworks indicate that large language models can\nbe meta-prompted to perform automatic prompt\nengineering, we argue that their potential is\nlimited due to insufficient guidance for com-\nplex reasoning in the meta-prompt. We fill\nthis gap by infusing into the meta-prompt three\nkey components: detailed descriptions, context\nspecification, and a step-by-step reasoning tem-\nplate. The resulting method, named PE2, show-\ncases remarkable versatility across diverse lan-\nguage tasks. It finds prompts that outperform\n\u201clet\u2019s think step by step\u201d by 6.3% on MultiArith\nand 3.1% on GSM8K, and outperforms compet-\nitive baselines on counterfactual tasks by 6.9%.\nFurther, we show that PE2 can make targeted\nprompt edits, rectify erroneous prompts, and\ninduce multi-step plans for complex tasks.\n1\nIntroduction\nLarge language models (LLMs) are powerful tools\nfor many natural language processing tasks when\ngiven the right prompts.1 However, due to the\nmodel\u2019s sensitivity (Jiang et al., 2020; Zhao et al.,\n2021; Reynolds and McDonell, 2021; Lu et al.,\n2022), finding the optimal prompt can be challeng-\ning, often necessitating extensive manual trial-and-\nerror efforts. Moreover, once an initial prompt is\ndeployed into production, unforeseeable edge cases\nmay emerge, demanding more rounds of manual\nefforts to further refine the prompt. These chal-\nlenges give rise to an emerging research field of\n\u2020Work done while interning at Microsoft.\n1In this paper, we limit the scope to textual prompts of task\ndescription (e.g., \u201cTranslate English to French\u201d) or instruction\n(e.g., \u201cLet\u2019s think step by step\u201d).\nInspect a prompt and a batch of failure examples \nwhen this prompt is used. Then provide feedback.\nPrompt: Let\u2019s think step by step.\nExample 1: George had 28 socks \u2026\nExample 2: Judy teaches 5 dance classes \u2026\nThe prompt should be edited to guide the model to \nperform subtraction.\nPropose a new prompt.\nLet\u2019s solve this problem step by step. Remember to add \nor subtract as needed. \nFigure 1: LLM-powered automatic prompt engineering\nmethods typically use a meta-prompt that guides an\nLLM to inspect the current prompt, provide feedback\n(sometimes refered to as textual \u201cgradients\u201d) and then\ngenerate an updated prompt. In this paper, we design\nand investigate meta-prompt variants to guide LLMs to\nperform automatic prompt engineering more effectively.\nautomatic prompt engineering. Within this field, a\nnotable line of methods involves leveraging the ca-\npabilities of LLMs themselves (Zhou et al., 2023b;\nPryzant et al., 2023). This entails meta-prompting\nLLMs with instructions such as \u201cinspect the current\nprompt and a batch of examples, provide feedback,\nthen propose a new prompt.\u201d (See Figure 1)\nWhile these methods achieve impressive perfor-\nmance, a subsequent question arises: What makes\na good meta-prompt for automatic prompt engi-\nneering? To answer this question, we connect two\nkey observations: (1) Prompt engineering, at its\ncore, is a natural language generation task that re-\nquires complex reasoning: it involves closely ex-\namining the model\u2019s errors, hypothesizing what is\nmissing or misleading in the current prompt, and\ncommunicating the task more clearly to the LLM.\n(2) Complex reasoning capabilities in LLMs can\nbe elicited by prompting the model to \u201cthink step\nby step\u201d (Wei et al., 2022a; Kojima et al., 2022)\nand can be further improved by instructing them to\n1\narXiv:2311.05661v2  [cs.CL]  19 Feb 2024\nMultiArith\nGSM8K\n40\n60\n80\n100\nAccuracy\n+6.3\n+3.1\nInstruction Induction\n(14 Tasks)\nBIG-bench Hard\n(27 Tasks)\nCounterfactual Eval\n(12 Tasks)\n25\n50\n75\n100\n(Average) Accuracy\n+7.4\n+5.9\n+10.6\nInitialization\nIterative APE\nAPO\nPE2 (This Work)\nProduction\nPrompt\nF1 Score\n+8.0\nFigure 2: Results Overview. Our method PE2 consistently brings improvements over the prompt initialization\n(marked with orange text). It matches or outperforms prompt optimization baselines Iterative APE (Zhou et al.,\n2023b) and APO (Pryzant et al., 2023) across a wide range of language tasks, with most significant performance\ngain observed on Conterfactual Eval (Wu et al., 2023). See detailed performance breakdown in Fig. 10-13.\nreflect on their outputs (Madaan et al., 2023; Chen\net al., 2023).\nBridging these two observations, in this work,\nwe prompt engineer a prompt engineer\u2014we aim to\nconstruct a meta-prompt that guide LLMs to per-\nform prompt engineering more effectively. We ar-\ngue that prior works do not provide sufficient guid-\nance in the meta-prompt, thereby limiting the poten-\ntial of LLMs for automatic prompt engineering. To\naddress this, we introduce new meta-prompt com-\nponents such as detailed two-step task descriptions,\ncontext specification, and a step-by-step reason-\ning template, to better equip LLMs throughout the\nprocess (\u00a73; Fig. 3).\nThe resulting method, named PE2, achieves\nstrong empirical performance (\u00a75.1). When using\ntext-davinci-003 as the task model, the prompts\nproduced by PE2 surpass the zero-shot chain-of-\nthought prompt \u201clet\u2019s think step by step\" (Kojima\net al., 2022) by 6.3% on MultiArith and 3.1% on\nGSM8K. Moreover, PE2 matches or outperforms\ntwo automatic prompt engineering baselines, Iter-\native APE (Zhou et al., 2023b) and APO (Pryzant\net al., 2023) in multiple settings (Fig. 2). PE2 is\nmost effective on counterfactual tasks (Wu et al.,\n2023), where the automatic prompt engineer is an-\nticipated to reason about non-standard situations\n(e.g., do addition in base-8 instead of base-10) and\nexplain such situation to the task model through\nthe prompt. Beyond academic datasets, we show\nthat PE2 can improve an expert-written production\nprompt consisting of over 5,000 tokens, resulting\nin an 8.0% increase in F1 score.\nUpon examining the prompt edit history of PE2\n(\u00a75.3), we find that PE2 consistently offers mean-\ningful prompt edits (Table 4). It is able to amend\nerroneous or incomplete prompts and enrich the\nprompts with additional details, which leads to im-\nproved final performance. It is also able to devise\nmulti-step plans for complex tasks. For example,\nin the task of movie recommendation, PE2 makes\nthe plan to \u201cconsider factors such as genre, plot\nand style\u201d in the prompt. Interestingly, when unin-\nformed about performing addition in base-8, PE2\nformulates its own arithmetic rules from the exam-\nples: \u201cIf both numbers are less than 50, add 2 to\nthe sum. If either number is 50 or greater, add 22 to\nthe sum.\u201d This demonstrates PE2\u2019s non-trivial abil-\nity to reason and adapt in counterfactual situations,\nwhile also raises concerns of \u201cshortcut learning\u201d in\nprompt optimization.\n2\nBackground\nIn this section, we provide a formal formulation\nof the prompt engineering problem (\u00a72.1), and de-\nscribe a general framework of automatic prompt\nengineering using LLMs and meta-prompts (\u00a72.2).\nBuilding on this foundation, we introduce new\nmeta-prompt components used in PE2 in \u00a73.\n2.1\nPrompt Engineering\nThe goal of prompt engineering is to find the textual\nprompt p\u2217 that achieves the best performance on a\ngiven dataset D when using a given LLM Mtask\nas the task model. More specifically, we assume all\ndatasets can be formatted as textual input-output\npairs, i.e., D = {(x, y)}. We are given a training\nset Dtrain for optimizing the prompt, Ddev for val-\nidation, and Dtest for final evaluation. Following\nthe notations in Zhou et al. (2023b), the prompt\nengineering problem can be described as:\np\u2217 = arg max\np\nX\n(x,y)\u2208Ddev\nf(Mtask(x; p), y) (1)\nwhere Mtask(x; p) is the output generated by the\nmodel when conditioning on the prompt p, and\nf is a per-example evaluation function. For ex-\nample, if the evaluation metric is exact match,\nf(Mtask(x; p), y) = 1[Mtask(x; p) = y].\n2\n2.2\nAutomatic Prompt Engineering with\nLLMs\nTo alleviate the intensive efforts of human prompt\nengineering, recent works explore automating this\nprocess by meta-prompting LLMs to paraphrase\nthe prompt (Zhou et al., 2023b) or refine the prompt\nby inspecting failure examples (Pryzant et al.,\n2023). In the following, we describe a framework\nthat encapsulates these prior works and is employed\nin our investigation in later sections. It has three\nparts: prompt initialization, new prompt proposal,\nand the search procedure.\nPrompt Initialization.\nTo start the prompt en-\ngineering process, a set of initial prompts P (0) is\nneeded. We consider two initialization methods:\n(1) Manual initialization is applicable for tasks\nthat has pre-existing prompts written by humans\nexperts. For example, \u201cLet\u2019s think step by step\u201d is\neffective on mathematical reasoning tasks and can\nbe used as the initialization for prompt optimiza-\ntion. In (2) Induction Initialization, we follow\nZhou et al. (2023b) by using a batch of examples\n{(x, y)} from Dtrain and a prompt pinit (\u201cHere are\nthe input-output pairs. What is the instruction?\u201d)\nto generate a set of initial prompts P (0).\nNew Prompt Proposal.\nGiven a set of ini-\ntial prompts, the automatic prompt engineer will\ncontinuously propose new and potentially better\nprompts. At timestamp t, the prompt engineer is\ngiven a prompt p(t) and expected to write a new\nprompt p(t+1). Optionally, a batch of examples\nB = {(x, y, y\u2032)} may be inspected in the new\nprompt proposal process. Here y\u2032 = Mtask(x; p)\nrepresents model-generated output and y represents\nthe ground-truth label. We use pmeta to denote\na meta-prompt that is used to instruct the LLM\nMproposal to propose new prompts. Therefore,\np(t+1) = Mproposal(p(t), B; pmeta)\n(2)\nConstructing a better meta-prompt pmeta to im-\nprove the quality of the proposed prompt p(t+1) is\nthe main focus of this study. We will describe in\nmore details in \u00a73.\nSearch Procedure.\nAs LLMs are sensitive to triv-\nial prompt variations, it is possible that the newly\nproposed prompt p(t+1) under-performs the origi-\nnal prompt p(t). Therefore, automatic prompt engi-\nneering is typically combined with a back-tracking\nenabled search procedure. At timestamp t, we se-\nlect n best-performing prompts from all prompt\ncandidates obtained in previous timestamps (i.e.,\nP (0)\u222aP (1)\u222a...\u222aP (t)). For each of these n prompts,\nwe sample m different batches B of model errors,\nand run the meta-prompt in Eq. 2 to produce m new\nprompts. This results in m\u00d7n new prompts, which\nwe denote as P (t+1) collectively and are used at\nthe next timestamp t + 1. The search algorithm is\ndescribed more formally in Algorithm 1.\n3\nPrompt Engineering a Prompt Engineer\nMuch like how the prompt plays an important role\nfor the end task performance, the meta-prompt\npmeta introduced in Eq. 2 plays an important role\nin the quality of newly proposed prompts, and\nthe overall quality of automatic prompt engineer-\ning. In this work, we focus on prompt engineer-\ning the meta-prompt pmeta\u2014we develop meta-\nprompt components that can potentially help im-\nprove LLMs\u2019 prompt engineering quality.\nIn the following, we reflect on the limitations of\nprior works and subsequently introduce three meta-\nprompt components targeting these limitations. We\nvisualize these components in Fig. 3. We name our\nmethod using these three components as PE2, a\nprompt engineered prompt engineer.\n(a)\nTwo-step\nTask\nDescription.\nIn\nAPO\n(Pryzant et al., 2023) the task of prompt engineer-\ning is decomposed into two steps (Fig. 1): In step 1,\nthe model is expected to inspect the current prompt\nand a batch. In step 2, the model is expected to\ngenerate an improved prompt. However, in APO,\neach step is explained briefly and on the fly. In\ncontrast, we consider clarifying the steps and\nexpectations upfront in the meta-prompt.\n(b) Context Specification.\nIn practice, how the\nprompt and the input text are formatted together is\nflexible. It may appear before the input text to de-\nscribe the task, e.g., \u201cTranslate English to French.\u201d\nIt may appear after the input text, e.g., \u201cLet\u2019s think\nstep by step\u201d to elicit reasoning capabilities. Recog-\nnizing these varying contexts, we explicitly specify\nthe layout of the prompt and the input.\n(c) Step-by-step Reasoning Template.\nTo en-\ncourage the model to examine each example in the\nbatch B closely and reflect on the limitations in\nthe current prompt, we guide the prompt proposal\nmodel Mproposal with a list of questions. For ex-\nample: Is the prompt correctly describing the task?\nIs it necessary to edit the prompt? If yes, provide\nactionable suggestions on prompt editing.\n3\nA prompt is a text paragraph that outlines the expected actions \nand instructs the model. In our collaboration, we'll work \ntogether to refine a prompt. The process consists of two steps:\n# Step 1\nExamine the prompt and a batch of examples\n# Step 2\nPropose a new prompt based on your reasoning\n(a) two-step task instruction\nSure! I'd be happy to help you.\n# Current Prompt\nLet\u2019s think step by step.\n# Full Template\n```\nQuestion: <input>\nAnswer: Let\u2019s think step by step.\n```\n# Examples\n## Example 1\nInput: George had 28 socks. If he threw away 4 socks \u2026\nOutput: 64\nReasoning: Step 1: George had 28 socks. Step 2: \u2026\nLabel: 60\n[More examples \u2026]\n(b) context specification\n(c) step-by-step reasoning template\n## Example 1\nOutput is correct? No.\nReasoning: the model didn't subtract the socks he threw away.\nPrompt describing the task correctly? Yes.\nNecessary to edit the prompt? Yes.\nSuggestions: The prompt should be edited to guide the model \nto perform subtraction.\n[More examples \u2026]\nNow carefully review your reasoning and proceed with step \n2: refine the prompt.\n# Current Prompt\nLet\u2019s think step by step.\n# Instructions\n* The total length should be less than 50 words\n* Reply with the prompt. Do not include other text.\nLet\u2019s solve this problem step by step. Remember to add or \nsubtract as needed. \n# Instruction\nFor each example, provide reasoning according to the \nfollowing template\n* Output is correct? \n* Necessary to edit the prompt?\n* If yes, suggestions on prompt editing?\nMeta-Prompt Components\nPrompt\nFeedback (\u201cGradients\u201d)\nLegend\nFigure 3: An redacted example to illustrate the meta-prompt components introduced in \u00a73.\nOther Meta-prompt Components We Tried.\nInspired by optimization concepts such as batch\nsize, step size and momentum, we considered\nadding their verbalized counterparts to the meta-\nprompt and investigate their effects. We also con-\nsidered adding a prompt engineering tutorial in the\nmeta-prompt to help the LLM better understand\nthe task. Our observations on these components\nare mixed. We report these results in Appendix B.\n4\nExperiment Setting\n4.1\nTasks\nWe use the following five groups of tasks to\nevaluate the effectiveness of PE2. More details\n(e.g., dataset sizes, license) are deferred in Ap-\npendix D.3.\n(1) Math Reasoning.\nWe use MultiArith (Roy\nand Roth, 2015) and GSM8K (Cobbe et al., 2021),\nwhich contain grade school math problems that\nrequires multiple steps of arithmetic operations.\n(2) Instruction Induction.\nInstruction Induction\n(Honovich et al., 2023) is a benchmark for inferring\nthe underlying instruction from few-shot examples.\nWe use 14 selected tasks that cover a wide range\nof use cases, e.g., \u201cFormality\u201d is a task that aims at\nrephrasing a sentence to be more formal.\n(3) BIG-bench Hard.\nBIG-bench Hard (Suzgun\net al., 2023) is a collection of 23 tasks (27 subtasks)\nthat are challenging to LLMs but the performance\nmay be improved with advanced prompting tech-\nniques (Wei et al., 2022b). Some of BIG-bench\nHard tasks are closely related to real-world appli-\ncations (e.g., movie recommendation).\n(4) Counterfactual Evaluation.\nWe use the\narithmetic, chess, and syntax tasks and their coun-\nterfactual variants introduced in Wu et al. (2023).\nFor arithmetic, the original task is base-10 addition,\nand the counterfactual tasks are base-8/9/11/16 ad-\ndition. We use this set of tasks to observe whether\nPE2 can reason about counterfactual situations and\ncommunicate them to the task model.\n(5) Production Prompt.\nLastly, we optimize an\ninternal production prompt for a hierarchical, multi-\nlabel classification task. The task is to classify a\nuser query into domains, intents and slots, and then\noutput a nested dictionary as the result. The initial-\nization prompt is carefully designed by experienced\nengineers and has more than 5k tokens.\n4.2\nCompared Methods\nWe compare PE2 with the following automatic\nprompt engineering methods.\n(a) APE (Zhou\net al., 2023b): The base version of APE is an\n4\ninitialization-only method and does not involve\nnew prompt proposal steps. It uses an initialization\nprompt pinit to generate multiple prompt candi-\ndates from a few examples, and select the best one\namong them based on Ddev performance. (b) Iter-\native APE (Zhou et al., 2023b): After initialization,\npmeta instructs the model to produce a paraphrase\nof p(t) and use it as p(t+1). (c) APO (Pryzant\net al., 2023): pmeta contains short instructions on\ninspecting the batch B, generating textual \u201cgra-\ndients\u201d (feedback), and producing a new prompt\np(t+1). We include the pinit and pmeta used in these\nbaseline methods in Appendix E.\n4.3\nExpeirment Details\nLLMs.\nBy default, we use gpt-4 (OpenAI,\n2023) as prompt proposal model Mproposal and\nuse text-davinci-003 (Ouyang et al., 2022) as\nthe task model Mtask performing the underlying\ntask. Experiments on BIG-bench Hard are con-\nducted at a later stage, and we use gpt-4-turbo\nand gpt-3.5-turbo-instruct to save costs and\ndemonstrate the compatibility of our methods. For\nfair comparison, we use the same set of LLMs\nwhen running all compared methods and PE2.\nPrompt Initialization.\nFor Math Reasoning and\nBIG-bench Hard, we use \u201cLet\u2019s think step by step.\u201d\n(Kojima et al., 2022) as the initialization prompt,\nwhich can elicit multi-step reasoning in LLMs to\nperform these tasks. For Instruction Induction, we\nfollow the setting in prior works and use induction\ninitialization. For Counterfactual Eval, we exper-\niment with both. For the production task, we use\nthe prompt written by an experienced engineer.\nSearch Budget.\nTo ensure fair comparison, we\nuse the same search budget for all prompt optimiza-\ntion methods. For experiments using induction ini-\ntialization, 30 prompts are generated by pinit and\nform the initial candidate set P (0). The number of\noptimization steps T is set to be 3. At each times-\ntamp, we select n = 4 best-performing prompts,\nand propose m = 4 prompts from each of them.\n5\nResults and Analysis\n5.1\nMain Results\nImproved baselines with more recent LLMs.\nIn Zero-shot CoT (Kojima et al., 2022) and APE\n(Zhou et al., 2023b), the results were obtained\nwith a earlier text-davinci-002 model.\nWe\nfirst rerun the prompts in these two works with\nMethod\nTask\nModel\nProposal\nModel\nMultiArith\nTest\nGSM8K\nTest\nFixed Prompt, Reported by Zhou et al. (2023b)\nZero-shot CoT\nTD002\n-\n78.7\n40.7\nAPE\nTD002\nTD002\n82.0\n43.0\nFixed Prompt, Reproduced\nZero-shot CoT\nTD003\n-\n86.0\n60.9\nAPE\nTD003\n-\n87.0\n61.5\nPrompt Optimization\nIterative APE\nTD003\nGPT-4\n88.5\n62.7\nAPO\nTD003\nGPT-4\n88.5\n63.1\nPE2 (this work)\nTD003\nGPT-4\n92.3\n64.0\nTable 1: Performance Comparison on Math Reasoning\nTasks. TD002/003 stand for text-davinci-002/003.\nSee Table 2 for the prompt contents.\ntext-davinci-003, an upgraded model. In Ta-\nble 1, we observe a significant performance boost\nby using text-davinci-003, suggesting that it\nis more capable of solving math reasoning prob-\nlems with Zero-shot CoT. Moreover, the gaps be-\ntween the two prompts are narrowed (MultiArith:\n3.3% \u2192 1.0%, GSM8K: 2.3% \u2192 0.6%), indicat-\ning text-davinci-003 has a reduced sensitivity\nto prompt paraphrasing. Given this, methods that\nrely on simple paraphrasing, such as Iterative APE,\nmay not enhance the final outcome as effectively.\nMore precise and targeted edits are necessary to\nimprove the performance.\nPE2 outperforms Iterative APE and APO on\nvarious tasks.\nPE2 is able to find a prompt that\nachieves 92.3% accuracy on MultiArith (+6.3%\ncompared to Zero-shot CoT) and 64.0% on\nGSM8K (+3.1%). Additionally, we demonstrate\nthe wide applicablity of PE2 on a wide range of\nlanguage tasks. In Fig. 2 we summarize the re-\nsults and show that PE2 outperforms Iterative APE\n(Zhou et al., 2023b) and APO (Pryzant et al., 2023)\nin multiple cases. Most notably, when induction\ninitialization is used, PE2 outperforms APO on 11\nout of 12 counterfactual tasks (Fig. 11), demon-\nstrating that PE2 is capable of reasoning about con-\ntradictions and counterfactual situations. We defer\nexperiment details and performance breakdown for\nthese benchmarks in Appendix F.\nPE2 generates targeted prompt edits and high-\nquality prompts.\nIn Fig. 4 we plot the quality of\nprompt proposals over the course of prompt opti-\nmization. We observe very distinct patterns for the\nthree prompt optimization methods: Iterative APE\nis based on paraphrasing, so the newly generated\nprompts have smaller variance. APO makes drasti-\n5\nMethod\nMultiArith Prompt\nFixed Prompt\nZero-shot CoT\nLet\u2019s think step by step.\nAPE\nLet\u2019s work this out in a step by step way to be sure\nwe have the right answer.\nPrompt Optimization\nIterative APE\nLet\u2019s proceed in a methodical, step-by-step manner.\nAPO\nGiven the scenario, perform the necessary calcula-\ntions step by step to find the final result. Consider all\nparts of the input and the sequence of events.\nPE2\n(this work)\nLet\u2019s solve this problem by considering all the details.\nPay attention to each piece of information, remem-\nber to add or subtract as needed, and perform the\ncalculations step by step.\nTable 2: Best prompts for MultiArith found by com-\npared prompt optimization methods.\nStart\n1\n2\n3\nTimestamp\n0.00\n0.25\n0.50\n0.75\n1.00\nMultiArith Dev Acc.\nComparing Prompt Optimization Methods\nIteratvie APE\nAPO\nPE2\nBest Prompt\nFigure 4: Prompt optimization dynamics of Iterative\nAPE, APO and PE2 on MultiArith.\ncally large prompt edits and thus the performance\ndrops in the first step. PE2 is the most stable one\namong the three methods. In Table 2, we list the\noptimal prompts found by these methods. Both\nAPO and PE2 are able to provide instructions on\n\u201cconsidering all parts / details\u201d. In addition, PE2 is\ndesigned to inspect the batch closely, enabling it\nmake very specific prompt edits such as \u201cremember\nto add or subtract as needed\u201d.\n5.2\nAblation Study\nTo demonstrate the effectiveness of the three meta-\nprompt components introduced in PE2, we run\nablation experiments by removing these compo-\nnents during prompt optimization on MultiArith\nand GSM8K. In these experiments, we make sure\nthat the meta-prompt still contains sufficient in-\nformation about the task of prompt engineering\nafter the removal. From the results in Table 3, we\nobserve that these three components contribute sig-\nnificantly to prompt engineering quality. As shown\nin Fig. 5, the exclusion of any one of these com-\nponents results in a higher variance in the quality\ndistribution of newly-proposed prompts. Moreover,\nwithout these components, the proposal model\nmore frequently suggests low-quality prompts.\nWe also conduct an ablation study on back-\nMethod\nMultiArith\nDev\nGSM8K\nDev\nPE2 (default)\n92.0\n68.0\nMeta-prompt Components\n- two-step task description\n89.0\n66.0\n- step-by-step reasoning template\n87.0\n61.0\n- context specification\n93.0\n63.0\nSearch Algorithm Configurations\n- back-tracking\n90.0\n66.0\n- hard negative sampling\n90.0\n68.0\nTable 3: Ablation study on meta-prompt components.\nStart\n1\n2\n3\nTimestamp\n0.2\n0.4\n0.6\n0.8\n1.0\nMultiArith Dev Acc.\nAblation on Meta-prompt Components\ndefault\n- task desc\n- reasoning\n- context\nBest Prompt\nFigure 5: Prompt optimization dynamics on MultiArith\nwhen removing meta-prompt components. By removing\none component, the new prompts have larger variance\nin their quality.\ntracking (i.e., at timestamp t, select top-performing\nprompts from \u222at\ni=0P (i) versus only P (t)) and hard\nnegative sampling (i.e., the batch B is sampled\nfrom the model\u2019s errors, versus the batch is ran-\ndomly sampled from Dtrain). We conclude that\nthese design choices are important to PE2.\n5.3\nCase Study\nPE2 amends erroneous or incomplete instruc-\ntions, and devises multi-step plans for complex\ntasks.\nIn Table 4 and Table 14, we present no-\ntable prompt edits made by PE2. In the task of\nrhymes (finding a word that rhymes with the input\nword), induction initialization mistakenly suggests\nthe task is about changing the first letter of a word.\nPE2 successfully correct this after one optimization\nstep. On the task of movie recommendation (Ta-\nble 4), PE2 is able to decompose the complex task\ninto concrete criteria, such as genre, plot and actor,\nwhen determining movie similarities. On the task\nof date understanding (Table 14), PE2 identifies\nthe key step of referencing information about \u201cto-\nday\u201d. We believe these behaviors on summarizing\nkey steps from failure examples and incorporating\nthem into prompts demonstrate the intelligence and\ncapabilities of PE2.\nLimitations on following the meta-prompt and\nhallucination.\nDespite the successes made by\n6\nTask\nt\nPrompt\nDev Acc.\nCorrect wrong or incomplete task instructions\nRhymes\n0\nRemove the first letter from each input word and then replace that first letter with a similar\nsounding letter or group of letters to form a new word.\n0.35\n1\nGenerate a word that rhymes with the input word.\n0.45\nLay out tailored multi-step plans for complex problems\nMovie\nRecommendation\n0\nLet\u2019s think step by step.\n0.58\n1\nConsider the genre, plot, and style of the input movies. Using this information, think step by\nstep to identify which of the following options is most similar to the given movies.\n0.74\n2\nConsidering factors such as genre, director, actors, release period, audience target, animation\nstyle, and humor, analyze the similarities among the given movies and identify the movie from\nthe options that shares the most similarities.\n0.82\nProduce shortcut solutions in counterfactual tasks\nBase-8 Addition\n(Induction Init.)\n0\nAdd the two numbers given as input to get the output.\n0.0\n3\nAdd the two numbers provided in the input. Then, adjust this sum based on the following rule:\nif both numbers are less than 50, add 2 to the sum. If either number is 50 or greater, add 22 to\nthe sum. The final result is the output.\n0.35\nTable 4: Notable prompt edits made by PE2. See Table 14 for additional examples.\nPE2, we note several factors that\u2019s limiting its per-\nformance when applied to challenging counterfac-\ntual tasks. We provide representative cases in Ta-\nble 5. Occasionally, PE2 insists that \u201cthe prompt\nis correct, but the label is incorrect\u201d and refuses to\npropose a new prompt. We also attempt to guide\nPE2 with hints (e.g., \u201cthe calculation may be per-\nformed in a different numerical base.\u201d). Regret-\ntably, this sometimes guides the model to generate\nincorrect solutions (e.g., base-80), and even create\nrationales to verify this imagined solution. These\noberservations highlight the critical need to im-\nprove instruction following abilities and address\nhallucination issues in LLMs.\nDiscussion on \u201cshortcut learning.\u201d\nWe find in-\nteresting yet concerning prompt edits on the coun-\nterfactual task of base-8 addition. When induction\ninitialization is used (i.e., PE2 is uninformed with\nthe information of base-8 and must infer it from\nthe examples), PE2 is able to devise its own arith-\nmetic rules that is partially correct (\u201c... if either\nnumber is 50 or greater, add 22 to the sum\u201d; see\nTable 4). On the positive side, it demonstrates\nPE2\u2019s ability to engage in sophisticated counter-\nfactual reasoning. However, it is concerning that\nmodels prompted with these self-induced short-\ncut solutions achieve a test accuracy of 37% (aver-\nage over 5 runs), which outperform models explic-\nitly prompted to perform base-8 addition (test acc:\n17%/28% before/after PE2 optimization). Shortcut\nlearning (Geirhos et al., 2020) has been studied\nextensively for gradient-based optimization. Our\nexperiments suggest that similar failure modes may\nbe present in textual prompt optimization as well.\n5.4\nAdditional Analysis\nDue to space limit, we summarize our other find-\nings below and defer the details to Appendix A.\nEffect of Initialization. (\u00a7A.1)\n(1) PE2 is able\nto recover from misleading or irrelevant prompt\ninitializations, however the final prompt after opti-\nmization is worse than when using an instructive\ninitialization. (2) We experiment with induction\ninitialization. In this case, PE2 is able to discover a\nhigh quality prompt from scratch that matches with\n\u201cLet\u2019s think step by step\u201d on MultiArith.\nEffect of Task Format/Difficulty. (\u00a7A.2)\nWe ex-\nperiment with using a generative format (i.e., gener-\nating the answer string) and a multi-choice format\n(i.e., selecting from given choices A/B/C/D) on the\nDate Understanding task in BIG-bench Hard. We\nobserve that automatic prompt engineering meth-\nods has limited effect on the multi-choice format,\nbut bring significant gains on the generative format.\nDo optimized prompts generalize to other\nLLMs? (\u00a7A.3)\nWe evaluate prompts optimized\nfor text-davinci-003 on other models such\nas mpt-7b-instruct and mistral-7b-instruct.\nOur results do not exhibit consistent generalization\ntrends. This suggests that, at least in our current\nexperiment setting, automatic prompt engineering\nmethods tend to find model-specific prompts.\n6\nRelated Work\nAutomatic Prompt Engineering.\nTo alleviate\nthe intensive trial-and-error efforts in manual\nprompt engineering, the research community has\n7\nTask\nMeta-prompt and Reasoning Snippets\nNeglecting instructions in the meta-prompt\nBase-9 Addition\nMeta-prompt: ... Note that the ground-truth labels are __absolutely correct__, but the prompts (task descriptions)\nmay be incorrect and need modification. ...\nReasoning: No, it is not necessary to edit the prompt. The prompt is correct, but the label is incorrect. ... The issue\nseems to be with the label, not the prompt.\nHallucination (when hints are provided in the meta-prompt)\nBase-8 Addition\nHint: The calculation may be performed in a different numerical base.\nReasoning: ... Given this, it\u2019s possible that the numbers are being added in base 80, not base 10. In base 80, adding\n20 to the sum of two numbers would be equivalent to adding 1 in base 10.\nNew Prompt: The inputs are two numbers separated by a \u2019+\u2019. Add these two numbers together in base 80, then add\n1 to give the output in base 10.\nTable 5: Limitations and failure cases of PE2.\ndeveloped various strategies to automate this pro-\ncess with techniques such as incremental editing\n(Prasad et al., 2023), reinforcement learning (Deng\net al., 2022; Zhang et al., 2022), algorithmic search\n(Xu et al., 2022), generating in-context demonstra-\ntions adaptively (Wan et al., 2023a,b), among oth-\ners. A line of work focus on meta-prompting LLMs\nthemselves for automatic prompt engineering (Hon-\novich et al., 2023; Zhou et al., 2023b; Pryzant et al.,\n2023). In our work, we discuss potential limita-\ntions in these methods and subsequently introduce\nnew meta-prompt components for improvement.\nPrompting LLMs for Complex Reasoning Tasks.\nRecent research works suggest that LLMs can per-\nform complex reasoning tasks, e.g., grade-school\nmath problems (Cobbe et al., 2021). There are two\nmajor techniques to boost LLMs\u2019 performance on\nthis: (1) prompting methods that guide the model\nto produce intermediate reasoning steps, either with\nfew-shot demonstrations (Nye et al., 2021; Wei\net al., 2022a; Yao et al., 2023) or with zero-shot\nprompts (Kojima et al., 2022); (2) self-reflection\nmethods that progressively guide the model to in-\nspect its current output and refine it (Chen et al.,\n2023; Madaan et al., 2023; Paul et al., 2023; Kim\net al., 2023a). At its core, prompt engineering is a\ncomplex language generation task. Human prompt\nengineers usually examine the failure cases pro-\nduced by the current prompt closely, make hypothe-\nses, and compose a new prompt. In this work, we\nexplore various prompting strategies when building\nan LLM-powered automatic prompt engineer.\nSelf-training and Self-improving for LLMs.\nSelf-training refers to the technique of using a weak\nmodel to annotate input-label pairs and using them\nfor further training (Rosenberg et al., 2005). In\nthe context of LLMs, STaR (Zelikman et al., 2022)\nand Self-Improve (Huang et al., 2023) show that\nemploying LLMs to generate high-quality reason-\ning chains, followed by model fine-tuning on these\nchains, can significantly improve the model\u2019s rea-\nsoning capabilities. In this work, we consider tex-\ntual prompts as the \u201cparameters\u201d of LLMs, and we\noptimize these \u201cparameters\u201d with LLMs. This may\nbe categorized as a case of self-improving (Good-\nman, 2023). More discussion in Appendix C.1.\n7\nConclusion\nIn this paper, we introduced three meta-prompt\ncomponents that lead to improved performance\non automatic prompt engineering. The resulting\nmethod PE2 refines prompts written by human ex-\nperts and surpasses established automatic prompt\nengineering baselines across various scenarios, no-\ntably on counterfactual tasks and production appli-\ncations. Through comprehensive analysis and case\nstudies, we illustrate PE2\u2019s ability to make targeted\nprompt edits and generate high-quality prompts.\nThe challenge of prompt engineering a prompt\nengineer remains ongoing. As highlighted in our\ncase study, we believe improving the LLM\u2019s in-\nstruction following abilities and mitigating hallu-\ncination issues will be crucial for improving auto-\nmatic prompt engineering. As the capabilities of\nLLM continue to evolve, their potential involve-\nment in optimization or feedback loops necessi-\ntates a deeper empirical understanding of their\nfailure modes, including shortcut learning discov-\nered in this study. Looking ahead, we are also\nexcited about applying PE2 to optimize its own\nmeta-prompt in a self-referential way, in line with\nMetz et al. (2020); Irie et al. (2022); Fernando et al.\n(2023); Zelikman et al. (2023).\n8\nLimitations\nFirstly, we opt for a relatively small prompt search\nbudget (T = 3, m = 4, n = 4; see \u00a74.3) due to\ncost considerations. In most of our experiments,\nthe performance tends to plateau after T = 3 opti-\nmization steps. However, it\u2019s important to consider\nthat the use of the initialization prompt \"let\u2019s think\nstep by step\" in many cases might introduce a po-\ntential confounding factor. This prompt could be\nalready near-optimal, leading to fast convergence\nduring prompt optimization. Given the stochas-\ntic nature of natural language generation sampling\nand prompt optimization dynamics, it is possible\nthat a larger prompt search budget or different ex-\nperimental settings could yield new insights and\nconclusions.\nSecondly, our study uses proprietary models\nsuch as gpt-4 and text-davinci-003.\n(1) It\nraises reproducibility concerns as proprietary mod-\nels may undergo upgrades or discontinuation over\ntime. However, we believe the core concepts in-\ntroduced in this paper is model-agnostic. This is\nsupported by our experiments where we use two dif-\nferent sets of (Mproposal, Mtask) (see \u00a74.3). Con-\nsequently, we anticipate that PE2 could be adapted\nfor use with open models once their capabilities\ncatch on. (2) It also raises concerns on data con-\ntamination, as the tasks and prompts included in\nour study may or may not have been part of the\nmodel\u2019s training data.\nLastly, apart from the three translation tasks in\nthe Instruction Induction benchmark, our study pre-\ndominantly focuses on tasks in English. We rec-\nognize the importance of inclusivity in language\ntechnology and acknowledge the need to extend\nour research to a multilingual setting in the future.\nReferences\nHoward Chen, Huihan Li, Danqi Chen, and Karthik\nNarasimhan. 2022.\nControllable text genera-\ntion with language constraints.\narXiv preprint\narXiv:2212.10466.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yi-\nhan Wang, Han Guo, Tianmin Shu, Meng Song, Eric\nXing, and Zhiting Hu. 2022. RLPrompt: Optimizing\ndiscrete text prompts with reinforcement learning.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3369\u20133391, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nChrisantha\nFernando,\nDylan\nBanarse,\nHenryk\nMichalewski, Simon Osindero, and Tim Rock-\nt\u00e4schel. 2023.\nPromptbreeder:\nSelf-referential\nself-improvement via prompt evolution.\narXiv\npreprint arXiv:2309.16797.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2023. Pal: Program-aided language\nmodels. In International Conference on Machine\nLearning, pages 10764\u201310799. PMLR.\nRobert\nGeirhos,\nJ\u00f6rn-Henrik\nJacobsen,\nClaudio\nMichaelis,\nRichard\nZemel,\nWieland\nBrendel,\nMatthias Bethge, and Felix A Wichmann. 2020.\nShortcut learning in deep neural networks. Nature\nMachine Intelligence, 2(11):665\u2013673.\nNoah Goodman. 2023. Meta-prompt: A simple self-\nimproving language agent.\nOr Honovich, Uri Shaham, Samuel R. Bowman, and\nOmer Levy. 2023. Instruction induction: From few\nexamples to natural language task descriptions. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1935\u20131952, Toronto, Canada.\nAssociation for Computational Linguistics.\nXinyu Hu, Pengfei Tang, Simiao Zuo, Zihan Wang,\nBowen Song, Qiang Lou, Jian Jiao, and Denis X\nCharles. 2024.\nEvoke: Evoking critical thinking\nabilities in LLMs via reviewer-author prompt editing.\nIn The Twelfth International Conference on Learning\nRepresentations.\nJiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi\nWang, Hongkun Yu, and Jiawei Han. 2023. Large\nlanguage models can self-improve. In Proceedings\nof the 2023 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1051\u20131068, Singa-\npore. Association for Computational Linguistics.\nKazuki Irie, Imanol Schlag, R\u00f3bert Csord\u00e1s, and J\u00fcr-\ngen Schmidhuber. 2022. A modern self-referential\nweight matrix that learns to modify itself. In Proc.\nInt. Conf. on Machine Learning (ICML), Baltimore,\nMD, USA.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423\u2013438.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer.\n2023a. Language models can solve computer tasks.\narXiv preprint arXiv:2303.17491.\n9\nSeungone Kim, Se Joo, Doyoung Kim, Joel Jang,\nSeonghyeon Ye, Jamin Shin, and Minjoon Seo.\n2023b. The CoT collection: Improving zero-shot\nand few-shot learning of language models via chain-\nof-thought fine-tuning. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 12685\u201312708, Singapore.\nAssociation for Computational Linguistics.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances\nin Neural Information Processing Systems.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-\ncient memory management for large language model\nserving with pagedattention. In Proceedings of the\nACM SIGOPS 29th Symposium on Operating Systems\nPrinciples.\nMoxin Li, Wenjie Wang, Fuli Feng, Yixin Cao, Jizhi\nZhang, and Tat-Seng Chua. 2023. Robust prompt\noptimization for large language models against distri-\nbution shifts. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1539\u20131554, Singapore. Association for\nComputational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086\u20138098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with\nself-feedback. arXiv preprint arXiv:2303.17651.\nLuke Metz, Niru Maheswaranathan, C Daniel Freeman,\nBen Poole, and Jascha Sohl-Dickstein. 2020. Tasks,\nstability, architecture, and compute: Training more\neffective learned optimizers, and using them to train\nthemselves. arXiv preprint arXiv:2009.11243.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2021. Show your work: Scratch-\npads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114.\nOpenAI. 2023.\nGpt-4 technical report.\nArXiv,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nDebjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat-\nriz Borges, Antoine Bosselut, Robert West, and\nBoi Faltings. 2023. Refiner: Reasoning feedback\non intermediate representations.\narXiv preprint\narXiv:2304.01904.\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit\nBansal. 2023. GrIPS: Gradient-free, edit-based in-\nstruction search for prompting large language models.\nIn Proceedings of the 17th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, pages 3845\u20133864, Dubrovnik, Croatia.\nAssociation for Computational Linguistics.\nReid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang\nZhu, and Michael Zeng. 2023. Automatic prompt op-\ntimization with \u201cgradient descent\u201d and beam search.\nIn Proceedings of the 2023 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n7957\u20137968, Singapore. Association for Computa-\ntional Linguistics.\nNing Qian. 1999. On the momentum term in gradi-\nent descent learning algorithms. Neural networks,\n12(1):145\u2013151.\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\ngramming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the\n2021 CHI Conference on Human Factors in Comput-\ning Systems, pages 1\u20137.\nChuck Rosenberg, Martial Hebert, and Henry Schnei-\nderman. 2005. Semi-supervised self-training of ob-\nject detection models. 2005 Seventh IEEE Work-\nshops on Applications of Computer Vision (WACV/-\nMOTION\u201905) - Volume 1, 1:29\u201336.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1743\u20131752, Lisbon, Portu-\ngal. Association for Computational Linguistics.\nNoah Shinn, Federico Cassano, Beck Labash, Ash-\nwin Gopinath, Karthik Narasimhan, and Shunyu\nYao. 2023.\nReflexion:\nLanguage agents with\nverbal reinforcement learning.\narXiv preprint\narXiv:2303.11366.\nJiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian\nHu, Rahul Gupta, John Wieting, Nanyun Peng, and\nXuezhe Ma. 2023. Evaluating large language models\non controlled generation tasks. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3155\u20133168, Singapore.\nAssociation for Computational Linguistics.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc Le, Ed Chi, Denny\nZhou, and Jason Wei. 2023. Challenging BIG-bench\ntasks and whether chain-of-thought can solve them.\nIn Findings of the Association for Computational Lin-\nguistics: ACL 2023, pages 13003\u201313051, Toronto,\nCanada. Association for Computational Linguistics.\n10\nXingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik,\nand Tomas Pfister. 2023a. Better zero-shot reasoning\nwith self-adaptive prompting. In Findings of the As-\nsociation for Computational Linguistics: ACL 2023,\npages 3493\u20133514, Toronto, Canada. Association for\nComputational Linguistics.\nXingchen Wan, Ruoxi Sun, Hootan Nakhost, Hanjun\nDai, Julian Eisenschlos, Sercan Arik, and Tomas\nPfister. 2023b. Universal self-adaptive prompting.\nIn Proceedings of the 2023 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n7437\u20137462, Singapore. Association for Computa-\ntional Linguistics.\nXinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Hao-\ntian Luo, Jiayou Zhang, Nebojsa Jojic, Eric Xing, and\nZhiting Hu. 2024. Promptagent: Strategic planning\nwith language models enables expert-level prompt op-\ntimization. In The Twelfth International Conference\non Learning Representations.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022a. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 24824\u201324837. Curran Associates,\nInc.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022b. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 24824\u201324837. Curran Associates,\nInc.\nZhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky\u00fcrek,\nBoyuan Chen, Bailin Wang, Najoung Kim, Jacob An-\ndreas, and Yoon Kim. 2023. Reasoning or reciting?\nexploring the capabilities and limitations of language\nmodels through counterfactual tasks. arXiv preprint\narXiv:2307.02477.\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yang-\ngang Wang, Haiyu Li, and Zhilin Yang. 2022. Gps:\nGenetic prompt search for efficient few-shot learning.\narXiv preprint arXiv:2210.17041.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\nQuoc V Le, Denny Zhou, and Xinyun Chen. 2023.\nLarge language models as optimizers. arXiv preprint\narXiv:2309.03409.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference\non Learning Representations.\nEric Zelikman, Eliana Lorch, Lester Mackey, and\nAdam Tauman Kalai. 2023. Self-taught optimizer\n(stop): Recursively self-improving code generation.\narXiv preprint arXiv:2310.02304.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-\nman. 2022. Star: Bootstrapping reasoning with rea-\nsoning. In Advances in Neural Information Process-\ning Systems, volume 35, pages 15476\u201315488. Curran\nAssociates, Inc.\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale\nSchuurmans, and Joseph E Gonzalez. 2022. Tem-\npera: Test-time prompting via reinforcement learning.\narXiv preprint arXiv:2211.11890.\nTony Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nInternational Conference on Machine Learning.\nWangchunshu Zhou, Yuchen Eleanor Jiang, Ethan\nWilcox, Ryan Cotterell, and Mrinmaya Sachan.\n2023a. Controlled text generation with natural lan-\nguage instructions. In Proceedings of the 40th Inter-\nnational Conference on Machine Learning, volume\n202 of Proceedings of Machine Learning Research,\npages 42602\u201342613. PMLR.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2023b. Large language models are human-level\nprompt engineers.\nIn The Eleventh International\nConference on Learning Representations.\n11\nA\nAdditional Results and Analysis\nA.1\nEffect of Initialization\nPreviously, we use \u201cLet\u2019s think step by step.\u201d as\nthe initialization for math reasoning tasks. We fur-\nther experiment with using a misleading prompt,\nan irrelevant prompt and induction initialization\n(inducing from a few examples). The results are\npresented in Table 6 and the optimization dynamics\nare visualized in Fig. 6.\nInitialization\nMultiArith\nDev\nGSM8K\nDev\ndefault (Let\u2019s think step by step.)\n92.0\n68.0\nmisleading\u2020 (Don\u2019t think. Just feel.)\n81.0\n50.0\nirrelevant\u2020 (It\u2019s a beautiful day.)\n73.0\n49.0\ninduction from few-shot examples\n84.0\n43.0\nno-op (Let\u2019s think step by step.)\n85.0\n57.0\nTable 6: Effect of Initialization.\n\u2020 The prompts are\noriginally from (Kojima et al., 2022).\nStart\n1\n2\n3\nTimestamp\n0.00\n0.25\n0.50\n0.75\n1.00\nMultiArith Dev Acc.\nEffect of Prompt Initialization\ndefault\nirrelevant\nmisleading\ninduction\nBest Prompt\nFigure 6: Prompt optimization dynamics on MultiArith\nwhen different prompt initializations are used.\nIn general, performance drops when alternative\ninitialization methods are used, which highlights\nthe importance of high-quality initialization. Still,\nPE2 is able to override the irrelevant or misleading\nprompts and gradually improve the performance\n(Fig. 6). Remarkably, PE2 is able to discover a high\nquality prompt2 by itself using induction initializa-\ntion (84% on MultiArith-Dev) that almost matches\nwith \u201cLet\u2019s think step by step\u201d (85%) designed\nby highly-experienced human prompt engineers.\nThis demonstrates the impressive prompt engineer-\ning capability of PE2 and suggests its potential for\nfinding even better prompts when given additional\ncomputational resources.\n2MultiArith prompt found by PE2 using induction initial-\nization: \u201cAnalyze the problem and perform the calculations.\nConsider addition, subtraction, division, multiplication and\nperform them in the order they appear. If required, round up\nresults to the nearest whole number. Subtract done tasks from\ntotal when necessary.\u201d\nA.2\nEffect of Task Format\nFor Date Understanding from BIG-bench Hard,\nwe experiment with both a generative format (i.e.,\ngenerating the answer string; used in Gao et al.\n(2023)) and a discriminative/multi-choice format\n(i.e., selecting from given choices A/B/C/D; used\nin Suzgun et al. (2023)). For Movie Recommenda-\ntion, we experiment with two different multi-choice\nformats. See Table 7 for the formats that we used.\nTask\nExample\nDate Understanding\n(multi-choice)\nToday is 9/7.\nJane is watching NFL 2003.\nWhat is the date tomorrow in MM/DD/YYYY?\nOptions: (A) 09/08/1916 (B) 09/13/2003 (C)\n08/18/2003 (D) 09/08/2003 (E) 09/15/2003 (F)\n09/01/2003 (D)\nDate Understanding\n(generative)\nMay 6, 1992 is like yesterday to Jane, but that is\nactually ten years ago. What is the date a month\nago in MM/DD/YYYY? 04/06/2002\nMovie Recommendation\n(multi-choice 1)\nFind a movie similar to Rocky, Star Wars\nEpisode IV - A New Hope, Toy Story, The\nTerminator: Options: (A) Dracula Dead and\nLoving It (B) Independence Day (C) The Ex-\ntraordinary Adventures of Ad\u00e8le Blanc-Sec (D)\nThe American President (B)\nMovie Recommendation\n(multi-choice 2)\nWhat movie is simlar to Apollo 13, Jurassic\nPark, Die Hard With a Vengeance, Forrest\nGump?\nChoose from the following: Killer\nMovie, Stealth, The Last Man on Earth, True\nLies. True Lies\nTable 7: Different Task Formats for Date Understanding\nand Movie Recommendation. The correct answer is\nmarked in blue.\nDate\n(Multi-choice)\nDate\n(Generative)\nMovie\n(Multi-choice 1)\nMovie\n(Multi-choice 2)\n0\n20\n40\n60\n80\n100\nTest Accuracy\nZeroshot CoT\nIterative APE\nAPO\nPE2\nFigure 7: Effect of Task Format. See Table 7 for the\nformats used.\nWe report the results in Fig. 7. For Date Un-\nderstanding, the multi-choice format narrows the\noutput space and thus lower the difficulty of the\ntask. We hypothesize that in combination with\nZero-shot CoT, the task performance is close to sat-\nuration and automatic prompt engineering method\ndoes not provide extra benefit. The task is more\nchallenging in the generative format and the de-\ntailed instructions in the optimized prompt bring\nsignificant performance gains.\nFor Movie Recommendation, we found that\nprompt optimization methods bring significant per-\nformance gains in both cases.\nThe optimized\nprompts contain multi-step plans (e.g., consider\n12\ngenre, director, ...), which boost the task perfor-\nmance. Minor formatting decisions such as out-\nputting the option letter or the option string can\nstill mildly affect the final accuracy.\nOverall, the question on \u201cwhen is automatic\nprompt optimization most effective\u201d is dependent\non many factors, including task format, task dif-\nficulty, output space size, task model capability,\netc.\nA.3\nDo optimized prompts generalize to other\nLLMs?\nWe evaluate 5 GSM8K prompts for our prompt\ngeneralization study (see Table 8).\nNote that\nthe APO and PE2 prompts are optimized for\ntext-davinci-003. The two prompts reported in\nOPRO (Yang et al., 2023) are optimized for PaLM\n2-L.\nWe\ninvestigate\nthe\ngeneralization\nof\nop-\ntimized\nprompts\nby\nevaluating\nthem\non\nfour\nmodels:\ngpt-3.5-turbo-instruct,\nmistral-7b-instruct-v0.2,\nyi-6b,\nand\nmpt-7b-instruct. We report the results in Fig. 8.\nMethod\nGSM8K Prompt\nZero-shot CoT\nLet\u2019s think step by step.\nAPO\nGiven the scenario, perform necessary calculations and\nprovide a step-by-step explanation to arrive at the correct\nnumerical answer. Consider all information provided.\nPE2\nLet\u2019s solve the problem step-by-step and calculate the\nrequired total value correctly.\nOPRO (1)\nTake a deep breath and work on this problem step-by-step.\nOPRO (2)\nLet\u2019s combine our numerical command and clear thinking\nto quickly and accurately decipher the answer.\nTable 8: Prompts used in transferability study in Fig. 8.\ntext-davinci\n003\ngpt-3.5-turbo\ninstruct\nmistral-7b\ninstruct-v0.2\nyi-6b\nmpt-7b\ninstruct\n0\n20\n40\n60\n80\nGSM8K Test Accuracy\nNot Available\nZeroshot CoT\nAPO\nPE2\nOPRO (1)\nOPRO (2)\nFigure 8: Analysis on generalizability of prompts across\nmodels. See Table 8 for the prompts used in this study.\nOur results do not exhibit consistent gen-\neralization\ntrends.\nOptimized\nprompts\ngenerally\noutperforms\nthe\noriginal\nzero-\nshot\nCoT\nprompt\non\ntext-davinci-003,\nmistral-7b-instruct-v0.2, yi-6b. However,\nwith\ngpt-3.5-turbo-instruct,\nthe original\nCoT prompt outperforms all optimized prompts.\nOur hypothesis is that \u201cLet\u2019s think step by\nstep.\u201d\nis included in public instruction tuning\ncollections (Kim et al., 2023b) and may be used\nduring the training of gpt-3.5-turbo-instruct.\nAs the instruction tuning mixture used for\ngpt-3.5-turbo-instruct are not disclosed, we\ncannot further investigate this.\nOverall, our results suggest that current auto-\nmatic prompt optimization methods tend to find\nmodel-specific prompts that do not reliably general-\nize to alternative models. This conclusion contrasts\nwith the findings in PromptAgent (Wang et al.,\n2024), which we attribute to discrepancies in exper-\nimental setup. To maintain consistency with prior\nresearch (Zhou et al., 2023b), we limit the prompt\nlength to be 50 or 200 tokens.\nFuture work may develop robust prompt opti-\nmization methods that operate across multiple task\nmodels, in a way similar to Li et al. (2023) which\noperates across domains. This may help identify\nhigh-quality prompts invariant to the underlying\ntask model, so that when new and more powerful\nmodels (e.g., GPT-5) are released, the optimized\nprompt may be used directly.\nB\nOther Meta-Prompt Components We\nTried\nIn addition to the meta-prompt components studies\nin the main paper, we also tried other components.\nAs the results are mixed and inconclusive on these\ncomponents, we report them here in the appendix.\nWe illustrate these components in Fig. 9.\nProviding Detailed Instructions and Context.\n(d) Prompt Engineering Tutorial. To help the\nLLM better understand the task of prompt en-\ngineering, we provide an online tutorial of\nprompt engineering in the meta-prompt.3\nIncorporating Common Optimizer Concepts.\nThe prompt engineering problem described in Eq. 1\nis essentially an optimization problem, and the\nprompt proposal in Eq. 2 can be considered as\ndoing one optimization step. Thus, we consider\nthe following concepts commonly used in gradient-\nbased optimization and develop their verbalized\ncounterparts to be used in our meta-prompt.\n(e) Batch Size. Batch size is the number of (fail-\nure) examples that is used in each prompt pro-\nposal step (Eq. 2). By default PE2 uses a batch\n3https://www.promptingguide.ai/introduction.\nPublished under MIT license.\n13\nLet\u2019s read a blogpost on prompt engineering:\nPrompt engineering is a relatively new discipline for \ndeveloping and optimizing prompts to efficiently use \nlanguage models (LMs) \u2026\n(d) prompt engineering tutorial\nA prompt is a text paragraph that outlines the expected actions \nand instructs the model. In our collaboration, we'll work \ntogether to refine a prompt. The process consists of two steps:\n# Step 1\nExamine the prompt and a batch of examples\n# Step 2\nPropose a new prompt based on your reasoning\n(a) two-step task instruction\nSure! I'd be happy to help you.\n# Current Prompt\nLet\u2019s think step by step.\n# Full Template\n```\nQuestion: <input>\nAnswer: Let\u2019s think step by step.\n```\n# Examples\n## Example 1\nInput: George had 28 socks. If he threw away 4 socks \u2026\nOutput: 64\nReasoning: Step 1: George had 28 socks. Step 2: \u2026\nLabel: 60\n[More examples \u2026]\n(b) context specification\n(c) step-by-step reasoning template\n(e) batch size\n## Example 1\nOutput is correct? No.\nReasoning: the model didn't subtract the socks he threw \naway.\nPrompt describing the task correctly? Yes.\nNecessary to edit the prompt? Yes.\nSuggestions: The prompt should be edited to guide the \nmodel to perform subtraction.\n[More examples \u2026]\nNow carefully review your reasoning and proceed with step \n2: refine the prompt.\n# Current Prompt\nLet\u2019s think step by step.\n# Optimization History\nAt time 0, the prompt was \u201c\u2026\u201d, it was edited \u2026\n# Instructions\n* You are allowed to change up to 10 words\n* The total length should be less than 50 words\n* Reply with the prompt. Do not include other text.\nLet\u2019s solve this problem step by step. Remember to add or \nsubtract as needed. \n# Instruction\nFor each example, provide reasoning according to the \nfollowing template\n* Output is correct?\n* Prompt describing the task correctly?\n* Necessary to edit the prompt?\n* If yes, suggestions on prompt editing?\n(g) optim history\n(f) step size\nMeta-Prompt: Instruction and Context\nMeta-Prompt: Optimizer Concepts\nPrompt\nFeedback (\u201cGradients\u201d)\nFigure 9: Illustration of meta-prompt components considered in Appendix B.\n14\nMethod\nMultiArith\nDev\nGSM8K\nDev\nPE2 (default)\n92.0\n68.0\nMeta-prompt Components\n+ prompt engineering tutorial\n90.0\n63.0\n+ tune batch size {1, 2, 4, 8}\n92.0\n68.0\n+ tune step size {5, 10, 15, None}\n95.0\n68.0\n+ optim history and momentum\n93.0\n67.0\nTable 9: Investigation on meta-prompt components and\nconfigurations (Appendix).\nsize of 2. We experiment with batch sizes of\n{1, 4, 8} additionally in this section.\n(f) Step Size. In gradient-based optimization, the\nstep size determines the extent to which the\nmodel\u2019s weights are updated. In prompt engi-\nneering, the counterpart would be the number\nof words (tokens) that can be modified. We di-\nrectly specify that \u201cYou are allowed to change\nup to s words in the original prompt\u201d in the\nmeta-prompt, where s \u2208 {5, 10, 15, None}.4\n(g) Optimization History and Momentum. Mo-\nmentum (Qian, 1999) is a technique to accel-\nerate optimization and avoid oscillations by\nmaintaining the moving average of past gra-\ndients. To develop the verbalized counterpart\nof momentum, we include all past prompts (at\ntimestamp 0, 1, ..., t \u2212 1), their performance\non the dev set, and a summary of prompt edits.\nResults and disucssion.\nWe do not observe sig-\nnificant improvement by incorporating prompt en-\ngineering tutorial. As the tutorial is excessively\nlong (2500+ tokens) and slows down the runtime,\nwe do not include it in the final version of PE2. The\noptimizer-inspired concepts can improve the per-\nformance occasionally, but the current experiments\ndo not give a consistent conclusion regarding their\nutilities.\nSimilar to the challenges encountered in\ngradient-based optimization, the process of hyper-\nparameter selection is inherently noisy and often\nvaries depending on the task at hand. For discrete\nprompt optimization, this complexity is further\ncompounded by factors such as the task model\u2019s\nsensitivity to prompts and the proposal model\u2019s ca-\npability to follow instructions in the meta-prompt.\nFor example, Sun et al. (2023) point out that LLMs\n4Chen et al. (2022) and Zhou et al. (2023a) showed that\nLLMs could follow text generation constraints specified in\nnatural language.\nstruggled at meeting fine-grained constraints such\nas \u201cgenerate exactly 5 words,\u201d which could poten-\ntially diminish the effectiveness of the (f) step size\ncomponent. Additionally, (g) momentum requires\nmultiple optimization steps to accumulate, yet our\nexperiments are restricted to three steps due to cost\nconstraints.\nThe potential of these meta-prompt components\nwarrants re-examination in the future, particularly\nas models become more capable, and the efficiency\nand scalability of automatic prompt engineering\nmethods improves.\nC\nAdditional Discussion\nC.1\nRecent Works\nA recent work (Yang et al., 2023) introduced the\nconcept of large language models as optimizers\nand proposed optimization by prompting (OPRO).\nIn the following, we discuss the differences and\nconnections between OPRO and our work.\n(1) Focus of the work. Both OPRO and our work\nconduct experiments on prompt optimization; the\nfocus of the two works differ. OPRO can be applied\nto general optimization problems, including linear\nregression and traveling salesman problem. In our\nwork we limit the scope to prompt optimization,\nwith a specific focus on proposing and investigating\ndifferent components in the meta-prompt.\n(2) Optimization strategy.\nThe optimization\nstrategies of the two works are different. PE2 is\nlargely inspired by the concepts in APO (Pryzant\net al., 2023), instructing the model to produce tex-\ntual feedback (\u201cgradient\u201d) explicitly. It is more\nanalogous to gradient descent. OPRO uses the\nexecution accuracy as rewards to guide the opti-\nmization indirectly, which, in our understanding, is\nmore analogous to in-context RL methods (Shinn\net al., 2023). For future work, it would be interest-\ning to compare the effectiveness and efficiency of\nboth methods in a controlled setup.\n(3) Challenges in making direct comparison.\nYang et al. (2023) mainly uses PaLM 2-L model\nand text-bison model as the task model (scorer),\nand optimizes the prompt for up to 200 steps. In\nour work, we mainly use text-davinci-003 and\nGPT-4, and optimize the prompt for 3 steps by de-\nfault. Due to access and budget constraints, we are\nunable to make direct comparison with OPRO.\nIn addition to OPRO, several recent works have\nexplored automatic prompt engineering using di-\nverse strategies. PromptBreeder (Fernando et al.,\n15\nAlgorithm 1 Search Procedure\n1: P (0) = Pinit or P (0) = Minit(x1, y1, ..., xn, yn; pinit)\n\u25b7 Manual init. or induction init.\n2: for t = 0, ..., T \u2212 1 do\n3:\nP (t+1) = \u2205\n4:\nfor p(t) \u2208 Select-Best(\u222at\ni=0P (i), n) do\n\u25b7 Select best n prompts based on Ddev\n5:\nfor j = 1...m do\n6:\nB = Sample(Dtrain)\n\u25b7 Sample a batch\n7:\np(t+1) = Moptim(p(t), B; pmeta)\n\u25b7 New prompt proposal\n8:\nP (t+1) = P (t+1) \u222a {p(t+1)}\n9:\nend for\n10:\nend for\n11: end for\n12: return Select-Best(\u222aT\ni=0P (i), 1)\n\u25b7 Return the final best prompt based on Ddev\n2023) adopts a self-referential prompt evolution\nframework, employing mutation prompts (similar\nto the concept of \u201cmeta-prompts\u201d discussed in this\npaper) to edit task prompts, and hyper-mutation\nprompts to edit mutation prompts. PromptAgent\n(Wang et al., 2024) adopts the Monte Carlo Tree\nSearch algorithm that iteratively performs selec-\ntion, expansion, simulation and back-propagation\nfor strategic prompt editing. Evoke (Hu et al., 2024)\nintroduces a collaborative approach where an LLM-\nReviewer and an LLM-author work together to\nrefine the prompt using critical thinking. In par-\nallel to these works, we focus on the design and\nevaluation of the meta-prompt in LLM-powered\nautomatic prompt engineering in this paper.\nC.2\nDiscussion on using PE2 to optimize its\nown meta-prompt\nConceptually, we can replace p(t) and p(t+1) in\nEq. 2 with the meta-prompt pmeta directly to en-\nable PE2 to optimize the meta-prompt. However,\nthree challenges (and broader questions) arise if\nwe pursue this method, and we look forward to\naddressing these challenges in the future:\n1. How to collect data for such a study? To ensure\nthis meta-prompt is general we may need a large\ncollection of tasks along with prompt optimiza-\ntion history associated with them. Creating a\nresource like this will be a large effort.\n2. How to automatically optimize the meta-prompt\nwhen there are no ground truth labels for prompt\nengineering? Math problems have ground-truth\nanswers so that PE2 can reason on them and pro-\nvide feedback on prompt refinement. The task\nof prompt engineering does not have ground\ntruth labels, and this potentially makes the meta-\nprompt optimization process more noisy.\n3. It would be very costly to run and even evaluate\na system like this. To evaluate one meta-prompt\ncandidate, we will need to use it for prompt\noptimization on various tasks. We would expect\nthe optimization process of the meta-prompt to\nbe a magnitude more costly.\nD\nAdditional Experiment Details\nD.1\nPrompt Search Algorithm\nSee Algorithm 1.\nD.2\nControlling Prompt Length\nBy default the max length of prompts is set to be\n50 tokens, following Zhou et al. (2023b). For coun-\nterfactual tasks, to allow more space to explain the\ncounterfactual situations, the max length is set to\nbe 200 tokens.\nD.3\nTasks and Data\nWe summarize the dataset size and data split in-\nformation in Table 10. We summarize the source\nand license information of the datasets in Table 11.\nTo the best of our knowledge, our usage of these\ndatasets are consistent with their intended use; the\ndata we use do not contain personal or sensitive\ninformation. Most of the datasets are in English\nand not domain-specific.\n(1) Mathematical Reasoning.\nThe MultiArith\ndataset (Roy and Roth, 2015) contains 600 exam-\nples. As our prompt optimization method requires\na training set, we randomly split into 100/100/400\nfor train/dev/test. This creates a slight discrepancy\nwhen comparing the results with past reported re-\nsults. We ensure our reproduction is fair across\ndifferent methods by using this fixed split. The\nGSM8K dataset (Cobbe et al., 2021) has a provided\ntest split (1319 examples). We randomly selected\n200 examples for the original train split, and use\n100 as Dtrain and 100 as Ddev.\n16\nDataset\nSubtasks\n|Ttrain|\n|Tdev|\n|Ttest|\n# Random Samples\nMultiArith (Roy and Roth, 2015)\n-\n100\n100\n400\n1\nGSM8K (Cobbe et al., 2021)\n-\n100\n100\n1319\n1\nInstruction Induction (Honovich et al., 2023)\n14 Subtasks\n100\n20\n100\n5\nCounterfactual Eval (Wu et al., 2023)\n12 Subtasks\n100\n20\n100\n5\nBIG-Bench Hard (BBH format used in Suzgun et al. (2023))\n27 Subtasks\n100\n100\n50\n1\nBIG-Bench Hard (Alternative format; see \u00a7A.2)\n2 Subtasks\n100\n100\n500\n1\nTable 10: Dataset sizes and data splits.\nDataset\nLicense\nSource\nMultiArith (Roy and Roth, 2015)\nUnknown\nhttps://github.com/wangxr14/Algebraic-Word-Problem-Solver/\nGSM8K (Cobbe et al., 2021)\nMIT\nhttps://github.com/openai/grade-school-math\nInstruction Induction (Honovich et al., 2023)\nApache-2.0\nhttps://github.com/orhonovich/instruction-induction\nCounterfactual Eval (Wu et al., 2023)\nUnknown\nhttps://github.com/ZhaofengWu/counterfactual-evaluation\nBIG-bench Hard (Suzgun et al., 2023)\nApache-2.0\nhttps://github.com/google/BIG-bench (original)\nhttps://github.com/suzgunmirac/BIG-Bench-Hard (reformatted)\nTable 11: License and Source of the datasets used in this study.\n(2) Instruction Induction.\nWe closely follow\nthe settings in Zhou et al. (2023b).\nFor\neach subtask, we randomly sample 5 different\nDtrain/Ddev/Dtest of size 100/20/100. We list\nthe sub-tasks in Instruction Induction benchmark\nin Table 12. We removed 8 tasks (active to passive,\ndiff, first word letter, letters list, num to verbal, sin-\ngular to plural, sum), because our baseline method\nAPE (Zhou et al., 2023b) already achieves near\nperfect accuracies (95%+) on these tasks. We also\nremoved 2 tasks (cause and effect, common con-\ncept) because they have less than 50 examples in\ntotal, and it is challenging to create train/dev/test\nsplit from these examples.\n(3) BIG-bench Hard Tasks.\nWe mainly exper-\niment with the BBH task format used in Suzgun\net al. (2023). As the public BBH repository have\n250 examples per task, we randomly split them\ninto 100/100/50 for Dtrain/Ddev/Dtest. For Date\nUnderstanding and Movie Recommendation, we\nconsider using alternative tasks formats to study\nthe their effect (see \u00a7A.2). We obtain the data from\nthe original BIG-bench repository which contains\nmore examples per task. Hence we randomly sam-\nple 100/100/500 examples for Dtrain/Ddev/Dtest\nin these two experiments.\n(4) Counterfactual Evaluation.\nWe use three\nsubtasks in this evaluation suite: arithmetic, chess\nand syntax. For each subtask, we randomly sample\n5 different Dtrain/Ddev/Dtest of size 100/20/100.\n(5) Production Prompt.\nWe use a randomly sam-\npled sub-set of human annotated queries and labels\n(> 150), which are derived from user reported er-\nrors. The data is divided between training (50%),\nvalidation (25%) and testing (25%). We use the\nF1-score for evaluating model outputs and report\nthe absolute change in score with the initialization\nprompt.\nD.4\nInfrastructure and Runtime\nInfrastructure.\nWe use OpenAI API5 to access\ngpt-4,gpt-4-turbo, gpt-3.5-turbo-instruct,\ntext-davinici-003.\nFor prompt generaliza-\ntion experiments using mistral-7b-instruct,\nmpt-7b-instruct and yi-6b, we run experiments\nlocally using one Nvidia RTX A6000 GPU and the\nvLLM toolkit (Kwon et al., 2023).\nRuntime.\nOne prompt optimization experiment\nusing gpt-4/text-davinici-003 as task/pro-\nposal model takes about 90 minutes. This is also\nsubject to API rate limits.\nCosts.\nWhen using gpt-4/text-davinci-003 it\ncosts about $25 USD for one prompt optimiza-\ntion experiment. In the later stage of this project,\nwe use gpt-4-turbo/gpt-3.5-turbo-instruct\nwhich are newer and cheaper, and the cost is re-\nduced to about $3 USD per experiment.\nE\nMeta-prompts\nWe implement the meta-prompts using the guid-\nance toolkit 6, which enables multi-round conver-\nsations and supports basic handlebars-style syntax\nto control the workflow.\n5https://openai.com/blog/openai-api\n6https://github.com/guidance-ai/guidance\n17\nTask\nInstruction\nDemonstration\nSubtasks used in this work (14)\nSecond Letter\nExtract the first letter of the input word.\ncat \u2192 a\nStarting With\nExtract the words starting with a given letter from\nthe input sentence.\nThe man whose car I hit last week sued me. [m] \u2192\nman, me\nNegation\nNegate the input sentence.\nTime is finite \u2192 Time is not finite.\nAntonyms\nWrite a word that means the opposite of the input\nword.\nwon \u2192 lost\nSynonyms\nWrite a word with a similar meaning to the input\nword.\nalleged \u2192 supposed\nMembership\nWrite all the animals that appear in the given list.\ncat, helicopter, cook, whale, frog, lion \u2192 frog, cat,\nlion, whale\nRhymes\nWrite a word that rhymes with the input word.\nsing \u2192 ring\nInformal to Formal\nRephrase the sentence in formal language.\nPlease call once you get there \u2192 Please call upon\nyour arrival.\nTranslation EN-DE\nTranslate the word into German.\ngame \u2192 spiel\nTranslation EN-ES\nTranslate the word into Spanish.\ngame \u2192 jeugo\nTranslation EN-FR\nTranslate the word into French.\ngame \u2192 jeu\nSentiment\nDetermine whether a movie review is positive or\nnegative.\nThe film is small in scope, yet perfectly formed. \u2192\npositive\nSentence Similarity\nRate the semantic similarity of two sentences on a\nscale of 0 to 5\nSentence 1: A man is smoking. Sentence 2: A man\nis skating. \u2192 0 - definitely not\nWord in Context\nDetermine whether an input word has the same mean-\ning in the two sentences.\nSentence 1: Approach a task. Sentence 2: To ap-\nproach the city. Word: approach \u2192 not the same\nSubtasks removed due to near-perfect accuracy (95%+) with baseline method (8)\nFirst Letter\nExtract the first letter of the input word.\ncat \u2192 c\nList Letters\nBreak the input word into letters, separated by spaces.\ncat \u2192 c a t\nSingular to Plural\nConvert the input word to its plural form.\ncat \u2192 cats\nActive to Passive\nWrite the input sentence in passive form.\nThe artist introduced the scientist. \u2192 The scientist\nwas introduced by the artist.\nLarger Animal\nWrite the larger of the two given animals.\nkoala, snail \u2192 koala\nSum\nSum the two given numbers.\n22 10 \u2192 32\nDiff\nSubtract the second number from the first.\n32 22 \u2192 10\nNumber to Word\nWrite the number in English words.\n26 \u2192 twenty-six\nSubtask removed due to small dataset size (2)\nCause and Effect\nFind which of the two given cause and effect sen-\ntences is the cause.\nSentence 1: The soda went flat. Sentence 2: The\nbottle was left open. \u2192 The bottle was left open.\nCommon Concept\nFind a common characteristic for the given objects.\nguitars, pendulums, neutrinos \u2192 involve oscillations.\nTable 12: Details of Instruction Induction dataset. Adapted from Table 4 in Honovich et al. (2023).\nE.1\nInitialization Prompt pinit\nThe initialization prompt is originally from APE\n(Zhou et al., 2023b). In this paper, it is shared by\nall methods (Iterative APE, APO and PE2).\n1\n{{#system~}}\n2\nYou are a helpful assistant.\n3\n{{~/ system}}\n4\n5\n{{#user~}}\n6\nI gave a friend an instruction and {{n_demo}}\ninputs. The friend read the instruction and\nwrote an output for every one of the\ninputs.\n7\nHere are the input -output pairs:\n8\n9\n{{demos}}\n10\n11\nWhat was the instruction? It has to be less than\n{{max_tokens}} tokens.\n12\n{{~/user}}\n13\n14\n{{#assistant~}}\n15\nThe instruction was {{gen 'instruction ' [[\nGENERATION_CONFIG]]}}\n16\n{{~/ assistant}}\nE.2\nAPE\n1\n{{#system~}}\n2\nYou are a helpful assistant.\n3\n{{~/ system}}\n4\n5\n{{#user~}}\n6\nGenerate a variation of the following\ninstruction while keeping the semantic\nmeaning.\n7\n8\n{{prompt}}\n9\n10\nThe new instruction has to be less than {{\nmax_tokens}} words.\n11\nReply with the new instruction. Do not include\nother text.\n12\n{{~/user}}\n13\n14\n{{#assistant~}}\n15\n{{gen 'new_prompt ' [[GENERATION_CONFIG]]}}\n16\n{{~/ assistant}}\nE.3\nAPO\nPart 1 - Generating \u201cgradients\u201d\n1\n{{#system~}}\n18\nTask\nCategory\nDemonstration\nArithmetic - Two-digit addition\nBase-10\nOriginal\n22+10 \u2192 32\nBase-8\nCounterfactual\n76+76 \u2192 174\nBase-9\nCounterfactual\n76+14 \u2192 101\nBase-11\nCounterfactual\n76+14 \u2192 8A\nBase-16\nCounterfactual\nEC+DD \u2192 1C9\nChess - Legality of a 4-move opening\nNormal Rules\nOriginal\n1. g3 Ng6 2. b3 Kf8 * \u2192 illegal\nSwapping bishops and knights\nCounterfactual\n1. g3 Ng6 2. b3 Kf8 * \u2192 legal\nSyntax - Identify the main subject and the main verb of a sentence\nSVO\nOriginal\nhe has good control . \u2192 he has\nSOV\nCounterfactual\nhe good control has . \u2192 he has\nVSO\nCounterfactual\nhas he good control . \u2192 he has\nVOS\nCounterfactual\nhas good control he . \u2192 he has\nOVS\nCounterfactual\ngood control has he . \u2192 he has\nOSV\nCounterfactual\ngood control he has . \u2192 he has\nTable 13: Details of Couterfactual Evaluation dataset.\n2\nYou are a helpful assistant.\n3\n{{/system~}}\n4\n5\n{{#user~}}\n6\nI'm trying to write a zero -shot classifier\nprompt.\n7\n8\nMy current prompt is:\n9\n\"{{prompt}}\"\n10\n11\nBut this prompt gets the following examples\nwrong:\n12\n{{failure_string}}\n13\n14\nGive {{n_reasons}} reasons why the prompt could\nhave gotten these examples wrong. Do not\ninclude other text.\n15\n{{/user~}}\n16\n17\n{{#assistant~}}\n18\n{{gen 'gradients ' temperature=0.0}}\n19\n{{/assistant~}}\nPart 2 - Refining the prompt\n1\n{{#system~}}\n2\nYou are a helpful assistant.\n3\n{{/system~}}\n4\n5\n{{#user~}}\n6\nI'm trying to write a zero -shot classifier.\n7\n8\nMy current prompt is:\n9\n\"{{prompt}}\"\n10\n11\nBut it gets the following examples wrong:\n12\n{{failure_string}}\n13\n14\nBased on these examples the problem with this\nprompt is that:\n15\n{{gradient}}\n16\n17\nBased on the above information, I wrote an\nimproved prompt. The total length of the\nprompt should be less than {{max_tokens}}\nwords.\n18\n{{/user~}}\n19\n20\n{{#assistant~}}\n21\nThe improved prompt is {{gen 'new_prompt '\ntemperature=0.0}}\n22\n{{/assistant~}}\nE.4\nPE2\n1\n{{#system~}}\n2\nYou are a helpful assistant.\n3\n{{~/ system}}\n4\n5\n{{#if instruction}}\n6\n{{#user~}}\n7\nLet 's read a blogpost on prompt engineering:\n8\n{{instruction}}\n9\n{{~/user}}\n10\n{{/if}}\n11\n12\n{{#user~}}\n13\nA prompt is a text paragraph that outlines the\nexpected actions and instructs the model to\ngenerate a specific output. This prompt is\nconcatenated with the input text, and the\nmodel then creates the required output.\n14\n15\nIn our collaboration, we'll work together to\nrefine a prompt. The process consists of\ntwo main steps:\n16\n17\n## Step 1\n18\nI will provide you with the current prompt, how\nthe prompt is concatenated with the input\ntext (i.e., \"full template \"), along with {{\nbatch_size}} example(s) that are associated\nwith this prompt. Each examples contains\nthe input, the reasoning process generated\nby the model when the prompt is attached,\nthe final answer produced by the model, and\nthe ground -truth label to the input. Your\ntask is to analyze the examples,\ndetermining whether the existing prompt is\ndecsribing the task reflected by these\nexamples precisely, and suggest changes to\nthe prompt.\n19\n20\n## Step 2\n21\nNext, you will carefully review your reasoning\nin step 1, integrate the insights to craft\na new, optimized prompt. Optionally, the\nhistory of refinements made to this prompt\nfrom past sessions will be included. Some\nextra instructions (e.g., the number of\nwords you can edit) will be provided too.\n22\n{{~/user}}\n23\n24\n{{#assistant~}}\n25\nSure, I'd be happy to help you with this prompt\nengineering problem.\n19\n26\nPlease provide me with the prompt engineering\nhistory, the current prompt, and the\nexamples you have.\n27\n{{~/ assistant}}\n28\n29\n{{#user~}}\n30\n## Prompt\n31\n{{prompt}}\n32\n33\n## Full Template\n34\nThis describes how the prompt of interested is\nconcatenated with the input text.\n35\nThe prompt may appear before the input text, or\nafter the input text.\n36\nOptionally the full template may contain other\ntemplate information.\n37\n```\n38\n{{full_prompt}}\n39\n```\n40\n41\n## Examples\n42\n{{examples}}\n43\n44\n## Instructions\n45\nFor some of these examples, the output does not\nmatch with the label. This may be due to\nthe prompt being misleading or not\ndescribing the task precisely.\n46\n47\nPlease examine the example(s) carefully. Note\nthat the ground -truth labels are\n__absolutely correct__, but the prompts (\ntask descriptions) may be incorrect and\nneed modification. For each example,\nprovide reasoning according to the\nfollowing template:\n48\n49\n### Example <id>\n50\nInput: <input >\n51\nOutput: <output >\n52\nLabel: <label >\n53\nIs the output correct compared to the label: <\nyes or no, and your reasoning >\n54\nIs the output correctly following the given\nprompt: <yes or no, and your reasoning >\n55\nIs the prompt correctly describing the task\nshown by the input -label pair: <yes or no,\nand your reasoning >\n56\nTo output the correct label, is it necessary to\nedit the prompt: <yes or no, and your\nreasoning >\n57\nIf yes, provide detailed analysis and actionable\nsuggestions to edit the prompt: <analysis\nand suggestions >\n58\n{{~/user}}\n59\n60\n{{#assistant~}}\n61\n{{gen 'reasoning ' temperature=0}}\n62\n{{~/ assistant}}\n63\n64\n{{#user~}}\n65\nNow please carefully review your reasoning in\nStep 1 and help with Step 2: refining the\nprompt.\n66\n67\n{{#if history}}\n68\n## Prompt Refinement History from the Past\n69\nNote that higher accuracy means better. If some\nedits are useful in the past, it may be a\ngood idea to make edits along the same\ndirection.\n70\n{{history}}\n71\n{{/if}}\n72\n73\n## Current Prompt\n74\n{{prompt}}\n75\n76\n## Instructions\n77\n{{#if step_size}}\n78\n* You are allowed to change up to {{step_size}}\nwords in the original prompt.\n79\n{{/if}}\n80\n{{#if max_tokens}}\n81\n* The total length of the prompt should be less\nthan {{max_tokens}} words.\n82\n{{/if}}\n83\n* Please help edit the prompt so that the\nupdated prompt will not fail on these\nexamples anymore.\n84\n* Reply with the prompt. Do not include other\ntext.\n85\n{{~/user}}\n86\n87\n{{#assistant~}}\n88\n{{gen 'new_prompt ' temperature=0.7 max_tokens=30\n0}}\n89\n{{~/ assistant}}\n90\n91\n{{#if history}}\n92\n{{#user~}}\n93\nNow please summarize what changes you 've made to\nthe prompt, in the following format. Make\nsure the summariy is concise and contains\nno more than 200 words.\n94\n95\n\" * At step {{timestamp}}, the prompt has\nlimitations such as <summary of limitations\n>. Changes to the prompt include <summary\nof changes >.\"\n96\n97\nReply with the summarization. Do not include\nother text.\n98\n{{~/user}}\n99\n100\n{{#assistant~}}\n101\n{{gen 'new_history ' temperature=0.7 max_tokens=2\n00}}\n102\n{{~/ assistant}}\n103\n{{/if}}\nF\nAdditional Result Figures and Tables\nNotable Prompt Edits.\nAdditional examples on\nnotable prompt edits made by PE2 are in Table 14.\nResults Breakdown.\nWe report the results on\neach subtask in Instruction Induction in Fig. 10.\nFor counterfactual tasks, results using induction\ninitialization are in Fig. 11 and results using man-\nual initialization are in Fig. 12. For BIG-bench\nHard tasks, we report the results in Fig. 13. We re-\nport the results on Date Understanding and Movie\nRecommendation when alternative task formats are\nused in Table 19.\n20\nantonyms\ninformal_to_formal\nnegation\northography_starts_with\nrhymes\nsecond_word_letter sentence_similarity\n0.00\n0.25\n0.50\n0.75\n1.00\nMean Score\nAPE\nIterative APE\nAPO\nPE2\nsentiment\nsynonyms\ntaxonomy_animal\ntranslation_en-de\ntranslation_en-es\ntranslation_en-fr\nword_in_context\n0.00\n0.25\n0.50\n0.75\n1.00\nMean Score\nFigure 10: Results on the Instruction Induction Benchmark. The performance of APO and PE2 are close to each\nother on most tasks. Our hypothesis is that tasks in Instruction Induction Benchmark are relatively easier compared\nto the other benchmarks, leading to performance saturation.\nArithmetic:Base-8\nArithmetic:Base-9\nArithmetic:Base-11\nArithmetic:Base-16\nChess:Original\nChess:Counterfactual\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nAPE\nIterative APE\nAPO\nPE2\nSyntax:SVO\nSyntax:SOV\nSyntax:OSV\nSyntax:OVS\nSyntax:VOS\nSyntax:VSO\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nFigure 11: Results on Counterfactual Eval (Induction Initialization).\nArithmetic:Base-8\nArithmetic:Base-9\nArithmetic:Base-11\nArithmetic:Base-16\nChess:Original\nChess:Counterfactual\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nManual Initialization\nIterative APE\nAPO\nPE2\nSyntax:SVO\nSyntax:SOV\nSyntax:OSV\nSyntax:OVS\nSyntax:VOS\nSyntax:VSO\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nFigure 12: Results on Counterfactual Eval (Manual Initialization).\n21\nboolean_expressions\ncausal_judgement\ndate_understanding\ndisambiguation_qa\ndyck_languages\nformal_fallacies\ngeometric_shapes\n0.00\n0.25\n0.50\n0.75\n1.00\nAccuracy\nManual Initialization\nIterative APE\nAPO\nPE2\nhyperbaton\nlogical_deduction_five_objects\nlogical_deduction_seven_objects\nlogical_deduction_three_objects\nmovie_recommendation\nmultistep_arithmetic_two\nnavigate\n0.00\n0.25\n0.50\n0.75\n1.00\nAccuracy\nobject_counting\npenguins_in_a_table\nreasoning_about_colored_objects\nruin_names\nsalient_translation_error_detection\nsnarks\nsports_understanding\n0.00\n0.25\n0.50\n0.75\n1.00\nAccuracy\ntemporal_sequences\ntracking_shuffled_objects_five_objects\ntracking_shuffled_objects_seven_objects\ntracking_shuffled_objects_three_objects\nweb_of_lies\nword_sorting\n0.00\n0.25\n0.50\n0.75\n1.00\nAccuracy\nFigure 13: Results on the BIG-bench Hard (Suzgun et al., 2023).\n22\nTask\nt\nPrompt\nDev Acc.\nCorrect wrong or incomplete task instructions\nAntonyms\n0\nWrite the opposite of the given word by adding an appropriate prefix.\n0.3\n1\nFind the opposite of the given word. If applicable, add or remove an appropriate prefix to form\nthe opposite.\n0.6\nProvide more specific context and details\nSecond Word Letter\n0\nFind the second letter in each word.\n0.9\n1\nIdentify the second character in the provided word.\n0.95\n2\nIdentify the second character from the start of the given word.\n1.0\nSentence Similarity\n0\nRate the similarity between Sentence 1 and Sentence 2 on a scale from 1 to 5, with 1 being\n\u2019probably not similar\u2019 and 5 being \u2019perfectly similar\u2019.\n0.0\n1\nRate the similarity between Sentence 1 and Sentence 2 as \u20191 - probably not similar\u2019, \u20192 -\npossibly\u2019, \u20193 - moderately\u2019, \u20194 - almost perfectly\u2019, or \u20195 - perfectly similar\u2019.\n0.15\nLay out tailored multi-step plans for complex problems\nDate\nUnderstanding\n0\nLet\u2019s think step by step.\n0.39\n2\nAnalyzing the given information, let\u2019s calculate the solution. Remember to consider the context\nprovided, such as references to \u2019today\u2019 or specific dates.\n0.54\nProduce short-cut solutions in counterfactual tasks\nBase-9 Addition\n(Induction Init.)\n0\nAdd the numbers in each input together to get the output.\n0.0\n1\nAdd the numbers in each input together and then add 11 to get the output.\n0.2\nTable 14: Notable prompt edits made by PE2 (Part 2).\nG\nPrompt Optimization Results\nSee Table 15-19.\n23\nTable 15: Prompts find by prompt optimization methods on math reasoning and instruction induction tasks. For\ninstruction induction, experiments were run with 5 random data splits; In this table we report the prompts found in\none run (seed=0).\nTask\nMethod\nPrompt\nMath Reasoning\nMultiArith\nZero-shot CoT\nLet\u2019s think step by step.\nAPE\nLet\u2019s work this out in a step by step way to be sure we have the right answer.\nIterative APE\nLet\u2019s proceed in a methodical, step-by-step manner.\nAPO\nGiven the scenario, perform the necessary calculations step by step to find the final result. Consider all parts\nof the input and the sequence of events.\nPE2\nLet\u2019s solve this problem by considering all the details. Pay attention to each piece of information, remember\nto add or subtract as needed, and perform the calculations step by step.\nGSM8K\nZero-shot CoT\nLet\u2019s think step by step.\nAPE\nLet\u2019s work this out in a step by step way to be sure we have the right answer.\nIterative APE\nLet\u2019s dissect this and tackle it gradually, one phase at a time.\nAPO\nGiven the scenario, perform necessary calculations and provide a step-by-step explanation to arrive at the\ncorrect numerical answer. Consider all information provided.\nPE2\nLet\u2019s solve the problem step-by-step and calculate the required total value correctly.\nInstruction Induction\nantonyms\nAPO\nProvide the opposite or a negative form of the given input word.\nPE2\nProvide the opposite or a negative form of the given input word.\ninformal_to_formal\nAPO\nConvert each sentence into a formal version, preserving the original structure, meaning, and tone. Avoid\nexcessive formality, unnecessary changes, and maintain idiomatic expressions. Handle contractions appro-\npriately.\nPE2\nPlease transform each sentence into a version that maintains the original meaning but is expressed in a more\nformal or polite manner.\nnegation\nAPO\nNegate the statement given in the input.\nPE2\nNegate the statement given in the input.\northography_starts_with\nAPO\nIdentify the word or phrase in the sentence that starts with the given letter, considering the context and\ngrammar. Include articles if they precede the word or phrase.\nPE2\nFind the word or phrase in the sentence that starts with the given letter, and write it as the output.\nrhymes\nAPO\nRemove the first letter of the given word. Find a word that rhymes with the remaining part, has the same\nsyllable count, and is not a derivative or the same as the original word.\nPE2\nGenerate a word that rhymes with the given word.\nsecond_word_letter\nAPO\nIdentify the second character from the start in each input word and provide it as the output.\nPE2\nIdentify the second character from the start of the given word.\nsentence_similarity\nAPO\nRate the similarity between Sentence 1 and Sentence 2 using the scale: 1 - \u2019probably not\u2019, 2 - \u2019possibly\u2019, 3 -\n\u2019probably\u2019, 4 - \u2019likely\u2019, 5 - \u2019perfectly\u2019.\nPE2\nRate the similarity between Sentence 1 and Sentence 2 using the scale: 1 - \u2019probably not\u2019, 2 - \u2019possibly\u2019, 3 -\n\u2019probably\u2019, 4 - \u2019likely\u2019, 5 - \u2019perfectly\u2019.\nsentiment\nAPO\nDetermine if the given movie review statement is positive or negative.\nPE2\nDetermine if the given movie review statement is positive or negative.\nsynonyms\nAPO\nProvide a single word that is closely related to the given input, considering its most common usage.\nPE2\nIdentify a word that is closely connected, in meaning or context, with the provided input word.\ntaxonomy_animal\nAPO\nRemove all items from the list that are not animals.\nPE2\nRemove all items from the list that are not animals.\ntranslation_en-de\nAPO\nTranslate each English word into German.\nPE2\nTranslate each English word into German.\ntranslation_en-es\nAPO\nProvide the most commonly used Spanish translation for the given English word.\nPE2\nTranslate the given term from English to Spanish. Note that the translation may be a single word or a phrase.\ntranslation_en-fr\nAPO\nProvide the French equivalent for the given English word.\nPE2\nTranslate the following word from English to its most common equivalent in French.\nword_in_context\nAPO\nDetermine if the word provided is used in the same sense/context in both sentences. If it is, write \u2019same.\u2019 If\nnot, write \u2019not the same.\u2019\nPE2\nDetermine if the word provided is used in the same sense/context in both sentences. If it is, write \u2019same.\u2019 If\nnot, write \u2019not the same.\u2019\n24\nTable 16: Prompts find by prompt optimization methods on Counterfactual Eval (Wu et al., 2023) using induction\ninitialization (i.e., the model is not informed of the counterfactual situation). Experiments were run with 5 random\ndata splits; In this table we report the prompts found in one run (seed=0).\nTask\nMethod\nPrompt\nCounterfactual Evaluation (Induction Initialization)\narithmetic_base11\nAPO\nGiven two numbers in hexadecimal format (0-9, A-F), convert each number to decimal. Add the two decimal\nnumbers together. Output the sum in hexadecimal format. If the sum exceeds the range of a single hexadecimal\ndigit (0-F), represent it appropriately in hexadecimal. For example, if the input is \u2019A\u2019 and \u2019B\u2019, the output should be\n\u201915\u2019 as \u2019A\u2019 is 10 and \u2019B\u2019 is 11 in decimal, and their sum is 21 which is \u201915\u2019 in hexadecimal.\nPE2\nConvert both numbers in each pair from hexadecimal to decimal, then add them together. Output the resultant sum\nin hexadecimal. For instance, if the input is A4+61, convert A4 and 61 to decimal (164 and 97 respectively), add\nthem together to get 261, and convert this back to hexadecimal to get 105.\narithmetic_base16\nAPO\nGiven two hexadecimal numbers as input, add them together using base 16 arithmetic. The input hexadecimal\nnumbers will be in uppercase and may have different number of digits. Align the numbers from right to left, similar\nto traditional addition, and handle any overflow or carry appropriately. Output the sum as an uppercase hexadecimal\nnumber.\nPE2\nAdd the input hexadecimal numbers together and output the sum as a hexadecimal number. For example, if the input\nis \"44+E7\", the output should be \"12B\", because the sum of hexadecimals 44 and E7 equals 12B in hexadecimal.\narithmetic_base8\nAPO\nGiven an input string containing two numbers separated by a \u2019+\u2019, calculate the sum of these two numbers. Then,\nadd 20 to this sum to get the output. For example, if the input is \u201922+47\u2019, first add 22 and 47 to get 69, then add\n20 to 69 to get the final output of 89. Similarly, if the input is \u201974+26\u2019, first add 74 and 26 to get 100, then add 20\nto 100 to get the final output of 120. The \u2019+\u2019 symbol should be interpreted as an addition operator, and the order\nof operations should be to add the two numbers first, then add 20 to the sum. The input will always be formatted\ncorrectly, with no spaces or other characters around the \u2019+\u2019 symbol.\nPE2\nTo find the correct output, first add the two numbers given as input. Once you have the sum of these two numbers,\nadd an additional 22 to this sum. For example, if the input is \"17+65\", you should first add 17 and 65 to get 82, then\nadd 22 to 82. The correct output in this case would be 104.\narithmetic_base9\nAPO\nAdd the numbers together.\nPE2\nAdd the two numbers given as input and then add 10 to the result to generate the output. For example, if the input is\n\u201925+18\u2019, the output should be \u201953\u2019 because 25 plus 18 equals 43, and adding 10 gives 53.\nchess_cf\nAPO\nDetermine if the given sequence of chess moves, starting from the initial game position, is legal or not according to\nthe standard rules of chess. Consider the unique movements and restrictions of each piece, the alternating turns of\nthe players (white and black), and the entire game state up to the given point. Evaluate the sequence as a whole, not\njust individual moves. Note that the sequence ends with an asterisk (*).\nPE2\nPlease assess the legality of the following sequence of chess moves based on standard chess rules. If all moves\nare valid according to the rules of chess, indicate \"Legal.\" If there is any move that violates standard chess rules,\nrespond with \"Illegal\". For example, if the sequence is \"1. e4 e5 2. Nf3 d6\", your response should be \"Legal\". If the\nsequence is \"1. e4 e5 2. Kf2\", your response should be \"Illegal\" because the king cannot be exposed to check.\nchess_original\nAPO\nDetermine if the given sequence of chess moves is legal or illegal.\nPE2\nDetermine if the given sequence of chess moves is legal or illegal.\nsyntax_osv\nAPO\nIdentify the main subject and verb in the sentence. The subject should be a proper noun directly associated with the\nmain verb. Focus on the main clause that conveys the primary information. If the sentence is complex, extract the\nsubject and verb from the primary clause. For compound verbs or verb phrases, include only the main verb, not\nauxiliary verbs. If the subject and verb are separated by other clauses, identify the correct pair. If the subject is\nimplied, make a reasonable guess. Write the subject and verb as a pair in the output.\nPE2\nIdentify the subject and verb at the end of the sentence. The subject may not always be a proper noun. The verb\nshould be in the present tense. Write them out as a pair in the output. For example, in the sentence \u2019The market was\nsupported by gains on Wall Street, dealers said\u2019, the output should be \u2019dealers, said\u2019.\nsyntax_ovs\nAPO\nIdentify the first instance of a subject in the sentence, which could be a pronoun (\u2019he\u2019, \u2019she\u2019, \u2019it\u2019, \u2019they\u2019, \u2019we\u2019,\netc.) or a noun/noun phrase. Find the verb that is associated with this subject, considering the sentence\u2019s structure,\nintervening phrases, and possible verb phrases. The verb may not directly follow the subject and could precede it. If\nthe sentence is in passive voice, identify the verb associated with the subject. In cases of multiple subjects, focus on\nthe verb related to the first subject. If the subject is part of a prepositional phrase, consider the verb that the phrase\nis modifying. Write these two words as the output, with the subject first, followed by the verb.\nPE2\nIdentify the first personal pronoun in the sentence and find the verb that is semantically linked to it. Write these\ntwo words as your output. For instance, in the sentence \u2019They believe technology is their best bet\u2019, the words to be\nidentified are \u2019they believe\u2019, not \u2019they is\u2019, as \u2019believe\u2019 is semantically linked to \u2019they\u2019.\nsyntax_sov\nAPO\nIdentify the main subject and the main verb in the sentence. Consider the overall context, complex sentence\nstructures, conjunctions, passive voice, and sentences with multiple clauses. Output the main subject and the main\nverb together, as they appear in the input. The main subject is the one that the main action of the sentence revolves\naround, and the main verb is the primary action or state of being that the subject is performing or experiencing.\nPE2\nIdentify the subject and the main verb in the sentence and write them together in the same order as they appear in\nthe sentence, excluding any additional words in between. The subject generally denotes the \"doer\" of the action or\nthe one it is happening to. The main verb expresses the action or state of being. For instance, in \"The cat sat on the\nmat\", the subject is \"The cat\" and the main verb is \"sat\". So, the output should be \"The cat sat\". Ensure the subject\nand main verb are directly linked without extra words. For example, in \"dealers said\", \"dealers\" is the subject and\n\"said\" is the verb, forming \"dealers said\".\nContinued on next page\n25\nTask\nMethod\nPrompt\nsyntax_svo\nAPO\nYour task is to identify the subject and the main verb of the primary clause in an input sentence. Start from the\nbeginning of the sentence and identify the first subject-verb pair. Ignore auxiliary verbs and focus on the main verb\nthat drives the action. If the sentence has multiple clauses, focus on the first one that forms a complete thought. Do\nnot include any intervening words or phrases between the subject and verb. In case of compound verbs, include\nthe verb that is most integral to the action. Ignore prepositional phrases and do not include any implied subjects or\nverbs. Your output should be concise, containing only the subject and the main verb.\nPE2\nRead the input sentence and identify the subject and the verb of the main clause. Your output should exclude any\nauxiliary verbs, objects, or additional details from the sentence. For example, if the input is \"John is eating an\napple\", the output should be \"John eating\", not \"John is eating\" or \"John eating apple\".\nsyntax_vos\nAPO\nIdentify the first and last words of each sentence, considering a sentence as a group of words that starts with a\ncapital letter and ends with a period, question mark, or exclamation point. Ignore any punctuation, numbers, and\nconjunctions/prepositions at the beginning or end of the sentence. Write these two words in reverse order. If the\nsentence begins and ends with the same word, write it once. Treat compound words or phrases as single words. For\nexample, \u2019uniroyal\u2019 and \u2019has\u2019 should be treated as \u2019uniroyal has\u2019.\nPE2\nIdentify the main subject and verb in each input sentence and form a pair. The subject is usually a noun or pronoun\nthat the verb refers to. The verb should be the main verb of the sentence, not an auxiliary verb. For example, if the\ninput is \"The cat chased the mouse.\", the output should be \"cat chased\". If the input is \"She has eaten the cake.\", the\noutput should be \"She eaten\", not \"She has\".\nsyntax_vso\nAPO\nIdentify the main subject and the primary verb in the given sentence, regardless of their position or the complexity\nof the sentence. Construct a new sentence using only these two words, maintaining the order \u2019subject verb\u2019. Ignore\nadditional information, context, or implied subjects/verbs. If the subject and verb are separated by parenthetical\nelements, conjunctions, or other grammatical structures, still identify them as the main subject and verb. Your task\nis to simplify the sentence to its most basic \u2019subject verb\u2019 form.\nPE2\nIdentify the main subject and the corresponding verb in the given sentence and construct a new short sentence using\nonly these two words. The order should be \u2019subject verb\u2019. For example, in the sentence \"The dog barked at the\nmailman\", the main subject is \u2019dog\u2019 and the corresponding verb is \u2019barked\u2019. So, the new sentence would be \"Dog\nbarked\".\nTable 17: Prompts find by prompt optimization methods on Counterfactual Eval (Wu et al., 2023) using manual\ninitialization. Experiments were run with 5 random data splits; In this table we report the prompts found in one run\n(seed=0).\nTask\nMethod\nPrompt\nCounterfactual Evaluation (Manual Initialization)\narithmetic_base11\nManual Init.\nYou are a mathematician. Assuming that all numbers are in base-11 where the digits are 0123456789A, compute\nthe sum of the following two numbers.\nAPO\nYou are a mathematician. Assuming that all numbers are in base-11 where the digits are 0123456789A, compute\nthe sum of the following two numbers.\nPE2\nYou are a mathematician. Assuming that all numbers are in base-11 where the digits are 0123456789A, compute\nthe sum of the following two numbers.\narithmetic_base16\nManual Init.\nYou are a mathematician. Assuming that all numbers are in base-16 where the digits are 0123456789ABCDEF,\ncompute the sum of the following two numbers.\nAPO\nYou are a mathematician working with base-16 (hexadecimal) numbers. The digits are 0123456789ABCDEF, where\n\u2019A\u2019 to \u2019F\u2019 represent 10 to 15 respectively. Add the two given hexadecimal numbers. If the sum of two digits exceeds\n15, carry the excess to the next higher digit. For instance, \u2019F\u2019 + \u20192\u2019 equals \u201911\u2019 in base-16, which is \u20191\u2019 with a\ncarryover of \u20191\u2019. The input will be two hexadecimal numbers separated by a \u2019+\u2019. The output should be the sum in\nbase-16.\"\nPE2\nAs a base-16 mathematician, your task is to add the provided hexadecimal numbers together. In hexadecimal system,\ndigits go from 0 to F, with A to F representing 10 to 15 respectively. For example, to add \u2019B7\u2019 and \u20195B\u2019, convert\nthem to decimal first: \u2019B7\u2019 becomes 183 and \u20195B\u2019 becomes 91. Their sum, 274, is \u2019112\u2019 in hexadecimal.\narithmetic_base8\nManual Init.\nYou are a mathematician. Assuming that all numbers are in base-8 where the digits are 01234567, compute the sum\nof the following two numbers.\nAPO\nYou are a mathematician specializing in the octal (base-8) number system. Your task is to add two octal numbers\nand provide the result in octal form. In base-8, when the sum of two digits is 8 or more, you carry the value to the\nnext higher place. For example, 7+1 in base-8 is 10. Here are some examples:\nPE2\nAs a mathematician, your task is to add the following two numbers which are represented in base-8 (octal) format.\nThe base-8 system uses digits from 0 to 7. Please ensure you compute the sum correctly by using base-8 arithmetic,\nnot base-10. For example, in base-8, 7+1 equals 10, not 8. Compute the base-8 sum of these numbers, ensuring that\nyour answer matches the provided label. For instance, if the input is \"25+55\", the correct output would be \"102\".\nNow, compute the base-8 sum of these numbers:\narithmetic_base9\nManual Init.\nYou are a mathematician. Assuming that all numbers are in base-9 where the digits are 012345678, compute the\nsum of the following two numbers.\nAPO\nYou are a mathematician working with base-9 numbers, where digits range from 0 to 8. Your task is to add two\nbase-9 numbers. If the sum of two digits exceeds 8, carry the excess to the next higher place value, similar to\nbase-10 arithmetic. For instance, \u20198+1\u2019 in base-9 equals \u201910\u2019. It\u2019s crucial to interpret and present all numbers,\nincluding the final sum, in base-9. For example, if you\u2019re adding \u201916\u2019 and \u201924\u2019 in base-9, the correct sum is \u201941\u2019, not\n\u201940\u2019. Now, compute the sum of the following two base-9 numbers.\nContinued on next page\n26\nTask\nMethod\nPrompt\nPE2\nYou are a mathematician. Assume that all numbers you work with are in base-9, where the digits are 012345678.\nYour task is to add the following two numbers together, but remember to carry over any value that equals or exceeds\n9 to the next digit, as is the rule when adding in base-9. For example, if you have to add 8 and 2 in base-9, the result\nwould be 11 because 10 is not a valid number in base-9. Now, compute the sum of the following two numbers.\nchess_cf\nManual Init.\nYou are a chess player. You are playing a chess variant where the starting positions for knights and bishops are\nswapped. For each color, the knights are at placed that where bishops used to be and the bishops are now placed at\nwhere knights used to be. Given an opening, determine whether the opening is legal. The opening doesn\u2019t need to\nbe a good opening. Answer \"legal\" if all moves are legal. Answer \"illegal\" if the opening violates any rules of chess.\nAPO\nYou are evaluating a chess variant where knights and bishops have swapped starting positions. Knights are placed\nwhere bishops usually start, and bishops are placed where knights usually start. However, their movement rules\nremain the same: knights move in an L-shape and bishops move diagonally. Your task is to determine the legality\nof a given opening. An opening is \u2019legal\u2019 if all moves comply with the standard rules of chess, considering the\nswapped starting positions. If all moves are legal, answer \u2019legal\u2019. If any move violates the chess rules, answer\n\u2019illegal\u2019. The opening doesn\u2019t need to be a good strategy, it just needs to be legal.\nPE2\nYou are a chess enthusiast, playing a variant of the game where knights and bishops have swapped their starting\npositions and movements. The knights, now placed where the bishops were, move as bishops. The bishops,\npositioned where knights were, move as knights. Your task is to assess the legality of a given opening, irrespective\nof its strategic soundness. Consider only the unique rules of this chess variant: If all moves are in accordance with\nthese rules, your response should be \"legal\". However, if any move contravenes these rules, respond with \"illegal\".\nFor instance, if a sequence begins with \u2019Bf6\u2019, it would be illegal since a bishop (moving like a knight in this variant)\ncannot reach \u2019f6\u2019 on its first move.\nchess_original\nManual Init.\nYou are a chess player. Given an opening, determine whether the opening is legal. The opening doesn\u2019t need to be a\ngood opening. Answer \"legal\" if all moves are legal. Answer \"illegal\" if the opening violates any rules of chess.\nAPO\nYou are a chess expert. Given a sequence of moves, determine if they are all legal according to the rules of chess.\nConsider the type of piece, its legal moves, the turn order, and whether the king is put in check by its own player.\nIf all moves are legal, answer \"legal\". If any move violates the rules of chess, answer \"illegal\". Remember, the\nopening doesn\u2019t need to be a good one, it just needs to follow the rules of chess.\nPE2\nAs a chess expert, your task is to examine the given opening sequence in a chess game and determine if it adheres to\nthe official rules of chess. Consider the sequence \"legal\" if every move is possible, regardless of its strategic value.\nHowever, if any move breaks a chess rule, such as moving a piece in a way it is not allowed (e.g., a knight moving\nlike a bishop), classify the sequence as \"illegal\". Your response should be one of two words: \"legal\" or \"illegal\".\nsyntax_osv\nManual Init.\nYou are an expert in linguistics. Imagine a language that is the same as English with the only exception being that it\nuses the object-subject-verb order instead of the subject-verb-object order. Your task is to identify the main verb and\nthe main subject in a sentence in this imaginary language. Show the main verb (a single word) and its subject (also a\nsingle word).\nAPO\nYou are a linguistics expert. Your task is to identify the main verb and subject in a sentence of a language identical\nto English, but with an object-subject-verb order. The main verb is the primary action word, excluding auxiliary\nverbs. The main subject is the primary entity performing the action. In complex sentences, focus on the main clause.\nIf the main subject or verb is a phrase, identify the key word that encapsulates the action or entity. If the main\nsubject or verb is a proper noun, treat it as a single word. Your output should be a phrase consisting of the main\nsubject and verb. For example, if the sentence is \u2019a milk for hispanic tastes goya concocts\u2019, your output should be\n\u2019goya concocts\u2019.\nPE2\nAs a linguistics expert, your task is to analyze sentences from a language that, while similar to English, employs an\nobject-subject-verb order instead of the English subject-verb-object order. You need to identify the primary subject,\nwho is the main entity carrying out the action, and the last verb, which is the final action described in the sentence.\nOutput the main subject and the last verb in a single word each, and arrange them in the English order. For instance,\nfor \"apple the eats boy\", your output should be \"boy eats\". Similarly, for sentences like \"$ 4 million it will pay\nhunter in exchange for agreements not to compete cilcorp said\", the response should be \"cilcorp said\", recognizing\n\u2019cilcorp\u2019 as the main subject and \u2019said\u2019 as the last verb.\nsyntax_ovs\nManual Init.\nYou are an expert in linguistics. Imagine a language that is the same as English with the only exception being that it\nuses the object-verb-subject order instead of the subject-verb-object order. Your task is to identify the main verb and\nthe main subject in a sentence in this imaginary language. Show the main verb (a single word) and its subject (also a\nsingle word).\nAPO\nYou are a linguistics expert analyzing a language similar to English, but with an object-verb-subject (OVS) order.\nYour task is to identify the main verb and the main subject in a sentence. The main verb is the primary action word,\nand the main subject is the primary doer of the action. They may not always be adjacent. If the main verb or subject\nis a compound or phrase, choose the most significant word. For sentences with auxiliary verbs, the main verb is the\none conveying the primary action. After identifying, reverse the order to subject-verb for your output. For example,\nif the OVS order is \u2019apple ate John\u2019, your output should be \u2019John ate\u2019. Remember, your output should always be in\nsubject-verb order.\nPE2\nYou are an expert in linguistics. Imagine a language that is the same as English with the only exception being that it\nuses the object-verb-subject order instead of the subject-verb-object order. Your task is to identify the last subject\nand the verb directly associated with this subject in a sentence in this imaginary language. Show the subject first\n(a single word) and then the verb (also a single word). For example, in the sentence \"interest pay they only for\n115 months , with principal payments beginning thereafter\", though the last verb is \"beginning\", the verb directly\nassociated with the subject \"they\" is \"pay\". Therefore, the answer is \"they pay\".\nsyntax_sov\nManual Init.\nYou are an expert in linguistics. Imagine a language that is the same as English with the only exception being that it\nuses the subject-object-verb order instead of the subject-verb-object order. Your task is to identify the main verb and\nthe main subject in a sentence in this imaginary language. Show the main verb (a single word) and its subject (also a\nsingle word).\nContinued on next page\n27\nTask\nMethod\nPrompt\nAPO\nYou are a linguistics expert. Your task is to identify the main subject and the main verb in a sentence of an imaginary\nlanguage identical to English, but with a subject-object-verb order. Your output should be in the original English\norder (subject-verb). Choose the most crucial word if the subject or verb is a phrase. Ignore auxiliary verbs,\nadditional clauses, prepositional phrases, and implied words. Your output should be two single words: the main\nsubject and the main verb. For instance, in the sentence \u2019John the ball threw\u2019, your output should be \u2019John threw\u2019.\nIn complex sentences, focus on the primary clause. For example, in \u2019that speculators a higher offer is in the wings\nare betting indicates\u2019, your output should be \u2019that indicates\u2019.\nPE2\nAs a linguistics expert, consider an alternate version of English that uses the subject-object-verb order instead of\nthe traditional subject-verb-object order. Given a sentence in this alternate order, your task is to identify the main\nsubject and the main verb and present them in the order of subject-verb. Please provide the main subject (one word)\nand its verb (one word) in each sentence, without considering the object. For instance, in the sentence \"Jane the\napple ate\", \"Jane\" is the subject and \"ate\" is the verb. Therefore, the answer would be \"Jane ate\".\nsyntax_svo\nManual Init.\nYou are an expert in linguistics. Your task is to identify the main verb and the main subject in a sentence. Show the\nmain verb (a single word) and its subject (also a single word).\nAPO\nYou are a language analyst. Your task is to identify the primary subject and the primary verb in a sentence, in the\norder they appear. The primary subject is the main entity performing the action, and the primary verb is the main\naction performed by the subject. They should be part of the same clause. In complex sentences, focus on the main\naction and the entity performing it, considering the overall context. If there are multiple verbs or subjects, choose\nthe pair that is most central to the sentence\u2019s meaning. Ignore conjunctions, prepositions, or other linking words that\nmight separate the primary subject from the primary verb. If the primary subject or verb is implied, infer it from the\ncontext. Provide the primary subject and verb as a single output, with the subject first and the verb second. Both\nshould be single words. Do not include punctuation in your output.\nPE2\nAs a linguistics expert, your task is to determine the main verb and the main subject in a given sentence. Identify\nthem as a single word each. The subject usually is the one performing the action, while the verb represents the\naction or the state of the subject. For instance, in the sentence \"John plays football\", \u2019John\u2019 is the subject, and \u2019plays\u2019\nis the verb. Please provide the subject first, followed by the verb.\nsyntax_vos\nManual Init.\nYou are an expert in linguistics. Imagine a language that is the same as English with the only exception being that it\nuses the verb-object-subject order instead of the subject-verb-object order. Your task is to identify the main verb and\nthe main subject in a sentence in this imaginary language. Show the main verb (a single word) and its subject (also a\nsingle word).\nAPO\nYou are a linguistics expert. Your task is to identify the main verb and subject in a sentence of a language identical\nto English, but with verb-object-subject order. Focus on the verb and subject that carry the main action or idea. If\nthere are multiple verbs or subjects, choose the ones that are most central to the sentence\u2019s meaning. If the verb or\nsubject is part of a complex structure or is implied, state it explicitly. If the verb or subject is a phrase, identify the\nentire phrase. Your output should be in the format: \u2019Subject Verb\u2019. Remember, the subject and verb may not be\nadjacent or single words. Use your linguistic expertise to determine the main verb and subject.\nPE2\nYou are a linguistics expert tasked with analyzing sentences in a language similar to English but with a key difference:\nthe order of the verb, object, and subject is changed. Your task is to identify the main subject and the first word of\nthe verb phrase in each sentence. However, present your answer in the subject-verb-object order commonly used in\nEnglish. In other words, reveal the main subject (a single word) followed by the first word of the verb phrase (also\na single word). For example, if the sentence is \"continue to lead gold stocks and utilities , may signal that is the\nmarket in for rough times it\", your answer should be \"it signal\".\nsyntax_vso\nManual Init.\nYou are an expert in linguistics. Imagine a language that is the same as English with the only exception being that it\nuses the verb-subject-object order instead of the subject-verb-object order. Your task is to identify the main verb and\nthe main subject in a sentence in this imaginary language. Show the main verb (a single word) and its subject (also a\nsingle word).\nAPO\nYou are a language expert analyzing a unique language similar to English, but with verb-subject-object order. Your\ntask is to identify the main verb and subject in a sentence. The main verb is the key action, and the main subject is\nwho or what is doing this action. In complex sentences, focus on the most important action. If multiple verbs or\nsubjects exist, choose the most central to the sentence\u2019s meaning. Treat auxiliary or compound verbs as one unit\nwith their main verb. Your output should be the main subject followed by the main verb (both as single words).\"\nPE2\nAs a linguistics expert, consider an alternative English language that uses verb-subject-object order instead of the\nstandard subject-verb-object order. Your task is to identify the main subject and the main verb in a sentence in\nthis imaginary language. Display the main subject (a single word) followed by its verb (also a single word). For\ninstance, if the input is \"compares that with 3.5 % butterfat for whole milk\", the output should be \"that compares\".\nSimilarly, for \"believe they is technology one of their best bets\", the output should be \"they believe\".\nTable 18: Prompts find by prompt optimization methods on BIG-bench Hard tasks. Here we use the task formats in\n(Suzgun et al., 2023). In these experiments, we use gpt-3.5-turbo-instruct as the task model and gpt-4 as the\nprompt proposal model. See Table 19 when alternative formats are used.\nTask\nMethod\nPrompt\nBIG-bench Hard (Manual Initialization - \u201cLet\u2019s think step by step.\u201d)\nboolean_expressions\nIterative APE\nConsider approaching this methodically, one step at a time.\nAPO\nFirst, resolve \u2019not\u2019 immediately before it. Then evaluate expressions within parentheses. Next, perform \u2019and\u2019\noperations, and finally \u2019or\u2019. Determine the truth value step by step.\nPE2\nEvaluate the logical expression step by step following precedence: \u2019not\u2019, \u2019and\u2019, \u2019or\u2019. Conclude by affirming the\ntruth of the full statement.\ncausal_judgement\nIterative APE\nApproach each stage sequentially.\nContinued on next page\n28\nTask\nMethod\nPrompt\nAPO\nIdentify the direct cause of the outcome: was it the immediate action or condition without which the event\nwouldn\u2019t have occurred?\nPE2\nDetermine if the action was intentional and a contributing factor to the outcome. Answer \u2019Yes\u2019 if intentional and\ncausative, \u2019No\u2019 otherwise.\ndisambiguation_qa\nIterative APE\nConsider each phase one by one.\nAPO\nIn each sentence, determine the pronoun\u2019s antecedent based on typical roles and actions. If unclear, mark as\nambiguous.\"\nPE2\nLet\u2019s think step by step, using grammatical structure and context to identify the antecedent of the pronoun or\ndetermine if it is ambiguous.\ndate_understanding\nIterative APE\nConsider the process in a sequential manner.\nAPO\nLet\u2019s think step by step.\nPE2\nDetermine the date referred to as \u2019today\u2019 from the context given, then calculate the specified date step by step in\nMM/DD/YYYY format.\ndyck_languages\nIterative APE\nLet\u2019s think step by step.\nAPO\nOutput the sequence of closing brackets needed to properly close all open brackets in the given order: parentheses\n\u2019()\u2019, square \u2019[]\u2019, curly \u2019\u2019, and angle \u2019<>\u2019. Ignore all other characters.\nPE2\nGiven an incomplete sequence of brackets, output only the missing closing brackets required to complete the\nsequence. Exclude any part of the original sequence.\nformal_fallacies\nIterative APE\nConsider each phase one at a time.\nAPO\nDetermine if the conclusion necessarily follows from the premises using deductive logic. Identify any logical\nfallacies or overlooked counterexamples. Is the argument valid or invalid?\nPE2\nTo assess if an argument is deductively valid, examine if the conclusion logically follows from the premises\nwithout exception.\ngeometric_shapes\nIterative APE\nConsider our actions in a sequential manner.\nAPO\nClassify the shape formed by the SVG path\u2019s \u2019d\u2019 attribute, analyzing vertices, line segments, and closure to\ndistinguish polygons and figures.\nPE2\nAnalyze the SVG path commands to determine the shape they draw. Consider each command and visualize the\npath step by step.\nhyperbaton\nIterative APE\nApproach this gradually, step by step.\nAPO\nChoose the sentence with adjectives in the correct order: opinion, size, age, shape, color, origin, material,\npurpose, noun.\"\nPE2\nLet\u2019s think step by step, considering the standard order of adjectives in English: opinion, size, age, shape, color,\norigin, material, purpose.\nlogical_deduction\n_five_objects\nIterative APE\nLet\u2019s proceed with a methodical, step-by-step approach.\nAPO\nLet\u2019s think step by step.\nPE2\nLet\u2019s think step by step.\nlogical_deduction\n_seven_objects\nIterative APE\nConsider each phase one at a time.\nAPO\nArrange seven objects in order using the clues provided. Identify the correct position for each object from the\noptions listed, ensuring logical consistency.\"\nPE2\nLet\u2019s analyze the information provided step by step, ensuring each deduction follows logically from the statements\ngiven to arrive at the correct answer.\nlogical_deduction\n_three_objects\nIterative APE\nLet\u2019s proceed with a methodical, stepwise approach.\nAPO\nLet\u2019s think step by step.\nPE2\nLet\u2019s carefully analyze the information to determine the accurate ranking of objects based on their attributes.\nmultistep_arithmetic\n_two\nIterative APE\nApproach this step-by-step, tackling each phase sequentially.\nAPO\nLet\u2019s think step by step.\nPE2\nLet\u2019s meticulously solve the math problem by simplifying each part of the equation, verifying our calculations at\nevery step before moving forward.\nmovie\n_recommendation\nIterative APE\nConsider each phase methodically.\nAPO\nSelect the movie that best matches the reference in terms of genre and cultural significance, ignoring minor\ndetails. Choose the closest match:\"\nPE2\nEvaluate the given movies for genres, themes, narrative styles, tone, and characters to select the most similar one\nfrom the options.\nnavigate\nIterative APE\nLet\u2019s carefully assess each stage of our plan in sequence.\nAPO\nCalculate the net distance from the starting point after following these step-by-step instructions, considering\n\u2019forward\u2019 as positive and \u2019backward\u2019 as negative movement.\nPE2\nLet\u2019s calculate each movement\u2019s effect on our position. Start at zero and add or subtract steps as instructed,\nconsidering the direction each time to ensure accuracy.\nContinued on next page\n29\nTask\nMethod\nPrompt\nobject_counting\nIterative APE\nLet\u2019s continue by taking systematic, sequential steps.\nAPO\nLet\u2019s think step by step.\nPE2\nLet\u2019s identify and count the instances of the specified category of items mentioned, tallying multiples, to\ndetermine their total quantity.\npenguins_in_a_table\nIterative APE\nConsider the procedure progressively.\nAPO\nLet\u2019s think step by step.\nPE2\nLet\u2019s think step by step.\nreasoning_about\n_colored_objects\nIterative APE\nConsider each phase methodically.\nAPO\nLet\u2019s think step by step.\nPE2\nLet\u2019s think step by step and pay close attention to details such as colors and quantities.\nruin_names\nIterative APE\nLet\u2019s proceed with our tasks one by one.\nAPO\nIdentify the funniest edit of the given name by selecting the option that best incorporates a pun or playful twist\nrelated to the original.\nPE2\nIdentify the most humorous edit by considering only puns or clever wordplay that creates a witty variation of the\noriginal name, excluding misspellings or simple pluralizations.\nsalient_translation\n_error_detection\nIterative APE\nLet\u2019s take this gradually, step by step.\nAPO\nLet\u2019s think step by step.\nPE2\nLet\u2019s analyze the source and translation for errors. Check if there are any changes or inaccuracies in Named\nEntities, Numerical Values, Modifiers, Negation, Facts, or missing details. Identify which type of error occurs.\nsnarks\nIterative APE\nEvaluate each stage sequentially.\nAPO\nIdentify the sarcastic statement by considering the reversal of expectations and societal norms. Look for irony\nand implied meanings contrary to the literal words.\nPE2\nTo identify which statement is sarcastic, consider that sarcasm often means saying the opposite of what\u2019s true in\na mocking way.\nsports_understanding\nIterative APE\nLet\u2019s think step by step.\nAPO\nDetermine if the sentence is plausible: Match the athlete\u2019s known sport, current activity status, and sport-specific\nterms to assess accuracy in context.\nPE2\nLet\u2019s think step by step.\ntemporal_sequences\nIterative APE\nConsider each stage methodically.\nAPO\nIdentify the time period when the person was not seen and the location was open. Exclude times when the person\nwas observed elsewhere or the location was closed.\nPE2\nAnalyze the timeline to pinpoint time slots when the individual was not seen, which indicate when events could\nhave occurred.\ntracking_shuffled\n_objects_five_objects\nIterative APE\nWe\u2019ll tackle this systematically, one stage at a time.\nAPO\nTrack ball swaps and position changes separately. List each swap, update positions and ball ownership after\neach, and determine final states for both.\nPE2\nLet\u2019s carefully track each player\u2019s position swaps step by step to determine their final positions.\ntracking_shuffled\n_objects_seven_objects\nIterative APE\nApproach each stage with systematic thought.\nAPO\nLet\u2019s think step by step.\nPE2\nLet\u2019s carefully track each book exchange step by step to determine the final owner of each book.\ntracking_shuffled\n_objects_three_objects\nIterative APE\nReflect on each stage individually.\nAPO\nLet\u2019s think step by step.\nPE2\nLet\u2019s analyze each position swap in sequence to determine the final positions. Confirm the last known positions\nof all players before concluding.\nweb_of_lies\nIterative APE\nLet\u2019s proceed with a methodical, stepwise approach.\nAPO\nGiven a sequence of people\u2019s statements about others\u2019 truthfulness, determine the truth status of the final person.\nAssume the first statement\u2019s truth is known. Apply logical negation for each liar\u2019s statement.\nPE2\nTo determine who is truthful, invert the claim of a liar and trust a truthful person\u2019s claim. Apply this logic until\nthe last claim.\nword_sorting\nIterative APE\nProgress through each stage sequentially.\nAPO\nSort words alphabetically, ignoring case. Exclude \u2019List:\u2019 label. For same first letters, sort remaining letters.\nOutput in lowercase.\nPE2\nLet\u2019s analyze and sort words. Ignore words that are part of instructions, like \"List:\", and then arrange the\nremaining words in alphabetical order.\n30\nTable 19: Results on Date Understanding and Movie Recommendation from BIG-bench Hard (Suzgun et al., 2023).\nIn these experiments, we use a format different from those in (Suzgun et al., 2023). See \u00a7A.2 for detailed discussion\non the effect of task format. We use gpt-3.5-turbo-instruct as the task model and gpt-4 as the prompt proposal\nmodel.\nMethod\nFinal Prompt\nTest Acc.\nDate Understanding (Generative)\nZero-shot CoT\nLet\u2019s think step by step.\n0.391\nIterative APE\nLet\u2019s dissect it and ponder over each phase.\n0.467\nAPO\nDetermine the exact date from the scenario, considering cultural date formats, time zones, and periods.\nUse the provided date as a reference. Account for leading zeros, leap years, relative dates, and event-based\ntime references. Provide the result in MM/DD/YYYY format.\n0.450\nPE2\nAnalyzing the given information, let\u2019s calculate the solution. Remember to consider the context provided,\nsuch as references to \u2019today\u2019 or specific dates.\n0.544\nMovie Recommendation (Multi-choice 2)\nZero-shot CoT\nLet\u2019s think step by step.\n0.570\nIterative APE\nLet\u2019s dissect it and consider every step in order.\n0.673\nAPO\nIdentify the movie that shares the most significant themes and narrative structure with the given movies.\nPrioritize these factors over tone and pacing. Choose the most similar movie from the options, explaining\nyour choice.\n0.750\nPE2\nConsidering factors such as genre, director, actors, release period, audience target, animation style, and\nhumor, analyze the similarities among the given movies and identify the movie from the options that\nshares the most similarities.\n0.790\n31\n"
  },
  {
    "title": "Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization",
    "link": "https://arxiv.org/pdf/2311.06243.pdf",
    "upvote": "17",
    "text": "Technical Report\nPARAMETER-EFFICIENT ORTHOGONAL FINETUNING VIA\nBUTTERFLY FACTORIZATION\nWeiyang Liu1,2,* Zeju Qiu1,* Yao Feng1,3,\u2020 Yuliang Xiu1,\u2020 Yuxuan Xue4,\u2020 Longhui Yu2,\u2020 Haiwen Feng1\nZhen Liu5 Juyeon Heo2 Songyou Peng1,3 Yandong Wen1 Michael J. Black1 Adrian Weller2,6 Bernhard Sch\u00a8olkopf1\n1Max Planck Institute for Intelligent Systems - T\u00a8ubingen\n2University of Cambridge\n3ETH Z\u00a8urich\n4University of T\u00a8ubingen\n5Mila, Universit\u00b4e de Montr\u00b4eal\n6The Alan Turing Institute\nboft.wyliu.com\nABSTRACT\nLarge foundation models are becoming ubiquitous, but training them from scratch\nis prohibitively expensive. Thus, efficiently adapting these powerful models to\ndownstream tasks is increasingly important. In this paper, we study a principled\nfinetuning paradigm \u2013 Orthogonal Finetuning (OFT) \u2013 for downstream task adap-\ntation. Despite demonstrating good generalizability, OFT still uses a fairly large\nnumber of trainable parameters due to the high dimensionality of orthogonal matri-\nces. To address this, we start by examining OFT from an information transmission\nperspective, and then identify a few key desiderata that enable better parameter-\nefficiency. Inspired by how the Cooley-Tukey fast Fourier transform algorithm\nenables efficient information transmission, we propose an efficient orthogonal\nparameterization using butterfly structures. We apply this parameterization to OFT,\ncreating a novel parameter-efficient finetuning method, called Orthogonal Butter-\nfly (BOFT). By subsuming OFT as a special case, BOFT introduces a generalized\northogonal finetuning framework. Finally, we conduct an extensive empirical study\nof adapting large vision transformers, large language models, and text-to-image\ndiffusion models to various downstream tasks in vision and language.\n1\nINTRODUCTION\nRecent models such as ChatGPT [4, 9] and Stable Diffusion [73], demonstrate the remarkable\ngeneralization ability of large foundation models. The rapid increase in the performance of such\nmodels is paired with a dramatic increase in the number of parameters (e.g., GPT-3 has around 175B\nparameters). As a result, it has become increasingly challenging for researchers to train a foundation\nmodel from scratch. Broad progress in the field therefore requires the ability to adapt such models\nwithout retraining them from scratch. That is, we must be able to efficiently adapt existing foundation\nmodels to downstream tasks. There are primarily three ways to perform efficient task adaptation:\nmodel finetuning [6, 23, 29, 67, 69, 92, 97], where the model architecture remains unchanged and a\nsubset of the model parameters gets finetuned; adapter tuning [24, 28, 48, 65, 71], where additional\ntrainable parameters are added to the original model; and prompt tuning [39, 42], where additional\ntrainable prefix tokens are attached to the input. Among these methods, model finetuning distinguishes\nitself as a simple yet powerful approach, and more importantly, introduces no inference latency.\nThe fundamental principle behind model finetuning is to ensure that the pretrained model and the\nfinetuned model are similar based on certain measurements, such that the pretraining knowledge is\npreserved. For instance, current model finetuning methods often adopt a small learning rate. This ad\nhoc approach encourages a small discrepancy between the pretrained and the finetuned model. Given\nthe structured nature of weight matrices, a more principled approach tries to preserve the relational\ninformation of the weight matrices, i.e. the pairwise angles between neurons. This insight leads to a\nnovel model finetuning framework, known as Orthogonal Finetuning (OFT) [67], where neurons in the\nsame layer are transformed by the same orthogonal matrix such that pairwise angles between neurons\nare provably preserved throughout the finetuning process. Although OFT has demonstrated promising\ngeneralizability and convergence for finetuning text-to-image diffusion models [67], the number of\ntrainable parameters in OFT can be quite large due to the high dimensionality of orthogonal matrices.\nTo address this, OFT introduces a block-diagonal structure to reduce the number of parameters.\nHowever, the parameter efficiency also comes at a price \u2013 the orthogonal matrix has a fixed sparsity\npattern and the orthogonal transformation is applied independently in different blocks. This block-\n*Equal first author contribution. \u2020Equal second author contribution. Equal contributors are listed alphabetically.\n1\narXiv:2311.06243v1  [cs.LG]  10 Nov 2023\nTechnical Report\ndiagonal sparsity pattern, despite saving parameters, may introduce some undesirable inductive biases\n(e.g., the block-diagonal orthogonal matrix reduces expressiveness and cannot approximate classic\nlinear transforms), and more importantly, how to find a good sparsity pattern remains a mystery.\nThe key to addressing this problem is to generate a dense orthogonal matrix, while still being\nparameter-efficient. While this may seem infeasible at first glance since a d-dimensional dense\northogonal matrix requires O(d2) parameters, we take a novel route to compose a dense orthogonal\nmatrix with multiple factorized sparse matrices. This approach is guided by the insight that the\nnumber of trainable parameters can be reduced by trading computation time for space. Since we\nrepresent the orthogonal matrix with the multiplication of sparse matrices, the multiplication has to\nbe performed repeatedly in each training iteration. To put the matrix factorization into perspective, we\ninterpret the generation of a dense orthogonal matrix as an information transmission problem. Under\nthis problem formulation, generating a dense orthogonal matrix by multiplying sparse matrices can\nbe understood as transmitting information on a grid-structured graph. This information transmission\nframework enables us to design many possible ways to perform sparse matrix factorization that limit\nthe number of trainable parameters while still being expressive enough to generate dense matrices.\nTo achieve parameter efficiency in our information transmission framework, we draw inspiration\nfrom the butterfly structures in the Cooley-Tukey fast Fourier transform algorithm [12] in which\ninformation from different nodes can be exchanged efficiently [36]. Specifically, the butterfly graph in\nthe Cooley-Tukey algorithm inherently induces a way to perform sparse matrix factorization, called\nbutterfly factorization. With butterfly factorization, we are able to generate a d-dimensional dense\nmatrix with a product of O(log d) sparse matrices, each with O(d) non-zero entries. By ensuring\nthat each sparse matrix is orthogonal, we arrive at an efficient orthogonal parameterization with\nonly O(d log d) parameters, which is a significant reduction from the original parameterization. By\nleveraging such an efficient orthogonal parameterization, we propose a novel parameter-efficient\nfinetuning method \u2013 Orthogonal Butterfly (BOFT). BOFT subsumes OFT as a special case and\nprovides a general orthogonal finetuning framework. There is a shared characteristic for the block-\ndiagonal structure and the butterfly structure \u2013 sparsity. Both structures introduce data sparsity into\northogonal matrices to reduce the effective number of trainable parameters. It is interesting to contrast\nour approach with the low-rank structure in LoRA [29]; both low-rank matrices and sparse matrices\nare major families of structured matrices [5] that achieve parameter efficiency.\nCompared to the block-diagonal structure that OFT uses to trade off expressivity and regularity,\nBOFT uses the butterfly structure to construct a smoother interpolation between matrices from the full\northogonal group (expressivity) and identity matrices (regularity). This enables us to find a smaller\nhypothesis class within the orthogonal group for downstream tasks. Given the widespread use of the\nbutterfly structure in many fast linear transforms, such as the discrete Fourier and discrete cosine\ntransforms, we argue that our structured approach to parameter efficiency will introduce an implicit\ninductive bias that benefits generalizability and prevents overfitting. Our intuition comes from the\nproperty that the butterfly structure can easily recover many classic linear transforms while it is\nimpossible for the block-diagonal structure in OFT to recover any. Our contributions are listed below:\n\u2022 We study the problem of parameter efficiency in orthogonal finetuning with a novel information\ntransmission framework. This framework transforms the task of crafting a parameter-efficient dense\northogonal matrix into an information transmission problem within a grid-structured graph.\n\u2022 Inspired by the butterfly structures in the Cooley-Tukey algorithm, we propose Orthogonal Butterfly,\na parameter-efficient orthogonal finetuning method, within the information transmission framework.\n\u2022 We provide a few theoretical insights for why BOFT is able to significantly reduce the number of\ntrainable parameters while still yielding a good expressivity and training stability. Thanks to the\nmatrix factorization, BOFT also comes with an intriguing weight interpolation (see Figure 10).\n\u2022 For the very first time, we apply orthogonal finetuning to various tasks beyond controllable text-\nto-image generation [67], showing its great potential as a generic model finetuning method. To\nthis end, we apply BOFT to a number of adaptation applications ranging from computer vision to\nnatural language processing. BOFT outperforms current state-of-the-art methods by a considerable\nmargin, validating its superior parameter-efficiency and generalization ability.\n2\nRELATED WORK\nParameter-efficient finetuning (PEFT). As foundation models (e.g., [4, 35, 68, 73]) become\nincreasingly large and powerful, finetunig these models for downstream tasks in a parameter-efficient\n2\nTechnical Report\nway has sparked considerable interest [18, 45, 82]. Among many PEFT methods [1, 3, 8, 10, 19,\n21, 23, 24, 28\u201331, 33, 39, 42, 46, 49, 54, 56, 78, 79, 86, 88, 92, 98], reparameterization-based\nmethods [1, 19, 29] are the most relevant to our work. LoRA [29] updates the pretrained weight\nmatrix by adding a product of two low-rank matrices, achieving promising performance on natural\nlanguage tasks. Since the rank of all added matrices is set to a constant in LoRA, several methods\n[84, 95, 97] dynamically adjust the rank for different layers such that the parameter budget is\nadequately allocated. Due to its simplicity, such a low-rank weight reparameterization has gained great\npopularity [6, 16, 102]. Inspired by how hyperspherical energy characterizes generalization [50, 52],\n[67] proposes orthogonal finetuning, an alternative yet effective method to finetune text-to-image\ndiffusion models. Specifically, OFT learns an orthogonal matrix to transform the neurons of the\nsame layer, and it achieves stronger generalization and consistently more stable training than LoRA.\nDespite strong performance, OFT generally has more trainable parameters than LoRA. Therefore,\nmaking OFT more parameter-efficient is a useful goal. Moreover, whether OFT is applicable to\na wider spectrum of adaptation tasks (beyond controlling text-to-image diffusion models [67]) is\nunknown. BOFT improves the parameter efficiency of OFT via butterfly factorization. Thanks to this,\nwe are now able to demonstrate the power of orthogonal finetuning in general adaptation tasks.\nButterfly structures. The radix-2 Cooley-Tukey algorithm recursively reduces the N-point discrete\nFourier transform to two N\n2 -point discrete Fourier transforms, and this process induces a butterfly\nstructure that can be written as a product of multiple sparse matrices (the product is also called a\nbutterfly matrix). Butterfly matrices have already been used to parameterize orthogonal matrices\nto avoid pivoting in Gaussian elimination and improve efficiency [63], to stabilize the training of\nrecurrent neural networks [32] and in kernel approximation [60]. [13, 14] learn fast linear transforms\nwith butterfly parameterizations. [7, 15] utilize butterfly matrices to enable the efficient training of\nneural networks. Butterfly structures are also found useful in fast matrix-vector multiplication [57, 61],\ndata-sparse matrix approximation [43], and network transmission [36, 40, 70]. In contrast to previous\nwork, we focus on harnessing butterfly structures to enhance the parameter efficiency of OFT.\n3\nAN INFORMATION TRANSMISSION VIEW ON ORTHOGONAL FINETUNING\nPretrained\nWeight Matrix\nW \nd\nn\nx d\n+\nPretrained\nWeight Matrix\nW \nn\nd\n...\nOrthogonal\nMatrx R\nb\nr\nd\nr\nn\nLow-rank Matrix\nAB\n(a) Low-rank Structure in LoRA\n(b) Orthogonal Structure in OFT\nA\nB\n0\n0\nFigure 1: A comparison of reparameterization between LoRA and OFT.\nWe start with some preliminaries\nof OFT. To finetune the pretrained\nweight matrix, OFT reparameter-\nizes the new weight matrix as the\nproduct of a learnable orthogonal\nmatrix and the frozen pretrained\nweight matrix. Compared to LoRA\nwhich updates the weights with an additive low-rank matrix, OFT uses a multiplicative orthogonal\nmatrix to update the weights. To achieve parameter-efficiency, LoRA uses a low-rank structure, and\nin contrast, the original OFT uses a block-diagonal orthogonal structure [67] (the smaller the size of\ndiagonal blocks is, the more parameter-efficient OFT is). An intuitive comparison is given in Figure 1.\nThe motivation for applying orthogonal transformation to finetune the weight matrix is to preserve\nthe pair-wise angles of neurons [50, 52, 67], such that the semantic knowledge from pretraining can\nbe largely preserved. Concretely, OFT optimizes an orthogonal matrix R \u2208 Rd\u00d7d for a pretrained\nlinear layer W 0 \u2208 Rd\u00d7n, and modifies the forward pass from z = (W 0)\u22a4x to z = (RW 0)\u22a4x,\nwhere x \u2208 Rd and z \u2208 Rn are the input and output vector, respectively. To achieve zero initialization,\nOFT initializes R as an identity matrix. To ensure the orthogonality of R throughout the finetuning\nprocess, we follow [52, 67] to employ Cayley parameterization, i.e., R = (I + Q)(I \u2212 Q)\u22121\nwhere Q is a skew-symmetric matrix with Q = \u2212Q\u22a4. For parameter-efficiency, the block-diagonal\nstructure is introduced by parameterizing the orthogonal matrix R as diag(R1, R2, \u00b7 \u00b7 \u00b7 , Rr) where\nRi \u2208 Rb\u00d7b, \u2200i is a small orthogonal matrix and br = d. The parameter-efficiency brought by the\nblock-diagonal structure comes at a price \u2013 it introduces an assumption that the dimensions of a\nneuron (i.e., a column vector of the weight matrix W 0) are divided by r groups and dimensions in\ndifferent groups are transformed separately using different orthogonal matrices. Despite the empirical\neffectiveness of the block-diagonal structure, it makes no sense to divide the dimensions of a neuron\ninto r groups based on their indices, which makes dense orthogonal matrices desirable. A natural\nquestion arises: Can we construct a dense orthogonal matrix without losing the parameter-efficiency?\nTo address this question, we propose to compose a dense orthogonal matrix with a product of multiple\nsparse orthogonal matrices. To provide a unified yet intuitive perspective to study the sparsity pattern\n3\nTechnical Report\nLevel 1\nLevel 2\nLevel 3\nLevel 4\nLevel 5\nB1\nB2\nB3\nB4\nB5\nB2B1\nB3B2B1\nB4B3B2B1\nB5B4B3B2B1\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\nZero\nNon-zero\nLevel 6\nFigure 2: An illustration of the information transmission view on\ngenerating dense matrices. This example uses d = 4 and m = 5.\nof orthogonal matrix factorization, we frame the\nproblem of generating a dense orthogonal matrix in\nOFT as an information transmission problem. Specif-\nically, generating a dense matrix R \u2208 Rd\u00d7d by a\nproduct of m square matrices R=BmBm\u22121 \u00b7 \u00b7 \u00b7 B1\ncan be viewed as transmitting information in a grid\nwith d\u00d7(m+1) nodes, as illustrated in Figure 2. The\nmotivation behind the information transmission view\ncomes from the observation that a d-dimensional\ndense square matrix can be interpreted as a dense\nconnectivity from d nodes to another d nodes. For\nthe matrix R, if the element Rij is zero, then it\nindicates that information from the j-th node can-\nnot flow to the i-th node. If Rij is non-zero, then\nthe information can be transmitted. Therefore, rep-\nresenting the dense matrix R with multiple matrices\nBmBm\u22121 \u00b7 \u00b7 \u00b7 B1 can also be interpreted as perform-\ning sequential information exchange based on the graphs induced by Bi, \u2200i. The information flows\nfollowing B1 first and Bn in the end. As a concrete example in Figure 2, we consider the factorization\nR = B5B4B3B2B1 whose sparsity patterns and induced graph are visualized. The graph in Fig-\nure 2 is the result of unrolling the matrix multiplication. In the induced graph, the matrix Bi is viewed\nas the connectivity matrix from the i-th level nodes to the (i+1)-th level nodes. More specifically, the\n(j1, j2) element of Bi denotes whether there is a directed edge from the j2-th node in the i-th level to\nthe j1-th node in the (i + 1)-th level (zero means no edge). For B5B4B3B2B1 to be a dense matrix,\nevery node in the first level should be able to transmit information to all the nodes in the 6-th level. If\nwe only consider R = B4B3B2B1 which corresponds to the source nodes in the first level and the\nreceiver nodes in 5-th level, then we find that information from the node 1 cannot be transmitted to\nthe node 3. Therefore, the sparsity pattern of B4B3B2B1 has a zero element at the (3, 1) position.\nConsidering R = B3B2B1 and R = B2B1, the same correspondence holds between the induced\ngraph and the sparsity pattern. Generally, for a matrix R \u2208 Rd\u00d7d to be dense, the m factorization\nmatrices Bm, \u00b7 \u00b7 \u00b7 , B1 needs to correspond to a set of directed edges on a d \u00d7 (m + 1) grid where\none directed edge can only connect two nodes between adjacent levels (i.e., columns), such that\ninformation from every node in the first level can be transmitted to every node in the (m + 1)-th level.\n1\n2\n3\n4\n1\n2\n3\n4\nFigure 3: An example of block-\ndiagonal structure in OFT.\nThe information transmission view can help us gain a better understanding\nof the sparsity pattern of factorization matrices in OFT. Figure 3 visualizes\nthe block-diagonal structure of R in the original OFT. Despite reducing the\nnumber of trainable parameters, the block-diagonal structure cannot construct\na dense matrix R. Our goal is to compose a dense orthogonal matrix with\nm sparse orthogonal matrices, using as few effective trainable parameters\nas possible. Under the information transmission view, the general desiderata\ntowards our goal are (i) dense connectivity: every node in the first level has at least one path to\nevery node in the last level, and (ii) minimum free edges: the total number of edges should be as\nsmall as possible under the orthogonality constraint. We note that orthogonality injects a delicate\nconstraint to the edges between adjacent levels. For example, for each matrix Bi to be full-rank (a\nnecessary condition of orthogonality), we need to have d edges to form a bijection between all the\nnodes in the i-th level and all the nodes in the (i = 1)-th level, which makes the number of edges\nbetween adjacent levels at least d (e.g., 4 for the example in Figure 2). These d edges is necessary for\northogonality and should not be counted into the number of edges, because these elements are not\ntrainable (e.g., for a d \u00d7 d orthogonal with d non-zero entries, these entries can only be \u00b11). Because\northogonal matrices require less number of parameters than full matrices, the orthogonality constraint\nwill bring in additional dependency among edges. As an example, for a 2 \u00d7 2 orthogonal matrix, one\nzero at the (1, 1) position will imply another zero at the (2, 2) position (i.e., one missing edge could\nimply another missing edge). Therefore, for each feasible set of edge connections, the orthogonality\nmay sometimes add or remove some edges. By visualizing the non-zero pattern of the composed\northogonal matrix, the information transmission framework is particularly useful in OFT, because we\nonly care about the non-zero trainable elements of R and their specific values do not matter.\nA naive dense connection between two levels takes O(d2) edges (i.e., a single dense orthogonal\nmatrix), yielding d2\u2212d edges (for d = 4, it is 12 edges). Figure 2 gives an example of a feasible matrix\n4\nTechnical Report\nfactorization and it takes 10 edges in total, which is actually less than a single dense orthogonal matrix.\nThis framework enables us to study the parameter-efficiency of OFT from a graphical perspective,\nand we can easily come up with feasible factorizations with this framework. We draw inspiration from\nan interesting topology from the Cooley-Tukey algorithm, called butterfly graphs [12], which can\ndensely connect d source nodes and d receiver nodes efficiently with O(d log d) edges. For example,\nthe topology in Figure 2 takes 10 edges to achieve dense connectivity, while the butterfly network\nonly takes 8 edges. Next, we introduce how the butterfly structure can improve parameter efficiency.\n4\nORTHOGONAL PARAMETERIZATION BY BUTTERFLY FACTORIZATION\nLevel 1\nLevel 2\nLevel 3\nLevel 4\nB(8,8)\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\n~\nB(8,4)\n~\nB(8,2)\n~\nFigure 4: The butterfly structure (d = 8).\nThe butterfly structure is originally used in the Cooley-Tukey al-\ngorithm to perform fast Fourier transform. In Fourier transform, a\nlocal change in the frequency domain can cause a global change in\nthe spatial domain, which is conceptually similar to our information\ntransmission problem \u2013 every node in the first level can transmit\ninformation to all the nodes in the last level. The butterfly structure\nalso becomes a popular computer network topology [41, 75] used\nfor efficient information exchange. Assuming that k \u2265 2 is a power\nof 2, we start by defining the butterfly factor \u02dc\nBF (k) as\n\u02dc\nBF (k) =\n\u0014\ndiag(d1)\ndiag(d2)\ndiag(d3)\ndiag(d4)\n\u0015\n\u2208 Rk\u00d7k,\n(1)\nwhere di \u2208 R\nk\n2 , \u2200i are some vectors. With d = 2N, we then define\nthe d-dimensional butterfly matrix B(d) \u2208 Rd\u00d7d recursively as\nB(d) = \u02dc\nB(d, d)\u00b7\n\u0014\nB1( d\n2)\n0\n0\nB2( d\n2)\n\u0015\n= \u02dc\nB(d, d) \u02dc\nB(d, d\n2) \u00b7 \u00b7 \u00b7 \u02dc\nB(d, 2), (2)\nwhere B1( d\n2) and B2( d\n2) are two d\n2-dimensional butterfly matrices.\nWe then define the butterfly component as \u02dc\nB(d, k)=diag(BF\n1 (k), \u00b7 \u00b7 \u00b7 , BF\nd/k(k)) that is a block-\ndiagonal matrix of size d \u00d7 d with the block size k, where BF\ni (k), \u2200i are butterfly factors defined in\nEquation 1. Now we are ready to use the butterfly matrix to parameterize an orthogonal matrix. To\nachieve this, we only need to ensure that all multiplicative factors \u02dc\nB(d, k), \u2200k in the butterfly matrix\nB(d) are orthogonal. We first look into the block-diagonal matrix \u02dc\nB(d, 2) with the block size 2, and\nwe can easily guarantee \u02dc\nB(d, 2) to be orthogonal with Cayley transform (or 2-dimensional rotation)\nto parameterize each block, same as [52, 67]. The non-zero pattern of every butterfly component\ncan be viewed as a permutation of the non-zero pattern of \u02dc\nB(d, 2), so all the butterfly components\ncan be easily parameterized as orthogonal matrices. This gives us an efficient parameterization of\northogonal matrices built upon many 2 \u00d7 2 orthogonal matrices. We generalize the butterfly matrices\nfollowing [7], and define a block butterfly component \u02dc\nBb(d, k) where each entry in di, \u2200i becomes a\nb \u00d7 b matrix. To guarantee the block butterfly component \u02dc\nBb(d, 2) to be orthogonal, we parameterize\neach 2b \u00d7 2b block matrix to be orthogonal. The non-zero pattern of the other butterfly components\n\u02dc\nBb(d, k), k > 2 are the block-wise permutation of the non-zero pattern of \u02dc\nBb(d, 2) and therefore\ncan be similarly turned into orthogonal matrices. Combining pieces, the forward pass in BOFT is\nz =\n\u0000R(m, b) \u00b7 W 0\u0001\u22a4x,\ns.t.\n\u001a\nR(m, b) =\nm\nY\ni=1\n\u02dc\nBb\n(d,i) &\n\u0000 \u02dc\nBb\n(d,j)\n\u0001\u22a4 \u02dc\nBb\n(d,j) = \u02dc\nBb\n(d,j)\n\u0000 \u02dc\nBb\n(d,j)\n\u0001\u22a4 = Id\n|\n{z\n}\n\u2200j\u2208[1,m]\n\u001b\n,\nwhere we denote \u02dc\nBb(d, 2m\u2212i+1) as \u02dc\nBb\n(d,i) for simplicity, and Id is an identity matrix of size d.\nThe orthogonal matrix R(m, b) \u2208 Rd\u00d7d is composed of a product of multiple orthogonal butterfly\ncomponents. For convenience, we denote BOFT with R(m, b\n2) as BOFT(m,b), where b \u2265 2. When\nm=1, then BOFT(1,b) reduces to the block-diagonal OFT [67] with the block size b. BOFT(1,d)\nreduces to the original OFT [67] with an unconstrained full orthogonal matrix. BOFT(log 2d\nb ,b)\nuses the block butterfly matrix B\nb\n2 (d) as R, and yields a dense orthogonal matrix R. In general,\nBOFT(m,b) takes 1\n2(b\u22121)dm effective trainable parameters for finetuning a linear layer of size d\u00d7n.\nIf we use the butterfly matrix, i.e., m = log d, b = 2, BOFT uses O(d log d) parameters. In contrast,\nthe original OFT with a full dense orthogonal matrix uses O(d2) parameters, and the block-diagonal\nOFT with the block number r uses O(bd). Therefore, the original OFT has to use the block size\nb = d to generate a dense orthogonal matrix, while BOFT can use any b to achieve this.\n5\nTechnical Report\nIdentity initialization for BOFT. Finetuning methods usually start with the exact pretrained model\nsuch that the finetuned model will not deviate too much from the pretrained one. For example, LoRA\nuses zero initialization for the low-rank weights. In BOFT, we initialize all the butterfly components\nwith identity matrices (i.e., the skew-symmetric matrix is initialized as zeros in Cayley transform).\nMultiplicative Dropout for BOFT. LoRA [29] further implements a Dropout layer for the low-rank\nweight update to prevent overfitting. The conventional Dropout [77] naturally works for LoRA, but\nnot for BOFT due to our multiplicative weight update. To address this, we propose a multiplicative\nDropout for BOFT. Because the orthogonal matrix R(m, b) is composed of m orthogonal butterfly\ncomponents which can be easily permuted to 2b \u00d7 2b block-diagonal orthogonal matrices. The\nmultiplicative Dropout first randomly picks p1 percent of the butterfly components and p2 percent of\nthe diagonal blocks in each butterfly component, and then replaces these blocks as identity matrices.\n5\nINTRIGUING INSIGHTS AND DISCUSSIONS\n103\n104\nNumber of trainable parameters\n0.6\n0.7\n0.8\n0.9\n1\nScaled approximation error\nBOFT(1:9,2)\nBOFT(1:8,4)\nBOFT(1:6,16)\nBOFT(1,2)\nBOFT(9,2)\nBOFT(8,2)\nBOFT(1,4)\nBOFT(6,16)\nBOFT(1,16)\nFigure 5: Expressiveness of BOFT.\nExpressivity of BOFT. The butterfly structure along with permutations\ncan perfectly recover many classic fast linear transform [13, 14] (e.g.,\nfast Fourier transform, Hadamard transform), but how well our orthogo-\nnal butterfly matrix can approximate a general orthogonal matrix remains\nunknown. We start by conducting a simulation to approximate a random\ndense orthogonal matrix [2] with size 512 \u00d7 512. The results in Figure 5\nare averaged over 10 random seeds. The y-axis denotes the approxi-\nmation error, and the x-axis denotes the number of effective trainable\nparameters. Each curve with the same color denotes BOFT with the same\nblock size, and the leftmost point is the error of BOFT(1,b) (i.e., the original block-diagonal OFT\nwith block size b). BOFT generally yields better parameter efficiency than OFT. For example, the\nexpressiveness of BOFT(9,2) is better than that of BOFT(1,16) but has much less parameters. BOFT\nwith smaller b and larger m is generally more parameter-efficient. For example, BOFT(6,4) uses\nmuch less parameters but yields a similar approximation error to BOFT(2,16). In general, the butterfly\nmatrix represents a more structured subset of the orthogonal group (compared to the block-diagonal\nstructure), which makes BOFT provably more expressive than OFT with the same block size.\nTheorem 1 (Expressivity of BOFT). BOFT is more expressive than OFT with the same block size.\nFor the butterfly matrix to approximate all orthogonal matrices of size d, we can multiply butterfly\nmatrices with Bd\u22121,1(d)B\u22a4\nd\u22121,2(d) \u00b7 \u00b7 \u00b7 B1,1(d)B\u22a4\n1,2(d), where Bi,j(d), \u2200i, \u2200j are butterfly matrices.\nTheorem 1 suggests a simple generalization for BOFT \u2013 the final orthogonal matrix is generalized to\nRG(m1, b1, m2, b2, l)=Rl,1(m1, b1)RT\nl,2(m2, b2) \u00b7 \u00b7 \u00b7 R1,1(m1, b1)RT\n1,2(m2, b2) where RT\ni,j(m, b)\ndenotes the orthogonal matrix used in BOFT. When m1 = m2 = log d, b1 = b2 = 2 and l = d \u2212 1,\nthen RG(m1, b1, m2, b2, l) can represent the entire orthogonal group. Such a matrix composition is\nalso called kaleidoscope hierarchy [14]. However, we note that better expressiveness does not always\nlead to better performance in finetuning, as full finetuning, despite its universal expressiveness, often\nyields unsatisfactory performance. The trade-off between expressivity and regularity is the key to the\ngeneralizability of model finetuning. BOFT enlarges the finetuning parameter space with structural\npriors, which enables us to find a better trade-off between expressivity and regularity.\nSpectral properties. Orthogonal finetuning generally yields better spectral property than LoRA,\nbecause it perfectly preserves the spectral norm of the pretrained weight matrix W 0. We can see\nthis by singular value decomposition: W 0 = U\u03a3V \u22a4 where U, V are orthogonal matrice and \u03a3 is\na singular vale diagonal matrix. Both OFT and BOFT multiply an orthogonal matrix R to the left\nand obtain the finetuned weights RU\u03a3V \u22a4, which does not affect the largest singular value (i.e., the\nspectral norm of W 0). Such a preservation has been shown to greatly benefit training stability and\ngeneralization [58, 90]. We introduce more interesting mathematical properties in Appendix G.\nOrthogonal finetuning as learning bilinear similarity. BOFT can be written as learning the bilinear\nsimilarity w0\ni Rx where w0\ni is the i-th neuron (i.e., column vector) of the weight matrix W 0. BOFT\ncan be viewed as learning the bilinear similarity matrix R with a strong regularity (i.e., R needs to\nbe orthogonal), which intrinsically connects to distance metric learning [89] and bilinear form [72].\nInductive bias and generalization in BOFT. Since R(m, b) in BOFT usually represents a structured\nsubset of the orthogonal group which constrains the hypothesis class, BOFT will naturally induce\nan inductive bias. We argue that the structured inductive bias induced by butterfly factorization is\n6\nTechnical Report\nbeneficial to generalization, as it has a shared structured pattern of many classic linear transforms [14],\nsuch as discrete Fourier transform, discrete sine/cosine transform and Hadamard transform. Moreover,\nthe sparse matrix factorization in BOFT may also bring some implicit inductive bias [22, 44, 51].\nComparison to butterfly-based sparse training. There are quite a few works [7, 13\u201315] that study\nsparse training with the butterfly parameterization. They typically focus on reparameterizing the\nweight matrices directly with the butterfly parameterization and training neural networks from scratch.\n[15] considers to finetune the pretrained weights by first projecting the weights on a variant of butterfly\nmatrices and then optimizing the projected components for downstream tasks. BOFT proposes a very\ndifferent finetuning strategy that transforms the weights with layer-shared weight matrices.\n6\nAPPLICATIONS AND EMPIRICAL RESULTS\nWe apply BOFT to finetune large language models (DeBERTaV3 [25], Llama-2 [81]), vision founda-\ntion models (DINOv2 [62], SAM [35]), and text-to-image generative models (Stable Diffusion [73])\non various downstream tasks. To ensure a fair comparison, we use exactly the same settings for all\nthe compared baselines. The results are averaged over 5 random seeds, and the gains have passed\nsignificant tests with p < 0.05. Experimental details and more results are provided in the appendices.\n6.1\nADAPTATION OF LARGE LANGUAGE MODELS (LLMS)\nMethod\n# Param MNLI SST-2 CoLA QQP\nQNLI\nRTE\nMRPC STS-B\nAll\nFull Finetuning\n184M\n89.90\n95.63\n69.19 92.40 94.03 83.75\n89.46\n91.60\n88.25\nBitFit [92]\n0.1M\n89.37\n94.84\n66.96 88.41 92.24 78.70\n87.75\n91.35\n86.20\nH-Adapter [28]\n1.22M\n90.13\n95.53\n68.64 91.91 94.11 84.48\n89.95\n91.48\n88.28\nP-Adapter [65]\n1.18M\n90.33\n95.61\n68.77 92.04 94.29 85.20\n89.46\n91.54\n88.41\nLoRAr=8 [29]\n1.33M\n90.65\n94.95\n69.82 91.99 93.87 85.20\n89.95\n91.60\n88.50\nAdaLoRA [97]\n1.27M\n90.76\n96.10\n71.45 92.23 94.55 88.09\n90.69\n91.84\n89.46\nOFT b=16\n0.79M\n90.33\n96.33\n73.91 92.10 94.07 87.36\n92.16\n91.91\n89.77\nBOFT m=2\nb=8\n0.75M\n90.25\n96.44\n72.95 92.10 94.23 88.81\n92.40\n91.92\n89.89\nTable 1: Results on the GLUE development set. We report the matched accuracy for MNLI,\nMatthew\u2019s correlation for CoLA, average correlation for STS-B and accuracy for other tasks.\nNatural language under-\nstanding. To evaluate the per-\nformance of BOFT on LLM\nadaptation, we first finetune a\npretrained DeBERTaV3-base\nmodel [25] on the GLUE\nbenchmark [87], which con-\nsists of some representative\nsentence classification tasks\nand is widely used for assess-\ning the natural language un-\nderstanding ability [17, 25, 53]. Results are presented in Table 1. \u201c# Param\u201d in the table denotes\nthe total number of effective trainable parameters for each method. We note that OFT [67] with the\nblock size 16 is BOFT(1,16). One can observe that orthogonal finetuning performs better than current\nstate-of-the-art methods. More importantly, BOFT outperforms OFT while still using less parameters.\nMMLU (5-shot)\nMMLU (0-shot)\nMethod\n# Param Hums. STEM Social Other Avg. Hums. STEM Social Other Avg.\nLlama-2-7B\n-\n43.0\n36.9\n51.6\n52.1 45.7\n38.8\n33.3\n46.8\n45.0 40.8\nLoRAr=16 0.125%\n42.9\n38.5\n54.5\n53.8 47.0\n42.5\n37.1\n51.5\n52.3 45.5\nLoRAr=32\n0.25%\n42.9\n38.7\n54.6\n54.7 47.3\n42.5\n36.7\n52.8\n52.7 45.9\nOFT b=16\n0.13%\n44.0\n38.9\n54.2\n54.3 47.5\n44.0\n36.7\n52.9\n52.0 46.2\nBOFT m=2\nb=8\n0.12%\n44.5\n39.0\n54.4\n55.1 47.9\n44.3\n37.4\n53.1\n52.8 46.7\nTable 2: Accuracy (%) on MMLU. \u201c# Param\u201d denotes the percentage of finetuned parameters.\nMassive multitask language un-\nderstanding. We use Alpaca [80]\nas our finetuning dataset and\nevaluate both zero-shot and few-\nshot performance on the MMLU\ndataset [27] which consists of\n57 language tasks. All methods\nuse the pretrained Llama-2-7B\nmodel [81]. Results in Table 2 show a consistent improvement over LoRA, but BOFT uses fewer\nparameters. Notably, BOFT(2,8) produces a block-diagonal orthogonal matrix with the block size 16,\nand yet still outperforms OFT with the same block size (i.e., BOFT(1,16)) by a considerable margin.\nThis result implies that the butterfly structure can incorporate a generalizable inductive bias.\nMethod\n# Param GSM8K MATH\nLlama-2-7B\n-\n14.6\n2.5\nLoRAr=32\n0.25%\n50.2\n7.8\nOFT b=16\n0.13%\n50.1\n8.4\nBOFT m=2\nb=8\n0.12%\n50.6\n8.6\nTable 3: Results on GSM8K and MATH.\nMathematical question answering. We also evaluate our method in\nmathematical question answering using two challenging benchmarks:\nGSM8K [11] and MATH [27]. For all the finetuning methods, we\nuse MetaMathQA-40K [91] as the finetuning dataset, and the Llama-\n2-7B model [81] as the pretrained backbone. As can be observed in\nTable 3, BOFT excels in mathematical reasoning on both datasets.\nWe note that even though improvement on the MATH dataset is in\nfact quite challenging, BOFT achieves more than 10% relative improvement over LoRA while only\nusing half of the number of trainable parameters for LoRA. Moreover, BOFT outperforms OFT even\nwith the same effective block number, again verifying that the butterfly structure can introduce a\ngeneralizable inductive bias. We also provide a case study of a few questions in Appendix E.\n7\nTechnical Report\nNatural\nSpecialized\nStructured\n# param (M)\nCifar100\nCaltech101\nDTD\nFlower102\nPets\nSVHN\nSun397\nCamelyon\nEuroSAT\nResisc45\nRetinopathy\nClevr-Count\nClevr-Dist\nDMLab\nKITTI-Dist\ndSpr-Loc\ndSpr-Ori\nsNORB-Azim\nsNORB-Ele\nAverage\nFull Finetuning\n304.4 67.6 91.7 77.9 99.7 93.7 92.8 52.3 88.1 96.1 90.9 77.2 67.2 59.8 58.1 82.8 83.6 62.0 36.9 39.4 74.6\nLinear Probing\n0\n73.2 90.9 78.1 99.7 95.2 40.3 59.3 84.2 92.9 86.8 75.6 48.1 44.4 45.9 65.4 25.5 37.0 18.5 30.9 62.7\nBitFit [92]\n0.27\n78.5 91.7 80.4 99.7 95.0 67.3 60.2 85.2 96.1 90.7 75.7 84.1 63.0 52.7 78.9 83.8 61.9 28.0 37.7 74.2\nFacTttr=16 [31]\n0.12\n76.2 89.4 77.3 99.7 94.7 89.6 58.9 87.1 94.3 88.7 74.0 83.1 63.3 56.2 83.1 61.7 37.1 23.3 32.6 72.1\nFacTtkr=32 [31] 0.12\n75.0 89.1 78.6 99.7 95.0 92.1 58.9 86.1 94.6 89.5 74.2 84.3 62.0 57.7 85.2 68.4 38.3 31.2 44.2 73.9\nLoRAr=4 [29]\n1.77\n77.2 92.8 80.3 99.7 94.8 92.7 59.5 88.3 96.4 91.4 77.4 74.7 62.4 58.1 85.2 85.8 57.2 31.8 37.2 76.6\nGLoRAr=4 [6]\n4.87\n80.1 93.7 80.2 99.7 94.4 89.6 59.9 85.9 96.0 91.0 76.2 61.8 62.3 56.9 85.8 65.7 57.2 37.0 41.4 74.5\nOFT b=16\n2.10\n77.7 91.9 80.1 99.7 94.7 92.9 59.3 88.4 96.4 91.5 77.2 81.0 64.7 60.5 84.0 92.2 61.1 34.8 40.3 77.3\nBOFT m=4\nb=4\n1.77\n78.2 91.4 79.6 99.7 94.9 92.8 59.4 88.1 96.4 91.6 76.2 81.9 65.4 60.0 84.5 92.9 61.3 37.1 39.3 77.4\nBOFT m=6\nb=2\n1.11\n78.3 91.5 79.9 99.7 95.0 92.0 60.2 88.2 96.5 91.4 77.2 80.5 64.1 61.4 85.0 91.6 60.8 34.0 38.5 77.1\nTable 4: Results (%) on the VTAB-1K benchmark. \u201c# param\u201d specifies the number of trainable parameters of each method. The average accuracy\nis obtained by averaging over all 19 tasks. The best results are marked with \u201cbold\u201d, and the second/third best results are marked with \u201cunderline\u201d.\n6.2\nADAPTATION OF VISION FOUNDATION MODELS\nModel\n# Param\nDIS\nCOIFT\nHRSOD\nThinObject\nAverage\nmIoU mBIoU mIoU mBIoU mIoU mBIoU mIoU mBIoU mIoU mBIoU\nSAM (baseline)\n0\n62.0\n52.8\n92.1\n86.5\n90.2\n83.1\n73.6\n61.8\n79.5\n71.1\nFinetune SAM\n4.06M 78.9\n70.3\n93.9\n89.3\n91.8\n83.4\n89.4\n79.0\n88.5\n80.5\nHQ-SAM [34]\n1.33M 78.6\n70.4\n94.8\n90.1\n93.6\n86.9\n89.5\n79.9\n89.1\n81.8\nBOFT-SAM m=3\nb=4\n0.04M 78.2\n69.7\n94.9\n90.5\n93.1\n86.0\n91.7\n80.1\n89.5\n81.6\nTable 5: Results on HQSeg-44K [34] (DIS [66], COIFT [47], HRSOD [93], ThinObject [47]).\nTransfer learning on VTAB-1K.\nWe evaluate the finetuning perfor-\nmance of BOFT on the VTAB-\n1K benchmark [94], which has\nbeen extensively used to evaluate\nparameter-efficient transfer learn-\ning algorithms. VTAB-1K con-\nsists of 19 image classification tasks that are divided into three categories: natural images, specialized\ntasks (e.g., remote sensing and medical images), and structured tasks (e.g., depth and orientation\nprediction). In VTAB-1k, each dataset provides 800 labeled training set samples, a subset of their\noriginal training set. We use them to fine-tune our base model and the Top-1 classification accuracy on\ntheir respective original test set is used as the performance measure. Notably, all compared methods\nintroduce no inference latency, so they have the same inference speed. Because the final classification\nlayer will always get retrained and the trainable parameters of that linear classification layer vary\nacross different tasks, we follow the common practice and do not take them into account when\nreporting the total trainable parameters for each method. Different from previous work [6], we use a\nmuch larger pretrained vision transformer [62] (DINOv2-large) with more than 300M parameters.\nThe accuracy scores are presented in Table 4. We observe that orthogonal finetuning achieves the best\noverall testing accuracy on the VTAB-1K benchmark, and BOFT with m = 4, b = 4 again achieves\nthe best performance. Remarkably, BOFT\u2019s performance enhancement is both stable and consistent\nacross tasks, as almost all our results outperform the simplest full finetuning baseline. BOFT is\nmarginally worse than full finetuning on three tasks: dSpr-Ori (\u22120.7%), Caltech101 (\u22120.3%) and\nsNORB-Ele (\u22120.1%). In contrast, LoRA is significantly worse than full finetuning on sNORB-Azim\nand dSpr-Ori by 5%. These results validate the effectiveness of BOFT for vision transformers.\nSAM\nBOFT-SAM\nFigure 6: Qualitative comparison of between SAM and BOFT-SAM.\nHigh-quality segmentation with SAM. The Seg-\nment Anything Model (SAM) [35] is a vision\nfoundation model for promptable image segmen-\ntation, demonstrating impressive zero-shot capa-\nbilities. SAM consists of three main components:\na pre-trained image encoder to generate a fea-\nture embedding of the input image, a prompt en-\ncoder to embed prompts, and a mask decoder\nto map these input embeddings to a segmenta-\ntion mask. Despite its impressive performance\nin general image segmentation, SAM lacks the\nability to perform highly accurate segmentation\nin challenging situations. To address this, HQ-\nSAM [34] proposes to train an additional HQ-Output Token and a global-local feature fusion module\non a high-quality segmentation dataset, HQSeg-44K [34], to improve the mask quality, achieving\nstate-of-the-art performance in high-quality segmentation. Using the same dataset and loss function\nas HQ-SAM, we finetune the original SAM with BOFT for 10 epochs. Specifically, we only apply\nBOFT to all linear layers of the mask decoder of SAM, while keeping the other part of SAM frozen.\n8\nTechnical Report\nText prompt: a woman with long black hair\nText prompt: a man with blonde hair\nText prompt: a woman with her mouth open\nText prompt: a man wearing a hat\nControl signal\nControl signal\nControl signal\nControl signal\nLoRA\nLoRA\nLoRA\nLoRA\nOFT\nOFT\nOFT\nOFT\nBOFT\nBOFT\nBOFT\nBOFT\nFigure 7: Qualitative comparison of controllable generation. The figure is best viewed digitally, in color and significantly zoomed in.\nWe compare to finetuning the entire mask decoder, training the HQ-SAM modules [34] and finetuning\nlinear layers with BOFT. Table 5 shows that BOFT-SAM uses only 3% of trainable parameters used\nin HQ-SAM, and yet matches its performance. Moreover, since the multiplicative weights learned by\nBOFT can be combined back to the weights of SAM, BOFT-SAM has exactly the same inference\nspeed as SAM, while, in contrast, HQ-SAM has additional modules that affect its inference speed.\n6.3\nCONTROLLING TEXT-TO-IMAGE DIFFUSION MODELS\nMethod\n# Param\nError\nLoRAr=128\n20.17M\n8.038\nLoRAr=16\n2.52M\n8.878\nOFT r=16\n2.71M\n8.876\nOFT r=4\n10.50M\n6.537\nBOFT m=2\nr=32\n2.66M\n8.070\nBOFT m=5\nr=16\n12.93M\n6.387\nBOFT m=4\nr=8\n20.76M\n5.667\nTable 6: Face landmark error between\ncontrol signal and prediction.\nSince OFT is originally used to control text-to-image diffusion mod-\nels [67], we also evaluate BOFT with the same task for better compar-\nison. We finetune the pretrained Stable Diffusion [73] for two tasks:\ncontrollable generation (e.g., [59, 96]) and subject-driven generation\n(e.g., [74]). Controllable generation enables adding spatial control sig-\nnals to the text-to-image diffusion models. Subject-driven generation\naims to synthesize images of a subject in novel contexts by finetuning on\na few images of that subject to learn a unique identifier. We follow the\nsame setting as [67] for evaluating controllable generation. To be easily\ncomparable to OFT, where the block structure is characterized by the\nnumber of blocks r, we also use the number of blocks to characterize\nBOFT (instead of the block size b). Because rd = b, larger r indicates less number of parameters. For\nexample, for BOFT with r = 32 to generate a dense orthogonal matrix, we need to have m = 6. We\nstart by comparing LoRA, OFT and BOFT with a small parameter budget (less than 3M parameters).\nWe see from Table 6 that BOFT with r = 32, m = 2 yields significantly better performance than\nboth LoRA and OFT with the block number 16 under the 3M parameter budget. Under this small\nbudget setting, we also provide a qualitative comparison among LoRA (r = 16), OFT (r = 16) and\nBOFT (r = 32, m = 2) in Figure 7. We also evaluate how BOFT performs with a dense orthogonal\nmatrix using r = 16, m = 5 and r = 8, m = 4. We observe that BOFT with r = 8, m = 4 achieves\nthe best performance and significantly outperforms LoRA with a similar number of parameters.\n5\n10\n15\n20\nNumber of epochs\n5\n10\n15\n20\n25\n30\nLandmark error\nOFT, r=32\nBOFT, r=32, m=2\nBOFT, r=32, m=4\nBOFT, r=32, m=6\nFigure 8: How m affects controllability.\nFinally, we conduct an ablation study on how the number of butterfly\ncomponents m affects the performance of controllable generation. We\nfirst fix the block number as r = 32, and then vary the number of\nbutterfly components in BOFT from 0 (i.e., OFT with the block number\n32) to 6 (BOFT with a dense orthogonal matrix). Figure 8 shows that\nBOFT with a larger m yields better control performance for Stable\nDiffusion finetuning. More interestingly, we also find that, with the\nsame number of blocks, an increased number of butterfly components\ngenerally leads to faster and more stable convergence. This implies that\northogonal finetuning with a denser orthogonal matrix converges faster\nin finetuning text-to-image diffusion models. This also matches our intuition that a dense orthogonal\nmatrix can transform neurons more effectively due to its more efficient information transmission.\nBOFT also performs consistently better than both LoRA and the original OFT in subject-driven\ngeneration. A qualitative comparison is given in Figure 9 and Appendix C. For all the compared\nmethods, we use the best possible hyperparameters. We empirically observe that BOFT can generally\ncapture more intrinsic identity characteristics of the input subject, and therefore, the generated images\nare visually more plausible in terms of the subject identity preservation. We can see from Figure 9\nthat the original OFT also shows good performance in preserving subject identity while LoRA has\n9\nTechnical Report\na [V] bowl with a wheat field in the background\na [V] bowl with a city in the background\nLoRA\nOFT\nBOFT\na shiny [V] backpack\na [V] backpack on top of pink fabric\nLoRA\nOFT\nBOFT\nInput images\nInput images\na [V] sneaker with the Eiffel Tower in the background\na cube shaped [V] toy\na [V] toy with a tree and autumn leaves in the background\nLoRA\nOFT\nBOFT\na [V] sneaker on the beach\nLoRA\nOFT\nBOFT\nInput images\nInput images\nFigure 9: Qualitative comparison of subject-driven generation. The figure is best viewed digitally, in color and significantly zoomed in.\nControl signal\nBOFT (6 matrices)\nSD* (0 matrix)\n5 matrices\n4 matrices\n3 matrices\n2 matrices\n1 matrix\nFigure 10: Model weight interpolation by setting the trained butterfly components one by one to identity matrix. We use BOFT(m = 5, r = 16)\nto finetune Stable Diffusion (SD). No retraining is performed when we gradually set each trained orthogonal matrix ( \u02dc\nBi) to an identity. The\nnumber in the figure denotes the number of remaining orthogonal butterfly components that has not been set to identity. Text prompt: a man\nwith a beard smiling (for the first row) and a smiling woman (for the second row). *0 matrix is the case of SD with a learned control head.\nbetter text prompt following ability. In sharp contrast, BOFT can achieve the best of both worlds\nby simultaneously demonstrating good subject identity preservation as well as accurate text prompt\nfollowing ability. Notably, for the bottom-left toy duck case in Figure 9, we observe that BOFT can\ncapture the essence of the toy and generate a cubed shaped toy with a conceptually similar color.\nBOFT comes with free weight interpolation. We have a surprising yet interesting discovery that\nuniquely distinguishes BOFT from existing methods in controllable generation. BOFT consists of\nmultiple orthogonal matrices (i.e., multiple butterfly components), and the product of these matrices\ngives the complete finetuned model. However, what will happen if we set the trained orthogonal\nbutterfly components to identity matrix one by one without retraining? If we set all the butterfly\ncomponents to identity, the model reduces to Stable Diffusion. If no butterfly components are set\nto identity, then we have the full BOFT-finetuned model. After the BOFT training, the structure\nof multiple butterfly components provides us with a free weight interpolation on the orthogonal\nmanifold. We perform the weight interpolation for all the BOFT-finetuned layers in Stable Diffusion.\nSpecifically, we use BOFT with m = 5, r = 16, so we have 6 butterfly components. We set the\nbutterfly components one by one to identity matrix, starting from left-hand side. The results are given\nin Figure 10 Surprisingly, although these interpolated weights have not been retrained, they can still\ngenerate plausible images. In fact, as we set more butterfly components to identity, the interpolated\nmodel produces a smooth interpolated result, from a landmark-controlled image to an uncontrolled\nStable Diffusion image. These results well validate that the hypothesis weight space (i.e., model\nspace) in BOFT can well preserve the semantics and effectively eliminate many bad local minima.\n7\nCONCLUDING REMARKS AND LIMITATIONS\nOur paper proposes Orthogonal Butterfly, a generic parameter-efficient finetuning method for foun-\ndation models based on the butterfly structure. The key insight to better parameter-efficiency is to\nparameterize a dense orthogonal matrix with the product multiple sparse orthogonal matrices. To\n10\nTechnical Report\neasily find feasible matrix factorizations, we propose a graphical information transmission framework.\nUnder this framework, we find that the butterfly structure can effectively achieve our desiderata\nof sparse orthogonal matrix factorization. We demonstrate the empirical effectiveness of BOFT in\nfinetuning large language models, large vision models and text-to-image generative models. Our\nexperiments also validate the superiority of BOFT as a generic mode finetuning method.\nDespite empirical effectiveness, BOFT is by no means perfect. Since the final orthogonal matrix in\nBOFT is the product of multiple orthogonal matrix, the training runtime overhead is slightly larger\nthan OFT. How to improve BOFT\u2019s training runtime remains an open problem. Fortunately, after the\nfinetuning stage, the BOFT-learned orthogonal matrices can be directly multiplied into the pretrained\nmodel and there is no additional inference latency. Moreover, whether the butterfly network is the\nmost efficient way to transmit information is also unknown. Our information transmission framework\nfurther enables us to draw inspiration from a distinct research discipline \u2013 computer networking,\nwhere the efficiency of a network topology for transmitting information is heavily studied. We expect\nthat more efficient network structures can be used for composing dense orthogonal matrices.\nACKNOWLEDGMENT AND AUTHOR CONTRIBUTION\nThe authors would like to sincerely thank Peter Kulits for careful proofreading, Tim Z. Xiao and many\nother colleagues at Max Planck Institute for Intelligent Systems for countless helpful suggestions.\nThis work was supported by the German Federal Ministry of Education and Research (BMBF):\nTubingen AI Center, FKZ: 01IS18039B, and by the Machine Learning Cluster of Excellence, EXC\nnumber 2064/1 \u2013 Project number 390727645. WL was supported by the German Research Foundation\n(DFG): SFB 1233, Robust Vision: Inference Principles and Neural Mechanisms, TP XX, project\nnumber: 276693517. YF and SP are partially supported by the Max Planck ETH Center for Learning\nSystems. Yuliang Xiu is funded by the European Union\u2019s Horizon 2020 research and innovation\nprogramme under the Marie Sk\u0142odowska-Curie grant agreement No.860768 (CLIPE). AW acknowl-\nedges support from a Turing AI Fellowship under grant EP/V025279/1, The Alan Turing Institute,\nand the Leverhulme Trust via CFI. MJB has received research gift funds from Adobe, Intel, Nvidia,\nMeta/Facebook, and Amazon. MJB has financial interests in Amazon, Datagen Technologies, and\nMeshcapade GmbH. While MJB is a consultant for Meshcapade, his research in this project was\nperformed solely at, and funded solely by, the Max Planck Society.\nWeiyang Liu and Zeju Qiu contributed equally as the joint first author. Yao Feng, Yuliang Xiu, Yuxuan\nXue and Longhui Yu contributed equally as the joint second author. Authors with equal contributions\nare listed in alphabetical order and allowed to change their orders freely on their resume and website.\nMichael J. Black, Adrian Weller and Bernhard Sch\u00a8olkopf jointly supervised the project, provided\ngenerous computing support and contributed significantly to the project (including but not limit to\nidea discussion, direction suggestion and paper writing).\nWeiyang Liu initialized the core idea, organized the project, co-developed the current method, co-\nsupervised the experiments, and wrote the draft. Zeju Qiu co-initialized the core idea, co-developed\nthe current method, implemented most of the prototypes, conducted the GLUE, VTAB-1K and SAM\nfinetuning experiments, co-supervised the experiments and contributed to the draft writing. Yao Feng\nimplemented a fast version of BOFT in CUDA, contributed to the method development, and conducted\nthe experiments of multi-modal finetuning. Yuliang Xiu led the experimental efforts in controlling\ntext-to-image diffusion models, conducted the experiments of both controllable (ControlNet) and\nsubject-driven (DreamBooth) generation, contributed to the method development and paper writing.\nYuxuan Xue contributed to the method development and the experiments of vision foundation\nmodels. Longhui Yu led the experimental efforts in MMLU and mathematical question answering\nand contributed to the method development. Haiwen Feng contributed to the method development\nand the experiments of text-to-image generation. Juyeon Heo conducted the robustness experiments.\nAll the team members made necessary contributions to the method development and paper writing.\nLarge language models: the experiments are jointly led by Zeju Qiu and Longhui Yu. Weiyang Liu\ncontributed to the model debugging.\nVision foundation models: the experiments are led by Zeju Qiu. Yuxuan Xue contributed to the\nbaseline experiments and model debugging. Weiyang Liu contributed to the model debugging.\nText-to-image diffusion models: the experiments are led by Yuliang Xiu. Zeju Qiu, Weiyang Liu\nand Haiwen Feng contributed to the model debugging.\n11\nTechnical Report\nREFERENCES\n[1] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the\neffectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255, 2020. 3\n[2] Theodore W Anderson, Ingram Olkin, and Les G Underhill. Generation of random orthogonal\nmatrices. SIAM Journal on Scientific and Statistical Computing, 1987. 6\n[3] Alan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan Vuli\u00b4c. Composable sparse fine-tuning\nfor cross-lingual transfer. In ACL, 2022. 3\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. In NeurIPS, 2020. 1, 2\n[5] Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component\nanalysis? Journal of the ACM, 2011. 2\n[6] Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, and Zhiqiang Shen. One-for-all:\nGeneralized lora for parameter-efficient fine-tuning. arXiv preprint arXiv:2306.07967, 2023.\n1, 3, 8, 19\n[7] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher\nRe. Pixelated butterfly: Simple and efficient sparse training for neural network models. In\nICLR, 2022. 3, 5, 7\n[8] Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient\nfine-tuning design spaces. In ICLR, 2023. 3\n[9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating\nlarge language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 1\n[10] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping\nLuo. Adaptformer: Adapting vision transformers for scalable visual recognition. In NeurIPS,\n2022. 3\n[11] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021. 7\n[12] James W Cooley and John W Tukey. An algorithm for the machine calculation of complex\nfourier series. Mathematics of computation, 19(90):297\u2013301, 1965. 2, 5\n[13] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\u00b4e. Learning fast\nalgorithms for linear transforms using butterfly factorizations. In ICML, 2019. 3, 6, 7\n[14] Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski,\nAtri Rudra, and Christopher R\u00b4e. Kaleidoscope: An efficient, learnable representation for all\nstructured linear maps. In ICLR, 2020. 3, 6, 7, 29\n[15] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander\nLiu, Aniruddh Rao, Atri Rudra, and Christopher R\u00b4e. Monarch: Expressive structured matrices\nfor efficient and accurate training. In ICML, 2022. 3, 7\n[16] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient\nfinetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. 3\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding, 2019. 7\n[18] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu,\nYulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale\npre-trained language models. Nature Machine Intelligence, 2023. 3\n12\nTechnical Report\n[19] Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, and Mehdi\nRezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint\narXiv:2212.10650, 2022. 3\n[20] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and\nDaniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using\ntextual inversion. arXiv preprint arXiv:2208.01618, 2022. 20\n[21] Mozhdeh Gheini, Xiang Ren, and Jonathan May. Cross-attention is all you need: Adapting\npretrained transformers for machine translation. In EMNLP, 2021. 3\n[22] Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati\nSrebro. Implicit regularization in matrix factorization. In NeurIPS, 2017. 7\n[23] Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efficient transfer learning with diff\npruning. arXiv preprint arXiv:2012.07463, 2020. 1, 3\n[24] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. To-\nwards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366,\n2021. 1, 3\n[25] Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-\nstyle pre-training with gradient-disentangled embedding sharing. In ICLR, 2022. 7, 19\n[26] Wu Hecong. ControlLoRA: A Lightweight Neural Network To Control Stable Diffusion\nSpatial Information. https://github.com/HighCWu/ControlLoRA, 2023. 20\n[27] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. In ICLR, 2021. 7\n[28] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning\nfor nlp. In ICML, 2019. 1, 3, 7\n[29] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu\nChen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 1, 2, 3, 6, 7,\n8\n[30] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan,\nand Ser-Nam Lim. Visual prompt tuning. In ECCV, 2022.\n[31] Shibo Jie and Zhi-Hong Deng. Fact: Factor-tuning for lightweight adaptation on vision\ntransformer. In AAAI, 2023. 3, 8\n[32] Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark,\nand Marin Solja\u02c7ci\u00b4c. Tunable efficient unitary neural networks (eunn) and their application to\nrnns. In ICML, 2017. 3\n[33] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient\nlow-rank hypercomplex adapter layers. In NeurIPS, 2021. 3\n[34] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher\nYu. Segment anything in high quality. In NeurIPS, 2023. 8, 9, 20\n[35] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything.\narXiv preprint arXiv:2304.02643, 2023. 2, 7, 8, 20\n[36] Ralf Klasing, Burkhard Monien, Regine Peine, and Elena A St\u00a8ohr. Broadcasting in butterfly\nand debruijn networks. Discrete Applied Mathematics, 53(1-3):183\u2013197, 1994. 2, 3\n[37] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-\nconcept customization of text-to-image diffusion. In CVPR, 2023. 20\n13\nTechnical Report\n[38] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,\nSean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel\nHaziza. xformers: A modular and hackable transformer modelling library. https://github.com/\nfacebookresearch/xformers, 2022. 20\n[39] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient\nprompt tuning. arXiv preprint arXiv:2104.08691, 2021. 1, 3\n[40] S-YR Li, Raymond W Yeung, and Ning Cai. Linear network coding. IEEE transactions on\ninformation theory, 2003. 3\n[41] Shuo-Yen Robert Li, Qifu Tyler Sun, and Ziyu Shao. Linear network coding: Theory and\nalgorithms. Proceedings of the IEEE, 2011. 5\n[42] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.\nIn ACL, 2021. 1, 3\n[43] Yingzhou Li, Haizhao Yang, Eileen R Martin, Kenneth L Ho, and Lexing Ying. Butterfly\nfactorization. Multiscale Modeling & Simulation, 13(2):714\u2013732, 2015. 3\n[44] Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.\nAlgorithmic regularization in over-\nparameterized matrix sensing and neural networks with quadratic activations. In COLT,\n2018. 7\n[45] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up: A guide\nto parameter-efficient fine-tuning. arXiv preprint arXiv:2303.15647, 2023. 3\n[46] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features:\nA new baseline for efficient model tuning. In NeurIPS, 2022. 3\n[47] Jun Hao Liew, Scott Cohen, Brian Price, Long Mai, and Jiashi Feng. Deep interactive thin\nobject selection. In WACV, 2021. 8\n[48] Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language\nmodel via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. 1\n[49] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and\nColin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context\nlearning. In NeurIPS, 2022. 3\n[50] Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, and Le Song. Learning\ntowards minimum hyperspherical energy. In NeurIPS, 2018. 3\n[51] Weiyang Liu, Zhen Liu, James M Rehg, and Le Song. Neural similarity learning. In NeurIPS,\n2019. 7\n[52] Weiyang Liu, Rongmei Lin, Zhen Liu, James M Rehg, Liam Paull, Li Xiong, Le Song, and\nAdrian Weller. Orthogonal over-parameterized training. In CVPR, 2021. 3, 5\n[53] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert\npretraining approach, 2019. 7\n[54] Gen Luo, Minglang Huang, Yiyi Zhou, Xiaoshuai Sun, Guannan Jiang, Zhiyu Wang, and\nRongrong Ji. Towards efficient visual adaption via structural re-parameterization. arXiv\npreprint arXiv:2302.08106, 2023. 3\n[55] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. Peft:\nState-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft,\n2022. 20\n[56] Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih,\nand Madian Khabsa. Unipelt: A unified framework for parameter-efficient language model\ntuning. arXiv preprint arXiv:2110.07577, 2021. 3\n14\nTechnical Report\n[57] Eric Michielssen and Amir Boag. A multilevel matrix decomposition algorithm for analyzing\nscattering from large structures. IEEE Transactions on Antennas and Propagation, 1996. 3\n[58] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normaliza-\ntion for generative adversarial networks. In ICLR, 2018. 6\n[59] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu\nQie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 9\n[60] Marina Munkhoeva, Yermek Kapushev, Evgeny Burnaev, and Ivan Oseledets. Quadrature-\nbased features for kernel approximation. In NeurIPS, 2018. 3\n[61] Michael O\u2019Neil, Franco Woolfe, and Vladimir Rokhlin. An algorithm for the rapid evaluation\nof special function transforms. Applied and Computational Harmonic Analysis, 2010. 3\n[62] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,\nPierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2:\nLearning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.\n7, 8, 19\n[63] Douglass Stott Parker. Random butterfly transformations with applications in computational\nlinear algebra. UCLA Computer Science Department, 1995. 3, 30\n[64] John Peca-Medlin. Numerical, spectral, and group properties of random butterfly matrices.\nUniversity of California, Irvine, 2021. 30, 31, 32\n[65] Jonas Pfeiffer, Aishwarya Kamath, Andreas R\u00a8uckl\u00b4e, Kyunghyun Cho, and Iryna Gurevych.\nAdapterfusion: Non-destructive task composition for transfer learning.\narXiv preprint\narXiv:2005.00247, 2020. 1, 7\n[66] Xuebin Qin, Hang Dai, Xiaobin Hu, Deng-Ping Fan, Ling Shao, and Luc Van Gool. Highly\naccurate dichotomous image segmentation. In ECCV, 2022. 8\n[67] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian\nWeller, and Bernhard Sch\u00a8olkopf. Controlling text-to-image diffusion by orthogonal finetuning.\nIn NeurIPS, 2023. 1, 2, 3, 5, 7, 9, 20\n[68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021. 2\n[69] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. JMLR, 2020. 1\n[70] Indra Rajasingh, Paul Manuel, N Parthiban, D Azubha Jemilet, and R Sundara Rajan. Trans-\nmission in butterfly networks. The Computer Journal, 2016. 3\n[71] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains\nwith residual adapters. In NeurIPS, 2017. 1\n[72] Steven Roman, S Axler, and FW Gehring. Advanced linear algebra, volume 3. Springer, 2005.\n6\n[73] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 2, 7, 9, 20\n[74] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aber-\nman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.\nIn CVPR, 2023. 9, 20\n[75] Yan Solihin. Fundamentals of parallel multicore architecture. CRC Press, 2015. 5\n15\nTechnical Report\n[76] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In\nICLR, 2021. 20\n[77] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overfitting. JMLR, 2014. 6\n[78] Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with fixed sparse\nmasks. NeurIPS, 2021. 3\n[79] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder side-tuning for parameter and\nmemory efficient transfer learning. In NeurIPS, 2022. 3\n[80] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/stanford alpaca, 2023. 7, 20\n[81] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 7\n[82] Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R Ciosici,\nMichael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, et al. Efficient methods for\nnatural language processing: A survey. Transactions of the Association for Computational\nLinguistics, 2023. 3\n[83] Thomas Trogdon. On spectral and numerical properties of random butterfly matrices. Applied\nMathematics Letters, 95:48\u201358, 2019. 30\n[84] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. Dylora: Parameter\nefficient tuning of pre-trained models using dynamic search-free low-rank adaptation. arXiv\npreprint arXiv:2210.07558, 2022. 3\n[85] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif\nRasul, Mishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models.\nhttps://github.com/huggingface/diffusers, 2022. 20\n[86] Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. Spot: Better frozen model\nadaptation through soft prompt transfer. In ACL, 2022. 3\n[87] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding, 2019.\n7, 19\n[88] Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and\nJianfeng Gao. Adamix: Mixture-of-adapter for parameter-efficient tuning of large language\nmodels. In EMNLP, 2022. 3\n[89] Eric Xing, Michael Jordan, Stuart J Russell, and Andrew Ng. Distance metric learning with\napplication to clustering with side-information. In NeurIPS, 2002. 6\n[90] Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generaliz-\nability of deep learning. arXiv preprint arXiv:1705.10941, 2017. 6\n[91] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok,\nZhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical\nquestions for large language models. arXiv preprint arXiv:2309.12284, 2023. 7, 20\n[92] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple Parameter-efficient\nFine-tuning for Transformer-based Masked Language-models. In ACL, 2022. 1, 3, 7, 8\n[93] Yi Zeng, Pingping Zhang, Jianming Zhang, Zhe Lin, and Huchuan Lu. Towards high-resolution\nsalient object detection. In ICCV, 2019. 8\n16\nTechnical Report\n[94] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme,\nMario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy,\net al. A large-scale study of representation learning with the visual task adaptation benchmark.\narXiv preprint arXiv:1910.04867, 2019. 8\n[95] Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, and Yiming Qian.\nIncrelora: Incremental parameter allocation method for parameter-efficient fine-tuning. arXiv\npreprint arXiv:2308.12043, 2023. 3\n[96] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image\ndiffusion models. In ICCV, 2023. 9, 20\n[97] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,\nand Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In ICLR, 2023.\n1, 3, 7, 19\n[98] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv preprint\narXiv:2206.04673, 2022. 3\n[99] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.\nScene parsing through ade20k dataset. In CVPR, 2017. 20\n[100] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio\nTorralba. Semantic understanding of scenes through the ade20k dataset. International Journal\nof Computer Vision, 2019. 20\n[101] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and\nChen Change Loy. CelebV-HQ: A large-scale video facial attributes dataset. In ECCV, 2022.\n20\n[102] Bojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-Fai Wong, and Lei Zhang. Delta-\nlora: Fine-tuning high-rank parameters with the delta of low-rank matrices. arXiv preprint\narXiv:2309.02411, 2023. 3\n17\nTechnical Report\nAppendix\nTable of Contents\nA\nExperimental Details\n19\nA.1\nNatural Language Understanding . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nA.2\nTransfer Learning on VTAB-1K . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nA.3\nExperimental Details in Llama Finetuning . . . . . . . . . . . . . . . . . . . .\n19\nA.4\nExperimental Details in SAM Finetuning\n. . . . . . . . . . . . . . . . . . . .\n20\nA.5\nExperimental Details in ControlNet and DreamBooth . . . . . . . . . . . . . .\n20\nB\nMore Qualitative Results on High-quality Segmentation\n21\nC\nMore Qualitative Results in Subject-driven Generation\n22\nD\nMore Qualitative Results in Controllable Generation\n26\nE\nMathematical Question-Answering Case Study\n28\nF\nProof of Theorem 1\n29\nG\nMathematical Properties of Butterfly Matrices\n30\nG.1\nBalanced Entry-wise Learning Rate\n. . . . . . . . . . . . . . . . . . . . . . .\n30\nG.2\nAn Alternative Definition of Orthogonal Butterfly Matrices . . . . . . . . . . .\n30\nG.3\nTopological Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nG.4\nInput Sensitivity\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nG.5\nRandom Orthogonal Butterfly Matrices\n. . . . . . . . . . . . . . . . . . . . .\n32\nH\nIllustration of Block Butterfly Matrices\n33\n18\nTechnical Report\nA\nEXPERIMENTAL DETAILS\nA.1\nNATURAL LANGUAGE UNDERSTANDING\nFor our experiments on the GLUE benchmark [87], we follow the setting of [97] and only tune\nthe learning rate, the multiplicative dropout rate, and the number of training epochs. We use the\npre-trained DeBERTaV3 [25]1 as our base model and apply the OFT and BOFT to every linear layer\nin every transformer blocks. All runs can be trained on a single NVIDIA A100-SXM4-80GB GPU.\nSee the hyperparameters used in our runs in Table 7.\nMethod\nDataset\nMNLI\nSST-2\nMRPC\nCoLA\nQNLI\nQQP\nRTE\nSTS-B\nDeBERTaV3-base\nOFT (16)\nBatch Size\n32\n32\n32\n32\n32\n32\n32\n32\n# Epochs\n5\n2\n14\n5\n4\n9\n34\n11\nLearning Rate\n8E-05\n2E-04\n9E-04\n4E-04\n2E-04\n3E-04\n3E-04\n7E-04\nOFT Dropout\n1E-01\n1E-01\n1E-01\n5E-02\n1E-01\n1E-01\n5E-02\n1E-01\nMax Seq. Len.\n256\n128\n320\n64\n512\n320\n320\n128\nDeBERTaV3-base\nOFT (8,1)\nBatch Size\n32\n32\n32\n32\n32\n32\n32\n32\n# Epochs\n10\n11\n16\n15\n4\n10\n6\n8\nLearning Rate\n7E-05\n5E-05\n8E-04\n5E-04\n2E-04\n3E-04\n4E-04\n7E-04\nOFT Dropout\n15E-02\n15E-02\n1E-01\n15E-02\n5E-02\n5E-02\n5E-02\n5E-02\nMax Seq. Len.\n256\n128\n320\n64\n512\n320\n320\n128\nTable 7: Hyperparameter setup used for DeBERTaV3-base on the GLUE benchmark.\nA.2\nTRANSFER LEARNING ON VTAB-1K\nIn our VTAB-1K experiments, we employ the DINOv2-large [62]2 as our base model for fine-tuning.\nOur architecture design aligns with GLoRA [6], injecting trainable OFT and BOFT weights into every\nlinear layer in all multihead self-attention (MSA) and MLP blocks. To ensure a fair comparison, we\nmaintain the identical training setups for our and the baseline methods: a total number of 30 training\nepochs, a fixed training batch size of 64, an AdamW optimizer, and a cosine learning rate scheduler\nwith a warmup ratio of 0.1. Notably, due to the supernet structure of GLoRA [6], we tested both\ntraining for 30 and 100 epochs. We conduct a grid search on the learning rate for both our method\nand baseline methods and report the best final test set\u2019s Top-1 classification accuracy after the final\nepoch. For BOFT and OFT, we additionally adopt a multiplicative dropout rate of 0.1 and apply a\nweight decay rate of 0.02. All methods are trained on a single NVIDIA A100-SXM4-80GB GPU.\nThe exact learning rate details for OFT and BOFT can be found in Table 8.\nDataset\nCifar100\nCaltech101\nDTD\nFlowers102\nPets\nSVHN\nSun397\nCamelyon\nEuroSAT\nResisc45\nRetinopathy\nClevr-Count\nClevr-Dist\nDMLab\nKITTI-Dist\ndSpr-Loc\ndSpr-Ori\nsNORB-Azim\nsNORB-Ele\nOFT b=16\n8e-4\n5e-4\n6e-4\n2e-3\n3e-4\n3e-3\n1e-3\n9e-4\n9e-4\n5e-4\n1e-3\n2e-4\n4e-4\n2e-3\n1e-3\n2e-3\n3e-4\n4e-3\n6e-4\nBOFT m=4\nb=4\n8e-4\n5e-4\n6e-4\n2e-3\n3e-4\n3e-3\n1e-3\n1e-3\n6e-4\n9e-4\n2e-3\n2e-4\n3e-4\n3e-3\n9e-4\n4e-3\n5e-4\n5e-3\n8e-4\nBOFT m=6\nb=2\n8e-4\n9e-4\n1e-3\n1e-3\n8e-4\n3e-3\n1e-3\n4e-3\n9e-4\n1e-3\n2e-3\n4e-4\n4e-4\n4e-3\n3e-3\n4e-3\n8e-4\n4e-3\n9e-4\nTable 8: Hyperparameter setup (i.e., learning rate) used for DINOv2-large on the VTAB-1K benchmark.\nA.3\nEXPERIMENTAL DETAILS IN LLAMA FINETUNING\nIn the Llama-related finetuning experiments, language understanding and mathematical question\nanswering, we fixed the batch size as 64 and the training epoch as 2. For all the Lora, OFT, and BOFT\nexperiments, we use the cosine learning scheduler and the warmup of the first 100 learning steps.\nWe finetune the Llama model on the first generated 512 tokens, which is sufficient for these two\ntasks. We use the AdamW optimizer with a 1e-4 learning rate and 8e-4 learning rate for the language\nunderstanding task and mathematical question-answering task, respectively. The multiplicative\ndropout used for language understanding and mathematical question answering is 0.1 and 0.05,\n1https://huggingface.co/microsoft/deberta-v3-base\n2https://huggingface.co/facebook/dinov2-large\n19\nTechnical Report\nrespectively. For language understanding, we evaluate the performance on MMLU with both zero-\nshot and 5-shot evaluation. For mathematical question answering, we basically follow the evaluation\ntools in MetaMathQA [91], where they use the Alpaca [80] prompt and evaluate the model in\nzero-shot. The generation temperature is set as 0 for both tasks.\nA.4\nEXPERIMENTAL DETAILS IN SAM FINETUNING\nWe generally follow the training and evaluation settings as HQ-SAM [34]. Specifically, we use a\nlearning rate of 0.0005, a cosine annealing learning rate scheduler, AdamW optimizer with a weight\ndecay rate of 0.01 and a multiplicative dropout rate of 0.005. During fine-tuning, we keep the SAM\nmodel [35]3 frozen.\nA.5\nEXPERIMENTAL DETAILS IN CONTROLNET AND DREAMBOOTH\nFor our experiments on the ControlNet [96] and DreamBooth [74], we mainly follow the setting\nof OFT [67] but re-implement them using HuggingFace\u2019s Diffusers [85] and Parameter-Efficient\nFine-Tuning (PEFT) [55]4. Specifically, we use Stable Diffusion [73] (v2.1)5 as our pretrained model,\nand DDIMScheduler [76] as our scheduler function. The attached fine-tuned PEFT layers within UNet\nare {to q, to v, to k, query, value, key}. Both training and testing are conducted\non NVIDIA A100-SXM4-80GB, memory efficient attention from xFormers [38] is employed. Some\nspecific settings for ControlNet and DreamBooth are as follows:\nControlNet\nWe train ControlNet image encoder (lightweight 8-conv-layer network same as Con-\ntrolLoRA [26]) and the attached PEFT layers within UNet, with the learning rate of 1e-5. Regarding\nthe optimizer, we use AdamW with a weight decay as 1e-2, adam epsilon as 1e-8, and a constant learn-\ning rate scheduler. For datasets, we train Segmentation-to-Image (S2I) task on ADE20K [99, 100],\nand Landmark-to-Face (L2F) task on CelebV-HQ [101], both for 20 epochs, with 16 batch size.\nSpecifically, we employ dropout (p2 = 0.1) and {PEFT} only bias type on PEFT layers (BOFT,\nOFT, LoRA).\nDreamBooth\nInstead of fine-tuning text transformer [20, 37], we exclusively fine-tune the cross-\nattention layers (K, V, Q) of UNet with a learning rate of 3e-5, batch size of 4 for 2000 steps.\nRegarding the optimizer, we use AdamW with a weight decay as 1e-2 and Adam epsilon as 1e-8,\na constant learning rate scheduler. Furthermore, we pre-generate 200 images conditioned on each\nclass of input images, for prior preservation training weighted by 1.0. We use the same dataset as\nDreamBooth [74] with the resolution of 512. Same as ControlNet, we employ dropout (p2 = 0.1)\nand {PEFT} only bias type on PEFT layers (BOFT, OFT, LoRA).\nTask\nMetric\nLoRA\nOFT\nBOFT\n# Params\n2.52M\n20.89M\n20.76M\nS2I\nmIoU \u2191\n24.72\n29.44\n28.83\nmAcc \u2191\n37.88\n42.12\n41.24\naAcc \u2191\n60.43\n67.53\n67.74\nL2F\nError \u2193\n8.038\n6.537\n5.667\nTable 9: Quantitative evaluation of S2I and L2F. The\nbest results are marked with \u201cbold\u201d, and the second\nbest results are marked with \u201cunderline\u201d.\nMore Results\nFor ControlNet, apart from Landmark-to-\nFace (L2F) generation, we also benchmark different fine-\ntuning methods on Segmentation-to-Image (S2I) generation.\nThe quantitative results are given in Table 9. Notably, these\nreported numbers represent the best possible results achieved\nby methods, across all variations of parameter configurations\n(#Param spans from 2M to 20M), trained for 20 epochs. We\nobserve that different control task requires different finetun-\ning flexbility and some control task may be easier than the\nother. We find that the S2I task is generally easier than the L2F task and does not require strong\nfinetuning flexibility. In the S2I task, the best performance of BOFT from 2M to 20M is similar\nto that of OFT. However, BOFT can still achieve better control performance than OFT given less\namount of parameter budget, demonstrating its parameter efficiency.\nMore qualitative results of ControlNet are shown in Figure 17 and Figure 16. Regarding DreamBooth,\nmore subject-driven generations are shown from Figure 9 to Figure 15.\n3https://github.com/facebookresearch/segment-anything\n4https://huggingface.co/docs/peft\n5https://huggingface.co/stabilityai/stable-diffusion-2-1\n20\nTechnical Report\nB\nMORE QUALITATIVE RESULTS ON HIGH-QUALITY SEGMENTATION\nSAM\nBOFT-SAM\nSAM\nBOFT-SAM\nSAM\nBOFT-SAM\nBOFT-SAM\nSAM\nFigure 11: More qualitative comparison of mask prediction between SAM and BOFT-SAM.\n21\nTechnical Report\nC\nMORE QUALITATIVE RESULTS IN SUBJECT-DRIVEN GENERATION\nText prompt: a [V] backpack floating on top of water\nText prompt: a [V] backpack in the snow\nText prompt: a [V] backpack on top of a dirt road\nText prompt: a [V] teapot with a tree and autumn leaves in the background\nText prompt: a [V] teapot on top of a wooden floor\nText prompt: a [V] teapot on top of a dirt road\nLoRA\nOFT\nBOFT\nInput images\nFigure 12: Qualitative comparison of Subject-driven Generation.\n22\nTechnical Report\nText prompt: a [V] vase on a cobblestone street\nText prompt: a [V] vase on top of the sidewalk in a crowded street\nText prompt: a [V] vase on top of a white rug\nText prompt: a [V] sneaker on top of a dirt road\nText prompt: a [V] sneaker on top of green grass with sunflowers around it\nText prompt: a [V] sneaker with a city in the background\nLoRA\nOFT\nBOFT\nInput images\nFigure 13: Qualitative comparison of Subject-driven Generation.\n23\nTechnical Report\nText prompt: a [V] glasses with a blue house in the background\nText prompt: a [V] glasses with a mountain in the background\nText prompt: a [V] glasses on top of a white rug\nText prompt: a [V] toy on the beach\nText prompt: a shiny [V] toy\nText prompt: a [V] toy with a wheat field in the background\nLoRA\nOFT\nBOFT\nInput images\nFigure 14: Qualitative comparison of Subject-driven Generation.\n24\nTechnical Report\nText prompt: a [V] dog with a blue house in the background\nText prompt: a [V] dog wearing a santa hat\nText prompt: a [V] dog on top of a purple rug in a forest\nText prompt: [V] stuffed animal in the jungle\nText prompt: a [V] stuffed animal floating on top of water\nText prompt: a wet [V] stuffed animal\nLoRA\nOFT\nBOFT\nInput images\nFigure 15: Qualitative comparison of Subject-driven Generation.\n25\nTechnical Report\nD\nMORE QUALITATIVE RESULTS IN CONTROLLABLE GENERATION\nText prompt: a man with a beard\nGT\nLoRA\nControl signal\nOFT\nBOFT\nText prompt: a man with a beard\nGT\nLoRA\nControl signal\nOFT\nBOFT\nText prompt: a man with sunglasses on\nGT\nLoRA\nControl signal\nOFT\nBOFT\nText prompt: a man with grey hair and beards\nGT\nLoRA\nControl signal\nOFT\nBOFT\nText prompt: a man with long hair\nGT\nLoRA\nControl signal\nOFT\nBOFT\nText prompt: a man with wet hair\nGT\nLoRA\nControl signal\nOFT\nBOFT\nText prompt: a woman wearing sunglasses and holding a rose\nGT\nLoRA\nControl signal\nOFT\nBOFT\nText prompt: a woman wearing sunglasses and holding a rose\nGT\nLoRA\nControl signal\nOFT\nBOFT\nFigure 16: More qualitative results in landmark-to-face generation.\n26\nTechnical Report\nText prompt: a house\nGT\nLoRA\nControl signal\nOFT\nBOFT\nGT\nLoRA\nControl signal\nOFT\nBOFT\nText prompt: a brick building\nGT\nLoRA\nControl signal\nOFT\nBOFT\nGT\nLoRA\nControl signal\nOFT\nBOFT\nGT\nLoRA\nControl signal\nOFT\nBOFT\nGT\nLoRA\nControl signal\nOFT\nBOFT\nGT\nLoRA\nControl signal\nOFT\nBOFT\nGT\nLoRA\nControl signal\nOFT\nBOFT\nText prompt: a building with stairs leading up to it\nText prompt: a thomas train at disneyland world\nText prompt: a man with a beard\nText prompt: people looking at an aquarium\nText prompt: an empty room\nText prompt: a conference room\nFigure 17: More qualitative results in segmentation map-to-image generation.\n27\nTechnical Report\nE\nMATHEMATICAL QUESTION-ANSWERING CASE STUDY\nExample E.1: Case Study 1\nQuestion: James buys 5 packs of beef that are 4 pounds each. The price of beef is $5.50 per pound. How\nmuch did he pay? (Ground-truth answer is 110)\nLlama-2 Answer: James bought 5 packs of beef that are 4 pounds each. The price of beef is $5.50 per pound.\nSo he paid 5 * 4 * 5.50 = 137.50 dollars. The answer is 137.50.\nBOFT Answer: James bought 5 packs of beef, and each pack is 4 pounds, so he bought a total of 5 * 4 = 20\npounds of beef. The price of beef is $5.50 per pound, so James paid 20 * $5.50 = $110. The answer is: 110\nExample E.2: Case Study 2\nQuestion: James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total\nmeters does he run a week? (Ground-truth answer is 540)\nLlama-2 Answer: James runs 60 meters each sprint. So he runs 60 * 3 = 180 meters each week. The answer\nis 180.\nBOFT Answer: James runs 3 sprints 3 times a week, so he runs 3 sprints x 3 times = 9 sprints in a week.\nEach sprint is 60 meters, so James runs 9 sprints x 60 meters = 540 meters in a week. Therefore, James runs a\ntotal of 540 meters in a week. The answer is: 540\nExample E.3: Case Study 3\nQuestion: Carl buys ten packs of cookies. Each pack of cookies has six cookies inside. Each cookie cost\n$0.10. How much change does Carl receive if he pay with a $10 bill? (Ground-truth answer is 4)\nLlama-2 Answer: Carl buys ten packs of cookies. Each pack of cookies has six cookies inside. Each cookie\ncost $0.10. So he pays $10 for 60 cookies. Now he gets 60 * 0.10 = $6 change. The answer is $6.\nBOFT Answer: Carl buys ten packs of cookies, so he buys 10 * 6 = 60 cookies.The total cost of the cookies\nis 60 * $0.10 = $6.Carl pays with a $10 bill, so he receives $10 - $6 = $4 in change. The answer is: 4\nExample E.4: Case Study 4\nQuestion: Miguel uses 2 pads of paper a week for his drawing. If there are 30 sheets of paper on a pad of\npaper, how many sheets of paper does he use every month? (Ground-truth answer is 240)\nLlama-2 Answer: Miguel uses 2 pads of paper a week. There are 30 sheets of paper on a pad of paper. So he\nuses 2 * 30 = 60 sheets of paper every week. Now he uses 60 * 4 = 240 sheets of paper every month. The\nanswer is 240.\nBOFT Answer: Miguel uses 2 pads of paper a week, and there are 30 sheets of paper on a pad, so he uses 2\nx 30 = 60 sheets of paper every week. Since there are 4 weeks in a month, Miguel uses 60 x 4 = 240 sheets of\npaper every month. Therefore, Miguel uses 240 sheets of paper every month. The answer is: 240\nExample E.5: Case Study 5\nQuestion: After transferring to a new school, Amy made 20 more friends than Lily. If Lily made 50 friends,\nhow many friends do Lily and Amy have together? (Ground-truth answer is 120)\nLlama-2 Answer: Amy made 20 more friends than Lily. So Lily had 50 - 20 = 30 friends. Amy had 30 + 20\n= 50 friends. So Lily and Amy have 50 + 50 = 100 friends together. The answer is 100.\nBOFT Answer: If Lily made 50 friends, and Amy made 20 more friends than Lily, then Amy made 50 + 20\n= 70 friends. Together, Lily and Amy have 50 + 70 = 120 friends. The answer is: 120\nExample E.6: Case Study 6\nQuestion: After Andrea saved some money, she then spent the rest of her money on an $11 sweater and gave\nher brother $4. If she had $36 in the beginning, how much did Andrea save? (Ground-truth answer is 21)\nLlama-2 Answer: Andrea started with 36 dollars. She then saved some money. Then she spent the rest of\nher money on an $11 sweater and gave her brother $4. So she spent 36 - (saved money) - (sweater price) -\n(brother price) = 36 - 10 - 11 - 4 = 11 dollars. The answer is 11.\nBOFT Answer: After Andrea saved some money, she then spent the rest of her money on an $11 sweater\nand gave her brother $4. If she had $36 in the beginning, how much did Andrea save?Andrea saved $36 - $11\n- $4 = $21Therefore, Andrea saved $21. The answer is: 21\n28\nTechnical Report\nF\nPROOF OF THEOREM 1\nWe first show that BOFT is always more expressive than OFT with the same block size. We write\ndown the final orthogonal matrix in BOFT with block size 2b:\nRBOFT(m, b) =\nm\nY\ni=1\n\u02dc\nBb(d, 2m\u2212i+1)\n= \u02dc\nBb(d, 2m) \u00b7 \u00b7 \u00b7 \u02dc\nBb(d, 2) \u00b7 \u02dc\nBb(d, 21)\n(3)\nwhere we can let \u02dc\nBb(d, 2m), m \u2208 [2, m] to be identity matrices. Then we have\nRBOFT(m, b) = \u02dc\nBb(d, 2m) \u00b7 \u00b7 \u00b7 \u02dc\nBb(d, 22)\n|\n{z\n}\n=I\n\u00b7 \u02dc\nBb(d, 2)\n= \u02dc\nBb(d, 2)\n(4)\nwhich is an orthogonal matrix with the block size 2b. This is exactly the orthogonal matrix used in\nOFT with the block size 2b. Therefore, BOFT with m > 1 is always more expressive than OFT with\nthe same block size. When m = 1, BOFT reduces to OFT.\nThen we proceed to prove that the following expression can represent any orthogonal matrix:\nRExt = Bd\u22121,1(d)B\u22a4\nd\u22121,2(d) \u00b7 \u00b7 \u00b7 B1,1(d)B\u22a4\n1,2(d)\n(5)\nwhere Bi,j(d), \u2200i, \u2200j are butterfly matrices. RExt fall into the category of kaleidoscope matrices [14]\n(with the diagonal matrix being an identity). [14] has shown that the orthogonal kaleidoscope matrix\ncan represent any orthogonal matrices. The overall proof idea is simple and can be given by the\nfollowing two results from [14]:\n\u2022 Any orthogonal matrix can be represented by QR factorization which can be decomposed\nby n \u2212 1 Householder reflections.\n\u2022 All Householder reflections can be represented by Bi(d)B\u22a4\ni (d).\nThen we can easily arrive at the conclusion that RExt can represent any orthogonal matrix.\n29\nTechnical Report\nG\nMATHEMATICAL PROPERTIES OF BUTTERFLY MATRICES\nDue to its nice spectral and numerical properties, butterfly structures (especially random butterfly\nmatrices) are introduced by [63] to remove the need of pivoting in Gaussian elimination. In this\nsection, we discuss a few intriguing mathematical properties of butterfly matrices. Because orthogonal\nbutterfly matrices are a special subset of the orthogonal matrices, they introduce additional inductive\nbiases that help to further regularize the finetuned model. To gain a deeper understanding of the\ninduced inductive biases, we delve into the mathematical properties of orthogonal butterfly matrices\nwith the hope to understand the effect of such additional inductive biases. We note that these properties\nare natural and direct consequences of the established results in [64, 83]. For our results to be self-\ncontained, we also provide the brief proof of the results here.\nG.1\nBALANCED ENTRY-WISE LEARNING RATE\nButterfly matrices also have an interesting balanced learning rate property. We take 8-dimensional\nbutterfly matrix as an example. Assume that we fill all the entries in each butterfly component with 1.\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u00b7\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u00b7\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n1\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n1\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nwhich means that the butterfly matrix preserves the learning rate in each entry of the butterfly\ncomponent. For block butterfly matrices, we consider the following example:\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n1\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n1\n1\n1\n1\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n1\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u00b7\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n1\n1\n1\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n1\n1\n1\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nwhich shows that the product of all-one block butterfly components still have the balanced learning\nrate property. However, we also note that whenever we initialize a butterfly component, the elements\nare not the same (Each butterfly component is initialized as an identity matrix). While butterfly\nmatrices have the property of balanced entry-wise learning rate, we note that the same property does\nnot necessarily exist for general sparse matrix factorization.\nG.2\nAN ALTERNATIVE DEFINITION OF ORTHOGONAL BUTTERFLY MATRICES\nIn the main paper, we define the butterfly matrices by first constructing the pattern of non-zero\nelements in each butterfly component and then constraining each block matrix as an orthogonal\nmatrix. Alternatively, orthogonal butterfly matrices can also be defined in an reverse order by first\nconstructing orthogonal matrices and then build the butterfly matrix from ground up.\n30\nTechnical Report\nDefinition 1 (Generalized Rotation Matrix). A generalized rotation matrix is a N \u00d7 N matrix (N is\neven) of the following form:\n\u0014\nC\nS\n\u2212S\nC\n\u0015\n,\n(6)\nwhere C and S are commuting, symmetric real matrices of the size N\n2 \u00d7 N\n2 , and C2 + S2 = I.\nThe scalar rotation matrices use scalar matrices C, S. The diagonal rotation matrices use diagonal\nmatrices C, S.\nDefinition 2 (Orthogonal Butterfly Matrix). An orthogonal butterfly matrix, denoted collectively as\nOB(N), is an iteratively defined matrix of order N = 2n:\n\u0014\nCA1\nSA2\n\u2212SA1\nCA2\n\u0015\n=\n\u0014\nC\nS\n\u2212S\nC\n\u0015\n\u00b7\n\u0014\nA1\n0\n0\nA2\n\u0015\n,\n(7)\nwhere A1, A2 \u2208 OB( N\n2 ), and\n\u0014\nC\nS\n\u2212S\nC\n\u0015\nis a generalized rotation matrix. An orthogonal butterfly\nmatrix is simple if A1 = A2 at each iteration step, and is non-simple otherwise. When N = 1, the\northogonal butterfly matrix is defined as 1.\nDefinition 3 (Diagonal and Scalar Butterfly Matrix). The diagonal and scalar butterfly matrices are\nthe orthogonal butterfly matrices constructed iteratively with diagonal and scalar rotation matrices,\nrespectively. We denote the order-N simple scalar butterfly matrices as OBs(N). For B \u2208 OBs(N),\nwe define B = OB(A, \u03b8) where A = OBs( N\n2 ) if B is of the following form:\n\u0014\ncos(\u03b8)A\nsin(\u03b8)A\n\u2212 sin(\u03b8)A\ncos(\u03b8)A\n\u0015\n=\n\u0014\ncos(\u03b8)I\nsin(\u03b8)I\n\u2212 sin(\u03b8)I\ncos(\u03b8)I\n\u0015\n\u00b7\n\u0014\nA\n0\n0\nA\n\u0015\n=\n\u0014\nA\n0\n0\nA\n\u0015\n\u00b7\n\u0014\ncos(\u03b8)I\nsin(\u03b8)I\n\u2212 sin(\u03b8)I\ncos(\u03b8)I\n\u0015\n.\n(8)\nWe use OB(N) to denote the non-simple scalar butterfly matrices.\nWe also define that\nOB(\u03b8) =\nn\nO\ni=1\nOB(n \u2212 i + 1) = OB(\u03b8n) \u2297 \u00b7 \u00b7 \u00b7 \u2297 OB(\u03b81).\n(9)\nwhere \u2297 denotes the Kronecker product and OB(\u03b8i) \u2208 SO(2).\nRemark 1. We note that the butterfly matrices considered in the main paper is what we define\nas non-simple diagonal butterfly matrices here. Despite not directly discussing the properties of\nsuch family of butterfly matrices, we aim to provide some useful insights through a few necessary\nsimplifications.\nG.3\nTOPOLOGICAL PROPERTIES\nProposition 1 ([64]). OB(N) and OBs(N) are compact spaces in SO(N), which are homeomorphic\nto quotients of higher dimensional tori Tn and TN\u22121 where N = 2n.\nProposition 2 ([64]). The diagonal simple and non-simple butterfly matrices are compact spaces in\nSO(N), which are homeomorphic to quotients of higher dimensional tori TN\u22121 and T\n1\n2 Nn (N = 2n),\nrespectively.\nThese two propositions characterize the topological properties of orthogonal butterfly matrices. There\nare also many interesting group properties. For example, OBs(N) is a compact abelian subgroup\nof SO(N). These topological and group structures and connections provide many unique inductive\nbiases for the orthogonal finetuning.\n31\nTechnical Report\nG.4\nINPUT SENSITIVITY\nOne interesting property to study for the butterfly matrices is how the input parameter changes the\noutput matrix norm. Specifically, we can upper bound the difference between the input-perturbed\nbutterfly matrix and the original butterfly matrix. Specifically, we have the following result from [64]:\nProposition 3 (Upper Bound for Simple Scalar Butterfly Matrices). Let OB(\u03b8) \u2208 OBs(N) and\n\u03f5 \u2208 Rn where N = 2n. Then we have that\n\u2225OB(\u03b8) \u2212 OB(\u03b8 + \u03f5)\u2225F \u2264\n\u221a\nN \u2225\u03f5\u22251\n(10)\nProposition 4 (Upper Bound for General Scalar Butterfly Matrices). Let OB(\u03b8) \u2208 OB(N) and\n\u03f5 \u2208 RN\u22121. Then we have that\n\u2225OB(\u03b8) \u2212 OB(\u03b8 + \u03f5)\u2225F \u2264\n\u221a\nN \u2212 1 \u2225\u03f5\u22252\n(11)\nThe above two propositions show that the map \u03b8 \u2192 OB(\u03b8) is Lipshitz continuous.\nG.5\nRANDOM ORTHOGONAL BUTTERFLY MATRICES\nDefinition 4 (Random Orthogonal Butterfly Matrix). A random orthogonal butterfly matrix is a\nbutterfly matrix that is generated by random generalized rotation matrices with\n\u03a3 := {Cj, Sj}j\u22651\n(12)\nwhich is an independent sequence of pairs of matrices. Each pair {Cj, Sj} generates a random\ngeneralized rotation matrix of order 2j\u22121\n32\nTechnical Report\nH\nILLUSTRATION OF BLOCK BUTTERFLY MATRICES\nWe provide an illustration for block butterfly components and matrices in Figure 18.\nB(16,8)\n~\nB(16,4)\n~\nB(16,2)\n~\nB(16,16)\n~\n(a) 16-dimensional butterfly matrix with the block size 1\nB (8,8)\n~\nB (8,4)\n~\nB (8,2)\n~ 2\n2\n2\n(b) 8-dimensional butterfly matrix with the block size 2\nB (4,4)\n~\nB (4,2)\n~ 4\n4\n(c) 4-dimensional butterfly matrix with the block size 4\nFigure 18: Block butterfly components and matrices with the block size 1, 2 and 4.\n33\n"
  },
  {
    "title": "Language Models can be Logical Solvers",
    "link": "https://arxiv.org/pdf/2311.06158.pdf",
    "upvote": "14",
    "text": "Language Models can be Logical Solvers\nJiazhan Feng1\u2217\nRuochen Xu2\nJunheng Hao2\nHiteshi Sharma2\nYelong Shen2\nDongyan Zhao1\nWeizhu Chen2\n1Peking University, Beijing\n2Microsoft Azure AI, Redmond\n{fengjiazhan,zhaody}@pku.edu.cn\n{ruox,junhenghao,hitshar,yeshe,wzchen}@microsoft.com\nAbstract\nLogical reasoning is a fundamental aspect\nof human intelligence and a key component\nof tasks like problem-solving and decision-\nmaking. Recent advancements have enabled\nLarge Language Models (LLMs) to potentially\nexhibit reasoning capabilities, but complex log-\nical reasoning remains a challenge. The state-\nof-the-art, solver-augmented language mod-\nels, use LLMs to parse natural language log-\nical questions into symbolic representations\nfirst and then adopt external logical solvers to\ntake in the symbolic representations and out-\nput the answers. Despite their impressive per-\nformance, any parsing errors will inevitably\nresult in the failure of the execution of the\nexternal logical solver and no answer to the\nlogical questions. In this paper, we introduce\nLOGIPT, a novel language model that directly\nemulates the reasoning processes of logical\nsolvers and bypasses the parsing errors by learn-\ning to strict adherence to solver syntax and\ngrammar. LOGIPT is fine-tuned on a newly\nconstructed instruction-tuning dataset derived\nfrom revealing and refining the invisible reason-\ning process of deductive solvers. Experimen-\ntal results on two public deductive reasoning\ndatasets demonstrate that LOGIPT outperforms\nstate-of-the-art solver-augmented LMs and few-\nshot prompting methods on competitive LLMs\nlike ChatGPT or GPT-4.\n1\nIntroduction\nLogical reasoning is a foundational element of hu-\nman intelligence, holding a pivotal role in tasks\nlike problem-solving, decision-making, and criti-\ncal thinking (Huang and Chang, 2023). Recently,\nsubstantial advancements have been achieved in\nthe field of NLP through the development of large\nlanguage models (LLMs) (OpenAI, 2022, 2023;\nGoogle, 2023; Touvron et al., 2023a,b). It has\n\u2217 Work done during Jiazhan\u2019s internship at Microsoft\nAzure AI.\nbeen noted that language models (LMs) could po-\ntentially display reasoning capabilities when they\nreach a certain scale threshold (e.g., training com-\npute, model parameters, etc.) (Kaplan et al., 2020;\nWei et al., 2022a; Hoffmann et al., 2022). To this\nend, LLMs can answer logical questions with ex-\nplicit reasoning steps when prompted with a simple\nsnippet: \u201cLet\u2019s think step by step.\u201d (Kojima et al.,\n2022) or step-wise explanations of reasoning (i.e.,\n\u201cchain of thoughts\u201d) (Wei et al., 2022b).\nWhile LLMs have made significant progress,\ncomplex logical reasoning remains challeng-\ning (Valmeekam et al., 2022; Liu et al., 2023b).\nSome prior work (Tafjord et al., 2022; Ling et al.,\n2023) aimed to enable LMs to perform logical rea-\nsoning via specialized module fine-tuning, where\nreasoning is in natural language (NL). However,\nthe ambiguity and complexity of NL can lead to\nundesired issues like hallucinations and unfaith-\nful reasoning (Saparov and He, 2023; Gao et al.,\n2023). To this end, recent work has begun to aug-\nment LLMs with access to external Solvers (Chen\net al., 2022; Ye et al., 2023; Pan et al., 2023). In\nthis paper, we focus on the logical solvers, which\nare theorem provers that can be any automated rea-\nsoning tool for checking the truth value of logical\nformulas in symbolic language (SL). Invoking log-\nical solvers can guarantee the accuracy of logical\nreasoning and relieve the burden of LLMs to exe-\ncute intricate and precise deductive reasoning.\nThe data flow of the aforementioned solver-\naugmented LMs is depicted in Figure 1(a). At\nthe outset, the information of logical questions is\nstored in NL. It is subsequently fed into a LM for\nparsing into a symbolic representation suitable for\nsolver-input format. Finally, the SL information\nis dispatched to a symbolic solver, which yields\nthe truth value of the logical question. However,\nduring this process, any NL-to-SL parsing errors\nwill inevitably result in the failure of the reasoning\nprocess and no answer to the question. In our pre-\narXiv:2311.06158v1  [cs.CL]  10 Nov 2023\nNL Logical \nQuestions\nLMs\nSymbolic \nSolvers\na) Solver-augmented LMs (only inference)\nNL Logical \nQuestions\nLMs\nSymbolic \nSolvers\nLoGiPT\nb) Our pipeline for fine-tuning \nNL Logical \nQuestions\nLoGiPT\nSL Reasoning \n& Answers\nc) Our pipeline for inference \nSL Reasoning \n& Answers\nOnly Answers\nNL-to-SL\nParse\nCollect Training Pairs\nIf syntax\nis valid\nRemedial \nMeasures \nOtherwise\nFine-\ntune\nNL-to-SL\nParse\nIf syntax\nis valid\nAll furry people \nare quiet.\nFurry($x, True) \u2192 \nQuiet($x, True)\nNL Context\nSL Facts/Rules/Query\nFigure 1: Data flow of current solver-augmented LMs for inference (a), and our pipeline for LOGIPT (b,c).\nliminary experiments, we observed that the parsing\nsuccessful rate (i.e., percentage of executable log-\nical formulations) of Vicuna-13B (Chiang et al.,\n2023) on ProofWriter (Tafjord et al., 2021) is only\n17%, significantly below the expected performance.\nIn addressing parsing failures, current methods ei-\nther directly use LLMs to reason in NL solely or\nrely on the solver\u2019s erroneous message to regen-\nerate parsing results, but these approaches don\u2019t\nfundamentally resolve the problem.\nIn this paper, we introduce LOGIPT, a novel LM\ndesigned to mimic the reasoning process of logi-\ncal solvers, enabling it to solve deductive reason-\ning tasks. We first construct an instruction-tuning\ndataset containing NL logical questions and their\ncorresponding solver\u2019s symbolic reasoning process.\nAfter filtering out cases having invalid syntax, we\nfine-tune open-source LMs like Vicuna or CodeL-\nlama (Roziere et al., 2023) with this data to create\nLOGIPT. Then, LOGIPT can generate all implied\nfacts given premises and rules, allowing us to deter-\nmine the truth value of a logical query by matching\nit with implied facts or outputting \u2018unknown\u2019 if it\ncannot be determined. The data flow of our pipeline\nis presented in Figure 1(b,c). We can bypass the\nsyntax or grammatical errors derived from NL-to-\nSL parsing by directly outputting the answers with\na fine-tuned LOGIPT.\nOur approach is akin to the process of distilla-\ntion, whereby we distill knowledge from a sym-\nbolic model (i.e., solver) into a neural network (i.e.,\nLM). However, the reasoning process of solvers\nis invisible to users and we can only obtain the\nanswers without intermediate reasoning steps. We\ndesign a pipeline to reveal and formalize solvers\u2019\ninvisible reasoning processes, creating instruction-\ntuning datasets with visible and interpretable sym-\nbolic reasoning steps (see Figure 3).\nOur main contributions are three-fold:\n\u2022 To the best of our knowledge, we are the\nfirst to propose empowering LLMs to directly\nlearn the reasoning process of logical solvers,\nthereby acquiring similar reasoning capability\nfor addressing deductive reasoning tasks.\n\u2022 Our proposed LOGIPT, can directly act as a\ndeductive solver and output all Facts implied\nfrom NL logical questions while bypassing\nthe syntax or grammatical errors derived from\nNL-to-SL parsing of solver-augmented LMs.\n\u2022 Evaluation results on two public deductive rea-\nsoning datasets show that LOGIPT can outper-\nform state-of-the-art solver-augmented LMs,\nand few-shot prompting methods on competi-\ntive LLMs like ChatGPT or GPT-4.\n2\nPreliminary\n2.1\nDeductive Reasoning\nDeductive reasoning is an essential type of logi-\ncal reasoning problem. It typically commences\nwith known facts and rules from logical context,\nthen proceeds through a series of inference steps\nuntil the query can be proved or disproved (Poole\nand Mackworth, 2010). In this paper, we consider\nthe Prolog logic programming language (Clocksin\nand Mellish, 2003; K\u00f6rner et al., 2022), which\nstands as the most prominent symbolic language\nfor describing deductive reasoning problems. We\nshowcased a deductive reasoning question along\nwith its corresponding Prolog syntax representa-\ntion in Figure 2.\nFor each question, we denote the NL description\nas Context. The Context can further be parsed\nContext:\nCharlie is green. (\u2026) All green, white people are nice. (\u2026) \nTrue, false, or unknown? Charlie is not green.\nFacts:\nGreen('Charlie', True)\nRules: Green($x, True) \u2227 White($x, True) \u2192 Nice($x, True)\nQuery:\nGreen('Charlie', False)\nFigure 2: A deductive reasoning question derived from\nProofWriter and its parsed Facts, Rules, and Query.\ninto Facts, Rules, and Query1. Specifically, a\nFact F = P(a1, \u00b7 \u00b7 \u00b7 , at) is a symbolic statement\nwith a predicate P and t arguments {a1, \u00b7 \u00b7 \u00b7 , at}\nwhere ai can be a variable, entity, number or bool.\nFor example, Green(\u2019Charlie\u2019, True) means\n\u201cCharlie is green\u201d; Rules are presented in the form\nof clauses F1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 Fm \u2192 Fm+1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 Fn,\nwhere Fi is a Fact.\nThe Rule means \u201cif each\nFi \u2208 {F1, \u00b7 \u00b7 \u00b7 , Fm} is true, then we can imply that\nall Facts in {Fm+1, \u00b7 \u00b7 \u00b7 , Fn} are also true.\u201d For\nexample, Furry($x, True) \u2192 Quiet($x, True)\nindicates if variable $x is furry, then $x is quiet; a\nQuery Q is also in the format of a Fact that needs\nto be proved based on Facts and Rules.\n2.2\nSolver-augmented LMs\nSolver-augmented LMs have demonstrated remark-\nable performance in deductive reasoning tasks. As\nshown in Figure 1(a), these model can be gener-\nally divided into two stages: Problem Formulation\n(from LMs to Symbolic Solvers) and Symbolic Rea-\nsoning (from Symbolic Solvers to Answers).\nIn Problem Formulation stage, an LM is used to\nparse an NL logical question into symbolic repre-\nsentation (Figure 2). The process can be accom-\nplished by providing LM with detailed instructions\nabout the grammar of Prolog, alongside a few\ndemonstrations as in-context examples (Ouyang\net al., 2022). The LM is expected to identify the\nsymbolic Facts, Rules, and Query from the NL\nlogical question following the instructions; In Sym-\nbolic Reasoning stage, a solver takes in the sym-\nbolic representation obtained in the previous stage\nand conducts symbolic reasoning. The reasoning\nprocess of external off-the-shelf solver, e.g., pyke\nexpert system (Frederiksen, 2008), is deterministic\nand invisible. Then, the truth value of the parsed\nQuery, which is the only output of the solver, can\n1In this paper, the term \u2018Query\u2019 refers to a specific sentence\nof statement or comment, while \u2018question\u2019 is used in a broader\nsense to denote the description of a logical problem.\nModel\nProofWriter\nPrOntoQA\nVicuna-13B\n17.00\n40.80\nCodeLlama-13B-Base\n0.33\n0.40\nCodeLlama-13B-Instruct\n71.33\n77.80\nTable 1: Parsing successful rate (%) of our selected\nopen-source LLMs on two deductive reasoning datasets.\nbe treated as the answer to the given question.\n2.3\nAnalysis on the Parsing Successful Rate\nThrough the aforementioned two phases, once\nthe solver-augmented LMs correctly formulate the\nproblem, the answers obtained through symbolic\nreasoning will be faithful, attributed to the deter-\nministic nature of the solver. However, this heav-\nily relies on the in-context learning capabilities of\nLMs. Therefore, we first calculate the parsing suc-\ncessful rate of three selected open-source LLMs\non two deductive reasoning datasets in Table 1.\nFirstly, we observe that CodeLlama-13B-Base\n(CodeLlama-13b-hf) is unable to effectively con-\nduct NL-to-SL parsing due to the limited in-context\nlearning capabilities in natural languages. Then we\ncan find that replacing the Base model with the\nInstruct version (CodeLlama-13b-Instruct-hf)\ncan alleviate this issue, which may be attributed\nto the fact that the Instruct version is further fine-\ntuned with an additional approx. 5B tokens to better\nfollow human instructions. Overall, open-source\nLLMs still exhibit parsing performance signifi-\ncantly lower than expected in some cases.\n3\nLoGiPT\nIn this paper, we aim to mitigate the parsing is-\nsue and present a novel LM, LOGIPT instructed\nto imitate the logical reasoning process of Solvers\nfor deductive reasoning tasks. To achieve this, we\nfirst reveal the solver reasoning process when solv-\ning logical problems (\u00a73.1). Then, we construct\na solver-derived instruction-tuning dataset, com-\nprising NL logical questions and corresponding\nSL reasoning steps (\u00a73.2). Finally, we fine-tune\nopen-source LLMs using this dataset to develop\nLOGIPT (\u00a73.3).\n3.1\nRevealing the Solver Reasoning Process\nBefore operating on the solvers, we first adopt\ngpt-4 as the problem formulator for NL-to-SL\nparsing with instructions about the grammar and\nTurn-1 (from human):\nTask Description: Given a statement containing logical facts \nand rules in natural languages as context, the task is to:\n1. define all the predicates in logic forms from the context\n2. parse the statement into known facts and rules based on the \ndefined predicates\n3. begin reasoning to obtain all implied facts\nContext:\nCharlie is green. Charlie is white. Dave is blue. Dave is \nfurry. Dave is white. Fiona is blue. Fiona is green. Fiona is \nnice. Fiona is rough. Fiona is white. All furry people are \nquiet. All green, white people are nice. If someone is blue \nand quiet then they are green.\nReasoning:\nTurn-2 (from LM):\nWe first define following predicates and corresponding natural \nlanguage explanations:\nGreen($x, bool) ::: Is x green?\n(... more predicates here ...)\nWe have following known facts from the context:\nGreen(\u2019Charlie\u2019, True)\n(... more facts here ...)\nWe have following known rules from the context:\nrule1: Furry($x, True) \u2192 Quiet($x, True)\nrule2: Green($x, True) \u2227 White($x, True) \u2192 Nice($x, True)\nrule3: Blue($x, True) \u2227 Quiet($x, True) \u2192 Green($x, True)\nNow begin reasoning to obtain all implied facts:\nUse rule1: Furry($x, True) \u2192 Quiet($x, True)\nBind $x to \u2019Dave\u2019\nObtain a new implied fact: Quiet(\u2019Dave\u2019, True)\nUnbind $x\nFinish implied with rule1\nUse rule2: Green($x, True) \u2227 White($x, True) \u2192 Nice($x, True)\nBind $x to \u2019Charlie\u2019\nObtain a new implied fact: Nice(\u2019Charlie\u2019, True)\nUnbind $x\nBind $x to \u2019Fiona\u2019\nObtain an already known or implied fact: Nice(\u2019Fiona\u2019, True)\nUnbind $x\nFinish implied with rule2\nUse rule3: Blue($x, True) \u2227 Quiet($x, True) \u2192 Green($x, True)\nBind $x to \u2019Dave\u2019\nObtain a new implied fact: Green(\u2019Dave\u2019, True)\nReuse rule2: Green($x, True) \u2227 White($x, True) \u2192 Nice($x, True)\nBind $x to \u2019Dave\u2019\nObtain a new implied fact: Nice(\u2019Dave\u2019, True)\nFinish implied with rule2\nUnbind $x\nBind $x to \u2019Fiona\u2019\nUnbind $x\nFinish implied with rule3\nFinally, we obtain following implied facts:\nGreen(\u2019Dave\u2019, True)\n(... more facts here ...)\nFinish reasoning\nTurn-3 (from human):\nQuestion: Based on the above information, is the following \ncomment true, false, or unknown? Dave is not green.\nOptions:\nA) True\nB) False\nC) Unknown\nNow you should:\n1. reason first carefully with the above known facts and \nimplied facts\n2. then output your answer strictly in the format of \u201cSo, the \ncorrect option is: X\u201d. The X should be replaced by \u201cA) True\u201d, \n\u201cB) False\u201d or \u201cC) Unknown\u201d\nTurn-4 (from LM):\nThe comment can be parsed into: Green(\u2019Dave\u2019, False)\nAccording to the implied fact: Green(\u2019Dave\u2019, True)\nThe comment contradicts the fact.\nSo, the correct option is: B) False\nFigure 3: A comprehensive 4-turn training example of our instruction-tuning data. We highlight the initial\noccurrences of each functionality described in \u00a73.1 using the corresponding colors. We omit some predicates and\nFacts in Turn-2 due to limited space. Hint: this figure is color-sensitive.\nfew-shot demonstrations2, and obtain the SL rep-\nresentations of all training logical questions of the\ngiven logical datasets. Then, consistent with solver-\naugmented methods, we adopt pyke expert system\nas the symbolic solver in this work that can make\ninferences using the Prolog symbolic language.\nGiven a logical question, pyke first sets up a knowl-\nedge base and injects all known Facts and Rules\n(Figure 2) from solver\u2019s inputs. Then, it iteratively\napplies Rules on already known or implied Facts,\naiming at obtaining more implied Facts until the\nQuery is proved or disproved.\nThe reasoning process executed by pyke solver\nis invisible to users and solver-augmented LMs\nuse the solver as a black-box. We hypothesis the\n\u2018chain-of-thought\u2019 reasoning process of the solver\nis valuable and LLMs are able to learn from it. To\nthis end, we first modify the source code of the\npyke3 to achieve the following functionalities:\n2Detailed instructions for NL-to-SL Parsing are shown in\nAppendix A and B.\n3https://pyke.sourceforge.net/\n1. For each application of a Rule, explicitly state\nthe Rule being \u2018Used\u2019, or \u2018Reused\u2019 if the\nRule has been applied before.\n2. When finishing the application of a Rule, ex-\nplicitly state the \u2018Finish\u2019 action.\n3. When assigning a value (e.g., an entity) to\na variable (e.g., $x) within a Fact in a Rule,\nexplicitly specify the variable being assigned\nusing \u2018Bind\u2019 and its corresponding value.\n4. Similarly, when the variable assignment is\ncomplete, provide an explicit indication via\n\u2018Unbind\u2019.\n5. When obtaining a new implied Fact, explicitly\nstate the \u2018New Fact obtained\u2019. If this Fact is\nan \u2018Already known or implied Fact\u2019, this\nshould also be noted explicitly.\n6. Upon the completion of reasoning, explicitly\ndisplay \u2018All newly implied Facts\u2019 in the\nknowledge base.\nWith the aforementioned instructions, we can\nobtain the revealed solver\u2019s reasoning process for\nthe construction of training data. We also high-\nlighted the initial occurrences of each functionality\nusing the corresponding colors in Figure 3 (Turn-\n2), where a case will be described in detail in the\nnext section.\n3.2\nConstructing the Instruction-tuning Data\nHowever, as previously mentioned, we cannot guar-\nantee that LMs can definitely complete the NL-to-\nSL parsing on arbitrary questions. To this end,\nwe first filter out all unsuccessfully parsed training\ncases that cannot be executed by pyke. Then we\nreorganize and refine the filtered training data to\nenhance the interpretability of the solver-derived\nreasoning steps. For each case, we divide the rea-\nsoning process into four conversational turns (Turn-\n1&3 for human and Turn-2&4 for LM), which\nwill be described elaborately in the following para-\ngraphs. We also provide a comprehensive training\nexample of our instruction-tuning data4 in Figure 3,\nand the full version is also included in Appendix C.\nTurn-1: Instructions & NL logical Context.\nFor each NL logical question within the training\nset, we begin by stripping away the specific Query\nstatement while retaining the question Context and\nsubsequently integrating it with elaborately crafted\ninstructions. Taking the case in Figure 3 as an ex-\nample, we temporarily exclude the Query \u2018Dave\nis not green\u2019 from the \u2018Context\u2019 field. Here, we\nonly consider Query-agnostic question description\nto ensure that LMs initially focus on the logical\nbackground itself. This is because sometimes the\nground-truth answer is \u2018Unknown\u2019 (e.g., cases in\nProofWriter). The truth value of the Query can-\nnot be inferred from the Context, and therefore we\nneed to deduce all implied Facts first.\nTurn-2: Query-agnostic Solver-derived Reason-\ning.\nAs we have acquired the solver\u2019s symbolic\nreasoning data in the revealing phase, our goal in\nTurn-2 is to further refine and enhance the reason-\ning process to achieve a more readable form of\nthe solver\u2019s reasoning process. Specifically, for\neach logical question, we first define all necessary\npredicates and append the corresponding natural\nlanguage explanations. Then we list the known\nFacts and Rules extracted from the Context with\ninterleaved NL instructions.\nAfter that, we represent the application of each\nRule by utilizing separate blocks, line by line. We\n4In the original case, the Query is \u2018Charlie is not green.\u2019.\nWe replace it with \u2018Dave is not green.\u2019 for better illustration.\nstrive to preserve as many solver actions as possi-\nble, such as \u2018Binding\u2019 and \u2018Unbinding\u2019, as well as\nthe acquisition of new implied Facts, and so forth.\nNoting that this information has already been ob-\ntained during the revealing phase, we focus on the\nrefinement of the solver-derived reasoning process.\nFinally, we enumerate all newly implied Facts to\nenable the model to perform an interim review.\nTurn-3: Query & Answering Instructions.\nIn\nTurn-3, we present instructions for answering a\ngiven Query. Following prior works (Ceri et al.,\n1989; Tafjord et al., 2021), a Query can be con-\nsidered true within a certain logical context if it\nis explicitly mentioned or if it can be implied\nthrough several Rule applications. To handle nega-\ntion, we consider two distinct assumptions: 1) the\nopen-world assumption (OWA) that treats any fact\nthat cannot be provable as special truth value \u2018un-\nknown\u2019; 2) the closed-world assumption (CWA)\nwhere any fact not provable is assumed \u2018false\u2019. Fol-\nlowing both assumptions, we adjust the answering\ninstructions, particularly the \u2018Options\u2019 part.\nTurn-4: Query-based Reasoning & Formatted\nAnswer.\nIn the final Turn-4, we compare the\nparsed Query with all the known Facts and im-\nplied Facts, expecting the model to perform basic\nlanguage inference and generate answer options in\nthe desired format.\n3.3\nFine-tuning Open-source LLMs\nAfter obtaining the refined deductive reasoning\ninstruction-tuning dataset, we can perform fine-\ntuning on open-source LLMs with the expectation\nthat the trained model (i.e., LOGIPT) can possess\nreasoning abilities similar to those of solvers. Con-\nsequently, for any given Query, we can bypass the\nsyntax or grammatical errors derived from NL-to-\nSL parsing by directly generating the answer with\na fine-tuned LOGIPT.\n4\nExperiments\nWe construct our solver-derived instruction-tuning\ndata on two public deductive reasoning datasets\nand evaluate LOGIPT on corresponding test sets.\n4.1\nDatasets\nProofWriter\n(Tafjord et al., 2021) is a commonly\nemployed dataset for deductive logical reasoning.\nFollowing Pan et al. (2023), we adopt the open-\nworld assumption (OWA) subset where the answer\nModel\nPrompting Methods\nProofWriter\nPrOntoQA\nRandom Answering\n-\n33.33\n50.00\nclosed-source LMs\nChatGPT (gpt-3.5-turbo)\nFew-shot Standard\n35.50\n47.40\nChatGPT (gpt-3.5-turbo)\nFew-shot CoT\n49.17\n67.80\nGPT-3.5 (text-davinci-003)\nFew-shot Standard\n36.16\n51.80\nGPT-3.5 (text-davinci-003)\nFew-shot CoT\n48.33\n83.00\nGPT-4 (gpt-4)\nFew-shot Standard\n52.67\n77.40\nGPT-4 (gpt-4)\nFew-shot CoT\n68.11\n98.79\nopen-source LMs\nVicuna-13B (vicuna-13b-v1.5-16k)\nFew-shot Standard\n35.50\n53.80\nVicuna-13B (vicuna-13b-v1.5-16k)\nFew-shot CoT\n41.50\n37.40\nCodeLlama-13B-Base (CodeLlama-13b-hf)\nFew-shot Standard\n0.00\n0.00\nCodeLlama-13B-Base (CodeLlama-13b-hf)\nFew-shot CoT\n36.00\n50.00\nCodeLlama-13B-Instruct (CodeLlama-13b-Instruct-hf)\nFew-shot Standard\n36.83\n52.20\nCodeLlama-13B-Instruct (CodeLlama-13b-Instruct-hf)\nFew-shot CoT\n32.67\n66.40\nsolver-argumented LMs\nLogicLM (gpt-3.5-turbo)\nFew-shot CoT\n58.33\n61.00\nLogicLM (text-davinci-003)\nFew-shot CoT\n71.45\n85.00\nLogicLM (gpt-4)\nFew-shot CoT\n79.66\n83.20\nours\nLOGIPT (vicuna-13b-v1.5-16k)\nFour-turn CoT\n81.17\n96.40\nLOGIPT (CodeLlama-13b-hf)\nFour-turn CoT\n89.50\n95.60\nLOGIPT (CodeLlama-13b-Instruct-hf)\nFour-turn CoT\n81.67\n96.20\nTable 2: Main results on two evaluation datasets. The best results of LOGIPT are in bold and the best results within\neach dataset are underlined.\nof each example is one of {True, False, Unknown}.\nThe original dataset is partitioned into 5 subsets\nwhere each part requiring 0, \u22641, \u22642, \u22643, and \u22645\nhops of reasoning, respectively. For evaluation, we\nadopted the version provided by Pan et al. (2023),\nwhich comprises 600 samples from the most chal-\nlenging 5-hop subsets with balanced label distribu-\ntion. For training, we merged all training subsets\nand obtained 41,433 training examples after the\nconstruction stage.\nPrOntoQA\n(Saparov and He, 2023) is a synthetic\nlogical reasoning dataset created recently to test the\ngeneral deductive reasoning capacity of LLMs. We\nadopt the hardest fictional characters version of\nthe dataset following Pan et al. (2023) where the\nentities of Facts are fictional concept names (e.g.,\n\u2018wumpus\u2019 instead of \u2018cat\u2019), to avoid any confound-\ning effects from knowledge acquired during the\npretraining phase. Similar to ProofWriter, PrOn-\ntoQA is organized into several subsets based on\nthe number of required reasoning steps. We use\nthe hardest 5-hop subset for evaluation. Contrary\nto ProofWriter, PrOntoQA is in a closed-world as-\nsumption (CWA) subset where the answer of each\nexample is one of {True, False}. For training, we\nmerely merge all subsets with fictional characters\nand obtained 15,940 training cases after filtering\nout syntax-invalid ones.\n4.2\nBaselines\nWe consider comparing LOGIPT with following\ngroups of baselines:\nClosed-source LMs:\nWe include the Chat-\nGPT (gpt-3.5-turbo) (OpenAI, 2022), GPT-3.5\n(text-davinci-003) (Ouyang et al., 2022) and\nGPT-4 (gpt-4) (OpenAI, 2023) as closed-source\nLMs for evaluation following Pan et al. (2023).\nOpen-source LMs:\nWe also evaluate open-\nsource LMs for research community. Specifically,\nwe choose Vicuna-13B (vicuna-13b-v1.5-16k)\n(Chiang et al., 2023), a chatbot trained by fine-\ntuning LLaMA-2 (Touvron et al., 2023b) on user-\nshared conversations collected from ShareGPT5,\nand CodeLlama-13B (Roziere et al., 2023), foun-\ndation models for code tasks. We select the base\nversion (CodeLlama-13b-hf), and instruction fine-\ntuned version (CodeLlama-13b-Instruct-hf).\nSolver-argumented LMs:\nFinally, we compare\nour model against the solver-argumented LMs.\nWe focus on the representative LogicLM (Pan\n5https://sharegpt.com/\net al., 2023) with underlying LLMs ChatGPT\n(gpt-3.5-turbo), GPT-3.5 (text-davinci-003)\nand GPT-4 (gpt-4), which serve as the state-of-the-\nart deductive reasoning methods.\nApart from the LMs, we also analyze two types\nof prompting methods: i) Standard prompting that\nuses in-context learning with few-shot demonstra-\ntions to directly answer the given question; ii)\nChain-of-Thought (CoT) that utilizes step-by-step\nproblem-solving process to generate explanations\nwhere few-shot demonstrations are also provided,\nand then outputs the final answer. For a fair compar-\nison, we use the same in-context examples, shown\nin Appendix A and B, for NL-to-SL parsing when\nevaluating all models on the same dataset, consis-\ntent with Pan et al. (2023). To enhance the clarifica-\ntion, we also provide a specific baseline \u2018Random\nAnswering\u2019 that randomly outputs answer options.\n4.3\nImplementation Details\nDuring the fine-tuning phase, we use a batch size of\n32 per GPU and a learning rate of 1e-5 for all open-\nsource LMs. We train our model on 8 Nvidia A100-\n80G GPUs with DeepSpeed ZeRO-3 (Rasley et al.,\n2020) for 12 hours on 2 epochs. For reproducibility,\nwe use greedy decoding and set the temperature to\n0 and the maximum context length to 8192. As for\nbaselines, we strictly follow the setting of Pan et al.\n(2023). Given that all instances are presented in the\nform of multiple-choice questions, we assess the\nmodel\u2019s performance by the accuracy of selecting\nthe correct answer option.\n4.4\nMain Results\nWe report the results of LOGIPT and baselines on\nTable 2 and have following main findings:\n1) When prompting with few-shot examples,\nopen-source LMs exhibit notably poor deductive\nreasoning capabilities, with their outputs closed to\nrandom answering. Even the Standard prompting\nmodels of ChatGPT (gpt-3.5-turbo) and GPT-\n3.5 (text-davinci-003) exhibit a similar perfor-\nmance to random answering.\nThis once again\ndemonstrates that it is considerably difficult for\nmany LLMs to solve logical reasoning tasks.\n2) LOGIPT is significantly superior to the\nstate-of-the-art solver-augmented LMs by a large\nmargin on both deductive reasoning bench-\nmarks. In ProofWriter, our best-performing model,\nLOGIPT (CodeLlama-13b-hf), outperforms the\ncurrently state-of-the-art LogicLM (gpt-4) by\nan absolute improvement of 9.84%.\nMean-\nwhile, in PrOntoQA, our best-performing model\nLOGIPT (vicuna-13b-v1.5-16k) exhibits an\neven higher absolute improvement of 13.20% than\nLogicLM (gpt-4). This indicates that our approach\nis better than the pipeline of problem formulation\nfirst and then reasoning with solvers, and fine-\ntuning with solver-derived reasoning data can facil-\nitate the deductive reasoning capacity of LMs.\n3) LOGIPT significantly outperforms all se-\nlected open/closed-source LMs on both datasets,\nexcept for the CoT experiment on the PrOntoQA\ndata where LOGIPT achieves comparable results\nwith GPT-4 CoT. This is surprising considering that\nour underlying open-source LMs are merely 13B\nparameters in size. As for the baseline experiments\nof GPT-4, our performance on ProofWriter also sig-\nnificantly surpasses that of GPT-4\u2019s Standard and\nCoT prompting versions, as well as the Standard\nversion of PrOntoQA. These results further demon-\nstrate that open-source LMs, when coupled with\nsolver-simulated reasoning capacity, can achieve\nperformance on par with or even superior to closed-\nsource GPT models.\n4) The accuracy of CodeLlama-13B-Base\n(CodeLlama-13b-hf) with Standard prompting\nwas 0.00, and the performance of the CoT version\nwas close to random answering. By examining the\noutputs, we found that this is due to the CodeLlama-\n13B-Base\u2019s inability to follow the provided few-\nshot demonstrations, resulting in outputting no an-\nswering options. The introduction of the Instruct\nversion of CodeLlama-13B mitigates this issue to\nsome extent. However, after training with LOGIPT,\nthe CodeLlama models far less encounter this issue\n(i.e., following the right answering format in both\ntest sets) and even achieve better performance than\nthe Vicuna version of LOGIPT. This demonstrates\nthe potential of code foundation models in logical\nreasoning tasks, consistent with the finding on prior\nwork (Yue et al., 2023).\n5\nFurther Analysis\n5.1\nImpact of Solver-derived Reasoning\nFormats\nWe further investigate the impact of different solver-\nderived reasoning formats on the model\u2019s perfor-\nmance. Specifically, we consider the following\nformat variations: 1) w/o \u2018unbind\u2019 statements that\nwe remove all \u2018Unbind\u2019 statements from Turn-2\nto investigate the utility of the explicit retention\nof this action from the solver; 2) w/o \u2018fail & back-\nModel\nAccuracy\nLOGIPT (vicuna-13b-v1.5-16k)\n81.17\n+ (w/o \u2018unbind\u2019 statements)\n80.67\n+ (w/o \u2018fail & backtrack\u2019 statements)\n84.00\n+ (w/ NL representation)\n66.33\nLOGIPT (CodeLlama-13b-hf)\n89.50\n+ (w/o \u2018unbind\u2019 statements)\n93.33\n+ (w/o \u2018fail & backtrack\u2019 statements)\n87.17\n+ (w/ NL representation)\n52.33\nLOGIPT (CodeLlama-13b-Instruct-hf)\n81.67\n+ (w/o \u2018unbind\u2019 statements)\n79.00\n+ (w/o \u2018fail & backtrack\u2019 statements)\n84.83\n+ (w/ NL representation)\n66.33\nTable 3: The accuracy of the variations on solver-derived\nreasoning format, and replacing SL representations with\nNL on ProofWriter. The best results on each underlying\nLMs are underlined.\nTrain set\nTest Set\nVCN\nCLB\nCLI\nPrOntoQA\nPrOntoQA\n96.40\n95.60\n96.20\nBoth\nPrOntoQA\n91.00\n87.00\n89.00\nBoth (Reformat)\nPrOntoQA\n90.00\n87.00\n77.80\nProofWriter\nProofWriter\n81.17\n89.50\n81.67\nBoth\nProofWriter\n79.33\n87.17\n79.67\nBoth (Reformat)\nProofWriter\n79.00\n90.83\n84.50\nTable 4: The accuracy of LOGIPT trained with merged\ndata and tested on single data with different underlying\nLMs. \u2018VCN\u2019, \u2018CLB\u2019, and \u2018CLI\u2019 respectively represent\nVicuna-13B, CodeLlama-13B-Base, and CodeLlama-\n13B-Instruct. \u2018Both\u2019 means \u2018ProofWriter + PrOntoQA\u2019.\ntrack\u2019 statements that we removing all \u2018Fail &\nbacktrack\u2019 statements from Turn-2. During the\nsolver\u2019s reasoning process, it is expected to en-\ncounter situations in which, after binding a value,\nthe solver realizes that not all premises are satis-\nfied (e.g., \u2018Fiona is blue\u2019 but \u2018Fiona is not\nquiet\u2019 for application of Rule3 in Figure 3). Con-\nsequently, a \u2018Fail & backtrack\u2019 operation occurs\n(highlighted in color in Figure 3). We explore the\neffectiveness of explicitly stating these operations.\nWe present the accuracy of the variations on\nsolver-derived reasoning format on ProofWriter in\nTable 3 where several observations can be made:\n1) regardless of using the default format, remov-\ning \u2018Unbind\u2019 statements, or removing \u2018Fail &\nbacktrack\u2019 statements, it can not be determined\nwhich format guarantees the optimal results. To\nretain the maximum amount of action information\nthat the solver can provide, we still adopt the de-\nfault settings in LOGIPT; 2) whether \u2018Unbind\u2019\nstatements are removed or \u2018Fail & backtrack\u2019\nstatements are removed, there is always an experi-\nment under each open-source LMs that can surpass\nthe default LOGIPT results. This further enhances\nthe best performance of LOGIPT shown in Table 2.\n5.2\nImpact of SL Reasoning Representations\nWe are also curious about the impact of SL reason-\ning representations. Therefore, we include addi-\ntional experiments in Table 3, denoted as w/ NL\nrepresentation that we re-translate the symbolic\nrepresentation (e.g., Green(\u2019Charlie\u2019, True))\nback to its original NL version (e.g., Charlie is\ngreen.) and replace the original symbolic repre-\nsentation in Turn-2. From the table, we can find\nthat replacing SL representations with NL results\nin a significant decrease in model performance, fur-\nther emphasizing that symbolic representations are\nsuperior to NL representations in deductive reason-\ning tasks.\n5.3\nEffectiveness of Merging Data from\nDifferent Reasoning Assumptions\nSince ProofWriter is an open-world assumption\nand PrOntoQA is labeled within a closed-world\nassumption, we also perform a further investiga-\ntion on whether both reasoning assumptions can\nbenefit each other. Specifically, we first merge both\nconstructed training data and then test LOGIPT on\neach test set. The experimental results are shown\nin Table 4. We can conclude that if we directly mix\nthe two types of data for training, the results on\ntheir respective test sets will be slightly lower than\nthose obtained from training solely on their respec-\ntive datasets. Therefore, we conducted an in-depth\nanalysis of the underlying reasons and observed\nthat in PrOntoQA, the majority of Rules are in the\nformat of \u2018Every/Each A is (not) B\u2019 or \u2018A are (not)\nB\u2019. While in ProofWriter, the predominant structure\nof Rules consists of: \u2018If someone is A, then they\nare B\u2019 or \u2018If something is A, then it is B\u2019. Therefore,\nwe conducted an additional set of experiments in\nwhich the Rule format of two training sets was ran-\ndomly reformatted into the four aforementioned\ntypes using regular expression (denoted as \u2018Both\n(Reformat)\u2019). Then, we test the model on the orig-\ninal test sets. We can observe that by employing\nthis approach, the code models yield improved per-\nformance on ProofWriter. Thus, the style/genre of\nlogical context must also be taken into considera-\ntion to maximize the efficacy of transfer learning\nin logical reasoning.\n6\nRelated Work\nLogical Reasoning with LMs.\nRecent efforts\nin adapting Large Language Models (LLMs) for\nlogical reasoning tasks generally adopt direct fine-\ntuning specialized modules (Clark et al., 2020;\nTafjord et al., 2021, 2022; Yang et al., 2022) or\nin-context learning (Zhou et al., 2022; Lyu et al.,\n2023; Ling et al., 2023), where reasoning in NL is\nused by both groups of methods. Fine-tuning ap-\nproaches involve training the full model or special-\nized modules, enhancing LLMs with module-level\nlogical reasoning skills like proof, enumeration,\nand abduction (Tafjord et al., 2021). The in-context\nlearning approaches create specific prompts to en-\ncourage LLMs\u2019 step-by-step reasoning skills. Com-\nmon methods encompass chain-of-thought prompt-\ning (Wei et al., 2022b; Chen et al., 2023), which\nproduces explanations before delivering a final an-\nswer, and least-to-most prompting (Zhou et al.,\n2022), which deconstructs a problem into sim-\npler components that can be resolved individually.\nSome recent work has focused on combining neu-\nral networks with symbolic reasoning (Tian et al.,\n2022; Pryor et al., 2022; Pan et al., 2023), espe-\ncially the solver-augmented LMs that parse NL log-\nical questions into symbolic representations, then\nutilizing external logical solvers for answering. De-\nspite their impressive performance, parsing errors\ncan lead to solver execution failure and logical\nquestion-answering issues. To address this, we pro-\npose LOGIPT, which directly imitates the solver\u2019s\nreasoning ability and outputs the answer.\nAugmented LMs for Reasoning.\nRecent work\nhas begun to augment LMs to overcome their in-\nherent limitations such as the incapacity to access\nup-to-date information or conduct accurate math-\nematical reasoning. They augment with external\ntools and resources, such as the information re-\ntriever (Shi et al., 2023; Lazaridou et al., 2022),\nplanner (Liu et al., 2023a) and other pre-trained\nmodels (Shen et al., 2023). Specifically, to en-\nhance the reasoning capacity, recent work resort to\nexternal off-the-shelf Solvers including program-\nmatic interpreters (Chen et al., 2022; Gao et al.,\n2023), satisfiability solvers (Ye et al., 2023), logical\nsolvers (Pan et al., 2023) or their hybrids (Poesia\net al., 2023). Most of them utilize the LMs to parse\nthe NL question to symbolic representations and\nthen invoke solvers to reason in SL. In this paper,\nwe concentrate on logical solvers, automated tools\nfor validating the truth value of logical formulas.\n7\nConclusion\nIn this paper, we propose a novel LOGIPT that can\ndirectly act as a logical solver for deductive rea-\nsoning tasks. LOGIPT can output all facts implied\nfrom NL logical questions, while bypassing the syn-\ntax or grammatical errors derived from NL-to-SL\nparsing of solver-augmented LMs. We conducted\nnumerous analytical experiments on two public de-\nductive reasoning benchmarks. Evaluation results\nshow that LOGIPT can significantly outperform\nstate-of-the-art solver-augmented LMs, and surpass\nor be comparable with few-shot prompting meth-\nods on competitive LLMs like ChatGPT or GPT-4.\nReferences\nStefano Ceri, Georg Gottlob, Letizia Tanca, et al. 1989.\nWhat you always wanted to know about datalog(and\nnever dared to ask). IEEE transactions on knowledge\nand data engineering, 1(1):146\u2013166.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W Cohen. 2022.\nProgram of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588.\nZhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong,\nWayne Xin Zhao, and Ji-Rong Wen. 2023. Chat-\ncot: Tool-augmented chain-of-thought reasoning on\nchat-based large language models. arXiv preprint\narXiv:2305.14323.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2020.\nTransformers as soft reasoners over language. In Pro-\nceedings of the Twenty-Ninth International Joint Con-\nference on Artificial Intelligence, IJCAI-20, pages\n3882\u20133890. International Joint Conferences on Arti-\nficial Intelligence Organization. Main track.\nWilliam F Clocksin and Christopher S Mellish. 2003.\nProgramming in PROLOG. Springer Science & Busi-\nness Media.\nBruce Frederiksen. 2008. Applying expert system tech-\nnology to code reuse with pyke. PyCon: Chicago.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2023. Pal: Program-aided language\nmodels. In International Conference on Machine\nLearning, pages 10764\u201310799. PMLR.\nGoogle. 2023. Google bard. https://bard.google.com/.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nJie Huang and Kevin Chen-Chuan Chang. 2023. To-\nwards reasoning in large language models: A survey.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, pages 1049\u20131065, Toronto,\nCanada. Association for Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\narXiv\npreprint arXiv:2001.08361.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems, 35:22199\u2013\n22213.\nPhilipp K\u00f6rner, Michael Leuschel, Jo\u00e3o Barbosa,\nV\u00edtor Santos Costa, Ver\u00f3nica Dahl, Manuel V\nHermenegildo, Jose F Morales, Jan Wielemaker,\nDaniel Diaz, Salvador Abreu, et al. 2022. Fifty years\nof prolog and beyond. Theory and Practice of Logic\nProgramming, 22(6):776\u2013858.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\narXiv preprint arXiv:2203.05115.\nZhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang,\nMingu Lee, Roland Memisevic, and Hao Su. 2023.\nDeductive verification of chain-of-thought reasoning.\narXiv preprint arXiv:2306.03872.\nBo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu,\nShiqi Zhang, Joydeep Biswas, and Peter Stone.\n2023a. Llm+ p: Empowering large language models\nwith optimal planning proficiency. arXiv preprint\narXiv:2304.11477.\nHanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji\nZhou, and Yue Zhang. 2023b. Evaluating the logical\nreasoning ability of chatgpt and gpt-4. arXiv preprint\narXiv:2304.03439.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,\nDelip Rao, Eric Wong, Marianna Apidianaki, and\nChris Callison-Burch. 2023.\nFaithful chain-of-\nthought reasoning. arXiv preprint arXiv:2301.13379.\nOpenAI. 2022. Chatgpt: Optimizing language models\nfor dialogue. https://openai.com/blog/chatgpt/.\nOpenAI. 2023.\nGpt-4 technical report.\nArXiv,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nLiangming Pan, Alon Albalak, Xinyi Wang, and\nWilliam Yang Wang. 2023. Logic-lm: Empower-\ning large language models with symbolic solvers\nfor faithful logical reasoning.\narXiv preprint\narXiv:2305.12295.\nGabriel Poesia, Kanishk Gandhi, Eric Zelikman, and\nNoah D Goodman. 2023. Certified reasoning with\nlanguage models. arXiv preprint arXiv:2306.04031.\nDavid L Poole and Alan K Mackworth. 2010. Artificial\nIntelligence: foundations of computational agents.\nCambridge University Press.\nConnor Pryor, Charles Dickens, Eriq Augustine, Alon\nAlbalak, William Wang, and Lise Getoor. 2022. Ne-\nupsl: Neural probabilistic soft logic. arXiv preprint\narXiv:2205.14268.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. 2020. Deepspeed: System optimiza-\ntions enable training deep learning models with over\n100 billion parameters. In Proceedings of the 26th\nACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, pages 3505\u20133506.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nAbulhair Saparov and He He. 2023. Language models\nare greedy reasoners: A systematic formal analysis\nof chain-of-thought. In The Eleventh International\nConference on Learning Representations.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models.\narXiv\npreprint arXiv:2301.12652.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.\nProofWriter: Generating implications, proofs, and\nabductive statements over natural language. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 3621\u20133634, Online.\nAssociation for Computational Linguistics.\nOyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark.\n2022. Entailer: Answering questions with faithful\nand truthful chains of reasoning. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2078\u20132093, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nJidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao,\nHao He, and Yaohui Jin. 2022. Weakly supervised\nneural symbolic learning for cognitive tasks. In Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 36, pages 5888\u20135896.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a.\nLlama:\nOpen and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan,\nand Subbarao Kambhampati. 2022. Large language\nmodels still can\u2019t plan (a benchmark for llms on plan-\nning and reasoning about change). arXiv preprint\narXiv:2206.10498.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022a. Emergent abilities of large language models.\nTransactions on Machine Learning Research.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nKaiyu Yang, Jia Deng, and Danqi Chen. 2022. Gen-\nerating natural language proofs with verifier-guided\nsearch. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 89\u2013105, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nXi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. 2023.\nSatisfiability-aided language models using declara-\ntive prompting. arXiv preprint arXiv:2305.09656.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wen-\nhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\n2023. Mammoth: Building math generalist models\nthrough hybrid instruction tuning. arXiv preprint\narXiv:2309.05653.\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc V Le, et al. 2022.\nLeast-to-most prompting enables complex reasoning\nin large language models. In The Eleventh Interna-\ntional Conference on Learning Representations.\nA\nInstructions for NL-to-SL Parsing on\nProofWriter\nTask Description: You are given a problem description\nand a question. The task is to:\n1) define all the predicates in the problem\n2) parse the problem into logic rules based on the defined\npredicates\n3) write all the facts mentioned in the problem\n4) parse the question into the logic form\nProblem:\nAnne is quiet. Erin is furry. (... more context here ...) All\nred people are young.\nQuestion:\nBased on the above information, is the following statement\ntrue, false, or unknown? Anne is white.\nPredicates:\nQuiet($x, bool) ::: Is x quiet?\nFurry($x, bool) ::: Is x furry?\n(... more predicates here ...)\nYoung($x, bool) ::: Is x young?\nFacts:\nQuite(Anne, True) ::: Anne is quiet.\nFurry(Erin, True) ::: Erin is furry.\n(... more facts here ...)\nQuite(Harry, True) ::: Harry is quiet.\nWhite(Harry, True) ::: Harry is white.\nRules:\nYoung($x, True) >>> Furry($x, True) ::: Young people\nare furry.\nQuite(Anne, True) >>> Red($x, True) ::: If Anne is quiet\nthen Anne is red.\n(... more rules here ...)\nRed($x, True) >>> Young($x, True) ::: All red people\nare young.\nQuery:\nWhite(Anne, True) ::: Anne is white.\n\u2014\u2014\nProblem:\n(new problem here)\nQuestion:\n(new question here)\nB\nInstructions for NL-to-SL Parsing on\nPrOntoQA\nTask Description: You are given a problem description\nand a question. The task is to:\n1) define all the predicates in the problem\n2) parse the problem into logic rules based on the defined\npredicates\n3) write all the facts mentioned in the problem\n4) parse the question into the logic form\nProblem:\nEach jompus is fruity. Every jompus is a wumpus. (...\nmore context here ...) Alex is a tumpus.\nQuestion:\nTrue or false: Alex is not shy.\nPredicates:\nJompus($x, bool) ::: Does x belong to Jompuses?\nFruity($x, bool) ::: Is x fruity?\n(... more predicates here ...)\nLiquid($x, bool) ::: Is x liquid?\nZumpus($x, bool) ::: Does x belong to Zumpuses?\nFacts:\nTumpus(Alex, True) ::: Alex is a tumpus.\nRules:\nJompus($x, True) >>> Fruity($x, True) ::: Each jompus\nis fruity.\nJompus($x, True) >>> Wumpus($x, True) ::: Every\njompus is a wumpus.\n(... more rules here ...)\nRompus($x, True) >>> Liquid($x, True) ::: Each rompus\nis liquid.\nRompus($x, True) >>> Zumpus($x, True) ::: Rompuses\nare zumpuses.\nQuery:\nShy(Alex, False) ::: Alex is not shy.\n\u2014\u2014\nProblem:\n(new problem here)\nQuestion:\n(new question here)\nC\nA Comprehensive Training Example of\nour Instruction-tuning Data\nTurn-1 (from human):\nTask Description: Given a statement containing logical facts \nand rules in natural languages as context, the task is to:\n1. define all the predicates in logic forms from the context\n2. parse the statement into known facts and rules based on the \ndefined predicates\n3. begin reasoning to obtain all implied facts\nContext:\nCharlie is green. Charlie is white. Dave is blue. Dave is \nfurry. Dave is white. Fiona is blue. Fiona is green. Fiona is \nnice. Fiona is rough. Fiona is white. All furry people are \nquiet. All green, white people are nice. If someone is blue \nand quiet then they are green.\nReasoning:\nTurn-2 (from LM):\nWe first define following predicates and corresponding natural \nlanguage explanations:\nGreen($x, bool) ::: Is x green?\nWhite($x, bool) ::: Is x white?\nBlue($x, bool) ::: Is x blue?\nFurry($x, bool) ::: Is x furry?\nNice($x, bool) ::: Is x nice?\nRough($x, bool) ::: Is x rough?\nQuiet($x, bool) ::: Is x quiet?\nWe have following known facts from the context:\nGreen(\u2019Charlie\u2019, True)\nGreen(\u2019Fiona\u2019, True)\nWhite(\u2019Charlie\u2019, True)\nWhite(\u2019Dave\u2019, True)\nWhite(\u2019Fiona\u2019, True)\nBlue(\u2019Dave\u2019, True)\nBlue(\u2019Fiona\u2019, True)\nFurry(\u2019Dave\u2019, True)\nNice(\u2019Fiona\u2019, True)\nRough(\u2019Fiona\u2019, True)\nWe have following known rules from the context:\nrule1: Furry($x, True) \u2192 Quiet($x, True)\nrule2: Green($x, True) \u2227 White($x, True) \u2192 Nice($x, True)\nrule3: Blue($x, True) \u2227 Quiet($x, True) \u2192 Green($x, True)\nNow begin reasoning to obtain all implied facts:\nUse rule1: Furry($x, True) \u2192 Quiet($x, True)\nBind $x to \u2019Dave\u2019\nObtain a new implied fact: Quiet(\u2019Dave\u2019, True)\nUnbind $x\nFinish implied with rule1\nUse rule2: Green($x, True) \u2227 White($x, True) \u2192 Nice($x, True)\nBind $x to \u2019Charlie\u2019\nObtain a new implied fact: Nice(\u2019Charlie\u2019, True)\nUnbind $x\nBind $x to \u2019Fiona\u2019\nObtain an already known or implied fact: Nice(\u2019Fiona\u2019, True)\nUnbind $x\nFinish implied with rule2\nUse rule3: Blue($x, True) \u2227 Quiet($x, True) \u2192 Green($x, True)\nBind $x to \u2019Dave\u2019\nObtain a new implied fact: Green(\u2019Dave\u2019, True)\nReuse rule2: Green($x, True) \u2227 White($x, True) \u2192 Nice($x, True)\nBind $x to \u2019Dave\u2019\nObtain a new implied fact: Nice(\u2019Dave\u2019, True)\nFinish implied with rule2\nUnbind $x\nBind $x to \u2019Fiona\u2019\nUnbind $x\nFinish implied with rule3\nFinally, we obtain following implied facts:\nGreen(\u2019Dave\u2019, True)\nNice(\u2019Charlie\u2019, True)\nNice(\u2019Dave\u2019, True)\nQuiet(\u2019Dave\u2019, True)\nFinish reasoning\nTurn-3 (from human):\nQuestion: Based on the above information, is the following \ncomment true, false, or unknown? Dave is not green.\nOptions:\nA) True\nB) False\nC) Unknown\nNow you should:\n1. reason first carefully with the above known facts and \nimplied facts\n2. then output your answer strictly in the format of \u201cSo, the \ncorrect option is: X\u201d. The X should be replaced by \u201cA) True\u201d, \n\u201cB) False\u201d or \u201cC) Unknown\u201d\nTurn-4 (from LM):\nThe comment can be parsed into: Green(\u2019Dave\u2019, False)\nAccording to the implied fact: Green(\u2019Dave\u2019, True)\nThe comment contradicts the fact.\nSo, the correct option is: B) False\nFigure 4: The full version of the comprehensive 4-turn training example of our instruction-tuning data shown in\nFigure 3.\n"
  },
  {
    "title": "FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores",
    "link": "https://arxiv.org/pdf/2311.05908.pdf",
    "upvote": "10",
    "text": "FlashFFTConv: Efficient Convolutions for\nLong Sequences with Tensor Cores\nDaniel Y. Fu\u2217,1, Hermann Kumbong\u2217,1, Eric Nguyen2, Christopher R\u00b4e1\n\u2217Equal contribution. 1Department of Computer Science, Stanford University.\n2Department of Biongineering, Stanford University.\n{danfu,kumboh,etnguyen,chrismre}@stanford.edu\nNovember 13, 2023\nAbstract\nConvolution models with long filters have demonstrated state-of-the-art reasoning abilities in many\nlong-sequence tasks but lag behind the most optimized Transformers in wall-clock time.\nA major\nbottleneck is the Fast Fourier Transform (FFT)\u2014which allows long convolutions to run in O(N log N)\ntime in sequence length N but has poor hardware utilization. In this paper, we study how to optimize\nthe FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix\nmultiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we\npropose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using\nmatrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two\nsparse convolution algorithms\u20141) partial convolutions and 2) frequency-sparse convolutions\u2014which can\nbe implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for\nmemory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93\u00d7 over\nPyTorch and achieves up to 4.4\u00d7 speedup end-to-end. Given the same compute budget, FlashFFTConv\nallows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve\n3.3 points higher GLUE score\u2014matching models with twice the parameter count. FlashFFTConv\nalso achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously\nachieved better than 50%. Furthermore, partial convolutions enable longer-sequence models\u2014yielding\nthe first DNA model that can process the longest human genes (2.3M base pairs)\u2014and frequency-sparse\nconvolutions speed up pretrained models while maintaining or improving model quality.\n1\nIntroduction\nA key challenge in machine learning is to efficiently reason over long sequences. Recently, convolutions have\nemerged as a key primitive for sequence modeling, underpinning state-of-the-art performance in language\nmodeling [42, 76, 94, 110], time-series analysis [36, 46, 103, 115], computer vision [74, 81, 109], DNA\nmodeling [82], and more [27, 55, 61, 71, 77, 80]. Despite these strong quality results\u2014and other benefits\nranging from better scaling in sequence length [46] to greater stability [9, 106]\u2014convolutional sequence models\nstill lag behind Transformers in wall-clock time.\nA major reason is poor hardware support. Unlike classical convolutions used in vision applications, which\noften have short filters (e.g., 3 \u00d7 3 or 7 \u00d7 7 [53, 63]), convolutions for sequence modeling often use filters as\nlong as the input sequence [71, 97]. Such long filters necessitate the use of the FFT convolution algorithm,\nwhich computes the convolution between an input u and convolution kernel k via a conversion to frequency\nspace:\n(u \u2217 k)[i] =\ni\nX\nj\nu[i]k[j \u2212 i]\n\u223c=\nu \u2217 k = F\u22121(Fu \u2299 Fk),\n(1)\nwhere F is the FFT, which can be computed in O(N log N) time in sequence length N, and \u2299 is elementwise\nmultiplication. Despite its asymptotic efficiency, the FFT convolution algorithm has poor wall-clock time\n1\narXiv:2311.05908v1  [cs.LG]  10 Nov 2023\nReg\nSRAM\nHBM\nRegisters\n64 KB\nSRAM\n20 MB\nHBM\n40 GB\nGPU Memory\nHierarchy\nFN1\nT\nFN2\nFN2\n-1\nFN1\nkf\nFFT\nPointwise\niFFT\nOrder-p Monarch Decomposition of\nFFT Convolution on Tensor Cores\nSpeedup Through Fusion\nFlashFFTConv\nPyTorch\nTime (ms)\nPad\nFFT\nPointwise\niFFT\nUnpad\nFused\nKernel\n0\n1\n2\nTime\nFrequency\nSparse Convolution\nFilters\n-1\n1.5\nTB/s\n19 TB/s\nFigure 1: Left: GPU memory hierarchy. Middle left: Order-p Monarch decomposition of FFT convolution,\nwith p = 2. Middle right: Kernel fusion for end-to-end speedup. Right: FlashFFTConv introduces\nanalogues of sparsity for convolutions.\non modern accelerators. In contrast, systems advances have pushed Transformers to the limits of modern\naccelerators\u2014achieving more than 72% FLOP utilization end-to-end with FlashAttention-v2 [22, 24].\nIn this paper, we study how to optimize the FFT convolution algorithm on modern accelerators, to enable\nlonger-context abilities. Just as systems advances such as FlashAttention yielded improvements in modeling\nquality [1, 70] and the development of new attention algorithms [2, 66, 73, 92], we hope that understanding\nhow to optimize the FFT convolution can also inspire algorithmic innovation, thus improving the quality of\nconvolutional sequence models.\nFor short sequences, the FFT convolution is relatively easy to optimize. Kernel filters are often shared\nacross many batches, which allows pre-computing the FFT of the filter kf = Fk and re-using it in a batch:\n(u \u2217 k) = F\u22121(Fu \u2299 kf). Thus the FFT convolution is pleasantly parallel across batches and filters, and\nintermediate outputs of the convolution can be cached in SRAM or registers via kernel fusion.\nHowever, as sequence length increases, we find that two key bottlenecks emerge. First, FFT convolutions\ndo not effectively use the specialized matrix-matrix multiply units available on modern accelerators\u2014e.g., the\nH100 can use tensor cores to compute matrix-matrix multiply at 1.0 PetaFLOP/s compared to 67 TeraFLOP/s\nfor general arithmetic. Second, sequences become too large to fit in SRAM, and kernel fusion fails, resulting\nin expensive I/O costs (Figure 1 middle right). These I/O costs can be exacerbated by padding operations\nfor causality, and conversions from real-valued inputs/outputs to complex-valued FFT intermediates.\nIn response, we propose FlashFFTConv, a new system that optimizes the FFT convolution for long\nsequences using a Monarch decomposition of the FFT. An order-p Monarch decomposition rewrites the\nFFT as a series of p matrix-matrix multiply operations (Figure 1 middle left), which can be efficiently\nmapped onto hardware [23]. The order p controls the number of matrix multiply operations and introduces a\ntradeoff: higher values of p incur lower FLOP cost via smaller matrices, but require more I/O to communicate\nintermediate results. Using a simple GPU cost model, we show how to adjust p based on the sequence length\nto balance the FLOP cost and I/O cost. This decomposition introduces a second benefit: a reduction in the\namount of the sequence that needs to be kept in SRAM, which makes kernel fusion viable at longer sequence\nlengths. As a result, FlashFFTConv scales across four orders of magnitude in sequence length, from 256 to\n4 million. FlashFFTConv also exploits a real-valued FFT algorithm to cut the length of the FFT operation\nin half [102], and selectively skips portions of the matrix-multiply operations when the input is zero-padded.\nFinally, the matrix view of the FFT convolution presents a natural interface to implement two architectural\nmodifications: partial convolutions, which learn with k that is shorter than the input sequence, and frequency-\nsparse convolutions, which zero out portions of the kernel kf in frequency space. These can be viewed\nas convolutional analogues to sparse/approximate attention in Transformers [8, 50, 51, 62, 92], and map\nnaturally on to FlashFFTConv: both algorithms can be implemented simply by skipping portions of the\nmatrix decomposition, thus reducing memory footprint and wall-clock runtime.\nEvaluation\nWe show that FlashFFTConv speeds up the FFT convolution, yielding higher-quality, more\nefficient, and longer-sequence models.\n\u2022 Quality FlashFFTConv improves the quality of convolutional sequence models via better efficiency: for\nthe same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity [94],\n2\ne\u00b12\u03c0ijk/N\n0\n4\n8\n1\n5\n9\n2\n6\n10\n3\n7\n11\n12 13 14 15\n1: Reshape Input\nF\nF\nT\nF\nF\nT\nF\nF\nT\nF\nF\nT\n1\n2\n3\n4\n2: FFT on Columns\n3: Twiddle\nF\nF\nF\nF\nF\nF\nT\nT\nT\n1\n2\n3\nF\nF\nT\n4\n4: FFT on Rows\n5: Transpose Output\n0\n1\n2\n4\n5\n6\n8\n9\n10\n12\n13\n14\n3\n7 11 15\nFigure 2: Illustration of Monarch FFT decomposition.\nand allows M2-BERT-base [42] to achieve up to 3.3 higher average GLUE score\u2014a gain in performance\nequivalent to doubling the parameters of the model.\n\u2022 Efficiency FlashFFTConv makes convolutions more efficient across four orders of magnitude in sequence\nlength, yielding speedups of up to 7.93\u00d7 and memory savings of up to 5.60\u00d7 over PyTorch. FlashFFTConv\nachieves up to 62.3% end-to-end FLOP utilization\u2014only 10% less than FlashAttention-v2\u2014and is faster in\nwall-clock time than FlashAttention-v2 end-to-end at sequence lengths 2K and longer due to lower FLOP\ncosts.\n\u2022 Longer Sequence Models FlashFFTConv enables longer-sequence models. In high-resolution image\nclassification, FlashFFTConv yields the first model that can solve the challenging Path-512 task (sequence\nlength 256K) from the long range arena benchmark [104]. In DNA modeling, FlashFFTConv uses partial\nconvolutions to extend HyenaDNA [82] to 4M sequence length\u2014yielding the first model that can embed\nthe longest human genes (up to 2.3M base pairs) at single nucleotide resolution.\nOverall, we hope that FlashFFTConv enables further adoption of convolutional sequence models and that\nthe insights from our work helps inform the design of better hardware-efficient architectures.\n2\nBackground\nWe provide some background on the FFT convolution and the Monarch FFT decomposition, and discuss the\nperformance characteristics of GPUs.\n2.1\nFFT Convolution\nRecall the definition of a convolution operation: (u \u2217 k)[i] = Pi\nj ujki\u2212j. Computing this formula directly\nincurs O(NNk) FLOPs in sequence length N and kernel length Nk. For long convolutions, where Nk = N,\na popular strategy is to use the Fourier transform to convert the signal u and kernel k to the frequency\ndomain, and compute the convolution using pointwise multiplication in frequency domain, using Equation 1.\nCritically, a Fourier transform FN over an input of length N can be computed in O(N log N) time using the\nFFT\u2014bringing the overall cost of the long convolution from O(N 2) to O(N log N). We note that the FFT\nconvolution technically computes a circular convolution PN\nj ujki\u2212j, where i \u2212 j < 0 loops back to the end of\nk. For this reason, u and k are often padded with zeros to compute a causal convolution.\nMonarch FFT Decomposition\nFigure 2 shows a demonstration of the order-2 Monarch FFT decomposi-\ntion. For N = N1N2, an order-2 Monarch FFT decomposition rewrites FN = P(IN2\u2297FN1)DP\u22121(IN1\u2297FN2)P,\nwhere \u2297 denotes the Kronecker product, FN is the N \u00d7 N discrete Fourier matrix, P is a permutation\nmatrix that reshapes the input to N1 \u00d7 N2, transposes it to N2 \u00d7 N1, and then reshapes it back to N, and\nD \u2208 CN\u00d7N is a diagonal matrix containing correctional values called Twiddle factors [6]. Higher-order\nMonarch decompositions recursively apply the order-2 decomposition to FN1 or FN2, which reduces FLOP\ncosts but increases the number of permutation operations, increasing I/O cost.\n2.2\nGPU Performance Characteristics\nWe provide some background on the GPU memory hierarchy and available compute units, as well as compute-\nbound vs. memory-bound operations. We focus on GPU programming in this paper, but the general principles\n3\nextend to most modern hardware accelerators [35, 57, 68, 114].\nGPU Compute Model and Memory Hierarchy\nGPUs have a memory hierarchy consisting of global\nmemory (HBM), shared memory (SRAM), and registers, as shown in Figure 1 Left. Lower/larger levels of\nthe memory hierarchy have more space but are much slower, whereas higher/smaller levels of the memory\nhierarchy have less space but are much faster [83\u201385]. The memory hierarchy is closely tied to the GPU\ncompute model. A GPU is composed of many independent streaming multiprocessors (SMs), each of which is\ncomposed of independent threads. HBM is shared among all SMs, but each SM has an independent SRAM.\nThe SRAM is shared among all the threads in the SM. Each thread has access to its own registers, but cannot\naccess the registers of other threads. Thus, performing global operations between SMs requires moving data\nto and from HBM, whereas independent work in each SM can remain local to SRAM.\nGPU Compute Units\nModern GPUs (since the V100 [83]) have specialized matrix multiply units called\ntensor cores, which can compute matrix-matrix multiply operations with much higher TFLOPs than the\ngeneral-purpose compute units. For example, the H100 tensor core can compute matrix multiplication\nbetween 16 \u00d7 16 matrices at 1.0 PFLOPs, whereas the general-purpose compute units can only compute at\n67 TFLOPs [85].\nMemory-Bound vs. Compute-Bound Operations\nGPU operations can be memory-bound or compute-\nbound. Memory-bound operations are bottlenecked by the amount of I/O between HBM and registers they\nneed to perform, and are limited by the bandwidth of the memory hierarchy. Examples include simple\npointwise operations such as addition or multiplication, as well as most traditional FFT implementations.\nCompute-bound operations are bottlenecked by the amount of FLOPs they need to execute, and are limited\nby the speed of the compute units. Examples include large matrix multiply operations.\nKernel Fusion\nA popular method for reducing I/O costs is kernel fusion\u2014loading data for multiple\noperations into SRAM, computing them independently in each SM, and then writing the final results back to\nHBM. Kernel fusion is common (and can be automated) for pointwise operations [93], but is more challenging\nfor complex operations that require referencing multiple pieces of data. For example, fusing the operations in\nattention was not common until the development of FlashAttention [24].\n3\nFlashFFTConv\nF\nF\nF\nF\nBroadcast over B, H\nBroadcast over\nSequence\nF\nF\nTranspose in SRAM\nReplaces Permutations\nF\n, T\nSRAM\nFigure 3: Top: FlashFFTConv adapts\nthe Monarch FFT decomposition to broad-\ncast matrix multiply operations over the\nsequence instead of over the batch and hid-\nden dimensions. Bottom: This converts\nHBM permutations simple matrix transpose\noperations in SRAM.\nSection 3.1 provides a broad overview of FlashFFTConv and\nshows how to adapt the Monarch FFT decomposition to con-\nvolutions, which involves broadcasting the matrix multiply in\nparallel across the input sequence. We also describe our kernel\nfusion strategy and how we exploit domain-specific properties\nof the convolution in ML for further optimization. Section 3.2\npresents a cost model characterizing the relative cost of different\norder-p decompositions of the FFT as sequence length changes,\nalong with a simple heuristic for selecting p given hardware\ncharacteristics. Finally, Section 3.3 discusses architectural ex-\ntensions by presenting analogues to sparsity in convolutional\nkernels.\n3.1\nFlashFFTConv Algorithm\nWe describe the core FlashFFTConv algorithm. Algorithm 1\nprovides an overview.\nWe first describe how we adapt the\nMonarch FFT decomposition for convolutions. Then, we dis-\ncuss how the Monarch decomposition enables kernel fusion for\n4\nAlgorithm 1 FlashFFTConv core algorithm, with order-2 Monarch decomposition. We assume N = N 2\n1\nfor simplicity here.\nInput: Input u \u2208 RB\u00d7H\u00d7N, convolution kernel kf \u2208 CH\u00d7N, FFT matrices F \u2208 CN1\u00d7N1, F\u22121 \u2208 CN1\u00d7N1,\nTwiddle factors t \u2208 CN, tinv \u2208 CN, B tile size Btile, H tile size Htile.\nOutput: Output y \u2208 RB\u00d7H\u00d7N.\nfor SMs in parallel across B/Btile \u00d7 H/Htile do\nLoad F, F\u22121, t, tinv from HBM.\nfor h \u2190 1 to Htile do\nLoad Kf \u2190 kf[h] from HBM, reshaped to N1 \u00d7 N1.\nfor b \u2190 1 to Btile do\nLoad X \u2190 u[b, h] from HBM, reshaped to N1 \u00d7 N1.\nX \u2190 ((F\u22a4X) \u2217 t)F\n\u25b7 FFT, decomposed into two steps\nX \u2190 X \u2217 Kf\n\u22a4\n\u25b7 Elementwise multiply with kf\nY \u2190 ((XF\u22121)\u22a4 \u2217 tinv)F\u22121\n\u25b7 Inverse FFT, decomposed into two steps\nWrite Y\u22a4 to HBM.\nlong sequences. We conclude by presenting domain-specific\noptimizations.\nAdapting Monarch for Fusion\nThe Monarch FFT decomposition, as well as classical algorithms such\nas Bailey\u2019s FFT algorithm [6], traditionally broadcasts the matrix operation against the batch dimension\nand the hidden dimension, as shown in Figure 3 top left. This allows each FN1 operation in the IN2 \u2297 FN1\nmatrix to run independently. However, it also makes kernel fusion difficult; fusing across the matrix multiply\nand permutation operations requires loading at least 16 sequences at once into SRAM to fill out the matrix\nmultiply unit\u2014limiting sequence length to around 2K on A100 and H100.\nInstead, we broadcast the matrix operation across the entire sequence, as shown in Figure 3 top right, and\nrun the algorithm in parallel across the batch and hidden dimensions. This reduces the SRAM requirements\nfor kernel fusion, since we only need to load a single sequence into SRAM at a time\u2014allowing us to fuse\nthe entire kernel for sequences up to 32K on A100 and H100. Broadcasting along the sequence has an\nadded benefit: the permutations simply become matrix transposes (Figure 3 bottom), which can be done\nquickly using well-established routines on-chip [84]. Finally, we also tile the computation across the B and H\ndimensions to reduce the cost of loading kf, F, and the twiddle factors from HBM. The core algorithm is\nshown in Algorithm 1 for a two-way decomposition. Higher-order decompositions and more details are given\nin Appendix A.\nKernel Fusion and Recomputation\nThe Monarch decomposition allows kernel fusion for long sequences.\nInner layers of the decomposition do not require the entire sequence, which reduces the SRAM requirements for\nfusion. Thus, for long sequences, we can fuse the innermost matrix operations and elementwise multiplications,\nand take an I/O each for the outermost matrix operations. We use also use recomputation in the backward\npass to reduce the memory footprint and I/O cost. Instead of storing intermediate results on HBM for the\nbackward pass (e.g., the intermediate result of FNu), we simply recompute them in the backward pass.\nDomain-Specific Optimizations\nFinally, we use a few domain-specific optimizations to adapt the\nconvolution specifically for the sequence learning workload. First, since the convolutions used in sequence\nlearning are real-to-real convolutions (with real kernel weights), we can use a classic algorithm called one-stage\ndecimation in time to compute the FFT of a sequence of length N using a complex FFT of length N/2 (see\nAppendix A)\u2014cutting the FFT cost in half. Second, inputs and outputs are often padded with zeros in\nthe convolution to compute a causal convolution [42, 46, 94]. We special-case this padding, and use it to\neliminate half of the outermost matrix multiply operations in the FFT and iFFT. We also fuse in additional\noperations around the convolution, such as elementwise-gating, to further reduce I/O.\n5\nCost Model for Order-p Monarch Decompositions\nSequence Length N\nCost Per Token C/N\n256\n1K\n4K\n16K\n64K\n256K\n1M\n4M\np = 4\np = 3\np = 2\n10\n100\nMatrices Too Small for Tensor Cores\nSRAM Limit\nFigure 4: Compute costs of different order-p Monarch decompositions as sequence length increases on A100.\nTradeoff points correspond to when the matrices in the Monarch decomposition reach the size of tensor cores\non A100 and when the sequence becomes too long for SRAM.\n3.2\nCost Model of order-p Monarch Decomposition\nWe present a formal cost model for an order-p Monarch decomposition of the convolution based on sequence\nlength. The cost model accounts for both the cost of compute and I/O, similar to a roofline analysis [54].\nLet B and H be the batch size and model hidden dimension, respectively, and assume that we compute the\nconvolution in half precision. Let N be the sequence length, and let N = \u03a0p\ni=1Ni be the product of p factors.\nFor simplicity, we will assume that N is a power of 2. Let \u00b5 be the size of the matrix-matrix multiply unit on\nthe GPU (e.g., 16 for A100 [84] and H100 [85]). Let \u03c4G and \u03c4M be the empirically-achievable FLOPs on the\nGPU for general-purpose arithmetic, and matrix-matrix multiply arithmetic, respectively. For convenience,\ndefine \u03b3(Ni) as a helper function that returns \u03c4G if Ni < \u00b5, and \u03c4M if Ni \u2265 \u00b5. Finally, let \u03c3H and \u03c3S be\nempirically-achievable bandwidth for HBM and SRAM, respectively. Sample values for these constants are\ngiven in Appendix C.\nNow, we can present the cost of an FFT convolution with an order-p Monarch decomposition. Let \u03c9(i) be\na helper function that returns the bandwidth of the memory where the intermediate results of decomposition\nstep i is stored. The overall cost of the convolution using an order-p Monarch decomposition is given by the\nfollowing:\nC = BH\np\nX\ni=1\n16NNi\n\u03b3(Ni) + 4N\n\u03c9(i)\n(2)\nFigure 4 graphs Equation 2 for different order-p decompositions on different sequence lengths for A100, for\np \u2208 {2, 3, 4}. For cases where N1 = \u00b7 \u00b7 \u00b7 = Np, the total FLOP cost of an order-p decomposition grows with\nO(N (p+1)/p). However, for shorter sequences, higher-order decompositions are actually more expensive, since\nthey decompose to matrices that are smaller than the matrix-matrix multiply unit (corresponding to the\nearly bumps). Note also the bump in cost for p = 3 between 32K and 64K, which is a result of running out of\nSRAM but which is mediated by an extra decomposition for p = 4.\n3.3\nArchitectural Extensions: Sparsity in Convolutions\nWe present 2 architectural extensions to FlashFFTConv: partial convolutions and frequency-sparse\nconvolutions. These can be thought of as convolutional analogues to sparse attention and present opportunities\nfor further optimization.\nPartial Convolutions\nIn partial convolutions, we zero out later portions of the convolution kernel,\nanalogous to local attention. This has two benefits. First, it reduces the memory footprint, since it requires\nfewer elements to be held in GPU memory at once. Second, it allows for natural extensions of a pretrained\nconvolutional model to longer sequences (i.e., via a sliding window approach).\n6\nTable 1: Improvement in quality given a fixed compute budget.\nModel (Metric)\nPyTorch\nFlashFFTConv\nM2-BERT-base-110M (GLUE Score \u2191)\n77.6\n80.9\nHyena-s-155M (PPL \u2193)\n13.4\n11.1\nTable 2: Classification accuracy (\u2191) on Path-X and Path-512 from the long range arena benchmark [104].\nFlashFFTConv allows for higher-resolution classification. \u2717 indicates out of memory.\nTask (seq. len.)\nPyTorch\nFlashFFTConv\nPath-X (16K)\n96.9\n96.9\nPath-512 (256K)\n\u2717\n96.1\nFrequency-Sparse Convolutions\nIn frequency-sparse convolutions, we zero out portions of the convolution\nkernel in frequency space, i.e. zeroing out portions of kf. This can be thought of as a variant of partial\nconvolutions in frequency space. Here, the specific sparsity pattern can yield computational benefits. Zeroing\nout the right portions of the kernel can obviate the need to compute portions of the matrix-matrix multiplies\nin the Monarch decomposition. We present examples of such sparsity patterns in Appendix A.\n4\nExperiments\nIn this section, we evaluate FlashFFTConv in terms of quality and efficiency. First (Section 4.1), we show\nthat FlashFFTConv allows models to achieve better quality for the same compute budget in language\nmodeling\u2014matching the performance of models with twice the parameters for free. FlashFFTConv also\nenables higher quality via higher resolution in image classification\u2014solving the challenging Path-512 task for\nthe first time simply via increased sequence length. Next (Section 4.2), we demonstrate FlashFFTConv\u2019s\nspeedup over other implementations of convolutions, evaluate its efficiency gains when used in convolutional\nmodels, and compare a convolutional model using FlashFFTConv to Transformers using FlashAttention-v2.\nFinally (Section 4.3), we evaluate partial and frequency-sparse convolutions. Partial convolutions yield the\nfirst DNA model that can embed the longest genes at single nucleotide resolution (2.3M base pairs), and\nfrequency-sparse convolutions yield speedup while maintaining\u2014or improving\u2014quality.\n4.1\nImpact of Efficiency on Quality\nWe study how FlashFFTConv impacts downstream quality. First, given two implementations with the\nsame compute budget, FlashFFTConv achieves higher quality due to higher training throughput. Second,\nwe show that improved efficiency can lead to higher quality via longer sequence length.\nImprovement in Quality with Fixed Compute Budget\nTo evaluate the impacts of efficiency on\ndownstream quality, we train two popular convolutional language models, M2-BERT-base [42] and Hyena-\ns [94], from scratch. These models are trained BERT-style (masked language modeling) and GPT-style (next\ntoken prediction), respectively. We compare the quality of models trained with the same compute budget but\ndifferent implementations of the convolution\u2014either FlashFFTConv or a PyTorch implementation of the\nFFT convolution. FlashFFTConv achieves higher pretraining throughput, which allows the models to see\nmore data during pretraining. These efficiency gains improve average GLUE score by up to 3.4 points for\nM2-BERT-base and perplexity by 2.3 points for Hyena-s. For context, these improvements in quality are\nsimilar in magnitude to the effect of doubling the number of parameters in the model (see Appendix B for\nreference results).\nLonger Sequence Models\nNext, we show how increased efficiency can lead to higher quality via longer\nsequence lengths. We evaluate long convolution models on Path-X and Path-512, high-resolution imaging\n7\nTable 3: Time (\u2193) to compute the forward pass of a convolution with FlashFFTConv in milliseconds on\none H100-SXM, as well as ablations removing specific optimizations. We also show memory savings. All\nresults scaled to batch size 64, hidden dimension 768. p indicates the order of the Monarch decomposition.\np = 2\np = 3\np = 4\nSequence Length\n256\n1K\n4K\n8K\n16K\n32K\n1M\n2M\n4M\nPyTorch\n0.43\n1.57\n6.65\n13.7\n28.6\n62.1\n2,346.3\n4,892.1\n10,127.6\nFlashFFTConv\n0.09\n0.24\n1.37\n3.19\n9.27\n21.8\n1,492.8\n2,695.1\n7,587.0\nFusion-Only/cuFFTdx\n0.21\n0.67\n3.51\n7.71\n21.4\n45.5\n\u2013\n\u2013\n\u2013\nSpeedup over PyTorch\n4.78\u00d7\n6.54\u00d7\n4.85\u00d7\n4.29\u00d7\n3.09 \u00d7\n2.85\u00d7\n1.57\u00d7\n1.82\u00d7\n1.33\u00d7\nMemory Savings\n8.21\u00d7\n7.73\u00d7\n7.61\u00d7\n7.59\u00d7\n7.21\u00d7\n6.57\u00d7\n2.64\u00d7\n2.63\u00d7\n2.63\u00d7\ntasks from the long range arena (LRA) benchmark [104].1 These tasks take an image (128\u00d7128 for Path-X\nand 512\u00d7512 for Path-512), flatten it out, and require a sequence model to classify whether two dots in the\nimage are connected by a path.\nExisting PyTorch implementations of convolutional sequence models (or even prior optimized implemen-\ntations [43]) fail to achieve better-than-random (50%) accuracy on Path-512 due to out of memory errors\nand a lack of support for such long sequences. However, Table 2 shows that FlashFFTConv allows a\nconvolutional sequence model to solve Path-512 for the first time simply by increasing the available sequence\nlength and reducing the memory footprint of the model through fusion.\n4.2\nEfficiency\nWe evaluate FlashFFTConv on how fast it computes convolutions compared to a PyTorch baseline,\nand how much speedup it yields for convolutional sequence models end-to-end. We also evaluate memory\nsavings compared to PyTorch and compare end-to-end efficiency against highly-optimized Transformers using\nFlashAttention-v2 [22].\nFlashFFTConv Speeds up Convolutions\nWe benchmark the speed of the convolution compared\nagainst an FFT convolution implemented in PyTorch. We also benchmark ablations evaluating kernel fusion\nwithout using tensor cores\u2014which recovers the strong baseline of using Nvidia\u2019s cuFFTdx kernel fusion\nlibrary [87]\u2014and FlashFFTConv without its domain-specific optimizations.\nTable 3 shows that FlashFFTConv outperforms PyTorch FFT convolution across all sequence lengths,\nby up to 6.54\u00d7. Speedups are greatest for short sequences, where the PyTorch FFT convolution is dominated\nby I/O costs. Speedup is more modest for longer sequences, which incur additional I/O costs (between\nregisters and SRAM for the p = 3 and between SRAM and HBM for p = 4). Without using the Monarch\ndecomposition for tensor cores (fusion-only), FlashFFTConv becomes bottlenecked by the speed of general\narithmetic operations on GPUs, and does not support sequences longer than 32K due to a lack of SRAM\nspace. Further benchmarks are given in Appendix B.\nDomain-Specific Optimizations Provide Further Speedup\nWe also benchmark domain-specific\noptimizations in FlashFFTConv. Table 4 shows the performance of a gated convolution y = v\u2299((u\u2299w)\u2217k),\nwhere v and w are linear projections of the input u. This pattern is common in convolutional and SSM-based\narchitectures for language modeling [43, 44, 78, 94]. A PyTorch implementation of a gated convolution incurs\nadditional I/O overhead from the gating operations, whereas FlashFFTConv fuses the gating operations\ninto the convolution. This fusion results in further speedup over PyTorch, up to 7.93\u00d7. Benchmarks of\nfurther domain-specific optimizations such as implicit padding (i.e., padding the input to ensure causality,\nwithout running an extra padding operation) are given in Appendix B.\n1We refer to Path-512 as a scaled-up version of Path-256.\n8\nTable 4: Time (\u2193) to compute the forward pass of a gated convolution with FlashFFTConv in milliseconds\non one H100-SXM. We also show memory savings. All results scaled to batch size 64, hidden dimension 768.\np indicates the order of the Monarch decomposition.\np = 2\np = 3\np = 4\nSequence Length\n256\n1K\n4K\n8K\n16K\n32K\n1M\n2M\n4M\nPyTorch\n0.62\n2.30\n9.49\n19.4\n29.9\n84.8\n3,071.4\n6,342.6\n13,031.2\nFlashFFTConv\n0.11\n0.29\n1.43\n3.58\n12.2\n26.3\n1,768.9\n4,623.5\n10,049.4\nSpeedup\n5.64\u00d7\n7.93\u00d7\n6.64\u00d7\n5.42\u00d7\n2.45\u00d7\n3.22\u00d7\n1.74\u00d7\n1.37\u00d7\n1.30\u00d7\nMemory Savings\n6.65\u00d7\n6.40\u00d7\n6.35\u00d7\n6.34\u00d7\n6.17\u00d7\n5.87\u00d7\n2.82\u00d7\n2.81\u00d7\n2.81\u00d7\nTable 5: End-to-end throughput (\u2191) of convolutional sequence models against PyTorch.\nModel (size, seqlen, unit)\nPyTorch\nFlashFFTConv\nSpeedup\nM2-BERT-base (110M, 128, seqs/s)\n4,480\n8,580\n1.9\u00d7\nHyena-s-4K (155M, 4K, seqs/s)\n84.1\n147\n1.7\u00d7\nLong convs, Path-X (102M, 16K, images/s)\n126\n308\n2.4\u00d7\nSaShiMi (5.4M, 64K, audio clips/s)\n38.7\n50.3\n1.3\u00d7\nHyenaDNA (1M, seqs/s)\n0.69\n3.03\n4.4\u00d7\nFlashFFTConv Provides Memory Savings\nTables 3 and 4 also show the memory savings from\nFlashFFTConv compared to PyTorch. FlashFFTConv reduces the memory footprint of convolutions\nand gated convolutions by using recomputation in the backward pass and kernel fusion. The absolute\nmemory savings for gated convolutions is greater, since FlashFFTConv does not need to store intermediate\nactivations from the gating operations (see Appendix B), but the relative memory savings is smaller since\ngated convolutions take more memory.\nFlashFFTConv Speeds Up Convolutional Sequence Models\nWe benchmark end-to-end throughput\nof convolutional sequence models across various modalities and sequence lengths spanning four orders of\nmagnitude. We benchmark M2-BERT-base [42], a BERT-style language model that has sequence length\n128; Hyena-s-4K [94], a GPT-style language model with sequence length 4K; a long-convolutional model [44]\ntrained on Path-X with sequence length 16K [104]; SaShiMi [45], an audio generation model trained on\n1-second audio clips sampled at 64 KHz; and HyenaDNA-1M [82], a DNA modeling model trained on\n1M sequence length. Details of the architectures and architecture-specific optimizations (such as fusing\nmultiplicative gating for M2 and Hyena models) are given in Appendix C.\nTable 5 shows that FlashFFTConv speeds up these models end-to-end. Speedup varies vary by the size\nof the models and the relative amount of time spent computing the convolution compared to other parts of\nthe models. For example, FlashFFTConv only speeds up the SaShiMi model by 1.3\u00d7, since the model\ninterleaves convolutions with SSM-based filter generation, pooling layers, and MLPs, which reduces the\nrelative amount of time spent computing the convolution itself. Speedup is greatest for HyenaDNA, where\nPyTorch is bottlenecked by small batch size. The PyTorch implementation only allows batch size 1 on an\n80GB GPU, whereas FlashFFTConv allows batch size 4\u2014yielding significant speedup.\nFlashFFTConv is Faster than FlashAttention-v2\nWe compare end-to-end efficiency of a 2.7B-\nparameter Hyena model using FlashFFTConv against a 2.7B-parameter GPT model using FlashAttention-\nv2 [22] at three sequence lengths. Table 6 shows throughput, end-to-end FLOP utilization, and speedup.\nFlashFFTConv achieves lower end-to-end FLOP utilization than FlashAttention-v2 but achieves higher\nthroughput, since convolutions incur fewer overall FLOPs.\n9\nTable 6: End-to-end throughput (\u2191) in thousands of tokens per second, FLOP utilization, and speedup of\nHyena against GPT running FlashAttention-v2 [22] across sequence lengths for A100.\nModel\n2K\n8K\n16K\nGPT-2.7B, FA-v2 [22]\n33.8\n27.8\n21.6\nHyena-2.7B, FlashFFTConv\n35.2\n35.2\n32.3\nFA-v2 FLOP Utilization\n65.7\n72.1\n78.5\nFlashFFTConv FLOP Utilization\n62.3\n61.9\n56.5\nFlashFFTConv Speedup\n1.1\u00d7\n1.3\u00d7\n1.5\u00d7\nTable 7: Quality and memory footprint of partial convolutions during training across sequence lengths.\nHyena-s-8K\n8K\n4K\n2K\n1K\n512\n256\nPPL (\u2193)\n13.8\n13.8\n13.8\n13.9\n14.0\n14.2\nMemory Footprint (\u2193)\n32.5G\n15.3G\n11.8G\n8.4G\n6.1G\n5.8G\n4.3\nPartial and Frequency-Sparse Convolutions\nWe evaluate the impact of partial convolutions on downstream quality and memory footprint and on how\nwell they can extend the sequence length of existing models. We evaluate the impact of frequency-sparse\nconvolutions on downstream quality, and we show that frequency-sparse convolutions can yield up to 1.4\u00d7\nadditional speedup in the convolution without impacting quality.\nPartial Convolutions Reduce Memory Footprint and Increase Sequence Length\nPartial convo-\nlutions reduce the memory footprint of models, in both language modeling and DNA modeling. A large\nproportion of the convolution filters can be pruned without impacting downstream quality. Table 7 shows\nthat a Hyena-s-8K model can be pretrained with a much shorter convolution kernel\u2014as short as 2K\u2014without\nnegatively impacting quality.\nPartial convolutions yield another benefit: we can naturally extend the sequence length of existing\npretrained models. We extend a pretrained HyenaDNA-1M model to 4M sequence length with promising\nPPL results (Table 8)\u2014yielding the first model that can embed the longest human genes at single-nucleotide\nresolution (2.3M base pairs) (See Appendix B for a visualization of gene embeddings).\nFrequency-Sparse Convolutions Increase Throughput\nFrequency-sparse convolutions can increase\nthe speed of convolutions\u2014and may also have positive effects on quality. Table 9 shows that we can set up to\n79% of the entries of the kernel kf to zero without losing quality. Sparsification in frequency space may even\nimprove the quality of pretrained models slightly; the PPL of a pretrained HyenaDNA-1M model improves\nby 0.01 points after its kernels are 75% sparsified in frequency space\u2014potentially as a result of removing\nhigh-frequency noise. Sparsification also yields up to 1.4\u00d7 speedup in the convolution via skipping entire\nblocks of the matrix-matrix multiplies in the Monarch decomposition. Appendix C provides more details\nabout the sparsity patterns used in Table 9.\n5\nRelated Work\nLong Convolutions in Sequence Modeling\nLong convolutional models have emerged as a promising\nalternative to Transformers for sequence modeling [42\u201344, 46\u201348, 52, 76, 82, 94, 96, 97, 101]. These methods\ndiffer in how they generate the convolutional kernels; for example, the S4 line of work uses learned state\nspace models [46, 49, 76, 78], while other works [94, 96, 97] parameterize the convolution using an MLP\nfrom positional encodings. However, all the models operate by taking a convolution over the input sequence\nwith a kernel as long as the input: y = u \u2217 k, where u \u2208 RB\u00d7H\u00d7N, k \u2208 RH\u00d7N, and the kernel k is broadcast\nalong the B dimension. When used for language modeling, these models often incorporate elementwise\n10\nTable 8: PPL (\u2193) from using partial convolutions to extend the sequence length of HyenaDNA to longer\nsequences. At 4M sequence length, the models are able to embed the longest human genes.\nBase Filter Length\n1M\n2M\n4M\nHyenaDNA-450K\n2.91\n2.91\n2.91\nHyenaDNA-1M\n2.91\n2.91\n2.90\nTable 9: Applying frequency-sparsity to the filters of a pretrained HyenaDNA-1M model.\nSparsity Fraction\n0%\n50%\n75%\n79%\n84%\n91%\nPPL (\u2193)\n2.91\n2.91\n2.90\n2.91\n2.93\n2.98\nConvolution Speedup (\u2191)\n1.0\u00d7\n1.2\u00d7\n1.3\u00d7\n1.4\u00d7\n1.5\u00d7\n1.8\u00d7\nmultiplicative gating as well: y = f(u) \u2299 ((g(u) \u2299 h(u)) \u2217 k), where f, g, and h are linear maps along the H\ndimension [42, 43, 78, 94, 110].\nLong-Context Applications\nLong convolutional models have especially been helpful for long-context\napplications, such as DNA modeling and speech synthesis. In DNA modeling, most longer-context genomic\nmodels have relied on either tokenization [56, 107, 113] or downsampling [3, 38]. However, recent work has\nsuggested that modeling DNA directly from base pairs can yield downstream improvements in quality, which\nrequires long sequence lengths [82].\nLike DNA modeling, speech synthesis has also benefited from long-context modeling. While traditional\nspeech synthesis pipelines use intermediate representations such as spectrograms [64, 95, 99], linguistic\nfeatures [10, 59, 89], or discrete audio codes [30, 31, 67, 108], recent work has shown that modeling the speech\ndirectly from the raw waveform can yield downstream improvements in quality [45]. Again, such models\nrequire long sequences to model audio at the rate at which it is naturally sampled, necessitating long-sequence\nmodeling.\nFFT Algorithms\nThere is a long history of efficient FFT algorithms, ranging from the Cooley-Tukey FFT\nalgorithm published in 1965 [19] to parallel FFT algorithms [4] and more [5, 6, 18]. These algorithms have\nenabled fundamental progress in a range of disciplines, from control theory [7, 12] to signal processing [90, 91].\nAs FFTs prove more useful for modern deep learning applications, such as long convolutions, new techniques\nare required to run them efficiently on modern accelerators. Our work continues a line of work exploring how\nto use tensor cores for the FFT convolution [43, 44, 69], and extends the algorithmic capabilities to much\nlonger sequences.\nSparsity in Deep Learning\nAs deep learning models have grown larger and deeper [11, 13, 17], there is\nincreasing interest in reducing the cost of training and running models. Sparsity in particular has received\na great deal of attention, and has a long history in machine learning, including work in pruning neural\nnetworks [32, 50, 51, 72, 98] and finding lottery tickets [39\u201341]. Our work in partial convolutions and frequency-\nsparse convolutions relates to this line of work, as an analogue of sparsity in convolutional filters. The\nMonarch decomposition is also closely related to structured matrices. Structured matrices have subquadratic\n(o(n2) for dimension n\u00d7n) parameters and runtime, such as sparse and low-rank matrices, and fast transforms\n(Fourier, Chebyshev, sine/cosine, orthogonal polynomials) [23]. Structured matrices can often be computed\nwith simple divide-and-conquer schemes, and can be used to represent many fast transforms [28, 34, 58, 100].\nOptimization of deep learning primitives\nThere is a rich history of optimizing deep learning primitives.\nMany techniques, such as kernel fusion, aim to reduce data movement. Recently, libraries such as PyTorch\n2.0 [93] have added kernel fusion automatically. Other techniques include checkpointing, wherein one stores\nfewer intermediate results and recomputes the others on-the-fly where they are needed, trading additional\ncompute for memory [65, 111]. Many algorithms also have hand-optimizations that can remove unnecessary\ncomputation or memory accesses [79].\n11\nAnother line of optimization techniques aims to reduce FLOPs. MLPs and attention are particularly\npopular targets of FLOP reduction, via sparse factorizations of weights [14, 19, 23, 25, 26, 29, 39, 116], or\nsparse/low-rank approximations of attention [8, 16, 21, 33, 37, 60, 62, 75, 112, 116] and their combinations [15,\n105].\n6\nConclusion\nWe present FlashFFTConv, a new system for optimizing FFT convolutions for long sequences. We show\nthat FlashFFTConv improves quality under a fixed compute budget, enables longer-sequence models, and\nimproves the efficiency of long convolutions. We also show that analogues of sparsity in convolution filters\nmap naturally on to FlashFFTConv\u2019s compute model, and can reduce memory footprint and runtime.\nWe hope that our work will help support further adoption of convolutional sequence models, and that our\ninsights can help inform the design of future architectures.\nAcknowledgments\nWe gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882\n(ASED); NIH under No.\nU54EB020405 (Mobilize), NSF under Nos.\nCCF1763315 (Beyond Sparsity),\nCCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No.\nN000141712266 (Unifying\nWeak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba,\nTSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation,\nAmerican Family Insurance, Google Cloud, Microsoft Azure, Swiss Re, Brown Institute for Media Innovation,\nDepartment of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship\n(NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research\nFellowship Program, Texas Instruments Stanford Graduate Fellowship in Science and Engineering, and\nmembers of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and\nInfosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes\nnotwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations\nexpressed in this material are those of the authors and do not necessarily reflect the views, policies, or\nendorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.\nReferences\n[1] Gustaf Ahdritz, Nazim Bouatta, Sachin Kadyan, Qinghui Xia, William Gerecke, Timothy J O\u2019Donnell,\nDaniel Berenberg, Ian Fisk, Niccol`o Zanichelli, Bo Zhang, et al. Openfold: Retraining alphafold2 yields\nnew insights into its learning mechanisms and capacity for generalization. bioRxiv, pages 2022\u201311, 2022.\n[2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding,\nQing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention\nmechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop\non Efficient Systems for Foundation Models@ ICML2023, 2023.\n[3] \u02c7Ziga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R\nTaylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression\nprediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196\u20131203,\n2021.\n[4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding\ntransformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068\u20131081,\n2011.\n[5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on\nnetwork-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255\u2013269, 2009.\n12\n[6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE\nconference on Supercomputing, pages 234\u2013242, 1989.\n[7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016.\n[8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150, 2020.\n[9] Alberto Bietti and Julien Mairal.\nInvariance and stability of deep convolutional representations.\nAdvances in neural information processing systems, 30, 2017.\n[10] Miko laj Bi\u00b4nkowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman Casagrande,\nLuis C Cobo, and Karen Simonyan. High fidelity speech synthesis with adversarial networks. In\nInternational Conference on Learning Representations, 2019.\n[11] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities\nand risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\n[12] E Oran Brigham. The fast Fourier transform and its applications. Prentice-Hall, Inc., 1988.\n[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot\nlearners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[14] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher R\u00b4e.\nPixelated butterfly: Simple and efficient sparse training for neural network models. 2021.\n[15] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R\u00b4e. Scatterbrain: Unifying\nsparse and low-rank attention. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\n[16] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with\nperformers. arXiv preprint arXiv:2009.14794, 2020.\n[17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[18] Eleanor Chu and Alan George. Inside the FFT black box: serial and parallel fast Fourier transform\nalgorithms. CRC press, 1999.\n[19] James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier\nseries. Mathematics of computation, 19(90):297\u2013301, 1965.\n[20] Fiona Cunningham, James E Allen, Jamie Allen, Jorge Alvarez-Jarreta, M Ridwan Amode, Irina M\nArmean, Olanrewaju Austine-Orimoloye, Andrey G Azov, If Barnes, Ruth Bennett, et al. Ensembl\n2022. Nucleic acids research, 50(D1):D988\u2013D995, 2022.\n[21] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential\nredundancy for efficient language processing. Advances in neural information processing systems,\n33:4271\u20134282, 2020.\n[22] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv\npreprint arXiv:2307.08691, 2023.\n[23] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu,\nAniruddh Rao, Atri Rudra, and Christopher R\u00b4e. Monarch: Expressive structured matrices for efficient\nand accurate training. In International Conference on Machine Learning. PMLR, 2022.\n13\n[24] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing\nSystems, 2022.\n[25] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\u00b4e. Learning fast algorithms for\nlinear transforms using butterfly factorizations. arXiv preprint arXiv:1903.05895, 2020.\n[26] Tri Dao, Nimit S. Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri\nRudra, and Christopher R\u00b4e. Kaleidoscope: An efficient, learnable representation for all structured\nlinear maps. arXiv preprint arXiv:2012.14966, 2021.\n[27] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision s4: Efficient sequence-\nbased rl via state spaces layers. In The Eleventh International Conference on Learning Representations,\n2022.\n[28] Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher R\u00b4e, and Atri Rudra. A two-pronged\nprogress in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual\nACM-SIAM Symposium on Discrete Algorithms, pages 1060\u20131079. SIAM, 2018.\n[29] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing\nperformance. arXiv preprint arXiv:1907.04840, 2019.\n[30] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever.\nJukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020.\n[31] Sander Dieleman, Aaron van den Oord, and Karen Simonyan. The challenge of realistic music generation:\nmodelling raw audio at scale. Advances in neural information processing systems, 31, 2018.\n[32] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise\noptimal brain surgeon. Advances in Neural Information Processing Systems, 30, 2017.\n[33] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models\nwith mixture-of-experts. In International Conference on Machine Learning, pages 5547\u20135569. PMLR,\n2022.\n[34] Yuli Eidelman and Israel Gohberg. On a new class of structured matrices. Integral Equations and\nOperator Theory, 34(3):293\u2013324, 1999.\n[35] Murali Emani, Venkatram Vishwanath, Corey Adams, Michael E Papka, Rick Stevens, Laura Florescu,\nSumti Jairath, William Liu, Tejas Nama, and Arvind Sujeeth. Accelerating scientific applications with\nsambanova reconfigurable dataflow architecture. Computing in Science & Engineering, 23(2):114\u2013119,\n2021.\n[36] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar,\nChunxi Liu, Yangyang Shi, Ozlem Kalinli, Mike Seltzer, et al. Multi-head state space model for speech\nrecognition. arXiv preprint arXiv:2305.12498, 2023.\n[37] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232\u20135270,\n2022.\n[38] Quentin Fournier, Ga\u00b4etan Marceau Caron, and Daniel Aloise. A practical survey on faster and lighter\ntransformers. ACM Computing Surveys, 2021.\n[39] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\nnetworks. arXiv preprint arXiv:1803.03635, 2018.\n14\n[40] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode con-\nnectivity and the lottery ticket hypothesis. In International Conference on Machine Learning, pages\n3259\u20133269. PMLR, 2020.\n[41] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing the\nlottery ticket hypothesis. arXiv preprint arXiv:1903.01611, 2019.\n[42] Daniel Y. Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W. Thomas,\nBenjamin F. Spector, Michael Poli, Atri Rudra, and Christopher R\u00b4e. Monarch Mixer: A simple\nsub-quadratic GEMM-based architecture. In Advances in Neural Information Processing Systems, 2023.\n[43] Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00b4e. Hungry\nHungry Hippos: Towards language modeling with state space models. In International Conference on\nLearning Representations, 2023.\n[44] Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra,\nand Christopher R\u00b4e. Simple hardware-efficient long convolutions for sequence modeling. International\nConference on Machine Learning, 2023.\n[45] Karan Goel, Albert Gu, Chris Donahue, and Christopher R\u00b4e. It\u2019s raw! audio generation with state-space\nmodels. arXiv preprint arXiv:2202.09729, 2022.\n[46] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state\nspaces. In International Conference on Learning Representations, 2021.\n[47] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00b4e. On the parameterization and initialization\nof diagonal state space models. In Advances in Neural Information Processing Systems, 2022.\n[48] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R\u00b4e. How to train your hippo:\nState space models with generalized orthogonal basis projections. arXiv preprint arXiv:2206.12037,\n2022.\n[49] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured\nstate spaces. In Advances in Neural Information Processing Systems, 2022.\n[50] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks\nwith pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.\n[51] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient\nneural network. Advances in neural information processing systems, 28, 2015.\n[52] Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela\nRus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022.\n[53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\n[54] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach. Elsevier,\n2011.\n[55] Md Mohaiminul Islam, Mahmudul Hasan, Kishan Shamsundar Athrey, Tony Braskich, and Gedas\nBertasius.\nEfficient movie scene detection using state-space transformers.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18749\u201318758, 2023.\n[56] Yanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri. DNABERT: pre-trained bidirectional\nencoder representations from transformers model for DNA-language in genome.\nBioinformatics,\n37(15):2112\u20132120, 2021.\n15\n[57] Norm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay\nSubramanian, Andy Swing, Brian Towles, et al. Tpu v4: An optically reconfigurable supercomputer for\nmachine learning with hardware support for embeddings. In Proceedings of the 50th Annual International\nSymposium on Computer Architecture, pages 1\u201314, 2023.\n[58] Thomas Kailath, Sun-Yuan Kung, and Martin Morf.\nDisplacement ranks of matrices and linear\nequations. Journal of Mathematical Analysis and Applications, 68(2):395\u2013407, 1979.\n[59] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart,\nFlorian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio\nsynthesis. In International Conference on Machine Learning, pages 2410\u20132419. PMLR, 2018.\n[60] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00b8cois Fleuret. Transformers are\nrnns: Fast autoregressive transformers with linear attention. In International Conference on Machine\nLearning, pages 5156\u20135165. PMLR, 2020.\n[61] Sanghyeon Kim and Eunbyung Park. Smpconv: Self-moving point representations for continuous\nconvolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 10289\u201310299, 2023.\n[62] Nikita Kitaev,  Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv\npreprint arXiv:2001.04451, 2020.\n[63] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional\nneural networks. Advances in neural information processing systems, 25, 2012.\n[64] Kundan Kumar, Rithesh Kumar, Thibault De Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo,\nAlexandre De Brebisson, Yoshua Bengio, and Aaron C Courville. Melgan: Generative adversarial\nnetworks for conditional waveform synthesis. Advances in neural information processing systems, 32,\n2019.\n[65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph\ntheoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in\nNeural Information Processing Systems, 32, 2019.\n[66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving\nwith pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems\nPrinciples, 2023.\n[67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh\nNguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language\nmodeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336\u20131354,\n2021.\n[68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper,\n2022.\n[69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\narXiv preprint arXiv:2104.11471, 2021.\n[70] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,\nMarc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you!\narXiv preprint arXiv:2305.06161, 2023.\n[71] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional\nmodels great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022.\n[72] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. Advances in neural information\nprocessing systems, 30, 2017.\n16\n[73] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. arXiv preprint\narXiv:2305.19370, 2023.\n[74] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A\nconvnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11976\u201311986, 2022.\n[75] Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer.\nLuna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34:2441\u2013\n2453, 2021.\n[76] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and\nLuke Zettlemoyer. Mega: moving average equipped gated attention. arXiv preprint arXiv:2209.10655,\n2022.\n[77] Temesgen Mehari and Nils Strodthoff. Towards quantitative precision for ecg analysis: Leveraging\nstate space models, self-supervision and patient metadata. IEEE Journal of Biomedical and Health\nInformatics, 2023.\n[78] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling\nvia gated state spaces. arXiv preprint arXiv:2206.13947, 2022.\n[79] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint\narXiv:1805.02867, 2018.\n[80] Koichi Miyazaki, Masato Murata, and Tomoki Koriyama. Structured state space decoder for speech\nrecognition and synthesis. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023.\n[81] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and\nChristopher R\u00b4e. S4nd: Modeling images and videos as multidimensional signals with state spaces. In\nAdvances in neural information processing systems, 2022.\n[82] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman\nPatel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. Hyenadna: Long-range genomic\nsequence modeling at single nucleotide resolution. In Advances in Neural Information Processing\nSystems, 2023.\n[83] NVIDIA. Nvidia Tesla V100 GPU architecture, 2017.\n[84] NVIDIA. Nvidia A100 tensor core GPU architecture, 2020.\n[85] NVIDIA. Nvidia H100 tensor core GPU architecture, 2022.\n[86] NVIDIA. Cuda c++ programming guide, 2023. https://docs.nvidia.com/cuda/cuda-c-programming-\nguide/index.html.\n[87] NVIDIA. cufftdx v1.1.0 documentation, 2023. https://docs.nvidia.com/cuda/cufftdx/index.html.\n[88] NVIDIA. Cutlass 3.2, 2023. https://github.com/NVIDIA/cutlass.\n[89] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal\nKalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.\narXiv preprint arXiv:1609.03499, 2016.\n[90] Alan V Oppenheim. Applications of digital signal processing. Englewood Cliffs, 1978.\n[91] Alan V Oppenheim, John R Buck, and Ronald W Schafer. Discrete-time signal processing. Vol. 2.\nUpper Saddle River, NJ: Prentice Hall, 2001.\n17\n[92] Daniele Paliotta, Matteo Pagliardini, Martin Jaggi, and Fran\u00b8cois Fleuret. Fast causal attention with\ndynamic sparsity. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023.\n[93] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-\nperformance deep learning library. Advances in neural information processing systems, 32, 2019.\n[94] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio,\nStefano Ermon, and Christopher R\u00b4e. Hyena hierarchy: Towards larger convolutional language models.\nProceedings of the 40th International Conference on Machine Learning (ICML 2023), 2023.\n[95] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A flow-based generative network for\nspeech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 3617\u20133621. IEEE, 2019.\n[96] David W Romero, Robert-Jan Bruintjes, Jakub M Tomczak, Erik J Bekkers, Mark Hoogendoorn, and\nJan C van Gemert. Flexconv: Continuous kernel convolutions with differentiable kernel sizes. arXiv\npreprint arXiv:2110.08059, 2021.\n[97] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub Mikolaj Tomczak, and Mark Hoogendoorn.\nCkconv: Continuous kernel convolution for sequential data. In International Conference on Learning\nRepresentations, 2021.\n[98] Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning.\nAdvances in Neural Information Processing Systems, 33:20378\u201320389, 2020.\n[99] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng\nChen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al. Natural tts synthesis by conditioning wavenet\non mel spectrogram predictions. In 2018 IEEE international conference on acoustics, speech and signal\nprocessing (ICASSP), pages 4779\u20134783. IEEE, 2018.\n[100] Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep\nlearning. Advances in Neural Information Processing Systems, 28, 2015.\n[101] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence\nmodeling. In The Eleventh International Conference on Learning Representations, 2023.\n[102] H V Sorensen, D Jones, Michael Heideman, and C Burrus. Real-valued fast fourier transform algorithms.\nIEEE Transactions on acoustics, speech, and signal processing, 35(6):849\u2013863, 1987.\n[103] Siyi Tang, Jared A Dunnmon, Liangqiong Qu, Khaled K Saab, Christopher Lee-Messer, and Daniel L\nRubin. Spatiotemporal modeling of multivariate signals with graph neural networks and structured\nstate space models. arXiv preprint arXiv:2211.11176, 2022.\n[104] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu\nYang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers.\nIn International Conference on Learning Representations, 2020.\n[105] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM\nComputing Surveys, 55(6):1\u201328, 2022.\n[106] Yi Tay, Mostafa Dehghani, Jai Prakash Gupta, Vamsi Aribandi, Dara Bahri, Zhen Qin, and Donald\nMetzler. Are pretrained convolutions better than pretrained transformers? In Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers), pages 4349\u20134359, 2021.\n[107] Yi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin,\nSimon Baumgartner, Cong Yu, and Donald Metzler. Charformer: Fast character transformers via\ngradient-based subword tokenization. arXiv preprint arXiv:2106.12672, 2021.\n18\n[108] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural\ninformation processing systems, 30, 2017.\n[109] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid.\nSelective structured state-spaces for long-form video understanding. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 6387\u20136397, 2023.\n[110] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention.\narXiv preprint arXiv:2212.10544, 2022.\n[111] Qipeng Wang, Mengwei Xu, Chao Jin, Xinran Dong, Jinliang Yuan, Xin Jin, Gang Huang, Yunxin\nLiu, and Xuanzhe Liu. Melon: Breaking the memory wall for resource-efficient on-device machine\nlearning. In Proceedings of the 20th Annual International Conference on Mobile Systems, Applications\nand Services, pages 450\u2013463, 2022.\n[112] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768, 2020.\n[113] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOntanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer\nsequences. Advances in neural information processing systems, 33:17283\u201317297, 2020.\n[114] Dan Zhang, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna Goldie, and Azalia\nMirhoseini. A full-stack search technique for domain optimized deep learning accelerators. In Proceedings\nof the 27th ACM International Conference on Architectural Support for Programming Languages and\nOperating Systems, pages 27\u201342, 2022.\n[115] Michael Zhang, Khaled Kamal Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher Re. Effectively\nmodeling time series with simple discrete state spaces.\nIn International Conference on Learning\nRepresentations, 2022.\n[116] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and\nBryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. Advances in\nNeural Information Processing Systems, 34:17723\u201317736, 2021.\n19\nAppendix\nWe present additional algorithmic details (Appendix A), additional experimental results (Appendix B), and\nexperimental details (Appendix C).\nA\nAlgorithm Details\nA.1\nDomain-Specific Optimizations\nWe review the details of how to compute a real-to-real FFT of size N using a complex FFT of size N/2,\nfollowing a tutorial by [102].\nFor this section, we adopt notation common in describing FFT algorithms. Let x(n) be an input sequence\nof length N, and let X(k) be the result of its discrete Fourier transform. Recall that:\nX(k) =\nN\u22121\nX\nn=0\nx(n)W nk\nN ,\n(3)\nfor k = 0, 1, . . . , N \u2212 1, where WN = e\u22122\u03c0i/N is the Nth root of unity.\nFirst, if x(n) is real, then symmetries emerge in X(k). In particular, we have X(k) = X\u2217(\u2212k) = X\u2217(N \u2212k),\nwhere \u2217 denotes complex conjugation. These symmetries allow us to have an algorithm for computing X(k)\nusing a single complex DFT of size N/2.\nIn particular:\nX(k) =\nN\u22121\nX\nn=0\nx(n)W nk\nN\n=\nN/2\u22121\nX\nn=0\nx(2n)W nk\nN/2 + W k\nN\nN/2\u22121\nX\nn=0\nx(2n + 1)W nk\nN/2,\nfor k = 0, 1, . . . , N \u2212 1. The DFT is now decomposed into two parts: a DFT over the even-indexed elements\nof x(n), and over the odd-indexed elements of x(n).\nWe can now create a third complex sequence, of length N/2, and put the even-indexed elements of x(n)\nin the real part, and the odd-indexed elements of x(n) in the imaginary part. Let:\nz(n) = x(2n) + ix(2n + 1),\nfor n = 0, 1, . . . , N/2 \u2212 1. Then, we compute the N/2-sized DFT Z(k), and we can recover the DFT over the\neven and odd parts of x(n) (Xe[k] and Xo[k], respectively):\nXe[k] = Z[k] + Z\u2217[N/2 \u2212 k]\n2\nXo[k] = \u2212iZ[k] \u2212 Z\u2217[N/2 \u2212 k]\n2i\n.\nWe can now recover X[k], k = 0 . . . , N \u2212 1 using:\nX[k] = Xe[k\nmod N/2] + Xo[k\nmod N/2]W k\nN.\nThe inverse FFT proceeds similarly. The goal is to recover x(n) given an input X[k], using a simple\ncomplex inverse DFT of length N/2.\nFirst, we recover Xe[k] and Xo[k]:\nXe[k] = X[k] + X\u2217[N/2 \u2212 k]\n2\nXo[k] = X[k] \u2212 X\u2217[N/2 \u2212 k]\n2\nW k\nN,\n20\nfor k = 0, . . . , N/2 \u2212 1. Then, we construct Z[k]:\nZ[k] = Xe[k] + iXo[k], k = 0 . . . , N/2 \u2212 1.\nWe use the inverse DFT to recover z(n), and then recover x(n) from the real and imaginary parts of z(n):\nx(2n) = Re(zn)\nx(2n + 1) = Im(zn),\nfor n = 0, . . . , N/2 \u2212 1.\nTo implement these in our kernels, we perform the bookkeeping after reading the inputs or before writing\nthe output, and then use the FFT/iFFT implementations as detailed in Algorithm 1 and others.\nA.2\nLow-level CUDA details\nTo ensure high performance, we implement CUDA kernels for each specific sequence length, allowing us to\ncater to specific performance nuances that arise from the decomposition at that sequence length. In this\nsection, we dive into some of the low-level implementation details for FlashFFTConv.\nMatrix Multiplication Using CUDA Tensor cores\nCUDA Tensor cores can perform the multiplication\nof two m \u00d7 k and k \u00d7 n matrices for bfloat16 or float16 elements, using around the same number of cycles as\nis required for the multiplication of two scalars. m \u00d7 k \u00d7 n must be of one of the following: 16 \u00d7 16 \u00d7 16,\n32 \u00d7 8 \u00d7 16,\n8 \u00d7 32 \u00d7 16. This informs our choice of radix for decomposition when performing the FFT and\niFFT. In particular our implementation breaks down matrix-matrix multiplications into blocked matrix-matrix\nmultiplications where m \u00d7 k \u00d7 n = 16 \u00d7 16 \u00d7 16. We note the following about matrix-matrix multiplication\non tensor cores [86]:\n\u2022 Tensor cores are utilized at the level of the warp and programmatic access of the tensor cores is via the\nWarp Level Matrix Multiply Accumulate (WMMA) API.\n\u2022 Tensor core operands are held in register fragments (wmma :: matrix a, and wmma :: matrix b) and\nresults are written to a register fragment (wmma :: accumulator).\n\u2022 The operand fragments can hold data in row-major or column-major format and data in the wmma ::\naccumulator fragment can be written to memory in row-major or column-major format.\n\u2022 The specific mapping of items in a fragment to threads in warp is unspecified, however, the mapping of\nitems to threads in the wmma :: accumulator fragment exactly matches that for the wmma :: matrix a\nfragment read row-major, allowing us to directly copy the results of a matrix-matrix multiplication and\nuse as the operand for another matrix-matrix multiply.\nTo perform a matrix-matrix multiplication C = A \u00d7 B using the tensor cores, a warp loads the contents\nof A and B into registers (WMMA fragments in CUDA parlance), performs the matrix-matrix multiplication,\nand writes the results which are stored in an accumulator fragment back to memory.\nRegister Reuse\nA key part of ensuring high performance is minimizing I/O across different levels of the\nmemory hierarchy: from HBM to SRAM and from SRAM to registers. To ensure this, we move the output\nfrom the accumulator fragment directly into matrix a fragment for use in subsequent matrix multiplications,\navoiding an extra trip to SRAM. However, this is only possible if the output from the previous matrix-matrix\nmultiply does not need to be transposed before using it as an operand for the next one. When this is not the\ncase, we need to make a trip to SRAM and back. In Algorithm 2 we detail I/O from SRAM to registers.\nLocality and Tiling\nThe algorithm is trivially parallelizable across B and H, allowing us to tile in both\ndimensions at the threadblock level. In Algorithm 3 , all loops from i \u2190 1 to N1 are warp-tiled.\n21\nAlgorithm 2 Detailed Annotation of FlashFFTConv core algorithm showing I/O from SRAM to register\nfragments, with two-way Monarch decomposition. We assume N = N 2\n1 for simplicity here.\nInput: Input u \u2208 RB\u00d7H\u00d7N, convolution kernel kf \u2208 CH\u00d7N, FFT matrices F \u2208 CN1\u00d7N1, F\u22121 \u2208 CN1\u00d7N1,\nTwiddle factors t \u2208 CN, tinv \u2208 CN, B tile size Btile, H tile size Htile.\nOutput: Output y \u2208 RB\u00d7H\u00d7N.\nfor SMs in parallel across B/Btile \u00d7 H/Htile do\nLoad F, F\u22121, t, tinv from HBM.\nfor h \u2190 1 to Htile do\nLoad Kf \u2190 kf[h] from HBM, reshaped to N1 \u00d7 N1.\nfor b \u2190 1 to Btile do\nLoad X \u2190 u[b, h] from HBM, reshaped to N1 \u00d7 N1.\nX \u2190 F\u22a4X\n\u25b7 F\u22a4 (matrix a), X (matrix b) output to accumulator\nLoad X from accumulator to matrix a\nX \u2190 X \u2217 t\n\u25b7 Elementwise multiply directly in matrix a\nX \u2190 XF\n\u25b7 X (matrix a), F (matrix b) output to accumulator\nLoad X from accumulator to matrix a\nX \u2190 X \u2217 Kf\n\u22a4\n\u25b7 Elementwise multiply with kf directly in matrix a\nX \u2190 XF\u22121\n\u25b7 X (matrix a), F\u22121 (matrix b) output to accumulator\nWrite X from accumulator fragment to SRAM\nLoad X\u22a4 from SRAM to matrix a fragment\nX \u2190 X\u22a4 \u2217 tinv\n\u25b7 Elementwise multiply with tinv directly in matrix a\nY \u2190 XF\u22121\n\u25b7 X (matrix a), F\u22121 (matrix b) output to accumulator\nWrite Y\u22a4 to HBM.\nMiscellaneous optimizations\nIn addition to the above optimizations, we also perform some other\noptimizations that provide marginal speedup. These include: utilizing vector intrinsics/types for performing\nmemory reads/writes and arithmetic for 16-bit floating point (fp16) and brain float point (bf16), allowing\nnon-tensor core operations on these types to be performed at around twice the normal speed. Furthermore,\nwe double buffer I/O movements across all levels of the memory hierarchy, reducing warp stalls. We also\naggressively tune our kernel hyperparameters such as block and tile dimensions, and loop unrolling factors\nfor the best performance on the specific underlying hardware.\nA.3\nGeneralization to 3-way and 4-way Monarch Decompositions\nWe provide algorithm listings for 3-way and 4-way Monarch Decompositions.\n3-Way Decomposition\nAlgorithm 3 shows the algorithm for a 3-way Monarch decomposition. It involves\none extra matrix multiply operation on either side of the FFT and iFFT, and proceeds over the algorithm in\nAlgorithm 1 in an inner loop.\n4-way Decomposition\nFor the 4-way decomposition, we assume that we need to write intermediate\noutputs to HBM. Here, we treat the 3-way decomposition as a sub-routine, and assume it has a fused kernel\n(i.e., Algorithm 3). We compute one matrix multiply for the FFT and one for the iFFT, and then call the\nkernel for the 3-way decomposition over the rows of the output. The algorithm is listed in Algorithm 4.\nA.4\nFrequency-Sparse Patterns\nWe describe frequency-sparse patterns and the matmul savings in more detail here. We use the full 4-way\ndecomposition case, since the algorithms generalize to lower-order decompositions.\nLet N = N 4\n1 , and consider a kernel kf \u2208 CN. Consider the matrix multiply and looping operations that\noccur when computing the FFT portions of FlashFFTConv (u, kf) (the iFFT portions are the same, in\nthe opposite order):\n22\nAlgorithm 3 FlashFFTConv algorithm for 3-way decomposition. We assume N = N 3\n1 for simplicity here.\nInput: Input u \u2208 RB\u00d7H\u00d7N, convolution kernel kf \u2208 CH\u00d7N, FFT matrices F \u2208 CN1\u00d7N1, F\u22121 \u2208 CN1\u00d7N1,\nTwiddle factors t1 \u2208 CN 2\n1 , t1,inv \u2208 C\u22ae\nN 2\n1 , t2 \u2208 CN, t2,inv \u2208 CN, B tile size Btile, H tile size Htile.\nOutput: Output y \u2208 RB\u00d7H\u00d7N.\nfor SMs in parallel across B/Btile \u00d7 H/Htile do\nLoad F, F\u22121, t, tinv from HBM.\nfor h \u2190 1 to Htile do\nLoad Kf \u2190 kf[h] from HBM, reshaped to N 2\n1 \u00d7 N1.\nKf \u2190 KT\nf .\n\u25b7 Transpose last two dimensions.\nReshape Kf to N1 \u00d7 N 2\n1 .\nfor b \u2190 1 to Btile do\nLoad X \u2190 u[b, h] from HBM, reshaped to N1 \u00d7 N1 \u00d7 N1.\nfor i \u2190 1 to N1 do\nX\u2032 \u2190 FX[:, i \u2217 N1 : (i + 1) \u2217 N1]\nX[:, i \u2217 N1 : (i + 1) \u2217 N1] \u2190 X\u2032\n\u25b7 Transpose, matmul, transpose.\nX \u2190 X \u2217 t2\nfor i \u2190 1 to N1 do\n\u25b7 Loop over rows\nX\u2032 \u2190 FX[i]\nReshape X\u2032 to N1 \u00d7 N1\nX\u2032 \u2190 ((F\u22a4X\u2032) \u2217 t)F\n\u25b7 FFT, decomposed into two steps\nX\u2032 \u2190 X\u2032 \u2217 Kf[i]\u22a4\n\u25b7 Elementwise multiply with kf\nY\u2032 \u2190 ((X\u2032F\u22121)\u22a4 \u2217 tinv)F\u22121\n\u25b7 Inverse FFT, decomposed into two steps\nY\u2032 \u2190 Y\u2032\u22a4\nY[i] \u2190 Y\u2032\n\u25b7 Finish inner loop\nY \u2190 Y \u2217 t2,inv\nfor i \u2190 1 to N1 do\nY\u2032 \u2190 FY[:, i \u2217 N1 : (i + 1) \u2217 N1]\nY[:, i \u2217 N1 : (i + 1) \u2217 N1] \u2190 Y\u2032\n\u25b7 Transpose, matmul, transpose.\nWrite Y to HBM.\n1. In Algorithm 4, there is one FFT operation over the columns of u, reshaped to N1 \u00d7 N/N1, and a\nTwiddle correction..\n2. Then, Algorithm 3 iterates over the rows of u for \u03b1 := N1 steps.\n3. Let u\u2032 be the row in a specific iteration. In Algorithm 3, there is an FFT over the columns of u\u2032,\nreshaped to N1 \u00d7 N 2\n1 , and a Twiddle correction.\n4. Then, the inner loop iterates over the rows of u\u2032 for \u03b2 := N1 steps.\n5. In each loop, u\u2032 has one FFT operation with a twiddle factor correction. Let the matrix of this FFT\noperation be denoted A.\n6. Then there is a second FFT operation. Let the matrix of this FFT operation be denoted B.\nNow, reshape kf to N1 \u00d7N1 \u00d7N1 \u00d7N1. Let us consider how sparsity along the each of the four dimensions\nof kf lets us skip operations in the above steps.\n\u2022 Sparsity in the first dimension allows us to skip computation in B, exactly in proportion to how much of\nthe first dimension we eliminate. This can result in cost savings, as long as B can still be expressed using\nthe tensor cores on-chip after skipping the computation. For example, if B is 32\u00d732, then N1 = 32,\nand it does not make sense to eliminate more than half of the first dimension.\n\u2022 Sparsity in the second dimension works exactly the same way, except it allows us to skip computation\nin A.\n23\nAlgorithm 4 FlashFFTConv algorithm for 4-way decomposition. We assume N = N 4\n1 for simplicity here.\nInput: Input u \u2208 RB\u00d7H\u00d7N, convolution kernel kf \u2208 CH\u00d7N, FFT matrices F \u2208 CN1\u00d7N1, F\u22121 \u2208 CN1\u00d7N1,\nTwiddle factors t \u2208 CN, tinv \u2208 C\u22ae\nN, t2 \u2208 CN, t2,inv \u2208 CN.\nOutput: Output y \u2208 RB\u00d7H\u00d7N.\nReshape u to B \u00d7 H \u00d7 N1 \u00d7 (N/N1).\nReshape kf to H \u00d7 N1 \u00d7 (N/N1).\nkf \u2190 k\u22a4\nf .\n\u25b7 Transpose last two dimensions.\nReshape kf to HN1 \u00d7 N/N1.\nu \u2190 Fu\n\u25b7 Computes the FFT over the columns of u.\nReshape u to B \u00d7 (HN1) \u00d7 (N/N1).\n\u25b7 Move N1 into H dimension.\nReshape kf to (HN1) \u00d7 (N/N1).\nCall FlashFFTConv (u, kf).\n\u25b7 Call 3-way FlashFFTConv.\nReshape u to B \u00d7 H \u00d7 N1 \u00d7 (N/N1).\ny \u2190 F\u22121u\n\u25b7 Computes the iFFT over the columns of u.\nReturn y.\nTable 10: Sparsity patterns for kf and sparsity fraction for the frequency-sparse convolution experiment in\nTable 9.\nSparsity Pattern\nS\na=0,b=0,c=0,d=0\n0\na=16,b=0,c=0,d=0\n50\na=16,b=16,c=0,d=0\n75\na=16,b=16,c=4,d=4\n79\na=16,b=16,c=8,d=8\n84\na=16,b=16,c=16,d=16\n91\n\u2022 Sparsity in the third dimension lets us reduce \u03b2. Each row of the third dimension that we remove lets\nus skip one iteration of the inner loop in step 4 above.\n\u2022 Sparsity in the fourth dimension lets us reduce \u03b1. Each row of the fourth dimension that we remove\nlets us skip one iteration of the outer loop in step 2 above.\nAs an example, we reveal the sparsity dimensions that we applied in the experiment detailed in Table 9 in\nthe main paper. Conceptually, we use the full 2-million length kernel kf, and reshape it to 32 \u00d7 32 \u00d7 32 \u00d7 64.\nLet a, b, c, and d be variables describing how much of each dimension we set to zero. Specifically, we set\nkf[a :, :, :, :] = 0, kf[:, b :, :, :] = 0, kf[:, :, c :, :] = 0, and kf[:, :, :, d :] = 0 sequentially. The formula the sparsity\nfraction S given a, b, c, d in this case is given by:\nS = 1 \u2212 (32 \u2212 a)(32 \u2212 b)(32 \u2212 c)(64 \u2212 d),\nor more generally, 1 minus the product of the fraction of each dimension that is removed. Table 10 lists the\nconfigurations of the sparsity patterns and the sparsity fractions used for the experiment in Table 9.\nA.5\nHardware Support\nFlashFFTConv was developed on A100 GPUs, and tested on A100 and H100 GPUs. Older generations of\nGPU such as V100 are not supported, since the sizes of the tensor cores are different. We look forward to\nintegrating more general libraries such as Cutlass [88] to support a wider range of GPUs, and developing\nsupport for non-GPU accelerators.\n24\nTable 11: Full results for the forward pass of a convolution with FlashFFTConv compared to PyTorch in\nmilliseconds on one H100-SXM. Batch size 64, hidden dimension 768.\nSeq Len\nPyTorch\nFlashFFTConv\nSpeedup\n256\n0.43\n0.09\n4.69\n512\n0.81\n0.15\n5.34\n1024\n1.57\n0.24\n6.61\n2048\n3.27\n0.55\n5.95\n4096\n6.65\n1.37\n4.87\n8192\n13.72\n3.19\n4.30\n16384\n28.58\n9.27\n3.09\n32768\n62.09\n21.84\n2.84\n65536\n141.15\n67.96\n2.08\n131072\n292.26\n147.26\n1.98\n262144\n582.76\n308.48\n1.89\n524288\n1,167.28\n742.26\n1.57\n1048576\n2,346.26\n1,492.84\n1.57\n2097152\n4,892.09\n2,695.51\n1.81\n4194304\n10,127.56\n7,586.96\n1.33\nB\nAdditional Results\nB.1\nFull Results for All Sequence Lengths\nWe report full results for all sequence lengths in powers of two between 256 and 4M. We report full results for\nfive cases:\n\u2022 Table 11: Standard forward pass, where the FFT size is the same as the input size. This is equivalent\nto a circular convolution.\n\u2022 Table 12: Gated forward pass, where the FFT size is the same as the input size.\n\u2022 Table 13: Forward pass, where the input size is half the FFT size. This is equivalent to a causal\nconvolution.\n\u2022 Table 14: Gated forward pass, where the input size is half the FFT size.\n\u2022 Table 15 Standard backward pass, where the FFT size is the same as the input size.\n\u2022 Table 16 Memory use for FlashFFTConv compared to PyTorch for a convolution, scaled to batch\nsize 64, hidden dimension 768.\n\u2022 Table 17 Memory use for a gated convolution using FlashFFTConv compared to PyTorch for a\nconvolution, scaled to batch size 64, hidden dimension 768.\nSpeedups vary, but generally follow the trend from the results in the body of the paper. FlashFFTConv\nachieves significant memory savings over PyTorch due to recomputation in the backward pass and kernel\nfusion. To measure memory savings, we measure the relative additional memory from calling the convolution\noperations (we do not measure the footprint of hte original inputs).\nB.2\nReference Larger Models\nTable 18 gives performance numbers for larger models trained for the same number of tokens and steps as\nthe reference PyTorch models in Table 1 in the main paper.\nThe GPT-style PyTorch models are trained for 5B tokens, with batch size 512K tokens. The BERT-style\nPyTorch models are trained for 16000 steps, with batch size 64K tokens. In contrast, the FlashFFTConv\nmodels, with higher training throughput, are trained for 15B tokens and 70000 steps in the same compute\nbudget, respectively.\n25\nTable 12: Full results for the forward pass of a gated convolution with FlashFFTConv compared to PyTorch\nin milliseconds on one H100-SXM. Batch size 64, hidden dimension 768.\nSeq Len\nPyTorch\nFlashFFTConv\nSpeedup\n256\n0.62\n0.11\n5.76\n512\n1.18\n0.19\n6.14\n1024\n2.30\n0.29\n7.81\n2048\n4.70\n0.67\n7.05\n4096\n9.49\n1.43\n6.65\n8192\n19.38\n3.58\n5.42\n16384\n39.91\n12.18\n3.28\n32768\n84.79\n26.32\n3.22\n65536\n186.69\n79.84\n2.34\n131072\n382.98\n181.51\n2.11\n262144\n764.08\n376.96\n2.03\n524288\n1,530.34\n878.93\n1.74\n1048576\n3,071.37\n1,768.94\n1.74\n2097152\n6,342.58\n4,623.46\n1.37\n4194304\n13,031.21\n10,049.42\n1.30\nTable 13: Full results for the forward pass of a convolution where the input is half the length of the convolution\nsize with FlashFFTConv compared to PyTorch in milliseconds on one H100-SXM. Batch size 64, hidden\ndimension 768.\nSeq Len\nPyTorch\nFlashFFTConv\nSpeedup\n256\n0.44\n0.09\n4.64\n512\n0.82\n0.16\n5.03\n1024\n1.57\n0.24\n6.45\n2048\n3.25\n0.53\n6.08\n4096\n6.59\n1.37\n4.83\n8192\n13.60\n3.13\n4.34\n16384\n28.37\n8.82\n3.22\n32768\n61.87\n21.34\n2.90\n65536\n141.42\n77.32\n1.83\n131072\n292.26\n151.28\n1.93\n262144\n582.82\n315.99\n1.84\n524288\n1,167.21\n757.33\n1.54\n1048576\n2,343.55\n1,525.13\n1.54\n2097152\n4,922.63\n3,321.71\n1.48\n4194304\n10,179.86\n7,305.61\n1.39\n26\nTable 14: Full results for the forward pass of a gated convolution where the input is half the length of the\nconvolution size with FlashFFTConv compared to PyTorch in milliseconds on one H100-SXM. Batch size\n64, hidden dimension 768.\nSeq Len\nPyTorch\nFlashFFTConv\nSpeedup\n256\n0.54\n0.11\n4.71\n512\n1.01\n0.19\n5.27\n1024\n1.94\n0.29\n6.75\n2048\n3.97\n0.59\n6.69\n4096\n8.01\n1.41\n5.68\n8192\n16.42\n3.46\n4.75\n16384\n34.04\n10.62\n3.21\n32768\n73.15\n25.03\n2.92\n65536\n163.75\n78.88\n2.08\n131072\n337.37\n153.13\n2.20\n262144\n672.48\n319.47\n2.10\n524288\n1,346.99\n763.97\n1.76\n1048576\n2,704.91\n1,538.89\n1.76\n2097152\n5,644.20\n3,545.79\n1.59\n4194304\n11,625.79\n8,132.32\n1.43\nTable 15: Full results for the backward pass of a convolution with FlashFFTConv compared to PyTorch in\nmilliseconds on one H100-SXM. Batch size 64, hidden dimension 768.\nSeq Len\nPyTorch\nFlashFFTConv\nSpeedup\n256\n0.76\n0.24\n3.24\n512\n1.45\n0.22\n6.43\n1024\n2.83\n0.65\n4.37\n2048\n5.76\n1.48\n3.90\n4096\n11.56\n2.86\n4.05\n8192\n23.11\n6.16\n3.75\n16384\n46.85\n18.57\n2.52\n32768\n103.85\n57.68\n1.80\n65536\n241.81\n111.76\n2.16\n131072\n489.38\n239.32\n2.04\n262144\n976.24\n519.49\n1.88\n524288\n1,960.31\n1,240.95\n1.58\n1048576\n3,938.92\n2,708.36\n1.45\n2097152\n7,909.27\n4,977.93\n1.59\n4194304\n16,552.21\n12,932.02\n1.28\n27\nTable 16: Memory usage in GB for FlashFFTConv compared to PyTorch. Scaled up to batch size 64,\nhidden dimension 768.\nSeq Len\nPyTorch\nFlashFFTConv\nMemory Reduction\n256\n0.42\n0.05\n8.21\u00d7\n512\n0.80\n0.10\n8.19\u00d7\n1024\n1.58\n0.20\n7.73\u00d7\n2048\n3.12\n0.39\n7.94\u00d7\n4096\n6.21\n0.82\n7.61\u00d7\n8192\n12.39\n1.63\n7.59\u00d7\n16384\n24.93\n3.46\n7.21\u00d7\n32768\n50.43\n7.68\n6.57\u00d7\n65536\n121.60\n46.08\n2.64\u00d7\n131072\n243.21\n92.18\n2.64\u00d7\n262144\n486.41\n184.39\n2.64\u00d7\n524288\n972.83\n368.91\n2.64\u00d7\n1048576\n1945.65\n738.34\n2.64\u00d7\n2097152\n3889.23\n1477.69\n2.63\u00d7\n4194304\n7778.45\n2961.56\n2.63\u00d7\nTable 17: Memory usage in GB for FlashFFTConv for a gated convolution compared to PyTorch. Scaled\nup to batch size 64, hidden dimension 768.\nSeq Len\nPyTorch\nFlashFFTConv\nMemory Reduction\n256\n0.66\n0.10\n6.65\u00d7\n512\n1.28\n0.19\n6.61\u00d7\n1024\n2.54\n0.40\n6.40\u00d7\n2048\n5.04\n0.78\n6.49\u00d7\n4096\n10.05\n1.58\n6.35\u00d7\n8192\n20.07\n3.17\n6.34\u00d7\n16384\n40.29\n6.53\n6.17\u00d7\n32768\n81.15\n13.83\n5.87\u00d7\n65536\n164.61\n58.37\n2.82\u00d7\n131072\n329.22\n116.75\n2.82\u00d7\n262144\n658.44\n233.54\n2.82\u00d7\n524288\n1316.89\n467.21\n2.82\u00d7\n1048576\n2633.78\n934.95\n2.82\u00d7\n2097152\n5265.48\n1870.90\n2.81\u00d7\n4194304\n10530.97\n3747.99\n2.81\u00d7\n28\nTable 18: Reference quality numbers for models when trained for the same number of steps and training data.\nModel (Metric)\nM2-BERT-base-110M (GLUE Score \u2191)\n77.6\nM2-BERT-large-260M (GLUE Score \u2191)\n81.0\nHyena-s-155M (PPL \u2193)\n13.4\nHyena-m-355M (PPL \u2193)\n11.1\nUnprocessed Pseudogene\nmiRNA\nIncRNA\nProtein Coding\nProcessed Pseudogene\nsnRNA\nMiscellaneous RNA\nTEC\nsnoRNA\nDystrophin Gene\nLongest Human Gene\nDystrophin, 2.3M Base Pairs\nFigure 5: t-SNE visualization of various genes and DNA segments using our new HyenaDNA-4M. The longest\nhuman gene, Dystrophin, is annotated.\nB.3\nDNA Embeddings\nWe use our 4M-sequence length HyenaDNA model to generate embeddings for various DNA segments\nfollowing the procedure from [82]. The DNA classes include human genes corresponding to different biological\nfunction annotations from the Ensembl genome dataset known as biotypes [20]. The longest human gene, the\ndystrophin gene, is annotated.\nC\nExperiment Details\nC.1\nCompute\nAll experiments were conducted on a box with 8xA100-40GB GPUs or a box with 8xH100-SXM GPUs.\nC.2\nFixed Compute Budget Experiment\nFor the experiment in Table 1, we train an M2-BERT-base model from scratch, and a Hyena-s-155M model\nfrom scratch.\n29\nWe train the M2-BERT-base model using masked language modeling of 30% on the C4 dataset, and fine-\ntune it on GLUE using the protocol from [42]. The FlashFFTConv model has higher training throughput,\nso it trains for more tokens; we train the FlashFFTConv model for 70,000 steps with a batch size of 64K\ntokens. The PyTorch model, with lower training throughput, only trains for 16,000 steps, with the same\nbatch size. The M2-BERT-base model we use is parameter-matched with a Transformer BERT-base. It has\n12 hidden layers, with a model dimension of 960, and an expansion factor of four. It also uses a block-diagonal\nMLP with four blocks. The M2 Hyena filter has embedding dimension 5, filter order 128, and initial sine\nactivation factor of 10. We train with learning rate 8e-4, weight decay 1e-5, and 6% warmup with a cosine\ndecay.\nWe train the Hyena-s-155M model using a causal language modeling objective on the Pile. We train the\nFlashFFTConv model for 15M tokens, and the PyTorch model for 5M tokens. The Hyena-s-155M model\nmatches the configuration from [94] and has 18 layers, with a hidden dimension of 864, and an expansion\nfactor of 4. The Hyena filter has embedding dimension 33, filter order 64, and initial sine activation factor of\n14. We train with learning rate 6e-4, with 1% warmup time and a cosine decay.\nC.3\nPath-X and Path-512 Experiments\nFor the experiment in Table 2, we use simple convolutional language models, as in [44].\nFor Path-X, we use the same model and hyperparameters as the convolutional model from [44]. We use a\nconvolutional model with 6 layers, prenorm batch norm, and hidden dimension of 256. For the convolution\nfilter parameters, we use kernel dropout 0.3, kernel learning rate 0.0005, \u03bb factor 0.001, and two channels on\nthe filter. We use an overall learning rate of 0.0005 and weight decay 0.05. We train for 500000 steps, with\n10000 steps of warmup with a cosine decay, and global batch size 16.\nFor Path-512, we scale up the resolution of Path-256. We train for 200000 steps, with 10000 steps warmup,\nlearning rate 0.0005, and weight decay 0.05. For the model, we train with 4 layers, and hidden dimension 256.\nWe use kernel dropout 0.1, kernel learning rate 0.0005, \u03bb factor 0.001, and two channels on the filter. We\nkeep the filter length to be 65536.\nC.4\nConvolution Benchmarks\nFor the experiments in Table 3, we time the forward pass of a convolution with batch size 64, hidden dimension\n768, and varying sequence length. If we run out of memory for a sequence length, we split the batch and\nhidden dimension and call the forward pass multiple times. We time each call 30 times and take the average\nof the runs. We use the same protocol for the backward pass in Table ??.\nC.5\nEnd-to-End Modeling Details\nFor the experiments in Table 5, we run forward pass of each model, and use it to compute throughput.\nBatch sizes vary by model, and we check throughput calculations with a few batch sizes to make sure the\nresult is consistent. For the M2-BERT-base model, we use a 110M model from Monarch Mixer [42]. For\nthe Hyena-s-4K model, we use an identical model to the one in Table 1, but with a filter length of 4K. For\nthe long convs Path-X model, we use the same model as in Table 2. For the SaShiMi model, we use the\nstandalone SaShiMi model from the official implementation [45], and we use 8 layers with hidden dimension\n64, and 4 up pool and down pool layers. For the HyenaDNA model, we use the official 1M-sequence length\ncheckpoint from [82]. For M2-BERT-base, Hyena-s-4K, and HyenaDNA, we additionally use a fast depthwise\nconvolution kernel for short kernels. For M2-BERT-base, Hyena-s-4K, and HyenaDNA, we report results\nbenchmarked on one H100-SXM. For the others, we report performance on one A100-40GB.\nC.6\nComparison to Transformers\nFor the comparison against Transformers in Table 6, we use the official implementations with the FlashAttention-\nv2 release [22]. We use a Hyena model, and match the number of layers, hidden dimension, and expansion\nfactor to the 2.7B Transformer model. To compute the FLOP usage, we take the formula:\n2 \u2217 num tokens \u2217 num parameters\n30\nTable 19: Measured Constants for Cost Model for A100-40GB.\nConstant\nA100-40GB\n\u03c3H\n1.35 TB/s\n\u03c3S\n9.5 TB/s\n\u03c4M\n234 TFLOPs\n\u03c4G\n17.6 TFLOPs\nfor the parametric FLOPs. For the non-parameter FLOPs, we add the raw FLOP count from our cost model\nin Equation 2 (without the adjustment for speed of tensor core FLOPs).\nC.7\nPartial Convolutions for Hyena\nFor the measurement of memory footprint reduction in Table 7, we use the same Hyena-s model as in Tables 1\nand 5, except we cut the filter short. This lets us offload parts of the input, which reduces the memory\nfootprint.\nC.8\nExtending HyenaDNA-1M\nIn Table 8, we use a sliding window approach to extend the HyenaDNA-1M and HyenaDNA-450K models to\nlonger sequences. This mimics training a 4M-sequence HyenaDNA with a short filter.\nC.9\nFrequency-Sparse Convolutions\nTo evaluate frequency-sparse convolutions, we take the pretrained HyenaDNA-1M model, and sparsify kf\nusing the strategy described in Appendix A.4. We then run standard validation using the validation set\nfrom [82].\nC.10\nEmpirical GPU Profiling\nTable 19 gives empirically-measured GPU stats for an A100-40GB, which we used to generate Figure 4. The\nstatistics are specialized to the Monarch decomposition workload. To measure the achievable tensor core\nFLOPs, we measured the utilization of real fp16 matrix multiply. To measure achievable general arithmetic\nFLOPs, we measured the utilization of continuously applying Twiddle factors. To measure the achievable\nHBM bandwidth, we measured the speed of torch.clone of a tensor. To measure the achievable SRAM\nbandwidth, we measured the slow down from writing intermediate results to SRAM between matrix multiply\ninstructions.\n31\n"
  },
  {
    "title": "ADaPT: As-Needed Decomposition and Planning with Language Models",
    "link": "https://arxiv.org/pdf/2311.05772.pdf",
    "upvote": "8",
    "text": "ADAPT: As-Needed Decomposition and Planning with Language Models\nArchiki Prasad\u2663\nAlexander Koller\u2661\nMareike Hartmann\u2661\nPeter Clark\u2660\nAshish Sabharwal\u2660\nMohit Bansal\u2663\nTushar Khot\u2660\n\u2663 UNC Chapel Hill\n\u2660 Allen Institute for AI\n\u2661 Saarland University\nAbstract\nLarge Language Models (LLMs) are increas-\ningly being used for interactive decision-\nmaking tasks requiring planning and adapt-\ning to the environment.\nRecent works em-\nploy LLMs-as-agents in broadly two ways:\niteratively determining the next action (itera-\ntive executors) or generating plans and execut-\ning sub-tasks using LLMs (plan-and-execute).\nHowever, these methods struggle with task\ncomplexity, as the inability to execute any\nsub-task may lead to task failure.\nTo ad-\ndress these shortcomings, we introduce As-\nNeeded Decomposition and Planning for com-\nplex Tasks (ADAPT), an approach that explic-\nitly plans and decomposes complex sub-tasks\nas-needed, i.e., when the LLM is unable to ex-\necute them. ADAPT recursively decomposes\nsub-tasks to adapt to both task complexity and\nLLM capability. Our results demonstrate that\nADAPT substantially outperforms established\nstrong baselines, achieving success rates up to\n28.3% higher in ALFWorld, 27% in WebShop,\nand 33% in TextCraft \u2013 a novel compositional\ndataset that we introduce. Through extensive\nanalysis, we illustrate the importance of multi-\nlevel decomposition and establish that ADAPT\ndynamically adjusts to the capabilities of the\nexecutor LLM as well as to task complexity.1\n1\nIntroduction\nRecent advances in Large Language Models\n(LLMs) have expanded their application beyond\nconventional NLP tasks to more complex tasks\ninvolving mathematical, symbolic, and common-\nsense reasoning (Wei et al., 2022; Huang and\nChang, 2023). Recent models have even been ap-\nplied to decision-making tasks, such as perform-\ning household chores (in a simulated environment),\nnavigating a webpage, etc., that require interac-\ntions with external environments or tools (Yao et al.,\n2023b; Qin et al., 2023).\n1Project: https://allenai.github.io/adaptllm\nPrior works on using LLMs for decision-making,\nsuch as ReAct (Yao et al., 2023b), iteratively gen-\nerate the next action to be executed in the environ-\nment given the history of actions and observations\n(see Fig. 1; top-left). However, as the tasks become\nmore complex, LLMs struggle due to their limited\ncomposition ability (Dziri et al., 2023) and inability\nto deal with the distractors (Shi et al., 2023) in a\nlong action-observation trajectory.\nTo mitigate this, modular approaches (Khot et al.,\n2023; Yang et al., 2023; Sun et al., 2023) incorpo-\nrate a separate planner module that utilizes an LLM\nto create a high-level plan.2 The planner then dele-\ngates simpler sub-tasks to an executor LLM module\nthereby reducing the compositional complexity and\nlength of action trajectory required by the execu-\ntor. We refer to this category broadly as plan-and-\nexecute approaches (see Fig. 1; top-right). While\nthe plans enable these methods to guide the execu-\ntion and track progress (Wang et al., 2023b), their\nnon-adaptive nature poses a limitation when con-\nfronting unachievable sub-tasks. These approaches\ninherently lack the flexibility to adapt to task com-\nplexity and manage execution failures, as shown\nin Fig. 1(top-right), where just one sub-task that is\ntoo complex results in overall task failure.\nTo address such failures, we propose As-Needed\nDecomposition and Planning for complex Tasks\n(ADAPT), a recursive algorithm that further de-\ncomposes sub-tasks when necessary, to dynami-\ncally accommodate to task complexity. We uti-\nlize separate planner and executor LLM modules\nwithin our framework but only decompose a task\nusing the planner, if the executor LLM detects a\nfailure. As shown in Fig. 1, the overall task of\nputting a clean mug on a desk in an unfamiliar\n2By \u201cplanning\u201d, we refer to the colloquial concept of\ndesigning a list of sub-tasks to accomplish a complex task\nrather than its usage in classical AI-planning literature. E.g.,\na \u201cplan\u201d for preparing a lasagna could be to cook the pasta,\nprepare the sauce, layer the ingredients, and then bake it.\narXiv:2311.05772v1  [cs.AI]  8 Nov 2023\n> Go to countertop 1.\nYou reached loc 1...\n> Go to cabinet 12.\n...\n> Think: Mug not found.\nTask failed!\nYou reached loc 20 ...\nIterative Executor (ReAct)\nPlan:\nStep 1: Find and\ntake the mug AND\nStep 2: Clean the\nmug AND\nStep 3: Put the\nclean mug on desk\nPlan-and-Execute\n   ADaPT (Recursive Decomposition, As-needed)\nTask: Put a clean mug on desk.\nNot Executed\nExecute: Task\nPlan: \nStep 1: Find and take the mug AND\nStep 2: Clean the mug AND\nStep 3: Put the clean mug on desk\nExecute: Step 1\nPlan: \nStep 1a: Find and take the mug from countertops OR\nStep 1b: Find and take the mug from cabinets OR\nExecute: Step 1a\nExecute: Step 1b\nExecute: Step 3\nExecute: Step 2\n...\nOn execution failure,\ndecompose further\nSuccessful sub-task allows\nexecution to resume\nExecute:\nStep 1\nExecute:\nStep 2\nExecute:\nStep 3\nOR\nADaPT(Task)\nADaPT(Step1)\nFigure 1: Top-Left: Iterative executors such as Re-\nAct (Yao et al., 2023b) interact directly with the envi-\nronment, performing planning implicitly. Top-Right:\nPlan-and-Execute, e.g., Yang et al. (2023), creates a\nfixed plan for the task, without accounting for complex-\nity in executing step 1. Bottom: ADAPT dynamically\ndecomposes based on success of the executor.\nhousehold is too complex for the model, leading to\nfailure of the iterative executor. While a plan-and-\nexecute-style approach initially breaks down the\ntask into three sub-tasks, it falls short in accounting\nfor the complexity in finding a mug. Moreover, it is\nchallenging to anticipate the difficulty of such a sub-\ntask in advance, as the executor could find a mug\nin the first attempt or in an obscure location. There-\nfore, ADAPT employs its recursive structure to\ndynamically adapt to execution failures (assessed\nby LLMs), by further decomposing the complex\nsub-task of finding a mug via the planner.\nEmpirically, we demonstrate the effectiveness\nof ADAPT on three datasets involving interactive\nenvironments: ALFWorld (Shridhar et al., 2021),\nWebShop (Yao et al., 2022), and a new compo-\nsitional text game for crafting Minecraft recipes\ncalled TextCraft (Sec. 4.1). Using GPT-3.5 as the\nunderlying LLM, ADAPT outperforms strong base-\nlines (discussed in Sec. 4.2) such as ReAct (Yao\net al., 2023b), and Plan-and-Solve (Wang et al.,\n2023b) by up to 28.3%, 27%, and 33% absolute\npoints on ALFWorld, WebShop, and TextCraft re-\nspectively (Sec. 5). Compared to Reflexion (Shinn\net al., 2023), an adaptive approach that addresses\nfailures in the full task trajectory, ADAPT yields\nhigher success rates by 14.1% and 9% on ALF-\nWorld and WebShop, respectively. Through exten-\nsive analysis of ADAPT, we establish the impor-\ntance of recursive decomposition (Sec. 6.1), show-\ncasing its dynamic adaptation to the capabilities of\nthe executor LLM (Sec. 6.2), which enhances per-\nformance for both weaker and stronger executors.\nLastly, we demonstrate that ADAPT adapts to task\ncomplexity (in Sec. 6.3), where the extent of its\nrecursive decomposition aligns with the inherent\ntask complexity.\nTo summarize, our contributions include:\n1. We present ADAPT, a recursive algorithm that\ndynamically decomposes complex sub-tasks on\nan as-needed basis, i.e., intervening only if the\ntask is too complex for the executor.\n2. On three diverse datasets, ALFWorld, WebShop,\nand TextCraft, ADAPT improves success rate\nof GPT-3.5 over previous approaches by up to\n28.3%, 27%, and 33% points respectively.\n3. Analysis of ADAPT underscores the signifi-\ncance of recursive decomposition and the ability\nto adapt dynamically to varying LLM execution\ncapabilities and task complexities.\n2\nRelated Work\nLLMs for Decision-Making.\nLLMs have been\nsuccessfully used as agents to perform a wide vari-\nety of decision-making tasks such as robotic nav-\nigation (Ahn et al., 2022; Huang et al., 2023b;\nSingh et al., 2023), complex multi-modal games\nlike Minecraft (Fan et al., 2022; Wang et al., 2023a),\ntext-based environments (Shridhar et al., 2021; Liu\net al., 2023). While most of these works focus\non learning from trajectories, ReAct (Yao et al.,\n2023b) uses few-shot prompting to build an agent\nthat reasons about the current state (thoughts) and\ngenerates the next action in the environment, given\nprior actions and observations. Their iterative ap-\nproach (shown in Fig. 1; top-left) can handle fail-\nures, but they have to keep track of the entire plan\nimplicitly while deciding every local action (see\ncontrast to ADAPT in Fig. 7). By incorporating\nplanning and execution into separate modules and\nenabling dynamic adaptation we are able to achieve\nhigher success rates (refer to Sec. 5).\nSeveral follow-up works improve upon the Re-\nAct framework by incorporating feedback in future\ntrials (Madaan et al., 2023; Shinn et al., 2023), or\nusing LLMs to develop heuristics for search (Yao\net al., 2023a; Zhou et al., 2023). In contrast to\nADAPT, they do not employ task decomposition,\nleading to unnecessary computation as they explore\nmultiple trajectories or trials for the whole task,\neven though the LLM struggles with just one sub-\ntask. Such works are complementary to ADAPT\nas they can be incorporated within the planner or\nexecutor modules to strengthen LLM performance\n(just like they augment ReAct).\nDecomposition and Modularity.\nOur work fol-\nlows extensive literature in NLP on decomposing\ntasks into neural modules (Andreas et al., 2016;\nGupta et al., 2019; Jiang and Bansal, 2019) or\nseq2seq models (Min et al., 2019; Talmor and Be-\nrant, 2018; Khot et al., 2021; Perez et al., 2020;\nSaha et al., 2023b). With the advent of few-shot\nprompted black-box LLMs, this paradigm of pro-\ngrammatic decomposition into LLMs has become\nmore popular (Yao et al., 2023b; Khot et al., 2023;\nWang et al., 2023b, inter alia), referred to as LLM\nPrograms (Schlag et al., 2023; Dohan et al., 2022).\nADAPT also decomposes complex tasks (us-\ning the planner module) and assigns them to the\nexecutor module. However, ADAPT automati-\ncally adapts to complex sub-tasks and executor\nfailures by decomposing them further as-needed to\nalign with the executor\u2019s capabilities. Therefore,\nADAPT\u2019s dynamic nature distinguishes us from\nmost prior works with a non-adaptive structure.\nSpecifically, ADAPT can be viewed as a general-\nization of recursive and hierarchical decomposition\nin DecomP (Khot et al., 2023). While DecomP was\nlimited to exploring basic algorithmic tasks without\nthe need for interactions with environments, com-\nmunication between modules, or handling execu-\ntion failures, ADAPT excels in real-world textual\nenvironments, such as online shopping, and can\nhandle all these complexities.\nHierarchical Problem Solving.\nIn AI problem-\nsolving, there is a longstanding tradition of hi-\nerarchical task decomposition employed in plan-\nning (Ghallab et al., 2004; Georgievski and Aiello,\n2014; H\u00f6ller et al., 2020), reinforcement learning\n(Sutton et al., 1999; Barto and Mahadevan, 2003;\nNachum et al., 2018; Zhang et al., 2021), and navi-\ngation (She et al., 2014; Sharma et al., 2022; Blukis\net al., 2022; Min et al., 2022; Song et al., 2023).\nThese approaches, such as Hierarchical Task Net-\nworks (Erol et al., 1994), leverage domain knowl-\nedge, e.g., hand-specified library of plans, to break\ncomplex problems into simpler tasks. Our work\nembraces this tradition but distinguishes itself by\ninvestigating how LLMs can autonomously decom-\npose complex tasks by leveraging their extensive\nworld knowledge, without the need for predefined\nplan libraries. ADAPT also performs dynamic\nhierarchical planning by employing its recursive\nstructure to adapt to the capabilities of the executor\nLLM and task complexity.\n3\nMethodology\nWe introduce As-Needed Decomposition and\nPlanning for complex Tasks (ADAPT), a mod-\nular approach for decision-making that integrates\nan LLM as an executor and a planner (Secs. 3.1\nand 3.2) within an LLM program called the con-\ntroller (Sec. 3.3). In Fig. 1, when ADAPT is given\na complex task, it first attempts to accomplish the\nentire task by running the executor iteratively, and\nresorting to the LLM planner for further decom-\nposition into sub-tasks if the executor fails. Sub-\nsequently, ADAPT is recursively called for each\nsub-task to ensure their successful completion, ulti-\nmately leading to overall task success.\n3.1\nLLM as an Executor\nOverview.\nIn a given environment, the executor\nis provided with a concise natural language task\nspecification, as shown in Fig. 2 (left). Following\nYao et al. (2023b), the executor iteratively interacts\nwith the environment via actions generated by the\nLLM. This interaction continues until the task is\neither completed or a preset maximum iteration\nlimit is reached. Consistent with Ahn et al. (2022),\nwe provide the LLM with in-context demonstra-\ntions of low-level \u201catomic\u201d skills specific to the\nenvironment (listed in Table 1), such as knowing\nhow to correctly heat objects in ALFWorld. This\napproach offers two advantages: (i) it allows us\nto employ the same executor with environment-\nspecific knowledge for all baselines (Sec. 4.2); and\n(ii) it enables the planner (discussed in Sec. 3.2) to\nwork at a higher level of abstraction, leveraging the\nLLM\u2019s general world knowledge.\nExecution Capabilities of an LLM.\nAt a min-\nimum, the LLM executor should reliably execute\natomic skills. While we provide demonstrations for\nsuccessful execution of atomic skills, LLMs can\n[LLM] Think:  Input assumption: I am carrying a\nmug. Now I need to verify this.\n[LLM]> inventory\n[LLM]> go to sinkbasin 1\n[LLM]> clean mug 1 with sinkbasin 1\n[LLM] Think: I cleaned the mug.Task completed!\nADaPT(Task, k)\nExecutor(Task)\nSuccess?\nPlanner(Task)\nStep 2\nStep 1\nStep 3\nADaPT(Step1,k+1)\nADaPT(Step2,k+1)\nADaPT(Step3,k+1)\nTask: Put a clean mug on desk\nTrue\nFalse\nLogic\nTrue\nFalse\nClean the mug\nExecutor(           )\nOK.\nYou are carrying: a mug 1\nYou reached loc 13, you see ...\nYou clean mug 1\n# Think: To do this task, I first need to find a\nmug, then clean, it and put it on the desk. I\nneed to perform these tasks sequentially.\nStep 1: Find and take a mug AND\n# Think: Now I found a mug, I will clean it.\nStep 2: Clean the mug with sinkbasin AND\n# Think: Now I cleaned the mug, I will put\nthe clean mug on the desk.\nStep 3: Put clean mug on desk\n LLM\nPlanner(           )\nPut a clean\nmug on desk\nController\nAND\nFigure 2: Block diagram of the ADAPT pipeline with an example from ALFWorld. Left: Use of LLM as an\nexecutor to interact iteratively with the environment along with an example execution trajectory. Middle: Overall\nrecursive algorithm (depth k \u2264 dmax) that embeds the executor and planner, refer to Algorithm 1 for details. Right:\nOutline of using LLM as a planner to generate sub-tasks (steps) and logical operators combining them.\nadapt to failures by combining multiple skills to\nperform complex tasks, as discussed in Sec. 6.2.\nFor instance, in Fig. 2 (left), we show the LLM suc-\ncessfully cleaning a mug it\u2019s carrying (an atomic\nskill). An advanced executor could combine \u201cfind-\ning a mug\u201d with the \u201ccleaning\u201d skill to accomplish\n\u201cfind a clean mug\u201d without explicit planning.\nSelf-generated Success Heuristic.\nIn order to\ndecompose based on the abilities of the executor,\nwe need to determine whether the executor is capa-\nble of finishing the given (sub-)task independently\nor if further decomposition is required. To this\nend, we employ the executor LLM to determine\nthe completion of the (sub-)task without relying\non the environment for obtaining gold rewards for\n(sub-)tasks. We include a simple instruction in the\nexecutor prompt to output \u201ctask completed\u201d if it de-\ntermines it has succeeded, otherwise output \u201ctask\nfailed\u201d in case it cannot proceed. Refer to example\nin Fig. 2 (left). Our success heuristic aligns with bi-\nnary classification models like those in Shinn et al.\n(2023), providing a way to simulate intermediate\nrewards, which complements end-of-task environ-\nment rewards (Rengarajan et al., 2022). We further\nstudy this LLM-generated heuristic in Appendix F.\n3.2\nLLM as a Planner\nOverview.\nThe objective of the planner is to\nbreak down complex tasks into smaller sub-tasks.\nTo achieve this, we instruct the LLM to generate\na concise yet comprehensive plan consisting of a\nfew steps, typically 3-5, as shown in Fig. 2 (right).\nWe opt for shorter, more abstract plans because ex-\npecting a detailed, fine-grained plan upfront can be\nimpractical, especially in unexplored environments.\nE.g., devising a 10-step plan to put a clean mug on\na desk in an unfamiliar room, without prior knowl-\nedge of the mug\u2019s location, can lead to cascading\nerrors due to incorrect assumptions. Therefore, we\ntask the LLM with generating short plans, given\nthe flexibility to decompose further in subsequent\niterations, based on the executor\u2019s capabilities. For\na comparison with plan-and-execute approaches\nthat need detailed plans, refer to Sec. 5.\nComposition Logic for Sub-tasks.\nAlong with\nthe sub-tasks, we prompt the planner to generate\nlogical operators to combine various sub-tasks in\nthe plan to accomplish the task. We allow for two\nlogical operators: \u201cAND\u201d and \u201cOR\u201d. Sub-tasks\nare linked using AND when they must be executed\nsequentially for the task to succeed. However, in\ncases requiring exploration, such as finding an item\nin an unknown room, we employ the OR operator\nto simulate conditional checks. Here, the task suc-\nceeds if any of the sub-tasks are successful. For\ninstance, in Fig. 1, the plan to \u201cfind a mug\u201d would\nbe to \u201cfind a mug on the countertop\u201d OR \u201cfind a\nmug in the cabinet\u201d. We execute the latter only\nif the agent has not found the mug yet. While ex-\namples in Figs. 1 and 2 show homogeneous logic,\nADAPT can handle complex logical expressions\nas described in Appendix B.\n3.3\nController \u2013 LLM Program\nOverall Pipeline.\nThus far, we describe two\nLLM-based modules that can perform the roles\nof low-level execution and high-level planning. We\nincorporate these modules into ADAPT via the\ncontroller which is a pre-determined and recursive\nalgorithm \u2013 making the overall pipeline of ADAPT\nAtomic Skill\nDescription\nALFWorld\nput\nAssuming that the robot is carrying\nan object, put it on a given receptacle.\ntake\nTake a specified object from a speci-\nfied receptacle.\nclean/heat/cool Assuming that the robot is carrying\nan object, clean/heat/cool the object.\nexamine\nAssuming the robot is at a desk with a\ndesk lamp, use it to look at an object.\nWebShop\nsearch\nPut a given query in the search box,\nresults in a page with list of products.\nshortlist\nBased on the search page and query,\nget list of any matching products.\nmatch\nGiven a product ID and query, navi-\ngate to the product page and verify it\nmatches the query.\nbuy\nGiven a product ID and query, buy\nproduct by selecting relevant options.\nTextCraft\ncraft\nAssuming the agent has all the ingre-\ndients in the inventory, craft a target\nobject by picking an appropriate com-\nmand from the list of crafting recipes.\nfetch\nLook for a given object in the inven-\ntory or get it directly from the game.\ninventory\nLook-up the game inventory.\nTable 1: Overview of atomic skills used in Sec. 3.1.\nan LLM program (Schlag et al., 2023; Dohan et al.,\n2022), shown in Algorithm 1. The overall flow\nof the controller program is as follows: (i) given\nan input task, the controller calls the executor to\ncheck if it can succeed in performing the task di-\nrectly; (ii) if the executor does not succeed, the\ncontroller delegates decomposing the complex task\nto the planner and recursively calls ADAPT for\neach sub-task until we hit a termination criterion.\nFig. 2 (mid) shows the control flow of ADAPT.\nA complex task such as \u201cput a clean mug on the\ndesk\u201d is first assigned to the executor. If the execu-\ntor does not succeed, then ADAPT calls the planner\nto decompose the task into sub-tasks along with a\nlogical operator (AND or OR) indicating how to\ncompose them. Each sub-task (referred to as \u2018step\u2019\nin Fig. 2) is then assigned recursively to ADAPT\nand is combined using the logical operator. In the\nend, the success of sub-tasks after recursive decom-\nposition ensures overall task success (unrolled calls\nto planner and executor are shown in Fig. 1).\nRoles played by Controller.\nThe controller per-\nforms two crucial roles in the overall functioning\nof ADAPT. First, it serves as the communication\nbridge between planner and executor, propagating\nsalient information across the two depending on\nthe task. Second, since ADAPT is a recursive al-\ngorithm, the controller determines the termination\nAlgorithm 1 Algorithm for ADAPT\n1: function ADAPT(Task T, Current depth k)\n2:\n// ADAPT(\u00b7) Generates success heuristic value\ncompleted for the task T. Initialized with k = 1.\n3:\n// Base case: terminate on reaching maximum depth\n4:\nif k > dmax then return False\n5:\n// Execute the task/sub-task to assess if the LLM can\ndirectly perform it using LLM-generated success.\n6:\ncompleted \u2190 executorLLM(T)\n7:\n// Plan only when the executor fails.\n8:\nif completed is False then\n9:\n// Using the LLM, decompose the task into a set\nof sub-tasks, P, and a Boolean function, logic(\u00b7),\nthat combines output of the sub-tasks.\n10:\nP, logic \u2190 plannerLLM(T)\n11:\n// Get the outputs for individual sub tasks\n12:\nO = {ADAPT(Tsub, k+1)|Tsub \u2208 P}\n13:\n// Combine the outputs of the sub tasks\n14:\ncompleted \u2190 logic(O)\n15:\nreturn completed\ncriterion using the logical expression from the plan-\nner and success heuristic from the executor or if a\nmaximum depth dmax (\u22651) is reached.\n4\nExperimental Setup\nWe first describe the datasets we use for experimen-\ntation (Sec. 4.1) and the baselines that we compare\nADAPT with (Sec. 4.2).\n4.1\nDatasets\nWe employ LLMs-as-agents to perform tasks in the\nfollowing three environments and use task success\nrate as our evaluation metric in Secs. 5 and 6.\nALFWorld.\nALFWorld (Shridhar et al., 2021)\nis a text-based game version of the embodied AL-\nFRED benchmark (Shridhar et al., 2020) imple-\nmented in the TextWorld environment (C\u00f4t\u00e9 et al.,\n2019). It encompasses 6 distinct task types, where\nan agent is required to accomplish high-level tasks\nthrough navigation and interaction via text-based\nactions in a simulated household that gives textual\nfeedback to an agent (e.g., put a clean mug on desk\ndiscussed earlier in Fig. 2). Following Shridhar\net al. (2021), we present results on 134 unseen eval-\nuation games (test set) with a separate dev set of\n10 games per task from the seen evaluation games\nsplit. Following Yao et al. (2023b), along with\natomic skills, we add example gold trajectories for\ntwo tasks: heat and look in the executor prompt.3\n3Unlike Yao et al. (2023b), we use a standardized executor\nprompt for all ALFWorld tasks, avoiding the agent to know the\ntask-type apriori. Table 8 in Appendix C further demonstrates\nthat ADAPT still improves over task-specific executors.\nWebShop.\nWebShop (Yao et al., 2022) is an on-\nline shopping website environment featuring 1.18\nmillion real-world products containing 500 user\nqueries in the test set.\nIt serves as a complex\ndecision-making environment with practical appli-\ncations wherein an agent must navigate a website\nthrough a variety of commands to purchase an item\nmatching a user specification (e.g., grey sectional\nsofa priced less than $300 with fast delivery). Fol-\nlowing Shinn et al. (2023), we report performance\non 100 user instructions and use a different subset\nof 40 queries as the dev set.\nTextCraft.\nWe create a new text-only environ-\nment for crafting Minecraft4 items similar to Word-\nCraft (Coenen et al., 2021). Unlike existing agent-\nbased environments, tasks in TextCraft exhibit a\nnatural compositional structure, resembling cook-\ning recipes with steps of varying complexity, where\nsome sub-tasks are more intricate, such as layering\na lasagna, while others are simpler, like baking it.\nTextCraft: Environment Details.\nIn TextCraft,\nthe objective is to obtain target Minecraft items\nby crafting them from available items in the envi-\nronment. We define an environment with three ac-\ntions: craft <item> using <ingredients>, get\n<item>, and inventory. We utilize Minecraft\u2019s\ncrafting recipes to specify craftable items and their\ningredients, assuming that all other items are ob-\ntainable from the environment. Similar to Alf-\nWorld, our agent can directly execute these op-\nerations in the embodied game. The game begins\nwith a list of crafting commands provided to the\nagent that detail recipes that can be used to craft\nthe final target, its ingredients along with some dis-\ntractors (details in Appendix E). A reward of 1 is\ngenerated when the target item gets added to the\nagent\u2019s inventory. An illustrative gold trajectory\nfrom TextCraft is shown in Fig. 3.\nTasks in TextCraft are inherently decomposable.\nIn Fig. 3, crafting a beehive necessitates crafting\nits ingredients, like planks and honeycomb, which\nmay require further decomposition. The agent thus\nneeds to identify and adapt to varying task com-\nplexity, e.g., crafting a plank is easier than crafting\na beehive. Moreover, some recipes allow using any\nitem from a particular category. For instance, craft-\ning a beehive uses planks (a category), requiring\nthe agent to use linguistic knowledge for proper\nitem selection (e.g., select oak planks, a specific\n4https://www.minecraft.net\nCrafting Commands:\ncraft 4 oak planks using 1 oak log\ncraft 1 honeycomb block using 4 honeycomb\ncraft 1 beehive using 6 planks, 3 honeycomb\nGoal: craft beehive\n[Action] get 1 oak log\n[Env] Got 1 oak log\n[Action] craft 4 oak planks using 1 oak log\n[Env] Crafted 4 minecraft:dark_oak_planks\n[Action] get 1 oak log\n[Env] Got 1 oak log\n[Action] craft 4 oak planks using 1 oak log\n[Env] Crafted 4 minecraft:dark_oak_planks\n[Action] craft 1 beehive using 6 oak planks, 3 honeycomb \n[Env] Crafted 1 minecraft:beehive\n...\n1x\n1x\n4x\n4x\n3x\n1x\nFigure 3: Example gold trajectory in TextCraft for a\ntask with recipe depth of 2.\nitem in the category planks).\nWe evaluate our approach on a test set of 200\ntasks where the target items have recipe trees of\ndepth 2, 3, and 4 (example tree of depth 2 is shown\nin Fig. 3). We use all the items with recipe tree\ndepth of 3 (123 tasks) and 4 (11 tasks) and 77 (out\nof 297) tasks with depth of 2 in our test set, and the\nrest constitute the dev set.\n4.2\nBaseline Approaches\nWe compare ADAPT with four classes of baseline\napproaches described below.\nIterative Executor-Only (ReAct).\nIn this setting,\nwe employ the executor to interact iteratively with\nthe environment, adopting the think-act-observe\nprompting style from ReAct (Yao et al., 2023b). All\nmethods discussed below, including ADAPT, share\nthe same executor, ensuring a standardized impact\nof the executor\u2019s strength and design choices when\ncomparing relative performance in Sec. 5. Specif-\nically, when dmax = 1, ADAPT solely relies on\nthis executor. As all other baselines build upon\nthis executor, we increase the maximum allowed\ninteractions for ReAct by a factor of dmax when\npresenting results in Tables 2 to 4.\nPlan-and-Execute.\nAs shown in Fig. 1, in this\nsetting, we generate a plan first and then assign\neach sub-task to the executor. This approach only\nplans once and as a result has a non-adaptive struc-\nture (consistent with Wang et al. (2023b); Yang\net al. (2023); Sun et al. (2023)). To ensure each\nMethod (dmax = 3)\nPick Clean Heat Cool Look Pick2\nAll\nReAct\n33.3\n67.7\n43.5\n33.3\n55.6\n11.8\n43.3\nPlan-and-Execute\n29.2\n61.3\n47.8\n38.1\n61.1\n11.8\n43.3\nTry Again with ReAct 50.0\n51.6\n60.8\n47.6\n61.1\n5.9\n47.8\nReflexion\n70.8\n61.3\n61.0\n66.7\n61.1\n5.9\n57.5\nADAPT (Ours)\n87.5\n80.6\n60.8\n76.2\n61.1\n52.9\n71.6\nTable 2: ADAPT yields the highest the overall success rates (%)\ncompared to baselines from prior work (discussed in Sec. 4.2) on\nALFWorld (test split). Best (highest) success rates are highlighted in\nbold and second-highest rates are underlined.\nMethod (dmax = 3)\nSuccess Rate\nReAct\n32.0\nPlan-and-Execute\n17.0\nTry Again with ReAct\n30.0\nReflexion*\n35.0\nLATS (Zhou et al., 2023)*\n38.0\nADAPT (Ours)\n44.0\nTable 3: ADAPT yields the highest success\nrate on WebShop (test split). *We use per-\nformance reported by Zhou et al. (2023)\nplan step is executable without further decompo-\nsition, we design new prompts with more detailed\nplans. We use this baseline to evaluate the impor-\ntance of dynamic decomposition in ADAPT (refer\nto Appendix B). Note that ADAPT with dmax =2\ndiffers from plan-and-execute as it is adaptive, i.e.,\ndecomposes only when executor fails and generates\nrelatively shorter plans.\nTry Again with ReAct.\nBy design, ADAPT\nmakes multiple calls to the executor module, albeit\nwith different (sub-)tasks. Like Yang et al. (2023),\nwe design a simple controller that requests the ex-\necutor to retry the task in a total of dmax separate\ntrials and then uses the trial with the best perfor-\nmance for each task instance. This setting uses\ncomparable number of executor calls as ADAPT.\nReflexion.\nShinn et al. (2023) execute the en-\ntire task first, and if unsuccessful, reflect and store\nfeedback in memory for subsequent dmax\u22121 trials.\nWhile adaptive, this approach repeats the entire\ntrial even if a single sub-task fails, redundantly\nre-executing previously successful sub-tasks.\nADAPT and Shared Implementation Details.\nFollowing (Yao et al., 2023b; Shinn et al., 2023;\nZhou et al., 2023), by default, we use the GPT-\n3.5 (Ouyang et al., 2022) LLM for both planning\nand execution in ADAPT and other baselines. We\nuse the completion-based models for ALFWorld\nand TextCraft and the chat-based model for Web-\nShop.5 Further, we use ADAPT (and other base-\nlines) with dmax =3 for ALFWorld, and WebShop\nand increase to dmax = 4 for TextCraft to accom-\nmodate recipes with a depth of 4 (Sec. 4.1). Addi-\ntionally, ADAPT uses ReAct as the underlying ex-\necutor. For additional details, refer to Appendix A.\n5We use the completion model as chat variants of GPT-3.5\nconsistently underperform their completion counterparts (Liu\net al., 2023; Yang et al., 2023). We discuss the use of different\nLLMs in the planner in Sec. 6.3.\n5\nMain Results\nUsing GPT-3.5 as the underlying LLM, in this sec-\ntion, we show that ADAPT yields the highest suc-\ncess rate compared to baselines from prior work on\nALFWorld, WebShop, and TextCraft datasets.\nALFWorld.\nIn Table 2, we observe that ADAPT\nachieves the highest overall success rate, while\nusing ReAct alone results in the lowest overall\nperformance. By leveraging adaptive decomposi-\ntion, ADAPT improves over ReAct\u2019s performance\nby 28.3% points (absolute) as well as over Plan-\nand-Execute and Try Again by 28.3% and 23.8%\npoints, respectively. Lastly, we find that ADAPT\nyields 14.1% points higher overall success rate\nthan Reflexion, despite the latter having access\nto dedicated memory and natural language feed-\nback. Specifically, we find baselines yield poor\nresults on \u2018pick2\u2019 tasks (<12% success rate) as\nthey require the agent to compose two \u2018pick\u2019-style\ntasks involving a longer action history. However,\nADAPT yields significant improvements (by over\na factor of 4\u00d7) for this type of tasks.\nWebShop.\nTable 3 shows a similar trend with\nADAPT surpassing all baselines and achieving\nthe highest success rate. ADAPT outperforms Re-\nAct, Plan-and-Execute, and Try-Again baselines\nby up to 27% points. We corroborate the findings\nof Shinn et al. (2023) and observe that natural lan-\nguage feedback offers limited gains in performance,\nas compared to ADAPT (which surpasses Reflex-\nion by 9% points). Additionally, we compare with\na recent search-based baseline LATS (Zhou et al.,\n2023) and find that ADAPT outperforms the suc-\ncess rate of LATS by 6% points. Note that we\nfocus on success rate and not the (soft) score as our\nmetric. This is because it is possible to obtain a\nnon-zero score by naively buying a product, as il-\nlustrated in Appendix D. Success rate, on the other\nhand, is robust to such naive baselines.\nMethod (dmax = 4)\nSuccess Rate\nReAct\n19.0\nPlan-and-Execute\n27.0\nTry Again with ReAct\n15.0\nADAPT (Ours)\n52.0\nTable 4: ADAPT achieves the highest success rate (%)\ncompared to other baselines on TextCraft (test split).\nTextCraft.\nOur results on TextCraft are summa-\nrized in Table 4. First, we observe that ADAPT\nachieves an improvement of 33% compared to the\nReAct executor. In contrast to Plan-and-Execute,\ni.e., starting with a fixed plan, having the dynamic\nability to adapt to complex sub-tasks (in this case,\ncrafting complex ingredients) in ADAPT improves\nperformance by 25% points. On the other hand, we\nfind that retrying task execution does not outper-\nform ReAct when it is allowed longer interactions\nwith the environment. This suggests that decompo-\nsition is needed to effectively address the executor\u2019s\nlimitations in crafting multiple items by adaptive\nand as-needed planning to achieve the goal.\n6\nAnalysis and Discussion\nWe analyze ADAPT in detail by addressing the\nfollowing research questions on dev data splits.\n6.1\nHow does performance of ADAPT scale\nwith the depth of decomposition?\nSetup.\nTo assess the impact of adaptive decom-\nposition, we study ADAPT under three settings\nwith increasing maximum depth dmax \u2208 {1, 2, 3}\nfor ALFWorld, WebShop, and TextCraft. Note\nthat dmax = 1 setting corresponds to the iterative\nexecutor-only baseline (ReAct).\n1\n2\n3\nValue of dmax in ADaPT\n10\n20\n30\n40\n50\n60\n70\nSuccess Rate\nALFWorld\nWebShop\nTextCraft\nFigure 4: Success rate of ADAPT increases with the\nmaximum depth dmax for all datasets (dev splits).\nResults.\nFig. 4 shows that across all datasets, per-\nformance of ADAPT scales with increasing the\nmaximum depth dmax. Consistently, we find a sig-\nnificant improvement in success rates as we move\nfrom dmax =1 to dmax =2, i.e., adding the planner\nto decompose a complex task when executor fails\nproves to be effective. Finally, the performance\nincrease from dmax = 2 to dmax = 3 validates our\nhypothesis that some sub-tasks are difficult for the\nLLM to directly execute successfully, and decom-\nposing these further boosts overall performance.\n6.2\nDoes ADAPT cater to different execution\ncapabilities of LLMs?\nA major advantage of ADAPT\u2019s as-needed decom-\nposition is the ability to adapt to different strengths\nor capabilities of the executor LLM, making our ap-\nproach widely-applicable. We illustrate this ability\nwith two targeted experiments.\nSame LLM, different execution capabilities.\nWe run ADAPT on three different executor\nprompts on ALFWorld: (i) task-specific gold tra-\njectories, (ii) atomic skills and common gold-\ntrajectories for 2 tasks (hybrid),3 and (iii) only\natomic skills. Intuitively, for a given LLM, task-\nspecific gold trajectories align most closely with\nthe task at inference-time and thus, should exhibit\nhighest performance. On the other hand, an ex-\necutor that only sees atomic skills, each of which\nare insufficient to complete a single (sub-)task by\nitself, will need to rely on the inherent composition\nabilities of the LLM, thus exhibiting the weakest\nperformance. We are interested in examining if\nADAPT can improve success rates for all settings.\nTask-Specific\nHybrid\nAtomic\nExecutor Settings\n0\n10\n20\n30\n40\n50\n60\nSuccess Rate\nExecutor-Only\nADaPT (dmax=3)\nFigure 5: ADAPT improves success rates across varying\nsettings capturing different executor capabilities (i.e.,\nexecutor-only performance) on ALFWorld (dev).\nResults.\nIn Fig. 5, we observe that ADAPT con-\nsistently improves over the executor-only baseline\nfor all diverse executor settings. As expected, the\nexecutor prompted with task-specific trajectories\nperforms the best (left), while the executor with\nonly atomic skills performs the worst (right). No-\ntably, ADAPT substantially improves performance\nof the relatively weak executor, improving success\nrate from 3.3% to 41.7%.\nDifferent executor LLMs.\nThe planner and ex-\necutor modules of ADAPT do not need to necessar-\nily use the same underlying model. Following, Lin\net al. (2023) we explore if a relatively smaller LLM\ncan be used to perform local actions in the executor\nand a more advanced LLM be used to devise plans.\nTo this end, we explore different combinations of\nplanner and executor LLM, with the latter using\nboth gated and open-source models on ALFWorld.\nExecutor LM\nPlanner LM\nSuccess Rate\nGPT-3.5\n\u2212\n38.4\nGPT-3.5\nGPT-3.5\n58.3\nLLaMA-2-70B\n\u2212\n28.3\nLLaMA-2-70B\nGPT-3.5\n43.3\nTable 5: ADAPT improves performance on ALFWorld\n(dev) when using different planner and executor LLMs.\nResults.\nTable 5 shows that ADAPT can suc-\ncessfully be used to generate plans from one LLM\nthat are useful to a different, possibly smaller, ex-\necutor LLM, improving success rates by up to\n19.9% compared to the executor-only (ReAct) set-\nting. Interestingly, using an open-source model,\nsuch as LLaMA-2-70B-chat (Touvron et al., 2023)\ncan be used as an executor with a more advanced\nLLMs such as GPT-3.5 to improve success rates\nby 15% points. Since the planner LLM is used\nsparingly, open-source executors can dramatically\ndecrease the monetary or computational costs of\nusing ADAPT. We leave further exploration of\ncombining knowledge from stronger and weaker\nLMs within ADAPT, as studied in the context of\nmathematical reasoning (Fu et al., 2023; Saha et al.,\n2023a), to future work.\n6.3\nDoes ADAPT incorporate task\ncomplexity?\nWe now demonstrate that ADAPT adapts to the\ncomplexity of the underlying task instance within\nthe same dataset.\nCase Study: WebShop.\nBy default, Yao et al.\n(2023b) use a search page with only the top-3\nsearch results displayed. Intuitively, increasing the\nnumber of products on the search page requires the\nmodel to choose from a wider array of products\nand track all their information to determine the best\nfit to the user query, making the overall task harder.\nTherefore, we apply ADAPT on Webshop in two\nsettings with 3, and 10 products per search page.\nMethod\n#Products\nSuccess Rate\nReAct\n3\n27.5\nADAPT (dmax = 3)\n3\n47.5\nReAct\n10\n20.0\nADAPT (dmax = 3)\n10\n42.5\nTable 6: ADAPT improves WebShop (dev) performance\nirrespective of how many products (3 or 10) are chosen\nfrom the search page.\nResults.\nFrom Table 6, we observe that ADAPT\neffectively improves success rate by 20.0% and\n22.5% for 3 and 10 products respectively over the\nReAct baseline. The difference in ReAct perfor-\nmance for both settings corroborates our hypothesis\nthat increasing number of products on the search\npage increases task complexity, all else equal. No-\ntably, we show that ADAPT yields higher improve-\nment for more complex task settings.\nCase Study: TextCraft\nBy the compositional\ndesign of TextCraft, complexity of each task in the\ndataset can be defined with respect to the depth of\nthe crafting recipe, i.e., recipes with higher depth\nwould be more complex to craft. We evaluate effi-\ncacy of ADAPT and the ReAct baseline on the test\nset of TextCraft with increasing recipe depth.6 Fur-\nthermore, while we provide ADAPT with a maxi-\nmum budget of dmax = 4, we study how the maxi-\nmum decomposition depth utilized by ADAPT to\nsucceed (kmax) varies with task complexity.\nMethod\nRecipe Depth kmax Success Rate\nReAct\n2\n1.0\n26.9\nADAPT (dmax = 4)\n2\n1.9\n78.2\nReAct\n3\n1.0\n1.8\nADAPT (dmax = 4)\n3\n2.8\n38.7\nTable 7: ADAPT improves TextCraft (test) performance\neven as recipe depth increases. The maximum decom-\nposition depth used by ADAPT to succeed at the task\n(kmax) also scales with the recipe depth.\n6As we have only 11 tasks with recipe depth of 4, we\nexclude them from this analysis.\nResults.\nIn Table 7 we observe that ADAPT im-\nproves success rates for games with recipe depth\nof 2 from 26.9% \u2192 78.2%, and of depth 3 from\n1.8%\u219238.7% as compared to the ReAct baseline.\nAs expected, the executor alone is unable to handle\ncomplex recipes with depth \u2265 3, but with the help\nof ADAPT the performance improves significantly.\nAdditionally, given the same budget dmax = 4,\nas the recipe depth (complexity) increases from\n2 \u2192 3, ADAPT\u2019s level of decomposition (kmax)\nalso increases from 1.9 \u2192 2.8. This showcases\nthat ADAPT leverages as-needed decomposition\nto incorporate task complexity.\n7\nConclusion\nWe introduce ADAPT, a recursive algorithm de-\nsigned to harness the planning capabilities of\nLLMs, dynamically decomposing complex tasks\nwhen the LLM acting as an executor encoun-\nters challenges. Our evaluation across three di-\nverse decision-making tasks, ALFWorld, WebShop,\nand TextCraft, reveals impressive performance of\nADAPT, surpassing existing baselines by substan-\ntial margins of up to 28.3%, 27%, and 33% points,\nrespectively. This not only underscores the effec-\ntiveness of ADAPT but also highlights the signif-\nicance of as-needed decomposition in enhancing\ntask performance. Moreover, our findings demon-\nstrate that ADAPT not only adapts to the capabili-\nties of the underlying executor LLM but also takes\ninto account the complexity of individual task in-\nstances, showcasing its versatility and effectiveness.\nAcknowledgements\nPart of this work was done during internship at\nAI2 and was partially supported at UNC by NSF-\nCAREER Award 1846185, NSF-AI Engage Insti-\ntute DRL-2112635, DARPA Machine Common-\nsense (MCS) Grant N66001-19-2-4031,. We sin-\ncerely thank Bodhisattwa Prasad Majumder, Chris\nCallison-Burch, Shashank Gupta, Peter Jansen, Bill\nYuchen Lin and the Aristo team for their valuable\nfeedback. We also thank Swarnadeep Saha, Elias\nStengel-Eskin, and Peter Hase for their helpful\ncomments.\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nChuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, et al. 2022. Do as i can, not as i say: Ground-\ning language in robotic affordances. arXiv preprint\narXiv:2204.01691.\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and\nDan Klein. 2016. Neural module networks. In Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition, pages 39\u201348.\nAndrew G Barto and Sridhar Mahadevan. 2003. Re-\ncent advances in hierarchical reinforcement learning.\nDiscrete event dynamic systems, 13(1-2):41\u201377.\nValts Blukis, Chris Paxton, Dieter Fox, Animesh Garg,\nand Yoav Artzi. 2022. A persistent spatial semantic\nrepresentation for high-level natural language instruc-\ntion execution. In Conference on Robot Learning,\npages 706\u2013717. PMLR.\nAndy Coenen, Luke Davis, Daphne Ippolito, Emily\nReif, and Ann Yuan. 2021. Wordcraft: a human-ai\ncollaborative editor for story writing. arXiv preprint\narXiv:2107.07430.\nMarc-Alexandre C\u00f4t\u00e9, Akos K\u00e1d\u00e1r, Xingdi Yuan, Ben\nKybartas, Tavian Barnes, Emery Fine, James Moore,\nMatthew Hausknecht, Layla El Asri, Mahmoud\nAdada, et al. 2019. Textworld: A learning environ-\nment for text-based games. In Computer Games:\n7th Workshop, CGW 2018, Held in Conjunction with\nthe 27th International Conference on Artificial In-\ntelligence, IJCAI 2018, Stockholm, Sweden, July\n13, 2018, Revised Selected Papers 7, pages 41\u201375.\nSpringer.\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Ja-\ncob Austin, David Bieber, Raphael Gontijo Lopes,\nYuhuai Wu, Henryk Michalewski, Rif A Saurous,\nJascha Sohl-Dickstein, et al. 2022. Language model\ncascades. arXiv preprint arXiv:2207.10342.\nNouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine\nLi, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra\nBhagavatula, Ronan Le Bras, Jena D Hwang, et al.\n2023. Faith and fate: Limits of transformers on com-\npositionality. arXiv preprint arXiv:2305.18654.\nKutluhan Erol, James Hendler, and Dana S Nau. 1994.\nHtn planning: Complexity and expressivity. In AAAI,\nvolume 94, pages 1123\u20131128.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Man-\ndlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar.\n2022. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. Advances in\nNeural Information Processing Systems, 35:18343\u2013\n18362.\nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and\nTushar Khot. 2023. Specializing smaller language\nmodels towards multi-step reasoning. arXiv preprint\narXiv:2301.12726.\nIlche Georgievski and Marco Aiello. 2014. An overview\nof hierarchical task network planning. arXiv preprint\narXiv:1403.7426.\nMalik Ghallab, Dana Nau, and Paolo Traverso. 2004.\nAutomated Planning: theory and practice. Elsevier.\nNitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and\nMatt Gardner. 2019. Neural module networks for\nreasoning over text. In International Conference on\nLearning Representations.\nDaniel H\u00f6ller, Gregor Behnke, Pascal Bercher, Susanne\nBiundo, Humbert Fiorino, Damien Pellier, and Ron\nAlford. 2020. Hddl: An extension to pddl for ex-\npressing hierarchical planning problems. In Proceed-\nings of the AAAI conference on artificial intelligence,\npages 9883\u20139891.\nJie Huang and Kevin Chen-Chuan Chang. 2023. To-\nwards reasoning in large language models: A survey.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, pages 1049\u20131065, Toronto,\nCanada. Association for Computational Linguistics.\nJie\nHuang,\nXinyun\nChen,\nSwaroop\nMishra,\nHuaixiu Steven Zheng, Adams Wei Yu, Xiny-\ning Song, and Denny Zhou. 2023a. Large language\nmodels cannot self-correct reasoning yet.\narXiv\npreprint arXiv:2310.01798.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tomp-\nson, Igor Mordatch, Yevgen Chebotar, et al. 2023b.\nInner monologue: Embodied reasoning through plan-\nning with language models. In Conference on Robot\nLearning, pages 1769\u20131782. PMLR.\nYichen Jiang and Mohit Bansal. 2019. Self-assembling\nmodular networks for interpretable multi-hop rea-\nsoning. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4474\u20134484, Hong Kong, China. Association for Com-\nputational Linguistics.\nTushar Khot, Daniel Khashabi, Kyle Richardson, Peter\nClark, and Ashish Sabharwal. 2021. Text modular\nnetworks: Learning to decompose tasks in the lan-\nguage of existing models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1264\u20131279, Online.\nAssociation for Computational Linguistics.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2023. Decomposed prompting: A modular\napproach for solving complex tasks. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nBill Yuchen Lin, Yicheng Fu, Karina Yang, Prithvi-\nraj Ammanabrolu, Faeze Brahman, Shiyu Huang,\nChandra Bhagavatula, Yejin Choi, and Xiang Ren.\n2023. Swiftsage: A generative agent with fast and\nslow thinking for complex interactive tasks. arXiv\npreprint arXiv:2305.17390.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu\nLei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen\nMen, Kejuan Yang, et al. 2023. Agentbench: Evaluat-\ning llms as agents. arXiv preprint arXiv:2308.03688.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with\nself-feedback. arXiv preprint arXiv:2303.17651.\nSewon Min, Victor Zhong, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019. Multi-hop reading compre-\nhension through question decomposition and rescor-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6097\u20136109, Florence, Italy. Association for Compu-\ntational Linguistics.\nSo Yeon Min, Devendra Singh Chaplot, Pradeep Kumar\nRavikumar, Yonatan Bisk, and Ruslan Salakhutdinov.\n2022. Film: Following instructions in language with\nmodular methods. In International Conference on\nLearning Representations.\nOfir Nachum, Shixiang Shane Gu, Honglak Lee, and\nSergey Levine. 2018. Data-efficient hierarchical rein-\nforcement learning. Advances in neural information\nprocessing systems, 31.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nEthan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun\nCho, and Douwe Kiela. 2020. Unsupervised question\ndecomposition for question answering. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n8864\u20138880, Online. Association for Computational\nLinguistics.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, et al. 2023. Toolllm: Facilitating large\nlanguage models to master 16000+ real-world apis.\narXiv preprint arXiv:2307.16789.\nDesik Rengarajan, Gargi Vaidya, Akshay Sarvesh,\nDileep Kalathil, and Srinivas Shakkottai. 2022. Rein-\nforcement learning with sparse rewards using guid-\nance from offline demonstration. In International\nConference on Learning Representations.\nSwarnadeep Saha, Peter Hase, and Mohit Bansal. 2023a.\nCan language models teach weaker agents? teacher\nexplanations improve students via theory of mind.\narXiv preprint arXiv:2306.09299.\nSwarnadeep Saha, Shiyue Zhang, Peter Hase, and Mo-\nhit Bansal. 2023b. Summarization programs: Inter-\npretable abstractive summarization with neural mod-\nular trees. In The Eleventh International Conference\non Learning Representations.\nImanol Schlag, Sainbayar Sukhbaatar, Asli Celikyilmaz,\nWen-tau Yih, Jason Weston, J\u00fcrgen Schmidhuber,\nand Xian Li. 2023. Large language model programs.\narXiv preprint arXiv:2305.05364.\nPratyusha Sharma, Antonio Torralba, and Jacob An-\ndreas. 2022. Skill induction and planning with latent\nlanguage. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1713\u20131726, Dublin,\nIreland. Association for Computational Linguistics.\nLanbo She, Shaohua Yang, Yu Cheng, Yunyi Jia, Joyce\nChai, and Ning Xi. 2014. Back to the blocks world:\nLearning new actions through situated human-robot\ndialogue. In Proceedings of the 15th annual meet-\ning of the special interest group on discourse and\ndialogue (SIGDIAL), pages 89\u201397.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed Huai hsin Chi, Nathanael\nScharli, and Denny Zhou. 2023. Large language\nmodels can be easily distracted by irrelevant context.\nIn International Conference on Machine Learning.\nNoah Shinn, Federico Cassano, Beck Labash, Ash-\nwin Gopinath, Karthik Narasimhan, and Shunyu\nYao. 2023.\nReflexion:\nLanguage agents with\nverbal reinforcement learning.\narXiv preprint\narXiv:2303.11366, 14.\nMohit Shridhar, Jesse Thomason, Daniel Gordon,\nYonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke\nZettlemoyer, and Dieter Fox. 2020. Alfred: A bench-\nmark for interpreting grounded instructions for ev-\neryday tasks. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition,\npages 10740\u201310749.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9,\nYonatan Bisk,\nAdam Trischler,\nand Matthew\nHausknecht. 2021. ALFWorld: Aligning Text and\nEmbodied Environments for Interactive Learning.\nIn Proceedings of the International Conference on\nLearning Representations (ICLR).\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit\nGoyal, Danfei Xu, Jonathan Tremblay, Dieter Fox,\nJesse Thomason, and Animesh Garg. 2023. Prog-\nprompt: Generating situated robot task plans using\nlarge language models. In 2023 IEEE International\nConference on Robotics and Automation (ICRA),\npages 11523\u201311530. IEEE.\nChan Hee Song, Jiaman Wu, Clayton Washington,\nBrian M Sadler, Wei-Lun Chao, and Yu Su. 2023.\nLlm-planner: Few-shot grounded planning for em-\nbodied agents with large language models. In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision, pages 2998\u20133009.\nKaya Stechly, Matthew Marquez, and Subbarao Kamb-\nhampati. 2023. Gpt-4 doesn\u2019t know it\u2019s wrong: An\nanalysis of iterative prompting for reasoning prob-\nlems. arXiv preprint arXiv:2310.12397.\nSimeng Sun, Y. Liu, Shuo Wang, Chenguang Zhu, and\nMohit Iyyer. 2023. Pearl: Prompting large language\nmodels to plan and execute actions over long docu-\nments. ArXiv, abs/2305.14564.\nRichard S Sutton, Doina Precup, and Satinder Singh.\n1999. Between mdps and semi-mdps: A framework\nfor temporal abstraction in reinforcement learning.\nArtificial intelligence, 112(1-2):181\u2013211.\nAlon Talmor and Jonathan Berant. 2018. The web as\na knowledge-base for answering complex questions.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 641\u2013651, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-\nima Anandkumar. 2023a. Voyager: An open-ended\nembodied agent with large language models. arXiv\npreprint arXiv:2305.16291.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,\nYunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.\n2023b. Plan-and-solve prompting: Improving zero-\nshot chain-of-thought reasoning by large language\nmodels. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 2609\u20132634, Toronto,\nCanada. Association for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nJohn Yang, Akshara Prabhakar, Karthik Narasimhan,\nand Shunyu Yao. 2023. Intercode: Standardizing\nand benchmarking interactive coding with execution\nfeedback. arXiv preprint arXiv:2306.14898.\nShunyu Yao, Howard Chen, John Yang, and Karthik\nNarasimhan. 2022. Webshop: Towards scalable real-\nworld web interaction with grounded language agents.\nAdvances in Neural Information Processing Systems,\n35:20744\u201320757.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023a. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao.\n2023b. React: Synergizing reasoning and acting\nin language models. In The Eleventh International\nConference on Learning Representations.\nJesse Zhang, Haonan Yu, and Wei Xu. 2021. Hierarchi-\ncal reinforcement learning by discovering intrinsic\noptions. In International Conference on Learning\nRepresentations.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman,\nHaohan Wang, and Yu-Xiong Wang. 2023.\nLan-\nguage agent tree search unifies reasoning acting\nand planning in language models. arXiv preprint\narXiv:2310.04406.\nA\nADAPT Implementation Details\nExecutor.\nWe use a common ReAct executor for\neach dataset. To this end, we provide the LLM\nin the executor with in-context example trajecto-\nries for each atomic skill (refer to Table 1 for an\nexhaustive list). Atomic skills are inherently task\ndependent, and thus, vary with the underlying envi-\nronment. For ALFWorld, in which the agent needs\nto navigate and perform tasks in the household, the\natomic skills include: taking an object, putting it\ndown at a location, cleaning, heating, etc. On the\nother hand, the goal in WebShop is to buy a product\nbased on user queries, thus, atomic skills include:\nsearching a specified query, shortlisting products\nbased on search page, matching if a product sat-\nisfies a criteria, and buying a product. Lastly, the\natomic skills in TextCraft are fetching objects from\nthe environment, and crafting them given the recipe\nand the ingredients. Following Yao et al. (2023b),\nwe add gold trajectories for two tasks: heat and\nlook in the executor prompt for ALFWorld, and\none full gold trajectory for TextCraft.\nPlanner.\nWe provide the LLM with a brief de-\nscription of atomic skills and in-context demonstra-\ntions of few task decompositions for each dataset.\n\u2022 ALFWorld: The planner includes 6 demonstra-\ntions of task decompositions for one household\nconfiguration.\nSpecifically, \u201cfind\u201d is not an\natomic skill for the executor, and therefore, needs\nto be handled by the planner (refer to Fig. 2).\n\u2022 WebShop: The planner breaks down a given task\nin terms of the atomic skills described in Table 1\nvia 2 in-context demonstrations.\n\u2022 TextCraft: The planner determines the necessary\ningredients for each item and creates a plan to\nobtain them and then craft the item, illustrated\nvia 2 examples with different crafting commands.\nController.\nAs discussed in Sec. 3.3, the con-\ntroller serves as the communication bridge between\nthe planner and the executor by propagating task-\ndependent salient information described below:\n\u2022 ALFWorld: In the controller, we propagate the\nlast successful action from a previous execution\nrun to subsequent calls of the executor. Note that\ninformation is only propagated from successful\nsub-tasks. For sub-tasks connected via \u201cOR\u201d,\neach receives the same information from the con-\ntroller. Unlike Shinn et al. (2023), executor does\nnot get text feedback from prior failures.\n\u2022 WebShop: We propagate the current page vis-\nible to the agent along with past unsuccessful\nexecutor tasks to the planner (without any ratio-\nnales). Once we find a matching product, we also\npropagate the product ID in future executor calls.\n\u2022 TextCraft: We propagate the current inventory\nof the agent to the executor. This is akin to ex-\necutors starting with the inventory command\nas the first step to keep stock of which items are\nmissing and need to be fetched or crafted.\nFor partial rolled-out trajectories with ADAPT re-\nfer to examples in Figs. 7 to 9. The communication\nbetween the planner and executor modules is high-\nlighted in gray box(es) .\nLLM-related Hyperparameters.\nFollowing pre-\nvious works (Shinn et al., 2023; Liu et al., 2023)\nwe use text-davinci-003 from the OpenAI\nAPI for ALFWorld. For WebShop, we use the\ngpt-3.5-turbo models, and for TextCraft we use\nthe gpt-3.5-turbo-instruct models. All execu-\ntors have a maximum budget of iterations to inter-\nact with the environment and execute the task. We\nset this budget to 20, 15, and 20 respectively for\nALFWorld, WebShop, and TextCraft respectively.\nFor try again with ReAct, we sample additional tra-\njectories with a temperature of 0.7. As discussed in\nSec. 4.2, we run the iterative executor-only baseline\nfor 60, 45, 60 iterations for ALFWorld, WebShop,\nand TextCraft respectively. For both planner and\nexecutor modules, we use a fixed prompt consisting\nof few in-context examples (as described above)\nfor each dataset. We show all executor and planner\nprompts to the LLM in Appendix G.\nMethod\nPick Clean Heat Cool Look Pick2\nAll\nReAct\n66.7\n41.9\n47.8\n80.9\n83.3\n23.5\n56.7\nPlan-and-Execute\n87.5\n58.1\n73.9\n52.4\n83.3\n17.6\n63.4\nTry Again with ReAct 75.0\n38.7\n60.9\n76.2\n66.7\n23.5\n56.7\nReflexion\n83.3\n61.3\n73.9\n85.7\n61.1\n29.4\n67.2\nADAPT (Ours)\n91.7\n67.7\n78.3\n81.0\n100\n64.7\n79.8\nTable 8: Comparison of success rates (%) achieved by ADAPT\nand other baselines from prior work on ALFWorld (test split)\nwith executor used by Yao et al. (2023b)\nMethod\nScore Success Rate\nIterative Executor-Only\n42.1\n29.0\nStatic Decomposition\n27.7\n17.0\nRetry Execution\n45.4\n30.0\nNaive\n58.3\n24.0\nReflexion*\n64.2\n35.0\nLATS (Zhou et al., 2023)*\n75.9\n38.0\nADAPT (Ours)\n60.0\n44.0\nTable 9: Performance comparison of differ-\nent methods on WebShop.\nPlan: Put a clean mug on desk\n# Think: To do this task, ....\nStep 1: Find and take the mug AND\n# Think: Now that I have found it, ....\nStep 2: Clean the mug using sinkbasin AND\n# Think: Now that I have cleaned ....\nStep 3: Put clean mug on desk\nPlan: Find and take the mug\n# Think: To do this task, ....\nStep 1: Find and take mug from countertop OR\n# Think: If I do not find the mug, ....\nStep 2: Find and take mug from cabinet OR\n# Think: If I do not find the mug, ....\nStep 3: Find and take mug from drawer\nPlan: Put a clean mug on desk\n# Think: To do this task, ....\nStep 1: Find and take mug from\ncountertop OR\n# Think: If I do not find the mug, ....\nStep 2: Find and take mug from cabinet OR\n# Think: If I do not find the mug, ....\nStep 3: Find and take mug from drawer AND\n# Think: Now that I have found it, ....\nStep 4: Clean the mug using sinkbasin AND\n# Think: Now that I have cleaned ....\nStep 5: Put clean mug on desk\nLogic: ((Step 1 OR Step 2 OR Step 3) AND\nStep 4 AND Step 5)\nAdaptive Multi-level Plans in ADaPT\nDetailed Plans in Plan-and-Execute\nFigure 6: Illustration of how multiple levels of plans\nfrom ADAPT, can be collapsed into one detailed plan\nin non-adaptive settings as used in the plan-and-execute\nbaseline (Sec. 4.2). Our controller can handle complex\n(non-homogeneous) logical expressions.\nB\nHandling Complex Logic in Plans\nWhile the examples in Figs. 1 and 2 show homoge-\nneous logic across sub-tasks in the plan, our con-\ntroller can handle complex logical expressions in-\ncluding both \u201cAND\u201d and \u201cOR\u201d operators. Specif-\nically, we provide instructions to the planner to\noutput this logical expressing at the end of the plan\nwith a fixed prefix: Execution Order. We then\nbuild a deterministic parser that can parse complex\nlogical expressions that the controller can process.\nWe do so by splitting the logical expression into a\nseries of homogeneous expression each passed to\nADAPT. Whenever the task given to ADAPT com-\nprises of multiple sub-tasks connected via (one)\nlogical operator, we automatically decompose this\ntask as per the logical expression. For example,\nin Fig. 6, a detailed plans used by the plan-and-\nexecute baseline (discussed in Sec. 4.2) comprised\nof logical expressions using both AND, and OR\noperators. Therefore, the parser will break auto-\nmatically break this into multiple levels, i.e., Step\n6 = Step 1 OR Step 2 OR Step 3, followed by Step\n6 AND Step 4 AND Step 5. While such complex\nlogical expressions are mostly associated with the\nplan-and-execute baseline, they can be easily used\nwithin the ADAPT framework. Furthermore, this\nallows the plan-and-execute baseline to simulate\na multi-level planning structure via detailed plans\nwithout being adaptive to the executor.\nC\nTask-specific Executors in ALFWorld\nIn Table 2, we use a standardized executor with\nin-context demonstrations of atomic skills and two\ngold trajectories. While this allows for a common\nexecutor across different sub-tasks, task-specific\nexecutors yield higher performance on the specific\nsub-tasks. We now show ADAPT can also be used\non top of task-specific executors used by Yao et al.\n(2023b). The results are shown in Table 8. First,\nwe observe that ADAPT yields the overall success\nrate by up to 23.1% points and also surpasses base-\nlines on all but 1 task types. Interestingly, we find\nstrong performance of the plan-and-execute base-\nline when using a stronger executor (as compared\nto Table 2) possibly as such an executor can handle\ncomplex sub-tasks better. Consistent with Table 2,\nADAPT outperforms Reflexion by 12.6% points\ndespite lack of dedicated memory and natural lan-\nguage feedback.\nRoom: You are in the middle of a room. Looking quickly around you, you\nsee a cabinet 21, a cabinet 20, a cabinet 19, a cabinet 18, a cabinet 17,\na cabinet 16, a cabinet 15, a cabinet 14, a cabinet 13, a cabinet 12, a\ncabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet 7, a cabinet\n6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a\ncoffeemachine 1, a countertop 2, a countertop 1, a diningtable 1, a\ndrawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a\ngarbagecan 1, a microwave 1, a sinkbasin 1, a stoveburner 4, a\nstoveburner 3, a stoveburner 2, a stoveburner 1, desk 1, and a toaster 1.\nYour task is to: Put a clean mug on desk.\nExecute: Put a clean mug on desk\n> Go to countertop 1\nYou reached loc 1, ...\n> Go to countertop 2\nYou reached loc 2, ...\n> Think: Mug not found, task failed!\nPlan: Put a clean mug on desk\n# Think: To do this task, ....\nStep 1: Find and take the mug AND\n# Think: Now that I have found it, ....\nStep 2: Clean the mug using sinkbasin AND\n# Think: Now that I have cleaned ....\nStep 3: Put clean mug on desk\nExecute: Find and take the mug\n> Think: Task failed!\nPlan: Find and take the mug\n# Think: To do this task, ....\nStep 1: Find and take mug from countertop OR\n# Think: If I do not find the mug, ....\nStep 2: Find and take mug from cabinet OR\n# Think: If I do not find the mug, ....\nStep 3: Find and take mug from drawer\nExecute: Find and take mug from countertop\n> Think: Task failed!\nExecute: Find and take mug from cabinet\n> Think: Task completed!\nExecute: Clean the mug using sinkbasin\n> Think: Task completed!\nExecute: Find and take the mug from countertop\n> Think: Task completed!\nPreviously executed: \n> take mug 1 from cabinet 7\nYou picked up mug 1\n...\n...\nRoom: You are in the middle of a room. Looking quickly around you, you\nsee a cabinet 21, a cabinet 20, a cabinet 19, a cabinet 18, a cabinet 17,\na cabinet 16, a cabinet 15, a cabinet 14, a cabinet 13, a cabinet 12, a\ncabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet 7, a cabinet\n6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a\ncoffeemachine 1, a countertop 2, a countertop 1, a diningtable 1, a\ndrawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a\ngarbagecan 1, a microwave 1, a sinkbasin 1, a stoveburner 4, a\nstoveburner 3, a stoveburner 2, a stoveburner 1, desk 1, and a toaster 1.\nYour task is to: Put a clean mug on desk.\nExecute: Put a clean mug on desk\n> Think: To solve the task, I need to find and take a mug, then clean it\nwith sinkbasin, then put on the desk.\nOK.\n> Think: First I need to find a mug. A mug is more likely to appear in\nfridge (1), countertop (1-2), dining table (1), cabinet (1-21), drawer\n(1-5). I can check one by one, starting with fridge 1.\nOK.\n> open fridge 1\nNothing happens.\n> go to fridge 1\nYou arrive at loc 15, fridge 1 is closed.\n> open fridge 1\n        You open the fridge 1. The fridge 1 is open. In it, you see a bowl 1, a\ncup 1, a egg 1, a lettuce 1, a plate 1, and a tomato 1.\n> Think: There is no mug in fridge 1, I need to go to countertop 1 next.\nOK.\n> go to countertop 1\nYou reached loc 1, on it you see a cd 2, a cellphone 3, a\ncellphone 1, a keychain 3, a pen 2, a pencil 3 and a pencil 2.\n> go to countertop 2\n    You reached loc 2, on it you see a egg 3, a knife 2, a knife 1, a potato\n1, a potato 2, a bread 1, a dishsponge 3, and a saltshaker 1.\n> Think: There is no mug in countertop 2, I need to go to cabinet 1.\nOK.\n> go to cabinet 1\nYou arrive at loc 4, cabinet 1 is closed.\n> open cabinet 1\n                    You open cabinet 1. Cabinet 1 is open. In it, you see bowl 3,\nfork 2, soapbottle 1, and a cloth 1.\n> go to cabinet 2\nYou arrive at loc 5, cabinet 2 is closed.\n> open cabinet 2\nYou open cabinet 2. Cabinet 2 is open. In it, you see nothing.\n> open cabinet 4\nNothing happens.\n> go to cabinet 4\nYou arrive at loc 7, cabinet 4 is closed.\n> open cabinet 4\n        You open cabinet 4. Cabinet 4 is open. In it, you see a spatula 1, a\ncloth 2, a peppershaker 1, cup 3, a apple 2, and a toiletpaper 1.\n> take mug 3 from cabinet 4\nNothing happens.\n> take cup 3 from cabinet 4\nYou pick up cup 3 from cabinet 4\n> Think: Now I found cup (3). Next I need to clean it. For this ...\n...\n   ADaPT (Adaptive Decomposition, Dynamic Depth)\nIterative Executor (ReAct)\n...\n...\n...\n...\nFigure 7: Comparison of iterative executors such as ReAct with ADAPT. On left, ReAct uses interleaved \u201cthought\u201d\nstatements to set milestones and track their progress. However, due to a large action history, it struggles to follow\nthe plan exactly and hallucinates the wrong object (highlighted in red). ADAPT, on the right, decomposes complex\ntasks into smaller sub-tasks whenever the executor fails, leading to shorter action trajectories for easy execution.\nD\nEvaluation Metrics in WebShop\nWe focus on success rate and not the (soft) score\nas the primary metric for this task because it is\npossible to get a non-zero score by naively buying\na product. To this effect, we construct a naive\nexecutor that inputs the user query in the search bar\nand buys the first available product. Table 9 shows\nthat while this baseline yields the lowest success\nrate, it surprisingly yields a high success rate of\n58.3. In contrast, our executors often do not buy\nproducts especially when the previous sub-goals\nfail which can adversely impact scores even though\nthe success rate remains unaffected. Therefore, we\nargue for optimizing the success rate instead of the\nscore as opposed to prior works (Zhou et al., 2023).\nWebpage: [Search]\nTask: I am looking to buy a product. i need a leak proof travel bottle that\nis reusable and comes in 6 pack, and price lower than 30.00 dollars\nExecute: Task\n> search[leak proof reusable bottle]\nPage: \n[B0956WL7LT] ...\n[B098XRX6M2] ...\n> Think: Task failed!\nPlan: Task\n# Think: To do this task, ....\nStep 1: Search \"leak proof travel bottle reusable 6 pack\"  AND\n# Think: Now that I have found it, ....\nStep 2: Match product on search page AND\n# Think: Now that I have matched ....\nStep 3: Buy matching product\nExecute: Search \"leak proof travel bottle reusable 6 pack\"\n> Think: Task completed!\nPlan: Match product on search page\n# Think: To do this task, ....\nStep 1: Detail Match [B09...7LT] on product page OR\n# Think: If that does not match, ....\nStep 2: Detail Match [B08...GBG] on product page OR\n# Think: If that does not match, ....\nStep 3: Search \"leak proof bottle reusable 6 pack $30\"\nExecute: Detail Match [B09...7LT] on product page\n> Think: Task failed!\nExecute: Detail Match [B08...GBG] on product page\n> Think: Task completed!\nExecute: Buy matching product\n> Think: Task completed!\nMatching Product: \n[B08BGBPGBG] \n...\n...\n   ADaPT (Adaptive Decomposition, Dynamic Depth)\n...\n...\n...\nExecute: Match product on search page\n> Think: Task failed!\n...\nSearch Results:\n[B0956WL7LT] Portable Travel Bottles Set ...\n[B08BGBPGBG] JND Plastic Squeeze Bottle ...\n \nPrev: Failed Match product on search page\n...\nFigure 8: Partial rolled out trajectories for WebShop\nwith ADAPT. In the gray box we communicate to the\nplanner the current (search) page that is visible to the\nagent, and once a matching product is found, we propa-\ngate it to future executor runs. Note \u201cmatch on search\npage\u201d corresponds to shortlist skill in Table 1, and \u201cde-\ntail match on product page\u201d corresponds to match skill.\nE\nTextCraft\nWe create the TextCraft environment using\nMinecraft v1.16.5 recipes. We only consider the\nrecipes craftable using a crafting table. We consider\nboth shapeless (only count matters) and shaped\n(position of ingredients matters) recipes and con-\nvert them into crafting commands (e.g. craft 4\nsticks using 2 planks). Items that do not have\nany recipe are considering obtainable via the get\ncommand, e.g. get 4 diamond.\nSince the entire set of crafting commands would\nCrafting Commands: \ncraft andesite slab using 3 andesite\ncraft 2 andesite using 1 diorite, 1 cobblestone\ncraft 6 diorite wall using 6 diorite\ncraft 2 diorite using 2 quartz, 2 cobblestone\nTask: craft andesite slab\nExecute: craft andesite slab\n> get 3 andesite\nCould not execute: get 3 andesite\n> get 1 diorite\nCould not execute: get 1 diorite\n> Think: Task failed!\nPlan: craft andesite slab\n# Think: To do this task, ....\nStep 1: Fetch 3 andesite  AND\n# Think: Now that I have found it, ....\nStep 2: Craft andesite slab using 3 andesite\nExecute: Fetch 3 andesite\n> Think: Task failed!\nPlan: Fetch 3 andesite\n# Think: To do this task, ....\nStep 1: Fetch 1 diorite AND\n# Think: Fetch the next ingredient, ....\nStep 2: Fetch 1 cobblestone AND\n# Think: Now that I have found, ....\nStep 3: Craft 2 andesite using 1 diorite, 1 cobblestone AND\n# Think: Since I need 3 andesite, ....\nStep 4: Craft 2 andesite using 1 diorite, 1 cobblestone\nExecute: Fetch 1 diorite\n> Think: Task completed!\nExecute: Fetch 1 cobblestone\n> Think: Task completed!\nExecute: Craft andesite slab using 3 andesite\n> Think: Task completed!\n...\n...\n   ADaPT (Adaptive Decomposition, Dynamic Depth)\n...\n...\n...\n...\nInventory: [diorite] (2)\nInventory: [diorite] (2), [cobblestone] (1)\nExecute:  Craft 2 andesite using 1 diorite, 1\ncobblestone\n> Think: Task completed!\n...\nInventory: [diorite] (1), [cobblestone] (1), [andesite] (2)\nExecute:  Craft 2 andesite using 1 diorite, 1\ncobblestone\n> Think: Task completed!\nInventory: [andesite] (4)\nFigure 9: Partial rolled out trajectories for TextCraft\nusing ADAPT. In the gray box, we propagate the in-\nventory of the agent to subsequent executor calls. Note\nthat while \u201cdiorite\u201d is not directly present in the envi-\nronment, i.e., it needs to be crafted. The executor LLM\nis able to inherently compose skills to fetch it without\nfurther decomposition.\nnot fit in the context of modern LLMs, we create\na set of relevant crafting commands for every task.\nApart from the set of gold crafting commands (i.e,\ncrafting commands for all the items in the recipe\ntree), we also add up to 10 distractor commands.\nTo create this distractor set, we sub-sample up to\n10 recipes for every ingredient in the recipes of our\nALFWorld\nWebShop\nTextCraft\n0\n20\n40\n60\n80\nSuccess Rate\nGold Environment Reward\nSelf-generated Success Heuristic\nFigure 10: Comparison of LLM-generated success\nheuristic with gold environment rewards to compute\nsuccess rates for all datasets.\ngold recipe tree. We finally sub-sample up to 10\ndistractors from this entire set to ensure a reason-\nable context size. Note that we do not provide the\nlist of valid get commands as that can be inferred\nfrom the craft commands.\nF\nEvaluation of Success Heuristic\nIn Sec. 3.1, we describe the executor module used\nin ADAPT. For tasks assigned to the executor,\nwe prompt the LLM to generate a binary success\nheuristic. We use this heuristic repeatedly to evalu-\nate if the (sub-)task needs to be decomposed further.\nWe now study the ability of LLMs to generate this\nsuccess heuristic on all our datasets. To this end,\nwe run ADAPT and in the end compare the success\nrate when using the LLM\u2019s self-assessed task suc-\ncess with the gold reward from the environment in\nFig. 10. On ALFWorld and TextCraft, we find the\nLLM slightly over-estimates its overall task suc-\ncess. This is to be expected as the underlying tasks\ninvolve minimal subjectivity (e.g., the agent either\nhas an item on its inventory or not). However, on\nWebShop, where a product can match the user crite-\nria to different degrees (partially or fully), we find\nthat the LLM\u2019s assessment is significantly inflated\ncompared to the environment reward (>30 points).\nThis imperfect feedback affects downstream per-\nformance of ADAPT, as the algorithm terminates\neven though further decomposition is needed. We\nleave it to future work to address the shortcomings\nof self-evaluation with LLMs (Huang et al., 2023a;\nStechly et al., 2023).\nG\nPrompts\nWe provide all the prompts used in our planner and\nexecutor modules for ALFWorld, WebShop, and\nTextCraft datasets in the following pages.\nTextCraft Executor Prompt\nYour task is to come up with a short plan to help me accomplish my goal in a couple of steps using at most ONE of the provided\ncrafting commands. You can take the help of crafting commands below to create new objects.\nCraft command can be understood as follows: craft [target] using [ingredients], where target is item/object generated by the craft\ncommand as output and ingredient are the inputs. You are given an agent that can \"craft\" or \"fetch\" objects.\nHere is are some examples.\nCrafting commands:\ncraft 3 dark oak sign using 6 dark oak planks, 1 stick\ncraft 4 dark oak planks using 1 dark oak log\ncraft 1 stick using 1 planks\ncraft 4 stick using 2 bamboo\ncraft 4 oak planks using 1 oak log\ncraft 1 dark oak fence using 2 stick, 4 dark oak planks\ncraft 1 warped stairs using 6 warped planks\ncraft 3 oak sign using 6 oak planks, 1 stick\nGoal: craft dark oak sign.\n# Think: My target is a dark oak sign. From the list of crafting commands, only 1 command generates my target: craft 3 dark oak\nsign using 6 oak planks, 1 stick. I will use this command to devise a plan. My ingredients are: 6 dark oak planks, 1 stick.\nI should first get all the ingredients and then use the crafting command.\nStep 1: fetch 6 dark oak planks\nStep 2: fetch 1 stick\n# Think: Now that I have collected the input ingredients, I can craft the dark oak sign using given command.\nStep 3: craft dark oak sign using 6 dark oak planks, 1 stick\n# Think: To succeed, I need to perform all these steps, one after the other. So I need to use the \"AND\" operator.\nExecution Order: (Step 1 AND Step 2 AND Step 3)\nGoal: fetch 6 dark oak planks.\n# Think: My target is 6 dark oak planks. From the list of crafting commands, only 1 command generates my target: craft 4 dark oak\nplanks using 1 dark oak log. My ingredients are: 1 dark oak log. To successfully accomplish the goal, I should first get all\nthe ingredients and then use the crafting command.\nStep 1: fetch 1 dark oak log\n# Think: Now that I have collected the input ingredients, I can craft dark oak planks using given command. I know that I cannot\nuse a partial recipe.\nStep 2: craft 4 dark oak planks using 1 dark oak log\n# Think: This gives me 4 dark oak planks which is less than my desired 6 dark oak planks. I know that I cannot use a partial\nrecipe. So my goal is not satisfied, I need to craft more dark oak planks by repeating Step 2 one more time.\nStep 3: craft 4 dark oak planks using 1 dark oak log\n# Think: To succeed, I need to perform all these steps, one after the other. So I need to use the \"AND\" operator.\nExecution Order: (Step 1 AND Step 2 AND Step 3)\nHere is a different goal with different craft commands. Your task is to come up with a short plan to help me accomplish my goal in\na couple of steps using at most ONE of the provided crafting commands. You can take the help of crafting commands below to\ncreate new objects. Keep in mind that:\n- It is okay to generate more target objects than your goal.\n- Be very careful with the count of objects, SAME object counts mentioned in the input crafting command.\n- You cannot use a partial crafting command recipe, i.e. if the recipe generates 2 objects you CANNOT alter it to produce just 1.\n- Also, you can use ONLY 1 crafting command in your plan.\nTextCraft Planner Prompt\nYour task is to come up with a short plan to help me accomplish my goal in a couple of steps using at most ONE of the\nprovided crafting commands. You can take the help of crafting commands below to create new objects.\nCraft command can be understood as follows: craft [target] using [ingredients], where target is item/object generated by\nthe craft command as output and ingredient are the inputs. You are given an agent that can \"craft\" or \"fetch\" objects.\nHere is are some examples.\nCrafting commands:\ncraft 3 dark oak sign using 6 dark oak planks, 1 stick\ncraft 4 dark oak planks using 1 dark oak log\ncraft 1 stick using 1 planks\ncraft 4 stick using 2 bamboo\ncraft 4 oak planks using 1 oak log\ncraft 1 dark oak fence using 2 stick, 4 dark oak planks\ncraft 1 warped stairs using 6 warped planks\ncraft 3 oak sign using 6 oak planks, 1 stick\nGoal: craft dark oak sign.\n# Think: My target is a dark oak sign. From the list of crafting commands, only 1 command generates my target: craft 3 dark\noak sign using 6 oak planks, 1 stick. I will use this command to devise a plan. My ingredients are: 6 dark oak\nplanks, 1 stick. I should first get all the ingredients and then use the crafting command.\nStep 1: fetch 6 dark oak planks\nStep 2: fetch 1 stick\n# Think: Now that I have collected the input ingredients, I can craft the dark oak sign using given command.\nStep 3: craft dark oak sign using 6 dark oak planks, 1 stick\n# Think: To succeed, I need to perform all these steps, one after the other. So I need to use the \"AND\" operator.\nExecution Order: (Step 1 AND Step 2 AND Step 3)\nGoal: fetch 6 dark oak planks.\n# Think: My target is 6 dark oak planks. From the list of crafting commands, only 1 command generates my target: craft 4\ndark oak planks using 1 dark oak log. My ingredients are: 1 dark oak log. To successfully accomplish the goal, I\nshould first get all the ingredients and then use the crafting command.\nStep 1: fetch 1 dark oak log\n# Think: Now that I have collected the input ingredients, I can craft dark oak planks using given command. I know that I\ncannot use a partial recipe.\nStep 2: craft 4 dark oak planks using 1 dark oak log\n# Think: This gives me 4 dark oak planks which is less than my desired 6 dark oak planks. I know that I cannot use a\npartial recipe. So my goal is not satisfied, I need to craft more dark oak planks by repeating Step 2 one more time.\nStep 3: craft 4 dark oak planks using 1 dark oak log\n# Think: To succeed, I need to perform all these steps, one after the other. So I need to use the \"AND\" operator.\nExecution Order: (Step 1 AND Step 2 AND Step 3)\nHere is a different goal with different craft commands. Your task is to come up with a short plan to help me accomplish my\ngoal in a couple of steps using at most ONE of the provided crafting commands. You can take the help of crafting\ncommands below to create new objects. Keep in mind that:\n- It is okay to generate more target objects than your goal.\n- Be very careful with the count of objects, SAME object counts mentioned in the input crafting command.\n- You cannot use a partial crafting command recipe, i.e. if the recipe generates 2 objects you CANNOT alter it to produce\njust 1.\n- Also, you can use ONLY 1 crafting command in your plan.\nALFWorld Hybrid Executor Prompt\nHere is a demo of actions you can perform.\nYou are in the middle of a room. Looking quickly around you, you see a desk 1, microwave 1, a cabinet 3, a cabinet 9, a\ndrawer 2, a coffeemachine 1, a stoveburner 4, a drawer 5, a cabinet 11, a drawer 3, a stoveburner 1, a drawer 1, a\ntoaster 1, a fridge 1, a stoveburner 2, a cabinet 6, a cabinet 10, a countertop 1, a cabinet 13, a cabinet 7, a\ngarbagecan 1, a cabinet 2, a cabinet 8, a cabinet 12, a drawer 4, a cabinet 1, a sinkbasin 1, a cabinet 5, a\nstoveburner 3, and a cabinet 4.\nYour task is to: Put a tomato in cabinet 1.\n> think: Input assumption: I am carrying a tomato. Now I need to verify this.\nOK.\n> inventory\nYou are carrying: a tomato 1.\n> examine tomato 1\nThere's nothing special about tomato 1.\n> think: Based on previous context assumption is true, now I can proceed.\n> go to cabinet 1\nThe cabinet 1 is closed.\n> open cabinet 1\nYou open the cabinet 1. The cabinet 1 is open. In it, you see a candle 1, and a spraybottle 2.\n> put tomato 1 in/on cabinet 1\nYou put the tomato 1 in/on the cabinet 1.\n> think: End state: Now I put tomato (1) in cabinet (1). Task completed!\nYour task is to: Take a mug from cabinet 5\n> think: Input assumption: None\nOK.\n> go to cabinet 5\nThe cabinet 5 is closed.\n> open cabinet 5\nYou open the cabinet 5. The cabinet 5 is open. In it, you see a mug 3.\n> take mug 3 from cabinet 5\nYou pick up the mug 3 from the cabinet 5.\n> think: End state: Now I take a mug (3) from cabinet (5). Task completed!\nYour task is to: Heat an egg.\n> think: Input assumption: I am carrying an egg. Now I need to verify this.\nOK.\n> inventory\nYou are carrying: a egg 2.\n> examine egg 2\nThere's nothing special about egg 2.\n> think: Based on previous context assumption is true, now I can proceed.\nOK.\n> go to microwave 1\nThe microwave 1 is closed.\n> heat egg 2 with microwave 1\nYou heat the egg 2 using the microwave 1.\n> think: End state: Now I heat a egg (2). Task completed!\nYour task is to: Cool a mug.\n> think: Input assumption: I am carrying a mug. Now I need to verify this.\nOK.\n> inventory\nYou are carrying: a mug 3.\n> examine mug 3\nThere's nothing special about mug 3.\n> think: Based on previous context assumption is true, now I can proceed.\nOK.\n> go to fridge 1\nThe fridge 1 is closed.\n> cool mug 3 with fridge 1\nYou cool the mug 3 using the fridge 1.\n> think: End state: Now I cool a mug (3). Task completed!\nYour task is to: Clean a lettuce.\n> think: Input assumption: I am carrying a lettuce. Now I need to verify this.\nOK.\n> inventory\nYou are carrying: a lettuce 1.\n> examine lettuce 1\nThere's nothing special about lettuce 1.\n> think: Based on previous context assumption is true, now I can proceed.\nOK.\n> go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n> clean lettuce 1 with sinkbasin 1\nYou clean the lettuce 1 using the sinkbasin 1.\n> think: End state: Now I clean a lettuce (1). Task completed!\nALFWorld Hybrid Executor Prompt (cont.)\nYour task is to: Use a desklamp to look at tomato 1.\n> think: Input assumption: I am at the location that contains desklamp (1). Now I need to verify this.\nOK.\n> inventory\nYou are carrying: a tomato 1.\n> look\nYou are facing desk 1. Next to it, you see nothing.\n> examine desk 1\nOn desk 1, you see a spoon 1, a desklamp 1, and a pencil 2.\n> think: Based on previous context assumption is true, now I can proceed.\nOK.\n> use desklamp 1\nYou turn on the desklamp 1.\n> think: Now I use desklamp (1). Task completed!\nHere are two example tasks.\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 13, a cabinet 12, a cabinet 11, a cabinet 10,\na cabinet 9, a cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1,\na coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3,\na shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a\ntoaster 1.\nYour task is to: put a hot apple in fridge.\n> think: To solve the task, I need to find and take an apple, then heat it with microwave, then put it in fridge.\nOK.\n> think: First I need to find an apple. An apple is more likely to appear in fridge (1), diningtable (1), coffeetable (1),\ndrawer (1), cabinet (1-13), garbagecan (1). I can check one by one, starting with fridge 1.\nOK.\n> go to fridge 1\nThe fridge 1 is closed.\n> open fridge 1\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 1, and a egg 1.\n> go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a fork 3, a fork 2, a fork 1, a mug 2, a peppershaker 3, a plate 2, a\npot 1, a soapbottle 3, a spatula 1, a spoon 3, a spoon 2, a spoon 1, a winebottle 3, and a winebottle 2.\n> think: Now I find an apple (1). Next, I need to take it.\nOK.\n> take apple 1 from diningtable 1\nYou pick up the apple 1 from the diningtable 1.\n> think: Now I take an apple (1). Next, I need to go to a microwave (1) and heat it.\nOK.\n> go to microwave 1\nThe microwave 1 is closed.\n> heat apple 1 with microwave 1\nYou heat the apple 1 using the microwave 1.\n> think: Now I heat an apple (1). Next, I need to put it in/on fridge 1.\nOK.\n> go to fridge 1\nThe fridge 1 is open. In it, you see a cup 1, and a egg 1.\n> put apple 1 in/on fridge 1\nYou put the apple 1 in/on the fridge 1.\n> think: Task completed!\nYou are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 1, a drawer 3, a drawer 2, a drawer 1,\na garbagecan 1, a safe 1, a shelf 5, a shelf 4, a shelf 3, a shelf 2, a shelf 1, a sidetable 2, and a sidetable 1.\nYour task is to: look at bowl under the desklamp.\n> think: To solve the task, I need to find and take a bowl, then find and use a desklamp.\nOK.\n> think: First I need to find a bowl. A bowl is more likely to appear in drawer shelf (1-5), drawer (1-3). I can check one\nby one, starting with drawer 1.\nOK.\n> go to shelf 1\nOn the shelf 1, you see nothing.\n> go to shelf 2\nOn the shelf 2, you see a bowl 1.\n> think: Now I find a bowl (1). Next, I need to take it.\nOK.\n> take bowl 1 from shelf 2\nYou pick up the bowl 1 from the shelf 2.\n> think: Now I take a bowl (1). Next, I need to find a desklamp. A desklamp is more likely to appear in desk (1), sidetable\n(1-2), and drawer (1-3). I can check one by one, starting with desk 1.\nOK.\n> go to desk 1\nOn the desk 1, you see a alarmclock 2, a book 1, a cellphone 1, a keychain 1, a laptop 2, a laptop 1, and a pen 3.\n> go to sidetable 1\nOn the sidetable 1, you see a cd 1, a pen 1, and a pencil 1.\n> go to sidetable 2\nOn the sidetable 2, you see a alarmclock 1, a desklamp 1, and a pen 2.\n> think: Now I find a desklamp (1). Next, I need to use it.\nOK.\n> use desklamp 1\nYou turn on the desklamp 1.\n> think: Task completed!\nALFWorld Planner Prompt\nHere are some examples.\nYou are in the middle of a room. Looking quickly around you, you see a desk 1, microwave 1, a cabinet 3, a cabinet 9, a\ndrawer 2, a coffeemachine 1, a stoveburner 4, a drawer 5, a cabinet 11, a drawer 3, a stoveburner 1, a drawer 1, a\ntoaster 1, a fridge 1, a stoveburner 2, a cabinet 6, a cabinet 10, a countertop 1, a cabinet 13, a cabinet 7, a\ngarbagecan 1, a cabinet 2, a cabinet 8, a cabinet 12, a drawer 4, a cabinet 1, a sinkbasin 1, a cabinet 5, a\nstoveburner 3, and a cabinet 4.\nGoal: Put a mug in/on desk.\nCome up with an abstract plan to perform this task in a couple of steps.\n# Think: To perform this task, I need to find and take mug and then put it on desk. First, I will focus on finding mug.\nStep 1: Find and take mug\n# Think: Now that I am carrying mug, I will focus on putting it in/on desk.\nStep 2: Put mug in/on desk\nExecution Order: (Step 1 AND Step 2)\nGoal: Clean mug and put it in/on desk.\nCome up with an abstract plan to perform this task in a couple of steps.\n# Think: To perform this task, I need to find and take mug, clean it, and then put it on desk. First, I will focus on\nfinding mug.\nStep 1: Find and take mug\n# Think: Now that I am carrying mug, I will focus on cleaning it.\nStep 2: Clean mug with sinkbasin\n# Think: Now that I have cleaned mug, I will focus on putting it in/on desk.\nStep 3: Put cleaned mug in/on desk\nExecution Order: (Step 1 AND Step 2 AND Step 3)\nGoal: Cool mug and put it in/on desk.\nCome up with an abstract plan to perform this task in a couple of steps.\n# Think: To perform this task, I need to find and take mug, cool it, and then put it on desk. First, I will focus on\nfinding mug.\nStep 1: Find and take mug\n# Think: Now that I am carrying mug, I will focus on cooling it.\nStep 2: Cool mug with fridge\n# Think: Now that I have cooled mug, I will focus on putting it in/on desk.\nStep 3: Put cooled mug in/on desk\nExecution Order: (Step 1 AND Step 2 AND Step 3)\nGoal: Heat mug and put it in/on desk.\nCome up with an abstract plan to perform this task in a couple of steps.\n# Think: To perform this task, I need to find and take mug, heat it, and then put it on desk. First, I will focus on\nfinding mug.\nStep 1: Find and take mug\n# Think: Now that I am carrying mug, I will focus on heating it.\nStep 2: Heat mug with microwave\n# Think: Now that I have heated mug, I will focus on putting it in/on desk.\nStep 3: Put heated mug in/on desk\nExecution Order: (Step 1 AND Step 2 AND Step 3)\nGoal: Look at mug under desklamp.\nCome up with an abstract plan to perform this task in a couple of steps.\n# Think: To perform this task, I need to find and take mug, and then go to the desklamp and use it. First, I will focus on\nfinding mug.\nStep 1: Find and take mug\n# Think: Now that I have found and taken mug, I will focus on using the desklamp.\nStep 2: Use the desklamp\nExecution Order: (Step 1 AND Step 2)\nGoal: Find and take mug\nCome up with an abstract plan to perform this task in a couple of steps.\n# Think: To perform this task I need to find mug in the room. mug is likely to be in desk, cabinet, countertop, or drawer.\nNow I will focus on finding mug in each of these locations one by one.\nStep 1: Find and take mug from desk\n# Think: If mug not found so far, I will next look in the cabinet.\nStep 2: Find and take mug from cabinet\n# Think: If mug not found so far, I will next look in the countertop.\nStep 3: Find and take mug from countertop\n# Think: If mug not found so far, I will next look in the drawer.\nStep 4: Find and take mug from drawer\nExecution Order: (Step 1 OR Step 2 OR Step 3 OR Step 4)\nHere is the goal.\n<room>\nGoal: <task>.\nCome up with an abstract plan to perform this task in a couple of steps. Constraints: The robot can hold/take/put only one\nobject at a time to a location.\nEnsure each step can be understood independently and mentions the name of object.\nWhen stating the execution order, ensure that 'AND'/'OR' statements are properly nested using brackets '()'.\nWebShop Executor Prompt: Buy\nInstruction: Buy product [B078GWRC1J] that matches criteria: 3 ounce bottle of bright citrus deodorant for sensitive skin,\nand price lower than 50.00 dollars\nAction: think['I should be on the search page where I can see a list of products, I should verify that.']\nObservation: OK.\nAction: load\nObservation:\n[Back to Search]\nPage 1 (Total results: 50)\n[Next >]\n[B078GWRC1J]\nBright Citrus Deodorant by Earth Mama | Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-\nOunce\n$10.99\n[B08KBVJ4XN]\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar &\nPatchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)\n$35.95\n[B078GTKVXY]\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n$10.99\nAction: think['I can spot product [B078GWRC1J] and it is a clickable link since its enclosed in square brackets. I should\nclick on it.]\nObservation: OK.\nAction: click[B078GWRC1J]\nObservation:\n[Back to Search]\n[< Prev]\nscent [assorted scents][bright citrus][calming lavender][ginger fresh][simply non-scents]\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2-pack)]\nBright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\nPrice: $10.99\nRating: N.A.\n[Description]\n[Features]\n[Reviews]\n[Buy Now]\nAction: think[I'm already on the product page. I see multiple possible options enclosed in '[]' for scent and size for this\nproduct. To match the input criteria, I should click on options '[bright citrus]' for scent and '[3 ounce (pack of 1)\n]' for size one by one and then buy in the end.]\nObservation: OK.\nAction: click[bright citrus]\nObservation: You have clicked bright citrus.\nAction: click[3 ounce (pack of 1)]\nObservation: You have clicked 3 ounce (pack of 1).\nAction: think[My task is to buy the product, for it should to click 'buy now']\nObservation: OK.\nAction: click[Buy Now]\nObservation: You have clicked buy now.\nAction: think[I finished buying the product. Task completed!]\nHere is another task in which you need to buy a product. When you finish buying the product with the most relevant choices,\nuse 'think[Task completed']. If you cannot find the matching options or proceed, think['Task failed']. Note that you\ncan only click on text enclosed in '[]' on the webpage. Everything else is only a description, not valid with the \"\nclick\" action.\nInstruction: Buy product [{}] that matches the criteria: {}\nWebShop Executor Prompt: Match (cont.)\nYou are given a webpage of an item on an online shopping website and a criteria. Your task is to answer if the product on\nthe page exactly matches the criteria. Not the criteria could have multiple requirements that should be checked one\nby one and all must satisfy for an exact match.\nHere are a few examples:\nCriteria: 3 ounce bottle of citrus deodorant for sensitive skin that is priced lower than $30 and natural.\nItem Page:\n[Back to Search]\n[< Prev]\nscent [assorted scents][bright citrus][calming lavender][ginger fresh][simply non-scents]\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2-pack)]\nBright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\nPrice: $10.99\nRating: N.A.\n[Description]\nFeatures:\nNEW from Earth Mama (formerly Earth Mama Angel Baby), formulated especially for pregnancy, breastfeeding and sensitive\nskin\nContains organic grapefruit, tangerine and calendula\nNO propylene glycol, artificial fragrance, parabens or aluminum\nDermatologist tested and clinically tested for irritation\nBetter than natural organic! NSF/ANSI 305 Certified by Oregon Tilth\n[Reviews]\n[Attributes]\n[Buy Now]\nAnswer: The product is available in 3 ounce size, is citrus and suitable for sensitive skin. It is also organic or natural.\nIts price is $10.99 which is less than $30.\nThus, the answer is True (exact match).\nCriteria: 3 ounce bottle of citrus deodorant for sensitive skin that is priced lower than $30 and natural.\nItem Page:\n[Back to Search]\n[< Prev]\nsize [3 ounce][3 ounce (pack of 1)]\nunit count [2.0][3.0]\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar &\nPatchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)\nPrice: $15.95\nRating: N.A.\n[Description]\nFeatures:\nAbout this item WHY ALUMINUM-FREE DEODORANT? Aluminum-free deodorants use more natural ingredients unlike antiperspirants,\nwhich use chemicals to block sweat. Safely fight odor for 24 hours with Barrel & Oak's deodorantsour gentle formula\nis easy on sensitive skin. START SMELLING LIKE THE MAN YOU WANT TO BE: Our mountain sage aluminum-free men's\ndeodorant is naturally fragranced with an outdoorsy scent of crisp conifer, sage, & citrus. Think sweet notes of\ncitrus with earthy tones of cedar & patchouli. PREMIUM INGREDIENTS FOR NATURAL FRAGRANCES: Our deodorants for men are\ncomposed of natural, essential oil-based scents. These natural fragrance deodorants are more subtle than their\nsynthetic counterparts, but they're better for you & the planet. DESIGNED FOR THE MODERN MAN: Barrel & Oak has a full\nspectrum of grooming & body care products that are designed with function, fragrance, & effective ingredients for\nthe health-conscious & practical modern man. Give your body what it deserves. EARTH-FRIENDLY, YOU-FRIENDLY, WALLET-\nFRIENDLY: Our premium products for men are scented with natural fragrances & essential oils, free of parabens,\nphthalates, & SLS, packaged in recyclable materials, cruelty-free, & vegan or vegetarian.\n[Reviews]\n[Attributes]\n[Buy Now]\nAnswer: The product is not citrus in nature. It does not match the criteria. It's price is $15.95 which is less than $30.\nThus, the answer is False (not an exact match).\nNow here is the criteria and item page for the another task. Try you best to determine exact match, otherwise, respond with\n\"False\", i.e., no exact match. Generate an explanation before the answer to justify your decision.\nCriteria: {}\nItem Page:\n{}\nAnswer:\nWebShop Executor Prompt: Shortlist (cont.)\nYou are given a search page on an online shopping site with a list of products along with name and price. Based on this\ninformation, your task is return a list of product IDs (enclosed in []) of all products that exactly match all\nrequirements in the criteria. If the information provided is not enough to make a determination, return an empty list.\nHere are a few examples.\nSearch Page:\n[Back to Search]\nPage 1 (Total results: 50)\n[Next >]\n[B078GWRC1J]\nBright Citrus Deodorant by Earth Mama | Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-\nOunce\n$10.99\n[B08KBVJ4XN]\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar &\nPatchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)\n$35.95\n[B078GTKVXY]\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n$10.99\n[B08SMG4WB9]\nEach & Every 2-Pack Natural Aluminum-Free Deodorant for Sensitive Skin with Essential Oils, Plant-Based Packaging (Citrus &\nVetiver, 2.5 Ounce (Pack of 2))\n$25.0\n[B08KVCCSD6]\nEach & Every 3-Pack, Natural Aluminum-Free Deodorant for Sensitive Skin Made with Essential Oils, 2.5 Oz. (Lavender & Lemon,\nCitrus & Vetiver, and Coconut & Lime)\n$35.0\nCriteria: less than 5 ounce citrus deodorant sensitive skin, price less than $30.\nAnswer: My requirements are 5 ounce, citrus deodrant, suitable for sensitive skin, and price less than $30. Looks like this\ninformation is available on the search page, so I can proceed.\nProducts B078GWRC1J, B08SMG4WB9 look suitable as they are less than 5 ounce, citrus and have price 10.99 and $25 less than\n$30. Thus, shortlisted IDs are shortlisted=['B078GWRC1J', 'B08SMG4WB9']\nCriteria: less than 5 ounce citrus deodorant sensitive skin, cruelty free.\nAnswer: My requirements are 5 ounce, citrus deodrant, suitable for sensitive skin, and cruelty-free. Since there is no\ninformation about cruelty free on the search page, I cannot proceed. Task failed!\nHere is another task with a different search page and criteria. List all the product ids (enclosed in []) from the search\npage that match ALL the requirements in the criteria. Name this list shortlisted. If you cannot make the\ndetermination about even 1 sub-criteria, do not make a guess, output \"task failed!\". Generate an explanation before\nthe answer to justify your decision.\nSearch Page:\n{}\nCriteria: {}\nAnswer:\nWebShop Planner Prompt\nWrite an abstract plan to successfully complete the goal. In each step of the plan mention which module (including\narguments) that need to be called. Learn from and incorporate information from previous runs, e.g. do not repeat\npreviously successful or unsuccesful commands. Here are some examples:Information from previous run: -\nGoal: Buy 3 ounce bottle of citrus deodorant for sensitive skin, that is natural and priced less than 50.00 dollars.\n# Think: Based on the criteria and the search bar, I should query 3 ounce citrus deodorant sensitive skin. I have the\nfollowing constraints: natural and price lower than $30 which I can use to narrow down search results.\nStep 1: Search[3 ounce citrus deodorant sensitive skin]\n# Think: Now I will need to narrow down the search results for price lower than $30 and natural\nStep 2: SimpleMatch[3 ounce citrus deodorant sensitive skin with price lower than $50 and natural]\n# Think: Since it returns a list of up to 3 products, I will pick the first suitable product. For now, Ill denote its id as\nprod_id for placeholder.\nStep 3: Buy[prod_id, \"3 ounce bottle of citrus deodorant for sensitive skin, that is natural and priced less than 30.00\ndollars\"]\n#Think: My plan requrires all these steps to succeed sequentially, so I will use the \"AND\" operator.\nExecution Order: (Step 1 AND Step 2 AND Step 3)\nInformation from previous run:\n- Unable to get matching product using: SimpleMatch[3 ounce citrus deodorant sensitive skin with price lower than $30 and\nnatural]\n- Search results page:\n[Back to Search]\nPage 1 (Total results: 50)\n[Next >]\n[B078GWRC1J]\nBright Citrus Deodorant by Earth Mama | Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-\nOunce\n$10.99\n[B08KBVJ4XN]\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar &\nPatchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)\n$35.95\n[B078GTKVXY]\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n$10.99\n[B08SMG4WB9]\nEach & Every 2-Pack Natural Aluminum-Free Deodorant for Sensitive Skin with Essential Oils, Plant-Based Packaging (Citrus &\nVetiver, 2.5 Ounce (Pack of 2))\n$25.0\n[B08KVCCSD6]\nEach & Every 3-Pack, Natural Aluminum-Free Deodorant for Sensitive Skin Made with Essential Oils, 2.5 Oz. (Lavender & Lemon,\nCitrus & Vetiver, and Coconut & Lime)\n$35.0\n[B087WKSR2G]\nGoal: Narrow down search results for 3 ounce bottle of citrus deodorant for sensitive skin that is priced lower than $30\nand natural. You cannot search again.\n#Think: Based on the search results and previous information, SimpleMatch failed because my criteria was too complex. Price\nconstraint is easy to verify, I will narrow down based on that first then examine in detail for natural constraint\n#Think: Based on price, I narrow down my search to B078GWRC1J, B08SMG4WB9 as they look suitable. These are on my shortlist\nto examine the natural constraint in detail one by one.\nStep 1: DetailMatch[B078GWRC1J, 3 ounce bottle of for sensitive skin, that is natural and priced less than 30.00 dollars]\nStep 2: DetailMatch[B08SMG4WB9, 3 ounce bottle of citrus deodorantcitrus deodorant for sensitive skin, that is natural and\npriced less than 30.00 dollars]\n#Think: If none of the products exactly match my criteria, I will search again with a new query that includes the natural\ncriteria too. This ensures my plan is compelete.\nStep 3: Search[3 ounce citrus deodrant natural and sensitive skin]\n#Think: Since these steps are linked by an if condition, I only need one of them to succeed. I will connect them using the\n\"OR\" operator.\nExecution Order: (Step 1 OR Step 2 OR Step 3)\nHere is a new goal. Write an abstract plan to successfully complete the goal. In each step of the plan mention which module\n(including arguments) that need to be called. Learn from and incorporate information from previous runs, e.g. do not\nrepeat previously successful or unsuccesful commands. In the end, output the intended execution order.\nInformation from previous run: {}\nGoal: {}\n"
  },
  {
    "title": "PolyMaX: General Dense Prediction with Mask Transformer",
    "link": "https://arxiv.org/pdf/2311.05770.pdf",
    "upvote": "6",
    "text": "PolyMaX: General Dense Prediction with Mask Transformer\nXuan Yang* Liangzhe Yuan\nKimberly Wilber\nAstuti Sharma\nXiuye Gu\nSiyuan Qiao\nStephanie Debats\nHuisheng Wang\nHartwig Adam\nMikhail Sirotenko\nLiang-Chieh Chen\u2020\nGoogle Research\nAbstract\nDense prediction tasks, such as semantic segmentation,\ndepth estimation, and surface normal prediction, can be eas-\nily formulated as per-pixel classification (discrete outputs)\nor regression (continuous outputs). This per-pixel prediction\nparadigm has remained popular due to the prevalence of\nfully convolutional networks. However, on the recent fron-\ntier of segmentation task, the community has been witness-\ning a shift of paradigm from per-pixel prediction to cluster-\nprediction with the emergence of transformer architectures,\nparticularly the mask transformers, which directly predicts\na label for a mask instead of a pixel. Despite this shift, meth-\nods based on the per-pixel prediction paradigm still domi-\nnate the benchmarks on the other dense prediction tasks that\nrequire continuous outputs, such as depth estimation and sur-\nface normal prediction. Motivated by the success of DORN\nand AdaBins in depth estimation, achieved by discretizing\nthe continuous output space, we propose to generalize the\ncluster-prediction based method to general dense prediction\ntasks. This allows us to unify dense prediction tasks with\nthe mask transformer framework. Remarkably, the resulting\nmodel PolyMaX demonstrates state-of-the-art performance\non three benchmarks of NYUD-v2 dataset. We hope our\nsimple yet effective design can inspire more research on ex-\nploiting mask transformers for more dense prediction tasks.\nCode and model will be made available1.\n1\nIntroduction\nEntering the deep learning era [38, 49, 86], enormous\nefforts have been made to tackle dense prediction problems,\nincluding but not limited to, image segmentation [15\u201317,\n65, 116], depth estimation [29, 32, 51, 71, 80, 81], surface\nnormal prediction [28,47,69,95], and others [1,23,27,59,\n70, 112]. Early attempts formulate these problems as per-\npixel prediction (i.e., assigning a predicted value to every\n*Corresponding to xuanyang@google.com.\n\u2020Work done while at Google. Now at ByteDance.\n1https://github.com/google-research/deeplab2\nsemantic \nsegmentation\ndepth \nestimation\nnormal \nestimation\nModel\nimage\ninput queries\nFigure 1. Cluster-prediction forms the foundation of our uni-\nfied Mask-Transformer-based framework PolyMaX for dense\nprediction tasks. In the cluster-prediction paradigm, the model\nlearns to transform input queries to cluster centers, where each\ncluster center learns to group similar pixels together. Pixels of the\nsame group are assigned to the same label. This paradigm works\nwell for discrete output space, such as segmentation. To extend\nit for continuous and even high-dimensional output space (e.g.,\ndepth estimation and surface normal), we dynamically partition the\noutput space into clusters, dependent on the input image.\npixel) via fully convolutional networks [65]. Specifically,\nwhen the desired prediction of each pixel is discrete, such\nas image segmentation, the task is constructed as per-pixel\nclassification, while other tasks whose target outputs are\ncontinuous, such as depth and surface normal, are instead\ncast as per-pixel regression problems.\nRecently, a new paradigm for segmentation tasks is\ndrawing attention because of its superior performance com-\npared to previous per-pixel classification approaches. In-\nspired by the object detection network DETR [13], MaX-\nDeepLab [91] and MaskFormer [22] propose to classify each\nsegmentation mask as a whole instead of pixel-wise, by ex-\ntending the concept of object queries in DETR to represent\nthe clusters of pixels in segmentation masks. Specifically,\nwith the help of pixel clustering via conditional convolu-\ntions [42,89,97], these Mask-Transformer-based works em-\narXiv:2311.05770v1  [cs.CV]  9 Nov 2023\nploy transformer decoders [90] to convert object queries to a\nset of (mask embedding vector, class prediction) pairs, which\nfinally yield a set of binary masks by multiplying the mask\nembedding vectors with the pixel features. Such methods are\neffective when the target domain is discretely quantized (e.g.,\none semantic label is encoded by one integer scalar, as in\nimage segmentation). However, it is unclear how this frame-\nwork can be generalized to other dense prediction tasks,\nwhose outputs are continuous or even multi-dimensional,\nsuch as depth estimation and surface normal prediction.\nOn the contrary, in the field of depth estimation,\nDORN [32] demonstrates the potential of discretizing the\ncontinuous depth range into a set of fixed bins, and Ad-\naBins [7] further adaptively estimates the bin centers, de-\npendent on the input image. The continuous depth values\nare then estimated by linearly combining the bin centers.\nPromising results are achieved by jointly learning the bin\ncenters and performing per-pixel classification on those bins.\nThis insight \u2013 of using classification to perform dense pre-\ndiction in a continuous domain \u2013 opens up the possibility of\nperforming many other continuous dense prediction tasks\nwithin the Mask-Transformer-based framework, a powerful\ntool for discrete value predictions.\nConsequently, a few natural questions emerge: Can we ex-\ntend these Mask-Transformer-based frameworks even further,\nto solve more continuous dense prediction tasks? Can the\nresulting framework generalize to other multi-dimensional\ncontinuous domain, e.g., surface normal estimation? In this\nwork, we provide affirmative answers to those questions by\nproposing a new general architecture for dense prediction\ntasks. Specifically, building on top of the insight from [7,32],\nwe generalize the mask transformer framework [109] to mul-\ntiple dense prediction tasks by using the cluster centers (i.e.,\nobject queries) as the intermediate representation. We evalu-\nate the resulting model, called PolyMaX, on the challenging\nNYUD-v2 [71] and Taskonomy [112] datasets. Remarkably,\nour simple yet effective approach demonstrates new state-\nof-the-art performance on semantic segmentation, depth es-\ntimation, and surface normal prediction, without using any\nextra modality as inputs (e.g., multi-modal RGB-D inputs\nas in CMNeXt [115]), heavily pretrained backbones (e.g.,\nStable-Diffusion [82] as in VPD [117]) or complex pretrain-\ning schemes (e.g., a mix of 12 datasets as in ZoeDepth [9]).\nOur contributions are summarized as follows:\n\u2022 We propose PolyMaX, which generalizes dense pre-\ndiction tasks with a unified Mask-Transformer-based\nframework. We take surface normal as a concrete exam-\nple to demonstrate how general dense prediction tasks\ncan be solved by the proposed framework.\n\u2022 We evaluate PolyMaX on NYUD-v2 and Taskonomy\ndatasets, and it sets new state-of-the-arts on multi-\nple benchmarks on NYUD-v2, achieving 58.08 mIoU,\n0.250 root-mean-square (RMS) error and 13.09 mean\nerror on semantic segmentation, depth estimation and\nsurface normal prediction, respectively.\n\u2022 We further perform the scalability study, which demon-\nstrates that PolyMaX scales significantly better than the\nconventional per-pixel regression based methods as the\npretraining data increases.\n\u2022 Lastly, we provide the high-quality pseudo-labels of\nsemantic segmentation for Taskonomy dataset, aiming\nto compensate for the scarcity of existing large-scale\nmulti-task datasets and facilitate the future research.\n2\nRelated Work\nDense Prediction in the Discrete Domain\nImage seg-\nmentation partitions images into multiple segments by their\nsemantic classes.\nFor example, fully convolutional net-\nworks [66] train semantic segmentation in an end-to-end\nmanner mapping pixels into their classes [53, 61, 62, 110].\nAtrous convolution increases the network receptive field for\nsemantic segmentation without additional learnable parame-\nters [15\u201317,19,78,94,105]. Many state-of-the-art methods\nuse an encoder-decoder meta architecture for stronger global\nand local information integration [3,18,21,33,36,44,73,88,\n92,107,111].\nDense Prediction in the Continuous Domain\nUn-\nlike image segmentation that produces discrete predic-\ntions, depth and surface normal estimation expect the mod-\nels to predict continuous values [29, 76]. Most early at-\ntempts [50, 60, 78, 102] solve it as a standard regression\nproblem. To alleviate training instability, DORN [32] pro-\nposes treating the problem as classification by discretizing\nthe continuous depth into a set of pre-defined intervals (bins).\nAdaBins [7] adaptively learns the depth bins, conditioned\non the input samples. LocalBins [8] further learns the depth\nrange partitions from local regions instead of global distri-\nbution of depth ranges. BinsFormer [56] views adaptive\nbins generation as a direct set prediction problem [14]. The\ncurrent state-of-art on depth estimation is set by VPD [117],\nwhich employs the Stable-Diffusion [82] pretrained with\nthe large-scale LAION-5B [85] dataset. Among the non-\ndiffusion-based models, ZoeDepth [9] shows the strongest\ncapability by pretraining on 12 datasets with relative depth,\nand then finetuning on two datasets with metric depth.\nSurface Normal Prediction\nDespite having a contin-\nuous output domain like depth estimation, the surface nor-\nmal problem remains under-explored for discretization ap-\nproaches. Most prior works on surface normal estimation\nfocus on improving the loss [4,25,58], reducing the distri-\nbution bias [4] and shift [25], and resolving the artifacts in\nground-truth by leveraging other modalities [40,77]. Until\nrecently, iDisc [75], a method based on discretizing inter-\nnal representations, shows promise for depth estimation and\nsegmentation \nprediction\ndepth \nprediction\ndepth estimation\nMLPs\nper-pixel \nembeddings\npixel encoder/\ndecoder\ntransformer \ndecoder\n\u2297\ninput \nqueries\neq. (2)\neq. (2)\nargmax\ncluster \ncenters\n1D bin\ncenters\nsemantic segmentation\ncluster \ncenters\nnormal \nprediction\ncluster \ncenters\nsurface normal prediction\n3D unit \nvectors\neq. (2)\n\u2297\n{       }\nFigure 2. The proposed model PolyMaX unifies dense prediction tasks with the mask transformer framework, where cluster centers\nare used as an intermediate representation. The mask transformer framework contains two paths: (1) pixel encoder/decoder to generate\nper-pixel embeddings, and (2) transformer decoder to generate cluster centers (i.e., object queries) from input queries. As shown in Eq. (2),\nthe probability distribution map is generated by first multiplying the per-pixel embeddings with the cluster centers, followed by a softmax\noperation. For semantic segmentation, the final segmentation map is obtained by applying the argmax operation to find the maximum\nprobability semantic label. For depth estimation, we further apply another MLP (omitted in the Figure) to estimate the bin center values,\nwhich are then linearly combined with the probability distribution map to form the final depth map. Similarly, for surface normal prediction,\nthe estimated 3D unit vectors are linearly combined with the probability distribution map to form the final surface normal map. N denotes\nthe linear combination operation.\nsurface normal prediction. However, it differs from depth\nestimation discretization methods by enforcing discretiza-\ntion in the feature space rather than the target output space.\nWe are more interested in the latter approach, as it is more\ngeneralizable to other dense prediction tasks.\nOther Efforts to Unify Dense Prediction Tasks\nReal-\nizing segmentation, depth and surface normal are all pixel-\nwise mapping problem, previous works [10,52,67,72,96]\nhave deployed them to the same framework. Some works [11,\n12, 103, 104, 106] improve performance by exploring the\nrelations among dense prediction tasks. UViM [48] and\nPainter [96] unify segmentation and depth estimation, but\nneither of them is based on discretizing the continuous out-\nput space, and neither considers surface normal estimation.\nBy contrast, we focus on a complementary perspective: a\nunified architecture for dense prediction that models both\ndiscrete and continuous tasks, covering image segmentation,\ndepth estimation, and surface normal prediction.\nMask Transformer\nWitnessing the success of trans-\nformers [90] in NLP, the vision community has begun ex-\nploring them for more computer vision tasks [26]. Inspired\nby DETR [14] and conditional convolutions [42], MaX-\nDeepLab [91] and MaskFormer [22] switch the segmentation\nparadigm from per-pixel classification to mask classifica-\ntion, by introducing the mask transformer, where each input\nquery learns to correspond to a mask prediction together\nwith a class prediction. Several works [87, 100, 118] also\nintroduce transformer into semantic segmentation. Further\nimprovements are made on attention mechanism to enhance\nthe performance of mask transformers [20,55,120]. CMT-\nDeepLab [108], kMaX-DeepLab [109], and ClustSeg [57]\nreformulate the cross-attention learning in transformer as a\nclustering process. Our proposed PolyMaX builds on top\nof the cluster-based mask transformer architecture [109].\nRather than focusing on segmentation tasks, PolyMaX uni-\nfies dense prediction tasks (i.e., image segmentation, depth\nestimation, and surface normal estimation) by extending the\ncluster-based mask transformer to support both discrete and\ncontinuous outputs.\n3\nMethod\nIn this section, we first describe how segmentation [109]\nand depth estimation [7] can be transformed from per-pixel\nprediction problems to cluster-prediction problems (Sec. 3.1).\nWe then introduce our proposed method, PolyMaX, a new\nmask transformer framework for general dense predictions\n(Sec. 3.2). We take surface normal prediction as a concrete\nexample to explain how general dense prediction problems\ncan be reformulated in a similar fashion, allowing us to unify\nthem into the same mask transformer framework.\n3.1\nMask-Transformer-Based Segmentation\nand Depth Estimation\nCluster-Prediction Paradigm for Segmentation\nTwo\nearlier works, MaX-DeepLab [91] and MaskFormer [22],\ndemonstrate how to shift image segmentation from per-pixel\nclassification to the cluster-prediction paradigm. Along the\nsame direction, we follow the recently proposed clustering\nperspective [108,109], which casts object queries to cluster\ncenters. This paradigm is realized by the following two\nsteps:\n1. pixel-clustering:\ngroup pixels into K\nclusters,\nrepresented by segmentation masks {mi|mi\n\u2208\n[0, 1]H\u00d7W }K\ni=1, where H and W are height and width.\nNote that mi denotes soft segmentation masks.\ninput\ncluster 0\n1D depth \nbin centers\n1\n2\n 3\n\u2297\nFigure 3. For the task of depth prediction, we illustrate one\nexample with four depth bins (i.e., K = 4). The probability dis-\ntribution map (i.e., output of Eq. (2)) has shape HW \u00d74 (generated\nby four cluster centers), which represents the predicted probability\ndistribution over 4 bins for each pixel. As shown in the figure, the\nfirst cluster center (red) learns to highlight closest pixels; the second\nand third clusters (green/blue) highlight mid-range pixels, and the\nfourth (yellow) cluster highlights the furthest pixels. These maps\nare linearly combined (denoted as N) with their corresponding 1D\nbin centers to produce the final depth map prediction.\n2. cluster-classification: assign semantic label to each\ncluster with a probability distribution over C classes.\nThe ith cluster\u2019s probability distribution is a 1D vector,\ndenoted as pi, where {pi|pi \u2208 [0, 1]C, PC\nc=1 pi,c =\n1}K\ni=1.\nThese clusters and probability distributions are jointly\nlearned to predict the output S, a set of K cluster-probability\npairs:\nS = {(mi, pi)}K\ni=1\n(1)\nThis cluster-prediction paradigm is general for semantic [39],\ninstance [37], and panoptic [46] segmentation. When only\nhandling semantic segmentation, the framework can be fur-\nther simplified by setting K = C (i.e., number of clusters\nis equal to number of classes) which has a fixed matching\nbetween cluster centers (i.e., object queries) and semantic\nclasses. We adopt this simplification, since the datasets we\nexperimented with only support semantic segmentation.\nCluster-Prediction Paradigm for Depth Estimation\nAt first glance, depth estimation seems incompatible with\nthis cluster-prediction paradigm because of its continuous\nnature. However, recent works [7, 32] propose promising\nsolutions by dividing the continuous depth range into K\nlearnable bins and regarding the task as a classification prob-\nlem. By this means, the depth estimation task fits neatly\ninto the above cluster-classification paradigm. Specifically,\nthe pixel-clustering step outputs the range attention map\n{ri|ri \u2208 [0, 1]H\u00d7W }K\ni=1, which (after softmax) represents\nthe predicted probability distribution over the K bins for\neach pixel.\nWhile the cluster-prediction step estimates\nbin centers {bi}K\ni=1, adaptively discretizing the continuous\ndepth range for each image. Here, K controls the granularity\nof the depth range partition. Therefore, the output of depth\nestimation can be expressed as {(ri, bi)}K\ni=1, sharing the\nsame formulation as segmentation (Eq 1). The final depth\nprediction is then generated by a linear combination between\nthe depth values of bin centers and the range attention map.\nMask Transformer Framework\nAfter describing the\ncluster-prediction paradigm for both segmentation and depth\ntarget domain\noutput space R\nDT\nsegmentation\ndiscrete\n1\ndepth estimation\ncontinuous\n1\nsurface normal\ncontinuous\n3\nTable 1. General dense prediction learns the pixel mapping\nfrom the input RGB space to the structured output space R\nwith different dimensionalities DT .\nestimation, we now explain how to integrate them to the\nmask transformer framework. In the framework, the K clus-\nter centers are learned from the K input queries through\nthe transformer-decoder branch (blue block in Fig. 2). It\nassociates the clusters, represented as query embedding\nvectors, with the pixel features extracted from the pixel\nencoder-decoder path [83] (pink block in Fig. 2) through\nself-attention and cross-attention blocks. The aggregated\ninformation is gradually refined as the pixel features go from\nlower-resolution to higher-resolution. The probability distri-\nbution map is then generated from the following equation:\nsoftmax\nK\n(F \u00d7 QT ),\n(2)\nwhere F \u2208 RHW \u00d7D and Q \u2208 RK\u00d7D denote the final pixel\nfeatures (i.e., per-pixel embeddings) and cluster centers (i.e.,\nobject queries). D is the channel dimension of pixel features\nand queries. For segmentation, the probability distribution\nmap corresponds to the segmentation masks {mi}, while\nfor depth estimation, it becomes the range attention map\n{ri}. Finally, another Multi-Layer Perceptrons (MLPs) are\nadded on top of the cluster centers Q to predict the proba-\nbility distribution pi or bin centers bi for segmentation and\ndepth estimation, respectively. Consequently, the cluster\ncenters in the mask transformer are used as an intermediate\nrepresentation in both segmentation and depth estimation.\n3.2\nMask-Transformer-Based General Dense\nPrediction\nGeneral dense prediction tasks can be expressed as fol-\nlows:\nT : RH\u00d7W \u00d73 \u2192 RH\u00d7W \u00d7DT , DT \u2208 N\n(3)\nAs presented in the above formulation, the objective of\ngeneral dense prediction is to learn the pixel mapping T\nfrom the input RGB space to the structured output space\nR with different dimensionalities (DT ) and topologies (out-\nput space R is either discrete or continuous). For instance,\nsegmentation and depth estimation task correspond to the\nsingle-dimensional (DT = 1) output space of discrete and\ncontinuous topology, respectively (Tab. 1). Given that the\ncluster-prediction methods have been effectively applied to\nthese tasks (as described in Sec. 3.1), we take a step further\nand study how general dense prediction tasks, particularly\nmulti-dimensional continuous target space (e.g., surface nor-\nmal), can be learned by the cluster-prediction approach.\nTask of Surface Normal\nSurface normal is a represen-\ntative dense prediction task that targets a multi-dimensional\n\u2297\ninput\ncluster 0\n1\n2\n3\n3D unit \nvectors\nFigure 4. To illustrate the task of surface normal prediction, we\nshow an example model with four cluster centers that roughly\ncorrespond to cardinal directions (3-vectors on the unit ball). Each\nchannel of the HW \u00d7 4 probability distribution map represents the\npredicted probability of the corresponding vector direction. These\nmaps are linearly combined (denoted as N) with their correspond-\ning 3D unit vectors to generate the final surface normal prediction.\ncontinuous output space, in which case DT = 3 and output\nspace R is continuous. This task aims at predicting, for each\npixel, the direction that is perpendicular to the tangent plane\nof the point in a 3D world, which can be expressed by a 3D\nunit vector v = [Nx, Ny, Nz], where N 2\nx + N 2\ny + N 2\nz = 1.\nCluster-Prediction for Surface Normal\nFrom the ge-\nometric perspective, a surface normal lies on the surface of\na unit 3D ball. Therefore, the problem essentially becomes\nlearning the pixel mapping T from the 2D image space to\nthe 3D point on the unit ball surface. The geometric meaning\nof the surface normal prediction allows us to naturally adopt\nthe clustering-prediction approach.\nSimilar to [7,32], we discretize the surface normal output\nspace into K pieces of partitioned 3D spheres, converting\nthe problem to classification among the K pieces. We refer\nthe partitioned sphere as sphere segments for simplicity 2,\nwhich can be viewed as clusters of 3D points on the unit\nball surface, and thus are likely to have irregular shapes.\nFortunately, with such a simplification, we can adopt the\nsame paradigm as segmentation and depth estimation to\ntackle surface normals, where pi can be regarded as the\nprobability over the K sphere segments. We jointly learn\nthe center coordinates of sphere segments with the 3D-point-\ncluster and probability distribution. The final surface normal\nis predicted as a linear combination of the center coordinates\nof K sphere segments and the range attention map.\nMask Transformer Framework for General Dense\nPrediction\nExtending the cluster-prediction paradigm to\ngeneral dense prediction tasks allows us to deploy and unify\nthem into the mask transformer framework described in\nSec. 3.1. Going from the single-dimension (DT = 1) to\nmulti-dimension (DT > 1), the major upgrade happens at\nthe place where the cluster center Q \u2208 RK\u00d7D is mapped\n(via MLPs) to RK\u00d7DT (DT > 1), instead of RK, when\npredicting DT -dimensional bin centers. For instance, in the\ncase of surface normal, each cluster center (i.e., object query)\npredicts a unit vector vi \u2208 R3.\nModel Instantiation\nWe build the proposed general\n2Note that here the sphere segment does not rigorously match the math-\nematical definition of spherical segment, which refers to the solid produced\nby cutting a sphere with a pair of parallel planes.\nframework on top of kMaX-DeepLab [109] with the official\ncode-base [98]. We call the resulting model PolyMaX, a\nPolymath with Mask Xformer for general dense prediction\ntasks. The model architecture is illustrated in Fig. 2.\n4\nExperimental Results\nIn this section, we first provide the details of our exper-\nimental setup. We then report our main results on NYUD-\nv2 [71] and Taskonomy [112]. We also present visualizations\nto obtain deeper insights of the proposed cluster-prediction\nparadigm, followed by ablation studies.\n4.1\nExperimental Setup\nDataset\nWe focus on semantic segmentation, monoc-\nular depth estimation and surface normal tasks during ex-\nperiments, as they comprehensively represent the diverse\ntarget space of dense prediction (one-dimensional to multi-\ndimensional, and discrete to continuous). Specifically, we\nuse NYUD-v2 [71] dataset. The official release provides 795\ntraining and 654 testing images with real (instead of pseudo\nlabels) ground-truths of those three tasks.\nTo complement the small scale of NYUD-v2 dataset, we\nalso conduct experiments on the Taskonomy [112] dataset,\nwhich is composed of 4.6 million images (train: 3.4M, val:\n538K: test: 629K images) from 537 different buildings with\nindoor scenes. This dataset is originally developed to facil-\nitate the study of task transfer learning, thereby containing\nground-truths of various tasks, including semantic segmenta-\ntion, depth estimation, surface normal, and so on. The depth\nZ-buffer and surface normal ground-truth are programmati-\ncally computed from image registration and mesh alignment,\nresulting in high quality annotations. However, the provided\nsemantic segmentation annotations are only pseudo-labels\ngenerated by Li et al. [53], an out-dated model trained on\nCOCO [59]. After carefully examining these pseudo-labels,\nwe notice that their quality does not satisfy the require-\nment for evaluating or improving state-of-the-art models.\nTherefore, we adopt the kMaX-DeepLab [109] model with\nConvNeXt-L [64] backbone pretrained on COCO dataset\nto regenerate the pseudo-labels for semantic segmentation.\nWe visually compare both labels in Fig. 5. The Taskonomy\ndataset enhanced by the new pseudo labels can serve as a\nmeaningful complementary to this research field, particularly\ngiven the scarcity of the available large-scale high-quality\nmulti-task dense prediction datasets. We will release the\nhigh-quality pseudo labels to facilitate future research3\nEvaluation Metrics\nSemantic segmentation is evalu-\nated with mean Intersection-over-Union (mIoU) [30]. For\ndepth estimation, the metrics are root mean square error\n(RMS), Absolute mean relative error (A.Rel), absolute er-\nror in log-scale (Log10) and pixel inlier ratio (\u03b4i) with error\nthreshold at 1.25i [29]. For surface normal metrics, fol-\nlowing [28,31] we use mean (Mean) and median (Median)\n3Will release at https://github.com/google-research/deeplab2\ninput\nours\noriginal\nFigure 5. Visualization of Taskonomy pseudo-labels: ours (mid-\ndle) vs. original ones by Li et al. [53] (bottom). Our pseudo-labels\ndemonstrate higher quality than the existing ones.\nabsolute error, RMS angular error, and pixel inlier ratio (\u03b41,\n\u03b42, \u03b43) with thresholds at 11.5\u25e6, 22.5\u25e6 and 30\u25e6, respectively.\nImplementation Details\nWe adopt the pixel en-\ncoder/decoder and the transformer decoder modules from\nthe kMaX-DeepLab [109]. Additional L2 normalization is\nneeded at the end of surface normal head to yield 3D unit\nvectors. During training, we adopt the same losses from\nkMaX-DeepLab for semantic segmentation. For depth esti-\nmation, when training on NYUD-v2 dataset, we use scale\ninvariant logarithmic error [29], relative squared error [34],\nfollowing ViP-DeepLab [78]. We also include the multi-\nscale gradient loss proposed by MegaDepth [54] to improve\nthe visual sharpness of the predicted depth map. While train-\ning depth estimation on Taskonomy, we switch to robust\nCharbonnier loss [6], since this dataset has maximum depth\nvalue 128m with more outliers. The loss function we apply to\nsurface normal is simply L2 loss, since it has better training\nstability than truncated angular loss [25]. The experiments\non NYUD-v2 are conducted by first pretraining PolyMaX on\nTaskonomy dataset, and then finetuning on NYUD-v2 train\nsplit. The finetuning step is unnecessary when evaluating on\nTaskonomy test split. The learning rate at the pretraining and\nfinetuning stages are 5e-4 and 5e-5, respectively. To ensure\nfair comparisons with previous works, we closely follow\ntheir experiment setup. However, due to the use of different\ntraining data splits for each of the three tasks in previous\nstudies, we have to train each task independently.\n4.2\nMain Results\nNYUD-v2\nIn Tab. 2, we compare PolyMaX with state-\nof-the-art models for dense prediction tasks on the NYUD-v2\ndataset. We group the existing models based on the number\nand type of the tasks they support on this dataset. We choose\nthe best numbers reported in the prior works. As shown in\nthe table, in such a competitive comparison, PolyMaX still\nsignificantly outperforms all the existing models.\nSpecifically, in the semantic segmentation task, most\nof the recent models use additional modalities such as\ndepth, but PolyMaX surpasses the existing best two models\nCMX [114] and CMNeXt [115] by 1.2% mIoU, without\nusing any additional modalities. In the depth estimation\ntask, the current best model VPD [117] is built upon Stable-\nDiffusion [82] and pretrained with LAION-5B dataset [85].\nDespite of only using a pretraining dataset (i.e., Taskon-\nomy) with 0.1% of that scale, PolyMaX achieves better\nperformance on all the depth metrics than VPD. Further-\nmore, PolyMaX breaks the close competition among non-\ndiffusion-based models by a meaningful improvement from\nabove 0.27 to 0.25 RMS (all the recent non-diffusion-based\nmodels achieve around 0.27 RMS), similarly for all the other\nmetrics. This is non-trivial, particularly given that the best\nnon-diffusion-based depth model ZoeDepth [9] uses the pre-\ntrained BeiT384-L [5] backbone and 2-stage training with a\nmixture of 12 datasets. On the surface normal benchmark,\nPolyMaX continues to outperform all the existing models by\na substantial margin, with the mean error being reduced from\nabove 14.60 to 13.09. Finally, when comparing with models\nthat support all three tasks on this dataset, the improvement\nof PolyMaX is further amplified for all metrics. Overall,\nPolyMaX is not only among the few models that support\nall the three tasks, but also sets a new state-of-the-art on all\nthree dense prediction benchmarks.\nTaskonomy\nIn Tab. 3, we report our results on Taskon-\nomy, along with a solid baseline4 DeepLabv3+ [19]. The per-\nformance of the same baseline model on NYUD-v2 bench-\nmarks in Tab. 2, compared with state-of-the-art models, can\njustify our choice of it as baseline on Taskonomy bench-\nmarks. On all three tasks, PolyMaX significantly outper-\nforms the baseline model. Specifically, the performance on\nsemantic segmentation, depth estimation and surface normal\nis improved by 8.3 mIoU, 0.13 RMS and 1.0 mean error,\nrespectively. These results further validate the effectiveness\nof the proposed framework.\nVisualizations and Limitations\nWe visualize the pre-\ndictions of PolyMaX on all the three dense preiction tasks\nwith their corresponding ground-truth in Fig. 6. As shown in\nthe top row, PolyMaX successfully resolves the fine-grained\ndetails and irregular object shapes, and predicts high quality\nresults. The bottom row shows a challenging case where\nPolyMaX can be further improved. Similar to many other\ndepth models, it is difficult to correctly infer the depth map\nwith glass, mirror and other reflective surfaces, due to the\nartifacts in the available ground-truth. Lastly, another di-\nrection for future improvement is the visual sharpness of\nthe predicted surface normal. We find that the multi-scale\ngradient loss proposed by MegaDepth [54] can effectively\nimprove the edge sharpness for depth estimation, and slightly\nimproves depth metrics (e.g., around 0.002 RMS improve-\nment). It is promising to adapt the loss to surface normal\nprediction to improve visual quality.\n4We notice recent works [10,11] also report numbers on this benchmark,\nbut with the unreleased code and different experimental setup, we can not\nconsider them here as fair baselines.\ninput\nsem seg pred.\nground-truth\ndepth pred.\nground-truth\nnormal pred.\nground-truth\nFigure 6. Visualization of model inputs and outputs for semantic segmentation, depth estimation and normal prediction. PolyMaX\nresolves fine details on scenes with complex structure (top row), and even performs acceptably when ground-truth is unreliable (last row).\nsem seg\ndepth estimation\nsurface normal\nmodel\nencoder\npretraining data\nmIoU \u2191\nRMS \u2193\nA.Rel \u2193\nLog10 \u2193\n\u03b41 \u2191\nMean \u2193\nMed \u2193\nRMS \u2193\n\u03b41 \u2191\nOMNIVORE [35]\nSwin-L [63]\nIN-1K [84], Kinetics [43], SUN [99]\n56.80\n-\n-\n-\n-\n-\n-\n-\n-\nCMX [114]\nMiT-B5 [100]\nIN-1K [84]\n56.90\n-\n-\n-\n-\n-\n-\n-\n-\nCMNeXt [115]\nMiT-B4 [100]\nIN-1K [84]\n56.90\n-\n-\n-\n-\n-\n-\n-\n-\nBinsFormer [56]\nSwin-L [63]\nIN-22K [84]\n-\n0.330\n0.094\n0.040\n0.925\n-\n-\n-\n-\nDINOv2 [74]\nViT-g [113]\nLVD-142M [74]\n-\n0.279\n0.091\n0.037\n0.950\n-\n-\n-\n-\nAiT-P [72]\nSwin-L [63]\nIN-22K [84]\n-\n0.275\n0.076\n0.033\n0.954\n-\n-\n-\n-\nZoeDepth [9]\nBEiT384-L [5]\nM12 [9]\n-\n0.270\n0.075\n0.032\n0.955\n-\n-\n-\n-\nDo et al. [25]\nResNeXt-101 [101]\nIN-1K [84]\n-\n-\n-\n-\n-\n16.20\n8.200\n25.30\n59.50\nBae et al. [4]\nMiT-B5 [100]\nIN-1K [84]\n-\n-\n-\n-\n-\n14.90\n7.500\n23.50\n62.20\nMultiMAE [2]\nViT-B [26]\nIN-1K [84]\n56.00\n-\n-\n-\n0.864\n-\n-\n-\n-\nPainter [96]\nViT-L [26]\nIN-1K [84]\n-\n0.288\n0.080\n-\n0.950\n-\n-\n-\n-\nMTFormer [103]\nSwin [63]\nIN-22K [84]\n50.04\n0.490\n-\n-\n-\n-\n-\n-\n-\nVPD [117]\nSD [82]\nLAION-5B [85], CLIP [79]\n-\n0.254\n0.069\n0.030\n0.964\n-\n-\n-\n-\nASNDepth [67]\nHRNet-48 [93]\nIN-1K [84]\n-\n0.377\n0.101\n0.044\n0.890\n20.00\n13.400\n-\n43.50\niDisc [75]\nSwin-L [63]\nIN-22K [84]\n-\n0.313\n0.086\n0.037\n0.940\n14.60\n7.300\n22.80\n63.80\nARTC [12]\nHRNet-48 [93]\nIN-1K [84]\n46.33\n0.536\n-\n-\n-\n20.18\n-\n-\n-\nInvPT [106]\nSwin-L [63]\nIN-22K [84]\n51.76\n0.502\n-\n-\n-\n19.39\n-\n-\n-\nInvPT [106]\nViT-L [26]\nIN-22K [84]\n53.56\n0.518\n-\n-\n-\n19.04\n-\n-\n-\nMQTrans. [104]\nSwin-L [63]\nADE20K [119]\n54.84\n0.533\n-\n-\n-\n19.69\n-\n-\n-\nDeepLabv3+ [19]\u2020\nResNet-50 [38]\nIN-1K [84], Taskonomy [112]\n50.14\n0.375\n0.108\n0.046\n0.887\n14.81\n8.291\n22.49\n61.44\nPolyMaX\nResNet-50 [38]\nIN-1K [84], Taskonomy [112]\n51.30\n0.317\n0.084\n0.037\n0.936\n14.03\n7.789\n21.45\n63.25\nPolyMaX\nConvNeXt-T [64]\nIN-22K [84], Taskonomy [112]\n54.59\n0.282\n0.076\n0.033\n0.954\n13.73\n7.533\n21.18\n64.07\nPolyMaX\nConvNeXt-S [64]\nIN-22K [84], Taskonomy [112]\n56.34\n0.273\n0.073\n0.032\n0.959\n13.42\n7.351\n20.74\n64.73\nPolyMaX\nConvNeXt-B [64]\nIN-22K [84], Taskonomy [112]\n57.21\n0.260\n0.071\n0.030\n0.965\n13.23\n7.181\n20.58\n65.48\nPolyMaX\nConvNeXt-L [64]\nIN-22K [84], Taskonomy [112]\n58.08\n0.250\n0.067\n0.029\n0.969\n13.09\n7.117\n20.40\n65.66\nTable 2. NYUD-v2 test set results. Our PolyMaX with ConvNeXt-L consistently outperforms prior arts on three benchmarks of NYUD-v2.\n\u2020: Our reimplemented DeepLabv3+ of the pixel-regression method for depth estimation and surface normal prediction serves as an additional\nstrong baseline.\nsem seg\ndepth estimation\nsurface normal\nmodel\nencoder\nmIoU \u2191\nRMS \u2193\nA.Rel \u2193\nLog10 \u2193\n\u03b41 \u2191\nMean \u2193\nMed \u2193\nRMS \u2193\n\u03b41 \u2191\nDeepLabv3+ [19]\nResNet-50 [38]\n32.87\n0.6303\n0.1472\n0.0618\n82.61\n11.45\n4.938\n19.65\n56.13\nPolyMaX\nResNet-50 [38]\n35.00\n0.5885\n0.1236\n0.0538\n86.49\n10.96\n4.922\n18.83\n55.27\nPolyMaX\nConvNeXt-T [64]\n35.23\n0.5287\n0.1140\n0.0479\n88.56\n10.87\n4.859\n18.49\n55.39\nPolyMaX\nConvNeXt-S [64]\n38.99\n0.5097\n0.1077\n0.0452\n89.59\n10.67\n4.709\n18.46\n56.05\nPolyMaX\nConvNeXt-B [64]\n41.17\n0.4981\n0.1034\n0.0433\n90.37\n10.58\n4.646\n18.37\n56.43\nPolyMaX\nConvNeXt-L [64]\n40.94\n0.4988\n0.1029\n0.0432\n90.41\n10.46\n4.550\n18.25\n56.76\nTable 3. Taskonomy test set results. The DeepLabv3+ baseline and PolyMaX are trained with the same configuration.\n4.3\nAblation Studies\nScalability of Mask-Transformer-Based Framework\nWe investigate the scalability of PolyMaX, since this prop-\nerty has become increasingly crucial in the large scale model\nera. Fig. 7 demonstrates that PolyMaX\u2019s scalability with\nthe percentage of pretraining dataset. For a fair compari-\nson, we build the baseline models upon DeepLabv3+ [19],\nusing pixel-classification prediction head for semantic seg-\nmentation, and pixel-regression prediction head for depth\nestimation and surface normal prediction. The training con-\nfigurations are the same within each task, except that the\nDeepLabv3+ based depth model requires the robust Char-\nbonnier loss [6] to overcome the typical training instability\nissue of regression with outliers.\nAs shown in Fig. 7a, PolyMaX scales significantly better\nthan the baseline model on semantic segmentation. When\nall models are pretrained with only ImageNet [84] (i.e., 0%\nTaskonomy data is introduced in pretraining), PolyMaX per-\nforms worse than the baseline, especially the ConvNeXt-L\nmodel variant fails disappointingly. This is because the\n0\n25\n50\n75\n100\npercentage (%) of pretraining data\n10\n20\n30\n40\n50\n60\nmIoU (%)\n10.34\n36.06\n37.14\n56.49\n50.00\n47.87\n57.28\n50.99\n49.16\n57.99\n51.57\n49.10\n58.08\n51.30\n50.14\nPolyMaX w/ ConvNeXt-L\nPolyMaX w/ R50\nDeepLabv3+ w/ R50\n(a) Scalability of semantic segmentation model\n0\n25\n50\n75\n100\npercentage (%) of pretraining data\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRMS\n0.4438\n0.4330\n0.3373\n0.4010\n0.3450\n0.2775\n0.3880\n0.3323\n0.2670\n0.3845\n0.3202\n0.2576\n0.3747\n0.3165\n0.2499\nDeepLabv3+ w/ R50\nPolyMaX w/ R50\nPolyMaX w/ ConvNeXt-L\n(b) Scalability of depth estimation model\n0\n25\n50\n75\n100\npercentage (%) of pretraining data\n13\n14\n15\n16\n17\n18\nMean\n17.11\n17.79\n15.48\n15.31\n14.57\n13.50\n15.07\n14.32\n13.26\n14.92\n14.04\n13.19\n14.81\n14.03\n13.09\nDeepLabv3+ w/ R50\nPolyMaX w/ R50\nPolyMaX w/ ConvNeXt-L\n(c) Scalability of surface normal prediction model\nFigure 7. Experiments of model scalability with different pretraining data percentage of Taskonomy on NYUD-v2. Our PolyMaX\ndemonstrates better scalability than the solid per-pixel prediction baseline DeepLabv3+ on all three tasks.\nprobability distribution maps (depth estimation, 16 clusters)\nprobability distribution maps (normal estimation, 8 clusters)\nnormal prediction\ndepth prediction\nimage\nFigure 8. Visualization of probability distribution maps for\ndepth prediction (middle) and normal estimation (bottom) tasks. It\nillustrates the attention on regions with different depth and different\nangles, which validate the effectiveness of PolyMaX with cluster-\nprediction paradigm.\nlarger capacity of PolyMaX can not be fully exploited with\nonly 795 training samples that are available for semantic\nsegmentation from NYUD-v2 dataset. Thus, after including\nonly 25% of the Taskonomy data in pretraining, PolyMaX\nwith ResNet-50 and ConvNeXt-L immediately outperform\nthe baseline model with more than 2% mIoU and 9% mIoU,\nrespectively. This performance gaps remain when the per-\ncentage of pretraining data increases, and stay at more than\n1% and 7% mIoU after using the entire Taskonomy dataset\nfor pretraining.\nThe scalability of depth estimation is shown in Fig. 7b.\nWhen no Taskonomy pretraining data is used, the improve-\nment of PolyMaX over the baseline (both with ResNet-50)\nis only 0.01 RMS, implying negligible positive impact of\ncluster-prediction paradigm. The more than 0.1 RMS perfor-\nmance improvement of PolyMaX with ConvNeXt-L mainly\ncomes from the more powerful backbone. However, Poly-\nMaX has better scalability with the increase of pretraining\ndata. When 25% Taskonomy data is involved in pretrain-\ning, the model performance improvements of PolyMaX with\nResNet-50 and ConvNeXt-L increase from 0.01 and 0.1\nRMS to 0.05 and 0.12 RMS, respectively. With all Taskon-\nomy dataset being used in pretraining, the performance gaps\nof the two PolyMaX come to 0.6 and 0.12 RMS.\nThe scalability of PolyMaX on surface normal, showing\na similar trend as depth estimation, is illustrated in Fig. 7c.\nEven though PolyMaX with ResNet-50 does not perform\nbetter than the baseline model when no Tasknomy pretrain-\ning data is used, it significantly surpasses the baseline model\nby 0.7 and 0.9 mean error at 25% and 100% of pretraining\ndata. While PolyMaX with ConvNeXt-L substantially out-\nperforms the baseline model by 1.2 and 1.8 mean error at\n25% and 100% of pretraining data.\nVisualization of Cluster Representation\nTo obtain\ndeeper understanding of the model behaviour, we further\nlook into the visualization of the learned cluster centers and\nthe probability distribution across those clusters for depth\nestimation and surface normal, shown in Fig. 8. Interestingly,\n12 out of the 16 probability maps from the depth estimation\nmodel look similar, implying redundancy in the learned 16\ncluster centers. Similar findings are observed for surface\nnormal. This can also explain the small impact of cluster\ncount K on model performance in tables of supplementary.\nDespite the redundancy, the rest unique probability maps\nclearly depict the cluster-prediction paradigm. For instance,\nthe last 4 probability maps of the depth model correspond to\nthe attention on close, medium, futher and furthest regions in\nthe scene. Similarly, the unique probability maps of surface\nnormal illustrate the attention on surfaces with different\nangles, including left, right, upwards, downwards.\n5\nConclusion\nWe have generalized dense prediction tasks with the same\nmask transformer framework, realized by casting seman-\ntic segmentation, depth estimation and surface normal to\ncluster-prediction paradigm. The proposed PolyMaX has\ndemonstrated state-of-the-art results on the three benchmarks\nof NYUD-v2 dataset. We hope the superior performance of\nPolyMaX can enable many impactful applications, including\nbut not limited to scene understanding, image generation and\nediting, and augmented reality.\nAcknowledgement We thank Yukun Zhu, Jun Xie and\nShuyang Sun for their support on the code-base.\nReferences\n[1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-\ntendra Malik. Contour detection and hierarchical image\nsegmentation. IEEE TPAMI, 33(5):898\u2013916, May 2011. 1\n[2] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir\nZamir. Multimae: Multi-modal multi-task masked autoen-\ncoders. In ECCV, 2022. 7\n[3] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.\nSegnet: A deep convolutional encoder-decoder architecture\nfor image segmentation. IEEE TPAMI, 39(12):2481\u20132495,\n2017. 2\n[4] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Es-\ntimating and exploiting the aleatoric uncertainty in surface\nnormal estimation. In ICCV, 2021. 2, 7, 13, 15\n[5] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-\ntraining of image transformers. In ICLR, 2022. 6, 7\n[6] Jonathan T Barron.\nA general and adaptive robust loss\nfunction. In CVPR, 2019. 6, 7\n[7] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.\nAdabins: Depth estimation using adaptive bins. In CVPR,\n2021. 2, 3, 4, 5\n[8] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.\nLocalbins: Improving depth estimation by learning local\ndistributions. In ECCV, 2022. 2\n[9] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka,\nand Matthias M\u00a8uller. Zoedepth: Zero-shot transfer by com-\nbining relative and metric depth. arXiv:2302.12288, 2023.\n2, 6, 7\n[10] Deblina Bhattacharjee, Sabine S\u00a8usstrunk, and Mathieu Salz-\nmann. Vision transformer adapters for generalizable multi-\ntask learning. arXiv:2308.12372, 2023. 3, 6\n[11] Deblina Bhattacharjee, Tong Zhang, Sabine S\u00a8usstrunk, and\nMathieu Salzmann. Mult: An end-to-end multitask learning\ntransformer. In CVPR, 2022. 3, 6\n[12] David Br\u00a8uggemann, Menelaos Kanakis, Anton Obukhov,\nStamatios Georgoulis, and Luc Van Gool. Exploring re-\nlational context for multi-task dense prediction. In ICCV,\n2021. 3, 7\n[13] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In ECCV, 2020.\n1\n[14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In ECCV, 2020.\n2, 3\n[15] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Semantic image segmen-\ntation with deep convolutional nets and fully connected crfs.\nIn ICLR, 2015. 1, 2\n[16] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille.\nDeeplab: Semantic\nimage segmentation with deep convolutional nets, atrous\nconvolution, and fully connected crfs. IEEE TPAMI, 2017.\n1, 2\n[17] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\nHartwig Adam. Rethinking atrous convolution for semantic\nimage segmentation. arXiv:1706.05587, 2017. 1, 2\n[18] Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, and\nAlan L Yuille. Attention to scale: Scale-aware semantic\nimage segmentation. In CVPR, 2016. 2\n[19] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nECCV, 2018. 2, 6, 7\n[20] Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexan-\nder Kirillov, Rohit Girdhar, and Alexander G Schwing.\nMask2former for video instance segmentation. In CVPR,\n2022. 3\n[21] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,\nThomas S Huang, Hartwig Adam, and Liang-Chieh Chen.\nPanoptic-DeepLab: A Simple, Strong, and Fast Baseline for\nBottom-Up Panoptic Segmentation. In CVPR, 2020. 2\n[22] Bowen Cheng, Alexander G Schwing, and Alexander Kir-\nillov. Per-pixel classification is not all you need for semantic\nsegmentation. In NeurIPS, 2021. 1, 3\n[23] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe\nFranke, Stefan Roth, and Bernt Schiele. The cityscapes\ndataset for semantic urban scene understanding. In CVPR,\n2016. 1\n[24] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\nvan, and Quoc V Le. Autoaugment: Learning augmentation\npolicies from data. In CVPR, 2019. 13\n[25] Tien Do, Khiem Vuong, Stergios I Roumeliotis, and\nHyun Soo Park. Surface normal estimation of tilted im-\nages via spatial rectifier. In ECCV, 2020. 2, 6, 7\n[26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021. 3,\n7\n[27] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip\nHausser, Caner Hazirbas, Vladimir Golkov, Patrick Van\nDer Smagt, Daniel Cremers, and Thomas Brox. Flownet:\nLearning optical flow with convolutional networks. In ICCV,\n2015. 1\n[28] David Eigen and Rob Fergus. Predicting depth, surface\nnormals and semantic labels with a common multi-scale\nconvolutional architecture. In ICCV, 2015. 1, 5\n[29] David Eigen, Christian Puhrsch, and Rob Fergus. Depth\nmap prediction from a single image using a multi-scale deep\nnetwork. In NeurIPS, 2014. 1, 2, 5, 6\n[30] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. IJCV, 88:303\u2013338, 2010. 5\n[31] David F Fouhey, Abhinav Gupta, and Martial Hebert. Data-\ndriven 3d primitives for single image understanding. In\nICCV, 2013. 5\n[32] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-\nmanghelich, and Dacheng Tao. Deep ordinal regression\nnetwork for monocular depth estimation. In CVPR, 2018. 1,\n2, 4, 5\n[33] Jun Fu, Jing Liu, Yuhang Wang, Jin Zhou, Changyong Wang,\nand Hanqing Lu. Stacked deconvolutional network for se-\nmantic segmentation. IEEE TIP, 2019. 2\n[34] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? the kitti vision benchmark\nsuite. In CVPR, 2012. 6\n[35] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der\nMaaten, Armand Joulin, and Ishan Misra. Omnivore: A\nsingle model for many visual modalities. In CVPR, 2022. 7\n[36] Xiuye Gu, Yin Cui, Jonathan Huang, Abdullah Rashwan,\nXuan Yang, Xingyi Zhou, Golnaz Ghiasi, Weicheng Kuo,\nHuizhong Chen, Liang-Chieh Chen, and David A Ross.\nDataseg: Taming a universal multi-dataset multi-task seg-\nmentation model. NeurIPS, 2023. 2\n[37] Bharath Hariharan, Pablo Arbel\u00b4aez, Ross Girshick, and Ji-\ntendra Malik. Simultaneous detection and segmentation. In\nECCV, 2014. 4\n[38] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 1, 7\n[39] Xuming He, Richard S Zemel, and Miguel \u00b4A Carreira-\nPerpi\u02dcn\u00b4an. Multiscale conditional random fields for image\nlabeling. In CVPR, 2004. 4\n[40] Steven Hickson, Karthik Raveendran, Alireza Fathi, Kevin\nMurphy, and Irfan Essa. Floors are flat: Leveraging se-\nmantics for real-time surface normal prediction. In ICCV\nworkshop, 2019. 2\n[41] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. In ECCV,\n2016. 13\n[42] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V\nGool. Dynamic filter networks. In NeurIPS, 2016. 1, 3\n[43] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics\nhuman action video dataset. arXiv:1705.06950, 2017. 7\n[44] Alex Kendall, Vijay Badrinarayanan, and Roberto Cipolla.\nBayesian segnet:\nModel uncertainty in deep convolu-\ntional encoder-decoder architectures for scene understand-\ning. arXiv:1511.02680, 2015. 2\n[45] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015. 13\n[46] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten\nRother, and Piotr Doll\u00b4ar. Panoptic segmentation. In CVPR,\n2019. 4\n[47] Iasonas Kokkinos. Ubernet: Training a universal convolu-\ntional neural network for low-, mid-, and high-level vision\nusing diverse datasets and limited memory. In CVPR, 2017.\n1\n[48] Alexander Kolesnikov, Andr\u00b4e Susano Pinto, Lucas Beyer,\nXiaohua Zhai, Jeremiah Harmsen, and Neil Houlsby. Uvim:\nA unified modeling approach for vision with learned guiding\ncodes. In NeurIPS, 2022. 3\n[49] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In NeurIPS, 2012. 1\n[50] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-\nerico Tombari, and Nassir Navab. Deeper depth prediction\nwith fully convolutional residual networks. In 3DV, 2016. 2\n[51] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong\nSuh. From big to small: Multi-scale local planar guidance\nfor monocular depth estimation. arXiv:1907.10326, 2019. 1\n[52] Wei-Hong Li, Xialei Liu, and Hakan Bilen. Learning multi-\nple dense prediction tasks from partially annotated data. In\nCVPR, 2022. 3\n[53] Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei.\nFully convolutional instance-aware semantic segmentation.\nIn CVPR, 2017. 2, 5, 6, 16\n[54] Zhengqi Li and Noah Snavely. Megadepth: Learning single-\nview depth prediction from internet photos. In CVPR, 2018.\n6\n[55] Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima\nAnandkumar, Jose M Alvarez, Tong Lu, and Ping Luo.\nPanoptic segformer: Delving deeper into panoptic segmen-\ntation with transformers. In CVPR, 2022. 3\n[56] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang.\nBinsformer: Revisiting adaptive bins for monocular depth\nestimation. arXiv:2204.00987, 2022. 2, 7, 13\n[57] James Liang, Tianfei Zhou, Dongfang Liu, and Wenguan\nWang. Clustseg: Clustering for universal segmentation. In\nICML, 2023. 3\n[58] Shuai Liao, Efstratios Gavves, and Cees GM Snoek. Spheri-\ncal regression: Learning viewpoints, surface normals and 3d\nrotations on n-spheres. In CVPR, 2019. 2\n[59] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 1, 5\n[60] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid.\nLearning depth from single monocular images using deep\nconvolutional neural fields. IEEE TPAMI, 38(10):2024\u2013\n2039, 2015. 2\n[61] Nianfeng Liu, Haiqing Li, Man Zhang, Jing Liu, Zhenan\nSun, and Tieniu Tan. Accurate iris segmentation in non-\ncooperative environments using fully convolutional net-\nworks. In ICB, 2016. 2\n[62] Wei Liu, Andrew Rabinovich, and Alexander C Berg.\nParsenet: Looking wider to see better. arXiv:1506.04579,\n2015. 2\n[63] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV, 2021. 7\n[64] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\nenhofer, Trevor Darrell, and Saining Xie. A convnet for the\n2020s. In CVPR, 2022. 5, 7\n[65] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In CVPR,\n2015. 1\n[66] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In CVPR,\n2015. 2\n[67] Xiaoxiao Long, Cheng Lin, Lingjie Liu, Wei Li, Christian\nTheobalt, Ruigang Yang, and Wenping Wang. Adaptive\nsurface normal constraint for depth estimation. In ICCV,\n2021. 3, 7\n[68] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019. 13\n[69] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Mar-\ntial Hebert. Cross-stitch networks for multi-task learning. In\nCVPR, 2016. 1\n[70] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu\nCho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and\nAlan Yuille. The role of context for object detection and\nsemantic segmentation in the wild. In CVPR, 2014. 1\n[71] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob\nFergus. Indoor segmentation and support inference from\nrgbd images. In ECCV, 2012. 1, 2, 5\n[72] Jia Ning, Chen Li, Zheng Zhang, Zigang Geng, Qi Dai, Kun\nHe, and Han Hu. All in tokens: Unifying output space of\nvisual tasks via soft token. arXiv:2301.02229, 2023. 3, 7\n[73] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.\nLearning deconvolution network for semantic segmentation.\nIn ICCV, 2015. 2\n[74] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv:2304.07193, 2023. 7\n[75] Luigi Piccinelli, Christos Sakaridis, and Fisher Yu. idisc:\nInternal discretization for monocular depth estimation. In\nCVPR, 2023. 2, 7\n[76] Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun,\nand Jiaya Jia. Geonet: Geometric neural network for joint\ndepth and surface normal estimation. In CVPR, 2018. 2\n[77] Xiaojuan Qi, Zhengzhe Liu, Renjie Liao, Philip HS Torr,\nRaquel Urtasun, and Jiaya Jia.\nGeonet++: Iterative ge-\nometric neural network with edge-aware refinement for\njoint depth and surface normal estimation. IEEE TPAMI,\n44(2):969\u2013984, 2020. 2\n[78] Siyuan Qiao, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. Vip-deeplab: Learning visual perception\nwith depth-aware video panoptic segmentation. In CVPR,\n2021. 2, 6\n[79] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In ICML, 2021. 7\n[80] Ren\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. In ICCV, 2021. 1\n[81] Ren\u00b4e Ranftl, Katrin Lasinger, David Hafner, Konrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE TPAMI, 44(3), 2022. 1, 13, 15\n[82] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022. 2, 6,\n7\n[83] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn MICCAI, 2015. 4\n[84] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael S. Bernstein, Alexander C. Berg,\nand Li Fei-Fei. Imagenet large scale visual recognition\nchallenge. IJCV, 115:211\u2013252, 2015. 7\n[85] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. In NeurIPS, 2022. 2, 6,\n7\n[86] Karen Simonyan and Andrew Zisserman. Very deep con-\nvolutional networks for large-scale image recognition. In\nICLR, 2015. 1\n[87] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia\nSchmid. Segmenter: Transformer for semantic segmentation.\nIn ICCV, 2021. 3\n[88] Shuyang Sun, Weijun Wang, Qihang Yu, Andrew Howard,\nPhilip Torr, and Liang-Chieh Chen. Remax: Relaxing for\nbetter training on efficient panoptic segmentation. NeurIPS,\n2023. 2\n[89] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convo-\nlutions for instance segmentation. In ECCV, 2020. 1\n[90] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 2,\n3\n[91] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. Max-deeplab: End-to-end panoptic seg-\nmentation with mask transformers. In CVPR, 2021. 1, 3\n[92] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. Axial-DeepLab: Stand-\nAlone Axial-Attention for Panoptic Segmentation. In ECCV,\n2020. 2\n[93] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,\nChaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui\nTan, Xinggang Wang, et al. Deep high-resolution represen-\ntation learning for visual recognition. IEEE TPAMI, 2020.\n7\n[94] Panqu Wang, Pengfei Chen, Ye Yuan, Ding Liu, Zehua\nHuang, Xiaodi Hou, and Garrison Cottrell. Understanding\nconvolution for semantic segmentation. In WACV, 2018. 2\n[95] Xiaolong Wang, David Fouhey, and Abhinav Gupta. Design-\ning deep networks for surface normal estimation. In CVPR,\n2015. 1\n[96] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and\nTiejun Huang. Images speak in images: A generalist painter\nfor in-context visual learning. In CVPR, 2023. 3, 7\n[97] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chun-\nhua Shen. SOLOv2: Dynamic and fast instance segmenta-\ntion. In NeurIPS, 2020. 1\n[98] Mark Weber,\nHuiyu Wang,\nSiyuan Qiao,\nJun Xie,\nMaxwell D. Collins, Yukun Zhu, Liangzhe Yuan, Dahun\nKim, Qihang Yu, Daniel Cremers, Laura Leal-Taixe, Alan L.\nYuille, Florian Schroff, Hartwig Adam, and Liang-Chieh\nChen. DeepLab2: A TensorFlow Library for Deep Labeling.\narXiv: 2106.09748, 2021. 5\n[99] Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio\nTorralba, and Aude Oliva. Sun database: Exploring a large\ncollection of scene categories. IJCV, 119:3\u201322, 2016. 7\n[100] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\nJose M Alvarez, and Ping Luo. Segformer: Simple and\nefficient design for semantic segmentation with transformers.\nIn NeurIPS, 2021. 3, 7\n[101] Saining Xie, Ross Girshick, Piotr Doll\u00b4ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In CVPR, 2017. 7\n[102] Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, and\nElisa Ricci. Structured attention guided convolutional neural\nfields for monocular depth estimation. In CVPR, 2018. 2\n[103] Xiaogang Xu, Hengshuang Zhao, Vibhav Vineet, Ser-Nam\nLim, and Antonio Torralba. Mtformer: Multi-task learning\nvia transformer and cross-task reasoning. In ECCV, 2022.\n3, 7\n[104] Y Xu, Xiangtai Li, Haobo Yuan, Yibo Yang, and Lefei\nZhang. Multi-task learning with multi-query transformer for\ndense prediction. arXiv:2205.14354, 2022. 3, 7\n[105] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan\nYang. Denseaspp for semantic segmentation in street scenes.\nIn CVPR, 2018. 2\n[106] Hanrong Ye and Dan Xu. Invpt: Inverted pyramid multi-task\ntransformer for dense scene understanding. In ECCV, 2022.\n3, 7\n[107] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-\nChieh Chen. Convolutions die hard: Open-vocabulary seg-\nmentation with single frozen convolutional clip. NeurIPS,\n2023. 2\n[108] Qihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao,\nMaxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille,\nand Liang-Chieh Chen. Cmt-deeplab: Clustering mask trans-\nformers for panoptic segmentation. In CVPR, 2022. 3\n[109] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins,\nYukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh\nChen. k-means Mask Transformer. In ECCV, 2022. 2, 3, 5,\n6\n[110] Yading Yuan, Ming Chao, and Yeh-Chi Lo. Automatic skin\nlesion segmentation using deep fully convolutional networks\nwith jaccard distance. IEEE transactions on medical imag-\ning, 36(9):1876\u20131886, 2017. 2\n[111] Yuhui Yuan, Xilin Chen, and Jingdong Wang.\nObject-\ncontextual representations for semantic segmentation. In\nECCV, 2020. 2\n[112] Amir R. Zamir, Alexander Sax, William B. Shen, Leonidas J.\nGuibas, Jitendra Malik, and Silvio Savarese. Taskonomy:\nDisentangling task transfer learning. In CVPR, 2018. 1, 2,\n5, 7\n[113] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and\nLucas Beyer. Scaling vision transformers. In CVPR, 2022.\n7\n[114] Jiaming Zhang, Huayao Liu, Kailun Yang, Xinxin Hu, Ruip-\ning Liu, and Rainer Stiefelhagen. Cmx: Cross-modal fusion\nfor rgb-x semantic segmentation with transformers. IEEE\nTransactions on Intelligent Transportation Systems, 2023. 6,\n7\n[115] Jiaming Zhang, Ruiping Liu, Hao Shi, Kailun Yang, Si-\nmon Rei\u00df, Kunyu Peng, Haodong Fu, Kaiwei Wang, and\nRainer Stiefelhagen. Delivering arbitrary-modal semantic\nsegmentation. In CVPR, 2023. 2, 6, 7\n[116] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nCVPR, 2017. 1\n[117] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie\nZhou, and Jiwen Lu. Unleashing text-to-image diffusion\nmodels for visual perception. arXiv:2303.02153, 2023. 2, 6,\n7\n[118] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with trans-\nformers. In CVPR, 2021. 3\n[119] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba. Scene parsing through\nade20k dataset. In CVPR, 2017. 7\n[120] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. In ICLR, 2021. 3\nSupplementary Materials\nIn the supplementary materials, we provide additional\ninformation as listed below:\n\u2022 Sec. A provides detailed training protocol used in the\nexperiments.\n\u2022 Sec. B provides additional ablations studies.\n\u2022 Sec. C provides more visualizations of (1) model predic-\ntions, (2) failure modes, (3) learned probability distribu-\ntion maps, and (4) our generated high-quality pseudo-\nlabels for Taskonomy semantic segmentation.\nA\nTraining Protocol\nThe training configurations of PolyMaX closely follow\nkMaX-DeepLab, including the regularization, drop path [41],\ncolor jitting [24], AdamW optimizer [45, 68] with weight\ndecay 0.05, and learning rate multiplier 0.1 for backbone.\nAdditionally, for depth estimation and surface normal, we\nfollow the data preprocessing in [56], except that we disable\nrandom scaling and rotation for surface normal.\nB\nAdditional Ablation Studies\nImpact of Cluster Granularity\nWe analyze the impact\nof cluster granularity (i.e., K cluster centers) for depth es-\ntimation and surface normal, which are presented in Tab. 4\nand Tab. 5. Note that, we skip this analysis for semantic seg-\nmentation, as we can simply assign the number of clusters as\nthe number of classes. In both Tab. 4 and Tab. 5, we observe\nthat the cluster granularity does not have a significant impact\non the model performance on either benchmarks. Among\nthe different cluster settings, 16 clusters and 8 clusters per-\nform the best for depth estimation and for surface normal,\nrespectively.\nK\nRMS \u2193\nA.Rel \u2193\nLog10 \u2193\n\u03b41 \u2191\n\u03b42 \u2191\n\u03b43 \u2191\n4\n0.2544\n0.0689\n0.0295\n96.65\n99.53\n99.91\n8\n0.2578\n0.0691\n0.0296\n96.64\n99.58\n99.89\n16\n0.2499\n0.0670\n0.0288\n96.90\n99.58\n99.90\n32\n0.2520\n0.0685\n0.0293\n96.44\n99.56\n99.91\n64\n0.2537\n0.0688\n0.0295\n96.77\n99.61\n99.90\nTable 4. Impact of number of clusters (K) on depth estimation.\nK\nMean \u2193\nMed \u2193\nRMS \u2193\n\u03b41 \u2191\n\u03b42 \u2191\n\u03b43 \u2191\n4\n13.10\n7.075\n0.2046\n65.74\n82.19\n87.74\n8\n13.09\n7.117\n0.2040\n65.66\n82.28\n87.83\n16\n13.15\n7.111\n0.2051\n65.70\n82.17\n87.73\n32\n13.11\n7.075\n0.2048\n65.75\n82.23\n87.77\nTable 5. Impact of number of clusters (K) on surface normal.\nC\nAdditional Visualization\nModel Predictions\nIn Fig. 9, we show more model pre-\ndictions of semantic segmentation, depth estimation, and sur-\nface normal prediction. As shown in the figure, our proposed\nPolyMaX can capture fine details on scenes with complex\nstructures.\nFailure Modes\nTo better understand the limitations of\nthe proposed model, we also look into the failure modes. As\nshown in Fig. 10, PolyMaX struggles to predict the depth\nand surface normal for transparent and reflective objects,\nwhich are the most challenging issues in the tasks of depth\nand surface normal estimation. The difficulties can also\nbe reflected by the unreliable ground-truth annotations for\nthose cases. In Fig. 11, our model sometimes predicts over-\nsmoothed depth and surface normal results. The findings\nof [4,81] (e.g., a better loss function) may alleviate this issue,\nwhich is left for future exploration.\nProbability Distribution Maps\nWe provide additional\nvisualizations of the learned probability distribution maps\nfor depth estimation and surface normal prediction in Fig. 12\nand Fig. 13, respectively. As shown in the figures, the learned\nprobability distribution maps effectively cluster pixels for\ndifferent distances (for depth task) or angles (for surface\nnormal task).\nTaskonomy Pseudo-Labels\nIn Fig. 14, we show ad-\nditional visualization of the generated high-quality pseudo-\nlabels for Taskonomy semantic segmentation.\ninput\nsem seg pred.\nground-truth\ndepth pred.\nground-truth\nnormal pred.\nground-truth\nFigure 9. Visualization of model inputs and outputs for semantic segmentation, depth estimation and normal prediction. PolyMaX is\ncapable of capturing fine details on scenes with complex structures. Interestingly, as shown in the bottom row, PolyMaX can even reasonably\nestimate the depth for the glass door, where depth models typically struggle.\ninput\nsem seg pred.\nground-truth\ndepth pred.\nground-truth\nnormal pred.\nground-truth\nFigure 10. [Failure mode] PolyMaX still has difficulties with correctly predicting the depth and surface normal for transparent and reflective\nsurfaces (e.g. mirror in first row, glass in second row). These are well-known challenges for such tasks, especially the ground-truths in these\nscenarios are also often unreliable, as shown in these examples.\ninput\nsem seg pred.\nground-truth\ndepth pred.\nground-truth\nnormal pred.\nground-truth\nFigure 11. [Failure mode] Although PolyMaX achieves superior performance on all three benchmarks on NYUD-v2 dataset, we observe\nthat it still suffers from the over-smoothness issue for depth estimation and surface normal tasks, which other prior works [4,81] attempt to\ntackle.\nprobability distribution maps (depth estimation, 16 clusters)\ndepth prediction\nimage\nFigure 12. Additional visualization of probability distribution maps for depth estimation. Despite of the redundancy in the 16 probability\ndistribution maps, the unique ones clearly demonstrate that the pixels are clustered as closest, mid-range, and furthest distances, which\nvalidate the effectiveness of PolyMaX with cluster-prediction paradigm.\nprobability distribution maps (surface normal, 8 clusters)\nnormal prediction\nimage\nFigure 13. Additional visualization of probability distribution maps for surface normal prediction. These probability maps highlight\nregions with different angles, demonstrating PolyMaX is capable of clustering pixels based on the normal directions.\ninput\nours\noriginal\nFigure 14. Additional visualization of Taskonomy pseudo-labels: ours (middle) vs. original ones by Li et al. [53] (bottom). Our\npseudo-labels demonstrate higher quality than the existing ones.\n"
  },
  {
    "title": "Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities",
    "link": "https://arxiv.org/pdf/2311.05698.pdf",
    "upvote": "6",
    "text": "MIRASOL3B:\nA\nMULTIMODAL\nAUTOREGRESSIVE\nMODEL\nFOR\nTIME-ALIGNED\nAND\nCONTEXTUAL\nMODALITIES\nAJ Piergiovanni*\nGoogle DeepMind\nIsaac Noble\u2217\nGoogle Research\nDahun Kim\nGoogle DeepMind\nMichael S. Ryoo\nGoogle DeepMind\nVictor Gomes\nGoogle Research\nAnelia Angelova\nGoogle DeepMind\nABSTRACT\nOne of the main challenges of multimodal learning is the need to combine hetero-\ngeneous modalities (e.g., video, audio, text). For example, video and audio are\nobtained at much higher rates than text and are roughly aligned in time. They are\noften not synchronized with text, which comes as a global context, e.g. a title, or\na description. Furthermore, video and audio inputs are of much larger volumes,\nand grow as the video length increases, which naturally requires more compute\ndedicated to these modalities and makes modeling of long-range dependencies\nharder.\nWe here decouple the multimodal modeling, dividing it into separate, focused au-\ntoregressive models, processing the inputs according to the characteristics of the\nmodalities. We propose a multimodal model, called Mirasol3B, consisting of an\nautoregressive component for the time-synchronized modalities (audio and video),\nand an autoregressive component for the context modalities which are not neces-\nsarily aligned in time but are still sequential. To address the long-sequences of the\nvideo-audio inputs, we propose to further partition the video and audio sequences\nin consecutive snippets and autoregressively process their representations. To that\nend, we propose a Combiner mechanism, which models the audio-video informa-\ntion jointly within a timeframe. The Combiner learns to extract audio and video\nfeatures from raw spatio-temporal signals, and then learns to fuse these features\nproducing compact but expressive representations per snippet.\nOur approach achieves the state-of-the-art on well established multimodal bench-\nmarks, outperforming much larger models. It effectively addresses the high com-\nputational demand of media inputs by both learning compact representations, con-\ntrolling the sequence length of the audio-video feature representations, and mod-\neling their dependencies in time.\n1\nINTRODUCTION\nMultimodal models aim to combine the signals from multiple varied sources, which makes them\nboth universal and useful for practical applications. However, these modalities have diverse charac-\nteristics and are challenging to combine uniformly by a single model. For example, video and text\nhave disparate sampling rates: a video has many frames per second, but text or other similar types of\nglobal context, e.g., a description or title, can be provided once per video, or asynchronously to the\nvideo. Video also takes a larger portion of the input. At the same time, video and audio are naturally\nco-occurring and appear (almost) synchronously. They are roughly aligned and complementary.\nThis co-occurrence in time can contribute to their joint learning and serve as a rich self-supervisory\nlearning signal, applied more frequently than global text signals. So, ideally, these modalities need\nto be processed by differently-synchronized model components, which process more adequately\ninputs of different frequencies and allocate more parameters to the more abundant modalities.\n\u2217Equal contribution. Contact: ajpiergi, isaacn, anelia @google.com\n1\narXiv:2311.05698v2  [cs.CV]  13 Nov 2023\nFollowing the success of large language models, where text input sequences are processed autore-\ngressively, many recent multimodal models reuse the autoregressive text models, feeding in other\nmodalities, e.g., as embeddings, (Chen et al., 2023b; Wang et al., 2022b; Piergiovanni et al., 2022a;\nLi et al., 2021a; 2022b; Alayrac et al., 2022), or by tokenizing the visual inputs to be processed\ntogether with the text token sequence (Wang et al., 2022d; Aghajanyan et al., 2022; Yu et al., 2023;\nReed et al., 2022; Zellers et al., 2022)). However, the imbalance of the information volume is large\nand models which are well suited to encode/decode text sequences process only highly compressed\nimage or video features (Alayrac et al., 2022; Nagrani et al., 2021). For example, the Flamingo\nmodel (Alayrac et al., 2022), subsamples the video features significantly, dedicating only about 1%\nof the parameters to the image and video inputs, leaving the rest for text processing. Alternatively,\nmethods that process the video running each frame independently through an encoder or tokenizer,\nreach an upper bound on the number of frames quickly (Yan et al., 2022; Piergiovanni et al., 2023b).\nFor longer inputs, these representations are insufficient to properly represent the modalities, which\ninherently limits the ability to model fine-grained or long-range dependencies.\nWe here propose an audio-video-text multimodal model, Mirasol3B, where we decouple the autore-\ngressive modeling into a component for time-aligned modalities, e.g., audio and video, which are\nprocessed in time, autoregressively, and an autoregressive component for non-time-aligned contex-\ntual modalities e.g., text. Cross-attention weights coordinate the learning between these components.\nThis decoupling allows for better parameter distribution within the model, allocating sufficient ca-\npacity for the media modalities (video and audio), and leads to smaller models overall. Furthermore,\nwe propose to partition the time-aligned modalities into time segments, where audio-video joint rep-\nresentations are learned and accumulated in time. To that end, we introduce a joint feature learning\nmechanism for the media modalities, called the Combiner, which fuses their features and produces\na more compact representation. We extract low level spatio-temporal representation from the raw\nmedia inputs in order to capture the dynamic nature of videos in a high-level representation and com-\nbine it with audio features within concurrent timesteps. Our model enables consuming multimodal\ninputs at different rates and scales well with longer videos. The Combiner effectively balances the\nneed for efficient media representations and ones which are expressive enough to preserve the media\ncontent. It sufficiently represents the events and activities in the videos and other concurrent modal-\nities and can be handled by subsequent autoregressive models, which allows for learning long-range\ndependencies. Our contributions are:\n\u2022 An autoregressive multimodal model, subdividing learning into autoregressive modeling\nfor time-aligned media modalities and non-time-aligned contextual modalities.\n\u2022 Joint feature representation learning via the Combiner to balance the learning of efficient\nmedia representations which are also sufficiently expressive to preserve the media content.\n\u2022 We demonstrate learning with 128-512 video frames without increase in model parameters.\nThis is in contrast to prior works that use 32 (Gao et al., 2023) or 8 frames (Yan et al., 2022).\nOur model outperforms the state-of-the-art on multiple benchmarks, with large margins on audio-\nvideo-text datasets and on long video datasets. It outperforms much larger models, as well.\n2\nRELATED WORK\nArchitectures for video-language understanding commonly use a joint transformer, where video in-\nputs are fed in together with text tokens and processed autoregressively (Fu et al., 2021; Zellers\net al., 2021)). This is often accomplished with tokenizing the visual inputs. Video-text pretraining\napproaches (Miech et al., 2019; 2020; Wang et al., 2022e;c; Li et al., 2023; 2022a) use masked\ntoken modeling and reconstruction (Fu et al., 2021), masking with cross-attention on multimodal in-\nputs (Alayrac et al., 2022), or contrastive learning (Wang et al., 2022c; Xu et al., 2023; Zellers et al.,\n2021; Dong et al., 2023). Visual synthesis models have extensively used autoregressive models, by\nlearning to generate pixel-wise predictions (van den Oord et al., 2016), or by learned discrete tokens\nfrom images or videos, e.g., N \u00a8UWA (Wu et al., 2022), VideoGPT (Yan et al., 2021), GODIVA (Wu\net al., 2021). In other models, encoder-decoder or decoder-only architectures extend an image-text\nmodel to a video-text one (Wang et al., 2022b; Yan et al., 2022), where video is processed by\nindividual frames which are then combined. Some architectures instead extract full video signals\n(typically as embeddings) before feeding them to the model (Xu et al., 2023). Another option is to\n2\nFigure 1: Mirasol3B model architecture consists of an autoregressive model for the time-aligned\nmodalities, such as audio and video, which are partitioned in chunks (left) and an autoregressive\nmodel for the unaligned context modalities, which are still sequential, e.g., text (right). This allows\nadequate computational capacity to the video/audio time-synchronized inputs, including process-\ning them in time autoregressively, before fusing with the autoregressive decoder for unaligned text\n(right). Joint feature learning is conducted by the Combiner, balancing the need for compact repre-\nsentations and allowing sufficiently informative features to be processed in time.\nattach a projection or re-tokenization layers e.g., as in Perceiver in Flamingo (Alayrac et al., 2022),\nto reduce the amount of visual tokens added to the model. Our approach differs substantially, as\nthe media input features have a specifically designed component to learn them jointly and in time,\nproducing more abstract representations, suitable for modeling long videos.\nMultimodal audio-video-text models have also gained popularity: UAVM (Gong et al., 2022) pro-\npose joint learning of audio and video by building invariant transformer module which can be reused\nby either signal. Multimodal Transformer (Tsai et al., 2021) proposes cross-attention mechanisms,\nfor cross-modal learning on all pairs of video-audio-text data, which Pellegrain et al. (2021) extends\nto longer sequences. Zellers et al. (2022) demonstrate joint multimodal audio-video-text learning\nbut only aligning text and audio. Gong et al. (2023) use contrastive audio-video learning, whereas\nHuang et al. use masked autoencoder for audio-video learning. Both approaches tokenize the audio\nvideo inputs independently in 2D patches which are used for further processing. Rouditchenko et al.\n(2021) and Korbar et al. (2018), propose contrastive learning for audio-video signals, leveraging the\ntime-alignment between them. Audio-video late fusion is also common, e.g., (Pibre et al., 2022).\nOur work is related to long-form video understanding (Wu & Krahenbuhl, 2021; Sun et al., 2022).\nLong-form videos have been handled by hierarchical feature learning e.g., the Temporal Window\nAttention (Sun et al., 2022) where dependencies are learned locally and then further propagated to\nhigher level cross-attention modules. Ashutosh et al. (2023) propose contrastive learning at different\nhierarchical levels. Gao et al. (2023) segment videos then pool their features into a small represen-\ntation. Memory-augmented model for long videos (Wu et al., 2023) accumulate prior context in\nlearnable \u2018memory\u2019, to be referenced at each step of learning. Our work contributes by proposing a\nbalanced approach of locally learning important features, jointly within the modalities.\n3\nAPPROACH\nAutoregressive models are powerful generative models that are well suited for data which appears\nin a sequence, modeling the probability of the current value, conditioned of previous ones. Video\nand audio information is sequential but also roughly time-synchronized. At the same time, other\nmodalities e.g., text, might be provided globally per video as context and applied to the full video\nrather than to specific parts1. To address the challenges of modeling diverse multimodal inputs, we\npropose to subdivide the autoregressive modeling by learning separate autoregressive models: one\nfor the time-aligned modalities (audio-video), Section 3.3, and another one for modalities which are\nnot necessarily aligned in time but are still sequential, Section 3.4. Learning across these is coordi-\n1Text, e.g., ASR, might also appear concurrently with audio/video and can contribute to improved under-\nstanding of the video content. We leave this to future work.\n3\nnated by cross-attention mechanisms, where here the media inputs (audio-video) are allocated a lot\nmore parameters and are properly modeled in time. A learning module, called the Combiner (Sec-\ntion 3.2), combines the lower-level signals from video/audio snippets. Here information is processed\nspatio-temporally, extracting features particularly relevant to dynamic changes in the inputs.\nArchitecture overview. At a high level, the architecture consists of two main learning components\n(Figure 1): The first one is an autoregressive component which is designed to process (almost)\nsynchronized multimedia inputs e.g., video+audio and combine their inputs in time (Figure 2, left).\nIn order to process the video and audio signals, and to accommodate longer video/audio inputs, they\nare partitioned into smaller chunks (roughly synchronized in time) for which a joint audio-visual\nrepresentation is learned via the Combiner as described below. The second component processes\nthe context, or the signals not aligned in time, such as global text information, which are often still\nsequential. It is autoregressive as well, and uses the combined latent space as cross-attention inputs.\nProblem formulation: We have an input video sequence of N frames v = {vf\n1 , vf\n2 , . . . vf\nN}, and\naudio wave signal of M timesteps a = {af\n1, af\n2, . . . af\nM}, where the audio signal is captured during\nthe duration of the video and corresponds to the given video input. Additionally we have an input\ntext sequence t = {tf\n1, tf\n2, . . . tf\nP }, which is related to the video/audio and might vary according to\nthe tasks e.g., it can be a description, a question-answer pair, meta information and etc.\nPartitioning of the media inputs: In order to process the video sequence efficiently and to learn the\ncorrelation of features in time, we partition the input video into into T non-overlapping segments or\nchunks, with vt and at denoting the video and audio input per segment. Here each chunk captures\nall input data between two timestamps (i.e., video and audio snippets), as follows:\nvf\n1 , vf\n2 , . . . , vf\nN/T ,\n|\n{z\n}\nv1\nvf\nN/T +1, vf\nN/T +2, . . . , vf\n2N/T ,\n|\n{z\n}\nv2\n. . . vf\n(T \u22121)N/T +1, vf\n(T \u22121)N/T +2, . . . , vf\nN,\n|\n{z\n}\nvT\n(1)\nThus the video is represented by its chunks instead, v = {v1, v2, . . . vT }, and more specifically\nlatent features will be extracted from each chunk to represent the video (as described in Section 3.1).\nA similar partitioning is done for the audio signals, where they are partitioned in T chunks to be\nsynchronized in time to the video chunks, a = {a1, a2, . . . aT }. Here too we assume that audio\nfeatures will be extracted from the raw audio signal, as described in Section 3.1.\n3.1\nAUDIO/VIDEO FEATURES\nVideo features. Prior models captured video information at individual sparsely sampled frames,\nwhich lacks the temporal information essential to video understanding and which might miss dy-\nnamic events. Alternatively, 3D convolutions (Wu et al., 2022), sparse 3D tubes (Piergiovanni et al.,\n2023a) and others learn spatio-temporally, which can capture key dynamic changes in the videos.\nWe expand on these ideas and extract sparse 3D tubes (Piergiovanni et al., 2023a) from the videos\nwhich span all 3 dimensions of the video snippet starting at various locations. The sparse 3D tubes,\ntogether with standard 2D patches are processed via a ViT encoder. Rather than applying offsets, the\ntubes are applied at the beginning of the snippet. Per video chunk t we denote \u02c6vt as the time-aligned\nfeatures for this chunk, and thus \u02c6v = {\u02c6v1, \u02c6v2, . . . , \u02c6vT ) are the time-aligned video representations\nfor the whole video.\nAudio features. Audio inputs arrive at a predefined frequency and can be processed in various ways.\nWe here represent the audio as a spectrogram. The spectrogram is created so that the time bands\nmatch the 25 frames per second used in the videos, and thus can easily be split into snippets aligned\nwith the video. The spectrogram for each snippet is processed by a ViT model, after an audio input\nprojection layer. The ViT backbone is the same as the one used for video features. Reusing the\nvisual component is previously shown to be advantageous (Gong et al., 2022). Similarly to above,\nwe denote \u02c6at to be the audio feature per chunk t and \u02c6a = {\u02c6a1, \u02c6a2, . . . , \u02c6aT ) for the full video.\n3.2\nMODALITY COMBINER\nThe task of the Combiner module is two-fold: 1) to combine the video (and audio) features at a\nspecific snippet of time, learning their joint representation and 2) effectively compress the represen-\ntation from each video/audio snippet, which allows our model to scale to longer videos.\n4\nTransformer\n(a) Transformer combiner. All features are \ninput to the transformer, a smaller number of \nm features are selected as combined features.\n(b) TTM Combiner. Uses the TTM \nmechanism to store a memory and compute \nthe m combined features for each time step.\nThis process is repeated for each time step.\nVideo + \nAudio features\nMemory\nRead\nProcess\nWrite\nSingle step Video + Audio features\nMLP\nFeature selection\nFigure 2: Autoregressive modeling of video and audio in time (left). Combiners (right).\nWhen partitioning the inputs, the features for each modality, video and audio in this case, are\n(roughly) time-aligned latent features \u02c6v = {\u02c6v1, \u02c6v2, . . . , \u02c6vT ) and \u02c6a = {\u02c6a1, \u02c6a2, . . . , \u02c6aT ), where\nthe maximum timestamp for any data incorporated into \u02c6vt or \u02c6at is less than the minimum timestamp\nof any data incorporated into \u02c6vt+1 or \u02c6at+1. Explicitly \u02c6vt is composed of f features of size d giving\nit a shape of (f, d) and \u02c6at is composed of s features also of size d with shape (s, d). The role of\nthe combiner is to map such time-aligned modal latent features into a smaller set of shared latent\nfeatures. Specifically let \u02c6c = {\u02c6c1, \u02c6c2, . . . , \u02c6cT } where \u02c6ct = (\u02c6vt, \u02c6at) having size (n, d) and n = f +s\nbe the set of all time-aligned features from all modalities. The combiner then maps \u02c6c to a shared\nlatent feature space x = {x1, x2, . . . , xT } where xt has shape (m, d) where n >> m.\nSince the features produced by the combiner are going to be used in the sequential autoregressive\nmodeling of video/audio, it is important for the combiner to not break causality thus:\nxt = Combiner(\u02c6c1, \u02c6c2, . . . , \u02c6ct)\n(2)\nWe utilize two different architectures for the combiner, a standard Transformer one and a memory\nbased approach, based on the Token Turing Machines (Ryoo et al., 2023), to reduce memory.\nCausal Transformer Combiner.\nWe explore a straightforward version of the Combiner, which\nconsists of a standard Transformer model, here of K layers (here, K = 8). For each step t it\nmaps the original set of features \u02c6ct to xt where xt is of much lower dimensionality, i.e., effectively\nreducing the number of tokens (here m = 32) (Figure 2, right). The inputs to the Combiner are the\nlatent features of the video and audio, which are concatenated before being fed to the Combiner. We\nhere specifically implement a causal version of the transformer as it masks out inputs from future\ntimestamps (i.e., > t). The attention mechanism of the transformer is modified to mask features at\nthe time-chunk level as described later in Section 3.3.1 (using Equation 8), thus all features from \u02c6ct\nand the previous time steps are used to compute each output feature in xt as in Equation 2. This\neffectively applies attention mechanisms to all the modality inputs jointly while respecting causality.\nToken Turing Machine Combiner.\nToken Turing Machine (TTM) (Ryoo et al., 2023) is a re-\ncurrent sequential model with Transformers and token-based operations. It maintains an external\n\u2018memory\u2019 Mt as a set of features, and updates it at every time step by reading and writing. Given\ninputs \u02c6ct at each time step, it first \u2018reads\u2019 features to be processed, from input features as well as\nmemory features. Such features, zt, are passed to the \u2018processor\u2019, which is implemented as a stan-\ndard Transformer, generating a set of intermediate output features ot. These intermediate outputs\nare then used to update Mt (i.e., memory \u2018write\u2019) as well as to produce the final output xt.\nzt = Read(\u02c6ct, Mt)\n(3)\not = Process(zt)\n(4)\nMt+1 = Write(Mt, ot, \u02c6ct)\n(5)\nxt = Output(ot)\n(6)\n5\nThe key idea is to make the Transformer processor generate the outputs by utilizing memory Mt\ninstead of the entire history of features {\u02c6c1, . . . , \u02c6ct\u22121}. Once trained, the differentiable read and\nwrite operations will be optimized to maintain Mt so that it stores important features from the\nprevious time steps {\u02c6c1, . . . , \u02c6ct\u22121} and updates it, at every step.\nWe implement TTM as the Combiner module to sequentially combine \u02c6c. The function \u2018Process\u2019 is\nimplemented with a standard Transformer with layers of multi-head self-attention and MLPs. The\nfunctions \u2018Read\u2019, \u2018Write\u2019, and \u2018Output\u2019 are implemented with TokenLearner (Ryoo et al., 2021)\n(which is similar to Perceiver (Jaegle et al., 2021) and attention pooling (Lee et al., 2019)). Note\nthat we are able to separately control the number of features in the memory as well as the number of\n\u2018Output\u2019 function features, allowing efficient combiner computation and feature generation.\nThe key advantage of the TTM Combiner is its utilization of memory features to sequentially process\n\u02c6ct. The number of such memory features are much smaller than the total number of history features\n({\u02c6c1, . . . , \u02c6ct\u22121}) in general (e.g., 256 vs. \u223c10k). This not only makes TTM a natural fit for the\nmodel, but also reduces the total time complexity of the TTM Combiner to be constant with respect\nto t, instead of O(t) or O(t2) in Transformers. We observe that the TTM Combiner saves memory\nin both training and inference, using about 30% less memory and reduces the runtime by about 18%.\n3.3\nTIME-ALIGNED VIDEO/AUDIO AUTOREGRESSIVE MODELING\nWe describe the autoregressive modeling of time-aligned video and audio. We apply autoregressive\nmodeling strategy where we condition video/audio representations corresponding to a time interval\non feature representations from previous time intervals. These representations are learned jointly\nby the Combiner, as described in Section 3.2. As mentioned, the video is first partitioned in T\nsmaller video snippets. Each of the snippets itself can be of size 4-64 frames (overlap is possible but\ncurrently not used). We extract spatio-temporal information into latent video features \u02c6vt and audio\nfeatures \u02c6at from the same video partition, apply Combiner to produce xt. The feature representa-\ntions per video chunk xt are then fed sequentially to the autoregressive model, where at each step\nwe reconstruct the features from the previous step, conditioned on the prior inputs and the latent\nvector h which corresponds to the latent representations learned within the autoregressive model:\np(v, a) =\nT\nY\nt=1\np(vt+1, at+1|ht)p(ht|xt)p(xt|vt, at)\n(7)\nwhere {v1, v2, . . . vT }, and {a1, a2, . . . aT } are the feature representations from the video and\naudio, p(xt\u22121|vt\u22121, at\u22121) is estimated by the combiner, and p(ht\u22121|xt\u22121) is estimated by the latent\ncausal model. This allows for learning from previous representations in the sequence (in time) and\naims to predict the next-step feature representation (Figure 2, left). While autoregressive modeling\nhas been used for videos and images, it is often done on a pixel-by-pixel basis (Weissenborn et al.,\n2020) which is highly inefficient and captures only short-term dependencies. With our approach,\nwith autoregressive modeling and the Combiner, we address both shortcomings.\nLatent Causal Modeling.\nThe autoregressive latent model estimates: QT\nt=1 p(ht|xt). This is\ndone by applying an autoregressive transformer to x = {x1, x2, . . . , xT } to produce \u02c6h =\n{\u02c6h1, \u02c6h2, . . . , \u02c6hT } where the target of \u02c6ht is xt+1 so the difference between x2,...,T and \u02c6h1,...,T \u22121 is\nused as a loss to control the latent representation of the combiner output \u02c6x. Since we are modeling\ndata autoregressively in time, care must be taken with the attention mechanism during training, the\ntransformer uses a modified attention mechanism as described below in Section 3.3.1, Eq. 8.\nModality Reconstruction.\nThe autoregressive modality reconstruction models estimate\nQT\nt=1 p(vt+1, at+1|\u02c6ht). This is done by applying a separate transformer to \u02c6h to produce recon-\nstructions of the audio and video signals \u02c6v and \u02c6a, which is added as an optional loss below. To save\non computation, the video input v is down sampled to vsmall for the reconstruction target, thus the\nactual reconstruction is \u02c6vsmall.\n3.3.1\nATTENTION MECHANISMS FOR AUTOREGRESSIVE MODELING\nSince the autoreggressive models are trained in time, masking is done to satisfy causality. We note\nthat the attention mechanisms within and across chunks need to be modified when masking. This\n6\napplies to both the Combiner and the Autoregressive learning (Sections 3.2 and 3.3). When mask-\ning features for autoregressive modeling, the standard pattern of masking each feature individually\nwould mask features from within the same time-chunk from each other. While this would still satisfy\ncausality, it unnecessarily restricts the model, preventing features from within the same time-chunk\nfrom interacting based on position within the time-chunk. To allow features in the same chunk to\ninteract, the autoregressive mask between all features i, which fall in a time-chunk t, and another\nfeature j is computed as follows (N is the number of features and T the number of time-chunks):\nmaski\nj =\n\u001a0\nj <= ceil(t \u2217 T/N) \u2217 N/T\n1\notherwise.\n(8)\n3.4\nCOMBINING ALIGNED AND NON-ALIGNED AUTOREGRESSIVE MODELING\nText, or other context information, might not necessarily be aligned in time with the video and\naudio modalities. It is still sequential. So here it is modeled by a separate autoregressive model,\ndevoted to text representations and to combining the visual-audio information together. Assum-\ning tokenization for the input text t = {tf\n1, tf\n2, . . . tf\nP } is provided, obtaining a tokenized text\nsequence t = {w1, w2, . . . wL} of length L, we model the text sequentially as conditioned on\naudio and video. In order to combine the outputs of the video/audio autoregressive model we use\ncross-attention strategy (Alayrac et al., 2022). Here, unlike prior work, all feature representations\n\u02c6h = {\u02c6h1, \u02c6h2, . . . , \u02c6hT } from the latent causal model are used in the main text model.\np(w|\u02c6h) =\nL\nY\nl=1\np(wl|wl\u22121, \u02c6h).\n(9)\nThe autoregressive text model estimates Eq. 9 by applying a transformer to the input text sequence\nw = {w1, w2, . . . , wL} and using the latent model output \u02c6h as cross-attention to produce \u02c6\nw. The\nloss is the standard cross-entropy loss between target w and output text sequences \u02c6\nw. This provides\nfurther feedback to the combiner latent representation \u02c6h through the cross-attention layer. Of note\nis that since all parts of the model are autoregressive, it is naturally applicable to streaming videos.\n3.5\nMODEL LOSSES\nWe use the following losses: Latent space reconstruction loss is the difference between x2,...,T\nand \u02c6h1,...,T \u22121 in autoregressive setting such that \u02c6ht = xt+1. We apply a L2 normalization and then\ntake dot product between the feature vectors as the loss (i.e., cosine similarity). Video reconstruc-\ntion loss. Similar to latent space reconstruction above the video reconstruction loss approximates the\ndifference between \u02c6vsmall and vsmall also in an autoregressive setting such that \u02c6vsmall\nt\n= vsmall\nt+1\n.\nWe use the same distance metric on the video reconstruction as we use on the latent space recon-\nstruction problem. Unaligned text cross entropy loss is the standard cross-entropy loss between w\nand \u02c6\nw for the unaligned text output. These losses are weighted to compute the final loss.\n3.6\nIMPLEMENTATION DETAILS\nModel: Our video+audio model model has 3B parameters; without audio it is 2.9B. A little over\nhalf of the parameters are for the audio+video autoregressive model. Our models work on 128\nframes customarily, but can handle more for longer videos (e.g., 512). We use 16 chunks and\nCombiner dimension m = 32. We apply random masking to the combiner output features at a ratio\nof 0.75% as a form of dropout regularization as we found this stabilizes the causal model latent\nreconstruction. Due to the design of our model (partitioning and Combiner), adding more frames,\nor increasing the chunk size, number of chunks, etc. lead to only marginal increase in parameters.\nIncreasing the number of chunks, while not leading to parameter increases, increases memory, which\nunderscores the importance of the Combiner and particularly the TTM. Model training: The model\nis pretrained on the Video-Text Pairs (VTP) dataset which is collected from noisy video-text pairs\nfrom the web (Alayrac et al., 2022). We use only about 12% of the data. All losses are given equal\nweight during pretraining. During finetuning the unaligned text loss is increased 10-fold to better\nalign the training loss with the final evaluation. The model is trained for 10 epochs on MSRVTT-QA,\nVGG-Sound, 80 on ActivityNet-QA, 20 on NExT-QA, 8 on Epic-Sound, 4 on Kinetics-Sound.\n7\nTable 1: Video QA results on MSRVTT-QA. Results in gray show VideoQA as classification.\nMethod\nAccuracy (%)\nJust Ask (Yang et al., 2021)\n41.5\nALPRO (Li et al., 2022a)\n42.1\nMERLOT (Zellers et al., 2021)\n43.1\nVIOLETv2 (Fu et al., 2023)\n44.5\nVindLU (Cheng et al., 2022)\n44.6\nVideoOFA (Chen et al., 2023c)\n45.4\nGIT2 (Wang et al., 2022b)\n45.6\nIterative Co-Tok (Piergiovanni et al., 2022b)\n45.7\nVideoCoca (Yan et al., 2022)\n46.3\nAll-in-one (Wang et al., 2022a)\n46.8\nUMT-L (Li et al., 2023)\n47.1\nPaLI-X (Chen et al., 2023a)\n47.1\nInternVideo (Wang et al., 2022e)\n47.1\nFlamingo (Alayrac et al., 2022)\n47.4\nM-PLUG2 (Xu et al., 2023)\n48.0\nMaMMUT (Kuo et al., 2023)\n49.5\nMirasol3B - TTM\n50.01\nMirasol3B\n50.42\n4\nEXPERIMENTS\nBelow we report results on standard Video Question Answering (VideoQA) benchmarks, on long-\nvideo VideoQA benchmarks and on Audio+Video benchmarks. We report results using the open-\nended text-generative evaluation, following (Li et al., 2021b; Lei et al., 2022). Our model gen-\nerates a free-form text response which is compared to the target response for an exact match. This\nis more challenging than a classification setting, as our model might generate a correct answer but\nwhich is not among the desired classes. This evaluation is more general and widely applicable.\nVideo Question Answering. We first report Video Question Answering results on the MSRVTT-\nQA VideoQA dataset (Xu et al., 2016), as the most popular Video QA benchmark. The results are\nshown in Table 1 alongside the best state-of-the-art (SOTA) performances. Our method outperforms\nthe SOTA, including the ones with classification evaluation, and models much larger than ours,\ne.g. PaLI-X (Chen et al., 2023a) of 55B parameters and Flamingo (Alayrac et al., 2022) of 80B\nparameters.\nLong video Question Answering. We further report Video QA results on long video datasets.\nActivityNet-QA (Yu et al., 2019) contains longer videos of about 160 seconds per video. NExT-\nQA (Xiao et al., 2021) is a targeting complex events with long videos of about 44 seconds. We\nsample up to 512 frames. Results are in Table 2, showing we outperform the SOTA approaches.\nAudio-Video Results.\nTable 3 shows results on three Audio-Video benchmarks:\nKinetics-\nSound (Arandjelovic & Zisserman, 2017), VGG-Sound (Chen et al., 2020) and Epic-Sound (Huh\net al., 2023). Since these datasets are Audio-Video classification, we treat the task as open-ended\ngeneration: we input the text \u2018Classify the video audio clip.\u2019 and expect the output to be the target\nclass name e.g., \u2018playing drums\u2019. Across all datasets, we outperform the SOTA with large mar-\ngins, despite the more challenging open-text generation evaluation. Methods shown in the table\nare MBT (Nagrani et al., 2021) One-Peace (Wang et al., 2023), SSAST, ASF (Huh et al., 2023),\nUAVM (Gong et al., 2022).\n4.1\nABLATIONS\nThe ablations (Table 4), are conducted with the video and text model in order to understand the main\nbehaviors of this architecture. We also use a smaller model and configuration, and where applicable,\n2x fewer pretraining steps with the same batch size to save compute (details are in the Appendix).\n8\nTable 2: Long video results on ActivityNet and NExT-QA. Gray is for classification setting.\n(a) ActivityNet-QA.\nMethod\nAcc %\nJust Ask (Yang et al., 2021)\n38.9\nMERLOT (Zellers et al., 2021)\n41.4\nFrozenBiLM (Yang et al., 2022)\n43.2\nVideoCoca (Yan et al., 2022)\n56.1\nSing-Temp (Lei et al., 2022)\n44.1\nVindLU (Cheng et al., 2022)\n44.7\nUMT-L (Li et al., 2023)\n47.9\nPaLI-X (Chen et al., 2023a)\n49.4\nMirasol3B - 512 frames TTM\n49.85\nMirasol3B - 128 frames\n48.25\nMirasol3B - 512 frames\n51.13\n(b) NExT-QA.\nMethod\n(Acc %)\nCLIP (single frame)\n43.7\nVQA-T (Yang et al., 2021)\n52.32\nAIO (Wang et al., 2022a)\n50.60\nATP (Buch et al., 2022)\n54.3\nVGT Xiao et al. (2022)\n55.02\nMIST - CLIP Gao et al. (2023)\n57.18\nHiTeA Ye et al. (2022)\n63.1\nMirasol3B - 512 frames TTM\n73.2\nMirasol3B - 128 frames\n68.2\nMirasol3B - 512 frames\n72.0\nTable 3: Audio-Video results on Kinetics-Sound, VGG-Sound, and Epic-Sound.\n(a) Kinetics-Sound.\nMethod\nAcc. %\nMBT (A+V)\n85.0\nMirasol3B (Sm, Video)\n81.3\nMirasol3B (Sm, A+V)\n85.0\nMirasol3B TTM (A+V)\n88.3\nMirasol3B (A+V)\n90.1\n(b) VGG-Sound.\nMethod\nAcc. %\nUAVM\n65.8\nMMT (Zhu et al., 2022)\n66.2\nMAViL (Huang et al.)\n67.1\nONE-PEACE\n68.2\nMirasol3B TTM (A+V)\n66.4\nMirasol3B (A+V)\n69.8\n(c) Epic-Sound..\nMethod\nAcc. %\nSSAST\n53.47\nASF\n53.75\nMirasol3B (Audio)\n62.4\nMirasol3B (Video)\n72.4\nMirasol3B TTM (A+V)\n79.4\nMirasol3B (A+V)\n78.2\nTable 4: Ablation studies.\n(a) Effects of proposed components.\nModel\nFrames/Chunks\nAcc.\nBaseline\n32/4\n41.5\n+ AR\n32/4\n43.2\n+ Combiner\n32/4\n42.1\n+ AR + Combiner\n32/4\n44.7\n+ Pretraining\n32/4\n45.2\n+ AR + Comb. + PT\n32/4\n47.9\n(b) Combiner types.\nCombiner type\nFr./Ch.\nAcc.\nPerceiver\n32/4\n43.1\nTransf.+CLS\n32/4\n43.7\nOurs-Transf.\n32/4\n44.2\nOurs-TTM\n32/4\n44.8\n(c) Autoregressive model more frames.\nModel\nFrames/Chunks\nAcc.\nBaseline\n64/1\n41.8\nOurs-Autoreg.\n64/8\n45.1\nOurs + BD\n64/8\n45.1\nOurs-Autoreg.\n128/8\n45.8\n(d) Combiner dimension.\nModel\nFr./Ch.\nDim\nAcc.\nOurs-8\n32/4\n8\n42.53\nOurs-16\n32/4\n16\n43.36\nOurs-32\n32/4\n32\n44.20\nOurs-64\n32/4\n64\n44.22\n9\nMain model components: We start with ablations where we study the effect of each component\n(Table 4a). We find that on top of a baseline model, adding each part, the autoregressive (AR) model,\nthe Combiner, and pretraining, each individually help and the combination of all three further help.\nCombiner type ablations: We compare the Combiners: transformer-based (ours, CLS and Per-\nceiver (Alayrac et al., 2022)) and TTM. The CLS-token inspired combiner appends m learnable\nfeatures to the end of the sequence and takes their values as the combined features after passing the\nwhole sequence through the transformer. These are visualized in Figure 3. We use the same settings\nfor direct comparison. Table 4b shows that our proposed combiners perform best.\nAutoregressive modeling in time: We ablate the Autoregressive part of the model. Table 4c shows\nthat processing the video in chunks autoregressively in time is more advantageous than learning\nfrom the full video at once, with a large jump in performance (first two rows). Not only is our\nautoregressive model feasible for longer videos but it is also more beneficial for the same size inputs.\nMore frames per chunk contribute to the improvements (rows two and four). We also compare to a\nbidirectional (BD) model, finding that the performance is the same as the autoregressive portion.\nCombiner size ablations. We further compare the number of features output by the combiner per\ntimestep. We noticed a trend for larger combiner outputs giving better results, lines 3-4 (Table 4d).\nWe chose 32 as a trade-off between sufficiently compact feature length and sufficiently expressive.\n5\nCONCLUSIONS\nWe propose a multimodal autoregressive model which decouples the autoregressive modeling into a\ncomponent, devoted to time-aligned modalities (video, audio) and another one for the non-aligned,\ncontextual modalities (text). To address long video/audio inputs we propose to partition the media\ninputs and learn from them jointly by a Combiner, which allows to control the sequence lengths. The\nmodel can handle 512 frames, without increasing its size. Our approach not only enables working\nwith long videos effectively but also outperforms SOTA, achieving gains over much larger models.\n6\nETHICS STATEMENT\nThe proposed model is based on transformer model architectures, which are widely used and avail-\nable. The model described is trained on video/audio and text data which might be noisy and with\ninaccurate labeling. This might propagate inaccuracies or biases into the model. We have used the\nmodel for evaluation purposes and to compare to the established benchmarks in the field. We evalu-\nate on publicly available datasets, which have been previously used for evaluation and benchmarking\npurposes. No new datasets are proposed or studies with use of human subjects are conducted.\nACKNOWLEDGMENTS\nWe thank Claire Cui, Tania Bedrax-Weiss, Abhijit Ogale, Yunhsuan Sung, Ching-Chung Chang,\nMarvin Ritter, Kristina Toutanova, Ming-Wei Chang, Ashish Thapliyal, Xiyang Luo, Weicheng\nKuo, Aren Jansen, Bryan Seybold, Ibrahim Alabdulmohsin, Jialin Wu, Luke Friedman, Trevor\nWalker, Keerthana Gopalakrishnan, Jason Baldridge, Radu Soricut, Mojtaba Seyedhosseini, Alexan-\nder D\u2019Amour, Oliver Wang, Paul Natsev, Tom Duerig, Younghui Wu, Slav Petrov, Zoubin Ghahra-\nmani for their help and support.\nREFERENCES\nArmen Aghajanyan, Bernie Huang, Candace Ross, Vlad Karpukhin, Hu Xu, Naman Goyal, Dmytro\nOkhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. CM3: A causal\nmasked multimodal model of the internet. In ArXiv:2201.07520, 2022.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian\nBorgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo\nBarreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language\nmodel for few-shot learning, 2022. URL https://arxiv.org/abs/2204.14198.\n10\nRelja Arandjelovic and Andrew Zisserman. Look, listen and learn. In ICCV, 2017.\nKumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, and Kristen Grauman. Hiervl: Learning hier-\narchical video-language embeddings. In CVPR, 2023.\nShyamal Buch, Crist\u00b4obal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos\nNiebles.\nRevisiting the\u201d video\u201d in video-language understanding.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pp. 2917\u20132927, 2022.\nHonglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. VGG-Sound: A large-scale\naudio-visual dataset. In ICASSP, 2020.\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Car-\nlos Riquelme Ruiz, Xiao Wang Sebastian Goodman, Yi Tay, Daniel Salz Siamak Shakeri,\nMostafa Dehghani, Mario Lucic, Michael Tschannen, Arsha Nagrani, and Bo Pang Ceslee\nMontgomery Paulina Pietrzyk Marvin Ritter AJ Piergiovanni Matthias Minderer Filip Pavetic\nAustin Waters Gang Li Ibrahim Alabdulmohsin Lucas Beyer Julien Amelot Kenton Lee An-\ndreas Peter Steiner Yang Li Daniel Keysers Anurag Arnab Yuanzhong Xu Keran Rong Alexander\nKolesnikov Mojtaba Seyedhosseini Anelia Angelova Xiaohua Zhai Neil Houlsby Radu Soricut\nHexiang Hu, Mandar Joshi. Pali-x: On scaling up a multilingual vision and language model. In\nArXiv:2305.18565, 2023a.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Se-\nbastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, , Alexander Kolesnikov, Joan\nPuigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thap-\nliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan,\nCarlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Sori-\ncut. Pali: A jointly-scaled multilingual language-image model. In ICLR, 2023b.\nXilun Chen, Lili Yu, Wenhan Xiong, Barlas Oguz, Yashar Mehdad, and Wen-Tau Yih. Videoofa:\nTwo-stage pre-training for video-to-text generation. In arXiv:abs/2305.03204, 2023c.\nFeng Cheng, Xizi Wang, Jie Lei1 David Crandall, Mohit Bansal, and Gedas Bertasius. Vindlu : A\nrecipe for effective video-and-language pretraining. arXiv preprint arXiv:2212.05051, 2022.\nSixun Dong, Huazhang Hu, Dongze Lian, Weixin Luo, Yicheng Qian, and Shenghua Gao. Weakly\nsupervised video representation learning with unaligned text for sequential videos. In CVPR,\n2023.\nTsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng\nLiu. Violet: End-to-end video-language transformers with masked visual-token modeling. In\narXiv:2111.1268, 2021.\nTsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu.\nAn empirical study of end-to-end video-language transformers with masked visual modeling. In\nConference on Computer Vision and Pattern Recognition (CVPR), 2023.\nDifei Gao, Luowei Zhou, Lei Ji, Linchao Zhu, Yi Yang, and Mike Zheng Shou. Mist: Multi-modal\niterative spatial-temporal transformer for long-form video question answering.\nIn CVPR, pp.\n14773\u201314783, 2023.\nJort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing\nMoore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for\naudio events. In IEEE ICASSP, 2017.\nMariana-Iuliana Georgescu, Eduardo Fonseca, Radu Tudor Ionescu, Mario Lucic, Cordelia Schmid,\nand Anurag Arnab. Audiovisual masked autoencoders. arXiv preprint arXiv:2212.05922, 2022.\nYuan Gong, Alexander H. Liu, Andrew Rouditchenko, and James Glass. Uavm: Towards unifying\naudio and visual models. In IEEE SIGNAL PROCESSING LETTERS, 2022.\nYuan Gong, Andrew Rouditchenko, Alexander H. Liu, David Harwath, Leonid Karlinsky, Hilde\nKuehne, and James Glass. Contrastive audio-visual masked autoencoder. In ICLR, 2023.\n11\nPo-Yao Huang, Vasu Sharma, Hu Xu, Chaitanya Ryali, Haoqi Fan, Yanghao Li, Shang-Wen Li,\nGargi Ghosh, Jitendra Malik, and Christoph Feichtenhofer. Mavil: Masked audio-video learners.\nIn ArXiv:2212.08071.\nJaesung Huh, Jacob Chalk, Evangelos Kazakos, Dima Damen, and Andrew Zisserman.\nEPIC-\nSOUNDS: A Large-Scale Dataset of Actions that Sound. In IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP), 2023.\nAndrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira.\nPerceiver: General perception with iterative attention, 2021.\nBruno Korbar, Du Tran, and Lorenzo Torresani. Cooperative learning of audio and video models\nfrom self-supervised synchronization. In Adv. Neural Inform. Process. Syst., 2018.\nWeicheng Kuo, AJ Piergiovanni, Dahun Kim, Xiyang Luo, Ben Caine, Wei Li, Abhijit Ogale, An-\ndrew Dai Luowei Zhou, Zhifeng Chen, Claire Cui, and Anelia Angelova. MaMMUT: A simple\narchitecture for joint learning for multimodal tasks. In Transactions on Machine Learning Re-\nsearch, 2023.\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set\ntransformer: A framework for attention-based permutation-invariant neural networks. 2019.\nJie Lei, Tamara L. Berg, and Mohit Bansal. Revealing single frame bias for video-and-language\nlearnin. ArXiv:abs/2206.03428, 2022.\nDongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven Hoi. Align and prompt:\nVideo-and-language pre-training with entity prompts. In CVPR, 2022a.\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven\nChu Hong Hoi. Align before fuse: Vision and language representation learning with momentum\ndistillation. NeurIPS, 2021a.\nJunnan Li, Ramprasaath R. Selvaraju, Akhilesh D. Gotmare, Shafiq Joty, Caiming Xiong, and\nSteven C.H. Hoi. Align before fuse: Vision and language representation learning with momentum\ndistillation. In NeurIPS, 2021b.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip:\nBootstrapping language-\nimage pre-training for unified vision-language understanding and generation.\narXiv preprint\narXiv:2201.12086, 2022b.\nKunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked\nteacher: Towards training-efficient video foundation models. In ICCV, 2023.\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef\nSivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video\nclips. In ICCV, 2019.\nAntoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zis-\nserman. End-to-end learning of visual representations from uncurated instructional videos. In\nCVPR, 2020.\nArsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. Attention\nbottlenecks for multimodal fusion. In Adv. Neural Inform. Process. Syst., 2021.\nDaniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and\nQuoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition.\narXiv preprint arXiv:1904.08779, 2019.\nVictor Pellegrain, Myriam Tami, Michel Batteux, and C\u00b4eline Hudelot. Streamult: Streaming mul-\ntimodal transformer for heterogeneous and arbitrary long sequential data. In ArXiv:2110.08021,\n2021.\nLionel Pibre, Francisco Madrigal, Cyrille Equoy, Fr\u00b4ed\u00b4eric Lerasle, Thomas Pellegrini, Julien Pin-\nquier, and Isabelle Ferran\u00b4e. Audio-video fusion strategies for active speaker detection in meetings.\nIn ArXiv:2206.10411, 2022.\n12\nAJ Piergiovanni, Wei Li, Weicheng Kuo, Mohammad Saffar, Fred Bertsch, and Anelia An-\ngelova.\nAnswer-me: Multi-task open-vocabulary visual question answering.\narXiv preprint\narXiv:2205.00949, 2022a.\nAJ Piergiovanni, Kairo Morton, Weicheng Kuo, Michael Ryoo, and Anelia Angelova. Video ques-\ntion answering with iterative video-text co-tokenization. ECCV, 2022b.\nAJ Piergiovanni, Weicheng Kuo, and Anelia Angelova. Rethinking video vits: Sparse video tubes\nfor joint image and video learning. CVPR, 2023a.\nAJ Piergiovanni, Weicheng Kuo, Wei Li, and Anelia Angelova. Dynamic pretraining of vision-\nlanguage models. In First workshop on Multimodal Representation Learning, International Con-\nference on Learning Representations (ICLR), 2023b.\nScott Reed, Konrad \u02d9Zo\u0142na, Emilio Parisotto, Sergio G\u00b4omez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gim\u00b4enez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\nEccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,\nOriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. In ArXiv:2205.06175,\n2022.\nAndrew Rouditchenko, Angie Boggust, David Harwath, Brian Chen, Dhiraj Joshi, Samuel Thomas,\nKartik Audhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio Feris, Brian Kingsbury, Michael\nPicheny, Antonio Torralba, and James Glass. Avlnet: Learning audio-visual language representa-\ntions from instructional videos. In INTERSPEECH, 2021.\nMichael S. Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. To-\nkenlearner: Adaptive space-time tokenization for videos. In Adv. Neural Inform. Process. Syst.,\n2021.\nMichael S. Ryoo, Keerthana Gopalakrishnan, Kumara Kahatapitiya, Ted Xiao, Kanishka Rao,\nAustin Stone, Yao Lu, Julian Ibarz, and Anurag Arnab. Token turing machines. In CVPR, 2023.\nYuchong Sun, Hongwei Xue, Ruihua Song, Bei Liu, Huan Yang, and Jianlong Fu. Long-form video-\nlanguage pre-training with multimodal temporal contrastive learning.\nIn Adv. Neural Inform.\nProcess. Syst., 2022.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-Philippe Morency, and\nRuslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences. In\nArXiv:2110.08021, 2021.\nAaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, and Koray Kavukcuoglu\nAlex Graves. Conditional image generation with pixelcnn decoders. In arXiv:1606.05328, 2016.\nAlex Jinpeng Wang, Yixiao Ge, Rui Yan, Ge Yuying, Xudong Lin, Guanyu Cai, Jianping Wu, Ying\nShan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-\ntraining. arXiv preprint arXiv:2203.07303, 2022a.\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu,\nand Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv\npreprint arXiv:2205.14100, 2022b.\nJunke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie,\nCe Liu, Yu-Gang Jiang, and Lu Yuan. Omnivl:one foundation model for image-language and\nvideo-language tasks. Adv. Neural Inform. Process. Syst., 2022c.\nPeng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang,\nand Chang Zhou.\nOne-peace: Exploring one general representation model toward unlimited\nmodalities. In ArXiv:2305.11172, 2023.\nWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language:\nBeit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022d.\n13\nYi Wang, Kunchang Li, Yizhuo Li, Yinan He1, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan\nXu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang,\nand Yu Qiao. Internvideo: General video foundation models via generative and discriminative\nlearning. In arXiv preprint arXiv:2212.03191, 2022e.\nDirk Weissenborn, Oscar T\u00a8ackstr\u00a8om, and Jakob Uszkoreit. Scaling autoregressive video models. In\nICLR, 2020.\nChao-Yuan Wu and Philipp Krahenbuhl. Towards long-form video understanding. In CVPR, 2021.\nChao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, and Jitendra Malik amd\nChristoph Feichtenhofer. Memvit: Memory-augmented multiscale vision transformer for efficient\nlong-term video recognition. In ArXiv:2201.08383, 2023.\nChenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan\nDuan. Godiva: Generating open-domain videos from natural descriptions. In ArXiV:2104.14806,\n2021.\nChenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. Nuwa: Visual\nsynthesis pre-training for neural visual world creation. In ArXiv:2111.12417, 2022.\nJunbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.\nNext-qa:next phase of question-\nanswering to explaining temporal actions. CVPR, 2021.\nJunbin Xiao, Pan Zhou, Tat-Seng Chua, and Shuicheng Yan. Video graph transformer for video\nquestion answering. In European Conference on Computer Vision, pp. 39\u201358. Springer, 2022.\nHaiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi,\nQi Qian, Wei Wang, Guohai Xu, Ji Zhang, Songfang Huang, Fei Huang, and Jingren Zhou.\nmplug-2: A modularized multi-modal foundation model across text, image and video. In arXiv\npreprint arXiv:2302.00402, 2023.\nJun Xu, Tao Mei, Ting Yao, , and Yong Rui. Msr-vtt: A large video description dataset for bridging\nvideo and language. In CVPR, 2016.\nShen Yan, Tao Zhu, ZiRui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, and Jiahui\nYu. Videococa: Video-text modeling with zero-shot transfer from contrastive captioners. In\nArXiV:2212.04979, 2022.\nWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using\nvq-vae and transformers. ArXiv:abs/2104.10157, 2021.\nAntoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to\nanswer questions from millions of narrated videos. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 1686\u20131697, 2021.\nAntoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video\nquestion answering via frozen bidirectional language models. ArXiv:abs/2206.08155, 2022.\nQinghao Ye, Guohai Xu, Ming Yan, Haiyang Xu, Qi Qian, Ji Zhang, and Fei Huang. Hitea: Hierar-\nchical temporal-aware video-language pre-training. arXiv preprint arXiv:2212.14546, 2022.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun\nBabu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes,\nVasu Sharma, Puxin Xu, Hovhannes Tamoyan1, Oron Ashual, Uriel Singer, Shang-Wen Li,\nSusan Zhang Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke\nZettlemoyer, and Armen Aghajanyan.\nScaling autoregressive multi-modal models: Pretrain-\ning and instruction tuning. In https://ai.meta.com/research/publications/scaling-autoregressive-\nmulti-modal-models-pretraining-and-instruction-tuning/, 2023.\nZhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-\nQA: A dataset for understanding complex web videos via question answering. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, 2019.\n14\nRowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and\nYejin Choi. Merlot: Multimodal neural script knowledge models. In Adv. Neural Inform. Process.\nSyst., 2021.\nRowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya\nKusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge\nthrough vision and language and sound. In CVPR, 2022.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\nrisk minimization. arXiv preprint arXiv:1710.09412, 2017.\nWentao Zhu, Jingru Yi, Xiaohang Sun, Xiang Hao, Linda Liu, and Mohamed Omar. Multiscale\nmultimodal transformer for multimodal action recognition. In ICLR, 2022.\nA\nAPPENDIX\nA.1\nDATASETS DETAILS\nThe following datasets have been used for evaluation in the paper:\nMSRVTT-QA (Xu et al., 2016) is a popular Video QA dataset of about 10K video clips and 243K\nquestion-answer pairs. It is derived from the MSRVTT dataet by automatic question-answer pairs\nand contains a certain level of noise. Videos are about 14 seconds in length, on average.\nActivityNet-QA (Yu et al., 2019) is a commonly used benchmark for understanding of longer videos.\nIt contains 5,800 videos and 58,000 question-answer pairs. It has much longer videos which entail\nlonger and more complex scenes. The video length is about 160 seconds per video on average.\nNExT-QA (Xiao et al., 2021) dataset is also addressing long video understanding. It contains 5,440\nvideos and about 52K manually annotated question-answer pairs. The average length of the videos\nis 44 seconds. Apart from questions related to descriptions and in the video, NExT-QA focuses on\nquestions related to events and sequence of events within the video, e.g., causal (\u2018Why\u2019 and \u2018How\u2019\nquestions), and temporal - questions related to order of events, or related to concurrent activities and\nothers.\nVGG-Sound (Chen et al., 2020) is a large-scale audio-video dataset, featuring over 200,000 videos\naccompanied by audio sounds. The data is formulated as classification tasks with 300 audio classes.\nEpic-Sound (Huh et al., 2023) is an audio-video dataset based on the Epic-Kitchens dataset. It has\n78.4k examples and 44 target classes.\nKinetics-Sound (Arandjelovic & Zisserman, 2017) is a dataset derived from the popular Kinetics-\n400 video recognition dataset. Kinetics-Sound includes audio inputs sampled together with the\nvideo and has 36 classes.\nAll the abovementioned audio-video datasets used in the paper, have been formulated as datasets for\nclassification tasks. Here we use the class outputs (which are typically short phrases describing an\nactivity, instrument or type of sound e.g \u2019Knocking on a door\u2019) and treat them as open-ended text\ngeneration tasks and thus they are now audio-video-text datasets.\nA.2\nCOMBINER VISUALIZATIONS.\nIn Figure 3, we visualize the different combiners we explored. The Transformer combiner, CLS\ncombiner and Perceiver combiner are all based on transformers taking input of all the video + audio\nfeatures and reducing them to m combined features. We found our main combiner to outperform\nthe other two in Table 4b. We note that the Perceiver combiner is an adaptation of our combiner by\napplying Perceiver resampling (Jaegle et al., 2021). The TTM combiner is conceptually different:\nrather than taking all the previous features as input, it takes only the current timestep features as\ninput and uses a memory mechanism with read and write operations to update it. It then uses a MLP\nto produce the m combined output features. This reduces memory and compute use and sometimes\nreduces accuracy.\n15\nA.3\nADDITIONAL MODEL AND IMPLEMENTATION DETAILS\nModel Details. The autoregressive text model contains about 1.3B parameters, 400M are for cross-\nattention weights and 400M for the vocab embeddings and following specifications: layers=18,\nmodel dims=1536, hidden dims=12288, heads=12, and head dims=128. About 100M parameters are\nfor the additional weights associated with audio. The remaining parameters are for the video input\nprocessor, combiner, causal latent model and video reconstruction model (a bit over 1.5B parameters\nin total). The combiner, causal latent model and video reconstruction model are transformers with\n128M parameters and the following specifications: layers=8, model dims=1024, hidden dims=4096,\nheads=16, and head dims=64. The video chunk processor has roughly 630M parameters, following\nViT-Huge. The convolutional tubes have 1.5M parameters and the transformer has 630M parameters\nand following specifications: layers=32, model dims=1280, hidden dims=5120, heads=16, and head\ndims=80. The total parameter size is 3B parameters.\nThe smaller model used for ablations keeps the same combiner, causal latent model, and video re-\nconstruction model as the main model. However the autoregressive text model is reduced to 128M\nparameters with the same settings as the combiner, and has 20M cross-attention weights and 260M\nparameters for the vocab embedding. The audio parameters are held roughly the same. The video\ninput processor is reduced to ViT-Large which has 300M parameters and the following specifica-\ntions: layers=24, model dims=1024, hidden dims=4096, heads=16, and head dims=80. The total\nparameter size is 1.15B parameters.\nThe TTM Combiner, as mentioned is implemented by a TokenLearner (Ryoo et al., 2021) function\nand a transformer. The output dimension K = 32 is the same as the output dimension for the\nstandard Transformer Combiner. The output dimensions for the \u2018Read\u2019 and \u2018Write\u2019 functions are\n512 and 256, respectfully. These two parameters can be controlled independently to allow more\nor less capacity to the TTM Combiner. The transformer used within the \u2018Process\u2019 function is of 2\nlayers, 128 hidden dimension and 12 heads. These are fixed throughout the paper.\nModel Pretraining. The pretraining data is the Video-Text Pairs (VTP) dataset which is collected\nfrom noisy video-text pairs from the web (Alayrac et al., 2022). The main pretraining is done for\nthe autoregressive, combiner, and the learning components processing the low-level video features\n(e.g., video tubes convolutions). The model\u2019s image and text backbones and cross attention layers\nare initialized from a contrastively image-text pretrained model, as in CLIP. The text backbone\nis frozen during pretraining while the other components including the cross attention weights are\nunfrozen. During pretraining, the combiner model, causal latent reconstruction model and video\nreconstruction model and video tubes are all randomly initialized. All losses are given equal weight\nduring pretraining. For pretraining, we used a learning rate of 1 \u00d7 10\u22125, batch size of 32, image\nresolution of 224 \u00d7 224, 128 frames.\nFine-tuning. During finetuning all parameters are unfrozen. In addition the unaligned text loss is\ngiven extra weight and increased 10-fold to better align the training loss with the final evaluation,\nsince the latent space and video reconstruction are not evaluated. The model is trained for 10 epochs\nfor the MSRVTT-QA dataset and for 80 epochs on ActivityNet-QA and 20 epochs on NExT-QA. For\nthese datasets, we finetune with a learning rate of 5 \u00d7 10\u22126, weight decay of 0.01, image resolution\nof 448 \u00d7 448, batch size of 32. We use 128 frames for the main experiments, except for the long\nvideo benchmarks where we also report performance with 512. Sampling more frames from the\nother benchmarks is not porductive as they contain relatively short videos. We used dropout of 0.1,\nlabel smoothing of 0.2\nVideo-Audio Implementation Details. Since the model is pretrained on VTP data which most\nvideos lack audio, we try adding a further audio pretraining step here. We use AudioSet-2M (Gem-\nmeke et al., 2017) and train the model to output the text of the class names. In this step, we freeze\nthe weights of the model, other than the audio weights, allowing the model to learn to handle the\nspectrogram inputs. During fine-tuning on the eval datasets, we fully train the model. During fine-\ntuning, we also use Mixup (Zhang et al., 2017), specaugment (Park et al., 2019), dropout and label\nsmoothing, following the settings of previous works (e.g., (Georgescu et al., 2022)). We use a learn-\ning rate of 1 \u00d7 10\u22125, with the Adam optimizer (default settings), weight decay of 0.0001, cosine\nlearning rate decay. We use an image resolution of 448 \u00d7 448, batch size of 32, and 128 frames.\nAblation experiments details. The ablation experiments in Tables 4a, 4b, 4c, 4d are conducted with\nour small model. The Baseline in Table 4a uses partitioning, as the rest of the approaches tested in\n16\nCombiner\nVideo \nEnc.\nAudio  \nEnc.\nVideo, audio features \nper timestep\n    Video     Audio\nsive Model\nTransformer\n(c) CLS combiner. Adds m features to the \ninput, then after the transformer, takes those \nm features as the combined features.\nTransformer\n(a) Transformer combiner. All features are \ninput to the transformer, a smaller number of \nm features are selected as combined features.\n(b) TTM Combiner. Uses the TTM \nmechanism to store a memory and compute \nthe m combined features for each time step.\nThis process is repeated for each time step.\nVideo + \nAudio features\nVideo + Audio features\nCLS features\nCross-Attn Transformer\n(d) Perceiver combiner. Adds m latent \nfeatures for cross attention to the transformer, \nresulting in m outputs.\nK,V: Video + Audio features\nQ: m latent \nfeatures\nMemory\nRead\nProcess\nWrite\nSingle step Video + Audio features\nMLP\nFeature selection\nFigure 3: Visualization of the different combiners we explored in this paper. The Transformer\ncombiner, which is the main one we used, simply takes the last m features of the output to represent\nthe combined inputs. We found this to work well. The CLS combiner and Perceiver combiner we\nfound both underperformed the base combiner. The TTM combiner is different, it uses a memory to\nstore the previous representations and has read, process and write operations. We found this method\nsaved memory with some tradeoff for accuracy for some datasets.\nthe table, and concatenation of the features to be maximally comparable to others. The ablations in\nTable 4b use a slightly lower learning rate than the others.\n17\n"
  },
  {
    "title": "Hiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems",
    "link": "https://arxiv.org/pdf/2311.05884.pdf",
    "upvote": "5",
    "text": "Hiformer: Heterogeneous Feature Interactions Learning with\nTransformers for Recommender Systems\nHuan Gui\nGoogle DeepMind\nMountain View, California, USA\nhgui@google.com\nRuoxi Wang\nGoogle DeepMind\nMountain View, California, USA\nruoxi@google.com\nKe Yin\nGoogle Inc\nMountain View, California, USA\nkeyin@google.com\nLong Jin\nGoogle Inc\nMountain View, California, USA\nlongjin@google.com\nMaciej Kula\nGoogle DeepMind\nMountain View, California, USA\nmaciejkula@google.com\nTaibai Xu\nGoogle Inc\nMountain View, California, USA\ntaibaixu@google.com\nLichan Hong\nGoogle DeepMind\nMountain View, California, USA\nlichan@google.com\nEd H. Chi\nGoogle DeepMind\nMountain View, California, USA\nedchi@google.com\nABSTRACT\nLearning feature interaction is the critical backbone to building\nrecommender systems. In web-scale applications, learning feature\ninteraction is extremely challenging due to the sparse and large\ninput feature space; meanwhile, manually crafting effective fea-\nture interactions is infeasible because of the exponential solution\nspace. We propose to leverage a Transformer-based architecture\nwith attention layers to automatically capture feature interactions.\nTransformer architectures have witnessed great success in many\ndomains, such as natural language processing and computer vi-\nsion. However, there has not been much adoption of Transformer\narchitecture for feature interaction modeling in industry. We aim\nat closing the gap. We identify two key challenges for applying\nthe vanilla Transformer architecture to web-scale recommender\nsystems: (1) Transformer architecture fails to capture the hetero-\ngeneous feature interactions in the self-attention layer; (2) The\nserving latency of Transformer architecture might be too high to\nbe deployed in web-scale recommender systems. We first propose\na heterogeneous self-attention layer, which is a simple yet effec-\ntive modification to the self-attention layer in Transformer, to take\ninto account the heterogeneity of feature interactions. We then\nintroduce Hiformer (Heterogeneous Interaction Transformer) to\nfurther improve the model expressiveness. With low-rank approx-\nimation and model pruning, Hiformer enjoys fast inference for\nonline deployment. Extensive offline experiment results corrobo-\nrates the effectiveness and efficiency of the Hiformer model. We\nhave successfully deployed the Hiformer model to a real world\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nConference\u201917, July 2017, Washington, DC, USA\n\u00a9 2023 Association for Computing Machinery.\nACM ISBN 123-4567-24-567/08/06...$15.00\nhttps://doi.org/10.475/123_4\nlarge scale App ranking model at Google Play, with significant\nimprovement in key engagement metrics (up to +2.66%).\nKEYWORDS\nHeterogeneous Feature Interaction, Transformer, Recommender\nSystem\nACM Reference Format:\nHuan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan\nHong, and Ed H. Chi. 2023. Hiformer: Heterogeneous Feature Interactions\nLearning with Transformers for Recommender Systems. In Proceedings\nof ACM Conference (Conference\u201917). ACM, New York, NY, USA, 10 pages.\nhttps://doi.org/10.475/123_4\n1\nINTRODUCTION\nThe internet is inundated with a plethora of information, making\nit challenging for users to effectively navigate and locate relevant\ncontent. Recommender systems filter information and present the\nmost relevant content to individual users [35, 49]. It is common to\nformulate recommender systems as a supervised machine learning\nproblem, with the goal of increasing user\u2019s positive engagements\nwith the recommendations, such as clicks [40, 45, 46], watches [50],\nor purchases [36]. Therefore, the deployment of recommender sys-\ntems with high prediction accuracy is of paramount importance as\nit could have a direct influence on business\u2019 financial performance,\nsuch as sales and revenue [3\u20135, 49].\nFeature interactions are where multiple features have complex\ncollaborative effects on the prediction of an outcome [43, 46]. It\nis one of the crucial components of recommender system. For ex-\nample, it\u2019s observed that users frequently download food delivery\napps during meal times [8], indicating the feature interactions of\napp ids (e.g. food delivery apps) and temporal context (e.g. meal\ntimes) can provide vital signal for predicting user behavior and\nmaking personalized recommendations. However, modeling fea-\nture interactions in web-scale recommender systems presents three\nsignificant challenges. First of all, the right feature interactions are\noften domain-specific, manually crafting the feature interactions\nis time-consuming and requires domain knowledge. Secondly, for\narXiv:2311.05884v1  [cs.IR]  10 Nov 2023\nConference\u201917, July 2017, Washington, DC, USA\nHuan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong, and Ed H. Chi\nweb-scale applications, due to the large number of raw features,\nthe search of possible feature interactions has an exponential so-\nlution space. This makes it infeasible to manually extract all the\ninteractions. Last but not least, the extracted feature interactions\nmay not generalize well to other tasks. Due to its importance, fea-\nture interaction continues to attract increasing attention from both\nacademic and industry [4, 5, 27, 33, 49].\nIn recent years, deep neural networks (DNNs) have garnered\nsignificant attention in various research fields, due to their excep-\ntional model performance as well as their ability in representation\nlearning [22, 49]. DNNs have been widely adopted for recommender\nsystem for sparse input features representation learning [30] and\nfeature interaction learning [1, 3, 8, 26, 29, 32, 40, 45, 46, 49]. It has\nbecome a powerful tool to map the large and sparse input into a\nlow-dimensional semantic space. Some of the recent works [8, 24,\n29, 32, 46] in feature interaction learning are based on explicit fea-\nture interaction functions, such as Factorization Machine [26, 34],\ninner product [32], kernel Factorization Machine [33], and Cross\nNetwork [45, 46]. DNNs are also leveraged to learn the implicit\nfeature interactions with multiple layers of artificial neurons with\nnon-linear activation functions [3, 29].\nAnother line of work is based on the attention mechanism, in\nparticular, the Transformer-based architecture [44] for feature in-\nteraction learning. The Transformer architecture has become the\nstate-of-the-art (SOTA) model for a variety of tasks, including com-\nputer vision (CV) [9, 21] and natural language processing (NLP) [19].\nAutoInt [40] and InterHAt [25] proposed to leverage the multi-head\nself-attention layer for feature interaction learning. However, un-\nlike the NLP field, the feature semantics in recommender systems\nare dynamic under different contexts.\nSuppose we are recommending food delivery apps to users and\nwe have the features: app_id, hour_of_day, and user_country.\nIn this case, app_id could mean one thing for hour_of_day and\nanother for user_country. To effectively interact different features,\nwe would need semantic awareness and semantic space alignment.\nTransformer models unfortunately do not consider this. In the\nvanilla attention layer, the features are projected through the same\nprojection matrices (i.e., WQ and Wk) that are shared across all\nfeatures. This is a natural design for applications where the feature\n(text token) semantics are independent of the context; however\nfor recommender systems where the feature semantic are often\ndependent of the context, this homogeneous design would lead to\nlimited model expressiveness for feature interaction learning.\nBesides the limitation of not being able to capture heterogeneous\nfeature interactions, another limitation of Transformer models is\non latency. In web-scale applications, there are frequently a vast\nnumber of users and requests. It is crucial that the models are capa-\nble of managing an extremely high number of queries per second,\nto deliver an optimal user experience. However, the inference cost\nof Transformer architectures scales quadratically with the input\nlength, which makes real-time inference using Transformer archi-\ntectures prohibitive in web-scale recommender systems.\nDespite the limitations, we deem it important to study and im-\nprove Transformer-based architecture for feature interaction. First\nof all, due to the success of the Transformer architecture in many\ndomains, a significant number of advancements and modifications\nhave been made to the architecture to improve its performance\nand capabilities. Enabling Transformer-based feature interaction\nmodels for recommender system can serve as a bridge, to allow\nfor the recent advancements in Transformer architectures to be ap-\nplied in the recommendation domain. Secondly, with its widespread\nadoption, the hardware design (e.g., chip design [28]) might favor\nTransformer-like architectures. Hence, recommender systems with\nTransformer-based architectures could also enjoy the optimizations\nbrought by hardware. Thirdly, as Transformer models for feature\ninteraction learning are based on the attention mechanism, they\nprovide good model explainability [40]. Building recommender\nmodels with attention mechanism in web-scale applications might\nopen up new research opportunities.\nIn this paper, we consider the limitations of vanilla Transformer\nmodel architectures and propose a new model: Hiformer (Heterog-\neneous Feature Interaction Transformer) for feature interaction\nlearning in recommender systems. Hiformer is feature semantic\naware and, more importantly, efficient for online deployment. To\nprovide feature semantic awareness in feature interaction learning,\nwe design a novel heterogeneous attention layer. To comply with\nthe stringent latency requirements of web-scale applications, we\nleverage low-rank approximation and model pruning techniques\nto improve the efficiency.\nWe use a large-scale App ranking model at Google Play as a\ncase study. We perform both offline and online experiments with\nHiformer to corroborate its effectiveness in feature interaction\nmodeling and efficiency in serving. To the best of our knowledge,\nwe show for the first time that a Transformer-based architecture\n(i.e., Hiformer ) can outperform SOTA recommendation models\nfor feature interaction learning. We have successfully deployed\nHiformer as the production model.\nTo summarize, the contributions of the paper are as follows:\n\u2022 We propose the Hiformer model with a novel heterogeneous\nattention layer to capture the complex collaborative effects\nof features in feature interaction learning. Compared with\nexisting vanilla Transformer based approaches, our model\nhas more model expressiveness.\n\u2022 We leverage low-rank approximation and model pruning to\nreduce the serving latency of the Hiformer model, without\ncompromising the model quality.\n\u2022 We conduct extensive offline comparison with a web-scale\ndataset to demonstrate the importance of capturing hetero-\ngeneous feature interactions. We show that hiformer, as\na Transformer-based architecture, can outperform SOTA\nrecommendation models.\n\u2022 We perform online A/B testing to measure the impact of\ndifferent models on key engagement metrics, and the Hi-\nformer model shows significant online gain compared with\nthe baseline model with limited latency increase.\n2\nPROBLEM DEFINITION\nThe goal of recommender system is to improve users\u2019 positive\nengagement, such as clicks, conversion, etc. Similar to previous\nstudies [26, 27], we formulate it as a supervised machine learning\ntask for engagement predictions.\nDefinition 2.1 (Recommender System). Let x \u2208 R\ud835\udc51\ud835\udc65 denote the\nfeatures of a (user, item) pair. There are categorical features xC\nHiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems\nConference\u201917, July 2017, Washington, DC, USA\nTask Token\nCategorical Features\n\u2026\nDense Scalar Features\n\u2026\nLearned \nembedding\nEmbedding \nLook-up\nEmbedding \nLook-up\nNormalize, Concat, & Project\nemb\nemb\nemb\nemb\nemb\nemb\nemb\nemb\nemb\nemb\n\u2026\nLearned \nembedding\nemb\nemb\nTask 1\nTask 2\nTask 1\nTask 2\nInput Layer\nPreprocessing Layer\nOutput Layer\nFeature Interaction Layer\n\u2026\n\u2026\nFigure 1: There are four components in the framework: Input\nLayer, Preprocessing Layer, Feature Interaction layer, and\nOutput Layer. We leverage a novel Hiformer model for het-\nerogeneous feature interaction learning.\nand dense features xD. Categorical features are represented with\none-hot encoding. The dense features can be considered as special\nembedding features with embedding dimension being 1. The num-\nbers of categorical and dense features are |C| and |D| respectively.\nThe problem of recommender system is to predict if the user will\nengage with the item based on input features x.\nAs aforementioned, feature interaction learning enables the\nmodel to capture more complex relationships among features, thereby\nprovides virtual information for more accurate recommendations.\nWe formally define heterogeneous feature interaction learning.\nDefinition 2.2 (Heterogeneous \ud835\udc67-order Feature Interaction). Given\ninput with |F | features and the input as x = {x\ud835\udc56}| F|\n\ud835\udc56=1, a \ud835\udc67-order\nfeature interaction is to learn a unique non-additive mapping func-\ntion \ud835\udf0c Z(\u00b7) that map from the feature list Z with \ud835\udc67 features (i.e.,\n[x\ud835\udc561, \u00b7 \u00b7 \u00b7 x\ud835\udc56\ud835\udc67]) to a new representation or a scalar to capture the\ncomplex relationships among the the feature list Z. When z = 2,\n\ud835\udf0c Z(\u00b7) captures the second order feature interactions.\nFor example, \ud835\udc54([x1, x2]) = \ud835\udc641x1 + \ud835\udc642x2 is not a 2-order feature\ninteraction, as \ud835\udc641x1 + \ud835\udc642x2 is additive. Meanwhile, \ud835\udc54([x1, x2]) =\nx1\ud835\udc47 x2 is a 2-order feature interaction; and \ud835\udf0c1,2(\u00b7) = \ud835\udf0c2,1(\u00b7) = x\ud835\udc47\n1 x2,\nmeaning the heterogeneous feature interaction is symmetric.\nBefore diving into model details, we define our notations. Scalars\nare denoted by lower case letters (\ud835\udc4e,\ud835\udc4f, ...), vectors by bold lower\ncase letters (a, b, ...), matrices by bold upper case letters (A, B, ...).\n3\nMODEL\nWe will first present the overall framework in Section 3.1. We then\npropose a heterogeneous attention layer in Section 3.3, based on\nwhich we further introduce the Hiformer model in Section 3.4.\n3.1\nOverview\n3.1.1\nInput Layer. The input layer includes the categorical features,\nand dense features. Additionally, we have the task embedding. The\ntask embedding can be regarded as the CLS token [6]. The pro-\nposed framework can also be easily extended to multi-task [2] with\nmultiple task embeddings, and each task embedding represents\ninformation of the corresponding training objective. The task em-\nbeddings in the input layers are model parameters and are learned\nthrough the end-to-end model training. We denote the number of\ntask embeddings as \ud835\udc61.\n3.1.2\nPreprocessing Layer. We have the preprocessing layer to\ntransform the input layer into a list of feature embeddings, in ad-\ndition to the task embeddings, as input to the feature interaction\nlayers. We have one preprocessing layer per feature type.\nCategorical Features. The input categorical features are often\nsparse and of high dimensions. Directly using the one-hot encoding\nfor model training would easily lead to model overfitting. Therefore,\na common strategy is to project the categorical features into a low-\ndimension dense space. In particular, we learn a projection matrix\nWC\n\ud835\udc56 \u2208 R\ud835\udc49\ud835\udc56 \u00d7\ud835\udc51 related to each categorical feature, and\ne\ud835\udc56 = xC\n\ud835\udc56 WC\n\ud835\udc56 ,\nwhere e\ud835\udc56 \u2208 R\ud835\udc51, xC\n\ud835\udc56\n\u2208 {0, 1}\ud835\udc49\ud835\udc56 , \ud835\udc49\ud835\udc56 is dimensions of the one-hot\nencoding for feature xC\n\ud835\udc56 , and \ud835\udc51 is the model dimension. We will\nthen use the dense embedded vector e\ud835\udc56 as the representation of\nthe categorical feature \ud835\udc65 C\n\ud835\udc56 . {WC\n\ud835\udc56 }| C| are the model parameters and\nreferred to as embedding look-up tables.\nDense Scalar Features. The dense scalar features are numerical\nfeatures. As the dense scalar features are potentially from very\ndifferent distributions, we will need to transform the features to a\nsimilar distribution (i.e., uniform distribution) to ensure numerical\nstability [51]. Additionally, in web-scale applications, there are a\nlarge number of dense scalar features, with much less compared\nwith the categorical features. Therefore, we aggregate all the dense\nscalar features and project them into \ud835\udc5bD embeddings [29], where\n\ud835\udc5bD is a hyper-parameter chosen by empirical results, \ud835\udc5bD \u226a |D| to\nreduce the total number of features. The projection function \ud835\udc53D (\u00b7)\ncould be a multilayer perceptron (MLP) with nonlinear activations:\ne\ud835\udc56 = split\ud835\udc56\n\u0012\n\ud835\udc53D\n\u0012\nconcat\u0000normalize({\ud835\udc65 D\n\ud835\udc56 })\u0001\u0013\n, split_size = \ud835\udc51\n\u0013\n,\nwhere e\ud835\udc56 \u2208 R\ud835\udc51, concat(\u00b7) is the concatenation function, normalize(\u00b7)\nis the feature transformation function, and split\ud835\udc56 (\u00b7, split_size)\nis a function to split the input tensor into tensors with equal di-\nmension of split_size and takes the \ud835\udc56-th tensor as output. In\nTransformer-based architectures, the inference cost scales quadrat-\nically with the length of input embedding list. By aggregating |D|\ndense scalar features into \ud835\udc5bD embeddings, the length of feature\nembeddings is reduced, and as a consequence, the inference cost is\nalso reduced, albeit with a trade-off of adding \ud835\udc53\ud835\udc37 (\u00b7).\nTo summarize on preprocessing layers, we transform the |C|\ncategorical features, |D| dense scalar features, and the \ud835\udc61 task em-\nbeddings into a list of embeddings with the model dimension \ud835\udc51.\nThe number of output embeddings from preprocessing layers is\n\ud835\udc3f = |C| + \ud835\udc5bD +\ud835\udc47. For ease of discussion, the task embeddings are\nalso regarded as special features in the embedding list. Therefore,\nfeature interactions refers to not only interaction among features,\nbut also between features and tasks.\n3.1.3\nFeature Interaction Layer. The feature interaction layer is\nused to learn the complex collaborative effects among features,\nConference\u201917, July 2017, Washington, DC, USA\nHuan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong, and Ed H. Chi\nwhich is crucial to capture users\u2019 preferences under different con-\ntexts. The input to the feature interaction layer is the output em-\nbedding list of preprocessing layers.\n3.1.4\nOutput Layer and Training Objectives. We only use the en-\ncoded task embeddings from the feature interaction layer for task\nprediction. We train a MLP tower to project the encoded task em-\nbedding to final predictions. The training tasks can be flexibly\nconfigured based on the recommendation application. For exam-\nple, the training task can be a classification task if the labels are\nclick [39, 40], installation [15, 47], or purchase [23, 31], and may\nalso be a regression task, such as in the case of watch time [50].\nWithout loss of generality, we consider classification task, with a\nvalue of 0 indicating negative engagement and 1 indicating positive\nengagement. Thus, the training objective can be formulated as a\nbinary classification problem with the Binary Cross Entropy (i.e.,\nLog Loss):\n\u2113 = 1\n\ud835\udc41\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc56\n\u2212\ud835\udc66\ud835\udc56 log(\ud835\udc5d\ud835\udc56) \u2212 (1 \u2212 \ud835\udc66\ud835\udc56) log(1 \u2212 \ud835\udc5d\ud835\udc56),\nwhere \ud835\udc66\ud835\udc56 and \ud835\udc5d\ud835\udc56 are the ground truth label and predicted probabil-\nity of engagements for example \ud835\udc56 respectively, and \ud835\udc41 is the total\nnumber of training examples.\n3.2\nHeterogeneous Feature Interactions\nThe Transformer architecture has become the de-facto standard for\nmany tasks such as NLP, CV, etc., and multi-head self-attention layer\nhas achieved remarkable performance in modeling complicated re-\nlations in sequences [9, 19, 21, 44]. Nevertheless, the Transformer\narchitecture has not been extensively applied to web-scale rec-\nommender systems. It has been observed that Transformer-based\narchitectures, such as AutoInt [40], may not demonstrate optimal\nperformance when applied to web-scale recommender systems.\nRecall that the attention score computation of vanilla Trans-\nformer, the parameterization of feature interactions learning are\nshared across all feature groups in the multi-head self-attention\nlayer. This parameter sharing design comes naturally in NLP tasks,\nas the input to a Transformer in NLP tasks is usually a sentence\nrepresented by a list of text embeddings. Text embeddings primarily\nencode information that is context independent. We refer this setup\nas homogeneous feature interactions. The detailed illustration of\nthe Transformer architecture is shown in Figure 4(a).\nBy sharp contrast, there is diverse contextual information in\nrecommender systems. The learned embedding representations are\nmore dynamic and encompass information from multiple perspec-\ntives. In the food delivery app recommendation example, we have\nthree features: app_id (e.g., Zomato), hour_of_day (e.g., 12pm),\nand user_country (e.g., India). The learned embedding represen-\ntation for the app Zomato encodes various implicit information,\nincluding the nature of the app, the countries in which it is pop-\nular, the times at which it is frequently used, and the language\nof its users, etc. Not all the encoded information would be use-\nful during certain feature interaction learning. When learning the\nfeature interaction between app_id and user_country, likely the\ninformation of the countries in which the app is popular is much\nmore important than the others. Thus, the core of heterogeneous\nfeature interaction learning is to provide contextual awareness for\ninformation selection and transformation, to achieve more accurate\nfeature interaction learning.\n3.3\nHeterogeneous Attention Layer\nWe first introduce a simple but effective modification to the multi-\nhead self-attention layer in the Transformer model: the heteroge-\nneous multi-head self-attention layer, to capture the heterogeneous\nsecond-order feature interactions. For simplicity, we refer to the het-\nerogeneous multi-head self-attention layer as the heterogeneous\nattention layer, and the corresponding model as heterogeneous\nattention model. In particular, we redesign the multi-head self-\nattention score computation for feature \ud835\udc56 and \ud835\udc57:\nAtt(\ud835\udc56, \ud835\udc57)\u210e =\nexp\u0000\ud835\udf19\u210e\n\ud835\udc56,\ud835\udc57 (e\ud835\udc56, e\ud835\udc57)\u0001\n\u00cd\ud835\udc3f\n\ud835\udc5a=1 exp\u0000\ud835\udf19\u210e\n\ud835\udc56,\ud835\udc5a(e\ud835\udc56, e\ud835\udc5a)\u0001 ,\n(1)\nwhere \u210e refers to the \u210e-th head among total \ud835\udc3b heads, \ud835\udc3f is the\nembedding list length, \ud835\udf19\u210e\n\ud835\udc56,\ud835\udc57 (e\ud835\udc56, e\ud835\udc57) measures semantic correlation\nbetween embedding e\ud835\udc56 and e\ud835\udc57 regarding head \u210e. We will then refer\nthe vanilla self-attention layer as homogeneous attention layer.\nFor every feature pair (\ud835\udc56, \ud835\udc57), we have a unique\ud835\udf19\u210e\n\ud835\udc56,\ud835\udc57 (\u00b7, \u00b7) to measure\nsemantic correlation. In other words, \ud835\udf19\u210e\n\ud835\udc56,\ud835\udc57 (\u00b7, \u00b7) serves the purpose\nof selecting the most relevant information from e\ud835\udc56 and e\ud835\udc57 for the\ncontext of feature pair (\ud835\udc56, \ud835\udc57). We can have arbitrary functions for\n\ud835\udf19\u210e\n\ud835\udc56,\ud835\udc57 (\u00b7, \u00b7) : [R\ud835\udc51, R\ud835\udc51] \u2192 R, such as some explicit non-additive func-\ntions, a neural network with nonlinear transformation. Due to its\nsimplicity, we opt for the dot-product function. Accordingly,\n\ud835\udf19\u210e\n\ud835\udc56,\ud835\udc57 (e\ud835\udc56, e\ud835\udc57) =\ne\ud835\udc56Q\u210e\n\ud835\udc56 (e\ud835\udc57K\u210e\n\ud835\udc57 )\ud835\udc47\n\u221a\ufe01\n\ud835\udc51\ud835\udc58\n,\n(2)\nwhere Q\ud835\udc56 \u2208 R\ud835\udc51\u00d7\ud835\udc51\ud835\udc58 , K\ud835\udc57 \u2208 R\ud835\udc51\u00d7\ud835\udc51\ud835\udc58 are the query and key projections\nfor feature \ud835\udc57 and \ud835\udc57 respectively, and\n\u221a\ufe01\n\ud835\udc51\ud835\udc58 is to normalize magnitude\nof the dot product, which is often set to be \ud835\udc51\ud835\udc58 = \ud835\udc51/\ud835\udc3b.\nWith the attention weights computed in Eq (1), we can then\ncompute the output of the heterogeneous attention layer as follows:\no\ud835\udc56 = concat\n\u0012\b \u2211\ufe01\n\ud835\udc57\nAtt(\ud835\udc56, \ud835\udc57)\u210ee\ud835\udc57V\u210e\n\ud835\udc57\n\t\ud835\udc3b\n\u210e=1\n\u0013\nO\ud835\udc57,\n(3)\nwhere V\ud835\udc57 \u2208 R\ud835\udc51\u00d7\ud835\udc51\ud835\udc63, O\ud835\udc57 \u2208 R\ud835\udc3b\ud835\udc51\ud835\udc63\u00d7\ud835\udc51 as value and output projections,\nand \ud835\udc51\ud835\udc63 is frequently set as \ud835\udc51\ud835\udc63 = \ud835\udc51/\ud835\udc3b.\nSimilar to the heterogeneous attention layer, we also design a\nheterogeneous fully connected Feed Forward Net (FFN) per feature.\nThe FFN is implemented through two linear transformation func-\ntion with Gaussian Error Linear Unit (GELU) activation [12, 37]:\nFFN\ud835\udc56\nGELU(o\ud835\udc56) = GELU(o\ud835\udc56W\ud835\udc56\n1 + b\ud835\udc56\n1)W\ud835\udc56\n2 + b\ud835\udc56\n2,\n(4)\nwhere W\ud835\udc56\n1 \u2208 R\ud835\udc51\u00d7\ud835\udc51\ud835\udc53 , W\ud835\udc56\n2 \u2208 R\ud835\udc51\ud835\udc53 \u00d7\ud835\udc51, b\ud835\udc56\n1 \u2208 R\ud835\udc51\ud835\udc53 , b\ud835\udc56\n2 \u2208 R\ud835\udc51, and \ud835\udc51\ud835\udc53 is the\nlayer size of the intermediate layer. Following existing studies [44],\n\ud835\udc51\ud835\udc53 is set as \ud835\udc51\ud835\udc53 = 4\ud835\udc51, \ud835\udc51 is the dimensionality of model.\nAs we can see the key change of the heterogeneous attention\nlayer, compared with self-attention layer is that we learn individual\nquery, key, and value (QKV) projection matrices (i.e., Q, K, V) per\nfeature. Therefore, we increase the number of parameters in the\nheterogeneous attention layer, compared with the vanilla Trans-\nformer model. The number of parameters scales linearly with the\nHiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems\nConference\u201917, July 2017, Washington, DC, USA\n(a) Vanilla Transformer Attention Layer.\n(b) Heterogeneous Attention Layer.\nFigure 2: Attention patterns with feature interaction layers.\nlength of input embedding list. However, it is worth pointing out\nthat the total numbers of FLOPs are the same for the heterogeneous\nattention layer and homogeneous one (the standard multi-head\nself-attention layer in the Transformer). This is because we have\nexactly the same ops compared with homogeneous Transformer\nlayer, but with different parameterizations.\nWe visualize the learned attention pattern Att(\u00b7, \u00b7) in Figure 2.\nThe attention pattern learned with the vanilla Transformer model,\nin Figure 2(a), is relatively sparse and exhibits a strong diagonal\npattern, indicating that the attention layer might not effectively cap-\nture the collaborative effects between different features. Meanwhile,\nfor the heterogeneous attention layer, there is a dense attention pat-\ntern. Intuitively, the observation could be explained by the fact that\nthe heterogeneous attention layer might be better at capturing the\ncollaborative effects among features thanks to the transformation\nmatrix for feature semantic alignment.\n3.4\nHiformer\nNow we are ready to introduce the Hiformer model, where we fur-\nther increase the model expressiveness, by introducing composite\nfeature learning in the QKV projections. Take key projection as an\nexample. Instead of learning key feature projection per feature, we\nredefine the key projection as follows:\n[\u02c6k\u210e\n1, . . . \u02c6k\u210e\n\ud835\udc3f] = concat([e\u210e\n1, . . . e\u210e\n\ud835\udc3f]) \u02c6K\u210e,\n(5)\nwhere \u02c6K\u210e \u2208 R\ud835\udc3f\ud835\udc51\u00d7\ud835\udc3f\ud835\udc51\ud835\udc58 . Compared with key projection matrices in\nSection 3.3, [\ud835\udc3e\u210e\n\ud835\udc56 , . . . , \ud835\udc3e\u210e\n\ud835\udc3f ] \u2208 R\ud835\udc51\u00d7\ud835\udc3f\ud835\udc51\ud835\udc58 , we increase model expressive-\nness by adding more parameters to key projections. We call the new\nprojection \u02c6K\u210e as Composite projection. Mathematically, instead of\nlearning feature interactions explicitly with feature pairs, we first\ntransform the feature embedding list into composite features as\nkeys and values; then we learn heterogeneous feature interaction\nbetween composite features and task embeddings. Similarly, we\napply cross projection to query and value projections.\nSimilarly as in Eq (1), the Hiformer model computes the atten-\ntion scores between the query q\ud835\udc56 and and key k\ud835\udc56 as follows:\nAttComposite(\ud835\udc56, \ud835\udc57)\u210e =\nexp\u0000\u02c6q\u210e\n\ud835\udc56 (\u02c6k\u210e\n\ud835\udc56 )\ud835\udc47 /\n\u221a\ufe01\n\ud835\udc51\ud835\udc58\n\u0001\n\u00cd\ud835\udc3f\n\ud835\udc59 exp\u0000\u02c6q\u210e\n\ud835\udc56 (\u02c6k\u210e\n\ud835\udc59 )\ud835\udc47 /\n\u221a\ufe01\n\ud835\udc51\ud835\udc58\n\u0001 ,\nWith the attention score, we can then compute the output of\nattention layer in Hiformer model as:\n\u02c6o\ud835\udc56 = concat\n\u0012\b \u2211\ufe01\n\ud835\udc5a\nAtt\u210e\nComposite(\ud835\udc56, \ud835\udc57)\u02c6v\u210e\n\ud835\udc56\n\t\ud835\udc3b\n\u210e=1\n\u0013\nO\ud835\udc57 .\n(6)\n3.5\nBetter Efficiency for Hiformer\nCompared with the Transformer model and the heterogeneous\nattention layer, the Hiformer model comes with more model ex-\npressiveness, but also computation cost during inference. Due to\nthe introduced Composite attention layer, the existing efficient\nTransformer architectures [42] cannot be directly applied.\nThe length of the feature embedding list is \ud835\udc3f = |C|+\ud835\udc5bD +\ud835\udc61. With\nthe model dimension as \ud835\udc51, we have inference cost breakdown 1 for\nHiformer: QKV projection: 3\ud835\udc3f2\ud835\udc512, attention score computation:\n2\ud835\udc3f2\ud835\udc51, output projection: \ud835\udc3f\ud835\udc512, FFN network: 8\ud835\udc3f\ud835\udc512. As a result, total\ncomputation is O(\ud835\udc3f2\ud835\udc512 + \ud835\udc3f2\ud835\udc51 + \ud835\udc3f\ud835\udc512). To deploy Hiformer for real-\ntime inference, we reduce the inference cost of Hiformer through\nlow-rank approximation and pruning.\n3.5.1\nLow-rank approximation. As we can see, the outstanding\nterm for Hiformer inference cost is QKV projections with compu-\ntation complexity of O(\ud835\udc3f2\ud835\udc512).\nWith Composite key projection defined in (5), we can approxi-\nmate \u02c6K\u210e with low-rank approximation, i.e.,\n\u02c6Kh = L\u210e\n\ud835\udc58 (R\u210e\n\ud835\udc58)\ud835\udc47,\n(7)\nwhere L\u210e\ud835\udc63 \u2208 R\ud835\udc3f\ud835\udc51\u00d7\ud835\udc5f\ud835\udc63, R\u210e\ud835\udc63 \u2208 R\ud835\udc3f\ud835\udc51\ud835\udc58 \u00d7\ud835\udc5f\ud835\udc63, and \ud835\udc5f\ud835\udc63 is the rank of the low-\nrank approximation for \u02c6K\u210e. Low-rank approximation reduces the\ncomputation cost for value projection to O(\ud835\udc3f\ud835\udc5f\ud835\udc63(\ud835\udc51 + \ud835\udc51\ud835\udc58)). Similarly,\nit can be applied to query and value composite projection with\nrank of \ud835\udc5f\ud835\udc58 and \ud835\udc5f\ud835\udc63. If \ud835\udc5f\ud835\udc58 < \ud835\udc3f\ud835\udc51\ud835\udc58/2 and \ud835\udc5f\ud835\udc63 < \ud835\udc3f\ud835\udc51\ud835\udc63/2, the cost will be\nreduced. The low-rank structure is also observed in data analysis.\nTaking \u02c6V\u210e as an example, we plot the singular values in Figure 3,\nwhere we can see that the \u02c6V\u210e matrix is indeed low-rank.\nFigure 3: Singular values for \u02c6Vh in Composite projection.\nAfter applying the low-rank approximation, the Hiformer model\ncomplexity is computed as follows: the query and key projection:\nO(\ud835\udc3f\ud835\udc5f\ud835\udc58 (\ud835\udc51 + \ud835\udc51\ud835\udc58)), value projection O(\ud835\udc3f\ud835\udc5f\ud835\udc63(\ud835\udc51 + \ud835\udc51\ud835\udc63)), attention layer\nO(\ud835\udc3f2\ud835\udc51), output projection O(\ud835\udc3f\ud835\udc512), and FFN layer O(\ud835\udc3f\ud835\udc512). With\n\ud835\udc51\ud835\udc58 = \ud835\udc51/\ud835\udc3b, \ud835\udc51\ud835\udc63 = \ud835\udc51/\ud835\udc3b, and \ud835\udc5f\ud835\udc58 < \ud835\udc51\ud835\udc58,\ud835\udc5f\ud835\udc63 < \ud835\udc51\ud835\udc63, we have the computa-\ntion complexity as O(\ud835\udc3f2\ud835\udc51 + \ud835\udc3f\ud835\udc512), which scales quadratically with\nthe length of the input embedding list \ud835\udc3f.\n1This computation is based on the setting where \ud835\udc51\ud835\udc58 = \ud835\udc51/\ud835\udc3b,\ud835\udc51\ud835\udc63 = \ud835\udc51/\ud835\udc3b, and \ud835\udc51\ud835\udc53 = 4\ud835\udc51.\nConference\u201917, July 2017, Washington, DC, USA\nHuan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong, and Ed H. Chi\nTask Token\nFeature 1\nFeature 2\nScaled Dot-Prod (Multi-Head) Attention\nTask Token\nFeature 1\nFeature 2\nFFN\nAdd & Norm\nTask Token\nFeature 1\nFeature 2\nAdd & Norm\nLinear (Query)\nLinear (Key)\nLinear (Value)\n(a) Vanilla Transformer.\nTask Token\nFeature 1\nFeature 2\nLinear \n(Query)\nLinear \n(Key)\nLinear \n(Value)\nScaled Dot-Prod (Multi-Head) Attention\nTask Token\nFeature 1\nFeature 2\nAdd & Norm\nTask Token\nFeature 1\nFeature 2\nAdd & Norm\nLinear \n(Query)\nLinear \n(Key)\nLinear \n(Value)\nLinear \n(Query)\nLinear \n(Key)\nLinear \n(Value)\nFFN\nFFN\nFFN\n(b) Transformer with Heterogeneous Attention.\nLinear \n(Query)\nTask Token\nFeature 1\nFeature 2\nScaled Dot-Prod (Multi-Head) Attention\nTask Token\nFeature 1\nFeature 2\nAdd & Norm\nTask Token\nFeature 1\nFeature 2\nAdd & Norm\nFFN\nFFN\nFFN\nReshape\nReshape\nReshape\nConcat\nConcat\nConcat\nLinear \n(Key)\nLinear \n(Value)\n(c) Hiformer.\nFigure 4: Illustration on capturing feature interaction with different Transformer-based architectures. with one task token\nand two feature tokens. In 4(a), apply vanilla Transformer for feature interaction learning. In 4(b), we explicitly model the\nheterogeneous feature interaction through a heterogeneous interaction layer. In 4(c), we propose the Hiformer model, which\nfurther improves the model expressiveness compared with the heterogeneous attention layer in 4(b). \ud835\udc51 is hidden layer size, \ud835\udc51\ud835\udc5e,\nand \ud835\udc51\ud835\udc63 are Query and Value dimensions, \ud835\udc3f is the total input length, \ud835\udc5f\u2217 corresponding to the rank of the corresponding QKV\nprojections (see Eq (7)).\n3.5.2\nModel Pruning. Recall that in the Output Layer (see Figure 1),\nwe are only using the encoded task embeddings for final objective\ntraining and predictions. Therefore, we can save some computation\nin the last layer of the Hiformer model by pruning the compu-\ntation of encoding feature embeddings. In particular, the pruning\ntranslates to only using task embeddings as query, and using both\ntask embedding and feature embeddings as key and value in the\nHiformer attention layer. Therefore, we have computation com-\nplexity of the last Hiformer layer: query projection: O(\ud835\udc61\ud835\udc5f\ud835\udc58 (\ud835\udc51 +\ud835\udc51\ud835\udc58)),\nkey projection: O(\ud835\udc3f\ud835\udc5f\ud835\udc58 (\ud835\udc51 + \ud835\udc51\ud835\udc58)), value projection: O(\ud835\udc3f\ud835\udc5f\ud835\udc63(\ud835\udc51 + \ud835\udc51\ud835\udc63)),\nattention layer computation: O(\ud835\udc3f\ud835\udc61\ud835\udc51), output projection: O(\ud835\udc61\ud835\udc512),\nFFN layer: O(\ud835\udc61\ud835\udc512). Suppose the number of task \ud835\udc61 (\u226a \ud835\udc3f) is consid-\nered to be a constant. We have the computation complexity of the\nlast layer of Hiformer as O(\ud835\udc3f(\ud835\udc5f\ud835\udc58 + \ud835\udc5f\ud835\udc63)\ud835\udc51 + \ud835\udc3f\ud835\udc61\ud835\udc51 + \ud835\udc61\ud835\udc512).\nPruning enables us to reduce the computation complexity of the\nlast Hiformer layer from scaling quadratically to scaling linearly\nwith \ud835\udc3f. This pruning technique can be applied to all Transformer-\nbased architectures. Some of the recent work , such as Perceiver [14],\nhas also introduced a similar idea to reduce the serving cost of\nTransformer models from scaling quadratically to linearly.\n4\nEXPERIMENT\nWe undertake a thorough empirical examination of the proposed\nmodel architectures with the web-scale App ranking model at\nGoogle Play. We conduct both offline and online experiments, with\nwhich we aim at answering the following research questions:\nQ1. Is the heterogeneous attention layer able to capture the het-\nerogeneous feature interactions to give better recommenda-\ntions, compared with vanilla Transformer models?\nQ2. With more model expressiveness in the Hiformer model,\ncould it further improve model quality?\nQ3. How is the serving efficiency of the Hiformer model com-\npared with SOTA models?\nQ4. How do the hyper-parameter settings impact the model per-\nformance and serving latency and how to make the perfor-\nmance and efficiency trade-offs?\n4.1\nOffline Evaluation Setup\n4.1.1\nDataset. For offline evaluation, we use the logged data from\nthe ranking model. Users\u2019 engagements are training labels, either 0\nor 1 indicating if the engagement happens between user and the\napp and the training loss is LogLoss (see Eq (3.1.4)). We train the\nmodel with 35 days of rolling-window data, and evaluate on the\n36th day data. The model is retrained from scratch everyday with\nmost recent data. There are 31 categorical features and 30 dense\nscalar features to describe the apps, such as app ID, app title, user\nlanguage, etc. The vocabulary size of the categorical features varies\nfrom hundreds to millions.\n4.1.2\nEvaluation Metrics. We will evaluate the model performance\nusing AUC (Area Under the Receiver Operating Characteristic\nCurve). The AUC metric can be interpreted as the probability of\nthe model assigning a randomly chosen positive example a higher\nranking than a randomly chosen negative example. In the web-scale\napplication dataset, due to the large size of the evaluation dataset,\nwe notice that the improvement of AUC at 0.001 level achieves\nstatistical significance. Similar observations have been made in\nother web-scale applications [3, 8, 15]. Additionally, we report the\nnormalized LogLoss with Transformer one layer as the baseline.\nHiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems\nConference\u201917, July 2017, Washington, DC, USA\nWe train the model on TPU [16, 17], and we report the training\nQueries per second (QPS) to measure the training speed. We es-\ntimate the latency through offline simulation, where we perform\ninference of the models on 20 batches of examples with batch size\nbeing 1024. We then normalize the latency of all models with re-\nspect to the baseline model, which is a one-layer Transformer model\nwith pruning. Due to the large number of online serving requests,\nthe serving latency is much more critical than training QPS.\n4.1.3\nModel Architectures. Note that our work is on feature inter-\naction learning with Transformers, which is drastically different\nfrom user history sequence modeling with Transformers [20, 41].\nTherefore, we focus on comparing our proposed models with the\nfollowing SOTA feature interaction models:\n\u2022 AutoInt [40] is proposed to learn feature interaction through\nthe multi-head self-attention layer, and then learn the final pre-\ndictions through a Multi-layer Perceptrons (MLP) layers.\n\u2022 DLRM [29] is to learn feature interaction through factorization\nmachine, and then learn implicit feature interactions through a\nMLP layer, for the final predictions.\n\u2022 DCN [45] (i.e., DCN-v2) is one of the SOTA models on feature\ninteraction learning in web-scale application, which has a cross\nnetwork that explicitly creates bounded-degree feature crosses\nwhere the feature interaction order increases with layer depth.\n\u2022 Transformer [44] with multi-head self-attention layer for fea-\nture interaction learning. The main difference between Trans-\nformer and AutoInt is: 1. Transformer use encoded task embed-\ndings for predictions; while AutoInt uses all encoded feature and\ntask embeddings; 2. There is FFN layer in Transformer architec-\nture, but not in the AutoInt layer.\n\u2022 Transformer + PE is to add Position Encoding to the Trans-\nformer architecture, where we learn an additional per-feature\nembedding as feature encoding.\n\u2022 HeteroAtt is our proposed Heterogeneous Attention layer to\ncapture the heterogeneous feature interactions.\n\u2022 Hiformer is our proposed model with low-rank approximation\nand model pruning.\n4.1.4\nImplementation Details. The model training and serving is\nbuilt on Tensorflow 2 [38] and Model Garden [48]. To ensure a\nfair comparison, we keep all the model components, except for the\nfeature interaction layers, to be the same. In the embedding layer,\nwe set the embedding dimensions to be 128 for all models. Note\nthat for Transformer architectures, HeteroAtt, and Hiformer , we\nonly use the encoded CLS token (i.e., learned task token) for the\nfinal task predictions.\nIn our experiments on Transformers, we set model dimension-\nality \ud835\udc51 = 128, number of heads in the attention layer \ud835\udc3b = 4, and\nthe layer size of FFN to be \ud835\udc51\ud835\udc53 = 512. We set \ud835\udc51\ud835\udc58 = 16 and \ud835\udc51\ud835\udc63 = 64,\ninstead of setting \ud835\udc51\ud835\udc58 = \ud835\udc51/\ud835\udc3b,\ud835\udc51\ud835\udc63 = \ud835\udc51/\ud835\udc3b as in the Transform model\nand the other Transformer-based models. For the other methods,\nwe perform a hyper-parameters tuning to get the best performance.\nFor online experiments, we tune the hyperparameters based on one\nday\u2019s (say Feb 1st) data, and fix the settings for the following days\n(i.e., dates after Feb 2nd).\n4.2\nHomogeneous vs Heterogeneous Feature\nInteraction (Q1)\nFirst of all, we observe that Transformer+PE model performs sim-\nilarly to the vanilla Transformer model. This is consistent with\nour hypothesis. Though coming with more parameters, the feature\nencoding only learns a bias embedding term per feature, which is\nnot sufficient to learn the feature transformation for heterogeneous\nfeature interaction learning. Secondly, we observe that Transformer\nmodel with heterogeneous attention layer (HeteroAtt) performs\nsignificantly better than the vanilla Transformer model. This result\nvalidates our argument that HeteroAtt can effectively capture the\ncomplex feature interactions through the transformation matrices\nM\ud835\udc56,\ud835\udc57. Moreover, we observe that the parameter # of the two-layer\nHeteroAtt model is much larger than the two-layer Transformer\nmodel, as the HeteroAtt model has more model expressiveness.\nTherefore, we can answer affirmatively to Q1, that the heteroge-\nneous feature interaction with better feature context awareness is\ncrucial for providing personalized recommendations.\n4.3\nModel Performance Comparison (Q2)\nThe offline comparison of the baseline models and our proposed\nmodels are summarized in Table 1. We report the best performance\nof each model, with the corresponding number of layers.\nWe take a closer look at the AutoInt, DLRM, and vanilla Trans-\nformer model. In the feature interaction learning of these models,\nthere is no feature awareness and semantic alignment. Thus, their\nperformance are relatively weaker compared with the other models.\nIn DCN model, the Cross Net implicitly generates all the pairwise\ncrosses and then projects it to a lower-dimensional space. The pa-\nrameterization of all pairwise crosses are different, which is similar\nto providing feature awareness. Therefore, DCN model performance\nis comparable with our HeteroAtt models.\nCompared with the heterogeneous attention layer (i.e., Het-\neroAtt), Hiformer model provides more model expressiveness in\nthe QKV projections. It achieves the best model performance with\njust one layer, outperforming the HeteroAtt model and the other\nSOTA models.\n4.4\nServing Efficiency (Q3)\nWe compare the serving efficiency of all the models. First of all,\nwe compare the Transformer two layer model with the one layer\nmodel. As we can see, the latency of the two layer model is more\nthan 2x of the one layer model. This is due to the fact that we only\napply pruning to the second layer in the two layer model, and we\nhave already applied pruning to the one layer model. Similarly,\nwe can draw the same conclusion for the HeteroAtt layer as the\ntwo models have the same operators, thus latency. However, such\npruning technique cannot be applied to AutoInt model. Therefore,\nthe one-layer AutoInt model is much more expensive than the\nvanilla Transformer model and the HeteroAtt model.\nSecondly, we compare the Hiformer model with the one-layer\nHeteroAtt model. Both the one-layer Hiformer model and the\none-layer HeteroAtt model enjoy the inference optimization from\npruning. However, because of the more expressive QKV projection,\nwe see that the Hiformer model is 50.05% more expensive.\nConference\u201917, July 2017, Washington, DC, USA\nHuan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong, and Ed H. Chi\nModel\nLayer #\nParameter #\nAUC (\u2191)\nLogLoss (%, \u2193)\nTrain QPS (\u2191)\nServing Latency (\u2193)\nAutoInt\n1\n12.39M\n0.7813\n-0.37096\n5.45e6\n2.28\nDLRM\n-\n5.95M\n0.7819\n-0.47695\n5.14e6\n0.95\nDCN\n1\n13.73M\n0.7857\n-0.79491\n5.76e6\n1.46\nTransformer\n1\n0.74M\n0.7795\n0\n5.75e6\n1.00\n2\n0.84M\n0.7811\n-0.31797\n4.12e6\n3.03\n3\n0.97M\n0.7838\n-0.45045\n3.13e6\n5.05\nTransformer+PE\n3\n1.08M\n0.7833\n-0.39746\n3.12e6\n5.06\nHeteroAtt (ours)\n1\n2.36M\n0.7796\n-0.05299\n5.71e6\n1.01\n2\n10.50M\n0.7856\n-0.82141\n4.10e6\n3.11\nHiformer (ours)\n1\n16.68M\n0.7875\n-0.87440\n5.69e6\n1.52\nTable 1: Offline model performance comparison regarding number of parameters, AUC, and normalized efficiency. The number\nof parameters excludes the embedding layer. \u2191 (\u2193) means the higher (lower) the better.\nNevertheless, we want to point out that without low-rank ap-\nproximation, the Hiformer model would potentially be much more\nexpensive. In our experiment, we set rank of query and key as\n\ud835\udc5f\ud835\udc58 = 128, and value \ud835\udc5f\ud835\udc63 = 1024. We compare the Hiformer with and\nwithout low-rank approximation in Table 2. The low-rank approxi-\nmation gives 62.7% inference latency saving. Additionally, there is\nno significant model quality loss, which is expected because of the\nobserved low-rank structures of Hiformer query, key, and value\nprojection matrices (e.g., V, shown in Figure 3).\nHiformer Model\nParameter #\nAUC\nLatency\nlow-rank approx.\n16.68M\n0.7875\n1.52\nw/o low-rank approx.\n59.95M\n0.7882\n3.35\nTable 2: Low-rank approximation for Hiformer .\n4.5\nParameter Sensitivity (Q4)\nSince one of the challenges is to reduce the serving latency, we are\nalso interested in the parameter sensitivity of the model as in RQ4.\nMore formally, we aim to answer the following question:\nAs we previously discussed in Section 4, instead of directly using\n\ud835\udc51\ud835\udc58 = \ud835\udc51/\ud835\udc3b and \ud835\udc51\ud835\udc63 = \ud835\udc51/\ud835\udc3b, we find that the choice of \ud835\udc51\ud835\udc58 and \ud835\udc51\ud835\udc63\npresents certain trade-offs between model performance and serving\nlatency for the HeteroAtt and Hiformer model.\nFirstly, for \ud835\udc3b\ud835\udc51\ud835\udc58 in HeteroAtt in Figure 5(a), after reducing the\n\ud835\udc3b\ud835\udc51\ud835\udc58 from \ud835\udc51 = 128 to 64, there is no model quality loss. However, we\ncan harvest a free latency improvement (3%). If we further decreas-\ning \ud835\udc3b\ud835\udc51\ud835\udc58, there is considerable model quality loss. Therefore, we\nopt for \ud835\udc3b\ud835\udc51\ud835\udc58 = 64, as highlighted by the grey line. By comparison,\nincreasing \ud835\udc3b\ud835\udc51\ud835\udc58 for HeteroAtt model gives model quality improve-\nment, but also significant latency increasing (in Figure 5(b)). For\nquality and latency trade-off, we set \ud835\udc3b\ud835\udc51\ud835\udc63 = 256.\nSimilarly, we observe in Figure 5(c), for Hiformer model, de-\ncreasing \ud835\udc3b\ud835\udc51\ud835\udc58 to 64 gives no quality loss, but unlike HeteroAtt model,\nthere is almost no latency gain. This is due to the low-rank approx-\nimation in the QK projection, especially we set the rank \ud835\udc5f\ud835\udc58 = 256,\nwhich is relatively small, compared with\ud835\udc5f\ud835\udc63 = 1024. Accordingly, the\nquery projection dominates the computation cost. With increased\n\ud835\udc3b\ud835\udc51\ud835\udc63, the latency increases significantly, shown in Figure 5(d).\nThis observation on the model quality and latency trade-off of\n\ud835\udc3b\ud835\udc51\ud835\udc63 provides uniqueness of the HeteroAtt and Hiformer model. It\ncan serves as a instrument to tune the model capacity at the cost\nof latency. For example, when the number of requests per query\nis relatively small, we can achieve better model performance by\nincreasing \ud835\udc51\ud835\udc63. Such an instrument might not not available for the\nother SOTA models, such as DCN [46].\n4.6\nOnline A/B Testing\nIn addition to the extensive offline experiments, we also have con-\nducted an online A/B testing experiment with the proposed model\nframework. For each of the control and treatment groups, there is\n1% randomly selected users, who receive the item recommendations\nbased on different ranking algorithms. To understand the improve-\nment we gain through heterogeneous feature interaction learning,\nthe baseline model is leveraging a one-layer Transformer model\nto capture feature interactions. We collect the users\u2019 engagement\nmetrics for 10 days, as shown in Table 3. The one layer HeteroAtt\nmodel significantly outperforms the one layer Transformer model,\nwhich is consistent with our offline analysis. Additionally, two\n(a) HeteroAtt: \ud835\udc3b\ud835\udc51\ud835\udc58.\n(b) HeteroAtt: \ud835\udc3b\ud835\udc51\ud835\udc63.\n(c) Hiformer : \ud835\udc3b\ud835\udc51\ud835\udc58.\n(d) Hiformer : \ud835\udc3b\ud835\udc51\ud835\udc63.\nFigure 5: Parameter Sensitivity. With increased \ud835\udc51\ud835\udc58 and \ud835\udc51\ud835\udc63, the\nmodel quality gain saturates; however, the model inference\ncost increases drastically. The black dotted lines mark the\nselected \ud835\udc51\ud835\udc58, \ud835\udc51\ud835\udc63 values, which gives the best quality and infer-\nence cost trade-off.\nHiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems\nConference\u201917, July 2017, Washington, DC, USA\nlayer HeteroAtt model further improves over the one-layer one.\nThe Hiformer performs the best among all models, including the\nDCN model. Such an observation indicates that Transformer-based\narchitectures can outperform SOTA feature interaction models. We\nhave successfully deployed the Hiformer model to production.\nModel\nLayer Num\nEngagement Metrics\nTransformer\n1\n+0.00%\nHeteroAtt (ours)\n1\n+1.27%\u2217\nHeteroAtt (ours)\n2\n+2.33%\u2217\nHiformer (ours)\n1\n+2.66%\u2217\nDCN\n1\n+2.20%\u2217\nTable 3: The online engagement and serving latency metrics\nof different models. \u2217 indicates statistically significant.\n5\nRELATED WORK\nBefore the era of deep learning, people often manually crafted\ncross features and added them to logistic regression models to\nimprove their performance. Some also leveraged decision tree based\nmodels [11] to learn feature crosses. Later on, the development\nof embedding techniques have led to the design of Factorization\nMachines (FMs) [34], which was proposed to model second-order\nfeature interactions with the inner product of two latent vectors.\nRecently, DNNs have become the backbone of many models in\nindustry. To improve the efficiency of feature cross learning, many\nworks explicitly model feature interactions by designing a function\n\ud835\udc54(\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57) while leveraging the implicit feature crosses learned by\nDNNs. Wide & deep model [3] combines a DNN model with a wide\ncomponent that consists of crosses of raw features. Since then, many\nwork has been proposed to automate the manual feature cross work\nof wide & deep by introducing FM-like operations (inner products)\nor Hadamard products. DeepFM [8] and DLRM [29] adopt FM in\nthe model, Neural FM [10] generalizes FM by incorporating the\nHadamard product, PNN [32] also uses inner products and outer\nproducts to capture pairwise feature interactions. These methods\nhowever can only model up to second order feature crosses. There\nis also work that can model higher-order feature interactions. Deep\nCross Network (DCN) [45, 46] designs a cross network to explicitly\nlearn high-order feature interactions where its interaction order in-\ncreases with layer depth. DCN-V2 [46] makes DCN more expressive\nand practical in large-scale industrial settings. xDeepFM [26] also\nimproves the expressiveness of cross network in DCN and relies\non the Hadamard product to capture high-order feature crosses.\nAnother line of work leverages the attention mechanism in Trans-\nformer models to capture feature interactions. AutoInt [40] was\nproposed to capture the feature interaction through the multi-head\nself-attention layer. Nevertheless, as the Transformer was designed\nto model relationships that are context independent, such as text\ntokens, directly applying the multi-head self-attention layer for\nfeature interaction has very limited model expressiveness. Simi-\nlarly, [25] is based on the attention mechanism with cross-product\ntransformation to capture hierarchical attention, without providing\nfeature awareness and semantic alignment.\nThe Heterogeneous Graph Transformer [13] work is related to\nours, where node types are considered in the heterogeneous mutual\nattention layer. Though we share some similarities in the motiva-\ntions, our work is substantially different because Hiformer model\nis proposed to capture feature interactions for web-scale recom-\nmender system, and the design of the Hiformer attention layer has\nmore expressiveness. The field-aware factorization machine [18]\nwork is also relevant. In [18], the feature awareness of feature in-\nteraction learning is implemented through individual embedding\nlookup tables, while our proposed methods are through the informa-\ntion transformation in the heterogeneous attention layer. PNN [33]\nleverages kernel Factorization Machine to consider feature dynam-\nics in feature interaction learning, which shares similarity with the\nheterogeneous attention layer. Nevertheless, heterogeneous atten-\ntion layer is attention based while PNN is derived from factorization\nmachine. Additionally, we leverage Query and Key projection to\nlearn heterogeneous feature interactions while PNN is based on ker-\nnel matrices. Moreover, we further increase model expressiveness\nin Hiformer with cost reduction.\n6\nCONCLUSION AND FUTURE WORK\nIn this work, we propose a heterogeneous attention layer, which is\na simple but effective modification of the self-attention layer to pro-\nvide feature awareness for feature interaction learning. We then fur-\nther improve the model expressiveness, and propose Hiformer to\nimprove feature interaction learning. We also improved the serv-\ning efficiency of Hiformer such that it could satisfy the serving\nlatency requirement for web-scale applications. The key change in\nour method is to identify the different relationships among differ-\nent features in the attention mechanism, such that we can capture\nsemantic relevance given the dynamic information in learned fea-\nture embeddings. Both offline and online experiments on the world\nleading digital distribution service platform demonstrate the effec-\ntiveness and efficiency of our method. These experiment results\nindicate that a Transformer based model for feature interaction\nlearning can perform better than SOTA methods in web-scale ap-\nplications.\nOur work serves as a bridge to bring Transformer-based archi-\ntectures to applications in web-scale recommender system. For\nfuture work, we are interested in how the recent advances in Trans-\nformer architectures [9] in other domains domains, such as NLP\nand CVR, can be translated to recommender systems. With the new\nfeature interaction component introduced in this paper, it can also\nbe leveraged in neural architecture search [7] to further improve\nrecommender systems.\nConference\u201917, July 2017, Washington, DC, USA\nHuan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong, and Ed H. Chi\nREFERENCES\n[1] Beutel, A., Covington, P., Jain, S., Xu, C., Li, J., Gatto, V., and Chi, E. H. Latent\ncross: Making use of context in recurrent recommender systems. In Proceedings\nof the Eleventh ACM International Conference on Web Search and Data Mining\n(2018), pp. 46\u201354.\n[2] Caruana, R. Multitask learning. Machine learning 28, 1 (1997), 41\u201375.\n[3] Cheng, H.-T., Koc, L., Harmsen, J., Shaked, T., Chandra, T., Aradhye, H.,\nAnderson, G., Corrado, G., Chai, W., Ispir, M., et al. Wide & deep learning\nfor recommender systems. In Proceedings of the 1st workshop on deep learning for\nrecommender systems (2016), pp. 7\u201310.\n[4] Covington, P., Adams, J., and Sargin, E. Deep neural networks for youtube\nrecommendations. In Proceedings of the 10th ACM conference on recommender\nsystems (2016), pp. 191\u2013198.\n[5] Davidson, J., Liebald, B., Liu, J., Nandy, P., Van Vleet, T., Gargi, U., Gupta, S.,\nHe, Y., Lambert, M., Livingston, B., et al. The youtube video recommendation\nsystem. In Proceedings of the fourth ACM conference on Recommender systems\n(2010), pp. 293\u2013296.\n[6] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805 (2018).\n[7] Elsken, T., Metzen, J. H., and Hutter, F. Neural architecture search: A survey.\nThe Journal of Machine Learning Research 20, 1 (2019), 1997\u20132017.\n[8] Guo, H., Tang, R., Ye, Y., Li, Z., and He, X. Deepfm: a factorization-machine\nbased neural network for ctr prediction. arXiv preprint arXiv:1703.04247 (2017).\n[9] Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A., Xu,\nC., Xu, Y., et al. A survey on vision transformer. IEEE transactions on pattern\nanalysis and machine intelligence (2022).\n[10] He, X., Liao, L., Zhang, H., Nie, L., Hu, X., and Chua, T.-S. Neural collaborative\nfiltering. In Proceedings of the 26th international conference on world wide web\n(2017), pp. 173\u2013182.\n[11] He, X., Pan, J., Jin, O., Xu, T., Liu, B., Xu, T., Shi, Y., Atallah, A., Herbrich, R.,\nBowers, S., et al. Practical lessons from predicting clicks on ads at facebook.\nIn Proceedings of the eighth international workshop on data mining for online\nadvertising (2014), pp. 1\u20139.\n[12] Hendrycks, D., and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 (2016).\n[13] Hu, Z., Dong, Y., Wang, K., and Sun, Y. Heterogeneous graph transformer. In\nProceedings of the web conference 2020 (2020), pp. 2704\u20132710.\n[14] Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J.\nPerceiver: General perception with iterative attention. In International conference\non machine learning (2021), PMLR, pp. 4651\u20134664.\n[15] Joglekar, M. R., Li, C., Chen, M., Xu, T., Wang, X., Adams, J. K., Khaitan, P.,\nLiu, J., and Le, Q. V. Neural input search for large scale recommendation models.\nIn Proceedings of the 26th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining (2020), pp. 2387\u20132397.\n[16] Jouppi, N., Young, C., Patil, N., and Patterson, D. Motivation for and evaluation\nof the first tensor processing unit. ieee Micro 38, 3 (2018), 10\u201319.\n[17] Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R.,\nBates, S., Bhatia, S., Boden, N., Borchers, A., et al. In-datacenter performance\nanalysis of a tensor processing unit. In Proceedings of the 44th annual international\nsymposium on computer architecture (2017), pp. 1\u201312.\n[18] Juan, Y., Zhuang, Y., Chin, W.-S., and Lin, C.-J. Field-aware factorization\nmachines for ctr prediction. In Proceedings of the 10th ACM conference on recom-\nmender systems (2016), pp. 43\u201350.\n[19] Kalyan, K. S., Rajasekharan, A., and Sangeetha, S. Ammus: A survey of\ntransformer-based pretrained models in natural language processing. arXiv\npreprint arXiv:2108.05542 (2021).\n[20] Kang, W.-C., and McAuley, J. Self-attentive sequential recommendation. In 2018\nIEEE international conference on data mining (ICDM) (2018), IEEE, pp. 197\u2013206.\n[21] Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., and Shah, M.\nTransformers in vision: A survey. ACM computing surveys (CSUR) 54, 10s (2022),\n1\u201341.\n[22] LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. nature 521, 7553 (2015),\n436\u2013444.\n[23] Li, D., Hu, B., Chen, Q., Wang, X., Qi, Q., Wang, L., and Liu, H. Attentive\ncapsule network for click-through rate and conversion rate prediction in online\nadvertising. Knowledge-Based Systems 211 (2021), 106522.\n[24] Li, M., Liu, Z., Smola, A. J., and Wang, Y.-X. Difacto: Distributed factorization\nmachines. In Proceedings of the Ninth ACM International Conference on Web\nSearch and Data Mining (2016), pp. 377\u2013386.\n[25] Li, Z., Cheng, W., Chen, Y., Chen, H., and Wang, W. Interpretable click-through\nrate prediction through hierarchical attention. In Proceedings of the 13th Interna-\ntional Conference on Web Search and Data Mining (2020), pp. 313\u2013321.\n[26] Lian, J., Zhou, X., Zhang, F., Chen, Z., Xie, X., and Sun, G. xdeepfm: Combining\nexplicit and implicit feature interactions for recommender systems. In Proceedings\nof the 24th ACM SIGKDD international conference on knowledge discovery & data\nmining (2018), pp. 1754\u20131763.\n[27] Lu, J., Wu, D., Mao, M., Wang, W., and Zhang, G. Recommender system\napplication developments: a survey. Decision Support Systems 74 (2015), 12\u201332.\n[28] Mirhoseini, A., Goldie, A., Yazgan, M., Jiang, J. W., Songhori, E., Wang, S., Lee,\nY.-J., Johnson, E., Pathak, O., Nazi, A., et al. A graph placement methodology\nfor fast chip design. Nature 594, 7862 (2021), 207\u2013212.\n[29] Naumov, M., Mudigere, D., Shi, H.-J. M., Huang, J., Sundaraman, N., Park,\nJ., Wang, X., Gupta, U., Wu, C.-J., Azzolini, A. G., et al. Deep learning rec-\nommendation model for personalization and recommendation systems. arXiv\npreprint arXiv:1906.00091 (2019).\n[30] Okura, S., Tagami, Y., Ono, S., and Tajima, A. Embedding-based news rec-\nommendation for millions of users. In Proceedings of the 23rd ACM SIGKDD\ninternational conference on knowledge discovery and data mining (2017), pp. 1933\u2013\n1942.\n[31] Pan, X., Li, M., Zhang, J., Yu, K., Wen, H., Wang, L., Mao, C., and Cao, B. Metacvr:\nConversion rate prediction via meta learning in small-scale recommendation\nscenarios. In Proceedings of the 45th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval (2022), pp. 2110\u20132114.\n[32] Qu, Y., Cai, H., Ren, K., Zhang, W., Yu, Y., Wen, Y., and Wang, J. Product-based\nneural networks for user response prediction. In 2016 IEEE 16th International\nConference on Data Mining (ICDM) (2016), IEEE, pp. 1149\u20131154.\n[33] Qu, Y., Fang, B., Zhang, W., Tang, R., Niu, M., Guo, H., Yu, Y., and He, X.\nProduct-based neural networks for user response prediction over multi-field\ncategorical data. ACM Transactions on Information Systems (TOIS) 37, 1 (2018),\n1\u201335.\n[34] Rendle, S. Factorization machines. In 2010 IEEE International conference on data\nmining (2010), IEEE, pp. 995\u20131000.\n[35] Resnick, P., and Varian, H. R. Recommender systems. Communications of the\nACM 40, 3 (1997), 56\u201358.\n[36] Schafer, J. B., Konstan, J., and Riedl, J. Recommender systems in e-commerce.\nIn Proceedings of the 1st ACM conference on Electronic commerce (1999), pp. 158\u2013\n166.\n[37] Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202\n(2020).\n[38] Singh, P., Manure, A., Singh, P., and Manure, A. Introduction to tensorflow\n2.0. Learn TensorFlow 2.0: Implement Machine Learning and Deep Learning Models\nwith Python (2020), 1\u201324.\n[39] Song, Q., Cheng, D., Zhou, H., Yang, J., Tian, Y., and Hu, X. Towards automated\nneural interaction discovery for click-through rate prediction. In Proceedings of\nthe 26th ACM SIGKDD International Conference on Knowledge Discovery & Data\nMining (2020), pp. 945\u2013955.\n[40] Song, W., Shi, C., Xiao, Z., Duan, Z., Xu, Y., Zhang, M., and Tang, J. Autoint:\nAutomatic feature interaction learning via self-attentive neural networks. In Pro-\nceedings of the 28th ACM International Conference on Information and Knowledge\nManagement (2019), pp. 1161\u20131170.\n[41] Sun, F., Liu, J., Wu, J., Pei, C., Lin, X., Ou, W., and Jiang, P. Bert4rec: Sequential\nrecommendation with bidirectional encoder representations from transformer. In\nProceedings of the 28th ACM international conference on information and knowledge\nmanagement (2019), pp. 1441\u20131450.\n[42] Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: A\nsurvey. ACM Computing Surveys 55, 6 (2022), 1\u201328.\n[43] Tsang, M., Cheng, D., and Liu, Y. Detecting statistical interactions from neural\nnetwork weights. arXiv preprint arXiv:1705.04977 (2017).\n[44] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,\nKaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural\ninformation processing systems 30 (2017).\n[45] Wang, R., Fu, B., Fu, G., and Wang, M. Deep & cross network for ad click\npredictions. In Proceedings of the ADKDD\u201917. 2017, pp. 1\u20137.\n[46] Wang, R., Shivanna, R., Cheng, D., Jain, S., Lin, D., Hong, L., and Chi, E. Dcn\nv2: Improved deep & cross network and practical lessons for web-scale learning\nto rank systems. In Proceedings of the Web Conference 2021 (2021), pp. 1785\u20131797.\n[47] Yang, J., Yi, X., Zhiyuan Cheng, D., Hong, L., Li, Y., Xiaoming Wang, S., Xu, T.,\nand Chi, E. H. Mixed negative sampling for learning two-tower neural networks\nin recommendations. In Companion Proceedings of the Web Conference 2020 (2020),\npp. 441\u2013447.\n[48] Yu, H., Chen, C., Du, X., Li, Y., Rashwan, A., Hou, L., Jin, P., Yang, F., Liu, F.,\nKim, J., et al. Tensorflow model garden. GitHub (2020).\n[49] Zhang, S., Yao, L., Sun, A., and Tay, Y. Deep learning based recommender\nsystem: A survey and new perspectives. ACM Computing Surveys (CSUR) 52, 1\n(2019), 1\u201338.\n[50] Zhao, Z., Hong, L., Wei, L., Chen, J., Nath, A., Andrews, S., Kumthekar, A.,\nSathiamoorthy, M., Yi, X., and Chi, E. Recommending what video to watch\nnext: a multitask ranking system. In Proceedings of the 13th ACM Conference on\nRecommender Systems (2019), pp. 43\u201351.\n[51] Zhuang, H., Wang, X., Bendersky, M., and Najork, M. Feature transformation\nfor neural ranking models. In Proceedings of the 43rd international ACM SIGIR\nconference on research and development in information retrieval (2020), pp. 1649\u2013\n1652.\n"
  },
  {
    "title": "FMViT: A multiple-frequency mixing Vision Transformer",
    "link": "https://arxiv.org/pdf/2311.05707.pdf",
    "upvote": "5",
    "text": "FMViT: A multiple-frequency mixing Vision Transformer\nWei Tan, Yifeng Geng, Xuansong Xie\nDAMO Academy, Alibaba Group\nziyuan.tw@alibaba-inc.com, cangyu.gyf@alibaba-inc.com, xingtong.xxs@taobao.com\nAbstract\nThe transformer model has gained widespread adoption in\ncomputer vision tasks in recent times. However, due to the\nquadratic time and memory complexity of self-attention,\nwhich is proportional to the number of input tokens, most\nexisting Vision Transformers (ViTs) encounter challenges in\nachieving efficient performance in practical industrial deploy-\nment scenarios, such as TensorRT and CoreML, where tra-\nditional CNNs excel. Although some recent attempts have\nbeen made to design CNN-Transformer hybrid architectures\nto tackle this problem, their overall performance has not\nmet expectations. To tackle these challenges, we propose an\nefficient hybrid ViT architecture named FMViT. This ap-\nproach enhances the model\u2019s expressive power by blend-\ning high-frequency features and low-frequency features with\nvarying frequencies, enabling it to capture both local and\nglobal information effectively. Additionally, we introduce\ndeploy-friendly mechanisms such as Convolutional Multi-\ngroup Reparameterization (gMLP), Lightweight Multi-head\nSelf-Attention (RLMHSA), and Convolutional Fusion Block\n(CFB) to further improve the model\u2019s performance and re-\nduce computational overhead. Our experiments demonstrate\nthat FMViT surpasses existing CNNs, ViTs, and CNN-\nTransformer hybrid architectures in terms of latency/accuracy\ntrade-offs for various vision tasks. On the TensorRT platform,\nFMViT outperforms Resnet101 by 2.5% (83.3% vs. 80.8%)\nin top-1 accuracy on the ImageNet dataset while maintaining\nsimilar inference latency. Moreover, FMViT achieves com-\nparable performance with EfficientNet-B5, but with a 43%\nimprovement in inference speed. On CoreML, FMViT out-\nperforms MobileOne by 2.6% in top-1 accuracy on the Im-\nageNet dataset, with inference latency comparable to Mo-\nbileOne (78.5% vs. 75.9%). Our code can be found at\nhttps://github.com/tany0699/FMViT.\n1\nIntroduction\nVision Transformers (ViTs) have recently succeeded in var-\nious computer vision applications such as image classifica-\ntion, object detection, and semantic segmentation and have\nreceived extensive attention from industry and academia.\nDespite this, Convolutional Neural Networks (CNNs) re-\nmain the preferred choice for real-world vision tasks, pri-\nmarily because ViTs typically exhibit slower performance\nthan traditional CNNs, such as ResNets. The inference speed\nof Transformer models is constrained by elements such\nNext-ViT-S\nNext-ViT-B\nFMViT-T\nFMViT-S\nFMViT-M\nFMViT-B\nFMViT-L\nMobileViT-XXS\nMobileViT-XS\nMobileViTv2-1.0\nMobileViTv2-2.0\nMobileOne-S0\nMobileOne-S1\nMobileOne-S2\nMobileOne-S4\nMobileformer-52M\nMobileformer-96M\nMobileformer-151M\nMobileformer-214M\nMobileformer-294M\nMobileformer-508M\nFasterNet-T0\nFasterNet-T1\nFasterNet-T2\nFasterNet-S\nFasterNet-M\nFastViT-T8\nFastViT-T12\nFastViT-SA24\nEfficientViT-B1\nSwin-T\nSwin-S\nMobileNetV3-S\nPoolFormer-M36\nPoolFormer-M48\nEfficientFormer-L7 \n65\n70\n75\n80\n85\n-1.0\n1.0\n3.0\n5.0\n7.0\n9.0\n11.0\n13.0\n15.0\n17.0\n19.0\n21.0\n23.0\n25.0\nImageNet 1K Top-1 \nCoreML batch 1 inference time (ms)\nFigure 1: Speed-performance trade-off on ImageNet1K\nas the Multi-head Self-attention (MHSA) mechanism, non-\nfusible LayerNorm, and GELU layers, along with frequent\nmemory accesses.\nNumerous endeavors have been undertaken to liberate\nVits from the high-latency issue. For instance, models such\nas Swin (Liu et al. 2021), PoolFormer (Yu et al. 2022),\nReformer (Kitaev, Kaiser, and Levskaya 2020), MaxViT\n(Tu et al. 2022), SepViT (Li et al. 2022b), and Mobile-\nViT (Li et al. 2022b), among others, strive to develop spa-\ntial attention methods that are more efficient and mitigate\nthe quadratic surge in computational complexity of MHSA.\nConcurrently, other initiatives, including EfficientFormer\n(Li et al. 2022d) and MobileViT (Li et al. 2022b), are explor-\ning ways to construct CNN-Transformer hybrid architec-\ntures that balance accuracy and latency. This is achieved by\nintegrating effective convolutional blocks with potent Trans-\nformer blocks. Notably, most of the current state-of-the-art\n(SOTA) models are designed as CNN-Transformer hybrids.\nThese models predominantly utilize convolutional blocks in\nthe preliminary stages and reserve the stacking of Trans-\nformer blocks for the final stages.\nPresently, neither the Convolutional Block nor the Trans-\nformer Block can simultaneously achieve efficiency and per-\nformance in existing works. Although the precision-latency\narXiv:2311.05707v1  [cs.CV]  9 Nov 2023\ntradeoff has improved over the years, the overall perfor-\nmance of modern hybrid systems still needs to improve.\nThis study introduces four critical components for design-\ning effective vision Transformer networks to address these\nchallenges. Firstly, inspired by NextViT\u2019s (Li et al. 2022a)\nmixing of high-frequency features and low-frequency fea-\ntures, a potent Multi-Frequency Fusion Block (FMB) is in-\ntroduced, amalgamating multiple high-frequency and low-\nfrequency features to enhance the model\u2019s information flow\nand expressive capacity. Secondly, a Lightweight Convolu-\ntion Fusion Block (CFB) is proposed to efficiently blend the\nlocal modeling capability of convolution with convolution\nmulti-group reparameterization, further bolstering modeling\nperformance. Thirdly, convolutional multi-group reparame-\nterization is suggested. It integrates the spatial information\nof different subchannels during the training phase. It fuses\nthem into a convolution in the inference phase, improving\nthe model\u2019s accuracy while maintaining the inference speed.\nLastly, a lightweight self-attention block, termed RLMHSA,\nis developed. It employs a lightweight and reparameterized\ndesign to augment the modeling ability and expedite the in-\nference stage.\nA CNN-Transformer hybrid architecture, FMViT, is intro-\nduced based on the above methods. Analogous to NextViT\n(Li et al. 2022a), the use of TensorRT and CoreML signi-\nfies real deployed architectures in server-side and mobile\ndevices, respectively, with their inference latency represent-\ning the actual time consumption in the industry. As depicted\nin Figure 1, FMViT achieves an optimal balance between\ndelay and accuracy in the ImageNet-1K classification task.\nOn TensorRT, FMViT surpasses Resnet101 by 2.5% in top-1\naccuracy on the ImageNet dataset, maintaining a compara-\nble inference latency. Concurrently, it exhibits performance\non par with EfficientNet-B5, enhancing the inference speed\nby 43%. On CoreML, the top-1 accuracy on the ImageNet\ndataset exceeds MobileOne by 2.6% while maintaining a\nsimilar inference latency.\nOur major contributions are outlined below:\n\u2022 An efficient multi-frequency Fusion Block (FMB) is pro-\nposed to combine multiple sets of high-frequency and\nlow-frequency features, enhance the information flow of\nthe model, and enhance the expression ability of the\nmodel.\n\u2022 Proposes a lightweight Convolutional Fusion Block\n(CFB), which efficiently blends the local modeling ca-\npabilities of Convolutions and uses convolutional multi-\ngroup reparameterization to further provide modeling\nperformance.\n\u2022 Convolutional multi-group reparameterization is pro-\nposed, which fuses the spatial information of different\nsubchannels in the training stage and fuses it into a con-\nvolution in the inference stage, so as to improve the ac-\ncuracy of the model while the inference speed is un-\nchanged.\n\u2022 Multiple groups of Multilayer Perceptron Layer (gMPL)\nblocks were proposed to fuse global signals and local in-\nformation to enhance the expression ability of the model.\n\u2022 Proposes\na\nLightweight\nSelf-Attention\nBlock\n(RLMHSA), which adopts a lightweight and repa-\nrameterized design, enhances the global modeling ability\nof the module, and improves the speed of the inference\nstage.\n2\nRelated Work\nConvolutional Networks\nConvolutional Neural Networks (CNNs) have been the de\nfacto vision architecture standard for various computer vi-\nsion applications, such as semantic segmentation, object\nidentification, and image classification, since 2012. ResNet\n(He et al. 2016) uses residual connections to stop the net-\nwork from deteriorating and keep the network deep and\nable to capture high-level abstractions. On the other hand,\nDenseNet (Huang, Liu, and Weinberger 2016) promotes the\nconcatenation of feature maps and the reuse of features.\nConvolution is introduced point-wise and depth-wise by\nMobileNets (Howard et al. 2017; Sandler et al. 2018) to cre-\nate models with less memory usage and quicker response\ntimes. By using group point-wise convolution and channel\nshuffling, ShuffleNet (Zhang et al. 2018) substantially low-\ners computing expenses. According to ShuffleNetv2 (Ma\net al. 2018), network architecture design should prioritize\ndirect metrics, such as speed, over indirect metrics, like\nFLOPs. ConvNeXt (Ma et al. 2018) explores the structure of\nvision Transformers and proposes a pure CNN model that,\nwhile retaining the simplicity and efficiency of traditional\nCNNs, can effectively compete with state-of-the-art hierar-\nchical vision Transformers across a range of computer vision\nbenchmarks.\nVision Transformers\nThe concept of the Transformer was initially introduced in\nthe field of natural language processing (NLP). ViT (Doso-\nvitskiy et al. 2021), in implementing self-attention, segments\nthe image into patches and treats these patches as words,\nthereby demonstrating the Transformer\u2019s efficacy in various\nvision-related tasks. The teacher-student method proposed\nby DeiT (Dosovitskiy et al. 2021) is designed explicitly\nfor Transformers. T2T-ViT (Yuan et al. 2021) introduces a\nunique token-to-token process to progressively tokenize im-\nages into tokens and aggregate them structurally. The Swin\nTransformer (Liu et al. 2021) introduces a universal back-\nbone that constructs hierarchical features with a computa-\ntional cost linearly proportional to the image size. Mean-\nwhile, PiT (Heo et al. 2021) incorporates a pooling layer\ninto ViT and conducts experiments to validate its efficacy.\nHybrid Models\nRecent studies indicate that a hybrid design (Zhou et al.\n2022; Li et al. 2022d; Srinivas et al. 2021; Mehta and\nRastegari 2022), integrating both convolution and Trans-\nformer, effectively leverages the strengths of both architec-\ntures. BoTNet (Srinivas et al. 2021) employs global self-\nattention to supplant the spatial convolutions of the final\nthree bottleneck blocks in ResNet. Concurrently, lightweight\nand efficient ViTs, such as MobileViT (Mehta and Rastegari\n2022) and MobileViTv2 (Mehta and Rastegari 2022), have\nbeen introduced for mobile devices. The fusion of Mobile-\nFormer (Chen et al. 2022) with the proposed lightweight\ncross-attention model enhances computational efficiency\nand boosts representational power. EfficientFormer (Li et al.\n2022d) and EfficientFormerV2 (Li et al. 2022c) adhere\nto size-consistent designs, seamlessly employing hardware-\nfriendly 4D MetaBlocks and potent 3D MHSA blocks for\njoint size and speed search via NAS. ToMe presents a ViT\nmodel that accelerates without training. BiFormer estab-\nlishes an efficient pyramid network architecture through\nbidirectional attention routing. NextViT (Li et al. 2022a)\ncaptures one high-frequency feature and one low-frequency\nfeature in the network separately, which are then blended to\nenhance the modeling capabilities of the model.\nStructural Reparameterization\nReparameterization employs complex modules to enhance\nmodel performance during the training phase. It consoli-\ndates these complex modules into simpler ones during the\ninference phase, following the linear principle of the convo-\nlution operator. This process aims to boost the model\u2019s in-\nference speed without compromising performance. ACNet\n(Ding et al. 2019) pioneered reparameterization to merge\n3x3 convolutions into a 1x1 convolution, while RepVGG\n(Ding et al. 2021c) applied reparameterization to skip con-\nnections, thereby reducing memory access costs. DBB (Ding\net al. 2021a) further expanded upon six prevalent reparame-\nterization methods. The concept of linear training time over-\nparameterization was introduced to augment the power of\nsuch models (Huang et al. 2022; Hu et al. 2022). MobileOne\n(Vasu et al. 2022) employs over-parameterization to enhance\nthe performance of vision converters for mobile devices.\n3\nMethods\nThis section introduces the proposed FMViT architecture,\nfollowed by a discussion and analysis of its key designs.\nThese include the Convolutional Fusion Module (CFB), the\nMulti-frequency Mixing Module (FMB), the Lightweight\nMulti-head Attention Module (RLMHSA), the Convolu-\ntional Multi-group Reparameterization method (gMLP), and\nthe MLP module constructed using this method.\nOverview\nFigure 2 illustrates the comprehensive FMViT architec-\nture. FMViT utilizes a conventional pyramidal structure,\nwith each stage comprising a downsampling module and\na Convolutional Fusion Module (CFB) with convolution\nonly or a Multi-frequency Mixing Module (FMB) with\ntransformers. The stem reduces the spatial resolution to a\nquarter of the original input image, and each subsequent\nstage progressively halves the spatial resolution while in-\ncrementally increasing the number of channels. We ex-\nplore the information interaction module and, inspired by\nMetaFormer (Yu et al. 2022), introduce the Convolutional\nFusion Module (CFB) to address short-range data depen-\ndencies. A Multi-frequency Mixing Module (FMB) is pro-\nposed to further fuse local and global data by decompos-\ning multiple frequency bands. This multi-channel frequency\nfusion enhances the model\u2019s capacity for modeling. To de-\ncrease the computational complexity of Multi-head Atten-\ntion (MHSA), we propose a lightweight RLMHSA mod-\nule. The model\u2019s inference speed is improved without sig-\nnificantly compromising accuracy through parameter shar-\ning and re-parameterization. These core modules develop a\nseries of CNN-Transformer hybrid architectures that achieve\nan optimal balance between accuracy and latency on mobile-\nside CPUs and server-side GPUs, surpassing the state-of-\nthe-art model.\nConvolutional multi-group reparameterization\nThe 1x1 convolution is a linear fusion or channel conversion\nmechanism with global modeling capacity. The translation\ninvariance characteristic of the kxk convolution is utilized\nto depict local space. The lack of local modeling between\nspecific neighboring channel features limits ungrouped con-\nvolution operators\u2019 efficient information fusion capabilities.\nDuring the training phase, we suggest group reparameteriza-\ntion of kxk convolutions, initially defining the convolution\noperation as CONV(Kh,Kw,G), where Kh and Kw repre-\nsent the convolution kernel sizes, and G denotes the convo-\nlution group size.\nAssuming that the original convolution is defined as\nCONVA=CONV(Kh,Kw,G), during the training phase,\nmultiple convolutions with diverse group sizes are con-\nnected in parallel:\nCONV B = CONV A +\nN\nX\ni=1\nCONV (Khi, Kwi, Gi) (1)\nWhere \u2200i \u2208 N, Gi >= G, Khi <= Kh, Kwi <= Kw, and\nN is a predefined constant.\nFigure\n3\nillustrates\nthe\nreparameterization\nof\nCONVB into CONVA during the inference phase. Any\nCONV(Khi,Kwi,Gi) convolution is equivalent to the sparse\nCONV(Kh,Kw, G) convolution, signifying that the weight\nof the dotted line in the figure remains at a constant zero,\nwhile the other weights are unchanged. Based on additivity\n(Ding et al. 2021a), the inference stage for two convolutions\nwith the same number of groups can be reparameterized as\nthe convolution CONVA, as depicted in the lower portion\nof Figure 3. Here, the left side represents the training\nstage, and the right side represents the inference stage,\nboth of which are equivalent. Convolutional multi-group\nreparameterization enhances model performance without\naffecting the primary model\u2019s inference time.\nRepMLP (Ding et al. 2021b) proposes the reparameter-\nization of the multilayer perceptron (MLP). RepMLP em-\nploys convKxK for fusion into FC, but its weight con-\nversion is sparse, and the parameters post-conversion be-\ncome KxK times the original, rendering it unsuitable for\nlightweight scenarios. In the Transformer, MLPs signifi-\ncantly contribute to performance due to their global mod-\neling capabilities. However, the absence of local modeling\ncapability restricts its potential. To aid the original convolu-\ntions in grouping modeling of local channels, multiple par-\nallel conv1x1 convolutions with G\u2032 > 1 are introduced to\nthe two original CONV(1,1,1) convolutions, reconstructing\nstem\nCFB\nFMB\nFMB\nFMB\nBN\nAvg-Pool\nFC\nStage 1\nx N1\nStage 2\nx N2\nStage 3\nx N3\nStage 4\nx N4\ninput\noutput\npatch embedding\npatch embedding\npatch embedding\npatch embedding\nDWConv3x3\nCFB\nBN\nConv1x1g\nBN\nMLP\nTraining\nInference\nDWConv3x3\nMLP\nConv3x3\nS2\nReLU\nStem\nBN\nConv1x1\nS2\nBN\nConv3x1\nS2\nBN\nConv1x3\nS2\nBN\nDWConv3x3\nS2\nBN\nConv1x1\nS2g\nBN\nReLU\nConv1x1S1\nBN\nConv1x1S1g\nBN\nReLU\nConv3x3\nS2\nReLU\nDWConv3x3\nS2\nReLU\nConv1x1S1\nReLU\nTraining\nInference\nconcat\nRLMHSA\nMLP\nCFB\nFMB\nCFB\nCFB\nConv1x1\npatch \nembedding\npatch \nembedding\npatch \nembedding\npatch \nembedding\nConv1X1\nMLP\nBN\nConv1x1g\nDWConv3x3\nConv1x1g\nBN\nReLU\nConv1x1\nConv1x1g\nConv1x1\nDWConv3x3\nReLU\nConv1x1\nTraining\nInference\nk\nv\nq\nrp\nConv1x1\nT\nS\nrp\nrp\nrp\nConv1x1g\nConv3x3g\nk\nv\nq\nrp\nConv3x3\nT\nS\nrp\nrp\nrp\nTraining\nInference\nRLMHSA\nIdentity\ninp=outp,s=1\ninp!=outp,s=1\nConv1X1\nBN\nConv1X1g\nBN\ns=2\nConv1X1\nBN\nConv1X1g\nBN\nAvg-pool\nIdentity\ninp=outp,s=1\ninp!=outp,s=1\nConv1X1\nBN\ns=2\nConv1X1\nBN\nAvg-pool\nInference\nTraining\npatch embedding\nFigure 2: The figure shows the overall structure of FMViT. It mainly includes the Convolutional Fusion Block (CFB), the multi-\nfrequency Fusion Block (FMB), the lightweight Multi-head Attention Module (RLMHSA), and the parameterized Multi-layer\nPerceptron Module (gMLP).\nthe MLP module. This approach concurrently focuses on in-\nformation from different representation subspaces at various\nlocations for efficient local representation learning.\nCONV\n\u2032X(1, 1, 1) = CONV X(1, 1, 1)+\nN\nX\ni=1\nCONV (1, 1, Gi)\n(2)\nIn this context, X=1 or X=2 signifies either CONV1\nor CONV2. CONV1 and CONV2 represent the two pre-\nreparameterized convolutions of the MLP, while the post-\nreparameterized convolutions of the MLP are also denoted\nas CONV\n\u20321 and CONV\n\u20322.\nTo enhance the hybrid modeling of global and local as-\npects in MLP, a depthwise convolution is incorporated be-\ntween the two conv1x1s. A shortcut connection ensures that\nthe global and local information flows do not interfere, and\nthe added depthwise convolution is reparameterized. Ex-\nperimental results indicate that augmenting the depthwise\nconvolution capacity for local information fusion enhances\nMLP performance by 1.96% on Imagenet1k.\nConvolutional Fusion Block(CFB)\nTransformer blocks have demonstrated significant results\nacross various vision tasks, with the attention-based token\nmixer module and the MetaFormer (Yu et al. 2022) paradigm\nunderscoring their inherent advantages. However, the in-\nference speed of Transformer blocks could be more effi-\ncient, particularly on mobile devices where the performance\nof multi-head attention, LayerNorm, and GELU calcula-\ntions is suboptimal. We have demonstrated the success of\nthe MetaFormer paradigm and, building upon it, propose\nan efficient CFB module that exclusively employs depth-\nwise separable convolutions (DWConv) as the token mixer.\nCFB maintains the deployment advantage of the BottleNeck\nblock while achieving comparable performance to a Trans-\nformer block, as depicted in Figure 2. The CFB is con-\nstructed using DWConv and MLP, adhering to the general\nMetaFormer design. CFB ensures performance while signif-\nicantly improving inference deployment performance. Addi-\ntionally, reparameterization is implemented during training\nto enhance further CFB\u2019s performance, where DWConv uti-\nlizes a standard, widely-used reparameterization. The MLP\nemploys the convolutional multigroup reparameterization\nproposed in this work.\nMulti-frequency Fusion Block(FMB)\nWhile CFB has effectively learned local representation, the\npressing need to handle global information collection re-\nmains. Transformer blocks capture low-frequency signals,\nproviding global information such as shape and structure.\nExisting research indicates that Transformer blocks may par-\ntially diminish high-frequency information, including local\ntexture details. To extract more fundamental and unique fea-\ntures, signals from various frequency bands must be metic-\nulously integrated, as they are crucial to the human visual\nsystem.\nHigh-frequency signals provide local information, which\nis integral to preserving the completeness of the information.\nDistinct frequency features contribute unique information,\nrendering high-frequency signals vulnerable to degradation\nFigure 3: Top: A schematic depiction illustrating the notion of convolutional multi-group reparameterization, CONV(Khi, Kwi,\nGi) is comparable to sparse CONV(Kh, Kw, G). Bottom: By reparameterizing numerous groups of convolutions in the training\nphase, different groups of convolutions in the training phase are equal to a single convolution in the inference phase.\nby the Transformer Block. The amalgamation of various\nhigh-frequency characteristics with low-frequency features\ncould enhance the model\u2019s information flow and expressive\ncapacity, drawing inspiration from information distillation\nand frequency mixing in image super-resolution (Hui et al.\n2019). As illustrated in Figure 2, the CFB module initially\ncaptures high-frequency features, subsequently outputting\nthree sets of high-frequency features at different frequen-\ncies. Patch embedding fusion is then employed to splice the\noutput of the lightweight multi-head attention module, cre-\nating a signal with both high and low frequencies. Through\nMLP layers, more fundamental and pronounced traits are ex-\ntracted. The following formula can be expressed as follows:\nz1 = f1(xl\u22121)\nz2 = f2(z1)\nz3 = f3(z2)\nz4 = f4(z3)\nz = CONCAT(xl\u22121, z1, z2, z3, z4)\nxl = z + MLP(z)\nHere, xl\u22121 is defined as the input of the (l\u22121)th block, while\nxl signifies the output of the lth block. CONCAT refers to\nthe CAT join operation. f1\u2013f3 represent high-pass filters\nthat generate different high-frequency signals, as exempli-\nfied by CFB. f4 is the low-pass filter that produces the low-\nfrequency signal, as demonstrated by RLMHSA.\nUnlike LN and GELU, FMB consistently employs BN\nand ReLU as the effective norm and activation layers (Li\net al. 2022a). These operators can be efficiently computed,\nespecially on mobile devices, with minimal performance\nloss due to their speed-friendly nature.\nFMB can gather and integrate multi-frequency informa-\ntion within a lightweight framework, thereby significantly\nenhancing the model\u2019s performance compared to the tradi-\ntional pure Transformer module.\nLightweight Multi-head Self-Attention (RLMHSA)\nThe computational demand of the Transformer is propor-\ntional to the square of the input token dimension, making\nit computationally intensive when dealing with large input\ndimensions. Despite its relatively small number of param-\neters, the inference time on mobile devices, for instance,\nis extensive, necessitating a more lightweight design for\nthe self-attention module. ViT-LSLA (Hechen, Huang, and\nZhao 2022) substitutes the Key (K) and Value (V) of self-\nattention with the original input (X) to achieve a lightweight\nself-attention structure. As depicted in Figure 2, we propose\na lightweight multi-head self-attention method that shares\nparameters and then applies reparameterization in this study.\nThe original MSA is defined as follows:\nAtten(Q, K, V ) = Softmax(QKT )V\n(3)\nWhere Q = XWq, K = XWk, and V = XWv, input\nX \u2208 RM\u00d7d, Query, Key, and Value matrices Wq, Wk, and\nWv \u2208 Rd\u00d7d, respectively, M is the number of tokens, and\nd is the dimension of tokens. By deforming Equation 3, we\nobtain:\nAtten(Q, K, V ) = Softmax(XWq(XWk)T )XWv\n= Softmax(XWqW T\nk XT )XWv\n= Softmax(XWqkXT )XWv\n= Softmax(X(XW T\nqk)T )XWv\n= Atten(X, K\u2032, V )\nThe projection matrices of Q and K are consolidated for\nparameter sharing, which equates to a new matrix Wqk with\nK = XW T\nqk. Moreover, allowing Wqk and Wv to share\nparameters, they share a projection convolution, denoted as\nW = W T\nqk = Wv, then:\nAtten(X, K\u2032, V \u2032) = Softmax(X(XW)T )XW\n(4)\nWhere K\u2032 = XW, V \u2032 = XW, and the structure is shown\nin Figure 2 for RLMHSA. Consequently, a single convo-\nlution is required to map the input vector, with the K and\nQ vectors sharing the same convolution, thereby eliminat-\ning the need for two separate convolutions. During training,\nconvolutional multi-group parameterization is employed to\nemulate the blend of local and global information that char-\nacterizes the RLMHSA module and enhances MHSA per-\nformance. Experimental results indicate that, compared to\nMHSA on the Imagenet1k classification task, RLMHSA re-\nduces the parameter count by 8M, accelerates speed by 3%,\nand enhances performance by 0.5%.\nStem block and Patch Embedding block\nThe model\u2019s initial stage employs a stem with two down-\nsampling operations to reduce computational load, as sug-\ngested by FastViT (Vasu et al. 2023). A lightweight struc-\nture is achieved through the use of a Conv3x3+DWConv3x3\ndesign. Initial convolutions utilize Conv3x1 and Conv1x3 to\nreparameterize vertical and horizontal edge extraction.\nSeveral scenarios exist for patch embedding: no opera-\ntion is necessary if the input and output channel numbers\nand token dimensions are identical. Conv1x1 is employed\nfor channel number conversion when the input and output\nchannel numbers differ, but the token dimension remains the\nsame. If the token dimensions vary, a lightweight downsam-\npling operation, avg-pool, is utilized for downsampling, fol-\nlowed by a Conv1x1 convolution for fusion or transforma-\ntion. During the training phase, convolutional multi-group\nreparameterization is also applied to enhance accuracy.\nFMViT Architectures\nAs delineated in Table 1, this study presents five model\nstructures for reference based on the number of parameters\nand model size, specifically FMViT-T, FMViT-S, FMViT-\nM, FMViT-B, and FMViT-L. Here, \u201cChannels\u201d refers to the\nnumber of output channels from the internal submodule of\nthe module; \u201cFM-Channels\u201d denotes the number of FMB in-\ntermediate frequency division channels, and \u201cS\u201d represents\nthe stride in convolution, or Avg-pool. The expansion ratio\nfor each MLP layer is set at 2, and the head dimension in\nRLMHSA is fixed at 32. In alignment with Nextvit (Li et al.\n2022a), BatchNorm, and ReLU are employed for normaliza-\ntion and activation functions.\nTable 1: Architecture details of FMViT variants.\nStages\nOutput size\nLayers\nFMViT-T\nFMViT-S\nFMViT-M\nFMViT-B\nFMViT-L\nStem\n(H/4, W/4)\nCNN Layers\nConv3x3, S=2\nDWConv3x3, S=2\nConv1x1, S=1\n(32,32,32)\n(48,48,48)\n(64,64,64)\nStage1\n(H/4, W/4)\nPatch Embedding\nConv1x1, S=1\n32\n48\n64\nCFB\nChannels\n(32,32,32)\n(48,48,48)\n(64,96,96)\nBlocks\n3\n3\n3\n3\n6\nStage2\n(H/8, W/8)\nPatch Embedding\nAvg-Pool,S=2\nConv1x1,S=1\n32\n48\n96\nFMB\nChannels\n(32,64,80)\n(48,96,160)\n(96,128,160)\n(96,256,320)\nFM-Channels\n16\n32\n32\n64\nBlocks\n1\n1\n1\n1\nStage3\n(H/16, W/16)\nPatch Embedding\nAvg-Pool,S=2\nConv1x1,S=1\n80\n160\n160\n320\nFMB\nChannels\n(80,128,160)\n(160,192,320)\n(160,320,480)\n(320,384,480)\nFM-Channels\n32\n64\n96\n96\nBlocks\n1\n1\n1\n2\n5\nStage4\n(H/32, W/32)\nPatch Embedding\nAvg-Pool,S=2\nConv1x1,S=1\n160\n320\n480\nFMB\nChannels\n(160,192,320)\n(320,384,640)\n(480,512,960)\n(480,640,1280)\nFM-Channels\n64\n128\n192\n256\nBlocks\n1\n1\n1\n1\n4\nExperimental Results\nIn this experiment segment, we utilize PyTorch version\n1.12.1 for PyTorch inference latency and the TensorRT-8.5.3\nframework for TensorRT (TRT) inference latency. Both are\nmeasured in a hardware environment with an A10 GPU and\nCUDA 11.3. CoreML inference latency is gauged using an\niPhone 13 with iOS 16.6. All batch sizes are uniformly set\nto 1.\nTable 2: Comparison of different state-of-the-art classifica-\ntion methods for ImageNet-1K.\nMethod\nImage\nParam\nFLOPs\nLatency(ms)\nTop-1\nSize\n(M)\n(G)\npytorch\nTRT\nCoreML\n(%)\nMobileViT-XXS\n224\n1.3\n0.3\n8.1\n1.2\n13.10\n69.0\nMobileformer-52M\n224\n3.5\n0.1\n13.3\n2.9\n1.17\n72.8\nMobileformer-96M\n224\n4.6\n0.1\n14.3\n3.1\n1.86\n68.7\nRepVGG-A0\n224\n8.3\n1.4\n2.1\n0.7\n1.06\n72.4\nMobileNetV1\n224\n4.2\n0.6\n2.2\n0.5\n0.84\n70.6\nMobileNetV3-S\n224\n2.5\n0.1\n5.0\n0.7\n7.65\n67.4\nFasterNet-T0\n224\n3.9\n0.3\n5.1\n0.8\n0.72\n71.9\nMobileOne-S0\n224\n2.1\n0.3\n2.5\n0.4\n0.65\n71.4\nMobileNetV2\u00d71.0\n224\n3.5\n0.3\n5.0\n0.7\n0.87\n71.8\nDeiT-T\n224\n5.9\n1.2\n5.1\n1.2\n1.68\n72.2\nFMViT-T\n224\n2.0\n0.3\n6.7\n0.8\n0.55\n72.9\nRepVGG-B1\n224\n51.8\n11.8\n3.1\n2.5\n4.12\n78.4\nRepVGG-A2\n224\n25.5\n5.1\n2.4\n1.4\n2.11\n76.5\nMobileformer-151M\n224\n7.6\n0.2\n18.9\n4.3\n2.61\n75.2\nMobileformer-214M\n224\n9.4\n0.2\n20.1\n4.6\n2.93\n76.7\nMobileformer-294M\n224\n11.4\n0.3\n19.5\n5.0\n3.27\n77.9\nFasterNet-T1\n224\n7.6\n0.9\n5.4\n0.9\n0.93\n76.2\nMobileOne-S1\n224\n4.8\n0.8\n2.4\n0.6\n0.87\n75.9\nMobileOne-S2\n224\n7.8\n1.3\n2.5\n0.6\n0.92\n77.4\nEfficientNet-B0\n224\n5.3\n0.4\n7.9\n1.1\n1.64\n77.1\nFastViT-T8\n256\n3.6\n0.7\n4.9\n1.3\n0.92\n75.6\nEfficientFormerV2-S0\n224\n3.5\n0.4\n10.2\n1.3\n0.89\n75.7\nMobileViTv2-1.0\n256\n4.9\n1.8\n8.3\n2.4\n5.12\n78.1\nMobileViT-XS\n224\n2.3\n0.8\n8.6\n1.4\n20.84\n74.8\nFMViT-S\n224\n6.4\n0.8\n7.0\n1.1\n0.83\n78.5\nFasterNet-T2\n224\n15.0\n1.9\n5.1\n1.2\n1.44\n78.9\nMobileformer-508M\n224\n14.0\n0.5\n19.9\n5.7\n4.14\n79.3\nEfficientNet-B1\n224\n7.8\n0.6\n11.4\n1.6\n2.08\n79.1\nEfficientViT-B1\n224\n9.1\n0.5\n8.8\n0.7\n21.95\n79.4\nFastViT-T12\n256\n6.8\n1.4\n6.0\n1.7\n1.42\n79.1\nMobileOne-S4\n224\n14.8\n3.0\n4.5\n1.2\n1.52\n79.4\nDeiT-S\n224\n22.0\n4.6\n5.2\n2.0\n3.74\n79.8\nPoolFormer-S24\n224\n21.1\n3.4\n9.8\n3.9\n2.45\n80.3\nResNeXt101-32x4d\n224\n44.2\n8.0\n13.5\n3.9\n3.65\n78.8\nEfficientFormer-L1\n224\n12.3\n1.3\n6.9\n1.3\n1.57\n79.2\nFMViT-M\n224\n12.8\n2.0\n7.1\n1.5\n1.42\n80.3\nEfficientNet-B3\n224\n12.0\n1.0\n12.5\n2.0\n2.71\n81.6\nMobileViTv2-2.0\n256\n18.5\n7.2\n8.2\n3.7\n11.83\n81.2\nEfficientViT-B2\n224\n24.3\n1.6\n12.0\n2.4\n36.90\n82.1\nFasterNet-S\n224\n31.1\n4.5\n6.3\n2.2\n2.41\n81.3\nResNeSt50\n224\n27.5\n5.4\n13.0\n2.9\n32.90\n81.1\nConvNeXt-T\n224\n29.0\n4.5\n5.6\n3.5\n68.00\n82.1\nSwin-T\n224\n29.0\n4.5\n8.0\n2.4\n14.75\n81.3\nPoolFormer-S36\n224\n31.2\n5.0\n13.0\n5.7\n3.40\n81.4\nEfficientFormer-L3\n224\n31.4\n3.9\n10.3\n2.6\n2.61\n82.4\nResNet101\n224\n44.6\n7.9\n12.7\n3.4\n3.46\n80.8\nRegNetY-8G\n224\n39.2\n8.0\n11.7\n3.5\n3.65\n81.7\nResNet152\n224\n60.2\n4.0\n19.0\n6.1\n4.54\n81.7\nPoolFormer-M36\n224\n56.1\n8.8\n13.4\n7.0\n5.68\n82.1\nFastViT-SA24\n256\n20.6\n3.8\n10.1\n3.4\n2.84\n82.6\nNext-ViT-S\n224\n31.7\n5.8\n13.5\n2.8\n2.90\n82.5\nFMViT-B\n224\n24.3\n4.2\n9.1\n2.4\n2.40\n82.4\nEfficientNet-B5\n224\n30.0\n2.4\n20.1\n4.5\n4.66\n83.6\nPoolFormer-M48\n224\n73.0\n11.6\n17.4\n9.2\n7.21\n82.5\nRegNetY-16G\n224\n83.6\n16.0\n13.4\n5.9\n6.67\n82.9\nEfficientFormer-L7\n224\n82.2\n10.2\n15.4\n5.1\n6.49\n83.3\nFasterNet-M\n224\n53.5\n8.7\n9.4\n3.7\n4.28\n83.0\nResNeSt101\n224\n48.0\n10.2\n25.0\n5.7\n42.20\n83.0\nConvNeXt-S\n224\n50.0\n8.7\n10.1\n6.5\n147.30\n83.1\nSwin-S\n224\n50.0\n8.7\n15.3\n4.3\n20.63\n83.0\nNext-ViT-B\n224\n44.8\n8.3\n20.5\n4.0\n3.75\n83.2\nFMViT-L\n224\n35.3\n7.1\n15.2\n3.9\n3.30\n83.3\nImageNet-1K Classification\nImplementation\nWe executed an image classification ex-\nperiment on the ImageNet-1K (Russakovsky et al. 2015)\ndataset, comprising approximately 1.28 million training im-\nages and 50,000 validation images across 1,000 categories.\nTo maintain fairness, we replicated the training parame-\nters of the most recent vision Transformer with minor ad-\njustments. All FMViT variants underwent training on eight\nV100 GPUs with a total batch size 2048 for 300 iterations.\nThe input image was resized to a resolution of 224 x 224.\nUtilizing a weight decay of 0.1, we employed the AdamW\n(Loshchilov and Hutter 2019) optimizer. For all FMViT vari-\nants, the learning rate was gradually reduced based on the\ncosine strategy, starting at 4e-5, and a linear warm-up ap-\nproach was used for 20 epochs.\nComparison with State-of-the-art Models\nAs illustrated\nin Table 2, our method achieves an optimal balance of ac-\ncuracy and latency when juxtaposed with recent state-of-\nthe-art techniques such as CNNs, ViTs, and hybrid net-\nworks. When benchmarked against renowned CNNs like\nResNet101 (He et al. 2016), FMViT surpasses ResNet101\nby 2.5% in top-1 accuracy on the ImageNet dataset (83.3%\nvs. 80.8%) and is 45% faster on CoreML (3.5 ms vs. 2.4 ms).\nConcurrently, its performance is on par with EfficientNet-B5\n(Tan and Le 2019), with a 43% improvement in inference\nspeed. In the context of ViT, FMViT-B outperforms Swin-T\n(Liu et al. 2021) by being up to 6x faster on CoreML and\nTensorRT, yet with a 1.1% superior performance. FMViT-S\nexceeds DeiT-T (Touvron et al. 2021) by 6.3% on TensorRT\nat an equivalent inference speed (78.5% vs. 72.2%). FMViT-\nS surpasses CoreML by 8.1 percent (80.3% vs. 72.2%) with\na similar inference speed. FMViT-L matches the perfor-\nmance of EfficientFormer-L7 (Li et al. 2022d) when com-\npared to the hybrid approach, but the inference is 30% and\n96% faster on TensorRT and CoreML, respectively. FMViT-\nS achieves 2.6% higher performance (78.5% vs. 75.9%) with\na comparable CoreML inference speed when compared to\nMobileOne-S1 (Vasu et al. 2022), and CoreML achieves\n11% faster inference performance while achieving 2.9%\nhigher accuracy (78.5% vs. 75.6%). These results suggest\nthat the proposed FMViT design is a feasible and promising\nparadigm.\nObject Detection and Instance Segmentation\nImplementation\nWe evaluate FMViT on object detection\nand instance segmentation tasks based on the Mask R-CNN\n(He et al. 2017) architecture and COCO2017 (Lin et al.\n2014). Specifically, all our models are initially trained on\nImageNet-1K and subsequently fine-tuned using the settings\nfrom previous work (Liu et al. 2021). The AdamW optimizer\nis employed with a weight decay of 0.05, and the training\nspans 12 epochs. A warm-up of 500 iterations is performed\nduring training, and the learning rate is decreased by 10 at\nthe 8th and 11th epochs. Input resolution is 1344x800. For\nan equitable comparison, we solely assess backbone latency,\nand the testing environment aligns with that of classification.\nComparison\nwith\nState-of-the-art\nModels\nTable\n3\npresents the evaluation results utilizing the Mask R-CNN\narchitecture. For fairness, we exclusively measured the\nbackbone latency. As per Table 3, FMViT-B surpasses\nResNet101 3.7 AP b while achieving a 16% faster infer-\nence on TensorRT. FMViT-B matches the inference speed\nof PoolFormer-S12 (Yu et al. 2022), on TensorRT but with\na 6.8 AP b enhancement. Compared to EfficientFormer-L3,\nFMViT-B exhibits a 7% faster inference on TensorRT and\na 2.7 AP b superior performance. Against Next-ViT-S (Li\net al. 2022a), FMViT-B demonstrates a 3.9 times faster in-\nference on CoreML and a 0.3 AP b increased performance.\nFMViT-L outperforms EfficientFormer-L7 by 3.8 AP b, and\nits inference is 32% quicker on TensorRT. FMViT-L and\nResNeSt101 have identical inference speeds on TensorRT,\nbut FMViT-L shows a 1.2 AP b higher performance. The\nAP for masks exhibits a similar advantage. In conclusion,\nFMViT excels in object detection and instance segmentation\nwhile maintaining a reduced inference latency.\nADE20K Semantic Segmentation\nImplementation\nWe conducted semantic segmentation\ntests utilizing the ADE20K (Zhou et al. 2017) dataset, which\ncomprises approximately 20K training images and 2K val-\nidation images across 150 categories. For a fair compari-\nson, we adhered to the training protocol of the preceding\nvision transformer on the Semantic FPN (Kirillov et al.\n2019) framework. The model was initially pre-trained on\nImageNet-1K at a resolution of 224x224, then trained on\nADE20K with an input size of 512x512. For the Semantic\nFPN framework, we employed the AdamW optimizer with\na learning rate and weight decay of 0.0001. We trained the\nentire network for 40K iterations with a total batch size of\n32. Given the complexity of implementing various modules\nof Mask R-CNN on TensorRT and CoreML, we limited our\nlatency assessment to the backbone for a fair comparison,\nmaintaining the same test setup as for classification. For sim-\nplicity, We used an input size of 512x512 to measure latency.\nComparison with State-of-the-art Models\nTable 4 illus-\ntrates that FMViT-B surpasses ResNet101 (He et al. 2016)\nby 4.7 mIoU while maintaining consistent inference speed\non TensorRT and CoreML. It exceeds Swin-T(Liu et al.\n2021) by 2.0 mIoU. Compared to PoolFormer-S24 (Yu et al.\n2022), it achieves 3.2 mIoU higher performance and is 8%\nfaster in TensorRT inference. Our performance improvement\nof 0.5 mIoU is accompanied by 18% and 43% faster infer-\nence on TensorRT and CoreML, respectively, compared to\nNext-ViT-S. FMViT-L outperforms Swin-S by 1.7 mIoU and\nis 4.5 mIoU higher than CoreML while being 25 times faster\nthan ResNeSt101 (Zhang et al. 2022). It matches the infer-\nence performance of PoolFormer-S36 but with a 4.9 mIoU\nadvantage. Inference on TensorRT and CoreML is 2.5% and\n29% faster than Next-ViT-B, with comparable mIoU. Com-\nprehensive testing indicates that our FMViT holds signifi-\ncant potential for segmentation tasks.\nAblation Study\nWe established a series of experiments to validate the effi-\nciency of the FMB, gMPL, and RLMHSA modules within\nFMViT, as depicted in Table 5. Here, we incorporated our\nproposed modules into the FMViT-T0 model and adhered\nto the same training methodology as the original model.\nRLMHSA substitutes the traditional MHSA, gMPL super-\nsedes the standard MPL, and FMB is not utilized for mixing;\nonly the standard MHSA output features are directly fed into\nthe MLP.\nThe experimental findings indicate that substituting the\nstandard MHSA with our more streamlined RLMHSA de-\ncreases classification performance despite increasing param-\neters and FLOPs. When the conventional MLP module is re-\nplaced with the convolutional multi-group reparameterized\ngMLP, the number of parameters and FLOPs during the in-\nference stage remain comparable, yet classification perfor-\nmance improves. Lastly, introducing the FMB module sig-\nnificantly increases the number of parameters and FLOPs,\nTable 3: Comparison of different backbones on Mask R-CNN-based object detection and instance segmentation tasks. The\nsuperscripts b and m denote box detection and mask instance segmentation.\nbackbone\nImage\nParam\nFLOPs\nLatency(ms)\nMask R-CNN\nSize\n(M)\n(G)\npytorch\nTRT\nCoreML\nAP b\nAP b\n50\nAP b\n75\nAP m\nAP m\n50\nAP m\n75\nResNet101\n1344x800\n44.5\n167.8\n34.0\n26.9\n66.2\n40.4\n61.1\n44.2\n36.4\n57.7\n38.8\nResNeXt101-32x4d\n1344x800\n44.2\n171.7\n40.5\n35.1\n69.2\n41.9\n62.5\n45.9\n37.5\n59.4\n40.2\nResNeSt50\n1344x800\n27.5\n115.6\n35.9\n24.1\n/\n42.6\n/\n/\n38.1\n/\n/\nSwin-T\n1344x800\n47.8\n264.0\n/\n/\n/\n42.2\n64.4\n46.2\n39.1\n64.6\n42.0\nPoolFormer-S12\n1344x800\n11.9\n39.0\n48.3\n23.1\n32.1\n37.3\n59.0\n40.1\n34.6\n55.8\n36.9\nPoolFormer-S24\n1344x800\n21.4\n73.1\n95.8\n45.6\n58.8\n40.1\n62.2\n43.4\n37.0\n59.1\n39.6\nEfficientFormer-L3\n1344x800\n31.4\n89.6\n30.5\n24.7\n66.1\n41.4\n63.9\n44.7\n38.1\n61.0\n40.4\nNext-ViT-S\n1344x800\n51.8\n301.1\n45.5\n34.7\n843.0\n43.8\n65.7\n47.9\n39.8\n63.0\n42.6\nFMViT-B\n1344x800\n72.1\n269.9\n27.8\n23.1\n213.8\n44.1\n66.1\n48.0\n41.8\n65.1\n45.0\nResNeXt101-64x4d\n1344x800\n83.5\n332.5\n62.8\n65.2\n107.8\n42.8\n63.8\n47.3\n38.4\n60.6\n41.3\nResNeSt101\n1344x800\n48.3\n219.4\n63.2\n42.1\n/\n45.2\n/\n/\n40.2\n/\n/\nSwin-S\n1344x800\n69.1\n354.0\n/\n/\n/\n44.8\n66.6\n48.9\n40.9\n63.4\n44.2\nPoolFormer-S36\n1344x800\n30.9\n107.2\n143.3\n68.9\n89.2\n41.0\n63.1\n44.8\n37.7\n60.1\n40.0\nEfficientFormer-L7\n1344x800\n82.2\n228.6\n66.9\n55.6\n155.5\n42.6\n65.1\n46.1\n39.0\n62.2\n41.7\nNext-ViT-B\n1344x800\n64.9\n354.1\n61.8\n48.1\n932.0\n45.3\n67.0\n49.7\n41.0\n64.2\n44.2\nFMViT-L\n1344x800\n94.9\n333.0\n45.4\n42.0\n276.0\n46.4\n68.1\n51.2\n41.9\n65.4\n45.1\nTable 4: Comparison of different backbones on the ADE20K\nsemantic segmentation task.\nbackbone\nImage\nLatency(ms)\nSemantic FPN 80k\nSize\npytorch\nTRT\nCoreML\nParam(M)\nFLOPs(G)\nmIoU\nResNet101\n512x512\n12.9\n7.4\n10.9\n44.5\n40.9\n38.8\nResNeXt101-32x4d\n512x512\n13.8\n10.1\n14.1\n44.2\n41.8\n39.7\nResNeSt50\n512x512\n12.7\n6.5\n356.0\n27.5\n28.2\n39.7\nSwin-T\n512x512\n/\n/\n/\n31.9\n182.0\n41.5\nPoolFormer-S12\n512x512\n9.1\n4.1\n6.2\n11.9\n9.5\n37.2\nPoolFormer-S24\n512x512\n17.8\n7.8\n10.8\n21.4\n17.8\n40.3\nEfficientFormer-L3\n512x512\n10.5\n6.6\n10.0\n31.4\n20.7\n43.5\nNext-ViT-S\n512x512\n14.5\n8.5\n15.6\n36.3\n52.0\n43.0\nFMViT-B\n512x512\n9.0\n7.2\n10.9\n24.8\n22.2\n43.5\nResNeXt101-64x4d\n512x512\n17.9\n17.4\n22.5\n83.5\n81.1\n40.2\nResNeSt101\n512x512\n25.1\n11.3\n423.0\n48.3\n53.5\n42.4\nSwin-S\n512x512\n/\n/\n/\n53.2\n274.0\n45.2\nPoolFormer-S36\n512x512\n26.4\n11.5\n15.2\n30.9\n26.1\n42.0\nPoolFormer-M36\n512x512\n41.5\n16.5\n22.1\n56.1\n46.0\n42.4\nEfficientFormer-L7\n512x512\n16.9\n14.4\n21.4\n82.2\n53.6\n45.1\nNext-ViT-B\n512x512\n20.8\n12.2\n21.1\n49.3\n64.9\n47.1\nFMViT-L\n512x512\n15.2\n11.9\n16.3\n36.5\n37.4\n46.9\nbut it also boosts classification accuracy to the final level.\nTable 5: Compare different modules.\nFMB\ngMPL\nRLMHSA\nParam(M)\nFLOPs(M)\nTop-1(%)\n2.61\n479.24\n68.63\n\u2713\n2.25\n338.05\n67.34\n\u2713\n\u2713\n2.25\n338.05\n69.30\n\u2713\n\u2713\n\u2713\n2.52\n395.30\n72.90\nVisualization\nNextViT (Li et al. 2022a) establishes that CNN convo-\nlutional blocks favor high-frequency signals, while ViT\nis inclined towards low-frequency signals. Our proposed\nFMB simultaneously captures diverse high-frequency and\nlow-frequency signals, thereby enabling the acquisition of\nricher texture information and more precise global infor-\nmation, enhancing the modeling capability of FMViT. To\nbetter understand FMViT, we visualize the Fourier spec-\ntrum of RLMHSA output features in FMVIT-T0 at both\nhigh and low frequencies. Within RLMHSA are five fea-\ntures with varying frequencies, each representing different\nfrequency characteristics, denoted as f1\u2013f5. Figure 4 illus-\ntrates this. The RLMHSA output feature, f1, captures the\nlow-frequency signal, suggesting that RLMHSA excels at\nmodeling global information. f2-f5, the outputs of various\nCFBs, capture different high-frequency signals. Each output\ncorresponds to a distinct frequency, so they are proficient\nat handling various textures. The fusion of f1-f5 frequency\nfeatures enhances the model\u2019s expressive capacity.\nLayer3\nLayer2\nLayer1\nf1\nf2\nf3\nf4\nf5\nFigure 4: The Fourier spectrum of the output features of dif-\nferent modules in FMViT.\n5\nConclusion\nIn this study, we introduce a hybrid ViT architecture op-\ntimized for efficient deployment on mobile devices and\nserver GPUs. This architecture enhances the model\u2019s pre-\ndictive power by amalgamating high-frequency and low-\nfrequency features at varying frequencies, thereby bolster-\ning the model\u2019s capacity to capture both local and global\ninformation. Experimental results demonstrate that FMViT\nachieves state-of-the-art latency and accuracy trade-offs\nacross multiple vision tasks, including image classification,\nobject detection, and semantic segmentation. However, the\nmodels we provided are stacked together without further\nscrutiny. Future work could employ Network Architecture\nSearch (NAS) or other stacking methods to explore the im-\npact of different combination models on performance.\nReferences\nChen, Y.; Dai, X.; Chen, D.; Liu, M.; Dong, X.; Yuan, L.;\nand Liu, Z. 2022. Mobile-Former: Bridging MobileNet and\nTransformer.\nIn IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2022, New Orleans,\nLA, USA, June 18-24, 2022, 5260\u20135269. IEEE.\nDing, X.; Guo, Y.; Ding, G.; and Han, J. 2019.\nACNet:\nStrengthening the Kernel Skeletons for Powerful CNN via\nAsymmetric Convolution Blocks. In 2019 IEEE/CVF In-\nternational Conference on Computer Vision, ICCV 2019,\nSeoul, Korea (South), October 27 - November 2, 2019,\n1911\u20131920. IEEE.\nDing, X.; Zhang, X.; Han, J.; and Ding, G. 2021a. Diverse\nBranch Block: Building a Convolution as an Inception-Like\nUnit. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2021, virtual, June 19-25, 2021, 10886\u2013\n10895. Computer Vision Foundation / IEEE.\nDing, X.; Zhang, X.; Han, J.; and Ding, G. 2021b. RepMLP:\nRe-parameterizing Convolutions into Fully-connected Lay-\ners for Image Recognition. CoRR, abs/2105.01883.\nDing, X.; Zhang, X.; Ma, N.; Han, J.; Ding, G.; and Sun,\nJ. 2021c. RepVGG: Making VGG-Style ConvNets Great\nAgain. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2021, virtual, June 19-25, 2021, 13733\u2013\n13742. Computer Vision Foundation / IEEE.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\nHe, K.; Gkioxari, G.; Doll\u00b4ar, P.; and Girshick, R. B. 2017.\nMask R-CNN. In IEEE International Conference on Com-\nputer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017,\n2980\u20132988. IEEE Computer Society.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Resid-\nual Learning for Image Recognition. In 2016 IEEE Confer-\nence on Computer Vision and Pattern Recognition, CVPR\n2016, Las Vegas, NV, USA, June 27-30, 2016, 770\u2013778.\nIEEE Computer Society.\nHechen, Z.; Huang, W.; and Zhao, Y. 2022. ViT-LSLA: Vi-\nsion Transformer with Light Self-Limited-Attention. CoRR,\nabs/2210.17115.\nHeo, B.; Yun, S.; Han, D.; Chun, S.; Choe, J.; and Oh, S. J.\n2021. Rethinking Spatial Dimensions of Vision Transform-\ners. In 2021 IEEE/CVF International Conference on Com-\nputer Vision, ICCV 2021, Montreal, QC, Canada, October\n10-17, 2021, 11916\u201311925. IEEE.\nHoward, A. G.; Zhu, M.; Chen, B.; Kalenichenko, D.; Wang,\nW.; Weyand, T.; Andreetto, M.; and Adam, H. 2017. Mo-\nbileNets: Efficient Convolutional Neural Networks for Mo-\nbile Vision Applications. CoRR, abs/1704.04861.\nHu, M.; Feng, J.; Hua, J.; Lai, B.; Huang, J.; Gong, X.; and\nHua, X. 2022. Online Convolutional Re-parameterization.\nCoRR, abs/2204.00826.\nHuang,\nG.;\nLiu,\nZ.;\nand\nWeinberger,\nK.\nQ.\n2016.\nDensely Connected Convolutional Networks.\nCoRR,\nabs/1608.06993.\nHuang, T.; You, S.; Zhang, B.; Du, Y.; Wang, F.; Qian, C.;\nand Xu, C. 2022. DyRep: Bootstrapping Training with Dy-\nnamic Re-parameterization. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2022, New\nOrleans, LA, USA, June 18-24, 2022, 578\u2013587. IEEE.\nHui, Z.; Gao, X.; Yang, Y.; and Wang, X. 2019. Lightweight\nImage Super-Resolution with Information Multi-distillation\nNetwork. In Amsaleg, L.; Huet, B.; Larson, M. A.; Gravier,\nG.; Hung, H.; Ngo, C.; and Ooi, W. T., eds., Proceedings\nof the 27th ACM International Conference on Multimedia,\nMM 2019, Nice, France, October 21-25, 2019, 2024\u20132032.\nACM.\nKirillov, A.; Girshick, R. B.; He, K.; and Doll\u00b4ar, P. 2019.\nPanoptic Feature Pyramid Networks. In IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2019,\nLong Beach, CA, USA, June 16-20, 2019, 6399\u20136408. Com-\nputer Vision Foundation / IEEE.\nKitaev, N.; Kaiser, L.; and Levskaya, A. 2020. Reformer:\nThe Efficient Transformer. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nLi, J.; Xia, X.; Li, W.; Li, H.; Wang, X.; Xiao, X.; Wang, R.;\nZheng, M.; and Pan, X. 2022a. Next-ViT: Next Generation\nVision Transformer for Efficient Deployment in Realistic In-\ndustrial Scenarios. CoRR, abs/2207.05501.\nLi, W.; Wang, X.; Xia, X.; Wu, J.; Xiao, X.; Zheng, M.;\nand Wen, S. 2022b. SepViT: Separable Vision Transformer.\nCoRR, abs/2203.15380.\nLi, Y.; Hu, J.; Wen, Y.; Evangelidis, G.; Salahi, K.; Wang,\nY.; Tulyakov, S.; and Ren, J. 2022c.\nRethinking Vi-\nsion Transformers for MobileNet Size and Speed. CoRR,\nabs/2212.08059.\nLi, Y.; Yuan, G.; Wen, Y.; Hu, J.; Evangelidis, G.; Tulyakov,\nS.; Wang, Y.; and Ren, J. 2022d. EfficientFormer: Vision\nTransformers at MobileNet Speed. In NeurIPS.\nLin, T.; Maire, M.; Belongie, S. J.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll\u00b4ar, P.; and Zitnick, C. L. 2014. Microsoft\nCOCO: Common Objects in Context. In Fleet, D. J.; Pa-\njdla, T.; Schiele, B.; and Tuytelaars, T., eds., Computer Vi-\nsion - ECCV 2014 - 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings, Part V,\nvolume 8693 of Lecture Notes in Computer Science, 740\u2013\n755. Springer.\nLiu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.;\nand Guo, B. 2021. Swin Transformer: Hierarchical Vision\nTransformer using Shifted Windows.\nIn 2021 IEEE/CVF\nInternational Conference on Computer Vision, ICCV 2021,\nMontreal, QC, Canada, October 10-17, 2021, 9992\u201310002.\nIEEE.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\ncay Regularization.\nIn 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net.\nMa, N.; Zhang, X.; Zheng, H.; and Sun, J. 2018.\nShuf-\nfleNet V2: Practical Guidelines for Efficient CNN Archi-\ntecture Design. In Ferrari, V.; Hebert, M.; Sminchisescu,\nC.; and Weiss, Y., eds., Computer Vision - ECCV 2018 -\n15th European Conference, Munich, Germany, September 8-\n14, 2018, Proceedings, Part XIV, volume 11218 of Lecture\nNotes in Computer Science, 122\u2013138. Springer.\nMehta, S.; and Rastegari, M. 2022.\nMobileViT: Light-\nweight, General-purpose, and Mobile-friendly Vision Trans-\nformer. In The Tenth International Conference on Learn-\ning Representations, ICLR 2022, Virtual Event, April 25-29,\n2022. OpenReview.net.\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh,\nS.; Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein,\nM. S.; Berg, A. C.; and Fei-Fei, L. 2015. ImageNet Large\nScale Visual Recognition Challenge. Int. J. Comput. Vis.,\n115(3): 211\u2013252.\nSandler, M.; Howard, A. G.; Zhu, M.; Zhmoginov, A.; and\nChen, L. 2018. MobileNetV2: Inverted Residuals and Lin-\near Bottlenecks.\nIn 2018 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2018, Salt Lake City,\nUT, USA, June 18-22, 2018, 4510\u20134520. Computer Vision\nFoundation / IEEE Computer Society.\nSrinivas, A.; Lin, T.; Parmar, N.; Shlens, J.; Abbeel, P.;\nand Vaswani, A. 2021. Bottleneck Transformers for Visual\nRecognition. In IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2021, virtual, June 19-25, 2021,\n16519\u201316529. Computer Vision Foundation / IEEE.\nTan, M.; and Le, Q. V. 2019. EfficientNet: Rethinking Model\nScaling for Convolutional Neural Networks. In Chaudhuri,\nK.; and Salakhutdinov, R., eds., Proceedings of the 36th In-\nternational Conference on Machine Learning, ICML 2019,\n9-15 June 2019, Long Beach, California, USA, volume 97\nof Proceedings of Machine Learning Research, 6105\u20136114.\nPMLR.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J\u00b4egou, H. 2021. Training data-efficient image trans-\nformers & distillation through attention.\nIn Meila, M.;\nand Zhang, T., eds., Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event, volume 139 of Proceedings of Machine\nLearning Research, 10347\u201310357. PMLR.\nTu, Z.; Talebi, H.; Zhang, H.; Yang, F.; Milanfar, P.; Bovik,\nA. C.; and Li, Y. 2022. MaxViT: Multi-axis Vision Trans-\nformer. In Avidan, S.; Brostow, G. J.; Ciss\u00b4e, M.; Farinella,\nG. M.; and Hassner, T., eds., Computer Vision - ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23-27,\n2022, Proceedings, Part XXIV, volume 13684 of Lecture\nNotes in Computer Science, 459\u2013479. Springer.\nVasu, P. K. A.; Gabriel, J.; Zhu, J.; Tuzel, O.; and Ranjan,\nA. 2022. An Improved One millisecond Mobile Backbone.\nCoRR, abs/2206.04040.\nVasu, P. K. A.; Gabriel, J.; Zhu, J.; Tuzel, O.; and Ranjan,\nA. 2023. FastViT: A Fast Hybrid Vision Transformer using\nStructural Reparameterization. CoRR, abs/2303.14189.\nYu, W.; Luo, M.; Zhou, P.; Si, C.; Zhou, Y.; Wang, X.; Feng,\nJ.; and Yan, S. 2022. MetaFormer is Actually What You\nNeed for Vision. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition, CVPR 2022, New Orleans,\nLA, USA, June 18-24, 2022, 10809\u201310819. IEEE.\nYuan, L.; Chen, Y.; Wang, T.; Yu, W.; Shi, Y.; Jiang, Z.; Tay,\nF. E. H.; Feng, J.; and Yan, S. 2021. Tokens-to-Token ViT:\nTraining Vision Transformers from Scratch on ImageNet. In\n2021 IEEE/CVF International Conference on Computer Vi-\nsion, ICCV 2021, Montreal, QC, Canada, October 10-17,\n2021, 538\u2013547. IEEE.\nZhang, H.; Wu, C.; Zhang, Z.; Zhu, Y.; Lin, H.; Zhang, Z.;\nSun, Y.; He, T.; Mueller, J.; Manmatha, R.; Li, M.; and\nSmola, A. J. 2022.\nResNeSt: Split-Attention Networks.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, CVPR Workshops 2022, New Or-\nleans, LA, USA, June 19-20, 2022, 2735\u20132745. IEEE.\nZhang, X.; Zhou, X.; Lin, M.; and Sun, J. 2018. ShuffleNet:\nAn Extremely Efficient Convolutional Neural Network for\nMobile Devices. In 2018 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2018, Salt Lake City,\nUT, USA, June 18-22, 2018, 6848\u20136856. Computer Vision\nFoundation / IEEE Computer Society.\nZhou, B.; Zhao, H.; Puig, X.; Fidler, S.; Barriuso, A.; and\nTorralba, A. 2017. Scene Parsing through ADE20K Dataset.\nIn 2017 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2017, Honolulu, HI, USA, July 21-26,\n2017, 5122\u20135130. IEEE Computer Society.\nZhou, D.; Yu, Z.; Xie, E.; Xiao, C.; Anandkumar, A.; Feng,\nJ.; and \u00b4Alvarez, J. M. 2022.\nUnderstanding The Robust-\nness in Vision Transformers. In Chaudhuri, K.; Jegelka, S.;\nSong, L.; Szepesv\u00b4ari, C.; Niu, G.; and Sabato, S., eds., In-\nternational Conference on Machine Learning, ICML 2022,\n17-23 July 2022, Baltimore, Maryland, USA, volume 162 of\nProceedings of Machine Learning Research, 27378\u201327394.\nPMLR.\n"
  }
]