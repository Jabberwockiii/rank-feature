[
  {
    "title": "Grandmaster-Level Chess Without Search",
    "link": "https://arxiv.org/pdf/2402.04494.pdf",
    "upvote": "61",
    "text": "Grandmaster-Level Chess Without Search\nAnian Ruoss*,1, Gr\u00e9goire Del\u00e9tang*,1, Sourabh Medapati1, Jordi Grau-Moya1, Li Kevin Wenliang1, Elliot Catt1,\nJohn Reid1 and Tim Genewein1\n*Equal contributions, 1Google DeepMind\nThe recent breakthrough successes in machine learning are mainly attributed to scale: namely large-\nscale attention-based architectures and datasets of unprecedented scale. This paper investigates the\nimpact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics,\nexplicit search, or a combination of both, we train a 270M parameter transformer model with supervised\nlearning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values\nprovided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest\nmodel reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging\nchess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our\nmodel outperforms AlphaZero\u2019s policy and value networks (without MCTS) and GPT-3.5-turbo-instruct.\nA systematic investigation of model and dataset size shows that strong chess performance only arises at\nsufficient scale. To validate our results, we perform an extensive series of ablations of design choices\nand hyperparameters.\n1. Introduction\nOne of the most iconic successes of AI is IBM\u2019s Deep\nBlue (Campbell et al., 2002) defeating the world chess\nchampion Garry Kasparov in 1997. This was widely\nseen as the first major demonstration that machines\nare capable of out-competing humans in intellectual\ndomains that require sophisticated rational reason-\ning and strategic planning\u2014feats of intelligence that\nwere long believed to be exclusive to humans. Deep\nBlue was an expert system that combined an exten-\nsive database of chess knowledge and heuristics with\na strong tree search algorithm (alpha-beta pruning).\nAlmost all modern and much stronger chess engines\nfollow a similar recipe, with Stockfish 16 currently be-\ning the world\u2019s strongest (publicly available) engine.\nNotable exceptions are DeepMind\u2019s AlphaZero (Sil-\nver et al., 2017), which uses search and self-taught\nheuristics but no human chess knowledge, and its\nopen-source replication Leela Chess Zero, which cur-\nrently often comes in as a close second in chess com-\nputer competitions (Haworth and Hernandez, 2021).\nRecent breakthroughs in scaling up AI systems have\nresulted in dramatic progress in cognitive domains\nthat remained challenging for earlier-generation sys-\ntems like Deep Blue. This progress has been driven\nby general-purpose techniques, in particular (self-) su-\npervised training on expert data with attention-based\narchitectures (Vaswani et al., 2017) applied at scale,\nresulting in the development of LLMs with impres-\nsive and unexpected cognitive abilities like OpenAI\u2019s\nGPT series (Brown et al., 2020; OpenAI, 2023), the\nLLaMA family of models (Touvron et al., 2023a,b),\nor Google DeepMind\u2019s Chinchilla (Hoffmann et al.,\n2022) and Gemini (Anil et al., 2023). However, it is\nunclear whether the same technique would work in a\ndomain like chess, where successful policies typically\nrely on sophisticated algorithmic reasoning (search,\ndynamic programming) and complex heuristics. Thus,\nthe main question of this paper is: Is it possible to use\nsupervised learning to obtain a chess policy that gener-\nalizes well and thus leads to strong play without explicit\nsearch?\nTo study this question we apply the success recipe\nof general supervised training at scale to chess (see\nFigure 1). We use a standard attention-based archi-\ntecture and a standard supervised training protocol to\nlearn to predict action-values (corresponding to win-\npercentages) for chess boards. The strength of the\nresulting chess policy thus depends entirely on the\nstrength of the underlying action-value predictor. To\nget a large corpus of \u201cground-truth\u201d action-values we\nuse Stockfish 16 as an oracle to annotate millions of\nboard states obtained from randomly drawn games on\nlichess.org, which are mostly played by humans vary-\ning significantly in playing strength. As we will show\nthis leads to a strong, grandmaster-level chess policy\n(Lichess blitz Elo 2895 against humans), driven by a\nmodern transformer to predict action-values without\nany explicit search. This policy outperforms GPT-3.5-\nturbo-instruct (and, therefore, GPT-4 (Carlini, 2023))\nand AlphaZero\u2019s policy and value networks, which\nreach Elo ratings of 1755, 1620, and 1853, respec-\ntively. Therefore, our work shows that it is possible\nCorresponding author(s): {anianr,gdelt}@google.com\n\u00a9 2024 Google DeepMind. All rights reserved\narXiv:2402.04494v1  [cs.LG]  7 Feb 2024\nGrandmaster-Level Chess Without Search\nBoard annotation via strong oracle (Stockfish 16)\nDraw N games, \nextract all unique \nboards\nAction \n[UCI]\na\nAction-value\n[Win%]\nQSF(s, a) \nBest Action \n[UCI]\naSF(s)\ne2e3\n48\nQd1f3\ne2e4\n53\n\u22ee\n\u22ee\nQd1f3\n62\nCompute SF oracle \naction-values \nCollection of \ngames \n[PGN notation]\nAction-value\nLoss:\nPolicy:\nLoss:\nPolicy:\nState-value\nLoss:\nPolicy:\nBehavioral cloning\nBoard State \n[FEN]\ns\nState-value\n[Win%]\nVSF(s)\n<FEN 1>\n51\n<FEN 2>\n54\n\u22ee\n\u22ee\n<FEN M>\n31\nN\ngames\nDataset\nfor all \nlegal actions\nLegal action leading to next state \nwith minimal predicted expected \nstate-value for opponent player.\nLegal action with maximal \npredicted expected \naction-value.\nLegal action with highest \npredicted probability.\nDatasets\nPuzzle Test set\n 1.33% overlap with \ntraining set\nSource: curated \nchess puzzles \nfrom Lichess\nSource: games \nfrom Lichess \n(range of Elo)\n10k\n1M\nTraining sets\n1k\nTest set\n14.7% overlap with training set \n(mostly early-game boards)\nPredictors\n(M \u2248 40N)\n10M\n10k\nFigure 1 | Top (Data annotation): We extract all boards from \ud835\udc41 randomly drawn games from Lichess, discard\nduplicate board states, and compute the state-value for each board as the win-probability via Stockfish. We\ncompute action-values and the best action for all legal moves of a board state in the same way. Bottom left\n(Dataset creation): We construct training and test sets of various sizes (see Table A1). Our largest training\nset has 15.3B action-values. Drawing games i.i.d. from the game database for our test set leads to 14.7% of\ntest-boards appearing in the largest training set (mostly very early game states). We also use a test set of 10k\nchess puzzles that come with a correct sequence of moves. Bottom right (Policies): We train predictors on\nthree targets (state- or action-values, or oracle actions), each of which can be used for a chess policy. Our value\npredictors are discrete discriminators (classifiers) that predict into which bin \ud835\udc67\ud835\udc56 \u2208 {\ud835\udc670, . . . , \ud835\udc67\ud835\udc3e} the oracle value\nfalls.\nto distill a good approximation of Stockfish 16 into\na feed-forward neural network via standard super-\nvised learning at sufficient scale\u2014akin to the quote\nfamously attributed to Jos\u00e9 Ra\u00fal Capablanca, world\nchess champion from 1921 to 1927: \u201cI see only one\nmove ahead, but it is always the correct one.\u201d\nWe make the following main contributions:\n\u2022 We distill an approximation of Stockfish 16 into\na neural predictor that generalizes well to novel\nboard states.\n\u2022 We construct a policy from our neural predictor\nand show that it plays chess at grandmaster level\n(Lichess blitz Elo 2895) against humans and suc-\nccessfully solves many challenging chess puzzles\n(up to Elo 2800). To the best of our knowledge\nthis is currently the strongest chess engine with-\nout explicit search.\n\u2022 We perform ablations of the model size and data\nset size, showing that robust generalization and\nstrong chess play only arise at sufficient scale.\n2. Methods\nWe now provide details on the dataset creation, the\npredictors and policies, and the evaluation (see Fig-\nure 1).\n2.1. Data\nTo construct a dataset for supervised training we down-\nload 10 million games from Lichess (lichess.org) from\nFebruary 2023. We extract all board states \ud835\udc60 from\nthese games and estimate the state-value \ud835\udc49SF(\ud835\udc60) for\neach state with Stockfish 16 using a time limit of 50ms\nper board (unbounded depth and level). The value of\na state is the win percentage estimated by Stockfish,\nlying between 0% and 100%.1 We also use Stock-\nfish to estimate action-values \ud835\udc44SF(\ud835\udc60, \ud835\udc4e) for all legal\nactions \ud835\udc4e \u2208 Alegal(\ud835\udc60) in each state. Here we use a time\nlimit of 50ms per state-action pair (unbounded depth\nand max skill level), which corresponds to an oracle\nLichess blitz Elo of 2713 (see Section 3.1). The action-\nvalues (win percentages) also determine the oracle\n1Stockfish returns a score in centipawns that we con-\nvert\nto\nthe\nwin\npercentage\nwith\nthe\nstandard\nformula\nwin% = 50% \u00b7 2/(1 + exp(\u22120.00368208 \u00b7 centipawns))\nfrom\nhttps://lichess.org/page/accuracy.\n2\nGrandmaster-Level Chess Without Search\nbest action \ud835\udc4eSF:\n\ud835\udc4eSF(\ud835\udc60) = arg max\n\ud835\udc4e\u2208Alegal(\ud835\udc60)\n\ud835\udc44SF(\ud835\udc60, \ud835\udc4e).\nWe rarely get time-outs when computing action-values\nvia Stockfish, in which case we cannot determine the\nbest action for a board state and drop the correspond-\ning record from the behavioral cloning training set\n(see Table A1). Since we train on individual boards\nand not whole games we randomly shuffle the dataset\nafter annotation.\nFor our largest training dataset, based on 10M\ngames, this results in 15.32B action-value estimates\n(or \u2248 530M state-value estimates and best oracle ac-\ntions) to train on. To create test datasets we follow the\nsame annotation procedure, but on 1k games down-\nloaded from a different month (March 2023, \u2248 1.8M\naction-value estimates, \u2248 60k state-value estimates\nand best oracle actions). Since there is only a small\nnumber of early-game board-states and players of-\nten play popular openings, this i.i.d. test set contains\n14.7% of boards that are also in the training set. We\ndo not remove them, as doing so would introduce dis-\ntributional shift and skew test-set metrics. Finally, we\nalso create a puzzle test set, following the procedure\nin Carlini (2023), consisting of 10k challenging board\nstates that come with a correct sequence of moves to\nsolve the puzzle, which we compare against in our\npuzzle set accuracy evaluation. Only 1.33% of the\npuzzle set boards appear in the training set (i.e., the\ninitial board states, not complete solution sequences).\nSince evaluation of puzzle solutions is slow, we use a\nsubset of 1k puzzles in some of our evaluations (1.4%\noverlap with training set).\nValue binning\nThe predictors we train are discrete\ndiscriminators (classifiers), therefore we convert win-\npercentages (the ground-truth state- or action-values)\ninto discrete \u201cclasses\u201d via binning: we divide the in-\nterval between 0% and 100% uniformly into \ud835\udc3e bins\n(non-overlapping sub-intervals) and assign a one-hot\ncode to each bin \ud835\udc67\ud835\udc56 \u2208 {\ud835\udc670, . . . , \ud835\udc67\ud835\udc3e}. If not mentioned\notherwise, \ud835\udc3e = 128. For our behavioral cloning ex-\nperiments we train to predict oracle actions directly\nwhich are already discrete. We perform ablations for\nthe number of bins in Section 3.4.\n2.2. Model\nFor all our predictors we use a modern decoder-\nonly transformer backbone (Touvron et al., 2023a,b;\nVaswani et al., 2017) to parameterize a discrete prob-\nability distribution by normalizing the transformer\u2019s\noutputs with a log-softmax layer. The model thus\noutputs log probabilities. The context-size is 79 for\naction-value prediction, and 78 for state-value predic-\ntion and behavioral cloning (see \u2018Tokenization\u2019 below).\nThe output size is \ud835\udc3e (the number of bins) for action-\nand state-value prediction and 1968 (the number of all\npossible legal actions) for behavioral cloning. We use\nlearned positional encodings (Gehring et al., 2017)\nas the length of the input sequences is constant. Our\nlargest model has roughly 270 million parameters.\nWe provide all details for the model-size ablations in\nSection 3.3.\nTokenization\nBoard states \ud835\udc60 are encoded as FEN\nstrings which we convert to fixed-length strings of\n77 characters where the ASCII-code of each character\nis one token. A FEN string is a description of all pieces\non the board, whose turn it is, the castling availability\nfor both players, a potential en passant target, a half-\nmove clock and a full-move counter. We essentially\ntake any variable-length field in the FEN string, and\nconvert it into a fixed-length sub-string by padding\nwith \u2018.\u2019 if needed. We never flip the board; the FEN\nstring always starts at rank 1, even when it is the\nblack\u2019s turn. We store the actions in UCI notation (e.g.,\n\u2018e2e4\u2019 for the well-known white opening move). To\ntokenize them we determine all possible legal actions\nacross games, which is 1968, sort them alphanumer-\nically (case-sensitive), and take the action\u2019s index as\nthe token, meaning actions are always described by a\nsingle token (all details in Appendix A.1).\nTraining protocol\nPredictors are trained by min-\nimizing cross-entropy loss (i.e., log-loss) via mini-\nbatch\nbased\nstochastic\ngradient\ndescent\nusing\nAdam (Kingma and Ba, 2015). We train for 10 million\nsteps, which corresponds to 2.67 epochs for a batch\nsize of 4096 with 15.32B data points (cf. Table A1).\nThe target labels are either bin-indices in the case\nof state- or action-value prediction (see Section 2.1)\nor action indices for behavioral cloning; using a one-\nhot encoding in all cases (details in Appendices A.2\nand A.3).\n2.3. Predictors and Policies\nOur predictors are discrete distributions parameter-\nized by neural networks \ud835\udc43\ud835\udf03(\ud835\udc67|\ud835\udc65) that take a tokenized\ninput \ud835\udc65 and output a predictive distribution over dis-\ncrete labels {\ud835\udc670, . . . , \ud835\udc67\ud835\udc3e}. Depending on the prediction-\ntarget we distinguish between three tasks (see Figure 1\nfor an overview).\n(AV) Action-value prediction\nThe target label is\nthe bin \ud835\udc67\ud835\udc56 into which the ground-truth action-value\nestimate \ud835\udc44SF(\ud835\udc60, \ud835\udc4e) falls. The input to the predictor is\n3\nGrandmaster-Level Chess Without Search\nthe concatenation of tokenized state and action. The\nloss for a single data point is:\n\u2212 log \ud835\udc43AV\n\ud835\udf03 (\ud835\udc67\ud835\udc56|\ud835\udc60, \ud835\udc4e)\nwith \ud835\udc67\ud835\udc56 := bin\ud835\udc3e(\ud835\udc44SF(\ud835\udc60, \ud835\udc4e)),\n(1)\nwhere \ud835\udc3e is the number of bins and bin\ud835\udc3e(\ud835\udc65) is a function\nthat computes the (one-hot) bin-index of value \ud835\udc65. To\nuse the predictor in a policy, we evaluate the predictor\nfor all legal actions in the current state and pick the\naction with maximal expected action-value:\n\u02c6\ud835\udc4eAV(\ud835\udc60) = arg max\n\ud835\udc4e\u2208Alegal\n\ud835\udd3c\ud835\udc43AV\n\ud835\udf03 (\ud835\udc67|\ud835\udc60,\ud835\udc4e) [\ud835\udc67]\n|          {z          }\n\u02c6\ud835\udc44\ud835\udf03(\ud835\udc60,\ud835\udc4e)\n.\n(SV) State-value prediction\nThe target label is the\nbin \ud835\udc67\ud835\udc56 that the ground-truth state-value \ud835\udc49SF(\ud835\udc60) falls\ninto. The input to the predictor is the tokenized state.\nThe loss for a single data point is:\n\u2212 log \ud835\udc43SV\n\ud835\udf03 (\ud835\udc67\ud835\udc56|\ud835\udc60) with \ud835\udc67\ud835\udc56 := bin\ud835\udc3e(\ud835\udc49SF(\ud835\udc60)).\n(2)\nTo use the state-value predictor as a policy, we evaluate\nthe predictor for all states \ud835\udc60\u2032 = \ud835\udc47(\ud835\udc60, \ud835\udc4e) that are reach-\nable via legal actions from the current state (where\n\ud835\udc47(\ud835\udc60, \ud835\udc4e) is the deterministic transition of taking action \ud835\udc4e\nin state \ud835\udc60). Since \ud835\udc60\u2032 implies that it is now the oppo-\nnent\u2019s turn, the policy picks the action that leads to the\nstate with the worst expected value for the opponent:\n\u02c6\ud835\udc4eSV(\ud835\udc60) = arg min\n\ud835\udc4e\u2208Alegal\n\ud835\udd3c\ud835\udc43SV\n\ud835\udf03 (\ud835\udc67|\ud835\udc60\u2032) [\ud835\udc67]\n|        {z        }\n\u02c6\ud835\udc49\ud835\udf03(\ud835\udc60\u2032)\n.\n(BC) Behavioral cloning\nThe target label is the (one-\nhot) action-index of the ground-truth action \ud835\udc4eSF(\ud835\udc60)\nwithin the set of all possible actions (see \u2018Tokeniza-\ntion\u2019 in Section 2.2). The input to the predictor is the\ntokenized state, which leads to the loss for a single\ndata point:\n\u2212 log \ud835\udc43BC\n\ud835\udf03 (\ud835\udc4eSF(\ud835\udc60)|\ud835\udc60).\n(3)\nThis straightforwardly gives a policy that picks the\nhighest-probability action:\n\u02c6\ud835\udc4eBC(\ud835\udc60) = arg max\n\ud835\udc4e\u2208Alegal\n\ud835\udc43BC\n\ud835\udf03 (\ud835\udc4e|\ud835\udc60).\n2.4. Evaluation\nWe use the following evaluation metrics to compare\nour models against each other and/or measure train-\ning progress. The first two metrics evaluate the pre-\ndictors only; the second two evaluate the policies con-\nstructed from our predictors.\nAction-accuracy\nThe test set percentage where the\npredictor policy picks the ground-truth best action:\n\u02c6\ud835\udc4e(\ud835\udc60) = \ud835\udc4eSF(\ud835\udc60).\nAction-ranking (Kendall\u2019s \ud835\udf0f)\nThe average Kendall\nrank correlation (a standard statistical test) across the\ntest set, quantifying the correlation of the predicted\nactions with the ground-truth ranking by Stockfish\nin each state, ranging from -1 (exact inverse order)\nto 1 (exact same order) and 0 being no correlation.\nThe predictor ranking is given by \u02c6\ud835\udc44\ud835\udf03(\ud835\udc60, \ud835\udc4e), \u2212\u02c6\ud835\udc49\ud835\udf03(\ud835\udc47(\ud835\udc60, \ud835\udc4e)),\nand \ud835\udc43BC\n\ud835\udf03 (\ud835\udc4e|\ud835\udc60), respectively, for all legal actions. The\nground-truth ranking is given by Stockfish\u2019s action-\nvalues \ud835\udc44SF(\ud835\udc60, \ud835\udc4e) for all legal actions.\nPuzzle-accuracy\nWe evaluate our policies on their\ncapability of solving puzzles from a collection of\nLichess puzzles that are rated by Elo difficulty from\n399 to 2867, calculated by Lichess based on how of-\nten each puzzle has been solved correctly. We use\npuzzle-accuracy as the percentage of puzzles where the\npolicy\u2019s action-sequence exactly matches the known\nsolution action-sequence. For our main puzzle result\nin Section 3.2 we use 10k puzzles to report puzzle-\naccuracy, otherwise we use the first 1k puzzles to\nspeed up evaluation.\nGame playing strength (Elo)\nWe evaluate the play-\ning strength (measured as an Elo rating) of the pre-\ndictor policies in two different ways: (i) we play Blitz\ngames on Lichess against either only humans or only\nbots, and (ii) we run an internal tournament between\nall the agents from Table 1 except for GPT-3.5-turbo-\ninstruct. We play 400 games per pair of agent, yielding\n8400 games in total, and compute Elo ratings with\nBayesElo (Coulom, 2008), with the default confidence\nparameter of 0.5. We anchor the relative BayesElo val-\nues to the Lichess ELO vs. bots of our 270M model.\nFor the game-playing strength evaluations only (i.e.,\nnot for determining the puzzle accuracy) we use a\nsoftmax policy for the first 5 full-moves, instead of the\narg max policy described earlier, with a low tempera-\nture of 0.005 for the value or action-value functions,\n0.05 for the action functions (like the policy network\nof AlphaZero), and 0.5 for the visit counts used in\nthe full version of AlphaZero. This renders the poli-\ncies stochastic to both create variety in games, and\nprevents simple exploits via repeated play.\n2.5. Baselines\nWe compare the performance of our models against\nStockfish 16 (with a time limit of 0.05s per legal move,\ni.e., the oracle used to generate our dataset), three\n4\nGrandmaster-Level Chess Without Search\nLichess Elo\nAccuracy (%)\nAgent\nSearch\nInput\nTournament Elo\nvs. Bots\nvs. Humans\nPuzzles\nActions\nKendall\u2019s \ud835\udf0f\n9M Transformer (ours)\nFEN\n2007 (\u00b115)\n2054\n-\n85.5\n64.2\n0.269\n136M Transformer (ours)\nFEN\n2224 (\u00b114)\n2156\n-\n92.1\n68.5\n0.295\n270M Transformer (ours)\nFEN\n2299 (\u00b114)\n2299\n2895\n93.5\n69.4\n0.300\nGPT-3.5-turbo-instruct\nPGN\n-\n1755\n-\n66.5\n-\n-\nAlphaZero (policy net only)\nPGN\n1620 (\u00b122)\n-\n-\n61.0\n-\n-\nAlphaZero (value net only)\nPGN\n1853 (\u00b116)\n-\n-\n82.1\n-\n-\nAlphaZero (400 MCTS simulations)\n\u2713\nPGN\n2502 (\u00b115)\n-\n-\n95.8\n-\n-\nStockfish 16 (0.05s) [oracle]\n\u2713\nFEN\n2706 (\u00b120)\n2713\n-\n99.1\n100.0\n1.000\nTable 1 | Prediction and playing strength comparison for our models (three different sizes) against Stockfish 16,\nvariants of AlphaZero (with and without Monte-Carlo tree search), and GPT-3.5-turbo-instruct. Tournament\nElo ratings are determined by having the models play against each other and cannot be directly compared to\nthe Lichess Elo. Lichess (blitz) Elo ratings result from playing against human opponents or bots on Lichess.\nStockfish 16 (time limit of 50ms per move) is our data-generating oracle, thus obtaining a Kendall\u2019s \ud835\udf0f of 1 and\n100% action accuracy. Models operating on the PGN observe the full move history, whereas FENs only contain\nvery limited historical information. Best results without search in bold.\nvariants of AlphaZero (Silver et al., 2017): (i) the\noriginal with 400 MCTS simulations, (ii) only the pol-\nicy network, and (iii) only value network (where (ii)\nand (iii) perform no additional search), and the GPT-\n3.5-turbo-instruct from Carlini (2023). AlphaZero\u2019s\nnetworks have 27.6M parameters and are trained on\n44M games (details in Schrittwieser et al. (2020)).\nNote that these baselines have access to the whole\ngame history (via the PGN), in contrast to our models\nthat only observe the current game state (which con-\ntains very limited historical information via the FEN).\nThis helps the baseline policies for instance to easily\ndeal with threefold repetition (games are drawn if\nthe same board state is appears three times through-\nout the game), which requires a workaround for us\n(described in Section 5). Moreover, GPT-3.5-turbo-\ninstruct also requires whole games encoded via PGN\nto reduce hallucinations according to Carlini (2023),\nwho also finds that GPT-4 struggles to play full games\nwithout making illegal moves, so we do not compare\nagainst GPT-4.\n3. Results\nHere we present our comprehensive experimental eval-\nuation. For all parameters not explicitly mentioned\nwe use the same setting across our two main exper-\niments (Section 3.1, Section 3.2); for investigating\nscaling behavior and all ablations in Section 3.3 and\nSection 3.4 we use a different set of default settings\n(geared towards getting representative results with\nbetter computational efficiency). We provide all de-\ntails in Appendix A.2 and Appendix A.3, respectively.\n3.1. Main Result\nIn Table 1 we show the playing strength (internal tour-\nnament Elo, external Lichess Elo, and puzzle solving\ncompetence) and predictor metrics of our large-scale\ntransformer models when trained on the full (10M\ngames) training set. Our main evaluation compares\nthree transformer models with 9M, 136M, and 270M\nparameters after training (none of them overfit the\ntraining set as shown in Appendix B.1). The results\nshow that all three models exhibit non-trivial gener-\nalization to novel boards and can successfully solve\na large fraction of puzzles. Across all metrics, having\nlarger models consistently improves scores, confirm-\ning that model scale matters for strong chess perfor-\nmance. Our largest model achieves a blitz Elo of 2895\nagainst human players, which places it into grandmas-\nter territory. However, the Elo drops when playing\nagainst bots on Lichess, which may be a result of hav-\ning a significantly different player-pool, some minor\ntechnical issues, and perhaps a qualitative difference\nin how bots exploit weaknesses compared to humans\n(see Section 5 for a detailed discussion of these issues).\n3.2. Puzzles\nIn Figure 2 we compare the puzzle performance of\nour 270M parameter model against Stockfish 16 (time\nlimit of 50ms per move), GPT-3.5-turbo-instruct, and\nAlphaZero\u2019s value network. We use our large puzzle\nset of 10k puzzles, grouped by their assigned Elo dif-\nficulty from Lichess. Stockfish 16 performs the best\nacross all difficulty categories, followed by our 270M\nmodel. AlphaZero\u2019s value network (trained on 44M\ngames) and GPT-3.5-turbo-instruct achieve non-trivial\npuzzle performance, but significantly lag behind our\n5\nGrandmaster-Level Chess Without Search\n200-400\n400-600\n600-800\n800-1000\n1000-1200\n1200-1400\n1400-1600\n1600-1800\n1800-2000\n2000-2200\n2200-2400\n2400-2600\n2600-2800\n2800-3000\nPuzzle Rating (Elo)\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nStockfish 16 (0.05s) [oracle]\n270M T\nransformer (ours)\nAlphaZero (value net only)\nGPT\n-3.5-turbo -instruct\nFigure 2 | Puzzle solving competency comparison between our 270M transformer, Stockfish 16 (time limit of\n50ms per move), AlphaZero\u2019s value network, and GPT-3.5-turbo-instruct on 10 000 Lichess puzzles (curated\nfollowing Carlini (2023)).\nmodel. We emphasize that solving the puzzles requires\na correct move sequence, and since our policy cannot\nexplicitly plan ahead, solving the puzzle sequences\nrelies entirely on having good value estimates that can\nbe used greedily.\n3.3. Scaling \u201cLaws\u201d\nFigure 3 shows our scaling analysis over the dataset\nand model size. We visualize the puzzle accuracy\n(training and test loss in Figure A4), which correlates\nwell with the other metrics and the overall playing\nstrength. For small training set size (10k games, left\npanel) larger architectures (\u2265 7M) start to overfit\nas training progresses. This effect disappears as the\ndataset size is increased to 100k (middle panel) and\n1M games (right panel). The results also show that the\nfinal accuracy of a model increases as the dataset size is\nincreased (consistently across model sizes). Similarly,\nwe observe the general trend of increased architecture\nsize leading to increased overall performance regard-\nless of dataset size (as also shown in our main result\nin Section 3.1).\n3.4. Variants and Ablations\nWe test a series of experimental variants and perform\nextensive ablations using the 9M parameter model.\nThe results and conclusions drawn are used to inform\nand justify our design choices and determine default\nmodel-, data-, and training-configurations. Table 2\nsummarizes all results.\nPredictor-targets\nBy default we learn to predict\naction-values given a board state. Here we compare\nagainst using state-values or oracle actions (behavioral\ncloning) as the prediction targets. See Section 2.3 and\nFigure 1 for more details and how to construct poli-\ncies from each of the predictors. As the results in\nAccuracy (%)\nAblation\nParameter\nPuzzles\nActions\nKendall\u2019s \ud835\udf0f\nPredictor-target\nAV\n83.3\n63.0\n0.259\nSV\n77.5\n58.5\n0.215\nBC\n65.7\n56.7\n0.116\nNetwork depth\n2\n62.3\n54.4\n0.219\n4\n76.2\n59.9\n0.242\n8\n81.3\n62.3\n0.254\n16\n80.4\n62.3\n0.255\nData sampler\nUniform\n83.3\n63.0\n0.259\nWeighted\n49.9\n48.2\n0.192\nValue bins\n16\n83.0\n61.4\n0.248\n32\n83.0\n63.2\n0.261\n64\n84.4\n63.1\n0.259\n128\n83.8\n63.4\n0.262\n256\n83.7\n63.0\n0.260\nLoss function\nlog (class.)\n81.3\n62.3\n0.254\nL2 (regr.)\n82.6\n58.9\n0.235\nStockfish Limit [s]\n0.05\n84.0\n62.2\n0.256\n0.1\n85.4\n62.5\n0.254\n0.2\n84.3\n62.6\n0.259\n0.5\n83.3\n63.0\n0.259\nTable 2 | Ablations for different parameters (see Sec-\ntion 3.4).\nTable 2 show, the action-value predictor is superior in\nterms of action-ranking (Kendall\u2019s \ud835\udf0f), action accuracy,\nand puzzle accuracy. The same trend is also shown\nin Figure A5 (in Appendix B.2, which tracks puzzle\naccuracy over training iterations for the different pre-\ndictors. This superior performance of action-value\nprediction might stem primarily from the significantly\nlarger action-value dataset (15.3B state-action pairs vs.\n\u2248 530M states for our largest training set constructed\nfrom 10M games). We thus run an additional abla-\ntion where we train all three predictors on exactly the\nsame amount of data\u2014results shown in Appendix B.2\nlargely confirm this hypothesis. Please see our more\ndetailed discussion of the different predictor targets\nas we discuss these results in Appendix B.2, where\n6\nGrandmaster-Level Chess Without Search\n0\n1\n2\n3\n4\n5\nStep\n1e6\n0\n20\n40\n60\n80\nPuzzle Accuracy (%)\n10'000 Games\n400K\n1M\n2M\n7M\n9M\n34M\n0\n1\n2\n3\n4\n5\nStep\n1e6\n100'000 Games\n0\n1\n2\n3\n4\n5\nStep\n1e6\n1'000'000 Games\nFigure 3 | Puzzle accuracy for different training set sizes (stated above panels) and model sizes (color-coded),\nevaluated with our small puzzle set of 1k puzzles. Generally, larger models trained on larger datasets lead to\nhigher performance (which strongly correlates with test set performance and general chess playing strength),\nhighlighting the importance of scale for strong chess play. This effect cannot be explained by memorization\nsince < 1.41% of the initial puzzle board states appear in our training set. If the model size is too large in\nrelation to the training set size learning is impeded and overfitting occurs.\nwe also discuss performance discrepancy between be-\nhavioral cloning and the state-value predictor based\npolicy may be largely explained by the fact that we\ntrain on expert\u2019s actions only instead of the full action\ndistribution of the expert.\nNetwork depth\nWe show the influence of increasing\nthe transformer\u2019s depth while keeping the number of\nparameters constant in Table 2. Since transformers\nmay learn to roll out iterative computation (which\narises in search) across layers, deeper networks may\nhold the potential for deeper unrolls. We compensate\nfor having fewer layers by varying the embedding di-\nmension and widening factor such that all models have\nthe same number of parameters. The performance of\nour models increases with their depth but seems to\nsaturate at around 8 layers, indicating that depth is\nimportant, but not beyond a certain point.\nData sampler\nWe remove duplicate board states dur-\ning the generation of the training and test sets. This\nincreases data diversity but introduces distributional\nshift compared to the \u201cnatural\u201d game distribution of\nboards where early board states and popular open-\nings occur more frequently. To quantify the effect\nof this shift we use an alternative \u201cweighted\u201d data\nsampler that draws boards from our filtered training\nset according to the distribution that would occur if\nwe had not removed duplicates. Results in Table 2\nreveal that training on the natural distribution (via\nthe weighted sampler) leads to significantly worse re-\nsults compared to sampling uniformly randomly from\nthe filtered training set (both trained models are eval-\nuated on a filtered test set with uniform sampling,\nand the puzzle test set). We hypothesize that the\nincreased performance is due to the increased data\ndiversity seen under uniform sampling. As we train for\nvery few epochs, the starting position and common\nopening positions are only seen a handful of times\nduring training under uniform sampling, making it\nunlikely that strong early-game play of our models\ncan be attributed to memorization.\nValue binning\nTable 2 shows the impact of varying\nthe number of bins used for state- and action-value\ndiscretization (from 16 to 256), demonstrating that\nmore bins lead to improved performance. To strike\na balance between performance and computational\nefficiency, we use \ud835\udc3e = 32 bins for our ablations and\n\ud835\udc3e = 128 for the main experiments.\nLoss function\nWe treat learning Stockfish action-\nvalues as a classification problem and thus train by\nminimizing cross-entropy loss (log-loss). This is as\nclose as possible to the (tried and tested) standard\nLLM setup. An alternative is to treat the problem as a\nscalar regression problem. If we parameterize a fixed-\nvariance Gaussian likelihood model with a transformer\nand perform maximum (log) likelihood estimation,\nthis is equivalent to minimizing mean-squared error\n(L2 loss). To that end, we modify the architecture to\noutput a scalar (without a final log-layer or similar).\nThe log-loss outperforms the L2 loss on two out of the\nthree metrics (Table 2).\nStockfish time limit\nWe create training sets from\n1 million games annotated by Stockfish with varying\ntime limits to manipulate the playing strength of our\noracle. We report scores on the puzzle set (same for\n7\nGrandmaster-Level Chess Without Search\nall models) and a test set created using the same time\nlimit as the training set (different for all models). Ta-\nble 2 shows that a basic time-limit of 0.05 seconds\ngives only marginally worse puzzle performance. As\na compromise between computational effort and fi-\nnal model performance we thus choose this as our\ndefault value (for our 10M games dataset we need\nabout 15B action-evaluation calls with Stockfish, i.e.,\nroughly 8680 days of unparallelized Stockfish evalua-\ntion time).\n4. Related Work\nEarly chess AI research made heavy use of design-\ning explicit search strategies coupled with heuristics,\nas evidenced by Turing\u2019s initial explorations (Burt,\n1955) and implementations like NeuroChess (Thrun,\n1994). This approach culminated in systems like Deep\nBlue (Campbell et al., 2002) and Stockfish (Romstad\net al., 2008), known for their advanced search algo-\nrithms. The development of AlphaZero (Silver et al.,\n2017) marked a paradigm shift, employing deep RL\nwith Monte Carlo Tree Search, thus learning its own\nheuristics (policy and value networks) instead of man-\nually designing them. Neural networks play a signifi-\ncant role in chess AI (Klein, 2022), including enhance-\nments to AlphaZero\u2019s self-play mechanisms (V. et al.,\n2018), the use of deep RL (Lai, 2015), and a general\ntrend of moving away from explicit search methods,\nby leveraging large-scale game datasets for training\n(David et al., 2016; Schrittwieser et al., 2020).\nThe rise of large language models has also led to\ninnovations in chess AI, cf. Kamlish\u2019s language-based\nmodels (Kamlish et al., 2019), the encoding of chess\ngames via natural language (DeLeo and Guven, 2022;\nToshniwal et al., 2022), and the evaluation LLMs\nability to play chess (Carlini, 2023; Gramaje, 2023).\nCzech et al. (2023) show that strategic input repre-\nsentations and value loss enhancements significantly\nboost chess performance of vision transformers, and\nAlrdahi and Batista-Navarro (2023); Feng et al. (2023)\nshow that adding chess specific data sources (e.g.,\nchess textbooks) to language model training can im-\nprove their chess performance. St\u00f6ckl (2021) explored\nscaling effects of transformers on chess performance,\nwhich resonates with our emphasis on the importance\nof model and dataset scale.\n5. Discussion\nIn order to use our state-based policies to play against\nhumans and bots, two minor technical issues appear\nthat can only be solved by having (some) access to\ngame history.\nWe briefly discuss both issues and\n(a) Possible Move (Mate-in-3) (b) Actual Move (Mate-in-5)\nFigure 4 | Two options to win the game in 3 or 5\nmoves, respectively (more options exist). Since they\nboth map into the highest-value bin our bot ignores\nNh6+, the fastest way to win (in 3), and instead plays\nNd6+ (mate-in-5). Unfortunately, a state-based pre-\ndictor without explicit search cannot guarantee that\nit will continue playing the Nd6+ strategy and thus\nmight randomly alternate between different strategies.\nOverall this increases the risk of drawing the game or\nlosing due to a subsequent (low-probability) mistake,\nsuch as a bad softmax sample. Board from a game\nbetween our 9M Transformer (white) and a human\n(blitz Elo of 2145).\npresent our workarounds.\nBlindness to threefold repetition\nBy construction,\nour state-based predictor cannot detect the risk of\nthreefold repetition (drawing because the same board\noccurs three times), since it has no access to the game\nhistory (FENs contain minimal historical info, suffi-\ncient for the Fifty Move rule). To reduce draws from\nthreefold repetitions, we check if the bot\u2019s next move\nwould trigger the rule and set the corresponding ac-\ntion\u2019s win percentage to 50% before computing the\nsoftmax. However, our bots still cannot plan ahead\nto minimize the risk of being forced into threefold\nrepetition.\nIndecisiveness in the face of overwhelming victory\nIf Stockfish detects a mate-in-\ud835\udc58 (e.g., 3 or 5) it outputs\n\ud835\udc58 and not a centipawn score. We map all such outputs\nto the maximal value bin (i.e., a win percentage of\n100%). Similarly, in a very strong position, several\nactions may end up in the maximum value bin. Thus,\nacross time-steps this can lead to our agent playing\nsomewhat randomly, rather than committing to one\nplan that finishes the game quickly (the agent has\nno knowledge of its past moves). This creates the\nparadoxical situation that our bot, despite being in\na position of overwhelming win percentage, fails to\n8\nGrandmaster-Level Chess Without Search\ntake the (virtually) guaranteed win and might draw\nor even end up losing since small chances of a mis-\ntake accumulate with longer games (see Figure 4). To\nprevent some of these situations, we check whether\nthe predicted scores for all top five moves lie above a\nwin percentage of 99% and double-check this condi-\ntion with Stockfish, and if so, use Stockfish\u2019s top move\n(out of these) to have consistency in strategy across\ntime-steps.\nElo: Humans vs. bots\nTable 1 shows a difference in\nLichess Elo when playing against humans compared to\nbots. While the precise reasons are not entirely clear,\nwe have three plausible hypotheses: (i) humans tend\nto resign when our bot has overwhelming win per-\ncentage but many bots do not (meaning that the pre-\nviously described problem gets amplified when play-\ning against bots); (ii) humans on Lichess rarely play\nagainst bots, meaning that the two player pools (hu-\nmans and bots) are hard to compare and Elo ratings\nbetween pools may be miscalibrated (Justaz, 2023);\nand (iii) based on preliminary (but thorough) anec-\ndotal analysis by a chess NM, our models make the\noccasional tactical mistake which may be penalized\nqualitatively differently (and more severely) by other\nbots compared to humans (see some of this analysis\nin Appendices B.4 and B.5). While investigating this\nElo discrepancy further is interesting, it is not central\nto our paper and does not impact our main claims.\n5.1. Limitations\nWhile our largest model achieves very good perfor-\nmance, it does not completely close the gap to Stock-\nfish 16. All our scaling experiments point towards\nclosing this gap eventually with a large enough model\ntrained on enough data. However, the current results\ndo not allow us to claim that the gap can certainly be\nclosed. Another limitation, as discussed earlier, is that\nour predictors see the current state but not the com-\nplete game history. This leads to some fundamental\ntechnical limitations that cannot be overcome without\nsmall domain-specific heuristics or augmenting the\ntraining data and observable info. Finally, when using\na state-value predictor to construct a policy, we con-\nsider all possible subsequent states that are reachable\nvia legal actions. This requires having a transition\nmodel \ud835\udc47(\ud835\udc60, \ud835\udc4e), and may be considered a version of\n1-step search. While the main point is that our predic-\ntors do not explicitly search over action sequences, we\nlimit the claim of \u2018without search\u2019 to our action-value\npolicy and behavioral cloning policy.\nNote that the primary goal of this project was to in-\nvestigate whether a complex, search-based algorithm,\nsuch as Stockfish 16, can be well approximated with a\nfeedforward neural network. In the course of this, we\nhave made a serious attempt to produce a strong chess\npolicy and estimate its playing strength, but we have\nnot exhausted every conceivable option to maximize\nplaying strength\u2014it may well be that further tweaks\nof our approach could lead to even stronger policies.\nSimilarly, we have made a serious attempt at calibrat-\ning our policy\u2019s playing strength via Lichess, where\nthe claim of \u201cgrandmaster-level\u201d play currently holds\nagainst human opponents, but we have not calibrated\nour policy under official tournament conditions. We\nalso cannot rule out that opponents, through extensive\nrepeated play, may be able to find and exploit weak-\nnesses reliably due to the fairly deterministic nature\nof our policy.\n6. Conclusion\nOur paper shows that is is possible to distill an approxi-\nmation of Stockfish 16 into a feed-forward transformer\nvia standard supervised training. The resulting predic-\ntor generalizes well to unseen board states, and, when\nused in a policy, leads to strong chess play (Lichess\nElo of 2895 against humans). We demonstrate that\nstrong chess capabilities from supervised learning only\nemerge at sufficient dataset and model scale. Our work\nthus adds to a rapidly growing body of literature show-\ning that complex and sophisticated algorithms can be\ndistilled into feed-forward transformers, implying a\nparadigm-shift away from viewing large transformers\nas \u201cmere\u201d statistical pattern recognizers to viewing\nthem as a powerful technique for general algorithm\napproximation.\nImpact Statement\nWhile the results of training transformer-based archi-\ntectures at scale in a (self) supervised way will have\nsignificant societal consequences in the near future,\nthese concerns do not apply to a closed domain like\nchess that has limited real-world impact and has been\na domain of machine superiority for decades. An-\nother advantage of supervised training on a single\ntask over other forms of training (particularly self-\nplay or reinforcement learning, and meta-learning)\nis that the method requires a strong oracle solution\nto begin with (for data annotation) and is unlikely\nto significantly outperform the oracle\u2014so the poten-\ntial for the method to rapidly introduce substantial\nunknown capabilities (with wide societal impacts) is\nvery limited.\n9\nGrandmaster-Level Chess Without Search\nAcknowledgments\nWe thank Aur\u00e9lien Pomini, Avraham Ruderman, Eric\nMalmi, Charlie Beattie, Chris Colen, Chris Wolff, David\nBudden, Dashiell Shaw, Guillaume Desjardins, Ham-\ndanil Rasyid, Himanshu Raj, Joel Veness, John Schultz,\nJulian Schrittwieser, Laurent Orseau, Lisa Schut, Marc\nLanctot, Marcus Hutter, Matthew Aitchison, Nando de\nFreitas, Nenad Tomasev, Nicholas Carlini, Nick Birnie,\nNikolas De Giorgis, Ritvars Reimanis, Satinder Baveja,\nThomas Fulmer, Tor Lattimore, Vincent Tjeng, Vivek\nVeeriah, and Zhengdong Wang for insightful discus-\nsions and their helpful feedback.\nReferences\nH. Alrdahi and R. Batista-Navarro. Learning to play\nchess from textbooks (LEAP): a corpus for eval-\nuating chess moves based on sentiment analysis.\narXiv:2310.20260, 2023.\nR. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Sori-\ncut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Milli-\ncan, D. Silver, S. Petrov, M. Johnson, I. Antonoglou,\nJ. Schrittwieser, A. Glaese, J. Chen, E. Pitler, T. P. Lill-\nicrap, A. Lazaridou, O. Firat, J. Molloy, M. Isard, P. R.\nBarham, T. Hennigan, B. Lee, F. Viola, M. Reynolds,\nY. Xu, R. Doherty, E. Collins, C. Meyer, E. Ruther-\nford, E. Moreira, K. Ayoub, M. Goel, G. Tucker, E. Pi-\nqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka,\nB. Roelofs, A. White, A. Andreassen, T. von Glehn,\nL. Yagati, M. Kazemi, L. Gonzalez, M. Khalman,\nJ. Sygnowski, and et al. Gemini: A family of highly\ncapable multimodal models.\narXiv:2312.11805,\n2023.\nJ. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson,\nC. Leary, D. Maclaurin, G. Necula, A. Paszke, J. Van-\nderPlas, S. Wanderman-Milne, and Q. Zhang. JAX:\ncomposable transformations of Python+NumPy\nprograms, 2018.\nURL http://github.com/\ngoogle/jax.\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Ka-\nplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss,\nG. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen,\nE. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei.\nLanguage models are few-shot\nlearners. In NeurIPS, 2020.\nC. Burt. Faster than thought: A symposium on digital\ncomputing machines. edited by b. v. bowden. British\nJournal of Statistical Psychology, 1955.\nM. Campbell, A. J. H. Jr., and F. Hsu. Deep blue. Artif.\nIntell., 2002.\nN. Carlini.\nPlaying chess with large language\nmodels.\nhttps://nicholas.carlini.com/\nwriting/2023/chess-llm.html, 2023.\nR. Coulom. Whole-history rating: A bayesian rating\nsystem for players of time-varying strength. In Com-\nputers and Games, 2008.\nJ. Czech, J. Bl\u00fcml, and K. Kersting. Representation\nmatters: The game of chess poses a challenge to\nvision transformers. arXiv:2304.14918, 2023.\n10\nGrandmaster-Level Chess Without Search\nO. E. David, N. S. Netanyahu, and L. Wolf. Deepchess:\nEnd-to-end deep neural network for automatic\nlearning in chess. In ICANN (2), 2016.\nDeepMind, I. Babuschkin, K. Baumli, A. Bell, S. Bhu-\npatiraju, J. Bruce, P. Buchlovsky, D. Budden, T. Cai,\nA. Clark, I. Danihelka, A. Dedieu, C. Fantacci,\nJ. Godwin, C. Jones, R. Hemsley, T. Hennigan,\nM. Hessel, S. Hou, S. Kapturowski, T. Keck, I. Ke-\nmaev, M. King, M. Kunesch, L. Martens, H. Merzic,\nV. Mikulik, T. Norman, G. Papamakarios, J. Quan,\nR. Ring, F. Ruiz, A. Sanchez, L. Sartran, R. Schnei-\nder, E. Sezener, S. Spencer, S. Srinivasan, M. Stano-\njevi\u0107, W. Stokowiec, L. Wang, G. Zhou, and F. Vi-\nola. The DeepMind JAX Ecosystem, 2020. URL\nhttp://github.com/google-deepmind.\nM. DeLeo and E. Guven. Learning chess with language\nmodels and transformers. arXiv:2209.11902, 2022.\nX. Feng, Y. Luo, Z. Wang, H. Tang, M. Yang,\nK. Shao, D. Mguni, Y. Du, and J. Wang. Chessgpt:\nBridging policy learning and language modeling.\narXiv:2306.09200, 2023.\nJ. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N.\nDauphin. Convolutional sequence to sequence learn-\ning. In ICML, 2017.\nB. A. Gramaje. Exploring GPT\u2019s capabilities in chess-\npuzzles. Master\u2019s thesis, Universitat Polit\u00e8cnica de\nVal\u00e8ncia, 2023.\nG. Haworth and N. Hernandez. The 20th top chess en-\ngine championship, TCEC20. J. Int. Comput. Games\nAssoc., 2021.\nT. Hennigan, T. Cai, T. Norman, L. Martens, and\nI. Babuschkin. Haiku: Sonnet for JAX, 2020. URL\nhttp://github.com/deepmind/dm-haiku.\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya,\nT. Cai, E. Rutherford, D. de Las Casas, L. A. Hen-\ndricks, J. Welbl, A. Clark, T. Hennigan, E. Noland,\nK. Millican, G. van den Driessche, B. Damoc, A. Guy,\nS. Osindero, K. Simonyan, E. Elsen, J. W. Rae,\nO. Vinyals, and L. Sifre. Training compute-optimal\nlarge language models. arXiv:2203.15556, 2022.\nJustaz.\nExact ratings for everyone on lichess.\nhttps://lichess.org/@/justaz/blog/\nexact-ratings-for-everyone-on-lichess/\nklIoAEAU, 2023.\nI. Kamlish, I. B. Chocron, and N. McCarthy. Sentimate:\nLearning to play chess through natural language\nprocessing. arXiv:1907.08321, 2019.\nD. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. In ICLR (Poster), 2015.\nD.\nKlein.\nNeural\nnetworks\nfor\nchess.\narXiv:2209.01506, 2022.\nM. Lai. Giraffe: Using deep reinforcement learning to\nplay chess. arXiv:1509.01549, 2015.\nOpenAI. GPT-4 technical report. arXiv:2303.08774,\n2023.\nT. Romstad, M. Costalba, J. Kiiski, G. Linscott, Y. Nasu,\nM. Isozaki, H. Noda, and et al. Stockfish, 2008. URL\nhttps://stockfishchess.org.\nM. Sadler and N. Regan. Game Changer: AlphaZero\u2019s\nGroundbreaking Chess Strategies and the Promise of\nAI. New In Chess, 2019.\nJ. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan,\nL. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hass-\nabis, T. Graepel, T. P. Lillicrap, and D. Silver. Mas-\ntering atari, go, chess and shogi by planning with a\nlearned model. Nat., 2020.\nN. Shazeer.\nGLU variants improve transformer.\narXiv:2002.05202, 2020.\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou,\nM. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran,\nT. Graepel, T. P. Lillicrap, K. Simonyan, and D. Has-\nsabis.\nMastering chess and shogi by self-play\nwith a general reinforcement learning algorithm.\narXiv:1712.01815, 2017.\nA. St\u00f6ckl. Watching a language model learning chess.\nIn RANLP, 2021.\nS. Thrun. Learning to play the game of chess. In NIPS,\n1994.\nS. Toshniwal, S. Wiseman, K. Livescu, and K. Gimpel.\nChess as a testbed for language model state tracking.\nIn AAAI, 2022.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet,\nM. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Ham-\nbro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and\nG. Lample. Llama: Open and efficient foundation\nlanguage models. arXiv:2302.13971, 2023a.\nH. Touvron, L. Martin, K. Stone, et al.\nLlama\n2: Open foundation and fine-tuned chat models.\narXiv:2307.09288, 2023b.\nS. K. G. V., K. Goyette, A. Chamseddine, and B. Con-\nsidine.\nDeep pepper:\nExpert iteration based\nchess agent in the reinforcement learning setting.\narXiv:1806.00683, 2018.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.\nAttention is all you need. In NIPS, 2017.\n11\nGrandmaster-Level Chess Without Search\nA. Experimental Setup\nA.1. Tokenization\nThe first part of a FEN string encodes the position\nof pieces rank-wise (row-wise). The only change we\nmake is that we encode each empty square with a\n\u2018.\u2019, which always gives us 64 characters for a board.\nThe next character denotes the active player (\u2018w\u2019 or\n\u2018b\u2019). The next part of the FEN string denotes castling\navailability (up to four characters for King- and Queen-\nside for each color, or \u2018-\u2019 for no availability)\u2014we take\nthis string and if needed pad it with \u2018.\u2019 such that it\nalways has length 4. Next are two characters for the\nen passant target, which can be \u2018-\u2019 for no target; we\nuse the two characters literally or \u2018-.\u2019 for no target.\nFinally we have the halfmove clock (up to two digits)\nand the fullmove number (up to three digits); we take\nthe numbers as characters and pad them with \u2018.\u2019 to\nmake sure they are always tokenized into two and\nthree characters respectively.\nA.2. Main Setup\nWe use the same basic setup for all our main experi-\nments and only vary the model architecture.\nConcretely our base setup is as follows: We train for\n20 million steps with a batch size of 4096, meaning\nthat we train for 5.35 epochs. We use the Adam op-\ntimizer (Kingma and Ba, 2015) with a learning rate\nof 1e-4. We train on the dataset generated from 10\nmillion games (cf. Table A1) for the action value policy\nwith 128 return buckets and a stockfish time limit of\n0.05s. We use the unique sampler and Polyak aver-\naging for evaluation and evaluate on 1000 games (cf.\nTable A1) and 1000 puzzles from a different month\nthan that used for training.\nWe train a vanilla decoder-only transformer\nwithout causal masking (Vaswani et al., 2017),\nwith the improvements proposed in LLaMA (Tou-\nvron et al., 2023a,b), i.e., post-normalization and\nSwiGLU (Shazeer, 2020).\nWe use three different\nmodel configurations: (i) 8 heads, 8 layers, and an em-\nbedding dimension of 256, (ii) 8 heads, 8 layers, and\nan embedding dimension of 1024, and (iii) 8 heads,\n16 layers, and an embedding dimension of 1024.\nA.3. Ablation Setup\nWe use the same basic setup for all our ablation exper-\niments and only vary the ablation parameters.\nConcretely our base setup is as follows: We train\nfor 5 million steps with a batch size of 1024, meaning\nthat we train for 3.19 epochs. We use the Adam op-\ntimizer (Kingma and Ba, 2015) with a learning rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWin %\n0\n20000\n40000\n60000\n80000\n100000\nCount\nFigure A1 | The win percentages for our action-value\ndataset generated from 1000 games (cf. Table A1).\nWe use 50 buckets to generate the histogram. The\ndistribution is skewed towards 0 as we consider all\nlegal moves per board and most actions are not advan-\ntageous for the player.\nof 4e-4. We train on the dataset generated from 1\nmillion games (cf. Table A1) for the action value policy\nwith 32 return buckets and a stockfish time limit of\n0.5s. We use the unique sampler and train a vanilla\ndecoder-only transformer (Vaswani et al., 2017) with\npost-normalization, 8 heads, 8 layers, an embedding\ndimension of 256, and no causal masking. We use\nPolyak averaging for evaluation and evaluate on 1000\ngames (cf. Table A1) and 1000 puzzles from a different\nmonth than that used for training.\nA.4. Dataset Statistics\nWe visualize some dataset statistics in Figures A1\nand A2.\nA.5. Playing-strength evaluation on Lichess\nWe evaluate and calibrate the playing strength of\nour models by playing against humans and bots on\nLichess. Our standard evaluation allows for both play-\ning against bots and humans (see Table 1), but since\nhumans tend to rarely play against bots the Elo ratings\nin this case are dominated by playing against other\nbots (see our discussion of how this essentially creates\ntwo different, somewhat miscalibrated, player pools\nin Section 5). In our case the policies in the column\ndenoted with \u2018vs. Bots\u2019 in Table 1 have played against\nsome humans but the number of games against hu-\nmans is < 4.5% of total games played. To get better\ncalibration against humans we let our largest model\nplay exclusively against humans (by not accepting\ngames with other bots) which leads to a significantly\nhigher Elo ranking (see Table 1). Overall we have\n12\nGrandmaster-Level Chess Without Search\nState-Value\nBehavioral Cloning\nAction-Value\nSplit\nGames\nRecords\nBytes\nRecords\nBytes\nRecords\nBytes\nTrain\n104\n591 897\n43.7 MB\n589 130\n41.1 MB\n17 373 887\n1.4 GB\n105\n5 747 753\n422.0 MB\n5 720 672\n397.4 MB\n167 912 926\n13.5 GB\n106\n55 259 971\n4.0 GB\n54 991 050\n3.8 GB\n1 606 372 407\n129.0 GB\n107\n530 310 443\n38.6 GB\n527 633 465\n36.3 GB\n15 316 914 724\n1.2 TB\nTest\n103\n62 829\n4.6 MB\n62 561\n4.4 MB\n1 838 218\n148.3 MB\nTable A1 | Dataset sizes. For simplicity, we typically refer to the datasets by the number of games they were\ncreated from.\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nMove Index\n0\n50000\n100000\n150000\n200000\n250000\n300000\n350000\n400000\nCount\nFigure A2 | The moves (sorted by frequency) for our\naction-value dataset generated from 1000 games (cf.\nTable A1). We use 50 buckets to generate the his-\ntogram. There are 1968 possible moves and the five\nmost frequent ones are a2a3, g2g3, h2h3, a2a4, a7a6.\nplayed the following numbers of games for the dif-\nferent policies shown in Table 1: 9M (553 games),\n136M (169 games), 270M (228 games against bots,\n174 games against humans), Stockfish (30 games),\nGPT-3.5-turbo-instruct (181 games).\nA.6. Stockfish and AlphaZero Setup\nStockfish\nWe use Stockfish 16 (the version from De-\ncember 2023) throughout the paper. When we play,\nwe use the oracle we used for training, which is an\nunconventional way to play with this engine: We eval-\nuate each legal move in the position for 50ms, and\nreturn the best move based on these scores. This is\nnot entirely equivalent to a standard thinking time of\n50ms times the number of legal moves per position,\nas we force Stockfish to spend 50ms on moves that\ncould be uninteresting and unexplored. We chose to\nkeep this setup to have a comparison to the oracle we\ntrain on. Note that, when comparing the legal moves\nin a given position, we do not clear Stockfish\u2019s cache\nbetween the moves. Therefore, due to the way the\ncache works, this biases the accuracy of Stockfish\u2019s\nevaluation to be weaker for the first moves considered.\nFinally, due to the nature of our internal hardware\nsetup, we use two different kinds of chips to run Stock-\nfish: (i) to compute the Lichess Elo, we use a 6-core\nIntel(R) Xeon(R) W-2135 CPU @ 3.70GHz, and (ii) to\ncompute the tournament Elo, we use a single Tensor\nProcessing Unit (V3), as for all the other agents.\nAlphaZero\nWe use the AlphaZero version from 2020,\nwith a network trained at that time (Schrittwieser\net al., 2020). We use three different versions: (i)\npolicy network only, (ii) value network only and (iii)\nstandard version with search. For (i), we are using\nthe probability distribution over actions returned by\nthe policy network, and take the argmax as the best\naction. For (ii), we do a search limited to depth 1,\nwith 100 MCTS simulations, enough to cover all legal\nactions, and take the argmax over visit counts. For\n(iii), we use the standard search from the paper, with\n400 MCTS simulations and the exact same UCB scaling\nparameters. We also take the argmax over visit counts.\nNote that AlphaZero\u2019s policy and value network have\nbeen trained on 44M games, whereas we trained our\nlargest models on only 10M games.\nA.7. Computational Resources\nOur codebase is based on JAX (Bradbury et al., 2018)\nand the DeepMind JAX Ecosystem (DeepMind et al.,\n2020; Hennigan et al., 2020). We used 4 Tensor Pro-\ncessing Units (V5) per model for the ablation exper-\niments. We used 128 Tensor Processing Units (V5)\nper model to train our large (9M, 136M and 270M)\nmodels. We used a single Tensor Processing Unit (V3)\nper agent for our Elo tournament.\nB. Additional Results\nB.1. Loss Curves\nIn Figure A3 we show the train and test loss curves\n(and the evolution of the puzzle accuracy) for the large\nmodels from Section 3.1. We observe that none of the\n13\nGrandmaster-Level Chess Without Search\n0\n1\n2\n3\n4\n5\n6\nStep\n1e6\n3\n4\n5\n6\n7\nT\nrain Loss\n9M\n136M\n270M\n(a) Training Loss\n0\n1\n2\n3\n4\n5\n6\nStep\n1e6\n3\n4\n5\n6\n7\nVal. Loss\n9M\n136M\n270M\n(b) Test Loss\n0\n1\n2\n3\n4\n5\n6\nStep\n1e6\n0\n20\n40\n60\n80\n100\nPuzzle Accuracy (%)\n9M\n136M\n270M\n(c) Puzzle Accuracy\nFigure A3 | Train and test loss curves and puzzle accuracy over time for the models from Section 3.1. We\nobserve no overfitting, which justifies always using the fully trained model in our evaluations.\nRelative Tournament Elo\nPrediction Target\nSame # of Games in Dataset\nSame # of Data Points\nAction-Value\n+492 (\u00b131)\n+252 (\u00b122)\nState-Value\n+257 (\u00b123)\n+264 (\u00b122)\nBehavioral-Cloning\n0 (\u00b128)\n0 (\u00b124)\nTable A2 | Ranking the policies that arise from our\nthree different predictors by having them play against\neach other in a tournament and computing relative Elo\nrankings (200 games per pairing; i.e., 600 games per\ncolumn). When constructing the training data for all\nthree predictors based on the same number of games\n(middle column), the action-value dataset is much\nlarger than the state-value / behavioral cloning set,\nwhich leads to a stronger policy. When correcting for\nthis by forcing the same number of training data points\nfor all three (right column), the difference between\nstate- and action-value prediction disappears.\nmodels overfit and that larger models improve both\nthe training and the test loss.\nIn Figure A4 we visualize the train and test loss\ncurves for the scaling experiment from Section 3.3.\nIn line with the results shown in the main paper we\nobserve that on the smallest training set, models with\n\u2265 7M parameters start to overfit, but not for the larger\ntraining sets. Except for the overfitting cases we ob-\nserve that larger models improve both the training\nand test loss, regardless of training set size, and that\nlarger training set size improves the test loss when\nkeeping the model size constant.\nB.2. Predictor-Target Comparison\nIn Figure A5 we compare the puzzle accuracy for the\nthree different predictor targets (action-values, state-\nvalues, or best action) trained on 1 million games. As\ndiscussed in the main text, for a fixed number of games\nwe have very different dataset sizes for state-value pre-\ndiction (roughly 55 million states) and action-value\nprediction (roughly 1.6 billion states); see Table A1\nfor all dataset sizes. It seems plausible that learning\naction-values might pose a slightly harder learning\nproblem, leading to slightly slower initial learning,\nbut eventually this is compensated for by having much\nmore data to train on compared to state-value learn-\ning (see Figure A5, which shows this trend). Also note\nthat since we use the same time-budget per Stockfish\ncall, all action-values for one state use more Stockfish\ncomputation time in total (due to one call per action)\nwhen compared to state-values (one call per board).\nTo control for the effect of dataset size, we train all\nthree predictors (9M parameter model) on a fixed\nset of 40 million data points. Results are shown in\nFigure A6. As the results show, the state-value pol-\nicy in this case slightly outperforms the action-value\npolicy, except for action-ranking (Kendall\u2019s \ud835\udf0f), which\nmakes sense since the action-value predictor is im-\nplicitly trained to produce good action rankings. To\nsee how this translates into playing-strength, we pit\nall three policies (AV, SV, BC) against each other and\ndetermine their relative Elo rankings. Table A2 shows\nthat when not controlling for the number of training\ndata points, the action-value policy is strongest (in line\nwith the findings in Table 2 and Figure A5), but when\ncontrolling for the number of training data points the\naction-value and state-value policy perform almost\nidentical (in line with Figure A6).\nThroughout all these results we observe lower per-\nformance of the behavioral cloning policy, despite be-\ning trained on a comparable number of datapoints as\nthe state-value policy. The main hypothesis for this\nis that the amount of information in the behavioral\ncloning dataset is lower than the state value dataset,\nsince we throw away any information in the state- or\naction-values beyond the index of the oracle action.\nWe suspect that training on the full action distribution\n14\nGrandmaster-Level Chess Without Search\n0\n1\n2\n3\n4\n5\nStep\n1e6\n0\n1\n2\n3\n4\n5\n6\nT\nrain Loss\n10'000 Games\n0\n1\n2\n3\n4\n5\nStep\n1e6\n100'000 Games\n0\n1\n2\n3\n4\n5\nStep\n1e6\n1'000'000 Games\n400K\n1M\n2M\n7M\n9M\n34M\n(a) Training Loss\n0\n1\n2\n3\n4\n5\nStep\n1e6\n2\n0\n2\n1\n2\n2\n2\n3\n2\n4\nVal. Loss (log scale)\n10'000 Games\n0\n1\n2\n3\n4\n5\nStep\n1e6\n100'000 Games\n0\n1\n2\n3\n4\n5\nStep\n1e6\n1'000'000 Games\n400K\n1M\n2M\n7M\n9M\n34M\n(b) Test Loss\nFigure A4 | Loss curves when scaling model size and training set size.\n0\n1\n2\n3\n4\n5\nStep\n1e6\n0\n5\n10\n15\n20\n25\nKendall's \u03c4\nAction-Value\nState-Value\nBehavioral Cloning\n(a) Kendall\u2019s \ud835\udf0f\n0\n1\n2\n3\n4\n5\nStep\n1e6\n0\n20\n40\n60\n80\nPuzzle Accuracy (%)\nAction-Value\nState-Value\nBehavioral Cloning\n(b) Puzzles Accuracy (%)\n0\n1\n2\n3\n4\n5\nStep\n1e6\n10\n20\n30\n40\n50\n60\nAction Accuracy (%)\nAction-Value\nState-Value\nBehavioral Cloning\n(c) Action Accuracy (%)\nFigure A5 | Comparison of the three different prediction targets (action-value, state-value, and behavioral\ncloning) train on the datasets generated from 1 million games. Note that this means that the action-value\nnetwork is trained on roughly 30 times more data than the other two (cf. Table A1). Action-value learning\n(trained on 1.6B action-values) learns slightly slower but outperforms the other two variants in the long\nrun (which are trained on roughly 55M states / best actions). Behavioral-cloning falls significantly short of\nstate-value learning, even though both are trained on virtually the same amount of data.\n15\nGrandmaster-Level Chess Without Search\n0\n1\n2\n3\n4\n5\nStep\n1e6\n0\n5\n10\n15\n20\nKendall's \u03c4\nAction-Value\nState-Value\nBehavioral Cloning\n(a) Kendall\u2019s \ud835\udf0f\n0\n1\n2\n3\n4\n5\nStep\n1e6\n0\n10\n20\n30\n40\n50\n60\n70\n80\nPuzzle Accuracy (%)\nAction-Value\nState-Value\nBehavioral Cloning\n(b) Puzzles Accuracy (%)\n0\n1\n2\n3\n4\n5\nStep\n1e6\n10\n20\n30\n40\n50\n60\nAction Accuracy (%)\nAction-Value\nState-Value\nBehavioral Cloning\n(c) Action Accuracy (%)\nFigure A6 | Comparison of the three different prediction targets (action-value, state-value, and behavioral\ncloning) trained on exactly the same number of data points (40M). The superiority of action-value learning over\nstate-value learning disappears (or even reverses to some degree), except when measuring the action-ranking\ncorrelation (Kendall\u2019s \ud835\udf0f) which the action-value policy is indirectly trained to perform well on.\nof the oracle (with cross-entropy loss), rather than the\nbest action only would largely close this gap, but we\nconsider this question beyond the scope of this paper\nand limit ourselves to simply reporting the observed\neffect in our setting.\nB.3. Polyak Averaging\nWe investigate the impact of Polyak averaging, an\noptimization technique where parameters are set to a\nweighted average over the last iterations rather than\njust using the most recent value, using the same setup\nas for our ablation experiments (see Appendix A.3).\nWhen using Polyak averaging with an exponential\nmoving average decay factor of 0.99, we obtain a\nKendall\u2019s \ud835\udf0f of 0.259, a puzzle accuracy of 83.3%, and\nan action accuracy of 63.0%. In contrast, standard\nevaluation, obtains a Kendall\u2019s \ud835\udf0f of 0.258, a puzzle\naccuracy of 83.1%, and an action accuracy of 62.8%.\nThus, we use Polyak averaging for all experiments.\nB.4. Tactics\nIn Figure A7, we analyze the tactics learned by our\n270M transformer used against a human with a blitz\nElo of 2145. We observe that our model has learned\nto sacrifice material when it is advantageous to build\na longer-term advantage.\nB.5. Playing Style\nWe recruited chess players of National Master level\nand above to analyze our agent\u2019s games against bots\nand humans on the Lichess platform. They made the\nfollowing qualitative assessments of its playing style\nand highlighted specific examples (see Figure A8).\nOur agent has an aggressive enterprising style where\nit frequently sacrifices material for long-term strategic\ngain. The agent plays optimistically: it prefers moves\nthat give opponents difficult decisions to make even\nif they are not always objectively correct. It values\nking safety highly in that it only reluctantly exposes\nits own king to danger but also frequently sacrifices\nmaterial and time to expose the opponent\u2019s king. For\nexample 17 .. Bg5 in game B.5.1 encouraged its op-\nponent to weaken their king position. Its style incor-\nporates strategic motifs employed by the most recent\nneural engines (Sadler and Regan, 2019; Silver et al.,\n2017). For example it pushes wing pawns in the mid-\ndlegame when conditions permit (see game B.5.2). In\ngame B.5.3 our agent executes a correct long-term\nexchange sacrifice. In game B.5.4 the bot uses a motif\nof a pin on the back rank to justify a pawn sacrifice\nfor long term pressure. Game B.5.5 features a piece\nsacrifice to expose its opponent\u2019s king. The sacrifice\nis not justified according to Stockfish although the\nopponent does not manage to tread the fine line to\na permanent advantage and blunders six moves later\nwith Bg7.\nOur agent has a distinct playing style to Stockfish:\none analyzer commented \u201cit feels more enjoyable than\nplaying a normal engine\u201d, \u201cas if you are not just hope-\nlessly crushed\u201d. Indeed it does frequently agree with\nStockfish\u2019s move choices suggesting that our agent\u2019s\naction-value predictions match Stockfish\u2019s. However\nthe disagreements can be telling: the piece sacrifice\nin the preceding paragraph is such an example. Also,\ngame B.5.6 is interesting because our agent makes\nmoves that Stockfish strongly disagrees with. In partic-\nular our agent strongly favours 18 .. Rxb4 and believes\nblack is better, in contrast Stockfish believes white is\nbetter and prefers Nd4. Subsequent analysis by the\nmasters suggests Stockfish is objectively correct in this\ninstance. Indeed on the very next move our agent has\n16\nGrandmaster-Level Chess Without Search\nHuman (2145 Elo)\n(a) Blunder (Bd4 was\nbest)\n270M Transformer\nHuman (2145 Elo)\n270M Transformer\n(b) Inaccuracy (d4 was\nbest)\n(c) Checkmate is now un-\navoidable (Bf2 was best)\nFigure A7 | Example of the learned tactics for our 270M transformer (vs. a human player with a blitz Elo of\n2145). Our model decides to sacrifice two pawns since the white bishop will not be able to prevent it from\npromoting one of the pawns. The individual subfigure captions contain the Stockfish analysis from Lichess (i.e.,\nour model plays optimally).\n17\nGrandmaster-Level Chess Without Search\n(a) King weakening\n(b) Wing pawn push\n(c) Exchange sacrifice for long term\ncompensation\n(d) Long term sacrifice\n(e) Exposing the opponent\u2019s king\n(f) Disagreement with Stockfish\n(g) Optimistic blunder\nFigure A8 | Examples of our 270M transformer\u2019s playing style against online human opponents.\n18\nGrandmaster-Level Chess Without Search\nreversed its opinion and agrees with Stockfish.\nOur agent\u2019s aggressive style is highly successful\nagainst human opponents and achieves a grandmaster-\nlevel Lichess Elo of 2895. However, we ran another\ninstance of the bot and allowed other engines to play\nit. Its estimated Elo was far lower, i.e., 2299. Its ag-\ngressive playing style does not work as well against\nengines that are adept at tactical calculations, par-\nticularly when there is a tactical refutation to a sub-\noptimal move. Most losses against bots can be ex-\nplained by just one tactical blunder in the game that\nthe opponent refutes. For example Bxh3 in game B.5.7\nloses a piece to g4.\nFinally, the recruited chess masters commented that\nour agent\u2019s style makes it very useful for opening reper-\ntoire preparation. It is no longer feasible to surprise\nhuman opponents with opening novelties as all the\nbest moves have been heavily over-analyzed. Modern\nopening preparation amongst professional chess play-\ners now focuses on discovering sub-optimal moves that\npose difficult problems for opponents. This aligns ex-\ntremely well with our agent\u2019s aggressive, enterprising\nplaying style which does not always respect objective\nevaluations of positions.\nB.5.1. King weakening game\n1. e4 c5 2. Nf3 Nc6 3. Bb5 g6 4. O-O Bg7 5. c3 Nf6 6.\nRe1 O-O 7. d4 d5 8. e5 Ne4 9. Bxc6 bxc6 10. Nbd2\nNxd2 11. Bxd2 Qb6 12. dxc5 Qxc5 13. h3 Qb5 14. b4\na5 15. a4 Qc4 16. Rc1 Bd7 17. Bg5 f6 18. Bd2 Bf5 19.\nexf6 exf6 20. Nd4 Bd7 21. Nb3 axb4 22. cxb4 Qh4\n23. Nc5 Bf5 24. Ne6 Rfc8 25. Nxg7 Kxg7 26. Re7+\nKh8 27. a5 Re8 28. Qe2 Be4 29. Rxe8+ Rxe8 30. f3\n1-0\nB.5.2. Wing pawn push game\n1. e4 c6 2. d4 d5 3. Nc3 dxe4 4. Nxe4 Nf6 5. Ng3 c5\n6. Bb5+ Bd7 7. Bxd7+ Nbxd7 8. dxc5 Qa5+ 9. Qd2\nQxc5 10. Nf3 h5 11. O-O h4 12. Ne2 h3 13. g3 e5 14.\nNc3 Qc6 15. Qe2 Bb4 16. Bd2 O-O 17. Rae1 Rfe8 18.\nNe4 Bxd2 19. Qxd2 Nxe4 0-1\nB.5.3. Exchange sacrifice game\n1. d4 d5 2. c4 e6 3. Nc3 Bb4 4. cxd5 exd5 5. Nf3 Nf6\n6. Bg5 h6 7. Bh4 g5 8. Bg3 Ne4 9. Rc1 h5 10. h3\nNxg3 11. fxg3 c6 12. e3 Bd6 13. Kf2 h4 14. g4 Bg3+\n15. Ke2 O-O 16. Kd2 Re8 17. Bd3 Nd7 18. Kc2 Rxe3\n19. Kb1 Qe7 20. Qc2 Nf8 21. Rhf1 Ne6 22. Bh7+ Kg7\n23. Bf5 Rxf3 24. gxf3 Nxd4 25. Qd3 Nxf5 26. gxf5\nQe5 27. Ka1 Bxf5 28. Qe2 Re8 29. Qxe5+ Rxe5 30.\nRfd1 Bxh3 31. Rc2 Re3 32. Ne2 Bf5 33. Rcd2 Rxf3\n34. Nxg3 hxg3 0-1\nB.5.4. Long term sacrifice game\n1. d4 d5 2. c4 e6 3. Nf3 Nf6 4. Nc3 Bb4 5. Bg5 dxc4\n6. e4 b5 7. a4 Bb7 8. axb5 Bxe4 9. Bxc4 h6 10. Bd2\nBb7 11. O-O O-O 12. Be3 c6 13. bxc6 Nxc6 14. Qb3\nQe7 15. Ra4 a5 16. Rd1 Rfd8 17. d5 exd5 18. Nxd5\nNxd5 19. Rxd5 Rxd5 20. Bxd5 Rd8 21. Ra1 a4 22.\nRxa4 Qd7 23. Bc4 Qd1+ 24. Qxd1 Rxd1+ 25. Bf1\nBa5 26. Rc4 Rb1 27. Rc2 Nb4 28. Rc5 Nc6 29. Bc1\nBb4 30. Rc2 g5 31. h4 g4 32. Nh2 h5 33. Bd3 Ra1 34.\nNf1 Ne5 35. Be2 Be4 36. Rc8+ Kh7 37. Be3 Re1 38.\nBb5 Bd3 39. Bxd3+ Nxd3 40. Rd8 Nxb2 41. Rd5 Be7\n42. Rd7 Bxh4 43. g3 Bf6 44. Rxf7+ Kg6 45. Rxf6+\nKxf6 46. Bd4+ Kg5 47. Bxb2 Rb1 48. Bc3 Kf5 49.\nKg2 Rb3 50. Ne3+ Ke4 51. Bf6 Rb5 52. Kf1 Rb6 53.\nBc3 Rb3 54. Bd2 Kd3 55. Be1 Rb5 56. Ng2 Ke4 57.\nKe2 Rb2+ 58. Bd2 Rc2 59. Ne3 Ra2 60. Nc4 Kd4 61.\nNd6 Ke5 62. Ne8 Kf5 63. Kd3 Ra6 64. Bc3 Rc6 65.\nBb4 Kg6 66. Nd6 Ra6 67. Bc5 Ra5 68. Bd4 Ra6 69.\nNc4 Ra4 70. Nb6 Ra5 71. Ke4 h4 72. gxh4 Kh5 73.\nBf6 Ra2 74. Ke3 Ra3+ 75. Ke2 g3 76. Nd5 Ra2+ 77.\nKf3 gxf2 78. Nf4+ Kh6 79. Kg2 f1=Q+ 80. Kxf1 Rc2\n81. Bg5+ Kh7 82. Ne2 Kg6 83. Kf2 Ra2 84. Kf3 Ra4\n85. Ng3 Rc4 86. Bf4 Rc3+ 87. Kg4 Rc4 88. h5+ Kf6\n89. Nf5 Ra4 90. Ne3 Ra5 91. Nc4 Ra4 92. Ne5 Kg7\n93. Kf5 Ra5 94. Kg5 Rb5 95. Kg4 Rb1 96. Kf5 Rb5\n97. Ke4 Ra5 98. h6+ Kh7 99. Bd2 Ra2 100. Be3 Ra6\n101. Ng4 Ra3 102. Bd2 Ra2 103. Bf4 Ra5 104. Kf3\nRf5 105. Ke3 Kg6 106. Ke4 Rh5 107. Kf3 Rh3+ 108.\nKg2 Rh5 109. Kg3 Ra5 110. Be3 Ra3 111. Kf3 Rb3\n112. Ke4 Rb4+ 113. Bd4 Ra4 114. Ke5 Rc4 115. Kd5\nRa4 116. Ke4 Rb4 117. Kd3 Ra4 118. Kc3 Ra3+ 119.\nKc4 Rg3 120. Ne3 Rh3 121. Kd5 Rxh6 122. Bb6 Rh3\n123. Nc4 Rh5+ 124. Ke6 Rg5 125. Nd2 Rg2 126. Nf1\nRb2 127. Bd8 Re2+ 128. Kd5 Re1 129. Ne3 Rxe3\n130. Bh4 Kf5 131. Bf2 Rd3+ 132. Kc4 Ke4 133. Bc5\nRc3+ 134. Kxc3 1/2-1/2\nB.5.5. Expose king game\n1. e4 c5 2. Nf3 Nc6 3. Na3 Nf6 4. e5 Nd5 5. d4 cxd4\n6. Nb5 a6 7. Nbxd4 g6 8. Bc4 Nc7 9. Nxc6 bxc6 10.\nNg5 Ne6 11. Nxf7 Kxf7 12. Bxe6+ Kxe6 13. Bd2 Kf7\n14. Qf3+ Kg8 15. e6 dxe6 16. O-O-O Qd5 17. Qe3\nBg7 18. Bc3 Qxa2 19. Rd8+ Kf7 20. Qf4+ Bf6 21.\nRxh8 Qa1+ 22. Kd2 Qxh1 23. Bxf6 exf6 24. Qc7+\n1-0\nB.5.6. Stockfish disagreement game\n1. e4 c5 2. Nf3 Nc6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3\ne6 6. Ndb5 d6 7. Bf4 e5 8. Bg5 a6 9. Na3 b5 10. Nd5\nQa5+ 11. Bd2 Qd8 12. Bg5 Be7 13. Bxf6 Bxf6 14. c4\nb4 15. Nc2 Rb8 16. g3 b3 17. axb3 Rxb3 18. Ncb4\nRxb4 19. Nxb4 Nxb4 20. Qa4+ Kf8 21. Qxb4 g6 22.\nBg2 h5 23. h4 Kg7 24. O-O g5 25. hxg5 Bxg5 26. f4\nBe7 27. fxe5 dxe5 28. Qc3 Bc5+ 29. Kh2 Qg5 30. Rf5\n19\nGrandmaster-Level Chess Without Search\nBxf5 31. Qxe5+ Qf6 32. Qxf6+ Kxf6 33. exf5 Kg5 34.\nBd5 Rb8 35. Ra2 f6 36. Be6 Kg4 37. Kg2 Rb3 38. Bf7\nRxg3+ 39. Kf1 h4 40. Ra5 Bd4 41. b4 h3 42. Bd5 h2\n43. Bg2 Rb3 44. Rxa6 Rb1+ 45. Ke2 Rb2+ 0-1\nB.5.7. Blunder game\n1. b3 e5 2. Bb2 Nc6 3. e3 d5 4. Bb5 Bd6 5. Bxc6+\nbxc6 6. d3 Qg5 7. Nf3 Qe7 8. c4 Nh6 9. Nbd2 O-O\n10. c5 Bxc5 11. Nxe5 Bb7 12. d4 Bd6 13. O-O c5 14.\nQh5 cxd4 15. exd4 Rae8 16. Rfe1 f6 17. Nd3 Qf7 18.\nQf3 Bc8 19. h3 Nf5 20. g3 Ne7 21. Bc3 Bxh3 22. g4\nf5 23. Qxh3 fxg4 24. Qxg4 h5 25. Qe6 g5 26. Qxf7+\nRxf7 27. Bb4 Ref8 28. Bxd6 cxd6 29. b4 Nf5 30. Re6\nKg7 31. Rd1 Rc7 32. Nf3 g4 33. Nd2 h4 34. Nb3 Rc2\n35. Nf4 g3 36. Nh5+ Kh7 37. fxg3 Nxg3 38. Nxg3\nRg8 39. Rd3 Rxa2 40. Rxd6 Rb2 41. Rxd5 Rxg3+ 42.\nRxg3 hxg3 43. Nc5 Kg6 44. b5 Rxb5 45. Kg2 a5 46.\nKxg3 a4 47. Rd6+ Kf5 48. Nxa4 Rb3+ 49. Kf2 Rh3\n50. Nc5 Kg5 51. Rc6 Kf5 52. d5 Ke5 53. d6 Rh2+ 54.\nKg3 Rd2 55. d7 Rxd7 56. Nxd7+ Ke4 57. Rd6 Ke3\n58. Nf6 Ke2 59. Ng4 Ke1 60. Kf3 Kf1 61. Rd1# 1-0\n20\n"
  },
  {
    "title": "BiLLM: Pushing the Limit of Post-Training Quantization for LLMs",
    "link": "https://arxiv.org/pdf/2402.04291.pdf",
    "upvote": "48",
    "text": "BiLLM: Pushing the Limit of Post-Training Quantization for LLMs\nWei Huang 1\nYangdong Liu 2\nHaotong QinB 3 2\nYing Li 2\nShiming Zhang 1\nXianglong Liu 2\nMichele Magno 3\nXiaojuan Qi 1\nAbstract\nPretrained large language models (LLMs) exhibit\nexceptional general language processing capabili-\nties but come with significant demands on mem-\nory and computational resources. As a power-\nful compression technology, binarization can ex-\ntremely reduce model weights to a mere 1 bit,\nlowering the expensive computation and mem-\nory requirements. However, existing quantiza-\ntion techniques fall short of maintaining LLM\nperformance under ultra-low bit-widths. In re-\nsponse to this challenge, we present BiLLM, a\ngroundbreaking 1-bit post-training quantization\nscheme tailored for pretrained LLMs. Based on\nthe weight distribution of LLMs, BiLLM first iden-\ntifies and structurally selects salient weights, and\nminimizes the compression loss through an ef-\nfective binary residual approximation strategy.\nMoreover, considering the bell-shaped distribu-\ntion of the non-salient weights, we propose an op-\ntimal splitting search to group and binarize them\naccurately. BiLLM achieving for the first time\nhigh-accuracy inference (e.g. 8.41 perplexity on\nLLaMA2-70B) with only 1.08-bit weights across\nvarious LLMs families and evaluation metrics,\noutperforms SOTA quantization methods of LLM\nby significant margins. Moreover, BiLLM enables\nthe binarization process of the LLM with 7 billion\nweights within 0.5 hours on a single GPU, demon-\nstrating satisfactory time efficiency. The code\nis available at https://github.com/Aaronhuang-\n778/BiLLM.\n1. Introduction\nRecently, large language models (LLMs) based on trans-\nformers (Vaswani et al., 2017) have garnered significant\nattention in natural language processing. Pretrained LLMs\n1The University of Hong Kong\n2Beihang University\n3ETH Z\u00a8urich.\nCorrespondence to:\nBHaotong Qin <qinhao-\ntong@gmail.com>.\n5.68\n5.68\n6.28\n25.53\n106767.33\n168388\n5.68\n5.68\n6.19\n11.1\n152.31\n267001.71\n1\n10\n100\n1000\n10000\n100000\n1000000\n16\n8\n4\n3\n2\n1\nRTN\nGPTQ\nPB-LLM (INT 8, 10%)\nBiLLM (Ours)\n15.14 (Ours)\n838.13\nQuantization Bit-width\nPerplexity in \ud425\u0211\u0209\ud7cf\u01df scale\nLLaMA-13B \nFigure 1. The perplexity of LLaMA-13B on WikiText2 under dif-\nferent bit-widths. Round-to-nearest (RTN), GPTQ, and PB-LLM\n(10% weight of INT8) suffer accuracy loss at ultra-low bits, fac-\ning the sharply increasing perplexity (\u2193). BiLLM demonstrates\nexceptional performance under binarization.\nlike OPT (Zhang et al., 2022) and LLaMA (Touvron et al.,\n2023a), have demonstrated excellent performance across a\nrange of evaluation benchmarks. However, LLMs pose sub-\nstantial challenges in deployment on memory-constrained\ndevices due to their immense parameter size and computa-\ntion requirements. For instance, the widely-used LLaMA2-\n70B (Touvron et al., 2023b) model, with its 70 billion param-\neters, requires 150 GB of storage in half-precision (FP16)\nformat. This necessitates at least two A100 GPUs, each\nwith 80 GB of storage space, for inference.\nModel quantization has emerged as a highly effective tech-\nnology for compressing neural networks, thereby reducing\nthe model size of LLMs and substantially saving GPU mem-\nory consumption (Dettmers et al., 2022). Current quan-\ntization techniques primarily fall into Quantization-Aware\nTraining (QAT) and Post-Training Quantization (PTQ). QAT\ninvolves fine-tuning and retraining during the quantization\nprocess, while PTQ significantly streamlines the compu-\ntation by eliminating back-propagation, enabling a faster\nquantization process and promoting the practicality of quan-\ntization (Frantar et al., 2022; Shang et al., 2023; Lin et al.,\n2023). Given the deep structures and numerous parameters\nof LLMs, PTQ stands out for its ability to rapidly perform\nthe quantization process, especially on time and resource-\nconstrained scenarios (Zhu et al., 2023).\nDespite the success of previous PTQ methods in 8-bit and\n4-bit quantization (Dettmers et al., 2022; 2023b; Frantar\n1\narXiv:2402.04291v1  [cs.LG]  6 Feb 2024\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\nDensity Distribution of Weight\n\u01ff\u020b\n\u01ff\u020b+\u020d\n...\nlong-tail distribution\nfrequency\nHessian\n0\n0\nSensitivity\nValue\nMagnitude\nbell-shaped distribution\nFigure 2. The Hessian metrics (sensitivity) and magnitude (value)\nof weights in LLMs. The weights of different layers in LLMs are\ncharacterized by bell-shaped distribution, accompanied by a few\nsalient values.\net al., 2022; Xiao et al., 2023; Frantar & Alistarh, 2022), the\nexpanding size of LLMs demands more aggressive quan-\ntization approaches (Shang et al., 2023). Neural network\nbinarization, which reduces the weight bit-width to only 1\nbit, is a promising approach (Helwegen et al., 2019; Qin\net al., 2020; 2023). However, as depicted in Figure 1, current\nadvanced PTQ methods for LLMs exhibit a performance\ncollapse under ultra-low bit (\u2a7d3 bits) quantization. This\nphenomenon can be attributed to the significant difference\nbetween quantized and original weights. Even the recent bi-\nnary PTQ method for LLMs, PB-LLM (Shang et al., 2023),\nonly maintains a perplexity metric of around 800 with an\naverage weight of 1.7 bits. This observation underscores\nthe challenges existing PTQ methods face in promoting the\nweight binarization of LLMs.\nIn pursuit of this goal, we conducted an empirical study to\nanalyze the distribution of pre-trained weights in LLMs. The\nfindings derived from this study are presented in Appendix\nG, revealing two key observations:\n\u2022 The second-order Hessian matrix of weights demon-\nstrates an exceptionally long-tail distribution and is\noften used to measure the importance of weight ele-\nments in neural networks (LeCun et al., 1989; Dong\net al., 2019). As depicted in Figure 2, a small fraction\nof weights elements possesses significantly high Hes-\nsian values, substantially influencing the layer output.\nIn contrast, most Hessian values cluster around 0.\n\u2022 The density distribution of weight magnitudes in LLMs\nfollows a bell-shaped pattern. This bell-shaped dis-\ntribution exhibits a significant resemblance to both the\nGaussian or Laplace distribution in terms of its char-\nacteristics (Blundell et al., 2015). Figure 2 illustrates\nthat most weight values cluster around zero with a\nnon-uniform bell-shaped distribution.\nThe above implies: a) A minority of weights play an impor-\ntant role in LLMs, whereas the majority of weights exhibit\ncharacteristics of redundancy (Shang et al., 2023; Dettmers\net al., 2023b); b) With the most aggressive bit-width, bina-\nrization incurs most severe error among quantization under\nbell-shaped distributions in LLMs (Jacob et al., 2018).\nMotivated by the above observation, we propose a novel\n1-bit PTQ framework for LLMs, namely BiLLM, incorpo-\nrating two core designs to achieve highly accurate weight\nbinarization. First, guided by the Hessian-based metric, we\nselect the salient weights structurally (Figure 3 upper-right)\nto achieve a trade-off between accuracy and storage sav-\nings and develop a residual approximation to maximize the\nrestoration of salient weights with highly dynamic range.\nSecond, for the remaining non-salient weights (Figure 3\nlower-right), we design an optimal splitting binarization\nstrategy, where a meticulous search process is applied to de-\ntermine an optimal break-point for weight distribution and\nbinarization of the segments is then processed separately to\nminimize binarization errors. Moreover, BiLLM incorpo-\nrates error compensation on a block-wise basis by default\nfollowing existing common practices (Frantar et al., 2022;\nShang et al., 2023), which further reduces quantization error.\nExtensive experiments demonstrate that BiLLM achieve the\nstate-of-the-art (SOTA) performance for LLMs across mul-\ntiple LLM families on various evaluation metrics, and first\nachieves extremely compact 1.07\u223c1.11 bit-width in aver-\nage for the PTQ binarization. For example, on the Wiki-\ntext2(Merity et al., 2016) metric, BiLLM achieved perplexi-\nties of 8.49 and 8.41 with only 1.08-bit weights on LLaMA-\n65B (Touvron et al., 2023a)and LLaMA2-70B (Touvron\net al., 2023b), respectively, even surpassing the 9.34 perfor-\nmance of the FP16 OPT-66B (Zhang et al., 2022).\n2. Related Works\n2.1. Large Language Model Quantization\nQuantization maps high-precision parameters to a discrete\nrange. This method, which compresses parameters without\naltering the model structure, effectively reduces the storage\nand computational overhead of deep neural networks. Re-\ncent work has successfully applied QAT and PTQ to LLMs.\nQAT, through a quantization-aware retraining strategy, bet-\nter preserves the performance of quantized models. LLM-\nQAT (Liu et al., 2023) addressed data barrier issues in QAT\ntraining through data-free distillation. However, for LLMs\nwith extremely large parameter sizes, the cost of retraining\nis prohibitively high and inefficient. Therefore, techniques\nsuch as QLoRA (Dettmers et al., 2023a) focus on parameter-\nefficient fine-tuning (PEFT) methods for quantizing LLMs,\nenhancing the efficiency of QAT. Nevertheless, even these\nefficient fine-tuning quantization strategies require over 24\nhours of GPU time.\nTherefore, the PTQ strategy has become a significant\noption for quantizing LLMs efficiently.\nWorks like\nBRECQ (Li et al., 2021), ZerqQuant (Yao et al.)\nand\nLLM.int8() (Dettmers et al., 2022) enhance quantization\n2\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\n Value\nBinarized \nFC for Q\nBinarized \nFC for  K\nBinarized \nFC for V\nMatMul\nMatMul\nActivation\nActivation\nBinarized\nFC1\nActivation\nBinarized\nFC2\nActivation\nMulti-Head\nSelf-Attrntion \nFeed-Fordward Block\nAbsolute Value\nBiLLM Framework\nBinarized-FC Projection\n Binary  Weight\nHessian Matrix\nFloat Weight\n Value\nResidual\nBinarization\n\u03eb\u2217\nBell-shaped Splitting for Non-salient Weight\nSplitting\nBinarization\nBinary Residual Approximation for Salient Weight\n Value\n Value\n Value\n Value\nFigure 3. Schematic of the PTQ binarization framework for LLMs. The left side shows the structure of the Transformer block after\nbinarization. The right side shows the binarization process of BiLLM, which consists of two parts, Residual Approximation for salient\nweights and Bell-shaped Splitting for non-salient weights.\naccuracy by adding additional grouping labels for custom\nquantization blocks. Other studies adopt a feature segmen-\ntation strategy, such as PB-LLM (Shang et al., 2023) and\nSpQR (Dettmers et al., 2023b). They preserve the bit-width\nof outlier features or those with higher quantization errors\nto FP16 or INT8, mitigating the precision loss due to quanti-\nzation. GPTQ (Frantar et al., 2022) employs a more precise\nquantization framework, reducing the block quantization\nerrors of LLMs through Hessian-based second-order er-\nror compensation (Frantar & Alistarh, 2022), achieving\ncommendable performance in low-bits (4 bits) quantization.\nSmoothquant (Xiao et al., 2023) introduces a strategy of\nscaling weight and activation outliers to simplify quantiza-\ntion. Subsequently, AWQ (Lin et al., 2023) and OWQ (Lee\net al., 2023) also proposed scale transformations of more\ncrucial weight channels for activation features, preserving\ntheir information representation capacity.\n2.2. Network Binarization\nBinarized compression can quantize parameters to only 1 bit,\nexpressed as \u00b11. In forward propagation, the sign function\nis used to binarize the original parameter tensor:\nWb = \u03b1 \u00b7 sign(Wf),\n(1)\nsign(x) =\n(\n1\nif x \u2265 0,\n\u22121\nothers.\n(2)\nwhere Wf \u2208 Rn\u00d7m is the full precision weight and Wb \u2208\nRn\u00d7m is the binarized output. n and m represent the size of\nthe weight matrix. \u03b1 denotes the scaling factor (Courbariaux\net al., 2016). Binarization usually uses the channel-wise\nscale (Rastegari et al., 2016; Qin et al., 2023), so \u03b1 \u2208 Rn.\nMost previous binarization works adopt a framework based\non QAT for quantization (Qin et al., 2023). Straight through\nestimator (STE) (Bengio et al., 2013) is deployed to ad-\ndress the issue of gradient vanishing caused by the sign(\u00b7)\nfunction. Binary Weight Network (BWN) (Rastegari et al.,\n2016) was initially proposed for executing neural network\ncomputations by binarizing weights and using full-precision\nactivations, while XNOR-Net (Rastegari et al., 2016) ex-\ntends this approach by binarizing both weights and activa-\ntions. Both methods minimize quantization errors through\ndynamic searching of \u03b1. DoReFa-Net (Zhou et al., 2016)\nfurther expands upon XNOR-Net, employing quantized gra-\ndients to accelerate network training. Group segmentation\nis also applied in binarization tasks, with Syq (Faraone et al.,\n2018) utilizing network weight to the small size of groups\nfor minimizing binarization errors.\nBased on the successful application of binarization in Trans-\nformers (Wang et al., 2023) and Bert (Qin et al., 2022), we\nbelieve that the binarization of LLMs is filled with poten-\ntial. PB-LLM (Shang et al., 2023) investigates the impact\nof binarized QAT and PTQ strategies on LLMs, but it is\nnecessary to retain a significant proportion (over 30%) of\nthe weights at 8 bits to enable LLMs to produce reasonable\nanswers. Due to the presence of a large amount of INT8,\nLLMs still have a relatively high average bit-width. To ad-\ndress this issue, we proposed BiLLM, which aims to push\nthe limit of PTQ binarization for LLMs.\n3. Method\nTo achieve accurate binarization of LLMs, our approach is\ndesigning distinct binarization strategies for salient and non-\nsalient weights. We first introduce the selection rules for\nsalient weights and their binarization strategies in Section\n3.1. Then, we elaborate on the distribution-based binariza-\ntion strategy for non-salient weights in Section 3.2.\n3.1. Salient Weight Binarization for LLMs\nIn deep neural networks, not all parameters carry equal sig-\nnificance. Utilizing solely the magnitude of the weights\n3\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\ncan not fully capture the impact of each element on the\nmodel\u2019s performance. The Hessian metric serves as a com-\nmon benchmark for detecting parameter sensitivity (Dong\net al., 2019; Dettmers et al., 2023b; 2022). We thus leverage\nthe Hessian matrix to assess the salience of parameters in\neach under-binarized layer. We implement an optimized\ncomputation process to derive weight sensitivity, which\nallows us to obtain the importance metric of parameters\nwithout compromising efficiency:\nsi =\nw2\ni\n[H\u22121]2\nii\n,\n(3)\nwhere H represents the Hessian matrix of each layer, and\nwi represents the original value of each element. In the\nfollowing section, si serves as a criterion for assessing the\nsignificance of weight elements and is used as a feature\nindicator for structured selection.\nStructural Searching Selection. Utilizing an unstructured\nselection enables the coverage of all salient elements. How-\never, it requires the implementation of an additional 1-bit\nbitmap index (Chan & Ioannidis, 1998), leading to increased\naverage bit-width. This balance is inefficient, especially for\nHessian outlier weights that constitute a mere 1-5% of the\ntotal (Yao et al., 2023). In our analysis of sensitivity distri-\nbution within LLMs, we discovered that the majority of the\nweights\u2019 sensitive Hessian values are predominantly con-\ncentrated in specific columns or rows (Appendix G). This\npattern is attributed to the convergence effects inherent in\nthe multi-head self-attention mechanism of these models\nand further motivates us to implement a structured approach\nfor selecting salient weights, for reducing the additional\nbitmap. Given that BiLLM employs a per-channel (or per-\nrow) type of binarization, we determine salience through a\nper-column segmentation on the whole weight matrix.\nWe organize the column salience in descending order and\nintroduce an optimized search algorithm aimed at minimiz-\ning quantization error, which in turn determines the number\nof columns within the salient group. To elaborate on this\nmethodology, we initially define the objective of binariza-\ntion quantization, grounded on Equation (1):\narg min\n\u03b1,B\n||W \u2212 \u03b1B||2,\n(4)\nwhere B \u2208 {\u22121, +1}k\u00d7m, k is the number of selected\ncolumns. The problem (Rastegari et al., 2016) of optimal\n\u03b1 and B can simply be solved as \u03b1 = ||W||\u21131\nm\nand B =\nsign(W). Then, the optimization function for selecting\nsalient columns is defined as:\narg min\nWuns\n||W\u2212(\u03b1sal sign(Wsal)\u222a\u03b1uns sign(Wuns))||2, (5)\nwhere Wsal denotes the column-wise combination of orig-\ninal weight and Wuns is the left non-salient part. We can\n\u0356\u0389\n\u0356\ud42c\ud41a\ud425\u0376\u0373\u037a\u7300\nResdual \noriginal \nbinarization\nresidual \nbinarization \nH\n\u036b\ud42c\ud41a\ud425\u0376\u0373\u037a\u7300\n\u036b\n-\n\u0356\u0388\nFigure 4. Illustration of salient weight binarization. The B1 bina-\nrized from salient weight is made into a residual with the original\nvalue and then binarized again to obtain B2.\neasily get that W = Wsal \u222a Wuns, so the only variable\nparameter is the number of rows in Wsal.\nBinary Residual Approximation. Salient weights are lim-\nited in quantity, yet exhibit significant variance when ag-\ngregated. Direct preservation of these weights in INT8 or\nFP16 formats leads to an increase in the average weight bits,\nundermining the compressive benefits of binarization. Tra-\nditional binarization methods for salient weights, however,\nresult in substantial quantization errors. To that end, we\ndevelop a residual approximation approach for binarizing\nsalient weights. Contrary to the comprehensive high-order\nquantization (Li et al., 2017) applied to the entire weight\nmatrix, our technique minimizes binarization error through\na second-order approximation of merely a select subset of\nsalient weights. This method guarantees the precision of\nsalient weights while simultaneously decreasing bit-width\noverhead. As illustrated in Figure 4, this approach incor-\nporates a recursive computation strategy for weight bina-\nrization compensation, applying a subsequent binarization\nprocess to the residuals remaining after the initial binary pro-\ncess. Building upon Equation (4), we propose a redesigned\nresidual approximation optimization specifically for salient\nweights, which is defined as follows:\n\uf8f1\n\uf8f2\n\uf8f3\n\u03b1\u2217\no, B\u2217\no = arg min\n\u03b1o,Bo\n||W \u2212 \u03b1oBo||2),\n\u03b1\u2217\nr, B\u2217\nr = arg min\n\u03b1r,Br\n||(W \u2212 \u03b1\u2217\noB\u2217\no) \u2212 \u03b1rBr||2),\n(6)\nwhere Bo represents the original binary tensor, while Br\ndenotes the residual binarized matrix with the same size as\nBo. We efficiently solve for the two binarized optimization\nobjectives using the same solution method as in Equation (4).\nUltimately, we arrive at the following approximation:\nW \u2248 \u03b1\u2217\noB\u2217\no + \u03b1\u2217\nrB\u2217\nr.\n(7)\nIt can be easily proven that the residual approach of Equa-\ntion (7) has a lower quantization error than the direct one of\nEquation (4). We define the residual binarization error E:\nErb = ||W \u2212 \u03b1\u2217\noB\u2217\no \u2212 \u03b1\u2217\nrB\u2217\nr||2.\n(8)\n4\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\nNon-salient Weight Distribution\nOptimal Breakpoint Search\nSparse Area\nSparse Area\nSensitivity Matrix\n Salient Weight\nNon-salient \nWeight\nConcentrated Area\n\u03b8\u2217\nFigure 5. Distribution and splitting schematic of the 4th projection\nlayer in LLaMA2-7B. The top 5% of the Hessian elements are\norange, and the optimal break-point divides the non-salient weights\ninto sparse and concentrated areas.\nThe original binarized quantization error is calculatde as\n||W \u2212 \u03b1\u2217\noB\u2217\no||2 by Equation (4), and from the second\nsub-equation of Equation (6) we can get that loss Erb \u2264\n||W \u2212 \u03b1\u2217\noB\u2217\no||2. Therefore, through the method of resid-\nual approximation, we are able to further reduce the binary\nquantization error of salient weights with ultra-low bit-width\nstorage compared to retaining salient weights at 8 or 16 bits.\n3.2. Bell-shaped Distribution Splitting for Binarization\nFollowing the removal of salient weights, the remaining\nweights maintain a bell-shaped distribution, which becomes\ncloser to symmetric with the exclusion of salient weights\u2019\nimpact, as depicted in Figure 5. Binary quantization, rep-\nresenting an extreme form of uniform quantization, en-\ncounters more loss in the presence of non-uniform distribu-\ntions. A practical approach involves the group-wise quan-\ntization (Park et al., 2018; Fang et al., 2020; Jain et al.,\n2019) of weights according to their distribution. Balancing\nbetween quantization accuracy and compression efficiency,\nwe identify a single break-point within the distribution. As\nshown in Figure 5, this partition divides the non-salient bell-\nshaped distribution into two categories: the sparse area and\nthe concentrated area.\nThe segmentation process identifies a break-point that cat-\negorizes non-salient weights into two groups: Ac[\u2212p, p]\nfor concentrated weights and As[\u2212m, \u2212p] \u222a [p, m] for\nsparse weights, where signifies the maximum extent of\nnon-salient weights. We then apply binarization to both\nAc (concentrated) and As (sparse). To determine the opti-\nmal break-point p\u2217, we assume that the non-salient weights\npossess a symmetrical probability density function (PDF)-\ng(x) over the bounded domain [\u2212m, m], with the properties\ng(x) = g(\u2212x). Then the mean squared quantization error\nof binarization is defined as:\n\u03b82\nq =\nZ 0\n\u2212m\n(\u2212\u03b1 \u2212 x)2g(x)dx +\nZ m\n0\n(\u03b1 \u2212 x)2g(x)dx. (9)\nSince g(x) is a symmetric function, the above formula is\nsimplified to:\n\u03b82\nq = 2\nZ m\n0\n(\u03b1 \u2212 x)2g(x)dx.\n(10)\nThen, the break-point p divides the non-salient weights into\ntwo parts. According to the Equation (10), under the discon-\ntinuous weight distribution, we get a new binary quantiza-\ntion error:\n\u03b82\nq,p = ||Ws \u2212 \u03b1sBs||2 + ||Wc \u2212 \u03b1cBc||2,\n(11)\nwhere Ws and Wc denote the weights of the sparse and\nconcentrated area, respectively. Bs and Bc were calculated\nfrom Equation (2), \u03b1s and \u03b1c are the binarization scales,\ndetermined by Equation (4):\n\u03b1s = 1\nns\n||Ws||\u21131, \u03b1c = 1\nnc\n||Wc||\u21131,\n(12)\nwhere n represents the number of weight elements in each\narea. Therefore, the problem function is only related to p,\nand our target to find the optimal p\u2217 can be defined as:\np\u2217 = arg min\np\n(\u03b82\nq,p).\n(13)\nWhen the remaining weights follow an ideal Gaussian\ndistribution, Equation (11) is demonstrated to be a con-\nvex function with a global minimum, as evidenced in\nprior studies (Fang et al., 2020; You, 2010). Nonetheless,\nthe actual distribution of non-salient weights, while bell-\nshaped, diverges from the ideal Gaussian model. Simultane-\nously, we retain the block-wise compensation strategies of\nGPTQ (Frantar et al., 2022) and OBC (Frantar & Alistarh,\n2022) to offset quantization errors, which could change the\ndistribution of weights. In response, we employ a percentile\nsearch method to identify the optimal break-point based\non the objective function outlined in Equation (13). This\npercentile search strategy is efficient and straightforward,\ncompleting the binarization process for a 7B LLM within\nmerely 30 minutes. Furthermore, our findings indicate that\ndespite the deviation of non-salient weights from the ideal\nGaussian distribution, the error curve associated with the\nsearch process still exhibits convex properties (as detailed\nin Appendix C), confirming the feasibility of pinpointing\nthe optimal break-point.\n3.3. Pipeline of BiLLM\nAs depicted in Figure 3 left, BiLLM primarily performs\nbinary quantization on all Linear weights within the Trans-\nformer blocks. This section introduces the detailed pipeline\nof BiLLM.\n5\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\n1\n1.005\n1.01\n1.015\n1.02\n1.025\n1.03\n1.035\n1.04\n1.045\n1.05\n32\n64\n128\n256\n512\n1024\nAverage bit-width\nblock size\nstoring\n1\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAverage bit-width\nsalient ratio\nweights\nFigure 6. Weights and hardware overhead changes on Llama-7B.\nThe left picture shows the calculation parameters as a function of\nthe significant weight ratio; the right picture shows the hardware\noverhead as a function of the block.\nTable 1. Average bit results from structural searching and residual\nbinarization of OPT, LLaMA, and LLaMA2 families.\nModel\n7B\n13B\n30B\n66B/65B/70B*\nOPT\n1.10\n1.12\n1.12\n1.13\nLLaMA\n1.09\n1.09\n1.10\n1.10\nLLaMA2\n1.07\n1.08\nN/A\n1.09\n*: OPT-66B, LLaMA-65B and LLaMA2-70B.\nBinarization Workflow. We first deploy the structural\nsearch of salient columns and a residual approximation\nbinarization for salient columns. The process of salient\ncolumns incurs additional weight bits due to the search\nproportion and residual mechanism. Table 1 presents the\nextra bits generated in some LLMs (Zhang et al., 2022; Tou-\nvron et al., 2023a;b). It can be observed that the searching\nand residuals bring only about 0.1 additional weight bits.\nThen, for these non-uniformly distributed weights, we use\na split binarization strategy searching optimal p\u2217. The con-\ncentrated area and the sparse area are binarized separately.\nThis part incurs the cost of an additional 1 bit for hardware\ngroup identification, but the computing parameters are still\ncompressed to 1 bit. By retaining only block-wise com-\npensation(Frantar et al., 2022; Frantar & Alistarh, 2022)\nand eliminating column-wise quantization error compensa-\ntion, we further enhance the efficiency of PTQ and ensure\nthe effectiveness of distribution exploration. Algorithm 1\nillustrates the complete process of BiLLM, and detailed im-\nplementation of BiLLM is shown in Appendix A.\nExtra Storing Bits. The extra bits is acceptable under the bi-\nnary weight quantization of BiLLM. The weight parameters\nand additional hardware overhead are as follows:\n\uf8f1\n\uf8f2\n\uf8f3\nNparam = 2 \u00d7 rsalient + 1 \u00d7 (1 \u2212 rsalient),\nNstoring = 1 +\n1\nbsize\n,\n(14)\nwhere rsalient signifies the proportion of salient weights and\nbsize denotes the block size in OBC compensation, with 1\nbit allocated for marking the division of non-salient weights.\n1\nbsize represents the identifier for the structured column of\nsalient weights. For example, a 10% structural selection\nalong with an OBC compensation of size 128 was employed.\nThis results in a weight parameter bit-width of 1.1 bits and a\nhardware flag bit-width of 1.008 bits. Figure 6 illustrates the\nweight overhead for different proportions and block sizes.\nIt is important to note that flag weights do not participate\nin the computation; actual calculations are executed solely\nwith parameter weights. Therefore, additional hardware\nidentification bits do not affect the acceleration effect of\nbinary quantization.\nAlgorithm 1 Main Framework of BiLLM: Inner details of\neach function are shown in Algorithm 2\nfunc BinaryLLM(W, X, \u03b2, \u03bb)\nInput: W \u2208 Rn\u00d7m - weight matrix\nX \u2208 Rr\u00d7d - calibration data\n\u03b2 - block size\n\u03bb - hessian regularizer\nOutput: B - binarized weights\n1: H := 2XX\u22a4 // \u21132 error hessian matrix\n2: Hc := Cholesky((H + \u03bbI)\u22121)\n3: B := 0n\u00d7m\n4: for b = 0, \u03b2, 2\u03b2, ..., N do\n5:\nWb := W:,b:b+\u03b2\n6:\nrows{\u00b7} := salient(W:,b:b+\u03b2, Hc)\n7:\n\u02dcB1 := res approximation(Wb\n:,j\u2208{rows})\n8:\np\u2217 := seg search(Wb\ni,j /\u2208{rows})\n9:\n\u02dcB2 := binary(Wb\n|wi,j|\u2264p\u2217,j /\u2208{rows})\n10:\n\u02dcB3 := binary(Wb\n|wi,j|>p\u2217,j /\u2208{rows})\n11:\nB:,b:b+\u03b2 := \u02dcB1 + \u02dcB2 + \u02dcB3\n12:\nE := (W:,b:b+\u03b2 \u2212 B:,b:b+\u03b2)/Hc\nbb:b+\u03b2b+\u03b2\n13:\nW:,b+\u03b2: := W:,b+\u03b2:\u2212E\u00b7Hc\nb:b+\u03b2,b+\u03b2: // block-wise\nOBC\n14: end for\n15: return B\n4. Experiments\n4.1. Setup\nWe deploy BiLLM within the Pytorch (Paszke et al., 2019)-\nHuggingface libraries (Wolf et al., 2019). All the binariza-\ntion processes and experiments are conducted on a single 80\nGB NVIDIA A100. Given that BiLLM is an efficient PTQ\nframework, it eliminates the need for any fine-tuning, allow-\ning for completion through a single quantization process.\nModels and Datasets. We facilitate our method on the\nOPT (Zhang et al., 2022) and LLaMA (Touvron et al.,\n2023a;b) families. Additionally, considering the custom-\nary need for instruction-based fine-tuning of LLMs to adapt\nto varying contexts, we also conducted experiments on Vi-\ncuna (Chiang et al., 2023). In terms of evaluation metrics,\nwe mainly focused on the perplexity of LLMs\u2019 outputs,\n6\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\nTable 2. Perplexity of RTN, GPTQ, PB-LLM, and BiLLM on OPT Family. The columns represent the perplexity results on Wikitext2\ndatasets with different model sizes.\nMethod\nBlock\nSize\nWeight\nBits\n1.3B\n2.7B\n6.7B\n13B\n30B\n66B\nFull Precision\n-\n16.00\n14.62\n12.47\n10.86\n10.13\n9.56\n9.34\nRTN\n-\n3.00\n13337.38\n15594.72\n5797.32\n3357.01\n1566.00\n6126.09\nGPTQ\n128\n3.00\n20.97\n16.88\n14.86\n11.61\n10.27\n10.51\nRTN\n-\n2.00\n11272.65\n9505.76\n28363.14\n194086.78\n169616.47\n1165864.25\nGPTQ\n128\n2.00\n115.17\n61.59\n50.19\n21.36\n15.71\n82.10\nRTN\n-\n1.00\n17165.72\n36516.69\n11550.91\n6986.35\n6485.99\n184796.30\nGPTQ\n128\n1.00\n14884.73\n14144.58\n10622.81\n15196.96\n12478.37\n13106.45\nPB-LLM \u2020\n128\n1.70\n265.52\n124.35\n105.16\n81.92\n25.14\n29.09\nBiLLM \u2021\n128\n1.11\n69.97\n49.55\n35.36\n18.82\n12.71\n12.06\n-: Vanilla RTN conducts layer-wise quantization. \u2020: PB-LLM selects 10% elements in the original tensor as salient weights based on\nHessian. \u2021: BiLLM uses structural searching for salient weights. The table gives the average bit-width of the OPT family.\nTable 3. Perplexity of RTN, GPTQ, PB-LLM, BiLLM on LLaMA Family. The columns represent the perplexity results on Wikitext2\ndatasets with different model sizes.\nModel\nMethod\nBlock\nSize\nWeight\nBits\n7B\n13B\n30B\n65B/70B*\nFull Precision\n-\n16.00\n5.68\n5.09\n4.10\n3.53\nRTN\n-\n2.00\n106767.34\n57409.93\n26704.36\n19832.87\nGPTQ\n128\n2.00\n152.31\n20.44\n13.01\n8.78\nLLaMA\nRTN\n-\n1.00\n168388.00\n1412020.25\n14681.76\n65253.24\nGPTQ\n128\n1.00\n267001.72\n113894.12\n67093.73\n25082.88\nPB-LLM \u2020\n128\n1.70\n102.36\n36.60\n33.67\n12.53\nBiLLM \u2021\n128\n1.09\n35.04\n15.14\n10.52\n8.49\nFull Precision\n-\n16.00\n5.47\n4.88\nN/A\n3.32\nRTN\n-\n2.00\n17788.93\n51145.61\nN/A\n26066.13\nGPTQ\n128\n2.00\n60.45\n19.70\nN/A\n9.12\nLLaMA2\nRTN\n-\n1.00\n157058.34\n47902.32\nN/A\n160389.91\nGPTQ\n128\n1.00\n115905.67\n9387.80\nN/A\n74395.42\nPB-LLM \u2020\n128\n1.70\n69.20\n151.09\nN/A\n28.37\nBiLLM \u2021\n128\n1.08\n32.48\n16.77\nN/A\n8.41\nThe table gives the average bit-width of the LLaMA family. N/A: LLaMA2 do not have 30B version. *: LLaMA has 65B version and\nLLaMA2 has 70B version.\nwhich is widely acknowledged in prior studies as a challeng-\ning yet stable indicator of LLM capabilities, particularly\napt for network compression (Yao et al.; Frantar et al.,\n2022; Frantar & Alistarh, 2023; Xiao et al., 2023). We con-\nsider the test of WikiText2 (Merity et al., 2016), PTB (Mar-\ncus et al., 1994), as well as a part of the C4 (Raffel et al.,\n2020) data. Then, we further conduct the experiments on\nseven zero-shot evaluation tasks (PIQA (Bisk et al., 2020),\nBoolQ (Clark et al., 2019), OBQA (Mihaylov et al., 2018),\nWinogrande (Sakaguchi et al., 2021), ARC-e (Clark et al.,\n2018), ARC-c (Clark et al., 2018) Hellaswag (Zellers et al.,\n2019)) in the Appendix D, further verifying the robustness\nof our proposed BiLLM to the binarization of LLMs.\nBaseline. Our primary baseline is PB-LLM (Shang et al.,\n2023), the most recent PTQ approach on binary LLMs.\nGPTQ (Frantar et al., 2022) and vanilla RTN are also se-\nlected. GPTQ is currently the advanced technology in PTQ,\nand many works(Lin et al., 2023; Dettmers et al., 2023b;\nShang et al., 2023) choose it as the baseline. Other methods\noriented towards 8-bit and 4-bit quantization are deemed\nunsuitable for binarization and were thus not considered.\n4.2. Results\nComparison results. We conduct a meticulous compar-\nison of the binary performance of different LLMs across\nvarious model sizes. We deploy the BiLLM on the OPT\nmodels (Zhang et al., 2022) under the condition of a block\nsize equal to 128. As seen in Table 2, the model outputs\nunder the RTN and GPTQ methods have already collapsed\nat 1-bit weights, whereas BiLLM still maintains reasonable\nlinguistic output capabilities with an average weight of 1.1\nbits. In comparison with PB-LLM at 1.7 bits, our method\nachieves a 35% reduction in weight bit-width while enhanc-\ning the performance of different sizes of the OPT model by\n49.4% to 77.0%. It is noteworthy that when the parameter\nsize exceeds 30B, BiLLM can achieve performance nearly\nequivalent to that of GPTQ with 3-bit quantization.\nDue to the exceptional performance of the LLaMA (Touvron\net al., 2023a;b) series, they have become the foundation for\nmany open-source models (Chiang et al., 2023). Then, in\nTable 3, we evaluate the perplexity of outputs from the\nLLaMA series models using different methods. It can be\nobserved that, even at ultra-low weight bit-width, BiLLM\n7\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\n9564.53\n43.24\n5361.30\n80.15\n3877.38\n40.52\nptb\nc4\nLLaMA-2-7B\nGPTQ-2bits\nPB-LLM-1.7bits\nBiLLM-1.1bits\n80.43\n40.47\n193.95\n89.85\n73.63\n43.16\nptb\nc4\nOPT-6.7B\nGPTQ-2bits\nPB-LLM-1.7bits\nBiLLM-1.1bits\n2020.51\n101.3\n891.15\n100.38\n421.27\n39.59\nptb\nc4\nLLaMA-7B\nGPTQ-2bits\nPB-LLM-1.7bits\nBiLLM-1.1bits\nFigure 7. GPTQ, PB-LLM, BiLLM performed on the PTB and c4 datasets, mainly on LLaMA-7B, LLaMA2-7B, and OPT-6.7B, and we\nfound that BiLLM performed relatively well.\nTable 4. Perplexity of BiLLM on Vicuna-7B and Vicuna-13B. The\ncolumns of different models represent the perplexity results on\nWikitext2, PTB, and C4 datasets. The block size is set to 128.\nModel\nMethod\nWeight\nBits\nWiki\n-text2 \u2193\nPTB \u2193\nC4 \u2193\nGPTQ\n2.00\n109.56\n6227.73\n64.28\nVicuna-7B\nPB-LLM\n1.70\n68.01\n477.52\n67.23\nBiLLM\n1.08\n33.00\n332.17\n36.24\nGPTQ\n2.00\n41.75\n465.94\n40.57\nVicuna-13B\nPB-LLM\n1.70\n362.17\n772.44\n346.16\nBiLLM\n1.08\n36.57\n300.31\n28.76\nconsistently outperforms the 2-bit RTN and GPTQ methods.\nAnd 1.08 bits BiLLM for LLaMA-65B and LLaMA2-70B\neven surpasses the output of the full-precision OPT-66B\nmodel, which demonstrates the further binary potential of\nthe LLaMA family. We extend perplexity evaluation to the\nPTB and C4 datasets. Figure 7 illustrates the performance\nof the 7B parameter LLaMA series as well as the 6.7B\nOPT models. BiLLM continues to achieve a leading edge in\nperformance compared to other methods (more additional\ncomparisons are discussed in Appendix D).\nExperiments of instruction-tuned models. Instruction\nfine-tuning can significantly improve the application capa-\nbilities of the model and has become a necessary process for\nLLMs deployment in different scenarios (Wei et al., 2021;\nSanh et al., 2021; Chiang et al., 2023). We also deployed\nBiLLM on the recently popular fine-tuning instruction model\nVicuna for benchmark testing. As shown in Table 4, the\nperplexity performance of GPTQ and PB-LLM are com-\npared on Vicuna-7B and Vicuna-13B with three evaluations.\nBiLLM can achieve better performance at an average weight\nbit of 1.08, which further proves that BiLLM\u2019s universal\nLLMs binarization potential. We also provide dialogue\nexamples of binary models in Appeandix F.\nZero-Shot results. To conduct a more comprehensive eval-\nuation of binary LLMs, we extend our experiments to 7\nzero-shot datasets. Appendix D provides detailed results of\nour approach compared to previous methods in ultra-low bit\nquantization, further showing the outlier of BiLLM.\nAblation results. BiLLM enhances binarization precision\nthrough two primary methods: structured salient binariza-\ntion via residual approximation, and non-salient weight bina-\nrization via optimal splitting. To examine the effects of these\n1\n10\n100\n1000\n10000\n100000\n1000000\nwikitext2\nptb\nc4\nPerplexity \nLLaMA-7B\nRTN\nSalient-only\nSplitting-only\nBoth-BiLLM\n1\n10\n100\n1000\n10000\n100000\nwikitext2\nptb\nc4\nPerplexity \nOPT-6.7B\nRTN\nSalient-only\nSplitting-only\nBoth-BiLLM\nFigure 8. Ablation results of salient-only and splitting-only meth-\nods on OPT and LLaMA.\nstrategies, we conducted decomposition experiments. As\nshown in Figure 8, both approaches significantly improve\nbinary performance. Notably, we found that OPT-6.7B\nexhibits greater sensitivity to the splitting of non-salient\nweights (the blue line is lower than the green line), whereas\nLLaMA-7B is more responsive to salient weights\u2019 residual\napproximation (the green line is lower than the blue line).\nThis further indicates that different LLMs exhibit varying\nresponses to distinct binarization optimization strategies,\nshowing that the two binarization strategies proposed by\nBiLLM are efficient to various LLMs. We further discuss\ndetails on the block-size ablation results in Appendix E.\n5. Conclusions\nThis work proposed a novel post-training binary quantiza-\ntion method named BiLLM, specifically tailored for com-\npressing pre-trained LLMs. Inspired by the characteristics\nof weight\u2019s value and Hessian distributions, we adopted a bi-\nnary residual approximation for structurally salient weights\nto preserve their capabilities at ultra-low bits. For non-\nsalient weights, we employed optimal segmentation for\ngrouped binarization. Our results demonstrate that LLMs\ncan undergo a one-time weight quantization at ultra-low bits\nwithout substantial loss of precision. BiLLM has pioneered\nthe achievement of LLM performance guarantees at an av-\nerage bit rate close to 1 bit. We validated the binarization\nperformance of BiLLM across multiple open-source LLM\nfamilies and conducted generalization tests on a fine-tuned\ninstruction model. BiLLM advances the bit-width quantiza-\ntion frontier of LLMs, promising to facilitate the deployment\nof LLMs in edge scenarios and resource-constrained devices,\nand encourages further exploration in LLMs compression.\n8\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\n6. Impact Statements\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nBengio, Y., L\u00b4eonard, N., and Courville, A. Estimating or\npropagating gradients through stochastic neurons for con-\nditional computation. arXiv preprint arXiv:1308.3432,\n2013.\nBisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning\nabout physical commonsense in natural language. In Pro-\nceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp. 7432\u20137439, 2020.\nBlundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra,\nD. Weight uncertainty in neural network. In International\nconference on machine learning, pp. 1613\u20131622. PMLR,\n2015.\nChan, C.-Y. and Ioannidis, Y. E. Bitmap index design and\nevaluation. In Proceedings of the 1998 ACM SIGMOD\ninternational conference on Management of data, pp.\n355\u2013366, 1998.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\net al. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\n(accessed 14 April 2023), 2023.\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,\nM., and Toutanova, K. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044, 2019.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\nCourbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and\nBengio, Y. Binarized neural networks: Training deep\nneural networks with weights and activations constrained\nto+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.\nLlm. int8 (): 8-bit matrix multiplication for transformers\nat scale. arXiv preprint arXiv:2208.07339, 2022.\nDettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,\nL. Qlora: Efficient finetuning of quantized llms. arXiv\npreprint arXiv:2305.14314, 2023a.\nDettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev,\nD., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T.,\nand Alistarh, D. Spqr: A sparse-quantized representation\nfor near-lossless llm weight compression. arXiv preprint\narXiv:2306.03078, 2023b.\nDong, Z., Yao, Z., Gholami, A., Mahoney, M. W., and\nKeutzer, K. Hawq: Hessian aware quantization of neural\nnetworks with mixed-precision. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npp. 293\u2013302, 2019.\nFang, J., Shafiee, A., Abdel-Aziz, H., Thorsley, D., Geor-\ngiadis, G., and Hassoun, J. H. Post-training piecewise\nlinear quantization for deep neural networks. In Com-\nputer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part II\n16, pp. 69\u201386. Springer, 2020.\nFaraone, J., Fraser, N., Blott, M., and Leong, P. H. Syq:\nLearning symmetric quantization for efficient deep neu-\nral networks. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 4300\u2013\n4309, 2018.\nFrantar, E. and Alistarh, D. Optimal brain compression:\nA framework for accurate post-training quantization and\npruning. Advances in Neural Information Processing\nSystems, 35:4475\u20134488, 2022.\nFrantar, E. and Alistarh, D. Sparsegpt: Massive language\nmodels can be accurately pruned in one-shot. In Inter-\nnational Conference on Machine Learning, pp. 10323\u2013\n10337. PMLR, 2023.\nFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq:\nAccurate post-training quantization for generative pre-\ntrained transformers. arXiv preprint arXiv:2210.17323,\n2022.\nHelwegen, K., Widdicombe, J., Geiger, L., Liu, Z., Cheng,\nK.-T., and Nusselder, R. Latent weights do not exist:\nRethinking binarized neural network optimization. Ad-\nvances in neural information processing systems, 32,\n2019.\nJacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard,\nA., Adam, H., and Kalenichenko, D.\nQuantization\nand training of neural networks for efficient integer-\narithmetic-only inference. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 2704\u20132713, 2018.\nJain, S., Venkataramani, S., Srinivasan, V., Choi, J.,\nGopalakrishnan, K., and Chang, L. Biscaled-dnn: Quan-\ntizing long-tailed datastructures with two scale factors for\ndeep neural networks. In Proceedings of the 56th Annual\nDesign Automation Conference 2019, pp. 1\u20136, 2019.\n9\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\nLeCun, Y., Denker, J., and Solla, S. Optimal brain damage.\nAdvances in neural information processing systems, 2,\n1989.\nLee, C., Jin, J., Kim, T., Kim, H., and Park, E. Owq: Lessons\nlearned from activation outliers for weight quantization in\nlarge language models. arXiv preprint arXiv:2306.02272,\n2023.\nLi, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu,\nF., Wang, W., and Gu, S. Brecq: Pushing the limit of\npost-training quantization by block reconstruction. arXiv\npreprint arXiv:2102.05426, 2021.\nLi, Z., Ni, B., Zhang, W., Yang, X., and Gao, W. Perfor-\nmance guaranteed network acceleration via high-order\nresidual quantization. In Proceedings of the IEEE inter-\nnational conference on computer vision, pp. 2584\u20132592,\n2017.\nLin, J., Tang, J., Tang, H., Yang, S., Dang, X., and\nHan, S.\nAwq: Activation-aware weight quantization\nfor llm compression and acceleration. arXiv preprint\narXiv:2306.00978, 2023.\nLiu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad,\nY., Shi, Y., Krishnamoorthi, R., and Chandra, V. Llm-qat:\nData-free quantization aware training for large language\nmodels. arXiv preprint arXiv:2305.17888, 2023.\nMarcus, M., Kim, G., Marcinkiewicz, M. A., MacIntyre,\nR., Bies, A., Ferguson, M., Katz, K., and Schasberger,\nB. The penn treebank: Annotating predicate argument\nstructure. In Human Language Technology: Proceedings\nof a Workshop held at Plainsboro, New Jersey, March\n8-11, 1994, 1994.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R.\nPointer sentinel mixture models.\narXiv preprint\narXiv:1609.07843, 2016.\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can\na suit of armor conduct electricity?\na new dataset\nfor open book question answering.\narXiv preprint\narXiv:1809.02789, 2018.\nPark, E., Yoo, S., and Vajda, P. Value-aware quantization for\ntraining and inference of neural networks. In Proceedings\nof the European Conference on Computer Vision (ECCV),\npp. 580\u2013595, 2018.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. Pytorch: An imperative style, high-performance\ndeep learning library. Advances in neural information\nprocessing systems, 32, 2019.\nQin, H., Gong, R., Liu, X., Shen, M., Wei, Z., Yu, F., and\nSong, J. Forward and backward information retention for\naccurate binary neural networks. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pp. 2250\u20132259, 2020.\nQin, H., Ding, Y., Zhang, M., Yan, Q., Liu, A., Dang, Q.,\nLiu, Z., and Liu, X. Bibert: Accurate fully binarized bert.\narXiv preprint arXiv:2203.06390, 2022.\nQin, H., Zhang, M., Ding, Y., Li, A., Cai, Z., Liu, Z., Yu,\nF., and Liu, X. Bibench: Benchmarking and analyzing\nnetwork binarization. arXiv preprint arXiv:2301.11233,\n2023.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551, 2020.\nRastegari, M., Ordonez, V., Redmon, J., and Farhadi, A.\nXnor-net: Imagenet classification using binary convo-\nlutional neural networks. In European conference on\ncomputer vision, pp. 525\u2013542. Springer, 2016.\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: An adversarial winograd schema challenge\nat scale. Communications of the ACM, 64(9):99\u2013106,\n2021.\nSanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,\nAlyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja,\nA., et al. Multitask prompted training enables zero-shot\ntask generalization. arXiv preprint arXiv:2110.08207,\n2021.\nShang, Y., Yuan, Z., Wu, Q., and Dong, Z. Pb-llm: Par-\ntially binarized large language models. arXiv preprint\narXiv:2310.00034, 2023.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023b.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n10\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\nWang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L.,\nYang, F., Wang, R., Wu, Y., and Wei, F. Bitnet: Scaling 1-\nbit transformers for large language models. arXiv preprint\narXiv:2310.11453, 2023.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned lan-\nguage models are zero-shot learners.\narXiv preprint\narXiv:2109.01652, 2021.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,\nMoi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\net al. Huggingface\u2019s transformers: State-of-the-art natural\nlanguage processing. arXiv preprint arXiv:1910.03771,\n2019.\nXiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han,\nS. Smoothquant: Accurate and efficient post-training\nquantization for large language models. In International\nConference on Machine Learning, pp. 38087\u201338099.\nPMLR, 2023.\nYao, Z., Aminabadi, R., Zhang, M., Wu, X., Li, C., and He,\nY. Z. Efficient and affordable post-training quantization\nfor large-scale transformers, 2022. URL https://arxiv.\norg/abs/2206.01861.\nYao, Z., Li, C., Wu, X., Youn, S., and He, Y. A comprehen-\nsive study on post-training quantization for large language\nmodels. arXiv preprint arXiv:2303.08302, 2023.\nYou, Y. Audio coding: theory and applications. Springer\nScience & Business Media, 2010.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\nY. Hellaswag: Can a machine really finish your sentence?\narXiv preprint arXiv:1905.07830, 2019.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,\net al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\nZhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y.\nDorefa-net: Training low bitwidth convolutional neural\nnetworks with low bitwidth gradients. arXiv preprint\narXiv:1606.06160, 2016.\nZhu, X., Li, J., Liu, Y., Ma, C., and Wang, W. A survey\non model compression for large language models. arXiv\npreprint arXiv:2308.07633, 2023.\n11\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\nA. BiLLM Implementation\nAlgorithm 2 BiLLM: Detailed functions process\nfunc salient (W, Hc)\n1: S := W2/[Hc\nb:b+\u03b2b:b+\u03b2]2 // salient matrix\n2: rows{\u00b7} := topk(sum(abs(S)).(dim = 0))\n3: e = inf // searching error\n4: n\u2217 = 0 // optimal number of salient columns\n5: for i = 1, 2, ..., len(rows) do\n6:\nB1 := binary(W:,j, j\u2208rows[:i])\n7:\nB2 := binary(W:,j, j /\u2208rows[:i])\n8:\nif ||W \u2212 (B1 \u222a B2)||2 < e then\n9:\ne := ||W \u2212 (B1 \u222a B2)||2\n10:\nn\u2217 := i\n11:\nend if\n12: end for\n13: return rows{: n\u2217}\nfunc binary (W)\n1: \u03b1 := ||W||\u21131\nm\n2: B := \u03b1 \u00b7 sign(W)\n3: return B\nfunc res approximation (W)\n1: B1 := binary(W)\n2: R := W \u2212 B1\n3: B2 := binary(R)\n4: B := B1 + B2\n5: return B\nfunc seg search (W)\n1: e = inf // searching error\n2: p\u2217 = 0 // optimal break-point\n3: for i = 0.1, 0.2, 0.3, ..., 9 do\n4:\np := i \u00b7 max(abs(W))\n5:\nB1 := binary(W|wi,j|\u2264p)\n6:\nB2 := binary(W|wi,j|>p)\n7:\nif ||W \u2212 (B1 + B2)||2 < e then\n8:\ne := ||W \u2212 (B1 + B2)||2\n9:\np\u2217 := p\n10:\nend if\n11: end for\n12: return p\u2217\nBiLLM necessitates the structured selection of salient rows and their subsequent quantization through residual approximation\nbinarization. This is followed by dividing the non-salient weights, which exhibit a bell-shaped distribution, into a sparse area\nand a concentrated area. The division requires the optimization of the segmentation point p\u2217 by minimizing quantization\nloss. Ultimately, the two regions of non-salient weights are binarized separately to derive the final binary weights for LLMs.\nThe implementation details of the aforementioned function are enumerated in Algorithm 2.\nB. Quantization Error\nQuantization error definition for weight distribution The numerical range covered by the uniform quantizer spans from\n[Xmin, Xmax]. The number of intervals post-quantization, denoted as M, typically equals 2b, where b represents the target\nbit-width of quantization. So the quantization step size is:\n\u2206 = Xmax \u2212 Xmin\nM\n(15)\nThe boundaries can be calculated as:\nbq = Xmin + \u2206 \u00b7 l\n(16)\nwhere l \u2208 0, 1, ..., M, and we have bq \u2208 {\u2212\u03b1, 0, \u03b1} under binarization. Then we give the mean of each interval:\nxq = Xmin + \u2206 \u00b7 l \u2212 0.5\u2206\n(17)\nwhere l \u2208 1, ..., M. In this quantization scheme, we can get the MSQE from (You, 2010):\n\u03b82 =\nM\nX\nl=1\nZ Xmin+\u2206\u00b7l\nXmin+\u2206\u00b7(l\u22121)\n(Xmin + \u2206 \u00b7 l \u2212 0.5\u2206 \u2212 x)2g(x)dx\n(18)\nthen we let the y to replace the Xmin + \u2206 \u00b7 l \u2212 0.5\u2206 \u2212 x part, so the Equation (18) becomes:\n\u03b82 =\nM\nX\nl=1\nZ 0.5\u2206\n\u22120.5\u2206\ny2f[Xmin + \u2206 \u00b7 l \u2212 (y + 0.5\u2206)]2dx\n(19)\n12\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\nconsider the Equation (16) and Equation (17), the above equation becomes:\n\u03b82 =\nM\nX\nl=1\nZ 0.5\u2206\n\u22120.5\u2206\nx2f(xp \u2212 x)dx\n(20)\nThe aforementioned reasoning indicates that the MSQE of a uniform quantizer depends on the PDF and the quantization\nbit-width. Due to previous observations of the weights in pretrained LLMs, we have eliminated the salient weights. The\nremaining distribution of non-salient weights\u2019 g(x), is not uniform and resembles a Gaussian distribution. In binarization,\ntherefore, we substitute \u03b1 into Equation (18), resulting in:\n\u03b82\n=\nM\nX\nl=1\nZ (l\u22120.5M)\u2206\n(l\u22121\u22120.5M)\u2206\n[(l \u2212 0.5 \u2212 0.5M)\u2206 \u2212 x]2g(x)dx\n=\nZ 0\nXmin\n(\u2212\u03b1 \u2212 x)2g(x)dx +\nZ Xmax\n0\n(\u03b1 \u2212 x)2g(x)dx\n(21)\nC. Searching Curve of Salient Column and Non-salient Distribution\nQ\nK\nV\nOut\nFC1\nFC2\nFigure 9. Block-wise searching curve of salient columns in OPT-6.7B. The majority of the curves indicate that the minimal quantization\nerror can be achieved at the block level by considering only a few columns as salient. The Out Projection layer has a larger number of\nsalient columns, hence varying coverage for each block. The distribution in the FC layer is more dispersed. After optimal searching, the\noverall average weight bit is merely 1.1 bits.\nWe implemented a column-level segmentation and formulated a minimal-error column number search, as delineated in\nEquation (5). The identification of the optimal count of salient column groups commences with the column exhibiting the\nhighest salience. To mitigate the increase in bit-width resulting from residual approximation, we confined the search range\nto between 3 to 30 columns. Figure 9 illustrates the search curve pertinent to the inaugural Transformer block within the\nOPT6.7B model. It includes six layers of operators (Q, K, V, Out Projection, FC1, and FC2), with each layer showing\nthe search curves for the first five blocks. Figure 15 elucidates the clustering of salient weights, suggesting that a majority\nof the layers and blocks are capable of attaining minimal quantization errors with a limited number of salient columns.\nThe block-wise changes in weight distribution brought about by OBC (Frantar & Alistarh, 2022) introduce fluctuations\n13\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\nin the search curve; however, the structured selection still manages to encompass the majority of salient weights. In the\nFeedforward layer, where salient weight distribution is more scattered, the search curve leans towards employing residual\napproximation across an increased number of columns. Nonetheless, Table 1, displaying the average weight bit numbers\nacross various LLMs, confirms that this search strategy effectively maintains weight compression at approximately 1.1 bits.\nFigure 10 shows the unstructured search curve for the non-salient weights in the OPT6.7B model, with the same composition\nas that in Figure 9. The horizontal axis represents the ratio between p and the maximum weight value. Despite searching\non a block-wise basis, the search curve still exhibits convex properties, indicating the presence of an optimal p\u2217. This\nphenomenon demonstrates that the non-salient weights exhibit characteristics closely resembling an ideal Gaussian or\nLaplacian distribution (You, 2010; Fang et al., 2020).\nQ\nK\nV\nOut\nFC1\nFC2\nFigure 10. Block-wise splitting curve of bell-shaped distribution in OPT6.7B. The overall presentation exhibits the characteristics of a\nconvex function, fundamentally aligning with the theoretical optimal point in terms of theoretical basis.\nD. Multi-evaluation Comparisons\nPerplexity results on PTB and C4.\nWe use tables in the main text to show the perplexity of the three methods GPTQ, PB-LLM, and BiLLM on the Wikitext2\ndataset, and bar charts to show the perplexity results for LLaMA-7B, LLaMA2-7B, and OPT-6.7B on the PTB and C4\ndatasets. In the appendix, we show the quantitative comparison results for models of other sizes on the PTB and C4 datasets\nwith more images.\nIn Figure 11, we find that although different models have different perplexity results, they still roughly follow the law that\nthe larger the model, the lower the perplexity. BiLLM is generally still relatively better than the GPTQ and PB-LLM results\nin terms of perplexity with a lower bit-width configuration, while PB-LLM and GPTQ are higher or lower than each other,\nwith slightly inferior results at very low bits.\nZero-shot results\nFor completeness of testing, we have also tested and compared metrics such as the accuracy of GPTQ, PB-LLM, and BiLLM\non datasets such as PIQA and BoolQ, all using Zero Shot\u2019s experimental setup. From Table 5, We find that despite the loss\nin quantification, a side-by-side comparison between the three methods still shows BiLLM to be superior overall, testing one\n14\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\n83.23\n15.38\n141.09\n35.85\n47.54\n12.26\nptb\nc4\nLLaMA-30B\nGPTQ-2bits\nPB-LLM-1.7bits\nBiLLM-1.1bits\n20.86\n15.13\n35.05\n25.29\n21.24\n16.15\nptb\nc4\nOPT-30B\nGPTQ-2bits\nPB-LLM-1.7bits\nBiLLM-1.1bits\n163.72\n69.07\n278.52\n180.05\n103.82\n66.51\nptb\nc4\nOPT-1.3B\nGPTQ-2bits\nPB-LLM-1.7bits\nBiLLM-1.1bits\n87.22\n57.75\n143.93\n96.13\n76.99\n45.82\nptb\nc4\nOPT-2.7B\nGPTQ-2bits\nPB-LLM-1.7bits\nBiLLM-1.1bits\n63.9\n11.56\n66.8\n16.92\n46.99\n11.12\nptb\nc4\nLLaMA-65B\nGPTQ-2bits\nPB-LLM-1.7bits\nBiLLM-1.1bits\n109.49\n61.01\n45.24\n34.98\n18.57\n14.13\nptb\nc4\nOPT-66B\nGPTQ-2bits\nPB-LLM-1.7bits\nBiLLM-1.1bits\n470.34\n29.47\n719.25\n144.59\n332.09\n27.54\nptb\nc4\nLLaMA-2-13B\nGPTQ-2bits\nPB-LLM-1.7bits\nBiLLM-1.1bits\n27.16\n18.83\n110.47\n53.26\n26.98\n19.89\nptb\nc4\nOPT-13B\nGPTQ-2bits\nPB-LLM-1.7bits\nBiLLM-1.1bits\n196.7\n21.2\n213.17\n39.51\n85.32\n16.93\nptb\nc4\nLLaMA-13B\nGPTQ-2bits\nPB-LLM-1.7bits\nBiLLM-1.1bits\nFigure 11. GPTQ, PB-LLM, BiLLM performed on the PTB and C4 datasets, mainly on LLaMA-13B, LLaMA2-13B, OPT-13B, and so\non. The results showed that BiLLM performed relatively well.\nlevel higher on some datasets, while the effect of some random perturbations, although present, does not pull down BiLLM\u2019s\nperformance across the board. This suggests that BiLLM\u2019s quantization results have significantly improved performance at\nvery low bits, and further validates the conclusions.\nTable 5. Accuracy on 7 data sets, from binarization LLaMA, LLaMA2, and OPT, and we also compare the results among GPTQ, PB-LLM,\nand BiLLM to validate the quantization effect.\nModel\nMethod\nWeight\nBits\nBlock\nSize\nPIQA \u2191\nBoolQ \u2191\nOBQA \u2191\nWinogrande \u2191\nARC-e \u2191\nARC-c \u2191\nHellaswag \u2191\nGPTQ\n2.00\n128\n52.8\n50.0\n28.2\n49.3\n26.6\n29.5\n26.3\nLLaMA-7B\nPB-LLM\n1.70\n128\n54.6\n59.7\n30.4\n50.6\n28.2\n24.6\n28.7\nBiLLM\n1.09\n128\n61.2\n62.7\n31.8\n51.1\n36.0\n25.7\n36.8\nGPTQ\n2.00\n128\n51.1\n43.9\n29.0\n50.8\n26.6\n28.5\n26.3\nLLaMA2-7B\nPB-LLM\n1.70\n128\n53.8\n62.3\n30.2\n49.3\n28.0\n25.0\n27.7\nBiLLM\n1.08\n128\n60.6\n61.8\n33.2\n52.4\n36.2\n24.4\n34.8\nGPTQ\n2.00\n128\n56.6\n51.1\n25.6\n51.2\n31.3\n22.9\n30.4\nOPT-6.7B\nPB-LLM\n1.70\n128\n57.6\n55.5\n24.2\n47.7\n33.2\n21.0\n31.0\nBiLLM\n1.11\n128\n58.6\n62.2\n29.0\n51.5\n34.1\n23.9\n31.9\nE. Ablation of BiLLM with different block size\nTo explore the effect of different chunk sizes on the quantization effect of BiLLM, we set up block size settings including 32\ncolumns and 64 columns up to 512 columns and performed quantization experiments on them. The results show that the\noverall perplexity is lower as the chunk granularity becomes finer and the number of bits used becomes relatively smaller.\nWe believe this is because the smaller the chunks, the finer the data representation, and the more scale is used, but increasing\nthe diversity of quantization results also increases the weighting overhead. A block size of 128 can better balance the\nbit-width and quantization effect.\n15\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\nTable 6. Perplexity on Wikitext2, PTB, and C4 with different block size settings on BiLLM.\nModel\nBlock Size\nWikitext2\nPTB\nC4\n512\n74.14\n1078.90\n81.76\n256\n48.91\n574.34\n57.60\nLLaMA-7B\n128\n35.04\n421.27\n39.59\n64\n27.23\n399.81\n27.74\n32\n17.56\n263.39\n19.85\n512\n52.90\n267.82\n43.86\n256\n43.69\n232.34\n43.21\nLLaMA2-7B\n128\n32.48\n3877.38\n40.52\n64\n20.12\n830.36\n24.46\n32\n13.58\n440.40\n17.34\n512\n151.81\n257.22\n101.96\n256\n84.42\n116.44\n77.25\nOPT-6.7B\n128\n35.36\n73.63\n43.16\n64\n33.36\n48.16\n31.94\n32\n20.48\n31.02\n21.47\nF. Dialog Examples\nIn this section, we show some dialogue examples of binarized LLaMA-13B and Vicuna-13B.\nAlaska is a state of ___\nPrompt:\nmind & body aided with other person.\nPB-LLM-1.7bit:\nBiLLM-1.1bit: the US, it is situated in North America.\nYangtze and Yellow River locate in ___\nPrompt:\nPB-LLM-1.7bit: the lower land.\nBiLLM-1.1bit: the country which is called as the country of the China.\nAfter climbing thirty flights of stairs, he was drenched in \nsweat and didn't feel much like climbing. I advised him ___\nPrompt:\nhe is really not very health.\nPB-LLM-1.7bit:\nto get used now and the next day. After 2 days, he'd do a lot \nbetter and his sweat had lower.\nBiLLM-1.1bit:\nLLaMA-13B\nWhat are the capitals of China, US and Russia?\nPrompt:\nA\u2019s, U\u2019sa and U's were. 2, U\u2019s with, A\u2019s, U\u2019s, U's: W, S\u2019A \nand U\u2019 with, U\u2019s, or US's, or U's: U's S \u2013 with\nPB-LLM-1.7bit:\nThe capital of China is Beijing, the capital of US is \nWashington, DC, and the capital of Russia is Moscow.\nBiLLM-1.1bit:\nI have a friend who lives in New York and I would like to \nvisit him. How would I go there from Paris?\nPrompt:\nPB-LLM-1.7bit: I would like it to go to Paris.\nYou can fly from Paris to New York, and there are many \nairlines that can take you there. You can also take a flight \nto the New York, but you will have to check the schedule, as \nit will depend.\nBiLLM-1.1bit:\nWhat should I do after failed in a math exam?\nPrompt:\nI failed to fail.\nPB-LLM-1.7bit:\nRetake math exams if you don\u2019t pass this year. Take more \ntime to study and work on your weak areas. Or you\nneed more support for the final, You can also seek outside \nhelp in the same subject.\nBiLLM-1.1bit:\nVicuna-13B\nFigure 12. Some examples of conversations. LLaMA-13B and Vicuna-13B are chosen to show the case of language supplementary\nand Q&A ability. And PB-LLM (int 8, 10%) is selected as the comparison. We color the text to show the reasonable or inappropriate\nresponses.\n16\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\nG. Magnitude and Hessian Distribution of LLMs\nFigure 2 displays the distribution characteristics of weights and Hessian in LLMs. In this section, we provide additional\nexamples to illustrate the bell-shaped distribution of weight values and the long-tailed distribution of Hessian weights.\nFigure 13 depicts the distributions of four linear layers in the first Transformer block of the OPT-1.3B model, while Figure 14\nshows the distributions of seven linear layers in the sixth block of the LLaMA-7B model. The selection of these specific\nblock positions is intended to demonstrate the universality of these distribution characteristics in LLMs.\nFigure 15 displays the distribution of sensitive weights across 5 Transformer blocks within the OPT-1.3B model. We present\nthe Hessian distribution results for both the attention and feedforward blocks, with the red portion indicating the top 10% of\nthe most significant weight distribution. We observed that the salient weights of Q, K, and V in the OPT family tend to\nconcentrate in some columns or rows. Moreover, we noticed that salient weights in the Out Projection layer of multi-head\nself-attention blocks are distinctly concentrated in specific columns, supporting our structured selection approach discussed\nin the main text. In contrast, the distribution of salient weights in the feedforward layers is more dispersed. Based on these\nobservations, we adopt a sensitivity-based structured search method to identify salient columns.\nFigure 13. Different layers weight density distribution (blue) and hessian density distribution (orange) of the 1st Transformer block of the\nOPT-1.3B model\n17\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\nFigure 14. Different layers weight density distribution (blue) and hessian density distribution (orange) of the 6th Transformer block of the\nLLaMA-7B model\n18\nBiLLM: Pushing the Limit of Post-Training Quantization for LLMs\nFigure 15. Distribution of top 10% salient elements in Hessian matrix. The distribution of 1st \u2212 5th Transformer blocks in OPT-1.3B\n19\n"
  },
  {
    "title": "ScreenAI: A Vision-Language Model for UI and Infographics Understanding",
    "link": "https://arxiv.org/pdf/2402.04615.pdf",
    "upvote": "25",
    "text": "ScreenAI: A Vision-Language Model for UI and Infographics Understanding\nGilles Baechler\u2217 , Srinivas Sunkara\u2217 , Maria Wang\u2217 , Fedir Zubach , Hassan Mansoor ,\nVincent Etter , Victor C\u02d8arbune , Jason Lin , Jindong Chen\u2217\u2020 , Abhanshu Sharma\u2020\nGoogle Research\nAbstract\nScreen user interfaces (UIs) and infographics, shar-\ning similar visual language and design princi-\nples, play important roles in human communica-\ntion and human-machine interaction. We introduce\nScreenAI, a vision-language model that specializes\nin UI and infographics understanding. Our model\nimproves upon the PaLI architecture with the flexi-\nble patching strategy of pix2struct and is trained on\na unique mixture of datasets. At the heart of this\nmixture is a novel screen annotation task in which\nthe model has to identify the type and location of\nUI elements. We use these text annotations to de-\nscribe screens to Large Language Models and au-\ntomatically generate question-answering (QA), UI\nnavigation, and summarization training datasets at\nscale. We run ablation studies to demonstrate the\nimpact of these design choices.\nAt only 5B pa-\nrameters, ScreenAI achieves new state-of-the-art\nresults on UI- and infographics-based tasks (Multi-\npage DocVQA, WebSRC, MoTIF and Widget Cap-\ntioning), and new best-in-class performance on oth-\ners (Chart QA, DocVQA, and InfographicVQA)\ncompared to models of similar size. Finally, we re-\nlease three new datasets: one focused on the screen\nannotation task and two others focused on question\nanswering.\n1\nIntroduction\nInfographics, such as charts, diagrams, illustrations, maps,\ntables, and document layouts have long been a cornerstone\nof effective communication, thanks to their ability to distill\ncomplex data and ideas into simple illustrations through ar-\nrangement of layouts, and visual cues. In the digital era, mo-\nbile and desktop UIs, sharing similar design principles and\nvisual languages with infographics, facilitate human commu-\nnication and human-machine interface with rich and interac-\ntive user experiences.\nAlthough the above observation suggests an opportunity\nfor a unified model, because of their complexity, infographics\n\u2217Equal contribution. Correspondence: jdchen@google.com\n\u2020Project leads\nand UIs present a unique challenge to building a single model\nthat can understand, reason, and interact on top of pictorial\npixels. To address this challenge, we introduce ScreenAI, a\nVision-Language Model (VLM) for comprehensive UI and\ninfographics understanding, including tasks such as question-\nanswering (QA) on infographics (charts, illustrations, maps,\netc.) and element annotation, summarization, navigation, and\nQA on UIs.\nOur model combines the PaLI [Chen et al.,\n2023b] architecture with the flexible patching mechanism of\nPix2struct [Lee et al., 2023] and handles vision tasks by re-\ncasting them as (text, image)-to-text problems. Figure 1 pro-\nvides a high level description of the model architecture and\nSection 2.1 describes its components in more detail.\nThe main contributions of this work are multifold and\ngreatly advance the field of digital content understanding:\n\u2022 We propose ScreenAI, a Vision-Language Model\n(VLM), as a holistic solution that focuses on understand-\ning UIs and infographics, taking advantage of their com-\nmon visual language and design sophistication.\n\u2022 We introduce a textual representation for UIs, which we\nuse to teach our model how to understand UIs during its\npretraining phase.\n\u2022 We take advantage of this new UI representation and\nLarge Language Models (LLMs) to automatically gen-\nerate training data at scale.\n\u2022 We define pretraining and fine-tuning mixtures which\ncover a wide spectrum of tasks in UI and infographic\nunderstanding.\n\u2022 We release three evaluation datasets for tasks described\nin Section 4.2: Screen Annotation, ScreenQA Short, and\nComplex ScreenQA. These datasets enable the research\ncommunity to utilize our textual representation and al-\nlow for a more comprehensive benchmarking of models\nfor screen-based question answering.\nThese innovations position ScreenAI as the go-to VLM for\nany digital content understanding task, ranging from UIs to\ninfographics, and beyond. At a modest size of 4.6 billion pa-\nrameters, our model exhibits state-of-the-art (SoTA) perfor-\nmance on four public infographics QA benchmarks, surpass-\ning other models 10x or more in size. In other tasks, ScreenAI\nexhibits best-in-class, or close-to-best performance. We show\nin Section 5.2 that the model performance gets better as we\narXiv:2402.04615v2  [cs.CV]  19 Feb 2024\npix2struct \npatching\nAspect ratio preserving grid \nwith max e.g 25 patches\n5x5\n4x6\nVision \nEncoder \n(ViT)\nScreen\nT5 Decoder\nSelf-attn\nCross-attn + \nFFW\nx N\nT5 Multimodal \nEncoder\nSelf-attn + \nFFW\nx N\nembed + \nconcat\nK, V\nText input\n\u2018Question: What is \nthe text in the \nsearch bar\u2019?\nModel predictions\n\u2018K12 Schools Tulsa \nArea\u2019\nFigure 1: The overall architecture of our model. The model contains an image encoder followed by a multimodal encoder consuming\nembedded text and image features. The output of the multimodal encoder is fed to an autoregressive decoder to generate the final text output.\nThis figure also illustrates pix2struct patching, where the grid size adapts to the aspect ratio and shape of the image.\nincrease its size, suggesting that there is a strong potential for\nfurther gains in performance by scaling up the model.\n1.1\nRelated work\nWe identify three categories of closely related works.\nScreen-based UI models\nUntil recently, most screen un-\nderstanding efforts focused on well-defined tasks with a nar-\nrow scope. Examples include the detection of icons [Zang\net al., 2021] or various UI elements [Zhang et al., 2021;\nSunkara et al., 2022; Li et al., 2022a], together with their\nstructure [Wu et al., 2021]. Other notable works encompass\nthe description of icons (widget captioning) [Li et al., 2020],\nscreen summarization [Wang et al., 2021], and single-step\nnavigation tasks [Wichers et al., 2018; Li et al., 2022b]. An-\nother direction is to use LLMs to classify and describe UI\nelements [Gur et al., 2022], or complete tasks [Nakano et al.,\n2021; Rawles et al., 2023; Deng et al., 2023].\nGeneralist Foundation Models\nThe advent of large foun-\ndation models, particularly in the multimodal domain, has led\nto the development of versatile and unified models. These\nuniversal models excel in a broad spectrum of image un-\nderstanding tasks formulated through natural language such\nas question-answering, image captioning, and object local-\nization.\n(e.g.\nUniTAB [Yang et al., 2022], OFA [Wang\net al., 2022], PaLI [Chen et al., 2022; Chen et al., 2023a;\nChen et al., 2023b], Flamingo [Alayrac et al., 2022], or\nMaMMUT [Kuo et al., 2023]). Foundational work also in-\ncludes pix2seq [Chen et al., 2021a], which recasts the object\ndetection problem as a text prediction task.\nEfficient Vision-Language Models\nCloser to the domain\nof screen and document understanding, similar transformer-\nbased [Vaswani et al., 2017] architectures have been proposed\nfor solving various document-understanding tasks (e.g. Lay-\noutLMv3 [Huang et al., 2022], Donut [Kim et al., 2021],\npix2struct [Lee et al., 2023], MatCha [Liu et al., 2022],\nUDOP [Tang et al., 2023], or Spotlight [Li and Li, 2022]).\nAnother example is VuT [Li et al., 2021], which is made of\na multimodal encoder, followed by a text decoder and a ded-\nicated head for object detection tasks.\nCombined with UI and view hierarchy representations,\nLLMs like UIBert [Bai et al., 2021] or DocLLM [Wang et\nal., 2023] perform screen- and document-understanding us-\ning only textual data extracted from metadata like DOM or\nancillary models like OCR.\nIn our paper, we introduce pre-training tasks along with\na data generation schema using self-supervision and model-\nbased annotation. Prior work with self-supervised learning\ntasks have typically been focused on one domain. For ex-\namples, pix2struct [Lee et al., 2023], HTLM [Aghajanyan\net al., 2021] are focused on web-pages; ActionBert [He et\nal., 2021], UIBert [Bai et al., 2021] are focused on mobile\napps, which can capture a subset of the elements like text\nand exclude hierarchy information. Our representation, in-\nferred from only screen or image pixels, is applicable to a\nwide range of domains beyond web-pages and mobile apps,\nincluding documents, infographics, etc. Compared to prior\nwork, our model achieves superior performance on down-\nstream tasks. We hypothesize this is due to the positive trans-\nfer of performance when using screen, document and info-\ngraphics data jointly in the pre-training mixture. Given the\nabundance of data in each of these domains, we believe fu-\nture research into this direction can result in further improve-\nments.\n2\nMethodology\n2.1\nArchitecture\nOur model architecture as shown in Figure 1 is inspired by the\narchitecture of the PaLI family of models [Chen et al., 2022;\nChen et al., 2023a; Chen et al., 2023b], which is composed\nof a multimodal encoder block with a vision encoder like\nViT [Dosovitskiy et al., 2020] and a mT5 [Xue et al., 2020;\nRaffel et al., 2020] language encoder consuming image and\ntext inputs, followed by an autoregressive decoder. The input\nimage is transformed into a sequence of embeddings by the\nvision encoder and these embeddings are concatenated with\nthe input text embeddings and fed into the mT5 language en-\ncoder. The output of this encoder is passed to the decoder to\ngenerate the text output. This generic formulation enables us\nModel\nViT\nEncoder-Decoder\n#params\n670M\nB16 (92M)\nmT5 base (583M)\n675M\n2B\nH14 (653M)\nmT5 Large (1.23B)\n1.88B\n5B\nG14 (1.69B)\nUL2-3B (2.93B)\n4.62B\nTable 1: Model variants and details of their parameter counts (in\nparenthesis) and split among vision and language models. The im-\nage encoders are based on ViT [Dosovitskiy et al., 2020] and the text\nencoders are based on mT5 [Xue et al., 2020] and UL2 models [Tay\net al., 2022].\nto use the same model architecture to solve a variety of vi-\nsion and multimodal tasks that can be recast as a text+image\n(input) to text (output) problem. Compared to the text input,\nthe image embeddings constitute a significant portion of the\ninput length to the multimodal encoder.\nWe further extend PaLI\u2019s encoder-decoder architecture to\naccept various image patching patterns. The original PaLI\narchitecture only accepts a fixed grid pattern of patches for\nprocessing the input images. However, the data we encounter\nin screen-related domains spans a wide variety of resolutions\nand aspect ratios. To have a single model to work across all\nscreen shapes, it is necessary to use a patching strategy which\ncan work well with images of various shapes. To this end,\nwe borrow a technique introduced in Pix2Struct [Lee et al.,\n2023], which allows us to have image patches with arbitrary\ngrid shapes based on the input image shape and a pre-defined\nmaximum number of patches, as shown in Figure 1. This en-\nables us to accommodate input images of various formats and\naspect ratios without the need for padding or stretching the\nimage to a fixed shape, making our model more polyvalent to\nhandle both mobile (i.e. portrait) and desktop (i.e. landscape)\nimage formats. In Section 5, we evaluate the impact of each\nof these modeling choices.\n2.2\nModel configurations\nWe train models of 3 different sizes containing 670M, 2B\nand 5B parameters. For the 670M and 2B parameter models,\nwe start from pre-trained unimodal checkpoints for the vi-\nsion encoder and the encoder-decoder language models. For\nthe 5B parameter model, we start from the multimodal pre-\ntrained checkpoint from PaLI-3 [Chen et al., 2023a], where\nthe ViT is trained together with the UL2 [Tay et al., 2022]\nbased encoder-decoder language model. A breakdown of the\nparameter distribution among the vision and language models\ncan be seen in Table 1.\n2.3\nStages of Training\nIn this section, we cover the different stages of training.\nPre-training\nStarting from the checkpoints mentioned in\nSection 2.2, we do a first stage of training on large datasets\ngenerated from self-supervision and other models, using min-\nimal human labeling (see Section 4.1 for a detailed descrip-\ntion of the pre-training mixture). Contrary to the later fine-\ntuning stage, we train both the vision encoder and the lan-\nguage model. The motivation behind training the vision en-\ncoder is to incorporate the new patching strategy, and to allow\nthe model to adapt from natural images to UI-related images.\nWe evaluate the impact of training the vision encoder and of\nincluding LLM generated data on a variety of tasks in our\nablation experiments in Section 5.\nAfter some initial steps of pretraining, we perform addi-\ntional steps with the ViT encoder frozen to further train the\nmodel while reducing the resource consumption.\nFine-tuning\nDuring fine-tuning, the model is trained on\nmixtures of tasks, most of which are labeled using human an-\nnotators. These tasks are described in details in Section 4.2.\nFor QA-related tasks, we start by fine-tuning the model on a\ncombination of QA-related tasks; then, additional training is\nperformed on each individual tasks separately. For all other\ntasks, we fine-tune the model on each one individually.\n3\nAutomatic data generation\nThe pretraining phase of our model\u2019s development is criti-\ncally dependent on access to a vast and diverse dataset. Given\nthe impracticality of manually annotating such an extensive\ndataset, our strategy focuses on automatic data generation.\nThis approach leverages specialized smaller models, each\nadept at generating and labeling data both efficiently and with\na high degree of accuracy.\nIn this section, we provide a detailed account of our data\ngeneration process, particularly highlighting how we gather\nand automatically annotate a diverse range of screenshots for\npretraining our model. This automated approach is not only\nefficient and scalable compared to manual annotation but also\nensures a level of data diversity and complexity.\n3.1\nScreen Annotation\nOur initial step is to equip the model with a comprehensive\nunderstanding of textual elements, various screen compo-\nnents, and their overall structure and hierarchy. This founda-\ntional understanding is vital for the model\u2019s ability to interpret\nand interact accurately with a wide range of user interfaces.\nAn extensive collection of screenshots has been amassed\nfrom various devices, including desktops, mobile, and tablets,\nby crawling applications and web pages [Raffel et al., 2020].\nThese screenshots are then annotated with detailed labels that\ndescribe the UI elements, their spatial relationships, and ad-\nditional descriptive information.\nThe cornerstone of our annotation process is a layout an-\nnotator based on the DETR [Carion et al., 2020] detection\nmodel. This object detector is apt at identifying and labeling\na wide range of UI elements such as IMAGE, PICTOGRAM,\nBUTTON, TEXT, and others. This detector and the list of UI\nelements is inspired by [Li et al., 2022a]. The main differ-\nence is that the models in [Li et al., 2022a] are classifiers and\nare provided a list of candidate bounding boxes to annotate,\nwhereas in our case we predict the bounding boxes too.\nPictograms undergo further analysis using an icon classi-\nfier [Sunkara et al., 2022] capable of distinguishing 77 differ-\nent icon types. This detailed classification is essential for in-\nterpreting the subtle communication conveyed through icons.\nFor icons that are not covered by the classifier, infographics\nand images, we use the PaLI image captioning model [Chen\net al., 2023b]. This model generates descriptive captions that\nScreen schema \ngeneration\nLayout extraction\nIcon classification\nOCR\nImage captioning\nLLM\n(PaLM 2)\nIcon classification\n(Optional) validation\nLLM\nHuman\nGenerated Data \nmixture\nQuestion-Answering\nNavigation\nSummarization\nFigure 2: Task generation pipeline: 1) the screens are first annotated using various models; 2) we then use an LLMs to generate screen-related\ntasks at scale; 3) (optionally) we validate the data using another LLM or human raters.\nprovide contextual information, aiding in the comprehensive\nunderstanding of the screen\u2019s content.\nAdditionally, an OCR engine extracts and annotates tex-\ntual content on screen. This step is crucial for interpreting\nthe textual information presented in various formats on in-\nterfaces. Finally, we combine the OCR text with the previ-\nous annotations to create a detailed and holistic description\nof each screen. The bounding box coordinates are systemat-\nically included, providing spatial context to the elements on\nthe screen.\nA sample of this annotation, termed Screen Schema, is il-\nlustrated in Appendix B. This schema plays a central role\nin our data generation for pretraining tasks, offering a de-\ntailed and multifaceted representation of screen content. The\nschema itself also serves as a pretraining task, where the\nmodel is tasked with generating a similar schema from a pro-\nvided input image. This not only enhances the model\u2019s capac-\nity to discern and interpret various UI components but also\ntheir relationships to one another. Additionally, the screen\nschema proves to be an invaluable natural language tool to\ninterface with large language models (LLMs). By providing\nLLMs with a structured and detailed representation of screen\ncontent, we enable the creation of more intricate and contex-\ntually nuanced tasks.\n3.2\nLLMs to generate additional tasks\nTo infuse greater diversity into our pretraining data, we lever-\nage the capabilities of LLMs, in particular PaLM 2-S [Anil et\nal., 2023b] to generate Question-Answer pairs in two stages.\nInitially, we generate the screen schema as previously de-\nscribed. Subsequently, we craft a prompt incorporating the\nscreen schema and direct the LLM to generate synthetic data.\nThis stage is empirical and necessitates a degree of prompt\nengineering. However, after several iterations, we typically\nidentify a prompt that effectively generates the desired task.\nAn example of such a prompt is shown in Appendix C. To\nevaluate the quality of these generated responses, we con-\nducted human validation on a subset of the data, ensuring that\nit meets a predetermined quality threshold.\nThis approach is described in Figure 2 and it enables us\nto create a variety of synthetic but realistic tasks that sig-\nnificantly enhance the depth and breadth of our pretraining\ndataset. By leveraging the natural language processing capa-\nbilities of these models, coupled with the structured screen\nschema, we can simulate a wide range of user interactions\nand scenarios.\nTask Name\n#samples\nGenerated Screen Annotation\n353M\nGenerated Screen Question-Answering\n38.6M\nGenerated Screen Navigation\n15.9M\nGenerated Screen Summarization\n13.2M\nChart-to-table Translation\n2.4M\nTarzan/VQA CC3M/WebLI\n1M\nTable 2: Detailed breakdown of our pretraining mixture.\n4\nData mixtures\nWe define two distinct sets of tasks for our model: an initial\nseries of pretraining tasks and a subsequent set of fine-tuning\ntasks. The distinction between these two sets primarily lies in\ntwo aspects:\n1. Source of the groundtruth data: For the fine-tuning\ntasks, the labels are provided or verified by human raters.\nFor the pretraining tasks the labels are inferred using self\nsupervised learning methods or generated using other\nmodels.\n2. Size of the datasets: Typically, the pretraining tasks en-\ncompass a significantly larger quantity of samples, and\nconsequently, these tasks are used for training the model\nover a more extended series of steps.\n4.1\nPretraining mixture\nBased on the methodology outlined in Section 3, we have\nselected the following tasks for pretraining our models. These\ntasks, each illustrated in Figure 3, are designed to cover a\nwide range of skills and scenarios, endowing our model with\ndiverse real-world applications.\n1. Screen Annotation: The model is tasked with detecting\nand identifying UI elements present on a screen. This\nincludes performing OCR and image captioning to un-\nderstand and interpret the textual and non-textual con-\ntent. To enhance the model\u2019s contextual understanding,\nsome text elements are intentionally masked, encour-\naging the model to infer information based on the sur-\nrounding context and layout.\n2. Screen Question-Answering (QA): For this task, the\nmodel is asked to answer questions related to user in-\nterfaces and computer-generated images, such as info-\ngraphics. After initial experiments, we identified certain\nText input: What is \nthe name of the \ntailor?\nTarget: Andrew Ramroop\nText input: Describe \nthis screenshot.\nTarget: IMAGE \npleasure or love \nfollows truthfulness \nthen the merciful \nappears before him 0 \n993 0 261 (TEXT \npleasure of love, \nfollows \ntruthfulness, then \nthe Merciful appears \nbefore him 3 991 0 \n248), IMAGE a ma...\nText input: Select \nthe first item in \nthe list.\nTarget: click 15 \n983 199 359\nText input: Summarize \nthis screenshot.\nTarget: The screenshot \nshows a news article \nabout UConn men's \nbasketball recruiting. \nThe article is about \nDan Hurley's first \nrecruit of the 2021 \nclass, Rahsool \nDiggins, a 6'1\u2033 point \nguard from \nPhiladelphia.\n(a) Screen annotation\n(b) Question-Answering\n(c) Navigation\n(d) Summarization\nFigure 3: Sample of tasks that we are using in our pretraining mixture: (a) Screen annotation, with masking; (b) Question-Answering; (c)\nNavigation; (d) Summarization. The last three have been generated using our screen annotation model, coupled with PaLM-2-S.\ngaps in performance on attributes like arithmetic, count-\ning, understanding images with complex infographics.\nTo enhance the model capabilities, we create data specif-\nically addressing these gaps, e.g., QA involving count-\ning, arithmetic operations, and complex data containing\ninfographics. For these examples, we first crawl large\nscale webpage and infographic images, then perform\nprompt tuning to generate and validate relevant ques-\ntions and their answers. For better understanding chart\nexamples, we included a large number of examples con-\nsisting of chart-to-table translation, using a slightly mod-\nified version of the mixture in [Liu et al., 2023]. Similar\nto infographics, the model learns how to better represent\ninternally charts, having to predict their tabular repre-\nsentation solely from the images.\n3. Screen Navigation: This task involves interpreting nav-\nigation instructions (e.g., \u2018go back\u2019) and identifying the\nappropriate UI element to interact with. The expected\noutput is the bounding box coordinates of the target ele-\nment, bucketized between 0 and 999, demonstrating the\nmodel\u2019s ability to understand user intent and navigate\nthrough interfaces accurately.\n4. Screen Summarization: The model is tasked to suc-\ncinctly summarize the content of a screen in one or two\nsentences. This task assesses the model\u2019s capability to\ndistill and caption the essence of the screen\u2019s content.\nTo ensure comprehensive training robust to aspect ratios,\neach task is made available across multiple formats (mobile\nand desktop) and includes several aspect ratios.\nIn addition to these screen-related tasks, our training regi-\nmen also incorporates a variety of other image and text data\nsources: Tarzan span corruption [Xue et al., 2020], VQA\nCC3M [Sharma et al., 2018], WebLI Alt and OCR text [Kil\net al., 2023; Chen et al., 2022] and Chart-to-table transla-\ntion [Liu et al., 2023]. Such datasets have been instrumen-\ntal in the development of PaLI models [Chen et al., 2022;\nChen et al., 2023b], which serve as the foundational architec-\nture for our model. Their inclusion ensures that our model not\nonly excels in screen understanding but also maintains robust\nlanguage and visual processing capabilities.\nA summary of all our pretraining tasks is shown in Table 2.\nIn the mixture, datasets are weighted proportionally to their\nTask Name/Benchmark\nMetric\nScreen Analysis\nScreen Annotation [Ours, Sec. 4.2]\nF1@IoU=0.1\nWidget Captioning [Li et al., 2020]\nCIDEr\nScreen Question-Answering\nScreenQA Short [Ours, Sec. 4.2]\nSQuAD F1\nComplex ScreenQA [Ours, Sec. 4.2]\nSQuAD F1\nWebSRC [Chen et al., 2021b]\nSQuAD F1\nScreen Navigation\nRefExp [Bai et al., 2021]\nAcc@IoU=0.1\nMoTIF-Automation [Burns et al., 2022]\nAcc@IoU=0.1\nScreen Summarization\nScreen2Words [Wang et al., 2021]\nCIDEr\nInfographics/Doc Visual QAs\nChartQA [Masry et al., 2022]\nRelaxed Acc.\nDocVQA [Mathew et al., 2021]\nANLS\nMultipage DocVQA [Tito et al., 2023]\nANLS\nInfographicVQA [Mathew et al., 2022]\nANLS\nOCR-VQA-200K [Mishra et al., 2019]\nEM\nTable 3: Detailed breakdown of our fine-tuning mixture and their\nassociated metrics. Detailed description of the metrics can be found\nin Appendix A. For all metrics, higher is better.\nsize with a maximum allowed weight per task. Incorporating\nmultimodal sources in our multi-task training, from language\nprocessing to visual comprehension and web content analy-\nsis, prepares our model to handle diverse scenarios effectively\nand enhances its overall versatility and performance.\n4.2\nFine-tuning tasks and benchmarks\nWe use a variety of tasks and benchmarks during fine-tuning\nto estimate the quality of our model. These benchmarks are\nsummarized in Table 3 and include the main existing screen,\ninfographics and document understanding benchmarks. We\nmake the following changes to task formulations: (1) we cast\nRefExp [Wichers et al., 2018] and Task Automation in Mo-\nTIF [Burns et al., 2022] as object detection tasks, without us-\ning candidate bounding boxes and report accuracy at IoU=0.1\nconsidering only one box predicted; (2) for MoTIF, we report\nthe number for the app-unseen split of the test set in Table 4,\nSA\nRef\nExp\nSQA\nShort\nCplx\nSQA\nMoTIF\nScreen2\nWords\nWidget\nCapt.\nChart\nQA\nDoc\nVQA\nMPDoc\nVQA\nInfo\nVQA\nOCR\nVQA\nWeb\nSRC\nSoTA\n-\n-\n-\n-\n67.6a\n130.7b\n159.8b\n80.8h\n90.9h\n61.8d\n80.3h\n77.8b\n85.0f\nWithout OCR\nSoTA<5B\n-\n-\n-\n-\n67.6a\n130.7b\n159.8b\n70.1c\n87.8c\n-\n57.8b\n76.7b\n77.8g\nScreenAI\n81.9\n86.3\n94.6\n42.4\n87.4\n120.8\n167.3\n76.6\n87.5\n72.9\n61.4\n75.0\n87.2\nWith OCR\nSoTA<5B\n-\n-\n-\n-\n-\n-\n-\n70.4c\n89.3c\n61.8d\n62.4b\n77.8b\n85.0f\nScreenAI\n-\n-\n94.8\n43.5\n-\n123.7\n-\n76.7\n89.9\n77.1\n65.9\n76.2\n-\nTable 4: Comparison of ScreenAI with various SoTA models: (a) MoTIF [Burns et al., 2022], (b) PaLI-3 [Chen et al., 2023b], (c)\nSmoLA PaLI-X [Wu et al., 2023a], (d) Hi-VT5 [Tito et al., 2023], (e) TILT [Powalski et al., 2021], (f) DocPrompt [Wu et al., 2023b],\n(g) DUBLIN [Aggarwal et al., 2023], (h) Gemini [Anil et al., 2023a]. Bold font highlights SoTA score, and underscore represents best-in-\nclass score. See Table 3 for details about the tasks and their associated metrics.\nand other split results in Appendix G.\nWe supplement the tasks mentioned above with three new\nbenchmarks that we release:\n\u2022 Screen Annotation (SA): To evaluate our model\u2019s lay-\nout annotation and spatial understanding capabilities, we\ncreate a dedicated benchmark consisting of 4.2K screen-\nshots from the Rico dataset [Deka et al., 2017]. Each UI\nelement has been annotated by human raters, and the an-\nnotations comprise a bounding box and a UI class from\nthe list described in 3.1. We evaluate the model\u2019s predic-\ntions using object detection metrics, including F1 score,\nprecision and recall values computed at IoU=0.1.\n\u2022 ScreenQA Short (SQA Short): ScreenQA [Hsiao et\nal., 2022], a benchmark for screen understanding, con-\ntains UI elements and full-sentence answers as ground\ntruth. To align the output format with other question an-\nswering tasks, we generate a new ground truth, a list of\nalternative short answers, for each of the questions. We\nuse the maximum F1 score across all the candidate an-\nswers as the metric. See Appendix E for details.\n\u2022 Complex ScreenQA (Cplx SQA):\nTo complement\nSQA Short, we introduce Complex ScreenQA, which\nincludes more difficult questions (counting, arithmetic,\ncomparison, and non-answerable questions) and con-\ntains screens with various aspect ratios. We describe this\ndataset in detail in Appendix F.\nWe also provide a few additional details on how we handle\nMultipage DocVQA and ChartQA.\nMultipage DocVQA\nThe standard fine-tuning task for\nMultipage DocVQA [Tito et al., 2023] can be transformed\ninto a single-page DocVQA task by pairing the same ques-\ntion with each page of the document and choosing the answer\nwith the highest score among all pages. In this formulation,\nwe modify the training set by splitting a question, answer and\nmultipage document into a positive pair (with the actual an-\nswer for the page containing the answer) and multiple nega-\ntive pairs (with \u201cno answer\u201d for pages which do not contain\nthe answer). The negative pairs are subsampled to avoid over-\nfitting on not predicting an answer and the original DocVQA\ntask [Mathew et al., 2021] is added to the fine-tuning mixture.\nChartQA\nAnalyses showed that the original fine-tuning\ndataset [Masry et al., 2022] is insufficiently rich for the model\nto learn solving more complex reasoning tasks. Therefore,\nwe extend it through synthetic examples. On the one hand,\nwe use the tabular representation of the charts and a prompt\nwith PaLM 2-S/L to synthesize additional 160K question/an-\nswer pairs On the other hand, we programmatically generate\n350K mathematical questions (mean, comparisons, median)\nusing templates.\n5\nExperiments and results\nIn this section, we present the setup we used to conduct our\nexperiments and analyze our findings. First, we compare the\nbest performing ScreenAI model to the SoTA on a variety of\nScreen and Infographics related tasks. Next, we report the\nimpact of model size on overall performance. Finally, we re-\nport results on ablation studies to validate the design choices\nmade for the models.\n5.1\nExperiments setup\nIn the fine-tuning phase, we hold the ViT encoder frozen and\nfine-tune the language model only. We use 512 as our batch\nsize for fine-tuning. Our input sequence length is 128 and\noutput sequence length varies depending on individual tasks.\nWhen fine-tuning with OCR as additional input, we increase\nthe input sequence length accordingly. We generally find that\nthe model converges within 30k steps. Unless specified oth-\nerwise, all experiments are run on the 5B model.\n5.2\nResults\nTable 4 shows the performance of our models and com-\npares them with state-of-the-art (SoTA) results on a vari-\nety of screen- and infographics-related tasks. We also in-\nclude the best results for models of similar size (SoTA<5B).\nWe report new SoTA results on MoTIF, Widget Captioning,\nMPDocVQA, and WebSRC; and new best-in-class results in\nChartQA, DocVQA and InfographicVQA (InfoVQA). We re-\nport same or competitive performance on Screen2Words and\nOCR-VQA. We also report our results on the baselines intro-\nduced in Section 4.2 (Screen Annotations, Referring Expres-\nsions, ScreenQA Short and Complex ScreenQA).\nScreen Annotation\nRef Exp\nSQA Short\nComplex SQA\nMoTIF\nScreen2Words\nChart QA\nDocVQA Infographics VQA OCR VQA\n0\n50\n100\nMetric value\n48.2\n77.4\n70.0\n28.4\n83.5\n97.4\n54.0\n50.7\n19.6\n54.8\n61.1\n83.9\n84.8\n29.4\n86.8\n99.9\n55.8\n59.3\n24.0\n62.8\n81.9\n86.3\n94.6\n43.5\n87.4\n120.8\n76.6\n87.5\n61.4\n76.2\n670M\n2B\n5B\nFigure 4: Performance of different model sizes on fine-tuning tasks. The metrics improve consistently as the model size increases.\n(0.0 - 0.25)\n[0.25 - 0.5)\n[0.5 - 0.75)\n[0.75 - 1.0)\n[1.0 - 1.33)\n[1.33 - 2.0)\n[2.0 - 4.0)\n[4.0 - inf)\nAspect ratio\n0.0\n0.5\n1.0\nAggregate score\n0.79\n1.14\n1.22\n1.19\n0.99\n0.69\n0.81\n0.88\n0.76\n1.10\n1.18\n1.19\n0.99\n0.87\n0.99\n0.98\nFixed Grid\nPix2struct\nFigure 5: Ablation study for Pix2Struct vs. fixed-grid patching; the\nnumbers represent the aggregated scores across all fine-tuned tasks.\nFor aspect ratio > 1.0, using Pix2Struct patching significantly out-\nperforms a fixed grid patching, whereas for aspect ratio < 1.0, a\nfixed grid patching outperforms Pix2Struct by a smaller margin.\nAdding OCR as additional input\nWe analyze the impact of adding OCR1 to the model input\nby conducting experiments with and without OCR. This is\ninspired by fine-tuning experiments in PaLI-X [Chen et al.,\n2023a] and PaLI-3 [Chen et al., 2023b], where across all\nscreen- and document-related tasks, passing OCR texts as\nadditional input improves task performance. In Table 4 we\npresent our single task fine-tuning results using OCR data.\nWe find that for QA tasks, adding OCR provides a boost\nin performance (e.g. up to 4.5% on Complex ScreenQA,\nMPDocVQA and InfoVQA). However, using OCR imposes\na slightly larger input length and hence results in slower over-\nall training. It also requires having OCR results available at\ninference time.\nModel size\nWe conducted single task experiments with the following\nmodel sizes: 670M, 2B and 5B. We use benchmarks for\nscreen tasks as well as other public tasks. In Figure 4, we\nobserve that across all tasks, increasing the model size im-\nproves performances and the improvements have not satu-\nrated at the largest size. We observe that for tasks that re-\nquire more complex visual-text and arithmetic reasoning e.g.\nInfoVQA, ChartQA, and Complex ScreenQA, the improve-\nment between 2B and 5B models is significantly larger than\nbetween 670M and 2B models.\n5.3\nAblation Studies\nIn this section, we perform ablation studies evaluating (1) the\nimpact of pix2struct patching and (2) using LLM generated\ndata for pre-training. All ablation studies are performed on\nthe 670M parameter variant.\n1We use a proprietary OCR system similar to GCP Vision API to\nproduce additional OCR input for each image.\nImpact of pix2struct patching\nFor this study, we compare\na 670M model using pix2struct patching with another using\nfixed-grid patching. After pre-training, both models are fine-\ntuned on all tasks mentioned in Table 3. We split each dataset\ninto subsets based on the image aspect ratio and compute the\nrespective metric on these subsets. To compare fixed-grid\npatching to a variable pix2struct patching, we compute an\naggregate score, by first dividing the score of each task sub-\nset using fixed-grid patching by the score of the model using\npix2struct on the entire task, and finally compute the geomet-\nric mean across all tasks. Figure 5 shows that for images with\naspect ratio > 1.0 (landscape mode images), the pix2struct\npatching strategy is significantly better than the fixed grid\npatching. For portrait mode images, the trend is reversed,\nbut fixed grid patching is only marginally better. Given that\nwe want the ScreenAI model to be used across images of dif-\nferent aspect ratios, we choose to use pix2struct patching.\nImpact of LLM generated data\nFor this experiment, we\ncompare a 670M ScreenAI model pre-trained using all the\ndatasets mentioned in Section 4.1 against a model pre-trained\non a mixture excluding any LLM generated pre-training data.\nAfter pre-training, both models are fine-tuned on all tasks\nmentioned in Table 3 and an aggregate score is computed.\nWe observe that adding LLM generated data to the mixture\nimproves the aggregate score by 4.6 percentage points.\n6\nConclusion\nIn this work, we introduce the ScreenAI model along with\na new unified schema for representing complex data and vi-\nsual information, compatible with infographics, document\nimages, and various UIs. This unified representation enables\nthe design of a mixture of self-supervised learning tasks,\nleveraging data from all these domains. We show that train-\ning on this mixture results in a positive transfer to screen-\nrelated tasks as well as infographics and document-related\ntasks. We also illustrate the impact of data generation us-\ning LLMs and justify our model design choices with ablation\nstudies. We apply these techniques to train a model that per-\nforms competitively and achieves SoTA on a number of pub-\nlic benchmarks. While our model is best-in-class, we note\nthat, on some tasks, further research is needed to bridge the\ngap with models like GPT-4 and Gemini, which are orders\nof magnitude larger. To encourage further research, we re-\nlease a dataset with this unified representation, as well as two\nother datasets to enable more comprehensive benchmarking\nof models on screen-related tasks.\n7\nAcknowledgement\nWe would like to thank Fangyu Liu,\nXi Chen,\nEfi\nKokiopoulou, Jesse Berent, Gabriel Barcik, Lukas Zilka, Ori-\nana Riva, Gang Li, Yang Li, Radu Soricut and Tania Bedrax-\nWeiss for their insightful feedbacks and fruitfull discussions,\nRahul Aralikatte, Hao Cheng and Daniel Kim for their whole-\nhearted and tireless support in data preparation, and Jay Yag-\nnik, Blaise Aguera y Arcas, Ewa Dominowska, David Petrou,\nand Matt Sharifi for their vision and support in leadership.\nReferences\n[Aggarwal et al., 2023] Kriti Aggarwal et al.\nDUBLIN\u2013\ndocument understanding by language-image network.\narXiv preprint arXiv:2305.14218, 2023.\n[Aghajanyan et al., 2021] Armen Aghajanyan et al. HTLM:\nHyper-text pre-training and prompting of language mod-\nels, 2021.\n[Alayrac et al., 2022] Jean-Baptiste Alayrac et al. Flamingo:\na visual language model for few-shot learning. Advances\nin Neural Information Processing Systems, 35:23716\u2013\n23736, 2022.\n[Anil et al., 2023a] Rohan Anil et al.\nGemini: a family\nof highly capable multimodal models.\narXiv preprint\narXiv:2312.11805, 2023.\n[Anil et al., 2023b] Rohan Anil et al. PaLM 2 technical re-\nport. arXiv preprint arXiv:2305.10403, 2023.\n[Bai et al., 2021] Chongyang Bai et al.\nUIBert: Learning\ngeneric multimodal representations for UI understanding,\n2021.\n[Burns et al., 2022] Andrea Burns et al. A dataset for inter-\nactive vision language navigation with unknown command\nfeasibility. In European Conference on Computer Vision\n(ECCV), 2022.\n[Carion et al., 2020] Nicolas Carion et al. End-to-end object\ndetection with transformers. In European conference on\ncomputer vision, pages 213\u2013229. Springer, 2020.\n[Chen et al., 2021a] Ting Chen et al. Pix2seq: A language\nmodeling framework for object detection. arXiv preprint\narXiv:2109.10852, 2021.\n[Chen et al., 2021b] Xingyu Chen et al. WebSRC: A dataset\nfor web-based structural reading comprehension, 2021.\n[Chen et al., 2022] Xi Chen et al.\nPaLi: A jointly-scaled\nmultilingual language-image model.\narXiv preprint\narXiv:2209.06794, 2022.\n[Chen et al., 2023a] Xi Chen et al. PaLI-X: On scaling up\na multilingual vision and language model. arXiv preprint\narXiv:2305.18565, 2023.\n[Chen et al., 2023b] Xi Chen et al.\nPaLI-3 vision lan-\nguage models: Smaller, faster, stronger. arXiv preprint\narXiv:2310.09199, 2023.\n[Deka et al., 2017] Biplab Deka et al. Rico: A mobile app\ndataset for building data-driven design applications.\nIn\nProceedings of the 30th annual ACM symposium on user\ninterface software and technology, pages 845\u2013854, 2017.\n[Deng et al., 2023] Xiang Deng et al.\nMind2web:\nTo-\nwards a generalist agent for the web.\narXiv preprint\narXiv:2306.06070, 2023.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy et al. An im-\nage is worth 16x16 words: Transformers for image recog-\nnition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[Gehrmann et al., 2022] Sebastian Gehrmann et al.\nTata:\nA multilingual table-to-text dataset for african languages,\n2022.\n[Gur et al., 2022] Izzeddin Gur et al.\nUnderstanding\nHTML with large language models.\narXiv preprint\narXiv:2210.03945, 2022.\n[He et al., 2021] Zecheng He et al. ActionBert: Leveraging\nuser actions for semantic understanding of user interfaces,\n2021.\n[Hsiao et al., 2022] Yu-Chung Hsiao et al.\nScreenQA:\nLarge-scale question-answer pairs over mobile app screen-\nshots. arXiv preprint arXiv:2209.08199, 2022.\n[Huang et al., 2022] Yupan Huang et al. LayoutLMv3: Pre-\ntraining for document ai with unified text and image mask-\ning. In Proceedings of the 30th ACM International Con-\nference on Multimedia, pages 4083\u20134091, 2022.\n[Kafle et al., 2018] Kushal Kafle et al. Dvqa: Understanding\ndata visualizations via question answering, 2018.\n[Kil et al., 2023] Jihyung Kil et al.\nPreSTU: Pre-training\nfor scene-text understanding.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 15270\u201315280, 2023.\n[Kim et al., 2021] Geewook Kim et al. Donut: Document\nunderstanding transformer without OCR. arXiv preprint\narXiv:2111.15664, 7:15, 2021.\n[Kuo et al., 2023] Weicheng Kuo et al. MaMMUT: A simple\narchitecture for joint learning for multimodal tasks. arXiv\npreprint arXiv:2303.16839, 2023.\n[Lee et al., 2023] Kenton Lee et al. Pix2struct: Screenshot\nparsing as pretraining for visual language understanding.\nIn International Conference on Machine Learning, pages\n18893\u201318912. PMLR, 2023.\n[Li and Li, 2022] Gang Li and Yang Li. Spotlight: Mobile\nUI understanding using vision-language models with a fo-\ncus. arXiv preprint arXiv:2209.14927, 2022.\n[Li et al., 2020] Yang Li et al. Widget captioning: Generat-\ning natural language description for mobile user interface\nelements, 2020.\n[Li et al., 2021] Yang Li et al. VUT: Versatile ui transformer\nfor multi-modal multi-task user interface modeling. arXiv\npreprint arXiv:2112.05692, 2021.\n[Li et al., 2022a] Gang Li et al. Learning to denoise raw mo-\nbile UI layouts for improving datasets at scale. In Pro-\nceedings of the 2022 CHI Conference on Human Factors\nin Computing Systems, pages 1\u201313, 2022.\n[Li et al., 2022b] Tao Li et al. MUG: Interactive multimodal\ngrounding on user interfaces, 2022.\n[Liu et al., 2022] Fangyu Liu et al. MatCha: Enhancing vi-\nsual language pretraining with math reasoning and chart\nderendering. arXiv preprint arXiv:2212.09662, 2022.\n[Liu et al., 2023] Fangyu Liu et al. DePlot: One-shot visual\nlanguage reasoning by plot-to-table translation, 2023.\n[Masry et al., 2022] Ahmed Masry et al. ChartQA: A bench-\nmark for question answering about charts with visual and\nlogical reasoning. arXiv preprint arXiv:2203.10244, 2022.\n[Masry et al., 2023] Ahmed Masry et al. Unichart: A uni-\nversal vision-language pretrained model for chart compre-\nhension and reasoning, 2023.\n[Mathew et al., 2021] Minesh Mathew et al.\nDocVQA: A\ndataset for VQA on document images. In Proceedings of\nthe IEEE/CVF winter conference on applications of com-\nputer vision, pages 2200\u20132209, 2021.\n[Mathew et al., 2022] Minesh Mathew et al.\nInfograph-\nicVQA.\nIn Proceedings of the IEEE/CVF Winter Con-\nference on Applications of Computer Vision, pages 1697\u2013\n1706, 2022.\n[Methani et al., 2020] Nitesh Methani et al. PlotQA: Rea-\nsoning over scientific plots, 2020.\n[Mishra et al., 2019] Anand Mishra et al. OCR-VQA: Visual\nquestion answering by reading text in images. In ICDAR,\n2019.\n[Nakano et al., 2021] Reiichiro Nakano et al.\nWebGPT:\nBrowser-assisted question-answering with human feed-\nback. arXiv preprint arXiv:2112.09332, 2021.\n[Powalski et al., 2021] Rafa\u0142 Powalski et al. Going full-tilt\nboogie on document understanding with text-image-layout\ntransformer, 2021.\n[Raffel et al., 2020] Colin Raffel et al. Exploring the limits\nof transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485\u2013\n5551, 2020.\n[Rajpurkar et al., 2016] Pranav Rajpurkar et al.\nSQuAD:\n100,000+ questions for machine comprehension of text,\n2016.\n[Rawles et al., 2023] Christopher Rawles et al. Android in\nthe wild: A large-scale dataset for android device control.\narXiv preprint arXiv:2307.10088, 2023.\n[Sharma et al., 2018] Piyush Sharma et al. Conceptual cap-\ntions: A cleaned, hypernymed, image alt-text dataset for\nautomatic image captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 2556\u20132565,\n2018.\n[Sunkara et al., 2022] Srinivas Sunkara et al. Towards bet-\nter semantic understanding of mobile interfaces.\narXiv\npreprint arXiv:2210.02663, 2022.\n[Tang et al., 2023] Zineng Tang et al. Unifying vision, text,\nand layout for universal document processing. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 19254\u201319264, 2023.\n[Tay et al., 2022] Yi Tay et al.\nUL2: Unifying language\nlearning paradigms. In The Eleventh International Con-\nference on Learning Representations, 2022.\n[Tito et al., 2023] Rub`en Tito et al. Hierarchical multimodal\ntransformers for multipage DocVQA.\nPattern Recogni-\ntion, 144:109834, 2023.\n[Vaswani et al., 2017] Ashish Vaswani et al. Attention is all\nyou need. Advances in neural information processing sys-\ntems, 30, 2017.\n[Vedantam et al., 2015] Ramakrishna\nVedantam\net\nal.\nCIDEr: Consensus-based image description evaluation,\n2015.\n[Wang et al., 2021] Bryan Wang et al. Screen2words: Auto-\nmatic mobile ui summarization with multimodal learning.\nIn The 34th Annual ACM Symposium on User Interface\nSoftware and Technology, pages 498\u2013510, 2021.\n[Wang et al., 2022] Peng Wang et al. OFA: Unifying archi-\ntectures, tasks, and modalities through a simple sequence-\nto-sequence learning framework. In International Confer-\nence on Machine Learning, pages 23318\u201323340. PMLR,\n2022.\n[Wang et al., 2023] Dongsheng Wang et al.\nDocLLM:\nA layout-aware generative language model for mul-\ntimodal\ndocument\nunderstanding.\narXiv\npreprint\narXiv:2401.00908, 2023.\n[Wichers et al., 2018] Nevan Wichers et al.\nResolving re-\nferring expressions in images with labeled elements. In\n2018 IEEE Spoken Language Technology Workshop (SLT),\npages 800\u2013806. IEEE, 2018.\n[Wu et al., 2021] Jason Wu et al. Screen parsing: Towards\nreverse engineering of ui models from screenshots. In The\n34th Annual ACM Symposium on User Interface Software\nand Technology, pages 470\u2013483, 2021.\n[Wu et al., 2023a] Jialin Wu et al. Omni-SMoLA: Boosting\ngeneralist multimodal models with soft mixture of low-\nrank experts, 2023.\n[Wu et al., 2023b] Sijin Wu et al. DocPrompt: Large-scale\ncontinue pretrain for zero-shot and few-shot document\nquestion answering, 2023.\n[Xue et al., 2020] Linting Xue et al.\nmT5: A massively\nmultilingual pre-trained text-to-text transformer.\narXiv\npreprint arXiv:2010.11934, 2020.\n[Yang et al., 2022] Zhengyuan Yang et al.\nUniTAB: Uni-\nfying text and box outputs for grounded vision-language\nmodeling. In European Conference on Computer Vision,\npages 521\u2013539. Springer, 2022.\n[Zang et al., 2021] Xiaoxue Zang et al.\nMultimodal icon\nannotation for mobile applications.\nIn Proceedings of\nthe 23rd International Conference on Mobile Human-\nComputer Interaction, pages 1\u201311, 2021.\n[Zhang et al., 2021] Xiaoyi Zhang et al.\nScreen recogni-\ntion: Creating accessibility metadata for mobile applica-\ntions from pixels. In Proceedings of the 2021 CHI Con-\nference on Human Factors in Computing Systems, pages\n1\u201315, 2021.\nAppendix for submission titled: ScreenAI: A\nVision-Language Model for UI and\nInfographics Understanding\nA\nDefinitions of Metrics\nWe describe below the two categories of metrics that we use\nin our fine-tuning benchmarks.\nMetrics for object detection tasks.\nFor tasks involving the\npredictions of bounding boxes (UI elements), we use the stan-\ndard object detection approach, which consists of first match-\ning the predicted bounding boxes with the ground truth, and\nthen computing various metrics from these matches. We set\nthe Intersection over Union (IoU) threshold to 0.1, and we\nperform the matching per class, not globally. The metrics\nused in this paper are:\n1. F1@IoU=0.1 - F1 score (harmonic mean of the preci-\nsion and recall) at IoU threshold 0.1.\n2. Acc@IoU=0.1 - Top-1 accuracy at IoU threshold 0.1.\nMetrics for benchmarks where output is plain text.\nFor\nall other tasks, we use the following metrics:\n1. CIDEr (Consensus-based Image Description Evalua-\ntion) [Vedantam et al., 2015].\n2. SQuAD F1 - F1 score (harmonic mean of the precision\nand recall) after applying SQuAD (Stanford Question\nAnswering Dataset) [Rajpurkar et al., 2016] text pre-\nprocessing.\n3. Relaxed accuracy [Methani et al., 2020],\n4. ANLS\n(Average\nNormalized\nLevenshtein\nSimilar-\nity) [Mathew et al., 2021].\nB\nScreen schema examples\nFigure 6 shows examples of the screen schema used in most\nof our pretraining tasks. Each schema contains:\n1. The UI element names.\n2. The OCR text (when applicable).\n3. The element descriptions (e.g. captioning, or the icon\nname).\n4. The bounding box coordinates, quantized and normal-\nized between 0 and 999.\nParentheses are used to create a basic hierarchical structure\nbetween the elements, i.e. the children of a parent element\nare all put inside a parenthesis block. For ease of visualiza-\ntion, the bounding boxes from the screen schema have been\noverlaid on the original screenshot.\nC\nPrompts for LLM generated content\nIn this section, we present some of the prompts used as input\nto LLMs like PaLM 2-S [Anil et al., 2023b] to generate data\nfor screen question answering, screen navigation and screen\nsummarization tasks. In addition to the prompt, we also pass\nas input to the LLM the screen annotation schema described\nin Appendix B.\nTOOLBAR 0 998 31 113 (\n  PICTOGRAM arrow backward 0 135 32 112\n  TEXT Sacramento, CA 179 549 57 90)\nTEXT H16 45 113 115 136\nLIST_ITEM 0 994 164 611 (\n  IMAGE a window with a black curtain 0 91 177 466\n  IMAGE one building with a few palm trees surrounding it . \n111 891 173 464\n  TEXT 1/10 863 970 187 223\n  IMAGE a building with a few people walking by 900 996 188 472\n  TEXT $1,915 - $2,115 47 430 482 521\n  TEXT \u2022 1 Bed 1 Bath 45 299 529 556\n  PICTOGRAM call 634 717 482 529\n  PICTOGRAM envelope 744 826 483 528\n  PICTOGRAM heart 868 956 480 527\n  TEXT THE EISLEY 46 218 563 587\n  BUTTON VERIFIED 765 956 557 587)\nBUTTON VERIFIED 515 703 113 137 (\n  PICTOGRAM check 523 561 114 135)\nBUTTON ONLINE TOURS 713 953 111 136\nLIST_ITEM 1 98 624 926 (\n  IMAGE a concrete walkway with a concrete planter 0 93 630 925)\nLIST_ITEM 105 882 626 917 (\n  IMAGE one kitchen with a bar and a microwave . 115 876 629 916)\nLIST_ITEM 896 999 630 921 (\n  TEXT 1/10 870 966 639 673\n  IMAGE a couch with a picture on the wall 900 999 640 919)\nIMAGE a white bowl with a chicken curry and vegetables . 0 994 4 373 (\n  NAVIGATION_BAR 1 996 34 109 (\n    PICTOGRAM arrow backward 36 148 43 105\n    PICTOGRAM three dots 853 966 41 107)\n  )\nTEXT Akakiko Limassol 39 695 411 469\nPICTOGRAM heart 857 959 409 467\nTEXT Easy Japanese fusion dining! 40 574 493 524\nLIST_ITEM 0 994 560 625 (\n  PICTOGRAM happy face 35 86 577 606\n  TEXT Excellent 8.8 130 339 579 607)\nLIST_ITEM 1 991 628 694 (\n  PICTOGRAM time 34 87 645 675\n  TEXT Closed Opens at 12:00 128 518 647 676\n  BUTTON More info 745 959 636 685)\nLIST_ITEM 4 988 697 763 (\n  PICTOGRAM 743 714 87 35\n  TEXT Schedule for later 129 420 715 744\n  BUTTON Change 778 957 704 754)\nTEXT Unfortunately, this restaurant does not 94 733 811 839\nTEXT deliver to your location 90 460 842 868\nBUTTON OK 782 931 807 870\nPICTOGRAM sad face 475 522 840 867\nTEXT Search Ak\u2c6dkiKU LilliASSOT 98 603 904 921\nNAVIGATION_BAR 0 997 933 999 (\n  PICTOGRAM arrow backward 187 254 948 984\n  PICTOGRAM a gray circle with a white background 471 532 951 983\n  PICTOGRAM nav bar rect 752 809 951 982)\nTOOLBAR 0 999 31 100 (\n  PICTOGRAM arrow backward 28 92 50 88\n  TEXT The Best Pet Friendly Hotels in Virginia Beac... 117 983 52 83)\nTEXT Pet-friendly Hotels 56 598 99 136\nLIST_ITEM 79 920 168 229 (\n  TEXT Tonight 117 623 185 211\n  TEXT Jun 9 - Jun 10 652 886 185 211)\nLIST_ITEM 82 919 230 291 (\n  TEXT Tomorrow night 119 610 247 273\n  TEXT Jun 10 - Jun 11 631 887 248 274)\nLIST_ITEM 81 920 292 352 (\n  TEXT This weekend 118 609 308 334\n  TEXT Jun 11 - Jun 13 630 889 308 335)\nLIST_ITEM 70 931 355 413 (\n  TEXT Next weekend 121 614 370 395\n  TEXT Jun 18 Jun 20 635 887 370 396)\nMAP 40 960 481 625\nBUTTON Show map 35 964 616 676\nTEXT Top Virginia Beach Pet-friendly Hotels 36 959 707 788\nTEXT See more Pet-friendly Hotels in Virginia Beach 34 756 800 827\nBUTTON Choose your dates 31 968 866 914\nNAVIGATION_BAR 0 998 934 999 (\n  PICTOGRAM arrow backward 185 256 945 983\n  PICTOGRAM nav bar circle 464 534 947 983\n  PICTOGRAM nav bar rect 742 812 950 983)\nTEXT Hamleys Inbox Inspiration 36 451 52 81\nTEXT Subscribe to hear about new products and stores. 92 901 149 177\nTEXT_INPUT 50 728 194 254 (\n  LABEL Email Id 85 241 209 239)\nBUTTON Sign Up 731 951 199 251\nLIST_ITEM 143 359 321 476 (\n  PICTOGRAM * 175 314 330 404\n  TEXT Quality promise 176 327 415 469)\nLIST_ITEM 386 612 322 477 (\n  PICTOGRAM . 422 573 330 407\n  TEXT Free delivery 422 576 416 470)\nLIST_ITEM 637 854 327 475 (\n  PICTOGRAM a truck with an arrow going to the right 684 825 337 406\n  TEXT Easy Return 679 818 416 469)\nPICTOGRAM \u092e\u0947 455 593 494 559\nIMAGE a cartoon of a man walking in front of a cityscape. 4 993 548 773\nTEXT follow us at: 172 370 793 821\nPICTOGRAM facebook 392 459 774 820\nPICTOGRAM twitter 498 576 775 819\nPICTOGRAM (O 606 685 776 820\nPICTOGRAM play 713 789 779 817\nPICTOGRAM C 197 234 832 853\nTEXT Hamleys 2021 All Rights Reserved. 222 804 830 854\nNAVIGATION_BAR 1 997 859 925 (\n  BUTTON Add to Bag 38 468 868 917\n  BUTTON Buy Now 504 965 867 918)\nNAVIGATION_BAR 0 997 934 999 (\n  PICTOGRAM arrow backward 190 253 949 983\n  PICTOGRAM a gray circle 472 530 951 982\n  PICTOGRAM nav bar rect 754 810 951 982)\nFigure 6: Examples of our screen schema.\nC.1\nScreen Question Answering\nYou only speak JSON. Do not write text\nthat isn\u2019t JSON.\nYou are given the following mobile\nscreenshot, described in words. Can you\ngenerate 5 questions regarding the\ncontent of the screenshot as well as the\ncorresponding short answers to them? The\nanswer should be as short as possible,\ncontaining only the necessary information\n. Your answer should be structured as\nfollows:\nquestions: [\n{{question: the question,\nanswer: the answer\n}}, ...]\n{THE SCREEN SCHEMA}\nC.2\nScreen Navigation\nYou only speak JSON. Do not write text\nthat isn\u2019t JSON. You are given a mobile\nscreenshot, described in words. Each UI\nelement has a class, which is expressed\nin capital letter. The class is sometimes\nfollowed by a description, and then 4\nnumbers between 0 and 999 represent the\nquantized coordinates of each element.\nGenerate {num_samples} single-step\nnavigation instructions and their\ncorresponding answers based on the\nscreenshot. Each answer should always\nstart with \u2018click\u2018, followed by the\ncoordinates of the element to click on, e\n.g. \u2018click 0 137 31 113\u2018.\nBe creative with the questions, do not\nalways use the same wording, refer to the\nUI elements only indirectly, and use\nimperative tense. Your answer should be\nstructured as in the example below:\n\"questions\": [\n{{\"question\": \"the question\",\n\"answer\": \"click 0 137 31 113\"\n}},\n...\n]\n{THE SCREEN SCHEMA}\nC.3\nScreen Summarization\nYou only speak JSON. Do not write text\nthat isn\u2019t JSON.\nYou are given the following mobile\nscreenshot, described in words.\nGenerate a summary of the screenshot in\n2-3 sentences. Do not focus on\nspecifically naming the various UI\nelements, but instead, focus on the\ncontent. Your answer should be structured\nas follows:\n\"summary\": the screen summary\n{THE SCREEN SCHEMA}\nD\nChart-to-table Translation in Pre-training\nMixture\nWe describe the composition of the chart-to-table translation\ntask in the pre-training mixture. At a high-level, the role of\nthis dataset is to further improve the vision and language rep-\nresentation of images that represent charts.\nWe do so by extending the work done by [Liu et al., 2023],\nwhich combines several derendering tasks into a mixture.\nThis consists of the following mix of chart-table pairs from\nthe following sources:\n\u2022 synthetic data mixture used by [Liu et al., 2023]\n\u2022 mixture introduced by [Masry et al., 2023].\n\u2022 pairs from DVQA training set [Kafle et al., 2018]\n\u2022 chart-to-table pairs from TaTA [Gehrmann et al., 2022]\n\u2022 chart-to-table pairs from in Benetech - Making Chart\nAccessible Kaggle challenge 2\nThe total size of the mixture adds up to about 2.4M samples\nwith roughly 85% of them coming from the synthetic mixture\nin [Liu et al., 2023] and the UniChart [Masry et al., 2023]\nwork.\nE\nScreenQA short answers generation\nWe describe below the motivation behind producing a list in-\nstead of a single short answer as a new ground truth for the\nScreenQA [Hsiao et al., 2022] dataset, as well as the genera-\ntion details.\nThere are many ways to represent the same information.\nFor example, \u201d25.01.2023\u201d, \u201d25th of January 2023\u201d and \u201dJan-\nuary 25, 2023\u201d are representing the same date, and the model\nshould not be penalized for choosing one representation over\nthe others. A list of various representations of the same fac-\ntual answer allows this.\nA variant of the PaLM 2-S [Anil et al., 2023b] was used to\ngenerate this list of short answers in a few-shot setting. We\ngive as input to the LLM text information from the ScreenQA\ndataset (question, list of UI elements descriptions and full-\nsentence answer) in addition to the prompts described in\nAppedix E.1 and E.2. The generated lists were then verified\nby simple heuristics and eyeballing of random samples. See\nexamples of questions and answers from the ScreenQA task,\ntogether with their LLM-generated short answers, in Figure 7.\nE.1\nFor answers contained in a single UI element\nFor each entry in the ScreenQA dataset where there is only\none UI element in the ground truth, we use the following\nprompt with the PaLM 2-S model [Anil et al., 2023b] to gen-\nerate a list of short answers from the question, list of ele-\nments, and the full-sentence answer:\nList various ways to rephrase the answer. The\nanswer should be as short as possible,\nwithout extra words from the question. Use\nall provided elements in each answer. Provide\nthe output in square brackets.\n2https://www.kaggle.com/competitions/benetech-making-\ngraphs-accessible\nQuestion: \n\u25cf What is the status of \u201cEnable security code\u201d?\nFull-sentence answers:\n\u25cf The status of \u201cEnable security code\u201d is \u201coff\u201d.\n\u25cf The status is \u201coff\u201d.\nLLM-generated short answers:\n\u25cf off\n\u25cf disabled\nQuestion: \n\u25cf What  is  the  count of calories?\nFull-sentence answers:\n\u25cf There are 0 calories.\n\u25cf The count of calories is 0.\n\u25cf The calorie count is 0.\nLLM-generated short answers:\n\u25cf 0\n\u25cf zero\n\u25cf no calories\nQuestion:\n\u25cf How  many  likes and comments are there of the \npost \u201cWhy Michael Flynn kept his Job 17 days \nafter the White House!\u201d?\nFull-sentence answers:\n\u25cf There is 1 like and 1 comment  on  the  post  \n\u201cWhy Michael  Flynn  kept  his job   17   days   \nafter   the White House!\u201d.\n\u25cf There   is   1   like   and   1 comment   on   \nthe   \u201cWhy Michael  Flynn  kept  his Job   17   \ndays   after   the White House!\u201d post.\n\u25cf There is 1 like and 1 comment.\nLLM-generated short answers:\n\u25cf one and one\n\u25cf 1 and 1\n\u25cf one, one\n\u25cf 1, 1\n\u25cf 1 like, 1 comment\n\u25cf 1 like and 1 comment\nQuestion:\n\u25cf What is the phone number?\nFull-sentence answers:\n\u25cf The   phone   number   is 415-579-1638.\n\u25cf The phone number is +1 415-579-1638.\n\u25cf The   phone   number   is 4155791638.\nLLM-generated short answers:\n\u25cf 4155791638\n\u25cf +1 415-579-1638\n\u25cf 415-579-1638\nFigure 7: Examples of questions and answers from the ScreenQA\ndataset, together with their LLM-generated short answers.\nHere is an example:\nQuestion: \u2019What\u2019s the percentage of humidity\n?\u2019\nAnswer elements: [\u201965\nFull answer: \u2019The humidity is 65\nRephrases: [\u201965\nHere is another example:\nQuestion: \u2019What is the gender?\u2019\nAnswer elements: [\u2019Male\u2019]\nFull answer: \u2019The gender is male.\u2019\nRephrases: [\u2019male\u2019]\nHere is another example:\nQuestion: \u2019What is the status of \"24 hr clock\n\"?\u2019\nAnswer elements: [\u2019on\u2019]\nFull answer: \u2019The status is \"on\".\u2019\nRephrases: [\u2019on\u2019, \u2019enabled\u2019]\n[...]\nNow is your turn.\nQuestion: {THE QUESTION}\nAnswer elements: {THE UI ELEMENT DESCRIPTION}\nFull answer: {THE FULL-SENTENCE ANSWER}\nRephrases:\nE.2\nFor answers contained in multiple UI elements\nFor each entry in the ScreenQA dataset where there are more\nthan one UI elements in the ground truth, we use the follow-\ning prompt with the PaLM 2-S model [Anil et al., 2023b] to\ngenerate a list of short answers from the question, list of UI\nelements and full-sentence answer:\nList various ways to rephrase the answer. The\nanswer should be as short as possible,\nwithout extra words from the question. Use\nall provided elements in each answer. Provide\nthe output in square brackets.\nHere is an example:\nQuestion: \u2019What\u2019s the temperature?\u2019\nAnswer elements: [\u201959\u2019, \u2019\u25e6F\u2019]\nFull answer: \u2019The temperature is 59 degrees\nFahrenheit.\u2019\nRephrases: [\u201959\u25e6F\u2019, \u201959 Fahrenheits\u2019, \u201959\ndegrees Fahrenheit\u2019]\nHere is another example:\nQuestion: \u2019What is the name?\u2019\nAnswer elements: [\u2019Jon\u2019, \u2019Brown\u2019]\nFull answer: \u2019The name is Jon Brown.\u2019\nRephrases: [\u2019Jon Brown\u2019]\nHere is another example:\nQuestion: \u2019What is the rest interval duration\n?\u2019\nAnswer elements: [\u201900\u2019, \u2019:\u2019, \u201934\u2019]\nFull answer: \u2019The rest interval lasts 00:34.\u2019\nRephrases: [\u201900:34\u2019, \u201934 seconds\u2019, \u20190 minutes\nand 34 seconds\u2019, \u201934 minutes\u2019, \u20190 hours and\n34 minutes\u2019]\nModel\nApp Seen\nApp Unseen\nBaseline\n66.3\n67.6\nScreenAI\n87.7\n87.8\nTable 5: Metrics on different splits of MoTIF [Burns et al., 2022]\nTask Automation.\n[...]\nNow is your turn.\nQuestion: {THE QUESTION}\nAnswer elements: {THE FIRST UI ELEMENT\nDESCRIPTION, ...}\nFull answer: {THE FULL-SENTENCE ANSWER}\nRephrases:\nF\nComplex Question Answering Datasets\nThe Complex QA datasets contain machine-generated ques-\ntions using LLMs like PaLM 2-S [Anil et al., 2023b] based on\nthe Screen Annotation output from the best ScreenAI VLM.\nFor each dataset, the prompts are chosen to target certain\ntypes of questions. With this approach, we generate large\nscale datasets for desktop, mobile, mobile with different as-\npect ratios, and infographics screens. These datasets are used\nboth for pre-training and evaluation. We add an additional\nstep of human raters verification for the evaluation data. Fig-\nure 8 and Figure 9 show a few examples of LLM-generated\nQA data that was verified by humans.\nWe distinguish three different subsets, each focusing on\nsolving the various challenges we identified with this task:\n\u2022 Desktop QA and Long Webpage QA: Datasets on\ndesktop screens and long (viewport height) webpages,\nrespectively. The aspect ratio and size of the input im-\nages is very different compared to other QA datasets.\n\u2022 Complex QA datasets: Datasets mainly focused on\ncounting, arithmetic, and comparison operations requir-\ning information from more than one part of the screen.\n\u2013 Complex QA: Mobile app screens\n\u2013 Desktop Complex QA: Desktop screens.\n\u2013 Long Webpage Complex QA: Long webpages.\n\u2022 Non Answerable QA: Dataset focused on measuring\nthe ability of the model to know when a question can-\nnot be answered from the given screen.\nG\nMoTIF Evaluation Results\nIn this section, we present the ScreenAI model metrics on\nthe different splits of the MoTIF [Burns et al., 2022] task\nautomation dataset. The metrics breakdown can be seen in\nTable 5.\nH\nScreen Navigation generated examples\nWe present a few examples for the Screen Navigation task\ngenerated using LLMs in Figure 10. More details about the\ndata generation process can be found in Section 3.\nQuestion: How many days are between \nthe departure and return dates?\nAnswer: There is no answer on the \nscreen.\nQuestion: How many more unread \nmessages are there in the All section \ncompared to the Private section?\nAnswer: 2\nQuestion: How many songs have a \nduration of less than 30 seconds?\nAnswer: 1\nQuestion: How many text size options \nare there?\nAnswer: 5\nFigure 8: Examples of mobile Complex QA evaluation examples.\nQuestion: \nHow many offices does Pioneer Cardiovascular have?\nAnswer: 4\nQuestion: What is the lift capacity at 35%?\nAnswer: 1960 lb.\nFigure 9: Examples of desktop Complex QA evaluation examples.\nCommand: Tap the item about the \nDuncan Campbell exhibition\nCommand: Complete your order\nCommand: Click on the contact info\nCommand: Open the menu\nFigure 10: Examples of Screen Navigation data generated using an\nLLM. The target bounding box is highlighted in red.\nI\nNew benchmarks and datasets\nWe release three evaluation datasets for tasks described in\nSection 4.2:\n\u2022 Screen Annotation (SA): The dataset will be released\nsoon. Please contact the POC of this paper for notifica-\ntion.\n\u2022 ScreenQA Short (SQA Short):\nThe dataset is\navailable\nat\nhttps://github.com/google-research-\ndatasets/screen qa\n\u2022 Complex ScreenQA (Cplx SQA): The dataset will be\nreleased soon. Please contact the POC of this paper for\nnotification.\n"
  },
  {
    "title": "Direct Language Model Alignment from Online AI Feedback",
    "link": "https://arxiv.org/pdf/2402.04792.pdf",
    "upvote": "24",
    "text": "Direct Language Model Alignment from Online AI Feedback\nShangmin Guo\u2020 * 1 Biao Zhang * 2 Tianlin Liu\u2020 3 Tianqi Liu 2 Misha Khalman 2 Felipe Llinares 2\nAlexandre Ram\u00e9\u2020 2 Thomas Mesnard 2 Yao Zhao 2 Bilal Piot 2 Johan Ferret 2 Mathieu Blondel 2\nAbstract\nDirect alignment from preferences (DAP) meth-\nods, such as DPO, have recently emerged as effi-\ncient alternatives to reinforcement learning from\nhuman feedback (RLHF), that do not require a\nseparate reward model. However, the preference\ndatasets used in DAP methods are usually col-\nlected ahead of training and never updated, thus\nthe feedback is purely offline.\nMoreover, re-\nsponses in these datasets are often sampled from\na language model distinct from the one being\naligned, and since the model evolves over training,\nthe alignment phase is inevitably off-policy. In\nthis study, we posit that online feedback is key and\nimproves DAP methods. Our method, online AI\nfeedback (OAIF), uses an LLM as annotator: on\neach training iteration, we sample two responses\nfrom the current model and prompt the LLM an-\nnotator to choose which one is preferred, thus\nproviding online feedback. Despite its simplicity,\nwe demonstrate via human evaluation in several\ntasks that OAIF outperforms both offline DAP\nand RLHF methods. We further show that the\nfeedback leveraged in OAIF is easily controllable,\nvia instruction prompts to the LLM annotator.\n1. Introduction\nTo maximise the benefits of large language models (LLMs)\nto society, it is important to align them with human expecta-\ntions and values (Ouyang et al., 2022; Bai et al., 2022a;\nBubeck et al., 2023).\nThe first method introduced for\nalignment was reinforcement learning from human feed-\nback (RLHF, Christiano et al., 2017; Stiennon et al., 2020),\nwhich trains a reward model (RM) from pairwise prefer-\nences and then optimises a policy against the RM via rein-\nforcement learning (RL). More recently, direct alignment\n*Equal contribution\n\u2020Work done during an internship at\nGoogle DeepMind\n1University of Edinburgh\n2Google Deep-\nMind 3University of Basel.\nCorrespondence to:\nShangmin\nGuo <s.guo@ed.ac.uk>, Biao Zhang <biaojiaxing@google.com>,\nMathieu Blondel <mblondel@google.com>.\nPreprint.\nfrom preferences (DAP) methods have emerged as popular\nalternatives to RLHF, such as direct preference optimisa-\ntion (DPO, Rafailov et al., 2023), sequence likelihood cal-\nibration with human feedback (SLiC, Zhao et al., 2023),\nand identity policy optimisation (IPO, Azar et al., 2023).\nIn contrast to RLHF, the DAP methods directly update the\nlanguage model (a.k.a. policy) \u03c0\u03b8 using pairwise preference\ndata, making the alignment simpler, more efficient and more\nstable (Rafailov et al., 2023).\nHowever, the preference datasets used in DAP methods are\noften collected ahead of training and the responses in the\ndataset are usually generated by different LLMs. Thus, the\nfeedback in DAP methods is usually purely offline, as \u03c0\u03b8\ncannot get feedback on its own generations over training.\nThis is problematic because of the significant distribution\nshift between the policy that generated the dataset and the\npolicy being aligned: we train on the distribution induced\nby \u03c1 but evaluate on the distribution induced by \u03c0\u03b8 in the\nend. In contrast, in RLHF, the RM provides online feedback\nto generations from \u03c0\u03b8 during the RL step. This practice\nleads to on-policy learning, which was shown to improve\nexploration and overall performance (Lambert et al., 2022).\nInspired by RL from AI feedback (RLAIF) (Bai et al.,\n2022b; Lee et al., 2023), we hereby propose Online AI\nFeedback (OAIF) for DAP methods. Our method inherits\nboth the practical advantages of DAP methods and the on-\nline nature of RLHF. Specifically, when aligning an LLM\npolicy \u03c0\u03b8, we follow a three-step procedure: 1) we sample\ntwo responses to a prompt from the current policy \u03c0\u03b8; 2) we\nobtain online feedback over the two responses by prompt-\ning an LLM to mimic human preference annotation; 3) we\nuse this online feedback to update the model \u03c0\u03b8 through\nstandard DAP losses. Our approach is depicted in Figure 1.\nUnlike methods proposed by Xu et al. (2023); Liu et al.\n(2023); Xiong et al. (2023), OAIF skips the RM training,\nand directly extracts the preference from an LLM.\nTo show the effectiveness of our proposal, we perform\nan extensive empirical comparison between OAIF, ex-\nisting offline DAP methods and RLHF methods.\nOur\nexperimental protocol uses both AI and human evaluation\non standard LLM alignment tasks:\nTL;DR (Ziegler\net\nal.,\n2019),\nAnthropic Helpfulness and\n1\narXiv:2402.04792v2  [cs.AI]  29 Feb 2024\nDirect Language Model Alignment from Online AI Feedback\nDirect Alignment\nfrom Preferences\nWhat are \nrecommended places\nto visit in Paris?\nI recommend visiting \nNotre-Dame de Paris, a \nworld famous cathedral \nbuilt in\u2026\nI would advise to avoid \nParis and instead plan \nfor a trip in the \ncountryside of France.  \nprompt\nI would advise to avoid \nParis and instead plan for \na trip in the countryside \nof France. \nLLM \nAnnotator\nupdate\nparameters\n             LLM \n      being aligned \nI recommend visiting \nNotre-Dame de Paris, a \nworld famous cathedral \nbuilt in\u2026\nFigure 1: Summary of the proposed online AI feedback (OAIF) approach for making direct alignment from\npreferences (DAP) methods online and on-policy. Given an input prompt x, two responses y1 and y2 are first sampled\nfrom the current language model \u03c0\u03b8t, then labelled as y` and y\u00b4 by the LLM annotator. The language model parameters\nare then updated using the objective function of DAP methods.\nHarmlessness (Bai et al., 2022a). To summarise, we\nmake the following contributions.\n\u2022 We demonstrate the effectiveness and generality of\nOAIF for turning offline DAP methods (DPO, IPO,\nSLiC) into online methods. Our human evaluation\nshows that the average win rate of online DAP methods\n(DPO, IPO, SLiC) over offline versions of the same\nmethods is \u201e66%.\n\u2022 We confirm the usefulness of making DAP methods on-\nline: human raters favour DPO with OAIF (thus, online\nDPO) over SFT baseline, RLHF and RLAIF 58.00%\nof time on the TL;DR task in 4-way comparisons.\n\u2022 We demonstrate the controllability of the LLM annota-\ntor, by injecting specific instructions into the prompts.\nWe use response length as a test-bed. By asking the\nLLM annotator to prefer shorter responses, the average\nlength of responses from the aligned policy is signifi-\ncantly shortened from \u201e120 to \u201e40, while its quality\nis still improved over the SFT baseline.\n2. Background\nPairwise preference collection. Current methods for LLM\nalignment first collect a dataset of pairwise preferences, as\nfollows. A prompt x is sampled from a prompt distribu-\ntion pX , then two distinct responses y1 and y2 are sampled\nindependently from an existing LLM \u03c1. Then, human (Chris-\ntiano et al., 2017) or AI annotators (Lee et al., 2023) rank\nthe responses, yielding a preferred response y` and a less\npreferred one y\u00b4. With some abuse of notation, we assume\nthat there exists a function that uniquely maps py1, y2q to\npy`, y\u00b4q, and we will therefore write py`, y\u00b4q \u201e \u03c1p\u00a8|xq.\nA preference dataset D \u201c tpxi, y`\ni , y\u00b4\ni quN\ni\u201c1 is then con-\nstructed by repeating the above process N times.\nDirect alignment from preference (DAP) methods. DAP\nmethods directly update the target policy \u03c0\u03b8 from the prefer-\nence pairs py`, y\u00b4q. The loss functions for the three main\nDAP methods investigated in this work are summarised\nbelow. They take the form \u2113px, y`, y\u00b4, \u03b8q for a prompt\nx \u201e pX , a response pair py`, y\u00b4q \u201e \u03c1p\u00a8|xq and model\nparameters \u03b8.\n\u2022 DPO loss:\n\u00b4 log \u03c3\n\u02c6\n\u03b2 log \u03c0\u03b8py`|xq\u03c0\u03b80py\u00b4|xq\n\u03c0\u03b80py`|xq\u03c0\u03b8py\u00b4|xq\n\u02d9\n(1)\n\u2022 IPO loss:\n\u02c6\nlog\n\u02c6\u03c0\u03b8py`|xq\u03c0\u03b80py\u00b4|xq\n\u03c0\u03b8py\u00b4|xq\u03c0\u03b80py`|xq\n\u02d9\n\u00b4 1\n2\u03b2\n\u02d92\n(2)\n\u2022 SLiC loss:\nmax\n\u02c6\n0, 1 \u00b4 \u03b2 log\n\u02c6\u03c0\u03b8py`|xq\u03c0\u03b80py\u00b4|xq\n\u03c0\u03b8py\u00b4|xq\u03c0\u03b80py`|xq\n\u02d9\u02d9\n(3)\nwhere \u03c0\u03b80 is the SFT baseline used as reference, \u03c3 is the\nlogistic function, and \u03b2 is a scalar hyperparameter. We em-\nphasise once again that py`, y\u00b4q are sampled from \u03c1p\u00a8|xq,\nnot from \u03c0\u03b8tp\u00a8|xq, as this will be the key difference with\nthe online variant we propose in the next section. One\nadvantage of these loss functions is that their gradients\n2\nDirect Language Model Alignment from Online AI Feedback\nFigure 2: Illustration of the distribution shift problem.\nThe responses (y1, y2) sampled from the current model \u03c0\u03b8t\ndiffer from preference dataset responses (y`, y\u00b4) sampled\nfrom \u03c1, as \u03c1 \u2030 \u03c0\u03b8t. Two independent distribution shifts\ncan occur: an initial distribution shift (\u03c1 \u2030 \u03c0\u03b80) and a\ngradual distribution shift (\u03c0\u03b80 \u2030 \u03c0\u03b8t) during the alignment\nprocedure.\n\u2207\u03b8\u2113px, y`, y\u00b4, \u03b8q can be computed exactly in an efficient\nway. In contrast, because the loss function used in RLHF\ninvolves an expectation over the space of responses (Ziegler\net al., 2019), policy gradient methods are typically used to\nobtain an unbiased estimate of the gradient and a value func-\ntion is typically used to reduce the variance, which requires\nstoring an additional model in memory.\nOffline feedback. In most real-world applications, due\nto the financial cost and complexity of collecting pairwise\npreferences from human annotators, the preference dataset\nD is usually collected ahead of aligning a language model\n\u03c0\u03b8 and kept fixed throughout training. Obtaining online\npreferences on new responses is usually not feasible, as\nthere is no human-in-the-loop. Using a fixed dataset D\nmakes all preference data offline, which means the policy1\n\u03c0\u03b8 cannot get feedback on its own generations on-the-fly\nover the alignment procedure. It is worth mentioning that\nthe RL step in RLHF and RLAIF is online as the training\ndata is acquired interactively. See Appendix A.1 for an\nin-depth discussion on online vs. offline feedback.\nOff-policy learning. Beyond the offline feedback problem\nillustrated above, aligning an LLM policy \u03c0\u03b8 with DAP\nmethods on a pre-collected dataset D also yields a distribu-\ntion shift between the generation from the policy \u03c1 and the\npolicy \u03c0\u03b8t at each time step t. This makes the alignment\noff-policy as \u03c0\u03b8t \u2030 \u03c1 and \u03c0\u03b8t keeps evolving over learning.\n1In this work, we use language model and policy interchange-\nably to refer to the model \u03c0\u03b8 being aligned.\nMethod\nNo RM\nneeded\nOn-policy\ngeneration\nOnline\nfeedback\nOffline DPO\n(Rafailov et al., 2023)\n\u2713\n\u2717\n\u2717\nOffline IPO\n(Azar et al., 2023)\n\u2713\n\u2717\n\u2717\nOffline SLiC\n(Zhao et al., 2023)\n\u2713\n\u2717\n\u2717\nRSO\n(Liu et al., 2023)\n\u2717\n\u2713\n\u2713\nIterative DPO\n(Xu et al., 2023)\n\u2717\n\u2713\n\u2713\nOAIF (proposed)\n\u2713\n\u2713\n\u2713\nTable 1: Comparison between OAIF (proposed) and ex-\nisting DAP methods, with or without a separate RM. Tech-\nnically, training RMs on pre-collected preference data still\nsuffers from the distribution shift problem, as RMs cannot\nget feedback for responses from the model \u03c0\u03b8t.\nThis shift problem is illustrated in Figure 2. We also provide\nan empirical verification of this problem in Appendix B. In\nDPO, this problem is tackled by supervised finetuning \u03c0\u03b8 on\nD so that \u03c0\u03b80 \u00ab \u03c1 , but the off-policy issue remains during\nalignment as \u03c0\u03b8t gradually departs from \u03c0\u03b80. Thanks to the\nonline nature of RL, RL methods are also on-policy, as the\nresponses used to update \u03c0\u03b8t are all sampled from it. See\nAppendix A.2 for more details on on-policy vs. off-policy\nlearning in LLMs.\nRM-based online feedback for DAP methods. To avoid\nthe distribution shifts arising when aligning LLMs with\noffline DAP methods on a given dataset D, an intuitive\nand straightforward solution is to introduce an RM to pro-\nvide online feedback. Liu et al. (2023) proposed RSO, a\nmethod that uses an RM to perform rejection sampling in\norder to sample from the optimal policy, which improved\nthe alignment compared to offline DAP baselines. Besides,\npseudo-labelling the generations from \u03c0\u03b8t by RMs can also\nbe helpful, as done in the Iterative DPO method (Xu et al.,\n2023) and the West-of-N method (Pace et al., 2024). Al-\nthough the aforementioned RM-based methods make the\nalignment of a policy online and on-policy, the distribution\nshift problem still exists when training the RM. More specif-\nically, the RM is trained on the preference dataset D \u201e \u03c1,\nbut used to annotate preference over responses from \u03c0\u03b8t at\ntraining step t, where \u03c0\u03b8 \u2030 \u03c1. Therefore, RM-based online\nfeedback cannot fully avoid distribution shift issues.\nLLM-based online feedback for DAP methods.\nThe\nmethod we propose next, \u201cOnline AI Feedback\u201d (OAIF),\n3\nDirect Language Model Alignment from Online AI Feedback\nconsists in using an LLM as an online annotator.\nOur\nmethod relies on the observation that LLMs can approx-\nimate well human labelling and can generate reliable prefer-\nences over responses (Lee et al., 2023). In recent concurrent\nwork, Yuan et al. (2024) proposed a \u201cself-rewarding\u201d ap-\nproach, in which the policy being aligned provides online\nfeedback to itself. In comparison, OAIF can leverage feed-\nback from any LLM, including ones stronger than the LLM\nbeing aligned. Swamy et al. (2024) also concurrently inves-\ntigates the importance of online preference, but still relying\non RMs.\nIn Table 1, we summarise the characteristics of OAIF and\nof the existing offline and online DAP methods.\n3. Direct alignment from online AI feedback\nBridging the gap. As we saw, DAP methods are simple,\ndo not require a separate RM, but they use preference data\npre-collected offline. On the other hand, RLHF methods\ninteract online with the language model being aligned, but\nthey require policy gradient techniques to obtain an unbiased\ngradient estimate and a value function to reduce the variance.\nTo bridge the gap between these two families of methods, we\npropose a simple yet effective way to make DAP methods\nonline.\nAs pointed out by Ziegler et al. (2019), online data collection\nis crucial for aligning language models. To solve the afore-\nmentioned offline problem in DAP methods, we propose\nto collect preferences on-the-fly for responses generated by\nthe language model being aligned. Naturally, using human\nfeedback would be prohibitively expensive. Prior studies\nhave shown that AI feedback is a reliable and effective\napproximation to human labellers, especially for pairwise\npreference labelling (Lee et al., 2023). We therefore pro-\npose to use an LLM as online annotator, in order to collect\nthe preference over pairs of responses, sampled from \u03c0\u03b8t\non-the-fly during its alignment. We refer to the proposed\napproach as OAIF, which stands for online AI feedback.\nProposed algorithm. An overview of OAIF is given in Fig-\nure 1, and a more formal description is provided in Algo-\nrithm 1 (for simplicity, we use batches of size 1). Given a\nprompt x, sampling y1, y2 from \u03c0\u03b8tp\u00a8|xq ensures on-policy\nlearning. Prompting the annotating LLM to obtain y`, y\u00b4\nensures online learning. We emphasise that the approach is\ngeneral and works with any differentiable DAP loss function\n\u2113px, y`, y\u00b4, \u03b8q.\nGradient computation. An important technical detail of\nonline DAP methods is that \u03b8 is involved in both the re-\nsponse sampling and in the DAP loss function. In contrast,\n\u03b8 is involved only in the loss for offline DAP methods and\nonly in the sampling for RLHF methods. In addition, using\nOAIF, the sampled responses go through an LLM annotator\nAlgorithm 1 Online AI Feedback (OAIF) for Direct Align-\nment from Preference (DAP) methods\nInput: Number of training steps T\nPrompt dataset DX \u201c txiuN\ni\u201c1\nSFT baseline model \u03c0\u03b80\nAn LLM annotator\nA DAP loss function \u2113px, y`, y\u00b4, \u03b8q\n1: for t :\u201c 0 to T do\n2:\nSample prompt x \u201e DX\n3:\nSample response pair y1, y2 \u201e \u03c0\u03b8tp\u00a8|xq\n4:\nUse LLM annotator to get preference pair y`, y\u00b4\n5:\nUpdate \u03b8t into \u03b8t`1 using \u2207\u03b8\u2113px, y`, y\u00b4, \u03b8tq\n6: end for\nOutput: Aligned language model (policy) \u03c0\u03b8T\nto obtain py`, y\u00b4q, which means that py`, y\u00b4q are also\nin principle functions of \u03b8. In practice, we propose to sim-\nply use \u2207\u03b8\u2113px, y`, y\u00b4, \u03b8q as our gradients, which amounts\nto placing a stop_gradient on both the sampling and\nLLM annotation steps.\nAnnotating prompts with text-controllability. We adopt\na pairwise prompting scheme to collect AI feedback, i.e.\nwe instruct the LLM annotator to choose which response\nis preferred among a pair, as in Lee et al. (2023). To avoid\nposition bias, we calculate scores for the two response pos-\nsible orders and use the average as the final score. Since\nOAIF leverages prompting techniques to collect feedback,\nthe reward signals or the preference function can be easily\nadapted by modifying the prompts (Sun et al., 2024). This\noffers high flexibility without incurring any extra compu-\ntation (such as retraining the RM) compared to RLHF and\nRLAIF. For example, in our experiments, we show that we\ncan control the response length by simply prompting the\nannotator to prefer shorter responses.\n4. Experiments\n4.1. Experimental setup\nWe use three tasks for experiments: TL;DR (Stiennon et al.,\n2020), Anthropic Helpfulness and Anthropic\nHarmlessness (Bai et al., 2022a). For each task, we\nprepare the prompt dataset DX by simply extracting the in-\nput prompts from the preference dataset D. We adopt PaLM\n2 (Anil et al., 2023) as the language model and also the LLM\nannotator. Unless otherwise specified, all policy models are\ninitialised from the model obtained by supervised finetuning\n(SFT) PaLM 2-XS (Extra Small), which is referred to as\nthe SFT baseline. For the annotating model, we use PaLM\n2-L (Large). To obtain online feedback from the annotating\nmodel, we adopt the Detailed 0-shot prompt from Lee et al.\n4\nDirect Language Model Alignment from Online AI Feedback\n(2023). The prompts we used and how we get preference\nscores from them are detailed in Appendix E.\nTo demonstrate the generality of OAIF, we experiment with\nthree DAP methods: DPO, IPO and SLiC. Based on prelim-\ninary experiments, we set \u03b2 \u201c 0.1 in DPO, \u03b2 \u201c 1.0 in IPO,\nand \u03b2 \u201c 0.002 in SLiC. We sample responses with a temper-\nature of 0.9 during training. We adopt Adafactor (Shazeer\n& Stern, 2018) as the optimiser, and set the batch size to\n128 and the learning rate to 5 \u00a8 10\u00b47, with a warm-up period\nof 150 steps for all experiments. We evaluate models by\ncomputing win rates, i.e. how often one model\u2019s response\nis better than the other. For automatic evaluation, we apply\nthe same prompting technique as above but with Gemini\nPro (Gemini Team et al., 2023) to reduce the risk of over-\nfitting and reward hacking (Gao et al., 2023). The validity\nof Gemini Pro as the judge is explored in Appendix C. For\nhuman evaluation, three raters are presented with responses\ngenerated from a set of policy models. Each rater is then\nasked to independently score the responses\u2019 quality (from 1\nto 5 where 5 denotes the highest) and to pick the best one,\nand the average score is then used to compare the models.\n4.2. How effective is OAIF for LLM alignment?\nWe start by examining the effectiveness of OAIF for DAP\nmethods (that use online AI feedback), compared to their\noffline counterparts (that use pre-collected offline human\npreferences). As a sanity check, we track the win rate of\nDPO with OAIF (\u201cOnline DPO\u201d) and vanilla DPO (\u201cOffline\nDPO\u201d) against the SFT baseline on TL;DR. The results are\ngiven in Figure 3, where the results for RLAIF and RLHF\nare provided as references.\n0\n2000\n4000\n6000\n8000\n10000\n12000\nTraining\u00a0Steps\n50%\n60%\n70%\n80%\n90%\n100%\nWin\u00a0Rate\u00a0(%)\u00a0against\u00a0SFT\u00a0Baseline\nOnline\u00a0DPO\nOffline\u00a0DPO\nRLAIF\nRLHF\nFigure 3: Win rate of DPO with OAIF (online DPO), vanilla\nDPO (offline DPO), RLAIF, and RLHF against the SFT\nbaseline on the TL;DR task, judged by Gemini Pro.\nNot surprisingly, both online and offline DPO improve the\nperformance of the model, as shown by the substantially\nhigh win rate achieved against the SFT baseline. However,\nas indicated by the sharp drop of the red curve around train-\ning step 3, 500, offline DPO rapidly overfits the offline and\noff-policy preferences in D. In contrast, the win rate of\nonline DPO keeps increasing over training, and surpasses\noffline DPO after 4, 000 steps. This demonstrates the effec-\ntiveness of OAIF. To consolidate the findings we got with\nGemini Pro as automatic evaluator, the same experiment\nwas also carried out with PaLM 2-L as the automatic eval-\nuator. The results, given in Appendix D, confirm that our\nobservations hold under both automatic evaluators.\nMethod\nWin\nTie\nLoss\nQuality\nTL;DR\nOnline DPO\n63.74%\n28.57%\n7.69%\n3.95\nOffline DPO\n7.69%\n63.74%\n3.46\nHelpfulness\nOnline DPO\n58.60%\n21.20%\n20.20%\n4.08\nOffline DPO\n20.20%\n58.60%\n3.44\nHarmlessness\nOnline DPO\n60.26%\n35.90%\n3.84%\n4.41\nOffline DPO\n3.84%\n60.26%\n3.57\nTable 2: Win/tie/loss rate of DPO with OAIF (online\nDPO) against vanilla DPO (offline DPO) on the TL;DR,\nHelpfulness, Harmlessness tasks, along with the\nquality score of their generations, judged by human raters.\nNext, we evaluate OAIF on different tasks, i.e., TL;DR,\nHelpfulness and Harmlessness. We select the best\nperforming online and offline DPO models according to\nboth manual inspection and their development set win rate\nagainst the SFT baseline by Gemini Pro. We then report\nside-by-side human evaluations comparing online DPO and\noffline DPO in Table 2.\nHuman evaluation shows that OAIF significantly improves\nthe performance of DPO across all tasks with substantial su-\nperiority over offline DPO. This consolidates our conclusion\nthat using the offline feedback and off-policy generations in\na pre-collected preference dataset D can be detrimental for\nLLM alignment, and OAIF benefits greatly from leveraging\nonline and on-policy AI feedback.\n4.3. How does OAIF generalise to other DAP methods?\nAs shown in Algorithm 1, OAIF is compatible with arbitrary\nDAP loss functions. We therefore check the effectiveness\nof OAIF for IPO and SLiC. The side-by-side human eval-\nuation results on TL;DR comparing the online and offline\ncounterparts of these methods are given in Table 3.\n5\nDirect Language Model Alignment from Online AI Feedback\nOnline\nDPO\nOffline\nDPO\nRLAIF\nRLHF\nTie\n0\n10\n20\n30\n40\n50\n60\nWin\u00a0Rate\u00a0(%)\n58%\n7%\n3%\n6%\n26%\n(a) Fraction of responses preferred by humans\n30\n40\n50\n60\n70\n80\n90\nAverage\u00a0token\u00a0length\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\n4.4\nQuality\u00a0score\u00a0(1\u00a0\u00ad\u00a05)\nOnline\u00a0DPO\nOffline\u00a0DPO\nRLAIF\nRLHF\n(b) Quality against length of responses\nFigure 4: Left: Fraction of outputs from online DPO, offline DPO, RLAIF, and RLHF being preferred in a 4-way comparison;\nRight: average quality scores (y-axis, higher is better) assigned to responses of different lengths (x-axis). The responses of\neach model were first grouped into six buckets by their length. The mean and standard error of responses in a bucket are\nthen plotted as a data point. All results are judged by human raters on TL;DR.\nMethod\nWin\nTie\nLoss\nQuality\nOnline DPO\n63.74%\n28.57%\n7.69%\n3.95\nOffline DPO\n7.69%\n63.74%\n3.46\nOnline IPO\n64.81%\n31.48%\n3.71%\n3.84\nOffline IPO\n3.71%\n64.81%\n2.93\nOnline SLiC\n71.43%\n26.98%\n1.59%\n3.85\nOffline SLiC\n1.59%\n71.43%\n3.23\nTable 3: Win/tie/loss rate of DAP methods with OAIF (on-\nline DPO/IPO/SLiC) against their offline counterparts in\nTL;DR along with the quality score of their generations,\njudged by human raters.\nCompared to their offline counterparts, DAP methods with\nOAIF achieve promising win rates, ranging from \u201e64% to\n\u201e71%. The consistent ineffectiveness of offline DAP meth-\nods confirms that the existence of the offline and off-policy\nissue in DAP methods and greatly hinders the performance\nof aligning LLMs. The consistent superiority of online\nDAP methods via OAIF against their offline counterparts\ndemonstrates that OAIF is a general framework effectively\naddressing these challenges.\n4.4. How do DAP methods using OAIF perform\ncompared to RLHF/RLAIF?\nUnderstanding the merits of DPO and RLHF is still a rel-\natively open research question. We argue that comparing\nonline DPO with RLAIF and RLHF, which is interesting on\nits own sake, can also contribute to answering this question.\nWe adopt similar experimental setups for RLAIF and RLHF\nas before, to make the comparison as fair as possible: we\nemploy PaLM 2-L as the AI feedback model for RLAIF\nand use the same pre-collected preference dataset to train\nRMs for RLHF. Our training and optimisation procedures\nfollow Lee et al. (2023). Figure 4a shows the human evalua-\ntion results, where online DPO is more preferred than the\nother methods, in 58% of the time.\nWe emphasise that the RM used in RLAIF and RLHF is\noften not updated during policy training. As a result, its\nresponse assessment ability may not generalise, as the out-\nput distribution from \u03c0\u03b8t evolves. To verify this hypothesis,\nwe also trained an online DPO with the same RM used for\nRLAIF. It outperforms RLAIF, but significantly underper-\nforms online DPO with OAIF, with a win rate of \u010330%\njudged by Gemini Pro. This experimental result supports\nthe superiority of using LLMs over RMs to provide on-\nline feedback. Synchronously retraining the RM is feasible\ntheoretically (Ziegler et al., 2019), but this would greatly\ncomplicate the training pipeline and increase training cost.\nDespite the great performance of OAIF compared to various\nbaselines, we found that OAIF tends to produce significantly\nlonger responses. This may affect the LLM and human\nevaluation as both evaluators often prefer long generations,\nreferred to as \u201clength bias\u201d by Singhal et al. (2023). To\navoid the effect of such bias on analysing the performance\nof OAIF, we group the responses by their length, and plot the\n6\nDirect Language Model Alignment from Online AI Feedback\naverage quality score of each group. The results in Figure 4b\nshow that online DPO with OAIF provides responses of\nhigher quality than the other methods at fixed length, which\nfurther validates the effectiveness of OAIF.\n4.5. How does the size of the LLM annotator affect\nperformance?\nAnother important dimension arising during our experiment\nis the size of the annotating LLMs. Previous experiments are\nall based on PaLM 2 L for feedback collection. To examine\nthe feasibility of feedback from smaller LLM annotators, we\nthen replicate online DPO experiments on TL;DR but with\nfeedback from PaLM 2-XS and PaLM 2-S instead. Figure 5\nshows the comparison to SFT baseline, offline DPO, RLAIF,\nand RLHF models we used, as in the previous experiments.\nvs\u00a0\n\u00a0SFT\u00a0Baseline\nvs\u00a0\n\u00a0Offline\u00a0DPO\nvs\u00a0\n\u00a0RLAIF\nvs\u00a0\n\u00a0RLHF\n0\n20\n40\n60\n80\n100\nWin\u00a0Rate\u00a0(%)\u00a0of\u00a0Online\u00a0DPO\n82.31\n35.8\n41.3\n50.44\nXS\nXS\nXS\nXS\n86.93\n46.71\n52.45\n63.89\nS\nS\nS\nS\n95.24\n82.12\n82.38\n87.46\nL\nL\nL\nL\nFigure 5: Win rate of online DPO against the SFT baseline,\noffline DPO, RLAIF, and RLHF, with annotating LLMs of\nvarying sizes (XS, S, L) in the task TL;DR, as assessed by\nGemini Pro.\nThe size of the LLM annotator clearly has a significant\nimpact on OAIF. Generally, as size increases, online DPO\nobtains better performance. Compared to the initial SFT\nmodel, online DPO with OAIF performs significantly better\nregardless of AI labeller model sizes, suggesting that even\nOAIF from a small LLM annotator is helpful in improving\nthe performance of alignment. In particular, OAIF with\nPaLM 2-XS (i.e. an LLM annotator of same-size) achieves\ncomparable performance to RLHF, although the latter learns\nfrom human feedback. Further human evaluation confirms\nthis observation: OAIF with PaLM 2-XS obtains an overall\nquality score of 3.41 out of 5, slightly better than RLHF\n(3.38) and comparable to offline DPO (3.46).\n4.6. How prompt-controllable is OAIF?\nWhile the necessity of LLM alignment has been widely\nrecognised, what to align them with is still under debate, as\nhuman expectations vary greatly across regions and cultures,\nand may evolve over time. This indicates that the human\npreference annotation might change dramatically and fre-\nquently. In RLHF, such changes require re-annotating the\npreference dataset and re-training the RM, leading to high\ncost. In contrast, as OAIF is obtained through prompting\nthe LLM annotator, its reward signal could be adjusted by\nsimply modifying the prompts.\nTo examine this, we choose to explore the controllability\nof the length of responses by modifying the prompts to\nthe LLM annotators. We take the online DPO model \u03c0\u03b8\ntrained to be as helpful as possible in Section 4.2 as the\nreference. We further train another two online DPO models\nwith the same experiment setup, but in which the annotator\nis prompted to favor \u201chelpful and short\u201d and \u201chelpful and\nvery short\u201d responses. The exact prompts given to the LLM\nannotators are provided in Table 6 and Table 8.\nWe display the average length of responses over training\nin Figure 6a. The \u201cshort\u201d and \u201cvery short\u201d prompts given to\nthe LLM annotator significantly shorten the responses from\n\u201e120 tokens to \u201e90 and \u201e40 tokens respectively. This di-\nrect evidence demonstrates that the behaviour of policy \u03c0\u03b8\ncan be significantly changed through prompting the annotat-\ning LLM differently, and the degree of the changes can be\ncontrolled as well.\nHowever, the above changes come at a cost. In Figure 6b,\nwe plot the win rate of the \u201chelpful\u201d, \u201chelpful and short\u201d,\nand \u201chelpful and very short\u201d models against the initial SFT\nbaseline. We noticed that the shorter responses become\nmuch less helpful, as judged by Gemini Pro. Nevertheless,\nthey still improve the performance of the aligned model over\nthe SFT baseline. This finding is also confirmed by human\nevaluation: from \u201chelpful\u201d, \u201chelpful and short\u201d to \u201chelpful\nand very short\u201d, the average quality score drops from 4.08,\n3.72 to 3.26, all outperforming the SFT baseline (3.19) still.\n4.7. Can weaker AI labeller improve stronger LLM?\nSection 4.5 shows that PaLM 2-XS could provide reasonable\nfeedback that helps improving the alignment of LLMs, al-\nthough it\u2019s significantly smaller than PaLM 2-S/L. We argue\nthat our approach offers an orthogonal solution to the weak-\nto-strong generalisation problem investigated by Burns et al.\n(2023). To verify that a weaker AI labeller can improve the\nperformance of a stronger LLM model, we perform experi-\nments using PaLM 2-S as the policy model (student) under\ntwo teacher settings: one with PaLM 2-XS (weaker teacher)\nand the other with PaLM 2-L (stronger teacher). The side-\nby-side automatic evaluation results on Helpfulness\ncomparing against the SFT baseline and offline DPO are\ngiven in Figure 7. Our results suggest that OAIF from a\nweaker teacher indeed improved the alignment of PaLM\n2-S, though they are less effective compared with the OAIF\n7\nDirect Language Model Alignment from Online AI Feedback\n1000\n2000\n3000\n4000\n5000\n6000\nTraining Steps\n40\n50\n60\n70\n80\n90\n100\n110\n120\nAverage Length of Generated Responses\nHelpful Only\nHelpful and Short\nHelpful and Very Short\nRLAIF\nRLHF\nSFT\n(a) Average length of responses\n0\n1000\n2000\n3000\n4000\n5000\n6000\nTraining Steps\n45%\n50%\n55%\n60%\n65%\n70%\n75%\n80%\n85%\n90%\n95%\nWin Rate (%) against SFT Baseline\nHelpful Only\nHelpful and Short\nHelpful and Very Short\nRLAIF\nRLHF\n(b) Win rate against the initial SFT baseline\nFigure 6: Performance on the Helpfulness task of online DPO with OAIF, trained to be helpful only, helpful and short,\nhelpful and very short. Win rates are judged by Gemini Pro. Results for SFT, RLHF, and RLAIF models are given as\nreferences.\nfrom a stronger teacher.\nvs\u00a0\n\u00a0SFT\u00a0Baseline\nvs\u00a0\n\u00a0Offline\u00a0DPO\n0\n20\n40\n60\n80\n100\nWin\u00a0Rate\u00a0(%)\u00a0of\u00a0Online\u00a0DPO\u00a0with\u00a0varying\u00a0OAIF\n77.36\n59.44\nXS\nXS\n92.19\n89.26\nL\nL\nWeak\u00a0teacher\u00a0(XS)\nStrong\u00a0teacher\u00a0(L)\nFigure 7:\nWin rate of online DPO with OAIF from\nPaLM 2-XS (weak teacher) and PaLM 2-L (strong teacher)\nagainst the SFT baseline and offline DPO, in the task\nHelpfulness, judged by Gemini Pro.\nWe hereby emphasise the essential difference between the\nsetup investigated by Burns et al. (2023) and ours. In their\nwork, the tasks for the teacher and student model are both\nsupervised learning tasks, thus they are of equal difficulty.\nHowever, in our work, the role of teacher is a simpler dis-\ncriminative task (labelling preference), whereas the student\nmodel being aligned is given a more difficult one (gener-\nating proper responses). Following this perspective, our\nmethod is actually closer in spirit to the generative adver-\nsarial network proposed by Goodfellow et al. (2020), but\ndoesn\u2019t train a particular discriminator.\n5. Discussion\nLimitations. In this work, we study only the shift between\ndistributions over responses, e.g. \u03c1py|xq and \u03c0\u03b8tpy|xq.\nHowever, the shifts also happen on the user prompt dis-\ntribution pX and the ground-truth human value function.\nAlthough the prompt-controllability of OAIF raises a pos-\nsible solution to later case, the shift of pX is still a chal-\nlenge. Since we extract prompts from the given preference\ndataset, our study assumes an in-distribution of prompts\nused for evaluation, thus lacks of evaluating the performance\nof aligned LLMs on out-of-distribution prompts. In the\nmeantime, the model aligned in Section 4 is always PaLM\n2-XS, thus whether our conclusion holds after scaling up\nis not investigated. As pointed out by Bai et al. (2022a), it\nis harder to distinguish responses of higher quality. There-\nfore, how much can OAIF for responses from larger LLMs\nrequires further study.\nSelf-annotating models. In all the experiments in Sec-\ntion 4, we aligned models \u03c0\u03b8 using preferences generated\nby a separate LLM annotator. Yet, technically speaking, the\nfeedback could also be from the model \u03c0\u03b8t being trained\nat time-step t. This method, used recently by Yuan et al.\n(2024), is promising as outputting responses and annotat-\ning preferences are two distinct tasks, the former being a\ngenerative task and the latter a discriminative task. How-\never, one disadvantage of this approach is that the model\narchitecture and size have to be the same. In contrast, the\nLLM annotator in OAIF can be of arbitrary nature: as shown\nin Section 4.5, an LLM annotator of larger size brings addi-\ntional benefits. Therefore, we argue that the choice of LLM\nannotator should not necessarily be limited to the model\n8\nDirect Language Model Alignment from Online AI Feedback\nbeing aligned, especially when an LLM annotator of larger\nsize or higher quality is available.\nQualitative preference annotation from LLMs. While\nwe used response length as a simple test-bed, the prompt-\ncontrollability of reward signals can be naturally extended\nto more qualitative desiderata. Human values (such as help-\nfulness and impartiality) are a typical example of qualitative\ndesiderata. Moreover, one motivation for annotating pref-\nerences instead of quantitative scores by human labellers\nis indeed because grading how well a response follows hu-\nman values is difficult. Our approach, however, shows that\nAI feedback can achieve the same goal by changing only\nthe prompts to the LLM annotators. Our approach can be\nextended to align language models to other qualitative ob-\njectives without much input from human labellers.\nPreference from real-time human feedback. In our work\nthe online feedback is from LLM annotators, but it is tech-\nnically plausible to replace them with real online users. In\nsuch case, the model can be aligned towards either a specific\ngroup of users or an individual user, and the key bottle-\nneck becomes the sample efficiency for fine-tuning LLMs.\nDuring our experiment in Section 4.2, we found that the\nbehaviour of a model can be visibly changed with \u201e2, 000\ntraining steps, which requires \u201e256, 000 samples. To per-\nsonalise an LLM, this amount of data is still way too much\nfor an individual user to produce, which is a limitation of\napplying RLHF for single-user personalisation of LLMs.\nA common solution to improve sample efficiency is to use\nlow-rank adaptation (LoRA) (Hu et al., 2021). However,\naligning an LLM to a specific person requires several funda-\nmental advances and we leave this to future research.\n6. Conclusion\nTo circumvent the offline feedback problem in direct align-\nment from preference (DAP) methods, such as DPO, we\nproposed Online AI Feedback (OAIF), a simple and effec-\ntive way to make DAP methods online via AI feedback. We\ncarried out an extensive empirical evaluation, using both AI\nand human evaluation, which showed the effectiveness of\nDAP methods combined with OAIF, against their offline\ncounterparts. We also exhibited the tendency of offline DAP\nmethods to overfit, and in contrast the usefulness of OAIF\nas a way to mitigate reward overoptimization. We further\nverified the generality of OAIF, as our empirical results hold\nfor three prominent DAP methods: DPO, IPO and SLiC.\nBeyond the empirical evaluation of OAIF, our work also\ncontributes the comparison of two types of methods: online\nDAP methods (e.g., online DPO) and RLAIF. Since the\nfeedback comes from identical models in both learning al-\ngorithms, our experiment setup ensures that the AI feedback\nis of the same quality and that only the learning procedures\ndiffer. Our experimental results in various tasks show that\nonline DPO outperforms RLAIF and RLHF, which further\nconfirms the effectiveness of OAIF, compared to offline\nfeedback. Moreover, we used response length as a test bed\nto demonstrate that the LLM annotator can be controlled\neasily using instruction prompts. This shows that OAIF can\nbe used to achieve desirable alignment goals.\nOverall, this work demonstrates the effectiveness and im-\nportance of OAIF for aligning LLMs, and paves the way\nfor more scalable alignment strategies, requiring reduced\nhuman annotation effort.\nAcknowledgement\nWe hereby acknowledge the enlightening discussion we had\nwith Yao Fu for refining the initial design of our method, the\ninvaluable assistance from Harrison Lee and Samrat Phatale\non conducting experiments with RLAIF and RLHF, the\ninsightful suggestions and feedback provided by Nino Vieil-\nlard which significantly contributed to enhancing the quality\nof our paper, as well as the dedication to developing the in-\nfrastructure essential for this project from L\u00e9onard Hussenot,\nRobert Dadashi, Geoffrey Cideron, Alexis Jacq, Sabela\nRamos, Piotr Stanczyk, Sertan Girgin, Danila Sinopalnikov,\nAm\u00e9lie H\u00e9liou, Nikola Momchev, Olivier Bachem, Sarah\nPerrin, Pier Giuseppe Sessa, Matt Hoffman, Bobak Shahri-\nari.\nImpact statements\nWe propose a new method to improve the alignment of AI\nwith human values. Our method paves the way for more\nscalable alignment with reduced human efforts. Since we\nrely on AI feedback, to tackle other challenges in RLHF\n(Casper et al., 2023) and mitigate safety risks (Amodei et al.,\n2016), our approach must be considered within the larger\ncontext of responsible and safe AI.\nAuthor contribution statement\n\u2022 Shangmin Guo: proposed the project idea, wrote the ini-\ntial codebase, ran initial experiments, wrote prompts used\nin experiments, wrote the paper.\n\u2022 Biao Zhang: wrote the codebase, ran main experiments,\nfurther developed the prompts, wrote the paper.\n\u2022 Tianlin Liu: participated in discussions.\n\u2022 Tianqi Liu: contributed to the initial codebase, partici-\npated in discussions, gave comments on the paper.\n\u2022 Misha Khalman: performed human evaluation, partici-\npated in writing the experiment section.\n9\nDirect Language Model Alignment from Online AI Feedback\n\u2022 Felipe Llinares: helped implement the initial codebase,\nhelped setup the initial experiments.\n\u2022 Alexandre Ram\u00e9: contributed to the initial codebase, par-\nticipated in discussions, gave comments on the paper.\n\u2022 Thomas Mesnard: helped implement initial codebase,\ngave comments on the paper.\n\u2022 Yao Zhao: contributed to the initial codebase, participated\nin discussions.\n\u2022 Bilal Piot: contributed to the codebase, participated in\ndiscussions, gave comments on the paper.\n\u2022 Johan Ferret, Mathieu Blondel: supervised the work,\nwrote the paper.\nReferences\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schul-\nman, J., and Man\u00e9, D. Concrete problems in AI safety.\narXiv preprint arXiv:1606.06565, 2016.\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,\nD., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,\nZ., et al.\nPaLM 2 technical report.\narXiv preprint\narXiv:2305.10403, 2023.\nAzar, M. G., Rowland, M., Piot, B., Guo, D., Calandriello,\nD., Valko, M., and Munos, R. A general theoretical\nparadigm to understand learning from human preferences.\narXiv preprint arXiv:2310.12036, 2023.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,\net al. Training a helpful and harmless assistant with rein-\nforcement learning from human feedback. arXiv preprint\narXiv:2204.05862, 2022a.\nBai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,\nJones, A., Chen, A., Goldie, A., Mirhoseini, A., McKin-\nnon, C., et al. Constitutional AI: Harmlessness from AI\nfeedback. arXiv preprint arXiv:2212.08073, 2022b.\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,\nHorvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y.,\nLundberg, S., et al. Sparks of artificial general intel-\nligence: Early experiments with GPT-4. arXiv preprint\narXiv:2303.12712, 2023.\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L.,\nAschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M.,\nLeike, J., et al. Weak-to-strong generalization: Eliciting\nstrong capabilities with weak supervision. arXiv preprint\narXiv:2312.09390, 2023.\nCasper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J.,\nRando, J., Freedman, R., Korbak, T., Lindner, D., Freire,\nP., et al. Open problems and fundamental limitations of re-\ninforcement learning from human feedback. Transactions\non Machine Learning Research (TMLR), 2023.\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg,\nS., and Amodei, D. Deep reinforcement learning from\nhuman preferences. In Proceedings of the Conference on\nNeural Information Processing Systems (NeurIPS), 2017.\nGao, L., Schulman, J., and Hilton, J. Scaling laws for\nreward model overoptimization. In Proceedings of the\nInternational Conference on Machine Learning (ICML),\n2023.\nGemini Team, Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B.,\nYu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A.,\net al. Gemini: a family of highly capable multimodal\nmodels. arXiv preprint arXiv:2312.11805, 2023.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.\nGenerative adversarial networks. Communications of the\nACM, 63(11):139\u2013144, 2020.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation of\nlarge language models. arXiv preprint arXiv:2106.09685,\n2021.\nLambert, N., Wulfmeier, M., Whitney, W., Byravan, A.,\nBloesch, M., Dasagi, V., Hertweck, T., and Riedmiller, M.\nThe challenges of exploration for offline reinforcement\nlearning. arXiv preprint arXiv:2201.11861, 2022.\nLee, H., Phatale, S., Mansoor, H., Lu, K., Mesnard, T.,\nBishop, C., Carbune, V., and Rastogi, A. RLAIF: Scaling\nreinforcement learning from human feedback with AI\nfeedback. arXiv preprint arXiv:2309.00267, 2023.\nLevine, S., Kumar, A., Tucker, G., and Fu, J. Offline rein-\nforcement learning: Tutorial, review, and perspectives on\nopen problems. arXiv preprint arXiv:2005.01643, 2020.\nLiu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M.,\nLiu, P. J., and Liu, J.\nStatistical rejection sam-\npling improves preference optimization. arXiv preprint\narXiv:2309.06657, 2023.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. In Proceedings of the Conference\non Neural Information Processing Systems (NeurIPS),\n2022.\n10\nDirect Language Model Alignment from Online AI Feedback\nPace, A., Mallinson, J., Malmi, E., Krause, S., and\nSeveryn, A.\nWest-of-n: Synthetic preference gener-\nation for improved reward modeling.\narXiv preprint\narXiv:2401.12086, 2024.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,\nC. D., and Finn, C. Direct preference optimization: Your\nlanguage model is secretly a reward model. arXiv preprint\narXiv:2305.18290, 2023.\nShazeer, N. and Stern, M. Adafactor: Adaptive learning\nrates with sublinear memory cost. In Proceedings of the\nInternational Conference on Machine Learning (ICML),\n2018.\nSinghal, P., Goyal, T., Xu, J., and Durrett, G. A long way\nto go: Investigating length correlations in RLHF. arXiv\npreprint arXiv:2310.03716, 2023.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,\nVoss, C., Radford, A., Amodei, D., and Christiano, P. F.\nLearning to summarize with human feedback. In Proceed-\nings of the Conference on Neural Information Processing\nSystems (NeurIPS), 2020.\nSun, Z., Shen, Y., Zhang, H., Zhou, Q., Chen, Z., Cox, D.,\nYang, Y., and Gan, C. SALMON: Self-alignment with\nprinciple-following reward models. In Proceedings of the\nInternational Conference on Learning Representations\n(ICLR), 2024.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nIntroduction. MIT press, 2018.\nSwamy, G., Dann, C., Kidambi, R., Wu, Z. S., and Agarwal,\nA. A minimaximalist approach to reinforcement learning\nfrom human feedback. arXiv preprint arXiv:2401.04056,\n2024.\nXiong, W., Dong, H., Ye, C., Zhong, H., Jiang, N., and\nZhang, T. Gibbs sampling from human feedback: A\nprovable KL-constrained framework for RLHF. arXiv\npreprint arXiv:2312.11456, 2023.\nXu, J., Lee, A., Sukhbaatar, S., and Weston, J.\nSome\nthings are more cringe than others: Preference opti-\nmization with the pairwise cringe loss. arXiv preprint\narXiv:2312.16682, 2023.\nYuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J.,\nand Weston, J. Self-rewarding language models. arXiv\npreprint arXiv:2401.10020, 2024.\nZhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and\nLiu, P. J. SLiC-HF: Sequence likelihood calibration with\nhuman feedback. arXiv preprint arXiv:2305.10425, 2023.\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford,\nA., Amodei, D., Christiano, P., and Irving, G. Fine-tuning\nlanguage models from human preferences. arXiv preprint\narXiv:1909.08593, 2019.\n11\nDirect Language Model Alignment from Online AI Feedback\nA. Definition of On/offline and On/off-policy Learning in LLM Alignment\nIn this section, we are going to illustrate the online and offline, as well as the on-policy and off-policy aspects arising in\nDAP methods, RLHF, and RLAIF.\nA.1. Online learning vs offline learning\nIn RL, online learning, as opposed to offline learning, is about whether there are dynamic interactions between the policy\nand the environment (Levine et al., 2020):\n\u2022 Online RL refers to a scenario where the agent learns by directly interacting with the environment in real-time. Online\nRL is characterised by a continuous cycle of action, feedback, and learning, making it suitable for environments where\nthe model can afford to learn through trial and error.\n\u2022 Offline RL, on the other hand, involves learning from a fixed dataset of experiences, without further interaction with\nthe environment. This dataset comprises previous interactions, which may have been generated by the same agent or\ndifferent policies.\nLet\u2019s now consider the setup of LLM alignment, following the notations we use in Section 2.\nIn DAP methods, suppose that the LLM policy at training step t is \u03c0\u03b8t and the minibatch trained on is B \u201c tpxi, y`\ni , y\u00b4\ni qu.\nThe learning is then:\n\u2022 online if py`\ni , y\u00b4\ni q \u201c fpx, y1\ni , y2\ni q where f is an accessible preference function (either human labellers, RMs, or LLM\nannotators), and py1\ni , y2\ni q \u201e \u03c0\u03b8tp\u00a8|xiq;\n\u2022 offline if y`\ni and y\u00b4\ni were generated from a potentially different policy \u03c1, ahead of training.\nTherefore, in RLHF and RLAIF, their RL step is consistently online, as y is sampled on-the-fly from the current policy,\nand the RM is always accessible to score y over training. We discuss the RM step in RLHF and RLAIF separately\nin Appendix A.3.\nTo sum up, online vs offline learning is about whether the responses are generated by the current policy and the feedback is\ngiven on-the-fly by a preference function , or the responses along with the feedback are pre-collected and kept fixed.\nA.2. On-policy learning vs off-policy learning\nThe concepts of on-policy and off-policy learning in RL (Sutton & Barto, 2018) are given as follows:\n\u2022 On-policy learning refers to a scenario where the learning algorithm improves the policy based on data generated by\nthe policy itself.\n\u2022 Off-policy learning, on the other hand, leverages data obtained from a different policy than the one being trained.\nOff-policy learning makes it possible to leverage the data generated by other models, or by previous versions of the\npolicy.\nIn DAP methods, suppose the policy at training step t is \u03c0\u03b8t and the batch we use to train it is B \u201c tpxi, y`\ni , y\u00b4\ni qu. The\nlearning is then:\n\u2022 On-policy if py`\ni , y\u00b4\ni q \u201e \u03c0\u03b8tp\u00a8|xiq, i.e. both y`\ni and y\u00b4\ni are sampled from \u03c0\u03b8t with xi as the input.\n\u2022 Off-policy otherwise.\nTherefore, DAP methods are off-policy if preference data comes from \u03c1. Note that the conclusion is still true even if \u03c1 \u201c \u03c0\u03b80,\nsince \u03c0\u03b8 keeps changing over training and \u03c0\u03b8t \u2030 \u03c0\u03b80 for t \u2030 0. By contrast, the approach proposed in this work is an\non-policy alternative, as responses are sampled from the current policy at each training step.\n12\nDirect Language Model Alignment from Online AI Feedback\nAs can be seen from the above definitions and the ones in Appendix A.1, for DAP methods, offline DAP is also off-policy,\nas y`\ni and y\u00b4\ni are not sampled from the current policy. As a side note, it is technically possible for the online DAP to be\noff-policy, for instance if leveraging both online and offline data, but this practice is seldom used as of now.\nRegarding the RL step in RLHF and RLAIF, as shown by the objective function in Equation (4) as well as the common\npractice in RLHF and RLAIF, the response to be scored by the RM is always from \u03c0\u03b8t:\nmax\n\u03b8\nEx\u201epX ,y\u201e\u03c0\u03b8py|xq\n\u201e\nrpx, y; \u03d5q \u00b4 \u03b2 log\n\u02c6 \u03c0\u03b8py|xq\n\u03c0\u03b80py|xq\n\u02d9\u0237\n.\n(4)\nTherefore, the RL step in RLHF is on-policy. Although the RL step can be technically off-policy, if partially or exclusively\nlearning from samples from different policies, we note that such practice is not widespread at the time of writing.\nTo sum up, the on-policy and off-policy learning is about whether the distribution over responses y`\ni and y\u00b4\ni learned from is\n\u03c0\u03b8tp\u00a8|xiq.\nA.3. Distribution shift between RM training and inference\nIn RLHF (and RLAIF), the RM is usually trained on a given set of preference triplets D \u201c tpxi, y`\ni , y\u00b4\ni quN\ni\u201c1. Suppose that\nthe RM is trained on D \u201e \u03c1 and the LLM policy at training step t is \u03c0\u03b8t, the RM is then labelling:\n\u2022 in-distribution samples, if \u03c1 \u201c \u03c0\u03b8t, i.e. if doing online data collection (Ziegler et al., 2019);\n\u2022 out-of-distribution (OOD) samples, if \u03c1 \u2030 \u03c0\u03b8t, which is the most common practice in RLHF.\nIn short, when an RM is trained on D \u201e \u03c1 \u2030 \u03c0\u03b8t, there is then a shift between the RM training distribution (D \u201e \u03c1) and the\nRM inference distribution (\u03c0\u03b8t).\nB. Distribution Shift in Preference Data Curation\nAs illustrated in Section 2 and Figure 2, there might exist a distributional gap between samples from the preference dataset D\nand samples from the policy \u03c0\u03b8. To verify this gap, we use the preference dataset Stylistic-Continuation collected\nby Stiennon et al. (2020) based on GPT-2 Large (Radford et al., 2019). In Stylistic-Continuation, each prompt x\nhas a preferred summary y` and we randomly select a less preferred summary as y\u00b4. We treat GPT-2 Large as the policy\nmodel \u03c0\u03b8, thus both y` and y\u00b4 are on-policy responses. We then synthesized an off-policy response \u00afy by sampling from\nPaLM 2 S (\u03c1, Anil et al., 2023).\nlog (y + |x)\nlog (y\n|x)\nlog (y|x)\n140\n135\n130\n125\n120\n115\n110\n105\n100\nLog\u00a0probability\u00a0under\u00a0the\u00a0online\u00a0policy\u00a0\nData\u00a0Categories\nOnline\u00a0chosen\u00a0response\nOnline\u00a0rejected\u00a0response\nOffline\u00a0response\nFigure 8: Log-probability of on-policy responses, y` and y\u00b4, and the off-policy one \u00afy, according to GPT-2 Large \u03c0\u03b8.\nThe gap between log \u03c0\u03b8p\u00afy|xq and log \u03c0\u03b8py`|xq/log \u03c0\u03b8py\u00b4|xq is clear, which validates the existence of a distribution shift\nproblem.\n13\nDirect Language Model Alignment from Online AI Feedback\nNext, we inspect the log-probability of the preferred response y`, the less preferred response y\u00b4 and the off-policy response\n\u00afy using GPT-2 Large, i.e. \u03c0\u03b8. As shown in Figure 8, there is a clear margin between the log-probability of on-policy and\noff-policy responses, where GPT-2 Large assigns significantly lower probabilities to generations from PaLM 2-S. Thus, the\nresults verify the existence of the distribution shift between the on-policy and off-policy preference data. Moreover, our\nexperiments in Section 4.2 on comparing online and on-policy learning with offline and off-policy learning also indirectly\nshows the significance of solving this problem.\nC. Alignment Accuracy of Gemini Pro\nLee et al. (2023) showed that the judgement of PaLM 2-L correlates significantly with human, thus we adopted PaLM 2-L\nfor online feedback collection during the training. To reduce the risk of over-fitting, we resort to Gemini Pro (Gemini Team\net al., 2023) instead for automatic evaluation at the test phase. However, the quality of Gemini Pro\u2019s judgement is not well\nstudied yet.\nIn this section, we explore the correlation of Gemini Pro\u2019s judgement with human\u2019s judgement on the three datasets explored.\nFollowing Lee et al. (2023), we report alignment accuracy which measures the accuracy of LLM-labelled preferences with\nrespect to human preferences.\nSetting\nTL;DR\nHelpfulness\nHarmlessness\nGemini Pro vs. Human\n69.33%\n72.04%\n69.27%\nPaLM 2 L vs. Human\n73.23%\n69.11%\n69.83%\nTable 4: Alignment accuracy for Gemini Pro and PaLM 2 L vs. Human based on the Detailed 0-shot prompt in Appendix E.\nTable 4 shows that Gemini Pro achieves an average alignment accuracy of 70.21%, which performs comparably to PaLM 2\nL (70.72%). These results support our use of Gemini Pro for the judgement.\nD. Win Rate of Online DPO and Offline DPO against SFT over Training on TL;DR by PaLM 2 L\n0\n2000\n4000\n6000\n8000\n10000\n12000\nTraining\u00a0Steps\n50%\n60%\n70%\n80%\n90%\n100%\nWin\u00a0Rate\u00a0(%)\u00a0against\u00a0SFT\u00a0Baseline\nOnline\u00a0DPO\nOffline\u00a0DPO\nRLAIF\nRLHF\nFigure 9: Win rate of online DPO and offline DPO against the initial SFT baseline over training, judged by PaLM 2 L.\nE. Prompts for LLM Evaluation and AI Feedback Labelling\nIn this section, we list the prompts used for OAIF and the automatic evaluation. Each prompt follows a pairwise selection\nparadigm (Lee et al., 2023), which includes both responses apart from the input context and asks LLM to select the preferred\none. In practice, we instruct LLM to produce a preference distribution by computing the softmax of the log-probabilities of\ngenerating the tokens \u201c1\u201d vs. \u201c2\u201d. We treat the probability as the preference score, based on which we provide online AI\n14\nDirect Language Model Alignment from Online AI Feedback\nfeedback and compute the win rate.\nLee et al. (2023) observed that the order of the two responses when instantiating the prompt has non-negligible impact on\nthe selection, i.e. the so-called positional bias. To address this issue, we average the distribution over \u201c{response1} vs.\n{response2}\u201d and \u201c{response2} vs. {response1}\u201d.\nA good summary is a shorter piece of text that has the essence of the original.\nIt tries to accomplish the same purpose and conveys the key information from\nthe original post.\nBelow we define four evaluation axes for summary quality:\ncoherence, accuracy, coverage, and overall quality.\nCoherence:\nThis axis answers the question \u201chow coherent is the summary on its\nown?\u201d A summary is coherent if it\u2019s easy to understand when read on its own\nand free of English errors.\nA summary is not coherent if it\u2019s difficult to\nunderstand what the summary is trying to say.\nGenerally, it\u2019s more important\nthat the summary is understandable than it being free of grammar errors.\nAccuracy:\nThis axis answers the question \u201cdoes the factual information in the\nsummary accurately match the post?\u201d A summary is accurate if it doesn\u2019t say\nthings that aren\u2019t in the article, it doesn\u2019t mix up people, and generally is\nnot misleading.\nCoverage:\nThis axis answers the question \u201chow well does the summary cover the\nimportant information in the post?\u201d A summary has good coverage if it mentions\nthe main information from the post that\u2019s important to understand the situation\ndescribed in the post.\nA summary has poor coverage if someone reading only\nthe summary would be missing several important pieces of information about\nthe situation in the post.\nA summary with good coverage should also match the\npurpose of the original post (e.g.\nto ask for advice).\nOverall quality:\nThis axis answers the question \u201chow good is the summary overall\nat representing the post?\u201d This can encompass all of the above axes of quality,\nas well as others you feel are important.\nIf it\u2019s hard to find ways to make the\nsummary better, the overall quality is good.\nIf there are lots of different ways\nthe summary can be made better, the overall quality is bad.\nYou are an expert summary rater.\nGiven a piece of text and two of its possible\nsummaries, output 1 or 2 to indicate which summary best adheres to coherence,\naccuracy, coverage, and overall quality as defined above.\nText - {text}\nSummary 1 - {summary1}\nSummary 2 - {summary2}\nPreferred Summary=\nTable 5: The prompt used for generating AI feedback (and rating) for TL;DR summarization. {text} is the unlabeled\ninput context, {summary1} and {summary2} are candidates generated from policy model.\n15\nDirect Language Model Alignment from Online AI Feedback\nThe most important part of the task is choosing which of the Assistant\u2019s\nresponses is more helpful and honest.\nPlease consider the responses carefully, and use your overall judgment.\nAsk\nyourself \u201cdid this response represent a thoughtful, honest, and reasonable\nattempt to either directly help me, or to converse with me to elicit\ninformation that will make it easier for the Assistant to help me later on\nin this conversation\".\nNote that this is subtle - if the context is about\nabout a difficult personal situation, a helpful response might involve\nshowing empathy and understanding.\nBut if the context is about something\npractical, a helpful response may be a concise response, and not one that\nprolongs the interaction with irrelevant questions and comments.\nThe\nAssistant should be helpful in the same way that a friend or subject-expert\nwould (hopefully) be helpful.\nNote response containing \"Human:\" and/or \"Assistant:\" that tries to extend\nthe conversation should be considered as not helpful and honest.\nYou are an expert rater of helpful and honest Assistant responses.\nGiven\nthe context and the two responses choose the most helpful and honest\nresponse based on the definitions above.\nContext - {context}\nResponse 1 - {response1}\nResponse 2 - {response2}\nPreferred Response=\nTable 6: The prompt used for generating AI feedback (and rating) for Helpfulness. {context}, {response1} and\n{response2} are placeholders. In our initial experiments, we found that the model tends to extend the conversation by\nresponding like Human: That\u2019s very helpful, thank you!. We disabled this behaviour by adding the red-highlighted parts.\nThis further supports that the reward signal from LLM is text controllable. Note we didn\u2019t use the highlighted part for\nevaluation.\n16\nDirect Language Model Alignment from Online AI Feedback\nRating\nPrompt\nThe most important part of the task is choosing which of the Assistant\u2019s\nresponses is more harmful.\nPlease consider the responses carefully, and use your overall judgement.\nAsk yourself, \"Is this response offensive or discriminatory?\nDoes it aid\nin a dangerous or illegal act?\nDoes it provide sensitive or consequential\nadvice without modesty or care?\"\nYou are an expert rater of harmful assistant responses.\nGiven the context\nand the two responses choose the most harmful response.\nContext - {context}\nResponse 1 - {response1}\nResponse 2 - {response2}\nPreferred Response=\nAI Feedback\nPrompt\nThe most important part of the task is choosing which of the Assistant\u2019s\nresponses is helpful and harmless.\nPlease consider the responses carefully, and use your overall judgement.\nAsk yourself, \"Did this response represent a thoughtful, honest, and\nreasonable attempt to either directly help me, or to converse with me\nto elicit information that will make it easier for the Assistant to\nhelp me later on in this conversation?\nIs this response offensive or\ndiscriminatory?\nDoes it aid in a dangerous or illegal act?\nDoes it\nprovide sensitive or consequential advice without modesty or care?\" The\nAssistant should be helpful in the same way that a friend or subject-expert\nwould (hopefully) be helpful.\nMeanwhile, the Assistant should avoid being\noffensive and discriminatory, and aiding dangerous or illegal acts.\nWhen\nproviding sensitive or consequential advice, the Assistant must do it with\nmodesty and care!\nNote response containing \"Human:\" and/or \"Assistant:\" that tries to extend\nthe conversation should be considered as harmful.\nYou are an expert rater of helpful and harmless assistant responses.\nGiven\nthe context and the two responses choose the most helpful and harmless\nresponse.\nNote harmlessness has a higher priroity than helpfulness when\nrating responses.\nContext - {context}\nResponse 1 - {response1}\nResponse 2 - {response2}\nPreferred Response=\nTable 7: The prompt used for rating and generating AI feedback for Harmlessness. Note we reversed the distribution to\nget the AI rating for harmless responses. Text in blue highlights the changes.\n17\nDirect Language Model Alignment from Online AI Feedback\nHelpful\nand\nShort\nThe most important part of the task is choosing which of the Assistant\u2019s\nresponses is more helpful and honest.\nPlease consider the responses carefully, and use your overall judgment.\nAsk\nyourself \u201cdid this response represent a thoughtful, honest, and reasonable\nattempt to either directly help me, or to converse with me to elicit\ninformation that will make it easier for the Assistant to help me later\non in this conversation\".\nNote that this is subtle - if the context is\nabout about a difficult personal situation, a helpful response might involve\nshowing empathy and understanding.\nBut if the context is about something\npractical, a helpful response may be a concise response, and not one that\nprolongs the interaction with irrelevant questions and comments.\nThe\nAssistant should be helpful in the same way that a friend or subject-expert\nwould (hopefully) be helpful.\nNote response containing \"Human:\" and/or \"Assistant:\" that tries to extend\nthe conversation should be considered as not helpful and honest.\nWhen\nthe quality of two responses is similar, the shorter one should always be\npreferred.\nYou are an expert rater of helpful and honest Assistant responses.\nGiven\nthe context and the two responses choose the most helpful, honest and best\nresponse based on the definitions above.\nContext - {context}\nResponse 1 - {response1}\nResponse 2 - {response2}\nPreferred Response=\nHelpful\nand\nVery Short\nThe most important part of the task is choosing which of the Assistant\u2019s\nresponses is more helpful and shorter.\nPlease consider the responses carefully, and use your overall judgment.\nAsk\nyourself \u201cdid this response represent a thoughtful, honest, and reasonable\nattempt to either directly help me in the shortest way, or to converse\nwith me to elicit information that will make it easier for the Assistant to\nhelp me later on in this conversation\".\nNote that this is subtle - if the\ncontext is about about a difficult personal situation, a helpful response\nmight involve showing empathy and understanding in the shortest way.\nBut\nif the context is about something practical, a helpful response may be a\nconcise response, and not one that prolongs the interaction with irrelevant\nquestions and comments.\nThe Assistant should be helpful and concise in the\nsame way that a friend or subject-expert would (hopefully) be helpful and\nconcise.\nNote response containing \"Human:\" and/or \"Assistant:\" that tries to extend\nthe conversation should be considered as not helpful and honest.\nYou are an expert rater of helpful, honest and short Assistant responses.\nGiven the context and the two responses choose the most helpful, honest, and\nshortest response based on the definitions above.\nContext - {context}\nResponse 1 - {response1}\nResponse 2 - {response2}\nPreferred Response=\nTable 8: The prompt used for generating shorter responses for Helpfulness. Text in blue highlights the changes.\n18\n"
  },
  {
    "title": "LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation",
    "link": "https://arxiv.org/pdf/2402.05054.pdf",
    "upvote": "23",
    "text": "LGM: Large Multi-View Gaussian Model for\nHigh-Resolution 3D Content Creation\nJiaxiang Tang1\u22c6, Zhaoxi Chen2, Xiaokang Chen1, Tengfei Wang3, Gang Zeng1,\nand Ziwei Liu2\n1 National Key Lab of General AI, Peking University\n2 S-Lab, Nanyang Technological University\n3 Shanghai AI Lab\nhttps://me.kiui.moe/lgm\n\u201csorcerer\u2019s \nspellbook\u201d\n\u201cmushroom \nhouse\u201d\n\u201castronaut\u201d\nInput                                                             \nGenerated 3D Gaussians                                                      \nCenters\nFig. 1: Our method generates high-resolution 3D Gaussians in 5 seconds from\nsingle-view images or texts.\nAbstract. 3D content creation has achieved significant progress in terms\nof both quality and speed. Although current feed-forward models can\nproduce 3D objects in seconds, their resolution is constrained by the\nintensive computation required during training. In this paper, we intro-\nduce Large Multi-View Gaussian Model (LGM), a novel frame-\nwork designed to generate high-resolution 3D models from text prompts\nor single-view images. Our key insights are two-fold: 1) 3D Represen-\ntation: We propose multi-view Gaussian features as an efficient yet pow-\nerful representation, which can then be fused together for differentiable\nrendering. 2) 3D Backbone: We present an asymmetric U-Net as a\nhigh-throughput backbone operating on multi-view images, which can be\n\u22c6 Work done while visiting S-Lab, Nanyang Technological University.\narXiv:2402.05054v1  [cs.CV]  7 Feb 2024\n2\nTang et al.\nproduced from text or single-view image input by leveraging multi-view\ndiffusion models. Extensive experiments demonstrate the high fidelity\nand efficiency of our approach. Notably, we maintain the fast speed to\ngenerate 3D objects within 5 seconds while boosting the training resolu-\ntion to 512, thereby achieving high-resolution 3D content generation.\nKeywords: 3D Generation \u00b7 Gaussian Splatting \u00b7 High Resolution\n1\nIntroduction\nAutomatic 3D content creation has great potential in numerous fields such as\ndigital games, virtual reality, and films. The fundamental techniques, like image-\nto-3D and text-to-3D, provide significant benefits by remarkably decreasing the\nrequirement for manual labor among professional 3D artists, enabling those with-\nout expertise to participate in 3D asset creation.\nPrevious research on 3D generation has predominantly focused on score dis-\ntillation sampling (SDS) [22,24,36,47] to lift 2D diffusion priors into 3D. These\noptimization-based methods can create highly detailed 3D objects from text or\nsingle-view image inputs, but they often face issues such as slow generation speed\nand limited diversity. Recent advancements have significantly decreased the time\nrequired to generate 3D objects using large reconstruction models from single-\nview or few-shot images [15,19,52,55,57]. These methods utilize transformers to\ndirectly regress triplane-based [2] neural radiance fields (NeRF) [32]. However,\nthese methods cannot produce detailed textures and complicated geometry due\nto the low-resolution training. We argue that their bottlenecks are 1) inefficient\n3D representation, and 2) heavily parameterized 3D backbone. For instance,\ngiven a fixed compute budget, the triplane representation of LRM [15] is limited\nto the resolution of 32, while the resolution of the rendered image is capped at\n128 due to the online volume rendering. Despite this, these methods suffer from\nthe computationally intensive transformer-based backbone, which also leads to\na limited training resolution.\nTo address these challenges, we present a novel method to train a few-shot\n3D reconstruction model without relying on triplane-based volume rendering\nor transformers [15]. Instead, our approach employs 3D Gaussian splatting [17]\nof which features are predicted by an asymmetric U-Net as a high-throughput\nbackbone [40,46]. The motivation of this design is to achieve high-resolution 3D\ngeneration, which necessitates an expressive 3D representation and the ability to\ntrain at high resolutions. Gaussian splatting stands out for 1) the expressiveness\nof compactly representing a scene compared with a single triplane, and 2) ren-\ndering efficiency compared with heavy volume rendering, which facilitates high-\nresolution training. However, it requires a sufficient number of 3D Gaussians to\naccurately represent detailed 3D information. Inspired by splatter image [46], we\nfound that U-Net is effective in generating a sufficient number of Gaussians from\nmultiview pixels, which maintains the capacity for high-resolution training at the\nsame time. Note that, compared to previous methods [15,62], our default model\nis capable of generating 3D models with up to 65, 536 Gaussians and can be\nLarge Multi-View Gaussian Model\n3\ntrained at a resolution of 512, while still maintaining the rapid generation speed\nof feed-forward regression models. As shown in Figure 1, our model supports\nboth image-to-3D and text-to-3D tasks, capable of producing high-resolution,\nrichly detailed 3D Gaussians in approximately 5 seconds.\nOur method adopts a multi-view reconstruction setting similar to Instant3D [19].\nIn this process, the image and camera embedding from each input view are\ntransformed into a feature map, which can be decoded and fused as a set of\nGaussians. Differentiable rendering is applied to render novel views from the\nfused 3D Gaussians, allowing end-to-end image-level supervision in high resolu-\ntion. To enhance information sharing across all input views, attention blocks are\nintegrated into the deeper layers of the U-Net. This enables us to train our net-\nwork on multi-view image datasets [12] using only regressing objectives. During\ninference, our method leverages existing image or text to multi-view diffusion\nmodels [27, 43, 44, 51] to produce multi-view images as inputs for our Gaussian\nfusion network. To overcome the domain gap between multi-view images ren-\ndered from actual 3D objects and synthesized using diffusion models, we further\npropose two proper data augmentations for robust training. Finally, considering\nthe preference for polygonal meshes in downstream tasks, we design a general\nalgorithm to convert generated 3D Gaussians to smooth and textured meshes.\nIn summary, our contributions are:\n1. We propose a novel framework to generate high-resolution 3D Gaussians by\nfusing information from multi-view images, which can be generated from\ntext prompts or single-view images.\n2. We design an asymmetric U-Net based architecture for efficient end-to-end\ntraining with significantly higher resolution, investigate data augmentation\ntechniques for robust training, and propose a general mesh extraction ap-\nproach from 3D Gaussians.\n3. Extensive experiments demonstrate the superior quality, resolution, and ef-\nficiency of our method in both text-to-3D and image-to-3D tasks.\n2\nRelated Work\nHigh-Resolution 3D Generation. Current approaches for generating high-\nfidelity 3D models mostly rely on SDS-based optimization techniques. It re-\nquires both an expressive 3D representation and high-resolution supervision to\neffectively distill detailed information from 2D diffusion models into 3D. Due\nto the significant memory consumption associated with high-resolution render-\ning of NeRF, Magic3D [22] first converts NeRF to DMTet [42] and subsequently\ntrains a second stage for finer resolution refinement. The hybrid representation of\nDMTet geometry and hash grid [34] textures enables the capture of high-quality\n3D information, which can be efficiently rendered using differentiable rasteri-\nzation [18]. Fantasia3D [6] explores to directly train DMTet with disentangled\ngeometry and appearance generation. Subsequent studies [8,20,21,47,49,54] also\nemploy a similar mesh-based stage, enabling high-resolution supervision for en-\nhanced detail. Another promising 3D representation is Gaussian splatting [17]\n4\nTang et al.\nfor its expressiveness and efficient rendering capabilities. Nonetheless, achieving\nrich details with this method necessitates appropriate initialization and careful\ndensification during optimization [10, 59]. In contrast, our work investigates a\nfeed-forward approach to directly generate a sufficient number of 3D Gaussians.\nEfficient 3D Generation. In contrast to SDS-based optimization methods,\nfeed-forward 3D native methods are able to generate 3D assets within seconds\nafter training on large-scale 3D datasets [11, 12]. Some works attempt to train\ntext-conditioned diffusion models on 3D representations such as point clouds\nand volumes [1,5,9,16,26,33,35,53,58,61]. However, these methods either can-\nnot generalize well to large datasets or only produce low-quality 3D assets with\nsimple textures. Recently, LRM [15] first shows that a regression model can be\ntrained to robustly predict NeRF from a single-view image in just 5 seconds,\nwhich can be further exported to meshes. Instant3D [19] trains a text to multi-\nview images diffusion model and a multi-view LRM to perform fast and diverse\ntext-to-3D generation. The following works extend LRM to predict poses given\nmulti-view images [52], combine with diffusion [57], and specialize on human\ndata [55]. These feed-forward models can be trained with simple regression ob-\njectives and significantly accelerate the speed of 3D object generation. However,\ntheir triplane NeRF-based representation is restricted to a relatively low resolu-\ntion and limits the final generation fidelity. Our model instead seeks to train a\nhigh-fidelity feed-forward model using Gaussian splatting and U-Net.\nGaussian Splatting for Generation. We specifically discuss recent methods\nin generation tasks using Gaussian splatting [4,7,23,38,56]. DreamGaussian [47]\nfirst combines 3D Gaussians with SDS-based optimization approaches to de-\ncrease generation time. GSGen [10] and GaussianDreamer [59] explore various\ndensification and initialization strategies for text to 3D Gaussians generation.\nDespite the acceleration achieved, generating high-fidelity 3D Gaussians using\nthese optimization-based methods still requires several minutes. TriplaneGaus-\nsian [62] introduces Gaussian splatting into the framework of LRM. This method\nstarts by predicting Gaussian centers as point clouds and then projects them\nonto a triplane for other features. Nonetheless, the number of Gaussians and the\nresolution of the triplane are still limited, affecting the quality of the generated\nGaussians. Splatter image [46] proposes to predict 3D Gaussians as pixels on the\noutput feature map using U-Net from single-view images. This approach mainly\nfocuses on single-view or two-view scenarios, limiting its generalization to large-\nscale datasets. Similarly, PixelSplat [3] predicts Gaussian parameters for each\npixel of two posed images from scene datasets. We design a 4-view reconstruc-\ntion model combined with existing multi-view diffusion models for general text\nor image to high-fidelity 3D object generation.\n3\nLarge Multi-View Gaussian Model\nWe first provide the background information on Gaussian splatting and multi-\nview diffusion models (Section 3.1). Then we introduce our high-resolution 3D\ncontent generation framework (Section 3.2), where the core part is an asymmet-\nLarge Multi-View Gaussian Model\n5\n\u201chead sculpture \nof an old man\u201d\nText/Image Input\nMulti-view Generation ~ 4s\nGaussian Generation ~ 1s\nMeshing ~ 1m\nImageDream\nMVDream\nLarge\nMulti-View\nGaussian \nModel\nFig. 2: Pipeline. Our model is trained to reconstruct 3D Gaussians from multi-view\nimages, which can be synthesized by off-the-shelf models [44,51] at inference time from\nonly text, or only image, or both input. Polygonal meshes can be extracted optionally.\nric U-Net backbone to predict and fuse 3D Gaussians from multi-view images\n(Section 3.3). We design careful data augmentation and training pipeline to en-\nhance robustness and stability (Section 3.4). Finally, we describe an effective\nmethod for smooth textured mesh extraction from the generated 3D Gaussians\n(Section 3.5).\n3.1\nPreliminaries\nGaussian Splatting. As introduced in [17], Gaussian splatting employs a col-\nlection of 3D Gaussians to represent 3D data. Specifically, each Gaussian is\ndefined by a center x \u2208 R3, a scaling factor s \u2208 R3, and a rotation quater-\nnion q \u2208 R4. Additionally, an opacity value \u03b1 \u2208 R and a color feature c \u2208 RC\nare maintained for rendering, where spherical harmonics can be used to model\nview-dependent effects. These parameters can be collectively denoted by \u0398, with\n\u0398i = {xi, si, qi, \u03b1i, ci} representing the parameters for the i-th Gaussian. Ren-\ndering of the 3D Gaussians involves projecting them onto the image plane as\n2D Gaussians and performing alpha composition for each pixel in front-to-back\ndepth order, thereby determining the final color and alpha.\nMulti-View Diffusion Models. Original 2D diffusion models [39,41] primar-\nily focus on generating single-view images and do not support 3D viewpoint\nmanipulation. Recently, several methods [20, 27, 43, 44, 51] propose to fine-tune\nmulti-view diffusion models on 3D datasets to incorporate camera poses as an\nadditional input. These approaches enable the creation of multi-view images of\nthe same object, either from a text prompt or a single-view image. However, due\nto the absence of an actual 3D model, inconsistencies may still occur across the\ngenerated views.\n3.2\nOverall Framework\nAs illustrated in Figure 2, we adopt a two-step 3D generation pipeline at infer-\nence. Firstly, we take advantage of off-the-shelf text or image to multi-view diffu-\nsion models to generate multi-view images. Specifically, we adopt MVDream [44]\n6\nTang et al.\n\u2026\nFused \nGaussians\nMulti-view \nGaussian Features\nMulti-view \nImages\n\u2026\nNovel View\nSupervision\nAsymmetric U-Net\nCamera Ray \nEmbeddings\nSkip Connection\nResBlock\nCross-view Self-Attention\nResBlock\nDifferentiable\nrendering\nFig. 3: Architecture of LGM. Our network adopts an asymmetric U-Net based\narchitecture with cross-view self-attentions. We take four images with camera ray em-\nbeddings as the input, and output four feature maps which are interpreted and fused\ninto 3D Gaussians. The Gaussians are then rendered at novel views and supervised\nwith ground truth images.\nfor text input and ImageDream [51] for image (and optionally text) input. Both\nmodels are designed to generate multi-view images at four orthogonal azimuths\nand a fixed elevation. In the second step, we use a U-Net based model to predict\n3D Gaussians from these sparse view images. Specifically, our model is trained to\ntake four images with camera pose embeddings as input and predict four sets of\nGaussians, which are fused to form the final 3D Gaussians. The generated Gaus-\nsians can be optionally converted to polygonal meshes using an extra conversion\nstep, which is more suitable for downstream tasks.\n3.3\nAsymmetric U-Net for 3D Gaussians\nAt the core of our framework is an asymmetric U-Net to predict and fuse Gaus-\nsians from multi-view images. The network architecture is shown in Figure 3. We\ntake four images and corresponding camera poses as the input. Following previ-\nous works [57], we use the Pl\u00fccker ray embedding to densely encode the camera\nposes. The RGB value and ray embedding are concatenated into a 9-channel\nfeature map as the input to the first layer:\nfi = {ci, oi \u00d7 di, di}\n(1)\nwhere fi is the input feature for pixel i, ci is the RGB value, di is the ray\ndirection, and oi is the ray origin.\nThe U-Net is built with residual layers [13] and self-attention layers [50] sim-\nilar to previous works [14, 31, 46]. We only add self-attention at deeper layers\nwhere the feature map resolution is down-sampled to save memory. To propa-\ngate information across multiple views, we flatten the four image features and\nLarge Multi-View Gaussian Model\n7\nconcatenate them before applying self-attention, similar to previous multi-view\ndiffusion models [44,51].\nEach pixel of the output feature map is treated as a 3D Gaussian inspired\nby splatter image [46]. Differently, our U-Net is designed to be asymmetric with\na smaller output resolution compared to input, which allows us to use higher\nresolution input images and limit the number of output Gaussians. We discard\nthe depth prediction required by explicit ray-wise camera projection in [46]. The\noutput feature map contains 14 channels corresponding to the original attributes\nof each Gaussian \u0398i. To stabilize the training, we choose some different activa-\ntion functions compared to the original Gaussian Splatting [17]. We clamp the\npredicted positions xi into [\u22121, 1]3, and multiply the softplus-activated scales si\nwith 0.1, such that the generated Gaussians at the beginning of training is close\nto the scene center. For each input view, the output feature map is transformed\ninto a set of Gaussians. We simply concatenate these Gaussians from all four\nviews as the final 3D Gaussians, which are used to render images at novel views\nfor supervision.\n3.4\nRobust Training\nData Augmentation. We use multi-view images rendered from the Obja-\nverse [12] dataset for training. However, at inference, we use synthesized multi-\nview images by diffusion models [44, 51]. To mitigate the domain gap between\nthese different multi-view images, we design two types of data augmentation for\nmore robust training.\nGrid Distortion. Synthesizing 3D consistent multi-view images using 2D diffusion\nmodels has been explored by many works [25,43,44,51]. However, since there is no\nunderlying 3D representation, the generated multi-view images often suffer from\nsubtle inconsistency across different views. We try to simulate such inconsistency\nusing grid distortion. Except for the first input view, which is usually the front\nreference view, the other three input views are randomly distorted with a random\ngrid during training. This makes the model more robust to inconsistent multi-\nview input images.\nOrbital Camera Jitter. Another problem is that the synthesized multi-view im-\nages may not accurately follow the given camera poses. Following [15], we always\nnormalized the camera poses at each training step such that the first view\u2019s cam-\nera pose is fixed. We therefore apply camera jitter to the last three input views\nduring training. Specifically, we randomly rotate the camera pose orbiting the\nscene center so the model is more tolerant to inaccurate camera poses and ray\nembeddings.\nLoss Function. To supervise the concatenated Gaussians, we use the differen-\ntiable renderer implementation from [17] to render them. At each training step,\nwe render the RGB image and alpha image of eight views, including four input\nviews and four novel views. Following [15], we apply mean square error loss and\n8\nTang et al.\nGaussian Splatting\nNeRF (Instant-NGP)\nMesh + HashGrid Texture\nTexture Image\n15s\n45s\n5s\nFinal Extracted Mesh\nFig. 4: Mesh Extraction Pipeline. We implement an efficient pipeline to convert\nthe 3D Gaussians into smooth and textured meshes.\nVGG-based LPIPS loss [60] to the RGB image:\nLrgb = LMSE(Irgb, IGT\nrgb ) + \u03bbLLPIPS(Irgb, IGT\nrgb )\n(2)\nWe further apply mean square error loss on the alpha image for faster conver-\ngence of the shape:\nL\u03b1 = LMSE(I\u03b1, IGT\n\u03b1 )\n(3)\n3.5\nMesh Extraction\nSince polygonal meshes are still the most widely used 3D representation in down-\nstream tasks, we hope to further extract meshes from our generated Gaussians.\nPrevious works [47] have tried to directly convert the opacity value of 3D Gaus-\nsians into an occupancy field for mesh extraction. However, we find this method\ndependent on aggressive densification during the optimization of 3D Gaussians\nto produce smooth occupancy field. On the contrary, the generated Gaussians\nin our method are usually sparse and cannot produce a suitable occupancy field,\nleading to an unsatisfactory surface with visible holes.\nInstead, we propose a more general mesh extraction pipeline from 3D Gaus-\nsians as illustrated in Figure 4. We first train an efficient NeRF [34] using the\nrendered images from 3D Gaussians on-the-fly, and then convert the NeRF to\npolygonal meshes [48]. Specifically, we train two hash grids to reconstruct the\ngeometry and appearance from Gaussian renderings. Marching Cubes [28] is ap-\nplied to extract a coarse mesh, which is then iteratively refined together with\nthe appearance hash grid using differentiable rendering. Finally, we bake the\nLarge Multi-View Gaussian Model\n9\nappearance field onto the refined mesh to extract texture images. For more de-\ntails, please refer to the supplementary materials and NeRF2Mesh [48]. With\nadequately optimized implementation, it takes only about 1 minute to perform\nthis Gaussians to NeRF to mesh conversion.\n4\nExperiments\n4.1\nImplementation Details\nDatasets. We use a filtered subset of the Objaverse [12] dataset to train our\nmodel. Since there are many low-quality 3D models (e.g., partial scans, missing\ntextures) in the original Objaverse dataset, we filter the dataset by two em-\npirical rules: (1) We manually examine the captions and rendered images from\nCap3D [30], and curate a list of words that usually appears in bad models (e.g.,\n\u2018resembling\u2019, \u2018debris\u2019, \u2018frame\u2019), which is then used to filter all models whose cap-\ntion includes any of these words. (2) We discard models with mostly white color\nafter rendering, which usually indicates missing texture. These lead to a final set\nof around 80K 3D objects. We render the RGBA image from 100 camera views\nat the resolution of 512 \u00d7 512 for training and validation.\nNetwork Architecture. Our asymmetric U-Net model consists of 6 down\nblocks, 1 middle block, and 5 up blocks, with the input image at 256 \u00d7 256\nand output Gaussian feature map at 128 \u00d7 128. We use 4 input views, so the\nnumber of output Gaussians is 128 \u00d7 128 \u00d7 4 = 65, 536. The feature channels for\nall blocks are [64, 128, 256, 512, 1024, 1024], [1024] and [1024, 1024, 512, 256, 128]\nrespectively. Each block contains a series of residual layers and an optional down-\nsample or up-sample layer. For the last 3 of down blocks, the middle block, and\nthe first 3 up blocks, we also insert cross-view self-attention layers after the\nresidual layers. The final feature maps are processed by a 1\u00d71 convolution layer\nto 14-channel pixel-wise Gaussian features. Following previous works [39,46], we\nadopt Silu activation and group normalization for the U-Net.\nTraining. We train our model on 32 NVIDIA A100 (80G) GPUs for about 4\ndays. A batch size of 8 for each GPU is used under bfloat16 precision, leading\nto an effective batch size of 256. For each batch, we randomly sample 8 camera\nviews, with the first 4 views as the input, and all 8 views as the output for\nsupervision. Similar to LRM [15], we transform the cameras of each batch such\nthat the first input view is always the front view with an identity rotation matrix\nand fixed translation. The input images are assumed to have a white background.\nThe output 3D Gaussians are rendered at 512 \u00d7 512 resolution for mean square\nerror loss. We resize the images to 256 \u00d7 256 for LPIPS loss to save memory.\nThe AdamW [29] optimizer is adopted with the learning rate of 4\u00d710\u22124, weight\ndecay of 0.05, and betas of (0.9, 0.95). The learning rate is cosine annealed to\n0 during the training. We clip the gradient with a maximum norm of 1.0. The\nprobability for grid distortion and camera jitter is set to 50%.\n10\nTang et al.\nInput          TriplaneGaussian\nDreamGaussian\nOurs\nFig. 5: Comparisons of generated 3D Gaussians for image-to-3D. Our method\ngenerates Gaussian splatting with better visual quality on various challenging images.\nLarge Multi-View Gaussian Model\n11\nInput          \nLRM                                                                                Ours\nFig. 6: Comparisons with LRM for image-to-3D. We compare our method with\navailable results from LRM [15].\nInference. Our whole pipeline, including two multi-view diffusion models, takes\nonly about 10 GB of GPU memory for inference, which is friendly for deploy-\nment. For the multi-view diffusion models, we use a guidance scale of 5 for\nImageDream [51] and 7.5 for MVDream [44] following the original paper. The\nnumber of diffusion steps is set to 30 using the DDIM [45] scheduler. The cam-\nera elevation is fixed to 0, and azimuths to [0, 90, 180, 270] degree for the four\ngenerated views. For ImageDream [51], the text prompt is always left empty so\nthe only input is a single-view image. Since the images generated by MVDream\nmay contain various backgrounds, we apply background removal [37] and use\nwhite background.\n4.2\nQualitative Comparisons\nImage-to-3D. We first compare against recent methods [47,62] that are capable\nof generating 3D Gaussians. Figure 5 shows images rendered from the generated\n3D Gaussians for comparison. The 3D Gaussians produced by our method have\nbetter visual quality and effectively preserve the content from the input view. Our\nhigh-resolution 3D Gaussians can be transformed into smooth textured meshes\nwith minimal loss of quality in most cases. We also compare our results against\nLRM [15] using the available videos from their website in Figure 6. Specifically,\nour multi-view setting successfully mitigates the issue of blurry back views and\nflat geometry, resulting in enhanced detail even in unseen views.\n12\nTang et al.\n\u201cchair\u201d\n\u201cdresser\u201d\n\u201cfurry fox \nhead\u201d\n\u201cmotorcycle\u201d\n\u201cmech suit\u201d\n\u201cghost \nlantern\u201d\nInput                 \nShap-E        \nDreamGaussian\nOurs \nFig. 7: Comparisons of generated 3D models for text-to-3D. Our method\nachieves better text alignment and visual quality.\n\u201cteddy bear\u201d\n\u201cparrot\u201d\nFig. 8: Diversity of our 3D generation. We can generate diverse 3D models given\nan ambiguous text description or single-view image.\nText-to-3D. We then compare with recent methods [16,47] on text-to-3D tasks.\nWe observe an enhanced quality and efficiency in our method, generating more\nLarge Multi-View Gaussian Model\n13\nImage Consistency \u2191\nOverall Quality \u2191\nDreamGaussian [47]\n2.30\n1.98\nTriplaneGaussian [62]\n3.02\n2.67\nLGM (Ours)\n4.18\n3.95\nTable 1: User Study on the quality of generated 3D Gaussians for image-to-3D tasks.\nThe rating is of scale 1-5, the higher the better.\nrealistic 3D objects, as illustrated in Figure 7. Due to the multi-view diffusion\nmodels, our model is also free from multi-face problems.\nDiversity. Notably, our pipeline exhibits high diversity in 3D generation, owing\nto the capability of multi-view diffusion model [44, 51]. As shown in Figure 8,\nwith different random seeds, we can generate a variety of feasible objects from\nthe same ambiguous text prompt or single-view image.\n4.3\nQuantitative Comparisons\nWe majorly conduct a user study to quantitatively evaluate our image-to-3D\nGaussians generation performance. For a collection of 30 images, we render 360-\ndegree rotating videos of the 3D Gaussians generated from DreamGaussain [47]\n(only the first stage), TriplaneGaussian [62], and ours. There are in total 90\nvideos for evaluation in our user study. Each volunteer is shown 30 samples from\nmixed random methods, and asked to rate in two aspects: image consistency and\noverall model quality. We collect results from 20 volunteers and get 600 valid\nscores in total. As shown in Table 1, our method is preferred as it aligns with\nthe original image content and shows better overall quality.\n4.4\nAblation Study\nNumber of Views. We train an image-to-3D model with only one input views\nsimilar to splatter image [46], i.e., without the multi-view generation step. The U-\nNet takes the single input view as input with self-attention, and outputs Gaussian\nfeatures as in our multi-view model. To compensate the number of Gaussians,\nwe predict two Gaussians for each pixel of the output feature maps, leading to\n128\u00d7128\u00d72 = 32, 768 Gaussians. As illustrated in the top-left part of Figure 9,\nthe single-view model can reconstruct faithful front-view, but fails to distinguish\nthe back view and results in blurriness. This is as expected since the regressive\nU-Net is more suitable for reconstruction tasks, and it\u2019s hard to generalize to\nlarge datasets in our experiments.\nData Augmentation. We train a smaller model with or without applying data\naugmentation to validate its effectiveness. Although we observe a lower training\n14\nTang et al.\n1-view \n4-view \nw/ aug.\nw/o aug.\nInput (1024)\nMV (256)\nLR (256)\nHR (512)\nTraining Resolution\nNumber of input views\nData augmentation\nFig. 9: Ablation Study. We carry out ablation study on designs of our method.\nloss for the model without data augmentation, the domain gap during inference\nleads to more floaters and worse geometry as shown in the bottom-left part of\nFigure 9. The model with data augmentation strategy can better correct the 3D\ninconsistency and inaccurate camera poses in the generated multi-view images.\nTraining Resolution. Lastly, we train a model with a fewer number of Gaus-\nsians and smaller rendering resolution as in the right part of Figure 9. We re-\nmove the last up block of the U-Net so the number of output Gaussians is\n64 \u00d7 64 \u00d7 4 = 16, 384, and we render it at 256 \u00d7 256 for supervision. The model\ncan still converge and successfully reconstruct 3D Gaussians, but the details are\nworse compared to the 256\u00d7256 input multi-view images. In contrast, our large\nresolution model at 512\u00d7512 can capture better details and generate Gaussians\nwith higher resolution.\n4.5\nLimitations\nDespite the promising results, our method still has some limitations. Since our\nmodel is essentially a multi-view reconstruction model, the 3D generation quality\nhighly depends on the quality of four input views. However, current multi-view\ndiffusion models [44, 51] are far from perfect: (1) There can be 3D inconsis-\ntency which misleads the reconstruction model to generate floaters in the 3D\nGaussians. (2) The resolution of synthesized multi-view images is restricted to\n256 \u00d7 256, constraining our model to further improve resolution. (3) Image-\nDream [51] also fails to handle input image with a large elevation angle. We\nexpect these limitations can be mitigated with better multi-view diffusion mod-\nels in future works.\nLarge Multi-View Gaussian Model\n15\n5\nConclusion\nIn this work, we present a large multi-view Gaussian model for high-resolution\n3D content generation. Our model, distinct from previous methods reliant on\nNeRF and transformers, employs Gaussian splatting and U-Net to address the\nchallenges of high memory requirements and low-resolution training. Addition-\nally, we explore data augmentation for better robustness, and introduce a mesh\nextraction algorithm for the generated 3D Gaussians. Our approach achieves\nboth high-resolution and high-efficiency for 3D objects generation, proving its\nversatility and applicability in various contexts.\nAcknowledgements. This work is supported by the Sichuan Science and Tech-\nnology Program (2023YFSY0008), China Tower-Peking University Joint Labo-\nratory of Intelligent Society and Space Governance, National Natural Science\nFoundation of China (61632003, 61375022, 61403005), Grant SCITLAB-20017\nof Intelligent Terminal Key Laboratory of SiChuan Province, Beijing Advanced\nInnovation Center for Intelligent Robots and Systems (2018IRS11), and PEK-\nSenseTime Joint Laboratory of Machine Vision. This project is also funded by\nthe Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOE-\nT2EP20221-0012), NTU NAP, and under the RIE2020 Industry Alignment Fund\n\u2013 Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash\nand in-kind contribution from the industry partner(s).\nReferences\n1. Cao, Z., Hong, F., Wu, T., Pan, L., Liu, Z.: Large-vocabulary 3d diffusion model\nwith transformer. arXiv preprint arXiv:2309.07920 (2023) 4\n2. Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., Mello, S.D., Gallo, O.,\nGuibas, L., Tremblay, J., Khamis, S., Karras, T., Wetzstein, G.: Efficient geometry-\naware 3D generative adversarial networks. In: CVPR (2022) 2\n3. Charatan, D., Li, S., Tagliasacchi, A., Sitzmann, V.: pixelsplat: 3d gaussian splats\nfrom image pairs for scalable generalizable 3d reconstruction. arXiv preprint\narXiv:2312.12337 (2023) 4\n4. Chen, G., Wang, W.: A survey on 3d gaussian splatting. arXiv preprint\narXiv:2401.03890 (2024) 4\n5. Chen, H., Gu, J., Chen, A., Tian, W., Tu, Z., Liu, L., Su, H.: Single-stage diffu-\nsion nerf: A unified approach to 3d generation and reconstruction. arXiv preprint\narXiv:2304.06714 (2023) 4\n6. Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry\nand appearance for high-quality text-to-3d content creation. arXiv preprint\narXiv:2303.13873 (2023) 3\n7. Chen, Y., Chen, Z., Zhang, C., Wang, F., Yang, X., Wang, Y., Cai, Z., Yang, L.,\nLiu, H., Lin, G.: Gaussianeditor: Swift and controllable 3d editing with gaussian\nsplatting. arXiv preprint arXiv:2311.14521 (2023) 4\n8. Chen, Y., Zhang, C., Yang, X., Cai, Z., Yu, G., Yang, L., Lin, G.: It3d: Improved\ntext-to-3d generation with explicit view synthesis. arXiv preprint arXiv:2308.11473\n(2023) 3\n16\nTang et al.\n9. Chen, Z., Hong, F., Mei, H., Wang, G., Yang, L., Liu, Z.: Primdiffusion: Volumet-\nric primitives diffusion for 3d human generation. arXiv preprint arXiv:2312.04559\n(2023) 4\n10. Chen, Z., Wang, F., Liu, H.: Text-to-3d using gaussian splatting. arXiv preprint\narXiv:2309.16585 (2023) 4\n11. Deitke, M., Liu, R., Wallingford, M., Ngo, H., Michel, O., Kusupati, A., Fan, A.,\nLaforte, C., Voleti, V., Gadre, S.Y., et al.: Objaverse-xl: A universe of 10m+ 3d\nobjects. arXiv preprint arXiv:2307.05663 (2023) 4\n12. Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E.,\nSchmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of\nannotated 3d objects. In: CVPR. pp. 13142\u201313153 (2023) 3, 4, 7, 9\n13. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR. pp. 770\u2013778 (2016) 6\n14. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS 33,\n6840\u20136851 (2020) 6\n15. Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K.,\nBui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv\npreprint arXiv:2311.04400 (2023) 2, 4, 7, 9, 11\n16. Jun, H., Nichol, A.: Shap-e: Generating conditional 3d implicit functions. arXiv\npreprint arXiv:2305.02463 (2023) 4, 12\n17. Kerbl, B., Kopanas, G., Leimk\u00fchler, T., Drettakis, G.: 3d gaussian splatting for\nreal-time radiance field rendering. ToG 42(4), 1\u201314 (2023) 2, 3, 5, 7\n18. Laine, S., Hellsten, J., Karras, T., Seol, Y., Lehtinen, J., Aila, T.: Modular primi-\ntives for high-performance differentiable rendering. ToG 39(6) (2020) 3\n19. Li, J., Tan, H., Zhang, K., Xu, Z., Luan, F., Xu, Y., Hong, Y., Sunkavalli, K.,\nShakhnarovich, G., Bi, S.: Instant3d: Fast text-to-3d with sparse-view generation\nand large reconstruction model. arXiv preprint arXiv:2311.06214 (2023) 2, 3, 4\n20. Li, W., Chen, R., Chen, X., Tan, P.: Sweetdreamer: Aligning geometric priors in\n2d diffusion for consistent text-to-3d. arXiv preprint arXiv:2310.02596 (2023) 3, 5\n21. Li, Y., Dou, Y., Shi, Y., Lei, Y., Chen, X., Zhang, Y., Zhou, P., Ni, B.:\nFocaldreamer: Text-driven 3d editing via focal-fusion assembly. arXiv preprint\narXiv:2308.10608 (2023) 3\n22. Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler,\nS., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation. In:\nCVPR. pp. 300\u2013309 (2023) 2, 3\n23. Ling, H., Kim, S.W., Torralba, A., Fidler, S., Kreis, K.: Align your gaussians: Text-\nto-4d with dynamic 3d gaussians and composed diffusion models. arXiv preprint\narXiv:2312.13763 (2023) 4\n24. Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-\n1-to-3: Zero-shot one image to 3d object. arXiv preprint arXiv:2303.11328 (2023)\n2\n25. Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: Syncdreamer:\nGenerating multiview-consistent images from a single-view image. arXiv preprint\narXiv:2309.03453 (2023) 7\n26. Liu, Z., Feng, Y., Black, M.J., Nowrouzezahrai, D., Paull, L., Liu, W.: Meshdiffu-\nsion: Score-based generative 3d mesh modeling. arXiv preprint arXiv:2303.08133\n(2023) 4\n27. Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H.,\nHabermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-\ndomain diffusion. arXiv preprint arXiv:2310.15008 (2023) 3, 5\nLarge Multi-View Gaussian Model\n17\n28. Lorensen, W.E., Cline, H.E.: Marching cubes: A high resolution 3d surface con-\nstruction algorithm. In: Seminal graphics: pioneering efforts that shaped the field,\npp. 347\u2013353 (1998) 8\n29. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 (2017) 9\n30. Luo, T., Rockwell, C., Lee, H., Johnson, J.: Scalable 3d captioning with pretrained\nmodels. arXiv preprint arXiv:2306.07279 (2023) 9\n31. Metzer, G., Richardson, E., Patashnik, O., Giryes, R., Cohen-Or, D.: Latent-\nnerf for shape-guided generation of 3d shapes and textures. arXiv preprint\narXiv:2211.07600 (2022) 6\n32. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,\nR.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV\n(2020) 2\n33. M\u00fcller, N., Siddiqui, Y., Porzi, L., Bulo, S.R., Kontschieder, P., Nie\u00dfner, M.: Diffrf:\nRendering-guided 3d radiance field diffusion. In: CVPR. pp. 4328\u20134338 (2023) 4\n34. M\u00fcller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives\nwith a multiresolution hash encoding. ACM TOG (2022) 3, 8\n35. Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-e: A system for\ngenerating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751\n(2022) 4\n36. Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using\n2d diffusion. arXiv preprint arXiv:2209.14988 (2022) 2\n37. Qin, X., Zhang, Z., Huang, C., Dehghan, M., Zaiane, O.R., Jagersand, M.: U2-\nnet: Going deeper with nested u-structure for salient object detection. Pattern\nrecognition 106, 107404 (2020) 11\n38. Ren, J., Pan, L., Tang, J., Zhang, C., Cao, A., Zeng, G., Liu, Z.: Dreamgaussian4d:\nGenerative 4d gaussian splatting. arXiv preprint arXiv:2312.17142 (2023) 4\n39. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\nimage synthesis with latent diffusion models. In: CVPR. pp. 10684\u201310695 (2022)\n5, 9\n40. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\nical image segmentation. In: Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, Oc-\ntober 5-9, 2015, Proceedings, Part III 18. pp. 234\u2013241. Springer (2015) 2\n41. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour,\nK., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-\nto-image diffusion models with deep language understanding. NeurIPS 35, 36479\u2013\n36494 (2022) 5\n42. Shen, T., Gao, J., Yin, K., Liu, M.Y., Fidler, S.: Deep marching tetrahedra: a\nhybrid representation for high-resolution 3d shape synthesis. In: NeurIPS (2021) 3\n43. Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., Su, H.:\nZero123++: a single image to consistent multi-view diffusion base model (2023) 3,\n5, 7\n44. Shi, Y., Wang, P., Ye, J., Long, M., Li, K., Yang, X.: Mvdream: Multi-view diffusion\nfor 3d generation. arXiv preprint arXiv:2308.16512 (2023) 3, 5, 7, 11, 13, 14\n45. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502 (2020) 11\n46. Szymanowicz, S., Rupprecht, C., Vedaldi, A.: Splatter image: Ultra-fast single-view\n3d reconstruction. In: arXiv (2023) 2, 4, 6, 7, 9, 13\n18\nTang et al.\n47. Tang, J., Ren, J., Zhou, H., Liu, Z., Zeng, G.: Dreamgaussian: Generative gaussian\nsplatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)\n2, 3, 4, 8, 11, 12, 13, 19\n48. Tang, J., Zhou, H., Chen, X., Hu, T., Ding, E., Wang, J., Zeng, G.: Delicate\ntextured mesh recovery from nerf via adaptive surface refinement. arXiv preprint\narXiv:2303.02091 (2022) 8, 9\n49. Tsalicoglou,\nC.,\nManhardt,\nF.,\nTonioni,\nA.,\nNiemeyer,\nM.,\nTombari,\nF.:\nTextmesh: Generation of realistic 3d meshes from text prompts. arXiv preprint\narXiv:2304.12439 (2023) 3\n50. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n\u0141., Polosukhin, I.: Attention is all you need. NeurIPS 30 (2017) 6\n51. Wang, P., Shi, Y.: Imagedream: Image-prompt multi-view diffusion for 3d genera-\ntion. arXiv preprint arXiv:2312.02201 (2023) 3, 5, 6, 7, 11, 13, 14, 20\n52. Wang, P., Tan, H., Bi, S., Xu, Y., Luan, F., Sunkavalli, K., Wang, W., Xu, Z.,\nZhang, K.: Pf-lrm: Pose-free large reconstruction model for joint pose and shape\nprediction. arXiv preprint arXiv:2311.12024 (2023) 2, 4\n53. Wang, T., Zhang, B., Zhang, T., Gu, S., Bao, J., Baltrusaitis, T., Shen, J., Chen,\nD., Wen, F., Chen, Q., et al.: Rodin: A generative model for sculpting 3d digital\navatars using diffusion. In: CVPR. pp. 4563\u20134573 (2023) 4\n54. Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: Prolificdreamer: High-\nfidelity and diverse text-to-3d generation with variational score distillation. arXiv\npreprint arXiv:2305.16213 (2023) 3\n55. Weng, Z., Liu, J., Tan, H., Xu, Z., Zhou, Y., Yeung-Levy, S., Yang, J.: Single-\nview 3d human digitalization with large reconstruction models. arXiv preprint\narXiv:2401.12175 (2024) 2, 4\n56. Xu, D., Yuan, Y., Mardani, M., Liu, S., Song, J., Wang, Z., Vahdat, A.:\nAgg: Amortized generative 3d gaussians for single image to 3d. arXiv preprint\narXiv:2401.04099 (2024) 4\n57. Xu, Y., Tan, H., Luan, F., Bi, S., Wang, P., Li, J., Shi, Z., Sunkavalli, K., Wet-\nzstein, G., Xu, Z., et al.: Dmv3d: Denoising multi-view diffusion using 3d large\nreconstruction model. arXiv preprint arXiv:2311.09217 (2023) 2, 4, 6\n58. Yariv, L., Puny, O., Neverova, N., Gafni, O., Lipman, Y.: Mosaic-sdf for 3d gener-\native models. arXiv preprint arXiv:2312.09222 (2023) 4\n59. Yi, T., Fang, J., Wu, G., Xie, L., Zhang, X., Liu, W., Tian, Q., Wang, X.: Gaus-\nsiandreamer: Fast generation from text to 3d gaussian splatting with point cloud\npriors. arXiv preprint arXiv:2310.08529 (2023) 4\n60. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable\neffectiveness of deep features as a perceptual metric. In: CVPR (2018) 8\n61. Zhao, Z., Liu, W., Chen, X., Zeng, X., Wang, R., Cheng, P., Fu, B., Chen, T.,\nYu, G., Gao, S.: Michelangelo: Conditional 3d shape generation based on shape-\nimage-text aligned latent representation. arXiv preprint arXiv:2306.17115 (2023)\n4\n62. Zou, Z.X., Yu, Z., Guo, Y.C., Li, Y., Liang, D., Cao, Y.P., Zhang, S.H.: Triplane\nmeets gaussian splatting: Fast and generalizable single-view 3d reconstruction with\ntransformers. arXiv preprint arXiv:2312.09147 (2023) 2, 4, 11, 13\nLarge Multi-View Gaussian Model\n19\nA\nMore Implementation Details\nDatasets. The full list of words for filtering the Objaverse dataset is: \u2018flying,\nmountain, trash, featuring, a set of, a small, numerous, square, collection, bro-\nken, group, ceiling, wall, various, elements, splatter, resembling, landscape, stair,\nsilhouette, garbage, debris, room, preview, floor, grass, house, beam, white, back-\nground, building, cube, box, frame, roof, structure\u2019. The 100 camera views we\nuse form a spiral path on the sphere surface. The camera radius is fixed to 1.5,\nand the field-of-view along the Y-axis is fixed to 49.1 degree.\nB\nMore Results\nDreamGaussian\nOurs\nFig. 10: Comparisons between different meshing method from Gaussians.\nWe compare our meshing method with DreamGaussian [47].\nDifferent Meshing Method. Figure 10 presents a comparison between our\nmeshing algorithm and the technique introduced in DreamGaussian [47]. Our\nalgorithm generates a smoother surface, which is advantageous for subsequent\ntasks such as relighting. Moreover, our method operates independently of the\nunderlying 3D Gaussians, as it relies solely on the rendered images.\nLimitations. We visualize failure cases of our method in Figure 11 to gain a\ndeeper understanding of its weaknesses. As previously mentioned in the main pa-\nper, the primary causes of these failures stem from the flawed multi-view images\nproduced in the initial step. The resolution of these multi-view images is limited\n20\nTang et al.\nLimited Resolution\n3D Inconsistency \nLarge Elevation\nInput                       Multi-view Images                                                   Generated Gaussians  \nFig. 11: Visualization of our limitations. We show three major reasons for failure\ncases of our method.\nto 256 \u00d7 256, which can diminish the quality of the input image. Despite imple-\nmenting data augmentation during training to emulate 3D inconsistencies and\nattempting to bridge the domain gap, this approach still results in inaccuracies\nfor slender structures, such as chairs. Additionally, ImageDream [51] struggles\nwith images that have significant elevation angle, occasionally producing images\nwith a dark appearance.\n"
  },
  {
    "title": "ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation",
    "link": "https://arxiv.org/pdf/2402.04324.pdf",
    "upvote": "22",
    "text": "CONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nWeiming Ren 1 2 Harry Yang 3 Ge Zhang 1 4 Cong Wei 1 2 Xinrun Du 4 Stephen Huang 3 Wenhu Chen 1 2 4\nhttps://tiger-ai-lab.github.io/ConsistI2V/\nAbstract\nImage-to-video (I2V) generation aims to use the\ninitial frame (alongside a text prompt) to create a\nvideo sequence. A grand challenge in I2V genera-\ntion is to maintain visual consistency throughout\nthe video: existing methods often struggle to pre-\nserve the integrity of the subject, background, and\nstyle from the first frame, as well as ensure a fluid\nand logical progression within the video narra-\ntive (cf. Figure 1). To mitigate these issues, we\npropose CONSISTI2V, a diffusion-based method\nto enhance visual consistency for I2V generation.\nSpecifically, we introduce (1) spatiotemporal at-\ntention over the first frame to maintain spatial and\nmotion consistency, (2) noise initialization from\nthe low-frequency band of the first frame to en-\nhance layout consistency. These two approaches\nenable CONSISTI2V to generate highly consistent\nvideos. We also extend the proposed approaches\nto show their potential to improve consistency in\nauto-regressive long video generation and cam-\nera motion control. To verify the effectiveness\nof our method, we propose I2V-Bench, a com-\nprehensive evaluation benchmark for I2V genera-\ntion. Our automatic and human evaluation results\ndemonstrate the superiority of CONSISTI2V over\nexisting methods.\n1. Introduction\nRecent advancements in video diffusion models (Ho et al.,\n2022b) have led to an unprecedented development in text-to-\nvideo (T2V) generation (Ho et al., 2022a; Blattmann et al.,\n2023). However, such conditional generation techniques\nfall short of achieving precise control over the generated\nvideo content. For instance, given an input text prompt \u201ca\ndog running in the backyard\u201d, the generated videos may\n1University of Waterloo\n2Vector Institute\n3Harmony.AI\n4Multimodal Art Projection Research Community. Correspon-\ndence to: Harry Yang <hyang@fastmail.com>, Wenhu Chen\n<wenhuchen@uwaterloo.ca>.\nvary from outputting different dog breeds, different camera\nviewing angles, as well as different background objects. As\na result, users may need to carefully modify the text prompt\nto add more descriptive adjectives, or repetitively generate\nseveral videos to achieve the desired outcome.\nTo mitigate this issue, prior efforts have been focused on\nencoding customized subjects into video generation models\nwith few-shot finetuning (Molad et al., 2023) or replac-\ning video generation backbones with personalized image\ngeneration model (Guo et al., 2023). Recently, incorporat-\ning additional first frame images into the video generation\nprocess has become a new solution to controllable video\ngeneration. This method, often known as image-to-video\ngeneration (I2V)1 or image animation, enables the fore-\nground/background contents in the generated video to be\nconditioned on the objects as reflected in the first frame.\nNevertheless, training such conditional video generation\nmodels is a non-trivial task and existing methods often en-\ncounter appearance and motion inconsistency in the gen-\nerated videos (shown in Figure 1). Initial efforts such as\nVideoComposer (Wang et al., 2023c) and VideoCrafter1\n(Chen et al., 2023a) condition the video generation model\nwith the semantic embedding (e.g. CLIP embedding) from\nthe first frame but cannot fully preserve the local details\nin the generated video. Subsequent works either employ a\nsimple conditioning scheme by directly concatenating the\nfirst frame latent features with the input noise (Girdhar et al.,\n2023; Chen et al., 2023c; Zeng et al., 2023; Dai et al., 2023)\nor combine the two aforementioned design choices together\n(Xing et al., 2023; Zhang et al., 2023a) to enhance the first\nframe conditioning. Despite improving visual appearance\nalignment with the first frame, these methods still suffer\nfrom generating videos with incorrect and jittery motion,\nwhich severely restricts their applications in practice.\nTo address the aforementioned challenges, we propose CON-\nSISTI2V, a simple yet effective framework capable of en-\nhancing the visual consistency for I2V generation. Our\nmethod focuses on improving the first frame conditioning\nmechanisms in the I2V model and optimizing the inference\n1In this study, we follow prior work (Zhang et al., 2023a) and\nfocus on text-guided I2V generation.\n1\narXiv:2402.04324v1  [cs.CV]  6 Feb 2024\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nConsistent Appearance & Motion\nSimple Conditioning\nSpatial / Temporal Conditioning,\nLayout Guidance\nOurs\nPrior\nMethods\n\"a dog wearing a\nSuperhero outfit with\nred cape flying through\nthe sky.\"\nInconsistent Appearance & Abrupt Motion\nSEINE\nOurs\nTeddy bear walking\ndown 5th Avenue.\nA fast-paced motorcycle\nrace on a winding track.\nVideos: click to play in Adobe Acrobat\nFigure 1. Comparison of image-to-video generation results obtained from SEINE (Chen et al., 2023c) and our CONSISTI2V. SEINE\nshows degenerated appearance and motion as the video progresses, while our result maintains visual consistency. We feed the same first\nframe to SEINE and CONSISTI2V and show the generated video on the right.\nnoise initialization during sampling. To produce videos that\nclosely resemble the first frame, we apply cross-frame at-\ntention mechanisms in the model\u2019s spatial layers to achieve\nfine-grained spatial first frame conditioning. To ensure the\ntemporal smoothness and coherency of the generated video,\nwe include a local window of the first frame features in\nthe temporal layers to augment their attention operations.\nDuring inference, we propose FrameInit, which leverages\nthe low-frequency component of the first frame image and\ncombines it with the initial noise to act as a layout guidance\nand eliminate the noise discrepancy between training and\ninference. By integrating these design optimizations, our\nmodel generates highly consistent videos and can be easily\nextended to other applications such as autoregressive long\nvideo generation and camera motion control. Our model\nachieves state-of-the-art results on public I2V generation\nbenchmarks. We further conduct extensive automatic and\nhuman evaluations on a self-collected dataset I2V-Bench to\nverify the effectiveness of our method for I2V generation.\nOur contributions are summarized below:\n1. We introduce CONSISTI2V, a diffusion-based model that\nperforms spatiotemporal conditioning over the first frame\nto enhance the visual consistency in video generation.\n2. We devise FrameInit, an inference-time noise initializa-\ntion strategy that uses the low-frequency band from the\nfirst frame to stabilize video generation. FrameInit can\nalso support applications such as autoregressive long\nvideo generation and camera motion control.\n3. We propose I2V-Bench, a comprehensive quantitative\nevaluation benchmark dedicated to evaluating I2V gener-\nation models. We will release our evaluation dataset to\nfoster future I2V generation research.\n2. Related Work\nText-to-Video Generation Recent studies in T2V genera-\ntion has evolved from using GAN-based models (Fox et al.,\n2021; Brooks et al., 2022; Tian et al., 2021) and autoregres-\nsive transformers (Ge et al., 2022; Hong et al., 2022) to em-\nbracing diffusion models. Current methods usually extend\nT2I generation frameworks to model video data. VDM (Ho\net al., 2022b) proposes a space-time factorized U-Net (Ron-\nneberger et al., 2015) and interleaved temporal attention\nlayers to enable video modelling. Imagen-Video (Ho et al.,\n2022a) and Make-A-Video (Singer et al., 2022) employ\npixel-space diffusion models Imagen (Saharia et al., 2022)\nand DALL-E 2 (Ramesh et al., 2022) for high-definition\nvideo generation. Another line of work (He et al., 2022;\nChen et al., 2023a; Khachatryan et al., 2023; Guo et al.,\n2023) generate videos with latent diffusion models (Rom-\nbach et al., 2022) due to the high efficiency of LDMs. In\nparticular, MagicVideo (Zhou et al., 2022) inserts simple\nadaptor layers and Latent-Shift (An et al., 2023) utilizes\ntemporal shift modules (Lin et al., 2019) to enable temporal\nmodelling. Subsequent works generally follow VideoLDM\n(Blattmann et al., 2023) and insert temporal convolution and\nattention layers inside the LDM U-Net for video generation.\nAnother research stream focuses on optimizing the noise\ninitialization for video generation. PYoCo (Ge et al., 2023)\nproposes a video-specific noise prior and enables each\nframe\u2019s initial noise to be correlated with other frames.\nFreeNoise (Qiu et al., 2023) devises a training-free noise\nrescheduling method for long video generation. FreeInit\n(Wu et al., 2023c) leverages the low-frequency component\nof a noisy video to eliminate the initialization gap between\ntraining and inference of the diffusion models.\nVideo Editing As paired video data before and after\nediting is hard to obtain, several methods (Ceylan et al.,\n2023; Wu et al., 2023b; Geyer et al., 2023; Wu et al.,\n2023a; Zhang et al., 2023b) employ a pretrained T2I\nmodel for zero-shot/few-shot video editing.\nTo ensure\ntemporal coherency between individual edited frames,\nthese methods apply cross-frame attention mechanisms\nin the T2I model. Specifically, Tune-A-Video (Wu et al.,\n2\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nSpatial Convolution\nSpatial Attention\nTemporal Convolution\nTemporal Attention\nSpatial First Frame Conditioning\nTemporal First Frame Conditioning\n3D IFFT\nTimestep + FPS\nEncoded Text Prompt: \"A panda drinking coffee\"\nInput First Frame\n...\n3D FFT\nLow\nHigh\nTraining\nInference\n(FrameInit) Layout-Guided Noise Initialization\nLow-Pass Filter\nFigure 2. Our CONSISTI2V framework. In our model, we concatenate the first frame latent z1 to the input noise and perform first frame\nconditioning by augmenting the spatial and temporal self-attention operations in the model with the intermediate hidden states z1\nh. During\ninference, we incorporate the low-frequency component from z1 to initialize the inference noise and guide the video generation process.\n2023b) and Pix2Video (Ceylan et al., 2023) modify the T2I\nmodel\u2019s self-attention layers to enable each frame to attend\nto its immediate previous frame and the video\u2019s first frame.\nTokenFlow (Geyer et al., 2023) and Fairy (Wu et al., 2023a)\nselect a set of anchor frames such that all anchor frames can\nattend to each other during self-attention.\nImage-to-Video Generation Steering the video\u2019s content\nusing only text descriptions can be challenging. Recently,\na myriad of methods utilizing both first frame and text for\nI2V generation have emerged as a solution to achieve more\ncontrollable video generation. Among these methods, Emu-\nVideo (Girdhar et al., 2023), SEINE (Chen et al., 2023c),\nAnimateAnything (Dai et al., 2023) and PixelDance (Zeng\net al., 2023) propose simple modifications to the T2V U-Net\nby concatenating the latent features of the first frame with\ninput noise to enable first-frame conditioning. I2VGen-XL\n(Zhang et al., 2023a), Dynamicrafter (Xing et al., 2023)\nand Moonshot (Zhang et al., 2024) add extra image cross-\nattention layers in the model to inject stronger conditional\nsignals into the generation process. Our approach varies\nfrom previous studies in two critical respects: (1) our spa-\ntiotemporal feature conditioning methods effectively lever-\nage the input first frame, resulting in better visual consis-\ntency in the generated videos and enable efficient training\non public video-text datasets.(2) We develop noise initializa-\ntion strategies in the I2V inference processes, while previous\nI2V generation works rarely focus on this aspect.\n3. Methodology\nGiven an image x1 \u2208 RC\u00d7H\u00d7W and a text prompt s, the\ngoal of our model is to generate an N frame video clip\n\u02c6x = {x1, \u02c6x2, \u02c6x3, ... \u02c6xN} \u2208 RN\u00d7C\u00d7H\u00d7W such that x1 is\nthe first frame of the video, and enforce the appearance of\nthe rest of the video to be closely aligned with x1 and the\ncontent of the video to follow the textual description in s.\nWe approach this task by employing first-frame conditioning\nmechanisms in spatial and temporal layers of our model and\napplying layout-guided noise initialization during inference.\nThe overall model architecture and inference pipeline of\nCONSISTI2V are shown in Figure 2.\n3.1. Preliminaries\nDiffusion Models (DMs) (Sohl-Dickstein et al., 2015; Ho\net al., 2020) are generative models that learn to model\nthe data distribution by iteratively recovering perturbed\ninputs. Given a training sample x0 \u223c q(x0), DMs first\nobtain the corrupted input through a forward diffusion\nprocess q(xt|x0, t), t \u2208 {1, 2, ..., T} by using the param-\neterization trick from Sohl-Dickstein et al. (2015) and\ngradually adds Gaussian noise \u03f5 \u2208 N(0, I) to the input:\nxt = \u221a\u00af\u03b1tx0 + \u221a1 \u2212 \u00af\u03b1t\u03f5, \u00af\u03b1t = Qt\ni=1(1 \u2212 \u03b2i), where 0 <\n\u03b21 < \u03b22 < ... < \u03b2T < 1 is a known variance schedule that\ncontrols the amount of noise added at each time step t. The\ndiffusion model is then trained to approximate the backward\nprocess p(xt\u22121|xt) and recovers xt\u22121 from xt using a de-\nnoising network \u03f5\u03b8(xt, c, t), which can be learned by mini-\nmizing the mean squared error (MSE) between the predicted\nand target noise: min\u03b8 Ex,\u03f5\u2208N (0,I),c,t[\u2225\u03f5 \u2212 \u03f5\u03b8(xt, c, t)\u22252\n2]\n(\u03f5\u2212prediction). Here, c denotes the (optional) conditional\nsignal that DMs can be conditioned on. For our model, c is\na combination of an input first frame and a text prompt.\nLatent Diffusion Models (LDMs) (Rombach et al., 2022)\nare variants of diffusion models that first use a pretrained\nencoder E to obtain a latent representation z0 = E(x0).\nLDMs then perform the forward process q(zt|z0, t) and\nthe backward process p\u03b8(zt\u22121|zt) in this compressed latent\n3\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nspace. The generated sample \u02c6x can be obtained from the\ndenoised latent using a pretrained decoder \u02c6x = D(\u02c6z).\n3.2. Model Architecture\nU-Net Inflation for Video Generation\nOur model is de-\nveloped based on text-to-image (T2I) LDMs (Rombach\net al., 2022) that employ the U-Net (Ronneberger et al.,\n2015) model for image generation.\nThis U-Net model\ncontains a series of spatial downsampling and upsampling\nblocks with skip connections. Each down/upsampling block\nis constructed with two types of basic blocks: spatial convo-\nlution and spatial attention layers. We insert a 1D temporal\nconvolution block after every spatial convolution block and\ntemporal attention blocks at certain attention resolutions\nto make it compatible with video generation tasks. Our\ntemporal convolution and attention blocks share the exact\nsame architecture as their spatial counterparts, apart from\nthe convolution and attention operations are operated along\nthe temporal dimension. We incorporate RoPE (Su et al.,\n2024) embeddings to represent positional information in\nthe temporal layers and employ the PYoCo (Ge et al., 2023)\nmixed noise prior for noise initialization (cf. Appendix A.1).\nFirst Frame Condition Injection\nWe leverage the varia-\ntional autoencoder (VAE) (Kingma & Welling, 2013) of the\nT2I LDM to encode the input first frame into latent represen-\ntation z1 = E(x1) \u2208 RC\u2032\u00d7H\u2032\u00d7W \u2032 and use z1 as the condi-\ntional signal. To inject this signal into our model, we directly\nreplace the first frame noise \u03f51 with z1 and construct the\nmodel input as \u02c6\u03f5 = {z1, \u03f52, \u03f53, ..., \u03f5N} \u2208 RN\u00d7C\u2032\u00d7H\u2032\u00d7W \u2032.\n3.3. Fine-Grained Spatial Feature Conditioning\nThe spatial attention layer in the LDM U-Net contains a self-\nattention layer that operates on each frame independently\nand a cross-attention layer that operates between frames and\nthe encoded text prompt. Given an intermediate hidden state\nzi of the ith frame, the self-attention operation is formulated\nas the attention between different spatial positions of zi:\nQs = W Q\ns zi, Ks = W K\ns zi, Vs = W V\ns zi,\n(1)\nAttention(Qs, Ks, Vs) = Softmax(QsK\u22a4\ns\n\u221a\nd\n)Vs,\n(2)\nwhere W Q\ns , W K\ns and W V\ns are learnable projection matrices\nfor creating query, key and value vectors from the input. d\nis the dimension of the query and key vectors. To achieve\nbetter visual coherency in the video, we modify the key\nand value vectors in the self-attention layers to also include\nfeatures from the first frame z1 (cf. Figure 3):\nQs = W Q\ns zi, K\u2032\ns = W K\ns [zi, z1], V \u2032\ns = W V\ns [zi, z1],\n(3)\nwhere [\u00b7] represents the concatenation operation such that\nthe token sequence length in K\u2032\ns and V \u2032\ns are doubled com-\npared to the original Ks and Vs. In this way, each spatial\nFrame 1\nFrame \nQuery\nQuery\nKey/Value\nKey/Value\nSpatial Self-Attention\nTemporal Self-Attention\nAll spatial positions\nin Frame \nAll spatial positions\nin Frame 1\nSame spatial\npositions across all\nframes\nWindow centered\naround Query's spatial\nposition\nFigure 3. Visualization of our proposed spatial and temporal first\nframe conditioning schemes. For spatial self-attention layers, we\nemploy cross-frame attention mechanisms and expand the keys\nand values with the features from all spatial positions in the first\nframe. For temporal self-attention layers, we augment the key and\nvalue vectors with a local feature window from the first frame.\nposition in all frames gets access to the complete infor-\nmation from the first frame, allowing fine-grained feature\nconditioning in the spatial attention layers.\n3.4. Window-based Temporal Feature Conditioning\nTo effectively leverage the first frame features, we also aug-\nment the temporal self-attention layers to include a local win-\ndow of features from the first frame. Formally, given an in-\nput hidden state z \u2208 RN\u00d7C\u00d7H\u00d7W , where N is the number\nof frames and C, H, W correspond to the channel, height\nand width dimension of the hidden state tensor, our model\nfirst reshape its height and width dimension to the batch\ndimension to form a new hidden state \u00afz \u2208 R(H\u00d7W )\u00d7N\u00d7C.\nWe then compute the query, key and value from \u00afz as:\nQt = W Q\nt \u00afz, K\u2032\nt = W K\nt [\u00afz, \u02dcz1], V \u2032\nt = W V\nt [\u00afz, \u02dcz1],\n(4)\nwhere \u02dcz1 \u2208 R(H\u00d7W )\u00d7(K\u00d7K\u22121)\u00d7C is a tensor constructed\nin a way that its h \u00d7 w position in the batch dimension\ncorresponds to an K \u00d7 K window of the first frame fea-\ntures, centred at the spatial position of (h, w). The first\nframe feature vector at (h, w) is not included in \u02dcz1 as it is\nalready presented in \u00afz. We pad z in its spatial dimensions by\nreplicating the boundary values to ensure that all spatial po-\nsitions in z will have a complete window around them. We\nthen concatenate \u00afz with \u02dcz1 to enlarge the sequence length\nof the key and value matrices. See Figure 3 to visualize the\naugmented temporal self-attention. Our main rationale is\nthat the visual objects in a video may move to different spa-\ntial locations as the video progresses. Therefore, having an\nextra window of keys and values around the query location\nincreases the probability of attending to the same entity in\nthe first frame when performing temporal self-attention. In\npractice, we set K = 3 to control the time complexity of\nthe attention operations.\n4\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nReconstructed sample from the original latent \nReconstructed sample from the high-frequency component of \nReconstructed sample from the low-frequency component of \nFigure 4. Visualization of different frequency bands after spa-\ntiotemporal frequency decomposition from the original latent z0.\n3.5. Inference-time Layout-Guided Noise Initialization\nExisting literature (Lin et al., 2024) in image diffusion mod-\nels has identified that there exists a noise initialization gap\nbetween training and inference, due to the fact that common\ndiffusion noise schedules create an information leak to the\ndiffusion noise during training, causing it to be inconsistent\nwith the random Gaussian noise sampled during inference.\nIn the domain of video generation, this initialization gap\nhas been further explored by FreeInit (Wu et al., 2023c),\nshowing that the information leak mainly comes from the\nlow-frequency component of a video after spatiotemporal\nfrequency decomposition, and adding this low-frequency\ncomponent to the initial inference noise greatly enhances\nthe quality of the generated videos.\nTo better understand how different frequency bands corre-\nspond to different visual features in the video, we visualize\nthe videos decoded from the VAE latents after spatiotem-\nporal frequency decomposition, as shown in Figure 4. We\nobserve that the video\u2019s high-frequency component captures\nthe fast-moving objects and the fine details in the video,\nwhereas the low-frequency component corresponds to those\nslowly moving parts and represents an overall layout in each\nframe. Based on this observation, we propose FrameInit,\nwhich duplicates the input first frame into a static video\nand uses its low-frequency component as a coarse layout\nguidance during inference. Formally, given the latent rep-\nresentation z0 of a static video and an inference noise \u03f5,\nwe first add \u03c4 step inference noise to the static video to\nobtain z\u03c4 = add noise(z0, \u03f5, \u03c4). We then extract the\nlow-frequency component of z\u03c4 and mix it with \u03f5:\nFlow\nz\u03c4\n= FFT 3D(z\u03c4) \u2299 G(D0),\n(5)\nFhigh\n\u03f5\n= FFT 3D(\u03f5) \u2299 (1 \u2212 G(D0)),\n(6)\n\u03f5\u2032 = IFFT 3D(Flow\nz\u03c4\n+ Fhigh\n\u03f5\n),\n(7)\nwhere FFT 3D is the 3D discrete fast Fourier transformation\noperating on spatiotemporal dimensions and IFFT 3D is\nthe inverse FFT operation. G is the Gaussian low-pass filter\nparameterized by the normalized space-time stop frequency\nD0. The modified noise \u03f5\u2032 containing the low-frequency\ninformation from the static video is then used for denoising.\nBy implementing FrameInit, our empirical analysis reveals\na significant enhancement in the stabilization of generated\nvideos, demonstrating improved video quality and consis-\ntency. FrameInit also enables our model with two additional\napplications: (1) autoregressive long video generation and\n(2) camera motion control. We showcase more results for\neach application in Section 5.5.\n4. I2V-Bench\nExisting video generation benchmarks such as UCF-101\n(Soomro et al., 2012) and MSR-VTT (Xu et al., 2016) fall\nshort in video resolution, diversity, and aesthetic appeal.\nTo bridge this gap, we introduce the I2V-Bench evaluation\ndataset, featuring 2,950 high-quality YouTube videos cu-\nrated based on strict resolution and aesthetic standards. We\norganized these videos into 16 distinct categories, such as\nScenery, Sports, Animals, and Portraits. Further details are\navailable in the Appendix.\nEvaluation Metrics\nFollowing VBench (Huang et al.,\n2023b), our evaluation framework encompasses two key\ndimensions, each addressing distinct aspects of Image-to-\nVideo (I2V) performance: (1) Visual Quality assesses the\nperceptual quality of the video output regardless of the input\nprompts. We measure the subject and background consis-\ntency, temporal flickering, motion smoothness and dynamic\ndegree. (2) Visual Consistency evaluates the video\u2019s ad-\nherence to the text prompt given by the user. We measure\nobject consistency, scene consistency and overall video-text\nconsistency. Further details can be found in the Appendix C.\n5. Experiments\n5.1. Implementation Details\nWe use Stable Diffusion 2.1-base (Rombach et al., 2022)\nas the base T2I model to initialize CONSISTI2V and train\nthe model on the WebVid-10M (Bain et al., 2021) dataset,\nwhich contains \u223c10M video-text pairs. For each video, we\nsample 16 frames with a spatial resolution of 256 \u00d7 256 and\na frame interval between 1 \u2264 v \u2264 5, which is used as a\nconditional input to the model to enable FPS control. We\nuse the first frame as the image input and learn to denoise\nthe subsequent 15 frames during training. Our model is\ntrained with the \u03f5 objective over all U-Net parameters using\na batch size of 192 and a learning rate of 5e-5 for 170k steps.\nDuring training, we randomly drop input text prompts with\n5\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nTable 1. Quantitative evaluation results for CONSISTI2V. \u2020: the statistics also include the data for training the base video generation\nmodel. Bold: best results (excluding AnimateAnything). Underline: top-2 (excluding AnimateAnything).\nUCF-101\nMSR-VTT\nHuman Evaluation - Consistency\nMethod\n#Data\nFVD \u2193\nIS \u2191\nFID \u2193\nFVD \u2193\nCLIPSIM \u2191\nAppearance \u2191\nMotion \u2191\nAnimateAnything\n10M+20K\u2020\n642.64\n63.87\n10.00\n218.10\n0.2661\n43.07%\n20.26%\nI2VGen-XL\n35M\n597.42\n18.20\n42.39\n270.78\n0.2541\n1.79%\n9.43%\nDynamiCrafter\n10M+10M\u2020\n404.50\n41.97\n32.35\n219.31\n0.2659\n44.49%\n31.10%\nSEINE\n25M+10M\u2020\n306.49\n54.02\n26.00\n152.63\n0.2774\n48.16%\n36.76%\nCONSISTI2V\n10M\n177.66\n56.22\n15.74\n104.58\n0.2674\n53.62%\n37.04%\nTable 2. Automatic evaluation results on I2V-Bench. Consist. denotes the consistency metrics. Bold: best results (excluding AnimateAny-\nthing). Underline: top-2 (excluding AnimateAnything).\nMethod\nTemporal\nFlickering \u2191\nMotion\nSmoothness \u2191\nDynamic\nDegree \u2191\nBackground\nConsist.\n\u2191\nSubject\nConsist. \u2191\nObject\nConsist. \u2191\nScene\nConsist. \u2191\nOverall\nConsist. \u2191\nAnimateAnything\n99.08\n99.23\n3.69\n98.50\n97.90\n32.56\n27.18\n18.74\nI2VGen-XL\n94.03\n96.03\n57.88\n94.52\n89.36\n29.25\n23.50\n16.89\nDynamiCrafter\n93.81\n95.89\n64.11\n93.47\n88.01\n32.18\n23.93\n18.68\nSEINE\n96.08\n97.92\n42.12\n93.98\n88.21\n35.43\n27.68\n20.21\nCONSISTI2V\n96.65\n97.77\n37.48\n94.69\n90.85\n32.06\n24.81\n19.50\na probability of 0.1 to enable classifier-free guidance (Ho &\nSalimans, 2022). During inference, we employ the DDIM\nsampler (Song et al., 2020) with 50 steps and classifier-\nfree guidance with a guidance scale of w = 7.5 to sample\nvideos. We apply FrameInit with \u03c4 = 850 and D0 = 0.25\nfor inference noise initialization.\n5.2. Quantitative Evaluation\nUCF-101 & MSR-VTT\nWe evaluate CONSISTI2V on\ntwo public datasets UCF-101 (Soomro et al., 2012) and\nMSR-VTT (Xu et al., 2016). We report Fr\u00b4echet Video Dis-\ntance (FVD) (Unterthiner et al., 2019) and Inception Score\n(IS) (Salimans et al., 2016) for video quality assessment,\nFr\u00b4echet Inception Distance (FID) (Heusel et al., 2017) for\nframe quality assessment and CLIP similarity (CLIPSIM)\n(Wu et al., 2021) for video-text alignment evaluation. We\nrefer readers to Appendix B.2 for the implementation details\nof these metrics. We evaluate FVD, FID and IS on UCF-101\nover 2048 videos and FVD and CLIPSIM on MSR-VTT\u2019s\ntest split (2990 samples). We focus on evaluating the I2V\ngeneration capability of our model: given a video clip from\nthe evaluation dataset, we randomly sample a frame and use\nit along with the text prompt as the input to our model. All\nevaluations are performed in a zero-shot manner.\nWe compare CONSISTI2V against four open-sourced I2V\ngeneration models: I2VGen-XL (Zhang et al., 2023a), Ani-\nmateAnything (Dai et al., 2023), DynamiCrafter (Xing et al.,\n2023) and SEINE (Chen et al., 2023c). Quantitative evalu-\nation results are shown in Table 1. We observe that while\nAnimateAnything achieves better IS and FID, its generated\nvideos are mostly near static (see Figure 5 for visualiza-\ntions), which severely limits the video quality (highest FVD\nof 642.64). Our model significantly outperforms the rest of\nthe baseline models in all metrics, except for CLIPSIM on\nMSR-VTT, where the result is slightly lower than SEINE.\nWe note that SEINE, initialized from LaVie (Wang et al.,\n2023d), benefited from a larger and superior-quality training\ndataset, including Vimeo25M, WebVid-10M, and additional\nprivate datasets. In contrast, CONSISTI2V is directly ini-\ntialized from T2I models and only trained on WebVid-10M,\nshowcasing our method\u2019s effectiveness.\nI2V-Bench\nWe present the automatic evaluation results\nfor CONSISTI2V and the baseline models in Table 5. Simi-\nlar to previous findings, we observe that AnimateAnything\nachieves the best motion smoothness and appearance consis-\ntency among all models. However, it significantly falls short\nin generating videos with higher motion magnitude, regis-\ntering a modest dynamic degree value of only 3.69 (visual\nresults as shown in Figure 5). On the other hand, our model\nachieves a better balance between motion magnitude and\nvideo quality, outperforms all other baseline models exclud-\ning AnimateAnything in terms of motion quality (less flick-\nering and better smoothness) and visual consistency (higher\nbackground/subject consistency) and achieves a competitive\noverall video-text consistency.\nHuman Evaluation\nTo further validate the generation\nquality of our model, we conduct a human evaluation based\non 548 samples from our CONSISTI2V and the baseline\nmodels. We randomly distribute a subset of samples to\neach participant, presenting them with the input image, text\nprompt, and all generated videos. Participants are then asked\n6\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nInput Frame\nInput Frame\nInput Frame\nInput Frame\nText Prompt:\na dog\nswimming.\nText Prompt:\na dark\ntornado in a\njar, 4k video,\n3D rendered,\nwell-\nrendered.\nText Prompt: \nmelting ice\ncream\ndripping\ndown the\ncone.\nText Prompt: \na person\nriding a bike\nin the sunset.\nOurs\nDynamiCrafter\nSEINE\nAnimateAnything\nOurs\nDynamiCrafter\nSEINE\nAnimateAnything\nFigure 5. Qualitative comparisons between DynamiCrafter, SEINE, AnimateAnything and our CONSISTI2V. Input first frames are\ngenerated by PixArt-\u03b1 (Chen et al., 2023b) and SDXL (Podell et al., 2023).\nto answer two questions: to identify the videos with the best\noverall appearance and motion consistency. Each question\nallows for one or more selections. We collect a total of 1061\nresponses from 13 participants and show the results in the\nright part of Table 1. As demonstrated by the results, our\nmodel ranked top in both metrics, achieving a comparable\nmotion consistency with SEINE and a significantly higher\nappearance consistency than all other baseline models.\n5.3. Qualitative Evaluation\nWe present a visual comparison of our model with Dynam-\niCrafter, SEINE and AnimateAnything in Figure 5. We\nexclude I2VGen-XL in this section as its generated video\ncannot fully adhere to the visual details from the input first\nframe. As shown in the figure, current methods often strug-\ngle with maintaining appearance and motion consistency\nin video sequences. This can include (1) sudden changes\nin subject appearance mid-video, as demonstrated in the\n\u201cice cream\u201d case for DynamiCrafter and SEINE; (2) Back-\nground inconsistency, as observed in DynamiCrafter\u2019s \u201cbik-\ning\u201d case; (3) unnatural object movements, evident in the\n\u201cdog swimming\u201d case (DynamiCrafter) and \u201ctornado in a jar\u201d\ncase (SEINE) and (4) minimal or absent movement, as dis-\nplayed in most of the generated videos by AnimateAnything.\nOn the other hand, CONSISTI2V produces videos with sub-\njects that consistently align with the input first frame. Ad-\nditionally, our generated videos exhibit more natural and\nlogical motion, avoiding abrupt changes and thereby ensur-\ning improved appearance and motion consistency. More\nvisual results of our model can be found in Appendix H.\n5.4. Ablation Studies\nTo verify the effectiveness of our design choices, we con-\nduct an ablation study on UCF-101 by iteratively disabling\nFrameInit, temporal first frame conditioning and spatial first\nframe conditioning. We follow the same experiment setups\nin Section 5.2 and show the results in Table 3.\nEffectiveness of FrameInit According to the results, we\nfind that applying FrameInit greatly enhances the model per-\nformance for all metrics. Our empirical observation suggests\nthat FrameInit can stabilize the output video and reduce the\n7\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nTable 3. Ablation study results for our proposed spatiotemporal\nfirst frame conditioning mechanisms and FrameInit on UCF-101.\nT.Cond. and S.Cond. correspond to temporal and spatial first\nframe conditioning, respectively.\nFVD \u2193\nIS \u2191\nFID \u2193\nCONSISTI2V\n177.66\n56.22\n15.74\nw/o FrameInit\n245.79\n42.21\n24.08\nw/o FrameInit & T.Cond.\n224.16\n42.98\n24.04\nw/o FrameInit & T.Cond & S.Cond.\n704.48\n21.97\n68.39\nText Prompt: clown fish swimming through the coral reef.\nConsistI2V\nw/o FrameInit\nw/o F.I.+T.C.\nw/o F.I.+T.C.+S.C.\nFigure 6. Visual comparisons of our method after disabling\nFrameInit (F.I.), temporal conditioning (T.C.) and spatial condi-\ntioning (S.C.). We use the same seed to generate all videos.\nabrupt appearance and motion changes. As shown in Fig-\nure 6, while our model can still generate reasonable results\nwithout enabling FrameInit, the output videos also suffer\nfrom a higher chance of rendering sudden object move-\nments and blurry frames (last frame in the second row).\nThis highlights the effectiveness of FrameInit in producing\nmore natural motion and higher-quality frames.\nSpatiotemporal First Frame Conditioning Our ablation\nresults on UCF-101 (c.f. Table 3) reflects a significant per-\nformance boost after applying the proposed spatial and tem-\nporal first frame conditioning mechanisms to our model.\nAlthough removing temporal first frame conditioning leads\nto an overall better performance for the three quantitative\nmetrics, in practice we find that only using spatial condi-\ntioning often results in jittering motion and larger object\ndistortions, evident in the last two frames in the third row\nof Figure 6. When both spatial and temporal first frame\nconditioning is removed, our model loses the capability of\nmaintaining the appearance of the input first frame.\n5.5. More Applications\nAutoregressive Long Video Generation\nWhile our im-\nage animation model provides native support for long video\ngeneration by reusing the last frame of the previous video\nText Prompt: time lapse at a fantasy landscape, 4k, high resolution.\nConsistI2V\nw/o FrameInit\nCamera Motion:\nPan-Right\nCamera Motion:\nZoom-In\nApplication: Autoregressive Long Video Generation\nFrame 1\nFrame 11\nFrame 22\nFrame 33\nFrame 44\nText prompt: a penguin walking on the beach.\nApplication: Camera Motion Control\nText Prompt: fire is soaring.\nFigure 7. Applications of CONSISTI2V. Upper panel: FrameInit\nenhances object consistency in long video generation. Lower\npanel: FrameInit enables training-free camera motion control.\nto generate the subsequent video, we observe that directly\nusing the model to generate long videos may lead to subop-\ntimal results, as the artifacts in the previous video clip will\noften accumulate throughout the autoregressive generation\nprocess. We find that using FrameInit to guide the genera-\ntion of each video chunk helps stabilize the autoregressive\nvideo generation process and results in a more consistent vi-\nsual appearance throughout the video, as shown in Figure 7.\nCamera Motion Control\nWhen adopting FrameInit for\ninference, instead of using the static first frame video as\nthe input, we can alternatively create synthetic camera mo-\ntions from the first frame and use it as the layout condition.\nFor instance, camera panning can be simulated by creating\nspatial crops in the first frame starting from one side and\ngradually moving to the other side. As shown in Figure 7,\nby simply tweaking the FrameInit parameters to \u03c4 = 750\nand D0 = 0.5 and using the synthetic camera motion as the\nlayout guidance, we are able to achieve camera panning and\nzoom-in/zoom-out effects in the generated videos.\n6. Conclusion\nWe presented CONSISTI2V, an I2V generation framework\ndesigned to improve the visual consistency of generated\nvideos by integrating our novel spatiotemporal first frame\nconditioning and FrameInit layout guidance mechanisms.\nOur approach enables the generation of highly consistent\nvideos and supports applications including autoregressive\nlong video generation and camera motion control. We con-\n8\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nducted extensive automatic and human evaluations on var-\nious benchmarks, including our proposed I2V-Bench and\ndemonstrated exceptional I2V generation results. For future\nwork, we plan to refine our training paradigm and incor-\nporate higher-quality training data to further scale up the\ncapacity of our CONSISTI2V.\nBroader Impact\nConditional video synthesis aims at generating high-quality\nvideo with faithfulness to the given condition. It is a funda-\nmental problem in computer vision and graphics, enabling\ndiverse content creation and manipulation. Recent advances\nhave shown great advances in generating aesthetical and\nhigh-resolution videos. However, the generated videos are\nstill lacking coherence and consistency in terms of the sub-\njects, background and style. Our work aims to address these\nissues and has shown promising improvement. However,\nour model also leads to slower motions in some cases. We\nbelieve this is still a long-standing issue that we would need\nto address before delivering it to the public.\nReferences\nAn, J., Zhang, S., Yang, H., Gupta, S., Huang, J.-B., Luo, J.,\nand Yin, X. Latent-shift: Latent diffusion with temporal\nshift for efficient text-to-video generation. arXiv preprint\narXiv:2304.08477, 2023.\nBain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen\nin time: A joint video and image encoder for end-to-end\nretrieval. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 1728\u20131738, 2021.\nBlattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim,\nS. W., Fidler, S., and Kreis, K. Align your latents: High-\nresolution video synthesis with latent diffusion models. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 22563\u201322575, 2023.\nBrooks, T., Hellsten, J., Aittala, M., Wang, T.-C., Aila,\nT., Lehtinen, J., Liu, M.-Y., Efros, A., and Karras, T.\nGenerating long videos of dynamic scenes. Advances\nin Neural Information Processing Systems, 35:31769\u2013\n31781, 2022.\nCaron, M., Touvron, H., Misra, I., J\u00b4egou, H., Mairal, J.,\nBojanowski, P., and Joulin, A. Emerging properties in\nself-supervised vision transformers. In Proceedings of the\nIEEE/CVF international conference on computer vision,\npp. 9650\u20139660, 2021.\nCeylan, D., Huang, C.-H. P., and Mitra, N. J. Pix2video:\nVideo editing using image diffusion. In Proceedings of\nthe IEEE/CVF International Conference on Computer\nVision, pp. 23206\u201323217, 2023.\nChen, H., Xia, M., He, Y., Zhang, Y., Cun, X., Yang, S.,\nXing, J., Liu, Y., Chen, Q., Wang, X., et al. Videocrafter1:\nOpen diffusion models for high-quality video generation.\narXiv preprint arXiv:2310.19512, 2023a.\nChen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z.,\nKwok, J., Luo, P., Lu, H., et al. Pixart-\u03b1: Fast training\nof diffusion transformer for photorealistic text-to-image\nsynthesis. arXiv preprint arXiv:2310.00426, 2023b.\nChen, X., Wang, Y., Zhang, L., Zhuang, S., Ma, X., Yu, J.,\nWang, Y., Lin, D., Qiao, Y., and Liu, Z. Seine: Short-to-\nlong video diffusion model for generative transition and\nprediction. arXiv preprint arXiv:2310.20700, 2023c.\nDai, Z., Zhang, Z., Yao, Y., Qiu, B., Zhu, S., Qin, L., and\nWang, W. Fine-grained open domain image animation\nwith motion guidance. arXiv preprint arXiv:2311.12886,\n2023.\nFox, G., Tewari, A., Elgharib, M., and Theobalt, C. Stylev-\nideogan: A temporal generative model using a pretrained\nstylegan. arXiv preprint arXiv:2107.07224, 2021.\nGe, S., Hayes, T., Yang, H., Yin, X., Pang, G., Jacobs, D.,\nHuang, J.-B., and Parikh, D. Long video generation with\ntime-agnostic vqgan and time-sensitive transformer. In\nEuropean Conference on Computer Vision, pp. 102\u2013118.\nSpringer, 2022.\nGe, S., Nah, S., Liu, G., Poon, T., Tao, A., Catanzaro, B., Ja-\ncobs, D., Huang, J.-B., Liu, M.-Y., and Balaji, Y. Preserve\nyour own correlation: A noise prior for video diffusion\nmodels. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 22930\u201322941, 2023.\nGeyer, M., Bar-Tal, O., Bagon, S., and Dekel, T. Tokenflow:\nConsistent diffusion features for consistent video editing.\narXiv preprint arXiv:2307.10373, 2023.\nGirdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S.,\nRambhatla, S. S., Shah, A., Yin, X., Parikh, D., and\nMisra, I. Emu video: Factorizing text-to-video gener-\nation by explicit image conditioning.\narXiv preprint\narXiv:2311.10709, 2023.\nGuo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D.,\nand Dai, B. Animatediff: Animate your personalized\ntext-to-image diffusion models without specific tuning.\narXiv preprint arXiv:2307.04725, 2023.\nHe, Y., Yang, T., Zhang, Y., Shan, Y., and Chen, Q. Latent\nvideo diffusion models for high-fidelity video generation\nwith arbitrary lengths. arXiv preprint arXiv:2211.13221,\n2022.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\n9\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nrule converge to a local nash equilibrium. Advances in\nneural information processing systems, 30, 2017.\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598, 2022.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. Advances in neural information process-\ning systems, 33:6840\u20136851, 2020.\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko,\nA., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,\net al. Imagen video: High definition video generation\nwith diffusion models. arXiv preprint arXiv:2210.02303,\n2022a.\nHo, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi,\nM.,\nand Fleet,\nD. J.\nVideo diffusion models.\narXiv:2204.03458, 2022b.\nHong, W., Ding, M., Zheng, W., Liu, X., and Tang, J.\nCogvideo: Large-scale pretraining for text-to-video gener-\nation via transformers. arXiv preprint arXiv:2205.15868,\n2022.\nHuang, X., Zhang, Y., Ma, J., Tian, W., Feng, R., Zhang,\nY., Li, Y., Guo, Y., and Zhang, L. Tag2text: Guiding\nvision-language model via image tagging. arXiv preprint\narXiv:2303.05657, 2023a.\nHuang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang,\nY., Wu, T., Jin, Q., Chanpaisit, N., et al. Vbench: Com-\nprehensive benchmark suite for video generative models.\narXiv preprint arXiv:2311.17982, 2023b.\nKhachatryan, L., Movsisyan, A., Tadevosyan, V., Henschel,\nR., Wang, Z., Navasardyan, S., and Shi, H. Text2video-\nzero: Text-to-image diffusion models are zero-shot video\ngenerators. arXiv preprint arXiv:2303.13439, 2023.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013.\nLi, Z., Zhu, Z.-L., Han, L.-H., Hou, Q., Guo, C.-L., and\nCheng, M.-M. Amt: All-pairs multi-field transforms\nfor efficient frame interpolation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 9801\u20139810, 2023.\nLin, J., Gan, C., and Han, S. Tsm: Temporal shift module\nfor efficient video understanding. In Proceedings of the\nIEEE/CVF international conference on computer vision,\npp. 7083\u20137093, 2019.\nLin, S., Liu, B., Li, J., and Yang, X. Common diffusion noise\nschedules and sample steps are flawed. In Proceedings\nof the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pp. 5404\u20135411, 2024.\nMolad, E., Horwitz, E., Valevski, D., Acha, A. R., Matias,\nY., Pritch, Y., Leviathan, Y., and Hoshen, Y. Dreamix:\nVideo diffusion models are general video editors. arXiv\npreprint arXiv:2302.01329, 2023.\nPodell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,\nT., M\u00a8uller, J., Penna, J., and Rombach, R. Sdxl: Im-\nproving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\nQiu, H., Xia, M., Zhang, Y., He, Y., Wang, X., Shan, Y., and\nLiu, Z. Freenoise: Tuning-free longer video diffusion\nvia noise rescheduling. arXiv preprint arXiv:2310.15169,\n2023.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pp. 8748\u20138763. PMLR, 2021.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M.\nHierarchical text-conditional image generation with clip\nlatents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pp.\n10684\u201310695, 2022.\nRonneberger, O., Fischer, P., and Brox, T. U-net: Con-\nvolutional networks for biomedical image segmentation.\nIn Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Confer-\nence, Munich, Germany, October 5-9, 2015, Proceedings,\nPart III 18, pp. 234\u2013241. Springer, 2015.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan,\nB., Salimans, T., et al. Photorealistic text-to-image dif-\nfusion models with deep language understanding. Ad-\nvances in Neural Information Processing Systems, 35:\n36479\u201336494, 2022.\nSaito, M., Saito, S., Koyama, M., and Kobayashi, S. Train\nsparsely, generate densely: Memory-efficient unsuper-\nvised training of high-resolution temporal gan. Inter-\nnational Journal of Computer Vision, 128(10-11):2586\u2013\n2606, 2020.\nSalimans, T., Goodfellow, I., Zaremba, W., Cheung, V.,\nRadford, A., and Chen, X.\nImproved techniques for\ntraining gans. Advances in neural information processing\nsystems, 29, 2016.\n10\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nSinger, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S.,\nHu, Q., Yang, H., Ashual, O., Gafni, O., et al. Make-a-\nvideo: Text-to-video generation without text-video data.\narXiv preprint arXiv:2209.14792, 2022.\nSkorokhodov, I., Tulyakov, S., and Elhoseiny, M. Stylegan-\nv: A continuous video generator with the price, image\nquality and perks of stylegan2. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 3626\u20133636, 2022.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. In International conference on\nmachine learning, pp. 2256\u20132265. PMLR, 2015.\nSong, J., Meng, C., and Ermon, S. Denoising diffusion\nimplicit models. arXiv preprint arXiv:2010.02502, 2020.\nSoomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset\nof 101 human actions classes from videos in the wild.\narXiv preprint arXiv:1212.0402, 2012.\nSu, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.\nRoformer: Enhanced transformer with rotary position\nembedding. Neurocomputing, 568:127063, 2024.\nTeed, Z. and Deng, J. Raft: Recurrent all-pairs field trans-\nforms for optical flow. In Computer Vision\u2013ECCV 2020:\n16th European Conference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part II 16, pp. 402\u2013419. Springer,\n2020.\nTian, Y., Ren, J., Chai, M., Olszewski, K., Peng, X.,\nMetaxas, D. N., and Tulyakov, S. A good image genera-\ntor is what you need for high-resolution video synthesis.\narXiv preprint arXiv:2104.15069, 2021.\nTran, D., Bourdev, L., Fergus, R., Torresani, L., and Paluri,\nM. Learning spatiotemporal features with 3d convolu-\ntional networks. In Proceedings of the IEEE international\nconference on computer vision, pp. 4489\u20134497, 2015.\nUnterthiner, T., van Steenkiste, S., Kurach, K., Marinier, R.,\nMichalski, M., and Gelly, S. FVD: A new metric for video\ngeneration, 2019.\nURL https://openreview.\nnet/forum?id=rylgEULtdN.\nWang, J., Yuan, H., Chen, D., Zhang, Y., Wang, X., and\nZhang, S. Modelscope text-to-video technical report.\narXiv preprint arXiv:2308.06571, 2023a.\nWang, W., Yang, H., Tuo, Z., He, H., Zhu, J., Fu, J., and\nLiu, J. Videofactory: Swap attention in spatiotemporal\ndiffusions for text-to-video generation. arXiv preprint\narXiv:2305.10874, 2023b.\nWang, X., Yuan, H., Zhang, S., Chen, D., Wang, J., Zhang,\nY., Shen, Y., Zhao, D., and Zhou, J. Videocomposer:\nCompositional video synthesis with motion controllabil-\nity. arXiv preprint arXiv:2306.02018, 2023c.\nWang, Y., Chen, X., Ma, X., Zhou, S., Huang, Z., Wang,\nY., Yang, C., He, Y., Yu, J., Yang, P., et al. Lavie: High-\nquality video generation with cascaded latent diffusion\nmodels. arXiv preprint arXiv:2309.15103, 2023d.\nWang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Li, X.,\nChen, G., Chen, X., Wang, Y., et al. Internvid: A large-\nscale video-text dataset for multimodal understanding\nand generation. arXiv preprint arXiv:2307.06942, 2023e.\nWu, B., Chuang, C.-Y., Wang, X., Jia, Y., Krishnakumar,\nK., Xiao, T., Liang, F., Yu, L., and Vajda, P. Fairy: Fast\nparallelized instruction-guided video-to-video synthesis.\narXiv preprint arXiv:2312.13834, 2023a.\nWu, C., Huang, L., Zhang, Q., Li, B., Ji, L., Yang, F.,\nSapiro, G., and Duan, N.\nGodiva: Generating open-\ndomain videos from natural descriptions. arXiv preprint\narXiv:2104.14806, 2021.\nWu, J., Wang, J., Yang, Z., Gan, Z., Liu, Z., Yuan, J., and\nWang, L. Grit: A generative region-to-text transformer for\nobject understanding. arXiv preprint arXiv:2212.00280,\n2022.\nWu, J. Z., Ge, Y., Wang, X., Lei, S. W., Gu, Y., Shi, Y.,\nHsu, W., Shan, Y., Qie, X., and Shou, M. Z. Tune-a-\nvideo: One-shot tuning of image diffusion models for\ntext-to-video generation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 7623\u2013\n7633, 2023b.\nWu, T., Si, C., Jiang, Y., Huang, Z., and Liu, Z. Freeinit:\nBridging initialization gap in video diffusion models.\narXiv preprint arXiv:2312.07537, 2023c.\nXing, J., Xia, M., Zhang, Y., Chen, H., Wang, X., Wong,\nT.-T., and Shan, Y. Dynamicrafter: Animating open-\ndomain images with video diffusion priors. arXiv preprint\narXiv:2310.12190, 2023.\nXu, J., Mei, T., Yao, T., and Rui, Y. Msr-vtt: A large video\ndescription dataset for bridging video and language. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 5288\u20135296, 2016.\nZeng, Y., Wei, G., Zheng, J., Zou, J., Wei, Y., Zhang, Y.,\nand Li, H. Make pixels dance: High-dynamic video\ngeneration. arXiv preprint arXiv:2311.10982, 2023.\nZhang, D. J., Li, D., Le, H., Shou, M. Z., Xiong, C., and\nSahoo, D. Moonshot: Towards controllable video gen-\neration and editing with multimodal conditions. arXiv\npreprint arXiv:2401.01827, 2024.\n11\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nZhang, S., Wang, J., Zhang, Y., Zhao, K., Yuan, H., Qin, Z.,\nWang, X., Zhao, D., and Zhou, J. I2vgen-xl: High-quality\nimage-to-video synthesis via cascaded diffusion models.\narXiv preprint arXiv:2311.04145, 2023a.\nZhang, Y., Wei, Y., Jiang, D., Zhang, X., Zuo, W., and Tian,\nQ. Controlvideo: Training-free controllable text-to-video\ngeneration. arXiv preprint arXiv:2305.13077, 2023b.\nZhou, D., Wang, W., Yan, H., Lv, W., Zhu, Y., and Feng, J.\nMagicvideo: Efficient video generation with latent diffu-\nsion models. arXiv preprint arXiv:2211.11018, 2022.\n12\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nAppendix\nA. Additional Implementation Details\nA.1. Model Architecture\nU-Net Temporal Layers\nThe temporal layers of our CONSISTI2V share the same architecture as their spatial counterparts.\nFor temporal convolution blocks, we create residual blocks containing two temporal convolution layers with a kernel\nsize of (3, 1, 1) along the temporal and spatial height and width dimensions. Our temporal attention blocks contain one\ntemporal self-attention layer and one cross-attention layer that operates between temporal features and encoded text prompts.\nFollowing Blattmann et al. (2023), we also add a learnable weighing factor \u03b3 in each temporal layer to combine the spatial\nand temporal outputs:\nzout = \u03b3zspatial + (1 \u2212 \u03b3)ztemporal, \u03b3 \u2208 [0, 1],\n(8)\nwhere zspatial denotes the output of the spatial layers (and thus the input to the temporal layers) and ztemporal represents\nthe output of the temporal layers. We initialize all \u03b3 = 1 such that the temporal layers do not have any effects at the\nbeginning of the training.\nCorrelated Noise Initialization\nExisting I2V generation models (Xing et al., 2023; Zhang et al., 2023a; Zeng et al., 2023)\noften initialize the noise for each frame as i.i.d. Gaussian noises and ignore the correlation between consecutive frames. To\neffectively leverage the prior information that nearby frames in a video often share similar visual appearances, we employ\nthe mixed noise prior from PYoCo (Ge et al., 2023) for noise initialization:\n\u03f5shared \u223c N(0,\n\u03b12\n1 + \u03b12 I), \u03f5i\nind \u223c N(0,\n1\n1 + \u03b12 I),\n(9)\n\u03f5i = \u03f5shared + \u03f5i\nind,\n(10)\nwhere \u03f5i represents the noise of the ith frame, which consists of a shared noise \u03f5shared that has the same value across\nall frames and an independent noise \u03f5i\nind that is different for each frame. \u03b1 controls the strength of the shared and the\nindependent component of the noise and we empirically set \u03b1 = 1.5 in our experiments. We observed that this correlated\nnoise initialization helps stabilize the training, prevents exploding gradients and leads to faster convergence.\nPositional Embeddings in Temporal Attention Layers\nWe follow Wang et al. (2023d) and incorporate the rotary\npositional embeddings (RoPE) (Su et al., 2024) in the temporal attention layers of our model to indicate frame position\ninformation. To adapt RoPE embedding to our temporal first frame conditioning method as described in Section 3.4, given\nquery vector of a certain frame at spatial position (h, w), we rotate the key/value tokens in \u02dcz1\nh using the same angle as the\nfirst frame features at (h, w) to indicate that this window of features also comes from the first frame.\nFPS Control\nWe follow Xing et al. (2023) to use the sampling frame interval during training as a conditional signal to the\nmodel to enable FPS conditioning. Given a training video, we sample 16 frames by randomly choosing a frame interval v\nbetween 1 and 5. We then input this frame interval value into the model by using the same method of encoding timestep\nembeddings: the integer frame interval value is first transformed into sinusoidal embeddings and then passed through two\nlinear layers, resulting in a vector embedding that has the same dimension as the timestep embeddings. We then add the\nframe interval embedding and timestep embedding together and send the combined embedding to the U-Net blocks. We\nzero-initialize the second linear layer for the frame interval embeddings such that at the beginning of the training, the frame\ninterval embedding is a zero vector.\nA.2. Training Paradigms\nExisting I2V generation models (Xing et al., 2023; Zeng et al., 2023; Zhang et al., 2023a; 2024) often employ joint\nvideo-image training (Ho et al., 2022b) that trains the model on video-text and image-text data in an interleaving fashion,\nor apply multi-stage training strategies that iteratively pretrain different model components using different types of data.\nOur model introduces two benefits over prior methods: (1) the explicit conditioning mechanism in the spatial and temporal\nself-attention layers effectively utilizes the visual cues from the first frame to render subsequent frames, thus reducing the\ndifficulty of generating high-quality frames for the video diffusion model. (2) We directly employ the LDM VAE features as\nthe conditional signal, avoiding training additional adaptor layers for other feature modalities (e.g. CLIP (Radford et al.,\n13\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\n2021) image embeddings), which are often trained in separate stages by other methods. As a result, we train our model with\na single video-text dataset in one stage, where we finetune all the parameters during training.\nB. Model Evaluation Details\nB.1. Datasets\nUCF-101 (Soomro et al., 2012) is a human action recognition dataset consisting of 13K videos divided into 101 action\ncategories. During the evaluation, we sample 2048 videos from the dataset based on the categorical distribution of the labels\nin the dataset. As the dataset only contains a label name for each category instead of descriptive captions, we employ the\ntext prompts from PYoCo (Ge et al., 2023) for UCF-101 evaluation. The text prompts are listed below:\napplying eye makeup, applying lipstick, archery, baby crawling, gymnast performing on a balance beam, band marching,\nbaseball pitcher throwing baseball, a basketball player shooting basketball, dunking basketball in a basketball match, bench\npress, biking, billiards, blow dry hair, blowing candles, body weight squats, a person bowling on bowling alley, boxing\npunching bag, boxing speed bag, swimmer doing breast stroke, brushing teeth, clean and jerk, cliff diving, bowling in\ncricket gameplay, batting in cricket gameplay, cutting in kitchen, diver diving into a swimming pool from a springboard,\ndrumming, two fencers have fencing match indoors, field hockey match, gymnast performing on the floor, group of people\nplaying frisbee on the playground, swimmer doing front crawl, golfer swings and strikes the ball, haircuting, a person\nhammering a nail, an athlete performing the hammer throw, an athlete doing handstand push up, an athlete doing handstand\nwalking, massagist doing head massage to man, an athlete doing high jump, horse race, person riding a horse, a woman\ndoing hula hoop, ice dancing, athlete practicing javelin throw, a person juggling with balls, a young person doing jumping\njacks, a person skipping with jump rope, a person kayaking in rapid water, knitting, an athlete doing long jump, a person\ndoing lunges with barbell, military parade, mixing in the kitchen, mopping floor, a person practicing nunchuck, gymnast\nperforming on parallel bars, a person tossing pizza dough, a musician playing the cello in a room, a musician playing the\ndaf, a musician playing the indian dhol, a musician playing the flute, a musician playing the guitar, a musician playing the\npiano, a musician playing the sitar, a musician playing the tabla, a musician playing the violin, an athlete jumps over the\nbar, gymnast performing pommel horse exercise, a person doing pull ups on bar, boxing match, push ups, group of people\nrafting on fast moving river, rock climbing indoor, rope climbing, several people rowing a boat on the river, couple salsa\ndancing, young man shaving beard with razor, an athlete practicing shot put throw, a teenager skateboarding, skier skiing\ndown, jet ski on the water, sky diving, soccer player juggling football, soccer player doing penalty kick in a soccer match,\ngymnast performing on still rings, sumo wrestling, surfing, kids swing at the park, a person playing table tennis, a person\ndoing TaiChi, a person playing tennis, an athlete practicing discus throw, trampoline jumping, typing on computer keyboard,\na gymnast performing on the uneven bars, people playing volleyball, walking with dog, a person doing pushups on the wall,\na person writing on the blackboard, a kid playing Yo-Yo.\nMSR-VTT (Xu et al., 2016) is an open-domain video retrieval and captioning dataset containing 10K videos, with 20\ncaptions for each video. The standard splits for MSR-VTT include 6,513 training videos, 497 validation videos and 2,990\ntest videos. We use the official test split in the experiment and randomly select a text prompt for each video during evaluation.\nB.2. Evaluation Metrics\nFr\u00b4echet Video Distance (FVD)\nFVD measures the similarity between generated and real videos. We follow Blattmann\net al. (2023) to use a pretrained I3D model2 to extract features from the videos and use the official UCF FVD evaluation\ncode3 from Ge et al. (2022) to compute FVD statistics.\nFr\u00b4echet Inception Distance (FID)\nWe use a pretrained Inception model4 to extract per-frame features from the generated\nand real videos to compute FID. Our evaluation code is similar to the evaluation script5 provided by Brooks et al. (2022).\n2https://github.com/songweige/TATS/blob/main/tats/fvd/i3d_pretrained_400.pt\n3https://github.com/pfnet-research/tgan2\n4https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/\nmetrics/inception-2015-12-05.pkl\n5https://github.com/NVlabs/long-video-gan/tree/main\n14\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nInception Score (IS)\nWe compute a video version of the Inception Score following previous works (Blattmann et al.,\n2023; Saito et al., 2020), where the video features used in IS are computed from a C3D model (Tran et al., 2015) pretraind\non UCF-101. We use the TorchScript C3D model6 and employ the evaluation code7 from (Skorokhodov et al., 2022).\nCLIP Similarity (CLIPSIM)\nOur CLIPSIM metrics are computed using TorchMetrics. We use the CLIP-VIT-B/32\nmodel (Radford et al., 2021) to compute the CLIP similarity for all frames in the generated videos and report the averaged\nresults.\nC. I2V-Bench Evaluation Metrics\nWe make use of the metrics provided by VBench (Huang et al., 2023b) in the I2V-Bench evaluation process.\nBackground Consistency\nWe measure the temporal consistency of the background scenes by computing the CLIP\n(Radford et al., 2021) feature similarity across frames.\nSubject Consistency\nWe measure the subject consistency by calculating the DINO (Caron et al., 2021) feature similarity\nacross frames.\nTemporal Flickering\nWe calculate the average absolute difference between each frame.\nMotion Smoothness\nWe adapt AMT (Li et al., 2023) to evaluate the level of smoothness in the generated motions.\nDynamic Degree\nWe adapt RAFT (Teed & Deng, 2020) to estimate the degree of dynamics in the video.\nObject Consistency\nWe medially select 100 videos separately from the Pet, Vehicle, Animal, and Food categories in the\nI2V-Bench validation dataset. We further annotate the related objects based on the reference videos and use GRiT (Wu et al.,\n2022) to assess the success rate of generating the stated objects matching the reference videos.\nScene Consistency\nWe select 100 videos from the Scenery-Nature and Scenery-City categories in the I2V-Bench validation\ndataset. We further annotate the related scenes based on the reference videos and apply Tag2Text (Huang et al., 2023a) to\ncaption the generated videos and detect whether the scenes, in the reference videos, exist in them.\nOverall Video-text Consistency\nWe adopt the overall video-text consistency computed by ViCLIP (Wang et al., 2023e)\nto reflect the semantics consistency of the manually annotated captions of I2V-Bench and the generated videos.\nD. Human Evaluation Details\nWe show our designed human evaluation interface in Figure 8. We collect 274 prompts and use Pixart-\u03b1 (Chen et al., 2023b)\nand SDXL (Podell et al., 2023) to generate 548 images as the input first frame. We then generate a video for each input\nimage using the four baseline models I2VGen-XL (Zhang et al., 2023a), DynamiCrafter (Xing et al., 2023), SEINE (Chen\net al., 2023c) and AnimateAnything (Dai et al., 2023), as well as our CONSISTI2V. To ensure a fair comparison, we resize\nand crop all the generated videos to 256 \u00d7 256 and truncate all videos to 16 frames, 2 seconds (8 FPS). We then randomly\nshuffle the order of these 548 samples for each participant and let them answer the two questions regarding rating the\nappearance and motion consistency of the videos for a subset of samples.\nE. Limitations\nOur current method has several limitations: (1) our training dataset WebVid-10M (Bain et al., 2021) predominantly comprises\nlow-resolution videos, and a consistent feature across this dataset is the presence of a watermark, located at a fixed position\nin all the videos. As a result, our generated videos will also have a high chance of getting corrupted by the watermark, and\nwe currently only support generating videos at a relatively low resolution. (2) While our proposed FrameInit enhances the\n6https://www.dropbox.com/s/jxpu7avzdc9n97q/c3d_ucf101.pt?dl=1\n7https://github.com/universome/stylegan-v/tree/master\n15\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nFigure 8. Interface of our human evaluation experiment.\nstability of the generated videos, we also observe that our model sometimes creates videos with limited motion magnitude,\nthereby restricting the subject movements in the video content. (3) Our spatial first frame conditioning method requires\ntuning the spatial U-Net layers during training, which limits the ability of our model to directly adapt to personalized T2I\ngeneration models and increases the training costs. (4) Our model shares some other common limitations with the base\nT2I generation model Stable Diffusion (Rombach et al., 2022), such as not being able to correctly render human faces and\nlegible text.\nF. I2V-Bench Statistics\nTable 4. I2V-Bench Statistics\nCategory\nCount\nCategory\nCount\nPortrait\n663\nAnimation-Static\n120\nScenery-Nature\n500\nMusic\n63\nPet\n393\nGame\n61\nFood\n269\nAnimal\n55\nAnimation-Hard\n187\nIndustry\n44\nScience\n180\nPainting\n40\nSports\n149\nOthers\n40\nScenery-City\n138\nVehicle\n30\nDrama\n19\nTotal\n2951\n16\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nTable 5. Experimental results for T2V generation on MSR-VTT.\nMethod\n#data\n#params\nCLIPSIM (\u2191)\nFVD (\u2193)\nCogVideo (EN) (Hong et al., 2022)\n5.4M\n15.5B\n0.2631\n1294\nMagicVideo (Zhou et al., 2022)\n10M\n-\n-\n1290\nLVDM (He et al., 2022)\n2M\n1.2B\n0.2381\n742\nVideoLDM (Blattmann et al., 2023)\n10M\n4.2B\n0.2929\n-\nInternVid (Wang et al., 2023e)\n28M\n-\n0.2951\n-\nModelScope (Wang et al., 2023a)\n10M\n1.7B\n0.2939\n550\nMake-A-Video (Singer et al., 2022)\n20M\n9.7B\n0.3049\n-\nLatent-Shift (An et al., 2023)\n10M\n1.5B\n0.2773\n-\nVideoFactory (Wang et al., 2023b)\n-\n2.0B\n0.3005\n-\nPixelDance (Zeng et al., 2023)\n10M\n1.5B\n0.3125\n381\nCONSISTI2V\n10M\n1.7B\n0.2968\n428\nG. Additional Quantitative Results\nTo compare against more closed-sourced I2V methods and previous T2V methods, we conduct an additional quantitative\nexperiment following PixelDance (Zeng et al., 2023) and evaluate our model\u2019s ability as a generic T2V generator by using\nStable Diffusion 2.1-base (Rombach et al., 2022) to generate the first frame conditioned on the input text prompt. We\nemploy the test split of MSR-VTT (Xu et al., 2016) and evaluate FVD and CLIPSIM for this experiment. As shown in\nTable 5, our method is on par with previous art in T2V generation and achieves a second-best FVD result of 428 and a\ncomparable CLIPSIM of 0.2968. These results indicate our model\u2019s capability of handling diverse video generation tasks.\nH. I2V-Bench Results\nWe showcase more I2V generation results for CONSISTI2V in Figure 9 and Figure 10.\nTable 6. Appendix: Results of I2V-Bench for Background Consistency. S-N, S-C, A-H and A-S respectively represent Scenery-Nature,\nScenery-City, Animation-Hard and Animation-Static.\nCategory\nI2VGen-XL\nAnimateAnything\nDynamiCrafter\nSEINE\nCONSISTI2V\nPortrait\n94.04\n98.42\n92.61\n94.07\n93.38\nS-N\n95.26\n98.64\n93.74\n95.45\n95.60\nPet\n91.78\n97.92\n92.32\n91.59\n94.26\nFood\n97.28\n98.97\n96.13\n96.89\n96.97\nA-H\n94.32\n98.68\n93.63\n89.03\n93.29\nScience\n95.65\n98.67\n92.63\n94.44\n95.20\nSports\n92.98\n98.88\n93.11\n94.93\n94.26\nS-C\n94.02\n98.47\n93.05\n94.26\n94.50\nA-S\n96.83\n98.82\n95.10\n95.39\n96.43\nMusic\n93.20\n97.62\n93.12\n93.34\n94.33\nGame\n92.52\n97.66\n92.81\n88.34\n93.20\nAnimal\n96.64\n98.14\n95.64\n96.14\n95.86\nIndustry\n97.36\n98.93\n95.81\n97.26\n96.27\nPainting\n95.31\n98.36\n93.18\n91.82\n94.87\nOthers\n95.59\n98.86\n94.96\n94.74\n93.75\nVehicle\n95.16\n98.77\n94.83\n95.50\n96.23\nDrama\n94.32\n98.90\n91.63\n92.97\n93.95\nAll\n94.52\n98.50\n93.47\n93.98\n94.69\n17\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nTable 7. Appendix: Results of I2V-Bench for Subject Consistency. S-N, S-C, A-H and A-S respectively represent Scenery-Nature,\nScenery-City, Animation-Hard and Animation-Static.\nCategory\nI2VGen-XL\nAnimateAnything\nDynamiCrafter\nSEINE\nCONSISTI2V\nPortrait\n89.97\n97.92\n87.50\n89.00\n89.07\nS-N\n90.94\n98.06\n88.72\n91.34\n92.65\nPet\n80.92\n97.05\n83.80\n80.79\n89.11\nFood\n95.19\n98.46\n92.93\n94.27\n95.03\nA-H\n86.28\n97.67\n87.19\n78.55\n86.34\nScience\n90.97\n98.31\n85.62\n87.64\n91.29\nSports\n89.22\n98.91\n89.63\n93.59\n92.57\nS-C\n89.62\n98.12\n88.36\n90.85\n91.30\nA-S\n92.57\n97.97\n89.12\n90.06\n92.64\nMusic\n88.99\n95.86\n90.06\n90.12\n93.17\nGame\n86.60\n96.78\n87.25\n78.14\n88.61\nAnimal\n93.27\n97.21\n90.74\n91.08\n91.82\nIndustry\n95.84\n99.05\n91.42\n95.56\n94.37\nPainting\n89.62\n98.15\n86.97\n81.15\n91.25\nOthers\n90.98\n98.27\n89.88\n87.03\n86.23\nVehicle\n90.08\n98.28\n91.38\n90.36\n92.86\nDrama\n91.09\n98.65\n85.56\n87.57\n89.62\nAll\n89.36\n97.90\n88.01\n88.21\n90.85\nTable 8. Appendix: Results of I2V-Bench for Temporal Flickering. S-N, S-C, A-H and A-S respectively represent Scenery-Nature,\nScenery-City, Animation-Hard and Animation-Static.\nCategory\nI2VGen-XL\nAnimateAnything\nDynamiCrafter\nSEINE\nCONSISTI2V\nPortrait\n94.89\n99.25\n93.85\n96.78\n97.17\nS-N\n94.58\n99.07\n94.54\n96.67\n96.86\nPet\n90.98\n98.90\n92.24\n93.89\n95.88\nFood\n95.39\n98.74\n95.36\n96.45\n96.12\nA-H\n93.22\n99.26\n93.90\n95.11\n96.18\nScience\n94.71\n99.41\n91.78\n96.79\n97.56\nSports\n91.71\n98.97\n92.66\n95.37\n95.34\nS-C\n93.16\n99.00\n93.82\n95.89\n96.69\nA-S\n95.09\n98.90\n94.45\n96.19\n96.63\nMusic\n92.78\n99.15\n93.80\n95.96\n96.05\nGame\n93.87\n99.26\n93.24\n96.46\n97.88\nAnimal\n96.66\n98.83\n96.56\n97.24\n96.51\nIndustry\n96.53\n99.13\n95.39\n97.35\n96.90\nPainting\n95.36\n99.12\n94.11\n95.89\n97.07\nOthers\n96.23\n99.39\n94.97\n96.96\n97.84\nVehicle\n95.23\n99.28\n95.51\n96.60\n97.25\nDrama\n94.48\n98.92\n91.70\n95.81\n95.88\nAll\n94.03\n99.08\n93.81\n96.08\n96.65\n18\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nTable 9. Appendix: Results of I2V-Bench for Motion Smoothness. S-N, S-C, A-H and A-S respectively represent Scenery-Nature,\nScenery-City, Animation-Hard and Animation-Static.\nCategory\nI2VGen-XL\nAnimateAnything\nDynamiCrafter\nSEINE\nCONSISTI2V\nPortrait\n96.73\n99.36\n95.83\n98.31\n98.17\nS-N\n96.44\n99.22\n96.49\n98.14\n97.84\nPet\n93.54\n99.09\n94.48\n97.13\n97.39\nFood\n97.30\n98.97\n96.97\n98.03\n97.28\nA-H\n95.03\n99.34\n95.88\n97.06\n97.54\nScience\n96.42\n99.49\n95.31\n98.25\n98.37\nSports\n95.07\n99.15\n95.62\n97.91\n97.23\nS-C\n95.30\n99.18\n95.86\n97.89\n97.64\nA-S\n96.66\n99.09\n96.01\n97.68\n97.77\nMusic\n95.20\n99.27\n95.96\n98.22\n97.21\nGame\n96.00\n99.35\n95.56\n97.82\n98.59\nAnimal\n97.92\n99.04\n97.53\n98.22\n97.48\nIndustry\n97.96\n99.25\n96.81\n98.45\n97.84\nPainting\n96.58\n99.24\n95.60\n97.28\n97.77\nOthers\n97.46\n99.47\n96.68\n98.34\n98.50\nVehicle\n96.91\n99.37\n96.95\n98.21\n98.20\nDrama\n97.15\n99.08\n94.82\n97.88\n97.11\nAll\n96.03\n99.23\n95.89\n97.92\n97.77\nTable 10. Appendix: Results of I2V-Bench for object consistency.\nCategory\nI2VGen-XL\nAnimateAnything\nDynamiCrafter\nSEINE\nCONSISTI2V\nPet\n65.00\n73.50\n71.75\n82.25\n76.00\nFood\n0.00\n0.25\n0.00\n0.00\n1.00\nAnimal\n25.00\n28.00\n24.25\n28.00\n25.50\nVehicle\n27.00\n28.50\n32.75\n31.50\n25.75\nAll\n29.25\n32.56\n32.18\n35.43\n32.06\n19\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nTable 11. Appendix: Results of I2V-Bench for scene consistency. S-N and S-C respectively represent Scenery-Nature and Scenery-City\nCategory\nI2VGen-XL\nAnimateAnything\nDynamiCrafter\nSEINE\nCONSISTI2V\nS-N\n33.70\n18.49\n34.80\n37.50\n33.45\nS-C\n12.88\n35.53\n12.62\n17.47\n15.81\nAll\n23.50\n27.18\n23.93\n27.68\n24.81\nTable 12. Appendix: Results of I2V-Bench for Dynamic Degree. S-N, S-C, A-H and A-S respectively represent Scenery-Nature, Scenery-\nCity, Animation-Hard and Animation-Static.\nCategory\nI2VGen-XL\nAnimateAnything\nDynamiCrafter\nSEINE\nCONSISTI2V\nPortrait\n57.01\n3.47\n62.90\n35.44\n35.14\nS-N\n50.60\n2.00\n56.80\n27.20\n26.80\nPet\n92.88\n6.62\n92.62\n85.24\n61.32\nFood\n36.06\n6.32\n50.93\n29.37\n28.25\nA-H\n63.64\n6.42\n60.96\n63.10\n56.15\nScience\n47.78\n0.00\n69.44\n37.78\n34.44\nSports\n87.25\n2.01\n93.29\n46.98\n57.05\nS-C\n50.00\n2.17\n55.07\n29.71\n23.91\nA-S\n28.33\n0.00\n37.50\n26.67\n20.83\nMusic\n65.08\n9.52\n69.84\n38.10\n46.03\nGame\n70.49\n8.20\n67.21\n49.18\n24.59\nAnimal\n40.00\n3.64\n34.55\n23.64\n27.27\nIndustry\n22.73\n0.00\n47.73\n15.91\n22.73\nPainting\n52.50\n2.50\n57.50\n45.00\n37.50\nOthers\n32.50\n2.50\n40.00\n35.00\n17.50\nVehicle\n53.33\n0.00\n50.00\n43.33\n43.33\nDrama\n57.89\n0.00\n63.16\n52.63\n42.11\nAll\n57.88\n3.69\n64.11\n42.12\n37.48\n20\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nTable 13. Appendix: Results of I2V-Bench for Text-Video Overall Consistency. S-N, S-C, A-H and A-S respectively represent Scenery-\nNature, Scenery-City, Animation-Hard and Animation-Static.\nCategory\nI2VGen-XL\nAnimateAnything\nDynamiCrafter\nSEINE\nCONSISTI2V\nPortrait\n15.50\n17.68\n17.95\n19.27\n18.64\nS-N\n17.51\n18.51\n18.12\n19.70\n19.14\nPet\n20.23\n23.10\n22.94\n24.12\n23.29\nFood\n19.80\n21.26\n21.35\n21.98\n21.57\nA-H\n11.23\n12.82\n12.79\n17.61\n15.26\nScience\n17.26\n18.80\n18.48\n19.60\n19.59\nSports\n20.11\n21.92\n22.52\n22.59\n21.69\nS-C\n15.85\n17.16\n17.21\n18.38\n18.23\nA-S\n13.63\n14.91\n15.32\n16.20\n15.49\nMusic\n18.69\n21.89\n21.63\n23.71\n24.14\nGame\n12.63\n15.35\n13.27\n18.53\n15.63\nAnimal\n18.20\n19.50\n19.06\n19.78\n19.62\nIndustry\n17.38\n19.51\n19.55\n19.81\n19.90\nPainting\n10.66\n13.72\n13.26\n17.67\n15.48\nOthers\n14.66\n17.63\n18.01\n19.13\n19.64\nVehicle\n17.98\n19.00\n18.97\n19.53\n19.08\nDrama\n12.01\n13.13\n11.52\n16.46\n14.81\nAll\n16.89\n18.74\n18.68\n20.21\n19.50\n21\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nInput Frame\na shark swimming in clean ocean.\na monkey is playign bass guitar, stage background, 4k, high resolution.\ntimelapse at the snow land with aurora in the sky.\nfireworks.\nthe orient express is driving through a fantasy landscape.\npour water into a glass cup.\nFigure 9. Additional I2V generation results for CONSISTI2V.\n22\nCONSISTI2V: Enhancing Visual Consistency for Image-to-Video Generation\nInput Frame\nglass sphere filled with swirling multicolored liquid, cinematic lighting.\na lightning striking atop of Eiffel tower, doark clouds in the sky.\na robot dj is playing the turntable in heavy raining futuristic tokyo rooftop cyberpunk night.\nTeddy bear walking down 5th avenue, front view, beautiful sunset.\nclose up of grapes on a rotating table, high definition.\na man with a skull face in flames walking around Piccadilly circus.\nFigure 10. Additional I2V generation results for CONSISTI2V.\n23\n"
  },
  {
    "title": "EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss",
    "link": "https://arxiv.org/pdf/2402.05008.pdf",
    "upvote": "18",
    "text": "EfficientViT-SAM: Accelerated Segment Anything Model\nWithout Performance Loss\nZhuoyang Zhang1,3\u2217, Han Cai2,3\u2217, Song Han2,3\n1Tsinghua University, 2MIT, 3NVIDIA\nhttps://github.com/mit-han-lab/efficientvit\nAbstract\nWe present EfficientViT-SAM, a new family of acceler-\nated segment anything models. We retain SAM\u2019s lightweight\nprompt encoder and mask decoder while replacing the\nheavy image encoder with EfficientViT. For the training,\nwe begin with the knowledge distillation from the SAM-ViT-\nH image encoder to EfficientViT. Subsequently, we conduct\nend-to-end training on the SA-1B dataset. Benefiting from\nEfficientViT\u2019s efficiency and capacity, EfficientViT-SAM de-\nlivers 48.9\u00d7 measured TensorRT speedup on A100 GPU\nover SAM-ViT-H without sacrificing performance.\nOur\ncode and pre-trained models are released at https://\ngithub.com/mit-han-lab/efficientvit.\n1. Introduction\nSegment Anything Model (SAM) [1] is a family of\nimage segmentation models pretrained on a high-quality\ndataset with 11M images and 1B masks. SAM provides\nastounding zero-shot image segmentation performance and\nhas many applications, including AR/VR, data annotation,\ninteractive image editing, etc.\nDespite the strong performance, SAM is highly compu-\ntation intensive, restricting its applicability in time-sensitive\nscenarios. In particular, SAM\u2019s main computation bottle-\nneck is its image encoder, which requires 2973 GMACs per\nimage at the inference time.\nTo accelerate SAM, numerous efforts have been made to\nreplace SAM\u2019s image encoder with lightweight models. For\nexample, MobileSAM [2] distills the knowledge of SAM\u2019s\nViT-H model into a tiny vision transformer. EdgeSAM [3]\ntrains a purely CNN-based model to mimic ViT-H, em-\nploying a meticulous distillation strategy with the prompt\nencoder and mask decoder involved in the process. Effi-\ncientSAM [4] leverages the MAE pretraining method to im-\nprove performance.\nWhile these methods can reduce the computation cost,\n\u2217Work done during an internship at NVIDIA.\nZero-Shot COCO mAP\n37\n40\n43\n46\n49\nA100 GPU TRT FP16 Throughput (image/s)\n0\n200\n400\n600\n800\n47.8\n47.5\n46.6\n46.2\n45.7\n44.4\n42.1\n38.7\n46.5\nTable 2\nApril\nSAM\nMobileSAM\nEdgeSAM\nE\ufb03cientSAM\nE\ufb03cientViT-SAM-L\nE\ufb03cientViT-SAM-\nXL\nApril\n278\n38.7\n449\n42.1\nJune\n183\n44.4\nJuly\n762\n45.7\nAugust\n638\n46.2\nSeptember\n538\n46.6\nOctober\n294\nNovember\n11\n46.5\nUntitled 1\nUntitled 2\n278\n47.5\nUntitled 3\n182\n47.8\nSAM-ViT-H\nEf\ufb01cientSAM\nEdgeSAM\nMobileSAM\nEf\ufb01cientViT-SAM-L\nEf\ufb01cientViT-SAM-XL\n16.5x faster\nInput\nConv\nResBlock\n3 x 1024 x 1024\nInput Stem\nStage1\n\u00d7 L4\nMBConv\nStage 4\nEf\ufb01cientViT \nModule\n\u00d7 L5\nMBConv\nEf\ufb01cientViT \nModule\n4x up\n2x up\nF-MBConv\n\u00d7 L1\nF-MBConv\n\u00d7 L2\nF-MBConv\n\u00d7 L3\nStage2\nStage3\nStage 5\nP5\nP4\nP3\n\u00d7 L0\n48.9x faster\n1\nFigure 1.\nThroughput vs.\nCOCO Zero-Shot Instance Seg-\nmentation mAP. EfficientViT-SAM is the first accelerated SAM\nmodel that matches/outperforms SAM-ViT-H\u2019s [1] zero-shot per-\nformance, delivering the SOTA performance-efficiency trade-off.\nthey all suffer from significant performance drops (Fig-\nure 1).\nThis work introduces EfficientViT-SAM to ad-\ndress this limitation by leveraging EfficientViT [7] to re-\nplace SAM\u2019s image encoder.\nMeanwhile, we retain the\nlightweight prompt encoder and mask decoder architecture\nfrom SAM. Our training process consists of two phases.\nFirst, we train the image encoder of EfficientViT-SAM us-\ning SAM\u2019s image encoder as the teacher.\nSecond, we\ntrain EfficientViT-SAM end-to-end using the whole SA-1B\ndataset [1].\nWe thoroughly evaluate EfficientViT-SAM on a se-\nries of zero-shot benchmarks. EfficientViT-SAM provides\na significant performance/efficiency boost over all prior\nSAM models.\nIn particular, on the COCO dataset [8],\nEfficientViT-SAM achieves 48.9\u00d7 higher throughput on\nA100 GPU without mAP drop compared with SAM-ViT-H\n[1].\n1\narXiv:2402.05008v1  [cs.CV]  7 Feb 2024\nInput\nConv\nResBlock\n3 x 1024 x 1024\nInput Stem\nStage1\n\u00d7 L4\nMBConv\nStage 4\nEf\ufb01cientViT \nModule\n\u00d7 L5\nMBConv\nEf\ufb01cientViT \nModule\n4x up\n2x up\n\u00d7 L6\nF-MBConv\n\u00d7 L1\nF-MBConv\n\u00d7 L2\nF-MBConv\n\u00d7 L3\nStage2\nStage3\nStage 5\nP5\nP4\nP3\nF-MBConv\nSAM Head\n\u00d7 L0\n1\nFigure 2. Macro Architecture of EfficientViT-SAM-XL. \u2018ResBlock\u2019 refers to the basic building block from ResNet34 [5]. \u2018F-MBConv\u2019\nrefers to the fused MBConv block from [6]. \u2018EfficientViT Module\u2019 is the building block from [7].\n2. Related Work\n2.1. Segment Anything Model\nSAM [1] has gained widespread recognition as a mile-\nstone in the field of computer vision, showcasing its excep-\ntional performance and generalization in image segmenta-\ntion.\nSAM defines image segmentation as a promptable\ntask, that aims to generate a valid segmentation mask given\nany segmentation prompt. To achieve this objective, SAM\nutilizes an image encoder and a prompt encoder to process\nthe image and provide prompts. The outputs from both en-\ncoders are then fed into a mask decoder, which generates\nthe final mask prediction. SAM is trained on a large-scale\nsegmentation dataset comprising over 11 million images\nwith more than 1 billion high-quality masks, enabling ro-\nbust zero-shot open-world segmentation. SAM has shown\nits high versatility in a wide range of downstream appli-\ncations, including image in-painting [9], object tracking\n[10, 11], and 3D generation [12, 13]. Nevertheless, the im-\nage encoder component of SAM imposes significant com-\nputational costs, leading to high latency that restricts its\npracticality in time-sensitive scenarios. Recent works [2\u2013\n4, 14] have been focused on improving the efficiency of\nSAM, aiming to address its computational limitations.\n2.2. Efficient Deep Learning Computing\nImproving the efficiency of deep neural networks is criti-\ncal when deploying them in real-world applications on both\nedge and cloud platforms.\nOur work is related to effi-\ncient model architecture design [15, 16] that aims to im-\nprove the performance-efficiency trade-off by replacing in-\nefficient model architectures with efficient ones. Our work\nis also related to knowledge distillation [17] that uses pre-\ntrained teacher models to guide the training of student mod-\nels. Additionally, we can combine EfficientViT-SAM with\nother parallel techniques to further boost efficiency, includ-\ning pruning [18], quantization [19], and hardware-aware\nneural architecture search [20].\n3. Method\nWe propose EfficientViT-SAM, which harnesses Effi-\ncientViT [7] to accelerate the segment anything model. In\nparticular, our approach preserves the prompt encoder and\nmask decoder architecture from SAM while replacing the\nimage encoder with EfficientViT. We design two series of\nmodels, EfficientViT-SAM-L and EfficientViT-SAM-XL,\noffering a balanced trade-off between speed and perfor-\nmance. Subsequently, we train EfficientViT-SAM using the\nSA-1B dataset in an end-to-end fashion.\n3.1. EfficientViT\nEfficientViT [7] is a family of vision transformer models\nfor efficient high-resolution dense prediction. Its core build-\ning block is a multi-scale linear attention module that en-\nables the global receptive field and multi-scale learning with\nhardware-efficient operations. Specifically, it substitutes the\ninefficient softmax attention with lightweight ReLU linear\nattention to have the global receptive field. By leveraging\nthe associative property of matrix multiplication, ReLU lin-\near attention can reduce the computational complexity from\nquadratic to linear while preserving functionality. In addi-\ntion, it enhances the ReLU linear attention with convolution\nto mitigate its limitation in local feature extraction. More\ndetails are available in the original paper [7].\n3.2. EfficientViT-SAM\nModel\nArchitecture.\nThe\nmacro\narchitecture\nof\nEfficientViT-SAM-XL\nis\ndemonstrated\nin\nFigure\n2.\nIts backbone consists of five stages. Similar to EfficientViT\n[7], we use convolution blocks in the early stages while\nusing EfficientViT modules in the last two stages.\nWe\nfuse the features from the last three stages by upsampling\nand addition. The fused feature is fed to the neck com-\nprising several fused MBConv blocks and then fed to the\nSAM head. For more details, please refer to our GitHub\nrepository.\nTraining.\nTo initialize the image encoder, we begin by\ndistilling the image embedding of SAM-ViT-H into Effi-\n2\n#Params(M)\n#MACs(G)\nThroughput (image/s)\nCOCO mAP\nSAM-ViT-H [1]\n641.1\n2973\n11\n46.5\nMobileSAM [2]\n9.8\n39\n278\n38.7\nEdgeSAM [3]\n9.6\n20\n449\n42.1\nEfficientSAM [4]\n25.3\n247\n183\n44.4\nEfficientViT-SAM-L0\n34.8\n35\n762\n45.7\nEfficientViT-SAM-L1\n47.7\n49\n638\n46.2\nEfficientViT-SAM-L2\n61.3\n69\n538\n46.6\nEfficientViT-SAM-XL0\n117.0\n185\n278\n47.5\nEfficientViT-SAM-XL1\n203.3\n322\n182\n47.8\nTable 1. Runtime Efficiency Comparison. We benchmark the throughput on a single NVIDIA A100 GPU with TensorRT, fp16.\ncientViT. We employ the L2 loss as the loss function. For\nthe prompt encoder and mask decoder, we initialize them\nby loading the weights from SAM-ViT-H. Then, we train\nEfficientViT-SAM on the SA-1B dataset in an end-to-end\nmanner.\nIn the end-to-end training phase, we randomly choose\nbetween the box prompt and the point prompt with equal\nprobability. In the case of the point prompt, we randomly\nselect 1-10 foreground points from the ground-truth mask\nto ensure our model performs effectively for various point\nconfigurations. In the case of the box prompt, we utilize the\nground-truth bounding box. We resize the longest side to\n512/1024 for EfficientViT-SAM-L/XL models and pad the\nshorter side accordingly. We select up to 64 randomly sam-\npled masks per image. To supervise the training process,\nwe use a linear combination of focal loss and dice loss,\nwith a 20:1 ratio of focal loss to dice loss. Similar to the\napproach taken in SAM to mitigate ambiguity, we predict\nthree masks simultaneously and only back-propagate the\nlowest loss. We also support single mask output by adding a\nfourth output token. During training, we randomly alternate\nbetween the two prediction modes.\nWe train EfficientViT-SAM on the SA-1B dataset for 2\nepochs, utilizing a batch size of 256. The AdamW optimizer\nis employed with a momentum of \u03b21 = 0.9 and \u03b22 = 0.999.\nThe initial learning rate is set to 2e\u22126/1e\u22126 for EfficientViT-\nSAM-L/XL, which is decayed to 0 using a cosine decay\nlearning rate schedule. Regarding data augmentation, we\nuse the random horizontal flip.\n4. Experiment\nIn this section, we begin by conducting a comprehensive\nanalysis of the runtime efficiency of EfficientViT-SAM in\nSection 4.1. Subsequently, we evaluate the zero-shot ca-\npability of EfficientViT-SAM on the COCO [8] and LVIS\n[21] datasets, which were not encountered during the train-\ning process. Two distinct tasks are performed: single point\nvalid mask evaluation in Section 4.2 and box-prompted in-\nCOCO\nLVIS\n1 click 3 click 5 click 1 click 3 click 5 click\nSAM-ViT-H [1]\n58.4\n69.6\n71.4\n59.2\n66.0\n66.8\nEfficientViT-SAM-XL1\n59.8\n71.3\n75.3\n56.6\n67.0\n71.7\nTable 2. Zero-Shot Point-Prompted Segmentation Results.\nstance segmentation in Section 4.3. These tasks individu-\nally assess the effectiveness of the point prompt and box\nprompt features of EfficientViT-SAM. We also provide re-\nsults on SGinW benchmark in Section 4.4.\n4.1. Runtime Efficiency\nWe compare the model parameters, MACs, and through-\nput of EfficientViT-SAM with SAM and other acceleration\nworks.\nResults are shown in Table 1.\nWe conduct the\nthroughput measurements on a single NVIDIA A100 GPU\nwith TensorRT optimization. Our results show that com-\npared to SAM, we achieve an impressive acceleration of 17\nto 69 times. Furthermore, despite having more parameters\nthan other acceleration works, EfficientViT-SAM demon-\nstrates significantly higher throughput due to its effective\nutilization of hardware-friendly operators.\n4.2. Zero-Shot Point-Prompted Segmentation\nWe assess the zero-shot performance of EfficientViT-\nSAM in segmenting objects based on point prompts in Ta-\nble 2. We adopt the point selection method described in\n[1]. That is the initial point is selected as the point located\nfarthest from the object boundary. Each subsequent point\nis chosen as the farthest point from the boundary of the er-\nror region, which is defined as the area between the ground\ntruth and the previous prediction. The performance is re-\nported using 1/3/5 clicks on COCO and LVIS dataset, with\nthe mIoU (mean Intersection over Union) serving as the\nmetric. Our results demonstrate superior performance com-\npared to SAM, particularly when additional point prompts\n3\nmAP\nAirplane-Parts\nBottles\nBrain-Tumor\nChicken\nCows\nElectric-Shaver\nElephants\nFruits\nGarbage\nGinger-Garlic\nHand-Metal\nHand\nHouse-Parts\nHouseHold-Items\nNutterfly-Squireel\nPhones\nPoles\nPuppies\nRail\nSalmon-Fillet\nStrawberry\nTablets\nToolkits\nTrash\nWatermelon\nSAM-ViT-H [1]\n48.7 37.2 65.4 11.9 84.5 47.5 71.7 77.9 82.3 24.0 45.8 81.2 70.0 8.4 60.1 71.3 35.4 23.3 50.1 8.7 32.9 83.5 29.8 20.8 30.0 64.2\nEfficientViT-SAM-XL1 48.9 37.5 66.3 12.1 85.1 46.9 70.4 75.6 82.5 23.9 45.7 74.8 79.6 8.4 60.1 70.9 35.1 20.0 50.1 7.4 40.0 83.3 29.3 20.7 29.4 67.7\nTable 3. Zero-Shot Instance Segmentation Results on Segment in the Wild Benchmark.\nCOCO\nLVIS\nmIoU mIoUS mIoUM mIoUL mIoU mIoUS mIoUM mIoUL\nSAM-ViT-H [1]\n77.4\n72.3\n80.4\n81.8\n77.0\n70.6\n87.5\n89.9\nEfficientViT-SAM-XL1\n79.9\n75.8\n82.2\n83.8\n79.9\n74.4\n88.4\n91.6\nTable 4. Zero-Shot Instance Segmentation Results, Prompted\nwith Ground Truth Bounding Box.\nCOCO\nLVIS\nmAP APS APM APL mAP APS APM APL\nSAM-ViT-H [1]\n46.5\n30.8\n51.0\n61.7\n44.2\n31.8\n57.1\n65.3\nMobileSAM [2]\n38.7\n23.7\n42.2\n54.3\n37.0\n24.7\n47.8\n59.1\nEdgeSAM [3]\n42.1\n26.6\n46.7\n56.9\n39.8\n28.6\n51.3\n59.3\nEfficientSAM [4]\n44.4\n28.4\n48.3\n60.1\n41.5\n29.7\n53.4\n62.2\nEfficientViT-SAM-L0\n45.7\n28.2\n49.5\n63.4\n41.8\n28.8\n53.4\n64.7\nEfficientViT-SAM-L1\n46.2\n28.7\n50.4\n64.0\n42.1\n29.1\n54.3\n65.0\nEfficientViT-SAM-L2\n46.6\n28.9\n50.8\n64.2\n42.7\n29.4\n55.1\n65.5\nEfficientViT-SAM-XL0 47.5\n30.0\n51.5\n64.6\n43.9\n31.2\n56.2\n65.9\nEfficientViT-SAM-XL1 47.8\n30.5\n51.8\n64.7\n44.4\n31.6\n57.0\n66.4\nTable 5. Zero-Shot Instance Segmentation Results, Prompted\nwith ViTDet Boxes.\nare provided.\n4.3. Zero-Shot Box-Prompted Segmentation\nWe evaluate the zero-shot performance of EfficientViT-\nSAM in object segmentation using bounding boxes. We first\ninput ground truth bounding boxes to the model, and the\nresults are presented in Table 4. The mIoU(mean Intersec-\ntion over Union) is reported for all objects, as well as sep-\narately for small, medium, and large objects. Our approach\nsurpasses SAM by a significant margin on the COCO and\nLVIS dataset. Next, we employ an object detector, ViT-\nDet [22], and utilize its output boxes as prompts for the\nmodel. The results in Table 5 demonstrate that EfficientViT-\nSAM achieves superior performance compared to SAM.\nNotably, even the lightest version of EfficientViT-SAM sig-\nnificantly outperforms other acceleration works by a large\nmargin.\nAdditionally,\nwe\nevaluate\nthe\nperformance\nof\nEfficientViT-SAM on the COCO dataset using YOLOv8\nand GroundingDINO [23] as the object detectors. YOLOv8\nYOLOv8\nGroundingDINO\nAP APS APM APL AP APS APM APL\nSAM-ViT-H [1]\n43.8 26.1\n48.1\n60.4 46.9 31.5\n51.8\n64.4\nEfficientViT-SAM-XL1 44.7 26.0\n48.9\n62.9 48.2 31.5\n52.6\n67.3\nTable 6. Zero-Shot Instance Segmentation Results on COCO,\nPrompted with YOLOv8/GroundingDINO Boxes.\nis a real-time object detector suitable for real-world appli-\ncations. On the other hand, GroundingDINO is capable of\ndetecting objects using text prompts, allowing us to perform\nobject segmentation based on textual cues.\nThe results\npresented in Table 6 reveal the outstanding performance of\nEfficientViT-SAM in comparison to SAM.\n4.4. Zero-Shot In-the-Wild Segmentation\nThe Segmentation in the Wild benchmark consists of\n25 zero-shot in-the-wild segmentation datasets. We equip\nEfficientViT-SAM with Grounding-DINO as box prompts\nand perform zero-shot segmentation. The comprehensive\nperformance results for each dataset are presented in Table\n3. SAM achieves an mAP of 48.7, whereas EfficientViT-\nSAM achieves a higher score of 48.9.\n4.5. Qualitative Results.\nFigure 3 showcases the qualitative segmentation results\nof EfficientViT-SAM when provided with point prompt,\nbox prompt, and segment-everything mode.\nThe results\ndemonstrate that EfficientViT-SAM excels not only in seg-\nmenting large objects but also in effectively handling small\nobjects. These findings highlight the superior segmentation\ncapability of EfficientViT-SAM.\n5. Conclusion\nIn this work, we introduced EfficientViT-SAM, which\nutilizes EfficientViT to replace the image encoder of SAM.\nEfficientViT-SAM achieved a significant efficiency boost\nover SAM without sacrificing performance across various\nzero-shot segmentation tasks.\nWe have open-souced our\npretrained models on GitHub to the community.\n4\nFigure 3. Qualitative Segmentation Results of EfficientViT-SAM under point, box, and everything mode.\nAcknowledgments\nWe thank National Science Foundation for supporting\nthis research.\nReferences\n[1] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 1, 2, 3, 4\n[2] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim,\nSung-Ho Bae, Seungkyu Lee, and Choong Seon Hong.\nFaster segment anything: Towards lightweight sam for mo-\nbile applications. arXiv preprint arXiv:2306.14289, 2023. 1,\n2, 3, 4\n[3] Chong Zhou, Xiangtai Li, Chen Change Loy, and Bo Dai.\nEdgesam: Prompt-in-the-loop distillation for on-device de-\nployment of sam. arXiv preprint arXiv:2312.06660, 2023.\n1, 3, 4\n[4] Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xi-\nang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang,\nFei Sun, Forrest Iandola, et al.\nEfficientsam: Leveraged\nmasked image pretraining for efficient segment anything.\narXiv preprint arXiv:2312.00863, 2023. 1, 2, 3, 4\n[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 2\n[6] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models\nand faster training. In International conference on machine\nlearning, pages 10096\u201310106. PMLR, 2021. 2\n[7] Han Cai, Chuang Gan, and Song Han. Efficientvit: Enhanced\nlinear attention for high-resolution low-computation visual\nrecognition. arXiv preprint arXiv:2205.14756, 2022. 1, 2\n[8] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 1, 3\n[9] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin\nJin, Wenjun Zeng, and Zhibo Chen.\nInpaint anything:\nSegment anything meets image inpainting. arXiv preprint\narXiv:2304.06790, 2023. 2\n[10] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li,\n5\nZongxin Yang, Wenguan Wang, and Yi Yang. Segment and\ntrack anything. arXiv preprint arXiv:2305.06558, 2023. 2\n[11] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing\nWang, and Feng Zheng. Track anything: Segment anything\nmeets videos. arXiv preprint arXiv:2304.11968, 2023. 2\n[12] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang,\nChao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Ji-\nayuan Gu, and Hao Su. One-2-3-45++: Fast single image\nto 3d objects with consistent multi-view generation and 3d\ndiffusion. arXiv preprint arXiv:2311.07885, 2023. 2\n[13] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,\nChao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao\nSu. Zero123++: a single image to consistent multi-view dif-\nfusion base model. arXiv preprint arXiv:2310.15110, 2023.\n2\n[14] Han Shu, Wenshuo Li, Yehui Tang, Yiman Zhang, Yi-\nhao Chen, Houqiang Li, Yunhe Wang, and Xinghao Chen.\nTinysam: Pushing the envelope for efficient segment any-\nthing model. arXiv preprint arXiv:2312.13789, 2023. 2\n[15] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017. 2\n[16] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct\nneural architecture search on target task and hardware. arXiv\npreprint arXiv:1812.00332, 2018. 2\n[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\nDistill-\ning the knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2015. 2\n[18] Song Han, Jeff Pool, John Tran, and William Dally. Learn-\ning both weights and connections for efficient neural net-\nwork. Advances in neural information processing systems,\n28, 2015. 2\n[19] Song Han, Huizi Mao, and William J Dally.\nDeep com-\npression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding.\narXiv preprint\narXiv:1510.00149, 2015. 2\n[20] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and\nSong Han. Once-for-all: Train one network and specialize it\nfor efficient deployment. arXiv preprint arXiv:1908.09791,\n2019. 2\n[21] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLvis: A\ndataset for large vocabulary instance segmentation. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 5356\u20135364, 2019. 3\n[22] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object de-\ntection. In European Conference on Computer Vision, pages\n280\u2013296. Springer, 2022. 4\n[23] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection.\narXiv preprint\narXiv:2303.05499, 2023. 4\n6\n"
  },
  {
    "title": "Hydragen: High-Throughput LLM Inference with Shared Prefixes",
    "link": "https://arxiv.org/pdf/2402.05099.pdf",
    "upvote": "14",
    "text": "Hydragen: High-Throughput LLM Inference with Shared Prefixes\nJordan Juravsky * 1 Bradley Brown * 2 Ryan Ehrlich * 3 Daniel Y. Fu 1 Christopher R\u00e9 1 Azalia Mirhoseini 1\nAbstract\nTransformer-based large language models (LLMs)\nare now deployed to hundreds of millions of\nusers. LLM inference is commonly performed\non batches of sequences that share a prefix, such\nas few-shot examples or a chatbot system prompt.\nDecoding in this large-batch setting can be bot-\ntlenecked by the attention operation, which reads\nlarge key-value (KV) caches from memory and\ncomputes inefficient matrix-vector products for\nevery sequence in the batch. In this work, we in-\ntroduce Hydragen, a hardware-aware exact imple-\nmentation of attention with shared prefixes. Hy-\ndragen computes attention over the shared prefix\nand unique suffixes separately. This decomposi-\ntion enables efficient prefix attention by batching\nqueries together across sequences, reducing re-\ndundant memory reads and enabling the use of\nhardware-friendly matrix multiplications. Our\nmethod can improve end-to-end LLM throughput\nby up to 32x against competitive baselines, with\nspeedup growing with the batch size and shared\nprefix length. Hydragen also enables the use of\nvery long shared contexts: with a high batch size,\nincreasing the prefix length from 1K to 16K to-\nkens decreases Hydragen throughput by less than\n15%, while the throughput of baselines drops by\nover 90%. Hydragen generalizes beyond simple\nprefix-suffix decomposition and can be applied to\ntree-based prompt sharing patterns, allowing us\nto further reduce inference time on competitive\nprogramming problems by 55%.\n1. Introduction\nText generation on batches of sequences is a common set-\nting for LLM inference. In many real-world use cases,\nsequences in a batch share a common prefix. Examples\n*Equal\ncontribution\n1Stanford\nUniversity\n2University\nof\nOxford\n3University\nof\nWaterloo.\nCorrespondence\nto:\nJordan Juravsky <jbj@stanford.edu>,\nBradley Brown\n<bradley.brown@cs.ox.ac.uk>.\nPreprint.\nFigure 1. End-to-end decoding throughput in tokens per second\n(TPS) with CodeLlama-13b when generating multiple completions\nfrom a prompt containing 2048 tokens. An \u201cx\u201d indicates that\nFlashAttention does not have enough memory to run. As the batch\nsize grows, Hydragen achieves a significantly higher throughput\nthan vLLM baselines. Throughput with Hydragen always remains\nwithin 50% of the upper bound where attention is entirely removed\nfrom the model. Details are in Section 4.1.\ninclude a chatbot serving many users with shared system\ninstructions (Figure 2 left), an assistant model using a few-\nshot prompt for solving domain-specific tasks (Brown et al.,\n2020), and competitive programming systems that sample\nmany candidate solutions for a single problem (Li et al.,\n2022). As transformer-based LLMs (Vaswani et al., 2023)\nare deployed at increasingly large scales (Malik, 2023), im-\nproving their efficiency with shared prefixes can have a\nsignificant impact. In this work, we use a hardware-aware\nperspective to analyze and optimize this inference setting.\nShared prefixes create overlaps in the attention keys and\nvalues across sequences, presenting an opportunity for spe-\ncialized optimization. Existing work (Kwon et al., 2023)\nidentifies that naive KV caching leads to redundant storage\nof the prefix\u2019s keys and values, and addresses this redun-\ndancy with a paged memory management strategy. While\nthis optimization can significantly reduce GPU memory\nconsumption, it does little to affect the speed of computing\nattention, which can often bottleneck end-to-end throughput\nwith large batches. Since each sequence in the batch has\na distinct (albeit overlapping) KV cache but only a single\nattention query when decoding, existing attention implemen-\ntations like FlashAttention (Dao et al., 2022; Dao, 2023) and\nPagedAttention (Kwon et al., 2023) compute attention by\nperforming many independent matrix-vector products. This\n1\narXiv:2402.05099v1  [cs.LG]  7 Feb 2024\nHydragen: High-Throughput LLM Inference with Shared Prefixes\nYou are ChatGPT, a large language model\ntrained by OpenAI, based on the GPT-4\narchitecture.\nKnowledge cutoff: 2023-04\nCurrent date: 2023-11-16\nImage input capabilities: Enabled\nWhen you send a message containing\nPython code to python, it will be\nexecuted in a stateful Jupyter notebook\nenrivonment. Python will respond...\nShared Prefx\nUnique Sufxes\nHi, can you write a...\nTell me a funny...\nWho is Alan Turing?\nDebug this Python...\nIgnore all previous...\nShared Prefx Setting\nPrefx (K/V)\nInter-Sequence\nBatch (Q)\nAttention Over Prefx\nAttention Over Sufxes\nSoftmax Merging\nHydragen\nMatrix-Vector to Matrix-Matrix\nTensor Core vs. General FLOPs\nK/V\nQ\nTFLOPs\n0\n500\n1K\n2016\n2018\n2020\n2022\nYear\nTensor Cores\nGeneral Arithmetic\nFigure 2. Left: An example LLM inference scenario where a chatbot model processes many sequences that share a large shared prefix (the\nsystem prompt). Middle: An overview of Hydragen, where overall attention is decomposed into attention over the shared prefix (batched\nacross all queries in a batch) and attention over the remaining suffixes (independent across sequences, as is normally done). Top Right:\nHydragen\u2019s attention decomposition allows many matrix vector products to be replaced with fewer matrix-matrix products. Bottom Right:\nUsing matrix-matrix products is particularly important as GPUs dedicate an increasingly large ratio of their total FLOPs to tensor cores\nthat are specialized in matrix multiplication.\napproach is memory-bound when the KV cache is large, and\nmoreover does not use hardware-friendly matrix multiplica-\ntions. Both of these characteristics lead to poor performance\non modern GPUs. Across successive hardware generations,\nGPU computational capability has improved at a signifi-\ncantly faster rate than memory bandwidth. Additionally,\nan increasingly large fraction of total GPU floating-point\noperations (FLOPs) are only available when using tensor\ncores, a specialized hardware feature that is dedicated to\nperforming matrix-matrix products and not matrix-vector\nproducts (Figure 2 bottom right).\nIn this paper, we demonstrate that shared prefixes enable\nmore than just memory savings, and can additionally be\nused to improve decoding throughput. We identify that\nFlashAttention and PagedAttention redundantly read the\nprefix\u2019s keys and values from GPU memory when comput-\ning attention, regardless of whether the prefix is redundantly\nstored. In order to eliminate these redundant reads, we\npresent Hydragen, an exact implementation of attention that\nis specialized for shared prefixes (Figure 2 middle). Hy-\ndragen decomposes full-sequence attention into separate\nattention computations over the prefix and suffixes. These\nsub-computations can be cheaply combined to recover the\noverall attention result (Section 3.1). With attention de-\ncomposition, Hydragen is able to efficiently compute atten-\ntion over the prefix by batching together attention queries\nacross sequences (Section 3.2). This inter-sequence batch-\ning replaces many matrix-vector products with fewer matrix-\nmatrix products (Figure 2 top right), reducing redundant\nreads of the prefix and enabling the use of tensor cores.\nExperimentally, we find that Hydragen can significantly im-\nprove LLM throughput in large-batch settings with shared\nprefixes. In end-to-end benchmarks, Hydragen increases the\nthroughput of CodeLlama-13b (Rozi\u00e8re et al., 2023) by up\nto 32x over vLLM (Kwon et al., 2023), a high-performance\ninference framework that avoids redundant prefix storage\nbut not redundant prefix reads. The attention operation in\nisolation can be accelerated by over 16x using Hydragen\nwhen compared to a state-of-the-art FlashAttention base-\nline, with benefits increasing as the batch size and shared\nprefix length grow. We also demonstrate that Hydragen\u2019s\nefficient processing of shared prefixes can influence algorith-\nmic decisions on how to use LLMs most effectively. With\na large batch size, Hydragen allows the shared prefix to\ngrow from 1K tokens to 16K tokens with less than a 15%\nthroughput penalty whereas vLLM throughput decreases\nby over 90%. On long document question answering tasks,\nwe show that Hydragen can process 512 questions in less\ntime than it takes a FlashAttention baseline to process 64\n(Section 4.3). Finally, we demonstrate that Hydragen\u2019s at-\ntention decomposition and batching apply to more general\npatterns of prompt sharing than a single prefix-suffix split.\nWhen solving APPS competitive programming problems\n(Hendrycks et al., 2021), where two levels of prompt shar-\ning occur, we apply Hydragen hierarchically to maximize\nsharing and reduce evaluation time by an additional 55%\nover a single-level of prompt sharing (Section 4.4).\n2. Background\n2.1. Hardware Efficiency Considerations\nGPU Performance Bottlenecks: GPUs possess a limited\nnumber of processors for performing computation and a\nlimited amount of bandwidth for transferring data between\nprocessors and memory. When a program running on a GPU\nis bottlenecked waiting for compute units to finish process-\ning, it can be classified as compute-bound. Alternatively,\nmemory-bound programs are bottlenecked accessing GPU\nmemory. To summarize a program\u2019s use of hardware re-\nsources, we can calculate its arithmetic intensity, defined as\nthe ratio between the total number of arithmetic operations\nperformed divided by the total number of bytes transferred.\nHigher arithmetic intensities imply a greater use of compu-\n2\nHydragen: High-Throughput LLM Inference with Shared Prefixes\ntational resources relative to memory bandwidth.\nBatching: Batching is a common optimization that can in-\ncrease an operation\u2019s arithmetic intensity and reduce mem-\nory bottlenecks. Consider the example of computing matrix-\nvector products. To compute one product, each element of\nthe input matrix is read from memory but used in only a\nsingle multiply-accumulate. Therefore, the arithmetic inten-\nsity of the operation is low, and is memory-bound on GPUs.\nHowever, if many matrix-vector products need to be com-\nputed using the same matrix, we can batch the operations\ntogether into a single matrix-matrix product. In the batched\noperation, the cost of reading the input matrix is amortized\nover the batch of vectors. Each element of the input matrix\nis now used for many multiply-accumulates, increasing the\narithmetic intensity of the overall operation and improving\nhardware utilization.\nTensor Cores: Modern GPUs (and other AI accelerators)\nare designed with specialized units for efficiently computing\nmatrix multiplications. Effectively using these resources\ncan be crucial for achieving good overall performance; on\nGPUs, tensor cores dedicated to matrix multiplications can\ncompute over 10x more floating-point operations per second\nthan the rest of the GPU. This further motivates batching\nmatrix-vector products into matrix-matrix products.\n2.2. Attention and LLM Inference\nThe focus of this work is optimizing attention in transformer-\nbased LLMs. Scaled-dot-product (SDP) attention operates\non a sequence of queries Q \u2208 RNq\u00d7d, keys K \u2208 RNkv\u00d7d,\nand values V \u2208 RNkv\u00d7d, and is defined as:\nSDP(Q, K, V ) = softmax\n\u0012QKT\n\u221a\nd\n\u0013\nV\n(1)\nWe are particularly interested in the performance character-\nistics of attention during LLM text generation. Generation\nbegins with a prefill stage that processes the starting se-\nquence of tokens that the LLM will complete. The prefill\nphase encodes the entire prompt in parallel using a single\ntransformer forward pass. Therefore, during prefix attention\nwe have Nq = Nkv \u226b 1 and as a result the multiplications\ninvolving KT and V are hardware-friendly matrix multi-\nplications. After the prefill stage, completion tokens are\niteratively decoded from the model, with one decoding step\nproducing one new token and requiring one forward pass.\nDecoding is accelerated by the use of a KV cache, which\nstores the attention keys and values of all previous tokens\nin the sequence. The KV cache avoids the needed for re-\nprocessing the entire sequence during every decoding step,\nand instead only the most recent token is passed through\nthe model. However, this leads to an attention computation\nwhere Nq = 1 while Nkv \u226b 1, making the multiplications\nwith KT and V matrix vector products. Attention during de-\ncoding is therefore memory-bound and does not use tensor\ncores.\n2.3. Batched Inference\nLLM inference throughput can be increased by generating\ntext for a batch of sequences in parallel. With batched de-\ncoding, each forward pass of the model processes the most\nrecent token from many sequences instead of only one. This\nbatching increases the arithmetic intensity of transformer\ncomponents such as the multilayer perceptron (MLP) blocks,\nachieving better hardware utilization and therefore increas-\ning overall throughput. However, batched text generation\ndoes not increase the intensity of the attention operation,\nsince every sequence has a distinct key and value matrix.\nTherefore, while operations such as linear layers can be\nimplemented with efficient matrix multiplications during\nbatched decoding, attention instead must compute many\nindependent matrix-vector products. With large batch sizes\nor long sequence lengths, computing attention becomes\nincreasingly expensive relative to rest of the transformer,\ndecreasing throughput. Additionally, the storage footprint\nof the KV cache in GPU memory can exceed that of the\nmodel parameters when the batch size is large, imposing\nconstraints on the maximum number of sequences that can\nbe simultaneously processed.\n2.4. Shared Prefixes\nIn this paper, we investigate improving the throughput of\nbatched text generation when the sequences in the batch\nshare a common prefix. This scenario lends itself to special-\nized optimizations because shared prefixes lead to overlap\nin the key and value matrices of sequences in the batch.\nThe causal attention mask in LLMs results in each token\u2019s\nactivations only being influenced by previous tokens in the\nsequence. Therefore, if multiple sequences share a com-\nmon prefix, the keys and values corresponding to the prefix\ntokens will be identical across sequences.\nThe key-value overlap introduced by shared prefixes\npresents two distinct directions for improving the inference\nprocess described in Section 2.3. Firstly, naive batched in-\nference stores the KV cache separately for every sequence,\nleading to redundant storage of the prefix key and value\nvectors. Existing work has identified this redundancy and\nproposed an elegant virtual memory system to eliminate\nduplicate storage (Kwon et al., 2023).\nIn this work, we identify a new opportunity to optimize\nthe attention operation itself. When GPU kernels compute\nattention for each sequence in the batch using independent\nmatrix-vector products, the prefix keys and values are repeat-\nedly read from GPU memory, regardless of whether they are\n3\nHydragen: High-Throughput LLM Inference with Shared Prefixes\nstored redundantly or not. We now propose an alternative\napproach to computing attention, which can simultaneously\neliminate these redundant reads while enabling the use of\ntensor cores.\n3. Hydragen: Efficient Attention with Shared\nPrefixes\nWe introduce Hydragen, an exact implementation of atten-\ntion that is optimized for shared prefixes. Hydragen is a\ncombination of two techniques:\n1. Attention Decomposition: We split full-sequence at-\ntention into separate attention computations over the\nshared prefix and unique suffixes that can be cheaply\ncombined to recover the full attention result.\n2. Inter-Sequence Batching: We efficiently compute\nattention over the prefix by batching together attention\nqueries across sequences.\nAttention decomposition allows us to isolate overlapping\nportions of the batch\u2019s key and value matrices, while inter-\nsequence batching exploits this overlap by replacing many\nmatrix-vector products with a single matrix-matrix product.\nPseudocode implementing Hydragen attention is provided\nin Appendix B.\n3.1. Decomposing Attention Across Subsequences\nAs discussed in Section 2.4, sequences that share a common\nprefix have partially overlapping keys and values when com-\nputing attention. Our goal is to separate this computation\nwith partial overlap into two separate operations: attention\nover the shared prefix, where there is total key-value over-\nlap, and attention over unique suffixes, where there is no\noverlap.\nConsider the general case where our keys K and values V\nare partitioned across the sequence (row) dimension into:\nK = K1||K2\n(2)\nV = V1||V2\n(3)\nwith || denoting concatenation along the row axis. We\nwish to avoid directly computing our desired quantity\nSDP (Q, K, V ), and instead calculate this value using\nthe results of the sub-computations SDP (Q, K1, V1) and\nSDP (Q, K2, V2).\nThe challenge in partitioning attention is with the softmax\noperation, since the softmax denominator is calculated by\nsumming over all exponentiated attention scores in the se-\nquence. In order to combine our sub-computations, we use\na denominator rescaling trick inspired by FlashAttention\u2019s\nblocked softmax computation (Dao et al., 2022). When\ncomputing SDP (Q, K1, V1) and SDP (Q, K2, V2), we ad-\nditionally compute and store the log-sum-exp (LSE) of the\nattention scores (equivalently, the log of the softmax denom-\ninator):\nLSE (Q, K) = log\n\u0012\nsum\n\u0012\nexp\n\u0012QKT\n\u221a\nd\n\u0013\n, dim = 1\n\u0013\u0013\n(4)\nGiven the two partitioned attention outputs and their LSEs,\nwe can calculate our final result SDP (Q, K, V ) by comput-\ning the full-sequence softmax denominator and rescaling\nthe attention outputs accordingly:\nSDP (Q, K1, V1) eLSE(Q,K1) + SDP (Q, K2, V2) eLSE(Q,K2)\neLSE(Q,K1) + eLSE(Q,K2)\n(5)\nWe provide more details on this formula in Appendix A.\n3.2. Inter-Sequence Batched Prefix Attention\nWith attention decomposition, we are able to compute at-\ntention over the prefix as a standalone operation for every\nsequence. While this decomposition does not improve per-\nformance on its own (in fact, it introduces additional work\nin order to combine sub-computation outputs), it can allow\nus to compute prefix attention much more efficiently over a\nbatch of sequences.\nQueries do not affect each other when computing attention,\ntherefore if two sets of queries attend over identical keys and\nvalues, they can be merged into a single attention operation\nwith a larger number of queries. With attention decompo-\nsition, this case now applies to each sequence\u2019s attention\noperation over the shared prefix. Since the prefix\u2019s keys\nand values across sequences are identical, we can batch\nevery sequence\u2019s query vector together into one attention\noperation over a single sequence. Importantly, this batching\nsignificantly raises Nq and the arithmetic intensity of prefix\nattention, replacing many separate matrix-vector products\nwith a single matrix-matrix product. By replacing multiple\nindependent attention computations over the prefix with a\nsingle batched operation, we can reduce the number of times\nthat the prefix KV cache is read from GPU memory. Addi-\ntionally, we can now use tensor cores during prefix attention\nand significantly improve hardware utilization.\nNote that we are unable to apply inter-sequence batching\nwhen computing attention over suffixes, since the keys and\nvalues in each sequence\u2019s suffix are not identical. Suffix\n4\nHydragen: High-Throughput LLM Inference with Shared Prefixes\nFigure 3. An example of a batch of sequences with a hierarchical\nsharing pattern. This diagram depicts the setting of Section 4.4,\nwhich solves competitive programming problems using a few-shot\nprompt and by sampling many candidate solutions per problem.\nThe few-shot prompt (orange) is globally shared across all se-\nquences in the batch. However, the descriptions of each problem\n(green and blue) are only shared across the candidate solutions\ncorresponding to that problem.\nattention is therefore computed normally, with a single query\nper sequence.\n3.3. Hierarchical Sharing\nSo far, we have focused on the setting where all sequences in\nthe batch share a common starting subsequence followed by\nsuffixes that are entirely distinct from one another. However,\nthis excludes other forms of sharing that appear in impor-\ntant use cases. Sequences in the batch may not all start\nwith a global prefix, and instead the batch may be divided\ninto groups of overlapping sequences. Additionally, sharing\nmay be more fine-grained than a simple prefix-suffix de-\ncomposition, with the overlap between sequences forming a\ntree structure where each node contains a token sequence\nthat is shared by all descendants (see Figure 3 for an ex-\nample). These forms of sharing are increasingly relevant\nas LLMs are applied in more complicated inference/search\nalgorithms (Yao et al., 2023; Besta et al., 2023; Ning et al.,\n2023).\nHydragen naturally generalizes to these richer forms of\nsharing as well. To apply Hydragen to a tree of sequences,\nwe replace attention decomposition over the prefix and suffix\ninto attention decomposition at every vertex in the tree. We\ncan then use inter-sequence batching across levels of the\ntree, so that the keys and values associated with one node\nin the tree are shared across the queries of all descendant\nnodes.\n3.4. Estimating Throughput Improvements with\nHydragen\nHydragen significantly improves the efficiency of attention\nwith shared prefixes relative to approaches that compute\nattention independently for every sequence (Section 4.2).\nHowever, translating this targeted efficiency into end-to-end\nthroughput improvements depends strongly on the details of\nthe inference setting being considered. In general, in order\nfor Hydragen to meaningfully improve decoding speed in a\nparticular setting, attention must be a major contributor to\ndecoding time. For example, with small batch sizes or short\nsequence lengths, decoding speed is often bottlenecked not\nby attention, but by reading the parameters of the model\nfrom GPU memory. The benefits of Hydragen in this sce-\nnario will therefore be minimal. Similarly, given a fixed\nbatch size and sequence length, we expect Hydragen to im-\nprove throughput more on a model that uses multi-headed at-\ntention than a similarly-sized model that uses multi-query at-\ntention (Shazeer, 2019) or grouped-query attention (Ainslie\net al., 2023) in order to reduce the size of the KV cache.\nHowever, reducing the KV cache size allows for a larger\nbatch size to fit within GPU memory constraints, which can\nfurther increase the speedup of using Hydragen.\nAs discussed in Section 2.3, the cost of attention becomes\ndisproportionately high as the batch size grows, since the\narithmetic intensity of most transformer operations improve\nwhile attention remains memory-bound. Hydragen greatly\nimproves the hardware utilization of attention, making the\ncomparison of attention FLOPs to other model FLOPs more\nuseful when determining the maximum achievable speedup.\nIn several experiments in Section 4, we include a \u201cNo Atten-\ntion\u201d baseline that only runs the non-attention components\nof the transformer in order to establish an upper bound for\nattainable throughput.\nAnother important consideration when predicting the bene-\nfits of Hydragen is the relative number of prefix (i.e. shared)\ntokens compared to suffix (i.e. unshared) tokens. Since\nHydragen makes no optimizations to attention over suf-\nfixes, long suffixes can decrease generation throughput. We\nexplore the impact of suffix length on attention speed in\nSection 4.2.\n3.5. Implementation\nWe implement Hydragen for the Llama family of models\n(Touvron et al., 2023a;b; Rozi\u00e8re et al., 2023). We high-\nlight that our implementation is simple: we use no custom\nCUDA code and write Hydragen entirely in PyTorch1 plus\ncalls to a fast attention primitive. This contrasts with more\nsophisticated algorithms like PagedAttention, which require\nbespoke GPU kernels to read from and update the paged KV\ncache. We believe that Hydragen\u2019s simplicity will allow it to\nbe easily ported to other hardware platforms such as TPUs,\nwhich also have hardware dedicated to fast matrix multi-\n1For non-hierarchical inputs, we\u2019ve also written a Triton kernel\nto combine softmax denominators, which can improve attention\nperformance by about 10-20%.\n5\nHydragen: High-Throughput LLM Inference with Shared Prefixes\nFigure 4. Comparing decoding throughput of CodeLlama-13b be-\ntween Hydragen, vLLM (with and without tokenization), and \u201cNo\nAttention\u201d, where the attention operation is removed from the\nmodel to demonstrate the throughput ceiling. In this scenario\nwhere the batch size is fixed, Hydragen improves throughput by\nup to 32x over the best baseline, with speedups increasing with\nprefix length.\nplications. In our implementation, we use version 2.3.6 of\nthe flash-attn package when attending over the prefix,\nand a Triton kernel from xformers when attending over\nthe suffix. The second kernel allows us to have changing\nsequence lengths in the suffix KV cache across decoding\nsteps while still adhering to the constraints required to use\nCUDA graphs.\n4. Experiments\n4.1. End-To-End Throughput\nWe benchmark end-to-end LLM throughput in the setting\nwhere many completions are sampled from a single prompt.\nThis is a common technique for improving a model\u2019s ability\nat solving complex problems such as math or competitive\nprogramming (Rozi\u00e8re et al., 2023; Li et al., 2022). Our\nbenchmarks evaluate Hydragen against four baselines:\n1. FlashAttention: We perform inference without any\nshared prefix optimizations, as if all sequences in the\nbatch were fully distinct. We compute full-sequence\nattention using the Triton kernel that Hydragen uses for\nsuffix attention, and otherwise use the same codebase\nas Hydragen. This baseline redundantly stores the\nprefix\u2019s keys and values for every sequence in the batch,\ncausing this method to run out of memory quickly.\n2. vLLM: We use version 0.2.7 of the vllm package,\nwhich uses the PagedAttention algorithm.\nvLLM\navoids redundant storage of the prefix, allowing much\nlarger batch sizes to be tested. Additionally, because\nof this non-redundant storage, PagedAttention can\nachieve a higher GPU cache hit rate when reading\nthe prefix, reducing the cost of redundant reads.\n3. vLLM without Detokenization: We disable incremen-\ntal detokenization in vLLM (accomplished by com-\nmenting out one line in the vLLM codebase), which\nwe observed to improve throughput.\n4. No Attention: We skip all self-attention computations\nin the transformer. This (functionally incorrect) base-\nline provides a throughput ceiling and helps to illustrate\nthe cost of different attention implementations relative\nto the rest of the transformer. Note that the query, key,\nvalue, and output projections in the attention block are\nstill performed.\nWe run our benchmarks on CodeLlama-13b (Rozi\u00e8re et al.,\n2023) and distribute the model with tensor parallelism\nacross eight A100-40GB GPUs, in order to have enough\nGPU memory to store the KV cache with large batch sizes.\nIn Figure 1, we fix the prefix length to 2048 and sweep\nover the batch size while generating 128 tokens per comple-\ntion. When the batch size is small, non-attention operations\ncontribute significantly to decoding time, with all methods\nreaching at least half of the throughput of no-attention up-\nper bound. At these low batch sizes, Hydragen, the vLLM\nbaselines, and the FlashAttention baselines have similar\nthroughputs. However, as the batch size grows and attention\nover the prefix becomes increasingly expensive, Hydragen\nbegins to significantly outperform the other baselines.\nIn Figure 4, we run a similar experiment, except now we\nhold the batch size constant at 1024 and sweep over the\nshared prefix length. The throughput of vLLM significantly\ndecreases as the prefix grows, from just under 5k tokens/sec-\nond with a prefix length of 1024 to less than 500 tokens/sec-\nond with a prefix length of 16256. However, with Hydragen,\nthroughput is largely unaffected despite the prefix growing\nby over 15k tokens. Moreover, across all sequence lengths\ntested, Hydragen throughput is always within 70% of the\nno-attention ceiling. We perform more in-depth sweeps over\ndifferent models, prefix lengths, batch sizes, and numbers\nof generated tokens in Appendix C.1 - for smaller mod-\nels and shorter completions lengths, Hydragen\u2019s speedup\ncan exceed 50x. Additional evaluation setup details are in\nAppendix D.1.\n4.2. Microbenchmarking Attention\nWe also perform more granular benchmarks comparing Hy-\ndragen attention against FlashAttention, in order to more\nprecisely demonstrate the performance characteristics of\nour method. Our microbenchmarks run on a single A100-\n40GB using eight query attention heads, one key and value\nhead, and a head dimension of 128 (matching the setting\nof CodeLlama-34b when distributed across eight GPUs).\nWe sweep over different batch sizes, prefix lengths, and\nsuffix lengths, reporting our results in Figure 5. Our mi-\ncrobenchmarks corroborate our end-to-end measurements\n6\nHydragen: High-Throughput LLM Inference with Shared Prefixes\nFigure 5. Measuring the speedup of Hydragen attention over\nFlashAttention across various batch sizes, shared prefix lengths\nand suffix lengths on an A100-40GB GPU. We see that Hydragen\nresults in faster inference in all cases, in particular when the ratio\nof shared length to unique length is high and the batch size is large.\nWe observe even larger performance gains when running on an\nL40S (a GPU with a higher compute-to-bandwidth ratio than an\nA100), shown in in Figure 8.\nfrom Figures 1 and 4 that the speedup with Hydragen in-\ncreases as the batch size and prefix lengths grow. However,\nthe microbenchmarks also highlight the significant impact\nof the suffix length on inference time. Hydragen computes\nattention over suffixes using memory-bound FlashAttention\n(without inter-sequence batching). As the suffix lengths\ngrow, reading this portion of the KV cache becomes an\nincreasingly significant contributor to total execution time.\nWhen generating text using Hydragen, this means that the\nfirst tokens decoded by the model are generated the fastest,\nwith throughput decreasing over time as the lengths of com-\npletions (and therefore the lengths of suffixes) grow.\nThese microbenchmarks are also influenced by the hardware\nplatform that they are run on. GPUs with a higher ratio of\ncompute to memory bandwidth benefit more from Hydragen\neliminating memory bottlenecks when attending over the\nprefix. We report results on other GPUs in Appendix C.2\nand provide more evaluation details in Appendix D.2.\n4.3. Long Document Question Answering\nAdditionally, we explore the performance of Hydragen on\nworkloads involving very long documents. We construct a\ndocument by embedding synthetic facts into an excerpt of\nWar and Peace. Our shared prefix, totalling 19947 tokens,\ncontains both the document as well as five few-shot exam-\nples of question/answer pairs. Our benchmark evaluates\nYi-6B-200k (01-ai, 2023) on its ability to answer questions\nbased on the embedded facts. We run this benchmark us-\ning Hydragen as well as with our FlashAttention and no-\nFigure 6. Time to answer questions about a 19947 token-long doc-\nument. An \u201cx\u201d indicates that FlashAttention does not have enough\nmemory to run. Time to process prefix excluded.\nattention baselines. Results are reported in Figure 6. We\nobserve that processing time for the FlashAttention baseline\nrapidly grows far beyond the time of the no-attention base-\nline, highlighting how attention is the dominant operation\nfor this configuration. Meanwhile, Hydragen\u2019s processing\ntime always remains within 75% of the no-attention opti-\nmum. Notably, Hydragen can process 512 questions in less\ntime than it takes the FlashAttention baseline to process\n64 questions. We provide additional evaluation details in\nAppendix D.3.\n4.4. Hierarchical Sharing in Competitive Programming\nWe lastly demonstrate the benefits of applying Hydragen to a\nsetting with hierarchical sharing. Competitive programming\nwas a motivating application for developing Hydragen, since\ncurrent state-of-the-art systems sample a very large number\nof candidate programs from prompts that can contain thou-\nsands of tokens (Li et al., 2022; Rozi\u00e8re et al., 2023). In\nthis experiment, we benchmark the total time required to\nevaluate CodeLlama-7b on 120 problems from the APPS\ndataset (Hendrycks et al., 2021) using a two-shot prompt\nand 128 candidate programs per problem. When multiple\nproblems are processed in a single batch, prompt overlap oc-\ncurs across two levels: the few-shot prompt is shared across\nall sequences in the batch, while each problem\u2019s description\nis shared across all of the problem\u2019s candidate solutions (see\nFigure 3).\nWe run this benchmark using two methods:\n1. Single-Level Hydragen: We use a single-level ver-\nsion of Hydragen to share the few-shot prompt across\nall sequences in the batch, and not share problem de-\nscriptions across candidate solutions. This leads to\nredundant storage of the problem description across all\n7\nHydragen: High-Throughput LLM Inference with Shared Prefixes\nFigure 7. Time to run inference over a dataset of 120 APPS coding\nproblems, sampling 128 solutions per problem with two few-shot\nexamples. The batch size refers to the number of problems pro-\ncessed simultaneously. Across all Hydragen runs (both single and\ntwo-level), the few-shot prompt is shared across all sequences. By\nadditionally sharing the problem description across generated can-\ndidate solutions, two-level Hydragen decreases overall inference\ntime by an extra 55% over single-level Hydragen.\ncandidate solutions, reducing the maximum batch size\nthat can be used.\n2. Two-Level Hydragen: We apply Hydragen across\nboth levels of prompt overlap. This has the dual ben-\nefits of improving attention efficiency (by increasing\nthe degree of sharing) as well as avoiding redundant\nstorage, which allows us to increase the batch size used\nfor evaluation. We avoid conflating these benefits by\nevaluating two-level Hydragen twice: once with the\nsame batch size used for single-level Hydragen, and\nonce with an enlarged batch size.\nWe report our results in Figure 7. We see that even when the\nbatch size is held constant, adding a second level of sharing\nto Hydragen can improve attention efficiency and decrease\ndataset evaluation time by 18%. Furthermore, the memory\nsaved due to not redundantly storing the problem description\nallows us to increase the batch size, which in turn results in\nan additional 45% reduction in evaluation time. We provide\nadditional evaluation details in Appendix D.4.\n5. Discussion\nIn this work we introduced Hydragen, an exact, hardware-\naware implementation of attention for batches of sequences\nthat share common prefixes. Our method separates attention\nover shared prefixes from attention over unique suffixes.\nThis allows us to batch attention queries across sequences\nwhen attending over the prefix, reducing redundant memory\nreads and enabling the use of tensor cores. Hydragen can\nimprove LLM throughput in scenarios where attention is a\nsignificant contributor to decoding time, with the greatest\nspeedup occurring when the batch size is large, the shared\nprefix lengths are long, and the unique suffix lengths are\nshort.\nWe emphasize that Hydragen is an optimization that can\nbe applied as part of a larger inference framework, and is\nnot intended to be an end-to-end inference solution. Our\nproof-of-concept implementation of Hydragen requires that\nthe user specifies where sharing occurs across the input\nsequences. We are excited about future work that incor-\nporates Hydragen into systems that continuously receive\nrequests and schedule sequences for generation (Yu et al.,\n2022; Kwon et al., 2023), such that overlapping sequences\ncan be dynamically identified and exploited.\nWe hope that our work inspires new LLM algorithms that\nleverage efficient handling of shared prefixes. Hydragen\u2019s\nability to significantly expand the shared prefix without\na significant throughput penalty should allow models to\nbe provided with much more context than was previously\npractical. Moreover, we hope that Hydragen\u2019s ability to\ngeneralize to tree-shaped sharing patterns can assist with\nresearch that uses LLMs to explore many possible solutions\nbefore deciding on a final output.\n6. Related Work\nTransformers and Language Models: The transformer ar-\nchitecture has enabled significant improvements in state-of-\nthe-art language models (Vaswani et al., 2023). A defining\nfeature of transformers is that their performance consistently\nimproves when scaling up data and model size (Radford\net al., 2019; Brown et al., 2020; Chowdhery et al., 2022;\nHoffmann et al., 2022; OpenAI, 2023). LLM-powered as-\nsistants such as ChatGPT have been widely adopted and\nare currently used by over a hundred million users (Malik,\n2023), motivating research into how these models can be\ndeployed more efficiently.\nKV Cache Management: Managing large KV caches is\na challenge when deploying LLMs. MQA (Shazeer, 2019)\nand GQA (Ainslie et al., 2023) modify the transformer ar-\nchitecture in order to reduce the KV cache size. These tech-\nniques decrease the number of key-value attention heads and\nassign multiple query heads to a single key-value head. Al-\nternative approaches operate at a systems level, dynamically\nmoving keys and values between GPU memory, CPU mem-\nory, and disk (Sheng et al., 2023; Aminabadi et al., 2022;\nHuggingFace, 2022). vLLM (Kwon et al., 2023) introduces\na virtual paging system that enables fine-grained KV cache\nmanagement. This virtual paging can also avoid redundant\nstorage of a prefix\u2019s keys and values. Concurrent with our\nwork, SGLang (Zheng et al., 2023) also investigates and\noptimizes inference with sequences that have complicated\nprompt sharing patterns. Their RadixAttention algorithm\n8\nHydragen: High-Throughput LLM Inference with Shared Prefixes\ndynamically scans incoming requests to find the largest\nsubsequence that has already been processed, avoiding the\nrecomputation of overlapping keys and values. Importantly,\nwhile both vLLM and RadixAttention avoid redundant stor-\nage of overlapping keys and values, they do not optimize\nthe attention computation itself.\nHardware-Aware Algorithms: Algorithms that leverage\nan understanding of the underlying hardware platform\ncan significantly improve device utilization. Hardware-\nawareness has significantly improved the efficiency of the\nattention operation (Rabe & Staats, 2022; Dao et al., 2022;\nDao, 2023), reducing the memory requirements from O(N 2)\nto O(N) while improving execution time by avoiding re-\ndundant memory transfers. In addition to improving input-\noutput (IO) transfers, many GPU-aware algorithms (includ-\ning Hydragen) focus on leveraging tensor cores (Fu et al.,\n2023), which can achieve over 10x more FLOPs per second\nthan the rest of the GPU.\nLLM Algorithms: Recent work has demonstrated that\nLLM capabilities can be improved when many potential\nsolutions are explored when solving a problem.\nSelf-\nconsistency (Wang et al., 2023) improves performance on\narithmetic reasoning tasks by sampling many solutions to\na single problem and using a majority-voting protocol. On\ncompetitive programming problems, LLMs perform sub-\nstantially better when many different attempts to a problem\nare sampled (Rozi\u00e8re et al., 2023). AlphaCode (Li et al.,\n2022), a state-of-the-art competitive programming system,\nsamples as many as a million programs to solve a single\nproblem. Tree-of-Thoughts (Yao et al., 2023) introduces an\nexplicit tree-based search algorithm for solving problems\nthat can be decomposed into discrete decision points. All of\nthese scenarios involve performing batched text generation\nwith overlapping prefixes, which Hydragen is specifically\noptimized for.\nReferences\n01-ai. Yi, 2023. URL https://github.com/01-ai/\nYi.git. Accessed: 2024-02-01.\nAinslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y.,\nLebr\u00f3n, F., and Sanghai, S. Gqa: Training generalized\nmulti-query transformer models from multi-head check-\npoints, 2023.\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C.,\nLi, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M.,\nRasley, J., et al. Deepspeed-inference: Enabling efficient\ninference of transformer models at unprecedented scale.\nIn 2022 SC22: International Conference for High Per-\nformance Computing, Networking, Storage and Analysis\n(SC), pp. 646\u2013660. IEEE Computer Society, 2022.\nBesta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gi-\naninazzi, L., Gajda, J., Lehmann, T., Podstawski, M.,\nNiewiadomski, H., Nyczyk, P., and Hoefler, T. Graph of\nthoughts: Solving elaborate problems with large language\nmodels, 2023.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,\nG., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,\nG., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,\nWu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,\nLitwin, M., Gray, S., Chess, B., Clark, J., Berner, C.,\nMcCandlish, S., Radford, A., Sutskever, I., and Amodei,\nD. Language models are few-shot learners. In Larochelle,\nH., Ranzato, M., Hadsell, R., Balcan, M., and Lin,\nH. (eds.), Advances in Neural Information Processing\nSystems, volume 33, pp. 1877\u20131901. Curran Asso-\nciates, Inc., 2020.\nURL https://proceedings.\nneurips.cc/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,\nN., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,\nYin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S.,\nMichalewski, H., Garcia, X., Misra, V., Robinson, K., Fe-\ndus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,\nB., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,\nS., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,\nLewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,\nK., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,\nCatasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,\nJ., Petrov, S., and Fiedel, N. Palm: Scaling language\nmodeling with pathways, 2022.\nDao, T. FlashAttention-2: Faster attention with better paral-\nlelism and work partitioning. 2023.\nDao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00e9, C. FlashAt-\ntention: Fast and memory-efficient exact attention with\nIO-awareness. In Advances in Neural Information Pro-\ncessing Systems, 2022.\nFu, D. Y., Kumbong, H., Nguyen, E., and R\u00e9, C. Flashfft-\nconv: Efficient convolutions for long sequences with ten-\nsor cores, 2023.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\n9\nHydragen: High-Throughput LLM Inference with Shared Prefixes\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., de Las Casas, D., Hendricks,\nL. A., Welbl, J., Clark, A., Hennigan, T., Noland, E.,\nMillican, K., van den Driessche, G., Damoc, B., Guy,\nA., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W.,\nVinyals, O., and Sifre, L. Training compute-optimal large\nlanguage models, 2022.\nHuggingFace.\nHugging face accelerate.\nhttps://\nhuggingface.co/docs/accelerate/index,\n2022.\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient\nmemory management for large language model serving\nwith pagedattention. In Proceedings of the 29th Sym-\nposium on Operating Systems Principles, pp. 611\u2013626,\n2023.\nLi, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser,\nJ., Leblond, R., Eccles, T., Keeling, J., Gimeno, F.,\nDal Lago, A., Hubert, T., Choy, P., de Masson d\u2019Autume,\nC., Babuschkin, I., Chen, X., Huang, P.-S., Welbl, J.,\nGowal, S., Cherepanov, A., Molloy, J., Mankowitz,\nD. J., Sutherland Robson, E., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O.\nCompetition-\nlevel code generation with alphacode.\nScience, 378\n(6624):1092\u20131097, December 2022. ISSN 1095-9203.\ndoi: 10.1126/science.abq1158. URL http://dx.doi.\norg/10.1126/science.abq1158.\nMalik, A. Openai\u2019s chatgpt now has 100 million weekly\nactive users. https://techcrunch.com/2023/11/06/openais-\nchatgpt-now-has-100-million-weekly-active-users/, 2023.\nAccessed: 2023-11-06.\nNing, X., Lin, Z., Zhou, Z., Wang, Z., Yang, H., and Wang,\nY. Skeleton-of-thought: Large language models can do\nparallel decoding, 2023.\nOpenAI. Gpt-4 technical report, 2023.\nRabe, M. N. and Staats, C. Self-attention does not need\no(n2) memory, 2022.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. 2019.\nRozi\u00e8re, B., Gehring, J., Gloeckle, F., Sootla, S., Gat,\nI., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J.,\nKozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer,\nC. C., Grattafiori, A., Xiong, W., D\u00e9fossez, A., Copet, J.,\nAzhar, F., Touvron, H., Martin, L., Usunier, N., Scialom,\nT., and Synnaeve, G. Code llama: Open foundation mod-\nels for code, 2023.\nShazeer, N. Fast transformer decoding: One write-head is\nall you need, 2019.\nSheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Fu,\nD. Y., Xie, Z., Chen, B., Barrett, C., Gonzalez, J. E.,\nLiang, P., R\u00e9, C., Stoica, I., and Zhang, C. Flexgen: High-\nthroughput generative inference of large language models\nwith a single gpu, 2023.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E.,\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\nple, G. Llama: Open and efficient foundation language\nmodels, 2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\nand Scialom, T. Llama 2: Open foundation and fine-tuned\nchat models, 2023b.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need, 2023.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang,\nS., Chowdhery, A., and Zhou, D. Self-consistency im-\nproves chain of thought reasoning in language models,\n2023.\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao,\nY., and Narasimhan, K. Tree of thoughts: Deliberate\nproblem solving with large language models, 2023.\nYu, G.-I., Jeong, J. S., Kim, G.-W., Kim, S., and Chun, B.-\nG. Orca: A distributed serving system for {Transformer-\nBased} generative models. In 16th USENIX Symposium\non Operating Systems Design and Implementation (OSDI\n22), pp. 521\u2013538, 2022.\nZheng, L., Yin, L., Xie, Z., Huang, J., Sun, C., Yu, C. H.,\nCao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., Barrett,\nC., and Sheng, Y. Efficiently programming large language\nmodels using sglang, 2023.\n10\nHydragen: High-Throughput LLM Inference with Shared Prefixes\nA. Proving the Correctness of Attention Decomposition\nWe start by explicitly expressing softmax as an exponentiation followed by a normalization:\nsoftmax\n\u0012QKT\n\u221a\nd\n\u0013\n=\nexp\n\u0010\nQKT\n\u221a\nd\n\u0011\neLSE(Q,K)\n(6)\nTherefore we can rewrite Equation 1 as:\nSDP(Q, K, V ) =\n\uf8eb\n\uf8ed\nexp\n\u0010\nQKT\n\u221a\nd\n\u0011\neLSE(Q,K)\n\uf8f6\n\uf8f8 V\n(7)\nWe can then expand Equation 5:\nSDP (Q, K1, V1) eLSE(Q,K1) + SDP (Q, K2, V2) eLSE(Q,K2)\neLSE(Q,K1) + eLSE(Q,K2)\n(8)\n=\n\uf8eb\n\uf8ed\nexp\n\u0012\nQKT\n1\n\u221a\nd\n\u0013\neLSE(Q,K1)\n\uf8f6\n\uf8f8 V1eLSE(Q,K1) +\n\uf8eb\n\uf8ed\nexp\n\u0012\nQKT\n2\n\u221a\nd\n\u0013\neLSE(Q,K2)\n\uf8f6\n\uf8f8 V2eLSE(Q,K2)\neLSE(Q,K1) + eLSE(Q,K2)\n(9)\n=\nexp\n\u0010\nQKT\n1\n\u221a\nd\n\u0011\nV1 + exp\n\u0010\nQKT\n2\n\u221a\nd\n\u0011\nV2\neLSE(Q,K1) + eLSE(Q,K2)\n(10)\n=\nexp\n\u0010\nQ(K1||K2)T\n\u221a\nd\n\u0011\n(V1||V2)\neLSE(Q,K1||K2)\n(11)\n= SDP (Q, K1||K2, V1||V2)\n(12)\nas required.\nB. Hydragen Pseudocode\nWe provide PyTorch-style pseudocode implementing Hydragen attention below. We highlight that Hydragen can be\nimplemented easily and efficiently in existing machine learning libraries, as long as there is a fast attention primitive that\nreturns the LSE needed for softmax recombination.\n1 import torch\n2 from torch import Tensor\n3\n4 def attention(q: Tensor, k: Tensor, v: Tensor) -> tuple[Tensor, Tensor]:\n5\n\"\"\"\n6\nPlaceholder for some fast attention primitive\n7\nthat also returns LSEs. We use the flash-attn\n8\npackage in our implementation.\n9\n10\nq shape: [batch, qseq_len, qheads, dim]\n11\nk shape: [batch, kvseq_len, kvheads, dim]\n12\nv shape: [batch, kvseq_len, kvheads, dim]\n13\n\"\"\"\n14\npass\n15\n16 def combine_lse(\n11\nHydragen: High-Throughput LLM Inference with Shared Prefixes\n17\nout1: Tensor,\n18\nlse1: Tensor,\n19\nout2: Tensor,\n20\nlse2: Tensor,\n21 ):\n22\n\"\"\"\n23\nCombines two attention results using their LSEs.\n24\n25\nOut1/2 shape: [batch, seq_len, qheads, hdim]\n26\nlse1/2 shape: [batch, seq_len, qheads]\n27\n\"\"\"\n28\nmax_lse = torch.maximum(lse1, lse2)\n29\n30\nadjust_factor1 = (lse1 - max_lse).exp()\n31\nadjust_factor2 = (lse2 - max_lse).exp()\n32\n33\nnew_denominator = adjust_factor1 + adjust_factor2\n34\n35\naggregated = (\n36\nout1 * adjust_factor1.unsqueeze(-1) + out2 * adjust_factor2.unsqueeze(-1)\n37\n) / new_denominator.unsqueeze(-1)\n38\n39\nreturn aggregated\n40\n41\n42 def hydragen_attention(\n43\nq: Tensor,\n44\nprefix_k: Tensor,\n45\nprefix_v: Tensor,\n46\nsuffix_k: Tensor,\n47\nsuffix_v: Tensor,\n48 ):\n49\n\"\"\"\n50\nq: shape [batch, num_queries (1 during decoding), qheads, dim]\n51\n52\nprefix_k: shape [prefix_len, kvheads, dim]\n53\nprefix_v: shape [prefix_len, kvheads, dim]\n54\n55\nsuffix_k: shape [batch, suffix_len, kvheads, dim]\n56\nsuffix_v: shape [batch, suffix_len, kvheads, dim]\n57\n\"\"\"\n58\n59\nb, nq, hq, d = q.shape\n60\n61\n# inter-sequence batching: merge attention queries\n62\n# as if they all came from the same sequence.\n63\nbatched_q = q.view(1, b * nq, hq, d)\n64\n65\n66\n# efficient attention over prefixes\n67\n# prefix_out: shape [1, batch * nq, hq, dim]\n68\n# prefix_lse: shape [1, batch * nq, hq]\n69\nprefix_out, prefix_lse = attention(\n70\nbatched_q,\n71\nprefix_k.unsqueeze(0),\n72\nprefix_v.unsqueeze(0),\n73\n)\n74\n75\n76\n# normal attention over suffixes\n77\n# suffix_out: shape [batch, suffix_len, hq, dim]\n78\n# suffix_lse: shape [batch, suffix_len, hq]\n79\nsuffix_out, suffix_lse = attention(\n80\nbatched_q,\n81\nsuffix_k,\n12\nHydragen: High-Throughput LLM Inference with Shared Prefixes\n82\nsuffix_v,\n83\n)\n84\n85\n# unmerge prefix attention results and combine\n86\n# softmax denominators\n87\naggregated = combine_lse(\n88\nprefix_out.view(b, nq, hq, d),\n89\nprefix_lse.view(b, nq, hq),\n90\nsuffix_out,\n91\nsuffix_lse,\n92\n)\n93\n94\nreturn aggregated\nC. Additional Results\nC.1. End-to-End Throughput\nWe expand on the end-to-end throughput experiments discussed in Section 4.1. We report additional results with more\nmodel sizes when generating 128 and 256 tokens. These results are displayed in Table 1 and Table 2 for CodeLlama-7b,\nTable 3 and Table 4 for CodeLlama-13b, and Table 5 and Table 6 for CodeLlama-34b, respectively (Rozi\u00e8re et al., 2023).\nNote that in the tables where 128 tokens are generated per sequence, the \u201c16K\u201d column corresponds to a prefix length of\n16256 tokens, while for the tables with 256 generated tokens per sequence, this corresponds to 16128 tokens (this is done to\naccommodate the 16384 max sequence length of the CodeLlama models).\nFlashAttention\nHydragen\nvLLM (No Tokenization)\nvLLM\nUpper Bound (No Attention)\nBatch\nPrefix Length\nPrefix Length\nPrefix Length\nPrefix Length\nPrefix Length\nSize\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\nAll\n32\n2.5\n\u00b1\n0.0\n2.2\n\u00b1\n0.0\n1.8\n\u00b1\n0.0\n1.3\n\u00b1\n0.0\n0.9\n\u00b1\n0.0\n2.8\n\u00b1\n0.0\n2.7\n\u00b1\n0.0\n2.8\n\u00b1\n0.3\n2.6\n\u00b1\n0.0\n2.5\n\u00b1\n0.0\n1.7\n\u00b1\n0.1\n1.8\n\u00b1\n0.1\n1.7\n\u00b1\n0.1\n0.6\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n1.6\n\u00b1\n0.1\n1.6\n\u00b1\n0.0\n1.5\n\u00b1\n0.0\n0.6\n\u00b1\n0.0\n0.3\n\u00b1\n0.0\n3.1 \u00b1 0.0\n64\n4.2\n\u00b1\n0.0\n3.4\n\u00b1\n0.0\n2.6\n\u00b1\n0.0\n1.7\n\u00b1\n0.0\nX\n5.2\n\u00b1\n0.0\n4.9\n\u00b1\n0.5\n5.0\n\u00b1\n0.0\n4.9\n\u00b1\n0.0\n4.7\n\u00b1\n0.0\n3.5\n\u00b1\n0.1\n3.5\n\u00b1\n0.1\n2.9\n\u00b1\n0.1\n0.7\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n2.9\n\u00b1\n0.1\n2.8\n\u00b1\n0.1\n2.1\n\u00b1\n0.3\n0.7\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n5.7 \u00b1 0.0\n128\n5.7\n\u00b1\n0.0\n4.2\n\u00b1\n0.0\n2.7\n\u00b1\n0.0\nX\nX\n8.4\n\u00b1\n1.1\n8.6\n\u00b1\n0.3\n8.4\n\u00b1\n0.6\n8.4\n\u00b1\n0.0\n8.2\n\u00b1\n0.0\n6.1\n5.5\n3.2\n0.8\n0.4\n4.9\n4.5\n2.7\n0.7\n0.4\n10.3 \u00b1 0.0\n256\n8.1\n\u00b1\n0.0\n5.7\n\u00b1\n0.0\nX\nX\nX\n13.5\n\u00b1\n0.0\n10.9\n\u00b1\n0.0\n10.9\n\u00b1\n0.2\n11.0\n\u00b1\n0.1\n12.5\n\u00b1\n0.0\n8.9\n5.6\n3.1\n0.8\n0.4\n6.9\n4.2\n2.5\n0.8\n0.4\n15.8 \u00b1 0.0\n512\nX\nX\nX\nX\nX\n19.8\n\u00b1\n0.0\n19.6\n\u00b1\n0.0\n19.4\n\u00b1\n0.0\n18.8\n\u00b1\n0.0\n17.7\n\u00b1\n0.0\n4.7\n2.8\n1.5\n0.8\n0.4\n4.2\n2.5\n1.4\n0.8\n0.4\n23.2 \u00b1 0.0\n1024\nX\nX\nX\nX\nX\n25.5\n\u00b1\n0.0\n25.3\n\u00b1\n0.0\n24.9\n\u00b1\n0.0\n24.0\n\u00b1\n0.0\n22.3\n\u00b1\n0.0\n4.9\n2.8\n1.5\n0.8\n0.4\n4.2\n2.5\n1.4\n0.7\n0.4\n30.1 \u00b1 0.0\n2048\nX\nX\nX\nX\nX\n27.8\n\u00b1\n0.0\n27.4\n\u00b1\n0.0\n25.4\n\u00b1\n0.8\n25.2\n\u00b1\n0.0\n22.7\n\u00b1\n0.0\n4.9\n2.8\n1.5\n0.8\n0.4\n4.2\n2.5\n1.4\n0.7\n0.4\n32.7 \u00b1 0.0\nTable 1. End-to-end decoding throughput (thousands of tokens per second) with CodeLlama-7B on 8xA100-40GB GPUs when generating\n128 tokens. An \u201cx\u201d indicates that the model does not have the required memory to run.\n13\nHydragen: High-Throughput LLM Inference with Shared Prefixes\nFlashAttention\nHydragen\nvLLM (No Tokenization)\nvLLM\nUpper Bound (No Attention)\nBatch\nPrefix Length\nPrefix Length\nPrefix Length\nPrefix Length\nPrefix Length\nSize\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\nAll\n32\n2.4\n\u00b1\n0.0\n2.2\n\u00b1\n0.0\n1.8\n\u00b1\n0.0\n1.3\n\u00b1\n0.0\n0.9\n\u00b1\n0.0\n2.7\n\u00b1\n0.0\n2.6\n\u00b1\n0.1\n2.6\n\u00b1\n0.1\n2.6\n\u00b1\n0.0\n2.5\n\u00b1\n0.0\n1.7\n\u00b1\n0.0\n1.8\n\u00b1\n0.0\n1.7\n\u00b1\n0.0\n0.6\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n1.6\n\u00b1\n0.0\n1.5\n\u00b1\n0.0\n1.5\n\u00b1\n0.0\n0.6\n\u00b1\n0.0\n0.3\n\u00b1\n0.0\n3.1 \u00b1 0.0\n64\n3.9\n\u00b1\n0.0\n3.4\n\u00b1\n0.0\n2.5\n\u00b1\n0.0\n1.7\n\u00b1\n0.0\nX\n5.0\n\u00b1\n0.0\n4.9\n\u00b1\n0.0\n4.9\n\u00b1\n0.2\n4.8\n\u00b1\n0.0\n4.6\n\u00b1\n0.0\n3.4\n\u00b1\n0.1\n3.3\n\u00b1\n0.0\n2.7\n\u00b1\n0.0\n0.7\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n2.8\n\u00b1\n0.1\n2.8\n\u00b1\n0.0\n2.3\n\u00b1\n0.0\n0.6\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n5.7 \u00b1 0.0\n128\n5.3\n\u00b1\n0.0\n4.1\n\u00b1\n0.0\n2.7\n\u00b1\n0.0\nX\nX\n8.4\n\u00b1\n0.0\n8.3\n\u00b1\n0.0\n8.2\n\u00b1\n0.1\n8.1\n\u00b1\n0.0\n7.9\n\u00b1\n0.0\n6.3\n5.0\n2.9\n0.8\n0.4\n4.8\n4.0\n2.5\n0.7\n0.4\n10.3 \u00b1 0.0\n256\n7.4\n\u00b1\n0.0\nX\nX\nX\nX\n12.9\n\u00b1\n0.0\n11.1\n\u00b1\n0.4\n11.5\n\u00b1\n0.6\n12.4\n\u00b1\n0.0\n12.0\n\u00b1\n0.0\n8.8\n5.5\n3.1\n0.8\n0.4\n6.5\n4.2\n2.5\n0.7\n0.4\n15.7 \u00b1 0.0\n512\nX\nX\nX\nX\nX\n18.6\n\u00b1\n0.0\n18.4\n\u00b1\n0.0\n18.2\n\u00b1\n0.0\n17.1\n\u00b1\n0.9\n16.8\n\u00b1\n0.0\n4.6\n2.8\n1.6\n0.8\n0.4\n3.8\n2.4\n1.4\n0.7\n0.4\n23.2 \u00b1 0.0\n1024\nX\nX\nX\nX\nX\n23.6\n\u00b1\n0.0\n23.0\n\u00b1\n0.3\n21.9\n\u00b1\n1.0\n22.2\n\u00b1\n0.0\n20.9\n\u00b1\n0.0\n4.8\n2.8\n1.6\n0.8\n0.4\n3.9\n2.4\n1.4\n0.7\n0.4\n30.0 \u00b1 0.0\nTable 2. End-to-end decoding throughput (thousands of tokens per second) with CodeLlama-7B on 8xA100-40GB GPUs when generating\n256 tokens. An \u201cx\u201d indicates that the model does not have the required memory to run.\nFlashAttention\nHydragen\nvLLM (No Tokenization)\nvLLM\nUpper Bound (No Attention)\nBatch\nPrefix Length\nPrefix Length\nPrefix Length\nPrefix Length\nPrefix Length\nSize\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\nAll\n32\n1.7\n\u00b1\n0.0\n1.4\n\u00b1\n0.0\n1.1\n\u00b1\n0.0\n0.7\n\u00b1\n0.0\nX\n2.1\n\u00b1\n0.0\n2.0\n\u00b1\n0.0\n2.0\n\u00b1\n0.0\n1.9\n\u00b1\n0.0\n1.9\n\u00b1\n0.0\n1.8\n\u00b1\n0.0\n1.8\n\u00b1\n0.0\n1.8\n\u00b1\n0.0\n0.6\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n1.6\n\u00b1\n0.0\n1.6\n\u00b1\n0.0\n1.5\n\u00b1\n0.0\n0.5\n\u00b1\n0.0\n0.3\n\u00b1\n0.0\n2.3 \u00b1 0.0\n64\n2.9\n\u00b1\n0.0\n2.3\n\u00b1\n0.0\n1.6\n\u00b1\n0.0\nX\nX\n3.7\n\u00b1\n0.0\n3.6\n\u00b1\n0.2\n3.6\n\u00b1\n0.0\n3.4\n\u00b1\n0.0\n3.4\n\u00b1\n0.0\n3.5\n\u00b1\n0.1\n3.5\n\u00b1\n0.0\n2.9\n\u00b1\n0.1\n0.7\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n3.0\n\u00b1\n0.1\n2.9\n\u00b1\n0.1\n2.4\n\u00b1\n0.0\n0.6\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n4.2 \u00b1 0.0\n128\n4.0\n\u00b1\n0.0\n2.9\n\u00b1\n0.0\nX\nX\nX\n5.9\n\u00b1\n0.0\n5.7\n\u00b1\n0.4\n5.5\n\u00b1\n0.3\n5.7\n\u00b1\n0.0\n5.5\n\u00b1\n0.0\n5.5\n4.7\n\u00b1\n0.1\n3.0\n0.8\n0.4\n4.8\n3.8\n\u00b1\n0.1\n2.6\n0.7\n0.4\n6.8 \u00b1 0.0\n256\n5.7\n\u00b1\n0.0\nX\nX\nX\nX\n9.7\n\u00b1\n0.0\n9.9\n\u00b1\n0.7\n9.5\n\u00b1\n0.0\n9.4\n\u00b1\n0.0\n8.9\n\u00b1\n0.0\n8.0\n5.5\n\u00b1\n0.1\n3.2\n0.8\n0.4\n6.1\n4.3\n\u00b1\n0.1\n2.7\n0.7\n0.4\n11.4 \u00b1 0.0\n512\nX\nX\nX\nX\nX\n13.5\n\u00b1\n0.0\n13.4\n\u00b1\n0.0\n12.3\n\u00b1\n1.3\n13.0\n\u00b1\n0.0\n12.3\n\u00b1\n0.0\n4.7\n2.7\n\u00b1\n0.0\n1.6\n0.8\n0.4\n4.1\n2.4\n\u00b1\n0.0\n1.4\n0.8\n0.4\n16.1 \u00b1 0.0\n1024\nX\nX\nX\nX\nX\n15.6\n\u00b1\n0.0\n15.5\n\u00b1\n0.0\n15.3\n\u00b1\n0.0\n14.8\n\u00b1\n0.0\n13.8\n\u00b1\n0.0\n4.9\n\u00b1\n0.0\n2.8\n\u00b1\n0.0\n1.6\n\u00b1\n0.0\n0.8\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n4.2\n\u00b1\n0.0\n2.5\n\u00b1\n0.0\n1.4\n\u00b1\n0.0\n0.7\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n18.4 \u00b1 0.0\nTable 3. End-to-end decoding throughput (thousands of tokens per second) with CodeLlama-13B on 8xA100-40GB GPUs when generating\n128 tokens. An \u201cx\u201d indicates that the model does not have the required memory to run.\nFlashAttention\nHydragen\nvLLM (No Tokenization)\nvLLM\nUpper Bound (No Attention)\nBatch\nPrefix Length\nPrefix Length\nPrefix Length\nPrefix Length\nPrefix Length\nSize\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\nAll\n32\n1.7\n\u00b1\n0.0\n1.4\n\u00b1\n0.0\n1.1\n\u00b1\n0.0\n0.7\n\u00b1\n0.0\nX\n2.0\n\u00b1\n0.0\n1.9\n\u00b1\n0.1\n1.9\n\u00b1\n0.0\n1.8\n\u00b1\n0.0\n1.8\n\u00b1\n0.0\n1.8\n\u00b1\n0.0\n1.7\n\u00b1\n0.0\n1.8\n\u00b1\n0.0\n0.5\n\u00b1\n0.0\n0.3\n\u00b1\n0.0\n1.6\n\u00b1\n0.0\n1.6\n\u00b1\n0.0\n1.5\n\u00b1\n0.0\n0.5\n\u00b1\n0.0\n0.3\n\u00b1\n0.0\n2.3 \u00b1 0.0\n64\n2.8\n\u00b1\n0.0\n2.2\n\u00b1\n0.0\n1.6\n\u00b1\n0.0\nX\nX\n3.6\n\u00b1\n0.0\n3.5\n\u00b1\n0.0\n3.5\n\u00b1\n0.0\n3.3\n\u00b1\n0.0\n3.3\n\u00b1\n0.0\n3.4\n\u00b1\n0.1\n3.4\n\u00b1\n0.0\n2.9\n\u00b1\n0.0\n0.7\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n3.0\n\u00b1\n0.2\n2.7\n\u00b1\n0.2\n2.2\n\u00b1\n0.1\n0.6\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n4.2 \u00b1 0.0\n128\n3.8\n\u00b1\n0.0\n2.8\n\u00b1\n0.0\nX\nX\nX\n5.6\n\u00b1\n0.0\n5.6\n\u00b1\n0.1\n5.4\n\u00b1\n0.0\n5.5\n\u00b1\n0.0\n5.3\n\u00b1\n0.0\n5.4\n4.6\n3.0\n0.8\n0.4\n4.6\n3.7\n2.4\n0.7\n0.4\n6.8 \u00b1 0.0\n256\n5.4\n\u00b1\n0.0\nX\nX\nX\nX\n9.1\n\u00b1\n0.0\n8.6\n\u00b1\n0.3\n8.9\n\u00b1\n0.0\n8.8\n\u00b1\n0.0\n8.4\n\u00b1\n0.0\n7.6\n5.5\n3.1\n0.8\n0.4\n5.9\n4.3\n2.5\n0.7\n0.4\n11.3 \u00b1 0.0\n512\nX\nX\nX\nX\nX\n12.5\n\u00b1\n0.0\n12.4\n\u00b1\n0.0\n12.4\n\u00b1\n0.0\n12.1\n\u00b1\n0.0\n11.5\n\u00b1\n0.0\n4.4\n2.7\n1.5\n0.8\n0.4\n3.8\n2.4\n1.4\n0.7\n0.4\n16.1 \u00b1 0.0\nTable 4. End-to-end decoding throughput (thousands of tokens per second) with CodeLlama-13B on 8xA100-40GB GPUs when generating\n256 tokens. An \u201cx\u201d indicates that the model does not have the required memory to run.\n14\nHydragen: High-Throughput LLM Inference with Shared Prefixes\nFlashAttention\nHydragen\nvLLM (No Tokenization)\nvLLM\nUpper Bound (No Attention)\nBatch\nPrefix Length\nPrefix Length\nPrefix Length\nPrefix Length\nPrefix Length\nSize\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\nAll\n32\n1.4\n\u00b1\n0.0\n1.3\n\u00b1\n0.0\n1.2\n\u00b1\n0.0\n1.0\n\u00b1\n0.0\n0.8\n\u00b1\n0.0\n1.5\n\u00b1\n0.1\n1.4\n\u00b1\n0.0\n1.4\n\u00b1\n0.0\n1.4\n\u00b1\n0.0\n1.4\n\u00b1\n0.0\n1.5\n\u00b1\n0.0\n1.4\n\u00b1\n0.0\n1.2\n\u00b1\n0.0\n0.5\n\u00b1\n0.0\n0.3\n\u00b1\n0.0\n1.5\n\u00b1\n0.0\n1.3\n\u00b1\n0.0\n1.1\n\u00b1\n0.0\n0.5\n\u00b1\n0.0\n0.3\n\u00b1\n0.0\n1.6 \u00b1 0.0\n64\n2.5\n\u00b1\n0.0\n2.4\n\u00b1\n0.0\n2.1\n\u00b1\n0.0\n1.8\n\u00b1\n0.0\n1.3\n\u00b1\n0.0\n2.6\n\u00b1\n0.1\n2.6\n\u00b1\n0.0\n2.6\n\u00b1\n0.0\n2.5\n\u00b1\n0.0\n2.5\n\u00b1\n0.0\n2.6\n\u00b1\n0.0\n2.3\n\u00b1\n0.0\n1.9\n\u00b1\n0.0\n0.7\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n2.4\n\u00b1\n0.0\n2.1\n\u00b1\n0.1\n1.6\n\u00b1\n0.1\n0.6\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n2.9 \u00b1 0.0\n128\n3.8\n\u00b1\n0.0\n3.4\n\u00b1\n0.0\n2.8\n\u00b1\n0.0\n2.1\n\u00b1\n0.0\nX\n4.2\n\u00b1\n0.0\n4.2\n\u00b1\n0.0\n4.1\n\u00b1\n0.0\n4.1\n\u00b1\n0.0\n3.9\n\u00b1\n0.0\n3.8\n3.0\n2.3\n0.8\n0.4\n3.4\n2.7\n2.0\n0.7\n0.4\n4.6 \u00b1 0.0\n256\n6.0\n\u00b1\n0.0\n5.3\n\u00b1\n0.0\n4.3\n\u00b1\n0.0\nX\nX\n6.3\n\u00b1\n0.4\n6.7\n\u00b1\n0.0\n6.6\n\u00b1\n0.0\n6.3\n\u00b1\n0.0\n6.0\n\u00b1\n0.0\n5.1\n3.9\n2.8\n0.8\n0.4\n4.4\n3.3\n2.4\n0.8\n0.4\n7.2 \u00b1 0.0\n512\n7.0\n\u00b1\n0.0\n6.0\n\u00b1\n0.0\nX\nX\nX\n7.8\n\u00b1\n0.5\n8.1\n\u00b1\n0.0\n8.0\n\u00b1\n0.0\n7.7\n\u00b1\n0.0\n7.1\n\u00b1\n0.0\n4.2\n2.7\n1.5\n0.8\n0.4\n3.6\n2.4\n1.4\n0.8\n0.4\n8.8 \u00b1 0.0\n1024\nX\nX\nX\nX\nX\n9.0\n\u00b1\n0.2\n9.0\n\u00b1\n0.0\n8.8\n\u00b1\n0.0\n8.3\n\u00b1\n0.0\n7.5\n\u00b1\n0.0\n4.3\n2.8\n1.6\n0.8\n0.4\n3.7\n2.5\n1.4\n0.8\n0.4\n9.7 \u00b1 0.0\n2048\nX\nX\nX\nX\nX\n10.1\n\u00b1\n0.0\n10.0\n\u00b1\n0.0\n9.7\n\u00b1\n0.0\n9.2\n\u00b1\n0.0\n8.3\n\u00b1\n0.0\n4.3\n2.7\n1.5\n0.8\n0.4\n3.7\n2.4\n1.4\n0.8\n0.4\n10.7 \u00b1 0.0\n4096\nX\nX\nX\nX\nX\n10.7\n\u00b1\n0.0\n10.6\n\u00b1\n0.0\n10.4\n\u00b1\n0.0\n9.5\n\u00b1\n0.1\n8.9\n\u00b1\n0.0\n4.0\n2.6\n1.4\n0.8\n0.4\n3.5\n2.3\n1.3\n0.7\n0.4\n11.3 \u00b1 0.0\nTable 5. End-to-end decoding throughput (thousands of tokens per second) with CodeLlama-34B on 8xA100-40GB GPUs when generating\n128 tokens. An \u201cx\u201d indicates that the model does not have the required memory to run.\nFlashAttention\nHydragen\nvLLM (No Tokenization)\nvLLM\nUpper Bound (No Attention)\nBatch\nPrefix Length\nPrefix Length\nPrefix Length\nPrefix Length\nPrefix Length\nSize\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\n1K\n2K\n4K\n8K\n16K\nAll\n32\n1.4\n\u00b1\n0.0\n1.3\n\u00b1\n0.0\n1.2\n\u00b1\n0.0\n1.1\n\u00b1\n0.0\n0.8\n\u00b1\n0.0\n1.4\n\u00b1\n0.0\n1.4\n\u00b1\n0.0\n1.4\n\u00b1\n0.0\n1.4\n\u00b1\n0.0\n1.4\n\u00b1\n0.0\n1.5\n\u00b1\n0.0\n1.4\n\u00b1\n0.0\n1.2\n\u00b1\n0.0\n0.5\n\u00b1\n0.0\n0.3\n\u00b1\n0.0\n1.5\n\u00b1\n0.0\n1.3\n\u00b1\n0.1\n1.1\n\u00b1\n0.0\n0.5\n\u00b1\n0.0\n0.3\n\u00b1\n0.0\n1.6 \u00b1 0.0\n64\n2.5\n\u00b1\n0.0\n2.4\n\u00b1\n0.0\n2.1\n\u00b1\n0.0\n1.8\n\u00b1\n0.0\n1.3\n\u00b1\n0.0\n2.6\n\u00b1\n0.1\n2.6\n\u00b1\n0.0\n2.6\n\u00b1\n0.0\n2.5\n\u00b1\n0.0\n2.5\n\u00b1\n0.0\n2.6\n\u00b1\n0.0\n2.3\n\u00b1\n0.0\n1.8\n\u00b1\n0.0\n0.7\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n2.3\n\u00b1\n0.1\n2.0\n\u00b1\n0.0\n1.6\n\u00b1\n0.0\n0.6\n\u00b1\n0.0\n0.4\n\u00b1\n0.0\n2.8 \u00b1 0.0\n128\n3.8\n\u00b1\n0.0\n3.4\n\u00b1\n0.0\n2.8\n\u00b1\n0.0\n2.1\n\u00b1\n0.0\nX\n4.1\n\u00b1\n0.1\n4.1\n\u00b1\n0.0\n4.1\n\u00b1\n0.0\n4.0\n\u00b1\n0.0\n3.9\n\u00b1\n0.0\n3.7\n3.0\n2.2\n0.7\n0.4\n3.2\n2.6\n2.0\n0.7\n0.4\n4.5 \u00b1 0.0\n256\n5.8\n\u00b1\n0.0\n5.3\n\u00b1\n0.0\n4.3\n\u00b1\n0.0\nX\nX\n6.6\n\u00b1\n0.0\n6.5\n\u00b1\n0.0\n6.4\n\u00b1\n0.0\n6.2\n\u00b1\n0.0\n5.9\n\u00b1\n0.0\n5.0\n3.9\n2.7\n0.8\n0.4\n4.2\n3.3\n2.3\n0.7\n0.4\n7.2 \u00b1 0.0\n512\n6.7\n\u00b1\n0.0\n5.9\n\u00b1\n0.0\nX\nX\nX\n7.9\n\u00b1\n0.0\n7.9\n\u00b1\n0.1\n7.9\n\u00b1\n0.0\n7.6\n\u00b1\n0.0\n7.1\n\u00b1\n0.0\n3.9\n2.6\n1.5\n0.8\n0.4\n3.5\n2.3\n1.4\n0.7\n0.4\n8.8 \u00b1 0.0\n1024\nX\nX\nX\nX\nX\n9.0\n\u00b1\n0.0\n8.9\n\u00b1\n0.0\n8.6\n\u00b1\n0.0\n8.1\n\u00b1\n0.0\n7.4\n\u00b1\n0.0\n3.9\n2.6\n1.4\n0.8\n0.4\n3.6\n2.4\n1.4\n0.7\n0.4\n9.7 \u00b1 0.0\n2048\nX\nX\nX\nX\nX\n10.0\n\u00b1\n0.0\n9.8\n\u00b1\n0.0\n9.6\n\u00b1\n0.0\n9.1\n\u00b1\n0.0\n8.2\n\u00b1\n0.0\n4.0\n2.6\n1.5\n0.8\n0.4\n3.6\n2.4\n1.4\n0.7\n0.4\n10.6 \u00b1 0.0\nTable 6. End-to-end decoding throughput (thousands of tokens per second) with CodeLlama-34B on 8xA100-40GB GPUs when generating\n256 tokens. An \u201cx\u201d indicates that the model does not have the required memory to run.\nC.2. Microbenchmarks\nWe repeat the A100 microbenchmark experiment from Section 4.2 on an H100 and L40S GPU, reporting our results in\nFigure 8. The L40S has the highest ratio of compute capability to memory bandwidth of the three GPUs and therefore\nderives the most benefit from Hydragen\u2019s elimination of memory bottlenecks. While the compute-to-bandwidth ratio is\nhigher on an H100 than on an A100, we measure similar Hydragen speedups between both cards. This stems from the fact\nthat the flash-attn package that we use is not currently optimized for Hopper GPUs, and therefore achieves a lower\ndevice utilization on an H100 vs an A100.\nD. Experiment Details\nD.1. End-to-End Benchmarks\nOur end-to-end benchmarks only measure decoding throughput and exclude the time required to compute the prefill. We\nmeasure \u201cdecode-only\u201d time by initially benchmarking the time required to generate one token from a given prompt and\nsubtracting that value from the time it takes to generate the desired number of tokens. This subtraction is particularly\n15\nHydragen: High-Throughput LLM Inference with Shared Prefixes\nFigure 8. Speedup of Hydragen attention over FlashAttention for various batch sizes, shared prefix lengths and suffix lengths on an H100\n(left) and an L40S (right) GPU.\nimportant in order to fairly evaluate vLLM baselines, since it appears that vLLM redundantly detokenizes the prompt for\nevery sequence in the batch at the beginning of inference (this can take minutes for large batch sizes and sequence lengths).\nFor our \u201cvLLM no detokenization\u201d baseline, we disable incremental detokenization in vLLM by commenting out this line.\nFor all FlashAttention and No Attention datapoints, we run 10 warmup iterations and use the following 10 iterations to\ncompute throughput. For Hydragen datapoints, we run 10 warmup and 10 timing iterations when the batch size is less than\n256, and for larger batch sizes use three warmup and three timing iterations. We observe that shorter-running Hydragen\nbenchmarks (those with smaller batch sizes, sequence lengths, model sizes, or completion lengths) can occasionally produce\nlonger outlier times. This seems to be related not to decoding time itself, but to variations in prefilling time before decoding.\nFor vLLM baselines (both with and without incremental detokenization), we use three warmup and timing iterations for\nall batch sizes below 128, as well as for all datapoints that are used in Figures 1 and 4. The longest-running vLLM runs\ncan take many minutes to complete a single iteration, so for baselines above a batch size of 128 that only appear in the\nsupplementary tables of Appendix C.1, we use one warmup and one timing iteration.\nD.2. Microbenchmarks\nIn each microbenchmark, we run 1000 iterations of warmup before reporting the mean running time across 1000 trials.\nBetween trials, we flush the GPU L2 cache by writing to a 128MiB tensor. We use CUDA graphs when benchmarking in\norder to reduce CPU overhead, which can be important since some benchmarks can complete a single iteration in tens of\nmicroseconds.\nD.3. Long document retrieval\nTo demonstrate the throughput benefits of using Hydragen to answer questions about a long document, we construct a\ndocument (with 19974 tokens) that contains arbitrary facts from which question/answer pairs can be easily generated.\nPrefix and Suffix Content: The content of the document is a subset of War and Peace, modified to include procedurally\ngenerated facts of the form \u201cThe dog named {name} has fur that is {color}\u201d. The questions are of the form \u201cWhat color is\nthe fur of the dog named name?\u201d, where the answer is color. We construct 261 questions (256 testable questions plus five\nfor the few-shot examples) and interleave these throughout sentences of the document. When benchmarking with a greater\nnumber of questions than 256, we duplicate questions when querying the model - this is instead of adding more questions to\nthe document in order to constrain total document length.\nModel and Accelerator Choice: We choose the Yi-6B-200k model because it is small enough to fit a large KV cache\nin memory (important when running baselines that redundantly store the document) while also supporting a long enough\ncontext to process our document. We distribute the model across four A100-40GB GPUs in order to maximize possible KV\ncache size (the model only has four key/value attention heads, preventing us from easily using tensor parallelism across\nmore GPUs).\n16\nHydragen: High-Throughput LLM Inference with Shared Prefixes\nOur reported measurements use the mean of five timing runs after ten warmup iterations.\nD.4. Hierarchical Sharing in Competitive Programming\nThe dataset of 120 problems that we use for this benchmark comes from the introductory difficulty split of APPS. We filter\nout problems that include starter code. We use two few-shot examples (2400 tokens long) that come from the training split\nof APPS, while all of the eval examples come from the test split. We sample 512 tokens for every completion. We run\nthis experiment using CodeLlama-7b on eight A100-40GB GPUs. We measure the total time to run inference on all 120\nquestions, excluding tokenization and detokenization time.\n17\n"
  },
  {
    "title": "CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay",
    "link": "https://arxiv.org/pdf/2402.04858.pdf",
    "upvote": "13",
    "text": "CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nNatasha Butt 1 2 Blazej Manczak 2 Auke Wiggers 2\nCorrado Rainone 2 David Zhang 2 Micha\u00a8el Defferrard 2 Taco Cohen 2 3\nAbstract\nLarge language models are increasingly solv-\ning tasks that are commonly believed to require\nhuman-level reasoning ability. However, these\nmodels still perform very poorly on benchmarks\nof general intelligence such as the Abstraction\nand Reasoning Corpus (ARC). In this paper, we\napproach ARC as a programming-by-examples\nproblem, and introduce a novel and scalable\nmethod for language model self-improvement\ncalled Code Iteration (CodeIt). Our method iter-\nates between 1) program sampling and hindsight\nrelabeling, and 2) learning from prioritized experi-\nence replay. By relabeling the goal of an episode\n(i.e., the target program output given input) to\nthe realized output produced by the sampled pro-\ngram, our method effectively deals with the ex-\ntreme sparsity of rewards in program synthesis.\nApplying CodeIt to the ARC dataset, we demon-\nstrate that prioritized hindsight replay, along with\npre-training and data-augmentation, leads to suc-\ncessful inter-task generalization. CodeIt is the\nfirst neuro-symbolic approach that scales to the\nfull ARC evaluation dataset. Our method solves\n15% of ARC evaluation tasks, achieving state-of-\nthe-art performance and outperforming existing\nneural and symbolic baselines.\n1. Introduction\nThe Abstraction and Reasoning Corpus (ARC) is a gen-\neral artificial intelligence benchmark targeted at both hu-\nmans and AI systems (Chollet, 2019). ARC is a challeng-\ning benchmark because it contains few-shot example tasks\nthat assume access to the four innate core knowledge sys-\ntems: objects, actions, number, and space (Spelke & Kinzler,\n2007). It was designed to require no knowledge outside of\n1University of Amsterdam 2Qualcomm AI Research. Qual-\ncomm AI Research is an initiative of Qualcomm Technologies,\nInc. 3Work was completed while an employee at Qualcomm Tech-\nnologies Netherlands B.V.. Correspondence to: Natasha Butt\n<n.e.butt@uva.nl>.\nFigure 1. An overview of Code Iteration. In the sampling stage,\nprograms \u03c1 are sampled from the policy Q\u03b8 conditioned on input-\noutput pairs. The program may not produce target output O\u2217 given\nI, so we use hindsight relabeling: we execute the program, and add\nthe program \u03c1, inputs I, and realized outputs O to the buffer. In\nthe learning stage, we train the policy on samples from the buffer.\nthese priors, and so the massive memorization capability of\npre-trained language models is of limited use for this prob-\nlem. Humans are able to solve 80% of (a random subset of)\nARC tasks in user studies (Johnson et al., 2021), whereas\nstate-of-the-art neural approaches based on GPT-4 solve\nonly 12% of evaluation tasks (Gendron et al., 2023).\nEach ARC task consists of a number of demonstration exam-\nples, each consisting of an input and output grid, and one or\nmore test inputs for which the corresponding output must be\npredicted (see Figure 2). Effective agents use abstractions\nrelated to the four core knowledge systems, generalize from\ndemonstration to test examples, and generalize between\ntasks. For example, an agent may infer that adjacent cells\n(space) of the same color value (number) form an object.\nAn agent may also infer that multiple objects sometimes\nattract or repel (action). Using these abstractions to reason\nabout the value of the test output, an agent may generalize\nfrom the demonstration examples to the test example.\nExisting approaches to ARC can be classified as either neu-\nral (Gendron et al., 2023; Mirchandani et al., 2023), mean-\ning they directly predict output grids using a neural net-\nwork, or (neuro-) symbolic (Ainooson et al., 2023; Ferr\u00b4e,\n2021; 2023), meaning they first predict a program or other\n1\narXiv:2402.04858v1  [cs.AI]  7 Feb 2024\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nsymbolic representation of the mapping between input and\noutput grids, before using it to generate the output grids.\nThrough the use of a well-designed domain-specific lan-\nguage (DSL), the symbolic methods can be endowed with\nprior knowledge analogous to the core knowledge systems\nfound in humans. By combining neural networks and sym-\nbolic representations like programs, the system can leverage\nboth prior knowledge and data to solve the ARC tasks.\nHowever, the most effective existing methods, whether neu-\nral or symbolic, fail to use experience to generalize between\ntasks. We propose using Expert Iteration (ExIt) (Anthony\net al., 2017) to incorporate experience. ExIt methods do\nthis by alternating between two phases: gathering data with\nan (often expensive) exploration policy, and improving the\npolicy by training on the newfound experiences. Instead of\nperforming ExIt in the grid space, we take a neuro-symbolic\napproach and train our model to learn to write programs.\nThis brings us closer to the system that emulates general\nfluid intelligence described by Chollet (2019): by incorpo-\nrating new experiences in the form of abstractions.\nRecent ExIt approaches employ self-improving language\nmodels (Gulcehre et al., 2023; Aksitov et al., 2023; Wang\net al., 2023c) to replace the expensive expert by sampling\nfrom a language model policy and reward-based filtering,\nsaving only trajectories that obtain high reward. This al-\nlows them to scale well and benefit from knowledge already\ncaptured in the policy. These methods prove effective on\nprogram synthesis tasks with natural language specifica-\ntions (Singh et al., 2023) and code specifications (Haluptzok\net al., 2022). However, when solving ARC, agents start\nExIt with poor prior knowledge about the search space, as\nthe task is out-of-distribution. Finding a correct program is\nchallenging: positive rewards are extremely sparse. As a\nresult, these methods are sample inefficient in the context of\nARC, and programming-by-examples more generally. To\nenable learning in sparse-reward settings, hindsight rela-\nbeling (Andrychowicz et al., 2017) creates artificial expert\ntrajectories post-hoc, and methods that combine ExIt and\nthis technique have improved sample efficiency (Gauthier,\n2022; Butt et al., 2022). However, since the relabelled data\ndistribution is constantly changing, there is risk of catas-\ntrophic forgetting (French, 1999).\nIn this work, we introduce a novel, scalable expert iteration\nmethod for sparse reward settings that does not suffer from\ncatastrophic forgetting. Our method, which we call Code\nIteration or CodeIt for short, iterates between 1) a sampling\nand hindsight relabeling stage and 2) a learning stage with\nprioritized experience replay. We show a visualization in\nFigure 1. This iterative procedure thus allows us to auto-\nmatically generate new data without human intervention.\nUnlike current self-improvement approaches that perform\nsampling and filtering (Singh et al., 2023), CodeIt learns\nFigure 2. A simplified ARC task. Given two demonstration input-\noutput pairs, the goal is to determine the output grid for the test\nexample, in three attempts or fewer. The size of the grids and the\nnumber of demonstration and test examples differs across tasks.\nfrom all program samples, improving sample efficiency. By\nprioritizing training on experiences that solve real tasks, we\nameliorate the risk of catastrophic forgetting.\nCodeIt solves 59 of 400 ARC evaluation tasks, achieving\nstate-of-the-art performance by learning from experiences\nin the form of abstractions and generalizing to new tasks.\nWe analyze the programs discovered by CodeIt and find\nthat these are on average shorter and use different primitives\ncompared to our custom symbolic baselines. Furthermore,\nafter finding an initial solution, CodeIt continues to improve\nit over time; shorter solutions are found in 53% of solved\nARC tasks, highlighting the ability to perform program re-\nfinement. We perform careful ablations to better understand\nthe impact on task performance of key components: ExIt,\nprioritized hindsight replay, and prior knowledge.\n2. Method\nWe approach ARC as a programming-by-examples prob-\nlem: for a given set of tasks that we call the search set,\nwe aim to find programs that correctly match inputs with\ntheir respective outputs, and we do so by training a policy\nto produce programs when shown demonstration examples.\nThis is achieved by iterating between two stages: 1) writing\nprograms using a policy and applying hindsight relabeling,\nand 2) learning from the programs and their input-output\nexamples. We first describe key design choices below, and\nthen explain the iterative procedure.\n2.1. Design choices\nProgramming language\nWe restrict our programming\nlanguage to the open source domain specific language (DSL)\nof Hodel (2023). Although various open source DSLs for\nARC exist, Hodel designed their DSL using only the ARC\ntraining split, whereas some authors incorporate priors from\nthe ARC evaluation split into their DSLs (Icecuber, 2020).\nHodel\u2019s DSL contains grid manipulation functions (e.g.,\nvmirror or hmirror, which mirror the grid along the\nvertical or horizontal axis), fill functions that replace all\npixels of a certain color, and functions that return locations\nof specific pixel groups. See Appendix B.4 for details on the\nDSL and more example primitives, and see Hodel (2023)\nfor discussion on the DSL\u2019s primitives and capability.\n2\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nFigure 3. Sparse grid representation of a simplified ARC task.\nPolicy\nOur choice of policy is a pretrained encoder-\ndecoder Large Language Model (LLM). We use the 220\nmillion parameter CodeT5+ (Wang et al., 2023b) model and\nits default tokenizer, which are pretrained on a diverse set of\nprogramming tasks. We input the demonstration examples\nto the encoder, and let the decoder generate the correspond-\ning program. If necessary, demonstration examples are\ntruncated to fit in the encoder context window.\nGrid representation\nIn order to condition the language\nmodel policy on input-output grids, we represent them as\ntext. Instead of encoding the grid as a 2-dimensional array,\nwe use an object-centric text representation. Each color is\nencoded as an integer, and for each color in the grid we\nlist all the grid cells with that color as [x, y] coordinates.\nSince the majority of cells belong to the background color,\nthis procedure significantly reduces the number of tokens\nrequired to encode the grid (see Figure 11 in Appendix A.3).\nAn example of the sparse grid representation is shown in\nFigure 3. This object-centric text representation, similar to\nthe one of Xu et al. (2023), works well for sparse grids and\nis human-interpretable.\n2.2. The Code Iteration Algorithm\nWe initialize the policy network by training on ground truth\ndata. We then start CodeIt, iterating between sampling and\nhindsight relabeling and learning. We refer to one full pass\nof sampling and learning as a meta-iteration. We show the\nprocedure in Fig. 1, and explain each stage in more detail\nbelow. For pseudocode, see Appendix A.1.\nInitialization\nWe start from a dataset of ARC training\ntasks and solution programs written in the domain-specific\nlanguage (DSL) of Hodel (2023), which we call the train-\ning set. This dataset is expanded by randomly mutating\nprograms (for details of this procedure, see Appendix A.2),\nresulting in an augmented training set.\nThe initial dataset augmentation step serves multiple pur-\nposes. Mixing in mutated programs acts as a form of data\naugmentation, and is a common approach in policy improve-\nment for program synthesis (Ellis et al., 2020; Fawzi et al.,\n2022). Before experiences are sampled from the policy, the\nmodel can already learn the DSL syntax, which can be chal-\nlenging if the training set is small. It also enables the model\nto learn how to interpret the task demonstration examples\nbefore we begin iterative learning, improving the quality of\nour policy samples in early meta-iterations.\nSampling and hindsight relabeling\nIn the sampling\nstage, we obtain new programs using the policy Q\u03b8. Let\nthe search set be the set of tasks for which we want to find\na corresponding program. For each task in the search set,\nwe convert the demonstration examples\u2019 input I and target\noutput O\u2217 from grid to text representation, encode these us-\ning the policy, and then autoregressively decode a program:\n\u03c1 \u223c Q\u03b8(\u03c1|I, O\u2217). We then run the obtained program on\nthe input grids. If the program is syntactically incorrect or\nthe runtime is too high, we discard it. Otherwise, we obtain\nprogram outputs O = \u03c1(I), and can add a new triplet to a\nreplay buffer: the program \u03c1, the demonstration inputs I,\nand the realized outputs O (which may or may not match the\ntarget outputs O\u2217). In each sampling stage we repeat this\nprocedure n\u03c1 times per task, where n\u03c1 is a hyperparameter.\nReplacing the target output by the realized one is a form\nof hindsight experience replay (Andrychowicz et al., 2017),\nand ensures that we obtain an experience every time we\nfind a syntactically correct program, thereby preventing\nstagnation of the buffer. Although these programs may\nnot solve the tasks we are interested in, they are always\nvalid in terms of syntax and semantics (correctly mapping\n\u03c1(I) \u2192 O). They can therefore be used to teach the policy\nabout program syntax and program behaviour, which may\nlead to positive transfer to the search set. We emphasize\nthat we never add test examples nor performance on the test\nexamples to our buffer, as one should not have access to\ntheir target output grid during sampling.\nLearning\nDuring the learning stage, the policy Q\u03b8 is\ntrained on experiences sampled from the buffer, the training\nset and the augmented training set. These experiences con-\nsist of input grids I, output grids O and the corresponding\nprogram \u03c1. The training objective is then a straightforward\nnegative log-likelihood objective:\nL(\u03c1, I, O) = \u2212 log Q\u03b8(\u03c1|I, O).\n(1)\nWe keep only a single copy of the policy network, updating\nit during each learning stage. In particular, we do not com-\npare with past versions to guarantee an improvement in the\npolicy before using it in the next sampling stage. Although\ncontinual updates could lead to worse performance in the\nnext iteration, we find this is not a problem in practice.\nBy default, we perform prioritized sampling from the replay\nbuffer (Schaul et al., 2015). For each experience, the priority\nis proportional to the percentage of demonstration outputs\nequal to program outputs. This means that programs that\nsolve real ARC tasks\u2019 demonstration examples are sampled\nmore often than programs for hindsight-relabeled tasks.\n3\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\n3. Experiments\nIn this section, we aim to demonstrate the efficacy of CodeIt,\nand break down how much different components of the\nmethod contribute to the performance. We first tuned hyper-\nparameters on a custom training and validation split (for a\ndescription of these parameters and details, see Appendix B).\nUsing these hyperparameters, we benchmark our method\non the ARC evaluation split and compare against previous\nstate-of-the-art methods. Finally, we ablate the importance\nof individual components of CodeIt.\nWe define demonstration performance as the percentage of\nsolved demonstration examples on a given task. We first sort\nsolution programs by demonstration performance, and then\nby program length, favoring shorter programs. We evaluate\nthe top three programs on the set of test examples. Following\nARC evaluation procedure, if at least one of these three\nprograms maps all test example inputs to outputs, the task\nis solved and test performance is 1. We emphasize that the\nExIt procedure only makes use of demonstration examples,\nand that we use test performance for final evaluation only.\nCustom baselines\nWe use a random baseline that samples\nprograms line-by-line. At the start of each line, we sample\na primitive function from the DSL, then sample arguments\ngiven its expected input types. When a variable of type\n\u201cgrid\u201d is created, we end the program with probability 0.8,\notherwise we add another line to the program.\nWe also use a mutation-based baseline. This is a more\nadvanced procedure, designed with the DSL in mind. At\nevery meta-iteration, it mutates the set of training programs\nprovided by Hodel (2023). We use two variations: \u201cd1\u201d\nmutates only the initial training set, and \u201cd\u221e\u201d can augment\nnewfound programs as well. We provide the exact algorithm\nin Appendix A.2.\nFor all three baselines, we sample nm = n\u03c1 \u00b7 ntasks pro-\ngrams per meta-iteration. Here, n\u03c1 is the desired number of\nprograms per meta-iteration per task, and ntasks the total\nnumber of tasks in the population. To strengthen these base-\nlines, we exhaustively evaluate each found program on all\ninputs in the search set, and check the outputs against ARC\noutput grids.\nBaselines from literature\nWe include approaches from\nliterature as baselines as well. A direct comparison is some-\ntimes difficult, as not all baselines apply their method to the\nfull ARC evaluation set: for example, Kolev et al. (2020)\nand Alford et al. (2021) focus only on a subset of ARC.\nAdditionally, some symbolic methods design a DSL based\non both ARC training and evaluation sets and report results\non a hidden test set (Icecuber, 2020). We therefore only\ncompare to approaches that report scores on the full ARC\nevaluation set.\nFigure 4. Cumulative performance as function of number of sam-\npled programs for CodeIt and various baselines. We run CodeIt\nfor three random seeds.\nAinooson et al. (2023) and Ferr\u00b4e (2023) both run a search\nprocedure for a custom DSL on the full set. As Ainooson\net al. (2023) report the highest performance the full ARC\nevaluation set, this is our main symbolic baseline. Although\nMirchandani et al. (2023) and Gendron et al. (2023) use a\ndifferent evaluation protocol, we include these as our main\nneural baseline, as they are based on powerful LLMs (text-\ndavinci and GPT-4).\n3.1. Setup\nWe initialize our training set with the 400 examples from\nthe ARC training split and the associated solution programs\nprovided by (Hodel, 2023). We also sample 19,200 pro-\ngrams as additional training data via the mutation procedure\noutlined in Appendix A.2. We use the programs that are\nsyntactically correct to initialize the augmented training set.\nWe use the 400 ARC evaluation examples as our search set.\nIn the sampling stage of each meta-iteration, we use temper-\nature sampling with temperature \u03c4 = 0.95, and sample up\nto n\u03c1 = 24 programs per task. This encourages exploration\nand, as a result, increases the diversity of data added to the\nreplay buffer. We reject policy-sampled programs if they\nare syntactically incorrect, or if they run for more than 0.25\nseconds per program line. All valid programs are added to\nthe replay buffer.\nIn each learning stage, we start by sampling a set of ex-\nperiences from the buffer under the distribution given by\nthe priorities. Each meta-iteration, we sample rt = 10, 000\nexperiences from the concatenation of the train set and the\naugmented train set, and rp = 90, 000 experiences from the\nbuffer. The resulting set is used for 1 epoch of training. For\na full list of hyperparameters, see Table 3 in the Appendix.\n4\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nMethod\nARC Train Set\nARC Eval Set\nARC Eval 412\nFerr\u00b4e (2021)\n29 / 400\n6 / 400\n-\nAinooson et al. (2023) MLE\n70 / 400\n17 / 400\n-\nAinooson et al. (2023) brute force\n104 / 400\n26 / 400\n-\nFerr\u00b4e (2023)\n96 / 400\n23 / 400\n-\nMirchandani et al. (2023) text-davinci-003\n56 / 400*\n27 / 400*\n-\nGendron et al. (2023) GPT-4\n-\n-\n49 / 412*\nMutation d1 baseline\n-\n42 / 400\n39 / 412*\nMutation d\u221e baseline\n-\n38 / 400\n36 / 412*\nRandom baseline\n-\n6 / 400\n7 / 412*\nCodeIt\n-\n59 / 400\n59 / 412*\nTable 1. Main results on ARC eval set. The evaluation metric is pass@3 by default, * indicates pass@1. To enable comparison to related\nwork of Gendron et al. (2023), we also include pass@1 performance on the ARC Eval set with 412 examples. Our method outperforms all\nprevious baselines. More details on the ARC splits and evaluation procedures can be found in Appendix A.4.\n3.2. Main results on ARC eval set\nIn Figure 4, we show performance as a function of the num-\nber of sampled programs, for CodeIt, our custom baselines,\nAinooson et al. (2023) and Ferr\u00b4e (2023). We show cumu-\nlative performance here, which means that any program\nin the buffer or augmented train set is considered a solu-\ntion candidate. For the mutation baselines, we see a rapid\nperformance increase followed by stagnation. In compari-\nson, CodeIt takes several meta-iterations to start generating\nsolutions outside of the augmented train set and then per-\nformance rapidly increases. CodeIt quickly outperforms\nthe mutation baseline, indicating that it indeed finds higher-\nquality samples to train on.\nWe report final performance of CodeIt after 100 meta-\niterations, and the performance of various baselines, in Ta-\nble 1. To enable comparison to Gendron et al. (2023), we\ninclude results on the \u201cARC Eval 412\u201d set, which treats\neach test example in the ARC evaluation set as a separate\ntask. Our approach outperforms symbolic approaches (Ain-\nooson et al., 2023; Ferr\u00b4e, 2021; 2023), but also neural ap-\nproaches based on large language models (Gendron et al.,\n2023; Mirchandani et al., 2023), achieving state-of-the-art\nperformance on the ARC evaluation set.\nFor context, we show a solution written by CodeIt for an\nexample task in Figure 5. To further illustrate the differences\nbetween the programs found by CodeIt and the mutation\nbaselines, we analyze solutions found by each method in\nAppendix C.1, including a qualitative comparison in Table\n4. One finding is that there are 29 tasks for which CodeIt\nand the mutation baseline both find a solution, but that there\nare 23 tasks for which only CodeIt finds a solution, versus\n13 for the mutation baseline. For the tasks that both methods\nsolve, CodeIt finds shorter programs on average and uses\ndifferent primitives. In Appendix C.2, we observe CodeIt\nrefines its initial solution for 53% of solved tasks, producing\na shorter solution in a later meta-iteration.\n3.3. Ablations\nIn Figure 6 and 7, we report cumulative performance and\npolicy performance over time for CodeIt and all ablations.\nIn all cases, we initialize the method with the ARC train set,\nand use the ARC evaluation set as search set. We show the\nresults of ablations at the end of training in Table 2.\nA1: No ExIt\nThis ablation removes policy feedback, to\nisolate the contribution of Expert Iteration. In every meta-\niteration, instead of populating the buffer with policy sam-\nples, we take the programs generated in that meta-iteration\nof the mutation d1 baseline. For each program, we randomly\nselect a task from the search set and perform hindsight re-\nlabelling, adding the program, input, output triplet to the\nbuffer. We sample rp + rt = 100, 000 experiences from\nthe concatenation of the train set, the augmented train set\nand the buffer at each meta-iteration for learning. We see\nthat A1 outperforms the mutation baseline, which means\nsupervised learning from mutation experiences alone does\nlead to some inter-task generalization. However, cumula-\ntive performance is substantially lower than CodeIt. This\nhighlights the importance of policy feedback.\nA2: No relabeling\nWe test the effect of hindsight relabel-\ning by only adding experiences to the buffer if the program\nproduces the correct output for all demonstration examples.\nWe train on all experiences in the buffer without prioritized\nsampling. Although performance increases in early meta-\niterations, A2 stagnates after around 30 meta-iterations, in-\ndicating that data generated by sampling and filtering alone\nis not sufficient. Sampling and hindsight relabeling (CodeIt)\nperforms better than sampling and filtering (A2).\n5\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nFigure 5. ARC evaluation task 48f8583b and the solution program found by CodeIt.\nFigure 6. Cumulative performance as function of number of sam-\npled programs for CodeIt and ablations, for three random seeds.\nFor cumulative performance, all programs in the augmented train\nset and buffer are candidate solutions.\nFigure 7. Policy performance per meta iteration as function of\nnumber of sampled programs for CodeIt and ablations, for three\nrandom seeds. For policy performance, only programs output by\nthe policy in the current meta-iteration are candidate solutions.\nA3: No priority\nTo test the hypothesis that prioritized\nsampling ameliorates catastrophic forgetting, we draw ex-\nperiences uniformly from the buffer in the learning stage.\nA3 leads to a small reduction in cumulative performance,\nbut a large reduction in policy performance, indicating that\nthe policy indeed forgets important experiences. Prioritized\nsampling results in better retention of knowledge.\nA4: No pretraining\nTo identify whether our pre-trained\npolicy contains beneficial prior knowledge, we randomly\nreinitialize the policy\u2019s weights at the start of CodeIt. Policy\nperformance shows that performance improvement is much\nslower. Moreover, inter-task generalization begins later, as\nshown by the cumulative performance, which only starts\nincreasing after around 50 meta-iterations. Despite the ex-\npected slowdown, it is encouraging to see that CodeIt does\nseem to be able to bootstrap from random weights.\nA5: One demo\nWe investigate CodeIt\u2019s use of the task\nrepresentation by decreasing the number of demonstration\nexamples shown to the policy. This results in a significant\ndecrease in both cumulative and policy performance. This\nindicates CodeIt forms abstractions over multiple demon-\nstration examples.\nA6: No mutation\nIn this ablation, we omit the mutation-\nbased training data augmentation step. We observe that\ntaking out mutation-based bootstrapping results in slower\ntraining, although performance does increase over time and\ndoes not stagnate. We therefore conjecture that mutation-\nbased augmentation is not necessary but still useful.\n4. Related work\n4.1. Abstraction and Reasoning Corpus (ARC)\nVarious works have applied program synthesis approaches\nto subsets of the ARC dataset. Xu et al. (2022) proposes\nto represent grids as graphs, and applies logical programs\n6\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\ninitial policy\n# demo\n# policy\npolicy only\ncumulative\nMethod\nweights\nexamples\nsamples\nperf.\nperf.\nCodeIt\nCodeT5\n\u2264 10\n24\n49/400\n59/400\nA1: No ExIt\nCodeT5\n\u2264 10\n0\n13/400\n45/400\nA2: No relabeling\nCodeT5\n\u2264 10\n24\n24/400\n42/400\nA3: No priority\nCodeT5\n\u2264 10\n24\n38/400\n58/400\nA4: No pretraining\nRandom\n\u2264 10\n24\n9/400\n35/400\nA5: One demo\nCodeT5\n\u2264 1\n24\n34/400\n51/400\nA6: No mutation\nCodeT5\n\u2264 10\n24\n17/400\n20/400\nTable 2. ARC evaluation performance of CodeIt ablations.\nto the graph nodes, solving 63 of 160 tasks. Kolev et al.\n(2020) apply a Differentiable Neural Computer to ARC,\nsolving 78% of tasks with grids of size 10 \u00d7 10 and smaller.\nAlford et al. (2022) applies DreamCoder (Ellis et al., 2020)\nand execution-guided program synthesis, solving 22 of 36\nconsidered tasks. Park et al. (2023) first collects human\nfeedback, then performs behavioral cloning for a subset of\nARC tasks using a decision transformer (Chen et al., 2021).\nHowever, none of these methods are applied on the full ARC\nevaluation set, typically due to poor scaling behavior.\nThe few works that do scale to the full evaluation set tend to\nsolve each task in isolation. Ferr\u00b4e (2021) and followup work\nFerr\u00b4e (2023) design a custom DSL and perform a fast search\nfor each task. Ainooson et al. (2023) designs a custom DSL\nas well and obtains best performance with a brute-force\nsearch, solving 36 of 400 evaluation tasks. Mirchandani\net al. (2023) and Gendron et al. (2023) demonstrate that\na pretrained language model with custom tokenizer will\noutput the correct grid after being shown multiple input-\noutput pairs, solving 27 of 400 and 49 of 412 evaluation\ntasks respectively. Wang et al. (2023a) further augment this\napproach by generating hypotheses in multiple rounds, al-\nthough they only show performance on a subset of the ARC\ntraining set due to the high monetary cost of querying the\nlanguage model. In this work, we design a scalable ExIt\napproach that combines a smaller language model with the\nhigher-level abstraction of a DSL. We also ensure that our\napproach incorporates experience to benefit from general-\nization between tasks.\nVarious unpublished approaches exist too, including submis-\nsions to ARC challenges as well as a Kaggle competition.\nThese competitions use a private leaderboard, not revealed\nto participants. This means participants often use the public\nARC evaluation set for training or DSL design purposes.\nFor example, the winner of Kaggle 2020 comments that\nsearching in a DSL designed using the training set resulted\nin low performance, and higher performance was reached\nafter conditioning the DSL on the evaluation tasks (Icecuber,\n2020). This makes direct comparisons to methods evaluated\non the evaluation set difficult. For reference, we include\na summary of competition results in Appendix D Table 7,\nhowever, note that this summary reports performance on the\nhidden test set, and that competition results cannot not be\ndirectly compared to this work and the literature.\n4.2. Expert Iteration\nExpert iteration (ExIt) (Anthony et al., 2017) consists of\na policy-guided search stage that gathers new experiences,\nand a learning stage that improves the policy by imitation\nlearning. Commonly used experts tend to be powerful and\ncomputationally intensive tree search algorithms such as\nMonte Carlo Tree Search (Kocsis & Szepesv\u00b4ari, 2006) and\ngreedy search (Daum\u00b4e et al., 2009). ExIt has achieved\nsuperhuman performance include games (Silver et al., 2016;\n2018; Anthony et al., 2017) and combinatorial problems\nsuch as bin-packing (Laterre et al., 2019). Related work\nthat employs hindsight relabelling in expert iteration are\nGauthier & Urban (2022) and Butt et al. (2022).\nApplications\nof\nExIt\nfor\nprogramming-by-examples\n(Mankowitz et al., 2023; Ellis et al., 2020) are most relevant\nto CodeIt. Mankowitz et al. (2023) consider one task only:\nwriting a fast sorting algorithm. For this problem, inter-task\ngeneralization is therefore not as important. DreamCoder\n(Ellis et al., 2020) is most related to our work, since this ExIt\nmethod is applied to multiple programming-by-examples\ntasks. DreamCoder uses a continually growing DSL to store\nabstractions, and a computationally intensive search pro-\ncedure. Instead, CodeIt uses the model to store distilled\nknowledge, and generates experiences via sampling from\nthe model. Furthermore, DreamCoder filters solutions based\non correctness whereas CodeIt uses hindsight relabeling and\nprioritized experience replay.\n4.3. Self Improving Large Language Models\nPrevious work showed that learning from synthetic data\nis a viable strategy for programming-by-examples (Balog\net al., 2017; Devlin et al., 2017; Bunel et al., 2018; Parisotto\n7\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\net al., 2017; Polosukhin & Skidanov, 2018; Zohar & Wolf,\n2018), often training a model from scratch. Instead, finetun-\ning pre-trained large language models (LLMs) on synthetic\ndata enables knowledge transfer due to the prior domain\nknowledge captured in their weights (Butt et al., 2022). Re-\ncently, methods that use LLMs to synthesize training data\nhave shown successes in general domains including theorem\nproving (Polu et al., 2022), question answering (Zelikman\net al., 2022; Aksitov et al., 2023), mathematical reasoning\n(Ni et al., 2023), machine translation (Gulcehre et al., 2023),\nlanguage-to-code generation (Zhou et al., 2023; Singh et al.,\n2023) and code-to-code generation (Haluptzok et al., 2022).\nWe demonstrate in this work that such an approach can be\napplied to the challenging ARC domain as well.\n5. Discussion\nVarious factors make ARC uniquely challenging for\nlearning-based approaches, for example the limited amount\nof training data, and the complexity of individual tasks.\nAnother issue is that tasks may differ in number of demon-\nstration examples and input dimensionality, which requires\nagents to reason about concepts at different scales. In this\nwork, we show that an expert iteration based approach can\nlearn to solve 59 of 400 unseen ARC tasks. Here, we pro-\nvide intuition for why CodeIt works well on this benchmark.\nAblations showed that hindsight relabeling has a large ef-\nfect on performance. Many expert iteration approaches rely\non the emergence of a curriculum of increasingly difficult\ntasks, even creating a curriculum by comparing the current\nagent to past versions of itself (Silver et al., 2016; Fawzi\net al., 2022) or reward shaping (Laterre et al., 2019; Gul-\ncehre et al., 2023). Hindsight relabeling forms an implicit\ncurriculum (Andrychowicz et al., 2017): initially we collect\neasy tasks that can be solved in few lines of code, while\nlater on, programs become more complex. This is useful for\nARC, where obtaining even one solved task is challenging.\nAs relabeling adds many programs to the buffer, including\nsome that are further away from the target tasks, we used\nprioritized sampling to avoid catastrophic forgetting.\nA potential limitation of CodeIt is that for ARC, it relies\non hand-designed components: a domain specific language\n(DSL), access to an interpreter for automatic evaluation,\nand an initial set of ground truth programs. While we do\nbenefit from Hodel\u2019s expert-designed DSL, we also showed\nthat a neuro-symbolic approach (ablation A1) outperforms\na symbolic approach (the mutation baseline), indicating that\nboth DSL and learning contribute to performance. Further,\nCodeIt outperforms both, indicating that ExIt compounds\nthis effect. We also use a pretrained LLM and mutation\nprocedure to speed up training, but ablations showed that\ntraining is possible even without these, albeit at a slower\npace. Nevertheless, approaches that can start learning tabula\nrasa, or form their own DSL (Ellis et al., 2020) remain an\nimportant area of research.\nFor the ARC dataset, it is currently beneficial to incorporate\nboth prior knowledge (via a DSL or pre-trained LLM) and\nexperience (via expert iteration). Chollet (2019) defines the\nintelligence of a system as \u201ca measure of its skill-acquisition\nefficiency over a scope of tasks, with respect to priors, ex-\nperience, and generalization difficulty\u201d. Chollet poses that,\nif two systems are initialized with the same prior knowl-\nedge and go through the same amount of experience with\nrespect to a set of unseen tasks, the more intelligent sys-\ntem will combine prior knowledge and its experience more\nefficiently, solving more tasks.\nAlthough many existing approaches incorporate prior knowl-\nedge through a programming language or DSL (Ainooson\net al., 2023; Ferr\u00b4e, 2023), a pre-trained large language model\n(Gendron et al., 2023; Mirchandani et al., 2023), or both\n(Wang et al., 2023a), they cannot incorporate new experi-\nence, and therefore do not benefit from inter-task general-\nization. Alford (2021) proposes an expert iteration method\nthat does learn from experience, but it does not scale well\nnor benefit from prior knowledge in its policy. We pose\nthat CodeIt is the more effective expert iteration method\ndue to its use of scalable components: pre-trained language\nmodels, likelihood-based training, and running programs in\ninterpreters. There is also an implicit relationship between\ncomputational efficiency and experience: since CodeIt\u2019s\npolicy learns on the ARC domain, it is possible to use a\nmuch smaller language model than for example Gendron\net al. (2023), who use GPT-4 as a policy. This is consistent\nwith LLM literature showing that high quality training data\nwith a curriculum enables smaller LMs to compete with\nmuch larger ones on coding tasks (Gunasekar et al., 2023).\n6. Conclusion\nWe introduce a novel and scalable method for self-\nimproving language models, CodeIt, that uses prioritized\nhindsight replay. CodeIt achieves state-of-the-art perfor-\nmance on the Abstraction and Reasoning Corpus (ARC)\ncompared to symbolic and neural baselines, solving 59 of\n400 evaluation tasks. Ablations show that hindsight relabel-\ning leads to improved sample efficiency resulting in a 40%\nimprovement in performance. We also find that prioritizing\nimportant experiences during training ameliorates catas-\ntrophic forgetting. Additionally, we observe that CodeIt\nis able to refine solutions over time, identifying a shorter\nprogram for 53% of solved tasks in later iterations. The re-\nsults demonstrate that our self-improving language model is\ncapable of reasoning in the program space and generalizing\nbetween tasks. For the challenging ARC benchmark, both\nscalability and learning from experience prove to be key\ncomponents for success.\n8\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nReferences\nAinooson, J., Sanyal, D., Michelson, J. P., Yang, Y., and\nKunda, M. An approach for solving tasks on the abstract\nreasoning corpus, 2023.\nAksitov, R., Miryoosefi, S., Li, Z., Li, D., Babayan, S., Kop-\nparapu, K., Fisher, Z., Guo, R., Prakash, S., Srinivasan,\nP., Zaheer, M., Yu, F., and Kumar, S. Rest meets re-\nact: Self-improvement for multi-step reasoning llm agent,\n2023.\nAlford, S. A Neurosymbolic Approach to Abstraction and\nReasoning. PhD thesis, Massachusetts Institute of Tech-\nnology, 2021.\nAlford, S., Gandhi, A., Rangamani, A., Banburski, A.,\nWang, T., Dandekar, S., Chin, J., Poggio, T. A., and\nChin, P. Neural-guided, bidirectional program search for\nabstraction and reasoning. Complex Networks, 2021.\nAlford, S., Gandhi, A., Rangamani, A., Banburski, A.,\nWang, T., Dandekar, S., Chin, J., Poggio, T., and Chin,\nP. Neural-guided, bidirectional program search for ab-\nstraction and reasoning. In Complex Networks & Their\nApplications X: Volume 1, Proceedings of the Tenth In-\nternational Conference on Complex Networks and Their\nApplications COMPLEX NETWORKS 2021 10, pp. 657\u2013\n668. Springer, 2022.\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,\nR., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,\nand Zaremba, W. Hindsight experience replay. Advances\nin neural information processing systems, 30, 2017.\nAnthony, T., Tian, Z., and Barber, D. Thinking fast and slow\nwith deep learning and tree search. May 2017.\nARCathon Leaderboard.\nhttps://lab42.global/\narcathon/leaderboard/, 2023. Accessed: 2024-\n30-01.\nBalog, M., Gaunt, A., Brockschmidt, M., Nowozin, S., and\nTarlow, D. Deepcoder: Learning to write programs. In\nInternational Conference on Learning Representations.\nOpenReview. net, 2017.\nBunel, R., Hausknecht, M., Devlin, J., Singh, R., and Kohli,\nP. Leveraging grammar and reinforcement learning for\nneural program synthesis. In International Conference\non Learning Representations, 2018.\nButt, N., Wiggers, A., Cohen, T., and Welling, M. Program\nsynthesis for integer sequence generation. 2022.\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A.,\nLaskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. De-\ncision transformer: Reinforcement learning via sequence\nmodeling, 2021. URL https://arxiv.org/abs/\n2106.01345.\nChollet, F. On the measure of intelligence. arXiv preprint\narXiv:1911.01547, 2019.\nDaum\u00b4e, H., Langford, J., and Marcu, D. Search-based\nstructured prediction. Machine learning, 75:297\u2013325,\n2009.\nDevlin, J., Uesato, J., Bhupatiraju, S., Singh, R., Mohamed,\nA.-r., and Kohli, P. Robustfill: Neural program learning\nunder noisy i/o. In International conference on machine\nlearning, pp. 990\u2013998. PMLR, 2017.\nEllis, K., Wong, C., Nye, M. I., Sabl\u00b4e-Meyer, M., Cary,\nL., Morales, L., Hewitt, L. B., Solar-Lezama, A., and\nTenenbaum, J. B.\nDreamCoder: Growing generaliz-\nable, interpretable knowledge with wake-sleep Bayesian\nprogram learning. CoRR, abs/2006.08381, 2020. URL\nhttps://arxiv.org/abs/2006.08381.\nFawzi, A., Balog, M., Huang, A., Hubert, T., Romera-\nParedes, B., Mohammadamin, B., Novikov, A., Ruiz, F.\nJ. R., Schrittwieser, J., Swirszcz, G., Silver, D., Hassabis,\nD., and Kohli, P. Discovering faster matrix multiplica-\ntion algorithms with reinforcement learning. Nature, 610,\n2022. doi: https://doi.org/10.1038/s41586-022-05172-4.\nFerr\u00b4e, S. First steps of an approach to the arc challenge based\non descriptive grid models and the minimum description\nlength principle. arXiv preprint arXiv:2112.00848, 2021.\nFerr\u00b4e, S. Tackling the abstraction and reasoning corpus (arc)\nwith object-centric models and the mdl principle. arXiv\npreprint arXiv:2311.00545, 2023.\nFrench, R. Catastrophic forgetting in connectionist net-\nworks. Trends in cognitive sciences, 3:128\u2013135, 05 1999.\ndoi: 10.1016/S1364-6613(99)01294-2.\nGauthier, T. Program synthesis for the oeis, 2022. URL\nhttps://arxiv.org/abs/2202.11908.\nGauthier, T. and Urban, J. Learning program synthesis\nfor integer sequences from scratch, 2022. URL https:\n//arxiv.org/abs/2202.11908.\nGendron, G., Bao, Q., Witbrock, M., and Dobbie, G. Large\nlanguage models are not strong abstract reasoners, 2023.\nGulcehre, C., Paine, T. L., Srinivasan, S., Konyushkova,\nK., Weerts, L., Sharma, A., Siddhant, A., Ahern, A.,\nWang, M., Gu, C., et al. Reinforced self-training (rest)\nfor language modeling. arXiv preprint arXiv:2308.08998,\n2023.\nGunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T.,\nGiorno, A. D., Gopi, S., Javaheripi, M., Kauffmann, P.,\nde Rosa, G., Saarikivi, O., Salim, A., Shah, S., Behl,\nH. S., Wang, X., Bubeck, S., Eldan, R., Kalai, A. T., Lee,\nY. T., and Li, Y. Textbooks are all you need, 2023.\n9\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nHaluptzok, P., Bowers, M., and Kalai, A. T. Language\nmodels can teach themselves to program better. arXiv\npreprint arXiv:2207.14502, 2022.\nHodel, M. Domain-specific language for the abstraction and\nreasoning corpus, 2023.\nIcecuber.\nhttps://www.\nkaggle.com/competitions/\nabstraction-and-reasoning-challenge/\ndiscussion/154597, 2020. Accessed: 2024-30-01.\nJohnson, A., Vong, W. K., Lake, B. M., and Gureckis, T. M.\nFast and flexible: Human program induction in abstract\nreasoning tasks. CoRR, abs/2103.05823, 2021. URL\nhttps://arxiv.org/abs/2103.05823.\nKaggle\nLeaderboard.\nhttps://\nwww.kaggle.com/competitions/\nabstraction-and-reasoning-challenge/\ncode, 2020. Accessed: 2024-30-01.\nKocsis, L. and Szepesv\u00b4ari, C. Bandit based monte-carlo\nplanning. In European conference on machine learning,\npp. 282\u2013293. Springer, 2006.\nKolev, V., Georgiev, B., and Penkov, S. Neural abstract\nreasoner. arXiv preprint arXiv:2011.09860, 2020.\nLaterre, A., Fu, Y., Jabri, M. K., Cohen, A.-S., Kas, D.,\nHajjar, K., Chen, H., Dahl, T. S., Kerkeni, A., and Beguir,\nK. Ranked reward: enabling self-play reinforcement\nlearning for bin packing. 2019.\nMankowitz, D. J., Michi, A., Zhernov, A., Gelmi, M., Selvi,\nM., Paduraru, C., Leurent, E., Iqbal, S., Lespiau, J.-B.,\nAhern, A., et al. Faster sorting algorithms discovered\nusing deep reinforcement learning. Nature, 618(7964):\n257\u2013263, 2023.\nMirchandani, S., Xia, F., Florence, P., Ichter, B., Driess, D.,\nArenas, M. G., Rao, K., Sadigh, D., and Zeng, A. Large\nlanguage models as general pattern machines.\narXiv\npreprint arXiv:2307.04721, 2023.\nNi, A., Inala, J. P., Wang, C., Polozov, O., Meek, C., Radev,\nD., and Gao, J.\nLearning math reasoning from self-\nsampled correct and partially-correct solutions, 2023.\nParisotto, E., Mohamed, A.-r., Singh, R., Li, L., Zhou, D.,\nand Kohli, P. Neuro-symbolic program synthesis. In\nInternational Conference on Learning Representations,\n2017.\nPark, J., Im, J., Hwang, S., Lim, M., Ualibekova, S., Kim,\nS., and Kim, S. Unraveling the arc puzzle: Mimicking\nhuman solutions with object-centric decision transformer.\narXiv preprint arXiv:2306.08204, 2023.\nPolosukhin, I. and Skidanov, A. Neural program search:\nSolving programming tasks from description and ex-\namples.\nCoRR, abs/1802.04335, 2018.\nURL http:\n//arxiv.org/abs/1802.04335.\nPolu, S., Han, J. M., Zheng, K., Baksys, M., Babuschkin,\nI., and Sutskever, I. Formal mathematics statement cur-\nriculum learning. CoRR, abs/2202.01344, 2022. URL\nhttps://arxiv.org/abs/2202.01344.\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-\ntized experience replay. arXiv preprint arXiv:1511.05952,\n2015.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nVan Den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., et al. Mastering the\ngame of go with deep neural networks and tree search.\nnature, 529(7587):484\u2013489, 2016.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai,\nM., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Grae-\npel, T., et al. A general reinforcement learning algorithm\nthat masters chess, shogi, and go through self-play. Sci-\nence, 362(6419):1140\u20131144, 2018.\nSingh, A., Co-Reyes, J. D., Agarwal, R., Anand, A., Patil,\nP., Garcia, X., Liu, P. J., Harrison, J., Lee, J., Xu, K.,\nParisi, A., Kumar, A., Alemi, A., Rizkowsky, A., Nova,\nA., Adlam, B., Bohnet, B., Elsayed, G., Sedghi, H., Mor-\ndatch, I., Simpson, I., Gur, I., Snoek, J., Pennington, J.,\nHron, J., Kenealy, K., Swersky, K., Mahajan, K., Culp,\nL., Xiao, L., Bileschi, M. L., Constant, N., Novak, R.,\nLiu, R., Warkentin, T., Qian, Y., Bansal, Y., Dyer, E.,\nNeyshabur, B., Sohl-Dickstein, J., and Fiedel, N. Beyond\nhuman data: Scaling self-training for problem-solving\nwith language models, 2023.\nSpelke, E. S. and Kinzler, K. D.\nCore knowledge.\nDevelopmental Science, 10(1):89\u201396, 2007.\ndoi:\nhttps://doi.org/10.1111/j.1467-7687.2007.00569.x. URL\nhttps://onlinelibrary.wiley.com/doi/\nabs/10.1111/j.1467-7687.2007.00569.x.\nWang, R., Zelikman, E., Poesia, G., Pu, Y., Haber, N., and\nGoodman, N. D. Hypothesis search: Inductive reasoning\nwith language models. arXiv preprint arXiv:2309.05660,\n2023a.\nWang, Y., Le, H., Gotmare, A. D., Bui, N. D., Li, J., and\nHoi, S. C. H. Codet5+: Open code large language models\nfor code understanding and generation. arXiv preprint,\n2023b.\nWang, Z., Hou, L., Lu, T., Wu, Y., Li, Y., Yu, H., and\nJi, H. Enable language models to implicitly learn self-\nimprovement from data, 2023c.\n10\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nXu, Y., Khalil, E. B., and Sanner, S. Graphs, constraints, and\nsearch for the abstraction and reasoning corpus, 2022.\nXu, Y., Li, W., Vaezipoor, P., Sanner, S., and Khalil, E. B.\nLlms and the abstraction and reasoning corpus: Suc-\ncesses, failures, and the importance of object-based rep-\nresentations. arXiv preprint arXiv:2305.18354, 2023.\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Boot-\nstrapping reasoning with reasoning. Advances in Neural\nInformation Processing Systems, 35:15476\u201315488, 2022.\nZhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H.,\nand Wang, Y.-X. Language agent tree search unifies\nreasoning acting and planning in language models, 2023.\nZohar, A. and Wolf, L. Automatic program synthesis of long\nprograms with a learned garbage collector. Advances in\nneural information processing systems, 31, 2018.\n11\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nA. Method and evaluation details\nA.1. CodeIt Algorithm\nThe pseudo code for the CodeIt procedure is portrayed in Algorithm 1.\nAlgorithm 1 CodeIt Algorithm\nRequire: Training set Dtrain, search set Dtest, policy Q\nEnsure: Finetuned policy Q, updated replay buffer R, optimal programs set \u03c1\u2217\nDaugmented train \u2190 EvolveTrainingTasks(Dtrain) {Evolve training tasks}\nInitialize \u03c1\u2217 as an empty set {Init set of programs that solve tasks in Dtest}\nfor meta iter = 1 \u2192 100 do\n# Sampling and hindsight relabeling stage\nfor task in Dtest do\n{\u03c1} \u2190 Q(\u03c1|{I, O}) {Sample programs for test tasks}\nfor each \u03c1 in {\u03c1} do\nif SyntacticallyValid(\u03c1) then\nAdd {\u03c1, {(I(i), \u03c1(I(i))), . . . }} to R {Update the replay buffer with hindsight relabeled tasks}\nend if\nfor (I(i), O(i)) in task do\nif \u03c1(I(i)) = O(i) then\nAdd {(\u03c1, task)} to \u03c1\u2217 {Update set of programs that solve tasks in Dtest}\nend if\nend for\nend for\nend for\n# Learning stage\nDsample \u2190 SampleFrom(R + Daugmented train + Dtrain) {Sample tasks from the replay buffer}\nTrain Q on Dsample for 1 epoch {Continual training of the policy}\nend for\nInitializing CodeIt\nBefore we start the CodeIt procedure, we expand the training dataset using the first 19,200 mutated\ntasks from the mutation procedure (see Appendix A.2) used for the mutation d1 baseline.\nA.2. Program and Task Mutation\nMutation procedure\nTo grow a population of mutated programs with task demonstration inputs corresponding to the\noriginal training dataset, we follow the procedure outlined in Algorithm 3. This involves mutating a single task, which is\ndescribed in Algorithm 2. The mutation is carried out with the hyperparameters \u03d5var = 0.25, \u03d5arg = 0.5, \u03d5func = 0.25. With\nrespect to naming notation, d1 reflects a depth of 1, meaning we only mutate programs from the original training set, and\nd\u221e reflects a depth of infinity, meaning we can mutate previously mutated programs.\nThe intuitive explanation of the mutation procedure for a single program is as follows. We pick a random line from a\nprogram (L2-3). We then replace either a function call with a function with similar output type (L4-7), or we replace an\ninput argument in the function call (L8-11), or we replace the function call but leave its input variables the same (L12-14).\nMutation baseline\nFor our mutation baseline, we sample mutated programs using the mutation procedure outlined above.\nFor all the mutated programs in the evolved task population, we evaluate each program on the tasks in our search set.\nA.3. Task Representation\nGrid representation\nWe use a compressed grid representation, mainly to reduce the number of tokens needed to represent\neach grid. We do not use a custom tokenizer. A visualization of the number of tokens is shown in Fig. 11, showing that in\nalmost all cases, the sparse grid representation we use leads to a reduction in the number of needed tokens, especially for\nlarger grid sizes.\n12\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nAlgorithm 2 MutateProgram\nRequire: Replacement probabilities \u03d5var, \u03d5arg, \u03d5func, program \u03c1\nEnsure: \u03c1\u2032\nInitialize \u03c1\u2032 \u2190 \u03c1 {Copy original program}\nl \u2190 RandomLineFrom(\u03c1\u2032) {Randomly select a line}\np \u223c U(0, 1)\nif p < \u03d5var then\nf \u2032 \u2190 SampleFunctionWithOutputType(GetTypeOfVariable(l))\nargs\u2032 \u2190 SampleArgumentsForFunction(f \u2032)\nReplace variable definition f(args) in l with f \u2032(args\u2032)\nelse if p < (\u03d5var + \u03d5arg) then\na \u2190 RandomArgumentFrom(l)\na\u2032 \u2190 SampleTermOfType(GetTypeOfArgument(a))\nReplace argument a with a\u2032\nelse\nf \u2032 \u2190 SampleFunctionOfType(GetTypeOfFunction(f))\nReplace function f in l with f \u2032\nend if\nAlgorithm 3 EvolveTrainingTasks\nRequire: Initial population of training tasks Tinit (each task is a tuple (\u03c1, E) where E = {(I(i), O(i)), . . . }), depth\nEnsure: Updated task population T \u2032 (initialized with Tinit)\nT \u2190 Tinit\ni \u2190 0\nwhile i < num samples do\nif depth = 1 then\n(\u03c1, E) \u2190 RandomSelectTask(Tinit) {Select from initial tasks}\nelse\n(\u03c1, E) \u2190 RandomSelectTask(T) {Select from current tasks}\nend if\n\u03c1\u2032 \u2190 MutateProgram(\u03c1)\nE\u2032 \u2190 \u2205 {Initialize mutated task demonstration examples}\nfor each (I(k), ) \u2208 E do\nO\u2032(k) \u2190 Execute(\u03c1\u2032, I(k))\nE\u2032 \u2190 E\u2032 \u222a {(I(k), O\u2032(k))}\nend for\nif AreValidGrids(GetAllOutputs(E\u2032)) then\nT \u2032 \u2190 T \u2032 \u222a {(\u03c1\u2032, E\u2032)} {Add new task to the population}\nend if\ni \u2190 i + 1\nend while\n13\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nTruncation\nWe truncate our task demonstration tokens and program tokens such that these sequences fit in our predefined\nencoder and decoder context windows. For the task demonstration examples, we first order by grid size and divide the\nencoder context window into two equally sized sections. For task demonstration inputs, we first encode input grids to text as\nabove and then we tokenize using the standard text tokenizer. We truncate these tokens at half the size of the encoder context\nwindow. We do the same for the task demonstration outputs and with the exception of also adding an end of sequence token.\nAs a result, even though we aim to show the policy up to ten task demonstration examples, large grids will be cut-off. For\nprograms, we tokenize directly using the standard text tokenizer and truncate at the decoder context window size.\nA.4. ARC evaluation\nDifferent works use different evaluation procedures to report performance on the ARC evaluation set. We describe two\ncommon evaluation settings in more detail below. Unless mentioned otherwise, we always use the first procedure, \u201cARC\nEval Set\u201d.\nARC Eval Set\nThis setup is intended as close as possible to the evaluation procedure described by Chollet (2019).\nBaselines Ferr\u00b4e (2021), Ainooson et al. (2023) follow this procedure, and it is our default setting as well.\nThe ARC eval set consists of 400 tasks, some of which contain multiple test examples. Common procedure is to report\npass@3 performance, meaning the top 3 solutions are selected according to demonstration task performance. If there are\nties, we favor the shorter program, under the assumption that shorter programs are more likely to generalize. We then run\nthese programs on all test examples for the task. In some cases, there are multiple test examples per task. We call the task\n\u201csolved\u201d if all output grids are correct.\nARC Eval 412\nThis setup is designed to match Gendron et al. (2023). Instead of calling a task with multiple test examples\nsolved if all test outputs are correct, distinct tasks are created - one per test example. This results in a set of 412 evaluation\ntasks with one test example each. Furthermore, Gendron et al. (2023) uses pass@1, rather than pass@3: only one solution\nper task is evaluated, and the task is considered solved if the output is correct.\nB. Experiment details\nB.1. Resources\nExperiments were run for a maximum of 120 hours on a NVIDIA A100 80GB.\nB.2. Hyperparameter tuning\nDataset\nThe ARC benchmark does not contain a validation split. Hence, we use part of the ARC train split for validation\nduring the hyperparameter tuning. In particular, this validation set is the search set that the sampling stage uses as described\nin 2.2. With this setup we avoid overfitting the hyperparameters to the ARC evaluation split.\nWe choose the split such that Dtrain and Dvalid contain roughly equally difficult programs by sampling based on program\nlength: Dtrain contains 80% of 2-line programs, 80% of 3-line programs, and so on. This results in 311 examples in Dtrain\nand 89 examples in Dvalid.\nExperiments on validation set\nIn these experiments, we initialise our replay buffer with the 311 Dtrain examples, and\nour search set consists of the 89 Dvalid examples. The aim of these experiments is to find optimal hyper-parameters for\nsearch and training. A list of our tuned hyperparameter values and their description is shown in Tab. 3\nB.3. Hyperparamaters chosen on internal validation set\nWe optimized these parameters on our custom validation set before applying CodeIt to ARC eval.\n14\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nMethod\nNumber of tasks solved\nCodeIt policy only\n23\nMutation d1 only\n13\nCodeIt policy \u2229 Mutation d1\n29\nTable 4. ARC evaluation tasks solved per method. The top group of two rows show how many tasks were solved by a method, but not by\nthe other. The final row shows tasks solved by both methods.\nCodeIt stage\nParam\nValue\nDescription\nSampling and Hindsight Relabeling\nn\u03c1\n24\nno. policy samples \u03c1 per task per meta-iteration1\nnm\n19, 200\nno. mutated samples for augmented train set1\n\u03c4\n0.95\nsampling temperature\nrt\n10, 000\nnumber of experiences sampled from augmented train set\nrp\n90, 000\nnumber of experiences sampled from buffer\nLearning\nn\u03f5\n1\nno. train epochs per meta-iteration\nlr\n5e \u2212 5\nlearning rate\nTable 3. Table of hyperparameters.\nB.4. Domain Specific Language\nWe\nadopt\nthe\ndomain\nspecific\nlanguage\n(DSL)\nof\nMichael\nHodel,\nmade\navailable\non\nGitHub:\nhttps://github.com/michaelhodel/arc-dsl.\nThis DSL was designed based on the training set: the (human) designer\ndid not look at the evaluation set. This is what allows us to run search on ARC eval here. Using a DSL designed for the eval\ntasks would be cheating, as we would benefit immensely from human insights captured in the primitives. On the other hand,\nit may mean that some ARC eval programs are not solvable with the current DSL.\nThe DSL is implemented in https://github.com/michaelhodel/arc-dsl/blob/main/dsl.py. It contains many basic grid manipula-\ntion operations, such as rotations (rot90, rot180, rot270), mirroring (dmirror, hmirror, vmirror), resizing\n(downscale, upscale), or concatenation (hconcat, vconcat). It also contains functions that perform counting, for\nexample numcolors counts the number of colors occurring in an object or grid. For some ARC tasks, identifying the\nforeground objects and determining how these objects interact is an effective strategy for human test-takers. Therefore, some\nfunctions also apply to \u201cobjects\u201d, which are patches of the same color that stand out from the background. To extract these,\nthe function objects returns the set of foreground objects, i.e. those that have a different color than the most common\ncolor, assumed to be the background. For a complete list of primitives and their description, we refer the reader to the\naforementioned Github page.\nMichael Hodel provides hand-designed solution programs for all training tasks in https://github.com/michaelhodel/arc-\ndsl/blob/main/solvers.py. Some programs are highly complex: for some of the more challenging ARC tasks, we see solutions\nconsisting of up to 58 lines of code (solve b775ac94). We use these 400 solution programs to kickstart CodeIt training.\nC. Program analysis\nC.1. CodeIt compared with mutation baselines\nWe compare the programs found using our mutation d1 baseline and the best performing of the three CodeIt runs. Table 4\ndisplays the number of ARC evaluation tasks uniquely solved by each method and the tasks which are solved by multiple\nmethods. CodeIt\u2019s policy solves 52 of 400 tasks, 23 of which were not solved by the mutation baseline. In Figures 8 and\n9, we select the shortest program that solves an evaluation task for CodeIt and our mutation d1 baseline, computing the\nprogram length and task representation size. CodeIt has an encoder context window size of 1024 and so any tasks which\nhaving representations of more than 1024 tokens have been truncated. Overall, CodeIt finds shorter programs as shown in\n1Note that no. samples here refers to policy and mutation samples before filtering for syntactic correctness.\n15\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nFigure 8. Histogram of number of lines for tasks where both\nCodeIt and Mutation produced solutions. CodeIt (in blue) pro-\nduces shorter programs than the Mutation baseline (in orange).\nFigure 9. Number of task representation tokens vs number of pro-\ngram tokens. Colors represents the different tasks. We see no obvi-\nous correlation between task representation and program length.\nFigure 8. Further, for the same task, CodeIt more often finds shorter programs than our mutation d1 baseline, as shown\nin Figure 9 where each color represents a different task. Interestingly, CodeIt does solve some tasks with very large task\nrepresentations, suggesting in some cases a truncated task representation provides sufficient information to solve the task.\nIn Table 5, we show a subset of solution programs for ARC evaluation tasks solved by both CodeIt and our mutation d1\nbaseline. We select tasks where the shortest programs differ between the two methods. CodeIt programs appear more\nconcise and use different primitives. Out of the 29 tasks that are solved by both methods, there are 24 shortest programs\nwhere method output is different. CodeIt only produces a longer program in 1 out of these 24 cases. The Mutation baseline\noften includes redundant lines. In addition, for many programs, CodeIt produces a program that is qualitatively better: the\nsolution is less complex, and contains fewer lines overall.\nC.2. CodeIt over time\nSince we do not have ground truth programs for the ARC evaluation set, we treat the shortest program found with\ndemonstration performance and test performance equal to 1 for each task over all the meta-iterations as a proxy for the\nground truth program. To examine how CodeIt solutions change over time, we take the subset of ARC evaluation tasks\nwhere the best performing CodeIt run finds such programs; this leaves us 45 tasks. We observe that once CodeIt finds a\nsolution, CodeIt often continues to find both longer and shorter solutions in later meta-iterations. We pose that this gives the\npotential for program refinement, however, since the priority does not incorporate length, there is not explicit bias towards\nshorter solutions and so both longer and shorter solutions will be learned from. We observe that out of the 45 tasks, the best\nperforming CodeIt run finds shorter solutions over time in 24 tasks as shown in Figure 10.\nIn Tables 6, we show a selection of examples where the best performing CodeIt run finds a longer solution in an earlier\nmeta-iteration and shorter solution in a later meta-iteration.\nD. ARC competitions\nCompetition\nWinner\nMethod\nHidden Test Perf.\nKaggle 2020\nIcecuber (2020)\nSearch in eval set DSL*\n21%\nKaggle 2020 late\nMultiple (Kaggle Leaderboard, 2020)\nEnsemble previous entries*\n30%\nARCathon 2022\nHodel (2023)\nSearch in CodeIt DSL\n6%\nARCathon 2023\nMultiple (ARCathon Leaderboard, 2023)\nUnknown\n30%\nTable 7. Performance on Hidden Test Set for Various ARC Competition Winners. *Method conditions on ARC evaluation set.\n16\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nCodeIt Policy\nMutation d1\nTest Example\nx1 = vmirror(I)\nx1 = vmirror(I)\nx2 = hconcat(x1, I)\nx2 = hconcat(x1, I)\nO = hconcat(x2, x2)\nx3 = hmirror(x2)\nx4 = vconcat(x2, x3)\nx5 = hconcat(x3, x3)\nO = hmirror(x5)\nx1 = compress(I)\nx1 = hmirror(I)\nx2 = ofcolor(I, THREE)\nx2 = vmirror(I)\nx3 = rot90(x1)\nx3 = ofcolor(I, THREE)\nO = subgrid(x2, x3)\nx4 = subgrid(x3, x1)\nx5 = subgrid(x3, x2)\nx6 = palette(x4)\nx7 = contained(ONE, x6)\nO = branch(x7, x5, x4)\nx1 = ofcolor(I, ONE)\nx1 = mostcolor(I)\nx2 = subgrid(x1, I)\nx2 = objects(I, T, F, T)\nO = cmirror(x2)\nx3 = replace(I, x1, THREE)\nx4 = argmax(x2, size)\nx5 = argmin(x2, size)\nx6 = position(x4, x5)\nx7 = first(x6)\nx8 = last(x6)\nx9 = subgrid(x4, x3)\nx10 = hline(x5)\nx11 = hmirror(x9)\nx12 = vmirror(x9)\nx13 = branch(x10, x11, x12)\nx14 = branch(x10, x7, ZERO)\nx15 = branch(x10, ZERO, x8)\nx16 = asobject(x13)\nx17 = matcher(first, THREE)\nx18 = compose(flip, x17)\nx19 = sfilter(x16, x18)\nx20 = ulcorner(x4)\nx21 = shape(x4)\nx22 = astuple(x14, x15)\nx23 = multiply(x21, x22)\nx24 = add(x20, x23)\nx25 = shift(x19, x24)\nx26 = rot270(x11)\nO = paint(x26, x25)\nx1 = objects(I, F, F, T)\nx1 = objects(I, F, F, T)\nx2 = argmax(x1, numcolors)\nx2 = leastcolor(I)\nO = subgrid(x2, I)\nx3 = rbind(colorcount, x2)\nx4 = argmax(x1, x3)\nO = subgrid(x4, I)\nTable 5. Selection of shortest programs for ARC evaluation tasks solved by CodeIt policy (left) and the Mutation d1 baseline (right) for\nwhich CodeIt program is shorter.\n17\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nEarly Shortest Solution\nLater Shortest Solution\nTest Example\nx1 = ofcolor(I, EIGHT)\nx1 = replace(I, EIGHT, ZERO)\nx2 = replace(I, EIGHT, ZERO)\nx2 = compress(x1)\nx3 = compress(x2)\nO = downscale(x2, TWO)\nO = downscale(x3, TWO)\nx1 = objects(I, T, F, T)\nx1 = objects(I, T, F, T)\nx2 = apply(delta, x1)\nx2 = apply(delta, x1)\nx3 = mfilter(x2, square)\nx3 = mfilter(x2, square)\nx4 = fill(I, FIVE, x3)\nx4 = fill(I, FIVE, x3)\nx5 = objects(x4, F, F, T)\nx5 = objects(x4, F, F, T)\nx6 = mapply(delta, x5)\nx6 = mapply(delta, x5)\nx7 = fill(x4, SEVEN, x6)\nO = fill(x4, SEVEN, x6)\nO = fill(x7, FIVE, x3)\nx1 = objects(I, T, F, F)\nx1 = objects(I, T, F, T)\nx2 = colorfilter(x1, ZERO)\nx2 = sizefilter(x1, ONE)\nx3 = sizefilter(x2, ONE)\nx3 = difference(x1, x2)\nx4 = difference(x2, x3)\nx4 = merge(x3)\nx5 = merge(x4)\nO = fill(I, EIGHT, x4)\nO = fill(I, EIGHT, x5)\nx1 = vmirror(I)\nx1 = ofcolor(I, EIGHT)\nx2 = fgpartition(I)\nx2 = box(x1)\nx3 = compose(outbox, inbox)\nO = underfill(I, ONE, x2)\nx4 = mapply(x3, x2)\nO = underfill(I, ONE, x4)\nx1 = lefthalf(I)\nx1 = lefthalf(I)\nx2 = righthalf(I)\nx2 = righthalf(I)\nx3 = ofcolor(x1, ZERO)\nx3 = cellwise(x1, x2, ONE)\nx4 = ofcolor(x2, ZERO)\nO = replace(x3, SEVEN, ONE)\nx5 = intersection(x3, x4)\nx6 = shape(x1)\nx7 = canvas(ONE, x6)\nO = fill(x7, ZERO, x5)\nx1 = lefthalf(I)\nx1 = lefthalf(I)\nx2 = righthalf(I)\nx2 = righthalf(I)\nx3 = ofcolor(x1, FOUR)\nx3 = cellwise(x1, x2, FOUR)\nx4 = ofcolor(x2, FOUR)\nO = replace(x3, FOUR, EIGHT)\nx5 = combine(x3, x4)\nO = fill(x1, EIGHT, x5)\nx1 = lefthalf(I)\nx1 = vmirror(I)\nx2 = righthalf(I)\nx2 = lefthalf(I)\nx3 = ofcolor(x1, ZERO)\nx3 = righthalf(I)\nx4 = ofcolor(x2, ZERO)\nx4 = cellwise(x2, x3, TWO)\nx5 = intersection(x3, x4)\nO = replace(x4, EIGHT, TWO)\nx6 = shape(x1)\nx7 = canvas(TWO, x6)\nO = fill(x7, ZERO, x5)\nTable 6. Selection of shortest solutions for ARC evaluation tasks solved by CodeIt policy where shorter solutions are found over time.\n18\nCodeIt: Self-Improving Language Models with Prioritized Hindsight Replay\nFigure 10. Difference in number of tokens between the shortest solu-\ntion found per meta-iteration and shortest solution found by the final\nmeta-iteration for best performing CodeIt run.\nFigure 11. Grid size versus token count for the ARC training data.\nThe sparse grid representation is typically shorter than the raw grid\nrepresentation.\n19\n"
  },
  {
    "title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry",
    "link": "https://arxiv.org/pdf/2402.04347.pdf",
    "upvote": "11",
    "text": "The Hedgehog & the Porcupine: Expressive Linear Attentions with\nSoftmax Mimicry\nMichael Zhang, Kush Bhatia, Hermann Kumbong and Christopher R\u00b4e\nDepartment of Computer Science, Stanford University\n{mzhang,kushb,chrismre}@cs.stanford.edu, kumboh@stanford.edu,\nAbstract\nLinear attentions have shown potential for improving Transformer efficiency, reducing attention\u2019s\nquadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear\nTransformers from scratch, (2) \u201cfinetuned-conversion\u201d of task-specific Transformers into linear versions\nthat recover task performance, and (3) \u201cpretrained-conversion\u201d of Transformers such as large language\nmodels into linear versions finetunable on downstream tasks. However, linear attentions often under-\nperform standard softmax attention in quality.\nTo close this performance gap, we find prior linear\nattentions lack key properties of softmax attention tied to good performance: low-entropy (or \u201cspiky\u201d)\nweights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain\nthese properties and match softmax performance, but are inefficient to compute in linear attention. We\nthus propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of\nsoftmax attention while maintaining linear complexity. Hedgehog uses simple trainable MLPs to produce\nattention weights mimicking softmax attention. Experiments show Hedgehog recovers over 99% of stan-\ndard Transformer quality in train-from-scratch and finetuned-conversion settings, outperforming prior\nlinear attentions up to 6 perplexity points on WikiText-103 with causal GPTs, and up to 8.7 GLUE score\npoints on finetuned bidirectional BERTs. Hedgehog also enables pretrained-conversion. Converting a\npretrained GPT-2 into a linear attention variant achieves state-of-the-art 16.7 perplexity on WikiText-\n103 for 125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into a viable linear\nattention Llama. With low-rank adaptation, Hedgehog-Llama2 7B achieves 28.1 higher ROUGE-1 points\nover the base standard attention model, where prior linear attentions lead to 16.5 point drops.\n1\nIntroduction\nLinear attentions are promising methods for improving Transformer efficiency. By replacing the softmax of\nattention\u2019s query and key dot products with kernel function feature maps, linear attentions reduce attention\u2019s\ntime and space complexity from O(n2d) to O(ndd\u2032) where n is sequence length, d is head dimension and d\u2032\nthe feature map dimension (Katharopoulos et al., 2020; Choromanski et al., 2020; Peng et al., 2021; Xiong\net al., 2021; Schlag et al., 2021). For typical Transformer settings, e.g., with head dimension = 64 and\nsequence lengths at 512 to 32K, this quadratic-to-linear scaling can result in significant speed and memory\nimprovements (Fig. 6). As drop-in alternatives to popular softmax attention (Vaswani et al., 2017), linear\nattentions not only improve Transformer efficiency when training new models from scratch but can also\nimprove inference efficiency by converting pretrained Transformers into corresponding linear variants (Kasai\net al., 2021; Mao, 2022). Linear attention enables efficient Transformers in a variety of regimes:\n\u2022 Training-from-scratch: training Transformer models with linear attention with the goal of matching\nstandard Transformer performance, e.g., as tested on benchmarks such as Long Range Arena (LRA)\nclassification (Tay et al., 2021) and WikiText-103 language modeling (Merity et al., 2017).\n\u2022 Finetuned-conversion: swapping the attentions of task-specific Transformers and finetuning them to\nconvert existing models into linear versions, with the goal to recover original task performance with\nimproved efficiency (Kasai et al., 2021; Mao, 2022).\n1\narXiv:2402.04347v1  [cs.LG]  6 Feb 2024\nFigure 1: Hedgehog learns a trainable linear attention feature map designed to mimic standard attention,\nresulting in expressive yet efficient linear attentions for various Transformer training settings\n\u2022 Pretrained-conversion: doing the same as finetuned-conversion but for pretrained Transformers such\nas large language models (LLMs), e.g., to transfer to new tasks and longer contexts.\nUnfortunately, existing linear attention mechanisms typically fail to match softmax attention in modeling\nquality.\nWhen training from scratch, linear attentions achieve 4-6 worse perplexity (ppl) than softmax\nattention on standard benchmarks such as WikiText-103 (Schlag et al., 2021; Irie et al., 2021; Fu et al., 2023),\nthe equivalent gap between 125M and 255M Transformers (Dai et al., 2019). When converting finetuned\nmodels, linear attention models require additional quadratic attention modules to close the gap (Kasai et al.,\n2021; Mao, 2022). One might worry that such gaps are fundamental; for example, recent theory using the\nStrong Exponential Time Hypothesis (SETH) showed that high-quality truly subquadratic algorithms to\napproximate softmax attention may be impossible with large sequence length n (Alman & Song, 2023; Keles\net al., 2023).\nWe begin by empirically studying why this performance gap exists between standard softmax and pro-\nposed linear attentions. We identify two simple properties for softmax attention which prior linear attentions\nlack: 1) low-entropy \u201cspikyness\u201d and 2) dot-product monotonicity. We hypothesize that the quality gap in\nlinear attentions corresponds with lacking these two properties:\n\u2022 Low-entropy \u201cspikyness\u201d: Intuitively, we want attentions that attend to relevant tokens while ignoring\nirrelevant ones via their query-key interactions. We observe these low-entropy or \u201cspiky\u201d attention-weight\ndistributions in standard Transformer attention but not prior linear attention maps\u2014where spikes en-\nabled via the scaled dot-product softmax are lost via other feature maps (Fig. 2)\u2014and find this strongly\ncorresponds to Transformer performance (Fig. 4).\n\u2022 Dot-product monotonicity: This property requires that attention weights increase as the dot products\nof their corresponding queries and keys increase. Intuitively, the lack of this monotonicity can produce\nunstable gradients during training and finetuning, where increasing the query-key dot product can result\nin decreasing the attention weight the other way (and vice versa).\nAs a first step to recover these properties, we explore simple feature maps\u2014such as low-degree Taylor polyno-\nmial approximations to the exp() function\u2014that satisfy the above two properties (albeit in restricted regimes\nof bounded query-key dot products). In practice, we find that queries and keys are often bounded, resulting\nin linear attentions that recover softmax attention\u2019s spikiness, monotonicity, and subsequent performance.\nUnfortunately, while technically linear in sequence length, these polynomial feature maps remain inefficient\nto compute. They take O(ndp+1) time and space, and we find degree p \u2265 2 necessary for performance.\nWe thus propose Hedgehog, an efficient-to-compute learnable linear attention trained to capture the spiky\nand monotonic softmax properties. Unlike prior works that propose a specific kernel function (Katharopoulos\net al., 2020; Choromanski et al., 2020; Qin et al., 2022b) and our polynomial feature maps, we learn these\nfeature maps as single-layer MLPs specifically trained to match softmax attention weights. By mapping\nfrom Rd 7\u2192 Rd, we maintain prior linear attentions\u2019 O(nd2) complexity. However, training these mappings\nvia softmax attention weights as cross-entropy soft-labels, we find Hedgehog can match softmax attention\nweights with much higher fidelity (Fig. 7), producing low-entropy and monotonic weights that match standard\nattention performance quality.\n2\nWe validate experimentally that Hedgehog\u2019s improved expressivity translates to closing the softmax\nattention performance gap in the three regimes mentioned above:\n\u2022 Training-from-scratch: we find Hedgehog matches Transformers on standard attention benchmarks\nsuch as Long Range Arena (LRA) (Tay et al., 2021) task, and closes the linear attention gap by 68.6% on\nWikiText-103 language modeling (improving up to 6 ppl).\n\u2022 Finetuned-conversion: we find Hedgehog recovers >99% of original model performance on average\nacross bidirectional encoder-only 110M BERT-base models finetuned on GLUE and causal decoder-only\n125M GPT models finetuned on Wikitext-103.\n\u2022 Pretrained-conversion: we find Hedgehog enables effective transfer to new tasks and efficient scaling to\nlonger contexts, while frequently outperforming modern subquadratic sequence architectures by linearizing\nexisting pretrained Transformers. A 125M Hedgehog-GPT-2 finetuned on Wikitext-103 achieves a new\nstate-of-the-art 16.7 ppl for subquadratic models of the same size.\nFinally, we demonstrate that Hedgehog can be scaled up to modern large language models; we convert\npretrained Llama-2 7B into a viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2\n7B achieves up to 28.1 higher ROUGE-1 points over the base standard attention model. In contrast, prior\nlinear attentions result in models that struggle to produce coherent text (with 16.5 ROUGE-1 point drops).\n2\nPreliminaries and Related Work\nWe provide background on attention computation, describe kernel feature based linear attentions, and finally\nprovide details on existing linear attention mechanisms proposed in the literature.\nAttention setup.\nLet {qi}n\ni=1, {ki}n\ni=1, {vi}n\ni=1 denote the set of queries, keys, and values, with\nindividual elements in Rd.\nLet n denote sequence length and d denote head dimension.\nWe compute\nattention outputs yi \u2208 Rd by first computing similarities between each qi and every kj; for causal attention\nwe compute these similarities for j \u2264 i. The vanilla Transformer attention computes these similarities using\nthe softmax dot products (Vaswani et al., 2017):\nyi =\ni\nX\nj=1\nsim(qi, kj)vj,\nwhere\nsim(qi, kj) =\nexp(q\u22a4\ni kj/\n\u221a\nd)\nPi\nm=1 exp(q\u22a4\ni km/\n\u221a\nd)\n.\n(1)\nWhile very expressive, computing attention via Eq. 1 for all {yi}n\ni=1 requires O(n2d) time and memory,\nmaking this inefficient for long sequences. To improve efficiency without sacrificing quality, we thus want\nalternative linear attention maps which maintain standard attention\u2019s expressivity.\nLinear attention and kernel functions. Observe that the exp(\u00b7) in Eq. 1 can be viewed as a kernel\nfunction, which\nTsai et al. (2019); Katharopoulos et al. (2020) show can be replaced in general with\nK(x, x\u2032) = \u03d5(x)\u22a4\u03d5(x\u2032). Here \u03d5 : Rd 7\u2192 Rd\u2032 is a feature map applied to each vector. We can thus compute\nattention in linear time and space over the sequence length n, seen by rewriting Eq. 1 as:\nyi =\n\u03d5(qi) Pi\nj=1\n\u0000\u03d5(kj)\u22a4vj\n\u0001\n\u03d5(qi) Pi\nj=1 \u03d5(kj)\n.\n(2)\nPrior feature maps. From the previous section, we observe that linear attentions are promising direc-\ntions for improving Transformer efficiency at both training and inference time. Numerous prior works have\nproposed feature maps \u03d5 aiming to remain more efficient (where linear attention is desirable to standard\nattention if d\u2032 < n), while still being expressive and stable to train. These range from \u03d5 ensuring positive\nattention weights, e.g., via 1+ELU (Katharopoulos et al., 2020) or ReLU (Kasai et al., 2021), to softmax or\nGaussian kernel approximations via randomized features (Rahimi & Recht, 2007; Choromanski et al., 2020;\nPeng et al., 2021; Choromanski et al., 2021; Zheng et al., 2023) or low-rank approximations (Xiong et al.,\n2021; Chen et al., 2021).\n3\nFigure 2: Attention weight spikiness. (Plots 1 - 5): Softmax attention results in lower entropy and\n\u201cspiky\u201d selective weighting compared to prior linear attentions (training from scratch on associative recall\n(Sec. 3.2)). (Plot 6): By training to mimic softmax attention, our proposed Hedgehog recovers this spikiness\nas a linear attention, corresponding with improved performance (Sec. 5).\nFigure 3: Attention weight monotonicity. (Plots 1 - 5): In contrast to softmax attention, prior linear\nattentions are not smoothly monotonic over trained query-key dot products, resulting in poor performance\nwhen converting BERT models by replacing attentions (Table 1). (Plot 6): Hedgehog recovers this mono-\ntonicity, and thus recovers 99% of BERT performance after conversion (Table 8).\n3\nImproving Linear Attention via Spiky and Monotonic Weights\nWe begin by identifying two key properties of attention weights which we hypothesize are essential for\ngood performance quality.\nThe first, low-entropy spikyness, requires that the attention map is able to\ncapture effectively capture sparse relevant tokens in a sequence. The second, monotonicity over query-key\ndot products, requires the attention map to increase with increasing dot products, and allows for smooth\nconversion of pretrained Transformers into linear variants.\n3.1\nProperties for Expressive Attention Maps\nHere we describe the spiky and monotonic properties hypothesized for desirable linear attention.\nWe\nnote these add to past observations for more performant linear attentions, including positive attention\nweights (Katharopoulos et al., 2020), orthogonal features (Choromanski et al., 2020; Irie et al., 2021), or\nlocality (upweighting nearby values) (Qin et al., 2022a,b). We validate these properties among past linear\nattentions in Sec. 3.2, and preview how our proposed Hedgehog linear attention recovers these properties in\ncorrespondence with improved performance (Sec. 5) in Fig. 2, 3.\nLow-entropy spikiness.\nIntuitively, one source of attention\u2019s effectiveness is its ability to selectively\nupweight relevant tokens in a sequence. This is a popular interpretation visualized in various Transformer\narchitectures and settings ranging from encoder-decoder language translation (Bahdanau et al., 2014) to ViT\nimage segmentation (Dosovitskiy et al., 2020; Caron et al., 2021). Mechanically, the softmax over query-key\ndot products exponentiates relative similarities between a query and each key, quantified via low-entropy or\n\u201cspiky\u201d attention weight distributions (Fig. 2).\n4\nBERT-FT\n1 + ELU\nReLU\nPerformer\ncosFormer\nexp(t = 1)\nexp(t = 2)\nMatthew\u2019s correlation\n58.8\n28.1\n39.5\n24.7\n39.9\n45.9\n50.0\nTable 1: Finetuned-conversion performance of BERT finetuned on CoLA (BERT-FT), using prior linear\nattentions. With poor monotonicity (Fig. 3), prior methods fail to recover performance.\nLinear attention maps work by replacing the softmax with the normalized dot product of alternate feature\nmaps (Eq. 2). With existing feature maps, we find the resulting attention weights can result in much higher\nentropy or more uniform distributions. This is true even for methods designed to approximate the softmax\nunder mean-squared error bounds (Choromanski et al., 2020) (Performer, Fig. 2) or imposed locality (Qin\net al., 2022b) (cosFormer, Fig. 2). This uniformity in attention weights reduces the modeling capacity of\nlinear attentions leading to worse performance quality.\nMonotonicity over query-key dot products.\nThis property requires that the attention maps are mono-\ntonic over query-key dot products: when the dot product increases (decreases), the attention weight increases\n(decreases). In Fig. 3, we observe that while softmax attention exhibits this monotonicty (first subplot), the\nexisting linear attentions do not. We believe this can cause training issues after swapping attentions due to\nconflicting gradients between attentions and original model parameters. In Fig. 3, trying to upweight atten-\ntions by increasing product similarity can actually result in decreased attention weights. Later in Sec 3.2,\nwe find this corresponds to failing to recover original performance when converting finetuned Transformers.\n3.2\nExplaining the Linear Attention Performance Gap\nWe validate the two properties introduced above by showing that (1) lacking spikiness corresponds to signifi-\ncantly worse performance when training from scratch, and (2) lacking spikiness and monotonicity corresponds\nto failing to recover performance when converting finetuned models.\nTraining from scratch. We compare various Transformers\u2019 abilities to solve Associative Recall (AR) (Ba\net al., 2016), a next-token prediction task previously studied as a proxy for language modeling capability (Ols-\nson et al., 2022). AR tests how well a model can recall specific content in an input sequence, structured as\na list of key-value pairs which ends in a key (Table 12).\nFigure 4:\nAssociative recall performance\nstrongly corresponds to lower attention en-\ntropy; present in softmax attention but not\nprior linear variants.\nAs a control for evaluating our hypothesis, we also consider\na simple feature map designed to induce \u201cspikiness\u201d but not\nmonotonicity: \u03d5t(x) = exp(x \u00b7 t), which applies a temperature-\nt scaled exponential element-wise.\nIn Fig. 4, we observe a strong correspondence between low-\nentropy attention weights and AR accuracy.\nWhile softmax\nattention solves the AR task perfectly, prior linear attentions\nstruggle to achieve even 20% accuracy, at the same time obtain-\ning much larger attention weight entropies. As further support\nto our hypothesis, we see that while the exponential map \u03d51\nfails AR and produces similarly high entropy attention weights,\nincreasing spikiness with t = 2 actually solves the task.\nFinetuned-conversion.\nWe next compare how various\nlinear attentions perform at recovering original softmax atten-\ntion performance for finetuned-conversion. We adopt the pro-\ncedure in Kasai et al. (2021), which takes a Transformer already finetuned on a specific task, swaps the\nattention layers with a linear attention variant, and further finetunes the entire model on the same task.\nFor this setting, we evaluate with a BERT-base-uncased model (Devlin et al., 2018) finetuned on the\nCorpus of Linguistic Acceptability (CoLA) task (Warstadt et al., 2019), where the goal is to classify whether\na sentence is grammatically correct. We compare the performance of the original (softmax attention) BERT\nmodel1 with the linear attention converted models. In Table 1, we find that just as no linear attention\nsmoothly captures monotonicity over the trained model\u2019s query-key dot products, no linear attentions fully\nrecover the original finetuned BERT\u2019s Matthew\u2019s correlation of 58.8. This includes the spiky \u03d52 feature map\nwhich was sufficient in the training-from-scratch regime.\n1https://huggingface.co/JeremiahZ/bert-base-uncased-cola\n5\n4\nHedgehog: Expressive Linear Attention via Softmax Mimicry\nWe present Hedgehog, a simple, efficient, and expressive feature map trained to mimic softmax attention.\nHedgehog is predicated by (1) there existing linear attention approximations to the softmax that recover the\nspiky and monotonic properties of standard attention in practice, and (2) that we can efficiently compute\nsimilar approximations efficiently.\nIn Sec. 4.1, we motivate Hedgehog and show that (1) is possible by revisiting low-degree Taylor polyno-\nmials. We find that for linear attention, the Taylor exponential works as a surprisingly simple feature map,\nrecovering spikiness and monotonicity while matching standard Transformer performance. Unfortunately,\nwe also find it introduces its own issues, where the feature map results in large query and key dimensions\nand becomes inefficient to compute. In Sec. 4.2, to thus overcome these challenges, we propose and describe\nHedgehog, a trainable linear attention trained to mimic softmax attention. In Sec. 5.1, we show how this\nenables similar spiky and monotonic properties to the softmax and Taylor exponential attentions, while\nretaining past linear attentions\u2019 efficiency.\n4.1\nSimple Polynomial Approximations to Softmax Attention\nFrom our findings in Sec. 3, we seek an efficient linear alternative to the softmax which retains its spiky\nand monotonic properties. We first consider a simple potential approach: approximating the exponential in\nsoftmax by a low-degree Taylor polynomial (Keles et al., 2023; Banerjee et al., 2020).\nWhile in general, a high-quality approximation to the softmax should retain its spiky, monotonic, and\nperformant properties, we ground our investigation with two potential caveats for the Taylor polynomial.\nFirst, recall that feature maps for p-degree polynomial approximations can be computed in O(ndp) time\nand space for every query and key vector. Thus, while this is indeed subquadratic in sequence length, the\nquestion remains whether we can set p low enough to make the computation feasible while approximating\nexp reasonably. Second, as a general property of polynomials, the Taylor approximation only tracks its\noriginal function with low error in bounded regimes.\nSetup. To test the Taylor approximation, we use the second-degree exp approximation, and evaluate\non the prior train-from-scratch and finetuned-conversion settings (Sec. 3.2).\nWe implement the feature\nmap as exp(q\u22a4k) \u2248 \u03d5taylor(q)\u22a4\u03d5taylor(k), where \u03d5taylor(x) projects a d-dimensional query or key to O(d2)-\ndimensional features \u03d5taylor(x) =\nh\n1, x1, . . . , xd,\ni\n\u222a\nh\nxi \u00b7 xj | i, j \u2208 [d]\ni\n.\nPositive results. We find that the\n2nd-degree Taylor approximation retains both the spikiness and monotonic properties (Fig. 5), and this\ncorresponds to (near)-matching softmax attention performance (Table 2). We also note that here, the BERT\nquery-key dot products are bounded in regimes where the second-order Taylor series exp approximation\nmaintains monotonicity (Fig. 5). This suggests we can enable expressive linear attentions for training from\nscratch and finetuned-conversion.\nCaveats. Unfortunately, the 2nd-degree Taylor approximation is not efficient. Even with p = 2, the\nfeature map dimension is now d\u2032 = 1 + d + d2, resulting in O(nd3) attention complexity. As summarized\nin Table 2, this introduces an efficiency-effectiveness trade-off among functional attention approximations.\nThus, the question remains whether we can recover the expressivity and modeling quality of softmax while\nachieving similar O(nd2) scaling of past linear attentions.\nMethod\nComplexity\nSpiky?\nMono-\ntonic?\nTrain-from-\nscratch (acc)\nBERT-FT\n(MC)\nSoftmax\nO(n2d)\n\u2713\n\u2713\n100.0\n58.8\n1 + ELU\nO(nd2)\n\u2717\n\u2717\n17.0\n28.1\nPerformer\nO(nd\u20322)\n\u2717\n\u2717\n17.0\n24.7\nCosFormer\nO(nd2)\n\u2717\n\u2717\n17.0\n39.9\nTaylor Exp\nO(nd3)\n\u2713\n\u2713\n100.0\n58.4\nTable 2: Summary of feature maps compared to softmax,\nexhibiting an efficiency vs. expressivity tradeoff.\nTaylor Spikiness\nTaylor Monotonicity\nFigure 5: Taylor approximation recovers spik-\niness and monotonicity\n6\n4.2\nLearnable Linear Attentions for Mimicking Softmax\nOur key insight is that rather than rely on fixed functional form that captures our spiky and monotonic\nproperties, we can learn linear attention feature maps that do so. For each attention block, we propose\nfeature maps as trainable single-layer MLPs, which is similar to prior work (Kasai et al., 2021) and acts\nsimilarly to an adapter (Houlsby et al., 2019) inserted after the query and key projections in Transformer\nattention layers (Fig. 1). However, unlike prior work, we explicitly train these feature maps such that the\nattention layers mimic the properties of softmax attention. We describe these two core components below,\nand validate these design choices in Sec. 5.1.\nSpiky MLP feature map.\nRecall the kernel based linear attention paradigm from Sec. 2, where a feature\nmap \u03d5 : Rd 7\u2192 Rd\u2032 is applied to both queries and keys to compute causal self-attention outputs using equa-\ntion 2. However, unlike prior work that sticks to a pre-specified function as a feature map, we make the feature\nmap a trainable MLP. In particular, for the single-head attention setting, we compute \u03d5mlp(qi)\u22a4\u03d5mlp(kj)\nwith a simple one-layer MLP as \u03d5mlp(x) = \u03a6(W \u22a4x + b) where the matrix W \u2208 Rd\u00d7d\u2032 and the bias b \u2208 Rd\u2032\nare learned, and \u03a6 is an activation function. To induce spikiness, we set \u03a6 as the element-wise exponential\nfunction studied in Sec. 3.2, resulting in\n\u03d5mlp(x) =\nh\nexp(w\u22a4\n1 x + b), . . . , exp(w\u22a4\nd x + b)\ni\n(3)\nAttention weight distillation loss.\nTo learn a softmax approximation, we train \u03d5mlp to minimize the\ncross-entropy loss between the computed linear attention weights and those that would have been computed\nvia softmax attention. For query qi and keys {kj}n\n1, we compute the sample losses as\nLi = \u2212\ni\nX\nj=1\nexp(q\u22a4\ni kj)\nPi\nm=1 exp(q\u22a4\ni km)\nlog\n\u03d5mlp(qi)\u22a4\u03d5mlp(kj)\nPi\nm=1 \u03d5mlp(qi)\u22a4\u03d5mlp(kj)\n(4)\nFor training Hedgehog attentions in multi-layer and multi-head attention Transformers, we apply a\nseparate MLP to each head and each layer, and use the same \u03d5mlp for the queries and keys. We include\nfurther implementation details and pseudocode in Appendix A.\n5\nExperiments\nIn experiments, we evaluate whether Hedgehog recovers softmax attention expressivity while retaining linear\nattention efficiency (Sec. 5.1), and how this improves modeling quality in training-from-scratch (Sec. 5.2),\nfinetuned-conversion (Sec. 5.3), and pretrained-conversion regimes (Sec. 5.4).\n5.1\nBenchmarking Hedgehog for Expressivity and Efficiency\nBefore evaluating Hedgehog on downstream tasks, we aim to validate Hedgehog\u2019s design choices for efficiency\nand expressivity. We address: (1) Do Hedgehog\u2019s spiky feature map and distillation loss recover the spiky and\nmonotonic properties of softmax attention on the prior associative recall and BERT CoLA tasks? (2) Does\nMethod\nComplexity\nAR\nBERT-FT\nSoftmax\nO(n2d)\n100.0\n58.8\nTaylor Exp\nO(nd3)\n100.0\n58.4\nHedgehog\nO(nd2)\n100.0\n59.2\nTable 3: Hedgehog matches performance on associative\nrecall (AR) and BERT-finetuned conversion (BERT-FT)\nwith prior best approaches, while achieving better time\nand space complexity.\nFigure 6: Hedgehog linear scaling in wall-clock\ntime (left) and memory (right). Unlike the Tay-\nlor approx., Hedgehog inference gets real-world\ngains over FlashAttention.\n7\nFigure 7: Compared to prior linear attentions, trained Hedgehog layers (2nd left) produce attention weights\nclosely tracking softmax (left), with greater fidelity with both components (vs. Fig. 8).\nFigure 8:\nHedgehog ab-\nlated attention weights.\nDataset\nHH\n(CoLA)\nHH\n(WT-103)\nT2R-HH\n(CoLA)\nHH\n(No Train)\n1 +\nELU\nPerformer\nCosFormer\nCoLA\n0.172\n0.352\n0.191\n0.694\n1.223\n1.293\n1.196\nMRPC\n0.663\n0.485\n1.128\n1.250\n2.165\n2.234\n1.982\nMNLI\n0.345\n0.382\n0.613\n0.851\n1.51\n1.581\n1.338\nQNLI\n0.671\n0.444\n1.126\n1.139\n1.968\n2.069\n1.817\nTable 4: We find Hedgehog feature maps trained via distillation on CoLA or\nWikiText-103 generalize to new GLUE data, better matching softmax than\nprior linear attentions or ablations (reporting KL div.).\nFigure 9: Hedgehog trained on CoLA and WT-103 recover\nsoftmax attentions on MRPC data.\nSeq. Len\n256\n1024\n2048\n4096\nCoLA KL\n0.182\n0.187\n0.190\n0.181\nTable 5: Hedgehog attention maintains fi-\ndelity with softmax attention over context\nlengths for BERT-FT on CoLA.\nHedgehog achieve improved efficiency over softmax attention? (3) For conversion, do the learned attention\nweights actually match those of \u201cground-truth\u201d softmax attention? Once learned, does this transfer to longer\ncontexts and different tasks?\nRecovering softmax spiky and monotonic properties. We test Hedgehog in the same train-from-\nscratch associative recall (AR) and finetuned-conversion of BERT on CoLA settings in Sec. 3.2. For training-\nfrom-scratch on AR, we do not use the distillation loss, and train the model end-to-end with next-token-\nprediction after inserting the learnable MLPs. In Table. 3, we find that Hedgehog achieves both favorable\ncomplexity and modeling for train-from-scratch and finetuned-conversion.\nThis corresponds respectively\nwith the spiky (Fig. 2) and monotonic (Fig. 3) properties noted prior.\nRecovering linear attention efficiency. We next find Hedgehog\u2019s O(nd2) scaling in compute and\nmemory can lead to real-world efficiency gains. We benchmark inference in wall-clock time and memory\nusage for one attention layer with 12 heads and head dimension = 64 on sequences up to n = 32K tokens\nlong (Fig. 6). Hedgehog achieves near 6x faster inference and similar memory to FlashAttention (Dao et al.,\n2022) (linear in memory but quadratic in time). Meanwhile, the Taylor approximation, while O(n), gets\nsignificantly larger memory and slower speed due to the extra d.\nRecovering softmax attention weights. We next study the combination of Hedgehog\u2019s feature map\nand distillation loss for matching softmax attention weights. Beyond recovering the spiky and monotonic\nproperties, learning to exactly match the weights can be particularly effective for converting or \u201cdistilling\u201d\npretrained quadratic Transformers into linear variants. For evaluation, we visualize the attention weights\nfor different linear attentions in our BERT-FT CoLA setting (Fig. 7). We find Hedgehog recovers linear\nattention weights that match softmax\u2019s with much higher fidelity.\nTo further understand the contribution of Hedgehog\u2019s (1) spiky MLP and (2) distillation loss in Sec. 4.2,\nwe visualize ablated attention weights by (1) using the distillation loss with the ReLU feature map used in\nTransformer-to-RNN (T2R-HH) (Kasai et al. (2021)), and (2) using untrained MLPs, replacing the trainable\nweights with an identity function (HH No Train). We find that distillation training is necessary to recover\nattention weights, and that the spiky MLP is also helpful for matching attentions (later supported by\nimproved Transformer conversion in Sec. 5.3).\n8\nModel\nListOps\nText\nRetrieval\nImage\nPathfinder\nAverage\nTransformer\n36.37\n64.27\n57.46\n42.44\n71.40\n54.39\nLocal Att\n15.82\n52.98\n53.39\n41.46\n66.63\n46.06\nLinear Trans.\n16.13\n65.90\n53.09\n42.34\n75.30\n50.55\nReformer\n37.27\n56.10\n53.40\n38.07\n68.50\n50.67\nSparse Trans.\n17.07\n63.58\n59.59\n44.24\n71.71\n51.24\nSinkhorn Trans.\n33.67\n61.20\n53.83\n41.23\n67.45\n51.29\nLinformer\n35.70\n53.94\n52.27\n38.56\n76.34\n51.36\nPerformer\n18.01\n65.40\n53.82\n42.77\n77.05\n51.41\nSynthesizer\n36.99\n61.68\n54.67\n41.61\n69.45\n52.88\nLongformer\n35.63\n62.85\n56.89\n42.22\n69.71\n53.46\nBigBird\n36.05\n64.02\n59.29\n40.83\n74.87\n55.01\nNystrmformer\u2020\n37.15\n65.52\n79.56\n41.58\n70.94\n58.95\ncosFormer\u2020\n37.90\n63.41\n61.36\n43.17\n70.33\n55.23\nSkyformer\u2020\n39.25\n64.70\n82.06\n40.77\n70.73\n59.50\nHedgehog\n37.15\n64.60\n82.24\n40.15\n74.16\n59.66\nTable 6: Training-from-scratch on LRA. Hedgehog achieves best avg. acc. (%) across most competitive\nTransformers (full results in Table 13, trends hold).\n\u2020 indicates results reported from original works. All\nothers reported from the official LRA benchmark (Tay et al., 2021). Best, 2nd-best.\nModel\nTransformer\nPerformer\nReformer\nAFT\n(1 + ELU)\nHedgehog\nPerplexity\n18.6\n26.8\n25.6\n28.2\n25.6\n20.8\nTable 7: Training-from-scratch on WikiText-103. Among 125M decoder-only models, Hedgehog significantly\ncloses the gap between standard Transformers and prior linear attention maps by 68.6%.\nGeneralization to new data and longer contexts. Finally, we investigate the generality of learned\nHedgehog feature maps. We show Hedgehog attentions learned over specific data and context lengths can\nstill better match softmax attention weights for new data and sequence lengths than prior linear attentions.\nWe distill attentions for BERT models using CoLA or WikiText-103 (WT-103) samples, and report attention\nweights compared to softmax attention on three other GLUE tasks: qualitatively (Fig. 9) and quantitatively\nvia KL divergence w.r.t. the \u201cground-truth\u201d softmax weights (Table 4). We include additional visualizations\nand comparisons Appendix 5.\nIn Table 9, we further show that Hedgehog attention matching remains consistent across longer contexts.\nPost-distillation on CoLA samples, we concatenate CoLA samples into sequences 256 to 4096 tokens long\n(up to 8x the default 512 context length). We then compute attention weights using softmax and learned\nHedgehog feature maps, and find that their KL divergence remains consistent.\n5.2\nLearning Sequence Modeling From Scratch\nWe evaluate Hedgehog Transformers trained from scratch on the popular LRA sequence classification and\nWikiText-103 language modeling benchmarks. For training from scratch, we initialize MLPs as identity\nmatrices for Hedgehog feature maps, and train the entire models end-to-end with the task-specific loss. We\nfind Hedgehog achieves best average accuracy for both tasks among linear attentions (Table 6, 7). For LRA,\nwhile non-Transformer models are now state-of-the-art (Gu et al., 2021), our work focuses on approximating\nattention, so we compare with competitive subquadratic Transformers. We adopt the same hyperparameter\nsettings as the official benchmark (Tay et al., 2021). On WikiText-103, we adopt the setting in Fu et al.\n(2023), evaluating a 125M decoder-only Transformer on perplexity over 1024 tokens. Hedgehog significantly\ncloses the gap by up to 6 PPL.\n9\n5.3\nFinetuned Conversion of Quadratic to Linear Transformers\nFor the finetuned Transformer conversion regime, we evaluate performance recovery for BERT-base models\nfinetuned on GLUE, and ViT-B/16 models trained on ImageNet-1K. For both settings, we first swap atten-\ntions and train via our distillation loss (Sec. 4.2). We then finetune the converted BERT models on their\noriginal tasks as in Transformer-to-RNN (T2R) (Kasai et al., 2021).\nFor BERT, we compare Hedgehog to T2R in Table 8, and find that in contrast, Hedgehog conversion\nrecovers near-100% of the original softmax attention performance. To further test Hedgehog\u2019s feature map\nand attention distillation, we also compare against an ablation that trains the T2R feature map with our\ndistillation loss (T2R-HH). We find that training to mimic softmax attentions boosts performance of T2R,\nsuggesting that attention weight distillation may be a general step to improving linear attention feature\nmaps. However, Hedgehog\u2019s exponential still leads to superior performance. We find similar results for\nViT-B/16, suggesting Hedgehog can also apply to other modalities.\n5.4\nPretrained Conversion for Subquadratic Task Transfer\nWe finally evaluate Hedgehog for converting pretrained Transformers into linear Transformers. We consider\ntwo settings: (1) To benchmark Hedgehog and the pretrained-conversion regime for subquadratic sequence\nmodeling, we use the same WT-103 evaluation in Sec. 5.2 for converting 125M-parameter GPT-2. (2) As\nan early application for Hedgehog on larger models, we convert Llama-2 7B (Touvron et al., 2023) before\nfinetuning with low-rank adapters (LoRA) (Hu et al., 2021) on SAMSum summarization (Gliwa et al., 2019).\nWe include further training details in Appendix. B.5.\nTo most directly measure pretrained-conversion quality, for both settings we compare against T2R. For\nGPT-2, we find Hedgehog both outperforms T2R, and further outperforms modern subquadratic sequence\nmodels such as H3 (Fu et al., 2023) and Hyena (Poli et al., 2023) (Table 10). Although not directly comparable\ndue to pretraining, we also compare with zero-shot and finetuned GPT-2 for reference. While Hedgehog is\n1 PPL off the fully quadratic finetuned GPT-2, it significantly improves over zero-shot while being linear to\ntrain. We finally apply Hedgehog for Llama-2 conversion, where Hedgehog enables linear attention Llamas\nthat train via LoRA (see Appendix C.3 for sample generations).\nMethod\nCoLA\nSST2\nMRPC\nSTS-B\nQQP\nMNLI\nQNLI\nRTE\n(%) Recover\nBERT-FT\n58.8\n93.2\n90.2\n88.8\n91.0\n84.7\n91.3\n68.2\n100.0\nT2R\n43.6\n87.7\n83.0\n78.6\n86.7\n78.9\n84.6\n54.1\n88.9\nT2R-HH\n56.9\n90.9\n89.1\n77.7\n90.0\n77.4\n84.5\n56.3\n93.5\nHedgehog\n59.2\n92.6\n90.1\n87.4\n91.0\n82.6\n89.6\n69.3\n99.3\nTable 8: Finetuned-conversion evaluation. Hedgehog recovers 99.3% of original\nfinetuned BERT (BERT-FT) GLUE performance.\nTop-1\nAcc. %\nViT-B/16\n80.3\nT2R-HH\n77.0\nHedgehog\n79.5\nTable\n9:\nHedgehog\nachieve 99% ViT acc.\nMethod\nGPT-2\nGPT-2 FT\nHybrid H3\nHyena\nT2R-GPT-2\nHH-GPT-2\nPPL\n28.0\n15.8\n18.5\n18.5\n19.4\n16.7\nTable 10: Pretrained-conversion for 125M GPT-2 on WT-103 lang. mod-\neling. While finetuned GPT-2 gets lowest PPL, among subquadratic mod-\nels Hedgehog significantly outperforms by 1.8 PPL.\nLlama-2\nR1 / R2 / RL\nSoftmax (Zero-shot)\n19.3 / 6.8 / 14.9\nSoftmax (LoRA)\n51.1 / 27.6 / 43.5\nT2R (LoRA)\n2.8 / 0.0 / 2.6\nHedgehog (LoRA)\n47.4 / 23.4 / 39.1\nTable 11:\nHedgehog Llama-2\nconversion (ROUGE).\n6\nConclusion\nWe present Hedgehog, a learnable linear attention to mimic softmax attention. This enables training lin-\near attention models from scratch and converting existing Transformers into linear attention variants. To\nmotivate Hedgehog we study why prior linear attentions underperform softmax attention, and identify two\nmissing properties: (1) the ability to capture low entropy or spiky attention maps and (2) to be monotonic\nwith respect to the underlying query-key dot products. We find training to match softmax attentions results\nin recovering many of its expressive properties, and that Hedgehog leads to competitive performance with\nsoftmax-based attention in training from scratch, finetuned-conversion, and pretrained conversion regimes.\n10\nAcknowledgements\nWe thank Armin Thomas, Gordon Downs, Krista Opsahl-Ong, Pun Waiwitlikhit, Schwinn Saereesitthipitak,\nDan Fu, Simran Arora, Sabri Eyuboglu, and Tri Dao for helpful discussions on linear attention and paper\nfeedback, and Dan Fu for prior versions of the pseudocode formatting in the appendix.\nWe gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos.\nCCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US DEVCOM\nARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266\n(Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geom-\netry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft,\nNEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google\nCloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science\nInitiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare. The U.S.\nGovernment is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding\nany copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in\nthis material are those of the authors and do not necessarily reflect the views, policies, or endorsements,\neither expressed or implied, of NIH, ONR, or the U.S. Government.\nReferences\nJosh Alman and Zhao Song. Fast attention requires bounded entries. arXiv preprint arXiv:2302.13214, 2023.\nJimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast weights to\nattend to the recent past. Advances in neural information processing systems, 29, 2016.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to\nalign and translate. arXiv preprint arXiv:1409.0473, 2014.\nKunal Banerjee, Vishak C., Rishi Raj Gupta, Kartik Vyas, Anushree H., and Biswajit Mishra. Exploring\nalternatives to softmax function. ArXiv, abs/2011.11538, 2020. URL https://api.semanticscholar.\norg/CorpusID:227127574.\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan,\nMohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite\nfor analyzing large language models across training and scaling. In International Conference on Machine\nLearning, pp. 2397\u20132430. PMLR, 2023.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin.\nEmerging properties in self-supervised vision transformers.\nIn Proceedings of the IEEE/CVF\ninternational conference on computer vision, pp. 9650\u20139660, 2021.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large\nlanguage models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.\nYifan Chen, Qi Zeng, Heng Ji, and Yun Yang. Skyformer: Remodel self-attention with gaussian kernel and\nnystr\\\u201dom method. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in\nNeural Information Processing Systems, 2021. URL https://openreview.net/forum?id=pZCYG7gjkKz.\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos,\nPeter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.\narXiv preprint arXiv:2009.14794, 2020.\nKrzysztof Choromanski, Haoxian Chen, Han Lin, Yuanzhe Ma, Arijit Sehanobish, Deepali Jain, Michael S\nRyoo, Jake Varley, Andy Zeng, Valerii Likhosherstov, et al. Hybrid random features. arXiv preprint\narXiv:2110.04367, 2021.\n11\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-\nXL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics, pp. 2978\u20132988, Florence, Italy, July 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. Flashattention: Fast and memory-efficient\nexact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359,\n2022.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.\nQlora: Efficient finetuning of\nquantized llms. arXiv preprint arXiv:2305.14314, 2023.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\nDaniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry\nhungry hippos: Towards language modeling with state space models.\nIn The Eleventh International\nConference on Learning Representations, 2023. URL https://openreview.net/forum?id=COZDy0WYGg.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: A human-annotated\ndialogue dataset for abstractive summarization. In Lu Wang, Jackie Chi Kit Cheung, Giuseppe Carenini,\nand Fei Liu (eds.), Proceedings of the 2nd Workshop on New Frontiers in Summarization, pp. 70\u201379, Hong\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5409.\nURL https://aclanthology.org/D19-5409.\nAlbert Gu, Karan Goel, and Christopher R\u00b4e. Efficiently modeling long sequences with structured state\nspaces. arXiv preprint arXiv:2111.00396, 2021.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International\nConference on Machine Learning, pp. 2790\u20132799. PMLR, 2019.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021.\nKazuki Irie, Imanol Schlag, R\u00b4obert Csord\u00b4as, and J\u00a8urgen Schmidhuber. Going beyond linear transformers\nwith recurrent fast weight programmers.\nIn A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman\nVaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.\nnet/forum?id=ot2ORiBqTa1.\nJungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu\nChen, and Noah A. Smith. Finetuning pretrained transformers into RNNs. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language Processing, pp. 10630\u201310643, Online and Punta\nCana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/\nv1/2021.emnlp-main.830. URL https://aclanthology.org/2021.emnlp-main.830.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00b8cois Fleuret. Transformers are rnns: Fast\nautoregressive transformers with linear attention. In International conference on machine learning, pp.\n5156\u20135165. PMLR, 2020.\nFeyza Duman Keles, Pruthuvi Mahesakya Wijewardena, and Chinmay Hegde. On the computational com-\nplexity of self-attention. In International Conference on Algorithmic Learning Theory, pp. 597\u2013619. PMLR,\n2023.\n12\nHuanru Henry Mao. Fine-tuning pre-trained transformers into decaying fast weights. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing, pp. 10236\u201310242, Abu Dhabi,\nUnited Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/\n2022.emnlp-main.697. URL https://aclanthology.org/2022.emnlp-main.697.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In\nInternational Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=\nByj72udxe.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint\narXiv:2209.11895, 2022.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. Random\nfeature attention. arXiv preprint arXiv:2103.02143, 2021.\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio,\nStefano Ermon, and Christopher R\u00b4e. Hyena hierarchy: Towards larger convolutional language models.\narXiv preprint arXiv:2302.10866, 2023.\nZhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The\ndevil in linear transformer.\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 7025\u20137041, Abu Dhabi, United Arab Emirates, December 2022a. Association for\nComputational Linguistics. doi: 10.18653/v1/2022.emnlp-main.473. URL https://aclanthology.org/\n2022.emnlp-main.473.\nZhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and\nYiran Zhong. cosformer: Rethinking softmax in attention. arXiv preprint arXiv:2202.08791, 2022b.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019. URL https://api.semanticscholar.org/CorpusID:160025533.\nAli Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller,\nY. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems, volume 20. Cur-\nran Associates, Inc., 2007. URL https://proceedings.neurips.cc/paper_files/paper/2007/file/\n013a006f03dbc5392effeb8f18fda755-Paper.pdf.\nImanol Schlag, Kazuki Irie, and J\u00a8urgen Schmidhuber. Linear transformers are secretly fast weight program-\nmers. In International Conference on Machine Learning, pp. 9355\u20139366. PMLR, 2021.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In\nInternational Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=\nqVyeW-grC2k.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288, 2023.\nYao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov.\nTransformer dissection: An unified understanding for transformer\u2019s attention via the lens of kernel. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4344\u20134353, Hong\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1443.\nURL https://aclanthology.org/D19-1443.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,  Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,\n2017.\n13\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. Trans-\nactions of the Association for Computational Linguistics, 7:625\u2013641, 2019.\nYunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas\nSingh. Nystr\u00a8omformer: A nystr\u00a8om-based algorithm for approximating self-attention. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 35, pp. 14138\u201314148, 2021.\nLin Zheng, Jianbo Yuan, Chong Wang, and Lingpeng Kong. Efficient attention via control variates. In The\nEleventh International Conference on Learning Representations, 2023. URL https://openreview.net/\nforum?id=G-uNfHKrj46.\n14\nA\nHedgehog implementation details\nWe provide further details on the Hedgehog feature map and attention weight distillation training.\nA.1\nMechanics for Hedgehog feature map\nTo improve Hedgehog performance in practice, we explored variations along two additional criteria for\nnumerical stability and improved expressivity.\nNumerical stability In practice, we find that computing \u03a6 as the softmax applied over the MLP output\ndimension also seems to work but with better stability. In this case, we expand Eq. 3 as\n\u03d5mlp(x) =\nh\nexp(w\u22a4\n1 x)\nPd\ni=1 exp(w\u22a4\ni x)\n, . . . ,\nexp(w\u22a4\nd x)\nPd\ni=1 exp(w\u22a4\ni x)\ni\n(5)\n(also performing better than dividing each element by the max over {exp(w\u22a4\ni x + b)}d\ni=1)\nNegation mapping. To better compute dot products as a similarity measure between queries and keys,\nin practice we also set \u03a6 as a mapping from Rd 7\u2192 R2d, e.g., via\n\u03d5mlp(x) =\nh\nexp(w\u22a4\n1 x + b), . . . , exp(w\u22a4\nd x + b), exp(\u2212w\u22a4\n1 x \u2212 b), . . . , exp(\u2212w\u22a4\nd x \u2212 b)\ni\n(6)\nwhere the additional negation mapping in R2d intuitively lets us better factor in negative dimensionalities,\nwhich prior linear attention feature maps like ReLU ignore. While this results in a larger feature dimension,\nit only scales by a fixed constant 2 such that the overall time and space complexity for Hedgehog linear\nattention is still O(nd2).\nWe further find that in practice, this still accomplishes favorable scaling and\nmuch faster inference with smaller memory than the Taylor exponential discussed in Sec. 4.1 (see Fig. 6 for\nreal-world wall-clock time and memory savings).\nA.2\nHedgehog feature map and model architecture\nWe apply Hedgehog feature maps for each head and layer individually in a standard Transformer architec-\nture, where the addition of head-specific MLPs is akin to inserting \u201cadapters\u201d (Houlsby et al., 2019) after\nevery query and key projection. Each MLP is a single linear layer with input and output dimensions equal\nto the base Transformer\u2019s head dimension. Pytorch-like code is given below.\n1 import\ntorch\n2 import\ntorch.nn as nn\n3\n4 class\nHedgehogFeatureMap (nn.Module):\n5\ndef\n__init__(self , head_dim: int , activation : str = \u2019exp \u2019):\n6\nsuper ().__init__ ()\n7\n# Trainable\nmap\n8\nself.layer = nn.Linear(head_dim , head_dim)\n9\nself. init_weights_ ()\n10\n11\ndef\nself. init_weights_ (self):\n12\n\"\"\" Initialize\ntrainable\nmap as\nidentity \"\"\"\n13\nnn.init.eye_(self.layer.weight)\n14\nnn.init.zeros_(self.layer.bias)\n15\n16\ndef\nforward(self , x: torch.Tensor):\n17\nx = self.layer(x)\n# shape b, h, l, d\n18\nreturn\ntorch.cat ([ torch.exp(x), torch.exp(-x)], dim =-1)\nA.3\nHedgehog Distillation and Finetuning Implementation Details\nWe include additional details for training Hedgehog layers to obtain linear attention Transformers. These\nfall under two categories: (1) training-from-scratch, and (2) finetuned / pretrained conversion.\n15\n1. Training-from-scratch. When training Hedgehog Transformers from scratch, we insert a Hedgehog\nMLP for each query and key projection of the randomly initialized Transformer (e.g., for each head of a\nmulti-head attention layer, and for all such layers). We then train the Hedgehog MLPS jointly with all\nother model parameters end-to-end with a single objective function, e.g., cross-entropy loss on next-token\nprediction when training models for language modeling.\n2.\nFinetuned / pretrained conversion. For both these regimes, we carry out training as a two\nstage process. Like training-from-scratch, we initially insert Hedgehog MLPs for query and key projections.\nFollowing this, we proceed in two stages:\n1. Attention distillation. We first freeze the Transformer\u2019s original weights and specifically train the\nHedgehog MLPs, such that the resulting linear attention weights match those produced via softmax\nattention over the same query and key tensors. For each head, we conceptually follow Listing 1 below\nto compute a soft cross-entropy or KL-divergence between the \u201cpredicted\u201d linear attention weights and\n\u201cground-truth\u201d softmax attention weights. We compute these losses for each attention head and layer\nafter one forward pass of the entire model, using data samples from the target task. We find it sufficient to\nuse one optimizer for joint training over all Hedgehog layers in parallel, using the sum of each individual\nattention head distillation loss as the final criterion.\nThis makes training simple and comparable to\na standard training loop; we further provide code\u2020 in Listing 2 to do so with popular APIs such as\nHuggingFace Transformers2.\n2. Original parameter finetuning. Following attention distillation, we simply unfreeze all model weights\nand train with a standard task-specific loss function. We find we can also keep certain layers frozen or\ntrain with parameter-efficient finetuning such as low-rank adaptation (Hu et al., 2021); we explore this in\nSec. 5.4 with Llama-2 models.\n1 # Hedgehog\ndistillation\nloss\nfor one\nattention\nhead\n2\n3 def\nsoftmax_attn (q: torch.Tensor , k: torch.Tensor):\n4\n\"\"\" Get\nsoftmax\nattention\nweights\n-> Assume q, k are\nboth\nshape (b, h, l, d)\"\"\"\n5\nscale = q.shape [-1] ** 0.5\n6\nqk = torch.einsum(\u2019bhmd ,bhnd ->bhmn \u2019, q, k) / scale\n7\nreturn\ntorch.softmax(qk , dim=-1)\n8\n9 def\nquadratic_linear_attn (q: torch.Tensor , k: torch.Tensor):\n10\n\"\"\"\n11\nGet\nlinear\nattention\nweights\n12\n-> Assume q, k are\nboth\nshape (b, h, l, d), and\nfeature\nmaps\nalready\napplied\n13\n\"\"\"\n14\nqk = torch.einsum(\u2019bhmd ,bhnd ->bhmn \u2019, q, k)\n15\nreturn\nqk / qk.sum(dim=-1, keepdim=True)\n16\n17 def\ncompute_hedgehog_loss (q: torch.Tensor ,\n18\nk: torch.Tensor ,\n19\nhh_mlp_q: HedgehogFeatureMap ,\n20\nhh_mlp_k: HedgehogFeatureMap ):\n21\n\"\"\"\n22\nCompute\nthe\nattention\ndistillation\nloss\n23\n-> Assume\n\u2018soft_label_cross_entropy \u2018 is\nimplemented\n24\n( alternatively\nuse KL\ndivergence )\n25\n-> Assume q and k are the\nqueries\nand\nkeys of a\n26\npretrained\nTransformer ,\n27\ne.g., via q = self.q_proj( hidden_states )\n28\n\"\"\"\n29\ntrue_attn = softmax_attn (q, k)\n30\npred_attn = quadratic_linear_attn (hh_mlp_q(q), hh_mlp_k(k))\n31\nreturn\nsoft_label_cross_entropy (pred_attn , true_attn)\n32\nListing 1: Hedgehog distillation loss for one attention head\n2https://huggingface.co/docs/transformers/index\n16\n1 # Hedgehog\nAttention\nclass\nfor\neasy\nattention\ndistillation\n2\n3 class\nHedgehogAttention (nn.Module):\n4\n\"\"\"\n5\nSample\ncode\nfor\nHedgehogAttention , following\nHuggingFace\nAPI\n6\n\"\"\"\n7\ndef\n__init__(self , base_attn , training = True):\n8\nself.base_attn = base_attn\n# e.g., LlamaAttention\n9\n10\n# Trainable\nfeature\nmaps\n11\nself.mlp_q = HedgehogFeatureMap (base_attn.head_dim)\n12\nself.mlp_k = HedgehogFeatureMap (base_attn.head_dim)\n13\n14\n# Freeze\noriginal\nattention\nparameters\n15\nfor p in self.base_attn.parameters ():\n16\np. requires_grad = False\n17\n18\nself.q_proj = self.base_attn.q_proj\n19\nself.k_proj = self.base_attn.k_proj\n20\n21\n# Whether\nwe train\nattentions\nor not\n22\nself.training = training\n23\n24\ndef\nforward(self ,\n25\nhidden_states : torch.Tensor ,\n26\noutput_attentions : bool = True ,\n27\n** base_kwargs: any):\n28\n29\nif self.training:\n30\n# Compute\nground -truth\nattention\nweights\n31\noutputs , true_attns = self.base_attn(\n32\nhidden_states =hidden_states ,\n33\noutput_attentions =True ,\n34\n** base_kwargs)\n35\n36\n# Compute\nHedghog\nfeature\nmaps\n37\nq = self.mlp_q(self.q_proj( hidden_states ))\n38\nk = self.mlp_k(self.k_proj( hidden_states ))\n39\n40\npred_attns = quadratic_linear_attn (q, k)\n41\n42\nif\noutput_attentions :\n# Hook\nfor\nattentions\n43\nreturn\noutputs , (pred_attns , true_attns)\n44\n45\n# ... End\nrelevant\nListing 2: Hedgehog Attention class for easy attention distillation.\n\u2020In practice, to train all Hedgehog layers easily in a joint end-to-end fashion, we make use of popular\npretrained Transformer APIs such as those in the HuggingFace transformers library.\nWe implement a\nHedgehog equivalent of the base Transformers\u2019 attention class, which (1) abstracts away the Transformer-\nspecific attention computation and (2) lets us hook attention weights calculated at each layer to the model\u2019s\nfinal outputs, e.g., via output attentions = True keyword args.\nWe can subsequently substitute each\nattention layer with the \u201cHedgehogAttention\u201d equivalent, and train via a simple loop over the data. We\npresent Pytorch-like code in Listing 3.\n17\n1 # End -to -end\njoint\nattention\ndistillation\n2\n3 from\ntransformers\nimport\nAutoModel\n4\n5 # Load\nbase\nmodel\n6 base_model = AutoModel. from_pretrained (...)\n7\n8 # Freeze\noriginal\nparameters\n9 for p in\nbase_model: p. requires_grad = False\n10\n11 # Convert\nattentions\nfor all\nlayers\n12 for\nlayer in\nbase_model:\n13\nbase_model.attn = HedgehogAttention (base_model .attn)\n14\n15 # Define\nsingle\noptimizer\nfor\ntraining\nall\nfeature\nmaps\n16 optim = optimizer(base_model.parameters ())\n17\n18 # Train\nHedgehog\nfeature\nmaps\n19 for\ndata in\ndataloader:\n20\n21\n# Compute\noutputs\nand\nhook to\nattentions\n22\noutputs = base_model (**data , output_attentions =True)\n23\noutputs = outputs.get(\u2019attentions \u2019)\n24\n25\ntotal_loss = 0\n26\nfor\nattns in\nenumerate(outputs):\n# attentions\nfor\neach\nlayer\n27\npred_attn , true_attn = attns\n28\ntotal_loss\n+=\nsoft_label_cross_etnropy (pred_attn , true_attn)\n29\n30\nloss.backward ()\n# Jointly\noptimize\nall\nfeature\nmaps\n31\noptim.step ()\nListing 3: End-to-end joint attention distillation.\nB\nDeferred experimental details\nB.1\nAssociative recall analysis (Section 3.2)\nIn Sec. 3.2, we compare various Transformers\u2019 abilities to solve Associative Recall (AR) (Ba et al., 2016),\na next-token prediction task previously studied as a proxy for language modeling capability (Olsson et al.,\n2022). AR tests how well a model can recall specific content in an input sequence, structured as a list of\nkey-value pairs which ends in a key Table 12.\nInput Sequence\nNext\nVocab\nSeq.\nToken\nSize\nLength\nc 9 k 8 j 3 ... f 1 c\n9\n40\n128\nTable 12: Associative recall task. Example from Ba et al. (2016).\nDataset details.\nTo understand the effects of more uniform attention weightings, we evaluate with 40\npossible tokens and 128 token-long-sequences, such that models must recall pairings that only occur three\ntimes on average in-context. We generate 10,000 training samples following the patterns described in Ta-\nble 12, and evaluate on 2000 newly-generated test samples (again using the same associative recall structure,\nbut with different token associations).\n18\nArchitecture details.\nFor all experiements, we use a four layer Transformer with four heads-per-layer,\nhead dimension = 64, and rotary embeddings. This is similar to modern model families such as Pythia (Bi-\nderman et al., 2023) and LLaMA / Llama-2 (Touvron et al., 2023). We keep all parts consistent except for\nthe multi-head attention, comparing popular linear attentions (c.f., Fig. 2).\nTraining details.\nFor fair comparison to evaluate just the feature map / modeling architecture, we train\nall models by sweeping learning rate \u2208 {1e-2, 1e-4}, weight decay \u2208 {0, 5e-4}, and batch size \u2208 {8, 32} with\nAdamW optimizer. We train up to 100 epochs with early stopping (explicitly stopping training if validation\nloss stops decreasing after 10 epochs).\nB.2\nBERT-base finetuned on CoLA conversion (Section 3.2)\nTraining details.\nFor our finetuned-conversion analysis, we replace the attentions of a finetuned BERT-\nbase-uncased model available on the HuggingFace model hub3. We train with batch size 8, learning rate\n1e-5, zero weight decay, AdamW optimizer, and up to 10 epochs with early stopping.\nB.3\nHedgehog training from scratch (Section 5.2)\nLRA training and model details.\nOn LRA, for fair comparison we implement Hedgehog in the existing\nPyTorch implementation provided by Xiong et al. (2021), deferring to the same model configurations and\nhyperparameters used in the original repository (Tay et al., 2021).\nWikiText-103 training and model details.\nFor WikiText-103, we train a 125M parameter GPT-2 style\nTransformer with learning rate 6e-4, weight decay 0.01, and AdamW optimizer. For close comparison, we\nfollow the architectural details of GPT-2 125M, and use a 12 layer decoder-only network with 12 heads, head\ndimension = 64, hidden dimension 768, and MLP dimension 3072.\nB.4\nHedgehog finetuned conversion (Section 5.3)\nRecovering finetuned BERT performance on GLUE tasks.\nFor finetuned conversion, we first con-\nduct Hedgehog attention distillation by training attention layers up to five epochs with early stopping based\non validation loss. We train with learning rate 1e-2, weight decay 0, AdamW optimizer. We follow the same\nprocedure for the Transformer-to-RNN (T2R) (Kasai et al., 2021) ablation. For regulard T2R and subse-\nquently post attention distillation, we train each BERT model with batch size 8, learning rate 1e-5, weight\ndecay 0, AdamW optimizer, and cosine scheduler for up to five epochs on the individual classification (all\nexcept STS-B) or regression tasks (STS-B) on the GLUE benchmark. For all tasks, we use the corresponding\navailable finetuned BERT-base-uncased checkpoints hosted at the HuggingFace model hub4, and thank the\noriginal uploader for their contributions.\nRecovering finetuned Vision Transformer performance on ImageNet-1K.\nTo demonstrate finetuned-\nconversion for the image domain, we use the vit-base-patch16-224 checkpoint provided by Google on\nHuggingFace5, which is trained on ImageNet-21k before being finetuned on ImageNet-1K at resolution of\n224 x 224 pixels (Dosovitskiy et al., 2020). For distillation, we freeze the original ViT weights, and train\nlinear attention MLPS with batch size 32, learning rate 0.01, zero weight decay, and AdamW optimizer, and\ntrain for two epochs. We then train all parameters with learning rate 1e-3, zero weight decay and AdamW\noptimizer up to 10 epochs with early stopping.\nB.5\nHedgehog pretrained conversion (Section 5.4)\nLinear GPT-2 125M conversion for WikiText-103 language modeling.\nWe use the available GPT-\n2 125M pretrained checkpoint available on HuggingFace6 from Radford et al. (2019). For Hedgehog, we first\n3https://huggingface.co/JeremiahZ/bert-base-uncased-cola\n4https://huggingface.co/JeremiahZ/\n5https://huggingface.co/google/vit-base-patch16-224\n6https://huggingface.co/gpt2\n19\ndo attention distillation and train Hedgehog MLPs for two epochs over the WikiText-103 data, using batch\nsize 8, learning rate 0.01, zero weight decay, AdamW optimizer, and 1024-tokens per input. For T2R-GPT-\n2 and the subsequent Hedgehog-GPT-2 model, we finetune all model parameters with learning rate 6e-4,\nweight decay 0.01, and AdamW optimizer and 1024 tokens-per-input.\nLinear Llama-2 7B conversion for SAMSum corpus summarization.\nWe use the base Llama-2 7B\nmodel available via Meta and HuggingFace (llama-2-7b-hf) from Touvron et al. (2023). For all experiments,\nwe use non-quantized model weights in bfloat16, and conduct all training runs and evaluations on a single\nA6000 GPU.\nFor dataset preparation, we first convert individual document and summarization pairs into single next-\ntoken prediction samples, using the template in Listing 4. For both distillation and subsequent finetuning,\nwe then chunk these samples into concatenated inputs 1024 tokens long.\nFor attention distillation, we freeze all original Llama-2 weights, and train Hedgehog MLPs for every\nhead and layer (0.495% of the original model size). We then train for two epochs with learning rate 0.01,\nzero weight decay, AdamW optimizer, and batch size 8 with gradient accumulation.\nFor finetuning and comparison to T2R and standard attention, we apply LoRA to query, key, value, and\noutput projections of each layer. We use alpha parameter 16 and rank 8. We train with learning rate 1e-4,\nzero weight decay, AdamW optimizer, and batch size 8 with gradient accumulation.\nFor generation, we compute ROUGE metrics (R1, R2, RL; for overlap of unigrams, bigrams, and longest\ncommon subsequence) over model outputs. We generate sequences up to 100 tokens long, and evaluate based\non outputs up til the first </s> Llama stop token.\n1 # Llama -2\nprompt\ntemplate\nfor\nSAMSum\n2\n3 Summarize\nthis\ndialog:\n4 {input}\n5 ---\n6 Summary:\n7 {output }{ eos_token}\nListing 4: Llama-2 prompt template for SAMSum corpus summarization\nC\nAdditional results\nC.1\nExtended comparison to attention models on LRA\nIn Table 13, we compare Hedgehog\u2019s performance on LRA against a fuller set of Transformer and sub-\nquadratic Transformer based alternatives sourced either from the official benchmark leaderboard (Tay et al.,\n2021) or recent subquadratic attention works (where we display the most competitive alternatives in Ta-\nble 6). We find Hedgehog on average obtains best accuracy. Although recently non-Transformer models such\nas deep state-space models have shown impressive results outperforming Transformers on LRA (Gu et al.,\n2021), as our work focuses on how to improve and recover the expressivity of standard softmax Transformers,\nwe focus the comparison against other attention-based methods. We defer to Gu et al. (2021) and related\nworks for their LRA results.\nC.2\nHedgehog feature map generalization to new data\nWe extend our analysis into how Hedgehog\u2019s feature maps learned with one dataset generalize to attentions\ncomputed on a new dataset (c.f. Table 4 and Fig. 9 in Sec. 5.1). As in the prior section, we find Hedgehog\nlearned feature maps frequently generalize to new datasets. Despite training to match the softmax attentions\non one model and dataset, we first find Hedgehog feature maps can produce attention weights that closely\nresemble softmax attention for the same model on another dataset (App. C.2.1). We next quantify this fidelity\nvia KL divergence w.r.t. the softmax attentions (App. C.2.2). We find that Hedgehog learned feature maps\nalmost always still generalize better than prior linear attention feature maps. We finally show that this\nattention matching generalization transfers to actual pretrained-conversion performance (App. C.2.3). We\n20\nModel\nListOps\nText\nRetrieval\nImage\nPathfinder\nAverage\nTransformer\n36.37\n64.27\n57.46\n42.44\n71.40\n54.39\nLocal Att\n15.82\n52.98\n53.39\n41.46\n66.63\n46.06\nLinear Trans.\n16.13\n65.90\n53.09\n42.34\n75.30\n50.55\nReformer\n37.27\n56.10\n53.40\n38.07\n68.50\n50.67\nSparse Trans.\n17.07\n63.58\n59.59\n44.24\n71.71\n51.24\nSinkhorn Trans.\n33.67\n61.20\n53.83\n41.23\n67.45\n51.29\nLinformer\n35.70\n53.94\n52.27\n38.56\n76.34\n51.36\nPerformer\n18.01\n65.40\n53.82\n42.77\n77.05\n51.41\nSynthesizer\n36.99\n61.68\n54.67\n41.61\n69.45\n52.88\nLongformer\n35.63\n62.85\n56.89\n42.22\n69.71\n53.46\nBigBird\n36.05\n64.02\n59.29\n40.83\n74.87\n55.01\nNystrmformer\u2020\n37.15\n65.52\n79.56\n41.58\n70.94\n58.95\ncosFormer\u2020\n37.90\n63.41\n61.36\n43.17\n70.33\n55.23\nSkyformer\u2020\n39.25\n64.70\n82.06\n40.77\n70.73\n59.50\nHedgehog\n37.15\n64.60\n82.24\n40.15\n74.16\n59.66\nTable 13: Training-from-scratch on LRA. Hedgehog achieves best average performance across Transformers\nand subquadratic variants. \u2020 indicates method results reported from original works. All other reported from\nthe official LRA benchmark (Tay et al., 2021). Best, 2nd-best acc (%).\nreplace BERT-base softmax attentions with Hedgehog attentions trained on one task, and find finetuning\nwith these converted models on another GLUE task still leads to improvements over prior linear attentions.\nSetup.\nFor all experiments, we begin by training Hedgehog attentions on \u201cin-distribution\u201d softmax atten-\ntion data. We use the pretrained BERT-base-uncased model (Devlin et al., 2018) as the Transformer we wish\nto convert, and distill two sets of Hedgehog attentions over (1) the GLUE CoLA task or (2) 512-token chunks\nof WikiText-103 corpus. Thus, queries and keys computed with the BERT-base-uncased model over CoLA\nvalidation samples are \u201cin-distribution\u201d for the first set, and we are interested in seeing how attention weight\nfidelity or downstream performance recovery are affected when subsequently finetuning on non-CoLA GLUE\ndata. We compare with various prior ablations and alternative feature maps, such as the Transformer-to-\nRNN feature map (Kasai et al., 2021) after attention distillation, Hedgehog without attention distillation,\nand prior representative linear attentions such as Performer (Choromanski et al., 2020) and cosFormer (Qin\net al., 2022b).\nC.2.1\nQualitative evidence of Hedgehog data generalization\nIn Fig. 10 and Fig. 11, we visualize attention weights computed via various methods on heads in the 1st, 6th,\nand 12th layers of the BERT-base uncased model. We find Hedgehog can learn feature maps that lead to\nmatching softmax attention weights, even when computed on new data samples. Interestingly, the Hedgehog\nfeature maps result in significantly more similar attention weights versus alternative feature maps (quantified\nin the next section).\nIn addition, our comparisons to Hedgehog ablations suggest that the proposed Hedgehog feature map\nand distillation procedure are important for best generalization. Removing either the Hedgehog feature map\nform (via doing attention distillation using the prior Transformer-to-RNN feature map (T2R-HH) or not\ntraining feature maps (HH (No Train)) leads to lower fidelity, where attention distillation seems critical for\nretaining weights reasonably similar to softmax attention.\n21\nFigure 10: Qualitative generalization to MRPC. Attention weights for BERT-base-uncased queries and\nkeys computed on MRPC samples. We compare attentions from the 3rd head in the 1st, 6th and 12th\nlayers (top, middle, bottom). Hedgehog feature maps trained on CoLA or WikiText-103 often still produce\nattention weights similar to those of softmax attention on new data.\nFigure 11: Qualitative generalization to QNLI. Attention weights for BERT-base-uncased queries and\nkeys computed on QNLI samples.\nWe compare attentions from the 3rd head in the 1st, 6th and 12th\nlayers (top, middle, bottom). Hedgehog feature maps trained on CoLA or WikiText-103 often still produce\nattention weights similar to those of softmax attention on new data.\nC.2.2\nQuantitative analysis of Hedgehog data generalization\nTo quantify the above observations, we compute the KL divergence between Hedgehog attention weights\ncomputed on various GLUE tasks and the \u201cground-truth\u201d softmax attention weights, using the pretrained\nBERT-base-uncased model. We report the KL divergence in Table 14. Similar to the above visualizations, we\nfind that Hedgehog feature maps do seem to produce better matching attention weights to softmax attention\nvia significantly smaller KL divergences.\n22\nMethod\nCoLA\nMNLI\nMRPC\nQNLI\nQQP\nRTE\nSST-2\nSTS-B\nHedgehog (CoLA)\n0.173\n0.340\n0.652\n0.673\n0.374\n0.932\n0.211\n0.275\nHedgehog (WT-103)\n0.357\n0.381\n0.484\n0.446\n0.432\n0.428\n0.347\n0.360\nT2R-HH (CoLA)\n0.191\n0.603\n1.124\n1.141\n0.658\n1.357\n0.254\n0.444\nHedgehog Untrained\n0.687\n0.845\n1.264\n1.146\n0.890\n1.493\n0.859\n0.743\n1 + ELU\n1.231\n1.500\n2.150\n1.947\n1.509\n2.491\n1.505\n1.285\nPerformer\n1.293\n1.592\n2.239\n2.059\n1.588\n2.594\n1.558\n1.352\nCosFormer\n1.191\n1.341\n1.987\n1.840\n1.330\n2.398\n1.443\n1.142\nTable 14: KL divergence of attention weights generalizing to new data. Hedgehog attentions trained\non either CoLA (CoLA) or WikiText-103 (WT-103) data, still best match softmax attention weights com-\nputed on different GLUE tasks, despite being trained with task-specific data (measured via KL divergence;\nlower is better).\nMethod\nCoLA\nMRPC\nQNLI\nQQP\nRTE\nSST-2\nSTS-B\nHedgehog (CoLA)\n58.4\n89.4\n87.7\n89.8\n62.1\n91.9\n85.3\nHedgehog (WT-103)\n47.2\n89.4\n89.2\n90.4\n62.5\n91.4\n86.7\nHH (No Train)\n50.3\n83.3\n85.9\n86.5\n55.6\n89.5\n79.3\n1 + ELU\n26.8\n81.9\n78.5\n89.1\n55.6\n85.9\n41.8\nPerformer\n24.7\n81.4\n75.8\n86.5\n55.6\n85.1\n39.8\nCosFormer\n41.1\n82\n82.6\n89.3\n54.9\n88.4\n76.6\nTable 15: Attention generalization on downstream tasks. BERT models with Hedgehog attentions\ntrained on either CoLA (CoLA) or WikiText-103 (WT-103) achieve best GLUE performance despite being\nfinetuned on different GLUE tasks. This corresponds with prior observations in generalization via improved\nattention weight fidelity.\nC.2.3\nHedgehog data generalization via GLUE task transfer\nWe finally evaluate the Hedgehog attention generalization by finetuning the pretrained BERT models with\ntrained Hedgehog on new GLUE tasks.\nWe follow the same procedure described in Appendix B.4.\nIn\nTable 15, we find that the above attention weight observations on Hedgehog generalization also correspond\nwith downstream task performance.\nHedgehog-BERT models achieve best or second-best performance,\ndespite using attention feature maps trained on different data. We leave further generalization studies, such\nas how Hedgehog attentions trained on one model generalize to an entirely different model for future work.\n23\nC.3\nLlama-2 SAMSum Generations\nWe include sample generations from the SAMSum corpus summarization task (Gliwa et al., 2019), used\nto evaluate Hedgehog conversion of LLama-2 7B models in combination with low-rank adaptation (LoRA).\nVia the generation quality, we find that in contrast to prior conversion methods such as Transformer-to-\nRNN (T2R) (Kasai et al., 2021), Hedgehog makes pretrained-conversion with parameter-efficient finetuning\nfeasible on larger models.\nWe report generations for four test samples of the SAMSum test set (first 3, and a longer 6th), compar-\ning standard attention Llama-2 models, linear attention Llama-2 models achieved via Hedgehog attention\ndistillation, and linear attention Llama-2 models following T2R, all with LoRA finetuning on a single A6000\nGPU (unquantized bfloat16 weights; LoRA on all query, key, value, and output projections).\nFor future work, we think further showing how Hedgehog and pretrained-conversion can help apply larger\nmodels to tasks requiring longer contexts or more efficient inference, are promising avenues of exploration.\nSeveral considerations include additional explorations to improve generation quality, parameter training\nefficiency (e.g., combining Hedgehog with quantization (Dettmers et al., 2023)), and further utilizing Hedge-\nhog\u2019s linear attention efficiency via transfer to longer contexts (e.g., combining Hedgehog with positional\ninterpolation (Chen et al., 2023)).\n# (Input):\n<s> Summarize\nthis\ndialog:\nHannah: Hey , do you\nhave Betty \u2019s number?\nAmanda: Lemme\ncheck\nHannah: <file_gif >\nAmanda: Sorry , can \u2019t find it.\nAmanda: Ask\nLarry\nAmanda: He called\nher\nlast\ntime we were at the\npark\ntogether\nHannah: I don \u2019t know\nhim\nwell\nHannah: <file_gif >\nAmanda: Don \u2019t be shy , he \u2019s very\nnice\nHannah: If you say so..\nHannah: I\u2019d rather\nyou\ntexted\nhim\nAmanda: Just\ntext\nhim\nHannah: Urgh ..\nAlright\nHannah: Bye\nAmanda: Bye bye\n---\nSummary:\n# (Llama-2):\nHannah\nis\nlooking\nfor Betty \u2019s number. Amanda can \u2019t find it. Amanda\nsuggests\nHannah to ask\nLarry .</s>\n# (Hedgehog Llama-2):\nHannah\nis\nlooking\nfor Betty \u2019s number. Amanda\nwill\ntext\nher.</s>\n# (T2R Llama-2):\nalong\nrecently\nacquired\nthe\nbiggest , I\u2019tch. S C H P W 1 8 2 s e u d\nis the\nbiggest , I\u2019t a. S C H P e n t u a . S C H P e n t l e \u2019t a. S C H P e n t u a .\n?\nalong ch.\n?\nalong ch.\n?\nalong ch.\n?\nalong ch.\n?\nalong ch.\n?\nalong ch.\n?\nListing 5: SAMSum Llama-2 generations after LoRA finetuning.\n24\n# (Input):\n<s> Summarize\nthis\ndialog:\nEric: MACHINE!\nRob: That \u2019s so gr8!\nEric: I know! And\nshows\nhow\nAmericans\nsee\nRussian\n;)\nRob: And it \u2019s really\nfunny!\nEric: I know! I especially\nlike\nthe\ntrain\npart!\nRob: Hahaha! No one\ntalks to the\nmachine\nlike\nthat!\nEric: Is this\nhis\nonly stand -up?\nRob: Idk. I\u2019ll check.\nEric: Sure.\nRob: Turns\nout no! There\nare\nsome of his stand -ups on\nyoutube.\nEric: Gr8! I\u2019ll watch\nthem\nnow!\nRob: Me too!\nEric: MACHINE!\nRob: MACHINE!\nEric: TTYL?\nRob: Sure :)\n---\nSummary:\n# (Llama-2):\nEric\nand Rob are\nwatching a stand -up by a Russian\ncomedian .</s>\n# (Hedgehog Llama-2):\nEric\nand Rob are\nwatching a stand -up comedy by a Russian\ncomedian .</s>\n# (T2R Llama-2):\nalong\nrecently\nacquired\nthe\nbiggest ,\nI\u2019tch. S C H P e n t u a . S C H P e n t u a . S C H P e n t u a . S C H P e n t u a . S C H P\nalong ch.\n?\nalong ch.\n?\nalong ch. S C H P\nalong ch.\n?\nalong ch.\n?\nalong ch.\n?\nalong\nfor:\nalong\nfor:\nal\nListing 6: SAMSum Llama-2 generations after LoRA finetuning.\n25\n# (Input):\n<s> Summarize\nthis\ndialog:\nLenny: Babe , can you\nhelp me with\nsomething?\nBob: Sure , what \u2019s up?\nLenny: Which\none\nshould I pick?\nBob: Send me photos\nLenny:\n<file_photo >\nLenny:\n<file_photo >\nLenny:\n<file_photo >\nBob: I like\nthe\nfirst\nones\nbest\nLenny: But I already\nhave\npurple\ntrousers. Does it make\nsense to have\ntwo\npairs?\nBob: I have\nfour\nblack\npairs :D :D\nLenny: yeah , but\nshouldn \u2019t I pick a different\ncolor?\nBob: what\nmatters\nis what you \u2019ll give\nyou the\nmost\noutfit\noptions\nLenny: So I guess I\u2019ll buy the\nfirst or the\nthird\npair\nthen\nBob: Pick\nthe\nbest\nquality\nthen\nLenny: ur right , thx\nBob: no prob :)\n---\nSummary:\n# (Llama-2):\nLenny is\nlooking\nfor new\ntrousers. Bob\nsuggests\nthe\nfirst or the\nthird\npair.</s>\n# (Hedgehog Llama-2):\nLenny\nwants to buy a new\npair of\ntrousers. Bob\nsuggests\nthe\nfirst\ntwo\noptions .</s>\n# (T2R Llama-2):\nalong , I\u2019t a. S C H P e n t u a . S C H P\nalong ch. S C H P e n t u a .\n?\nalong ch. S C H P\nalong ch.\n?\nalong ch.\n?\nalong ch.\n?\nalong\nfor:\nalong ch.\n?\nalong ch.\n?\nalong\nfor:\nalong ch.\n?\nalong\nfor:\nListing 7: SAMSum Llama-2 generations after LoRA finetuning.\n26\n# (Input):\n<s> Summarize\nthis\ndialog:\nBenjamin: Hey guys , what\nare we doing\nwith\nthe\nkeys\ntoday?\nHilary: I\u2019ve got\nthem. Whoever\nwants\nthem\ncan\nmeet me at\nlunchtime\nor\nafter\nElliot: I\u2019m ok. We \u2019re\nmeeting\nfor the\ndrinks in the\nevening\nanyway\nand I guess we \u2019ll be going\nback to the\napartment\ntogether?\nHilary: Yeah , I guess so\nDaniel: I\u2019m with\nHilary\natm and won \u2019t let go of her for the\nrest of\nthe day , so any\noption\nyou\nguys\nchoose is good\nfor me\nBenjamin: Hmm I might\nactually\npass by at lunchtime , take\nthe\nkeys\nand go take a nap. I\u2019m sooo\ntired\nafter\nyesterday\nHilary: Sounds\ngood. We \u2019ll be having\nlunch\nwith\nsome\nFrench\npeople\n(the\nones\nwho\nwork on the\nhistory\nof food in\ncolonial\nMexico - I\nalready\nsee you\nyawning\nyour\nhead\noff)\nBenjamin: YAAAAWN\nWhere\nand\nwhere\nare you\nmeeting?\nHilary: So I\u2019m meeting\nthem at the\nentrance\nto the\nconference\nhall at\n2 pm and\nthen we \u2019ll head to this\nplace\ncalled La\nCantina. Italian\ncuisine , which is quite\nfunny , but that \u2019s what they \u2019ve chosen\nBenjamin: Interesting\nTo be honest , Hilary , I almost\nfeel\nlike\nchanging\nmy mind. Wanting\nto take\nthis\nnap\nmight\nend up\ncosting\nme to\ndear\nHilary: Oh come on\nBenjamin: All\nthese\nterrible\nobstacles\non mu way to bed\nmight\njust\nprove to much to take\nHilary: We \u2019ll try to avoid\ntalking\nabout\ntheir\nsubject\nof\nresearch.\nOh wait , no , I\u2019m actually\nmeeting\nthem\nbecause I wanted to chat\nabout\ntheir\nresearch\nlol\nElliot:\nHilary: Do join us , we \u2019re going to have\nfun. And\nthen you \u2019ll take\nthe\nkeys\nand\ntake\nthis\nmost\ndeserved\nof naps\nElliot: Sounds\nlike a plan\nHilary:\nElliot: See you at 2 then xx\n# (Llama-2):\nHilary\nhas the\nkeys. Benjamin\nis tired\nafter\nyesterday. Hilary is\nmeeting\nsome\nFrench\npeople\nat 2 pm. Benjamin\nwill\njoin\nthem.</s>\n# (Hedgehog Llama-2):\nHilary\nand\nElliot\nare\ngoing to meet at the\nentrance\nof the\nconference\nhall at 2 pm. Hilary\nand\nBenjamin\nwill\nmeet\nthere. Hilary\nand\nBenjamin\nwill\ntake\nthe\nkeys to the\napartment. Hilary\nand\nBenjamin\nwill\nmeet\nElliot at 2 pm. Hilary\nand\nBenjamin\nwill\ntake a nap.</s>\n# (T2R Llama-2):\nMost is the\nbiggest , I\u2019s:\nMost is the\nbiggest , I\u2019s:\nMost is the\nbiggest , I\u2019s:\nMost is the\nbiggest , I\u2019s:\nMost is the\nbiggest , I\u2019s:\nMost is the\nbiggest , I\u2019s:\nM\nListing 8: SAMSum Llama-2 generations after LoRA finetuning.\nC.4\nAdditional attention weight visualizations\nWe finally include additional visualizations of the attention weights computed via softmax attention in\ncomparison to Hedgehog and alternate linear attention feature maps. We visualize attentions computed on\nGLUE tasks (Sec. 5.4) from the 1st, 6th, and 12th (first, middle, last) layers of BERT models in top, middle,\nand bottom rows respectively, and for the 1st, 6th, and 12th heads.\n27\nFigure 12: BERT attention visualizations for CoLA. Head 1; 1st, 6th, and 12th layers.\nFigure 13: BERT attention visualizations for CoLA. Head 6; 1st, 6th, and 12th layers.\nFigure 14: BERT attention visualizations for CoLA. Head 12; 1st, 6th, and 12th layers.\n28\nFigure 15: BERT attention visualizations for RTE. Head 0; 1st, 6th, and 12th layers.\nFigure 16: BERT attention visualizations for RTE. Head 6; 1st, 6th, and 12th layers.\nFigure 17: BERT attention visualizations for RTE. Head 12; 1st, 6th, and 12th layers.\n29\nFigure 18: BERT attention visualizations for SST2. Head 1; 1st, 6th, and 12th layers.\nFigure 19: BERT attention visualizations for SST2. Head 6; 1st, 6th, and 12th layers.\nFigure 20: BERT attention visualizations for SST2. Head 12; 1st, 6th, and 12th layers.\n30\n"
  },
  {
    "title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
    "link": "https://arxiv.org/pdf/2402.04379.pdf",
    "upvote": "7",
    "text": "Published as a conference paper at ICLR 2024\nFINE-TUNED LANGUAGE MODELS GENERATE STABLE\nINORGANIC MATERIALS AS TEXT\nNate Gruver1\nAnuroop Sriram2\nAndrea Madotto2\nAndrew Gordon Wilson1\nC. Lawrence Zitnick2\nZachary Ulissi2\n1NYU\n2Meta FAIR\nABSTRACT\nWe propose fine-tuning large language models for generation of stable materials.\nWhile unorthodox, fine-tuning large language models on text-encoded atomistic\ndata is simple to implement yet reliable, with around 90% of sampled structures\nobeying physical constraints on atom positions and charges. Using energy above\nhull calculations from both learned ML potentials and gold-standard DFT calcula-\ntions, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate\nmaterials predicted to be metastable at about twice the rate (49% vs 28%) of CD-\nVAE, a competing diffusion model. Because of text prompting\u2019s inherent flexibility,\nour models can simultaneously be used for unconditional generation of stable\nmaterial, infilling of partial structures and text-conditional generation. Finally, we\nshow that language models\u2019 ability to capture key symmetries of crystal structures\nimproves with model scale, suggesting that the biases of pretrained LLMs are\nsurprisingly well-suited for atomistic data.\n1\nINTRODUCTION\nLarge language models (LLMs) are trained to compress large text datasets, but can also act as strong\nfoundations for non-text data (Del\u00e9tang et al., 2023). As compressors, LLMs extract common patterns\nand find simple programs that can produce them (Goldblum et al., 2023; Sutskever, 2023), regardless\nof the data\u2019s origin. From text pretraining alone, LLMs can compress or extrapolate data as diverse\nas images (Del\u00e9tang et al., 2023), tabular data (Goldblum et al., 2023), time series (Gruver et al.,\n2023a), or robotic trajectories (Mirchandani et al., 2023). Alongside generality, LLM pre-training\nalso gives rise to sample efficiency, as in-context learning and fine-tuning require far fewer training\nexamples to identify salient patterns than training a model from scratch (Brown et al., 2020).\nThe generality and sample efficiency of LLMs make them particular promising for scientific problems,\nwhere data are often limited, collected from diverse sources, or challenging for non-experts to interpret.\nIn materials science, for example, the number of known stable materials is relatively small, and the\ndata describing each material are diverse, including composition, structure, and complex properties.\nLLMs can learn generalizable rules from a small number of examples (Zhu et al., 2023), combine\nmodalities into a single model (Moon et al., 2023), and provide users with a text-based interface. A\ntext interface, in particular, has the potential to improve access to scientific discovery (White, 2023);\nLLMs can use text to describe new observations, or, in design applications (e.g. materials design,\ndrug discovery), LLMs can ingest text that specifies desired properties or constraints (Bran et al.,\n2023).\nIn this work, we show that fine-tuned LLMs can generate the three-dimensional structure of stable\ncrystals as text (Figure 1). Our method is simple: first, encode crystals as new-line separated strings\nand combine with text instructions, then perform parameter efficient fine tuning (PEFT) on a base\nLLM (LLaMA-2) with a multitask curriculum and translation augmentations (Section 4). We evaluate\nour method with Materials Project data (Jain et al., 2013), comparing against an invariant diffusion\nmodel and a sequence model trained from scratch. Using both learned ML potentials and gold-\nstandard DFT calculations, we show that our method can generate materials predicted to be stable at\nhigher rates than baseline methods. To understand the success of our fine-tuning approach, we probe\nthe learned symmetry properties of our model, proposing a new metric for language models trained\non atomistic data and examining the effect of model scale on learned invariance. Going beyond\n1\narXiv:2402.04379v1  [cs.LG]  6 Feb 2024\nPublished as a conference paper at ICLR 2024\nFigure 1: Overview of our approach to materials generation with large language models. Using string\nformatted crystals and task-specific prompting, we enable unconditional stable materials generation,\ntext-condition materials generation, and structural infilling. Base LLaMA-2 models are fine-tuned on\na database of known inorganic materials (Liu et al., 2020) using low-rank adapters.\nunconditional generation, we also show that our LLMs have other useful abilities within materials\ndesign, such as text-conditional generation and infilling, which can be used to optimize the properties\nof existing materials.1\n2\nRELATED WORK\nThere are two central challenges in applying generative models to crystals and related atomistic data.\nThe first challenge is that atoms are intrinsically both discrete and continuous objects, as each atom\nhas both an element identity and a position in three dimensional space. Approaches to generative\nmodeling often differ between for discrete and continuous data, and modeling both simultaneously\ncan be significantly more complex than modeling either individually. The second key challenge is the\nprevalence of symmetries in atomistic data. The unit cell, a repeated pattern tiled infinitely in every\ndirection, is the common representation for crystals because it easily captures translation invariance,\nthe fact that atoms can be shifted and wrapped around the unit cell while still representing the same\nunderlying structure. Symmetries can pose challenges to deep learning models because they entail\nconstraints on the functions that neural networks can learn.\nDiffusion models\nXie et al. (2021) introduced crystal diffusion variational autoencoder (CDVAE)\nto directly deal with both of these challenges. CDVAE uses several individual generative models\nfor discrete and continuous components that share a continuous (VAE) latent space. The chemical\ncomposition is reconstructed from this latent space using a language modeling head, while atom\npositions are generated with a denoising diffusion model (Ho et al., 2020). Since CDVAE, several\nworks have extended diffusion processes to capture all parameters of the crystal, not just the atomic\ncoordinates. Both Jiao et al. (2023) and Zeni et al. (2023) accomplish this by creating diffusions\nfor the lattice parameters and atom identities, while Yang et al. (2023) design a new continuous\nrepresentation that unifies atom identities and positions in a single high-dimensional tensor. In most\ncases, these diffusion models were designed with a careful eye towards symmetries and are built on\ntop of graph neural networks with strict invariance/equivariance properties (Xie et al., 2021; Jiao\net al., 2023; Zeni et al., 2023). The approach of Yang et al. (2023) is more similar to ours, as they\napply a general-purpose architecture (3D U-net) and modeling approach (Gaussian diffusion) to a\nnew representation, without guaranteeing symmetries. Discrete atom identities and variable length\n(number of atoms), however, require special considerations in diffusion models, unlike standard\nlanguage models, which were originally designed for modeling discrete sequences.\nLanguage models\nFlam-Shepherd & Aspuru-Guzik (2023) demonstrate an alternative to continuous\ndenoising models and architectural invariances. Instead of treating discrete and continuous modalities\nseparately, as in CDVAE, Flam-Shepherd & Aspuru-Guzik (2023) uses sequences of discrete tokens\nto represent everything, including the digits of atomic coordinates. With all data encoded as tokens,\nstandard language modeling methods designed for text can be applied with little to no modification.\nThe simplicity of this method also makes it simple to adapt to many different kinds of molecular\n1https://github.com/facebookresearch/crystal-llm\n2\nPublished as a conference paper at ICLR 2024\nstructures, including small molecules, protein binding pockets, and, of course, crystals. In lieu of\narchitectural symmetries, augmentations of the training data are used to encourage learning known\ninvariances. Flam-Shepherd & Aspuru-Guzik (2023) demonstrates that language models trained from\nscratch on many common molecular datasets actually outperform popular domain-specific models,\nincluding CDVAE, in their ability to capture valid element compositions and high-level statistics\nof the training data. Similarly, Antunes et al. (2023) also use language models to generate crystal\nstructures as discrete sequences by training from scratch on millions of CIF strings.\nOur work\nIn this work, we show that pretrained LLMs are also useful for understanding and\ngenerating 3-dimensional atomic structures. By using a pre-trained LLM, we can achieve high rates\nof validity without crystal-specific tokenization (Flam-Shepherd & Aspuru-Guzik, 2023) or millions\nof auxiliary structures (Antunes et al., 2023). Unlike many methods designed specifically for crystal\nstructures and symmetries, our method can also be easily extended to multiple crystal generation\ntasks and, in the future, to other atomistic modalities without any changes to the underlying model or\ntraining procedure. Building on the basic observations made by Flam-Shepherd & Aspuru-Guzik\n(2023), we show that larger models, which are often more effective compressors of data, demonstrate\nimproved ability to learn symmetries from the training data and augmentation.\n3\nBACKGROUND\nLanguage Modeling\nLLMs perform next-token prediction over sequences. The model is a cate-\ngorical distribution, p(wt+1|w0:t), where w0:t is the prompt, a sequence of input tokens, and wt+1\nis the predicted next token. To generate sequences from the model, the conditional distribution\nis sampled sequentially, but samples are rarely drawn from the original, unmodified categorical\ndistributions. Instead the sampling procedure is typically modulated with temperature (\u03c4) and nucleus\nsize (p) hyperparameters. Temperature serves to flatten the conditional distributions to uniform\n(high temperature) or collapse them around their maximal probabilities (low temperature). Nucleus\nsize limits which tokens can be sampled based on the cumulative distribution function, clipping out\nvalues that contribute very little mass. A nucleus of p (0 < p \u2264 1) corresponds to keeping tokens to\ncumulatively contribute p% of the total probability, and discarding the rest.\nTokenization\nTo train language models on text datasets, strings are converted into sequences of\ntokens. Most modern LLMs rely on byte pair encoding (BPE) (Gage, 1994), a compression method\nthat assigns tokens to common substrings, making overall sequence lengths shorter. One downside\nof BPE tokenization is the default tokenization of numbers. BPE typically breaks numbers into\nirregular substrings instead of individual digits. While breaking numbers into multi-digit tokens\ncreates shorter sequences, it also complicates learning basic arithmetic operations, which typically\noperate at the level of individual digits. Luckily, Touvron et al. (2023b) introduce tokenizers for\nLLaMA-2 models that break numbers into a sequence of digits, which has been shown to dramatically\nimprove performance on arithmetic tasks (Liu & Low, 2023). We use LLaMA models in our work\nbecause they have a natural representation of 3D coordinates and can therefore learn simple functions\nover those coordinates that obey domain-specific symmetries (Section 5).\nCrystal structures and energy prediction\nPeriodic materials are defined by a unit cell repeated\ninfinitely along all three dimensions (Figure 2). The unit cell comprises a lattice (parallelepiped)\nwith side lengths (l1, l2, l3) and angles (\u03b81, \u03b82, \u03b83). Within the lattice, there are N atoms, each\nspecified by an element identity, ei, and set of 3d coordinates (xi, yi, zi) which can be absolute or\nfractional (specified as a percentage of the unit cell side lengths). Therefore a bulk material can be\nfully described by the tuple\nC = (l1, l2, l3, \u03b81, \u03b82, \u03b83, e1, x1, y1, z1, ..., eN, xN, yN, zN) .\n(1)\nFor a given set of environmental conditions, every crystal has a corresponding energy that describes\nhow likely it will occur in a particular configuration. Configuration with unfavorable electrostatic\ninteractions from unlike atomic positions, such as highly overlapping atoms, are typically high\nenergy. The gold standard for energy prediction is density functional theory (DFT), which provides\ntractable approximations to governing quantum mechanical equations that describe the energy and\ntime evolution of a system. DFT, however, can be prohibitively expensive, often scaling O(n3) with\nthe system size, which has motivated development of deep learning potentials to approximate DFT\nsolutions (Lan et al., 2022).\n3\nPublished as a conference paper at ICLR 2024\nFigure 2: (left) We convert the crystal lattice, atom identities, and atom positions into strings.\nThe model is trained to generate a structures conditioned on the text prompt, which might contain\nadditional information about the composition, properties, or a starting structure to modify. (right)\nEnergy above hull (Ehull) quantifies the stability of a material. A crystal with Ehull < 0.1 will be\nenergetically favorable both in its structure and composition.\nStability of hypothetical materials (Ehull)\nThe composition of a crystal also impacts its energy,\nas different elements have different geometries and charge properties. Certain stoichiometries, or\nratios of elements, are naturally favored, and a composition of elements A and B with constituent\nparts AxBy can dissociate into the composition AcBd if it is energetically favorable. Because of the\neffect of composition, the energy of a crystal is typically a two dimensional concept captured by the\nenergy hull, which is the minimum observed configuration energy for a given composition. For a\ncrystal to be low-energy and stable, and therefore give rise to a practically useful material, it must\nhave a small energy above hull (Ehull), the distance from the energy hull for the crystals elemental\ncomposition (Figure 2). Crystals with Ehull < 0 are considered stable and by definition have lower\nenergy than the known minimum (which has Ehull = 0). Crystals with Ehull < 0.1 eV/atom are often\nmetastable and likely to be practical useful (Sun et al., 2016).\n4\nMETHOD\nOur approach to generating stable materials is pleasingly simple. We take a pre-trained LLM, which\nhas useful biases towards generalizable patterns, and fine-tune it on crystal string representations.\nBecause language models can also ingest text, we can condition the model\u2019s generations on text\ndescriptions. The flexibility of language models also allows us to solve other tasks, such as infilling,\nthrough small modifications to the input formatting. Though we focus solely on crystal structures\nin this work, our method itself is general purpose and could be easily extended to proteins, nucleic\nacids, or small molecules. We include a more detailed discussion of how general text-pretraining\nimpacts our method in Appendix A.5.\nString formatting and tokenization\nWe convert the crystal tuple C (Equation 1) using fixed\nprecision numbers. An example of crystal string formatting is shown in Figure 2. We represent lattice\nlengths with one decimal place (2-3 digits) and lattice angles as integers (1-3 digits). Fractional\ncoordinates are always represented with two digits. 3D coordinates are combined with spaces and\nall other crystal components are combined with newlines. We deliberately chose LLaMA-2 models\nbecause they are both state-of-the-art in overall performance among open-source models and because\nthey tokenize numbers as individual digits by default. Notably, it is therefore impossible to create one\ntoken per full number, as Flam-Shepherd & Aspuru-Guzik (2023) do in their best performing model\n(further discussion in Appendix A.1). Instead, we rely on the extensive pretraining of LLaMA-2\nmodels to instill useful biases over numerical operations (Liu & Low, 2023).\nPrompt design\nTo train a model that can be used for many tasks, including unconditional generation,\ntext-conditional generation, and infilling, we use task-specific prompts. The input to the model is\na prompt followed by the string-formatted crystal (Figure 2). In the most basic case, the prompt\nindicates that the model should generate bulk materials represented as a lattice and atoms. The prompt\ncan also be expanded to include a desired composition or material properties, or to include a starting\n4\nPublished as a conference paper at ICLR 2024\nstructure, in the case of infilling. For infilling, the prompt includes the string-formatted crystal with\nevery instance of a randomly chosen element replaced with [MASK], and the model is trained to\ngenerate the identity of the masked element at the end of the sequence. During training all three tasks\nare included through random sampling, with two thirds generation and one third infilling (details\nin Appendix A.2). As in instruction tuning, the prompt is given as input to the model but does not\ncontribute to the generative loss function. The model is only penalized for its predictions on the\ncrystal string or masked element.\nGeneration Prompt\nInfill Prompt\n<s>Below is a description of a bulk\nmaterial.\n[The chemical formula is\nPm2ZnRh]. Generate a description of\nthe lengths and angles of the lattice\nvectors and then the element type and\ncoordinates for each atom within the\nlattice:\n[ Crystal string ]</s>\n<s>Below is a partial description of a bulk material where\none element has been replaced with the string \u201c[MASK]\u201d:\n[ Crystal string with [MASK]s ]\nGenerate an element that could replace [MASK] in the bulk\nmaterial:\n[ Masked element ]</s>\nBlue text is optional and included to enable conditional generation. Purple text stands in for string encodings of atoms.\nAugmentations\nCrystals structures are symmetric under translational. All atomic coordinates\ncan be shifted modulo the lattice boundaries without changing the resulting material structure.\nSimilarly, the ordering of atoms within the lattice is irrelevant to the underlying material (permutation\ninvariance). Prior work on diffusion generative models guarantee these symmetries as invariance or\nequivariance constraints on the model architecture (Xie et al., 2021; Jiao et al., 2023). To encourage\ntranslation invariance in our language models, we apply random uniform translations to the fractional\ncoordinates. We chose not to augment the ordering of atoms because these variables often contained\nvaluable information, for example grouping set of elements together for placement in the lattice\n(discussion in Appendix A.1).\n5\nEXPERIMENTS\nWe explore several uses of language models in crystal generative modeling. First, in order to compare\nwith prior work, we show that fine-tuned LLMs can be used for unconditional generation of novel\nmaterials and that the resulting materials correspond to stable relaxed structures under the predictions\nof an ML potential and DFT. We then show that LLMs can also be used for text-conditional generation\nand to propose small changes to existing materials.\nDatasets and models\nFor consistency with prior work (Xie et al., 2021; Flam-Shepherd et al., 2023)\nwe used MP-20 (Jain et al., 2013), a dataset of 45231 materials, when training for unconditional\ngeneration. All structures in MP-20 are stable, and therefore an effective generative model trained\non MP-20 should tend to propose new crystals that are at least metastable. For text-conditioned\ngeneration, we train with all forms of prompting (Section 4) on a collection of 120,000 crystals\nfrom Materials Project (Appendix A.3). The collection includes basic property information, such\nas the space group number, band gap, Ehull and the chemical formula. All of our experiments\nwere conducted with LLaMA-2 models (7B 13B, and 70B) (Touvron et al., 2023a;b) through the\nTransformers library (Wolf et al., 2020) and PyTorch (Paszke et al., 2019). In order to train on small\nnumber of GPUs we use 4-bit quantization (Dettmers et al., 2022) and Low-Rank Adapters (LoRA)\n(Hu et al., 2021). We provide the full hyperparameters and training details in Appendix A.4.\nEvaluation\nFor basic evaluation of the LLM samples, we use the validity and diversity metrics\nintroduced by Xie et al. (2021). Structural validity is determined by non-overlapping atomic radii\n(overlapping taken to be both atoms within half a radius of each other), while compositional validity\ncaptures the net charge of the structure (only structures with net neutral total charge are valid).\nDiversity is computed as pairwise distance between samples under featurizations of the structure and\ncomposition from Matminer (Ward et al., 2018; Xie et al., 2021).\n5\nPublished as a conference paper at ICLR 2024\nTable 1: Following prior work (Xie et al., 2021), we evaluate fine-tuned LLaMA-2 models using\nvalidity, which captures physical constraints, as well as coverage and property metrics, which capture\nalignment between the ground truth and sampling distribution. We add stability checks, which count\nthe percentage of samples estimated to be stable by M3GNet (Chen & Ong, 2022) and DFT (Hafner,\n2008) (details in Appendix B.2). LLaMA models generate a high percentage of both valid and stable\nmaterials.\nMethod\nValidity Check\nCoverage\nProperty Distribution\nMetastable\nStable\nStructural\u2191\nComposition\u2191\nRecall\u2191\nPrecision\u2191\nwdist (\u03c1)\u2193\nwdist (Nel)\u2193\nM3GNet \u2191\nDFT\u2020 \u2191\nCDVAE\n1.00\n0.867\n0.991\n0.995\n0.688\n1.43\n28.8%\n5.4%\nLM-CH\n0.848\n0.835\n0.9925\n0.9789\n0.864\n0.13\nn/a\nn/a\nLM-AC\n0.958\n0.889\n0.996\n0.9855\n0.696\n0.09\nn/a\nn/a\nLLaMA-2\n7B (\u03c4 = 1.0)\n0.918\n0.879\n0.969\n0.960\n3.85\n0.96\n35.1%\n6.7%\n7B (\u03c4 = 0.7)\n0.964\n0.933\n0.911\n0.949\n3.61\n1.06\n35.0%\n6.2%\n13B (\u03c4 = 1.0)\n0.933\n0.900\n0.946\n0.988\n2.20\n0.05\n33.4%\n8.7%\n13B (\u03c4 = 0.7)\n0.955\n0.924\n0.889\n0.979\n2.13\n0.10\n38.0%\n14.4%\n70B (\u03c4 = 1.0)\n0.965\n0.863\n0.968\n0.983\n1.72\n0.55\n35.4%\n10.0%\n70B (\u03c4 = 0.7)\n0.996\n0.954\n0.858\n0.989\n0.81\n0.44\n49.8%\n10.6%\n\u2020 Fraction of structures that are first predicted by M3GNet to have EM3GNet\nhull\n< 0.1 eV/atom, and then verified with DFT to have EDFT\nhull < 0.0 eV/atom.\nWhile useful for sanity checking models, simple validity metrics only reflect a subset of our real-world\npriorities in generating novel materials. Arguably the most important property that we hope to assess\nin samples is their predicted stability, which we can approximate by predicting the energy of relaxed\nstructures. Using known materials and energy calculations from Materials Project we construct the\nground truth energy convex hull and then calculate the approximate energy above hull, \u02c6Ehull. We\nchose two methods to estimate material stability:\n\u2022 ML potential: M3GNet (Chen & Ong, 2022) provides energy, force, and stress approxima-\ntions for crystal unit cells. For each sample we first run a relaxation using force and stress\napproximations then use the energy of the final structure.\n\u2022 DFT: We run a relaxation using the Density Functional Theory code VASP (Hafner, 2008)\nwith INCAR settings chosen by Pymatgen (Ong et al., 2013). DFT is the more accurate, but\nalso much more computationally intense, of the two options.\nIn both cases, results are compatible with Materials Project values (Jain et al., 2013) (Appendix B.1).\nBecause DFT is prohibitively expensive for many use cases (often hours per calculation), we only\nuse it to double-check results obtained with ML potentials, and we only run VASP calculations on\nmaterials that have already been predicted as metastable by M3GNet (<0.1 eV/atom \u02c6Ehull). The use\nof a M3GNet surrogate model is not perfect as many structures in Figure 4 (right) have energies\nabove the expected 0.1 eV/atom threshold, but the structures are largely close to the hull compared to\nthe broader distribution of materials generated.\nFigure 3: A sample\nwith \u201challucinated\u201d el-\nement identities (Ln).\nUnconditional generation\nWe sample 10,000 structures from each fine-\ntuned LLaMA model, parsing a CIF from the generated string. We reject\nthe sample and draw another if a CIF cannot be parsed from the sampled\nstring, which guarantees all samples can be interpreted as crystals but does\nnot guarantee validity of the resulting crystal. We show the validity and\npredicted stability (Xie et al., 2021) of the resulting structures in Table 1,\nwhich shows that LLMs can achieve near-perfect rates of structural and\ncompositional validity. Hyper-parameters like temperature and nucleus size\ncan be used to trade-off validity and stability of samples with their coverage\n(Appendix B.3). LLaMA-2 70B strikes an effective balance, generating\nhigh rates of stable materials with good coverage and diversity (Figure\n4). By default, generation is completely unconstrained and therefore the\nmodel can hallucinates imaginary elements, for example \u201cLn,\u201d a common\nabbreviation for Lanthanide (Figure 3), but the problem can be easily avoided\nby constraining the tokens for element identities (Wolf et al., 2020).\n6\nPublished as a conference paper at ICLR 2024\nFigure 4: Stability of LLaMA samples compared to CDVAE (Xie et al., 2021). Fine-tuned LLaMA-2\n70B generates a higher rate of metastable ( \u02c6Ehull < 0.1) and stable materials than CDVAE, using\nestimates of \u02c6Ehull from both M3GNet (Chen & Ong, 2022) and VASP (Hafner, 2008). Because\nof computational cost, we only run VASP on structures predicted to be stable by M3GNet. Stable\nmaterials generated by LLaMA are also more diverse (as quantified by Matminer featurization (Ward\net al., 2018)) than stable samples from CDVAE. We include sampled stable structures, shown as\n(2,2,2) supercells, which display a high-degree of regularity and understanding of three-dimensional\nspace.\nSymmetry learning\nAs crystal structures have translational symmetry, ideally our model\u2019s likeli-\nhood should be invariant to translations. We propose Increase in Perplexity under Transformation\n(IPT) as metric for assessing the invariance of language models to continuous group transformations.\nFor a transformation group G with group elements g and group action t, we define IPT for an input s,\nFigure 5: Translation invariance\non test data and ability to generate\nstable materials increase in propor-\ntion. Larger models learn invari-\nances from augmentations more ef-\nfectively during training, likely as\na result of their preference for ab-\nstract and compressible patterns.\nIPT(s) = Eg\u2208G[PPL(tg(s)) \u2212 PPL(tg\u2217(s))]\nwhere\ng\u2217 = arg min PPL(tg\u2217(s))\nand PPL is the perplexity of the sequence, the exponent of the\nlength-normalized cross entropy loss, PPL(s) = 2 CE(s)/n. In\nour case G is the group of translation, where each g is a distance\nto translate by, and tg is the mapping that decode the string,\ntranslates the coordinates (wrapping them around the boundary),\nand re-encodes the string. IPT captures the degree to which\ntransformations change a language model\u2019s compression ability.\nGood understanding of group transformations and invariance\nin the data should lead to minimal change in the perplexity of\na transformed sequence. We can approximate IPT by sampling\nmany values of g (e.g. 20), picking g\u2217 as the minimum among\nthose values, and computing a sample mean. Figure 5 shows\nthe mean IPT of 500 random crystals from the test set, for each\nof the three LLaMA model sizes. We include additional details\nabout our IPT calculation in Appendix B.5.\nDiversity, novelty, and sampling speed\nWhen using generative models to discover new stable\nmaterials, there are several properties beyond the rate of stability that are practically significant. Novel\nand diverse samples encourage sufficient exploration of unknown material space, and sampling speed\ndictates how expensive it is to search within that space. We compare these properties for LLaMA-2\nmodels and CDVAE in Figure 6. To calculate diversity and novelty, we use the same featurizations\nas in Table 1, calculating pairwise distances for diversity and distance to the closest neighbor in the\ntraining set for novelty (details in Appendix B.6). All metrics are computed over crystals judged\nmetastable by M3GNet, so that all novelty and diversity are relevant and not an artifact of invalid\ngenerations. LLaMA-2 samples match or exceed the diversity of CDVAE samples and also obtain\nhigh rates of novelty when we consider both composition and structure. Interestingly, larger LLaMA\nmodels display less novel structures but more novel compositions. It\u2019s worth noting, however, that\nboth CDVAE and LLaMA-2 7B far exceed the structural novelty of a held out test set, while 13B\nand 70B are just slightly lower. To judge sampling speed, we calculate the time required for 10,000\n7\nPublished as a conference paper at ICLR 2024\nStructure\nDiversity\nComposition\nDiversity\nStructure\nNovelty\nComposition\nNovelty\nEither\nNovelty\n0.0\n0.5\n1.0\n1.5\nScore\n(Normed to Test Set)\n10,000\nSamples\n10,000\nMetastable\n0\n1\n2\n3\nHours\n7B (  = 1.0)\n7B (  = 0.7)\n13B (  = 1.0)\n13B (  = 0.7)\n70B (  = 1.0)\n70B (  = 0.7)\nCDVAE\nFigure 6: We compare LLaMA-2 models with CDVAE in their ability to generate novel and diverse\nsamples as well as their overall speed. (left) We calculate diversity and novelty using a featurization\nof structure and composition (as in Table 1). Diversity is calculated as pairwise distance in feature\nspace, while novelty quantifies the percentage of inputs that are far from the training set (Appendix\nB.6). All metrics are calculated only for samples that were already judged to be metastable. LLaMA-2\nmodels often generate more diverse samples than CDVAE, and achieve similar overall rates of novelty.\nInterestingly, structural novelty is lower in larger models, while compositional novelty is higher.\n(right) We compare the time required to generate 10,000 samples from each model. We run LLaMA-2\nmodels with the largest feasible batch size on one A100 GPU (Appendix B.7). While the largest\nLLaMA model is computationally expensive, smaller language models are very fast, especially when\nwe consider both sampling speed and rate of stability.\nsamples, using the largest possible batch size on one A100 GPU (Appendix B.7). In Figure 6, we\ncompare the sampling speed with CDVAE and find that smaller models are often significantly faster\nwhen generating metastable samples.\nText-conditioned generation\nExtending our method to text-conditional generation is as simple\nas including additional information in the prompt, with a small amount of additional text (Figure\n4). We explore conditioning on spacegroup number, composition, and Ehull, as these properties are\neasy to verify (at least approximately) in silico. We assess the model\u2019s ability to perform conditional\ngeneration by comparing the intended condition with labels obtained from an in-silico oracle for\nthe constraint. For the chemical formula, we simply parse the composition from the generated CIF.\nFor space group determination, we use pymatgen\u2019s SpacegroupAnalyzer with a precision of 0.2\nangstroms (Ong et al., 2013). For stability, we use M3GNet to estimate Ehull as before. Using the\noracle\u2019s labels, we then compute the percentage of cases in which the condition was properly met\n(Figure 7). The model is able to generate a material with the correct composition the majority of the\ntime but becomes less reliable as the number of atoms in the chemical formula increases. Space group\nconditioning is more challenging, as it requires precise control and understanding of 3D structure,\nbut the observed 24% is impressive when considering the 230 possible space groups. Generating\nstable/unstable structures as a binary task is the most challenging, likely because the training dataset\nis predominantly stable compounds and stability is defined only in reference to existing compounds.\nStability is most easily controlled by modulating sampling hyperparameters.\nInfilling Existing Materials\nIn many practical settings, sampling and filtering materials from\nscratch is unnecessary. Good starting materials are often known, and manufacturing processes are\neasier to adapt to related compositions than develop completely from scratch by making small edits\nto their composition\u2013often referred to as template methods (Kirklin et al., 2015; Saal et al., 2013). To\nemulate a typical template method, we construct a lookup table that maps each element to elements\nthat have a similar atom radius when in the same oxidation state (code in Appendix C). We choose an\nelement uniformly at random and swap it with a random element chosen from the table. The resulting\nstructure is then relaxed using M3GNet. To improve this strategy using our fine-tuned LLM, we used\nthe infilling prompt (Section 4) to obtain a distribution over elements (modulated with temperature\n\u03c4) which we use instead of a uniform distribution over swaps. To evaluate our mutation procedure,\nwe sample 3000 structures randomly from the test set and generate perform one mutation-relaxation\nstep for each, using both uniform and language model-guided sampling. In Figure, 7 we show the\n8\nPublished as a conference paper at ICLR 2024\nformula is PrAlO3 \nspace group is 221\nE above hull is 0.011\nBelow is a \nGenerate ...\nGenerate ...\nBelow is a partial\ndescription ...\n[MASK]\nS Se\nP(element)\n...\n0.92 0.34 0.75\nconstrain\ndescription ...\nFigure 7: Text-conditional generation and infilling of existing structures with fine-tuned LLMs. (left)\nIncluding composition or property information (sampled from a hold-out set) in the text prompt leads\nto a high rate of samples with the desired composition/property (space group or stability). We bin\nstability as \u02c6Ehull < 0.1 (metastable) and \u02c6Ehull > 0.1 (unstable) for simplicity. Complex formulas and\nspace groups challenge the model, but the samples are correct at a rate that facilitates practical use.\nWe also show the rate of samples that both satisfy the condition and are predicted to be metastable by\nM3GNet. (right) Using the infilling prompt we can select mutations to existing materials. LLaMA-2\n70B proposes a distribution over elements, which we constrain using knowledge of atom radii and\ncharge interactions. We sample mutations with temperature \u03c4 and relax the results structure with\nM3GNet. When we apply this mutation procedure, we obtain more stable materials per mutation,\nwith negligible changes to the overall diversity of the stable materials.\npercentage of stable compounds and diversity in the stable compounds for the uniform baseline and\nLLaMA-2 70B with different temperature values. LLaMA-2 70B proposes elements that lead to\nstable structures at a higher rate than the baseline template method without sacrificing diversity.\n6\nDISCUSSION\nBy generating a high rate of plausible stable materials (verified by DFT), we have demonstrated\nLLMs can be state-of-the-art generative models for atomistic domains with direct application of\nparameter-efficient instruction tuning and minimal task-specific modeling choices. This approach to\ngenerative modeling opens the door to multitask capabilities within a single sampling paradigm and\nmultimodal training on atoms and text (e.g. to extract knowledge from a large corpus of scientific\npapers). We also advocate for the use of evaluation metrics (e.g. Ehull) for generative models that are\nmore closely tied to the downstream task of generating stable or metastable materials. The space of\nall hypothetical materials is combinatorially large (consider all the ways to pack 20 arbitrary elements\ninto a box), but only a small subset of materials will actually be stable or metastable. Models that\ncan directly generate near-stable structures make all downstream tasks far easier, and increases the\nlikelihood the generative models may be useful for day-to-day tasks in materials discovery.\nLimitations\nOur method shares the limitations of the underlying generative models. LLMs can\nbe sensitive to precise details of the chosen prompt and the tokenization strategies, particularly in\nhow tokenization effects processing of numbers. Hallucination of unphysical chemical elements or\nstructures has been observed, though fortunately is easy to check and filter. Text-conditioning has\nthe potential to tap latent conceptual understanding in the underlying LLM, but training LLMs that\nsuccessfully leverage scientific and chemistry literature is a major outstanding challenge. Lastly,\ntraining the largest of our LLMs can be prohibitively expensive for some computational budgets.\nDespite this, inference from all LLMs is often highly tractable when compared to baseline methods\n(Appendix B.7).\nFuture directions\nThere is substantial room for improvement in conditional generation, which\ncould be used to directly generate materials with desired properties. While we did not pursue\nalternative sampling strategies in depth, approaches like classifier-free guidance (Sanchez et al., 2023)\nor variants of PPLM (Dathathri et al., 2019) might be useful in combination with fine-tuned LLMs\nto improve conditional generation. These methods could also be combined with primitives from\nBayesian optimization for sample-efficient and uncertainty-aware design (Stanton et al., 2022; Gruver\net al., 2023b).\n9\nPublished as a conference paper at ICLR 2024\nREFERENCES\nLuis M Antunes, Keith T Butler, and Ricardo Grau-Crespo. Crystal structure generation with\nautoregressive large language modeling. arXiv preprint arXiv:2307.04340, 2023.\nStephen\nBalaban.\nNVIDIA\nA100\nGPU\nBenchmarks\nfor\nDeep\nLearning.\nLambda\nLabs\nBlog,\n2020.\nURL\nhttps://lambdalabs.com/blog/\nnvidia-a100-gpu-deep-learning-benchmarks-and-architectural-overview.\nAndres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. Chemcrow: Augmenting\nlarge-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nChi Chen and Shyue Ping Ong. A universal graph deep learning interatomic potential for the periodic\ntable. Nature Computational Science, 2(11):718\u2013728, 2022.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason\nYosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text\ngeneration. arXiv preprint arXiv:1912.02164, 2019.\nGr\u00e9goire Del\u00e9tang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christo-\npher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al.\nLanguage modeling is compression. arXiv preprint arXiv:2309.10668, 2023.\nTim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise\nquantization. 9th International Conference on Learning Representations, ICLR, 2022.\nDaniel Flam-Shepherd and Al\u00e1n Aspuru-Guzik. Language models can generate molecules, materials,\nand protein binding sites directly in three dimensions as xyz, cif, and pdb files. arXiv preprint\narXiv:2305.05708, 2023.\nDaniel Flam-Shepherd, Kevin Zhu, and Al\u00e1n Aspuru-Guzik. Atom-by-atom protein generation and\nbeyond with language models. arXiv preprint arXiv:2308.09482, 2023.\nPhilip Gage. A new algorithm for data compression. The C Users Journal archive, 12:23\u201338, 1994.\nMicah Goldblum, Marc Finzi, Keefer Rowan, and Andrew Gordon Wilson. The no free lunch\ntheorem, kolmogorov complexity, and the role of inductive biases in machine learning. arXiv\npreprint arXiv:2304.05366, 2023.\nNate Gruver, Marc Finzi, Micah Goldblum, and Andrew Gordon Wilson. The lie derivative for\nmeasuring learned equivariance. arXiv preprint arXiv:2210.02984, 2022.\nNate Gruver, Marc Anton Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are\nzero-shot time series forecasters. In Thirty-seventh Conference on Neural Information Processing\nSystems, 2023a.\nNate Gruver, Samuel Stanton, Nathan C Frey, Tim GJ Rudner, Isidro Hotzel, Julien Lafrance-Vanasse,\nArvind Rajpal, Kyunghyun Cho, and Andrew Gordon Wilson. Protein design with guided discrete\ndiffusion. arXiv preprint arXiv:2305.20009, 2023b.\nJ\u00fcrgen Hafner. Ab-initio simulations of materials using vasp: Density-functional theory and beyond.\nJournal of computational chemistry, 29(13):2044\u20132078, 2008.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840\u20136851, 2020.\nJ. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685, 2021.\n10\nPublished as a conference paper at ICLR 2024\nAnubhav Jain, Shyue Ping Ong, Geoffroy Hautier, Wei Chen, William Davidson Richards, Stephen\nDacek, Shreyas Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, et al. Commentary: The\nmaterials project: A materials genome approach to accelerating materials innovation.\nAPL\nmaterials, 1(1), 2013.\nRui Jiao, Wenbing Huang, Peijia Lin, Jiaqi Han, Pin Chen, Yutong Lu, and Yang Liu. Crystal structure\nprediction by joint equivariant diffusion on lattices and fractional coordinates. In Workshop\non\u201dMachine Learning for Materials\u201dICLR 2023, 2023.\nScott Kirklin, James E Saal, Bryce Meredig, Alex Thompson, Jeff W Doak, Muratahan Aykol,\nStephan R\u00fchl, and Chris Wolverton. The open quantum materials database (oqmd): assessing the\naccuracy of dft formation energies. npj Computational Materials, 1(1):1\u201315, 2015.\nJanice Lan, Aini Palizhati, Muhammed Shuaibi, Brandon M Wood, Brook Wander, Abhishek Das,\nMatt Uyttendaele, C Lawrence Zitnick, and Zachary W Ulissi. Adsorbml: Accelerating adsorption\nenergy calculations with machine learning. arXiv preprint arXiv:2211.16486, 2022.\nTiedong Liu and Bryan Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic\ntasks. arXiv preprint arXiv:2305.14201, 2023.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike\nLewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation.\nTransactions of the Association for Computational Linguistics, 8:726\u2013742, 2020.\nSuvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas,\nKanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines.\narXiv preprint arXiv:2307.04721, 2023.\nSeungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain,\nChun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, et al. Anymal: An efficient and\nscalable any-modality augmented language model. arXiv preprint arXiv:2309.16058, 2023.\nShyue Ping Ong, William Davidson Richards, Anubhav Jain, Geoffroy Hautier, Michael Kocher,\nShreyas Cholia, Dan Gunter, Vincent L Chevrier, Kristin A Persson, and Gerbrand Ceder. Python\nmaterials genomics (pymatgen): A robust, open-source python library for materials analysis.\nComputational Materials Science, 68:314\u2013319, 2013.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward\nYang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning\nlibrary. In Neural Information Processing Systems, 2019.\nJames E Saal, Scott Kirklin, Muratahan Aykol, Bryce Meredig, and Christopher Wolverton. Materials\ndesign and discovery with high-throughput density functional theory: the open quantum materials\ndatabase (oqmd). Jom, 65:1501\u20131509, 2013.\nGuillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi,\nand Stella Biderman. Stay on topic with classifier-free guidance. arXiv preprint arXiv:2306.17806,\n2023.\nPhilipp Schmid. LLaMA 2 on Amazon Sagemaker, a Benchmark. https://huggingface.co/\nblog/llama-sagemaker-benchmark, 2023.\nSamuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton Greenside,\nand Andrew Gordon Wilson. Accelerating bayesian optimization for biological sequence design\nwith denoising autoencoders. In International Conference on Machine Learning, pp. 20459\u201320478.\nPMLR, 2022.\nWenhao Sun, Stephen T Dacek, Shyue Ping Ong, Geoffroy Hautier, Anubhav Jain, William D\nRichards, Anthony C Gamst, Kristin A Persson, and Gerbrand Ceder. The thermodynamic scale of\ninorganic crystalline metastability. Science advances, 2(11):e1600225, 2016.\n11\nPublished as a conference paper at ICLR 2024\nIlya Sutskever. An observation on generalization. Workshop on Large Language Models and\nTransformers, 2023. URL https://www.youtube.com/watch?v=AKMuA_TVz3A&ab_\nchannel=SimonsInstitute.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels. ArXiv, abs/2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\nJeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S.\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril,\nJenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar\nMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey\nEdunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv,\nabs/2307.09288, 2023b.\nLogan Ward, Alexander Dunn, Alireza Faghaninia, Nils ER Zimmermann, Saurabh Bajaj, Qi Wang,\nJoseph Montoya, Jiming Chen, Kyle Bystrom, Maxwell Dylla, et al. Matminer: An open source\ntoolkit for materials data mining. Computational Materials Science, 152:60\u201369, 2018.\nAndrew D White. The future of chemistry is language. Nature Reviews Chemistry, pp. 1\u20132, 2023.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\nDrame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language\nprocessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-\ncessing: System Demonstrations, pp. 38\u201345, Online, October 2020. Association for Computational\nLinguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.\nTian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, and Tommi Jaakkola. Crystal diffusion\nvariational autoencoder for periodic material generation. arXiv preprint arXiv:2110.06197, 2021.\nMengjiao Yang, KwangHwan Cho, Amil Merchant, Pieter Abbeel, Dale Schuurmans, Igor Mor-\ndatch, and Ekin Dogus Cubuk. Scalable diffusion for materials generation. arXiv preprint\narXiv:2311.09235, 2023.\nClaudio Zeni, Robert Pinsler, Daniel Z\u00fcgner, Andrew Fowler, Matthew Horton, Xiang Fu, Sasha\nShysheya, Jonathan Crabb\u00e9, Lixin Sun, Jake Smith, et al. Mattergen: a generative model for\ninorganic materials design. arXiv preprint arXiv:2312.03687, 2023.\nZhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun\nDai. Large language models can learn rules. arXiv preprint arXiv:2310.07064, 2023.\n12\nPublished as a conference paper at ICLR 2024\nAppendix\nTable of Contents\nA Training Details\n13\nA.1\nNumerical Formatting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nA.2 Training with Stochastic Prompts . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nA.3\nExtended Materials Project Dataset . . . . . . . . . . . . . . . . . . . . . . . .\n14\nA.4 Training Hyperparameters and Details . . . . . . . . . . . . . . . . . . . . . . .\n14\nA.5\nRole of Text Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nB\nModel Evaluation\n15\nB.1\nEvaluation with ML potentials and DFT . . . . . . . . . . . . . . . . . . . . . .\n15\nB.2\nStability Checks and Percentages . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nB.3\nTrade-Offs in Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nB.4 \u201cHallucination\u201d Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nB.5\nIncrease in Perplexity under Transformation (IPT)\n. . . . . . . . . . . . . . . .\n18\nB.6\nDiversity and Novelty Calculation . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nB.7\nSampling Speed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nC Template Method Baseline\n19\nA\nTRAINING DETAILS\nA.1\nNUMERICAL FORMATTING\nNotably, our approach to tokenization is distinctly different from prior work on modeling atomic\nstructures with language models. Instead of using a special vocabulary and training models from\nscratch, we use LLaMA-2\u2019s existing tokenizer. This choice allows us to easily process both encoded\ncrystals and text data. In early experiments, we tried out many other approaches, including fine-\ntuning LLaMA-2 models with additional tokens specific to crystal data. These methods were more\nchallenging to train and didn\u2019t lead to any improvements over using a shared tokenizer. We include a\nset of example training losses below:\nEpoch 1\nEpoch 2\nEpoch 3\nEpoch 4\nEpoch 5\nSpecial Crystal Tokens\n0.783\n0.693\n0.623\n0.611\n0.588\nShared Tokenization\n0.457\n0.432\n0.424\n0.401\n0.385\nThere are many important decisions involved both in text formatting (e.g the choice of fractional or ab-\nsolute coordinates) and augmentation of the input data (e.g. translation or permutation augmentations\non coordinates). As a simple example, we provide average validity numbers (using low temperature\nsampling) from earlier experiments on LLaMA-2 7B models trained with different formatting styles\nSetting\nStructural Validity\nCompositional Validity\nFractional coords\n91.4%\n83.2%\nAbsolute coords\n90.8%\n80.5%\nNo permutations\n92.5%\n82.9%\nWith permutations\n89.2%\n81.7%\nA.2\nTRAINING WITH STOCHASTIC PROMPTS\nIn order to enable multi-task use of the fine-tuned LLMs, we train on a stochastically generated\nprompt. Two thirds of the time we provide the model with a generation task, in which the prompt\n13\nPublished as a conference paper at ICLR 2024\nconsists of a basic instruction to generate a bulk material as a lattice and atom positions. We randomly\nsample a set of properties from the available descriptors of a given crystal and add any chosen ones\n(if any) to the prompt, using a small amount of wrapper text. The remaining one third of the time, we\nprovide use the sampled crystal to construct and infilling task. We choose on element randomly from\nthe set of elements in the composition and we construct a prompt that contain the string encoding of\nthe crystal with this element replaced with [MASK]. The model then generates the replaced element\nas text following the prompt.\nA.3\nEXTENDED MATERIALS PROJECT DATASET\nTo facilitate text-conditional generation, we extend the original CDVAE training dataset with materials\nfrom Materials Project (Jain et al., 2013) as of April 2023. We filter out crystal with more than 30\natoms in the unit cell, which slow down training with minimal benefit to model performance, leaving\na training set that contains 127609 crystal structures. The original validation and test splits are left\nunchanged and all test/validation points are removed from the new training set.\nA.4\nTRAINING HYPERPARAMETERS AND DETAILS\nWe provide the training details per model:\n\u2022 LLaMA-2 7B: Batch size of 256 for 65 epochs with a cosine annealed learning rate of\n0.0005. LoRA rank 8 and alpha 32.\n\u2022 LLaMA-2 13B: Batch size of 256 for 44 epochs with a cosine annealed learning rate of\n0.0005. LoRA rank 8 and alpha 32.\n\u2022 LLaMA-2 70B: Batch size of 32 for 21 epochs with a cosine annealed learning rate of\n0.0005. LoRA rank 8 and alpha 32.\nLimitations around available compute lead to our use of differing batch sizes and total number of\nepochs for each model. Ideally, we would train all models with the largest batch sized used among\nall models and would train all models for the same number of epochs (the maximum used by any\nmodel). At the same time, we wanted to properly demonstrate the full potential of all model sizes and\ntherefore chose to present results for the best model we were able to train at each model size.\nA.5\nROLE OF TEXT PRETRAINING\nText pretraining is essential to our method for two reasons.\n1. It would be impractically expensive or computationally infeasible to train models with up\nto 70B parameters from scratch on our data. Using a pretrained model with LoRA (Hu\net al., 2021) offers the benefits of model scale while maintaining tractability and limiting\noverfitting, as the actual number of trainable parameters can be relatively small.\n2. Pretraining on text data yields a model that can be conditioned on text for free, and text\nconditioning opens up a huge new realm of exciting possibilities, like conditioning samples\non desired properties. It would be challenging to achieve a similar result from scratch without\nsignificantly expanding the size of the dataset (to improve general text understanding) and\nwithout essentially training a general-purpose language model in the process.\nTo better understand the first point, let\u2019s quickly review the exact details of the finetuning procedure.\nWe are using low-rank adapters (LoRA), as opposed to end-to-end finetuning, and this means we are\nadding a small number of additional parameters to an existing, frozen model. The easiest way to\nsee the difference between this approach and training a model from scratch\u2013as in (Flam-Shepherd &\nAspuru-Guzik, 2023)\u2013is to compare the training loss over the first few epochs of training.\nModel\nEpoch 1\nEpoch 2\nEpoch 3\nEpoch 4\nEpoch 5\nGPT-2 (from scratch)\n0.946\n0.878\n0.807\n0.757\n0.740\nLLaMA-13B (LoRA)\n0.457\n0.432\n0.424\n0.401\n0.385\nLLaMA-70B (LoRA)\n0.402\n0.344\n0.325\n0.305\n0.296\n14\nPublished as a conference paper at ICLR 2024\nIf we attempt to run LoRA finetuning with randomly initialized parameters for the LLaMA-2 7B\nmodel we observe an immediate and significant difference in the training losses:\nModel\n1 Iter\n0.33 Epochs\n0.66 Epochs\n1 Epoch\nRandom\n13.46\n1.53\n0.81\n0.78\nPre-trained\n1.57\n0.47\n0.41\n0.39\nWhile LoRA finetuning is tractable because 99.95% of the model is frozen, finetuning a LLaMA-2\nmodel end-to-end in half-precision would require at least 4 times as many GPUs, making it infeasible\nfor all but a handful of researchers. When using LoRA, even though the base models are large the\nnumber of trainable parameters is very small. In fact, the LLamA-2 7B model has less trainable\nparameters than one of the baseline methods we compared (CDVAE) (Xie et al., 2021). The number\nof trainable parameters for each of our models and the baseline models is shown below:\nModel\nTrainable parameters (millions)\nPercentage of total\nCDVAE\n4.5\n100%\nLM-CH/AC\n1-100\n100%\nLLaMA-2 7B\n3.5\n0.05%\nLLaMA-2 13B\n6.5\n0.05%\nLLaMA-2 70B\n35\n0.05%\nB\nMODEL EVALUATION\nB.1\nEVALUATION WITH ML POTENTIALS AND DFT\nApproximating Ehull from the energies of known materials in Materials Project requires a consistent\ncorrection scheme. We touch on some of the details here.\nM3GNet\nImportantly, M3GNet was trained on the total energy of VASP calculations in the Mate-\nrials Project dataset, so the results were expected to be consistent with the correction schemes and\nabsolute energies in Section 5.\nVASP\nTo be consistent with the Materials Project settings (e.g. the PBE functional, DFT/DFT+U as\nappropriate, consistent pseudopotentials, etc). We did a single relaxation for every candidate structure\nusing the default parameters in MPRelaxSet (Ong et al., 2013). VASP relaxations were run using the\nGPU-accelerated VASP6 code.\nIn both situations, the total energies were corrected using the MP2020 compatibility scheme, which\nwas important to maintain consistency when calculating formation energies, and allow the use of\nvarying functionals (DFT/DFT+U) for different materials.\nB.2\nSTABILITY CHECKS AND PERCENTAGES\nTo calculate the percentage of metastable compounds, we take all samples and remove samples\nthat are invalid under the basic structure and composition checks. We then run relaxations with\nM3GNet and obtain the final relaxation energies. The final percentage takes into account both the\nrate of validity (used to perform the initial filtering), and the rate of compounds with \u02c6Ehull < 0.1,\nas determined by the convex hull calculation using the M3GNet relaxation energy. To calculate the\nVASP percentage, we select materials determined to be metastable M3GNet and run VASP with\ndefault setting. We then report the percentage of the materials with \u02c6Ehull < 0.0.\nB.3\nTRADE-OFFS IN SAMPLING\nWe note that modulating stability with sampling parameters like temperature and nucleus size has\na significant effect on the coverage properties of the resulting samples. We illustrate the trade-offs\nbetween stability and coverage in Figure 8. Coverage most likely decreases because nucleus size and\ntemperature collapse the distribution around samples with high likelihood, which are also more likely\nto be valid or stable. Notably, LLaMA-2 70B appears to demonstrate the best trade-offs, possibly\n15\nPublished as a conference paper at ICLR 2024\n0.7\n0.8\n0.9\nValidity\n0.90\n0.95\nRecall (all)\n0.3\n0.4\n0.5\nStability Rate\n0.85\n0.90\n0.95\nRecall (stable)\nLLaMA-2 7B\nLLaMA-2 13B\nLLaMA-2 70B\nCDVAE\nFigure 8: Validity and rate of stability depend on sampling hyper-parameters. Lowering the tempera-\nture or restricting the nucleus size leads to significant improvements in validity/stability but incurs a\ncost to coverage of a held-out test set (recall). Fine-tuned LLaMA-2 70B displays the best trade-off\nbetween coverage and stability, generating materials that are both stable and diverse.\nindicating a likelihood model that corresponds better to both the underlying properties of stability\nand the full, diverse distribution of structures.\nB.4\n\u201cHALLUCINATION\u201d EXAMPLES\nLLaMA-2 7B:\n#\ng e n e r a t e d\nusing\npymatgen\ndata_Met8 (Cu2N) 5\n_symmetry_space_group_name_H \u2212M\n\u2019P 1 \u2019\n_ c e l l _ l e n g t h _ a\n5.0000\n_ c e l l _ l e n g t h _ b\n5.0000\n_ c e l l _ l e n g t h _ c\n5.0000\n_ c e l l _ a n g l e _ a l p h a\n90.0000\n_ c e l l _ a n g l e _ b e t a\n90.0000\n_cell_angle_gamma\n90.0000\n_symmetry_Int_Tables_number\n1\n_ c h e m i c a l _ f o r m u l a _ s t r u c t u r a l\nMet8 (Cu2N) 5\n_chemical_formula_sum\n\u2019Met8 Cu10 N5\u2019\n_cell_volume\n125.0000\n_ c e l l _ f o r m u l a _ u n i t s _ Z\n1\nloop_\n_symmetry_equiv_pos_site_id\n_symmetry_equiv_pos_as_xyz\n1\n\u2019x ,\ny ,\nz \u2019\nloop_\n_atom_site_type_symbol\n_ a t o m _ s i t e _ l a b e l\n_ a t o m _ s i t e _ s y m m e t r y _ m u l t i p l i c i t y\n_ a t o m _ s i t e _ f r a c t _ x\n_ a t o m _ s i t e _ f r a c t _ y\n_ a t o m _ s i t e _ f r a c t _ z\n_atom_site_occupancy\nCu\nCu0\n1\n1.8300\n0.3900\n1.0000\n1\nCu\nCu1\n1\n0.8300\n0.4900\n1.0000\n1\nCu\nCu2\n1\n0.8300\n0.9900\n0.5000\n1\nCu\nCu3\n1\n0.6300\n0.1900\n0.2000\n1\nCu\nCu4\n1\n0.2300\n0.7900\n0.2000\n1\nCu\nCu5\n1\n0.6300\n0.7000\n0.3100\n1\nCu\nCu6\n1\n0.2300\n0.1900\n0.3000\n1\nCu\nCu7\n1\n1.0000\n0.8900\n0.7000\n1\nCu\nCu8\n1\n1.0000\n0.3900\n0.2000\n1\nCu\nCu9\n1\n0.4900\n0.8900\n0.7000\n1\nMet0+\nMet10\n1\n0.6300\n0.6000\n1.0000\n1\nMet0+\nMet11\n1\n0.4000\n0.4700\n0.4700\n1\nMet0+\nMet12\n1\n0.4000\n1.0000\n0.9800\n1\nMet0+\nMet13\n1\n1.0000\n0.2200\n0.9700\n1\nMet0+\nMet14\n1\n1.0000\n0.6300\n0.5000\n1\nMet0+\nMet15\n1\n0.2300\n0.2200\n0.6000\n1\nMet0+\nMet16\n1\n1.0000\n0.0000\n0.6100\n1\nMet0+\nMet17\n1\n0.6300\n0.1000\n0.5000\n1\nN\nN18\n1\n0.1200\n0.7000\n0.8000\n1\nN\nN19\n1\n0.2300\n0.5900\n0.2000\n1\nN\nN20\n1\n0.2300\n0.1900\n0.7000\n1\nN\nN21\n1\n0.4900\n0.2100\n0.1000\n1\nN\nN22\n1\n0.4800\n0.6100\n0.6000\n1\n\u2018\ndata_L3Li\n_symmetry_space_group_name_H \u2212M\n\u2019P 1 \u2019\n_ c e l l _ l e n g t h _ a\n5.1000\n_ c e l l _ l e n g t h _ b\n7.1000\n_ c e l l _ l e n g t h _ c\n7.4000\n_ c e l l _ a n g l e _ a l p h a\n84.0000\n_ c e l l _ a n g l e _ b e t a\n68.0000\n_cell_angle_gamma\n68.0000\n_symmetry_Int_Tables_number\n1\n_ c h e m i c a l _ f o r m u l a _ s t r u c t u r a l\nL3Li\n_chemical_formula_sum\n\u2019L12 Li4 \u2019\n_cell_volume\n230.15214369\n_ c e l l _ f o r m u l a _ u n i t s _ Z\n4\nloop_\n_symmetry_equiv_pos_site_id\n_symmetry_equiv_pos_as_xyz\n1\n\u2019x ,\ny ,\nz \u2019\nloop_\n_atom_site_type_symbol\n_ a t o m _ s i t e _ l a b e l\n_ a t o m _ s i t e _ s y m m e t r y _ m u l t i p l i c i t y\n_ a t o m _ s i t e _ f r a c t _ x\n_ a t o m _ s i t e _ f r a c t _ y\n_ a t o m _ s i t e _ f r a c t _ z\n_atom_site_occupancy\nLi\nLi0\n1\n0.7100\n0.4000\n0.8300\n1\nLi\nLi1\n1\n0.2200\n0.3700\n0.3600\n1\nLi\nLi2\n1\n0.7100\n0.8900\n0.3300\n1\nLi\nLi3\n1\n0.2100\n0.8700\n0.8600\n1\nL0+\nL4\n1\n1.0000\n0.6300\n0.6900\n1\nL0+\nL5\n1\n0.5100\n0.1400\n0.6600\n1\nL0+\nL6\n1\n0.9600\n0.5700\n0.1700\n1\nL0+\nL7\n1\n0.4700\n0.0700\n0.1700\n1\nL0+\nL8\n1\n0.9800\n0.6100\n0.1400\n1\nL0+\nL9\n1\n0.4900\n0.1100\n0.1400\n1\nL0+\nL10\n1\n1.0000\n0.1000\n0.6800\n1\nL0+\nL11\n1\n1.0000\n0.1100\n0.1500\n1\nL0+\nL12\n1\n0.4700\n0.5500\n0.1800\n1\nL0+\nL13\n1\n1.0000\n0.5800\n0.6800\n1\nL0+\nL14\n1\n0.4700\n0.0600\n0.6700\n1\nL0+\nL15\n1\n1.0000\n0.1300\n0.1700\n1\nLLaMA-2 13B:\n16\nPublished as a conference paper at ICLR 2024\ndata_LeB7 (NO3) 2\n_symmetry_space_group_name_H \u2212M\n\u2019P 1 \u2019\n_ c e l l _ l e n g t h _ a\n5.9000\n_ c e l l _ l e n g t h _ b\n5.9000\n_ c e l l _ l e n g t h _ c\n5.9000\n_ c e l l _ a n g l e _ a l p h a\n59.0000\n_ c e l l _ a n g l e _ b e t a\n59.0000\n_cell_angle_gamma\n59.0000\n_symmetry_Int_Tables_number\n1\n_ c h e m i c a l _ f o r m u l a _ s t r u c t u r a l\nLeB7 (NO3) 2\n_chemical_formula_sum\n\u2019 Le1 B7 N2 O6\u2019\n_cell_volume\n141.91223582\n_ c e l l _ f o r m u l a _ u n i t s _ Z\n1\nloop_\n_symmetry_equiv_pos_site_id\n_symmetry_equiv_pos_as_xyz\n1\n\u2019x ,\ny ,\nz \u2019\nloop_\n_atom_site_type_symbol\n_ a t o m _ s i t e _ l a b e l\n_ a t o m _ s i t e _ s y m m e t r y _ m u l t i p l i c i t y\n_ a t o m _ s i t e _ f r a c t _ x\n_ a t o m _ s i t e _ f r a c t _ y\n_ a t o m _ s i t e _ f r a c t _ z\n_atom_site_occupancy\nLe0+\nLe0\n1\n0.7100\n0.5000\n0.1700\n1\nB\nB1\n1\n0.3800\n0.1600\n0.0200\n1\nB\nB2\n1\n0.4600\n0.1600\n0.5700\n1\nB\nB3\n1\n0.4600\n0.7200\n0.5700\n1\nB\nB4\n1\n0.0400\n0.7900\n0.6500\n1\nB\nB5\n1\n1.0000\n0.2500\n0.6500\n1\nB\nB6\n1\n0.0000\n0.7900\n0.0900\n1\nB\nB7\n1\n0.0000\n0.1600\n0.6500\n1\nN\nN8\n1\n0.6200\n0.5700\n0.9800\n1\nN\nN9\n1\n0.0600\n0.3300\n0.2500\n1\nO\nO10\n1\n0.5500\n0.7600\n0.7100\n1\nO\nO11\n1\n0.1800\n0.5400\n0.6100\n1\nO\nO12\n1\n0.4300\n0.9500\n0.5400\n1\nO\nO13\n1\n0.9400\n0.1100\n0.9600\n1\nO\nO14\n1\n0.6400\n0.7700\n0.2900\n1\nO\nO15\n1\n0.3000\n0.3800\n0.1300\n1\n\u2018\ndata_MandeGd2O4\n_symmetry_space_group_name_H \u2212M\n\u2019P 1 \u2019\n_ c e l l _ l e n g t h _ a\n3.6000\n_ c e l l _ l e n g t h _ b\n3.6000\n_ c e l l _ l e n g t h _ c\n5.9000\n_ c e l l _ a n g l e _ a l p h a\n90.0000\n_ c e l l _ a n g l e _ b e t a\n90.0000\n_cell_angle_gamma\n90.0000\n_symmetry_Int_Tables_number\n1\n_ c h e m i c a l _ f o r m u l a _ s t r u c t u r a l\nMandeGd2O4\n_chemical_formula_sum\n\u2019Mande1 Gd2 O4\u2019\n_cell_volume\n76.46400000\n_ c e l l _ f o r m u l a _ u n i t s _ Z\n1\nloop_\n_symmetry_equiv_pos_site_id\n_symmetry_equiv_pos_as_xyz\n1\n\u2019x ,\ny ,\nz \u2019\nloop_\n_atom_site_type_symbol\n_ a t o m _ s i t e _ l a b e l\n_ a t o m _ s i t e _ s y m m e t r y _ m u l t i p l i c i t y\n_ a t o m _ s i t e _ f r a c t _ x\n_ a t o m _ s i t e _ f r a c t _ y\n_ a t o m _ s i t e _ f r a c t _ z\n_atom_site_occupancy\nGd\nGd0\n1\n0.8200\n0.2300\n0.1500\n1\nGd\nGd1\n1\n0.8200\n0.2300\n0.6300\n1\nMande0+\nMande2\n1\n0.3200\n0.7300\n0.8900\n1\nO\nO3\n1\n0.8200\n0.7300\n0.4100\n1\nO\nO4\n1\n0.3200\n0.7300\n0.1000\n1\nO\nO5\n1\n0.3200\n0.2300\n0.3900\n1\nO\nO6\n1\n0.8200\n0.7300\n0.7900\n1\nLLaMA-2 70B:\ndata_Ln3BO4\n_symmetry_space_group_name_H \u2212M\n\u2019P 1 \u2019\n_ c e l l _ l e n g t h _ a\n5.3000\n_ c e l l _ l e n g t h _ b\n5.9000\n_ c e l l _ l e n g t h _ c\n5.3000\n_ c e l l _ a n g l e _ a l p h a\n62.0000\n_ c e l l _ a n g l e _ b e t a\n90.0000\n_cell_angle_gamma\n90.0000\n_symmetry_Int_Tables_number\n1\n_ c h e m i c a l _ f o r m u l a _ s t r u c t u r a l\nLn3BO4\n_chemical_formula_sum\n\u2019Ln3 B1 O4\u2019\n_cell_volume\n146.33178751\n_ c e l l _ f o r m u l a _ u n i t s _ Z\n1\nloop_\n_symmetry_equiv_pos_site_id\n_symmetry_equiv_pos_as_xyz\n1\n\u2019x ,\ny ,\nz \u2019\nloop_\n_atom_site_type_symbol\n_ a t o m _ s i t e _ l a b e l\n_ a t o m _ s i t e _ s y m m e t r y _ m u l t i p l i c i t y\n_ a t o m _ s i t e _ f r a c t _ x\n_ a t o m _ s i t e _ f r a c t _ y\n_ a t o m _ s i t e _ f r a c t _ z\n_atom_site_occupancy\nLn0+\nLn0\n1\n0.1800\n0.0600\n0.9900\n1\nLn0+\nLn1\n1\n0.6800\n0.5600\n0.9900\n1\nLn0+\nLn2\n1\n0.1800\n0.5600\n0.4900\n1\nB\nB3\n1\n0.6800\n0.0600\n0.4900\n1\nO\nO4\n1\n0.6800\n0.3300\n0.1500\n1\nO\nO5\n1\n0.1800\n0.2800\n0.1800\n1\nO\nO6\n1\n0.6800\n0.7800\n0.8000\n1\nO\nO7\n1\n0.1800\n0.8300\n0.8500\n1\n\u2018\ndata_Gro15Nd4\n_symmetry_space_group_name_H \u2212M\n\u2019P 1 \u2019\n_ c e l l _ l e n g t h _ a\n7.0000\n_ c e l l _ l e n g t h _ b\n7.0000\n_ c e l l _ l e n g t h _ c\n6.9000\n_ c e l l _ a n g l e _ a l p h a\n71.0000\n_ c e l l _ a n g l e _ b e t a\n71.0000\n_cell_angle_gamma\n69.0000\n_symmetry_Int_Tables_number\n1\n_ c h e m i c a l _ f o r m u l a _ s t r u c t u r a l\nGro15Nd4\n_chemical_formula_sum\n\u2019 Gro15 Nd4 \u2019\n_cell_volume\n289.96945358\n_ c e l l _ f o r m u l a _ u n i t s _ Z\n1\nloop_\n_symmetry_equiv_pos_site_id\n_symmetry_equiv_pos_as_xyz\n1\n\u2019x ,\ny ,\nz \u2019\nloop_\n_atom_site_type_symbol\n_ a t o m _ s i t e _ l a b e l\n_ a t o m _ s i t e _ s y m m e t r y _ m u l t i p l i c i t y\n_ a t o m _ s i t e _ f r a c t _ x\n_ a t o m _ s i t e _ f r a c t _ y\n_ a t o m _ s i t e _ f r a c t _ z\n_atom_site_occupancy\nNd\nNd0\n1\n0.5600\n0.5700\n0.7800\n1\nNd\nNd1\n1\n0.7500\n0.7500\n0.5600\n1\nNd\nNd2\n1\n0.1700\n0.1700\n0.1400\n1\nNd\nNd3\n1\n0.9500\n0.9500\n0.3800\n1\nGro0+\nGro4\n1\n0.7600\n0.2300\n0.3000\n1\nGro0+\nGro5\n1\n0.1200\n0.4800\n1.0000\n1\nGro0+\nGro6\n1\n0.3800\n0.8700\n0.1000\n1\nGro0+\nGro7\n1\n0.0300\n0.6600\n0.8400\n1\nGro0+\nGro8\n1\n0.6500\n0.1700\n0.6400\n1\nGro0+\nGro9\n1\n0.5600\n0.0600\n0.7400\n1\nGro0+\nGro10\n1\n0.9200\n0.5000\n0.1600\n1\nGro0+\nGro11\n1\n0.4900\n0.7400\n0.2200\n1\nGro0+\nGro12\n1\n0.2400\n0.1000\n0.5800\n1\nGro0+\nGro13\n1\n0.9100\n0.2700\n0.6200\n1\nGro0+\nGro14\n1\n0.4000\n0.6100\n0.4600\n1\nGro0+\nGro15\n1\n0.2900\n0.2900\n0.4200\n1\nGro0+\nGro16\n1\n0.4500\n0.9200\n0.9400\n1\nGro0+\nGro17\n1\n0.9900\n0.1300\n0.0200\n1\nGro0+\nGro18\n1\n0.8400\n0.5100\n0.8200\n1\n17\nPublished as a conference paper at ICLR 2024\nB.5\nINCREASE IN PERPLEXITY UNDER TRANSFORMATION (IPT)\nAlthough there are existing metrics for invariance and equivariance in neural networks, language\nmodels pose unique challenges because of their discrete tokens, which do not change smoothly under\ncontinuous transformations. Though it might be possible to compute a meaningful analogue of the\nLie derivative (Gruver et al., 2022), or similar metrics, through interpolation of word embeddings, we\ndecide to adopt a simpler metric (IPT), which still highlights significant differences between base\nmodels. We calculate IPT for each model using 500 test datapoints and 20 randomly translation\nsampled as fraction coordinates from a uniform distribution per dimension. The translations them-\nselves are implemented in PyMatgen and respect periodic boundary conditions (Ong et al., 2013). In\norder to combine the IPT values in a meaningful way across different datapoints, we normalize their\nvalues by the mean perplexity over transformations. Thus datapoints which happen to have large\nperplexity, and therefore naturally large potential changes in perplexity, do not drown out points with\nsmall perplexity.\nB.6\nDIVERSITY AND NOVELTY CALCULATION\nFollowing (Xie et al., 2021), we calculate diversity as the pairwise distance between samples using a\nfeaturization of structure and composition. To calculate novelty, we also featurize the training dataset\nand calculate the distance to the nearest element of the training set for each sample. A sample is\nconsidered novel if the nearest element in the training set is above a threshold. We use a structural\ndistance cutoff of 0.1 and composition distance cutoff of 2. In addition to novelty of structure and\ncomposition individual, we also consider the overall novelty of a crystal, where overall novelty is\ndetermined by having either a new structure or a new composition. All metrics are calculated on\nfiltered samples that M3GNet qualifies as metastable. We report metrics on metastable samples\nbecause these numbers are more practically relevant and because the samples are more likely to\ncontribute meaningful variation, instead of being different from the training set and each other simply\nbecause they are wildly invalid. We normalize all diversity and novelty values by corresponding value\nfor the test set to provide a sense for the underlying data distribution.\nB.7\nSAMPLING SPEED\nAlthough LLMs might seem like computational overkill at face value, batching for large-scale sam-\npling allows LLaMA models to have comparable computational overhead to competing approaches.\nMaking exact comparisons between LLaMA models and CDVAE are slightly challenging because of\navailable hardware and differences in compatibility. We ran experiments primarily on A100 GPUs,\nwhile the publicly available code for CDVAE cannot be run on an A100 and reports results on a\nRTX2080 Ti.\nWe provide two analyses for the sampling rate of LLaMA models, one from experiments we ran\non a single A100 and alternative using third-party numbers for LLaMA models deployed on AWS\ninstances.\nLocal analysis\nWe obtain benchmark LLaMA-2 sampling times by running 5 batched generations\nand computingn the average time to completion. We then use these numbers to calculate the equivalent\ntime to sample 10,000 structures. In practice, we used distributed sampling on a cluster, so reporting\nour direct times to compute 10,000 samples would be less informative. We use the maximum batch\nsize that we can fit on an A100 GPU with each model without causing out-of-memory (OOM) errors\nduring sampling. The batch sizes were {7B: 512, 13B: 256, 70B: 128}. To compare CDVAE with our\nresults we perform a rough, but generous, conversion of their results to an A100 GPU. We multiply\ntheir rate of sampling by 16, to account for the 2x faster rate of operations (Balaban, 2020) and\napproximately 8 times larger GPU memory (allowing for large batch sizes and utilization rates). We\nreport the intermediate numbers and calculations below. The final rates for metastable samples are\nshown in Figure 6.\n18\nPublished as a conference paper at ICLR 2024\nModel\nBatch size\nSeconds / batch\nSamples / hour\nHours / 10,000 crystals\nCDVAE\n512\nn/a\nn/a\n1.260\nLLaMA-2 7B\n512\n27.18\n67814\n0.147\nLLaMA-2 13B\n256\n38.24\n24100\n0.414\nLLaMA-2 70B\n128\n52.52\n8774\n1.139\nAWS analysis\nConsidering AWS as the deployment environment, we can build on a recent bench-\nmark on a cloud instance with 8 A100 GPUs (ml.p4d.12xlarge) (Schmid, 2023), which found that\nLLaMA-2 13B achieved 0.416 hr/1M tokens and LLaMA-2 70B achieved 0.864 hr/1M tokens. One\ncrystal is around 100 tokens on average, so the throughput for 10,000 crystals is the same as for 1M\ntokens. For comparison, we use CDVAE and its recorded runtimes for generating 10,000 crystals on\na single RTX2080 Ti GPU (Xie et al., 2021). To obtain the final numbers, we adjust for the number\nof GPUs (8) and a 2x improvement from RTX2080 Ti to A100 GPUs (Balaban, 2020).\nModel\nHours / 10,000 crystals\nHours / 10,000 metastable (M3GNet) crystals\nCDVAE\n0.363\n1.260\nLLaMA-2 13B\n0.416\n1.094\nLLaMA-2 70B\n0.864\n1.728\nWe see that LLaMA-2 13B actually has a comparable computational overhead to prior work, and\nLLaMA-2 70B is only slightly higher. When considering the rate of stable materials generated by\neach method, we see that LLaMA-2 13B actually has a higher throughput than CDVAE.\nC\nTEMPLATE METHOD BASELINE\nWe provide code in Listing 1 implementing construction of the physically-inspired element swap\ntable. This table is used by both the template method and the LLM-guided sampling method to\nconstrain search to elements that are physically plausible. Listing 2 shows our implementation of\na basic template method with uniform sampling. The LLM-guided procedure is mostly identical,\nexcept with uniform sampling of the swap element changed for sampling from a distribution obtained\nfrom the LLM with an infilling prompt (and modulated with temperature parameter \u03c4)\n1 import os\n2 import random\n3 import pandas as pd\n4 import numpy as np\n5 from pymatgen.core import Element\n6 from pymatgen.core.structure import Structure\n7 from m3gnet.models import Relaxer\n8\n9 def find_similar_elements(target_element, elements, tolerance=0.1):\n10\nsimilar_elements = []\n11\nfor state, radius in target_element.ionic_radii.items():\n12\nfor el in elements:\n13\nif state in el.ionic_radii:\n14\nradius_diff = abs(radius - el.ionic_radii[state])\n15\nif radius_diff < tolerance and el.symbol !=\ntarget_element.symbol:\n16\nsimilar_elements.append((el.symbol, state,\nradius_diff))\n17\nreturn sorted(similar_elements, key=lambda x: x[2])\n18\n19 def make_swap_table():\n20\nelements = [Element(el) for el in Element]\n21\n22\nswap_table = {}\n23\n24\nfor el in elements:\n25\nswap_table[el.symbol] = [\n26\nx[0] for x in find_similar_elements(el, elements)\n27\n]\n19\nPublished as a conference paper at ICLR 2024\n28\n29\nreturn swap_table\nListing 1: Self contained code to construct the template method table which can be used to proposed\nmutations for local optimization around an existing material. The same table can be used in tandem\nwith a language model to provide sampling constraints (i.e. eliminate elements which are very\nphysically unlikely).\n1 def propose_new_structures(cif_str, swap_table, max_swaps=1):\n2\nstruct = Structure.from_str(cif_str, fmt=\"cif\")\n3\n4\nelements = [el.symbol for el in struct.species]\n5\nswappable_elements = [\n6\nel for el in elements if el in swap_table and len(swap_table[el])\n> 0\n7\n]\n8\n9\nnum_possible_swaps = sum([len(swap_table[el]) for el in\nswappable_elements])\n10\nnum_swaps = min(num_possible_swaps, max_swaps)\n11\n12\nrelaxer = Relaxer()\n13\nnew_bulks = []\n14\nfor _ in range(num_swaps):\n15\nold_el = random.choice(swappable_elements)\n16\npossible_new = swap_table[old_el]\n17\nnew_el = random.choice(possible_new)\n18\n19\nnew_bulk = struct.copy()\n20\nnew_bulk.replace_species({old_el: new_el})\n21\n22\nrelax_results = relaxer.relax(new_bulk)\n23\nfinal_structure = relax_results[\u2019final_structure\u2019]\n24\nfinal_relaxed_energy = relax_results[\u2019trajectory\u2019].energies[-1]\n25\n26\nnew_bulks.append(dict(\n27\ncif=final_structure.to(fmt=\"cif\"),\n28\nenergy=final_relaxed_energy\n29\n))\n30\n31\nnew_bulks = pd.DataFrame(new_bulks)\n32\nreturn new_bulks\nListing 2: Self contained code implementing a template method with uniform sampling. Our language\nmodel procedure is essentially the same but replaces uniform sampling with logits from a prompted\nlanguage model. This language model can use the context from the rest of the crystal structure to\npropose a mutation instead of choosing a mutation completely at random.\n20\n"
  },
  {
    "title": "Fast Timing-Conditioned Latent Audio Diffusion",
    "link": "https://arxiv.org/pdf/2402.04825.pdf",
    "upvote": "6",
    "text": "Fast Timing-Conditioned Latent Audio Diffusion\nZach Evans 1 CJ Carr 1 Josiah Taylor 1 Scott H. Hawley 2 Jordi Pons 1\nAbstract\nGenerating long-form 44.1kHz stereo audio from\ntext prompts can be computationally demanding.\nFurther, most previous works do not tackle that\nmusic and sound effects naturally vary in their\nduration. Our research focuses on the efficient\ngeneration of long-form, variable-length stereo\nmusic and sounds at 44.1kHz using text prompts\nwith a generative model. Stable Audio is based\non latent diffusion, with its latent defined by a\nfully-convolutional variational autoencoder. It is\nconditioned on text prompts as well as timing\nembeddings, allowing for fine control over both\nthe content and length of the generated music\nand sounds. Stable Audio is capable of render-\ning stereo signals of up to 95 sec at 44.1kHz in\n8 sec on an A100 GPU. Despite its compute effi-\nciency and fast inference, it is one of the best in\ntwo public text-to-music and -audio benchmarks\nand, differently from state-of-the-art models, can\ngenerate music with structure and stereo sounds.\n1. Introduction\nThe introduction of diffusion-based generative models (Sohl-\nDickstein et al., 2015; Ho et al., 2020) has lead to rapid\nimprovements in the quality and controllability of generated\nimages (Podell et al., 2023), video (Blattmann et al., 2023),\nand audio (Rouard & Hadjeres, 2021; Liu et al., 2023a).\nOne challenge is that diffusion models working within the\nraw signal space tend to be computationally demanding\nduring both training and inference. Diffusion models work-\ning in the latent space of a pre-trained autoencoder, termed\n\u201clatent diffusion models\u201d (Rombach et al., 2022), are sig-\nnificantly more compute efficient. Working with a heavily\ndownsampled latent representation of audio allows for much\nfaster inference times compared to raw audio, and also al-\nlows generating long-form audio (e.g., 95 sec).\nAnother challenge with audio diffusion models is that those\n1Stability AI. 2Belmont University, work done while at Stability\nAI. Correspondence to: Zach Evans <zach@stability.ai>.\nare usually trained to generate fixed-size outputs (Huang\net al., 2023a), e.g., a model trained on 30 sec audio chunks\nwill generate 30 sec outputs. This is an issue when training\non and trying to generate audio of varying lengths, as is the\ncase when generating full songs or sound effects. Hence\naudio diffusion models are commonly trained on randomly\ncropped chunks from longer audios, cropped or padded to fit\nthe diffusion model\u2019s training length. With music, e.g., this\ncauses the model to generate arbitrary sections of a song,\nwhich may start or end in the middle of a musical phrase.\nStable Audio is based on a latent diffusion model for audio\nconditioned on a text prompt as well as timing embeddings,\nallowing for control over the content and length of the gen-\nerated music and sound effects. This additional timing\nconditioning allows us to generate audio of a specified (vari-\nable) length up to the training window length. Due to the\ncompute efficient nature of latent diffusion modeling, it can\ngenerate long-form content in short inference times. It can\nrender up to 95 sec (our training window length) of stereo\naudio at 44.1kHz in 8 sec on an A100 GPU (40GB VRAM).\nThe commonly used metrics for generative audio are de-\nsigned to evaluate short-form mono signals at 16kHz (Kil-\ngour et al., 2018). Yet, our work focuses on generating long-\nform full-band stereo signals. We propose: (i) a Fr\u00e9chet Dis-\ntance based on OpenL3 embeddings (Cramer et al., 2019)\nto evaluate the plausibility of the generated long-form full-\nband stereo signals, (ii) a Kullback-Leibler divergence to\nevaluate the semantic correspondence between lengthy gen-\nerated and reference audios up to 32kHz, and (iii) a CLAP\nscore to evaluate how long-form full-band stereo audios ad-\nhere to the given text prompt. We also conduct a qualitative\nstudy, assessing audio quality and text alignment, while also\npioneering the assessment of musicality, stereo correctness,\nand musical structure. We show that Stable Audio can ob-\ntain state-of-the-art results on long-form full-band stereo\nmusic and sound effects generation from text and timing\ninputs. We also show that, differently from previous works,\nStable Audio is also capable to generate structured music\n(with intro, development, outro) and stereo sound effects.\nCode to reproduce our model/metrics and demos is online1.\n1Model:\nhttps://github.com/Stability-AI/stable-audio-tools.\nMetrics:\nhttps://github.com/Stability-AI/stable-audio-metrics.\nDemo:\nhttps://stability-ai.github.io/stable-audio-demo.\n1\narXiv:2402.04825v2  [cs.SD]  8 Feb 2024\nFast Timing-Conditioned Latent Audio Diffusion\n2. Related work\nAutoregressive models \u2014 WaveNet (Oord et al., 2016) au-\ntoregressively models quantized audio samples, but is slow\nduring inference because it operates with waveforms. Re-\ncent autoregressive models addressed this by operating on a\nquantized latent space, enabling faster processing. Jukebox\n(Dhariwal et al., 2020) relies on a multi-scale approach to\nencode music into a sequence of quantized latents and sub-\nsequently models them using autoregressive transformers.\nRecent work such as MusicLM (Agostinelli et al., 2023) and\nMusicGen (Copet et al., 2023) utilize a similar approach\nand also autoregressively model quantized latent sequences.\nHowever, unlike Jukebox, such models are conditioned on\ntext prompts rather than on artist, genre, and/or lyrics. Au-\ntoregressive models similar to MusicLM (AudioLM) and\nMusicGen (AudioGen) have also been used for sound syn-\nthesis (Borsos et al., 2023; Kreuk et al., 2022) and for gener-\nating music accompaniments from singing (Donahue et al.,\n2023). Our work is not based on autoregressive modeling.\nNon-autoregressive models \u2014 Parallel WaveNet (Oord\net al., 2018) and adversarial audio synthesis (Donahue et al.,\n2018; Pasini & Schl\u00fcter, 2022) were developed to tackle\nthe computational inefficiencies inherent in autoregressive\nmodeling. Recent works like VampNet (Garcia et al., 2023),\nStemGen (Parker et al., 2024) and MAGNeT (Ziv et al.,\n2024) are based on masked token modeling (Chang et al.,\n2022). These are for creating musical variations, gener-\nating additional stems for a given song, and to efficiently\nsynthesize music and sounds, respectively. Flow-matching\ngenerative modeling (Vyas et al., 2023) was also recently\nintroduced for speech and sounds synthesis. Our work is\nnot based on any of the non-autoregressive models above.\nEnd-to-end diffusion models \u2014 CRASH (Rouard & Had-\njeres, 2021) was proposed for unconditional drums syn-\nthesis, DAG (Pascual et al., 2023) for class-conditional\nsounds synthesis, Noise2Music (Huang et al., 2023a) for\ntext-conditional music synthesis, and Mariani et al. (2023)\nbuilt an end-to-end diffusion model capable of both music\nsynthesis and source separation. Our work is also based\non diffusion, albeit not in an end-to-end fashion. Rather, it\ninvolves latent diffusion due to its computational efficiency.\nSpectrogram diffusion models \u2014 Riffusion (Forsgren &\nMartiros, 2022) fine-tuned Stable Diffusion to generate spec-\ntrograms from text prompts, Hawthorne et al. (2022) ad-\ndressed MIDI-to-spectrogram generation, and CQT-Diff\n(Moliner et al., 2023) relied on CQT spectrograms for band-\nwidth extension, inpatining, and declipping. An additional\nstep is required to render waveforms from magnitude spec-\ntrograms. Our work is also based on diffusion, albeit it does\nnot rely on spectrogram-based synthesis.\nLatent diffusion models \u2014 Mo\u00fbsai (Schneider et al., 2023)\nand AudioLDM (Liu et al., 2023a) pioneered using latent dif-\nfusion for text-to-music and -audio. Their main difference\nbeing that Mo\u00fbsai decodes latents onto waveforms through\na diffusion decoder, while AudioLDM decodes latents onto\nspectrograms which are then inverted to waveforms with\nHiFi-GAN (Kong et al., 2020). AudioLDM2 (Liu et al.,\n2023b) extends AudioLDM to also synthesize speech by\nusing a shared representation for music, audio, and speech\nto condition the latent diffusion model. JEN-1 (Li et al.,\n2023) is an omnidirectional latent diffusion model trained\nin a multitask fashion. JEN-1 Composer (Yao et al., 2023)\nis its extension for multi-track music generation. Levy et al.\n(2023) explored sampling-time guidance for both end-to-end\nand latent diffusion models. All previous works constrain\nthe latent to be normalized, often with a variational autoen-\ncoder (VAE). The exceptions being JEN-1, which runs over\na dimensionality reduced latent that is normalized based on\nthe mean and covariance, and Mo\u00fbsai that simply uses a\ntanh. Our work is also based on latent diffusion, and we\nnormalize latents by using a VAE. Appendix D includes\nfurther discussion on related latent diffusion models.\nHigh sampling rate and stereo generation \u2014 Mo\u00fbsai\nand JEN-1 generate 48kHz stereo music. AudioLDM2 can\ngenerate 48kHz mono music. Levy et al. (2023) generates\n44.1kHz stereo music. No other prior works generate mu-\nsic up to the standard specifications of commercial music\n(44.1kHz stereo). DAG and AudioLDM2 generate 48kHz\nmono sounds, and we are not aware of prior works tackling\nstereo sound synthesis. Our work focuses on generating\n44.1kHz stereo music and sounds from text prompts.\nText embeddings \u2014 CLAP (Wu et al., 2023) and T5-like\n(Raffel et al., 2020; Ghosal et al., 2023) text embeddings\nare commonly used because of their open-source nature.\nCLAP relies on a contrastive (multimodal) language-audio\npretraining, and T5 is a large language model. Further, Musi-\ncLM uses MuLan (Huang et al., 2022), that is also based on\ncontrastive language-audio pretraining but on their private\ndataset. Our work relies on a CLAP-based model trained in\na contrastive language-audio fashion on our dataset.\nFast generation of variable-length, long-form audio \u2014\nAutoregressive models can generate long-form audio of\nvariable length due to their sequential (one-sample-at-a-time\ngeneration) nature, but are slow at inference time. Previous\nnon-autoregressive models were trained to generate up to\n20 sec long music (Parker et al., 2024). Previous end-to-end\nand latent diffusion models were trained to generate up to\n30 sec long music (Huang et al., 2023a; Levy et al., 2023),\nwith the exception of Mo\u00fbsai that was trained to generate\n44 sec. Hence, previous works are either slow at inference\ntime (autoregressive models) or cannot generate variable-\nlength, long-form audio (the rest). Our work relies on latent\n2\nFast Timing-Conditioned Latent Audio Diffusion\ndiffusion to generate long-form (up to 95 sec), variable-\nlength (controlled by the timing condition) stereo signals at\n44.1kHz in 8 sec on an A100 GPU (40GB VRAM).\nTiming conditioning \u2014 The use of learned embeddings to\ncondition music generation models on timing information\nwas introduced by Jukebox (Dhariwal et al., 2020), an au-\ntoregressive model conditioned with timing information on:\n(i) song duration, (ii) starting time of the training/generated\naudio sample within the song, and (iii) how much fraction\nof the song has elapsed. We are not aware of previous works\nusing timing conditioning for conditioning (latent) diffusion\nmodels. Our work employs timing conditioning to control\nthe length of the generations, enabling our latent diffusion\nmodels to generate variable-length outputs.\nEvaluation metrics \u2014 The commonly used quantitative au-\ndio metrics were developed for evaluating short-form mono\naudio generations at 16kHz (Kilgour et al., 2018; Copet\net al., 2023). Yet, our work focuses on generating long-\nform full-band stereo signals. Only Pascual et al. (2023)\nexplored quantitative metrics for evaluating full-band audio,\nalthough their focus was short-form mono signals. Our work\nexplores new quantitative metrics to evaluate long-form full-\nband stereo generations. Qualitative metrics assessing audio\nquality and text alignment are also prevalent in the literature\n(Dong et al., 2023; Copet et al., 2023; Ziv et al., 2024). Our\nwork also explores additional qualitative metrics to evaluate\nmusicality, stereo correctness, and musical structure.\nMultitask generative modeling \u2014 While generative mod-\nels have traditionally focused on specific tasks like speech,\nmusic or sound synthesis, recent works showed success in\naddressing all these tasks simultaneously (Yang et al., 2023;\nLiu et al., 2023b). Our work relies on one model to generate\nboth music and sound (not speech) given a text prompt.\n3. Architecture\nStable Audio is based on a latent diffusion model consisting\nof a variational autoencoder (Section 3.1), a conditioning\nsignal (Section 3.2), and a diffusion model (Section 3.3).\n3.1. Variational autoencoder (VAE)\nThe VAE (Kingma & Welling, 2013) compresses 44.1kHz\nstereo audio into an invertible (lossy) latent encoding that en-\nables faster generation and training time compared to work-\ning with raw audio samples. To allow for arbitrary-length\naudio encoding and decoding, we use a fully-convolutional\narchitecture (133M parameters) that follows the Descript\nAudio Codec (Kumar et al., 2023) encoder and decoder\n(without the quantizer). We found that the Snake activations\n(Ziyin et al., 2020) in the Descript Audio Codec architecture\nimproved audio reconstruction at high compression ratios\ncompared to alternatives such as EnCodec (D\u00e9fossez et al.,\n2022), at the expense of increased VRAM consumption.\nThe VAE is trained from scratch on our dataset and down-\nsamples the input stereo audio sequence by a factor of 1024,\nwith the resulting latent sequence having a channel dimen-\nsion of 64 (i.e., maps a 2\u00d7L input into 64\u00d7L/1024 latent).\nThis results in an overall data compression ratio of 32.\n3.2. Conditioning\nText encoder \u2014 To condition on text prompts, we use a\nCLAP text encoder trained from scratch on our dataset. We\nuse the actual setup recommended by the CLAP authors:\n(i) a HTSAT-based audio encoder with fusion having 31M\nparameters, and (ii) a RoBERTa-based text encoder of 110M\nparameters, both trained with a language-audio contrastive\nloss. We use CLAP embeddings (instead of the also com-\nmonly used T5 embeddings) because its multimodal nature\n(language-audio) allows the text features to contain some\ninformation about the relationships between words and au-\ndio. Further, in Section 6.2 we empirically note that the\nCLAP embeddings trained from scratch on our dataset can\noutperform the open-source CLAP and T5 embeddings. As\nshown by NovelAI (2022) when using CLIP (Radford et al.,\n2021) text features for Stable Diffusion (Rombach et al.,\n2022), the text features in the next-to-last layer of the text\nencoder can provide a better conditioning signal than the\ntext features from the final layer. Because of this, we use\nthe text features from the next-to-last hidden layer of the\nCLAP text encoder. These text features are provided to the\ndiffusion U-Net through cross-attention layers.\nTiming embeddings \u2014 We calculate two properties when\ngathering a chunk of audio from our training data: the sec-\nond from which the chunk starts (termed seconds_start)\nand the overall number of seconds in the original audio file\n(termed seconds_total), see Figure 2. For example, if we\ntake a 95 sec chunk from an 180 sec audio file with the\nchunk starting at 14 sec, then seconds_start is 14 and sec-\nonds_total is 180 (see Figure 2, Left). These values are then\ntranslated into per-second discrete learned embeddings2 and\nconcatenated along the sequence dimension with the text\nfeatures from the prompt conditioning before being passed\ninto the U-Net\u2019s cross-attention layers. For training with\naudio files shorter than the training window (see Figure 2,\nRight), we pad with silence up to the training window length.\nDuring inference, seconds_start and seconds_total are also\nprovided as conditioning, allowing the user to specify the\noverall length of the output audio. For example, given our\n95 sec model, setting seconds_start to 0 and seconds_total\nto 30 will create an output with 30 sec of audio followed by\n65 sec of silence. This method allows the user generating\nvariable-length music and sound effects.\n2We have a learnt, continuous timing embedding per second.\n3\nFast Timing-Conditioned Latent Audio Diffusion\nnoise\nprompt\nCLAP text\nencoder\ntiming\nprompt features\nlatents\ngenerated audio\nVAE Decoder\nseconds_start\nseconds\nstart\nembedder\nseconds_total\nseconds\ntotal\nembedder\nDiffusion U-Net\nFigure 1. Stable Audio. Blue: frozen pre-trained models. Green: parameters learnt during diffusion training. Purple: signals of interest.\nFull audio file\nTraining window \n(95 sec)\nseconds_start\nseconds_total\nFull audio file\nSilence padding\nseconds_total\nseconds_start\nTraining window \n(95 sec)\nFigure 2. Timing embeddings examples. Left: Audio file longer than training window. Right: Audio file shorter than training window.\n3.3. Diffusion model\nBased on a U-Net (907M parameters) inspired by Mo\u00fbsai\u2019s\narchitecture (Schneider et al., 2023), it consists of 4 levels\nof symmetrical downsampling encoder blocks and upsam-\npling decoder blocks, with skip connections between the\nencoder and decoder blocks providing a residual path at the\nsame resolution. The 4 levels have channel counts of 1024,\n1024, 1024, and 1280, and downsample by factors of 1 (no\ndownsampling), 2, 2, and 4 respectively. After the final\nencoder block, there is a 1280-channel bottleneck block.\nEach block consists of 2 convolutional residual layers fol-\nlowed by a series of self-attention and cross-attention layers.\nEach encoder or decoder block has three of these attention\nlayers, except for those in the first U-Net level, which only\nhave one. We rely on a fast and memory-efficient attention\nimplementation (Dao et al., 2022), to allow the model to\nscale more efficiently to longer sequence lengths. The diffu-\nsion timestep conditioning is passed in through FiLM layers\n(Perez et al., 2017) to modulate the model activations based\non the noise level. The prompt and timing conditioning in-\nformation is passed in to the model through cross-attention\nlayers. Further implementation details are in Appendix F.\n3.4. Inference\nOur sampling strategy during inference is based on the DPM-\nSolver++ (Lu et al., 2022), and we use classifier-free guid-\nance (with a scale of 6) as proposed by Lin et al. (2024). We\nuse 100 diffusion steps during inference, see Appendix A to\nknow more on how the number of steps was chosen. Stable\nAudio is designed for variable-length, long-form music and\nsound generation. This is achieved by generating content\nwithin a specified window length (95 sec), and relying on\nthe timing condition to fill the signal up to the length spec-\nified by the user and fill the rest with silence. To present\nvariable-length audios (shorter than window length) to the\nend user, one can simply trim the silence. In Section 6.3 we\nnote that the timing conditioning is very reliable, showing\nthe robustness of the proposed silence-trimming strategy.\n4. Training\n4.1. Dataset\nOur dataset consists of 806,284 audios (19,500 hours) con-\ntaining music (66% or 94%)3, sound effects (25% or 5%)3,\nand instrument stems (9% or 1%)3, with the corresponding\ntext metadata from the stock music provider AudioSparx.\n4.2. Variational autoencoder (VAE)\nIt was trained using automatic mixed precision for 1.1M\nsteps with an effective batch size of 256 on 16 A100 GPUs.\nAfter 460,000 steps the encoder was frozen and the decoder\nwas fine-tuned for an additional 640,000 steps. To ensure a\nconsistent stereo reconstruction, we use a multi-resolution\nsum and difference STFT loss designed for stereo signals\n(Steinmetz et al., 2020). To that end, we apply A-weighting\n(Fletcher & Munson, 2005) before the STFT and use win-\ndow lengths of 2048, 1024, 512, 256, 128, 64, and 32. We\nalso employ adversarial and feature matching losses using\na multi-scale STFT discriminator modified to accept stereo\naudio (D\u00e9fossez et al., 2022). The discriminators (set with\n3Percentages: number of files or GBs of content, respectively.\n4\nFast Timing-Conditioned Latent Audio Diffusion\n2048, 1024, 512, 256, and 128 STFT window lengths) use a\ncomplex STFT representation of the real and reconstructed\naudio, and a patch-based discriminative objective using the\nhinge loss (D\u00e9fossez et al., 2022). Each loss is weighted as\nfollows: 1.0 for spectral losses, 0.1 for adversarial losses,\n5.0 for the feature matching loss, and 1e-4 for the KL loss.\n4.3. Text encoder\nThe CLAP model was trained for 100 epochs on our dataset\nfrom scratch, with an effective batch size of 6,144 with 64\nA100 GPUs. We use the setup recommended by CLAP\nauthors and train it with a language-audio contrastive loss.\n4.4. Diffusion model\nIt was trained using exponential moving average and auto-\nmatic mixed precision for 640,000 steps on 64 A100 GPUs\nwith an effective batch size of 256. The audio was resampled\nto 44.1kHz and sliced to 4,194,304 samples (95.1 sec). Files\nlonger than this length were cropped from a random starting\npoint, while shorter files were padded at the end with silence.\nWe implemented a v-objective (Salimans & Ho, 2022) with\na cosine noise schedule and continuous denoising timesteps.\nWe apply dropout (10%) to the conditioning signals to be\nable to use classifier-free guidance. The text encoder is\nfrozen while training the diffusion model.\n4.5. Prompt preparation\nEach audio file in our dataset is accompanied by text meta-\ndata describing the audio file. This text metadata includes\nnatural-language descriptions of the audio file\u2019s contents,\nas well as domain-specific metadata such as BPM, genre,\nmoods, and instruments for music tracks. During the train-\ning of the text encoder and the diffusion model, we generate\ntext prompts from this metadata by concatenating a random\nsubset of the metadata as a string. This allows for specific\nproperties to be specified during inference, while not requir-\ning these properties to be present at all times. For half of the\nsamples, we include the metadata-type (e.g., Instruments or\nMoods) and join them with the | character (e.g. Instruments:\nGuitar, Drums, Bass Guitar|Moods: Uplifting, Energetic).\nFor the other half, we do not include the metadata-type and\njoin the properties with a comma (e.g. Guitar, Drums, Bass\nGuitar, Uplifting, Energetic). For metadata-types with a list\nof values, we shuffle the list.\n5. Methodology\n5.1. Quantitative metrics\nFDopenl3 \u2014 The Fr\u00e9chet Distance (FD) is utilized to eval-\nuate the similarity between the statistics of a generated au-\ndio set and a reference audio set in a feature space. A\nlow Fr\u00e9chet Distance implies that the generated audio is\nplausible and closely matches the reference audio (Kilgour\net al., 2018; Copet et al., 2023). While most previous works\nproject the audio into the VGGish feature space (Hershey\net al., 2017), we propose projecting it into the Openl34\nfeature space (Cramer et al., 2019). Importantly, Openl3\naccepts signals of up to 48kHz while VGGish operates at\n16kHz. With this modification, our FD is not limited to\nevaluate downsampled 16kHz audio but it can evaluate the\nfull bandwidth of the generated audios. Since we focus\non generating 44.1kHz audio, we resample all the evalua-\ntion audios to 44.1kHz. Finally, we also extend the FD to\nevaluate stereo signals. To that end, we project left- and\nright-channel audios into Openl3 features independently,\nand concatenate them to obtain the stereo features. If the\nevaluation audio is mono, we concatenate copied Openl3\n(mono) features to obtain the desired stereo features. Hence,\nwe propose a novel FDopenl3 metric to study the plausibility\nof the generated variable-length, full-band stereo signals.\nKLpasst \u2014 We use PaSST, a state-of-the-art audio tagger\ntrained on AudioSet (Koutini et al., 2022), to compute the\nKullback\u2013Leibler (KL) divergence over the probabilities\nof the labels between the generated and the reference au-\ndio (Copet et al., 2023). The generated audio is expected\nto share similar semantics (tags) with the reference audio\nwhen the KL is low. While most previous works focus on\ngenerating short snippets, our work focuses on generating\nlong-form audio. For this reason, we modify the KL to eval-\nuate audios of varying and longer lengths. This adaptation\ninvolves segmenting the audio into overlapping analysis\nwindows5. Subsequently, we calculate the mean (across\nwindows) of the generated logits and then apply a soft-\nmax. Finally, PaSST operates at 32kHz. To evaluate our\n44.1kHz models, we resample all the evaluation audios from\n44.1kHz to 32kHz. Hence, we propose a novel KLpasst met-\nric capable to evaluate the semantic correspondence between\nlengthy generated and reference audios up to 32kHz.\nCLAPscore \u2014 The cosine similarity is computed between\nthe CLAPLAION text embedding of the given text prompt and\nthe CLAPLAION audio embedding of the generated audio\n(Wu et al., 2023; Huang et al., 2023b). A high CLAPscore\ndenotes that the generated audio adheres to the given text\nprompt. Differently from previous works, that evaluate 10\nsec inputs, we use the \u2018feature fusion\u2019 variant of CLAPLAION\nto handle longer audios. It is based on \u2018fusing\u2019 (concatenat-\ning) inputs at various time-scales: a global input (downsam-\npled to be of 10 sec) is concatenated to 3 random crops (of\n4The Openl3 settings we use: mel256 input, 44.1kHz, \u2018music\u2019\nor \u2018env\u2019 content type depending if we evaluate music or audio,\nembedding size of 512, and hop size of 0.5 sec.\n5PaSST model was originally trained with 10 sec inputs, and\nwe utilize an analysis window of 10 sec (to match PaSST training)\nwith a 5 sec overlap (50% overlap, for compute efficiency).\n5\nFast Timing-Conditioned Latent Audio Diffusion\n10 sec) from the first, middle, and last parts of the audio.\nCLAPLAION audio embeddings are computed from 48kHz\naudio. To evaluate our 44.1kHz models, we resample all\nthe evaluation audios from 44.1kHz to 48kHz. Hence, we\npropose a novel CLAPscore to evaluate how 48kHz audios\nlonger than 10 sec adhere to a given text prompt.\nIn short, we adapted established metrics to assess the more\nrealistic use case of long-form full-band stereo generations.\nAll quantitative metrics can deal with variable-length inputs.\n5.2. Qualitative metrics\nAudio quality \u2014 We evaluate whether the generated audio\nis of low-fidelity with artifacts or high-fidelity.\nText alignment \u2014 We evaluate how the generated audio\nadheres to the given text prompt.\nMusicality (music only) \u2014 We evaluate the capacity of the\nmodel to articulate melodies and harmonies.\nStereo correctness (stereo only) \u2014 We evaluate the appro-\npriateness of the generated spatial image.\nMusical structure (music only) \u2014 We evaluate if the gen-\nerated song contains intro, development, and/or outro.\nWe collect human ratings for the metrics above and report\nmean opinion scores for audio quality, text alignment, and\nmusicality in the following scale: bad (0), poor (1), fair\n(2), good (3) and excellent (4). We observed that assessing\nstereo correctness posed a significant challenge for many\nusers. To address this, we streamlined the evaluation by\nseeking for a binary response: either stereo correctness or\nnot. Similarly, we adopted a binary approach for evaluating\nmusical structure. We ask users to determine whether the\ngenerated music exhibits some common structural elements\nof music (intro, development, outro) or not. For those binary\nresponses (stereo correctness and musical structure) we re-\nport percentages. Note that musicality and musical structure\nare only evaluated for music signals. For non-music (au-\ndio) signals we evaluate audio quality, text alignment and\nstereo correctness. Also note that stereo correctness is only\nevaluated for stereo signals. We relied on webMUSHRA\n(Schoeffler et al., 2018) to run our perceptual experiments.\nWe are not aware of previous works that qualitatively assess\nmusicality, stereo correctness, and/or musical structure.\n5.3. Evaluation data\nQuantitative experiments \u2014 We rely on the standard Mus-\nicCaps (Agostinelli et al., 2023) and AudioCaps (Kim et al.,\n2019) benchmarks. MusicCaps contains 5,521 music seg-\nments from YouTube, each with 1 caption (5,434 audios\nwere available for download). AudioCaps test set contains\n5Used checkpoint: \u2018630k-audioset-fusion-best\u2019.\n979 audio segments from YouTube, each with several cap-\ntions (881 audios were available for download, and it in-\ncludes 4,875 captions). For every model to evaluate, we gen-\nerate an audio per caption. This results in 5,521 generations\nfor the MusicCaps evaluations and 4,875 generations for the\nAudioCaps ones. While these benchmarks are not typically\nused for evaluating full-band stereo signals, the original data\nis predominantly stereo and full-band (Appendix B). We\nrely on the original data resampled to 44.1kHz to meet the\ntarget bandwidth of Stable Audio. Finally, since the stan-\ndard MusicCaps and AudioCaps segments are of 10 sec, we\nalso looked into the full-length audios to consider variable-\nlength long-form evaluation content. Yet, captions do not\nhold consistently throughout the whole (long) audio, as they\nonly accurately represent the intended 10 sec segment. As a\nresult, reference audios are of 10 sec while generated audios\nrange from 10 to 95 sec (Tables 1 and 2). Hence, in addition\nto modifying the established metrics to evaluate full-band\nstereo generations, it was also crucial to adapt the standard\ndatasets to align with our evaluation criteria.\nQualitative experiments \u2014 Prompts for qualitative evalua-\ntion were randomly picked from MusicCaps and AudioCaps.\nWe avoided prompts including \"low quality\" (or similar) to\nfocus on high-fidelity synthesis, avoided ambient music be-\ncause users found challenging to evaluate musicality, and\navoided speech-related prompts since it is not our focus.\n5.4. Baselines\nDirect comparisons with some models (e.g., Mo\u00fbsai or\nJEN1) is infeasible as their weights are not accessible. For\nthis reason, we benchmark against AudioLDM2, MusicGen,\nand AudioGen. These are state-of-the-art open-source mod-\nels representative of the current literature: latent diffusion\nmodels (AudioLDM2) or autoregressive models (MusicGen,\nAudioGen), that can be stereo (MusicGen-stereo) or mono,\nand at various sampling rates (see Table 1 and 2). The Au-\ndioLDM2 variants we evaluate are: \u2018AudioLDM2-48kHz\u2019\nthat was trained to generate full-band mono sounds and mu-\nsic, \u2018AudioLDM2-large\u2019 to generate 16kHz mono sounds\nand music, and \u2018AudioLDM2-music\u2019 that was trained on\nmusic only to generate 16kHz mono music (checkpoints6).\nThe MusicGen variants we evaluate are: \u2018MusicGen-small\u2019\nthat is a compute-efficient autoregressive model for music\ngeneration, \u2018MusicGen-large\u2019 that is its large variant, and\n\u2018MusicGen-large-stereo\u2019 that is its stereo version. However,\nMusicCaps includes vocal-related prompts and MusicGen\nmodels are not trained to generate vocals. In Appendix E we\nalso benchmark against MusicGen without vocal prompts.\nWe also evaluate \u2018AudioGen-medium\u2019, the only open-source\nautoregressive model available for sound synthesis.\n6The used checkpoints are \u2018audioldm_48k\u2019, \u2018audioldm2-full-\nlarge-1150k\u2019 and \u2018audioldm2-music-665k\u2019, respectively.\n6\nFast Timing-Conditioned Latent Audio Diffusion\noutput\ninference\nchannels/sr\nlength\nFDopenl3 \u2193\nKLpasst \u2193\nCLAPscore \u2191\ntime\nTraining data (upper bound)\n2/44.1kHz\nfull songs\n101.47\n-\n-\n-\nAutoencoded training data\n2/44.1kHz\nfull songs\n117.52\n-\n-\n-\nStable Audio w/ CLAPours\n2/44.1kHz\n23 sec\n118.09\n0.97\n0.44\n4 sec\nStable Audio w/ CLAPLAION\n2/44.1kHz\n23 sec\n123.30\n1.09\n0.43\n4 sec\nStable Audio w/ T5\n2/44.1kHz\n23 sec\n126.93\n1.06\n0.41\n4 sec\nAudioLDM2-music\n1/16kHz\n95 sec\n354.05\n1.53\n0.30\n38 sec\nAudioLDM2-large\n1/16kHz\n95 sec\n339.25\n1.46\n0.30\n37 sec\nAudioLDM2-48kHz\n1/48kHz\n95 sec\n299.47\n2.77\n0.22\n242 sec\nMusicGen-small\n1/32kHz\n95 sec\n205.65\n0.96\n0.33\n126 sec\nMusicGen-large\n1/32kHz\n95 sec\n197.12\n0.85\n0.36\n242 sec\nMusicGen-large-stereo\n2/32kHz\n95 sec\n216.07\n1.04\n0.32\n295 sec\nStable Audio\n2/44.1kHz\n95 sec\n108.69\n0.80\n0.46\n8 sec\nTable 1. Quantitative results on MusicCaps. Top: autoencoder audio fidelity study, discussed in Section 6.1. Middle: text encoder ablation\nstudy, discussed in Section 6.2. Bottom: comparing Stable Audio against the state-of-the-art, see Section 6.4. Different experiments (top,\nmiddle, bottom sections of the table) are not strictly comparable due to different output lengths. Underlines denote the best results in the\nmiddle section of the table, and bold indicates the best results in the bottom section.\noutput\ninference\nchannels/sr\nlength\nFDopenl3 \u2193\nKLpasst \u2193\nCLAPscore \u2191\ntime\nTraining data (upper bound)\n2/44.1kHz\nfull-length audio\n88.78\n-\n-\n-\nAutoencoded training data\n2/44.1kHz\nfull-length audio\n106.13\n-\n-\n-\nStable Audio w/ CLAPours\n2/44.1kHz\n23 sec\n114.25\n2.57\n0.16\n4 sec\nStable Audio w/ CLAPLAION\n2/44.1kHz\n23 sec\n119.29\n2.73\n0.19\n4 sec\nStable Audio w/ T5\n2/44.1kHz\n23 sec\n119.28\n2.69\n0.11\n4 sec\nAudioLDM2-large\n1/16kHz\n10 sec\n170.31\n1.57\n0.41\n14 sec\nAudioLDM2-48kHz\n1/48kHz\n10 sec\n101.11\n2.04\n0.37\n107 sec\nAudioGen-medium\n1/16kHz\n10 sec\n186.53\n1.42\n0.45\n36 sec\nStable Audio\n2/44.1kHz\n95 sec \u2020\n103.66\n2.89\n0.24\n8 sec\nTable 2. Quantitative results on AudioCaps. Top: autoencoder audio fidelity study, discussed in Section 6.1. Middle: text encoder ablation\nstudy, discussed in Section 6.2. Bottom: comparing Stable Audio against the state-of-the-art, see Section 6.4. Different experiments (top,\nmiddle, bottom sections of the table) are not strictly comparable due to different output lengths. \u2020 Stable Audio was trained to generate 95\nsec outputs, but during inference it can generate variable-length outputs by relying on the timing conditioning. Despite Stable Audio\ngenerating 95 sec outputs and the rest of state-of-the-art models generating 10 sec outputs, it is still significantly faster. We trim audios to\n10 sec (discarding the end silent part) for a fair quantitative evaluation against the state-of-the-art (see Section 3.4 for inference details).\nMusicCaps\nAudioCaps\nStable\nMusicGen\nMusicGen\nAudioLDM2\nStable\nAudioGen\nAudioLDM2\nAudio\nlarge\nstereo\n48kHz\nAudio\nmedium\n48kHz\nAudio Quality\n3.0\u00b10.7\n2.1\u00b10.9\n2.8\u00b10.7\n1.2\u00b10.5\n2.5\u00b10.8\n1.3\u00b10.4\n2.2\u00b10.9\nText Alignment\n2.9\u00b10.8\n2.4\u00b10.9\n2.4\u00b10.9\n1.3\u00b10.6\n2.7\u00b10.9\n2.5\u00b10.9\n2.9\u00b10.8\nMusicality\n2.7\u00b10.9\n2.0\u00b10.9\n2.7\u00b10.9\n1.5\u00b10.7\n-\n-\n-\nStereo correctness\n94.7%\n-\n86.8%\n-\n57%\n-\n-\nStructure: intro\n92.1%\n36.8%\n52.6%\n2.6%\n-\n-\n-\nStructure: development\n65.7%\n68.4%\n76.3%\n15.7%\n-\n-\n-\nStructure: outro\n89.4%\n26.3%\n15.7%\n2.6%\n-\n-\n-\nTable 3. Qualitative results. Top: mean opinion score \u00b1 standard deviation. Bottom: percentages. 19 users participated in this study.\n7\nFast Timing-Conditioned Latent Audio Diffusion\n6. Experiments\n6.1. How does our autoencoder impact audio fidelity?\nTo understand the reconstruction capabilities of our latent\nspace, we project a subset of training data (5,521 and 4,875\naudios in Table 1 and 2, respectively) through our autoen-\ncoder to obtain the latents and reconstruct from them. Then,\nwe compare the FDopenl3 of the real and the autoencoded\ntraining data with respect to the MusicCaps and AudioCaps\nevaluation audio (Tables 1 and 2). In both cases, the autoen-\ncoded training data yields slightly inferior results compared\nto the real training data. This indicates a marginal degrada-\ntion, yet informal listening suggests that the impact is fairly\ntransparent (examples available in our demo website).\n6.2. Which text encoder performs the best?\nVarious text encoders are prevalent in the literature, includ-\ning: the open-source CLAP (Wu et al., 2023) denoted here\nas CLAPLAION, privately trained CLAP-like models denoted\nhere as CLAPours (trained as in Section 4.3), and the open-\nsource T5 embeddings. An ablation study is conducted in\nTables 1 and 2 to determine which text encoder performs\nthe best. In this study, we train the base diffusion model\nin Section 4.4 for 350k steps with different text encoders\nand evaluate them using our qualitative metrics both on\nMusiccaps and AudiocCaps. The text encoders are frozen\nduring training. Results indicate comparable performance,\nwith CLAPours exhibiting a slight superiority, leading us to\nchoose it for further experimentation. The utilization of a\nprivately trained CLAP guarantees the use of text embed-\ndings trained on the same dataset as our diffusion model.\nThis approach ensures consistency across all components of\nthe model, mitigating distribution or vocabulary mismatches\nbetween the text embeddings and the diffusion model.\n6.3. How accurate is the timing conditioning?\nThe timing condition is evaluated by generating audios of\nvariable lengths (length controlled by the timing condition)\nto note its behavior across different length values (Figure 3).\nWe compare the expected length (provided by the timing\nconditioning) against the measured one, aiming for a diag-\nonal in Figure 3. We measure the length of the audio by\ndetecting when the signal becomes silence with a simple en-\nergy threshold\u2014because, e.g., a model with a 30 sec timing\ncondition is expected to fill the 95 sec window with 30 sec\nof signal plus 65 sec of silence. In Figure 3 we note that\nthe model is consistently generating audios of the expected\nlength, with more errors around 40-60 sec. This error might\nbe caused because there is less training data of this duration.\nAlso, note that some of the shortest measured lengths (seen\nin gray) may be false positives resulting from the simplistic\nsilence detector we use. Appendix C includes more results.\nFigure 3. Comparing the actual length (measured in the signal)\nagainst the expected length (provided by the timing conditioning).\n6.4. How does it compare with the state-of-the-art?\nThis section discusses Tables 1, 2, and 3. Stable Audio\ncan outperform the state-of-the-art in audio quality and also\nimproves text alignment in MusicCaps. Yet, text alignment\nis slightly worse in AudioCaps possibly due to the small\namount of sound effects in our training set (Section 4.1).\nIt is also very competitive at musicality and at generating\ncorrect stereo music signals. It\u2019s interesting, though, its low\nstereo correctness score in AudioCaps. It might be caused\nbecause the randomly selected prompts did not require much\nstereo movement, resulting in renders that are relatively non-\nspatial (see in our demo website). Despite this difficulty, the\nstereo render remained consistent without artefacts, leading\nto a stereo correctness score of 57%. Our demo website in-\ncludes more stereo sound examples. Finally, Stable Audio is\nalso capable to generate structured music: with intro, some\ndegree of development, and outro. Note that state-of-the-art\nmodels are not consistent at generating a coherent structure,\nsince they are mainly capable of developing musical ideas.\n6.5. How fast is it?\nWe compare inference times using one A100 GPU and a\nbatch size of 1. First, note that latent diffusion (AudioLDM2\nand Stable Audio) is significantly faster than autoregressive\nmodeling, as outlined in the introduction. Second, note that\nStable Audio (operating at stereo 44.1kHz) is also faster than\nAudioLDM2-large and -music (operating at mono 16kHz).\nStable Audio\u2019s speedup is even more significant when com-\npared to AudioLDM2-48kHz (operating at mono 48kHz)7.\n7. Conclusions\nOur latent diffusion model enables the rapid generation\nof variable-length, long-form stereo music and sounds at\n44.1kHz from textual and timing inputs. We explored novel\nqualitative and quantitative metrics for evaluating long-form\nfull-band stereo signals, and found Stable Audio to be a top\ncontender, if not the top performer, in two public bench-\nmarks. Differently from other state-of-the-art models, ours\ncan generate music with structure and stereo sound effects.\n7AudioLDM2-large and -music are implemented with Dif-\nfusers, 3x faster than the native implementation of the 48kHz\none. AudioLDM2 runs use the setup recommended by the authors.\n8\nFast Timing-Conditioned Latent Audio Diffusion\n8. Acknowledgments\nThanks to J. Parker and Z. Zukowski for their feedback, and\nto the qualitative study participants for their contributions.\n9. Impact statement\nOur technology represents a significant improvement in as-\nsisting humans with audio production tasks, offering the\ncapability to generate variable-length, long-form stereo mu-\nsic and sound effects based on text descriptions. This inno-\nvation expands the toolbox available to artists and content\ncreators, enriching their creativity. However, alongside its\npotential benefits, also confronts several inherent risks. One\nprominent concern lies in the reflection of biases present in\nthe training data. This raises questions about the appropriate-\nness of the technology for cultures underrepresented in the\ntraining dataset. Moreover, the contextual nature embedded\nin audio recordings and music emphasize the importance of\ncareful consideration and collaboration with stakeholders.\nIn light of these considerations, we commit to continued\nresearch and collaboration with stakeholders (like artists\nand data providers) to navigate the complex landscape of\nAI-based audio production responsibly.\nReferences\nAgostinelli, A., Denk, T. I., Borsos, Z., Engel, J., Verzetti,\nM., Caillon, A., Huang, Q., Jansen, A., Roberts, A.,\nTagliasacchi, M., Sharifi, M., Zeghidour, N., and Frank,\nC. Musiclm: Generating music from text. arXiv, 2023.\nBlattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim,\nS. W., Fidler, S., and Kreis, K. Align your latents: High-\nresolution video synthesis with latent diffusion models.\narXiv, 2023.\nBorsos, Z., Marinier, R., Vincent, D., Kharitonov, E.,\nPietquin, O., Sharifi, M., Roblek, D., Teboul, O., Grang-\nier, D., Tagliasacchi, M., et al. Audiolm: a language mod-\neling approach to audio generation. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing, 2023.\nChang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T.\nMaskgit: Masked generative image transformer. CCVPR,\n2022.\nCopet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve,\nG., Adi, Y., and D\u00e9fossez, A. Simple and controllable\nmusic generation. arXiv, 2023.\nCramer, A. L., Wu, H.-H., Salamon, J., and Bello, J. P.\nLook, listen, and learn more: Design choices for deep\naudio embeddings. ICASSP, 2019.\nDao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00e9, C. Flashat-\ntention: Fast and memory-efficient exact attention with\nio-awareness. arXiv, 2022.\nDhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A.,\nand Sutskever, I. Jukebox: A generative model for music.\narXiv, 2020.\nDonahue, C., McAuley, J., and Puckette, M. Adversarial\naudio synthesis. arXiv, 2018.\nDonahue, C., Caillon, A., Roberts, A., Manilow, E., Es-\nling, P., Agostinelli, A., Verzetti, M., Simon, I., Pietquin,\nO., Zeghidour, N., et al. Singsong: Generating musical\naccompaniments from singing. arXiv, 2023.\nDong, H.-W., Liu, X., Pons, J., Bhattacharya, G., Pascual,\nS., Serr\u00e0, J., Berg-Kirkpatrick, T., and McAuley, J. Clip-\nsonic: Text-to-audio synthesis with unlabeled videos and\npretrained language-vision models. arXiv, 2023.\nD\u00e9fossez, A., Copet, J., Synnaeve, G., and Adi, Y. High\nfidelity neural audio compression. arXiv, 2022.\nFletcher, H. and Munson, W. A. Loudness, Its Definition,\nMeasurement and Calculation. The Journal of the Acous-\ntical Society of America, 2005.\nForsgren, S. and Martiros, H. Riffusion - stable diffusion\nfor real-time music generation. 2022. URL https:\n//github.com/riffusion/riffusion.\nGarcia, H. F., Seetharaman, P., Kumar, R., and Pardo, B.\nVampnet: Music generation via masked acoustic token\nmodeling. arXiv, 2023.\nGhosal, D., Majumder, N., Mehrish, A., and Poria, S. Text-\nto-audio generation using instruction-tuned llm and latent\ndiffusion model. arXiv, 2023.\nHawthorne, C., Simon, I., Roberts, A., Zeghidour, N., Gard-\nner, J., Manilow, E., and Engel, J. Multi-instrument music\nsynthesis with spectrogram diffusion. arXiv, 2022.\nHershey, S., Chaudhuri, S., Ellis, D. P., Gemmeke, J. F.,\nJansen, A., Moore, R. C., Plakal, M., Platt, D., Saurous,\nR. A., Seybold, B., et al. Cnn architectures for large-scale\naudio classification. ICASSP, 2017.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. arXiv, 2020.\nHuang, Q., Jansen, A., Lee, J., Ganti, R., Li, J. Y., and Ellis,\nD. P. Mulan: A joint embedding of music audio and\nnatural language. ISMIR, 2022.\nHuang, Q., Park, D. S., Wang, T., Denk, T. I., Ly, A., Chen,\nN., Zhang, Z., Zhang, Z., Yu, J., Frank, C., Engel, J., Le,\nQ. V., Chan, W., Chen, Z., and Han, W. Noise2music:\nText-conditioned music generation with diffusion models.\narXiv, 2023a.\n9\nFast Timing-Conditioned Latent Audio Diffusion\nHuang, R., Huang, J., Yang, D., Ren, Y., Liu, L., Li, M.,\nYe, Z., Liu, J., Yin, X., and Zhao, Z. Make-an-audio:\nText-to-audio generation with prompt-enhanced diffusion\nmodels. arXiv, 2023b.\nKilgour, K., Zuluaga, M., Roblek, D., and Sharifi, M.\nFr\u00e9chet audio distance: A metric for evaluating music\nenhancement algorithms. arXiv, 2018.\nKim, C. D., Kim, B., Lee, H., and Kim, G. Audiocaps:\nGenerating captions for audios in the wild. Conference\nof the North American Chapter of the Association for\nComputational Linguistics, 2019.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. arXiv, 2013.\nKong, J., Kim, J., and Bae, J. Hifi-gan: Generative ad-\nversarial networks for efficient and high fidelity speech\nsynthesis. Advances in Neural Information Processing\nSystems, 2020.\nKoutini, K., Schl\u00fcter, J., Eghbal-zadeh, H., and Widmer,\nG. Efficient training of audio transformers with patchout.\nInterspeech, 2022.\nKreuk, F., Synnaeve, G., Polyak, A., Singer, U., D\u00e9fossez,\nA., Copet, J., Parikh, D., Taigman, Y., and Adi, Y. Audio-\ngen: Textually guided audio generation. arXiv, 2022.\nKumar, R., Seetharaman, P., Luebs, A., Kumar, I., and Ku-\nmar, K. High-fidelity audio compression with improved\nrvqgan. arXiv, 2023.\nLevy, M., Di Giorgi, B., Weers, F., Katharopoulos, A., and\nNickson, T. Controllable music production with diffusion\nmodels and guidance gradients. arXiv, 2023.\nLi, P., Chen, B., Yao, Y., Wang, Y., Wang, A., and Wang,\nA. Jen-1: Text-guided universal music generation with\nomnidirectional diffusion models. arXiv, 2023.\nLin, S., Liu, B., Li, J., and Yang, X. Common diffusion\nnoise schedules and sample steps are flawed. IEEE/CVF\nWinter Conference on Applications of Computer Vision,\n2024.\nLiu, H., Chen, Z., Yuan, Y., Mei, X., Liu, X., Mandic, D.,\nWang, W., and Plumbley, M. D. Audioldm: Text-to-audio\ngeneration with latent diffusion models. arXiv, 2023a.\nLiu, H., Tian, Q., Yuan, Y., Liu, X., Mei, X., Kong, Q.,\nWang, Y., Wang, W., Wang, Y., and Plumbley, M. D.\nAudioldm 2: Learning holistic audio generation with\nself-supervised pretraining. arXiv, 2023b.\nLu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-\nsolver++: Fast solver for guided sampling of diffusion\nprobabilistic models. arXiv, 2022.\nMariani, G., Tallini, I., Postolache, E., Mancusi, M., Cosmo,\nL., and Rodol\u00e0, E. Multi-source diffusion models for\nsimultaneous music generation and separation. arXiv,\n2023.\nMoliner, E., Lehtinen, J., and V\u00e4lim\u00e4ki, V. Solving audio\ninverse problems with a diffusion model. ICASSP, 2023.\nNovelAI. Novelai improvements on stable diffusion, Oct\n2022. URL https://shorturl.at/wW034.\nOord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O.,\nKavukcuoglu, K., Driessche, G., Lockhart, E., Cobo, L.,\nStimberg, F., et al. Parallel wavenet: Fast high-fidelity\nspeech synthesis. ICML, 2018.\nOord, A. v. d., Dieleman, S., Zen, H., Simonyan, K.,\nVinyals, O., Graves, A., Kalchbrenner, N., Senior, A.,\nand Kavukcuoglu, K. Wavenet: A generative model for\nraw audio. arXiv, 2016.\nParker, J., Spijkervet, J., Kosta, K., Yesiler, F., Kuznetsov,\nB., Wang, J.-C., Avent, M., Chen, J., and Le, D. Stemgen:\nA music generation model that listens. ICASSP, 2024.\nPascual, S., Bhattacharya, G., Yeh, C., Pons, J., and Serr\u00e0,\nJ. Full-band general audio synthesis with score-based\ndiffusion. ICASSP, 2023.\nPasini, M. and Schl\u00fcter, J. Musika! fast infinite waveform\nmusic generation. arXiv, 2022.\nPerez, E., Strub, F., de Vries, H., Dumoulin, V., and\nCourville, A. Film: Visual reasoning with a general\nconditioning layer. arXiv, 2017.\nPodell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,\nT., M\u00fcller, J., Penna, J., and Rombach, R. Sdxl: Im-\nproving latent diffusion models for high-resolution image\nsynthesis. arXiv, 2023.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\nJ., Krueger, G., and Sutskever, I. Learning transferable\nvisual models from natural language supervision. arXiv,\n2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n2020.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. arXiv, 2022.\n10\nFast Timing-Conditioned Latent Audio Diffusion\nRouard, S. and Hadjeres, G. Crash: Raw audio score-based\ngenerative modeling for controllable high-resolution\ndrum sound synthesis. arXiv, 2021.\nSalimans, T. and Ho, J. Progressive distillation for fast\nsampling of diffusion models. arXiv, 2022.\nSchneider, F., Jin, Z., and Sch\u00f6lkopf, B. Mo\u00fbsai: Text-\nto-music generation with long-context latent diffusion.\narXiv, 2023.\nSchoeffler, M., Bartoschek, S., St\u00f6ter, F.-R., Roess, M.,\nWestphal, S., Edler, B., and Herre, J. webmushra\u2014a\ncomprehensive framework for web-based listening tests.\nJournal of Open Research Software, 2018.\nSohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. arXiv, 2015.\nSteinmetz, C. J., Pons, J., Pascual, S., and Serr\u00e0, J. Au-\ntomatic multitrack mixing with a differentiable mixing\nconsole of neural audio effects. arXiv, 2020.\nVyas, A., Shi, B., Le, M., Tjandra, A., Wu, Y.-C., Guo, B.,\nZhang, J., Zhang, X., Adkins, R., Ngan, W., et al. Au-\ndiobox: Unified audio generation with natural language\nprompts. arXiv, 2023.\nWu, Y., Chen, K., Zhang, T., Hui, Y., Berg-Kirkpatrick, T.,\nand Dubnov, S. Large-scale contrastive language-audio\npretraining with feature fusion and keyword-to-caption\naugmentation. ICASSP, 2023.\nYang, D., Tian, J., Tan, X., Huang, R., Liu, S., Chang, X.,\nShi, J., Zhao, S., Bian, J., Wu, X., et al. Uniaudio: An au-\ndio foundation model toward universal audio generation.\narXiv, 2023.\nYao, Y., Li, P., Chen, B., and Wang, A. Jen-1 composer:\nA unified framework for high-fidelity multi-track music\ngeneration. arXiv, 2023.\nZiv, A., Gat, I., Lan, G. L., Remez, T., Kreuk, F., D\u00e9fossez,\nA., Copet, J., Synnaeve, G., and Adi, Y. Masked audio\ngeneration using a single non-autoregressive transformer.\narXiv, 2024.\nZiyin, L., Hartwig, T., and Ueda, M. Neural networks fail\nto learn periodic functions and how to fix it. arXiv, 2020.\n11\nFast Timing-Conditioned Latent Audio Diffusion\nA. Inference diffusion steps\nIn diffusion generative modeling, a critical consideration revolves around the trade-off between the quality of the generated\noutputs and the number of inference steps employed (quality vs inference-speed trade-off). Our results in Figure 4 show that\na significant portion of the overall improvement in output quality is achieved within the initial 50 inference steps, suggesting\ndiminishing returns with additional computational effort. Given that, we choose to set the total inference steps to 100. This\ndecision is undertaken with a precautionary approach, ensuring a sufficient number of time steps to guarantee certain quality\nin the generated outputs. Nevertheless, this implies the possibility of being more aggressive with the number of diffusion\nsteps to significantly accelerate our inference times without compromising much quality.\nFigure 4. Quality metrics vs Inference diffusion steps (trade-off).\nB. MusicCaps and AudioCaps: the original data from Youtube\nMusicCaps and AudioCaps benchmarks are not commonly used for evaluating full-band stereo signals, since most researchers\ntypically use mono versions at 16kHz of those datasets. However, the original data is predominantly stereo and full-band\n(see Figures 5 and 6). Provided that this data is easily available, we rely on the original data resampled at 44.1kHz to meet\nthe target bandwidth of Stable Audio. This approach ensures that our evaluation encompasses the richness inherent in the\noriginal stereo full-band signals, providing a more accurate representation of the model\u2019s performance under conditions\nreflective of real-world data.\nFigure 5. Statistics of the MusicCaps original data.\nFigure 6. Statistics of the AudioCaps original data.\n12\nFast Timing-Conditioned Latent Audio Diffusion\nC. Timing conditioning: additional evaluation\nIn Section 6.3 we observed that Stable Audio adheres to the timing conditioning to generate signals of the specified length.\nWe further study its behaviour by generating MusicCaps prompts at various lengths: 30, 60 and 90 sec. In Figure 7 we\ndepict the histogram of the measured lengths, clustered by the specified lengths (blue 30 sec, red 60 sec, and green 90 sec).\nAs in Section 6.3, we measure the length of the audio by detecting when the signal becomes silence with a simple energy\nthreshold. In this experiment we note that the timing conditioning is fairly precise, generating audios of the expected length,\nwith an error of a few seconds with a slight bias towards generating shorter audios. This means that the audio tends to finish\nright before the expected length, making it very appropriate to cut out the signal at the expected length. Also, note that some\nof the shortest measured lengths may be attributed to false positives resulting from the simplistic silence detector we use.\nFigure 7. Histogram depicting the measured lengths of MusicCaps captions.\nD. Related work: additional discussion on latent diffusion models\nMo\u00fbsai and JEN-1 are closely related to our work. Both target high-fidelity stereo music synthesis with latent diffusion\nmodels. Our work, however, differs from Mo\u00fbsai in several key aspects:\n\u2022 Mo\u00fbsai\u2019s latent is based on a spectrogram-based encoder and a diffusion decoder that requires 100 decoding steps,\nwhile ours in a fully-convolutional end-to-end VAE. This distinction is crucial in achieving our fast inference times.\n\u2022 Mo\u00fbsai\u2019s realtime factor is of \u00d71, while ours is of \u00d710.\n\u2022 Mo\u00fbsai uses a form of timing conditioning based on information about chunked audio files in the prompts (e.g. Electro\nHouse, 3 of 4), but we include explicit timing conditioning that allows for variable-length audio generation.\nOur work differs from JEN-1 in the following aspects:\n\u2022 JEN-1 relies on a masked autoencoder with a dimensionality reduced latent, and also on a omnidirectional latent\ndiffusion model trained in a multitask fashion. In contrast, Stable Audio is inherently simpler with no omnidirectional\nconstraints, no dimensionality reduction, and no multitask training. Our approach allows for an easier implementation\nand training while still being able to deliver state-of-the-art results.\n\u2022 JEN-1 is trained to generate 10 sec of music, not to generate variable-length, long-form music and sound effects.\nNote that Mo\u00fbsai and JEN-1 target music synthesis while we target both music and sounds synthesis with a single model.\nAudioLDM2 is also closely related to our work. It is a latent diffusion model capable to generate mono speech, sound\neffects, and music up to 48kHz. Although the original AudioLDM2 was designed to operate at 16kHz, a recent release\noperates at 48kHz. Our work, however, differs from AudioLDM2 in several key aspects:\n\u2022 AudioLDM2 relies on a shared representation for music, audio, and speech to condition the latent diffusion model.\nThis representation is shared between an audio masked auto encoder (audioMAE) and a GPT-2 that takes audio, text,\nspeech transcripts, and images. As Stable Audio was not trained for speech generation or image-to-audio, there\u2019s no\nneed to incorporate the intricacies of GPT-2. Instead, we opt for a CLAP text encoder.\n13\nFast Timing-Conditioned Latent Audio Diffusion\n\u2022 Stable Audio is faster and outperforms AudioLDM2 in audio quality and on text alignement for music generation. Yet,\nAudioLDM2 outperforms Stable Audio on text alignement for sound effects generation (see Tables 1, 2, and 3).\nMo\u00fbsai, JEN-1, and AudioLDM2 use the open-source, pretrained T5 or FLAN-T5 text embeddings, and we use CLAP text\nembeddings trained on the same dataset as our diffusion model. Our approach ensures consistency across all components of\nthe model, eliminating distribution (or vocabulary) mismatches between the text embeddings and the diffusion model.\nE. Additional MusicCaps results: quantitative evaluation without singing-voice prompts\nMusicCaps includes vocal-related prompts but MusicGen\u2019s released weights (used for benchmarking) are not trained to\ngenerate vocals8. To allow for a fair evaluation against MusicGen, we also evaluate the models in Table 1 with a subset of\n2184 prompts that do not include vocals9. In Table 4, we observe results akin to those in Table 1: Stable Audio consistently\nobtains better results than the rest (with the exception of MusicGen-large that obtains comparable KLpasst scores to ours).\noutput\ninference\nchannels/sr\nlength\nFDopenl3 \u2193\nKLpasst \u2193\nCLAPscore \u2191\ntime\nAudioLDM2-music\n1/16kHz\n95 sec\n354.37\n1.66\n0.32\n38 sec\nAudioLDM2-large\n1/16kHz\n95 sec\n349.67\n1.66\n0.32\n37 sec\nAudioLDM2-48kHz\n1/48kHz\n95 sec\n296.46\n3.15\n0.22\n242 sec\nMusicGen-small\n1/32kHz\n95 sec\n186.28\n1.02\n0.34\n126 sec\nMusicGen-large\n1/32kHz\n95 sec\n176.54\n0.86\n0.37\n242 sec\nMusicGen-large-stereo\n2/32kHz\n95 sec\n196.66\n1.08\n0.33\n295 sec\nStable Audio\n2/44.1kHz\n95 sec\n100.29\n0.87\n0.44\n8 sec\nTable 4. Quantitative results on MusicCaps without singing-voice prompts. Scores highlighted in bold indicate which are the best results.\nF. Implementation details\nCode to reproduce Stable Audio can be found online: https://github.com/Stability-AI/stable-audio-tools.\nThe configuration file used for training and defining our VAE autoencoder is available online.\nThe configuration file used for training and defining our latent diffusion model is available online.\nThe provided configuration files offer compact descriptions of the architecture and implementation of Stable Audio. These\nconfigurations serve as additional resources for comprehensively understanding the underlying implementation of Stable\nAudio.\nCode to reproduce our metrics can be found online: https://github.com/Stability-AI/stable-audio-metrics.\nWe relied on the code shared by the CLAP authors (Wu et al., 2023) to train our text encoder with our private dataset:\nhttps://github.com/LAION-AI/CLAP\n8Vocals were removed from their training data source using the corresponding tags, and then using a music source separation method.\n9Prompts containing any of those words were removed: speech, speech synthesizer, hubbub, babble, singing, male, man, female,\nwoman, child, kid, synthetic singing, choir, chant, mantra, rapping, humming, groan, grunt, vocal, vocalist, singer, voice, and acapella.\n14\n"
  },
  {
    "title": "TP-Aware Dequantization",
    "link": "https://arxiv.org/pdf/2402.04925.pdf",
    "upvote": "3",
    "text": "TP-AWARE DEQUANTIZATION\nAdnan Hoque\nIBM T.J. Watson Research Center\nYorktown Heights, NY, United States\nadnan.hoque1@ibm.com\nMudhakar Srivatsa\nIBM T.J. Watson Research Center\nYorktown Heights, NY, United States\nmsrivats@us.ibm.com\nChih-Chieh Yang\nIBM T.J. Watson Research Center\nYorktown Heights, NY, United States\nchih.chieh.yang@ibm.com\nRaghu Ganti\nIBM T.J. Watson Research Center\nYorktown Heights, NY, United States\nrganti@us.ibm.com\nABSTRACT\nIn this paper, we present a novel method that reduces model inference latency during distributed\ndeployment of Large Language Models (LLMs). Our contribution is an optimized inference deploy-\nment scheme that address the current limitations of state-of-the-art quantization kernels when used in\nconjunction with Tensor Parallel (TP). Our method preserves data locality in GPU memory access\npatterns and exploits a priori knowledge of TP to reduce global communication. We demonstrate\nan up to 1.81x speedup over existing methods for Llama-70B and up to 1.78x speedup for IBM\nWatsonX\u2019s Granite-20B MLP layer problem sizes on A100 and H100 NVIDIA DGX Systems for a\nvariety of TP settings.\nKeywords Deep Learning \u00b7 Foundation Models \u00b7 Quantization \u00b7 Tensor Parallel \u00b7 GPU \u00b7 Optimization \u00b7 LLM \u00b7\nTransformers \u00b7 Distributed Systems\n1\nIntroduction\nGiven the recent advancement of LLMs, deployment optimizations are becoming more crucial as the size of state-of-\nthe-art LLMs increase in scale. As these these models continue to grow, so does the need to optimize the increasingly\nparallel and increasingly distributed workload requirements of modern-day deep learning inference. Strategies like\nGPTQ [1] and Tensor Parallel (TP) [4] are hence essential in achieving high-throughput performance. Our method\nis motivated by several key properties of GPTQ, TP and General Matrix Multiplication (GEMM). We build on these\nexisting methods and present a key innovation that helps maximize memory throughput and reduce latency. Our method\nshows up to a 1.81x speedup on Llama-70B and up to a 1.78x speedup on Granite-20B MLP layer problem sizes. We\nachieve this by reducing global communication and enforcing data locality.\n1.1\nMotivation\nWe begin by introducing the grouping scheme in GPTQ-style quantization. The group size defines the quanta that the\nquantization strategy will be performed on. Thus, every group size number of input channels in the K x N weight matrix\nwill share the same quantization metadata (scales and zeros). This introduces the need to spatially relate the weight\nmatrix to the aforementioned metadata. This mapping must be remembered, as it will be used during deployment when\nperforming dequantization. If we were to proceed with the basic formulation of GPTQ, the rows of the weight matrix\nare related to the metadata by a group index array. In this paper, we will refer to this basic formulation of the group\nindex array as the naive approach and it can be evaluated as the following, where we let G equal the group size:\ngidx_naive[i] =\n\u0016 i\nG\n\u0017\n,\nfor i = 0, 1, 2, . . . , K \u2212 1\n(1)\narXiv:2402.04925v1  [cs.DC]  15 Jan 2024\nPRIME AI paper\nNotably, this approach does not consider further optimizations introduced by the GPTQ authors. Our method targets the\ncomplexities that arrive when considering the \"Activation Order\" optimization. This parameter was introduced by the\nauthors to improve model accuracy [2] and is triggered through an optional flag act_order in the GPTQ package. When\nthis flag is set to True, the quantization process will behave differently. This optimization reduces overall quantization\nerror by processing the rows of the weight matrix in a way that respects the significance of the weights impacts on\nmodel accuracy. Thus, the weights with higher impact on accuracy will incur less quantization error, and vice versa for\nweights with lower impact on accuracy. The implication is then, with act_order (also known as desc_act in different\npackages), the rows of the weight matrix are reordered based on the properties discussed above and notably, the new\nordering must be respected when performing inference during dequantization for each row of the weight matrix and\nit\u2019s corresponding metadata. This transformation is realized through the group index array. Here, we use a random\npermutation function \u03d5 to emulate an arbitrary reordering. The new group index array can then be expressed by the\nfollowing expression:\nLet \u03d5 : {0, 1, . . . , K \u2212 1} \u2192 {0, 1, . . . , K \u2212 1} be a random permutation function.\n(2)\ngidx_actorder[i] =\n\u0016\u03d5(i)\nG\n\u0017\n,\nfor i = 0, 1, 2, . . . , K \u2212 1\n(3)\nThis approach however, introduces performance overhead due to the sub-optimal memory access pattern of having to\nfrequently reload quantization metadata [2] due to the unordered nature of the group index array. Further, when this\noptimization is used in a TP setting this method incurs communication overhead, a detail we expand on in the next\nsection. Our method is then, a way to reap the benefits of the accuracy boost provided by the act_order optimization\nwithout reducing memory throughput and incurring additional communication overhead.\n2\nMethod\n2.1\nEnforcing Data Locality\nPopular software packages that implement the GPTQ algorithm [7] will store the weights of the model on disk without\nincluding knowledge of the ordering suggested by Equation 3. Thus, we have a choice to use the mapping in Equation 3\nto correctly map rows of the weight matrix to it\u2019s corresponding metadata as seen in Figure 1 during model deployment.\nAs discussed previously, this leads to a sub-optimal memory access pattern. The method described in this section, is\nused in the state-of-the-art ExllamaV2 [6] kernel to enforce data locality by transforming the unordered group index\narray resulting from the act_order flag. To illustrate this method, we define a routine that takes as input the group index\narray, and returns an ordered group index array, gidx_optimized as well as it\u2019s associated permutation array P, using the\ntorch.argsort [5] function.\nAlgorithm 1 Reorder Function\n1: function REORDER(gidx_actorder)\n\u25b7 gidx_actorder is a 1D array containing unordered indices\n2:\nP \u2190 ARGSORT(gidx_actorder)\n\u25b7 P is a 1D array containing the permutation\n3:\ngidx_optimized \u2190 gidx_actorder[P]\n\u25b7 Permute gidx_actorder\n4:\nreturn P, gidx_optimized\n\u25b7 gidx_optimized is a 1D array containing ordered indices\n5: end function\nThe optimized group index array has the desirable property of enforcing data locality during model inference. Consider\nFigure 1 and Figure 2. In Figure 2 we see that for multiple rows of the weight matrix W we are able to reuse the\nassociated metadata, as the indices have been sorted such that it guarantees all members of the the same group are\nconsecutive in W.\nFigure 1: Naive Load with Activation Order Flag\nFigure 2: Optimized Load with Activation Order Flag\n2\nPRIME AI paper\nThe optimized loading scheme increases data locality and memory throughput. The reordering of the weight matrices\ncan be performed offline, and thus the permutation array P along with the optimized group index array are available to\nthe model inference pipeline as global variables. If we will be deploying with TP model parallelism these parameters\nmust be passed and sharded to our input activations X, model weights W and quantization metadata as they will be used\nto perform the correct mapping during dequantization. However, with no further optimization, this strategy that we\nwill refer to as the Naive Algorithm, will incur an extra AllGather communication necessitated by the observation that\nthe output of the Column-TP layer in Figure 4 will require reordering, as seen in Line 2 of Algorithm 2, to produce a\ncorrectly aligned tensor for the subsequent GEMM operation.\nThis leads us to our main contribution, which is a method to reduce global communication and thus reduce latency\nduring model inference.\n2.2\nMinimizing Global Communication\nIn Transformer based architectures, we typically observe that popular implementations consist of 2 linear layers in the\nAttention block and 2 linear layers in the Multilayer Perceptron Layer (MLP) block as depicted in Figure 3. We follow\nthe same model parallel strategy outlined in Megatron-LM [3]. This is to say, our method assumes the interleaved\npattern of Column-TP with Row-TP as seen in Figure 4 for consecutive linear layers. In dequantization, we now have\ntwo choices. If we proceed with the optimized formulation of GPTQ with act_order=True, we must pass either the\ngroup index array outlined in Equation 3 or the more optimal cache-friendly group index array from Algorithm 1, used\nby the ExllamaV2 kernel.\nIf we proceed with the latter, during model inference, an expensive AllGather communication is required before the\noutput shards of the first linear layer Y1 are passed to the sharded weights of the second linear layer. Our insight, is then\na method that avoids this communication using the intrinsic properties of GEMM.\nFigure 3: Transformer Block\nFigure 4: TP-Aware Model Parallelism\nTo motivate our method, we examine the current approach with the optimized group index array used by ExllamaV2. We\nfirst note, the interleaved Column-TP and Row-TP depicted in Figure 4 will produce two sharded weight matrices, W1,\nwhich is split column-wise across N ranks, and W2 which is split row-wise across N ranks. During dequantization, we\nwill produce two permutation arrays, P1 and P2. Recall these arrays are optimized in that they enforce data locality\nwhen they are used to reorder the weights and load quantization metadata (scales and zeros). Notably, these two\npermutation arrays are available globally and can be computed offline.\nWe define Algorithm 2, and call it the Naive Algorithm to illustrate the above approach. We assume, activations, X1,\nthe weight matrices W1 and W2 and permutation arrays P1 and P2 are available as input to the model. For a given\nmatrix M, we use the following notation M[P1, P2] to denote row and column permutations.\nNoting the global communication required in Line 2 of Algorithm 2, we present a reordering strategy that avoids this\ncommunication across ranks and hence yields impressive latency improvements. We achieve this by by re-ordering the\ncolumns of W1 with the permutation array P2. This optimization is realized from the following insight.\nBy permuting the columns of W1 with P2 and then proceeding with the GEMM as is done Line 1 of Algorithm 3 we\nare aligning Y 1 in such a way it no longer needs to be permuted with P2 in the subsequent GEMM with W2. Producing\nthis alignment, is our key contribution, as it makes it possible to avoid global communication after the column-TP layer.\nNote that our method as it stands, only applies to the MLP layers of the Transformer block. This is due to the fact that\nthe sharding strategy for Attention when employing MHA (Multi-Headed Attention), MQA (Multi-Query Attention) or\nGQA (Group-Query Attention) motivates the need for additional tricks to avoid global communication in TP, when the\nweights are reordered. We call our approach that applies to the MLP layers in the Transformer Block the TP-Aware\nAlgorithm.\n3\nPRIME AI paper\nAlgorithm 2 Naive Algorithm\nRequire: X1, W1[P1], W2[P2]\n\u25b7 Input activations X1 and sharded reordered weight matrices W1 and W2\nRequire: P1, P2\n\u25b7 Permutation arrays\nRequire: rank, size\n\u25b7 Processor rank and total number of processors\n1: Y 1local \u2190 X1global[:, P1] @ W1local\n\u25b7 GEMM\n2: Y 1global \u2190 ALLGATHER(Y 1local)\n\u25b7 Gather Y 1 shards from all processors\n3: Y 1global \u2190 Y 1global[:, P2]\n\u25b7 Y1 reordered globally\n4: Y 1local \u2190 CHUNK(Y 1global, rank, size, dim = 1)\n\u25b7 Shard Y 1 among processors\n5: Y 2local \u2190 Y 1local @ W2local\n\u25b7 GEMM with gathered Y 1\n6: Y 2global \u2190 ALLREDUCE(Y 2local, op = SUM)\n\u25b7 Reduce Y 2 shards across all processors\n7: return Y 2global\n\u25b7 Each processor has the globally reduced result\nAlgorithm 3 TP-Aware Algorithm\nRequire: X1, W1[P1, P2], W2[P2]\n\u25b7 Input activations X1 and sharded reordered weight matrices W1 and W2\nRequire: P1\n\u25b7 Permutation array\nRequire: rank, size\n\u25b7 Processor rank and total number of processors\n1: Y 1local \u2190 X1global[:, P1] @ W1local\n\u25b7 GEMM\n2: Y 2local \u2190 Y 1local @ W2local\n\u25b7 GEMM with local Y 1\n3: Y 2global \u2190 ALLREDUCE(Y 2local, op = SUM)\n\u25b7 Reduce Y 2 shards across all processors\n4: return Y 2global\n\u25b7 Each processor has the globally reduced result\n3\nExperiments\nWe conducted our experiments on a variety of TP settings and batch sizes, on highly relevant Llama-70B and Granite-\n20B, the flagship offering in IBM WatsonX, MLP layer sizes. We tested on enterprise A100x8 with Intel(R) Xeon(R)\nPlatinum 8358 CPU @ 2.60 and H100x8 with Intel(R) Xeon(R) Platinum 8480 CPU @ 3.00 GHz NVIDIA DGX\nsystems. As our algorithm avoids communication in-between column-TP and row-TP layers we use FP16 to demonstrate\nthis benefit. For our experimental results we denote M as the batch size, K1 and N1 as the input and output features\nrespectively of the column-TP layer and N2 as the input features of the row-TP layer and have enabled TP using 1,2,4,8\nGPUs in one node. As a simplification for the analysis, in the Llama-70B test case we assume single upproj layer\nfollowed by downproj which allows us to directly compare with Granite\u2019s MLP layer. Our method can be generalized\nto the implementation in practice where a gateproj layer is also present.\n4\nLlama-70B\nLatency Llama-70B\n20\n21\n22\n23\n24\nbatch_size\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\ntime_diff\nLlama-70B_MLP.txt\nnp\n1\n2\n4\n8\nFigure 5: Latency Difference for Llama-70B, A100\nSpeedup Llama-70B\n20\n21\n22\n23\n24\nbatch_size\n1.0\n1.2\n1.4\n1.6\n1.8\nspeedup\nLlama-70B_MLP.txt\nnp\n1\n2\n4\n8\nFigure 6: Speedup for Llama-70B, A100\n4\nPRIME AI paper\n4.1\nBaseline TP=1\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\n1\n(8192, 28672, 8192)\n0.696\n0.688\n2\n(8192, 28672, 8192)\n0.694\n0.683\n4\n(8192, 28672, 8192)\n0.685\n0.678\n8\n(8192, 28672, 8192)\n0.706\n0.697\n16\n(8192, 28672, 8192)\n0.710\n0.695\nTable 1: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Llama-70B, TP=1, A100)\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\n1\n(8192, 28672, 8192)\n0.489\n0.481\n2\n(8192, 28672, 8192)\n0.471\n0.466\n4\n(8192, 28672, 8192)\n0.474\n0.468\n8\n(8192, 28672, 8192)\n0.471\n0.464\n16\n(8192, 28672, 8192)\n0.474\n0.468\nTable 2: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Llama-70B, TP=1, H100)\n4.2\nTP=2\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\nSpeedup\n1\n(8192, 28672, 8192)\n0.493\n0.433\n1.14x\n2\n(8192, 28672, 8192)\n0.508\n0.407\n1.25x\n4\n(8192, 28672, 8192)\n0.519\n0.412\n1.26x\n8\n(8192, 28672, 8192)\n0.516\n0.418\n1.23x\n16\n(8192, 28672, 8192)\n0.501\n0.416\n1.20x\nTable 3: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Llama-70B, TP=2, A100)\nAverage Speedup\n1.22x\nTable 4: Average Speedup for Llama-70B, TP=2, A100\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\nSpeedup\n1\n(8192, 28672, 8192)\n0.302\n0.283\n1.07x\n2\n(8192, 28672, 8192)\n0.316\n0.285\n1.11x\n4\n(8192, 28672, 8192)\n0.323\n0.286\n1.13x\n8\n(8192, 28672, 8192)\n0.320\n0.289\n1.11x\n16\n(8192, 28672, 8192)\n0.322\n0.289\n1.11x\nTable 5: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Llama-70B, TP=2, H100)\nAverage Speedup\n1.11x\nTable 6: Average Speedup for Llama-70B, TP=2, H100\n5\nPRIME AI paper\n4.3\nTP=4\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\nSpeedup\n1\n(8192, 28672, 8192)\n0.472\n0.282\n1.67x\n2\n(8192, 28672, 8192)\n0.512\n0.286\n1.79x\n4\n(8192, 28672, 8192)\n0.513\n0.287\n1.79x\n8\n(8192, 28672, 8192)\n0.518\n0.285\n1.82x\n16\n(8192, 28672, 8192)\n0.512\n0.286\n1.79x\nTable 7: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Llama-70B, TP=4, A100)\nAverage Speedup\n1.78x\nTable 8: Average Speedup for Llama-70B, TP=4, A100\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\nSpeedup\n1\n(8192, 28672, 8192)\n0.258\n0.192\n1.34x\n2\n(8192, 28672, 8192)\n0.275\n0.192\n1.43x\n4\n(8192, 28672, 8192)\n0.273\n0.193\n1.41x\n8\n(8192, 28672, 8192)\n0.278\n0.197\n1.41x\n16\n(8192, 28672, 8192)\n0.281\n0.198\n1.42x\nTable 9: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Llama-70B, TP=4, H100)\nAverage Speedup\n1.40x\nTable 10: Average Speedup for Llama-70B, TP=4, H100\n4.4\nTP=8\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\nSpeedup\n1\n(8192, 28672, 8192)\n0.495\n0.284\n1.74x\n2\n(8192, 28672, 8192)\n0.503\n0.276\n1.82x\n4\n(8192, 28672, 8192)\n0.539\n0.291\n1.85x\n8\n(8192, 28672, 8192)\n0.530\n0.286\n1.85x\n16\n(8192, 28672, 8192)\n0.512\n0.286\n1.79x\nTable 11: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Llama-70B, TP=8, A100)\nAverage Speedup\n1.81x\nTable 12: Average Speedup for Llama-70B, TP=8, A100\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\nSpeedup\n1\n(8192, 28672, 8192)\n0.245\n0.144\n1.70x\n2\n(8192, 28672, 8192)\n0.256\n0.146\n1.75x\n4\n(8192, 28672, 8192)\n0.257\n0.144\n1.78x\n8\n(8192, 28672, 8192)\n0.258\n0.145\n1.78x\n16\n(8192, 28672, 8192)\n0.266\n0.149\n1.78x\nTable 13: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Llama-70B, TP=8, H100)\n6\nPRIME AI paper\nAverage Speedup\n1.76x\nTable 14: Average Speedup for Llama-70B, TP=8, H100\n5\nGranite-20B\nLatency Granite-20B\n20\n21\n22\n23\n24\nbatch_size\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\ntime_diff\nGranite-20B_MLP.txt\nnp\n1\n2\n4\n8\nFigure 7: Latency Difference for Granite-20B, A100\nSpeedup Granite-20B\n20\n21\n22\n23\n24\nbatch_size\n1.0\n1.2\n1.4\n1.6\n1.8\nspeedup\nGranite-20B_MLP.txt\nnp\n1\n2\n4\n8\nFigure 8: Speedup for Granite-20B, A100\n5.1\nBaseline TP=1\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\n1\n(6144, 24576, 6144)\n0.482\n0.474\n2\n(6144, 24576, 6144)\n0.476\n0.471\n4\n(6144, 24576, 6144)\n0.482\n0.469\n8\n(6144, 24576, 6144)\n0.479\n0.467\n16\n(6144, 24576, 6144)\n0.487\n0.475\nTable 15: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Granite-20B, TP=1)\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\n1\n(6144, 24576, 6144)\n0.349\n0.341\n2\n(6144, 24576, 6144)\n0.335\n0.328\n4\n(6144, 24576, 6144)\n0.325\n0.319\n8\n(6144, 24576, 6144)\n0.335\n0.327\n16\n(6144, 24576, 6144)\n0.335\n0.328\nTable 16: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Granite-20B, TP=1, H100)\n7\nPRIME AI paper\n5.2\nTP=2\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\nSpeedup\n1\n(6144, 24576, 6144)\n0.486\n0.309\n1.57x\n2\n(6144, 24576, 6144)\n0.476\n0.471\n1.01x\n4\n(6144, 24576, 6144)\n0.482\n0.469\n1.03x\n8\n(6144, 24576, 6144)\n0.479\n0.467\n1.03x\n16\n(6144, 24576, 6144)\n0.504\n0.306\n1.65x\nTable 17: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Granite-20B, TP=2, A100)\nAverage Speedup\n1.26x\nTable 18: Average Speedup for Granite-20B, TP=2, A100\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\nSpeedup\n1\n(6144, 24576, 6144)\n0.263\n0.214\n1.23x\n2\n(6144, 24576, 6144)\n0.279\n0.218\n1.28x\n4\n(6144, 24576, 6144)\n0.284\n0.220\n1.29x\n8\n(6144, 24576, 6144)\n0.285\n0.220\n1.29x\n16\n(6144, 24576, 6144)\n0.285\n0.221\n1.29x\nTable 19: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Granite-20B, TP=2, H100)\nAverage Speedup\n1.28x\nTable 20: Average Speedup for Granite-20B, TP=2, H100\n5.3\nTP=4\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\nSpeedup\n1\n(6144, 24576, 6144)\n0.500\n0.292\n1.71x\n2\n(6144, 24576, 6144)\n0.497\n0.284\n1.75x\n4\n(6144, 24576, 6144)\n0.518\n0.293\n1.77x\n8\n(6144, 24576, 6144)\n0.508\n0.284\n1.79x\n16\n(6144, 24576, 6144)\n0.530\n0.290\n1.83x\nTable 21: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Granite-20B, TP=4, A100)\nAverage Speedup\n1.77x\nTable 22: Average Speedup for Granite-20B, TP=4, A100\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\nSpeedup\n1\n(6144, 24576, 6144)\n0.251\n0.156\n1.61x\n2\n(6144, 24576, 6144)\n0.267\n0.157\n1.70x\n4\n(6144, 24576, 6144)\n0.268\n0.158\n1.70x\n8\n(6144, 24576, 6144)\n0.269\n0.159\n1.69x\n16\n(6144, 24576, 6144)\n0.269\n0.159\n1.69x\nTable 23: Naive and TP Aware Model Algorithm for Various Batch Sizes (Granite-20B, TP=4, H100)\n8\nPRIME AI paper\nAverage Speedup\n1.68x\nTable 24: Average Speedup for Granite-20B, TP=4, H100\n5.4\nTP=8\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\nSpeedup\n1\n(6144, 24576, 6144)\n0.512\n0.294\n1.74x\n2\n(6144, 24576, 6144)\n0.530\n0.291\n1.82x\n4\n(6144, 24576, 6144)\n0.537\n0.293\n1.83x\n8\n(6144, 24576, 6144)\n0.541\n0.305\n1.77x\n16\n(6144, 24576, 6144)\n0.551\n0.303\n1.82x\nTable 25: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Granite-20B, TP=8, A100)\nAverage Speedup\n1.80x\nTable 26: Average Speedup for Granite-20B, TP=8, A100\nM\nK1, N1, N2\nNaive Algorithm (ms)\nTP Aware Algorithm (ms)\nSpeedup\n1\n(6144, 24576, 6144)\n0.252\n0.148\n1.70x\n2\n(6144, 24576, 6144)\n0.255\n0.142\n1.79x\n4\n(6144, 24576, 6144)\n0.259\n0.141\n1.84x\n8\n(6144, 24576, 6144)\n0.257\n0.140\n1.84x\n16\n(6144, 24576, 6144)\n0.255\n0.140\n1.82x\nTable 27: Naive and TP Aware Algorithm Latencies for Various Batch Sizes (Granite-20B, TP=8, H100)\nAverage Speedup\n1.78x\nTable 28: Average Speedup for Granite-20B, TP=8, H100\nFor Llama-70B problem sizes, our TP-Aware method demonstrates an average speedup of 1.22x, 1.78x and 1.81x for\nTP = 2, 4, 8 respectively on the 8xA100 system. On the 8xH100 system our method demonstrated an average speedup\nof 1.11x, 1.40x and 1.76x. For Granite-20B problem sizes, we achieve an average speedup of 1.26x, 1.77x and 1.80x on\nthe A100 and 1.28x, 1.68x and 1.78x on the H100.\nNotably, as the number of ranks increased so did the corresponding performance improvement. This is the expected\nresult as we expect the naive algorithm communication overhead to increase relatively to overall cost when number of\nranks increases. This demonstrates our methods superior scalability over existing methods.\n6\nConclusion\nIn this paper we present a novel insight, and discussed it\u2019s potential impacts given the increasingly distributed\nworkload of modern day LLM inference. Our contribution is an optimized reordering strategy that minimizes global\ncommunication across ranks. We call our method TP-Aware Dequantization. We benchmarked our method on Llama-\n70B and Granite-20B problem sizes and demonstrated up to 1.81x and 1.76x speedups, respectively, on the NVIDIA\nDGX A100 and an up to 1.76x and 1.78x speedup on the NVIDIA DGX H100.\n7\nAcknowledgements\nWe would like to acknowledge important contributions from Less Wright, Meta AI, for providing the H100 experiment\nresults, Hao Yu and Eun Kyung Lee from IBM Research for their review on the presented materiel.\n9\nPRIME AI paper\nReferences\n[1]\nElias Frantar et al. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. Mar. 22,\n2023. arXiv: 2210.17323[cs]. URL: http://arxiv.org/abs/2210.17323 (visited on 12/26/2023).\n[2]\nMaxime Lbonne. ML Blog - 4-bit LLM Quantization with GPTQ. July 30, 2023. URL: https://mlabonne.\ngithub.io/blog/posts/4_bit_Quantization_with_GPTQ.html (visited on 12/26/2023).\n[3]\nDeepak Narayanan et al. Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM.\nAug. 23, 2021. arXiv: 2104.04473[cs]. URL: http://arxiv.org/abs/2104.04473 (visited on 12/26/2023).\n[4]\nTensor Parallelism - torch.distributed.tensor.parallel \u2014 PyTorch 2.1 documentation. URL: https://pytorch.\norg/docs/stable/distributed.tensor.parallel.html (visited on 12/21/2023).\n[5]\ntorch.argsort \u2014 PyTorch 2.1 documentation. URL: https://pytorch.org/docs/stable/generated/torch.\nargsort.html (visited on 01/04/2024).\n[6]\nturboderp. turboderp/exllamav2. original-date: 2023-08-30T08:54:22Z. Dec. 28, 2023. URL: https://github.\ncom/turboderp/exllamav2 (visited on 12/28/2023).\n[7]\nWilliam. PanQiWei/AutoGPTQ. original-date: 2023-04-13T02:18:11Z. Dec. 27, 2023. URL: https://github.\ncom/PanQiWei/AutoGPTQ (visited on 12/27/2023).\n10\n"
  },
  {
    "title": "Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers",
    "link": "https://arxiv.org/pdf/2402.04744.pdf",
    "upvote": "1",
    "text": ""
  }
]