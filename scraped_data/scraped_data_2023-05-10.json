[
  {
    "title": "To Compress or Not to Compress- Self-Supervised Learning and Information Theory: A Review",
    "link": "https://arxiv.org/pdf/2304.09355.pdf",
    "upvote": "3",
    "text": "To Compress or Not to Compress\nTo Compress or Not to Compress - Self-Supervised Learning\nand Information Theory: A Review\nRavid Shwartz-Ziv New York University\nravid.shwartz.ziv@nyu.edu\nYann LeCun New York University & Meta AI - FAIR\nAbstract\nDeep neural networks excel in supervised learning tasks but are constrained by the need\nfor extensive labeled data. Self-supervised learning emerges as a promising alternative,\nallowing models to learn without explicit labels. Information theory, and notably the\ninformation bottleneck principle, has been pivotal in shaping deep neural networks. This\nprinciple focuses on optimizing the trade-off between compression and preserving relevant\ninformation, providing a foundation for efficient network design in supervised contexts.\nHowever, its precise role and adaptation in self-supervised learning remain unclear. In\nthis work, we scrutinize various self-supervised learning approaches from an information-\ntheoretic perspective, introducing a unified framework that encapsulates the self-supervised\ninformation-theoretic learning problem. We weave together existing research into a cohesive\nnarrative, delve into contemporary self-supervised methodologies, and spotlight potential\nresearch avenues and inherent challenges. Additionally, we discuss the empirical evaluation of\ninformation-theoretic quantities and their estimation methods. Overall, this paper furnishes\nan exhaustive review of the intersection of information theory, self-supervised learning, and\ndeep neural networks.\nKeywords:\nSelf-Supervised Learning, Information Theory, Representation Learning\n1. Introduction\nDeep neural networks (DNNs) have revolutionized fields such as computer vision, natural\nlanguage processing, and speech recognition due to their remarkable performance in super-\nvised learning tasks (Alam et al., 2020; He et al., 2015; LeCun et al., 2015). However, the\nsuccess of DNNs is often limited by the need for vast amounts of labeled data, which can\nbe both time-consuming and expensive to acquire. Self-supervised learning (SSL) emerges\nas a promising alternative, enabling models to learn from data without explicit labels by\nleveraging the underlying structure and relationships within the data itself.\nRecent advances in SSL have been driven by joint embedding architectures, such as Siamese\nNets (Bromley et al., 1993), DrLIM (Chopra et al., 2005; Hadsell et al., 2006), and SimCLR\n(Chen et al., 2020a). These approaches define a loss function that encourages representations\nof different versions of the same image to be similar while pushing representations of distinct\nimages apart.\nAfter optimizing the surrogate objective, the pre-trained model can be\nemployed as a feature extractor, with the learned features serving as inputs for downstream\nsupervised tasks like image classification, object detection, instance segmentation, or pose\nestimation (Caron et al., 2021; Chen et al., 2020a; Misra and van der Maaten, 2020; Shwartz-\nZiv et al., 2022b). Although SSL methods have shown promising results in practice, the\n1\narXiv:2304.09355v5  [cs.LG]  21 Nov 2023\ntheoretical underpinnings behind their effectiveness remain an open question (Arora et al.,\n2019; Lee et al., 2021a).\nInformation theory has played a crucial role in understanding and optimizing deep neural\nnetworks, from practical applications like the variational information bottleneck (Alemi\net al., 2016) to theoretical investigations of generalization bounds induced by mutual\ninformation (Steinke and Zakynthinou, 2020; Xu and Raginsky, 2017). Building upon\nthese foundations, several researchers have attempted to enhance self-supervised and semi-\nsupervised learning algorithms using information-theoretic principles, such as the Mutual\nInformation Neural Estimator (MINE) (Belghazi et al., 2018b) combined with the information\nmaximization (InfoMax) principle (Linsker, 1988).\nHowever, the plethora of objective\nfunctions, contradicting assumptions, and various estimation techniques in the literature\ncan make it challenging to grasp the underlying principles and their implications.\nIn this paper, we aim to achieve two objectives. First, we propose a unified framework\nthat synthesizes existing research on self-supervised and semi-supervised learning from an\ninformation-theoretic standpoint. This framework allows us to present and compare current\nmethods, analyze their assumptions and difficulties, and discuss the optimal representation\nfor neural networks in general and self-supervised networks in particular. Second, we explore\ndifferent methods and estimators for optimizing information-theoretic quantities in deep\nneural networks and investigate how recent models optimize various theoretical-information\nterms.\nBy reviewing the literature on various aspects of information-theoretic learning, we provide\na comprehensive understanding of the interplay between information theory, self-supervised\nlearning, and deep neural networks. We discuss the application of the information bottleneck\nprinciple (Tishby et al., 1999a), connections between information theory and generalization,\nand recent information-theoretic learning algorithms. Furthermore, we examine how the\ninformation-theoretic perspective can offer insights into the design of better self-supervised\nlearning algorithms and the potential benefits of using information theory in SSL across a\nwide range of applications.\nIn addition to the main structure of the paper, we dedicate a section to the challenges and\nopportunities in extending the information-theoretic perspective to other learning paradigms,\nsuch as energy-based models. We highlight the potential advantages of incorporating these\nextensions into self-supervised learning algorithms and discuss the technical and conceptual\nchallenges that must be addressed.\nThe structure of the paper is as follows. Section 2 introduces the key concepts in supervised,\nsemi-supervised, self-supervised learning, information theory, and representation learning.\nSection 3 presents a unified framework for multiview learning based on information theory.\nWe first discuss what an optimal representation is and why compression is beneficial for\nlearning. Next, we explore optimal representation in single-view supervised learning models\nand how they can be extended to unsupervised, semi-supervised, and multiview contexts.\nThe focus then shifts to self-supervised learning, where the optimal representation remains\nan open question. Using the unified framework, we compare recent self-supervised algorithms\nand discuss their differences. We analyze the assumptions behind these models, their effects\n2\nTo Compress or Not to Compress\non the learned representation, and their varying perspectives on important information\nwithin the network.\nSection 5 addresses several technical challenges, discussing both theoretical and practical\nissues in estimating theoretical information terms. We present recent methods for estimating\nthese quantities, including variational bounds and estimators. Section 6 concludes the paper\nby offering insights into potential future research directions at the intersection of information\ntheory, self-supervised learning, and deep neural networks. Our aim is to inspire further\nresearch that leverages information theory to advance our understanding of self-supervised\nlearning and to develop more efficient and effective models for a broad range of applications.\n2. Background and Fundamental Concepts\n2.1 Multiview Representation Learning\nMultiview learning has gained increasing attention and great practical success by using\ncomplementary information from multiple features or modalities. The multiview learning\nparadigm divides the input variable into multiple views from which the target variable\nshould be predicted (Zhao et al., 2017b). Using this paradigm, one can eliminate hypotheses\nthat contradict predictions from other views and provide a natural semi-supervised and\nself-supervised learning setting. A multiview dataset consists of data captured from multiple\nsources, modalities, and forms but with similar high-level semantics (Yan et al., 2021). This\nmechanism was initially used for natural-world data, combining image, text, audio, and\nvideo measurements. For example, photos of objects are taken from various angles, and\nour supervised task is to identify the objects. Another example is identifying a person by\nanalyzing the video stream as one view and the audio stream as the other.\nAlthough these views often provide different and complementary information about the\nsame data, directly integrating them does not produce satisfactory results due to biases\nbetween multiple views (Yan et al., 2021). Thus, multiview representation learning involves\nidentifying the underlying data structure and integrating the different views into a common\nfeature space, resulting in high performance. In recent decades, multiview learning has been\nused for many machine learning tasks and influenced many algorithms, such as co-training\nmechanisms (Kumar and Daum\u00b4e, 2011), subspace learning methods (Xue et al., 2019), and\nmultiple kernel learning (MKL) (Bach and Jordan, 2002). Li et al. (2018) proposed two\ncategories for multiview representation learning: (i) multiview representation fusion, which\ncombines different features from multiple views into a single compact representation, and (ii)\nalignment of multiview representation, which attempts to capture the relationships among\nmultiple different views through feature alignment. In this case, a learned mapping function\nembeds the data of each view, and the representations are regularized to form a multiview-\naligned space. In this research direction, an early study is the Canonical Correlation Analysis\n(CCA) (Hotelling, 1936) and its kernel extensions (Bach and Jordan, 2003; Hardoon et al.,\n2004; Sun, 2013). In addition to CCA, multiview representation learning has penetrated a\nvariety of learning methods, such as dimensionality reduction (Sun et al., 2010), clustering\nanalysis (Yan et al., 2015), multiview sparse coding (Cao et al., 2013; Jia et al., 2010;\nLiu et al., 2014), and multimodal topic learning (Pu et al., 2020). However, despite their\n3\npromising results, these methods use handcrafted features and linear embedding functions,\nwhich cannot capture the nonlinear properties of multiview data.\nThe emergence of deep learning has provided a powerful way to learn complex, nonlinear,\nand hierarchical representations of data. By incorporating multiple hierarchical layers, deep\nlearning algorithms can learn complex, subtle, and abstract representations of target data.\nThe success of deep learning in various application domains has led to a growing interest in\ndeep multiview methods, which have shown promising results. Examples of these methods\ninclude deep multiview canonical correlation analysis (Andrew et al., 2013) as an extension\nof CCA, multiview clustering via deep matrix factorization (Zhao et al., 2017a), and the deep\nmultiview spectral network (Huang et al., 2019). Moreover, deep architectures have been\nemployed to generate effective representations in methods such as multiview convolutional\nneural networks (Liu et al., 2021a), multimodal deep Boltzmann machines (Srivastava and\nSalakhutdinov, 2014), multimodal deep autoencoders (Ngiam et al., 2011; Wang et al., 2015),\nand multimodal recurrent neural networks (Donahue et al., 2015; Karpathy and Fei-Fei,\n2015; Mao et al., 2014).\n2.2 Self-Supervised Learning\nSelf-supervised learning (SSL) is a powerful technique that leverages unlabeled data to\nlearn useful representations. In contrast to supervised learning, which relies on labeled\ndata, SSL employs self-defined signals to establish a proxy objective between the input\nand the signal. The model is initially trained using this proxy objective and subsequently\nfine-tuned on the target task. Self-supervised signals, derived from the inherent co-occurrence\nrelationships in the data, serve as self-supervision. Various such signals have been used to\nlearn representations, including generative and joint embedding architectures (Bachman\net al., 2019; Bar et al., 2022; Chen et al., 2020a,b).\nTwo main categories of SSL architectures exist: (1) generative architectures based on\nreconstruction or prediction and (2) joint embedding architectures (Liu et al., 2021b). Both\narchitecture classes can be trained using either contrastive or non-contrastive methods.\nWe begin by discussing these two main types of architectures:\n1. Generative Architecture: Generative architectures employ an objective function\nthat measures the divergence between input data and predicted reconstructions, such\nas squared error. The architecture reconstructs data from a latent variable or a\ncorrupted version, potentially with a latent variable\u2019s assistance. Notable examples of\ngenerative architectures include auto-encoders, sparse coding, sparse auto-encoders, and\nvariational auto-encoders (Kingma and Welling, 2013; Lee et al., 2006; Ng et al., 2011).\nAs the reconstruction task lacks a single correct answer, most generative architectures\nutilize a latent variable, which, when varied, generates multiple reconstructions. The\nlatent variable\u2019s information content requires regularization to ensure the system\nreconstructs regions of high data density while avoiding a collapse by reconstructing\nthe entire space. PCA regularizes the latent variable by limiting its dimensions, while\nsparse coding and sparse auto-encoders restrict the number of non-zero components.\nVariational auto-encoders regularize the latent variable by rendering it stochastic\n4\nTo Compress or Not to Compress\nand maximizing the entropy of the distribution relative to a prior. Vector quantized\nvariational auto-encoders (VQ-VAE) employ binary stochastic variables to achieve\nsimilar results (Van Den Oord et al., 2017).\n2. Joint Embedding Architectures (JEA): These architectures process multiple\nviews of an input signal through encoders, producing representations of the views. The\nsystem is trained to ensure that these representations are both informative and mutually\npredictable. Examples include Siamese networks, where two identical encoders share\nweights (Chen et al., 2020a; Chen and He, 2021; Grill et al., 2020; He et al., 2020),\nand methods permitting encoders to differ (Bardes et al., 2021). A primary challenge\nwith JEA is preventing informational collapse, in which the representations contain\nminimal information about the inputs, thereby facilitating their mutual prediction.\nJEA\u2019s advantage lies in the encoders\u2019 ability to eliminate noisy, unpredictable, or\nirrelevant information from the input within the representation space.\nTo effectively train these architectures, it is essential to ensure that the representations\nof different signals are distinct. This can be achieved through either contrastive or non-\ncontrastive methods:\n\u2022 Contrastive Methods: Contrastive methods utilize data points from the training\nset as positive samples and generate points outside the region of high data density as\ncontrastive samples. The energy (e.g., reconstruction error for generative architectures\nor representation predictive error for JEA) should be low for positive samples and\nhigher for contrastive samples. Various loss functions involving the energies of pairs or\nsets of samples can be minimized to achieve this objective.\n\u2022 Non-Contrastive Methods: Non-contrastive methods prevent the energy land-\nscape\u2019s collapse by limiting the volume of space that can take low energy, either\nthrough architectural constraints or through a regularizer in the energy or training\nobjective. In latent-variable generative architectures, preventing collapse is achieved by\nlimiting or minimizing the information content of the latent variable. In JEA, collapse\nis prevented by maximizing the information content of the representations.\nWe now present a few concrete examples of popular models that employ various combinations\nof generative architectures, joint embedding architectures, contrastive training, and non-\ncontrastive training:\nThe Denoising Autoencoder approach in generative architectures (Devlin et al., 2018; He\net al., 2022; Vincent et al., 2008) using a triplet loss which utilizes a positive sample, which\nis a vector from the training set that should be reconstructed perfectly, and a contrastive\nsample consisting of data vectors, one from the training set and the other being a corrupted\nversion of it. In SSL, the combination of JEA models with contrastive learning has proven\nhighly effective. In contrastive learning, the objective is to attract different augmented views\nof the same image (positive points) while repelling dissimilar augmented views (negative\npoints). Recent self-supervised visual representation learning examples include MoCo (He\net al., 2020) and SimCLR (Chen et al., 2020a). The InfoNCE loss is a commonly used\n5\nobjective function in many contrastive learning methods:\nEx,x+,x\u2212\n\"\n\u2212 log\n \nef(x)T f(x+)\nP k = 1Kef(x)T f(xk)\n!#\nwhere x+ is a sample similar to x, xk are all the samples in the batch, and f is an encoder.\nHowever, contrastive methods heavily depend on all other samples in the batch and require a\nlarge batch size. Additionally, recent studies (Jing et al., 2021) have shown that contrastive\nlearning can lead to dimensional collapse, where the embedding vectors span a lower-\ndimensional subspace instead of the entire embedding space. Although positive and negative\npairs should repel each other to prevent dimensional collapse, augmentation along feature\ndimensions and implicit regularization cause the embedding vectors to fall into a lower-\ndimensional subspace, resulting in low-rank solutions.\nTo address these problems, recent works have introduced JEA models with non-contrastive\nmethods. Unlike contrastive methods, these methods employ regularization to prevent the\ncollapse of the representation and do not explicitly rely on negative samples. For example,\nseveral papers use stop-gradients and extra predictors to avoid collapse (Chen and He, 2021;\nGrill et al., 2020), while Caron et al. (2020) employed an additional clustering step. VICReg\n(Bardes et al., 2021) is another non-contrastive method that regularizes the covariance\nmatrix of representation. Consider two embedding batches Z = [f(x1), . . . , f(xN)] and\nZ\u2032 = [f(x\u20321), . . . , f(x\u2032N)], each of size (N \u00d7 K). Denote by C the (K \u00d7 K) covariance\nmatrix obtained from [Z, Z\u2032]. The VICReg triplet loss is defined by:\nL= 1\nK\nK\nX\nk=1\n\uf8eb\n\uf8ed\u03b1 max\n\u0010\n0, \u03b3 \u2212\np\nCk,k + \u03f5\n\u0011\n+\u03b2\nX\nk\u2032\u0338=k\n\u0000Ck,k\u2032\u00012\n\uf8f6\n\uf8f8\n+ \u03b3\u2225Z \u2212 Z\u2032\u22252\nF /N.\n2.3 Semi-Supervised Learning\nSemi-supervised learning employs both labeled and unlabeled data to enhance the model\nperformance (Chapelle et al., 2009). Consistency regularization-based approaches (Laine\nand Aila, 2016; Miyato et al., 2018; Sohn et al., 2020) ensure that predictions remain stable\nunder perturbations in input data and model parameters. Certain techniques, such as those\nproposed by Grandvalet and Bengio (2006) and Miyato et al. (2018), involve training a model\nby incorporating a regularization term into a supervised cross-entropy loss. In contrast, Xie\net al. (2020) utilizes suitably weighted unsupervised regularization terms, while Zhai et al.\n(2019) adopts a combination of self-supervised pretext loss terms. Moreover, pseudo-labeling\ncan generate synthetic labels based on network uncertainty to further aid model training\n(Lee et al., 2013).\n2.4 Representation Learning\nRepresentation learning is an essential aspect of various computer vision, natural language\nprocessing, and machine learning tasks, as it uncovers the underlying structures in data\n(Bengio et al., 2013). By extracting relevant information for classification and prediction\ntasks from the data, we can improve performance and reduce computational complexity\n6\nTo Compress or Not to Compress\n(Goodfellow et al., 2016). However, defining an effective representation remains a challenging\ntask. In probabilistic models, a useful representation often captures the posterior distribution\nof explanatory factors beneath the observed input (LeCun et al., 2015). Bengio and LeCun\n(2007) introduced the idea of learning highly structured yet complex dependencies for AI\ntasks, which require transforming high-dimensional input structures into low-dimensional\noutput structures or learning low-level representations. As a result, identifying relevant input\nfeatures becomes challenging, as most input entropy is unrelated to the output (Shwartz-Ziv\nand Tishby, 2017). Ben-Shaul et al. (2023) demonstrated that self-supervised learning\ninherently promotes the clustering of samples based on semantic labels. Intriguingly, this\nclustering is driven by the objective\u2019s regularization term and aligns with semantic classes\nacross multiple hierarchical levels.\n2.4.1 Minimal Sufficient Statistic\nA possible definition of an effective representation is based on minimal sufficient statistics.\nDefinition 1 Given (X, Y ) \u223c P(X, Y ), let T := t(X), where t is a deterministic function.\nWe define T as a sufficient statistic of X for Y if Y \u2212 T \u2212 X forms a Markov chain.\nA sufficient statistic captures all the information about Y in X. Cover (1999) proved this\nproperty:\nTheorem 2 Let T be a probabilistic function of X. Then, T is a sufficient statistic for Y\nif and only if I(T(X); Y ) = I(X; Y ).\nHowever, the sufficiency definition also encompasses trivial identity statistics that only \u201dcopy\u201d\nrather than \u201dextract\u201d essential information. To prevent statistics from inefficiently utilizing\nobservations, the concept of minimal sufficient statistics was introduced:\nDefinition 3 (Minimal sufficient statistic (MSS)) A sufficient statistic T is minimal if, for\nany other sufficient statistic S, there exists a function f such that T = f(S) almost surely\n(a.s.).\nIn essence, MSS are the simplest sufficient statistics, inducing the coarsest sufficient partition\non X. In MSS, the values of X are grouped into as few partitions as possible without\nsacrificing information. MSS are statistics with the maximum information about Y while\nretaining the least information about X as possible (Koopman, 1936).\n2.4.2 The Information Bottleneck\nThe majority of distributions lack exact minimal sufficient statistics, leading Tishby et al.\n(1999b) to relax the optimization problem in two ways: (i) allowing the map to be stochastic,\ndefined as an encoder P(T|X), and (ii) permitting the capture of only a small amount of\nI(X; Y ). The information bottleneck (IB) was introduced as a principled method to extract\nrelevant information from observed signals related to a target. This framework finds the\noptimal trade-off between the accuracy and complexity of a random variable y \u2208 Y with a\njoint distribution for a random variable x \u2208 X. The IB has been employed in various fields\nsuch as neuroscience (Buesing and Maass, 2010; Palmer et al., 2015), slow feature analysis\n7\n(Turner and Sahani, 2007), speech recognition (Hecht et al., 2009), and deep learning (Alemi\net al., 2016; Shwartz-Ziv and Tishby, 2017).\nLet X be an input random variable, Y a target variable, and P(X, Y ) their joint distribution.\nA representation T is a stochastic function of X defined by a mapping P(T | X). This\nmapping transforms X \u223c P(X) into a representation of T \u223c P(T) :=\nR\nPT|X(\u00b7 | x)dPX(x).\nThe triple Y \u2212X \u2212T forms a Markov chain in that order with respect to the joint probability\nmeasure PX,Y,T = PX,Y PT|X and the mutual information terms I(X; T) and I(Y ; T).\nWithin the IB framework, our goal is to find a representation P(T | X) that extracts as much\ninformation as possible about Y (high performance) while compressing X maximally (keeping\nI(X; T) small). This can also be interpreted as extracting only the relevant information\nthat X contains about Y .\nThe data processing inequality (DPI) implies that I(Y ; T) \u2264 I(X; Y ), so the compressed\nrepresentation T cannot convey more information than the original signal. Consequently,\nthere is a trade-off between compressed representation and the preservation of relevant\ninformation about Y . The construction of an efficient representation variable is characterized\nby its encoder and decoder distributions, P(T | X) and P(Y | T), respectively. The efficient\nrepresentation of X involves minimizing the complexity of the representation I (T; X) while\nmaximizing I (T; Y ).\nFormally, the IB optimization involves minimizing the following\nobjective function:\nL =\nmin\nP(t|x);p(y|t) I(X; T) \u2212 \u03b2I(Y ; T) ,\n(1)\nwhere \u03b2 is the trade-off parameter controlling the complexity of T and the amount of relevant\ninformation it preserves. Intuitively, we pass the information that X contains about Y\nthrough a \u201cbottleneck\u201d via the representation T. It has been shown that:\nI(T : Y ) = I(X : Y ) \u2212 Ex\u223cP(X),t\u223cP(T|x) [D [P(Y |x)||P(Y |t)]]\n(2)\n2.5 Representation Learning and the Information Bottleneck\nInformation theory traditionally assumes that underlying probabilities are known and do not\nrequire learning. For instance, the optimality of the initial IB work (Tishby et al., 1999b)\nrelied on the assumption that the joint distribution of input and labels is known. However,\na significant challenge in machine learning algorithms is inferring an accurate predictor for\nthe unknown target variable from observed realizations. This discrepancy raises questions\nabout the practical optimality of the IB and its relevance in modern learning algorithms.\nThe following section delves into the relationship between the IB framework and learning,\ninference, and generalization.\nLet X \u2208 X and a target variable Y \u2208 Y be random variables with an unknown joint\ndistribution P(X, Y ). For a given class of predictors f : X \u2192 \u02c6Y and a loss function \u2113 : Y \u2192 \u02c6Y\nmeasuring discrepancies between true values and model predictions, our objective is to find\nthe predictor f that minimizes the expected population risk.\nLP(X,Y ) (f, \u2113) = EP(X,Y ) [\u2113(Y, f(X))]\n8\nTo Compress or Not to Compress\nSeveral issues arise with the population risk. Firstly, it remains unclear which loss function\nis optimal. A popular choice is the logarithmic loss (or error\u2019s entropy), which has been\nnumerically demonstrated to yield better results (Erdogmus, 2002). This loss has been\nemployed in various algorithms, including the InfoMax principle (Linsker, 1988), tree-based\nalgorithms (Quinlan, 2014), deep neural networks (Zhang and Sabuncu, 2018), and Bayesian\nmodeling (Wenzel et al., 2020). Painsky and Wornell (2018) provided a rigorous justification\nfor using the logarithmic loss and showed that it is an upper bound to any choice of the loss\nfunction that is smooth, proper, and convex for binary classification problems.\nIn most cases, the joint distribution P(X, Y ) is unknown, and we have access to only n\nsamples from it, denoted by Dn := (xi, yi) | i = 1, . . . , n. Consequently, the population risk\ncannot be computed directly. Instead, we typically choose the predictor that minimizes the\nempirical population risk on a training dataset:\n\u02c6LP(X,Y ) (f, \u2113, Dn) = 1\nn\nn\nX\ni=1\n[\u2113(yi, f(xi))]\nThe generalization gap, defined as the difference between empirical and population risks, is\ngiven by:\nGenP(X,Y ) (f, \u2113, Dn) := LP(X,Y ) (f, \u2113) \u2212 \u02c6LP(X,Y ) (f, \u2113, Dn)\nInterestingly, the relationship between the true loss and the empirical loss can be bounded\nusing the information bottleneck term. Shamir et al. (2010) developed several finite sample\nbounds for the generalization gap. According to their study, the IB framework exhibited\ngood generalizability even with small sample sizes. In particular, they developed non-uniform\nbounds adaptive to the model\u2019s complexity. They demonstrated that for the discrete case,\nthe error in estimating mutual information from finite samples is bounded by O\n\u0010\n|X| log n\n\u221an\n\u0011\n,\nwhere |X| is the cardinality of X (the number of possible values that the random variable X\ncan take). The results support the intuition that simpler models generalize better, and we\nwould like to compress our model. Therefore, optimizing eq. (1) presents a trade-off between\ntwo opposing forces. On the one hand, we want to increase our prediction accuracy in our\ntraining data (high \u03b2).\nOn the other hand, we would like to decrease \u03b2 to narrow the generalization gap. Vera\net al. (2018) extended their work and showed that the generalization gap is bounded by the\nsquare root of mutual information between training input and model representation times\nlog n\nn . Furthermore, Russo and Zou (2019) and Xu and Raginsky (2017) demonstrated that\nthe square root of the mutual information between the training input and the parameters\ninferred from the training algorithm provides a concise bound on the generalization gap.\nHowever, these bounds critically depend on the Markov operator that maps the training set\nto the network parameters, whose characterization is not trivial.\nAchille and Soatto (2018) explored how applying the IB objective to the network\u2019s parameters\nmay reduce overfitting while maintaining invariant representations. Their work showed\n9\nthat flat minima, which have better generalization properties, bound the information with\nthe weights, and the information in the weights bound the information in the activations.\nChelombiev et al. (2019) found that the generalization precision is positively correlated with\nthe degree of compression of the last layer in the network. Shwartz-Ziv et al. (2018) showed\nthat the generalization error depends exponentially on the mutual information between the\nmodel and the input once it is smaller than log 2n - the query sample complexity. Moreover,\nthey demonstrated that M bits of compression of X are equivalent to an exponential factor\nof 2M training examples. Piran et al. (2020) extended the original IB to the dual form,\nwhich offers several advantages in terms of compression.\nThese studies illustrate that the IB leads to a trade-off between prediction and complexity,\neven for the empirical distribution. With the IB objective, we can design estimators to\nfind optimal solutions for different regimes with varying performance, complexity, and\ngeneralization.\n3. Information-Theoretic Objectives\nBefore delving into the details, this section aims to provide an overview of the information-\ntheoretic objectives in various learning scenarios, including supervised, unsupervised, and\nself-supervised settings. We will also introduce a general framework to understand better\nthe process of learning optimal representations and explore recent methods working towards\nthis goal.\nDeveloping a novel algorithm entails numerous aspects, such as architecture, initialization\nparameters, learning algorithms, and pre-processing techniques. A crucial element, however,\nis the objective function. As demonstrated in Section 2.4.2, the IB approach, originally\nintroduced by Tishby et al. (1999b), defines the optimal representation in supervised scenarios,\nenabling us to identify which terms to compress during learning. However, determining the\noptimal representation and deriving information-based objective functions in self-supervised\nsettings are more challenging. In this section, we introduce a general framework to understand\nthe process of learning optimal representations and explore recent methods striving to achieve\nthis goal.\n3.1 Setup and Methodology\nUsing a two-channel input allows us to model complex multiview learning problems. In\nmany real-world situations, data can be observed from multiple perspectives or modalities,\nmaking it essential to develop learning algorithms capable of handling such multiview data.\nConsider a two-channel input, X1 and X2, and a single-channel label Y for a downstream\ntask, all possessing a joint distribution P(X1, X2, Y ). We assume the availability of n labeled\nexamples S = (xi\n1, xi\n2, yi)n\ni=1 and t unlabeled examples U = (xi\n1, xi\n2)n+t\ni=n+1, both independently\nand identically distributed. Our objective is to predict Y using a loss function.\nIn our model, we use a learned encoder with a prior P(Z) to generate a conditional\nrepresentation (which may be deterministic or stochastic) Zi|Xi = P\u03b8i(Zi|Xi), where i = 1, 2\n10\nTo Compress or Not to Compress\nrepresents the two views. Subsequently, we utilize various decoders to \u2019decode\u2019 distinct\naspects of the representation:\nFor the supervised scenario, we have a joint embedding of the label classifiers from both\nviews, \u02c6Y1,2 = Q\u03c1(Y |Z1, Z2), and two decoders predicting the labels of the downstream task\nbased on each individual view, \u02c6Yi = Q\u03c1i(Y |Zi) for i = 1, 2.\nFor the unsupervised case, we have direct decoders for input reconstruction from the\nrepresentation, \u00af\nXi = Q\u03c8i(Xi|Zi) for i = 1, 2.\nFor self-supervised learning, we utilize two cross-decoders attempting to predict one represen-\ntation based on the other, \u02dc\nZ1|Z2 = q\u03b71(Z1|Z2) and \u02dc\nZ2|Z1 = q\u03b72(Z2|Z1). Figure 1 illustrates\nthis structure.\nThe information-theoretic perspective of self-supervised networks has led to confusion\nregarding the information being optimized in recent work. In supervised and unsupervised\nlearning, only one \u2019information path\u2019 exists when optimizing information-theoretic terms:\nthe input is encoded through the network, and then the representation is decoded and\ncompared to the targets. As a result, the representation and corresponding information\nalways stem from a single encoder and decoder.\nHowever, in the self-supervised multiview scenario, we can construct our representation\nusing various encoders and decoders. For instance, we need to specify the associated random\nvariable to define the information involved in I(X1; Z1). This variable could either be based\non the encoder of X1 - P\u03b81(Z1|X1), or based on the encoder of X2 - P\u03b82(Z2|X2), which\nis subsequently passed to the cross-decoder Q\u03b71(Z1|Z2) and then to the direct decoder\nQ\u03c81(X1|Z1).\nTo fully understand the information terms, we aim to optimize and distinguish between\nvarious \u201dinformation paths,\u201d we marked each information path differently. For example,\nI,P(X1),P(Z1|X1),P(Z2|Z1) (X1, Z2) is based on the path P(X1) \u2192 P(Z1|X1) \u2192 P(Z2|Z1). In\nthe following section, we will \u201dtranslate\u201d previous work into our present framework and\nexamine the loss function.\n3.2 Optimization with Labels\nAfter establishing our framework, we can now incorporate various learning algorithms. We\nbegin by examining classical single-view supervised information bottleneck algorithms for\ndeep networks that utilize labeled data during training and extend them to the multiview\nscenario. Next, we broaden our perspective to include unsupervised learning, where in-\nput reconstruction replaces labels, and semi-supervised learning, where information-based\nregularization is applied to improve predictions.\n3.2.1 Single-View Supervised Learning\nIn classical single-view supervised learning, the task of representation learning involves\nfinding a distribution p(z|x) that maps data observations x \u2208 X to a representation z \u2208 Z,\ncapturing only the relevant features of the input Shwartz-Ziv (2022). The goal is to predict\na label y \u2208 Y using the learned representation. Achille and Soatto (2018) defined the\n11\n \n \nComplexity\nUnsupervised\nReconstruction \nSupervised Prediction\nSelf-Supervised\nPredication\nSupervised Prediction\nUnsupervised\nReconstruction \nComplexity\nFigure 1: Multiview information bottleneck diagram for self-supervised, unsupervised, and\nsupervised learning\nsufficiency of Z for Y as the amount of label information retained after passing data through\nthe encoder:\nDefinition 4 Sufficiency: A representation Z of X is sufficient for Y if and only if\nI(X; Y |Z) = 0.\nFederici et al. (2020) showed that Z is sufficient for Y if and only if the amount of information\nregarding the task remains unchanged by the encoding procedure. A sufficient representation\ncan predict Y as accurately as the original data X. In Section 2.4, we saw a trade-off\nbetween prediction and generalization when there is a finite amount of data. To reduce the\ngeneralization gap, we aim to compress X while retaining as much predicate information on\nthe labels as possible. Thus, we relax the sufficiency definition and minimize the following\nobjective:\nL = I(X; Z) \u2212 \u03b2I(Z; Y )\n(3)\nThe mutual information I(Y ; Z) determines how much label information is accessible and\nreflects the model\u2019s ability to predict performance on the target task. I(X; Z) represents the\ninformation that Z carries about the input, which we aim to compress. However, I(X; Z)\ncontains both relevant and irrelevant information about Y . Therefore, using the chain rule\nof information, Federici et al. (2020) proposed splitting I(X, Z) into two terms:\nI(X; Z) =\nI(X; Z|Y )\n|\n{z\n}\nsuperfluous information\n+\nI(Z; Y )\n| {z }\npredictive information\n(4)\nThe conditional information I(X, Z|Y ) represents information in Z that is not predictive\nof Y , i.e., superfluous information. The decomposition of input information enables us to\ncompress only irrelevant information while preserving the relevant information for predicting\nY . Several methods are available for evaluating and estimating these information-theoretic\nterms in the supervised case (see Section 5 for details).\n12\nTo Compress or Not to Compress\n3.2.2 The Information Bottleneck Theory of Deep Learning\nThe IB hypothesis for deep learning proposes two distinct phases of training neural networks\n(Shwartz-Ziv and Tishby, 2017): the fitting and compression phases. The fitting phase\ninvolves extracting information from the input and converting it into learned representations,\ncharacterized by increased mutual information between inputs and hidden representations.\nConversely, the compression phase, which is much longer, concentrates on discarding un-\nnecessary information for target prediction, decreasing mutual information between learned\nrepresentations and inputs. In contrast, the mutual information between representations\nand targets increases. For more information, see Geiger (2020). Despite the elegance and\nplausibility of the IB hypothesis, empirically investigating it remains challenging (Amjad\nand Geiger, 2018).\nThe study of representation compression in Deep Neural Networks (DNNs) for supervised\nlearning has shown inconsistent results. For instance, Chelombiev et al. (2019) discovered\na positive correlation between generalization accuracy and the compression level of the\nnetwork\u2019s final layer. Shwartz-Ziv et al. (2018) also examined the relationship between\ngeneralization and compression, demonstrating that generalization error exponentially de-\npends on mutual information, I(X; Z). Furthermore, Achille et al. (2017) established that\nflat minima, known for their improved generalization properties, constrain the mutual\ninformation. However, Saxe et al. (2019) showed that compression was not necessary for\ngeneralization in deep linear networks. Basirat et al. (2021) revealed that the decrease in\nmutual information is essentially equivalent to geometrical compression. Other studies have\nfound that the mutual information between training inputs and inferred parameters provides\na concise bound on the generalization gap (Pensia et al., 2018; Xu and Raginsky, 2017).\nLastly, Achille and Soatto (2018) explored using an information bottleneck objective on\nnetwork parameters to prevent overfitting and promote invariant representations.\n3.2.3 Multiview IB Learning\nThe IB principle offers a rigorous method for learning encoders and decoders in supervised\nsingle-view problems. However, it is not directly applicable to multiview learning problems,\nas it assumes only one information source as the input. A common solution is to concatenate\nmultiple views, though this neglects the unique characteristics of each view. To address this\nissue, Xu et al. (2014) introduced the large-margin multiview IB (LMIB) as an extension of\nthe original IB problem. LMIB employs a communication system where multiple senders\nrepresent various views of examples. The system extracts specific components from different\nsenders by compressing examples through a \u201dbottleneck,\u201d and the linear projectors for each\nview are combined to create a shared representation. The large-margin principle replaces the\nmaximization of mutual information in prediction, emphasizing the separation of samples\nfrom different classes. Limiting Rademacher complexity improves the solution\u2019s accuracy\nand generalization error bounds. Moreover, the algorithm\u2019s robustness is enhanced when\naccurate views counterbalance noisy views.\nHowever, the LMIB method has a significant limitation: it utilizes linear projections for\neach view, which can restrict the combined representation when the relationship between\ndifferent views is complex. To overcome this limitation, Wang et al. (2019) proposed using\n13\ndeep neural networks to replace linear projectors. Their model first extracts concise latent\nrepresentations from each view using deep networks and then learns the joint representation\nof all views using neural networks. They minimize the objective:\nL = \u03b1IP(X1),P(Z1|X1)(X1; Z1) + \u03b2IP(X2),P(Z2|X2)(X2; Z2) \u2212 IP(Z2|X2),P(Z2|X1)(Z1,2; Y )\nHere, \u03b1 and \u03b2 are trade-off parameters, Z1 and Z2 are the two neural networks\u2019 represen-\ntations, and Z1,2 is the joint embedding of Z1 and Z2. The first two terms decrease the\nmutual information between a view\u2019s latent representation and its original data representa-\ntion, resulting in a simpler and more generalizable model. The final term forces the joint\nrepresentation to maximize the discrimination ability for the downstream task.\n3.2.4 Semi-Supervised IB Learning: Leveraging Unlabeled Data\nObtaining labeled data can be challenging or expensive in many practical scenarios, while\nmany unlabeled samples may be readily available. Semi-supervised learning addresses this\nissue by leveraging the vast amount of unlabeled data during training in conjunction with\na small set of labeled samples. Common strategies to achieve this involve adding regular-\nization terms or adopting mechanisms that promote better generalization. Berthelot et al.\n(2019) grouped regularization methods into three primary categories: entropy minimization,\nconsistency regularization, and generic regularization.\nVoloshynovskiy et al. (2020) introduced an information-theoretic framework for semi-\nsupervised learning based on the IB principle. In this context, the semi-supervised classi-\nfication problem involves encoding input X into the latent space Z while preserving only\nclass-relevant information. A supervised classifier can achieve this if there is sufficient\nlabeled data. However, when the number of labeled examples is limited, the standard label\nclassifier p(y|z) becomes unreliable and requires regularization.\nTo tackle this issue, the authors assumed a prior on the class label distribution p(y). They\nintroduced a term to minimize the DKL between the assumed marginal prior and the\nempirical marginal prior, effectively regularizing the conditional label classifier with the\nlabels\u2019 marginal distribution. This approach reduces the classifier\u2019s sensitivity to the scarcity\nof labeled examples. They proposed two variational IB semi-supervised extensions for the\npriors:\nHandcrafted Priors: These priors are predefined for regularization and can be based on\ndomain knowledge or statistical properties of the data. Alternatively, they can be learned\nusing other networks. Handcrafted priors in this context are similar to priors used in the\nVariational Information Bottleneck (VIB) formalism (Alemi et al., 2016; Wang et al., 2019).\nLearnable Priors: Voloshynovskiy et al. (2020) also suggests using learnable priors as an\nalternative to handcrafted regularization priors on the latent representation. This method\ninvolves regularizing Z through another IB-based regularization with two components: (i)\nlatent space regularization and (ii) observation space regularization. In this case, an additional\nhidden variable M is introduced after the representation to regulate the information flow\nbetween Z and Y . An auto-encoder q(m|z) is employed, and the optimization process\naims to compress the information flowing from Z to M while retaining only label-relevant\n14\nTo Compress or Not to Compress\ninformation. The IB objective is defined as:\nL = DKL(q(m|z)||p(m|z)) \u2212 \u03b2DKL(q(x|m)||p(x|m)) \u2212 \u03b2yDKL(p(y|z)||p(y))\n\u21d4 I(M; Z) \u2212 \u03b2I(M; X) \u2212 \u03b2yI(Y ; Z)\n(5)\nHere, \u03b2 and \u03b2y are hyperparameters that balance the trade-off between the relevance of M\nto the labels and the compression of Z into M.\nFurthermore, Voloshynovskiy et al. (2020) demonstrated that various popular semi-supervised\nmethods can be considered special cases of the optimization problem described above.\nNotably, the semi-supervised AAE (Makhzani et al., 2015), CatGAN (Springenberg, 2015),\nSeGMA (Smieja et al., 2019), and VAE (Kingma et al., 2014) can all be viewed as specific\ninstantiations of this framework.\n3.2.5 Unsupervised IB learning\nIn the unsupervised setting, data samples are not directly labeled by classes. Voloshynovskiy\net al. (2020) defined unsupervised IB as a \u2019compressed\u2019 parameterized mapping of X to Z,\nwhich preserves some information in Z about X through the reverse decoder \u00afX = Q(X|Z).\nTherefore, the Lagrangian of unsupervised IB can be defined as follows:\nIP(X),P(Z|X)(X; Z) \u2212 \u03b2IP(Z),Q(X|Z)(Z; \u00afX)\nwhere I(X; Z) is the information determined by the encoder q(z|x) and I(Z; \u00afX) is the\ninformation determined by the decoder q(x|z), i.e., the reconstruction error.\nIn other\nwords, unsupervised IB is a special case of supervised IB, where labels are replaced with the\nreconstruction performance of the training input. Alemi et al. (2016) showed that Variational\nAutoencoder (VAE) (Kingma and Welling, 2019) and \u03b2-VAE (Higgins et al., 2017) are\nspecial cases of unsupervised variational IB. Voloshynovskiy et al. (2020) extended their\nresults and showed that many models, including adversarial autoencoders (Makhzani et al.,\n2015), InfoVAEs (Zhao et al., 2017c), and VAE/GANs (Larsen et al., 2016), could be viewed\nas special cases of unsupervised IB. The main difference between them is the bounds on the\ndifferent mutual information of the IB. Furthermore, unsupervised IB was used by U\u02d8gur\net al. (2020) to derive lower bounds for their unsupervised generative clustering framework,\nwhile Roy et al. (2018) used it to study vector-quantized autoencoders.\nVoloshynovskiy et al. (2020) pointed out that for the classification task in supervised IB, the\nlatent space Z should be sufficient statistics for Y , whose entropy is much lower than X.\nThis results in a highly compressed representation where sequences close in the input space\nmight be close in the latent space, and the less significant features will be compressed. In\ncontrast, in the unsupervised setup, the IB suggests compressing the input to the encoded\nrepresentation so that each input sequence can be decoded uniquely. In this case, the latent\nspace\u2019s entropy should correspond to the input space\u2019s entropy, and compression is much\nmore difficult.\n15\n4. Self-Supervised Multiview Information Bottleneck Learning\nHow can we learn without labels and still achieve good predictive power? Is compression\nnecessary to obtain an optimal representation? This section analyzes and discusses how\nto achieve optimal representation for self-supervised learning when labels are not available\nduring training. We review recent methods for self-supervised learning and show how they\ncan be integrated into a single framework. We compare their objective functions, implicit\nassumptions, and theoretical challenges. Finally, we consider the information-theoretic\nproperties of these representations, their optimality, and different ways of learning them.\nOne approach to enhance deep learning methods is to apply the InfoMax principle in a\nmultiview setting (Linsker, 1988; Wiskott and Sejnowski, 2002). As one of the earliest\napproaches, Linsker (1988) proposed maximizing information transfer from input data to\nits latent representation, showing its equivalence to maximizing the determinant of the\noutput covariance under the Gaussian distribution assumption. Becker and Hinton (1992)\nintroduced a representation learning approach based on maximizing an approximation of\nthe mutual information between alternative latent vectors obtained from the same image.\nThe most well-known application is the Independent Component Analysis (ICA) Infomax\nalgorithm (Bell and Sejnowski, 1995), designed to separate independent sources from their\nlinear combinations. The ICA-Infomax algorithm aims to maximize the mutual information\nbetween mixtures and source estimates while imposing statistical independence among\noutputs. The Deep Infomax approach (Hjelm et al., 2018) extends this idea to unsupervised\nfeature learning by maximizing the mutual information between input and output while\nmatching a prior distribution for the representations. Recent work has applied this principle\nto a self-supervised multiview setting (Bachman et al., 2019; Henaff, 2020; Hjelm et al.,\n2018; Tian et al., 2020a), wherein these works maximize the mutual information between the\nviews Z1 and Z2 using the classifier q(z1|z2), which attempts to predict one representation\nfrom the other.\nHowever, Tschannen et al. (2019) demonstrated that the effectiveness of InfoMax models is\nmore attributable to the inductive biases introduced by the architecture and estimators than\nto the training objectives themselves, as the InfoMax objectives can be trivially maximized\nusing invertible encoders. Moreover, a fundamental issue with the InfoMax principle is that\nit retains irrelevant information about the labels, contradicting the core concept of the IB\nprinciple, which advocates compressing the representation to enhance generalizability.\nTo resolve this problem, Sridharan and Kakade (2008) proposed the multiview IB framework.\nAccording to this framework, in the multiview without labels setting, the IB principle of\npreserving relevant data while compressing irrelevant data requires assumptions regarding\nthe relationship between views and labels. They presented the MultiView assumption, which\nasserts that either view (approximately) would be sufficient for downstream tasks. By this\nassumption, they define the relevant information as the shared information between the\nviews. Therefore, augmentations (such as changing the image style) should not affect the\nlabels.\n16\nTo Compress or Not to Compress\nAdditionally, the views will provide most of the information in the input regarding down-\nstream tasks. We improve generalization without affecting performance by compressing the\ninformation not shared between the two views. Their formulation is as follows:\nAssumption 1 The MultiView Assumption:\nThere exists a \u03f5info (which is assumed to\nbe small) such that\nI(Y ; X2|X1) \u2264 \u03f5info,\nI(Y ; X1|X2) \u2264 \u03f5info.\nAs a result, when the information sharing parameter, \u03f5info, is small, the information shared\nbetween views includes task-relevant details. For instance, in self-supervised contrastive\nlearning for visual data (Hjelm et al., 2018), views represent various augmentations of the\nsame image. In this scenario, the MultiView assumption is considered mild if the downstream\ntask remains unaffected by the augmentation (Geiping et al., 2022). Image augmentations\ncan be perceived as altering an image\u2019s style without changing its content. Thus, Tsai et al.\n(2020) contends that the information required for downstream tasks should be preserved in\nthe content rather than the style. This assumption allows us to separate the information into\nrelevant (shared information) and irrelevant (not shared) components and to compress only\nthe unimportant details that do not contain information about downstream tasks. Based\non this assumption, we aim to maximize the relevant information I(X2; Z1) and minimize\nI(X1; Z1 | X2) - the exclusive information that Z1 contains about X1, which cannot be\npredicted by observing X2. This irrelevant information is unnecessary for the prediction task\nand can be discarded. In the extreme case, where X1 and X2 share only label information,\nthis approach recovers the supervised IB method without labels. Conversely, if X1 and X2\nare identical, this method collapses into the InfoMax principle, as no information can be\naccurately discarded.\nFederici et al. (2020) used the relaxed Lagrangian objective to obtain the minimal sufficient\nrepresentation Z1 for X2 as:\nL1 = IP(Z1|X1)(Z1; X1 | X2) \u2212 \u03b21IP(Z2|X2),Q(Z1|Z2)(X2; Z1)\nand the symmetric loss to obtain the minimal sufficient representation Z2 for X1:\nL2 = IP(Z2|X2)(Z2; X2 | X1) \u2212 \u03b22IP(Z1|X1),Q(Z2|Z1)I(X1; Z2)\nwhere \u03b21 and \u03b22 are the Lagrangian multipliers introduced by the constraint optimization.\nBy defining Z1 and Z2 on the same domain and re-parameterizing the Lagrangian multipliers,\nthe average of the two loss functions can be upper bounded as:\nL = \u2212IP(Z1|X1),Q(Z2|Z1)(Z1; Z2) + \u03b2DSKL[p(z1 | x1)||P(z2 | x2)]\nwhere DSKL represents the symmetrized KL divergence obtained by averaging the expected\nvalue of DKL(p(z1 | x1)||p(z2 | x2)) and DKL(p(z2 | x2)||p(z1 | x1)). Note that when the\nmapping from X1 to Z1 is deterministic, I(Z1; X1 | X2) minimization and H(Z1 | X2)\nminimization are interchangeable and the algorithms of Federici et al. (2020) and Tsai et al.\n17\n(2020) minimize the same objective. Another implementation of the same idea is based on\nthe Conditional Entropy Bottleneck (CEB) algorithm (Fischer, 2020) and proposed by Lee\net al. (2021b). This algorithm adds the residual information as a compression term to the\nInfoMax objective using the reverse decoders q(z1 | x2) and q(z2 | x1).\nIn conclusion, all the algorithms mentioned above are based on the Multiview assump-\ntion. Utilizing this assumption, they can distinguish relevant information from irrelevant\ninformation. As a result, all these algorithms aim to maximize the information (or the\npredictive ability) of one representation with respect to the other view while compressing\nthe information between each representation and its corresponding view. The key differences\nbetween these algorithms lie in the decomposition and implementation of these information\nterms.\nDubois et al. (2021) offers another theoretical analysis of the IB for self-supervised learning.\nTheir work addresses the question of the minimum bit rate required to store the input but\nstill achieve high performance on a family of downstream tasks Y \u2208 Y. It is a rate-distortion\nproblem, where the goal is to find a compressed representation that will give us a good\nprediction for every task. We require that the distortion measure is bounded:\nDT(X, Z) = sup\nY \u2208Y\nH(Y | Z1) \u2212 H(Y | X1) \u2264 \u03b4.\nAccessing the downstream task is necessary to find the solution during the learning process.\nAs a result, Dubois et al. (2021) considered only tasks invariant to some equivalence relation,\nwhich divides the input into disjoint equivalence classes. An example would be an image\nwith labels that remain unchanged after augmentation. This is similar to the Multiview\nassumption where \u03f5info \u2192 0. By applying Shannon\u2019s rate-distortion theory, they concluded\nthat the minimum achievable bit rate is the rate-distortion function with the above invariance\ndistortion. Thus, the optimal rate can be determined by minimizing the following Lagrangian:\nL =\nmin\nP(Z1|X1) IP(Z1|X1)(X1; Z1) + \u03b2H(Z2 | X1).\n(6)\nUsing this objective, the maximization of information with labels is replaced by maximizing\nthe prediction ability of one view from the original input, regularized by direct information\nfrom the input. Similarly to the above results, we would like to find a representation Z1\nthat compresses the input X1 so that Z1 has the maximum information about X2.\n4.1 Implicit Compression in Self-Supervised Learning Methods\nWhile the optimal IB representation is based on the Multiview assumption, most self-\nsupervised learning models only use the infoMax principle and maximize the mutual in-\nformation I(Z1; Z2) without an explicit regularization term. However, recent studies have\nshown that contrastive learning creates compressed representations that include only relevant\ninformation (Tian et al., 2020b; Wang et al., 2022). The question is, why is the learned\nrepresentation compressed? The maximization of I(Z1; Z2) could theoretically be sufficient\n18\nTo Compress or Not to Compress\nto retain all the information from both X1 and X2 by making the representations invertible.\nIn this section, we attempt to explain this phenomenon.\nWe begin with the InfoMax principle (Linsker, 1988), which maximizes the mutual information\nbetween the representations of random variables Z1 and Z2 of the two views. We can lower-\nbound it using:\nI(Z1; Z2) = H(Z) \u2212 H(Z1 | Z2) \u2265 H(Z1) + E[log q(z1 | z2)]\n(7)\nThe bound is tight when q(z1|z2) = p(z1|z2), in which case the first term equals the\nconditional entropy H(Z1|Z2). The second term of eq. (7) can be considered a negative\nreconstruction error or distortion between Z1 and Z2.\nIn the supervised case, where Z is a learned stochastic representation of the input and Y is\nthe label, we aim to optimize\nI(Y ; Z) \u2265 H(Y ) + E [log q(Y | Z)]\n(8)\n. Since Y is constant, optimizing the information I(Z; Y ) requires only minimizing the\nprediction term E [log q(Y |Z)] by making Z more informative about Y . This term is the\ncross-entropy loss for classification or the square loss for regressions. Thus, we can minimize\nthe log loss without any other regularization on the representation.\nIn contrast, for the self-supervised case, we have a more straightforward option to minimize\nH(Z1|Z2): Making Z1 easier to predict by Z2, which can be achieved by reducing its variance\nalong specific dimensions. If we do not regularize H(Z1), it will decrease to zero, and we will\nobserve a collapse. This is why, in contrastive methods, the variance of the representation\n(large entropy) is significant only in the directions with a high variance in the data, which is\nenforced by data augmentation (Jing et al., 2021). According to this analysis, the network\nbenefits from making the representations \u201dsimple\u201d (easier to predict). Hence, even though\nour representation does not have explicit information-theoretical constraints, the learning\nprocess will compress the representation.\n4.2 Beyond the Multiview Assumption\nAccording to the Multiview IB analysis presented in Section 4, the optimal way to create a\nuseful representation is to maximize the mutual information between the representations of\ndifferent views while compressing irrelevant information in each representation. In fact, as\ndiscussed in Section 4.1, we can achieve this optimal compressed representation even without\nexplicit regularization. However, this optimality is based on the Multiview assumption,\nwhich states that the relevant information for downstream tasks comes from the information\nshared between views. Therefore, Tian et al. (2020b) concluded that when a minimal\nsufficient representation has been obtained, the optimal views for self-supervised learning\nare determined by downstream tasks.\nHowever, the Multiview assumption is highly constrained, as all relevant information must\nbe shared between all views. In cases where this assumption is incorrect, such as with\n19\naggressive data augmentation or multiple downstream tasks or modalities, sharing all the\nnecessary information can be challenging. For example, if one view is a video stream while\nthe other is an audio stream, the shared information may be sufficient for object recognition\nbut not for tracking. Furthermore, relevant information for downstream tasks may not be\ncontained within the shared information between views, meaning that removing non-shared\ninformation can negatively impact performance.\nKahana and Hoshen (2022) identified a series of tasks that violate the Multiview assumption.\nTo accomplish these tasks, the learned representation must also be invariant to unwanted\nattributes, such as bias removal and cross-domain retrieval.\nIn such cases, only some\nattributes have labels, and the objective is to learn an invariant representation for the\ndomain for which labels are provided while also being informative for all other attributes\nwithout labels. For example, for face images, only the identity labels may be provided, and\nthe goal is to learn a representation that captures the unlabeled pose attribute but contains\nno information about the identity attribute. The task can also be applied to fair decisions,\ncross-domain matching, model anonymization, and image translation.\nWang et al. (2022) formalized another case where the Multiview assumption does not\nhold when non-shared task-relevant information cannot be ignored. In such cases, the\nminimal sufficient representation contains less task-relevant information than other sufficient\nrepresentations, resulting in inferior performance. Furthermore, their analysis shows that in\nsuch cases, the learned representation in contrastive learning is insufficient for downstream\ntasks, which may overfit the shared information.\nAs a result of their analysis, Wang et al. (2022) and Kahana and Hoshen (2022) proposed\nexplicitly increasing mutual information between the representation and input to preserve\ntask-relevant information and prevent the compression of unshared information between\nviews. In this case, the two regularization terms of the two views are incorporated into the\noriginal InfoMax objective, and the following objective is optimized:\nL =\nmin\nP(Z1|X1),p(Z2|X2) \u2212IP(Z1|X1)(X1; Z1) \u2212 IP(Z2|X2)(X2; Z2) \u2212 \u03b2IP(Z1|X1),P(Z2|Z1)(Z1; Z2).\n(9)\nWang et al. (2022) demonstrated the effectiveness of their method for SimCLR (Chen\net al., 2020a), BYOL (Grill et al., 2020), and Barlow Twins (Zbontar et al., 2021) across\nclassification, detection, and segmentation tasks.\n4.3 To Compress or Not to Compress?\nAs seen in Eq. 9, when the Multiview assumption is violated, the objective for obtaining an\noptimal representation is to maximize the mutual information between each input and its\nrepresentation. This contrasts with the situation in which the Multiview assumption holds,\nor the supervised case, where the objective is to minimize the mutual information between\nthe representation and the input. In both supervised and unsupervised cases, we have direct\naccess to the relevant information, which we can use to separate and compress irrelevant\ninformation. However, in the self-supervised case, we depend heavily on the Multiview\nassumption. If this assumption is violated due to unshared information between views that\n20\nTo Compress or Not to Compress\nis relevant for the downstream task, we cannot separate relevant and irrelevant information.\nFurthermore, the learning algorithm\u2019s nature requires that this information be protected by\nexplicitly maximizing it.\nAs datasets continue to expand in size and models are anticipated to serve as base models for\nvarious downstream tasks, the Multiview assumption becomes less pertinent. Consequently,\ncompressing irrelevant information when the Multiview assumption does not hold presents\none of the most significant challenges in self-supervised learning. Identifying new methods\nto separate relevant from irrelevant information based on alternative assumptions is a\npromising avenue for research. It is also essential to recognize that empirical measurement\nof information-theoretic quantities and their estimators plays a crucial role in developing\nand evaluating such methods.\n5. Optimizing Information in Deep Neural Networks: Challenges and\nApproaches\nRecent years have seen information-theoretic analyses employed to explain and optimize deep\nlearning techniques (Shwartz-Ziv and Tishby, 2017). Despite their elegance and plausibility,\nempirically measuring and analyzing information in deep networks presents challenges. Two\ncritical problems are (1) information in deterministic networks and (2) estimating information\nin high-dimensional spaces.\nInformation in Deterministic Networks\nInformation-theoretic methods have significantly impacted deep learning (Alemi et al., 2016;\nShwartz-Ziv and Tishby, 2017; Steinke and Zakynthinou, 2020). However, a key challenge is\naddressing the source of randomness in deterministic DNNs.\nThe mutual information between the input and representation is infinite, leading to ill-posed\noptimization problems or piecewise constant outcomes (Amjad and Geiger, 2019; Goldfeld\net al., 2018). To tackle this issue, researchers have proposed various solutions. One common\napproach is to discretize the input distribution and real-valued hidden representations by\nbinning, which facilitates non-trivial measurements and prevents the mutual information\nfrom always taking the maximum value of the log of the dataset size, thus avoiding ill-posed\noptimization problems (Shwartz-Ziv and Tishby, 2017).\nHowever, binning and discretization are essentially equivalent to geometrical compression\nand serve as clustering measures (Goldfeld et al., 2018). Moreover, this discretization\ndepends on the chosen bin size and does not track the mutual information across varying\nbin sizes Goldfeld et al. (2018); Ross (2014). To address these limitations, researchers have\nproposed alternative approaches such as interpreting binned information as a weight decay\npenalty Elad et al. (2019b), estimating mutual information based on lower bounds assuming\na continuous input distribution without making assumptions about the network\u2019s output\ndistribution properties (Shwartz-Ziv et al., 2022a; Wang and Isola, 2020; Zimmermann et al.,\n2021), injecting additive noise, and considering data augmentation as the source of noise\n(Dubois et al., 2021; Goldfeld et al., 2018; Lee et al., 2021b; Shwartz-Ziv and Tishby, 2017).\n21\nMeasuring Information in High-Dimensional Spaces\nEstimating mutual information in high-dimensional spaces presents a significant challenge\nwhen applying information-theoretic measures to real-world data. This problem has been\nextensively studied (Gao et al., 2015; Paninski, 2003), revealing the inefficiency of solutions\nfor large dimensions and the limited scalability of known approximations with respect to\nsample size and dimension. Despite these difficulties, various entropy and mutual information\nestimation approaches have been developed, including classic methods like k-nearest neighbors\n(KNN) (Kozachenko and Leonenko, 1987) and kernel density estimation techniques (Hang\net al., 2018), as well as more recent efficient methods.\nChelombiev et al. (2019) developed adaptive mutual information estimators based on\nentropies-equal bins and scaled noise kernel density estimator. Generative decoder networks,\nsuch as PixelCNN++ (Van den Oord et al., 2016), have been employed to estimate a lower\nbound on mutual information (Darlow and Storkey, 2020; Nash et al., 2018; Shwartz-Ziv\net al., 2023). Another strategy includes ensemble dependency graph estimators, adaptive\nmutual information estimation methods (EDGE) by merging randomized locality-sensitive\nhashing (LSH), dependency graphs, and ensemble bias reduction techniques (Noshad and\nHero III, 2018). The Mutual Information Neural Estimator (MINE) (Belghazi et al., 2018a)\nmaximizes KL divergence using the dual representation of Donsker and Varadhan (1975) and\nhas been employed for direct mutual information estimation (Elad et al., 2019a). Shwartz-Ziv\nand Alemi (2020) developed a controlled framework that utilized the neural tangent kernels\n(Jacot et al., 2018), in order to obtain tractable information measures.\nImproving mutual information estimation can be achieved using larger batch sizes, although\nthis may negatively impact generalization performance and memory requirements. Alterna-\ntively, researchers have suggested employing surrogate measures for mutual information, such\nas log-determinant mutual information (LDMI), based on second-order statistics (Erdogan,\n2022; Ozsoy et al., 2022), which reflects linear dependence. Goldfeld and Greenewald (2021)\nproposed the Sliced Mutual Information (SMI), defined as an average of MI terms between\none-dimensional projections of high-dimensional variables. SMI inherits many properties\nof its classic counterpart. It can be estimated with optimal parametric error rates in all\ndimensions by combining an MI estimator between scalar variables with an MC integrator\n(Goldfeld and Greenewald, 2021). The k-SMI, introduced by Goldfeld et al. (2022), extends\nthe SMI by projecting to k-dimensional subspace, which relaxes the smoothness assumptions,\nimproves scalability, and enhances performance.\nIn conclusion, estimating and optimizing information in deep neural networks presents\nsignificant challenges, particularly in deterministic networks and high-dimensional spaces.\nResearchers have proposed various approaches to address these issues, including discretization,\nalternative estimators, and surrogate measures. As the field continues to evolve, it is expected\nthat more advanced techniques will emerge to overcome these challenges and facilitate the\nunderstanding and optimization of deep learning models.\n22\nTo Compress or Not to Compress\n6. Future Research Directions\nDespite the solid foundation established by existing self-supervised learning methods from\nan information theory perspective, several potential research directions warrant exploration:\nSelf-supervised learning with non-shared information.\nAs discussed in Section 4,\nthe separation of relevant (preserved) and irrelevant (compressed) information relies on\nthe Multiview Assumption. This assumption, which states that only shared information is\nessential for downstream tasks, is rather restrictive. For example, situations may arise where\neach view contains distinct information relevant to a downstream task or multiple tasks\nnecessitate different features. Some methods have been proposed to tackle this problem, but\nthey mainly focus on maximizing the network\u2019s information without explicit constraints.\nFormalizing this scenario and exploring differentiating between relevant and irrelevant data\nbased on non-shared information represents an intriguing research direction.\nSelf-supervised learning for tabular data.\nAt present, the internal compression of\nself-supervised learning methods may compress relevant information due to improper aug-\nmentation 4.1. Consequently, we must heavily rely on generating the two views, which must\naccurately represent information related to the downstream process. Custom augmentation\nmust be developed for each domain, taking into account extensive prior knowledge on data\naugmentation. While some papers have attempted to extend self-supervised learning to\ntabular data (Arik and Pfister, 2021; Ucar et al., 2021), further work is necessary from\nboth theoretical and practical standpoints to achieve high performance with self-supervised\nlearning for tabular data (Shwartz-Ziv and Armon, 2022). The augmentation process is\ncrucial for the performance of current vision and text models. In the case of tabular data,\nemploying information-theoretic loss functions that do not require information compression\nmay help harness the benefits of self-supervised learning.\nIntegrating other learning methods into the information-theoretic framework.\nPrior works have investigated various supervised, unsupervised, semi-supervised, and self-\nsupervised learning methods, demonstrating that they optimize information-theoretic quanti-\nties. However, state-of-the-art methods employ additional changes and engineering practices\nthat may be related to information theory, such as the stop gradient operation utilized by\nmany self-supervised learning methods today (Chen and He, 2021; Grill et al., 2020). The\nExpectation-Maximization (EM) algorithm (Dempster et al., 1977) can be employed to\nexplain this operation when one path is the E-step and the other is the M-step. Additionally,\nElidan and Friedman (2012) proposed an IB-inspired version of the EM, which could help\ndevelop information-theoretic-based objectives using the stop gradient operation.\nExpanding the analysis to usable information.\nWhile information theory offers a\nrigorous conceptual framework for describing information, it neglects essential aspects of\ncomputation. (Conditional) entropy, for example, is directly related to the predictability\nof a random variable in a betting game where agents are rewarded for accurate guesses.\nHowever, the standard definition assumes that agents have no computational bounds and\ncan employ arbitrarily complex prediction schemes (Cover, 1999). In the context of deep\n23\nlearning, predictive information H(Y |Z) measures the amount of information that can be\nextracted from Z about Y given access to all decoders p(y|z) in the world. Recently, Xu\net al. (2020) introduced predictive V-information as an alternative formulation based on\nrealistic computational constraints.\nExtending self-supervised learning\u2019s information-based perspective to energy-\nbased model optimization.\nUntil now, research combining self-supervised learning\nwith information theory has focused on probabilistic models with tractable likelihoods.\nThese models enable specific optimization of model parameters concerning the tractable\nlog-likelihood (Dinh et al., 2016; Germain et al., 2015; Graves, 2013; Rezende and Mohamed,\n2015) or a tractable lower bound of the likelihood (Alemi et al., 2016; Kingma and Welling,\n2019). Although models with tractable likelihoods offer certain benefits, their scope is\nlimited and necessitates a particular format. Energy-based models (EBMs) present a more\nflexible, unified framework. Rather than specifying a normalized probability, EBMs define\ninference as minimizing an unnormalized energy function and learning as minimizing a loss\nfunction. The energy function does not require integration and can be parameterized with any\nnonlinear regression function. Inference typically involves finding a low-energy configuration\nor sampling from all possible configurations such that the probability of selecting a specific\nconfiguration follows a Gibbs distribution (Huembeli et al., 2022; Song and Kingma, 2021).\nInvestigating energy-based models for self-supervised learning from both theoretical and\npractical perspectives can open up numerous promising research directions. For instance, we\ncould directly apply tools developed for energy-based models and statistical machines to\noptimize the model, such as Maximum Likelihood Training with MCMC (Younes, 1999),\nscore matching (Hyv\u00a8arinen, 2006), denoising score matching (Song et al., 2020; Vincent,\n2011), and score-based generation models (Song and Ermon, 2019).\nExpanding the multiview framework to accommodate more views and tasks.\nThe multiview self-supervised IB framework can be extended to cases involving more than\ntwo views (X1, \u00b7 \u00b7 \u00b7 , Xn) and multiple downstream tasks (Y1, \u00b7 \u00b7 \u00b7 , YK). A simple extension of\nthe multiview IB framework can be achieved by setting the objective function to maximize\nthe joint mutual information of all views\u2019 representations I(Z1; \u00b7 \u00b7 \u00b7 Zn) and compressing the\nindividual information for each view I(Xi; Zi),\n1 \u2264 i \u2264 N However, to ensure the optimality\nof this objective, we must expand the multiview assumption to include more than two views.\nIn this scenario, we need to assume that relevant information is shared among all different\nviews and tasks, which might be overly restrictive. As a result, defining and analyzing a\nmore refined version of this naive solution is essential. One potential approach involves\nutilizing the Multi-feature Information Bottleneck (MfIB) (Lou et al., 2013), which extends\nthe original IB. The MfIB processes multiple feature types simultaneously and analyzes\ndata from various sources. This framework establishes a joint distribution between the\nmultivariate data and the model. Rather than solely preserving the information of one feature\nvariable maximally, the MfIB concurrently maintains multiple feature variables\u2019 information\nwhile compressing them. The MfIB characterizes the relationships between different sources\nand outputs by employing the multivariate Information Bottleneck (Friedman et al., 2013)\nand setting Bayesian networks.\n24\nTo Compress or Not to Compress\n7. Conclusion\nIn this study, we delved deeply into the concept of optimal representation in self-supervised\nlearning through the lens of information theory. We synthesized various approaches, high-\nlighting their foundational assumptions and constraints, and integrated them into a unified\nframework. Additionally, we explored the key information-theoretic terms that influence\nthese optimal representations and the methods for estimating them.\nWhile supervised and unsupervised learning offer more direct access to relevant information,\nself-supervised learning depends heavily on assumptions about the relationship between data\nand downstream tasks. This reliance makes distinguishing between relevant and irrelevant\ninformation considerably more challenging, necessitating further assumptions.\nDespite these challenges, information theory stands out as a robust and versatile framework\nfor analysis and algorithmic development. This adaptable framework caters to a range of\nlearning paradigms and elucidates the inherent assumptions underpinning data and model\noptimization.\nWith the rapid growth of datasets and the increasing expectations placed on models to\nhandle multiple downstream tasks, the traditional Multi-view assumption might become less\nreliable. One significant challenge in self-supervised learning is the precise compression of\nirrelevant information, especially when these assumptions are compromised.\nFuture research avenues might involve expanding the Multi-view framework to include more\nviews and tasks and deepening our understanding of information theory\u2019s impact on facets\nof deep learning, such as reinforcement learning and generative models.\nIn summary, information theory is a crucial tool in our quest to understand better and\noptimize self-supervised learning models. By harnessing its principles, we can more adeptly\nnavigate the intricacies of deep neural network development, paving the way for creating\nmore effective models.\n-\n25\nReferences\nAlessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in\ndeep representations. The Journal of Machine Learning Research, 19(1):1947\u20131980, 2018.\nAlessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep\nneural networks. arXiv preprint arXiv:1711.08856, 2017.\nMahbubul Alam, Manar D Samad, Lasitha Vidyaratne, Alexander Glandon, and Khan M\nIftekharuddin. Survey on deep neural networks in speech and vision systems. Neurocom-\nputing, 417:302\u2013321, 2020.\nAlexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational\ninformation bottleneck. arXiv:1612.00410, 2016. URL http://arxiv.org/abs/1612.\n00410.\nRana Ali Amjad and Bernhard C Geiger. How (not) to train your neural network using the\ninformation bottleneck principle. arXiv preprint arXiv:1802.09766, 2018.\nRana Ali Amjad and Bernhard Claus Geiger. Learning representations for neural network-\nbased classification using the information bottleneck principle. IEEE transactions on\npattern analysis and machine intelligence, 2019.\nGalen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep canonical correlation\nanalysis. In International conference on machine learning, pages 1247\u20131255. PMLR, 2013.\nSercan \u00a8O Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 6679\u20136687,\n2021.\nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj\nSaunshi. A theoretical analysis of contrastive unsupervised representation learning. arXiv\npreprint arXiv:1902.09229, 2019.\nFrancis R Bach and Michael I Jordan. Kernel independent component analysis. Journal of\nmachine learning research, 3(Jul):1\u201348, 2002.\nFrancis R. Bach and Michael I. Jordan. Kernel independent component analysis. J. Mach.\nLearn. Res., 3(null):1\u201348, mar 2003. ISSN 1532-4435. doi: 10.1162/153244303768966085.\nURL https://doi.org/10.1162/153244303768966085.\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by\nmaximizing mutual information across views. Advances in neural information processing\nsystems, 32, 2019.\nAmir Bar, Xin Wang, Vadim Kantorov, Colorado J Reed, Roei Herzig, Gal Chechik, Anna\nRohrbach, Trevor Darrell, and Amir Globerson. Detreg: Unsupervised pretraining with\nregion priors for object detection.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 14605\u201314615, 2022.\n26\nTo Compress or Not to Compress\nAdrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance\nregularization for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.\nMina Basirat, Bernhard C. Geiger, and Peter M. Roth.\nA geometric perspective on\ninformation plane analysis.\nEntropy, 23(6), 2021.\nISSN 1099-4300.\nURL https://\nwww.mdpi.com/1099-4300/23/6/711.\nSuzanna Becker and Geoffrey E Hinton. Self-organizing neural network that discovers\nsurfaces in random-dot stereograms. Nature, 355(6356):161\u2013163, 1992.\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio,\nR. Devon Hjelm, and Aaron C. Courville. Mutual information neural estimation. In\nICML, 2018a.\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio,\nAaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv\npreprint arXiv:1801.04062, 2018b.\nAnthony J Bell and Terrence J Sejnowski. An information-maximization approach to blind\nseparation and blind deconvolution. Neural computation, 7(6):1129\u20131159, 1995.\nIdo Ben-Shaul, Ravid Shwartz-Ziv, Tomer Galanti, Shai Dekel, and Yann LeCun. Reverse\nengineering self-supervised learning. arXiv preprint arXiv:2305.15614, 2023.\nYoshua Bengio and Yann LeCun. Scaling learning algorithms towards ai ,in l. bottou, o.\nchapelle, d. decoste, and j. weston, editors,. Large Scale Kernel Machines,MIT Press.,\n2007.\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review\nand new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35\n(8):1798\u20131828, 2013.\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and\nColin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in\nNeural Information Processing Systems, 32, 2019.\nJane Bromley, Isabelle Guyon, Yann LeCun, Eduard S\u00a8ackinger, and Roopak Shah. Signature\nverification using a\u201d siamese\u201d time delay neural network. Advances in neural information\nprocessing systems, 6, 1993.\nLars Buesing and Wolfgang Maass. A spiking neuron as information bottleneck. Neural\ncomputation, 22(8):1961\u20131992, 2010.\nTian Cao, Vladimir Jojic, Shannon Modla, Debbie Powell, Kirk Czymmek, and Marc\nNiethammer. Robust multimodal dictionary learning. In Kensaku Mori, Ichiro Sakuma,\nYoshinobu Sato, Christian Barillot, and Nassir Navab, editors, Medical Image Computing\nand Computer-Assisted Intervention \u2013 MICCAI 2013, pages 259\u2013266, Berlin, Heidelberg,\n2013. Springer Berlin Heidelberg. ISBN 978-3-642-40811-3.\n27\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand\nJoulin.\nUnsupervised learning of visual features by contrasting cluster assignments.\nAdvances in Neural Information Processing Systems, 33:9912\u20139924, 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin.\nEmerging properties in self-supervised vision transformers.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages\n9650\u20139660, 2021.\nOlivier Chapelle, Bernhard Scholkopf, and Alexander Zien.\nSemi-supervised learning\n(chapelle, o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20\n(3):542\u2013542, 2009.\nIvan Chelombiev, Conor Houghton, and Cian O\u2019Donnell. Adaptive estimators show informa-\ntion compression in deep neural networks. arXiv preprint arXiv:1902.09037, 2019.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework\nfor contrastive learning of visual representations. In International conference on machine\nlearning, pages 1597\u20131607. PMLR, 2020a.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n15750\u201315758, 2021.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020b.\nSumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively,\nwith application to face verification. In 2005 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR\u201905), volume 1, pages 539\u2013546. IEEE,\n2005.\nThomas M Cover. Elements of information theory. John Wiley & Sons, 1999.\nLuke Nicholas Darlow and Amos Storkey. What information does a resnet compress? arXiv\npreprint arXiv:2003.06254, 2020.\nArthur P Dempster, Nan M Laird, and Donald B Rubin.\nMaximum likelihood from\nincomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B\n(Methodological), 39(1):1\u201322, 1977.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert:\nPre-\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp.\narXiv preprint arXiv:1605.08803, 2016.\n28\nTo Compress or Not to Compress\nJeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini\nVenugopalan, Kate Saenko, and Trevor Darrell.\nLong-term recurrent convolutional\nnetworks for visual recognition and description. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2625\u20132634, 2015.\nMonroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov\nprocess expectations for large time, i. Communications on Pure and Applied Mathematics,\n28(1):1\u201347, 1975.\nYann Dubois, Benjamin Bloem-Reddy, Karen Ullrich, and Chris J Maddison. Lossy com-\npression for lossless prediction. Advances in Neural Information Processing Systems, 34,\n2021.\nAdar Elad, Doron Haviv, Yochai Blau, and Tomer Michaeli.\nDirect validation of the\ninformation bottleneck principle for deep nets. In Proceedings of the IEEE International\nConference on Computer Vision Workshops, 2019a.\nAdar Elad, Doron Haviv, Yochai Blau, and Tomer Michaeli. The effectiveness of layer-by-layer\ntraining using the information bottleneck principle, 2019b. URL https://openreview.\nnet/forum?id=r1Nb5i05tX.\nGal Elidan and Nir Friedman. The information bottleneck em algorithm. arXiv preprint\narXiv:1212.2460, 2012.\nAlper T Erdogan. An information maximization based blind source separation approach for\ndependent and independent sources. In ICASSP 2022-2022 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pages 4378\u20134382. IEEE, 2022.\nDeniz Erdogmus. Information theoretic learning: Renyi\u2019s entropy and its applications to\nadaptive system training. University of Florida, 2002.\nMarco Federici, Anjan Dutta, Patrick Forr\u00b4e, Nate Kushman, and Zeynep Akata. Learn-\ning robust representations via multi-view information bottleneck.\narXiv preprint\narXiv:2002.07017, 2020.\nIan Fischer. The conditional entropy bottleneck. Entropy, 22(9):999, 2020.\nNir Friedman, Ori Mosenzon, Noam Slonim, and Naftali Tishby. Multivariate information\nbottleneck. arXiv preprint arXiv:1301.2270, 2013.\nShuyang Gao, Greg Ver Steeg, and Aram Galstyan. Efficient estimation of mutual information\nfor strongly dependent variables. In Artificial Intelligence and Statistics, pages 277\u2013286,\n2015.\nBernhard C Geiger. On information plane analyses of neural network classifiers\u2013a review.\narXiv preprint arXiv:2003.09671, 2020.\nJonas Geiping, Micah Goldblum, Gowthami Somepalli, Ravid Shwartz-Ziv, Tom Goldstein,\nand Andrew Gordon Wilson. How much data are augmentations worth? an investigation\ninto scaling laws, invariance, and implicit regularization. arXiv preprint arXiv:2210.06441,\n2022.\n29\nMathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle.\nMade: Masked\nautoencoder for distribution estimation. In International Conference on Machine Learning,\npages 881\u2013889. PMLR, 2015.\nZ. Goldfeld, E. van den Berg, K. Greenewald, I. Melnyk, N. Nguyen, B. Kingsbury, and\nY. Polyanskiy. Estimating Information Flow in Neural Networks. ArXiv e-prints, 2018.\nZiv Goldfeld and Kristjan Greenewald. Sliced mutual information: A scalable measure of\nstatistical dependence. Advances in Neural Information Processing Systems, 34:17567\u2013\n17578, 2021.\nZiv Goldfeld, Kristjan Greenewald, Theshani Nuradha, and Galen Reeves. k-sliced mu-\ntual information: A quantitative study of scalability with dimension. arXiv preprint\narXiv:2206.08526, 2022.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. Book in preparation\nfor MIT Press, 2016. URL http://www.deeplearningbook.org.\nYves Grandvalet and Yoshua Bengio. Entropy regularization., 2006.\nAlex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.\nJean-Bastien Grill, Florian Strub, Florent Altch\u00b4e, Corentin Tallec, Pierre Richemond,\nElena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad\nGheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised\nlearning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an\ninvariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR\u201906), volume 2, pages 1735\u20131742. IEEE, 2006.\nHanyuan Hang, Ingo Steinwart, Yunlong Feng, and Johan AK Suykens. Kernel density\nestimation for dynamical systems. The Journal of Machine Learning Research, 19(1):\n1260\u20131308, 2018.\nDavid R. Hardoon, Sandor Szedmak, and John Shawe-Taylor. Canonical correlation analysis:\nAn overview with application to learning methods. Neural Computation, 16(12):2639\u20132664,\n2004. doi: 10.1162/0899766042321814.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. CoRR, abs/1512.03385, 2015.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 9729\u20139738, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00b4ar, and Ross Girshick. Masked\nautoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022.\n30\nTo Compress or Not to Compress\nRon M Hecht, Elad Noor, and Naftali Tishby. Speaker recognition by gaussian information\nbottleneck. In Tenth Annual Conference of the International Speech Communication\nAssociation, 2009.\nOlivier Henaff. Data-efficient image recognition with contrastive predictive coding. In\nInternational Conference on Machine Learning, pages 4182\u20134192. PMLR, 2020.\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew\nBotvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual\nconcepts with a constrained variational framework. In ICLR, 2017.\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman,\nAdam Trischler, and Yoshua Bengio. Learning deep representations by mutual information\nestimation and maximization. arXiv preprint arXiv:1808.06670, 2018.\nHarold Hotelling. Relations between two sets of variates. Biometrika, 28(3/4):321\u2013377, 1936.\nISSN 00063444. URL http://www.jstor.org/stable/2333955.\nZhenyu Huang, Joey Tianyi Zhou, Xi Peng, Changqing Zhang, Hongyuan Zhu, and Jiancheng\nLv. Multi-view spectral clustering network. In IJCAI, pages 2563\u20132569, 2019.\nPatrick Huembeli, Juan Miguel Arrazola, Nathan Killoran, Masoud Mohseni, and Peter\nWittek. The physics of energy-based models. Quantum Machine Intelligence, 4(1):1\u201313,\n2022.\nAapo Hyv\u00a8arinen. Some extensions of score matching, 2006.\nArthur Jacot, Franck Gabriel, and Cl\u00b4ement Hongler. Neural tangent kernel: Convergence\nand generalization in neural networks. Advances in neural information processing systems,\n31, 2018.\nYangqing Jia, Mathieu Salzmann, and Trevor Darrell. Factorized latent spaces with struc-\ntured sparsity.\nIn J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Cu-\nlotta, editors, Advances in Neural Information Processing Systems, volume 23. Cur-\nran Associates, Inc., 2010. URL https://proceedings.neurips.cc/paper/2010/file/\na49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf.\nLi Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. Understanding dimensional\ncollapse in contrastive self-supervised learning. arXiv preprint arXiv:2110.09348, 2021.\nJonathan Kahana and Yedid Hoshen. A contrastive objective for learning disentangled\nrepresentations. arXiv preprint arXiv:2203.11284, 2022.\nAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image\ndescriptions. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 3128\u20133137, 2015.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114, 2013.\n31\nDiederik P Kingma and Max Welling. An introduction to variational autoencoders. arXiv\npreprint arXiv:1906.02691, 2019.\nDurk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-\nsupervised learning with deep generative models. Advances in neural information processing\nsystems, 27, 2014.\nBernard Osgood Koopman. On distributions admitting a sufficient statistic. Transactions\nof the American Mathematical society, 39(3):399\u2013409, 1936.\nLyudmyla F Kozachenko and Nikolai N Leonenko. Sample estimate of the entropy of a\nrandom vector. Problemy Peredachi Informatsii, 23(2):9\u201316, 1987.\nAbhishek Kumar and Hal Daum\u00b4e. A co-training approach for multi-view spectral clustering.\nIn Proceedings of the 28th international conference on machine learning (ICML-11), pages\n393\u2013400. Citeseer, 2011.\nSamuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv\npreprint arXiv:1610.02242, 2016.\nAnders Boesen Lindbo Larsen, S\u00f8ren Kaae S\u00f8nderby, Hugo Larochelle, and Ole Winther.\nAutoencoding beyond pixels using a learned similarity metric. In International conference\non machine learning, pages 1558\u20131566. PMLR, 2016.\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, \u201d\u201d, 2015.\nDong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning\nmethod for deep neural networks. In Workshop on challenges in representation learning,\nICML, volume 3, page 896, 2013.\nHonglak Lee, Alexis Battle, Rajat Raina, and Andrew Ng. Efficient sparse coding algorithms.\nIn B. Sch\u00a8olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Pro-\ncessing Systems, volume 19. MIT Press, 2006. URL https://proceedings.neurips.cc/\npaper_files/paper/2006/file/2d71b2ae158c7c5912cc0bbde2bb9d95-Paper.pdf.\nJason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already\nknow helps: Provable self-supervised learning. Advances in Neural Information Processing\nSystems, 34, 2021a.\nKuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, and Ian Fischer. Com-\npressive visual representations. Advances in Neural Information Processing Systems, 34,\n2021b.\nYingming Li, Ming Yang, and Zhongfei Zhang. A survey of multi-view representation\nlearning. IEEE transactions on knowledge and data engineering, 31(10):1863\u20131883, 2018.\nRalph Linsker. Self-organization in a perceptual network. Computer, 21(3):105\u2013117, 1988.\nShiming Liu, Yifan Xia, Zhusheng Shi, Hui Yu, Zhiqiang Li, and Jianguo Lin. Deep learning\nin sheet metal bending with a novel theory-guided deep neural network. IEEE/CAA\nJournal of Automatica Sinica, 8(3):565\u2013581, 2021a.\n32\nTo Compress or Not to Compress\nWeifeng Liu, Dacheng Tao, Jun Cheng, and Yuanyan Tang. Multiview hessian discriminative\nsparse coding for image annotation. Computer Vision and Image Understanding, 118:\n50\u201360, 2014. ISSN 1077-3142. doi: https://doi.org/10.1016/j.cviu.2013.03.007. URL\nhttps://www.sciencedirect.com/science/article/pii/S1077314213001550.\nXiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang.\nSelf-supervised learning: Generative or contrastive. IEEE Transactions on Knowledge\nand Data Engineering, 2021b.\nZhengzheng Lou, Yangdong Ye, and Xiaoqiang Yan. The multi-feature information bottleneck\nwith application to unsupervised image categorization. In Twenty-Third International\nJoint Conference on Artificial Intelligence, 2013.\nAlireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey.\nAdversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.\nJunhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille. Deep caption-\ning with multimodal recurrent neural networks (m-rnn). arXiv preprint arXiv:1412.6632,\n2014.\nIshan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant\nrepresentations. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 6707\u20136717, 2020.\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial\ntraining: a regularization method for supervised and semi-supervised learning. IEEE\ntransactions on pattern analysis and machine intelligence, 41(8):1979\u20131993, 2018.\nCharlie Nash, Nate Kushman, and Christopher KI Williams. Inverting supervised repre-\nsentations with autoregressive neural density models. arXiv preprint arXiv:1806.00400,\n2018.\nAndrew Ng et al. Sparse autoencoder. CS294A Lecture notes, 72(2011):1\u201319, 2011.\nJiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y.\nNg. Multimodal deep learning. In Proceedings of the 28th International Conference on\nInternational Conference on Machine Learning, ICML\u201911, page 689\u2013696, Madison, WI,\nUSA, 2011. Omnipress. ISBN 9781450306195.\nMorteza Noshad and Alfred O Hero III. Scalable mutual information estimation using\ndependence graphs. arXiv preprint arXiv:1801.09125, 2018.\nSerdar Ozsoy, Shadi Hamdan, Sercan Arik, Deniz Yuret, and Alper Erdogan. Self-supervised\nlearning with an information maximization criterion. Advances in Neural Information\nProcessing Systems, 35:35240\u201335253, 2022.\nAmichai Painsky and Gregory W Wornell. On the universality of the logistic loss function.\narXiv preprint arXiv:1805.03804, 2018.\n33\nStephanie E Palmer, Olivier Marre, Michael J Berry, and William Bialek.\nPredictive\ninformation in a sensory population. Proceedings of the National Academy of Sciences,\n112(22):6908\u20136913, 2015.\nLiam Paninski. Estimation of entropy and mutual information. Neural Comput., 15(6):\n1191\u20131253, 2003. ISSN 0899-7667. doi: 10.1162/089976603321780272.\nAnkit Pensia, Varun Jog, and Po-Ling Loh. Generalization error bounds for noisy, iterative\nalgorithms. In 2018 IEEE International Symposium on Information Theory (ISIT), pages\n546\u2013550. IEEE, 2018.\nZoe Piran, Ravid Shwartz-Ziv, and Naftali Tishby. The dual information bottleneck. arXiv\npreprint arXiv:2006.04641, 2020.\nShi Pu, Yijiang He, Zheng Li, and Mao Zheng.\nMultimodal topic learning for video\nrecommendation. arXiv preprint arXiv:2010.13373, 2020.\nJ Ross Quinlan. C4. 5: programs for machine learning. Elsevier, 2014.\nDanilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In\nInternational conference on machine learning, pages 1530\u20131538. PMLR, 2015.\nBrian C Ross. Mutual information between discrete and continuous data sets. PLoS ONE, 9\n(2):e87357, 2014. doi: 10.1371/journal.pone.0087357. URL https://doi.org/10.1371/\njournal.pone.0087357.\nAurko Roy, Ashish Vaswani, Arvind Neelakantan, and Niki Parmar. Theory and experiments\non vector quantized autoencoders. arXiv preprint arXiv:1805.11063, 2018.\nDaniel Russo and James Zou. How much does your data exploration overfit? controlling bias\nvia information usage. IEEE Transactions on Information Theory, 66(1):302\u2013323, 2019.\nAndrew M Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Bren-\ndan D Tracey, and David D Cox. On the information bottleneck theory of deep learning.\nJournal of Statistical Mechanics: Theory and Experiment, 2019(12):124020, 2019.\nOhad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the informa-\ntion bottleneck. Theoretical Computer Science, 411(29):2696 \u2013 2711, 2010. ISSN 0304-3975.\ndoi: https://doi.org/10.1016/j.tcs.2010.04.006. URL http://www.sciencedirect.com/\nscience/article/pii/S030439751000201X. Algorithmic Learning Theory (ALT 2008).\nRavid Shwartz-Ziv.\nInformation flow in deep neural networks.\narXiv preprint\narXiv:2202.06749, 2022.\nRavid Shwartz-Ziv and Alexander A Alemi. Information in infinite ensembles of infinitely-\nwide neural networks. In Symposium on Advances in Approximate Bayesian Inference,\npages 1\u201317. PMLR, 2020.\nRavid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need.\nInformation Fusion, 81:84\u201390, 2022.\n34\nTo Compress or Not to Compress\nRavid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via\ninformation. arXiv preprint arXiv:1703.00810, 2017.\nRavid Shwartz-Ziv, Amichai Painsky, and Naftali Tishby. Representation compression and\ngeneralization in deep neural networks, 2018.\nRavid Shwartz-Ziv, Randall Balestriero, and Yann LeCun.\nWhat do we maximize in\nself-supervised learning? arXiv preprint arXiv:2207.10081, 2022a.\nRavid Shwartz-Ziv, Micah Goldblum, Hossein Souri, Sanyam Kapoor, Chen Zhu, Yann\nLeCun, and Andrew G Wilson. Pre-train your loss: Easy bayesian transfer learning with\ninformative priors. Advances in Neural Information Processing Systems, 35:27706\u201327715,\n2022b.\nRavid Shwartz-Ziv, Randall Balestriero, Kenji Kawaguchi, Tim GJ Rudner, and Yann LeCun.\nAn information-theoretic perspective on variance-invariance-covariance regularization.\narXiv preprint arXiv:2303.00633, 2023.\nM Smieja, M Wolczyk, J Tabor, and B Geiger. Segma: Semi-supervised gaussian mixture\nauto-encoder. arXiv preprint arXiv:1906.09333, 2019.\nKihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel,\nEkin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-\nsupervised learning with consistency and confidence. Advances in Neural Information\nProcessing Systems, 33:596\u2013608, 2020.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data\ndistribution. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00b4e-Buc, E. Fox, and\nR. Garnett, editors, Advances in Neural Information Processing Systems, volume 32.\nCurran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/\nfile/3001ef257407d5a371a96dcd947c7d93-Paper.pdf.\nYang Song and Diederik P Kingma. How to train your energy-based models. arXiv preprint\narXiv:2101.03288, 2021.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon,\nand Ben Poole. Score-based generative modeling through stochastic differential equations.\narXiv preprint arXiv:2011.13456, 2020.\nJost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical\ngenerative adversarial networks. arXiv preprint arXiv:1511.06390, 2015.\nKarthik Sridharan and Sham Kakade. An information theoretic framework for multi-view\nlearning. SO, 01 2008.\nNitish Srivastava and Ruslan Salakhutdinov. Multimodal learning with deep boltzmann\nmachines. Journal of Machine Learning Research, 15(84):2949\u20132980, 2014. URL http:\n//jmlr.org/papers/v15/srivastava14b.html.\n35\nThomas Steinke and Lydia Zakynthinou. Reasoning about generalization via conditional\nmutual information. In Conference on Learning Theory, pages 3437\u20133452. PMLR, 2020.\nLiang Sun, Betul Ceran, and Jieping Ye. A scalable two-stage approach for a class of dimen-\nsionality reduction techniques. In Proceedings of the 16th ACM SIGKDD international\nconference on Knowledge discovery and data mining, pages 313\u2013322, 2010.\nShiliang Sun. A survey of multi-view machine learning. Neural Computing and Applications,\n23:2031\u20132038, 2013.\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In European\nconference on computer vision, pages 776\u2013794. Springer, 2020a.\nYonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola.\nWhat makes for good views for contrastive learning? Advances in Neural Information\nProcessing Systems, 33:6827\u20136839, 2020b.\nN. Tishby, F.C. Pereira, and W. Biale. The information bottleneck method. In The 37th\nannual Allerton Conf. on Communication, Control, and Computing, pages 368\u2013377, 1999a.\nURL https://arxiv.org/abs/physics/0004057.\nNaftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method.\nIn Proceedings of the 37-th Annual Allerton Conference on Communication, Control and\nComputing, 1999b.\nYao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Self-\nsupervised learning from a multi-view perspective. arXiv preprint arXiv:2006.05576,\n2020.\nMichael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lu-\ncic. On mutual information maximization for representation learning. arXiv preprint\narXiv:1907.13625, 2019.\nRichard Turner and Maneesh Sahani. A maximum-likelihood interpretation for slow feature\nanalysis. Neural computation, 19(4):1022\u20131038, 2007.\nTalip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. Subtab: Subsetting features of\ntabular data for self-supervised representation learning. Advances in Neural Information\nProcessing Systems, 34:18853\u201318865, 2021.\nYi\u02d8git U\u02d8gur, George Arvanitakis, and Abdellatif Zaidi. Variational information bottleneck\nfor unsupervised clustering: Deep gaussian mixture embedding. Entropy, 22(2):213, 2020.\nAaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al.\nConditional image generation with pixelcnn decoders. Advances in neural information\nprocessing systems, 29, 2016.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances\nin neural information processing systems, 30, 2017.\n36\nTo Compress or Not to Compress\nMat\u00b4\u0131as Vera, Pablo Piantanida, and Leonardo Rey Vega. The role of information complexity\nand randomization in representation learning. arXiv preprint arXiv:1802.05355, 2018.\nPascal Vincent. A connection between score matching and denoising autoencoders. Neural\nComputation, 23(7):1661\u20131674, 2011. doi: 10.1162/NECO a 00142.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting\nand composing robust features with denoising autoencoders. In Proceedings of the 25th\ninternational conference on Machine learning, pages 1096\u20131103, 2008.\nSlava Voloshynovskiy, Olga Taran, Mouad Kondah, Taras Holotyak, and Danilo Rezende.\nVariational information bottleneck for semi-supervised classification. Entropy, 22(9), 2020.\nISSN 1099-4300. doi: 10.3390/e22090943. URL https://www.mdpi.com/1099-4300/22/\n9/943.\nHaoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu. Rethinking minimal sufficient\nrepresentation in contrastive learning. arXiv preprint arXiv:2203.07004, 2022.\nQi Wang, Claire Boudreau, Qixing Luo, Pang-Ning Tan, and Jiayu Zhou. Deep Multi-view\nInformation Bottleneck, pages 37\u201345. A, 2019. doi: 10.1137/1.9781611975673.5. URL\nhttps://epubs.siam.org/doi/abs/10.1137/1.9781611975673.5.\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through\nalignment and uniformity on the hypersphere. In International Conference on Machine\nLearning, pages 9929\u20139939. PMLR, 2020.\nWeiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. On deep multi-view repre-\nsentation learning. In Proceedings of the 32nd International Conference on International\nConference on Machine Learning - Volume 37, ICML\u201915, page 1083\u20131092. JMLR.org,\n2015.\nFlorian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub \u00b4Swikatkowski, Linh Tran, Stephan\nMandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin.\nHow good is the bayes posterior in deep neural networks really?\narXiv preprint\narXiv:2002.02405, 2020.\nLaurenz Wiskott and Terrence J. Sejnowski. Slow feature analysis: Unsupervised learning of\ninvariances. Neural Computation, 14(4):715\u2013770, 2002. doi: 10.1162/089976602317318938.\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data\naugmentation for consistency training. Advances in Neural Information Processing Systems,\n33:6256\u20136268, 2020.\nAolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability\nof learning algorithms. Advances in Neural Information Processing Systems, 30, 2017.\nChang Xu, Dacheng Tao, and Chao Xu. Large-margin multi-viewinformation bottleneck.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 36:1559\u20131572, 2014.\n37\nYilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory of\nusable information under computational constraints. arXiv preprint arXiv:2002.10689,\n2020.\nZhe Xue, Junping Du, Dawei Du, and Siwei Lyu.\nDeep low-rank subspace ensemble\nfor multi-view clustering. Information Sciences, 482:210\u2013227, 2019. ISSN 0020-0255.\ndoi: https://doi.org/10.1016/j.ins.2019.01.018. URL https://www.sciencedirect.com/\nscience/article/pii/S0020025519300271.\nXiaoqiang Yan, Yangdong Ye, and Zhengzheng Lou. Unsupervised video categorization\nbased on multivariate information bottleneck method. Knowledge-Based Systems, 84:\n34\u201345, 2015.\nXiaoqiang Yan, Shizhe Hu, Yiqiao Mao, Yangdong Ye, and Hui Yu. Deep multi-view\nlearning methods: A review. Neurocomputing, 448:106\u2013129, 2021. ISSN 0925-2312. doi:\nhttps://doi.org/10.1016/j.neucom.2021.03.090. URL https://www.sciencedirect.com/\nscience/article/pii/S0925231221004768.\nLaurent Younes. On the convergence of markovian stochastic algorithms with rapidly\ndecreasing ergodicity rates. In STOCHASTICS AND STOCHASTICS MODELS, pages\n177\u2013228, 1999.\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00b4ephane Deny. Barlow twins: Self-\nsupervised learning via redundancy reduction. In International Conference on Machine\nLearning, pages 12310\u201312320. PMLR, 2021.\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l: Self-supervised\nsemi-supervised learning. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 1476\u20131485, 2019.\nZhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural\nnetworks with noisy labels. Advances in neural information processing systems, 31, 2018.\nHandong Zhao, Zhengming Ding, and Yun Fu. Multi-view clustering via deep matrix\nfactorization. In Thirty-first AAAI conference on artificial intelligence, 2017a.\nJing Zhao, Xijiong Xie, Xin Xu, and Shiliang Sun. Multi-view learning overview: Recent\nprogress and new challenges. Information Fusion, 38:43\u201354, 2017b. ISSN 1566-2535. doi:\nhttps://doi.org/10.1016/j.inffus.2017.02.007. URL https://www.sciencedirect.com/\nscience/article/pii/S1566253516302032.\nShengjia Zhao, Jiaming Song, and Stefano Ermon.\nInfovae: Information maximizing\nvariational autoencoders. arXiv preprint arXiv:1706.02262, 2017c.\nRoland S Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland\nBrendel.\nContrastive learning inverts the data generating process.\nIn International\nConference on Machine Learning, pages 12979\u201312990. PMLR, 2021.\n38\n"
  },
  {
    "title": "Recommender Systems with Generative Retrieval",
    "link": "https://arxiv.org/pdf/2305.05065.pdf",
    "upvote": "3",
    "text": "Recommender Systems with Generative Retrieval\nShashank Rajput\u22c6\nUniversity of Wisconsin-Madison\nNikhil Mehta\u22c6\nGoogle DeepMind\nAnima Singh\nGoogle DeepMind\nRaghunandan Keshavan\nGoogle\nTrung Vu\nGoogle\nLukasz Heldt\nGoogle\nLichan Hong\nGoogle DeepMind\nYi Tay\nGoogle DeepMind\nVinh Q. Tran\nGoogle\nJonah Samost\nGoogle\nMaciej Kula\nGoogle DeepMind\nEd H. Chi\nGoogle DeepMind\nMaheswaran Sathiamoorthy\nGoogle DeepMind\nAbstract\nModern recommender systems perform large-scale retrieval by embedding queries\nand item candidates in the same unified space, followed by approximate nearest\nneighbor search to select top candidates given a query embedding. In this paper,\nwe propose a novel generative retrieval approach, where the retrieval model au-\ntoregressively decodes the identifiers of the target candidates. To that end, we\ncreate semantically meaningful tuple of codewords to serve as a Semantic ID for\neach item. Given Semantic IDs for items in a user session, a Transformer-based\nsequence-to-sequence model is trained to predict the Semantic ID of the next\nitem that the user will interact with. We show that recommender systems trained\nwith the proposed paradigm significantly outperform the current SOTA models on\nvarious datasets. In addition, we show that incorporating Semantic IDs into the\nsequence-to-sequence model enhances its ability to generalize, as evidenced by\nthe improved retrieval performance observed for items with no prior interaction\nhistory.\n1\nIntroduction\nOrange shoes, Brand X\nAtomic Item ID: 233\nRed shoes, Brand Y\nAtomic Item ID: 515\nOrange shoes, Brand Y\nAtomic Item ID: 64\nSemantic ID\nGenerator\n(5, 23, 55)\n(5, 25, 78)\n(5, 25, 55)\nItem \nLookup\nGenerative \nRetrieval\nUser History\nNext item\nNeurips Version\nFigure 1: Overview of the Transformer Index for GEnerative\nRecommenders (TIGER) framework. With TIGER, sequential\nrecommendation is expressed as a generative retrieval task by\nrepresenting each item as a tuple of discrete semantic tokens.\nRecommender systems help users discover\ncontent of interest and are ubiquitous in\nvarious recommendation domains such as\nvideos [4, 43, 9], apps [3], products [6, 8],\nand music [18, 19]. Modern recommender\nsystems adopt a retrieve-and-rank strategy,\nwhere a set of viable candidates are se-\nlected in the retrieval stage, which are then\nranked using a ranker model. Since the\nranker model works only on the candidates\nit receives, it is desired that the retrieval\nstage emits highly relevant candidates.\n\u22c6 Equal contribution. Work done when SR was at Google.\nCorrespondence to rajput.shashank11@gmail.com, nikhilmehta@google.com, nlogn@google.com.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.05065v3  [cs.IR]  3 Nov 2023\nThere are standard and well-established methods for building retrieval models. Matrix factoriza-\ntion [19] learns query and candidate embeddings in the same space. In order to better capture the\nnon-linearities in the data, dual-encoder architectures [39] (i.e., one tower for the query and another\nfor the candidate) employing inner-product to embed the query and candidate embeddings in the same\nspace have become popular in recent years. To use these models during inference, an index that stores\nthe embeddings for all items is created using the candidate tower. For a given query, its embedding is\nobtained using the query tower, and an Approximate Nearest Neighbors (ANN) algorithm is used\nfor retrieval. In recent years, the dual encoders architectures have also been extended for sequential\nrecommendations [11, 24, 41, 17, 32, 6, 44] that explicitly take into account the order of user-item\ninteractions.\nWe propose a new paradigm of building generative retrieval models for sequential recommendation.\nInstead of traditional query-candidate matching approaches, our method uses an end-to-end generative\nmodel that predicts the candidate IDs directly. We propose to leverage the Transformer [36] memory\n(parameters) as an end-to-end index for retrieval in recommendation systems, reminiscent of Tay et al.\n[34] that used Transformer memory for document retrieval. We refer to our method as Transformer\nIndex for GEnerative Recommenders (TIGER). A high-level overview of TIGER is shown in Figure 1.\nTIGER is uniquely characterized by a novel semantic representation of items called \"Semantic ID\"\n\u2013 a sequence of tokens derived from each item\u2019s content information. Concretely, given an item\u2019s\ntext features, we use a pre-trained text encoder (e.g., SentenceT5 [27]) to generate dense content\nembeddings. A quantization scheme is then applied on the embedding of an item to form a set\nof ordered tokens/codewords, which we refer to as the Semantic ID of the item. Ultimately, these\nSemantic IDs are used to train the Transformer model on the sequential recommendation task.\nRepresenting items as a sequence of semantic tokens has many advantages. Training the transformer\nmemory on semantically meaningful data allows knowledge sharing across similar items. This allows\nus to dispense away with the atomic and random item Ids that have been previously used [33, 42, 11, 8]\nas item features in recommendation models. With semantic token representations for items, the\nmodel is less prone to the inherent feedback loop [1, 26, 39] in recommendation systems, allowing\nthe model to generalize to newly added items to the corpus. Furthermore, using a sequence of tokens\nfor item representation helps alleviate the challenges associated with the scale of the item corpus; the\nnumber of items that can be represented using tokens is the product of the cardinality of each token\nin the sequence. Typically, the item corpus size can be in the order of billions and learning a unique\nembedding for each item can be memory-intensive. While random hashing-based techniques [16]\ncan be adopted to reduce the item representation space, in this work, we show that using semantically\nmeaningful tokens for item representation is an appealing alternative. The main contributions of this\nwork are summarized below:\n1. We propose TIGER, a novel generative retrieval-based recommendation framework that assigns\nSemantic IDs to each item, and trains a retrieval model to predict the Semantic ID of an item that\na given user may engage with.\n2. We show that TIGER outperforms existing SOTA recommender systems on multiple datasets as\nmeasured by recall and NDCG metrics.\n3. We find that this new paradigm of generative retrieval leads to two additional capabilities in\nsequential recommender systems: 1. Ability to recommend new and infrequent items, thus\nimproving cold-start recommendations, and 2. Ability to generate diverse recommendations\nusing a tunable parameter.\nPaper Overview. In Section 2, we provide a brief literature survey of recommender systems,\ngenerative retrieval, and the Semantic ID generation techniques we use in this paper. In Section 3,\nwe explain our proposed framework, and outline the various techniques we use for Semantic ID\ngeneration. We present the result of our experiments in Section 4, and conclude the paper in Section 5.\n2\nRelated Work\nSequential Recommenders.\nUsing deep sequential models in recommender systems has developed\ninto a rich literature. GRU4REC [11] was the first to use GRU based RNNs for sequential recom-\nmendations. Li et al. [24] proposed Neural Attentive Session-based Recommendation (NARM),\nwhere an attention mechanism along with a GRU layer is used to track long term intent of the user.\n2\n(a) Semantic ID generation for items using\nquantization of content embeddings.\nBidirectional Transformer Encoder\nUser_5\nt_u5\nItem 515\nSem. ID = (5, 25, 78)\nItem 233\nSem. ID = (5, 23, 55)\nt_23\nt_55\nt_5\nTokens\nTransformer Decoder\n<BOS>\nItem 64\nSem. ID = (5, 25, 55)\nItem Interaction History of User 5\nt_25\nt_78\nt_5\nEncoded \nContext\nt_25\nt_5\nt_5\n<EOS>\nt_55\nt_25\nt_55\nNext Item\nNeurips Version\n(b) Transformer based encoder-decoder setup for building the\nsequence-to-sequence model used for generative retrieval.\nFigure 2: An overview of the modeling approach used in TIGER.\nAttRec [41] proposed by Zhang et al. used self-attention mechanism to model the user\u2019s intent\nin the current session, and personalization is ensured by modeling user-item affinity with metric\nlearning. Concurrently, Kang et al. also proposed SASRec [17], which used self-attention similar to\ndecoder-only transformer models. Inspired by the success of masked language modeling in language\ntasks, BERT4Rec [32] and Transformers4Rec [6] utilize transformer models with masking strate-\ngies for sequential recommendation tasks. S3-Rec [44] goes beyond just masking by pre-training\non four self-supervised tasks to improve data representation. The models described above learn a\nhigh-dimensional embedding for each item and perform an ANN in a Maximum Inner Product Search\n(MIPS) space to predict the next item. In contrast, our proposed technique, TIGER, uses Generative\nRetrieval to directly predict the Semantic ID of the next item.\nP5 [8] fine-tunes a pre-trained large language models for multi-task recommender systems. The P5\nmodel relies on the LLM tokenizer (SentencePiece tokenizer [29]) to generate tokens from randomly-\nassigned item IDs. Whereas, we use Semantic ID representation of items thay are learned based on the\ncontent information of the items. In our experiments (Table 2), we demonstrate that recommendation\nsystems based on Semantic ID representation of items yield much better results than using random\ncodes.\nSemantic IDs.\nHou et al. proposed VQ-Rec [12] to generate \u201ccodes\u201d (analogous to Semantic IDs)\nusing content information for item representation. However, their focus is on building transferable\nrecommender systems, and do not use the codes in a generative manner for retrieval. While they\nalso use product quantization [15] to generate the codes, we use RQ-VAE to generate Semantic IDs,\nwhich leads to hierarchical representation of items (Section 4.2). In a concurrent work to us, Singh et\nal. [31] show that hierarchical Semantic IDs can be used to replace item IDs for ranking models in\nlarge scale recommender systems improves model generalization.\nGenerative Retrieval.\nWhile techniques for learning search indices have been proposed in the past\n[20], generative retrieval is a recently developed approach for document retrieval, where the task is to\nreturn a set of relevant documents from a database. Some examples include GENRE [5], DSI [34],\nNCI [37], and CGR [22]. A more detailed coverage of the related work is in Appendix A. To the best\nof our knowledge, we are the first to propose generative retrieval for recommendation systems using\nSemantic ID representation of items.\n3\nProposed Framework\nOur proposed framework consists of two stages:\n1. Semantic ID generation using content features. This involves encoding the item content features\nto embedding vectors and quantizing the embedding into a tuple of semantic codewords. The\nresulting tuple of codewords is referred to as the item\u2019s Semantic ID.\n2. Training a generative recommender system on Semantic IDs. A Transformer model is trained on\nthe sequential recommendation task using sequences of Semantic IDs.\n3\nEmbedding\n+\n+\n=\nDNN\nDecoder\n7 , \n1  , \n4)    \n(\n0\n1\n2\n3\n4\n5\n6\n7\ncodebook_1\n0\n1\n2\n3\n4\n5\n6\n7\ncodebook_2\n0\n1\n2\n3\n4\n5\n6\n7\ncodebook_3\nSemantic codes\nQuantized representation \nd=2 \n=\n-\n3\n2\nd=1 \n=\n-\n2\n1\nd=0 \n=\n-\n1\nEmbedding\nDNN\nEncoder\nResidual Quantization\nNeurips Version\nFigure 3: RQ-VAE: In the figure, the vector output by the DNN Encoder, say r0 (represented by the blue\nbar), is fed to the quantizer, which works iteratively. First, the closest vector to r0 is found in the first level\ncodebook. Let this closest vector be ec0 (represented by the red bar). Then, the residual error is computed as\nr1 := r0 \u2212 ec0. This is fed into the second level of the quantizer, and the process is repeated: The closest vector\nto r1 is found in the second level, say ec1 (represented by the green bar), and then the second level residual\nerror is computed as r2 = r1 \u2212 e\u2032\nc1. Then, the process is repeated for a third time on r2. The semantic codes\nare computed as the indices of ec0, ec1, and ec2 in their respective codebooks. In the example shown in the\nfigure, this results in the code (7, 1, 4).\n3.1\nSemantic ID Generation\nIn this section, we describe the Semantic ID generation process for the items in the recommendation\ncorpus. We assume that each item has associated content features that capture useful semantic\ninformation (e.g. titles or descriptions or images). Moreover, we assume that we have access to a\npre-trained content encoder to generate a semantic embedding x \u2208 Rd. For example, general-purpose\npre-trained text encoders such as Sentence-T5 [27] and BERT [7] can be used to convert an item\u2019s text\nfeatures to obtain a semantic embedding. The semantic embeddings are then quantized to generate a\nSemantic ID for each item. Figure 2a gives a high-level overview of the process.\nWe define a Semantic ID to be a tuple of codewords of length m. Each codeword in the tuple comes\nfrom a different codebook. The number of items that the Semantic IDs can represent uniquely is\nthus equal to the product of the codebook sizes. While different techniques to generate Semantic IDs\nresult in the IDs having different semantic properties, we want them to at least have the following\nproperty: Similar items (items with similar content features or whose semantic embeddings are close)\nshould have overlapping Semantic IDs. For example, an item with Semantic ID (10, 21, 35) should\nbe more similar to one with Semantic ID (10, 21, 40), than an item with ID (10, 23, 32). Next, we\ndiscuss the quantization schemes which we use for Semantic ID generation.\nRQ-VAE for Semantic IDs. Residual-Quantized Variational AutoEncoder (RQ-VAE) [40] is a\nmulti-level vector quantizer that applies quantization on residuals to generate a tuple of codewords\n(aka Semantic IDs). The Autoencoder is jointly trained by updating the quantization codebook and\nthe DNN encoder-decoder parameters. Fig. 3 illustrates the process of generating Semantic IDs\nthrough residual quantization.\nRQ-VAE first encodes the input x via an encoder E to learn a latent representation z := E(x). At the\nzero-th level (d = 0), the initial residual is simply defined as r0 := z. At each level d, we have a\ncodebook Cd := {ek}K\nk=1, where K is the codebook size. Then, r0 is quantized by mapping it to the\nnearest embedding from that level\u2019s codebook. The index of the closest embedding ecd at d = 0, i.e.,\nc0 = arg mini \u2225r0 \u2212 ek\u2225, represents the zero-th codeword. For the next level d = 1, the residual is\ndefined as r1 := r0 \u2212 ec0. Then, similar to the zero-th level, the code for the first level is computed\nby finding the embedding in the codebook for the first level which is nearest to r1. This process is\nrepeated recursively m times to get a tuple of m codewords that represent the Semantic ID. This\nrecursive approach approximates the input from a coarse-to-fine granularity. Note that we chose to\nuse a separate codebook of size K for each of the m levels, instead of using a single, mK-sized\ncodebook. This was done because the norm of residuals tends to decrease with increasing levels,\nhence allowing for different granularities for different levels.\n4\nOnce we have the Semantic ID (c0, . . . , cm\u22121), a quantized representation of z is computed as\nbz := Pm\u22121\nd=0 eci. Then bz is passed to the decoder, which tries to recreate the input x using\nbz. The RQ-VAE loss is defined as L(x) := Lrecon + Lrqvae, where Lrecon := \u2225x \u2212 bx\u22252, and\nLrqvae := Pm\u22121\nd=0 \u2225sg[ri] \u2212 eci\u22252 + \u03b2\u2225ri \u2212 sg[eci]\u22252. Here bx is the output of the decoder, and sg is\nthe stop-gradient operation [35]. This loss jointly trains the encoder, decoder, and the codebook.\nAs proposed in [40], to prevent RQ-VAE from a codebook collapse, where most of the input gets\nmapped to only a few codebook vectors, we use k-means clustering-based initialization for the\ncodebook. Specifically, we apply the k-means algorithm on the first training batch and use the\ncentroids as initialization.\nOther alternatives for quantization. A simple alternative to generating Semantic IDs is to use\nLocality Sensitive Hashing (LSH). We perform an ablation study in Subsection 4.2 where we find that\nRQ-VAE indeed works better than LSH. Another option is to use k-means clustering hierarchically\n[34], but it loses semantic meaning between different clusters [37]. We also tried VQ-VAE, and\nwhile it performs similarly to RQ-VAE for generating the candidates during retrieval, it loses the\nhierarchical nature of the IDs which confers many new capabilities that are discussed in Section 4.3.\nHandling Collisions. Depending on the distribution of semantic embeddings, the choice of codebook\nsize, and the length of codewords, semantic collisions can occur (i.e., multiple items can map to the\nsame Semantic ID). To remove the collisions, we append an extra token at the end of the ordered\nsemantic codes to make them unique. For example, if two items share the Semantic ID (12, 24, 52),\nwe append additional tokens to differentiate them, representing the two items as (12, 24, 52, 0)\nand (12, 24, 52, 1). To detect collisions, we maintain a lookup table that maps Semantic IDs to\ncorresponding items. Note that collision detection and fixing is done only once after the RQ-VAE\nmodel is trained. Furthermore, since Semantic IDs are integer tuples, the lookup table is efficient in\nterms of storage in comparison to high dimensional embeddings.\n3.2\nGenerative Retrieval with Semantic IDs\nWe construct item sequences for every user by sorting chronologically the items they have interacted\nwith. Then, given a sequence of the form (item1, . . . , itemn), the recommender system\u2019s task is to pre-\ndict the next item itemn+1. We propose a generative approach that directly predicts the Semantic ID of\nthe next item. Formally, let (ci,0, . . . , ci,m\u22121) be the m-length Semantic ID for itemi. Then, we con-\nvert the item sequence to the sequence (c1,0, . . . , c1,m\u22121, c2,0, . . . , c2,m\u22121, . . . , cn,0, . . . , cn,m\u22121).\nThe sequence-to-sequence model is then trained to predict the Semantic ID of itemn+1, which is\n(cn+1,0, . . . , cn+1,m\u22121). Given the generative nature of our framework, it is possible that a generated\nSemantic ID from the decoder does not match an item in the recommendation corpus. However, as\nwe show in appendix (Fig. 6) the probability of such an event occurring is low. We further discuss\nhow such events can be handled in appendix E.\n4\nExperiments\nDatasets. We evaluate the proposed framework on three public real-world benchmarks from the\nAmazon Product Reviews dataset [10], containing user reviews and item metadata from May 1996\nto July 2014. In particular, we use three categories of the Amazon Product Reviews dataset for the\nsequential recommendation task: \u201cBeauty\u201d, \u201cSports and Outdoors\u201d, and \u201cToys and Games\u201d. We\ndiscuss the dataset statistics and pre-processing in Appendix C.\nEvaluation Metrics. We use top-k Recall (Recall@K) and Normalized Discounted Cumulative Gain\n(NDCG@K) with K = 5, 10 to evaluate the recommendation performance.\nRQ-VAE Implementation Details. As discussed in section 3.1, RQ-VAE is used to quantize the\nsemantic embedding of an item. We use the pre-trained Sentence-T5 [27] model to obtain the\nsemantic embedding of each item in the dataset. In particular, we use item\u2019s content features such\nas title, price, brand, and category to construct a sentence, which is then passed to the pre-trained\nSentence-T5 model to obtain the item\u2019s semantic embedding of 768 dimension.\nThe RQ-VAE model consists of three components: a DNN encoder that encodes the input semantic\nembedding into a latent representation, residual quantizer which outputs a quantized representation,\nand a DNN decoder that decodes the quantized representation back to the semantic input embedding\n5\n(a) The ground-truth category distribution for\nall the items in the dataset colored by the\nvalue of the first codeword c1.\n(b) The category distributions for items having the Semantic ID\nas (c1, \u2217, \u2217), where c1 \u2208 {1, 2, 3, 4}. The categories are color-\ncoded based on the second semantic token c2.\nFigure 4: Qualitative study of RQ-VAE Semantic IDs (c1, c2, c3, c4) on the Amazon Beauty dataset. We show\nthat the ground-truth categories are distributed across different Semantic tokens. Moreover, the RQVAE semantic\nIDs form a hierarchy of items, where the first semantic token (c1) corresponds to coarse-level category, while\nsecond/third semantic token (c2/c3) correspond to fine-grained categories.\nspace. The encoder has three intermediate layers of size 512, 256 and 128 with ReLU activation,\nwith a final latent representation dimension of 32. To quantize this representation, three levels of\nresidual quantization is done. For each level, a codebook of cardinality 256 is maintained, where\neach vector in the codebook has a dimension of 32. When computing the total loss, we use \u03b2 = 0.25.\nThe RQ-VAE model is trained for 20k epochs to ensure high codebook usage (\u2265 80%). We use\nAdagrad optimizer with a learning rate of 0.4 and a batch size of 1024. Upon training, we use the\nlearned encoder and the quantization component to generate a 3-tuple Semantic ID for each item. To\navoid multiple items being mapped to the same Semantic ID, we add a unique 4th code for items that\nshare the same first three codewords, i.e. two items associated with a tuple (7, 1, 4) are assigned (7, 1,\n4, 0) and (7, 1, 4, 1) respectively (if there are no collisions, we still assign 0 as the fourth codeword).\nThis results in a unique Semantic ID of length 4 for each item in the recommendation corpus.\nSequence-to-Sequence Model Implementation Details. We use the open-sourced T5X frame-\nwork [28] to implement our transformer based encoder-decoder architecture. To allow the model to\nprocess the input for the sequential recommendation task, the vocabulary of the sequence-to-sequence\nmodel contains the tokens for each semantic codeword. In particular, the vocabulary contains 1024\n(256 \u00d7 4) tokens to represent items in the corpus. In addition to the semantic codewords for items,\nwe add user-specific tokens to the vocabulary. To keep the vocabulary size limited, we only add 2000\ntokens for user IDs. We use the Hashing Trick [38] to map the raw user ID to one of the 2000 user ID\ntokens. We construct the input sequence as the user Id token followed by the sequence of Semantic\nID tokens corresponding to a given user\u2019s item interaction history. We found that adding user ID to\nthe input, allows the model to personalize the items retrieved.\nWe use 4 layers each for the transformer-based encoder and decoder models with 6 self-attention\nheads of dimension 64 in each layer. We used the ReLU activation function for all the layers. The\nMLP and the input dimension was set as 1024 and 128, respectively. We used a dropout of 0.1.\nOverall, the model has around 13 million parameters. We train this model for 200k steps for the\n\u201cBeauty\u201d and \u201cSports and Outdoors\u201d dataset. Due to the smaller size of the \u201cToys and Games\u201d dataset,\nit is trained only for 100k steps. We use a batch size of 256. The learning rate is 0.01 for the first 10k\nsteps and then follows an inverse square root decay schedule.\n4.1\nPerformance on Sequential Recommendation\nIn this section, we compare our proposed framework for generative retrieval with the following\nsequential recommendation methods (which are described briefly in Appendix B): GRU4Rec [11],\nCaser [33], HGN [25], SASRec [17], BERT4Rec [32], FDSA [42], S3-Rec [44], and P5 [8]. Notably\n6\nTable 1: Performance comparison on sequential recommendation. The last row depicts % improvement with\nTIGER relative to the best baseline. Bold (underline) are used to denote the best (second-best) metric.\nMethods\nSports and Outdoors\nBeauty\nToys and Games\nRecall\n@5\nNDCG\n@5\nRecall\n@10\nNDCG\n@10\nRecall\n@5\nNDCG\n@5\nRecall\n@10\nNDCG\n@10\nRecall\n@5\nNDCG\n@5\nRecall\n@10\nNDCG\n@10\nP5 [8]\n0.0061\n0.0041\n0.0095\n0.0052\n0.0163\n0.0107\n0.0254\n0.0136\n0.0070\n0.0050\n0.0121\n0.0066\nCaser [33]\n0.0116\n0.0072\n0.0194\n0.0097\n0.0205\n0.0131\n0.0347\n0.0176\n0.0166\n0.0107\n0.0270\n0.0141\nHGN [25]\n0.0189\n0.0120\n0.0313\n0.0159\n0.0325\n0.0206\n0.0512\n0.0266\n0.0321\n0.0221\n0.0497\n0.0277\nGRU4Rec [11]\n0.0129\n0.0086\n0.0204\n0.0110\n0.0164\n0.0099\n0.0283\n0.0137\n0.0097\n0.0059\n0.0176\n0.0084\nBERT4Rec [32] 0.0115\n0.0075\n0.0191\n0.0099\n0.0203\n0.0124\n0.0347\n0.0170\n0.0116\n0.0071\n0.0203\n0.0099\nFDSA [42]\n0.0182\n0.0122\n0.0288\n0.0156\n0.0267\n0.0163\n0.0407\n0.0208\n0.0228\n0.0140\n0.0381\n0.0189\nSASRec [17]\n0.0233\n0.0154\n0.0350\n0.0192\n0.0387\n0.0249\n0.0605\n0.0318\n0.0463\n0.0306\n0.0675\n0.0374\nS3-Rec [44]\n0.0251\n0.0161\n0.0385\n0.0204\n0.0387\n0.0244\n0.0647\n0.0327\n0.0443\n0.0294\n0.0700\n0.0376\nTIGER [Ours] 0.0264\n0.0181\n0.0400\n0.0225\n0.0454\n0.0321\n0.0648\n0.0384\n0.0521\n0.0371\n0.0712\n0.0432\n+5.22% +12.55% +3.90% +10.29% +17.31% +29.04% +0.15% +17.43% +12.53% +21.24% +1.71% +14.97%\nall the baselines (except P5), learn a high-dimensional vector space using dual encoder, where the\nuser\u2019s past item interactions and the candidate items are encoded as a high-dimensional representation\nand Maximum Inner Product Search (MIPS) is used to retrieve the next candidate item that the user\nwill potentially interact with. In contrast, our novel generative retrieval framework directly predicts\nthe item\u2019s Semantic ID token-by-token using a sequence-to-sequence model.\nRecommendation Performance. We perform an extensive analysis of our proposed TIGER on\nthe sequential recommendation task and compare against the baselines above. The results for all\nbaselines, except P5, are taken from the publicly accessible results3 made available by Zhou et al.\n[44]. For P5, we use the source code made available by the authors. However, for a fair comparison,\nwe updated the data pre-processing method to be consistent with the other baselines and our method.\nWe provide further details related to our changes in Appendix D.\nThe results are shown in Table 1. We observe that TIGER consistently outperforms the existing\nbaselines4. We see significant improvement across all the three benchmarks that we considered. In\nparticular, TIGER performs considerably better on the Beauty benchmark compared to the second-best\nbaseline with up to 29% improvement in NDCG@5 compared to SASRec and 17.3% improvement\nin Recall@5 compared to S3-Rec. Similarly on the Toys and Games dataset, TIGER is 21% and 15%\nbetter in NDCG@5 and NDCG@10, respectively.\n4.2\nItem Representation\nIn this section, we analyze several important characteristics of RQ-VAE Semantic IDs. In particular,\nwe first perform a qualitative analysis to observe the hierarchical nature of Semantic IDs. Next, we\nevaluate the importance of our design choice of using RQ-VAE for quantization by contrasting the\nperformance with an alternative hashing-based quantization method. Finally, we perform an ablation\nto study the importance of using Semantic IDs by comparing TIGER with a sequence-to-sequence\nmodel that uses Random ID for item representation.\nQualitative Analysis. We analyze the RQ-VAE Semantic IDs learned for the Amazon Beauty dataset\nin Figure 4. For exposition, we set the number of RQ-VAE levels as 3 with a codebook size of 4, 16,\nand 256 respectively, i.e. for a given Semantic ID (c1, c2, c3) of an item, 0 \u2264 c1 \u2264 3, 0 \u2264 c2 \u2264 15\nand 0 \u2264 c3 \u2264 255. In Figure 4a, we annotate each item\u2019s category using c1 to visualize c1-specific\ncategories in the overall category distribution of the dataset. As shown in Figure 4a, c1 captures the\nhigh-level category of the item. For instance, c1 = 3 contains most of the products related to \u201cHair\u201d.\nSimilarly, majority of items with c1 = 1 are \u201cMakeup\u201d and \u201cSkin\u201d products for face, lips and eyes.\nWe also visualize the hierarchical nature of RQ-VAE Semantic IDs by fixing c1 and visualizing the\ncategory distribution for all possible values of c2 in Fig. 4b. We again found that the second codeword\nc2 further categorizes the high-level semantics captured with c1 into fine-grained categories. The\nhierarchical nature of Semantic IDs learned by RQ-VAE opens a wide-array of new capabilities\nwhich are discussed in Section 4.3. As opposed to existing recommendation systems that learn item\nembeddings based on random atomic IDs, TIGER uses Semantic IDs where semantically similar\n3https://github.com/aHuiWang/CIKM2020-S3Rec\n4We show in Table 9 that the standard error in the metrics for TIGER is insignificant.\n7\nTable 2: Ablation study for different ID generation techniques for generative retrieval. We show that RQ-VAE\nSemantic ID (SID) perform significantly better compared to hashing SIDs and Random IDs.\nMethods\nSports and Outdoors\nBeauty\nToys and Games\nRecall\n@5\nNDCG\n@5\nRecall\n@10\nNDCG\n@10\nRecall\n@5\nNDCG\n@5\nRecall\n@10\nNDCG\n@10\nRecall\n@5\nNDCG\n@5\nRecall\n@10\nNDCG\n@10\nRandom ID\n0.007\n0.005\n0.0116\n0.0063\n0.0296\n0.0205\n0.0434\n0.0250\n0.0362\n0.0270\n0.0448\n0.0298\nLSH SID\n0.0215\n0.0146\n0.0321\n0.0180\n0.0379\n0.0259\n0.0533\n0.0309\n0.0412\n0.0299\n0.0566\n0.0349\nRQ-VAE SID\n0.0264\n0.0181\n0.0400\n0.0225\n0.0454\n0.0321\n0.0648\n0.0384\n0.0521\n0.0371\n0.0712\n0.0432\n(a) Recall@K vs. K, (\u03f5 = 0.1).\n(b) Recall@10 vs. \u03f5.\nFigure 5: Performance in the cold-start retrieval setting.\nitems have overlapping codewords, which allows the model to effectively share knowledge from\nsemantically similar items in the dataset.\nHashing vs. RQ-VAE Semantic IDs. We study the importance of RQ-VAE in our framework by\ncomparing RQ-VAE against Locality Sensitive Hashing (LSH) [14, 13, 2] for Semantic ID generation.\nLSH is a popular hashing technique that can be easily adapted to work for our setting. To generate\nLSH Semantic IDs, we use h random hyperplanes w1, . . . , wh to perform a random projection of the\nembedding vector x and compute the following binary vector: (1w\u22a4\n1 x>0, . . . , 1w\u22a4\nh x>0). This vector\nis converted into an integer code as c0 = Ph\ni=1 2i\u221211w\u22a4\ni x>0. This process is repeated m times using\nan independent set of random hyperplanes, resulting in m codewords (c0, c1, . . . , cm\u22121), which we\nrefer to as the LSH Semantic ID.\nIn Table 2, we compare the performance of LSH Semantic ID with our proposed RQ-VAE Semantic\nID. In this experiment, for LSH Semantic IDs, we used h = 8 random hyperplanes and set m = 4 to\nensure comparable cardinality with the RQ-VAE. The parameters for the hyperplanes are randomly\nsampled from a standard normal distribution, which ensures that the hyperplanes are spherically\nsymmetric. Our results show that RQ-VAE consistently outperforms LSH. This illustrates that\nlearning Semantic IDs via a non-linear, Deep Neural Network (DNN) architecture yields better\nquantization than using random projections, given the same content-based semantic embedding.\nRandom ID vs. Semantic ID. We also compare the importance of Semantic IDs in our generative\nretrieval recommender system. In particular, we compare randomly generated IDs with the Semantic\nIDs. To generate the Random ID baseline, we assign m random codewords to each item. A Random\nID of length m for an item is simply (c1, . . . , cm), where ci is sampled uniformly at random from\n{1, 2, . . . , K}. We set m = 4, and K = 255 for the Random ID baseline to make the cardinality\nsimilar to RQ-VAE Semantic IDs. A comparison of Random ID against RQ-VAE and LSH Semantic\nIDs is shown in Table 2. We see that Semantic IDs consistently outperform Random ID baseline,\nhighlighting the importance of leveraging content-based semantic information.\n4.3\nNew Capabilities\nWe describe two new capabilities that directly follow from our proposed generative retrieval frame-\nwork, namely cold-start recommendations and recommendation diversity. We refer to these capabil-\nities as \u201cnew\u201d since existing sequential recommendation models (See the baselines in section 4.1)\ncannot be directly used to satisfy these real-world use cases. These capabilities result from a synergy\nbetween RQ-VAE based Semantic IDs and the generative retrieval approach of our framework. We\ndiscuss how TIGER is used in these settings in the following sections.\n8\nCold-Start Recommendation. In this section, we study the cold-start recommendation capability of\nour proposed framework. Due to the fast-changing nature of the real-world recommendation corpus,\nnew items are constantly introduced. Since newly added items lack user impressions in the training\ncorpus, existing recommendation models that use a random atomic ID for item representation fail\nto retrieve new items as potential candidates. In contrast, the TIGER framework can easily perform\ncold-start recommendations since it leverages item semantics when predicting the next item.\nFor this analysis, we consider the Beauty dataset from Amazon Reviews. To simulate newly added\nitems, we remove 5% of test items from the training data split. We refer to these removed items as\nunseen items. Removing the items from the training split ensures there is no data leakage with respect\nto the unseen items. As before, we use Semantic ID of length 4 to represent the items, where the first\n3 tokens are generated using RQ-VAE and the 4th token is used to ensure a unique ID exists for all\nthe seen items. We train the RQ-VAE quantizer and the sequence-to-sequence model on the training\nsplit. Once trained, we use the RQ-VAE model to generate the Semantic IDs for all the items in the\ndataset, including any unseen items in the item corpus.\nGiven a Semantic ID (c1, c2, c3, c4) predicted by the model, we retrieve the seen item having the same\ncorresponding ID. Note that by definition, each Semantic ID predicted by the model can match at\nmost one item in the training dataset. Additionally, unseen items having the same first three semantic\ntokens, i.e. (c1, c2, c3) are included to the list of retrieved candidates. Finally, when retrieving a set\nof top-K candidates, we introduce a hyperparameter \u03f5 which specifies the maximum proportion of\nunseen items chosen by our framework.\nWe compare the performance of TIGER with the k-Nearest Neighbors (KNN) approach on the\ncold-start recommendation setting in Fig. 5. For KNN, we use the semantic representation space\nto perform the nearest-neighbor search. We refer to the KNN-based baseline as Semantic_KNN.\nFig. 5a shows that our framework with \u03f5 = 0.1 consistently outperforms Semantic_KNN for\nall Recall@K metrics. In Fig. 5b, we provide a comparison between our method and Seman-\ntic_KNN for various values of \u03f5. For all settings of \u03f5 \u2265 0.1, our method outperforms the baseline.\nTable 3: The entropy of the category distribution predicted\nby the model for the Beauty dataset. A higher entropy\ncorresponds more diverse items predicted by the model.\nTemperature\nEntropy@10\nEntropy@20\nEntropy@50\nT = 1.0\n0.76\n1.14\n1.70\nT = 1.5\n1.14\n1.52\n2.06\nT = 2.0\n1.38\n1.76\n2.28\nRecommendation diversity. While Recall\nand NDCG are the primary metrics used to\nevaluate a recommendation system, diversity\nof predictions is another critical objective of\ninterest. A recommender system with poor\ndiversity can be detrimental to the long-term\nengagement of users. Here, we discuss how\nour generative retrieval framework can be used\nto predict diverse items. We show that temperature-based sampling during the decoding process can\nbe effectively used to control the diversity of model predictions. While temperature-based sampling\ncan be applied to any existing recommendation model, TIGER allows sampling across various levels\nof hierarchy owing to the properties of RQ-VAE Semantic IDs. For instance, sampling the first token\nof the Semantic ID allows retrieving items from coarse-level categories, while sampling a token from\nsecond/third token allows sampling items within a category.\nTable 4: Recommendation diversity with temperature-based decoding.\nTarget Category\nMost-common Categories for top-10 predicted items\nT = 1.0\nT = 2.0\nHair Styling Products\nHair Styling Products\nHair Styling Products, Hair Styling Tools, Skin Face\nTools Nail\nTools Nail\nTools Nail, Makeup Nails\nMakeup Nails\nMakeup Nails\nMakeup Nails, Skin Hands & Nails, Tools Nail\nSkin Eyes\nSkin Eyes\nHair Relaxers, Skin Face, Hair Styling Products, Skin Eyes\nMakeup Face\nTools Makeup Brushes,Makeup Face\nTools Makeup Brushes, Makeup Face,Skin Face, Makeup Sets, Hair Styling Tools\nHair Loss Products\nHair Loss Products,Skin Face, Skin Body\nSkin Face, Hair Loss Products, Hair Shampoos,Hair & Scalp Treatments, Hair Conditioners\nWe quantitatively measure the diversity of predictions using Entropy@K metric, where the entropy\nis calculated for the distribution of the ground-truth categories of the top-K items predicted by\nthe model. We report the Entropy@K for various temperature values in Table 3. We observe that\ntemperature-sampling in the decoding stage can be effectively used to increase the diversity in the\nground-truth categories of the items. We also perform a qualitative analysis in Table 4.\n9\nTable 5: Recall and NDCG metrics for different number layers.\nNumber of Layers\nRecall@5\nNDCG@5\nRecall@10\nNDCG@10\n3\n0.04499\n0.03062\n0.06699\n0.03768\n4\n0.0454\n0.0321\n0.0648\n0.0384\n5\n0.04633\n0.03206\n0.06596\n0.03834\n4.4\nAblation Study\nWe measure the effect of varying the number of layers in the sequence-to-sequence model in Table 5.\nWe see that the metrics improve slightly as we make the network bigger. We also measure the effect\nof providing user information, the results for which are provided in Table 8 in the Appendix.\n4.5\nInvalid IDs\nSince the model decodes the codewords of the target Semantic ID autoregressively, it is possible\nthat the model may predict invalid IDs (i.e., IDs that do not map to any item in the recommendation\ndataset). In our experiments, we used semantic IDs of length 4 with each codeword having a\ncardinality of 256 (i.e., codebook size = 256 for each level). The number of possible IDs spanned\nby this combination = 2564, which is approximately 4 trillion. On the other hand, the number of\nitems in the datasets we consider is 10K-20K (See Table 6). Even though the number of valid IDs is\nonly a fraction of all complete ID space, we observe that the model almost always predicts valid IDs.\nWe visualize the fraction of invalid IDs produced by TIGER as a function of the number of retrieved\nitems K in Figure 6. For top-10 predictions, the fraction of invalid IDs varies from \u223c 0.1% \u2212 1.6%\nfor the three datasets. To counter the effect of invalid IDs and to always get top-10 valid IDs, we can\nincrease the beam size and filter the invalid IDs.\nIt is important to note that, despite generating invalid IDs, TIGER achieves state-of-the-art perfor-\nmance when compared to other popular methods used for sequential recommendations. One extension\nto handle invalid tokens could be to do prefix matching when invalid tokens are generated by the\nmodel. Prefix matching of Semantic IDs would allow retrieving items that have similar semantic\nmeaning as the tokens generated by the model. Given the hierarchical nature of our RQ-VAE tokens,\nprefix matching can be thought of as model predicting item category as opposed to the item index.\nNote that such an extension could improve the recall/NDCG metrics even further. We leave such an\nextension as a future work.\n5\nConclusion\nThis paper proposes a novel paradigm, called TIGER, to retrieve candidates in recommender systems\nusing a generative model. Underpinning this method is a novel semantic ID representation for items,\nwhich uses a hierarchical quantizer (RQ-VAE) on content embeddings to generate tokens that form\nthe semantic IDs. Our framework provides results in a model that can be used to train and serve\nwithout creating an index \u2014 the transformer memory acts as a semantic index for items. We note\nthat the cardinality of our embedding table does not grow linearly with the cardinality of item space,\nwhich works in our favor compared to systems that need to create large embedding tables during\ntraining or generate an index for every single item. Through experiments on three datasets, we show\nthat our model can achieve SOTA retrieval performance, while generalizing to new and unseen items.\nReferences\n[1] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. The unfair-\nness of popularity bias in recommendation. arXiv preprint arXiv:1907.13286, 2019.\n[2] Moses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings\nof the thiry-fourth annual ACM symposium on Theory of computing, pages 380\u2013388, 2002.\n[3] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye,\nGlen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for\n10\nrecommender systems. In Proceedings of the 1st workshop on deep learning for recommender\nsystems, pages 7\u201310, 2016.\n[4] Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommenda-\ntions. In Proceedings of the 10th ACM conference on recommender systems, pages 191\u2013198,\n2016.\n[5] Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity\nretrieval. arXiv preprint arXiv:2010.00904, 2020.\n[6] Gabriel de Souza Pereira Moreira, Sara Rabhi, Jeong Min Lee, Ronay Ak, and Even Oldridge.\nTransformers4rec: Bridging the gap between nlp and sequential/session-based recommendation.\nIn Fifteenth ACM Conference on Recommender Systems, pages 143\u2013153, 2021.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[8] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation\nas language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5).\narXiv preprint arXiv:2203.13366, 2022.\n[9] Carlos A Gomez-Uribe and Neil Hunt. The netflix recommender system: Algorithms, business\nvalue, and innovation. ACM Transactions on Management Information Systems (TMIS), 6(4):1\u2013\n19, 2015.\n[10] Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion\ntrends with one-class collaborative filtering. In proceedings of the 25th international conference\non world wide web, pages 507\u2013517, 2016.\n[11] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based\nrecommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939, 2015.\n[12] Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. Learning vector-quantized\nitem representation for transferable sequential recommenders. arXiv preprint arXiv:2210.12316,\n2022.\n[13] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the\ncurse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of\ncomputing, pages 604\u2013613, 1998.\n[14] Piotr Indyk, Rajeev Motwani, Prabhakar Raghavan, and Santosh Vempala. Locality-preserving\nhashing in multidimensional spaces. In Proceedings of the twenty-ninth annual ACM symposium\non Theory of computing, pages 618\u2013625, 1997.\n[15] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor\nsearch. IEEE transactions on pattern analysis and machine intelligence, 33(1):117\u2013128, 2010.\n[16] Wang-Cheng Kang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Ting Chen, Lichan\nHong, and Ed H Chi. Learning to embed categorical features without embedding tables for\nrecommendation. arXiv preprint arXiv:2010.10784, 2020.\n[17] Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In 2018\nIEEE international conference on data mining (ICDM), pages 197\u2013206. IEEE, 2018.\n[18] Dongmoon Kim, Kun-su Kim, Kyo-Hyun Park, Jee-Hyong Lee, and Keon Myung Lee. A music\nrecommendation system with a dynamic k-means clustering algorithm. In Sixth international\nconference on machine learning and applications (ICMLA 2007), pages 399\u2013403. IEEE, 2007.\n[19] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recom-\nmender systems. Computer, 42(8):30\u201337, 2009.\n[20] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 international conference on management of data,\npages 489\u2013504, 2018.\n11\n[21] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive\nimage generation using residual quantization. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 11523\u201311532, 2022.\n[22] Hyunji Lee, Jaeyoung Kim, Hoyeon Chang, Hanseok Oh, Sohee Yang, Vlad Karpukhin, Yi Lu,\nand Minjoon Seo. Contextualized generative retrieval. arXiv preprint arXiv:2210.02068, 2022.\n[23] Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon Seo. Generative retrieval for long sequences.\narXiv preprint arXiv:2204.13596, 2022.\n[24] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. Neural attentive\nsession-based recommendation. In Proceedings of the 2017 ACM on Conference on Information\nand Knowledge Management, pages 1419\u20131428, 2017.\n[25] Chen Ma, Peng Kang, and Xue Liu. Hierarchical gating networks for sequential recommen-\ndation. In Proceedings of the 25th ACM SIGKDD international conference on knowledge\ndiscovery & data mining, pages 825\u2013833, 2019.\n[26] Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad Mobasher, and Robin\nBurke. Feedback loop and bias amplification in recommender systems, 2020.\n[27] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and\nYinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models.\nIn Findings of the Association for Computational Linguistics: ACL 2022, pages 1864\u20131874,\nDublin, Ireland, May 2022. Association for Computational Linguistics.\n[28] Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel\nAndor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor\nLewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini\nSoares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis\nBulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan\nLee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten\nBosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan\nSaeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling\nup models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022.\n[29] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[30] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare\nwords with subword units. In Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany,\nAugust 2016. Association for Computational Linguistics.\n[31] Anima Singh, Trung Vu, Raghunandan Keshavan, Nikhil Mehta, Xinyang Yi, Lichan Hong,\nLukasz Heldt, Li Wei, Ed Chi, and Maheswaran Sathiamoorthy. Better generalization with\nsemantic ids: A case study in ranking for recommendations. arXiv preprint arXiv:2306.08121,\n2023.\n[32] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec:\nSequential recommendation with bidirectional encoder representations from transformer. In Pro-\nceedings of the 28th ACM international conference on information and knowledge management,\npages 1441\u20131450, 2019.\n[33] Jiaxi Tang and Ke Wang. Personalized top-n sequential recommendation via convolutional\nsequence embedding. In Proceedings of the eleventh ACM international conference on web\nsearch and data mining, pages 565\u2013573, 2018.\n[34] Yi Tay, Vinh Q Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai\nHui, Zhe Zhao, Jai Gupta, et al. Transformer memory as a differentiable search index. arXiv\npreprint arXiv:2202.06991, 2022.\n[35] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\n12\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[37] Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen,\nYuqing Xia, Chengmin Chi, Guoshuai Zhao, et al. A neural corpus indexer for document\nretrieval. arXiv preprint arXiv:2206.02743, 2022.\n[38] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. Feature\nhashing for large scale multitask learning. In Proceedings of the 26th annual international\nconference on machine learning, pages 1113\u20131120, 2009.\n[39] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee Kumthekar,\nZhe Zhao, Li Wei, and Ed Chi. Sampling-bias-corrected neural modeling for large corpus item\nrecommendations. In Proceedings of the 13th ACM Conference on Recommender Systems,\npages 269\u2013277, 2019.\n[40] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi.\nSoundstream: An end-to-end neural audio codec. CoRR, abs/2107.03312, 2021.\n[41] Shuai Zhang, Yi Tay, Lina Yao, and Aixin Sun. Next item recommendation with self-attention.\narXiv preprint arXiv:1808.06414, 2018.\n[42] Tingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Deqing Wang, Guan-\nfeng Liu, and Xiaofang Zhou. Feature-level deeper self-attention network for sequential\nrecommendation. In IJCAI, pages 4320\u20134326, 2019.\n[43] Zhe Zhao, Lichan Hong, Li Wei, Jilin Chen, Aniruddh Nath, Shawn Andrews, Aditee Kumthekar,\nMaheswaran Sathiamoorthy, Xinyang Yi, and Ed Chi. Recommending what video to watch\nnext: a multitask ranking system. In Proceedings of the 13th ACM Conference on Recommender\nSystems, pages 43\u201351, 2019.\n[44] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan\nWang, and Ji-Rong Wen. S3-rec: Self-supervised learning for sequential recommendation with\nmutual information maximization. In Proceedings of the 29th ACM International Conference\non Information & Knowledge Management, pages 1893\u20131902, 2020.\n13\nA\nRelated Work (cont.)\nGenerative Retrieval\nDocument retrieval traditionally involved training a 2-tower model which\nmapped both queries and documents to the same high-dimensional vector space, followed by perform-\ning an ANN or MIPS for the query over all the documents to return the closest ones. This technique\npresents some disadvantages like having a large embedding table [22, 23]. Generative retrieval is\na recently proposed technique that aims to fix some of the issues of the traditional approach by\nproducing token by token either the title, name, or the document id string of the document. Cao et\nal. [5] proposed GENRE for entity retrieval, which used a transformer-based architecture to return,\ntoken-by-token, the name of the entity referenced to in a given query. Tay et al. [34] proposed DSI\nfor document retrieval, which was the first system to assign structured semantic DocIDs to each\ndocument. Then given a query, the model autoregressively returned the DocID of the document\ntoken-by-token. The DSI work marks a paradigm shift in IR to generative retrieval approaches and is\nthe first successful application of an end-to-end Transformer for retrieval applications. Subsequently,\nLee et al. [23] show that generative document retrieval is useful even in the multi-hop setting, where\na complex query cannot be answered directly by a single document, and hence their model generates\nintermediate queries, in a chain-of-thought manner, to ultimately generate the output for the complex\nquery. Wang et al. [37] supplement the hierarchical k-means clustering based semantic DocIDs\nof Tay et al. [34] by proposing a new decoder architecture that specifically takes into account the\nprefixes in semantic DocIDs. In CGR [22], the authors propose a way to take advantage of both\nthe bi-encoder technique and the generative retrieval technique, by allowing the decoder, of their\nencoder-decoder-based model, to learn separate contextualized embeddings which store information\nabout documents intrinsically. To the best of our knowledge, we are the first to use generative\nSemantic IDs created using an auto-encoder (RQ-VAE [40, 21]) for retrieval models.\nVector Quantization.\nWe refer to Vector Quantization as the process of converting a high-\ndimensional vector into a low-dimensional tuple of codewords. One of the most straightforward\ntechniques uses hierarchical clustering, such as the one used in [34], where clusters created in a\nparticular iteration are further partitioned into sub-clusters in the next iteration. An alternative popular\napproach is Vector-Quantized Variational AutoEncoder (VQ-VAE), which was introduced in [35] as\na way to encode natural images into a sequence of codes. The technique works by first passing the\ninput vector (or image) through an encoder that reduces the dimensionality. The smaller dimensional\nvector is partitioned and each partition is quantized separately, thus resulting in a sequence of codes:\none code per partition. These codes are then used by a decoder to recreate the original vector (or\nimage).\nRQ-VAE [40, 21] applies residual quantization to the output of the encoder of VQ-VAE to achieve\na lower reconstruction error. We discuss this technique in more detail in Subsection 3.1. Locality\nSensitive Hashing (LSH) [14, 13] is a popular technique used for clustering and approximate nearest\nneighbor search. The particular version that we use in this paper for clustering is SimHash [2], which\nuses random hyperplanes to create binary vectors which serve as hashes of the items. Because it has\nlow computational complexity and is scalable [13], we use this as a baseline technique for vector\nquantization.\nB\nBaselines\nBelow we briefly describe each of the baselines with which we compare TIGER\n\u2022 GRU4Rec [11] is the first RNN-based approach that uses a customized GRU for the sequential\nrecommendation task.\n\u2022 Caser [33] uses a CNN architecture for capturing high-order Markov Chains by applying horizon-\ntal and vertical convolutional operations for sequential recommendation.\n\u2022 HGN [25]: Hierarchical Gating Network captures the long-term and short-term user interests via\na new gating architecture.\n\u2022 SASRec [17]: Self-Attentive Sequential Recommendation uses a causal mask Transformer to\nmodel a user\u2019s sequential interactions.\n\u2022 BERT4Rec [32]: BERT4Rec addresses the limitations of uni-directional architectures by using a\nbi-directional self-attention Transformer for the recommendation task.\n14\n\u2022 FDSA [42]: Feature-level Deeper Self-Attention Network incorporates item features in addition\nto the item embeddings as part of the input sequence in the Transformers.\n\u2022 S3-Rec [44]: Self-Supervised Learning for Sequential Recommendation proposes pre-training a\nbi-directional Transformer on self-supervision tasks to improve the sequential recommendation.\n\u2022 P5 [8]: P5 is a recent method that uses a pretrained Large Language Model (LLM) to unify\ndifferent recommendation tasks in a single model.\nC\nDataset Statistics\nTable 6: Dataset statistics for the three real-world benchmarks.\nDataset\n# Users\n# Items\nSequence Length\nMean\nMedian\nBeauty\n22,363\n12,101\n8.87\n6\nSports and Outdoors\n35,598\n18,357\n8.32\n6\nToys and Games\n19,412\n11,924\n8.63\n6\nWe use three public benchmarks from the Amazon Product Reviews dataset [10], containing user\nreviews and item metadata from May 1996 to July 2014. We use three categories of the Amazon\nProduct Reviews dataset for the sequential recommendation task: \u201cBeauty\u201d, \u201cSports and Outdoors\u201d,\nand \u201cToys and Games\u201d. Table 6 summarizes the statistics of the datasets. We use users\u2019 review history\nto create item sequences sorted by timestamp and filter out users with less than 5 reviews. Following\nthe standard evaluation protocol [17, 8], we use the leave-one-out strategy for evaluation. For each\nitem sequence, the last item is used for testing, the item before the last is used for validation, and the\nrest is used for training. During training, we limit the number of items in a user\u2019s history to 20.\nD\nModifications to the P5 data preprocessing\nTable 7: Results for P5[8] with the standard pre-processing.\nMethods\nSports and Outdoors\nBeauty\nToys and Games\nRecall@5\nNDCG@5\nRecall@10\nNDCG@10\nRecall@5\nNDCG@5\nRecall@10\nNDCG@10\nRecall@5\nNDCG@5\nRecall@10\nNDCG@10\nP5\n0.0061\n0.0041\n0.0095\n0.0052\n0.0163\n0.0107\n0.0254\n0.0136\n0.0070\n0.0050\n0.0121\n0.0066\nP5-ours\n0.0107\n0.0076\n0.01458\n0.0088\n0.035\n0.025\n0.048\n0.0298\n0.018\n0.013\n0.0235\n0.015\nThe P5 source code 5 pre-processes the Amazon dataset to first create sessions for each user, containing\nchronologically ordered list of items that the user reviewed. After creating these sessions, the original\nitem IDs from the dataset are remapped to integers 1, 2, 3, . . .\n6. Hence, the first item in the first\nsession gets an id of \u20181\u2019, the second item, if not seen before, gets an id of \u20182\u2019, and so on. Notably,\nthis pre-processing scheme is applied before creating training and testing splits. This results in the\ncreation of a sequential dataset where many sequences are of the form a, a + 1, a + 2, . . . . Given\nthat P5 uses Sentence Piece tokenizer [30] (See Section 4.1 in [8]), the test and train items in a user\nsession may share the sub-word and can lead to information leakage during inference.\nTo resolve the leakage issue, instead of assigning sequentially increasing integer ids to items, we\nassigned random integer IDs, and then created splits for training and evaluation. The rest of the code\nfor P5 was kept identical to the source code provided in the paper. The results for this dataset are\nreported in Table 7 as the row \u2018P5\u2019. We also implemented a version of P5 ourselves from scratch,\nand train the model on only sequential recommendation task. The results for our implementation are\ndepicted as \u2018P5-ours\u2019. We were also able to verify in our P5 implementation that using consecutive\ninteger sequences for the item IDs helped us get equivalent or better metrics than those reported in\nP5.\n15\nTable 8: The effect of providing user information to the recommender system\nRecall@5\nNDCG@5\nRecall@10\nNDCG@10\nNo user information\n0.04458\n0.0302\n0.06479\n0.0367\nWith user id (reported in the paper)\n0.0454\n0.0321\n0.0648\n0.0384\nTable 9: The mean and stand error of the metrics for different dataset (computed using 3 runs with\ndifferent random seeds)\nDatasets\nRecall@5\nNDCG@5\nRecall@10\nNDCG@10\nBeauty\n0.0441 \u00b1 0.00069\n0.0309 \u00b1 0.00062\n0.0642 \u00b1 0.00092\n0.0374 \u00b1 0.00061\nSports and Outdoors\n0.0278 \u00b1 0.00069\n0.0189 \u00b1 0.00043\n0.0419 \u00b1 0.0010\n0.0234 \u00b1 0.00048\nToys and Games\n0.0518 \u00b1 0.00064\n0.0375 \u00b1 0.00039\n0.0698 \u00b1 0.0013\n0.0433 \u00b1 0.00047\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19 20\nK\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nInvalid IDs (%)\n(a) Sports and Outdoors\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19 20\nK\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nInvalid IDs (%)\n(b) Beauty\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19 20\nK\n0\n1\n2\n3\n4\n5\n6\nInvalid IDs (%)\n(c) Toys and Games\nFigure 6: Percentage of invalid IDs when generating Semantic IDs using Beam search for various\nvalues of K. As shown, \u223c 0.3% \u2212 6% of the IDs are invalid when retrieving the top-20 items.\nE\nDiscussion\nEffects of Semantic ID length and codebook size. We tried varying the Semantic ID length and\ncodebook size, such as having an ID consisting of 6 codewords each from a codebook of size 64.\nWe noticed that the recommendation metrics for TIGER were robust to these changes. However,\nnote that the input sequence length increases with longer IDs (i.e., more codewords per item ID),\nwhich makes the computation more expensive for our transformer-based sequence-to-sequence model.\nScalability. To test the scalability of Semantic IDs, we ran the following experiment: We combined\nall the three datasets and generated Semantic IDs for the entire set of items from the three datasets.\nThen, we used these Semantic IDs for the recommendation task on the Beauty dataset. We compare\nthe results from this experiment with the original experiment where the Semantic IDs are generated\nonly from the Beauty dataset. The results are provided in Table 10. We see that there is only a small\ndecrease in performance here.\nInference cost. Despite the remarkable success of our model on the sequential recommendation\ntask, we note that our model can be more computationally expensive than ANN-based models during\ninference due to the use of beam search for autoregressive decoding. We emphasize that optimizing\nthe computational efficiency of TIGER was not the main objective of this work. Instead, our work\nopens up a new area of research: Recommender Systems based on Generative Retrieval. As part of\nfuture work, we will consider ways to make the model smaller or explore other ways of improving\nthe inference efficiency.\nMemory cost of lookup tables. We maintain two lookup hash tables for TIGER: an Item ID to\nSemantic ID table and a Semantic ID to Item ID table. Note that both of these tables are generated\nonly once and then frozen: they are generated after the training of the RQ-VAE-based Semantic\nID generation model, and after that, they are frozen for the training of the sequence-to-sequence\n5https://github.com/jeykigung/P5\n6https://github.com/jeykigung/P5/blob/0aaa3fd8366bb6e708c8b70708291f2b0ae90c82/preprocess/data_preprocess_amazon.ipynb\n16\nTable 10: Testing scalability by generating the Semantic IDs on the combined dataset vs generating\nthe Semantic IDs on only the Beauty dataset.\nRecall@5\nNDCG@5\nRecall@10\nNDCG@10\nSemantic ID [Combined datasets]\n0.04355\n0.3047\n0.06314\n0.03676\nSemantic ID [Amazon Beauty]\n0.0454\n0.0321\n0.0648\n0.0384\ntransformer model. Each Semantic ID consists of a tuple of 4 integers, each of which are stored in 8\nbits, hence totalling to 32 bits per item. Each item is represented by an Item ID, stored as a 32 bit\ninteger. Thus, the size of each lookup table will be of the order of 64N bits, where N is the number\nof items in the dataset.\nMemory cost of embedding tables. TIGER uses much smaller embedding tables compared to\ntraditional recommender systems. This is because where traditional recommender systems store an\nembedding for each item, TIGER only stores an embedding for each semantic codeword. In our\nexperiments, we used 4 codewords each of cardinality 256 for Semantic ID representation, resulting\nin 1024 (256\u00d74) embeddings. For traditional recommender systems, the number of embeddings is\nN, where N is the number of items in the dataset. In our experiments, N ranged from 10K to 20K\ndepending on the dataset. Hence, the memory cost of TIGER\u2019s embedding table is 1024d, where d is\nthe dimension of the embedding, whereas the memory cost for embedding lookup tables in traditional\nrecommendation systems is Nd.\n17\n"
  },
  {
    "title": "InternChat: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language",
    "link": "https://arxiv.org/pdf/2305.05662.pdf",
    "upvote": "3",
    "text": "InternGPT: Solving Vision-Centric Tasks\nby Interacting with ChatGPT Beyond Language\nZhaoyang Liu\u22171, Yinan He\u22171, Wenhai Wang\u2217\u20201, Weiyun Wang\u22171, Yi Wang\u22171, Shoufa Chen\u22172,1\nQinglong Zhang\u22171, Zeqiang Lai\u22173,1, Yang Yang1, Qingyun Li1, Jiashuo Yu1, Kunchang Li4,1, Zhe Chen5,1,\nXue Yang1, Xizhou Zhu6,1, Yali Wang4,1, Limin Wang5,1, Ping Luo2,1, Jifeng Dai7,1, Yu Qiao1\n1OpenGVLab, Shanghai AI Laboratory\n2The University of Hong Kong\n3Beijing Institute of Technology\n4Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences\n5Nanjing University\n6SenseTime Research\n7Tsinghua University\nhttps://github.com/OpenGVLab/InternGPT\nWe\u2019re going to use the best pointing device in the world. We\u2019re going to use a pointing device that\nwe\u2019re all born with \u2014 born with ten of them. We\u2019re going to use our fingers. We\u2019re going to touch\nthis with our fingers.\n\u2014 Steve Jobs\nAbstract\nWe present an interactive visual framework named InternGPT, or iGPT for short.\nThe framework integrates chatbots that have planning and reasoning capabilities,\nsuch as ChatGPT, with non-verbal instructions like pointing movements that enable\nusers to directly manipulate images or videos on the screen. Pointing (including\ngestures, cursors, etc.) movements can provide more flexibility and precision in\nperforming vision-centric tasks that require fine-grained control, editing, and gener-\nation of visual content. The name InternGPT stands for interaction, nonverbal, and\nchatbots. Different from existing interactive systems that rely on pure language,\nby incorporating pointing instructions, the proposed iGPT significantly improves\nthe efficiency of communication between users and chatbots, as well as the accu-\nracy of chatbots in vision-centric tasks, especially in complicated visual scenarios\nwhere the number of objects is greater than 2. Additionally, in iGPT, an auxiliary\ncontrol mechanism is used to improve the control capability of LLM, and a large\nvision-language model termed Husky is fine-tuned for high-quality multi-modal\ndialogue (impressing ChatGPT-3.5-turbo with 93.89% GPT-4 Quality). We hope\nthis work can spark new ideas and directions for future interactive visual systems.\n1\nIntroduction\nVision-centric tasks aim to enable computers to understand what they see from the world and\nreact accordingly. In the past, these tasks were solved one by one using specific vision foundation\nmodels (VFMs) that were pre-defined and trained for specific visual concepts (e.g., classes, masks,\netc.). However, this approach is limited by the availability and quality of labeled data and the diversity\nof visual scenarios. Recently, the blossom of large language models (LLMs) such as ChatGPT [47],\nGPT-4 [46], and LLaMA [61] has opened up new possibilities for solving vision-centric tasks.\nThis paradigm leverages LLMs to learn unified real-world concepts and make decisions or plans\nwith vision foundation models (VFM). This approach is user-friendly, requiring almost no domain\nknowledge for daily or professional tasks via dialogues. It has led to the development of various\nkiller applications, e.g., Visual ChatGPT [73], MM-REACT [78], HuggingGPT [57], etc.\n* Equal contribution.\n\u2020 Project lead\narXiv:2305.05662v4  [cs.CV]  2 Jun 2023\nW\nRemove the object.\nI have used the tool \"Remove the masked object\" with the \ninput \"tmp/img001.png\", and the output is \n\"tmp/img002.png\"\nW\nclick\nI have used the tool \"Segment the Clicked Region\" to \nsegment the clicked region in the image, and the output \nfile name is \"tmp/img001.png\".\nsuccess\n(b) pointing-language-driven InternGPT (ours)\nReceived.\nW\nW\nRemove the boy in green pants.\nI have used the tool \"Remove Something From The Photo\" \nto remove the boy in green pants from \"tmp/img001.png\" \nand the result is saved as \"tmp/img002.png\". \nfailed\n(a) previous purely language-driven interactive systems\nPurely language-driven interactive systems,\nlike Visual ChatGPT, HuggingGPT, may not\nbe sufficient for handling complicated\nvisual scenarios.\nNow you have a pointing device.\nFigure 1: Advantage of our pointing-language-driven interactive system.\nAlthough words are convenient for defining tasks and describing objects, actions, scenes, events,\netc, and language-based instructions powered by LLMs allow us to enjoy the capabilities of AI\nsystems, current interactive systems [73, 78, 57] have limitations in connecting vision and language\nmodels. They rely mainly on text instructions to interact with visual instances. This becomes highly\ninefficient when dealing with complicated visual scenarios involving multiple instances since we\nneed to describe the desired instance in length by giving details to discriminate it from others.\nPointing movements such as gestures and cursors are the most common form of non-verbal in-\nstructions for communication, showing accuracy in pointing and brevity in operations. Modern\noperating systems rely heavily on pointing devices such as touchscreens and mice for navigation\nand interaction. To overcome the aforementioned limitations in current interactive systems, we need\nto explore new methods to integrate visual perception and language understanding, including both\nverbal and non-verbal instructions. This will enable efficient and accurate interactions in complicated\nvisual environments.\nInspired by this, we present an advanced interactive visual framework, termed InternGPT (iGPT),\nwhich combines the advantages of pointing and language instructions to perform complex vision-\ncentric tasks. It consists of three main components: a perception unit that handles pointing instructions\non images or videos, an LLM controller with an auxiliary control mechanism that can accurately parse\nlanguage instructions, an open-world toolkit that integrates various online models from HuggingFace,\nprivate models trained by users, as well as other applications (e.g., calculator, search engine).\nInternGPT gives equal importance to pointing and language instructions and uses the perception\nunit and the LLM controller to coordinate and execute the applications in the toolkit to accomplish\ncomplicated vision-centric tasks. In addition, for high-quality multi-modal dialogue including image\n2\ncaptioning and visual question answering, we provide a large-scale visual language model named\nHusky. Husky is quite efficient and effective, which impresses ChatGPT-3.5-turbo with 93.89 %\nGPT-4 Quality with only 7B parameters. Comparison between Husky with MiniGPT-4 [84] and\nLLaVA [39] are shown in Figure 2, and we see that Husky works better in complex reasoning\nscenarios at a photo casually shot at Shanghai AI Laboratory.\n1. First, you would likely check the temperature at the top\nof the computer monitor to see if the screen gets hot.\n2. You would then check the temperature of the fan that\nis situated on the desk.\n3. If the temperature rises above an acceptable level, you\nwould proceed to open the fan and manually adjust the\nspeed or ventilation to cool the device down.\n4. While doing so, you would also check the temperature\nof other components, such as the CPU and RAM, to\nensure that they are not excessively hot and functioning\noptimally.\n5. If you notice that the device still feels hot, you would\nproceed to check the room temperature to determine if\nit's too high and potentially causing the overheating\nproblem.\n6. Finally, you would implement a plan to address the\nissue, such as adjusting the airflow in the room, turning\non the air conditioner or opening a window to let in some\nfresh air.\"\nHusky (Ours)\nIf I feel hot, my plan would be to use the small fan that is\nsitting on the wooden desk. The fan would help to create\na cooling breeze and make me more comfortable in the\nroom. Additionally, I might also take breaks to cool down\nby stepping away from the computer and engaging in a\nlight activity, such as stretching or taking a short walk. It is\nessential to maintain a comfortable working environment\nto prevent fatigue and improve productivity.\nIf I feel hot, I will turn on the air conditioner to cool down\nthe room. I will also open the windows to let in fresh air\nand turn on a fan to circulate the air. I will also drink\nwater to stay hydrated and take breaks to cool down. I\nwill also use a cooling gel pack or take a cool shower to\ncool down my body.\nLLaVA\nMiniGPT-4\nHuman: With the objects in the image,\nif you feel hot, what will you do, write\nyour plan and explain it.\nThe photo shot at Shanghai AI Laboratory \nFigure 2: Comparsion results of Husky, MiniGPT-4 and LLaVA.\nCompared with previous pure-language interactive frameworks, iGPT excels in achieving higher\naccuracy and efficiency in complicated visual tasks including accurate interactive image or video\nediting, understanding, question-answering, visual content creation, etc. As evidenced in Figure 1,\nour system can successfully carry out intricate interactive tasks while pure-language systems fail.\nMoreover, our user surveys have revealed that combining pointing instructions with language instruc-\ntions can enhance work efficiency significantly, particularly in complicated scenarios that involve\nmany objects (object number greater than 2). We aim to establish this work as an open baseline\nfor visual interactive systems and will continue updating it with the capability of our VFMs (e.g.,\nInternImage [65] and InternVideo [69]) and the contribution/pull request from the community to\nachieve even better results.\n2\nRelated Work\nLarge language model. Recent LLMs [3, 46, 61, 81] have demonstrated a range of significant\nabilities, including language generation, in-context learning, world knowledge, and reasoning. The\npresence of these capabilities enables LLMs to perform complex tasks based on user instructions\nand prompts in a zero-shot manner. GPT-3 [3], the first language model with over 100 billion\nparameters, has achieved impressive zero-shot performance on various benchmarks. However, it\ndoes not consistently outperform smaller models, such as T5 [54], on some tasks. InstructGPT\nmodels [49], which are finetuned on a dataset consisting of prompts with the corresponding human-\nannotated desired behavior, can be aligned with users, generate outputs that are preferred over those\nfrom GPT-3 and show improvements in truthfulness and reductions. Instruction-tuned models have\nalso demonstrated a remarkable ability to generalize zero-shot to new tasks. Thus, instruction-\ntuning [44, 71, 21, 8] is considered key to eliciting the abilities of LLMs [17]. In addition to\nGPT model family [52, 53, 3, 46], several other LLMs exist, including OPT [83], LLaMA [61],\nMOSS [11] and GLM [81]. These models also achieve high performance and are open-sourced,\nproviding valuable experience in training large models and serving as a base for further fine-tuning\nfor different purposes. For example, Alpaca [70] proposes a self-instruct framework to instruction\ntune the LLaMA model family without relying heavily on human-written instruction data. Another\nactive research area on LLMs is chain-of-thought prompting (CoT) [72, 25, 18, 68]. CoT prompts\nmodels to solve problems step by step, greatly improving their reasoning ability of LLMs and making\nit possible to utilize LLMs for task splitting. As a result, LLMs can be combined with a variety\nof APIs [34, 45] and models [57, 73] trained for different modalities and serve as a controller to\nschedule them. This method liberates LLMs from pure language instructions and paves the way for a\nmulti-modal interactive system.\n3\nPerception model. The emergence of the AlexNet [26] can be considered as the beginning of\nthe development history of deep convolutional neural networks (CNN). Drawing on the success of\nAlexNet, many CNN with deeper networks, more parameters, and better performance have been\nproposed and successfully applied to computer vision. However, blindly deepening the network and\nincreasing parameters will not improve the performance without limit but will cause overfitting and\nincrease the cost of experiments. In 2004, GoogleNet [59] overcame the above issues by processing\nimages at multiple scales thanks to the proposed Inception mechanism, which combines convolution\noperations and pooling operations of different core sizes. In the same year, the concise VGG [58]\nonly used (3\u00d73) convolution and (2\u00d72) pooling to win the second place in classification and the\nfirst place in object detection. The ResNet [20] has extended the number of layers of the network\nto an unprecedented scale and solved the issue of deep network degradation, finally achieving\n3.57% error on the ImageNet [13] test set. The above models have achieved the dominant position\nof CNN in the visual field, and it was not until the birth of Vision Transformer (ViT) [14] that\nthis pattern was changed. Benefiting from the powerful Transformer [62] structure, many more\nadvanced transformer-based vision models, e.g., PVT [66, 67], Swin Transformer [41], etc., have\nbeen proposed. At the same time, CNN has also been revived (e.g., ConvNeXt [43], InternImage [65]),\nand some hybrid methods (e.g., ConViT [15], CeiT [80], and CoAtNet [12]) of CNN and Transformer\nhave appeared to make full use of the advantages of both. The benign competition of the visual\nfoundation model has also greatly promoted the development of other visual recognition tasks,\nsuch as object detection [55, 38, 4, 85, 77, 75, 76, 79], segmentation [7, 22, 35, 6, 23, 19], video\nunderstanding [64, 37, 16, 2, 1, 30, 60, 5, 63, 31, 32, 74, 42], etc. These models have a strong ability\nto perceive the physical world and can serve as the sensory organs for LLMs.\nLLM-based interactive system. The success of LLMs [3, 46] has led to the development of AI\nsystems that integrate perception models and LLMs for multimodal reasoning and action. One\nsuch system is Visual ChatGPT [73], which connects ChatGPT with visual foundation models to\ngenerate and edit images during chatting. Another paradigm is MM-REACT [78], which integrates\nChatGPT with a pool of vision experts for multimodal reasoning and action. HuggingGPT [57]\nutilizes numerous sophisticated AI tasks in different modalities and domains from HuggingFace\u2019s\nabundant AI models to achieve impressive results. TaskMatrix.AI [36] connects foundation models\nwith millions of APIs from other AI models and systems to perform diversified tasks in both digital\nand physical domains. However, these systems are limited by their dependence on pure language\ninstructions, which can hinder effective communication and task performance, just like a console\ncannot achieve what GUI can. By incorporating pointing instructions, the proposed iGPT significantly\nimproves the efficiency of communication between users and chatbots, as well as the accuracy of\nchatbots in vision-centric tasks.\n3\nInternGPT\nInternGPT (iGPT) aims to provide an intuitive, user-friendly, and efficient way of human-computer\ninteraction (HCI) in AI that leverages the integration of a large language model (LLM), pointing\ndevices, and computer vision algorithms to perform vision-centric tasks. As illustrated in Figure 3,\niGPT consists of three main components: (1) a perception unit that interprets the user\u2019s pointing\ngestures on images and videos, enabling precise object selection and identification; (2) an LLM\ncontroller that processes the user\u2019s language commands, facilitating natural communication and (3)\nan open-world toolkit that integrates various off-the-shelf models/applications to offer a versatile\nplatform for different tasks.\niGPT\u2019s design allows it to operate effectively at multiple levels, catering to diverse needs:\nLevel 1: basic interaction. An intuitive way to use iGPT is to give it simple commands that trigger\npre-defined tasks, such as \u201ccaption this photo\u201d. iGPT then calls the appropriate model, such\nas BLIP [28], to produce the desired results. At this level, iGPT acts as a front-end of traditional\nsingle-task foundation models, without requiring complicated interactive logic, such as chain of\nthought, contextual reasoning, etc.\nLevel 2: Language-guided interaction. Real-world tasks often involve more complex and diverse\ndemands than traditional pre-defined tasks at level 1. To accomplish these tasks, language instructions\nwith clear specifications or multiple dialogue turns are needed. For example, the command \u201cremove\nthe black dog near the table in the image\u201d requires open-vocabulary detection models\n4\nInternGPT\nclick, drag,\ndraw, ...\nlocation, object,\nstroke, ...\ntoolkit\n\u2022\nBLIP\n\u2022\nStable Diffusion\n\u2022\nPix2Pix\n\u2022\nControlNet\n\u2022\nInternImage\n\u2022\nInternVideo\n\u2022\nHuggingFace\n\u2022\nCalculator\n\u2022\nGoogle\n...\npointing\nrequirements\nfeedback\nusers\nperception unit\n\u2022\nSAM\n\u2022\nOCR\n...\n\"DEF\"\noutput\nchat, instruct,\nask, ...\nplanning,\ncalling, ...\nlanguage\n\"ABC\"\nLLM controller\n\u2022\nChatGPT, GPT-4\n\u2022\nLLaMA\n...\nFigure 3: Overall architecture of InternGPT. It has three main components: perception unit, LLM\ncontroller, and open-world toolkit.\nto identify the target object based on the language description, and then apply the erase model to\nremove it from the image. At this level, iGPT is an assistant that communicates with users to resolve\nany ambiguity using natural language.\nLevel 3: pointing-language enhanced interaction. When tasks require more precise specifications\nthat language instructions alone cannot provide, pointing-language instructions become essential. For\nexample, editing/recognizing/OCRing a particular part of an image can be difficult to describe with\nwords. Nonverbal cues, such as gestures or cursor movements, help select, move, or draw objects in\nthe image.\nCompared to existing systems like ChatGPT and Visual ChatGPT, iGPT represents a significant\nadvancement in user-centric interaction by combining pointing and language instructions to accom-\nplish complex vision-centric tasks. Practical use cases include image editing, object manipulation,\nvideo annotation, and more, demonstrating its potential impact across various academic and industrial\nfields. We detail iGPT\u2019s design in the following.\n3.1\nInteracting with User\nAs shown in Figure 3, iGPT\u2019s framework accomplishes its tasks through continuous interaction\nwith users. Each time the user provides their requirements via pointing and language instructions,\nthe perception unit analyzes the pointing instructions to identify the target or the content created.\nSimultaneously, the LLM controller interprets the user\u2019s language instructions, breaking down the\ntask into smaller subtasks and selecting the appropriate tool.\nThese components work in tandem, delivering accurate and efficient results to users. Similar to previ-\nous methods [73], our system keeps a record of the conversation history, enabling users to revisit prior\ntasks and results. This capability ensures continuous improvement and better performance over time.\n3.2\nPerception Unit\nBuilt on community open-source projects like SAM [24] and OCR [10], iGPT\u2019s perception unit parses\npointing instructions and performs various operations, such as pick, drag, and draw. It uses click,\nstroke, drag, and draw gestures for object selection, movement, and content creation. Specifically, the\ndrag gesture is used to move objects to different positions, while the draw gesture is used to create or\ncomplete shapes, aided by image generation technology.\nThe current system uses simple logic to interpret pointing instructions. After the pointing gesture\nis finished, we handle it in three ways: (1) For typical objects, SAM detects the semantic region to\n5\nenable selection. (2) For scene text, OCR technology extracts pointed text content. (3) For generation\ntasks, gestures are stored as strokes and fed into AIGC tools.\nThe perception unit\u2019s versatility and proficiency in executing various operations based on user input\nare crucial to our system\u2019s success. Whether the user needs to select an object, move it, or create\nnew content, the perception unit makes it possible. By combining cutting-edge AI solutions with\nuser-friendly interfaces, we ensure a seamless and engaging user experience.\n3.3\nLLM Controller\nSimilar to previous interactive systems [73, 78, 57], iGPT manages complex tasks based on large\nlanguage models (LLMs). It autonomously parses user language requests, decomposes them into\nmultiple tasks, and plans the task order and dependency based on LLM knowledge. The LLM\ncontroller allocates the parsed tasks to the corresponding APIs based on the model descriptions.\nThrough continuous analysis of historical data and user interactions, the system improves task\nallocation and execution for efficient management.\nAccurate task execution with the auxiliary control mechanism. Even top language models like\nChatGPT and GPT-4 struggle with invoking APIs, especially when parsing and passing arguments.\nTo tackle this issue, iGPT employs auxiliary control when LLM fails to act as a controller. It\nworks as follows: (1) Parse verbs and nouns to identify the API before execution. (2) Extract\nrelevant arguments from prior dialogues based on the API. For example, in the case of \u201cremove the\nmasked object\u201d, query the LLM: \u201cWhat\u2019s the image_path and mask_path of the \u2018remove\nthe masked object\u2019 API?\u201d This retrieves the \u201cimage_path\u201d and \u201cmask_path\u201d from the chat\nhistory. (3) Check argument validity and apply rule-based corrections if necessary. (4) Invoke the API\nwith the identified arguments. This mechanism refines abstract instructions into specific commands,\nresulting in more accurate task execution.\nSpeech transcription. Additionally, iGPT also offers speech transcription features, such as whisper\n[51] and bark [9], enabling users to communicate without typing.\n3.4\nToolkit\niGPT\u2019s toolkit is called upon by the system. Different from previous works [73, 78, 57], it supports\ninput from pointing devices in addition to standard API descriptions, inputs, and outputs. Here, we\nshow four representative examples of API descriptions as follows:\nExample 1: remove the masked object.\nInput: original image, mask (pick); Output: re-\nsult image;\nPrompt:\n\u201cuseful when you want to remove an object by masking the\nregion in the image, like:\nremove the object by the masked region.\nThe\ninput to this tool should be a comma-separated string of two, representing\nthe image_path and mask_path.\nThe input to this tool should be a string,\nrepresenting the image_path\u201d.\nExample 2: question the masked object. Input: original image, mask (pick); Output: result\ntext; Prompt: \u201cuseful when you need an answer for a question based on a masked\nimage.\nlike:\nwhat is the background color in the masked region, how many\ncats are in this masked figure, what is in this masked figure.\nThe input\nto this tool should be a comma-separated string of two, representing the\nimage_path and the question\u201d.\nExample 3: conditional image generation.\nInput: stroke draft (drag & draw); Output:\nresult image;\nPrompt:\n\u201cuseful when you want to replace an object by clicking\nin the image with another object or something.\nlike:\nreplace the masked\nobject with a new object or something.\nThe input to this tool should be\na comma-separated string of three, representing the image_path and the\nmask_path and the prompt\u201d.\nExample 4: video highlight interpretation. Input: original video, mask at timestamp t (pick);\nOutput:\nresult video;\nPrompt:\n\u201cuseful when you want to generate a video with\nTikTok style based on prompt, like:\ncut this video to a TikTok video based\n6\non a prompt.\nThe input to this tool should be a comma-separated string of\ntwo, representing the video_path and prompt.\u201d\nA comprehensive summary of the toolkit\u2019s APIs is provided in Table 1, covering applications\nincluding vision, vision-language foundation models, as well as other applications such as calculators\nand search engines. This diverse range of APIs enables users to harness advanced techniques for\nvarious tasks and achieve their goals more efficiently.\nSpecifically, for the large vision-language model named Husky in Table 1, we follow the approach\nof BLIP-2 [27], and replace the language model with LLaMA-7B, which is trained on 52k En-\nglish instruction-following data generated by GPT-4 [46]. To handle multi-modal tasks without\ncompromising the text-only generation capabilities, we strictly adhere to the input format used in\ninstruct-tuned LLaMA and carry out finetuning of Husky in three stages. As a result, Husky exhibits\nimpressive capabilities, such as visual captioning, visual question-answering, complex reasoning,\ncode generation similar to those of MiniGPT-4 [84] and LLaVA [39].\nCategory\nAPI\nVision\nStable Diffusion [56], ControlNet [82], InternImage [65], InternVideo [69],\nSAM [24], DINOv2 [48], DragGAN [50], ImageBind [19], etc.\nVision-Language\nBLIP-2 [27], Grounding DINO [40], GLIP [33], VideoChat [29], Husky, etc.\nOthers\nCalculator, Google, HuggingFace, etc.\nTable 1: Representive APIs supported by InternGPT.\n4\nExperiment\n4.1\nUser Studies\nThe iGPT framework enhances the communication effectiveness of the interactive system with the\nuser by incorporating verbal and nonverbal instructions. To prove the effectiveness of our mixed-\nmode instructions, which combine language and pointing gestures, in comparison to purely linguistic\ninstructions, we conduct a user study involving 10 human participants. These participants interact\nwith Visual ChatGPT [73] and our iGPT through chat and provide their feedback. We present various\nfindings from the user study as follows:\nEfficiency. We ask users to create an image-centric work by designing input instructions for Visual\nChatGPT [73] and iGPT, which involve removing and replacing objects. Users can refine their\ninstructions if the results are unsatisfactory. We assume that a user has the patience to attempt up to\nten times. Cases with over ten attempts are considered as failed. The number of attempts and prompt\nlengths required to achieve satisfactory results are presented in Table 2 and Table 3, demonstrating\nthat the iGPT is more efficient and user-friendly.\nHuman preference. Users were asked to rank the interactive systems based on their user experience,\nfocusing specifically on the results generated by these systems. Ten unbiased human evaluators were\nthen responsible for assessing the quality of the outcomes. The evaluation results can be found in\nTable 2 and Table 3. As can be observed, our iGPT stands out for its efficiency and user-friendliness,\nand gains a higher preferences.\n4.2\nDemonstrations\niGPT is a versatile framework that can support a range of applications involving verbal and non-verbal\ninteractions. Users can communicate with the system using natural language, as well as gestures\nsuch as clicking, dragging, pointing, etc. We showcase several examples of interesting vision-centric\napplications:\nDemo 1: interactive image editing. Figure 4 exhibits interactive image editing examples. Except\nfor verbal instructions and an uploaded image, the iGPT receives a cursor clicking at the desired\noperating location of the image. The cursors interaction supplements user instructions to the LLM\n7\nOne-Object\nTwo-Object\nThree-Object\nComplex-I\nComplex-II\nExample\nVisual ChatGPT [73]\nPrompt length\n4.9\n7.0\n8.2\n7.3\n9.3\n# Try (<10)\n3.9\n9.5\n6.2\n9.6\n9.6\nScore (1-5)\n3.2\n1.5\n2.6\n0.3\n0.6\nExample Result\nInternGPT (ours)\nPrompt length\n2.7\n2.7\n3.0\n2.6\n3.3\n# Try (<10)\n1.0\n1.0\n1.3\n1.2\n1.0\nScore (1-5)\n4.9\n3.6\n3.2\n4.5\n3.4\nExample Result\nTable 2: User study for \u201cremove something\u201d. \u201c# Try\u201d indicates the number of attempts to get a\nsatisfactory result, not more than 10 times. \u201c-\u201d means trying more than 10 times and still getting bad\nresults.\nwhich controls the visual perception and generation instruments. With more concise and precise\ninstructions, the InternGPT demonstrates pleasing image editing performance.\nDemo 2: interactive visual question answering. Figure 5 presents interactive visual question-\nanswering examples. In the image-centric conversation with an intellective chatbot, the user\u2019s\ninstructions may revolve around just a region of the image, rather than the entire image. The user can\nindicate the location through the simplest clicking or touching operation and communicate with the\nchatbot about the concerned region.\nDemo 3: interactive image generation. Figure 6 demonstrates image creation examples. The iGPT\nstores multiple picking image parts as materials. The user can easily assemble the materials by the\ndragging interaction. After receiving a verbal prompt and the assembled images from the user, the\niGPT controls the generation instruments to create a fancy image, and give it a title.\nDemo 4: video highlight interpretation. We have implemented a feature called Video Highlight\nInterpretation, which is demonstrated in Figure 7. This feature is designed to help users quickly and\neasily understand the essence of a video without having to watch the entire thing. iGPT locates the\nbeginning and end of a video highlight based on its temporal position and then provides a lighthearted\nand humorous voice-over interpretation of the highlighted content. In addition to its primary use,\nthis feature can also be leveraged by users who want to quickly create derivative works based on a\npopular video\u2019s main points. By using this feature, users can save time and effort in creating new\ncontent based on existing videos, while still ensuring that the new content accurately reflects the\noriginal material.\nThese examples exhibit the convenient interactive capabilities and powerful performance on vision-\ncentric tasks, which come from the novel design of the intelligence system.\n8\nTest Case\nSingle-Object\nTwo-Object\nThree-Object\nComplex-I\nComplex-II\nExample\nVisual ChatGPT [73]\nPrompt length\n7.0\n9.3\n9.8\n7.8\n9.4\n# Try\n1.2\n4.8\n9.3\n8.5\n10\nScore (1-5)\n4.2\n2.2\n0.7\n0.2\n0.0\nExample Result\nInternGPT (ours)\nPrompt length\n6.6\n7.0\n6.1\n6.9\n7.3\n# Try\n1.0\n1.0\n1.2\n1.0\n4.6\nScore (1-5)\n4.5\n5.0\n3.8\n4.4\n1.9\nExample Result\nTable 3: User study for \u201creplace something with something\u201d \u201c# Try\u201d indicates the number\nof attempts to get a satisfactory result, not more than 10 times. \u201c-\u201d means trying more than 10 times\nand still getting bad results.\n4.3\nDemonstrations of Husky\nQualitive Evaluation. In this section, we present demonstrations across various scenarios to assess\nthe performance of the large vision-language model Husky. Qualitive results are shown in Figure 8.\nThese diverse examples showcase the strong capabilities of our Husky.\nQuantitative Evaluation. We also perform a quantitative evaluation on Husky. We follow the setting\nof LLaVA [39] and adopt Husky to predict the answers to the provided 90 questions of the 30 COCO\nvalidation images. We leverage ChatGPT-3.5-turbo to measure the quality of our model\u2019s generated\nresponses and apply GPT-4\u2019s predictions (provided by LLaVA) as a reference. As reported in Table 4,\nHusky can impress ChatGPT-3.5-turbo with 93.89 % GPT-4 Quality. It is worth mentioning that,\naccording to the analysis conducted by ChatGPT-3.5-turbo, Husky demonstrates a reasoning capacity\nthat is comparable to that of GPT-4. However, when it comes to conversation and providing detailed\ndescriptions, Husky is still lagging behind that of GPT-4.\nConversation\nDetail description\nComplex reasoning\nAll\n96.13\n83.87\n102.95\n93.89\nTable 4: Detailed Quantitative Results\n4.4\nLimitations\nGiven that iGPT is built upon online resources, potential limitations of this integrated system may\ninclude: Model Performance. iGPT\u2019s effectiveness largely hinges on the quality and accuracy of\n9\nW\nclick\nI have used the tool \"Segment the Clicked Region\" to \nsegment the clicked region in the image.\nI have used the tool \"Remove the masked object\" with \nthe input \"tmp/img001.png\", and the output is \n\"tmp/img002.png\"\nI have used the tool \u201cReplace the masked object\u201d to \nreplace the clicked region of \"tmp/img001.png\" with the \nEiffel Tower. The output image is \"tmp/img003.png\"\nW\nRemove the selected region.\nW\nReplace it with the Eiffel Tower.\nFigure 4: Interactive image editing.\nW\nclick\nI have used the tool \u201cImage OCR tool\u201d to segment the \nclicked region in the image. The text is \u201cROCK \nSHRIMP....................popcorn shrimp, glazed with chef's \nspecial mayo 8.00 \u201d\n\"Rock Shrimp\" is a dish consisting of small shrimp that \nhave been breaded and fried until crispy, similar to \n\"popcorn shrimp.\" The dish is then coated with a special \nmayonnaise glaze created by the chef. The cost of this \ndish is $8.00.\nW\nI want to eat one meat and one vegetarian. I have $20 in \nmy pocket. What can I buy? \nBased on the prices given in the menu, you can buy the \nfollowing:\nMeat dish - Hamachi Kama for $9.95 Vegetarian dish -\nEdamami for $4.50\nTotal cost: $14.45\nSince you have $20 in your pocket, you still have $5.55 \nleft. You can choose to add a seaweed salad or a drink to \ncomplete your meal.\nW\nWhat's it?\nFigure 5: Interactive visual question answer.\nthe underlying open-sourced models. Limitations or biases in these models could adversely affect\niGPT\u2019s performance.\nScalability. As user interactions grow more complex or encompass a greater number of instances,\nmaintaining accuracy and response times could prove challenging for the system. Moreover, the\ncurrent non-learnable cooperation between vision foundation models and language models, e.g., not\nbeing tuned by instruction data, could impede capitalizing on the full capacity of the used models.\nAdaptability. iGPT might struggle to adjust to novel or uncommon scenarios absent from the training\ndata of its employed models, leading to a compromised performance in unpredictable situations.\nUser Interface. Despite emphasizing user-friendliness, some users might still face difficulties\nutilizing the combined pointing and language instructions effectively, which could impact their\noverall experience.\nCompatibility. Achieving seamless integration with a diverse array of devices and platforms\ncould pose challenges due to varying hardware capabilities, software constraints, or accessibility\nrequirements.\n10\nW\nW\nGenerate a new image. My prompt is Prompts: An \nUltraman wear in overalls is holding a pan in his hand in \nthe boundless universe image/abc7b8f5.png.\nI have used the tool \"Generate Image Condition On \nSegmentations\" with the input \"tmp/img001.png\", and \nthe output is \"tmp/img002.png\"\nI have used the tool \"Segment Anything on Image\" with \nthe input \"tmp/img001.png\", and the output is \n\"tmp/img002.png\".\nW\nSegment Anything on Image of tmp/img001.png\ndraft \ncobbled by \nimage patches\n(using drag)\nFigure 6: Interactive image generation.\nI have used the \"Cut and Generate Video tool\" to cut out \nthe dancing part of the video ./tmp/001.mp4 and the \noutput file is ./tmp/002.mp4.\nW\nclick\nI have used the tool \"Segment the Clicked Region\" to \nsegment the clicked region in the video. \nThe generated videos are all clips of \nplaying basketball\nAttention, girls and boys! Here's a handsome \nyoung man named Xiao Shuai, who is not only \nan excellent dancer but also an amazing \nbasketball player! On stage, he demonstrated \nan unparalleled ability to perform with \nprecision, leaving everyone in awe.\nW\nCut out the part of dribbling the basketball.\nFigure 7: Video highlight interpretation.\nPrivacy and Security. As an AI-driven system, iGPT may raise concerns regarding data privacy and\nsecurity, especially if sensitive information is processed or shared within the platform.\n5\nConclusion\nWe have developed InternGPT (iGPT for short), a dynamic visual framework that tackles vision-\ncentric tasks. Designed with an emphasis on user-friendliness and efficacy, iGPT delivers a top-tier\nexperience among current open-source tools. Our methodology seamlessly merges pointing and text\ncommands to issue instructions, allowing users to harness the power of various open-sourced models\nwithout needing expert knowledge. The robustness of iGPT has been showcased in complex visual\nscenarios involving multiple instances, utilizing user-level text comprehension from the current LLM\nand employing cursors or gestures through pointing devices.\niGPT appreciates both pointing and linguistic directives, leveraging the perception unit and LLM\ncontroller to synchronize and execute applications within an open-world toolkit. Our system has\nsuccessfully performed intricate interactive tasks beyond the capabilities of purely language-based\nsystems. User surveys have demonstrated that integrating pointing and language instructions can\nboost work efficiency in challenging situations. Aspiring to be the foundational benchmark for\nvisual interactive systems, iGPT is committed to ongoing updates and improvements for exceptional\nperformance.\n11\nHuman: describe the image in detail.\nAssistant: The image features a man dressed in a business\nsuit with a chicken costume covering his entire body, giving\nhim an amusing and unique appearance. The man's face is\nnot visible, as it is hidden under the chicken head and mask.\nThis image is an artistic representation and not a real-life\nscenario.\nHuman: is this image funny?\nAssistant: The image has a humorous and whimsical quality\nto it, which may be interpreted as funny by some people.\nThe juxtaposition of a man in a business suit with a chicken\ncostume covering his entire body is unexpected and\namusing, invoking a sense of humor and playfulness.\nHowever, humor is subjective and may not be universally\nappreciated, so it may not be considered funny by everyone.\nHuman: write a poem about the image.\nAssistant: A young man and woman sit together on the\nrooftop of a tall building,\nA city skyline in view, a view that's quite enthralling.\nTheir embrace is tender, a moment captured in time,\nA moment to cherish, an image of love between these two.\nSitting on the roof of a tall building, they pose for a picture,\nEmbraced in each other, a scene that's quite captivating.\nTheir love so tender, it shines through in their eyes,\nA bond that's strong and lasting, that's quite impressive.\nA young couple in love, a moment captured in time,\nA city skyline in view, a beautiful sight to see.\n(a) Caption & Joke Understanding\n(b) Poem Generation\n(c) Complex Reasoning\nHuman: write a story between the\nboy and the panda according to the\nimage\nAssistant: Once upon a time, in a small village nestled\nbetween the mountains on a misty morning, a young boy\nand a giant panda went for their walk down the rain-soaked\nroad.\nThe boy, who was a curious young lad, was fascinated with\nthe huge panda walking down the road. The panda had a\ncurious look on his face, as he was not used to walking on\nsuch a narrow path. As the panda made his way down the\nroad, the boy followed him, trying to understand the\npanda's movements and behavior. The panda seemed to\nmove slowly and gracefully despite the heavy rain, and the\nboy was in awe of his strength and agility. The boy was\ndetermined to learn more about the panda, so he followed\nhis every step, soaking in the experience like a sponge. As\nthey walked together down the rain-soaked road, the boy\nand panda bonded in their shared journey, and the young\nboy was\nenriched\nwith\na unique\nand\nunforgettable\nexperience.\n(d) Story Generation\nHuman: What are the things I \nshould be cautious about when I \nvisit here?\nAssistant: When visiting this picturesque location, it is \nimportant to be cautious of several potential hazards. Firstly, \nthe serene lake and surrounding forest create a beautiful \nbackdrop, but it's crucial to avoid swimming or wading in \nshallow water, as sudden changes in water levels or currents \ncan pose risks. Similarly, navigating the wooden pier is also a \nconsideration, as it may be narrow and could pose a danger \nto larger vessels or those with multiple passengers. \nAdditionally, it's important to avoid approaching or feeding \nany wildlife in the area, as this can cause disruptions and \npotential harm to both the animal and the visitor. By being \naware of these potential hazards, visitors can fully enjoy the \nbeauty of the location without any unnecessary risks.\nAssistant: Be cautious of swimming, navigating the pier, \nstanding or setting up camera equipment, and \napproaching or feeding wildlife in the area. By doing so, \nyou can fully appreciate the beauty of the location while \nminimizing risks.\nHuman: Write it in short.\nFigure 8: Demonstrations of Husky across various scenarios.\n12\nReferences\n[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. Vivit: A\nvideo vision transformer. 2021 IEEE/CVF International Conference on Computer Vision, pages 6816\u20136826,\n2021. 4\n[2] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video\nunderstanding? In International Conference on Machine Learning, 2021. 4\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in Neural Information Processing Systems, 33:1877\u20131901, 2020. 3, 4\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision,\npages 213\u2013229. Springer, 2020. 4\n[5] Guo Chen, Sen Xing, Zhe Chen, Yi Wang, Kunchang Li, Yizhuo Li, Yi Liu, Jiahao Wang, Yin-Dong\nZheng, Bingkun Huang, et al. Internvideo-ego4d: A pack of champion solutions to ego4d challenges.\narXiv preprint arXiv:2211.09529, 2022. 4\n[6] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer\nadapter for dense predictions. In International Conference on Learning Representations, 2023. 4\n[7] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for\nsemantic segmentation. Advances in Neural Information Processing Systems, 34:17864\u201317875, 2021. 4\n[8] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416, 2022. 3\n[9] Bark contributors. Bark. https://github.com/suno-ai/bark, 2023. 6\n[10] EasyOCR contributors. Easyocr. https://github.com/JaidedAI/EasyOCR, 2023. 5\n[11] MOSS contributors. Moss. https://github.com/OpenLMLab/MOSS, 2023. 3\n[12] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for\nall data sizes. Advances in Neural Information Processing Systems, 34:3965\u20133977, 2021. 4\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255,\n2009. 4\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning\nRepresentations, 2021. 4\n[15] St\u00e9phane d\u2019Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun.\nConvit: Improving vision transformers with soft convolutional inductive biases. In International Conference\non Machine Learning, pages 2286\u20132296. PMLR, 2021. 4\n[16] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video\nrecognition. 2019 IEEE/CVF International Conference on Computer Vision, pages 6201\u20136210, 2018. 4\n[17] Hao Fu, Yao; Peng and Tushar Khot. How does gpt obtain its ability? tracing emergent abilities of language\nmodels to their sources. Yao Fu\u2019s Notion, Dec 2022. 3\n[18] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham\nNeubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022. 3\n[19] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin,\nand Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 15180\u201315190, 2023. 4, 7\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778,\n2016. 4\n13\n[21] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D\u00e1niel Simig, Ping Yu, Kurt\nShuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction\nmeta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022. 3\n[22] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One\ntransformer to rule universal image segmentation. arXiv preprint arXiv:2211.06220, 2022. 4\n[23] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, and\nPing Luo. Ddp: Diffusion model for dense visual prediction. arXiv preprint arXiv:2303.17559, 2023. 4\n[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\narXiv:2304.02643, 2023. 5, 7\n[25] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language\nmodels are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. 3\n[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional\nneural networks. Communications of the ACM, 60(6):84\u201390, 2017. 4\n[27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 7\n[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In International Conference on Machine\nLearning, pages 12888\u201312900. PMLR, 2022. 4\n[29] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 7\n[30] Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Jiao Qiao. Uniformer:\nUnified transformer for efficient spatiotemporal representation learning. In International Conference on\nLearning Representations, 2022. 4\n[31] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Limin Wang, and Yu Qiao. Uniformerv2:\nSpatiotemporal learning by arming image vits with video uniformer. arXiv preprint arXiv:2211.09552,\n2022. 4\n[32] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher:\nTowards training-efficient video foundation models. arXiv preprint arXiv:2303.16058, 2023. 4\n[33] Liunian Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan Li, Yiwu Zhong,\nLijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded\nlanguage-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022. 7\n[34] Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A\nbenchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244, 2023. 3\n[35] Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Ping Luo, and Tong\nLu. Panoptic segformer: Delving deeper into panoptic segmentation with transformers. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1280\u20131289, 2022. 4\n[36] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,\nShaoguang Mao, et al. Taskmatrix. ai: Completing tasks by connecting foundation models with millions of\napis. arXiv preprint arXiv:2303.16434, 2023. 4\n[37] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. 2019\nIEEE/CVF International Conference on Computer Vision, pages 7082\u20137092, 2018. 4\n[38] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object\ndetection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2980\u2013\n2988, 2017. 4\n[39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485, 2023. 3, 7, 9\n[40] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,\nHang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object\ndetection. arXiv preprint arXiv:2303.05499, 2023. 7\n14\n[41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 10012\u201310022, 2021. 4\n[42] Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, and Tong Lu. Tam: Temporal adaptive module for\nvideo recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages\n13708\u201313718, 2021. 4\n[43] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A\nconvnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 11976\u201311986, 2022. 4\n[44] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le,\nBarret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction\ntuning. arXiv preprint arXiv:2301.13688, 2023. 3\n[45] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering\nwith human feedback. arXiv preprint arXiv:2112.09332, 2021. 3\n[46] OpenAI. Gpt-4 technical report. arXiv, 2023. 1, 3, 4, 7\n[47] TB OpenAI. Chatgpt: Optimizing language models for dialogue. OpenAI, 2022. 1\n[48] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre\nFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual\nfeatures without supervision. arXiv preprint arXiv:2304.07193, 2023. 7\n[49] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022. 3\n[50] Xingang Pan, Ayush Tewari, Thomas Leimk\u00fchler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt.\nDrag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIGGRAPH\n2023 Conference Proceedings, 2023. 7\n[51] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust\nspeech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356, 2022. 6\n[52] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding\nby generative pre-training. OpenAI, 2018. 3\n[53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 3\n[54] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485\u20135551, 2020. 3\n[55] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection\nwith region proposal networks. In Advances in Neural Information Processing Systems, pages 91\u201399, 2015.\n4\n[56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10684\u201310695, 2022. 7\n[57] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. 1, 2,\n3, 4, 6\n[58] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-\ntion. In International Conference on Learning Representations, 2015. 4\n[59] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru\nErhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139, 2015. 4\n15\n[60] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient\nlearners for self-supervised video pre-training. In Advances in Neural Information Processing Systems,\n2022. 4\n[61] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023. 1, 3\n[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems,\n30, 2017. 4\n[63] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao.\nVideomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2023. 4\n[64] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal\nsegment networks: Towards good practices for deep action recognition. In European Conference on\nComputer Vision, pages 20\u201336, 2016. 4\n[65] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei\nLu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable\nconvolutions. arXiv preprint arXiv:2211.05778, 2022. 3, 4, 7\n[66] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 568\u2013578, 2021. 4\n[67] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media,\n8(3):415\u2013424, 2022. 4\n[68] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves\nchain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. 3\n[69] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu,\nYi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao.\nInternvideo: General video foundation models via generative and discriminative learning. arXiv preprint\narXiv:2212.03191, 2022. 3, 7\n[70] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv\npreprint arXiv:2212.10560, 2022. 3\n[71] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al.\nSuper-\nnaturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings\nof the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, 2022. 3\n[72] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of\nthought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. 3\n[73] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\nTalking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 1, 2,\n3, 4, 5, 6, 7, 8, 9\n[74] Min Yang, Guo Chen, Yin-Dong Zheng, Tong Lu, and Limin Wang. Basictad: an astounding rgb-only\nbaseline for temporal action detection. Computer Vision and Image Understanding, page 103692, 2023. 4\n[75] Xue Yang, Junchi Yan, Ziming Feng, and Tao He. R3det: Refined single-stage detector with feature\nrefinement for rotating object. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,\npages 3163\u20133171, 2021. 4\n[76] Xue Yang, Junchi Yan, Qi Ming, Wentao Wang, Xiaopeng Zhang, and Qi Tian. Rethinking rotated object\ndetection with gaussian wasserstein distance loss. In International Conference on Machine Learning, pages\n11830\u201311841. PMLR, 2021. 4\n16\n[77] Xue Yang, Jirui Yang, Junchi Yan, Yue Zhang, Tengfei Zhang, Zhi Guo, Xian Sun, and Kun Fu. Scrdet:\nTowards more robust detection for small, cluttered and rotated objects. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 8232\u20138241, 2019. 4\n[78] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and\naction. ArXiv, abs/2303.11381, 2023. 1, 2, 4, 6\n[79] Yi Yu, Xue Yang, Qingyun Li, Yue Zhou, Gefan Zhang, Junchi Yan, and Feipeng Da. H2rbox-v2: Boosting\nhbox-supervised oriented object detection via symmetric learning. arXiv preprint arXiv:2304.04403, 2023.\n4\n[80] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution\ndesigns into visual transformers. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 579\u2013588, 2021. 4\n[81] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\nZheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414,\n2022. 3\n[82] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv\npreprint arXiv:2302.05543, 2023. 7\n[83] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022. 3\n[84] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 3,\n7\n[85] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. In International Conference on Learning Representations,\n2021. 4\n17\n"
  },
  {
    "title": "AudioSlots: A slot-centric generative model for audio separation",
    "link": "https://arxiv.org/pdf/2305.05591.pdf",
    "upvote": "2",
    "text": "AUDIOSLOTS: A SLOT-CENTRIC GENERATIVE MODEL FOR AUDIO SEPARATION\nPradyumna Reddy\u2217\nUniversity College London\nScott Wisdom, Klaus Greff, John R. Hershey, Thomas Kipf\nGoogle Research\nABSTRACT\nIn a range of recent works, object-centric architectures have been\nshown to be suitable for unsupervised scene decomposition in the\nvision domain. Inspired by these methods we present AudioSlots, a\nslot-centric generative model for blind source separation in the au-\ndio domain. AudioSlots is built using permutation-equivariant en-\ncoder and decoder networks. The encoder network based on the\nTransformer architecture learns to map a mixed audio spectrogram\nto an unordered set of independent source embeddings. The spa-\ntial broadcast decoder network learns to generate the source spec-\ntrograms from the source embeddings. We train the model in an\nend-to-end manner using a permutation invariant loss function. Our\nresults on Libri2Mix speech separation constitute a proof of concept\nthat this approach shows promise. We discuss the results and limita-\ntions of our approach in detail, and further outline potential ways to\novercome the limitations and directions for future work.\nIndex Terms\u2014 Speech separation, object-centric representation\n1. INTRODUCTION\nRecently there has been a lot of research into neural network based\narchitectures that operate on set-structured data and architectures\nthat learn to map from unstructured inputs to set-structured output\nspaces. In particular, in the vision domain, slot-centric or object-\ncentric architectures underpin recent advances in object detection [1]\nand unsupervised object discovery [2, 3].\nThese object-centric architectures have an inbuilt inductive bias\nof permutation equivariance, making them a natural \ufb01t for the task of\naudio separation. In this paper we apply the core ideas from these ar-\nchitectures to the problem of sound separation: the task of separating\naudio sources from mixed audio signals without access to privileged\nknowledge about the sources or the mixing process. Sound separa-\ntion is inherently a set-based problem, as the ordering of the sources\nis arbitrary.\nWe frame sound separation as a permutation-invariant condi-\ntional generative modeling problem: we learn a mapping from a\nmixed audio spectrogram to an unordered set of independent source\nspectrograms. Our method, AudioSlots, separates audio into indi-\nvidual latent variables per source, which are then decoded into indi-\nvidual source spectrograms. It is built using permutation-equivariant\nencoder and decoder functions based on the Transformer architec-\nture [4] and thus invariant to the ordering of the source latent vari-\nables (\u201cslots\u201d).\nTo evaluate the promise of such an architecture, we train Au-\ndioSlots using a matching-based loss to generate separate sources\nfrom the mixed audio-signal. We demonstrate our method on a sim-\nple two-speaker speech separation task from Libri2Mix [5].\n\u2217Work done while interning at Google.\nWhile our results primarily constitute a proof of concept for this\nidea, we \ufb01nd that sound separation with slot-centric generative mod-\nels shows promise, but comes with certain challenges: the presented\nversion of our model struggles to generate high-frequency details, re-\nlies on heuristics for stitching independently predicted audio chunks,\nand still requires ground-truth reference audio sources for training.\nWe are optimistic that these challenges can be overcome in future\nwork, for which we outline possible directions in this paper.\n2. RELATED WORK\nOur work explores a novel set-based generative modeling approach\nfor sound separation. In the following, we provide a brief overview\non recent prior learning-based approaches for sound separation as\nwell as on set-based (or slot-centric) neural network architectures\nused in other domains.\nSound separation. A variety of neural network methods have\nbeen proposed for supervised sound separation, differing in terms\nof their sound reconstruction mechanisms and their overall architec-\ntures. Mask-based methods reconstruct sound by predicting sepa-\nration masks that are applied to an analysis/synthesis basis repre-\nsentation of the input audio signal (such as STFT or learned basis),\ne.g. [6, 7, 8, 9, 10, 11, 12, 13]. Alternatively, direct reconstruction\nmethods estimate the source signals or their spectra without explic-\nitly estimating masks [14, 15].\nMany generic architectures have been proposed for sound sep-\naration, including recurrent networks [16], convolutional networks\n[9], U-nets [14], attention networks [17], and their combinations.\nThese address the arbitrary permutation of output sources using a\npermutation-invariant loss during training [7, 8].\nSome methods have gone further to address permutation invari-\nance at the architecture level by producing un-ordered representa-\ntions corresponding to each source. Deep clustering and deep at-\ntractor networks [6, 18, 7], employ a permutation-equivariant archi-\ntecture, which operates in an attention-like way over embeddings of\neach time-frequency bin.\nOur approach produces an embedding for each source, using a\nslot-based attention mechanism, and decodes it to directly estimate\nthe source spectrogram, using a NeRF-like [19] method. This differs\nfrom mask-based methods in using direct prediction, and the method\nof direct prediction using NeRF architecture is novel. The slot-based\nattention mechanism is different from previous attention networks\nfor sound separation, and is more closely related to deep clustering.\nHowever our attention method works on higher level spectrogram\nregions rather than individual time-frequency bins, and uses general-\npurpose attention mechanisms instead of simple af\ufb01nity-based meth-\nods.\nRecent unsupervised approaches, such as mixture invariant\ntraining (MixIT) [20], us only audio mixtures for training. While\nwe only explore supervised sound separation using ground-truth iso-\nlated reference sources in our work, a training setup like in MixIT\narXiv:2305.05591v1  [cs.SD]  9 May 2023\nCNN\nTransformer\nSpatial\nBroadcast\nDecoder\nEncoder\nDecoder\nMatching\nPre-processing\n...\nLorem ipsum\nFig. 1: Architecture overview. The input waveform is \ufb01rst cropped and transformed into a spectrogram. Then the neural network encodes the\nspectrogram to a set of permutation invariant source embeddings s1...n, these embeddings are decoded to generate a set of individual source\nspectrograms. The whole pipeline is supervised with the groundtruth source spectrograms using a matching based permutation invariant loss\nfunction.\nis orthogonal to our approach and would be interesting to explore in\nfuture work.\nSlot-centric neural networks. Neural networks that operate on\nunordered sets of features have been studied for some time [21, 22,\n23]. Most closely related to our work are approaches that gener-\nate an unordered set of outputs conditioned on some input data [24,\n25, 1] and methods that use a set of latent variables (\u201cslots\u201d) to\nmodel permutation-invariant aspects of the input [2, 3], such as ob-\njects in visual scenes. We refer to the latter as slot-centric neural\nnetworks. In the context of vision, this class of models forms the\nbasis for many modern scene understanding approaches, including\nobject-detection [1], panoptic segmentation [26], and unsupervised\nobject discovery [2, 3, 27]. In our work, we demonstrate that slot-\ncentric generative models similarly hold promise for compositional,\npermutation-invariant tasks in audio processing, speci\ufb01cally for sep-\narating individual audio sources from a mixture.\n3. METHOD\nWe present a generalized permutation invariant training framework\nfor supervised sound separation. Unlike previous methods, we ap-\nproach the source-separation task from a generative perspective. The\nmain objective of our method is to project the input audio into a set of\nembeddings each representing a different source in the input. These\nembeddings are then used to generate the magnitude spectrograms\nof individual sources in a permutation invariant manner. The whole\npipeline is supervised with the ground-truth source spectrograms us-\ning a permutation-invariant loss. In rest of this section we elaborate\non different steps in our training pipeline.\nPreprocessing: Given a mixture waveform during training we\n\ufb01rst randomly crop a 0.5-second audio clip. Then following [28]\nwe transform the clip into a spectrogram using a short-time Fourier\ntransform with window size 512 and hop size 125. Then the abso-\nlute values of the complex spectrograms are non-linearly scaled by\nexponentiating them by power of 0.3 to emphasize low values. These\nscaled absolute values are passed as the input to the next step.\nEncoding: To infer source embeddings, the input spectrogram\nis \ufb01rst encoded using a ResNet-34 network [29] to a 32 \u00d7 8 grid of\nencoded \u201cimage\u201d features z. We use a reduced stride in the ResNet\nroot block to retain a higher spatial resolution. Next a transformer\nwith 4 layers maps z to source embeddings s1...n where n is the\nnumber of sources and si \u2208 Rd. Unlike the original formulation [4],\nin each transformer layer we \ufb01rst perform a self-attention operation\nbetween query vectors q and then perform cross-attention between\nthe outputs of the self-attention step and z. We use same variables\nas both the key and value in self-attention and cross-attention steps.\nThe initial queries q of dimensionality 4096 are learned via back-\npropagation, similar to DETR [1].\nDecoding: We use a spatial broadcast decoder [30] to generate\nindividual source spectrograms from s1...n. First each embedding\nsi are copied across a 2D grid to create tensor gi of shape F \u00d7\nT \u00d7 d where F, T are the frequency bins and timesteps of the output\nspectrogram. Then positional embeddings with fourier features [31]\nare appended to gi making its shape F \u00d7 T \u00d7 (d + e) where e is\nthe size of each fourier feature embeddings. Subsequently a fully\nconnected network with shared parameters is applied across all the\nvectors in gi to arrive at a set of spectrograms each with shape F \u00d7\nT. Note that this decoder is similar to a NeRF model [19], which\nsimilarly takes positional codes as input and learns a fully-connected\nnetwork to produce coordinate-dependent outputs. In our case, we\ndirectly produce spectrogram values, arranged on a 2D image grid,\nas outputs and further condition the network on the respective latent\nsource embeddding si for each generated spectrogram.\nObjective: The predicted spectrograms are generated in no pre-\ndetermined ordering as the source separation problem is permuta-\ntion invariant. Therefore we need to match estimated spectrograms\nwith ground-truth spectrograms to calculate the loss of the network.\nAmong all the possible matches between ground-truth and estimated\nspectrograms we seek to \ufb01nd the optimal assignment with minimum\nreconstruction error. We suggest to use the Hungarian matching al-\ngorithm to solve this assignment problem because of its speed, ac-\ncuracy and ability to handle large numbers of items ef\ufb01ciently (al-\nthough we only use datasets with a small number of sources in our\nexperiments). It works by assigning costs or weights to each possible\npair and then selects pairs which minimize the total cost or weight as-\nsociated with them. Finally the mean squared error between matched\nground-truth and estimated spectrograms is minimized as the train-\ning objective to optimize the network parameters.\nSource Separation: During testing, given an input mixture\nwaveform we \ufb01rst break it into multiple non-overlapping waveforms\nof length 0.5-seconds. We add zero-padding at the end in case the\ninput waveform length is not evenly divisible into chunks of 0.5s.\nThese waveforms are preprocessed as mentioned above and are\npassed as inputs to the network, trained using the above pipeline to\nestimate absolute values of the spectrograms of individual sources.\nThe estimated spectrograms are \ufb01rst rescaled by exponentiating\nthem by power of 1/0.3 to invert the scaling done during preprocess-\ning. These rescaled estimates are used to calculate the masks in the\noracle method to create complex spectrogram estimates of individ-\nual sources from the input. Given input spectrogram I the output\nsource spectrograms are computed as mi \u2217 I where mi is the mask\ncorresponding to the ith source estimated using the spectrograms\nTable 1:\nResults.\nWe use the estimated absolute spectrograms\nas masks over the input complex spectrogram to produce complex\nspectrograms of the individual sources [33]. In the table below we\npresent SI-SNR[dB] and SI-SNRi[dB] values with IBM and Wiener-\nlike masking. Higher is better. Notice that there is only a minimal\ndrop in performance between Autoencoder and AudioSlots showing\nthat our pipeline is able to learn to separate speech well.\nSI-SNR\nSI-SNRi\nSI-SNR\nSI-SNRi\nMasking Type\nIBM\nIBM\nWiener\nWiener\nOracle\n12.07\n12.07\n12.32\n12.33\nAutoencoder\n10.19\n10.20\n10.15\n10.15\nAudioSlots\n09.50\n09.50\n09.96\n09.97\nOracle (1-sec)\n13.15\n13.16\n13.46\n13.47\nAudioSlots (1-sec)\n09.66\n09.67\n10.20\n10.20\npredicted by the neural network. These complex spectrograms are\ninverted to waveforms using an inverse short-time Fourier transform\n(STFT) and then stitched together, resolving matching using the best\nmatch with the ground-truth signal for simplicity.\nTraining: We train using Adam [32] for 300k steps with a batch\nsize of 64, a learning rate of 2e-4, 2500 warmup steps and a cosine\ndecay schedule.\n4. EXPERIMENTS\nWe evaluate the performance of our method on speech separation\nusing the Libri2Mix [5] dataset. We use the anechoic version of\nthe dataset. Each instance in the dataset is sampled at 16kHz and\n10 seconds long. Libri2Mix contains contains utterances from both\nmale and female speakers drawn from LibriSpeech [34]. The train-\n360-clean split of the dataset contains 364 hours of mixtures and the\nsources are drawn without replacement.\nAs mentioned above we use the masking to estimate the com-\nplex spectrograms of the individual sources using the input spectro-\ngram and the network predictions. There are various masking func-\ntions that can be used [33]. In our experiments we use the ideal\nbinary mask (IBM) [35] and Wiener \ufb01lter like mask as mask func-\ntions which are de\ufb01ned as:\nIBM: mi =\n(\n1\ni = argmax(m)\n0\notherwise\n\u201cWiener like\u201d: mi =\n(mi)2\n\u03a3n\ni=1(mi)2\nMetrics: We measure the separation performance using scale-\ninvariant signal-to-noise ratio (SI-SNR) [36] and SI-SNR improve-\nment (SI-SNRi). Let y denote the target and \u02c6y denote the estimate\nobtained by our method. Then SI-SNR measures the \ufb01delity between\ny and \u02c6y within an arbitrary scale by rescaling the target:\nSI-SNR(y, \u02c6y) = 10 log10\n||\u03b1y||2\n||\u03b1y \u2212 \u02c6y||2\nwhere \u03b1 = argmina||ay \u2212 \u02c6y||2 = yT \u02c6y/||y||2. The SI-SNRi is the\ndifference between the SI-SNR of each source estimate after pro-\ncessing and the SI-SNR obtained using the input mixture as the es-\ntimate for each source. During evaluation we \ufb01rst match the targets\nand estimates to maximize SI-SNR and then average the resulting\nSI-SNR and SI-SNRi scores.\nResults: In Table 1 we compare our performance with an au-\ntoencoder variant of our method (which receives ground-truth refer-\nence sources as input) and the performance of separation obtained\nusing the (preprocessed) ground-truth signals. The metrics com-\nputed using the ground-truth signals represent the maximum val-\nues that can be obtained with our (lossy) preprocessing. In the au-\ntoencoder variant we train the network to reconstruct the individual\nsource signals with n = 1. We then use the individual estimates\nas the masks over the complex spectrogram of the mixture. Since\nthe spectrograms contain high-frequency features, this would help\nus understand the ability of our architecture to faithfully represent\nthese features. We also present an ablation by increasing the crop\nlength in the preprocessing step to 1 second.\nThe difference in performance between the autoencoder variant\nand the separation model is only 0.18\u00b10.01. This indicates that our\nmethod, AudioSlots, is able learn speech separation well, closely\napproaching the performance of the baseline which receives fully-\nseparated sources as input.\nStill, there is substantial headroom for improvement, both in\nterms of our model as well as our overall pipeline: prior masking-\nbased approaches [11, 12, 13] already solve Libri2Mix speaker sep-\naration to an impressive degree, achieving signi\ufb01cantly higher SI-\nSNR values than we report here. Very recently, diffusion-based ap-\nproaches have also shown competitive performance [37]. This gap\nis in part due to our lossy preprocessing pipeline: for example, com-\nputing STFTs on pre-chunked audio (done here for simplicity) intro-\nduces border artifacts which even reduces our ground-truth SI-SNR\nscores below what other models can achieve. We further zero-pad\nall audio signals to the same length for simplicity. Also, since we\ncreate masks from the generated spectrograms, we are also bound\nby the limitations of mask-based methods, e.g. that masked spectral\ncontent cannot be regenerated.\nLimitations: Our experimental comparison highlights the main\nlimitation of our method, which is reconstruction \ufb01delity: both the\nautoencoder baseline and our AudioSlots model encode the signal\nusing a latent bottleneck representation and tend to discard certain\nhigh-frequency details (among others).\nThis is also qualitatively\nvisible in Figure 2. The comparison further shows that the crop\nlength affects the separation performance: while this can be par-\ntially explained by boundary artifacts due to our simplistic cropping\napproach, it also hints towards sensitivity of AudioSlot\u2019s ability to\nboth separate and reconstruct audio spectrograms on chunk length.\nDiscussion: Our results show promise for addressing audio sep-\naration using slot-centric generative models that represent the audio\nusing a set of source-speci\ufb01c latent variables. This is a signi\ufb01cant\ndeparture from earlier methods that directly operate on the input\naudio using e.g. a masking-based approach [20]. Learning source-\nspeci\ufb01c latent variables further has the bene\ufb01t that these decomposed\nlatent variables can likely be used not just for generation, but also for\nrecognition tasks, similar to how slots in object-centric computer vi-\nsion models serve as a basis for object detection and segmentation.\nWe are optimistic that the current limitations of our approach\ncan be overcome in future work:\n\u2022 To address the issue of reconstruction \ufb01delity (blurry reconstruc-\ntions for high-frequency features), it is likely that moving away\nfrom a deterministic feedforward decoder to e.g. an autoregressive\ndecoding approach, as in AudioLM [38], or an iterative diffusion-\nbased decoder, as in [39], can bridge the gap to high-\ufb01delty gen-\neration.\n\u2022 At present, AudioSlots assumes supervision in the form of ground-\ntruth sources. An extension to fully-unsupervised training on raw,\nInput\nGroundtruth\nAutoencoder\nReconstruction\nOurs\nReconstruction\n#1\n#2\nFig. 2: Comparison between absolute value of the individual source spectrograms of Groundtruth, Autoencoder estimates and AudioSlots\n(Ours) estimates. The input spectrogram (top) is a mixture and rest of the rows show the spectrograms of the individual sources. The input\nand groundtruth spectrograms are preprocessed using the steps mentioned in Sec 3. Notice that our method is able to reconstruct harmonics\nfairly well, however struggles with estimating the high-frequency features (see highlighted example regions).\nmixed audio would be desirable. To this end, we explored replac-\ning the Transformer in AudioSlots with a Slot Attention [3] mod-\nule which has an inductive bias towards decomposition that allows\nit to be trained unsupervised in the context of visual scene decom-\nposition. In initial experiments we found, however, that this induc-\ntive bias might not suf\ufb01ce for decomposing audio spectrograms in\na fully-unsupervised fashion. A supervised version of AudioSlots\nwith a Slot Attention module, however, performed similar to the\nversion with a Transformer module in initial experiments, high-\nlighting that further exploration is still promising for future work.\n\u2022 We think that the limitation of processing individual chunks in\nisolation, which requires post-hoc stitching, can be overcome by\nusing a sequential extension of the model, where slots of the past\ntime step are used as initialization for the next time step as in Slot\nAttention for Video [40]. We leave this for future work.\n5. CONCLUSION\nWe present AudioSlots, a slot-centric generative architecture for\naudio spectrograms. We demonstrate a proof of concept that Au-\ndioSlots holds promise for addressing the task of audio source\nseparation using structured generative models. While our current\nimplementation of AudioSlots has several limitations, including low\nreconstruction \ufb01delity for high-frequency features and requiring\nseparated audio sources as supervision, we are optimistic that these\ncan be overcome and outline several possible directions for future\nwork.\n6. REFERENCES\n[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko, \u201cEnd-to-\nend object detection with transformers,\u201d in ECCV, 2020.\n[2] Klaus Greff, Rapha\u00a8el Lopez Kaufman, Rishabh Kabra, Nick\nWatters, Christopher Burgess, Daniel Zoran, Loic Matthey,\nMatthew Botvinick, and Alexander Lerchner,\n\u201cMulti-object\nrepresentation learning with iterative variational inference,\u201d in\nICML, 2019.\n[3] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner,\nAravindh Mahendran,\nGeorg Heigold,\nJakob Uszkoreit,\nAlexey Dosovitskiy, and Thomas Kipf, \u201cObject-centric learn-\ning with slot attention,\u201d NeurIPS, 2020.\n[4] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin, \u201cAttention is all you need,\u201d NeurIPS, 2017.\n[5] Joris Cosentino, Manuel Pariente, Samuele Cornell, Antoine\nDeleforge, and Emmanuel Vincent,\n\u201cLibrimix: An open-\nsource dataset for generalizable speech separation,\u201d\narXiv\npreprint arXiv:2005.11262, 2020.\n[6] John R Hershey, Zhuo Chen, Jonathan Le Roux, and Shinji\nWatanabe, \u201cDeep clustering: Discriminative embeddings for\nsegmentation and separation,\u201d in IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP),\n2016.\n[7] Yusuf Isik, Jonathan Le Roux, Zhuo Chen, Shinji Watanabe,\nand John R Hershey, \u201cSingle-channel multi-speaker separa-\ntion using deep clustering,\u201d arXiv preprint arXiv:1607.02173,\n2016.\n[8] Morten Kolb\u00e6k, Dong Yu, Zheng-Hua Tan, and Jesper Jensen,\n\u201cMultitalker speech separation with utterance-level permuta-\ntion invariant training of deep recurrent neural networks,\u201d\nIEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 2017.\n[9] Yi Luo and Nima Mesgarani, \u201cConv-tasnet: Surpassing ideal\ntime\u2013frequency magnitude masking for speech separation,\u201d\nIEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 2019.\n[10] Ethan Manilow, Gordon Wichern, Prem Seetharaman, and\nJonathan Le Roux,\n\u201cCutting music source separation some\nSlakh: A dataset to study the impact of training data quality\nand quantity,\u201d in Proc. IEEE Workshop on Applications of Sig-\nnal Processing to Audio and Acoustics (WASPAA), 2019.\n[11] Yi Luo, Zhuo Chen, and Takuya Yoshioka,\n\u201cDual-path\nrnn: ef\ufb01cient long sequence modeling for time-domain single-\nchannel speech separation,\u201d in IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), 2020.\n[12] Cem Subakan, Mirco Ravanelli, Samuele Cornell, Mirko\nBronzi, and Jianyuan Zhong,\n\u201cAttention is all you need in\nspeech separation,\u201d\nin IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 2021.\n[13] Kai Li, Runxuan Yang, and Xiaolin Hu, \u201cAn ef\ufb01cient encoder-\ndecoder architecture with top-down attention for speech sepa-\nration,\u201d arXiv preprint arXiv:2209.15200, 2022.\n[14] Marco Tagliasacchi, Yunpeng Li, Karolis Misiunas, and Do-\nminik Roblek, \u201cSEANet: A multi-modal speech enhancement\nnetwork,\u201d in Proc. Interspeech, 2020.\n[15] Zhong-Qiu Wang, Samuele Cornell, Shukjae Choi, Younglo\nLee, Byeong-Yeol Kim, and Shinji Watanabe,\n\u201cTf-gridnet:\nMaking time-frequency domain models great again for monau-\nral speaker separation,\u201d\narXiv preprint arXiv:2209.03952,\n2022.\n[16] Felix Weninger, John R Hershey, Jonathan Le Roux, and Bj\u00a8orn\nSchuller, \u201cDiscriminatively trained recurrent neural networks\nfor single-channel speech separation,\u201d in 2014 IEEE global\nconference on signal and information processing (GlobalSIP).\nIEEE, 2014, pp. 577\u2013581.\n[17] Yuma Koizumi, Shigeki Karita, Scott Wisdom, Hakan Erdo-\ngan, John R Hershey, Llion Jones, and Michiel Bacchiani,\n\u201cDf-conformer: Integrated architecture of conv-tasnet and con-\nformer using linear complexity self-attention for speech en-\nhancement,\u201d in 2021 IEEE Workshop on Applications of Signal\nProcessing to Audio and Acoustics (WASPAA). IEEE, 2021, pp.\n161\u2013165.\n[18] Zhuo Chen, Yi Luo, and Nima Mesgarani, \u201cDeep attractor net-\nwork for single-microphone speaker separation,\u201d in IEEE In-\nternational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP). IEEE, 2017.\n[19] Ben Mildenhall,\nPratul P Srinivasan,\nMatthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng, \u201cNerf:\nRepresenting scenes as neural radiance \ufb01elds for view synthe-\nsis,\u201d in ECCV, 2020.\n[20] Scott Wisdom, Efthymios Tzinis, Hakan Erdogan, Ron Weiss,\nKevin Wilson, and John Hershey, \u201cUnsupervised sound sepa-\nration using mixture invariant training,\u201d NeurIPS, 2020.\n[21] Oriol Vinyals, Samy Bengio, and Manjunath Kudlur,\n\u201cOr-\nder matters: Sequence to sequence for sets,\u201d arXiv preprint\narXiv:1511.06391, 2015.\n[22] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas\nPoczos, Russ R Salakhutdinov, and Alexander J Smola, \u201cDeep\nsets,\u201d NeurIPS, 2017.\n[23] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se-\nungjin Choi, and Yee Whye Teh, \u201cSet transformer: A frame-\nwork for attention-based permutation-invariant neural net-\nworks,\u201d in ICML, 2019.\n[24] Yan Zhang, Jonathon Hare, and Adam Prugel-Bennett, \u201cDeep\nset prediction networks,\u201d NeurIPS, 2019.\n[25] Adam R Kosiorek, Hyunjik Kim, and Danilo J Rezende, \u201cCon-\nditional set generation with transformers,\u201d\narXiv preprint\narXiv:2006.16841, 2020.\n[26] Yi Zhou, Hui Zhang, Hana Lee, Shuyang Sun, Pingjun Li,\nYangguang Zhu, ByungIn Yoo, Xiaojuan Qi, and Jae-Joon\nHan,\n\u201cSlot-vps: Object-centric representation learning for\nvideo panoptic segmentation,\u201d in CVPR, 2022.\n[27] Pradyumna Reddy, Paul Guerrero, and Niloy J Mitra, \u201cSearch\nfor concepts: Discovering visual concepts using direct opti-\nmization,\u201d arXiv preprint arXiv:2210.14808, 2022.\n[28] Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wo-\njciech Galuba,\nFlorian Metze,\nChristoph Feichtenhofer,\net al.,\n\u201cMasked autoencoders that listen,\u201d\narXiv preprint\narXiv:2207.06405, 2022.\n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun,\n\u201cDeep residual learning for image recognition,\u201d\nin CVPR,\n2016.\n[30] Nicholas Watters, Loic Matthey, Christopher P Burgess, and\nAlexander Lerchner, \u201cSpatial broadcast decoder: A simple ar-\nchitecture for learning disentangled representations in vaes,\u201d\narXiv preprint arXiv:1901.07017, 2019.\n[31] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan Barron, and Ren Ng, \u201cFourier features\nlet networks learn high frequency functions in low dimensional\ndomains,\u201d NeurIPS, 2020.\n[32] Diederik P Kingma and Jimmy Ba,\n\u201cAdam: A method for\nstochastic optimization,\u201d\narXiv preprint arXiv:1412.6980,\n2014.\n[33] Hakan Erdogan, John R. Hershey, Shinji Watanabe, and\nJonathan Le Roux, \u201cPhase-sensitive and recognition-boosted\nspeech separation using deep recurrent neural networks,\u201d in\nIEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2015.\n[34] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev\nKhudanpur,\n\u201cLibrispeech: an asr corpus based on public\ndomain audio books,\u201d in IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 2015.\n[35] Yipeng Li and DeLiang Wang,\n\u201cOn the optimality of ideal\nbinary time\u2013frequency masks,\u201d Speech Communication, 2009.\n[36] Jonathan Le Roux, Scott Wisdom, Hakan Erdogan, and John R\nHershey, \u201cSdr\u2013half-baked or well done?,\u201d in IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2019.\n[37] Shahar Lutati, Eliya Nachmani, and Lior Wolf,\n\u201cSeparate\nand diffuse: Using a pretrained diffusion model for improving\nsource separation,\u201d arXiv preprint arXiv:2301.10752, 2023.\n[38] Zal\u00b4an Borsos, Rapha\u00a8el Marinier, Damien Vincent, Eugene\nKharitonov, Olivier Pietquin, Matt Shari\ufb01, Olivier Teboul,\nDavid Grangier, Marco Tagliasacchi, and Neil Zeghidour, \u201cAu-\ndiolm: a language modeling approach to audio generation,\u201d\narXiv preprint arXiv:2209.03143, 2022.\n[39] Curtis Hawthorne, Ian Simon, Adam Roberts, Neil Zeghi-\ndour, Josh Gardner, Ethan Manilow, and Jesse Engel, \u201cMulti-\ninstrument music synthesis with spectrogram diffusion,\u201d arXiv\npreprint arXiv:2206.05408, 2022.\n[40] Thomas Kipf, Gamaleldin F. Elsayed, Aravindh Mahen-\ndran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jon-\nschkowski, Alexey Dosovitskiy, and Klaus Greff, \u201cConditional\nObject-Centric Learning from Video,\u201d in ICLR, 2022.\n"
  },
  {
    "title": "TidyBot: Personalized Robot Assistance with Large Language Models",
    "link": "https://arxiv.org/pdf/2305.05658.pdf",
    "upvote": "2",
    "text": "TidyBot: Personalized Robot Assistance with\nLarge Language Models\nJimmy Wu1*, Rika Antonova2, Adam Kan3, Marion Lepert2, Andy Zeng4,\nShuran Song5, Jeannette Bohg2, Szymon Rusinkiewicz1, Thomas Funkhouser1,4\n1*Princeton University, Princeton, NJ, USA.\n2Stanford University, Stanford, CA, USA.\n3The Nueva School, San Mateo, CA, USA.\n4Google, Mountain View, CA, USA.\n5Columbia University, New York, NY, USA.\n*Corresponding author(s). E-mail(s): jw60@cs.princeton.edu;\nContributing authors: rika.antonova@stanford.edu; adakan@nuevaschool.org;\nlepertm@stanford.edu; andyzeng@google.com; shurans@cs.columbia.edu;\nbohg@stanford.edu; smr@princeton.edu; funk@cs.princeton.edu;\nAbstract\nFor a robot to personalize physical assistance effectively, it must learn user preferences that can\nbe generally reapplied to future scenarios. In this work, we investigate personalization of household\ncleanup with robots that can tidy up rooms by picking up objects and putting them away. A key\nchallenge is determining the proper place to put each object, as people\u2019s preferences can vary greatly\ndepending on personal taste or cultural background. For instance, one person may prefer storing\nshirts in the drawer, while another may prefer them on the shelf. We aim to build systems that\ncan learn such preferences from just a handful of examples via prior interactions with a particular\nperson. We show that robots can combine language-based planning and perception with the few-shot\nsummarization capabilities of large language models (LLMs) to infer generalized user preferences that\nare broadly applicable to future interactions. This approach enables fast adaptation and achieves\n91.2% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in\nreal-world test scenarios.\nKeywords: service robotics, mobile manipulation, large language models\n1 Introduction\nBuilding a robot that provides personalized assis-\ntance for physical household tasks is a long-\nstanding goal of robotics research. In this paper,\nwe investigate the task of tidying up a room:\nmoving every object on the floor to its \u201cproper\nplace.\u201d One of the challenges in performing this\ntask is determining the correct receptacle (\u201cproper\nplace\u201d) for every object. This is difficult because\nwhere objects should go is highly personal, and\ndepends on cultural norms and individual pref-\nerences. One person may want to put shirts in\na dresser drawer, another may want them on\n1\narXiv:2305.05658v2  [cs.RO]  11 Oct 2023\nshelves, and a third may want them hanging in a\ncloset. There is no \u201cone size fits all\u201d solution.\nClassical approaches to the household cleanup\ntask ask a person to specify a target loca-\ntion for every object (Rasch et al., 2019; Yan\net al., 2021), which is tedious and imprac-\ntical in an autonomous setting. Other works\nlearn generic (non-personalized) rules about where\nobjects typically go inside a house by averag-\ning over many users (Taniguchi et al., 2021;\nKant et al., 2022; Sarch et al., 2022). Works\nthat focus on personalization aim to extrapolate\nfrom a few user examples given similar choices\nmade by other users, using methods such as\ncollaborative filtering (Abdo et al., 2015), spa-\ntial relationships (Kang et al., 2018), or learned\nlatent preference vectors (Kapelyukh and Johns,\n2022). However, all of these approaches require\ncollecting large datasets with user preferences or\ngenerating datasets from manually constructed,\nsimulated scenarios. Such datasets can be expen-\nsive to acquire and may not generalize well if they\nare too small.\nOur approach is to utilize the summarization\ncapabilities of large language models (LLMs) to\nprovide generalization from a small number of\nexample preferences. We ask a person to provide\na few example object placements using textual\ninput (e.g., yellow shirts go in the drawer, dark\npurple shirts go in the closet, white socks go in\nthe drawer), and then we ask the LLM to summa-\nrize these examples (e.g., light-colored clothes go\nin the drawer and dark-colored clothes go in the\ncloset) to arrive at generalized preferences for this\nparticular person.\nThe underlying insight is that the summa-\nrization capabilities of LLMs are a good match\nfor the generalization requirements of personalized\nrobotics. LLMs demonstrate astonishing abilities\nto perform generalization through summarization,\ndrawing upon complex object properties and rela-\ntionships learned from massive text datasets. By\nusing the summarization provided by LLMs for\ngeneralization in robotics, we hope to produce\ngeneralized rules from a small number of exam-\nples, in a form that is human interpretable (text)\nand is expressed in nouns that can be grounded\nin images using open-vocabulary image classifiers.\nUsing an off-the-shelf LLM also avoids expen-\nsive collection of user preference data and model\ntraining.\nFig. 1 We study the task of household cleanup, where each\nobject on the floor must be picked up and put away while\nfollowing user preferences.\nWe investigate the proposed approach in a\nreal-world robotic mobile manipulation system\nfor household cleanup, which we call TidyBot\n(Fig. 1). Before the robot begins cleanup, we ask\nthe user to provide a handful of example place-\nments for specific objects, which are passed to an\nLLM to be summarized into a generalized set of\nrules (personalized to that user) mapping object\ncategories to receptacles. The nouns of these gen-\neralized rules are provided to an open-vocabulary\nimage classifier in order to identify objects on the\nfloor and determine target receptacles for them\nusing the rules. The robot will then carry out\nthe cleanup task by repeatedly picking up objects,\nidentifying them, and moving them to their target\nreceptacles.\nWe evaluate our approach quantitatively on\nboth a text-based benchmark dataset and our real-\nworld robotic system. On the benchmark, we find\nthat our approach generalizes well, achieving an\naccuracy of 91.2% on unseen objects across all sce-\nnarios in the benchmark. In our real-world test\nscenarios, we find that TidyBot correctly puts\naway 85.0% of objects. We also show that our\napproach can be easily extended to infer gener-\nalized rules for manipulation primitive selection\n(e.g., pick and place vs. pick and toss) in addition\nto inferring object placements.\nOur contributions are: (i) the idea that text\nsummarization with LLMs provides a means for\ngeneralization in robotics, (ii) a publicly released\nbenchmark dataset for evaluating generalization\n2\nof receptacle selection preferences, and (iii) imple-\nmentation and evaluation of our approach on a\nreal-world mobile manipulation system.\nThis journal paper is an extended version of a\npreviously published conference paper (Wu et al.,\n2023). The new material in this journal version\nincludes:\n1. A user study that evaluates whether humans\nprefer the preferences learned by our approach,\nand whether human responses align with our\nbenchmark\u2019s ground truth\n2. Quantitative analysis of the perception com-\nponent of the real-world system, including\ncomparisons of different visual language models\n3. Additional statistics of our benchmark show-\ning representation of different sorting criteria in\nthe dataset, along with a breakdown of baseline\nresults according to these criteria\n4. A summary of the limitations of our system\nPlease see our project page at https://tidybot.\ncs.princeton.edu\nfor\nadditional\nsupplementary\nmaterial, benchmark dataset and code, and quali-\ntative videos of our real-world system TidyBot in\naction.\n2 Related Work\nHousehold\ncleanup. Many recent works in\nEmbodied AI have proposed benchmarks or meth-\nods for completing household tasks in simulated\nindoor environments (Kolve et al., 2017; Puig\net al., 2018; Shridhar et al., 2020, 2021; Szot et al.,\n2021; Li et al., 2022; Srivastava et al., 2022; Li\net al., 2022). For household cleanup in particu-\nlar, the object rearrangement task (Puig et al.,\n2018; Batra et al., 2020; Szot et al., 2021; Ehsani\net al., 2021; Weihs et al., 2021; Gan et al., 2022)\nrequires an embodied agent to pick up and move\nobjects so as to bring the environment into a\nspecified state. Household cleanup has also been\nstudied in robotics works, in which instructions\nfor object rearrangement are specified via pointing\ngestures (Rasch et al., 2019) or target layouts (Yan\net al., 2021). The drawback of these setups is\nthat a target location must be manually speci-\nfied for every object to be manipulated, which\ncan require significant human effort. Prior works\nhave addressed this challenge by automatically\ninferring object placements based on human pref-\nerences for where objects typically go inside a\nhouse (Taniguchi et al., 2021; Kant et al., 2022;\nSarch et al., 2022), eliminating the need to specify\nwhere every individual object goes. However, these\nworks predict human preferences that are generic\nrather than personalized. To handle the variability\nin preferences across different users, other works\nhave used collaborative filtering (Abdo et al.,\n2015), spatial relationships (Kang et al., 2018), or\nlearned latent preference vectors (Kapelyukh and\nJohns, 2022) to predict object placements that\nare based on personalized user preferences. These\nmethods require the collection of large crowd-\nsourced datasets for human preferences, which can\nbe expensive. By contrast, our approach uses off-\nthe-shelf LLMs with no additional training or data\ncollection. We are able to directly leverage the\ncommonsense knowledge and summarization abil-\nities of LLMs to build generalizable personalized\npreferences for each user.\nObject sorting. Object sorting has been stud-\nied in robotics using approaches such as cluster-\ning (Gupta and Sukhatme, 2012), active learn-\ning (Kujala et al., 2016; Herde et al., 2018),\nmetric learning (Zeng et al., 2022), or heuristic\nsearch (Huang et al., 2019; Song et al., 2020; Pan\nand Hauser, 2021). These setups carry out pre-\nspecified sorting rules using physical properties\nsuch as color (Szabo and Lie, 2012; Gupta and\nSukhatme, 2012; Kujala et al., 2016; Herde et al.,\n2018; Huang et al., 2019; Dewi et al., 2020; Song\net al., 2020; Pan and Hauser, 2021), shape (Herde\net al., 2018), size (Gupta and Sukhatme, 2012;\nHerde et al., 2018; Dewi et al., 2020), or mate-\nrial (Lukka et al., 2014). Notably, they are not\nable to sort based on semantics or commonsense\nknowledge, nor are they able to automatically\ninfer sorting rules. More recently, H\u00f8eg and Tin-\ngelstad (2022) studied whether classification of\nobjects into general high-level categories can be\nimproved by using an LLM to take in an object\ndetector\u2019s prediction and output a general cat-\negory for the object. In our work, we similarly\ntap into the commonsense knowledge of LLMs\nto reason about object sorting. However, whereas\ntheir setup uses pre-specified sorting rules based\non a fixed set of categories, ours is able to infer\ngeneralizable sorting rules automatically.\n3\nLLMs for robotics. Large language models\n(LLMs) have been shown to exhibit remarkable\ncommonsense reasoning abilities (Brown et al.,\n2020; Nye et al., 2021; Rytting and Wingate,\n2021; Wei et al., 2022a,b; Kojima et al., 2022;\nMadaan et al., 2022). As a result, there has been\nincreasing interest in harnessing the capabilities\nof LLMs to build more commonsense knowl-\nedge into robotic systems. Many recent works\nstudy how LLM-generated high-level robotic plans\n(typically produced using the few-shot learning\nparadigm (Brown et al., 2020)) can be grounded\nin the state of the environment. This can be done\nwith value functions (Brohan et al., 2022; Lin\net al., 2023), semantic translation into admissi-\nble actions (Huang et al., 2022), scene descrip-\ntion as context (Zeng et al., 2022; Mees et al.,\n2022; Chen et al., 2022; Singh et al., 2022),\nfeedback (Huang et al., 2022; Yao et al., 2022),\nor re-prompting (Raman et al., 2022). However,\nthese works assume a setup in which the LLM is\nexpected to output a single generic plan. This is\nnot a good fit for personalized household cleanup,\nbecause a \u201cone size fits all\u201d plan would not\naddress the wide variability in user preferences.\nInstead, our system generates personalized plans\nthat are tailored to the preferences of a par-\nticular user. Other works in robotics have used\nLLMs for PDDL planning (Silver et al., 2022),\ncode generation for robotic control policies (Liang\net al., 2022), parsing navigation instructions into\ntextual landmarks (Shah et al., 2022), room clas-\nsification (Chen et al., 2022), and tool manipula-\ntion (Ren et al., 2022). These works all use LLMs\nas a means of integrating commonsense knowledge\ninto robotic systems, which is also true in our case.\nHowever, unlike these works, we additionally show\nthat the summarization ability of LLMs enables\ngeneralization in robotics.\n3 Method\nWe use the summarization capabilities of an off-\nthe-shelf LLM to generalize user preferences from\na small number of examples. Below, we describe\nhow we use the LLM to infer personalized rules\nfor both receptacle selection and manipulation\nprimitive selection, and also how we deploy the\napproach on a real-world mobile manipulation\nsystem for household cleanup.\n3.1 Personalized receptacle selection\nOur system first receives a few examples of object\nplacements reflecting the personal preferences of a\nuser. For instance, the user may specify that yel-\nlow shirts and white socks go in the drawer, while\ndark purple shirts and black shirts go in the closet.\nWe provide these examples to an LLM, which then\ninfers personalized rules on where objects belong.\nSpecifically, the LLM (i) summarizes the examples\ninto general rules, and then (ii) uses the summary\nto determine where to place new objects.\nFollowing recent work (Zeng et al., 2022; Singh\net al., 2022), we convert the user examples into\nLLM prompts that are structured as Pythonic\ncode. This prompt form is advantageous because\nLLMs are trained on large amounts of code, and\nit also provides a structured output that is easy\nto parse. To represent the user examples, the\nprompt first contains a list of objects present in\nthe scene and a list of potential receptacles (see\nAppendix A for full prompt with in-context exam-\nples). This is followed by a series of pick and place\ncommands reflecting where the objects should be\nplaced according to the user. Then, we ask the\nLLM to complete the last line, which is a code\ncomment summarizing what the preceding code\nblock does. Here is an example LLM completion\nwhere the output from the LLM is highlighted:\nobjects = [\"yellow shirt\", \"dark purple shirt\",\n\"white socks\", \"black shirt\"]\nreceptacles = [\"drawer\", \"closet\"]\npick and place(\"yellow shirt\", \"drawer\")\npick and place(\"dark purple shirt\", \"closet\")\npick and place(\"white socks\", \"drawer\")\npick and place(\"black shirt\", \"closet\")\n# Summary: Put light-colored clothes in the\ndrawer and dark-colored clothes in the closet.\nIn this example, the LLM summarized the pro-\nvided object placements and inferred that light-\ncolored clothes go in the drawer while dark-colored\nclothes go in the closet. These examples lead\nto a generalized rule for where objects belong,\npersonalized to this particular user.\nNext, the summary is used by the LLM to\ngenerate placements for novel, unseen objects.\nThe prompt consists of the summary from the\nLLM summarization step (in the form of a code\ncomment), a list of the unseen objects, a list of\nreceptacles, and a partial pick and place command\n4\nfor the first object. We then ask the LLM to pro-\nvide a placement for each object by completing\nthe prompt:\n#\nSummary:\nPut\nlight-colored\nclothes\nin\nthe\ndrawer and dark-colored clothes in the closet.\nobjects = [\"black socks\", \"white shirt\", \"navy\nsocks\", \"beige shirt\"]\nreceptacles = [\"drawer\", \"closet\"]\npick and place(\"black socks\", \"closet\")\npick and place(\"white shirt\", \"drawer\")\npick and place(\"navy socks\", \"closet\")\npick and place(\"beige shirt\", \"drawer\")\nThe output pick and place commands can then\nbe parsed to determine where each unseen object\nshould be placed.\n3.2 Personalized primitive selection\nSimilar to the way we infer generalized rules for\nreceptacle selection, we can also infer generalized\nrules for how to manipulate objects, again leverag-\ning the summarization capabilities of LLMs. First,\nwe provide a few examples of objects along with\ntheir user-preferred manipulation primitive to the\nLLM, and ask it to summarize. Here is an exam-\nple completion where the output from the LLM is\nhighlighted:\nobjects = [\"yellow shirt\", \"dark purple shirt\",\n\"white socks\", \"black shirt\"]\npick and place(\"yellow shirt\")\npick and place(\"dark purple shirt\")\npick and toss(\"white socks\")\npick and place(\"black shirt\")\n# Summary: Pick and place shirts, pick and\ntoss socks.\nThe summary can then be used as a general-\nized rule to predict the appropriate primitive to\nuse for unseen objects:\n# Summary: Pick and place shirts, pick and toss\nsocks.\nobjects = [\"black socks\", \"white shirt\", \"navy\nsocks\", \"beige shirt\"]\npick and toss(\"black socks\")\npick and place(\"white shirt\")\npick and toss(\"navy socks\")\npick and place(\"beige shirt\")\n3.3 Real-world robotic system\nGiven generalized rules from LLM summarization,\nwe can now implement these rules on a robot\ntasked with tidying up a household environment.\nTo do so, we use a perception system to local-\nize and recognize objects in the environment, and\na predetermined set of manipulation primitives\nto move objects into receptacles. For our setup,\nwe use pick and place and pick and toss as our\nprimitives, as they are well-suited for household\ncleanup. However, other sets of primitives could\nalso be used.\nFor each new user, the system will receive a\nset of example preferences and run the previously\ndescribed LLM pipeline to get personalized rules\nfor the user. The rules contain a set of general-\nized object categories produced by summarization\n(e.g., light-colored clothes, dark-colored clothes),\neach of which is matched to a preferred recepta-\ncle and manipulation primitive for that category.\nThe robot will tidy up the environment by itera-\ntively performing the following steps until no more\nobjects remain on the floor: (1) localize the near-\nest object, (2) classify the object into a generalized\ncategory, (3) determine the appropriate receptacle\nand manipulation primitive for the object using\ngeneralized rules produced by the LLM, and (4)\nuse the manipulation primitive to put the object\ninto the receptacle. Fig. 2 provides a conceptual\nillustration of this procedure, and Algorithm 1\noutlines these steps in pseudocode.\nAlgorithm 1 System pipeline\nInput: Ereceptacle = {(o1, r1), (o2, r2), . . .}\nInput: Eprimitive = {(o1, p1), (o2, p2), . . .}\nSreceptacle = LLM.Summarize(Ereceptacle)\nSprimitive = LLM.Summarize(Eprimitive)\nC = LLM.GetCategories(Sreceptacle)\nrobot.Initialize()\nwhile True do\nItop = GetOverheadImage()\no = ViLD.GetClosestObject(Itop)\nrobot.MoveTo(o)\nIego = robot.GetEgocentricImage()\nc = CLIP.GetCategory(Iego, C)\nr = LLM.GetReceptacle(Sreceptacle, c)\np = LLM.GetPrimitive(Sprimitive, c)\nrobot.PickUp(o)\nrobot.MoveTo(r)\nrobot.ExecutePrimitive(p)\nend while\n5\nViLD\nOverhead image\nClosest object\nCLIP\nEgocentric image\nCategory: can\nLLM\nReceptacle: recycling bin\nPrimitive: toss\nImage classification\nObject detection\nObject placement and primitive selection\nFig. 2 System overview. Once the user\u2019s preferences have been summarized with an LLM, TidyBot will localize the closest\nobject on the floor, move to get a close-up view with its egocentric camera, predict the object\u2019s category using CLIP, use the\nLLM-summarized rules to select a receptacle and manipulation primitive, and then execute the primitive to put the object into\nthe selected receptacle, repeating this entire process until no more objects can be found on the floor.\nOne important aspect of our approach is that\nthe LLM summarization automatically provides\ncandidate categories to the perception system.\nNouns (or noun phrases) are extracted from the\nsummarization text as categories, and used as the\ntarget label set for CLIP (Radford et al., 2021),\nthe open-vocabulary image classification model we\nuse. For example, the following LLM prompt will\nextract the two general categories in the sum-\nmary text (light-colored clothing and dark-colored\nclothing):\n#\nSummary:\nPut\nlight-colored\nclothes\nin\nthe\ndrawer and dark-colored clothes in the closet.\nobjects = [\"light-colored clothing\",\n\"dark-colored clothing\"]\nThis combination of summarization and open-\nvocabulary classification is critical to the auton-\nomy of the system, as it enables the object classi-\nfier to work with a small set of generalized object\ncategories. The approach is (i) robust as there are\nonly a small number of categories to differentiate\nbetween, and (ii) flexible because it supports arbi-\ntrary sets of object categories for different users. In\ncontrast, without LLM summarization, the object\nclassifier would have to be able to recognize all\npossible fine-grained object classes, which is much\nmore difficult. Alternatively, the user would have\nto manually specify the list of objects present in\neach target scene, which would be impractical for\nan autonomous system.\n4 Experiments\nWe investigate the performance of our proposed\napproach with two types of evaluation. For the\nfirst type of evaluation, we design a benchmark for\ngeneralization of receptacle selection using text-\nbased examples, which enables direct comparison\nto alternative approaches and ablation studies,\nwith quantitative metrics. For the second type\nof evaluation, we deploy our approach in a real-\nworld mobile manipulation system for tidying up a\nroom based on user preferences. Unless otherwise\nspecified, the LLM we use is text-davinci-003, a\nvariant of GPT-3 (Brown et al., 2020). All LLM\nexperiments were run with temperature 0.\n4.1 Benchmark dataset\nIn order to evaluate the proposed approach and to\nquantitatively compare it to alternatives, we cre-\nated a benchmark dataset of object placements.\nThe benchmark is comprised of 96 scenarios, each\nof which has a set of objects, a set of recepta-\ncles, a set of example \u201cseen\u201d object placements\n(preferences), and a set of \u201cunseen\u201d evaluation\nplacements, all specified as text. The task is to\npredict the placements in the \u201cunseen\u201d set given\nthe examples in the \u201cseen\u201d set.\nThe benchmark scenarios are defined in 4 room\ntypes (living room, bedroom, kitchen, pantry\nroom), with 24 scenarios per room type. Each sce-\nnario contains 2\u20135 receptacles (potential places\nto put objects, such as shelves, cabinets, etc.),\n4\u201310 \u201cseen\u201d example object placements provided\nas input to the task, and an equal number of\n\u201cunseen\u201d object placements (distinct from the\nseen examples) provided for evaluation. There are\n2 seen and 2 unseen object placements per recep-\ntacle. In total, there are 672 seen and 672 unseen\nobject placements, which cumulatively reference\n87 unique receptacles and 1,076 unique objects.\n6\nTable 1 Representation of sorting criteria in benchmark\nCategory Attribute Function Subcategory Multiple\n86/96\n27/96\n24/96\n31/96\n17/96\nSuccess on this benchmark is measured by the\nobject placement accuracy: the number of objects\nplaced in the correct receptacle divided by the\ntotal number of objects. We evaluate accuracy sep-\narately for seen and unseen objects, to tease apart\nmemorization versus generalization. For each, we\ncompute the accuracy per scenario, and then aver-\nage the results across all scenarios to produce the\nnumbers shown in the tables.\nSince different people may sort items in the\nhome in many different ways, our benchmark con-\ntains a diversity of preferences with several kinds\nof sorting criteria represented in the dataset:\n\u2022 Category: Sort objects based on general cate-\ngories (e.g., put clothes here and toys there)\n\u2022 Attribute:\nSort\nobjects\nbased\non\nobject\nattributes (e.g., put plastic items here and metal\nitems there)\n\u2022 Function: Sort objects based on function (e.g.,\nput winter clothes here and summer clothes\nthere)\n\u2022 Subcategory: Sort objects such that a specific\n(subordinate) subcategory is separated from\nthe general (superordinate) category (e.g., put\nshirts on the sofa and other clothes in the closet)\n\u2022 Multiple categories: Sort objects from multi-\nple categories into one receptacle (e.g., put both\nbooks and toys on the shelf)\nWe show in Tab. 1 the representation of dif-\nferent sorting criteria in our benchmark dataset,\nindicated by the fraction of the 96 scenarios to\nwhich each criteria applies. Note that multiple\nsorting criteria may apply to a single scenario.\n4.2 Baseline comparisons\nIn our first set of experiments, we use the bench-\nmark to provide quantitative evaluation of our\napproach compared to several alternatives. The\nresults are in Tab. 2. We also show in Tab. 3 the\nsame results but broken down by the sorting crite-\nria described in Sec. 4.1. Since the main challenge\nis to generalize from objects in the examples (seen)\nto those in the evaluation set (unseen), we consider\nTable 2 Comparisons to baselines\nMethod\nAccuracy (unseen)\nExamples only\n78.5%\nWordNet taxonomy\n67.5%\nRoBERTa embeddings\n77.8%\nCLIP embeddings\n83.7%\nSummarization (ours)\n91.2%\na variety of baseline generalization approaches and\nreport placement accuracy metrics only for unseen\nobjects.\nThe following paragraphs describe each base-\nline and provide a discussion of how the perfor-\nmance compares to that of our proposed approach.\nExamples only. The first baseline provides a\ndirect comparison to a system like ours if it did\nnot use summarization. The LLM is given a list\nof objects, receptacles, and example placement\npreferences, along with a list of unseen objects\nfor a new scene. Then, the LLM is asked to\ndirectly infer the proper placements (highlighted\ntext) for unseen objects in the new scene, without\nsummarization as an intermediate step:\nobjects = [\"yellow shirt\", \"dark purple shirt\",\n\"white socks\", \"black shirt\"]\nreceptacles = [\"drawer\", \"closet\"]\npick and place(\"yellow shirt\", \"drawer\")\npick and place(\"dark purple shirt\", \"closet\")\npick and place(\"white socks\", \"drawer\")\npick and place(\"black shirt\", \"closet\")\nobjects = [\"black socks\", \"white shirt\", \"navy\nsocks\", \"beige shirt\"]\nreceptacles = [\"drawer\", \"closet\"]\npick and place(\"black socks\", \"drawer\")\npick and place(\"white shirt\", \"closet\")\npick and place(\"navy socks\", \"drawer\")\npick and place(\"beige shirt\", \"closet\")\nThe prediction accuracy of this method for\nunseen objects (78.5%) is significantly worse than\nthat of our method (91.2%). Since the main dif-\nference between this method versus ours is that\nour method leverages summarization, this result\npresents strong evidence for our main hypothesis\n\u2014 i.e., summarization is useful for generalization.\nThis finding is also consistent with recent work\nshowing that LLMs perform better when they are\nasked to output intermediate steps of reasoning\nbefore the final answer (Nye et al., 2021; Wei\n7\nTable 3 Comparisons to baselines by sorting criteria\nMethod\nCategory\nAttribute\nFunction\nSubcategory\nMultiple\nExamples only\n80.1%\n72.7%\n75.7%\n77.0%\n81.5%\nWordNet taxonomy\n69.1%\n59.8%\n61.4%\n71.3%\n74.1%\nRoBERTa embeddings\n78.6%\n75.5%\n71.8%\n71.7%\n87.5%\nCLIP embeddings\n84.6%\n79.8%\n85.5%\n84.7%\n87.9%\nSummarization (ours)\n91.0%\n85.6%\n93.9%\n90.1%\n93.5%\net al., 2022a). When looking at the predictions, we\nfind that this baseline approach generally predicts\nobject placements that are sensible but may not\nbe consistent with the user\u2019s preferences.\nWordNet taxonomy. This baseline uses a hand-\ncrafted lexical ontology called WordNet (Miller,\n1995) to generalize placements from seen to\nunseen objects. For each unseen object, we place\nit in the same receptacle as the most similar seen\nobject, where similarity is measured using the\nshortest path between two objects in the taxon-\nomy. Since WordNet is a hand-crafted taxonomy,\nit does not contain all possible object names. For\nthe 694 objects in our benchmark that are miss-\ning from WordNet, we manually mapped each of\nthem to the closest WordNet object name. Even\nwith the manual mapping, the performance of\nthis WordNet baseline for unseen objects (67.5%)\nis far worse than that of our method (91.2%).\nThis shows that LLM summarization provides\nbetter generalization than using the hierarchy pro-\nvided by a hand-crafted ontology. When looking\nat the breakdown in Tab. 3, we see that this\nbaseline performs worse on the two criteria that\nare not related to object categorization (attribute\nand function). We hypothesize that WordNet is\nnot able to generalize well along these dimen-\nsions because it was constructed mainly based on\nsemantic relationships between categories.\nText embedding. This baseline uses pretrained\ntext embeddings to assist with generalization. For\neach unseen object, we place it in the receptacle\nprovided for the most similar seen object, where\nsimilarity is defined by cosine similarity between\nencoded object names in the RoBERTa (Liu et al.,\n2019) or CLIP (Radford et al., 2021) embed-\nding space. For RoBERTa, we use the pretrained\nSentence-BERT (Reimers and Gurevych, 2019)\nmodel from the SentenceTransformers library.\nSpecifically, we use the all-distilroberta-v1\nTable 4 Ablation studies\nMethod\nSeen\nUnseen\nCommonsense\n45.0%\n45.6%\nSummarization\n91.8%\n91.2%\nHuman summary\n97.1%\n97.5%\nvariant which is a distilled (Sanh et al., 2019) ver-\nsion of the RoBERTa (Liu et al., 2019) model\nthat is fine-tuned on a dataset of 1 billion sen-\ntence pairs. For CLIP, we use the pretrained model\nprovided by OpenAI. In either case, the general-\nization performance for predicting placements of\nunseen objects does not reach the performance\nof our proposed summarization approach (77.8%\nfor RoBERTa and 83.7% for CLIP, versus 91.2%\nfor ours). Although text embeddings trained on\nlarge datasets encode many types of object simi-\nlarities, particularly for related object categories,\nthey may not encode the object attributes rele-\nvant to the preferences of a particular user (e.g.,\nlight objects go here, heavy object go there). In\ncontrast, our summarization approach is able to\ncorrectly encode a larger variety of user prefer-\nences.\n4.3 Ablation studies\nIn the second set of experiments, we use the\nbenchmark to evaluate the performance of sev-\neral variants to our method. The goal of these\nexperiments is to compare its performance to\nalternatives with far less information (using only\ncommon sense, without preferences) or far more\ninformation (using human-generated summariza-\ntions). We also study the impact of using different\nLLMs. The benchmark metrics for both seen and\nunseen objects are provided in Tabs. 4 and 5.\nCommonsense. Our first ablation study mea-\nsures how well an LLM can perform the bench-\nmark tasks using only commonsense reasoning \u2014\n8\nTable 5 Comparison of different LLMs\nModel\nCommonsense\nSummarization\nseen\nunseen\nseen\nunseen\ntext-davinci-003\n45.0%\n45.6%\n91.8%\n91.2%\ntext-davinci-002\n41.8%\n37.5%\n84.1%\n75.7%\ncode-davinci-002\n41.4%\n39.4%\n88.6%\n83.2%\nPaLM 540B\n45.5%\n49.6%\n84.6%\n75.7%\ni.e., without using the preferences at all. For each\nbenchmark scene, we give the LLM the list of\nobjects and list of receptacles, and then ask it\nto generate object placements (highlighted text)\nwithout using the provided user preferences:\n# Put objects into their appropriate receptacles.\nobjects = [\"black socks\", \"white shirt\", \"navy\nsocks\", \"beige shirt\"]\nreceptacles = [\"drawer\", \"closet\"]\npick and place(\"black socks\", \"drawer\")\npick and place(\"white shirt\", \"closet\")\npick and place(\"navy socks\", \"drawer\")\npick and place(\"beige shirt\", \"closet\")\nThis baseline performs poorly, even for seen\nobjects (45.0%), due to the high variability of\nobject placement preferences in the benchmark.\nThe predicted object placements are sensible but\nare not reflective of the particular user\u2019s prefer-\nences. In contrast, our method can learn pref-\nerences from examples via summarization and\nperforms much better for both seen and unseen\nobjects (91.8% and 91.2%).\nHuman summary. This ablation studies how\nthe summaries provided by the LLM compare\nto summaries crafted manually by a human. For\neach benchmark scenario, a human-written sum-\nmary was used by the LLM (in place of the\nLLM-produced summary) to predict object place-\nments for the test objects. The results achieved\nwith this \u201coracle\u201d summarization are better than\nthe LLM summarization by 6% for both seen\nand unseen objects. This result suggests that the\nLLM summarizations are already quite good, and\nthat improvements to LLM summarization could\nenable further gains for our method in the future.\nDifferent\nLLMs. Table 5 reports our per-\nformance\non\nthe\nbenchmark\nusing\ndifferent\nLLMs.\nWe\nfind\nthat\ntext-davinci-002\nand\ncode-davinci-002 (Chen et al., 2021), which are\nolder variants of GPT-3, are not as good as\nFig. 3 Example user study question. This screen-\nshot shows an example survey question from our user study.\nOn the left are preferences, on the right are two placement\noptions corresponding to the two methods being compared.\nThe participant is asked to select the option that is best\naligned with the given preferences.\nthe newest one (text-davinci-003). In particu-\nlar, there is a much larger gap between seen and\nunseen objects. This is because the older mod-\nels are more likely to generate summaries that\nlist out individual objects in the seen set, which\ndoes not generalize well to the unseen objects.\nFor PaLM 540B (Chowdhery et al., 2022), we find\nthat while it shows slightly higher performance\non commonsense reasoning, it does not do as well\nas text-davinci-003 on summarization, particu-\nlarly in scenarios where there is a larger number\nof receptacles to choose from.\n4.4 Human evaluation\nTo evaluate whether humans prefer the prefer-\nences learned by our method, we conduct a user\nstudy based on the scenarios in our benchmark\ndataset. The study asks participants to compare\nthe object placements generated by our method to\nthose of CLIP embeddings, which is the strongest\nbaseline. The study has 2 objectives:\n1. Evaluate whether humans prefer the object\nplacements generated by our LLM summariza-\ntion method over those of the CLIP embed-\ndings baseline\n2. Evaluate\nwhether\nhuman-preferred\nobject\nplacements align with the ground truth place-\nments in our benchmark\n9\nTable 6 User study results by sorting criteria\nMethod\nCategory\nAttribute\nFunction\nSubcategory\nMultiple\nOverall\nCLIP embeddings\n19.7%\n23.7%\n11.2%\n22.6%\n21.2%\n19.1%\nSummarization (ours)\n47.4%\n41.9%\n60.0%\n46.1%\n40.6%\n46.9%\nEqually preferred\n32.9%\n34.4%\n28.8%\n31.3%\n38.2%\n34.1%\nStudy setup. We recruited 40 participants (24\nmales and 16 females) consisting of affiliates from\nauthor institutions and asked them to fill out an\nonline survey. Each participant was assigned 24\nscenarios randomly selected from the 96 scenarios\nin the benchmark. Each scenario in the bench-\nmark is evaluated by 10 participants, giving 960\nevaluations in total.\nFor each scenario, we provide (i) example\nplacements of \u201cseen\u201d objects indicating user pref-\nerences, and (ii) placements of \u201cunseen\u201d objects\nfrom both our LLM summarization method and\nthe CLIP embeddings method (example shown in\nFig. 3). The participant is then asked to specify\nwhich of the two object placement options better\naligns with the given preferences, or if they are\nequally preferable. For the convenience of the par-\nticipants, we highlight the object placements that\ndiffer between the two methods. We randomize the\norder of scenarios as well as the order of meth-\nods for each scenario (the participant is unaware\nof which option goes with which method). For\nsome of the scenarios, both methods give the exact\nsame object placements, so we preselect the third\n\u201cequally preferred\u201d option and exclude them from\nthe surveys given to participants.\nResults. Our results across all 960 evaluations are\nshown in Tab. 6. Overall, we find that our LLM\nsummarization method is preferred over the CLIP\nembeddings baseline 46.9% of the time, whereas\nthe baseline is preferred 19.1% of the time, and\nboth methods are equally preferred 34.1% of the\ntime. When considering the results broken down\nby sorting criteria, we find that our method per-\nforms particularly well relative to the baseline\nfor the function criteria (e.g., formal vs. casual\nclothes). Even though the corresponding bench-\nmark accuracy is relatively high (CLIP embed-\ndings in Tab. 3), the baseline method usually\nsorts by object category (as described in Sec. 4.2)\nwhich can lead to egregiously wrong placements\n(e.g., store dress pants with sweatpants) when the\nintended sorting criteria is function.\nWe ran a statistical analysis with the following\nnull hypothesis (H0): There is no significant differ-\nence between the preference for our method versus\nthe baseline method. In other words, the mean\nfraction of time participants prefer our method\nover the baseline is equal to 0.5. For each study\nparticipant, we calculated the fraction of time our\nmethod was preferred over the baseline method\nacross the 24 scenarios for that participant. For\nscenarios where both methods were equally pre-\nferred, we gave them both equal weight. We then\nconducted a paired t-test, and found a significant\ndifference between our method and the baseline\nmethod, with a calculated t-statistic of 9.93 (df\n= 39), p < 0.001, indicating strong evidence to\nreject the null hypothesis and suggesting that the\nobserved difference in human preference between\nour method and the baseline is unlikely to have\noccurred due to random chance.\nWe also evaluate how well the participant\nresponses align with the ground truth in our\nbenchmark. For each scenario, we identify which\nof the two methods is closer to the benchmark\nground truth based on unseen object placement\naccuracy on that scenario. We then calculate\nthe percent of human responses that prefer the\nmethod that is closer to the ground truth. Over-\nall, across the 40 participants, we find that human\nresponses were aligned with benchmark ground\ntruth 82.2% \u00b1 7.7% of the time, or 95.4% \u00b1 4.1%\nif \u201cequally preferred\u201d is treated as a wildcard.\n4.5 Real-world experiments\nIn our final set of experiments, we test the\nproposed approach on a robot performing a\ncleanup task in the real world (Fig. 1). The\nrobot base is a holonomic vehicle capable of\nany 3-degree-of-freedom motion on the ground\nplane. This maneuverability comes from the vehi-\ncle\u2019s Powered-Caster Drive System (Holmberg\nand Khatib, 2000), which consists of four caster\nwheels that are powered to roll and steer as\nneeded to achieve the desired vehicle motion. The\n10\nrobot manipulator is a Kinova Gen3 7-DoF arm\nmounted on top of the mobile base with a Robotiq\n2F-85 parallel jaw gripper as its end effector.\nThe robot is placed inside a room with various\nobjects and receptacles on the floor and is then\ntasked with picking up all the objects and putting\nthem into the correct receptacles according to user\npreferences. The preferences are provided as a set\nof textual examples for a particular user (as in the\nbenchmark). As described in Sec. 3.3 and illus-\ntrated in Fig. 2, the robot iteratively locates the\nclosest object on the floor, navigates to it, rec-\nognizes its category, picks it up, determines the\nappropriate receptacle for the object, navigates to\nthe receptacle, and then puts the object inside.\nImplementation. The robot uses two overhead\ncameras for 2D robot pose estimation (x, y, \u03b8)\nand 2D object localization (x, y). The pose of\nthe robot base is estimated using ArUco fiducial\nmarkers (Garrido-Jurado et al., 2014) mounted\non its top plate (see Fig. 1). The object loca-\ntions are detected in the overhead camera using\nViLD (Gu et al., 2021), while the receptacle loca-\ntions are hard-coded for each scenario. We found\nthat these design choices work well for our mobile\nrobot system. However, other pose trackers and\nobject detectors could also be used instead.\nTo navigate in the scene, the robot calcu-\nlates the shortest collision-free path to the target\nposition using an occupancy map that includes\nobstacles in the scene such as receptacles. It then\nuses the pure pursuit algorithm (Coulter, 1992) to\nfollow the computed path.\nAfter the robot arrives at the closest object, it\nuses a camera mounted on its base (and pointed\nforward at the ground) to take a close-up, cen-\ntered image of the object, then determines the\nobject category using cosine similarity between\ntext and image features in the CLIP embedding\nspace (Radford et al., 2021). The set of object\ncategories in the LLM summary is automatically\nextracted and used as the target label set for\nCLIP. Note that without these categories from\nLLM summarization, a human would have to man-\nually specify a list of fine-grained object classes\npotentially present in the target scene in order to\nuse CLIP for object classification.\nAfter the object category is identified, the sys-\ntem uses the LLM summarization to predict the\nappropriate receptacle and manipulation primitive\nfor the object. The robot then moves the object\ninto the receptacle with a sequence of two high-\nlevel manipulation primitives: (i) pick and (ii)\nplace or toss. The \u201cpick\u201d primitive uses the grip-\nper to grasp at the center of the detected object.\nThe \u201cplace\u201d primitive moves the gripper to a loca-\ntion just above the selected receptacle and drops\nthe grasped object in. The \u201ctoss\u201d primitive swings\nthe robot arm and releases the gripper with tim-\ning that results in tossing (Zeng et al., 2020) of\nthe grasped object into the selected receptacle.\nReal-world evaluation. Using this mobile robot\nsystem, we ran tests on 8 real-world scenarios as\nshown in Fig. 4, each with its own set of 10 objects,\n2\u20135 receptacles, 4\u201310 \u201cseen\u201d examples indicating\npreferences for which objects should go into which\nreceptacles and which primitive should be used\nto put them there, as well as 10 \u201cunseen\u201d test\nobjects. Across all 8 scenarios, 70 unique \u201cunseen\u201d\ntest objects (Fig. 5) and 11 unique receptacles\n(Fig. 6) are represented.\nFor each scenario, we asked the robot to per-\nform 3 runs of the cleanup task and measured\nits success throughout operation. Overall, the\nsystem was able to put 85.0% of the objects\ninto the correct receptacle during these tests.\nFor qualitative examples, please refer to the sup-\nplementary material and additional videos at\nhttps://tidybot.cs.princeton.edu.\nLooking at the results in more detail, there\nwere 240 objects to be cleaned up in total (8\nscenarios, 10 objects per scenario, 3 runs per sce-\nnario). We observed that the overhead camera\nwas able to localize 92.5% of the objects, and the\nobject classifier correctly identified the object cat-\negory for 95.5% of the localized objects. Given the\npredicted object category, the LLM selected the\nappropriate receptacle and manipulation primi-\ntive for 100% of localized objects. Additionally,\nthe robot succeeded in executing the chosen prim-\nitive for 96.2% of the localized objects. In terms\nof speed, the robot took on average 15\u201320 seconds\nto pick up and put away each object.\nVisual language model (VLM) evaluation.\nIn this section, we perform a quantitative compar-\nison of different visual language models (VLMs)\nwithin our real robot system. Recall that for each\nobject successfully localized by the overhead cam-\nera, the real robot will first use its egocentric\ncamera to take a close-up image of the object\n11\nFig. 4 Real-world scenarios. We evaluate our mobile manipulation system in 8 real-world scenarios, encompassing a wide\nvariety of objects and receptacles.\nFig. 5 Real-world objects. 70 unique \u201cunseen\u201d test\nobjects are represented in our real-world scenarios.\nFig. 6 Real-world receptacles. 11 unique receptacles\nare represented in our real-world scenarios.\nbefore picking it up. This image is given to a VLM\nto determine the category of the object. To con-\nduct our analysis, we save all egocentric images\nfrom our real world evaluation (222 in total across\nall test runs) and annotate them.\nTo evaluate a VLM, we run all 222 images\nthrough the model and determine the fraction of\nimages in which the centered foreground object is\ncorrectly recognized. We compare along two axes:\nTable 7 Comparison of different VLMs\nCLIP\nViLD\nOWL-ViT\nSummarized categories\n95.5%\n76.1%\n45.9%\nScenario object names\n70.7%\n59.9%\n24.8%\nAll object names\n52.3%\n36.5%\n18.5%\n(i) model type and (ii) vocabulary used for the\ntarget label set. The model types we consider (all\nopen-vocabulary) are (i) CLIP (Radford et al.,\n2021), which was the image classifier used in our\nfinal system, and two alternatives, (ii) ViLD (Gu\net al., 2021) and (iii) OWL-ViT (Minderer et al.,\n2022). The vocabulary options we consider are\n(i) the set of categories output by LLM sum-\nmarization (e.g., clothing, fruit, ...), which was\nused in our final system, (ii) a list of human-\nannotated names for all objects in the current\nscenario (e.g., blue jeans, apple, ...), and (iii) a\nlist of human-annotated object names across all\nscenarios (instead of just one scenario). Note that\nthe human-annotated options for the vocabulary\nare for analysis only, as it would be infeasible to\nask a human to annotate every object encoun-\ntered during robot operation. Results are shown\nin Tab. 7.\nLooking at the results comparing different\nVLMs (columns of Tab. 7), we find that CLIP\nperforms the best out of all the models. One\nreason is that CLIP will always output a pre-\ndiction, whereas the object detectors (ViLD and\nOWL-ViT) will sometimes detect no objects in\nthe image. Additionally, ViLD and OWL-ViT are\nderived from CLIP, and it is possible that the\n12\nprocess of adapting the models to localize bound-\ning boxes degrades their performance on object\nclassification.\nQualitatively, the main failure mode of CLIP is\nreporting the class of an object in the background\nrather than that of the foreground object. This is\nexpected since CLIP performs an image-wide clas-\nsification. We also observe that CLIP is often not\nable to consider noun phrases as complete units.\nFor example, the phrase \u201cwhite socks\u201d may match\nstrongly with anything that looks white.\nFor ViLD and OWL-ViT (both object detec-\ntors), we use the bounding box closest to the\ncenter of the image as the detection, since the\negocentric camera is pointed directly at the pick\nlocation on the floor. We expected that this\nlocalization would improve accuracy since fore-\nground objects can be isolated from background\nobjects (unlike with CLIP). However, we find that\nquantitatively, both ViLD and OWL-ViT perform\nworse than CLIP. Qualitatively, ViLD works well\nwith small rigid objects, but struggles with larger\ndeformable objects (such as clothes or stuffed\nanimals), outputting many extraneous detections\ncorresponding to parts of objects. Additionally, for\nboth ViLD and OWL-ViT, we find that the fore-\nground object is sometimes not detected at all,\neven though it is always prominently placed in the\ncenter of the image.\nWhen looking at results for different vocab-\nularies (rows of Tab. 7), we find that using the\ncategories from the LLM summary performs the\nbest. This is partly because the VLM has to differ-\nentiate between a much smaller number of options\n(2\u20135 categories vs. 10 or 65 object names). Note\nagain that the use of object names is not actually\nfeasible in a real system due to the human anno-\ntation burden. By contrast, our use of LLM sum-\nmarized categories allows the system to directly\ngeneralize to novel objects as the VLM only needs\nto correctly identify the closest category rather\nthan what the specific object is.\n4.6 Limitations\nLLM summarization. While LLMs are gen-\nerally able to summarize preferences well, we\nfind that there are still cases in which the gen-\nerated summary is not quite right. The most\ncommon failure mode is when the generated sum-\nmary simply lists out the seen objects rather\nthan summarizing into categories. Summaries of\nthat nature are too specific and do not general-\nize well to unseen objects. Another failure mode is\nwhen the LLM summarizes receptacles by group-\ning them together (e.g., top drawer and bottom\ndrawer might be summarized as drawers), result-\ning in poor performance when using the summary\nfor receptacle selection.\nReal-world system. Our implementation of the\nreal-world system contains simplifications such as\nthe use of hand-written manipulation primitives,\nuse of top-down grasps, and assumption of known\nreceptacle locations. These limitations could be\naddressed by incorporating more advanced primi-\ntives into our system and expanding the capabil-\nities of the perception system. Additionally, since\nthe mobile robots cannot drive over objects, the\nsystem would not work well in excessive clutter. It\nwould be interesting to incorporate more advanced\nhigh-level planning, so that instead of always pick-\ning up the closest object, the robot could reason\nabout whether it needs to first clear itself a path\nto move through the clutter.\n5 Conclusion\nIn this work, we showed that the summarization\ncapabilities of large language models (LLMs) can\nbe used to generalize user preferences for per-\nsonalized robotics. Given a handful of example\npreferences for a particular person, we use LLM\nsummarization to infer a generalized set of rules to\nmanipulate objects according to the user\u2019s prefer-\nences. We show that our summarization approach\noutperforms several strong baselines on our bench-\nmark, and we also evaluate our approach on a\nreal-world mobile manipulator called TidyBot,\nwhich can successfully clean up test scenarios with\na success rate of 85.0%. Our approach provides\na promising direction for developing personalized\nrobotic systems that can learn generalized user\npreferences quickly and effectively from only a\nsmall set of examples. Unlike classical approaches\nthat require costly data collection and model\ntraining, we show that LLMs can be directly used\noff-the-shelf to achieve generalization in robotics,\nleveraging the powerful summarization capabili-\nties they have learned from vast amounts of text\ndata.\n13\nAcknowledgments\nThe authors would like to thank William Chong,\nKevin Lin, and Jingyun Yang for fruitful techni-\ncal discussions, and Bob Holmberg for mentorship\nand support in building up the mobile platforms.\nThis work was supported in part by the Princeton\nSchool of Engineering, Toyota Research Insti-\ntute, and the National Science Foundation under\nCCF-2030859, DGE-1656466, and IIS-2132519.\nAppendix A\nLLM prompts\nThis section contains the full prompts used for all\nLLM text completion tasks. Each prompt consists\nof 1\u20133 in-context examples in gray followed by a\ntest example that we ask the LLM to complete.\nThe portion of the test example that is generated\nby the LLM is highlighted. We use the same in-\ncontext examples across all scenarios in both the\nbenchmark and the real-world system. For each\nscenario, only the final test example is modified.\nA.1\nSummarization for receptacle\nselection\nobjects\n=\n[\"dried\nfigs\",\n\"protein\nbar\",\n\"cornmeal\", \"Macadamia nuts\", \"vinegar\", \"herbal\ntea\",\n\"peanut\noil\",\n\"chocolate\nbar\",\n\"bread\ncrumbs\", \"Folgers instant coffee\"]\nreceptacles\n=\n[\"top\nrack\",\n\"middle\nrack\",\n\"table\", \"shelf\", \"plastic box\"]\npick and place(\"dried figs\", \"plastic box\")\npick and place(\"protein bar\", \"shelf\")\npick and place(\"cornmeal\", \"top rack\")\npick and place(\"Macadamia nuts\", \"plastic box\")\npick and place(\"vinegar\", \"middle rack\")\npick and place(\"herbal tea\", \"table\")\npick and place(\"peanut oil\", \"middle rack\")\npick and place(\"chocolate bar\", \"shelf\")\npick and place(\"bread crumbs\", \"top rack\")\npick and place(\"Folgers instant coffee\", \"table\")\n# Summary: Put dry ingredients on the top rack,\nliquid ingredients in the middle rack, tea and\ncoffee\non\nthe\ntable,\npackaged\nsnacks\non\nthe\nshelf, and dried fruits and nuts in the plastic\nbox.\nobjects = [\"yoga pants\", \"wool sweater\", \"black\njeans\", \"Nike shorts\"]\nreceptacles = [\"hamper\", \"bed\"]\npick and place(\"yoga pants\", \"hamper\")\npick and place(\"wool sweater\", \"bed\")\npick and place(\"black jeans\", \"bed\")\npick and place(\"Nike shorts\", \"hamper\")\n# Summary: Put athletic clothes in the hamper\nand other clothes on the bed.\nobjects = [\"Nike sweatpants\", \"sweater\", \"cargo\nshorts\",\n\"iPhone\",\n\"dictionary\",\n\"tablet\",\n\"Under Armour t-shirt\", \"physics homework\"]\nreceptacles\n=\n[\"backpack\",\n\"closet\",\n\"desk\",\n\"nightstand\"]\npick and place(\"Nike sweatpants\", \"backpack\")\npick and place(\"sweater\", \"closet\")\npick and place(\"cargo shorts\", \"closet\")\npick and place(\"iPhone\", \"nightstand\")\npick and place(\"dictionary\", \"desk\")\npick and place(\"tablet\", \"nightstand\")\npick and place(\"Under\nArmour\nt-shirt\",\n\"backpack\")\npick and place(\"physics homework\", \"desk\")\n# Summary: Put workout clothes in the backpack,\nother clothes in the closet, books and homeworks\non the desk, and electronics on the nightstand.\n14\nobjects = [\"jacket\", \"candy bar\", \"soda can\",\n\"Pepsi can\", \"jeans\", \"wooden block\", \"orange\",\n\"chips\", \"wooden block 2\", \"apple\"]\nreceptacles = [\"recycling bin\", \"plastic storage\nbox\", \"black storage box\", \"sofa\", \"drawer\"]\npick and place(\"jacket\", \"sofa\")\npick and place(\"candy\nbar\",\n\"plastic\nstorage\nbox\")\npick and place(\"soda can\", \"recycling bin\")\npick and place(\"Pepsi can\", \"recycling bin\")\npick and place(\"jeans\", \"sofa\")\npick and place(\"wooden block\", \"drawer\")\npick and place(\"orange\", \"black storage box\")\npick and place(\"chips\", \"plastic storage box\")\npick and place(\"wooden block 2\", \"drawer\")\npick and place(\"apple\", \"black storage box\")\n# Summary: Put clothes on the sofa, snacks in\nthe plastic storage box, cans in the recycling\nbin, wooden blocks in the drawer, and fruits\nin the black storage box.\nA.2\nReceptacle selection\n# Summary: Put clothes in the laundry basket and\ntoys in the storage box.\nobjects = [\"socks\", \"toy car\", \"shirt\", \"Lego\nbrick\"]\nreceptacles = [\"laundry basket\", \"storage box\"]\npick and place(\"socks\", \"laundry basket\")\npick and place(\"toy car\", \"storage box\")\npick and place(\"shirt\", \"laundry basket\")\npick and place(\"Lego brick\", \"storage box\")\n# Summary: Put clothes on the sofa, snacks in the\nplastic storage box, cans in the recycling bin,\nwooden blocks in the drawer, and fruits in the\nblack storage box.\nobjects = [\"jacket\", \"candy bar\", \"soda can\",\n\"Pepsi can\", \"jeans\", \"wooden block\", \"orange\",\n\"chips\", \"wooden block 2\", \"apple\"]\nreceptacles = [\"recycling bin\", \"plastic storage\nbox\", \"black storage box\", \"sofa\", \"drawer\"]\npick and place(\"jacket\", \"sofa\")\npick and place(\"candy bar\", \"plastic storage\nbox\")\npick and place(\"soda can\", \"recycling bin\")\npick and place(\"Pepsi can\", \"recycling bin\")\npick and place(\"jeans\", \"sofa\")\npick and place(\"wooden block\", \"drawer\")\npick and place(\"orange\", \"black storage box\")\npick and place(\"chips\", \"plastic storage box\")\npick and place(\"wooden block 2\", \"drawer\")\npick and place(\"apple\", \"black storage box\")\nA.3\nSummarization for primitive\nselection\nobjects = [\"granola bar\", \"hat\", \"toy car\", \"Lego\nbrick\", \"fruit snacks\", \"shirt\"]\npick and toss(\"granola bar\")\npick and place(\"hat\")\npick and place(\"toy car\")\npick and place(\"Lego brick\")\npick and toss(\"fruit snacks\")\npick and place(\"shirt\")\n# Summary: Pick and place clothes and toys, pick\nand toss snacks.\nobjects = [\"jacket\", \"candy bar\", \"soda can\",\n\"Pepsi can\", \"jeans\", \"wooden block\", \"orange\",\n\"chips\", \"wooden block 2\", \"apple\"]\npick and place(\"jacket\")\npick and toss(\"candy bar\")\npick and toss(\"soda can\")\npick and toss(\"Pepsi can\")\npick and place(\"jeans\")\npick and place(\"wooden block\")\npick and toss(\"orange\")\npick and toss(\"chips\")\npick and place(\"wooden block 2\")\npick and toss(\"apple\")\n# Summary: Pick and place clothes and wooden\nblocks, pick and toss snacks and drinks.\n15\nA.4\nPrimitive selection\n# Summary: Pick and place clothes, pick and toss\nsnacks.\nobjects = [\"granola bar\", \"hat\", \"toy car\", \"Lego\nbrick\", \"fruit snacks\", \"shirt\"]\npick and toss(\"granola bar\")\npick and place(\"hat\")\npick and place(\"toy car\")\npick and place(\"Lego brick\")\npick and toss(\"fruit snacks\")\npick and place(\"shirt\")\n# Summary: Pick and place granola bars, hats, toy\ncars, and Lego bricks, pick and toss fruit snacks\nand shirts.\nobjects = [\"clothing\", \"snack\"]\npick and place(\"clothing\")\npick and toss(\"snack\")\n# Summary: Pick and place clothes and wooden\nblocks, pick and toss snacks and drinks.\nobjects = [\"jacket\", \"candy bar\", \"soda can\",\n\"Pepsi can\", \"jeans\", \"wooden block\", \"orange\",\n\"chips\", \"wooden block 2\", \"apple\"]\npick and place(\"jacket\")\npick and place(\"jeans\")\npick and place(\"wooden block\")\npick and place(\"wooden block 2\")\npick and toss(\"candy bar\")\npick and toss(\"soda can\")\npick and toss(\"Pepsi can\")\npick and toss(\"orange\")\npick and toss(\"chips\")\npick and toss(\"apple\")\nA.5\nCategory extraction for\nreal-world system\n# Summary: Put shirts on the bed, jackets and\npants on the chair, and bags on the shelf.\nobjects = [\"shirt\", \"jacket or pants\", \"bag\"]\n# Summary: Put pillows on the sofa, clothes on\nthe chair, and shoes on the rack.\nobjects = [\"pillow\", \"clothing\", \"shoe\"]\n# Summary: Put clothes on the sofa, snacks in the\nplastic storage box, cans in the recycling bin,\nwooden blocks in the drawer, and fruits in the\nblack storage box.\nobjects = [\"clothing\", \"snack\", \"can\",\n\"wooden block\", \"fruit\"]\nA.6\nReceptacle selection for\nreal-world system\n# Summary: Put clothes in the laundry basket and\ntoys in the storage box.\nobjects = [\"socks\", \"toy car\", \"shirt\", \"Lego\nbrick\"]\nreceptacles = [\"laundry basket\", \"storage box\"]\npick and place(\"socks\", \"laundry basket\")\npick and place(\"toy car\", \"storage box\")\npick and place(\"shirt\", \"laundry basket\")\npick and place(\"Lego brick\", \"storage box\")\n# Summary: Put clothes on the sofa, snacks in the\nplastic storage box, cans in the recycling bin,\nwooden blocks in the drawer, and fruits in the\nblack storage box.\nobjects = [\"clothing\", \"snack\", \"can\", \"wooden\nblock\", \"fruit\"]\nreceptacles = [\"recycling bin\", \"plastic storage\nbox\", \"black storage box\", \"sofa\", \"drawer\"]\npick and place(\"clothing\", \"sofa\")\npick and place(\"snack\", \"plastic storage box\")\npick and place(\"can\", \"recycling bin\")\npick and place(\"wooden block\", \"drawer\")\npick and place(\"fruit\", \"black storage box\")\n16\nA.7\nPrimitive selection for\nreal-world system\n# Summary: Pick and place clothes, pick and toss\nsnacks.\nobjects = [\"granola bar\", \"hat\", \"toy car\", \"Lego\nbrick\", \"fruit snacks\", \"shirt\"]\npick and toss(\"granola bar\")\npick and place(\"hat\")\npick and place(\"toy car\")\npick and place(\"Lego brick\")\npick and toss(\"fruit snacks\")\npick and place(\"shirt\")\n# Summary: Pick and place granola bars, hats, toy\ncars, and Lego bricks, pick and toss fruit snacks\nand shirts.\nobjects = [\"clothing\", \"snack\"]\npick and place(\"clothing\")\npick and toss(\"snack\")\n# Summary: Pick and place clothes and wooden\nblocks, pick and toss snacks and drinks.\nobjects = [\"clothing\", \"snack\", \"can\", \"wooden\nblock\", \"fruit\"]\npick and place(\"clothing\")\npick and place(\"wooden block\")\npick and toss(\"snack\")\npick and toss(\"can\")\npick and toss(\"fruit\")\nReferences\nRasch, R., Sprute, D., P\u00a8ortner, A., Battermann,\nS., K\u00a8onig, M.: Tidy up my room: Multi-agent\ncooperation for service tasks in smart envi-\nronments. Journal of Ambient Intelligence and\nSmart Environments (2019)\nYan, Z., Crombez, N., Buisson, J., Ruichck, Y.,\nKrajnik, T., Sun, L.: A quantifiable stratifica-\ntion strategy for tidy-up in service robotics.\nIn: 2021 IEEE International Conference on\nAdvanced Robotics and Its Social Impacts\n(ARSO) (2021)\nTaniguchi, A., Isobe, S., El Hafi, L., Hagiwara, Y.,\nTaniguchi, T.: Autonomous planning based on\nspatial concepts to tidy up home environments\nwith service robots. Advanced Robotics (2021)\nKant, Y., Ramachandran, A., Yenamandra, S.,\nGilitschenski, I., Batra, D., Szot, A., Agrawal,\nH.:\nHousekeep:\nTidying\nvirtual\nhouseholds\nusing commonsense reasoning. arXiv preprint\narXiv:2205.10712 (2022)\nSarch, G., Fang, Z., Harley, A.W., Schydlo, P.,\nTarr, M.J., Gupta, S., Fragkiadaki, K.: Tidee:\nTidying up novel rooms using visuo-semantic\ncommonsense priors. In: European Conference\non Computer Vision (2022)\nAbdo, N., Stachniss, C., Spinello, L., Burgard,\nW.: Robot, organize my shelves! tidying up\nobjects by predicting user preferences. In: 2015\nIEEE International Conference on Robotics and\nAutomation (ICRA) (2015)\nKang, M., Kwon, Y., Yoon, S.-E.: Automated task\nplanning using object arrangement optimiza-\ntion. In: 2018 15th International Conference on\nUbiquitous Robots (UR) (2018). IEEE\nKapelyukh, I., Johns, E.: My house, my rules:\nLearning tidying preferences with graph neu-\nral networks. In: Conference on Robot Learning\n(2022)\nWu, J., Antonova, R., Kan, A., Lepert, M.,\nZeng, A., Song, S., Bohg, J., Rusinkiewicz, S.,\nFunkhouser, T.: Tidybot: Personalized robot\nassistance with large language models. In:\nIEEE/RSJ International Conference on Intelli-\ngent Robots and Systems (IROS) (2023)\nKolve, E., Mottaghi, R., Han, W., VanderBilt, E.,\nWeihs, L., Herrasti, A., Gordon, D., Zhu, Y.,\nGupta, A., Farhadi, A.: Ai2-thor: An interac-\ntive 3d environment for visual ai. arXiv preprint\narXiv:1712.05474 (2017)\nPuig, X., Ra, K., Boben, M., Li, J., Wang, T.,\nFidler, S., Torralba, A.: Virtualhome: Simulat-\ning household activities via programs. In: Pro-\nceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (2018)\nShridhar, M., Thomason, J., Gordon, D., Bisk,\nY., Han, W., Mottaghi, R., Zettlemoyer, L.,\nFox, D.: Alfred: A benchmark for interpret-\ning grounded instructions for everyday tasks.\nIn: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition\n17\n(2020)\nShridhar,\nM.,\nYuan,\nX.,\nC\u02c6ot\u00b4e,\nM.-A.,\nBisk,\nY., Trischler, A., Hausknecht, M.J.: Alfworld:\nAligning text and embodied environments for\ninteractive learning. In: ICLR (2021)\nSzot, A., Clegg, A., Undersander, E., Wijmans, E.,\nZhao, Y., Turner, J., Maestre, N., Mukadam,\nM., Chaplot, D.S., Maksymets, O., et al.: Habi-\ntat 2.0: Training home assistants to rearrange\ntheir habitat. Advances in Neural Information\nProcessing Systems (2021)\nLi, C., Xia, F., Mart\u00b4\u0131n-Mart\u00b4\u0131n, R., Lingelbach,\nM., Srivastava, S., Shen, B., Vainio, K.E., Gok-\nmen, C., Dharan, G., Jain, T., et al.: igibson\n2.0: Object-centric simulation for robot learning\nof everyday household tasks. In: Conference on\nRobot Learning (2022)\nSrivastava, S., Li, C., Lingelbach, M., Mart\u00b4\u0131n-\nMart\u00b4\u0131n, R., Xia, F., Vainio, K.E., Lian, Z.,\nGokmen, C., Buch, S., Liu, K., et al.: Behav-\nior: Benchmark for everyday household activi-\nties in virtual, interactive, and ecological envi-\nronments. In: Conference on Robot Learning\n(2022)\nLi, C., Zhang, R., Wong, J., Gokmen, C., Srivas-\ntava, S., Mart\u00b4\u0131n-Mart\u00b4\u0131n, R., Wang, C., Levine,\nG., Lingelbach, M., Sun, J., et al.: Behavior-1k:\nA benchmark for embodied ai with 1,000 every-\nday activities and realistic simulation. In: 6th\nAnnual Conference on Robot Learning (2022)\nBatra, D., Chang, A.X., Chernova, S., Davison,\nA.J., Deng, J., Koltun, V., Levine, S., Malik,\nJ., Mordatch, I., Mottaghi, R., et al.: Rear-\nrangement: A challenge for embodied ai. arXiv\npreprint arXiv:2011.01975 (2020)\nEhsani, K., Han, W., Herrasti, A., VanderBilt,\nE., Weihs, L., Kolve, E., Kembhavi, A., Mot-\ntaghi, R.: Manipulathor: A framework for visual\nobject manipulation. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision\nand Pattern Recognition (2021)\nWeihs, L., Deitke, M., Kembhavi, A., Mottaghi,\nR.: Visual room rearrangement. In: Proceed-\nings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (2021)\nGan,\nC.,\nZhou,\nS.,\nSchwartz,\nJ.,\nAlter,\nS.,\nBhandwaldar, A., Gutfreund, D., Yamins, D.L.,\nDiCarlo, J.J., McDermott, J., Torralba, A.,\net al.: The threedworld transport challenge:\nA visually guided task-and-motion planning\nbenchmark towards physically realistic embod-\nied ai. In: 2022 International Conference on\nRobotics and Automation (ICRA) (2022)\nGupta, M., Sukhatme, G.S.: Using manipulation\nprimitives for brick sorting in clutter. In: 2012\nIEEE International Conference on Robotics and\nAutomation (2012)\nKujala, J.V., Lukka, T.J., Holopainen, H.: Clas-\nsifying and sorting cluttered piles of unknown\nobjects with robots: a learning approach. In:\n2016 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS) (2016)\nHerde, M., Kottke, D., Calma, A., Bieshaar, M.,\nDeist, S., Sick, B.: Active sorting\u2013an efficient\ntraining of a sorting robot with active learning\ntechniques. In: 2018 International Joint Confer-\nence on Neural Networks (IJCNN) (2018)\nZeng, A., Song, S., Yu, K.-T., Donlon, E., Hogan,\nF.R., Bauza, M., Ma, D., Taylor, O., Liu, M.,\nRomo, E., et al.: Robotic pick-and-place of\nnovel objects in clutter with multi-affordance\ngrasping and cross-domain image matching.\nThe International Journal of Robotics Research\n(2022)\nHuang, E., Jia, Z., Mason, M.T.: Large-scale\nmulti-object rearrangement. In: 2019 Interna-\ntional Conference on Robotics and Automation\n(ICRA) (2019)\nSong, H., Haustein, J.A., Yuan, W., Hang, K.,\nWang, M.Y., Kragic, D., Stork, J.A.: Multi-\nobject rearrangement with monte carlo tree\nsearch: A case study on planar nonprehen-\nsile sorting. In: 2020 IEEE/RSJ International\nConference on Intelligent Robots and Systems\n(IROS) (2020)\nPan, Z., Hauser, K.: Decision making in joint\npush-grasp action space for large-scale object\nsorting. In: 2021 IEEE International Conference\n18\non Robotics and Automation (ICRA) (2021)\nSzabo, R., Lie, I.: Automated colored object sort-\ning application for robotic arms. In: 2012 10th\nInternational Symposium on Electronics and\nTelecommunications (2012)\nDewi, T., Risma, P., Oktarina, Y.: Fruit sorting\nrobot based on color and size for an agricultural\nproduct packaging system. Bulletin of Electrical\nEngineering and Informatics (2020)\nLukka, T.J., Tossavainen, T., Kujala, J.V., Raiko,\nT.: Zenrobotics recycler\u2013robotic sorting using\nmachine learning. In: Proceedings of the Inter-\nnational Conference on Sensor-Based Sorting\n(SBS) (2014)\nH\u00f8eg, S.H., Tingelstad, L.: More than eleven thou-\nsand words: Towards using language models\nfor robotic sorting of unseen objects into arbi-\ntrary categories. In: Workshop on Language and\nRobotics at CoRL 2022 (2022)\nBrown, T., Mann, B., Ryder, N., Subbiah, M.,\nKaplan, J.D., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A., et al.: Lan-\nguage models are few-shot learners. Advances in\nneural information processing systems (2020)\nNye,\nM.,\nAndreassen,\nA.J.,\nGur-Ari,\nG.,\nMichalewski, H., Austin, J., Bieber, D., Dohan,\nD., Lewkowycz, A., Bosma, M., Luan, D., et\nal.: Show your work: Scratchpads for intermedi-\nate computation with language models. arXiv\npreprint arXiv:2112.00114 (2021)\nRytting, C., Wingate, D.: Leveraging the induc-\ntive bias of large language models for abstract\ntextual reasoning. Advances in Neural Informa-\ntion Processing Systems (2021)\nWei, J., Wang, X., Schuurmans, D., Bosma,\nM., Chi, E., Le, Q., Zhou, D.: Chain of\nthought prompting elicits reasoning in large lan-\nguage models. arXiv preprint arXiv:2201.11903\n(2022)\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph,\nB., Borgeaud, S., Yogatama, D., Bosma, M.,\nZhou, D., Metzler, D., et al.: Emergent abil-\nities of large language models. arXiv preprint\narXiv:2206.07682 (2022)\nKojima, T., Gu, S.S., Reid, M., Matsuo, Y.,\nIwasawa, Y.: Large language models are zero-\nshot reasoners. arXiv preprint arXiv:2205.11916\n(2022)\nMadaan, A., Zhou, S., Alon, U., Yang, Y.,\nNeubig, G.: Language models of code are\nfew-shot commonsense learners. arXiv preprint\narXiv:2210.07128 (2022)\nBrohan, A., Chebotar, Y., Finn, C., Hausman, K.,\nHerzog, A., Ho, D., Ibarz, J., Irpan, A., Jang,\nE., Julian, R., et al.: Do as i can, not as i\nsay: Grounding language in robotic affordances.\nIn: 6th Annual Conference on Robot Learning\n(2022)\nLin, K., Agia, C., Migimatsu, T., Pavone, M.,\nBohg, J.: Text2motion: From natural language\ninstructions to feasible plans. arXiv preprint\narXiv:2303.12153 (2023)\nHuang, W., Abbeel, P., Pathak, D., Mordatch,\nI.: Language models as zero-shot planners:\nExtracting actionable knowledge for embodied\nagents. arXiv preprint arXiv:2201.07207 (2022)\nZeng, A., Wong, A., Welker, S., Choromanski, K.,\nTombari, F., Purohit, A., Ryoo, M., Sindhwani,\nV., Lee, J., Vanhoucke, V., et al.: Socratic mod-\nels: Composing zero-shot multimodal reasoning\nwith language. arXiv preprint arXiv:2204.00598\n(2022)\nMees, O., Borja-Diaz, J., Burgard, W.: Grounding\nlanguage with visual affordances over unstruc-\ntured data. arXiv preprint arXiv:2210.01911\n(2022)\nChen, B., Xia, F., Ichter, B., Rao, K., Gopalakr-\nishnan, K., Ryoo, M.S., Stone, A., Kappler,\nD.: Open-vocabulary queryable scene represen-\ntations for real world planning. arXiv preprint\narXiv:2209.09874 (2022)\nSingh, I., Blukis, V., Mousavian, A., Goyal, A.,\nXu, D., Tremblay, J., Fox, D., Thomason,\nJ., Garg, A.: Progprompt: Generating situated\nrobot task plans using large language models.\narXiv preprint arXiv:2209.11302 (2022)\n19\nHuang, W., Xia, F., Xiao, T., Chan, H., Liang,\nJ., Florence, P., Zeng, A., Tompson, J., Mor-\ndatch, I., Chebotar, Y., et al.: Inner monologue:\nEmbodied reasoning through planning with lan-\nguage models. arXiv preprint arXiv:2207.05608\n(2022)\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I.,\nNarasimhan, K., Cao, Y.: React: Synergizing\nreasoning and acting in language models. arXiv\npreprint arXiv:2210.03629 (2022)\nRaman, S.S., Cohen, V., Rosen, E., Idrees, I.,\nPaulius, D., Tellex, S.: Planning with large\nlanguage models via corrective re-prompting.\narXiv preprint arXiv:2211.09935 (2022)\nSilver, T., Hariprasad, V., Shuttleworth, R.S.,\nKumar, N., Lozano-P\u00b4erez, T., Kaelbling, L.P.:\nPddl planning with pretrained large language\nmodels. In: NeurIPS 2022 Foundation Models\nfor Decision Making Workshop (2022)\nLiang, J., Huang, W., Xia, F., Xu, P., Hausman,\nK., Ichter, B., Florence, P., Zeng, A.: Code as\npolicies: Language model programs for embod-\nied control. arXiv preprint arXiv:2209.07753\n(2022)\nShah, D., Osinski, B., Ichter, B., Levine, S.: LM-\nNav: Robotic navigation with large pre-trained\nmodels of language, vision, and action. arXiv\npreprint arXiv:2207.04429 (2022)\nChen,\nW.,\nHu,\nS.,\nTalak,\nR.,\nCarlone,\nL.:\nLeveraging large language models for robot\n3d\nscene\nunderstanding.\narXiv\npreprint\narXiv:2209.05629 (2022)\nRen, A.Z., Govil, B., Yang, T.-Y., Narasimhan,\nK., Majumdar, A.: Leveraging language for\naccelerated learning of tool manipulation. arXiv\npreprint arXiv:2206.13074 (2022)\nRadford, A., Kim, J.W., Hallacy, C., Ramesh, A.,\nGoh, G., Agarwal, S., Sastry, G., Askell, A.,\nMishkin, P., Clark, J., et al.: Learning transfer-\nable visual models from natural language super-\nvision. In: International Conference on Machine\nLearning (2021)\nMiller, G.A.: Wordnet: a lexical database for\nenglish. Communications of the ACM (1995)\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M.,\nChen, D., Levy, O., Lewis, M., Zettlemoyer,\nL., Stoyanov, V.: Roberta: A robustly opti-\nmized bert pretraining approach. arXiv preprint\narXiv:1907.11692 (2019)\nReimers, N., Gurevych, I.: Sentence-bert: Sen-\ntence embeddings using siamese bert-networks.\nIn: Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP) (2019)\nSanh, V., Debut, L., Chaumond, J., Wolf, T.:\nDistilbert, a distilled version of bert: smaller,\nfaster, cheaper and lighter. arXiv preprint\narXiv:1910.01108 (2019)\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto,\nH.P.d.O., Kaplan, J., Edwards, H., Burda, Y.,\nJoseph, N., Brockman, G., et al.: Evaluating\nlarge language models trained on code. arXiv\npreprint arXiv:2107.03374 (2021)\nChowdhery, A., Narang, S., Devlin, J., Bosma,\nM., Mishra, G., Roberts, A., Barham, P.,\nChung, H.W., Sutton, C., Gehrmann, S., et al.:\nPalm: Scaling language modeling with path-\nways. arXiv preprint arXiv:2204.02311 (2022)\nHolmberg, R., Khatib, O.: Development and con-\ntrol of a holonomic mobile robot for mobile\nmanipulation tasks. The International Journal\nof Robotics Research (2000)\nGarrido-Jurado, S., Mu\u02dcnoz-Salinas, R., Madrid-\nCuevas, F.J., Mar\u00b4\u0131n-Jim\u00b4enez, M.J.: Automatic\ngeneration and detection of highly reliable fidu-\ncial markers under occlusion. Pattern Recogni-\ntion (2014)\nGu, X., Lin, T.-Y., Kuo, W., Cui, Y.: Open-\nvocabulary object detection via vision and lan-\nguage knowledge distillation. In: International\nConference on Learning Representations (2021)\nCoulter, R.C.: Implementation of the pure pur-\nsuit path tracking algorithm. Technical report,\nCarnegie-Mellon UNIV Pittsburgh PA Robotics\n20\nINST (1992)\nZeng, A., Song, S., Lee, J., Rodriguez, A.,\nFunkhouser, T.: Tossingbot: Learning to throw\narbitrary objects with residual physics. IEEE\nTransactions on Robotics (2020)\nMinderer, M., Gritsenko, A., Stone, A., Neumann,\nM., Weissenborn, D., Dosovitskiy, A., Mahen-\ndran, A., Arnab, A., Dehghani, M., Shen, Z.,\net al.: Simple open-vocabulary object detec-\ntion with vision transformers. arXiv preprint\narXiv:2205.06230 (2022)\n21\n"
  },
  {
    "title": "Code Execution with Pre-trained Language Models",
    "link": "https://arxiv.org/pdf/2305.05383.pdf",
    "upvote": "2",
    "text": "Code Execution with Pre-trained Language Models\nChenxiao Liu1\u2217, Shuai Lu2, Weizhu Chen2, Daxin Jiang2,\nAlexey Svyatkovskiy2, Shengyu Fu2, Neel Sundaresan2, Nan Duan2\n1 Peking University\n2 Microsoft\njadecxliu@gmail.com\n{shuailu, wzchen, djiang}@microsoft.com\n{alsvyatk, shengyfu, neels, nanduan}@microsoft.com\nAbstract\nCode execution is a fundamental aspect of pro-\ngramming language semantics that re\ufb02ects the\nexact behavior of the code. However, most pre-\ntrained models for code intelligence ignore the\nexecution trace and only rely on source code\nand syntactic structures.\nIn this paper, we\ninvestigate how well pre-trained models can\nunderstand and perform code execution. We\ndevelop a mutation-based data augmentation\ntechnique to create a large-scale and realis-\ntic Python dataset and task for code execu-\ntion, which challenges existing models such\nas Codex. We then present CodeExecutor, a\nTransformer model that leverages code execu-\ntion pre-training and curriculum learning to en-\nhance its semantic comprehension. We evalu-\nate CodeExecutor on code execution and show\nits promising performance and limitations. We\nalso demonstrate its potential bene\ufb01ts for code\nintelligence tasks such as zero-shot code-to-\ncode search and text-to-code generation. Our\nanalysis provides insights into the learning and\ngeneralization abilities of pre-trained models\nfor code execution.\n1\nIntroduction\nPre-trained models have achieved remarkable re-\nsults in natural language (NL) tasks (Radford et al.,\n2018; Devlin et al., 2019; Raffel et al., 2020), in-\nspiring the development of pre-trained models for\nprogramming language (PL) tasks (Kanade et al.,\n2020; Feng et al., 2020; Svyatkovskiy et al., 2020;\nWang et al., 2021b; Guo et al., 2021, 2022). These\nmodels leverage source code and code structures,\nsuch as abstract syntax tree (AST) (Wang et al.,\n2021a; Guo et al., 2022) and data \ufb02ow (Guo et al.,\n2021), to learn code-related tasks. These structures,\nwhile useful, are not suf\ufb01cient to represent the dy-\nnamic behavior of code during execution, which is\nre\ufb02ected in the execution trace. Using Figure 1 as\n\u2217Work done during internship at Microsoft. Shuai Lu and\nNan Duan are corresponding authors.\nan example, the execution trace shows how code be-\nhaves during execution, re\ufb02ecting the control \ufb02ow\nand the state changes of variables. On the other\nhand, as stated by Casalnuovo et al. (2020), source\ncode contains two channels of information: natural\n& formal. The natural channel (Hindle et al., 2012),\nsuch as identi\ufb01ers and comments, enables language\nmodels to be leveraged to understand code-related\ntasks. The formal channel is used by interpreters\nand compilers to specify execution and has precise\nsemantics. The formal channel is unique to code\nand is what makes it executable. Execution trace\nfalls into the second category since it reveals the\nformal channel of information that distinguishes\ncode from natural language, as well as enabling\ncode execution precisely (Casalnuovo et al., 2020;\nChakraborty et al., 2022).\nIn this work, we aim to teach pre-trained models\nthe real-world code execution process. We pro-\npose CodeExecutor, a Transformer-based model\nthat learns to execute arbitrary programs and pre-\ndict their execution traces. To support pre-training\non large-scale data, we construct the Python Co-\ndeNetMut dataset by producing mutations based\non submissions to competitive programming prob-\nlems from CodeNet (Puri et al., 2021), along with\nsingle-line Python transformations and programs\nadapted from Python of\ufb01cial tutorial. We design\na pre-training task that predicts both the line order\nand the intermediate states of the execution trace,\nand apply curriculum learning to gradually increase\nthe dif\ufb01culty of the programs.\nWe evaluate CodeExecutor on code execution\ntasks and show that it outperforms existing mod-\nels and demonstrates promising capabilities. We\nalso conduct an in-depth analysis of the model\u2019s\nperformance and reveal its strengths and weak-\nnesses. Furthermore, we show that CodeExecutor\ncan improve downstream tasks like zero-shot code-\nto-code search and text-to-code generation, indicat-\ning the potential of leveraging execution trace to\narXiv:2305.05383v1  [cs.PL]  8 May 2023\nh = 3 \nw = 7\nn = 10\nfor i in range(min(h, w)): \nn = n - max(h, w)      \nif n <= 0: \nprint(i + 1) \nbreak\n<line> 1 <state> h : 3 \n<line> 2 <state> h : 3 ; w : 7\n<line> 3 <state> h : 3 ; w : 7 ; n : 10\n<line> 4 <state> h : 3 ; w : 7 ; n : 10 ; i : 0\n<line> 5 <state> h : 3 ; w : 7 ; n : 3 ; i : 0\n<line> 6 <state> h : 3 ; w : 7 ; n : 3 ; i : 0\n<line> 4 <state> h : 3 ; w : 7 ; n : 3 ; i : 1\n<line> 5 <state> h : 3 ; w : 7 ; n : -4 ; i : 1\n<line> 6 <state> h : 3 ; w : 7 ; n : -4 ; i : 1\n<output> 2 \n<line> 7 <state> h : 3 ; w : 7 ; n : -4 ; i : 1\n<line> 8 <state> h : 3 ; w : 7 ; n : -4 ; i : 1\n1\n2\n3\n4\n5\n6\n7\n8\n(a) Source Code\n(b) Execution Trace\nFigure 1: Sample source code and its execution trace in the code execution task.\nenhance code intelligence. Our models and datasets\nare publicly available1. In summary, the contribu-\ntions of this paper are:\n\u2022 We present the \ufb01rst attempt at building a large-\nscale pre-training dataset for real-world code\nexecution using a mutation-based data aug-\nmentation approach.\n\u2022 We propose a novel pre-trained model named\nCodeExecutor that learns to predict the execu-\ntion traces using a code execution pre-training\ntask and curriculum learning.\n\u2022 We conduct a comprehensive evaluation of\nCodeExecutor for code execution tasks, pro-\nviding a detailed understanding of the model\u2019s\nperformance.\n\u2022 CodeExecutor signi\ufb01cantly improves code in-\ntelligence tasks like zero-shot code-to-code\nsearch and text-to-code generation.\n2\nRelated Work\n2.1\nLearning to Execute\nPrevious works form the learning to execute task\nas a problem that reads a program and com-\nputes the program\u2019s output. These works lever-\nage architectures such as recurrent neural networks\n(Zaremba and Sutskever, 2014), graph neural net-\nworks (Bieber et al., 2020; Wang et al., 2020) and\nTransformers (Dehghani et al., 2019; Yan et al.,\n2020; Austin et al., 2021; Nye et al., 2021). An-\nother related task algorithm induction is to read a\nshort program, such as integer addition or polyno-\nmial evaluation, and computes the output. Algo-\nrithm induction task (Graves et al., 2014; Kurach\net al., 2016; Kaiser and Sutskever, 2016; Graves\n1https://github.com/microsoft/CodeBERT/tree/\nmaster/CodeExecutor\net al., 2016; Reed and de Freitas, 2016; Dehghani\net al., 2019; Velickovic et al., 2020a,b; Nye et al.,\n2021) targets a particular algorithm with direct\nalgorithm-speci\ufb01c supervision compared with arbi-\ntrary programs in our code execution task.\nSome emerging works also employ pre-trained\nmodels to tackle the two tasks. Lu et al. (2022)\n\ufb01ne-tunes a small fraction of the weights in GPT-\n2 (Radford et al., 2019) on non-language tasks,\nincluding simple algorithm induction tasks like Bit\nXOR. Austin et al. (2021) evaluates models pre-\ntrained on web documents and dialog data ranging\nin size from 2 million to 137 billion parameters and\nshows that largest models are generally unable to\npredict the output of a program, whether few-shot\nor \ufb01ne-tuning. Nye et al. (2021) uses a \"scratchpad\"\nto store intermediate computation steps to perform\nmulti-step computations, improving the ability of\nmodels in Austin et al. (2021).\nDifferent from previous works that predict pro-\ngram\u2019s output and mainly deal with speci\ufb01c algo-\nrithms, we predict the program\u2019s whole execution\ntrace and focus on imitating the real-world arbi-\ntrary program execution behavior. Besides, by us-\ning execution to capture code semantics, our work\nis bene\ufb01cial for tasks related to code intelligence.\n2.2\nMathematical Problem Solving\nMathematical problem solving is a related domain\nof code execution. Recent works show the ability\nof language models to solve math problems, which\nrequires learning to execute a soft algorithm to ar-\nrive at a deterministic answer. Amini et al. (2019);\nLing et al. (2017) map math problems to opera-\ntion programs and focus on sequence-to-program\ngeneration. Saxton et al. (2019) introduce the Deep-\nMind Mathematics dataset, which contains plug-\nand-chug problems such as addition, list sorting,\nand function evaluation. Henighan et al. (2020)\nOperator\nDescription\nCRP\nConstant Replacement\nChange numeric and string literals.\nAOD\nArithmetic Operator Deletion\nDelete a unary arithmetic operator \u2018+\u2019 or \u2018-\u2019.\nAOR\nArithmetic Operator Replacement\nReplace an arithmetic operator with another one. E.g. x * y can be\nmutated to x / y.\nASR\nAssignment Operator Replacement\nSubstitute an extended assignment operator with another.\nBCR\nBreak Continue Replacement\nSwap keywords break and continue in a loop body.\nCOD\nConditional Operator Deletion\nDelete unary negation operator not or the negation of an membership\noperator not in.\nLCR\nLogical Connector Replacement\nSwap logical operators and with or and vice versa.\nROR\nRelational Operator Replacement\nSubstitutes relational operators. E.g. x <= y can be mutated to x > y.\nSIR\nSlice Index Removal\nDelete one argument of collection[start:end:step].\nOIL\nOne Iteration Loop\nExecute a loop only once by adding a break statement.\nRIL\nReverse Iteration Loop\nChange direction of loop iteration by the function reversed().\nZIL\nZero Iteration Loop\nInterrupt realization of a loop during its \ufb01rst iteration.\nTable 1: A set of mutation operators containing 12 operators we implement to mutate code examples.\nshows that the majority of problems in the Deep-\nMind Mathematics dataset can be straightforwardly\nsolved with large Transformers. Hendrycks et al.\n(2021) introduces the MATH dataset, consisting\nof competition math problems with step-by-step\nsolutions written in LATEX and natural languages.\nCobbe et al. (2021) releases GSM8K, including\ngrade school math questions and natural language\nsolutions. Recently, Zhou et al. (2022) proposes\nalgorithmic prompting to improve the performance\nof large language models on math problem solv-\ning, which starts from learning skills containing\naddition, subtraction, multiplication, and parity.\nCode execution involves calculations such as\naddition, subtraction, multiplication, division, ex-\nponentiation, and modulus, which are similar to\nsolving math problems. With the added complex-\nity of managing variables, data structures, control\n\ufb02ows, and other programming concepts, learning\ncode execution requires a different set of skills and\nknowledge from learning mathematics, although\nsome overlap exists.\n3\nMutation-based Data Augmentation\nThe goal of code execution task is to learn to emu-\nlate the execution without running a program by an\ninterpreter. We treat the task as a generation task:\ngiven a source code c, the execution trace t is re-\nquired to be generated. Execution trace consists of\ntwo components: one is the order in which the com-\nputer executes statements, and the other is how the\nstates of the variables change when jumping from\none statement to another. Normally, the statements\ninside a program are not executed sequentially, es-\npecially in a real-world scenario where programs\nembody complex logic and rich semantics. More-\nover, variables relate to various types of data struc-\ntures with diverse characteristics and operations.\nGiven the complexity and dif\ufb01culty of this task, it\nis of great importance to build a large-scale dataset\nand explore the capabilities and boundaries of large\nlanguage models for code execution.\n3.1\nMutating Source Code\nConstructing a large-scale Python dataset for real-\nworld code execution is very challenging. Pro-\ngrams retrieved from software development plat-\nforms such as GitHub 2 are mostly not executable at\nscale, as they depend on speci\ufb01c external resources\nwhich are not easily available. Examples of exter-\nnal resources include program inputs, \ufb01le contents,\nexternal modules, and third-party packages. For the\nsame reason, it is not practical to collect programs\nfrom posts in coding question-answering websites\nlike StackOver\ufb02ow 3.\nWe build the Python code execution dataset\nbased on submissions to competitive programming\nproblems from CodeNet benchmark (Puri et al.,\n2021). We run each submission in a sandbox en-\nvironment to get the execution trace and \ufb01lter out\nprograms that exceed time and trace limits or result\nin runtime errors.\nTo construct a large-scale dataset of executable\nprograms, we propose a mutation-based data aug-\nmentation approach. For each submission, the ap-\nproach modi\ufb01es some parts of a program to gen-\nerate diverse mutants, leading to different execu-\ntion traces. Speci\ufb01cations of these modi\ufb01cations\nare called mutation operators. It is inspired by\nmutation testing (Hamlet, 1977; Jia and Harman,\n2https://github.com/\n3https://stackoverflow.com/\n2011) in software engineering, a popular technique\nthat supports the design of high-quality test suites\nfor programs. Following Derezi\u00b4nska and Ha\u0142as\n(2014) that applies mutation testing technique to\nPython programs, we \ufb01rst present a set of muta-\ntion operators as shown in Table 1. Most of them\ncorrespond to selected operators used in strongly\ntyped general purpose languages and are adopted\nto the Python language. Operators designed for\nPython features are also included, such as Slice\nIndex Removal (SIR) and Reverse Iteration Loop\n(RIL). Then we convert a program into an AST\nand extract its node type information to get a candi-\ndate list of all mutable literals, operators and state-\nments. Finally, we generate mutants and eliminate\nthose that are not executable. We use the CodeNet\nMutants (CodeNetMut) to build the pre-training\ndataset. Greater detail of the dataset generation\nprocess can be found in Appendix A.\n3.2\nDataset Construction\nGiven the dif\ufb01culty of training the model on real-\nworld complete programs, we build two simpler\ndatasets along with CodeNetMut for pre-training.\nThe \ufb01rst is the Python SingleLine dataset col-\nlected by Fraser Greenlee 4, which consists of\nnearly nine million examples of single-line transfor-\nmations. Each example contains several variables\nspeci\ufb01ed in initial values, a single line of Python\ncode, and the new set of variables and values re-\nsulting from executing that line. We combine the\n\ufb01rst two as the input code, and use the last one as\nthe target trace. We do not re-execute the dataset.\nWhen pre-training on SingleLine data, we only ask\nthe model to predict the \ufb01nal states of the last code\nline without line-by-line illustration. Figure 2 (a)(b)\nshow examples of these data. Since individual lines\nof code constitute real-world complex programs,\nthe dataset serves as a foundation for learning about\ncode execution.\nThe second is the Python Tutorial dataset. This\ndataset is created by crawling and \ufb01ltering all the\nexecutable code examples that appear in the of\ufb01cial\nPython tutorial 5. The of\ufb01cial tutorial introduces\nthe basic concepts and most noteworthy features of\nthe Python language. To generate this dataset, we\napply the Constant Replacement operator (\ufb01rst row\nin Table 1) to change numeric literals into diverse\nvalues. This approach results in 3.4 million pro-\n4https://www.kaggle.com/frasergreenlee/\npython-state-changes\n5https://docs.python.org/3/tutorial\nc = 98 \nz = 3 \nc += z\n1\n2\n3\n(a)\nCode:\nTrace:\nc : 101; z : 3 \nf = 'ifailuhkqq\u2019 \nl = ['a', 'i\u2019] \nx = 2 \ny = 5 \nl = list(f[x:y])\n1\n2\n3\n4\n5\n(b)\nCode:\nTrace:\nf : \u2018ifailuhkqq\u2019;\nl : [\u2018a\u2019,\u2018i\u2019,\u2018l\u2019];\nx : 2; y : 5 \n1\n2\n3\n4\n5\n6\n7\n(c)\nCode:\nTrace:\n<line> 1 <state> stack:[3, 866, -325] \n<line> 2 <state> stack:[3, 866, -325, 6] \n<line> 3 <state> stack:[3, 866, -325, 6, 7] \n<line> 4 <state> stack:[3, 866, -325, 6] \n<line> 5 <state> stack:[3, 866, -325] \n<line> 6 <state> stack:[3, 866] \n<line> 7 <state> stack:[3, 866] \nstack = [3, 866, -325] \nstack.append(6) \nstack.append(7) \nstack.pop() \nstack.pop() \nstack.pop() \nstack\nFigure 2: (a) and (b) are examples from the SingleLine\ndataset. (c) is an example from the Tutorial dataset.\ngrams. Figure 2 (c) shows an example of a mutant.\nWhile the Tutorial dataset is not comprehensive\nand does not cover every single feature, it provides\na good representation of Python\u2019s \ufb02avor and style,\nwhich offers valuable supervision for modeling the\nexecution of commonly used code blocks.\nTherefore, the Python Code Execution datasets\nare a series of datasets following an easy-to-hard\nparadigm, including the SingleLine dataset, Tuto-\nrial dataset, and CodeNetMut dataset.\n4\nCodeExecutor\nOur CodeExecutor utilizes a Transformer-based\nframework to learn code execution through pre-\ntraining. We will \ufb01rst describe the model architec-\nture (\u00a74.1), then the pre-training task (\u00a74.2), and\n\ufb01nally, the curriculum learning strategy (\u00a74.3).\n4.1\nModel Architecture\nThe model is based on Transformer and adopts the\nsame architecture as UniXcoder (Guo et al., 2022).\nUniXcoder is a uni\ufb01ed cross-modal pre-trained\nmodel for programming language which has\nencoder-only, decoder-only and encoder-decoder\nmodes. It utilizes mask attention matrices (Dong\net al., 2019) with pre\ufb01x adapters to control the\nbehavior. We take the encoder-decoder manner\nby using a special token [E2D] as the pre\ufb01x in\nfront of the input. CodeExecutor consists of 12\nTransformer layers. Each transformer layer is ar-\nchitecturally identical, containing a multi-headed\nself-attention pooling (Vaswani et al., 2017) fol-\nlowed by a feed forward network.\n4.2\nPre-training Task\nWe propose a new pre-training task called code\nexecution. Our motivation for the task is to im-\nprove the ability of our model to understand and\nexecute code. Traditional pre-training tasks such as\nlanguage modeling or denoising objective do not\ninvolve code execution, and thus, models trained\non these tasks have limited ability to execute code.\nBy pre-training our model on the task of code ex-\necution, we aim to improve its ability by learning\nuseful patterns from bimodal data of code and trace.\nThis will enable our model to generate more accu-\nrate traces and understand the behavior of the code,\nwhich is crucial for a wide range of code intelli-\ngence applications that require code understanding.\nWith the knowledge of how the code works, the\nmodel can better understand the underlying logic\nof the code and use that understanding to better\nperform these tasks.\nWe continue pre-training UniXcoder on the\ntask. At the pre-training stage, our model receives\ncode as inputs and learns to generate traces.\nTo facilitate a better understanding of code,\nspecial tokens [i] indicating line numbers and\n[INDENT] [DETENT] indicating indentation\nare inserted into the code. Each line in trace can\nbe represented as [LINE], [i], [STATE], v1, :\n, s1, [DICTSEP], ..., [DICTSEP], vk, :, sk,\n[STATEEND], where k denotes the number\nof variables and the state of k-th variable vk\nis sk. The symbol [DICTSEP] separates the\npairs within the dictionary and [STATEEND]\nindicates the end of the states. This representation\nallows our model to learn the state of variables\nat each step of the execution, which is crucial for\nunderstanding the behavior of the code.\n4.3\nCurriculum Learning\nTo improve the generalization capacity, we follow\nthe curriculum learning strategy during pre-training.\nCurriculum learning (Bengio et al., 2009) (CL) is a\nlearning strategy that starts from easy instances and\nthen gradually handles harder ones, which imitates\nthe meaningful learning order in human curricula.\nIn our pre-training process, we organize the learn-\ning of the Python code execution datasets according\nto a curriculum that starts with simple instances, i.e.\nSingleLine data. First, we employ all the 9 million\nSingleLine transformations to pre-train CodeEx-\necutor until convergence. To achieve a balanced\ndataset, we then reserve 3 million instances in Sin-\nSingleLine\nTutorial\nCodeNetMut\nDif\ufb01culty Level\nEasy\nMedium\nHard\nLanguage\nPython\nPython\nPython\nPre-train #\n8,950,959\n3,422,943\n2,838,644\nTest #\n7,968\n13,744\n19,541\nAvg Code Len\n3.28\n4.90\n8.26\nAvg Trace Len\n1.00\n11.89\n22.80\nAvg State Num\n2.44\n1.34\n3.67\nTable 2: Statistics of pre-training dataset. \u201cAvg Code\nLen\u201d and \u201cAvg Trace Len\u201d represent the average num-\nber of lines in a program and a trace, respectively. \u201cAvg\nState Num\u201d denotes the average of the maximum num-\nber of states reached per line in a trace.\ngleLine that are most dif\ufb01cult for our model to gen-\nerate and add Tutorial data into the pre-training cor-\npus. We further add CodeNetMut data into the pre-\ntraining corpus and pre-train the model to converge\non all the examples. To help distinguish dif\ufb01culty\nlevel, we add a pre\ufb01x p \u2208 {[SINGLELINE],\n[TUTORIAL], [CODENETMUT]} in front\nof the input, indicating the kind of data, e.g.\n[SINGLELINE] means receiving SingleLine\ndata. More details about pre-training settings and\nmodel con\ufb01gurations can be found in Appendix B.\n5\nExperimental Setup\n5.1\nDataset\nWe build our pre-training dataset as described in\nSection 3. Table 2 shows some basic statistics. The\n19,541 examples in CodeNetMut test split are from\n39 unseen programming problems in CodeNet and\nhave not undergone the mutation process. Addition-\nally, we held out 10k programs from each dataset as\na validation split during pre-training. For Tutorial\nand CodeNetMut, the ground truth trace is the exe-\ncution result of the whole program. For SingleLine,\nsince the instances are simple programs consisting\nof variable declarations and one-line transforma-\ntions, the model is only asked to predict the \ufb01nal\nstates of variables, which is presented in the form\nof a one-line trace. We observe the average length\nof code and trace in CodeNetMut are about twice\nas long as those in Tutorial. Also, executing pro-\ngrams in CodeNetMut requires managing a larger\nnumber of variables in varying states.\n5.2\nModels\nWe evaluate several models on code execution\ntask. Codex model code-cushman-001 is a spe-\ncialized GPT model \ufb01ne-tuned on GitHub code\n(Chen et al., 2021).\nWe use few-shot learning\nDataset\nModel\nGeneral\nLine\nIdenti\ufb01er\nOutput Acc.\nTrace Acc.\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nSingeLine\nCodex\n-\n36.87\n36.87\n36.87\n36.87\n71.87\n69.34\n70.58\nCEL-S1\n-\n93.32\n93.32\n93.32\n93.32\n96.94\n96.86\n96.90\nCodeExecutor\n-\n94.03\n94.03\n94.03\n94.03\n97.28\n97.18\n97.23\nTutorial\nCodex\n13.07\n-\n-\n-\n-\n-\n-\n-\nCEL-S2\n79.51\n85.59\n95.94\n84.24\n89.71\n97.29\n87.30\n92.02\nCEL-S3\n7.89\n8.35\n26.58\n21.33\n23.67\n26.36\n19.47\n22.40\nCodeExecutor\n76.42\n80.09\n94.49\n76.74\n84.70\n95.91\n69.15\n80.36\nCodeNetMut\nCodex\n17.45\n-\n-\n-\n-\n-\n-\n-\nCEL-S3\n43.80\n29.44\n59.32\n41.76\n49.01\n68.30\n41.69\n51.78\nCodeExecutor\n48.06\n33.38\n58.70\n43.48\n49.96\n67.81\n45.29\n54.31\n-w/o CL\n45.93\n30.98\n60.21\n42.45\n49.79\n68.55\n41.58\n51.76\nTable 3: Results on the code execution task. In the Tutorial and CodeNetMut datasets, Codex cannot generate\nexecution traces in a uniform format. Therefore, we only report the output accuracy of Codex in these datasets.\nby giving Codex three code and execution trace\npairs for the code execution task. CodeExecutor-\nLimited (CEL) is a three-stage model pre-trained\nwith the code execution objective. CEL can only\naccess limited data in each stage, as opposed to\nCodeExecutor which can utilize all the datasets\nsimultaneously (see Appendix C for a detailed com-\nparison). It is initialized using the publicly avail-\nable checkpoint of UniXcoder and continues to be\ntrained with SingleLine data, resulting in the model\nCodeExecutorLimited-Stage1, which we call CEL-\nS1. In the second stage, we initialize it with CEL-\nS1 and employ Tutorial data to pre-train, so we\nget the model CEL-S2. By continuing pre-training\nCEL-S2, we use CodeNetMut to improve the ca-\npacity of executing real-world programs at the third\nstage. CEL-S3 is produced after these stages men-\ntioned above. CodeExecutor without Curriculum\nLearning(CodeExecutor w/o CL) is a single-stage\nmodel trained on all three datasets together.\n5.3\nEvaluation Metrics\nWe test model capabilities of executing code on the\ntest sets from three datasets. We measure functional\ncorrectness of the sampled trace from three perspec-\ntives. We report output accuracy and trace accuracy\nto evaluate the general aspect. Output accuracy\nchecks if the model prints the same message as\nthe code execution, calculated only for programs\nwith standard output. Trace accuracy checks if the\nmodel produces the same trace as the code execu-\ntion, regardless of the order of states in a line of the\ntrace. To evaluate the correctness of each line and\nthe states of identi\ufb01ers in the trace, we also assess\nper-line score and identi\ufb01er score. Line precision\nis determined by the ratio of correctly identi\ufb01ed\nlines among all the lines in the traces generated\nby the model. Line recall is the ratio of correctly\nidenti\ufb01ed lines predicted by the model among all\nthe lines in the ground truth traces. Similarly, we\nalso calculate scores for the identi\ufb01ers in the trace.\nTo deepen our understanding of model behav-\nior and error modes, we also conduct a qualitative\nanalysis by examining samples.\nWe randomly sample 50 code-trace pairs from\nthe test set and ask two programmers with at least\n5 years of experience to evaluate whether CodeEx-\necutor executes a program correctly in 7 aspects.\nThe category Basic includes basic knowledge for\na Python beginner like math operators, augmented\nassignment operators, comparison operators, vari-\nables. The category Lists, Tuples, etc. consists of\ntypical Python data structures, such as lists, tuples,\ndictionaries, sets, and related manipulation func-\ntions. As shown in Table 4, we build the taxonomy,\nalong with a handbook to guide classi\ufb01cation. Each\nreviewer examines the generated trace line by line\nand counts the occurrence frequency of each cate-\ngory. They count all these categories if a trace line\ninvolves multiple categories. When an error occurs,\nthey identify which kind of knowledge category\nthe model mistakes. Finally, they work together\nto discuss the divergence of error attribution and\ncome to an agreement.\n6\nResults and Analysis\nIn this section, we evaluate CodeExecutor on code\nexecution task(\u00a76.1), conduct an in-depth analy-\nsis to understand model behavior and error mode\n(\u00a76.2), followed by two downstream tasks (\u00a76.3).\nrec = ['10', '3', '5'] \nn, a, b = map(int, rec) \nnin = [a, b] \nnmax = min(nin) \nnmin = n - min(n, (n-nin[0])+(n-nin[1])) \nprint(str(nmax) + \" \" + str(nmin))\n1\n2\n3\n4\n5\n6\nCode:\nPrediction:\n<line> 1 <state> rec:[10, 3, 5]  \n<line> 2 <state> rec:[10, 3, 5]; n:10; a:3; b:5 \n<line> 3 <state> rec:[10, 3, 5]; n:10; a:3; b:5; nin:[3, 5] \n<line> 4 <state> rec:[10, 3, 5]; n:10; a:3; b:5; nin:[3, 5]; nmax:3 \n<line> 5 <state> rec:[10, 3, 5]; n:10; a:3; b:5; nin:[3, 5]; nmax:3; nmin:2\n<output> 3 2 \n<line> 6 <state> rec:[10, 3, 5]; n:10; a:3; b:5; nin:[3, 5]; nmax:3; nmin:2\nFigure 3: An Example from CodeNetMut test split, where CodeExecutor produces an imperfect prediction, with\nthe mistake highlighted by an underline.\n6.1\nOverall Results\nWe evaluate the performance of models on Single-\nLine, Tutorial and CodeNetMut datasets.\nWe show the result of SingleLine in Table 3\n(top).\nCodeExecutor is able to execute around\n94% of single-line transformations correctly, while\nCodex fails to do so in most cases. CodeExecutor\nalso brings a 0.7% improvement over CEL-S1, indi-\ncating learning hard programs during pre-training\nhelps better solve easier examples. Since each Sin-\ngleLine program always produces a one-line trace\nwithout standard outputs, we do not report output\naccuracy, and the line precision/recall scores are\nequal to trace accuracy.\nFor the Tutorial experiments in Table 3\n(medium), CodeExecutor signi\ufb01cantly outperforms\nCodex on output accuracy (76.42% vs.13.07%).\nThe lower score of CodeExecutor compared to\nCEL-S2 suggests a discrepancy between code ex-\namples in tutorials and CodeNet since the Tutorial\ndataset is composed of mutants from only a few\nprograms in tutorial websites, limiting its diversity.\nCEL-S3 struggles to produce traces, indicating that\nit forgets most knowledge acquired in Tutorial data\nin the last training stage.\nCodeNetMut results are much lower than those\nin SingleLine and Tutorial datasets, which shows\nthat it is more challenging to generate traces in\nreal-world scenarios. CodeExecutor produces the\ncorrect output for nearly half of the examples\n(48.06%), and about a third of the traces are the\nexact match for the ground truth (33.38%). By pre-\ntraining on the code execution task, CodeExecutor\nboosts the performance of output by 30.6% abso-\nlute points over Codex. Besides, CodeExecutor\nyields 4.3% output accuracy score and 3.9% trace\naccuracy score improvement than CEL-S3, which\nindicates the effectiveness of the training strategy\ndescribed in 4.3. After removing curriculum learn-\ning, the output accuracy score drops from 48.06%\nto 45.93% and the trace accuracy score drops from\n33.38% to 30.98%, which shows the contribution\nCategory\nTotal\nCorrect\nAccuracy\nBasic\n204\n183\n89.71\nBuilt-in Functions\n42\n35\n83.33\nLists, Tuples, etc.\n44\n34\n77.27\nStrings\n19\n10\n52.63\nConditional Statements\n60\n57\n95.00\nLoops\n25\n21\n84.00\nFunction Calls\n5\n5\n100.00\nTable 4:\nHuman evaluation results. To evaluate the\ncapability of CodeExecutor, we classify Python pro-\ngramming knowledge into seven categories and man-\nually analyze whether the generated trace is correct or\nwrong when dealing with these categories. The third\ncategory includes Python data structures, such as lists,\ntuples, dictionaries and sets.\nof curriculum learning.\nThese results demonstrate that the code execu-\ntion task is challenging for pre-trained models on\nsource code like Codex. However, our CodeExecu-\ntor model can achieve high performance to execute\nsimple programs and are capable of predicting com-\nplex execution traces for real-world programs.\n6.2\nIn-depth Study on Model Performance\nWe conduct a qualitative analysis of model perfor-\nmance by examining samples (Table 4), resulting\nin the following \ufb01ndings. More examples can be\nfound in Appendix D.\nThe Model Typically Has a Basic Sense of Con-\ntrol Flows\nConditional statements, loops, and\nfunction calls reveal the control \ufb02ow of the pro-\ngram. Control \ufb02ow re\ufb02ects the order in which the\nprogram\u2019s code executes. It is important for un-\nderstanding a program and is often complex, as\nit controls the code through certain decisions and\nmonitors which statements need to be executed\nand which should be skipped. From Table 4, we\n\ufb01nd that CodeExecutor has a rudimentary under-\nstanding of high-level multi-line control \ufb02ows, es-\npecially expert at conditional statements and func-\ntion calls. 57 out of 60 conditional statements and\nall 5 calls to user-de\ufb01ned functions are predicted\nModel\nMAP\nGraphCodeBERT\n23.08\n+ CodeExecutor\n55.94\nUniXcoder\n71.86\n+ CodeExecutor\n79.13\nTable 5: MAP score (%) on code-to-code search task\nin zero-shot setting.\ncorrectly. The accuracy of loops is 84%, while\nthe incorrect loops undergo wrong iterative times.\nTake Figure 1 (a) as an example. CodeExecutor\npredicts exactly the same trace as the ground truth\nin (b). Our model recognizes that the for loop oc-\ncurred on line 4 will execute several times. In the\nsecond iteration, \u201cn\u201d meets the condition of \u201cn <=\n0\u201d, resulting in the \u201cbreak\u201d statement and termi-\nnating the loop. The model behaves well on the\ncode block in the for loop, showing its capacity of\nunderstanding control \ufb02ows.\nThe Model Struggles to Handle the Intrica-\ncies of Operations, Particularly in Relation to\nData Structures\nComplex programs often in-\nvolve multiple categories of programming knowl-\nedge. Figure 3 shows an example that uses lists\nand strings. It determines the maximum and mini-\nmum possible number of people among \u201cn\u201d, who\nsubscribe to both Newspaper I and II, given that \u201ca\u201d\npeople subscribe to I and \u201cb\u201d people subscribe to\nII. CodeExecutor incorrectly calculates \u201cnmin\u201d in\nline 5, expected 0 but got 2. This calculation in-\nvolves retrieving values from a list, performing ad-\nditions, subtractions, and using the \"min\" function.\nThe compositionality of these operations makes it\nchallenging for our model to fully comprehend the\ncode and generate accurate states. Additionally, as\npresented by the relatively low accuracy on \u201cLists,\nTuples, etc.\u201d (77.27%) and \u201cStrings\u201d (52.63%) in\nTable 4, we observe that the model falls short of\nunderstanding data structures like lists and strings.\nThe understanding of data structures requires the\nmodel to learn the behavior of objects after they\nare created, modi\ufb01ed, added or deleted. These\noperations can be changeable and challenging for\nthe model to grasp. This suggests that the model\nmay struggle with complex programs that involve\nmultiple operations and data structures.\n6.3\nDownstream Tasks\nTo verify the effectiveness of CodeExecutor in rep-\nresenting code semantics, we apply it to two code\nModel\nPass@1\nPass@10\nCodex\n12.48\n45.59\n+ CodeExecutor\n17.87\n49.69\nTable 6:\nResults on HumanEval benchmark for the\ntext-to-code generation task. 50 solutions are evaluated\nfor each problem in both settings.\nintelligence tasks \u2013 the zero-shot code-to-code-\nsearch task and text-to-code generation task.\nZero-shot Code-to-code Search\nThe task is in-\ntroduced by Guo et al. (2022). To avoid duplication\nbetween the associate dataset and our pre-training\ncorpus, we construct a new dataset by collecting\n9,987 Python functions from CodeNet (Puri et al.,\n2021). Each function solves one of the 48 problems.\nGiven one function, we retrieve all the functions\nthat solve the same problem.\nWe \ufb01rst use the mean vectors of last hidden states\nof a baseline model to calculate the similarity be-\ntween two functions. To explore how code exe-\ncution facilitates code-to-code-search, we execute\neach function by providing a test case. We then\nutilize the program outputs extracted from the exe-\ncution trace generated by CodeExecutor, and sort\nthe candidates according to the edit similarity com-\npared with outputs of the query program.\nFrom table 5, we \ufb01nd that CodeExecutor boosts\nover 32.8 points compared with GraphCodeBERT\n(Guo et al., 2021), and provides about 7.2 points\nimprovement compared with UniXcoder, showing\nthat code execution can signi\ufb01cantly enhance the\ncomprehension of code semantics.\nText-to-code Generation\nWe use HumanEval\nbenchmark (Chen et al., 2021) which includes 164\nhuman-written programming problems.\nWe \ufb01rst leverage Codex (code-cushman-001)\nto generate 200 solutions for each problem. Then\nwe use CodeExecutor to predict the outputs of each\nsolution by feeding example test cases in problem\ndescriptions. We rank the 200 solutions by the\nedit similarity between their outputs and expected\noutputs. Finally, we evaluate the correctness of\nthe \ufb01rst 50 solutions for each problem. Note that\ndifferent from other \ufb01ltering strategies, our method\ndoesn\u2019t need a real-world code executor but only\nuses models to predict the execution results.\nTable 6 demonstrates that with CodeExecutor as\na solution \ufb01lter, the performance of text-to-code\ngeneration is improved, indicating CodeExecutor\nis bene\ufb01cial to other code intelligence tasks.\n7\nConclusion\nWe propose a mutation-based data augmentation\nmethod to create a large and realistic Python code\nexecution dataset and task, which pose a signi\ufb01-\ncant challenge for current models such as Codex.\nWe develop CodeExecutor, a Transformer model\nthat leverages code execution as a pre-training ob-\njective and adopts a curriculum learning strategy.\nCodeExecutor not only outperforms existing mod-\nels on code execution, but also demonstrates its\ngeneralizability to downstream tasks such as code-\nto-code search and text-to-code generation. Our\nwork offers a novel and effective solution for code\nexecution and other code intelligence tasks.\nLimitations\nSeveral limitations of CodeExecutor, such as its\napplication to only Python, the lack of faithfulness\nin the results produced, and the maximum length\nlimit for trace generation, point toward interesting\ndirections for future work.\nProgramming Language\nOne limitation of our\ncurrent model is that it is currently only applied\nto Python, which limits its use and effectiveness\nin executing programs written in other program-\nming languages. This highlights the need for future\nwork to expand the model\u2019s applicability to other\nlanguages.\nFaithfulness\nThe result may not be faithful\nenough when handling dif\ufb01cult examples, such\nas those with complex logic, long loops, or many\nbranches. For example, we observe that in two com-\nplicated programs that both contain the assignment\n\u201calpha = list(\u2019abcdefg\u2019)\u201d, our model correctly pre-\ndicts the value of \u201calpha\u201d in one case but incor-\nrectly in the other. The lack of faithfulness needs to\nbe studied for further research on code execution.\nGeneration Window Size\nWe limit the length\nof generated trace to 1024 tokens. It can be a lim-\nitation for programs with long execution traces,\nparticularly those with loops. Improving the ability\nof Transformers to handle longer sequences (Tay\net al., 2021, 2022) would likely be bene\ufb01cial for\nthe code execution task.\nEthical Statement\nThe work is conducted in compliance with ethical\nprinciples. The datasets introduced in this paper\nonly used publicly available data. The annotation in\nhuman evaluation was conducted by two authors of\nthe paper, and thus there are no associated concerns,\ne.g. regarding compensation. Therefore, there are\nno potential risks associated with the research.\nReferences\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. Mathqa: Towards interpretable math\nword problem solving with operation-based for-\nmalisms.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 2357\u20132367. Association for Computa-\ntional Linguistics.\nJacob Austin, Augustus Odena, Maxwell I. Nye,\nMaarten Bosma, Henryk Michalewski, David Do-\nhan, Ellen Jiang, Carrie J. Cai, Michael Terry,\nQuoc V. Le, and Charles Sutton. 2021.\nPro-\ngram synthesis with large language models. CoRR,\nabs/2108.07732.\nYoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProceedings of the 26th Annual International Con-\nference on Machine Learning, ICML 2009, Mon-\ntreal, Quebec, Canada, June 14-18, 2009, volume\n382 of ACM International Conference Proceeding\nSeries, pages 41\u201348. ACM.\nDavid Bieber, Charles Sutton, Hugo Larochelle, and\nDaniel Tarlow. 2020. Learning to execute programs\nwith instruction pointer attention graph neural net-\nworks. In Advances in Neural Information Process-\ning Systems 33: Annual Conference on Neural In-\nformation Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual.\nCasey Casalnuovo, Earl T. Barr, Santanu Kumar Dash,\nPrem Devanbu, and Emily Morgan. 2020. A theory\nof dual channel constraints.\nIn ICSE-NIER 2020:\n42nd International Conference on Software Engi-\nneering, New Ideas and Emerging Results, Seoul,\nSouth Korea, 27 June - 19 July, 2020, pages 25\u201328.\nACM.\nSaikat Chakraborty, Tou\ufb01que Ahmed, Yangruibo Ding,\nPremkumar T. Devanbu, and Baishakhi Ray. 2022.\nNatgen: generative pre-training by \"naturalizing\"\nsource code.\nIn Proceedings of the 30th ACM\nJoint European Software Engineering Conference\nand Symposium on the Foundations of Software En-\ngineering, ESEC/FSE 2022, Singapore, Singapore,\nNovember 14-18, 2022, pages 18\u201330. ACM.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harrison Edwards, Yuri Burda, Nicholas\nJoseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf,\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott\nGray, Nick Ryder, Mikhail Pavlov, Alethea Power,\nLukasz Kaiser, Mohammad Bavarian, Clemens Win-\nter, Philippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. 2021. Evalu-\nating large language models trained on code. CoRR,\nabs/2107.03374.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Ja-\ncob Hilton, Reiichiro Nakano, Christopher Hesse,\nand John Schulman. 2021.\nTraining veri\ufb01ers to\nsolve math word problems. CoRR, abs/2110.14168.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. OpenReview.net.\nAnna Derezi\u00b4nska and Konrad Ha\u0142as. 2014. Operators\nfor mutation testing of python programs. Res. Rep.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171\u20134186. Association for Computa-\ntional Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019.\nUni\ufb01ed language\nmodel pre-training for natural language understand-\ning and generation.\nIn Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 13042\u201313054.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nbert: A pre-trained model for programming and nat-\nural languages. In Findings of the Association for\nComputational Linguistics: EMNLP 2020, Online\nEvent, 16-20 November 2020, volume EMNLP 2020\nof Findings of ACL, pages 1536\u20131547. Association\nfor Computational Linguistics.\nAlex Graves, Greg Wayne, and Ivo Danihelka. 2014.\nNeural turing machines. CoRR, abs/1410.5401.\nAlex\nGraves,\nGreg\nWayne,\nMalcolm\nReynolds,\nTim Harley, Ivo Danihelka, Agnieszka Grabska-\nBarwinska, Sergio Gomez Colmenarejo, Edward\nGrefenstette, Tiago Ramalho, John P. Agapiou,\nAdri\u00e0 Puigdom\u00e8nech Badia, Karl Moritz Hermann,\nYori Zwols, Georg Ostrovski, Adam Cain, Helen\nKing, Christopher Summer\ufb01eld, Phil Blunsom, Ko-\nray Kavukcuoglu, and Demis Hassabis. 2016. Hy-\nbrid computing using a neural network with dy-\nnamic external memory. Nat., 538(7626):471\u2013476.\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming\nZhou, and Jian Yin. 2022. Unixcoder: Uni\ufb01ed cross-\nmodal pre-training for code representation. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 7212\u20137225. Association for Computa-\ntional Linguistics.\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,\nDuyu Tang, Shujie Liu, Long Zhou, Nan Duan,\nAlexey Svyatkovskiy, Shengyu Fu, Michele Tu-\nfano, Shao Kun Deng, Colin B. Clement, Dawn\nDrain, Neel Sundaresan, Jian Yin, Daxin Jiang, and\nMing Zhou. 2021.\nGraphcodebert:\nPre-training\ncode representations with data \ufb02ow.\nIn 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nOpenReview.net.\nRichard G. Hamlet. 1977. Testing programs with the\naid of a compiler.\nIEEE Trans. Software Eng.,\n3(4):279\u2013290.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021.\nMeasuring mathematical\nproblem solving with the MATH dataset.\nIn Pro-\nceedings of the Neural Information Processing Sys-\ntems Track on Datasets and Benchmarks 1, NeurIPS\nDatasets and Benchmarks 2021, December 2021,\nvirtual.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\nChristopher Hesse, Jacob Jackson, Heewoo Jun,\nTom B. Brown, Prafulla Dhariwal, Scott Gray, Chris\nHallacy, Benjamin Mann, Alec Radford, Aditya\nRamesh, Nick Ryder, Daniel M. Ziegler, John Schul-\nman, Dario Amodei, and Sam McCandlish. 2020.\nScaling laws for autoregressive generative modeling.\nCoRR, abs/2010.14701.\nAbram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel,\nand Premkumar T. Devanbu. 2012.\nOn the natu-\nralness of software. In 34th International Confer-\nence on Software Engineering, ICSE 2012, June 2-\n9, 2012, Zurich, Switzerland, pages 837\u2013847. IEEE\nComputer Society.\nYue Jia and Mark Harman. 2011. An analysis and sur-\nvey of the development of mutation testing. IEEE\nTrans. Software Eng., 37(5):649\u2013678.\nLukasz Kaiser and Ilya Sutskever. 2016. Neural gpus\nlearn algorithms. In 4th International Conference\non Learning Representations, ICLR 2016, San Juan,\nPuerto Rico, May 2-4, 2016, Conference Track Pro-\nceedings.\nAditya Kanade, Petros Maniatis, Gogul Balakrishnan,\nand Kensen Shi. 2020.\nLearning and evaluating\ncontextual embedding of source code. In Proceed-\nings of the 37th International Conference on Ma-\nchine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 5110\u20135121. PMLR.\nKarol\nKurach,\nMarcin\nAndrychowicz,\nand\nIlya\nSutskever. 2016. Neural random-access machines.\nIn 4th International Conference on Learning Repre-\nsentations, ICLR 2016, San Juan, Puerto Rico, May\n2-4, 2016, Conference Track Proceedings.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2017, Vancouver, Canada, July 30 - August 4,\nVolume 1: Long Papers, pages 158\u2013167. Association\nfor Computational Linguistics.\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mor-\ndatch. 2022. Frozen pretrained transformers as uni-\nversal computation engines.\nIn Thirty-Sixth AAAI\nConference on Arti\ufb01cial Intelligence, AAAI 2022,\nThirty-Fourth Conference on Innovative Applica-\ntions of Arti\ufb01cial Intelligence, IAAI 2022, The Twel-\nveth Symposium on Educational Advances in Arti\ufb01-\ncial Intelligence, EAAI 2022 Virtual Event, February\n22 - March 1, 2022, pages 7628\u20137636. AAAI Press.\nMaxwell I. Nye, Anders Johan Andreassen, Guy Gur-\nAri, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten\nBosma, David Luan, Charles Sutton, and Augus-\ntus Odena. 2021.\nShow your work: Scratchpads\nfor intermediate computation with language models.\nCoRR, abs/2112.00114.\nRuchir Puri, David S. Kung, Geert Janssen, Wei\nZhang, Giacomo Domeniconi, Vladimir Zolotov, Ju-\nlian Dolby, Jie Chen, Mihir R. Choudhury, Lindsey\nDecker, Veronika Thost, Luca Buratti, Saurabh Pu-\njar, Shyam Ramji, Ulrich Finkler, Susan Malaika,\nand Frederick Reiss. 2021. Codenet: A large-scale\nAI for code dataset for learning a diversity of coding\ntasks. In Proceedings of the Neural Information Pro-\ncessing Systems Track on Datasets and Benchmarks\n1, NeurIPS Datasets and Benchmarks 2021, Decem-\nber 2021, virtual.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019.\nLan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uni\ufb01ed text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1\u2013140:67.\nScott E. Reed and Nando de Freitas. 2016.\nNeural\nprogrammer-interpreters. In 4th International Con-\nference on Learning Representations, ICLR 2016,\nSan Juan, Puerto Rico, May 2-4, 2016, Conference\nTrack Proceedings.\nDavid Saxton, Edward Grefenstette, Felix Hill, and\nPushmeet Kohli. 2019.\nAnalysing mathematical\nreasoning abilities of neural models. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nAlexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,\nand Neel Sundaresan. 2020.\nIntellicode compose:\ncode generation using transformer.\nIn ESEC/FSE\n\u201920: 28th ACM Joint European Software Engineer-\ning Conference and Symposium on the Founda-\ntions of Software Engineering, Virtual Event, USA,\nNovember 8-13, 2020, pages 1433\u20131443. ACM.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang\nShen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu\nYang, Sebastian Ruder, and Donald Metzler. 2021.\nLong range arena : A benchmark for ef\ufb01cient trans-\nformers. In 9th International Conference on Learn-\ning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2022.\nEf\ufb01cient transformers: A survey.\nACM Computing Surveys, 55(6):1\u201328.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information process-\ning systems, 30.\nPetar Velickovic, Lars Buesing, Matthew C. Overlan,\nRazvan Pascanu, Oriol Vinyals, and Charles Blun-\ndell. 2020a. Pointer graph networks. In Advances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nPetar Velickovic, Rex Ying, Matilde Padovano, Raia\nHadsell, and Charles Blundell. 2020b. Neural execu-\ntion of graph algorithms. In 8th International Con-\nference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nXin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao\nWan, Xiao Liu, Li Li, Hao Wu, Jin Liu, and Xin\nJiang. 2021a.\nSyncobert:\nSyntax-guided multi-\nmodal contrastive pre-training for code representa-\ntion. arXiv preprint arXiv:2108.04556.\nYu Wang, Ke Wang, Fengjuan Gao, and Linzhang\nWang. 2020.\nLearning semantic program embed-\ndings with graph interval neural network.\nProc.\nACM Program. Lang., 4(OOPSLA):137:1\u2013137:27.\nYue Wang, Weishi Wang, Sha\ufb01q R. Joty, and Steven\nC. H. Hoi. 2021b.\nCodet5: Identi\ufb01er-aware uni-\n\ufb01ed pre-trained encoder-decoder models for code un-\nderstanding and generation. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021, Virtual Event\n/ Punta Cana, Dominican Republic, 7-11 November,\n2021, pages 8696\u20138708. Association for Computa-\ntional Linguistics.\nYujun\nYan,\nKevin\nSwersky,\nDanai\nKoutra,\nParthasarathy Ranganathan, and Milad Hashemi.\n2020.\nNeural execution engines:\nLearning to\nexecute subroutines. In Advances in Neural Infor-\nmation Processing Systems 33: Annual Conference\non Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nWojciech Zaremba and Ilya Sutskever. 2014. Learning\nto execute. CoRR, abs/1410.4615.\nHattie Zhou, Azade Nova, Hugo Larochelle, Aaron C.\nCourville, Behnam Neyshabur, and Hanie Sedghi.\n2022. Teaching algorithmic reasoning via in-context\nlearning. CoRR, abs/2211.09066.\nA\nDataset Detail\nTo obtain executable programs, we build the Python\nCode Execution dataset based on submissions to\ncompetitive programming problems from CodeNet\n(Puri et al., 2021). These human-written programs\nwith real-world complexity are derived from online\njudge websites AIZU 6 and AtCoder 7. CodeNet\ncontains 240k Python submissions, aiming to solve\n8,00 distinct programming problems. Each sub-\nmission is a single-\ufb01le Python program that reads\nfrom stdin and writes to stdout. Each programming\nproblem provides at least one sample input and at\nmost four sample inputs. Since executing a pro-\ngram relies on an input, we replace the statements\nthat read from input streams with assignment state-\nments that assign input values to variables. We\nrun each submission in a sandbox environment to\nget the execution trace for that program. Programs\nare restricted to one second of execution time and\n1024 lines of execution trace, and will be \ufb01ltered\n6https://onlinejudge.u-aizu.ac.jp/\n7https://atcoder.jp/\nout if they exceed the limits. We also remove the\nprograms that result in runtime errors during pars-\ning or execution, by catching Python exceptions\nraised in programs. This results in a dataset of 387k\nexecutable programs, each paired with a trace.\nTo construct a large-scale dataset of executable\nprograms, we propose a mutation-based data aug-\nmentation approach. we \ufb01rst present a set of muta-\ntion operators as shown in Table 1. Most of them\ncorrespond to selected operators used in strongly\ntyped general purpose languages and are adopted\nto the Python language. Operators designed for\nPython features are also included, such as Slice\nIndex Removal (SIR) and Reverse Iteration Loop\n(RIL). Then we leverage the tree-sitter8 to convert\na program into an abstract syntax tree and then ex-\ntract its node type information to get a candidate\nlist of all mutable literals, operators and statements.\nFor each mutable candidate, we apply the related\nmutation operators with 50% probability. Speci\ufb01-\ncally, we change a numeric literal x into a random\nnumber from a Gaussian distribution with mean\nx and standard deviation 100. We either extend a\nstring with one or two random characters or shorten\na string. We randomly pick one of the three loop-\nrelated operators or keep it as it is when handling\neach loop. All operators can be applied before a\nmutated program execution, and possible mutants\nwith errors are to be detected and eliminated during\nexecution. By mutating each program 20 times, we\nobtain 3.2M deduplicated programs, each paired\nwith a trace.\nWe use the CodeNet Mutants (CodeNetMut) to\nbuild the pre-training dataset. To prevent data leak-\nage, all submissions to the same problem become\npart of the same split. We use submissions of\n710 problems with their mutants to build the pre-\ntraining dataset. Since mutation greatly enhances\ndiversity, these programs embody rich semantics\nand complex operations. Other submissions (with-\nout mutations) are used to build the validation and\ntest dataset. These human-authored programs en-\nsure the quality of evaluation data.\nB\nModel Con\ufb01gurations\nWe build our model based on 12 layers of Trans-\nformer with 768 dimensional hidden states and 12\nattention heads. We add 210 additional special\ntokens into the vocabulary to represent 200 line\nnumbers, 3 pre-training dataset names, and trace\n8https://tree-sitter.github.io/tree-sitter/\nModel\nStage1 (S1)\nStage2 (S2)\nStage3 (S3)\nCEL\nSingleLine\nTutorial\nCodeNetMut\nCodeExecutor\nSingleLine\nSingleLine (3M), Tutorial\nSingleLine (3M), Tutorial, CodeNetMut\nTable 7:\nDatasets that CEL and CodeExecutor use for three-stage pre-training. \u201cSingleLine (3M)\u201d denotes 3\nmillion instances within SingleLine that are most dif\ufb01cult for CodeExecutor to generate.\nFigure 4: An example from CodeNetMut test split, which covers all the categories of Python programming knowl-\nedge. CodeExecutor gives the correct prediction.\nstructure described in \u00a74.2. During pre-training,\nwe set the max length of input sequence and batch\nsize to be 1024 and 256, respectively. We use the\nAdam optimizer to update model parameters with\n4e-4 learning rate. We \ufb01rst employ the SingleLine\ndataset to pre-train the model with the code execu-\ntion objective for 500k steps. We then reserve 3\nmillion instances in SingleLine that are most dif\ufb01-\ncult for our model to generate and add Tutorial data\ninto the corpus, pre-training for 300k steps. We add\nCodeNetMut into the corpus and further pre-train\nfor 300k steps. We pre-train the model on a cluster\nof 16 NVIDIA Tesla V100 with 32GB memory\nand the total training time is about a month. For\ninference, we set beam search as 10.\nC\nThree-stage Pre-training\nIn table 7, we list the datasets that CodeExecutor-\nLimited (CEL) and CodeExecutor use for three-\nstage pre-training, respectively.\nThe \ufb01rst stage of pre-training for CEL uses the\nSingleLine dataset, resulting in the model CEL-S1.\nIn the second stage, CEL is initialized with CEL-S1\nand pre-trained with the Tutorial dataset, resulting\nin the model CEL-S2. In the third stage, CEL is\ninitialized with CEL-S2 and pre-trained with the\nCodeNetMut dataset, resulting in the model CEL-\nS3.\nOn the other hand, CodeExecutor is \ufb01rst pre-\ntrained with the SingleLine dataset, then the 3 mil-\nlion most challenging SingleLine data is selected\nfor later training stages based on the model\u2019s loss.\nIn the second stage, CodeExecutor is pre-trained\nwith the 3 million dif\ufb01cult SingleLine data, along\nwith the Tutorial dataset. In the third stage, Code-\nExecutor is pre-trained with the 3 million dif\ufb01cult\nSingleLine data, the entire Tutorial dataset, and the\nCodeNetMut dataset.\nD\nQualitative Examples\nAdditional examples are shown here.\nFigure 4 shows an example that covers all the\ncategories of Python programming knowledge in\nTable 4. CodeExecutor generates the same trace as\nground truth.\nFigure 5 is an example of performing division\ncalculations with decimals. CodeExecutor is able\nto produce the correct \ufb01rst \ufb01fteen digits and makes\nerrors in the remaining two digits.\nFigure 5: An example of division calculations with dec-\nimals, where CodeExecutor correctly produce the \ufb01rst\n\ufb01fteen digits, with mistakes highlighted by an under-\nline.\n"
  },
  {
    "title": "Towards Building the Federated GPT: Federated Instruction Tuning",
    "link": "https://arxiv.org/pdf/2305.05644.pdf",
    "upvote": "2",
    "text": "Towards Building the Federated GPT:\nFederated Instruction Tuning\nJianyi Zhang1\u2217, Saeed Vahidian1\u2217, Martin Kuo1\u2217, Chunyuan Li2,\nRuiyi Zhang3, Tong Yu3, Yufan Zhou3, Guoyin Wang4, Yiran Chen1\u2020\n1 Duke University, 2 Microsoft Research, 3 Adobe Research, 4 Amazon\nAbstract\nWhile \"instruction-tuned\" generative large language models (LLMs) have demon-\nstrated an impressive ability to generalize to new tasks, the training phases heavily\nrely on large amounts of diverse and high-quality instruction data (such as ChatGPT\nand GPT-4). Unfortunately, acquiring high-quality instructions, especially when it\ncomes to human-written instructions, can pose significant challenges both in terms\nof cost and accessibility. Moreover, concerns related to privacy can further limit\naccess to such data, making the process of obtaining it a complex and nuanced\nundertaking. Consequently, this hinders the generality of the tuned models and\nmay restrict their effectiveness in certain contexts. To tackle this issue, our study\nintroduces a new approach called Federated Instruction Tuning (FedIT), which\nleverages federated learning (FL) as the learning framework for the instruction\ntuning of LLMs. This marks the first exploration of FL-based instruction tuning\nfor LLMs. This is especially important since text data is predominantly gener-\nated by end users. For example, collecting extensive amounts of everyday user\nconversations in different languages can be a useful approach to improving the gen-\neralizability of LLMs, allowing them to generate authentic and natural responses.\nTherefore, it is imperative to design and adapt FL approaches to effectively leverage\nthese users\u2019 diverse instructions stored on local devices, while preserving privacy\nand ensuring data security. In the current paper, by conducting widely used GPT-4\nauto-evaluation, we demonstrate that by exploiting the heterogeneous and diverse\nsets of instructions on the client\u2019s end with the proposed framework FedIT, we\nimproved the performance of LLMs compared to centralized training with only\nlimited local instructions. Further, in this paper, we developed a Github repository\nnamed Shepherd. This repository offers a foundational framework for exploring\nfederated fine-tuning of LLMs using heterogeneous instructions across diverse\ncategories. The framework is designed for ease of use, adaptability, and scalability\nto accommodate large datasets. Additionally, it facilitates the seamless integration\nof novel algorithms and configurations, making it a convenient tool for researchers\nand practitioners in the NLP community.\n\u2217Equal contribution, work done at Duke University.\n\u2020The condensed version of this paper was accepted by ICASSP 2024. This work is supported in part by the\ngrants NSF-2112562, NSF-2332744 and ARO W911NF-23-2-0224.\nCopyright 2024 IEEE. Published in ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), scheduled for 14-19 April 2024 in Seoul, Korea. Personal use of this material\nis permitted. However, permission to reprint/republish this material for advertising or promotional purposes\nor for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted\ncomponent of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and\nPermissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA.\nTelephone: + Intl. 908-562-3966.\narXiv:2305.05644v2  [cs.CL]  29 Jan 2024\n1\nIntroduction\nLarge Language Models (LLMs) have become ubiquitous in natural language processing (NLP)\n[6, 17, 64, 65], where one single model can perform well on various language tasks, including\nestablished tasks such as text generation, machine translation, and question answering, as well as\nnovel application-oriented tasks in human daily life [19, 69, 81]. To align LLM to follow human\nintents, instruction-tuning has been proposed by fine-tuning LLM on instruction-following data\n[62, 84, 85]. Though instruction-tuning has demonstrated great effectiveness in improving the zero\nand few-shot generalization capabilities of LLM, its performance on real-world tasks is contingent on\nthe quantity, diversity, and quality of the collected instructions [58, 84]. The process of collecting\nthese instructions can be expensive [75, 84].\nBeyond the commonly acknowledged constraints of time and labor expenses, the increasing aware-\nness of data sensitivity highlights a significant challenge in acquiring extensive and high-quality\ninstructions [3, 26, 34]. For instance, collecting vast amounts of daily conversations from users is a\nvaluable means of providing guidance for LLMs, enabling them to generate authentic and genuine\nresponses. However, privacy concerns may hinder users from sharing their conversations, resulting in\na limited quantity of instructions that are not fully representative of the target population. Likewise,\nmany companies treat their instructions as proprietary assets that are closely guarded. They are\nreluctant to share their instructions with external parties, as they often contain confidential and propri-\netary information that is critical to their success and profitability [27]. For example, pharmaceutical\ncompanies rely on meticulously-crafted instructions that may include details about the chemical\ncomposition of new drugs and the results of clinical trials [35]. Hence, the sensitive nature of these\ninstructions can pose significant challenges when utilizing traditional centralized approaches for\ninstruction tuning.\nWe aim to tackle these challenges by exploring the potential of federated learning (FL) as a promising\nsolution [56]. This collaborative learning technique enables many clients to learn a shared model\njointly without sharing their sensitive data. In particular, in our proposed federated instruction-tuning,\nclients initially download a global LLM from a central server and subsequently compute local model\nupdates using their respective local instructions. These local updates are then transmitted back to\nthe server, where they are aggregated and integrated to update the global LLM. Given that clients\noften have limited computational resources in comparison to traditional centralized training cloud\nservers, which can utilize thousands of GPUs to fully fine-tune all parameters of LLMs, we resort to\nparameter-efficient tuning techniques. This leads to a significant decrease in the computational and\ncommunication demands, as it reduces the number of trainable parameters on each device.\nThus, our proposed framework enables efficient utilization of the computational resources available\non local edge devices, which are commonly accessible, as well as their diverse local instructions. This\neliminates the need for depending on large cloud servers to fine-tune LLMs. Our major contributions\nare summarized as follows:\n\u2022 We make the first attempt to leverage FL for instruction tuning (FedIT) of LLMs. In the\npredominant instruction tuning of LLMs, acquiring access to extensive and high-quality\ninstructions can present significant obstacles due to the associated costs and privacy concerns.\nIn this work, we show that we are able to circumvent the above-mentioned limitations by\nexploiting the diverse sets of available instructions (especially in cross-device FL where\nthe number of clients is of the order of billion) from the users in the FL system. To make\ndeploying LLM within an FL setup viable in terms of communication and computation cost,\nwe suggest the clients exploit the parameter-efficient tuning (PETuning) methods.\n\u2022 A comprehensive study is conducted on the heterogeneity within the FL instruction tuning.\nWe employ the GPT-4 auto-evaluation method, which has been widely utilized in related\nresearch [11, 63], to demonstrate the effectiveness of our FedIT approach in enhancing re-\nsponse quality by leveraging diverse available instructions. We discuss potential avenues for\nfuture research to improve the FL-based LLM fine-tuning methods for practical deployment.\n\u2022 We have developed and released a GitHub repository called Shepherd3, which has been\ndesigned to provide ease of customization and adaptability, thereby offering benefits for\nfuture research endeavors in this field.\n3https://github.com/JayZhang42/FederatedGPT-Shepherd\n2\n2\nRelated Work\n2.1\nInstruction tuning of Large Language Models\nInstruction tuning has emerged as a simple yet effective approach to enhance the generalizability of\nLLMs for complicated real-world tasks. This research area has recently gained increasing attention,\nparticularly since the introduction of FLAN [85] that demonstrates significant zero-shot performance,\nand Instruct-GPT [62] that aligns GPT-3 [6] to follow human intents via supervised tuning and\nRLHF [13, 70]. The development of Instruct-GPT has been instrumental in the success of ChatGPT\n[60] and GPT-4 [61].\nIn general, current research efforts can be broadly classified into two main categories based on\nthe source of instructions: (1) human-annotated task prompts and feedback [62], and (2) machine-\ngenerated instruction-following data. For the latter, self-instruct [84] is utilized, where a strong\nteacher LLM is considered to generate a comprehensive collection of instructional data that a student\nLLM can then utilize to gain alignment capabilities. Thanks to the recently open-sourced LLM\nLLaMA [77], which has demonstrated performance on par with proprietary LLMs such as GPT-3, the\nopen-source community now has ample opportunities to actively explore promising solutions to build\ntheir own LLMs capable of following language and multimodal instructions [11, 45, 63, 75, 88, 98].\nIn this line of research, it is commonly assumed that instruction-following data can be centralized,\nregardless of its sources. However, we anticipate that decentralization is becoming a prevalent trend\nin sharing and accessing instruction-following data due to its sensitivity and popularity. As such, we\npropose the first attempt to address this issue using FL.\nParameter-Efficient Fine-Tuning (PEFT)\nThe fine-tuning of LLMs aims to optimize LLMs while\nminimizing the computational and storage demands associated with the training process. Various\ninnovative methods have been proposed to achieve this goal, each with distinctive characteristics,\nincluding LoRA [31], P-Tuning [49], Prefix Tuning [42, 48], Prompt Tuning [38]. We suggest\ninterested readers to refer to the DeltaPaper repository 4 and the Delta Tuning paper [20] for a com-\nprehensive understanding of the advanced PEFT methods. We consider LoRA in our FL framework\ndue to its promising performance in recent studies on instruction tuning, including Alpaca-lora 5 and\nBaize [88]. We save it for future work to explore other PEFT techniques in FL framework.\n2.2\nFederated Learning in NLP Tasks\nFederated Learning [55] is a decentralized and collaborative machine learning technique that enables\ndata to remain on user devices. Significant research efforts have focused on addressing privacy\nand heterogeneity challenges and developing advanced FL methods [33, 52, 59, 92, 93]. These\nadvancements include designing optimization methods with improved aggregation performance\n([10, 21, 28, 66, 68, 79, 102], increasing the framework\u2019s robustness against adversarial attacks [72],\ndevising effective client selection mechanisms [12, 25, 78, 95], enhancing personalization capabilities\n[18, 40, 80, 89], and boosting the overall efficiency of FL systems [37, 39, 54, 67, 74].\nFurthermore, recent research has explored the application of FL to NLP tasks, such as Language\nModeling [29, 90], Text Classification [8, 44], Sequence Tagging [23, 32], and Dialogue Generation\n[41, 51]. Several open benchmarks and repositories support the study of federated NLP tasks,\nincluding the Leaf benchmark [7], FedNLP benchmark [43], FedML [30], FedScale [36], and FATE\n[50]. Recent research has also highlighted the importance of pretraining models for federated learning\n[9, 73, 76, 86], as they offer a more powerful initialization for training instead of starting from\nscratch. This advantage improves the convergence and robustness of FL training in the face of data\nheterogeneity. Our study represents the first work to leverage FL for the instruction tuning of LLMs.\nWe hope it could inspire the two communities to explore the intersection.\n3\nFederated Instruction Tuning\nIn light of our review in FL and instruction tuning, we now proceed to our Federated Instruction\nTuning (FedIT). This section is structured as follows: first, we introduce the overall framework\n4https://github.com/thunlp/DeltaPapers\n5https://github.com/tloen/alpaca-lora\n3\nin subsection 3.1. Subsequently, we delve into the intricacies of this framework and examine the\nheterogeneity of the instruction dataset in subsection 3.2. Following this, we discuss the parameter-\nefficient technique, LoRA, that is integrated into our framework and explore its connections with\nother federated learning algorithms. Lastly, in subsection 3.4, we present our Python-based GitHub\nrepository framework, \u201cShepherd\" to facilitate research in federated instruction studies.\n3.1\nAn Overview of FedIT\nDrawing on the successful application of FL in various machine learning domains to offer privacy\nprotection, we introduce the FedIT framework. By harnessing the advantages of FL and PEFT,\nour framework enables secure and cost-effective LLM instruction tuning. The overall framework,\nillustrated in Figure 1 and Algorithm 1, involves two primary components: local training operations\non the client side and scheduling and aggregation operations on the server side, which work together\nto ensure efficient training.\nOur FedIT framework for instruction tuning is designed to address the challenges of collecting high-\nquality data and ensuring data privacy by keeping the instructions on the local devices throughout\nthe process. By ensuring data sensitivity protection, we can encourage more clients to participate\nin the federated instruction tuning. Consequently, the combined instruction dataset from all clients\ncan encompass a broader range of topics, tasks, and valuable information, as clients may come from\ndifferent areas and possess domain-specific expertise. This FL approach enables our framework to\neffectively adapt to diverse and evolving instruction datasets, resulting in more robust and generalized\nLLM performance. Moreover, our FedIT methodology incorporates a parameter-efficient finetuning\ntechnique, known as LoRA, to facilitate local training. This method reduces computational and\ncommunication overheads for local edge devices that have limited system resources. As a result, we\ncan leverage the computational capabilities of a multitude of distributed local edge devices that are\noften disregarded in conventional centralized instruction tuning. This feature enhances the scalability\nof our FedIT solution, enabling it to address large-scale instructional tuning challenges effectively.\nThe framework assigns an LLM to each client and performs client selection to determine which\nclients will participate in local instruction tuning. During instruction tuning, clients use their local\ninstruction dataset to update a small, trainable adapter that is added to the pre-trained model weights.\nThis approach reduces the cost of fine-tuning and makes it compatible with the limited computational\nresources of local devices. Upon completion, clients send the updated adapter back to the server,\nwhich aggregates the received adapters\u2019 parameters and conducts another round of client selection.\nThis iterative process continues until convergence is achieved.\nWe also wish to emphasize the importance of client selection. In real-world settings, not all clients\nmay be accessible for local instruction tuning, as local device processors may be occupied by other\ntasks. Hence, client selection can come into play to better simulate a real-world scenario. The\nserver can actively choose clients for training based on their distinct instructions and computational\nresources, thus improving the overall efficiency of the FL framework by identifying clients that best\nrepresent the overall data distribution.\n3.2\nHeterogeneity of Instructional Data\nBeyond the practical benefits of FedIT, our research makes a unique contribution by presenting a\nscenario for instruction tuning of LLMs where statistical heterogeneity can serve as a positive factor\nfor federated learning. Our work demonstrates that the extensive heterogeneous and diverse set\nof instructions available from multiple languages can, in fact, be a blessing factor for our FedIT\napproach. For instance, different clients may have different instruction tasks, such as open-domain\nQA and writing. The content and format of these instructions can be substantially different. For\nexample, QA tasks typically require fact-based questions and answers, while writing tasks involve\ninstructions for generating coherent and meaningful sentences.\nIn order to obtain a comprehensive understanding of data heterogeneity inherent in the instructional\ndataset utilized for this study, we performed an in-depth examination of the Databricks-dolly-15k6\ndataset. This publicly accessible dataset, consisting of instruction-following records generated by\na multitude of Databricks employees, spans a range of behavioral categories as outlined in the\n6https://huggingface.co/datasets/databricks/databricks-dolly-15k\n4\nFL Training Process\nA1, B1\nAn ,Bn\nC1\nC2\nCn\n\u2026\n\u2026\nA2 ,B2\nClient local instructions\nClient local LLM\nModel update by LORA module (A, B)\nTask: \nOpen_qa\nInstruction: What happens when the sun goes down?\nResponse: When the sun sets, the evening starts.\nContext: \u201c \u201d\nHeterogeneous Instructions\nInstruction: Please summarize what Linkedin does.\nResponse: Linkedin is a social platform that business\u2026\nContext: \u201cLinkedIn (/l\u026a\u014bkt\u02c8\u026an/) is a business and\u2026\u201d\nServer \n\u2026\nInstruction: Why mobile is bad for human\nResponse: We are always engaged one phone which\nis not good.\nContext: \u201c \u201d\nTask: \nBrainstorming \nTask: \nSummarization \nFigure 1: The framework of Federated Instruction Tuning (FedIT)\nAlgorithm 1 Federated Instruction Turning (FedIT)\nInitialization: each client\u2019s initial global large language model with parameters w and a lightweight\nadapter with parameters \u2206w(0), client index subset M = \u2205, K communication rounds, k = 0,\nTraining\nwhile k \u2264 K do\nServer updates M using specific strategies\n\u25b7 Select clients for local training\nfor n \u2208 M in parallel do \u25b7 Parameter-efficient finetuning on local instructions dataset\nClient freeze the LLM and update the adapter weights with \u2206w(k)\n\u2206w(k+1)\nn\n\u2190 InstructionTuning(\u2206w(k)\nn )\nend For\n\u2206w(k+1) \u2190 Aggregate(\u2206w(k+1)\nn\n) for n \u2208 M\n\u25b7 Aggregate the adapters at Server\nk \u2190 k + 1\nend while\nOutcome (m, \u03b8t\ng):\nDerive the final adapter with parameters \u2206w(K) and the global LLM with parameters w\nInstructGPT paper [62]. These categories encompass brainstorming, classification, closed QA,\ngeneration, and more. To emulate an FL environment with ten clients, we partitioned the entire\nDatabricks-dolly-15k dataset into ten shards using a widely adopted partitioning method [30, 36, 94],\nwith each shard assigned to an individual client. The category distribution for each client\u2019s instruction\ndataset is illustrated in Figure 2a. As is evident in the figure, each user\u2019s dataset contains imbalanced\ncategories of instructions, with some categories absent entirely. This reflects real-world scenarios\nwhere users may not possess expertise across all instruction categories. In the absence of FedIT, due\nto the challenges associated with collecting sensitive instruction data, the model can only be trained\non the local instruction dataset of each user, as depicted in the left subfigure of Figure 2b. However,\nby implementing our FedIT approach, the model can be trained on the local instruction datasets of all\nclients, as illustrated in the right subfigure of Figure 2b. As a result, FedIT allows for instruction\ntuning on a dataset with enhanced diversity, and a larger number of data points, encompassing\nthe complete Databricks-dolly-15k dataset. The distribution of this dataset is shown in the right\nsubfigure of Figure 2b. Comprising eight unique categories with varying numbers of instructions,\nthis dataset offers increased diversity, allowing the model to be more generalized and applicable to a\nwider array of tasks compared to training solely on each client\u2019s local instruction dataset with limited\ncategories and quantity.\nIn addition to task categories, language diversity adds a new dimension of heterogeneity to Federated\nInstruction Tuning. In real-world applications, LLMs are typically designed for multilingual capabili-\nties to cater to users from diverse regions and countries who speak various languages. Addressing\n5\nthe needs of a multilingual audience poses several challenges. Apart from understanding all the\nlanguages in the dataset, achieving fairness across languages, particularly for those underrepresented\nin the instruction dataset, is a crucial aspect that warrants further investigation in Federated Instruc-\ntion Tuning. Moreover, domain-specific instructions further compound the heterogeneity of the\nframework. Different domains have distinct contexts, each characterized by unique terminologies and\nsentence structures. For instance, legal or pharmaceutical contexts may require specific vocabulary\nand phrasing that do not apply to other domains. Additionally, there are other sources of heterogeneity,\nsuch as task complexity, task ambiguity, emotional tone, cultural factors, and more, which merit\nfurther investigation.\nOQAGQA\nC\nCQA\nB\nIE\nS\nCW\n0\n200\n400\n600\n800\n1000\n149\n10.0%\n598\n40.1%\n746\n50.0%\nClient #0\nOQAGQA\nC\nCQA\nB\nIE\nS\nCW\n0\n200\n400\n600\n800\n1000\n747\n49.3%\n767\n50.7%\nClient #1\nOQAGQA\nC\nCQA\nB\nIE\nS\nCW\n0\n200\n400\n600\n800\n1000\n377\n25.2%\n747\n50.0%\n370\n24.8%\nClient #2\nOQAGQA\nC\nCQA\nB\nIE\nS\nCW\n0\n200\n400\n600\n800\n1000\n985\n66.0%\n507\n34.0%\nClient #3\nOQAGQA\nC\nCQA\nB\nIE\nS\nCW\n0\n200\n400\n600\n800\n1000\n747\n49.7%\n757\n50.3%\nClient #4\nOQAGQA\nC\nCQA\nB\nIE\nS\nCW\n0\n200\n400\n600\n800\n1000\n746\n49.3%\n767\n50.7%\nClient #5\nOQAGQA\nC\nCQA\nB\nIE\nS\nCW\n0\n200\n400\n600\n800\n1000\n362\n24.2%\n747\n50.0%\n385\n25.8%\nClient #6\nOQAGQA\nC\nCQA\nB\nIE\nS\nCW\n0\n200\n400\n600\n800\n1000\n746\n50.0%\n483\n32.4%\n264\n17.7%\nClient #7\nOQAGQA\nC\nCQA\nB\nIE\nS\nCW\n0\n200\n400\n600\n800\n1000\n325\n21.8%\n468\n31.3%\n701\n46.9%\nClient #8\nOQAGQA\nC\nCQA\nB\nIE\nS\nCW\n0\n200\n400\n600\n800\n1000\n747\n49.0%\n777\n51.0%\nClient #9\n(a) The distribution of instruction dataset categories across each client\u2019s dataset.\nC\nCQA\nS\nOQA\nC\nOQA\nGQA\nIE\nOQA\nS\nCQA\nIE\nOQA\nGQA\nGQA\nB\nIE\nOQA\nC\nB\nGQA\nCQA\nCW\nC\nB\nClient #0 (1493)\nClient #1 (1514)\nClient #2 (1494)\nClient #3 (1492)\nClient #4 (1504) Client #5 (1513)\nClient #2 (1494)\nClient #0 (1493)\nClient #2 (1494)\nClient #9 (1524)\n24.0%\n(3611)\n14.6%\n(2191)\n14.2%\n(2136)\n12.1%\n(1823)\n11.8%\n(1768)\n10.1%\n(1512)\n8.4%\n(1263)\n4.7%\n(711)\nFedIT\nCategories\nOpen QA(OQA)\nGeneral QA(GQA)\nClassification(C)\nClosed QA(CQA)\nBrainstorming(B)\nInformation Extraction(IE)\nSummarization(S)\nCreative Writing(CW)\n(b) Training on a Higher-Quality, More Diverse Dataset with FedIT\nFigure 2: Illustrate the heterogeneity of FedIT with Databricks-dolly-15k instruction dataset. The\nmodel can be trained on only the particular local instruction categories of each user (bottom left), or\non the local instruction datasets of all clients with greater diversity and quantity of data points that\ncover the entire range of the subject matter by implementing our FedIT (bottom right).\n6\n3.3\nParameter Efficiency in Federated Instruction Tuning\nTaking into account the limited computational capabilities of local devices, which are unable to support\nfull fine-tuning of a large language model, it is crucial to implement a parameter-efficient fine-tuning\nstrategy that leverages local computational resources. For a weight matrix W0 \u2208 Rd\u00d7k belonging to a\nlarge pre-trained LLM, the method we adopt, Low-Rank Adaptation (LoRA) method, freezes W0 and\nconstrains its update \u2206W by representing it using a low-rank decomposition W0 +\u2206W = W0 +BA,\nwhere B \u2208 Rd\u00d7r, A \u2208 Rr\u00d7k are two trainable parameters, and the rank r \u226a min(d, k). For a linear\nlayer h = W0x, the modified forward pass is given by:\nh = W0x + BAx\nCompared to fully fine-tuning the LLM, LoRA considerably decreases the number of trainable\nparameters. Please refer to Section 4.1 and Table 1, which present the parameter counts for each\nmodel and the corresponding memory costs.\nOnce the local parameter-efficient fine-tuning with LoRA is completed, clients only need to transmit\nthe B and A matrices of parameters to the server, significantly reducing communication costs\ncompared to sending updates for all LLM parameters. Finally, the central server aggregates these\nlocal matrices of parameters into a new global model parameter by FedAvg. Notably, LoRA does not\nintroduce any additional latency of inference compared to fully fine-tuning the model when deployed\nin production, as we can explicitly compute, store W0 + BA, and conduct inference as usual.\nIt is important to note that the LoRA method we employ is scalable to accommodate varying system\nresources. If a specific client\u2019s communication or computational resources are significantly lower\nthan others, it can adjust its LoRA configurations by reducing the number of matrix W0 elements,\nwhich will be decomposed into low-rank A, B. Alternatively, it can also opt to decrease the rank r of\nA and B. Optimizing the aggregation process in the presence of substantial system heterogeneity\nwithin the FL framework, where different clients have distinct LoRA configurations, is an intriguing\nresearch topic to explore further.\n3.4\nShepherd: A GitHub Platform for FedIT Support\nWe introduce Shepherd7, a lightweight framework designed to implement Federated Parameter-\nEfficient Instruction Learning. Shepherd supports ongoing research in this area, as well as other\nNLP tasks, by providing a user-friendly and scalable platform capable of handling large datasets.\nThe framework allows for seamless integration of innovative algorithms and configurations and is\ncompatible with a range of recent popular large language models, such as Stanford Alpaca [75],\nVicuna [11], Pythia [5], Dolly 8, Baize [88], and Koala [24], among others. The Shepherd pipeline\nconsists of four main components: 1) client data allocation, 2) client participation scheduling, 3)\nsimulated local training, and 4) model aggregation.\nClient Data Allocation\nTo simulate the real-world scenario where each client has its\nunique dataset, we employ a \"synthetic\" partitioning process, which is implemented in the\nclient_data_allocation.py module. We offer two methods to replicate the non-independent\nand identically distributed (non-i.i.d) nature of the clients\u2019 datasets. In the first approach, we allocate\nn-class training data to each client, with the number of classes differing across clients, resulting in\nunbalanced class sizes. Despite this imbalance, the volume of data in each client\u2019s dataset is roughly\nequivalent. The second approach is similar to the first but stands out by having significantly varying\ndata volumes across each client\u2019s dataset.\nClient Participation Scheduling\nThe process of selecting clients to participate in the training\nis crucial and implemented in the fed_util/sclient_participation_scheduling.py module.\nOur vanilla version of Shepherd employs a random selection approach, and we aim to enhance the\nclient selection strategy with efficiency-driven methods that address data and system heterogeneity,\nsuch as those proposed in [37, 95].\n7https://github.com/JayZhang42/FederatedGPT-Shepherd\n8https://github.com/databrickslabs/dolly\n7\nSimulated Local Training\nThis core component of our Fed-PEIT framework is implemented in\nthe fed_util/client.py module. In real-world scenarios, all selected clients perform their local\ntraining simultaneously, which can be computationally expensive to simulate. To make it feasible for\nresearchers with limited resources, our framework conducts the local training of clients sequentially,\none at a time. To implement the LoRA method, we utilize the PEFT package [53] and the Alpaca-lora\nrepository 9 to encapsulate the frozen, original pre-trained model with LoRA configurations, enabling\nmore efficient parameter-efficient fine-tuning for our Shepherd framework.\nmodel = get_peft_model (model , LoRA_config)\nTo aid future researchers in understanding and implementing our framework, we have defined a\nPython class, GeneralClient, which represents a client in the Federated Learning (FL) training\nprocess and includes attributes that represent the specific client\u2019s required information.\nclass\nGeneralClient:\ndef\n__init__(self , model , ** args):\nself.model = model\nWe have also defined several methods for GeneralClient that conduct important components of\nthe local training process.\ndef\npreprare_local_dataset (self , ** args):\n...\nself. local_train_dataset =\n...\nself. local_eval_dataset\n=\n...\nThis method entails the preparation of the local dataset for the client by reading data from the specified\ndata path and transforming it using the required tokenizer and prompt. Its design allows for ease\nof use with new datasets and supports the exploration of various prompts and tokenizers for future\nresearch purposes.\ndef\nbuild_local_trainer (self , ** args):\n...\nself.local_trainer= transformers.Trainer(self.model , **\nargs)\nThis method constructs a local trainer for client-side training by leveraging the Hugging Face Trainer.\nThis approach allows for the design of customized and efficient training configurations with tailored\narguments based on specific requirements.\ndef\ninitiate_local_training (self):\n...\nThis method encompasses the preparatory steps for training. In our vanilla implementation, we\ncreate and modify certain attributes of the GeneralClient class for the convenience of recording\ninformation related to the model in parameter-efficient learning. It allows for the integration of\ncustom functions for various purposes in future applications.\ndef train(self):\nself.local_trainer.train ()\nThis method executes local training by leveraging the capabilities of the established local trainer.\ndef\nterminate_local_training (self , ** args):\n...\nreturn\nself.model , ...\nThe terminate_local_training method signifies the conclusion of the local training process. It saves\nthe locally trained model parameters and updates relevant information associated with the local\ntraining session.\n9https://github.com/tloen/alpaca-lora\n8\nTable 1: Prompt Template\nTemplate\nPrompt Input\nBelow is an instruction that describes a task, paired with an input that provides\nfurther context. Write a response that appropriately completes the request.\nInstruction: {instruction}\nInput: {input}\nResponse:\nPrompt No Input\nBelow is an instruction that describes a task. Write a response that appropri-\nately completes the request.\nInstruction: {instruction}\nResponse:\nModel Aggregation\nThis component is responsible for the combination of trained client mod-\nels into a single global model, with the objective of producing a more generalized and accurate\nmodel. In our parameter-efficient setting, model aggregation involves combining only the train-\nable parameters specified by the LoRA configuration instead of all the parameters of LLM to\nreduce computational and communication costs. The module for this component is implemented\nin fed_util/model_aggregation.py, which provides a platform for the adoption of various\nfederated optimization methods, including FedAvg [55].\nIn its current form, our Shepherd framework presents a fundamental and accessible vanilla version\ndesigned for ease of understanding and modification. In future iterations, we plan to expand the\nframework by incorporating more complex functionalities, such as novel client selection strategies\n[12, 25, 78, 95] and advanced optimization methods [10, 68, 79]. We also aim to support additional\ninstruction datasets and enable a wider range of NLP tasks. Furthermore, we believe that the\nframework\u2019s practicality in real-world scenarios can be significantly improved by integrating advanced\nsystem simulations that account for various factors such as computing time delays, communication\nlatencies, overheads, and bandwidth limitations.\n4\nQualitative Study\n4.1\nImplementation details\nIn our FL setup, we assume the presence of 100 clients. We proceed to apply the Shepherd frame-\nwork\u2019s second data partitioning technique to divide the residual data from the Databricks-dolly-15k\ndataset into 100 distinct portions. Each of these portions corresponds to an individual client\u2019s local\ninstruction dataset. We conduct a total of 20 communication rounds, with each round involving the\nrandom selection of 5 (5%) clients for training. Each client performs one epoch of local training with\ntheir respective instruction datasets on a single Nvidia Titan RTX with 24GB memory. We initialize\nthe model with the 7B LLaMA model. The model remains frozen during training, thereby reducing\nGPU memory usage and enhancing training speed. In alignment with Baize\u2019s settings [88], we apply\nLoRA to all linear layers with a rank of 8 to boost adaptation capabilities. Following [31], we use\nrandom Gaussian initialization for A and set B to zero, ensuring that the value of BA is zero at the\nbeginning of training. We employ the Adam optimizer to update LoRA parameters with a batch size\nof 32 and a learning rate of 1.5e-4. We set the maximum input sequence length to 512 and provide\nthe template of the prompt adopted from Alpaca-lora in Table 1. The implementation of FedIT is\ncompleted utilizing our repository, Shepherd, and the derived model is referred to as Shepherd-7B.\nWe detail the number of model parameters, training time, and GPU memory consumption in Table 2.\n9\nTable 2: Numbers of parameters (frozen&trainable), training time, and GPU memory cost on a single\nNvidia Titan RTX\nModel\nOrig. Param\nAdapt. Param\nTrainable\nTraining Time\nGPU Memory\nShepherd-7B\n7B\n17.9M\n0.26%\n2 hours\n23GB\n4.2\nQualitative Study with Automatic Evaluation\nFollowing the same evaluation approach of the Vicuna project [11] and GPT-4-LLM [63], we use\nGPT-4 to automatically assess the responses generated by our Shepherd-7B model and other baseline\nmodels on 20 unseen questions randomly sampled from the evaluation set of the Vicuna project [11],\nwhich pertain to unseen categories during the training, such as \"counterfactual question,\" \"femir\nquestion,\" \"math question\" and others. Each model produces one response per question, and GPT-4\nrates the response quality between the two models on a scale of 1 to 10. To minimize the impact of\nrandomness in GPT-4\u2019s scoring, we force it to rate each response pair three times and then average\nthe ratings. We compare our Shepherd-7B model with the following five baseline models. The\nfirst baseline model is a 7B LLaMA model without fine-tuning on the Databricks-dolly-15k dataset,\ndenoted as LLaMA. Comparison with this baseline demonstrates the improvement in response quality\nthrough the use of our FedIT framework. The subsequent three baseline models are three 7B LLaMA\nmodels fine-tuned on three different individual clients\u2019 local datasets for one epoch without model\naggregation in FL. The comparison between these models and ours highlights the advantages of\nutilizing diverse instruction datasets from multiple clients in our methodology. \"Local-1\" focuses on\nthe brainstorming task solely, \"Local-2\" on the closed question answering task, and \"Local-3\" on\nclassification and brainstorming tasks. The final strong baseline model, dubbed as \"CentralizedModel\n,\" is fine-tuned with the entire Databricks-dolly-15k dataset for one epoch, representing the ideal\ncentralized training scenario where the server could collect all clients\u2019 instructions. This serves as\nan upper bound, as we aim for FL to achieve comparable performance to centralized training in the\nfuture.\nWe apply the GPT-4 automatic evaluation on the responses generated by our model Shepherd-7B and\nother baseline models. We list the averaged scores provided by GPT-4 in Table 3.\nTable 3: A summary of the baselines and their corresponding scores evaluated by GPT-4. The scores\nare reported in the format of (Baseline\u2019s score, Shepherd-7B\u2019s score) and the Relative Score is defined\nas ( Shepherd-7B\u2019s score / Baseline\u2019s score)\nBaseline\nTask\nScores\nRelative Score\nCentralizedModel\nCentralized tuning with all the instructions\n(142.2, 130.7)\n0.919\nLLaMA\nNo instruction tuning\n(114.0, 131.7)\n1.155\nLocal-1\nBrainstorming instruction tuning\n(120.0, 131.0)\n1.092\nLocal-2\nClosed question answering instruction tuning\n(116.1, 129.0)\n1.111\nLocal-3\nClassification and brainstorming instruction tuning\n(121.3, 131.8)\n1.087\nAs demonstrated in Table 3, the performance of our proposed model, Shepherd-7B, significantly\nsurpasses that of the LLaMA model. This result serves as evidence that our FedIT approach is\nindeed effective. When compared to other baseline models, which are fine-tuned solely on local\ninstruction datasets, Shepherd-7B achieves considerably higher scores. This underlines the benefits of\nleveraging diverse instruction datasets from multiple clients in our FL approach, emphasizing that the\nheterogeneity and diversity of instructions within the FL framework can be advantageous to adopt the\nLLMs to different unseen tasks. However, a comparison with the robust CentralizedModel baseline\nreveals that our model still has room for improvement. This disparity is partly attributed to the\nfact that the local models aggregated at the server side are trained on instructions with substantially\ndifferent distributions, which can cause their local models to learn different representations of\n10\n50\n60\n70\n80\n90\n100\n110\nRelative Score (%)\nCentralizedModel\nLLaMA\nLocal-1\nLocal-2\nLocal-3\nShepherd-7B (ours)\n139.0/172.7    81%\n118.3/176.0    67%\n125.3/173.7    72%\n123.7/173.7    71%\n125.3/173.3    72%\n133.3/175.7    76%\nChatGPT\nCentralizedModel\nLLaMA\nLocal-1\nLocal-2\nLocal-3\nShepherd-7B (ours)\nFigure 3: The relative scores of all models against ChatGPT(GPT-3.5-turbo)\nthe instructions.Consequently, there is a need for further exploration of more efficient federated\noptimization methods and client scheduling methods, such as FA-LD [16] and FedCBS [95], which\nare inspired by bayesian sampling methods [47, 87, 96, 97, 99, 100], to enhance the aggregation\nprocess. In conclusion, as discussed in Section 3.2, statistical heterogeneity can be a beneficial factor\nfor FedIT, as it enhances the diversity of instruction data, thus improving the model\u2019s generalization\nability to unseen tasks. However, to fully utilize the benefits of data heterogeneity, advanced federated\noptimization methods need to be developed and integrated to manage and leverage heterogeneity\nmore effectively.\nTo evaluate the practical significance of this research, we further compare our proposed model, as\nwell as the baseline models, with established industry products such as ChatGPT. In line with our\nultimate goal of developing federated GPT models, we utilized GPT-4 auto-evaluation to compare\nthe responses of these models with the response of GPT-3.5-turbo (ChatGPT). The resulting Relative\nScores over ChatGPT are presented in Figure 3. As can be seen, our method achieves superior\nperformance compared to all baselines except the Centralized model, which supports its potential\nto effectively address future product development scenarios where instruction data may be scarce\ndue to the difficulties of collecting sensitive data. Overall, this evaluation highlights the value and\napplicability of our approach to real-world scenarios.\n4.3\nQualitative Study with Example Demonstration\nIn this section, we demonstrate the efficacy of FedIT by presenting examples that highlight the\ncomparative performance of our model against various baseline models. The first example is derived\nfrom the 20 unseen questions in [11], while the second example is sourced from the Databricks-\ndolly-15k dataset.\nAs illustrated in Table 4, our Shepherd-7B model generates a more accurate and pertinent response\ncompared to Local-3, which is also more detailed than LLaMA\u2019s response. Our response outlines\npotential discoveries Newton could have made in biology, such as evolution, genetics, cellular biology,\nimmunology, infectious diseases, and vaccines, whereas Local-3 incorporates extraneous information,\nrendering its response less relevant to the question. As assessed by GPT-4, the Local-2 response also\ndiscusses the potential implications of Newton concentrating on biology but predominantly empha-\n11\nTable 4: Response demonstration for a counterfactual question from [11]\nWhat if Isaac Newton had focused on biology instead of physics?\nCentralizedModel\nIf Isaac Newton had focused on biology, he likely would have dis-\ncovered key biological concepts such as evolution, genetics, and\ncellular biology. He might also have made important discoveries\nin immunology, infectious diseases, and vaccines. Isaac Newton\u2019s\nphysics theories changed the way we think about the laws of motion.\nIf he had focused on biology instead, he may not have made the\ndiscoveries that he did...\nLLaMA\nIf Isaac Newton had focused on biology instead of physics, he may\nhave discovered DNA, which could have had a profound impact on\nmedicine.\nLocal-1\nBiology is a much broader field than physics, and there are many\ndifferent subfields that Newton could have focused on, like the study\nof living organisms.\nLocal-2\nIsaac Newton was a physicist, mathematician, astronomer, a natural\nphilosopher, and a member of the Royal Society. But if he had\nfocused on biology instead of physics, he would have been a biologist,\nand we would have been much more likely to have found vaccines\nfor diseases like polio and smallpox, and we would have a much\nbetter understanding of the causes and treatment of some of the most\ndevastating illnesses of the 20th century.\nLocal-3\nIf Isaac Newton had focused on biology instead of physics, he may\nhave discovered that living organisms are made of carbon. As we\nnow know, carbon is an essential component of all living organisms,\nand it is even found in the DNA of every living cell. He would then\nhave discovered that carbon can be found in the form of coal, oil,\nnatural gas, and methane. 1. What are the key differences between\nrenewable and non-renewable energy sources? 2. What if Isaac\nNewton had focused on biology instead of physics? 3. What would\nhave happened if Isaac Newton had never focused on physics? ...\nShepherd-7B (ours)\nIf Isaac Newton had focused on biology, he likely would have dis-\ncovered key biological concepts such as evolution, genetics, and\ncellular biology. He might also have made important discoveries in\nimmunology, infectious diseases, and vaccines.\nChatGPT\nIf Isaac Newton had focused on biology instead of physics, his\ncontributions would have been different. Biology is a completely\ndifferent field of study, and Newton\u2019s expertise in physics may not\nhave been easily transferable to biology. However, Newton was a\nbrilliant intellectual, and his approach to scientific investigation was\ncharacterized by rigorous and systematic methodology. It is possible\nthat if he had focused on biology, he could have made significant\ncontributions in the field by applying his scientific method, logic,\nand insight to biological questions. It is impossible to know what\nexactly his contributions to biology might have been, but there is no\ndoubt that he would have made significant contributions in any field\nhe chose to study.\n12\nsizes vaccines and the understanding of disease causes and treatments. Although this speculation is\nintriguing, it is less focused on specific biological areas than our response, which emphasizes the\npotential areas of biological research where Newton might have made significant contributions. More-\nover, it briefly mentions Newton\u2019s actual background, which is not directly related to the question but\nprovides context.\nEven though baseline Local-1 is primarily fine-tuned on brainstorming instructions that share similar-\nities with counterfactual QA, since they both involve creative thinking and deal with hypothetical\nsituations, its response lacks depth and does not discuss the potential impact of Newton\u2019s focus on\nbiology. Counterfactual QA typically evaluates or analyzes past events, involving questions about\nalternative outcomes, necessitating an understanding of the factors leading to a specific event outcome\n[57]. This distinction from merely producing novel ideas or solutions without assessing past events\nas seen in brainstorming, highlights the necessity for LLMs to possess other capabilities such as\nsummarization, information extraction, and creative writing. Consequently, this emphasizes the\nsignificance of diverse instruction tuning for LLMs and illustrates the advantages of our methodology.\nTable 5: Response demonstration for a classification task from Databricks-dolly-15k dataset. The\nfact is that Binzasara is a percussion instrument and the Calabrian Lira is a stringed instrument.\nIdentify which instrument is string or percussion: Binzasara, Calabrian Lira\nCentralizedModel\nCalabrian Lira is string, Binzasara is percussion.\nLLaMA\nBinzasara is a string instrument. Calabrian Lira is a percussion\ninstrument.\nLocal-1\nBinzasara is string, Calabrian Lira is percussion.\nLocal-2\nBinzasara is string, Calabrian Lira is percussion.\nLocal-3\nCalabrian Lira is a string instrument and Binzasara is percussion.\nShepherd-7B (ours)\nCalabrian Lira is string, Binzasara is percussion.\nChatGPT\nBased on a quick internet search, both the Binzasara and Calabrian\nLira are stringed instruments.\nAlthough there remains room for improvement in our model\u2019s quality when compared to the Cen-\ntralizedModel and ChatGPT, the performance gap between our model and ChatGPT does not imply\nthat our model is consistently inferior. As evidenced in Table 5, our response accurately addresses\nthe question, while ChatGPT fails. Interestingly, ChatGPT falsely claims to have conducted an\ninternet search, which diminishes the response\u2019s helpfulness and honesty. In contrast, our model and\nLocal-3, which have encountered similar classification instructions, excel at this task. This result\nalso emphasizes the importance of diversity for LLM instruction tuning. We believe that as valuable\ninstructions become increasingly difficult and costly to collect due to sensitivity or other factors, our\nFedIT approach will find broader applications and add significant value to the development of LLMs.\n5\nFuture Directions\n5.1\nComputation and Communication Overhead\nDeploying LLM in FL poses major challenges in terms of the colossal communication cost and the\ncomputational and storage overhead of local clients. FL faces significant communication challenges\nas it requires frequent exchanges of model information (parameters or gradients) among distributed\nclients and services. When it comes to using FL for LLM, the communication overhead becomes\neven more significant, with gigabit-level data transmissions necessary to achieve centralized training\nperformance. This level of communication overhead is not acceptable for FL systems. Furthermore,\nlocal clients may not have the computing power to fine-tune the entire LLM, and storing different\n13\ninstances for various tasks is also memory-intensive. As a result, it is crucial to develop appropriate\nLM-empowered FL methods that can work within the constraints of communication and resources.\nInspired by this, proposing new parameter-efficient tuning (PETuning) methods such as Prefix-\ntuning [42], LoRA [31], and BitFit [91] which are tailored for FL systems and yield competitive\nresults can be a direction for future works. Those methods can naturally be a remedy for the\ncommunication and resource constraints mentioned above.\n5.2\nPrivacy\nFL has gained popularity in privacy-sensitive NLP applications due to its ability to preserve privacy,\nespecially when the client\u2019s data is highly sensitive and cannot be transmitted outside their device.\nEssentially, with preserving a notion of privacy, FL has emerged as a preferred approach for privacy-\nsensitive NLP tasks such as medical text tasks [71], and financial text classification [4]. The\nadvancement of large language models (PLMs) has created an opportunity to use FL in privacy-\nsensitive NLP applications by combining the two techniques. The progress made in PLMs has made\nit possible to consider the combination of PLMs and FL as a viable and promising solution.\nHowever, LLMs in FL pose distinctive core challenges, one of which is the potential of malicious\nclients polluting the FL process by injecting crafted instructions. Such instructions can lead to biased\nor suboptimal models. To fully unpack the benefits of FL to LLM, the mentioned concerns should\nbe addressed. Therefore, designing methods for robust aggregation and outlier detection techniques\nthat can detect and exclude clients with abnormal behavior particular to LLM can be an interesting\ndirection for future work in using FL for LLM.\n5.3\nPersonalization\nWith deploying FL in LLM, due to the differences among the language data (instructions) used in\ndistributed clients and averaging of learning updates across a decentralized population, personalization\nbecomes a critical requirement for FL systems [51]. The former can be further complicated by\nlanguage diversity, domain-specific instructions, task complexity, emotional tone, cultural factors,\netc., which are new aspects of heterogeneity [46, 86]. For instance, in multilingual applications,\nfairness across languages, especially for languages with fewer data samples, is essential but hard to\nachieve [82, 86]. In domain-specific contexts, distinct sentence structures add to the heterogeneity\nof the framework, requiring proposing new personalization methods to ensure the efficacy of the\nlanguage model. Methods, such as meta-learning [2, 14, 22] or few-shot learning [15, 83], that\ncombine personal embeddings with shared context embeddings, and preference embeddings, that\nfacilitate personalization without the need for backpropagation, etc. have the potential to revolutionize\nthe field of NLP.\n5.4\nDefense Against Attacks\nRecent research has highlighted the possibility of recovering text from the gradients of language\nmodels [3, 26]. This vulnerability can also arise due to the models\u2019 tendency to memorize their\ntraining data and can result in the inadvertent disclosure of sensitive information. In the context of\nFL, this issue becomes particularly concerning, as malicious users can leverage this vulnerability\nto extract local sensitive texts using various techniques. Although different methods, including\ngradient pruning [101] and Differentially Private Stochastic Gradient Descent (DPSGD) [1] have\nbeen proposed as defense mechanisms against these attacks, they often come at the cost of significant\nutility loss [26]. To address this issue, future research could explore more sophisticated defense\nstrategies that are specifically tailored to the characteristics of text data.\n6\nConclusion\nWe have explored for the first time the use of FL for the instruction tuning of LLMs. This is\nespecially crucial when instructional data is primarily generated by end-users who prefer not to\nshare the data. Collecting large amounts of diverse user conversations in various languages can\nbe an effective approach to improving the generalizability of LLMs. We assess the effectiveness\nof large language models by utilizing a diverse and varied range of instructions on the client-side.\n14\nThis method proves to enhance the model\u2019s performance when compared to fine-tuning using a\nlimited set of instructions. Additionally, we introduce Shepherd, a GitHub repository designed for\nexploring federated fine-tuning of LLMs using heterogeneous instructions across diverse categories.\nThe framework is user-friendly, adaptable, and scalable to accommodate large datasets and models.\nReferences\n[1] Mart\u00edn Abadi, Andy Chu, Ian J. Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal\nTalwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM\nSIGSAC Conference on Computer and Communications Security, Vienna, Austria, October\n24-28, 2016, pages 308\u2013318. ACM, 2016.\n[2] Omid Aramoon, Pin-Yu Chen, Gang Qu, and Yuan Tian. Meta federated learning. arXiv\npreprint arXiv:2102.05561, 2021.\n[3] Mislav Balunovic, Dimitar Dimitrov, Nikola Jovanovi\u00b4c, and Martin Vechev. Lamp: Extracting\ntext from gradients with language model priors. Advances in Neural Information Processing\nSystems, 35:7641\u20137654, 2022.\n[4] Priyam Basu, Tiasa Singha Roy, Rakshit Naidu, and Z\u00fcmr\u00fct M\u00fcft\u00fcoglu. Privacy enabled finan-\ncial text classification using differential privacy and federated learning. CoRR, abs/2110.01643,\n2021.\n[5] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff,\nAviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large\nlanguage models across training and scaling, 2023.\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. Advances in neural information processing systems, 33:1877\u2013\n1901, 2020.\n[7] Sebastian Caldas, Peter Wu, Tian Li, Jakub Kone\u02c7cn`y, H Brendan McMahan, Virginia\nSmith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint\narXiv:1812.01097, 2018.\n[8] Yatin Chaudhary, Pranav Rai, Matthias Schubert, Hinrich Sch\u00fctze, and Pankaj Gupta. Feder-\nated continual learning for text classification via selective inter-client transfer. arXiv preprint\narXiv:2210.06101, 2022.\n[9] Hong-You Chen, Cheng-Hao Tu, Ziwei Li, Han-Wei Shen, and Wei-Lun Chao. On pre-training\nfor federated learning. arXiv preprint arXiv:2206.11488, 2022.\n[10] Wei Chen, Kartikeya Bhardwaj, and Radu Marculescu.\nFedmax: mitigating activation\ndivergence for accurate and communication-efficient federated learning.\narXiv preprint\narXiv:2004.03657, 2020.\n[11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n[12] Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Client selection in federated learning: Convergence\nanalysis and power-of-choice selection strategies. ArXiv, abs/2010.01243, 2020.\n[13] Paul Francis Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario\nAmodei. Deep reinforcement learning from human preferences. ArXiv, abs/1706.03741, 2017.\n[14] Zhendong Chu, Hongning Wang, Yun Xiao, Bo Long, and Lingfei Wu. Meta policy learning for\ncold-start conversational recommendation. In Proceedings of the Sixteenth ACM International\nConference on Web Search and Data Mining, WSDM \u201923, page 222\u2013230, New York, NY,\nUSA, 2023. Association for Computing Machinery.\n15\n[15] Zhendong Chu, Ruiyi Zhang, Tong Yu, Rajiv Jain, Vlad I Morariu, Jiuxiang Gu, and Ani\nNenkova. Improving a named entity recognizer trained on noisy data with a few clean instances.\narXiv preprint arXiv:2310.16790, 2023.\n[16] Wei Deng, Qian Zhang, Yi-An Ma, Zhao Song, and Guang Lin. On convergence of federated\naveraging langevin dynamics. arXiv preprint arXiv:2112.05120, 2021.\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[18] Enmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Computation and communication efficient\nfederated learning for heterogeneous clients. arXiv preprint arXiv:2010.01264, 2020.\n[19] Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. Cognitive graph for\nmulti-hop reading comprehension at scale. arXiv preprint arXiv:1905.05460, 2019.\n[20] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding\nHu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Delta tuning: A comprehensive study of\nparameter efficient methods for pre-trained language models. arXiv preprint arXiv:2203.06904,\n2022.\n[21] Zhixu Du, Jingwei Sun, Ang Li, Pin-Yu Chen, Jianyi Zhang, Hai\" Helen\" Li, and Yiran\nChen. Rethinking normalization methods in federated learning. In Proceedings of the 3rd\nInternational Workshop on Distributed Machine Learning, pages 16\u201322, 2022.\n[22] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A\nmeta-learning approach. arXiv preprint arXiv:2002.07948, 2020.\n[23] Suyu Ge, Fangzhao Wu, Chuhan Wu, Tao Qi, Yongfeng Huang, and Xing Xie. Fedner:\nPrivacy-preserving medical named entity recognition with federated learning. arXiv preprint\narXiv:2003.09288, 2020.\n[24] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and\nDawn Song. Koala: A dialogue model for academic research. Blog post, April 2023.\n[25] Jack Goetz, Kshitiz Malik, Duc Bui, Seungwhan Moon, Honglei Liu, and Anuj Kumar. Active\nfederated learning. arXiv preprint arXiv:1909.12641, 2019.\n[26] Samyak Gupta, Yangsibo Huang, Zexuan Zhong, Tianyu Gao, Kai Li, and Danqi Chen. Recov-\nering private text in federated learning of language models. arXiv preprint arXiv:2205.08514,\n2022.\n[27] Mark Gurman.\nSamsung bans staff\u2019s ai use after spotting chatgpt data leak, May\n2023.\nhttps://www.bloomberg.com/news/articles/2023-05-02/samsung-bans-chatgpt-and-\nother-generative-ai-use-by-staff-after-leak.\n[28] Weituo Hao, Mostafa El-Khamy, Jungwon Lee, Jianyi Zhang, Kevin J Liang, Changyou Chen,\nand Lawrence Carin Duke. Towards fair federated learning with zero-shot data augmentation.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 3310\u20133319, 2021.\n[29] Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Fran\u00e7oise Beaufays,\nSean Augenstein, Hubert Eichner, Chlo\u00e9 Kiddon, and Daniel Ramage. Federated learning for\nmobile keyboard prediction. arXiv preprint arXiv:1811.03604, 2018.\n[30] Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang Wang,\nPraneeth Vepakomma, Abhishek Singh, Hang Qiu, et al. Fedml: A research library and\nbenchmark for federated machine learning. arXiv preprint arXiv:2007.13518, 2020.\n[31] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021.\n16\n[32] Abhik Jana and Chris Biemann. An investigation towards differentially private sequence\ntagging in a federated framework. In Proceedings of the Third Workshop on Privacy in\nNatural Language Processing, pages 30\u201335, Online, June 2021. Association for Computational\nLinguistics.\n[33] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Ar-\njun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings,\net al. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine\nLearning, 14(1\u20132):1\u2013210, 2021.\n[34] Oleksandra Klymenko, Stephen Meisenbacher, and Florian Matthes. Differential privacy in\nnatural language processing: The story so far. arXiv preprint arXiv:2208.08140, 2022.\n[35] Sandra Kraljevic, Peter J Stambrook, and Kre\u00b4simir Pavelic. Accelerating drug discovery.\nEMBO reports, 5(9):837\u2013842, 2004.\n[36] Fan Lai, Yinwei Dai, Sanjay S. Singapuram, Jiachen Liu, Xiangfeng Zhu, Harsha V. Mad-\nhyastha, and Mosharaf Chowdhury. FedScale: Benchmarking model and system performance\nof federated learning at scale. In International Conference on Machine Learning (ICML),\n2022.\n[37] Fan Lai, Xiangfeng Zhu, Harsha V. Madhyastha, and Mosharaf Chowdhury. Oort: Efficient\nfederated learning via guided participant selection. In 15th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI 21), pages 19\u201335. USENIX Association, July\n2021.\n[38] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient\nprompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 3045\u20133059, Online and Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics.\n[39] Ang Li, Jingwei Sun, Pengcheng Li, Yu Pu, Hai Li, and Yiran Chen. Hermes: An efficient\nfederated learning framework for heterogeneous mobile clients. In Proceedings of the 27th\nAnnual International Conference on Mobile Computing and Networking, MobiCom \u201921, page\n420\u2013437, New York, NY, USA, 2021. Association for Computing Machinery.\n[40] Ang Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen, and Hai Li. Lotteryfl:\nPersonalized and communication-efficient federated learning with lottery ticket hypothesis on\nnon-iid datasets. arXiv preprint arXiv:2008.03371, 2020.\n[41] Haoran Li, Ying Su, Qi Hu, Jiaxin Bai, Yilun Jin, and Yangqiu Song. Fedassistant: Dialog\nagents with two-side modeling.\n[42] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pages 4582\u20134597, Online, August 2021. Association for Computational Linguistics.\n[43] Bill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin Wang, Yufen Huang, Christophe Dupuy,\nRahul Gupta, Mahdi Soltanolkotabi, Xiang Ren, and Salman Avestimehr. Fednlp: Bench-\nmarking federated learning methods for natural language processing tasks. arXiv preprint\narXiv:2104.08815, 2021.\n[44] Zhengyang Lit, Shijing Sit, Jianzong Wang, and Jing Xiao. Federated split bert for heteroge-\nneous text classification. In 2022 International Joint Conference on Neural Networks (IJCNN),\npages 1\u20138. IEEE, 2022.\n[45] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n[46] Ming Liu, Stella Ho, Mengqi Wang, Longxiang Gao, Yuan Jin, and He Zhang. Federated\nlearning meets natural language processing: a survey. arXiv preprint arXiv:2107.12603, 2021.\n[47] Q. Liu and D. Wang. Stein variational gradient descent: A general purpose Bayesian inference\nalgorithm. In Neural Information Processing Systems (NIPS), 2016.\n17\n[48] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.\nP-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and\ntasks. arXiv preprint arXiv:2110.07602, 2021.\n[49] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang.\nGpt understands, too. arXiv preprint arXiv:2103.10385, 2021.\n[50] Yang Liu, Tao Fan, Tianjian Chen, Qian Xu, and Qiang Yang. Fate: An industrial grade\nplatform for collaborative learning with data protection. Journal of Machine Learning Research,\n22(226):1\u20136, 2021.\n[51] Yujie Lu, Chao Huang, Huanli Zhan, and Yong Zhuang. Federated natural language generation\nfor personalized dialogue system. arXiv preprint arXiv:2110.06419, 2021.\n[52] Dhurgham Hassan Mahlool and Mohammed Hamzah Abed. A comprehensive survey on\nfederated learning: Concept and applications. Mobile Computing and Sustainable Informatics:\nProceedings of ICMCSI 2022, pages 539\u2013553, 2022.\n[53] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, and Sayak Paul Younes Belkada.\nPeft: State-of-the-art parameter-efficient fine-tuning methods.\nhttps://github.com/\nhuggingface/peft, 2022.\n[54] Jiachen Mao, Zhongda Yang, Wei Wen, Chunpeng Wu, Linghao Song, Kent W. Nixon,\nXiang Chen, Hai Li, and Yiran Chen. Mednn: A distributed mobile system with enhanced\npartition and deployment for large-scale dnns. In 2017 IEEE/ACM International Conference\non Computer-Aided Design (ICCAD), pages 751\u2013756, 2017.\n[55] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\nCommunication-efficient learning of deep networks from decentralized data. In Artificial\nintelligence and statistics, pages 1273\u20131282. PMLR, 2017.\n[56] H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-\nefficient Learning of Deep Networks from Decentralized Data. Artificial Intelligence and\nStatistics, 2017.\n[57] Peter Menzies and Helen Beebee. Counterfactual Theories of Causation. In Edward N.\nZalta, editor, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford\nUniversity, Winter 2020 edition, 2020.\n[58] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task general-\nization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n3470\u20133487, Dublin, Ireland, May 2022. Association for Computational Linguistics.\n[59] Mahdi Morafah, Saeed Vahidian, Weijia Wang, and Bill Lin. Flis: Clustered federated learning\nvia inference similarity for non-iid data distribution. IEEE Open Journal of the Computer\nSociety, 4:109\u2013120, 2023.\n[60] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt/, November 2022.\n[61] OpenAI. GPT-4 Technical Report. arXiv e-prints, page arXiv:2303.08774, March 2023.\n[62] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models\nto follow instructions with human feedback. Advances in Neural Information Processing\nSystems, 35:27730\u201327744, 2022.\n[63] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction\ntuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.\n[64] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. 2018.\n[65] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n18\n[66] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone\u02c7cn\u00fd,\nSanjiv Kumar, and H. Brendan McMahan. Adaptive Federated Optimization. arXiv e-prints,\npage arXiv:2003.00295, February 2020.\n[67] Amirhossein Reisizadeh, Isidoros Tziotis, Hamed Hassani, Aryan Mokhtari, and Ramtin\nPedarsani. Straggler-resilient federated learning: Leveraging the interplay between statistical\naccuracy and system heterogeneity. arXiv preprint arXiv:2012.14453, 2020.\n[68] Anit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and Virginia\nSmith. Federated optimization for heterogeneous networks. arXiv preprint arXiv:1812.06127,\n1(2):3, 2018.\n[69] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted\ntraining enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.\n[70] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.\nAdvances in Neural Information Processing Systems, 33:3008\u20133021, 2020.\n[71] Dianbo Sui, Yubo Chen, Jun Zhao, Yantao Jia, Yuantao Xie, and Weijian Sun. Feded: Federated\nlearning via ensemble distillation for medical relation extraction. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 2118\u20132128, 2020.\n[72] Jingwei Sun, Ang Li, Binghui Wang, Huanrui Yang, Hai Li, and Yiran Chen. Soteria: Provable\ndefense against privacy leakage in federated learning from representation perspective. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 9311\u20139319, June 2021.\n[73] Yue Tan, Guodong Long, Jie Ma, Lu Liu, Tianyi Zhou, and Jing Jiang. Federated learning\nfrom pre-trained models: A contrastive learning approach. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2022.\n[74] Minxue Tang, Jianyi Zhang, Mingyuan Ma, Louis DiValentin, Aolin Ding, Amin Hassanzadeh,\nHai Li, and Yiran Chen. Fade: Enabling large-scale federated adversarial training on resource-\nconstrained edge devices. arXiv preprint arXiv:2209.03839, 2022.\n[75] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/stanford_alpaca, 2023.\n[76] Yuanyishu Tian, Yao Wan, Lingjuan Lyu, Dezhong Yao, Hai Jin, and Lichao Sun. Fedbert:\nWhen federated learning meets pre-training. ACM Trans. Intell. Syst. Technol., 13(4), aug\n2022.\n[77] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[78] Saeed Vahidian, Sreevatsank Kadaveru, Woonjoon Baek, Weijia Wang, Vyacheslav Kungurtsev,\nChen Chen, Mubarak Shah, and Bill Lin. When do curricula work in federated learning?\nCoRR, abs/2212.12712, 2022.\n[79] Saeed Vahidian, Mahdi Morafah, and Bill Lin. Personalized federated learning by structured\nand unstructured pruning under data heterogeneity. In 2021 IEEE 41st International Conference\non Distributed Computing Systems Workshops (ICDCSW), pages 27\u201334, 2021.\n[80] Saeed Vahidian, Mahdi Morafah, Weijia Wang, Vyacheslav Kungurtsev, Chen Chen, Mubarak\nShah, and Bill Lin. Efficient distribution similarity identification in clustered federated learning\nvia principal angles between client data subspaces. https://arxiv.org/abs/2209.10526, 2022.\n19\n[81] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461, 2018.\n[82] Haoyu Wang, Handong Zhao, Yaqing Wang, Tong Yu, Jiuxiang Gu, and Jing Gao. Fedkc:\nFederated knowledge composition for multilingual natural language understanding. In Pro-\nceedings of the ACM Web Conference 2022, WWW \u201922, page 1839\u20131850, New York, NY,\nUSA, 2022. Association for Computing Machinery.\n[83] Song Wang, Xingbo Fu, Kaize Ding, Chen Chen, Huiyuan Chen, and Jundong Li. Federated\nfew-shot learning. Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, 2023.\n[84] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,\nand Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc-\ntions. arXiv preprint arXiv:2212.10560, 2022.\n[85] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv\npreprint arXiv:2109.01652, 2021.\n[86] Orion Weller, Marc Marone, Vladimir Braverman, Dawn Lawrie, and Benjamin Van Durme.\nPretrained models for multilingual federated learning. arXiv preprint arXiv:2206.02291, 2022.\n[87] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In\nICML, 2011.\n[88] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model\nwith parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196, 2023.\n[89] Qian Yang, Jianyi Zhang, Weituo Hao, Gregory P. Spell, and Lawrence Carin. Flop: Federated\nlearning on medical datasets using partial networks. In Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining, KDD \u201921, page 3845\u20133853, New York,\nNY, USA, 2021. Association for Computing Machinery.\n[90] Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel\nRamage, and Fran\u00e7oise Beaufays. Applied federated learning: Improving google keyboard\nquery suggestions. arXiv preprint arXiv:1812.02903, 2018.\n[91] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient\nfine-tuning for transformer-based masked language-models, 2022.\n[92] Jianyi Zhang, Zhixu Du, Jingwei Sun, Ang Li, Minxue Tang, Yuhao Wu, Zhihui Gao, Martin\nKuo, Hai Helen Li, and Yiran Chen. Next generation federated learning for edge devices:\nAn overview. In 2022 IEEE 8th International Conference on Collaboration and Internet\nComputing (CIC), pages 10\u201315, 2022.\n[93] Jianyi Zhang, Zhixu Du, Jingwei Sun, Ang Li, Minxue Tang, Yuhao Wu, Zhihui Gao, Martin\nKuo, Hai-Helen Li, and Yiran Chen. Next generation federated learning for edge devices:\nAn overview. In 2022 IEEE 8th International Conference on Collaboration and Internet\nComputing (CIC), pages 10\u201315. IEEE, 2022.\n[94] Jianyi Zhang, Martin Kuo, Ruiyi Zhang, Guoyin Wang, Saeed Vahidian, and Yiran\nChen.\nShepherd:\nLarge language models with parameter-efficient federated finetun-\ning in the presence of heterogeneous instructions. https://github.com/JayZhang42/\nFederatedGPT-Shepherd, 2023.\n[95] Jianyi Zhang, Ang Li, Minxue Tang, Jingwei Sun, Xiang Chen, Fan Zhang, Changyou Chen,\nYiran Chen, and Hai Li. Fed-cbs: A heterogeneity-aware client sampling mechanism for\nfederated learning via class-imbalance reduction. In Proceedings of the 40th International\nConference on Machine Learning, 2023.\n20\n[96] Jianyi Zhang, Ruiyi Zhang, Lawrence Carin, and Changyou Chen.\nStochastic particle-\noptimization sampling and the non-asymptotic convergence theory. In Silvia Chiappa and\nRoberto Calandra, editors, Proceedings of the Twenty Third International Conference on Artifi-\ncial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research,\npages 1877\u20131887. PMLR, 26\u201328 Aug 2020.\n[97] Jianyi Zhang, Yang Zhao, and Changyou Chen. Variance reduction in stochastic particle-\noptimization sampling. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th\nInternational Conference on Machine Learning, volume 119 of Proceedings of Machine\nLearning Research, pages 11307\u201311316. PMLR, 13\u201318 Jul 2020.\n[98] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li,\nPeng Gao, and Qiao Yu. Llama-adapter: Efficient fine-tuning of language models with zero-init\nattention. arXiv preprint arXiv:2303.16199, 2023.\n[99] Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew Gordon Wilson. Cycli-\ncal stochastic gradient mcmc for bayesian deep learning. arXiv preprint arXiv:1902.03932,\n2019.\n[100] Yang Zhao, Jianyi Zhang, and Changyou Chen. Self-adversarially learned bayesian sampling.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5893\u20135900,\n2019.\n[101] Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In Advances in Neural\nInformation Processing Systems 32: Annual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 14747\u2013\n14756, 2019.\n[102] Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for het-\nerogeneous federated learning. In International Conference on Machine Learning, pages\n12878\u201312889. PMLR, 2021.\n21\n"
  },
  {
    "title": "WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset",
    "link": "https://arxiv.org/pdf/2305.05432.pdf",
    "upvote": "1",
    "text": "Wiki Workshop (10th edition) \u2013 May 11, 2023\nWikiWeb2M: A Page-Level Multimodal Wikipedia Dataset\nAndrea Burns\nBoston University\u2217\nKrishna Srinivasan\nGoogle\nJoshua Ainslie\nGoogle\nGeoff Brown\nGoogle\nKate Saenko\nFAIR, Boston University\nBryan A. Plummer\nBoston University\nJianmo Ni\nGoogle\nMandy Guo\nGoogle\nAbstract\nWebpages have been a rich resource for lan-\nguage and vision-language tasks. Yet only\npieces of webpages are kept: image-caption\npairs, long text articles, or raw HTML, never\nall in one place. Webpage tasks have result-\ningly received little attention and structured\nimage-text data underused. To study multi-\nmodal webpage understanding, we introduce\nthe Wikipedia Webpage 2M (WikiWeb2M)0\nsuite; the \ufb01rst to retain the full set of im-\nages, text, and structure data available in a\npage. WikiWeb2M can be used for tasks like\npage description generation, section summa-\nrization, and contextual image captioning.\nKeywords: Multimodal Data, Webpages, Machine\nLearning, Text Generation, Vision and Language\nIntroduction\nWebpages are multimodal, structured content which\ncan been used for pretraining and \ufb01ne-tuning. Large\nscale noisy datasets scraped from the web have been\nused to pretrain large language or contrastive mod-\nels (Ra\ufb00el et al., 2020; Jia et al., 2021). Downstream\ntasks built from webpages have included instruction\nfollowing, image captioning, news captioning, image-\nsentence retrieval, and image-article retrieval (Gur\net al., 2022; Biten et al., 2019; Tan et al., 2022).\nYet little prior work has studied tasks to evaluate\nmultimodal webpage understanding itself.\nMany classi\ufb01cation and generation problems could\nbe studied with webpages: taxonomic webpage clas-\nsi\ufb01cation, webpage retrieval, web image captioning,\nand webpage summarization. However, to date there\nis no open source, multimodal dataset that retains\nall webpage content.\nE.g., the Wikipedia Image\nText (WIT) dataset (Srinivasan et al., 2021) does\nnot keep HTML structure and misses out on many\n\u2217Work was done during an internship at Google.\n0Data is readily available at https://github.com/\ngoogle-research-datasets/wit/blob/main/\nwikiweb2m.md\ntext sections, as shown in Table 1. Uni\ufb01ed text, im-\nage, and structure data would allow for greater study\nof multimodal content understanding with many-to-\nmany text and image relationships. As a result, we\npropose the new Wikipedia Webpage (WikiWeb2M)\ndataset of over 2M pages, which uni\ufb01es webpage con-\ntent to include all text, images, and their location\n(e.g., section index) in one example. Table 2 (left)\nincludes the number of pages, sections, and images,\nalong with sample counts for downstream tasks.\nFigure 1 (left) shows how one webpage can be\nused for page description, section summarization,\nand contextual captioning. These tasks can improve\ninteraction with web content, e.g., a page description\nmay provide a user who is blind more agency by al-\nlowing them to preview content before listening to\nthe entire body with a screen reader (Vtyurina et al.,\n2019). On top of aiding assistive technology, tasks\nlike contextual image captioning and section summa-\nrization can be used for modern content generation,\nas there is growing interest in providing multimodal\nsnippets from the web (Nkemelu et al., 2023).\nThe WikiWeb2M Dataset\nWikiWeb2M is created by rescraping the \u223c2M En-\nglish articles in WIT. Each webpage sample includes\nthe page URL and title, section titles, text, and in-\ndices, images and their captions, and more; see Fig-\nure 1 (right). This di\ufb00ers from WIT which de\ufb01ned\nindividual samples as image-caption pairs with ad-\nditional metadata (e.g., originating section title).\nWe shu\ufb04e the WIT webpages to de\ufb01ne a random\n1.8M/100K/100K train/val/test split. Table 2 (left)\nshows the number of pages, sections, and images in\nour dataset after additional processing. In partic-\nular, we only retain content sections (e.g., not the\n\u201cSee Also\u201d section). For images, we keep JPEG and\nPNG and require the dimensions be greater than 1px\nto allow for a greater diversity of images to be in-\ncluded (e.g., icons)1. We include metadata on image\ndimensions to allow for additional \ufb01ltering.\nIn Table 1, we report the number of sections and\nimages compared to the English subset of WIT. We\n1We release image URLs, where they can be fetched.\n\u00a9 Copyright held by the owner/author(s), published under Creative Commons CC BY 4.0 License\narXiv:2305.05432v1  [cs.CL]  9 May 2023\nWiki Workshop (10th edition) \u2013 May 11, 2023\nadd nearly 1M total images to the dataset by keeping\nthe images on a webpage regardless of whether they\nhave image captions. We break down section counts\nby type: structural, heading, text, image, and both\ntext and image. Structural and heading sections do\nnot contain immediate section text (the former have\nsubsections). For heading sections, the section con-\ntent either linked to a di\ufb00erent article, was empty, or\nonly had tables. A notable 6.8M text sections are in\nWikiWeb2M, none of which were available in WIT.\nThe WikiWeb2M Tasks\nWe now describe WikiWeb2M\u2019s suite of multimodal\ngeneration tasks and task data processing. Table 2\n(left) shows data statistics and (right) downstream\ntask performance when using T5 and ViT base mod-\nels (Ra\ufb00el et al., 2020; Dosovitskiy et al., 2021).\nPage Description Generation The goal is to gen-\nerate a description of a page given the rest of the\nwebpage\u2019s image, text, and structure. We use the\nWikipedia-provided page descriptions for each arti-\ncle. We retain a page if the description has at least\n\ufb01ve words. A small subset of Wikipedia pages are\nlists2; we remove pages that explicitly have \u201clist of\u201d\nin their URL or fewer than two rich content sections.\nSection Summarization The goal is to generate a\nsentence that highlights the section\u2019s content given\nimages and (non-summary) text in the section and\nother context sections.\nWe take advantage of the\nleading sentence bias and use the \ufb01rst sentence of a\nsection its pseudo summary. In a small pilot, a ma-\njority of human annotators also deemed the \ufb01rst sen-\ntence as a reasonable summary. A section serves as\na target section if it has at least \ufb01ve sentences, con-\ntains neither a table nor list, and is not the root sec-\ntion. We \ufb01lter out the root because the root (\ufb01rst)\nsection is often the page description.\nContextual Image Captioning (Nguyen et al., 2022)\nproposed Wikipedia image captioning given the im-\nage\u2019s webpage context. With WikiWeb2M, we can\nnow utilize the entire webpage context for the image\ninstead of just the section it originally came from.\nWe only allow target images to be those from WIT\nto ensure quality captions. Following prior work, we\nalso use the reference description as the ground truth\ncaption to be generated and require it must have at\nleast three words. But, we do not input the attri-\nbution description, as it often contains large overlap\nwith the reference description.\nResults Table 2 (right) shows results for each task.\nFor contextual image captioning and section sum-\n2For\nexample,\nhttps://en.wikipedia.org/\nwiki/List_of_mammals_of_the_United_States\nmarization we verify that WikiWeb2M\u2019s additional\nsections (compared to only inputting the target sec-\ntion for image captioning or summarization) im-\nprove task performance; page description generation\nis only made possible with our dataset.\nReferences\n[Biten et al.2019] Ali Furkan Biten, Llu\u00b4\u0131s G\u00b4omez,\nMar\u00b8cal Rusi\u02dcnol, and Dimosthenis Karatzas. 2019.\nGood news, everyone! context driven entity-aware\ncaptioning for news images. In CVPR.\n[Dosovitskiy et al.2021] Alexey Dosovitskiy,\nLucas\nBeyer, Alexander Kolesnikov, Dirk Weissenborn,\nXiaohua Zhai, Thomas Unterthiner, Mostafa De-\nhghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby.\n2021.\nAn image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR.\n[Gur et al.2022] Izzeddin Gur, O\ufb01r Nachum, Yingjie\nMiao, Mustafa Safdari, Austin Huang, Aakanksha\nChowdhery, Sharan Narang, Noah Fiedel, and\nAleksandra Faust.\n2022.\nUnderstanding html\nwith large language models.\n[Jia et al.2021] Chao Jia, Yinfei Yang, Ye Xia, Yi-\nTing Chen, Zarana Parekh, Hieu Pham, Quoc V.\nLe, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.\n2021. Scaling up visual and vision-language rep-\nresentation learning with noisy text supervision.\nIn ICML.\n[Nguyen et al.2022] Khanh\nNguyen,\nAli\nFurkan\nBiten, Andres Ma\ufb02a, Lluis Gomez, and Dimos-\nthenis Karatzas. 2022. Show, interpret and tell:\nEntity-aware contextualised image captioning in\nwikipedia.\n[Nkemelu et al.2023] Daniel Nkemelu,\nPeggy Chi,\nDaniel Castro Chin, Krishna Srinivasan, and Ir-\nfan Essa. 2023. Automatic multi-path web story\ncreation from a structural article.\n[Ra\ufb00el et al.2020] Colin Ra\ufb00el, Noam Shazeer, Adam\nRoberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu.\n2020.\nExploring the limits of transfer learning\nwith a uni\ufb01ed text-to-text transformer. JMLR.\n[Srinivasan et al.2021] Krishna Srinivasan, Karthik\nRaman, Jiecao Chen, Michael Bendersky, and\nMarc Najork. 2021. Wit: Wikipedia-based image\ntext dataset for multimodal multilingual machine\nlearning. In SIGIR.\n[Tan et al.2022] Reuben Tan, Bryan A. Plummer,\nKate Saenko, J. P. Lewis, Avneesh Sud, and\nThomas Leung. 2022. Newsstories: Illustrating\narticles with visual summaries. In ECCV.\n[Vtyurina et al.2019] Alexandra\nVtyurina,\nAdam\nFourney, Meredith Ringel Morris, Leah Findlater,\nand Ryen W. White. 2019. Bridging screen read-\ners and voice assistants for enhanced eyes-free web\nsearch. In ASSETS.\n\u00a9 Copyright held by the owner/author(s), published under Creative Commons CC BY 4.0 License\nWiki Workshop (10th edition) \u2013 May 11, 2023\nDataset\n# Webpage Sections\n# Images\nStructural\nHeading\nText\nImage\nBoth\nTotal\nUnique\nTotal\nWIT (En)\n-\n-\n-\n199,872\n2,847,929\n3,047,801\n3,660,211\n4,955,835\nWikiWeb2M\n731,394\n686,376\n6,817,950\n221,523\n3,236,254\n11,693,497\n4,438,642\n5,940,431\nTable 1: Comparison of WikiWeb2M to WIT. We report the aggregate counts over all splits. WikiWeb2M and WIT\n(English subset) contain the same webpages.\nWikiWeb2M Statistic\nTrain\nVal\nTest\n# Pages\n1,803,225\n100,475\n100,833\n# Sections\n10,519,294\n585,651\n588,552\n# Total Images\n5,340,708\n299,057\n300,666\n# Task Samples\nPage Description\n1,435,263\n80,103\n80,339\nSection Summarization\n3,082,031\n172,984\n173,591\nContextual Captioning\n2,222,814\n124,703\n124,188\nDownstream Task\nB\nR\nC\nPage Description\n14.00\n38.50\n81.49\nSection Summarization\nTarget Section Only\n8.90\n27.82\n60.20\nWikiWeb2M\n10.12\n29.43\n69.89\nContextual Captioning\nTarget Section Only\n10.92\n36.21\n148.53\nWikiWeb2M\n11.84\n37.69\n158.19\nTable 2: Statistics and experimental results on the WikiWeb2M dataset. On the left we report the number of pages,\nsections, and images in the source WikiWeb2M dataset. Below, we report the number of samples for three task datasets\nthat we generate from WikiWeb2M with additional processing: page description generation, section summarization,\nand contextual image captioning. On the right we report the task performance achieved with T5 and ViT base models\n(metrics include BLEU-4 (B), ROUGE-L (R), and CIDEr (C)).\nFigure 1: Example tasks and data samples from WikiWeb2M. On the left we show how our dataset provides a uni\ufb01ed\nwebpage sample that contains all text, image, and structure, enabling new tasks like page description generation.\nFor image captioning and section summarization, remaining page text and images provide useful context, aiding task\nperformance. On the right we show the WikiWeb2M page sample for the same Wikipedia article on succulents; we\nonly include a subset of \ufb01elds due to space. E.g., the WikiWeb2M sample also contains the image alt-text, attribution\nand reference descriptions, along with other metadata, but it is not illustrated on the right.\n\u00a9 Copyright held by the owner/author(s), published under Creative Commons CC BY 4.0 License\n"
  },
  {
    "title": "NerfAcc: Efficient Sampling Accelerates NeRFs",
    "link": "https://arxiv.org/pdf/2305.04966.pdf",
    "upvote": "1",
    "text": "NerfAcc: Efficient Sampling Accelerates NeRFs\nRuilong Li\nUC Berkeley\nruilongli@berkeley.edu\nHang Gao\nUC Berkeley\nhangg@berkeley.edu\nMatthew Tancik\nUC Berkeley\ntancik@berkeley.edu\nAngjoo Kanazawa\nUC Berkeley\nkanazawa@berkeley.edu\npip install nerfacc\nEfficient Sampling\nTraining Speedup\nNeRF\nTensoRF\nInstant-NGP\nT-NeRF\nTiNeuVox\nK-Planes\nBARF\nStatic NeRFs\nDynamic NeRFs\nCam Opt. NeRFs\nFigure 1: NerfAcc Toolbox. Our proposed toolbox, NerfAcc, integrates advanced efficient sampling techniques that lead to\nsignificant speedups in training various recent NeRF papers with minimal modifications to existing codebases.\nAbstract\nOptimizing and rendering Neural Radiance Fields\nis computationally expensive due to the vast number of\nsamples required by volume rendering.\nRecent works\nhave included alternative sampling approaches to help\naccelerate their methods, however, they are often not\nthe focus of the work. In this paper, we investigate and\ncompare multiple sampling approaches and demonstrate\nthat improved sampling is generally applicable across NeRF\nvariants under an unified concept of transmittance estimator.\nTo facilitate future experiments, we develop NerfAcc, a\nPython toolbox that provides flexible APIs for incorporating\nadvanced sampling methods into NeRF related methods. We\ndemonstrate its flexibility by showing that it can reduce the\ntraining time of several recent NeRF methods by 1.5\u00d7 to\n20\u00d7 with minimal modifications to the existing codebase.\nAdditionally, highly customized NeRFs, such as Instant-NGP,\ncan be implemented in native PyTorch using NerfAcc. Our\ncode are open-sourced at https://www.nerfacc.com.\n1. Introduction\nNeural volume rendering has revolutionized the in-\nverse rendering problem, with the Neural Radiance Field\n(NeRF) [16] being a key innovation. The continuous radi-\nance field representation allows for rendering novel views of\na scene from any camera position. However, the optimiza-\ntion of a NeRF can be computationally expensive due to the\nneural representation of radiance field and the large number\nof samples required by volume rendering. These challenges\nhave limited practical applications of NeRF-based optimiza-\ntion and rendering.\nSeveral recent works have successfully reduced the com-\nputational cost of neural volume rendering by proposing\nmore efficient radiance field representations [17, 35, 36, 4,\n24, 7]. While there are differences in the specific radiance\nfield representations and their applications, most of these\nmethods share a similar volume rendering pipeline which\ninvolves creating samples along the ray and accumulating\nthem through alpha-composition.\nHowever, compared to the considerable efforts focused on\ndeveloping efficient radiance field representations, there has\narXiv:2305.04966v2  [cs.CV]  24 Oct 2023\nbeen limited attention devoted to reducing the computational\ncost of neural volume rendering through efficient sampling.\nWhile a few recent works have included alternative sampling\napproaches to accelerate their methods [17, 35, 24, 2], these\nmethods are often not the main focus of the paper. Moreover,\nimplementing advanced sampling approaches typically\nrequires non-trivial efforts. For example, Instant-NGP [17]\nand Plenoxels [35] both employ highly customized CUDA\nimplementations to achieve spatial skipping during ray\nmarching, which are tightly coupled with their respective\nradiance field implementations. Consequently, it can be\nchallenging for researchers to benefit from these advanced\nsampling approaches in their own research.\nIn this paper, we investigate and compare several ad-\nvanced sampling approaches from the literature and pro-\nvide mathematical proofs demonstrating that they can all be\nviewed in a unified way of creating an estimation of trans-\nmittance for importance sampling. Our analysis shows that\nby understanding the spectrum of sampling through transmit-\ntance estimator, novel sampling strategies can be explored.\nBased on this, we decouple the sampling procedure from\nthe neural volumetric rendering pipeline and demonstrate\nthat improved sampling is generally applicable across dif-\nferent variants of NeRF. Furthermore, to facilitate future\nexperiments, we propose NerfAcc, a plug-and-play toolbox\nthat provides a flexible Python API for integrating advanced\nsampling approaches into NeRF-related methods, ready for\nresearchers to incorporate into their own codebases. We\ndemonstrate that with less than 100 lines of code change us-\ning NerfAcc, various NeRF methods [16, 4, 5, 7, 14, 21, 17]\ncan enjoy 1.5\u00d7 to 20\u00d7 training speedup with better perfor-\nmance. Notably, using the NerfAcc library, one can train an\nInstant-NGP [17] model with pure Python code in the same\namount of time, and achieve slightly better performance\n(+0.2dB) than the official pure CUDA implementation.\nOur paper presents a unique contribution to the commu-\nnity. Unlike other papers that introduce novel algorithms,\nour work sheds light on the intricacies of various sampling\napproaches, which are often overlooked despite their\nsignificance. As far as we know, this work is the first paper\nthat dives deep into this crucial aspect in the context of\nneural radiance field, and offer a novel, unified concept that\nallows researchers to view existing sampling algorithms\nin a fresh perspective. In addition, we provide a toolbox\nthat facilitates research and development in this area. We\nbelieve translation of mathematical ideas into efficient,\neasy-to-modify implementation is fundamental to the\nresearch development. Overall, we hope that our proposed\nconcept, along with the toolbox, can inspire new research\nideas and accelerate progress in this field.\n2. Related Works\nNeRF Codebases.\nThe recent explosion of NeRF-related\nresearch has led to numerous papers, many of which have re-\nleased their own codebases [1, 2, 4, 7, 13, 16, 19, 37, 9].\nThese codebases address various tasks related to NeRF,\nincluding surface reconstruction [18, 32, 34], radiance\nfield representation [1, 2, 17, 24, 35], dynamic model-\ning [5, 7, 13, 21], and camera optimization [14, 33]. How-\never, each codebase is tailored to a specific task and supports\nonly a single approach. While most of these methods share\nthe same volume rendering pipeline of accumulating sam-\nples along the ray, transferring the implementation from one\ncodebase to another requires non-trivial efforts. In this work,\nwe address this common sampling problem by introducing\nNerfAcc, a plug-and-play toolbox that supports all the afore-\nmentioned tasks and methods, making it easy to integrate\ninto any existing codebase.\nNeRF Frameworks.\nRecently, several projects have been\nintroduced to integrate different NeRF variants into a sin-\ngle framework, such as NeRF-Factory [11], Nerfstudio [29],\nand Kaolin-Wisp [28]. These frameworks have made sig-\nnificant progress in facilitating future NeRF-related re-\nsearch. NeRF-Factory offers a collection of NeRF vari-\nants [16, 39, 24, 35, 1, 2, 30] with original implementations\nand focuses on comprehensive benchmarking. Nerfstudio\nconsolidates critical techniques introduced in existing litera-\nture [16, 1, 2, 17, 15] and provides modular components for\nthe community to easily build on. Kaolin-Wisp builds upon\nthe Kaolin [8] framework and implements a set of voxel-\nbased NeRF papers [27, 17, 26]. However, these frameworks\nare designed to encourage researchers to develop within the\nframework, and do not benefit users working on their own\ncodebases. Moreover, due to the high activity in the NeRF-\nrelated research, it is almost impossible to keep track of the\nlatest developments and integrate advanced techniques into\na single framework. Therefore, NerfAcc is designed as a\nstandalone library that can be plugged into any codebase. It\nsupports a wide range of NeRF related methods and can be\neasily maintained as new methods emerge.\n3. Importance Sampling via Transmittance\nSeveral advanced sampling approaches exist in the litera-\nture. For instance, Plenoxels [35] uses a sparse grid, Instant-\nNGP [17] uses an occupancy grid, NeRF [16] employs a\ncoarse-to-fine strategy, Mip-NeRF 360 [2] proposes proposal\nnetworks. However, creating a toolbox that can support all\nof these approaches is not an easy task since they operate in\ncompletely different ways. In this section, we demonstrate\nthat these approaches can all be viewed in a unified way of\nimportance sampling. We also provide a mathematical proof\nthat transmittance is all you need for importance sampling.\nThus, each method essentially has its own way of creating an\nPDF\nRay Direction\n0.0\n1.0\nTransmittance\nRay Direction\n0.0\n1.0\nTransmittance\nRay Direction\n0.0\n1.0\nPDF\nRay Direction\n0.0\n1.0\nPDF\nRay Direction\n0.0\n1.0\nTransmittance\nRay Direction\n0.0\n1.0\n       (a) Uniform\n{Coarse MLP NeRF...}\n    (b) Spatial Skipping\n{Instant-NGP, Plenoxels,...}\n   (c) PDF Approaches\n{NeRF, Mip-NeRF 360,...}\nReal Distribution\nEstimator\nSamples\nDensity\nRay Direction\n0.0\nInf\nDensity\nRay Direction\n0.0\nInf\nDensity\nRay Direction\n0.0\nInf\nImportance Sampling\nImportance Sampling\nImportance Sampling\nFigure 2: Illustration of Sampling via Transmittance Estimator. Although spatial skipping approaches (e.g., Occupancy\nGrid in Instant-NGP [17]) and PDF approaches (e.g., Proposal Network in Mip-NeRF 360 [2]) appear distinct from each other,\nthey can both be viewed as constructing a transmittance estimator from which samples can be created via importance sampling.\nestimation of the transmittance along the ray, which we refer\nto as the transmittance estimator. This observation enables\nus to unify different types of sampling approaches under the\nsame framework and leads to our NerfAcc toolbox.\n3.1. Formulation\nEfficient sampling is a well-explored problem in\nGraphics [6], wherein the emphasis is on identifying regions\nthat make the most significant contribution to the final\nrendering. This objective is generally accomplished through\nimportance sampling, which aims to distribute samples\nbased on the probability density function (PDF), denoted\nas p(t), between the range of [tn, tf]. By computing the\ncumulative distribution function (CDF) through integration,\ni.e., F(t) =\nR t\ntn p(v) dv, samples are generated using the\ninverse transform sampling method:\nt = F \u22121(u)\nwhere\nu \u223c U[0, 1].\n(1)\nIn volumetric rendering, the contribution of each sam-\nple to the final rendering is expressed by the accumulation\nweights T(t)\u03c3(t):\nC(r) =\nZ tf\ntn\nT(t) \u03c3(t) c(t) dt\nwhere\nT(t) = exp\n\u0012\n\u2212\nZ t\ntn\n\u03c3(s) ds\n\u0013\n.\n(2)\nHence, the PDF for volumetric rendering is p(t) = T(t)\u03c3(t)\nand the CDF F(t) =\nR t\ntn p(v) dv can be derived as a function\nof transmittance T(t):\nF(t) =\nZ t\ntn\nT(v)\u03c3(v) dv\n=\nZ t\ntn\nd\ndv\n\u0014\n\u2212 exp\n\u0012\n\u2212\nZ v\ntn\n\u03c3(s)ds\n\u0013\u0015\ndv\n= 1 \u2212 exp\n\u0012\n\u2212\nZ t\ntn\n\u03c3(s)ds\n\u0013\n= 1 \u2212 T(t).\n(3)\nTherefore, inverse sampling the CDF F(t) is equivalent\nto inverse sampling the transmittance T(t). Thus a transmit-\ntance estimator is sufficient to determine the optimal samples.\nIntuitively, this suggests to put more samples around regions\nSampling Methods\nPDF Estimator p(ti)\nTransmittance Estimator T(ti)\nUpdate F\nInstantiation\nUniform\nConstant\n1 \u2212\nt\u2212tn\ntf \u2212tn\n-\nCoarse MLP NeRF [16]\nSpatial Skipping\n1\n\u0002\n\u03c3(ti)>\u03c4\n\u0003\nPn\nj=1 1\n\u0002\n\u03c3(ti)>\u03c4\n\u0003\n1 \u2212\nPi\u22121\nj=1 1\n\u0002\n\u03c3(ti)>\u03c4\n\u0003\nPn\nj=1 1\n\u0002\n\u03c3(ti)>\u03c4\n\u0003\nEMA\nInstant-NGP [17]\nSGD\nPlenoxels [35]\nPDF Approaches\n\u03c3(ti) exp(\u2212\u03c3(ti) dt)\nexp(\u2212 Pi\u22121\nj=1 \u03c3(ti) dt)\nSGD\nFine MLP NeRF [16]\nSGD\nMip-NeRF 360 [2]\nTable 1: Mathematical Formulations of Different Sampling Approaches. We outlines the significant mathematical\ndistinctions among each sampling approach, under the perspective of transmittance estimator. See Section 3.1 for notations.\nwhere the transmittance changes rapidly \u2013 and that is exactly\nwhat happens when a ray hit a surface. Implementation wise,\nthis observation enables us to compute the CDF directly\nusing 1 \u2212 T(t), instead of the computationally expensive\nintegral\nR t\ntn T(v)\u03c3(v) dv, which is the standard implementa-\ntion adopted by many popular codebases [16, 1, 2, 29, 7].\nWhile advanced transmittance estimator techniques,\nsuch as delta tracking [31] with Monte Carlo sampling,\nare utilized in production-level volumetric rendering in\nGraphics [6], NeRFs operate in a distinct setting in which\nthe scene geometry is not predefined but optimized on the\nfly. During NeRF optimization, the radiance field changes\nbetween iterations, necessitating the dynamic update of the\ntransmittance estimator at each step k:\nF : T(t)k\u22121 7\u2192 T(t)k.\n(4)\nThis introduces additional challenges to efficient sampling\nbecause it becomes more difficult to accurately estimate\nthe transmittance from a radiance field that is constantly\nchanging. Current approaches employ either exponential\nmoving average (EMA) or stochastic gradient descent\n(SGD) as the update function F. However, we note that\nthere may be other update functions that could be explored.\nWith these concepts in mind, let us now examine some of\nthe existing approaches towards efficient sampling.\nUniform.\nIf a transmittance estimator is not available, the\nonly assumption we can make is that every point along the\nray contributes equally to the final rendering. Mathemat-\nically, this assumption translates to a constant PDF and a\nlinearly decaying transmittance T(t) = 1\u2212(t\u2212tn)/(tf\u2212tn).\nIn this case, the sampling process is equivalent to uniformly\nsampling along the ray, i.e., ti = tn + (tf \u2212 tn) \u00b7 ui. It\nis worth noting that every NeRF model that uses uniform\nsampling inherently assumes this linear transmittance decay,\nsuch as the coarse level in vanilla NeRF [16]. See Fig. 2(a)\nfor the illustration.\nSpatial Skipping.\nA more sophisticated approach to im-\nprove uniform sampling is to identify empty regions and skip\nthem during sampling, as proposed in Instant-NGP\u2019s Occu-\npancy Grid [17] and Plenoctrees\u2019 Sparse Grid [36]. This\ntechnique binarizes the density along the ray with a conser-\nvative threshold \u03c4: \u02c6\u03c3(ti) = 1\n\u0002\n\u03c3(ti) > \u03c4\n\u0003\n. Consequently,\nthe piece-wise constant PDF can be expressed as p(ti) =\n\u02c6\u03c3(ti)/ Pn\nj=1 \u02c6\u03c3(tj), and the piece-wise linear transmittance\nestimator is T(ti) = 1 \u2212 Pi\u22121\nj=1 \u02c6\u03c3(tj)/ Pn\nj=1 \u02c6\u03c3(tj). To up-\ndate this estimator during optimization, Instant-NGP [17]\ndirectly updates the cached density with exponential moving\naverage (EMA) over iteration k: \u03c3(ti)k = \u03b3 \u00b7 \u03c3(ti)k\u22121 +\n(1 \u2212 \u03b3) \u00b7 \u03c3(ti)k. Meanwhile, Plenoxels [35] updates the\ndensity via the gradient descent through the rendering loss.\nSee Fig. 2(b) for an illustration.\nPDF Approaches.\nAnother type of approach is to directly\nestimate the PDF along the ray with discrete samples. In\nvanilla NeRF [16], the coarse MLP is trained using vol-\numetric rendering loss to output a set of densities \u03c3(ti).\nThis allows for the creation of a piece-wise constant PDF:\np(ti) = \u03c3(ti) exp(\u2212\u03c3(ti) dt), and a piece-wise linear trans-\nmittance estimator T(ti) = exp(\u2212 Pi\u22121\nj=1 \u03c3(ti) dt). This\napproach was further improved in Mip-NeRF 360 [16] with\na PDF matching loss, which allows for the use of a much\nsmaller MLP in the coarse level, namely Proposal Network,\nto speedup the PDF construction. In both cases, the trans-\nmittance estimator is updated through gradient descent. See\nFig. 2(c) for an illustration.\nTable 1 provides a mathematical summary and compari-\nson of these approaches. Additionally, we present an illustra-\ntion in Figure 2 to provide an intuitive comparison of these\napproaches from the perspective of PDF (second row) and\ntransmittance (third row), as well as how the samples can\nbe created from the transmittance estimator via importance\nsampling (last row). This visualization also reveals some\npros and cons for each approach, which we will discuss in\nin Section. 3.3.\n3.2. Design Spaces\nChoice of Representations.\nThe transmittance estimator\ncan use either an explicit voxel [17, 35, 4], an MLP [1, 2, 16],\nor a hybrid representation [29]. Depending on whether the\nestimator is explicit or not, it can be updated with either\nrule-based EMA [17, 4] or gradient descent with some super-\nvision [35, 2, 16, 29]. Generally, voxel-based estimators are\nfaster than implicit (e.g., MLP-based) estimators but suffer\nmore from aliasing issues. It is worth noting that the trans-\nmittance estimator\u2019s representation can significantly benefit\nfrom advances in radiance field representation. For exam-\nple, the Nerfacto model [29] uses the most recent hybird-\nrepresentation HashEncoding [17] for both the radiance field\nand the sampling module, achieving the best quality-speed\ntradeoff in in-the-wild settings.\nHandling Unbounded Scenes.\nSo far, we have only dis-\ncussed sampling within a bounded area [tn, tf]. For un-\nbounded scenes, it is impossible to densely sample along the\nray. Similar to the mipmaps used in graphics rendering, a\ngeneral solution is to sample more coarsely as the ray goes\nfurther, as objects farther away appear in fewer pixels on the\nimage plane. This can be achieved by creating a bijective\nmapping function \u03a6 : s \u2208 [sn, sf] 7\u2192 t \u2208 [tn, +\u221e], and\nperforming sampling in the s-space instead of the t-space.\nSeveral papers [2, 39, 23] that work on unbounded scenes\nhave introduced different mapping functions \u03a6, to which we\nrefer our readers for details.\n3.3. Discussions\nPros and Cons.\nSampling with uniform assumption is the\neasiest one to implement but with lowest efficiency in most\ncases. Spatial skipping is a more efficient technique since\nmost of the 3D space is empty, but it still samples uniformly\nwithin occupied but occluded areas that contribute little to\nthe final rendering (e.g., the last sample in Figure 2(b)).\nPDF-based estimators generally provide more accurate trans-\nmittance estimation, enabling samples to concentrate more\non high-contribution areas (e.g., surfaces) and to be more\nspread out in both empty and occluded regions. However,\nthis also means that samples are always spread out through-\nout the entire space without any skipping, as shown in Fig-\nure 2(c). Moreover, the current approaches all introduce\naliasing effects to the volumetric rendering due to either (1)\nthe piece-wise linearity assumption for estimating transmit-\ntance along the ray [1, 2, 16, 35, 17] as illustrated in Figure 2,\nor (2) the underlying voxel representation for the transmit-\ntance estimator [35, 17] as discussed in Section 3.2. A recent\nwork, Zip-NeRF [3], addresses the aliasing problem tied to\nthese two exact issues (called \u201cz-aliasing\u201d and \u201cxy-aliasing\u201d\nin their work), which are naturally revealed under our unified\nframework.\nImplementation Difficulties.\nThe current implementa-\ntions for efficient sampling are all highly customized and\ntightly integrated with the specific radiance field proposed\nin each paper. For instance, spatial skipping is implemented\nwith customized CUDA kernels in Instant-NGP [17] and\nPlenoxels [35]. Mip-NeRF 360 [2], K-planes [7], and Ner-\nfacto [29] have a proposal network implemented but it is\nclosely integrated with their repositories and can only sup-\nport limited types of radiance fields that come with the repos-\nitory. However, as shown before, the sampling process is in-\ndependent of the radiance field representation, thus it should\nbe easily transferable across different NeRF variants. Due to\nthe various implementation details, it typically requires sig-\nnificant effort to correctly implement an efficient sampling\napproach from scratch. Therefore, having an implementa-\ntion that is easily transferable from repository to repository\nwould be valuable in supporting future research on NeRF.\nInsights from Unified Formulation.\nComprehending the\nsampling spectrum through the lens of Transmittance Estima-\ntor paves the way for investigating novel sampling strategies.\nFor example, our framework reveals that the Occupancy\nGrid from Instant-NGP [17] and the Proposal Network from\nMip-NeRF 360 [2] are not mutually exclusive but comple-\nmentary, as both aim to estimate the transmittance along the\nray. Therefore, combining them becomes straight-forward:\none can first compute the transmittance with the occupancy\ngrid, and then refine the estimated transmittance with a pro-\nposal network. This enables both skipping on the empty\nspace and concentrating the samples onto the surface. We\nexplore this approach in Section 4.4 and demonstrate that it\novercomes the limitation of the proposal network approach,\nwhich always samples the entire space. Furthermore, this\nformulation could potentially shed light on questions such\nas how to enhance the sampling procedure with depth in-\nformation or other priors, which we encourage readers to\ninvestigate further.\n4. NerfAcc Toolbox\nIn this paper, we present NerfAcc toolbox, designed for\nNeural radiance field Acceleration. It provides efficient\nsampling for volumetric rendering, that is universally appli-\ncable and easily integrable for a diverse range of radiance\nfields [16, 4, 17, 14, 32]. In this section, we first introduce\nthe design principles of this toolbox, along with critical\nimplementation details. To demonstrate its flexibility, we\nfurther show that it can significantly speedup the training\nof various NeRF-related papers by 1.5\u00d7 to 20\u00d7 with only\nminor modifications to existing codebases.\n4.1. Design Principles\nThis library is designed with these goals in mind:\n\u2022 Plug-and-play. Our primary objective is to ease the\nchallenges of implementing an efficient volumetric sam-\npling technique for the research community. Therefore,\nNerfAcc is designed as a standalone library that can be\neasily installed from PyPI on both Windows and Linux\nplatforms, and seamlessly integrated into any PyTorch\ncodebase.\n\u2022 Efficiency & Flexibility. To maximize the speed of\nthe code, we fuse the operations into CUDA kernels as\nAlgorithm 1: NerfAcc Rendering Pipeline. In NerfAcc, the\nsamples are created from a transmittance estimator, which can be\nupdated during the NeRF training. See Section 4.2 for details.\n# nerf: a radiance field model.\n# r_o: ray origins. (n_rays, 3)\n# r_d: ray normalized directions. (n_rays, 3)\n# e.g., Prop Net, Occ Grid.\nestimator = nerfacc.TransmittanceEstimator()\ndef density_fn(t0, t1, r_id):\n\"\"\"Query density from nerf.\"\"\"\nreturn nerf.density(r_o[r_id], r_d[r_id], t0, t1)\ndef rgb_density_fn(t0, t1, r_id):\n\"\"\"Query rgb and density from nerf.\"\"\"\nreturn nerf.forward(r_o[r_id], r_d[r_id], t0, t1)\n# Efficient sampling.\n# (t0, t1, r_id): packed samples. (all_samples,)\nt0, t1, r_id = nerfacc.sampling(\nr_o, r_d, estimator, density_fn=density_fn\n)\n# Differentiable volumetric rendering.\ncolor, opacity, depth, aux = nerfacc.rendering(\nt0, t1, r_id, rgb_density_fn=rgb_density_fn\n)\n# Update the transmittance estimator.\nestimator.update_every_n_steps(t0, t1, r_id, aux)\n# nerf, r_o and r_d all receive gradients.\nF.mse_loss(color, color_gt).backward()\nmuch as possible, while exposing flexible Python APIs\nto the users.\n\u2022 Radiance Field Complexity. We target on supporting\nany radiance fields that are designed for per-scene opti-\nmization, including both density-based and SDF-based\nfields, for both static and dynamic scenes.\n4.2. Implementation Details\nNerfAcc incorporates two advanced sampling methods\nthat can be decoupled from the radiance field representation,\nnamely the Occupancy Grid from Instant-NGP [17] and the\nProposal Network from Mip-NeRF 360 [2]. Algorithm 1\npresents the pseudo code for volumetric rendering using\nthe NerfAcc toolbox. In this section, we do not dive into\nthe details of each algorithm, as we basically follow the\noriginal paper\u2019s implementation. Instead, we discuss the\nimplementation designs that are crucial to maintain high\nefficiency and flexibility of this toolbox.\nSample as Interval.\nIn the NerfAcc toolbox, instead of\nrepresenting each sample with a coordinate x, we represent\nthe sample as an interval along the ray (t0, t1, r), where t0\nand t1 are the start and end of the interval along the r-th ray.\nThis interval-based representation offers three key advan-\ntages. Firstly, representing a sample as an interval instead of\na single point allows us to support research based on cone-\nbased rays for anti-aliasing, such as Mip-NeRF [1] and Mip-\nNeRF 360 [2]. Secondly, since in almost all cases ti does\nMethods\nDataset Speedup T (min) \u2193 PSNR \u2191 LPIPS \u2193\nTensoRF [4]\nT&T\n1.5\u00d7\n18.3\n28.13\n0.143\n+ nerfacc (occ)\n12.6\n28.10\n0.150\nTensoRF [4]\nSyn.\n1.6\u00d7\n10.6\n32.52\n0.047\n+ nerfacc (occ)\n6.5\n32.51\n0.044\nNeRF [16]\nSyn.\n20\u00d7\n>1000\n31.00\n0.081\n+ nerfacc (occ)\u2020\n52.0\n31.55\n0.072\nInstant-NGP [17]\nSyn.\n1.0\u00d7\n4.4\n32.35\n-\n+ nerfacc (occ)\u2020\n4.4\n32.55\n0.056\n+ nerfacc (prop)\u2020\n5.2\n31.40\n0.064\nInstant-NGP [17]\n360\n1.1\u00d7\n5.3\n25.93\n-\n+ nerfacc (occ)\u2020\n5.0\n26.41\n0.353\n+ nerfacc (prop)\u2020\n4.9\n27.58\n0.292\n(a) Static NeRFs.\nMethods\nDataset Speedup T (min) \u2193 PSNR \u2191 LPIPS \u2193\nTiNeuVox [20]\nHyper.\n1.7\u00d7\n56.3\n24.19\n0.425\n+ nerfacc (occ)\n33.0\n24.19\n0.434\n+ nerfacc (prop)\n34.3\n24.26\n0.398\nTiNeuVox [5]\nD-NeRF\n2.8\u00d7\n11.8\n31.14\n0.050\n+ nerfacc (occ)\n4.2\n31.75\n0.038\nK-Planes [7]\nD-NeRF\n1.6\u00d7\n63.9\n30.28\n0.043\n+ nerfacc (occ)\n38.8\n30.35\n0.042\nT-NeRF [21]\nD-NeRF\n20\u00d7\n>1000\n28.78\n0.069\n+ nerfacc (occ)\u2020\n58.0\n32.22\n0.040\n(b) Dynamic NeRFs.\nMethods\nDataset Speedup T (min) \u2193 PSNR \u2191 LPIPS \u2193ER / ET \u2193\nBARF [20]\nSyn.\n4.5\u00d7\n586\n28.83\n0.054\n0.19 / 0.74\n+ nerfacc (occ)\n130\n30.11\n0.044\n0.07 / 0.35\n(c) NeRFs for Camera Optimization. Camera rotational / transna-\ntional errors are denoted as ER/ET . ET is scaled by 100.\nTable 2: Improvements on Various NeRFs. Experiments\nare conducted by replacing necessary code in the official\nrepositories, except for those marked by \u2020 that are based on\nour re-implementations.\nnot require gradients, using (t0, t1, r) instead of (x0, x1) to\nrepresent the interval allows for the detachment of the sam-\npling process from the differentiable computational graph,\nthereby maximizing its speed. Lastly, the ray id r attached to\neach sample enables support for various numbers of samples\nacross rays with a packed tensor, which we will discuss in the\nnext paragraph. A similar representation has been adopted\nin Nerfstudio [29] to support various radiance fields.\nPacked Tensor.\nTo support sampling with spatial skipping,\nit is necessary to consider that each ray may result in a dif-\nferent number of valid samples. Storing the data as a tensor\nwith shape (n_rays, n_samples, ...) and an extra mask\nwith shape (n_rays, n_samples) to indicate which sam-\nples are valid leads to significant inefficient memory usage\nOriginal\n18.3min/28.13dB\n5.3min/25.93dB\n63.9min/30.28dB\n56.3min/24.19dB\n586min/28.83dB\nw/ NerfAcc\n12.6min/28.10dB\n5.0min/27.21dB\n38.8min/30.35dB\n34.3min/24.26dB\n130min/30.11dB\nTensoRF [4]\nInstant-NGP [17]\nT-NeRF [16]\nTiNeuVox [5]\nBARF [14]\nTanks&Temples Dataset\n360 Dataset\nD-NeRF Dataset\nHyperNeRF Dataset\nNeRF-Synthetic Dataset\nFigure 3: Qualitative Results. NerfAcc is able to significantly reduce the training time of various NeRF-related methods\nacross multiple datasets, while also yielding slightly improved quality in the majority of cases. The training time and test\nPSNR are shown on the left corner of each image.\nCode Snippets from BARF\nEquavlent code with NerfAcc\nFigure 4: Plug-and-play Example in BARF [14]\u2019s Repos-\nitory. With around 50 lines of code change, both training\nspeed and performance can be greatly improved with effi-\ncient sampling equipped in NerfAcc, as shown in Table 2b.\nwhen large portions of space are empty. To address this, in\nNerfAcc, we represent samples as \u201cpacked tensors\u201d with\nshape (all_samples, ...), in which only valid samples\nare stored (see Algo. 1). To keep track of the associated rays\nfor each sample, we also host an integer tensor with shape\n(n_rays, 2), which stores the start index in the packed ten-\nsors and the number of samples on this ray. This approach is\nsimilar to that used in Instant-NGP [17] and PyTorch3D [22].\nNo Gradient Filtering.\nAfter importance sampling, inac-\ncurate transmittance estimations can result in some samples\nlying in empty or occluded spaces, particularly in spatial\nskipping methods like Occupancy Grid. These samples can\nbe filtered before being included in PyTorch\u2019s differentiable\ncomputation graph by evaluating their transmittance using\nthe radiance field with gradients disabled. As backward\npasses are not required during filtering, this is much faster\n(\u223c10\u00d7) than keeping all samples in the computation graph.\nIn practice, samples with transmittance below 10\u22124 are dis-\nregarded during this process with almost no influence on the\nrendering quality. Note that this strategy is inspired from\nInstant-NGP [17]\u2019s implementation.\n4.3. Case Studies\nWe showcase the flexibility of NerfAcc on three types of\nNeRFs across seven papers: static NeRFs (NeRF [16], Ten-\nsoRF [4], Instant-NGP [17]); dynamic NeRFs (D-NeRF [21],\nK-Planes [7] TiNeuVox [5]); and a NeRF variation for cam-\nera optimization (BARF [14]). Although many of these meth-\nods, e.g., Instant-NGP, TensoRF, TiNeuVox and K-Planes,\nhave already been highly optimized for efficiency, we are\nstill able to accelerate their training by a large margin and\nachieve slightly better performance on nearly all cases. It is\nworth mentioning that experiments with TensoRF, TiNeuVox,\nK-Planes and BARF are conducted by integrating NerfAcc\ninto the official codebase with around 100 lines of code\nchange. The results of our experiments, including those\nof our baselines, are presented in Table 2a,2b, and2c, all\nof which were conducted under the same physical environ-\nment using a single NVIDIA RTX A5000 GPU to facilitate\ncomparison, as per [25]. Aside from the experiments re-\nported in this paper, NerfAcc has also been integrated into a\nfew popular open-source projects such as nerfstudio [29]\nfor density-based NeRFs, as well as sdfstudio [38] and\ninstant-nsr-pl [10] for SDF-based NeRFs.\nStatic NeRFs.\nIn this task, we experiment with three\nNeRF vaiants, including the original MLP-based NeRF [16],\nTensoRF [4] and Instant-NGP [17]. We show that Ner-\nfAcc works with both MLP-based and Voxel-based radi-\nance fields, on both bounded (NeRF-Synthetic dataset [16],\nTank&Template dataset [12]) and unbounded scenes (360\nDataset [2]). It is worth to note that with NerfAcc, one\ncan train an Instant-NGP model with pure Python code and\nachieve slightly better performance than the official pure\nCUDA implementation, as shown in Table 2a.\nDynamic NeRFs.\nIn this task, we apply the NerfAcc tool-\nbox to T-NeRF [21], K-Planes [7] and TiNeuVox [5], cov-\nering both the synthetic (D-NeRF [21]) and \u201cin-the-wild\u201d\ncaptures1 (that accompany HyperNeRF [20]). When apply-\ning the occupancy grid approach to accelerate those dynamic\nmethods, instead of representing a static scene with it, we\nshare the occupancy grid across all frames. In other words,\ninstead of using it to indicate the opacity of an area at a\nsingle timestamp, We use it to indicate the maximum opacity\nat this area over all the timestamps. This is not optimal but\nstill makes the rendering very efficient, due to the fact there\nare limited movements in these datasets.\nNeRFs for Camera Optimization.\nIn this task, we em-\nployed the NerfAcc toolbox to BARF [14] on the NeRF-\nSynthetic dataset with perturbed cameras. The goal is to\njointly optimize the radiance field and camera extrinsic for\nmulti-view images. We observed that the spatial skipping\nsampling provided by NerfAcc facilitated faster training and\nsignificantly improved both image quality and camera pose\nreconstruction. These improvements could be attributed to\nthe sparsity enforced in our sampling procedure. This finding\nmay provide interesting avenues for future research.\nAnalysis of Different Sampling Approaches.\nResults in\nTable 2a show that the choice between occupancy grid and\nproposal network sampling can noticeably affect run-time\nand performance on different datasets. As each approach\nrelies on a distinct set of hyperparameters, a systematic com-\nparison between the two methods is crucial by sweeping\nthe hyperparameter space. We varied the resolution and\nmarching step size for occupancy grid and the number of\nsamples and size of the proposal network for the proposal\nnetwork approach. We plot pareto curves for each approach\nfor both the NeRF-Synthetic and Mip-NeRF 360 datasets\nin Fig.5. This analysis indicates that occupancy grid sam-\npling is suitable for the NeRF-Synthetic dataset, whereas\nthe proposal network approach performs better on the 360\ndataset. This is likely because the NeRF-Synthetic dataset\n1These datasets teleport cameras and do not represent real captures [9].\ncontains more empty space that can be skipped effectively\nusing the occupancy grid approach. However, in the case\nof real, unbounded data, the use of the occupancy grid ap-\nproach is limited by the bounding box and the lack of empty\nspace to skip, making the proposal network approach more\neffective. These experiments used the radiance field from\nInstant-NGP [17], with the same training recipes.\n4.4. Combined Sampling\nA benefit from the unified concept of transmittance esti-\nmator introduced in Section 3, is that it\u2019s straight-forward\nto combine the two distinct sampling approaches, as both of\nthem essentially provide an estimation of the transmittance\nthat can be used for importance sampling. For example, we\nfind that simply stacking an occupancy grid on top of the\nproposal network, can significantly reduce the number of\nrays and shrink the near-far plane for the remaining rays on\nthe NeRF-Synthetic dataset. This leads to improvements in\nquality, from 31.40dB to 32.35dB, and a reduction in train-\ning time, from 5.2min to 4.3min, compared to using only the\nproposal network for importance sampling. Figure 6 shows\nan example with the FICUS scene, where the floaters are\ncleaned out with the combined sampling. This experiment is\nconducted using the HashEncoding from Instant-NGP [17]\nas the radiance field representation.\n5. Conclusions\nIn conclusion, this paper highlights the significant im-\npact of advanced sampling approaches on improving the\nefficiency of Neural Radiance Fields (NeRF) optimization\nand rendering. We demonstrate that advanced sampling can\nsignificantly speed up the training of various recent NeRF\npapers, while maintaining high-quality results. The devel-\nopment of NerfAcc, a flexible Python toolbox, enables re-\nsearchers to incorporate advanced sampling methods into\nNeRF-related methods easily. The exploration and compari-\nson of advanced sampling methods are important steps to-\nwards developing more efficient and accessible NeRF-based\nmethods. The presented results also demonstrate the poten-\ntial for further research to improve the performance of NeRF\nand other related techniques through advanced sampling\nstrategies.\nAcknowledgement\nThis project was supported in part by the Bakar Fellows\nProgram and the BAIR/BDD sponsors.\nReferences\n[1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neural\nPSNR\nPSNR\nPSNR\nPSNR\nT (min)\nFPS\n(a) NeRF-Synthetic Dataset\n(b) Mip-NeRF 360 Dataset\nT (min)\nFPS\nFigure 5: Comparison between Different Sampling Approaches in terms of Training Time and Rendering FPS. We\nsweep the hyper-parameters space for each sampling approach, and find out that occupancy grid approach gives the best\nperformance-speed trade-off on the NeRF-Synthetic dataset, while the proposal network approach performs the best on the\nMip-NeRF 360 dataset. Note T (min) denotes for training time and FPS is for rendering frames per second. All experiments\nuse the HashEncoding from Instant-NGP [17] as the radiance field representation. Please see the supplementary materials for\nthe hyperparameter space that we explored.\nRendered RGB\nRendered Opacity\nProposal\nNetwork\nProposal\nNetwork\nOccupancy\nGrid\n+\n5.2min/31.40dB\n4.3min/32.35dB\nFigure 6: Results of Combined Sampling on NeRF-\nSynthetic Dataset. Benefit from our unified concept of\ntransmittance estimator, we find it\u2019s straight-forward to com-\nbine the two distinct sampling approaches.\nradiance fields. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 5855\u20135864, 2021.\n[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5470\u20135479, 2022.\n[3] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-\nbased neural radiance fields. arXiv, 2023.\n[4] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. arXiv preprint\narXiv:2203.09517, 2022.\n[5] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xi-\naopeng Zhang, Wenyu Liu, Matthias Nie\u00dfner, and Qi Tian.\nFast dynamic radiance fields with time-aware neural voxels.\nIn SIGGRAPH Asia 2022 Conference Papers, 2022.\n[6] Julian Fong, Magnus Wrenninge, Christopher Kulla, and Ralf\nHabel. Production volume rendering: Siggraph 2017 course.\nIn ACM SIGGRAPH 2017 Courses, pages 1\u201379. 2017.\n[7] Sara Fridovich-Keil, Giacomo Meanti, Frederik Warburg,\nBenjamin Recht, and Angjoo Kanazawa. K-planes: Explicit\nradiance fields in space, time, and appearance. arXiv preprint\narXiv:2301.10241, 2023.\n[8] Clement Fuji Tsang, Maria Shugrina, Jean Francois Lafleche,\nTowaki Takikawa, Jiehan Wang, Charles Loop, Wenzheng\nChen, Krishna Murthy Jatavallabhula, Edward Smith, Artem\nRozantsev, Or Perel, Tianchang Shen, Jun Gao, Sanja Fi-\ndler, Gavriel State, Jason Gorski, Tommy Xiang, Jianing\nLi, Michael Li, and Rev Lebaredian. Kaolin: A pytorch\nlibrary for accelerating 3d deep learning research. https:\n//github.com/NVIDIAGameWorks/kaolin, 2022.\n[9] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and\nAngjoo Kanazawa. Monocular dynamic view synthesis: A\nreality check. In Advances in Neural Information Processing\nSystems, 2022.\n[10] Yuan-Chen Guo. Instant neural surface reconstruction, 2022.\n[11] Yoonwoo Jeong, Seungjoo Shin, and Kibaek Park. Nerf-\nfactory: An awesome pytorch nerf collection, 2022.\n[12] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\nKoltun. Tanks and temples: Benchmarking large-scale scene\nreconstruction. ACM Transactions on Graphics, 36(4), 2017.\n[13] Ruilong Li, Julian Tanke, Minh Vo, Michael Zollhofer, Jur-\ngen Gall, Angjoo Kanazawa, and Christoph Lassner. Tava:\nTemplate-free animatable volumetric actors. arXiv preprint\narXiv:2206.08929, 2022.\n[14] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon\nLucey. Barf: Bundle-adjusting neural radiance fields. In IEEE\nInternational Conference on Computer Vision (ICCV), 2021.\n[15] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,\nJonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-\nworth. Nerf in the wild: Neural radiance fields for uncon-\nstrained photo collections. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 7210\u20137219, 2021.\n[16] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view synthe-\nsis. Communications of the ACM, 65(1):99\u2013106, 2021.\n[17] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. arXiv preprint arXiv:2201.05989,\n2022.\n[18] Michael Oechsle, Songyou Peng, and Andreas Geiger.\nUnisurf: Unifying neural implicit surfaces and radiance\nfields for multi-view reconstruction. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 5589\u20135599, 2021.\n[19] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5865\u20135874, 2021.\n[20] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. arXiv preprint arXiv:2106.13228, 2021.\n[21] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-nerf: Neural radiance fields\nfor dynamic scenes. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10318\u201310327, 2021.\n[22] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor\nGordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari.\nAccelerating 3d deep learning with pytorch3d. arXiv preprint\narXiv:2007.08501, 2020.\n[23] Christian Reiser, Richard Szeliski, Dor Verbin, Pratul P Srini-\nvasan, Ben Mildenhall, Andreas Geiger, Jonathan T Barron,\nand Peter Hedman. Merf: Memory-efficient radiance fields\nfor real-time view synthesis in unbounded scenes. arXiv\npreprint arXiv:2302.12249, 2023.\n[24] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5459\u2013\n5469, 2022.\n[25] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Improved di-\nrect voxel grid optimization for radiance fields reconstruction.\narXiv preprint arXiv:2206.05085, 2022.\n[26] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas\nM\u00fcller, Morgan McGuire, Alec Jacobson, and Sanja Fidler.\nVariable bitrate neural fields.\nIn ACM SIGGRAPH 2022\nConference Proceedings, pages 1\u20139, 2022.\n[27] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis,\nCharles Loop, Derek Nowrouzezahrai, Alec Jacobson, Mor-\ngan McGuire, and Sanja Fidler. Neural geometric level of\ndetail: Real-time rendering with implicit 3d shapes. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 11358\u201311367, 2021.\n[28] Towaki Takikawa, Or Perel, Clement Fuji Tsang, Charles\nLoop, Joey Litalien, Jonathan Tremblay, Sanja Fidler, and\nMaria Shugrina.\nKaolin wisp:\nA pytorch library and\nengine for neural fields research.\nhttps://github.com/\nNVIDIAGameWorks/kaolin-wisp, 2022.\n[29] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent\nYi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake\nAustin, Kamyar Salahi, et al. Nerfstudio: A modular frame-\nwork for neural radiance field development. arXiv preprint\narXiv:2302.04264, 2023.\n[30] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\nJonathan T Barron, and Pratul P Srinivasan. Ref-nerf: Struc-\ntured view-dependent appearance for neural radiance fields.\nIn 2022 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 5481\u20135490. IEEE, 2022.\n[31] John Von Neumann. 13. various techniques used in connec-\ntion with random digits. Appl. Math Ser, 12(36-38):3, 1951.\n[32] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689, 2021.\n[33] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Vic-\ntor Adrian Prisacariu. Nerf\u2013: Neural radiance fields without\nknown camera parameters. arXiv preprint arXiv:2102.07064,\n2021.\n[34] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\nume rendering of neural implicit surfaces. Advances in Neural\nInformation Processing Systems, 34:4805\u20134815, 2021.\n[35] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks.\narXiv preprint\narXiv:2112.05131, 2021.\n[36] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\nAngjoo Kanazawa. Plenoctrees for real-time rendering of\nneural radiance fields. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 5752\u20135761,\n2021.\n[37] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4578\u20134587, 2021.\n[38] Zehao Yu, Anpei Chen, Bozidar Antic, Songyou Peng\nPeng, Apratim Bhattacharyya, Michael Niemeyer, Siyu Tang,\nTorsten Sattler, and Andreas Geiger. Sdfstudio: A unified\nframework for surface reconstruction, 2022.\n[39] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\nKoltun. Nerf++: Analyzing and improving neural radiance\nfields. arXiv preprint arXiv:2010.07492, 2020.\nA. Implementation Details for Case Studies\nThis section describes the usage of NerfAcc to enhance\nmultiple NeRFs, as discussed in Section 4.3 of the main\npaper. We provide detailed information on how to adapt\nNerfAcc to various NeRF models.\nA.1. Static NeRFs.\nVanilla NeRF [16].\nWe trained an 8-layer MLP with\nthe same structure as the vanilla NeRF paper. The paper\nemploys the PDF-based efficient sampling approach with\ntwo MLPs for course-to-fine sampling (64 + 128). Although\nthe PDF-based approach effectively concentrates samples\naround the surface, the MLP itself is computationally slow.\nTo accelerate the process, we used the spatial skipping ap-\nproach with Occupancy Grid provided by NerfAcc. Due to\nthe considerable amount of empty space in NeRF-Synthetic\ndataset, we are able to increase the number of samples per ray\nto 1024 without any memory issue. As a result, we achieved\na 20\u00d7 speedup and improved image quality by +0.5dB.\nTensoRF\n[4].\nInterestingly,\nincorporating\nNerfAcc\ninto its own repository results in a 1.5\u00d7 speedup for\nTensoRF [4], a more recent voxel-based method, on both\nthe NeRF-Synthetic and Tank-and-Temple datasets. We\nconjecture that this improvement is due to our update to\ntheir transmittance estimator. Although not discussed in its\npaper, TensoRF initially employs a skip-based transmittance\nestimator when sampling, similar to our occupancy grid.\nHowever, unlike our implementation, TensoRF\u2019s estimator\ndoes not maintain an accurate estimation of the main density\nfield and only updates twice throughout the entire training\nprocess. Our approach, on the other hand, distills a running\naverage of the density field every few training steps.\nInstant-NGP [17].\nFurthermore, we were able to repro-\nduce Instant-NGP [17] on the NeRF-Synthetic dataset with\nthe same training speed and slightly better performance\n(+0.2dB) using the Occupancy Grid. We also achieved\nsignificantly better results on the Mip-NeRF 360 dataset\nusing the proposal network approach. It is worth noting\nthat the original Instant-NGP implementation is in pure\nCUDA with all operations fused into the CUDA kernels.\nWith NerfAcc handling the underlying sampling logic, the\nentire training pipeline can be implemented in Python on top\nof PyTorch. We believe that our toolbox strikes a balance\nbetween high-performance computing and simplicity,\nbenefiting rapid research development.\nA.2. Dynamic NeRFs.\nT-NeRF [21]\nWe trained a T-NeRF model described in\nthe D-NeRF paper [21] on the D-NeRF dataset. By utilizing\nthe occupancy grid in NerfAcc, we achieved a remarkable\nspeedup of 20\u00d7 with a corresponding +3.5dB improvement\nin performance. It is worth noting that we performed this\nexperiment using our re-implementation of T-NeRF model\nand training recipe.\nK-Planes [7].\nFor K-Planes, we achieve around 1.6\u00d7\nspeedup on D-NeRF dataset by replacing its original\nproposal-based transmittance estimator with the occupancy\ngrid. We believe that the sparsity in the D-NeRF dataset\nmakes it more suitable for spatial skipping approaches.\nTiNeuVox [5].\nOur experiments with TiNeuVox on\nboth datasets suggest a universal speedup by using our\ntoolbox. Specifically, NerfAcc speeds up training by 2.8\u00d7\non D-NeRF and 1.7\u00d7 on HyperNeRF datasets, respectively.\nThe original TiNeuVox implementation extends upon\nDVGO [24] and directly marches rays within the main den-\nsity grid, which can be wasteful since space skipping does\nnot require precise geometry. Our occupancy grid overcomes\nthis issue and has been shown to be more effective. Addition-\nally, we found that a time-conditioned proposal network can\nachieve similar performance on the real bounded scenes in\nthe HyperNeRF dataset, which aligns with what we observed\nin the Instant-NGP experiments with the proposal network.\nA.3. Camera Optimization.\nBARF [14].\nIn the original implementation, BARF\nuniformly samples within the near-far range for each ray,\nanalogous to the course level in the vanilla NeRF [16].\nWe integrated the occupancy grid from our toolbox for\nspatial skipping, resulting in a 4\u00d7 speedup during training.\nAdditionally, this led to improved image quality (+1.3dB)\nand roughly 2\u00d7 lower camera registration error.\nB. Hyper-parameter Space\nFigure 5 in the main paper demonstrates the significant\nimpact of hyper-parameters on the performance of each\nsampling approach. The occupancy grid approach relies\non several primary hyper-parameters, such as the binary\nthreshold \u03c4, the grid resolution L3, and the marching\nstep size \u2206t. Meanwhile, the proposal network approach\ninvolves hyper-parameters related to the setup of the\nproposal network(s) F\u0398, as well as the number of samples\nN to be taken along each ray. To ensure the robustness of\nour experiments, each data point in Figure 5 is trained with\na random combination of hyper-parameters drawn from a\nreasonable range for each dataset.\n"
  },
  {
    "title": "Large Language Model Programs",
    "link": "https://arxiv.org/pdf/2305.05364.pdf",
    "upvote": "1",
    "text": "Large Language Model Programs\nImanol Schlag 1 Sainbayar Sukhbaatar 2 Asli Celikyilmaz 2 Wen-tau Yih 2 Jason Weston 2\nJ\u00a8urgen Schmidhuber 1 3 Xian Li 2\nAbstract\nIn recent years, large pre-trained language mod-\nels (LLMs) have demonstrated the ability to fol-\nlow instructions and perform novel tasks from a\nfew examples. The possibility to parameterise an\nLLM through such in-context examples widens\ntheir capability at a much lower cost than \ufb01ne-\ntuning. We extend this line of reasoning and\npresent a method which further expands the ca-\npabilities of an LLM by embedding it within an\nalgorithm or program. To demonstrate the bene-\n\ufb01ts of this approach, we present an illustrative ex-\nample of evidence-supported question-answering.\nWe obtain a 6.4% improvement over the chain\nof thought baseline through a more algorithmic\napproach without any \ufb01netuning. Furthermore,\nwe highlight recent work from this perspective\nand discuss the advantages and disadvantages in\ncomparison to the standard approaches.\n1. Introduction\nScaling language models to hundreds of billions of param-\neters (LLMs) and training them on terabytes of text data\nhas led to state-of-the-art performance on a large variety of\nnatural language processing tasks. Additionally, it has also\nled to the emergent ability to learn a new skill merely from\ninstructions or a few examples, as seen in GPT-3 (Brown\net al., 2020). Despite this, LLMs struggle to display algo-\nrithmic abilities, such as sorting or searching, even when\n\ufb01netuned on traces of such (Anil et al., 2022; Valmeekam\net al., 2022; Liu et al., 2023; Deletang et al., 2023).\nFinetuning a model on human traces (i.e. imitation learn-\ning) is a common strategy to infuse a model with complex\nbehaviours. For example, in the context of LLMs, recent\nwork trains on expert demonstrations of interacting with\na browser (Nakano et al., 2021) or a webshop (Yao et al.,\n2022) in order to improve their respective tasks. However,\n1The Swiss AI Lab IDSIA, SUPSI & USI 2Meta AI 3King\nAbdullah University of Science and Technology. Correspondence\nto: Imanol Schlag <imanol@idsia.ch>.\nPreprint. Under review. Copyright 2023 by the author(s).\nthere are reasons to question the correctness of the algo-\nrithms learned from such examples. First, the decoder-only\nTransformer language model (Vaswani et al., 2017) is not\ncomputationally universal because it can only condition on\na \ufb01nite length input (Fan et al., 2021). Second, previous\nwork demonstrated that the decoder-only architecture strug-\ngles to learn even simple programs through gradient descent\nthat would generalise out of distribution (Anil et al., 2022;\nDeletang et al., 2023).\nBesides \ufb01netuning, in-context learning may be leveraged\nto improve a model\u2019s ability to execute algorithms. E.g.,\nrecently Zhou et al. (2023b) introduce a prompt construction\nwhich improves the LLM\u2019s ability to perform arithmetic\nalgorithms. A key characteristic of such approaches is the\nuse of a single call to the model and the use of a single\nprompt. As a consequence, the method is limited by the size\nof the input and the necessity to prompt engineer multiple\nalgorithm steps within one prompt. The authors report that\nthis can lead to interference between steps which hurts the\n\ufb01nal performance.\nAs an alternative, we propose embedding LLMs into a pro-\ngram or algorithm. Crucially, instead of the LLM being\nresponsible for maintaining the current state of the program\n(i.e. its context), the LLM, for each step of the program,\nis only presented with a step-speci\ufb01c prompt and context.\nHiding information which is irrelevant to the current step\nallows us to focus on isolated subproblems whose results are\nfurther combined in future calls to the LLM. This intuitive\napproach allows us to extend the ability of an LLM to more\ncomplex tasks which are currently too dif\ufb01cult either be-\ncause of a lack of ability or an architectural constraint such\nas an insuf\ufb01ciently large context. Concurrent work proves\nthat embedding an LLM within a recurrent algorithm which\nenables the interaction with an external memory component\nmakes the system computationally universal (Schuurmans,\n2023).\nBoth approaches, embedding and \ufb01netuning, incorporate\ndomain-speci\ufb01c information and thus lead to a more spe-\ncialised model. Embedding the model in an algorithm, or\nprogram with an LLM, forces the LLM to follow a high-\nlevel procedure (whatever it may be) which may limit its\n\ufb02exibility and requires a high-level understanding of the\narXiv:2305.05364v1  [cs.LG]  9 May 2023\nLarge Language Model Programs\nFigure 1. An illustration of our two-part LLM program example. The \ufb01rst part (left) of our program \ufb01lters out irrelevant paragraphs from\na large set. This is achieved using an LLM and the current question. The second part (right) depicts the generation of the individual\nreasoning steps until they result in an answer. The tree search is over the choice of paragraph used within the context when generating a\nsingle step in the reasoning chain. We expand the reasoning chain which ranks the highest as described in Section 3.1.2.\ncorrect procedure for a given task. On the other hand, \ufb01ne-\ntuning requires the recognition, and possibly generation, of\ndomain-speci\ufb01c training data which requires a similar level\nof understanding. We argue that there are situations where\nit may be more promising to embed the LLM in a program\nexplicitly instead of training on traces of a desired program.\nDue to the dif\ufb01culty and cost of training LLMs we have\nalready seen problem-speci\ufb01c LLM programs with no or\nvery targeted training lead to better performance on speci\ufb01c\nproblems such as reasoning (Kazemi et al., 2022), question-\nanswering (Zhou et al., 2023a), text-summarization (Wu\net al., 2021a), text-generation (Yang et al., 2022b), among\nothers (Zeng et al., 2023; Xu et al., 2021; Karpas et al.,\n2022).\nIn this work, we present the advantages and disadvantages\nof programming with LLMs and present a general approach\nwhich we call a Large Language Model Program (Section\n2). In Section 3, we then present an LLM program example\nfor evidence-supported question answering. In order to\naccurately answer a question, the program \ufb01rst extracts the\nimportant facts from the knowledge sources and seamlessly\nincorporates them into the reasoning of its response. A\nsimple illustration of the program is given in Figure 1. We\nthen go on to describe how our general approach can be\napplied to various other settings, described in Section 4.\n2. Limitations of LLMs and the Bene\ufb01t of\nProgramming With Them\nLLMs are a product of three necessary ingredients: massive\nparallel compute, large amounts of data, and a highly paral-\nlelisable language modelling architecture. To this date, the\narchitecture deployed by all LLMs of 175B parameters or\nmore is the decoder-only Transformer (Vaswani et al., 2017).\nThe ability to parallelise allows for ef\ufb01cient training of large\nmodels and the use of large amounts of data prevents it from\nover\ufb01tting. In fact, when training LLMs they rarely see any\ntraining data twice.\nLLMs which were trained on internet-scale data are impres-\nsive text generators that can produce plausible paragraphs\nin the form of short stories, reviews, summaries, poems,\netc. Such raw LLMs are trained on data that includes high-\nquality texts such as those from Wikipedia or published\nbooks but also subjectively low-quality texts with toxic and\nbiased content which then leads to LLMs that are to a cer-\ntain degree toxic and biased themselves (Dev et al., 2020;\nSheng et al., 2021).\nAnother drawback of training on random internet text is that\nsuch raw LLMs also struggle to be conversational, follow\ninstructions, use tools, or interact with an environment. Al-\nthough \ufb01netuning on examples of such behaviour has been\nshown to improve the model\u2019s ability (Ouyang et al., 2022;\nWei et al., 2022a; Gupta et al., 2022), such an approach is\ndif\ufb01cult to scale due to the lack of such data and may not\ngeneralise systematically due to the possibility of leveraging\nalgorithmic short-cuts (Liu et al., 2023).\nAnother limitation of LLMs arises from their decoder-only\nTransformer architecture, which has a \ufb01nite context that\nrestricts their capability to only process information within\nthe prede\ufb01ned context. While this architecture may be ad-\nvantageous for tasks like translation or language modelling,\nit struggles to learn certain classes of algorithms accurately,\nwhich can affect its ability to generalize beyond its training\nLarge Language Model Programs\ndistribution (Csord\u00b4as et al., 2021).\nSuch issues are likely going to be present also at a larger\nscale if an LLM is, either implicitly or explicitly, \ufb01netuned\non traces of algorithms. One example is presented by Anil\net al. (2022) who \ufb01netune LLMs of up to 64B parameters on\na variation of parity and report large drops in performance\nfor just slightly longer (or shorter) sequences than it was\ntrained on. It is worthwhile to mention that it has long been\nknown that recurrent neural networks, such as the Long\nShort-Term Memory (LSTM), can generalise perfectly on\nparity and other counter-based context-sensitive languages\n(Hochreiter & Schmidhuber, 1997; Gers & Schmidhuber,\n2001; Deletang et al., 2023).\nTo overcome such limitations, we propose LLM programs,\na general approach to enhance the capability of an LLM-\nbased system. Using an LLM program, we recognise the\nlimitation of the LLM as a general agent. Instead of further\ntraining the model, we recursively deconstruct the expected\nbehaviour into simpler steps that the LLM can perform to\na suf\ufb01cient degree. These individual steps are then strung\ntogether by a classic computer program (such as Python)\nthat parses the outputs of previous steps, uses control \ufb02ow,\nand augments the prompts of succeeding steps.\nOne important distinction of LLM programs from previ-\nous work that relies on external systems like calculators\n(Thoppilan et al., 2022) is that the current state of the pro-\ngram is not maintained by the LLM but by the program in\nwhich the LLM is embedded. Consequently, every call to\nthe LLM only includes the information that is necessary for\nthe particular step. There are several advantages to LLM\nprograms:\n\u2022 Embedding an LLM in a program can signi\ufb01cantly\nexpand the theoretical and practical capabilities of the\nsystem with no or little \ufb01netuning and can help the\nsystem generalise more systematically.\n\u2022 LLM programs incorporate high-level algorithmic in-\nformation by breaking down a complex task into multi-\nple simpler steps with little or no training. In contrast,\nlearning to perform a complex task from examples\nalone may require a large amount of compute and high-\nquality data.\n\u2022 Understanding the problem decomposition allows for\nmore \ufb01ne-grained input and output speci\ufb01cations for\neach subproblem or step of the program. These speci-\n\ufb01cations allow for a more manageable and systematic\nevaluation of the LLM\u2019s performance by testing the\nmodel on each subproblem in isolation, either through\na test set or manual inspection.\n\u2022 An LLM program performs many queries to the model\ninstead of just one. Each query may contain a step-\nspeci\ufb01c prompt that improves the model\u2019s performance\nfor that particular step. Prompt engineering in a setting\nwith a single query to the LLM may \ufb01nd it much more\nchallenging to provide an effective prompt because ex-\namples or descriptions for different steps may interfere\nwith each other and the prompt overall will take up\nmore space within its already limited context.\nIn summary, LLM programs offer a promising way to ad-\ndress the limitations of LLMs and expand their practical\nutility in various domains. In the next section, we will take\nthe reader through an example of an LLM program for the\npurpose of question-answering.\n3. An LLM Program for Question Answering\nMotivating example.\nImagine using an LLM as an inter-\nface for a company\u2019s website. The LLM was not trained on\nthat website, which means that it would not be able to an-\nswer questions about the company\u2019s newest products, such\nas its features, speci\ufb01cations, or availability, or about the\nopening hours of a new store. In this example of reading\ncomprehension, an LLM would perform poorly because it\nhas not been trained on the company\u2019s website nor can it\n\ufb01t the entire website into its context. Finetuning the LLM\nmay not be practical, as training these models is costly and\nwebsites often undergo changes (e.g. a product may un-\nexpectedly become unavailable) which would require the\nmodel to be constantly retrained. Additionally, it is unclear\nto which extent the \ufb01netuned model would be faithful to the\nfacts of the \ufb01netuning data.\nA more practical approach is to use a frozen LLM that\nhas exclusive access to the latest version of the website\nwhere each page represents one document. Since we cannot\nload all documents at once, we may construct a program to\ncarefully select the documents necessary to answer a user\u2019s\nquestion.\nThe dataset.\nWe construct a dataset to develop an LLM\nprogram which can answer questions by leveraging many\nadditional knowledge sources. To this end, we take advan-\ntage of the StrategyQA dataset (Geva et al., 2021). The\nStrategyQA dataset is a binary classi\ufb01cation benchmark\nfor implicit multi-step question-answering. The reason-\ning traces in StrategyQA are more dif\ufb01cult than previous\nquestion-answering datasets due to different forms of ques-\ntion decomposition and the need for detailed knowledge\nfrom different domains. Consider e.g. the question Can sun-\nlight travel to the deepest part of the Black Sea?. In order to\nanswer this question the LLM needs to know that the deep-\nest part of the Black Sea is about 2k meters whereas sunlight\nonly penetrates water up to about 1k meters. Having that\ninformation, inferring the answer becomes signi\ufb01cantly eas-\nier for an LLM . See our experiments in Table 2 where the\nLarge Language Model Programs\nLLM performs best when it has all the necessary facts in its\ncontext.\nWe build a question-answering dataset from StrategyQA\nwhere every question is supported with a large number of\nknowledge sources of which only a few are actually relevant\nto the current question. Most of them will be randomly\nsampled from other questions and are thus very unlikely to\ncontain any useful information. Crucially, even just a hand-\nful of those paragraphs would be too long for the context\nlength of current LLMs.\nThe StrategyQA dataset in total consists of 2780 questions\nannotated with their decomposition and per-step evidence\nparagraphs sourced from Wikipedia. About 918 of the ques-\ntions in StrategyQA come with evidence paragraphs that\nsupport all reasoning steps. We will limit ourselves to this\nsubset to ensure that the additional knowledge sources ac-\ntually contain all information necessary to infer the answer.\nOtherwise, it would require the LLM to have learned the\nnecessary facts from its pre-training data, which we do not\nassume to be the case.\nPrevious work reported mixed performances for question-\nanswering on the regular StrategyQA benchmark (Chowdh-\nery et al., 2022). This might be because the facts used for\nindividual reasoning steps are very rare and unlikely to be\nremembered or because the reasoning is too complex. While\nimproved reasoning methods (such as chain of thought (Wei\net al., 2022b; Nye et al., 2022; Kojima et al., 2022; Wang\net al., 2023)) have shown little performance gain on this\ndataset with models of 175B parameters (Srivastava et al.,\n2022; Taylor et al., 2022), larger models and models which\nhave been trained longer do seem to improve (Chowdhery\net al., 2022; Hoffmann et al., 2022). This may indicate that\nthe lack of knowledge is more problematic on this dataset\nthan the ability to reason.\nThe LLM.\nIn our experiments, we use OPT-175B (Zhang\net al., 2022), a raw language model of 175B parameters\ntrained on 300B tokens from a large mix of news articles,\nbooks, Pushshift.io Reddit previously compiled by a third\nparty, and the Pile (Gao et al., 2020). Because it has not been\n\ufb01netuned on high-quality instructional or conversational\ndata, we classify it as a raw language model. As reported by\nZhang et al. (2022), we \ufb01nd OPT-175B struggles to follow\ninstructions. Nevertheless, we demonstrate that we can\nachieve more complex behaviour by embedding OPT within\na program.\n3.1. Developing the LLM Program\nFor the problem of evidence-supported question-answering,\nwe decompose the problem into a \ufb01ltering part and a tree\nsearch part. Given the question, the \ufb01ltering part will loop\nover the provided paragraphs and select the most relevant\nones. In the second part, we will search over reasoning\nchains by generating one reasoning step at a time. When\ngenerating a reasoning step we can choose which one of the\n\ufb01ltered paragraphs we want to condition on. Similar to a\ntree search, we rank the reasoning chains according to our\nranking metric and expand the highest-ranked one by gener-\nating n possible continuations given n evidence paragraphs.\nThis process repeats until it produced an answer or until\nit reached the maximum number of steps at which point it\nwill force the LLM to answer the question by evaluating the\nnegative log-likelihood (NLL) of a \ufb01xed yes or no answer.\nAn example trace.\nWhen answering a question like\nCould the members of The Police perform lawful arrests?\nwe use the LLM to select a few paragraphs from a much\nbigger collection of which many are not relevant to this\nparticular question. For this example, selecting the top 10\nparagraphs according to OPT is suf\ufb01cient to capture all the\nrelevant information to successfully reason about the answer.\nTo \ufb01nd a strong reasoning sequence, we generate each step\nusing a different paragraph as context. The tree search is\nover the choice of paragraph to be used as context for a\nparticular step. For the question about the band The Police\nthe highest ranked step has a paragraph in its context which\nis about the members of the band. Given that context, the\nmodel generates the \ufb01rst step The members of The Police\nare Sting, Stewart Copeland, Andy Summers, and Henry\nPadovani. Following the \ufb01rst step, the highest-ranked step\nis conditioned on another paragraph with more details about\nthe band The Police in general. This results in the second\nstep which says The members of The Police are not police\nof\ufb01cers. After that step, the model\u2019s highest-ranked next\nstep is conditioned on a paragraph which explains that only\npolice offers are allowed to arrest people. From that the gen-\nerated next step says Thus, the members of The Police could\nnot perform lawful arrests.. Finally, the model generates the\nlast step Thus, the answer is no., which is recognised as an\nanswer and evaluated to be correct.\nIn the following two subsections, we go into more detail\nabout each part of our LLM program.\n3.1.1. EVIDENCE FILTERING EXPERIMENTS\nThis section describes our approaches to the paragraph \ufb01lter-\ning part of our program and their performance. Developing\nand evaluating each call to the LLM in isolation allows us to\nexperiment, improve, and analyse the overall performance\nin a systematic way. Our \ufb01rst attempt will rely on few-shot\nexamples and prompt-engineering the model to classify one\nparagraph given the question.\nBlackbox prompting.\nOur \ufb01rst approach centres on few-\nshot prompting, using OPT to output a yes or no answer\nwhen asked whether a certain paragraph is relevant to a spe-\nLarge Language Model Programs\nci\ufb01c question. This binary classi\ufb01cation dataset comprises\n300 samples randomly selected from the StrategyQA data.\nEach sample includes a question and an associated para-\ngraph, and the objective is to classify whether the paragraph\nis relevant to the question.\nWe carefully designed the dataset for this subproblem so\nthat half of the questions are paired with paragraphs ran-\ndomly selected from a list of evidence paragraphs, while the\nother half are paired with unrelated paragraphs from other\nquestions. This ensures an equal distribution of relevant\nand irrelevant paragraphs which puts the random baseline\nat 50%. Our experiments in Table 1 indicate that neither\nOPT nor InstructGPT (Ouyang et al., 2022) achieves satis-\nfactory performance in classifying a single paragraph. Even\nTk-Instruct (Wang et al., 2022), a model which has been\ntrained on a large variety of distinct and expert-written tasks,\nfails to reliably recognise relevant paragraphs.\nThis is surprising since the classi\ufb01cation task seems sim-\nple for humans: it is often inconceivable how a randomly\nsampled paragraph could be relevant to the question. Fur-\nthermore, evidence paragraphs often share words or phrases\nor are semantically relevant to the question. Because of the\noutcome of our isolated experiment, we decide to explore\nan alternative approach.\nTable 1. Top-1 binary classi\ufb01cation accuracies for a single para-\ngraph using prompt engineering techniques. Random guessing is\nat 50%. Note that using this method to classify from a large set of\nparagraphs will signi\ufb01cantly reduce the accuracy.\nMETHOD\nACCURACY\nOPT-175B FEW-SHOT PROMPT\n53.33%\nTEXT-DAVINCI-002 (INSTRUCTGPT)\n55.67%\nOPT-175B FEW-SHOT + CHAIN OF THOUGHT\n56.00%\nTK-INSTRUCT 11B\n61.60%\nLikelihood evaluation.\nOur second approach does not\ntreat the LLM as a black box anymore but instead directly\nuses the average negative log-likelihood (NLL) of the ques-\ntion when following each paragraph to create a ranking of\nall paragraphs. To evaluate this subproblem in isolation,\nwe constructed a new dataset. Each question now consists\nof 100 paragraphs where one is sampled from the list of\nevidence paragraphs of that question and the other 99 are\nsampled from other questions. In this setting, random guess-\ning only achieves 1% accuracy. Notice that 100 paragraphs\nclearly exceed the context length of OPT. We present the\npseudo-code in Algorithm 1 (see Appendix).\nWe achieve good results without a description or few-shot\nexamples. We plot accuracy over top-n in Figure 3 and the\naverage likelihood of each paragraph given each question\nin Figure 2. Top-1 accuracy is 0.79 and top-5 is already at\nFigure 2. Average likelihood of each question (row) given a para-\ngraph (column) normalised over paragraphs.\n0.93 which shows that the ranking approach is vastly more\neffective than the blackbox prompting approach despite the\nsimplicity of the task and the instruction-\ufb01netuning of some\nof the models. It also serves as an example of the advantage\nof embedding an LLM within a program because the LLM\ndoes not have access to the NLL of its own outputs and,\nthus, this superior solution would have not been possible\notherwise.\n3.1.2. TREE SEARCH EXPERIMENT\nIn the previous subsection, we presented the \ufb01ltering part of\nour program: measuring the relevancy of each paragraph, or-\ndering them, and selecting the top n. It trivially generalises\nin a systematic way to more paragraphs since the process-\ning of the list of paragraphs is done by a Python for-loop\nand doesn\u2019t solely rely on an LLM\u2019s ability to implement\nit reliably. This is especially powerful if we know what the\noptimal algorithm or behaviour should be and if we want to\nhave control over it.\nIn this section, we present a more systematic search over\nreasoning steps to ensure generalisation where we consider\nit most crucial. As in the \ufb01ltering part, we develop and test\nthe tree search in isolation using a dedicated dataset.\nRaw LLMs do not have access to an external system when\ndeducing answers to dif\ufb01cult questions. This means that the\nmodel can only rely on the noisy and inevitably incomplete\nand outdated knowledge that has been stored in its weights.\nThe questions in the StrategyQA dataset seem to often re-\nquire knowledge that is not present in LLMs. For this reason,\nLarge Language Model Programs\nFigure 3. Top-n accuracy of selecting the true evidence paragraph\nfrom our likelihood ranking of 100 paragraphs. The random guess-\ning baseline for top-1 is 1%. The ranking approach signi\ufb01cantly\noutperforms the in-context approach from Table 1.\nwe introduce an evidence-supported chain of thought where\nthe reasoning chain is generated over multiple steps and\neach step has a different support paragraph within its con-\ntext. This allows the model to use information from the\nparagraph to generate reasoning steps it would otherwise\nbe unable to produce. For StrategyQA most questions rely\non reasoning steps which draw from different knowledge\ndomains. In order to \ufb01nd the most valuable paragraph for\neach reasoning step, we perform a tree search where the\nroot node is the question and each level of the tree spans\nover the set of possible evidence paragraphs used as context\nto generate the next step. Every path through the tree is\na (possibly incomplete) reasoning chain. Searching over\nall possible reasoning chains is infeasible which is why we\nrank all reasoning chains and continually expand the highest-\nranked chain. We present pseudo-code in Algorithm 2 (see\nAppendix).\nWe explore two ranking strategies. The \ufb01rst is the average\nNLL of the chain so far (where the average NLL of each\nstep S is computed when conditioning on its respective\nparagraph). With this approach, the model will expand the\nreasoning chain which has been the most likely so far. This\napproach works reasonably well but can lead to issues. We\n\ufb01nd that if an LLM directly copies entire phrases from the\nevidence paragraph P, they will be given a very low NLL.\nThis can lead to repetitions or ongoing deductive steps which\nquickly end up in arguing about things that are irrelevant to\nthe question Q. Our second ranking strategy is an approach\nwhich tries to mitigate such issues by ranking the generated\nreasoning steps by their average NLL difference: with and\nwithout the paragraph (\u2206P ), and with and without the ques-\ntion (\u2206Q ). Those length-normalised differences, \u2206P and\n\u2206Q , allow us to select reasoning chains which leverage the\nprovided context (which reduces hallucinations) but remain\non-topic (which reduces divergent reasoning).\nnll(x) = \u2212 log(x)/len(x)\n(1)\n\u2206P = nll(p(S|Q, P)) \u2212 nll(p(S|Q))\n(2)\n\u2206Q = nll(p(S|Q, P)) \u2212 nll(p(S|P))\n(3)\nGenerated reasoning steps with a negative \u2206P rely more\nstrongly on the paragraph and are thus more likely to incor-\nporate information from it. Generated reasoning steps with\na negative \u2206Q rely more strongly on the question which\nleads to less divergence and favours steps which remain on\ntopic. We \ufb01nd that steps conditioned on paragraphs where\n\u2206P + \u2206Q is the lowest leads to better reasoning chains and\nimproves accuracy.\nTo evaluate the tree search in isolation, we assume that the\nevidence paragraphs contain information about all reasoning\nsteps necessary to answer the question. This is not the\ncase for all StrategyQA questions which is why we limit\nourselves to the 918 StrategyQA questions which are fully\nsupported.\nNote that this technique is a variation of beam search\n(Medress et al., 1977), i.e. a heuristic search algorithm\nwhich explores a graph by expanding the highest-ranking\nnode in a limited set. It is quite different from the usual\nuse of beam search for generating text with language mod-\nels where in order to generate the next token we select the\nk tokens with the highest probability given the same con-\ntext. Instead, we always generate the most likely tokens but\nsearch over the limited set of k paragraphs to be used as\ncontext.\nOur experimental results in Table 2 demonstrate that the\nmodel\u2019s reasoning ability has improved over the OPT chain\nof thought baseline. To better highlight the advantage of our\ntree search we include a baseline where we add the golden\nfacts to the context of OPT using a custom prompt. Golden\nfacts are statements provided by the authors of StrategyQA\nwhich should contain the factual knowledge necessary to\ndeduce the correct answer. With the golden facts in its\ncontext, OPT achieves about 81.2%. This can be seen as\na performance upper bound for the OPT model since it\nrepresents the setting where the model has all the necessary\nfacts clearly within its context.\n4. Further LLM Program Examples\nTraining an LLM is a costly and complex engineering chal-\nlenge. For this reason, training and research on such models\nis mostly done in well-funded industrial labs. However, in\nrecent years, GPT-3 (Brown et al., 2020) has become easily\naccessible to the public through its API (Brockman et al.,\n2018) and the weights of LLMs from the OPT (Zhang et al.,\n2022) and BLOOM (Scao et al., 2022) family have been\nmade publicly available, sparking a \ufb02urry of LLM research.\nLarge Language Model Programs\nTable 2. Binary classi\ufb01cation accuracy on the StrategyQA subset with fully supported evidence. CoT stands for chain of thought. Golden\nfacts are the facts necessary to answer the question according to the StrategyQA dataset.\nMETHOD\nACCURACY\nOPT-175B, FEW-SHOT, NO-COT\n50.33%\nOPT-175B, FEW-SHOT, WITH-COT\n60.11%\nOPT-175B, FEW-SHOT, WITH-TREE-SEARCH, NLL RANKING\n65.98%\nOPT-175B, FEW-SHOT, WITH-TREE-SEARCH, DELTA RANKING\n66.41%\nOPT-175B, FEW-SHOT, WITH-GOLDEN-FACTS\n81.27%\nOPT-175B, FEW-SHOT, WITH-GOLDEN-FACTS, WITH-COT\n81.12%\nSome recent and concurrent work implicitly follows the\npresented LLM Program approach. Such work often has the\ncharacteristic that it describes stages or an algorithm and\nparameterises different steps with different prompts. In this\nSection, we\u2019ll go through all recent and concurrent work\nthat uses this emerging methodology.\nCreswell & Shanahan (2022) decompose the inference of\nan answer into a recurrent algorithm which consists of a\nselection and inference step, as well as a halting criterion.\nEach step then consists of a \ufb01netuned LLM which only has\naccess to the information necessary in order to prevent the\nmodel from learning shortcuts, such as directly predicting\nthe answer from the question instead of performing a deduc-\ntive step. The \ufb01nal system\u2019s capability of \ufb01nding proofs is\nincreased by over 20% in comparison with baselines.\nYang et al. (2022c) present a modular approach to extract\nrules from natural language facts. To achieve this they use\nan LLM to generate many candidate rules based on input\nfacts and the desired rule template. In four further steps, the\nsame LLM with a different prompt is used to \ufb01lter out rules\nwhich do not meet the necessary requirements for induction\n(e.g. triviality). The authors present experimental results for\nthe entire system as well as each module in isolation which\nindicate the bene\ufb01ts of their 5-step program.\nIn a similar vein, various works propose the use of veri\ufb01ers\nto improve the quality of the generated samples through\nsome form of repeated veri\ufb01cation (Cobbe et al., 2021). For\nexample, Saparov & He (2023) leverages diverse prompts\nand veri\ufb01es each individual step in a recurrent fashion, re-\nsulting in signi\ufb01cant gains in accuracy without any \ufb01netun-\ning.\nKim et al. (2021) present a 3-step program to identify un-\nveri\ufb01able presuppositions in the question-answering setting.\nE.g., the question Which linguist invented the lightbulb?\ncontains the false presupposition that a linguist has invented\nthe lightbulb. The authors report that false presuppositions\ncan make up a substantial share of the unanswerable ques-\ntions in the popular Natural Questions (Kwiatkowski et al.,\n2019) dataset. Given a question, their program \ufb01rst gener-\nates presuppositions, veri\ufb01es them, and \ufb01nally, generates an\nexplanation. The authors test different strategies where they\nuse neural and rule-based models for different steps, testing\neach in isolation.\nCurrent approaches to reasoning with language models are\nforward chaining, i.e. they start of with certain facts and\nsearch in the space of rules until they \ufb01nd a goal statement.\nThis also includes our own method presented in Section\n3.1.2, but Kazemi et al. (2022) argue that backwards chain-\ning, i.e. decomposing the goal statement until the subgoals\ncan be proven from the facts, is heavily favoured in the\nclassic automated proof-\ufb01nding literature. For this reason,\nthey implement a backwards-chaining using four modules:\nfact check, rule selection, goal decomposition, and sign\nagreement. Each of the modules is implemented using a\npre-trained LLM with a custom prompt and the modules are\nsubroutines of the LAMBADA program. Thanks to the ability\nof the LLM, the modules remain relatively high-level which\nresults in a relatively simple program with two nested for-\nloops. The LAMBADA approach signi\ufb01cantly outperforms\nother methods such as chain of thought (Wei et al., 2022b)\nand selection-inference (Creswell & Shanahan, 2022).\nAnother related line of works \ufb01rst decomposes the original\nquestion into simpler subquestions which should be easier\nto answer. With the answers to the subquestions, a \ufb01nal\nmodule answers the original question. Patel et al. (2022)\nuse questions decomposed by humans which signi\ufb01cantly\nimproves performance. Zhou et al. (2023a) use an LLM to\nautomatically decompose questions. Their empirical results\nindicate that they signi\ufb01cantly outperform chain of thought\nprompting. However, different from previous approaches,\nthe authors perform a question decomposition within a sin-\ngle call to an LLM instead of implementing a recurrent\nalgorithm. As a result, their approaches may be more prone\nto mistakes on more complex examples which are out of\ndistribution since the output speci\ufb01cation of such a compact\nstep includes many more possibilities than in a more broken\ndown program like LAMBADA. For further decomposition\nexamples see e.g. Perez et al. (2020); Yang et al. (2022a).\nRegular LLMs are isolated systems that can only access the\ninformation that has been transferred into their weights. As\nwe argued in Section 2, such systems may lack the neces-\nLarge Language Model Programs\nsary information to perform certain tasks. Leveraging an\nadditional, possibly non-neural, system could be a viable al-\nternative to an increase in model scale. E.g., Lazaridou et al.\n(2023) generate a google search query whose result will be\nadded as context to the question-answering prompt. The im-\nprovements in factuality and accuracy indicate the bene\ufb01ts\nof \u201dinference-type\u201d interventions, i.e. embedding the model\nwithin a simple program which during inference allows the\nsystem to leverage results from a classic document retrieval\nsystem without the need for a dedicated retrieval-augmented\nmodel.\nInterestingly, an LLM is not always able to use the knowl-\nedge that it has stored in its weights. This is shown by Liu\net al. (2022b) who demonstrate a simple 2-step program\nwhich \ufb01rst generates a question-speci\ufb01c set of facts before\nanswering the question can result in better performance\n(for similar methods see Paranjape et al. (2021); Li et al.\n(2022)). Liu et al. (2022a) further improve such knowledge\nextraction using reinforcement learning based on increased\nquestion-answering performance. See also the work of Co-\nhen et al. (2023) on extracting a knowledge base from a\nlanguage model using a multi-step program.\nApart from question-answering, we also see the emergents\nof LLM programs for generative tasks. Yang et al. (2022b)\nrecursively prompt an LLM with the story plan and the\ncurrent story state to generate coherent stories of up to 2500\nwords. Similarly, Wu et al. (2021b) summarise entire books\nby recursively summarising previous summaries of \ufb01xed-\nsize chunks. Both are great examples of recursive programs\nwhich overcome the \ufb01nite-context limitation of the decoder-\nonly Transformer.\nIn another line of work, LLMs are embedded in an ongoing\nmodel-environment loop to plan and suggest actions to a\nrobot given high-level goals (Ahn et al., 2022; Huang et al.,\n2022). However, current approaches give all context to the\nLLM and do not have a mechanism to update an external\nmemory. As a result, the input and output speci\ufb01cation\nof the model is likely to exceed the current capabilities of\nLLMs for certain extreme examples.\nA rapidly growing body of work further generalises the\nnotion of the environment which the LLM interacts with.\nThese approaches combine the strength of an LLM with the\nstrength of another connectionist or classic system. Exam-\nples of such include the LLMs which repeatedly generate\nshort executable programs as intermediate reasoning steps\n(Gao et al., 2022), which generate chess commentaries with\nthe help of symbolic reasoning engines (Lee et al., 2022),\nor which use a variety of other tools such as a calculator\n(Karpas et al., 2022).\nIn the future, we will likely see more elaborate algorithms\nbeing developed which embed several neural and symbolic\nmodules to solve increasingly complex tasks. To support\nsuch projects, new libraries have been recently created.\nLangChain (Chase, 2023) is a library which supports devel-\nopers to combine LLMs with other sources of computation.\nGPT-Index (Liu, 2022) is a collection of data structures de-\nsigned to make it easier to use external knowledge bases\nwith LLMs. Reppert et al. (2023) present ICE, an open-\nsource tool for visualizing the execution traces of LM pro-\ngrams.\nRecent work also studied the bene\ufb01ts of LLM programs.\nWu et al. (2022b) study the user needs when authoring their\nown LLM programs and present PromptChainer to sup-\nport building prototypes for applications. Wu et al. (2022a)\npresent an interactive \u201dchain authoring system\u201d and study\nhow users modify them to improve performance but also\nuser satisfaction.\nGeneral purpose programs.\nAnother interesting line of\nresearch is going to centre around task-independent LLM\nprograms which, so far, have received much less attention.\nShuster et al. (2022a) present a system which augments\na conversational agent with a knowledge retrieval step be-\nfore responding to the user. Similarly, Shuster et al. (2022b)\npresent BlenderBot 3, a conversational bot based on an LLM\nbut which is augmented with a classic long-term storage\nsuch that it can remember information about the user across\nseveral sessions (which e.g. ChatGPT (Schulman et al.,\n2022) cannot). Another approach is presented by Dalvi et al.\n(2022), who use a simple program in form of a dialogue\ntree to learn facts from the user. Facts are stored in a dy-\nnamic memory which the LLM also updates. Crucially, the\nprogram includes an interaction loop with the user which\nallows the user to correct the model which then updates its\ndynamic memory with new or corrected facts. Hence, this\nLLM program may be considered an example of an LLM\nprogram which implements a crude learning algorithm. This\nopens up an exciting direction for future research which can\nbene\ufb01t from symbolic learning algorithms such as e.g. the\nOptimal Ordered Problem Solver or the G\u00a8odel Machine\n(Schmidhuber, 2004; 2007).\n5. Discussion and Related Work\nThe presented method of programming with pre-trained\nmodels stands in opposition to the common deep-learning\nphilosophy of training a single omniferous black-box model\nwhich only has to be scaled in terms of parameter count\nand training data. Although central to the current success of\nLLMs, such an approach comes with several drawbacks:\n\u2022 The computation of deep connectionist models, such\nas a large pre-trained Transformer language model, is\nnotoriously dif\ufb01cult to interpret (although there is on-\nLarge Language Model Programs\ngoing research, see e.g. Elhage et al. (2021)). Breaking\na problem down into multiple steps can not just im-\nprove performance but also increase the interpretability\nof its inference.\n\u2022 LLMs are trained on large amounts of text which can\ncontain toxic, biased, or otherwise undesired content.\nAs a result, an LLM may also output undesired text.\nThe language model itself does not provide any safety\nmechanisms to prevent such outputs. Embedding an\nLLM within a program is a simple and effective way\nto include a safety mechanism which e.g. \ufb01lters out\nunwanted LLM responses.\n\u2022 Tuning an LLM on expert trajectories of complex be-\nhaviour requires large amounts of high-quality and\nbehaviour-speci\ufb01c data which is dif\ufb01cult and expen-\nsive to acquire. Breaking the problem into subproblems\nmay allow the identi\ufb01cation of speci\ufb01c lower-level ca-\npabilities that are missing. Focusing the data collection\non such blindspots is potentially a faster and more ef-\n\ufb01cient approach and lower-level capabilities may be\nuseful for a wide range of problems.\n\u2022 It is dif\ufb01cult to give any guarantees for neural models\ndue to the lack of interpretability. Furthermore, it is\nwell-known that neural networks struggle to generalise\nout of distribution. However, embedding one or several\nconnectionist modules with a program allows us to give\nsome trivial generalisation guarantees that do not hold\notherwise. E.g., the \ufb01ltering and search aspect of our\nprogram, as presented in Section 2, generalises trivially\nto a larger number of paragraphs.\n\u2022 So far, all LLMs of 100B parameters or more are varia-\ntions of the Transformer architecture. They thus inherit\nits limitations, such as a \ufb01nite context of a few thou-\nsand tokens (Hutchins et al., 2022). Embedding an\nLLM within a task-independent program that is respon-\nsible to select and load relevant documents into context\nor which summarises past text may serve as a way of\novercoming such limitations.\nWe believe that the method of programming with LLMs\nas presented in this work can mitigate many drawbacks in\nsettings where the expected processing of a query is well\nunderstood. Many recent papers have demonstrated the\nbene\ufb01ts of such an approach (see the previous Section 4),\nbut signi\ufb01cantly fewer have taken a higher-level view. Here\nwe mention some recent or concurrent works which are\nrelated to our general perspective.\nConcurrent to our work, Khot et al. (2023) present Decom-\nposed Prompting, a framework for decomposing tasks into\nsubtasks which can be solved in isolation by LLMs using a\nsubtask-speci\ufb01c prompt. The authors demonstrate better out-\nof-distribution performance on simple string manipulation\nand question-answering tasks.\nIn earlier work, Dohan et al. (2022) have presented a related\nperspective which describes compositions of pre-trained\nmodels as probabilistic programs. In so-called language\nmodel cascades, LLMs are random variables of type string\nwithin a probabilistic program. With such a perspective\nthe authors present a unifying view of existing algorithms\nsuch as chain of thought (Wei et al., 2022b), scratchpads\n(Nye et al., 2022), or STaR (Zelikman et al., 2022), which\ncomplements our more algorithmic perspective.\nSimilarly, Creswell & Shanahan (2022) brie\ufb02y describe\ntheir approach as algorithmic prompting where the response\nof a language model given the \ufb01rst prompt is integrated\ninto future prompts. The authors argue that such prompt\nmanipulations and constructions can be composed into en-\ntire algorithms to achieve more sophisticated behaviour. In\nfollow-up work, Shanahan (2022) argues that such an ap-\nproach is necessary to build a trustworthy reasoning system.\nZeng et al. (2023) propose a modular framework with multi-\nple pre-trained models which are composed to capture new\nmultimodal capabilities without the need for end-to-end\ntraining or \ufb01netuning.\nA different but related approach describes a looped trans-\nformer model as programmable computers (Giannou et al.,\n2023). Through a simple loop they iteratively call a trans-\nformer model with a scratchpad, memory, and instructions,\nanalogous to computer programs.\nOur approach is inspired by the \u201clearning to think\u201d report\n(Schmidhuber, 2015) (Sec. 5.3) where a controller net-\nwork C learns to send sequences of activations into another\nnetwork M after which C reads M\u2019s activations in order\nto exploit its knowledge. Our program, however, is not a\ntrained neural network but an explicit algorithm.\n6. Conclusion\nWe have presented LLM programs, the emerging methodol-\nogy of embedding pre-trained connectionist models, such\nas large language models, in a classic program to carry out\nmore complex tasks. It is central to this method to decom-\npose the main problem recursively into subproblems until\nthey can be solved by a single query to the model. With\nmore \ufb01ne-grained input and output speci\ufb01cations for each\nsubproblem, the model\u2019s capabilities can be developed and\ntested in isolation. We describe an example of this method\nin the setting of evidence-supported question-answering and\ndemonstrate an improvement in performance without any\n\ufb01netuning. We also list the advantages and disadvantages\nand highlight recent works from this perspective.\nLarge Language Model Programs\nReferences\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O.,\nDavid, B., Finn, C., Gopalakrishnan, K., Hausman, K.,\nHerzog, A., et al. Do as i can, not as i say: Ground-\ning language in robotic affordances.\nIn 6th Annual\nConference on Robot Learning, 2022. URL https:\n//openreview.net/forum?id=bdHkMjBJG_w.\nAnil, C., Wu, Y., Andreassen, A. J., Lewkowycz, A., Misra,\nV., Ramasesh, V. V., Slone, A., Gur-Ari, G., Dyer, E.,\nand Neyshabur, B. Exploring length generalization in\nlarge language models.\nIn Oh, A. H., Agarwal, A.,\nBelgrave, D., and Cho, K. (eds.), Advances in Neural\nInformation Processing Systems, 2022. URL https:\n//openreview.net/forum?id=zSkYVeX7bC4.\nBrockman, G., Murati, M., and Welinder, P., Sept\n2018.\nURL\nhttps://openai.com/blog/\nopenai-api/.\nBrown, T. B. et al. Language models are few-shot learn-\ners. In Proc. Advances in Neural Information Processing\nSystems (NeurIPS), Virtual only, December 2020.\nChase, H.\nLangchain.\nhttps://github.com/\nhwchase17/langchain, 2023. [Accessed 09-Jan-\n2023].\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., et al. Training veri\ufb01ers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\nCohen, R., Geva, M., Berant, J., and Globerson, A. Crawling\nthe internal knowledge-base of language models. arXiv\npreprint arXiv:2301.12810, 2023.\nCreswell, A. and Shanahan, M. Faithful reasoning using\nlarge language models. arXiv preprint arXiv:2208.14271,\n2022.\nCsord\u00b4as, R., Irie, K., and Schmidhuber, J. The devil is in\nthe detail: Simple tricks improve systematic generaliza-\ntion of transformers. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing, pp. 619\u2013634, Online and Punta Cana, Domini-\ncan Republic, November 2021. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2021.emnlp-main.\n49.\nURL https://aclanthology.org/2021.\nemnlp-main.49.\nDalvi, B., Tafjord, O., and Clark, P. Towards teachable\nreasoning systems. arXiv preprint arXiv:2204.13074,\n2022.\nDeletang, G., Ruoss, A., Grau-Moya, J., Genewein, T.,\nWenliang, L. K., Catt, E., Cundy, C., Hutter, M., Legg,\nS., Veness, J., and Ortega, P. A.\nNeural networks\nand the chomsky hierarchy.\nIn International Confer-\nence on Learning Representations, 2023. URL https:\n//openreview.net/forum?id=WbxHAzkeQcn.\nDev, S., Li, T., Phillips, J. M., and Srikumar, V. On measur-\ning and mitigating biased inferences of word embeddings.\nIn Proceedings of the AAAI Conference on Arti\ufb01cial In-\ntelligence, volume 34, pp. 7659\u20137666, 2020.\nDohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D.,\nLopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A.,\nSohl-Dickstein, J., et al. Language model cascades. arXiv\npreprint arXiv:2207.10342, 2022.\nElhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph,\nN., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly,\nT., DasSarma, N., Drain, D., Ganguli, D., Hat\ufb01eld-\nDodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt,\nL., Ndousse, K., Amodei, D., Brown, T., Clark, J.,\nKaplan, J., McCandlish, S., and Olah, C.\nA math-\nematical framework for transformer circuits.\nTrans-\nformer Circuits Thread, 2021.\nhttps://transformer-\ncircuits.pub/2021/framework/index.html.\nFan, A., Lavril, T., Grave, E., Joulin, A., and Sukhbaatar, S.\nAddressing some limitations of transformers with feed-\nback memory, 2021. URL https://openreview.\nnet/forum?id=OCm0rwa1lx1.\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\nFoster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\net al. The pile: An 800gb dataset of diverse text for\nlanguage modeling. arXiv preprint arXiv:2101.00027,\n2020.\nGao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y.,\nCallan, J., and Neubig, G. Pal: Program-aided language\nmodels. arXiv preprint arXiv:2211.10435, 2022.\nGers, F. A. and Schmidhuber, E. Lstm recurrent networks\nlearn simple context-free and context-sensitive languages.\nIEEE transactions on neural networks, 12(6):1333\u20131340,\n2001.\nGeva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and\nBerant, J. Did aristotle use a laptop? a question answering\nbenchmark with implicit reasoning strategies. Transac-\ntions of the Association for Computational Linguistics, 9:\n346\u2013361, 2021.\nLarge Language Model Programs\nGiannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D.,\nand Papailiopoulos, D.\nLooped transformers as pro-\ngrammable computers. arXiv preprint arXiv:2301.13196,\n2023.\nGupta, P., Jiao, C., Yeh, Y.-T., Mehri, S., Eskenazi, M., and\nBigham, J. P. Improving zero and few-shot generalization\nin dialogue through instruction tuning. arXiv preprint\narXiv:2205.12673, 2022.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural computation, 9(8):1735\u20131780, 1997.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., de las Casas, D., Hendricks, L. A.,\nWelbl, J., Clark, A., Hennigan, T., Noland, E., Millican,\nK., van den Driessche, G., Damoc, B., Guy, A., Osindero,\nS., Simonyan, K., Elsen, E., Vinyals, O., Rae, J. W., and\nSifre, L. An empirical analysis of compute-optimal large\nlanguage model training. In Oh, A. H., Agarwal, A.,\nBelgrave, D., and Cho, K. (eds.), Advances in Neural\nInformation Processing Systems, 2022. URL https:\n//openreview.net/forum?id=iBBcRUlOAPR.\nHuang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence,\nP., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y.,\nSermanet, P., Jackson, T., Brown, N., Luu, L., Levine,\nS., Hausman, K., and brian ichter. Inner monologue:\nEmbodied reasoning through planning with language\nmodels. In 6th Annual Conference on Robot Learning,\n2022. URL https://openreview.net/forum?\nid=3R3Pz5i0tye.\nHutchins, D., Schlag, I., Wu, Y., Dyer, E., and Neyshabur,\nB. Block-recurrent transformers. In Oh, A. H., Agarwal,\nA., Belgrave, D., and Cho, K. (eds.), Advances in Neural\nInformation Processing Systems, 2022. URL https:\n//openreview.net/forum?id=uloenYmLCAo.\nKarpas, E., Abend, O., Belinkov, Y., Lenz, B., Lieber, O.,\nRatner, N., Shoham, Y., Bata, H., Levine, Y., Leyton-\nBrown, K., et al.\nMrkl systems: A modular, neuro-\nsymbolic architecture that combines large language mod-\nels, external knowledge sources and discrete reasoning.\narXiv preprint arXiv:2205.00445, 2022.\nKazemi, S. M., Kim, N., Bhatia, D., Xu, X., and Ra-\nmachandran, D. Lambada: Backward chaining for au-\ntomated reasoning in natural language. arXiv preprint\narXiv:2212.13894, 2022.\nKhot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson,\nK., Clark, P., and Sabharwal, A. Decomposed prompt-\ning: A modular approach for solving complex tasks. In\nInternational Conference on Learning Representations,\n2023. URL https://openreview.net/forum?\nid=_nGgzQjzaRy.\nKim, N., Pavlick, E., Ayan, B. K., and Ramachandran, D.\nWhich linguist invented the lightbulb? presupposition\nveri\ufb01cation for question-answering. In Annual Meeting\nof the Association for Computational Linguistics, 2021.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y.\nLarge language models are zero-shot reasoners. In ICML\n2022 Workshop on Knowledge Retrieval and Language\nModels, 2022. URL https://openreview.net/\nforum?id=6p3AuaHAFiN.\nKwiatkowski, T., Palomaki, J., Red\ufb01eld, O., Collins, M.,\nParikh, A., Alberti, C., Epstein, D., Polosukhin, I., De-\nvlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M.,\nChang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and\nPetrov, S. Natural questions: A benchmark for question\nanswering research. Transactions of the Association for\nComputational Linguistics, 7:452\u2013466, 2019. doi: 10.\n1162/tacl a 00276. URL https://aclanthology.\norg/Q19-1026.\nLazaridou, A., Gribovskaya, E., Stokowiec, W. J., and Grig-\norev, N. Internet-augmented language models through\nfew-shot prompting for open-domain question answering,\n2023. URL https://openreview.net/forum?\nid=hFCUPkSSRE.\nLee, A., Wu, D., Dinan, E., and Lewis, M.\nImprov-\ning chess commentaries by combining language mod-\nels with symbolic reasoning engines.\narXiv preprint\narXiv:2212.08195, 2022.\nLi, J., Zhang, Z., and Zhao, H.\nSelf-prompting large\nlanguage models for open-domain qa. arXiv preprint\narXiv:2212.08635, 2022.\nLiu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and\nZhang, C. Transformers learn shortcuts to automata. In\nInternational Conference on Learning Representations,\n2023. URL https://openreview.net/forum?\nid=De4FYqjFueZ.\nLiu, J. GPT Index, 11 2022. URL https://github.\ncom/jerryjliu/gpt_index.\nLiu, J., Hallinan, S., Lu, X., He, P., Welleck, S., Hajishirzi,\nH., and Choi, Y.\nRainier: Reinforced knowledge in-\ntrospector for commonsense question answering. arXiv\npreprint arXiv:2210.03078, 2022a.\nLiu, J., Liu, A., Lu, X., Welleck, S., West, P., Le Bras,\nR., Choi, Y., and Hajishirzi, H. Generated knowledge\nprompting for commonsense reasoning. In Proceedings\nof the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 3154\u2013\n3169, Dublin, Ireland, May 2022b. Association for Com-\nputational Linguistics. doi: 10.18653/v1/2022.acl-long.\nLarge Language Model Programs\n225. URL https://aclanthology.org/2022.\nacl-long.225.\nMedress, M. F., Cooper, F. S., Forgie, J. W., Green, C.,\nKlatt, D. H., O\u2019Malley, M. H., Neuburg, E. P., Newell, A.,\nReddy, D., Ritea, B., et al. Speech understanding systems:\nReport of a steering committee. Arti\ufb01cial Intelligence, 9\n(3):307\u2013316, 1977.\nNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,\nC., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al.\nWebgpt: Browser-assisted question-answering with hu-\nman feedback. arXiv preprint arXiv:2112.09332, 2021.\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H.,\nAustin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma,\nM., Luan, D., Sutton, C., and Odena, A. Show your work:\nScratchpads for intermediate computation with language\nmodels, 2022. URL https://openreview.net/\nforum?id=iedYJm92o0a.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Gray, A.,\nSchulman, J., Hilton, J., Kelton, F., Miller, L., Simens,\nM., Askell, A., Welinder, P., Christiano, P., Leike, J., and\nLowe, R. Training language models to follow instruc-\ntions with human feedback. In Oh, A. H., Agarwal, A.,\nBelgrave, D., and Cho, K. (eds.), Advances in Neural\nInformation Processing Systems, 2022. URL https:\n//openreview.net/forum?id=TG8KACxEON.\nParanjape, B., Michael, J., Ghazvininejad, M., Hajishirzi,\nH., and Zettlemoyer, L. Prompting contrastive expla-\nnations for commonsense reasoning tasks.\nIn Work-\nshop on Commonsense Reasoning and Knowledge Bases,\n2021. URL https://openreview.net/forum?\nid=KFcjxSNdMBq.\nPatel, P., Mishra, S., Parmar, M., and Baral, C. Is a ques-\ntion decomposition unit all we need?\narXiv preprint\narXiv:2205.12538, 2022.\nPerez, E., Lewis, P., Yih, W.-t., Cho, K., and Kiela, D. Un-\nsupervised question decomposition for question answer-\ning. In Proceedings of the 2020 Conference on Empiri-\ncal Methods in Natural Language Processing (EMNLP),\npp. 8864\u20138880, Online, November 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.\nemnlp-main.713.\nURL https://aclanthology.\norg/2020.emnlp-main.713.\nReppert, J., Rachbach, B., George, C., Byun, L. S. J., Ap-\npleton, M., and Stuhlm\u00a8uller, A. Iterated decomposition:\nImproving science q&a by supervising reasoning pro-\ncesses. arXiv preprint arXiv:2301.01751, 2023.\nSaparov, A. and He, H. Language models can (kind of)\nreason: A systematic formal analysis of chain-of-thought.\nIn International Conference on Learning Representations,\n2023. URL https://openreview.net/forum?\nid=qFVVBzXxR2V.\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u00b4c, S., Hesslow,\nD., Castagn\u00b4e, R., Luccioni, A. S., Yvon, F., Gall\u00b4e, M.,\net al. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100, 2022.\nSchmidhuber, J. Optimal ordered problem solver. Machine\nLearning, 54(3):211\u2013254, 2004.\nSchmidhuber, J. G\u00a8odel machines: Fully self-referential\noptimal universal self-improvers. In Arti\ufb01cial general\nintelligence, pp. 199\u2013226. Springer, 2007.\nSchmidhuber, J. On learning to think: Algorithmic infor-\nmation theory for novel combinations of reinforcement\nlearning controllers and recurrent neural world models.\narXiv preprint arXiv:1511.09249, 2015.\nSchulman, J., Zoph, B., Kim, C., Hilton, J., Menick, J.,\nWeng, J., Ceron Uribe, J. F., Fedus, L., Metz, L., Pokorny,\nM., Gontijo Lopes, R., Zhao, S., Vijayvergiya, A., Sigler,\nE., Perelman, A., Voss, C., Heaton, M., Parish, J., Cum-\nmings, D., Nayak, R., Balcom, V., Schnurr, D., Kaftan,\nT., Hallacy, C., Turley, N., Deutsch, N., Goel, V., Ward,\nJ., Konstantinidis, A., Zaremba, W., Ouyang, L., Bog-\ndonoff, L., Gross, J., Medina, D., Yoo, S., Lee, T., Lowe,\nR., Mossing, D., Huizinga, J., Jiang, R., Wainwright,\nC., Almeida, D., Lin, S., Zhang, M., Xiao, K., Slama,\nK., Bills, S., Gray, A., Leike, J., Pachocki, J., Tillet, P.,\nJain, S., Brockman, G., and Ryder, N., Nov 2022. URL\nhttps://openai.com/blog/chatgpt/.\nSchuurmans, D.\nMemory augmented large language\nmodels are computationally universal. arXiv preprint\narXiv:2301.04589, 2023.\nShanahan, M. Talking about large language models. arXiv\npreprint arXiv:2212.03551, 2022.\nSheng, E., Chang, K.-W., Natarajan, P., and Peng, N. So-\ncietal biases in language generation: Progress and chal-\nlenges. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pp. 4275\u20134293, Online,\nAugust 2021. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2021.acl-long.330. URL https:\n//aclanthology.org/2021.acl-long.330.\nShuster, K., Komeili, M., Adolphs, L., Roller, S., Szlam, A.,\nand Weston, J. Language models that seek for knowledge:\nModular search & generation for dialogue and prompt\ncompletion. arXiv preprint arXiv:2203.13224, 2022a.\nLarge Language Model Programs\nShuster, K., Xu, J., Komeili, M., Ju, D., Smith, E. M.,\nRoller, S., Ung, M., Chen, M., Arora, K., Lane, J., et al.\nBlenderbot 3: a deployed conversational agent that con-\ntinually learns to responsibly engage. arXiv preprint\narXiv:2208.03188, 2022b.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nGarriga-Alonso, A., et al. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of language\nmodels. arXiv preprint arXiv:2206.04615, 2022.\nTaylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn,\nA., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085, 2022.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-\nshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,\nDu, Y., et al. Lamda: Language models for dialog appli-\ncations. arXiv preprint arXiv:2201.08239, 2022.\nValmeekam, K., Olmo, A., Sreedharan, S., and Kambham-\npati, S. Large language models still can\u2019t plan (a bench-\nmark for LLMs on planning and reasoning about change).\nIn NeurIPS 2022 Foundation Models for Decision Mak-\ning Workshop, 2022. URL https://openreview.\nnet/forum?id=wUU-7XTL5XO.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention\nis all you need. In Proc. Advances in Neural Information\nProcessing Systems (NIPS), pp. 5998\u20136008, Long Beach,\nCA, USA, December 2017.\nWang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi,\nE. H., Narang, S., Chowdhery, A., and Zhou, D. Self-\nconsistency improves chain of thought reasoning in lan-\nguage models. In International Conference on Learning\nRepresentations, 2023. URL https://openreview.\nnet/forum?id=1PL1NIMMrw.\nWang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y.,\nMirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A. S.,\nArunkumar, A., Stap, D., Pathak, E., Karamanolakis,\nG., Lai, H., Purohit, I., Mondal, I., Anderson, J., Kuz-\nnia, K., Doshi, K., Pal, K. K., Patel, M., Moradshahi,\nM., Parmar, M., Purohit, M., Varshney, N., Kaza, P. R.,\nVerma, P., Puri, R. S., Karia, R., Doshi, S., Sampat, S. K.,\nMishra, S., Reddy A, S., Patro, S., Dixit, T., and Shen, X.\nSuper-NaturalInstructions: Generalization via declarative\ninstructions on 1600+ NLP tasks. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Lan-\nguage Processing, pp. 5085\u20135109, Abu Dhabi, United\nArab Emirates, December 2022. Association for Compu-\ntational Linguistics. URL https://aclanthology.\norg/2022.emnlp-main.340.\nWei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester,\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned language\nmodels are zero-shot learners. In International Confer-\nence on Learning Representations, 2022a. URL https:\n//openreview.net/forum?id=gEZrGCozdqR.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter,\nXia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain of\nthought prompting elicits reasoning in large language\nmodels. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho,\nK. (eds.), Advances in Neural Information Processing\nSystems, 2022b. URL https://openreview.net/\nforum?id=_VjQlMeSB_J.\nWu, J., Ouyang, L., Ziegler, D. M., Stiennon, N., Lowe, R.,\nLeike, J., and Christiano, P. F. Recursively summarizing\nbooks with human feedback. CoRR, abs/2109.10862,\n2021a.\nURL https://arxiv.org/abs/2109.\n10862.\nWu, J., Ouyang, L., Ziegler, D. M., Stiennon, N., Lowe, R.,\nLeike, J., and Christiano, P. F. Recursively summarizing\nbooks with human feedback. CoRR, abs/2109.10862,\n2021b.\nURL https://arxiv.org/abs/2109.\n10862.\nWu, T., Terry, M., and Cai, C. J. Ai chains: Transparent\nand controllable human-ai interaction by chaining large\nlanguage model prompts. In Proceedings of the 2022 CHI\nConference on Human Factors in Computing Systems, pp.\n1\u201322, 2022a.\nWu, T. S., Jiang, E., Donsbach, A., Gray, J., Molina, A.,\nTerry, M., and Cai, C. J. Promptchainer: Chaining large\nlanguage model prompts through visual programming.\nCHI Conference on Human Factors in Computing Sys-\ntems Extended Abstracts, 2022b.\nXu, J., Szlam, A. D., and Weston, J. Beyond gold\ufb01sh mem-\nory: Long-term open-domain conversation. In Annual\nMeeting of the Association for Computational Linguistics,\n2021.\nYang, J., Jiang, H., Yin, Q., Zhang, D., Yin, B., and\nYang, D. SEQZERO: Few-shot compositional seman-\ntic parsing with sequential prompts and zero-shot mod-\nels.\nIn Findings of the Association for Computa-\ntional Linguistics: NAACL 2022, pp. 49\u201360, Seattle,\nUnited States, July 2022a. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2022.\ufb01ndings-naacl.\n5.\nURL https://aclanthology.org/2022.\nfindings-naacl.5.\nYang, K., Peng, N., Tian, Y., and Klein, D. Re3: Generating\nlonger stories with recursive reprompting and revision.\narXiv preprint arXiv:2210.06774, 2022b.\nLarge Language Model Programs\nYang, Z., Dong, L., Du, X., Cheng, H., Cambria, E., Liu,\nX., Gao, J., and Wei, F. Language models as inductive\nreasoners. arXiv preprint arXiv:2212.10923, 2022c.\nYao, S., Chen, H., Yang, J., and Narasimhan, K. R. Web-\nshop: Towards scalable real-world web interaction with\ngrounded language agents. In Oh, A. H., Agarwal, A.,\nBelgrave, D., and Cho, K. (eds.), Advances in Neural\nInformation Processing Systems, 2022. URL https:\n//openreview.net/forum?id=R9KnuFlvnU.\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. STar:\nBootstrapping reasoning with reasoning. In Oh, A. H.,\nAgarwal, A., Belgrave, D., and Cho, K. (eds.), Ad-\nvances in Neural Information Processing Systems,\n2022. URL https://openreview.net/forum?\nid=_3ELRdg2sgI.\nZeng, A., Attarian, M., brian ichter, Choromanski, K. M.,\nWong, A., Welker, S., Tombari, F., Purohit, A., Ryoo,\nM. S., Sindhwani, V., Lee, J., Vanhoucke, V., and Flo-\nrence, P. Socratic models: Composing zero-shot multi-\nmodal reasoning with language. In International Confer-\nence on Learning Representations, 2023. URL https:\n//openreview.net/forum?id=G2Q2Mh3avow.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,\net al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\nZhou, D., Sch\u00a8arli, N., Hou, L., Wei, J., Scales, N., Wang,\nX., Schuurmans, D., Cui, C., Bousquet, O., Le, Q. V., and\nChi, E. H. Least-to-most prompting enables complex rea-\nsoning in large language models. In International Confer-\nence on Learning Representations, 2023a. URL https:\n//openreview.net/forum?id=WZH7099tgfM.\nZhou, H., Nova, A., Courville, A., Larochelle, H.,\nNeyshabur, B., and Sedghi, H. Teaching algorithmic\nreasoning via in-context learning, 2023b. URL https:\n//openreview.net/forum?id=6dlC7E1H_9.\nLarge Language Model Programs\nA. Algorithms\nA.1. Ranking Algorithm\nAlgorithm 1 Ranking paragraphs by the average negative\nlog-likelihood of the question and returning the top-n.\nInput: question q, paragraphs P, n\ninitialise: nlls = [\n]\nfor paragraph p in P do\navg nll = LLM(q, p)\nnlls.append((i, avg nll))\nend for\nreturn sorted(nlls)[:n]\nA.2. Tree Search Algorithm\nAlgorithm 2 Tree search over steps generated conditioned\non different evidence paragraphs ranked by a ranking criteria\nr.\nInput: question q, paragraphs P, criteria r\ninitialise: chains = [[q]], complete = [\n]\nwhile len(complete) <= 3 do\nc = pop(sort(chains, criteria =r))\nfor paragraph p in P do\nc\u2032 = add step(c, p)\nif gives answer(c\u2032) then\ncomplete.append(c\u2032)\nelse\nchains.append(c\u2032)\nend if\nend for\nend while\nreturn complete\n"
  },
  {
    "title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance",
    "link": "https://arxiv.org/pdf/2305.05176.pdf",
    "upvote": "1",
    "text": "FrugalGPT: How to Use Large Language Models\nWhile Reducing Cost and Improving Performance\nLingjiao Chen, Matei Zaharia, James Zou\nStanford University\nAbstract\nThere is a rapidly growing number of large language models (LLMs) that users can query for\na fee. We review the cost associated with querying popular LLM APIs\u2014e.g. GPT-4, ChatGPT,\nJ1-Jumbo\u2014and \ufb01nd that these models have heterogeneous pricing structures, with fees that can\ndi\ufb00er by two orders of magnitude. In particular, using LLMs on large collections of queries and\ntext can be expensive. Motivated by this, we outline and discuss three types of strategies that\nusers can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation,\n2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet\n\ufb02exible instantiation of LLM cascade which learns which combinations of LLMs to use for di\ufb00erent\nqueries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can\nmatch the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or\nimprove the accuracy over GPT-4 by 4% with the same cost. The ideas and \ufb01ndings presented\nhere lay a foundation for using LLMs sustainably and e\ufb03ciently.\n1\nIntroduction\nWe are in the midst of an explosion of large language models (LLMs). The alluring possibilities of\nusing LLMs for large-scale applications such as commerce, science, and \ufb01nance have led a growing\nnumber of companies (OpenAI, AI21, CoHere, etc.) to o\ufb00er LLMs as services.\nWhile LLMs such as GPT-4 achieves unprecedented performance in tasks such as question answering,\nusing them for high-throughput applications can be very expensive. For example, ChatGPT is estimated\nto cost over $700,000 per day to operate [Cosa], and using GPT-4 to support customer service can\ncost a small business over $21,000 a month [Cosb]. In addition to the \ufb01nancial cost, using the largest\nLLMs encures substantial environmental and energy impact [BGMMS21, WRG+22], a\ufb00ecting the social\nwelfare of current and future generations.\nThere are many LLMs now available via APIs and they charge heterogeneous prices. The cost of\nusing a LLM API typically consists of three components: 1) prompt cost (proportional to the length of\nthe prompt), 2) generation cost (proportional to the generation length), and 3) sometimes a \ufb01xed cost\nper query. We compared the cost associated with using 12 di\ufb00erent commercial LLMs from mainstream\nproviders including OpenAI, AI21, CoHere and Textsynth (Table 1). Their cost can di\ufb00er by up to 2\norders of magnitudes: for example, the prompt cost for 10M tokens is $30 for OpenAI\u2019s GPT-4 but\nonly $0.2 for GPT-J hosted by Textsyth.\nGiven the heterogeneous cost and quality, how to e\ufb00ectively and e\ufb03ciently leverage the full set of\nLLM options is a key challenge for pracitioners. If the tasks are relatively simple, then aggregating\nmultiple responses from GPT-J [WK21] (whose size is 30x smaller than GPT-3) o\ufb00ers performance\nsimilar to GPT-3 [ANC+22], leading to \ufb01nancial and environmental savings. However, the performance\nof GPT-J can be much worse on di\ufb03cult tasks [TLI+23]. Moreover, relying on one API provider\nis not reliable if that provider becomes unavailable, potentially due to spiking demand. Existing\nmodel ensemble paradigms such as model cascade [VJ04, WLM11] and FrugalML [CZZ20, CZZ22] were\ndesigned for predictive tasks with a known set of labels and do not account for the full capabilities of\nLLM. How to use LLMs a\ufb00ordably and accurately therefore calls for new approaches.\nOur contributions.\nIn this paper, we lay out our vision of a \ufb02exible framework that uses LLM APIs\nto process natural language queries within a budget constraint, termed FrugalGPT. As shown in Figure\n1\narXiv:2305.05176v1  [cs.LG]  9 May 2023\nQuery\nAnswer\n(a) Existing LLM Usage\n(c) Performance and cost tradeoffs\nLLM Approximation\nPrompt  \nAdaptation\nLLM  \nCascade\nLLM \nApproximation\nQuery\nBudget\nAnswer\n(b) Proposed FrugalGPT\nGPT-4\nChatGPT\nGPT-J\nZero-shot\nCoT\nFew-shot ...\n...\nGPT-Neo\nFSQ\nGPT-J\n   J1\nJ1-G\nJ1-L\n CoHere\nFQ\nGPT-3\nChatGPT\nGPT-C\nGPT-4\n0\n10\n20\n30\n40\n0.7\n0.72\n0.74\n0.76\n0.78\n0.8\n0.82\n0.84\n0.86\n0.88\nFrugalGPT\nCost ($)\nAccuracy\nFigure 1: Our vision for reducing LLM cost while improving accuracy. (a) The standard usage sends\nqueries to a single LLM (e.g. GPT-4), which can be expensive. (b) Our proposal is to use prompt\nadaption, LLM approximation and LLM cascade to reduce the inference cost. By optimizing over the\nselection of di\ufb00erent LLM APIs (e.g., GPT-J, ChatGPT, and GPT-4) as well as prompting strategies\n(such as zero-shot [BMR+20], few-shot [LSZ+21], and chain-of-thought(CoT) [WWS+22]), we can\nachieve substantial e\ufb03ciency gains. (c) On HEADLINES (a \ufb01nancial news dataset), FrugalGPT can\nreduce the inference cost by 98% while exceeding the performance of the best individual LLM (GPT-4).\n1, we discuss three main strategies for cost reduction: prompt adaptation, LLM approximation, and\nLLM cascade. The prompt adaptation explores how to identify e\ufb00ective (often shorter) prompts to save\ncost. LLM approximation aims to create simpler and cheaper LLMs to match a powerful yet expensive\nLLM on speci\ufb01c tasks. LLM cascade focuses on how to adaptively choose which LLM APIs to use for\ndi\ufb00erent queries.\nTo illustrate the potential of these ideas, we implement and evaluate a simple version of FrugalGPT\nusing LLM cascade. On each dataset and task, FrugalGPT learns how to adaptively triage di\ufb00erent\nqueries in the dataset to di\ufb00erent combinations of LLMs, including ChatGPT [Cha], GPT-3 [BMR+20]\nand GPT-4 [Ope23]. Our experiments show that FrugalGPT can save up to 98% of the inference cost\nof the best individual LLM API while matching its performance on the downstream task. On the other\nhand, FrugalGPT can improve the performance by up to 4% with the same cost. We believe this is only\nthe tip of the iceberg and we hope FrugalGPT opens a new window toward reducing LLMs\u2019 inference\ncost and improving its performances.\nRelated Works.\nPrompt Engineering.\nPrompt engineering has emerged as a discipline for\ncrafting prompts to enhance LLMs\u2019 performance across various applications. Recent developments\ninclude few-shot [BMR+20], chain-of-thought [WWS+22], knowledge enhancement [LLL+21, KSL+22],\nand numerous other prompting techniques [MDL+23, KTF+22, ZSH+22, DGSG22]. Existing prompt\nengineering approaches often aim to provide more detailed task explanations and in-context examples,\nresulting in longer and more expensive prompts. In contrast, this paper explores the use of concise\nprompts to reduce costs.\nModel Ensemble. Model ensembles, which involve combining multiple ML models for prediction,\nhave gained popularity in supervised learning [VJ04, Fri02], unsupervised learning [YLLL14], semi-\nsupervised learning [GDMR22], and weakly supervised learning [DSP+17]. Model ensembles typically\nrequire white-box access to multiple models for training purposes, but LLM APIs are often black-box.\nMoreover, model ensembles necessitate querying all models for a single query, thereby increasing costs.\nSystem Optimization for LLMs. Numerous e\ufb00orts have aimed to accelerate the training and\ninference time of modern deep learning models through system optimization [HMD15, CHSV17, Cas19,\nJZA19, RRWN11]. Recent work focuses on post-training quantization [BHS+22, YLW+23, XLS+22],\ntraining pipeline parallelism [LZG+21], and hardware-aware pruning [KFA23] tailored for LLMs. System\noptimization requires modi\ufb01cations to LLMs\u2019 internal states (e.g., model weights), but many commercial\n2\nLLM APIs do not release their models. Additionally, the rapidly increasing size of LLMs renders\nretraining highly expensive.\nML-as-a-Service. LLM APIs constitute a crucial component of the rapidly expanding machine-\nlearning-as-a-service (MLaaS) industry. Recent studies have demonstrated the diversity of di\ufb00erent ML\nAPIs\u2019 predictions [BG18, KNL+20, CCZZ21] and proposed strategies for leveraging various classi\ufb01cation\nML APIs to improve performance [CZZ20, CZZ22]. The outputs of LLM APIs encompass the entire\nnatural language space, but existing work requires a \ufb01xed (and known) label set. Moreover, both\nprompt choices and LLM API selections signi\ufb01cantly impact generative tasks\u2019 performance, resulting in\na considerably larger optimization space than standard classi\ufb01cation.\nThe remaining part of the paper is organized as follows. We start by o\ufb00ering more context and the\nproblem statement in Section 2. Next in Section 3, we present our visions on how to use LLM APIs\na\ufb00ordability and accurately. Section 4 shows the empirical bene\ufb01ts of FrugalGPT using real-world LLM\nAPIs (including GPT-3, ChatGPT, and GPT-4). Finally, we discuss future prospects in Section 5.\n2\nScope and Problem Statement\nNatural language query answering.\nIn this paper, we concentrate on the standard natural\nlanguage query answering task, where the objective is to answer a query q sampled from a natural\nlanguage query distribution Q. Various real-world natural language tasks, such as news classi\ufb01cation,\nreading comprehension, and commonsense reasoning, can be formulated as query-answering problems.\nLLM marketplace.\nWe consider answering queries via the LLM market, which comprises K di\ufb00erent\nLLM APIs, denoted by {fi(\u00b7)}K\ni=1. Each fi(\u00b7) : P 7\u2192 A is a function that, given a prompt p from\nthe prompt space P, generates an answer from the answer distribution A. Note that to use LLM\nAPIs, one has to convert each query q to some corresponding prompt \ufb01rst. LLM APIs are associated\nwith their own cost, typically consisting of three components: a portion proportional to the length\nof the prompt, a portion proportional to the length of the generated answer, and (sometimes) a\n\ufb01xed cost per query. Formally, given a prompt p, the cost of using the ith LLM API is denoted by\nci(p) \u225c \u02dcci,2\u2225fi(p)\u2225 + \u02dcci,1\u2225p\u2225 + \u02dcci,0, where \u02dcci,j, j = 0, 1, 2 are constants.\nAn illustrative example.\nAdapting the case study provided by [Cosa], assume a small business\noperates a customer service using GPT-4. The company caters to 15,000 customers each month, with\neach customer asking three questions twice a week, totaling 360,000 queries per month. Suppose for\neach question, its corresponding prompt averages 1800 tokens, and the answer is around 80 tokens.\nConsidering that the input and response costs of GPT-4 are $0.03 and $0.06 per thousand tokens,\nrespectively, the total monthly cost amounts to 360 \u00d7 ($0.03 \u00d7 1800 + $0.06 \u00d7 80) \u2248 $21.2K. Such a\nhigh cost is prohibitive for many small businesses.\nProblem statement: budget-aware LLM API usage.\nOur primary goal in this paper is lever-\naging LLM APIs within a budget constraint. Formally, this can be formulated as maximizing the overall\ntask performance E(q,a)\u2208Q\u00d7A[r(a, \u02c6a(s, q))], while ensuring the average cost is bounded by a user-de\ufb01ned\nvalue b, i.e., E(q,a)\u2208Q\u00d7A[c(s, q)] \u2264 b. Here, a denotes the correct answer to the query q, \u02c6a(s, q) is the\ngenerated answer by some strategy s for query q, and c(s, q) is the associated cost for processing query\nq using strategy s. The reward function r(\u00b7, \u00b7) measures how closely the generated answer aligns with\nthe correct one. It is crucial to note that the search space for the strategy is vast, encompassing factors\nsuch as which prompts to use, which LLM APIs to employ, and how to aggregate their responses.\n3\nHow to Use LLMs A\ufb00ordably and Accurately\nNow we present our vision on how to use LLM APIs within a budget. As shown in Figure 1 (b), we\ndiscuss three cost-reduction strategies, namely, prompt adaptation, LLM approximation, and LLM\ncascade.\n3\nFine-Tuner\nQuery Concatenator\nPrompt Selector\nQ1: What is the result of \nand \n at high temperatures? \nQ2: What helps prey hide?\nQ: What helps prey hide?\nLLM Chain\ncamouflage\nCache\n Q': \n and \n at high temp leads to?\nA': nitrogen oxides\nWhat helps prey hide?\ncamouflage\nWhat is from echolocation? \nsonar\n Q: What is the result of \nand \n at high temperatures?\n(a) Prompt selection\n(b) Query concatenation\n(e) LLM cascade\n(c) Completion cache\n(d) Model fine-tuning\nscore < 0.5\nGPT-J\nGPT-4\nGPT-3\ncamouflage\ncamouflage\ncamouflage\nGPT-4\nGPT-4\nGPT-4\nGPT-4\nPrompt: Q1+A1,Q2+A2,Q3+A3,Q4+A4\n Q: What is the result of \nand \n at high temperatures?\n Q: What is the result of \nand \n at high temperatures?\nPrompt: Q2+A2,Q4+A4\n Q: What is the result of \n and \nat high temperatures?\nPrompt: Q1+A1,Q2+A2,Q3+A3,Q4+A4\n Q: What helps prey hide?\nPrompt: Q1+A1,Q2+A2,Q3+A3,Q4+A4\nPrompt: Q1+Q2,A1+A2,Q3+Q4,A3+A4\naccept answer\nscore < 0.9\naccept answer\naccept answer\n Q: What is the result of \nand \n at high temperatures?\no/w\nGPT-J\ncamouflage\nFigure 2:\nIllustrations of cost-saving strategies. (a) Prompt selection uses a subset of in-context\nexamples as the prompt to reduce the size of the prompt. (b) Query concatenation aggregates multiple\nqueries to share prompts. (c) Completion cache stores and reuses an LLM API\u2019s response when a\nsimilar query is asked. (d) Model \ufb01ne-tuning uses expensive LLMs\u2019 responses to \ufb01ne-tune cheap LLMs.\n(e) LLM cascade employs di\ufb00erent LLM APIs for di\ufb00erent queries.\n4\nStrategy 1: Prompt adaptation.\nThe cost of an LLM query increases linearly with the size of the\nprompt. Consequently, a logical approach to reduce the cost of using LLM APIs involves decreasing the\nprompt\u2019s size, a process we refer to as prompt adaptation. Prompt selection (as illustrated in Figure 2\n(a)) is a natural example of prompt adaptation: rather than employing a prompt containing numerous\nexamples that demonstrate how to perform a task, one can retain a small subset of examples in the\nprompt. This results in a smaller prompt and subsequently lower cost. An intriguing challenge of prompt\nselection lies in determining which examples to maintain for various queries without compromising task\nperformance.\nAn additional instantiation is query concatenation (Figure 2 (b)). It is important to note that\nprocessing queries individually necessitates sending the same prompt to an LLM API multiple times.\nTherefore, the fundamental concept of query concatenation involves sending the prompt only once\nto the LLM API while allowing it to address multiple queries, thereby preventing redundant prompt\nprocessing. To accomplish this, several queries must be concatenated into a single query, and the\nprompt must explicitly request the LLM API to process multiple queries. For instance, to handle two\nqueries using one prompt, the examples presented in the prompt can include both queries followed by\ntheir corresponding answers.\nStrategy 2: LLM approximation.\nThe concept of LLM approximation is quite simple: if an LLM\nAPI is too costly to utilize, one can approximate it using more a\ufb00ordable models or infrastructures.\nOne example is the completion cache: as depicted in Figure 2 (c), the fundamental idea involves storing\nthe response locally in a cache (e.g., a database) when submitting a query to an LLM API. To process\na new query, we \ufb01rst verify if a similar query has been previously answered. If so, the response is\nretrieved from the cache. An LLM API is invoked only if no similar query is discovered in the cache.\nThe completion cache provides substantial cost savings when similar queries are frequently posed. For\ninstance, consider a search engine powered by an LLM API. If numerous users search for the same or\nsimilar keywords simultaneously, the completion cache facilitates answering all their queries by invoking\nthe LLM only once.\nAnother example of LLM approximation is model \ufb01ne-tuning. As shown in Figure 2(d), this process\nconsists of three steps: \ufb01rst, collect a powerful but expensive LLM API\u2019s responses to a few queries;\nsecond, use the responses to \ufb01ne-tune a smaller and more a\ufb00ordable AI model; and \ufb01nally, employ\nthe \ufb01ne-tuned model for new queries. In addition to cost savings, the \ufb01ne-tuned model often does not\nrequire lengthy prompts, thus providing latency improvements as a byproduct.\nStrategy 3: LLM cascade.\nThe increasing availability of LLM APIs with heterogeneous performance\nand costs presents a unique opportunity for data-adaptive LLM selection. Di\ufb00erent LLM APIs have their\nown strengths and weaknesses for various queries. Consequently, appropriately selecting which LLMs\nto use can provide both cost reduction and performance improvements. LLM cascade, as illustrated\nin Figure 2 (e), is one such example. LLM cascade sends a query to a list of LLM APIs sequentially.\nIf one LLM API\u2019s response is reliable, then its response is returned, and no further LLMs in the list\nare needed. The remaining LLM APIs are queried only if the previous APIs\u2019 generations are deemed\ninsu\ufb03ciently reliable. Query cost is signi\ufb01cantly reduced if the \ufb01rst few APIs are relatively inexpensive\nand produce reliable generations.\nThe key components of LLM cascade consist of two elements: (i) a generation scoring function\nand (ii) an LLM router. The generation scoring function, denoted by g(\u00b7, \u00b7) : Q \u00d7 A 7\u2192 [0, 1], generates\na reliability score given a query and an answer produced by an LLM API. The LLM router selects\nm LLM APIs to include in the list. Let LLL \u2208 [K]m denote the indexes of the m APIs selected by the\nrouter. Given a new query, it iteratively invokes the ith API in the list to obtain an answer fLi(q).\nThen, it uses the scoring function to generate a score g(q, fLi(q)). It returns the generation if the score\nis higher than a threshold \u03c4\u03c4\u03c4 i, and queries the next service otherwise.\nThe scoring function can be obtained by training a simple regression model that learns whether\na generation is correct from the query and a generated answer. Learning the selected list LLL and the\n5\nthreshold vectors \u03c4\u03c4\u03c4 can be modeled as a constraint optimization problem:\nmax\nLLL,\u03c4\u03c4\u03c4\nE [r(a, fLz(q))]\ns.t. E\n\" z\nX\ni=1\n\u02dccLi,2\u2225fLi(q)\u2225 + \u02dccLi,1\u2225q\u2225 + \u02dccLi,0\n#\n\u2264 b,\nz = arg min\ni\ng(q, fLi(q)) \u2265 \u03c4\u03c4\u03c4 i\nHere, z denotes the LLM API at which the router stops and returns the answer, the \ufb01rst constraint\nensures the average cost is bounded by the budget, and the objective measures the quality of the\ngeneration fLz(q) for a query q compared to the true answer a. This problem is inherently a mixed-\ninteger optimization and thus computationally expensive to solve. To address this issue, we develop a\nspecialized optimizer that (i) prunes the search space of LLL by ignoring any list of LLMs with small\nanswer disagreement, and (ii) approximates the objective by interpolating it within a few samples. This\nresults in an e\ufb03cient implementation with satisfactory performance, as shown later in Figure 5.\nCompositions.\nCombining approaches within and across di\ufb00erent strategies can lead to further\ncost reduction and performance enhancement. For instance, joint prompt and LLM selection is a\ncomposition of prompt selection and LLM cascade: for a given query, it searches for the smallest prompt\nand most a\ufb00ordable LLM that achieves satisfactory task performance. Another example is to search\nacross both existing LLM APIs and \ufb01ne-tuned models. It is important to note that the composition of\ndi\ufb00erent approaches also increases the computational costs for training. Consequently, this paves the\nway for investigating trade-o\ufb00s between query costs, task performance, and computational costs.\n4\nLLM Cascade Reduces Cost and Improves Accuracy\nIn this section, we present an empirical study on the FrugalGPT LLM cascade. Our goals are three-fold:\n(i) understand what a simple instantiation of LLM cascade learns, (ii) quantify the cost savings attained\nby FrugalGPT while matching the best individual LLM API\u2019s performance, and (iii) measure the\ntrade-o\ufb00s between performance and cost enabled by FrugalGPT.\nSetups: LLM APIs, Tasks, Datasets, and FrugalGPT instances.\nWe have selected 12 LLM\nAPIs from 5 mainstream providers, namely, OpenAI [Ope], AI21 [AI2], CoHere [CoH], Textsynth [Tex],\nand ForeFrontAI [FFA]. The details are summarized in Table 1. FrugalGPT has been developed\non top of these APIs and evaluated on a range of datasets belonging to di\ufb00erent tasks, including\nHEADLINES [SK21], OVERRULING [ZGA+21], and COQA [RCM19]. The summary of these datasets\nis presented in Table 2. HEADLINES is a \ufb01nancial news dataset whose goal is to determine the gold\nprice trend (up, down, neutral, or none) by reading \ufb01nancial news titles. This is especially useful for\n\ufb01ltering relevant news in \ufb01nancial markets. OVERRULING is a legal document dataset where the goal\nis to determine whether a given sentence is an overruling, i.e., rejecting previous legal cases. COQA is\na reading comprehension dataset developed in a conversational setting, which we have adapted as a\ndirect query answering task. We focus on the LLM cascade approach with a cascade length of 3, as\nthis simpli\ufb01es the optimization space and already demonstrates good results. Each dataset is randomly\nsplit into a training set to learn the LLM cascade and a test set for evaluation.\nA Case Study.\nLet us begin with a case study on the HEADLINES dataset. We set the budget to\nbe $6.5, which is one-\ufb01fth of GPT-4\u2019s cost. We employ a DistilBERT [SDCW19] tailored to regression\nas the scoring function. It is important to note that DistilBERT is considerably smaller and therefore\nless expensive than all LLMs considered here. As depicted in Figure 3 (a), the learned FrugalGPT\nsequentially calls GPT-J, J1-L, and GPT-4. For any given query, it \ufb01rst extracts an answer from\nGPT-J. If the score of this answer is greater than 0.96, the answer is accepted as the \ufb01nal response.\nOtherwise, J1-L is queried. J1-L\u2019s answer is accepted as the \ufb01nal response if its score is greater than\n0.37; otherwise, GPT-4 is invoked to obtain the \ufb01nal answer. Interestingly, this approach outperforms\nGPT-4 for numerous queries. For instance, given a headline \u201dGold o\ufb00 the lows after dismal U.S. GDP\ndata\u201d from NASDAQ, FrugalGPT accurately predicts that the price is going down, while GPT-4\n6\nTable 1: Summary of commercial LLM APIs. We use 12 LLM APIs from 5 providers. The cost was\nretrieved in March 2023. The cost can have three additive components: input (proportional to the\nnumber of input tokens), output (proportional to the number of generated tokens) and a \ufb01xed cost per\nrequest. The LLMs\u2019s costs can di\ufb00er by up to 2 orders of magnitudes. For example, to process 10M\ninput tokens, GPT-J from Textsynth costs only $0.2, but OpenAI\u2019s GPT-4 needs $30.\nProvider\nAPI\nSize/B\nCost (USD)\n10M input tokens\n10M output tokens\nrequest\nOpenAI\nGPT-Curie\n6.7\n2\n2\n0\nChatGPT\nNA\n2\n2\n0\nGPT-3\n175\n20\n20\n0\nGPT-4\nNA\n30\n60\n0\nAI21\nJ1-Large\n7.5\n0\n30\n0.0003\nJ1-Grande\n17\n0\n80\n0.0008\nJ1-Jumbo\n178\n0\n250\n0.005\nCohere\nXlarge\n52\n10\n10\n0\nForeFrontAI\nQA\n16\n5.8\n5.8\n0\nTextsynth\nGPT-J\n6\n0.2\n5\n0\nFAIRSEQ\n13\n0.6\n15\n0\nGPT-Neox\n20\n1.4\n35\n0\nTable 2: Summary of datasets used in the FrugalGPT LLM cascade experiments.\nDataset\nDomain\nSize\n#Examples in the prompt\nHEADLINES\nFinance\n10000\n8\nOVERRULING\nLaw\n2400\n5\nCOQA\nPassage Reading\n7982\n2\nGPT-J\nGPT-4\nJ1-L\nscore<0.96?\nscore<0.37?\nNo\nNo\nYes\nYes\nFinancial News\n(a) Learned FrugalGPT strategy  \nGPT-4\nFrugalGPT\nprice down\nprice up\n(b) A query and response example \nAssets\nApproch\nAccuracy\nCost ($)\nGPT-4\n0.857\n33.1\nFrugalGPT\n0.872\n6.5\n(c) Overall performance and cost \nFigure 3: A case study of FrugalGPT on the HEADLINES dataset. (a) The cascade strategy that\nFrugalGPT learned on this dataset with overall budget $6.5, one \ufb01fth of GPT-4\u2019s cost. FrugalGPT\navoids querying GPT-4 as long as GPT-J and J1-L produce high-quality answers. (b) Sometimes\nGPT-4 makes a mistake, but FrugalGPT learns to use the correct answers by J-1 and GPT-J. (c)\nOverall, we observe that FrugalGPT reduces the cost by 80%, while improves the accuracy by 1.5%\ncompared to GPT-4.\n7\nGPT-C\nChatGPT\nGPT-3\nGPT-4\nJ1-L\nJ1-G\nJ1\nCoHere\nFA-Q\nGPT-J\nFSQ\nGPT-Neo\nGPT-C\nChatGPT\nGPT-3\nGPT-4\nJ1-L\nJ1-G\nJ1\nCoHere\nFA-Q\nGPT-J\nFSQ\nGPT-Neo\n0 18 17 21 9 14 17 14 13 12 16 16\n7\n0\n6\n7\n8\n7\n8\n7\n6\n9\n7\n8\n4\n4\n0\n6\n4\n5\n5\n4\n5\n5\n7\n6\n6\n2\n4\n0\n6\n5\n5\n5\n4\n6\n4\n5\n4 14 12 17 0\n9 12 8 10 7 13 12\n6\n9\n9 12 6\n0\n8\n6\n7\n7\n8\n9\n4\n5\n6\n7\n4\n4\n0\n4\n5\n5\n6\n6\n5 10 9 12 4\n6\n8\n0\n7\n5 10 9\n11 15 16 18 13 13 16 14 0 14 12 15\n4 12 10 14 4\n8 10 6\n8\n0 12 10\n15 16 18 18 16 15 17 17 12 18 0 17\n4\n7\n7\n9\n5\n5\n7\n6\n5\n6\n7\n0\n0\n2\n5\n8\n10\n12\n15\n18\n20\n(a) HEADLINES\nGPT-C\nChatGPT\nGPT-3\nGPT-4\nJ1-L\nJ1-G\nJ1\nCoHere\nFA-Q\nGPT-J\nFSQ\nGPT-Neo\nGPT-C\nChatGPT\nGPT-3\nGPT-4\nJ1-L\nJ1-G\nJ1\nCoHere\nFA-Q\nGPT-J\nFSQ\nGPT-Neo\n0 23 25 25 12 8 14 22 11 8\n8 11\n4\n0\n5\n5\n3\n5\n6\n2\n5\n8\n6\n4\n2\n1\n0\n2\n1\n2\n3\n0\n3\n5\n4\n2\n2\n1\n2\n0\n1\n2\n3\n1\n2\n5\n3\n2\n13 23 25 26 0 13 16 21 15 14 14 14\n11 28 29 29 15 0 13 28 12 4\n6 13\n10 21 22 22 11 6\n0 22 9\n2\n5\n9\n6\n6\n8\n9\n4\n9 10 0\n9 12 10 6\n11 25 26 26 15 9 13 25 0\n5\n8 12\n26 45 46 46 31 19 24 46 23 0 14 28\n16 33 35 34 21 11 17 33 15 4\n0 18\n11 24 26 26 13 10 14 22 12 10 10 0\n0\n10\n20\n30\n40\n(b) OVERRULING\nGPT-C\nChatGPT\nGPT-3\nGPT-4\nJ1-L\nJ1-G\nJ1\nCoHere\nFA-Q\nGPT-J\nFSQ\nGPT-Neo\nGPT-C\nChatGPT\nGPT-3\nGPT-4\nJ1-L\nJ1-G\nJ1\nCoHere\nFA-Q\nGPT-J\nFSQ\nGPT-Neo\n0\n7 18 18 13 15 16 13 14 12 13 10\n19 0 27 22 23 24 26 20 23 19 23 20\n6\n3\n0\n9\n8\n8\n9\n7\n8\n6\n8\n6\n9\n1 13 0 12 12 14 9 13 11 12 10\n8\n6 16 16 0 10 11 9\n9\n7\n9\n8\n7\n5 13 13 6\n0\n9\n7\n8\n7\n8\n7\n6\n5 12 13 6\n7\n0\n7\n7\n5\n7\n6\n10 4 16 14 10 12 13 0 11 9 11 9\n9\n6 16 17 8 11 12 10 0\n6 10 8\n12 7 19 19 11 14 15 12 10 0 12 10\n9\n7 16 16 9 11 12 10 10 8\n0\n8\n9\n6 17 17 11 13 14 10 11 9 11 0\n0\n5\n10\n15\n20\n25\n(c) COQA\nFigure 4: Maximum performance improvement (MPI) of each pair of LLMs. (a), (b), and (c) correspond\nto the three datasets, separately. One entry indicates the percent of cases that the LLM on its row is\nwrong but the LLM on its column gives the right answer. Overall, we observe that cheap LLMs can\nbe complementary to the expensive ones quite often. For example, for about 6% of the data, GPT-4\nmakes a mistake but GPJ-J (or J-L or GPT-C) gives the right answer on HEADLINES.\nprovides an incorrect answer (as shown in Figure 3(b)). Overall, FrugalGPT results in both accuracy\ngains and cost reduction. As illustrated in Figure 3(c), its cost is reduced by 80%, while the accuracy\nis even 1.5% higher.\nLLM diversity.\nWhy can multiple LLM APIs potentially produce better performance than the\nbest individual LLM? In essence, this is due to generation diversity: even an inexpensive LLM can\nsometimes correctly answer queries on which a more expensive LLM fails. To measure this diversity,\nwe use the maximum performance improvement, or MPI. The MPI of LLM A with respect to LLM B\nis the probability that LLM A generates the correct answer while LLM B provides incorrect ones. This\nmetric essentially measures the maximum performance gains achievable by invoking LLM A in addition\nto LLM B.\nMPI between each pair of LLM APIs for all datasets is displayed in Figure 4. Overall, we observe\nsigni\ufb01cant potential within the LLM marketplace. For instance, GPT-C, GPT-J, and J1-L can all\nenhance GPT-4\u2019s performance by up to 6% on the HEADLINES dataset. On the COQA dataset, there\nare 13% of data points where GPT-4 makes an error, but GPT-3 provides the correct answer. Although\nthese improvement upper bounds may not always be attainable, they do demonstrate the possibility of\nutilizing more a\ufb00ordable services to achieve better performance.\nTable 3: Cost savings by FrugalGPT to match the best individual LLM\u2019s performance.\nDataset\nBest invidual LLM\nCost to reach the same accuracy\nCost Savings\nBest individual LLM\nFrugalGPT\nHEADLINES\nGPT-4\n33.1\n0.6\n98.3%\nOVERULLING\nGPT-4\n9.7\n2.6\n73.3%\nCOQA\nGPT-3\n72.5\n29.6\n59.2%\nCost Savings.\nSubsequently, we examine whether FrugalGPT can reduce costs while maintaining\naccuracy and, if so, by how much. Table 3 displays the overall cost savings of FrugalGPT, which range\nfrom 50% to 98%. This is feasible because FrugalGPT identi\ufb01es the queries that can be accurately\nanswered by smaller LLMs and, as a result, only invokes those cost-e\ufb00ective LLMs. Powerful but\nexpensive LLMs, such as GPT-4, are utilized only for challenging queries detected by FrugalGPT.\nPerformance and Cost Trade-o\ufb00s.\nNow, we investigate the trade-o\ufb00s between performance and\ncost achieved by FrugalGPT, as illustrated in Figure 5. Several interesting observations can be made.\n8\nFrugalGPT\nKinross Gold\nupgraded to\noutperformer from\nneutral at CIBC\nFrugalGPT\nGPT-J 0.17 < 0.96\nGPT-4\nJ1-L\nUp\nUp\nNone\n0.13 < 0.37\nNone\nGPT-4\nNone\nGold holds ground\nat 1-month low on\nhawkish Fed\ncomments\n0.16 < 0.96\nGPT-J\nJ1-L\nUp\nDown\nneutral\nDown\nGold off the lows\nafter dismal U.S.\nGDP\nFrugalGPT\n0.97>0.96\nGPT-J\nDown\nDown\nGPT-4\nUp\n0.44>0.37\nGPT-4\nGPT-Neo\nFSQ\nGPT-J\n   J1\nJ1-G\nJ1-L\n CoHere\nFQ\nGPT-3\nChatGPT\nGPT-C\nGPT-4\n0\n10\n20\n30\n40\n0.7\n0.72\n0.74\n0.76\n0.78\n0.8\n0.82\n0.84\n0.86\n0.88\nFrugalGPT\nCost ($)\nAccuracy\nFrugalGPT\n[...] results from\ndenying an\navailable and\naccommodation. Q:\nIs it overruling?\nFrugalGPT\n0.2<0.9\nGPT-J\nGPT-3\nChatGPT\nYes\nNo\nNo\n0.8<0.9\nNo\nGPT-4\nNo\nThe court [...] was\nexpressly\noverruled by this\ncourt in lima. Q: Is\nit overruling?\n0.6< 0.9\nGPT-J\nJ1-L\nYes\nYes\nNo\nYes\nThe time has come\nto reconcile and\nregularize our\ncases in this field.\nQ: Is it overruling?\nFrugalGPT\n0.91>0.9\nGPT-J\nYes\nYes\nGPT-4\nNo\n1.0>0.9\nGPT-4\nFrugalGPT\nWhen I [...] a little\nblack-walnut shelf\n[...] Q: What was\nthe shelf made of?\nFrugalGPT\nGPT-3\nGPT-4\nJ1\nGPT-4\n[...] told every\nTuesday for their\nstory time. [...].  Q:\nwhen did they have\ntime free?\n0.1 < 0.2\nGPT-3\nJ1\n[..] Cap Winters [...]\nadded a thousand\ngrey hairs to his\nhead [...] Q: Did he\nhave red hair?\nFrugalGPT\n0.8>0.2\nGPT-3\nGPT-4\n0.6>0.3\nGPT-4\nThe text\ndoes not\nmention this.\nNo\nTuesday\ntheir day off\nfrom school\nblack-walnut\nblack-walnut\nTuesday\nblack-walnut\nblack walnut\nblack-walnut\ntheir day off\nfrom school\nTuesday\nNo\n0.1<0.2\n0.2<0.3\nWhen I [...] a little\nblack-walnut shelf\n[...] Q: What was\nthe shelf made of?\n[...] told every\nTuesday for their\nstory time. [...].  Q:\nwhen did they have\ntime free?\n[..] Cap Winters [...]\nadded a thousand\ngrey hairs to his\nhead [...] Q: Did he\nhave red hair?\n(a) HEADLINES\n(b) OVERRULING\n(a) COQA\nGPT-Neo\nGPT-J\nJ1\nJ1-G\nJ1-L\nCoHere\nFQ\nGPT-3\nCHATGPT\nGPT-4\n0\n20\n40\n60\n80\n100\n120\n140\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nFrugalGPT\nCost ($)\nAccuracy\nFSQ\nGPT-J\nJ1\nJ1-G\n CoHere\n    FQ\nGPT-3\nChatGPT\nGPT-C\nGPT-4\n0\n2\n4\n6\n8\n10\n12\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFrugalGPT\nCost ($)\nAccuracy\nFigure 5: Accuracy and cost tradeo\ufb00s achieved by FrugalGPT. Overall, FrugalGPT often achieves the\nsame performance of the best individual LLM API (e.g., GPT-4) with orders of magnitudes smaller\ncost. When incurring the same cost, FrugalGPT can improves the accuracy by up to 5%. Examples of\nLLM cascade for each dataset are shown on the right.\n9\nFirst, the cost ranking of di\ufb00erent LLM APIs is not \ufb01xed. For instance, J1 is the second most expensive\nLLM on the HEADLINES dataset, while GPT-3 holds that position on the OVERRULING and COQA\ndatasets. This is primarily due to the heterogeneous pricing mechanism: J1 incurs a high cost for\neach generated token but charges nothing for input tokens, whereas GPT-3 charges for both input and\noutput tokens. Moreover, more expensive LLM APIs sometimes result in worse performance than their\ncheaper counterparts. For example, J1 is costlier than GPT-3 on HEADLINES, but its performance\nis inferior. These observations underscore the importance of aptly selecting LLM APIs, even in the\nabsence of budget constraints.\nNext, we note that FrugalGPT enables smooth performance-cost trade-o\ufb00s across all evaluated\ndatasets. This o\ufb00ers \ufb02exible choices to LLM users and potentially helps LLM API providers save\nenergy and reduce carbon emissions. In fact, FrugalGPT can simultaneously reduce costs and improve\naccuracy. For example, on the OVERRULING dataset, FrugalGPT achieves a 1% accuracy gain while\nreducing costs by 73% compared to the best LLM API, GPT-4. This is likely because FrugalGPT\nintegrates knowledge from multiple LLMs.\nThe example queries shown in Figure 5 further aid in understanding why FrugalGPT can simul-\ntaneously improve performance and reduce costs. GPT-4 makes mistakes on some queries (e.g., the\n\ufb01rst example in part (a)), but some low-cost APIs provide correct predictions. FrugalGPT accurately\nidenti\ufb01es those queries and relies solely on the inexpensive APIs. For example, GPT-4 incorrectly infers\nno overruling from the legal statement \u201dThe time has come to reconcile and regularize our cases in\nthis \ufb01eld,\u201d as shown in Figure 5(b). However, FrugalGPT accepts GPT-J\u2019s correct answer, avoiding\nthe use of expensive LLMs and improving overall performance. Naturally, a single LLM API is not\nalways correct; LLM cascade overcomes this by employing a chain of LLM APIs. For example, in\nthe second example shown in Figure 5(a), FrugalGPT identi\ufb01es that GPT-J\u2019s generation may not be\nreliable and turns to the second LLM in the chain, J1-L, to \ufb01nd the correct answer. Again, GPT-4\nprovides the wrong answer. FrugalGPT is not perfect, and there remains ample room for cost reduction.\nFor example, in the third example in Figure 5(c), all LLM APIs in the chain give the same answer.\nHowever, FrugalGPT is unsure if the \ufb01rst LLMs are correct, resulting in the need to query all LLMs in\nthe chain. Identifying how to avoid such cases remains an open problem.\n5\nDiscussions, Limitations and Future Prospects\nThe substantial cost of employing LLMs in real-world scenarios presents a considerable barrier to their\nwidespread usage. In this paper, we outline and discuss practical strategies for reducing the inference\ncost of using LLM APIs. We also developed FrugalGPT to illustrate one of the cost-saving strategies,\nLLM cascade. Our empirical \ufb01ndings show that FrugalGPT can reduce costs by up to 98% while\npreserving the performance of cutting-edge LLMs.\nFrugalGPT lays the groundwork for optimizing task performance with LLM APIs under budget\nconstraints; however, it has some limitations. To train the LLM cascade strategy in FrugalGPT, we\nneed some labeled examples. And in order for the cascade to work well, the training examples should be\nfrom the same or similar distribution as the test examples. Moreover, learning the LLM cascade itself\nrequires resources. We view this as an one-time upfront cost; this is bene\ufb01cial when the \ufb01nal query\ndataset is larger than the data used to train the cascade. There are also other promising strategies\nfor cost saving, such as speeding up attention computation itself, that we do not discuss here. Given\nthe rapid development of LLM, this paper is not meant to be comprehensive or to provide a de\ufb01nitive\nsolution. Our goal is to lay a foundation for this important research agenda and to demonstrate that\neven simple cascade can already achieve promising savings.\nThere are also many related directions for future exploration. While FrugalGPT concentrates\non balancing performance and cost, real-world applications call for the evaluation of other critical\nfactors, including latency, fairness, privacy, and environmental impact. Incorporating these elements\ninto optimization methodologies while maintaining performance and cost-e\ufb00ectiveness is an important\navenue for future research. Furthermore, utilizing LLMs in risk-critical applications necessitates the\ncareful quanti\ufb01cation of uncertainty in LLM-generated outputs. As the \ufb01eld progresses, addressing the\nenvironmental rami\ufb01cations of training and deploying LLMs demands a joint e\ufb00ort from LLM users\nand API providers. The continuous evolution of LLMs and their applications will inevitably unveil new\nchallenges and opportunities, fostering further research and development in this dynamic \ufb01eld.\n10\nReferences\n[AI2]\nAI21 LLM API. https://www.ai21.com/. Accessed: 2023-03-31.\n[ANC+22]\nSimran Arora, Avanika Narayan, Mayee F Chen, Laurel J Orr, Neel Guha, Kush Bhatia,\nInes Chami, Frederic Sala, and Christopher R\u00b4e. Ask me anything: A simple strategy for\nprompting language models. arXiv preprint arXiv:2210.02441, 2022.\n[BG18]\nJoy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy dispari-\nties in commercial gender classi\ufb01cation. In Conference on fairness, accountability and\ntransparency, pages 77\u201391. PMLR, 2018.\n[BGMMS21] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell.\nOn the dangers of stochastic parrots: Can language models be too big? In Proceedings of\nthe 2021 ACM conference on fairness, accountability, and transparency, pages 610\u2013623,\n2021.\n[BHS+22]\nHaoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, and Michael R Lyu. Towards\ne\ufb03cient post-training quantization of pre-trained language models. Advances in Neural\nInformation Processing Systems, 35:1405\u20131418, 2022.\n[BMR+20]\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners.\nAdvances in neural information processing\nsystems, 33:1877\u20131901, 2020.\n[Cas19]\nStephen Cass. Taking ai to the edge: Google\u2019s tpu now comes in a maker-friendly package.\nIEEE Spectrum, 56(5):16\u201317, 2019.\n[CCZZ21]\nLingjiao Chen, Tracy Cai, Matei Zaharia, and James Zou.\nDid the model change?\ne\ufb03ciently assessing machine learning api shifts. arXiv preprint arXiv:2107.14203, 2021.\n[Cha]\nChatGPT Announcement. https://openai.com/blog/chatgpt. Accessed: 2023-03-31.\n[CHSV17]\nZhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low\nprecision by half-wave gaussian quantization. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 5918\u20135926, 2017.\n[CoH]\nCoHere LLM API. https://cohere.com/. Accessed: 2023-03-31.\n[Cosa]\nCost estimation of using GPT-3 for real applications. https://www.semianalysis.com/\np/the-inference-cost-of-search-disruption. Accessed: 2023-03-31.\n[Cosb]\nCost estimation of using GPT-3 for real applications.\nhttps://neoteric.eu/blog/\nhow-much-does-it-cost-to-use-gpt-models-gpt-3-pricing-explained. Accessed:\n2023-03-31.\n[CZZ20]\nLingjiao Chen, Matei Zaharia, and James Y Zou. Frugalml: How to use ml prediction\napis more accurately and cheaply. Advances in neural information processing systems,\n33:10685\u201310696, 2020.\n[CZZ22]\nLingjiao Chen, Matei Zaharia, and James Zou. E\ufb03cient online ml api selection for\nmulti-label classi\ufb01cation tasks. In International Conference on Machine Learning, pages\n3716\u20133746. PMLR, 2022.\n[DGSG22]\nDheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. Successive prompting\nfor decomposing complex questions. arXiv preprint arXiv:2212.04092, 2022.\n[DSP+17]\nAli Diba, Vivek Sharma, Ali Pazandeh, Hamed Pirsiavash, and Luc Van Gool. Weakly\nsupervised cascaded convolutional networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 914\u2013922, 2017.\n[FFA]\nforefront AI LLM API. https://beta.forefront.ai/. Accessed: 2023-03-31.\n11\n[Fri02]\nJerome H Friedman. Stochastic gradient boosting. Computational statistics & data\nanalysis, 38(4):367\u2013378, 2002.\n[GDMR22]\nAshit Gupta, Anirudh Deodhar, Tathagata Mukherjee, and Venkataramana Runkana.\nSemi-supervised cascaded clustering for classi\ufb01cation of noisy label data. arXiv preprint\narXiv:2205.02209, 2022.\n[HMD15]\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep\nneural networks with pruning, trained quantization and hu\ufb00man coding. arXiv preprint\narXiv:1510.00149, 2015.\n[JZA19]\nZhihao Jia, Matei Zaharia, and Alex Aiken. Beyond data and model parallelism for deep\nneural networks. Proceedings of Machine Learning and Systems, 1:1\u201313, 2019.\n[KFA23]\nEldar Kurtic, Elias Frantar, and Dan Alistarh. Ziplm: Hardware-aware structured pruning\nof language models. arXiv preprint arXiv:2302.04089, 2023.\n[KNL+20]\nAllison Koenecke, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha,\nConnor Toups, John R Rickford, Dan Jurafsky, and Sharad Goel. Racial disparities\nin automated speech recognition. Proceedings of the National Academy of Sciences,\n117(14):7684\u20137689, 2020.\n[KSL+22]\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher\nPotts, and Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language\nmodels for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024, 2022.\n[KTF+22]\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark,\nand Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex\ntasks. arXiv preprint arXiv:2210.02406, 2022.\n[LLL+21]\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi,\nand Hannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning.\narXiv preprint arXiv:2110.08387, 2021.\n[LSZ+21]\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.\nWhat makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.\n[LZG+21]\nZhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and\nIon Stoica. Terapipe: Token-level pipeline parallelism for training large-scale language\nmodels. In International Conference on Machine Learning, pages 6543\u20136552. PMLR,\n2021.\n[MDL+23]\nGr\u00b4egoire Mialon, Roberto Dess`\u0131, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru,\nRoberta Raileanu, Baptiste Rozi`ere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz,\net al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.\n[Ope]\nOpenAI LLM API. https://platform.openai.com/. Accessed: 2023-03-31.\n[Ope23]\nOpenAI. Gpt-4 technical report. arXiv preprint https://arxiv.org/abs/2303.08774, 2023.\n[RCM19]\nSiva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question\nanswering challenge. Transactions of the Association for Computational Linguistics,\n7:249\u2013266, 2019.\n[RRWN11]\nBenjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: A lock-free\napproach to parallelizing stochastic gradient descent. Advances in neural information\nprocessing systems, 24, 2011.\n[SDCW19]\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled\nversion of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108,\n2019.\n12\n[SK21]\nAnkur Sinha and Tanmay Khandait. Impact of news on the commodity market: Dataset\nand results. In Advances in Information and Communication: Proceedings of the 2021\nFuture of Information and Communication Conference (FICC), Volume 2, pages 589\u2013601.\nSpringer, 2021.\n[Tex]\nTextsynth LLM API. https://textsynth.com/. Accessed: 2023-03-31.\n[TLI+23]\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and e\ufb03cient foundation language models. arXiv preprint arXiv:2302.13971,\n2023.\n[VJ04]\nPaul Viola and Michael J Jones. Robust real-time face detection. International journal\nof computer vision, 57:137\u2013154, 2004.\n[WK21]\nBen Wang and Aran Komatsuzaki.\nGpt-j-6b: A 6 billion parameter autoregressive\nlanguage model, 2021.\n[WLM11]\nLidan Wang, Jimmy Lin, and Donald Metzler. A cascade ranking model for e\ufb03cient\nranked retrieval. In Proceedings of the 34th international ACM SIGIR conference on\nResearch and development in Information Retrieval, pages 105\u2013114, 2011.\n[WRG+22]\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani,\nKiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable\nai: Environmental implications, challenges and opportunities. Proceedings of Machine\nLearning and Systems, 4:795\u2013813, 2022.\n[WWS+22]\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and\nDenny Zhou. Chain of thought prompting elicits reasoning in large language models.\narXiv preprint arXiv:2201.11903, 2022.\n[XLS+22]\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant:\nAccurate and e\ufb03cient post-training quantization for large language models. arXiv preprint\narXiv:2211.10438, 2022.\n[YLLL14]\nFan Yang, Xuan Li, Qianmu Li, and Tao Li. Exploring the diversity in cluster ensemble\ngeneration: Random sampling and random projection. Expert Systems with Applications,\n41(10):4844\u20134866, 2014.\n[YLW+23]\nZhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He. A comprehen-\nsive study on post-training quantization for large language models.\narXiv preprint\narXiv:2303.08302, 2023.\n[ZGA+21]\nLucia Zheng, Neel Guha, Brandon R Anderson, Peter Henderson, and Daniel E Ho. When\ndoes pretraining help? assessing self-supervised learning for law and the casehold dataset\nof 53,000+ legal holdings. In Proceedings of the eighteenth international conference on\narti\ufb01cial intelligence and law, pages 159\u2013168, 2021.\n[ZSH+22]\nDenny Zhou, Nathanael Sch\u00a8arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables\ncomplex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.\n13\n"
  },
  {
    "title": "SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models",
    "link": "https://arxiv.org/pdf/2305.05189.pdf",
    "upvote": "1",
    "text": "SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion\nModels with Large Language Models\nShanshan Zhong\u2217\nzhongshsh5@mail2.sysu.edu.cn\nSun Yat-sen University\nGuangzhou, China\nZhongzhan Huang\u2217\nhuangzhzh23@mail2.sysu.edu.cn\nSun Yat-sen University\nGuangzhou, China\nWushao Wen\nwenwsh@mail.sysu.edu.cn\nSun Yat-sen University\nGuangzhou, China\nJinghui Qin\u2020\nscape1989@gmail.com\nGuangdong University of Technology\nGuangzhou, China\nLiang Lin\nlinliang@ieee.org\nSun Yat-sen University\nGuangzhou, China\nA collection of seven vintage glass bottles in different shapes\nand sizes, arranged on a windowsill\nA vibrant butterfly with iridescent wings in shades of blue,\ngreen, and purple, perched on a bright pink flower\nAn aristocratic girl in medieval finery and a headdress of\nbright feathers drinking afternoon tea\nThree fluffy white kittens playing with a ball of yarn on a\nbright green carpet\nFigure 1: 512\u00d7512 samples with various types of prompts (Counting, Color, Action, etc.), showing that SUR-adapter has powerful\ncapabilities of fine-grained semantic control.\nABSTRACT\nDiffusion models, which have emerged to become popular text-to-\nimage generation models, can produce high-quality and content-\nrich images guided by textual prompts. However, there are limita-\ntions to semantic understanding and commonsense reasoning in\nexisting models when the input prompts are concise narrative, re-\nsulting in low-quality image generation. To improve the capacities\nfor narrative prompts, we propose a simple-yet-effective parameter-\nefficient fine-tuning approach called the Semantic Understanding\nand Reasoning adapter (SUR-adapter) for pre-trained diffusion mod-\nels. To reach this goal, we first collect and annotate a new dataset\nSURD which consists of more than 57,000 semantically corrected\nmulti-modal samples. Each sample contains a simple narrative\nprompt, a complex keyword-based prompt, and a high-quality\nimage. Then, we align the semantic representation of narrative\nprompts to the complex prompts and transfer knowledge of large\nlanguage models (LLMs) to our SUR-adapter via knowledge distilla-\ntion so that it can acquire the powerful semantic understanding and\nreasoning capabilities to build a high-quality textual semantic rep-\nresentation for text-to-image generation. We conduct experiments\nby integrating multiple LLMs and popular pre-trained diffusion\n\u2217Both authors contributed equally to this research.\n\u2020Corresponding author.\nmodels to show the effectiveness of our approach in enabling dif-\nfusion models to understand and reason concise natural language\nwithout image quality degradation. Our approach can make text-\nto-image diffusion models easier to use with better user experience,\nwhich demonstrates our approach has the potential for further\nadvancing the development of user-friendly text-to-image genera-\ntion models by bridging the semantic gap between simple narrative\nprompts and complex keyword-based prompts. The code is released\nat https://github.com/Qrange-group/SUR-adapter.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192 Natural language processing;\nComputer vision; Machine learning algorithms.\nKEYWORDS\ndiffusion model, large language model, multimodal image genera-\ntion, adapter, knowledge distillation\n1\nINTRODUCTION\nIn recent years, diffusion model based multimodal text-to-image\ngeneration techniques have made impressive strides [50]. With\nthese models [38, 46] trained on massive amounts of data and model\nparameters, people are able to generate text-relevant and visually\nappealing images end-to-end through text prompts and other in-\nformation, without requiring complex painting skills. However, the\narXiv:2305.05189v4  [cs.CL]  29 Nov 2023\nPreprint, Technical Report,\nShanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, & Liang Lin\nFour freshly baked pies\nSeven vintage glass bottles\nA green forest with tall trees\nand a small waterfall\nA vibrant red sports car speeding\ndown a winding road\nCounting\nColor\nSimple prompt\nComplex prompt\nSimple prompt + LLM\n(a) Sementic understanding\n(b) Commonsense reasoning\nFigure 2: The semantic understanding and commonsense reasoning capability of text encoder in diffusion models.\nquality of image generation in these existing diffusion models heav-\nily relies on the complex and elaborate design of keyword-based\ntext prompts or other forms of text prompts. Furthermore, if the text\nprompts are concise narratives or short phrases that are daily ex-\npressions, the fidelity and text relevance of the generated images are\noften significantly compromised. This limitation makes diffusion\nmodels difficult to be controlled intuitively by concise narratives\nwith excellent user experience. The most important reason for this\nissue is that the text encoders of these diffusion models, which are\noften the text encoder of pre-trained CLIP [34] trained with image-\ntext contrastive learning, are unaligned to text-to-image generation\ntask, leading to poor semantic understanding and reasoning (SUR)\nfor image generation.\nTo be specific, CLIP is a multi-modal neural model trained on\nabout 400M image-text pairs with contrastive learning and its image\nencoder and text encoder have been widely applied in various multi-\nmodal tasks or models, such as diffusion models, since it can bridge\nthe association between images and text successfully. Although the\nlearning objective of CLIP is to establish image-text correspondence\nby only pulling the matched image and text pair closer in feature\nspace, the text describing the corresponding image is brief and\nmay only match partial semantics in the image, resulting in the\nincomplete feature generated by the text encoder. However, the\ntext-to-image generation task asks a text encoder can not only\nunderstand the semantics of a concise narrative but also reason out\nand complete the implicit commonsense or knowledge grounded in\nthe narrative so that a model can generate an accurate image that is\nhighly consistent with the narrative. Therefore, embedding the text\nencoder of CLIP into diffusion models for conditional text-to-image\ngeneration results in low-quality image generation when the input\ntext is natural language due to a lack of the capability of semantic\nunderstanding and commonsense reasoning in the text encoder.\nTo show these deficiencies, we first evaluate the semantic under-\nstanding capability of the text encoder in diffusion models using\nthree common types of text prompts in multi-modal visual ques-\ntion answering [1, 7, 8, 31]: \"counting\", \"color\", and \"action\". As\nshown in Table 1, we designed three different prompts for each\nTable 1: Evaluation of semantic accuracy (Acc.) in images gen-\nerated by simple prompts using diffusion models. The sim-\nple prompts consisted of three types of sentences, including\n\"Counting\", \"Color\", and \"Action\". Each prompt generated 130\nimages, and the images were manually checked for semantic\naccuracy. The results showed that the semantic accuracy of\nmost prompts is below 50%, and even two types of prompts\nhave the semantic accuracy rate of 0%.\nType\nPrompt\nAcc.\n< 50%?\nCounting\nFour freshly baked pies.\n63.08%\nSix colorful balloons floating over a picturesque landscape.\n8.46%\n\u2713\nSeven vintage glass bottles.\n0.00%\n\u2713\nColor\nA vibrant red sports car speeding down a winding road.\n86.15%\nThe blue glass containing red juice.\n17.69%\n\u2713\nA couple wearing blue and yellow solid color clothes.\n0.00%\n\u2713\nAction\nSomeone shooting a basketball on the sports field.\n41.54%\n\u2713\nGiraffes eating trees.\n25.38%\n\u2713\nA chef tossing a pizza dough in the air in a kitchen.\n15.38%\n\u2713\ntype, and for each text prompt, we generated 130 images using a\ntext-to-image diffusion model [38] and manually evaluated whether\nthe generated images fulfilled the semantic meaning of the given\ntext prompts. Through statistical analysis of the results, we found\nthat the accuracy of semantic understanding for most of the text\nprompts does not exceed 50%. Surprisingly, even seemingly sim-\nple narrative prompts such as \"Seven vintage glass bottles\" and \"A\ncouple wearing blue and yellow solid color clothes respectively\"\nhave 0% accuracy, indicating that the text encoder in the diffusion\nmodel completely fails to understand the semantics of these sim-\nple texts for image generation and resulted in severe information\nbias. Fig.2(a) further illustrates examples of semantic error due to\ninadequate semantic understanding capability.\nNext, we consider the commonsense reasoning ability of the\ntext encoder. If we hope the stable diffusion model to generate a\nbeautiful cat, according to widely verified generation techniques,\nwe need some complex and elaborate keyword-based prompts to\nobtain high-quality generated images, such as the following prompt:\nSUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models\nPreprint, Technical Report,\n(Complex prompt example) 8k uhd, a RAW photo, a beauti-\nful cat, (realistic:1.1), masterwork, RAW photo, real cat, RAW\nphotograph, ultra high res, photorealistic, best quality, (high\ndetailed skin,skin details), visible pores, shiny skin, an ex-\ntremely delicate and beautiful, extremely detailed 8K wallpa-\nper, 8k high quality, film grain, DSLR, beautiful cat with beau-\ntiful details, (looking at viewer), professional photography\nlighting, extremely detailed eyes and face, eyes with beauti-\nful details, analog style, cute and playful, adorable, (splendid\nand colorful:1.1), portrait picture of cat, <lora:mikeneko:0.7>,\nfrom side, full body, (brown black white fur)\nWe can observe that images generated using complex prompts,\nas shown in Fig.2 (b), have better details, more accurate outlines,\nand precise common sense (such as the cat\u2019s body is natural and\nnon-distorted) compared to images generated using simple prompts\nlike \"a beautiful cat\".\n(Simple prompt example) a beautiful cat\nInputting complex prompts is equivalent to directly injecting\nthe details and understanding between \"beautiful\" and \"cat\" into\nthe text encoder, allowing diffusion models to generate a pleasing\n\"beautiful cat\". This indicates that diffusion models have the poten-\ntial to generate semantically meaningful images, but are limited by\nthe text encoder\u2019s commonsense reasoning ability. Simple prompts\ndo not allow the text encoder to directly understand the meaning\nof \"beautiful cat\" well, nor can it deduce the meaning of \"beautiful\"\nfrom the encoder\u2019s own knowledge. Facing such a problem, recent\nadvances in large language models (LLMs) such as ChatGPT and\nLLaMA [45] have shown astonishing conversational capabilities,\nwith improved SUR abilities, creating new heights in the field of nat-\nural language processing (NLP). Therefore, we made an attempt to\ndescribe \"a beautiful cat\" using ChatGPT and obtained the following\ntext:\n(Commonsense reasoning of LLM) Cats are known for\ntheir captivating beauty, with their soft fur, expressive eyes,\nand graceful movements. A beautiful cat might have distinc-\ntive features such as a sleek coat with unique patterns, pierc-\ning eyes, and an elegant posture. Each cat is unique in its\nown way, and their beauty is subjective to the beholder\u2019s\nperspective.\nThis text demonstrates the LLMs understanding of \"beautiful\" and\n\"cat\", as well as its commonsense reasoning on what kind of \"cat\" is\nconsidered \"beautiful\". The image produced by this text is similar\nin quality to images generated using complex prompts, as shown\nin Fig.2 (b) bottom right.\nAll of the case studies above inspire us to consider whether\nwe can transfer the SUR abilities of LLMs to pre-trained diffusion\nmodels so that diffusion models can produce semantically correct\nand high-quality images even with simple narrative prompts.\nTo achieve this goal, in this paper, we first collect and annotate\na new dataset named SURD, which consists of more than 57,000 se-\nmantically corrected image-text pairs. Each image-text pair contains\na simple narrative prompt, a complex keyword-based prompt, and a\nhigh-quality image. Leveraging SURD, we propose the SUR-adapter\nto transfer the SUR abilities of LLMs to pre-trained diffusion models\nand align the representations of simple and complex prompts. Ex-\ntensive experiments and statistical tests confirm that our proposed\nSUR-adapter significantly enhances the text encoder of pre-trained\ndiffusion models and generates high-quality images that alleviate\nthe mismatch between concise narrative prompts and generated\nimages. In summary, our contributions are threefold:\n\u2022 We collect and annotate a dataset SURD, which includes over\n57,000 semantically corrected image-text pairs. Each image-\ntext pair contains a simple prompt, a complex prompt, and a\nhigh-quality corresponding image.\n\u2022 Based on SURD, we propose SUR-adapter to effectively trans-\nfer the semantic understanding and reasoning abilities of\nLLMs to pre-trained diffusion models, alleviating the issue of\nsemantic mismatch and low-quality images generated with\nsimple prompts.\n\u2022 We conduct extensive statistical tests and discussions on\nthe generated images using the proposed SUR-adapter to\nanalyze its effectiveness and further discuss its limitations.\n2\nRELATED WORKS\n2.1\nText-to-Image Diffusion\nDiffusion models have been extensively utilized in text-to-image\ngeneration [2, 11, 23, 25, 38, 41, 46]. Text-to-image diffusion uti-\nlizes textual input as a conditioning signal for diffusion models,\ngenerating text-related images via a process of noise addition and\nremoval [38]. The text encoder of text-to-image diffusion is often\naccomplished by leveraging pre-trained language models such as\nCLIP [34] to encode textual inputs into latent vectors. Text-to-image\ndiffusion is widely used in various fields, such as image super-\nresolution [27, 42], inpainting [32], manipulation [5, 54], semantic\nsegmentation [4, 12], video generation [51, 56], etc.\n2.2\nLarge Language Models\nRecently, the NLP field has witnessed a proliferation of LLMs [17].\nJozefowicz et al. [21] achieved state-of-the-art results on the Billion\nWord benchmark by scaling LSTMs to 1 billion parameters. Subse-\nquently, scaling transformers led to improvements on many NLP\ntasks, with notable models including BERT [10], GPT-2 [35], Mega-\ntronLM [43], and T5 [37]. The introduction of GPT-3 [6], a model\nwith 175 billion parameters, marked a significant breakthrough in\nthis area and led to the development of numerous LLMs, such as\nJurassic-1 [29], Megatron-Turing NLG [44], Gopher [36], Chinchilla\n[17], PaLM [9], OPT [57], GLM [52] and LLAMA [45]. Furthermore,\nseveral studies [15, 17, 22, 40, 49] have investigated the impact of\nscaling on LLM performance to enhance their ease of use.\n3\nSEMANTIC UNDERSTANDING AND\nREASONING DATASET\nSURD is a multi-modal dataset comprising 57,603 triplets of simple\nnarrative prompts, complex keyword-based prompts, and semanti-\ncally correct images, as shown in Fig. 3. To our knowledge, SURD\nis the first dataset that records both simple and complex prompts\nand focuses on providing semantically correct image-text pairs to\naid in solving the SUR problem of text-to-image diffusion models,\nPreprint, Technical Report,\nShanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, & Liang Lin\nComplex prompt: close up full body portrait of\ncutecreature style Tiny cute isometric alebrije emoji,\nsoft smooth lighting, with bold flat colors, 3d icon\nclay render, 100mm lens, 3d blender render,natural\nenvironment scenery, Mexican inspired background,\ntrending on polycount, modular constructivism,\nphysically based rendering, geometric, centered,\ndynamic pose. The alebrije should be inspired by\nMexican folk art and should be a combination of\nseveral different animals, such as a lion, eagle, or\nserpent., solo\nSimple prompt: a colorful animal with big eyes on\na blue background\nImage\nBLIP\nDiffusion models\nFigure 3: An example of SURD. We collect a diverse set of\ncomplex prompts and corresponding images generated by\ndiffusion models from publicly available websites and lever-\nage pre-trained BLIP to generate simple prompts.\nwhich allows diffusion models to generate high-quality images that\nare semantically consistent based on simple prompts alone.\n3.1\nData Collection\nRaw Data. To construct a content-rich and semantically reliable\ndataset, we extensively investigate various open-source image gen-\neration websites with reliable prompts and high-quality images.\nAmong them, we select three websites: Lexica 1, civitai 2, and Stable\nDiffusion Online 3. On these websites, publicly available images\nare often semantically correct and of high quality with complex\nprompts. Therefore, we collect the prompts from websites as com-\nplex prompts. In total, we collect 114,148 image-text pairs.\nData Cleaning. In order to ensure the correct semantic match of\neach sample in the SURD, we perform data cleaning in two steps. In\nthe first step, to ensure the semantic accuracy of the simple prompts\ngenerated by BLIP [13], we use the publicly available pre-trained\nmodel CLIP [34] for semantic cleaning since the text encoder in\nmost of diffusion models is the text encoder of the CLIP model,\nwhich will be explained in Section 4.1. If the CLIP model judges\nthe semantic of a simple prompt that matches the semantic of\nthe corresponding image, diffusion models are likely to be able\nto generate similar images according to the simple prompt. For\neach image, we ask CLIP to classify between its simple prompt\nand its complex prompt for selecting a prompt matching the image\nbest semantically. In general, a complex prompt often contains\nother semantically irrelevant information, such as image quality\ndescriptions, so a semantically correct simple prompt generally has\na higher CLIP score than the complex prompt. Therefore, if the\nCLIP score of a simple prompt is not lower than the corresponding\ncomplex prompt, we retain the sample. After the automatic semantic\ncleaning based on the CLIP score, we retain 66,408 samples. In\nthe second step, we further filter the samples retained in the first\nstep manually to ensure that all image-text pairs are semantically\nmatched. Finally, SURD contains 57,603 image-text pairs where\neach image-text pair contains an image, a simple prompt, and a\ncomplex prompt.\n1https://lexica.art, 2https://civitai.com, 3https://stablediffusionweb.com\nKnowledge from LLM. Since we hope to distill knowledge from\nLLM to improve the semantic understanding and reasoning capaci-\nties of a text encoder, we also save the knowledge of simple prompts\nfrom LLM in vectors. Specifically, we use the recently open-sourced\nlarge language model LLaMA [45] with three different parameter\nsizes: 7B (32 layers, dimension is 4096), 13B (40 layers, dimension\nis 5120), and 33B (60 layers, dimension is 6656). For each simple\nprompt, we compute the mean value of each token embeddings\ngenerated by the LLM as the knowledge representation so that we\ncan handle different samples with different lengths uniformly.\nIn addition, we resize all images to 512 \u00d7 512 uniformly. Further\ndetails regarding the usage of BLIP, CLIP, and LLM can be found in\nthe appendix.\n3.2\nData Analysis\nPrompt Length. Fig. 4 shows the distribution of sentence length for\nprompts, with (a) representing the distribution for complex prompts\nand (b) representing the distribution for simple prompts. In order\nto enhance visual clarity, prompts longer than 300 words have\nbeen incorporated into 300 words. The length distribution of simple\nprompts is relatively concentrated, with sentence lengths centered\naround 10, which is consistent with human language patterns. In\ncontrast, complex prompts, with a long tail distribution, not only\ncontain semantics but also include definitions and image quality\ninformation, resulting in sentence lengths that are significantly\nlonger than simple prompts.\nPrompt Content. A prompt for text-to-image generation usually\ncontains a significant number of nouns which could influence the\nquality and semantic coherence of the generated image greatly\nsince an image consists of different objects. Therefore, we con-\nduct a statistical analysis of the frequency distribution of nouns\noccurred in the SURD to demonstrate the diversity of both text\nand visual content. Fig. 4 (c) displays the frequency-proportional\ndistribution of selected entities from SURD. These entities cover a\ndiverse range of ordinary objects, such as people, animals, plants,\nand scenes, indicating the content diversity of SURD. Besides, the\ndiversity of these entities can make pre-trained diffusion models\nhave strong high-level understanding capacities of text and visual\ncontent in more complex scenes. Furthermore, we also present a\nword cloud of the text as shown in Fig. 4 (d) by filtering out stop\nwords to illustrate the overall distribution of text vocabulary in\nSURD. The most frequently occurred phrases, such as \"best quality\",\n\"masterpiece best\", and \"extremely detailed\", primarily constrain\nthe image quality and originate from complex prompts, indicating\nthat these consistent text constraints are important for high-quality\nimage generation. Therefore, the semantic representation of com-\nplex prompts will play a crucial role in enhancing the diffusion\nmodels with SUR-adapter through finetuning.\n4\nMETHOD\nIn this section, we introduce how SUR-adapter transfers the se-\nmantic understanding and reasoning capabilities of large language\nmodels and achieves the representation alignment between complex\nprompts and simple prompts.\nSUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models\nPreprint, Technical Report,\n(a)\n(b)\n(c)\n(d)\nFigure 4: (Left) Prompt length distributions and (Right) prompt content distributions.\nSimple Prompt\nImage\nNoise\n+\nPre-trained \nDiffusion Model\nText Encoder\nPredictor\nComplex Prompt\nLarge Language Model\n(LLM)\nAdapter\nNoise\n+\nFrozen\nLearnable\nForward\nDistillation\nTransformer\nFCN\nLLM\nText Encoder\n\u00d7\nAttention\n+\n+\nAdapter\nFigure 5: Illustration of SUR-adapter. FCN is a fully-\nconnected network. (Left) The fine-tuning pipeline for pre-\ntrained diffusion models. Given a pre-trained diffusion\nmodel, the adapter is used to transfer the semantic under-\nstanding and reasoning capabilities of large language mod-\nels and align the representation between complex and sim-\nple prompts. The weight coefficient, \ud835\udf02, is used to adjust the\nadapter\u2019s effect. (Right) The network structure of the adapter.\n4.1\nPreliminary\nDiffusion models are excellent methods for multi-modal image\ngeneration, which typically involve two stages: (1) Forward noise\nprocess. Assuming that the training data x0 comes from a given\ndistribution \ud835\udc5d(x0), the diffusion model first obtains a sequence\nx1, x2, ..., x\ud835\udc47 by adding \ud835\udc47 rounds of noise to x0, as follows:\n\ud835\udc5e(x\ud835\udc61 |x0) = \ud835\udc41 (x\ud835\udc61;\ud835\udefc\ud835\udc61x0, \ud835\udf0e2\n\ud835\udc61 I),\n(1)\nwhere \ud835\udf16 is sampled from the standard normal distribution \ud835\udc41 (0, I),\n\ud835\udf0e2\n\ud835\udc61 is a given noise strength that depends on \ud835\udc61, and \ud835\udefc\ud835\udc61 is generally set\nto \ud835\udefc\ud835\udc61 =\n\u221a\ufe03\n1 \u2212 \ud835\udf0e2\n\ud835\udc61 . At this point, we have x\ud835\udc61 = \ud835\udefc\ud835\udc61x0+\ud835\udf0e\ud835\udc61\ud835\udf16. (2) Reverse\ndenoising process. After obtaining the sequence x1, x2, ..., x\ud835\udc47 from\nthe forward noise process, the denoising process from x\ud835\udc61 to x\ud835\udc61\u22121\ncan be modeled by\n\ud835\udc5d\ud835\udf03 (x\ud835\udc61\u22121|x\ud835\udc61) = \ud835\udc41 (x\ud835\udc61\u22121; \u02c6\ud835\udf07\ud835\udf03 (x\ud835\udc61), \u02c6\u03a3\ud835\udf03 (x\ud835\udc61)),\n(2)\nwhere \u02c6\ud835\udf07\ud835\udf03 (x\ud835\udc61) and \u02c6\u03a3\ud835\udf03 (x\ud835\udc61)) are the predicted statistics, and \ud835\udf03 is the\nlearnable parameter. Many recent works [16, 38, 53] have shown\nthat Eq.(2) can be efficiently optimized via the following loss func-\ntion:\n\u2113\ud835\udc61\nsimple(\ud835\udf03) = E\u2225\ud835\udf16 \u2212 \u02c6\ud835\udf16\ud835\udf03 (\ud835\udefc\ud835\udc61x0 + \ud835\udf0e\ud835\udc61\ud835\udf16,\ud835\udc61)\u22252\n2,\n(3)\nwhere \u02c6\ud835\udf16\ud835\udf03 (\u00b7) is a learnable neural network that predicts the added\nnoise \ud835\udf16 in the input x\ud835\udc61. When this neural network is well-trained,\nwe can use x\ud835\udc61 = \ud835\udefc\ud835\udc61x0 + \ud835\udf0e\ud835\udc61\ud835\udf16 and some certain sampling methods\nto infer x0. Note that as \ud835\udc47 \u2192 \u221e or becomes sufficiently large,\nx\ud835\udc47 can be viewed as an approximation of a normally distributed\nnoise. Therefore, we can randomly sample noise \ud835\udf160 from a normal\ndistribution and use the neural network \u02c6\ud835\udf16\ud835\udf03 (\u00b7), also known as the\npredictor (as shown in Fig. 5), to generate an image \u02c6x0. To achieve\ncontrollable generation, a condition \ud835\udc50 can be added to the predictor,\ni.e., rewriting the predictor as \u02c6\ud835\udf16\ud835\udf03 (\u00b7,\ud835\udc50). For text-to-image generation\ntasks, the condition \ud835\udc50 is usually generated from a text prompt by a\ntext encoder, such as the text encoder of CLIP.\nAlgorithm 1 The Algorithm of Fine-tuning Pre-trained Diffusion\nModel with SUR-adapter.\n1: Input: The dataset SURD (\ud835\udc5d\ud835\udc56\ud835\udc50, \ud835\udc5d\ud835\udc56\ud835\udc60, \ud835\udc3c\ud835\udc56 )\ud835\udc41\n\ud835\udc56=1, a learnable transformation\n\ud835\udc54(\u00b7;\ud835\udf191) and Adapter \ud835\udc54Ada(\u00b7;\ud835\udf192); Large language model \ud835\udc53LLM and the\ntext encoder \ud835\udc53En with fixed parameters. Training step \ud835\udc470.\n2: while The training step \ud835\udc470 \u2265 0 do\n3:\n// Knowledge distillation from LLM\n4:\nCalculate the knowledge distillation loss \u2113LLM by Eq.(5)\n5:\nMeasure the semantic information \ud835\udc50\u2032\nLLM by Eq.(7)\n6:\n// Performance maintenance\n7:\nAdd noise to \ud835\udc3c\ud835\udc56 to obtain \ud835\udefc\ud835\udc61\ud835\udc3c\ud835\udc56 + \ud835\udf0e\ud835\udc61\ud835\udf16 by Eq.(1)\n8:\nUse \ud835\udc50\u2032\nLLM to measure \u2113\ud835\udc61\nsimple(\ud835\udf19) by Eq.(8)\n9:\n// Representation alignment\n10:\nMeasure \ud835\udc53\ud835\udc38\ud835\udc5b (\ud835\udc5d\ud835\udc56\ud835\udc50 ) by complex prompt \ud835\udc5d\ud835\udc56\ud835\udc50\n11:\nUse \ud835\udc50\u2032\nLLM and \ud835\udc53\ud835\udc38\ud835\udc5b (\ud835\udc5d\ud835\udc56\ud835\udc50 ) to measure \u2113CP(\ud835\udf19) by Eq.(9)\n12:\n// Update the parameters\n13:\nCalculate the total loss \u2113total(\ud835\udf19) by Eq.(10)\n14:\nUpdate the learnable parameters \ud835\udf19 = [\ud835\udf191,\ud835\udf192] by \u2113total(\ud835\udf19)\n15:\n\ud835\udc470 \u2190 \ud835\udc470 \u2212 1\n16: end while\n17: return \ud835\udf19\n4.2\nThe Fine-tuning Pipeline of SUR-adapter\nIn this section, we introduce our simple yet effective fine-tuning\napproach called the semantic understanding and reasoning adapter\nPreprint, Technical Report,\nShanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, & Liang Lin\nTable 2: Evaluation results of the diverse pre-trained models and controlled methods described in Section 5.1 in terms of various\nsemantic metrics.\nPre-trained\nModel\nControlled Method\nCLIP Score\nAction (%)\nColor (%)\nCounting (%)\nBaseline\nOurs\nBaseline\nOurs\nBaseline\nOurs\nBaseline\nOurs\nDM (1.5),\nLLM (13B)\n-\n0.498\n0.517 \u2191\n75.33\n80.67 \u2191\n81.33\n87.33 \u2191\n14.67\n36.67 \u2191\nControlNet (canny)\n0.508\n0.492 \u2193\n76.67\n84.67 \u2191\n68.67\n69.33 \u2191\n96.00\n94.00 \u2193\nControlNet (seg)\n0.481\n0.472 \u2193\n7.33\n9.33 \u2191\n10.00\n10.67 \u2191\n40.67\n62.00 \u2191\nPrompt Weighting\n0.486\n0.514 \u2191\n78.00\n85.33 \u2191\n91.33\n88.00 \u2193\n43.33\n58.00 \u2191\nMultiDiffusion\n0.470\n0.516 \u2191\n74.67\n88.67 \u2191\n87.33\n81.33 \u2193\n23.33\n62.67 \u2191\nSelf-attention Guidance\n0.474\n0.526 \u2191\n73.33\n86.00 \u2191\n86.00\n86.67 \u2191\n12.67\n14.00 \u2191\nDM (cartoon),\nLLM (13B)\n-\n0.467\n0.490 \u2191\n58.00\n68.67 \u2191\n82.00\n88.00 \u2191\n21.33\n38.00 \u2191\nControlNet (canny)\n0.514\n0.486 \u2193\n83.33\n81.33 \u2193\n47.33\n67.33 \u2191\n74.00\n86.67 \u2191\nControlNet (seg)\n0.509\n0.491 \u2193\n38.67\n51.33 \u2191\n28.00\n30.67 \u2191\n45.33\n62.00 \u2191\nPrompt Weighting\n0.554\n0.546 \u2193\n84.00\n79.33 \u2193\n88.67\n91.33 \u2191\n41.33\n50.00 \u2191\nMultiDiffusion\n0.413\n0.587 \u2191\n63.33\n80.67 \u2191\n88.00\n87.33 \u2193\n18.67\n36.67 \u2191\nSelf-attention Guidance\n0.440\n0.560 \u2191\n65.33\n73.33 \u2191\n72.67\n86.00 \u2191\n16.67\n39.33 \u2191\n(SUR-adapter) for the controllable text-to-image diffusion model.\nLet us consider the image-text pairs (\ud835\udc5d\ud835\udc56\ud835\udc50, \ud835\udc5d\ud835\udc56\ud835\udc60, \ud835\udc3c\ud835\udc56)\ud835\udc41\n\ud835\udc56=1 in the SURD\ndataset, where \ud835\udc5d\ud835\udc56\ud835\udc50 and \ud835\udc5d\ud835\udc56\ud835\udc60 are the complex and simple prompts, re-\nspectively, for the \ud835\udc56-th high-quality and semantically correct image\n\ud835\udc3c\ud835\udc56. As shown in Fig.5 (Left), we first freeze all learnable parameters\nof the large language model \ud835\udc53LLM, the text encoder \ud835\udc53\ud835\udc38\ud835\udc5b, and the\npredictor \ud835\udc53\ud835\udc5d\ud835\udc5f\ud835\udc52 in the pre-trained diffusion model, and then we add\ntwo trainable neural networks, a fully-connected network (FCN)\n\ud835\udc54(\u00b7;\ud835\udf191) and an adapter \ud835\udc54Ada(\u00b7;\ud835\udf192), with learnable parameters \ud835\udf191\nand \ud835\udf192.\n4.2.1\nKnowledge Distillation by LLM. The structure of the adapter\n\ud835\udc54Ada(\u00b7;\ud835\udf192) is shown in Fig. 5 (Right), and it consists of three learn-\nable transformations, \u210e\ud835\udc57 (\u00b7) for \ud835\udc57 = 1, 2, 3, which are implemented\nusing fully connected neural networks or Transformer [47]. For the\noutput \ud835\udc53\ud835\udc38\ud835\udc5b(\ud835\udc5d\ud835\udc56\ud835\udc60) of the text encoder, we construct \ud835\udc44\ud835\udc56 = \u210e3[\ud835\udc53\ud835\udc38\ud835\udc5b(\ud835\udc5d\ud835\udc56\ud835\udc60)]\nand \ud835\udc3e\ud835\udc56 = \u210e2[\ud835\udc53\ud835\udc38\ud835\udc5b(\ud835\udc5d\ud835\udc56\ud835\udc60)], and calculate an attention value as [10, 47]\natt\ud835\udc56 = softmax(\n\ud835\udc44\ud835\udc56\ud835\udc3e\ud835\udc47\n\ud835\udc56\n\u221a\n\ud835\udc51\n),\n(4)\nwhere \ud835\udc51 is the feature dimension of \ud835\udc44\ud835\udc56 and \ud835\udc3e\ud835\udc56. To ensure that the\nsemantic information of the simple prompt is not directly interfered\nwith, we directly set \ud835\udc49\ud835\udc56 = \ud835\udc53\ud835\udc38\ud835\udc5b(\ud835\udc5d\ud835\udc56\ud835\udc60) without any transformation. In\nparticular, to embed the powerful semantic understanding and\nreasoning capabilities of the LLM in att\ud835\udc56, we distill knowledge from\nLLM by the following loss function:\n\u2113LLM(\ud835\udf19) = KL[W0\ud835\udc53LLM(\ud835\udc5d\ud835\udc56\n\ud835\udc60)/\ud835\udf0f,\ud835\udc44\ud835\udc56/\ud835\udf0f],\n(5)\nHere, \ud835\udf0f is the temperature, which is typically set to 1, and KL is the\nKL divergence. W0 is a randomly initialized matrix using Kaiming\ninitialization and is unlearnable, which ensures that semantic in-\nformation of LLM is reserved as much as possible while aligning\nthe dimensions between \ud835\udc53LLM(\ud835\udc5d\ud835\udc56\ud835\udc60) and \ud835\udc44. Moreover, we obtain the\ncalibrated semantic information as \ud835\udc49 \u2032\n\ud835\udc56 = \ud835\udc49\ud835\udc56 \u2297 att\ud835\udc56. Finally, the out-\nput of the Adapter is transformed by the learnable transformation\n\ud835\udc54(\u00b7;\ud835\udf191) to obtain the output \ud835\udc50LLM with LLM semantic capabilities\nas prior works [14, 20, 58]:\n\ud835\udc54\n\b\n\ud835\udc54Ada(\ud835\udc53\ud835\udc38\ud835\udc5b(\ud835\udc5d\ud835\udc56\n\ud835\udc60);\ud835\udf192);\ud835\udf191\n\t\n= \ud835\udc54\n\b\n\ud835\udc49 \u2032\n\ud835\udc56 + \ud835\udc49\ud835\udc56 + \u210e1[\ud835\udc49 \u2032\n\ud835\udc56 + \ud835\udc49\ud835\udc56];\ud835\udf191\n\t\n,\n(6)\nand the semantic information input to the predictor is as follows:\n\ud835\udc50\u2032\nLLM = \ud835\udf02 \u00b7 \ud835\udc50LLM + (1 \u2212 \ud835\udf02) \u00b7 \ud835\udc53\ud835\udc38\ud835\udc5b(\ud835\udc5d\ud835\udc56\n\ud835\udc60).\n(7)\nwhere \ud835\udf02 is a constant.\n4.2.2\nPerformance Maintenance of DMs During Fine-tuning. To\nmaintain the performance of the diffusion model during fine-tuning,\nwe add varying levels of noise to the image \ud835\udc3c\ud835\udc56 by Eq.(1), and feed\nthe semantic information feature \ud835\udc50\u2032\nLLM obtained from Eq.(7) to the\npredictor, guided by the simple prompt \ud835\udc5d\ud835\udc56\ud835\udc60. To ensure that the pre-\ntrained diffusion model maintains sufficient denoising ability for\nnew images \ud835\udc3c\ud835\udc56 during fine-tuning, we minimize the following loss\nfunction:\n\u2113\ud835\udc61\nsimple(\ud835\udf19) = E\u2225\ud835\udf16 \u2212 \u02c6\ud835\udf16(\ud835\udefc\ud835\udc61\ud835\udc3c\ud835\udc56 + \ud835\udf0e\ud835\udc61\ud835\udf16,\ud835\udc61,\ud835\udc50\u2032\nLLM)\u22252\n2,\n(8)\nFurthermore, to ensure stable training of the added adapter and\nreduce its adverse impact on the pre-trained diffusion model dur-\ning the early stage of training, we follow the setting of previous\nworks [19, 54] by initializing all elements of the matrices in param-\neter \ud835\udf191 to 0.\n4.2.3\nAligning the Representation between Complex Prompts and\nSimple Prompts. From the description in Section 3, we know that\nimage \ud835\udc3c\ud835\udc56 is a semantically correct and high-quality image gener-\nated by \ud835\udc5d\ud835\udc56\ud835\udc50. In order to generate images of sufficient similarity and\nquality as \ud835\udc3c\ud835\udc56 by a simple prompt. we need to align the semantic\nrepresentation of feature between \ud835\udc50\u2032\nLLM and \ud835\udc53\ud835\udc38\ud835\udc5b(\ud835\udc5d\ud835\udc56\ud835\udc50). Specifically,\nwe consider minimizing the following loss function:\n\u2113CP(\ud835\udf19) = KL(\ud835\udc50\u2032\nLLM/\ud835\udf0f, \ud835\udc53\ud835\udc38\ud835\udc5b(\ud835\udc5d\ud835\udc56\n\ud835\udc50)/\ud835\udf0f),\n(9)\nwhere \ud835\udf0f is set as in Eq.(5) and KL denotes the KL divergence [26].\nIn summary, the final loss function for SUR-adapter training is\nas follows:\n\u2113total(\ud835\udf19) = \ud835\udf061 \u00b7 \u2113LLM(\ud835\udf19) + \ud835\udf062 \u00b7 \u2113CP(\ud835\udf19) + \u2113\ud835\udc61\nsimple(\ud835\udf19),\n(10)\nwhere \ud835\udf06\ud835\udc56 \u2264 1, \ud835\udc56 = 1, 2 are loss coefficients. We present the training\nprocess of SUR-adapter in Algorithm 1. After training, the fine-\ntuned diffusion model can generate images using the same sampling\nmethod as before.\nSUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models\nPreprint, Technical Report,\nTable 3: Evaluation results of the diverse pre-trained models and controlled methods described in Section 5.1 in terms of various\nquality metrics. We calculate the T-test for the means of two independent samples of scores, and if the resulting P-value is\ngreater than 0.05, it implies that there is no significant difference between the NR scores of the baselines and SUR-adapter,\nindicating that their generation quality is comparable.\nPre-trained\nModel\nControlled Method\nBRISQUE\nCLIP-IQA\nMUSIQ\nUser Preference (%)\nBaseline\nOurs (P > 0.05?)\nBaseline\nOurs (P > 0.05?)\nBaseline\nOurs (P > 0.05?)\nBaseline\nOurs\nDM (1.5),\nLLM (13B)\n-\n13.85\n14.78 (\u2713)\n0.686\n0.688 (\u2713)\n67.38\n67.04 (\u2713)\n48.31\n51.69\nControlNet (canny)\n22.68\n25.15 (\u00d7)\n0.673\n0.668 (\u2713)\n67.41\n67.14 (\u2713)\n49.81\n50.19\nControlNet (seg)\n39.86\n42.12 (\u2713)\n0.662\n0.668 (\u2713)\n64.12\n65.71 (\u00d7)\n53.56\n46.44\nPrompt Weighting\n13.29\n13.74 (\u2713)\n0.681\n0.691 (\u2713)\n66.97\n67.02 (\u2713)\n47.94\n52.06\nMultiDiffusion\n10.84\n11.83 (\u2713)\n0.696\n0.691 (\u2713)\n66.60\n67.95 (\u2713)\n52.06\n47.94\nSelf-attention Guidance\n15.08\n17.06 (\u2713)\n0.694\n0.706 (\u2713)\n67.51\n68.97 (\u2713)\n48.31\n51.69\nDM (cartoon),\nLLM (13B)\n-\n15.74\n19.53 (\u00d7)\n0.699\n0.707 (\u2713)\n66.07\n67.03 (\u2713)\n50.94\n49.06\nControlNet (canny)\n18.68\n18.49 (\u2713)\n0.697\n0.696 (\u2713)\n67.98\n67.95 (\u2713)\n50.56\n49.44\nControlNet (seg)\n35.84\n31.96 (\u00d7)\n0.710\n0.701 (\u2713)\n67.51\n67.68 (\u2713)\n51.69\n48.31\nPrompt Weighting\n17.62\n19.12 (\u2713)\n0.714\n0.698 (\u2713)\n67.38\n66.46 (\u2713)\n51.31\n48.69\nMultiDiffusion\n14.88\n15.96 (\u2713)\n0.709\n0.711 (\u2713)\n68.05\n67.26 (\u2713)\n47.94\n52.06\nSelf-attention Guidance\n20.44\n20.98 (\u2713)\n0.705\n0.706 (\u2713)\n67.74\n66.90 (\u2713)\n52.43\n47.57\nTable 4: The performance of diffusion models under various LLM settings. Bold and underline indicate the optimal and\nsuboptimal performance, respectively.\nPre-trained\nModel\nLLM Layer or\nControlled Method\nCLIP Score\nAction (%)\nColor (%)\nCounting (%)\nBRISQUE\nCLIP-IQA\nMUSIQ\nDM (1.5),\nLLM (13B)\n1\n0.414\n68.00\n82.00\n32.67\n13.89\n0.688\n67.04\n10\n0.502\n74.00\n84.67\n34.00\n15.28\n0.694\n68.04\n20\n0.496\n78.00\n81.33\n30.00\n15.77\n0.690\n67.27\n30\n0.482\n72.67\n90.00\n31.33\n17.85\n0.691\n67.25\n40\n0.517\n80.67\n87.33\n36.67\n14.78\n0.684\n67.47\nDM (cartoon),\nLLM (13B)\n1\n0.387\n70.00\n79.33\n26.67\n15.94\n0.707\n67.04\n10\n0.434\n72.67\n82.67\n34.67\n17.31\n0.703\n66.02\n20\n0.493\n76.00\n87.33\n31.33\n16.20\n0.707\n67.03\n30\n0.533\n78.00\n91.33\n38.00\n17.58\n0.707\n66.50\n40\n0.490\n68.67\n88.00\n38.00\n19.53\n0.695\n66.04\nDM (1.5),\nLLM (7B)\n-\n0.494\n80.67\n85.33\n35.33\n12.96\n0.688\n67.33\nControlNet (canny)\n0.476\n82.67\n68.67\n88.00\n22.80\n0.675\n67.30\nControlNet (seg)\n0.519\n8.00\n8.67\n60.67\n39.11\n0.670\n65.54\nPrompt Weighting\n0.601\n84.00\n83.33\n53.33\n14.53\n0.688\n67.09\nMultiDiffusion\n0.399\n92.00\n88.00\n63.33\n14.70\n0.691\n67.85\nSelf-attention Guidance\n0.514\n80.67\n85.33\n18.00\n17.76\n0.694\n67.15\nDM (1.5),\nLLM (33B)\n-\n0.523\n82.00\n88.67\n38.67\n14.38\n0.690\n67.66\nControlNet (canny)\n0.482\n84.67\n70.00\n94.67\n26.94\n0.671\n67.74\nControlNet (seg)\n0.505\n7.33\n8.00\n64.00\n39.39\n0.673\n65.54\nPrompt Weighting\n0.530\n84.67\n92.67\n58.67\n13.96\n0.702\n67.38\nMultiDiffusion\n0.496\n87.33\n88.67\n61.33\n13.84\n0.705\n67.92\nSelf-attention Guidance\n0.517\n86.00\n89.33\n20.67\n15.48\n0.706\n67.95\n5\nEXPERIMENTS\n5.1\nImplementation Details\nWe utilize two pre-trained diffusion models (DMs) and three LLMs\n[45] with different parameters. DM (1.5) [38] specialized in high-\nresolution image synthesis and DM (cartoon) 2 trained on modern\nanime feature film images. LLM (\ud835\udc60) means the LLaMa model with\nthe parameter size of \ud835\udc60. In addition, we validate the universality of\nSUR-adapter with various controlled methods. ControlNet [54] is\nan auxiliary network that introduces an additional condition. Our\nexperiments include 2 canonical pre-trained ControlNets, namely\nedge detection with ControlNet (canny) and semantic segmenta-\ntions with ControlNet (seg). Prompt weighting 3 is a straightforward\ntechnique that assigns higher attention weights to specific parts of\nthe text input. MultiDiffusion [3] defines a novel generation process\n2https://huggingface.co/nitrosocke/Ghibli-Diffusion\n3https://github.com/damian0815/compel\non top of a pre-trained diffusion model, which merges multiple dif-\nfusion generation methods. Self-attention Guidance [18] provides\ndirection from predictions that are not reliant on high-frequency\ndetails to fully conditioned images. The high-frequency details are\nextracted from the UNet self-attention maps.\nWe use the SURD dataset to evaluate models by two types of\nmetrics: semantic and quality. It is worth noting that all metrics\nare positively oriented. For semantic evaluation, we design three\ntypes of prompts [1, 7, 8, 31], namely Action, Color, and Counting,\neach with fifteen prompts. These prompts are used to evaluate\nthe semantic capabilities of the baselines and SUR-adapter. Action,\nColor, and Counting are all percentage metrics that indicate the\nproportion of images that meet the different types of semantics.\nDuring testing, we generate ten images for each prompt. To further\nevaluate the semantic quality, we also use the CLIP Score [34]. We\nuse CLIP to construct the binary classification problem for both\nPreprint, Technical Report,\nShanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, & Liang Lin\nthe baselines and SUR-adapter and select the most appropriate\nimages based on the prompts. After applying Softmax to avoid the\neffects of extreme values, we record the scores of the baselines and\nSUR-adapter, and use the mean value on the test set as the final\nCLIP score of the diffusion models. For quality evaluation, we use\nBRISQUE [33], CLIP-IQA [48], MUSIQ [24], and user preference\nstudy. The user preference study consists of single-choice questions\nwhere users choose the image with the best quality from a pair of\nimages generated by the baselines and SUR-adapter. We collected 89\nvalid questionnaires from the user preference study. In the appendix,\nwe provide detailed training recipes.\n5.2\nExperiment Analysis\nTable 2 shows the semantic capabilities of both baselines and\nSUR-adapter. Notably, the results demonstrate that SUR-adapter\ncan effectively enhance the SUR performance of the baselines in\nmost cases. Furthermore, we can draw the following conclusions:\n(a) the use of Softmax to obtain a relative score in CLIP can render\nthe CLIP Score unreliable, particularly when both the baselines and\nSUR-adapter yield equally poor results. For instance, ControlNet\n(seg) attains a relatively high score despite its subpar generation\neffects on Action and Color. (b) ControlNet performs well in Count-\ning scores since it utilizes image outlines with the correct amount of\ninformation as a reference. (c) Inaccurate image segmentation can\ncause diffusion models with ControlNet (seg) to disregard semantic\ninformation and generate entirely blurry images, thus resulting in\nunsatisfactory generation effects on Action and Color. Nonethe-\nless, the negative impact of ControlNet (seg) can be alleviated by\nSUR-adapter. (d) The SUR capability of pre-trained diffusion models\ncan be improved by employing Prompt Weighting and MultiDif-\nfusion, with further enhancement achievable through the use of\nSUR-adapter.\nAs shown in Fig. 5, while the extra added adapter helps enhance\nthe semantic understanding and reasoning abilities of diffusion\nmodels, adding additional parameters does not guarantee the preser-\nvation of the original image generation quality of the pre-trained\ndiffusion models, as the adapter and pre-trained models are not\ntrained simultaneously. However, our proposed SURD can be used\nto mitigate this issue by supplying high-quality images. In Table\n3, we demonstrate through multiple image quality metrics with\nT-tests and user preference study that SUR-adapter can maintain\nimage generation quality, meaning there is no significant differ-\nence between the image quality of SUR-adapter and the original\npre-trained diffusion model (P-value \u2265 0.05). Moreover, since these\nhigh-quality images of SURD also come from diffusion models, they\ndo not lead to the generation of images of higher quality than those\ngenerated by the pre-trained diffusion models in our method.\n6\nABLATION STUDY\nThe Analysis of LLMs. As introduced in Section 3.2, LLM (13B)\nhas 40 layers. The performance of LLM vectors with different layers\nis shown in the first two rows of Table 4. We find that in most\ncases, LLM vectors corresponding to the later layers are better.\nThis suggests that the high-level semantic features in the deeper\nlayers are more conducive to semantic distillation. Additionally,\nwe show in the last two rows of Table 4 the performance of LLMs\nwith different parameter sizes. Combining the analysis of Table 4,\n2, and 3, we find that there is no significant difference in diffusion\nmodel performance among LLMs with different parameter sizes.\nAlthough existing work suggests that models with larger parameter\nsizes have stronger SUR abilities, existing SUR-adapter may only\nbe able to transfer limited semantic knowledge from LLMs.\nThe Knowledge Distillation of SUR-adapter. As shown in Table\n5, we conduct ablation studies on the knowledge distillation of LLM\nrepresented by the green line and complex prompts represented by\nthe purple line in Fig. 5. Distilling the knowledge of LLM or complex\nprompts alone improves the SUR capability of SUR-adapter, and the\neffect of knowledge distillation based on LLM is stronger than that\nbased on complex prompts. Furthermore, distilling the knowledge\nof both can further enhance the performance of SUR-adapter.\nTable 5: Ablation study on the knowledge distillation of SUR-\nadapter.\nLLM\nComplex\nPrompts\nBRISQUE\nAction (%)\nColor (%)\nNumber (%)\n13.85\n75.33\n81.33\n14.67\n\u2713\n13.97\n78.67\n84.00\n34.67\n\u2713\n12.31\n74.00\n86.67\n32.00\n\u2713\n\u2713\n14.78\n80.67\n87.33\n36.67\n7\nLIMITATIONS\nAs shown in Table 2, SUR-adapter has limited capacity to improve\ndiffusion models and cannot completely address the SUR issue. For\ninstance, after improvement, the Counting of DM (1.5), LLM (13B)\nis only increased by 36.67%. However, addressing the deficiency of\nSUR may require a large-scale multimodal dataset to optimize the\ntext encoder of diffusion models, which is a costly and challenging\ntask. Moreover, as highlighted in Section 6, there is no significant\ndifference in performance among LLMs of different parameter sizes\nafter distilling, indicating that SUR-adapter can only transfer limited\nsemantic knowledge from LLMs due to factors such as parameter\nlimitations. Hence, further enhancements are necessary for SUR-\nadapter to more effectively distill semantic information from LLMs.\n8\nCONCLUSION\nIn this paper, we uncover the limitations of existing pre-trained\ndiffusion models in terms of their ability to comprehend seman-\ntics and engage in commonsense reasoning when presented with\nsimple narrative prompts as inputs, leading to suboptimal image\ngeneration. To mitigate this issue, we introduce a new dataset called\nSURD, which comprises over 57,000 semantically corrected image-\ntext pairs, and the SUR-adapter module that can distill semantic\nunderstanding and reasoning knowledge from complex keyword-\nbased prompts and large language models. Extensive experiments\nand rigorous evaluations conducted on SURD demonstrate that\nSUR-adapter can enhance the semantic understanding of diffusion\nmodels without compromising image generation quality.\nACKNOWLEDGMENTS\nThis work was supported in part by National Natural Science\nFoundation of China (NSFC) under Grant No.62206314 and Grant\nNo.U1711264, GuangDong Basic and Applied Basic Research Foun-\ndation under Grant No.2022A1515011835, China Postdoctoral Sci-\nence Foundation funded project under Grant No.2021M703687.\nSUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models\nPreprint, Technical Report,\nREFERENCES\n[1] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence\nZitnick, Devi Parikh, and Dhruv Batra. 2015. VQA: Visual Question Answering.\nInternational Journal of Computer Vision 123 (2015), 4\u201331.\n[2] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shiliang Pu, Yaole Wang, Gang\nYue, Yue Cao, Hang Su, and Jun Zhu. 2023. One Transformer Fits All Distributions\nin Multi-Modal Diffusion at Scale. ArXiv abs/2303.06555 (2023).\n[3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. 2023.\nMultiDiffu-\nsion: Fusing Diffusion Paths for Controlled Image Generation. arXiv preprint\narXiv:2302.08113 2 (2023).\n[4] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and\nArtem Babenko. 2021. Label-Efficient Semantic Segmentation with Diffusion\nModels. ArXiv abs/2112.03126 (2021).\n[5] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schonlieb, and Christian Etmann.\n2021. Conditional Image Generation with Score-Based Diffusion Models.\n[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Pra-\nfulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon\nChild, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020. Language Models are Few-Shot Learners. ArXiv\nabs/2005.14165 (2020).\n[7] Long Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, and Yueting Zhuang.\n2020. Counterfactual Samples Synthesizing for Robust Visual Question Answer-\ning. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n(2020), 10797\u201310806.\n[8] Long Chen, Yuhang Zheng, and Jun Xiao. 2022. Rethinking Data Augmentation\nfor Robust Visual Question Answering. ArXiv abs/2207.08739 (2022).\n[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua\nMaynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar\nPrabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke,\nAnselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ip-\npolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan\nSepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanu-\nmalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S.\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPaLM: Scaling Language Modeling with Pathways. ArXiv abs/2204.02311 (2022).\n[10] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova. 2018. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding. (2018).\n[11] Wanshu Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng, Lu Yuan, and Yu-\nChiang Frank Wang. 2022. Frido: Feature Pyramid Diffusion for Complex Scene\nImage Synthesis. ArXiv abs/2208.13753 (2022).\n[12] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. 2022.\nDiffusion models as plug-and-play priors. ArXiv abs/2206.09012 (2022).\n[13] Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li,\nDacheng Tao, and Steven CH Hoi. 2022. From Images to Textual Prompts: Zero-\nshot VQA with Frozen Large Language Models. arXiv preprint arXiv:2212.10846\n(2022).\n[14] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual Learning\nfor Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) (2015), 770\u2013778.\n[15] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Frederick Diamos,\nHeewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and\nYanqi Zhou. 2017. Deep Learning Scaling is Predictable, Empirically. ArXiv\nabs/1712.00409 (2017).\n[16] Jonathan Ho, Ajay Jain, and P. Abbeel. 2020. Denoising Diffusion Probabilistic\nModels. ArXiv abs/2006.11239 (2020).\n[17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den\nDriessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich\nElsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. 2022. Training Compute-Optimal\nLarge Language Models. ArXiv abs/2203.15556 (2022).\n[18] Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. 2022. Improv-\ning Sample Quality of Diffusion Models Using Self-Attention Guidance. arXiv\npreprint arXiv:2210.00939 (2022).\n[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large\nlanguage models. arXiv preprint arXiv:2106.09685 (2021).\n[20] Zhongzhan Huang, Senwei Liang, Mingfu Liang, and Haizhao Yang. 2019. DI-\nANet: Dense-and-Implicit Attention Network. In AAAI Conference on Artificial\nIntelligence.\n[21] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y. Wu. 2016. Exploring\nthe Limits of Language Modeling.\n[22] Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. 2020. Scaling\nLaws for Neural Language Models. ArXiv abs/2001.08361 (2020).\n[23] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Hui-Tang Chang, Tali Dekel,\nInbar Mosseri, and Michal Irani. 2022. Imagic: Text-Based Real Image Editing\nwith Diffusion Models. ArXiv abs/2210.09276 (2022).\n[24] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. 2021. Musiq:\nMulti-scale image quality transformer. In Proceedings of the IEEE/CVF International\nConference on Computer Vision. 5148\u20135157.\n[25] Gwanghyun Kim, Taesung Kwon, and Jong-Chul Ye. 2021. DiffusionCLIP: Text-\nGuided Diffusion Models for Robust Image Manipulation. 2022 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR) (2021), 2416\u20132425.\n[26] Solomon Kullback and Richard A Leibler. 1951. On information and sufficiency.\nThe annals of mathematical statistics 22, 1 (1951), 79\u201386.\n[27] Haoying Li, Yifan Yang, Meng Chang, Huajun Feng, Zhi hai Xu, Qi Li, and Yue ting\nChen. 2021. SRDiff: Single Image Super-Resolution with Diffusion Probabilistic\nModels. Neurocomputing 479 (2021), 47\u201359.\n[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping\nlanguage-image pre-training for unified vision-language understanding and\ngeneration. In International Conference on Machine Learning. PMLR, 12888\u201312900.\n[29] Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. 2021. Jurassic-1: Technical\ndetails and evaluation. White Paper. AI21 Labs 1 (2021).\n[30] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common\nObjects in Context. In European Conference on Computer Vision.\n[31] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Fang Yang, and Xiao-Ming Wu. 2021.\nSlake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual\nQuestion Answering. 2021 IEEE 18th International Symposium on Biomedical\nImaging (ISBI) (2021), 1650\u20131654.\n[32] Andreas Lugmayr, Martin Danelljan, Andr\u00e9s Romero, Fisher Yu, Radu Timo-\nfte, and Luc Van Gool. 2022. RePaint: Inpainting using Denoising Diffusion\nProbabilistic Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) (2022), 11451\u201311461.\n[33] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. 2012.\nNo-\nreference image quality assessment in the spatial domain. IEEE Transactions on\nimage processing 21, 12 (2012), 4695\u20134708.\n[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from natural language supervision.\nIn International conference on machine learning. PMLR, 8748\u20138763.\n[35] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language Models are Unsupervised Multitask Learners.\n[36] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,\nFrancis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,\nEliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,\nGeorge van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang,\nAmelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan\nUesato, John F. J. Mellor, Irina Higgins, Antonia Creswell, Nathan McAleese, Amy\nWu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme\nSutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lor-\nraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic\nDonato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, N. K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias\nPohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume, Yujia Li, Tay-\nfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas,\nAurelia Guy, Chris Jones, James Bradbury, Matthew G. Johnson, Blake A. Hecht-\nman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon\nOsindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stan-\nway, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.\n2021. Scaling Language Models: Methods, Analysis & Insights from Training\nGopher. ArXiv abs/2112.11446 (2021).\n[37] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the Limits of\nTransfer Learning with a Unified Text-to-Text Transformer. ArXiv abs/1910.10683\n(2019).\n[38] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\n2021. High-Resolution Image Synthesis with Latent Diffusion Models. 2022\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021),\n10674\u201310685.\n[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\nOmmer. 2022. High-Resolution Image Synthesis With Latent Diffusion Models. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR). 10684\u201310695.\n[40] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. 2019.\nA Constructive Prediction of the Generalization Error Across Scales. ArXiv\nPreprint, Technical Report,\nShanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, & Liang Lin\nabs/1909.12673 (2019).\n[41] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and\nKfir Aberman. 2022. DreamBooth: Fine Tuning Text-to-Image Diffusion Models\nfor Subject-Driven Generation. ArXiv abs/2208.12242 (2022).\n[42] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and\nMohammad Norouzi. 2021. Image Super-Resolution via Iterative Refinement.\nIEEE Transactions on Pattern Analysis and Machine Intelligence 45 (2021), 4713\u2013\n4726.\n[43] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,\nand Bryan Catanzaro. 2019. Megatron-LM: Training Multi-Billion Parameter\nLanguage Models Using Model Parallelism. ArXiv abs/1909.08053 (2019).\n[44] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam\nRajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,\nVijay Anand Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi,\nJulie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston,\nSaurabh Tiwary, and Bryan Catanzaro. 2022. Using DeepSpeed and Megatron to\nTrain Megatron-Turing NLG 530B, A Large-Scale Generative Language Model.\nArXiv abs/2201.11990 (2022).\n[45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[46] Dani Valevski, Matan Kalman, Y. Matias, and Yaniv Leviathan. 2022. UniTune:\nText-Driven Image Editing by Fine Tuning an Image Generation Model on a\nSingle Image. ArXiv abs/2210.09477 (2022).\n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[48] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. 2022. Exploring CLIP for\nAssessing the Look and Feel of Images. arXiv preprint arXiv:2207.12396 (2022).\n[49] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai\nhsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William\nFedus. 2022. Emergent Abilities of Large Language Models. ArXiv abs/2206.07682\n(2022).\n[50] Ling Yang, Zhilong Zhang, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia\nShao, Wentao Zhang, Ming-Hsuan Yang, and Bin Cui. 2022. Diffusion Models:\nA Comprehensive Survey of Methods and Applications. ArXiv abs/2209.00796\n(2022).\n[51] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. 2022. Diffusion Probabilis-\ntic Modeling for Video Generation. ArXiv abs/2203.09481 (2022).\n[52] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding,\nZhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,\nYufei Xue, Jidong Zhai, Wenguang Chen, P. Zhang, Yuxiao Dong, and Jie Tang.\n2022. GLM-130B: An Open Bilingual Pre-trained Model. ArXiv abs/2210.02414\n(2022).\n[53] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In-So Kweon. 2023.\nText-to-image Diffusion Models in Generative AI: A Survey. ArXiv abs/2303.07909\n(2023).\n[54] Lvmin Zhang and Maneesh Agrawala. 2023. Adding Conditional Control to\nText-to-Image Diffusion Models. ArXiv abs/2302.05543 (2023).\n[55] Lvmin Zhang and Maneesh Agrawala. 2023. Adding Conditional Control to\nText-to-Image Diffusion Models. arXiv:2302.05543 [cs.CV]\n[56] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo,\nLei Yang, and Ziwei Liu. 2022. MotionDiffuse: Text-Driven Human Motion\nGeneration with Diffusion Model. ArXiv abs/2208.15001 (2022).\n[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui\nChen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,\nMyle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali\nSridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained\nTransformer Language Models. ArXiv abs/2205.01068 (2022).\n[58] Shan Zhong, Wushao Wen, and Jinghui Qin. 2023. SPEM: Self-adaptive Pooling\nEnhanced Attention Module for Image Recognition. In Conference on Multimedia\nModeling.\nSUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models\nPreprint, Technical Report,\nA\nSUPPLEMENTAL DATASET INFORMATION\nA.1\nPre-trained Models\nBLIP. We utilize the BLIP (Bootstrapping Language-Image Pre-\ntraining) [13, 28] model to generate simple narrative prompts of\nimages for SURD. Specifically, we employ the BLIP caption base\nmodel, which has been fine-tuned on the MSCOCO [30] captioning\ndataset, using the load function provided in the official documenta-\ntion 4.\nCLIP. We utilize CLIP to ensure the correctness of both simple\nnarrative prompts and complex keyword-based prompts. Specif-\nically, we designed a data cleaning process, which is briefly de-\nscribed in Section 3.1 of the main text. We leverage the semantic\nsimilarity between images and prompts by asking CLIP to classify\nbetween simple and complex prompts, where the goal is to select\nthe prompts that best match the semantics of the images. Typically,\ncomplex prompts contain semantically irrelevant information, such\nas image quality descriptions, and therefore, semantically correct\nsimple prompts generally achieve higher CLIP scores than complex\nprompts. We retain a sample if the CLIP score of the corresponding\nsimple prompt is not lower than that of the complex prompt. We\nuse the publicly available pre-trained CLIP model, which has a\nViT-B/32 architecture, and load it using the function provided in\nthe official documentation 5.\nLLMs. In this paper, we utilize LLaMA [45], a collection of foun-\ndation language models ranging from 7B to 65B parameters, as\nknowledge distillation for large language models (LLMs). Specifi-\ncally, we save the vector representations of simple prompts in LLMs,\nwhich serve as the text understanding to finetune diffusion models.\nThe details of the LLMs used in our experiments, including the\nnumber of parameters, vector dimensions, and model structures,\nare shown in Table 6.\nTable 6: Model sizes and architectures of LLMs used in the\nmain text.\nLLM\nparams\ndimension\nn heads\nn layers\n7B\n6.7B\n4096\n32\n32\n13B\n13.0B\n5120\n40\n40\n33B\n32.5B\n6656\n52\n60\nA.2\nImpact and Ethics\nImpact and Usage. Improving the SUR ability of diffusion mod-\nels is an important issue that has received limited attention in the\nresearch community. In this paper, we approach this problem from\na novel perspective by constructing a semantically correct dataset,\nSURD, and using knowledge distillation to transfer semantic knowl-\nedge from complex prompts and LLM. SURD can not only be used\nto finetune diffusion models for solving SUR problems but can also\nbe directly used as a training dataset for diffusion models due to its\nensured semantic correctness.\nSocial Ethics. Unlike many multimodal datasets in the natural\ndomain, SURD is entirely built on data generated by DNNs. As\na result, it is less likely to be used in surveillance systems that\ncould potentially violate people\u2019s privacy. Moreover, during the\n4https://github.com/salesforce/LAVIS\n5https://github.com/openai/CLIP\ndata cleaning, a manual inspection stage ensures that SURD does\nnot contain any sensitive personal information, such as gender\nand race, nor does it include data that could exacerbate biases\ntowards underrepresented communities. Therefore, upon careful\nexamination of our dataset, we believe that it is unlikely to be used\nto directly harm individuals.\nFigure 6: Loss value during the training of SUR-adapter with\ndifferent initializations. The mathematical symbols corre-\nspond to Eq.(10).\nB\nSUPPLEMENTAL EXPERIMENTS\nB.1\nSupplemental Implementation details\nIn our study, we validate the universality of SUR-adapter with two\npre-trained diffusion models, three LLMs with different parameters,\nand various controlled methods. Unless otherwise specified, we\nfollow the settings of [3, 18, 39, 45, 55]. Specifically, all models are\ntrained on one Nvidia RTX 3090 GPU, with step set to 5000, batch\nsize set to 16, and resolution set to 512. During training, we apply\nmixed precision and standard data augmentation techniques such\nas normalization, center cropping, and horizontal flipping. The\nlearning rate and hyper-parameters in Eq.(7) and Eq.(10) are set to\n1e-5.\nAll control methods utilize the default settings of diffusers 6. Be-\nsides, we manually curated a set of images that satisfy the semantic\nrequirements. These images serve as conditional inputs for Con-\ntrolNet (canny) and ControlNet (seg). The setting of MultiDiffusion\nis that the pretrained model for DM (1.5) uses the schedulers of DM\n(cartoon), and vice versa, the pretrained model for DM (cartoon)\nuses the schedulers of DM (1.5).\nB.2\nThe Initiation of SUR-adapter\nAs shown in Fig. 5, we use a fully connected network to connect the\nadapter and the backbone. To ensure stable training of the adapter,\nwe initialize the FCN with 0, following some well-known adapter-\nrelated works [19, 54]. Additionally, as shown in Fig. 6, we also\ndemonstrate the impact of different initialization methods on the\nloss of SUR-adapter. We observe that different initializations have\n6https://github.com/huggingface/diffusers\nPreprint, Technical Report,\nShanshan Zhong, Zhongzhan Huang, WushaoWen, Jinghui Qin, & Liang Lin\nTable 7: Evaluation of semantic accuracy (Acc.) in images generated by simple prompts using diffusion models. The simple\nprompts consisted of three types of sentences, including \"counting\", \"color\", and \"action\". Each prompt generated 130 images,\nand the images were manually checked for semantic accuracy.\nType\nPrompt\nAccuracy\nAccuracy (Ours)\nCounting\nFour freshly baked pies.\n63.08%\n73.85%\nSix colorful hot air balloons floating over a picturesque landscape.\n8.46%\n41.54%\nSeven vintage glass bottles.\n0.00%\n36.92%\nColor\nA vibrant red sports car speeding down a winding road.\n86.15%\n93.85%\nThe blue glass containing red juice.\n17.69%\n20.00%\nA couple wearing blue and yellow solid color clothes respectively.\n0.00%\n6.92%\nAction\nSomeone shooting a basketball on the sports field.\n41.54%\n56.92%\nGiraffes eating trees.\n25.38%\n50.77%\nA chef tossing a pizza dough in the air in a kitchen.\n15.38%\n32.31%\nTable 8: Examples of testing prompts.\nType\nPrompt\nAction\nA gymnast performing a balance beam routine with graceful flips and twists.\nA skateboarder doing a kickflip over a set of stairs.\nA diver swimming underwater with colorful fish and coral all around him.\nColor\nA golden sun setting over a calm ocean, with orange and pink hues appearing in the sky.\nA tranquil scene of a meadow filled with wildflowers in shades of purple, pink, and yellow.\nA funky and retro diner with a color scheme of bright pink, teal, and silver.\nCounting\nA set of four antique teacups and saucers with intricate floral designs.\nFive different types of fresh fruit cut into slices and arranged on a platter.\nSeven colorful beach umbrellas on a sandy beach.\nFigure 7: Title, description, and some questions of the user\npreference study.\nlittle impact on \u2113LLM(\ud835\udf19) in Eq.(10), but have a significant effect on\nthe training of \u2113CP(\ud835\udf19) and the diffusion model, which is consistent\nwith existing works [19, 54].\nB.3\nAccuracy of SUR-adapter in Table 1\nWe have provided additional information on the semantic accuracy\nof SUR-adapter prompts in Table 7, which supplements the prompt\nexamples shown in Table 1 of the Introduction.\nB.4\nUser Preference Study\nIn this paper, there are two metrics that require manual judgment.\nOne is the semantic accuracy of the generated images (action, color,\ncounting), which is an objective metric. Therefore, it can be easily\nassessed and counted by the authors. The other metric that requires\nmanual judgment is user preference, as shown in Table 3. This\nmetric is subjective. To gather data for this metric, we collected a\ntotal of 89 valid questionnaires (an example of the questionnaire\nis provided in Fig. 7). We randomly presented images generated\nby our method and baselines to the participants and asked them\nto select a picture that they deemed of better quality based on the\nquestion, \"Which of the following pictures do you think is of better\nquality?\" Finally, based on the 89 questionnaires, we compiled and\nanalyzed the data.\nB.5\nTesting Prompts\nTo evaluate Semantic Understanding and Reasoning (SUR), we have\ndivided the semantics into three main types, namely Action, Color,\nand Counting, with each type having fifteen prompts whose exam-\nples are shown in Table 8. For each prompt, we generate ten images\nduring testing.\n"
  }
]