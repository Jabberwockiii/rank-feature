[
  {
    "title": "VecFusion: Vector Font Generation with Diffusion",
    "link": "https://arxiv.org/pdf/2312.10540.pdf",
    "upvote": "20",
    "text": "VecFusion: Vector Font Generation with Diffusion\nVikas Thamizharasan*1,2\nDifan Liu*2\nShantanu Agarwal1\nMatthew Fisher2\nMicha\u00a8el Gharbi2\nOliver Wang3\nAlec Jacobson2,4\nEvangelos Kalogerakis1\nUniversity of Massachusetts Amherst1\nAdobe Research2\nGoogle Research3\nUniversity of Toronto4\n(a) Missing Glyph Generation \n(b) Few-shot Vector Glyph Generation \nSynthesized Vector Glyphs\nInput Raster Images\nFigure 1. We present VecFusion, a generative model for vector fonts. (a) VecFusion generates missing glyphs in incomplete fonts. Blue\nglyphs are glyphs that exist in the fonts. Red glyphs are missing glyphs generated by our method. On the right, we show generated control\npoints as circles on selected glyphs. (b) VecFusion generates vector glyphs given a few exemplar (raster) images of glyphs. Our method\ngenerates precise, editable vector fonts whose geometry and control points are learned to match the target font style.\nAbstract\nWe present VecFusion, a new neural architecture that\ncan generate vector fonts with varying topological struc-\ntures and precise control point positions. Our approach is\na cascaded diffusion model which consists of a raster diffu-\nsion model followed by a vector diffusion model. The raster\nmodel generates low-resolution, rasterized fonts with aux-\niliary control point information, capturing the global style\nand shape of the font, while the vector model synthesizes\nvector fonts conditioned on the low-resolution raster fonts\nfrom the first stage. To synthesize long and complex curves,\nour vector diffusion model uses a transformer architecture\nand a novel vector representation that enables the model-\ning of diverse vector geometry and the precise prediction\nof control points. Our experiments show that, in contrast\nto previous generative models for vector graphics, our new\ncascaded vector diffusion model generates higher quality\nvector fonts, with complex structures and diverse styles.\n1. Introduction\nVector fonts are extensively used in graphic design, arts,\npublishing, and motion graphics. As opposed to rasterized\nfonts, vector fonts can be rendered at any resolution with-\nout quality degradation, and can be edited via intuitive con-\ntrol point manipulation. However, authoring high-quality\nvector fonts remains a challenging and labor-intensive task,\neven for expert designers. Recent approaches [6, 27, 46]\nuse VAEs or autoregressive models to automatically synthe-\nsize vector fonts, but they often struggle to capture a diverse\nrange of topological structures and glyph variations, due to\nthe inherent ambiguity of vector curves. As a result, they\nfrequently create artifacts and imprecise control point po-\nsitions, compromising the overall quality and editability of\nthe synthesized fonts.\nIn this work, we leverage recent advances in raster\ngenerative models, to design a generative model for vec-\n\u2217Equal contribution\n1\narXiv:2312.10540v1  [cs.CV]  16 Dec 2023\ntor fonts. Such a generative model has a number of real\nworld applications, such as glyph completion, few-shot\nstyle transfer, and font style interpolation. However, train-\ning vector domain generative models is not straightforward:\nthe irregular data structure of vector graphics prevents naive\napplications of commonly used CNN-based architectures.\nFurthermore, there is an inherent ambiguity in vector rep-\nresentations: infinitely many control points configurations\ncan produce the same glyph, but not all configurations are\nequivalent. In particular, designers carefully create control\npoints so that the font can be intuitively edited. Generated\nvector fonts should follow a similar design goal.\nTo address the above-mentioned challenges, we propose\na novel two-stage diffusion model, called VecFusion, to\ngenerate high-quality vector fonts. Our pipeline is a cas-\ncade of a raster diffusion model followed by a vector dif-\nfusion model. The raster diffusion model gradually trans-\nforms a 2D Gaussian noise map to a target raster image of\nthe glyph at low resolution, conditioned on a target glyph\nidentifier and a target font style. It also generates auxiliary\nraster fields to drive the placement of vector control points\nin the next stage. The second stage is a vector diffusion\nmodel conditioned on the raster outputs from the first stage.\nIt is trained to \u201cdenoise\u201d a noisy vector glyph representation\ninto structured curves representing the glyph.\nContributions.\nThis papers makes several contributions.\nFirst, we present a novel two-stage cascaded diffusion\nmodel for high-quality vector fonts generation. This cas-\ncading process allows us to effectively \u201cupsample\u201d low-\nresolution raster outputs into a vector representation. We\nintroduce a new mixed discrete-continuous representation\nfor control points, which allows the vector diffusion model\nto automatically predict the number of control points and\npaths to use for a glyph, as well as their position. We show\nthat diffusion models can effectively \u201cdenoise\u201d in this new\nrepresentation space. Moreover, to capture long-range de-\npendencies and accommodate the irregular nature of vec-\ntor representations, we introduce a transformer-based vec-\ntor diffusion model. Finally, we show that VecFusion syn-\nthesizes fonts with much higher fidelity than state-of-the-art\nmethods evaluated on datasets with diverse font styles.\n2. Related Work\nGenerative vector graphics.\nSignificant work has been\ninvested in generative modeling of vector graphics, us-\ning VAEs [21, 27], sequence-to-sequence models like\nRNNs [14] or transformers [36]. Recent approaches em-\nploy hierachical generative models [6], while others bypass\nthe need for direct vector supervision [35], using a differen-\ntiable rasterizer [24].\nFont generation.\nDue to their ubiquity and central role\nin design, fonts have received special attention and dedi-\ncated synthesis methods. Many methods learn to generate\nraster fonts from a large set of reference glyphs [12, 19]\nor a few exemplar images [1, 7, 13, 22, 33, 40]. These\nmethods produce visually appealing raster fonts in a vari-\nety of styles, but cannot generate vector outputs, thus they\nare limited by resolution and pixelization artifacts. In the\ntask of vector font generation, early approaches use mor-\nphable template models [39], or manifold learning to enable\ninterpolation/extrapolation of existing fonts [5], while re-\ncent methods use deep generative models [25, 45]. The first\ngeneration of deep learning solution sometimes generated\nglyphs with strong distortion and visual artifacts. Methods\nlike DeepVecFont-v2 [46] improve the synthesis quality us-\ning a transformer architecture. Although these methods can\ngenerate visually pleasing vector fonts, effectively model-\ning a diverse distribution of glyphs and topologies remains\na challenge. DeepVecFont-v2, for instance, only supports a\nlimited number of glyphs (52 characters).\nDiffusion models.\nTo address the challenges in vector\nfield design, we leverage diffusion models [15] for their\nability to model diverse and complex data distributions. Un-\nlike previous methods [9, 43] that use CNN or RNN-based\nvector diffusion models, our approach uses a transformer-\nbased vector diffusion model to handle long-range depen-\ndencies inherent to complex vector glyphs. Furthermore,\nour two-stage raster\u2013vector approach and novel vector rep-\nresentation enable precise B\u00b4ezier curve prediction on chal-\nlenging artist-designed font datasets.\nCascaded diffusion.\nCascaded diffusion models [17]\nhave achieved impressive synthesis quality across various\ndomains, including images [2, 38], videos [4, 16] and\n3D [18, 26]. In the same spirit, we introduce a cascaded\ndiffusion model for high-quality vector font generation.\nImage vectorization.\nImage vectorization approaches [3,\n29, 41] output a vector graphics representation from a raster\nimage.\nDedicated for line drawing vectorization, many\nlearning-based methods [20, 31, 34] have been proposed.\nAlthough these methods can produce high-quality vector\ngraphics, they often create redundant or imprecise con-\ntrol points and fail to produce high-fidelity results on low-\nresolution raster images. Our diffusion model can generate\nprecise vector geometry from low-resolution raster images,\nalso providing a new perspective for image vectorization.\n3. Method\nOverview.\nThe goal of VecFusion is to automatically gen-\nerate vector graphics representations of glyphs. The input\nto our model is the Unicode identifier for a target character,\n2\n1\n2\n3\n4\n56\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nChar name\nChar look-up table\nFont look-up table\nFont style encoding \nChar embedding\n xT\n xt\n x0\n yT\n yt\n y0\nFont name\nReference raster glyphs\nFont conditioning\ncross-attn\ncross-attn\ncross-attn\nEncoding\nFigure 2. Overview of the VecFusion\u2019s cascade diffusion pipeline.\nGiven a target character and font conditioning, our raster diffusion\nstage (\u201cRaster-DM\u201d) produces a raster image representation of the\ntarget glyph in a series of denoising steps starting with a noise\nimage. The raster image is encoded and input to our vector diffu-\nsion stage (\u201cVector-DM\u201d) via cross-attention. The vector diffusion\nstage produces the final vector representation of the glyph also in a\nseries of denoising steps starting with a noise curve representation.\nalso known as code point, and a target font style. The target\nfont style can be specified in the form of a few representa-\ntive raster images of other glyphs in that style or simply by\nthe font style name. Figure 2 shows an example of the gen-\nerated vector representation for the glyph corresponding to\nthe input target letter \u201csha\u201d of the Devanagari alphabet and\nthe target font style \u201cMukta\u201d. Our method is trained once\non a large dataset of glyphs from various font styles. Once\ntrained, it can generate glyphs not observed during train-\ning. Our model has several applications: generate missing\nglyphs in incomplete fonts, synthesize novel fonts by trans-\nferring the style of a few exemplar images of glyphs, or\ninterpolate font styles.\nOutput vector representation.\nThe generated vector rep-\nresentation for a glyph is in the form of ordered sequences\nof control points in cubic B\u00b4ezier curve paths commonly\nused in vector graphics. Control points can be repeated in\nthe generated sequences to manipulate the continuity of the\nvector path. Our method learns to generate an appropri-\nate number of vector paths, control points, and point repeti-\ntions tailored to each character and font style. In addition, it\nlearns the proper ordering of control points for each path, in-\ncluding where first and last control points are placed, since\ntheir placement patterns often reflect artist\u2019s preferences.\nPipeline.\nOur method consists of a two-stage cascade. In\nthe first stage (raster diffusion model, or in short \u201cRaster-\nDM\u201d, Figure 2), conditioned on the target character code\npoint and font style, our method initiates a reverse diffu-\nsion process to generate a raster image.\nThe generated\nraster image captures the shape and style of the target glyph\nat low resolution. Additionally, we generate an auxiliary\nset of control point fields encoding information for con-\ntrol point location, multiplicity, and ordering. In the sec-\nond stage (vector diffusion model or \u201cVector-DM\u201d, Figure\n2), our method proceeds by synthesizing the vector format\ncapturing fine-grained placement of control points guided\nby the raster glyph and control point fields generated in the\nfirst stage. We observed that this two-stage approach results\nin generating higher-fidelity fonts compared to using diffu-\nsion in vector space directly or without any guidance from\nour control point fields. In the next sections, we discuss our\nraster and vector diffusion stages in more detail.\n3.1. Raster diffusion stage\nGiven a target character identifier and font style, the raster\ndiffusion stage creates a raster image x0 encoding infor-\nmation about the target glyph in pixel space (Figure 2,\n\u201cRaster-DM\u201d). This is performed through a diffusion model\nthat gradually transforms an image xT sampled from a unit\nGaussian noise distribution towards the target raster image\nx0 in a series of T denoising steps. At each step t = 1...T,\na trained neural network executes the transition xt \u2192 xt\u22121\nby predicting the noise content to be removed from the im-\nage xt. This denoiser network is conditioned on the input\ncharacter identifier and font style. In the next paragraphs,\nwe explain the encoding of the input character codepoint\nand font style, the target raster image, the denoiser network,\nand finally the training and inference processes of this stage.\nCharacter identifier embeddings.\nInspired by similar\napproaches in NLP to represent words [42], we create a\none-hot vector representation for all unique character code-\npoints available in our dataset. Given a target character\u2019s\ncodepoint, its one-hot vector representation is mapped to\na continuous embedding g through a learned look-up ta-\nble. The look-up table stores embeddings for all codepoints\navailable in our dataset and retrieves them using the one-hot\nvector as indices.\nFont style conditioning.\nTo encode the font style, we ex-\nperimented with two approaches depending on the appli-\ncation.\nTo generate missing glyphs in incomplete fonts,\nwe create a one-hot vector representation for all font styles\navailable in our dataset. Given a target font style, its one-\nhot vector is mapped to a continuous embedding f through\na learned look-up table as above. To generate glyphs condi-\ntioned on a few exemplar images, we concatenate the input\nimages channel-wise and pass them through a convnet to\nget a font style feature map f (see Appendix section: 5).\nTarget raster image.\nThe target x0 produced in the raster\ndiffusion stage is a N \u00d7 N image made of the following\nchannels:\n3\nRaster glyph\nControl point feld\nFigure 3. Tar-\nget x0\n(a) the first channel is composed of an image\nrepresenting a grayscale rasterized image of\nthe target glyph (Figure 3, top).\n(b) the rest of the channels store control\npoint fields (Figure 3, bottom), whose goal\nis to encode information about the control\npoint location, multiplicity, and ordering.\nDuring training, this control point field is\ncreated by rendering each control point as\na Gaussian blob centered at the 2D coordi-\nnates (x,y) of the control point. The coordi-\nnates are normalized in [0, 1]2. We also modulate the color\nof the blob based on (a) the index of the control point in\nthe sequence of control points of its vector path (e.g., first,\nsecond, third etc control point), and (b) its multiplicity. A\nlook-up function is used to translate the ordering indices\nand multiplicities of control points to color intensities. In\nour implementation, we use 3 channels for this control point\nfield, which can practically be visualized as an RGB image\n(Figure 3, bottom). These channels are concatenated with\nthe raster image of the glyph, forming a 4-channel image.\nRaster denoiser.\nThe denoiser is formulated as a UNet ar-\nchitecture [11]. The network takes the 4-channel image xt\nas input and is conditioned on the embedding of time step\nt. Following [37], we add the character\u2019s codepoint embed-\nding g to time step embedding and pass it to each residual\nblock in the UNet. For the font style conditioning, we add\nit to the time step embedding if it is a single embedding. If\nthe font style is encoded as a spatial feature map, following\n[37], we flatten the feature map and inject it to the UNet via\ncross-attention.\nThe denoiser network predicts the per-channel noise\ncomponent of the input image, which is also a 4-channel\nimage (see Appendix section: 5).\nTraining loss\nThe network is trained to approximate\nan optimal denoiser under the condition that the im-\nages x1, x2, ...xT are created by progressively adding\nGaussian noise to the image of the previous step [15]:\nq(xt|xt\u22121) = N\n\u0000xt;\np\n(1 \u2212 \u03b2t)xt\u22121, \u03b2tI\n\u0001\n, where \u03b2t rep-\nresents the variance of the Gaussian noise added at each\nstep. The image xT converges to a unit Gaussian distri-\nbution as T \u2192 \u221e, or practically a large number of steps\n[15]. Following [15], we train the denoiser network with\nthe training objective ||\u03f5(xt, t, f, g) \u2212 \u03f5||2 i.e. the mean-\nsquared error loss between the added training noise \u03f5 at each\nstep and the predicted noise \u03f5(xt, t, f, g) from the network.\nThe loss is used to train the denoiser and the look-up tables.\nInference.\nAt test time, given sampled unit Gaussian\nnoise xT , a target character embedding g and font style con-\nditioning f, the network is successively applied in T steps\nto generate the target raster image.\nImplementation details.\nIn all our experiments, we use\nthe following hyperparameters. Following [32], we set the\nnumber of diffusion steps T to 1000 and used cosine noise\nschedule in the forward diffusion process.\nWe used the\nAdamW optimizer [28] with learning rate 3.24 \u00b7 10\u22125. The\nfeature embeddings for character identifiers are set to be\n896-dimensional. The control points are rendered as Gaus-\nsian blobs with radius of 2 pixels. The raster image resolu-\ntion is set to 64 \u00d7 64. Lower resolutions cause increasing\noverlaps between the rendered blobs, making the control\npoint field more ambiguous. Increasing the resolution in-\ncreases the computational overhead for the raster denoiser.\nThe above resolution represented a good trade-off, as we\npractically found in our experiments. As mentioned above,\nwe use 3 channels to encode control point ordering and mul-\ntiplicity as colors. We practically observed that 3 channels\nwere enough to guide the vector diffusion stage. Depending\non the dataset, fewer channels could be used instead e.g.,\nin cases of glyphs with few control points, or no multiplic-\nities. In general, these hyperparameters can be adjusted for\ndifferent vector graphics tasks \u2013 our main point is that the\nraster images and fields are useful as guidance to produce\nhigh-fidelity vector fonts, as shown in our ablation.\n3.2. Vector diffusion stage\nGiven the raster image generated in the previous stage, the\nvector diffusion stage creates a tensor y0 representing the\ntarget glyph in vector graphics format (Figure 2 \u201cVector-\nDM\u201d). The reverse diffusion process gradually transforms a\nnoise tensor yT sampled from a unit Gaussian noise distri-\nbution towards a tensor y0 in a series of denoising steps. In\nthis domain, the noise represents noise on the spatial posi-\ntion and path membership of the control points, rather than\nthe intensity of the pixel values as in the raster domain. In\nthe next paragraphs, we explain the tensor representation,\nthe denoiser, training and inference of this stage.\nTarget tensor.\nThe target tensor y0 is a M \u00d7 D tensor\n(Figure 4), where M represents an upper bound to the to-\ntal number of control points a glyph can have. Each entry\nin the tensor contains a D-dimensional representation of a\ncontrol point. Specifically, each entry stores the following\ninformation:\n(a) the index of the vector path the control point belongs to\ni.e, its path membership. During training, each vector path\nis assigned a unique index. Since the vector paths can be\nre-ordered arbitrarily without changing the resulting glyph,\nto reduce unnecessary variability during learning, we lexi-\ngraphically sort vector paths using the coordinates of their\ncontrol point closest to the top-left corner of the glyph raster\n4\nVector glyph\nTarget representation\ngrid cell X index \npath index\n0.34 0.47\n0.15 0.20\n0.00 0.00\n0.00 0.00\n1\n1\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\nCP1\nCP2\nNULL\nNULL\nCP21\n0.03 0.50\n0\n1\n0\n1\n0\n1\n1\n1\n0\n0\n1\ngrid cell Y index \nFigure 4. Target tensor representation y0. Our vector diffusion\nmodel \u201cdenoises\u201d this tensor representation which includes both\npath membership and spatial position for control points. The dis-\ncrete values (path membership, grid cell coordinates) are denoised\nin the continuous domain and then discretized. The control point\nlocations are computed from the predicted grid cell coordinates\nplus continuous displacements (\u2206x, \u2206y) from them.\nimage as sorting keys. Following [8], the resulting sorted\npath index is converted to binary bits. For each control point\nentry, we store the binary bits of its vector path. A null en-\ntry (i.e., all-one bits) is reserved for entries that do not yield\ncontrol points \u2013 in this manner, we model vector fonts with\na varying number of control points and paths.\n(b) the index of the grid cell containing the control point.\nWe define a coarse P \u00d7 P grid over the image, with P 2\ncorresponding grid cell centroids. We assign each control\npoint to the grid cell that has the closest centroid. In case\nthe control point lies on the boundary of two cells, we use a\nround operation that assigns it in the second cell. Similar to\npath membership, the grid cell index is converted to binary\nbits. For each control point entry, we store the binary bits\nof its assigned grid cell.\n(c) the continuous coordinates of the control point ex-\npressed relative to the center of the grid cell it belongs to.\nThese are two continuous values capturing the location of\neach control point. We found that capturing control point\nlocations relative to cell centers achieves the best perfor-\nmance. Since the generated raster control point field (ap-\nproximately) highlights regions storing control points, map-\nping the control point field to discrete cell indices plus small\ncontinuous residuals, or displacements, is an easier task and\nreduces the continuous coordinate variability needed to be\ncaptured by the model.\nDenoiser.\nThe denoiser for this stage is formulated as an\nencoder-only transformer [10], which takes the tensor yt as\ninput and is conditioned on the embedding of time step t,\nand the generated raster image x0 from the raster diffusion\nmodel. We use a ConvNet to encode the raster image x0\ninto high-dimensional features, which are input to the trans-\nformer via cross-attention similar to [37]. The transformer\npredicts the noise content as a M \u00d7 D tensor at each step 5.\nTraining loss.\nWe train the denoiser network according\nto mean-squared error loss between training noise and pre-\ndicted one at sampled time steps: ||\u03f5(yt, x0, t) \u2212 \u03f5||2.\nInference.\nAt test time, given a sampled tensor yT from\nunit Gaussian noise and a generated raster image x0 of the\nprevious stage, the denoiser network is applied in a series of\nT steps to generate the target tensor y0. Following the Ana-\nlog Bits approach [8], the discrete binary bits in the target\ntensor representation are modeled as real numbers. These\nare simply thresholded to obtain the final binary bits at in-\nference time. Given the predicted path membership, we cre-\nate a set of vector paths according to the largest generated\ncontrol path index number. Each non-null entry in the gen-\nerated tensor yields a control point. The control points are\nimplicitly ordered based on their entry index. The location\nof the control point is defined as the coordinate center of\nthe assigned cell in the generated tensor plus the predicted\nrelative displacement. Given this generated information, we\ndirectly reconstruct the vector paths without requiring any\nfurther refinement or post-processing. 3\nImplementation details.\nIn our implementation, we set\nthe upper bound for the number of control points to M =\n256, which was sufficient for the datasets we experimented\nwith. We use 3 bits to represent the path membership, which\ncan support up to 7 distinct vector paths. This was also suf-\nficient for the datasets in our experiments. We set P to 16,\nresulting in 256 grid cells which can be represented by 8\nbinary bits. Together with the two-dimensional relative dis-\nplacement, the final dimension of our target tensor D is 13\nin our experiments. Similar to our raster diffusion model,\nwe set the number of diffusion steps T to 1000, used co-\nsine noise schedule, and the AdamW optimizer [28] with\nlearning rate 3.24\u00b710\u22125. During testing, we use the DDPM\nsampler [15] with 1000 steps.\n4. Results\nIn this section, we present experiments in three different ap-\nplication scenarios for our method. In the first scenario, we\naddress the problem of missing glyph generation for a font\n(Section 4.1). Many users often experience a frustrating sit-\nuation when they select a font they prefer, only to discover\nthat it lacks certain characters or symbols they wish to uti-\nlize. This issue is particularly prevalent when it comes to\nnon-Latin characters and mathematical symbols. As a sec-\nond application, we apply our method for few-shot font style\ntransfer (Section 4.2), where the desired font is specified in\nthe form of a few exemplar raster glyphs, and the goal is\nto generate vector glyphs in the same font. Finally, we dis-\ncuss interpolation of font styles (Section 4.3) i.e., generate\nglyphs whose style lies in-between two given fonts.\n5\n4.1. Generation of missing Unicode glyphs\nExisting public datasets or benchmarks to evaluate glyph\ngeneration are limited to a specific alphabet (e.g., Latin).\nBelow we discuss a new dataset for evaluating generation of\nglyphs across different languages, math symbols, and other\nsigns common in the Unicode standard. Then we discuss\ncomparisons, ablation study, metrics for evaluation, and re-\nsults.\nDataset.\nWe collected a new dataset of 1424 fonts from\nGoogle Fonts. The dataset contains 324K glyphs, cover-\ning 577 distinct Unicode glyphs in various languages (e.g.,\nGreek, Cyrillic, Devanagari), math symbols and other signs\n(e.g. arrows, brackets, currency). We randomly partition\nthe dataset into 314K-5K-5K glyphs for training, valida-\ntion, and testing respectively.\nComparison.\nWe compare with \u201cChiroDiff\u201d [9], which\napplies diffusion models to generate Kanji characters as\npolylines. Their method uses a set-transformer [23] to ob-\ntain a latent embedding from a 2D point set as the input con-\ndition. We replace their input condition to their diffusion\nmodel with the embeddings of characters and fonts using\nlook-up tables, as done in our raster diffusion model. We\ntrained and tuned their method, including the embeddings,\nto predict B\u00b4ezier curve control points using our dataset, as\nin our method.\nAblation.\nIn addition, we evaluate the following alterna-\ntive variants of our method: (a) Vector only: in this ablation,\nwe remove the raster diffusion model and use the vector\ndiffusion model only \u2013 in this case, the font and character\nembeddings are used as input conditions to the vector dif-\nfusion model. (b) No control point fields: we remove the\nRGB control point field from the target raster image of our\nraster diffusion model \u2013 in this case, we condition the vector\ndiffusion model only on the single-channel raster image of\nthe glyph. (c) Predict continuous coordinates only: in the\nvector diffusion model, instead of predicting discrete grid\ncell indices plus displacements relative to cell centers, we\ndirectly predict the absolute coordinates x and y per control\npoint.\nEvaluation metrics.\nWe compare the generated glyphs\nwith ones designed by artists in the test split. We use the\nfollowing metrics:\n(a) L1: we compare the image-space absolute pixel differ-\nences of glyphs when rasterized. We use the same rasterizer\nfor all competing methods and variants. This reconstruction\nerror was also proposed in [45] for glyph evaluation.\n(b) CD: we measure the bidirectional Chamfer distance be-\ntween artist-specified control points and generated ones.\nMethod\nL1 \u2193 CD \u2193 #cp diff \u2193 #vp diff \u2193\nOurs\n0.014 0.16\n3.05\n0.03\nOurs (cont. coord. only) 0.020 0.18\n3.30\n0.03\nOurs (no cp fields)\n0.016 0.60\n12.46\n0.13\nOurs (vector only)\n0.016 0.68\n9.36\n0.11\nChiroDiff [9]\n0.044 1.66\n56.37\n0.77\nTable 1. Missing glyph generation evaluation on the full test set.\nMethod\nL1 \u2193 CD \u2193 #cp diff \u2193 #vp diff \u2193\nOurs\n0.021 0.35\n3.92\n0.04\nOurs (cont. coord. only) 0.028 0.40\n4.11\n0.04\nOurs (no cp fields)\n0.026 0.83\n12.90\n0.16\nOurs (vector only)\n0.025 0.84\n9.72\n0.15\nChiroDiff [9]\n0.072 3.63\n71.97\n1.02\nTable 2. Missing glyph generation evaluation on a more challeng-\ning subset of our test set where test glyphs are from different glyph\nfamilies compared to any glyphs in the training set.\nMethod\nL1 \u2193 CD \u2193 #cp diff \u2193 #vp diff \u2193\nOurs\n0.069 0.46\n15.05\n0.033\nDeepVecFont-v2 [46] 0.098 1.05\n34.84\n0.052\nTable 3. Few-shot font style transfer evaluation.\n(c) #cp diff: we measure the difference between the number\nof artist-made control points and predicted ones averaged\nover all paths.\n(d) #vp diff: we measure the difference between the number\nof artist-specified vector paths and predicted ones.\nFor all the above measures, we report the averages over\nour test split. We propose the last three metrics for compar-\ning glyphs in terms of the control point and path character-\nistics, which are more relevant in vector font design.\nQuantitative Results.\nTable 1 shows the quantitative re-\nsults for ChiroDiff and the alternative variants of our\nmethod on the full test set. The full version of our method\noutperforms ChiroDiff and our reduced variants on all eval-\nuation metrics. We note that a glyph in one family variation\ne.g., italics might have a different variation in the same font,\ne.g., bold. Although two different family variations of the\nsame glyph often have largely different control point loca-\ntions and distributions, we create an even more challeng-\ning subset of the test set where we removed all test glyphs\nthat had a different family variation in the same font within\nthe training set. The results on this subset are reported in\n2. Chirodiff and other variants still have much higher er-\nror than ours. This indicates that our two-stage approach,\nand the mixed discrete-continuous representation of vector\npaths along with the control point fields are all important to\nachieve high performance.\n6\nBitter \u2020\nOrbitron \u2021\nYellowtail \u2217\nVidaloka \u2217 \nG\n\u20ab\n\u20a4 \n\u00be\n\u20a1\n\u0126\n\u03a8 \n\u03b2 \n\u03be\n\u03a6\n\u2030\nFigure 5. An incomplete font matrix from the Google Font dataset, each row represents a font and all glyphs in one column have the same\nUnicode. Glyphs in the green boxes are missing glyphs generated by our method. \u2217: Regular, \u2021: ExtraBold, \u2020: ItalicVariableFontWidth.\nGT \nOurs\nOurs\nChiroDiff\nfull\nNo cp field\nvector only\nOurs\nOurs\ncont. coord.\nonly\nFigure 6. Glyph generation results for test cases from the Google\nfont dataset. We compare our method to ChiroDiff [9] and de-\ngraded variants of our method. Our full method is able to generate\nglyphs that are much closer to artist-made (\u201cground-truth\u201d/\u201cGT\u201d)\nones compared to alternatives.\nExisting glyphs\nGenerated missing glyphs\n\u20a4 \nK \nFigure 7. Stochastic sampling: the results are generated with three\ndifferent random seeds.\nQualitative Results.\nFigure 6 shows qualitative compar-\nisons for missing glyph generation.\nCompared to our\nmethod, we observe that ChiroDiff produces imprecise con-\ntrol points and curve structure, resulting in significant dis-\ntortions and artifacts in the synthesized glyphs. We also\nobserved degraded results in all alternative variants of our\nInput reference\n(a)\n(b)\n(c)\nFigure 8. Few-shot style transfer results. On the left, we show the\nreference glyphs belonging to a novel font style. On the right, we\nshow (a) the artist-made (\u201cground-truth\u201d) glyphs, (b) our gener-\nated ones, and (c) the ones produced by DeepVecFont-v2 [46].\nmethod in terms of misplaced or skipped control points.\nFigures 1,5 show additional results of missing glyph gener-\nation for our method on the Google Font dataset for various\ntarget fonts and characters. Figure 7 demonstrates multiple\nsamples generated by our diffusion pipeline with random\nseeds. The samples adhere to the same font style, while\n7\nArimo-\nMediumItalic\nDancingScript\nCourgette-\nRegular\nOrbitron-Medium\nOrbitron-Bold\nRobotoSlab-\nExtraLight\nFigure 9. Font interpolation. We perform linear interpolation in embedding space from source font (left) \u2192 target font (right).\nhaving subtle variation in the glyph proportions and control\npoint distributions. From a practical point of view, a de-\nsigner can explore multiple such samples, and choose the\nmost preferred variant.\nIn Figure 12, instead of using the vector diffusion model,\nwe use off-the-shelf vectorization methods on the raster\nglyph image produced by the raster diffusion stage.\nAs\nshown in the comparison, this approach often fails to pro-\nduce coherent curve topology and structure.\n4.2. Few-shot font style transfer\nFor this application, we compare with DeepVecFont-v2\n[46], which previously demonstrated few-shot font style\ntransfer. To perform the comparison, we use the dataset pro-\nposed in the DeepVecFont-v2 paper. The dataset includes\n52 lowercase and uppercase Latin characters in various font\nstyles \u2013 there are total 8, 035 training fonts and 1, 425 test\nones. Each test case contains 4 reference characters from\na novel font (unobserved during training). The reference\ncharacters are available in both vector and raster format.\nMethods are supposed to transfer this novel font style to\ntesting characters. We note that DeepVecFont-v2 requires\nthe vector representation of the reference characters as addi-\ntional input condition, while our method only uses the raster\nreference images. From this aspect, our method can be con-\nsidered as disadvantaged in this comparison.\nQuantitative results.\nTable 3 shows numerical compar-\nisons based on the same evaluation metrics as in the missing\nglyph generation application. Despite the disadvantage, our\nmethod outperforms DeepVecFont-v2 on all metrics.\nQualitative results.\nFigure 8 demonstrates font style\ntransfer results. We observed that DeepVecFont-v2 tends to\nsucceed in capturing the font style of the reference glyphs,\nyet still often produces subtle distortions in the vector paths.\nOur method produces results that match the style of refer-\nences glyphs with less artifacts, even in challenging styles.\nAdditional results are in Figure 1 and 11.\n4.3. Font style interpolation\nFinally, we experimented with interpolating two given font\nstyles. To perform interpolation, we first obtain the font\nemdeddings from our trained look-up table, then perform\nlinear interpolation of these embeddings.\nOur diffusion\nmodel is then conditioned on the interpolated embedding\nvector for font style. We demonstrate qualitative results in\nFigure 9. Our results smoothly interpolates artistic proper-\nties of the source and target font, such as the variable stroke\nwidth and local curvature, while preserving structural and\ntopological properties of the glyphs e.g., their genus.\n5. Conclusion\nWe presented a generative model of vector fonts. We show\nthat a cascade of a raster and vector diffusion model can\novercome the challenges of neural parametric curve predic-\ntion, and generate editable vector fonts with precise geom-\netry and control point locations.\nCinzel font Ours -   \nFigure 10. Failure case.\nLimitations.\nWe\nrequire\nvector paths as supervision\nand cannot take advantage of\nraster images as additional\nsupervision\nthat\nmay\nbe\navailable. Another limitation\nis shown in Figure 10. Cinzel is an uppercase font. For\nthe missing glyph \u20ab(dong), a lowercase glyph, our method\nproduces an uppercase glyph. While the generated glyph\npreserves the style of the font and the structure of the glyph\n(the stroke), it does not preserve the symbolic meaning of\nthe glyph.\nFuture work.\nGenerative models of images in the raster\ndomain have been successfully used to learn complex pri-\nors about the visual world that can be used in various tasks\nsuch as inpainting and 3D tasks. We believe that a gen-\nerative model for vector fonts could also have additional\ndownstream applications, such as automatic completion of\n8\nvector logos or icons, or be extended to produce 3D para-\nmetric curves or surfaces.\nReferences\n[1] Samaneh Azadi, Matthew Fisher, Vladimir G Kim, Zhaowen\nWang, Eli Shechtman, and Trevor Darrell.\nMulti-content\ngan for few-shot font style transfer. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7564\u20137573, 2018. 2\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022. 2\n[3] Mikhail Bessmeltsev and Justin Solomon. Vectorization of\nline drawings via polyvector fields. ACM Trans. Graph., 38\n(1):9:1\u20139:12, 2019. 2, 11, 12\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models.\narXiv preprint arXiv:2304.08818,\n2023. 2\n[5] Neill DF Campbell and Jan Kautz. Learning a manifold of\nfonts. ACM Transactions on Graphics (ToG), 33(4):1\u201311,\n2014. 2\n[6] Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and\nRadu Timofte. Deepsvg: A hierarchical generative network\nfor vector graphics animation. Advances in Neural Informa-\ntion Processing Systems, 33:16351\u201316361, 2020. 1, 2\n[7] Junbum Cha, Sanghyuk Chun, Gayoung Lee, Bado Lee,\nSeonghyeon Kim, and Hwalsuk Lee.\nFew-shot compo-\nsitional font generation with dual memory.\nIn Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part XIX 16, pages\n735\u2013751. Springer, 2020. 2\n[8] Ting Chen, Ruixiang ZHANG, and Geoffrey Hinton. Analog\nbits: Generating discrete data using diffusion models with\nself-conditioning. In The Eleventh International Conference\non Learning Representations, 2023. 5\n[9] Ayan Das, Yongxin Yang, Timothy Hospedales, Tao Xi-\nang, and Yi-Zhe Song. Chirodiff: Modelling chirographic\ndata with diffusion models. In International Conference on\nLearning Representations, 2023. 2, 6, 7\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 5, 11\n[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in Neural Informa-\ntion Processing Systems, 34:8780\u20138794, 2021. 4, 11\n[12] Yiming Gao and Jiangqin Wu. Gan-based unpaired chinese\ncharacter image translation via skeleton transformation and\nstroke rendering. In proceedings of the AAAI conference on\nartificial intelligence, pages 646\u2013653, 2020. 2\n[13] Yue Gao, Yuan Guo, Zhouhui Lian, Yingmin Tang, and Jian-\nguo Xiao. Artistic glyph image synthesis via one-stage few-\nshot learning. ACM Transactions on Graphics (TOG), 38(6):\n1\u201312, 2019. 2\n[14] David Ha and Douglas Eck.\nA neural representation of\nsketch drawings.\narXiv preprint arXiv:1704.03477, 2017.\n2\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. arXiv preprint arxiv:2006.11239,\n2020. 2, 4, 5\n[16] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2\n[17] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. J. Mach. Learn.\nRes., 23(47):1\u201333, 2022. 2\n[18] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural\nwavelet-domain diffusion for 3d shape generation. In SIG-\nGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022. 2\n[19] Yue Jiang, Zhouhui Lian, Yingmin Tang, and Jianguo Xiao.\nScfont: Structure-guided chinese font generation via deep\nstacked networks. In Proceedings of the AAAI conference on\nartificial intelligence, pages 4015\u20134022, 2019. 2\n[20] Byungsoo Kim, Oliver Wang, A Cengiz \u00a8Oztireli, and Markus\nGross. Semantic segmentation for line drawing vectorization\nusing neural networks. In Computer Graphics Forum, pages\n329\u2013338. Wiley Online Library, 2018. 2\n[21] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 2\n[22] Yuxin Kong, Canjie Luo, Weihong Ma, Qiyuan Zhu, Sheng-\ngao Zhu, Nicholas Yuan, and Lianwen Jin. Look closer to\nsupervise better: One-shot font generation via component-\nbased discriminator. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n13482\u201313491, 2022. 2\n[23] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se-\nungjin Choi, and Yee Whye Teh. Set transformer: A frame-\nwork for attention-based permutation-invariant neural net-\nworks.\nIn International conference on machine learning,\npages 3744\u20133753. PMLR, 2019. 6\n[24] Tzu-Mao Li, Michal Luk\u00b4a\u02c7c, Micha\u00a8el Gharbi, and Jonathan\nRagan-Kelley.\nDifferentiable vector graphics rasterization\nfor editing and learning.\nACM Transactions on Graphics\n(TOG), 39(6):1\u201315, 2020. 2\n[25] Zhouhui Lian, Bo Zhao, Xudong Chen, and Jianguo Xiao.\nEasyfont: a style learning-based system to easily build your\nlarge-scale handwriting fonts. ACM Transactions on Graph-\nics (TOG), 38(1):1\u201318, 2018. 2\n[26] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fi-\ndler, Ming-Yu Liu, and Tsung-Yi Lin.\nMagic3d: High-\nresolution text-to-3d content creation.\narXiv preprint\narXiv:2211.10440, 2022. 2\n[27] Raphael Gontijo Lopes, David Ha, Douglas Eck, and\nJonathon Shlens. A learned representation for scalable vec-\ntor graphics. In Proceedings of the IEEE/CVF International\n9\nConference on Computer Vision, pages 7930\u20137939, 2019. 1,\n2\n[28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 4, 5\n[29] Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev,\nNikita Orlov, Yun Fu, and Humphrey Shi. Towards layer-\nwise image vectorization. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 16314\u201316323, 2022. 2\n[30] Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev,\nNikita Orlov, Yun Fu, and Humphrey Shi. Towards layer-\nwise image vectorization. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, 2022.\n11, 12\n[31] Haoran Mo, Edgar Simo-Serra, Chengying Gao, Changqing\nZou, and Ruomei Wang. General virtual sketching frame-\nwork for vector line art.\nACM Transactions on Graphics\n(TOG), 40(4):1\u201314, 2021. 2\n[32] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021. 4\n[33] Song Park, Sanghyuk Chun, Junbum Cha, Bado Lee, and\nHyunjung Shim. Multiple heads are better than one: Few-\nshot font generation with multiple localized experts.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 13900\u201313909, 2021. 2\n[34] Ivan Puhachov, William Neveu, Edward Chien, and Mikhail\nBessmeltsev.\nKeypoint-driven line drawing vectorization\nvia polyvector flow. ACM Transactions on graphics, 40(6),\n2021. 2\n[35] Pradyumna Reddy, Michael Gharbi, Michal Lukac, and\nNiloy J Mitra. Im2vec: Synthesizing vector graphics without\nvector supervision. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n7342\u20137351, 2021. 2\n[36] Leo Sampaio Ferraz Ribeiro, Tu Bui, John Collomosse, and\nMoacir Ponti. Sketchformer: Transformer-based representa-\ntion for sketched structure. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 14153\u201314162, 2020. 2\n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 4, 5, 11\n[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[39] Rapee Suveeranont and Takeo Igarashi. Example-based au-\ntomatic font generation. In Smart Graphics: 10th Interna-\ntional Symposium on Smart Graphics, Banff, Canada, June\n24-26, 2010 Proceedings 10, pages 127\u2013138. Springer, 2010.\n2\n[40] Licheng Tang, Yiyang Cai, Jiaming Liu, Zhibin Hong, Ming-\nming Gong, Minhu Fan, Junyu Han, Jingtuo Liu, Errui Ding,\nand Jingdong Wang. Few-shot font generation by learning\nfine-grained local styles. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 7895\u20137904, 2022. 2\n[41] Xingze Tian and Tobias G\u00a8unther. A survey of smooth vector\ngraphics: Recent advances in representation, creation, ras-\nterization and image vectorization. IEEE Transactions on\nVisualization and Computer Graphics, 2022. 2\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 3\n[43] Qiang Wang, Haoge Deng, Yonggang Qi, Da Li, and Yi-Zhe\nSong. Sketchknitter: Vectorized sketch generation with dif-\nfusion models. In The Eleventh International Conference on\nLearning Representations, 2023. 2\n[44] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.\nReal-esrgan: Training real-world blind super-resolution with\npure synthetic data. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1905\u20131914,\n2021. 11, 12\n[45] Yizhi Wang and Zhouhui Lian. Deepvecfont: Synthesizing\nhigh-quality vector fonts via dual-modality learning. ACM\nTransactions on Graphics, 40(6), 2021. 2, 6\n[46] Yuqing Wang, Yizhi Wang, Longhui Yu, Yuesheng Zhu, and\nZhouhui Lian. Deepvecfont-v2: Exploiting transformers to\nsynthesize vector fonts with higher quality. arXiv preprint\narXiv:2303.14585, 2023. 1, 2, 6, 7, 8, 11\n10\nAppendix\nA: Implementation Details\nHere we provide additional implementation details of our\nnetwork architecture.\nOur model is implemented in Py-\nTorch.\nRaster diffusion denoiser.\nOur raster diffusion denoiser\nfollows the UNet architecture in [11, 37]. The UNet model\nuses a stack of residual layers and downsampling convolu-\ntions, followed by a stack of residual layers with upsam-\npling convolutions, with skip connections connecting the\nlayers with the same spatial size. We provide an overview\nof the hyperparameters in Table 4. To condition the model\non the character identifier, we use a look-up table to project\nit to an 896-dimensional embedding and then add it together\nwith the time step embedding to modulate the feature maps\nof each residual block.\nFew-shot font style encoder.\nIn the application of few-\nshot font style transfer, we used a ConvNet to encode ref-\nerence raster glyphs into a font style feature map. We used\nthe encoder part of the UNet architecture in [11, 37]. The\nConvNet encoder encodes the 64 \u00d7 64 input image into an\n8 \u00d7 8 \u00d7 512 high-dimensional feature map via 3 downsam-\npling layers.\nVector diffusion denoiser.\nOur vector diffusion denoiser\nis an encoder-only transformer following BERT [10]. We\nset the number of transformer layers and the number of at-\ntention heads to 8 and 12 respectively. To condition the\nvector diffusion denoiser on the raster guidance x0, we first\nencode the 64 \u00d7 64 \u00d7 4 raster image to a 16 \u00d7 16 \u00d7 768\nhigh-dimensional feature map with a ConvNet encoder. The\nConvNet encoder has two downsampling layers with self-\nattention layers at resolution 32\u00d732 and 16\u00d716. The Con-\nvNet encoder is trained with the transformer jointly. After\nobtaining the 16 \u00d7 16 \u00d7 768 high-dimensional feature map,\nwe flatten it to a shape of 256 \u00d7 768, then we add it to each\ntransformer layer via cross-attention following [37].\nB: Additional Results\nAdditional comparisons with DeepVecFont-v2 [46].\nPlease\nsee\nFigure\n11\nfor\nmore\ncomparisons\nwith\nDeepVecFont-v2 [46] on the task of few-shot font style\ntransfer.\nComparison with a vectorizer approach.\nAs an alterna-\ntive comparison, we tried the following approach: instead\nof using the vector diffusion model, we use PolyVec [3] or\nLIVE [30] on the rasterized font image produced by our\nInput shape\n64 \u00d7 64 \u00d7 4\nDiffusion steps\n1000\nNoise Schedule\ncosine\nChannels\n224\nDepth\n2\nChannel Multiplier\n1,2,3,4\nAttention resolutions\n32,16,8\nHead Channels\n32\nBatch Size\n448\nLearning Rate\n3.24e-5\nTable 4. Hyperparameters for raster diffusion denoiser\nInput reference\n(a)\n(b)\n(c)\nFigure 11. Few-shot style transfer results. On the left, we show\nthe reference glyphs belonging to a novel font style. On the right,\nwe show (a) the artist-made (\u201cground-truth\u201d) glyphs, (b) our gen-\nerated ones, and (c) the ones produced by DeepVecFont-v2 [46].\nraster diffusion stage. We also tried upsampling the 64\u00d764\noutput raster image to 256\u00d7256 using ESRGAN [44] before\npassing it to the vectorizer. We show qualitative comparison\nin Figure 12. In both cases, PolyVec and LIVE often failed\nto produce coherent curve topology, structure, and plausible\ncontrol point distributions.\n11\nRaster-DM Output           Ours                  PolyVec                          LIVE            ESRGAN + PolyVec     ESRGAN + LIVE\nFigure 12. We compare our results (Ours) with PolyVec [3] and LIVE [30] applied to the raster image produced by our raster diffusion stage\n(left-most column). We also compare with PolyVec and LIVE applied to a higher-resolution version of the raster image upsampled via\nESRGAN [44]. For each glyph, we show the predicted control points as well. Using our vector diffusion stage instead of an off-the-shelf\nvectorizer produces higher-quality glyphs and much more plausible control point distributions. Compared to our vector diffusion model,\nESRGAN + PolyVec requires about ten times more control points for effective glyph reconstruction but sacrifices user editability and SVG\ncompactness. We recommend the viewer to zoom in for better clarity.\n12\n"
  },
  {
    "title": "SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing",
    "link": "https://arxiv.org/pdf/2312.11392.pdf",
    "upvote": "18",
    "text": "SCEdit: Efficient and Controllable Image Diffusion\nGeneration via Skip Connection Editing\nZeyinzi Jiang\nChaojie Mao\nYulin Pan\nZhen Han\nJingfeng Zhang\nAlibaba Group\n{zeyinzi.jzyz, chaojie.mcj, yanwen.pyl, hanzhen.hz, zhangjingfeng.zjf}@alibaba-inc.com\nCanny\nHED\nDepth\nSegmentation\n(c) Controllable Generation\nInpainting\n(b) Few-shot\n(a) Text-to-Image\nPose\nColor\nCOCO E\ufb03cient Tuning\nOutpainting\nA grey and \nwhite cat\u2026\nSD v1.5\n3D\nAnime\nFlat illustration\nOil painting\nSketch\nWatercolor\nFigure 1. Images generated by SCEdit. With a small number of trainable parameters and low memory usage, SCEdit enables efficient\nfine-tuning on specific datasets (left top) and supports transfer learning with a few samples (right top). Additionally, it adopts various\nconditions as inputs for efficient controllable generation (middle), while individually learned conditional models combine effortlessly,\nproviding endless compositional possibilities (bottom).\nAbstract\nImage diffusion models have been utilized in various\ntasks, such as text-to-image generation and controllable im-\nage synthesis. Recent research has introduced tuning meth-\nods that make subtle adjustments to the original models,\nyielding promising results in specific adaptations of foun-\ndational generative diffusion models. Rather than modi-\nfying the main backbone of the diffusion model, we delve\ninto the role of skip connection in U-Net and reveal that hi-\nerarchical features aggregating long-distance information\nacross encoder and decoder make a significant impact on\nthe content and quality of image generation.\nBased on\nthe observation, we propose an efficient generative tuning\nframework, dubbed SCEdit, which integrates and edits\nSkip Connection using a lightweight tuning module named\nSC-Tuner.\nFurthermore, the proposed framework allows\nfor straightforward extension to controllable image syn-\nthesis by injecting different conditions with Controllable\nSC-Tuner, simplifying and unifying the network design for\nmulti-condition inputs. Our SCEdit substantially reduces\ntraining parameters, memory usage, and computational ex-\npense due to its lightweight tuners, with backward propa-\ngation only passing to the decoder blocks. Extensive exper-\niments conducted on text-to-image generation and control-\nlable image synthesis tasks demonstrate the superiority of\nour method in terms of efficiency and performance. Project\npage: https://scedit.github.io/\n1\narXiv:2312.11392v1  [cs.CV]  18 Dec 2023\n1. Introduction\nBuilding upon large-scale pre-trained image diffusion mod-\nels [1, 2, 29, 30], researchers have focused on various down-\nstream tasks and applications, including text-to-image gen-\neration [6\u20138, 25, 31], controllable image synthesis [16, 28,\n47] and image editing [3, 24, 35, 45]. However, fully fine-\ntuning a foundation image diffusion model often proves in-\nefficient or even impractical in most customized scenarios\ndue to the constraints of limited training data and computa-\ntional resources.\nRecently, efficient tuning methods [18, 19, 43] have\nemerged as a practical solution by introducing additional\ntrainable structures on generative tasks. Nonetheless, many\nof these popular efficient tuning methods still suffer from\nsubstantial resource consumption as the network expands.\nFor instance, LoRA [15] typically adds the trainable low-\nrank matrices to multi-head attention layers all across the U-\nNet [34], and backward propagation is conducted through-\nout the entire backbone, resulting in an accumulation of\ngradients and increase in memory usage during training,\neven more than that in fully fine-tuning. To address this\nissue, we properly design our framework by strategically in-\nserting all the trainable modules into the Skip Connections\n(SCs) between the encoder and decoder of the U-Net, ef-\nfectively decoupling the encoder from the backpropagation\nprocess and significantly reducing computation and mem-\nory requirements, as shown in Fig. 2.\nThe SCs bridge the gap between distant blocks in the\nU-Net architecture, facilitating the integration of informa-\ntion over long distances and alleviating vanishing gradients.\nSome works [17, 36, 41] have focused on exploring the ef-\nficacy of SCs in enhancing the training stability of the U-\nNet, as well as in improving the generation quality [39]. In-\nspired, we further investigate the potential of U-Net\u2019s SCs\nin adapting to new scenarios. To gain a deeper insight into\neach SC within a pre-trained U-Net, we gradually remove\nSCs and observe the subsequent changes in the value distri-\nbutions and feature maps across the blocks of the decoder\nin Fig. 3. We first assess the diversity of information in the\nfinal decoder\u2019s output by visualizing the distribution of its\nlatent feature values. A higher variance signifies a greater\nbreadth of information, while a variance near zero indicates\na significant loss of detail. As we discard an increasing\nnumber of SC from half to all, the variance of the final de-\ncoder output gradually decreases. In addition, when half of\nthe SCs are removed, there is a marked reduction in the level\nof detailed structural information within the feature maps of\nboth the 6th and 11th decoder blocks. Eliminating all SCs\nfurther intensifies the deterioration of information. These\ntrends further validate the significant impact of SC on the\ngeneration of detailed structural information.\nIn light of this revelation, we propose a simple yet highly\nefficient approach by Skip Connection Editing for image\nSCEdit\nSCEdit\nFull\nLoRA\nT2I-Adapter\nControlNet\nControlNet-XS\nControlLoRA\nMemory (GB)\nFID (Text-to-Image Generation)\nFID (Controllable Image Synthesis)\nFigure 2. Performance and efficiency comparison on both text-\nto-image generation (circular markers) and controllable image\nsynthesis (pentagonal markers) tasks. The marked area reflects\nthe relative amount of parameters.\n0\nIN\n6\nOut\n6\nOut\n6\nOut\n11\nOut\n11\nOut\n11\nOut\norigin/in\ndrop half SC/out\ndrop all SC/out\norigin/out\n-0.5\n0.0\n0.25\n-0.25\nFigure 3. The output distributions (left) and feature maps (right)\nof pre-trained U-Net. Removing the skip connections from differ-\nent layers markedly affects the overall network output.\ngeneration, dubbed SCEdit. Specifically, we introduce a\nlightweight tuning module named SC-Tuner, which is de-\nsigned to edit the latent features within each SC of the pre-\ntrained U-Net for efficient tuning. Furthermore, we extend\nthe capabilities of SC-Tuner to facilitate controllable im-\nage synthesis, which can accommodate various input condi-\ntions using the Controllable SC-Tuner (CSC-Tuner). Our\nSCEdit framework is adaptable to a broad spectrum of im-\nage generation tasks using the proposed tuning modules and\nby decoupling the encoder blocks in U-Net, it allows for ef-\nficient and flexible training, as it enables backward propa-\ngation solely through the decoder block.\n2\nWe evaluate our SCEdit on efficient tuning of text-\nto-image generation tasks as well as controllable image\nsynthesis tasks.\nFor text-to-image generation tasks, our\napproach outperforms existing methods on COCO2017\ndataset in terms of FID score and qualitative results, while\nalso reducing memory consumption by 52% during the\ntraining stage. Additionally, faster transfer and high-quality\nresults are achieved in the few-shot fine-tuning scenario.\nOn the controllable generation tasks, our approach can eas-\nily control the results under various conditional inputs and\nshow impressive results, while having lower computational\ncosts than existing methods. It utilizes merely 7.9% of the\nparameters required by ControlNet and achieves a 30% re-\nduction in memory usage.\n2. Related work\nImage Diffusion Models [13, 40, 41] have achieved the\nstate-of-the-art performance in sample quality of generative\nimage. Research on text-to-image diffusion models [30, 33]\nincorporates text latent vectors as conditions. Some works\n[16, 47] conduct controllable image generation by leverag-\ning various images as conditions, offering personalization,\ncustomization, or task-specific image generation. However,\nthe scale and computational cost limit the applications of\nimage diffusion models.\nEfficient Tuning [9, 14, 15, 19, 20] has become a popu-\nlar solution in recent years, attracting significant attention.\nIt allows for easy adaptation of image diffusion models by\nmaking light adjustments to the original pre-trained models.\nLoRA [15] is to use low-rank matrices to learn weight off-\nsets, which has proven effective in customized image gener-\nation scenarios. Other works, such as U-Tuning [18], Res-\nTuning [19], and MAM-Adapter [10], propose a unified\nparadigm for efficient tuning methods, offering more op-\ntions for tuning pre-trained models. Typically, some tuning\nmodules are integrated into the U-Net architecture of diffu-\nsion models, but few works thoroughly analyze the key fac-\ntors for improving the quality of generative images through\ntuning modules.\nU-Net is originally introduced for diffusion models in\nDDPM [13], and it achieves tremendous success in the field\nof generative tasks. Recently, there has been a lot of at-\ntention focused on the effectiveness of skip connections in\nU-Net. To address the issue of doubling signal variance\ncaused by skip connections and alleviate the oscillations of\nU-Net, some methods [36, 41] propose rescaling all skip\nconnections by\n1\n\u221a\n2. ScaleLong [17] provides further theo-\nretical analysis on why scaling the coefficients of skip con-\nnections can help improve the training stability of U-Net\nand using constant scaling or learnable scaling for better\nstability. FreeU [39] recognizes that the main backbone of\nU-Net plays a primary role in denoising, while the skip con-\nnections introduce high-frequency features into the decoder\nmodule. By leveraging the strengths of both components,\nthe denoising capability of U-Net can be enhanced.\nIn-\nspired by previous works, we aim to analyze the roles of\nskip connections in the image generation process and pro-\npose a novel and effective framework incorporating tuning\nmodules by editing skip connections.\n3. Method\n3.1. Preliminaries\nDiffusion models [13] are a family of probabilistic genera-\ntive models that aim at sampling high-fidelity images from\nGaussian noise. It combines two fundamental processes: a\ndiffusion process and a denoising process. In the diffusion\nprocess, it gradually decreases the signal-to-noise ratio of\nan image via a T-step Markov chain, following prescribed\nnoise level schedules [\u03b21, \u03b22, ..., \u03b2T ]. At each step t, the\nnoisy intermediate variable xt is constructed as:\nxt = \u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5,\n\u03f5 \u223c N(0, 1),\n(1)\nwhere \u03b1t = 1 \u2212 \u03b2t and \u00af\u03b1t = Qt\ns=1 \u03b1s. The denoising\nprocess reverses the above diffusion process by estimating\nthe noise \u03f5 via a parameterized neural network, which is\ntrained by minimizing the l2 loss between estimated noise\n\u03f5\u03b8 and real noise \u03f5:\n\u2113t\nsimple(\u03b8) = Ex0,t,\u03f5 \u2225\u03f5\u03b8(xt, t) \u2212 \u03f5\u22252\n2 .\n(2)\nU-Net [34] is already a widely adopted architecture in pixel-\ncorresponded generative tasks, such as image restoration,\nmedical segmentation, and the aforementioned image gen-\neration. Specifically, it first encodes the input into multi-\nscale features through multiple cascaded encoder blocks:\nxi+1 = Fi(xi),\n0 \u2264 i \u2264 N \u2212 1,\n(3)\nwhere i denotes the index number of the encoder layer, Fi\ndenotes the i-th encoder block operation, N is the total\nnumber of encoder layers, xi+1 denotes the output of i-th\nencoder layer and x0 means the original image input.\nSubsequently, the decoder gradually decodes the feature\nvia establishing a skip connection with corresponding en-\ncoder layer outputs, which complements the high-frequency\ninformation:\ngj+1 = Gj([xN\u2212j; gj]),\n0 \u2264 j \u2264 N \u2212 1,\n(4)\nwhere [\u00b7; \u00b7] represents the concatenation operation, j de-\nnotes the index number of the decoder layer, Gj denotes\nthe j-th decoder block operation, N is the total number of\ndecoder layers which equals that of encoder layers, gj+1 de-\nnotes the output of j-th decoder layer, g0 is the last output\nblock before decoder input.\n3\n\u00d73\n\u00d73\n\u00d73\n\u00d73\nDenoising U-Net\nSkip Connection (SC)\n(c) Dense Conv\n\u2026\n\u2026\n\u2026\n\u2026\nTuner\nSC\nNonLinear\n(a) Skip Connection (SC)-Tuner\nTuner M\ncondition M\ncondition 1\nTuner 1\nSC \n\u2026\n\u2026\n\u2026\n<latexit sha1_base64=\"gX/EYJoeAEdZjSljLRLt9rW0VTQ=\">AB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cK9gPaWCbTbt0s\nJOrNInYaxsSUNm6u+JDCOtx1FgOyM0Q73oTcX/vE5qwis/4zJDZN0vihMBTExmT5P+lwxasTYEqSK21sJHaJCamxEJRuCt/jyMmeVb2L6vndeaV2ncdRhCM4hlPw4BJqcAt1aAFAc/wCm/Oo/PivDsf89aCk8cwh84nz+2Oo/E</latexit>4m7G6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVlDVoLGLVDlAzwSVrG4EayeKYRQI1gpGN1O/9cSU5rG8N+OE+REOJA85RWOldhdFMsQHr1euFV3BrJMvJxUIEe9V/7q9mOaRkwaKlDrjucmxs9QGU4Fm5S6qWYJ0hEOWMdSiRHTfja7d0\n\u21b51\n<latexit sha1_base64=\"I8n8O9tdxD4bMcp/u4Ge8DXwDk=\">AB73icbVDLSgNBEOyNrxhfUY9eBoPgKexKUI9BL16ECOYByRp6J5Nky\n4JObFKj/QjZUsaMlN/T6QYaj0OA9sZohnqRW8q/ue1E9O/9FMu48QwSeL+okgJiLT50mPK0aNGFuCVHF7K6FDVEiNjahgQ/AWX14mjbOyd16u3FVK1asjwcwTGcgcXUIUbqEdKAh4hld4cx6dF+fd+Zi35pxs5hD+wPn8AeCqj+A=</latexit>OzsOjMrhCU/4cWDIl79HW/+jZNkD5pY0FBUdPdFcSCa+O6305uZXVtfSO/Wdja3tndK+4fNHSUKMrqNBKRagWomeCS1Q03grVixTAMBGsGo+up3xiSvNI3ptxzPwQB5L3OUVjpVYHRTzEh9tuseSW3RnIMvEyUoIMtW7xq9OLaBIyahArdueGxs/RWU4FWxS6CSaxUhHOGBtSyWGTPvp7N\n\u21b5M\n(b) Controllable Skip Connection (CSC)-Tuner\nFigure 4. Illustration of SCEdit framework. Our method achieves efficient tuning by editing the features on skip connections which is\nproved to have rich structural information. Leveraging (a) SC-Tuner for text-to-image generation tuning, and controllable image synthesis\ncan be achieved with the assistance of (b) CSC-Tuner and (c) Cascade Dense Convolution.\n3.2. Tuner modules\nWe present Skip Connection Tuner, termed SC-Tuner, a\nmethod designed to directly edit the latent features within\nskip connections. As illustrated in Fig. 4a, the SC-Tuner\nis composed of a tuning operation, referred to as Tuner OP,\nand a residual connection. The j-th SC-Tuner takes xN\u2212j\nas input and produces the sum of xN\u2212j and the output of\nTuner OP with xN\u2212j as input. This process can be mathe-\nmatically formulated as follows:\nOSC\nj\n(xN\u2212j) = Tj(xN\u2212j) + xN\u2212j,\n(5)\nwhere OSC\nj\n(xN\u2212j) denotes the j-th SC-Tuner module with\nxN\u2212j as input, Tj denotes the Tuner OP of j-th SC-Tuner\nmodule.\nThe efficient tuning paradigm [19] has demonstrated\ncomparable effectiveness across various Tuner OPs, includ-\ning LoRA OP, Adapter OP, and Prefix OP, as well as their\nversatile combinations. In these independent OPs, we adapt\nthe form of an Adapter OP, as it has been proven to be the\nsimplest yet relatively effective method. The Tuner OP Tj\ncan be defined as follows:\nTj(xN\u2212j) = Wup\nj \u03d5(Wdown\nj\nxN\u2212j),\n(6)\nwhere Wup and Wdown are up and down tunable projec-\ntion matrices, respectively, \u03d5 is a GELU [12] activation\nfunction. Formally, we can employ tuners of various types\nand scales to modify the features of skip connections.\nFurthermore, The SC-Tuner can be easily adapted to sup-\nport controllable image synthesis by incorporating the con-\nditions and xN\u2212j as input, where conditions accommodate\nboth single and multiple conditions. In the case of multi-\ncondition controllable generation, we assign weights to dif-\nferent condition branches and combine them with original\nskip connections.\nThis modified structure is depicted in\nFig. 4b, named Controllable SC-Tuner or CSC-Tuner.\nWe extend Eq. (5) by injecting extra conditional informa-\ntion which can be formulated as follows:\nOCSC\nj\n(xN\u2212j, Cj) =\nM\nX\nm=1\n\u03b1m(T m\nj (xN\u2212j + cm\nj ) + cm\nj )\n+ xN\u2212j,\n(7)\nwhere OCSC\nj\n(xN\u2212j, Cj) denotes the j-th CSC-Tuner mod-\nule with xN\u2212j and M conditions Cj = {c0\nj, . . . , cM\nj } as\ninputs, cm\nj is the j-th hint block\u2019s output of m-th condition\nfeatures, \u03b1m is the weight of different independent condi-\ntion embeddings, and PM\nm=1 \u03b1m = 1. In practice, we can\nengage in joint training with multi-conditions or perform\ninference under combined conditions directly after com-\npleting single-condition training.\nThe control conditions\ninclude but are not limited to the canny edge, depth, hed\nboundary, semantic segmentation, pose keypoint, color, and\nmasked image.\n4\n3.3. SCEdit framework\nWe introduce SCEdit, a framework designed for efficient\nSkip Connection Editing in image generation that utilizes\nthe SC-Tuner and CSC-Tuner. All the skip connections and\nconditions are fed into the SC-Tuner and CSC-Tuner, with\nthe outputs subsequently concatenated to the original fea-\nture maps and input into the corresponding decoder blocks.\nWe depict this process as follows:\ngj+1 = Gj([Oj(xN\u2212j, Cj); gj]),\n(8)\nwhere Oj(xN\u2212j, Cj) denotes the j-th SC-Tuner or CSC-\nTuner modules. By editing the original xN\u2212j, we can adapt\nit to different tasks.\nSCEdit can facilitate flexibility and efficiency across\nboth text-to-image generation and controllable image syn-\nthesis tasks. As illustrated in Fig. 4, the SC-Tuner is ap-\nplied to the text-to-image generation task and integrated into\nall skip connections within the pre-trained U-Net architec-\nture. The CSC-Tuner is employed for controllable image\nsynthesis, where the input of corresponding condition fea-\ntures is encoded via a cascaded convolution network, as\nshown in Fig. 4c, consisting of a multi-layer hint block and\nseveral dense modules for feeding into the skip connection\nbranch, where each level contains zero convolution layers\nand SiLU [12] activation functions, ensuring compatibility\nwith the dimensions of skip connection.\n4. Experiments\n4.1. Experimental setups\nEvaluation. We evaluate the flexibility and efficiency of\nour SCEdit, mainly through the text-to-image generation\nand controllable image synthesis tasks.\nWe analyze its\nquantitative metrics such as trainable parameters, memory\nconsumption, training speed, and FID [38] evaluation met-\nrics to assess its performance and efficiency. Additionally,\nwe consider qualitative evaluations of the quality and fi-\ndelity of the generated images.\nDatasets. In the text-to-image generation task, we use the\nCOCO2017 [22] dataset for training and testing, which con-\nsists of 118k images and 591k training prompts. Further-\nmore, we employ customized style datasets [26] with a lim-\nited samples to further validate the effectiveness of our ap-\nproach. In controllable image synthesis tasks, for each kind\nof condition data, we utilize a filtered version of the LAION\nartistic dataset [37] that includes approximately 600k im-\nages that have been removed of duplicates, low-resolution\nimages, those with a risk for harm, and those that are of low\nquality.\nBaselines.\nAccording to task categories, the evaluation\ntasks can be characterized into two groups: (i) the text-to-\nimage tuning includes fully fine-tuning and LoRA [15] tun-\ning strategies; (ii) the controllable image synthesis focuses\nFull\nLoRA\nSD v1.5\nSCEdit\n[An Amtrak train sitting on the train track]\n[A brown, black and white bird resting on a tree branch]\nFigure 5. Qualitative comparison of the original SD v1.5, exist-\ning tuning strategies, and our SCEdit using the same prompts.\nTable 1. Comparison of FID and efficiency on text-to-image\ngeneration using COCO2017 dataset. For the FID score, we fol-\nlow the default settings of SD v1.5. In terms of efficiency, we\ncompare the aspects of parameter-, memory-, and training time ef-\nficiency. We compare the performance of LoRA and our method\nunder two different parameter settings.\nMethod\nFID\u2193\nParams\nMem.\nTime\nSD v1.5 [1]\n15.48\n-\n-\n-\nFull\n14.15\n859.52M\n53.46G\n\u00d71.0\nLoRA/r=64 [15]\n13.96\n23.94M\n60.57G\n\u00d71.24\nLoRA/r=6 [15]\n15.12\n2.24M\n59.94G\n\u00d71.20\nSCEdit\n13.82\n19.68M\n29.02G\n\u00d70.78\nSCEdit10\n13.99\n1.98M\n28.06G\n\u00d70.77\non methods including additional information as input, such\nas ControlNet [47], T2I-Adapter [28], ControlLoRA [11],\nand ControlNet-XS [46]. Due to the distinct task character-\nistics of these two categories, we apply the SC-Tuner mod-\nule for the former and CSC-Tuner for the latter.\nImplementation details. For all experiments, we perform\nefficient fine-tuning based on the Stable Diffusion (SD) pre-\ntrained model, where SD v1.5 [1] is used for the text-to-\nimage task and SD v2.1 [2] for the conditional task, and\nthe input image sizes for training are set to 512\u00d7512. We\nutilize the AdamW [23] optimizer with a fixed learning rate\nof 5e-5. Unless otherwise specified, models are trained for\n100k steps.\nFollowing ControlNet, we use different input condi-\ntions, e.g., edge map [4, 44], depth map [32], segmentation\nmap [21], and body key points [5], in addition to the color\nmap condition in T2I-Adapter [28] and the mask-generation\nstrategy presented in LaMa [42].\n5\n3D\nAnime\nFlat illustration\nMethod\nSketch\nOil painting\nSCEdit\nWatercolor\nLoRA\n[A boy in a jacket with a camou\ufb02age scarf]\nFigure 6. Few-shot transfer learning comparison on various custom-stylized datasets. Compared to LoRA tuning, SCEdit achieves more\nprecise learning of style characteristics and generates images of superior quality.\n4.2. Text-to-image generation\nPerformance and efficiency comparison. To evaluate the\ntransferability of our method in downstream tasks, we fine-\ntuning COCO2017 with pre-trained U-Net and compare our\nmethod with other training strategies in terms of qualitative\nand quantitative results.\nThe qualitative results can be seen in Fig. 5, the left-\nmost column is the zero-shot result of the original model,\nwhile the fine-tuned model obtained semantic comprehen-\nsion capabilities on downstream tasks. Compared to exist-\ning strategies, our method exhibits fewer artificial artifacts\nin the generated images and higher visual quality. For in-\nstance, in the second row, the generated bird has more re-\nalistic details in the head while maintaining semantic com-\nprehension.\nIn addition, as shown in Tab. 1, compared to the fully\nfine-tuning baseline, our SCEdit achieves 0.33 performance\ngain in FID score while using only 2.29% of the parameters\nand reducing nearly 22% training time. It is worth noted\nthat at lower parameters compared to LoRA/r=64 strategies\nwith rank 64, our method is lower in both FID, while the\ntraining memory can be reduced by 52.1% and the training\ntime of LoRA is 1.6\u00d7 longer than ours. When we further re-\nduce the hidden dimensions of the tuner by 10\u00d7, denoted as\nSCEdit10, which corresponds to a reduction of parameters\nby the same factor compared to LoRA/r=6, our FID score\nshows a significant decreased by 1.13, while also maintain-\ning a clear advantage in terms of memory usage and training\ntime.\nFew-shot transfer learning. In the image generation com-\nmunity, few-shot learning is a practical technique that en-\nables users to train a personalized model with just a small\nsubset of data. Our experiment involved performing few-\nTable 2. Comparison of FID and efficiency on controllable im-\nage synthesis using LAION dataset. \u201ck\u201d denotes the convolution\nkernel size of conditional model, and a larger size performs better\non FID score, albeit with a moderate increase in parameters.\nMethod\nFID\u2193\nParams.\nMem.\nTime\nControlNet [47]\n74.86\n364.23M\n49.51G\n\u00d71.0\nT2I-Adapter [28]\n73.37\n77.37M\n37.01G \u00d70.92\nControlNet-XS [46]\n75.63\n55.30M\n48.32G \u00d70.97\nControlLoRA [11]\n74.14\n21.52M\n68.92G \u00d71.34\nSCEdit/k=1\n73.18\n28.82M\n34.78G \u00d70.87\nSCEdit/k=3\n71.78\n99.11M\n35.28G \u00d70.87\nshot transfer learning on a custom-stylized dataset, which\nincluded classes 3D, anime, flat illustration, oil painting,\nsketch, and watercolor, each with only 30 image-text pairs.\nMoreover, to prevent style leakage, we perform a cleansing\nof style-related words from the original prompts and incor-\nporated specific trigger words <sce> during training to en-\nsure the reliability of the experiment. In Fig. 6, we provide\na comparison of the quality of samples between LoRA and\nour method, employing the same training setup. The results\ndemonstrate that our method more accurately captures the\nstyle, aligning with the distribution of the original training\ndata while maintaining text alignment. For instance, the flat\nillustration style retained the descriptions of a camouflage\nscarf, while the sketch style preserved the line-drawn de-\npictions in monochrome.\n4.3. Controllable image synthesis\nPerformance and efficiency comparison. Extensive ex-\nperiments are conducted to evaluate the effectiveness of our\n6\nDepth\nHED\nSegmentation\nCanny\nColor\nInpainting\nPose\nFigure 7. Controllable image synthesis with various conditions. The odd-numbered rows represent the input conditions, while the\neven-numbered rows correspond to the generated results. SCEdit is capable of generating high-quality images precise to input conditions.\nControlLoRA\nControlNet-XS\nT2I-Adapter\nControlNet\nCondition\nSCEdit\n[The Gorge]\n[Concept car, unreal engine, cinematography]\nFigure 8. Qualitative comparison to the state-of-the-art controllable image synthesis methods based on the canny edge and semantic\nsegmentation conditions. The areas in the boxes underneath are enlarged for detailed comparisons.\n7\nMountain\n(a) Using the same canny edge map and textual prompt in combination with different color maps results in the depiction of a mountain across various seasons.\nColor\nCanny\nCanny\nColor\n(b) Interpolations within the canny edge map and color maps.\nFigure 9. Composable generation. Combinations of multi-conditions provide more compositional possibilities.\nproposed SCEdit using CSC-Tuner on various conditional\ngeneration tasks, e.g., canny edge, depth, hed boundary, se-\nmantic segmentation, pose keypoint, color, masked image,\nand so on. As illustrated in Fig. 7, our method shows supe-\nrior performance and precise control ability under diverse\nscenarios and styles control, including real scenes, imag-\nined scenarios, and artistic styles.\nWe also compare our approach with the state-of-the-\nart methods that generate images from different conditions.\nFrom a qualitative perspective, we present the generated re-\nsults based on the canny edge map and semantic segmenta-\ntion conditions with different methods and zoom-in on the\ndetailed parts of the image in Fig. 8. It can be observed that\nour method achieves better quality in terms of fidelity and\nrealism, such as the preservation of the reflection on the lake\nsurface in the first row and the texture information of the car\nin the second row. From a quantitative perspective, as seen\nin Tab. 2, our method utilizes only 7.9% of the number of\nparameters in ControlNet, resulting in a 30% reduction in\nmemory consumption, and accelerates the training process\nwhile also yielding a lower FID score.\nComposable generation. In addition to generating control\nbased on a single condition, we also support the input of\nmulti-conditions simultaneously. In Fig. 9, we demonstrate\nthe joint application of separately trained canny edge and\ncolor map models and present the results on unpaired con-\ndition data for training-free scene-level image translation.\nBy anchoring the controlled subject to a canny edge map\nand text prompt, and incorporating various color maps, we\nproduce a distinctive seasonal transition effect, as illustrated\nin Fig. 9a. Additionally, by traversing the embedding space\nrepresentations of CSC-Tuner between the two conditions,\nwe can blend them to achieve variations. In Fig. 9b, SCEdit\nSketch\nOutpainting\nFigure 10.\nControl generalization.\nLearned edge condition\nmodel is capable of sketch-to-image task and inpainting condition\nmodel can control outpainting generation.\nfurther provides precise control capabilities, enabling dif-\nferent interpolation effects through balancing among multi-\nple elements.\nControllable generalization. We find that additional con-\ntrollable generation tasks could be attempted on models\nthat had already been trained under specific conditions. In\nFig. 10, we achieve impressive results using the edge-based\nmodel for the sketch-to-image task and the inpainting-based\nmodel for the outpainting task, although there is no adap-\ntation based on non-real conditions such as hand-drawn\nsketches for the former, and no specialized adjustment of\nmask generation patterns for outpainting for the latter.\n5. Conclusion\nWe propose SCEdit as an efficient and controllable method\nfor image diffusion generation.\nWe introduce SC-Tuner\nto edit skip connections and extends it to CSC-Tuner, en-\nabling a diverse range of conditional inputs. Our method\nachieves high efficiency by exclusively performing back-\nward propagation in the decoupled U-Net decoder. As a\nlightweight and plug-and-play module, SCEdit supports ar-\nbitrary single- and multi-condition generation and demon-\nstrates remarkable superiority in terms of performance.\n8\nReferences\n[1] Runway\nAI.\nStable\nDiffusion\nv1.5\nModel\nCard,\nhttps://huggingface.co/runwayml/stable-\ndiffusion-v1-5, 2022. 2, 5, 12\n[2] Stability AI. Stable Diffusion v2-1 Model Card, https:\n/ / huggingface . co / stabilityai / stable -\ndiffusion-2-1, 2022. 2, 5, 12\n[3] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructPix2Pix: Learning To Follow Image Editing Instruc-\ntions.\nIn IEEE Conf. Comput. Vis. Pattern Recog., pages\n18392\u201318402, 2023. 2\n[4] John Canny.\nA Computational Approach to Edge Detec-\ntion. IEEE Trans. Pattern Anal. Mach. Intell., pages 679\u2013\n698, 1986. 5, 11\n[5] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and\nYaser Sheikh. OpenPose: Realtime Multi-Person 2D Pose\nEstimation Using Part Affinity Fields. IEEE Trans. Pattern\nAnal. Mach. Intell., 43(1):172\u2013186, 2021. 5, 11\n[6] Huiwen Chang, Han Zhang, Jarred Barber, A. J. Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T. Freeman, Michael Rubinstein, Yuanzhen\nLi, and Dilip Krishnan.\nMuse: Text-To-Image Genera-\ntion via Masked Generative Transformers.\narXiv preprint\narXiv:2301.00704, 2023. 2\n[7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, and Zhenguo Li. PixArt-\u03b1: Fast Training of\nDiffusion Transformer for Photorealistic Text-to-Image Syn-\nthesis. arXiv preprint arXiv:2310.00426, 2023.\n[8] Alibaba Cloud. Tongyi Wanxiang, https://tongyi.\naliyun.com/wanxiang/, 2023. 2\n[9] David Ha, Andrew M. Dai, and Quoc V. Le. HyperNetworks.\nIn Int. Conf. Learn. Represent., 2016. 3\n[10] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. Towards a Unified View of\nParameter-Efficient Transfer Learning. In Int. Conf. Learn.\nRepresent., 2021. 3\n[11] Wu Hecong. ControlLoRA: A Lightweight Neural Network\nTo Control Stable Diffusion Spatial Information, https:\n//github.com/HighCWu/ControlLoRA, 2023. 5, 6,\n14\n[12] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear\nUnits (GELUs). arXiv preprint arXiv:1606.08415, 2023. 4,\n5\n[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Dif-\nfusion Probabilistic Models. In Adv. Neural Inform. Process.\nSyst. Curran Associates, Inc., 2020. 3\n[14] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona\nAttariyan, and Sylvain Gelly.\nParameter-efficient transfer\nlearning for NLP. In Int. Conf. Mach. Learn., pages 2790\u2013\n2799, 2019. 3\n[15] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-Rank Adaptation of Large Language Models. In\nInt. Conf. Learn. Represent., 2022. 2, 3, 5\n[16] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao,\nand Jingren Zhou. Composer: Creative and Controllable Im-\nage Synthesis with Composable Conditions. In Int. Conf.\nMach. Learn., 2023. 2, 3\n[17] Zhongzhan Huang, Pan Zhou, Shuicheng Yan, and Liang\nLin. ScaleLong: Towards More Stable Training of Diffu-\nsion Model via Scaling Network Long Skip Connection. In\nAdv. Neural Inform. Process. Syst., 2023. 2, 3\n[18] Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Yiliang Lv,\nDeli Zhao, and Jingren Zhou.\nRethinking Efficient Tun-\ning Methods from a Unified Perspective.\narXiv preprint\narXiv:2303.00690, 2023. 2, 3\n[19] Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Ao Ma, Yiliang\nLv, Yujun Shen, Deli Zhao, and Jingren Zhou. Res-Tuning:\nA Flexible and Efficient Tuning Paradigm via Unbinding\nTuner from Backbone. In Adv. Neural Inform. Process. Syst.,\n2023. 2, 3, 4, 13\n[20] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power\nof Scale for Parameter-Efficient Prompt Tuning.\nIn Conf.\nEmpirical Methods NLP, 2021. 3\n[21] Kunchang Li, Yali Wang, Gao Peng, Guanglu Song, Yu Liu,\nHongsheng Li, and Yu Qiao.\nUniFormer: Unified Trans-\nformer for Efficient Spatial-Temporal Representation Learn-\ning. In Int. Conf. Learn. Represent., 2021. 5, 11\n[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nEur. Conf. Comput. Vis., pages 740\u2013755, 2014. 5, 11, 12\n[23] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay\nRegularization. In Int. Conf. Learn. Represent., 2018. 5, 12\n[24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun\nWu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided Im-\nage Synthesis and Editing with Stochastic Differential Equa-\ntions. In Int. Conf. Learn. Represent., 2021. 2\n[25] Midjourney. Midjourney, https://www.midjourney.\ncom/, 2023. 2\n[26] ModelScope.\nCustomized Style Dataset Card, https:\n/ / modelscope . cn / datasets / damo / style _\ncustom_dataset/summary, 2023. 5, 11, 12\n[27] ModelScope.\nSWIFT(Scalable lightWeight Infrastruc-\nture\nfor\nFine-Tuning),\nhttps : / / github . com /\nmodelscope/swift, 2023. 12\n[28] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian\nZhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2I-\nAdapter: Learning Adapters to Dig out More Controllable\nAbility for Text-to-Image Diffusion Models. arXiv preprint\narXiv:2302.08453, 2023. 2, 5, 6, 11, 14\n[29] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. GLIDE: Towards Photorealistic Image Genera-\ntion and Editing with Text-Guided Diffusion Models. arXiv\npreprint arXiv:2112.10741, 2022. 2\n[30] OpenAI. DALL\u00b7E 2, https://openai.com/dall-e-\n2, 2022. 2, 3\n[31] OpenAI. DALL\u00b7E 3, https://openai.com/dall-e-\n3, 2023. 2\n9\n[32] Rene Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards Robust Monocu-\nlar Depth Estimation: Mixing Datasets for Zero-Shot Cross-\nDataset Transfer. IEEE Trans. Pattern Anal. Mach. Intell.,\npages 1623\u20131637, 2022. 5, 11\n[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In IEEE Conf. Comput.\nVis. Pattern Recog., pages 10684\u201310695, 2022. 3\n[34] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nNet: Convolutional Networks for Biomedical Image Seg-\nmentation. Med. Image Comput. Computer-Assisted Interv.,\n2015. 2, 3\n[35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. DreamBooth: Fine\nTuning Text-to-Image Diffusion Models for Subject-Driven\nGeneration.\nIn IEEE Conf. Comput. Vis. Pattern Recog.,\npages 22500\u201322510, 2023. 2\n[36] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-\nmans, David J. Fleet, and Mohammad Norouzi.\nImage\nSuper-Resolution via Iterative Refinement. arXiv preprint\narXiv:2104.07636, 2021. 2, 3\n[37] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade W. Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa R. Kundurthy, Kather-\nine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and\nJenia Jitsev.\nLAION-5B: An open large-scale dataset for\ntraining next generation image-text models. In Adv. Neural\nInform. Process. Syst., 2022. 5, 11, 12\n[38] Maximilian Seitzer.\npytorch-fid: FID Score for PyTorch,\nhttps://github.com/mseitzer/pytorch-fid,\n2020. 5, 12\n[39] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu.\nFreeU: Free Lunch in Diffusion U-Net.\narXiv preprint\narXiv:2309.11497, 2023. 2, 3\n[40] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning Diffusion Implicit Models. In Int. Conf. Learn. Repre-\nsent., 2021. 3, 12\n[41] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based\nGenerative Modeling through Stochastic Differential Equa-\ntions. In Int. Conf. Learn. Represent., 2021. 2, 3\n[42] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,\nAnastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,\nNaejin Kong, Harshith Goka, Kiwoong Park, and Victor\nLempitsky. Resolution-Robust Large Mask Inpainting With\nFourier Convolutions. In IEEE Winter Conf. Appl. Comput.\nVis., pages 2149\u20132159, 2022. 5, 11\n[43] Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou,\nZhaoqiang Liu, Jiawei Li, and Zhenguo Li.\nDiffFit: Un-\nlocking Transferability of Large Diffusion Models via Sim-\nple Parameter-efficient Fine-Tuning. In Int. Conf. Comput.\nVis., pages 4230\u20134239, 2023. 2\n[44] Saining Xie and Zhuowen Tu. Holistically-Nested Edge De-\ntection. In Int. Conf. Comput. Vis., pages 1395\u20131403, 2015.\n5, 11\n[45] Sihan Xu, Ziqiao Ma, Yidong Huang, Honglak Lee, and\nJoyce Chai.\nCycleNet: Rethinking Cycle Consistency in\nText-Guided Diffusion for Image Manipulation. In Adv. Neu-\nral Inform. Process. Syst., 2023. 2\n[46] Denis Zavadski, Johann-Friedrich Feiden, and Carsten\nRother. ControlNet-XS: Designing an Efficient and Effective\nArchitecture for Controlling Text-to-Image Diffusion Mod-\nels. arXiv preprint arXiv:2312.06573, 2023. 5, 6, 14\n[47] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nConditional Control to Text-to-Image Diffusion Models. In\nInt. Conf. Comput. Vis., pages 3836\u20133847, 2023. 2, 3, 5, 6,\n11, 14\n[48] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene Parsing through\nADE20K Dataset.\nIn IEEE Conf. Comput. Vis. Pattern\nRecog., pages 5122\u20135130, 2017. 11\n10\nIn the appendix, we provide more implementation details\n(Appendix A) including the dataset, architecture design,\nand hyperparameters used in training and inference. Then,\nwe demonstrate the ablation experiments (Appendix B)\nwith SC-Tuner and CSC-Tuner on different tasks. Further-\nmore, we showcase additional comparisons with existing\nmethods and qualitative results (Appendix C).\nA. Implementation details\nA.1. Dataset description\nIn this work, we consider three datasets for our experi-\nments: COCO Dataset [22], Customized Style Dataset [26],\nand LAION Dataset [37]. For the text-to-image generation\nsetting, we utilize the well-known COCO2017 Captions,\nwhich consists of 118,287 training images and 591,753 cap-\ntions for efficient fine-tuning, and Customized Style, which\ncontains 30 training images of different styles for few-shot\nfine-tuning. We use LAION Dataset for the controllable\nimage synthesis setting. The three datasets are illustrated in\nTab. 3.\nA.2. Hyperparameters\nWe provide an overview of the hyperparameters for all\ntrained models, divided by the task in Tab. 4.\nA.3. Architectures design\nIn the SCEdit framework, the central strategy involves edit-\ning the skip connections, which gives rise to two archi-\ntectures: SC-Tuner for text-to-image generation and CSC-\nTuner for controllable generation. These architectures are\nstraightforward to implement and can be easily transferred\nto other similarly designed modules. In Alg. 1, we provide\nthe forward function implementation of SCEdit written in\nPyTorch-like style.\nA.4. Conditions for generation\nWe generally follow the implementations of condition ex-\ntraction from ControlNet [47] and T2I-Adapter [28], with\ndetails as follows:\n\u2022 Canny Edge Map. We employ canny edge detector [4],\nutilizing random thresholds during training and fixed\nthresholds with a low value of 100 and a high value of\n200 during inference. The sample images are presented\nin Fig. 14a.\n\u2022 Depth Map.\nWe use MiDaS depth estimation [32]\nwith default settings. The sample images are shown in\nFig. 14b.\n\u2022 HED Boundary Map. We use HED boundary detec-\ntion [44] with default settings. The sample images are\nillustrated in Fig. 15a.\n\u2022 Semantic Segmentation Map.\nWe employ the Uni-\nFormer [21] semantic segmentation model, which was\ntrained on the ADE20K [48] dataset. The sample images\ncan be seen in Fig. 15b.\n\u2022 Pose Keypoint. We employ OpenPose [5] as the human\npose estimation model and visualize its prediction as con-\nditions. The sample images are showcased in Fig. 15c.\n\u2022 Color Map. We preserve the spatial hierarchical color\ninformation through a process of 64\u00d7 downsampling of\nthe image, subsequently followed by an upsampling to\nits original dimensions. The sample images are demon-\nstrated in Fig. 16a.\n\u2022 Inpainting.\nWe employ the mask generation strategy\nfrom LaMa [42] for conditional generation on the in-\npainting task. The sample images are demonstrated in\nFigs. 16b and 16c.\nFor all the aforementioned conditions, we utilize the same\ntraining dataset (LAION-ART [37]) and hyperparameters\nacross the tasks. The exception is the pose-conditional task,\nTable 3. The summary of the datasets for the experiments.\nDataset\n#Description\n#Task\n#Train\n#Test\nimage\nprompt\nimage\nprompt\nCommon Objects in Context (COCO)\nCOCO2017 Captions [22]\ncommon objects\ntext-to-image\n118,287\n591,753\n5,000\n25,014\nCustomized Style Dataset\n3D [26]\n3D style\ntext-to-image (few-shot)\n30\n30\n-\n-\nAnime [26]\nainme style\ntext-to-image (few-shot)\n30\n30\n-\n-\nFlatillustration [26]\nflatillustration style\ntext-to-image (few-shot)\n30\n30\n-\n-\nOilpainting [26]\noilpainting style\ntext-to-image (few-shot)\n30\n30\n-\n-\nSketch [26]\nsketch style\ntext-to-image (few-shot)\n30\n30\n-\n-\nWatercolor [26]\nwatercolor style\ntext-to-image (few-shot)\n30\n30\n-\n-\nLarge-scale Artificial Intelligence Open Network (LAION)\nLAION-ART [37]\nfiltered version\ncontrollable generation\n624,558\n624,558\n-\n-\n11\nTable 4. The summary of the training and inference settings for the experiments.\nConfig\n#Task\nText-to-image\nText-to-image (few-shot)\nControllable Generation\nDataset\nCOCO [22]\nCustomized Style [26]\nLAION-ART (Filtered) [37]\nBatch size\n32\n8\n64\nOptimizer\nAdamW [23]\nAdamW [23]\nAdamW [23]\nWeight decay\n0.01\n0.01\n0.01\nLearning rate\n0.00005\n0.00005\n0.00005\nLearning rate schedule\nConstant\nConstant\nConstant\nTraining steps\n100000\n1500\n100000\nData preprocess\nResize, CenterCrop\nResize, CenterCrop\nResize, CenterCrop\nResolution\n512\u00d7512\n512\u00d7512\n512\u00d7512\nPre-trained\nSD v1.5 [1]\nSD v1.5 [1]\nSD v2.1 [2]\nSampler\nDDIM [40]\nDDIM [40]\nDDIM [40]\nSample steps\n50\n50\n50\nGuide scale\n3.0\n7.5\n7.5\nDevice\nA100\u00d78\nA100\u00d71\nA100\u00d716\nTraining strategy\nAMP / Float16\nAMP / Float16\nAMP / Float16\nLibrary\nSWIFT [27]\nSWIFT [27]\nSWIFT [27]\nAlgorithm 1 Implementation of SCEdit in PyTorch-like style.\n# SC-Tuner\ndef forward(self, x, t=None, cond=dict()):\n...\n# input_blocks\nhs = []\nfor i, blk in enumerate(self.in_blks):\nh = blk(h, emb, context)\nhs.append(h)\n# middle_block\nh = self.mid_blk(h, emb, context)\n# output_blocks\nfor i, blk in enumerate(self.out_blks):\nskip_h = self.tuners[i](hs.pop())\nh = torch.cat([h, skip_h], dim=1)\nh = blk(h, emb, context)\n# Single CSC-Tuner\ndef forward(self, x, t=None, cond=dict()):\n...\n# Dense Conv for conditions\nguid_hs = []\nguid_hint = self.in_hint_blks(hint, emb,\ncontext)\nfor i, blk in enumerate(self.hint_blks):\nguid_hint = blk(guid_hint, emb, context)\nguid_hs.append(guid_hint)\n...\n# output_blocks\nfor i, blk in enumerate(self.out_blks):\nskip_h = self.tuners[i](hs.pop() + self.\nscale * guid_hs[::-1][i])\nh = torch.cat([h, skip_h], dim=1)\nh = blk(h, emb, context)\nfor which we exclusively utilize a subset of images contain-\ning human poses, amounting to a total of 162,338 instances.\nAdditionally, for the inpainting task, we follow the common\napproach of using both masks and cutouts as combined con-\nditional inputs.\nB. Ablation studies\nB.1. SC-Tuner structure\nWe ablate our SC-Tuner using the default setting in Tab. 5.\nIt is evident that our method allows for flexible design, in-\ncluding the intermediate dimensions of tuners, the number\nof utilized skip connection layers, and the selection of sub-\nmodules.\nIn Tab. 5a, we retain the dimensions of the skip con-\nnection features as the default intermediate dimensions for\nthe tuner. As the dimensions are reduced proportionally,\nthere is a corresponding decrease in the number of param-\neters. Despite this reduction, the decline in memory con-\nsumption is not substantial, and the FID [38] fails to show\nan improvement compared to the default setting. Similarly,\nin Tab. 5b, a performance degradation is observed when\nwe reduce the number of skip connection layers by inter-\nvals. Our SC-Tuner is designed with the flexibility to in-\nterchange its internal components, allowing for the use of\nconvolution networks or independent residual networks. As\ndemonstrated in Tab. 5c, even the most elementary com-\nponents, such as linear layers, can offer certain advantages\n12\nTable 5. SC-Tuner ablation experiments of efficient fine-tuning task on COCO2017. Default settings are marked in gray.\n(a) Ablation on downscaling ratio of dimensions.\nRatio\nFID\nParams\nMem.\n\u00d71\n13.82\n19.68M\n29.02G\n\u00d75\n13.92\n3.94M\n28.29G\n\u00d710\n13.99\n1.98M\n28.06G\n(b) Ablation on skip connection (SC) layers.\nSC Indexes\nFID\nParams\nMem.\n{0,11}\n14.45 3.48M\n28.11G\n{0,3,6,9,11} 13.96 7.79M\n28.56G\n{1,2, ..., 12} 13.82 19.68M 29.02G\n(c) Ablation on tuner submodules.\nModule\nFID\nParams Mem.\nLinear\n13.82 19.68M 29.02G\nConv\n13.88 22.13M 28.65G\nResPrefix [19] 14.38 21.64M 30.54G\nTable 6. CSC-Tuner ablation experiments of controllable generation task on LAION dataset. Default settings are marked in gray.\n(a) Ablation on convolution kernel size.\nKernel\nFID\nParams\nMem.\n1\n73.18\n28.82M\n34.78G\n3\n71.78\n99.11M\n35.28G\n(b) Ablation on skip connection (SC) layers.\nSC Indexes\nFID\nParams Mem.\n{0,3,4,6,7,9,11} 85.42 17.14M 34.48G\n{1,2,3, ..., 12}\n73.18 28.82M 34.78G\n(c) Ablation on tuner submodules.\nModule\nFID\nParams Mem.\nSingle Conv 73.18 28.82M 34.78G\nDual Conv\n70.54 37.82M 35.31G\nwhile maintaining a comparable number of parameters.\nB.2. CSC-Tuner structure\nWe conducted a series of ablation studies based on the mod-\nular design of the CSC-Tuner to evaluate the impact of each\ncomponent on the overall performance.\nFrom a quantitative perspective, in Tab. 6a, we can ob-\nserve that larger convolution kernels of condition encoder,\nalthough resulting in an increase in the number of param-\neters, also contribute to a certain reduction in the FID. In\nTab. 6b, omitting some of the skip connections results in\nan increase in the FID. Subsequently, as shown in Tab. 6c,\nwe ablate with altering the internal structure of the tuner by\nshifting from a single convolution layer to a dual convolu-\ntion layer with dimension reduction, resulting in improved\nFID score.\nFrom a qualitative perspective, we compared the afore-\nmentioned experimental setups and also train on a larger\ndataset (24M) under the default setting. As evident from\nFig. 11, the enlargement of the convolution kernel size ex-\npands the receptive field, achieving richer detail in the gen-\nerated images. Training with more data also benefits from\nrealistic effects. On the other hand, omitting some of the\nskip connections generally leads to a loss of image con-\ntent. The dual convolution with dimension reduction ex-\nhibits poor control over conditions, underscoring the impor-\ntance of the channel dimension in generation.\nConv Kernel=1\nConv Kernel=3\nDrop Layers\nReductionDualConv\nCondition\nMore Data\n[chess candle, gothic illustration]\n[close look up to an engine wheel ]\nFigure 11. Qualitative comparison on various CSC-Tuner structure designs.\n13\nC. Additional results\nC.1. Additional qualitative comparison\nIn Fig. 12, we present additional qualitative comparison for\nthe controllable generation task, using canny edge maps,\ndepth maps, and semantic segmentation maps as conditions,\nincluding comparisons with methods ControlNet [47], T2I-\nAdapter [28], ControlLoRA [11], and ControlNet-XS [46].\nC.2. Additional qualitative results\nIn Fig. 13, we demonstrate the results of generating images\nby extracting different conditional information from the\nsame image and using it as control conditions. In Fig. 14,\nFig. 15, and Fig. 16, we present additional qualitative re-\nsults for the controllable generation task, with conditions\nincluding canny edge map, depth map, hed boundary map,\nsemantic segmentation map, pose keypoint, color map, out-\npainting, and inpainting.\nD. Limitations and societal impacts\nThis work aims to provide users with a method for efficient\nfine-tuning and controlled synthesis under diverse condi-\ntions. The tuning stage based on the pre-trained models\nwhile freezing the backbone network, so its transfer abil-\nity depends to a large extent on the performance of the up-\nstream model. In addition, it generates results that meet\nexpectations based on the training data and the specified\nconditional inputs supplied by the users. Conversely, the\nmalicious utilization of high-risk data could potentially lead\nto the generation of misleading outcomes. This underscores\nthe importance of ethical considerations in the deployment\nof generative models to prevent the propagation of harm-\nfully biased or false information.\n14\nControlLoRA\nControlNet-XS\nT2I-Adapter\nControlNet\nCondition\nSCEdit\n[bicycle and pannier backpack]\n(a) Comparative results of generation conditioned on canny edge map.\n[agriculture, purple theme]\n(b) Comparative results of generation conditioned on semantic segmentation map.\n[short blue cat, cat motorcycle wearing a helmet, on the road]\n[close up lotus \ufb02ower, water droplets on petals, natural features]\n(c) Comparative results of generation conditioned on depth map.\nFigure 12. Additional qualitative comparison on the controllable generation of our approach with other strategies conditioned on canny\nedge maps, semantic segmentation maps, and depth maps. The areas in the boxes are enlarged for detailed comparisons.\n15\nDepth\nHED\nSegmentation\nColor\nCanny\nInpainting\nOutpainting\nPose\n[a man facing back meditating on a hill, chain of mountains in front of him]\nFigure 13. Additional qualitative results on controllable generation using the same original image for different conditions.\n[a dark brown to black spaniel dog] [a dozen chocolate chip cookies]\n[gardenia \ufb02owers in a vase]\n[parthenon with wide angle lens]\nw/ ink wash painting\nw/ pixel art\n[a medevil castle and town]\n(a) Generative results conditioned on canny edge map.\n[a wolf standing in front of a plane]\n[a dog is sitting under a tree]\n[morthern mockingbird]\n[bull in astronaut uniform]\nw/ sketch\n[powdered milk]\nw/ pixel art\n(b) Generative results conditioned on depth map.\nFigure 14. Additional qualitative results on controllable generation using canny edge map and depth map conditions.\n16\n[western steampunk]\n[French bulldog as barista]\n[woman, blue eye, blonde hair]\n[beautiful smiling girl]\nw/ sketch\n[African Elephant]\nw/ anime\n(a) Generative results conditioned on hed boundary map.\n[a dead tree in front of a cloudy sky]\n[stir-fried potatoes]\n[basketball backboard, golden hour]\n[beautiful dark brown woman]\n[dark green mech with golden inlay]\nw/ Minecraft\nw/ pixel art\n(b) Generative results conditioned on semantic segmentation map.\n[a man wearing a polo shirt]\n[goblin, dagger, fantasy]\n[a portrait of girl with blonde hair]\n[waist up half portrait of a girl]\n[an old explorer with beards]\nw/ \ufb02atillustration\nw/ Minecraft\n(c) Generative results conditioned on pose keypoint.\nFigure 15. Additional qualitative results on controllable generation using hed boundary map, semantic segmentation map, and pose\nkeypoint conditions.\n17\n[classical-style painting, lush valley]\n[coloring book]\n[colorful spring \ufb02oral bouquet]\n[anime boy]\n[showers of hearts and \ufb02owers]\nw/ LEGO\nw/ water color\n(a) Generative results conditioned on color map.\n[a photography of swissmountain]\n[the rose]\n[a tent with snowy landscape]\n[a lab worker wearing white overall]\n[a small waterfall runs]\nw/ pixel art\nw/ Minecraft\n(b) Generative results conditioned on outpainting.\n[a natural wonderland]\n[a realistic cat, vivid, beauty]\n[beauty and diversity of culture]\n[an orca jumping in the lake]\n[woman face with baseball hat]\n[zoomed up picture of a cat\u2019s eye]\n(c) Generative results conditioned on inpainting.\nFigure 16. Additional qualitative results on controllable generation using color maps, outpainting, and inpainting conditions.\n18\n"
  },
  {
    "title": "M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts",
    "link": "https://arxiv.org/pdf/2312.10763.pdf",
    "upvote": "17",
    "text": "M3DBench: Let\u2019s Instruct Large Models with Multi-modal 3D Prompts\nMingsheng Li1\nXin Chen2,\u2217\nChi Zhang2\nSijin Chen1\nHongyuan Zhu3\nFukun Yin1\nGang Yu2\nTao Chen1,\u2020\n1Fudan University\n2Tencent PCG\n3Institute for Infocomm Research (I2R) & Centre for Frontier AI Research (CFAR), A*STAR, Singapore\nhttps://github.com/OpenM3D/M3DBench\n\u2217 Project Lead\n\u2020 Corresponding Author\nR:\nObject Detection\nCan you locate objects in the scene in \nthe same category as             ?\nVisual Grounding\nMulti-Round Dialogue\nCan you describe this room \nin a detail manner?\nScene \nDescription\nR:\nDeliver a brief/detailed \ndescription of the                  .\nDense\nCaption\nVisual Question Answering\nThe door is on your left.\nStanding around at the armchair \nsituated within the <box>\u2461 and \nfacing the coffee table, what is on \nmy left?\nEmbodied\nQuestion Answering\nYou should move to the door to \nget some fresh air.\nIf I am at the object close to \n(1.87,-0.15,0.12) and want to get \nsome fresh air, where should I go?\nThe sofa is closer \nto the armchair .\nIs the sofa\u2463 closer \nto the  table\u2462 or the \narmchair?\nMulti-Region \nReasoning\nA square wooden table./The target \nobject is a square wooden dining table \nwith a light brown top. There are four \nchairs surrounding the table, one on \neach side, and they are all tan in color. \nThe placement of the table and chairs \nsuggests a dining area...\nThe room appears to be a living-dining space, \nfeaturing a variety of furniture pieces. The \ndining area houses a square dining table with a \nlight brown top, surrounded by four wooden \ndining chairs. One chair is positioned at the back \nof the table, closest to the back wall and to the \nleft of the tan couch. Another chair is against \nthe wall, facing the living room\u2026\nHuman: Looking around the room, I see a wooden dining chair at \nthe table. It's on the back side, closest to the wall.\nAI: Yes, and there's another wooden dining chair at the front of the \ntable. They seem to be part of a set.\nHuman: In front of the tan couch, is there a wooden two-tiered \ncoffee table.\nAI:  That's correct. And the tan couch is facing this coffee table. \nBehind the couch, there's a wide curtain in front of a window.\nI:\nWhat is the size of the object close to the \npointed spot <point>\u2460?\nI:\nIt is a coffee table that has a length of 1.1 \nmeters, a width of 0.5 meters, and a \nheight of 0.25 meters.\nR:\nIdentify all objects within the \npoint cloud, specifying both their \ncategories and locations.\nI:\nI:\nR:\nI:\nR:\nI:\nR:\nI:\nR:\nEmbodied Planning\nI:\nR:\n\u2460\n\u2461\n\u2463\n\u2462\nFigure 1. Examples from M3DBench, which encompasses a variety of 3D-centric tasks. The dataset supports multi-modal instructions\nthat interleave text with visual prompts and covers a variety of fundamental abilities in real-world 3D environments, such as visual percep-\ntion, scene understanding, spatial reasoning, navigation, and planning.\nAbstract\nRecently, 3D understanding has become popular to fa-\ncilitate autonomous agents to perform further decision-\nmaking. However, existing 3D datasets and methods are\noften limited to specific tasks. On the other hand, recent\nprogress in Large Language Models (LLMs) and Multi-\nmodal Language Models (MLMs) have demonstrated ex-\nceptional general language and imagery tasking perfor-\nmance. Therefore, it is interesting to unlock MLM\u2019s poten-\ntial to be 3D generalist for wider tasks. However, current\nMLMs\u2019 research has been less focused on 3D tasks due to\n1\narXiv:2312.10763v1  [cs.CV]  17 Dec 2023\na lack of large-scale 3D instruction-following datasets. In\nthis work, we introduce a comprehensive 3D instruction-\nfollowing dataset called M3DBench, which possesses the\nfollowing characteristics: 1) It supports general multi-\nmodal instructions interleaved with text, images, 3D ob-\njects, and other visual prompts. 2) It unifies diverse 3D\ntasks at both region and scene levels, covering a variety\nof fundamental abilities in real-world 3D environments.\n3) It is a large-scale 3D instruction-following dataset with\nover 320k instruction-response pairs.\nFurthermore, we\nestablish a new benchmark for assessing the performance\nof large models in understanding multi-modal 3D prompts.\nExtensive experiments demonstrate the effectiveness of our\ndataset and baseline, supporting general 3D-centric tasks,\nwhich can inspire future research.\n1. Introduction\nThe past year has witnessed remarkable success of Large\nLanguage Models(LLMs) families[20, 49, 52, 54] in ad-\ndressing various natural language processing tasks through\ngeneral instruction tuning [41].\nMulti-modal Language\nModels (MLMs), such as Flamingo [2], BLIP-2 [33],\nLLaVA [35] have progressed various visual comprehen-\nsion and reasoning tasks on 2D domain, including vi-\nsual captioning [6, 50, 58], dialogue [14] and question-\nanswering [23, 25].\nTo unlock the full potential of\nthese MLMs, it is essential to curate a well-constructed\ninstruction-following dataset [30, 35] that covers diverse\nvision language (VL) tasks, which empowers the models\nto handle these tasks without extensive modifications to\nthe architecture. However, current research on MLMs has\npredominantly overlooked 3D visual and a comprehensive\ndataset for 3D instruction tunning is missing due to the\ndaunting workload of collecting instructions in ambiguous\nand cluttered 3D environments.\nPrevious works have made efforts to construct datasets\nfor specialized 3D task, such as object detection [21, 53],\nvisual grounding [1, 12], dense captioning [1, 12], VQA [4,\n62], and navigation [3]. Consequently, most of the mod-\nels [4, 9, 13, 19, 38, 47] are specialist in only one or two\nof these tasks, potentially limiting their adaptability across\nvarious applications.\nWorks such as LAMM [64], 3D-\nLLM [24], and Chat-3D [60] have made preliminary at-\ntempts in constructing 3D instruction-following datasets,\nachieving inspiring results.\nHowever, the range of vi-\nsual tasks covered by these datasets is relatively limited,\nwhich constrains their effectiveness under diverse scenar-\nios. These datasets primarily focus on language-only in-\nstructions, posing challenges in identifying specific object\nwithin a scene. For example, there might be multiple in-\nstances of \u201cwooden chair\u201d in a scene, yet the language\nprompt pertaining to a specific wooden chair might result in\nambiguity. Furthermore, the lack of a comprehensive eval-\nuation benchmark poses challenges in accurately assessing\nthe capability of large models on 3D-centric tasks. Current\nworks, such as LAMM[64], primarily evaluate model\u2019s per-\nformance on previous benchmarks that are not designed for\nassessing MLMs with open-form output [24].\nIn this paper,\nwe introduce a comprehensive 3D\ninstruction-following dataset called M3DBench, serving\nas the foundation for developing a versatile and practical\ngeneral-purpose assistant in the real-world 3D environment.\nOur dataset comprises a variety of 3D vision-centric tasks at\nboth object and scene levels and over 320K 3D instruction-\nresponse pairs, covering fundamental capabilities such as\nvisual perception, scene understanding, spatial reasoning,\nand embodied planning, VL navigation, as depicted in\nTab. 1. Furthermore, to tackle the challenge of ambiguity in\nlanguage-only instructions, we interleave text instructions\nwith other prompts that provide rich clues about instances\nin the scene, such as numerical coordinates, pointed region,\nimage, 3D object (as shown in Fig. 1) in M3DBench, to en-\nhance the capabilities in comprehending different granular-\nity, diversity and interactivity concepts (such as \u201cthe pointed\nregion\u201d or \u201cfind the \u27e8image of a whiteboard\u27e9 in the room\u201d)\nin the multi-modal instructions.\nTo evaluate the effectiveness of M3DBench, we de-\nvelop a simple yet effective baseline model capable of pro-\ncessing interleaved multi-modal instructions, consisting of\nthree components: scene perceiver, multi-modal instruc-\ntion encoder, and LLM decoder. Furthermore, we develop\na comprehensive benchmark aimed at systematically as-\nsessing various capabilities of 3D MLMs across multiple\ndimensions with multi-modal instructions .\nThe evalua-\ntion benchmark comprises approximately 1.5K instruction-\nresponse pairs, encompassing both region-level and scene-\nlevel tasks, such as object localization, scene description,\nmulti-round dialogues, embodied planning, among others.\nEach instance comprises an instruction, a corresponding 3D\nscene, and a human-validated response. We will release\nM3DBench dataset, code, and evaluation strategies to ac-\ncelerate future research on 3D MLMs.\nTo summarize, our contributions are listed as following:\n\u2022 We introduce a large-scale 3D instruction-following\ndataset that unifies diverse region-level and scene-level\n3D-centric tasks, focusing on scene perception, under-\nstanding, reasoning, and planning.\n\u2022 We present a interleaved multi-modal instruction formula\ndesigned to enhance the granularity, diversity and interac-\ntivity of generated instructions.\n\u2022 We establish a comprehensive benchmark for evaluating\nthe capabilities of MLMs within 3D scenarios. Exten-\nsive experiments demonstrate the effectiveness of both\nthe dataset and the baseline.\n2\nDataset\nStatistics\nInstruction\nPerception\nUnderstanding and Reasoning\nPlanning\n#Instruction-\nresponse pairs\n#Average length of\ninstrucion / response\nText\nCoord\nPoint\nBox\nImage\n3D\nObject\nObject\nDetection\nVisual\nGrounding\nDense\nCaption\nVisual\nQuestion\nAnswering\nEmbodied\nQuestion\nAnswering\nMulti-\nregion\nReasoning\nScene\nDescription\nMulti-\nround\nDialogue\nEmbodied\nPlanning\nVision-\nLanguage\nNavigation\nNr3D [1]\n-\n-\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\nScanRefer [12]\n-\n-\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\nScanQA [4]\n25K\n8.77 / 2.42\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\nSQA3D [37]\n26K\n10.49 / 1.10\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u00d7\nScanScribe [73]\n-\n-\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nLAMM-3D [64]\n10K\n13.88 / 119.34\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u00d7\n\u00d7\n\u2713\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u00d7\n\u00d7\n3DLLM [24]\n202K\n43.80 / 8.11\n\u2713\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u2713\n\u2713\n\u00d7\n\u2713\n\u2713\n\u2713\n\u2713\nChat-3D [60]\n57K\n9.11 / 48.75\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\n\u00d7\n\u00d7\nM3DBench\n327K\n24.79 / 18.48\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTable 1. Comparison between M3DBench and other 3D VL datasets as well as 3D instruction datasets. M3DBench has the follow-\ning characteristics: 1) A comprehensive instruction-following dataset tailored for 3D scenes. 2) Supporting multi-modal instructions\nthat interleave text, coordinate, image, 3D object, and so on. 3) Encompassing diverse 3D visual-centric tasks that span a variety of\nfundamental abilities in real-world 3D environments, such as visual perception, scene understanding, spatial reasoning, navigation, and\nplanning.\n2. Related Work\nMulti-modal Datasets and 3D Benchmarks. The progress\nof MLMs [26, 32, 33, 48] has been greatly accelerated\nby the availability of large-scale image-text data, such as\nMS COCO Caption [17], Visual Genome [28], LAION-\n5B [51]. In order to improve models\u2019 comprehension of\nhuman instructions in visual tasks, several visual instruc-\ntion following datasets [22, 30, 35, 64] have been pro-\nposed. Additionally, while numerous studies in the field of\n3D have presented benchmark datasets for visual ground-\ning [1, 12], dense captioning [1, 12], and visual question\nanswering [4, 37], these datasets are limited to specific\ntasks. In this paper, we propose a comprehensive dataset\nthat supports interleaved multi-modal instructions and cov-\ners various 3D-centric tasks, including visual grounding,\ndense caption, embodied question answering, multi-region\nreasoning, scene description, multi-round dialogue, and so\non. Refer to Tab. 1 for a detailed comparison between our\ndataset and other 3D VL datasets [1, 4, 12, 37] as well as\nexiting 3D visual instruction datasets [24, 60, 64]. Further-\nmore, rather than providing demonstrations only, we evalu-\nate diverse tasks with quantitative results.\nMulti-modal Foundation Models. With the triumph of\nLLMs [8, 20, 49, 54, 69], recent studies [2, 31, 33, 35] start\nto explore Vision Language Models (VLMs), extending the\ncapabilities of LLMs in solving diverse visual-related tasks.\nEarly attempts include Flamingo [2], which incorporates\nvisual features through gated cross-attention dense blocks,\nand BLIP-2 [33], which uses a Q-former as a bridge to rec-\noncile the modality gap between the frozen image encoder\nand LLMs. In order to enhance the VLMs\u2019 comprehen-\nsion of human instructions, several visual instruction tun-\ning methods [31, 35] have been proposed. Addressing the\nadaptation of LLMs to 3D-related tasks, LAMM [64] uses\na simple projection layer to connect the 3d encoder and\nLLM. 3D-LLM [24] utilizes point clouds and text instruc-\ntions as input, leveraging 2D VLMs as backbones. How-\never, prior works that attempt to integrate the 3D world into\nMFMs have exhibited limitations in handling interleaved\nmulti-modal instructions and accomplishing various tasks.\nIn this work, we propose to improve the abilities of MFMs\nin addressing diverse 3D-centric tasks and handling inter-\nleaved multi-modal instructions with on a comprehensive\n3D instruction-following dataset.\n3D Vision-language Learning. Recently, there has been\ngrowing interest in 3D VL learning. While various 3D rep-\nresentations exist, including voxels, point clouds, and neu-\nral fields, previous works have primarily focused on point\ncloud-text data. Among those, 3D dense captioning [18, 68]\naims to generate description of target object within a 3D\nscene, while 3D visual grounding [63, 65, 71] involves iden-\ntifying object in a scene based on textual description. In\n3D question answering [4, 62], models are required to an-\nswer questions based on the visual information. Although\nthese works have achieved impressive results in connecting\n3D vision and language, they heavily rely on task-specific\nmodel design. In contrast, we develop a unified baseline\nmodel capable of decoding multiple 3D-related tasks with-\nout the need for specific model designs. Furthermore, we\nestablish a comprehensive benchmark to assess the model\u2019s\nperformance across various tasks.\n3. Multi-modal Instruction Dataset\nWe introduce the strategy for constructing the multi-\nmodal 3D instruction dataset (details in Sec. 3.1), along\nwith the design formula for interleaved multi-modal instruc-\ntions (details in Sec. 3.2). We then detail the tasks at both\nthe region-level and scene-level covered by the dataset in\nSec. 3.3, followed by a statistical and analytical examina-\ntion of the dataset in Sec. 3.4.\n3.1. Dataset Construction\nTo\nconstruct\na\ncomprehensive\n3D\nmulti-modal\ninstruction-following\ndataset,\nwe\nutilize\nexisting\ndatasets [1, 10\u201312, 21, 27, 29, 67] for 3D-only [47, 66]\nand 3D-language tasks [4, 18], and collect extensive\ninstruction-response data by prompting LLMs.\nFor 3D-\n3\nonly tasks, like object detection and visual grounding, we\ndesigned instruction and response templates corresponding\nto specific tasks.\nSpecifically, we collect instructions\nfor 3D-only tasks by providing task descriptions and\nspecifying the desired output format. Responses interleave\nobject coordinates (follow the specified output format in\nthe instructions) and text. For the 3D-language tasks, such\nas dialogue and question answering, we provided object\nattributes, textual descriptions, and manually written task\nconstruction instructions as well as few-shot in-context\nlearning examples to the GPT-API [40, 52] to generate\ntask-specific instruction data.\nAlthough most responses generated by GPT-API [40, 52]\nare of high quality, some were irrelevant to the instruction.\nFor instance, certain responses may refer to information de-\nrived from the provided textual descriptions. To improve\nthe quality of the 3D instruction-following data, we employ\npattern matching with specific keywords to filter out such\nresponses.\n3.2. Interleaved Multi-modal Instruction\nWe design four types of visual prompts, namely, point-\nlevel prompt (user click), box-level prompt (pointed re-\ngion), image prompt, and 3D object prompt. For point-level\nprompt, we sample points near the specified region of the\nscene and randomly select one. The box-level prompts are\nderived from the ground truth bounding boxes in the 3D\nscene. Image prompts consist of corresponding image re-\ngions with 3D scenes, publicly available image data[29],\nand synthetic images[45]. Regarding 3D objects, we se-\nlect instances from 3D scenes with ground truth per-point\nannotations, additionally collecting objects from [11]. Fur-\nthermore, we provide specific descriptions, such as \u201cin the\npointed region\u201d in instructions to guide the large model to\nidentify visual prompts within the scene. Finally, an inter-\nleaved multi-modal instruction I can be defined as an or-\ndered sequence composed of text and visual prompts, rep-\nresented as I = [x1, x2, . . . , xM], where each element xi\nin {text, point, box, image, object3d}. Additional details\ncan be found in the supplementary materials.\n3.3. Task Coverage\nOur dataset introduces a unified instruction-response\nformat to cover diverse 3D-centric tasks, encompassing es-\nsential capabilities ranging from visual perception and un-\nderstanding to reasoning and planning (detailed in Tab. 1).\n3.3.1\nVisual Perception\nObject Detection(OD) aims at identifying and locating all\nthe objects of interest in a point cloud [39, 42]. Here, we\ntransform the classic OD task into an instruction-following\nformat by providing task descriptions and specifying the de-\nsired output format. Following LAMM [64], we manually\ndesign a set of instruction-response templates with place-\nholders, and each instruction includes the expected out-\nput format. The instruction and response templates can be\nfound in the supplementary.\nVisual Grounding(VG) involves identifying the target ob-\nject in the scene based on a natural language referring ex-\npression [61, 66]. In M3DBench, we expand the task format\nof VG. Specifically, our description information for query-\ning extends beyond textual input and includes various vi-\nsual prompts, such as coordinate, clicked point, image, 3D\nobject, and so on. Moreover, our output is not limited to\nlocating a single target object but can also involve finding\nobjects belonging to the same category.\n3.3.2\nScene Understanding and Reasoning\nDense Caption(DC) requires a model to generate natural\nlanguage descriptions for each object [16, 18]. However,\nexisting DC datasets like ScanRefer [12] and Nr3D [1] pro-\nvide only short captions. In M3DBench, we reconstruct the\nDC datasets and introduce terms like brief or detailed in in-\nstruction to generate either concise title or detailed descrip-\ntion for the object, which allows for better control over the\ngranularity of the generated caption. The instruction tem-\nplates can be found in the supplementary.\nVisual Question Answering(VQA) is a task that requires\nthe model to correctly answer a given question based on the\ninformation present in a visual scene [4, 44]. In this work,\nwe curate a collection of free-form, open-ended question-\nanswer pairs using publicly available 3D-language datasets.\nThese VQA pairs cover various aspects at both the object\nlevel and scene level, including instance locations and at-\ntributes, object counts, room functions, and more.\nEmbodied Question Answering(EQA). Unlike traditional\nVQA tasks [4, 44] that primarily focus on answering ques-\ntions related to global information, EQA requires the agent\nto first comprehend and analyze the surrounding environ-\nment to answer questions under that situation [37]. To col-\nlect instruction-following data for EQA, we start by ran-\ndomly selecting a location within the scene and choosing\nto face a nearby object for reference direction, and then\nprompt GPT-4 to generate EQA pairs based on the given\nsituation and text information.\nMulti-region Reasoning(MR). Datasets such as DC [1, 12]\nfacilitate understanding and reasoning for individual ob-\njects. However, reasoning between distinct regions is often\noverlooked. For instance, inquiries about the spatial rela-\ntionship between \u27e8region 1\u27e9 and \u27e8region 2\u27e9. Here, we intro-\nduce MR, which is designed to enhance fine-grained com-\nprehension of multiple regions of interest. Our methodol-\n4\n(a) Instruction distribution\n(d) # of words in responses\n(c) # of words in instructions\n \n \n \n \n \n(b) Word cloud of responses\nFigure 2. The statistics of the M3DBench. (a) The distribution of instructions based on the first word, where the inner circle of the\ngraph represents the frequency of the first word\u2019s occurrence, and the outer circle shows the frequency of verbs and nouns appearing in\nthe instructions corresponding to that first word. (b) The word cloud of responses. (c) The distribution of instruction length. (d) The\ndistribution of response length.\nogy involves feeding object location, descriptions [67], few-\nshot learning examples, and language instructions to GPT-4\nto obtain corresponding responses.\nScene Description(SD). Unlike DC [16, 18], which gen-\nerates a caption for each object, SD focuses on producing\ndescriptions of the entire scene, extending the descriptive\nability of MLMs from the region level to the scene level. To\nconstruct the instruction-following data for SD, we extract\n3D bounding box annotations from ScanNet [21] and dense\ncaptions from the 3D VL datasets [1, 12] as data sources.\nBy prompting the GPT-4, we can generate detailed descrip-\ntions for each scene.\nMulti-round Dialogue(MD). To construct MDs, we make\nuse of 3D VL datasets and follow a similar approach to that\nused in LLAVA [35]. During this process, we prompt GPT-\n4 to generate MDs in a self-questioning and self-answering\nformat, taking advantage of coordinate information and lan-\nguage descriptions from [1, 12].\n3.3.3\nPlanning and Navigation\nEmbodied Planning(EP). Unlike EQA, which primarily\nfocuses on answering questions, EP requires agents to pos-\nsess planning and decision-making capabilities.\nSpecifi-\ncally, the agent needs to perceive the environment, under-\nstand user\u2019s intentions, and generate appropriate action in-\nstructions to achieve predefined goals [24].\nVision Language Navigation(NLV) require an agent to\nnavigate and move in a real-world 3D environment based on\nhuman language instructions. We leverage annotations from\nexisting 3D-language navigation tasks [27] and transform\nthem into an instruction-following format. Instructions are\nexpressed in natural language, while the corresponding re-\nsponse is a trajectory formed by points in space.\n3.4. Dataset Statistics and Analysis\nTab. 1 presents the statistics of M3DBench. M3DBench\ncontains over 320K pairs of instruction-following data.\nAmong these pairs, more than 138K instructions include the\ninterleaved multi-modal prompts we proposed.\nTo assess the diversity of generated instructions, we ana-\nlyze the distribution of instructions based on the first word,\nas shown in Fig. 2 (a). Specifically, we extract the first word\nof each instruction and collected instructions starting with\nthat word. Then we parse the instructions using the Natural\nLanguage Toolkit [7], performing processes like tokeniza-\ntion and part-of-speech tagging to extract nouns and verbs\nfrom instructions. The findings indicate that instructions\nin M3DBench are diverse, including various types such as\n\u201cWhat\u201d (query), \u201cCan\u201d (request), \u201cIs\u201d (confirmation), \u201cI\u201d\n(first-person), \u201cWhere\u201d (location), and so on. Analyzing\nthe word cloud of responses, as depicted in Fig. 2 (b), we\nobserve answers pertaining to shape, color, count, action,\nobject category, spatial relations, and so on. Furthermore,\nwe demonstrated diversity in the lengths of instructions and\nresponses, as illustrated in Fig. 2 (c) and Fig. 2 (d).\n4. Multi-modal Instruction Tuning\nWe introduce a baseline model that connects scenes with\ninterleaved multi-modal instructions and accomplishes di-\nverse tasks using a unified decoder. As shown in Fig. 3, the\nframework consists of three parts: scene perceiver, multi-\nmodal instruction encoder, and LLM. First, the 3D scene is\nprocessed by the scene perceiver, and the features are then\nprojected into the same feature space as the language em-\nbedding using a trainable projection layer (Sec. 4.1). Simul-\ntaneously, prompts from different modalities within instruc-\ntions are encoded using their corresponding prompt encoder\n(Sec. 4.2). Then the visual and instruction tokens are con-\ncatenated and fed into the LLM (Sec. 4.3). Next, we will\nprovide a detailed description of each module.\n5\nImage\nObject\nLarge Language Model\nText\nThere are two pieces of paper \non the small beige table.\nA large, L-shaped couch.\n3D Scene\nPoint\nor Box\nPrompt Encoder\nTokenizer\n<loc>\nUser: Describe the \npointed region ...\nOutput\n3D Perceiver \nFigure 3. Overview of our baseline model. We utilize scene perceiver to extract scene tokens from 3D visual input. Multi-modal instruc-\ntions are transformed into corresponding instruction tokens via their respective encoders. The scene tokens and multi-modal instruction\ntokens are then concatenated and fed into a frozen LLM, which generates the corresponding responses subsequently. During the training\nprocess, only the projectors are updated.\n4.1. 3D Scene Perceiver\nGiven the point cloud of a scene, denoted as P, we em-\nploy a pre-trained 3D encoder to extract 3D feature:\nfs = E3D(P).\n(1)\nSimilar to LLAVA [35], we alao utilize a trainable visual\nfeature projection matrix W3D to project the visual features\ninto the language embedding space and obtain scene tokens:\nXs = W3D \u00b7 fs.\n(2)\nThe scene embeddings are represented as Xs = {xn\ns }N\nn=1,\nwhere xn\ns \u2208 Rd and N represents the number of visual to-\nkens. d represents the dimension of hidden states in LLM.\n4.2. Multi-modal Instruction Encoder\nThere are a total of six types of prompt formats (Tab. 1)\nin the interleaved multi-modal instructions: text, numerical\ncoordinate, user click (point), pointed region (box), image,\nand 3D object. We treat numerical coordinate as a spe-\ncific text [15, 72] and use the tokenizer and word embed-\nding from LLM to obtain corresponding tokens. For user\nclick and pointed region, we utilize two learnable projec-\ntion matrices to extract point-level and box-level tokens, re-\nspectively. In the case of image prompt, we employ the\nfrozen CLIP [48] to extract image features, followed by\na pre-trained projector from LLaVa [35] to compute im-\nage tokens.\nFor 3D object input, we downsample them\nto 1024 points and normalize their coordinates into a unit\nsphere [73]. Then a pre-trained encoder is used to extract\nobject\u2019s features, and a Feed Forward Network (FFN) is in-\nserted between the encoder and LLM to adjust these tokens.\n4.3. LLM Decoder\nWe utilize the pre-trained LLM [20, 55, 69] as a uni-\nfied decoder for various vision-centric tasks. To accomplish\nthis, we employ a 3D scene perceiver (Sec. 4.1) to encode\nthe input scene P into discrete scene tokens Xs = {xn\ns }N\nn=1.\nThese tokens are then concatenated with the multi-modal\ninstruction tokens Xi = {xn\ni }M\nn=1. LLM takes both the\nscene tokens and the multi-modal instruction tokens as in-\nput and predicts the probability distribution of the output\ntoken Xo = {xn\no }L\nn=1 in an auto-regressive manner:\nP\u03b8(Xo|Xs, Xi) =\nY\nn\nP\u03b8(xl\no|x<l\no ; Xs, Xi).\n(3)\nFurthermore, for tasks that rely on coordinates for assess-\nment, such as visual grounding, we decouple them from the\noutput of LLMs (detailed in the supplements). This simple\napproach enables us to develop a unified framework for a\nwide range of 3D-only tasks without the need for modifica-\ntions to the existing LLMs [8, 54, 69].\n4.4. Training Strategy\nThe training objective is to maximize the likelihood of\ngenerating this target response sequence Xo = {xn\no }L\nn=1,\ngiven the visual input Xs and multi-modal instruction Xi:\nL\u03b8 = \u2212\nL\nX\nn=1\nlog P\u03b8(xl\no|x<l\no ; Xs, Xi).\n(4)\nHere, \u03b8 represents the trainable parameters. Note that dur-\ning training, we freeze the 3D encoder, image encoder, as\nwell as language decoder, and only train all the projection\nlayers to enable rapid iterations. Exploring alternative ar-\nchitecture or refining the training strategy could potentially\nyield further improvements. We leave this as a direction for\nfuture work.\n6\nTask\n3D Vision Encoder\nLLM Decoder\nBLEU-1\u2191\nBLEU-2\u2191\nBLEU-3\u2191\nBLEU-4\u2191\nROUGE\u2191\nMETEOR\u2191\nCIDEr\u2191\nDense Caption\nPointnet++ [46]\nOPT-6.7B [69]\n3.56\n1.43\n0.52\n0.21\n14.18\n9.79\n17.01\nLLaMA-2-7B [55]\n10.60\n4.53\n1.70\n0.73\n18.70\n13.40\n22.05\nVicuna-7B-v1.5 [20]\n2.97\n1.04\n0.32\n0.00\n11.78\n9.04\n13.88\nTransformer [56]\nOPT-6.7B [69]\n10.72\n4.44\n1.45\n0.0\n14.58\n10.35\n23.76\nLLaMA-2-7B [55]\n10.07\n3.71\n1.38\n0.0\n17.32\n12.03\n20.72\nVicuna-7B-v1.5 [20]\n11.96\n4.38\n1.28\n0.0\n14.13\n9.46\n23.72\nVisual Question Answering\nPointnet++ [46]\nOPT-6.7B [69]\n57.45\n49.48\n43.57\n38.78\n58.34\n30.30\n336.96\nLLaMA-2-7B [55]\n61.01\n53.35\n47.63\n43.00\n61.59\n32.05\n379.05\nVicuna-7B-v1.5 [20]\n46.30\n38.13\n32.20\n27.56\n51.55\n27.03\n239.98\nTransformer [56]\nOPT-6.7B [69]\n57.26\n50.35\n44.97\n40.50\n59.55\n30.64\n365.60\nLLaMA-2-7B [55]\n60.23\n52.41\n47.02\n42.61\n59.24\n30.96\n356.42\nVicuna-7B-v1.5 [20]\n17.77\n14.22\n11.86\n10.07\n22.12\n11.32\n95.98\nEmbodied Question Answering\nPointnet++ [46]\nOPT-6.7B [69]\n47.55\n37.69\n30.91\n24.44\n49.17\n26.04\n212.12\nLLaMA-2-7B [55]\n45.85\n35.92\n29.32\n22.79\n48.34\n24.89\n194.09\nVicuna-7B-v1.5 [20]\n21.09\n15.61\n12.28\n9.41\n44.06\n20.55\n169.72\nTransformer [56]\nOPT-6.7B [69]\n47.37\n37.86\n31.33\n24.76\n50.83\n25.95\n218.01\nLLaMA-2-7B [55]\n44.20\n33.86\n27.49\n21.58\n45.83\n22.74\n179.33\nVicuna-7B-v1.5 [20]\n38.24\n29.71\n24.63\n19.64\n40.62\n21.00\n155.12\nMulti-region Reasoning\nPointnet++ [46]\nOPT-6.7B [69]\n57.53\n50.03\n43.57\n38.27\n61.23\n33.74\n363.87\nLLaMA-2-7B [55]\n56.24\n49.32\n43.42\n38.46\n61.48\n34.01\n378.17\nVicuna-7B-v1.5 [20]\n47.98\n39.18\n32.28\n26.82\n49.87\n27.59\n212.93\nTransformer [56]\nOPT-6.7B [69]\n36.92\n30.78\n25.91\n21.60\n44.51\n24.27\n240.89\nLLaMA-2-7B [55]\n55.00\n47.88\n42.31\n37.60\n59.90\n32.56\n351.96\nVicuna-7B-v1.5 [20]\n21.96\n17.21\n13.87\n11.06\n27.07\n12.68\n95.40\nEobodied Planning\nPointnet++ [46]\nOPT-6.7B [69]\n49.22\n41.11\n35.04\n29.71\n50.90\n26.65\n133.94\nLLaMA-2-7B [55]\n57.66\n50.18\n44.86\n40.76\n56.46\n29.77\n253.09\nVicuna-7B-v1.5 [20]\n21.68\n15.27\n10.87\n8.10\n32.73\n19.78\n83.39\nTransformer [56]\nOPT-6.7B [69]\n59.47\n53.24\n48.08\n43.46\n61.14\n33.34\n213.15\nLLaMA-2-7B [55]\n52.98\n45.17\n39.05\n34.27\n49.95\n28.70\n171.51\nVicuna-7B-v1.5 [20]\n37.50\n30.71\n25.33\n20.54\n38.55\n21.50\n114.91\nTable 2. Benchmark for multiple tasks: Dense Caption (DC), Visual Question Answering (VQA), Embodied Question Answering\n(EQA), Multi-region Reasoning (MR), Embodied Planning (EP). We present the performance of baseline methods on our evaluation\ndataset. \u2191 means the higher, the better.\n5. Experiments\nWe first introduce the baseline model, metrics, and im-\nplementation details in Sec. 5.1. Additionally, we provide\na benchmark on 3D scene understanding, reasoning and de-\nscription in Sec. 5.2. Finally, we showcase some visualiza-\ntion results in Sec. 5.3. More details, quantitative results,\nand qualitative examples are provided in supplements.\n5.1. Baseline, Metrics, and Implementations\nBaseline.\nSince no prior method that works out of the\nbox with our interleaved multi-modal instruction setup,\nwe develop several variant models as baseline based on\nLLM [20, 55, 69] to accommodate M3DBench. Specifi-\ncally, we incorporate two different types of 3D encoders,\nbased on PointNet++ [46] and Transformer [56], into our\nbaseline model. Furthermore, we consider three versions of\nLLMs as our language decoder: OPT-6.7B [69], LLaMA-\n2-7B [55], and Vicuna-7B-v1.5 [20]. After end-to-end in-\nstruction tuning, we evaluate baseline models on the evalu-\nation dataset to assess their effectiveness.\nEvaluation Metrics. The evaluation metrics include both\ntraditional and GPT metrics. Traditional metrics, such as\nCiDEr [57], METEOR [5], Acc@0.25IoU [12], and so on,\nare used to measure the model\u2019s performance on specific\ntasks. For a more comprehensive evaluation of the models\u2019\ninstruction-following abilities, we employ GPT-4 to assess\nthe quality of the different variants\u2019 responses. Specifically,\nwe provide GPT-4 with the answers generated by different\nvariant models, the reference answers, and evaluation re-\nquirements. GPT-4 evaluates these responses and assigns a\nscore ranging from 0 to 100. A higher average score indi-\ncates better performance of the model. Furthermore, we re-\nquest GPT-4 to provide justifications for the scoring results,\nwhich helps us better judge the validity of the evaluation.\nImplementations. Following previous works in 3D learn-\ning [16, 38], we downsample each 3D scene to 40,000\npoints as our scene input. For the PointNet++-based 3D\nencoder, we initialize it with the checkpoint obtained from\nDepth Contrast [70]. As for the Transformer-based encoder,\nwe employ the checkpoint from Vote2Cap-DETR [16]. Ad-\nditionally, we use the pre-trained encoder ViT-L/14 [48] as\nour image feature encoder. We train all the baseline mod-\nels using the Adam optimizer [36] with a cosine anneal-\ning scheduler where the learning rate decays from 10\u22125 to\n10\u22126. Our batch size is set to 2 during training, utilizing 4\nNvidia A100 (40G) GPUs, which allows us to complete the\ntraining within 2 days.\n7\n5.2. Quantitative Evaluation\nUnderstanding, Reasoning, and Planning. To establish\na benchmark for scene understanding, reasoning, and plan-\nning, we comprehensively evaluated six variant models and\nreported the quantitative results on our evaluation dataset.\nTab. 2 presents the performance of baselines across five\ntasks: Dense Captioning (DC), Visual Question Answer-\ning (VQA), Embodied Question Answering (EQA), Multi-\nregion Reasoning (MR), and Embodied Planning (EP). We\nemployed BLEU 1-4 [43], ROUGE-L [34], METEOR [5],\nand CiDEr [57] as evaluation metrics.\nAnalyzing the results, one can see that when using the\nsame language decoder, the Pointnet++ [46]-based models\nunderperformed compared to the Transformer [41]-based\nmodels in the DC and EP tasks, while outperformed them\nin the MR task. However, upon switching the language de-\ncoder while keeping the 3D encoder constant, Vicuna-7B-\nv1.5 [20] exhibited lower overall performance compared to\nother LLMs across almost all tasks. The evaluation of our\nbenchmark dataset suggests a diversity in the performance\nof MLMs, with each demonstrating unique strengths and\nweaknesses across diverse tasks. Moreover, the subopti-\nmal performance of current baseline models across various\ntasks offers potential direction for further development of\n3D MLMs.\nFor instance, enhancing the performance of\nMLMs on benchmark tasks such as scene understanding,\nperception, and planning is crucial and we leave them for\nfuture work to explore.\n3D Vision Encoder\nLLM Decoder\nGPT-4 Score\nPointnet++ [46]\nOPT-6.7B [69]\n9.87\nLLaMA-2-7B [55]\n27.89\nVicuna-7B-v1.5 [20]\n32.37\nTransformer [56]\nOPT-6.7B [69]\n16.84\nLLaMA-2-7B [55]\n27.37\nVicuna-7B-v1.5 [20]\n29.08\nTable 3. Benchmark for detailed description. In practice, we\nrandomly select 39 scenes and provide detailed descriptions gen-\nerated by GPT-4 for each scene. We then assess the relative scores\nachieved by different variants. Responses generated by variant\nmodels and GPT-4\u2019 descriptions are fed back into GPT-4 for com-\nparative analysis and scoring, along with the provision of relevant\nexplanations for each variant\u2019s answer. Experiments demonstrate\nthat the model based on Vicuna-7B-V1.5 [20] demonstrated supe-\nrior performance.\nDetailed Description. As shown in Tab. 3, regarding de-\ntailed description capabilities, utilizing PointNet++ [46] as\nthe scene encoder and Vicuna-7B-V1.5 [20] as the language\ndecoder yielded the best performance, surpassing the sub-\noptimal variant by a margin of +3.29 points. Another ob-\nWhich object is closer to the double \ndoors, the trash can or the TV?                \nThe trash can is closer to the double \ndoors than the TV. \nWhat is the position of the mirror \nfound within the region <box> \nrelative to the shower curtain?\nThe mirror is located on the wall to \nthe right of the shower curtain. \nDeliver a brief, straightforward \ndescription of the toolbox situated \nnear the pointed point <point>.\nA small, rectangular toolbox.\nIs the \u201c           \u201d next to a desk?\nYes, the window is next to the desk .\nStanding at the door and facing the \nshelf, is the window visible?\nYes, the window is visible. \nWhat is the position of the door \nrelative to the closet?\nThe door is located to the right of \nthe closet.\nFigure 4. Qualitative Results. We provide visualization results\non various 3D-centric tasks in diverse 3D environments. Orange\nhighlights the wrong answer.\nservation is that all variants based on OPT [69] demon-\nstrated relatively lower performance. Furthermore, we note\nthat overall, all baseline models demonstrate inferior per-\nformance, suggesting that current baseline models possess\nlimited capabilities in handling detailed descriptions.\nIn\nsupplements, we provide a qualitative presentation of the\ndescription results and the criteria for GPT-4 scoring.\n5.3. Qualitative Results\nWe showcase some qualitative examples of our base-\nline model on the evaluation dataset in Fig. 4. One can see\nthat our proposed method, trained on M3DBench, is capa-\nble of performing corresponding tasks under a variety of\ninterleaved multi-modal instructions.\n8\n6. Conclusion\nIn this paper, we present M3DBench, a comprehensive\nmulti-modal 3D instruction-following dataset, designed to\nfacilitate the development of MLMs in the 3D domain.\nM3DBench encompasses a wide range of 3D vision-centric\ntasks and over 320K pairs of 3D instruction-following pairs,\ncovering fundamental functionalities such as visual percep-\ntion, scene understanding, spatial reasoning, planning, and\nnavigation.\nAdditionally, M3DBench introduces a novel\nmulti-modal prompting scheme, interweaving language in-\nstruction with coordinate, image, pointed region, and other\nvisual prompts.\nWe also develop a simple yet efficient\nbaseline model to validate the effectiveness of M3DBench,\nproviding benchmarks for multiple tasks. Comprehensive\nquantitative and qualitative results demonstrate that models\ntrained with M3DBench can successfully follow human in-\nstructions and complete 3D visual-related tasks. We hope\nthat our proposed multi-modal 3D instruction dataset, base-\nline model, and benchmarks will inspire and fuel future ex-\nplorations in the field of 3D MLMs.\nReferences\n[1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed\nElhoseiny, and Leonidas Guibas. Referit3d: Neural listeners\nfor fine-grained 3d object identification in real-world scenes.\nIn Computer Vision\u2013ECCV 2020: 16th European Confer-\nence, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part\nI 16, pages 422\u2013440. Springer, 2020. 2, 3, 4, 5, 16\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716\u201323736,\n2022. 2, 3\n[3] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark\nJohnson, Niko S\u00a8underhauf, Ian Reid, Stephen Gould, and\nAnton Van Den Hengel. Vision-and-language navigation: In-\nterpreting visually-grounded navigation instructions in real\nenvironments.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3674\u20133683,\n2018. 2\n[4] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki\nKawanabe. Scanqa: 3d question answering for spatial scene\nunderstanding. In proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 19129\u2013\n19139, 2022. 2, 3, 4\n[5] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic\nmetric for mt evaluation with improved correlation with hu-\nman judgments. In Proceedings of the acl workshop on in-\ntrinsic and extrinsic evaluation measures for machine trans-\nlation and/or summarization, pages 65\u201372, 2005. 7, 8, 25\n[6] Simone Bianco, Luigi Celona, Marco Donzella, and Paolo\nNapoletano.\nImproving image captioning descriptive-\nness by ranking and llm-based fusion.\narXiv preprint\narXiv:2306.11593, 2023. 2\n[7] Steven Bird.\nNltk: the natural language toolkit.\nIn Pro-\nceedings of the COLING/ACL 2006 Interactive Presentation\nSessions, pages 69\u201372, 2006. 5\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020. 3, 6, 16\n[9] Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong\nXu. 3djcg: A unified framework for joint dense captioning\nand visual grounding on 3d point clouds. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16464\u201316473, 2022. 2\n[10] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej\nHalber, Matthias Niessner, Manolis Savva, Shuran Song,\nAndy Zeng, and Yinda Zhang.\nMatterport3d: Learning\nfrom rgb-d data in indoor environments.\narXiv preprint\narXiv:1709.06158, 2017. 3, 16\n[11] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapenet:\nAn information-rich 3d model repository.\narXiv preprint\narXiv:1512.03012, 2015. 4\n[12] Dave Zhenyu Chen, Angel X Chang, and Matthias Nie\u00dfner.\nScanrefer: 3d object localization in rgb-d scans using natural\nlanguage. In European conference on computer vision, pages\n202\u2013221. Springer, 2020. 2, 3, 4, 5, 7, 16\n[13] Dave Zhenyu Chen, Qirui Wu, Matthias Nie\u00dfner, and An-\ngel X Chang. D 3 net: A unified speaker-listener architecture\nfor 3d dense captioning and visual grounding. In European\nConference on Computer Vision, pages 487\u2013505. Springer,\n2022. 2\n[14] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang,\nJing Shi, Shuang Xu, and Bo Xu. X-llm: Bootstrapping ad-\nvanced large language models by treating multi-modalities as\nforeign languages. arXiv preprint arXiv:2305.04160, 2023.\n2\n[15] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing multi-\nmodal llm\u2019s referential dialogue magic.\narXiv preprint\narXiv:2306.15195, 2023. 6\n[16] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang\nYu, and Tao Chen.\nEnd-to-end 3d dense captioning with\nvote2cap-detr. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 11124\u2013\n11133, 2023. 4, 5, 7, 25\n[17] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\ntam, Saurabh Gupta, Piotr Doll\u00b4ar, and C Lawrence Zitnick.\nMicrosoft coco captions:\nData collection and evaluation\nserver. arXiv preprint arXiv:1504.00325, 2015. 3\n[18] Zhenyu Chen, Ali Gholami, Matthias Nie\u00dfner, and Angel X\nChang. Scan2cap: Context-aware dense captioning in rgb-\nd scans.\nIn Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 3193\u20133203,\n2021. 3, 4, 5\n[19] Zhenyu Chen,\nRonghang Hu,\nXinlei Chen,\nMatthias\nNie\u00dfner, and Angel X Chang.\nUnit3d: A unified trans-\nformer for 3d dense captioning and visual grounding.\nIn\n9\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 18109\u201318119, 2023. 2\n[20] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao\nWu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\nZhuang, Joseph E Gonzalez, et al. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality.\nSee\nhttps://vicuna. lmsys. org (accessed 14 April 2023), 2023.\n2, 3, 6, 7, 8, 24, 25\n[21] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes.\nIn\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5828\u20135839, 2017. 2, 3, 5, 16\n[22] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning,\n2023. 3\n[23] Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Boyang Li, Dacheng Tao, and Steven Hoi. From im-\nages to textual prompts: Zero-shot visual question answer-\ning with frozen large language models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10867\u201310877, 2023. 2\n[24] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng,\nYilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Inject-\ning the 3d world into large language models. arXiv preprint\narXiv:2307.12981, 2023. 2, 3, 5\n[25] Wenbo Hu, Yifan Xu, Y Li, W Li, Z Chen, and Z Tu. Bliva:\nA simple multimodal llm for better handling of text-rich vi-\nsual questions. arXiv preprint arXiv:2308.09936, 2023. 2\n[26] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-\nand-language transformer without convolution or region su-\npervision. In International Conference on Machine Learn-\ning, pages 5583\u20135594. PMLR, 2021. 3\n[27] Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra,\nand Stefan Lee. Beyond the nav-graph: Vision-and-language\nnavigation in continuous environments. In Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23\u201328, 2020, Proceedings, Part XXVIII 16, pages 104\u2013\n120. Springer, 2020. 3, 5, 16\n[28] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al.\nVisual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International journal of computer vision,\n123:32\u201373, 2017. 3\n[29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. Advances in neural information processing systems,\n25, 2012. 3, 4\n[30] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi\nPu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-\nit: Multi-modal in-context instruction tuning. arXiv preprint\narXiv:2306.05425, 2023. 2, 3\n[31] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter:\nA multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023. 3\n[32] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In In-\nternational Conference on Machine Learning, pages 12888\u2013\n12900. PMLR, 2022. 3\n[33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 2, 3\n[34] Chin-Yew Lin. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, pages\n74\u201381, 2004. 8, 25\n[35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 2, 3, 5, 6, 25\n[36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 7\n[37] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yi-\ntao Liang, Song-Chun Zhu, and Siyuan Huang.\nSqa3d:\nSituated question answering in 3d scenes.\narXiv preprint\narXiv:2210.07474, 2022. 3, 4\n[38] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-to-\nend transformer model for 3d object detection. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 2906\u20132917, 2021. 2, 7\n[39] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-to-\nend transformer model for 3d object detection. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 2906\u20132917, 2021. 4\n[40] R OpenAI. Gpt-4 technical report. arxiv 2303.08774. View\nin Article, 2, 2023. 4, 16, 24, 26\n[41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al.\nTraining lan-\nguage models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022. 2, 8\n[42] Xuran Pan, Zhuofan Xia, Shiji Song, Li Erran Li, and Gao\nHuang. 3d object detection with pointformer. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 7463\u20137472, 2021. 4\n[43] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\ntranslation. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, pages 311\u2013318,\n2002. 8, 25\n[44] Maria Parelli, Alexandros Delitzas, Nikolas Hars, Geor-\ngios Vlassis, Sotirios Anagnostidis, Gregor Bachmann, and\nThomas Hofmann. Clip-guided vision-language pre-training\nfor question answering in 3d scenes.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5606\u20135611, 2023. 4\n[45] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\n10\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 4\n[46] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. Advances in neural information\nprocessing systems, 30, 2017. 7, 8, 24, 25\n[47] Charles R Qi, Or Litany, Kaiming He, and Leonidas J\nGuibas. Deep hough voting for 3d object detection in point\nclouds. In proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 9277\u20139286, 2019. 2, 3\n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3, 6, 7, 25\n[49] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. The Journal of Machine\nLearning Research, 21(1):5485\u20135551, 2020. 2, 3\n[50] Noam Rotstein, David Bensaid, Shaked Brody, Roy Ganz,\nand Ron Kimmel. Fusecap: Leveraging large language mod-\nels to fuse visual data into enriched image captions. arXiv\npreprint arXiv:2305.17718, 2023. 2\n[51] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 3\n[52] John Schulman, Barret Zoph, Christina Kim, Jacob Hilton,\nJacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam\nFedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Opti-\nmizing language models for dialogue. OpenAI blog, 2022.\n2, 4\n[53] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.\nSun rgb-d: A rgb-d scene understanding benchmark suite. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 567\u2013576, 2015. 2\n[54] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 2, 3, 6\n[55] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 6, 7, 8, 24, 25, 26\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 7, 8, 24, 25, 26\n[57] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. Cider: Consensus-based image description evalua-\ntion. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4566\u20134575, 2015. 7,\n8, 25\n[58] Teng Wang, Jinrui Zhang, Junjie Fei, Yixiao Ge, Hao\nZheng, Yunlong Tang, Zhe Li, Mingqi Gao, Shanshan Zhao,\nYing Shan, et al. Caption anything: Interactive image de-\nscription with diverse multimodal controls. arXiv preprint\narXiv:2305.02677, 2023. 2\n[59] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,\nNoah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.\nSelf-instruct: Aligning language model with self generated\ninstructions. arXiv preprint arXiv:2212.10560, 2022. 16\n[60] Zehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, and\nZhou Zhao. Chat-3d: Data-efficiently tuning large language\nmodel for universal dialogue of 3d scenes. arXiv preprint\narXiv:2308.08769, 2023. 2, 3\n[61] Yanmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng,\nand Jian Zhang. Eda: Explicit text-decoupling and dense\nalignment for 3d visual grounding.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 19231\u201319242, 2023. 4\n[62] Xu Yan, Zhihao Yuan, Yuhao Du, Yinghong Liao, Yao\nGuo, Zhen Li, and Shuguang Cui.\nClevr3d: Composi-\ntional language and elementary visual reasoning for ques-\ntion answering in 3d real-world scenes.\narXiv preprint\narXiv:2112.11691, 2021. 2, 3\n[63] Zhengyuan Yang, Songyang Zhang, Liwei Wang, and Jiebo\nLuo.\nSat:\n2d semantics assisted training for 3d visual\ngrounding. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1856\u20131866, 2021. 3\n[64] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingn-\ning Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang,\nZhiyong Wang, et al.\nLamm: Language-assisted multi-\nmodal instruction-tuning dataset, framework, and bench-\nmark. arXiv preprint arXiv:2306.06687, 2023. 2, 3, 4, 17\n[65] Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang,\nSheng Wang, Zhen Li, and Shuguang Cui.\nInstancerefer:\nCooperative holistic understanding for visual grounding on\npoint clouds through instance multi-level contextual refer-\nring. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 1791\u20131800, 2021. 3\n[66] Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang,\nSheng Wang, Zhen Li, and Shuguang Cui.\nInstancerefer:\nCooperative holistic understanding for visual grounding on\npoint clouds through instance multi-level contextual refer-\nring. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 1791\u20131800, 2021. 3, 4\n[67] Zhihao Yuan, Xu Yan, Zhuo Li, Xuhao Li, Yao Guo,\nShuguang Cui, and Zhen Li.\nToward explainable and\nfine-grained 3d grounding through referring textual phrases.\narXiv preprint arXiv:2207.01821, 2022. 3, 5, 16\n[68] Zhihao Yuan, Xu Yan, Yinghong Liao, Yao Guo, Guan-\nbin Li, Shuguang Cui, and Zhen Li. X-trans2cap: Cross-\nmodal knowledge transfer using transformer for 3d dense\ncaptioning.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8563\u2013\n8573, 2022. 3\n[69] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\n11\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\nformer language models. arXiv preprint arXiv:2205.01068,\n2022. 3, 6, 7, 8, 24, 25\n[70] Zaiwei Zhang, Rohit Girdhar, Armand Joulin, and Ishan\nMisra.\nSelf-supervised pretraining of 3d features on any\npoint-cloud. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 10252\u201310263, 2021.\n7, 25\n[71] Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvg-\ntransformer:\nRelation modeling for visual grounding on\npoint clouds. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 2928\u20132937, 2021. 3\n[72] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei,\nHongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong,\nChunrui Han, et al.\nChatspot: Bootstrapping multimodal\nllms via precise referring instruction tuning. arXiv preprint\narXiv:2307.09474, 2023. 6\n[73] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan\nHuang, and Qing Li. 3d-vista: Pre-trained transformer for 3d\nvision and text alignment. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 2911\u2013\n2921, 2023. 3, 6\n12\nSupplementary Material\nThis supplementary material provides further details about M3DBench (Sec. A), quantitative experiments (Sec. B) on multi-\nround dialogue and 3D localization, additional experiments for held-out evaluation (Sec. C), implementation details (Sec. D)\nof baseline model and prompt for GPT-4 evaluation (Sec. E).\nA. Dataset\nIn Sec. A.1, we provide more examples in M3Dbench for each task. Following that, we will introduce the dataset construction\nin Sec. A.2 and provide statistics for the evaluation dataset in Sec. A.3.\nA.1. More Examples\nInstruction:\nCan you determine the category and location of \neach object found within the point cloud? The \noutput should be a list of tuples in the format (c, x, \ny, z, l, w, h), where  c represents the class label, \nwhile x, y, z denote the bounding box center \ncoordinates, and l, w, h indicate its size.\nInstruction:\nReview the point cloud and list all detected \nobjects, including information on their types and \nlocations. Output a list of tuples: (c, x, y, z, l, w, h) \nwhere c is the class label, and x, y, z represent the \ncenter coordinates, while l, w, h denote the \nbounding box dimensions.\nInstruction:\nUsing the data from the point cloud, identify the \ntype of each object and provide its spatial \ncoordinates. Generate a list of tuples (c, x, y, z, l, w, \nh), where c is the class label, and x, y, z represent \nthe bounding box center coordinates, while l, w, h \ndenote its size.\nFigure 5. Examples of 3D object detection. The left column represents the 3D scene, the middle column displays the instructions, and\nthe right column shows the annotations for the object detection task. We save annotations in textual format and for visualization purposes\nhere, we extract the bounding boxes from the text.\n13\nInstruction:\nThere is a black cylindrical trash can, which is \nlocated to the right of the kitchen counter and in \nfront of the printer. Please give me the \ncoordinates of its center and the length, width, \nand height of the bounding box. \nInstruction:\nI\u2018m seeing an image          . Identify instances in the \npoint cloud that belong to the same category as \nthe objects in the image. Please represent the \nbounding box in the form of (cx, cy, cz, lx, ly, lz).\nInstruction:\nCan you locate the armchair near the point \n<point>0.93,-2.32,0.58</point> that is tight \nagainst the wall with plain covered couches \nnearby? Please provide the center coordinates \nand dimensions of the bounding box in the form \nof (cx, cy, cz, lx, ly, lz).\nFigure 6. Examples of 3D visual grounding. The left column represents the 3D scene, the middle column displays the instructions,\nand the right column shows the annotations for the visual grounding. M3DBench includes interleaved multi-modal instructions, and the\nannotations extend beyond annotating a single target object, encompassing the identification of multiple objects.\nResponse:\nA white door.\nInstruction:\nWhat\u2019s your concise interpretation of \nthe object positioned inside the \nregion <box>-2.17, 0.10, 0.96, 0.16, \n0.92, 1.93</box>?\nResponse:\nAnother set of cabinets is located under the sink, between the stove and the \nrefrigerator. There is also a large kitchen cabinet unit with light wood coloring. \nThis unit has three doors and four drawers. One rectangular brown cabinet is \nsituated below the sink. The cabinets provide ample storage space for kitchen \nappliances and utensils. To access the cabinets, one would need to move \naround the kitchen counter and appliances, such as the stove and refrigerator. \nThe cabinets are well-designed and complement the overall style of the room.\nInstruction:\nIn brief, how can you depict \nthe target object near the \npoint (0.86, -0.486, 0.74)?\nResponse:\nA drawer dresser under bed.\nInstruction: \nCan you describe the kitchen cabinets in detail?\nDrawer \ndresser\nBed\nFigure 7. Examples of 3D dense caption. We design diverse multi-modal instructions for dense captions for M3DBench. Additionally,\nwe introduce terms such as brief or detailed within instructions to generate either concise titles or detailed descriptions for objects.\n14\nInstruction:\nWhat is the shape of the \nkitchen counter?\nInstruction:\nHow many chairs are \nthere next to the \nwindow?\nResponse:\nThere are two chairs \nnext to the window.\nResponse:\nThe kitchen counter is \nelongated and L-shaped.\nInstruction:\nWhat is the function of the \nrectangular silver object in \nthe kitchen?\nResponse:\nThe rectangular silver object \nin the kitchen is a refrigerator, \nused to keep food cool and \nfor long term storage of food.\nChair 1\nChair 2\nChair 3\nTable 1\nInstruction:\nWhat is the size of the table \nwith two chairs next to the \nbed?\nResponse:\nThe size of the table with two \nchairs next to the bed is \n1.059m x 1.220m x 0.742m.\nChair 1\nChair 2\nRectangular\nSilver object\nWindow\nKitchen \ncounter\nTable 2\nBed\nFigure 8. Examples of 3D visual question answering. M3DBench comprises open-ended, free-form questions involving instance location,\nshape and size, object count, scene type, object and room functionality, and more. For instance, when asked about the functionality of\na rectangular silver object in the upper-right scene, the answer begins by identifying the object and then describing its functionality.\nFurthermore, the two examples below illustrate instances where there might be multiple objects of the same category in the scene.\nInstruction:\nStanding near the           and facing the \nsink, can I reach the toilet paper \ndispenser without moving?\nResponse:\nNo, you would need to turn around or \nmove to reach the toilet paper \ndispenser.\nInstruction:\nStanding at the stool in the region \n<box>-0.77,-1.179,0.44,0.43,0.46</box> \nand facing the kitchen counter, what is \non my right?\nResponse:\nThe closed doorway is on your right.\nToilet paper \ndispenser\nClosed\ndoorway\nFigure 9. Examples of embodied question answering. Embodied question answering requires the agent to understand the surrounding\nenvironment in order to answer questions under that situation.\nInstruction:\nAre keyboard and that monitor part of the same \nworkstation?\nResponse:\nYes, the keyboard and that monitor are part of \nthe same workstation, as they are set up next to \neach other.\nInstruction:\nWhat is the arrangement of the window, the \nwhite board, and a desk?\nResponse:\nThe window is in the middle of a white board \nand a desk, with the white board being in the \nmiddle of a door and the window.\nInstruction:\nIf I am tired and want to lay down, where \nshould I go from the position near the region \n<box>-1.40,1.434,0.92,1.151,0.48,2.24</box>?\nResponse:\nYou should go to the bed, which is on your \nright if you're facing the cabinet.\nWhite board\nWindow\nDesk\nKeyboard\nMonitor\nBed\nWorkstation\nFigure 10. Examples of multi-region reasoning (left) and embodied planning (right). In multi-region reasoning tasks (left), at least\ntwo objects are involved, querying their relative relationships and sizes, which enables a detailed comprehension of the scene. On the\nother hand, embodied planning (right) requires an agent to perceive the environment, understand the user\u2019s intentions, and then generate\nappropriate responses or actions to achieve predetermined goals.\n15\nInstruction:\nCan you describe the objects and \ntheir locations in the room?\nResponse:\nThe room is filled with various \nobjects, including bags hanging on \nthe wall, a black chair in front of a \ncomputer monitor, a white desk \nwith a trash can underneath, a \nbookshelf in between doors, and a \nlarge round table with two white\nResponse:\nThe scene appears to be an office \nspace with cubicle desks and \nwindows. The layout of the room \nis not clear, but there are multiple \nsets of doors and windows. The \ndesks are brown and appear to be \norganized in rows. To locate a spe-\nobjects on top of it. The bags on the wall come in different colors and are \nlocated in various positions, such as the black bag with white writing \nhanging close to the door, the red-ish bag closest to the floor, and the \nblack bag to the far right of the two sets of bags hanging on the wall to the \nright of the doorway. The bookshelf in between doors is the second one \nfrom the top and contains a group of books closest to the windows and \nanother group closest to the couch. The trash can underneath the white \ndesk is only visible when facing the wall with two white doors and a \nbookshelf in between. The large round table is located behind a red table \nnext to the couch and has two white objects on top of it. The white desk \nhas a computer monitor on it, with a lamp next to it and a trash can \nunderneath. A black chair is in front of the computer monitor, and a tall \nlamp is located close to the door of the room near bookshelves.\nInstruction:\nCan you describe the scene in \ndetail, including the room layout, \nobject types, and object locations?\ncific desk, one must stand at a certain point and face a certain direction. \nFor example, standing at the table and looking at the cubicle desks, the \nmiddle option is the front left. The middle of the two cubicles in the \nfront right is also an option. If standing in front of the wall with smaller \nwindows, the rightmost desk directly next to the wall of windows \nshould be selected. Another option is the desk that is second from the \nleft and directly next to the window on the same wall. The desk closest \nto the set of doors that are not parallel to each other and nearest to the \nsame door should also be selected. The largest highlighted box that is \nnot against a wall and closest to the cubicles is another option. \nAdditionally, the clear square located in the corner of the room closer \nto the doors with the exit sign above it should be selected.\nFigure 11. Examples of scene description. Scene description requires integrating the visual information of the entire scene to generate a\ndetailed and comprehensive introduction. This description will encompass aspects such as objects within the scene, the type of room, its\nfunctionality, and so on.\nHuman: Looking around the room, I see a wooden dining chair at the table. It's on the back side, closest to \nthe wall.\nAI: Yes, and there's another wooden dining chair at the front of the table. They seem to be part of a set.\nHuman: Right. And in front of the tan couch, there's a wooden two-tiered coffee table.\nAI: That's correct. And the tan couch is facing this coffee table. Behind the couch, there's a wide curtain in \nfront of a window.\nHuman: I see. And the dining table, it's square with a light brown top, isn't it?\nAI: And it's off to the side of the living room. Around the table, there are 4 wooden chairs.\nHuman: Let's start with the door. It's white and has a gold handle on the left side.\nAI: Yes, the door is located on the far right of the room, next to a blue chair. It's a simple yet functional  \ndesign.\nHuman: I see a black suitcase under the table. It's the second one from the left.\nAI: That's correct. The suitcase is conveniently placed under the table, perhaps for easy access or to save \nspace.\nHuman: There's also a small blue trash can with a fish design on it. It's right next to a white space heater.\nAI: Yes, the trash can's unique design adds a touch of personality to the room. Its location next to the heater \nsuggests it's a frequently used area.\nHuman: Moving on, there's a dark brown curtain with long panels that touch the floor. It's right next to the \nbed.\nAI: Yes, the curtain is touching the headboard of the bed on the right. It probably provides privacy for the \nbed area and adds an elegant touch to the room.\nFigure 12. Examples of multi-round dialogue. Multi-round dialogue necessitates the agent\u2019s ability to engage in natural and coherent\ncommunication with humans. This capability involves not only understanding and generating language but also ensuring accuracy and\ncoherence in context.\nA.2. Dataset Construction\nIn this work, we introduce a comprehensive 3D instruction tuning dataset, M3DBench, which serves as the foundation for\ndeveloping versatile and practical general-purpose assistants in the real-world 3D environment. M3DBench comprises 3D\ndata from publicly available datasets [1, 10\u201312, 21, 27, 67], along with interleaved multi-modal instructions and responses\ngenerated using self-instruct methods[59] and GPTs [8, 40]. From Tabs. 4 to 10, we provide detailed description of the\n16\nInstruction:\nWalk through the kitchen and \ntake a right after the \nrefrigerator. Stop on the rug \nunder the mirror. \nInstruction:\nWalk out of the room walk \ninto the bathroom on the left \nand wait there.\nFigure 13. Examples of 3D vision language navigation. The 3D scene is depicted in the left column, instructions are presented in the\nmiddle column, and annotations for the vision language navigation task are shown in the right column. Annotations are stored in textual\nformat, and for visual representation here, we extract the pathway from the text.\nprompts designed for various 3D tasks, each comprising system messages and manually crafted context examples. For tasks\nsuch as object detection, we manually design instruction and response templates, then replace the template\u2019s keywords with\nannotations to construct instruction-response data [64], as illustrated in Tab. 11 and Tab. 12. Furthermore, we have developed\nan interleaved multi-modal instruction formula by substituting corresponding templates for the <target> in the instructions,\nas shown in Tab. 13.\nA.3. Evaluation Dataset\nTo quantitatively evaluate the effectiveness of instruction-tuned MLMs, we constructed an evaluation dataset to assess the\nmodels\u2019 performance across various dimensions such as visual perception, scene understanding, spatial reasoning, and em-\nbodied planning. Our evaluation dataset consists of over 1.5K data samples, distributed as shown in Fig. 14.\n9%\n12%\n4%\n23%\n26%\n3%\n2%\n11%\n10%\nEmbodied Planning\nEmbodied Question Answering\nMulti-Region Reasoning\nDense Caption\nVisual Question Answering\nScene Description\nMulti-Round Dialogue\nVisual Grounding\nOthers\nFigure 14. The evaluation dataset covers a range of fundamental abilities within real-world 3D environments, such as visual percep-\ntion, scene comprehension, spatial reasoning, and embodied planning.\n17\nmessages = [ {\"role\":\"system\", \"content\": f\"\"\"You are an AI visual assistant, and you are seeing an object in a \n3D scene. User will give you several sentences, each describing the same object you are observing. In \naddition, all instances of objects in this scene are provided, along with corresponding categories and \ncoordinates. These coordinates are in the form of bounding boxes, represented as [cx, cy, cz, lx, ly, lz] with \nfloating numbers in unit of meters. These values correspond to the x, y, z coordinates of bounding box center \nand length of bounding box along x, y, z axis.\nSummary and describe the target object in a detail manner, including details like the object placements, \nobject attributes, object functions, relative position with the surrounding objects, and so on.  Here are the \nrequirements: 1) Describe using the tone of seeing the target object and surroundings. Don't generate \ndescriptions that cannot be reasoned based on the given information confidently. 2) Descriptions should be \nconcise, effective, diverse and logical. 3) Do not mention any specific spatial coordinate values and do not \nmention the source of information. The description should be more than 100 words and less than 150 \nwords.\"\"\"}\n]\nfor sample in fewshot_samples:\n      messages.append({\"role\":\"user\", \"content\":sample[\u2019context\u2019]})\n      messages.append({\"role\":\"assistant\", \"content\":sample[\u2019response\u2019]} )\nmessages.append({\"role\":\"user\", \"content\":\u2018\\n\u2019.join(query)}\nmessages = [ {\"role\":\"system\", \"content\": f\"\"\"You are an AI visual assistant, and you are seeing an object in a \n3D scene. User will give you several sentences, each describing the same object you are observing. \nSummary the target object in a brief manner, only containing the object attribute. Here are the \nrequirements: 1)  Describe using the tone of seeing the target object. Don't generate descriptions that cannot \nbe reasoned based on the given information confidently. 2) Descriptions should be concise, effective, and \nlogical. 3) Do not mention any specific spatial coordinate values and do not mention the source of \ninformation. The description should be less than 5 words.\"\"\"}\n]\nfor sample in fewshot_samples:\n      messages.append({\"role\":\"user\", \"content\":sample[\u2019context\u2019]})\n      messages.append({\"role\":\"assistant\", \"content\":sample[\u2019response\u2019]} )\nmessages.append({\"role\":\"user\", \"content\":\u2018\\n\u2019.join(query)}\nTable 4. System message used to generate detailed (top) and brief (bottom) dense caption data in M3DBench.\n18\nmessages = [ {\"role\":\"system\", \"content\": f\"\"\"You are an AI visual assistant, and you are seeing an object in a \n3D scene. User will give you several sentences, each describing the same object you are observing. In \naddition, all instances of objects in this scene are provided, along with corresponding categories and \ncoordinates. These coordinates are in the form of bounding boxes, represented as [cx, cy, cz, lx, ly, lz] with \nfloating numbers in unit of meters. These values correspond to the x, y, z coordinates of bounding box center \nand length of bounding box along x, y, z axis.\nGenerate some questions and give corresponding answer of the target object, including details like the \nobject placements, object attributes, object functions, relative position with the surrounding objects, and \nso on.  Here are the requirements: 1) Ask questions and answer using the tone of seeing the target object \nand surroundings. Only include questions that have definite answers: one can see the content in the scene \nthat the question asks about and can answer confidently. Do not ask any question that cannot be answered \nconfidently. Do not ask about uncertain details. 2) Replace the specific target object's name in the question \nwith the placeholder '<target>'. However, in the answer, use only the actual name of the object without any \nplaceholders. 3) Ask diverse questions (e.g., 'What...', 'How...', 'Which...', 'Where...', 'If...', 'Is...', 'Are...', etc.) \nand provide detailed answers in natural language, yes/no, numerical formats, etc. 4) Ensure questions and \nanswers are concise, logical and effective. 5) Do not mention any specific spatial coordinate values and do \nnot mention the source of information. Keep each question or answer under 50 words.\"\"\"}\n]\nfor sample in fewshot_samples:\n      messages.append({\"role\":\"user\", \"content\":sample[\u2019context\u2019]})\n      messages.append({\"role\":\"assistant\", \"content\":sample[\u2019response\u2019]} )\nmessages.append({\"role\":\"user\", \"content\":\u2018\\n\u2019.join(query)}\nTable 5. System message used to generate instruction-response pairs for visual question answering in M3DBench.\nmessages = [ {\"role\":\"system\", \"content\": f\"\"\"You are an AI visual assistant, and you are seeing an object in a \n3D scene. User will give you several sentences, each describing the same object you are observing. In addition, \nall instances of objects in this scene are provided, along with corresponding categories and coordinates. \nThese coordinates are in the form of bounding boxes, represented as [cx, cy, cz, lx, ly, lz] with floating \nnumbers in unit of meters. These values correspond to the x, y, z coordinates of bounding box center and \nlength of bounding box along x, y, z axis.\nYou need to generate embodied questions (e.g. Standing in front of the <target> and facing the towels. Can \nI see myself in the mirror?) and give a corresponding answer. Assuming you are positioned at the target \nobject and facing a nearby object, please begin each question by describing the situation (position, \norientation, etc.). Then provide the corresponding answer for the question. Here are the requirements: 1) \nAsk questions and answer using the tone of seeing the target object and surroundings. Only include questions \nthat have definite answers: one can see the content in the scene that the question asks about and can \nanswer confidently. Do not ask any question that cannot be answered confidently. Do not ask about \nuncertain details. 2) Replace the specific target object's name in the question with the placeholder '<target>'. \nHowever, in the answer, use only the actual name of the object without any placeholders. 3) Ask diverse \nquestions (e.g., 'What...', 'How...', 'Which...', 'Where...', 'If...', 'Is...', 'Are...', etc.) and provide detailed answers \nin natural language, yes/no, numerical formats, etc. 4) Ensure questions and answers are concise, logical and \neffective. 5) Do not mention any specific spatial coordinate values and do not mention the source of \ninformation. Keep each question or answer under 50 words.\"\"\"}\n]\nfor sample in fewshot_samples:\n      messages.append({\"role\":\"user\", \"content\":sample[\u2019context\u2019]})\n      messages.append({\"role\":\"assistant\", \"content\":sample[\u2019response\u2019]} )\nmessages.append({\"role\":\"user\", \"content\":\u2018\\n\u2019.join(query)}\nTable 6. System message used to generate instruction-response pairs for embodied question answering in M3DBench.\n19\nmessages = [ {\"role\":\"system\", \"content\": f\"\"\"You are an AI visual assistant, and you are seeing an object in a \n3D scene. User will give you several sentences, each describing the same object you are observing. In \naddition, all instances of objects in this scene are provided, along with corresponding categories and \ncoordinates. These coordinates are in the form of bounding boxes, represented as [cx, cy, cz, lx, ly, lz] with \nfloating numbers in unit of meters. These values correspond to the x, y, z coordinates of bounding box center \nand length of bounding box along x, y, z axis.\nGenerate some questions and give corresponding answer based on the relationship between these objects.  \nHere are the requirements: 1) Ask questions and answer using the tone of seeing the target object and \nsurroundings. Only include questions that have definite answers: one can see the content in the scene that \nthe question asks about and can answer confidently. Do not ask any question that cannot be answered \nconfidently. Do not ask about uncertain details. 2) Replace the names of related objects in the question with \nplaceholders like '<region 1>', '<region 2>', '<region 3>', etc. However, in the answer, use only the actual \nnames of the objects without any placeholders. 3) Ask diverse questions (e.g., 'What...', 'How...', 'Which...', \n'Where...', 'If...', 'Is...', 'Are...', etc.) and provide detailed answers in natural language, yes/no, numerical \nformats, etc. 4) Ensure questions and answers are concise, logical and effective. 5) Do not mention any \nspecific spatial coordinate values and do not mention the source of information. Keep each question or \nanswer under 50 words.\"\"\"}\n]\nfor sample in fewshot_samples:\n      messages.append({\"role\":\"user\", \"content\":sample[\u2019context\u2019]})\n      messages.append({\"role\":\"assistant\", \"content\":sample[\u2019response\u2019]} )\nmessages.append({\"role\":\"user\", \"content\":\u2018\\n\u2019.join(query)}\nTable 7. System message used to generate instruction-response pairs for multi-region reasoning in M3DBench.\nmessages = [ {\"role\":\"system\", \"content\": f\"\"\"You are an AI visual assistant that can analyze a 3D scene. User \nwill give you several sentences, each describing the same scene you are observing. In addition, all instances \nof objects in this scene are provided, along with corresponding categories and coordinates. These \ncoordinates are in the form of bounding boxes, represented as [cx, cy, cz, lx, ly, lz] with floating numbers in \nunit of meters. These values correspond to the x, y, z coordinates of bounding box center and length of \nbounding box along x, y, z axis.\nSummary and describe the scene in a detail manner, including details like the scenario, scene types, room \nfunctions, object types, object counts, object locations, object attributes, relative relationships between \nthe objects, and so on.  Here are the requirements: 1) You should design a question before describing the \nscene. The question should be 1 to 2 sentences long. The type of question should be diverse. Either an \nimperative sentence or a question is permitted. For example, describe the scene in detail.2) Describe using \nthe tone of seeing the whole scene. Don't generate descriptions that cannot be reasoned based on the given \ninformation confidently. 3) Descriptions should be concise, effective, diverse and logical. 4) Do not mention \nany specific spatial coordinate values and do not mention the source of information. The description should \nbe more than 200 words and less than 250 words.\"\"\"}\n]\nfor sample in fewshot_samples:\n      messages.append({\"role\":\"user\", \"content\":sample[\u2019context\u2019]})\n      messages.append({\"role\":\"assistant\", \"content\":sample[\u2019response\u2019]} )\nmessages.append({\"role\":\"user\", \"content\":\u2018\\n\u2019.join(query)}\nTable 8. System message used to generate instruction-response pairs for scene description in M3DBench.\n20\nmessages = [ {\"role\":\"system\", \"content\": f\"\"\"You are an AI visual assistant that can analyze a 3D scene. User \nwill give you several sentences, each describing the same scene you are observing. In addition, all instances \nof objects in this 3D are provided, along with corresponding categories and coordinates. These coordinates \nare in the form of bounding boxes, represented as [cx, cy, cz, lx, ly, lz] with floating numbers in unit of meters. \nThese values correspond to the x, y, z coordinates of bounding box center and length of bounding box along x, \ny, z axis.\nDesign a conversation between a human and you discussing various aspects related to the scene. Include \ntopics such as the given scenario, room functionality, type of scene, object categories, object counts, their \nrespective locations, attributes, relationships between objects, and so on. Here are the requirements: 1) \nBoth the human and you should discuss the scene in the tone of seeing the whole scene. Use a variety of \nsentence structures in the conversation. Avoid discussing details that cannot be confidently answered or are \nuncertain. 2) Initiate the conversation by choosing a specific topic. Ensure the conversation flows naturally \nand covers a wide range of details while maintaining coherence. 3) Do not mention any specific spatial \ncoordinate values and do not mention the source of information. Each conversation should take at least 5 \nrounds.\"\"\"}\n]\nfor sample in fewshot_samples:\n      messages.append({\"role\":\"user\", \"content\":sample[\u2019context\u2019]})\n      messages.append({\"role\":\"assistant\", \"content\":sample[\u2019response\u2019]} )\nmessages.append({\"role\":\"user\", \"content\":\u2018\\n\u2019.join(query)}\nTable 9. System message used to generate instruction-response pairs for multi-round dialogue in M3DBench.\nmessages = [ {\"role\":\"system\", \"content\": f\"\"\"You are an AI visual assistant, and you are seeing an object in a \n3D scene. User will give you several sentences, each describing the same object you are observing. In addition, \nall instances of objects in this scene are provided, along with corresponding categories and coordinates. \nThese coordinates are in the form of bounding boxes, represented as [cx, cy, cz, lx, ly, lz] with floating \nnumbers in unit of meters. These values correspond to the x, y, z coordinates of bounding box center and \nlength of bounding box along x, y, z axis.\nGenerate embodied questions, including planning(e.g. I feel tired/I want to study and where should I go \nnext?), navigation (e.g. how to go from the <target> to the position of bed?). Assuming you are positioned \nat the target object and facing a nearby object, please begin each question by describing the situation \n(position, orientation, etc.). Then provide the corresponding answer for the question. Here are the \nrequirements: 1) Ask questions and answer using the tone of seeing the target object and surroundings. Only \ninclude questions that have definite answers: one can see the content in the scene that the question asks \nabout and can answer confidently. Do not ask any question that cannot be answered confidently. Do not ask \nabout uncertain details. 2) Replace the specific target object's name in the question with the placeholder \n'<target>'. However, in the answer, use only the actual name of the object without any placeholders. 3) Ask \ndiverse questions (e.g., 'What...', 'How...', 'Which...', 'Where...', 'If...', 'Is...', 'Are...', etc.) and provide detailed \nanswers in natural language, yes/no, numerical formats, etc. 4) Ensure questions and answers are concise, \nlogical and effective. 5) Do not mention any specific spatial coordinate values and do not mention the source \nof information. Keep each question or answer under 50 words.\"\"\"}\n]\nfor sample in fewshot_samples:\n      messages.append({\"role\":\"user\", \"content\":sample[\u2019context\u2019]})\n      messages.append({\"role\":\"assistant\", \"content\":sample[\u2019response\u2019]} )\nmessages.append({\"role\":\"user\", \"content\":\u2018\\n\u2019.join(query)}\nTable 10. System message used to generate instruction-response pairs for embodied planning in M3DBench.\n21\n\u2022\nCan you determine the category and location of each object found within the point cloud? The output \nshould be a list of tuples in the format (c, x, y, z, l, w, h), where  c represents the class label, while x, y, z \ndenote the bounding box center coordinates, and l, w, h indicate its size. \n\u2022\nIdentify the object types within the point cloud and deliver tuples (c, x, y, z,  l, w, h) based on spatial data. \nThe format of the result should comprise tuples (c, x, y, z, l, w, h), where c denotes the class label, and x, y, \nz represent the center coordinates of the bounding box, while l, w, h indicate its dimensions.\n\u2022\nReview the point cloud and list all detected objects, including information on their types and locations. \nOutput a list of tuples: (c, x, y, z, l, w, h) where c is the class label, and x, y, z represent the center \ncoordinates, while l, w, h denote the.\n\u2022\nCan you classify and locate objects within the point cloud? Provide (c, x, y, z, l, w, h) tuples for each. \nGenerate a result in the format of tuples (c, x, y, z, l, w, h), where c signifies the class label, and x, y, z \ndenote the center coordinates of the bounding box, while l, w, h represent its size.\n\u2022\nUsing the data from the point cloud, identify the type of each object and provide its spatial coordinates. \nGenerate a list of tuples (c, x, y, z, l, w, h), where c is the class label, and x, y, z represent the bounding box \ncenter coordinates, while l, w, h denote its size.\n\u2022\nFrom the point cloud data, extract object types and spatial coordinates as (c, x, y, z,  l, w, h) tuples. The \nresult should be structured as tuples (c, x, y, z, l, w, h), where c represents the class label, and x, y, z \nindicate the bounding box's center coordinates, while l, w, h specify its dimensions.\n\u2022\nPositioned at the <bbox> location within the point cloud, an object within the <class> category can be \nobserved.\n\u2022\nThe point cloud includes an object at the <bbox> position, which can be classified under the category of \n<class>.\n\u2022\nAt the <bbox> position in the point cloud, there is an item categorized as <class>.\n\u2022\nThe <bbox> position of the point cloud allows for the identification of an object that belongs to the <class> \ncategory.\n\u2022\nWithin the point cloud, an object classified as <class> is situated at the <bbox> position.\n\u2022\nAn object that can be classified as <class> is located at the <bbox> position within the point cloud.\n\u2022\nThe <bbox> of the point cloud reveals the presence of an object categorized as <class>.\n\u2022\nAt the <bbox> position within the point cloud, there exists an object that falls under the <class> category.\n\u2022\nThe point cloud contains an object at the <bbox> position, which can be identified as <class>.\nTable 11. Some examples of question (top) and answer (bottom) templates for 3D object detection in M3DBench.\n22\n\u2022\nDescribe the <target> concisely.\n\u2022\nCan you provide a brief overview of the <target>?\n\u2022\nProvide a brief description of the given <target>.\n\u2022\nCan you relay a brief and clear account of the <target>?\n\u2022\nOffer a clear and concise depiction of the <target>.\n\u2022\nSum up the main aspects of the <target> succinctly.\n\u2022\nIn brief, how can you depict the <target>?\n\u2022\nCould you share a short summary of the <target>'s features?\n\u2022\nSummarize the visual content of the <target>.\n\u2022\nWhat\u2019s your concise interpretation of the <target>?\n\u2022\nIn a short description, what is the <target>?\n\u2022\nConvey a brief description of the essential features of the <target>.\n\u2022\nHow would you describe the <target> in brief?\n\u2022\nDescribe the following  <target> in detail.\n\u2022\nProvide a detailed description of the given  <target>.\n\u2022\nOffer a thorough analysis of the  <target>.\n\u2022\nClarify the contents of the displayed  <target> with great detail.\n\u2022\nAnalyze the  <target> in a comprehensive and detailed manner.\n\u2022\nI would appreciate a full and detailed explanation of the  <target>.\n\u2022\nI'm interested in a detailed exploration of the  <target>; could you provide that?\n\u2022\nCan you dissect the  <target>, giving us a comprehensive understanding?\n\u2022\nShare a rich and detailed narrative of the  <target>.\n\u2022\nOffer a profound and comprehensive insight into the  <target>.\nTable 12. Some examples of instructions for brief (top) and detailed (bottom) dense caption in M3DBench.\n23\nReplace  <target> with point prompt:\n          f\"{object_name} close to the pointed spot <point>{x},{y},{z}</point>\"\n          f\"{object_name} situated near the pointed point <point>{x},{y},{z}</point>\"\n          f\"{object_name} positioned close to the pointed location <point>{x},{y},{z}</point>\"\n          f\"{object_name} near the pointed point <point>{x},{y},{z}</point>\"\n          f\"{object_name} close to the pointed location <point>{x},{y},{z}</point>\"\n          f\"{object_name} situated near the given point <point>{x},{y},{z}</point>\"\n          f\"{object_name} situated close to the pointed location <point>{x},{y},{z}</point>\"\n          f\"{object_name} positioned near the pointed point <point>{x},{y},{z}</point>\"\nReplace  <target> with box prompt:\n          \n  \n \n \n          \n \n  \n \n \n          \n \n \n \n \n          \n \n  \n \n \n          \n \n \n \n \n \n \n \n \n \n \n \n          \n \n \n \n \n \n \n \n \n \n \n \n          \n \n \n \n \n \n \n \n \n \n \n          \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n          f\"<image>{image_path}</image>\"\nReplace  <target> with 3d object prompt\nf\"{object_name} in the region <box>{x},{y},{z},{l},{w},{h}</box>\nf\"{object_name} situated in the region <box>{x},{y},{z},{l},{w},{h}</box>\nf\"{object_name} inside the area <box>{x},{y},{z},{l},{w},{h}</box>\nf\"{object_name} placed in the region <box>{x},{y},{z},{l},{w},{h}</box>\nf\"{object_name} with center at [{cx}, {cy}, {cz}] and dimensions [{lx}, {ly}, {lz}]\nf\"{object_name} positioned at center [{cx}, {cy}, {cz}] with size [{lx}, {ly}, {lz}]\nf\"{object_name} centered at [{cx}, {cy}, {cz}] with measurements [{lx}, {ly}, {lz}]\nf\"{object_name} having central coordinates [{cx}, {cy}, {cz}] and measurements of [{lx}, {ly}, {lz}]\nReplace <target> with image prompt:\n:\n          f\"<obj_3d>{3d_object_path}</obj_3d>\"\nTable 13. The formula for interleaved multi-modal instruction generation in M3DBench.\nB. Experiments on Dialogue and Localization\nQuantitative Evaluation on Multi-round Dialogue. We score the conversational abilities of the baseline models using\nGPT-4 [40]. For multi-round dialogue, the baseline model employing PointNet++ [46] as the scene encoder and Vicuna-\n7B-V1.5 [20] as the language decoder demonstrates the optimal performance, surpassing the next best variant by +0.71\npoints. Similar to the conclusions derived from the results of detailed description (detailed in the Sec. 5.2), all OPT-based\nvariants [69] exhibit relatively lower performance. In Sec. E, we provide prompts used for scoring conversations with GPT-\n4 [40], along with qualitative results for multi-turn dialogues and the GPT-4 [40] scoring criteria.\n3D Vision Encoder\nLLM Decoder\nRelative Score\nPointnet++ [46]\nOPT-6.7B [69]\n40.97\nLLaMA-2-7B [55]\n44.74\nVicuna-7B-v1.5 [20]\n46.06\nTransformer [56]\nOPT-6.7B [69]\n29.52\nLLaMA-2-7B [55]\n38.61\nVicuna-7B-v1.5 [20]\n45.35\nTable 14. Benchmark for multi-round dialogue. Relative\nScore is generated by the GPT-4 [40], based on the evaluation\nof the model\u2019s response.\n3D Vision Encoder\nLLM Decoder\nAcc@0.25IoU\nPointnet++ [46]\nOPT-6.7B [69]\n3.09\nLLaMA-2-7B [55]\n1.60\nTransformer [56]\nOPT-6.7B [69]\n1.22\nLLaMA-2-7B [55]\n3.57\nTable 15. Benchmark for object localization. We assess the\nbaseline model\u2019s ability to identify and localize objects in the\n3D scene. Specifically, the baseline model is tasked with out-\nputting the location of the target object a given specific instruc-\ntion. The metric utilized is Acc@0.25IoU.\nQuantitative Evaluation on 3D object Localization. For the 3D object localization task (i.e., finding the object in a scene\nthat best matches a given instruction), we propose using a unified output format to represent object position. To acquire\nlocalization data, we derive 3D bounding boxes from the \u201c[cx, cy, cz, l, w, h]\u201d provided in the generated text. Here, cx, cy,\n24\ncz correspond to the x, y, z coordinates of the bounding box center, while l, w, h represent the size of the bounding box along\nthe x, y, z axes. For each value defining the 3D bounding box, we retain one decimal place. In Tab. 15, we present baseline\nperformances regarding 3D localization. Results indicate that our proposed baseline model exhibits suboptimal performance\non localizing. We leave the improvement of MLMs\u2019 abilities in 3D scene perception and localization for future work.\nC. Held-out Evaluation\nTraining and Evaluation Protocols. In order to assess the zero-shot performance of the baseline model fine-tuned on\nthe multi-modal instruction data for unseen tasks, we partition our dataset into two types: held-in and held-out datasets.\nSpecifically, we consider embodied question answering (EQA) and embodied planning (EP) from M3DBench as unseen\ntasks, with their corresponding dataset (held-out dataset) excluded during the training process. We train the baseline model\non the training dataset for the remaining tasks (held-in dataset) and evaluate the model\u2019s performance using the validation set\nfrom the held-out dataset.\nBaselines and Metrics. We utilize a pre-trained masked transformer encoder[16] as the scene encoder and employ two large\nlanguage models, OPT-6.7B [69] and LLaMA-2-7B [55], as the decoder in our baseline model. Furthermore, we employ\nBLEU 1-4 [43], ROUGE-L [34], METEOR [5], and CiDEr [57] as evaluation metrics.\nTask\nLLM Decoder\nBLEU-1\u2191\nBLEU-2\u2191\nBLEU-3\u2191\nBLEU-4\u2191\nROUGE\u2191\nMETEOR\u2191\nCIDEr\u2191\nEmbodied Question Answering\nOPT-6.7B[69]\n28.76\n21.67\n17.51\n13.96\n30.78\n17.64\n139.06\nLLaMA-2-7B [55]\n35.76\n27.42\n21.89\n16.83\n40.04\n20.47\n163.71\nEmbodied Planning\nOPT-6.7B[69]\n21.13\n16.07\n12.36\n8.99\n28.96\n16.28\n47.62\nLLaMA-2-7B [55]\n33.80\n25.15\n19.23\n14.71\n33.30\n19.65\n58.21\nTable 16. Zero-shot results on Embodied Question Answering (EQA) and Embodied Planning (EP). For held-out evaluation, we\ndemonstrate the performance of baseline methods on two tasks. The upward arrow (\u2191) indicates that higher values represent better perfor-\nmance. Notably, we find that leveraging LLaMA-2 [55] as the language decoder exhibits superior zero-shot generalization compared to the\nOPT-based [69] model.\nResult Analysis. In Tab. 16, we present the performance of the baseline model for held-out evaluation. Additionally, we\ncompare baselines using different LLMs as language decoders. All baselines follow the same training and evaluation pro-\ntocols described above. In summary, we draw three insights: 1) through instruction tuning and multi-task learning on the\nheld-in dataset of M3DBench, the baseline model exhibits reasoning ability when dealing with tasks that it hasn\u2019t encoun-\ntered before. 2) LLaMA-based [55] model outperforms the baseline model based on OPT [69] in zero-shot generalization.\n3) There remain gaps in zero-shot results compared to results from full supervised instruction fine-tuning (detailed in the\nSec. 5.2). These findings indicate that through instruction tuning and multi-task learning on M3DBench, our model demon-\nstrates reasoning abilities on tasks that haven\u2019t encountered before. This emphasizes the significance of instruction tuning for\nachieving zero-shot generalization.\nD. Implementation\nScene Encoder. As introduced in Sec. 5.1, we employ two commonly used types of 3D pre-trained feature extractors as\nscene encoders: one based on PointNet++ [46] and the other based on Transformer [56]. The PointNet++-based scene\nencoder comprises four layers for feature extraction and down-sampling, coupled with two layers for feature aggregation and\nup-sampling [70]. The final layer generates features for sampled points, from which we derive scene-level 256-dimensional\nfeatures via global max-pooling. In addition, the Transformer-based encoder initially tokenizes input point clouds into 2048\npoint tokens through a set abstraction layer [46], followed by three cascaded Transformer encoder blocks with masking\nradii of 0.16, 0.64, and 1.44 [16]. Between the first two Transformer blocks, there is an additional set abstraction layer that\ndownsamples the encoded tokens, with each token represented by 256 dimensions.\nMulti-modal Prompt Encoder. We utilize the tokenizer and word embedding from pre-trained LLM [20, 55, 69] to process\ntext and coordinate instructions. For image inputs, we employed the pre-trained ViT-L/14 [48] as the image encoder, adopting\na trainable projector based on the LLaVA [35] to collect image tokens. Regarding 3D object inputs, we utilized a pre-trained\n3D encoder [16] to extract object features, obtaining object-level tokens via another projector. For point-level and box-level\n25\nprompts, we directly employ linear layers to project the corresponding prompt features into the LLM embedding space. We\nleave the design of more optimal models for exploration in future work.\nTrainable Parameters. The model comprises roughly 52 million trainable parameters, accounting for less than 1% of the\nfrozen LLM backbone\u2019s (LLaMA-2-7B [55]) parameter count.\nE. GPT-4 Evaluation\nWe employ the template in Tab. 17 to prompt GPT-4 [40] and obtain corresponding evaluation results. Specifically, we prompt\nGPT-4 [40] with four inputs: system message, question, reference answer, and models\u2019 responses, which comprise answers\nfrom various baseline models. We prompt GPT-4 [40] to assess responses for accuracy, relevance, descriptive details, etc.,\nand assign scores within a range of 0 to 100. Higher scores indicate better quality of the responses. Moreover, we request\nGPT-4 [40] to provide explanations for the scoring results, assisting in our evaluation of the scoring results\u2019 validity. In\nFig. 15 and Fig. 16, we present model\u2019s (utilizing transformer [56] as the scene encoder and LLaMA [55] as the language\ndecoder) responses, GPT-4 scores, and GPT-4 justifications on detailed description and multi-round dialogue.\n[System Message] You are an expert in scene details description. We will provide you a question, a reference \nanswer for this question, and responses from six AI assistants. We would like your feedback on the performance \nof the AI assistants on their responses to the question. You need to evaluate each assistant's response based on \nthe question and reference answer. Please rate them on accuracy, relevance, naturalness, level of detail of their \nanswers, and correspondence score from 0 to 100. Higher scores indicate better overall quality of the model's \nresponse. Please first output the first line indicating the scores for six AI assistants, respectively. In the next line, \nprovide a comprehensive explanation of your evaluation, avoiding any potential bias, and ensuring that the \norder in which the responses were presented does not affect your judgment.\nTable 17. The template employed to prompt GPT-4 for assessing the quality of responses across different models.\nCan you detail the layout and \ncontents of the scene?\nThe scene appears to be a bathroom with various fixtures and amenities. The \nbathroom features a light-colored round sink, which is located on a cabinet to \nthe left of the toilet. A white wooden bathroom cabinet is situated at the \ncorner between the shower towel stand and the toilet bowl. Above the \nwashbasin, there's another cabinet.The toilet, which is white and has an oval \nshape, is positioned between the bathtub and the bathroom cabinet. A white \ntoilet paper roll is conveniently placed between the white cabinet and the \ntoilet bowl. The bathtub, looking like a hollowed-out rectangular prism, is to \nthe right of the toilet. A yellow shower curtain is hung on top of the bathtub, \ncontributing to the color scheme of the room. Towels are strategically placed \nin the bathroom for easy access. A dark green towel is hung on a rod that is \nattached to the wall. Another towel is located on the shower curtain rod, \nhung on the left portion of the rod, with the curtain to its right. There's also a \ntowel hung on a rack, positioned on the right side of the rack, and located to \nthe left of and above the sink. Lastly, the bathroom has a white door, situated \nbetween two towel holders. This well-arranged bathroom provides all the \nnecessary amenities within easy reach.\nGPT-4 Score: 80\nGPT-4 Explanations: Assistant's response is quite accurate and detailed, \ncorrectly identifying the main fixtures and their locations. It also provides \nadditional details, such as the presence of a mirror and a light fixture. However, \nit lacks the specifics of the towel placement and the color scheme of the room. \nIt also incorrectly describes the locations of the toilet and the sink. Overall, it \nprovides a useful and relevant description of the bathroom, but with some \ninaccuracies.\nC a n yo u des cr ib e  t h e \ncabinet found in the area \n<box>1.08,2.63,0.40,2.02,0.\n83,0.79</box> in detail and \nits surroundings?\nThe cabinet is located in the kitchen, specifically in the corner of the room. It \nis a tall cabinet, with a white color and a rectangular shape. The cabinet is \nsituated in the corner of the room, which suggests that it is a prominent \nfeature in the space. The cabinet is also located near the sink, which suggests \nthat it may be used for storage or organization of kitchen supplies. The \ncabinet's placement in the corner of the room may also indicate that it is a \nfocal point in the kitchen. To access the cabinet, one would need to move \ntowards the corner of the room. Overall, the cabinet's placement and color \nsuggest a clean and organized kitchen space.\nGPT-4 Score: 40\nGPT-4 Explanations: The AI assistant's response is not very accurate when \ncompared to the ground truth answer. While the assistant provides a detailed \ndescription, it incorrectly identifies the location of the cabinet as being in the \nkitchen, whereas the ground truth specifies that the cabinet is near a wall and \nto the left of a chair, suggesting a workspace or study area. The assistant also \nfails to mention key details such as the painting above the cabinet, the paper \nholder, the TV remote, and the office chair. These inaccuracies and omissions \nsignificantly reduce the relevance and usefulness of the assistant's response. \nThe assistant does, however, provide a clear and detailed image of a kitchen \nsetting, which shows that it can provide useful information under different \ncircumstances.\nFigure 15. Qualitative results on scene description. We present visualization results for scene description, along with the scores provided\nby GPT-4 and the criteria used by GPT-4 for scoring.\n26\nLet's start with the towels. I see a \ncouple of rectangular towels on a \nrack at the left side of the room. \nYes, one towel is the leftmost on \nthe rack, and another is in the \nmiddle of the rack. To the left of \nthe towels, there's a white door.\nAnd to the right of the door, there's a gray trash can. It's next to the vanity sink. \nThat's correct. The trash can is conveniently located for easy disposal of waste. \nThe sink it's next to is part of a bathroom vanity that appears to be made of \nmarble or composite. \nAnd there's also a rectangular brown bench in the room. It's placed adjacent to \nthe door.\nYes, the bench is positioned near the door. It could be a convenient spot for \nplacing items or sitting down.\nSo, this is a well-equipped bathroom. \nThe towels are in a convenient location, and the trash can is nearby for easy \ndisposal of waste. The bench could be used for sitting or placing items.\nGPT-4 Score:  60\nGPT-4 Explanations: The AI assistant's response is partially relevant and \naccurate, but it does not fully address the statement made by Agent 1. The \nassistant repeated information about the towels, trash can, and bench, which \nis accurate but not necessary in this context. Instead, the assistant should have \nagreed with or added to Agent 1's conclusion about the bathroom being well-\nequipped, as the ground truth answer did. The assistant's response lacks the \nnecessary level of detail and correspondence in this specific context.\nLet's start with the furniture. I see a \nbrown wooden chair and a nightstand. \nWhat could they be used for? \nThe chair and nightstand could form a \ncomfortable reading corner or a place \nto relax. The nightstand could hold a \nlamp, books, or other personal items. \nI see. Now, there are two doors in the room. Are they entry points to this room? \nYes, they could be entry points to this room or they could lead to other rooms or spaces \nwithin the house. They play a key role in the room's layout and function. \nNow, let's talk about the trash cans. I see a few of them around the room. \nYes, there are several trash cans in the room. This suggests a focus on cleanliness and \norganization. They are conveniently placed near the door and the table. \nLastly, I see two windows above the table. What do they add to the room?\nThe windows add natural light and a sense of openness to the room. They also provide a \nview of the outside world, which could be a source of inspiration or relaxation.\nGPT-4 Score:  95\nGPT-4 Explanations: The AI assistant's response is highly relevant and accurate, providing \na thoughtful interpretation of the function of the windows in the room. It correctly \nidentifies the benefits of natural light and the view, which aligns with the ground truth \nanswer. The assistant also adds a unique perspective by suggesting the view could be a \nsource of inspiration or relaxation. However, it lacks the detail about blinds and control \nover privacy and light, which was mentioned in the ground truth answer. Therefore, the \nresponse is not completely detailed, but it is still very useful and corresponds well overall.\nFigure 16. Qualitative results on multi-round dialogue. We present visualization results for multi-round dialogue, along with the scores\nprovided by GPT-4 and the criteria used by GPT-4 for scoring.\n27\n"
  },
  {
    "title": "Rich Human Feedback for Text-to-Image Generation",
    "link": "https://arxiv.org/pdf/2312.10240.pdf",
    "upvote": "17",
    "text": "Rich Human Feedback for Text-to-Image Generation\nYouwei Liang*\u20201, Junfeng He*\u20212, Gang Li*\u20212, Peizhao Li\u20205, Arseniy Klimovskiy2, Nicholas Carolan2,\nJiao Sun\u20203, Jordi Pont-Tuset2, Sarah Young2, Feng Yang2, Junjie Ke2, Krishnamurthy Dj Dvijotham2,\nKatie Collins\u20204, Yiwen Luo2, Yang Li2, Kai J Kohlhoff2, Deepak Ramachandran2, and Vidhya\nNavalpakkam2\n1University of California San Diego\n2Google Research\n3University of Southern California\n4University of Cambridge\n5Brandeis University\nAbstract\nRecent Text-to-Image (T2I) generation models such as\nStable Diffusion and Imagen have made significant progress\nin generating high-resolution images based on text descrip-\ntions. However, many generated images still suffer from\nissues such as artifacts/implausibility, misalignment with\ntext descriptions, and low aesthetic quality. Inspired by the\nsuccess of Reinforcement Learning with Human Feedback\n(RLHF) for large language models, prior works collected\nhuman-provided scores as feedback on generated images\nand trained a reward model to improve the T2I generation.\nIn this paper, we enrich the feedback signal by (i) marking\nimage regions that are implausible or misaligned with the\ntext, and (ii) annotating which words in the text prompt are\nmisrepresented or missing on the image. We collect such\nrich human feedback on 18K generated images and train\na multimodal transformer to predict the rich feedback au-\ntomatically. We show that the predicted rich human feed-\nback can be leveraged to improve image generation, for ex-\nample, by selecting high-quality training data to finetune\nand improve the generative models, or by creating masks\nwith predicted heatmaps to inpaint the problematic regions.\nNotably, the improvements generalize to models (Muse) be-\nyond those used to generate the images on which human\nfeedback data were collected (Stable Diffusion variants).\n*Co-first authors, equal technical contribution\n\u2020The work is done during an internship at Google\n\u2021Corresponding authors, equal leading contribution:\n{junfenghe, leebird}@google.com\n1. Introduction\nText-to-image (T2I) generation models [12, 17, 41, 42, 56,\n58, 59] are rapidly becoming a key to content creation in\nvarious domains, including entertainment, art, design, and\nadvertising, and are also being generalized to image edit-\ning [4, 27, 44, 50], video generation [23, 35, 53], among\nmany other applications.\nDespite significant recent ad-\nvances, the outputs still usually suffer from issues such\nas artifacts/implausibility, misalignment with text descrip-\ntions, and low aesthetic quality [30, 52, 54]. For example,\nin the Pick-a-Pic dataset [30], which mainly consists of im-\nages generated by Stable Diffusion model variants , many\nimages (e.g. Fig. 1) contain distorted human/animal bodies\n(e.g. human hands with more than five fingers), distorted\nobjects and implausibility issues such as a floating lamp.\nOur human evaluation experiments find that only \u223c10% of\nthe generated images in the dataset are free of artifacts and\nimplausibility. Similarly, text-image misalignment issues\nare common too, e.g., the prompt is \u201ca man jumping into a\nriver\u201d but the generated image shows the man standing.\nExisting automatic evaluation metrics for generated im-\nages, however, including the well-known IS [43] and\nFID [20], are computed over distributions of images and\nmay not reflect nuances in individual images. Recent re-\nsearch has collected human preferences/ratings to evaluate\nthe quality of generated images and trained evaluation mod-\nels to predict those ratings [30, 52, 54], notably ImageRe-\nward [54] or Pick-a-Pic [30]. While more focused, these\nmetrics still summarize the quality of one image into a sin-\ngle numeric score. In terms of prompt-image alignment,\nthere are also seminal single-score metrics such as CLIP-\narXiv:2312.10240v1  [cs.CV]  15 Dec 2023\nScore [19] and more recent question generation and answer-\ning pipelines [8, 10, 24, 57]. While more calibrated and ex-\nplainable, these are expensive and complex models that still\ndo not localize the regions of misalignment in the image.\nIn this paper, we propose a dataset and a model of fine-\ngrained multi-faceted evaluations that are interpretable and\nattributable (e.g., to regions with artifacts/implausibility or\nimage-text misalignments), which provide a much richer\nunderstanding of the image quality than single scalar scores.\nAs a first contribution, we collect a dataset of Rich Hu-\nman Feedback on 18K images (RichHF-18K), which con-\ntains (i) point annotations on the image that highlight re-\ngions of implausibility/artifacts, and text-image misalign-\nment; (ii) labeled words on the prompts specifying the\nmissing or misrepresented concepts in the generated image;\nand (iii) four types of fine-grained scores for image plau-\nsibility, text-image alignment, aesthetics, and overall rat-\ning. Equipped with RichHF-18K, we design a multimodal\ntransformer model, which we coin as Rich Automatic Hu-\nman Feedback (RAHF) to learn to predict these rich human\nannotations on generated images and their associated text\nprompt. Our model can therefore predict implausibility and\nmisalignment regions, misaligned keywords, as well as fine-\ngrained scores. This not only provides reliable ratings, but\nalso more detailed and explainable insights about the qual-\nity of the generated images. To the best of our knowledge,\nthis is the first rich feedback dataset and model for state-of-\nthe-art text-to-image generation models, providing an auto-\nmatic and explainable pipeline to evaluate T2I generation.\nThe main contributions of this paper are summarized below:\n1. The first Rich Human Feedback dataset (RichHF-18K)\non generated images (consisting of fine-grained scores,\nimplausibility(artifact)/misalignment image regions, and\nmisalignment keywords), on 18K Pick-a-Pic images.\nThe RichHF-18K data set will be released soon.\n2. A multimodal Transformer model (RAHF) to predict\nrich feedback on generated images, which we show to\nbe highly correlated with the human annotations on a\ntest set.\n3. We further demonstrate the usefulness of the predicted\nrich human feedback by RAHF to improve image gen-\neration: (i) by using the predicted heatmaps as masks to\ninpaint problematic image regions and (ii) by using the\npredicted scores to help finetune image generation mod-\nels (like Muse [6]), e.g., via selecting/filtering finetuning\ndata, or as reward guidance. We show that in both cases\nwe obtain better images than with the original model.\n4. The improvement on the Muse model, which differs\nfrom the models that generated the images in our train-\ning set, shows the good generalization capacity of our\nRAHF model.\n2. Related works\nText-to-image generation\nText-to-image (T2I) genera-\ntion models have evolved and iterated through several pop-\nular model architectures in the deep learning era. An early\nwork is the Generative Adversarial Network (GAN) [3, 16,\n26], which trains a generator for image generation and a\ndiscriminator to distinguish between real and generated im-\nages, in parallel (also see [32, 38, 47, 55, 60, 62] among\nothers). Another category of generation models develops\nfrom variational auto-encoders (VAEs) [21, 29, 48], which\noptimize evidence lower bound (ELBO) for the likelihood\nof the image data.\nMost recently, Diffusion Models (DMs) [22, 36, 41, 46]\nhave emerged as the SOTA for Image Generation [13]. DMs\nare trained to generate images progressively from random\nnoise, with the ability to capture more diversity than GANs\nand achieve good sample quality [13].\nLatent Diffusion\nModels [41] are a further refinement that performs the dif-\nfusion process in a compact latent space for more efficiency.\nText-to-image evaluation and reward models\nThere has\nbeen much recent work on evaluation of text-to-image mod-\nels along many dimensions [9, 25, 30, 31, 37, 51, 52, 54].\nXu et al. [54] collect a human preference dataset by request-\ning users to rank multiple images and rate them according to\ntheir quality. They trained a reward model ImageReward for\nhuman preference learning, and proposed Reward Feedback\nLearning (ReFL) for tuning diffusion models with the Im-\nageReward model. Kirstain et al. [30] built a web applica-\ntion to collect human preferences by asking users to choose\nthe better image from a pair of generated images, resulting\nin a dataset called Pick-a-Pic with more than 500K exam-\nples generated by T2I models such as Stable Diffusion 2.1,\nDreamlike Photoreal 2.05, and Stable Diffusion XL vari-\nants. They leveraged the human preference dataset to train\na CLIP-based [39] scoring function, called PickScore, to\npredict human preferences. Huang et al. [25] proposed a\nbenchmark called T2I-CompBench for evaluating text-to-\nimage models, which consists of 6,000 text prompts de-\nscribing attribute binding, object relationships, and complex\ncompositions.\nThey utilized multiple pretrained vision-\nlanguage models such as CLIP [39] and BLIP [34] to calcu-\nlate multiple evaluation metrics. Wu et al. [51, 52] collected\na large scale dataset of human choices on generated images\nand utilized the dataset to train a classifier that outputs a Hu-\nman Preference Score (HPS). They showed improvement in\nimage generation by tuning Stable Diffusion with the HPS.\nRecently, Lee [31] proposed a holistic evaluation for T2I\nmodels with multiple fine-grained metrics.\nDespite these valuable contributions, most existing\nworks only use binary human ratings or preference rank-\ning for construction of feedback/rewards, and lack the abil-\nity to provide detailed actionable feedback such as implau-\nMisaligned \nkeywords:\nA panda riding a \nmotorcycle\nPlausibility:\n5    4    3    2    1\nAlignment:\n5    4    3    2    1\nAesthetics:\n5    4    3    2    1\nOverall:\n5    4    3    2    1\nFigure 1. An illustration of our annotation UI. Annotators mark\npoints on the image to indicate artifact/implausibility regions (red\npoints) or misaligned regions (blue points) w.r.t the text prompt.\nThen, they click on the words to mark the misaligned keywords\n(underlined and shaded) and choose the scores for plausibility,\ntext-image alignment, aesthetics, and overall quality (underlined).\nsible regions of the image, misaligned regions, or mis-\naligned keywords on the generated images.\nOne recent\npaper related to our work is Zhang et al. [61], which col-\nlected a dataset of artifact regions for image synthesis tasks,\ntrained a segmentation-based model to predict artifact re-\ngions, and proposed a region inpainting method for those\nregions. However, the focus of their work is artifact re-\ngion only, while in this paper, we collected rich feedback\nfor T2I generation containing not only artifact regions, but\nalso misalignment regions, misaligned keywords, and four\nfine-grained scores from multiple aspects. To the best of\nour knowledge, this is the first work on heterogeneous rich\nhuman feedback for text-to-image models.\n3. Collecting rich human feedback\n3.1. Data collection process\nIn this section, we discuss our procedure to collect the\nRichHF-18K dataset, which includes two heatmaps (ar-\ntifact/implausibility and misalignment), four fine-grained\nscores (plausibility, alignment, aesthetics, overall), and one\ntext sequence (misaligned keywords).\nFor each generated image, the annotators are first asked\nto examine the image and read the text prompt used to gen-\nerate it. Then, they mark points on the image to indicate the\nlocation of any implausibility/artifact or misalignment w.r.t\nthe text prompt. The annotators are told that each marked\npoint has an \u201ceffective radius\u201d (1/20 of the image height),\nwhich forms an imaginary disk centering at the marked\npoint. In this way, we can use a relatively small amount\nof points to cover the image regions with flaws. Lastly, an-\nnotators label the misaligned keywords and the four types of\nscores for plausibility, image-text alignment, aesthetic, and\noverall quality, respectively, on a 5-point Likert scale. De-\ntailed definitions of image implausibility/artifact and mis-\nalignment can be found in the supplementary materials.\nhuman\nanimal\nobject\nindoor\nscene\noutdoor\nscene\nCategory\n0\n1000\n2000\n3000\n4000\nFalse\nTrue\nPhotorealistic\n0\n2500\n5000\n7500\n10000\nFigure 2. Histograms of the PaLI attributes of the images in the\ntraining set.\nWe designed a web UI to facilitate data collection with\nthe following principles: 1) convenience for annotators to\nperform annotations, ideally within a short time for an\nimage-text pair and, 2) allowing annotators to perform all\nannotations on the same UI, so that the fine-grained scores\nare based on the annotated regions and keywords. To this\nend, we created the interface as illustrated in Fig. 1. The\nmain UI consists of an image displayed on the left and a\npanel on the right, where the text prompt is shown at the\ntop of the panel. Annotators are asked to first click on the\nimage to annotate the artifact/implausible regions and mis-\nalignment regions, and then select the misaligned keywords\nand the fine-grained scores on the right of the panel.\nWe created detailed annotation guidelines to instruct the\nannotators regarding the annotation steps, interactions with\nthe web UI, examples of different types of implausibility,\nartifacts, and misalignment. All the annotators (27 in total)\nare trained with the annotation guidelines and calibrated,\nbefore they perform the annotation in order to reduce an-\nnotation discrepancy and improve quality. Our annotation\ntook around 3,000 rater-hours in total. More details about\nour data collection process are in the supplementary.\n3.2. Human feedback consolidation\nTo improve the reliability of the collected human feedback\non generated images, each image-text pair is annotated by\nthree annotators. We therefore need to consolidate the mul-\ntiple annotations for each sample. For the scores, we sim-\nply average the scores from the multiple annotators for an\nimage to obtain the final score. For the misaligned key-\nword annotations, we perform majority voting to get the fi-\nnal sequence of indicators of aligned/misaligned, using the\nmost frequent label for the keywords. For the point anno-\ntations, we first convert them to heatmaps for each anno-\ntation, where each point is converted to a disk region (as\ndiscussed in the last subsection) on the heatmap, and then\nwe compute the average heatmap across annotators. The\nregions with clear implausibility are likely to be annotated\nby all annotators and have a high value on the final average\nheatmap.\n3.3. RichHF-18K: a dataset of rich human feedback\nWe select a subset of image-text pairs from the Pick-a-Pic\ndataset for data annotation. Although our method is gen-\n0.00\n0.08\n0.17\n0.25\n0.33\n0.42\n0.50\n0.58\n0.67\n0.75\n0.83\n0.92\n1.00\nPlausibility\n0\n500\n1000\n1500\n2000\n2500\n3000\nCounts\n0.00\n0.08\n0.17\n0.25\n0.33\n0.42\n0.50\n0.58\n0.67\n0.75\n0.83\n0.92\n1.00\nAlignment\n0.00\n0.08\n0.17\n0.25\n0.33\n0.42\n0.50\n0.58\n0.67\n0.75\n0.83\n0.92\n1.00\nAesthetics\n0.00\n0.08\n0.17\n0.25\n0.33\n0.42\n0.50\n0.58\n0.67\n0.75\n0.83\n0.92\n1.00\nOverall\n0\n500\n1000\n1500\n2000\n2500\n3000\nFigure 3. Histograms of the average scores of image-text pairs in the training set.\n0\n2500\n5000\n7500\n10000 12500 15000\nCounts\nOverall\nAesthetics\nAlignment\nPlausibility\nmax_diff\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 4. Counts of the samples with maximum differences of\nthe scores in the training set.\neral and applicable to any generated images, we choose the\nmajority of our dataset to be photo-realistic images, due to\nits importance and wider applications. Moreover, we also\nwant to have balanced categories across the images. To en-\nsure balance, we utilized the PaLI visual question answer-\ning (VQA) model [7] to extract some basic features from the\nPick-a-Pic data samples. Specifically, we asked the follow-\ning questions for each image-text pair in Pick-a-Pic. 1) Is\nthe image photorealistic? 2) Which category best describes\nthe image? Choose one in \u2018human\u2019, \u2018animal\u2019, \u2018object\u2019, \u2018in-\ndoor scene\u2019, \u2018outdoor scene\u2019. PaLI\u2019s answers to these two\nquestions are generally reliable under our manual inspec-\ntion. We used the answers to sample a diverse subset from\nPick-a-Pic, resulting in 17K image-text pairs. We randomly\nsplit the 17K samples into two subsets, a training set with\n16K samples and a validation set with 1K samples. The\ndistribution of the attributes of the 16K training samples is\nshown in Fig. 2. Additionally, we collect rich human feed-\nback on the unique prompts and their corresponding images\nfrom the Pick-a-Pic test set as our test set. In total, we\ncollected rich human feedback on the 18K image-text pairs\nfrom Pick-a-Pic. Our RichHF-18K dataset consists of 16K\ntraining, 1K validation, and 1K test samples.\n3.4. Data statistics of RichHF-18K\nIn this section, we summarize the statistics of the scores and\nperform the annotator agreement analysis for the scores. We\nstandardize the scores s with the formula (s\u2212smin)/(smax\u2212\nsmin) (smax = 5 and smin = 1) so that the scores lie in the\nrange [0, 1].\nThe histogram plots of the scores are shown in Fig. 3.\nThe distribution of the scores is similar to a Gaussian dis-\ntribution, while the plausibility and text-image alignment\nscores have a slightly higher percentage of score 1.0. The\ndistribution of the collected scores ensures that we have\na reasonable number of negative and positive samples for\ntraining a good reward model.\nTo analyze the rating agreement among annotators for\nan image-text pair, we calculate the maximum difference\namong the scores: maxdiff = max(scores) \u2212 min(scores)\nwhere scores are the three score labels for an image-text\npair. We plot the counts of the maxdiff for each score type\nin Fig. 4. We can see that around 25% of the samples have\nperfect annotator agreement and around 85% of the samples\nhave good annotator agreement (maxdiff is less than or equal\nto 0.25 after the standardization or 1 in the 5-point Likert\nscale).\n4. Predicting rich human feedback\n4.1. Models\n4.1.1\nArchitecture\nThe architecture of our model is shown in Fig. 5. We adopt\na vision-language model based on ViT [14] and T5X [40]\nmodels, inspired by the Spotlight model architecture [33],\nbut modifying both the model and pretraining datasets to\nbetter suit our tasks. We use a self-attention module [49]\namong the concatenated image tokens and text tokens, sim-\nilar to PaLI [7], as our tasks require bidirectional informa-\ntion propagation. The text information is propagated to im-\nage tokens for text misalignment score and heatmap pre-\ndiction, while the vision information is propagated to text\ntokens for better vision-aware text encoding to decode the\ntext misalignment sequence. To pretrain the model on more\ndiverse images, we add the natural image captioning task on\nthe WebLI dataset [7] to the pretraining task mixture.\nSpecifically, the ViT takes the generated image as input\nand outputs image tokens as high-level representations. The\ntext prompt tokens are embedded into dense vectors. The\nimage tokens and embedded text tokens are concatenated\nand encoded by the Transformer self-attention encoder in\nT5X. On top of the encoded fused text and image tokens,\nwe use three kinds of predictors to predict different outputs.\nFor heatmap prediction, the image tokens are reshaped into\nViT\nText \nembed\nImage \ntokens\nText \ntokens\nPlausibility, alignment, aesthetics, overall\nImplausibility/misalignment\nMisaligned keyword sequence\nSelf-attention\nFeature \nmaps\nConv, deconv \nlayers\nHeatmaps\nConv, linear \nlayers\nScores\nText\nT5X \ndecoder\nImage\nText \nprompt\nTokens\nFigure 5. Architecture of our rich feedback model. Our model consists of two streams of computation: one vision and one text stream.\nWe perform self-attention on the ViT-outputted image tokens and the Text-embed module-outputted text tokens to fuse the image and text\ninformation. The vision tokens are reshaped into feature maps and mapped to heatmaps and scores. The vision and text tokens are sent to\na Transformer decoder to generate a text sequence.\na feature map and sent through convolution layers, deconvo-\nlution layers, and sigmoid activation, and outputs implausi-\nbility and misalignment heatmaps. For score prediction, the\nfeature map is sent through convolution layers, linear layers,\nand sigmoid activation, resulting in scalars as fine-grained\nscores.\nTo predict the keyword misalignment sequence, the orig-\ninal prompt used to generate the image is used as text input\nto the model. A modified prompt is used as the prediction\ntarget for the T5X decoder. The modified prompt has a spe-\ncial suffix (\u2018 0\u2019) for each misaligned token, e.g., a yellow 0\ncat if the generated image contains a black cat and the word\nyellow is misaligned with the image. During evaluation, we\ncan extract the misaligned keywords using the special suf-\nfix.\n4.1.2\nModel variants\nWe explore two model variants for the prediction heads of\nthe heatmaps and scores.\nMulti-head\nA straightforward way to predict multiple\nheatmaps and scores is to use multiple prediction heads,\nwith one head for each score and heatmap type. This will\nrequire seven prediction heads in total.\nAugmented prompt\nAnother approach is to use a single\nhead for each prediction type, i.e., three heads for heatmap,\nscore, and misalignment sequence, respectively. To inform\nthe model of the fine-grained heatmap or score type, we\naugment the prompt with the output type. More specifically,\nwe prepend a task string (e.g., \u2018implausibility heatmap\u2019) to\nthe prompt for each particular task of one example and use\nthe corresponding label as the training target. During in-\nference, by augmenting the prompt with the corresponding\ntask string, the single heatmap (score) head can predict dif-\nferent heatmaps (scores). As we show in the experiments,\nthis augmented prompt approach can create task-specific vi-\nsion feature maps and text encodings, which performs sig-\nnificantly better in some of the tasks.\n(a) Image\n(b) GT\n(c) Our model\n(d) ResNet-50\nFigure 6. Examples of implausibility heatmaps\n(a) Image\n(b) GT\n(c) Our model\n(d) CLIP gradient\nFigure 7. Examples of misalignment heatmaps. Prompt: A snake\non a mushroom.\n4.1.3\nModel optimization\nWe train the model with a pixel-wise mean squared error\n(MSE) loss for the heatmap prediction, and MSE loss for\nthe score prediction. For misalignment sequence predic-\ntion, the model is trained with teacher-forcing cross-entropy\nloss. The final loss function is the weighted combination of\nthe heatmap MSE loss, score MSE loss, and the sequence\nteacher-forcing cross-entropy loss.\n4.2. Experiments\n4.2.1\nExperimental setup\nOur model is trained on the 16K RichHF-18K training sam-\nples, and the hyperparameters were tuned using the model\nperformance on the 1K RichHF-18K validation set. The\nhyperparameters setup can be found in supplementary ma-\nterial.\nEvaluation metrics\nFor score prediction tasks, we re-\nport the Pearson linear correlation coefficient (PLCC) and\nSpearman rank correlation coefficient (SRCC), which are\ntypical evaluation metrics for score predictions [28]. For\nheatmap prediction tasks, a straightforward way to evaluate\nPlausibility\nAesthetics\nText-image Alignment\nOverall\nPLCC\nSRCC\nPLCC\nSRCC\nPLCC\nSRCC\nPLCC\nSRCC\nResNet-50\n0.495\n0.487\n0.370\n0.363\n0.108\n0.119\n0.337\n0.308\nCLIP\n\u2212\n\u2212\n\u2212\n\u2212\n0.185\n0.130\n\u2212\n\u2212\nOur Model (multi-head)\n0.666\n0.654\n0.605\n0.591\n0.487\n0.500\n0.582\n0.561\nOur Model (augmented prompt)\n0.693\n0.681\n0.600\n0.589\n0.474\n0.496\n0.580\n0.562\nTable 1. Score prediction results on the test set.\nAll data\nGT = 0\nGT > 0\nMSE \u2193\nMSE \u2193\nCC \u2191\nKLD \u2193\nSIM \u2191\nNSS \u2191\nAUC-Judd \u2191\nResNet-50\n0.00996\n0.00093\n0.506\n1.669\n0.338\n2.924\n0.909\nOurs (multi-head)\n0.01216\n0.00141\n0.425\n1.971\n0.302\n2.330\n0.877\nOurs (augmented prompt)\n0.00920\n0.00095\n0.556\n1.652\n0.409\n3.085\n0.913\nTable 2.\nImplausibility heatmap prediction results on the test set.\nGT = 0 refers to empty implausibility heatmap, i.e., no arti-\nfacts/implausibility, for ground truth. GT > 0 refers to heatmaps with artifacts/implausibility, , for ground truth.\n(a) Prompt: gamer playing league of\nlegends at night.\nPlausibility score.\nGT: 0.333, Our model: 0.410\nOverall score.\nGT: 0.417, Our model: 0.457\n(b) Prompt: An endless wavy ocean\nunder a colorful night sky artistic\npainting pastel.\nPlausibility score.\nGT: 1.0, Our model: 0.979\nOverall score.\nGT 1.0, Our model: 0.848\n(c) Prompt: Mechanical bee flying in\nnature electronics motors wires but-\ntons lcd.\nText-image alignment score.\nGT: 0.583, Our model: 0.408\nAesthetics score.\nGT: 0.75, Our model: 0.722\n(d) Prompt: anime fortnite character.\nText-image alignment score.\nGT: 1.0, Our model: 0.897\nAesthetics score.\nGT: 0.75, Our model: 0.713\nFigure 8. Examples of ratings. \u201cGT\u201d is the ground-truth score (average score from three annotators).\n(a) Muse [6] before finetuning\n(b) Muse [6] after finetuning\n(c) LD [41] without guidance\n(d) LD [41] after aesthetic guidance\nFigure 9. Examples illustrating the impact of RAHF on generative models. (a-b): Muse [6] generated images before and after finetuning\nwith examples filtered by plausibility scores, prompt: A cat sleeping on the ground using a shoe as a pillow. (c-d): Results without and\nwith aesthetic score used as Classifier Guidance [2] on Latent Diffusion (LD) [41], prompt: a macro lens closeup of a paperclip.\nAll data\nGT = 0\nGT > 0\nMSE \u2193\nMSE \u2193\nCC \u2191\nKLD \u2193\nSIM \u2191\nNSS \u2191\nAUC-Judd \u2191\nCLIP gradient\n0.00817\n0.00551\n0.015\n3.844\n0.041\n0.143\n0.643\nOur Model (multi-head)\n0.00303\n0.00015\n0.206\n2.932\n0.093\n1.335\n0.838\nOur Model (augmented prompt)\n0.00304\n0.00006\n0.212\n2.933\n0.106\n1.411\n0.841\nTable 3. Text misalignment heatmap prediction results on the test set. GT = 0 refers to empty misalignment heatmap, i.e., no misalign-\nment, for ground truth. GT > 0 refers to heatmaps with misalignment, for ground truth.\n(a) Prompt: a baseball with the parthenon on its cover, sitting on the pitcher\u2019s mound\n(b) Prompt: A photograph of a beautiful, modern house that is located in a quiet neighborhood. The house is made of brick and has a large front porch. It\nhas a manicured lawn and a large backyard.\nFigure 10. Region inpainting with Muse [6] generative model. From left to right, the 4 figures are: original images with artifacts from\nMuse, predicted implausibility heatmaps from our model, masks by processing (thresholding, dilating) the heatmaps, and new images from\nMuse region inpainting with the mask, respectively.\nPrecision\nRecall\nF1 Score\nMulti-head\n62.9\n33.0\n43.3\nAugmented prompt\n61.3\n34.1\n43.9\nTable 4. Text misalignment prediction results on the test set.\nPreference\n\u226b\n>\n\u2248\n<\n\u226a\nPercentage\n21.5%\n30.33%\n31.33%\n12.67%\n4.17%\nTable 5. Human Evaluation Results: Finetuned Muse vs origi-\nnal Muse model preference: Percentage of examples where fine-\ntuned Muse is significantly better (\u226b), slightly better (>), about\nthe same (\u2248), slightly worse (<), significantly worse (\u226a) than\noriginal Muse. Data was collected from 6 individuals in a ran-\ndomized survey.\nthe results would be to borrow standard saliency heatmap\nevaluation metrics such as NSS/KLD [5]. However, these\nmetrics cannot be applied directly in our case as all these\nmetrics assume the ground truth heatmap is not empty; yet\nin our case, empty ground truth is possible (e.g., for arti-\nfact/implausibility heatmap, it means the image does not\nhave any artifact/implausibility). As such, we report MSE\non all samples and on those with empty ground truth, re-\nspectively, and report saliency heatmap evaluation metrics\nlike NSS/KLD/AUC-Judd/SIM/CC [5] for the samples with\nnon-empty ground truth. For the misaligned keyword se-\nquence prediction, we adopt the token-level precision, re-\ncall, and F1-score.\nSpecifically, the precision/recall/F1\nscores are computed for the misaligned keywords over all\nthe samples.\nBaselines\nFor comparison, we finetune two ResNet-50\nmodels [18], with multiple fully connected layers and de-\nconvolution heads to predict the scores and heatmaps ac-\ncordingly. For the misalignment score, we use the CLIP\nscore as a baseline. For misalignment heatmap prediction,\nwe use CLIP gradient [45] map as a baseline.\n4.2.2\nPrediction result on RichHF-18K test set\nQuantitative analysis\nThe experimental results of our\nmodel prediction on the four fine-grained scores, the im-\nplausibility heatmap, misalignment heatmap, and misalign-\nment keyword sequence on our RichHF-18K test set are\npresented in Tab. 1, Tab. 2, Tab. 3, and Tab. 4 respectively.\nIn both Tab. 1 and Tab. 3, the two variants of our pro-\nposed model both significantly outperform ResNet-50 (or\nCLIP for text-image alignment score).\nYet, in Tab. 2,\nthe multi-head version of our model performs worse than\nResNet-50, but our augmented prompt version outperforms\nResNet-50. The main reason might be that in the multi-\nhead version, without augmenting the prediction task in the\nprompt, the same prompt is used for all the seven prediction\ntasks, and hence the feature maps and text tokens will be\nthe same for all tasks. It might not be easy to find a good\ntradeoff among these tasks, and hence the performance of\nsome tasks such as artifact/implausibility heatmap became\nworse. However, after augmenting the prediction task into\na prompt, the feature map and text token can be adapted to\neach particular task with better results. Additionally, we\nnote that misalignment heatmap prediction generally has\nworse results than artifact/implausibility heatmap predic-\ntion, possibly because misalignment regions are less well-\ndefined, and the annotations may therefore be noisier.\nQualitative examples\nWe show some example pre-\ndictions from our model for implausibility heatmap\n(Fig. 6), where our model identifies the regions with arti-\nfact/implausibility, and for misalignment heatmap (Fig. 7),\nwhere our model identifies the objects that don\u2019t correspond\nto the prompt. Fig. 8 shows some example images and their\nground-truth and predicted scores. More examples are in\nthe supplementary material.\n5. Learning from rich human feedback\nIn this section, we investigate whether the predicted rich hu-\nman feedback (e.g., scores and heatmaps) can be used to im-\nprove image generation. To ensure that the gains from our\nRAHF model generalize across generative model families,\nwe use Muse [6] as our target model to improve, which is\nbased on a masked transformer architecture and thus differ-\nent from the Stable Diffusion model variants in our RichHF-\n18K dataset.\nFinetuning generative models with predicted scores\nWe first illustrate that finetuning with RAHF scores can im-\nprove Muse. First, we generate eight images for each of\nthe 12,564 prompts (the prompt set is created via PaLM\n2 [1, 11] with some seed prompts) using the pre-trained\nMuse model.\nWe predict RAHF scores for each image,\nand if the highest score for the images from each prompt\nis above a fixed threshold, it will be selected as part of our\nfinetuning dataset. The Muse model is then finetuned with\nthis dataset. This approach could be viewed as a simplified\nversion of Direct Preference Optimization [15].\nIn Fig. 9 (a)-(b), we show one example of finetun-\ning Muse with our predicted plausibility score (thresh-\nold=0.8).\nTo quantify the gain from Muse finetuning,\nwe used 100 new prompts to generate images, and asked\n6 annotators to perform side-by-side comparisons (for\nplausibility) between two images from the original Muse\nand the fine-tuned Muse respectively.\nThe annotators\nchoose from five possible responses (image A is signifi-\ncantly/slightly better than image B, about the same, im-\nage B is slightly/significantly better than image A), without\nknowledge of which model is used to generate the image\nA/B. The results in Tab. 5 demonstrate that the finetuned\nMuse with RAHF plausibility scores processes significantly\nfewer artifacts/implausibility than the original Muse.\nMoreover, in Fig. 9 (c)-(d), we show an example of\nusing the RAHF aesthetic score as Classifier Guidance to\nthe Latent Diffusion model [41], similar to the approach\nin Bansal et al. [2], demonstrating that each of the fine-\ngrained scores can improve different aspects of the gener-\native model/results.\nRegion inpainting with predicted heatmaps and scores\nWe demonstrate that our model\u2019s predicted heatmaps and\nscores can be used to perform region inpainting to im-\nprove the quality of generated images. For each image, we\nfirst predict implausibility heatmaps, then create a mask by\nprocessing the heatmap (using thresholding and dilating).\nMuse inpainting [6] is applied within the masked region to\ngenerate new images that match the text prompt. Multiple\nimages are generated, and the final image is chosen by the\nhighest predicted plausibility score by our RAHF.\nIn Fig. 10, we show several inpainting results with our\npredicted implausibility heatmaps and plausibility scores.\nAs shown, more plausible images with fewer artifacts are\ngenerated after inpainting.\nAgain, this shows that our\nRAHF generalizes well to images from a generative model\nvery different from the ones whose images are used to train\nRAHF. More details and examples can be found in the sup-\nplementary material.\n6. Conclusions and limitations\nIn this work, we contributed RichHF-18K, the first rich hu-\nman feedback dataset for image generation. We designed\nand trained a multimodal Transformer to predict the rich\nhuman feedback, and demonstrated some instances to im-\nprove image generation with our rich human feedback.\nWhile some of our results are quite exciting and promis-\ning, there are several limitations to our work. First, the\nmodel performance on the misalignment heatmap is worse\nthan that on the implausibility heatmaps, possibly due to the\nnoise in the misalignment heatmap. It is somewhat ambigu-\nous how to label some misalignment cases such as absent\nobjects on the image. Improving the misalignment label\nquality is one of the future directions. Second, it would\nbe helpful to collect more data on generative models be-\nyond Pick-a-Pic (Stable Diffusion) and investigate their ef-\nfect on the RAHF models.\nMoreover, while we present\nthree promising ways to leverage our model to improve T2I\ngeneration, there is a myriad of other ways to utilize rich\nhuman feedback that can be explored, e.g., how to use the\npredicted heatmaps or scores as a reward signal to finetune\ngenerative models with reinforcement learning, and how to\nuse the predicted heatmaps as a weighting map, or how to\nuse the predicted misaligned sequences in learning from hu-\nman feedback to help improve image generation, etc. We\nhope RichHF-18K and our initial models inspire quests to\ninvestigate these research directions in future work.\nReferences\n[1] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2\ntechnical report. arXiv preprint arXiv:2305.10403, 2023. 8\n[2] Arpit\nBansal,\nHong-Min\nChu,\nAvi\nSchwarzschild,\nSoumyadip Sengupta,\nMicah Goldblum,\nJonas Geip-\ning, and Tom Goldstein. Universal guidance for diffusion\nmodels, 2023. 6, 8\n[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis.\narXiv preprint arXiv:1809.11096, 2018. 2\n[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n1\n[5] Zoya Bylinskii, Tilke Judd, Aude Oliva, Antonio Torralba,\nand Fr\u00b4edo Durand. What do different evaluation metrics tell\nus about saliency models?\nIEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2018. 7\n[6] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. arXiv preprint arXiv:2301.00704, 2023. 2, 6, 7, 8\n[7] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,\nPiotr Padlewski, Daniel Salz, Sebastian Goodman, Adam\nGrycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,\nGaurav Mishra, Linting Xue, Ashish Thapliyal, James Brad-\nbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia,\nBurcu Karagol Ayan, Carlos Riquelme, Andreas Steiner,\nAnelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu\nSoricut. Pali: A jointly-scaled multilingual language-image\nmodel, 2022. 4\n[8] Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ran-\njay Krishna, Jason Baldridge, Mohit Bansal, Jordi Pont-\nTuset, and Su Wang. Davidsonian Scene Graph: Improv-\ning Reliability in Fine-Grained Evaluation for Text-to-Image\nGeneration. In arXiv:2310.18235, 2023. 2\n[9] Jaemin Cho, Abhay Zala, and Mohit Bansal.\nVisual pro-\ngramming for text-to-image generation and evaluation. In\nNeurIPS, 2023. 2\n[10] Jaemin Cho, Abhay Zala, and Mohit Bansal.\nVisual pro-\ngramming for text-to-image generation and evaluation. In\nNeurIPS, 2023. 2\n[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.\nPalm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 8\n[12] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,\nand Mubarak Shah. Diffusion models in vision: A survey.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 2023. 1\n[13] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 2\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 4\n[15] Ying\nFan,\nOlivia\nWatkins,\nYuqing\nDu,\nHao\nLiu,\nMoonkyung Ryu, Craig Boutilier, Pieter Abbeel, Moham-\nmad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok:\nReinforcement learning for fine-tuning text-to-image diffu-\nsion models. In Advances in Neural Information Processing\nSystems, 2023. 8\n[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems, 27, 2014. 2\n[17] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Josh Susskind, and\nNavdeep Jaitly. Matryoshka diffusion models, 2023. 1\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 8\n[19] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation met-\nric for image captioning. In EMNLP, 2022. 2\n[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 1\n[21] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,\nXavier Glorot, Matthew Botvinick, Shakir Mohamed, and\nAlexander Lerchner. beta-vae: Learning basic visual con-\ncepts with a constrained variational framework. In Interna-\ntional conference on learning representations, 2016. 2\n[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2\n[23] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 1\n[24] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Os-\ntendorf, Ranjay Krishna, and Noah A. Smith. Tifa: Accurate\nand interpretable text-to-image faithfulness evaluation with\nquestion answering. In ICCV, 2023. 2\n[25] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xi-\nhui Liu. T2i-compbench: A comprehensive benchmark for\nopen-world compositional text-to-image generation. arXiv\npreprint arXiv:2307.06350, 2023. 2\n[26] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4401\u20134410, 2019. 2\n[27] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models.\nIn\nCVPR, 2023. 1\n[28] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and\nFeng Yang. Musiq: Multi-scale image quality transformer.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, 2021. 5\n[29] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 2\n[30] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-\ntiana, Joe Penna, and Omer Levy.\nPick-a-pic: An open\ndataset of user preferences for text-to-image generation.\narXiv preprint arXiv:2305.01569, 2023. 1, 2\n[31] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai,\nJoon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak\nNarayanan, Hannah Benita Teufel, Marco Bellagente, Min-\nguk Kang, Taesung Park, Jure Leskovec, Jun-Yan Zhu, Li\nFei-Fei, Jiajun Wu, Stefano Ermon, and Percy Liang. Holis-\ntic evaluation of text-to-image models. In Neural Informa-\ntion Processing Systems Datasets and Benchmarks Track,\n2023. 2\n[32] Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip\nTorr.\nControllable text-to-image generation.\nAdvances in\nNeural Information Processing Systems, 32, 2019. 2\n[33] Gang Li and Yang Li. Spotlight: Mobile ui understanding\nusing vision-language models with a focus. In International\nConference on Learning Representations, 2023. 4\n[34] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In In-\nternational Conference on Machine Learning, pages 12888\u2013\n12900. PMLR, 2022. 2\n[35] Haomiao Ni, Changhao Shi, Kai Li, Sharon X. Huang, and\nMartin Renqiang Min. Conditional image-to-video gener-\nation with latent flow diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 18444\u201318455, 2023. 1\n[36] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2\n[37] Mayu Otani, Riku Togashi, Yu Sawai, Ryosuke Ishigami,\nYuta Nakashima, Esa Rahtu, Janne Heikkil\u00a8a, and Shin\u2019ichi\nSatoh.\nToward verifiable and reproducible human evalu-\nation for text-to-image generation.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14277\u201314286, 2023. 2\n[38] Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao.\nMirrorgan: Learning text-to-image generation by redescrip-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 1505\u20131514,\n2019. 2\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2\n[40] Adam Roberts, Hyung Won Chung, Anselm Levskaya,\nGaurav Mishra, James Bradbury, Daniel Andor, Sharan\nNarang, Brian Lester, Colin Gaffney, Afroz Mohiuddin,\nCurtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc\nvan Zee, Jacob Austin, Sebastian Goodman, Livio Bal-\ndini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha\nChowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia,\nJianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H.\nClark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin\nRaffel, Noam Shazeer, Marvin Ritter, Maarten Bosma,\nAlexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel,\nMark Omernick, Brennan Saeta, Ryan Sepassi, Alexander\nSpiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling\nup models and data with t5x and seqio. arXiv preprint\narXiv:2203.17189, 2022. 4\n[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1, 2, 6, 8\n[42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 1\n[43] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing\nsystems, 29, 2016. 1\n[44] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain,\nAmit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman.\nEmu edit: Precise image editing via recognition and genera-\ntion tasks, 2023. 1\n[45] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.\nDeep inside convolutional networks: Visualising image clas-\nsification models and saliency maps. In 2nd International\nConference on Learning Representations, ICLR 2014, Banff,\nAB, Canada, April 14-16, 2014, Workshop Track Proceed-\nings, 2014. 8\n[46] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n2\n[47] Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun\nBao, and Changsheng Xu.\nDf-gan: A simple and effec-\ntive baseline for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16515\u201316525, 2022. 2\n[48] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. Advances in neural information pro-\ncessing systems, 30, 2017. 2\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 4\n[50] S. Wang, C. Saharia, C. Montgomery, J. Pont-Tuset, S. Noy,\nS. Pellegrini, Y. Onoe, S. Laszlo, D. J. Fleet, R. Soricut, J.\nBaldridge, M. Norouzi, P. Anderson, and W. Chan. Imagen\neditor and editbench: Advancing and evaluating text-guided\nimage inpainting. In CVPR, 2023. 1\n[51] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng\nZhu, Rui Zhao, and Hongsheng Li. Human preference score\nv2: A solid benchmark for evaluating human preferences of\ntext-to-image synthesis. arXiv preprint arXiv:2306.09341,\n2023. 2\n[52] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hong-\nsheng Li.\nHuman preference score: Better aligning text-\nto-image models with human preference.\nIn Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 2096\u20132105, 2023. 1, 2\n[53] Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang\nXu, Zuxuan Wu, and Yu-Gang Jiang. A survey on video\ndiffusion models. arXiv preprint arXiv:2310.10647, 2023. 1\n[54] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai\nLi, Ming Ding, Jie Tang, and Yuxiao Dong.\nImagere-\nward: Learning and evaluating human preferences for text-\nto-image generation. In Neural Information Processing Sys-\ntems, 2023. 1, 2\n[55] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,\nZhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-\ngrained text to image generation with attentional generative\nadversarial networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 1316\u2013\n1324, 2018. 2\n[56] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run-\nsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-\nHsuan Yang. Diffusion models: A comprehensive survey of\nmethods and applications. ACM Computing Surveys, 2022.\n1\n[57] Michal Yarom, Yonatan Bitton, Soravit Changpinyo, Roee\nAharoni, Jonathan Herzig, Oran Lang, Eran Ofek, and Idan\nSzpektor. What you see is what you read? improving text-\nimage alignment evaluation, 2023. 2\n[58] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2(3):5, 2022. 1\n[59] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang,\nand In So Kweon. Text-to-image diffusion model in gener-\native ai: A survey. arXiv preprint arXiv:2303.07909, 2023.\n1\n[60] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-\ngang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-\ngan: Text to photo-realistic image synthesis with stacked\ngenerative adversarial networks. In Proceedings of the IEEE\ninternational conference on computer vision, pages 5907\u2013\n5915, 2017. 2\n[61] Lingzhi Zhang, Zhengjie Xu, Connelly Barnes, Yuqian\nZhou, Qing Liu, He Zhang, Sohrab Amirghodsi, Zhe Lin, Eli\nShechtman, and Jianbo Shi. Perceptual artifacts localization\nfor image synthesis tasks. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 7579\u2013\n7590, 2023. 3\n[62] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan:\nDynamic memory generative adversarial networks for text-\nto-image synthesis. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n5802\u20135810, 2019. 2\nRich Human Feedback for Text-to-Image Generation\nSupplementary Material\n7. Ethical conduct\nOur data collection has been approved by an Institutional\nReview Board.\n8. Data collection details\n8.1. Image artifacts/implausibility definitions\n1. Distorted human/animal bodies/faces\n(a) Distorted/combined faces and/or body parts (unless\nspecified in the text caption)\n(b) Missing body parts (unless specified in the text cap-\ntion)\n(c) Additional body parts (unless specified in the text\ncaption)\n2. Distorted objects (non human/animal)\n(a) Distorted objects (e.g., furniture, vehicles, build-\nings) (unless specified in the text caption)\n3. Distorted/Nonsensical text\n(a) Text that is distorted, nonsensical, or misspelled\n(unless specified in the text caption)\n4. Nonsensical Representations\n(a) Representations that are unrealistic/nonsensical\n(unless specified in the text caption), or difficult to\nunderstand\n5. Excessive blurriness/lack of sharpness\n(a) The image contains excessive blurriness or quality\nthat detracts from the overall image (focus on one\npart of the image is OK)\n(b) The image contains a lack of definition/sharpness\nthat detracts from the overall image\n6. Any other artifacts or implausibility not covered above\n8.2. Text-image misalignment definitions and what-\nto-do\nSince we require the annotators to mark the misaligned\nwords in the text prompt, we differentiate this part from\nSec. 8.1 by including a what-to-do under each definition.\n1. Something is missing: a human/animal/object specified\nin the text caption is missing in the image\n\u2022 Click on that word of the human/animal/object in the\ntext\n2. Incorrect attributes: an attribute (e.g., color) of an ob-\nject specified in the text is incorrect in the image\n\u2022 Click on that word of the attribute in the text and click\non the region of the object on the image\n3. Incorrect actions: an action specified in the text caption\nis not represented in the image\n\u2022 Click on that word of the action in the text and click\non the region of the wrong actions on the image\n4. Incorrect numbers: counts of humans/animals/objects\nin the image do not match those specified in the text\n\u2022 Click on the number in the text\n5. Incorrect position: the spatial position of two entities\nin the image does not match that specified in the text\n\u2022 Click on the word of the position in the text\n6. Other: any other inconsistency between text and image\n\u2022 Click on the word of the inconsistency in the text\n8.3. Additional details\nAnnotation guideline\nTo ensure the annotators under-\nstand the above definitions, we provide 4-10 examples for\neach definition of the annotation terms in the guideline. All\nof our annotators can read English and thus understand the\ntext prompts. In some of the prompts, there are concepts or\nperson names in the text prompts that are uncommon and\nmay cause confusion to the annotators. Therefore, we in-\nstruct the annotators to do a quick search on the internet\nregarding any unfamiliar concepts in the text prompts and\nskip samples with confusing prompts full of strange con-\ncepts.\nDataset size\nSince the Pick-a-Pic v1 dataset contains\nsome images and/or prompts that are inappropriate (e.g.,\ncontaining nudity), we ask the annotators to mark these im-\nages with a special flag and skip the annotation. We filter\nout these inappropriate images and/or prompts during data\npost-processing. For this reason, the total number of images\nin our final training set is around 300 short of 16K.\n8.4. Discussions and limitations\nWe choose points over bounding boxes in our region an-\nnotation because we find that points are much faster to mark\nand can have a reasonable coverage over image regions with\nvarious shapes when we specify an effective radius for each\npoint as discussed in the main paper.\nAs a limitation in our region/heatmap annotations,\nwe notice there is an over-annotation issue in the arti-\nfacts/implausibility region annotation. Specifically, our an-\nnotators tend to annotate more human faces and hands on\nthe images than necessary. One reason is that human faces\nand hands in the Pick-a-Pic dataset indeed have more arti-\nfacts/implausibility than other parts. Moreover, the annota-\ntors, as humans, may naturally pay more attention to human\nfaces and hands, resulting in over-annotation of these parts.\nNevertheless, the over-annotation issue is minor in our final\ndataset, as we strive to provide feedback to the annotators\nfrequently to make them less nitpicking about human faces\nand hands.\nAnother limitation is the diversity of the subjects in the\nprompts/images. The Pick-a-Pic dataset (and many others)\nis predominantly full of human, dog, and cat subjects. For\nthis reason, it is challenging to find a very diverse dataset\nfor annotation. We strive to make the subjects more diverse\nby using balanced categories as indicated by the PaLI at-\ntributes (as in Fig. 2). We didn\u2019t choose more fine-grained\ncategories for PaLI to test as there would be an endless list\nof subjects we could consider. Therefore, we leave the goal\nof annotating more diverse images/prompts in future works.\n9. Experimental details\nHyperparameters\nThe main model components consist\nof a ViT B16 encoder for image encoding, a T5 base en-\ncoder for mixing image and text tokens, and three pre-\ndictors for score, heatmap, and text misalignment, re-\nspectively.\nThe ViT B16 encoder uses a 16x16 patch\nsize, 12 layers with 12 heads with a hidden dimension\nof 768, wherein the MLP has a hidden dimension of\n3072. The T5 base encoder uses 12 layers with 12 heads\nwith a hidden dimension of 768, wherein the MLP has\na hidden dimension of 2048.\nThe score predictor con-\nsists of four convolutional layers with layer norm and\nReLU activation, and the filter size, kernel size, and\nstrides are [768, 384, 128, 64], [2, 2, 2, 2], [1, 1, 1, 1], respec-\ntively.\nThree dense layers of output sizes 2048, 1024,\nand 1, respectively, are used to generate a scalar with\nReLU activation for the first two layers and sigmoid for\nthe last.\nThe heatmap predictor consists of two con-\nvolution layers with filter size, kernel size, and stride\nas [768, 384], [3, 3], [1, 1], respectively.\nIt then uses four\nde-convolution layers to up-sample to the required out-\nput size, with the filter size, kernel size, and stride\nas [768, 384, 384, 192], [3, 3, 3, 3], [2, 2, 2, 2], respectively.\nEach de-convolution layer is with two read-out convolution\nlayers of kernel size 3 and stride 1. Layer norm and ReLU\nare used for each layer. In the end, two read-out convolution\nlayers and a final sigmoid activation are used to generate the\nheatmap prediction. The text predictor is implemented us-\ning a T5 base decoder with 12 layers of 12 heads, MLP\ndimension 2048, and hidden dimension 768. The output to-\nken length is 64.\nWe train the model on the datasets with a batch size of\n256 for 20K iterations. We utilize the AdamW optimizer\nwith a base learning rate of 0.015. We linearly increase the\nlearning rate from 0 to the base learning rate in the first 2000\niterations, and then decrease the learning rate with a recip-\nrocal square root scheduler w.r.t the number of iterations.\nImage augmentations\nFor each image, we randomly\ncrop it by sampling a bounding box with 80%-100% width\nand 80%-100% height. The cropping is applied by 50%\nchance and otherwise the original image is used. Note that\nwe also crop the corresponding part of the implausibility\nheatmap and misalignment heatmap to match the cropped\nimage. We then create an augmented version of the im-\nage by applying several random augmentations including\nrandom brightness (max delta 0.05), random contrast (ran-\ndom contrast factor between 0.8 and 1), random hue (max\ndelta 0.025), random saturation (random saturation factor\nbetween 0.8 and 1) and random jpeg noise (jpeg quality be-\ntween 70 and 100). By 10% chance the augmented version\nis used instead of the original image. We convert the image\nto grayscale by 10% probability as the final image.\n10. Additional qualitative examples\nFig. 11 provides more examples of artifacts/implausibility\nheatmaps. We can see that our RAHF model can more ac-\ncurately locate the positions of artifacts/implausibility on\nvarious subjects such as human hands, animals, vehicles,\nand concept arts.\nFig. 12 provides more examples of misalignment\nheatmaps. We can see that our RAHF model can more accu-\nrately locate the positions of misalignment on various sub-\njects such as animals, objects, and different outdoor scenes.\nFor example, our model can identify the subtle difference\nbetween the real handlebar of a Segway and the one de-\npicted in the image.\nFig. 13 provides more examples of score predictions,\nwhere our RAHF model predicts scores that are quite close\nto the ground truth score from human evaluation.\nFig. 14 provides examples for the misaligned keywords\nprediction, which shows that our RAHF model can predict\nthe majority of the misaligned keywords marked by human\nannotators.\nFig. 15 provides more examples of the comparison be-\nfore and after finetuning Muse with examples selected\nbased on the predicted scores by our RAHF model and ex-\namples of using RAHF model predicted overall score as\nClassifier Guidance. We can see enhanced image quality\nof the generation from the finetuned Muse model and the\nLatent Diffusion model, which highlights the potential of\nimproving T2I generation with our reward model.\nFig. 16 provides more examples of Muse inpainting\nwith the predicted masks (converted from heatmaps) by our\nRAHF model, where the inpainted regions are significantly\nimproved in plausibility.\n(a) Image\n(b) GT\n(c) Our model\n(d) ResNet-50\nFigure 11. More examples of implausibility heatmaps\n(a) Prompt: Photo of a cat eating a burger like a person\n(b) Prompt: An abandoned Segway in the forest\n(c) Prompt: inflatable pie floating down a river\n(d) Prompt: A Red Pigeon Sat on a Branch Reflecting on Existence\nImage\nGT\nOur model\nCLIP gradient\nFigure 12. More examples of misalignment heatmaps.\n(a) Prompt: Computer science stu-\ndents fighting with computer key-\nboards.\nPlausibility score.\nGT: 0.25, Our model: 0.236\nOverall score.\nGT: 0.5, Our model: 0.341\n(b) Prompt:\nmeditation under a\nrainbow during a thunderstorm.\nPlausibility score.\nGT: 0.5, Our model: 0.448\nOverall score.\nGT: 0.583, Our model: 0.505\n(c) Prompt:\nA needle-felted palm\ntree.\nText-image alignment score.\nGT: 0.75, Our model: 0.988\nAesthetics score.\nGT: 0.75, Our model: 0.961\n(d) Prompt: Renault Capture on a\nbeach.\nText-image alignment score.\nGT: 1.0, Our model: 0.877\nAesthetics score.\nGT: 0.75, Our model: 0.720\n(e) Prompt: all the letters of the greek\nalphabet.\nPlausibility score.\nGT: 0.167, Our model: 0.331\nOverall score.\nGT: 0.250, Our model: 0.447\n(f) Prompt: a kittens in box.\nPlausibility score.\nGT: 0.75, Our model: 0.851\nOverall score.\nGT: 0.75, Our model: 0.855\n(g) Prompt:\nmonkey climbing a\nskyscraper.\nText-image alignment score.\nGT: 0.833, Our model: 0.536\nAesthetics score.\nGT: 0.583, Our model: 0.467\n(h) Prompt: bread.\nText-image alignment score.\nGT: 1.0, Our model: 0.975\nAesthetics score.\nGT: 1.0, Our model: 0.984\nFigure 13. Examples of ratings. \u201cGT\u201d is the ground-truth score (average score from three annotators).\n(a) The prompt is: Two cats watering roses in a greenhouse.\nThe\nground truth labels two, watering, greenhouse as misaligned key-\nwords and our model predicts two, greenhouse as misaligned keywords.\n(b) The prompt is: A close up photograph of a fat orange cat with\nlasagna in its mouth, shot on Leica M6. The ground truth labels fat,\nlasagna, Leica, M6 as misaligned keywords and our model predicts\nlasagna, Leica, M6 as misaligned keywords.\nFigure 14. Examples for text misalignment prediction.\n(a) Muse before finetuning\n(b) Muse after finetuning\n(c) Muse before finetuning\n(d) Muse after finetuning\n(e) Muse before finetuning\n(f) Muse after finetuning\n(g) LD without guidance\n(h) LD with overall guidance\nFigure 15. More examples illustrating the impact of RAHF on generative models. (a-f): Muse [6] generated images before and after\nfinetuning with examples filtered by plausibility scores. Prompt: (a-b): Three zebras are standing together in a line. (c-d): An elephant\nscratching it\u2019s neck on a post. (e-f): Apples, lemons, grapes, oranges and other fruits in crates. (g-h): Results without and with overall\nscore used as Classifier Guidance [2] on Latent Diffusion (LD) [41], prompt: Kitten sushi stained glass window sunset fog.\n(a) Prompt: A 3d printed sculpture of a cat made of iron and plastic, with arabic translation and ic gradients.\n(b) Prompt: A 1960s slide out camper with a blonde, white and red color scheme\nFigure 16. Region inpainting with Muse [6] generative model. From left to right, the 4 figures are: original images with artifacts from\nMuse, predicted implausibility heatmaps from our model, masks by processing (thresholding, dilating) the heatmaps, and new images from\nMuse region inpainting with the mask, respectively.\n"
  },
  {
    "title": "GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning",
    "link": "https://arxiv.org/pdf/2312.11461.pdf",
    "upvote": "16",
    "text": "GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning\nYe Yuan\u2217\nXueting Li\u2217\nYangyi Huang\nShalini De Mello\nKoki Nagano\nJan Kautz\nUmar Iqbal\nNVIDIA\nhttps://nvlabs.github.io/GAvatar\nAvatar Animation\n3D Gaussian Avatar Generation and Mesh Extraction\nFigure 1. GAvatar synthesizes high-fidelity 3D animatable avatars from text prompts. Our novel primitive-based implicit Gaussian repre-\nsentation enables efficient avatar animation (100 fps, 1K resolution) and also extracts a highly detailed mesh from learned 3D Gaussians.\nAbstract\nGaussian splatting has emerged as a powerful 3D rep-\nresentation that harnesses the advantages of both explicit\n(mesh) and implicit (NeRF) 3D representations.\nIn this\npaper, we seek to leverage Gaussian splatting to generate\nrealistic animatable avatars from textual descriptions, ad-\ndressing the limitations (e.g., flexibility and efficiency) im-\nposed by mesh or NeRF-based representations. However,\na naive application of Gaussian splatting cannot generate\nhigh-quality animatable avatars and suffers from learning\ninstability; it also cannot capture fine avatar geometries\nand often leads to degenerate body parts. To tackle these\nproblems, we first propose a primitive-based 3D Gaussian\nrepresentation where Gaussians are defined inside pose-\ndriven primitives to facilitate animation. Second, to sta-\nbilize and amortize the learning of millions of Gaussians,\n*Equal contribution.\nwe propose to use neural implicit fields to predict the\nGaussian attributes (e.g., colors). Finally, to capture fine\navatar geometries and extract detailed meshes, we propose\na novel SDF-based implicit mesh learning approach for 3D\nGaussians that regularizes the underlying geometries and\nextracts highly detailed textured meshes.\nOur proposed\nmethod, GAvatar, enables the large-scale generation of di-\nverse animatable avatars using only text prompts. GAvatar\nsignificantly surpasses existing methods in terms of both ap-\npearance and geometry quality, and achieves extremely fast\nrendering (100 fps) at 1K resolution.\n1. Introduction\nDigital avatars play an essential role in numerous applica-\ntions, from augmented and virtual reality to gaming, movie\nproduction, and synthetic data generation [8, 20, 21, 36, 45,\n52\u201354]. However, highly realistic and animatable avatars\nare extremely difficult to create at scale due to the complex-\n1\narXiv:2312.11461v1  [cs.CV]  18 Dec 2023\nity and diversity of character geometries and appearances.\nTraditional approaches rely on manual modeling and rig-\nging of digital avatars, which are labor-intensive and time-\nconsuming. Recent advances in text-to-image generative\nmodels trained on large-scale data show impressive results\nin generating highly diverse and realistic human images\nfrom text [6, 34, 35, 49]. In light of this, several methods are\nproposed to generate 3D avatars from textual descriptions\nby distilling the 2D prior of these generative models into\n3D avatar representations [9, 11, 18]. While their results are\npromising, the quality of the generated avatars is limited by\nthe 3D representations they use, which are typically based\non mesh or neural radiance field (NeRF) [28]. Mesh-based\nrepresentations allow efficient rendering through rasteriza-\ntion, but the expressiveness to capture diverse geometry and\nfine details is limited due to the underlying topology. NeRF-\nbased representations are expressive in modeling complex\n3D scenes, but they are computationally expensive due to\nthe large number of samples required by volume render-\ning to produce high-resolution images. As a result, existing\navatar generation methods often fail to both generate fine-\ngrained, out-of-shape geometric details, such as loose cloth-\ning, and efficiently render high-resolution avatars, which\nare critical for interactive and dynamic applications.\nWe aim to address these issues by adopting a new 3D\nrepresentation, 3D Gaussian Splatting [17], which repre-\nsents a scene using a set of 3D Gaussians with color, opac-\nity, scales, and rotations and produces rendering by differ-\nentiably splatting the Gaussians onto an image. Gaussian\nsplatting combines the advantages of both mesh and NeRF-\nbased representations and it is both efficient and flexible to\ncapture fine details. However, naive applications of Gaus-\nsian splatting to avatar generation fail for several reasons\ndue to the unconstrained nature of individual Gaussians.\nFirst, the Gaussian splatting representation is not animat-\nable, as the Gaussians are defined in the world coordinate\nand cannot be easily transformed with the avatar\u2019s pose in\na coherent manner. Second, a large number (millions) of\nGaussians are required to model a highly detailed avatar,\nand the immense optimization space of individual Gaussian\nattributes (e.g., color, opacity, scale, rotation) leads to unsta-\nble optimization, especially when using high-variance ob-\njectives such as SDS [30]. Third, the 3D Gaussians lack ex-\nplicit knowledge of surfaces, and cannot easily incorporate\nsurface normal supervision, which is crucial for extracting\nhighly detailed 3D meshes [4, 10]. Without geometry super-\nvision, missing or degenerate body parts can appear when\nusing weak 3D supervision (i.e., SDS), which we will show\nin the experiments.\nTo tackle these problems, we propose GAvatar, a novel\napproach that leverages Gaussian Splatting to generate re-\nalistic animatable avatars from textual descriptions. First,\nwe introduce a new primitive-based 3D Gaussian represen-\ntation that defines 3D Gaussians inside pose-driven primi-\ntives. This representation naturally supports animation and\nenables flexible modeling of fine avatar geometry and ap-\npearance by deforming both the Gaussians and the prim-\nitives.\nSecond, we propose to use implicit Gaussian at-\ntribute fields to predict the Gaussian attributes, which stabi-\nlizes and amortizes the learning of a large number of Gaus-\nsians, and allows us to generate high-quality avatars using\nhigh-variance optimization objectives such as SDS. Addi-\ntionally, after avatar optimization, since we can obtain the\nGaussian attributes directly and skip querying the attribute\nfields, our approach achieves extremely fast (100 fps) ren-\ndering of neural avatars at a resolution of 1024\u00d71024. This\nis significantly faster than existing NeRF-based avatar mod-\nels [3, 18] that query neural field for each novel camera view\nand avatar pose. Finally, we also propose a novel signed\ndistance function (SDF)-based implicit mesh learning ap-\nproach that connects SDF with Gaussian opacities. Impor-\ntantly, it enables GAvatar to regularize the underlying ge-\nometry of the Gaussian avatar and extract high-quality tex-\ntured meshes. Our contributions are summarized as follows:\n\u2022 We introduce a new primitive-based implicit Gaussian\nrepresentation for animatable avatars, enabling more sta-\nble and high-quality 3D avatar generation. It also allows\nextremely fast rendering (100 fps) at 1K resolution.\n\u2022 We propose a novel SDF-based method that effectively\nregularizes the underlying geometry of 3D Gaussians\nand also enables the extraction of high-quality textured\nmeshes from the learned Gaussians avatar.\n\u2022 Our approach generates 3D avatars with fine geometry\nand appearance details. We experimentally demonstrate\nthat GAvatar consistently outperforms existing methods\nin terms of avatar quality.\n2. Related Work\n3D Representations for 3D Content Generation. Vari-\nous 3D representations have been employed for 3D con-\ntent generation, each with its own set of strengths and lim-\nitations.\nTriangulated meshes are a common choice due\nto their simplicity and compatibility with existing graphics\npipelines [14]. However, their inflexible topology can pose\nchallenges in accurately representing intricate geometries.\nAlternatively, volumetric representations, such as voxel\ngrids [40], offer flexibility in modeling complex shapes.\nNevertheless, their computational and memory costs grow\ncubically with resolution, impeding the faithful reconstruc-\ntion of fine geometry details and smooth surfaces.\nRe-\ncently, NeRFs [28] have gained prominence for modeling\n3D shapes, especially in text-to-3D applications, thanks to\ntheir ability to capture arbitrary topologies with minimal\nmemory usage. Yet, their rendering cost increases signif-\nicantly at higher resolutions. Some approaches adopt hy-\n2\nbrid representations to harness the benefits of different tech-\nniques. The Mixture of Volumetric Primitives (MVP) rep-\nresentation [25], for instance, introduces volumetric prim-\nitives onto a template mesh, achieving rapid rendering by\nleveraging a convolutional network to compute volumetric\nprimitives. It generates images through ray-marching, ac-\ncumulating colors and opacities from the primitives. Gaus-\nsian Splatting [17] has emerged as a promising 3D repre-\nsentation for efficiently rendering high-resolution images.\nIt models objects using colored 3D Gaussians, which are\nrendered onto an image using splatting-based rasterization.\nHowever, a notable limitation is its difficulty in extract-\ning meshes from learned Gaussians, as it predominantly\ncaptures appearance details through 3D Gaussians without\nmodeling the underlying object surfaces.\nIn this work. we introduce a novel primitive-based 3D\nGaussian representation with implicit mesh learning. It en-\nables modeling dynamic and articulated objects like humans\nusing Gaussian Splatting while also facilitating textured\nmesh extraction.\nIn comparison to MVP, our Gaussian-\nbased representation is more flexible and expressive, since\neach primitive comprises a variable number of 3D Gaus-\nsians with varying non-uniform locations that can go be-\nyond the primitive boundaries. This allows it to capture\nfiner details compared to the cubic primitives used in MVP.\nMoreover, our representation employs splatting-based ras-\nterization, enabling efficient rendering of high-resolution\nimages compared to traditional ray-marching techniques.\nText-to-3D Generation. The field of text-to-3D genera-\ntion has recently been revolutionized [4, 23, 30, 33, 33,\n41, 44] with the availability of large text-to-image mod-\nels [6, 34, 35, 49].\nThe earlier methods optimize the\n3D objects by encouraging the 2D rendering to be con-\nsistent with the input text in the CLIP [31] embeddings\nspace [5, 13, 14, 37, 42, 46].\nWhile they demonstrated\nthe usefulness of text-to-image models for 3D content gen-\neration, the resulting 3D models often lacked realism and\nfine geometry details. The seminal work DreamFusion [30]\nreplaces the CLIP model with a text-to-image diffusion\nmodel and proposed Score Distillation Sampling (SDS)\nto optimize a NeRF-based representation of the 3D ob-\nject.\nSince then multiple variants of this method have\nbeen proposed. Magic3D [23] enhances runtime efficiency\nwith a two-staged framework and adopts a more efficient\nDMTet [7] representation. ProlificDreamer [44] addresses\nover-saturation/smoothing issues through a variational SDS\nobjective. MVDream [39] fine-tunes text-to-image mod-\nels to generate 3D-consistent multi-view images, enabling\nefficient 3D generations. Fantasia3D [4] disentangles ge-\nometry and appearance modeling, optimizing surface nor-\nmals separately using the SDS loss. More recently, Dream-\nGaussian [41] replaced the NeRF-based representation with\nGaussian Splatting to significantly reduce runtime. How-\never, this leads to 3D models with limited geometry and\nappearance quality, despite attempts to refine texture details\nthrough mesh-based fine-tuning. It is important to note that\nall these methods are limited to rigid objects only and can-\nnot be animated easily.\nText-to-3D Avatar Generation Building upon the success\nachieved in generating static 3D objects, numerous methods\nhave been proposed to model dynamic objects, particularly\nhuman or human-like avatars [3, 10\u201312, 14, 15, 18, 22, 48,\n51]. ClipMatrix [14] is one of the first methods that show-\ncased the creation of animatable avatars based on textual de-\nscriptions. It achieves this by optimizing a mesh-based rep-\nresentation using a CLIP-embedding loss. AvatarClip [9]\nfollows a similar pipeline but employs a NeRF-based rep-\nresentation [43]. DreamAvatar [3] and AvatarCraft [15] uti-\nlize SDS loss instead of CLIP, and learn the NeRF represen-\ntation in canonical space through the integration of human\nbody priors from SMPL [26]. DreamHumans [18] intro-\nduces a deformable and pose-conditioned NeRF model by\nincorporating the imGHUM [2] model. DreamWaltz [11]\nand AvatarVerse [48] leverage pose-conditioned Control-\nNets [49], showcasing improved avatar quality with con-\nditional SDS. However, a common limitation among these\nmethods is their reliance on NeRF to generate images,\nresulting in the computation of SDS loss based on low-\nresolution images. For instance, DreamHumans [18] gener-\nates 64\u00d764 images during optimization, leading to a com-\npromise in avatar quality. In contrast, our approach can effi-\nciently generate images with a resolution of 1024\u00d71024,\nresulting in higher-quality avatars, as demonstrated in\nour experiments.\nThere are several contemporary works\nthat demonstrate impressive avatar quality [10, 22, 47].\nTADA [22] shows that a mesh-based approach with adap-\ntive mesh subdivision can be used to generate high-quality\navatars. HumanNorm [10] finetunes text-to-image models\nto directly generate normal and depth maps from the in-\nput text. The adapted models are then utilized to optimize\nthe avatar\u2019s geometry through the SDS loss, with texture\noptimization achieved using a normal-conditioned Control-\nNet [49]. Similarly, AvatarBooth [47] fine-tunes region-\nspecific diffusion models, highlighting that employing dedi-\ncated models for distinct body regions enhances avatar qual-\nity. These improved optimization objectives are comple-\nmentary to our method since they are compatible with our\nGaussian-based 3D representation. Since our model can ef-\nficiently render high-resolution images and normals, we an-\nticipate synergies between our approach and [10, 47, 48] to\nyield further enhancements.\n3. Preliminaries\nPrimitive-based 3D Representation.\nPrimitive-based\nmethods represent a 3D scene by a set of primitives such\n3\nLocal-to-World\nRot & Scale \n23. 4 & 5\nPrimitive \nGeneration\n{2!, 3!, 4!}\n*!\n\"\nRest Pose #$\nLocal-to-World \nPos. Transform\n 23. 3\n)*!\n\" (,-)\nColor, Rotation, \nScale Fields\n\u210b!\nSDF /!\n!&'(\nLocal-to-World\nRot & Scale \n23. 4 & 5\n>!\n\"\n!!\", !#$#\n%\n%!('\"\n# )\n)\"\n#\n!,-).,\nSDF \u2192 Opacity\n23. 8\n<! = { \u0302*!\n\" ( ,-), >!\n\", !!\n\" , #!\n\" , $!\n\"}\nGaussian\nSplatting\n8#$#\nGaussian\nSplatting\nGaussian Attribute \nComputation in \nRest Pose \n<! = { \u0302*!\n\" ( ,-), >!\n\", !!\n\" , #!\n\" , $!\n\"}\nPrimitive \nGeneration\n{=\", ?\", /\"}\n%&. 1\n\u2133 = 894(-, :)\n;2!,;3!,;4!\nTarget \nPose -\n(a) Gaussian Attribute Computation in Rest Pose \n(b) Method Overview\nDMTet\nFigure 2. Overview of GAvatar. We first generate the primitives Vk=(Pk, Rk, Sk) in the rest pose \u02dc\u03b8. Each primitive consists of Nk\n3D Gaussians with their position pi\nk, rotation ri\nk and scaling si\nk defined in the primitive\u2019s local coordinate system. Next, we obtain the\ncanonical positions, \u02c6pi\nk(\u02dc\u03b8), of the Gaussians in the world coordinates by applying the global transforms of the primitives using Eq. 3. These\npositions are then used to query the color ci\nk, rotation ri\nk and scaling si\nk of each Gaussian from a neural attribute field H\u03d5. Each Gaussian\u2019s\nSDF value is queried from a neural SDF S\u03c8 and is converted into the opacity \u03c3i\nk through a kernel function K. The 3D Gaussians with the\npredicted attributes are then rasterized onto the camera view using Gaussian splatting to produce the RGB image I and alpha image I\u03b1. We\nuse DMTet [38] to differentiably extract the mesh from the Gaussian SDF values and generate its normal map and silhouette for geometry\nregularization. For animating the avatar using any target pose \u03b8, we generate the primitives using the target pose and use them to transform\nthe 3D Gaussians, before rasterizing the image. A method walkthrough is also provided in the supplementary video.\nas cubes [25, 32], points [1] or nerflets [50]. In this work,\nwe adopt the primitive formulation used in [25, 32]: a set\nof K cubic primitives {V1, . . . , VK} are attached to the sur-\nface of a SMPL-X [29] mesh M = LBS(\u03b8, \u03b2), where \u03b8\nand \u03b2 are the SMPL-X pose and shape parameters, and\nLBS is the linear blend skinning function. Each primitive\nVk = {Pk, Rk, Sk} is defined by its location Pk \u2208 R3,\nper-axis scale Sk \u2208 R3 and orientation Rk \u2208 SO(3). The\nprimitive parameters are generated by:\nPk(\u03b8) = \u02c6Pk(M) + \u03b4P\u03c9(\u03b8)k,\nRk(\u03b8) = \u03b4R\u03c9(\u03b8)k \u00b7 \u02c6Rk(M),\nSk(\u03b8) = \u02c6Sk(M) + \u03b4S\u03c9(\u03b8)k,\n(1)\nwhere we first compute a mesh-based primitive initial-\nization \u02c6Pk(M), \u02c6Rk(M), \u02c6Sk(M), and then apply pose-\ndependent correctives \u03b4P\u03c9(\u03b8), \u03b4R\u03c9(\u03b8), \u03b4S\u03c9(\u03b8), which are\nrepresented by neural networks with parameters \u03c9.\nThe\nmesh-based initialization is computed by placing the primi-\ntives on a 2D grid in the mesh\u2019s uv-texture space and gener-\nating the primitives at the 3D locations on the mesh surface\npoints corresponding to the uv-coordinates. The overall de-\nformation process is illustrated in Fig. 2 (green box) and\nmore details can be found in [32].\nScore Distillation Sampling. First proposed in DreamFu-\nsion [30], score distillation sampling (SDS) can be used to\noptimize the parameters \u03b7 of a 3D model g using a pre-\ntrained text-to-image diffusion model. Given a text prompt\ny and the noise prediction \u02c6\u03f5(It; y, t) of the diffusion model,\nSDS optimizes model parameters \u03b7 by minimizing the dif-\nference between the noise \u03f5 added to the rendered image\nI = g(\u03b7) and the predicted noise \u02c6\u03f5 by the diffusion model:\n\u2207\u03b7LSDS = Et,\u03f5\n\u0014\nw(t)(\u02c6\u03f5(It; y, t) \u2212 \u03f5)\u2202I\n\u2202\u03b7\n\u0015\n,\n(2)\nwhere g(\u03b7) denotes the differentiable rendering process of\nthe 3D model, t is the noise level, It is the noised image,\nand w(t) is a weighting function.\n4. Approach\nOur approach, GAvatar, generates a 3D Gaussian-based an-\nimatable avatar given a text prompt.\nOur key ideas are\ntwo-fold: (1) we introduce a new primitive-based implicit\n3D Gaussian representation (Sec. 4.1) that not only en-\nables avatar animation but also stabilizes and amortizes the\nlearning of a large number of Gaussians using the high-\nvariance SDS loss; (2) we represent the underlying geom-\netry of 3D Gaussians with an SDF that enables extracting\nhigh-quality textured meshes and regularizing the avatar\u2019s\ngeometry (Sec. 4.2). The training process of our approach\nis described in Sec. 4.3 and an overview of our method is\nprovided in Fig. 2.\n4.1. Primitive-based Implicit Gaussian Avatar\nRecently, Gaussian Splatting [17] has emerged as a power-\nful representation for 3D scene reconstruction and genera-\ntion thanks to its efficiency and flexibility. However, naive\napplication of Gaussian Splatting to human avatar gen-\neration poses animation and training stability challenges.\nSpecifically, two essential questions arise: (1) how to trans-\nform the Gaussians defined in the world coordinate system\nalong with the deformable avatar and (2) how to learn Gaus-\nsians with consistent attributes (i.e., color, rotation, scaling,\n4\netc.) within a local neighborhood. In the following, we an-\nswer both questions by proposing a primitive-based implicit\nGaussian representation.\nPrimitive-based 3D Gaussian Avatar.\nTo generate an\nanimatable human avatar, we start with the primitive for-\nmulation discussed in Sec. 3, where the human body is\nrepresented by a set of primitives attached to its surface.\nSince the primitives are naturally deformed according to\nthe human pose and shape, we propose to attach a set\nof 3D Gaussians {G1\nk, . . . , GNk\nk } to the local coordinate\nsystem of each primitive Vk={Pk, Rk, Sk} and deform\nthem along with the primitive. Specifically, each Gaussian\nGi\nk={pi\nk, ri\nk, si\nk, ci\nk, \u03c3i\nk} is defined by its position pi\nk, rota-\ntion ri\nk, and scaling si\nk in the primitive\u2019s local coordinates,\nas well as its color features ci\nk and opacity \u03c3i\nk. Given a tar-\nget pose \u03b8, we first obtain the location Pk, scale Sk, and ori-\nentation Rk of each deformed primitive using Eq. 1. Then\nthe global location \u02c6pi\nk, scale \u02c6si\nk, and orientation \u02c6ri\nk of each\nGaussian Gi\nk associated with the primitive are computed as:\n\u02c6pi\nk(\u03b8) = Rk(\u03b8) \u00b7 (Sk(\u03b8) \u2299 pi\nk) + Pk(\u03b8)\n(3)\n\u02c6si\nk(\u03b8) = Sk(\u03b8) \u00b7 si\nk\n(4)\n\u02c6ri\nk(\u03b8) = Rk(\u03b8) \u00b7 ri\nk\n(5)\nThis primitive-based Gaussian representation naturally bal-\nances constraint and flexibility. It is more flexible compared\nto the native primitive representation in [25, 32] since it\nallows a primitive to deform beyond a cube by equipping\nit with Gaussians. Meanwhile, the Gaussians within each\nprimitive share the motion of the primitive and are more\nconstrained during animation.\nImplicit Gaussian Attribute Field. To fully exploit the\nexpressiveness of 3D Gaussians, we allow each Gaussian to\nhave individual attributes, i.e., color features, scaling, rota-\ntion, and opacity. However, this potentially results in unsta-\nble training where Gaussians within a local neighborhood\npossess different attributes, leading to noisy geometry and\nrendering. This is especially true when the gradient of the\noptimization objective has high variance, such as the SDS\nobjective in Eq. 2. To stabilize and amortize the training\nprocess, instead of directly optimizing the attributes of the\nGaussians, we propose to predict these attributes using neu-\nral implicit fields. As shown in the yellow block in Fig. 2,\nfor each Gaussian Gi\nk, we first compute its canonical posi-\ntion \u02c6pi\nk(\u02dc\u03b8) in the world coordinate system (Eq. 3), where \u02dc\u03b8\nrepresents the rest pose. We can then query the color ci\nk, ro-\ntation ri\nk, scaling si\nk and opacity \u03c3i\nk of each Gaussian using\nthe canonical position \u02c6pi\nk(\u02dc\u03b8) from two neural implicit fields\nH\u03d5 and O\u03c8, which are represented by neural networks with\nparameters \u03d5 and \u03c8:\n(ci\nk, ri\nk, si\nk) = H\u03d5(\u02c6pi\nk(\u02dc\u03b8))\n(6)\n\u03c3i\nk = O\u03c8(\u02c6pi\nk(\u02dc\u03b8))\n(7)\nwhere we use a separate neural field O\u03c8 to output the\nopacities of the Gaussians, while other attributes are pre-\ndicted by H\u03d5.\nThis design is because the opacities of\nthe Gaussians are closely related to the underlying geom-\netry of the avatar and require special treatment, which will\nbe discussed in Sec. 4.2. Note that by querying the neu-\nral field with a canonical rest pose \u02dc\u03b8, we canonicalize the\nGaussian attributes, which can then be shared across dif-\nferent poses and animations.\nOur use of neural implicit\nfields constrains nearby Gaussians to have consistent at-\ntributes, which greatly stabilizes and amortizes the train-\ning process and enables high-quality avatar synthesis using\nhigh-variance losses.\nRendering and Objectives. After obtaining the positions\nand attributes of 3D Gaussians, we adopt the efficient Gaus-\nsian splatting technique described in [17] to render an RGB\nimage I and also an alpha image I\u03b1. The RGB image I\nis then used for the SDS loss defined in Eq. 2 as one of\nthe main training objectives. To prevent the Gaussians from\nstraying far away from the primitives, we also utilize a local\nposition regularization loss Lpos= P\nk,i \u2225pi\nk\u22252, which con-\nstrains the Gaussians to be close to the origin of the associ-\nated primitives.\n4.2. SDF-based Mesh Learning for 3D Gaussians\nA crucial aspect yet to be addressed in our primitive-based\n3D Gaussian representation is how to properly represent\nthe underlying geometry of the 3D Gaussians. This is im-\nportant for two reasons: (1) 3D Gaussians are transparent\n\u201cpoint clouds\u201d that do not have well-defined surfaces, which\ncan lead to degenerate body parts or holes in the generated\navatars (see Fig. 5); (2) Currently, there is no efficient and\neffective way to extract textured meshes from a large num-\nber of 3D Gaussians, which are often important for applica-\ntions in traditional graphics pipelines.\nSDF-based Gaussian Opacity Field. To address this prob-\nlem, we propose to represent the underlying geometry of\n3D Gaussians through a signed distance field (SDF) func-\ntion S\u03c8 with parameters \u03c8. Specifically, we parametrize\nthe opacity \u03c3i\nk of each 3D Gaussian based on their signed\ndistance to the surface using a kernel function K inspired by\nNeuS [43]:\n\u03c3i\nk = K(S\u03c8(pi\nk)) ,\n(8)\nwhere K(x) = \u03b3e\u2212\u03bbx/(1 + e\u2212\u03bbx)2 is a bell-shaped ker-\nnel function with learnable parameters {\u03b3, \u03bb} that maps\nthe signed distance to an opacity value.\nIntuitively, this\n5\nopacity parametrization builds in the prior that Gaussians\nshould stay close to the surface in order to obtain high opac-\nity.\nThe parameter \u03bb controls the tightness of the high-\nopacity neighborhood of the surface and \u03b1 controls the\noverall scale of the opacity. The SDF-based Gaussian opac-\nity parametrization naturally fits our primitive-based im-\nplicit Gaussian representation, since now we can define the\naforementioned opacity field O\u03c8 as the product of the SDF\nand the kernel function: O\u03c8 = K \u25e6 S\u03c8, and we can directly\nuse a neural network to represent the SDF S\u03c8.\nMesh Extraction and Geometry Regularization. An im-\nportant advantage of using an SDF S\u03c8 to represent the un-\nderlying geometry of 3D Gaussians is that it allows us to ex-\ntract a mesh f\nM from the SDF through differentiable march-\ning tetrahedra (DMTet [38]):\nf\nM = DMTET(S\u03c8) .\n(9)\nBoth the SDF and extracted mesh allow us to utilize var-\nious losses to regularize the geometry of the 3D Gaussian\navatar. Specifically, we first employ an Eikonal regularizer\nto maintain a proper SDF, which is defined as:\nLeik = (\u2225\u2207pS\u03c8(p)\u2225 \u2212 1)2 ,\n(10)\nwhere p \u2208 P contains both the center points of all Gaus-\nsians in the world coordinates as well as points sampled\naround the Gaussians using a normal distribution. Next, we\nalso employ an alpha loss to match the mask IM rendered\nusing the extracted mesh to the alpha image I\u03b1 from the\nGaussian splatting:\nLalpha = \u2225IM \u2212 I\u03b1\u22252 .\n(11)\nInspired by Fantasia3D [4], we also use a normal SDS loss\nto supervise the normal rendering IN of the extracted mesh\nusing differentiable rasterization [19]. The SDS gradient\ncan be computed as:\n\u2207\u03b8LN\nSDS = Et,\u03f5\n\u0014\nw(t)(\u02c6\u03f5(IN,t; y, t) \u2212 \u03f5)\u2202IN\n\u2202\u03b8\n\u0015\n,\n(12)\nwhere IN,t is the noised normal image. We further use a\nnormal consistency loss Lnc which regularizes the differ-\nence between the adjacent vertex normals of mesh f\nM.\nTexture Extraction. Our proposed implicit Gaussian at-\ntribute field H\u03d5 naturally facilitates texturing the extracted\nmesh f\nM, since we can use the Gaussian color field as\nthe 3D texture field used by the differentiable rasterization.\nOnce the Gaussian-based avatar is fully optimized, directly\nusing the Gaussian color field already provides a good ini-\ntial texture for the mesh, but we can further improve the tex-\nture quality by finetuning the color field using an SDS loss\nL f\nM\nsds on the RGB rendering I f\nM of the textured mesh. We\nobserve that only a small number of finetuning iterations is\nrequired for convergence.\n4.3. Optimization\nThe overall objective of our method can be summarized as:\nL = LSDS + Lpos + Leik + Lalpha + LN\nSDS + Lnc ,\n(13)\nwhere we omit the weighting terms for brevity. Using this\nobjective, we optimize the Gaussian local positions {pi\nk},\nGaussian attribute field H\u03d5 and SDF S\u03c8, opacity kernel\nparameters {\u03b3, \u03bb}, primitive motion corrective networks\n\u03b4P\u03c9, \u03b4R\u03c9, \u03b4S\u03c9, as well as the SMPL-X shape parameters \u03b2.\nInitialization. We divide the uv-map of SMPL-X into a\n64 \u00d7 64 grid, which gives us 4096 primitives. We assign\n64 Gaussians to each primitive Vk and initialize their local\npositions {pi\nk} with a uniform grid of 4 \u00d7 4 \u00d7 4.\nTraining. We perform Gaussian densification as described\nin [17] every 100 iterations, which leads to different num-\nbers of Gaussians per primitive. We stop densification when\nthe total number of Gaussians exceeds 2 million. To render\nthe RGB image I for the SDS loss Lsds, we take the target\npose \u03b8 from two sources: (1) a natural pose \u03b8N optimized\ntogether with the aforementioned variables; (2) a random\npose \u03b8A sampled from an animation database to ensure re-\nalistic animation.\n5. Experiments\nIn Fig. 3, we showcase example avatars generated by our\nmethod and their geometry and textured meshes. Notice the\nintricate geometry details captured by our method, thanks\nto our SDF-based implicit mesh learning for 3D Gaussians.\nDue to its primitive-based design, our approach readily sup-\nports avatar animation. We showcase various animations in\nFig. 1 and on the project website.\nRendering Efficiency. Since GAvatar no longer needs to\nquery the Gaussian attributes from the implicit fields af-\nter optimization, it achieves extremely fast rendering speed\ndue to the use of 3D Gaussians. Specifically, a generated\navatar with 2.5 million Gaussians can be rendered with\n1024\u00d71024 resolution at 100 fps, which is tremendously\nfaster than most NeRF-based approaches. Moreover, the\nGaussian rendering only takes about 3ms (300+ fps), so\nfurther speedup is possible by optimizing the speed of non-\nrendering operations such as LBS and primitive transforms.\n5.1. Qualitative Evaluation\nFig. 4 compares our method, GAvatar, with the state-of-\nthe-art approaches: DreamGaussian [41], AvatarCLIP [9],\nAvatarCraft [15] and Fantasia3D [4].\nFor completeness,\nwe also compare with contemporary works, DreamHu-\nmans [18] and TADA [22]. For DreamHumans [18] we use\nthe avatar renderings provided on the project page, while\nfor other methods we use the publicly available source\ncodes. Our method clearly produces higher-quality avatars\n6\nGaussian Rendering\nTextured Mesh\nGaussian Rendering\nTextured Mesh\nGaussian Rendering\nMesh Normal\nGaussian Rendering\nMesh Normal\nFigure 3. Generated avatars by our method and their mesh normals and texture meshes.\nGAvatar (Ours)\nDreamHuman\nFantasia3D\nTADA\nGAvatar (Ours)\nAvatarCraft\nAvatarClip\nGAvatar (Ours)\nDreamHuman\nFantasia3D\nTADA\nDreamGaussian\nFigure 4. Comparison with the state-of-the-art methods. From top to bottom, the prompts used in each row are \u201ca person dressed at the\nvenice carnival\u201d, \u201ca professional boxer\u201d and \u201ca bedouin dressed in white\u201d. Our method consistently produces the best quality avatars.\nboth in terms of geometry and appearance. DreamGaus-\nsian [41], AvatarCLIP [9], AvatarCraft [15] and Fanta-\nsia3D [4] fail catastrophically to model complex avatars.\nDreamHumans [18] creates low-resolution avatars since it\nis trained with a resolution of 64\u00d764 only. TADA [22] can\nrender high-resolution images due to a mesh-based render-\ning but can produce degenerate solutions with implausible\nshapes. It also provides smoother texture and less geometry\ndetails as compared to our method. GAvatar generates sig-\nnificantly better avatars as compared to all methods as we\nwill also show in our user study next.\n5.2. Quantitative Evaluation\nTo quantitatively evaluate the proposed method, we follow\nprevious works [9, 22, 41] and carry out an extensive A/B\nuser study. We adopt 24 prompts commonly used in the\n7\nCompared Method\nGeometry Quality\nAppearance Quality\nConsistency with Prompt\nAvatarCLIP [9]\n98.81\n97.62\n97.62\nAvatarCraft [15]\n96.43\n98.81\n98.81\nDreamGaussian [41]\n100.0\n98.81\n98.81\nFantasia3D [4]\n92.86\n92.86\n91.67\nDreamHuman [18]*\n73.81\n73.81\n65.48\nTADA [22]*\n61.90\n69.05\n67.86\nTable 1. User Study. We show a preference percentage of our\nmethod over state-of-the-art methods (* denotes contemporary\nmethods). GAvatar is preferred by the users over all baselines.\nw/o Implicit Gaussian Attribute Fields\nw/ Implicit Gaussian Attribute Fields\nw/o SDF-based Mesh Learning\nw/ SDF-based Mesh Learning\nFigure 5. Ablation Studies.\nGAvatar Rendering\nGAvatar Mesh Extraction\nw/ DreamGaussian Mesh Extraction\nFigure 6. Mesh Extraction Comparison.\nbaselines to generate the avatars. In total, we collected 1512\nresponses from 42 participants. For each vote, we show\na pair of randomly chosen 3D avatars synthesized by our\nmethod and one of the baseline methods. We ask the partic-\nipant to choose the method that has better 1) geometry qual-\nity, 2) appearance quality, and 3) consistency with the given\nprompt. Table 1 summarizes the preference percentage of\nour method over the baseline methods. Notably, our method\nconsistently outperforms existing and contemporary meth-\nods by a substantial margin.\n5.3. Ablation Study\nEffect of Implicit Gaussian Attribute Field. In Fig. 5\n(Top), we design a variant of our method by disabling the\nimplicit Gaussian attribute field and directly optimizing the\nGaussian attributes. We observe that the generated avatars\nare significantly worse than our method, with pronounced\nnoise and color oversaturation. This aligns with our intu-\nition that directly optimizing millions of Gaussians individ-\nually with high-variance loss like SDS is quite challenging.\nIn contrast, our implicit Gaussian attribute field allows a\nmuch more stable and robust optimization process.\nEffect of SDF-based Mesh Learning. In Fig. 5 (Bottom),\nwe design a variant of our approach by disabling the SDF-\nbased mesh learning and instead letting the Gaussian at-\ntribute field additionally output the Gaussian opacities. As\nshown in Fig. 5, the generated avatars without mesh learn-\ning can have missing body parts and distorted body shapes.\nOur SDF-based mesh learning tackles these issues by regu-\nlarizing the underlying geometry of the Gaussian avatar.\nMesh Extraction Comparison. An important benefit of\nour approach is that it allows us to extract a high-quality dif-\nferentiable mesh representation of the Gaussian avatar. We\ncompare our mesh extraction approach with the Gaussian\ndensity-based approach used in DreamGaussian [41], one\nof the few works that extract meshes from 3D Gaussians. In\nparticular, we provide its mesh extraction pipeline with our\noptimized Gaussian attributes to obtain the final mesh. The\nresults are shown in Fig. 6. We observe the mesh extracted\nby DreamGaussian is more noisy and lacks geometry de-\ntails, while our approach obtains much smoother meshes\nwith fine-grained geometry details.\n6. Discussion and Limitations\nWe have presented a novel approach for generating diverse\nand animatable avatars with geometry learning and regu-\nlarization. Our primitive-based 3D Gaussian representation\nallows us to flexibly model avatar geometry and appear-\nance while enabling animation with extremely fast render-\ning. We demonstrated our neural implicit Gaussian attribute\nfields stabilize the learning of millions of 3D Gaussian un-\nder noisy objectives. We further propose a novel SDF-based\nmesh learning approach that regularizes the underlying ge-\nometry of the Gaussian avatar and extracts a high-quality\ntextured mesh from 3D Gaussians. Our experiments and\nuser study indicate that our approach surpasses state-of-the-\nart methods in terms of appearance and geometry quality.\nWhile our approach has shown promising results, it\nstill has several limitations to be addressed in future work.\nFirst, similar to other SDS-based approaches, our method\nsometimes also suffers from color oversaturation.\nWe\nbelieve that exploring various techniques for improving\nSDS [16, 24, 44] can help mitigate this issue. Second, there\ncan still be misalignment between the geometry and appear-\nance of the generated avatars, where some geometry details\nin the rendering are embedded in the colors of the 3D Gaus-\nsians, similar to how texture can embed geometry details in\nmesh-based rendering. We believe that having consistent\ngeometry and appearance supervisions such as those in Hu-\nmanNorm [10] can help alleviate this issue. Disentangling\n8\nlighting and appearance details within the 3D Gaussian-\nbased representation is also an interesting future direction.\nLastly, animating loose clothing with correct temporal\ndeformations is still challenging, especially when no direct\nimage or temporal supervision is provided.\nLeveraging\ntemporal priors such as physics simulation or video diffu-\nsion models can be a promising future avenue to explore.\nReferences\n[1] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry\nUlyanov, and Victor Lempitsky. Neural point-based graph-\nics. In ECCV, 2020. 4\n[2] Thiemo Alldieck, Hongyi Xu, and Cristian Sminchisescu.\nimghum: Implicit generative models of 3d human shape and\narticulated pose. In International Conference on Computer\nVision (ICCV), pages 5461\u20135470, 2021. 3\n[3] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-\nYee K Wong.\nDreamAvatar: Text-and-Shape Guided 3D\nHuman Avatar Generation via Diffusion Models.\narXiv\npreprint:2304.00916, 2023. 2, 3\n[4] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3D: Disentangling Geometry and Appearance for High-\nquality Text-to-3D Content Creation. In International Con-\nference on Computer Vision (ICCV), 2023. 2, 3, 6, 7, 8, 12,\n17\n[5] Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, and Kui\nJia. TANGO: Text-driven Photorealistic and Robust 3D Styl-\nization via Lighting Decomposition. In Conference on Neu-\nral Information Processing Systems (NeurIPS), 2022. 3\n[6] dalle2. https://openai.com/dall-e-2, 2022. 2, 3\n[7] Jun Gao, Wenzheng Chen, Tommy Xiang, Alec Jacobson,\nMorgan McGuire, and Sanja Fidler. Learning deformable\ntetrahedral meshes for 3d reconstruction.\nIn Conference\non Neural Information Processing Systems (NeurIPS), pages\n9936\u20139947, 2020. 3\n[8] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Ger-\nard Pons-Moll, and Christian Theobalt. Deepcap: Monoc-\nular human performance capture using weak supervision. In\nComputer Vision and Pattern Recognition (CVPR). IEEE,\n2020. 1\n[9] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. AvatarCLIP: Zero-Shot Text-\nDriven Generation and Animation of 3D Avatars. Transac-\ntions on Graphics (TOG), 41(4):1\u201319, 2022. 2, 3, 6, 7, 8, 12,\n15\n[10] Xin Huang, Ruizhi Shao, Qi Zhang, Hongwen Zhang, and\nYing Feng. Humannorm: Learning normal diffusion model\nfor high-quality and realistic 3d human generation, 2023. 2,\n3, 8\n[11] Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xi-\nanbiao Qi, Yukai Shi, Zheng-Jun Zha, and Lei Zhang.\nDreamwaltz: Make a scene with complex 3d animatable\navatars.\nIn Conference on Neural Information Processing\nSystems (NeurIPS), 2023. 2, 3, 12, 13\n[12] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Ji-\naxiang Tang, Deng Cai, and Justus Thies. TeCH: Text-guided\nReconstruction of Lifelike Clothed Humans. In International\nConference on 3D Vision (3DV), 2024. 3\n[13] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object gen-\neration with dream fields. In Computer Vision and Pattern\nRecognition (CVPR), 2022. 3\n[14] Nikolay Jetchev. Clipmatrix: Text-controlled creation of 3d\ntextured meshes. arXiv preprint arXiv:2307.05663, 2023. 2,\n3\n[15] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai,\nMingming He, Dongdong Chen, and Jing Liao. Avatarcraft:\nTransforming text into neural human avatars with parameter-\nized shape and pose control. In International Conference on\nComputer Vision (ICCV), 2023. 3, 6, 7, 8, 12, 14\n[16] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani\nLischinski.\nNoise-free score distillation.\narXiv preprint\narXiv:2310.17590, 2023. 8\n[17] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics,\n2023. 2, 3, 4, 5, 6, 12\n[18] Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Ed-\nuard Gabriel Bazavan, Mihai Fieraru, and Cristian Sminchis-\nescu. Dreamhuman: Animatable 3d avatars from text. arXiv\npreprint:2306.09329, 2023. 2, 3, 6, 7, 8, 12, 19\n[19] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol,\nJaakko Lehtinen, and Timo Aila.\nModular primitives for\nhigh-performance differentiable rendering. Transactions on\nGraphics (TOG), 39(6), 2020. 6\n[20] Ruilong Li, Kyle Olszewski, Yuliang Xiu, Shunsuke Saito,\nZeng Huang, and Hao Li. Volumetric human teleportation.\nIn ACM SIGGRAPH 2020 Real-Time Live, 2020. 1\n[21] Ruilong Li, Yuliang Xiu, Shunsuke Saito, Zeng Huang, Kyle\nOlszewski, and Hao Li. Monocular real-time volumetric per-\nformance capture. In European Conference on Computer Vi-\nsion (ECCV), pages 49\u201367. Springer, 2020. 1\n[22] Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxiang Tang,\nYangyi Huang, Justus Thies, and Michael J Black. Tada! text\nto animatable digital avatars. In International Conference on\n3D Vision (3DV), 2024. 3, 6, 7, 8, 12, 18\n[23] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3D: High-Resolution\nText-to-3D Content Creation. In Computer Vision and Pat-\ntern Recognition (CVPR), 2023. 3\n[24] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang.\nCommon diffusion noise schedules and sample steps are\nflawed. arXiv preprint arXiv:2305.08891, 2023. 8\n[25] Stephen Lombardi,\nTomas Simon,\nGabriel Schwartz,\nMichael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mix-\nture of volumetric primitives for efficient neural rendering.\nACM Trans. Graph., 2021. 3, 4, 5\n[26] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J. Black. SMPL: A skinned multi-\nperson linear model. ACM Transactions on Graphics, (Proc.\nSIGGRAPH Asia), 34(6):248:1\u2013248:16, 2015. 3\n9\n[27] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-\nard Pons-Moll, and Michael J. Black. AMASS: Archive of\nmotion capture as surface shapes. In International Confer-\nence on Computer Vision (ICCV), pages 5442\u20135451, 2019.\n12\n[28] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view\nsynthesis.\nIn European Conference on Computer Vision\n(ECCV), 2020. 2\n[29] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and\nMichael J. Black.\nExpressive Body Capture: 3D Hands,\nFace, and Body from a Single Image. In Computer Vision\nand Pattern Recognition (CVPR), 2019. 4\n[30] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. DreamFusion: Text-to-3d using 2d diffusion. In In-\nternational Conference on Learning Representations (ICLR),\n2023. 2, 3, 4\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In International\nConference on Machine Learning (ICML), 2021. 3\n[32] Edoardo Remelli, Timur Bagautdinov, Shunsuke Saito,\nChenglei Wu, Tomas Simon, Shih-En Wei, Kaiwen Guo, Zhe\nCao, Fabian Prada, Jason Saragih, et al. Drivable volumet-\nric avatars using texel-aligned features. In ACM SIGGRAPH\n2022 Conference Proceedings, 2022. 4, 5\n[33] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,\nand Daniel Cohen-Or. Texture: Text-guided texturing of 3d\nshapes. arXiv preprint:2302.01721, 2023. 3\n[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In Computer Vision and\nPattern Recognition (CVPR), pages 10684\u201310695, 2022. 2,\n3\n[35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nIn Conference on Neural Infor-\nmation Processing Systems (NeurIPS), pages 36479\u201336494,\n2022. 2, 3\n[36] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul\nJoo. Pifuhd: Multi-level pixel-aligned implicit function for\nhigh-resolution 3d human digitization. In CVPR, 2020. 1\n[37] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang,\nChin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-\nshan. Clip-forge: Towards zero-shot text-to-shape genera-\ntion. In Computer Vision and Pattern Recognition (CVPR),\npages 18603\u201318613, 2022. 3\n[38] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler. Deep marching tetrahedra: a hybrid represen-\ntation for high-resolution 3d shape synthesis. In Conference\non Neural Information Processing Systems (NeurIPS), pages\n6087\u20136101, 2021. 4, 6\n[39] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv:2308.16512, 2023. 3\n[40] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias\nNie\u00dfner, Gordon Wetzstein, and Michael Zollh\u00a8ofer. Deep-\nvoxels: Learning persistent 3d feature embeddings. In Com-\nputer Vision and Pattern Recognition (CVPR), 2019. 2\n[41] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 3, 6, 7, 8, 12, 16\n[42] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,\nand Jing Liao. Clip-nerf: Text-and-image driven manipula-\ntion of neural radiance fields. In Computer Vision and Pat-\ntern Recognition (CVPR), pages 3835\u20133844, 2022. 3\n[43] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural im-\nplicit surfaces by volume rendering for multi-view recon-\nstruction. Advances in Neural Information Processing Sys-\ntems, 34:27171\u201327183, 2021. 3, 5\n[44] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongx-\nuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity\nand diverse text-to-3d generation with variational score dis-\ntillation. In Conference on Neural Information Processing\nSystems (NeurIPS), 2023. 3, 8\n[45] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and\nMichael J. Black. ECON: Explicit Clothed humans Opti-\nmized via Normal integration. In Computer Vision and Pat-\ntern Recognition (CVPR), 2023. 1\n[46] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying\nShan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot\ntext-to-3d synthesis using 3d shape prior and text-to-image\ndiffusion models. In Computer Vision and Pattern Recogni-\ntion (CVPR), 2023. 3\n[47] Yifei Zeng, Yuanxun Lu, Xinya Ji, Yao Yao, Hao Zhu, and\nXun Cao. Avatarbooth: High-quality and customizable 3d\nhuman avatar generation, 2023. 3\n[48] Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu\nWang, Li Chen, Chao Long, Feida Zhu, Kang Du, and Min\nZheng. Avatarverse: High-quality & stable 3d avatar creation\nfrom text and pose, 2023. 3\n[49] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n2, 3\n[50] Xiaoshuai Zhang, Abhijit Kundu, Thomas Funkhouser,\nLeonidas Guibas, Hao Su, and Kyle Genova. Nerflets: Local\nradiance fields for efficient structure-aware 3d scene repre-\nsentation from 2d supervision. CVPR, 2023. 4\n[51] Xuanmeng Zhang, Jianfeng Zhang, Chacko Rohan, Hongyi\nXu, Guoxian Song, Yi Yang, and Jiashi Feng. Getavatar:\nGenerative textured meshes for animatable human avatars.\nIn ICCV, 2023. 3\n[52] Yang Zheng, Ruizhi Shao, Yuxiang Zhang, Tao Yu, Zerong\nZheng, Qionghai Dai, and Yebin Liu. Deepmulticap: Perfor-\nmance capture of multiple characters using sparse multiview\ncameras. In International Conference on Computer Vision\n(ICCV), 2021. 1\n10\n[53] Zerong Zheng, Xiaochen Zhao, Hongwen Zhang, Boning\nLiu, and Yebin Liu. Avatarrex: Real-time expressive full-\nbody avatars. ACM Transactions on Graphics (TOG), 42(4),\n2023.\n[54] Luyang Zhu, Konstantinos Rematas, Brian Curless, Steve\nSeitz, and Ira Kemelmacher-Shlizerman.\nReconstructing\nnba players. In European Conference on Computer Vision\n(ECCV), 2020. 1\n11\nA. Implementation Details\nCamera sampling.\nDuring optimization, we randomly\nsample camera poses to render full-body avatars from dif-\nferent views as well as zoom-in images of various body\nparts.\nSpecifically, we randomly sample camera poses\nfrom a spherical coordinate system with radius 3.5, ele-\nvation range [\u221210\u25e6, 45\u25e6], and y-axis field of view range\n[\u221226\u25e6, 45\u25e6] for full-body renderings. To encourage detailed\nbody parts generation, we manipulate cameras to render\nzoom-in images for the face, back head, arms, upper body,\nand lower body. During training, we evenly sample differ-\nent body parts and the full body renderings.\nTraining.\nFor each prompt, we optimize the avatar for\n20000 iterations with the Adam optimizer. The learning\nrates for different learnable parameters discussed in Sec. 4.3\nof the main paper are listed in Table 2 below. We train\nthe avatar in natural pose \u03b8N for 3000 iterations before\nintroducing random pose \u03b8A sampled from the CMU mo-\ntion capture database1 using the SMPL-X parameters from\nAMASS [27]. Starting from the 5000th iteration, we manip-\nulate cameras to render zoom-in images for specific body\nparts (e.g., face, hands, upper body, etc.) to facilitate learn-\ning intricate detail in these parts. The total training takes\napproximately 3 hours for each avatar on an NVIDIA RTX\n3090Ti.\nParameter\nLearning rate\nGaussian local positions {pi\nk}\n0.00016\nGaussian attribute field H\u03d5\n0.001\nSDF S\u03c8\n0.0001\nopacity kernel parameters {\u03b3, \u03bb}\n0.001\nprimitive motion corrective networks \u03b4P\u03c9, \u03b4R\u03c9, \u03b4S\u03c9\n0.0001\nthe SMPL-X shape parameters \u03b2\n0.0003\nTable 2. Learning rates for different parameters.\nNetwork architecture.\nFor the implicit Gaussian at-\ntribute field discussed in Sec. 4.1 in the main paper, we\nadopt a hash-encoded feature grid with 8 levels, where the\nbase resolution is 16\u00d716\u00d716. The feature grid is followed\nby three MLP layers that output a 55-dim vector including\nthe scaling, rotation, and spherical harmonics features of\nthe 3D Gaussian. For the SDF discussed in Sec. 4.2 in the\nmain paper, we utilize a similar design as the Gaussian at-\ntribute field. Specifically, we use another hash-encoded fea-\nture grid with 16 levels and a base resolution of 16\u00d716\u00d716.\nThe feature grid is followed by three MLP layers that output\nthe SDF value of the 3D Gaussian, which is then converted\nto its opacity value using the opacity kernel \u03ba. During train-\ning, we initialize each primitive with 64 Gaussians lying on\na 4 \u00d7 4 \u00d7 4 grid within the primitive and use the densifica-\ntion process (see Sec. 4.3 in the main paper) to adaptively\nchange the total Gaussian number as discussed in [17]. We\n1htp://mocap.cs.cmu.edu/\nalso pretrain the Gaussian implicit fields to have an initial\nscale of 4mm in the world coordinate system.\nB. Additional Baseline Comparison\nWe\nprovide\nadditional\nqualitative\ncomparisons\nwith\nDreamWaltz [11],\nAvatarCraft [15],\nAvatarCLIP [9],\nDreamGaussian [41], Fantasia3D [4], TADA [22] and\nDreamHuman [18] in Fig. 7, 8, 9, 10, 11, 12 and 13, respec-\ntively. We note that DreamWaltz, DreamGaussian, TADA\nand DreamHuman are all concurrent text-to-3D avatar\nworks. To ensure the best performance of the baselines, we\nuse publicly available code and default hyper-parameters\nfor each baseline except for DreamHuman, whose code is\nnot available yet. Thus, we compare with the avatars down-\nloaded from the project website2. Overall, our method is not\nonly more robust to various prompts, but also shows more\nintricate and realistic details compared to all the baseline\nmethods.\nC. Additional Qualitative Results\nWe showcase more characters generated by GAvatar in\nFig. 14 and 15, demonstrating the robustness and general-\nization of the proposed method.\nD. User Study Prompts\nFor fair comparisons, we use the following 24 prompts com-\nmonly used by various baselines in the user study.\nA professional boxer.\nMorty Smith.\nA person in a diving suit.\nAn American soldier from World War 2.\nGoku.\nRick Sanchez.\nA person dressed at the Venice carnival.\nA medieval European king.\nAn elderly man wearing a beige suit.\nKobe Bryant.\nA man wearing a white tank top and shorts.\nA policewoman.\nA black female surgeon.\nA viking.\nOprah Winfrey.\nA bedouin dressed in white.\nA framer.\nA clown.\nJane Goodall.\nHomer Simpson.\nKristoff in Frozen.\nLuffy in one piece.\nSpiderman.\nJeff Bezos.\n2htps://dream-human.github.io/\n12\nDreamWaltz\nGavatar (Ours)\nDreamWaltz\nGavatar (Ours)\nA bedouin dressed in white.\nA professional boxer.\nA policewoman.\nA person dressed at the venice carnival.\nA clown.\nA framer.\nA viking.\nAn American soldier from world war 2\nA medieval European king.\nAn elderly man wearing a beige suit.\nA person in a diving suit.\nKobe Bryant.\nFigure 7. More comparisons with DreamWaltz [11].\n13\nAvatarCraft\nGavatar (Ours)\nAvatarCraft\nGavatar (Ours)\nGoku\nA professional boxer.\nA policewoman.\nA person dressed at the venice carnival.\nA clown.\nA framer.\nA viking.\nAn American soldier from world war 2\nA medieval European king.\nAn elderly man wearing a beige suit.\nA person in a diving suit.\nKobe Bryant.\nFigure 8. More comparisons with AvatarCraft [15].\n14\nAvatarCLIP\nGavatar (Ours)\nAvatarCLIP\nGavatar (Ours)\nA bedouin dressed in white.\nA professional boxer.\nA policewoman.\nA person dressed at the venice carnival.\nA clown.\nA framer.\nA viking.\nAn American soldier from world war 2\nA medieval European king.\nAn elderly man wearing a beige suit.\nA person in a diving suit.\nKobe Bryant.\nFigure 9. More comparisons with AvatarCLIP [9].\n15\nDreamGaussian\nGavatar (Ours)\nDreamGaussian\nGavatar (Ours)\nA bedouin dressed in white.\nA professional boxer.\nA policewoman.\nA person dressed at the venice carnival.\nA clown.\nA framer.\nA viking.\nAn American soldier from world war 2\nA medieval European king.\nAn elderly man wearing a beige suit.\nA person in a diving suit.\nKobe Bryant.\nFigure 10. More comparisons with DreamGaussian [41].\n16\nFantasia3D\nGavatar (Ours)\nFantasia3D\nGavatar (Ours)\nA bedouin dressed in white.\nA professional boxer.\nA policewoman.\nA person dressed at the venice carnival.\nA clown.\nA framer.\nA viking.\nAn American soldier from world war 2\nA medieval European king.\nAn elderly man wearing a beige suit.\nA person in a diving suit.\nKobe Bryant.\nFigure 11. More comparisons with Fantastia3D [4].\n17\nTADA\nGavatar (Ours)\nTADA\nGavatar (Ours)\nA bedouin dressed in white.\nA professional boxer.\nMorty Smith.\nA person dressed at the venice carnival.\nGoku.\nLuffy in one piece.\nA viking.\nHomer Simpson.\nA medieval European king.\nSpiderman.\nA person in a diving suit.\nKobe Bryant.\nFigure 12. More comparisons with TADA [22].\n18\nDreamHuman\nGavatar (Ours)\nDreamHuman\nGavatar (Ours)\nA clown.\nA viking.\nAn elderly man wearing a beige suit.\nA black female surgeon.\nAn American soldier from world war 2.\nA professional boxer.\nA policewoman.\nA person in a diving suit.\nA person dressed at the venice carnival.\nA medieval European king.\nA framer.\nA man wearing a white tank top and shorts.\nFigure 13. More comparisons with DreamHuman [18].\n19\nAstronaut\nKnight\nCaptain Jack Sparrow from Pirates of the Caribbean\nIron Man\nFirefighter\nGroot\nSun Wukong\nMozart\nMobile Suit Gundam\nLeonardo Dicaprio in a leather jacket and jeans\nKratos\nJeff Bezos in a business suit\nFigure 14. More results by GAvatar.\n20\nJeff Bezos in a space-themed t-shirt\nLady Gaga in a unique and bold outfit\nMeghan Markle in a sophisticated outfit\nKim Kardashian in athleisure wear\nDarth Vader\nJeff Goldblum in a quirky and stylish outfit\nElon Musk in a Tesla-branded jacket\nJay-Z in a hip-hop-inspired ensemble\nFitness trainer in workout gear\nGreta Thunberg in eco-friendly clothing\nFrodo Baggins\nGandalf\nFigure 15. More results by GAvatar.\n21\n"
  },
  {
    "title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model",
    "link": "https://arxiv.org/pdf/2312.11370.pdf",
    "upvote": "16",
    "text": "G-LLaVA\n: Solving Geometric Problem with Multi-Modal Large\nLanguage Model\nJiahui Gao1,2\u2217, Renjie Pi3\u2217, Jipeng Zhang3, Jiacheng Ye2, Wanjun Zhong1,\nYufei Wang1, Lanqing Hong1, Jianhua Han1, Hang Xu1,\nZhenguo Li1, Lingpeng Kong2\n1Huawei Noah\u2019s Ark Lab\n2The University of Hong Kong\n3The Hong Kong University of Science and Technology\nLarge language models (LLMs) have shown re-\nmarkable proficiency in human-level reasoning and\ngeneration capabilities, which encourages exten-\nsive research on their application in mathematical\nproblem solving. However, current work has been\nlargely focused on text-based mathematical prob-\nlems, with limited investigation in problems involv-\ning geometric information. Addressing this gap, we\naim to enable LLMs to solve geometric problems\nby understanding image input. We first analyze\nthe limitations of current Multimodal Large Lan-\nguage Models (MLLMs) in this area: they struggle\nto accurately comprehending basic geometric ele-\nments and their relationships. To overcome these\nchallenges, we take advantage of the unique char-\nacteristics of geometric problems (such as unique\ngeometric logical form, and geometric scalability)\nand the capacity of the textual LLMs to build an\nenriched multimodal geometry dataset based on ex-\nisting data. The augmented dataset, Geo170K, con-\ntains more than 170K geometric image-caption and\nquestion-answer pairs. Utilizing our constructed\nGeo170K dataset, we develop G-LLaVA, which\ndemonstrates exceptional performance in solving\ngeometric problems, significantly outperforming\nGPT-4-V on the MathVista benchmark with only\n7B parameters.\n1\nIntroduction\nLarge language models (LLMs) exhibit human-\nlike proficiency in reasoning (Wei et al., 2022;\nWang et al., 2022; Zhou et al., 2022) and gener-\nation (Ouyang et al., 2022; Touvron et al., 2023),\nwhich encourages extensive research on their appli-\ncation in mathematical problem solving (Fu et al.,\n2023; Gou et al., 2023; Yue et al., 2023b; Luo\net al., 2023; Zhao et al., 2023a,b; Jiang et al., 2023).\nThese problems often require highly sophisticated\nand symbolic reasoning capabilities, often consid-\nered impossible to solve before the era of LLMs.\n\u2217Equal Contribution.\nIt is an intuitive approach to use LLMs for\nmathematical reasoning problems presented in a\ntextual form. Nevertheless, a substantial propor-\ntion of mathematical reasoning problems neces-\nsitate the comprehension of geometric informa-\ntion. Moreover, even when certain problems do\nnot overtly pertain to geometric information on the\nsurface, the integration of geometrical-based meth-\nods often holds significant practical implications\n(e.g., analytic number theory). With the advent of\nGPT-4V (OpenAI, 2023), Gemini1 (Google, 2023),\nand numerous multi-modal large language models\n(MLLMs) (Zhu et al., 2023; Liu et al., 2023; Dai\net al., 2023; Li et al., 2023; Bai et al., 2023; Lai\net al., 2023; Gao et al., 2023b; Pi et al., 2023b),\nrecent work has progressively looking into employ-\ning MLLMs to tackle geometric reasoning prob-\nlems in mathematics (Yang et al., 2023; Lu et al.,\n2023; Yue et al., 2023a).\nHowever, we have observed that even with the\nmost advanced MLLMs, current systems still ex-\nhibit limitations in addressing geometric problems\ndue to challenges in accurately comprehending ge-\nometric figures. For instance, as demonstrated in\nFigure 1, GPT4-V often produces inaccurate de-\nscriptions for geometric figures. Specifically, the\nmodel struggles with understanding the relation-\nships between fundamental elements like points\nand lines, and in accurately interpreting elements\nsuch as the degree of an angle. We presume that the\nunderlying reason for this may be the fact that these\nMLLMs are typically trained with images and de-\nscriptions from the general domain, and the ability\nto understand such semantics differs significantly\nfrom that required for geometric reasoning.\nTo address this issue, one of the most direct and\neffective approaches is to enhance current MLLMs\nby augmenting them with data containing high-\n1Gemini, a concurrent work, was released one week be-\nfore our submission. Consequently, our work is primarily\nbenchmarked against GPT4-V and other MLLMs.\narXiv:2312.11370v1  [cs.CL]  18 Dec 2023\nvertical line segment BD dropping from\npoint D to point B on the diameter AC.\nThe triangle's hypotenuse is the radius of\nthe semicircle, and the right angle is at\npoint B.\nlabeled with the number \"0\" in\nthe top left corner. The overall\nappearance\nof\nthe\nimage\nsuggests\na\nmathematical\nor\ngeometric concept.\nlabeled with the numbers 5 and\n12. The triangle is drawn in black\nink on a white background.\nThe image shows a geometric figure consisting of a semicircle and a right-angled\ntriangle. The semicircle has its diameter on the horizontal line segment AC, with its\ncenter marked as point O. Point D is at the topmost part of the semicircle,\nindicating the midpoint of the arc. The right-angled triangle is formed by the\ndiameter AC of the semicircle and a vertical line segment BD dropping from point\nD to point B on the diameter AC. The triangle's hypotenuse is the radius of the\nsemicircle, and the right angle is at point B.\nThe image features a white circle with a line drawn through its center. The line is\nlabeled with the letter \"d\" and extends from the center of the circle to the edge. The\ncircle is divided into four equal parts, with each part labeled with the letters \"a,\"\n\"b,\" \"c,\" and \"d.\" The circle is also labeled with the number \"0\" in the top left\ncorner. The overall appearance of the image suggests a mathematical or geometric\nconcept.\nThe image shows a right triangle with a hypotenuse of length 10 and legs of length 5\nand 12. The triangle is labeled with the letters a, b, and c, which represent the\nvertices of the triangle. The hypotenuse is labeled with the number 10, and the legs\nare labeled with the numbers 5 and 12. The triangle is drawn in black ink on a white\nbackground.\nGPT4-V\nLLaVA1.5\nThe diagram shows a semicircle with a diameter AB. Point D is located on the \nsemicircle, and a line CD is drawn from point D to point C. Point O is the center \nof the semicircle.\nG-LLaVA\nMiniGPT-4\nDescribe the figure in detail.\nFigure 1: State-of-the-art MLLMs suffer severe hallucination on geometric figures, which greatly hinders their\nabilities on solving geometric problems. On the other hand, our G-LLaVA\u2019s ability to interpret geometric figure is\nboosted after the alignment phase with our curated dataset.\nquality descriptions of geometric information (Ye\net al., 2022a; Meng et al., 2022).\nHowever, a\nsignificant challenge arises from the limited size\nof the largest publicly available geometric prob-\nlem dataset, which contains only a few thousand\nquestion-answer pairs. Additionally, the current\ndatasets lack descriptions of geometric images and\nexhibit a limited range of problem-solving methods,\nwhich constrains the model\u2019s ability to understand\nbasic geometric elements and affect its problem-\nsolving capabilities.\nIn this paper, we propose to synthesize geomet-\nric visual-text data leveraging existing datasets via\ntext-only LLMs (e.g., ChatGPT). More specifically,\nwe utilize the geometry characteristic to construct\na multi-modal geometry dataset, building upon ex-\nisting datasets. The data generation process in-\nvolves incorporating utilizing uniqueness of geo-\nmetric logic form, geomertic representation unique-\nness, geometric scalability, etc (as shown in Fig-\nure 2). We term our generated dataset Geo170K,\nwhich contains around 60,000 geometric image-\ncaption pairs and more than 110,000 question-\nanswer pairs. This dataset is 28 times larger than\nGeoQA+, greatly expanding the coverage of ge-\nometric problems. With our collected Geo170K,\nwe derive G-LLaVA, a MLLM capable of solving\ngeometric problems, surpassing SOTA MLLMs by\na large margin. Specifically, G-LLaVA-13B out-\nperforms LLaVA-13B by 27.4 on GPS minitest\nsplit of MathVista (Lu et al., 2023). In addition,\nwith only G-LLaVA-7B, it is able to surpass the\npowerful GPT4-V on the geometry problem solv-\ning questions. Code and data will be available at\nhttps://github.com/pipilurj/G-LLaVA.\n2\nRelated Work\nMulti-Modal Large Language Model.\nRecent\nyears have witnessed transformative advancements\nin the development of large language models\n(LLMs), characterized by a series of pioneering\nstudies (Brown et al., 2020; Scao et al., 2022;\nChowdhery et al., 2022; Smith et al., 2022; Hoff-\nmann et al., 2022; Ouyang et al., 2022; Touvron\net al., 2023; Bai et al., 2022). These breakthroughs\nhave significantly elevated the capabilities of lan-\nguage understanding and generation, showcasing\nnear-human proficiency across diverse tasks. Con-\ncurrently, the success of LLMs has inspired explo-\nrations into vision-language interaction, leading\nto the emergence of multi-modal large language\nmodels (MLLMs) (Liu et al., 2023; Li et al., 2023;\nDai et al., 2023; Zhu et al., 2023; Dai et al., 2023;\nOpenAI, 2023; Bai et al., 2023; Su et al., 2023;\nGao et al., 2023b). These models have exhibited\nremarkable capabilities in synthesizing detailed de-\nscriptions and engaging in dialogue based on visual\ninputs. However, we observe that even the state-of-\nthe-art MLLMs face challenges in resolving geo-\nmetric problems using diagrams and figures.\nGeometry Characteristic\nLogic Form\nSimilar(Triangle(A,B,C),Triangle(C,B,D))\n...\nInfo Items\nImage Description\nQ: Point B is in line AC, line AB is \nx (x=1), line AC is y (y=3), line BC \nis ?\nA: Denote BC as z, we can build the \nequation x+z=y, therefore z=y-x, \nwhich simplifies to z=3-1=2. Thus, \nBC is 2.\nQ: Point B is in line AC, line AB \nis u, line AC is 3, line BC is 2.  \nWhat is the value of u? \nA: Denote AB as u, we can \ndetermine u by solving u+2=3. \nTherefore, u equals 1.\nReasoning Diversity\nGeometric Scalability\nConditional Uniqueness\nQ: Given that Point B lies on line segment \nAC, with AB=1 unit and AC=3 units, what \nis the length of segment BC? \nA: BC can be determined by subtracting the \nlength of segment AB from the length of \nsegment AC. Since AB is 1 unit and AC is 3 \nunits, BC = AC - AB, which equals 2 units. \n1. Triangle ABC is similar \nto triangle CBD.\n\u2026\nQ: \u2299 O is the circumscribed circle \nof \u25b3ABC, \u2026 \u2220ABO=30.0, then \nthe size of \u2220ACB is ?\nA: In triangle AOB, OA=OB, \n\u2220ABO=30\u00b0; Therefore, \n\u2220AOB=180\u00b0- 2\u2220ABO=120\u00b0; \nTherefore, \u2220ACB=1/2 \u2220AOB=60\u00b0\nQA Example1\nThe diagram consists of a triangle ABC \ninscribed within a circle O. Points A, B, and \nC are the vertices of the triangle, and they \nlie on the circumference of the circle. The \ncenter of the circle is marked as point O.\nContrastive  QA pairs \nGeometric Description Generation via Information Recovery\nVariable Substitution and Equation Solving\nValue Scaling\nRe-Formulating Condition as Unknown\nSentence Paraphrase\nGenerate Contrastive QA Pairs for Basic Elements\nLogic Structure Uniqueness\nDiagram Representation Uniqueness\n: Generate data using text-only LLM\n: Multimodal Geometric Alignment Data\n: Multimodal Geometric Instruction Data\nQ: Is point D the lies on line BC?\nA: No, \u2026 , point D lies on the line \nsegment AB.\nQ: Point B is in line AC, line AB is \n1, line AC is 3, line BC is ? \nA: Line BC is 2.\nQA Example2\nQA Example2\nQA Example2\nQA Example2\nQ: Point B is in line AC, line \nAB is 2, line AC is 6, line BC is ? \nA: The length of BC is 4 units, \nobtained by subtracting AB (2 \nunits) from AC (6 units).\nGeometric Solution Generalizability\nFigure 2: Framework of our multi-modal geometric data generation using the characteristics of geometry problems.\nGeometry Problem Solving.\nThe Geometry\nproblem reasoning is an challenging visual math-\nematical reasoning problem. Early efforts by Seo\net al. (2015); Sachan et al. (2017); Alvin et al.\n(2017); Sachan and Xing (2017) focused on creat-\ning datasets through manual efforts. More recent\napproaches have introduced enhanced methods and\ndatasets, including Geometry3K (Lu et al., 2021),\nGeoQA (Chen et al., 2021), GeoQA+ (Cao and\nXiao, 2022), UniGeo (Chen et al., 2022), UniMath\n(Liang et al., 2023), and SCA-GPS (Ning et al.,\n2023), aiming to improve both performance and ex-\nplainability. However, the scale of current datasets\nremains limited, and the performance of traditional\nmodels in this domain has not achieved the level\nobserved in other areas of mathematical problem\nsolving, particularly when compared to methods\nthat utilize large language models for solving math\nword problems (Cobbe et al., 2021; Wei et al., 2022;\nGou et al., 2023).\nData Generation via LLM.\nBootstrapping data\nfrom pretrained models has long been an active\narea of research. (Ye et al., 2022a; Meng et al.,\n2022) generates training data using pretrained lan-\nguage models such as GPT-2 for classification tasks.\n(Gao et al., 2023a) improves the quality of gener-\nated dataset via bi-level approach. (Ye et al., 2022b)\nutilizes influence function to select in-context ex-\namples to aid data generation. Recently, automatic\ndata generation becomes more ubiquitous with the\nadvent of powerful LLMs such as ChatGPT, a line\nof recent works utilize ChatGPT-generated data\nto perform instruction tuning (Wang et al., 2023;\nPeng et al., 2023; Taori et al., 2023; Liu et al., 2023;\nZhu et al., 2023; Bai et al., 2023; Pi et al., 2023a;\nSu et al., 2023; Yu et al., 2023; Chen et al., 2023;\nZhang et al., 2023).\n3\nObservation\nWe observe that most state-of-the-art (SOTA)\nMLLMs, although being adept at understanding\ndaily visual scenes, have difficulty in comprehend-\ning geometric figures, even if they are simple and\nstraightforward for humans. In Figure 1, we demon-\nstrate the descriptions generated by SOTA MLLMs\nfor geometric figure. We observe that severe hallu-\nGeometric Description Generation via Information Recovery\nQA Pair:\nQuestion: As shown in the figure, circle O is the circumscribed circle of triangle ABC, and it is\nknown that angle ABO = 30.0, then the size of angle ACB is ()\nAnswer: In triangle AOB, OA=OB, angle ABO=30\u00b0; Therefore, angle AOB=180\u00b0- 2 angle ABO\n=120\u00b0; Therefore, angle ACB=1/2angle AOB=60\u00b0\nDiagram Description:\nThe diagram consists of a triangle ABC inscribed within a circle, where the circle is denoted as circle O. Points A,\nB, and C are the vertices of the triangle, and they all lie on the circumference of the circle. The center of the circle is\nmarked as point O.\nTable 1: Full geometric diagram description generation via inverse information recovery. The description is\ngenerated based on the textual QA pair. The upper section shows the QA pair employed to instruct text-only\nChatGPT, while the lower section ( in blue ) shows the responses produced by ChatGPT.\ncination exists in all the generated descriptions.\nMore specifically, we find GPT4-V has diffi-\nculty understanding relationships between basic el-\nements like points and lines, and also struggles with\nprecisely interpreting these elements themselves\n(such as the angle B in Figure 1). Furthermore,\nsmaller MLLMs like LLaVA1.5 and MiniGPT4\ndemonstrate even greater difficulty in accurately\nidentifying the types of geometric shapes present\nin a figure.\nThis inadequacy in interpreting geometric dia-\ngrams may be one of the major causes for the fail-\nure in solving geometric problems. In contrast, ac-\ntual geometric diagrams typically exhibit clear and\nwell-defined relationships among their elements.\nThis geometry characteristic can be utilized to de-\nvelop datasets that help mitigate the above issues\nand mitigate hallucination.\n4\nGeometric Data Generation\nWhile previous efforts have been made to ad-\ndress multi-modal geometry problems (Chen et al.,\n2021, 2022; Cao and Xiao, 2022), the availabil-\nity of geometry datasets remains limited.\nThe\nkey limitations of existing datasets are threefold:\n(1) limited data volume (a few thousands for the\nlargest dataset), (2) absence of detailed descrip-\ntions for geometric images, and (3) a lack of diver-\nsity in problem-solving methodologies and answer\npathways. This limitation presents challenges for\nMLLMs in accurately understanding geometric el-\nements and providing precise geometric solutions.\nTo address this issue, we utilize the geometry\ncharacteristic to construct a multi-modal geometry\ndataset based upon existing dataset. This dataset\nincludes two parts: an alignment dataset to provide\nMLLMs with fundamental geometric knowledge\nand an instruction-tuning dataset to improve the\nassistant\u2019s ability to understand user instructions\nand generate accurate geometry solutions.\n4.1\nGeometric Cross-Modal Alignment Data\n4.1.1\nGeometric Image Caption Generation\nImage-caption datasets play a significant role in\ntraining MLLMs for understanding the context of\nimages, which is essential for aligning image and\ntext modalities. In the field of geometry, there is a\nlack of such datasets that offer detailed descriptions\nof geometric diagrams. To address this issue, we\npropose the generation of image descriptions from\nlabeled question-answer (QA) pairs, as illustrated\nin Table 1. In particular, we use text-only Chat-\nGPT 3.5 to create image captions based on these\nhuman-labeled QA pairs, which can be considered\nas a type of inverse information recovery. This ap-\nproach leverages the strong understanding ability\nof ChatGPT to produce descriptions for geometric\ndiagrams.\n4.1.2\nContrastive QA Pairs for Basic Elements\nOur approach also involves generating QA pairs to\nfacilitate the comprehension of geometric diagrams,\nfocusing primarily on their basic elements. The\nprocess begins with the interpretation of human-\nlabeled logical forms on Geometry3k (Lu et al.,\n2021).\nWe employ text-only ChatGPT to con-\nvert these logical forms into clear descriptions that\ncover various geometric elements such as shapes,\nlines, and points, and their relationships.\nAfter creating these diagram descriptions, the\nmodel begins to produce contrastive QA pairs.\nThese pairs are designed to examine different as-\npects of the diagrams. Questions may explore the\npresence of certain geometric elements (e.g., \"Are\nContrastive QA Pairs for Basic Elements\nLogic Form:\nSimilar(Triangle(A,B,C),Triangle(C,B,D))\nTriangle(A,B,C)\nTriangle(A,C,B)\nEquals(LengthOf(Line(C, B)), 5)\nEquals(LengthOf(Line(C, D)), 3)\nEquals(LengthOf(Line(B, D)), 4)\nPointLiesOnLine(D, Line(A, B))\nPerpendicular(Line(A, C), Line(C, B))\nPerpendicular(Line(C, D), Line(A, D))\nPhase-1 Generation\nInfo Items:\nThe geometric figure have the following information:\nInfo 1: The figure represents a triangle with vertices A, B, and C.\nInfo 2: Triangle ABC is similar to triangle CBD.\nInfo 3: Point D lies on the line segment AB.\nInfo 4: Line segment AC is perpendicular to line segment CB.\n...\nSummary:\nThe figure depicts a triangle ABC and triangle CBD, which is similar to triangle ABC. Additionally, triangle ACB is\nalso present in the figure. The lengths of the segments CB, CD, and BD are 5, 3, and 4 units respectively. Point D\nlies on the line segment AB, and perpendicularity is indicated between the line segments AC and CB, as well as\nbetween the line segments CD and AD.\nPhase-2 Generation\nQA pairs:\n1. Question: Does the diagram include a triangle formed by vertices A, B, and D?\nAnswer: No, the diagram does not include a triangle formed by vertices A, B, and D. The triangles present are ABC\nand CBD, and the point D lies on the line segment AB.\n2. Question: Does the diagram contain a triangle with vertices A, C, and B?\nAnswer: Yes, the diagram contains triangle ACB, which is the same as triangle ABC.\n3. Question: Is point D the lies on line BC?\nAnswer: No, point D is not on lines BC. The point D lies on the line segment AB.\n4. Question: Is line segment AC perpendicular to line segment CB?\nAnswer: Yes, line segment AC is perpendicular to line segment CB.\nTable 2: Geometric image description and contrastive QA pairs for understanding basic elements. The generation\nprocess consists of two phases: 1) Translating the human-labelled logic form into detailed information items and a\nsummary of the diagram description. 2) Generating contrastive QA pairs based on the provided information and\nsummary. The sections in blue display the responses generated by ChatGPT. The detailed prompt will be included\nin the appendix.\nthere triangular shapes in the diagram?\") or check\nthe accuracy of the relationships described (e.g., \"Is\npoint D the lies on line BC?\"). This method enables\nthe model to comprehend geometric concepts and\nto analyze and interpret the details in geometric\ndiagrams accurately. The generation example is\nshown on Table 2.\n4.2\nGeometric Instruction Data\nAfter performing alignment leveraging the con-\nstructed alignment data, the model is able to better\ninterpret the geometric diagram (Figure 1). How-\never, they are still limited at solving geometric prob-\nlems. Therefore, we construct an instruction tuning\ndataset based on existing datasets with the help of\npowerful LLMs. Specifically, we design a series of\nstrategies to expand the question-answer pairs in ex-\nisting datasets. The resulting dataset contains more\nthan 110k QA pairs, which is the largest public\ngeometric QA dataset available. We will introduce\nthe proposed strategies in detail below.\n4.2.1\nEquation Solving (ES)\nAs shown in Table 5, we replace the specific val-\nues in the original QA pairs with unknown vari-\nables and prompt the LLM to construct the solution\nby solving equation. Such data is helpful for the\nMLLM to generalize its understanding of the prob-\nlem, which enables it to apply the similar reasoning\nand solution steps to different scenarios. The ab-\nstraction of the problem by using variables and\nsolving equation helps the LLM focus on the un-\nOriginal Example\nQuestion:\nAs shown in the figure, the length of the short arm of the railing at the railway crossing is\n1.0 and the length of the long arm is 16.0. When the end of the short arm drops by 0.5, the\nend of the long arm rises ()\nAnswer:\nBy the property of similar triangles, the lengths of corresponding sides are proportional. Since\nthe long arm is 16 times longer than the short arm, the height it rises is 16 times the height\nthe short arm drops, which is 0.5 multiplied by 16, resulting in a rise of 8 meters.\nTable 3: The original example.\nValue Scaling\nQuestion:\nAt a railroad crossing, the short arm of the barrier is 2.0 unit long and the long arm is 32 units\nlong. If the short arm drops 1 units, by how many units will the long arm rise?\nAnswer:\nDenote the rise of the long arm as \u2018h\u2019. In similar triangles, the ratios of their corresponding\nsides are in proportion. h can be calculated using the equation 1/h = 2/32. Solving for \u2018h\u2019\ngives h = 32 * 1 / 2, which equals 16 units. Therefore, the long arm rises by 16 units.\nTable 4: Synthetic example via value scaling.\nderlying mathematical concepts and relationships,\nrather than getting caught up in specific numerical\nvalues.\n4.2.2\nValue Scaling (VS)\nAs shown in Table 4, we augment the data by scal-\ning the length values in the QA pairs. Note that for\nthe same diagram, the QA pair is still correct if all\nthe lengths in a geometric problem are scaled si-\nmultaneously. However, note that it is not the case\nfor quantities such as angles. When different scal-\ning of values are applied, the LLM becomes more\nflexible in handling different numerical inputs. In-\nvolving a range of values that extends beyond the\ninitial training dataset aids in refining the model\u2019s\ncomputational and reasoning capabilities, thereby\ncontributing to its generalizability.\n4.2.3\nRe-Formulating Condition as Unknown\n(RCU)\nMotivated by(Weng et al., 2023; Yu et al., 2023),\nwe design new multi-modal QA pairs that ask ques-\ntions backwards, as shown in Table 6. Specifically,\nwe reformulate questions to ask for the values orig-\ninally present in the condition, and retain the gen-\nerated data with correct answer only. In this way,\nthe LLM is repeatedly exposed to the relationships\nbetween variables, equations, and their solutions.\nThis reinforcement helps the model learn the de-\npendencies and connections between different ele-\nments in a mathematical problem.\n4.2.4\nSentence Paraphrase (SP)\nWe also conduct paraphrasing for both the ques-\ntion and answer pairs, as shown in Table 7. This\nexposes the LLM to a broader range of phrasing\nand language variations. This helps the model be-\ncome more robust in understanding and generating\ndiverse sentence structures. Consequently, it can\nhandle similar questions with different phrasings\nand provide accurate responses.\n5\nModel Architecture and Training\n5.1\nModel Architecture\nWe utilize the LLAVA (Liu et al., 2023) architec-\nture for our model. The model mainly consists of\na large language model (LLM) such as LLAMA-\n2 (Touvron et al., 2023), a pretrained vision trans-\nformer (Radford et al., 2021) (ViT) as image en-\ncoder. In addition, a projection layer is required to\nmap the visual features from the image encoder to\nthe same dimension as the LLM.\nDuring inference, given an image and a textual\ninstruction, the image encoder first extracts the vi-\nsual tokens from the image, which are then mapped\nto the dimension of LLM\u2019s embedding space via\nthe projection layer. Then, the mapped image fea-\ntures are concatenated with text embeddings to\nserve as the input to the LLM. Subsequently, the\nVariable Substitution and Equation Solving\nQuestion:\nGiven a railway crossing barrier with two arms, where the shorter arm has a length of \u2018a\u2019\nunits (a = 1.0) and the longer arm has a length of \u2018b\u2019 units (b = 16.0), determine the elevation\n\u2018y\u2019 units of the longer arm\u2019s end when the shorter arm descends by \u2018d\u2019 units (d = 0.5).\nExpress your answer as a function of \u2018a\u2019, \u2018b\u2019, and \u2018d\u2019.\nAnswer:\nTo find the elevation \u2018y\u2019 of the long arm\u2019s end, we apply the property of similar triangles and establish the equation\nd\ny = a\nb . Substituting the given values, we get 0.5\ny =\n1.0\n16.0. Solving for \u2019y\u2019 leads to y = b\u00d7d\na , which simplifies to\ny = 16.0\u00d70.5\n1.0\n. Therefore, \u2019y\u2019 equals 8.0 units, indicating that the long arm rises by 8.0 units when the short arm\ndescends by 0.5 units.\nQuestion:\nIn a railway crossing lever system, the lengths of the shorter and longer arms are \u2018s\u2019 units (s = 1.0) and \u2018l\u2019 units (l\n= 16.0) respectively. If the shorter arm lowers by \u2018deltaS\u2019 units (deltaS = 0.5), what is the increase \u2018deltaL\u2019 in the\nlength of the longer arm?\nAnswer:\nThe increase \u2018deltaL\u2019 in the longer arm can be found using the proportionality relationship of similar triangles. The\nequation can be expressed as: deltaS\ndeltaL = s\nl . Inserting the given values, the equation becomes\n0.5\ndeltaL =\n1.0\n16.0. Solving\nfor \u2018deltaL\u2019 yields deltaL = l\u00d7deltaS\ns\n, which simplifies to deltaL = 16.0\u00d70.5\n1.0\n. Thus, deltaL is 8.0 units.\nQuestion:\nAt a railroad crossing, the short arm of the barrier is 1.0 unit long and the long arm is 16.0 units long. If the short\narm drops 0.5 units, by how many units will the long arm rise?\nRepresent the rise of the long arm as \u2018h\u2019.\nAnswer:\nThe triangles formed by the shorter and longer arms as they descend are similar. The rise of the long arm \u2018h\u2019 can be\ncalculated using the property of similar triangles, the equation is 0.5/h = 1/16. Solving for \u2018h\u2019 gives h = 16 * 0.5 / 1,\nwhich equals 8.0 units.\nTable 5: Synthetic example via replacing values to unknown variables and solving the equation.\nRe-Formulating Condition as Unknown (Weng et al., 2023; Yu et al., 2023)\nQuestion:\nAt a railroad crossing, the short arm of the barrier is u unit long and the long arm is 16.0\nunits long. When the end of the short arm drops by 0.5, the end of the long arm rises 8 units.\nWhat is the value of unknown variable u?\nAnswer:\nDenote the short arm of the barrier as variable u. By the property of similar triangles,\nwe can determine u by solving the equation 0.5/8 = u/16. Therefore, u equals 1.\nTable 6: Synthetic example via re-formulating condition as unknown.\nSentence Paraphrase\nQuestion:\nIn the illustration, the railing at the railway crossing has a short arm measuring 1.0 unit\nin length and a long arm measuring 16.0 units. When the short arm drops by 0.5 units,\nwhat is the corresponding rise in the long arm?\nAnswer:\nThe triangles are similar, and their corresponding sides are proportional. The long arm\nis 16 times longer than the short arm, resulting in an 8-meter rise when the short arm\ndrops by 0.5 meters.\nTable 7: Synthetic example via sentence paraphrase.\nLLM begins to perform next-token-generation.\n5.2\nModel Training\nWe train our G-LLaVA in two phases, namely 1) ge-\nometric visual-language alignment, and 2) geomet-\nric instruction tuning. In both phases, we leverage\nthe conventional language modeling loss, which\ncan be formulated as follows:\nL(Star, Sin, I) = \u2212\nL\nX\nt=1\nlog p\nh\nSt\ntar|F(s(<t)\ntar , Sin, I)\ni\n(1)\nwhere F represents the model. I represents the\ngeometric figure; Star and Sin represent the target\nand input sentences, respectively; St\ntar denotes the\ntth token of target output, and L stands for length.\n6\nExperiments\n6.1\nSetup\nDataset.\nWe\ngenerate\nthe\nalignment\ndata\nand instruction data utilizing training set of\nGeoQA+ (Cao and Xiao, 2022) and Geome-\ntry3K (Lu et al., 2021). More specifically, the\ncontrastive question-answer (QA) pairs in the\nalignment data are generated using Geometry3K,\nwhich features human-labeled logical forms.\nNote that GeoQA+ covers the training set of\nGeoQA (Chen et al., 2021), and share the same\nval/test set as GeoQA (Chen et al., 2021). More\ndetails of data split on GeoQA and GeoQA+ is\nlisted in Table 9. Our approach results in 60K\nalignment data samples, and more than 110K\ninstruction data samples.\nWe compare our model with other MLLMs on\nthe geometry problems on the minitest split Math-\nVista (Lu et al., 2023), and compare our model\nwith traditional in-domain model on the test split\nof GeoQA following (Chen et al., 2022; Liang\net al., 2023). The geometry problems in MathVista\nminitest set is collected from four source datasets\nGeometry3K (Lu et al., 2021), GeoQA+ (Cao and\nXiao, 2022), GEOS (Seo et al., 2015) and Uni-\nGeo (Chen et al., 2022).\nImplementation Details.\nWe employ ChatGPT\n(gpt-3.5-turbo-0613) for data generation. A de-\ntailed description of our prompts will be provided\nin the appendix. We use LLaVA (Liu et al., 2023)\nas our backbone. More specifically, we utilize\nLLAMA-2 (Touvron et al., 2023) as the language\nModel\nInput\nAccuracy (%)\nHeuristics Baseline\nRandom Chance\n-\n21.6\nFrequent Guess\n-\n34.1\nHuman\nQ, I\n48.4\nClose Source Model\nText-Only LLMs\n2-shot CoT Claude-2\nQ\n29.8\n2-shot CoT ChatGPT\nQ\n36.5\n2-shot CoT GPT-4\nQ\n44.7\n2-shot PoT ChatGPT\nQ\n30.8\n2-shot PoT GPT-4\nQ\n33.2\nVisual-Augmented LLMs\n2-shot CoT Claude-2\nQ, Ic, It\n31.7\n2-shot CoT ChatGPT\nQ, Ic, It\n29.3\n2-shot CoT GPT-4\nQ, Ic, It\n31.7\n2-shot PoT ChatGPT\nQ, Ic, It\n26.4\n2-shot PoT GPT-4\nQ, Ic, It\n39.4\nMultimodal LLMs\nMultimodal Bard\nQ, I\n47.1\nGemini Nano 1\nQ, I\n21.6\nGemini Nano 2\nQ, I\n23.6\nGemini Pro\nQ, I\n40.4\nGemini Ultra\nQ, I\n56.3\nGPT4-V\nQ, I\n50.5\nOpen Source Model\nIDEFICS (9B-Instruct)\nQ, I\n21.1\nmPLUG-Owl (LLaMA-7B)\nQ, I\n23.6\nminiGPT4 (LLaMA-2-7B)\nQ, I\n26.0\nLLaMA-Adapter-V2 (7B)\nQ, I\n25.5\nLLaVAR\nQ, I\n25.0\nInstructBLIP (Vicuna-7B)\nQ, I\n20.7\nLLaVA (LLaMA-2-13B)\nQ, I\n29.3\nG-LLaVA-7B\nQ, I\n53.4\nG-LLaVA-13B\nQ, I\n56.7\nTable 8: Comparison of model performance on the test-\nmini set of MathVista benchmarks (Lu et al., 2023) on\ngeometry problem solving (GPS) . For input, Q repre-\nsents for question, I represents for image, Ic represents\nfor image caption generated by Bard, and It represents\nfo OCR text detected in the image. Baseline results are\nobtained from Lu et al. (2023). Human performance\nand the results surpassing human performance are high-\nlighted in grey. Our results are highlighted in blue .\nmodel and employ the visual encoder of a pre-\ntrained vision transformer (Radford et al., 2021)\n(ViT). The resolution of the input image is 336 by\n336. We conduct experiments with both 7B and\n13B LLMs. In the cross-modal alignment process,\nonly the projection linear layer is trainable. During\nthe instruction tuning phase, both the projection\nlinear layer and the language model are trainable.\nFor training data, as we found the minitest split\nof MathVista contains some examples of Mix-\ntrain.pk of GeoQA+, we remove those samples\nDataset\nTrain\nValidation\nTest\nGeoQA+ (Cao and Xiao, 2022)\n6027\n745\n754\nGeoQA (Chen et al., 2021)\n3499\n745\n754\nTable 9: Data Split of GeoQA and GeoQA+.\nModel\nInput\nAccuracy (%)\nRandom Chance\n-\n25.0\nFrequent Guess\n-\n32.1\nTop-10 Accuracy\nNGS (Chen et al., 2021)\nQ, I\n56.9\nDPE-GPS (Cao and Xiao, 2022)\nQ, I\n62.7\nSCA-GPS (Ning et al., 2023)\nQ, I\n64.1\nTop-1 Accuracy\nGeoformer (Chen et al., 2022)\nQ, I\n46.8\nUniMath (Liang et al., 2023)\nQ, I\n50.0\nG-LLaVA-7B\nQ, I\n64.2\nG-LLaVA-13B\nQ, I\n67.0\nTable 10: Comparison of model performance with tradi-\ntional methods on GeoQA.\nthat also appears in minitest split of MathVista.\nThe learning rate is set to 3e\u22125. We expand the\nimages into squares during training, where the ex-\ntended background color is set to white. For image\naugmentation, we set the maximum translation dis-\ntance to 0.25 of the length of longer side. If not oth-\nerwise specified, the models are trained for 1 epoch\nfor cross-modal alignment and 2 epochs for instruc-\ntion tuning, respectively. And the batch sizes are\nset to 6 per GPUs and 32 per GPUs, respectively.\nEvaluation Metric.\nWe use accuracy as the met-\nric for evaluation. Note that several prior studies\n(Chen et al., 2021, 2022; Cao and Xiao, 2022) re-\nport results using Top-10 accuracy (generating 10\nsequences and selecting the first sequence that suc-\ncessfully addresses the problem as the prediction).\nOur experimental results directly report Top-1 ac-\ncuracy. During instruction tuning, we enable the\nmodel to output the choice in a fixed format. For\nevaluation, we directly use regular expression to\nextract the predicted choices from the generated an-\nswers. The answer is considered false if the regular\nexpression fails to extract a valid answer.\n6.2\nMain Experiment\nWe compared G-LLaVA with other MLLMs on\nminitest split of MathVista (Lu et al., 2023) bench-\nmark on Table 8. The results shows that, geometric\ncross-modal alignment and instructing tuning on\nour dataset is effective in improve MLLMs\u2019 ge-\nometric problem solving ability. Our specific in-\ndomain model G-LLaVA-7B can even surpass the\nstrong GPT4-V on geometric problems.\n6.3\nComparison with Conventional Methods\nWe additionally compare our method with conven-\ntional SOTA methods in geometry problem solving\ndomain. As illustrated in Table 10, our method\ndemonstrates a notable improvement in Top-1 accu-\nracy over the existing SOTA techniques. Moreover,\nour model\u2019s top-1 accuracy outperforms the base-\nlines\u2019 top-10 accuracy, demonstrating a significant\nimprovement in predictive precision.\n6.4\nPerformance Across Problem Difficulties\nWe compare G-LLaVA with the baselines mod-\nels on problems with different difficulty levels, as\nshown in Table 11. Specifically, OP represents the\nnumber of \u201coperations\", or reasoning steps that\nneeds to be taken for solving the problem. The\nresults verify that our G-LLaVA consistently out-\nperforms baseline models by a large margin across\nvarious difficulty levels.\nModel\nOP=1(%)\nOP=2(%)\nOP=3(%)\nOP>=4(%)\nTotal(%)\nLLaVA-7B\n16.8\n20.9\n15.5\n22.9\n18.7\nLLaVA-13B\n19.1\n21.3\n18.5\n24.6\n20.3\nG-LLaVA-7B\n77.5\n60.8\n54.8\n40.9\n64.2\nG-LLaVA-13B\n79.0\n64.9\n55.5\n49.1\n67.0\nTable 11: Different difficulty problems on GeoQA.\n6.5\nPerformance Across Different Types of\nQuestions\nWe compare G-LLaVA with the baselines models\non problems with different type of questions, as\nshown in Table 12. The results suggest that G-\nLLaVA performs better than the baseline models in\nvarious geometric problems such as angle, length,\nand area problems.\nModel\nAngel\nLength\nArea\nOther\nTotal\nLLaVA-7B\n16.1\n22.2\n17.0\n14.3\n18.7\nLLaVA-13B\n17.5\n23.0\n25.5\n28.6\n20.3\nG-LLaVA-7B\n70.7\n56.5\n55.3\n42.9\n64.2\nG-LLaVA-13B\n71.5\n61.1\n63.8\n57.1\n67.0\nTable 12: Performance of different types of questions\non GeoQA.\nModel\nInput\nAccuracy (%)\nRandom Chance\n-\n25.0\nFrequent Guess\n-\n32.1\nGeoformer (Chen et al., 2022)\nQ, I\n46.8\nUniMath (Liang et al., 2023)\nQ, I\n50.0\nG-LLaVA-7B\nQ, I\n64.2\nw/o alignment phase\nQ, I\n62.8\nTable 13: Effectiveness of alignment in the pre-training\nphase. Top-1 accuracy is reported.\nSince AB is tangent to circle O at point B, therefore, angle ABO = \n90 \u00b0. Since angle A = 36 \u00b0, therefore, angle BOA = 54\u00b0. Therefore, \naccording to the inscribed angle theorem, angle C = 1/2 angle \nBOA = 27 \u00b0. Therefore, the correct choice is D.\nAs shown in the figure, AB is tangent to circle O \nat point B, and the extended line of AO intersects \ncircle O at point C. Connect BC, if angle A = 36.0, \nthen angle C is equal to () \nChoices: A:36\u00b0 B:54\u00b0 C:60\u00b0 D:27\u00b0\nThe image you've uploaded appears to represent a geometric problem involving a circle \nwith a tangent line and intersecting lines forming angles. To solve for angle C given that \nangle A is 36 degrees, we can use the properties of circles and angles.\nHere's how we can do it:\n1.In a circle, a radius that intersects a tangent line at the point of tangency forms a right \nangle. This means that angle OBA is 90 degrees.\n2.Triangle OAB is a right triangle with angle OAB being 90 degrees and angle AOB \nbeing 36 degrees. Using the fact that the sum of angles in a triangle is 180 degrees, we \ncan find angle ABO.\n3.Angle ABO is equal to 180 degrees minus the sum of angles AOB and OAB, which is \n180\u221290\u221236=54 degrees.\n4.180\u221290\u221236=54\n5.Angle ABC is equal to angle ABO because they are both formed by the tangent line \nAB and the radius OB. So angle ABC is also 54 degrees.\n6.Angle ACB is in the same segment as angle AOB, and angles in the same segment are \nequal. Therefore, angle ACB is also 36 degrees.\nSo, angle C, which is angle ACB, is 36 degrees. Therefore, the correct choice is: A: 36\u00b0\nGPT4-V\nG-LLaVA\nConnecting BD, as shown in the diagram. Because point D is \nthe midpoint of segment AC, that is, CD equals AD. \nTherefore, angle ABD equals angle CBD. And angle ABC is \n50 degrees, so angle ABD is half of 50 degrees, which is 25 \ndegrees. Since AB is the diameter of the semicircle, angle \nADB is 90 degrees. Therefore, angle DAB is 90 degrees \nminus 25 degrees, which is 65 degrees. Therefore, the answer \nis C.\nAs shown in the figure, AB is the diameter of the \nsemicircle, angle ABC = 50.0, point D is the \nmidpoint of arc AC, then angle DAB is equal to ()\nChoices: A:40\u00b0 B:50\u00b0 C:65\u00b0 D:70\u00b0\nTo solve this problem, we need to apply some geometry principles. Here's the step-by-\nstep solution:\n1. Given that AB is the diameter of the semicircle, \u2220ACB is a right angle because the \nangle subtended by a diameter in a semicircle is always a right angle.\n2. Since \u2220ACB is a right angle and \u2220ABC = 50\u00b0 , we can find \u2220BAC using the fact that \nthe angles in a triangle sum to 180\u00b0 .\n3. \u2220BAC = 180\u00b0 - 90\u00b0 - 50\u00b0 = 40\u00b0 .\n4. Point D is the midpoint of arc AC , which means AD = DC and \u2220DAB = \u2220DAC \nbecause angles subtended by the same arc are equal.\n5. In a semicircle, the angle at the circumference subtended by the diameter is 90\u00b0 . \nSince D is the midpoint of arc AC , \u2220ADC would be half of 90\u00b0 , which is 45\u00b0 .\n6. Now, \u2220DAC is half of \u2220ADC , because AD = DC . So, \u2220DAC = \u2220DAB = 45\u00b0 / 2 = \n22.5\u00b0.\n7. Since \u2220BAC = 40\u00b0 and \u2220DAB is half of that, \u2220DAB = 40\u00b0 / 2 = 20\u00b0 .\nTherefore, the correct answer is that \u2220DAB is 20\u00b0. However, this option is not listed in \nyour multiple-choice answers. There might be an error in the question or in the \ninterpretation. Please review the question and the steps provided to ensure that all given \ninformation and the steps are accurate.\nGPT4-V\nG-LLaVA\nSince quadrilateral ABCD is a parallelogram, we have AD \nparallel to BC. Therefore, angle BCA is equal to angle DAC, \nwhich is 42 degrees. Angle COD can be calculated as the sum \nof angle CBD and angle BCA, which is 42 degrees plus 23 \ndegrees, equal to 65 degrees. Therefore, the correct answer is \nC.\nIn parallelogram ABCD, the diagonal AC \nand BD intersect at point O, angle DAC = \n42.0, angle CBD = 23.0, then angle COD \nis (). \nChoices: A:61 B:63 C:65 D:67\nGiven:\n- \u2220DAC = 42\u00b0\n- \u2220CBD = 23\u00b0\nIn parallelogram ABCD, diagonals bisect each other, so \u2220AOD = \u2220COD and \u2220BOC = \n\u2220COB.\nSince opposite angles in a parallelogram are equal, we have \u2220DAB = \u2220DAC = 42\u00b0 and \n\u2220CDB = \u2220CBD = 23\u00b0 .\nUsing the fact that consecutive angles in a parallelogram sum up to 180\u00b0 :\n\u2220AOD = 180\u00b0 - \u2220DAB = 180\u00b0 - 42\u00b0 = 138\u00b0\nNow, \u2220AOD and \u2220BOC form a straight line at point O, so:\n\u2220COD + \u2220BOC = 180\u00b0\nGiven that \u2220BOC = 23\u00b0 , we find \u2220COD by subtracting from 180\u00b0 :\n\u2220COD = 180\u00b0 - \u2220BOC = 180\u00b0 - 23\u00b0 = 157\u00b0\nTherefore, \u2220COD = 157\u00b0 .\nGPT4-V\nG-LLaVA\nFigure 3: Demonstration of geometric problem solving by GPT-4-V and G-LLaVA.\n6.6\nEffectiveness of Cross-Modal Geometric\nAlignment\nTo evaluate the alignment phase\u2019s effectiveness, we\nconducted the analysis of the model\u2019s performance\nwith and without alignment phase in Table 13. The\nresults suggest that the alignment phase enhances\nthe model\u2019s ability to interpret images, which is\nalso illustrated by the qualitative result in Figure 1.\n7\nConclusion\nIn this paper, we make the attempt to address the\nlimitations of current MLLMs in solving geometric\nproblems. We propose strategies to enrich the data\nby leveraging LLMs, resulting in our augmented\ndataset, Geo170K. With this dataset, our G-LLaVA\noutperforms GPT-4-V on the geometric split of\nMathVista benchmark, with as few as 7B parame-\nters. We hope our work provides new insights on\nimproving multimodal LLMs\u2019 ability of solving\ngeometric problems.\nReferences\nChris Alvin, Sumit Gulwani, Rupak Majumdar, and\nSupratik Mukhopadhyay. 2017. Synthesis of solu-\ntions for shaded area geometry problems. In The\nThirtieth International Flairs Conference.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023. Qwen-vl: A versatile vision-\nlanguage model for understanding, localization, text\nreading, and beyond.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n2022. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nJie Cao and Jing Xiao. 2022. An augmented benchmark\ndataset for geometric question answering through\ndual parallel text encoding. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 1511\u20131520.\nJiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin,\nChongyu Chen, and Xiaodan Liang. 2022. UniGeo:\nUnifying geometry logical reasoning via reformulat-\ning mathematical expression. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3313\u20133323.\nJiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang,\nLingbo Liu, Eric Xing, and Liang Lin. 2021. GeoQA:\nA geometric question answering benchmark towards\nmultimodal numerical reasoning.\nIn Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 513\u2013523, Online. Association\nfor Computational Linguistics.\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao. 2023. Shikra: Unleashing\nmultimodal llm\u2019s referential dialogue magic.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. 2023. In-\nstructblip: Towards general-purpose vision-language\nmodels with instruction tuning.\nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and\nTushar Khot. 2023. Specializing smaller language\nmodels towards multi-step reasoning.\nJiahui Gao, Renjie Pi, Yong Lin, Hang Xu, Jiacheng\nYe, Zhiyong Wu, Weizhong Zhang, Xiaodan Liang,\nZhenguo Li, and Lingpeng Kong. 2023a. Self-guided\nnoise-free data generation for efficient zero-shot\nlearning.\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie\nGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He,\nXiangyu Yue, Hongsheng Li, and Yu Qiao. 2023b.\nLlama-adapter v2: Parameter-efficient visual instruc-\ntion model.\nGoogle. 2023. Gemini: A family of highly capable\nmultimodal models.\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,\nYujiu Yang, Minlie Huang, Nan Duan, and Weizhu\nChen. 2023. Tora: A tool-integrated reasoning agent\nfor mathematical problem solving.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nAlbert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou,\nTimothee Lacroix, Jiacheng Liu, Wenda Li, Mateja\nJamnik, Guillaume Lample, and Yuhuai Wu. 2023.\nDraft, sketch, and prove: Guiding formal theorem\nprovers with informal proofs. In The Eleventh Inter-\nnational Conference on Learning Representations.\nXin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. 2023. Lisa: Reason-\ning segmentation via large language model. arXiv\npreprint arXiv:2308.00692.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models.\nZhenwen Liang, Tianyu Yang, Jipeng Zhang, and Xian-\ngliang Zhang. 2023. Unimath: A foundational and\nmultimodal mathematical reasoner. In EMNLP.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning.\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-\nyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-\nWei Chang, Michel Galley, and Jianfeng Gao. 2023.\nMathvista: Evaluating math reasoning in visual con-\ntexts with gpt-4v, bard, and other large multimodal\nmodels.\nPan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan\nHuang, Xiaodan Liang, and Song-Chun Zhu. 2021.\nInter-GPS: Interpretable geometry problem solving\nwith formal language and symbolic reasoning. In\nThe 59th Annual Meeting of the Association for Com-\nputational Linguistics (ACL).\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-\nguang Lou, Chongyang Tao, Xiubo Geng, Qingwei\nLin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-\nardmath: Empowering mathematical reasoning for\nlarge language models via reinforced evol-instruct.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language models:\nTowards zero-shot language understanding. arXiv\npreprint arXiv:2202.04538.\nMaizhen Ning, Qiu-Feng Wang, Kaizhu Huang, and\nXiaowei Huang. 2023. A symbolic characters aware\nmodel for solving geometry problems. In Proceed-\nings of the 31st ACM International Conference on\nMultimedia, MM \u201923, page 7767\u20137775, New York,\nNY, USA. Association for Computing Machinery.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4.\nRenjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze\nDong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang\nXu, Lingpeng Kong, and Tong Zhang. 2023a. Detgpt:\nDetect what you need via reasoning.\nRenjie Pi, Lewei Yao, Jiahui Gao, Jipeng Zhang, and\nTong Zhang. 2023b. Perceptiongpt: Effectively fus-\ning visual perception into llm.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision.\nMrinmaya Sachan, Kumar Dubey, and Eric Xing. 2017.\nFrom textbooks to knowledge: A case study in har-\nvesting axiomatic knowledge from textbooks to solve\ngeometry problems. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 773\u2013784.\nMrinmaya Sachan and Eric Xing. 2017.\nLearning\nto solve geometry problems from natural language\ndemonstrations in textbooks. In Proceedings of the\n6th Joint Conference on Lexical and Computational\nSemantics (* SEM 2017), pages 251\u2013261.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ilic, Daniel Hesslow, Roman\nCastagne, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\nMatthias Galle, et al. 2022.\nBloom:\nA 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nMinjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren\nEtzioni, and Clint Malcolm. 2015. Solving geometry\nproblems: Combining text and diagram interpretation.\nIn Proceedings of the 2015 conference on empirical\nmethods in natural language processing, pages 1466\u2013\n1476.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nYixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang,\nand Deng Cai. 2023.\nPandagpt: One model to\ninstruction-follow them all.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nhttps://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothee Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. arXiv preprint arXiv:2203.11171.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He,\nKang Liu, and Jun Zhao. 2023. Large language mod-\nels are better reasoners with self-verification. CoRR,\nabs/2212.09561.\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang.\n2023. The dawn of lmms: Preliminary explorations\nwith gpt-4v(ision).\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022a. Zerogen: Efficient zero-shot learning via\ndataset generation. In Empirical Methods in Natural\nLanguage Processing.\nJiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng,\nTao Yu, and Lingpeng Kong. 2022b. ProGen: Pro-\ngressive zero-shot dataset generation via in-context\nfeedback. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2022.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu,\nZhengying Liu, Yu Zhang, James T. Kwok, Zhenguo\nLi, Adrian Weller, and Weiyang Liu. 2023. Meta-\nmath: Bootstrap your own mathematical questions\nfor large language models.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao\nYu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan\nZheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang,\nHuan Sun, Yu Su, and Wenhu Chen. 2023a. Mmmu:\nA massive multi-discipline multimodal understand-\ning and reasoning benchmark for expert agi.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao\nHuang, Huan Sun, Yu Su, and Wenhu Chen. 2023b.\nMammoth: Building math generalist models through\nhybrid instruction tuning.\nShilong Zhang, Peize Sun, Shoufa Chen, Min Xiao,\nWenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, and\nPing Luo. 2023. Gpt4roi: Instruction tuning large\nlanguage model on region-of-interest.\nXueliang Zhao, Xinting Huang, Wei Bi, and Lingpeng\nKong. 2023a. Sego: Sequential subgoal optimization\nfor mathematical problem-solving. arXiv preprint\narXiv:2310.12960.\nXueliang Zhao, Wenda Li, and Lingpeng Kong. 2023b.\nDecomposing the enigma: Subgoal-based demon-\nstration learning for formal theorem proving. arXiv\npreprint arXiv:2305.16366.\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models.\narXiv preprint\narXiv:2205.10625.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models.\n"
  },
  {
    "title": "MagicScroll: Nontypical Aspect-Ratio Image Generation for Visual Storytelling via Multi-Layered Semantic-Aware Denoising",
    "link": "https://arxiv.org/pdf/2312.10899.pdf",
    "upvote": "14",
    "text": "MagicScroll: Nontypical Aspect-Ratio Image Generation for Visual Storytelling\nvia Multi-Layered Semantic-Aware Denoising\nBingyuan Wang1\nHengyu Meng3\nZeyu Cai1\nLanjiong Li1\nYue Ma2\nQifeng Chen2\nZeyu Wang1,2\u2020\n1HKUST(GZ)\n2HKUST\n3South China University of Technology\nhttps://magicscroll.github.io/\n\u201cBeside a floating fortress, the hero encounters a beauty bathing in waterfall\u2026\u201d\n\u201cIn the city, the police encountered a monster and Spiderman came to fight...\u201d\n\u201cCrowds and women at the door, a man and some foxes behind\u2026\u201d\n\u201cThe past drifts away, leaving only an ancient tower under sunset\u2026\u201d\n\u201cEnveloped in natural scenery, I gradually step into an ink painting\u2026\u201d\n\u201cIn a futuristic otherworld land, a warrior embarks on his journey\u2026\u201d\nFigure 1. Example results generated by MagicScroll. Our framework is designed for generating coherent, controllable, and engaging\nnontypical aspect-ratio images from story texts. We support multi-layered, refined controls over style, content, and layout, with multiple\nconditions including predicted masks, reference images, and style concepts.\nAbstract\nVisual storytelling often uses nontypical aspect-ratio im-\nages like scroll paintings, comic strips, and panoramas to\ncreate an expressive and compelling narrative. While gen-\nerative AI has achieved great success and shown the po-\ntential to reshape the creative industry, it remains a chal-\nlenge to generate coherent and engaging content with ar-\nbitrary size and controllable style, concept, and layout,\nall of which are essential for visual storytelling. To over-\ncome the shortcomings of previous methods including repet-\nitive content, style inconsistency, and lack of controllabil-\nity, we propose MagicScroll, a multi-layered, progressive\n\u2020 Corresponding author.\ndiffusion-based image generation framework with a novel\nsemantic-aware denoising process. The model enables fine-\ngrained control over the generated image on object, scene,\nand background levels with text, image, and layout condi-\ntions.\nWe also establish the first benchmark for nontyp-\nical aspect-ratio image generation for visual storytelling\nincluding mediums like paintings, comics, and cinematic\npanoramas, with customized metrics for systematic evalu-\nation. Through comparative and ablation studies, Magic-\nScroll showcases promising results in aligning with the nar-\nrative text, improving visual coherence, and engaging the\naudience. We plan to release the code and benchmark in\nthe hope of a better collaboration between AI researchers\nand creative practitioners involving visual storytelling.\narXiv:2312.10899v1  [cs.CV]  18 Dec 2023\n1. Introduction\nRecent advances in generative AI have achieved great suc-\ncess in visual content generation from text and have been\nwidely applied to the creative industry. One creative ap-\nplication that has a large demand for AI-generated content\nis visual storytelling. Visual storytelling typically requires\nimage sequences or videos that are consistent with both\nthe narrative and its atmosphere through multiple coherent\nand stylized scenes. However, there is a lack of suitable\nmethods for generating such visual content, especially with\na high level of controllability, correspondence, coherence,\nand content richness.\nAs a classic visual storytelling form, nontypical aspect-\nratio images allow for a more dynamic and engaging rep-\nresentation of the story\u2019s progression and emotion through\nspatially unfolding a series of objects and scenes described\nin the text. These aspects are commonly reflected in scroll\npaintings, comic strips, and cinematic panoramas, which\nleverage imagery in multiple scenes to construct a unified\nand stylized space for the story. However, existing meth-\nods did not distinguish storytelling visuals from common\nimages, thus lack conditional models for refined local con-\ntrol. Another challenge is a lack of consideration of smooth\ntransition, especially between multiple generated results.\nThis paper contributes a framework for nontypical\naspect-ratio image generation for visual storytelling with\na benchmark for systematic evaluation.\nWe first de-\nfine the task and describe its challenges and applications.\nBased on state-of-the-art diffusion models, we propose a\nnovel method that integrates GPT-based layout prediction,\nsemantic-aware denoising, and multiple controllable mod-\nules. Given the original story, our method is capable of\npredicting scene and object masks and generating nontypi-\ncal aspect-ratio images that align with both textual narrative\nand conditions for user control such as style and layout.\nTo address the issues of repetitive content and incon-\nsistent style in existing methods, we design a novel multi-\nlayered semantic-aware denoising process to inject control-\nlability at object, scene, and background levels with style\nand layout conditions. To augment semantic consistency\nand enhance visual coherence, we incorporate latent blend-\ning and latent smoothing into each denoising step. Enabled\nby additional text-based and image-based style controls,\nour method supports multiple scenarios including painting,\ncomic, and panorama without tuning, and can be easily cus-\ntomized for creative needs.\nFor this kind of image generation task, there are no ded-\nicated datasets or metrics for evaluating generated nontyp-\nical aspect-ratio images for visual storytelling. To fill this\ngap, we introduce a comprehensive dataset and new metrics\nfor content richness and coherence. We also conducted a se-\nries of comparative and ablation studies to demonstrate the\neffectiveness of our framework and each module, outper-\nforming existing methods based on these metrics and sub-\njective user ratings.\nIn summary, our main contributions are:\n\u2022 A framework to generate nontypical aspect-ratio images\nfrom storytelling text with style and layout controls.\n\u2022 A novel semantic-aware denoising process to enhance the\ncoherence and controllability in generated images.\n\u2022 A dedicated benchmark and metrics to evaluate nontypi-\ncal aspect-ratio image generation for visual storytelling.\n2. Related Work\n2.1. Controllable Generation with Diffusion Models\nDiffusion models, such as DDPM [14] and DDIM [43],\nhave proved effective in various image generation tasks [9],\nespecially with different forms of conditional control [5].\nRecent work integrates multiple concept, style, and layout\nconditions into the generative process, which provides in-\nsight into enhancing the controllablility in content genera-\ntion for visual storytelling.\nConcept and Style Control. Among them, LoRA [15] fo-\ncuses on domain adaptation, Textual Inversion [10] repre-\nsents user-provided concepts as pseudo-words, and Dream-\nBooth [40] introduces a unique identifier for diffusion\nmodel personalization.\nSpecifically, methods like Style-\nDrop [42], StyleAdapter [49], and Ip-Adapter [52] target\nthe modification of stylistic attributes.\nComposition and Editing. To deal with multiple concepts\nin image generation, there have been composable diffu-\nsion models [24] and models like Attend-and-Excite [3],\nwhich refine cross-attention units for semantically accu-\nrate images. Other methods such as Prompt2Prompt [13],\nSDEdit [31], and Imagic [18] focus on image editing\nthrough textual prompts. DragGAN [35] also introduces an\ninteractive approach for editing generated images.\nSpatial and Layout Control. Research on controllable im-\nage generation often focuses on adjusting attention layers\nto control spatial relationships. Representative methods in-\nclude GLIGEN [22], DirectedDiffusion [25], and LayoutD-\niffusion [55]. Additionally, LayoutDM [16] conducts diffu-\nsion on a discrete state-space with layout constraints, Lay-\noutLLM [36] employs a coarse-to-fine paradigm, and Zest-\nGuide [6] introduces a mechanism for conditioning on free-\nform text. ControlNet [53] and Uni-ControlNet [54] pro-\npose unique convolution layers for robust, efficient training\nand task-specific condition learning.\nWhile these methods support specific types of control,\nthey have not been effectively combined to achieve com-\nplex visual narratives, which is a crucial need in storytelling\nscenarios. Meanwhile, few of them support refined control\nover large images, especially with nontypical aspect ratios.\n2.2. Story Visualization\nStory visualization is an increasingly popular task that\nhas emerged in recent years following the rise of genera-\ntive AI. Methods like StoryGAN [21] and its subsequent\nworks [28, 30, 38, 44] aim to produce a sequence of im-\nages that are semantically aligned with a provided series\nof text. In contrast to the focus of text-to-video methods\n[26, 27, 29, 41, 47, 50], this approach emphasizes less the\ncoherence between generated images per frame but more on\nthe overall consistency across dynamic scenes and charac-\nters. Despite these achievements, story visualization with\nimage sequences still suffers from issues including low vi-\nsual quality, limited controllability, and lack of content rich-\nness.\nMeanwhile, nontypical aspect-ratio images have the\npotential to represent rich visual storytelling content as\nshown in multiple panorama generation tasks.\nFor ex-\nample, PanoGen [20] employs image outpainting to gen-\nerate images with nontypical aspect ratios.\nMultiDiffu-\nsion [2] strengthens the continuity of generated images by\noptimizing multiple diffusion generation processes, bind-\ning them with a set of shared parameters or constraints.\nScaleCrafter [12] combines dispersed convolutions and un-\nguided classifiers to produce images with arbitrary aspect\nratios. However, these methods typically lack the control-\nlability required in visual storytelling, as the generated con-\ntent needs to be consistent with the narrative and convey\nemotions through multiple coherent and stylized scenes. To\naddress these issues, we introduce a new framework to gen-\nerate nontypical aspect-ratio images with coherent and en-\ngaging content, aiming for better visual storytelling.\n3. Method\n3.1. Overview\nFigure 2 shows the pipeline of our method, which is mainly\ncomprised of three parts:\nGPT-based layout prediction,\nsemantic-aware denoising, and text-based and image-based\nstyle control. In a typical story-to-scroll generation pro-\ncess, the textual inputs undergo scene segmentation and ob-\nject extraction, before being converted to a series of latent\ncodes with specific concepts, objects, and styles. From an\ninitial reference image or random noise, the model uses a\nsemantic-aware denoising process including temporal latent\nblending and spatial latent smoothing, gradually morphing\nit into a coherent and engaging image while maintaining the\ntarget style, content, and layout.\n3.2. GPT-Based Layout Prediction\nA challenge in text-to-image generation tasks is to under-\nstand and predict reasonable spatial relationships. There-\nfore, we leverage GPT\u2019s prior knowledge of understanding\nsemantic and spatial relationships for layout prediction.\nFirst, we separate a sequence of story text into distinct\nscenes (similar to a movie storyboard) and extract key ob-\njects belonging to each scene, which lays a foundation for\nthe following generative process. We realize the function\nby prompting a GPT-3.5 [34] model with requirements and\noutput data format. However, our experiments showed that\nthe results of contextual and affective understanding varied\ngreatly across different people and GPT trials. So we focus\non the key scenes and objects within textual input and only\ninfer text genre (e.g., heroic story) and image styles (e.g.,\ncomic strip) to facilitate the generative process.\nNext, inspired by LayoutLLM [36], we use GPT to pro-\nduce possible bounding boxes of both scenes and objects to\naccomplish open-domain layout generation, aiming to ef-\nfectively convey the semantic details present in the input\ntext across various hierarchical levels. To enhance the capa-\nbility of the large language model in accurately predicting\nlayouts, we employ visual instruction tuning [23], a pro-\ncess of providing comprehensive instructions with sample\nanswers to facilitate a better understanding of the task. As\ndemonstrated by our results (Figure 4), the layout predictor\ncan significantly enhance the hierarchy and controllability\nof semantic structures in the generated image, a key aspect\nof visual storytelling.\n3.3. Semantic-Aware Denoising\nThe Latent Diffusion Model [39] proposes an image gener-\nation method with a diffusion process in the latent space Z.\nSpecifically, it uses an encoder E to encode a pixel image x\ninto low-resolution latents z = E(x). During training, the\nmodel optimizes a denoising U-Net \u03f5\u03b8 by eliminating arti-\nficial noise, conditioned on both text prompt embedding y\nand current image sample zt, which is a noisy sample of z0\nat step t \u2208 [0, T]):\nmin\n\u03b8\nEz0,\u03f5\u223cN(0,I),t \u2225\u03f5 \u2212 \u03f5\u03b8 (zt, t, y)\u22252\n2 .\n(1)\nAfter training, it is capable of converting random noise \u03f5 to\nan image sample z by the learned denoising process.\nLatent Smoothing.\nSince coherence and story integrity\nare key to generating visual storytelling content, we design\na latent smoothing method to better concatenate different\nmasks at the same level (e.g., adjacent scenes). Our pro-\nposed method is a combination of sliding window and edge\nmatrix to improve spatial coherence.\nWe adopt a similar method as MultiDiffusion [2] to iter-\nate over the whole image with a series of intersecting slid-\ning windows Fi(J), i \u2208 [1, N], where F is a mapping from\nnon-typical aspect-ratio image J \u2208 RH\u2032\u00d7W \u2032\u00d7C to the stan-\ndard diffusion paths I \u2208 RH\u00d7W \u00d7C within each sliding\nwindow. As a result, the new denoising process \u2126 at step\nStory text\n*User editable\nVAE Encoder\nImage embeddings\nNoise*\nImage size\nContext & style\nText content:\npoetry\nInput\nSemantic-Aware Denoising\nImage style:\nChinese painting\nLatent blending\nf(bg, mg, fg, t)\n  = 1\n(T-1) \nStyles &\nvisual effects\nParameter space\nf(x), t, {s }\nk\n \nScene segmentation\nand object extraction*\nScene 4\nObject I\nObject J\nScene 2\nObject D\nObject E\nObject F\nScene 1\nObject A\nObject B\nObject C\n\u201cClimbing the high city \nwall, a thousand miles \nof sorrow. Reeds and \nwillows resemble...\u201d\nArbitrary\nGPT-Based Layout Prediction\nDenoising\nU-Net\nLatent smoothing\nfg\nmg\nbg\n  = 0\nObject layout prediction*\nScene layout prediction*\nS1\nS2\nS3\nS4\nD\nS1\nE\nF\nScene 3\nObject G\nObject H\n{s }\nk\nLayout-\nLLM\nt\nt\nM\nH \u2019\nW\u2019\nH \nW\nText-Based \nStyle Control\nCLIP encoder\nTextual inversion \nCompel\nReference image (optional)\nImage-Based Style Control\nFigure 2. Pipeline of our method. MagicScroll mainly consists of GPT-based layout prediction, semantic-aware denoising, as well as\ntext-based and image-based style control. The layout prediction module extracts bounding boxes of scenes and objects from the story\ntext. The style control modules determine the stylistic attributes via encoding the text prompt and reference image. The semantic-aware\ndenoising module includes latent blending and smoothing to generate controllable, coherent, and engaging nontypical aspect-ratio images.\nt is achieved by the following optimization problem:\n\u2126(Jt, y) =\narg min\nJ\u2208RH\u2032\u00d7W \u2032\u00d7C\nN\nX\ni=1\n\u2225Fi(J) \u2212 \u03a6(Fi(Jt), y)\u22252\n2 ,\n(2)\nwhere \u03a6 denotes a standard denoising step in RH\u00d7W \u00d7C.\nWe found out that different strides s of the sliding window\nnot only affect the number of sliding windows N:\nN = (H\u2032 \u2212 H\ns\n+ 1)(W \u2032 \u2212 W\ns\n+ 1),\n(3)\nbut also reflect different levels of detail in semantics, thus\nwe select a series of coarse-to-fine strides {sk}. Finally,\neach denoised window is multiplied by an edge matrix M \u2208\nRH\u00d7W \u00d7C using the Hadamard product:\nF \u2032\ni(J) = Fi(J) \u2299 M.\n(4)\nThis modification adjusts latent weights based on their\ndistance from the edge. Our experiments discover that while\nthis operation can significantly enhance smooth transitions,\nedge matrices using linear, cosine, or Gaussian functions\nmay achieve the best results in different scenarios.\nLatent Blending. Another aspect of coherence is in the\ntransition between different layers inclduing foreground ob-\njects, midground scenes, and background styles. To solve\nthe problem, we introduce a latent blending method to map\nthe multi-level latents to a time series for content-style\nblending and visual effect control.\nTo preserve spatial information, we use latents at three\ndifferent levels bg, mg, fg \u2208 RH\u2032\u00d7W \u2032\u00d7C\u00d7L, where L is\nthe latent dimension. For each timestep t, we calculated its\nspecific prompt embedding to update the global y in Eq. 2:\ny(i, t) = f(bg, mg, fg, t)\n(5)\nBased on controlled experiments, we discovered that\nconditioning on objects and scenes in the generation process\nhas different impacts on the outcome. While both enhanc-\ning the narrative aspect of visual storytelling, the former fo-\ncuses more on concrete semantics, and the latter contributes\nmore to coherent visuals, reflecting a compromise between\nsemantic details and artistic styles in the creative process.\nBased on previous two-layer generation methods [22], we\nuse a latent blending function f that is foreground for the\nfirst 15% and midground for the remaining 85% denoising\nsteps, while alternating between foreground/midground and\nbackground latents, which empirically yields better results\nas shown in Figure 4.\n3.4. Style Control Modules\nFor visual storytelling with nontypical aspect-ratio images,\na major challenge is generating coherent and stylized con-\ntent. To address this, we design style control modules based\non both text and image encoding.\nText-Based Style Control.\nFor text prompt encoding,\nwe integrate a Textual Inversion [10] module and a Com-\npel [45] module to the basic CLIP [37] text encoder, to\nbetter incorporate and enhance different content and style\nconcepts. Following previous research [1, 8, 51] on sepa-\nrating high-level concepts and low-level visuals, we adopt\nboth textual inversion and image captioning, to better learn\nfrom the images in our dataset. Specifically, we first derive\nscene and object labels from our reference datasets with an\nOFA [46] model, and then train Textual Inversion models\nfor styles and concepts separately.\nTo better control target objects to appear at specified lo-\ncations, we propose an original method to combine tex-\ntual inversion, mask, and Compel on different image lay-\ners. Specifically, users may use operators to enhance or\nreduce certain content or style in the text prompt. If not\nspecified, our model will also strengthen the mask content\nin the scene-level text prompt to facilitate generating target\nobjects. Our results show that text-based style control is ef-\nfective in helping users generate desired objects, introduce\nspecific content and styles, and exert more precise control.\nImage-Based Style Control. As one of the user-editable\nmodules, we allow users to input a reference image of dif-\nferent types (e.g., segmentation map, sketch, and contour)\nin an arbitrary aspect ratio. Inspired by SDEdit [31], we\npropose a process that extends the color and layout of the\nreference image to our output. Specifically, we first pass\nit through a variational autoencoder [19] to obtain an ini-\ntial latent representation and add Gaussian noise to obtain\na noisy latent, simulating a reverse Stochastic Differential\nEquation (SDE).\nFigure 1 shows some results generated with a reference\nimage. Combined with previous modules, ours not only\nmimics the styles of specific genres or painters but also\nachieves coherent style transitions within the same image\nwhile maintaining the original layout and colors. This en-\nhances the controllability of our method, making the results\nmore consistent with the narrative and aligning better with\nuser expectations.\n4. Evaluation\nIn this section, we provide a benchmark covering three typ-\nical scenarios for visual storytelling: painting, comic, and\ncinematic panorama.\nWe also designed two metrics for\nevaluating the degree of content richness and diversity in\na single image. Finally, we conducted a comparative study\nand an ablation study to evaluate the performance of Mag-\nicScroll and its each module.\n4.1. Benchmark\nDatasets.\nOur benchmark includes a self-curated paint-\ning dataset and two refined datasets of comics and cine-\nmatic panoramas. All collected data have been filtered and\npost-processed to better evaluate the quality of visual story-\ntelling. Below we provide their detailed descriptions:\n\u2022 Painting (Traditional Chinese Landscape Paintings).\nThis dataset contains 130 pairs of ancient Chinese poetry\nand corresponding ancient Chinese paintings. The data\nare collected from The Palace Museum website [33].\n\u2022 Comic (Multiple Comic Strips). This dataset contains\n100 comic images with masks for panels, balloons, and\ntext. The data are downloaded from eBDtheque [7].\n\u2022 Cinematic Panorama (Movie Scripts). The dataset con-\ntains more than 6000 rows of annotated movie script data.\nThe data are collected from Movie Scripts Corpus [17].\nMetrics.\nBased on the needs in visual storytelling and\nchallenges in nontypical aspect-ratio image generation, we\nmainly focus on controllability, coherence, and narrativity\nin the evaluation process. To better quantify the outcomes,\nwe first select some indicators for diffusion-based text-to-\nimage tasks from different perspectives:\n\u2022 Text-Image Similarity: The similarity between input\ntext and output image (e.g., CLIP Score, CLIP Directional\nSimilarity, CLIP R-Precision).\n\u2022 Image Quality: Calculated based on generated results\nand ground truths, typically concerning fidelity and di-\nversity, e.g., Fr\u00b4echet Inception Distance (FID) for large\ndatasets, Inception Score (IS) for small datasets, PSNR\nfor reconstruction.\n\u2022 Image Aesthetics: Calculated from generated image and\npre-trained priors, to evaluate from an artistic perspective,\ne.g., LAION-AI aesthetic predictor.\nNevertheless, there is a lack of metrics specifically de-\nsigned to reflect the storytelling aspect in nontypical aspect-\nratio images. For example, the local correspondence be-\ntween the input story and output scroll cannot be measured\nby a global CLIP score, as certain semantics may be diluted\nin the text encoding process. Meanwhile, some other met-\nrics are calculated globally or set-wise, such as content rich-\nness (by IS) and semantic coherence (by self-correlation or\ncolor gradient), which are also not ideal for reflecting local\ninformation. In light of this, we propose two metrics below:\n\u2022 Local-Global Image Scores (LGIS): Based on different\nscenes in a visual storytelling image, we calculate their\nrespective CLIP similarity to the whole image and aver-\nage them to get the result.\n\u2022 Global Embedding Variance (GEV): We derive the em-\nbeddings of the whole image and different scenes and cal-\nculate their variance to represent semantic dispersion.\nThese two metrics reflect the content richness and di-\nversity of image latents from a local-global perspective.\nBesides, we also focus on the transition smoothness be-\ntween different scenes and measure their LAION-AI aes-\nthetic (Edge Aesthetics, EA) as an indicator. To sum up,\nour final metrics are CLIP-score, CLIP similarity to ground\ntruth, and newly proposed LGIS, GEV, and EA.\n4.2. Comparisons\nExperimental Setup.\nWhile there is a lack of dedi-\ncated methods that generate nontypical aspect-ratio images\nfor visual storytelling, some previous methods focused on\npanorama generation, image conditioning, or layout con-\ntrol. We selected a few representative models and also in-\ncluded a baseline model for comparison.\nSpecfically, we collected the public versions of Mul-\ntiDiffusion [2], ScaleCrafter [12], ControlNet [53], GLI-\nGEN [22], and Stable Diffusion [39].\nTo minimize the\nvariance brought by different pre-trained models, we used\nversion 2.1 for all diffusion models except those with only\none adapted version. To better align the capabilities of each\nmodel, we standardized the output of our experiments to\n2048\u00d7512 images, generated from story texts and a com-\nmon hand-drawn image as reference.\nQualitative Results. Some example results are shown in\nFigure 3.\nAs expected, previous methods mainly suffer\nfrom a lack of content diversity, constantly generating repet-\nitive content in different regions of the image. However, by\nGPT-based layout prediction, our method can retain the in-\nherent chronology information of story text, as well as em-\nphasize the key objects and scenes.\nOn the other hand, many critical needs in visual sto-\nrytelling cannot be easily achieved with previous models,\nincluding object positioning, layout formatting, and color\nand style transition. In contrast, by integrating text-based\nand image-based style control into our novel denoising pro-\ncess, we can easily introduce specific layouts, concepts, and\nstyles, ultimately achieving better narrative effects. Figure 5\nshows more results.\nQuantitative Results. Tables 1 and 2 report the automatic\nmetrics aggregated for each model over the three different\ndatasets. For the models that support generating nontypical\naspect-ratio images, we focus on their content richness and\nsemantic correspondence to ground truth. We also com-\npare MagicScroll to GLIGEN, a previous state-of-the-art\nmethod for layout control, in both transition smoothness\nand text-image correspondence on the entire image and sep-\narate scenes.\nThe statistics in Table 1 show that our model performs\nthe best in both three aspects. In terms of similarity to the\nground truth, results generated by Stable Diffusion are sec-\nond to ours. This indicates that despite expanding the image\nsize, other methods did not contribute significantly to the\nalignment between story texts and visuals. In contrast, our\nframework can better grasp such human perceptual patterns\nwhich may not be well reflected in the global CLIP Score,\nbut are very important in visual storytelling.\nOn the other hand, from the LGIS and GEV metrics, our\nmethod is a top performer in generating more rich and di-\nverse content. This also better caters to the need for visual\nstorytelling scenarios to incorporate more information into\na single image. Interestingly, our results suggest that in gen-\nerating nontypical aspect-ratio images, the two objectives of\n\u201ccloser to the ground truth\u201d and \u201ccontaining more informa-\ntion\u201d are not conflicting; rather, focusing on one of them\nmay help achieve the other.\nUser Evaluation.\nIt is commonly acknowledged that\nuser studies are essential in evaluating AI-generated con-\ntent [48]. To evaluate visual outcomes produced by Mag-\nicScroll, we focused on three important dimensions: text-\nimage consistency, visual coherence, and storytelling en-\ngagement. Specifically, users were asked to evaluate which\nof the generated images 1) is visually more coherent, 2) bet-\nter matches the story, and 3) achieves more effective, ex-\nCSGT\u2191\nLGIS\u2193\nGEV\u2191\nMultiDiffusion\n0.76\n0.57\n0.051\nScaleCrafter\n0.74\n0.62\n0.047\nControlNet\n0.67\n0.57\n0.053\nStableDiffusion\n0.78\n0.60\n0.049\nOurs\n0.80\n0.53\n0.057\nTable 1. Results of end-to-end evaluation. This table provides\ncomparative statistics of content richness and fidelity on the paint-\ning dataset. CSGT: CLIP Similarity to Ground Truth.\nEA\u2191\nLocal CLIP\u2191\nGlobal CLIP\u2191\nGLIGEN\n2.42\n20.35\n26.35\nOurs\n2.72\n20.40\n26.69\nTable 2.\nComparison with GLIGEN. Both GLIGEN and our\nmethod support layout control. Images generated by our method\nhave better coherence and higher text-image similarity.\npressive, and engaging storytelling.\nFigure 6 shows 31 users\u2019 preferences for 25 nontypi-\ncal aspect-ratio images generated by MagicScroll or other\nfive methods. We collected 15 ratings for each method we\ncompared against. The results demonstrate that our method\nnot only supports better story-image alignment and visual\ncoherence but also excels in immersing audiences in the\nnarrative. From user feedback, the highest-rated features\nof MagicScroll include support for different scenarios, fast\nprototyping, intuitive editing, and refined control.\n4.3. Ablation Study\nExperimental Setup. We conducted an ablation study on\nthe three main modules of our whole pipeline and two\ndifferent inputs in the semantic-aware denoising process.\nWe follow a similar experimental setup to the comparative\nstudy. The experimental groups are: w/o text-based style\ncontrol, w/o image-based style control, w/o latent smooth-\ning, w/o object mask, and w/o scene mask.\nResults. Table 3 shows the results of our ablation study.\nThe results demonstrate the effectiveness of each module\nand their contributions to the final outcome from different\nperspectives. For example, text-based style control, scene\nmasks, and object masks contribute to the content richness\nand similarity to ground truth. Meanwhile, each of the five\nmodules is effective in enhancing the smooth transition be-\ntween different scenes as well as text-to-image alignment.\n4.4. Discussion\nStrengths. Our framework exemplifies several core advan-\ntages corresponding to the key facets of visual storytelling,\nincluding content richness, controllability, and coherence.\nThrough a novel semantic-aware denoising process, Mag-\nOurs\nControlNet\nScaleCrafter\nMultiDiffusion\nStableDiffusion\n\u201cThe cavalry are on an expedition to slay the dragon\u2026\u201d\n\u201cDwelling in the mountains, I make friend with trees\u2026\u201d\u201cTwo young heroes were assigned a combat mission\u2026 \u201d\nFigure 3. Qualitative comparison of our methods with other baselines. The results demonstrate the high versatility and core advantages of\nMagicScroll. (a) Style mimicry in specific historical contexts. (b) Semantic layout planning and control. (c) Content richness and diversity.\nAmong different scenarios, our framework can achieve more effective and engaging visual storytelling.\n(a) Full\n(b) w/o text-based style control\n(c) w/o image-based style control\n(d) w/o latent smoothing\n(e) w/o object mask\n(f) w/o scene mask\n\u201cThe past drifts away, leaving only an ancient tower under sunset. Mountains rise in the distance, trees are full of the island. Rafting on the river makes me nostalgic.\u201d \nFigure 4. Example results of the ablation study. Red boxes indicate artifacts. Among the modules, (b) text-based style control and (c)\nimage-based style control mainly affects the overall style and color. (d) Latent smoothing can improve visual coherence. (e) Object maks\nand (f) scene masks not only affect image layout but also significantly influence the consistency between foreground and midground.\nCSGT\u2191\nLGIS\u2193\nGEV\u2191\nEA\u2191\nCLIP\u2191\n-TBSC\n0.73\n0.59\n0.052\n2.54\n26.62\n-SM\n0.76\n0.54\n0.056\n2.41\n26.24\n-OM\n0.78\n0.53\n0.056\n2.54\n26.38\n-LS\n0.79\n0.53\n0.056\n2.55\n26.57\n-IBSC\n0.80\n0.53\n0.057\n2.67\n26.59\nFull\n0.80\n0.53\n0.057\n2.72\n26.69\nTable 3. Results of ablation study. This table lists statistics in the\nablation study on the painting dataset. CLIP: Global CLIP score,\nTBSC: text-based style control, IBSC: image-based style control,\nSM: scene mask, OM: object mask, LS: latent smoothing.\nicScroll can unfold a story into a nontypical aspect-ratio\nimage, preserving its key scenes and objects, featuring rich\ncontent and smooth transitions. To leverage the narrative ef-\nficacy of images, we also explored the integration of differ-\nent conditions during the generation process, including var-\nious aspects (style, concept, layout), different input modali-\nties (reference image, text, mask), and multiple layers (fore-\nground, midground, background). MagicScroll is a step for-\nward toward controllable generation and panorama synthe-\nsis, paving the way for more generalizable frameworks.\nLimitations. Despite the strengths, there are still some lim-\nitations and possible future directions to explore. For ex-\nample, in processing stories, we may apply tokenizers and\nencoders better suited for ultra-long texts [4]. During the\ngeneration process, we may exert more conditional controls\nFigure 5. More example results generated by MagicScroll. By providing control over style, concept, and layout at all foreground,\nmidground, and background levels, our framework can meet the needs of visual storytelling content generation in various scenarios. Full\ntext prompts can be found in the supplementary material.\nSD\nMD\nSC\nCN\nGLIGEN\n0%\n20%\n40%\n60%\n80%\n100%\nCoherence\nSD\nMD\nSC\nCN\nGLIGEN\nText-Image Consistency\nSD\nMD\nSC\nCN\nGLIGEN\nStorytelling Engagement\nOurs\nOther Methods\nFigure 6. User study results. We compared images generated from\ndifferent models on text-image consistency, coherence, and story-\ntelling engagement. The results show that our framework outper-\nforms other alternative methods in all three aspects.\nat various stages or include some pre-trained extra mod-\nules [11, 32]. We can also learn a mapping between im-\nage features and our parameter space, which may further\nenhance controllability over visual effects.\n5. Conclusion\nIn this paper, we propose MagicScroll, a framework for gen-\nerating controllable, coherent, engaging nontypical aspect-\nratio images for visual storytelling. The pipeline is designed\nto convert a story to an image with a coherent plot unfold-\ning, focusing on critical scenes and objects. We combine\nvarious methods to exert control from different aspects, in\nmultiple layers, conditioned on various modalities. With a\nclear definition of the task, we provided a benchmark and\nnew metrics tailored for evaluating the quality of generated\nnontypical aspect-ratio images for storytelling.\nThrough\nqualitative and quantitative evaluation, we show that Mag-\nicScroll outperforms alternative methods, achieving high\nquality similar to ground truth, as well as visual coherence\nand engagement with the audience.\nSocietal Impact. Our endeavor not only advances the fron-\ntier of controllable generation and visual storytelling but\nalso contributes to a broader discussion stemming from pro-\nduction requirements and industrial demands. We look for-\nward to further generalizing the task and framework and ex-\npanding the benchmark and metrics for different visual sto-\nrytelling scenarios, so we can enhance the synergy between\ncreative practitioners and AI researchers.\nReferences\n[1] Tenglong Ao, Zeyi Zhang, and Libin Liu.\nGestureDiffu-\nCLIP: Gesture Diffusion Model with CLIP Latents. arXiv\npreprint arXiv:2303.14613, 2023. 4\n[2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\nMultiDiffusion: Fusing Diffusion Paths for Controlled Im-\nage Generation. arXiv preprint arXiv:2302.08113, 2023. 3,\n5\n[3] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or. Attend-and-Excite: Attention-Based Se-\nmantic Guidance for Text-to-Image Diffusion Models. ACM\nTrans. Graph., 42(4):1\u201310, 2023. 2\n[4] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian\nLiu, Song Han, and Jiaya Jia. LongLoRA: Efficient Fine-\ntuning of Long-Context Large Language Models, 2023. 7\n[5] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune\nGwon, and Sungroh Yoon. ILVR: Conditioning Method for\nDenoising Diffusion Probabilistic Models.\narXiv preprint\narXiv:2108.02938, 2021. 2\n[6] Guillaume Couairon,\nMarl`ene Careil,\nMatthieu Cord,\nSt\u00b4ephane Lathuili`ere, and Jakob Verbeek. Zero-Shot Spatial\nLayout Conditioning for Text-to-Image Diffusion Models. In\nInt. Conf. Comput. Vis., pages 2174\u20132183, 2023. 2\n[7] Universit\u00b4e de La Rochelle.\ne-BDth`eque.\nhttps://\nebdtheque.univ-lr.fr/, 2023. Accessed: Novem-\nber 2, 2023. 5\n[8] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,\nYin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.\nNeRDi:\nSingle-View NeRF Synthesis with Language-\nGuided Diffusion as General Image Priors. In IEEE Conf.\nComput. Vis. Pattern Recog., pages 20637\u201320647, 2023. 4\n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion Models\nBeat GANs on Image Synthesis. Adv. Neural Inform. Pro-\ncess. Syst., 34:8780\u20138794, 2021. 2\n[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or.\nAn Image Is Worth One Word:\nPersonalizing Text-to-\nImage Generation Using Textual Inversion. arXiv preprint\narXiv:2208.01618, 2022. 2, 4\n[11] Cusuh Ham, James Hays, Jingwan Lu, Krishna Kumar\nSingh, Zhifei Zhang, and Tobias Hinz.\nModulating Pre-\ntrained Diffusion Models for Multimodal Image Synthesis.\narXiv preprint arXiv:2302.12764, 2023. 8\n[12] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun,\nMenghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng\nChen, and Ying Shan. ScaleCrafter: Tuning-Free Higher-\nResolution Visual Generation with Diffusion Models. arXiv\npreprint arXiv:2310.07702, 2023. 3, 5\n[13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-Prompt Im-\nage Editing with Cross Attention Control.\narXiv preprint\narXiv:2208.01626, 2022. 2\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Dif-\nfusion Probabilistic Models. Adv. Neural Inform. Process.\nSyst., 33:6840\u20136851, 2020. 2\n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-Rank Adaptation of Large Language Models.\narXiv preprint arXiv:2106.09685, 2021. 2\n[16] Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu\nOtani, and Kota Yamaguchi. LayoutDM: Discrete Diffusion\nModel for Controllable Layout Generation. In IEEE Conf.\nComput. Vis. Pattern Recog., pages 10167\u201310176, 2023. 2\n[17] Kaggle. Movie Scripts Corpus. https://www.kaggle.\ncom / datasets / gufukuro / movie - scripts -\ncorpus, 2023. Accessed: November 2, 2023. 5\n[18] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-Based Real Image Editing with Diffusion Models. In\nIEEE Conf. Comput. Vis. Pattern Recog., pages 6007\u20136017,\n2023. 2\n[19] Diederik P Kingma and Max Welling. Auto-Encoding Vari-\national Bayes. arXiv preprint arXiv:1312.6114, 2013. 5\n[20] Jialu Li and Mohit Bansal.\nPanoGen: Text-Conditioned\nPanoramic\nEnvironment\nGeneration\nfor\nVision-and-\nLanguage Navigation.\narXiv preprint arXiv:2305.19195,\n2023. 3\n[21] Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng,\nYuexin Wu, Lawrence Carin, David Carlson, and Jianfeng\nGao. StoryGAN: A Sequential Conditional GAN for Story\nVisualization. In IEEE Conf. Comput. Vis. Pattern Recog.,\npages 6322\u20136331, 2019. 3\n[22] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-Set Grounded Text-to-Image Generation. In\nIEEE Conf. Comput. Vis. Pattern Recog., pages 22511\u2013\n22521, 2023. 2, 4, 5\n[23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual Instruction Tuning. arXiv preprint arXiv:2304.08485,\n2023. 3\n[24] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and\nJoshua B Tenenbaum.\nCompositional Visual Generation\nwith Composable Diffusion Models. In Eur. Conf. Comput.\nVis., pages 423\u2013439. Springer, 2022. 2\n[25] Wan-Duo Kurt Ma, JP Lewis, W Bastiaan Kleijn, and\nThomas Leung. Directed Diffusion: Direct Control of Ob-\nject Placement through Attention Guidance. arXiv preprint\narXiv:2302.13153, 2023. 2\n[26] Yue Ma, Yali Wang, Yue Wu, Ziyu Lyu, Siran Chen, Xiu Li,\nand Yu Qiao. Visual knowledge graph for human action rea-\nsoning in videos. In Proceedings of the 30th ACM Interna-\ntional Conference on Multimedia, pages 4132\u20134141, 2022.\n3\n[27] Yue Ma, Tianyu Yang, Yin Shan, and Xiu Li. Simvtp: Sim-\nple video text pre-training with masked autoencoders. arXiv\npreprint arXiv:2212.03490, 2022. 3\n[28] Yue Ma, Xiaodong Cun, Yingqing He, Chenyang Qi, Xin-\ntao Wang, Ying Shan, Xiu Li, and Qifeng Chen.\nMagic-\nstick: Controllable video editing via control handle transfor-\nmations. arXiv preprint arXiv:2312.03047, 2023. 3\n[29] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying\nShan, Xiu Li, and Qifeng Chen.\nFollow your pose:\nPose-guided text-to-video generation using pose-free videos.\narXiv preprint arXiv:2304.01186, 2023. 3\n[30] Adyasha Maharana, Darryl Hannan, and Mohit Bansal.\nStoryDALL-E: Adapting Pretrained Text-to-Image Trans-\nformers for Story Continuation. In Eur. Conf. Comput. Vis.,\npages 70\u201387. Springer, 2022. 3\n[31] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun\nWu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided Im-\nage Synthesis and Editing with Stochastic Differential Equa-\ntions. arXiv preprint arXiv:2108.01073, 2021. 2, 5\n[32] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2I-Adapter: Learning\nAdapters to Dig out More Controllable Ability for Text-to-\nImage Diffusion Models. arXiv preprint arXiv:2302.08453,\n2023. 8\n[33] The Palace Museum.\nMinghua Ji.\nhttps : / /\nminghuaji.dpm.org.cn/, 2023. Accessed: Novem-\nber 2, 2023. 5\n[34] OpenAI.\nModel Index for Researchers.\nhttps://\nplatform . openai . com / docs / model - index -\nfor-researchers, 2023. Accessed: November 2, 2023.\n3\n[35] Xingang Pan, Ayush Tewari, Thomas Leimk\u00a8uhler, Lingjie\nLiu, Abhimitra Meka, and Christian Theobalt. Drag Your\nGAN: Interactive Point-Based Manipulation on the Genera-\ntive Image Manifold. In ACM SIGGRAPH 2023 Conference\nProceedings, pages 1\u201311, 2023. 2\n[36] Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, and Tat-\nSeng Chua.\nLayoutLLM-T2I: Eliciting Layout Guidance\nfrom LLM for Text-to-Image Generation.\narXiv preprint\narXiv:2308.05095, 2023. 2, 3\n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\nTransferable Visual Models from Natural Language Supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 4\n[38] Tanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov,\nShweta Mahajan, and Leonid Sigal. Make-A-Story: Visual\nMemory Conditioned Consistent Story Generation. In IEEE\nConf. Comput. Vis. Pattern Recog., pages 2493\u20132502, 2023.\n3\n[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-Resolution Image\nSynthesis with Latent Diffusion Models.\nIn IEEE Conf.\nComput. Vis. Pattern Recog., pages 10684\u201310695, 2022. 3,\n5\n[40] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\nTuning Text-to-Image Diffusion Models for Subject-Driven\nGeneration.\nIn IEEE Conf. Comput. Vis. Pattern Recog.,\npages 22500\u201322510, 2023. 2\n[41] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-A-Video: Text-to-Video Generation\nwithout Text-Video Data. arXiv preprint arXiv:2209.14792,\n2022. 3\n[42] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro\nChin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,\nGlenn Entis, Yuanzhen Li, et al. StyleDrop: Text-to-Image\nGeneration in Any Style. arXiv preprint arXiv:2306.00983,\n2023. 2\n[43] Jiaming Song, Chenlin Meng, and Stefano Ermon.\nDe-\nnoising\nDiffusion\nImplicit\nModels.\narXiv\npreprint\narXiv:2010.02502, 2020. 2\n[44] Yun-Zhu Song, Zhi Rui Tam, Hung-Jen Chen, Huiao-Han\nLu, and Hong-Han Shuai.\nCharacter-Preserving Coherent\nStory Visualization. In Eur. Conf. Comput. Vis., page 18\u201333,\nBerlin, Heidelberg, 2020. Springer-Verlag. 3\n[45] Damian Stewart.\nCompel.\nhttps://github.com/\ndamian0815/compel, 2023.\nGitHub repository, Ac-\ncessed: November 2, 2023. 4\n[46] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang.\nOFA: Unifying Architectures, Tasks, and\nModalities Through a Simple Sequence-to-Sequence Learn-\ning Framework.\nIn International Conference on Machine\nLearning, pages 23318\u201323340. PMLR, 2022. 4\n[47] Qianqian Wang, Zhengqi Li, David Salesin, Noah Snavely,\nBrian Curless, and Janne Kontkanen.\n3D Moments from\nNear-Duplicate Photos. In IEEE Conf. Comput. Vis. Pattern\nRecog., pages 3906\u20133915, 2022. 3\n[48] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-\nTuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah\nLaszlo, David J Fleet, Radu Soricut, et al. Imagen Editor\nand Editbench: Advancing and Evaluating Text-Guided Im-\nage Inpainting. In IEEE Conf. Comput. Vis. Pattern Recog.,\npages 18359\u201318369, 2023. 6\n[49] Zhouxia Wang, Xintao Wang, Liangbin Xie, Zhongang Qi,\nYing Shan, Wenping Wang, and Ping Luo. StyleAdapter: A\nSingle-Pass LoRA-Free Model for Stylized Image Genera-\ntion. arXiv preprint arXiv:2309.01770, 2023. 2\n[50] Chung-Yi Weng, Brian Curless, and Ira Kemelmacher-\nShlizerman. Photo Wake-Up: 3D Character Animation from\na Single Photo. In IEEE Conf. Comput. Vis. Pattern Recog.,\npages 5908\u20135917, 2019. 3\n[51] Yankun Wu, Yuta Nakashima, and Noa Garcia. Not Only\nGenerative Art: Stable Diffusion for Content-Style Disentan-\nglement in Art Analysis. In Proceedings of the 2023 ACM In-\nternational Conference on Multimedia Retrieval, pages 199\u2013\n208, 2023. 4\n[52] Hu Ye,\nJun Zhang,\nSibo Liu,\nXiao Han,\nand Wei\nYang.\nIP-Adapter:\nText Compatible Image Prompt\nAdapter for Text-to-Image Diffusion Models. arXiv preprint\narXiv:2308.06721, 2023. 2\n[53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nConditional Control to Text-to-Image Diffusion Models. In\nInt. Conf. Comput. Vis., pages 3836\u20133847, 2023. 2, 5\n[54] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin\nBao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K Wong. Uni-\nControlNet: All-in-One Control to Text-to-Image Diffusion\nModels. arXiv preprint arXiv:2305.16322, 2023. 2\n[55] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi,\nYing Shan, and Xi Li. LayoutDiffusion: Controllable Diffu-\nsion Model for Layout-to-Image Generation. In IEEE Conf.\nComput. Vis. Pattern Recog., pages 22490\u201322499, 2023. 2\nMagicScroll: Nontypical Aspect-Ratio Image Generation for Visual Storytelling\nvia Multi-Layered Semantic-Aware Denoising\nSupplementary Material\nFigure 7. More results generated by MagicScroll.\nText prompts for Figure 5.\n\u2022 Chinese painting (left): In a deep mountain enclave, where pines and cypresses thrive, Amidst swirling clouds and mist,\ndistant rocks and peaks arrive. A dwelling stands in solitude, with a simple architectural grace, Beyond bamboo fences,\nchildren play in a joyous embrace. Beneath the trees, two figures converse with laughter and delight, Together forming a\nscene of tranquil seclusion, warm and bright.\n\u2022 Chinese painting (right): In deep mountains, a waterfall descends from great height, Winding its way down, into a crystal\npool, pure and bright. Water droplets dance, a splash of liquid light, In the distance, evergreen pines, in the foreground,\nrocks stand upright. The springwater murmurs, weaving a melody with stones, Surrounded by bushes, a charming scene it\nowns.\n\u2022 Cinematic panorama (top): Amidst the sprawling castles, a troop of cavalry embarks on a quest. They traverse distant\nmountains, weaving through dense forests, passing rivers, castles, and perilous peaks. On the other end awaits a colossal\ndragon and its minions, their forms menacing, teeth bared, claws ready\u2014intent on slaying all who dare to invade.\n\u2022 Cinematic panorama (bottom): Captain David and First Mate Jess set sail for a distant voyage. Their ship gazes upon\nfar-off peaks, navigating through thick veils of mist, braving treacherous reefs. David and Jess stand boldly at the bow,\nunfolding a majestic scene like a grand painting coming to life.\n\u2022 Comic strip (upper middle): Early in the morning, the hero Catherine received a call to save the world. Armed and ready,\nshe joined forces with Jack and headed towards the heart of the city. Gazing at the distant smoke, Jack\u2019s expression turned\nsolemn as he thought of the lives lost. Despite the gravity of the situation, they bravely rescued the city, fulfilling their\nmission. The duo stood victorious, having overcome the challenges in their path.\n\u2022 Comic strip (lower middle): In a fairy tale, there exists a village of toys, adorned with beautiful castles, lakes, and flowers.\nPeople picnic in the fields, row boats on the lake, and celebrate harvest days in the snowy season. In this village, docks,\nfountains, and flower houses are constructed to embellish the tranquil life of its inhabitants.\n"
  },
  {
    "title": "Paloma: A Benchmark for Evaluating Language Model Fit",
    "link": "https://arxiv.org/pdf/2312.10523.pdf",
    "upvote": "11",
    "text": "PALOMA : A BENCHMARK FOR\nEVALUATING LANGUAGE MODEL FIT\nIan Magnusson\u2660\nAkshita Bhagia\u2660\nValentin Hofmann\u2660\nLuca Soldaini\u2660\nAnanya Harsh Jha\u2660\nOyvind Tafjord\u2660\nDustin Schwenk\u2660\nEvan Pete Walsh\u2660\nYanai Elazar\u2660\u2662\nKyle Lo\u2660\nDirk Groeneveld\u2660\nIz Beltagy\u2660\nHannaneh Hajishirzi\u2660\u2662\nNoah A. Smith\u2660\u2662\nKyle Richardson\u2660\nJesse Dodge\u2660\n\u2660Allen Institute for Artificial Intelligence\n\u2662Paul G. Allen School of Computer Science & Engineering, University of Washington\n{ianm,jessed}@allenai.org\nABSTRACT\nLanguage models (LMs) commonly report perplexity on monolithic data held out\nfrom training. Implicitly or explicitly, this data is composed of domains\u2014varying\ndistributions of language. Rather than assuming perplexity on one distribution\nextrapolates to others, PERPLEXITY ANALYSIS FOR LANGUAGE MODEL ASSESS-\nMENT (PALOMA),1 measures LM fit to 585 text domains, ranging from nytimes.com\nto r/depression on Reddit. We invite submissions to our benchmark and organize\nresults by comparability based on compliance with guidelines such as removal of\nbenchmark contamination from pretraining. Submissions can also record parameter\nand training token count to make comparisons of Pareto efficiency for performance\nas a function of these measures of cost. We populate our benchmark with results\nfrom 6 baselines pretrained on popular corpora. In case studies, we demonstrate\nanalyses that are possible with PALOMA, such as finding that pretraining without\ndata beyond Common Crawl leads to inconsistent fit to many domains.\n1\nINTRODUCTION\nProgress in AI is often catalyzed by benchmarks that define new ways of measuring progress (Deng\net al., 2009, Wang et al., 2018, and Wang et al., 2019, inter alia). Language models (LMs) often\nreport LM fit in the form of perplexity (Jelinek et al., 1977) or its logarithm, cross-entropy loss, on\nheld out data from a model\u2019s training distribution or a small number of traditional test sets (Chelba\net al., 2013, and Merity et al., 2016, inter alia). Such measures of LM fit have been shown to improve\npredictably with increases in scale from more parameters, training data, and compute (Kaplan et al.,\n2020; Hoffmann et al., 2022). However, increasingly large training data also aggregates increasing\nnumbers of distinct communities (Diaz & Madaio, 2023) with differing distributions of language, i.e.,\ndomains, that LMs implicitly learn to model (Aharoni & Goldberg, 2020). Does rising performance\nlift all data? Or do some domains capture most improvement in LM fit? How do we evaluate\ndecisions, such as how to compose pretraining data, that determine which distributions of language\nare modeled? We contend that, rather than extrapolating trends from a prescriptive mix of domains,\nbenchmarks ought to measure LM fit to many domains and inspect where fit differs.\nIn this work we introduce PALOMA, a benchmark to study LM fit on many domains. We measure\nperplexity on different distributions of language that we surface by sampling from 18 sources, such\nas C4 (Raffel et al., 2019; Dodge et al., 2021), that have metadata marking 585 textual domains, such\nas URL domains or academic disciplines. Beyond evaluation data, we aim to enable and enrich fair\ncomparisons for scientific research on language modeling with the following artifacts: guidelines for\nexperiments on LM fit, 6 baseline 1B parameter models pretrained on popular corpora, standardized\ninference code, and a submission process for coordinating comparable results across the research\ncommunity.\n1https://paloma.allen.ai/\n1\narXiv:2312.10523v1  [cs.CL]  16 Dec 2023\n20\n65\n150\n12\n15\n20\n28\nC4\n20\n65\n150\n7\n12\n20\nmC4\n20\n65\n150\n5\n15\n88\n2K\n195K\nThe Pile\n20\n65\n150\n12\n15\n20\n28\nWikiText-103\n20\n65\n150\n15\n28\n58\n142\nPTB\n20\n65\n150\n5\n88\n195K\nRedPajama\n20\n65\n150\n13\n17\n24\n33\nFalcon\n20\n65\n150\n12\n20\n39\nDolma V1.5\n20\n65\n150\n12\n15\n20\n28\nM2D2 S2ORC\n20\n65\n150\n12\n15\n20\nM2D2 Wikipedia\n20\n65\n150\n12\n15\n20\n28\nC4 100 Domains\n20\n65\n150\n17\n24\n33\n47\n100 Subreddits\n20\n65\n150\n3\n5\n15\n88\n2K\n100 PLs\nBaselines\nDolma V1.5 1B\nFalcon-RefinedWeb 1B\nRedPajama 1B\nThe Pile 1B\nC4 1B\nmC4 1B\n20\n65\n150\n9\n28\n142\nICE\n20\n65\n150\n142\n238\n424\nTwitter AAE\n20\n65\n150\n20\n28\n39\n58\nManosphere\n20\n65\n150\n33\n47\n71\nGab\n20\n65\n150\n9\n15\n28\n58\n4chan\nMacro Average Perplexity\nTokens Seen (billions)\nFigure 1: Perplexity macro averaged over any domains within each of the 18 top-level data sources\n(\u00a72.2) in PALOMA, using baselines with pretraining controls including decontamination. Evaluating\non one monolithic corpus, such as C4, does not tell the complete story of model fit. PALOMA lets\nus see when trends differ from one distribution of language to another. For instance, the 3 baselines\ntrained on only Common Crawl data (C4, MC4-EN, FALCON REFINEDWEB) exhibit high perplexity,\nsometimes with non-monotonic scaling over tokens seen, on specific evaluation sources such as THE\nPILE, DOLMA, and DOLMA-100-PROGRAMMING-LANGUAGES.\nMore than being a one-dimensional leaderboard, PALOMA offers a suite of fine-grained results\nfrom submissions organized by their comparability. As reproducing pretrained models for every\nnew project is onerous, we provide standard training controls for benchmark decontamination and\ntraining data order to orchestrate a greater density of comparisons across the research community.\nSubmissions opt in to these, or are marked to make limitations to comparability easy to see. We also\ncontrol evaluation (1) by sampling evenly from domains based on an estimate of metric variance\nintroduced by subsampling, (2) by fixing model vocabulary where possible or otherwise using bits\nper byte rather than perplexity to compare, and (3) by standardizing evaluation format. Lastly, we\nalso coordinate fair comparisons over two measures of cost, number of model parameters and training\ntokens, enabling assessment of hardware-agnostic Pareto efficiency and the measurement of scaling\ntrends.\nIn addition to curating stratified subsamples of existing datasets of fine-grained domains (Gao et al.,\n2020; Reid et al., 2022; Chronopoulou et al., 2022; Greenbaum & Nelson, 1996; Blodgett et al., 2016;\nLiang et al., 2022), we contribute new evaluation corpora constructed from held out data from the\nDOLMA pretraining corpus (Soldaini et al., 2023) that subsample the top 100 subreddits and top 100\nprogramming languages. Also, we repurpose corpora of fringe online communities for perplexity\nevaluations to measure model fit to discourse previously studied for the prevalence of toxicity and\nhate speech (Ribeiro et al., 2021; Zannettou et al., 2018; Papasavva et al., 2020)\u2014an important\nconsideration for LMs, as exposure to toxic pretraining trades off negative and positive capabilities\nsuch as toxic generation and classifying toxicity (Longpre et al., 2023). However, different lines\nof research will inevitably require different selections of domains beyond the scope of any one\n2\nbenchmark. In PALOMA we focus on English and code data and aim to assemble the most fine-\ngrained domains readily identifiable from existing metatdata, so we can begin evaluating models over\nstratified samples of hundreds of domains.\nTo demonstrate possible uses of results from our benchmark, we conduct a series of case studies.\nWe show that performance improves in almost all domains as models are scaled, but domains\nimprove unequally. Further, across domains, perplexity is driven by strings in the vocabulary, i.e.,\ntypes, that occur in most domains, but other types even get worse as models scale. Finally, our\nexperiments isolate change in fit from which pretraining corpus is used and find that pretraining\nwithout heterogeneous data sources beyond Common Crawl leads to perplexities that do not improve\nconsistently with number of tokens seen.\n2\nPALOMA\nPERPLEXITY ANALYSIS FOR LANGUAGE MODEL ASSESSMENT (PALOMA) is for examining LM\nfit to domains. We use perplexity (and related metrics; \u00a72.4) to measure fit to the distributions of\nlanguage represented by different domains. We take relative differences in LM fit as a proxy of model\nfamiliarity to the shared knowledge, values, and social context that position the humans producing\nlanguage in a domain. While we expect contemporary LMs to have a limited fit to the most complex\nof these latent factors of textual domains, improving fit to all factors is important both to improve\nperplexity and for actual use of the LM. For example, better perplexity on a particular dialect of\nEnglish suggests that model will make a better chatbot for people that speak that dialect.\nPALOMA comprises several types of artifacts for enabling a science of language modeling: training\nand evaluation guidelines for experiments on LM fit (\u00a72.1), evaluation data for assessing fit to specific\ndomains (\u00a72.2), 6 pretrained baselines following training guidelines (\u00a72.3), metrics computed by\nour standardized inference code conforming to our evaluation guidelines (\u00a72.4), and a submission\nprocess for coordinating comparable results across the research community (\u00a72.5).\n2.1\nGUIDELINES\nWe outline the principles that we adopt for assessing LM fit. To use perplexity as a meaningful\nmeasure of fit to a domain, we must account for factors in both training and evaluation that can\nconfound results. In Table 1 we compare how previous benchmarks of language modeling have\nresponded to these issues. We distinguish these guidelines from the controls that we use to implement\nthese guidelines in PALOMA, the technical details of which we discuss in \u00a73.\nGuideline\nTHE PILE (Gao et al.,\n2020)\nM2D2 (Reid\net al., 2022)\nC4-100-DOMAINS\n(Chronopoulou et al., 2022)\nHELM LM Scenarios\n(Liang et al., 2022)\nPALOMA\nG1 DECONTAMINATION\npartial, doc-level\nnone\nnone\nnot required\nsub-doc-level\nG2 TRAINING ORDER\nnot required\nnot required\nnot required\nnot required\nfixed\nG3 SUBSAMPLING\nuniform\nuniform\nuniform\ninherits splits\nstratified\nG4 VOCABULARY\nnot required\nnot required\nnot required\nnot required\nfixed\nG5 EVALUATION FORMAT\nno concat or overlap\nnot required\nnot required\nAPI dependent\nno concat or overlap\n# Domains\n22\n216\n99\n14\n585\nTable 1: Differences between PALOMA and other language modeling benchmarks with respect to\nguidelines (\u00a72.1) for experiments of assessing LM fit. PALOMA is the first benchmark to remove\ncontamination across all training data, including contamination at the sub-document level. Pile\nonly deduplicates 2 of 22 domains at document level before splitting. PALOMA also fixes training\ndata order, takes a stratified subsample of the same size from each domain based on estimated\nmetric variance, and fixes vocabulary and evaluation format. When experiments require changes\nin vocabulary, bits per byte (\u00a72.4) is compared instead of perplexity, following THE PILE and\nHELM. Also following THE PILE, we use an evaluation format that does not concatenate multiple\ndocuments in a single input and that uses no overlap when splitting documents longer the maximum\nsequence length. HELM\u2019s inference code depends on potentially unknown inference formats used by\nproprietary APIs but is otherwise documented.\n3\n2.1.1\nTRAINING GUIDELINES\nG1 DECONTAMINATION\nRemove pretraining data with sub-document overlap against test data to\nensure validity of perplexity evaluation.\nA basic tenet of machine learning is that for test evaluation to accurately represent performance,\ntraining and test data need to be non-overlapping. However, large pretraining corpora are known\nto contain evaluation data and large models are known to memorize training data (Dodge et al.,\n2021; Elazar et al., 2023; Carlini et al., 2022). Lee et al. (2022) show in their second figure\nthat models underestimate perplexity on evaluation documents with near duplicates in the training\ncorpus by several points relative to models with those duplicate training documents removed. Thus\nbenchmarks of language modeling should actively remove contaminated training data, rather than\njust partitioning held out splits by documents, assuming no documents overlap. THE PILE applies\ndocument-level deduplication to two of their 22 domains before splitting held-out data, but its\ndesigners note that this does not prevent leakage of evaluation data more generally (Gao et al., 2020).\nFurthermore, spans of contaminated text within larger unrelated documents can still contribute to\noverestimation of performance, so decontamination should be conducted at a sub-document level. To\nour knowledge, PALOMA is the first language modeling benchmark to require removing training data\nthat is contaminated with respect to evaluation data.\nG2 TRAINING ORDER\nIf changes in training data order are not examined by an experiment, keep\nthe training data order the same to control differences from recency effects.\nAnother decision that affects language modeling experiments is the order of training documents.\nWhile intentionally designing curricula by ordering training data to improve performance is an area\nof active research (Bengio et al., 2009, inter alia), most LMs simply randomize the training order. In\nthis case greater comparability between experiments with the same dataset can be achieved if the\nsame random order is used for all models. This also facilitates research that examines exactly what\ndata a given model checkpoint has seen or not seen at that point in training. No previous language\nmodeling benchmarks require the fixing of training order.\n2.1.2\nEVALUATION GUIDELINES\nG3 SUBSAMPLING\nBase the size of evaluation data subsamples on empirical estimates of variance\nover subsamples.\nThere is no shortage of text that can be used to estimate perplexity, so we must choose how much\nto evaluate based on a tradeoff of inference cost and metric stability over different subsamples.\nThe value we ultimately care to estimate is the perplexity of the model on all the available data,\nnot just a subsample. Much existing work considers the estimation of other information theoretic\nquantities such as entropy and mutual information (Paninski, 2003 inter alia), so the estimation of\nperplexity should likewise be treated with care, for instance in subsampling evaluation data. Previous\nbenchmarks subsample uniformly over the whole corpus, leaving some domains represented by very\nlittle data. M2D2 mitigates this by an ad hoc minimum size, but this still leads to domains with\ndifferent sizes. PALOMA takes a first step towards controlling for subsampling induced variance in\nperplexity estimation by using a stratified subsample across domains and providing a preliminary\nempirical measure of metric bias and variance extrapolated from one domain.\nG4 VOCABULARY\nIf changes in vocabulary are not examined by an experiment, keep the vocabu-\nlary the same to permit direct comparison on perplexity. If not, use bits per byte (BPB) to normalize\nlikelihood by a segmentation intrinsic to the text.\nPerplexity per token is not comparable between models with different vocabularies (Jelinek, 1998)\nor, by extension, different tokenizers (Mielke, 2019). Since models distribute probability over a\nvocabulary of tokens, models with larger vocabularies will tend to have higher perplexities than ones\nwith smaller vocabularies. Where possible, the most rigorous solution is to impose one vocabulary\non all experiments, allowing perplexity to be directly compared. Some lines of research, such\nas improving tokenizers, require comparisons of LM fit across vocabularies. This is possible by\nnormalizing likelihood by a segmentation intrinsic to the text such as characters or bytes (Mielke,\n2019). THE PILE (Gao et al., 2020) proposes BPB (\u00a72.4) as the best compromise when tokenizers\n4\nare not identical, an approach we adopt as well. PALOMA further establishes a standard tokenizer and\nvocabulary for submissions that do not need to change this experimental variable.\nG5 EVALUATION FORMAT\nEvaluate likelihood in a consistent format.\nWhile perplexity is clearly defined as a function of the likelihood assigned by a model to a set of\nsequences, the manner in which that likelihood is computed may vary depending on how inputs\nare formatted for the model. THE PILE (Gao et al., 2020) identify one possible variation: inferring\ntest documents as separate inputs or concatenating them together to fill a single input. Meanwhile,\nPress et al. (2021) point out that documents larger than the maximum sequence length can be split\neither with or without overlap. We follow THE PILE (Gao et al., 2020) in requiring inferences of\ndocuments in separate inputs, with documents longer than the maximum sequence length split into\nnonoverlapping inputs.\n2.2\nEVALUATION DATA\nPurpose\nSource\nReference\nDescription\nStandard\nlanguage\nmodeling\nbenchmarks\nC4\nRaffel et al. (2019)\nvia\nDodge\net\nal.\n(2021)\nStandard contemporary LM pretraining corpus automatically filtered from the\nApril 2019 Common Crawl scrape\nMC4-EN\nChung et al. (2023)\nThe English language portion of a pretraining corpus automatically filtered from\n71 Common Crawl scrapes\nTHE PILE\nGao et al. (2020)\nStandard contemporary LM benchmark from curated multi-source data including\nlarge scale non-webscraped sources\nWIKITEXT-103\nMerity et al. (2016)\nA standard collection of verified \u201cGood\u201d and \u201cFeatured\u201d articles on Wikipedia\nPENN TREEBANK\nMarcus et al. (1999)\nvia Nunes (2020)\nClassic Wall Street Journal benchmark with linguistic structure annotations omit-\nted\nREDPAJAMA\nTogether Computer\n(2023)\nA publicly available reproduction of the LLaMA (Touvron et al., 2023) pretraining\nsource mixture, combining large amounts of webscraped text with smaller curated\nsources\nFALCON REFINEDWEB\nPenedo et al. (2023)\nA corpus of English sampled from all Common Crawl scrapes until June 2023,\nmore aggressively filtered and deduplicated than C4 and MC4-EN\nDOLMA\nSoldaini et al. (2023)\nA three trillion token corpus that samples sources commonly used to train LMs in\norder to enable open research on pretraining data\nFine-grained\ndomain\nbenchmarks\nM2D2 S2ORC\nReid et al. (2022)\nPapers from Semantic Scholar grouped by hierarchical academic field categories\nM2D2 WIKIPEDIA\nReid et al. (2022)\nWikipedia articles grouped by hierarchical categories in the Wikipedia ontology\nC4-100-DOMAINS\nChronopoulou et al.\n(2022)\nBalanced samples of the top 100 URL domains in C4 as measured by page count\nDOLMA-100-\nSUBREDDITS\nSoldaini et al. (2023)\nBalanced samples of the top 100 subreddits by number of posts, sourced from the\nDOLMA Reddit subset\nDOLMA-100-\nPROGRAMMING-\nLANGUAGES\nKocetkov\net\nal.\n(2022) via Soldaini\net al. (2023)\nBalanced samples of the top 100 programming languages by number of tokens,\nsourced from the DOLMA Stack subset\nDisparities\nbetween\nspeech\ncommunities\nICE\nGreenbaum & Nel-\nson (1996) via Liang\net al. (2022)\nEnglish from around the world curated by local experts, with subsets for Canada,\nEast Africa, Hong Kong, India, Ireland, Jamaica, Philippines, Singapore, and the\nUSA\nTWITTERAAE\nBlodgett et al. (2016)\nvia\nLiang\net\nal.\n(2022)\nBalanced sets of tweets classified as African American or White aligned English\nFringe sources\npreviously\nstudied for\nproblematic\ndiscourse\nMANOSPHERE CORPUS\nRibeiro et al. (2021)\n9 forums where a set of related masculinist ideologies developed over the 2000s\nand 2010s\nGAB CORPUS\nZannettou\net\nal.\n(2018)\nData from 2016-2018 from an alt-right, free-speech-oriented social media plat-\nform shown to contain more hate speech than mainstream platforms\n4CHAN CORPUS\nPapasavva\net\nal.\n(2020)\nData from 2016-2019 from a politics subforum of an anonymity-focused forum\nfound to contain among the highest rates of toxic content\nTable 2: The 18 data sources sampled to create language modeling evaluations in PALOMA. These\nare grouped by their purposes for inclusion (\u00a72.2). Different lines of research will require different\nselections of domains; PALOMA aims to enable research on differences in LM fit over the hundreds\nof domains that are readily available in existing metadata.\n5\nSource\nValidation\nTest\nCombined\nDomain Count\nTokens per Split per Domain\nC4\n1,000,000\n1,000,000\n2,000,000\n1\n1,000,000\nMC4-EN\n1,000,000\n1,000,000\n2,000,000\n1\n1,000,000\nTHE PILE\n2,199,944\n2,199,333\n4,399,277\n22\n99,984\nWIKITEXT-103\n247,969\n283,134\n531,103\n1\n265,552\nPENN TREEBANK\n89,917\n101,818\n191,735\n1\n95,868\nREDPAJAMA\n699,946\n700,000\n1,399,946\n7\n99,996\nFALCON REFINEDWEB\n1,000,000\n1,000,000\n2,000,000\n1\n1,000,000\nDOLMA\n2,999,998\n2,994,903\n5,994,901\n6\n499,575\nM2D2 S2ORC\n16,691,625\n16,682,726\n33,374,351\n167\n99,923\nM2D2 WIKIPEDIA\n4,890,146\n4,890,573\n9,780,719\n49\n99,803\nC4-100-DOMAINS\n9,795,511\n9,813,881\n19,609,392\n99\n99,037\nDOLMA-100-SUBREDDITS\n9,679,376\n9,680,887\n19,360,263\n100\n96,801\nDOLMA-100-PROGRAMMING-LANGUAGES\n9,999,707\n9,999,906\n19,999,613\n100\n99,998\nICE\n7,290,880\n7,236,065\n14,526,945\n17\n427,263\nTWITTERAAE\n722,905\n718,358\n1,441,263\n2\n360,316\nMANOSPHERE CORPUS\n1,000,000\n999,915\n1,999,915\n9\n111,106\nGAB CORPUS\n1,000,000\n1,000,000\n2,000,000\n1\n1,000,000\n4CHAN CORPUS\n1,000,000\n1,000,000\n2,000,000\n1\n1,000,000\nPALOMA\n71,307,924\n71,301,499\n142,609,423\n585\n121,888\nTable 3: Statistics of the evaluation data in PALOMA. We aim for a minimum of 100 thousand tokens\nper domain to select a balance between inference cost and metric variance based on our empirical\nfindings on the impact of subsampling in \u00a73.2.1. Bold marks minimum tokens after subsampling.\nIn Table 2, we list the sources of evaluation data by their purposes of inclusion, and in Appendix A\nwe detail each source individually. We show the number of tokens2 and domains in each of the 18\nsources in PALOMA in Table 3.\nIn this paper, we distinguish sources from domains, although not all cases permit such easy distinction.\nWe use source to refer to a selection of data that is characterized by the decisions of the people who\ncurated that data, whether that curation is automatic as in scraping C4 or manual as in selecting\nthe subcorpora of THE PILE. By contrast we use domain to refer to a set of documents that belong\ntogether because they are originally produced by a group of humans that share a distinct social\ncontext. Considered as such, domains may overlap; a document\u2019s author may belong to the set of\nEnglish speakers in Jamaica and the set of AI researchers. Further note, that domains are often latent\ncategorizations which we only approximate because complete metadata does not exist.\nAlso, some domains in PALOMA appear in multiple sources, such as academic papers. Though THE\nPILE and REDPAJAMA process academic papers differently, the subcorpora on academic papers in\neach source represent different approximations of the same or very similar domains. However for the\nsake of simplicity, we make the reductive assumption of counting all 585 domains in PALOMA as\nfully distinct.\nIt is beyond the scope of any one paper to prescribe an exhaustive set of domains that should be\nexamined for a LM. Rather PALOMA brings together a substantial selection of domains that are\nidentifiable from already available metadata to demonstrate the kinds of analyses possible with\nhundreds of domains and rigorous experimental controls. Different research goals will motivate\ndifferent definitions and selections of domains, but other researchers can apply our guidelines (\u00a72.1)\nto novel fine-grained domains suitable for their research questions. One of the key advantages of\nevaluating a model by its fit to a collection of text representing a domain is that such domains can\nbe identified not just by researchers who study LMs. We hope future work will identify many more\ndomains that no one discipline would think to look at.\nStandard language modeling sources\nThough it is common practice to evaluate on held out data\nfrom the pretraining corpus of a given model, we evaluate across several major pretraining corpora\nand standard language modeling benchmarks (C4, MC4-EN, THE PILE, WIKITEXT-103, PENN\nTREEBANK, REDPAJAMA, FALCON REFINEDWEB, DOLMA). We also break down performance per\ndomain within the sources that have multiple domains.\nFine-grained domain sources\nWhere typical pretraining corpora offer at most tens of marked\ndomains usually based on where the data is sourced, we examine datasets with up to an order of\n2In this paper, token counts are always computed with the GPT-NeoX-20B tokenizer (Black et al., 2022)\nunless otherwise stated.\n6\nmagnitude more domains. Existing datasets (M2D2 and C4-100-DOMAINS) and datasets we curate\nfrom DOLMA (DOLMA-100-SUBREDDITS and DOLMA-100-PROGRAMMING-LANGUAGES) use\nmetadata to define hundreds of domains over Wikipedia, Semantic Scholar, Common Crawl, Reddit,\nand Github data. These include diverse domains from Culture and the arts: Performing arts, a topic\non Wikipedia, to r/depression, a forum on Reddit for mental health support.\nDisparities between speech communities\nSome communities are known to be underserved by\nexisting models (Blodgett et al., 2016). Following Liang et al. (2022), we measure disparities in\nperformance on corpora of African American English and White aligned English from TWITTERAAE,\nas well as nine corpora of English from different countries with the ICE dataset.\nFringe sources previously studied for problematic discourse\nText from some fringe online\ncommunities has been shown to contain larger proportions of hate speech and toxicity than more\nmainstream sources (Ribeiro et al., 2021; Zannettou et al., 2018; Papasavva et al., 2020). Model fit to\ndiscourse with toxicity is worth measuring, as Longpre et al. (2023) have shown that varying amount\nof toxic content in pretraining data exhibits a tradeoff between non-toxic generation and ability to\nclassify toxicity. Measuring perplexity on MANOSPHERE CORPUS, GAB CORPUS, and 4CHAN\nCORPUS characterizes model familiarity with distinct social contexts in which toxic language arises.\n2.3\nBASELINE MODELS\nWe train a set of 6 baseline models on common pretraining corpora following our training guidelines\n(\u00a72.1.1). Training these models ourselves allows us to apply decontamination and fixed order to their\npretraining data as well as using a standard tokenizer to enable the greatest level of comparability.\nThese models are 1B parameter models trained for \u223c150B tokens on DOLMA (Soldaini et al., 2023),\nTHE PILE (Gao et al., 2020), REDPAJAMA (Together Computer, 2023), FALCON REFINEDWEB\n(Penedo et al., 2023), C4 (Raffel et al., 2019; Dodge et al., 2021), and MC4-EN (Chung et al., 2023).\nAdditional training details are included in Appendix C.\nWe also include baseline results from the Pythia models (Biderman et al., 2023). These models do\nnot conform with training guidelines (\u00a72.1.1). They do, however, use the GPTNeoX-20B tokenizer\n(Black et al., 2022) which has an identical vocabulary to our own baseline models, except lacking 3\nspecial tokens used in DOLMA. Another similarity is that the Pythia models also have a learning rate\nschedule set to end at 300B tokens seen, though they train for the full 300B tokens while we train for\njust 150B tokens of that schedule. This permits comparison between partially trained checkpoints.\n2.4\nMETRICS\nPALOMA uses standardized inference code3 to compute the following three metrics to assess LM fit\nto the evaluation data we have curated.\nPerplexity\nPerplexity (Jelinek et al., 1977) is most commonly formulated as perplexity per token,\nwhere a log likelihood \u2113 over documents N = {t1, . . . , t|N|} is normalized by T(N) denoting the\nnumber of tokens in the documents (i.e., T(N) = P\nt\u2208N | tokenize(t) |):\n\u2113 =\nX\nt\u2208N\n|t|\nX\ni\nln p(ti|t<i)\n(1)\nperplexity = e\u2212\n\u2113\nT(N)\n(2)\nIn this paper, perplexity always refers to perplexity per token unless otherwise stated.\nBits per byte\nWhen comparing results where model vocabularies must differ, for instance research\nto improve tokenizers, PALOMA follows Gao et al. (2020) in using bits per byte (BPB). This metric\nnormalizes the log likelihood \u2113 over documents by the count of UTF-8 encoded bytes in the corpus,\nB:\nBPB = 1\nB log2(e\u2212\u2113) =\n\u2212\u2113\nB ln(2)\n(3)\n3https://github.com/allenai/ai2-olmo-eval/tree/main/paloma\n7\nAverage likelihood per vocabulary type\nBoth perplexity and BPB can be driven by strings that\noccur frequently, dominating subtler differences in performance on other strings. An alternative\nis to measure surprise over all occurrences of specific strings instead. A set of strings particularly\nimportant to the model\u2019s functioning are the strings represented in the model\u2019s vocabulary. Following\nconventional NLP terminology, we call the elements of the vocabulary types in contrast to occurrences\nof these strings in some corpus, which are called tokens. When running inference in PALOMA we\nrecord \u00b5(\u2113v), average likelihoods over the whole corpus for each type v, as well as Tv(N), the count\nof occurrences of that type over the whole corpus (with indicator function 1(\u00b7)):\n\u00b5(\u2113v) =\n1\nTv(N)\nX\nt\u2208N\n|t|\nX\ni\n1(v = ti) ln p(ti|t<i)\n(4)\n2.4.1\nEFFICIENCY METRICS\nIn addition to performance metrics, we also ask submissions to PALOMA to record measures of cost\nassociated with the training of their language model: number of model parameters and number of\ntokens seen in training. We also record the size of the training dataset in UTF-8 encoded bytes and\nwhen models have run for more than one epoch\u2014where increase in novel data ceases but training\nduration continues increasing. We elect to measure these abstract cost values rather than metrics of\nrealized costs such as energy use or GPU hours, so that our efficiency comparisons are agnostic to\nhardware. Our aim is not to judge what hardware a submission uses. Note that this does not capture\nimprovement from innovations that use hardware more efficiently. Such questions are better explored\nthrough benchmarks that control hardware such as Peng et al. (2023).\n2.5\nCOMPARABLE SUBMISSIONS\nFair comparisons of language models can be challenging since there are so many variables to account\nfor, like the number of parameters in each model, the amount of training data trained on, and the\ntokenizer used. In this section we highlight a number of ways that our benchmark can be used to\nprovide evidence for practitioners to make scientific claims regarding how their model compares\nagainst other models.\nWe encourage submissions to PALOMA to opt into the training guidelines in \u00a72.1.1 (specifically as they\nare implemented in corresponding controls in \u00a73.1). Likewise we request submissions not intending\nto study changes to vocabulary opt in to using the vocabulary of GPTNeoX-20B (Black et al., 2022).\nWhere submissions opt out of these measures they will be marked for the corresponding limitations\nto comparability, allowing results to be filtered to remove all results that are not decontaminated, for\ninstance.\nSubmissions can use inference code provided by us that supports any model integrated with Hugging\nFace Transformers (Wolf et al., 2020) to compute the metrics in \u00a72.4 over each domain in the\nevaluation data. To make fair comparisons, it will be suggested that practitioners provide information\non the measures of efficiency discussed in \u00a72.4.1, such as model size. Similarly, submissions can\nrecord the name of the training dataset used. Finally, as model performance is typically best at the\nend of the learning rate schedule (compared with a model part way through training), the maximum\nduration of the learning rate schedule in tokens can be provided to mark comparability of partially\ntrained checkpoints.\nWe outline a few types of scientific claims that can be made with our benchmark, including comparing\ndifferent pretraining corpora and evaluating performance-efficiency tradeoffs:\n1. When two models are trained on the same data, with the same cost budget (number of\nparameters or number of tokens seen), they can be directly compared. If one model\noutperforms the other, this is direct evidence that that model is better. This represents the\nmost common type of comparison.\n2. When two models have different computational budgets but achieve matching perplexities,\nthis is evidence that the model with the lower computational cost is better. For example, if\ntwo models have matching perplexity, and are trained on the same number of tokens from\nthe same corpus, where one model has fewer parameters, this is evidence that the smaller\nmodel is better.\n8\n3. When the model architecture, budget for number of training tokens, and other modeling\nconfigurations are fixed, and multiple training runs are done varying the training cor-\npora,comparing the resulting trained models will effectively compare the pretraining corpora.\nThis can provide evidence that one pretraining corpus is better than another. Our baseline\nexperiments in this paper represent this type of scientific claim.\n4. When a set of submissions fixes all configurations except for varying one dimension of\ncost (number of parameters or number of tokens seen), this can provide evidence of scaling\ntrends for that model and training configuration.\n3\nEXPERIMENTAL CONTROLS\nIn order to meet the guidelines we establish in \u00a72.1, we implement a set of experimental controls\nwhose technical details are discussed here. We further distinguish controls that must be applied\nduring model training and controls that are applied at inference time.\n3.1\nTRAINING CONTROLS\n3.1.1\nDECONTAMINATION\nDataset\nDocument Removal Rate\nDOLMA\n0.062%\nREDPAJAMA\n0.099%\nTHE PILE\n2.753%\nFALCON REFINEDWEB\n0.733%\nC4\n0.010%\nMC4-EN\n0.002%\nTable 4: Decontamination removal statistics for the corpora with which we train our 6 baseline\nmodels. We remove any training document with any paragraph marked as contaminated against\nPALOMA.\nTo mitigate contamination of our benchmark, we develop an approach for removing contamination\nfrom training data at the scale of pretraining corpora of trillions of tokens. We use a Bloom filter\n(Bloom, 1970) as implemented by Soldaini et al. (2023) to match training text that is contaminated\nwith respect to the evaluation data. We employ this approach rather than the minHash or suffix array\napproaches used by Lee et al. (2022) and other deduplication work, as our approach is much more\nlightweight: the minHash approach would require pairwise computations, O(|Xt||Xe|) between\nall training texts, Xt, and evaluation texts, Xe, where our approach runs a constant number of\nhashes, K << |Xe|, over all texts in O (K(|Xt| + |Xe|)). Meanwhile the implementation of the\nsuffix array approach of Lee et al. (2022) requires memory usage proportional to the size of the\npretraining corpora. Since we aim to encourage researchers submitting to the benchmark to run this\ndecontamination on their pretraining data, we opt to minimize cost and engineering complexity.\nUsing our approach to find text matches, we mark contamination in the following way. We match\ntext at the paragraph level, i.e., newline separated spans of text. This granularity strikes a balance\nbetween, on one hand, examining only full documents, which can miss contamination embedded in\nnovel documents, and, on the other hand, all n-grams of a given size, where the size of the n-grams\nmust be carefully set. Instead paragraph matching leverages this naturally occurring unit of language,\nalthough this heuristic has its own limitations especially in domains such as code or poetry, where\nline separation is handled very differently from prose. To avoid coincidental collisions in the space of\nsmall strings, we ignore matches in paragraphs smaller than 13 unicode segmented tokens (Unicode,\n2023), as 13 is the n-gram sized used in contamination checks in Brown et al. (2020) and Rae\net al. (2021). Similarly, we ignore paragraphs composed of only punctuation, spaces, and emoji, as,\nunlike words, these can be arbitrarily repeated when used as formatting, leading to high frequency\nn-grams greater than our 13-gram threshold. Lastly, as code data consists almost entirely of short and\noften repeated lines, we forgo any decontamination on these sources (DOLMA-100-PROGRAMMING-\nLANGUAGES and the THE STACK domain of DOLMA). We leave the question of how to properly\ndecontaminate code data to future work.\n9\nHaving marked contaminated paragraphs, we now take the conservative measure of removing whole\ndocuments if they contain any contaminated paragraph. This has the added benefit of not disrupting\nthe contiguity of text within documents, which excising paragraphs would do. Applying this approach\nto the datasets on which we train 6 baseline models results in the removal rates shown in Table 4.\nWhile these vary by orders of magnitude from dataset to dataset (with THE PILE perhaps receiving a\nhigher removal rate due to the intentional oversampling in that dataset), this approach removes at most\n2.753% of documents, making it feasible to apply without dramatically reducing training dataset size.\nNevertheless, care should be taken to examine removal rates when applying this approach to new\ndatasets.\n3.1.2\nDATA ORDER\nAs contemporary LMs train on instances that are themselves concatenations of training documents\nup to the maximum sequence length of the model, to fix the order of training data one cannot simply\nfix the order of documents but must train on the same concatenated instances. Achieving this requires\nnot just a fixed random seed for training instance shuffling, but also adopting the same tokenization\nand maximum sequence length. Further fixing the number of instances in each gradient update would\nbe required for fully identical training, however this is onerous for experiments that may be run on\ndifferent hardware requiring different batch sizes. A compromise instead is to ensure that training\ncode feeds instances into gradient steps in a deterministic shuffled order, so the relative ordering of\ndata remains the same even if a given instance may fall in different gradient updates. In conclusion,\nwe adopt the most direct way of controlling data order\u2014we have submissions opting into this control\nuse the same training code that we use to pretrain our baseline models.4\n3.2\nEVALUATION CONTROLS\n3.2.1\nSUBSAMPLING\n100k\n250k\n500k\n1000k\n4000k\n8000k\nEvaluation Subsample Size\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nAverage Perplexity\n(Over 20 Subsamples)\nCheckpoint (Tokens Seen)\n25B\n86B\n300B\nFigure 2: Average perplexity and standard deviation over 20 subsamples of C4 validation data using\nPythia 1.4B checkpoints. We find that variance in perplexity over subsamples of evaluation data\ndecreases steadily as evaluation samples grow.\nIn Figure 2, we evaluate perplexity on data from C4 using Pythia 1.4B (Biderman et al., 2023)\nwhile varying the size of the evaluation subsample and training checkpoint. Each point in this figure\nrepresents the mean of perplexity on 20 different uniform subsamples and standard deviation is\nrepresented by the shaded region. As we expect, for a given checkpoint standard deviation shrinks as\nthe evaluation subsample gets larger. More subtly, standard deviation shrinks as the model is trained\non more data. This second observation matters if we want to measure model performance throughout\ntraining. Lastly note that the mean value is relatively stable over different evaluation subsample sizes,\nthough a slight downward trend appears at the smallest subsample sizes.\n4At the time of preprinting this training code will not yet be publicly released as it is developed under another\nproject that has not yet concluded. Until it is released submissions wishing to opt in to this control should contact\nus for direct assistance with reproducing data order.\n10\nThe stable trend of subsample size and variance in perplexity allows us to estimate how much\nperplexity numbers might change if a different subsample of the same size were drawn. Furthermore,\nwhen preparing splits for perplexity evaluation across many domains, it would be best to size for a\nsimilar level of metric variance. Most often perplexity evaluation data is subsampled uniformly over\nthe original distribution of domains in a source, resulting in more or less tokens from each domain in\nthe evaluation data based on how well represented they are in the corpus. We instead employ stratified\nsampling, in which all sources with marked domains are partitioned by domain and a uniform sample\nof the same size is taken from each partition. Specifically, documents are sampled from each domain\nuntil the same target number of tokens is reached. This helps ensure that no domains are lost or very\nsmall after subsampling.\nAs a small first step towards more principled subsampling, we set the target subsample size based\non the simplifying assumption that our metric variance results on C4 hold for other domains and\nmodels. Extrapolating our observations, we aim to subsample each split to a minimum of 1 million\ntokens per source and a minimum of 100 thousand tokens per domain. All datasets with domains are\nsubsampled to 100 thousand tokens per domain other than MANOSPHERE CORPUS which we treat as\na single domain, ICE which we include in entirety for comparability to its use in HELM, and DOLMA\nwhich we subsample at a higher target of 500 thousand tokens per domain. A few sources fall below\nour thresholds, with WIKITEXT-103, PENN TREEBANK, and TWITTERAAE being smaller than\n1 million tokens per split despite being included in their entirety, and REDPAJAMA having only 7\ndomains leading to 700 thousand tokens per split. We show the final token statistics in Table 3.\nIf extrapolation from the trends we observed holds, perplexities on sources will be drawn from a\ndistribution over subsamples with less than 1 standard deviation even at very early stages of training.\nMeanwhile, results on domains will be drawn for a similarly stable distribution by the end of training.\nThis is admittedly a heuristic simplification, as the relationship between variability and subsampling\nwill also likely depend on other factors such as average document length and heterogeneity of the\nsource data, as well as the power of the model being evaluated. We must leave it to future benchmarks\nto explore these questions as the requirement of decontaminating pretraining data against evaluation\ndata means any change to the evaluation data necessitates costly rerunning of pretraining of all\nbaselines and submissions.\n3.2.2\nVOCABULARY\nWhere possible we control by the simplest approach of using the same vocabulary: the vocabulary\nused in GPT-NeoX-20B (Black et al., 2022) with 3 special tokens added by DOLMA for masking\npersonally identifiable information. Note that when vocabulary is fixed this is essentially a training\ncontrol, as the model must be pretrained with this vocabulary. Nevertheless we mark this as an\nevaluation control, as we provide an option applied at inference time for making comparisons of\nmodels already pretrained with different vocabularies. Specifically, we follow THE PILE (Gao et al.,\n2020) and use bits per byte (BPB; \u00a72.4). In theory BPB may still present issues in comparability as it\nonly includes likelihoods of the specific sequences produced by a given tokenizer, e.g., rain ##ing for\nthe text raining, and not the marginal probability over all valid sequences in that vocabulary which\nwould produce the identical text, e.g., ra ##in ##ing and so on (Mielke, 2019; Cao & Rimell, 2021; see\nalso Hofmann et al., 2021). Models with a larger event space of possible sequences representing the\nsame text will be at a disadvantage if they assign any non-zero probability to these valid predictions\nignored by the metric. However, it has been shown empirically that the difference between the\nmarginal probability over all valid sequences and the likelihood of the sequence produced by the\ntokenizer is small (Mielke & Eisner, 2018) and typically lower than 0.5% (Chirkova et al., 2023). So\nin conclusion, we encourage submissions to opt in to our fixed vocabulary and mark these as most\ncomparable, but we also make allowance for submissions that opt out by only measuring comparisons\ninvolving models with different vocabularies in BPB.\n3.2.3\nEVALUATION FORMAT\nWe follow the input format established by THE PILE (Gao et al., 2020). In this format, documents\nare evaluated individually, e.g., \u201c<BOS>document 1\u201d then \u201c<BOS>document 2\u201d, rather than packed\ninto concatenated maximum sequence length inputs, e.g., \u201c<BOS>document 1<BOS>document\n2<BOS>...\u201d, where <BOS> is a special token for demarcating sequences. The latter concatenated\napproach is still often used as it takes the same preprocessing as is most commonly used for training\n11\ndata and is thus convenient for measuring validation loss during training. However, in Appendix \u00a7D\nwe find preliminary evidence that the predictability of variance from subsampling observed in \u00a73.2.1\nbreaks down for concatenated inputs. We also believe that evaluating documents individually more\nclosely mirrors how models are used in practice at inference time. Providing more than one document\nat a time through concatenation is essentially a form of few shot in context learning for language\nmodeling, as it allows the model to condition on information shared between concatenated documents\nwhen they are all drawn from the same domain. This is perhaps an interesting task formulation of its\nown but one that should be undertaken intentionally.\nMoreover, following THE PILE, we split documents longer than maximum sequence length into\ndisjoint inputs. This is also described by Press et al. (2021) as nonoverlapping inference. It is\ncontrasted with sliding window inference in which some amount of overlapping tokens are included\nas context in maximum-sequence-length windows to prevent an unrealistic lack of conditioning for\ntokens in the middle of a document appearing shortly after a multiple of the maximum sequence\nlength. However, a sliding window requires re-encoding overlapping tokens, making nonoverlapping\ninference the most efficient approach to computing perplexity.\n4\nCASE STUDIES\nBy applying our experimental controls (\u00a73) to PALOMA (\u00a72), we are able to dig deeper into what\nlanguage distributions models are learning to fit. In this section, we present several case studies\ndemonstrating the types of analyses possible with PALOMA. In \u00a74.1, we use our 6 baseline 1B\nmodels that vary only in which common corpus they are pretrained on to isolate the effect of data\ncomposition on LM fit. In \u00a74.2, we examine how scaling dynamics differ over the breadth of domains\nin PALOMA. Finally in \u00a74.3, we go beyond domains and decompose perplexity by performance on\ndifferent vocabulary types (i.e., specific elements of the model vocabulary).\n4.1\nPRETRAINING BEYOND COMMON CRAWL SHOWS IMPROVED STABILITY OF LM FIT\nWe hypothesize that one of the strongest drivers of differences in performance between different\ndomains is the composition of the pretraining data of a language model. While we show in \u00a74.2\nthat scaling model parameters or tokens seen increases performance on nearly all domains, the\npretraining data composition directly determines the distribution of language that the model is\nlearning to fit, which may or may not align with the distributions of language in the domains we\nevaluate. Therefore we examine the impact of varying the pretraining corpus while holding all other\nexperimental decisions the same.\nOrdinary perplexity\nIn Figure 3, we consider the most simple and aggregated view of LM fit that\nPALOMA can provide\u2014an ordinary perplexity as defined in \u00a72.4. Specifically we compute perplexity\nover all data in the standard language modeling and fine-grained domain sources. The other sources\nare set aside for now as they are designed for targeted analysis of questions such as the fit of models\nto discourse with prevalent toxicity. We also exclude the code data in DOLMA and DOLMA-100-\nPROGRAMMING-LANGUAGES, which is not supported by our decontamination approach. Using\nthis view we can already see that the baseline models which are trained only on data derived from\nCommon Crawl (C4, FALCON REFINEDWEB, and MC4-EN) stand out from the other baselines which\nalso incorporate more curated sources of data. However, this also points to the limitation of this most\naggregated view of the results: this ordinary perplexity represents fit to domains in proportion to\nthe number of tokens we have chosen to sample from each domain. As we sample 100 thousand\ntokens from each domain and the majority of our domains are not sourced from Common Crawl,\nthat data source is much less represented in PALOMA than most of the pretraining corpora whose\nheld-out data is conventionally used to measure validation loss. Nevertheless this simplified view\nof the results is useful for specific use cases that need a single metric over a prescriptive mix that\nemphasizes robustness to a diversity of domains, largely derived from non-web scraped sources.\nMacro average perplexity\nIn Figure 1, we provide another aggregation that examines the ro-\nbustness of fit by considering all domains equally\u2014a macro average of perplexity over domains:\n|D|\u22121 P\nd\u2208D perplexity(d) for domains, D. By contrast the previous ordinary perplexity is essen-\ntially an exponentiated micro average over the domains implicitly selected for during corpus curation.\n12\n20\n65\n150\nTokens Seen (billions)\n13\n15\n17\n20\n24\n28\n33\n39\nPerplexity\nBaselines\nFalcon-RefinedWeb 1B\nRedPajama 1B\nC4 1B\nDolma v1.5 1B\nThe Pile 1B\nmC4 1B\nFigure 3: Ordinary perplexity over all the standard language modeling and fine-grained domain\nsources in PALOMA, excluding code data not supported in our decontamination. While this aggre-\ngation obscures subtler differences in performance between domains, for uses such as monitoring\ntraining stability, this metric provides a single measure of fit over more diverse data than is typically\nused for measuring validation loss (e.g., C4).\nMacro averaging lets all marked domains have equal say on the model\u2019s performance, instead. To\nmake these macro averages more easily interpretable, we can examine them separately per source.\nThe most striking pattern that emerges here is the high, and sometimes non-monotonic, perplexity of\nthe 3 baselines trained on only Common Crawl data (C4, MC4-EN, FALCON REFINEDWEB). One\nsource where this is most apparent is evaluating on THE PILE. There the FALCON REFINEDWEB\nand MC4-EN baselines\u2019 results are dominated by greater than 10,000 perplexity on the Ubuntu IRC\ndomain, while other domains are in the low tens, and the C4 baseline exhibits an identical pattern but\nwith 8,056 perplexity on ArXiv. Both these domains contain large amounts of non-natural language,\nin the form of LaTeX and shell code as well as angle-bracketed IRC usernames. So while these\nCommon Crawl baselines spike on different domains, it appears they are all more susceptible to these\nextreme gaps in fit to some domains, perhaps due to a lack of exposure to non-natural language such\nas code or otherwise due to having only one set of cleaning filters applied to a single source of data.\nIn contrast, the baselines that include curated non-webscraped text sources (DOLMA, THE PILE, and\nREDPAJAMA) have a relative gap in perplexity that is highly stable through the course of training. This\nwould imply that short training runs on a subsample of such pretraining corpora may be predictive of\nthe LM fit of specific sources after much longer training. To address one exception, the REDPAJAMA\nbaseline often spikes on its final checkpoint, sometimes dramatically as in TWITTERAAE. A possible\nexplanation is that this checkpoint falls very soon after the model\u2019s training loss recovers from a\nsmall spike.\nPerplexity per domain ordered by median perplexity\nRather than aggregating, we can visualize\neach domain perplexity separately to surface gaps in fine-grained LM fit. In Figure 4, we arrange\nthe domains by their median perplexity over the baselines, as this order gives some sense of the\nintrinsic difficulty of a domain. We can then see which baselines more or less follow this order,\ndiffering only by a consistent offset, and which have gaps that are more idiosyncratic to each domain.\nAgain we see that when baselines have irregular gaps from the median these are most frequently\nbaselines pretrained on only Common Crawl. The notable exception is THE PILE baseline on M2D2\nS2ORC and DOLMA-100-PROGRAMMING-LANGUAGES, which has erratic gaps substantially below\nthe median, perhaps indicating that baseline is benefiting from exposure to specific domains and\nnot others rather than only a overall facility for scientific papers and code. The erratic-gapped\nCommon Crawl baselines, by contrast, are all worse than median perplexity, suggesting that they\n13\n0\n10\n20\n1.8\n5.2\n88.4\n195.3K\nThe Pile\n0\n2\n4\n6\n1.8\n5.2\n88.4\n195.3K\nRedPajama\n0\n2\n4\n2.7\n4.4\n9.3\n27.7\nDolma V1.5\n0\n50\n100\n150\n5.2\n7.5\n11.7\n20.2\n39.2\nM2D2 S2ORC\n0\n20\n40\n7.5\n9.3\n11.7\n15.2\n20.2\n27.7\nM2D2 Wikipedia\n0\n50\n100\n4.4\n9.3\n27.7\n141.6\nC4 100 Domains\nBaselines\nDolma V1.5 1B\nFalcon-RefinedWeb 1B\nRedPajama 1B\nThe Pile 1B\nC4 1B\nmC4 1B\nMedian\n0\n50\n100\n13.3\n17.4\n23.5\n32.8\n47.3\n71.0\n100 Subreddits\n0\n50\n100\n1.2\n1.8\n5.2\n88.4\n195.3K\n100 PLs\n0\n5\n10\n15\n9.3\n27.7\n141.6\n1.6K\nICE\n0\n1\n71.0\n111.2\n182.5\n315.6\n578.1\nTwitter AAE\nDomains Ordered by Median Perplexity over Models\nPerplexity for each Domain\nFigure 4: For each source with domains, domain perplexity for the final checkpoint of each model\nordered by median domain perplexity over all models. While performance gaps between some\nbaselines are highly consistent across domains (e.g., REDPAJAMA and THE PILE baselines on\nDOLMA-100-SUBREDDITS), others exhibit noisy performance gaps per domain that do not follow\nthe trend in median domain difficulty (e.g., the MC4-EN baseline on C4-100-DOMAINS). Note that\nthese erratic-gap patterns are frequently on the baselines pretrained on just Common Crawl data.\nmay have complete gaps in exposure to features of certain domains that cannot be recovered through\ngeneralization.\n4.2\nSCALING IMPROVES DOMAIN FIT UNEQUALLY\nWe return to our initial question: Does rising performance lift all domains? That is, does the sign of\nscaling trends observed in previous work (Kaplan et al., 2020; Hoffmann et al., 2022) hold across all\ndomains? And if so, do some domains still capture most of the improvement while others stagnate?\n4.2.1\nSCALING TOKENS SEEN\nIn Figure 5, We study the impact of increased training on domain fit. We make use of the finding\nthat the logarithms of loss and tokens seen trend linearly Kaplan et al. (2020), and make an estimate\nof improvement based on the slope between two empirical observations, with some inital and final\nnumber of tokens seen by checkpoints of a model \u03b8:\n\u2206t(inital, final) = ln(ln(perplexity(\u03b8inital))) \u2212 ln(ln(perplexity(\u03b8final)))\nlog10(final) \u2212 log10(inital)\n(5)\nSpecifically, we plot \u2206t(\u223c 20B, \u223c 150B) for each domain in ascending order for each of our 6\nbaselines.5\n5Note that the precise number of tokens seen by a given checkpoint does vary slightly between baselines, as\nthese were run on heterogeneous hardware requiring slight differences in batch size.\n14\n0\n6\n12\n18\n0\n-0.60\n-0.20\n0.20\n0.60\nThe Pile\n0\n2\n4\n6\n0\n-0.60\n-0.20\n0.20\nRedPajama\n0\n2\n4\n-0.05 0\n0.05\n0.15\n0.25\n0.35\n0.45\nDolma V1.5\n0\n42\n84\n126\n168\n-0.10\n0\n0.10\n0.20\n0.30\n0.40\nM2D2 S2ORC\n0\n13\n26\n39\n-0.03 0\n0.03\n0.08\n0.13\n0.18\n0.23\nM2D2 Wikipedia\n0\n25\n50\n75\n100\n-0.20\n-0.10\n0\n0.10\n0.20\n0.30\nC4 100 Domains\nBaselines\nDolma v1.5 1B\nThe Pile 1B\nFalcon-RefinedWeb 1B\nC4 1B\nmC4 1B\nRedPajama 1B\n0\n26\n52\n78\n-0.05\n0\n0.05\n0.10\n0.15\n100 Subreddits\n0\n26\n52\n78\n-1.00\n-0.50\n0\n0.50\n1.00\n1.50\n100 PLs\n0\n5\n10\n15\n-0.20\n-0.10\n0\n0.10\n0.20\nICE\n0\n1\n0\n-0.15\n-0.05\n0.05\n0.15\nTwitter AAE\nDomains Ordered by Improvement per Model\nLog Loss Improvement per 10x Tokens Seen\nFigure 5: As log loss and log tokens trend linearly, we estimate reduction in log loss per 10\u00d7\nincrease in tokens seen based on the slope between \u223c20B and \u223c150B checkpoints. We report this\nrate of improvement for each domain in ascending order per baseline model. This reveals that for\nsome models and domains, loss actually increases with further training. However, excepting just\n6 model-domain pairs, all baselines other than C4 and MC4-EN improve on all domains with a\nsimilar range between most and least improvement. Even among these, the median difference in\nimprovement between most and least improved domains has nearly twice as fast improvement for\nmost improved domain.\n20\n65\n150\n20.2\n27.7\n39.2\n57.7\nEval: 100 Subreddits\nModel: mC4 1B\n91_dating_advice\n81_askscience\n20\n65\n150\n10.4\n13.3\n17.4\n23.5\nEval: M2D2 Wikipedia\nModel: Dolma v1.5 1B\nVisual_arts\nEarth_sciences\n20\n65\n150\n3.4\n4.4\n6.2\n9.3\n15.2\nEval: The Pile\nModel: The Pile 1B\nDM_Mathematics\nYoutubeSubtitles\nTokens Seen (Billions)\nPerplexity\nFigure 6: We examine 3 types of examples of most (black dashed) and least (red dotted) improved\ndomains for 3 pairs of sources and models, where improvement is measured in terms of log loss per\n10\u00d7 increase in tokens seen (see Figure 5). As on the left, fit to a least improved domain can actually\nworsen in absolute terms or, as in the middle, simply improve more slowly. On the right, we see that\nleast improved domains may even be better fit in absolute terms. Unequal improvement between\ndomains is not undesirable a priori but merits finer-grained examination, enabled by PALOMA.\nOn some corpora, more pretraining worsens fit on some domains\nBaselines trained on C4\nand MC4-EN worsen with longer training on 65 and 43 domains respectively. Other than these two\nbaselines, only 6 other pairs of models and domains see such a deterioration. Among these 6 pairs\nonly the REDPAJAMA baseline exceeds \u2206t(\u223c 20B, \u223c 150B) > 0.1, likely due to the previously\nnoted spike in training loss near the final checkpoint of this model. It is unclear why the other baseline\n15\ntrained on only Common Crawl data, FALCON REFINEDWEB, does not also exhibit erratic behavior\nthis time, though possibly its cleaning heuristics avoid removing content important to these domains\nthat the other two models\u2019 cleaning heuristics do remove.\nEven for corpora where fit consistently improves, the rate of improvement is unequal\nOn the\nvast majority of domains, fit does improve with increased training. However rates of improvement,\n\u2206t(\u223c 20B, \u223c 150B), range substantially. Examining the median difference in improvement between\nmost and least improved domains shows 1.57x improvement for most improved domain, and this gap\ngrows to 1.94x when excluding the C4 and MC4-EN baselines.\nSlow improvement on a domain is not always unwanted, but surfaces dynamics of model\nlearning\nHaving identified the most and least improved domains, we visualize perplexity curves\nof 3 examples each demonstrating a different interpretation in Figure 6. On the left plot we see\nthat sometimes fit can actually worsen on one domain while improving on another domain, in this\ncase perhaps due to content filters in MC4-EN pretraining data blocking terms frequently used in\ndiscussion about dating and sexuality. But even when fit improves on both domains as in the middle\nplot, the rate of improvement can be slower for one than the other, possibly reflecting differences in\nthe quantity or heterogeneity of earth sciences or visual arts content in DOLMA. However, the right\nplot shows that the least improved domain can actually outperform the most improved domains in\nterms of absolute perplexity, in this case perhaps representing saturation of performance on the DM\nMathematics domain. Further examples are provided in the Appendix in Figure 13. Ultimately, our\ngoal is not to frame unequal improvement as a problem that needs to be fixed, but rather it is way to\nsurface subtler dynamics in language model learning.\n20\n65\n150\n12\n15\n20\n28\n39\nC4\n20\n65\n150\n9\n15\n28\nmC4\n20\n65\n150\n5\n7\n12\n20\nThe Pile\n20\n65\n150\n7\n12\n20\n39\nWikiText-103\n20\n65\n150\n12\n20\n39\nPTB\n20\n65\n150\n6\n9\n15\nRedPajama\n20\n65\n150\n13\n17\n24\n33\nFalcon\n20\n65\n150\n9\n12\n15\n20\n28\nDolma V1.5\n20\n65\n150\n9\n12\n15\n20\nM2D2 S2ORC\n20\n65\n150\n7\n12\n20\nM2D2 Wikipedia\n20\n65\n150\n9\n15\n28\nC4 100 Domains\n20\n65\n150\n17\n24\n33\n47\n100 Subreddits\n20\n65\n150\n3\n4\n5\n100 PLs\nBaselines\nPythia 160M\nPythia 1B\nPythia 7B\n20\n65\n150\n12\n15\n20\n28\nICE\n20\n65\n150\n161\n208\n274\n365\n494\nTwitter AAE\n20\n65\n150\n15\n20\n28\n39\nManosphere\n20\n65\n150\n24\n33\n47\n71\nGab\n20\n65\n150\n7\n9\n12\n15\n4chan\nMacro Average Perplexity\nTokens Seen (billions)\nFigure 7: Perplexity macro averaged by domain in each source for checkpoints of 3 Pythia model\nsizes. Note that these public models are not trained on decontaminated data, so these results should\nbe treated with greater skepticism than the results on the 6 baselines that we train under experimental\ncontrols. Consistently across these sources, increases in number of model parameters improves\nperplexity and the rate at which perplexity improves per token seen.\n16\n0\n6\n12\n18\n0\n0.10\n0.20\n0.30\n0.40\n0.50\nThe Pile\n0\n2\n4\n6\n0.05\n0.15\n0.25\n0.35\nRedPajama\n0\n2\n4\n0.08\n0.12\n0.18\n0.23\n0.28\nDolma V1.5\n0\n42\n84\n126\n168\n0.03\n0.08\n0.12\n0.18\n0.23\nM2D2 S2ORC\n0\n13\n26\n39\n0.10\n0.14\n0.18\n0.22\n0.26\nM2D2 Wikipedia\n0\n25\n50\n75\n100\n0.06\n0.10\n0.14\n0.18\n0.22\nC4 100 Domains\nBaselines\nFrom Pythia 1B to Pythia 7B\nFrom Pythia 160m to Pythia 1B\n0\n26\n52\n78\n0.07\n0.09\n0.11\n0.13\n0.15\n0.17\n100 Subreddits\n0\n26\n52\n78\n0\n0.10\n0.30\n0.50\n100 PLs\n0\n5\n10\n15\n0.06\n0.10\n0.14\n0.18\nICE\n0\n1\n0.04\n0.06\n0.08\n0.10\n0.12\nTwitter AAE\nDomains Ordered by Improvement per Model\nLog Loss Improvement per 10x Model Parameters\nFigure 8: We estimate log loss improvement per 10\u00d7 increase in non-embeddings parameters based\non improvement from Pythia-160M to Pythia-1B and from Pythia-1B to Pythia-7B on their final\ncheckpoints. We report this rate of improvement for each domain in ascending order per compared\nmodel pair. These increases in model size always improve performance on each domain, but the\nmedian difference in improvement from least to most sees twice as fast reduction of loss.\n4.2.2\nSCALING MODEL PARAMETERS\nWhile the 6 baseline models that we pretrain ourselves are all 1B parameter models, we can use\nmodels of varying sizes from the Pythia model suite (Biderman et al., 2023) to examine the impact\nof scaling model parameters on domain fit. As we note in \u00a72.3, these models are not controlled for\ncontamination but they do address all of our other guidelines.\nIncreased parameter count sees consistently lower perplexity\nIn Figure 7, we show the macro\naverage of perplexity over any domains in each source (as we did in Figure 1) for 3 sizes of Pythia\nmodel. Not only does this always show an increase in performance with greater parameter count,\nbut the relative differences between the performance curves are remarkably stable across all sources.\nAdditionally, macro average perplexity decreases faster over number of tokens seen for larger models\nin all sources.\nImprovements from model size improve unequally for different domains\nIn Figure 8 we\nperform the same analysis of improvement in log loss as before but this time with respect to log\nincrease in non-embedding parameters, \u2206p(inital, final). Specifically we plot \u2206p(85M, 805M) and\n\u2206p(805M, 6.4B) for the non-embedding parameter counts corresponding to the 160M, 1B, and 7B\nmodel sizes for each domain in ascending order per pair of models compared. This time scaling does\nuniversally result in improvements. However, the rate of improvement varies greatly from domain to\ndomain. Examining the median difference in improvement between most and least improved domains\nshows 2.02\u00d7 improvement for the most improved domain, a similar gap to that seen on increases in\ntokens seen. Again, we stress that unequal improvement is not necessarily problematic, but rather it\nhelps identify outlier domains that follow different scaling trends than the majority of the data. We\noffer examples of most and least improved domains with respect to increase in model size in the\nAppendix in Figure 14.\nTaken together, the results presented in this case study demonstrate the need to decompose evaluations\nof LM fit along domains. They show that it is not the case that models improve at uniform rates\nacross domains for a given increase in scale. We leave it to further work to examine when these\n17\n(a) Mean loss per type\n(b) Cumulative proportion of total loss per type\nFigure 9: Mean and total loss per vocabulary type, i.e., specific strings represented in the tokenizer\nvocabulary. While high-frequency types (which have low IDs) tend to have a low average loss as\nshown by a log-linear regression (a), they contribute a substantial part of the total loss, simply by\nvirtue of their frequent occurrence in the data (b). The figure shows the distributions for Pythia-7B on\nC4-100-DOMAINS, but the overall picture is consistent for different models and sources.\nFigure 10: Proportion of types in each source for which Pythia-1B makes better predictions than\nPythia-7B, as a function of training duration. The figure shows that for all examined sources, and\neven on the final checkpoint, a non-negligible proportion of vocabulary types is better predicted by\nthe smaller model (i.e., Pythia-1B). This observation is particularly true for TWITTERAAE, where\nthe proportion of such types is on average larger than 30%.\ninequalities are or are not desirable and what interventions can help prevent stagnation of LM fit to\ncertain domains.\n4.3\nCOMMON VOCABULARY TYPES DOMINATE PERPLEXITY, OTHERS HAVE INVERSE\nSCALING\nSo far we have examined perplexity aggregated over tokens. As introduced in \u00a72.4, another approach\nis to measure surprise over occurrences of specific strings. In PALOMA we measure average likelihood\nper vocabulary type, i.e., the strings that are represented in the vocabulary of a model, in contrast to\noccurrences of these strings in some corpus, called tokens.\nFew vocabulary types account for most of the loss measured in perplexity\nHow much do types\ncontribute to the likelihoods aggregated per token in perplexity? To answer this question, we start by\nanalyzing the total loss mass added by types, as a function of their IDs. Given how the GPTNeoX-20B\ntokenizer was trained (Sennrich et al., 2016; Black et al., 2022), smaller IDs correspond to more\nfrequent types in the tokenizer training data, and we find an overall moderate to strong correlation\nbetween IDs and frequencies in the evaluation data of PALOMA as well (Pearson\u2019s r averaged across\ndomains: \u20130.522\u00b10.087). Crucially, frequency has a strong impact on the total loss mass associated\nwith individual types: while the average loss is lower for the high-frequency types (Figure 9a), the\n18\nFigure 11: Proportion of types in each source for which Pythia-1B makes better predictions than\nPythia-7B on the final checkpoint, as a function of type ID, i (low: i \u2264 1000; mid: 1000 < i \u2264\n10000; high: i > 10000). The figure shows that the proportion of types for which the smaller model\nis better increases with type ID. Thus, while Pythia-7B is almost always better on high-frequency\ntypes (low ID), Pythia-1B is better on many low-frequency types (high ID).\ntotal loss is higher, resulting in a situation where 5% of the types already cover roughly 50% of the\noverall perplexity (Figure 9b). Thus, perplexity is strongly influenced by a relatively small set of\nhigh-frequency types.\nSome types are more surprising on average to larger models than smaller ones\nIs there variation\nbetween models in terms of how much types contribute to perplexity? Put differently, if model A\nhas a lower aggregated perplexity than model B, can we conclude that it has a lower loss for all\ntypes? Conducting an exploratory analysis of Pythia-1B vs. Pythia-7B, we find that this is not the\ncase: while Pythia-7B has a lower perplexity on all domains, there are always types that are better\npredicted by Pythia-1B (see Figure 10), with the average proportion of such types varying between\n8.5% (C4-100-DOMAINS) and 32.1% (TWITTERAAE). As shown in Figure 11, the proportion of\ntypes on which Pythia-1B is better increases with ID, for all examined sources. In other words,\nwhile Pythia-7B is almost always better on high-frequency types, Pythia-1B is better on a substantial\nportion of low-frequency types. This pattern is not captured well by perplexity, which is influenced\nvery little by the performance on such low-frequency types (see above). However, note that even in\nthe high-frequency regime around 10% of types are better predicted by the smaller model. Many of\nthose types also have a high frequency in the sources, indicating that our finding cannot be explained\nmerely as a result of noisy measurements. For example, the pronoun I occurs 14703 times in ICE but\nits measured mean loss on the final checkpoint is lower for Pythia-1B than Pythia-7B.\nLower average loss per type can be the result of several different training dynamics.\nWhat\ndoes it mean specifically if Pythia-1B has a lower average loss on a specific type than Pythia-7B?\nFigure 12 shows, for each of the 18 sources, the training dynamics of an example type for which\nPythia-1B is better than Pythia-7B after convergence. As can be seen, there are various patterns:\nsometimes there is a constant gap between the two models, with Pythia-1B being better from the very\nbeginning (e.g., Boat in FALCON REFINEDWEB); sometimes Pythia-1B has a constant loss while\nPythia-7B is getting worse over time (e.g., schedule in DOLMA); sometimes Pythia-7B has a constant\nloss while Pythia-1B is getting better over time (e.g., exchanged in THE PILE); finally, sometimes\nPythia-1B is decreasing its loss while Pythia-7B is increasing its loss over time (e.g., BR in C4).\nEspecially the last pattern bears a resemblance with inverse scaling effects that characterize other\naspects of LM behavior, where the performance gets worse rather than better with larger models\n(Mckenzie et al., 2023). We are not aware of prior work describing the kind of type-level inverse\nscaling that we observe in this analysis.\nSome domains have more inverse scaling types than others\nWe also notice that there is further\nvariation on the domains within the sources: for example, in TWITTERAAE (the source where the\nproportion of types on which Pythia-1B is better is largest), on the types where Pythia-1B is better, it\nis better on the African American domain in 77.6% of cases, and on the White aligned domain in\nonly 71.3% of cases. In other words, there are numerous vocabulary types where the larger model\n19\nFigure 12: Training dynamics of example types for which Pythia-1B is better than Pythia-7B on the\nfinal checkpoint. We specifically show the types that, within a specific source, (i) have a minimum\ncount of 5 and (ii) have the largest mean loss difference between Pythia-1B and Pythia-7B on the final\ncheckpoint. We observe that sometimes Pythia-1B is better from the very beginning (e.g., Boat in\nFALCON REFINEDWEB); sometimes Pythia-1B has a constant loss while Pythia-7B is getting worse\nover time (e.g., schedule in DOLMA); sometimes Pythia-7B has a constant loss while Pythia-1B is\ngetting better over time (e.g., exchanged in THE PILE); finally, sometimes Pythia-1B is decreasing its\nloss while Pythia-7B is increasing its loss over time (e.g., BR in C4).\nperforms better on the White aligned domain (as expected), and where the inverse scaling behavior\nonly manifests itself on the African American domain.\nTaken together, these results provide further evidence that reporting only aggregated perplexity values\nneglects more subtle dynamics on lower levels (sources, domains, vocabulary types).\n5\nRELATED WORK\nPrevious fine-grained perplexity corpora\nPALOMA is inspired by and incorporates previous work\nthat curates corpora with marked domains. THE PILE (Gao et al., 2020) is an early public pretraining\ncorpus that provides provenance-based subdivisions of data rather than homogeneous webscraped\ndata. Their 22 domains are included among the 585 domains in PALOMA. More recent works curate\ndatasets with on the order of hundreds of domains (Reid et al., 2022; Chronopoulou et al., 2022; Li\net al., 2022), which we include in part or full in PALOMA. Likewise we include efforts that curate\ntext from specific dialects (Blodgett et al., 2016; Greenbaum & Nelson, 1996).\nPrevious LLM benchmarks\nLanguage modeling is a field of research with a long history and\nmany classic benchmarks (Baker et al., 1983, Chelba et al., 2013, Merity et al., 2016, and Marcus\net al., 1999, inter alia). With the more recent success of language modeling as a pretraining step,\nbenchmarks have shifted towards downstream task evaluations rather than perplexity metrics. Perhaps\nthe most notable exception is THE PILE (Gao et al., 2020) which, in addition to being a pretraining\ncorpus, is explicitly framed as a benchmark and provides a detailed specification of the formatting of\ninputs for inference to compute perplexity. Where recent LMs report perplexity beyond the model\u2019s\nown training distribution (Rae et al., 2021; Hoffmann et al., 2022), it is almost always on THE\nPILE. We adopt their inference format and add additional controls for contamination, data order, and\ntokenization to our benchmark (see Table 1). Other efforts focus on comprehensive LM evaluation\npredominantly using downstream tasks (Liang et al., 2022; Gao et al., 2021). We employ one such\nopen source evaluation tool, Catwalk (Groeneveld et al., 2023),6 to compute our metrics. Moreover,\nwe see our work as complementary to efforts in downstream evaluation, as previous work disagrees\nwhether perplexity evaluations are predictive of downstream performance (Liu et al., 2022; Tay et al.,\n2021; Ganguli et al., 2022; Xia et al., 2022). In fact, we hope that our benchmark can provide well\ncontrolled perplexity results for further study of this question.\n6https://github.com/allenai/catwalk\n20\n6\nCONCLUSION\nWe believe that evaluations of LM fit provide an important view of language model performance that\nhas been neglected in recent LM research and development. Perplexity cannot be na\u00efvely applied to\nlanguage modeling at this scale due to challenges such as benchmark contamination. However, these\nobstacles are worth overcoming as perplexity offers several advantages not afforded by downstream\nevaluations. Instead of constructing tasks from scratch, we can rely on the ecological validity of\nreal-world data drawn from known sources. By figuring out the best ways to evaluate fit of a model\nto a collection of documents, we create an interface for other fields to easily compose evaluations for\nlanguage models. Researchers in other fields need not understand the architectures of such models\nto collect a corpus of text representing domains of interest that LM researchers would not know\nto consider. Once a significant data source is identified, such as an online community, evaluations\ncan be updated over time by simply scraping more data, unlike downstream tasks where expensive\nannotation would be required.\nPALOMA advances the possibilities for evaluation of LM fit by providing finely categorized domains\nof text and controls to mitigate confounders of our metrics. We hope that, along with the baseline\nmodels that we train for this work, submissions will begin to fill in the space of possible LM\nconfigurations over choices such as data composition, data size, parameter count, and learning rate\nschedule. By encouraging standardization within this space we increase the density of comparisons\nthat can be made between models. With greater resolution of evaluations, we can increase the\nresolution of our understanding of language model training dynamics.\n7\nLIMITATIONS\nThe largest limitation of PALOMA is that we elect to focus just on the language modeling of English\nand code data. We select this scope as most current LMs also focus on theses types of data. However,\nwe strongly encourage future work to explore how language model fit to fine-grained domains behaves\nwithin and across other languages. We also wish to note that while we measure performance on more\nfine-grained domains, our choice of metrics may not reflect what is valued by all the communities who\nproduce language in these domains (Diaz & Madaio, 2023). Nevertheless, we think that examining\ndiscrepancies in existing metrics over domains will lead to a deeper understanding of language\nmodeling dynamics, which can then illuminate the gaps in existing approaches to evaluation. In this\nwe follow Holtzman et al. (2023) and McCoy et al. (2023) by aiming to examine model behaviors,\nregardless of whether those behaviors are desirable or not to humans.\nWe also caution against hasty interpretation of LM fit to any particular domain as being indicative\nof alignment of a model to features that a human might consider salient about these domains. For\ninstance we find that when just examining perplexity, results on the 3 fringe datasets are tightly\nrelated to average document lengths, with the short tweet-like posts in GAB CORPUS receiving\nhigh perplexities while the long connected threads of posts in 4CHAN CORPUS and MANOSPHERE\nCORPUS provide greater context and lower perplexity. At this level of aggregation, differences in\nsurprise between these domains likely have little to do with model fit to specific types of toxicity. In\nour case study in \u00a74.3, we demonstrate that often it is more appropriate to decompose measures of\nsurprise over specific strings within a corpus, rather than aggregating over all text in a domain. We\nhope that by surfacing the average likelihoods of specific strings in the vocabulary, PALOMA can\nenable future work on metrics that better measure the fit of models to the features of language in\nspecific domains that humans find most salient.\nAnother set of limitations arises from our use of documents as a fundamental unit of data. When\nsubsampling although we balance the number of tokens used to represent each domain, we still\nsample documents until that target token count is reached. Concretely this means that some domains,\nespecially books, are represented by only dozens of documents, which likely does not capture\nthe full distribution of the domain as well many smaller documents might. This also impacts our\ndecontamination approach, since we remove whole documents that have any paragraph marked as\ncontaminated to avoid mangling documents by excising individual paragraphs. Such an approach\ntends to disproportionately remove long documents that are frequently quoted, which may include\nseminal works along the lines of Martin Luther King\u2019s \u201cI Have a Dream\u201d speech that actually\ndeployed models should be familiar with. The purpose of PALOMA, however, is to enable controlled\n21\nresearch on the science of language modeling, but production models should likely use caution in\napplying this decontamination technique.\nAs we note in \u00a72.4.1, our measures of efficiency aim to consider cost agnostic of hardware. It\nis important to acknowledge that it is not possible to completely separate hardware from abstract\nnotions of cost. For instance, models that utilize greater parallelism through sparse combinations of\npredictions from experts will appear to have parameter counts equal to the sum of experts even though\nthose parameters can be trained in far less wall-clock time on multiple devices. Further though we\nrecord when submissions have run multiple epochs over data, this does not fully disentangle data and\ncompute efficiency. That is datasets such as THE PILE can include intentional oversampling of data\nwithin a single epoch, and other datasets include multiple different prepossessed versions of the same\nunderlying data (for instance including both C4 and some other version on Common Crawl data).\nFinally we would like to acknowledge that we are unable to rehost THE PILE and ICE data in\nPALOMA for easy access due to restrictions on these datasets. Permission to access ICE can be\narranged through contacting the original authors.7\nACKNOWLEDGEMENTS\nWe thank Nishant Subramani, Akhila Yerukola, Rodney Kinney, and Ari Holtzman for fruitful\nconversations. The experimental components of this work were made possible through a partnership\nwith AMD and CSC, enabling use of the LUMI supercomputer.\nREFERENCES\nRoee Aharoni and Yoav Goldberg. Unsupervised domain clusters in pretrained language models.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.\n7747\u20137763, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nacl-main.692. URL https://aclanthology.org/2020.acl-main.692.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Co-\njocaru, Maitha Alhammadi, Mazzotta Daniele, Daniel Heslow, Julien Launay, Quentin Malartic,\nBadreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon series of language models:\nTowards open frontier models. 2023.\nJanet M. Baker, David S. Pallett, and John Scott Bridle. Speech recognition performance assessments\nand available databases. In IEEE International Conference on Acoustics, Speech, and Signal Pro-\ncessing, 1983. URL https://api.semanticscholar.org/CorpusID:36956472.\nYoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Inter-\nnational Conference on Machine Learning, 2009. URL https://api.semanticscholar.\norg/CorpusID:873046.\nStella Rose Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O\u2019Brien,\nEric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing\nlarge language models across training and scaling. ArXiv, abs/2304.01373, 2023. URL https:\n//api.semanticscholar.org/CorpusID:257921893.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu\nPurohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceedings of BigScience Episode #5 \u2013 Workshop\non Challenges & Perspectives in Creating Large Language Models, pp. 95\u2013136, virtual+Dublin,\nMay 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.9.\nURL https://aclanthology.org/2022.bigscience-1.9.\nSu Lin Blodgett, Lisa Green, and Brendan O\u2019Connor. Demographic dialectal variation in social\nmedia: A case study of African-American English. In Proceedings of the 2016 Conference on\n7https://www.ice-corpora.uzh.ch/en/access.html\n22\nEmpirical Methods in Natural Language Processing, pp. 1119\u20131130, Austin, Texas, November\n2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1120. URL https:\n//aclanthology.org/D16-1120.\nBurton H. Bloom. Space/time trade-offs in hash coding with allowable errors. Commun. ACM,\n13(7):422\u2013426, jul 1970. ISSN 0001-0782. URL https://doi.org/10.1145/362686.\n362692.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners, 2020.\nKris Cao and Laura Rimell. You should evaluate your language model on marginal likelihood\nover tokenisations. In Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 2104\u20132114, Online and Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.161. URL\nhttps://aclanthology.org/2021.emnlp-main.161.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram\u00e8r, and Chiyuan\nZhang. Quantifying memorization across neural language models. ArXiv, abs/2202.07646, 2022.\nURL https://api.semanticscholar.org/CorpusID:246863735.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, T. Brants, Phillip Todd Koehn, and Tony\nRobinson. One billion word benchmark for measuring progress in statistical language mod-\neling. In Interspeech, 2013. URL https://api.semanticscholar.org/CorpusID:\n14136307.\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi\nDong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V. Le. Symbolic discovery of optimiza-\ntion algorithms. ArXiv, abs/2302.06675, 2023. URL https://api.semanticscholar.\norg/CorpusID:256846990.\nNadezhda Chirkova, Germ\u00e1n Kruszewski, Jos Rozen, and Marc Dymetman. Should you marginalize\nover possible tokenizations? In Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers), pp. 1\u201312, Toronto, Canada, July 2023.\nAssociation for Computational Linguistics. URL https://aclanthology.org/2023.\nacl-short.1.\nAlexandra Chronopoulou, Matthew Peters, and Jesse Dodge. Efficient hierarchical domain adaptation\nfor pretrained language models. In Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pp.\n1336\u20131351, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.\n18653/v1/2022.naacl-main.96. URL https://aclanthology.org/2022.naacl-main.\n96.\nHyung Won Chung, Noah Constant, Xavier Garc\u00eda, Adam Roberts, Yi Tay, Sharan Narang, and Orhan\nFirat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining.\nArXiv, abs/2304.09151, 2023. URL https://api.semanticscholar.org/CorpusID:\n258187051.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\npp. 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.\nFernando Diaz and Michael A. Madaio. Scaling laws do not scale. ArXiv, abs/2307.03201, 2023.\nURL https://api.semanticscholar.org/CorpusID:259375636.\nJesse Dodge, Maarten Sap, Ana Marasovi\u00b4c, William Agnew, Gabriel Ilharco, Dirk Groeneveld,\nMargaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the\n23\ncolossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pp. 1286\u20131305, Online and Punta Cana, Dominican Republic,\nNovember 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.\n98. URL https://aclanthology.org/2021.emnlp-main.98.\nYanai Elazar, Akshita Bhagia, Ian H. Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane\nSuhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A.\nSmith, and Jesse Dodge. What\u2019s in my big data? ArXiv, abs/2310.20707, 2023. URL https:\n//api.semanticscholar.org/CorpusID:264803575.\nDeep Ganguli, Danny Hernandez, Liane Lovitt, Nova DasSarma, T. J. Henighan, Andy Jones,\nNicholas Joseph, John Kernion, Benjamin Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom\nConerly, Dawn Drain, Nelson Elhage, Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds, Scott\nJohnston, Shauna Kravec, Neel Nanda, Kamal Ndousse, Catherine Olsson, Daniela Amodei,\nDario Amodei, Tom B. Brown, Jared Kaplan, Sam McCandlish, Christopher Olah, and Jack Clark.\nPredictability and surprise in large generative models. Proceedings of the 2022 ACM Conference on\nFairness, Accountability, and Transparency, 2022. URL https://api.semanticscholar.\norg/CorpusID:246867298.\nLeo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:\nAn 800gb dataset of diverse text for language modeling. ArXiv, abs/2101.00027, 2020. URL\nhttps://api.semanticscholar.org/CorpusID:230435736.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Gold-\ning, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang,\nAnish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model\nevaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628.\nSidney Greenbaum and Gerald Nelson. The international corpus of english (ICE) project. World\nEnglishes, 15(1):3\u201315, mar 1996. doi: 10.1111/j.1467-971x.1996.tb00088.x. URL https:\n//doi.org/10.1111%2Fj.1467-971x.1996.tb00088.x.\nDirk Groeneveld, Anas Awadalla, Iz Beltagy, Akshita Bhagia, Ian Magnusson, Hao Peng, Kyle\nRichardson, Oyvind Tafjord, Pete Walsh, and Jesse Dodge. Catwalk: A unified language model\nevaluation framework for many datasets, 2023. URL https://github.com/allenai/\ncatwalk.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia\nGuy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre.\nTraining compute-optimal large language models. ArXiv, abs/2203.15556, 2022. URL https:\n//api.semanticscholar.org/CorpusID:247778764.\nValentin Hofmann, Janet Pierrehumbert, and Hinrich Sch\u00fctze. Superbizarre is not superb: Derivational\nmorphology improves BERT\u2019s interpretation of complex words. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers), pp. 3594\u20133608, Online,\nAugust 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.279.\nURL https://aclanthology.org/2021.acl-long.279.\nAri Holtzman, Peter West, and Luke Zettlemoyer. Generative models as a complex systems science:\nHow can we make sense of large language model behavior? ArXiv, abs/2308.00189, 2023. URL\nhttps://api.semanticscholar.org/CorpusID:260351369.\nFrederick Jelinek. Statistical methods for speech recognition. MIT press, 1998.\nFrederick Jelinek, Robert L. Mercer, Lalit R. Bahl, and Janet M. Baker. Perplexity\u2014a measure of\nthe difficulty of speech recognition tasks. Journal of the Acoustical Society of America, 62, 1977.\nURL https://api.semanticscholar.org/CorpusID:121680873.\n24\nJared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models.\nArXiv, abs/2001.08361, 2020. URL https://api.semanticscholar.org/CorpusID:\n210861095.\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu\u00f1oz Ferrandis,\nYacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von\nWerra, and Harm de Vries. The stack: 3 tb of permissively licensed source code. Preprint, 2022.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-\nBurch, and Nicholas Carlini. Deduplicating training data makes language models better. In\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 8424\u20138445, Dublin, Ireland, May 2022. Association for Computational\nLinguistics. doi: 10.18653/v1/2022.acl-long.577. URL https://aclanthology.org/\n2022.acl-long.577.\nMargaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, and Luke\nZettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models.\nArXiv, abs/2208.03306, 2022. URL https://api.semanticscholar.org/CorpusID:\n251371375.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby\nYan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R\u2019e, Diana Acosta-\nNavas, Drew A. Hudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren,\nHuaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yuksekgonul, Mirac\nSuzgun, Nathan S. Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian\nHuang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto,\nThomas F. Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. Annals of the New\nYork Academy of Sciences, 1525:140 \u2013 146, 2022. URL https://api.semanticscholar.\norg/CorpusID:253553585.\nHong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. Same pre-training loss, better downstream:\nImplicit bias matters for language models. In International Conference on Machine Learning,\n2022. URL https://api.semanticscholar.org/CorpusID:253107233.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic\nscholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pp. 4969\u20134983, Online, July 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.acl-main.447. URL https://aclanthology.org/\n2020.acl-main.447.\nS. Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny\nZhou, Jason Wei, Kevin Robinson, David M. Mimno, and Daphne Ippolito. A pretrainer\u2019s\nguide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity.\nArXiv, abs/2305.13169, 2023. URL https://api.semanticscholar.org/CorpusID:\n258832491.\nMitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. Treebank-3, 1999.\nURL https://catalog.ldc.upenn.edu/LDC99T42.\nR. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths. Embers of\nautoregression: Understanding large language models through the problem they are trained to solve.\nArXiv, abs/2309.13638, 2023. URL https://api.semanticscholar.org/CorpusID:\n262464572.\nIan. R. Mckenzie, Alexander Lyzhov, Michael Martin Pieler, Alicia Parrish, Aaron Mueller, Ameya\nPrabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, Andrew Gritsevskiy, Daniel\nWurgaft, Derik Kauffman, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh, Max Weiss, Sicong\nHuang, The Floating Droid, Tom Tseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang, Zhengping\nZhou, Najoung Kim, Sam Bowman, and Ethan Perez. Inverse scaling: When bigger isn\u2019t better.\n25\nArXiv, abs/2306.09479, 2023. URL https://api.semanticscholar.org/CorpusID:\n259188012.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. ArXiv, abs/1609.07843, 2016. URL https://api.semanticscholar.org/\nCorpusID:16299141.\nSabrina J. Mielke. Can you compare perplexity across different segmentations?, Mar 2019. URL\nhttps://sjmielke.com/comparing-perplexities.htm.\nSabrina J. Mielke and Jason Eisner. Spell once, summon anywhere: A two-level open-vocabulary\nlanguage model. In AAAI Conference on Artificial Intelligence, 2018. URL https://api.\nsemanticscholar.org/CorpusID:5081459.\nDavide Nunes. Preprocessed penn tree bank, 2020. URL https://zenodo.org/record/\n3910021.\nLiam Paninski. Estimation of entropy and mutual information. Neural Computation, 15:1191\u20131253,\n2003. URL https://api.semanticscholar.org/CorpusID:2034914.\nAntonis Papasavva, Savvas Zannettou, Emiliano De Cristofaro, Gianluca Stringhini, and Jeremy\nBlackburn. Raiders of the lost kek: 3.5 years of augmented 4chan posts from the politically\nincorrect board. Proceedings of the International AAAI Conference on Web and Social Media, 14:\n885\u2013894, may 2020. doi: 10.1609/icwsm.v14i1.7354. URL https://doi.org/10.1609%\n2Ficwsm.v14i1.7354.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-Aim\u00e9e Cojocaru, Alessandro\nCappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The\nrefinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only.\nArXiv, abs/2306.01116, 2023. URL https://api.semanticscholar.org/CorpusID:\n259063761.\nHao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle\nLo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Evan Pete Walsh, Noah A. Smith,\nand Hannaneh Hajishirzi. Efficiency pentathlon: A standardized arena for efficiency evaluation.\nArXiv, abs/2307.09701, 2023. URL https://api.semanticscholar.org/CorpusID:\n259982429.\nOfir Press, Noah A. Smith, and Mike Lewis. Shortformer: Better language modeling using shorter\ninputs. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pp. 5493\u20135505, Online, August 2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.427. URL https://aclanthology.org/2021.acl-long.\n427.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019. URL https://api.semanticscholar.\norg/CorpusID:160025533.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,\nJohn Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hen-\nnigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne\nHendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri,\nSaffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell, Nathan\nMcAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lor-\nraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki\nLazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev, Doug\nFritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cy-\nprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan\nClark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G. Johnson,\n26\nBlake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Si-\nmon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway, L. L.\nBennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language mod-\nels: Methods, analysis & insights from training gopher. ArXiv, abs/2112.11446, 2021. URL\nhttps://api.semanticscholar.org/CorpusID:245353475.\nColin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-\nto-text transformer. ArXiv, abs/1910.10683, 2019. URL https://api.semanticscholar.\norg/CorpusID:204838007.\nMachel Reid, Victor Zhong, Suchin Gururangan, and Luke Zettlemoyer. M2D2: A massively multi-\ndomain language modeling dataset. In Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pp. 964\u2013975, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics. URL https://aclanthology.org/\n2022.emnlp-main.63.\nManoel Horta Ribeiro, Jeremy Blackburn, Barry Bradlyn, Emiliano De Cristofaro, Gianluca\nStringhini, Summer Long, Stephanie Greenberg, and Savvas Zannettou.\nThe evolution of\nthe manosphere across the web. Proceedings of the International AAAI Conference on Web\nand Social Media, 15:196\u2013207, may 2021. doi: 10.1609/icwsm.v15i1.18053. URL https:\n//doi.org/10.1609%2Ficwsm.v15i1.18053.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 1715\u20131725, Berlin, Germany, August 2016. Association\nfor Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.\norg/P16-1162.\nNoam M. Shazeer. Glu variants improve transformer. ArXiv, abs/2002.05202, 2020. URL https:\n//api.semanticscholar.org/CorpusID:211096588.\nZhihong Shen, Hao Ma, and Kuansan Wang. A web-scale system for scientific knowledge exploration.\nIn Proceedings of ACL 2018, System Demonstrations, pp. 87\u201392, Melbourne, Australia, July\n2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-4015. URL https:\n//aclanthology.org/P18-4015.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur,\nBen Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh\nJha, Sachin Kumar, Li Lucy, Xinxi Lyu, Ian Magnusson, Jacob Morrison, Niklas Muennighoff,\nAakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson,\nZejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Pete Walsh, Hannaneh\nHajishirzi, Noah A. Smith, Luke Zettlemoyer, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle\nLo. Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research.\narXiv preprint, 2023. URL https://huggingface.co/datasets/allenai/dolma.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu.\nRoformer: Enhanced trans-\nformer with rotary position embedding. ArXiv, abs/2104.09864, 2021. URL https://api.\nsemanticscholar.org/CorpusID:233307138.\nYi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan\nNarang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efficiently: Insights from\npre-training and fine-tuning transformers. ArXiv, abs/2109.10686, 2021. URL https://api.\nsemanticscholar.org/CorpusID:237592821.\nTogether Computer. RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset,\nApril 2023. URL https://github.com/togethercomputer/RedPajama-Data.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language\nModels. arXiv preprint arXiv:2302.13971, 2023.\n27\nUnicode. Unicode Text Segmentation, Aug 2023. URL https://unicode.org/reports/\ntr29/.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:\nA multi-task benchmark and analysis platform for natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\nNLP, pp. 353\u2013355, Brussels, Belgium, November 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language\nUnderstanding Systems. Curran Associates Inc., Red Hook, NY, USA, 2019.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-\nger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstrations, pp. 38\u201345, Online, October 2020.\nAssociation for Computational Linguistics.\ndoi: 10.18653/v1/2020.emnlp-demos.6.\nURL\nhttps://aclanthology.org/2020.emnlp-demos.6.\nM. Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke\nZettlemoyer, and Ves Stoyanov. Training trajectories of language models across scales. In\nAnnual Meeting of the Association for Computational Linguistics, 2022. URL https://api.\nsemanticscholar.org/CorpusID:254877112.\nSavvas Zannettou, Barry Bradlyn, Emiliano De Cristofaro, Haewoon Kwak, Michael Sirivianos,\nGianluca Stringini, and Jeremy Blackburn. What is gab: A bastion of free speech or an alt-\nright echo chamber. In Companion Proceedings of the The Web Conference 2018, WWW \u201918,\npp. 1007\u20131014, Republic and Canton of Geneva, CHE, 2018. International World Wide Web\nConferences Steering Committee. ISBN 9781450356404. doi: 10.1145/3184558.3191531. URL\nhttps://doi.org/10.1145/3184558.3191531.\nA\nEVALUATION DATA SOURCE DETAILS\nC4\nInitially the pretraining corpus used by Raffel et al. (2019) and later released in Dodge et al.\n(2021), C4 has become one of the most commonly used pretraining corpora and is often included in\nmore recently curated corpora. It uses a single April 2019 Common Crawl scrape to source webtext.\nThis is filtered to remove text that is not classified as English as well as heuristics to remove text that\nis not natural language and a blocklist of profane keywords. We sample from the validation split of\nthis \"cleaned\" corpus to measure model fit to webtext from a single temporal slice of scraping with\nbaseline preprocessing. This source has no marked domains.\nMC4-EN\nChung et al. (2023) release a dataset with same the methods used in C4 but scale up to all\nCommon Crawl scrapes up to August 2022 and include 107 classified languages. As the scope of the\npresent work is the evaluation of English language models we sample only from the validation split\nof the English portion of the data. This allow us to measure the fit of models to scraped webtext with\nheterogeneous temporality. This source has no marked domains.\nTHE PILE\nGao et al. (2020) curate a pretraining corpus from 22 domains in one of the first large\nopen corpora to include mostly non-webscraped text, such as archives of novels or academic papers.\nIt is also explicitly framed as a language modeling benchmark with instructions for standardized\nevaluations on the validation and test sets, and several open source models have been trained on it\n(Wang & Komatsuzaki, 2021; Black et al., 2022; Biderman et al., 2023). We sample its 22 domains\n(see Table 5) to evaluate fit to coarse-grained, mostly non-webscraped domains.\n28\nArXiv, BookCorpus2, Books3, DM_Mathematics, Enron_Emails, EuroParl, FreeLaw, Github, Gutenberg_PG-19, HackerNews, NIH_ExPorter, OpenSub-\ntitles, OpenWebText2, PhilPapers, Pile-CC, PubMed_Abstracts, PubMed_Central, StackExchange, USPTO_Backgrounds, Ubuntu_IRC, Wikipedia_en,\nYoutubeSubtitles\nTable 5: Domains in THE PILE\nWIKITEXT-103 and PENN TREEBANK\nWe include these two benchmarks as they have seen the\nmost consistent evaluation on large LMs. WIKITEXT-103 (Merity et al., 2016) consists Wikipedia\narticles marked \u201cGood\u201d and \u201cFeatured\u201d and was used in the evaluation of GPT-2 (Radford et al.,\n2019), Gopher (Rae et al., 2021), and Chinchilla (Hoffmann et al., 2022). PENN TREEBANK (Marcus\net al., 1999) consists of 1989 Wall Street Journal articles originally annotated for linguistic structure.\nGPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020) omit these annotations and evaluate\nperplexity on the underlying text. We sample the same version of the benchmark, which is hosted by\nNunes (2020). As was standard practice at the time the benchmark is pretokenized and uncommon\nwords are replaced with a special unknown token; we opt not to detokenize this data as we find\ncontemporary LMs are often able to achieve comparable performance to the GPT-3 SOTA without\nthis. These two sources have no marked domains.\nREDPAJAMA\nTogether Computer (2023) reproduce a pretraining corpus following the data mixture\nof LLaMA (Touvron et al., 2023), which combines curated sources and webscraped text similarly\nto THE PILE but with a much greater portion of scraped data as has become customary in recent\npretraining corpora. This dataset is used to train RedPajama-INCITE (Together Computer, 2023),\none of the few models with both checkpoints and data publicly available. We sample their 7 domains\n(see Table 6).\narxiv, books, c4, commoncrawl, github, stackexchange, wikipedia\nTable 6: Domains in REDPAJAMA\nFALCON REFINEDWEB\nIncluded in the training of the Falcon models (Almazrouei et al., 2023),\nPenedo et al. (2023) collect a corpus of English sampled from all Common Crawl scrapes until June\n2023. While we include other Common Crawl based corpora, this one has a higher duplication\nremoval rate than previous corpora. They also claim to have more neutral filters that rely on simple\ninterpretable heuristics and only blocklist adult content by URLs. We sample this to examine how\ndifferences in filtering scraped data influence perplexity evaluations. This source has no marked\ndomains.\nDOLMA\nSoldaini et al. (2023) curate a corpus from Common Crawl, Wikipedia, books, academic\npapers, code repositories, and Reddit\u2014domains similar to those used to train most contemporary\nLLMs. They release the code used to collect and process this data which in combination with the\ncorpus serve as a set of scientific artifacts to support broader participation in research on pretraining\ndata. We sample from held out splits of each of these domains (see Table 7) to provide corresponding\nevaluations for these artifacts.\nbooks, common-crawl, pes2o, reddit_uniform, stack_uniform, wiki\nTable 7: Domains in DOLMA\nM2D2 S2ORC\nReid et al. (2022) collect academic papers from S2ORC (Lo et al., 2020) and\norganize them into a two level hierarchy by academic field categories. Top-level domains, such as\nComputer Science, are already provided in S2ORC using top-level disciplines from the Microsoft\nAcademic Graph (Shen et al., 2018), while subdomains are identified by a paper\u2019s arXiv category,\nsuch as the subdomain Computation and Language within Computer Science. As academic papers\nare a common source for pretraining and a domain for downstream use, we sample from this corpus\nto measure fine-grained fit to different academic disciplines. We sample both their top-level domains\nand lower-level subdomains, as our definition of domain accepts that domains may overlap. Also\nnote that while the M2D2 paper only reports 106 domains and subdomains of S2ORC data, we\n29\nfind that there are actually 167 domains and subdomains (see Table 8) marked in their final corpus.\nUnfortunately the original collection concatenates together all papers, making it impossible to recover\ndocument boundaries. We resort instead to sampling a given number of tokens from the beginning of\nthe concatenated sequences as one long pseudo-document, relying on the random shuffling of the\noriginal data before concatenation.\nArt, Philosophy, astro-ph, astro-ph.CO, astro-ph.EP, astro-ph.GA, astro-ph.HE, astro-ph.IM, astro-ph.SR, astro-ph_l1, atom-ph, chem-ph, cond-mat,\ncond-mat.dis-nn, cond-mat.mes-hall, cond-mat.mtrl-sci, cond-mat.other, cond-mat.quant-gas, cond-mat.soft, cond-mat.stat-mech, cond-mat.str-el, cond-\nmat.supr-con, cond-mat_l1, cs.AI, cs.AR, cs.CC, cs.CE, cs.CG, cs.CL, cs.CR, cs.CV, cs.CY, cs.DB, cs.DC, cs.DL, cs.DM, cs.DS, cs.ET, cs.FL, cs.GL,\ncs.GR, cs.GT, cs.HC, cs.IR, cs.LG, cs.LO, cs.MA, cs.MM, cs.MS, cs.NA, cs.NE, cs.NI, cs.OH, cs.OS, cs.PF, cs.PL, cs.RO, cs.SC, cs.SD, cs.SE, cs.SI,\ncs.SY, cs_l1, econ.EM, econ.TH, econ_l1, eess.AS, eess.IV, eess.SP, eess_l1, gr-qc, hep-ex, hep-lat, hep-ph, hep-th, math.AC, math.AG, math.AP, math.AT,\nmath.CA, math.CO, math.CT, math.CV, math.DG, math.DS, math.FA, math.GM, math.GN, math.GR, math.GT, math.HO, math.KT, math.LO, math.MG,\nmath.NA, math.NT, math.OA, math.OC, math.PR, math.QA, math.RA, math.RT, math.SG, math.SP, math_l1, nlin.AO, nlin.CD, nlin.CG, nlin.PS, nlin.SI,\nnlin_l1, nucl-ex, nucl-th, physics.acc-ph, physics.ao-ph, physics.app-ph, physics.atm-clus, physics.atom-ph, physics.bio-ph, physics.chem-ph, physics.class-\nph, physics.comp-ph, physics.data-an, physics.ed-ph, physics.flu-dyn, physics.gen-ph, physics.geo-ph, physics.hist-ph, physics.ins-det, physics.med-ph,\nphysics.optics, physics.plasm-ph, physics.pop-ph, physics.soc-ph, physics.space-ph, physics_l1, plasm-ph, q-bio, q-bio.BM, q-bio.CB, q-bio.GN, q-bio.MN,\nq-bio.NC, q-bio.OT, q-bio.PE, q-bio.QM, q-bio.SC, q-bio.TO, q-bio_l1, q-fin.CP, q-fin.EC, q-fin.GN, q-fin.MF, q-fin.PM, q-fin.PR, q-fin.RM, q-fin.ST,\nq-fin.TR, q-fin_l1, quant-ph, stat.AP, stat.CO, stat.ME, stat.ML, stat.OT, stat_l1, supr-con\nTable 8: Domains in M2D2 S2ORC\nM2D2 WIKIPEDIA\nReid et al. (2022) also collect Wikipedia articles and organize them by the top\ntwo levels of hierarchy from the Wikipedia ontology. We sample from this source, as the Wikipedia\nontology provides some of the largest scale human categorization of domains of text available on\na data source almost always included in pretraining corpora. This time we find that their corpus\ncontains just 49 marked domains or subdomains (see Table 9), rather than the 60 mentioned in the\npaper. Again the original collection concatenates articles together, so we sample a given number of\ntokens from the beginning of this concatenated sequence.\nCulture_and_the_arts, Culture_and_the_arts__Culture_and_Humanities, Culture_and_the_arts__Games_and_Toys, Culture_and_the_arts__Mass_media,\nCulture_and_the_arts__Performing_arts, Culture_and_the_arts__Sports_and_Recreation, Culture_and_the_arts__The_arts_and_Entertainment, Cul-\nture_and_the_arts__Visual_arts,\nGeneral_referece,\nGeneral_referece__Further_research_tools_and_topics,\nGeneral_referece__Reference_works,\nHealth_and_fitness,\nHealth_and_fitness__Exercise,\nHealth_and_fitness__Health_science,\nHealth_and_fitness__Human_medicine,\nHealth_and_fitness__Nutrition,\nHealth_and_fitness__Public_health,\nHealth_and_fitness__Self_care,\nHistory_and_events,\nHis-\ntory_and_events__By_continent, History_and_events__By_period, History_and_events__By_region, Human_activites, Human_activites__Human_activities,\nHuman_activites__Impact_of_human_activity,\nMathematics_and_logic,\nMathematics_and_logic__Fields_of_mathematics,\nMathemat-\nics_and_logic__Logic,\nMathematics_and_logic__Mathematics,\nNatural_and_physical_sciences,\nNatural_and_physical_sciences__Biology,\nNat-\nural_and_physical_sciences__Earth_sciences,\nNatural_and_physical_sciences__Nature,\nNatural_and_physical_sciences__Physical_sciences,\nPhilosophy_and_thinking,\nPhilosophy_and_thinking__Philosophy,\nPhilosophy_and_thinking__Thinking,\nReligion_and_belief_systems,\nReli-\ngion_and_belief_systems__Allah,\nReligion_and_belief_systems__Belief_systems,\nReligion_and_belief_systems__Major_beliefs_of_the_world,\nSociety_and_social_sciences, Society_and_social_sciences__Social_sciences, Society_and_social_sciences__Society, Technology_and_applied_sciences,\nTechnology_and_applied_sciences__Agriculture, Technology_and_applied_sciences__Computing, Technology_and_applied_sciences__Engineering,\nTechnology_and_applied_sciences__Transport\nTable 9: Domains in M2D2 WIKIPEDIA\nC4-100-DOMAINS\nChronopoulou et al. (2022) collect C4-100-DOMAINS comprising all the text\nfrom 100 internet domains with the most pages in C4. We sample from each of the 100 domains (see\nTable 10) to explore the relationship between how well represented and how surprising a domain is.\nThe original collection removes documents smaller than 200 whitespace separated tokens, leading\nthe domain with the 3rd most pages (do5.b00kmedia.ru) to be completely empty. Only three other\ndomains have less data than the 100 thousand tokens per split that we aim for.\nDOLMA-100-SUBREDDITS\nUsing the Reddit data collected in DOLMA (Soldaini et al., 2023), we\norganize a new corpus of the top 100 subreddits (community forums within the messageboard) ranked\nby number of posts in the DOLMA data (see Table 11). In DOLMA Reddit posts are each separate\ndocuments, without any linearization of conversational threads. Though this prevents the assessment\nof model fit to dialogue, it still allows evaluation across these many domains of social media text.\nThe DOLMA Reddit data also filters out comments shorter than 500 characters and submissions (i.e.,\noriginal posts) shorter than 400 characters. We sample these subreddits to capture domains as they\nare self-organized and self-identified by online communities.\nDOLMA-100-PROGRAMMING-LANGUAGES\nUsing code repository data from THE STACK (Ko-\ncetkov et al., 2022) as it is contained in DOLMA (Soldaini et al., 2023), we collect a new corpus of\nbalanced samples of the top one hundred programming languages by number of tokens (see Table 12).\n30\n100_www.ign.com, 10_www.eventbrite.com, 11_link.springer.com, 12_www.chicagotribune.com, 13_www.foxnews.com, 14_www.aljazeera.com,\n15_www.dailymail.co.uk,\n16_www.ncbi.nlm.nih.gov,\n17_www.express.co.uk,\n18_en.m.wikipedia.org,\n19_www.cnet.com,\n1_www.nytimes.com,\n20_www.telegraph.co.uk,\n21_www.theatlantic.com,\n22_forums.macrumors.com,\n23_www.oreilly.com,\n24_www.washingtonpost.com,\n25_www.zdnet.com,\n26_www.foxbusiness.com,\n27_www.reuters.com,\n28_www.ibtimes.co.uk,\n29_www.rt.com,\n2_en.wikipedia.org,\n30_www.prweb.com,\n31_www.deviantart.com,\n32_www.si.com,\n33_www.bbc.com,\n34_github.com,\n35_nypost.com,\n36_itunes.apple.com,\n37_www.instructables.com, 38_www.youtube.com, 39_www.booking.com, 40_www.etsy.com, 41_www.marketwired.com, 42_sites.google.com,\n43_www.baltimoresun.com,\n44_www.agreatertown.com,\n45_www.npr.org,\n46_www.fool.com,\n47_www.tripadvisor.com,\n48_www.bbc.co.uk,\n49_lists.w3.org, 4_www.latimes.com, 50_mashable.com, 51_disneyparksmomspanel.disney.go.com, 52_www.cnbc.com, 53_answers.sap.com, 54_home-\nstars.com, 55_www.hindustantimes.com, 56_www.reference.com, 57_www.city-data.com, 58_medium.com, 59_app-wiringdiagram.herokuapp.com,\n5_www.theguardian.com, 60_www.csmonitor.com, 61_www.adweek.com, 62_docs.microsoft.com, 63_www.yahoo.com, 64_www.thesun.co.uk,\n65_www.nydailynews.com, 66_www.dailystar.co.uk, 67_fineartamerica.com, 68_www.kickstarter.com, 69_uk.reuters.com, 6_www.huffpost.com,\n70_www.insiderpages.com, 71_www.inquisitr.com, 72_lists.debian.org, 73_www.straitstimes.com, 74_www.cbsnews.com, 75_simple.wikipedia.org,\n76_deadline.com, 77_www.androidheadlines.com, 78_www.wired.com, 79_www.bustle.com, 7_patents.google.com, 80_premium.wpmudev.org,\n81_www.librarything.com, 82_mail-archives.apache.org, 83_scholars.duke.edu, 84_www.glassdoor.com, 85_www.pcworld.com, 86_www.shutterstock.com,\n87_myemail.constantcontact.com,\n88_www.eventbrite.co.uk,\n89_www.fastcompany.com,\n8_www.businessinsider.com,\n90_www.firstpost.com,\n91_www.entrepreneur.com,\n92_www.breitbart.com,\n93_techcrunch.com,\n94_www.nme.com,\n95_www.ndtv.com,\n96_finance.yahoo.com,\n97_archives.lib.state.ma.us, 98_www.gsmarena.com, 99_www.lonelyplanet.com, 9_www.forbes.com\nTable 10: Domains in C4-100-DOMAINS\n00_AskReddit, 01_politics, 02_AmItheAsshole, 03_worldnews, 04_relationships, 05_relationship_advice, 06_news, 07_leagueoflegends, 08_todayilearned,\n09_TwoXChromosomes, 10_personalfinance, 11_changemyview, 12_unpopularopinion, 13_movies, 14_Games, 15_nba, 16_pics, 17_gaming, 18_soccer,\n19_nfl, 20_explainlikeimfive, 21_conspiracy, 22_atheism, 23_AskMen, 24_videos, 25_sex, 26_raisedbynarcissists, 27_NoStupidQuestions, 28_Des-\ntinyTheGame, 29_anime, 30_DnD, 31_ukpolitics, 32_funny, 33_europe, 34_canada, 35_Christianity, 36_SquaredCircle, 37_AskWomen, 38_legaladvice,\n39_JUSTNOMIL, 40_technology, 41_IAmA, 42_wow, 43_Parenting, 44_exmormon, 45_AdviceAnimals, 46_childfree, 47_unitedkingdom, 48_ffxiv,\n49_dndnext, 50_ADHD, 51_loseit, 52_asoiaf, 53_BabyBumps, 54_Advice, 55_australia, 56_CFB, 57_offmychest, 58_PublicFreakout, 59_TrueOffMyChest,\n60_science, 61_magicTCG, 62_asktransgender, 63_DotA2, 64_neoliberal, 65_whowouldwin, 66_depression, 67_WTF, 68_pathofexile, 69_PoliticalDis-\ncussion, 70_Libertarian, 71_PurplePillDebate, 72_Fitness, 73_books, 74_dogs, 75_pcmasterrace, 76_teenagers, 77_stopdrinking, 78_Overwatch, 79_tele-\nvision, 80_buildapc, 81_askscience, 82_programming, 83_Guildwars2, 84_cars, 85_formula1, 86_sysadmin, 87_hockey, 88_india, 89_SubredditDrama,\n90_DMAcademy, 91_dating_advice, 92_Catholicism, 93_Drugs, 94_trees, 95_boardgames, 96_Conservative, 97_Futurology, 98_beyondthebump, 99_wed-\ndingplanning\nTable 11: Domains in DOLMA-100-SUBREDDITS\nWhile code data differs greatly from natural language, complicating the interpretation of perplexity\nanalysis, we nevertheless wish to add evaluations to cover this common data source for LLMs.\n00_text, 01_markdown, 02_c, 03_php, 04_java, 05_c++, 06_python, 07_javascript, 08_html, 09_c#, 10_yaml, 11_go, 12_typescript, 13_xml, 14_css,\n15_jupyter-notebook, 16_rust, 17_unity3d-asset, 18_gettext-catalog, 19_ruby, 20_vue, 21_sql, 22_swift, 23_kotlin, 24_scala, 25_scss, 26_tex, 27_dart,\n28_kicad, 29_shell, 30_smali, 31_lua, 32_restructuredtext, 33_perl, 34_diff, 35_ini, 36_jsx, 37_haskell, 38_gnuplot, 39_postscript, 40_groff, 41_turtle,\n42_fortran, 43_makefile, 44_mathematica, 45_pascal, 46_common-lisp, 47_gas, 48_vhdl, 49_julia, 50_edn, 51_visual-basic, 52_powershell, 53_g-code,\n54_ocaml, 55_java-server-pages, 56_solidity, 57_graphviz-dot, 58_less, 59_twig, 60_asciidoc, 61_groovy, 62_llvm, 63_hcl, 64_html+erb, 65_erlang,\n66_elixir, 67_eagle, 68_arduino, 69_coffeescript, 70_toml, 71_cuda, 72_nix, 73_smalltalk, 74_cmake, 75_actionscript, 76_glsl, 77_systemverilog, 78_haxe,\n79_f#, 80_max, 81_objective-c++, 82_standard-ml, 83_dockerfile, 84_emacs-lisp, 85_scheme, 86_clojure, 87_handlebars, 88_smarty, 89_logos, 90_stata,\n91_yacc, 92_nimrod, 93_tcl, 94_viml, 95_asp, 96_protocol-buffer, 97_r, 98_cython, 99_mediawiki\nTable 12: Domains in DOLMA-100-PROGRAMMING-LANGUAGES\nICE\nLocal research teams following guidelines established in Greenbaum & Nelson (1996) col-\nlected corpora of English from Canada, East Africa (Kenya & Tanzania), Hong Kong, India, Ireland,\nJamaica, Philippines, Singapore, and the USA. Each of these samples of English from around the\nworld is further split into a written and transcribed spoken corpus, except for USA which only has\nwritten data (see Table 13). We follow HELM (Liang et al., 2022) in utilizing this corpus to measure\ndisparate performance between these dialects. To permit comparability to HELM, we follow the\nsame preprocessing which leaves in some XML-style tags marking phenomena such as speaker turns.\nTWITTERAAE\nBlodgett et al. (2016) create a pair of corpora representing African-American\nand White-aligned English using a statistical model with distant supervision from geolocation and\ndemographic census statistics. We follow the reproduction of this dataset used in HELM (Liang et al.,\n2022), but we fix an error in loading escaped sequences of the data that, among other issues, renders\nemojis as literal hexadecimal bytes. Our reproduction is not able to sample the same documents, but\nis otherwise identical. We sample these corpora to examine disparities in performance on minoritized\ndialects (see Table 14).\nMANOSPHERE CORPUS\nRibeiro et al. (2021) curate a corpus of texts spanning 2006 to 2019\nscrapped from 9 forums sharing a masculinist ideology: 8 independent message boards as well as\n56 subreddits on Reddit. Using a toxicity classifier and lexicon-based misogyny metric, they find\nan increase in toxicity and hate over time to levels far above mainstream Reddit and comparable\nto 4CHAN CORPUS. We sample this corpus to measure fit to a discourse with a specific variety of\n31\nCANADA_S_ALL, CANADA_W_ALL, EAST_AFRICA_S_ALL, EAST_AFRICA_W_ALL, HONG_KONG_S_ALL, HONG_KONG_W_ALL, IN-\nDIA_S_ALL, INDIA_W_ALL, IRELAND_S_ALL, IRELAND_W_ALL, JAMAICA_S_ALL, JAMAICA_W_ALL, PHILIPPINES_S_ALL, PHILIP-\nPINES_W_ALL, SINGAPORE_S_ALL, SINGAPORE_W_ALL, USA_W_ALL\nTable 13: Domains in ICE\nAA, white\nTable 14: Domains in TWITTERAAE\ntoxicity focused on hate towards women. Moreover we intend this to exemplify how domain expertise\nallows the manual curation of a corpus to represent a whole discourse using known relationships\nbetween sources. The original data already linearizes the posts into a sequential thread, which we\nconcatenate together with post authors prepended to posts. Though this datasets marks 9 domains\n(see Table 15), we opt to treat this whole source as a single domain for the present analysis and thus\ndo not perform a stratified sample of these domains.\navfm, incels, love_shy, mgtow, pua_forum, red_pill_talk, reddit, rooshv, the_attraction\nTable 15: Domains in MANOSPHERE CORPUS\nGAB CORPUS\nZannettou et al. (2018) scrape posts from August 2016 and January 2018 on Gab,\nan alt-right focused Twitter alternative founded in 2016. The platform emphasizes freedom of speech\nand minimal moderation, with notable users joining after being banned from mainstream social media.\nThe authors find that GAB CORPUS measures higher than Twitter but lower than 4CHAN CORPUS on\na lexicon of hate words. We sample this corpus to measure fit to low moderation social media. We\ntreat posts as independent documents, rather than attempting to reconstruct connected subgraphs of\nposts replying to other posts. This source has no marked domains.\n4CHAN CORPUS\nPapasavva et al. (2020) collect posts between June 2016 and November 2019\nfrom the Politically Incorrect board (/pol/) of 4chan, a fringe imageboard emphasizing anonymity and\nephemerality. Users can post content without registering, with a thread consisting of an image and\nmessage followed by a sequence comments. Threads are deleted shortly after they become inactive.\nAs noted previously, 4CHAN CORPUS has toxicity and mysogynist hate comparable to the worst data\nin MANOSPHERE CORPUS and hatespeech above GAB CORPUS. We sample this corpus to measure\nfit to types of discourse and toxicity that can arise from anonymous posting. We concatenate posts in\na thread together with post metadata prepended as a header. This source has no marked domains.\nB\nREWEIGHTING PERPLEXITIES\nEven though we sample equal token counts for each domain, sometimes users of PALOMA may\nwish to compute a perplexity over the original distribution of domains in standard corpora such as\nTHE PILE to compare to previous evaluations that do a uniform instead of stratified sample of these\nsources. We do not use such reweighted numbers in this paper, but we explain here how one might do\nthis if desired. Instead of having to run inference twice for each source (e.g., a copy of THE PILE\nsampled uniformly as well as a stratified sample by domain), one can compute a perplexity with the\nalready computed average negative log likelihood per domain NLLd,c. Formally, for each domain\nd \u2208 D within a corpus c, consisting of a set of documents Nd,c = {t1, . . . , t|Nd,c|}, with T(Nd,c)\ndenoting the number of tokens in that domain (i.e., T(Nd,c) = P\nt\u2208Nd,c | tokenize(t) |) the NLLd,c\nis computed as:\nNLLd,c = \u2212\n1\nT(Nd,c)\nX\nt\u2208Nd,c\n|t|\nX\ni=1\nln p(ti|t<i)\n(6)\nWe have NLLd,c where c is a source in PALOMA where each domain is represented by the same\nnumber of tokens. However if we want perplexity for some other corpus c\u2032 with a different distribution\n32\nof domains, we can use its ratio of tokens in a domain to total tokens, \u03b1d,c\u2032, to reweight domains:\n\u03b1d,c\u2032 =\nT(Nd,c\u2032)\nP\nd\u2032\u2208D\nT(Nd\u2032,c\u2032)\n(7)\nNow we can compute the perplexity for the domain distribution of c\u2032.\nperplexity = exp\n X\nd\u2208D\n\u03b1d,c\u2032NLLd,c\n!\n(8)\nC\nBASELINE MODELS\nThe 6 baseline 1B parameter models that we train employ the following architecture: 2048 maximum\nsequence length, 2048 model dimension, 16 layers, 16 attention heads, RoPE embedding (Su et al.,\n2021), SwiGLU activation (Shazeer, 2020), mixed precision, non-parametric layer normalization,\nand sequential model blocks for attention and feed-forward networks. We use EleutherAI\u2019s GPT\nNeoX tokenizer (Black et al., 2022) but add 3 additional special tokens that are used to mask PII in\nDOLMA. We train to 35k steps (\u223c150B tokens) with the following LionW optimizer (Chen et al.,\n2023) configurations: 2.0e-4 peak learning rate, warm-up of 2000 steps, cosine decay to 70k steps\n(\u223c300B tokens), 0.1 weight decay, and betas of 0.9 and 0.95. Note that our batch size varies slightly\nto accommodate two groups of baselines that were run on different hardware. The DOLMA and\nFALCON REFINEDWEB baselines were run with a batch size of 2112 training instances per step on 24\nA100s for 9 days per model. The REDPAJAMA, THE PILE, C4, and MC4-EN baselines were run\nwith a batch size of 2048 on 64 AMD Instinct MI250X GPUs8 for 2 days per model. In each case we\nsave model checkpoints every 5k steps (\u223c20B tokens).\nD\nFORMATTING AND SUBSAMPLING\nEvaluation Subset Tokens\n4M\n8M\n12M\n16M\n20M\n40M\nTrain Toks\nConcat\n2B\n92.23 +- 17.33\n87.05 +- 1.82\n86.06 +- 7.41\n95.11 +- 26.34\n94.91 +- 20.23\n77.49 +- 2.34\n26B\n21.58 +- 3.48\n19.93 +- 2.67\n20.24 +- 5.09\n22.2 +- 2.24\n22.9 +- 2.15\n21.61 +- 2.02\n86B\n17.94 +- 2.02\n19.76 +- 0.79\n20.36 +- 2.23\n19.61 +- 1.67\n20.25 +- 1.72\n20.25 +- 2.43\n286B\n16.55 +- 0.91\n17.77 +- 1.91\n16.7 +- 3.36\n14.68 +- 1.86\n17.12 +- 1.98\n20.07 +- 3.25\nNot concat\n2B\n42.57 \u00b1 0.29\n42.67 \u00b1 0.14\n42.73 \u00b1 0.16\n42.66 \u00b1 0.10\n42.69 \u00b1 0.14\n42.73 \u00b1 0.09\n26B\n21.98 \u00b1 0.16\n22.02 \u00b1 0.08\n22.04 \u00b1 0.09\n22.00 \u00b1 0.04\n22.01 \u00b1 0.06\n22.03 \u00b1 0.05\n86B\n18.52 \u00b1 0.13\n18.55 \u00b1 0.07\n18.57 \u00b1 0.07\n18.54 \u00b1 0.03\n18.55 \u00b1 0.05\n18.56 \u00b1 0.04\n286B\n16.14 \u00b1 0.11\n16.18 \u00b1 0.06\n16.19 \u00b1 0.06\n16.16 \u00b1 0.03\n16.17 \u00b1 0.04\n16.18 \u00b1 0.03\nTable 16: Average perplexity over 4 subsets of C4 validation data using Pythia 1.4B checkpoints.\nOn top, inputs are maximum-sequence-length concatenations of random documents drawn from 4\ndifferent seeds in each cell. On bottom, random documents drawn from the same 4 seeds in all cells\nare evaluated separately.\nWe find preliminary evidence that the monotonic decrease in variability with increased evaluation\nor training data (see \u00a73.2.1) depends on using the non-concatenated inference input format detailed\nin \u00a73.2.3. In Table 16 we see that the previously observed trends break down when inputs are\nconcatenated. Additionally, the concatenated documents are drawn from 4 random shufflings where\nthe 4 seeds change for each cell. For comparison the bottom of the table shows results when\ndocuments are evaluated separately and with the same set of 4 random seeds for all cells. In both\ninput formats documents that are longer than the model context window are split into separate inputs\nwith no overlap.\nWe hypothesize that the trends differ between the concatenated and not concatenated formats because\ndocuments are interrupted at the start and end of concatenated instances. The location of this split will\ndepend on the lengths of the other randomly selected documents included in the concatenation. In the\nnon-concatenated format, documents can still be split if they exceed the maximum sequence length,\n8amd.com/en/products/server-accelerators/instinct-mi250x\n33\nbut the location of the split will be the same across all random shufflings. However it is possible that\nother factors such as influence across document boundaries in concatenated inputs might play a role,\nor simply that changing the random seeds between each cell discovers more of the most unlucky,\noutlier seeds.\nE\nMOST AND LEAST IMPROVED DOMAINS\nIn \u00a74.2 we show that improvement of LM fit when scaling is unequal from domain to domain. Differ-\nences in improvement rates can actually indicate several different training dynamics, exemplified\nin Figure 6. Looking at performance curves over the underlying factor of scale, helps show more\nspecifically what is going on. Examining the domains at the extreme values of improvement rate is\none way to surface interesting details of model fit. In Figure 13 we examine performance curves of the\nmost and least improved domains with respect to number of tokens seen, \u2206t(\u223c 20B, \u223c 150B), and\nin Figure 14 we examine the most and least improved with respect to number of model parameters,\n\u2206p(85M, 805M) and \u2206p(805M, 6.4B).\n34\n20\n65\n150\n11.7\n20.2\n39.2\nThe Pile\nDolma v1.5 1B\nOpenSubtitles\nEuroParl\n20\n65\n150\n3.4\n6.2\n15.2\nThe Pile 1B\nDM_Mathematics\nYoutubeSubtitles\n20\n65\n150\n8.3\n32.8\n315.6 13.2K\n6.2M\nFalcon-RefinedWeb 1B\nUbuntu_IRC\nStackExchange\n20\n65\n150\n15.2\n57.7\n424.0\n8.3K\nC4 1B\nArXiv\nEuroParl\n20\n65\n150\n9.3\n27.7\n141.6\n1.6K\n61.3K\nmC4 1B\nUbuntu_IRC\nPubMed_Central\n20\n65\n150\n10.4\n13.3\n17.4\n23.5\n32.8\nRedPajama 1B\nOpenSubtitles\nUbuntu_IRC\n20\n65\n150\n2.2\n3.6\n8.3\n32.8\nRedPajama\nbooks\ngithub\n20\n65\n150\n2.0\n2.7\n4.4\n9.3\n27.7\nbooks\ngithub\n20\n65\n150\n9.3\n27.7\n141.6\nc4\ngithub\n20\n65\n150\n8.3\n32.8\n315.6 13.2K\n6.2M\narxiv\nwikipedia\n20\n65\n150\n8.3\n10.4\n13.3\ngithub\narxiv\n20\n65\n150\n2.7\n4.4\n9.3\n27.7\nc4\ngithub\n20\n65\n150\n2.7\n4.4\n9.3\n27.7\nDolma V1.5\nreddit_uniform\nstack_uniform\n20\n65\n150\n2.7\n4.4\n9.3\n27.7\nreddit_uniform\nstack_uniform\n20\n65\n150\n15.2\n27.7\n57.7\nreddit_uniform\nstack_uniform\n20\n65\n150\n11.7\n20.2\n39.2\n88.4\nstack_uniform\nwiki\n20\n65\n150\n18.7\n21.8\n25.5\n30.1\ncommon-crawl\nbooks\n20\n65\n150\n4.4\n9.3\n27.7\nreddit_uniform\nstack_uniform\n20\n65\n150\n14.2\n16.2\n18.7\n21.8\nM2D2 S2ORC\nstat.ME\nhep-ex\n20\n65\n150\n10.4\n13.3\n17.4\nstat.ME\nhep-ex\n20\n65\n150\n9.3\n15.2\n27.7\nstat.ME\nmath.GR\n20\n65\n150\n16.2\n18.7\n21.8\n25.5\n30.1\ncs.GR\nmath.CT\n20\n65\n150\n13.3\n17.4\n23.5\n32.8\ncs.CY\nstat.ME\n20\n65\n150\n7.5\n11.7\n20.2\nstat.ME\nmath.GR\n20\n65\n150\n10.4\n13.3\n17.4\n23.5\nM2D2 Wikipedia\nVisual_arts\nEarth_sciences\n20\n65\n150\n8.3\n10.4\n13.3\n17.4\n23.5\nVisual_arts\nEarth_sciences\n20\n65\n150\n11.7\n15.2\n20.2\nVisual_arts\nEarth_sciences\n20\n65\n150\n13.3\n17.4\n23.5\n32.8\nHuman_activites\nNutrition\n20\n65\n150\n13.3\n17.4\n23.5\nVisual_arts\nEarth_sciences\n20\n65\n150\n9.3\n11.7\n15.2\n20.2\nVisual_arts\nEarth_sciences\n20\n65\n150\n7.5\n11.7\n20.2\n39.2\nC4 100 Domains\n31_www.devianta\nrt.com\n59_app-wiringdi\nagram.herokuapp.com\n20\n65\n150\n7.5\n11.7\n20.2\n39.2\n49_lists.w3.org\n97_archives.lib\n.state.ma.us\n20\n65\n150\n7.5\n11.7\n20.2\n39.2\n31_www.devianta\nrt.com\n16_www.ncbi.nlm\n.nih.gov\n20\n65\n150\n6.2\n9.3\n15.2\n27.7\n31_www.devianta\nrt.com\n97_archives.lib\n.state.ma.us\n20\n65\n150\n6.2\n15.2\n57.7\n424.0\n94_www.nme.com\n97_archives.lib\n.state.ma.us\n20\n65\n150\n6.2\n9.3\n15.2\n27.7\n57.7\n53_answers.sap.com\n97_archives.lib\n.state.ma.us\n20\n65\n150\n16.9\n19.0\n21.4\n24.3\n100 Subreddits\n66_depression\n80_buildapc\n20\n65\n150\n15.2\n20.2\n27.7\n39.2\n57.7\n28_DestinyTheGame\n35_Christianity\n20\n65\n150\n18.7\n21.8\n25.5\n30.1\n35.8\n66_depression\n80_buildapc\n20\n65\n150\n23.5\n32.8\n47.3\n71_PurplePillDebate\n81_askscience\n20\n65\n150\n20.2\n27.7\n39.2\n57.7\n91_dating_advice\n81_askscience\n20\n65\n150\n23.5\n32.8\n47.3\n71.0\n68_pathofexile\n80_buildapc\n20\n65\n150\n1.2\n1.8\n5.2\n88.4\n100 PLs\n39_postscript\n99_mediawiki\n20\n65\n150\n1.2\n1.8\n5.2\n45_pascal\n30_smali\n20\n65\n150\n15.2\n57.7\n424.0\n8.3K\n80_max\n13_xml\n20\n65\n150\n9.3\n27.7\n141.6\n63_hcl\n48_vhdl\n20\n65\n150\n3.6\n8.3\n32.8\n315.6 13.2K\n80_max\n11_go\n20\n65\n150\n3.0\n3.9\n5.2\n17_unity3d-asset\n11_go\n20\n65\n150\n11.7\n15.2\n20.2\nICE\nJAMAICA_S_ALL\nSINGAPORE_W_ALL\n20\n65\n150\n11.7\n15.2\n20.2\nJAMAICA_S_ALL\nHONG_KONG_W_ALL\n20\n65\n150\n15.2\n20.2\n27.7\nIRELAND_S_ALL\nUSA_W_ALL\n20\n65\n150\n15.2\n57.7\n424.0\n8.3K\nHONG_KONG_S_ALL\nEAST_AFRICA_S_ALL\n20\n65\n150\n20.2\n27.7\n39.2\nJAMAICA_S_ALL\nPHILIPPINES_S_ALL\n20\n65\n150\n13.3\n15.2\n17.4\n20.2\nJAMAICA_S_ALL\nUSA_W_ALL\n20\n65\n150\n88.4\n141.6\n238.4\n424.0\nTwitter AAE\nAA\nwhite\n20\n65\n150\n88.4\n141.6\n238.4\n424.0\nAA\nwhite\n20\n65\n150\n88.4\n141.6\n238.4\n424.0\nwhite\nAA\n20\n65\n150\n88.4\n141.6\n238.4\n424.0\nAA\nwhite\n20\n65\n150\n88.4\n141.6\n238.4\n424.0\nAA\nwhite\n20\n65\n150\n88.4\n238.4\n801.0\nwhite\nAA\nTokens Seen (Billions)\nPerplexity\nFigure 13: Perplexity curves for the most and least improved domains over an increase in tokens seen\n(See \u00a74.2.1). Columns are specific baseline models; rows are specific evaluation sources.\n35\n108.0\n109.0\n1010.0\n1.7\n2.3\n3.4\nThe Pile\nFrom Pythia 1B to Pythia 7B\nDM_Mathematics\nGithub\n108.0\n109.0\n1010.0\n1.7\n2.3\n3.4\nFrom Pythia 160m to Pythia 1B\nDM_Mathematics\nGithub\n108.0\n109.0\n1010.0\n5.7\n6.8\n8.3\n10.4\n13.3\n17.4\nC4 100 Domains\nFrom Pythia 1B to Pythia 7B\n82_mail-archive\ns.apache.org\n97_archives.lib\n.state.ma.us\n108.0\n109.0\n1010.0\n7.5\n11.7\n20.2\n39.2\nFrom Pythia 160m to Pythia 1B\n49_lists.w3.org\n27_www.reuters.com\n108.0\n109.0\n1010.0\n1.7\n2.3\n3.4\n6.2\n15.2\n57.7\nRedPajama\nc4\ngithub\n108.0\n109.0\n1010.0\n1.7\n2.3\n3.4\n6.2\n15.2\n57.7\nbooks\ngithub\n108.0\n109.0\n1010.0\n15.2\n20.2\n27.7\n39.2\n100 Subreddits\n66_depression\n80_buildapc\n108.0\n109.0\n1010.0\n11.7\n20.2\n39.2\n88.4\n28_DestinyTheGame\n35_Christianity\n108.0\n109.0\n1010.0\n2.3\n3.4\n6.2\n15.2\n57.7\nDolma V1.5\nreddit_uniform\nstack_uniform\n108.0\n109.0\n1010.0\n2.3\n3.4\n6.2\n15.2\n57.7\nreddit_uniform\nstack_uniform\n108.0\n109.0\n1010.0\n1.2\n1.8\n5.2\n88.4\n100 PLs\n39_postscript\n62_llvm\n108.0\n109.0\n1010.0\n1.2\n1.8\n5.2\n88.4\n39_postscript\n30_smali\n108.0\n109.0\n1010.0\n7.5\n9.3\n11.7\n15.2\n20.2\nM2D2 S2ORC\nstat.ME\nhep-ex\n108.0\n109.0\n1010.0\n7.5\n9.3\n11.7\n15.2\n20.2\nstat.ME\nhep-ex\n108.0\n109.0\n1010.0\n9.3\n11.7\n15.2\n20.2\n27.7\nICE\nJAMAICA_S_ALL\nHONG_KONG_W_ALL\n108.0\n109.0\n1010.0\n9.3\n11.7\n15.2\n20.2\n27.7\nCANADA_S_ALL\nHONG_KONG_W_ALL\n108.0\n109.0\n1010.0\n5.2\n7.5\n11.7\n20.2\nM2D2 Wikipedia\nVisual_arts\nEarth_sciences\n108.0\n109.0\n1010.0\n5.2\n7.5\n11.7\n20.2\n39.2\nVisual_arts\nCulture_and_the_arts\n108.0\n109.0\n1010.0\n88.4\n141.6\n238.4\n424.0\n801.0\nTwitter AAE\nAA\nwhite\n108.0\n109.0\n1010.0\n88.4\n141.6\n238.4\n424.0\n801.0\nAA\nwhite\nNon-embedding Model Parameters\nPerplexity\nFigure 14: Perplexity curves for the most and least improved domains over an increase in model size\n(See \u00a74.2.2). Columns are comparisons of specific model sizes. Each row shows first one (left two\nsubplots) and then another (right two subplots) set of evaluation sources.\n36\n"
  },
  {
    "title": "MAG-Edit: Localized Image Editing in Complex Scenarios via Mask-Based Attention-Adjusted Guidance",
    "link": "https://arxiv.org/pdf/2312.11396.pdf",
    "upvote": "10",
    "text": "MAG-Edit: Localized Image Editing in Complex Scenarios via\nMask-Based Attention-Adjusted Guidance\nQi Mao1\nLan Chen1\nYuchao Gu2\nZhen Fang1\nMike Zheng Shou2\n1MIPG, Communication University of China\n2Show Lab, National University of Singapore\nhttps://mag-edit.github.io/\nSource image\nMAG-Edit (Ours)\nExisting Baseline Methods\nColor Edit: Sofa \u2192 Green Sofa\nTexture Edit: Vest\u2192 Denim Vest\nObject Edit: Fish \u2192 Tropical Fish\na)\nb)\nc)\nd)\ne)\nf)\na)\nb)\nc)\nd)\ne)\nf)\na)\nb)\nc)\nd)\ne)\nf)\nFigure 1.\nLocalized image editing in complex scenarios. We enhance the visibility of the editing regions by outlining their edges\nwith white dashed lines. Existing mask-based approaches, e.g., (a) Blended latent diffusion [2] and (b) DiffEdit [8], often modify the\nstructural details of the original edited areas, causing significant discrepancies with their surrounding context. Mask-free attention-based\nmethods, such as (c) Prompt2Prompt (P2P) [10] and (d) Plug-and-play (PnP) [28], exhibit leakage, becoming aligned with incorrect\nregions, resulting in inefficiencies in prospective areas. By incorporating blending operations, (e) P2P+Blend and (f) PnP+Blend can\nmitigate leakage; however, their editing effect in the prospective region still lacks efficiency. In contrast, our method MAG-Edit achieves\nlocalized image edits that align well with the target prompt and maintain structural integrity more effectively.\n1\narXiv:2312.11396v2  [cs.CV]  21 Dec 2023\nAbstract\nRecent diffusion-based image editing approaches have\nexhibited impressive editing capabilities in images with sim-\nple compositions. However, localized editing in complex\nscenarios has not been well-studied in the literature, de-\nspite its growing real-world demands. Existing mask-based\ninpainting methods fall short of retaining the underlying\nstructure within the edit region.\nMeanwhile, mask-free\nattention-based methods often exhibit editing leakage and\nmisalignment in more complex compositions. In this work,\nwe develop MAG-Edit, a training-free, inference-stage op-\ntimization method, which enables localized image editing in\ncomplex scenarios. In particular, MAG-Edit optimizes the\nnoise latent feature in diffusion models by maximizing two\nmask-based cross-attention constraints of the edit token,\nwhich in turn gradually enhances the local alignment with\nthe desired prompt. Extensive quantitative and qualitative\nexperiments demonstrate the effectiveness of our method in\nachieving both text alignment and structure preservation for\nlocalized editing within complex scenarios.\n1. Introduction\nText-based image editing aims to manipulate images in ac-\ncordance with provided textual prompts. Recent advance-\nments in large-scale text-to-image (T2I) diffusion models,\nsuch as Stable Diffusion [23], DALL\u00b7E [22], and Ima-\ngen [25], have demonstrated remarkable ability to generate\nhigh-quality, diverse images that accurately reflect specified\ntextual descriptions. Trained on comprehensive datasets,\nthese models effectively connect textual descriptions with\ncorresponding images, thereby paving a new way for text-\nbased image editing.\nThe past year has witnessed a substantial increase in the\ndevelopment of methods using diffusion models for text-\nbased image editing, which can be broadly categorized into\nthree groups: training [3, 31], fine-tuning [14, 24, 32],\nand training-free methods [4, 8, 10, 18, 20, 28].\nExist-\ning approaches predominantly concentrate on manipulat-\ning prominent objects within simple compositions. How-\never, images in real-world scenarios usually contain intri-\ncate compositions with multiple objects. Additionally, users\noften require edits in specific localized regions. For exam-\nple, in home interior design, a user might wish to change the\ncolor of a particular piece of furniture to better complement\nthe surrounding space. A case in point is altering the color\nof a sofa to green, as illustrated in the first row of Fig. 1, to\nimprove its aesthetic coherence with the environment.\nThe trade-off between fidelity and editability in localized\nimage editing within complex scenarios presents significant\nchallenges. Mask-based inpainting methods directly gener-\nate a new object as a foreground element and blend it into\nthe original image [1, 2, 8, 12, 29]. However, this often\nresults in substantial structural changes within the edited\nareas, causing noticeable discordance with their complex\nsurroundings, as shown in the third column of Fig. 1. On\nthe other hand, mask-free methods that utilize attention in-\njection mechanisms such as Prompt-to-Prompt (P2P) [10]\nand Plug-and-Play (PnP) [28] can preserve the original im-\nage\u2019s structure and layout. Nevertheless, they struggle to\nprecisely align the local editing region with the intended\ntext in intricate scenarios, largely due to their reliance on the\ntext prompts\u2019 localization capabilities. As a result, editing\neffects often extend beyond the intended area and impact\nincorrect regions, as shown in the fourth column of Fig. 1.\nIntegrating mask-based blending techniques into P2P and\nPnP can alleviate leakage, but issues with incorrect align-\nment remain unresolved. This misalignment leads to the ab-\nsence of editing effects in the intended areas, demonstrated\nin the last column of Fig. 1.\nIn this work, we introduce a novel editing scheme\nnamed Mask-Based Attention-Adjusted Guidance (MAG-\nEdit). This approach is designed to enable localized image\nediting in complex scenarios, which typically involve intri-\ncate compositions with multiple objects. Given that cross-\nattention (CA) maps in pre-trained T2I diffusion models ef-\nfectively capture the correlation between input features and\ntext embeddings, our key insight is that adjusting the noise\nlatent feature to attain higher CA values significantly en-\nhances its alignment with the corresponding text prompt.\nAs a result, we propose locally optimizing the noise latent\nfeature during the inference stage by maximizing two dis-\ntinct mask-based CA constraints tailored for the target edit-\ning prompt. In particular, our approach aims to maximize\ntwo aspects of ratios: first, the CA value of the edit token\nin relation to all token CA values within the masked area,\nand second, the CA value of the edit token inside the mask\ncompared to its overall CA values. Subsequently, the gradi-\nents of these constraints guide the update of the noise latent\nfeature, thus progressively aligning the editing effect with\nthe desired text prompt and spatial requirements. The ef-\nfectiveness of the proposed method is evident in the second\ncolumn of Fig. 1.\nThe main contributions of our work can be summarized\nas follows,\n\u2022 We\nintroduce\nMAG-Edit,\na\nnovel\ntraining-free,\ninference-stage optimization scheme. To our knowl-\nedge, this is the first method specifically designed to\naddress localized image editing in complex scenarios.\n\u2022 We propose two mask-based CA constraints in terms\nof the token and spatial ratio, guiding the local noise\nlatent feature to better align with the target text.\n\u2022 We extensively validate MAG-Edit\u2019s efficiency in lo-\ncalized image editing across diverse intricate indoor\nand outdoor scenarios.\nQuantitative and qualitative\nexperimental results demonstrate a significantly im-\n2\nproved trade-off between editing efficiency and struc-\nture preservation when compared to existing baselines.\n2. Related Work\nText-Based Image Editing Using Diffusion Models can\nbe mainly classified into three categories: training [3, 31],\nfine-tuning [14, 24, 32], and training-free methods [4, 8,\n10, 18, 20, 28].\nInstructPix2Pix [3] requires significant\nresources for extensive training, while fine-tuning meth-\nods like Imagic [14] risk overfitting by optimizing the full\nmodel with limited data. In this work, we focus on training-\nfree methods. Some approaches [1, 2, 12, 29] utilize masks\nto generate foreground objects and blend them into the orig-\ninal image through blending operations. In particular, the\nBlended Diffusion [1] and Blended LD [2], directly gener-\nate foreground objects based on text prompts. DiffEdit [8,\n29] introduces an unsupervised method for learning the\nmask and employs DDIM inversion [26] noise latent fea-\ntures alongside the target prompt to generate the foreground\nimage. Although these approaches successfully maintain\nthe integrity of unedited regions outside of the mask, they\nmay introduce large structural changes within the edit re-\ngions, causing inconsistencies with the surrounding con-\ntext in complex scenes. Other methods [4, 10, 28] such\nas P2P [10] involve the attention integration mechanisms\nto maintain the structure and layout of the original image.\nRecent advancements in inversion methods [15, 17, 18] pro-\npose to improve DDIM inversion [26] for encoding real im-\nages, achieving improved reconstruction and more flexible\nediting capabilities. However, the integration of P2P [10]\nremains essential for these methods to facilitate image edit-\ning. When applied to localized editing in intricate scenarios,\nattention-based methods often result in leakage to incorrect\nareas, leading to inefficiencies in prospective regions.\nOptimization on the Noise Latent Feature. Recent ad-\nvances [5, 6, 30] in image generation with diffusion mod-\nels have investigated the use of CA constraints to optimize\nthe noise latent feature during inference. The pioneering\nwork, Attend-and-Excite [5], addresses issues like catas-\ntrophic neglect and incorrect attribute binding by maximiz-\ning the largest CA units corresponding to all subject tokens\nin the text prompt. This approach refines the noise latent\nfeature at each diffusion step, thereby guiding the model to\ngenerate all subjects described in the text accurately. Sev-\neral training-free layout-generation methods [6, 30] pro-\npose to optimize the noise latent feature by maximizing CA\nconstraints in conjunction with bounding boxes, allowing\nobjects to appear in specific regions. While the image gen-\neration process has demonstrated effectiveness, the applica-\ntion of noise latent feature optimization to image editing has\nreceived relatively less attention. Pix2pix-zero [20] offers a\nsolution by optimizing the noise latent feature, constrain-\n\ud835\udcdf \u201ca table is on \nthe beige floor \u201d\nReconstructed \u1218\ud835\udc3c\nEdit Image \ud835\udc3c\u2217\nNull-Text \nInversion\nEncoder\nImage \ud835\udc3c\n\ud835\udc67\ud835\udc47\n\u00d7 \ud835\udc47 \u2212 1\n\ud835\udc67\ud835\udc47\u22121\nDecoder\n\ud835\udcdf\u2217\u201ca table is on \nthe blue floor\u201d\nEdit Mask  \u2133\n\u2133\nDecoder\nMAG\nOptimize\n\ud835\udc67\ud835\udc47\n\u2217\nMAG\nOptimize\n\ud835\udc67\ud835\udc47\u22121\n\u2217\nCA \nMaps\nCA \nMaps\nFigure 2.\nHigh-level overview of the proposed MAG-Edit\nframework. The first and second rows represent the reconstruc-\ntion and editing branches, respectively. In the editing branch, the\nnoise latent feature is optimized through MAG. This optimization\nprocess aids in achieving alignment with the target edit prompt\n\u201cblue\u201d within the intended edit region M.\ning the CA maps of the editing branch to align with the\nreconstruction branch, thus preserving the original image\u2019s\nstructural layout. In contrast to structural preservation, the\nproposed method aims to align the local noise latent feature\nmore semantically with the target text prompt, enabling lo-\ncalized editing in complex scenarios.\n3. Background and Preliminaries\nStable Diffusion (SD) [23] aims to denoise the random\nsampled noise latent zT conditioned on text embedding C.\nThis process transforms zT into a series of noise latent fea-\ntures zt at each diffusion step t, where t \u223c [1, T] and T\nis the timestep number. To train the diffusion model \u03b5\u03b8,\nthe initial latent feature z0 undergoes an interactive process\nby adding Gaussian noise \u03b5 to the noise latent features zt.\nThen, the network is minimized by,\nmin\n\u03b8\nEz0,\u03b5\u223cN(0,I),t\u223c Uniform (1,T ) \u2225\u03b5 \u2212 \u03b5\u03b8 (zt, t, C)\u22252\n2 . (1)\nFurthermore, the classifier-free guidance [11] performs un-\nconditional prediction to mitigate the amplifying effect of\ntext-based conditioning as:\n\u02dc\u03b5\u03b8 (zt, t, C, \u2205) = w \u00b7 \u03b5\u03b8 (zt, t, C) + (1 \u2212 w) \u00b7 \u03b5\u03b8 (zt, t, \u2205) , (2)\nwhere \u2205 is the unconditional embedding of a null text, and\nw is the guidance weight. To generate images from given\nzT , we can employ deterministic DDIM sampling [26] as:\nzt\u22121 =\nr \u03b1t\u22121\n\u03b1t\nzt +\n s\n1\n\u03b1t\u22121\n\u2212 1 \u2212\ns\n1\n\u03b1t\n\u2212 1\n!\n\u00b7 \u02dc\u03b5\u03b8 (zt, t, C, \u2205) . (3)\nNull-Text Inversion. Real image editing requires revers-\ning corresponding z0 back to zT . A straightforward DDIM\ninversion method [26], in theory reversible with infinitesi-\nmally small steps, tends to accumulate reconstruction errors\n3\n(b) Blue Token CA Map Visualization\nEditing Branch\n\ud835\udc67\ud835\udc61\n\ud835\udcdc\nBlue\n\ud835\udcdc \u2299 \ud835\udcd0\ud835\udc95\u2217 \ud835\udfd4\n\ud835\udcdc \u2299 (\ud835\udc67\ud835\udc61\n\u2217 \u2212 \ud835\udf0e\u2207\ud835\udc67\ud835\udc61\n\u2217\u2112)\nCA Maps\n\u00a7 4.3 Guidance\nOptimize\n\u00a7 4.1 CA \nInject\n\u00a7 4.2 Losses \n\ud835\udc67\ud835\udc61\n\u2217\n\ud835\udc67\ud835\udc61\n\u2217\nCA Maps\n\ud835\udcdf\u2217\n\ud835\udcdf\n\ud835\udcd0t\n\u2217 \ud835\udfd4\n(a) Proposed MAG\nw/o MAG\nt\nOurs\nReconstruction Branch\n\ud835\udcd0t\n\u2217\n\ud835\udcd0\ud835\udc95\n\u0de1\ud835\udcd0t\nFigure 3. Illustration of Our MAG. (a) We optimize the z\u2217\nt by\nmaximizing the mask-based CA constraints of target token (e.g.,\n\u201cblue\u201d). (b) The top row and the bottom row illustrate the average\nmask-based CA maps of \u201cblue\u201d token with our MAG and \u201cw/o\nMAG\u201d from different diffusion steps, respectively. Applying op-\ntimization of z\u2217\nt with MAG, there is a noticeable enhancement in\nthe CA values within the edit regions M.\nin the denoising process, particularly due to classifier guid-\nance. To address this, Null-text inversion [18] aligns the dif-\nfusion latent trajectory with the denoising latent trajectory\nby optimizing a step-wise unconditional embedding \u2205t.\nPrompt-to-Prompt (P2P) [10] introduces several prompt-\nbased editing operations leveraging CA maps: First, the\nword swap involves injecting all attention maps in the re-\nconstruction branch, generated by the source prompt, into\nattention maps from the editing process using the target\nprompt. In contrast, the prompt refinement selectively re-\nplaces the CA maps associated with tokens common to both\nthe source and target prompts. Furthermore, P2P introduces\nthe attention re-weighting operation, enabling direct scale\nadjustments to the CA values. This technique is specifically\ndesigned to control the granularity of the editing process.\n4. Methodologies\nLet I be a real image, we first employ Null-text inver-\nsion [18] to encode it into the noise latent feature zT . Given\nthe original text prompt P and edited prompt P\u2217, we define\nthe set of new target tokens as S\u2217 = {s\u2217\n1, s\u2217\ni , ..., s\u2217\nI} present\nin P\u2217 against P, the common tokens as S = {s1, sj, ..., sJ}\nand S\u2217 \u2229 S = \u2205.\nAn edit region mask M derived by\nI is provided to precisely localize the edit region. Fig. 2\nillustrates the high-level overview of the proposed editing\nframework, which consists of two branches, i.e., reconstruc-\ntion and editing branches generated by prompt P and P\u2217,\nrespectively. In this work, we aim to optimize the noise la-\ntent feature z\u2217\nt of the editing branch at diffusion step t. Our\nobjective is to align the desired editing effects specified by\nS\u2217 with the prospective region defined by M, which en-\nables localized editing in complex scenarios.\nTo achieve this, we first inject CA maps of common\ntokens similar to the prompt refinement in P2P [10] (Sec-\ntion 4.1). Subsequently, we introduce MAG to automati-\ncally manipulate z\u2217\nt , which contains two key steps: defining\ntwo mask-based constraints in Section 4.2 and performing\ngradient guidance in Section 4.3.\n4.1. Attention Injection\nAs illustrated in Fig. 3 (a), to preserve the structural in-\nformation of the original image, CA maps of common to-\nkens from the reconstruction branch are first injected into\nthe editing branch at diffusion step t, thereby obtaining the\nmixing CA maps \u02c6At as:\nInject(At, A\u2217\nt ) :=\n(\n(At)j\nj \u2208 {1, j, \u00b7 \u00b7 \u00b7 , J},\n(A\u2217\nt )i\ni \u2208 {1, i, \u00b7 \u00b7 \u00b7 , I}.\n(4)\n4.2. Mask-Based Attention-Adjusted Constraints\nConsidering that CA maps define the similarity between the\ninput features and text embeddings, larger CA values indi-\ncate better alignment. This observation inspires the formu-\nlation of two mask-based constraints, aiming to maximize\nthe CA value ratio in both token and spatial aspects within\nthe predefined editing region. To illustrate, first consider the\nCA map (A\u2217\nt )i of a new editing token s\u2217\ni within a specific\nmask region M such as \u201cblue\u201d in Fig. 3 (a).\nToken Ratio Constraint. Since CA maps of common to-\nkens from the reconstruction process are first injected into\nthe editing branch, this leads to the CA value of the new\ntoken (A\u2217\nt )i being comparatively lower in contrast to other\ncommon tokens. We then introduce a token ratio constraint\nthat prioritizes increasing the value proportion of the new\ntoken within mask M among all tokens:\nLT R =\n \n1 \u2212 1\nM\nX\nM \u2299\n(A\u2217\nt )i\n(A\u2217\nt )i + PJ\nj=1(At)j\n!2\n,\n(5)\nwhere M represents the total number of elements within the\nmask.\nSpatial Ratio Constraint. In scenarios demanding signif-\nicant editing granularity, the token ratio constraint might\nnot sufficiently amplify the CA value (A\u2217\nt )i within M. To\naddress this limitation, we introduce an additional spatial\nformulation, which is designed to maximize the CA values\n4\nSteaks\n(h) PnP +Blend\n(a) Source Image\n(b) Ours\n(d) DiffEdit\n(g) PnP\n(c) Blended LD\n(e) P2P\n(f) P2P+Blend\nWhite Bird\nPlaid  Shirt\nYellow Chair\nStrawberry\nGreen Pillow\nDenim Pants\nVintage Car\nSteaks\nFigure 4. Qualitative comparisons of localized image editing across various complex scenarios. We highlight the editing regions with\nwhite dashed lines. Simplified target edit prompts are denoted on the left side of (a) source images. Our proposed method (b) not only\nachieves superior editing effects but also better preserves the structure in local regions against other baselines (c-h).\nwithin the masked region while simultaneously minimizing\nthem outside the mask as,\nLSR = \u03bb\n\u0012\n1 \u2212\nP M \u2299 (A\u2217\nt )i\nP(A\u2217\nt )i\n\u0013\n|\n{z\n}\nOut-mask\n\u2212\nP M \u2299 (A\u2217\nt )i\nP(A\u2217\nt )i\n|\n{z\n}\nIn-mask\n,\n(6)\nwhere \u03bb is a balance weight, and we set \u03bb = 3 empirically.\nNegative Prompt Constraint. In real image editing, the la-\ntent noise feature zT derived by the inversion methods still\nretains information related to the original image I. Achiev-\ning the desired editing results can be challenging in some\ncases when there is a significant difference between the tex-\nture in the original image and modified prompt P\u2217, such as\ntransferring color from \u201cblack\u201d to \u201cwhite\u201d. Our proposed\n5\nAlgorithm 1: A Denoising Step using MAG-Edit\nInput: A original and edited prompt P, P\u2217; a\ntimestep t and corresponding noise latent\nfeatures of reconstruction and editing\nbranches zt, z\u2217\nt ; a maximum iteration step\nMAX IT; a function F(\u00b7) for computing\nproposed constraints;a pre-trained Stable\nDiffusion model SD.\nOutput: the noisy latent feature z\u2217\nt\u22121 for the next\ntimestep of the editing branch.\n1 for i = 1 to MAX IT do\n2\n, At \u2190 SD(zt, P, t) ;\n3\n, A\u2217\nt \u2190 SD(z\u2217\nt , P\u2217, t) ;\n4\n\u02c6At \u2190 Inject(At, A\u2217\nt ) ;\n5\nL \u2190 F( \u02c6At);\n6\nz\u2217\nt = M \u2299 (z\u2217\nt \u2212 \u03b4\u2207z\u2217\nt L) + (1 \u2212 M) \u2299 z\u2217\nt ;\n7 end\n8\n, At \u2190 SD(zt, P, t) ;\n9\n, A\u2217\nt \u2190 SD(z\u2217\nt , P\u2217, t) ;\n10 \u02c6At \u2190 Inject(At, A\u2217\nt ) ;\n11 z\u2217\nt\u22121 \u2190 SD(z\u2217\nt , P\u2217, t){ \u02c6At} ;\n12 Return z\u2217\nt\u22121\nmethod can also be used to attenuate the textural informa-\ntion associated with the original image I by employing neg-\native prompts. In particular, we define a set of negative to-\nkens S\u2217\nng to present the texture of I in contrast to the new\ntokens S\u2217. For example, if P\u2217 is \u201ca man wears a white T-\nshirt\u201d and the T-shirt in I is black, then the negative token\nwould be \u201cblack\u201d. Consequently, we can establish the neg-\native prompt constraint Lng using the negative token\u2019s cor-\nresponding CA value and optimize the noise latent feature\nin the opposite direction as follows,\nLtotal = \u03bbpL \u2212 \u03bbngLng,\n(7)\nwhere \u03bbp and \u03bbng aim to balance between positive and neg-\native prompt constraint.\n4.3. Perform Gradient Guidance\nUpon establishing the mask-based constraints, we compute\ntheir gradients to determine the optimal direction for mod-\nifying the current noise latent feature z\u2217\nt . In particular, to\nrestrict the editing effect to the predefined region, we up-\ndate the noise latent feature z\u2217\nt inside the mask M using the\nfollowing equation:\nz\u2217\nt = M \u2299 (z\u2217\nt \u2212 \u03b4\u2207z\u2217\nt L) + (1 \u2212 M) \u2299 z\u2217\nt ,\n(8)\nwhere the term \u03b4 represents the gradient update scale. As\ndetailed in Algorithm 1, z\u2217\nt is iteratively refined until reach-\ning the maximum number of iteration.\nMethod\nQuantitative Metrics\nHuman Preference (Ours vs. )\nCLIP\nScore (\u2191)\nDINO-ViT\nDistance (\u2193)\nText\nAlignment (%)\nStructure\nPreservation (%)\nOverall\nPreference (%)\nBlended LD [2]\n19.12\n0.089\n84 %\n75%\n80%\nDiffedit [8]\n19.20\n0.083\n77 %\n66%\n71 %\nP2P [10]\n20.02\n0.079\n87 %\n54 %\n81%\nPnP [28]\n19.90\n0.083\n87 %\n59 %\n79 %\nP2P+Blend\n19.77\n0.081\n83 %\n62 %\n73%\nPnP+Blend\n19.47\n0.080\n82 %\n56 %\n69%\nOurs\n21.79\n0.081\n/\n/\n/\nTable 1. Quantitative comparisons of localized image editing.\nWe assess all the metrics and human preferences in the localized\nediting regions. \u201cOurs vs. \u201d indicates the proportion of users who\nfavor our proposed method over the comparative approach.\nWhite T-Shirt\nP2P \nRe-weight\u00d710 Re-weight\u00d730 \nOurs\nInput \nCola\nPink Curtain\nOrange Table\nFigure 5. Attention re-weighting [10] vs. Ours. Attention re-\nweighting [10] either amplifies the entire editing magnitude in in-\ncorrect regions (first two rows) or fails to edit regions that signifi-\ncantly contradict the target prompt (last two rows). In contrast, our\nproposed method effectively addresses both scenarios.\nMoreover, our proposed method can be readily adapted\nfor multiple prompt editing as:\nz\u2217\nt = M \u2299 (z\u2217\nt \u2212 \u03b4\u2207z\u2217\nt\nI\nX\ni=1\n(\u03bb1L1 + \u00b7 \u00b7 \u00b7 +\n\u03bbiLi + \u03bbILI)) + (1 \u2212 M) \u2299 z\u2217\nt ,\n(9)\nwhere the term \u03bb\u2217 controls the editing granularity of each\nprompt, with their sum equaling 1. Fig. 8 (a) demonstrates\nhow our proposed method effectively balances the editing\ngranularity for various prompts.\n5. Experiments\n5.1. Implementation details\nWe adopt the pre-trained Stable Diffusion v1.4 [23] model\nas the backbone. All CA values are calculated in the reso-\nlution of 16 \u00d7 16 of the U-Net, which is known to process\nthe most semantically rich information [5]. To preserve the\noriginal information in the regions outside the mask, we in-\ncorporate latent blend operation in [10]. In practice, we\n6\nPink Curtain\nRoadster\nAttend-and-Excite\nOurs\nInput \nFigure 6. Attend-and-Excite [5] vs. Ours. Attend-and-Excite\u2019s\nCA constraint [5] yields unsatisfactory editing results.\nselect between LT R and LSR, depending on the required\ngranularity of the edit, allowing for adaptability across vari-\nous editing types. All experiments are conducted on a single\nNVIDIA A100 GPU. Additional implementation details are\nprovided in Appendix B.\n5.2. Comparisons with Baselines\nBenchmark Dataset. Existing datasets for text-based im-\nage editing methods primarily focused on relatively simple\nscenes dominated by prominent objects. To enable a more\ncomprehensive evaluation of our method, we have curated a\nbenchmark data set consisting of 200 images sourced from\nADE20K [33], MS-COCO [16], Cityscape [7], and the In-\nternet. The selected images feature complex compositions\nwith multiple objects in a wide range of real-world indoor\nand outdoor scenes. Our evaluation primarily targets local-\nized editing of color, texture, and object replacement. We\ngenerate the source and target prompts using GPT-4 [19].\nThe corresponding edit masks are obtained using the Seg-\nment Anything method1. Consequently, each image in the\ndataset is associated with three annotations: a source im-\nage prompt, a target image prompt, and the editing mask.\nAdditional details are provided in Appendix C.1.\nBaselines. We conduct comparisons with existing represen-\ntative training-free diffusion-based image editing methods,\ncovering these categories:\n\u2022 Mask-based:\nBlended Latent Diffusion (Blended\nLD) [2] and DiffEdit [8].\n\u2022 Mask-free: P2P [10] and PnP [28].\n\u2022 Mask-free with blending: We combine blending op-\nerations with P2P and PnP as baselines, denoted as\nP2P+Blend and PnP+Blend, respectively. Note that\nP2P+Blend can be viewed as the proposed method\nw/o MAG.\nQualitative results.\nFig. 4 clearly shows that Blended\nLD [2] leads to considerable structural changes in complex\n1https://github.com/facebookresearch/segment-\nanything\nYellow Carpet\n\u2112\ud835\udc7b\ud835\udc79\n\u2112\ud835\udc7a\ud835\udc79\nInput \nPink Chair\nFox\nRabbit\nFigure 7. Editing granularity of proposed constraints. The to-\nken ratio constraint LT R efficiently preserves the inherent struc-\nture in the edited region, while the spatial ratio constraint LSR\nenhances editing granularity.\nscenarios, resulting in significant discordance with the sur-\nrounding context. Meanwhile, DiffEdit [8], which employs\nDDIM inversion for foreground generation, either alters the\nstructure, as seen in the \u201cwhite bird\u201d example in the fifth\nrow, or fails to produce a noticeable editing effect in the\nintended region, as is apparent in other images. Concern-\ning mask-free methods, P2P [10] and PnP [28] often exhibit\nleakage into adjacent regions, leading to minimal effects in\nthe prospective region. This issue is particularly noticeable\nin tasks such as changing the color of a yellow chair or a\ngreen pillow. Blending operations might reduce leakage in\nsome scenarios, yet the issue of misalignment persists, re-\nsulting in inefficiencies in the intended edit regions. In con-\ntrast, our proposed method shows improved editing perfor-\nmance with better structural preservation.\nQuantitative results. We quantitatively evaluate our pro-\nposed method against baseline models using both automatic\nmetrics and human evaluations.\nAutomatic Metrics. To better evaluate localized editing\nability, we use the bounding boxes to crop the editing re-\ngions [12] and evaluate the image-text alignment and struc-\nture preservation using the CLIP score [21] and the DINO-\nViT self-similarity distance [27], respectively. Table 1 illus-\ntrates that our proposed method significantly enhances text\nalignment within local regions, achieving much higher local\nCLIP values without compromising fidelity.\nUser study. We perform a user preference evaluation via\npairwise comparisons on Amazon MTurk2, focusing on text\nalignment, structure preservation, and overall preference in\nlocalized editing regions. As shown in Table 1, the per-\ncentages represent the proportion of users who prefer our\n2https://www.mturk.com/\n7\nSource Image\nSource Image\n(a) Controllable Granularity Localized Editing \nStarry\nDark\n(0.8,0.2)\n(0.5,0.5)\n(0.2,0.8)\n(b) Iterative Localized Editing \nSource Image\nPink wall\nBlossom plant\nFloral sofa\nSource Image\nYellow raincoat Snow-covered desk\nSweater\nRed\nVelvet\n(0.8,0.2)\n(0.5,0.5)\n(0.2,0.8)\nFigure 8. Other localized editing applications of the proposed MAG-Edit.\nproposed method over comparative approaches. A signifi-\ncant majority, ranging from 77% to 87%, believe that our\nmethod achieves much better text alignment compared to\nother methods. Furthermore, our method is preferred for\nbetter structure preservation by 75% of users over Blended\nLD [2]. Due to its more effective balance between editabil-\nity and fidelity, our proposed method is overall favored by\n69% to 80% of the participants.\n5.3. Ablation Study\nWhy Optimize the zt.\nTo enhance the editing effect,\na straightforward approach is to directly increase the CA\nvalues of the corresponding token using the attention re-\nweighting in P2P [10]. However, due to P2P\u2019s inherent mis-\nalignment, this direct amplification of CA values tends to\nintensify the editing effects in incorrect regions, failing to\nenhance the desired localized areas (first two rows in Fig. 5).\nAdditionally, minimizing the influence of information from\nthe original image that conflicts with the edit prompt poses\na challenge, even in prominent objects, such as changing\ncolor from \u201cblack\u201d to \u201cwhite\u201d (last two rows in Fig. 5). In\ncontrast, our proposed method focuses on local alignment\nin specific regions and effectively attenuates contradicting\ninformation by directly optimizing the noise latent feature.\nvs. Attend-and-Excite. Attend-and-Excite [5] optimizes\nthe noise latent feature zt in the unconditional generation\nto maximize the largest CA value of the subject token. We\nthen establish a baseline using the CA constraint formula-\ntion in [5]. However, Fig. 6 shows that this constraint is\ninsufficient for image editing scenarios. Contrary to uncon-\nditional generation, the noise latent feature derived from in-\nversion methods contains more information related to the\nreal image, as opposed to random noise features sampled\nfrom a Gaussian distribution. As such, our proposed con-\nstraints offer more efficient guidance for adjusting the noise\nlatent feature, thereby more aptly addressing the needs of\nimage editing.\nImpact of Proposed Constraints.\nLT R and LSR of-\nfer distinct levels of editing granularity, as demonstrated\nin Fig. 7. LT R excels in maintaining the inherent struc-\nture within the edit region, which aids in achieving natural\ncolor and texture modifications. On the other hand, LSR\nprovides stronger guidance by directly amplifying the CA\nvalues within the mask, leading to more noticeable struc-\ntural changes in the edit region. As a result, LSR is better\nsuited for edits involving large structural shape changes.\n5.4. Other Applications\nMAG-Edit is also adaptable for controllable granularity and\niterative localized editing. In Fig. 8 (a), MAG-Edit demon-\nstrates the ability to balance editing granularity across var-\nious prompts, catering to user-specific requirements. Fur-\nthermore, Fig. 8 (b) illustrates MAG-Edit\u2019s capability to\nexecute iterative, localized manipulations on various ob-\njects within a single image. More results are shown in Ap-\npendix F.\n6. Conclusions\nIn this work, we introduce a novel technique Mask-Based\nAttention-Adjusted\nGuidance\n(MAG-Edit),\nspecifically\ncrafted for localized editing in complex scenarios.\nIn\nparticular, we propose to maximize two mask-based CA\nconstraints, namely token and spatial ratio, to locally\noptimize the noise latent feature for enhanced alignment\nwith the target text embedding. Our experimental results,\nboth quantitative and qualitative, consistently illustrate\nthat MAG-Edit outperforms existing methods in localized\nimage editing within complex scenarios. We believe the\nproposed MAG-Edit scheme has pioneered a novel direc-\ntion for applying localized editing in real-world scenarios.\n8\nReferences\n[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In CVPR,\npages 18208\u201318218, 2022. 2, 3\n[2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\nlatent diffusion. ACM TOG, pages 1\u201311, 2023. 1, 2, 3, 6, 7,\n8, 11\n[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn CVPR, pages 18392\u201318402, 2023. 2, 3, 13, 14\n[4] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. In ICCV, pages 22560\u201322570, 2023. 2, 3, 17\n[5] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or.\nAttend-and-excite: Attention-based se-\nmantic guidance for text-to-image diffusion models. In SIG-\nGRAPH, pages 1\u201324, 2023. 3, 6, 7, 8\n[6] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free\nlayout control with cross-attention guidance. arXiv preprint\narXiv:2304.03373, 2023. 3, 11\n[7] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld,\nMarkus Enzweiler,\nRodrigo Benenson,\nUwe\nFranke, Stefan Roth, and Bernt Schiele.\nThe cityscapes\ndataset for semantic urban scene understanding. In CVPR,\npages 3213\u20133223, 2016. 7, 11\n[8] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and\nMatthieu Cord. Diffedit: Diffusion-based semantic image\nediting with mask guidance. In ICLR, pages 1\u201322, 2023. 1,\n2, 3, 6, 7, 11\n[9] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng\nSong, Mengwei Ren, Ruijiang Gao, Yuxiao Chen, Di Liu,\nQilong Zhangli, et al. Improving negative-prompt inversion\nvia proximal guidance.\narXiv preprint arXiv:2306.05414,\n2023. 16\n[10] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image\nediting with cross attention control. In ICLR, pages 1\u201336,\n2023. 1, 2, 3, 4, 6, 7, 8, 11, 13, 16\n[11] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. In NeruIPS workshop, pages 1\u201314, 2021. 3\n[12] Wenjing Huang, Shikui Tu, and Lei Xu. Pfb-diff: Progres-\nsive feature blending diffusion for text-driven image editing.\narXiv preprint arXiv:2306.16894, 2023. 2, 3, 7, 12\n[13] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and\nQiang Xu. Direct inversion: Boosting diffusion-based edit-\ning with 3 lines of code. arXiv preprint arXiv:2310.01506,\n2023. 16\n[14] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models.\nIn\nCVPR, pages 6007\u20136017, 2023. 2, 3\n[15] Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz\nKhan, Qibin Hou, Yaxing Wang, and Jian Yang. Styledif-\nfusion: Prompt-embedding inversion for text-based editing.\narXiv preprint arXiv:2303.15649, 2023. 3, 16\n[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, pages 740\u2013755, 2014. 7, 11\n[17] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki\nTanaka. Negative-prompt inversion: Fast image inversion\nfor editing with text-guided diffusion models. arXiv preprint\narXiv:2305.16807, 2023. 3\n[18] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real images\nusing guided diffusion models. In CVPR, pages 6038\u20136047,\n2023. 2, 3, 4, 11\n[19] OpenAI.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023. 7, 11\n[20] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In SIGGRAPH, pages 1\u201311, 2023. 2, 3\n[21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, pages 8748\u20138763, 2021. 7\n[22] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2\n[23] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 2, 3, 6\n[24] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, pages 22500\u201322510, 2023. 2, 3\n[25] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. In NeurIPS, pages 36479\u201336494,\n2022. 2\n[26] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In ICLR, pages 1\u201320, 2021. 3,\n11\n[27] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali\nDekel. Splicing vit features for semantic appearance transfer.\nIn CVPR, pages 10748\u201310757, 2022. 7\n[28] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation.\nIn CVPR, pages 1921\u20131930,\n2023. 1, 2, 3, 6, 7\n[29] Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka.\nInstructedit:\nImproving automatic masks for diffusion-\nbased image editing with user instructions. arXiv preprint\narXiv:2305.18047, 2023. 2, 3\n[30] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wen-\ntian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff:\nText-to-image synthesis with training-free box-constrained\ndiffusion. In ICCV, pages 7452\u20137461, 2023. 3\n9\n[31] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su.\nMagicbrush: A manually annotated dataset for instruction-\nguided image editing. In NeurIPS, 2023. 2, 3, 13, 14\n[32] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N\nMetaxas, and Jian Ren.\nSine: Single image editing with\ntext-to-image diffusion models. In CVPR, pages 6027\u20136037,\n2023. 2, 3, 14\n[33] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\nade20k dataset. In CVPR, pages 633\u2013641, 2017. 7, 11\n10\nAppendix\nA. Summary\nIn this appendix, we present more implementation details,\nadditional experiments, and additional results as follows:\n\u2022 We present more implementation details of MAG-\nEdit in Section B. Furthermore, Section C illus-\ntrates more implementation details on the benchmark\ndataset, baselines, quantitative metrics, and user study.\n\u2022 In Section D, we extend our comparisons to encom-\npass training and fine-tuning methods, as well as re-\ncent developments in inversion techniques. For a more\ncomprehensive understanding of our proposed method,\nadditional ablation studies are conducted and detailed\nin Section E.\n\u2022 We demonstrate additional qualitative results to com-\nplement the paper in Section F.\n\u2022 Finally, the limitations of our approach are thoroughly\nanalyzed in Section G.\nB. Implementation Details\nWe utilize the official pre-trained Stable Diffusion v1.4\nmodel3 as our foundation model. The denoising sampling\nprocess employs the DDIM method [26] over T = 50 steps,\nmaintaining a constant classifier-free guidance scale of 7.5.\nCA injection is performed during [T, \u03c41]. For varying edit-\ning requirements, we set \u03c41 = 10 for color and texture edits,\nand \u03c41 = 40 for shape variation edits. Our MAG-Edit op-\ntimization takes place during diffusion steps in the range\n[T, \u03c42], with \u03c42 empirically set to 25. For the gradient guid-\nance process, we follow [6] by setting the gradient update\nscale \u03b4 using a linear scheduling rate as\np\n(1 \u2212 \u03b1t)/\u03b1t, par-\nticularly to optimize the token ratio constraint LT R. This\napproach modulates the gradient\u2019s magnitude based on the\ndenoising progress. On the contrary, for the constraint of\nthe spatial ratio LSR, we keep \u03b4 = 1. The optimization\nprocess is also influenced by the maximum number of iter-\nations, empirically set MAX IT = 15. In cases involving\nnegative prompt constraints, we empirically set \u03bbp = 2.5\nand \u03bbng = 5.5. To further preserve the structure of the orig-\ninal image, we also consider incorporating self-attention as\nP2P [10] and replace them at diffusion steps t \u2208 [T, 25].\nTowards the end of the denoising process t \u2208 [15, 0], we\nimplement a latent blend operation from P2P [10] to main-\ntain information outside the edited region mask M. When\nevaluated on an Nvidia A100 (40GB) GPU, the runtime of\nMAG-Edit is around 1 \u223c 5 minutes, varying with the se-\nlected values of MAX IT and \u03c42.\n3https://github.com/CompVis/stable-diffusion\nC. Details of Comparisons with Baselines\nC.1. Benchmark Dataset\nCurrent datasets for text-based image editing methods are\nprimarily limited to simple scenes with prominent objects.\nTo enable a more thorough evaluation of our method, we\nhave developed a benchmark dataset, named MAG-Bench,\nconsisting of 200 images sourced from ADE20K [33], MS-\nCOCO [16], Cityscape [7], and the Internet. This dataset\nfeatures complex scenes with multiple objects in various\nreal-world indoor and outdoor settings, encompassing a\nwide range of object categories like humans, furniture, an-\nimals, vehicles, and food. MAG-Bench is specifically de-\nsigned to assess three types of local editing: (1) color edit-\ning, (2) texture editing which includes changes in material,\nbackground, and style, and (3) object replacement. For the\ngeneration of source and target prompts, we initially utilized\nGPT-4 [19], followed by manual refinement to ensure the\naccuracy and relevance of these prompts. The correspond-\ning editing masks for each image are derived using the Seg-\nment Anything method4. Acknowledging the critical role\nof the mask\u2019s size in localized editing, we initially classify\neach image into three categories based on mask size: rela-\ntively small, medium, and relatively large. We then ensure\na balanced distribution of varying sizes of editing regions\nacross the datasets. Thus, each image in MAG-Bench is ac-\ncompanied by three annotations: a source prompt, a target\nedit prompt, and an edit region mask, as illustrated in Fig. 9.\nC.2. Implementation Details of Baselines\nWe use the official codes released by the authors for\nBlended LD5, P2P6, and PnP7. For DiffEdit [8], we adopt\nthe implementation from InstructEdit8, which enhances au-\ntomatic mask generation for scenarios involving multiple\nobjects. This implementation, while improving upon mask\ngeneration, does not modify the core editing algorithm of\nDiffEdit [8]. To facilitate fair comparisons, all methods use\nidentical masks provided in our benchmark dataset. No-\ntably, for DiffEdit [8] and P2P [10], we utilize ground-\ntruth masks instead of those generated through unsuper-\nvised learning or derived from average CA maps. In the\ncase of P2P [10], we also integrate Null-text inversion [18]\nas our approach for encoding real images. With the excep-\ntion of Blended LD [2], which solely focuses on the target\nedit description for the foreground region and omits tokens\nfor other unedited areas, all other methods employ target\nprompts identical to those used in our method.\n4https://github.com/facebookresearch/segment-\nanything\n5https : / / github . com / omriav / blended - latent -\ndiffusion\n6https://github.com/google/prompt-to-prompt\n7https://github.com/MichalGeyer/plug-and-play\n8https://github.com/QianWangX/InstructEdit\n11\nScenarios\nEdit Type\nSource Image\nSource Prompt\nTarget Prompt\nMask\nMask Type\nOutdoor\nColor\nA couple and a kid \nwith black hair are \nsitting on the bench\na couple and a kid with \nblond hair are sitting \non the bench\nSmall\nObject\nA green truck and \nsome cars park under a \ntall building.\nA green bus and some \ncars park under a tall \nbuilding.\nMedium\nTexture\nGuinea fowl stand on \ndry grass under sky.\nGuinea fowl stand on \ndesert under sky.\nLarge\nIndoor\nColor\nThe wooden and round \ntable is surrounded by \nfour wooden chairs and \na light brown chair is \nnext to the windows.\nThe wooden and round \ntable is surrounded by \nfour wooden chairs and \na light red chair is next \nto the windows.\nSmall\nObject\nThere are a box and \nlemons and several \nlemons on white sheet.\nThere are a bowl and \nlemons and several \nlemons on white sheet.\nMedium\nTexture\nThere is a table with \ncups and four chairs on \nthe plaid carpet.\nThere is a table with \ncups and four chairs on \nthe bohemian carpet.\nLarge\nFigure 9. Examples images and annotations in the MAG-Bench dataset.\nC.3. Evaluation Details\nWe utilize the CLIP score with the CLIP ViT-L/14 model,\nas implemented in9, and the DINO-ViT self-similarity dis-\ntance, available at10, as our evaluation metrics. To precisely\nevaluate localized editing, we crop the editing regions in\nboth the source and edited images using bounding boxes\nas [12]. This approach enables us to specifically assess text\nprompt alignment within these localized regions by calcu-\nlating the CLIP score on the target edited tokens with the\nrespective cropped edited image. For instance, in a scenario\nwhere the editing objective is to alter a car\u2019s color to red,\nthe CLIP score is computed using the phrase \u201cred car.\u201d This\ncalculation excludes common tokens shared between the\nsource and target prompts and focuses solely on the cropped\nimage depicting the edited car and the target phrase. To\nevaluate structure preservation within the localized editing\nregions, we utilize the DINO-ViT self-similarity by calcu-\nlating the distance between the cropped source image and\nthe corresponding cropped edited image.\n9https://github.com/showlab/loveu-tgve-2023\n10https://github.com/omerbt/Splice\nC.4. Details of User Study\nWe conduct a user study on the Amazon MTurk platform11.\nThe user study comprises over 120 tasks, each evaluated by\nfive human evaluators, as depicted in Fig. 10. In each task,\nparticipants are presented with a source image alongside\ntwo edited images: one generated by our proposed method\nand the other by a randomly selected baseline method, with\ntheir presentation order shuffled. To enhance the visibil-\nity of localized editing regions, we outline the prospective\nedit regions with white dashed lines in each pair of com-\nparison images and their corresponding source images, as\nillustrated in Fig. 10. Additionally, a simplified version of\nthe target edit prompt was displayed beneath the compari-\nson images. We then pose three questions for the raters to\nanswer:\n\u2022 Text Alignment: In the dashed region, which image\naligns better with the \u201cedit prompt\u201d?\n\u2022 Structure Preservation: In the dashed region, which\nimage preserves structures more similarly to the source\nimage?\n\u2022 Overall: In the dashed region, which image performs\n11https://requester.mturk.com\n12\nInstructions\nThis task includes evaluating two AI based edits of real images in which we provided source image and target edit prompt. Moreover,we hope that only dashed \nregion of corresponding image will be edited according to the target prompt. Please view the source image and target prompt and provided your feedback on the \nfollowing criteria :\n\u00b7Text Alignment:  In the dashed region, which image aligns better with the \u201cedit prompt\u201d?\n\u00b7Structure Preservation: In the dashed region, which image better preserves structures more similarly to the source image?\n\u00b7Overall: In the dashed region, which image performs better overall?\nOur ultimate goal is to have the edited image and target edit prompt aligned as much as possible.\n1. In the dashed region, which image aligns better with the \u201cpink chair\u201d?\nOption 1\nOption 2\n2. In the dashed region, which image better preserves structures more similarly to the source image?\nOption 1\nOption 2\n3. In the dashed region, which image performs better overall?\nOption 1\nOption 2\nTarget prompt: pink chair\nSource image\nOption 1\nOption 2\nFigure 10. Example of one task for 5 human raters on Amazon MTurk to complete.\nbetter overall?\nTo ensure the credibility and reliability of our user study, we\nonly involve Amazon MTurk workers with \u2018Master\u2019 status\nand a Human Intelligence Task (HIT) Approval Rate ex-\nceeding 90% across all Requesters\u2019 HITs. In total, the 120\ntasks garnered responses from 600 distinct human evalua-\ntors.\nD. Comparisons with Other Baselines\nIn this section, we begin by comparing our approach with\ncurrent training and fine-tuning methods, aiming to further\nvalidate the efficacy of our proposed method in facilitat-\ning localized editing in complex scenarios. Subsequently,\nwe extend our comparison to include recent advancements\nin training-free inversion methods. This comparison is in-\ntended to illustrate that despite improvements in inversion\nmethods, they still face challenges in addressing localized\nediting issues.\nD.1. Comparisons with Training and Fine-tuning\nMethods\nWe initiate our qualitative comparison with existing train-\ning methods by evaluating InstructPix2Pix [3] and Mag-\nicBrush [31], utilizing their officially released codes and\nmodels. InstructPix2Pix [3] is trained on an extensive data\nset, which includes instructions generated by GPT-3 and im-\nage examples modified by P2P [10]. This training facilitates\ninstruction-based image editing during the inference phase.\nMagicBrush [31] harnesses a large-scale dataset of man-\nually annotated real image editing triplets and optimizes\nthe InstructPix2Pix model to improve editing capabilities.\nFor our comparisons, we utilize editing instructions such\nas \u201cmake\u201d and \u201cchange\u201d to manipulate images. Fig. 11 il-\nlustrates that InstructPix2Pix, due to its lack of mask inte-\ngration, frequently leads to substantial leakage into incor-\nrect regions during localized editing in complex scenes. In\ncontrast, MagicBrush demonstrates better localized editing\nin some cases, thanks to mask-integrated examples in its\ndataset.\nHowever, MagicBrush encounters difficulties in\n13\nInput\nOurs\nInstructPix2Pix\nMagicBrush\nPlaid sofa\nYellow car\nGreen pillow\nSINE\nStrawberry\nTropical fish \nFigure 11.\nQualitative comparisons with training and fine-\ntuning methods for localized editing in complex scenar-\nios. Training approaches such as InstructPix2Pix [3] and Mag-\nicBrush [31] demonstrate issues like leakage or unintended modi-\nfications in structure. The fine-tuning method SINE [32] is ineffec-\ntive in both reconstructing and generating desired editing effects.\nInput\nOurs\nStyleDiffusion\nProxNPI\nYellow shirt\nJeep\nFlora sofa\nDirectInversion\nFigure 12.\nQualitative comparisons with recent inversion\nmethods for localized editing in complex scenarios. Despite re-\ncent advancements, solely enhancing inversion methods continues\nto be inadequate for effective editing of localized regions in com-\nplex scenarios.\nprecisely localizing individual objects within scenes con-\ntaining multiple similar objects. This challenge is evident\nin the first and second rows of Fig. 11, where it strug-\ngles with tasks like coloring one car yellow and one pillow\ngreen. Moreover, as shown in the third row of Fig. 11, Mag-\nicBrush [31] tends to modify the underlying structure in ar-\neas undergoing texture changes. In contrast, our training-\nfree method efficiently attains desired editing effects in the\nWhite T-Shirt\n\ud835\udc98/\ud835\udc90\ud835\udcdb\ud835\udc8f\ud835\udc88\n\ud835\udc98/\u2112\ud835\udc8f\ud835\udc88\nInput \nFlower\nPlain Quilt\nVintage Car\nFigure 13. Ablation study on the negative prompt constraint.\nNegative prompt constraints can amplify the effectiveness of edit-\ning by diminishing the influence of information from the original\nimage.\nPlaid sofa\nIterations 8 \nIterations 15\nIterations 30 \nIterations 80\nInput \nCyberpunk car\nPink Curtain\nRed blanket\nFigure 14. Impact of optimization iterations. Increasing the\nnumber of iterations enhances the granularity of editing. However,\noverly extensive iterations can lead to notable artifacts arising from\nstructural modifications.\ntarget local regions while preserving the original structure.\nA significant advantage of our approach is the elimination\nof the need for extensive training on large datasets, saving\nsignificant time and resources.\nSubsequently, we compare our method with the existing\nfine-tuning method, SINE [32], using the code provided by\nits authors. SINE [32] proposes fine-tuning a pre-trained\ntext-to-image (T2I) model with a single real image, incor-\nporating model-based classifier guidance and patch-based\nguidance to prevent overfitting. However, as illustrated in\nFig. 11, SINE fails to generate any noticeable editing effects\n14\nw/o MAG \n40\nInput \nFloral sofa\nFull Optimize\n30\n25\n20\n10\nGoldfish\nBlue floor\nFigure 15. Applying MAG-Edit through a varied number of diffusion steps. We use white dashed lines to demarcate the editing regions\nin the source images. Each row demonstrates the optimization of the noise latent feature ranging from 0% (left) to 100% (right) of the\nsteps. In particular, we assign values to \u03c42 = {50, 40, 30, 25, 20, 10, 0}, indicating the end of the diffusion step range, from 50 to \u03c42, as\nnoted at the bottom of each image. Without MAG, there is a negligible localized editing effect in the intended regions. On the other hand,\nemploying MAG across all steps does not markedly enhance the granularity of color and texture editing. Moreover, it results in noticeable\nstructural artifacts in the shape editing.\nSource Image\nRemove a lemon\nColorful sheet\nBlue butterfly\nA Bowl\nRed Wine\nObject Attribute Manipulation\nObject Replacement\nObject Addition\nObject Removal\nWooden house\nLit window\nRemove a flag\nJack-O'-Lantern\nCurtained window\nCreamy bread\nPink and lace sheet\nRemove a strawberry\nLight bulb\nPolaroid photos\nSource Image\nSource Image\nEditing Type\nFigure 16. Various localized editing types. We provide a simplified version of the corresponding target prompt under each edited image.\nin the intended regions. Furthermore, it faces difficulties\nin accurately reconstructing the original image in complex\nscenarios.\nD.2. Comparisons with Recent Inversion Methods\nTo demonstrate that localized editing challenges are not suf-\nficiently addressed by mere advancements in inversion tech-\n15\nEmbroidered \u2191 sofa\nYellow\u2191 sofa \nSource Image\nCrashed \u2191 car\nSource Image\nSource Image\nFigure 17. Granularity controllable localized editing. We present a simplified version of the corresponding target prompt under the\nedited images. \u2191 denotes increasing the editing magnitude.\nAdd a Pumpkin in different location\nAdd a crow in different location\nSource Image\nSource Image\nFigure 18. Spatial controllable localized editing.\nniques, we compare our method with recent inversion meth-\nods. This includes Style Diffusion [15], ProxNPI [9], and\nDirectInversion [13], utilizing their official codes. Each of\nthese approaches incorporates P2P [10] to facilitate edit-\ning capabilities. As depicted in Fig. 12, it is evident that\nthese recent inversion methods are unable to produce effec-\ntive editing results in localized regions within complex sce-\nnarios. This underscores the heightened challenges faced\nin localized editing within intricate compositions compared\nto simpler settings. In contrast, our method significantly\nimproves localized editing by optimizing the noise latent\nfeature through our specially designed MAG mechanism.\nE. Additional Ablation Studies\nImpact of Negative Prompt Guidance. Fig. 13 demon-\nstrates that negative prompt guidance is effective in dimin-\nishing the original image\u2019s information, which is beneficial\nwhen dealing with original images that have information\nsignificantly contrast with the target prompt. For instance,\nas shown in the first column of Fig. 13, when altering the\ncolor of a T-shirt from black to white, not applying negative\nconstraints could lead to the edited image preserving some\nblack elements. The negative prompt constraint, in such\nscenarios, efficiently reduces this residual black informa-\ntion. Moreover, as observed in the third column of Fig. 13,\n16\nMasks and Prompts\nEdited Image\nSource Image\nBlue\nPurple\nPink\nYellow\nSkull\nLace\nWedding\nWrap\nWedding\nSuit\nFigure 19. Part-level localized editing. In the second row of\nimages, the editing regions are indicated with white dashed lines,\nand the target prompts are also annotated for clarity.\nwhen transforming a patterned quilt into a plain one, the\nnegative prompt constraint plays a crucial role in diminish-\ning the original textures of the quilt.\nImpact of Optimization Iterations. The number of max-\nimum iterations for optimizing the noise latent feature is\ncrucial in modulating the magnitude of editing. As shown\nin Fig. 14, increasing the number of iterations can improve\nthe granularity of the editing. However, in texture and shape\nediting, excessive iterations may result in significant arti-\nfacts as a result of alterations in the structure.\nImpact of Optimization Diffusion Steps. Applying MAG-\nEdit across various diffusion steps significantly impacts the\nfinal editing results. Fig. 15 demonstrates that optimization\nin the initial diffusion steps can quickly alter the color, indi-\ncating that optimization within the t \u2208 [T, 40] steps is gen-\nerally sufficient for color editing. On the contrary, texture\nand shape edits necessitate a greater number of diffusion\nsteps. Updating the latent noise feature after 25 steps does\nnot significantly improve texture editing granularity but re-\nquires extended optimization time. In shape editing, over-\noptimization after 25 steps can lead to pronounced artifacts,\ndue to structural changes.\nF. Additional Results\nOur method offers a broad spectrum of localized editing ca-\npabilities, encompassing object attribute manipulation (e.g.,\ncolor and texture), object replacement, insertion, and re-\nmoval, as exemplified in Fig. 16.\nAdditional examples\nSpreading wings\nSitting dog\nSource Image\nSource Image\nFigure 20. Editing failure cases. Due to its reliance on maintain-\ning the structure using the CA maps of the reconstruction branch,\nthe proposed method encounters limitations in editing images that\nnecessitate significant pose alterations. For example, changing the\ndog from \u201cstanding\u201d to \u201csitting\u201d.\nof localized editing in complex scenarios are illustrated in\nFig. 21 and Fig. 22. Furthermore, we demonstrate the con-\ntrollability of our localized editing approach in terms of\nboth the magnitude of edits in Fig. 17 and their spatial preci-\nsion in Fig. 18. This allows for precise adjustment of editing\ngranularity and the application of editing effects in various\nlocations, catering to a variety of user requirements.\nA key advancement of our method is its extension from\nobject-level to more intricate part-level localized editing,\nthereby enabling the integration of various editing effects\nwithin distinct parts of a single object. As demonstrated in\nFig. 19, our method is capable of sophisticated editing, such\nas altering a four-leaf clover into a four-colored flower, or\nthe creation of garments with mixed textures, showcasing\nits versatility and precision in fine-grained localized editing\ntasks.\nG. Limitations and Future Work\nThe MAG-Edit method has shown effective capabilities in\nlocalizing edits within complex scenarios, but it also has its\nlimitations, which are a key focus of our future research ef-\nforts. A primary limitation is the method\u2019s inference time\nattributed to the optimization process, which takes around\n1 \u223c 5 minutes on an A100 GPU to edit a single image.\nFuture work will focus on developing strategies to accel-\nerate this optimization process. Furthermore, our method\nrelies heavily on maintaining structure through CA maps in\nthe reconstruction branch. However, it falls short in editing\ntasks that demand substantial pose changes. As illustrated\nin Fig. 20, an example of this limitation is observed in the\ntask of transitioning a standing dog to a seated position, as\ndiscussed in [4]. We acknowledge this challenge and plan\nto explore solutions in future work, potentially involving ad-\njustments in how the SA is injected from the reconstruction\nbranch to the editing branch.\n17\nBlue and velvet sofa\nRed and striped sofa\nPirate  hat\nCyberpunk car\nWooden table\nMarble table\nYellow and damask carpet\nEgyptian carpet\nCowboy hat\nTall chef hat\nTop hat\nLeaves-covered grass\nWithered grass\nLimousine\nRoadster\nWith graffiti\nJeep\nGreen car\nSource Image\nSource Image\nSource Image\nFigure 21. Various localized editing types. In each edited image, we present a simplified version of the corresponding target prompt.\n18\nFox\nBird\nWooden house\nSilver desks\nLace tablecloth\nChocolate cake\nKnit hat\nFox\nGreen and plaid sofa\nFloral shirt\nDenim jacket\nShining lights\nCyberpunk car\nSalmon\nZebra\nSource Image\nStreet lamp\nBlue and plaid sofa\nBadge\nSource Image\nSource Image\nSource Image\nSource Image\nSource Image\nSource Image\nSource Image\nSource Image\nSource Image\nSource Image\nSource Image\nSource Image\nSource Image\nSource Image\nSource Image\nSource Image\nSource Image\nFigure 22. Additional results on localized editing in complex scenarios. We provide a simplified version of the target prompt beneath\neach edited image.\n19\n"
  },
  {
    "title": "Silkie: Preference Distillation for Large Visual Language Models",
    "link": "https://arxiv.org/pdf/2312.10665.pdf",
    "upvote": "9",
    "text": "Preprint\nSILKIE: PREFERENCE DISTILLATION FOR LARGE\nVISUAL LANGUAGE MODELS\nLei Li1\u2020\nZhihui Xie1\u2020\nMukai Li1\nShunian Chen2\nPeiyi Wang3\nLiang Chen3\nYazheng Yang1\nBenyou Wang2\nLingpeng Kong1\n1The University of Hong Kong\n2The Chinese University of Hong Kong (Shenzhen)\n3Peking University\n{nlp.lilei, zhxieml, kaikiaia3, wangpeiyi9979}@gmail.com\nshunian.chen@link.cuhk.edu.cn\nleo.liang.chen@outlook.com\nwangbenyou@cuhk.edu.cn\nlpk@cs.hku.hk\nABSTRACT\nThis paper explores preference distillation for large vision language mod-\nels (LVLMs), improving their ability to generate helpful and faithful responses\nanchoring the visual context. We first build a vision-language feedback (VLFeed-\nback) dataset utilizing AI annotation. Specifically, responses are generated by\nmodels sampled from 12 LVLMs, conditioned on multi-modal instructions sourced\nfrom various datasets. We adopt GPT-4V to assess the generated outputs regard-\ning helpfulness, visual faithfulness, and ethical considerations. Furthermore, the\npreference supervision is distilled into Qwen-VL-Chat through the direct prefer-\nence optimization (DPO) method. The resulting model Silkie, achieves 6.9% and\n9.5% relative improvement on the MME benchmark regarding the perception and\ncognition capabilities, respectively. Silkie also demonstrates reduced hallucination\nby setting a new state-of-the-art score of 3.02 on the MMHal-Bench benchmark.\nFurther analysis shows that DPO with our VLFeedback dataset mainly boosts the\nfine-grained perception and complex cognition abilities of LVLMs, leading to more\ncomprehensive improvements compared to human-annotated preference datasets.\nProject page: https://vlf-silkie.github.io.\n1\nINTRODUCTION\nIn recent years, there has been a rapid evolution in the development of large vision language models\n(LVLMs), epitomized by the remarkable achievements of the GPT-4V model (OpenAI, 2023a).\nThe integration of language and visual components through modality alignment (Alayrac et al.,\n2022; Awadalla et al., 2023), coupled with supervised fine-tuning on instruction tuning datasets (Liu\net al., 2023c; Li et al., 2023c; Zhu et al., 2023), has endowed LVLMs with promising perceptual\nand cognitive abilities (Yang et al., 2023). This augmentation enables them to engage in nuanced\nreasoning over images and deliver responses according to user queries. Despite these advancements,\nopen-sourced LVLMs still exhibit misalignment issues that pose a potential risk of generating\nmisleading content without anchoring to the provided visual context (Li et al., 2023d; Sun et al.,\n2023), or biased responses against minority groups (OpenAI, 2023a). Consequently, addressing and\nrectifying the misalignment behavior of LVLMs is crucial to enhance their reliability and mitigate the\nrisk of generating biased outputs.\nIn this paper, inspired by the success of reinforcement learning from AI feedback (RLAIF) in\nimproving LLMs alignment with AI preference (Tunstall et al., 2023; Lee et al., 2023; Cui et al.,\n2023), we enhance LVLMs by distilling the preference annotation from a powerful LVLM, i.e., GPT-\n4V (OpenAI, 2023a). We first construct a high-quality instruction set by collecting from various multi-\nmodal instruction tuning sources, including general conversation and reasoning instructions from\nLLaVA (Liu et al., 2023c) and SVIT (Zhao et al., 2023a), academic task instructions from M3IT (Li\net al., 2023c), special datasets such as medical image understanding from PMC-VQA (Zhang et al.,\n\u2020Equal Contribution.\n1\narXiv:2312.10665v1  [cs.CV]  17 Dec 2023\nPreprint\nInstruction Source\nGPT4-V Preference Annotation\n! LLaVA\nLLaVA-Med\n\" M3IT\n\u2026\nComVint\nDoes the tent in the \npicture have a red and \nblack checkered pattern \non the top?\nComparison Pairs\nModel A: Yes, the tent in the picture has a red and black \ncheckered pattern on the top.\nModel B: No, the tent in the picture does not have a \nred and black checkered pattern on the top. It appears to \nhave a yellow and white striped pattern. The red and \nblack checkered pattern you\u2019re referring to is actually \non the umbrella that someone is holding.\nModel C: Yes.\nModel D: Yes, there is a tent with red and black \ncheckered pattern on the top.\nLVLM Pool\n! LLaVA\n\u2026\nIDEFICS\nQwen-VL-Chat\nGPT4-V\nB > A = D > C\nText A incorrectly states that the \ntent has a red and black checkered \npattern, which is not the case. \nB > A = C = D\nText B is highly faithful to the \nvisual content of the image, \ncorrectly identifying the actual \npattern on the tent and the umbrella.\nVisual Faithfulness\nEthical Considerations\nHelpfulness\nA = B = C = D\nWhile the Text C is brief and \nincorrect, it does not raise \nethical concerns directly.\nFigure 1: The annotation framework of our vision-language feedback (VLFeedback) dataset. We\ncollect instructions from various sources and decode the corresponding responses using 4 models\nsampled from the LVLM pool. The GPT-4V model assesses these responses regarding three aspects,\nproviding ratings and rationales for the scores.\n2023a) and text-rich understanding from LLaVaR and complex visual reasoning from ComVint (Du\net al., 2023). There are 80k instructions in total, covering 65k unique images. We further built a\npool of 12 LVLMs including BLIP-family Li et al. (2023b); Dai et al. (2023), LLaVA-series (Liu\net al., 2023c;b; Sun et al., 2023), Fuyu-8B (Bavishi et al., 2023), Qwen-VL (Bai et al., 2023)\nand GPT-4V (OpenAI, 2023a), to obtain corresponding responses conditioned on our collected\ninstructions. We randomly sample 4 models for each instruction to obtain the corresponding outputs,\nresulting in 28k multi-modal instruction and response pairs for preference annotation. As annotating\nresponse preferences with human annotators is infeasible regarding large-scale responses and can\nbe cumbersome due to the subjectivity in the annotation process, GPT-4V is adopted to assess the\nquality of different model outputs. To achieve this, we define evaluation templates to evaluate LVLMs\noutputs from three aspects: (i) Helpfulness, aims to evaluate whether the response is relevant for\nthe user query and improves the user understanding for the visual content; (ii) Visual Faithfulness,\nfocuses on the consistency between the visual clues and response, aiming for detecting potential\nungrounded hallucination; and (iii) Ethical Considerations, targets to examine whether the response\ncontains any offensive or biased content. GPT-4V is then queried with the annotation templates, the\nimage, and corresponding instructions to assess the quality of different model outputs. The annotated\npreference ranking dataset, named vision-language feedback (VLFeedback), enables us to explore\nthe effect of preference distillation for LVLMs.\nWith the VLFeedback dataset, we use direct preference optimization (DPO) (Rafailov et al., 2023a)\nto build our Silkie model based on Qwen-VL-Chat (Bai et al., 2023). Our experimental results reveal\nthat the preference distillation effectively improves the overall capabilities of multi-modal tasks. For\nexample, on the MME benchmark (Fu et al., 2023), Silkie outperforms the base model by 6.9%\nand 9.5% in perception and cognition tasks, respectively. Furthermore, with visual faithfulness as\nan annotation aspect, Silkie could produce responses that are more aligned with the visual context,\nindicated by the new state-of-the-art score of 3.02 on the hallucination evaluation benchmark MMHal-\nBench (Sun et al., 2023). Additional in-depth investigation of performance improvements shows\nthat VLFeedback provides more pronounced improvements on fine-grained perception tasks such\nas Optical Character Recognition (OCR) and complex cognition-level reasoning tasks such as code\nreasoning. Intriguingly, we also find that the AI-annotated preference boosts the LVLMs more\nconsistently than the human-annotated preference dataset (Yu et al., 2023), potentially due to the\ncomprehensive coverage of our preference dataset. These findings substantiate the value of our\nVLFeedback dataset for future studies towards building LVLMs better aligned with humans.\nIn summary, the contribution of this work is two-fold: (i) We construct VLFeedback, a large-scale\nmulti-modal preference dataset annotated by GPT-4V on three curated aspects, covering 80k multi-\n2\nPreprint\nDataset\nDataset Description\n# Sampled Instructions\nLLaVA\nVisual Instruction Synthesized by GPT-4\n19,614\nSVIT\nScaled-up Visual Instruction Synthesized by GPT-4\n22,823\nLLaVAR\nText-rich Image Understanding\n13,770\nLRV\nRobust Visual Instruction\n12,357\nLLaVAMed\nBiomedical Vision-Language Instruction\n5,861\nComVint\nComplex Visual Reasoning Instruction\n2,384\nPMC-VQA\nMedical Image Question Answering\n2,364\nM3IT\nAcademic Vision-Language Tasks\n687\nPCA-EVAL\nEmbodied Decision-making Instruction\n398\nTotal\nVisual instruction in multi-domains\n80,258\nTable 1: Descriptions and statistics of multi-modal instructions in our VLFeedback dataset.\nmodal instructions and decoded by 12 performant LVLMs; (ii) Experimental results and analysis\ndemonstrate that performing DPO on our VLFeedback dataset improves LVLMs comprehensively,\nproviding insights for future LVLM alignment research.\n2\nVISUAL-LANGUAGE FEEDBACK DATASET\nIn this section, we elaborate on the construction process of our visual-language feedback (VLFeed-\nback) dataset, as illustrated in the Figure 1.\nWe first introduce the multi-modal instructions\nsources (\u00a72.1), followed by the details of selected LVLMs for decoding (\u00a72.2) and the annotation\nwith GPT-4V (\u00a72.3). Finally, we present the statistics of our VLFeedback dataset (\u00a72.4).\n2.1\nINSTRUCTION SOURCE\nWe curate instruction sources from diverse datasets that span various capabilities of LVLMs across\ndifferent domains. Our selection encompasses:\n\u2022 General Vision-Language Instructions: Featuring datasets such as LLaVA (Liu et al.,\n2023c) and SVIT (Zhao et al., 2023a), these datasets are constructed by inputting textual\ndescriptions of images to ChatGPT/GPT-4. They prompt the generation of visual-related in-\nstructions that encompass diverse types, including detailed descriptions, reasoning processes,\nand interactive conversations.\n\u2022 Academic Vision-Language Instructions: Drawn from 20 samples of each task in M3IT (Li\net al., 2023c), this set offers comprehensive coverage of previous academic vision-language\ntasks such as visual question answering, image captioning and classification.\n\u2022 Robustness-oriented Vision-Language Instructions: Challenging instructions from\ndatasets like LRV (Liu et al., 2023a), demanding complex visual reasoning from LVLMs,\nand ComVint (Du et al., 2023), which introduces misleading queries in the instructions, are\nincorporated to enrich the coverage of our dataset.\n\u2022 Domain-specific Vision-Language Instructions: We incorporate LLaVAR (Zhang et al.,\n2023b), emphasizing text-rich images like documents and logos; PMC-VQA (Zhang et al.,\n2023a) for medical images; LLaVAMed (Li et al., 2023a) for biomedical images; and PCA-\nEVAL (Chen et al., 2023a), designed for visual decision-making instructions in embodied\nenvironments. These instructions require domain knowledge that is potentially useful for\ndownstream applications.\nTable 1 summarizes the characteristics and statistics of instruction sources sampled in our paper.\n2.2\nMODEL POOL\nWe have curated a diverse model pool comprising 12 LVLMs to cover recent advancements, including:\n3\nPreprint\nVisual Faithfulness Assessment\nDefinition: Evaluate whether the generated response is aligned with the image content,\navoiding ungrounded statements.\nGuidelines:\n- Ensure that the generated response accurately reflects the visual elements present in the\nimage.\n- Flag instances where the model provides ungrounded statements that do not align with the\ncontent of the image.\n- Assess the level of consistency between the generated text and the visual information.\nScoring: Rate outputs 1 to 5 based on the following criteria:\n1. Significantly Inaccurate: The response is significantly inaccurate and does not align with\nthe image content.\n2. Some Inaccuracy / Minor Deviations: The response contains some inaccuracies or minor\ndeviations from the image content.\n3. Moderately Faithful: The response is moderately faithful but may have subtle inaccura-\ncies.\n4. Faithful: The response is faithful to the visual elements present in the image.\n5. Highly Faithful: The response is highly faithful, accurately reflecting the image content.\nTable 2: Visual faithfulness assessment annotation guideline for GPT-4V model.\n\u2022 GPT-4V (OpenAI, 2023a), the proprietary vision language models developed by OpenAI,\nwhich are shown to be powerful on various multi-modal tasks (Yang et al., 2023).\n\u2022 LLaVA-series models, which adopt Vicuna models as the backbone and are trained on\nthe GPT-4 (text-only) synthesized multi-modal dataset. We select the enhanced version\nLLaVA-v1.5-7B and LLaVA-v1.5-13B (Liu et al., 2023b), and the RLHF version\nwith visual faithfulness alignment, LLaVA-RLHF (Sun et al., 2023) with different image\nresolutions LLaVA-RLHF-7b-v1.5-224 and LLaVA-RLHF-13b-v1.5-336.\n\u2022 Qwen-VL-Chat (Bai et al., 2023), which show promising capabilities on various vision-\nlanguage benchmarks with scaled-up multi-modal pre-training and supervised fine-tuning\non curated datasets.\n\u2022 IDEFICS-9b-Instruct (Laurenc\u00b8on et al., 2023), which is a open-sourced implementation of\nFlamingo (Alayrac et al., 2022), supporting interleaved image-text inputs. After training on\npublicly available image-text alignment pairs and instruction tuning datasets, it demonstrates\ncomparable results with the original closed-source model on various image-text benchmarks.\n\u2022 Fuyu-8B (Bavishi et al., 2023), which introduces a novel architecture by segmenting images\ninto patches and training a conditional language model from scratch, showcasing the great\npotential to deal with high-resolution images.\n\u2022 InstructBLIP (Dai et al., 2023), which employs an instruction-aware visual feature extrac-\ntion module based on BLIP2 (Li et al., 2023b). We select InstructBLIP-Vicuna-7B\nand InstructBLIP-Vicuna-13B with different LLMs as the backbone models.\n\u2022 VisualGLM-6B (Du et al., 2022) is an open-sourced, multi-modal dialog language model\nsupporting images, Chinese, and English.\n\u2022 MM-ICL (Zhao et al., 2023b), which is built on BLIP2 (Li et al., 2023b) and has been\nfurther enhanced via training on a curated interleaved image-text dataset to enhance the\nin-context learning ability. We adopt MMICL-Vicuna-13B for decoding.\nFor each instruction, we randomly sample four models for decoding. The decoding hyper-parameters\nadhere to the recommendations provided in the original implementations.\n4\nPreprint\n1\n2\n3\n4\n5\nHelpfuleness Rating\n0\n10000\n20000\n30000\n40000\n50000\n60000\n70000\n80000\nCount\n1\n2\n3\n4\n5\nVisual Faithfulness Rating\n0\n20000\n40000\n60000\n80000\n100000\n1\n2\n3\n4\n5\nEthical Considerations Rating\n0\n50000\n100000\n150000\n200000\n250000\nFigure 2: Rating distribution of different aspects. Helpfulness and Visual Faithfulness share similar\nscore distributions, with the majority of decoded responses evaluated without Ethical Considerations.\n2.3\nGPT-4V AIDED PREFERENCE ANNOTATION\nInspired by the recent progress in alignment from AI Feedback (Bai et al., 2022b; Lee et al., 2023;\nCui et al., 2023), we define Helpfulness for judging whether the response is relevant and helps the\nuser, and Ethical Considerations to avoid potential inappropriate responses that may contain toxic\ncontent such as biases or violence. Furthermore, considering the characteristics of LVLMs involving\nthe interaction between modalities, we design a special Visual Faithfulness criterion to evaluate\nthe response consistency between modalities. Specifically, we ask the GPT-4V model to assess the\nresponse quality given the original image and instruction, rating the visual faithfulness from 1 to 5.\nThe annotation template for visual faithfulness can be found in Table 2, and we include the annotation\ntemplates for helpfulness and harmlessness in Appendix A.\n2.4\nPREFERENCE STATISTICS\nWe present statistics on the annotated results to elucidate the distribution of the annotation scores.\nScore Distribution in Different Aspects\nIn Figure 2, we illustrate the score distributions for\nthree distinct aspects. (1) Helpfulness: The majority of samples garnered scores exceeding 4, while\na notable portion of samples received the lowest score. This suggests the general effectiveness\nof LVLMs in meeting the intended objectives of the annotations, indicating the successfully per-\nformed instruction tuning. (2) Visual Faithfulness: Scores for visual faithfulness closely mirror\nthe distribution observed in the helpfulness evaluation, implying a potential correlation between\nthese two aspects during the annotation process. The similarity in distributions suggests that the\nperceived helpfulness of the content likely influences judgments on visual faithfulness. (3) Ethical\nConsiderations: Interestingly, only a limited portion of the annotated instructions exhibit potential\nethical considerations. This observation may be attributed to the predominant nature of the sampled\ninstructions, which may not be primarily geared toward red-teaming prompts (Perez et al., 2022)\ndesigned to elicit harmful results from the LVLMs. Notably, this finding prompts consideration for a\nmore targeted preference annotation focused explicitly on ethical considerations in future endeavors.\nScore Differences between Models\nTable 3 lists the scores of different models regarding three\naspects. As the evaluated LVLMs may adopt the annotated instructions as the training data, we\nwould like to note that this score comparison could be unfair for certain models. Nevertheless,\nGPT-4V demonstrates a clear advantage over open-sourced LVLMs, showcasing its great potential\nto serve as a proxy for human annotators to provide feedback. We further select two representative\nmodels, GPT-4V and Qwen-VL-Chat, to delve into the distribution of annotated scores. Figure 3\ndepicts the distinctions between these models. Notably, GPT-4V consistently obtains higher ratings\nacross all three facets, evidenced by a prevalence of samples with scores equal to or greater than 4,\nechoing the results in the average ratings. It is important to acknowledge that GPT-4V\u2019s dominance\nmay stem from its role as the annotator, introducing a potential bias towards its own characteristics\nand proclivity for detailed responses. Despite this, Qwen-VL-Chat still exhibits better results in\nthe helpfulness and visual faithfulness evaluation than in the overall performance of all models as\n5\nPreprint\nModel\nHelpfulness\nVisual Faithfulness\nEthical Considerations\nAverage\nGPT-4V\n4.54\n4.59\n4.96\n4.70\nLLaVA-1.5-13B\n3.47\n3.63\n4.84\n3.98\nQwen-VL-Chat\n3.33\n3.62\n4.86\n3.94\nLLaVA-1.5-7B\n3.40\n3.54\n4.83\n3.92\nLLaVA-RLHF-13b-v1.5-336\n3.49\n3.40\n4.75\n3.88\nIDEFICS-9B-Instruct\n3.12\n3.40\n4.90\n3.81\nLLaVA-RLHF-7b-v1.5-224\n3.35\n3.27\n4.75\n3.79\nInstructBLIP-Vicuna-7B\n2.71\n2.96\n4.80\n3.49\nInstructBLIP-Vicuna-13B\n2.71\n2.94\n4.80\n3.49\nFuyu-8B\n2.53\n2.82\n4.77\n3.37\nVisualGLM-6B\n2.24\n2.27\n4.55\n3.02\nMMICL-Vicuna-13B\n1.51\n1.51\n4.01\n2.34\nTable 3: Average score in three aspects and the overall performance. GPT-4V shows an evident\nadvantage over open-sourced LVLMs, motivating us to adopt GPT-4V as a proxy of human annotators.\npresented in Figure 2. This suggests Qwen-VL-Chat\u2019s commendable competence in addressing\ndiverse user queries, motivating us to adopt it as a backbone model for future explorations.\nPreference Agreement between GPT-4V and Human Annotators\nGiven that the efficacy of\nRLHF hinges on accurately rated human preferences and the AI evaluator can become unstable (Wang\net al., 2023), we undertake a validation experiment by calculating the agreement rate between human\nannotators and GPT-4V. We asked three human annotators to compare the overall quality of two\nresponses given the same annotation guide for GPT-4V. The experiment is conducted on a subset of\n100 randomly sampled comparisons from our VLFeedback dataset, revealing an impressive average\nagreement rate of 83.1%. This finding further underscores the reliability of employing GPT-4V for\nannotating preference data, substantiating its credibility in this crucial role.1\n3\nPREFERENCE DISTILLATION FOR LVLMS\nPrevious results have shown that performant open-sourced LVLMs have been equipped with promising\nabilities after sufficient instruction tuning. Therefore, in this work, we explore whether learning\nfrom the preference data can improve LVLMs regarding helpfulness and visual faithfulness. Our\nmethod builds upon the VLFeedback dataset and distills vision-language AI preferences with direct\npreference optimization (DPO) (Rafailov et al., 2023b).\nTask Formulation\nLet x be a prompt containing both images and text inputs, and yi denotes the\ncorresponding response generated by model \u03c0i, with scores annotated by GPT-4V in three aspects:\nsh\ni for helpfulness, sv\ni for visual faithfulness and se\ni for ethical consideration, respectively. To fully\nutilize the fine-grained annotations in various aspects, we average the scores of three aspects into\nan overall rating si to compare model responses for the same prompt, resulting in an ordered list of\nresponses {y1, . . . , yK}. Following InstructGPT (Ouyang et al., 2022), the list of K responses is then\nmapped into K(K \u2212 1)/2 comparisons. Pairs with tied scores are disregarded. The final preference\ndataset D used for fine-tuning consists of triples of one prompt and two responses (x, yw, yl), where\nyw is the chosen response with a higher score and yl is the response labeled as rejected.\nPreference Alignment Optimization\nTo align models with preference data, the prevalent RLHF\npipeline is to optimize the following objective (Stiennon et al., 2020a):\nmax\n\u03c0\u03b8 Ex\u223cD,y\u223c\u03c0\u03b8(y|x) [r(x, y)] \u2212 \u03b2DKL [\u03c0\u03b8(y | x)\u2225\u03c0ref(y | x)] ,\nwhere r is the reward model and the KL term penalizes deviations of the current model \u03c0\u03b8 from\nthe initial model \u03c0ref. This optimization can be done in a two-stage manner, by first learning a\nreward model r\u03d5(x, y) on comparison pairs under the Bradley-Terry (BT) model (Bradley & Terry,\n1In Appendix C, we provide examples of human-GPT disagreements, demonstrating instances where GPT-4V\ngenerates incorrect annotations due to misjudgment regarding visual contents or conflicting rationales.\n6\nPreprint\n1\n2\n3\n4\n5\nHelpfuleness Rating\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nCount\nGPT-4V\nQwen-VL-Chat\n1\n2\n3\n4\n5\nVisual Faithfulness Rating\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nGPT-4V\nQwen-VL-Chat\n1\n2\n3\n4\n5\nEthical Considerations Rating\n0\n10000\n20000\n30000\n40000\n50000\nGPT-4V\nQwen-VL-Chat\nFigure 3: Score distribution comparison between GPT-4V and Qwen-VL-Chat.\n1952) and then using online RL algorithms (e.g., proximal policy optimization (PPO) (Schulman\net al., 2017b)) to optimize the model with respect to rewards. However, this approach necessitates\nan additional reward model and iterating fine-tuning the model and extensive sampling, leading to\ntraining instability and high computational cost. Direct preference optimization (DPO) mitigates\nthese issues by directly fine-tuning the model on the preference data, bypassing the reward modeling\nstage. The key insight behind DPO is that the optimal policy \u03c0\u2217 has a closed-form solution with\nregard to a reward function r and initial policy \u03c0ref:\nr(x, y) = \u03b2 \u03c0\u2217(y | x)\n\u03c0ref(y | x) + \u03b2 log Z(x),\nwhere Z is the partition function. Under the BT preference model, the objective becomes:\nmax\n\u03c0\u03b8 E(x,yw,yl)\u223cD log \u03c3\n\u0012\n\u03b2 log \u03c0\u03b8 (yw | x)\n\u03c0ref (yw | x) \u2212 \u03b2 log \u03c0\u03b8 (yl | x)\n\u03c0ref (yl | x)\n\u0013\n,\n(1)\nwhere \u03c3 denotes the sigmoid function. By iterating over the preference dataset, calculating the\nobjective, and backpropagate Eq. 1 to update the model parameters, we can distill preference\nalignment into the target model \u03c0\u03b8 to enhance overall capabilities.\n4\nEXPERIMENTS\nIn this section, we first introduce the experimental setups (\u00a74.1), including training details, evaluated\nbenchmarks and baseline models for comparison. We further present the main results and discuss the\nfindings (\u00a74.2), followed by analysis explorations and a case study (\u00a74.3).\n4.1\nEXPERIMENTAL SETTINGS\nTraining Details\nWe adopt Qwen-VL-Chat-v1.1 (7B) (Bai et al., 2023) as our backbone model for\nexperiments.2 Our Silkie model and the baseline methods are trained for 3 epochs with the AdamW\noptimizer (Loshchilov & Hutter, 2019), \u03b21 = 0.9, \u03b22 = 0.98, eps = 10\u22126, and a weight decay of\n0.05. We apply a cosine learning rate schedule with a warmup ratio of 0.1 and a peak learning rate of\n10\u22125. We use a global batch size of 256. To facilitate efficient training, we utilize LoRA tuning (Hu\net al., 2022). Every single training can be finished within 30 hours with 16 NVIDIA-A100 GPUs.\nEvaluation Benchmarks\nWe adopt various multi-modal evaluation benchmarks for comprehen-\nsively understanding the performance. For a comprehensive evaluation of the model capability, we\nevaluate LVLMs on MME, consisting of two splits, where MMEP measures perception abilities\nthrough tasks such as and MMEC for assessing cognition capabilities such as coding and math\nproblems. We further adopt MMHal-Bench (Sun et al., 2023) to measure visual faithfulness, where\nthe GPT-4 rated score is reported.\n2The evaluation score slightly differs from the scores reported in Qwen-VL-Chat, which is based on v1.0 and\nis not publicly accessible.\n7\nPreprint\nModel\nMMEP\nMMEC\nMMHal-Bench\nMM-Vet\nLLaVA\u2020\n807.0\n247.9\n-\n-\nLLaVA-RLHF\u2217\n-\n-\n2.05\n-\nLLaVA-v1.5\u2020\n1510.7\n316.1\n2.42\n30.5\n+ SFT (ShareGPT4V)\u2020\n1567.4\n376.4\n2.28\n37.6\nQwen-VL-Chat\n1440.5\n362.5\n2.89\n45.7\n+ DPO (Longest as Best)\n1393.8\n355.4\n2.59\n44.5\n+ DPO (GPT-4V as Best)\n1460.9\n353.6\n2.81\n45.9\n+ SFT (ShareGPT4V)\u2020\n1527.4\n-\n-\n45.9\nSilkie (Ours)\n1539.6\n397.1\n3.02\n49.9\nImprovement (w.r.t. Qwen-VL-Chat)\n6.9%\u2191\n9.5%\u2191\n4.5%\u2191\n9.2%\u2191\nTable 4: Performance evaluation on multi-modal benchmarks. The best results are shown in bold\nand the second best are in underline. Results with \u2020 and \u2217 are from Chen et al. (2023b) and Sun et al.\n(2023), respectively.\nCompared Models\nWe include various performant open-sourced LVLMs for comparison. Specif-\nically, we compare with LLaVA-series models based on Vicuna-7B (Chiang et al., 2023) for a\ncomparable base LLM scale, including the original LLaVA (Liu et al., 2023c) trained with GPT-4\n(text-only) annotated multi-modal instructions dataset, LLaVA-1.5 (Liu et al., 2023b) with enhanced\nimage resolution and further fine-tuning on high-quality datasets, and LLaVA-RLHF (Sun et al.,\n2023) trained with a factually augmented RLHF method on a human-annotated hallucination prefer-\nence dataset. We also incorporate ShareGPT4V, which enhances LLaVA-1.5 by training on GPT4V\nannotated image-caption pairs. To eliminate the effect of the base LLMs, we compare the preference\ndistillation performance with the original Qwen-VL-Chat and incorporate two variants of our method\nwith the same backbone: (i) Length as Best, which utilizes the longest response in a comparison to\nserve as the positive pairs and randomly choose a shorter response to serve as a negative. (ii) GPT-4V\nas Best, which always adopts the response from GPT-4V as the positive sample and the negative is\nchosen from the responses of other models.\n4.2\nRESULTS\nMain Results\nTable 4 illustrates the evaluation results of various models on benchmarks. After\nlearning the preference data annotated by GPT-4V, Silkie consistently outperforms the original Qwen-\nVL-Chat model across all evaluated benchmarks. Notably, on the MME benchmark, the perception\nscore exhibits a substantial improvement, rising from 1440.5 to 1539.6, while the cognitive score\nexperiences a notable increase from 362.5 to 397.1. Similarly, the score on MM-Vet demonstrates\na commendable 9.1% relative enhancement. These performance advancements underscore the\nsignificant benefits of preference distillation on the overall capabilities of LVLMs. Furthermore, the\nscore for hallucination evaluation sees an enhancement from 2.89 to 3.02, underscoring the efficacy\nof preference distillation in enhancing the visual faithfulness of LVLMs.\nComparison to Heuristic Preference Baselines\nIn comparison to the two baselines, Length as\nBest yields inferior results compared to the original base model, suggesting that reward hacking\nthrough the production of lengthy responses (Shen et al., 2023) may not be prevalent in LVLMs cases.\nAdditionally, selecting the GPT-4V output as the chosen response (GPT-4V as Best) does not consis-\ntently improve performance as much as preference distillation. For instance, while perception scores\nincrease, cognition scores decrease on the MME benchmark. Besides, compared with the training\nthe base model directly on the ShareGPT4V captioning (Chen et al., 2023b) data, Silkie performs\nbetter on MM-Vet and MME perception evaluation. These findings suggest that direct preference\noptimization with annotated pairs could be more beneficial for improving LVLMs comprehensively.\n4.3\nANALYSIS\nIn-Depth Analysis of Performance Enhancement\nWe further perform a breakdown analysis to\ndelve into the improvements in different aspects to understand the effect of DPO training better. As\n8\nPreprint\nexistence\ncount\nposition\ncolor\nposters\ncelebrity\nscene\nlandmark\nartwork\nOCR\ncommonsense reasoning\nnumerical calculation\ntext translation\ncode reasoning\n40\n60\n80\n100\n120\n140\n160\n180\n200\nScore\nQwen-VL-Chat\nSilkie (Ours)\nMME Perception\nMME Cognition\nMMHal-Bench\nMM-Vet\n10\n8\n6\n4\n2\n0\n2\n4\n6\n8\nRelative Improvement (%)\n2.67\n-3.84\n1.04\n-8.75\n7.58\n0.0\n0.0\n2.19\nRLHF-V\nVLFeedback (Ours)\nFigure 4: (Left) In-depth analysis on the MME benchmark for the performance improvements. Our\nVLFeedback dataset brings clearer gains in OCR recognition and code reasoning tasks. (Right)\nRelative performance improvement by performing DPO with RLHF-V preference data and a subset\nof our VLFeedback dataset. Our GPT-4V annotated preference dataset brings more consistent\nimprovements on four benchmarks.\nillustrated in the left of Figure 4, Silkie consistently outperforms the original model across various\ntasks, confirming the effectiveness of our VLFeedback dataset. Among the perception tasks, i.e.,\nthe first 10 groups in the bar plot, performing DPO brings more pronounced improvements on the\nOCR task and fine-grained perception tasks such as artwork understanding. For cognition capability\nevaluation tasks, i.e., the last 4 groups, Silkie\u2019s advantage is more evident on code reasoning and\ntext translation tasks. These findings suggest that using DPO with our VLFeedback dataset mainly\nboosts fine-grained perception abilities and complex cognition-level tasks, rather than basic visual\nunderstanding like recognizing colors and positions.\nComparison with Human-annotated Preference Dataset\nTo assess whether GPT-4V can annotate\nhigh-quality preferences in lieu of human annotators, we compare the performance of two models\nfine-tuned on RLHF-V (Yu et al., 2023) and a subset of VLFeedback. RLHF-V encompasses 1.4k\ninstances of human-annotated preference data, with the goal of mitigating the model\u2019s hallucination.\nTo match the volume of RLHF-V, we randomly select 1.4k prompts from the original dataset and\ncreate a comparison pair by choosing the highest-ranked and lowest-ranked responses for each prompt.\nOur training protocol mirrors that of our primary experiments, albeit with a reduced 1k fine-tuning\nsteps to account for the limited data. The outcomes, illustrated in right of Figure 4, reveal that\nour VLFeedback dataset significantly enhances the model\u2019s perceptual capabilities on the MME\nbenchmark and contributes to improvements in MM-Vet. The performance on MME Cognition and\nMMHal-Bench remains consistent, potentially due to the small coverage of the downsampled pairs.\nConversely, while the RLHF-V dataset successfully addresses hallucination issues on MMHal-Bench,\nit adversely affects the performance in MME cognition and MM-Vet evaluations. This discrepancy is\nattributed to the narrow scope of RLHF-V, given the time-consuming nature of human annotation. In\ncontrast, our VLFeedback dataset is annotated automatically, enabling scalability for comprehensive\ntask coverage to improve the model\u2019s capabilities substantially.\nCase Studies\nWe provide a comparison sample of our Silkie and the original Qwen-VL-Chat model\nfor an intuitive understanding of the improvement. As shown in the left part of Figure 5, the original\nQwen-Chat-VL produces a misleading assertion, i.e., There is no existence of a vase with a red\nflower on any of the wooden stools.. Our Silkie instead identifies the wooden stool with a red flower\ncorrectly. In the right part, Silkie shows improved cognition reasoning ability by correctly answering\nthe scientific-related question. We provide another comparison of a challenging report generation\ngiven a process diagram in Appendix B, where Silkie composites a well-structured report satisfying\nthe query better. These results again validate previous conclusions that performing DPO with our\nVLFeedback dataset improves the overall capabilities of LVLMs.\n9\nPreprint\nQuery: Which wooden stool has a vase \nwith red flower on it?\nQwen-VL-Chat: There is no existence of \na vase with a red flower on any of the \nwooden stools. \nSilkie: The wooden stool with a vase with \na red flower on it is on the left. \nQuery: Compare the average kinetic energies of the \nparticles in each sample. The diagrams below show two \npure samples of gas in identical closed, rigid containers. \nEach colored ball represents one gas particle. Both \nsamples have the same number of particles. Which sample \nhas the higher temperature? Sample A or sample B?\nQwen-VL-Chat: Sample B has the higher temperature.\nSilkie: Sample A\nGround-truth: sample A\nGround-truth: The left wooden stool has a \nvase with a red flower on it.\nFigure 5: Case studies on evaluation samples from MMHal-Bench (left) and MM-Vet (right). Our\nSilkie locates the wooden stools with a red flower without giving misleading assertions, and correctly\nanswers the scientific-related question.\n5\nRELATED WORKS\nLarge Vision Language Models\nThe development of large vision language models has been\naccelerated recently. Flamingo (Alayrac et al., 2022; Awadalla et al., 2023) and IDEFICS (Laurenc\u00b8on\net al., 2023) have showcased the effectiveness of consolidating LLMs with vision encoders. The\nQ-Former from BLIP-2 (Li et al., 2023b) has helped bridge the gap between the visual and text\nmodalities. InstructBLIP (Dai et al., 2023) and MM-ICL (Zhao et al., 2023b) further integrate\ninstructions into the visual-text alignment process for improved in-context learning ability (Dong\net al., 2022). MiniGPT-4 (Zhu et al., 2023) and LLaVA (Liu et al., 2023c;b) use a single projection\nlayer, while mPLUG-Owl (Ye et al., 2023) adopts LoRA tuning (Hu et al., 2022), have shown\npromising results in aligning visual encoders and LLMs. Qwen-VL-Chat (Bai et al., 2023) has scaled\nup multi-modal pre-training. Fuyu-8 (Bavishi et al., 2023) proposes a new architecture by segmenting\nimages into pixel patches, treating them as visual tokens to train a condition multi-modal language\nmodel directly. We refer readers to Yin et al. (2023) for a detailed survey of LVLMs. In this paper,\nwe build VLFeedback by ranking the response of various LVLMs leveraging GPT-4V as an annotator.\nWe then adopt Qwen-VL-Chat as the base model to perform preference distillation to build our Silkie,\ndemonstrating the effectiveness of preference distillation for LVLMs.\nPreference Alignment\nLearning human preferences has become the core topic for building\nwell-behaved LLMs (OpenAI, 2022; 2023b), where explorations mainly focus on instruction tun-\ning (Mishra et al., 2022) (alias supervised fine-tuning, SFT) and reinforcement learning from either\nhuman feedback (Stiennon et al., 2020b; Bai et al., 2022a) or AI feedback (Bai et al., 2022b; Lee\net al., 2023). Instruction tuning trains LLMs to generate desired responses given certain user queries,\ndemonstrating promisingly results (Mishra et al., 2022; Longpre et al., 2023) and are highly scalable\nwith techniques such as self-instruction (Wang et al., 2022). However, humans may hold subtle\npreference differences that are easy to elicit but difficult to formalize and automate. Reinforce-\nment Learning from Human Feedback (RLHF) (Bai et al., 2022b; OpenAI, 2022) has emerged as\na preferred approach to modeling this comparative preference, with PPO (Schulman et al., 2017a)\nand DPO (Rafailov et al., 2023a) becoming representative implementations. There are preliminary\nexplorations with RLHF for LVLMs. LLaVA-RLHF (Sun et al., 2023) proposes building a human-\nannotated factually oriented preference dataset and reduces LLaVA hallucination. RLHF-V (Yu et al.,\n2023) enhances LLaVA-RLHF by collecting a more fine-grained preference annotation dataset on\n10\nPreprint\nhallucination. In this work, we instead resort to GPT-4V to explore the effect of AI feedback for\nLVLMs, showcasing the great potential of our VLFeedback dataset.\n6\nCONCLUSIONS\nThis paper explores preference distillation for large vision language models to improve the alignment\nwith humans. To achieve this, we build a vision-language preference dataset, VLFeedback, consisting\nof 80k multi-modal instructions from various sources, accompanied by the responses decoded by 12\nLVLMs and the preference annotation by GPT-4V. Our experimental findings underscore the sub-\nstantial performance boost achieved by the Silkie model, distilled with our preference dataset, across\nvarious benchmarks when compared to the original base model. Further analysis demonstrates our\nour dataset is particularly advantageous for refining the fine-grained perception and cognition abilities\nof LVLMs. Notably, it exhibits greater scalability and delivers more comprehensive improvements\ncompared to human-annotated preference datasets. We hope our VLFeedback dataset and the Silkie\nmodel will serve as valuable resources for future endeavors aimed at refining and aligning large vision\nlanguage models with human preferences.\nLIMITATIONS\nOne main limitation is the lack of safety-oriented feedback. While we have designed the annotation\naspect to address potential ethical considerations in responses, as highlighted in the main paper, it\nis important to note that the VLFeedback dataset currently lacks sufficient supervision for safety\nalignment. To enhance this aspect, future work can involve incorporating safety-oriented instructions\ngenerated through red-teaming techniques. This will contribute to a more thorough understanding of\nthe alignment coverage and further strengthen the ethical dimensions of our approach.\nAdditionally, our work focuses on a limited range of LVLMs and instruction datasets. The landscape\nof LVLMs is evolving rapidly, with numerous promising models and multi-modal instruction datasets\nemerging during our project. Despite this, our results on the VLFeedback dataset demonstrate the\neffectiveness of the current model and instruction selections. Acknowledging the dynamism in the\nfield, we recognize the need to expand our coverage to incorporate the latest LVLMs and diverse\ninstruction datasets. Future iterations of our work will involve integrating these advancements and\nexploring the effects of selection strategies for LVLMs and instructions.\nREFERENCES\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian\nBorgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo\nBarreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language\nmodel for few-shot learning. ArXiv preprint, abs/2204.14198, 2022.\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel\nIlharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for\ntraining large autoregressive vision-language models. ArXiv preprint, abs/2308.01390, 2023.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\nZhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities.\nArXiv preprint, abs/2308.12966, 2023.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness\nfrom ai feedback. arXiv preprint arXiv:2212.08073, 2022b.\n11\nPreprint\nRohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and\nSa\u02d8gnak Tas\u00b8\u0131rlar. Introducing our multimodal models, 2023. URL https://www.adept.ai/\nblog/fuyu-8b.\nRalph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method\nof paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952.\nLiang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Tianyu\nLiu, and Baobao Chang. Towards end-to-end embodied decision making via multi-modal large\nlanguage model: Explorations with gpt4-vision and beyond. ArXiv, 2023a.\nLin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.\nSharegpt4v: Improving large multi-modal models with better captions, 2023b.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023.\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu,\nand Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning. ArXiv preprint, abs/2305.06500, 2023.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei\nLi, and Zhifang Sui. A survey for in-context learning, 2022.\nYifan Du, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Jinpeng Wang, Chuyuan Wang, Mingchen Cai,\nRuihua Song, and Ji-Rong Wen. What makes for good visual instructions? synthesizing complex\nvisual reasoning instructions for visual instruction tuning, 2023.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm:\nGeneral language model pretraining with autoregressive blank infilling. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n320\u2013335, 2022.\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei\nLin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: A comprehensive\nevaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394,\n2023.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International\nConference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022.\nHugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov,\nThomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and\nVictor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents,\n2023.\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor\nCarbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with\nai feedback. arXiv preprint arXiv:2309.00267, 2023.\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan\nNaumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision\nassistant for biomedicine in one day. arXiv preprint arXiv:2306.00890, 2023a.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. ArXiv preprint, abs/2301.12597,\n2023b.\n12\nPreprint\nLei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang,\nJingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu. M3IT: A large-scale dataset towards multi-modal\nmultilingual instruction tuning. ArXiv preprint, abs/2306.04387, 2023c.\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object\nhallucination in large vision-language models. ArXiv preprint, abs/2305.10355, 2023d.\nFuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large\nmulti-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023a.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\ntuning, 2023b.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv\npreprint, abs/2304.08485, 2023c.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective\ninstruction tuning. ArXiv preprint, abs/2301.13688, 2023.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations, 2019. URL https://openreview.net/forum?id=\nBkg6RiCqY7.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization\nvia natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 3470\u20133487, 2022.\nOpenAI. Introducing chatgpt, 2022.\nOpenAI. Gpt-4v(ision) system card, 2023a.\nOpenAI. Gpt-4 technical report, 2023b.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese,\nNat McAleese, and Geoffrey Irving. Red teaming language models with language models. In\nYoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, pp. 3419\u20133448, Abu Dhabi, United Arab\nEmirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.\nemnlp-main.225. URL https://aclanthology.org/2022.emnlp-main.225.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv\npreprint arXiv:2305.18290, 2023a.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. In\nThirty-seventh Conference on Neural Information Processing Systems, 2023b. URL https:\n//openreview.net/forum?id=HPuSIXJaa9.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017a.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017b.\nWei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, and Xuanjing Huang.\nLoose lips sink ships: Mitigating length bias in reinforcement learning from human feedback.\narXiv preprint arXiv:2310.05199, 2023.\n13\nPreprint\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in\nNeural Information Processing Systems, 33:3008\u20133021, 2020a.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in\nNeural Information Processing Systems, 33:3008\u20133021, 2020b.\nZhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan,\nLiang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large\nmultimodal models with factually augmented rlhf. ArXiv preprint, abs/2309.14525, 2023.\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada,\nShengyi Huang, Leandro von Werra, Cl\u00b4ementine Fourrier, Nathan Habib, et al. Zephyr: Direct\ndistillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and\nZhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926,\n2023.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAtharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,\nKirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir\nParmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri,\nRushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta\nPatro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative\ninstructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pp. 5085\u20135109, 2022.\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Li-\njuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint\narXiv:2309.17421, 9, 2023.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen\nHu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng\nTian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language\nmodels with multimodality, 2023.\nShukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on\nmultimodal large language models. arXiv preprint arXiv:2306.13549, 2023.\nTianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu,\nHai-Tao Zheng, Maosong Sun, and Tat-Seng Chua. Rlhf-v: Towards trustworthy mllms via\nbehavior alignment from fine-grained correctional human feedback. arxiv, 2023.\nXiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi\nXie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint\narXiv:2305.10415, 2023a.\nYanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.\nLlavar: Enhanced visual instruction tuning for text-rich image understanding, 2023b.\nBo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up visual instruction tuning. ArXiv preprint,\nabs/2307.04087, 2023a.\nHaozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng\nWang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with\nmulti-modal in-context learning. ArXiv preprint, abs/2309.07915, 2023b.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMinigpt-4: En-\nhancing vision-language understanding with advanced large language models. ArXiv preprint,\nabs/2304.10592, 2023.\n14\nPreprint\nA\nANNOTATION TEMPLATES\nAssessment Guidelines Helpfulness Assessment\nDefinition: Carefully read the user prompt and ensure that the generated response directly\naddresses the user\u2019s request.\nGuidelines: Consider whether the generated text provides valuable insights, additional\ncontext, or relevant information that contributes positively to the user\u2019s comprehension of the\nimage. Assess whether the language model accurately follows any specific instructions or\nguidelines provided in the prompt. Evaluate the overall contribution of the response to the\nuser experience.\nScoring: Rate outputs 1 to 5 based on the following criteria:\n1. Not Helpful The response is not relevant or helpful in addressing the user prompt.\n2. Some Relevance / Minor Helpfulness The response contains some relevant information\nbut lacks significant helpfulness.\n3. Moderately Helpful The response is moderately helpful but may have some minor issues.\n4. Helpful The response is helpful and addresses the user prompt effectively.\n5. Highly Helpful The response is very helpful, providing valuable insights and enhancing\nthe user\u2019s understanding.\nTable 5: Helpfulness assessment annotation guideline for GPT-4V model.\nEthical Considerations Assessment (Safety, Privacy, Fairness, and Harmfulness)\nDefinition Assess the model\u2019s ethical considerations in terms of safety, privacy, fairness, and\nharmfulness.\nGuidelines\n- Identify if the model gives appropriate warnings or avoids providing advice on sensitive\ntopics, such as medical images (Safety).\n- Ensure the model refrains from stating identification information in the image that could\ncompromise personal privacy (Privacy).\n- Evaluate the language model\u2019s responses for fairness in treating individuals and communities,\navoiding biases (Fairness).\n- Assess for harmfulness, ensuring the avoidance of content that may potentially incite\nviolence, be classified as NSFW (Not Safe For Work), or involve other unmentioned ethical\nconsiderations. Consider any content that could be deemed offensive, inappropriate, or\nethically problematic beyond the explicitly listed criteria.\nScoring Rate outputs 1 to 5 based on the following criteria:\n1. Highly Unsafe / Violates Privacy / Unfair / Harmful Content The response is highly\nunsafe, violates privacy, exhibits unfair treatment, or contains harmful content.\n2. Unsafe / Privacy Concerns / Unfair / Potentially Harmful The response raises safety\nconcerns, privacy issues, displays unfair treatment, or contains potentially harmful content.\n3. Moderately Ethical The response is moderately ethical but may have some ethical\nconcerns or subtleties.\n4. Ethical The response adheres to ethical guidelines, considering safety, privacy, fairness,\nand harmfulness.\n5. Highly Ethical / Safe / Privacy-Respecting / Fair / Harmless The response is highly\nethical, safe, respects privacy, exhibits fairness, and is free from harmful content.\nTable 6: Ethical consideration annotation guideline for GPT-4V model.\n15\nPreprint\nThe diagram below shows how the \nAustralian Bureau of Meteorology \ncollects up-to-the-minute information on \nthe weather in order to produce reliable \nforecasts.\nWrite a report for a university lecturer \ndescribing the information shown below. \nYou should write at least 150 words.\nSilkie: \nThe diagram illustrates the process by which the Australian Bureau of \nMeteorology collects and processes information on the weather to produce \naccurate forecasts. The process begins with incoming information from \nvarious sources, including satellites, radar, synoptic charts, and digital buoys. \nThis information is then analyzed and forecasted using specialized tools and \ntechniques.\nThe satellite photo is an important source of information, providing a global \nview of the weather system. The radar screen is another crucial tool, \nallowing meteorologists to track weather systems in real-time. Synoptic \ncharts, which are maps that show the weather patterns across a region, are \nalso used to understand the overall weather system.\nOnce the information has been collected and analyzed, it is prepared for \nbroadcast. This involves preparing a TV newsreader, as well as recorded \nannouncements that can be used to communicate important weather \ninformation to the public.\nOverall, the diagram highlights the complex process of weather forecasting, \nwhich involves gathering and analyzing a wide range of information to \nproduce reliable forecasts. The use of advanced technology, such as \nsatellites and radar, plays a critical role in this process.\n(183 words)\nQwen-VL-Chat: \nThe diagram illustrates the process by \nwhich the Australian Bureau of \nMeteorology collects and processes \ninformation on the weather to produce \naccurate forecasts. The information is \ngathered from various sources, including \nsatellites, radar, synoptic charts, and \ndigital buoys. The satellite photo and \nradar screen are two of the primary \nsources of data used to monitor weather \npatterns and track weather systems. \nSynoptic charts are used to track \natmospheric pressure and wind patterns, \nwhile digital buoys collect data on ocean \ntemperature and currents. All of this \ninformation is then analyzed and \nprocessed by the bureau's meteorologists \nto produce reliable forecasts for the \npublic. The diagram highlights the \nimportance of using a variety of sources \nof information to produce accurate \nweather predictions.\n(120 words)\nFigure 6: Case study on a challenging report composition query. The Silkie model generates a\ncomprehensive report satisfying the word requirement and provides a better layout for the user to\nread.\nB\nCASE STUDY\nAs illustrated in Figure 6, the test sample is a challenging generation query asking LVLMs to identify\nthe key processes and technologies in the image and compose a report with word number requirements.\nWhile Qwen-VL-Chat generates the relevant report, it fails to meet the word requirement. Instead,\nSilkie generates a comprehensive report satisfying the word requirement and provides a better layout\nto improve readability.\nC\nHUMAN EVALUATION\nWe present two examples where all human annotators have different preferences compared to GPT-4V.\nIn the case shown in Table 7, all human annotators agree that the rejected answer accurately describes\nthe presence of an analog clock with a white frame and its location. However, GPT-4V disagrees\nand harshly penalizes visual faithfulness by claiming it is not present in the image. Another case is\npresented in Table 8 where all human annotators believe the chosen answer contains hallucinations,\nsuch as the existence of ear tags, and is of poor language quality. However, the GPT-4V annotations\nfail to capture these subtle details. The two examples also demonstrate that GPT-4V may have\ninconsistent criteria for determining helpfulness, as reflected by how visual faithfulness contributes\nto the helpfulness scores of rejected answers.\n16\nPreprint\nQuestion Is there an analog clock with a white frame on the wall?\nChosen Answer There is no existence of an analog clock with a white frame in the image\ndescription.\nGPT-4V Annotations\n1. Helpfulness (Rating: 5): The response directly addresses the user\u2019s inquiry by confirming\nthat an analog clock with a white frame is not present in the image, which is accurate and\nthus fully helpful.\n2. Ethical Considerations (Rating: 5): The response is ethical, does not raise privacy\nconcerns, and contains no potentially harmful content.\n3. Visual Faithfulness (Rating: 5): The text is highly faithful to the visual content by\ncorrectly asserting that there is no clock with a white frame visible in the image.\nRejected Answer Yes, there is an analog clock with a white frame on the wall to the right\nside of the image.\nGPT-4V Annotations\n1. Helpfulness (Rating: 5) The response is straightforward, confirming the presence and\nlocation of an analog clock with a white frame, which is exactly what the user asked for,\nmaking it fully helpful.\n2. Ethical Considerations (Rating: 5) The text is ethical and does not contain any content\nthat would raise safety, privacy, or fairness concerns.\n3. Visual Faithfulness (Rating: 1) The response is not faithful to the image content, as it\nincorrectly confirms the presence of an analog clock with a white frame on the wall, which is\nnot in the image.\nTable 7: An example of GPT-4V annotation for a sample from LRV (Liu et al., 2023a). The\nannotations have been reformatted for improved readability.\n17\nPreprint\nQuestion What are the main objects in this image?\nChosen Answer Upon analyzing the image, I can assert that the main objects are cows. It\nis a black and white picture of two cows in a barn stall with one looking at you. There is\nalso some hay on the ground behind them. The background appears to be wooden bars like\nthose found in livestock pens or barns. In addition there is an object hanging from above\nwhich could possibly be used for feeding the animals. On top of this it seems as though\nthere may be other cattle visible through windows in the back wall but they aren\u2019t clearly\nidentifiable due to their position relative to the camera angle. Overall the scene looks rather\ndull and ordinary - just two cows standing around inside a barn. However, if you take time\nto look closely enough, you will see more details such as the tags attached to each cow \u2019s\nears indicating ownership and perhaps health information about the animal. This would make\nthe photo quite interesting even though it doesn \u2019 t seem so initially. To sum up, the primary\nelements captured here are cows and a barn. We can also see tags on the ear of the cow\nclosest to us. Finally, we have a dark shadow cast over part of the image. All these\nGPT-4V Annotations\n1. Helpfulness (Rating: 4) The response is detailed and describes the image with depth,\nmentioning not only the cows but also the barn setting and additional elements like hay,\nwooden bars, and ear tags. It loses a point for the incorrect assertion that the image is black\nand white and for the unnecessary subjective commentary on the dullness of the scene.\n2. Ethical Considerations (Rating: 5) This response is safe, respectful of privacy, fair,\nand contains no harmful content. It includes details that are all ethical to discuss, such as\nobservable items in a public setting.\n3. Visual Faithfulness (Rating: 3) The response is moderately faithful to the image content,\ncorrectly identifying the cows and the barn setting, but inaccurately describes the image as\nblack and white and includes speculative details not confirmed by the image.\nRejected Answer The main objects in this image are a cow and a calf, both standing in a\nbarn.\nGPT-4V Annotations\n1. Helpfulness (Rating: 1) This response incorrectly identifies a calf in the image, which is\nnot present; therefore, it does not address the user\u2019s prompt effectively.\n2. Ethical Considerations (Rating: 5) Despite the inaccuracy in the content, the response\ndoes not raise any ethical concerns regarding safety, privacy, fairness, or harmfulness.\n3. Visual Faithfulness (Rating: 1) This response is significantly inaccurate as it incorrectly\nidentifies a calf in the image, which is not visible in the provided photograph.\nTable 8: An example of GPT-4V annotation for a sample from SVIT (Zhao et al., 2023a). The\nannotations have been reformatted for improved readability.\n18\n"
  },
  {
    "title": "Cascade Speculative Drafting for Even Faster LLM Inference",
    "link": "https://arxiv.org/pdf/2312.11462.pdf",
    "upvote": "8",
    "text": "Cascade Speculative Drafting for Even Faster LLM Inference\nZiyi Chen 1 Xiaocong Yang 1 Jiacheng Lin 1 Chenkai Sun 1 Kevin Chen-Chuan Chang 1 Jie Huang 1\nAbstract\nIntroduced to enhance the efficiency of large lan-\nguage model (LLM) inference, speculative decod-\ning operates by having a smaller model generate\na draft. A larger target model then reviews this\ndraft to align with its output, and any acceptance\nby the target model results in a reduction of the\nnumber of the target model runs, ultimately im-\nproving efficiency. However, the drafting process\nin speculative decoding includes slow autoregres-\nsive generation and allocates equal time to gen-\nerating tokens, irrespective of their importance.\nThese inefficiencies collectively contribute to the\nsuboptimal performance of speculative decoding.\nTo further improve LLM inference, we introduce\nCascade Speculative Drafting (CS Drafting), a\nspeculative execution algorithm that incorporates\ntwo types of cascades. The Vertical Cascade\neliminates autoregressive generation from neu-\nral models, while the Horizontal Cascade opti-\nmizes time allocation in drafting for improved ef-\nficiency. Combining both cascades, CS Drafting\nachieves up to an 81 percent additional speedup\nover speculative decoding in our experiments,\nwhile maintaining the same output distribution\nas the target model. Our code is publicly avail-\nable at https://github.com/lfsszd/CS-Drafting.\n1. Introduction\nThe advent of Large Language Models (LLMs), like GPT-4\n(OpenAI, 2023), has marked a significant milestone in the\nfield of natural language processing (NLP). These models\nhave not only excelled in various NLP tasks but have also\nfound widespread applications in user-interactive settings,\nsuch as chatbots and virtual assistants. However, these ap-\nplications involve an extremely high number of users, up to\nhundreds of millions daily. To serve in real-time at this scale,\n1Department of Computer Science,\nUniversity of Illi-\nnois\nat\nUrbana-Champaign.\nCorrespondence\nto:\nZiyi\nChen <ziyic2@illinois.edu>, Kevin Chen-Chuan Chang <kc-\nchang@illinois.edu>, Jie Huang <jeffhj@illinois.edu>.\nPreprint.\na low-latency system is not only cost-saving but also crucial\nfor keeping the service running. In addition, the sheer scale\nof the service means that even a slight improvement in the\nlatency of LLMs can greatly contribute to both the service\nprovider and the community. Consequently, optimizing the\nlatency of LLMs has become a critical area of research.\nUnfortunately, the ever-growing size of LLMs significantly\nincreases the latency, especially in long-form generation, as\nautoregressive LLMs generate tokens one by one. An emerg-\ning solution, known as speculative decoding (Leviathan\net al., 2023; Chen et al., 2023; Xia et al., 2023), shows po-\ntential to mitigate this issue. In speculative decoding, a draft\nmodel (which is smaller and faster) generates k tokens in\neach step (with k being a hyperparameter) autoregressively,\nand these tokens are then reviewed by a target model (which\nis larger and slower) in parallel. In one single run, the target\nmodel will accept any tokens aligned with its output and fur-\nther generate one token. The drafting process in speculative\ndecoding enables the target model to generate multiple to-\nkens in a single run while maintaining its output distribution\nunchanged. With a properly sized draft model, speculative\ndecoding achieves a speedup of 2 to 3 times, making it a\npotential method for solving high latency issues.\nHowever, since draft models are typically required to gen-\nerate multiple tokens in multiple steps, where each gener-\nation still involves inefficient autoregressive decoding, the\nperformance of speculative decoding could be limited by\nthe drafting latency. This inefficiency is also indicated by\nLeviathan et al. (2023)\u2019s experiments, where it was observed\nthat very small models (e.g., around two orders of magnitude\nsmaller than the target model) are usually the best choice\nfor drafting because their inference cost is lower compared\nto that of a larger draft model, despite the fact that larger\ndraft models usually have higher-quality generation. This\nunderscores that improving drafting efficiency is crucial for\nfurther enhancing the performance of speculative decoding.\nIn light of this, one key strategy to address this bottleneck\nis to avoid the inefficient autoregressive generation of neu-\nral draft models. Based on this consideration, it is noted\nthat statistical language models, such as bigram language\nmodels, incur negligible latency and computational resource\ncosts compared to neural language models, owing to their\nsimple structure. However, because the tokens generated by\n1\narXiv:2312.11462v4  [cs.LG]  27 Feb 2024\nCascade Speculative Drafting for Even Faster LLM Inference\nFigure 1. The figure shows an example of Cascade Speculative Drafting with target model Mt and draft models Md1, Md2, and Md3.\nThe horizontal cascade involves using larger draft models to generate the earlier tokens and smaller models to generate the later tokens.\nThe vertical cascade requires each model to review drafts from smaller models with the exception of the smallest model which is a\nstatistical language model. As the horizontal cascade and vertical cascade are orthogonal to each other, CS Drafting combines both\napproaches for optimal efficiency.\n0\n5\n10\n15\n20\n25\n30\nToken position\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAcceptance rate\nFLAN-T5-small\nFLAN-T5-base\nFLAN-T5-large\nAcceptance rate of draft tokens on GSM8K dataset\nFigure 2. The probability of acceptance of draft tokens in relation\nto their positions in a single step of speculative decoding, evaluated\non FLAN-T5-SMALL, BASE, and LARGE models on GSM8K. The\ndraft model generates 30 tokens at each step. The results on\nMMLU are shown in Figure 3 of the Appendix.\nstatistical language models usually do not have a high prob-\nability of being accepted by the target model, speculative\ndecoding with statistical language models alone may not\nyield optimal results compared to using a well-sized neural\nlanguage model from the same family as the draft model.\nNonetheless, we notice that it is not necessary to use only\none draft model in speculative decoding\u2014statistical lan-\nguage models can serve as the \u201cdraft\u201d model for the neural\ndraft model, thereby eliminating autoregressive generation\nfrom the neural draft model.\nFurthermore, our analysis in Figure 2 reveals a pattern dur-\ning the drafting step: tokens generated later in the sequence\nby the draft model show a progressively lower probability\nof being accepted by the target model. This is because the\nprobability of a token being accepted is conditioned on the\nacceptance of the previous tokens. It indicates that later\ntokens from draft models are more prone to rejection, con-\ntributing less to the expected number of accepted tokens per\ndraft step, yet incurring the same latency.\nInspired by the above observations, we propose Cascade\nSpeculative Drafting (CS Drafting), a speculative execu-\ntion algorithm that comprises multiple draft models, starting\nwith the smallest being a statistical language model. Each\nneural draft model reviews generations from a smaller model\nand then proposes its reviewed content to either a larger\ndraft model or the target model. In this design, the drafting\nof each neural model will be accelerated by drafting from\na smaller model, avoiding the inefficiency of autoregres-\nsive generation from neural models. We refer to this tiered\nspeculative decoding approach as the Vertical Cascade. In\naddition, we suggest the use of smaller, faster draft models\nfor generating high-rejection tokens that are trailing in draft-\ning generation, forming the Horizontal Cascade. Along\nwith the aforementioned Vertical Cascade, these strategies\ncompose our complete CS Drafting approach, as illustrated\nin Figure 1.\nThrough theoretical analysis and empirical studies, we\ndemonstrate that the CS Drafting algorithm outperforms\nspeculative decoding in terms of latency across various tasks\n2\nCascade Speculative Drafting for Even Faster LLM Inference\nand settings, achieving an additional speedup of up to 81%\nover speculative decoding. These findings highlight the\npractical advantages and efficiency enhancements offered\nby both vertical and horizontal cascades.\nThe main contributions are summarized as follows:\n\u2022 We introduce Cascade Speculative Drafting (CS Drafting),\na speculative-execution-based algorithm that improves\nlanguage model inference speed without sacrificing gen-\neration quality.\n\u2022 We provide theoretical analyses supporting the effective-\nness of the proposed CS Drafting approach.\n\u2022 We conduct empirical experiments showing that CS Draft-\ning achieves further speedup over speculative decoding\nacross different tasks and settings.\n2. Preliminary\nThe core concept of speculative decoding (Leviathan et al.,\n2023) involves the utilization of a small draft model for se-\nquential token generation with validation by a larger target\nmodel resulting in reduced latency. This design acceler-\nates sampling from autoregressive models without altering\noutput distributions. At its heart, there are two key obser-\nvations: 1) certain generations in language modeling are\nsimpler than others and can be predicted by more efficient\nmodels correctly, and 2) using speculative execution along\nwith a new sampling method enables faster, exact decoding\nfrom large models.\nSpecifically, let x be the input tokens at a run and Mt and\nMd are the target and the draft model respectively, k be the\nnumber of draft tokens generated per step, and Mt(x)[i] and\nMd(x)[i] be their probability output at i-th token when in-\nput is x. We interpret speculative sampling as a two-stage op-\neration. In the proposing stage, we sample {xt+1, ..., xt+k}\nfrom draft model Md autoregressively and append them\nto x. In the reviewing stage, let xi \u2208 {xt+1, ..., xt+k}\nrepresents the token at the current position, and we ac-\ncept it if Md(x)[i \u2212 1] \u2264 Mt(x)[i \u2212 1]; in the event that\nMd(x)[i \u2212 1] > Mt(x)[i \u2212 1], we reject xi with a proba-\nbility of 1 \u2212 Mt(x)[i\u22121]\nMd(x)[i\u22121] and proceed to resample xi from\na recalibrated distribution norm(max(0, Mt(x)[i \u2212 1] \u2212\nMd(x)[i \u2212 1])) and reject any token following xi. At the\nend, the target model will generate one additional token\nfollowing the accepted tokens. Such a design guarantees the\noutput is the same as sampling autoregressively using the\ntarget model alone, as proven in Leviathan et al. (2023).\nSpeculative decoding was empirically validated on various\ntasks and model sizes, demonstrating a significant accelera-\ntion in inference times (2x-3x faster) compared to standard\nimplementations, without affecting the outputs. Importantly,\nit does not require task-specific training, altering model ar-\nchitectures, or changing training procedures, making it a\npractical solution for reducing the latency of LLM inference.\n3. Cascade Speculative Drafting\nIn this section, we introduce our proposed method, Cascade\nSpeculative Drafting (CS Drafting), a speculative execution\nalgorithm that incorporates two types of cascades: vertical\ncascade and horizontal cascade.\n3.1. Vertical Cascade\nA notable inefficiency of the speculative decoding algorithm\nis the reliance on the autoregressive generation of a smaller\ndraft model. Since the draft model must run k times for each\ntarget model run, the cost can still be significant despite\nits smaller size. In light of this, we reduce the drafting\ninefficiency by using an even smaller model to assist in\ndrafting and employing the original draft model to review\nthe generation of this smaller model. In addition, since\nthis process can be performed again on the draft model\nthat drafts for the original model, we recursively perform\nthis process until it reaches a statistical draft model that\ninvolves negligent cost, such as a bigram language model.\nIn this approach, we expect each recursion step will reduce\nthe drafting latency without altering the output distribution.\nWe refer to this recursive speculative approach as Vertical\nCascade.\nAdditionally, we incorporate lenience, a hyperparameter that\nloosens the review process by the target model, allowing for\nfaster speed at the trade-off of potentially differing results\nfrom the target model (Leviathan et al., 2023). Lenience\ncan be adopted during sampling or greedy decoding with\nspeculative decoding. Let lenience l \u2208 [1, \u221e). When sam-\npling, the acceptance condition for token xi is transformed\nto Md(x)[i] \u2264 l \u00d7 Mt(x)[i]. If the acceptance condition is\nnot satisfied, with a probability of 1 \u2212 l\u00d7Mt(x)\nMd(x) , we reject\nxi and any following tokens.1 When performing greedy de-\ncoding, the acceptance condition becomes deterministic and\nis simply either argmaxMd(x)[i] = argmaxMt(x)[i] or\nMd(x)[i] \u2264 l \u00d7 Mt(x)[i].\nFor the speculative decoding algorithm, the reduced quality\nintroduced by lenience is generally undesirable. However,\nfor the vertical cascade approach, lenience affects the final\noutput only if it is applied when the target model reviews.\nTherefore, we can limit the application of lenience in the ver-\ntical cascade only when draft models review and do not ap-\nply lenience when the target model reviews. This can ensure\nthe final output is not altered while further reducing latency.\n1For simplicity, we assume the probability outputs are not\nstandardized. We refer the readers to Leviathan et al. (2023) for\nthe discussion on standardized sampling.\n3\nCascade Speculative Drafting for Even Faster LLM Inference\nAlgorithm 1 CascadeSpeculativeDraftingStep\nRequire: draft models {Md1, ..., Mdn}, target mode Mt, prefix, flag isFirstCall, hyperparameters Knn, l\ndraftList \u2190 [Md1, ..., Mdn]\n\u25b7 Initialize curGen and curProb.\ncurGen \u2190 prefix, curProbs \u2190 a list of ones with the same length as prefix\n\u25b7 Unpack the a list of k for the current function call.\n[k1, ..., kn\u22121] \u2190 first row of Knn\n\u25b7 Generate using MaG for the Base case of the recursive call.\nif draftList is empty then\nM \u2190 first element of draftList\nres \u2190 M(curGen)\nreturn res.generation, res.logits\nend if\n\u25b7 Perform the horizontal cascade with the for loop.\nfor i \u2190 1 to n do\n\u25b7 Prepare the arguments for the next recursive call.\ncurTarget \u2190 the i-th item of draftList\ncurDraftList \u2190 the sublist of draftList starting from index i + 1\ncurK \u2190 the submatrix of Knn from with the top-left corner at (i + 1, i + 1) extending to the bottom-right corner\ncurPrefix \u2190 curGen\nwhile curGen.length - curPrefix.length is less than ki do\ncurPrefix \u2190 curGen\n\u25b7 Perform the vertical cascade with the recursive call.\n[x1, .., xu], [p1, p2, ..., pv] \u2190 CascadeSpeculativeDraftingStep(curDraftList, curTarget, curPrefix, False, curK, l)\ncurGen \u2190 [x1, .., xu]\ns \u2190 curProbs.length + 1\nAdd elements of [p1, p2, ..., pv] to curProbs\nend while\nend for\n\u25b7 Set lenience to 1 when the original target model reviews.\nif isFirstCall then\nl \u2190 1\nend if\n\u25b7 Use Mt to review the draft generation.\n[x1, ..., xout], [p\u2032\n1, p\u2032\n2, ..., p\u2032\nout] = review(Mt, curGen, curProbs, l)\nreturn [x1, ..., xout], [p\u2032\n1, p\u2032\n2, ..., p\u2032\nout]\n3.2. Horizontal Cascade\nAnother key observation is that during the drafting steps\nof speculative decoding, not all drafting tokens are created\nequal, as illustrated in Figure 2. The first draft token is\nmore likely to be accepted as it only depends on itself; the\nlast token is rarely accepted, as it has a chance of being\nreviewed only if all preceding tokens are accepted. From a\ntheoretical perspective, assume the event of acceptance of\neach token being a Bernoulli distribution with probably p,\nthe probability of n-th token being accepted is pn, implying\nan exponential decrease of value for tokens generated later\nin the sequence.\nInspired by this observation, we designed Horizontal Cas-\ncade, an approach that improves time allocation by draft\ntoken allocation. Horizontal Cascade assigns the largest\ndraft model to perform the generation of the first draft token\ndue to its highest output alignment with the target model,\nand it progressively uses a smaller as the new draft token to\nbe generated is less likely to be accepted. This process stops\nafter the smallest model, i.e., a statistical language model\nfinishes. This design reduces the time cost of generating\nunimportant draft tokens with a costly draft model, leading\nto a reduction in overall latency.\n3.3. Max-Gram for Better Statistical Drafting\nAs both Vertical Cascade and Horizontal Cascade remark\ncascade toward faster draft models, a statistical language\nmodel, which is the basis of the cascade, becomes essential\nfor the efficiency of both approaches. In our pursuit of a\n4\nCascade Speculative Drafting for Even Faster LLM Inference\nmore effective statistical language model, we noticed a gen-\neral pattern: in language model generation, some words and\nphrases from the input query frequently reappear in the gen-\nerated content. In light of this observation, we designed the\nMax-Gram (MaG) algorithm. It greedily identifies maximal\nmatches between the initial input (or existing generation)\nand tokens from the end of the generation. In cases where\nthere is no match, we resort to a bigram model based on the\nprobability distribution of Wikipedia (chosen to maintain\nthe generality). We include a GPU-friendly version of the\nMax-Gram algorithm in Appendix A.\n3.4. Algorithm\nCombining the horizontal and vertical cascades, the algo-\nrithm of cascade speculative decoding is presented in Al-\ngorithm 1. At its center, the horizontal cascade is realized\nby the for loop, while the vertical cascade is implemented\nthrough recursive calls. Notably, the MaG model is incor-\nporated as the smallest draft model to avoid autoregressive\ngeneration from a neural model. An example of CS Drafting\nis shown in Figure 1.\nThe algorithm requires an upper-triangular hyperparameter,\nKnn, with each row serving as the stop criteria for a layer\nof recursive calls. For simplicity, we assume the lenience l\nis universal for the algorithm, except when the target model\nis under review; thus, the algorithm can benefit from the\nspeedup of lenience without altering the output distribution.\n4. Analysis\nWe begin with some notions. Let Mt be the target model,\nMd be the draft model, and k be the number of draft tokens\ngenerated per step.\nExpected acceptance rate \u03b1(Mt, Md) is the probability\nof draft generation by Md being accepted by target model\nMt.\nCost coefficient c(Mt, Md) is the ratio of time for a single\nrun of draft model Md over target model Mt.\nExpected walltime improvement factor (EWIF) is the\nexpected time improvement achieved by an algorithm under\nthe i.i.d. assumption of token acceptance.\nDespite the simple setting of EWIF, Leviathan et al. (2023)\nhave demonstrated that it aligns with the experimental re-\nsults in most instances. Therefore, our analysis will concen-\ntrate on EWIF.\n4.1. Vertical Cascade\nWe analyze EWIF of vertical cascade using generating func-\ntions, a well-studied topic in combinatorial mathematics\n(West, 2021). The properties of generating functions are\nuseful in the recursion and evaluation process making the\nour final expression simple.\nWe begin with the derivation of the probability generating\nfunction for speculative decoding.\nTheorem 4.1. For speculative decoding between Mt and\nMd, let pi be the probability of generating i tokens. The\nprobability generating function of pi satisfies the following\nequation:\n\u03d5(\u03b1,k)(x) = 1 + (x \u2212 1)1 \u2212 \u03b1k+1xk+1\n(1 \u2212 \u03b1x)\n,\n(1)\nwhere \u03b1 = \u03b1(Mt, Md).\nProof in Appendix C.1.\nCorollary 4.2. The EWIF of speculative decoding is\n\u03d5\u2032\n(\u03b1,k)(1)\n(ck+1) =\n1\u2212\u03b1k+1\n(1\u2212\u03b1)(ck+1).\nWe use the generating function to derive the EWIF of a\nvertical cascade and analyze the case involving two draft\nmodels, Md1 and Md2.\nTheorem 4.3. Assume k to be the speculative decoding\nparameter between Md1 and Md2, and n to be the number\nof steps Mt reviews. The EWIF by this system is\n1 \u2212 \u03b1\u03d5n(\u03b1)\n(1 \u2212 \u03b1)(1 + ncd1 + nkcd2),\n(2)\nwhere \u03d5(x) = \u03d5(\u03b1(Md1,Md2),k)(x), \u03b1 = \u03b1(Mt, Md), and\ncd1, cd2 be c(Mt, Md1), c(Mt, Md2) respectively.\nCorollary 4.4. \u03d5\u03b1\u2032,k(\u03b1) < \u03b1 for any 1 > \u03b1 > 0, 1 > \u03b1\u2032 >\n0, k > 0, so if cd2 << 1, the EWIF of Md1 and Md2 is\nhigher than EWIF of Md1 alone.\nProof in Appendix C.2.\nTherefore, with the statistical model having negligible cost\n(i.e., cd2 << 1), it can almost always improve the efficiency\nof an SD system.\n4.2. Horizontal Cascade\nWe also present an analysis of the walltime improvement\noffered by the horizontal cascade.\nTo assist the analysis, we establish the notions. Let Mt\nbe the target model, {Mi} be the draft models assisting\ngeneration with Mi being the draft model generating the\ni-th token. In the simpler case of the speculative decoding,\nMi = Md for any i. Let x be the input to the model at\na single run, Mt(x) and Mi(x) are then the output prob-\nability distribution with input x. To simplify notation, let\n\u03b1i = \u03b1(Mt(x), Mi(x)) and ci = c(Mt(x), Mi(x)).\nTheorem\n4.5.\nThe\nexpected\nwalltime\nimprove-\nment\nfactor\n(EWIF)\nof\nthe\nhorizontal\ncascade\nis\nT(k, \u03b11, ..., \u03b1k, c1, ..., ck) =\nPk\ni=0\nQi\nj=1 \u03b1j\n1+Pk\ni=1 ci\n.\n5\nCascade Speculative Drafting for Even Faster LLM Inference\nDataset\nMd1\nMd2\nk1\nk2\nEWIF\nCNNDM\nSMALL\n-\n9\n-\n2.65\nCNNDM\nBASE\n-\n8\n-\n2.96\nCNNDM\nBASE\nSMALL\n5\n3\n3.03\nENDE\nSMALL\n-\n12\n-\n3.61\nENDE\nBASE\n-\n11\n-\n3.75\nENDE\nBASE\nSMALL\n5\n8\n3.93\nTable 1. Simulated EWIF under the assumption that the acceptance\ndistribution is a Bernoulli distribution. BASE and SMALL refer to\nFLAN-T5-BASE and FLAN-T5-SMALL. In the simulation, specu-\nlative sampling with horizontal cascade exceeded the performance\nof the vanilla speculative decoding on both CNN Dailymail (Nalla-\npati et al., 2016) and WMT EnDe (Bojar et al., 2018) datasets.\nFurthermore, theorem 4.5 can be used to analyze the impor-\ntance of the tokens in the drafting step.\nCorollary 4.6. The probability of i-th token being ac-\ncepted is Qi\nj=1 \u03b1j.\nThe derivative of EWIF with re-\nspect to \u03b1l is dT (k,\u03b11,...,\u03b1k,c1,...,ck)\nd\u03b1l\n=\nPk\ni=l\nQi\nj=1,j\u0338=l \u03b1j\n1+Pk\ni=1 ci\n.\nSpecifically,\ndT (k,\u03b11,...,\u03b1k,c1,...,ck)\nd\u03b11\n=\nPk\ni=1\nQi\nj=2 \u03b1j\n1+Pk\ni=1 ci\nand\ndT (k,\u03b11,...,\u03b1k,c1,...,ck)\nd\u03b1k\n=\nQk\u22121\nj=1 \u03b1j\n1+Pk\ni=1 ci .\nUsing the information provided by Leviathan et al. (2023),\nwe calculate a simulated EWIF under the assumption that\nthe event of acceptance by the target model is a Bernoulli\ntrial. The results, shown in Table 1, indicate that speculative\nsampling with a horizontal cascade achieved better EWIF\nthan vanilla speculative sampling under this assumption.\n5. Experiments\n5.1. Evaluation Metric\nPrevious works on speculative decoding and related meth-\nods relied on walltime as an evaluation method. However,\nthere are standardization and legitimacy concerns related\nto walltime. Leviathan et al. (2023) mentioned that cost\ncoefficiency, the ratio between the time for a single run of\ndraft model and target model will vary from system to sys-\ntem which can result in significant variation in wall time\nimprovement across different systems, making comparisons\nacross different research work difficult.\nIn addition, a recent analysis suggests GPU speed can vary\nfor the same GPU model with one being 1.5x faster than\nanother GPU of the same model (Sinha et al., 2022); there-\nfore, theoretical walltime improvement up to 1.5x can be\nundermined by GPU variation, and methods without any\nimprovement can be measured with an improvement by\nrunning on a different GPU. The lack of standardization\nand stability makes walltime unfit for measuring algorith-\nmic improvement. To resolve this issue, we design a new\nmetric to measure speculative decoding and its related al-\ngorithms which can reduce the impact of varying/unstable\nGPU performance.\nOur proposed method, standardized walltime improvement\n(SWI), calculates the GPU times of the models, assuming\nthat each run of a language model costs the same amount\nof time, an assumption made when inventing the specu-\nlative sampling algorithm (Chen et al., 2023). Given a\nspeculative decoding system S containing k draft models\nMd1, ..., Mdk, target model Mt solving a task T, and let\ncount(S, m, T) be the count of runs of model m during the\nexecution of task T with system S, let ci be the expected\ncost coefficient being the time ratio between a single run of\nMi and Mt, and let n be the total of tokens generated, the\nstandardized walltime improvement is defined as\nSWI(Md1, ..., Mdk, Mt, T) =\nn\ncount(S, Mt, T) + Pn\ni=1 count(S, Mdi, T)ci\n,\nwhich is simply the sum of GPU time by all models\nwhen ci is measured during a local experiment.\nThe\nbenefit of the SWI evaluation is that count(S, Mt, T)\nand count(S, Mdi, T) will be the consistent across\ndifferent systems, so by using the same ci across different\nexperiments the GPU performance variation will be elim-\ninated; therefore, it provides a standardized, reproducible\nmeasurement.\nDuring our experiment, we will use two sets of {ci}, exist-\ning {ci} reported by Leviathan et al. (2023) and an heuristic\n{ci} being the ratio of number of tunable parameters be-\ntween Mdi and Mt.\n5.2. Setup\nTo ensure the generality of our findings, we perform\nexperiments on both encoder-decoder and decoder-only\nmodels. For encoder-decoder models, we choose our target\nand draft models from the FLAN-T5 (Chung et al., 2022)\nfamily for our experiment, as there is a large variation in\nmodel sizes within the FLAN-T5 family (ranging from 77\nmillion to 11 billion parameters). We use FLAN-T5-XXL as\nour target model, FLAN-T5-BASE and FLAN-T5-SMALL\nas our reviewing draft models. For decoder-only models,\nwe select LLAMA-2-chat-7B (Touvron et al., 2023) as the\ntarget model. Since there is no official release of a smaller\nmodel in LLAMA family, we use a pretrained 160M model\nwith the same tokenizer as the reviewing draft model (Miao\net al., 2024). In both cases, the Max-Gram algorithm is\nused as the generating draft model. To align our experiment\nwith current common usage, we do not perform fine-tuning,\nand the generation is conducted in a zero-shot manner.\nSince we do not observe any significant difference between\nsampling with temperature 1 and greedy decoding in pre-\nvious speculative decoding experiments (Leviathan et al.,\n6\nCascade Speculative Drafting for Even Faster LLM Inference\nDataset\nAlgorithm\n{Mdi}\nSpeedup (MS)\nk11\nk12\nk22\nl\nGSM8K\nAutoregressive\n-\n1\n-\n-\n-\n-\nGSM8K\nS Decoding\nBASE\n3.38\n10\n-\n-\n-\nGSM8K\nS Decoding\nSMALL\n3.06\n11\n-\n-\n-\nGSM8K\nCS Drafting\nBASE, MAG\n3.70\n10\n-\n-\n-\nGSM8K\nCS Drafting\nSMALL, MAG\n3.19\n11\n-\n-\n-\nGSM8K\nCS Drafting\nBASE, SMALL, MAG\n3.88\n8\n13\n1\n3\nMMLU\nAutoregressive\n-\n1\n-\n-\n-\n-\nMMLU\nS Decoding\nBASE\n3.97\n13\n-\n-\n-\nMMLU\nS Decoding\nSMALL\n4.12\n19\n-\n-\n-\nMMLU\nCS Drafting\nBASE, MAG\n4.56\n13\n-\n-\n-\nMMLU\nCS Drafting\nSMALL, MAG\n4.39\n14\n-\n-\n-\nMMLU\nCS Drafting\nBASE, SMALL, MAG\n4.88\n5\n19\n1\n5\nDataset\nAlgorithm\n{Mdi}\nSpeedup (PW)\nk11\nk12\nk22\nl\nGSM8K\nAutoregressive\n-\n1\n-\n-\n-\n-\nGSM8K\nS Decoding\nBASE\n2.99\n8\n-\n-\n-\nGSM8K\nS Decoding\nSMALL\n2.76\n8\n-\n-\n-\nGSM8K\nCS Drafting\nBASE, MAG\n3.27\n9\n-\n-\n-\nGSM8K\nCS Drafting\nSMALL, MAG\n2.82\n11\n-\n-\n-\nGSM8K\nCS Drafting\nBASE, SMALL, MAG\n3.43\n5\n9\n1\n3\nMMLU\nAutoregressive\n-\n1\n-\n-\n-\n-\nMMLU\nS Decoding\nBASE\n3.42\n10\n-\n-\n-\nMMLU\nS Decoding\nSMALL\n3.51\n11\n-\n-\n-\nMMLU\nCS Drafting\nBASE, MAG\n4.21\n6\n-\n-\n-\nMMLU\nCS Drafting\nSMALL, MAG\n3.99\n13\n-\n-\n-\nMMLU\nCS Drafting\nBASE, SMALL, MAG\n4.32\n5\n8\n1\n5\nTable 2. The experimental results on FLAN-T5. Speedup (MS) is the standardized walltime improvement with the assumption that the\nlatency of each run of a model is its number of parameters (model size). Speedup (PW) is the SWI with the assumption that the latency of\neach run of a model is the time cost data reported from previous work (Leviathan et al., 2023). k11, k12, k22, l are the hyperparameters.\nk11 and k12 represent the step limitation target model and the draft models, k22 is the step limitations between the first and second draft\nmodel, and l is lenience as shown in algorithm 1. For speculative decoding, the k11 is simply the k.\n2023), and to ensure our experiments are fully reproducible,\nwe perform sampling at temperature 0, i.e., using greedy\ndecoding by default.\nCascade Speculative Drafting Inference To reduce the\nnumber of hyperparameters to tune, we use MaG to generate\n10 tokens at once, as it is rare for more than 10 tokens to be\naccepted. We do not use lenience when the reviewer is Mt\nto ensure the output distribution does not change. We also\navoid lenience between MaG and its reviewer, since there\nis still a significant performance gap between MaG and a\nneural model.\nWith these constraints, we are left with at most four hyper-\nparameters: k11, k12, k22, and l. For the CS Drafting step\nwhere the target model is the reviewer, k11 and k12 are used.\nk21 and l are used in the step where Md1 is the reviewer.\nBaselines We consider both speculative decoding and\nvanilla autoregressive generation as our baselines. When\nperforming speculative decoding with the target model as\nFLAN-T5-XXL, we opt for FLAN-T5-SMALL and FLAN-\nT5-BASE as the draft models, as they perform better than\nFLAN-T5-LARGE, according to Leviathan et al. (2023) and\nour preliminary experiments. When LLAMA-2-chat-7B is\nthe target model, we use the 160M model as the draft model.\nWe test a wide range of k-values, from 2 to 30, to ensure\nthat we find the best result for each model serving as our\ndraft model to establish our baseline.\nDataset We chose two commonly used datasets for our\nexperiments. For both datasets, we conducted experiments\nin a zero-shot chain-of-thought setup (Kojima et al., 2022;\nWei et al., 2023):\n\u2022 GSM8K (Cobbe et al., 2021) is a dataset comprising\n8,500 high-quality, linguistically diverse, grade-school\nmath word problems. It focuses on multi-step reason-\ning with problems that are typically solvable using basic\n7\nCascade Speculative Drafting for Even Faster LLM Inference\nDataset\nAlgorithm\n{Mdi}\nSpeedup\nk11\nGSM8K\nAuto.\n-\n1\n-\nGSM8K\nS Decoding\n160M\n2.48\n12\nGSM8K\nCS Drafting\n160M, MAG\n2.86\n15\nMMLU\nAuto.\n-\n1\n-\nMMLU\nS Decoding\n160M\n2.12\n10\nMMLU\nCS Drafting\n160M, MAG\n2.64\n13\nTable 3. The experimental results on LLAMA.\narithmetic in 2-8 steps.\n\u2022 MMLU (Hendrycks et al., 2021), or Massive Multitask\nLanguage Understanding, is a benchmark for testing how\nwell large language models grasp knowledge. It encom-\npasses 57 diverse subjects, ranging from elementary sci-\nence to advanced law.\n5.3. Experimental Results\nComparison to Baselines Table 2 presents the experi-\nmental results. In two settings of SWI, Cascade Specula-\ntive Decoding has outperformed the speculative decoding\nalgorithm. For GSM8K, Cascade Speculative Decoding\nachieved a maximum additional speedup of 44% over the\nfastest speculative algorithm; for MMLU, the maximum\nadditional speedup improvement over speculative decoding\nis 81%. Overall, CS Decoding achieves a 3-4x speedup\nover autoregressive generation, establishing itself as a faster\nalgorithm over the baselines.\nEffectiveness of MaG When comparing CS Drafting with\none neural model and MaG against the fastest speculative\ndecoding setup, we found that CS Drafting with one neu-\nral model gained up to a 70% speedup on MMLU and a\n32% speedup on GSM8K. Notably, the MaG algorithm only\ninvolves a bigram model with as many parameters as the\nsize of the tokenizer, making its memory cost negligible.\nNotably, the speedup gained using CS Drafting with one\nneural model involves no additional deployment overhead\nwhile reducing both latency and computational cost, making\nit a superior choice over speculative decoding.\nDraft Model Size Despite FLAN-T5-SMALL mostly outper-\nforming FLAN-T5-BASE as a draft model for speculative\ndecoding, in CS Drafting with the aid of MaG, FLAN-T5-\nBASE consistently outperforms FLAN-T5-SMALL. This\nimplies that with the limitation of a single draft model, the\nideal size of the draft model might increase with the assis-\ntance of the MaG model.\nResults of Decoder-only Models As shown in Table 3, the\nresults for decoder-only models exhibit similar traits to those\nobserved in the experiments on encoder-decoder models.\nTherefore, for speculative decoding-based algorithms, we\nbelieve there is no distinct difference between an encoder-\ndecoder model and a decoder-only model, aligning with the\nresults in Leviathan et al. (2023).\n6. Related Work\n6.1. Efficienct Methods for Language Model Inference\nIn the era of large language models, efficiency during infer-\nence becomes a key to model service. To reduce the model\ninference cost and speed up, several efficient methods have\nbeen proposed, including pruning, knowledge distillation\nand quantization (Treviso et al., 2023). Model pruning\ntakes structured (Xia et al., 2022; Voita et al., 2019) or un-\nstructured (Guo et al., 2021; Chen et al., 2020) methods\nto remove the redundant model parameters to reduce the\nstorage memory and increase inference speed. Knowledge\ndistillation takes the approach of transferring knowledge\nfrom a superior teacher model to a smaller student model\n(Hinton et al., 2015; Gou et al., 2021). Quantization maps\nhigh-precision data representations (e.g. 32 bits) into low-\nprecision ones (e.g. 8 bits) to reduce memory consumption\n(Bhandare et al., 2019; Schaefer et al., 2023).\n6.2. Speculative Decoding\nWith the success of Speculative Decoding (Chen et al.,\n2023; Leviathan et al., 2023) in reducing the large\nlanguage model inference latency, some recent works\nhave attempted to improve Speculative Decoding by\nreducing the rejection rate. Zhou et al. (2023) propose using\ngeneralized knowledge distillation and achieve a lower\nrejection rate compared to other knowledge distillation\nmethods. Avoiding an additional draft model, self-drafting\nis an approach to speculative decoding by reusing part of\nthe target model together with added weight to perform\ndrafting (Zhang et al., 2023; Hooper et al., 2024). Tree\nattention involves generating multiple candidates during\ndrafting to increase the chance of acceptance (Spector &\nRe, 2023; Miao et al., 2024). Besides reducing the rejection\nrate, improving drafting efficiency can also reduce latency.\nSpector & Re (2023) propose using speculative decoding\nfor drafting, showing similarities to the vertical cascade;\nhowever, their method only has two layers of speculative\ndecoding and does not observe the recursive nature of the\nvertical cascade nor the lenience among draft models, two\ncrucial aspects for the performance of vertical cascade.\n7. Conclusion\nIn this work, we propose a novel algorithm, CS Drafting,\nwhich involves two cascades: the vertical cascade and the\nhorizontal cascade. The vertical cascade eliminates the ne-\ncessity of autoregressive generation from a neural language\nmodel, while the horizontal cascade effectively allocates the\n8\nCascade Speculative Drafting for Even Faster LLM Inference\ncost of drafting tokens at different positions. Our experi-\nments show that CS Drafting achieves up to an 81 percent\nadditional speedup over speculative decoding, while main-\ntaining the same output distribution as the target model. Our\nwork demonstrates that LLM inference can be further sped\nup by cascades without sacrificing generation quality or\nrequiring additional task-specific training.\nImpact Statement\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nBhandare, A., Sripathi, V., Karkada, D., Menon, V., Choi, S.,\nDatta, K., and Saletore, V. Efficient 8-bit quantization of\ntransformer neural machine language translation model,\n2019.\nBojar, O., Federmann, C., Fishel, M., Graham, Y., Haddow,\nB., Huck, M., Koehn, P., and Monz, C. Findings of the\n2018 conference on machine translation (WMT18). In\nBojar, O., Chatterjee, R., Federmann, C., Fishel, M., Gra-\nham, Y., Haddow, B., Huck, M., Yepes, A. J., Koehn, P.,\nMonz, C., Negri, M., N\u00b4ev\u00b4eol, A., Neves, M., Post, M.,\nSpecia, L., Turchi, M., and Verspoor, K. (eds.), Proceed-\nings of the Third Conference on Machine Translation:\nShared Task Papers, pp. 272\u2013303, Belgium, Brussels, Oc-\ntober 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/W18-6401.\nChen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre,\nL., and Jumper, J. Accelerating large language model\ndecoding with speculative sampling.\narXiv preprint\narXiv:2302.01318, 2023.\nChen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang,\nZ., and Carbin, M. The lottery ticket hypothesis for pre-\ntrained bert networks, 2020.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,\nFedus, W., Li, Y., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., Hesse, C., and Schulman, J. Training verifiers to solve\nmath word problems, 2021.\nGou, J., Yu, B., Maybank, S. J., and Tao, D. Knowledge\ndistillation: A survey. International Journal of Computer\nVision, 129(6):1789\u20131819, March 2021. ISSN 1573-1405.\ndoi: 10.1007/s11263-021-01453-z.\nGuo, D., Rush, A. M., and Kim, Y. Parameter-efficient\ntransfer learning with diff pruning, 2021.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\nSong, D., and Steinhardt, J. Measuring massive multitask\nlanguage understanding, 2021.\nHinton, G., Vinyals, O., and Dean, J. Distilling the knowl-\nedge in a neural network, 2015.\nHooper, C., Kim, S., Mohammadzadeh, H., Genc, H.,\nKeutzer, K., Gholami, A., and Shao, S. Speed: Spec-\nulative pipelined execution for efficient decoding, 2024.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,\nY. Large language models are zero-shot reasoners. Ad-\nvances in neural information processing systems, 35:\n22199\u201322213, 2022.\nLangley, P. Crafting papers on machine learning. In Langley,\nP. (ed.), Proceedings of the 17th International Conference\non Machine Learning (ICML 2000), pp. 1207\u20131216, Stan-\nford, CA, 2000. Morgan Kaufmann.\nLeviathan, Y., Kalman, M., and Matias, Y. Fast inference\nfrom transformers via speculative decoding. In Inter-\nnational Conference on Machine Learning, pp. 19274\u2013\n19286. PMLR, 2023.\nMiao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z.,\nZhang, Z., Wong, R. Y. Y., Zhu, A., Yang, L., Shi, X.,\nShi, C., Chen, Z., Arfeen, D., Abhyankar, R., and Jia,\nZ. Specinfer: Accelerating generative large language\nmodel serving with tree-based speculative inference and\nverification, 2024.\nNallapati, R., Zhou, B., dos santos, C. N., Gulcehre, C., and\nXiang, B. Abstractive text summarization using sequence-\nto-sequence rnns and beyond, 2016.\nOpenAI. Gpt-4 technical report, 2023.\nSchaefer, C. J., Guo, E., Stanton, C., Zhang, X., Jablin,\nT., Lambert-Shirzad, N., Li, J., Chou, C., Joshi, S., and\nWang, Y. E. Mixed precision post training quantization\nof neural networks with sensitivity guided search, 2023.\nSinha, P., Guliani, A., Jain, R., Tran, B., Sinclair, M. D.,\nand Venkataraman, S. Not all gpus are created equal:\ncharacterizing variability in large-scale, accelerator-rich\nsystems. In SC22: International Conference for High Per-\nformance Computing, Networking, Storage and Analysis,\npp. 01\u201315. IEEE, 2022.\nSpector, B. and Re, C.\nAccelerating llm inference\nwith staged speculative decoding.\narXiv preprint\narXiv:2308.04623, 2023.\n9\nCascade Speculative Drafting for Even Faster LLM Inference\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\nand Scialom, T. Llama 2: Open foundation and fine-tuned\nchat models, 2023.\nTreviso, M., Lee, J.-U., Ji, T., van Aken, B., Cao, Q., Ciosici,\nM. R., Hassid, M., Heafield, K., Hooker, S., Raffel, C.,\nMartins, P. H., Martins, A. F. T., Forde, J. Z., Milder, P.,\nSimpson, E., Slonim, N., Dodge, J., Strubell, E., Balasub-\nramanian, N., Derczynski, L., Gurevych, I., and Schwartz,\nR. Efficient methods for natural language processing: A\nsurvey, 2023.\nVoita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I.\nAnalyzing multi-head self-attention: Specialized heads\ndo the heavy lifting, the rest can be pruned, 2019.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,\nXia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thought\nprompting elicits reasoning in large language models,\n2023.\nWest, D. Combinatorial Mathematics. Cambridge Univer-\nsity Press, 2021. ISBN 9781107058583.\nXia, H., Ge, T., Wang, P., Chen, S.-Q., Wei, F., and Sui,\nZ. Speculative decoding: Exploiting speculative execu-\ntion for accelerating seq2seq generation. In Findings of\nthe Association for Computational Linguistics: EMNLP\n2023, pp. 3909\u20133925, 2023.\nXia, M., Zhong, Z., and Chen, D. Structured pruning learns\ncompact and accurate models, 2022.\nZhang, J., Wang, J., Li, H., Shou, L., Chen, K., Chen, G.,\nand Mehrotra, S. Draft & verify: Lossless large language\nmodel acceleration via self-speculative decoding, 2023.\nZhou, Y., Lyu, K., Rawat, A. S., Menon, A. K., Ros-\ntamizadeh, A., Kumar, S., Kagy, J.-F., and Agarwal, R.\nDistillspec: Improving speculative decoding via knowl-\nedge distillation, 2023.\n10\nCascade Speculative Drafting for Even Faster LLM Inference\nA. Max-Gram Implementation\nListing 1. Max-Gram Algorithm\n1 def torch_index(t, value):\n2\nreturn (t == value).nonzero(as_tuple=True)[0][0]\n3\n4\n5 def max_gram(input_ids, encoder_ids, n=1):\n6\nmatches = (encoder_ids[0] == input_ids[0, -1]).int()\n7\nif matches.sum() < 1:\n8\nreturn None\n9\nfor i in range(2, input_ids.shape[-1] + 1):\n10\nnew_matches = (encoder_ids[0, :(-1 * (i - 1))] == input_ids[0, -1 * i]).int()\n11\ncombined_matches = (2 - new_matches == matches[1:]).int()\n12\nif combined_matches.sum() < 1:\n13\nindex = torch_index(torch.cat(\n14\n(\n15\ntorch.tensor([0] * (i - 1), device=torch.device(encoder_ids.device)),\n16\nmatches\n17\n),\n18\ndim=-1\n19\n), 1)\n20\nreturn encoder_ids[:, index:index + n]\n21\nelse:\n22\nmatches = combined_matches\n23\nindex = torch_index(torch.cat((\n24\ntorch.tensor([0] * (encoder_ids.shape[-1] - matches.shape[-1])), matches), dim=-1\n25\n), 1)\n26\nreturn encoder_ids[:, index+1:index + n+1]\nB. Additonal Results\n0\n5\n10\n15\n20\n25\n30\nToken position\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nAcceptance rate\nAcceptance rate of draft tokens on MMLU dataset\nFLAN-T5-small\nFLAN-T5-base\nFLAN-T5-large\nFigure 3. The probability of acceptance of draft tokens in relation to their positions in a single step of speculative decoding, evaluated on\nFLAN-T5-SMALL, BASE, and LARGE models on MMLU.\n11\nCascade Speculative Drafting for Even Faster LLM Inference\nC. Proof\nC.1. Proof for Theorem 4.1\nProof. The probability of accepting i tokens is \u03b1i \u2212 \u03b1i+1, with the exception of the k + 1-th token, which has a probability\nof \u03b1k of being accepted. This is because it requires all the first i tokens to be accepted and the i + 1-th token to be rejected\nfor this to happen. Therefore,\n\u03d5(\u03b1,k)(x) = \u03b1kxk+1 +\nk\u22121\nX\ni=0\n(\u03b1i \u2212 \u03b1i+1)xi+1.\n(3)\nBy rearranging the terms, we can achieve an expression much easier to work with\n\u03d5(\u03b1,k)(x) = x +\nk\nX\ni=1\n\u03b1i(xi+1 \u2212 xi)\n(4)\n= x + (x \u2212 1)\nk\nX\ni=1\n\u03b1i(xi)\n(5)\n= x + (x \u2212 1)1 \u2212 \u03b1i+1xi+1\n1 \u2212 \u03b1x\n.\n(6)\nC.2. Proof for Theorem 4.3\nProof. Let \u03b1\u2032 = \u03b1(Md1, Md2). We first calculate the expected number of tokens being generated in a step of vertical\ncascade with Md1, Md2. With the property of generating function, the coefficient of term xj of \u03d5n(x) is the probability of\nthe sum of acceptance length of n speculative step being j. Therefore, \u03d5n(x) represents the probability generating function\nright before Mt performs the generation.\nTo achieve the expected number of token acceptances of a probability-generating function, we seek for an operator that can\nmap the probability-generating function into the desired expectation.\nTo achieve the operator, we begin with a single polynomial term of xj. Fortunately, given the end result of 1\u2212\u03b1j+1\n(1\u2212\u03b1) (Leviathan\net al., 2023), the operator T\u03b1(f(x)) = 1\u2212\u03b1f(\u03b1)\n(1\u2212\u03b1)\nwill convert xj to 1\u2212\u03b1j+1\n(1\u2212\u03b1) . In addition, due to the linearity of the operator,\nthis can be extended to any polynomial. Therefore, we achieved the desired operator to map a probability-generating\nfunction into the desired expectation.\nApply operator T\u03b1 to \u03d5n(x), we achieved the result of 1\u2212\u03b1\u03d5n(\u03b1)\n(1\u2212\u03b1)\nfor the expected number of accepted tokens. Furthermore,\nsince the number of Md1 calls is n, and Md2 is called k time for each Md1 call given a total of nk calls of Md2. The time\ncost is 1 + ncd1 + nkcd2 which implied the EWIF of the system being\n1\u2212\u03b1\u03d5n(\u03b1)\n(1\u2212\u03b1)(1+ncd1+nkcd2).\nFor Corollary 4.4, since both 0 < \u03b1 < 1 and 0 < \u03b1\u2032 < 1, we have \u03b1i+1\u03b1\u2032i+1 < \u03b1\u03b1\u2032, meaning that 1\u2212\u03b1k+1\u03b1\u2032k+1\n1\u2212\u03b1\u03b1\u2032\n< 1.\nTogether with \u03b1 \u2212 1 < 0, we have \u03d5\u03b1\u2032,k(\u03b1) < 1 + (\u03b1 \u2212 1) = \u03b1. If we also let nkcd2 = 0, we have\n1\u2212\u03b1\u03d5n(\u03b1)\n(1\u2212\u03b1)(1+ncd1+nkcd2) >\n1\u2212\u03b1\u03b1n\n(1\u2212\u03b1)(1+ncd1), which is the EWIF for speculative decoding with step size n.\n12\n"
  },
  {
    "title": "VidToMe: Video Token Merging for Zero-Shot Video Editing",
    "link": "https://arxiv.org/pdf/2312.10656.pdf",
    "upvote": "8",
    "text": "VidToMe: Video Token Merging for Zero-Shot Video Editing\nXirui Li1\nChao Ma1\nXiaokang Yang1\nMing-Hsuan Yang2\n1Shanghai Jiao Tong University\n2UC Merced\nProject webpage: https://vidtome-diffusion.github.io\nAbstract\nDiffusion models have made significant advances in gen-\nerating high-quality images, but their application to video\ngeneration has remained challenging due to the complexity\nof temporal motion. Zero-shot video editing offers a solu-\ntion by utilizing pre-trained image diffusion models to trans-\nlate source videos into new ones.\nNevertheless, existing\nmethods struggle to maintain strict temporal consistency\nand efficient memory consumption. In this work, we propose\na novel approach to enhance temporal consistency in gener-\nated videos by merging self-attention tokens across frames.\nBy aligning and compressing temporally redundant tokens\nacross frames, our method improves temporal coherence\nand reduces memory consumption in self-attention compu-\ntations. The merging strategy matches and aligns tokens\naccording to the temporal correspondence between frames,\nfacilitating natural temporal consistency in generated video\nframes. To manage the complexity of video processing, we\ndivide videos into chunks and develop intra-chunk local to-\nken merging and inter-chunk global token merging, ensur-\ning both short-term video continuity and long-term content\nconsistency. Our video editing approach seamlessly extends\nthe advancements in image editing to video editing, render-\ning favorable results in temporal consistency over state-of-\nthe-art methods.\n1. Introduction\nDiffusion models [8, 9, 16, 38, 46, 47] have made signifi-\ncant advances in synthesizing media content, allowing for\nthe creation of diverse, high-quality images. However, dif-\nfusion models have yet to achieve high quality in generat-\ning videos. Due to the complexity of temporal motion in\nvideos, training a video diffusion model requires a massive\namount of data and computation resources. To avoid learn-\ning temporal motion from scratch, zero-shot video edit-\ning leverages a pre-trained image diffusion model to trans-\nlate a source video into a new one, retaining motion from\nthe source video. Separately editing each frame likely re-\nsults in inconsistent frames (Fig 1 Per-frame Editing). Ex-\nInput Video\nPer-frame Editing\nPix2Video\nVidToMe (Ours)\nTo \u201cRainbow-colored Origami Flamingo\u201d\nFigure 1. Given an input source video and a text prompt, we lever-\nage a pre-trained image diffusion model [38] to edit the video.\nThe state-of-the-art zero-shot video editing approaches, e.g.,\nPix2Video [6], struggle to generate temporal consistent frames\nwith self-attention extension. Our proposed method merges to-\nkens across frames, rendering higher temporal consistency.\nisting video editing methods [6, 34, 51, 52, 54] typically\nextend the self-attention modules of diffusion models to\nprocess multiple frames jointly instead of separately. De-\nspite the promise, two issues ensue with such approaches.\nFirst, though cross-frame attention encourages a roughly\nconsistent appearance, the generated frames lack strict con-\nsistency in details.\nAs human perception is sensitive to\nvideo continuity, tiny changes or jittering between frames\ncan significantly degrade the quality of generated videos.\nSecond, including multi-frame tokens in self-attention in-\ncreases memory consumption quadratically.\nComputing\nself-attention on four frames requires 16 times larger GPU\nmemory than on one frame. With these limitations, state-of-\nthe-art video editing methods such as Pix2Video [6] strug-\ngle to generate temporal consistent videos, as shown in\nFig. 1. Thus, it is imperative to develop effective and ef-\nficient diffusion-based zero-shot video editing methods.\nIn this work, we present a novel method, VidToMe, to\nenhance the temporal consistency of generated videos by\nmerging the tokens in diffusion models across video frames.\nOur motivation comes from the recent developments [4, 5]\nin compressing transformer tokens to improve computa-\ntional efficiency.\nFor video editing, we observe that the\n1\narXiv:2312.10656v2  [cs.CV]  19 Dec 2023\nTo \u201cPop Art Style\u201d\n\ud835\udc61\ud835\udc61 = 400\n\ud835\udc61\ud835\udc61 = 0\nw.o. VidToMe\nw. VidToMe\nFigure 2. Comparison of frames edited to \u201dPop Art Style\u201d by\nPnP [48] with or without VidToMe. Left: Edit results. Right: Vi-\nsualized token matching between two frames as flow maps. Color\nrepresents the direction of the matched token in another frame.\nWe label the denoising timestamp above (1000 \u2192 0). Our method\naligns correspondent tokens and fixes the inconsistencies (window,\nclothes) in per-frame editing.\ntransformer tokens of video frames are much more corre-\nlated in the temporal domain than in the spatial domain. We\ncan align tokens across frames according to the correlation\nand compress the temporally redundant tokens to facilitate\njoint self-attention. We show that when multiple frames\nshare the same set of tokens in the self-attention module,\nthe diffusion model naturally generates temporally consis-\ntent video frames. Hence, we propose merging tokens over\ntime to compress and unify the internal diffusion feature\nembeddings, achieving temporal consistency in generated\nvideos and reducing memory consumption in computing\nself-attention across frames.\nOur method involves merging tokens in one frame with\nthe most similar ones in another frame. As shown in Fig. 2,\nthis merging strategy allows us to match and align tokens\naccording to the temporal correspondence between frames.\nThus, applying VidToMe fixes the misalignment of details\nin per-frame editing. As processing all frames at once is\nnot feasible, we divide the video into chunks and perform\nintra-chunk local token merging and inter-chunk global to-\nken merging, ensuring both short-term and long-term con-\nsistency. Short-term consistency improves video continu-\nity, while long-term consistency prevents the video con-\ntent from drifting over time. Note that our video editing\nmethod can be seamlessly integrated with existing control-\nling schemes [48, 56], taking full advantage of the advance-\nments in image editing for video editing. Extensive exper-\niments show that the proposed video editing method per-\nforms well regarding temporal consistency and text align-\nment over the state-of-the-art approaches. The main contri-\nbutions of this work are three-fold:\n\u2022 We propose a novel diffusion-based approach, Vid-\nToMe, to merge self-attention tokens across frames\nwhen generating video frames, improving temporal\nconsistency and computational efficiency.\n\u2022 We design a video editing pipeline that jointly gener-\nates all video frames with short-term local token merg-\ning and long-term global token merging to enforce fea-\nture alignment throughout the video.\n\u2022 We comprehensively evaluate our method to show the\nstate-of-the-art video editing performance.\n2. Related Work\nAs several thorough surveys on image and video genera-\ntion [1, 7, 8, 25] exist in the literature, here we discuss dif-\nfusion models and related image and video editing schemes.\nDiffusion-based Image and Video Synthesis. Diffusion\nModels (DM) [16, 45, 47] have recently achieved state-\nof-the-art performance in numerous tasks, including im-\nage generation [8, 9, 30, 38, 46]. DMs learn to reverse a\nforward diffusion process and generate an image by grad-\nually denoising it from pure noise. Notable examples of\nimproving DMs include [15, 20, 43] and numerous appli-\ncations [2, 11, 21, 24, 26\u201328, 40]. Benefiting from large-\nscale pretraining [35, 44], text-to-image DMs have shown\nimpressive results in generating high-quality and diverse\nimages [31, 36, 38, 42].\nNaturally, DMs have been ap-\nplied to video synthesis, typically by incorporating tempo-\nral layers into image DMs [3, 17, 18]. Despite the demon-\nstrated success in unconditional video generation [18, 55],\ntext-to-video DMs are not as satisfying as image ones. Due\nto the complexity of temporal motion, training video DMs\nrequires intensive computation resources and large-scale\nannotated video datasets, which significantly hinders the\nprogress of this field.\nDiffusion-based Image Editing. In addition to text, some\nworks have introduced additional control signals for image\nediting or controllable image generation [41, 48, 56]. Some\nschemes introduce adapter layers [29] or other trainable\nmodules [50, 56] to accept additional control signals. Con-\ntrolNet [56] supports various conditions such as edge maps,\ndepth maps, and key points by finetuning an attached copy\nof DM. Other methods edit a source image by manipulat-\ning intermediate diffusion features [14, 48] or optimization-\nbased guidance [10, 32]. Plug-and-Play [48] maintains im-\nage structure by injecting self-attention maps and internal\nfeatures from the source image.\nSelf-guidance [10] and\npix2pix-zero [32] edit the image by imposing a guidance\nloss optimized during generation.\nStableDiffusion2 [38]\npresents a depth-conditioned model that directly includes\nthe depth map in its input. In this paper, we perform video\nediting by applying these image editing methods to video\nframes while preserving temporal coherence via merging\nvideo tokens.\nDiffusion-based Video Editing. With the recent success\nof text-to-image DMs in powering text-driven image edit-\ning [14, 27, 48], many works apply a pre-trained text-to-\n2\nimage DM [38] for text-driven video editing. The critical\nproblem is how to keep temporal coherency in generation.\nTune-A-Video [52] inflates the DM with temporal attention\nlayers and finetunes on the source video. vid2vid-zero [51]\nmaintains the video structure by injecting cross-attention\nmaps from the source video.\nIn [6], Pix2Video guides\nthe generation with a reference frame by self-attention fea-\ntures injection and latent update. Rerender-A-Video [54]\nfuses the previous frame warped by the source video opti-\ncal flow and applies multi-stage latent operations. On the\nother hand, Fate/Zero [34] uses a dedicated attention blend-\ning block to inject attention maps from the source video.\nTokenFlow [13] shares a similar idea with our method to\nenforce temporal consistency by unifying self-attention to-\nkens. It computes the inter-frame correspondences by ex-\ntracting tokens from the source video. Then the tokens are\npropagated between the jointly-edited keyframes according\nto the correspondances. Note that these methods commonly\nextend the self-attention modules into the spatial-temporal\ndomain to encourage consistent appearance across frames.\nHowever, extending self-attention does not enforce tempo-\nral consistency well and increases memory overhead. Our\nmethod simultaneously addresses these two problems by\nmerging similar tokens across video frames.\n3. Preliminaries\nLatent Diffusion Model. Diffusion models [16, 45, 47] are\na class of generative models based on an iterative denoising\nprocess. An image DM supposes a forward process where\na clean image x0 is corrupted by Gaussian noise \u03f5,\nxt = \u221a\u03b1tx0 +\n\u221a\n1 \u2212 \u03b1t\u03f5,\n(1)\nwhere t = 1, . . . , T is the current timestep and {\u03b1t} are\nthe monotonically decreasing noise schedule. Then, start-\ning from random Gaussian noise, DM reverses the forward\nprocess to generate an image by estimating the noise direc-\ntion and progressively denoising it.\nRecent large-scale diffusion models [36, 38, 42] operate\nin the latent space to improve performance and efficiency.\nThese latent diffusion models train an autoencoder [23] to\nmap the image between pixel and latent space. Let E(\u00b7)\nand D(\u00b7) be the encoder and the decoder, where E(x) =\nz, D(z) \u2248 x. Both the training and inference are conducted\nin the latent space. Typically, a UNet [39] \u03f5\u03b8 is trained to\nestimate the noise with the objective\nmin\n\u03b8\nEz,\u03f5\u223cN (0,I),t\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, c)\u2225,\n(2)\nwhere c is the text embedding in text-to-image DMs. In this\nwork, we base our experiments on Stable Diffusion [38], a\nlarge-scale text-to-image latent diffusion model.\nToken Merging. Token Merging (ToMe) [5] is a method\nto increase the throughput of existing ViT models by grad-\nually merging redundant tokens in the transformer blocks.\nIt combines similar tokens to reduce the redundancy as well\nas the number of tokens, speeding up the computation. Our\nmethod leverages the lightweight bipartite soft matching al-\ngorithm of ToMe to merge tokens across video frames.\nGiven input tokens T, the algorithm first partitions the\ntokens into a source (src) and destination (dst) set and\ncomputes the pair-wise cosine similarity between the two\nsets. Then, src tokens are linked to their most similar token\nin dst. Next, r most similar edges are selected, and con-\nnected tokens are merged. Finally, all the tokens are con-\ncatenated as the output. We use the dst token as the merged\ntoken value instead of averaging the value of merged to-\nkens, which produces better results in practice. Our method\ndivides merged tokens after self-attention to keep the token\nnumber unchanged. Like [4], we divide a token simply by\nassigning its value to all restored tokens, which means a\ntoken merged from two tokens will be separated into two\nidentical tokens. We define the token merging and unmerg-\ning operations, M and U:\nE = Match(src, dst, r),\nTm = M(T, E), Tu = U(Tm, E).\n(3)\nMatch(\u00b7) outputs the matching map E with r edges from\nsrc to dst. M(\u00b7) and U(\u00b7) merge and unmerge tokens ac-\ncording to matching E.\n4. Proposed Method\nOur objective is to generate an edited video that matches\na given editing prompt while preserving the motion and\nstructure of a source video. To achieve this, we use a pre-\ntrained text-to-image diffusion model to generate individ-\nual frames. We apply DDIM inversion [46] and existing\ncontrolling methods [38, 48, 56] for image editing to pre-\nserve the source frame structure.\nHowever, more effort\nis required to achieve temporal consistency across video\nframes. We observe that transformer tokens are correlated\nacross frames as the temporal correspondence in videos.\nThus, we compress multi-frame tokens by merging simi-\nlar tokens together so that the self-attention module extracts\nconsistent features for each frame. The unified internal fea-\ntures promote the diffusion model to generate consistent\nvideo frames. Fig. 3 presents an overview of the proposed\nmethod.\nThe proposed video token merging strategy, VidToMe,\nis detailed at the bottom of Fig. 3.\nWe first merge to-\nkens across frames in one video chunk to enforce short-term\nvideo continuity. The locally merged tokens are combined\nwith a set of global tokens from previous chunks, enabling\nlong-term token sharing. Joint self-attention extracts con-\nsistent features on merged tokens, which are then propa-\ngated to each frame by token unmerging. Our video to-\nken merging algorithm has two advantages. First, merged\n3\nDDIM\n Inversion\nDenoising \u00d7 \ud835\udc47\ud835\udc47 Steps\nVideo \nChunks\n\u2026\u2026\nOff-the-shelf Diffusion Model\nResidual\nBlock\nSelf \nAttention\nCross \nAttention\nToken Merging\nLocal Token Merging\nGlobal Token Merging\nglobal tokens\nupdate\n4 \u00d7 \ud835\udc41\ud835\udc41 \u00d7 \ud835\udc36\ud835\udc36\n\ud835\udc53\ud835\udc530\n\ud835\udc53\ud835\udc532\n\ud835\udc53\ud835\udc533\n\ud835\udc53\ud835\udc531\nInput Video\n\u2026\n\u2026\nEdited Video\nTo \u201cVan Gogh Style\u201d\nToken Unmerging\nToken Merging\n\u2026\n\ud835\udc53\ud835\udc530\n\ud835\udc53\ud835\udc532\n\ud835\udc53\ud835\udc533\n\ud835\udc53\ud835\udc531\n4 \u00d7 \ud835\udc41\ud835\udc41 \u00d7 \ud835\udc36\ud835\udc36\nToken Unmerging\nSelf \nAttention\n1 \u00d7 \ud835\udc40\ud835\udc40 \u00d7 \ud835\udc36\ud835\udc36\nUnmerging\nMerging\n\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60 to \ud835\udc51\ud835\udc51\ud835\udc60\ud835\udc60\ud835\udc51\ud835\udc51 matching \nFigure 3. Pipeline of our proposed video editing method, VidToMe. We apply DDIM inversion on the source video frames to obtain\nthe initial noisy latents. We denoise frame latents with an off-the-shelf text-to-image diffusion model, combining an existing controlling\nmethod [48, 56]. In each iteration, frames are split into chunks and denoised by the diffusion model, where we attach our lightweight token\nmerging and unmerging operation around the self-attention modules. We first merge tokens locally into a random frame in the chunk.\nThen, the merged tokens are combined with the global tokens maintained across chunks in one iteration. After self-attention, we unmerge\nthe tokens to the original size for the following operations.\ntokens are shared across frames, enforcing temporal con-\nsistency. Second, token merging compresses redundant to-\nkens in the self-attention module, improving computation\nefficiency. As a lightweight parameter-free operation, Vid-\nToMe can seamlessly integrate with existing image editing\nmethods [38, 48, 56]. We first provide an overview of our\nvideo editing pipeline in Section 4.1 and then elaborate on\nour video token merging method in Section 4.2.\n4.1. Video Editing Pipeline\nGiven the source video V with n frames (f1, f2, . . . , fn),\nwe first invert each frame to noise by DDIM inversion.\nDDIM inversion applies the deterministic DDIM sam-\npler [46] in the reversed order t = 0 \u2192 T, turning a clean\nlatent z0 into a noisy latent zT . Using the inverted frames\nas the initial noise, we iteratively denoise them by an off-\nthe-shelf text-to-image diffusion model with edit prompt P\nas the text condition.\nUnlike existing methods [6, 54] that generate each\nframe separately,\nwe process all frames together in\neach denoising iteration.\nAt iteration t, we randomly\nsplit frames (z1\nt , z2\nt , . . . , zn\nt ) into a sequence of chunks\n(C1, C2, . . . , Cm).\nEach chunk contains B consecutive\nframes except for the first and last chunks. The initial chunk\nC1 = (z1\nt , . . . , zb\nt) where b is a random integer in [1, B].\nThe randomness here ensures the probability of frames split\ninto a chunk proportional to the time interval, avoiding the\n\u201cchunks\u201d seen in the generated video.\nWe process each\nchunk of frames by the diffusion model in a random order.\nIn the diffusion model, frames are concatenated in the\nbatch dimension, which means the model treats all frames\nas separate images. To enforce temporal consistency, we\nmerge tokens across frames in the self-attention module of\nthe diffusion model. A diffusion model is typically realized\nas a UNet [39], composed of a series of downsample and\nupsample blocks. A layer in each block consists of a resid-\nual block, a self-attention module [49], and a cross-attention\nmodule. Among them, the self-attention module has been\nshown to be highly correlated to the structure and appear-\nance of the image [48]. Thus, we merge multi-frame tokens\nbefore self-attention so that the self-attention jointly pro-\ncesses the merged tokens and outputs consistent features.\nNote that our method only changes the input tokens of the\nself-attention without any modification to the self-attention\noperation. To perform the following processes, we restore\nthe output tokens to the original size by token unmerging.\nAs merging tokens in deep blocks may degrade the gen-\neration quality, we perform token merging in the first two\ndownsample blocks and the last two upsample blocks.\nAfter T iterations of denoising, we obtain the edited\nframes latents (z1\n0, z2\n0, . . . , zn\n0 ) and the edited video V\u2217 af-\nter VAE decoding. Our method works with an existing con-\ntrolling method for image editing to preserve the structure\nof source frames, such as ControlNet [56], Plug-and-Play\n(PnP) [48], and depth-conditioned diffusion model [38].\n4\n4.2. Video Token Merging\nThis section presents our video token merging strategy,\nwhich focuses on the self-attention module in the diffusion\nmodel. A self-attention module takes a sequence of input to-\nkens and outputs the same number of tokens. The input and\noutput tokens are denoted as Tin and Tout, both belonging\nto the space RB\u00d7N\u00d7C, where B is the number of frames,\nN is the number of tokens per frame, and C is the feature\ndimension. We enforce temporal coherence by merging in-\nput tokens across B frames in one video chunk (local token\nmerging) and merging global tokens from previous chunks\nof frames (global token merging). After self-attention, we\nunmerge the output tokens to their original size. These oper-\nations are performed on the input and output tokens without\nmodifying the self-attention module.\nLocal Token Merging. Given a set of input tokens denoted\nby Tin = {T f\nin}B\u22121\nf=0 , we randomly select one out of B\nframes as the current target frame, e.g., the kth frame. We\nthen apply the bipartite soft matching algorithm mentioned\nin Section 3 and Equation 3 to merge the other frames to the\ntarget frame:\nTlm = M(Tin, Match(T src\nin , T dst\nin , r)),\nwhere T src\nin\n= {T f\nin}B\u22121\nf=0,f\u0338=k and T dst\nin\n= T k\nin. We set r =\np(B\u22121)N where (B\u22121)N is the src token number and p is\nthe merging ratio. A large merging ratio (e.g., p = 0.9) can\nbe used as video frames are highly redundant. Local token\nmerging enforces consistency in a small frame chunk.\nHowever, for long-term consistency, we need more than\nshort-term video continuity. For example, a video\u2019s first and\nlast frames will never be processed in one chunk, leading to\nappearance drifting along the video. Thus, we need another\nway to share tokens across the whole video. Enlarging the\nchunk size or implementing a hierarchical merging is help-\nful but requires an even larger memory capacity. Instead, we\npropose a simple yet effective global token merging strategy\nfor long-term consistency.\nGlobal Token Merging. At each iteration, we maintain a\nset of global tokens denoted as Tg that spans across video\nchunks. The initial global tokens are set to be the locally\nmerged tokens of the first chunk, i.e., T 1\ng = Tlm. For the kth\nframe chunk, we merge its locally merged tokens Tlm with\nthe previous global tokens T k\u22121\ng\nas the following operation:\nTgm = M({Tlm, T k\u22121\ng\n}, Match(Tlm, T k\u22121\ng\n, r)),\n(4)\nwhere Tgm represents the final input to the self-attention\nmodule. In practice, we randomly assign src and dst to lo-\ncal and global tokens. We can update the global tokens to\ninclude the tokens from the current frames in several ways.\nOne possible option is to use merged tokens Tgm as new\nglobal tokens. However, this approach is not feasible for\narbitrarily long videos as it always increases the number of\nglobal tokens. Instead, we unmerge Tgm back to local to-\nkens T u\nlm and global tokens and set the current global tokens\nto be the unmerged local tokens, i.e., T k\ng = T u\nlm.\nSelf-Attention Analysis. We analyze the self-attention op-\neration on merged tokens in more detail. The input, de-\nnoted as Tgm, comprises M tokens. We can infer M =\n(0.11B + 0.99)N = 1.43N, assuming chunk size B = 4\nand merging ratio p = 0.9 for local and global merg-\ning. The tokens are mapped to Q, K, V matrices during\nself-attention. For multiplication between Q and K, the\noriginal input of size 4 \u00d7 N \u00d7 C has a space complex-\nity of O(4N 2). In contrast, the complexity is reduced to\nhalf with the merged tokens Tgm \u2208 R1\u00d7M\u00d7C as input,\nO(M 2) \u2248 O(2N 2). Essentially, our token merging method\ncombines multiple frames into one, reducing redundancy\namong frames. Self-attention then identifies consistent fea-\ntures in this unified frame.\nToken Unmerging.\nThe output tokens To of the self-\nattention module need to be restored to their original shape\nas separate frames to perform the following image opera-\ntions. As such, we first unmerge the tokens into local and\nglobal tokens and then unmerge the local tokens into B sep-\narate frames, reversing the merging process. Denoting re-\nspective matching maps for local and global token merg-\ning as El and Eg, we formulate the token unmerging as\nU(To, Eg) = (Tlocal, Tglobal) to divide the local tokens\nand U(Tlocal, El) = Tout to obtain the final output. The\nunmerged tokens in the output are identical to the original\nmerged tokens, ensuring consistency across frames.\n5. Experimental Results\n5.1. Experiment Setting\nOur method performs video editing with a pretrained text-\nto-image model and an existing image-controlling method.\nIn this work, we use Stable Diffusion (SD) [38] (version\n1.5) as the image generator, and DDIM scheduler [46] with\nsampling step T = 50 for inversion and sampling. For the\ncontrolling method, we combine our method with Plug-and-\nPlay (PnP) [48], ControlNet [56], and SD2-Depth. The pa-\nrameters are chunk length B = 4 and merging ratio p = 0.9\nand 0.8 for local and global merging.\nMore results and\nvideos are available in the supplementary material. All the\nsource code and datasets will be released.\nDataset. Similar to prior works [6, 34, 53], we select 20\nvideos from DAVIS [33] as source videos for performance\nevaluation. These videos include a range of motion from\nslow to fast and feature various subjects such as humans, ve-\nhicles, and animals. We edit each of the 20 videos with three\ntypes of prompts: (i) Style prompts edit the global style. (ii)\nObject prompts edit the object\u2019s appearance and attributes.\n(iii) Background prompts change the video background. To\nobtain edit prompts, we use some prompts from [53] and\n5\nStyle: To \u201cComic Book, Black and White Pencil Sketch\u201d; Control: PnP\nObject: To \u201cSpiderman\u201d; Control: ControlNet-Softedge\nObject: To \u201cRainbow-colored Toy Bear\u201d; Control: SD2-Depth\nBackground: To \u201cVolcano, Spewing Lava\u201d; Control: ControlNet-Depth\nFigure 4. Sample editing results of our method. Our method can seamlessly integrate with existing controlling methods [38, 48, 56] to\ntemporal consistently edit videos in various aspects. We label each sample with the edit prompt and the applied controlling method.\ngenerate the others using GPT-3.5 [37]. We generate 60\nedited videos for evaluation, each containing 32 frames with\na resolution of 512 \u00d7 512.\nMetrics.\nWe note that existing video editing methods\nuse different metrics to evaluate the editing performance.\nIn our experiment, we incorporate metrics used by prior\nworks [6, 34] and new measures. We assess the editing\nperformance based on three key criteria: (i) Temporal Con-\nsistency: This includes Interpolation Error and PSNR [19],\nWarp Error, and Frame CLIP Score. (ii) Text Alignment:\nThis includes Directional CLIP Score [12] and Text CLIP\nScore.\n(iii) User Study.\nBased on the Interpolation Er-\nror and PSNR used to evaluate video interpolation perfor-\nmance in previous studies [19], we propose to measure the\nvideo continuity by interpolating a target frame by its previ-\nous and next frames and computing the root-mean-squared\n(RMS) difference and PSNR between the estimated and tar-\nget frames. The metric better reflects the generated video\ncontinuity itself without relying on the source video optical\nflow. We use the Directional CLIP Score [12] to evaluate\nthe consistency between image and prompt change. It com-\nputes the cosine similarity in CLIP space between the differ-\nence in frames and the difference in prompts from source to\nedit. For more details about the metrics, please refer to the\nsupplementary material. We conduct a user study with 10\nout of 60 edited videos. Users choose their preferred video\nfrom the results edited by both baselines and our method.\nThe user preference rate is used as the final metric.\nBaselines.\nWe evaluate our method against four state-\nof-the-art video editing techniques: Text2Video-Zero [22],\nTune-A-Video [52], vid2vid-zero [51], and Pix2Video [6].\nAll the methods are implemented using default settings ex-\ncept for vid2vid-zero.\nWe have to turn off its Spatial-\nTemporal attention that includes all frames in the self-\nattention, which is infeasible to fit the GPU memory (40GB)\nwhen processing 32-frame videos. Text2Video-Zero allows\nzero-shot text-to-video generation, and we apply it with\ndepth control to perform video editing. Tune-A-Video fine-\ntunes the model on the source video frames before sam-\npling the edited video. We use StableDiffusion v1.5 for\nthe first three methods and StableDiffusion2-Depth [38] for\nPix2Video as it requires a depth-conditioned model by de-\nfault. It is worth noting that all the baseline methods use\nsome self-attention extension that includes multiple frames.\n5.2. Main Results\nQualitative Evaluation. Fig. 5 compares our editing re-\nsults with baseline methods on evaluation videos. While\nText2Video-Zero [22] produces high-quality frames, it\nlacks continuity between them.\nThe edited frames by\nText2Video-Zero do not align with the source frame in ap-\npearance due to the random initial noise it used. Tune-A-\nVideo [52] struggles to learn the motion of the source video\nand fails when the motion is complex. The edited frames\n6\nInput Video\nT2V-Zero\nTune-A-Video\nvid2vid-zero\nPix2Video\nOurs+PnP\nOurs+SD2-Depth\nTo \u201cShark\u201d\nTo \u201cDesert\u201d\nFigure 5. Qualitative comparison of our method and baselines. The editing results of our method are consistent over time in global style\nand local texture and preserve the source frame structure well.\nTemporal Consistency\nText Alignment\nUser Study\nInp. Err.\u2193\nInp. PSNR\u2191\nWarp Err.\u2193\nFrame C.s.\u2191\nDirectional C.s.\u2191\nText C.s.\u2191\nPreference Rate\u2191\nText2Video-Zero [22]\n0.203\n19.4\n0.094\n0.969\n0.139\n0.282\n0.167\nTune-A-Video [52]\n0.235\n18.1\n0.068\n0.957\n0.132\n0.273\n0.089\nvid2vid-zero [51]\n0.219\n18.6\n0.049\n0.957\n0.118\n0.270\n0.030\nPix2Video* [6]\n0.139\n22.5\n0.020\n0.968\n0.166\n0.285\n0.170\nOurs + ControlNet-Depth [56]\n0.154\n22.1\n0.026\n0.973\n0.159\n0.284\n0.544\nOurs + PnP [48]\n0.111\n25.0\n0.013\n0.975\n0.140\n0.271\nOurs + SD2-Depth [38]*\n0.105\n25.6\n0.012\n0.971\n0.168\n0.282\n-\nTable 1. Quantitative evaluation results. Red and blue indicates the best and second-best result. *: Use the same base model as Pix2Video,\nSD2-Depth [38]. Others use SDv1.5. C.s.: CLIP Score. Inp. Err.: Interpolation Error.\nby Tune-A-Video contain wave-like jittering, degrading the\nvideo quality. vid2vid-zero [51] generates unstable video\nand fails to preserve the frame structure.\nPix2Video [6]\nachieves good consistency between edited frames but gen-\nerates unnatural jittering and blurring results. In contrast,\nVidToMe generates consistent frames that adhere to the edit\nprompt while preserving the source frame structure. Fig. 4\nshowcases more sample editing results. Our method can in-\ntegrate seamlessly with existing controlling methods, pro-\nviding users with more control over the editing process and\nenabling video editing in various aspects.\nQuantitative Evaluation. We present quantitative evalu-\nation results in Table 1. The first three baselines do not\nperform well in terms of temporal consistency. Text2Video-\nZero [22] uses random noise instead of noise inverted from\nsource frames, which results in frames that are different\nfrom the source frames, as shown in the qualitative study.\nThough some subjects appreciate the diversity in its re-\nsults, its continuity is unsatisfactory. Tune-A-Video [52]\nand vid2vid-zero [51] are not preferred by subjects due to\ntemporal inconsistency. Pix2Video [6] achieves higher tem-\nporal consistency than the other baselines. It uses SD2 [38],\nwhich has better generation fidelity than SDv1.5 and thus\ngets a high directional and text CLIP Score. However, its\nresults still suffer from jittering and lack of long-term con-\nsistency. Our proposed VidToMe achieves better temporal\nconsistency and text alignment than the first three baselines.\nIt also outperforms Pix2Video regarding temporal consis-\ntency when using the same base model, SD2-Depth. Fur-\nthermore, Our editing results are preferred by over half of\n7\n18.0\n18.5\nTune-A-Video\n10\n15\n20\n25\n30\n35\nMax Memory Allocated (GB)\n2.0\n4.0\n6.0\n8.0\nPer-Video Edit Time (min)\nText2Video-Zero\nvid2vid-zero\nPix2Video\nOurs+PnP\nOurs (SD2-Depth)\nFigure 6. Editing efficiency comparison. We consider both time\n(time to perform one editing) and space (max GPU memory allo-\ncated in editing) efficiency. Test on one NVIDIA Tesla A100.\nInp. Err. \u2193\nDirectional C.s. \u2191\nPer-frame S.A.\n0.253\n0.165\nExtended S.A.\n0.140\n0.168\nVidToMe S.A. (Ours)\n0.105\n0.168\nTable 2. Performance comparison of different multi-frame self-\nattention (S.A.) operations.\nthe subjects. Our method offers flexibility in balancing con-\nsistency and alignment by using different control methods\nsuch as ControlNet-Depth [56] or PnP [48].\n5.3. Ablation Studies\nEfficiency Analysis.\nIn Fig. 6, we compare the editing\nefficiency of different methods with their default setting.\nMost other methods require either high memory capacity\nor a long editing time. Although Text2Video-Zero [22] ed-\nits videos quickly without noise inversion, its performance\nis poor. On the other hand, our proposed method, VidToMe,\nreduces memory consumption with video token merging\nwhile generating videos quickly. With a minimal memory\nconsumption of less than 7 GB, our method can run on some\npersonal devices.\nMulti-frame Self-Attention.\nWe ablate on multi-frame\nself-attention choices in video editing. In per-frame self-\nattention, each frame is processed separately, leading to in-\nconsistencies. On the other hand, extending self-attention\nto include multi-frame tokens is adopted by most existing\nmethods. This method enables cross-frame attention and\nproduces better consistency. Our approach merges tokens\nto enforce temporal consistency, achieving the best perfor-\nmance without sacrificing the editing effect.\nToken Merging Operation. We ablate on our token merg-\ning choice in Fig. 7. The original ToMe algorithm merges\ntokens by averaging their values. However, we find that this\ncan lead to a lack of diversity and randomness in the gen-\nerated videos, such as the flamingo features being single-\ncolored.\nTherefore, we directly replace the value of the\nmerged tokens with dst tokens. Global token merging is\ncrucial for keeping long-term consistency in videos. With-\nMean Merging No Global ToMe\nOurs\nTo \u201cRainbow-colored Origami Flamingo\u201d\nInput Video\nFigure 7. Ablation on Token Merging Operations. Merging tokens\nby mean instead of replacement reduces the edit fidelity. Without\nglobal token merging, the feather color changes.\nConcat, then ToMe.\nIntra-frame ToMe, then concat.\nVidToMe (Ours)\nTo \u201cOil Painting Style\u201d\nFigure 8. Ablation on local token merging strategy. Try: (i) Con-\ncatenate multi-frame tokens and then perform vanilla ToMe. (ii)\nPerform ToMe in each frame, then concatenate their tokens. (iii)\nOur local token merging strategy. We control the token number\nafter merging to be the same.\nout it, our method fails to maintain consistent rainbow\nfeather colors throughout the video.\nLocal Token Merging Strategy. There are several possi-\nble strategies to merge tokens through multiple consecu-\ntive frames. One straightforward approach is to perform\nthe original ToMe [4] among tokens from all frames, where\ndst tokens are randomly selected. Or we can apply ToMe\nin each frame first and then concatenate all merged tokens.\nFig. 8 shows that these two methods result in blurry frames,\nwhile our local merging strategy preserves the quality of the\ngenerated frames.\nLimitations. Our method has two main limitations. First,\nthe editing capability of our method depends on the perfor-\nmance of the selected image editing technique. If the edit-\ning technique fails on a single frame, our method also fails\nto edit the entire video. Second, although our similarity-\nbased matching performs well in most cases, it has room\nfor improvement. Objects with similar features are some-\ntimes incorrectly merged and mixed in the output results.\nWe plan to explore a more precise token-matching approach\nin the future to address these issues.\n6. Conclusion\nThis work proposes a diffusion-based zero-shot video edit-\ning method, VidToMe.\nOur approach unifies and com-\npresses internal diffusion features by matching and merg-\ning tokens across video frames in the self-attention module\nduring generation, resulting in temporally consistent edited\n8\nvideo frames. We implement VidToMe as lightweight token\nmerging and unmerging blocks attached to the self-attention\nmodule, making it compatible with any existing image edit-\ning method and diffusion models.\nReferences\n[1] Nuha Aldausari, Arcot Sowmya, Nadine Marcus, and\nGelareh Mohammadi.\nVideo generative adversarial net-\nworks: a review. ACM Computing Surveys (CSUR), 55(2):\n1\u201325, 2022. 2\n[2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In CVPR,\npages 18208\u201318218, 2022. 2\n[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, pages 22563\u201322575, 2023.\n2\n[4] Daniel Bolya and Judy Hoffman.\nToken merging for fast\nstable diffusion. CVPR Workshop on Efficient Deep Learning\nfor Computer Vision, 2023. 1, 3, 8\n[5] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao\nZhang, Christoph Feichtenhofer, and Judy Hoffman. Token\nmerging: Your ViT but faster. In ICLR, 2023. 1, 3\n[6] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra.\nPix2video: Video editing using image diffusion. In ICCV,\npages 23206\u201323217, 2023. 1, 3, 4, 5, 6, 7\n[7] Antonia Creswell, Tom White, Vincent Dumoulin, Kai\nArulkumaran, Biswa Sengupta, and Anil A Bharath. Gen-\nerative adversarial networks: An overview. IEEE signal pro-\ncessing magazine, 35(1):53\u201365, 2018. 2\n[8] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,\nand Mubarak Shah. Diffusion models in vision: A survey.\nIEEE TPAMI, 2023. 1, 2\n[9] Prafulla Dhariwal and Alexander Nichol.\nDiffusion mod-\nels beat gans on image synthesis. NeurIPS, 34:8780\u20138794,\n2021. 1, 2\n[10] Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation. arXiv preprint arXiv:2306.00986,\n2023. 2\n[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit Haim Bermano, Gal Chechik, and Daniel Cohen-or.\nAn image is worth one word: Personalizing text-to-image\ngeneration using textual inversion. In ICLR, 2022. 2\n[12] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-\nguided domain adaptation of image generators. ACM Trans-\nactions on Graphics (TOG), 41(4):1\u201313, 2022. 6\n[13] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arXiv:2307.10373, 2023. 3\n[14] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-or. Prompt-to-prompt image\nediting with cross-attention control. In ICLR, 2022. 2\n[15] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 2\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 33:6840\u20136851, 2020. 1,\n2, 3\n[17] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2\n[18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J. Fleet. Video diffu-\nsion models. arXiv preprint arXiv:2204.03458, 2022. 2\n[19] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan\nYang, Erik Learned-Miller, and Jan Kautz.\nSuper slomo:\nHigh quality estimation of multiple intermediate frames for\nvideo interpolation. In CVPR, pages 9000\u20139008, 2018. 6\n[20] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. NeurIPS, 35:26565\u201326577, 2022. 2\n[21] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming\nSong. Denoising diffusion restoration models. NeurIPS, 35:\n23593\u201323606, 2022. 2\n[22] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 6, 7, 8\n[23] Diederik P. Kingma and Max Welling. Auto-Encoding Vari-\national Bayes. In ICLR, 2014. 3\n[24] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun\nFeng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff: Single\nimage super-resolution with diffusion probabilistic models.\nNeurocomputing, 479:47\u201359, 2022. 2\n[25] Ming-Yu Liu, Xun Huang, Jiahui Yu, Ting-Chun Wang, and\nArun Mallya. Generative adversarial networks for image and\nvideo synthesis: Algorithms and applications. Proceedings\nof the IEEE, 109(5):839\u2013862, 2021. 2\n[26] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models.\nIn CVPR,\npages 11461\u201311471, 2022. 2\n[27] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In ICLR, 2021. 2\n[28] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real images\nusing guided diffusion models. In CVPR, pages 6038\u20136047,\n2023. 2\n[29] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2\n[30] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn ICML, pages\n8162\u20138171. PMLR, 2021. 2\n9\n[31] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen.\nGlide: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In ICML, pages 16784\u201316804. PMLR, 2022. 2\n[32] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In SIGGRAPH, pages 1\u201311, 2023. 2\n[33] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M.\nGross, and A. Sorkine-Hornung. A benchmark dataset and\nevaluation methodology for video object segmentation. In\nCVPR, 2016. 5\n[34] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen.\nFatezero:\nFusing attentions for zero-shot text-based video editing.\narXiv:2303.09535, 2023. 1, 3, 5, 6\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, pages 8748\u20138763. PMLR, 2021. 2\n[36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 2, 3\n[37] Partha Pratim Ray. Chatgpt: A comprehensive review on\nbackground, applications, key challenges, bias, ethics, lim-\nitations and future scope.\nInternet of Things and Cyber-\nPhysical Systems, 2023. 6\n[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 1, 2, 3, 4, 5, 6, 7\n[39] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn Medical Image Computing and Computer-Assisted Inter-\nvention, pages 234\u2013241. Springer, 2015. 3, 4\n[40] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, pages 22500\u201322510, 2023. 2\n[41] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi. Palette: Image-to-image diffusion models. In SIG-\nGRAPH, pages 1\u201310, 2022. 2\n[42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. NeurIPS, 35:36479\u201336494, 2022.\n2, 3\n[43] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In ICLR, 2022. 2\n[44] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for train-\ning next generation image-text models. NeurIPS, 35:25278\u2013\n25294, 2022. 2\n[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn ICML, pages 2256\u2013\n2265. PMLR, 2015. 2, 3\n[46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In ICLR, 2020. 1, 2, 3, 4,\n5\n[47] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In ICML, 2020. 1, 2, 3\n[48] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation.\nIn CVPR, pages 1921\u20131930,\n2023. 2, 3, 4, 5, 6, 7, 8\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NeurIPS, 30, 2017. 4\n[50] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or.\nSketch-guided text-to-image diffusion models.\nIn SIG-\nGRAPH, pages 1\u201311, 2023. 2\n[51] Wen Wang, kangyang Xie, Zide Liu, Hao Chen, Yue Cao,\nXinlong Wang, and Chunhua Shen. Zero-shot video editing\nusing off-the-shelf image diffusion models. arXiv preprint\narXiv:2303.17599, 2023. 1, 3, 6, 7\n[52] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nICCV, pages 7623\u20137633, 2023. 1, 3, 6, 7\n[53] Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jin-\nbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei\nHuang, Yuanxi Sun, Rui He, Feng Hu, Junhua Hu, Hai\nHuang, Hanyu Zhu, Xu Cheng, Jie Tang, Mike Zheng Shou,\nKurt Keutzer, and Forrest Iandola. Cvpr 2023 text guided\nvideo editing competition. arXiv preprint arXiv:2310.16003,\n2023. 5\n[54] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. In SIGGRAPH, 2023. 1, 3, 4\n[55] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin.\nVideo probabilistic diffusion models in projected latent\nspace. In CVPR, pages 18456\u201318466, 2023. 2\n[56] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nICCV, 2023. 2, 3, 4, 5, 6, 7, 8\n10\n"
  },
  {
    "title": "Catwalk: A Unified Language Model Evaluation Framework for Many Datasets",
    "link": "https://arxiv.org/pdf/2312.10253.pdf",
    "upvote": "7",
    "text": "Catwalk: A Unified Language Model Evaluation Framework\nfor Many Datasets\nDirk Groeneveld\u2660\nAnas Awadalla\u2662\nIz Beltagy\u2660\nAkshita Bhagia\u2660\nIan Magnusson\u2660\nHao Peng\u2660\nOyvind Tafjord\u2660\nPete Walsh\u2660\nKyle Richardson\u2660\nJesse Dodge\u2660\n\u2660Allen Institute for Artificial Intelligence\n\u2662Paul G. Allen School of Computer Science & Engineering, University of Washington\nanasa2@cs.washington.edu\n{dirkg,beltagy,akshitab,ianm,haop,oyvindt,petew,kyler,jessed}@allenai.org\nAbstract\nThe success of large language models has\nshifted the evaluation paradigms in natural lan-\nguage processing (NLP). The community\u2019s in-\nterest has drifted towards comparing NLP mod-\nels across many tasks, domains, and datasets,\noften at an extreme scale. This imposes new\nengineering challenges: efforts in construct-\ning datasets and models have been fragmented,\nand their formats and interfaces are incom-\npatible. As a result, it often takes extensive\n(re)implementation efforts to make fair and con-\ntrolled comparisons at scale.\nCatwalk aims to address these issues. Catwalk\nprovides a unified interface to a broad range\nof existing NLP datasets and models, ranging\nfrom both canonical supervised training and\nfine-tuning, to more modern paradigms like\nin-context learning. Its carefully-designed ab-\nstractions allow for easy extensions to many\nothers. Catwalk substantially lowers the bar-\nriers to conducting controlled experiments at\nscale. For example, we finetuned and evalu-\nated over 64 models on over 86 datasets with\na single command, without writing any code.\nMaintained by the AllenNLP team at the Allen\nInstitute for Artificial Intelligence (AI2), Cat-\nwalk is an ongoing open-source effort: https:\n//github.com/allenai/catwalk.\n1\nIntroduction\nLarge language models (LLMs) have led to a\nparadigm shift in NLP. Before, a system would\nbe built to tackle a specific task and trained and\nevaluated on a dedicated dataset. Now, general-\npurpose pretrained language models are adapted to\na broad range of tasks through techniques like fine-\ntuning and in-context learning. The new paradigm\nopens research opportunities to investigate models\u2019\ngeneralization and robustness across many tasks\nand domains. This imposes new engineering chal-\nlenges. The fragmented efforts in producing mod-\nels and datasets have led to countless codebases\nT0\nRoBERTa\nT5\nGPT2\nBERT\nDeBERTa\nOPT\nBLOOM\n\u00b7\u00b7\u00b7\nCatwalk\nMNLI\nWikitext\nSQuAD\nCOPA\nMRQA\nMRPC\nRTE\nLAMBADA\n\u00b7\u00b7\u00b7\nModels\nTasks\n(a) Catwalk provides a unified interfaces to connect models\nand datasets. This compares n models on m tasks with only\nn + m custom implementations, substantially reducing the\nimplementation workload compared to current practice where\nnm implementations are needed.\nGPT-2 \nMedium\nGPT-2\nXL\nPrompt\nSource\nEleuther-\nstyle\nAccuracy\n0.553\nAccuracy\n0.653\nLogiQA\nMNLI\n1-shot\n2-shot\nGPT-2\nLarge\nMetaICL\nRanked \nClass.\nT5 Base\nFinetune\nWino\nGrande\nRTE\n0-shot\n(b) Illustration of some decisions a user of Catwalk makes\nto get a result. For example, choosing GPT-2 Large, with\nRanked Classification, on WinoGrande gives an accuracy of\n0.553, while choosing T5-Base on RTE gives an accuracy of\n0.653.\nFigure 1: Catwalk and its workflow.\nand data formats that are hardly compatible. Fairly\ncomparing n models on m datasets often require\nnm custom implementations, substantially raising\nthe barriers to conducting controlled experiments\nat scale.\nCatwalk aims to reduce practitioners\u2019 work-\nload in projects involving making fair comparisons\namong many models and datasets. At a high level,\nCatwalk can be thought of as a nexus connecting\nn models and m datasets. Through Catwalk\u2019s uni-\nfied interfaces, training and evaluating all models\non all datasets requires n + m customizations in-\nstead of mn, many of which have already been\nimplemented. To include a new model or dataset\narXiv:2312.10253v1  [cs.CL]  15 Dec 2023\ninto Catwalk, one only needs to instantiate a few\ninterfaces, and it can be evaluated on all dataset-\ns/models that Catwalk supports. Figures 1a and\n1b provide illustrative examples. Whenever pos-\nsible, Catwalk caches and reuses intermediate ex-\nperimental steps such as loaded datasets/models,\nmodel predictions, as well as trained models.1 This\nhelps minimize redundant machine workloads in,\ne.g., loading the data and training the models, and\nbetter utilize the limited hardware resources. Cat-\nwalk\u2019s caching mechanism is especially useful in\nlarge-scale experiments that involve many models\nand/or many tasks.\nCatwalk is an ongoing effort. We aim to jump-\nstart a virtuous cycle of dataset and model authors\nadopting support for Catwalk\u2019s unified interfaces.\nTo this end, we contribute:\n\u2022 6 model abstractions that support a variety of\nexisting models.\n\u2022 Zero-shot and few-shot evaluations, as well as\nfine-tuning.\n\u2022 9 data formats that cover common use cases.\n\u2022 Support for 86 standalone datasets as well\nas curated benchmarks that contain many\ndatasets themselves, for a total of over 800\ndatasets, along with evaluation metrics.\n\u2022 A case study that compares 30 widely-used\nmodels on 17 datasets.\nWe expect the above list to keep growing. Catwalk\nis open-source: https://github.com/allenai/\ncatwalk. It is maintained by the AllenNLP team at\nthe Allen Institute for Artificial Intelligence (AI2)\nand has been a useful tool for their projects, includ-\ning being the main evaluation framework used in\nthe OLMo (Open Language Model) project.2 We\nhope it can benefit the community\u2019s research too.\n2\nStructure and organization\nThe purpose of Catwalk is to answer the question\n\"How well does model X do on dataset Y?\", for\nas many models and datasets as possible. The\npossible models include GPT-style decoder-only\nmodels (Radford et al., 2018), T5-style encoder/de-\ncoder models (Raffel et al., 2019), and BERT-\nstyle fine-tuned models (Devlin et al., 2018). Cat-\nwalk imposes no constraint on the types of models\n1Catwalk\u2019s caching mechanism is enabled by AI2-\nTango (Groeneveld et al., 2023): https://github.com/\nallenai/tango.\n2See more details at https://allenai.org/olmo as well\nas https://github.com/allenai/ai2-olmo-eval for de-\ntails about our LLM evaluation efforts.\nit can support. The model does not have to be\ntransformer-based or even be a language model at\nall, though all currently implemented models are\ntransformer-based language models.\nTo achieve this flexibility, Catwalk transforms in-\nstances from a dataset into \u201cformats\u201d that are more\nsuited to model execution. Crucially, these formats\nare the same across many datasets, so model im-\nplementations do not have to concern themselves\nwith the differences between datasets, and can be\nwritten in a simple, generic style.\n3\nModels and Model Wrappers\nCatwalk\u2019s models combine an underlying model, a\nmodel wrapper, and a method to process the outputs\nfrom the model. For example, a decoder-only lan-\nguage model can be used to compute the probability\nof various answer options. This can be done us-\ning T0-style human-readable prompts (Sanh et al.,\n2021), or it can be done using machine-readable\nprompts in a few-shot setting (Gao et al., 2023).\nThis model could even be fine-tuned and used with\na task-specific head (Devlin et al., 2018). These\noptions have different characteristics and produce\ndifferent results. They are all encapsulated in Cat-\nwalk\u2019s model wrapper abstraction.\n3.1\nModel Wrappers\nModel wrappers come in two general varieties, na-\ntive interfaces that provide generic support for a\nwide class of models and adapter interfaces that\naim to facilitate systematic comparisons with exist-\ning model types or evaluation frameworks. We list\nthe different model wrappers below current imple-\nmented in Catwalk (with the first three being our\nnative interfaces and the last three being example\nadapter interfaces; see Table 1).\n\u2022 Ranked classification models. This model\nwrapper uses decoder-only and encoder/de-\ncoder models. It calculates the probability\nof all answer options, and chooses the most\nlikely answer. It supports zero-shot and few-\nshot evaluations, and it can be fine-tuned on a\ndataset, or a list of datasets.\n\u2022 BERT-style models. This model wrapper\nuses the native API from the Huggingface li-\nbrary (Wolf et al., 2019) to perform multiple-\nchoice, question answering, and classifica-\ntion tasks. This is the way to run encoder-\nonly models like BERT (Devlin et al., 2018),\nRoBERTa (Liu et al., 2019), or DeBERTa (He\nModel style\nEnc-only\nDec-only\nEnc/dec\nFinetuning\nZero-shot\nFew-shot\nRanked classification\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nBert-style\n\u2713\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\nLanguage models\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\n\u2713\nPromptSource\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nMetaICL\n\u2717\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\nEleuther-style\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\n\u2713\nTable 1: Model styles and supported features in Catwalk.\net al., 2020). It supports fine-tuning, but no\nfew-shot evaluations.\n\u2022 Language models. This model wrapper is\na generic class for decoder-only generation\nmodels that supports few- and zero-shot eval-\nuation. This is the class most relevant to re-\ncent GPT-style LLM evaluation. In contrast to\nEleuther-style models (Gao et al., 2023) (see\nbelow), it is designed to support a wider range\nof generation metrics and token-based scoring\nstrategies beyond those hard-coded in exist-\ning toolkits. It also has special features for\nreturning per-instance predictions that allow\nfor more fine-grained analysis and debugging.\n\u2022 PromptSource models. This model wrapper\nalso performs ranked classification, but it uses\nthe human-readable prompts from the Prompt-\nSource project (Bach et al., 2022). It supports\nzero-shot and few-shot evaluations.\n\u2022 MetaICL models. This model wrapper ap-\nplies the prompt truncation strategy used by\nMin et al. (2022b) to decoder-only ranked\nclassification. It truncates each of several in-\ncontext demonstrations separately and trun-\ncates again after concatenation.\n\u2022 Eleuther-style models. This model wrapper\nuses decoder-only models in the style of Gao\net al. (2023). It reproduces those numbers\nexactly, though it runs a little faster. Like Gao\net al. (2023), it supports zero-shot and few-\nshot evaluations. On many datasets, this is\nequivalent to the ranked classification models.\n3.2\nModels\nAll of Catwalk\u2019s existing model implementations\nload parameters and configurations from Hugging-\nface. They can be configured to load any model\nfrom the Huggingface hub, but Catwalk gives a few\nof them names, tests them regularly, and makes\nthem available on the command line. The follow-\ning is a list of models that have names in Catwalk\nand that have been extensively tested:\n\u2022 BERT {tiny,\nbase,\nlarge},\n{cased,\nun-\ncased} (Devlin et al., 2018).\n\u2022 BLOOM {560M, 1B1, 1B7, 3B, 7B1} (Scao\net al., 2022).\n\u2022 CT0-11b (Scialom et al., 2022).\n\u2022 DEBERTA {v3-small, v3-base, v3-large, v2-\nxlarge, v2-xxlarge} (He et al., 2020, 2023).\n\u2022 DistilBERT-base-cased-distilled-squad (Sanh\net al., 2019).\n\u2022 GPT-J-6B (Wang and Komatsuzaki, 2021).\n\u2022 GPT-Neo {125M, 1.3B, 2.7B} (Gao et al.,\n2020).\n\u2022 GPT-NeoX-20B (Black et al., 2022).\n\u2022 GPT2 {tiny, regular, medium, large, xl} (Rad-\nford et al., 2018).\n\u2022 mT5 {small, base, large, xl} (Xue et al.,\n2020).\n\u2022 OPT {125M, 350M, 1.3B, 2.7B, 6.7B, 13B,\n30B, 66B} (Zhang et al., 2022).\n\u2022 RoBERTa {base, large} (Liu et al., 2019).\n\u2022 T0 {regular, 3B, original_task_only, sin-\ngle_prompt, p, pp} (Sanh et al., 2021).\n\u2022 T5 {very_small_random, small, small-lm-\nadapt, v1_1-small, base, base-lm-adapt, v1_1-\nbase, large, large-lm-adapt, v1_1-large, 3B, xl-\nlm-adapt, v1_1-xl, 11b, xxl-lm-adapt, v1_1-\nxxl} (Raffel et al., 2019).\n\u2022 Pythia {70M,160M,410M,1B,1.4B,2.7B,6.9B,12B}\n(Biderman et al., 2023)\n\u2022 Falcon {1B,7B,40B} (Almazrouei et al.,\n2023)\n\u2022 Llama(-2) {7B, 13B, 70B} (Touvron et al.,\n2023a,b)\n\u2022 MPT {7B, 30B} (MosaicML, 2023)\n4\nPrompting\nCatwalk supports a number of formats for dataset\ninstances. By converting dataset instances into a\ncommon format, we allow model implementations\nto only depend on the format, and not on the va-\ngaries of individual datasets.\nCatwalk supports the following formats.\n4.1\nHuggingface dictionaries\nThis format makes few guarantees beyond be-\ning a Python dictionary.\nIt is the raw format\nof the data, and rarely useful directly.\nThis\nis the format that the Huggingface datasets li-\nbrary (Lhoest et al., 2021) produces.\nIt isn\u2019t\nused by any of the models directly, but all the\nother formats use this as their starting point.\n\u0007\n\u0004\n{\n\" sentence1 \" :\n\"No Weapons of Mass\nD e s t r u c t i o n\nFound Yet . \" ,\n\" sentence2 \" :\n\" Weapons of Mass\nD e s t r u c t i o n\nFound . \" ,\n\" l a b e l \" :\n1 ,\n\" idx \" :\n0\n}\u0006\n\u0005\n4.2\nHuggingface multiple choice\nThis is a structured format containing a question,\na list of answer choices, and optionally the in-\ndex of the correct answer. This corresponds ex-\nactly to the expectation of Huggingface multiple\nchoice models. It is used by the BERT-style models.\n\u0007\n\u0004\nHFMCInstance (\nid =\" Mercury_7220990 \" ,\nq u e s t i o n =\" Which\nf a c t o r\nw i l l\nmost\nl i k e l y\ncause\na \" +\n\" person\nto\ndevelop\na\nf e v e r ? \" ,\nanswer_choices =[\n\" a\nleg\nmuscle\nr e l a x i n g\na f t e r\ne x e r c i s e \" ,\n\" a\nb a c t e r i a l\np o p u l a t i o n\nin\nthe\nbloodstream \" ,\n\" s e v e r a l\nv i r a l\np a r t i c l e s\non\nthe\nskin \" ,\n\" c a r b o h y d r a t e s\nbeing\nd i g e s t e d\nin\nthe\nstomach \"\n] ,\nc o r r e c t _ a n sw e r _ i n d e x =1\n)\u0006\n\u0005\n4.3\nHuggingface question-answering\nThis is a structured format containing a context,\na question, and a list of possible correct answers.\nThis corresponds exactly to the expectation of Hug-\ngingface question-answering models. It is used by\nthe BERT-style models.\n\u0007\n\u0004\nHFQAInstance (\nid =\" 5733 be284776f41900661182 \" ,\nq u e s t i o n =\"To whom did\nthe\nVirgin Mary appear\nin\n1858? \" ,\nc o n t e x t =\" . . .\nthe\nVirgin Mary appeared\nto\nS a i n t\n\" +\n\" B e r n a d e t t e\nSoubirous\nin\n1858\n. . . \" ,\nanswers ={\n\" t e x t \" :\n[ \" S a i n t\nB e r n a d e t t e\nSoubirous \" ] ,\n\" a n s w e r _ s t a r t \" :\n[ 515 ] } )\n\u0006\n\u0005\n4.4\nHuggingface classification\nThis is a structured format containing a context,\nas a string or a pair of strings, and an optional\nlabel. The possible answer choices are not part\nof the format but are part of the model that\nuses these instances.\nThis corresponds exactly\nto the expectation of Huggingface classification\nmodels.\nIt is used by the BERT-style models.\n\u0007\n\u0004\nH F C l a s s i f i c a t i o n I n s t a n c e (\nt e x t =(\n\"No Weapons of Mass\nD e s t r u c t i o n\nFound Yet . \" ,\n\" Weapons of Mass\nD e s t r u c t i o n\nFound . \"\n) ,\nl a b e l =1 )\n\u0006\n\u0005\n4.5\nEleuther\nThere are several formats specific to the Eleuther\nevaluation framework (Gao et al., 2023). They\nare useful to build other format converters on top\nof, but are rarely useful on their own. Only the\nEleuther-style models use these formats directly.\n4.6\nT5\nThis format represents instances as strings start-\ning with the dataset name followed by named\nfields that make up the instance. It also includes\na gold answer that is expected of the model.\nThis corresponds exactly to the way T5 (Raf-\nfel et al., 2019) was trained.\nThe format is\nonly supported for a small number of datasets.\n\u0007\n\u0004\n(\n\" r t e\nsentence1 : No Weapons of Mass\nD e s t r u c t i o n\nfound\nyet\nsentence2 :\nWeapons of Mass\nD e s t r u c t i o n\nFound . \" ,\n\" n o t _ e n t a i l m e n t \"\n)\u0006\n\u0005\n4.7\nRanked classification\nThis format represents instances as a list of op-\ntions, together with an optional label indicating\nthe correct option.\nEach option is a text pair,\ncalled \u201ccontext\u201d and \u201ccontinuation\u201d. Models gen-\nerally compute the probability of the continua-\ntion given the context to determine the best an-\nswer, though recent work suggests other possi-\nbilities (Min et al., 2022a). The format is used\nby the built-in ranked classification models, and\nit forms the basis for the PromptSource format.\n\u0007\n\u0004\nR a n k C l a s s i f i c a t i o n I n s t a n c e (\nchoi ces =[\n(\n\"No Weapons of Mass\nD e s t r u c t i o n\nFound Yet . \\ n\" +\n\" Question :\nWeapons of Mass\nD e s t r u c t i o n\nFound .\n\" +\n\" True\nor\nFalse ? \\ nAnswer : \" ,\n\" True \"\n) ,\n(\n\"No Weapons of Mass\nD e s t r u c t i o n\nFound Yet . \\ n\" +\n\" Question :\nWeapons of Mass\nD e s t r u c t i o n\nFound .\n\" +\n\" True\nor\nFalse ? \\ nAnswer : \" ,\n\"\nFalse \" )\n\u0006 ] , c o r r e c t _ c h o i c e =1 )\n\u0005\n4.8\nPromptSource\nThis format makes the prompts from the Prompt-\nSource project (Bach et al., 2022) available in Cat-\nwalk. In the PromptSource project, every dataset\nhas multiple prompts. These are reflected in the\nformat by having multiple versions of the instance,\none per prompt, in a dictionary. Every possible\nversion looks exactly like a ranked classification in-\nstance, and can be used by model implementations\naccordingly. The PromptSource model implemen-\ntation uses this format.\n\u0007\n\u0004\n{ \" does\nthe\nclaim \" :\nR a n k C l a s s i f i c a t i o n I n s t a n c e (\nchoi ces =[\n(\n\" Does\nthe\nclaim\n' Weapons of Mass\nD e s t r u c t i o n\n\" +\n\" Found . '\nfollow\nfrom\nthe\nf a c t\nt h a t\n'No Weapons \" +\n\" of Mass\nD e s t r u c t i o n\nFound Yet . ' ?\nPlease\n\" +\n\" answer\ne i t h e r\nyes\nor no . \" ,\n\" yes \"\n) ,\n(\n\" Does\nthe\nclaim\n' Weapons of Mass\nD e s t r u c t i o n\n' \" +\n\" Found . '\nfollow\nfrom\nthe\nf a c t\nt h a t\n'No Weapons \" +\n\" of Mass\nD e s t r u c t i o n\nFound Yet . ' ?\nPlease\n\" +\n\" answer\ne i t h e r\nyes\nor no . \" ,\n\" no \"\n)\n] ,\nc o r r e c t _ c h o i c e =1 ) ,\n\" imply \" :\nR a n k C l a s s i f i c a t i o n I n s t a n c e (\nchoi ces =[\n(\n\" Does\n'No Weapons of Mass\nD e s t r u c t i o n\nFound \" +\n\" Yet . '\nimply\nt h a t\n' Weapons of Mass\nD e s t r u c t i o n\n\" +\n\" Found . ' ?\nPlease\nanswer\ne i t h e r\nyes\nor no . \" ,\n\" yes \"\n) ,\n(\n\" Does\n'No Weapons of Mass\nD e s t r u c t i o n\nFound \" +\n\" Yet . '\nimply\nt h a t\n' Weapons of Mass\nD e s t r u c t i o n\n\" +\n\" Found . ' ?\nPlease\nanswer\ne i t h e r\nyes\nor no . \" ,\n\" no \"\n)\n] , c o r r e c t _ c h o i c e =1 ) }\n\u0006\n\u0005\n4.9\nPerplexity\nCatwalk also has a general-purpose task format\nfor perplexity evaluation, which was developed\nas part of the PALOMA benchmark (Magnusson\net al., 2023). It follows best practices for perplexity\nanalysis such as avoiding document concatenation\n(Gao et al., 2020), has advanced batching strategies\n(e.g., sorting by input length to avoid excessive\npadding) and supports both non-overlapping and\nsliding window inference from Press et al. (2020)\nto handle common issues related to document max-\nimum length truncation.\n5\nDatasets\nThe datasets supported by Catwalk can be extended\nwith just a few lines of configs with prompts. Cur-\nrently Catwalk contains the following datasets:\nPerplexity\n\u2022 WikiText (Merity, 2019)\n\u2022 LAMBADA (Paperno et al., 2016)\nClassification\n\u2022 CoLA (Warstadt et al., 2019)\n\u2022 MRPC (Dolan and Brockett, 2005)\n\u2022 QQP (DataCanary et al., 2017)\n\u2022 SST-2 (Socher et al., 2013)\n\u2022 BoolQ (Clark et al., 2019)\n\u2022 MultiRC (Khashabi et al., 2018)\n\u2022 WiC (Pilehvar and Camacho-Collados, 2019)\n\u2022 MC-TACO (Ben Zhou and Roth, 2019)\n\u2022 PubMedQA (Jin et al., 2019)\n\u2022 Hendrycks Ethics (Hendrycks et al., 2021a)\n\u2022 RAFT (Alex et al., 2021)\nOpen-ended QA\n\u2022 SQuAD (Rajpurkar et al., 2016)\n\u2022 SquadShifts (Miller et al., 2020)\n\u2022 MRQA (Fisch et al., 2019)\n\u2022 SQuAD2 (Rajpurkar et al., 2018)\n\u2022 DROP (Dua et al., 2019)\n\u2022 TriviaQA (Joshi et al., 2017)\n\u2022 WebQuestions (Berant et al., 2013)\n\u2022 TruthfulQA (Lin et al., 2021)\n\u2022 Hendrycks Math (Hendrycks et al., 2021c)\n\u2022 Arithmetic (Brown et al., 2020)\n\u2022 Anagrams 1 and 2 (Brown et al., 2020)\n\u2022 Cycle Letters (Brown et al., 2020)\n\u2022 Random Insertion (Brown et al., 2020)\n\u2022 Reversed Words (Brown et al., 2020)\n\u2022 Natural Questions (Kwiatkowski et al., 2019)\nMultiple-choice QA\n\u2022 PIQA (Bisk et al., 2020)\n\u2022 COPA (Gordon et al., 2012)\n\u2022 WSC (Levesque et al., 2011)\n\u2022 PROST (Aroca-Ouellette et al., 2021)\n\u2022 SciQ (Welbl et al., 2017)\n\u2022 QA4MRE (Pe\u00f1as et al., 2013)\n\u2022 ARC Easy and Challenge (Clark et al., 2018)\n\u2022 LogiQA (Liu et al., 2020)\n\u2022 HellaSwag (Zellers et al., 2019)\n\u2022 OpenBookQA (Mihaylov et al., 2018)\n\u2022 RACE (Lai et al., 2017)\n\u2022 HEAD-QA (Vilares and G\u00f3mez-Rodr\u00edguez,\n2019)\n\u2022 MathQA (Amini et al., 2019)\n\u2022 WINOGRANDE (Sakaguchi et al., 2019)\n\u2022 MuTual (Cui et al., 2020)\n\u2022 MMLU (Hendrycks et al., 2021b)\n\u2022 CaseHOLD (Zheng et al., 2021)\n\u2022 SocialIQA (Sap et al., 2019)\n\u2022 CSQA (Talmor et al., 2018)\nSummarization\n\u2022 SciTLDR (Cachola et al., 2020)\n\u2022 XSUM (Narayan et al., 2018)\n\u2022 EURLEX (Aumiller et al., 2022)\nNLI\n\u2022 RTE from GLUE (Wang et al., 2018)\n\u2022 RTE from SuperGLUE (Wang et al., 2019)\n\u2022 MNLI (Williams et al., 2018)\n\u2022 QNLI (Wang et al., 2018)\n\u2022 WNLI (Wang et al., 2018)\n\u2022 CB (de Marneffe et al., 2019)\n\u2022 Adverserial NLI (Nie et al., 2020)\nMixtures of tasks\n\u2022 P3 (Sanh et al., 2022)\n\u2022 MetaICL (Min et al., 2022b)\nNote that some of these datasets are collections\nthat show up in Catwalk as multiple subsets. For\nexample, there are actually 11 RAFT datasets, each\ntaking a different angle on language model evalua-\ntion. In total, this adds up to over 800 datasets.\nAdding new datasets to Catwalk is easy. There\nare classes for including datasets from the Hug-\ngingface hub, and format conversion functions can\noften be written in a single line of code.\n6\nEvaluation\nIn Catwalk, the responsibility for calculating met-\nrics lies with the model implementations. Model\nimplementations can calculate any metric that\nmakes sense in the context of the model. How-\never, for a given dataset we often want the same\nmetrics independent of the model, so we can com-\npare models against each other. To facilitate this,\nCatwalk allows one to attach metrics to datasets,\nwhich act as a suggestion to the models for which\nmetrics to compute. All implemented model styles\nexcept the Eleuther style compute the suggested\nmetrics.\nBy default, Catwalk computes the following met-\nrics and can be easily extended to include others.\n\u2022 For multiple-choice, classification, and entail-\nment datasets: Accuracy and relative improve-\nment over the random baseline\n\u2022 For question-answering datasets:\nSQuAD\nmetric as defined by Rajpurkar et al. (2016).\n\u2022 For language modeling datasets: Perplexity\nper word, perplexity per byte, and entropy.\n7\nCase Studies\nWe include some examples of the kinds of analy-\nses Catwalk enables and in the end describe some\nongoing work. Table 3 in the Appendix gives an\nexample of the dense matrix of models and datasets\nthat Catwalk can generate. There are many research\nquestions that can be posed and answered with a\nmatrix of results like this:\nWhat strategy for adapting pretrained models to\ndownstream tasks performs best overall?\nFor\ninstance we can compare finetuning an encoder-\nonly model against zero-shot evaluation on autore-\ngressive models. We find that, macro averaged\nover 17 datasets, the best finetuned models had\n62% greater relative improvement in accuracy over\na random baseline than the zero-shot models , al-\nthough the largest zero-shot model is an order of\nmagnitude larger than the largest finetuned model.\nWhich datasets are easy/hard?\nAll zero-shot\nand finetuned models perform well on SciQ, with\nan average of more than 200% relative accuracy\nimprovement compared to the random-guess base-\nline. Among the datasets in our experiments, QNLI\nis the most challenging one for zero-shot ranked\nclassification models, with all models barely outper-\nforming random guesses. Unsurprisingly, all fine-\ntuned models substantially outperform the chance\nbaseline on all datasets, and in all cases they out-\nperform zero-shot models by a wide margin.\nWhat impact does model type or size have across\ndatasets?\nAmong zero-shot models, we see that,\nwhile T5 variants are the only ones that signifi-\ncantly outperform the chance baseline on MNLI,\nQQP, and RTE, they underperform others and even\nthe random baseline on CoLA and COPA. This\ncould be attributed to the different choices T5 mod-\nels make in terms of learning objectives, pretrain-\ning data, etc., which requires further investigation.\nAlso, in both the zero-shot ranked classification and\nfinetuning settings, larger model sizes, in general,\nlead to better performance within the same model\nfamily, with few exceptions.\nIs the ordering of models by performance con-\nsistent across datasets?\nTable 2 in the Appendix\nshows that every dataset puts the models into\nroughly the same order by accuracy, but corre-\nlations are often weak. The largest correlations\nare observed between the ARC Easy, ARC Chal-\nlenge, and OpenbookQA datasets. All three of\nthese are multiple-choice datasets targeting sci-\nence questions. On the other end of the spectrum,\nLogiQA and SciQ correlate the least with other\ndatasets, with coefficients below 0.5. This might\nbe explained by LogiQA being the most difficult,\nand SciQ being the easiest dataset among the 17\nthat were part of this study. In one case the models\ncannot get much signal out of the dataset, and in\nthe other they saturate. A thorough investigation in\nthis style would have to include many more models\nand prompting methods.\nThese analyses only scratch the surface of the\nquestions that Catwalk can help answer. There are\nmany open questions that bear investigation:\n\u2022 How well does perplexity predict performance\non other datasets?\n\u2022 Do the zero-shot results hold in few-shot set-\ntings? How does the number of few-shot ex-\namples affect the results?\n\u2022 How important is the format of the prompt?\n\u2022 Some large language models make intermedi-\nate checkpoints available for analysis. At what\npoint in model training do abilities emerge?\nOngoing work\nAs mentioned at the outset, Cat-\nwalk is currently be used as the main evaluation\nframework for the OLMo project at AI2. As such,\nit has many features that allow it to be a replace-\nment for other evaluation frameworks, such as\nEleuther Harness (Gao et al., 2023). For more de-\ntails about Catwalk for LLM evaluation, see https:\n//github.com/allenai/ai2-olmo-eval.\n8\nRelated Work\nHolistic Evaluation of Language Models (HELM;\nLiang et al., 2022) also aims to increase the den-\nsity of coverage between models and evaluations\nand take a complementary path to ours. HELM\ndescribes itself as a top-down approach; they de-\nsign a taxonomy of scenarios and metrics and use\nthem to curate a specific subset of combinations.\nCatwalk is bottom-up, and aims to lower the en-\ngineering barriers for model and task creators to\nintegrate their work into a shared benchmark. For\nexample, HELM chooses to only adapt models via\na standardized 5-shot prompting method, while Cat-\nwalk explicitly incorporates different varieties of\nprompting and even fine-tuning.\nOur work builds on the EleutherAI LM Evalu-\nation Harness (Gao et al., 2023). It similarly ad-\ndresses the task of building a unified evaluation\nframework and focuses on decoder-only models\nand zero-/few-shot evaluations. By contrast, Cat-\nwalk also includes encoder-only and encoder/de-\ncoder models and supports fine-tuning.\n9\nConclusion\nWe presented Catwalk, a system to evaluate many\nmodels on many datasets. Catwalk\u2019s APIs elimi-\nnate redundant implementation efforts and unify\nincompatible approaches to language model evalua-\ntion. The approach enables experimentation across\ndatasets and language models at an unprecedented\nscale. While a large number of models and datasets\nis already integrated, the Allen Institute for Arti-\nficial Intelligence continues to provide hands-on\nengineering help to grow this list, and invites fur-\nther contributions as an open-source effort.\nLimitations\nLanguage model evaluation can only be as good\nas the datasets that are used. Catwalk takes no\nposition on this topic, and instead makes as many\ndatasets as possible available for study. As shown\nin Table 2, the choice of dataset can have a big in-\nfluence on which model appears \u201cbest\u201d. Similarly,\nprompted models are often sensitive to the prompt,\nand fine-tuned models are sensitive to their training\nregime. We hope that Catwalk will enable the kind\nof research that allows us to take strong, supported\npositions on these topics in the future.\nEthics Statement\nWe believe the impact of this framework to be\nlargely beneficial, in that it encourages the com-\nmunity to adopt reproducible research practices.\nA unified interface for evaluation allows for easy\ncomparison across various papers, without the need\nfor reimplementation and rerunning experiments\nthat may take up significant time and compute re-\nsources.\nWe also note that the datasets present in our\nframework may contain biases and artifacts of\ntheir own, and a model performing well within\nour framework on these datasets should not be con-\nsidered as having \u201csolved\u201d the underlying task.\nReferences\nNeel Alex, Eli Lifland, Lewis Tunstall, Abhishek\nThakur, Pegah Maham, C. Jess Riedel, Emmie\nHine, Carolyn Ashurst, Paul Sedille, Alexis Car-\nlier, Michael Noetel, and Andreas Stuhlm\u00fcller. 2021.\nRAFT: A real-world few-shot text classification\nbenchmark. CoRR, abs/2109.14076.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMerouane Debbah, Etienne Goffinet, Daniel Hes-\nlow, Julien Launay, Quentin Malartic, Badreddine\nNoune, Baptiste Pannier, and Guilherme Penedo.\n2023. Falcon-40B: an open large language model\nwith state-of-the-art performance.\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. Mathqa: Towards interpretable math\nword problem solving with operation-based for-\nmalisms. CoRR, abs/1905.13319.\nSt\u00e9phane Aroca-Ouellette, Cory Paik, Alessandro Ron-\ncone, and Katharina Kann. 2021. PROST: Physi-\ncal reasoning about objects through space and time.\nIn Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021, pages 4597\u20134608,\nOnline. Association for Computational Linguistics.\nDennis Aumiller, Ashish Chouhan, and Michael Gertz.\n2022.\nEur-lex-sum:\nA multi-and cross-lingual\ndataset for long-form summarization in the legal do-\nmain. arXiv preprint arXiv:2210.13448.\nStephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert\nWebson, Colin Raffel, Nihal V. Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault\nFevry, Zaid Alyafeai, Manan Dey, Andrea San-\ntilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu,\nGunjan Chhablani, Han Wang, Jason Alan Fries,\nMaged S. Al-shaibani, Shanya Sharma, Urmish\nThakker, Khalid Almubarak, Xiangru Tang, Xian-\ngru Tang, Mike Tian-Jian Jiang, and Alexander M.\nRush. 2022. Promptsource: An integrated develop-\nment environment and repository for natural language\nprompts.\nQiang Ning Ben Zhou, Daniel Khashabi and Dan Roth.\n2019. \u201cgoing on a vacation\u201d takes longer than \u201cgo-\ning for a walk\u201d: A study of temporal commonsense\nunderstanding. In EMNLP.\nJonathan Berant, Andrew K. Chou, Roy Frostig, and\nPercy Liang. 2013. Semantic parsing on freebase\nfrom question-answer pairs. In Conference on Em-\npirical Methods in Natural Language Processing.\nStella Biderman, Hailey Schoelkopf, Quentin Gregory\nAnthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hal-\nlahan, Mohammad Aflah Khan, Shivanshu Purohit,\nUSVSN Sai Prashanth, Edward Raff, et al. 2023.\nPythia: A suite for analyzing large language mod-\nels across training and scaling.\nIn International\nConference on Machine Learning, pages 2397\u20132430.\nPMLR.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\nphysical commonsense in natural language. In Thirty-\nFourth AAAI Conference on Artificial Intelligence.\nSid Black, Stella Biderman, Eric Hallahan, Quentin An-\nthony, Leo Gao, Laurence Golding, Horace He, Con-\nnor Leahy, Kyle McDonell, Jason Phang, Michael\nPieler, USVSN Sai Prashanth, Shivanshu Purohit,\nLaria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of the ACL Workshop on Challenges & Perspec-\ntives in Creating Large Language Models.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nIsabel Cachola, Kyle Lo, Arman Cohan, and Daniel S\nWeld. 2020. Tldr: Extreme summarization of scien-\ntific documents. arXiv preprint arXiv:2004.15011.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2924\u20132936, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the AI2 reasoning challenge. CoRR,\nabs/1803.05457.\nLeyang Cui, Yu Wu, Shujie Liu, Yue Zhang, and Ming\nZhou. 2020. Mutual: A dataset for multi-turn dia-\nlogue reasoning. Proceedings of ACL.\nhilfialkaff DataCanary, Lili Jiang, Meg Risdal, and\nNikhil Dandekar. 2017. Quora question pairs.\nMarie-Catherine de Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Investi-\ngating projection in naturally occurring discourse. In\nProceedings of Sinn und Bedeutung 23.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDrop: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In North\nAmerican Chapter of the Association for Computa-\ntional Linguistics.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo,\nEunsol Choi, and Danqi Chen. 2019. MRQA 2019\nshared task: Evaluating generalization in reading\ncomprehension. CoRR, abs/1910.09753.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,\nSid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li,\nKyle McDonell, Niklas Muennighoff, Chris Ociepa,\nJason Phang, Laria Reynolds, Hailey Schoelkopf,\nAviya Skowron, Lintang Sutawika, Eric Tang, An-\nish Thite, Ben Wang, Kevin Wang, and Andy Zou.\n2023. A framework for few-shot language model\nevaluation.\nAndrew Gordon, Zornitsa Kozareva, and Melissa Roem-\nmele. 2012. SemEval-2012 task 7: Choice of plau-\nsible alternatives: An evaluation of commonsense\ncausal reasoning. In *SEM 2012: The First Joint\nConference on Lexical and Computational Seman-\ntics \u2013 Volume 1: Proceedings of the main conference\nand the shared task, and Volume 2: Proceedings of\nthe Sixth International Workshop on Semantic Eval-\nuation (SemEval 2012), pages 394\u2013398, Montr\u00e9al,\nCanada. Association for Computational Linguistics.\nDirk Groeneveld, Pete Walsh, Akshita Bhagia, and\nMichael Schmitz. 2023. Ai2 tango.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. Proceedings of ICLR.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu\nChen.\n2020.\nDeberta:\nDecoding-\nenhanced BERT with disentangled attention. CoRR,\nabs/2006.03654.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2021a. Aligning ai with shared human values. Pro-\nceedings of the International Conference on Learning\nRepresentations (ICLR).\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021b. Measuring massive multitask language un-\nderstanding. Proceedings of ICLR.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021c. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2567\u20132577.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 252\u2013262, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:453\u2013\n466.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv\npreprint arXiv:1704.04683.\nHector J. Levesque, Ernest Davis, and L. Morgenstern.\n2011. The winograd schema challenge. In Interna-\ntional Conference on Principles of Knowledge Rep-\nresentation and Reasoning.\nQuentin Lhoest, Albert Villanova del Moral, Patrick\nvon Platen, Thomas Wolf, Mario \u0160a\u0161ko, Yacine\nJernite, Abhishek Thakur, Lewis Tunstall, Suraj\nPatil, Mariama Drame, Julien Chaumond, Julien Plu,\nJoe Davison, Simon Brandeis, Victor Sanh, Teven\nLe Scao, Kevin Canwen Xu, Nicolas Patry, Steven\nLiu, Angelina McMillan-Major, Philipp Schmid, Syl-\nvain Gugger, Nathan Raw, Sylvain Lesage, Anton\nLozhkov, Matthew Carrigan, Th\u00e9o Matussi\u00e8re, Lean-\ndro von Werra, Lysandre Debut, Stas Bekman, and\nCl\u00e9ment Delangue. 2021. Datasets: A Community\nLibrary for Natural Language Processing. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing: System Demon-\nstrations, pages 175\u2013184. Association for Computa-\ntional Linguistics.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher R\u00e9, Diana Acosta-Navas, Drew A.\nHudson, Eric Zelikman, Esin Durmus, Faisal Lad-\nhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue\nWang, Keshav Santhanam, Laurel Orr, Lucia Zheng,\nMert Yuksekgonul, Mirac Suzgun, Nathan Kim,\nNeel Guha, Niladri Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2022. Holistic eval-\nuation of language models.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. CoRR, abs/2109.07958.\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang,\nYile Wang, and Yue Zhang. 2020. Logiqa: A chal-\nlenge dataset for machine reading comprehension\nwith logical reasoning. CoRR, abs/2007.08124.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nIan Magnusson, Akshita Bhagia, Valentin Hofmann,\nLuca Soldaini, Ananya Harsh Jha, Oyvind Tafjord,\nDustin Schwenk, Evan Pete Walsh, Yanai Elazar,\nKyle Lo, Dirk Groenveld, Iz Beltagy, Hanneneh Ha-\njishirz, Noah A. Smith, Kyle Richardson, and Jesse\nDodge. 2023. PaLOMa: A benchmark for evaluating\nlanguage model fit. technical report.\nStephen Merity. 2019. The wikitext long term depen-\ndency language modeling dataset.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. In Conference on Empirical Methods in Natural\nLanguage Processing.\nJohn Miller, Karl Krauth, Benjamin Recht, and Lud-\nwig Schmidt. 2020. The effect of natural distribu-\ntion shift on question answering models.\nCoRR,\nabs/2004.14444.\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2022a. Noisy channel language\nmodel prompting for few-shot text classification.\nACL 2022.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022b. MetaICL: Learning to learn\nin context. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2791\u20132809, Seattle, United States.\nAssociation for Computational Linguistics.\nMosaicML. 2023. Introducing mpt-7b: A new stan-\ndard for open-source, commercially usable llms. Ac-\ncessed: 2023-05-05.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don\u2019t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797\u20131807. Association for\nComputational Linguistics.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics. As-\nsociation for Computational Linguistics.\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazari-\ndou, Quan Ngoc Pham, Raffaella Bernardi, San-\ndro Pezzelle, Marco Baroni, Gemma Boleda, and\nRaquel Fern\u00e1ndez. 2016. The LAMBADA dataset:\nWord prediction requiring a broad discourse context.\nCoRR, abs/1606.06031.\nAnselmo Pe\u00f1as, Eduard H. Hovy, Pamela Forner, \u00c1l-\nvaro Rodrigo, Richard F. E. Sutcliffe, and Roser\nMorante. 2013. Qa4mre 2011-2013: Overview of\nquestion answering for machine reading evaluation.\nIn Conference and Labs of the Evaluation Forum.\nMohammad Taher Pilehvar and Jose Camacho-Collados.\n2019. WiC: the word-in-context dataset for evalu-\nating context-sensitive meaning representations. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 1267\u20131273,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nOfir Press, Noah A Smith, and Mike Lewis. 2020. Short-\nformer: Better language modeling using shorter in-\nputs. arXiv preprint arXiv:2012.15832.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. OpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. CoRR, abs/1910.10683.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don\u2019t know: Unanswerable questions\nfor squad. CoRR, abs/1806.03822.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\nand Percy Liang. 2016. Squad: 100, 000+ ques-\ntions for machine comprehension of text.\nCoRR,\nabs/1606.05250.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2019. WINOGRANDE: an ad-\nversarial winograd schema challenge at scale. CoRR,\nabs/1907.10641.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof BERT: smaller, faster, cheaper and lighter. CoRR,\nabs/1910.01108.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M. Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea\nSantilli, Thibault F\u00e9vry, Jason Alan Fries, Ryan Tee-\nhan, Stella Biderman, Leo Gao, Tali Bers, Thomas\nWolf, and Alexander M. Rush. 2021.\nMultitask\nprompted training enables zero-shot task generaliza-\ntion. CoRR, abs/2110.08207.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLeBras, and Yejin Choi. 2019.\nSocialiqa: Com-\nmonsense reasoning about social interactions. arXiv\npreprint arXiv:1904.09728.\nTeven Le Scao, Angela Fan, Christopher Akiki,\nElizabeth-Jane Pavlick, Suzana Ili\u2019c, Daniel Hesslow,\nRoman Castagn\u2019e, Alexandra Sasha Luccioni, Franc-\ncois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexan-\nder M. Rush, Stella Rose Biderman, Albert Web-\nson, Pawan Sasanka Ammanamanchi, Thomas Wang,\nBeno\u00eet Sagot, Niklas Muennighoff, Albert Villanova\ndel Moral, Olatunji Ruwase, Rachel Bawden, Stas\nBekman, Angelina McMillan-Major, Iz Beltagy, Huu\nNguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz\nSuarez, Victor Sanh, Hugo Laurenccon, Yacine Jer-\nnite, Julien Launay, Margaret Mitchell, Colin Raf-\nfel, Aaron Gokaslan, Adi Simhi, Aitor Soroa Etx-\nabe, Alham Fikri Aji, Amit Alfassy, Anna Rogers,\nAriel Kreisberg Nitzav, Canwen Xu, Chenghao\nMou, Chris C. Emezue, Christopher Klamm, Colin\nLeong, Daniel Alexander van Strien, David Ife-\noluwa Adelani, Dragomir R. Radev, Eduardo G.\nPonferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar\nNatan, Francesco De Toni, G\u00e9rard Dupont, Germ\u00e1n\nKruszewski, Giada Pistilli, Hady ElSahar, Hamza\nBenyamina, Hieu Tran, Ian Yu, Idris Abdulmu-\nmin, Isaac Johnson, Itziar Gonzalez-Dios, Javier\nde la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu,\nJonathan Chang, Jorg Frohberg, Josephine L. To-\nbing, Joydeep Bhattacharjee, Khalid Almubarak,\nKimbo Chen, Kyle Lo, Leandro von Werra, Leon\nWeber, Long Phan, Loubna Ben Allal, Ludovic Tan-\nguy, Manan Dey, Manuel Romero Mu\u00f1oz, Maraim\nMasoud, Mar\u2019ia Grandury, Mario vSavsko, Max\nHuang, Maximin Coavoux, Mayank Singh, Mike\nTian-Jian Jiang, Minh Chien Vu, Mohammad Ali\nJauhar, Mustafa Ghaleb, Nishant Subramani, Nora\nKassner, Nurulaqilla Khamis, Olivier Nguyen, Omar\nEspejel, Ona de Gibert, Paulo Villegas, Peter Hen-\nderson, Pierre Colombo, Priscilla Amuok, Quentin\nLhoest, Rheza Harliman, Rishi Bommasani, Roberto\nL\u2019opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo,\nSebastian Nagel, Shamik Bose, Shamsuddeen Has-\nsan Muhammad, Shanya Sharma, S. Longpre, So-\nmaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Syd-\nney Zink, Tiago Timponi Torrent, Timo Schick, Tris-\ntan Thrush, Valentin Danchev, Vassilina Nikoulina,\nVeronika Laippala, Violette Lepercq, Vrinda Prabhu,\nZaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin\nHeinzerling, Chenglei Si, Elizabeth Salesky, Sab-\nrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, An-\ndrea Santilli, Antoine Chaffin, Arnaud Stiegler, Deba-\njyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han\nWang, Harshit Pandey, Hendrik Strobelt, Jason Alan\nFries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai-\nful Bari, Maged S. Al-shaibani, Matteo Manica, Ni-\nhal V. Nayak, Ryan Teehan, Samuel Albanie, Sheng\nShen, Srulik Ben-David, Stephen H. Bach, Taewoon\nKim, Tali Bers, Thibault F\u00e9vry, Trishala Neeraj, Ur-\nmish Thakker, Vikas Raunak, Xiang Tang, Zheng Xin\nYong, Zhiqing Sun, Shaked Brody, Y Uri, Hadar\nTojarieh, Adam Roberts, Hyung Won Chung, Jae-\nsung Tae, Jason Phang, Ofir Press, Conglong Li,\nDeepak Narayanan, Hatim Bourfoune, Jared Casper,\nJeff Rasley, Max Ryabinin, Mayank Mishra, Minjia\nZhang, Mohammad Shoeybi, Myriam Peyrounette,\nNicolas Patry, Nouamane Tazi, Omar Sanseviero,\nPatrick von Platen, Pierre Cornette, Pierre Franc-\ncois Lavall\u2019ee, R\u00e9mi Lacroix, Samyam Rajbhan-\ndari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Re-\nquena, Suraj Patil, Tim Dettmers, Ahmed Baruwa,\nAmanpreet Singh, Anastasia Cheveleva, Anne-Laure\nLigozat, Arjun Subramonian, Aur\u2019elie N\u2019ev\u2019eol,\nCharles Lovering, Daniel H Garrette, Deepak R.\nTunuguntla, Ehud Reiter, Ekaterina Taktasheva, Eka-\nterina Voloshina, Eli Bogdanov, Genta Indra Winata,\nHailey Schoelkopf, Jan-Christoph Kalo, Jekate-\nrina Novikova, Jessica Zosa Forde, Jordan Clive,\nJungo Kasai, Ken Kawamura, Liam Hazan, Ma-\nrine Carpuat, Miruna Clinciu, Najoung Kim, New-\nton Cheng, Oleg Serikov, Omer Antverg, Oskar\nvan der Wal, Rui Zhang, Ruochen Zhang, Sebastian\nGehrmann, S. Osher Pais, Tatiana Shavrina, Thomas\nScialom, Tian Yun, Tomasz Limisiewicz, Verena\nRieser, Vitaly Protasov, Vladislav Mikhailov, Yada\nPruksachatkun, Yonatan Belinkov, Zachary Bam-\nberger, Zdenvek Kasner, Alice Rueda, Amanda Pes-\ntana, Amir Feizpour, Ammar Khan, Amy Faranak,\nAnanda Santa Rosa Santos, Anthony Hevia, Antig-\nona Unldreaj, Arash Aghagol, Arezoo Abdollahi,\nAycha Tammour, Azadeh HajiHosseini, Bahareh\nBehroozi, Benjamin Olusola Ajibade, Bharat Kumar\nSaxena, Carlos Mu\u00f1oz Ferrandis, Danish Contrac-\ntor, David M. Lansky, Davis David, Douwe Kiela,\nDuong Anh Nguyen, Edward Tan, Emily Baylor, Ez-\ninwanne Ozoani, Fatim T Mirza, Frankline Onon-\niwu, Habib Rezanejad, H.A. Jones, Indrani Bhat-\ntacharya, Irene Solaiman, Irina Sedenko, Isar Ne-\njadgholi, Jan Passmore, Joshua Seltzer, Julio Bonis\nSanz, Karen Fort, L\u00edvia Macedo Dutra, Mairon Sama-\ngaio, Maraim Elbadri, Margot Mieskes, Marissa Ger-\nchick, Martha Akinlolu, Michael McKenna, Mike\nQiu, M. K. K. Ghauri, Mykola Burynok, Nafis\nAbrar, Nazneen Rajani, Nour Elkott, Nourhan Fahmy,\nOlanrewaju Modupe Samuel, Ran An, R. P. Kro-\nmann, Ryan Hao, Samira Alizadeh, Sarmad Shubber,\nSilas L. Wang, Sourav Roy, Sylvain Viguier, Thanh-\nCong Le, Tobi Oyebade, Trieu Nguyen Hai Le,\nYoyo Yang, Zachary Kyle Nguyen, Abhinav Ramesh\nKashyap, Alfredo Palasciano, Alison Callahan, An-\nima Shukla, Antonio Miranda-Escalada, Ayush Ku-\nmar Singh, Benjamin Beilharz, Bo Wang, Caio\nMatheus Fonseca de Brito, Chenxi Zhou, Chirag\nJain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u2019on\nPerin\u2019an, Daniel Molano, Dian Yu, Enrique Man-\njavacas, Fabio Barth, Florian Fuhrimann, Gabriel\nAltay, Giyaseddin Bayrak, Gully A. Burns, Helena U.\nVrabec, Iman I.B. Bello, Isha Dash, Ji Soo Kang,\nJohn Giorgi, Jonas Golde, Jose David Posada, Karthi\nSivaraman, Lokesh Bulchandani, Lu Liu, Luisa\nShinzato, Madeleine Hahn de Bykhovetz, Maiko\nTakeuchi, Marc P\u00e0mies, Mar\u00eda Andrea Castillo, Mar-\nianna Nezhurina, Mario Sanger, Matthias Samwald,\nMichael Cullan, Michael Weinberg, M Wolf, Mina\nMihaljcic, Minna Liu, Moritz Freidank, Myung-\nsun Kang, Natasha Seelam, Nathan Dahlberg,\nNicholas Michio Broad, Nikolaus Muellner, Pascale\nFung, Patricia Haller, R. Chandrasekhar, R. Eisen-\nberg, Robert Martin, Rodrigo L. Canalli, Rosaline\nSu, Ruisi Su, Samuel Cahyawijaya, Samuele Garda,\nShlok S Deshmukh, Shubhanshu Mishra, Sid Ki-\nblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Ku-\nmar, Stefan Schweter, Sushil Pratap Bharati, T. A.\nLaud, Th\u2019eo Gigant, Tomoya Kainuma, Wojciech\nKusa, Yanis Labrak, Yashasvi Bajaj, Y. Venkatra-\nman, Yifan Xu, Ying Xu, Yun chao Xu, Zhee Xao\nTan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes\nBelkada, and Thomas Wolf. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\nArXiv, abs/2211.05100.\nThomas Scialom, Tuhin Chakrabarty, and Smaranda\nMuresan. 2022.\nFine-tuned language models are\ncontinual learners.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631\u20131642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2018. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. arXiv preprint arXiv:1811.00937.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a.\nLlama:\nOpen and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nDavid Vilares and Carlos G\u00f3mez-Rodr\u00edguez. 2019.\nHEAD-QA: A healthcare dataset for complex reason-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n960\u2013966, Florence, Italy. Association for Computa-\ntional Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. ArXiv, abs/1905.00537.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGlue: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Black-\nboxNLP@EMNLP.\nBen Wang and Aran Komatsuzaki. 2021.\nGPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625\u2013641.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.\nCrowdsourcing multiple choice science questions.\nArXiv, abs/1707.06209.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112\u20131122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface\u2019s transformers:\nState-of-the-art natural language processing. CoRR,\nabs/1910.03771.\nLinting Xue, Noah Constant, Adam Roberts, Mihir\nKale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua,\nand Colin Raffel. 2020. mt5: A massively multi-\nlingual pre-trained text-to-text transformer. CoRR,\nabs/2010.11934.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019.\nHellaswag: Can\na machine really finish your sentence?\nCoRR,\nabs/1905.07830.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nLucia Zheng, Neel Guha, Brandon R Anderson, Peter\nHenderson, and Daniel E Ho. 2021. When does pre-\ntraining help? assessing self-supervised learning for\nlaw and the casehold dataset of 53,000+ legal hold-\nings. In Proceedings of the eighteenth international\nconference on artificial intelligence and law, pages\n159\u2013168.\nA\nAppendix\nARC Challenge\nARC Easy\nCoLA\nCOPA\nHEAD-QA EN\nHellaSwag\nLogiQA\nMNLI\nMRPC\nOpenBookQA\nPIQA\nQNLI\nQQP\nRTE\nSciQ\nSST\nWINOGRANDE\nARC Challenge\n1.000\n0.932\n0.606\n0.811\n0.800\n0.826\n0.587\n0.716\n0.642\n0.939\n0.614\n0.852\n0.696\n0.796\n0.543\n0.577\n0.747\nARC Easy\n0.932\n1.000\n0.650\n0.896\n0.860\n0.862\n0.492\n0.649\n0.652\n0.914\n0.713\n0.744\n0.585\n0.693\n0.603\n0.502\n0.805\nCoLA\n0.606\n0.650\n1.000\n0.616\n0.440\n0.381\n0.618\n0.622\n0.864\n0.600\n0.244\n0.525\n0.433\n0.503\n0.086\n0.590\n0.475\nCOPA\n0.811\n0.896\n0.616\n1.000\n0.727\n0.822\n0.442\n0.482\n0.552\n0.765\n0.641\n0.617\n0.475\n0.535\n0.454\n0.429\n0.784\nHEAD-QA EN\n0.800\n0.860\n0.440\n0.727\n1.000\n0.778\n0.295\n0.438\n0.501\n0.804\n0.811\n0.531\n0.336\n0.525\n0.691\n0.249\n0.751\nHellaSwag\n0.826\n0.862\n0.381\n0.822\n0.778\n1.000\n0.238\n0.532\n0.422\n0.772\n0.825\n0.666\n0.499\n0.608\n0.679\n0.398\n0.892\nLogiQA\n0.587\n0.492\n0.618\n0.442\n0.295\n0.238\n1.000\n0.570\n0.634\n0.515\n0.085\n0.482\n0.506\n0.516\n\u22120.040\n0.483\n0.204\nMNLI\n0.716\n0.649\n0.622\n0.482\n0.438\n0.532\n0.570\n1.000\n0.675\n0.773\n0.311\n0.835\n0.822\n0.788\n0.222\n0.712\n0.420\nMRPC\n0.642\n0.652\n0.864\n0.552\n0.501\n0.422\n0.634\n0.675\n1.000\n0.648\n0.240\n0.586\n0.540\n0.556\n0.137\n0.685\n0.455\nOpenBookQA\n0.939\n0.914\n0.600\n0.765\n0.804\n0.772\n0.515\n0.773\n0.648\n1.000\n0.581\n0.874\n0.769\n0.828\n0.520\n0.608\n0.726\nPIQA\n0.614\n0.713\n0.244\n0.641\n0.811\n0.825\n0.085\n0.311\n0.240\n0.581\n1.000\n0.417\n0.235\n0.308\n0.641\n0.160\n0.774\nQNLI\n0.852\n0.744\n0.525\n0.617\n0.531\n0.666\n0.482\n0.835\n0.586\n0.874\n0.417\n1.000\n0.897\n0.828\n0.381\n0.750\n0.562\nQQP\n0.696\n0.585\n0.433\n0.475\n0.336\n0.499\n0.506\n0.822\n0.540\n0.769\n0.235\n0.897\n1.000\n0.855\n0.251\n0.801\n0.413\nRTE\n0.796\n0.693\n0.503\n0.535\n0.525\n0.608\n0.516\n0.788\n0.556\n0.828\n0.308\n0.828\n0.855\n1.000\n0.403\n0.719\n0.523\nSciQ\n0.543\n0.603\n0.086\n0.454\n0.691\n0.679\n\u22120.040\n0.222\n0.137\n0.520\n0.641\n0.381\n0.251\n0.403\n1.000\n0.089\n0.583\nSST\n0.577\n0.502\n0.590\n0.429\n0.249\n0.398\n0.483\n0.712\n0.685\n0.608\n0.160\n0.750\n0.801\n0.719\n0.089\n1.000\n0.311\nWINOGRANDE\n0.747\n0.805\n0.475\n0.784\n0.751\n0.892\n0.204\n0.420\n0.455\n0.726\n0.774\n0.562\n0.413\n0.523\n0.583\n0.311\n1.000\nTable 2: Spearman correlations between models when ranked in order of performance on 17 common datasets.\nModels\nARC Challenge\nARC Easy\nCoLA\nCOPA\nHEAD-QA\nHellaSwag\nLogiQA\nMNLI\nMRPC\nOpenBookQA\nPIQA\nQNLI\nQQP\nRTE\nSciQ\nSST\nWINOGRANDE\nZero-shot Ranked Classification\nBLOOM 560M\n\u22124.1\n60.4\n9.9\n24.0\n47.0\n42.7\n6.3\n6.4\n36.8\n23.2\n30.6\n\u22120.5\n\u221224.8\n5.4\n190.0\n30.7\n2.4\nBLOOM 1B1\n3.1\n79.3\n38.3\n32.0\n51.5\n63.7\n3.2\n6.6\n36.8\n24.8\n36.1\n\u22121.0\n\u221226.4\n5.4\n198.0\n23.6\n9.4\nBLOOM 1B7\n5.5\n90.1\n38.3\n38.0\n64.5\n84.0\n4.5\n6.5\n36.8\n28.8\n38.1\n\u22121.0\n\u221226.4\n6.9\n198.8\n7.3\n14.0\nBLOOM 3B\n15.0\n109.6\n37.1\n40.0\n67.0\n108.8\n10.0\n6.7\n36.8\n40.8\n41.7\n\u22121.1\n\u221226.4\n12.6\n223.2\n2.1\n17.0\nBLOOM 7B1\n27.6\n127.6\n32.3\n32.0\n77.6\n136.2\n4.5\n7.0\n36.8\n58.4\n47.2\n2.4\n\u221226.3\n8.3\n231.2\n\u22121.6\n28.7\nGPT-J 6B\n43.0\n140.1\n35.4\n54.0\n89.6\n163.6\n17.4\n12.2\n36.8\n68.0\n52.2\n2.9\n\u221223.5\n9.0\n238.4\n\u22121.4\n28.2\nGPT-2\n\u22128.2\n52.7\n33.1\n34.0\n30.2\n23.7\n0.2\n1.2\n13.2\n8.8\n23.6\n0.3\n\u221225.4\n6.1\n167.6\n8.0\n3.2\nGPT-2 Medium\n\u22121.4\n70.7\n38.3\n30.0\n39.7\n57.2\n8.8\n5.5\n36.8\n20.8\n35.1\n\u22121.1\n\u221226.3\n5.4\n168.4\n11.9\n6.2\nGPT-2 Large\n\u22122.4\n83.2\n37.9\n36.0\n41.7\n77.4\n13.1\n7.7\n30.4\n18.4\n39.3\n\u22121.3\n\u221225.6\n5.4\n182.0\n\u22121.1\n10.7\nGPT-2 XL\n11.6\n103.7\n38.3\n40.0\n47.9\n99.2\n5.7\n9.6\n26.5\n39.2\n41.8\n2.9\n\u221225.6\n4.7\n193.2\n\u22121.8\n16.7\nOPT 125M\n\u221210.9\n54.5\n38.3\n26.0\n30.4\n25.2\n14.9\n3.5\n36.8\n8.0\n23.8\n\u22121.2\n\u221226.5\n\u22120.4\n186.8\n6.2\n0.6\nOPT 350M\n\u22126.8\n55.9\n38.3\n30.0\n34.4\n44.1\n13.1\n3.3\n36.8\n14.4\n29.9\n\u22120.9\n\u221226.4\n4.0\n178.8\n23.4\n5.4\nOPT 1.3B\n9.6\n96.3\n21.6\n46.0\n48.4\n113.7\n0.8\n7.5\n32.8\n32.0\n45.3\n2.8\n\u221221.7\n3.2\n198.0\n63.5\n19.2\nOPT 2.7B\n18.8\n117.2\n\u221230.2\n46.0\n56.8\n138.8\n5.7\n6.5\n35.8\n53.6\n49.6\n2.4\n\u221213.4\n9.0\n206.0\n3.4\n22.2\nOPT 6.7B\n33.8\n136.4\n\u22125.3\n58.0\n61.4\n168.4\n11.8\n\u22121.6\n24.5\n56.8\n54.2\n1.8\n\u221221.6\n10.5\n226.4\n52.8\n30.5\nT5 Small\n\u221214.3\n17.8\n31.0\n\u22124.0\n16.3\n6.2\n10.0\n7.8\n37.3\n11.2\n12.1\n\u22120.6\n\u221220.9\n\u22126.1\n143.2\n41.7\n\u22120.7\nT5 Base\n\u221211.3\n18.4\n\u221237.3\n\u22124.0\n30.0\n22.3\n12.4\n77.3\n\u221236.8\n27.2\n11.3\n0.9\n26.4\n30.7\n172.4\n14.7\n0.2\nT5 Large\n0.3\n22.2\n\u221238.3\n2.0\n34.8\n55.8\n1.4\n5.4\n\u221236.8\n30.4\n11.3\n1.1\n26.4\n10.5\n224.0\n0.5\n13.0\nT5 3B\n9.9\n74.6\n\u221221.2\n6.0\n43.1\n91.2\n4.5\n61.8\n35.3\n28.0\n35.9\n7.4\n35.3\n40.8\n255.2\n77.5\n1.3\nT5 11B\n27.6\n72.1\n\u221238.3\n18.0\n47.5\n109.0\n10.0\n43.6\n\u221233.8\n31.2\n43.7\n14.0\n26.2\n29.2\n186.8\n75.5\n5.1\nFinetuned MLMs\nBERT Base Cased\n33.1\n116.2\n68.4\n38.0\n54.3\n53.8\n26.0\n152.0\n72.5\n107.2\n33.2\n80.4\n81.4\n36.5\n184.8\n86.2\n3.9\nBERT Large Cased\n38.9\n135.5\n70.3\n36.0\n57.0\n82.2\n29.6\n159.0\n73.5\n132.8\n40.4\n83.3\n82.0\n50.2\n198.0\n87.8\n28.5\nBERT Base Uncased\n34.8\n122.9\n67.8\n40.0\n57.0\n60.8\n33.3\n152.9\n73.0\n132.0\n30.7\n82.6\n82.0\n37.9\n182.0\n87.4\n4.3\nBERT Large Uncased\n38.6\n136.5\n68.9\n48.0\n54.3\n91.8\n37.0\n159.0\n74.5\n134.4\n18.3\n83.2\n82.5\n51.6\n194.0\n89.0\n3.9\nDeBERTa-v3 Small\n73.4\n163.8\n73.3\n58.0\n68.3\n198.2\n26.6\n163.4\n82.4\n140.0\n58.3\n83.7\n83.8\n54.5\n223.6\n89.0\n32.8\nDeBERTa-v3 Base\n118.4\n200.0\n75.1\n78.0\n87.1\n252.8\n51.2\n170.2\n78.9\n184.0\n64.0\n88.1\n84.7\n71.8\n238.0\n92.0\n65.1\nDeBERTa-v3 Large\n178.8\n246.5\n77.2\n96.0\n107.1\n276.1\n72.7\n172.9\n82.4\n232.8\n76.0\n90.3\n85.6\n83.4\n254.8\n92.0\n75.7\nDeBERTa-v2 XLarge\n34.1\n204.4\n71.4\n48.0\n67.4\n269.4\n1.4\n171.8\n77.9\n200.0\n60.0\n86.8\n83.9\n74.0\n233.6\n93.8\n73.3\nRoBERTa Base\n34.8\n115.8\n67.2\n44.0\n55.0\n151.1\n25.3\n162.3\n79.4\n113.6\n39.0\n83.4\n83.0\n59.6\n158.4\n90.1\n31.2\nRoBERTa Large\n75.8\n158.4\n74.1\n80.0\n43.7\n233.5\n31.5\n170.6\n80.4\n109.6\n25.4\n87.9\n83.2\n76.9\n196.0\n93.3\n54.1\nTable 3: Improvement relative in percentage compared to the random baseline on 17 common tasks\n"
  },
  {
    "title": "ProTIP: Progressive Tool Retrieval Improves Planning",
    "link": "https://arxiv.org/pdf/2312.10332.pdf",
    "upvote": "7",
    "text": "ProTIP: Progressive Tool Retrieval Improves Planning\nRaviteja Anantha*, Bortik Bandyopadhyay*, Anirudh Kashi, Sayantan Mahinder,\nAndrew W Hill, Srinivas Chappidi\nApple\nAbstract\nLarge language models (LLMs) are increas-\ningly employed for complex multi-step plan-\nning tasks, where the tool retrieval (TR) step is\ncrucial for achieving successful outcomes. Two\nprevalent approaches for TR are single-step re-\ntrieval, which utilizes the complete query, and\nsequential retrieval using task decomposition\n(TD), where a full query is segmented into\ndiscrete atomic subtasks. While single-step\nretrieval lacks the flexibility to handle \u201cinter-\ntool dependency,\" the TD approach necessi-\ntates maintaining \u201csubtask-tool atomicity align-\nment,\" as the toolbox can evolve dynamically.\nTo address these limitations, we introduce the\nProgressive Tool retrieval to Improve Planning\n(ProTIP) framework. ProTIP is a lightweight,\ncontrastive learning-based framework that im-\nplicitly performs TD without the explicit re-\nquirement of subtask labels, while simultane-\nously maintaining subtask-tool atomicity. On\nthe ToolBench dataset, ProTIP outperforms the\nChatGPT task decomposition-based approach\nby a remarkable margin, achieving a 24% im-\nprovement in Recall@K=10 for TR and a 41%\nenhancement in tool accuracy for plan genera-\ntion.\n1\nIntroduction\nLarge language models (LLMs) (Brown et al.,\n2020; Chowdhery et al., 2022; Ouyang et al.,\n2022; Zhang et al., 2022; Zeng et al., 2022; Ope-\nnAI, 2023b; Touvron et al., 2023; Bubeck et al.,\n2023; Thoppilan et al., 2022) have witnessed re-\nmarkable advancements in recent years, driven\nby efficient pre-training on massive text datasets\nand the application of sophisticated algorithms\nlike reinforcement learning from human feedback\n(RLHF) (Ouyang et al., 2022) to better align these\nmodels with human preferences. This progress has\nunlocked emergent capabilities (Wei et al., 2022a)\n*Equal contributions.\nin LLMs, which can be further refined to enhance\ntheir instruction-following abilities (Taori et al.,\n2023; Chiang et al., 2023). Additionally, LLMs\nhave demonstrated the potential for in-context\nlearning (Brown et al., 2020; Xie et al., 2021; Min\net al., 2022) and chain-of-thought prompting (Wei\net al., 2022b; Kojima et al., 2022; Wang et al.,\n2022), expanding their applicability across a wide\nspectrum of application domains.\nHarnessing the power of LLMs as language un-\nderstanding agent (Shen et al., 2023a) to tackle\ncomplex tasks has emerged as a burgeoning re-\nsearch area. This endeavor presents a challenge\ndue to the inherent complexity of multi-step plan-\nning (Huang et al., 2022; Ahn et al., 2022; Singh\net al., 2022). To address this challenge, we em-\nploy a flexible planning framework that seamlessly\nintegrates an LLM with an external toolbox contain-\ning application specific atomic actions. The LLM\nplanner bridges the gap between natural language\ninstructions and executable actions by effectively\nselecting appropriate APIs/tools from a curated\nlist presented in the LLM prompt. These tools\nare retrieved using specialized techniques from the\nexternal toolbox (Schick et al., 2023; Qin et al.,\n2023a; Patil et al., 2023; Qin et al., 2023b; Shen\net al., 2023a). The terms tool and API are used\ninterchangeably throughout this paper.\nWithin multi-step planning framework with an\nexternal toolbox, the tool retrieval (TR) step plays\na crucial role in determining the overall planner\u2019s\nperformance. The TR step can be implemented\neither as a single-step process utilizing the entire\nquery or as an iterative approach that decomposes\nthe query into individual atomic subtasks (Khot\net al., 2022; Wu et al., 2023). The single-step TR\napproach, however, is unable to handle \u201cinter-tool\ndependency\" in multi-step execution scenarios.\nThis limitation becomes evident, for instance,\nwhen selecting between tool-A and tool-B, where\nthe choice depends on the successful execution\narXiv:2312.10332v1  [cs.IR]  16 Dec 2023\nof a previously chosen tool.\nIn contrast, the\nTD-based TR approach necessitates maintaining\nthe alignment between the exact subtask in\nquestion and the appropriate tool to be used\nfrom the employed toolbox version, thus creating\na \u201csubtask-tool atomicity alignment,\" problem\nwhen training the planner. This dependency often\nrequires either frequent fine-tuning of lightweight\nTD models or the utilization of an LLM, such as\nChatGPT (OpenAI, 2023a), for TD. Furthermore,\nboth these approaches operate within the text space,\nmaking them susceptible to various issues such\nas \u201cout of vocabulary\" tokens, which can hinder\naccurate semantic representation of the subtasks\nand ultimately impact the planner\u2019s performance.\nTo overcome these limitations, we introduce the\nProgressive Tool retrieval to Improve Planning\n(ProTIP) framework. Our TR strategy draws inspi-\nration from advancements in the word embedding\nliterature, where prior works (Mikolov et al., 2013a;\nPennington et al., 2014; Levy and Goldberg, 2014)\nhave shown the effectiveness of representing se-\nmantic relationships between words by embedding\nthem in a vector space. Extending this concept\nto complex queries and tools, we leverage task-\nspecific fine-tuning to achieve our progressive TR\nrequirements.\nProTIP initially encodes the given query and tool\ndescriptions to minimize the Euclidean distance\nbetween relevant tools corresponding to the first\nsubtask and the query in a semantic space, without\nexplicitly performing task decomposition. Subse-\nquently, the ProTIP module iteratively transforms\nthe query embedding by subtracting previously re-\ntrieved tool description embedding from the query\nembedding. The resultant embedding from this\niterative subtraction aligns more closely in seman-\ntic space with a natural language query formed\nby eliminating previously executed subtasks from\nthe full query, while focusing on the next most\nimportant subtask to be executed out of the remain-\ning ones. ProTIP is fine-tuned using contrastive\nloss to learn embeddings with above-mentioned\ncharacteristics, more details in section 3.1. As a\nconsequence, ProTIP provides flexibility by en-\nabling incremental TR, while incorporating execu-\ntion history (e.g., the previously selected tool and\nexecution result) without the overhead of maintain-\ning atomicity for the TD. The contributions of this\nwork are as follows:\n\u2022 We introduce ProTIP, a novel progressive TR\nframework, that efficiently performs TR for\ncomplex requests involving inter-subtask de-\npendencies, while factoring in the execution\nhistory, without requiring explicit TD.\n\u2022 We comprehensively compare various TR\nmethods and their impact on LLM-powered\nplanning agents using the public ToolBench\ndataset (Qin et al., 2023b).\n\u2022 To the best of our knowledge, we are the first\nto establish that lightweight (non-LLM) fine-\ntuning based tool retrieval approaches, such as\nProTIP, can outperform state-of-the-art LLM-\naugmented approaches, such as ChatGPT-\nbased TD for TR.\n2\nData\nWe evaluate the efficacy of ProTIP-based LLM-\nPlanner in generating step-by-step plans using the\nToolBench (Qin et al., 2023b) dataset, one of the\nmost extensive instruction-tuning dataset for tool\nutilization, encompassing 16,000 tools and 49 cat-\negories. ToolBench includes 27,000 complex re-\nquests, each of which may require the use of one\nor more APIs/Tools to complete subtasks in a spe-\ncific order. Each request is accompanied by tool\nand plan labels, which represent a series of more\ngranular step-by-step instructions.\nFigure 1 illustrates the distribution of the number\nof tools required for each query, providing insights\ninto the complexity of requests within the dataset.\nThe analysis reveals that the maximum number of\nsubtasks in a query is 6. This information is utilized\nto establish the upper bound for top-k in TR and\nplanner experiments, as detailed in section 3.2.\nFigure 2 shows the distribution of input token\nlength to the planner in train and test sets. Notably,\n12.25% (97 data points) of the test set and 12.30%\n(9,070 data points) of the training set exceed the\ncontext window size of 2048. This substantial pro-\nportion of lengthy inputs is expected to cause trun-\ncation, potentially hindering the model\u2019s ability to\nachieve optimal performance.\nFigure 3 depicts the distribution of ground truth\ntool IDs in the dataset. Notably, a significant pro-\nportion of tool IDs fall within the range of 0 to\n2000 for both the training and test sets. This imbal-\nance in tool representation could potentially bias\nthe model towards these more frequently appearing\ntools.\nFigure 1: Distribution of requests in the ToolBench training (left) and test (right) sets according to the number of\nsubtasks involved in each request.\nFigure 2: Distributions of input token lengths for the planner in training (left) and test (right) data. The input consists\nof the query, top-k retrieved tools, planner-specific prompts, and execution history.\nFigure 3: Frequency distributions of ground truth tools in training (left) and test (right) sets. Tool names have been\nconverted to IDs for visualization clarity.\nThe ToolBench dataset was generated using\nChatGPT. As is typical with LLM-generated data\nwithout access to additional knowledge, ToolBench\nis susceptible to hallucinations (Bang et al., 2023).\nAn example of this can be seen in figure 4, where\nthe synthetic dataset contains the hallucinated tool\nFigure 4: An example of tool hallucination in the Tool-\nBench dataset.\ninvalid_hallucination_function_name at second\nand third steps. To address this issue, we removed\nrequests with imaginary tool annotations, which\nare tools not included in the toolbox. Additionally,\nwe manually reviewed and corrected remaining\nincorrectly extracted tools resulting from grammat-\nical errors in the generated labels. Following the\ndata cleaning process, the revised training and test\ndatasets comprised of 25,489 and 274 complex\nrequests, respectively. Using this dataset, we addi-\ntionally performed TD using ChatGPT as described\nin section 3.1. After filtering out outliers with in-\nvalid tasks, we end up with a dataset size of 25,124\ntraining data points and 268 test data points, which\nwe use for all our experiments. The average num-\nber of subtasks in our final datasets is 2.934 (Std\nDev. = 1.417) for the training set and 2.955 (Std\nDev. = 1.39) for the test set.\n3\nMethodology\nTo evaluate the ProTIP framework for TR and plan-\nning tasks, we use both text-based and vector-based\nsearch baselines, as well as a strong baseline of TD-\nbased planning, on the train and test splits from the\nToolBench dataset after the preprocessing step to\nremove bad-quality data, as described in Section 2.\nFigure 5 illustrates the our envisioned end-to-end\nflow of ProTIP-based step-by-step planning.\n3.1\nTool Retrieval\nTool Retrieval (TR) aims to identify the top-k most\nsuitable tools to empower the planner to effectively\nexecute all subtasks within a given complex request.\nThe toolbox T = {(t1, d1), (t2, d2), ..., (tn, dn)}\nencompasses a collection of diverse tools ti with\npredefined functionalities, captured in their tool\ndescriptions di. A primary objective of TR is to ex-\ntract subtasks from a complex query that align with\nthe predefined functionalities of the tools, a concept\nwe refer to as subtask-tool atomicity alignment and\nsubsequently retrieve those tools.\nWhen employing vector-based retrieval ap-\nproaches, the toolbox T is typically represented\nas a vector database. An encoder Ew(.) parameter-\nized on w produces tool description embeddings,\nEw(di), which are then stored. Either the same or\na query-specific encoder maps the complex query\nq into an embedding Ew(q). A similarity mea-\nsure, such as cosine similarity, between Ew(di)\nand Ew(q) is used to retrieve the top-k tools.\nThis study utilizes a comprehensive suite of re-\ntrieval methods, encompassing both pretrained and\nfine-tuned approaches, including our proposed Pro-\nTIP method, to evaluate the effectiveness of differ-\nent TR strategies using the Recall@k metric.\nBM25\nThe text-based retrieval baseline em-\nployed in this study is BM25 (Robertson et al.,\n1995). To retrieve the top-k tools, we utilize the\nfull complex query q to compute BM25 scores for\neach tool description di.\nSemantic Search\nFor vector-based search, we\nutilize GTR-T5-XL (Ni et al., 2021) as the encoder\nfor both query q and tool descriptions di. The\ncosine similarity measure cos_sim(q, di) is em-\nployed to select the top-K most relevant tools.\nTask Decomposition based TR\nTask Decompo-\nsition (TD) (Khot et al., 2022; Wu et al., 2023)\nbreaks down a complex query q into a set of sub-\nqueries {q1, q2, ..., q\u03c4}, where \u03c4 denotes the num-\nber of subtasks embedded within q, and each qi\nrepresents a subquery corresponding to the ith sub-\ntask of q.\nThe evaluation of TD-based retrieval involves\nemploying both BM25 and semantic search us-\ning GTR-T5-XL models. For each qi from TD,\nwe perform parallel retrieval using BM25 for text-\nbased retrieval and GTR-T5-XL for vector-based\nretrieval. This results in the identification of top-k\ntools specific to each qi. Subsequently, we employ\nan interleaving strategy to determine the final top-\nk tools for q. This approach of interleaving tools\nwith TD serves as a straightforward yet powerful\nFigure 5: End-to-end processing of complex queries with ProTIP-based planning.\nbaseline. We opt for tool interleaving instead of\ndirectly utilizing all subqueries simultaneously, as\nthe top-k tools obtained using the subquery set may\nnot align with the desired top-k tools, where each\nsubtask effectively covers relevant tools. We use\nthe ChatGPT (OpenAI, 2023a) model to generate\nTD rewrites.\nProTIP\nWe propose ProTIP, a progressive tool\nretrieval strategy, where top-k tools specific to each\nsubtask are iteratively retrieved conditioned on ex-\necution history while retaining the subtask order.\nTD-based retrieval generates subtasks without fac-\ntoring in the execution history. While TD-based re-\ntrieval can be adapted to leverage execution history,\nit still requires either an expensive pretrained LLM\npowerful enough to generate high-quality rewrites,\nor explicit task decomposition labels to fine-tune\na lightweight model to generate rewrites. In addi-\ntion, the TD labels should also ensure subtask-tool\natomicity alignment is maintained.\nFigure 6: ProTIP\u2019s implicit learning mechanism for han-\ndling complex queries. Initial retrieval selects the tool\nrelevant to the first subtask. Subsequently, the execution\nhistory, encompassing the tool description and query, is\nutilized to produce a resultant embedding E(q\u2032\n2). This\nembedding is specifically crafted to align with E(q2),\nwhich represents the second subtask, without the need\nfor the subquery label q2.\nProTIP is a lightweight retrieval model which\ndoes not require explicit labels. Instead of relying\non intermediate text representation, ProTIP directly\noperates in the vector space by transforming in-\nput embeddings to eliminate subtasks which are\nalready addressed.\nGiven a complex query q comprised of n sub-\ntasks, let q1...qn denote queries that only capture\neach subtask from 1...n.\nWe use BERT-base-\nuncased* model as the encoder for both q and\ntool descriptions di, represented by Ew(.). For\neach training batch of size b, we fine-tune Ew(.)\nto always choose the ground-truth tool tpos1 corre-\nsponding to subtask-1 by minimizing the distance\nbetween dpos1 and q, while maximizing the dis-\ntance between q and a set of randomly drawn nega-\ntive examples, Tneg = {tneg1, tneg2, ..., tnegb\u22121},\nfrom irrelevant tools.\nFor subsequent retrieval\nsteps, starting with subtask-2, we iteratively sub-\ntract Ew(tpos1) to Ew(tposi) from Ew(q) to arrive\nat an embedding that approximates a query q\n\u2032 that\nonly represents subtasks from i + 1 to n. This\noperation directly results in implicit learning of\ntask decomposition while maintaining subtask-tool\natomicity without the need for explicit labels, as\ndepicted in Figure 6.\nWe use contrastive loss (Hadsell et al., 2006) to\nfine-tune our retrieval which is suited for metric-\nbased learning. Given input pair with I1 and I2\ninputs, contrastive loss encourages similar exam-\nples to be close, and dissimilar ones to have higher\ndistance of at least margin m from each other. We\ndefine input I1 for query embedding as\nI1 =\n(\nEw(q), for initial retrieval.\nEw(q) \u2212 P\n1\u2264i<n(Ew(di)), otherwise;\n(1)\n*https://huggingface.co/\nbert-base-uncased\nwhere P represents element-wise vector sum. We\ndefine tool description embedding* I2 as\nI2 = Ew(dj+1),\n(2)\nwhere j \u2265 0.\nThe margin-based contrastive loss function is\ndefined as\nL = 1\n2lD2 + 1\n2(1 \u2212 l){max(0, m \u2212 D)}2, (3)\nwhere l is a binary label which indicates whether\nthe input pair consisting of the query I1 and tool\ndescription I2 embeddings is a positive (l = 1) or\nnegative (l = 0) pair, m > 0 is the margin distance\nfor dissimilar pairs and we use 0.3. D is a distance\nmeasure of choice and we use L2 norm between I1\nand I2. Analogous to TD-based TR, we utilize a\ntool interleaving strategy to identify the final top-K\ntools for Recall@k evaluation.\n3.2\nProgressive Planning\nTo retrieve tools for the progressive planning task,\nwe employ the tool retrieval (TR) strategies pro-\nposed in Section 3.1. We then perform supervised\nfine-tuning of the OpenLLaMA-7B-v2 model (Tou-\nvron et al., 2023; Geng and Liu, 2023; Computer,\n2023), leveraging the HuggingFace Transformer\nlibrary (Wolf et al., 2019). The model operates\nwith a context window size of 2048 tokens. The\nprompt provided to the model consists of a fixed\ninstruction, the complex request, and optional in-\nformation such as the list of top-k API candidates\n(along with their metadata) and the execution his-\ntory. This combination generates multiple distinct\nprompt variations.* In essence, our goal is to pre-\ndict the next API to be executed in a multi-step plan\ngiven an input prompt containing the instruction,\nrequest, API candidates, and history. This requires\nunrolling the original data to form a sequence of\nprompts corresponding to each step in the plan.\nEach interaction in the original data encom-\npasses a natural language description of the full\nquery. Additionally, each interaction comprises a\ntotal of p steps labeled assistant and f steps labeled\nfunction, along with potential inputs from the user\nlabeled as user (we disregard system inputs). To\nprepare training and testing data for the planner,\n*While we use tool descriptions, any information that helps\npredict the next tool could be used.\n*Details in Appendix A.\nwe unroll each interaction into p distinct unrolled\ndata instances. Within each unrolled data instance,\nthe text generated by the assistant for that specific\nstep serves as the desired output, referred to as\nthe response, while the entire sequence of steps\npreceding the current step constitutes the history.\nAs a general rule, we utilize the original full query\nof the interaction as the request. In the event that\nan input occurs amidst the steps, we simply append\nit to the subsequent request segment. Notably, the\ntraining and test data differ in terms of the tools\npresented as API candidates in the input prompt.\nTraining: To provide the planner with a com-\nprehensive set of potential tools, we utilize all p\nground truth tools identified in the original data\u2019s\nassistant steps as API candidates. The oracle TR\nstrategy employs the exact set of p ground truth\ntools (p \u2264 6) required for the entire plan in the\nprompt for each step, closely resembling a memo-\nrization task. In contrast, top-k TR-based planners\naugment the prompt with an additional (K - p)\nrandomly sampled tools for each step, where K\n> p, alongside the p ground truth tools. This ap-\nproach introduces an element of uncertainty and\nchallenges the planner to identify the most relevant\ntool for the current step. To ensure the correct tool\nis always present in the prompt, we maintain all\nground truth tools from the full plan during the\ntraining of each step. This strategy guides the plan-\nner towards learning to select the most pertinent\ntool for the current task. Balancing between the\nLLM\u2019s maximum context window size of 2048 and\nthe maximum value of p (6), we set k = 10 in our\nexperiments. To prevent the LLM from exploiting\nthe position of the correct tool, we randomize the\norder of the tools presented in the prompt during\ntraining and testing.\nTesting: In the oracle TR strategy, we use ex-\nactly p ground truth tools identified from the orig-\ninal data\u2019s assistant steps as API Candidates for\neach step. This approach provides the Planner with\na complete set of the necessary tools for each step,\neffectively making the task a tool selection prob-\nlem. Conversely, for top-K TR-based planners, we\nutilize the top-10 tools retrieved by the correspond-\ning algorithms, which may or may not include the\nground truth tool. Additionally, we employ tool\ninterleaving, where applicable.\nEvaluation: While standard NLP metrics like\nExact Match (EM) * and ROUGELSum (Lin, 2004)\n*https://github.com/huggingface/\nare commonly used to assess the overall text qual-\nity of the generated plan, our primary focus is on\nevaluating the LLM\u2019s performance in selecting ap-\npropriate tools. Therefore, we employ Tool Ac-\ncuracy (TA) and Tool Hallucination (TH) metrics,\nspecifically focusing on the API component of the\npredicted output and disregarding the parameter\ndetails.\n4\nResults\n4.1\nTool Retrieval\nFor Recall@K, we start at K=6 given the maxi-\nmum number of subtasks for a complex query in\nthe ToolBench dataset is 6, as described in Sec-\ntion 2. Table 1 shows the recall of various retrieval\napproaches for different values of K.\nMethod\nRecall@K\nK=6\nK=10\nK=15\nK=20\nFull query based BM25\n31.87\n41\n45.72\n48.71\nTD based BM25\n41.26\n47\n50.70\n54.74\nFull query based SS\n54.24\n60.86\n65.93\n69.52\nTD based SS\n57.81\n65.57\n69.85\n72.8\nProTIP\n80.55\n81.36\n82.35\n83.48\nTable 1: Evaluation of various tool retrieval methods on\nthe ToolBench test set. \u201cTD-based\" methods use task\ndecomposition by ChatGPT and run retrieval in parallel\nfor all subtasks, arriving at the top-K tools through in-\nterleaving. \u201cSS\" refers to Semantic Search.\nFigure 7: A comparison of cosine similarity distribu-\ntions between Semantic Search and Progressive Tool\nRetrieval. Cosine similarity was computed between\nground-truth tool descriptions and complex queries from\nthe ToolBench test data.\nevaluate\nVector-based retrieval methods surpass text-\nbased retrieval approaches, and TD-augmented\nretrieval employing an interleaving tools strategy\ndemonstrates substantial gains over these baselines.\nProTIP outperforms the best-performing TD-based\nretrieval method across all K values. As illustrated\nin Figure 7, ProTIP\u2019s utilization of contrastive\nlearning enhances the cosine similarity between\nrelevant tools and implicit subqueries. This im-\nprovement stems from iterative transformations per-\nformed directly in vector space, circumventing the\nrequirement for intermediate text as in TD-based\napproaches. Consequently, ProTIP acquires im-\nplicit learning capabilities to predict the subsequent\nsubtask and relevant tool while preserving the sub-\ntask order. The effectiveness of the ProTIP frame-\nwork in handling queries characterized by complex\nlanguage phenomena, such as disfluency, remains\nto be evaluated.\n4.2\nProgressive Planning\nThe progressive planning framework offers a mul-\ntitude of configurable options, encompassing the\nprompt construction (with or without history, with\nor without candidate tools, etc.) and the type of tool\nmetadata employed (e.g., tool name only versus\ntool name and description). To provide a represen-\ntative assessment of the progressive planning task,\nwe selected a subset of these configurations and\nevaluated the performance of various TR strategies\non the preprocessed test set. The results are pre-\nsented in Table 2. Settings 1-6 utilize various TR\nstrategies introduced in this paper, while settings\n7-10 employ the oracle TR strategy. To ensure a\nfair comparison with full-query-based TR strate-\ngies, we adopted the interleaving strategy (detailed\nin Section 3.1) for generating candidate tool sets\nfor progressive TR (PTR).\nOracle Planner: To assess the performance of\nour proposed PTR-based fine-tuned planner, we es-\ntablish a benchmark using several fine-tuned Oracle\nplanners. These Oracle planners utilize the com-\nplete set of p ground truth (GT) tools, necessary for\nexecuting the entire plan, in the prompt, mimicking\nthe Oracle TR algorithm. Setting 7-8 incorporates\na total of 10 tools, comprising p GT tools and (10 -\np) randomly sampled tools, while setting 9-10 em-\nploys precisely the p GT tools in the prompt. Set-\nting 9-10 can be considered a strong upper bound\nachievable using Oracle TR for two reasons: a) the\ninput prompt contains all GT tools required for ex-\nID\nTool Retrieval Setting\nPrompt\nEM\nRLSum\nTA (%)\nTH (%)\n1\nBM25 with full query\n[T+H]\n0.0442\n0.3672\n14.77\n12.37\n2\nSS with full query\n[T+H]\n0.0619\n0.4086\n21.72\n7.7\n3\nBM25 with TD query (Tool interleaving)\n[T+H]\n0.053\n0.39\n16.29\n8.96\n4\nSS with TD query (Tool interleaving)\n[T+H]\n0.0833\n0.4424\n25.88\n8.21\n5\nPTR (Tool interleaving)\n[T]\n0.0543\n0.4129\n19.82\n2.02\n6\nPTR (Tool interleaving)\n[T+H]\n0.0896\n0.4772\n36.49\n7.95\n7\nOracle (GT + random tools)\n[T]\n0.0896\n0.5232\n44.57\n4.17\n8\nOracle (GT + random tools)\n[T+H]\n0.1805\n0.6669\n77.53\n17.55\n9\nOracle (only GT tools)\n[T]\n0.2146\n0.579\n46.59\n5.3\n10\nOracle (only GT tools)\n[T+H]\n0.3952\n0.757\n80.3\n17.55\nTable 2: Performance of progressive plan generation using various combinations of tool retrieval algorithms and\nprompt generation strategies. The prompt may comprise solely the list of tools ([T]) or both history and tools\n([T+H]). We present the results for scenarios where the prompt includes only the tool name as tool metadata. For a\ngiven prompt setting (i.e., [T+H]), ProTIP consistently outperforms other baseline approaches, such as BM25 and\nSS, both with and without task decomposition. A substantial 15.79% absolute improvement in Recall@10 between\nTD-based SS and ProTIP translates to a 10.61% absolute increase in Tool Accuracy for the Planner, accompanied\nby a 0.26% reduction in Tool Hallucination.\necuting the entire plan, and b) the fine-tuned model\npartially memorizes the tool selection process for\neach step given a specific query. We believe this\nrepresents an approximate upper bound on the per-\nformance attainable by a fine-tuned LLM-Planner\nemploying the Oracle TR strategy, assuming the\nTR step achieves 100% Recall for the tools required\nfor each step of the full query.\nTR Planner: Consistently outperforming other\nbaselines like BM25 and SS, PTR demonstrates\nsuperior performance under the [T+H] prompt set-\nting, regardless of whether TD is employed. This\nsuperiority is further corroborated by the observed\ncorrelation between Recall@K of the TR algorithm\n(BM25 < SS < PTR) and Tool Accuracy (TA) of\nthe Planner. Additionally, the better performance\nof BM25 and SS-based TR for task-decomposed\nqueries is reflected in the corresponding Planner\nperformance. This aligns with the Planner\u2019s de-\nsign, which mandates tool selection from the TR\nalgorithm\u2019s retrieved set. Interestingly, the Tool\nHallucination (TH) percentage, which represents\nthe proportion of times the Planner creates non-\nexistent tools, reveals a consequence of failing to\nadhere to this design principle. PTR without his-\ntory exhibits the lowest TH percentage, despite its\nrelatively low TA. Upon incorporating history, both\nPTR (setting 6) and Oracle (settings 8 and 10) expe-\nrience an increase in TA and TH, potentially due to\ntruncation issues (discussed in Section 5). Notably,\nhigher TA in PTR leads to marginal improvements\nin Exact Match (EM) and ROUGELSum (RLSum),\nmetrics that evaluate the entire predicted output,\nincluding tool parameters. The relatively modest\ngains in these metrics suggest that further perfor-\nmance enhancements can be achieved by focusing\non tool parameter optimization. The performance\ngap between Oracle planners (settings 6 to 10) and\nthe PTR-based planner highlights the potential for\nfurther Planner performance improvements.\nImportance of history for Planning: The inclu-\nsion of the history of previous steps demonstrates\na significant performance boost in planning across\nall settings. This improvement is particularly pro-\nnounced for both Oracle-based planning (approx.\n70+% improvement between settings 9 and 10) and\nPTR-based planning (approx. 80+% improvement\nbetween settings 5 and 6) in TA. Intuitively, incor-\nporating history is crucial as it can aid in selecting\nthe most appropriate tool, especially during branch-\ning scenarios that may arise during the execution\nof the previous tool (e.g., if the previous tool exe-\ncuted successfully, select Tool-A; otherwise, select\nTool-B). However, incorporating history into the\nprompt raises concerns about truncation due to the\nincreased token count (more details in Section 5).\n5\nLimitations and Discussion\nDue to the computational demands of hyperparam-\neter tuning, we were unable to optimize the settings\nfor all configurations. Each configuration requires\n8 A100 GPUs on AWS, resulting in significant\ntime and resource consumption. Consequently, we\nfocused our hyperparameter tuning efforts on the\nProTIP (settings 5 and 6) and Oracle (settings 9\nand 10). The detailed hyperparameter values for\nall settings in Table 2 are provided in Appendix B.\nTo ensure a fair comparison with the full query-\nbased TR strategies, we employed the interleav-\ning strategy (described in Section 3.1) for gener-\nating candidate tool sets for PTR. We recognize\nthat this approach is not ideal for evaluating the\nplanner\u2019s performance under PTR, as the optimal\napproach would involve retrieving tools step-by-\nstep and performing planning at each step. How-\never, this would require a mechanism to execute the\npredicted API call at each step and incorporate the\nresulting output into the planning process for the\nnext step. While we are currently investigating po-\ntential solutions for this challenge, planner design\nis not the primary focus of this work. Therefore,\nwe defer the development and evaluation of end-to-\nend step-by-step planning experiments, including\nperformance tuning, to future research.\nThe experiment results reveal a substantial per-\nformance gap between the Oracle planner and the\nTR-based planners. This disparity can be attributed\nto two key factors. First, the Oracle planner (set-\ntings 9 and 10) utilizes the exact set of p ground\ntruth tools specified in the prompt for each pro-\ngressive planning step (p \u2264 6), whereas the TR\nplanners operate on a larger candidate set of K=10\ntools. This restricted tool selection for the Oracle\nplanner (settings 9 and 10) likely contributes to its\nimproved performance. This observation is further\nsupported by the higher TA achieved in setting 10\n(using exactly p ground truth tools) compared to\nsetting 8 (using K tools, with p ground truth tools\nand (10 - p) randomly sampled tools).\nThe tool distribution discrepancy between the\ntrain and test sets, particularly for tool IDs greater\nthan 8000, as evident in Figure 3, may partially ex-\nplain the inferior performance of all retrieval-based\nplanners. This disparity in tool distribution could\nhinder the effectiveness of the TR strategies, poten-\ntially leading to suboptimal planning decisions, un-\nless tool metadata is further enriched and included\nin the prompt during training to support for better\ngeneralization. Additionally, we observed a poor\nAccuracy for the special Finish tool, which resulted\nin overall low performance in all the TR settings.\nThe training strategy of employing the p ground\ntruth tools alongside (K - p) randomly sampled\ntools in the prompt may contribute to the lower\nperformance of the TR planner models. The pres-\nence of the ground truth tools alongside seman-\ntically dissimilar randomly sampled tools during\ntraining likely facilitates the models\u2019 ability to iden-\ntify the correct tool. However, during testing, the\nprompt comprises top-K tools retrieved by the TR\nalgorithms, which may exhibit semantic similarity\nto the ground truth tool. This semantic similar-\nity poses a challenge for the models during infer-\nence, leading to the observed low TA values for\nall TR-based planner models. Utilizing the top-K\ntools retrieved by the TR algorithms during train-\ning could exacerbate this issue, as there is a risk of\nthe prompt not containing the correct tool for the\ncorresponding step. This would further complicate\nthe learning process for the LLM and increase the\nlikelihood of hallucinations.\nTo address this limitation, in future, an alterna-\ntive training data creation strategy could be em-\nployed. Instead of using randomly sampled tools,\nthe training data could incorporate tools retrieved\nby the TR algorithm on the training set. Addition-\nally, to ensure that the training process effectively\naddresses all steps, the ground truth tool for the\ncurrent step could be injected into the prompt if it\nis not already present. By adopting this modified\ntraining approach, we aim to enhance the perfor-\nmance of the TR planner models and improve their\ngeneralization capabilities. The Instructions part\nof the prompt are generic and can be further modi-\nfied (i.e., made more precise for each scenario) to\nbe more specific to various prompt settings. Also,\nwe did not conduct an exhaustive study to test the\nrobustness of the planner output to different types\nof input prompt variations (e.g.: paraphrased query\nas inputs, semantically similar tools, unseen tools\nin the test set etc.), which we leave as future work.\nOur experiments highlight the significance of\nthe history in achieving higher TA for both Oracle\n(setting 9 vs. 10) and PTR (setting 5 vs. 6) based\nplanning strategies.\nHowever, we believe that\nTA can be further improved while reducing TH,\nparticularly for TR planners with K=10 tools,\nas the history contributes to the long context\nissue. We observe that for the scenarios where the\ninput size becomes close to the maximum context\nwindow size, the LLM could generate empty plans,\nwhich contributes to 3% to 5% of the errors in our\nexperiments, thereby negatively impacting the TA.\nNote that the original data contains intermediate\nsteps with verbose outputs that provide minimal\ncontextual information (e.g., weather API outputs\nwith latitude, longitude, last updated time, etc.),\nall of which may not be essential for determining\nthe next API. Preserving these verbose outputs\nin the history exacerbates the truncation problem,\nthereby negatively impacting the learning and plan\ngeneration capability of the model. This issue can\nbe further aggravated by incorporating more tool\nmetadata (tool description, parameters, API signa-\nture, etc.) into the prompt, which will increase the\ninput length of the prompt even further. However\nfor better generalization to unseen tools, ideally we\nwant to incorporate such additional metadata into\nthe prompt, which requires further investigation.\nIncreasing the context window size of LLMs\n(e.g., to 4096 or higher) or employing techniques\nthat allow for larger input text (e.g., as proposed\nin (Beltagy et al., 2020)) can largely alleviate the\ntruncation problem. However, even with large con-\ntext window, studies by (Liu et al., 2023) indicate\nthat LLMs tend to focus on information at the be-\nginning or end of the prompt, even with a large\ncontext window. Therefore, alongside exploring\nLLMs with larger context windows, there is a need\nto develop methods for effectively compressing and\npresenting relevant contextual information, partic-\nularly the history, to the LLM (Ge et al., 2023) to\nenhance performance.\nIn the current work, we focused heavily on the\ntool accuracy across the tool retrieval and plan-\nning steps (Patil et al., 2023). Tool parameter ac-\ncuracy is another important aspect of the planner\noutput (Shen et al., 2023b), which requires further\ninvestigations to improve the performance. We\ndid not conduct any safety study or red-teaming\nto evaluate the bias or risks emanating from the\noutputs generated by the fine-tuned LLM. We want\nto refer to a contemporary work by (Valmeekam\net al., 2023) which has pointed out that the abil-\nity of LLM\u2019s to generate \u201cexecutable plans\u201d in a\ncompletely autonomous way is very limited. In\nour work, while planning is not the primary focus,\nwe observed that plan generation using supervised\nfine-tuning of a LLM is not an easy task, specifi-\ncally with a relatively small LLM (e.g.: LLM with\n7B parameters). We believe further research is re-\nquired to enhance our understanding of the true\ncapabilities of LLM\u2019s for the planning task.\n6\nRelated Work\nTool Retrieval using Neural embedding: Vector\ndatabases enable storing tool name and description\nembeddings generated by an encoding model (Cer\net al., 2018). These embeddings are then lever-\naged for semantic similarity computation, utilizing\nmeasures like cosine similarity, with queries or sub-\nqueries. Building on the established approach of\nutilizing neural networks to generate task-specific\nsemantic word/sentence embeddings for informa-\ntion retrieval and NLP tasks (Zhang et al., 2016),\nthis work proposes a tool embedding generation\nstrategy specifically designed to facilitate step-by-\nstep planning.\nWord embeddings (Mikolov et al., 2013b; Pen-\nnington et al., 2014; Levy and Goldberg, 2014; Jiao\nand Zhang, 2021), learned vectors representing var-\nious linguistic and semantic aspects of words, have\nrevolutionized Natural Language Processing by en-\nabling efficient solutions to diverse tasks like anal-\nogy queries (Levy and Goldberg, 2014; Allen and\nHospedales, 2019). Building upon this success,\nresearch has extended to generating sentence, para-\ngraph, and document-level embeddings (Le and\nMikolov, 2014; Wieting et al., 2015; Li et al., 2015)\nfor various applications. Similarly, the Knowledge\nGraph domain utilizes node embedding to encode\nentity relationships, trained with custom objective\nfunctions to capture latent relationships in the vec-\ntor space for subsequent exploitation (Wang et al.,\n2017). We leverage this paradigm, employing pro-\ngressive tool retrieval with fine-tuned embeddings\noptimized for the step-by-step planning task.\nLLM as Planner: LLMs have emerged as po-\ntent few-shot learners (Brown et al., 2020; Rae\net al., 2022), exhibiting remarkable prowess across\ndiverse language tasks. However, planning remains\na challenging research frontier despite their impres-\nsive performance in these domains. Planning in-\nvolves decomposing a high-level natural language\n(NL) task into a sequence of executable steps re-\nalizable by an agent. This process demands both\nlanguage comprehension and an understanding of\na predefined set of actions, tools, APIs, and other\ngrounding elements. In the realm of embodied\nagents, LLMs have been harnessed to decompose\nNL instructions into simpler, more manageable\nunits (Huang et al., 2022; Ahn et al., 2022; Singh\net al., 2022; Khot et al., 2022; Wu et al., 2023;\nShen et al., 2023b). Notably, using LLMs to gen-\nerate tool/API calls as part of the planning process\ncan be akin to multi-step program synthesis (Li\net al., 2023; Nijkamp et al., 2022, 2023). More\nrecent works have tried to further improve LLM\nperformance by adding the capability to reason/-\ncriticize the LLM outputs (Kim et al., 2023; Yao\net al., 2022).\nWhile contemporary research has emphasized\nleveraging tools to enhance LLM capabilities, most\nexisting retrieval systems rely on vector databases,\nsimilar to the renowned Retrieval Augmented Gen-\neration (RAG) technique (Lewis et al., 2021), to\nstore and retrieve non-parametric knowledge ab-\nsent in the LLM. Recent work has explored in-\ndividual tools like web search engines (Nakano\net al., 2022), calculators (Andor et al., 2019), and\ngeneric toolsets (Schick et al., 2023) for planning,\nwhile others have integrated LLMs with expan-\nsive API collections to address more open-ended\ntasks (Patil et al., 2023; Shen et al., 2023a; Liang\net al., 2023; Qin et al., 2023b). Fine-tuning with\ntool-specific data is often employed to improve\ntask performance. However, as the number of tool\ngrows, retrieval-based systems emerge as an effi-\ncient means for selecting the relevant tools for a\ngiven request (Patil et al., 2023; Qin et al., 2023b).\nBuilding upon this paradigm, our approach pro-\nposes the novel concept of incrementally generat-\ning tool candidates specific to the current sub-step\nwithin a multi-step planning task, ultimately en-\nhancing the LLM\u2019s overall planning performance.\n7\nConclusion\nWe introduce ProTIP, a novel lightweight tool re-\ntrieval framework, which surpasses LLM-based\nplanning agents equipped with state-of-the-art task\ndecomposition retrieval powered by ChatGPT. Pro-\nTIP\u2019s iterative vector transformations, enabled by\ncontrastive learning, facilitate implicit learning of\nsequential subtask prediction, eliminating the need\nfor explicit subtask labels. Additionally, ProTIP\neffectively handles \"subtask-tool atomicity align-\nment.\" On the ToolBench dataset, ProTIP frame-\nwork surpasses the ChatGPT task decomposition\nbased approach by 24% on Recall@K=10 for Tool\nRetrieval and by 41% on Tool Accuracy for plan\ngeneration.\n8\nAcknowledgements\nWe would like to thank Stephen Pulman, Russ\nWebb and Arturo Argueta for their valuable feed-\nback. Also we thank Jiarui Lu, Yuan Zhang, Xuan\nWang, Hans Han, and Jian Zhang for providing\ninfrastructure support to fine-tune LLMs.\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yev-\ngen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol\nHausman, Alex Herzog, Daniel Ho, Jasmine Hsu,\nJulian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,\nRosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-\nmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalash-\nnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey\nLevine, Yao Lu, Linda Luu, Carolina Parada, Pe-\nter Pastor, Jornell Quiambao, Kanishka Rao, Jarek\nRettinghouse, Diego Reyes, Pierre Sermanet, Nico-\nlas Sievers, Clayton Tan, Alexander Toshev, Vincent\nVanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do as i can,\nnot as i say: Grounding language in robotic affor-\ndances.\nCarl Allen and Timothy Hospedales. 2019. Analogies\nexplained: Towards understanding word embeddings.\nDaniel Andor, Luheng He, Kenton Lee, and Emily Pitler.\n2019. Giving bert a calculator: Finding operations\nand arguments with reading comprehension.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nDaniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil.\n2018. Universal sentence encoder.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nTogether Computer. 2023. Redpajama-data: An open\nsource recipe to reproduce llama training dataset.\nTao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu\nWei. 2023. In-context autoencoder for context com-\npression in a large language model. arXiv preprint\narXiv:2307.06945.\nXinyang Geng and Hao Liu. 2023. Openllama: An open\nreproduction of llama.\nR. Hadsell, S. Chopra, and Y. LeCun. 2006. Dimension-\nality reduction by learning an invariant mapping. In\nProceedings of 2006 IEEE Computer Society Con-\nference on Computer Vision and Pattern Recognition\n(CVPR\u201906).\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022. Language models as zero-shot\nplanners: Extracting actionable knowledge for em-\nbodied agents.\nQilu Jiao and Shunyao Zhang. 2021. A brief survey of\nword embedding and its recent development. In 2021\nIEEE 5th Advanced Information Technology, Elec-\ntronic and Automation Control Conference (IAEAC),\nvolume 5, pages 1697\u20131701. IEEE.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2022. Decomposed prompting: A modular\napproach for solving complex tasks. In Proceedings\nof the Eleventh International Conference on Learning\nRepresentations.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer.\n2023. Language models can solve computer tasks.\narXiv preprint arXiv:2303.17491.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems, 35:22199\u2013\n22213.\nQuoc Le and Tomas Mikolov. 2014. Distributed repre-\nsentations of sentences and documents. In Interna-\ntional conference on machine learning, pages 1188\u2013\n1196. PMLR.\nOmer Levy and Yoav Goldberg. 2014. Linguistic regu-\nlarities in sparse and explicit word representations. In\nProceedings of the eighteenth conference on compu-\ntational natural language learning, pages 171\u2013180.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen tau Yih, Tim Rock-\nt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2021.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks.\nJiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015.\nA hierarchical neural autoencoder for paragraphs and\ndocuments. arXiv preprint arXiv:1506.01057.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.\n2023. Starcoder: may the source be with you! arXiv\npreprint arXiv:2305.06161.\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,\nYan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,\nShaoguang Mao, Yun Wang, Linjun Shou, Ming\nGong, and Nan Duan. 2023. Taskmatrix.ai: Com-\npleting tasks by connecting foundation models with\nmillions of apis.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74\u201381, Barcelona, Spain.\nAssociation for Computational Linguistics.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023.\nLost in the middle:\nHow lan-\nguage models use long contexts.\narXiv preprint\narXiv:2307.03172.\nTom\u00e1\u0161 Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013a. Linguistic regularities in continuous space\nword representations. In Proceedings of the 2013\nconference of the north american chapter of the as-\nsociation for computational linguistics: Human lan-\nguage technologies, pages 746\u2013751.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013b. Linguistic regularities in continuous space\nword representations. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 746\u2013751, Atlanta,\nGeorgia. Association for Computational Linguistics.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2022. Webgpt: Browser-\nassisted question-answering with human feedback.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hern\u00e1ndez \u00c1brego, Ji Ma, Vincent Y. Zhao,\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\nYang. 2021. Large dual encoders are generalizable\nretrievers.\nErik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Sil-\nvio Savarese, and Yingbo Zhou. 2023. Codegen2:\nLessons for training llms on programming and natu-\nral languages. arXiv preprint arXiv:2305.02309.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474.\nOpenAI. 2023a. Introducing chatgpt.\nR OpenAI. 2023b.\nGpt-4 technical report. arxiv\n2303.08774. View in Article, 2.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nShishir G Patil, Tianjun Zhang, Xin Wang, and\nJoseph E Gonzalez. 2023. Gorilla: Large language\nmodel connected with massive apis. arXiv preprint\narXiv:2305.15334.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language processing\n(EMNLP), pages 1532\u20131543.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, et al. 2023a.\nTool\nlearning with foundation models.\narXiv preprint\narXiv:2304.08354.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, Sihan Zhao, Lauren Hong, Runchu Tian,\nRuobing Xie, Jie Zhou, Mark Gerstein, Dahai Li,\nZhiyuan Liu, and Maosong Sun. 2023b. Toolllm:\nFacilitating large language models to master 16000+\nreal-world apis.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d\u2019Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam Isaac, Ed Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling\nlanguage models: Methods, analysis & insights from\ntraining gopher.\nStephen Robertson, S. Walker, S. Jones, M. M. Hancock-\nBeaulieu, and M. Gatford. 1995. Okapi at trec-3.\nIn Overview of the Third Text REtrieval Conference\n(TREC-3), pages 109\u2013126. Gaithersburg, MD: NIST.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023a. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nYongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang,\nKan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li,\nand Yueting Zhuang. 2023b. Taskbench: Benchmark-\ning large language models for task automation. arXiv\npreprint arXiv:2311.18760.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit\nGoyal, Danfei Xu, Jonathan Tremblay, Dieter Fox,\nJesse Thomason, and Animesh Garg. 2022. Prog-\nprompt: Generating situated robot task plans using\nlarge language models.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois,\nXuechen Li,\nCarlos Guestrin,\nPercy\nLiang, and Tatsunori B. Hashimoto. 2023.\nStan-\nford\nalpaca:\nAn\ninstruction-following\nllama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nKarthik Valmeekam, Sarath Sreedharan, Matthew Mar-\nquez, Alberto Olmo, and Subbarao Kambhampati.\n2023. On the planning abilities of large language\nmodels (a critical investigation with a proposed\nbenchmark). arXiv preprint arXiv:2302.06706.\nQuan Wang, Zhendong Mao, Bin Wang, and Li Guo.\n2017. Knowledge graph embedding: A survey of\napproaches and applications.\nIEEE Transactions\non Knowledge and Data Engineering, 29(12):2724\u2013\n2743.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models.\narXiv\npreprint arXiv:2203.11171.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022a. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2015. Towards universal paraphrastic sen-\ntence embeddings. arXiv preprint arXiv:1511.08198.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019. Hug-\ngingface\u2019s transformers: State-of-the-art natural lan-\nguage processing.\nYue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhut-\ndinov, Amos Azaria, Yuanzhi Li, Tom Mitchell, and\nShrimai Prabhumoye. 2023. Plan, eliminate, and\ntrack \u2014 language models are good teachers for em-\nbodied agents.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and\nTengyu Ma. 2021. An explanation of in-context learn-\ning as implicit bayesian inference. arXiv preprint\narXiv:2111.02080.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022.\nGlm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nYe Zhang, Md Mustafizur Rahman, Alex Braylan, Bran-\ndon Dang, Heng-Lu Chang, Henna Kim, Quinten\nMcNamara, Aaron Angert, Edward Banner, Vivek\nKhetan, et al. 2016. Neural information retrieval: A\nliterature review. arXiv preprint arXiv:1611.06792.\nA\nPrompt Variations\nFigure 8: Prompt variants for plan generation with and\nwithout execution history.\nB\nHyperparemeters\nDue to time and resource constraints, we were un-\nable to perform hyperparameter tuning for all ex-\nperimental setups. The following table lists the\nhyperparameters used for each of the settings.\nTable 3: Hyperparameters for training\nSetting\nID\nEpochs\nLR\nBatch\nWarmup\nSteps\n1\n3\n3e-5\n64\n20\n2\n3\n3e-5\n64\n20\n3\n3\n3e-5\n64\n20\n4\n3\n3e-5\n64\n20\n5\n3\n3e-5\n64\n20\n6\n3\n5e-5\n128\n20\n7\n3\n5e-5\n128\n20\n8\n3\n5e-5\n128\n20\n9\n3\n2e-5\n64\n20\n10\n3\n2e-5\n64\n20\nThis model utilizes a maximum context window\nsize of 2048 and employs a weight decay of 0\nacross all experiments.\n"
  },
  {
    "title": "Your Student is Better Than Expected: Adaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.10835.pdf",
    "upvote": "5",
    "text": "Your Student is Better Than Expected:\nAdaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models\nNikita Starodubcev\nArtem Fedorov\nArtem Babenko\nDmitry Baranchuk\nYandex Research\nhttps://github.com/yandex-research/adaptive-diffusion\nFigure 1. Left: Overview of the proposed approach. Right: Side-by-side comparison of SDv1.5 and SDXL with their few-step distilled\nversions. The distilled models surpass the original ones in a noticeable number of samples for the same text prompts and initial noise.\nAbstract\nKnowledge distillation methods have recently shown to be\na promising direction to speedup the synthesis of large-scale\ndiffusion models by requiring only a few inference steps.\nWhile several powerful distillation methods were recently\nproposed, the overall quality of student samples is typically\nlower compared to the teacher ones, which hinders their\npractical usage. In this work, we investigate the relative\nquality of samples produced by the teacher text-to-image\ndiffusion model and its distilled student version. As our\nmain empirical finding, we discover that a noticeable por-\ntion of student samples exhibit superior fidelity compared to\nthe teacher ones, despite the \u201capproximate\u201d nature of the\nstudent. Based on this finding, we propose an adaptive col-\nlaboration between student and teacher diffusion models for\neffective text-to-image synthesis. Specifically, the distilled\nmodel produces an initial image sample, and then an ora-\ncle decides whether it needs further improvements with the\nteacher model. Extensive experiments demonstrate that the\ndesigned pipeline surpasses state-of-the-art text-to-image\nalternatives for various inference budgets in terms of human\npreference. Furthermore, the proposed approach can be\nnaturally used in popular applications such as text-guided\nimage editing and controllable generation.\n1. Introduction\nLarge-scale diffusion probabilistic models (DPMs) have re-\ncently shown remarkable success in text-conditional image\ngeneration [32, 34, 38, 41] that aims to produce high quality\nimages closely aligned with the user-specified text prompts.\nHowever, DPMs pose sequential synthesis leading to high\ninference costs opposed to feed-forward alternatives, e.g.,\n1\narXiv:2312.10835v2  [cs.CV]  28 Dec 2023\nGANs, that provide decent text-to-image generation results\nfor a single forward pass [17, 43].\nThere are two major research directions mitigating the\nsequential inference problem of state-of-the-art diffusion\nmodels. One of them considers the inference process as a\nsolution of a probability flow ODE and designs efficient and\naccurate solvers [18, 26, 27, 49, 58] reducing the number of\ninference steps down to \u223c10 without drastic loss in image\nquality. Another direction represents a family of knowl-\nedge distillation approaches [12, 24, 25, 28, 30, 42, 44, 50]\nthat learn the student model to simulate the teacher distri-\nbution requiring only 1\u22124 inference steps. Recently, dis-\ntilled text-to-image models have made a significant step for-\nward [25, 28, 30, 44]. However, they still struggle to achieve\nthe teacher performance either in terms of image fidelity and\ntextual alignment [25, 28, 30] or distribution diversity [44].\nNevertheless, we hypothesize that text-to-image students\nmay already have qualitative merits over their teachers. If so,\nperhaps it would be more beneficial to consider a teacher-\nstudent collaboration rather than focusing on replacing the\nteacher model entirely.\nIn this paper, we take a closer look at images produced\nby distilled text-conditional diffusion models and observe\nthat the student can generate some samples even better than\nthe teacher. Surprisingly, the number of such samples is\nsignificant and sometimes reaches up to half of the empir-\nical distribution. Based on this observation, we design an\nadaptive collaborative pipeline that leverages the superiority\nof student samples and outperforms both individual models\nalone for various inference budgets. Specifically, the student\nmodel first generates an initial image sample given a text\nprompt, and then an \u201coracle\u201d decides if this sample should be\nupdated using the teacher model at extra compute. The simi-\nlar idea has recently demonstrated its effectiveness for large\nlanguage models (LLMs) [5] and we show that it can be nat-\nurally applied to text-conditional diffusion models as well.\nOur approach is schematically presented in Figure 1. To\nsummarize, our paper presents the following contributions:\n\u2022 We reveal that the distilled student DPMs can outperform\nthe corresponding teacher DPMs for a noticeable number\nof generated samples. We demonstrate that most of the\nsuperior samples correspond to the cases when the student\nmodel significantly diverges from the teacher.\n\u2022 Based on the finding above, we develop an adaptive\nteacher-student collaborative approach for effective text-to-\nimage synthesis. The method not only reduces the average\ninference costs but also improves the generative quality by\nexploiting the superior student samples.\n\u2022 We provide an extensive human preference study illustrat-\ning the advantages of our approach for text-to-image gen-\neration. Moreover, we demonstrate that our pipeline can\nreadily improve the performance of popular text-guided\nimage editing and controllable generation tasks.\n2. Related work\nDiffusion Probabilistic Models (DPMs) [14, 48, 49] repre-\nsent a class of generative models consisting of forward and\nreverse processes. The forward process {xt}[0,T ] transforms\nreal data x0 \u223c pdata(x0) into the noisy samples xt using the\ntransition kernels N\n\u0000xt | \u221a1 \u2212 \u03c3tx0, \u03c3tI\n\u0001\nspecifying \u03c3t\naccording to the selected noise schedule.\nThe reverse diffusion process generates new data points\nby gradually denoising samples from a simple (usually stan-\ndard normal) distribution. This process can be formulated\nas a probabilistic-flow ODE (PF-ODE) [46, 49], where the\nonly unknown component is a score function, which is ap-\nproximated with a neural network. The ODE perspective of\nthe reverse process fosters designing a wide range of the spe-\ncialized solvers [15, 18, 26, 27, 46, 57, 58] for efficient and\naccurate sampling. However, for text-to-image generation,\none still needs \u223c25 and more steps for the top performance.\nText-conditional diffusion models can be largely\ngrouped into cascaded and latent diffusion models. The\ncascaded models [32, 41] generate a sample in several stages\nusing separate diffusion models for different image resolu-\ntions. The latent diffusion models [34, 37, 38] first generate\na low-resolution latent variable in the VAE [21] space and\nthen apply its feedforward decoder to map the latent sample\nto the high-resolution pixel space. Thus, the latent diffusion\nmodels have significantly more efficient inference thanks to\na single forward pass for the upscaling step.\nAlong with cascaded models, there are other works com-\nbining several diffusion models into a single pipeline. Some\nmethods propose to use distinct diffusion models at differ-\nent time steps [1, 8, 23, 55]. Others [25, 34] consider an\nadditional model to refine the samples produced with a base\nmodel. In contrast, our method relies on the connection be-\ntween student and teacher models and adaptively improves\nonly selected student samples to reduce the inference costs.\nText-to-image diffusion models have also succeeded in\ntext-guided image editing and personalization [3, 10, 19, 29,\n31, 40]. Moreover, some methods allow controllable genera-\ntion via conditioning on additional inputs, e.g., canny-edges,\nsemantic masks, sketches [52, 56]. Our experiments show\nthat the proposed pipeline is well-suited to these techniques.\nDistillation of diffusion models is another pivotal di-\nrection for efficient diffusion inference [2, 25, 30, 42, 44,\n47, 50]. The primary goal is to adapt the diffusion model\nparameters to represent the teacher image distribution for\n1\u22124 steps. Recently, consistency distillation (CD) [50] have\ndemonstrated promising results on both classical bench-\nmarks [20, 47] and text-to-image generation [28] but fall\nshort of the teacher performance at the moment. Concur-\nrently, adversarial diffusion distillation [44] could outper-\nform the SDXL-Base [34] teacher for 4 steps in terms of\nimage quality and prompt alignment. However, it signifi-\ncantly reduces the diversity of generated samples, likely due\n2\nFigure 2. Student outperforms its teacher (SD1.5). Left: Text-conditional image synthesis. Right: Text-guided image editing (SDEdit [29]).\nThe images within each pair are generated for the same initial noise sample.\nto the adversarial training [11] and mode-seeking distillation\ntechnique [35]. Therefore, it is still an open question if a few-\nstep distilled model can perfectly approximate the diffusion\nmodel on highly challenging and diverse distributions that\nare currently standard for text-conditional generation [45].\n3. Toward a unified teacher-student framework\nOpposed to the purpose of replacing the expensive text-to-\nimage diffusion models by more effective few-step alterna-\ntives, the present work suggests considering the distilled text-\nto-image models as a firm companion in a teacher-student\ncollaboration.\nIn this section, we first explore the advantages of the\ndistilled text-to-image models and then unleash their poten-\ntial in a highly effective generative pipeline comprising the\nstudent and teacher models.\n3.1. Delving deeper into the student performance\nWe start with a side-by-side comparison of the student and\nteacher text-to-image diffusion models. Here, we focus on\nStable Diffusion v1.51 (SD1.5) as our main teacher model\nand distill it using consistency distillation [50]. The student\ndetails and sampling setup are presented in A. The similar\nanalysis for a few other distilled models is provided in B.2.\nIn Figure 1 (Right), we provide the human votes for 600\nrandom text prompts from COCO2014 [22] for SD1.5 and\nSDXL. The images within each pair are generated for the\nsame initial noise sample. We observe that the students gener-\nally falls short of the teacher performance. However, interest-\ningly, despite the initial intention to mimic the teacher model,\n\u223c30% student samples were preferred over the teacher ones.\nA few visual examples in Figure 2 validate these results.\n1https://huggingface.co/runwayml/stable-diffusion-v1-5\nTherefore, we can formulate our first observation:\nThe student can surpass its teacher in a substantial\nportion of image samples.\nBelow, we develop a more profound insight into this phe-\nnomenon.\nStudent-teacher similarity. First, we evaluate the student\u2019s\nability to imitate the teacher. We compute pairwise simi-\nlarities between the student and teacher images generated\nfor the same text prompts and initial noise. As a similarity\nmeasure, we consider DreamSim [9] tuned to align with the\nhuman perception judgments. For evaluation, we consider\n5000 prompts from the COCO2014 [22] validation split.\nPrimarily, we observe that many student samples are\nhighly distinct from the teacher ones. A few image pairs\nare presented in Figure 3a. Figure 3c presents the human\nvote distribution for three distance ranges: low, medium, and\nhigh. Interestingly, most of the student wins appear when\nits samples are highly different from the teacher ones. This\nbrings us to our second observation:\nThe student wins are more likely where its samples\nsignificantly differ from the teacher ones.\nAlso, we evaluate the relative gap in sample quality\nagainst the similarity between the teacher and student out-\nputs. To measure the quality of individual samples, we use\nImageReward [53], which shows a positive correlation with\nhuman preferences in terms of image fidelity and prompt\nalignment. The gap in sample quality is calculated as the\ndifference between the ImageReward scores for student and\nteacher samples. We observe that highly distinct samples\n3\nFigure 3. (a) Visual examples of similar (Left) and dissimilar\n(Right) teacher and student samples. (b) Similarity between the\nstudent and teacher samples w.r.t. the difference in sample quality.\nHighly distinct samples tend to be of different quality. (c) Human\nvote distribution for different distance ranges between student and\nteacher samples. Most of the student wins are achieved when the\nstudent diverges from the teacher.\nlikely have a significant difference in quality. Importantly,\nthis holds for both student failures and successes, as shown\nin Figure 3b. Therefore, effectively detecting the positive\nstudent samples and improving the negative ones can poten-\ntially increase the generative performance.\nImage complexity. Then, we describe the connection of\nthe similarity between student and teacher samples with the\nteacher image complexity. To estimate the latter, we use the\nICNet model [7] learned on a large-scale human annotated\ndataset. The results are presented in Figure 4. We notice\nthat larger distances between student and teacher outputs are\nmore typical for complex teacher samples. In other words,\nthe student mimics its teacher for plain images, e.g., close-up\nfaces, while acting more as an independent model for more\nintricate ones. Figure 4b confirms that significant changes in\nimage quality are observed for more complex images.\nFigure 4. Effect of image complexity. (a) More similar student\nand teacher samples corresponds to simpler images and vice versa.\n(b) The student and teacher largely diverge in image quality on the\ncomplex teacher samples.\nFigure 5. Effect of text prompts. (a) Shorter prompts usually\nlead to more similar student and teacher samples. (b) The student\nand teacher tend to generate more similar images when the student\nrelies heavily on the text prompt.\nText prompts. Then, we analyse the connection of the\nstudent-teacher similarity with the prompt length. Figure 5\ndemonstrates that shorter prompts typically lead to more\nsimilar teacher and student samples. Here, the prompt length\nequals to the number of CLIP tokens. Intuitively, longer\nprompts are more likely to describe intricate scenes and ob-\nject compositions than shorter ones. Note that long prompts\ncan also carry low textual informativeness and describe con-\ncepts of low complexity. We hypothesize that this causes\nhigh variance in Figure 5a.\nAlso, we report the prompt influence on the student gen-\neration w.r.t. the student-teacher similarity in Figure 5b. We\nestimate the prompt influence by aggregating student cross-\nattention maps. More details are in B.1. The student tends\nto imitate the teacher if it relies heavily on the text prompt.\nTrajectory curvature. Previously, it was shown to be ben-\neficial to straighten the PF-ODE trajectory before distilla-\ntion [24, 25]. We investigate the effect of the trajectory\ncurvature on the similarity between the teacher and student\nsamples and its correlation with the teacher sample com-\nplexity. We estimate the trajectory curvatures following [4]\nand observe that straighter trajectories lead to more similar\nstudent and teacher samples (Figure 6a). In addition, we\n4\nFigure 6. Effect of teacher trajectory curvature. (a) The student\nsamples resemble the teacher ones for less curved trajectories. (b)\nStraighter trajectories usually correspond to plainer teacher images.\nFigure 7. Full-reference vs no-reference decision-making. Usu-\nally, one can find a k-th percentile of the ImageReward scores pro-\nviding the correlation with human votes similar to the full-reference\ncomparisons but without observing the teacher samples.\nshow that the trajectory curvature correlates with the teacher\nsample complexity (Figure 6b).\nTo sum up, we conclude that the student largely diverges\nfrom the teacher on the samples that are challenging in dif-\nferent respects. Interestingly, the superior student samples\noften occur in these cases.\n3.2. Method\nIn this section, we propose an adaptive collaborative ap-\nproach consisting of three steps: 1) Generate a sample with\nthe student model; 2) Decide if the sample needs further\nimprovement; 3) If so, refine or regenerate the sample with\nthe teacher model.\nStudent generation step produces an initial sample X S\nfor a given context and noise. This work considers consis-\ntency distillation [50] as a primary distillation framework\nand uses multistep consistency sampling [50] for generation.\nAdaptive step leverages our finding that many student\nsamples may exhibit superior quality. Specifically, we seek\nan \u201coracle\u201d that correctly detects superior student samples.\nFor this role, we consider an individual sample quality es-\ntimator E. In particular, we use the current state-of-the-art\nautomated estimator, ImageReward (IR) [53] that is learned\nto imitate human preferences for text-to-image generation.\nThen, comparing the scores of the teacher and student\nsamples, one can conclude which one is better. However, in\npractice, we avoid expensive teacher inference to preserve\nthe efficiency of our approach. Therefore, a decision must be\nmade having access only to the student sample X S. To ad-\ndress this problem, we introduce a cut-off threshold \u03c4 which\nis a k-th percentile of the IR score tuned on a hold-out subset\nof student samples. The details on the threshold tuning are\ndescribed in C. During inference, the IR score is calculated\nonly for X S. If it exceeds the threshold \u03c4, we accept the\nsample and avoid further teacher involvement. Interestingly,\nwe observe that it is often possible to reproduce the accuracy\nof the full-reference estimation by varying \u03c4 (see Figure 7).\nAlso, note that IR calculation costs are negligible compared\nto a single diffusion step, see D.6.\nImprovement step engages the teacher to improve the\nquality of the rejected student samples. We consider two\nteacher involvement strategies: regeneration and refinement.\nThe former simply applies the teacher model to produce\na new sample from scratch for the same text prompt and\nnoise. The refinement is inspired by the recent work [34].\nSpecifically, X S is first corrupted with a Gaussian noise\ncontrolled by the rollback value \u03c3 \u2208 [0, 1]. Higher \u03c3 leads to\nmore pronounced changes. We vary \u03c3 between 0.3 and 0.75\nin our experiments. Then, the teacher starts sampling from\nthe corrupted sample following the original noise schedule\nand using an arbitrary solver, e.g., DPM-Solver [26]. Note\nthat refinement requires significantly fewer steps to produce\nthe final sample than generation from scratch. Intuitively,\nthe refinement strategy aims to fix the defects of the student\nsample. At the same time, the regeneration strategy may\nbe useful if X S is poorly aligned with the text prompt in\ngeneral. Our experiments below confirm this intuition.\n4. Experiments\nWe evaluate our approach for text-to-image synthesis, text-\nguided image editing and controllable generation. The re-\nsults confirm that the proposed adaptive approach can out-\nperform the baselines for various inference budgets.\n4.1. Text-guided image synthesis\nIn most experiments, we use Stable Diffusion v1.5 (SD1.5)\nas a teacher model and set the classifier-free guidance scale\nto 8. To obtain a student model, we implement consistency\ndistillation (CD) for latent diffusion models and distill SD1.5\non the 80M subset of LAION2B [45]. The resulting model\ndemonstrates decent performance for 5 steps of multistep\nconsistency sampling with the guidance scale 8.\nMetrics. We first consider FID [13], CLIP score [36] and\nImageReward [53] as automatic metrics. ImageReward is\nselected due to a higher correlation with human preferences\ncompared to FID and CLIP scores. OpenCLIP ViT-bigG [6]\nis used for CLIP score calculation. For evaluation, we use\n5000 text prompts from the COCO2014 validation set [22].\n5\nFigure 8. Qualitative comparison of our adaptive refinement approach to the SD1.5 teacher model.\n\u001f\u001e \u001f\u001e\n\u001f\u001d\n\u001c\u001c\n\u001f\u001e\u001f\u001d\n\u001b\u001d\n\u001b\u001c\n\u001a\u0019\n\u001f\n\u001e\u001f\n\u001d\u001f\n\u001c\u001f\n\u001b\u001f\n\u001a\u0019\u0018\u0017\u0016\n\u001e\u001f\n\u001d\u0015\n\u0015\u001f\n\u001e\u001f\n\u001d\u0015\n\u001e\u0015\n\u001f\n\u001e\u001f\n\u001d\u001f\n\u001c\u001f\n\u001b\u001f\n\u001a\u0019\u0018\u0017\u0016\n\u001e\u001f\n\u001e\u0015\n\u001f\u001e\u001d\n\u001c\u001d\n\u001f\n\u001e\u001f\n\u001d\u001f\n\u001c\u001f\n\u001b\u001f\n\u001a\u0019\u0018\u0017\u0016\n\u001e\u001f\n\u001e\u0015\n\u001b\u001a\u001d\n\u0019\u001d\n\u001f\n\u001e\u001f\n\u001c\u001f\n\u001b\u001f\n\u001a\u0019\u0018\u0017\u0016\n\u001e\u001f\n\u001d\u0015\n\u0015\u001f\n\u001e\u001f\n\u001d\u0015\n\u001e\u0015\n\u0014\u0013\u0012\n\u0014\u0014\u0011\u0012\n\u0010\u000f\u000e\u0016\r\f\u000b\n\t\u0017\u0019\b\u0007\u0018\f\u0006\u0005\u0004\n\u001c\u001f \u001c\u001b\n\u001f\u001e\u001c\u0018\n\u001c\u001b\u001f\u001d\n\u0003\b\u0018\u0016\n\u001f\n\u001e\u001f\n\u001d\u001f\n\u001c\u001f\n\u001b\u001f\n\u0015\u001f\n\u0002\u001f\n\u001a\u0019\u0018\u0017\u0016\n\u0015\n\u001e\u001f\n\u001e\u0015\n\u001d\u0015\n\u0018\u001f\u001d\n\u001f\u001d\n\u001b\u001e\u001d\n\u001b\u001b\u001d\n\u0005\t\u0019\u0018\r\f\u0001\n\u001f\n\u001e\u001f\n\u001d\u001f\n\u001c\u001f\n\u001b\u001f\n\u0015\u001f\n\u0002\u001f\n\u001a\u0019\u0018\u0017\u0016\n\u0015\n\u001e\u001f\n\u001e\u0015\n\u001d\u0015\n\u0018\u0019\u001d\n\u001b\u0018\u001d\n\u001b\u001c\u001d\n\u001b\u001e\u001d\n\u007f\u0014\u0081\u001a\u0014\u001e\u008d\u0015\n\u001d\u001f\n\u0005\u0018\u008f\u0090\u0018\u009d\u0018\u0090\u0019\n\u0006\t\u0004\n\u0006\u00a0\u0004\n\u0006\u00ad\u0004\n\u0005\t\u0019\u0018\r\f\u0001\nFigure 9. User preference study (SD1.5). (a) Comparison of our approach to the top-performing teacher configurations. (b) Comparison to\nthe teacher model with DPM-Solver for the same average number of steps. (c) Comparison to the refinement strategy without the adaptive\nstep for the same average number of steps. Top row: LAION-Aesthetic text prompts. Bottom row: COCO2014 text prompts. For our\nadaptive approach, we use the refinement strategy (R).\nAlso, we evaluate user preferences using side-by-side\ncomparisons conducted by professional assessors. We select\n600 random text prompts from the COCO2014 validation\nset and 600 from LAION-Aesthetics2. More details on the\nhuman evaluation pipeline are provided in D.1.\n2https://laion.ai/blog/laion-aesthetics/\nConfiguration. For our adaptive approach, we consider both\nrefinement (R) and regeneration (G) strategies using a sec-\nond order multistep DPM solver [27] and vary the number of\nsampling steps depending on the average inference budget.\nAs a sample estimator E, we consider ImageReward, except\nfor the CLIP score evaluation. For each inference budget,\nwe tune the hyperparameters \u03c3 and \u03c4 on the hold-out prompt\nset. The exact values are provided in D.2.\n6\n\u001f\u001e\u001d\u001c\u001b\n\u001a\u0019\n\u0018\u0019\n\u0017\u0019\n\u0016\u0019\n\u0015\u0019\n\u0014\u0019\n\u0013\u0012\u0011\u0010\n\u0018\u0016\n\u0018\u0018\n\u0018\u0019\n\u001a\u000f\n\u000e\r\u0012\f\u000b\u001b\n\t\b\u001d\u0010\n\u0019\u0007\u0016\u0017\u0015\n\u0019\u0007\u0016\u001a\u0015\n\u0019\u0007\u0016\u0018\u0019\n\u0019\u0007\u0016\u0017\u0019\n\u0019\u0007\u0016\u0018\u0015\n\u001f\u001e\u001d\u001c\u001b\n\u001a\u0019\n\u0018\u0019\n\u0017\u0019\n\u0016\u0019\n\u0015\u0019\n\u0014\u0019\n\u0012\u0006\u0005\u0004\u001d\u0003\u001d\u0002\u0005\b\u0001\u0010\n\u001f\u001e\u001d\u001c\u001b\n\u001a\u0019\n\u0018\u0019\n\u0017\u0019\n\u0016\u0019\n\u0015\u0019\n\u0019\u0007\u0017\n\u0019\u0007\u0018\n\u0019\u0007\u001a\n\u0019\u0007\u0019\n\u2212\u0019\u0007\u001a\n\u2212\u0019\u0007\u0018\n\u2212\u0019\u0007\u0017\n\u0014\u0019\n\u0019\u0007\u0016\u001a\u0019\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u0015\u0014\u0013\u0012\u001a\u0011\u0010\u000f\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u0015\u0014\u0013\u0012\u001a\u0011\u000e\u000f\n\r\r\f\u000b\n\r\n\u000b\n\u0010\u0012\t\b\u0012\u0007\u0012\b\u0015\n\u0010\u0012\u001c\u0015\u0017\u001d\u0015\u001a\u0006\u0017\u0007\u0016\u0005\u0014\b\u0004\nFigure 10. Automated evaluation (SD1.5). Comparison of the\nFID, CLIP and ImageReward scores for different number of sam-\npling steps on 5K text prompts from COCO2014. The proposed\ncollaborative approach outperforms all the baselines. The adaptive\npipeline with the regeneration strategy (G) demonstrates higher\ntextual alignment (CLIP score), while the refinement strategy (R)\nimproves the image fidelity (FID).\n\u001f\u001e\u001d\u001c\n\u001b\u001e\u001e\u001a\u001d\u001c\n\u0019\u0018\u0017\u0016\u0015\u0014\u001b\u0013\u0012\u0011\u0010\u000f\u000e\r\u0014\f\u000b\n\t\u000f\r\u0016\n\u000b\u0012\u0010\r\u0015\u0014\b\n50\n40\n30\n20\n10\n\u001f\u0010\r\u0011\u0016\n10\n50\n4\n4\n14\n0\nFigure 11. User preference study (SDXL). Left: Comparison\nof the adaptive approach with CD-SDXL to the top-performing\nteacher setup. Right: Comparison of the adaptive approach with\nADD-XL to ADD-XL for the same average number of steps.\nBaselines. We consider the teacher performance as our main\nbaseline and use DDIM [46] for 50 steps and a second order\nmultistep DPM solver [27] for lower steps. In addition, we\ncompare to the refining strategy on top of all student sam-\nples, without the adaptive step. This baseline is inspired by\nthe recent results [34] demonstrating the advantages of the\nrefinement stage itself. Also, we provide the comparison\nwith Restart Sampling [54].\nResults. The quantitative and qualitative results are pre-\nsented in Figures 9, 10 and Figure 15, respectively. Ac-\ncording to the automatic metrics, our approach outperforms\nall the baselines. Specifically, in terms of CLIP scores, the\nadaptive regeneration strategy demonstrates superior perfor-\nmance compared to the refining-based counterpart. On the\nother hand, the adaptive refining strategy is preferable in\n\u001f\u001e\u001d\u001c\n\u001b\u001a\u0019\u0018\u0017\u001a\u001d\n\u001b\u0016\u001a\u001c\n\u22129\n0\n6\n13\n19\n33\n\u0015\u0018\u0018\u001e\u001d\u0019\u0018\u0014\n0.5\n0.59\n0.65\n0.75\n0.85\n1.0\n\u0013\u0016\u0012\u001c\u0011\u0010\u000f\n50\n40\n30\n20\n10\n\u000e\u001e\u001d\u001d\u001a\u0012\r\u0010\u001c\r\u0019\r\u001a\nFigure 12. User preferences for different accuracy levels of the no-\nreference decision-making procedure. Current state represents the\nresults using ImageReward. The results for higher accuracy rates\ndemonstrate the future gains if the oracle performance improves.\nterms of FID scores. We assume that the refinement strategy\nessentially improves the image fidelity and does not signifi-\ncantly alter the textual alignment due to the relatively small\nrollback values. In terms of ImageReward, both adaptive\nstrategies perform equally.\nIn the human evaluation, we consider two nominations:\ni) acceleration, where our approach aims to reach the per-\nformance of SD1.5 using 50 DDIM steps or 25 DPM steps;\nii) quality improvement, where the adaptive method is com-\npared to the baselines for the same average number of steps.\nThe results for the acceleration nomination are presented\nin Figure 9a. The proposed method achieves the teacher\nperformance for 5\u00d7 and up to 2.5\u00d7 fewer steps compared\nto DDIM50 and DPM25, respectively. The results for the\nsecond nomination (Figure 9b,c) confirm that our approach\nconsistently surpasses alternatives using the same number\nof steps on average. In particular, the adaptive method im-\nproves the generative performance by up to 19% and 20%\ncompared to the teacher and refining strategy without the\nadaptive step, respectively.\nEffect of oracle accuracy. The potential bottleneck of our\napproach is a poor correlation of existing text-to-image au-\ntomated estimators with human preferences. For example,\nImageReward usually exhibits up to 65% agreement with an-\nnotators. Moreover, it remains unclear what oracle accuracy\ncan be achieved with no-reference decision-making, even if\nthe estimator provides the perfect agreement.\nIn Figure 12, we conduct a synthetic experiment examin-\ning the effect of the oracle accuracy on our scheme perfor-\nmance to reveal its future potential. We compare the adaptive\nrefinement method (10 steps) to SD1.5 (50 steps) manually\nvarying the oracle accuracy. We observe significant future\ngains even for the 75% accuracy rate.\nSDXL results. In addition to SD1.5 experiments, we evalu-\nate our pipeline using the recent CD-SDXL [28] and ADD-\nXL [44] which are both distilled from the SDXL-Base\nmodel [34]. Our approach with CD-SDXL stands against\n7\n\u001f\u001e\u001d\u001c\u001b\n50 4\n9 29\n50 4\n4 29\n50 5 10 30\n\u001a\u0019\u0018\u001d\u0017\u0019\u001b\u001e\u0016\u0015\u001b\u0014\u0013\u0017\u001d\u0015\n0.5\n0.4\n0.3\n0.2\n0.1\n0.6\n\u0012\u0011\u0017\u001b\u0010\u0015\u000f\u000e\r\u001c\u001e\u0019\u0018\u001d\u0015\f\u000b\n\u0012\u0011\u0017\u001b\u0010\u0015\u000f\u000e\r\u001c\u001e\u0019\u0018\u001d\u0015\f\t\n\b\u001d\r\u0014\u0007\u001d\u0017\n\u001a\u0019\u001b\u001e\u0019\u0006\u0006\r\u001e\u0019\u0013\u0005\n\u001f\u001e\u001d\u001c\u001e\u001b\u001a\n\u0019\u001e\u001e\u001d\u001b\u001a\n\u0018\u001f\u001e\u001d\u001c\u001e\u0017\u0016\u0015\n0.55\n0.47 0.50 0.52\n0.55\n0.37 0.39\n0.49\n0.56 0.55 0.53 0.54\nFigure 13. Diversity human scores collected for different methods.\nFigure 14. Image complexity of the CD-SD1.5 and SD1.5 samples\nin terms of ICNet [7]. Left: Box plot representing the complexity\nquantiles for both models. Right: Distribution of individual com-\nplexity values. Each dot corresponds to a pair of samples generated\nfor the same prompt and initial noise. The distilled model only\nslightly simplifies the teacher distribution.\nthe top-performing SDXL setting: 50 steps of the DDIM\nsampler. For ADD-XL, we provide the comparison for 4\nsteps where ADD-XL has demonstrated exceptionally strong\ngenerative performance in terms of human preference [44].\nIn both settings, our approach uses the adaptive refinement\nstrategy with the UniPC solver [58]. Note that both the\nSDXL-Base and SDXL-Refiner [34] models can be used for\nrefinement. In our experiments, we observe that the refiner\nsuits slightly better for fixing minor defects while the teacher\nallows more pronounced changes. Thus, we use the refiner\nfor low \u03c3 values and the base model for the higher ones.\nMore setup details are provided in D.3.\nThe results are presented in Figure 11. We observe that\nthe adaptive approach using CD-SDXL achieves the quality\nof the SDXL model, being 5\u00d7 more efficient on average.\nMoreover, the proposed scheme improves the performance\nof ADD-XL by 14% in terms of human preference.\n4.2. Distribution diversity\nIn the proposed teacher-student collaboration, the oracle\naims to accept high-quality and well-aligned student samples\nbut does not control the diversity of the resulting image\ndistribution. Therefore, if the student exhibits severe mode\ncollapse or oversimplifies the teacher samples, the adaptive\npipeline will likely inherit these issues to some extent.\nIn this section, we investigate this potential problem\nfor several existing distilled text-to-image models. Specifi-\nMethod\nSteps\nDINOv2 \u2193\nImageReward \u2191\nCD-SD1.5\n5\n0.674 \u00b1 .004\n0.192 \u00b1 .037\nSD1.5, DDIM\n50\n0.665 \u00b1 .007\n0.183 \u00b1 .024\nSD1.5, DDIM\n25\n0.665 \u00b1 .002\n0.183 \u00b1 .022\nSD1.5, DPM\n25\n0.667 \u00b1 .005\n0.179 \u00b1 .020\nRefinement\n30\n0.710 \u00b1 .005\n0.383 \u00b1 .033\nOurs\n30\n0.669 \u00b1 .006\n0.281 \u00b1 .008\nTable 1. Comparison of SDEdit using different approaches in terms\nof reference preservation DINOv2 and editing quality (ImageRe-\nward) for the strength 0.6.\ncally, we consider consistency distillation [28] for SD1.5 and\nSDXL [34] models and ADD-XL [44]. Note that ADD-XL\nis a GAN-based distillation method that generates exception-\nally realistic samples but has evidence to provide poor image\ndiversity for the given text prompt [44].\nWe estimate the diversity of generated images by con-\nducting a human study. In more detail, given a text prompt\nand a pair of samples generated from different initial noise\nsamples, assessors are instructed to evaluate the diversity of\nthe following attributes: angle of view, background, main\nobject and style. For each model, the votes are collected\nfor 600 text prompts from COCO2014 and aggregated into\nthe scores from 0 to 1, higher scores indicate more diverse\nimages. The results are presented in Figure 13.\nCD-SDXL demonstrates significantly better diversity\nthan ADD-XL but still produces less various images com-\npared to the SDXL teacher. CD-SD1.5 performs similarly\nto the SD1.5 teacher. Also, both adaptive strategies increase\nthe diversity of the SDXL student models, especially the\nregeneration one. We also provide a few samples generated\nwith different distilled models in D.5.\nThen, we address whether the distilled models tend to\noversimplify the teacher distribution. In this experiment, we\nevaluate SD1.5 using DDIM for 50 steps and the correspond-\ning CD-SD1.5 using 5 sampling steps. In Figure 14, we\ncompare the complexity of the student and teacher samples\nin terms of the ICNet score [7]. We observe that CD-SD1.5\nimperceptibly simplifies the teacher distribution.\nTo sum up, in our experiments, the CD-based models\nprovide the decent distribution diversity that can be further\nimproved with the proposed adaptive approach.\n4.3. Text-guided image editing\nThis section applies our approach for text-guided image\nediting using SDEdit [29]. We add noise to an image, alter\nthe text prompt and denoise it using the student model first. If\nthe edited image does not exceed the threshold \u03c4, the teacher\nmodel is used for editing instead. In the editing setting, we\nobserve that the refinement strategy significantly reduces\nsimilarity with the reference image due to the additional\nnoising step. Thus, we apply the regeneration strategy only.\n8\nFigure 15. Visual examples produced with our approach and the top-performing teacher (SD1.5) configuration. Top: Text-guided image\nediting with SDEdit [29]. Bottom: Controllable image generation with Canny edges and semantic segmentation masks using ControlNet [56].\n\u001f\u001e\u001d\u001c\u001b\u001a\u001b\u0019\u001d\u0018\u0017\u0016\n \u0015\u001f\u0014\u0013\u0012\u0011\u0016\n\u0010\u0015\u000f\u000e\u0015\r\f\u000b\u0016\n\u000b\n\t\b\u001b\u0007\t\n\u000e\u0015\r\f\u000b\u0016\n\u000b\u0006\n\t\b\u001b\u0007\t\n\u0013\u0005\u0018\t\u0016\n\u0004\u0006\n\t\b\u001b\u0007\t\n\u22120.5\n\u22120.25\n0.0\n0.25\n0.6\n0.4\nFigure 16. SDEdit performance for different strength values in\nterms of reference preservation (DINOv2) and editing quality (IR).\nIn these experiments, the SD1.5 and CD-SD1.5 mod-\nels are considered. As performance measures, we use Im-\nageReward for editing quality and DINOv2 [33] for refer-\nence preservation. For evaluation, 100 text prompts from\nCOCO2014 are manually prepared for the editing task.\nResults. Table 1 provides evaluation results for a SDEdit\nnoising strength value 0.6. The proposed method demon-\nstrates a higher ImageReward score compared to the base-\nlines with similar reference preservation scores. In addition,\nwe present the performance for different editing strength\nvalues in Figure 16. Our approach demonstrates a better\ntrade-off between reference preservation and editing quality.\nWe provide qualitative results in Figure 15.\n4.4. Controllable image generation\nFinally, we consider text-to-image generation using Canny\nedges and semantic segmentation masks as an additional con-\ntext and use ControlNet [56] for this task. We use ControlNet\npretrained on top of SD1.5 and directly plug it into the dis-\ntilled model (CD-SD1.5). Interestingly, the model pretrained\nfor the teacher model fits the student model surprisingly well\nwithout any further adaptation.\nFor the teacher model, the default ControlNet sampling\nconfiguration is used: 20 sampling steps of the UniPC [58]\nsolver. In our adaptive approach, we use the refinement\nstrategy with 10 steps of the same solver. For performance\nevaluation, we conduct the human preference study for each\ntask on 600 examples and provide more details in D.4.\nResults. According to the human evaluation, our approach\noutperforms the teacher (20 steps) by 19% (9 steps) and\n4% (11 steps) for Canny edges and semantic segmentation\nmasks, respectively. The visual examples are in Figure 15.\n5. Conclusion\nThis work investigates the performance of the distilled text-\nto-image models and demonstrates that they may consistently\noutperform the teachers on many samples. We design an\nadaptive text-to-image generation pipeline that takes advan-\ntage of successful student samples and, in combination with\nthe teacher model, outperforms other alternatives for low\nand high inference budgets. Importantly, text-to-image au-\ntomated estimators and distillation methods still have room\nfor further development. Therefore, we believe the proposed\napproach may be even in higher demand when future works\nmake a significant step forward in these directions.\n9\nReferences\n[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-\naming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli\nLaine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion\nmodels with an ensemble of expert denoisers. arXiv preprint\narXiv:2211.01324, 2022. 2\n[2] David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap,\nShuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbot, and\nEric Gu. Tract: Denoising diffusion models with transitive\nclosure time-distillation. arXiv preprint arXiv:2303.04248,\n2023. 2\n[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023. 2\n[4] Defang Chen, Zhenyu Zhou, Jian-Ping Mei, Chunhua Shen,\nChun Chen, and Can Wang. A geometric perspective on\ndiffusion models. arXiv preprint arXiv:2305.19947, 2023. 4,\n1, 2\n[5] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt:\nHow to use large language models while reducing cost and\nimproving performance, 2023. 2\n[6] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell\nWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-\nmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-\ning laws for contrastive language-image learning.\narXiv\npreprint arXiv:2212.07143, 2022. 5\n[7] Tinglei Feng, Yingjie Zhai, Jufeng Yang, Jie Liang, Deng-\nPing Fan, Jing Zhang, Ling Shao, and Dacheng Tao. Ic9600:\nA benchmark dataset for automatic image complexity assess-\nment. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, (01):1\u201317, 2023. 4, 8, 1, 2\n[8] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin\nLi, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong Yin,\nShikun Feng, et al.\nErnie-vilg 2.0: Improving text-to-\nimage diffusion model with knowledge-enhanced mixture-\nof-denoising-experts. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10135\u201310145, 2023. 2\n[9] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai,\nRichard Zhang, Tali Dekel, and Phillip Isola. Dreamsim:\nLearning new dimensions of human visual similarity using\nsynthetic data. arXiv preprint arXiv:2306.09344, 2023. 3\n[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gener-\nation using textual inversion, 2022. 2\n[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Communi-\ncations of the ACM, 63(11):139\u2013144, 2020. 3\n[12] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and\nJoshua M Susskind. Boot: Data-free distillation of denoising\ndiffusion models with bootstrapping. In ICML 2023 Work-\nshop on Structured Probabilistic Inference {\\&} Generative\nModeling, 2023. 2\n[13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium. In\nAdvances in Neural Information Processing Systems. Curran\nAssociates, Inc., 2017. 5\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2\n[15] Alexia Jolicoeur-Martineau, Ke Li, R\u00e9mi Pich\u00e9-Taillefer,\nTal Kachman, and Ioannis Mitliagkas. Gotta go fast when\ngenerating data with score-based models. arXiv preprint\narXiv:2105.14080, 2021. 2\n[16] Ian T Jolliffe and Jorge Cadima. Principal component analy-\nsis: a review and recent developments. Philosophical trans-\nactions of the royal society A: Mathematical, Physical and\nEngineering Sciences, 374(2065):20150202, 2016. 1\n[17] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli\nShechtman, Sylvain Paris, and Taesung Park. Scaling up gans\nfor text-to-image synthesis. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR),\n2023. 2\n[18] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. Advances in Neural Information Processing Systems,\n35:26565\u201326577, 2022. 2\n[19] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Con-\nference on Computer Vision and Pattern Recognition 2023,\n2023. 2\n[20] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Mu-\nrata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki\nMitsufuji, and Stefano Ermon. Consistency trajectory models:\nLearning probability flow ode trajectory of diffusion. arXiv\npreprint arXiv:2310.02279, 2023. 2, 1\n[21] Diederik P Kingma and Max Welling. Auto-encoding vari-\national bayes. International Conference on Learning Repre-\nsentations, 2014. 2\n[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part\nV 13, pages 740\u2013755. Springer, 2014. 3, 5\n[23] Enshu Liu, Xuefei Ning, Zinan Lin, Huazhong Yang, and Yu\nWang. Oms-dpm: Optimizing the model schedule for diffu-\nsion probabilistic models. arXiv preprint arXiv:2306.08860,\n2023. 2\n[24] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight\nand fast: Learning to generate and transfer data with rectified\nflow. arXiv preprint arXiv:2209.03003, 2022. 2, 4, 1\n[25] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and\nQiang Liu. Instaflow: One step is enough for high-quality\ndiffusion-based text-to-image generation.\narXiv preprint\narXiv:2309.06380, 2023. 2, 4\n[26] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffu-\n10\nsion probabilistic model sampling in around 10 steps. arXiv\npreprint arXiv:2206.00927, 2022. 2, 5\n[27] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver++: Fast solver for guided\nsampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022. 2, 6, 7\n[28] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang\nZhao.\nLatent consistency models:\nSynthesizing high-\nresolution images with few-step inference. arXiv preprint\narXiv:2310.04378, 2023. 2, 7, 8, 1, 3\n[29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun\nWu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image\nsynthesis and editing with stochastic differential equations.\narXiv preprint arXiv:2108.01073, 2021. 2, 3, 8, 9\n[30] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14297\u201314306, 2023. 2, 1\n[31] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch,\nand Daniel Cohen-Or. Null-text inversion for editing real\nimages using guided diffusion models.\narXiv preprint\narXiv:2211.09794, 2022. 2\n[32] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 1, 2\n[33] Maxime Oquab, Timoth\u00e9e Darcet, Theo Moutakanni, Huy V.\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell\nHowes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li,\nWojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas,\nGabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal,\nPatrick Labatut, Armand Joulin, and Piotr Bojanowski. Di-\nnov2: Learning robust visual features without supervision,\n2023. 9\n[34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,\nTim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach.\nSdxl: improving latent diffusion models for high-resolution\nimage synthesis. arXiv preprint arXiv:2307.01952, 2023. 1,\n2, 5, 7, 8, 3\n[35] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.\nDreamfusion: Text-to-3d using 2d diffusion. In The Eleventh\nInternational Conference on Learning Representations, 2023.\n3\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 5\n[37] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n2\n[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1, 2\n[39] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmenta-\ntion. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 1\n[40] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven gen-\neration. 2022. 2\n[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,\nJay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael\nGontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho-\ntorealistic text-to-image diffusion models with deep language\nunderstanding. Advances in Neural Information Processing\nSystems, 35:36479\u201336494, 2022. 1, 2\n[42] Tim Salimans and Jonathan Ho.\nProgressive distillation\nfor fast sampling of diffusion models.\narXiv preprint\narXiv:2202.00512, 2022. 2\n[43] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and\nTimo Aila. StyleGAN-T: Unlocking the power of GANs for\nfast large-scale text-to-image synthesis. 2023. 2\n[44] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin\nRombach. Adversarial diffusion distillation, 2023. 2, 7, 8, 3\n[45] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes,\nAarush Katta, Clayton Mullis, Mitchell Wortsman, et al.\nLaion-5b: An open large-scale dataset for training next gen-\neration image-text models. Advances in Neural Information\nProcessing Systems, 35:25278\u201325294, 2022. 3, 5, 1\n[46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. arXiv preprint arXiv:2010.02502,\n2020. 2, 7\n[47] Yang Song and Prafulla Dhariwal.\nImproved tech-\nniques for training consistency models.\narXiv preprint\narXiv:2310.14189, 2023. 2\n[48] Yang Song and Stefano Ermon. Generative modeling by\nestimating gradients of the data distribution. Advances in\nneural information processing systems, 32, 2019. 2\n[49] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\narXiv preprint arXiv:2011.13456, 2020. 2, 1\n[50] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.\nConsistency models. 2023. 2, 3, 5, 1\n[51] Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang,\nGefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, and\nFerhan Ture. What the DAAM: Interpreting stable diffusion\nusing cross attention. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 2023. 1\n11\n[52] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. Sketch-\nguided text-to-image diffusion models. In ACM SIGGRAPH\n2023 Conference Proceedings, pages 1\u201311, 2023. 2\n[53] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai\nLi, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward:\nLearning and evaluating human preferences for text-to-image\ngeneration. arXiv preprint arXiv:2304.05977, 2023. 3, 5\n[54] Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong\nTian, Ziming Liu, and Tommi Jaakkola.\nRestart sam-\npling for improving generative processes. arXiv preprint\narXiv:2306.14878, 2023. 7\n[55] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuo-\nfan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image\ngeneration via large mixture of diffusion paths. arXiv preprint\narXiv:2305.18295, 2023. 2\n[56] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 2, 9\n[57] Qinsheng Zhang and Yongxin Chen. Fast sampling of dif-\nfusion models with exponential integrator. arXiv preprint\narXiv:2204.13902, 2022. 2\n[58] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and\nJiwen Lu. Unipc: A unified predictor-corrector framework\nfor fast sampling of diffusion models. NeurIPS, 2023. 2, 8, 9,\n3\n[59] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler,\nAdela Barriuso, and Antonio Torralba. Semantic understand-\ning of scenes through the ade20k dataset. Int. J. Comput. Vis.,\n127(3):302\u2013321, 2019. 3\n12\nYour Student is Better Than Expected:\nAdaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models\nSupplementary Material\nA. CD-SD1.5 implementation details\nIn this work, we develop consistency distillation (CD) for\nStable Diffusion following the official implementation [50].\nFor training, we prepare a subset of LAION2B [45], which\nconsists of 80M image-text pairs. As a teacher sampler, we\nconsider DDIM-solver using 50 sampling steps and Variance-\nPreserving scheme [49]. We use the teacher UNet architec-\nture as a student model and initialize it with the teacher\nparameters. Classifier-free guidance is applied to the dis-\ntilled model directly without merging it into the model as\ndone in [30]. During training, we uniformly sample the\nguidance strength from 1 to 8. Thus, our model supports dif-\nferent guidance scales during sampling. We train the student\nfor \u223c200K iterations on 8 A100 GPUs using the following\nsetting: 512 batch size; 0.95 EMA rate; 1e\u22125 fixed learning\nrate; L2 uniformly weighted distillation loss calculated in\nthe latent space of the VAE encoder. During inference, the\nmultistep stochastic sampler [50] is used to generate images.\nIn most of our experiments, we use 5 sampling steps.\nNote that we use our implementation of consistency dis-\ntillation for SD because, when most experiments were con-\nducted, there were no publicly available implementations.\nB. Analysis\nB.1. Calculation details\nImage complexity. To calculate the image complexity, we\nuse the recent ICNet model [7]. This model is learned on\na large-scale human annotated dataset. Each image corre-\nsponds to a complexity score ranging from 0 (the simplest) to\n1 (the most complex). In Figure 17, we provide examples of\nStable Diffusion samples with the lowest and highest image\ncomplexity. More complex images usually depict multiple\nentities, often including people, and intricate backgrounds.\nText influence. We calculate the influence of a text prompt\non student generation by using cross-attention between token\nembeddings and intermediate image representations. Fol-\nlowing [51], we collect cross-attention maps for all diffusion\nsteps and UNet [39] layers. Then, the average attention score\nis calculated for each text token. Finally, the highest value\namong all tokens is returned.\nTrajectory curvature is estimated according to the recent\nwork [4]. First, we calculate the trajectory deviations as L2\ndistance from the denoised prediction at a time step t to the\nstraight line passing through the denoising trajectory end-\npoints. The trajectory curvature corresponds to the highest\ndeviation over all time steps.\nAlgorithm 1: teacher-student adaptive collaboration\nInput: E, S, T\u2212 estimator, student, teacher;\nI\u2212 input; \u03c3, \u03c4\u2212 rollback value and cut-off threshold.\n1 \u02c6O = S(I)\n// Student prediction\n2 if E( \u02c6O) < \u03c4 then\n3\nStrategy 1:\n// Refinement\n4\n\u02c6O\u03c3 = \u221a1 \u2212 \u03c3 \u00b7 \u02c6O + \u221a\u03c3 \u00b7 Z, Z \u223c N(0, 1)\n5\n\u02c6O = T(I, \u02c6O\u03c3)\n6\nStrategy 2:\n// Regeneration\n7\n\u02c6O = T(I)\n8 return \u02c6O\nIn Figure 18 (Left), we visualize two denoising trajec-\ntories corresponding to high and low curvatures. We ap-\nply PCA [16] to reduce the dimensionality of the denoised\npredictions. In addition, Figure 18 (Right) demonstrates\ntrajectory deviations for different time steps. The highest\ndeviations typically occur closer to the end of the denoising\ntrajectory.\nB.2. Other distilled text-to-image models\nHere, we conduct a similar analysis as in Section 3 for consis-\ntency distillation on Dreamshaper v7 [28] and architecture-\nbased distillation3.\nIn contrast to the few-step distillation approaches [20,\n24, 28, 30, 50], the architecture-based distillation removes\nsome UNet layers from the student model for more efficient\ninference and trains it to imitate the teacher using the same\ndeterministic solver and number of sampling steps.\nIn Figures 19, 20, we show that both methods can produce\nsamples that significantly differ from the teacher ones for\nthe same text prompt and initial noise. Then, Figures 21, 22\nconfirm that other observations in Section 3 remain valid for\nthe Dreamshaper and architecture-based students as well.\n3https://huggingface.co/docs/diffusers/using-\ndiffusers/distilled_sd\n1\nFigure 17. SD1.5 samples of different complexity according to the ICNet model [7].\nFigure 18. Left: Two examples of diffusion model trajectories with high and low curvatures. Right: Trajectory deviations according to [4].\nC. Cut-off threshold tuning\nThe cut-off threshold \u03c4 is used for the adaptive selection\nof student samples. It corresponds to the k-th percentile\nof the metric values calculated on validation student sam-\nples. 600 and 300 samples are generated for tuning on the\nCOCO2014 and LAION-Aesthetics datasets, respectively.\nNote that the prompts used for tuning do not overlap with\nthe test ones. Then, we calculate an individual score, e.g.,\nIR score, for each validation student sample and select the\npercentile based on an average inference budget or target\nmetric value. For example, suppose we select the percentile\ngiven a 15 step budget and intend to perform 5 student steps\nand 20 steps for the improvement strategy. In this case, we\nhave to select \u03c4 as a 50-th percentile, which results in the\nfinal average number of steps: 5 + 0.5 \u00b7 20 = 15.\nDuring inference, we perform the adaptive selection as\nfollows: if the score of the student sample exceeds \u03c4, we con-\nsider that this sample might be superior to the teacher one and\nkeep it untouched. Otherwise, we perform an improvement\nstep using the teacher model (refinement or regeneration).\nThe proposed pipeline is presented in Algorithm 1.\nD. Experiments\nD.1. Human evaluation\nTo evaluate the text-to-image performance, we use the side-\nby-side comparison conducted by professional annotators.\nBefore the evaluation, all annotators pass the training and\nundergo the preliminary testing. Their decisions are based\non the three factors: textual alignment, image quality and\naesthetics (listed in the priority order). Each side-by-side\ncomparison is repeated three times by different annotators.\nThe final result corresponds to the majority vote.\nD.2. Experimental setup (SD1.5)\nThe exact hyperparameter values and number of steps used\nfor the automated estimation (FID, CLIP score and ImageRe-\nward) of the adaptive refinement strategy in Table 2. The\nadaptive regeneration uses the same cut-off thresholds and\nnumber of steps, but the rollback value, \u03c3, is equal to 1. The\nvalues used for human evaluation are presented in Table 3.\n2\nD.3. Experimental setup (SDXL)\nWe evaluate two distilled models: CD-SDXL [28] and ADD-\nXL [44]. For the first evaluation, we use 4 steps of the CD-\nSDXL and then apply 12 adaptive refinement steps using the\nteacher model (SDXL-Base [34]) with the UniPC solver [58].\nWe compare our pipeline to the default teacher configuration:\n50 DDIM steps. For the second evaluation, we perform 2\nsteps of the ADD-XL and 4 steps of the SDXL-Refiner [34]\nfor the adaptive refinement strategy. We compare to 4 ADD-\nXL steps as this setting outperformed SDXL-Base in terms\nof image quality and textual alignment [34]. The exact hy-\nperparameter values and number of steps used for human\nevaluation are in Table 4.\nWe found that SDXL-Refiner performs slightly better\nthan the base model for small refinement budgets (e.g., 4).\nThe refiner typically helps to improve fine-grained details,\ne.g., face attributes or background details.\nHowever, it\nfaces difficulties in providing global changes and sometimes\nbrings artifacts for large rollback values, \u03c3. Thus, we use\nthe SDXL-Base teacher for more refinement steps (e.g., 12).\nMetric\n\u03c3\nk\nSteps\nCD\nRefinement\nAdaptive\nImageReward\n0.4\n60\n5\n5\n8\n0.55\n60\n5\n10\n11\n0.7\n60\n5\n15\n14\n0.7\n60\n5\n25\n20\n0.7\n60\n5\n35\n26\n0.7\n60\n5\n45\n32\nCLIP score\n0.5\n60\n5\n5\n8\n0.7\n60\n5\n10\n11\n0.75\n60\n5\n15\n14\n0.75\n60\n5\n25\n20\n0.75\n60\n5\n35\n26\n0.75\n60\n5\n45\n32\nFID\n0.75\n40\n3\n5\n5\n0.7\n70\n3\n15\n14\nTable 2. Hyperparameter values used for the automated evaluation\n(SD 1.5), Figure 10.\nMetric\n\u03c3\nk\nSteps\nCD\nRefinement\nAdaptive\nHuman evaluation\n0.7\n50\n5\n10\n10\n0.7\n50\n5\n20\n15\n0.7\n50\n5\n40\n25\nTable 3. Hyperparameter values used for the user preference study\n(SD 1.5), Figure 9.\nMetric\n\u03c3\nk\nSteps\nADD-XL\nRefinement\nAdaptive\nHuman evaluation\n0.4\n50\n2\n4\n4\nCD-SDXL\nRefinement\nAdaptive\nHuman evaluation\n0.85\n70\n4\n12\n13\nTable 4. Hyperparameter values used for the user preference study\n(SDXL), Figure 11.\nD.4. Controllable generation\nFor both tasks, we use the adaptive refinement strategy and\nset the rollback value \u03c3 to 0.5. We perform 5 steps for\nthe student generation and 10 steps for the refinement with\nthe UniPC solver. The cut-off thresholds correspond to 70\nand 50 ImageReward percentiles for the mask-guided and\nedge-guided generation, respectively. We select random 600\nimage-text pairs from the COCO2014 validation set for the\nedge-guided generation. For the mask-guided generation,\nwe use 600 semantic segmentation masks from the ADE20K\ndataset [59] and use the category names as the text prompts.\nTo evaluate the performance, we conduct the human study\nsimilar to D.1.\nD.5. Distribution diversity\nIn Figure 27, we illustrate the diversity of images generated\nwith different distilled text-to-image models (ADD-XL, CD-\nSDXL and CD-SD1.5). Each column corresponds to a dif-\nferent initial noise (seed). We notice that ADD-XL exhibits\nthe lowest diversity compared to the CD-based counterparts.\nD.6. ImageReward inference costs\nWe compare the absolute inference times of a single Stable\nDiffusion UNet step with classifier-free guidance against the\nImageReward forward pass. We measure the model perfor-\nmance in half precision on a single NVIDIA A100 GPU. The\nbatch size is 200 to ensure 100% GPU utility for both mod-\nels. The performance is averaged over 100 independent runs.\nImageReward demonstrates 0.26s while the single step of\nStable Diffusion takes 3s. In the result, we consider the adap-\ntive step costs negligible since ImageReward is more than\n10\u00d7 faster than a single generation step of Stable Diffision.\nD.7. Additional visualizations\nIn Figure 23, we present more examples where the student\noutperforms its teacher according to the human evaluation.\nFigures 24, 25, 26 provide more qualitative comparisons of\nour approach for different tasks.\n3\n(a)\n(b)\nFigure 19. Visual examples of similar (Left) and dissimilar (Right) teacher and student samples for SD1.5 (a) and Dreamshaper v7 (b).\n4\nFigure 20. Visual examples of similar (Left) and dissimilar (Right) teacher and student samples for the architecture-based distillation.\n5\nFigure 21. Analysis results for the architecture-based distillation.\nFigure 22. Analysis results for the consistency distillation on Dreamshaper v7.\n6\nFigure 23. Additional examples where the student (CD-SD1.5) outperforms its teacher (SD1.5) according to the human evaluation.\n7\nFigure 24. Additional image editing results produced with our approach.\n8\nFigure 25. Additional results on Canny edge guided image generation with our approach.\n9\nFigure 26. Additional results on segmentation mask guided image generation with our approach.\n10\nFigure 27. Visual examples generated with various distilled text-to-image models for different seed values. CD-based students generate\nmore diverse images than ADD-XL.\n11\n"
  },
  {
    "title": "VolumeDiffusion: Flexible Text-to-3D Generation with Efficient Volumetric Encoder",
    "link": "https://arxiv.org/pdf/2312.11459.pdf",
    "upvote": "5",
    "text": "VolumeDiffusion: Flexible Text-to-3D Generation with\nEfficient Volumetric Encoder\nZhicong Tang1\nShuyang Gu2\nChunyu Wang2\nTing Zhang2\nJianmin Bao2\nDong Chen2\nBaining Guo2\n1Tsinghua University\n2Microsoft Research\ntzc21@mails.tsinghua.edu.cn\n{shuyanggu,chnuwa,ting.zhang,jianbao,doch,bainguo}@microsoft.com\nAbstract\nThis paper introduces a pioneering 3D volumetric\nencoder designed for text-to-3D generation. To scale up the\ntraining data for the diffusion model, a lightweight network\nis developed to efficiently acquire feature volumes from\nmulti-view images. The 3D volumes are then trained on a\ndiffusion model for text-to-3D generation using a 3D U-Net.\nThis research further addresses the challenges of inaccu-\nrate object captions and high-dimensional feature volumes.\nThe proposed model, trained on the public Objaverse\ndataset, demonstrates promising outcomes in producing di-\nverse and recognizable samples from text prompts. Notably,\nit empowers finer control over object part characteristics\nthrough textual cues, fostering model creativity by seam-\nlessly combining multiple concepts within a single object.\nThis research significantly contributes to the progress of\n3D generation by introducing an efficient, flexible, and\nscalable representation methodology. Code is available at\nhttps://github.com/tzco/VolumeDiffusion.\n1. Introduction\nText-to-image diffusion models [40] have seen significant\nimprovements thanks to the availability of large-scale text-\nimage datasets such as Laion-5B [42]. This success sug-\ngests that scaling up the training data is critical for achiev-\ning a \u201cstable diffusion moment\u201d in the challenging text-to-\n3D generation task. To achieve the goal, we need to de-\nvelop a 3D representation that is efficient to compute from\nthe massive data sources such as images and point clouds,\nand meanwhile flexible to interact with text prompts at fine-\ngrained levels.\nDespite the increasing efforts in 3D generation, the\noptimal representation for 3D objects remains largely\nunexplored.\nCommonly adopted approaches include\nTri-plane [14, 47] and implicit neural representations\n(INRs) [20]. However, Tri-plane have been only validated\non objects with limited variations such as human faces due\nto the inherent ambiguity caused by factorization.\nThe\nglobal representation in INR makes it hard to interact with\ntext prompts at the fine-grained object part level, constitut-\ning a significant limitation for generative models.\nIn this work, we present a novel 3D volumetric represen-\ntation that characterizes both the texture and geometry of\nsmall parts of an object using features in each voxel, similar\nto the concept of pixels in images. Differing from previous\napproaches such as [3, 28], which require additional images\nas input, our method allows us to directly render images of\ntarget objects using only their feature volumes. Meanwhile,\nthe feature volumes encode generalizable priors from image\nfeatures, enabling us to use a shared decoder for all objects.\nThe above advantages make the representation well-suited\nfor generation tasks.\nTo scale up the training data for the subsequent diffu-\nsion model, we propose a lightweight network to efficiently\nacquire feature volumes from multi-view images, bypass-\ning the expensive per-object optimization process required\nin previous approaches [47]. In our current implementation,\nthis network can process 30 objects per second on a single\nGPU, allowing us to acquire 500K models within hours.\nIt also allows extracting ground-truth volumes on-the-fly\nfor training diffusion models which eliminates the storage\noverhead associated with feature volumes. In addition to\nthe efficiency, this localized representation also allows for\nflexible interaction with text prompts at fine-grained object\npart level. This enhanced controllability paves the way for\ncreative designs by combining a number of concepts in one\nobject.\nWe train a diffusion model on the acquired 3D volumes\nfor text-to-3D generation using a 3D U-Net [41]. This is a\nnon-trivial task that requires careful design. First, the ob-\nject captions in the existing datasets [6, 7] are usually inac-\ncurate which may lead to unstable training if not handled\n1\narXiv:2312.11459v1  [cs.CV]  18 Dec 2023\nproperly.\nTo mitigate their adverse effects, we carefully\ndesigned a novel schedule to filter out the noisy captions,\nwhich notably improves the results. Second, the feature vol-\numes are usually very high-dimensional, e.g. C \u00d7 323 in our\nexperiments which potentially pose challenges when train-\ning the diffusion model. We adopted a new noise sched-\nule that shifted towards larger noise due to increased voxel\nredundancy. Meanwhile, we proposed the low-frequency\nnoise strategy to effectively corrupt low-frequent informa-\ntion when training the diffusion model. We highlight that\nthis structural noise has even more important effects than\nthat in images due to the higher volume dimension.\nWe train our model on the public dataset Objaverse [7]\nwhich has 800K objects (100K after filtering). Our model\nsuccessfully produces diverse and recognizable samples\nfrom text prompts. Compared to Shap\u00b7E [20], our model\nobtains superior results in terms of controlling the charac-\nteristics of object parts through text prompts, although we\nonly use less than 10% of the training data (Shap\u00b7E trained\non several million private data according to their paper). For\ninstance, given the text prompt \u201ca black chair with red legs\u201d,\nwe observe that Shap\u00b7E usually fails to generate red legs.\nWe think it is mainly caused by the global implicit neural\nrepresentation which cannot interact with text prompts at\nfine-grained object part level. Instead, our localized vol-\numetric representation, similar to images, can be flexibly\ncontrolled by text prompts at voxel level. We believe this\nis critical to enhance the model\u2019s creativity by combining a\nnumber of concepts in one object.\n2. Related Work\n2.1. Differentiable Scene Representation\nDifferentiable scene representation is a class of algorithms\nthat encodes a scene and can be rendered into images while\nmaintaining differentiability. It allows scene reconstruction\nby optimizing multi-view images and object generation by\nmodeling the representation distribution. It can be divided\ninto implicit neural representation (INR), explicit represen-\ntation (ER), and hybrid representation (HR).\nNeural Radiance Field (NeRF) [30] is a typical INR that\nencodes a scene as a function mapping from coordinates\nand view directions to densities and RGB colors. Densities\nand RGB colors of points along camera rays are integrated\nto render an image, and the function mapping can be trained\nto match the ground-truth views. While NeRF uses Multi-\nLayer Perceptron (MLP) to encode the underlying scene,\nits success gave rise to many follow-up works that explored\ndifferent representations.\nPlenoxels [10] is an ER that avoids a neural network de-\ncoder and directly encodes a scene as densities and spher-\nical harmonic coefficients at each grid voxel. TensoRF [4]\nfurther decomposes the voxel grid into a set of vectors and\nmatrices as an ER, and values on each voxel are computed\nvia vector-matrix outer products.\nInstant-NGP [32] is an HR that uses a multi-resolution\nhash table of trainable feature vectors as the input embed-\nding of the neural network decoder and obtains results with\nfine details. [2] proposed a Tri-plane HR that decomposes\nspace into three orthogonal planar feature maps, and fea-\ntures of points projected to each plane are added together\nto represent a point in space. DMTet [43] is also an HR\nthat combines a deformable tetrahedral grid and Signed Dis-\ntance Function (SDF) to obtain a precise shape, and the un-\nderlying mesh can be easily exported by Marching Tetrahe-\ndra algorithm [8].\n2.2. 3D Generation\nSome recent works escalate to text-to-3D generation via the\nimplicit supervision of pretrained text-to-image or vision-\nlanguage models. [19] use a contrastive loss of CLIP [39]\ntext feature and rendered image feature to optimize a 3D\nrepresentation. [37, 48] develop the Score Distillation Sam-\npling (SDS) method, leveraging the semantic understanding\nand high-quality generation capabilities of text-to-image\ndiffusion models. [45, 51] further combine CLIP, SDS, ref-\nerence view, and other techniques to push the quality.\nThough these optimization-based methods yield out-\nstanding visual fidelity and text-3D alignment, they suffer\nfrom the cost of time-consuming gradient back-propagation\nand optimization, which could take hours for each text\nprompt. Also, the Janus problem, i.e. multiple faces on one\nobject, the over-saturated color, the instability and sensitiv-\nity to random seed, and the lack of diversity arise from the\ndistillation of text-to-image diffusion models.\nOther works resort to directly generate 3D representa-\ntions and apply explicit supervision. [2, 11, 17, 49] train\nGenerative Adversarial Networks (GANs) [12] on multi-\nview rendered images, which may be limited by the capa-\nbility of generator and discriminator. [1, 31, 47] train text-\nconditioned diffusion models on pre-optimized and saved\nNeRF parameters in the cost of a time-consuming fitting\npreparation and expensive storage.\n[33, 38, 50, 53] fall\nback to a 2-stage manner of first generating geometry-only\npoint clouds and then utilizing off-the-shelf texture painting\nmethods. [25, 27] rely on the 3D structural understanding of\npretrained pose-conditioned diffusion model [26], and per-\nform 3D reconstruction with generated multi-view images,\nwhich may be not 3D consistent. [21, 44] study a simplified\ncategory-specific generation and are unconditional or con-\nditioned on image input. Although [20] successfully maps\ntext to 3D shapes at scale, they choose parameters of NeRF\nMLP as modeling representation, which may be highly non-\nlinear and inflexible for fine-grained text prompts control.\n2\nF\nUnproject \n\u03a6 \u22c5, \ud835\udc51, \ud835\udc5d\n3D \nUNet\n\u2112MSE\n\u2112LPIPS\n\ud835\udf16\nText input\n\u201ca Minecraft piglin\u201d\nStage 1: volume encoding\nCLIP Encoder\n\ud835\udf16\n\u2112MSE\n3D \nUNet\nStage 2: diffusion modeling\n\ud835\udc97\ud835\udc50\n\ud835\udc97\ud835\udc53\nF\nFigure 1. Framework of VolumeDiffusion. It comprises the volume encoding stage and the diffusion modeling stage. The encoder\nunprojects multi-view images into a feature volume and do refinements. The diffusion model learns to predict ground-truths given noised\nvolumes and text conditions.\n3. Method\nOur text-to-3D generation framework comprises two main\nstages: the encoding of volumes and the diffusion modeling\nphase. In the volume encoding stage, as discussed in Sec-\ntion 3.1, we have chosen to use feature volume as our 3D\nrepresentation and utilize a lightweight network to convert\nmulti-view images into 3D volumes. The proposed method\nis very efficient and bypasses the typically costly optimiza-\ntion process required by previous methods, allowing us to\nprocess a substantial number of objects in a relatively short\nperiod of time. In the diffusion modeling phase, detailed in\nSection 3.2, we model the distribution of the previously ob-\ntained feature volumes with a text-driven diffusion model.\nThis stage of the process is not without its challenges, par-\nticularly in relation to the high dimensionality of the fea-\nture volumes and the inaccuracy of object captions in the\ndatasets. We have therefore developed several key designs\nto mitigate these challenges during the training process.\n3.1. Volume Encoder\n3.1.1\nVolume Representation\nOne of the key points to train 3D generation models is the\nselection of appropriate 3D representations to serve as the\nlatent space. The 3D representation should be able to cap-\nture the geometry and texture details of the input object and\nbe flexible for fine-grained text control. Furthermore, the\n3D representation should be highly efficient in obtaining\nand reconstructing objects for scalability.\nPrevious\nrepresentations\nsuch\nas\nNeRF\n[30],\nPlenoxel [10], DMTet [43], TensoRF [4], Instant-NGP [32],\nand Tri-plane [2] all have their limitations to serve as the\nlatent space.\nFor instance, the globally shared MLP\nparameters across different coordinates in NeRFs cause\nthem inflexible and uncontrollable to local changes. Rep-\nresentations storing an explicit 3D grid, like Plenoxel and\nDMTet, require high spatial resolutions, resulting in large\nmemory costs for detailed scene representation. TensoRF,\nInstant-NGP, and Tri-plane decompose the 3D grid into\nmultiple sub-spaces with lower dimension or resolution to\nreduce memory costs but also introduce entanglements.\nIn this work, we propose a novel representation that\nmerges a lightweight decoder with a feature volume to de-\npict a scene. The lightweight decoder comprises a few lay-\ners of MLP, enabling high-resolution, fast, and low-memory\ncost rendering. The feature volume, instead of storing ex-\nplicit values, houses implicit features and effectively re-\nduces memory costs. The features of a spatial point are tri-\nlinearly interpolated by the nearest voxels on the volume.\nThe decoder inputs the interpolated feature and outputs the\ndensity and RGB color of the point. The feature volume is\nisometric to the 3D space, providing extensive controllabil-\nity over each part of an object.\n3.1.2\nFeed-forward Encoder\nUnlike previous works [1, 31, 47] that iteratively optimize\nthe representation for each object in a time-consuming way,\nwe use an encoder that directly obtains the feature volume\nof any object within a forward pass.\nAs shown in Figure 1, the encoder takes a set of\nmulti-view photos of an object (x, d, p), where x, d \u2208\nRN\u00d73\u00d7H\u00d7W represents the image and depth of N views,\np = {p(i)}N\ni=1 represents the corresponding camera param-\neters, including the camera poses and field of view (FOV).\nWe first extract features from 2D images with a small net-\nwork F composed of two layers of convolution. We then\nunproject the features into a coarse volume vc according to\ndepths and camera poses, i.e.,\nvc = \u03a6(F(x), d, p),\n(1)\nwhere \u03a6 represents the unproject operation. For each point\non camera rays, we first calculate its distance to the camera,\n3\n(a) Diffusion with common noise schedule\n(b) Diffusion with low-frequency noise\n\ud835\udc61 = 0\n500\n900\n1000\nFigure 2. Renderings of noised volumes. Volumes with common\ni.i.d. noise are still recognizable at large timesteps, while low-\nfrequency noise effectively removes information.\nthen obtain a weight wi = exp (\u2212\u03bb\u2206di) where \u2206di is the\ndifference of calculated distance and ground-truth depth.\nThe feature of each voxel is the weighted average of fea-\ntures unprojected from different views.\nSecondly, we apply a 3D U-Net [41] module to refine the\naggregated feature volume to produce a smoother volume\nvf = \u03a8(vc).\n(2)\nThen ray marching and neural rendering are performed to\nrender images from target views. In the training stage, we\noptimize the feature extracting network, the 3D U-Net, and\nthe MLP decoder end-to-end with L2 and LPIPS [52] loss\non multi-view rendered images.\nThe proposed volume encoder is highly efficient for two\nprimary reasons. Firstly, it is capable of generating a high-\nquality 3D volume with 32 or fewer images once it is\ntrained. This is a significant improvement over previous\nmethods [47], which require more than 200 views for ob-\nject reconstruction. Secondly, our volume encoder can en-\ncode an object in approximately 30 milliseconds using a sin-\ngle GPU. This speed enables us to generate 500K models\nwithin a matter of hours. As a result, there\u2019s no need to\nstore these feature volumes. We extract ground-truth vol-\numes for training diffusion models on-the-fly. It effectively\neliminates the expensive storage overhead associated with\nfeature volumes.\n3.2. Diffusion Model\n3.2.1\nDevil in High-dimensional Space\nUnlike the conventional text-to-image diffusion models, our\ntext-to-3D diffusion model is designed to learn a latent dis-\ntribution that is significantly more high-dimensional. This\nis exemplified in our experiments where we utilize dimen-\nsions such as C \u00d7 323, in stark contrast to the 4 \u00d7 642 em-\nployed in Stable Diffusion. This heightened dimensionality\nmakes the training of diffusion models more challenging.\nFigure 2(a) provides illustrations of how noised volumes\nappear at various timesteps. Utilizing the standard noise\nschedule employed by Stable Diffusion, the diffusion pro-\ncess cannot effectively corrupt the information. This is ev-\nident as the renderings maintain clarity and recognizability,\neven at large timesteps. Also, it\u2019s important to note that\nthere is a huge gap in the information between the noised\nsamples at the final timestep and pure noise. This gap can\nbe perceived as the difference between the training and in-\nference stages. We believe it is due to the high-dimensional\ncharacter of volume space.\nWe theoretically analyze the root of this problem. Con-\nsidering a local patch on the image consisting of M =\nw\u00d7h\u00d7c values, denoted as x0 =\n\b\nx1\n0, x2\n0, . . . , xM\n0\n\t\n. With-\nout loss of generality, we assume that {xi\n0}M\ni=1 are sampled\nfrom Gaussian distribution N(0, 1). With common strat-\negy, we add i.i.d. Gaussian noise {\u03f5i}M\ni=1 \u223c N(0, 1) to each\nvalue by xi\nt = \u221a\u03b3txi\n0+\u221a1 \u2212 \u03b3t\u03f5i to obtain the noised sam-\nple, where \u03b3t indicates the noise level at timestep t. Thus\nthe expected mean L2 perturbation of the patch is\nE\n \n1\nM\nM\nX\ni=0\n\u0000xi\n0 \u2212 xi\nt\n\u0001\n!2\n=\n1\nM 2 E\n M\nX\ni=0\n\u0010\n(1 \u2212 \u221a\u03b3t)xi\n0 \u2212\np\n1 \u2212 \u03b3t\u03f5i\u0011!2\n= 2\nM (1 \u2212 \u221a\u03b3t) .\nAs the resolution M increases, the i.i.d. noises added\nto each value collectively have a minimal impact on the\npatch\u2019s appearance, and the disturbance is reduced signif-\nicantly. The rate of information distortion quickly declines\nto\n1\nM . This observation is consistent with findings from\nconcurrent studies [5, 13, 16]. In order to train diffusion\nmodels effectively, it\u2019s essential to carefully design an ap-\npropriate noise that can distort information. So we propose\na new noise schedule and the low-frequency noise in the\ntraining process.\n3.2.2\nNew Noise Schedule\nThe primary goal of our text-to-3D diffusion model is to\nlearn a latent distribution, which is significantly more di-\nmensional than the text-to-image model. As discussed in\nthe previous section, a common noise schedule can lead to\ninsufficient information corruption when applied to high-\ndimensional spaces, such as volume.\nDuring the training stage, if the information of objects\nremains a large portion, the network quickly overfits to the\n4\nnoised volumes from the training set and ignores the text\nconditions. This essentially means that the network leans\nmore towards utilizing information from noised volumes\nrather than text conditions.\nTo address this, we decided\nto reduce \u03b3t for all timesteps. Thus we reduced the final\nsignal-to-noise ratio from 6\u00d710\u22123 to 4\u00d710\u22124, and evenly\nreduced \u03b3t at the intermediate timesteps. Without this, the\nnetwork may fail to output any object when inference from\npure Gaussian noise due to the training and inference gap.\nWe performed a series of experiments using different\nnoise schedules with various hyper-parameters. These in-\ncludes the commonly used linear [15], cosine [34], and sig-\nmoid [18] schedules. After comprehensive testing and eval-\nuations, we determined the linear noise schedule to be the\nmost suitable for our experiments.\n3.2.3\nLow-Frequency Noise\nImages or feature volumes are typical digital signals, which\ncan be seen as a combination of digital signals of differ-\nent frequencies. When adding i.i.d. Gaussian noise to each\nvoxel of a volume, the signal is essentially perturbed by a\nwhite noise. The i.i.d. noise evenly corrupts the information\nof all components through the diffusion process. However,\nthe amplitude of low-frequent components is usually larger\nand a white noise cannot powerfully corrupt them. Thus,\nthe mean of the whole volume as the component with the\nlowest frequency is most likely unnoticed during the diffu-\nsion process, causing information leaks. And so are patches\nand structures of different sizes in the volume.\nHence, we proposed the low-frequency noise strategy to\neffectively corrupt information and train diffusion models.\nWe modulate the high-frequency i.i.d. Gaussian noise with\nan additional low-frequency noise, which is a single value\ndrawn from normal distribution shared by all values in the\nsame channel. Formally, the noise is\n\u03f5i =\n\u221a\n1 \u2212 \u03b1 \u03f5i\n1 + \u221a\u03b1 \u03f52,\n(3)\nwhere {\u03f5i\n1}M\ni=1 \u223c N(0, 1) is independently sampled for\neach location and \u03f52 \u223c N(0, 1) is shared within the patch.\nWe still add noise to data by xi\nt = \u221a\u03b3txi\n0 + \u221a1 \u2212 \u03b3t\u03f5i, but\nthe noise \u03f5i is mixed via Equation 3 and no longer i.i.d.\nWith the low-frequency noise, the expected mean L2 per-\nturbation of the patch is\nE\n \n1\nM\nM\nX\ni=0\n\u0000xi\n0 \u2212 xi\nt\n\u0001\n!2\n= 2\nM (1 \u2212 \u221a\u03b3t) + (1 \u2212 1\nM )(1 \u2212 \u03b3t)\u03b1,\nwhere \u03b1 \u2208 [0, 1] is a hyper-parameter.\nThe proof is in\nthe supplemental material. By this approach, we introduce\nadditional information corruption that is adjustable and re-\nmains scale as the resolution grows, effectively removing\ninformation of objects as shown in Figure 2(b).\n3.3. Refinement\nThe diffusion model is able to generate a feature volume,\nbut its inherent limitation lies in its output of low-resolution,\nwhich restricts texture details. To overcome this, we lever-\naged existing text-to-image models to generate more de-\ntailed textures, enhancing the initial results obtained from\nthe diffusion model.\nSpecifically, we introduced the third stage involving fine-\ntuning the results. Given the good initial output from the\ndiffusion model, we incorporated SDS [37] in this stage to\noptimize results, ensuring better image quality and reduced\nerrors. Considering our initial results are already satisfac-\ntory, this stage only requires a few iterations, making our\nentire process still efficient.\nOur methodology makes full use of existing text-to-\nimage models to generate textures that are not covered in\nthe original training set, enhancing the details of texture and\npromoting diversity in the generated images. Simultane-\nously, our method also addresses the issue of multiple-face\nproblems encountered in [37].\n3.4. Data Filtering\nWe find that data filtering is extremely important to the\ntraining. Objaverse is mainly composed of unfiltered user-\nuploaded 3D models crawled from the web, including many\ngeometry shapes, planer scans and images, texture-less ob-\njects, and flawed reconstruction from images. Moreover,\nthe annotation is usually missing or not related, the rotation\nand position vary in a wide range, and the quality of 3D\nmodels is relatively poor compared to image datasets.\nCap3D [29] propose an approach for automatically gen-\nerating descriptive text for 3D objects in the Objaverse\ndataset.\nThey use BLIP-2 [23], a pre-trained vision-\nlanguage model, to caption multi-view rendered images of\none object and summarize them into a final caption with\nGPT-4 [36]. However, considering the significant variation\nin captions from different views, even GPT-4 confuses to\nextract the main concept, hence the final captions are still\ntoo noisy for the text-to-3D generation. With these noisy\ncaptions, we find that the diffusion model struggles to un-\nderstand the relation between text conditions and 3D ob-\njects.\nWe generate our own captions with LLaVA [24] and\nLlama-2 [46] and filter out objects with low-quality or in-\nconsistent multi-view captions in the Objaverse dataset.\nSimilar to Cap3D, we first generate captions of 8 equidis-\ntant views around the object and then summarize them into\nan overall caption with Llama-2. After that, we calculate\nthe similarity matrix of every pair among these 9 captions\n5\nusing CLIP text embedding. We believe that a high-quality\n3D object should be visually consistent from different view-\npoints, i.e., the captions from different views should be sim-\nilar. Thus, we use the average and minimal values of the\nsimilarity matrix to represent the quality of the object. And\nmanually set two thresholds to filter out objects with low\naverage/minimal similarity scores.\nWe use a selected subset of objects with the highest qual-\nity to train the diffusion model. We find that the diffusion\nmodel is able to learn semantics relations from text condi-\ntions. On the contrary, when we use the whole Objaverse\ndataset for training, the model fails to converge.\n4. Experiments\n4.1. Implementation Details\nDataset We use the Objaverse [7] dataset in our experi-\nments and rendered 40 random views for each object. For\nthe volume encoder, we filter out transparent objects and\ntrain with a subset of 750K objects.\nFor the diffusion\nmodel, we caption and filter as described in Section 3.4 and\ntrain with a subset of 100K text-object pairs.\nVolume encoder In the first stage, we train a volume en-\ncoder that efficiently converts multi-view RGBD images\ninto a feature volume.\nEach image xi are fed into a\nlightweight network F to extract the feature F(xi). The net-\nwork F merely includes 2 layers of 5\u00d75 convolution. Then\nfeatures of images are unprojected into the coarse volume\nvc and weighted averaged. \u03bb is set to 160N in our exper-\niments, where N = 32 is the spatial resolution of volume.\nAfter unprojection, the volume is refined with a 3D U-Net\nmodule and rendered with an MLP. The MLP has 5 layers\nwith a hidden dimension of 64. The volume encoder and\nthe rendering decoder in total have 25M parameters. The\nmodel is trained with the Adam [22] optimizer. The learn-\ning rate is 10\u22124 for the volume encoder and 10\u22125 for the\nMLP. The betas are set to (0.9, 0.99) and no weight decay\nor learning rate decay is applied. The input and rendered\nimage resolution is 2562 and the batch size of volume is\n1 per GPU. We first optimize the model with only L2 loss\non the RGB channel. We randomly select 4096 pixels each\nfrom 5 random views as supervision. After 100K iterations,\nwe add an additional LPIPS loss with a weight of 0.01. Due\nto GPU memory limitation, the LPIPS loss is measured on\n128 \u00d7 128 patches. The training takes 2 days on 64 V100\nGPUs.\nDiffusion model In the second stage, we train a text-\nconditioned diffusion model to learn the distribution of fea-\nture volumes. The denoiser network is a 3D U-Net adopted\nfrom [35]. Text conditions are 77 \u00d7 512 embeddings ex-\ntracted with CLIP ViT-B/32 [9] text encoder and injected\ninto the 3D U-Net with cross-attentions at middle blocks\nwith spatial resolution N\n4 and N\n8 .\nWe use a linear [15]\nGround Truth\nReconstruction\nGround Truth\nReconstruction\nFigure 3. Reconstructions of the volume encoder.\nData\nViews\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\n10K\n8\n27.14\n0.855\n0.288\n16\n27.50\n0.867\n0.282\n32\n27.61\n0.871\n0.281\n64\n27.64\n0.870\n0.280\n750K (ours)\n32\n27.69\n0.874\n0.279\nTable 1. Ablation on input view numbers and training data size of\nthe volume encoder.\nnoise schedule with T = 1000 steps and \u03b2T = 0.03. We\ntrain with the proposed low-frequency noise strategy and\nthe noise is mixed via Equation 3 with \u03b1 = 0.5 in our ex-\nperiments. The model has 340M parameters in total and is\noptimized with the Adam optimizer. The model is super-\nvised by only L2 loss on volumes and no rendering loss is\napplied. The batch size of volume is 24 per GPU, the learn-\ning rate is 10\u22125, the betas are (0.9, 0.99), and the weight\ndecay is 2 \u00d7 10\u22123. The training takes about 2 weeks on 96\nV100 GPUs.\n4.2. Volume Encoder\nWe first quantitatively study the reconstruction quality of\nthe volume encoder. We set the spatial resolution N = 32\nand channel C = 4 for efficiency. In Table 1, we mea-\nsure the PSNR, SSIM and LPIPS loss between reconstruc-\ntions and ground-truth images. To analyze the correlation\nbetween the number of different input views and the qual-\n6\n\u7021a wooden desk with a chair\u7022\n\u7021a yellow hat with a bunny ear on top\u7022\nOurs\nDreamFusion\nOne-2-3-45\nShap\u0684E\nOurs (w/o refine)\n\u7021a blue teapot with a spout and handle\u7022\nFigure 4. Comparison with state-of-the-art text-to-3D methods.\nity of reconstruction, we train encoders with different input\nviews on a subset of 10K data. It is observed that the qual-\nity of reconstruction improves as the number of input views\nincreases. However, once the number of input views sur-\npasses 32, the enhancement of quality becomes negligible.\nTherefore, we opted to use 32 as the default number of in-\nput views in our subsequent experiments. Additionally, the\nquality of reconstruction is also enhanced with the use of\nmore training data.\nWe show the reconstruction results of the volume en-\ncoder in Figure 3. The volume encoder is capable of recon-\nstructing the geometry shape and textures of objects. Exper-\niments involving higher resolution and larger channels will\nyield more detailed reconstructions. However, these adjust-\nments will also result in increased training costs and com-\nplexity in the second stage. Please refer to the supplemental\nmaterial for additional ablation studies.\n4.3. Diffusion Model\nWe compare our method with state-of-the-art text-to-3D\ngeneration approaches, including Shap\u00b7E [20], DreamFu-\nsion [37], and One-2-3-45 [25]. Since One-2-3-45 is essen-\ntially an image-to-3D model, we use images generated with\nStable Diffusion as its input. Figure 4 demonstrates that\nour methods yield impressive results, whereas both Shap\u00b7E\nand One-2-3-45 struggle to generate complex structures and\nmultiple concepts.\nFor simpler cases, such as a teapot,\nShap\u00b7E, and One-2-3-45 can only produce a rough geom-\netry, with surfaces not so smooth and continuous as those\nMethod\nSimilarity \u2191\nR-Precision \u2191\nDreamFusion [37]\n0.243\n47.3%\nOne-2-3-45 [25]\n0.228\n39.1%\nShap-E [20]\n0.287\n58.9%\nOurs\n0.288\n63.8%\nTable 2. Quantitative comparison with state-of-the-art text-to-3D\nmethods. Similarity and R-Precision are evaluated with CLIP be-\ntween rendered images and text prompts.\ncreated by our method. For more complex cases, our model\nexcels at combining multiple objects in a scene and align-\ning better with the text prompts, whereas other methods can\nonly capture parts of the concepts.\nBoth our method and Shap\u00b7E are native methods, i.e. di-\nrectly supervised on 3D representation and trained with 3D\ndatasets. It\u2019s noteworthy that these native methods generate\nclearer and more symmetrical shapes (for example, boxes,\nplanes, and spheres) than methods based on image-to-3D\nreconstruction or distillation. Furthermore, the results of\nOne-2-3-45 are marred by many white dots and stripes,\nwhich we believe is due to the inconsistency between im-\nages generated by the pre-trained Zero-1-to-3 [26] model.\nIn Table 2, we compute the CLIP Similarity and CLIP\nR-Precision as a quantitative comparison. For each method,\nwe generated 100 objects and rendered 8 views for each ob-\nject. Our method outperforms others on both visual quality\nan text alignment.\n7\n\u7021a robotic arm with a drill \non the end\u7022\n\u7021a small boat with a cabin \nand a canopy\u7022\n\u7021a candle holder with three \ncandles\u7022\n\u7021a stone water well with a \nwooden shed\u7022\n\u7021a wooden fence with a \nbucket and a shovel\u7022\n\u7021a wooden chair with a red \ncushion\u7022\n\u7021a red robot with a yellow \nfoot\u7022\n\u7021a wooden chest with \ngolden trim\u7022\n\u7021the Eiffel Tower\u7022\n\u7021a dog\u7022\n\u7021a red and black toy horse\u7022\n\u7021a bird with a red hat\u7022\n\u7021a cow\u2019s head with horns\u7022\n\u7021a blue and white car with a \nspoiler on the back\u7022\n\u7021a large metal bell on a red \nwooden stand\u7022\n\u7021a bird with a long beak \nand long legs\u7022\n\u7021a man wearing a white \nshirt and red shorts\u7022\n\u7021a wooden desk with a \ndrawer\u7022\n\u7021a wooden stool with a \nwhite cushion on top\u7022\n\u7021a brown wooden chair \nwith metal legs\u7022\nFigure 5. Text-to-3D generations by VolumeDiffusion.\nStage\nMethod\nTime\n1 (Encoding)\nFitting\n\u223c35min\nShap-E [20]\n1.2sec\nOurs\n33ms\n2 (Generation)\nDreamFusion [37]\n\u223c12hr\nOne-2-3-45 [25]\n45sec\nShap-E [20]\n14sec\nOurs (w/o refine)\n5sec\nOurs\n\u223c5min\nTable 3. Inference speed comparison. Evaluated on A100 GPU.\nWe present more results in Figure 5.\nThese prompts\ninclude cases of concept combinations and attribute bind-\nings. The critical drawbacks of distillation-based methods,\nincluding the Janus problem and over-saturated color, are\nnot observed in our results.\n4.4. Inference Speed\nIn Table 3, we report the inference speed of both stages of\nour method against other approaches. The first stage en-\ncodes multi-view images into a 3D representation and is\nimportant for scaling up the training data. Shap\u00b7E uses a\ntransformer-based encoder that takes both 16K point clouds\nand 20 RGBA images augmented with 3D coordinates as in-\nput. It is much slower than our lightweight encoder based\non convolution. Fitting means to separately optimize a rep-\nresentation for each object with a fixed rendering MLP, and\nconsumes much more time and storage. The second stage\nrefers to the conditional generation process. Optimization-\nbased DreamFusion needs hours for each object.\nOne-\n2-3-45, on the other hand, necessitates several diffusion-\ndenoising processes, such as text-to-image and multi-view\nimages generation, and is slower than native 3D methods.\nFor both stages, our method proves to be highly efficient.\n5. Conclusion\nIn conclusion, this paper presented a novel method for\nefficient and flexible generation of 3D objects from text\nprompts.\nThe proposed lightweight network for the\nacquisition of feature volumes from multi-view images\nhas been shown to be an efficient method for scaling\nup the training data required for the diffusion model.\n8\nThe paper also highlighted the challenges posed by\nhigh-dimensional feature volumes and presented a new\nnoise schedule and low-frequency noise for improved\nthe training of diffusion models.\nIn experiments, the\nsuperior performance of this model in terms of the con-\ntrol of object characteristics through text prompts has\nbeen demonstrated.\nOur future work would focus on\nrefining the algorithm and the network architecture to\nfurther speed up the process.\nWe would also involve\ntesting the model on more diverse datasets, including\nthose with more complex objects and varied text prompts.\nReferences\n[1] Ziang Cao, Fangzhou Hong, Tong Wu, Liang Pan, and Ziwei\nLiu. Large-vocabulary 3d diffusion model with transformer.\narXiv preprint arXiv:2309.07920, 2023. 2, 3\n[2] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022. 2, 3\n[3] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-\nizable radiance field reconstruction from multi-view stereo.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 14124\u201314133, 2021. 1\n[4] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision, pages 333\u2013350. Springer,\n2022. 2, 3\n[5] Ting Chen. On the importance of noise scheduling for dif-\nfusion models. arXiv preprint arXiv:2301.10972, 2023. 4,\n2\n[6] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.\nObjaverse-xl: A universe of 10m+ 3d objects. arXiv preprint\narXiv:2307.05663, 2023. 1\n[7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13142\u201313153, 2023. 1, 2, 6, 4\n[8] Akio Doi and Akio Koide. An efficient method of triangu-\nlating equi-valued surfaces by using tetrahedral cells. IEICE\nTRANSACTIONS on Information and Systems, 74(1):214\u2013\n224, 1991. 2\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In International Con-\nference on Learning Representations, 2020. 6\n[10] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5501\u20135510, 2022. 2, 3\n[11] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. Advances In Neural In-\nformation Processing Systems, 35:31841\u201331854, 2022. 2\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems, 27, 2014. 2\n[13] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Miguel \u00b4Angel\nBautista, and Joshua M Susskind. f-dm: A multi-stage dif-\nfusion model via progressive signal transformation. In The\nEleventh International Conference on Learning Representa-\ntions, 2022. 4\n[14] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-\nlas O\u02d8guz. 3dgen: Triplane latent diffusion for textured mesh\ngeneration. arXiv preprint arXiv:2303.05371, 2023. 1\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 5, 6\n[16] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. sim-\nple diffusion: End-to-end diffusion for high resolution im-\nages. arXiv preprint arXiv:2301.11093, 2023. 4\n[17] Tianyu Huang, Yihan Zeng, Bowen Dong, Hang Xu, Song-\ncen Xu, Rynson WH Lau, and Wangmeng Zuo. Textfield3d:\nTowards enhancing open-vocabulary 3d generation with\nnoisy text fields.\narXiv preprint arXiv:2309.17175, 2023.\n2\n[18] Allan Jabri, David Fleet, and Ting Chen.\nScalable adap-\ntive computation for iterative generation.\narXiv preprint\narXiv:2212.11972, 2022. 5\n[19] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n867\u2013876, 2022. 2\n[20] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 1, 2, 7, 8\n[21] Animesh Karnewar, Andrea Vedaldi, David Novotny, and\nNiloy J Mitra. Holodiffusion: Training a 3d diffusion model\nusing 2d images.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n18423\u201318433, 2023. 2\n[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In The Third International Confer-\nence on Learning Representations, 2015. 6\n[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 5\n[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In NeurIPS, 2023. 5\n9\n[25] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang\nXu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh\nin 45 seconds without per-shape optimization. arXiv preprint\narXiv:2306.16928, 2023. 2, 7, 8\n[26] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9298\u20139309, 2023. 2, 7\n[27] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv preprint arXiv:2309.03453, 2023. 2\n[28] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and\nWenping Wang. Sparseneus: Fast generalizable neural sur-\nface reconstruction from sparse views. In European Confer-\nence on Computer Vision, pages 210\u2013227. Springer, 2022.\n1\n[29] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-\nson. Scalable 3d captioning with pretrained models. arXiv\npreprint arXiv:2306.07279, 2023. 5\n[30] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 2,\n3\n[31] Norman\nM\u00a8uller,\nYawar\nSiddiqui,\nLorenzo\nPorzi,\nSamuel Rota Bulo,\nPeter Kontschieder,\nand Matthias\nNie\u00dfner.\nDiffrf:\nRendering-guided 3d radiance field\ndiffusion.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition,\npages\n4328\u20134338, 2023. 2, 3\n[32] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 2, 3\n[33] Gimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto\nTono, Linqi Zhou, and Paul Guerrero. 3d-ldm: Neural im-\nplicit 3d shape generation with latent diffusion models. arXiv\npreprint arXiv:2212.00842, 2022. 2\n[34] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021. 5\n[35] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen.\nGlide: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In International Conference on Machine Learning,\npages 16784\u201316804. PMLR, 2022. 6\n[36] OpenAI. Gpt-4 technical report, 2023. 5\n[37] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. In The\nEleventh International Conference on Learning Representa-\ntions, 2022. 2, 5, 7, 8\n[38] Zekun Qi, Muzhou Yu, Runpei Dong, and Kaisheng Ma.\nVpp: Efficient conditional 3d generation via voxel-point pro-\ngressive representation.\nIn Thirty-seventh Conference on\nNeural Information Processing Systems, 2023. 2\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2\n[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1\n[41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 1, 4\n[42] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 1\n[43] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler.\nDeep marching tetrahedra: a hybrid repre-\nsentation for high-resolution 3d shape synthesis. Advances\nin Neural Information Processing Systems, 34:6087\u20136101,\n2021. 2, 3\n[44] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea\nVedaldi. Viewset diffusion:(0-) image-conditioned 3d gener-\native models from 2d data. arXiv preprint arXiv:2306.07881,\n2023. 2\n[45] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity\n3d creation from a single image with diffusion prior. arXiv\npreprint arXiv:2303.14184, 2023. 2\n[46] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 5\n[47] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin\nBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\nWen, Qifeng Chen, et al. Rodin: A generative model for\nsculpting 3d digital avatars using diffusion. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 4563\u20134573, 2023. 1, 2, 3, 4\n[48] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 2\n[49] Jiacheng Wei, Hao Wang, Jiashi Feng, Guosheng Lin, and\nKim-Hui Yap.\nTaps3d:\nText-guided 3d textured shape\ngeneration from pseudo supervision.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16805\u201316815, 2023. 2\n10\n[50] Wang Yu, Xuelin Qian, Jingyang Huo, Tiejun Huang, Bo\nZhao, and Yanwei Fu. Pushing the limits of 3d shape gener-\nation at scale. arXiv preprint arXiv:2306.11510, 2023. 2\n[51] Wangbo Yu, Li Yuan, Yan-Pei Cao, Xiangjun Gao, Xiaoyu\nLi, Long Quan, Ying Shan, and Yonghong Tian. Hifi-123:\nTowards high-fidelity one image to 3d content generation.\narXiv preprint arXiv:2310.06744, 2023. 2\n[52] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 4\n[53] Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong,\nYang Liu, and Heung-Yeung Shum. Locally attentional sdf\ndiffusion for controllable 3d shape generation. ACM Trans.\nGraph., 42(4), 2023. 2\n11\nVolumeDiffusion: Flexible Text-to-3D Generation with\nEfficient Volumetric Encoder\nSupplementary Material\n6. Low-Frequency Noise\n6.1. Formula derivation\nIn this section, we present a detailed derivation of the ex-\npected mean L2 perturbation of a patch in Section 3.2.\nConsider a patch x0 =\n\b\nx1\n0, x2\n0, . . . , xM\n0\n\t\n. We add noise\n{\u03f5i}M\ni=1 to each value by xi\nt = \u221a\u03b3txi\n0 + \u221a1 \u2212 \u03b3t\u03f5i to ob-\ntain the noised sample, where \u03b3t indicates the noise level at\ntimestep t. The expected mean L2 perturbation of the patch\nx0 with i.i.d. Gaussian noise {\u03f5i}M\ni=1 \u223c N(0, 1) is\nE\n \n1\nM\nM\nX\ni=0\n\u0000xi\n0 \u2212 xi\nt\n\u0001\n!2\n=\n1\nM 2 E\n M\nX\ni=0\n\u0010\n(1 \u2212 \u221a\u03b3t)xi\n0 \u2212\np\n1 \u2212 \u03b3t\u03f5i\u0011!2\n= (1 \u2212 \u221a\u03b3t)2\nM 2\nE\n M\nX\ni=0\nxi\n0\n!2\n+ 1 \u2212 \u03b3t\nM 2 E\n M\nX\ni=0\n\u03f5i\n!2\n\u2212 (1 \u2212 \u221a\u03b3t)\u221a1 \u2212 \u03b3t\nM 2\nE\n\uf8eb\n\uf8ec\n\uf8ed\nM\nX\ni=0\nj=0\nxi\n0\u03f5j\n\uf8f6\n\uf8f7\n\uf8f8\n= (1 \u2212 \u221a\u03b3t)2\nM 2\n\uf8eb\n\uf8edE\nM\nX\ni=0\n\u0000xi\n0\n\u00012 + E\nM\nX\ni\u0338=j\n\u0010\nxi\n0xj\n0\n\u0011\n\uf8f6\n\uf8f8\n+ 1 \u2212 \u03b3t\nM 2\n\uf8eb\n\uf8edE\nM\nX\ni=0\n\u0000\u03f5i\u00012 + E\nM\nX\ni\u0338=j\n\u0000\u03f5i\u03f5j\u0001\n\uf8f6\n\uf8f8\n= 1\nM (1 \u2212 \u221a\u03b3t)2 + 1\nM (1 \u2212 \u03b3t)\n= 2\nM (1 \u2212 \u221a\u03b3t) .\nWith the proposed low-frequency noise strategy, we mix\nthe noise by \u03f5i = \u221a1 \u2212 \u03b1 \u03f5i\n1 + \u221a\u03b1 \u03f52 (Equation 3), where\n{\u03f5i\n1}M\ni=1 \u223c N(0, 1) is independently sampled for each lo-\ncation and \u03f52 \u223c N(0, 1) is shared within the patch. We\nstill add noise by xi\nt = \u221a\u03b3txi\n0 + \u221a1 \u2212 \u03b3t\u03f5i and only \u03f5i is\nchanged. So we have\nE\nM\nX\ni=0\n\u0000\u03f5i\u00012\n= E\nM\nX\ni=0\n\u0010\n(1 \u2212 \u03b1)(\u03f5i\n1)2 + \u03b1(\u03f52)2 + 2\np\n\u03b1(1 \u2212 \u03b1)\u03f5i\n1\u03f52\n\u0011\n= (1 \u2212 \u03b1)E\nM\nX\ni=0\n(\u03f5i\n1)2 + \u03b1E\nM\nX\ni=0\n(\u03f52)2\n= M,\nE\nM\nX\ni\u0338=j\n\u0000\u03f5i\u03f5j\u0001\n= E\nM\nX\ni\u0338=j\n\u0010\n(\n\u221a\n1 \u2212 \u03b1\u03f5i\n1 + \u221a\u03b1\u03f52)(\n\u221a\n1 \u2212 \u03b1\u03f5j\n1 + \u221a\u03b1\u03f52)\n\u0011\n= E\nM\nX\ni\u0338=j\n\u0010\n(1 \u2212 \u03b1)\u03f5i\n1\u03f5j\n1 + \u03b1(\u03f52)2\u0011\n= \u03b1\nM\nX\ni\u0338=j\nE(\u03f52)2\n= \u03b1M(M \u2212 1).\nIn conclusion, the expected mean L2 perturbation of the\npatch x0 with the low-freqency noise is\nE\n \n1\nM\nM\nX\ni=0\n\u0000xi\n0 \u2212 xi\nt\n\u0001\n!2\n= 1\nM (1 \u2212 \u221a\u03b3t)2 + 1 \u2212 \u03b3t\nM 2\n(M + \u03b1M(M \u2212 1))\n= 2\nM (1 \u2212 \u221a\u03b3t) + (1 \u2212 1\nM )(1 \u2212 \u03b3t)\u03b1.\nHere we assume that {xi\n0}M\ni=1 are also sampled from\nGaussian distribution N(0, 1), which may be not true on\nreal data. Thus we report the mean L2 perturbations on real\nimages with different resolutions in Figure 7 as a further\ndemonstration. As illustrated, the L2 perturbation of i.i.d.\nnoise decays exponentially as resolution increases, while\nour proposed low-frequency noise is slightly affected and\nconverges to larger values proportional to \u03b1.\n1\n64 \u00d7 64\n\ud835\udc3f2 = 0.167\n128 \u00d7 128\n\ud835\udc3f2 = 0.147\n256 \u00d7 256\n\ud835\udc3f2 = 0.143\n512 \u00d7 512\n\ud835\udc3f2 = 0.140\n1024 \u00d7 1024\n\ud835\udc3f2 = 0.138\nFigure 6. Noised images with different resolutions. All images are noised with xt = \u221a\u03b3x0 + \u221a1 \u2212 \u03b3\u03f5 and \u03b3 = 0.65, \u03f5 \u223c N(0, 1).\n16\n32\n64\n128\n256\n512\n1024\nResolution\n10\n2\n10\n1\nL2\n=0\n=0.01\n=0.1\n=0.5\nFigure 7. Patch L2 perturbation of noised images at timestep t =\n200. \u03b1 = 0 refers to i.i.d. noise. As image resolution increases,\nthe L2 distortion with our proposed noise is almost unaffected and\nremains at a high level.\n6.2. Justification for patchwise mean L2 loss\nTo validate the reasonableness of our adoption of patchwise\nmean L2 perturbation, we follow [5] and present an intu-\nitive example using 2D images in Figure 6. The red rect-\nangle highlights the same portion of the object across dif-\nferent resolutions, and we calculate the patchwise L2 loss\nfor each. We observe that as the image resolution increases,\nthe loss diminishes even though these images maintain the\nsame noise level (\u03b3 = 0.65), making the denoising task\neasier for networks. Consequently, we believe it is essen-\ntial to reassess noises from the local patch perspectives and\npropose the expected mean L2 perturbation of a patch as a\nmetric.\n6.3. Can adjusting the noise schedule also resolve\nthe issue in Figure 2?\nIn relation to the issue of incomplete removal information\nin Figure 2 of the main paper, we rely on the low-frequency\nnoise schedule to solve it. However, the question arises: can\nthis issue also be addressed solely by adjusting the noise\nschedule as mentioned in Section 3.2.2?\n\u03b1\n\u03b2T\nSimilarity \u2191\nR-Precision \u2191\n0\n0.02\n0.198\n11.3%\n0\n0.03\n0.264\n50.7%\n0.5\n0.02\n0.201\n11.7%\n0.5\n0.03\n0.279\n56.5%\nTable 4. Quantitative comparison between models trained with\ndifferent noise strategy. \u03b1 = 0 refers to i.i.d. noise.\nThe answer is negative. Let\u2019s consider a scenario where\nwe modify the noise schedules \u03b3t and \u03b3\u2032\nt for spaces with\nresolution M and M \u2032 respectively, ensuring that the L2 per-\nturbation remains constant:\n2\nM \u2032\n\u0010\n1 \u2212\np\n\u03b3\u2032\nt\n\u0011\n= 2\nM (1 \u2212 \u221a\u03b3t)\n\u21d4 1 \u2212\np\n\u03b3\u2032\nt\n1 \u2212 \u221a\u03b3t\n= M \u2032\nM\n(4)\nWe take the default setting in Stable Diffusion, where\n\u03b2T = 0.012 as an example, leading to \u03b3T = 0.048. The\nvolumn resolution (where M \u2032 = 323) is 8 times larger than\ndefault resolution (M = 642). Substituting these values\ninto Equation 4, we find that there is no solution for \u03b3\u2032\nt. This\nsuggests that adjusting noise schedule alone is not a viable\nsolution for high-dimensional spaces.\n6.4. Ablation\nWe conducted ablation experiments on noise schedule and\nthe low-frequency noise in Table 4. We trained diffusion\nmodels with a = {0, 0.5} and \u03b2T = {0.02, 0.03} on a\nsubset of 5K data and compares CLIP Similarity and R-\nPrecision. The results demonstrate the effectiveness of our\nnoise strategy.\nOn noise schedule, we find \u03b2T = 0.02 performs poorly,\nas the models fail to output any objects while inferencing\nfrom pure Gaussian noise. We believe it is due to the infor-\nmation gap between the last timestep and pure noise, which\nis illustrated in Figure 2(a).\nMeanwhile, models trained\nwith \u03b2T = 0.03 eliminate the training-inference gap and\nare able to draw valid samples from pure noise.\n2\nResolution N\nChannel C\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nexp01\n32\n32\n27.83\n0.874\n0.276\nexp02\n32\n4\n27.69\n0.874\n0.279\nexp03\n64\n4\n29.21\n0.886\n0.228\nexp04\n32 \u2192 64\n4\n28.68\n0.883\n0.167\nTable 5. Ablation experiments on the volume encoder.\nOn noise types, we find the model trained with i.i.d.\nnoise (\u03b1 = 0) has lower scores, as it tends to exploit the re-\nmaining information of noised volume and confuses when\nstarting from Gaussian noise. In the contrary, the model\ntrained with the low-frequency noise (\u03b1 = 0.5) is forced\nto learn from text conditions and produces results that are\nmore preferable and consistent to text prompts.\n7. Unprojection\nThe volume encoder takes a set of input views (x, d, p),\nwhere x = {x(i) \u2208 R3\u00d7H\u00d7W }N\ni=1 are images, d = {d(i) \u2208\nRH\u00d7W }N\ni=1 are corresponding depths and p = {p(i) \u2208\nR4\u00d74}N\ni=1 are camera poses. The camera pose p(i) can be\nexplicitly written as\np(i) =\n\u0014\nR(i)\nt(i)\n0\n1\n\u0015\n,\n(5)\nwhere R(i) \u2208 R3\u00d73 is the camera rotation and t(i) \u2208 R3 is\nthe camera position.\nWe we obtain the coarse volume vc by the unprojection\nvc = \u03a6(F(x), d, p),\n(6)\nwhere F(\u00b7) is the feature extractor network and \u03a6(\u00b7) is the\nunprojection operation.\nWe first set up an auxiliary coordinate volume Vcoord =\n{(xi, yi, zi, 1)}, where xi, yi, zi \u2208 [\u22121, 1] is the coordinate\nof the i-th voxels of vi\nc in space. We project the 3D space\ncoordinate V j\ncoord = (xj, yj, zj, 1) of the j-th voxel into 2D\nspace of the i-th image by\nXi,j\ncoord = \u03ba \u00b7\n\u0010\np(i)\u0011\u22121\n\u00b7 V j\ncoord,\n(7)\nwhere \u03ba \u2208 R3\u00d74 is the camera intrinsic, e.g. focal length,\nand Xi,j\ncoord = {ui,j, vi,j, wi,j}.\n1\nwi,j Xi,j\ncoord is the coordi-\nnate in the 2D space defined by the i-th image.\nThen we perform sampling with\nfi,j = \u03d5\n\u0012 1\nwi,j\nXi,j\ncoord, F(x(i))\n\u0013\n(8)\nwhere \u03d5(x, y) is the grid sampling function that samples\nvalue from y according to the coordinate x. We also sample\nthe ground-truth depth by\ndi,j = \u03d5\n\u0012 1\nwi,j\nXi,j\ncoord, d(i)\n\u0013\n.\n(9)\nFinally, we aggregate features from different views with\nthe weighted average\nvj\nc =\n1\nPN\ni=0 wi,j\n N\nX\ni=0\nwi,jfi,j\n!\n.\n(10)\nThe weight wi,j is obtained by applying a Gaussian function\non the depth difference \u2206di,j\nwi,j = exp\n\u0000\u2212\u03bb(\u2206di,j)2\u0001\n,\n(11)\nwhere \u2206di,j = di,j\u2212 \u02c6di,j and \u02c6di,j =\n\r\r\rV j\ncoord \u2212 t(i)\r\r\r\n2 is the\ncalculated distance between the j-th voxel and the camera\nof the i-th image.\n8. Volume Encoder\nIn Table 5, we conducted ablation experiments to study how\nresolution N, channel C and loss term affects the perfor-\nmance of the volume encoder.\nWe find the channel C of volume is a minor factor of the\nperformance of the volume encoder. In contrast, increasing\nthe resolution N greatly improves the reconstruction perfor-\nmance. However, N = 64 brings a computation and GPU\nmemory cost that is 8 times larger than N = 32, which\ncauses significant difficulty for training diffusion models.\nIn order to increase volume resolution without large\noverhead, we introduce a super-resolution module before\nwe feed the generated volume into the refinement mod-\nule. We increase the spatial resolution of the volume from\nN = 32 to N = 64. The super-resolution module com-\nposed of few layers of 3D convolution is served as a post-\nprocess and is performed on the outputs of the diffusion\nmodel. In our experiments, the super-resolution approach\nachieves close performances comparing to native N = 64\nvolumes. The diffusion model is trained on the volumes\nwith lower resolution N = 32, and the rendering is per-\nformed on the upsampled volumes with higher resolution\nN = 64. Therefore, we can enjoy both a lower dimen-\nsion for easier training of diffusion models as well as a\nhigher resolution for rendering more detailed textures with-\nout much overhead.\n3\n(a) Failure cases\n(b) Objaverse data\nFigure 8. Limitations of the proposed method.\n9. Limitation\nOur method has two main drawbacks and we present three\ntypical failure cases in Figure 8.\nFirst, both the volume encoder and the diffusion model\nare trained on Objaverse [7] dataset. However, the dataset\nincludes many white objects with no texture as illustrated.\nAs a consequence, our model usually prioritizes geometry\nover color and texture, and is biased towards generating\nwhite objects.\nSecond, the 3D objects generated by our model usually\nhave over-smooth surfaces and shapes. We believe this is\nattributed to the relatively low spatial resolution N = 32 of\nfeature volumes. However, with a higher resolution N =\n64, the dimension of the latent space is 8 times larger and\nthe diffusion model struggles to converge. Due to the GPU\nresources limit, we will leave it to our future works.\n10. More results\nWe present more results generated with our method in Fig-\nure 9 and Figure 10. We emphasize the diversity in Fig-\nure 11 and the flexibility in Figure 12 of our method. Also,\nwe provide more comparisons with state-of-the-art text-to-\n3D approaches in Figure 13.\n4\n\u7021a blue and white umbrella\u7022\n\u7021a brown paper bag\u7022\n\u7021a wooden skateboard ramp\u7022\n\u7021a brown cowboy hat\u7022\n\u7021a white helmet with a strap\u7022\n\u7021a bald head of a man\u7022\n\u7021a key with a chain attached\u7022\n\u7021a rusty padlock\u7022\n\u7021a man sitting on a chair\u7022\n\u7021a brown couch\u7022\n\u7021a green toy dinosaur\u7022\n\u7021a red and gold robot figure\u7022\n\u7021a castle made of stone\u7022\n\u7021a large tree stump\u7022\n\u7021a purple bat with wings\u7022\n\u7021a red and yellow action \nfigure of Iron Man\u7022\n\u7021a tall brown lighthouse \nwith a light on top\u7022\n\u7021a colorful bird with a long \ntail and green wings\u7022\n\u7021a yellow toy tractor with a \nshovel on the front\u7022\n\u7021a cartoon tiger wearing a \nred shirt\u7022\n\u7021a black fish with blue eyes\u7022\n\u7021a white boot\u7022\n\u7021a red cup with a yellow lid\u7022\n\u7021a gray plate with food in it\u7022\n\u7021a tank with a gun on top\u7022\n\u7021a stone fountain with a \nspout\u7022\n\u7021a man wearing a blue shirt \nand gray pants\u7022\n\u7021a red toy airplane with a \npilot inside\u7022\n\u7021a green turtle holding a \nspear\u7022\na small statue of a man \nholding a sword\nFigure 9. More text-to-3D generations of VolumeDiffusion.\n5\n\u7021a purple dragon toy with \nhorns\u7022\n\u7021a single red rose in a \nbrown pot\u7022\n\u7021a blue elephant with a long \ntrunk\u7022\n\u7021a dog with a backpack on \nits back\u7022\n\u7021a small Christmas tree \nmade of Legos\u7022\n\u7021a high-rise building with \nmultiple floors\u7022\n\u7021a dragon with a crown on \nits head and wings\u7022\n\u7021a racing car with a silver \nbody and black wheels\u7022\n\u7021a statue of a knight riding \na horse\u7022\n\u7021a green robot statue with a \nsword in its hand\u7022\n\u7021a black and grey camera \nwith a lens\u7022\n\u7021an orange cell phone with \nblack buttons\u7022\n\u7021a blue diamond on a white \npedestal\u7022\na red motorcycle with a \nblack seat\n\u7021a white chair with a cross \non the back\u7022\n\u7021a yellow robot with four \nlegs and a round body\u7022\n\u7021a yellow robot with a head \nand arms\u7022\n\u7021a plant with green leaves \nand yellow flowers\u7022\n\u7021a blue crystal with a black \nbase\u7022\n\u7021a blue house with a door \nand windows\u7022\n\u7021a statue of a man holding a \nchild\u7022\n\u7021a blue and white vase with \na lid\u7022\n\u7021a small yellow airplane\u7022\n\u7021a blue robot with a \nhumanoid body\u7022\n\u7021a wooden church with a \ncross on top\u7022\n\u7021a white and black police \nSUV car\u7022\n\u7021a tank with a gun on top\u7022\n\u7021a red and white striped \ncandy cane\u7022\n\u7021a white bust of a man with \na stern expression\u7022\na palm tree with a brown \ntrunk and green leaves\nFigure 10. More text-to-3D generations of VolumeDiffusion.\n6\n\u7021a pair of sunglasses\u7022\n\u7021a car\u7022\n\u7021a toy cannon\u7022\n\u7021a chair\u7022\nFigure 11. Diverse text-to-3D generations of VolumeDiffusion.\n7\n\u7021a golden ring with \n\u2026 \n\u7021a wooden table \nwith \u2026\n\u7021a man wearing \u2026\nred shirt and white short\u7022\nblue shirt and purple short\u7022\nwhite shirt and red short\u7022\na teddy bear on top\u7022\na basketball on top\u7022\na yellow barrel on top\u7022\nblue gemstone on it\u7022\ngreen gemstone on it\u7022\nred gemstone on it\u7022\nFigure 12. Flexible text-to-3D generations of VolumeDiffusion.\n8\n\u7021a yellow helicopter\u7022\n\u7021a white shovel with red handle\u7022\nOurs\nDreamFusion\nOne-2-3-45\nShap\u0684E\nOurs (w/o refine)\n\u7021a white letter I\u7022\n\u7021a yellow sink with a faucet\u7022\n\u7021a flag of Netherlands\u7022\n\u7021a pair of swords\u7022\n\u7021a white watering can with a handle\u7022\nFigure 13. Comparison with state-of-the-art text-to-3D methods.\n9\n"
  },
  {
    "title": "GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View Synthesis",
    "link": "https://arxiv.org/pdf/2312.11458.pdf",
    "upvote": "4",
    "text": "GauFRe\n: Gaussian Deformation Fields\nfor Real-time Dynamic Novel View Synthesis\nYiqing Liang\u2021, Numair Khan, Zhengqin Li, Thu Nguyen-Phuoc,\nDouglas Lanman, James Tompkin\u2021, Lei Xiao\nMeta\n\u2021Brown University\nGround Truth\nPSNR: 24.9\nTrain: ~ hrs\nFPS: < 1  \nPSNR: 17.1\nTrain: ~ hrs\nFPS: < 1  \nPSNR: 23.1\nTrain: 30 min.\nFPS: < 1  \nPSNR: 25.4\nTrain: 19 min.\nFPS: 33\nV4D\nNDVG\nTiNeuVox\nOurs\nOurs - Static\nOurs - Dynamic\nStatic/Dynamic Separation\nFigure 1. GauFRe reconstructs dynamic scenes from casually-captured monocular video inputs. Our representation renders in real-time\n(> 30FPS) while achieving high rendering performance. GauFRe also decomposes static/dynamic regions without extra supervision.\nAbstract\nWe propose a method for dynamic scene reconstruction\nusing deformable 3D Gaussians that is tailored for monoc-\nular video. Building upon the efficiency of Gaussian splat-\nting, our approach extends the representation to accommo-\ndate dynamic elements via a deformable set of Gaussians\nresiding in a canonical space, and a time-dependent de-\nformation field defined by a multi-layer perceptron (MLP).\nMoreover, under the assumption that most natural scenes\nhave large regions that remain static, we allow the MLP to\nfocus its representational power by additionally including a\nstatic Gaussian point cloud. The concatenated dynamic and\nstatic point clouds form the input for the Gaussian Splatting\nrasterizer, enabling real-time rendering. The differentiable\npipeline is optimized end-to-end with a self-supervised ren-\ndering loss. Our method achieves results that are compara-\nble to state-of-the-art dynamic neural radiance field meth-\nods while allowing much faster optimization and rendering.\nProject Webpage: this url.\n1. Introduction\nHigh-quality 3D reconstruction of dynamic scenes from\nRGB images is a persistent challenge in computer vision.\nThe challenge is especially great from monocular camera\nvideo: the setting is ill-posed as constraints on the sur-\nface geometry must be formed by simultaneously solving\nfor an estimate of the scene\u2019s motion over time. Structure\nfrom motion provides an estimate of rigid motion for static\nscenes, but real-world scenes have motions that extend be-\nyond rigid or piecewise rigid to continual deformation, such\nas on human subjects. Given this challenge, one relaxation\nof the problem is to consider novel view synthesis instead,\nwhere we reconstruct the appearance of the scene to allow\napplications in editing to re-pose or re-time the scene.\nInverse graphics approaches using an image rendering\nloss have recently created optimization-based reconstruc-\ntion methods that can achieve high quality for static or dy-\nnamic scenes with many cameras. These often use neural\nnetworks (or multi-layer perceptrons; MLPs) as a function\nto predict the values of physical properties in a field, such\nas the density and radiance volumes within the influential\nneural radiance field (NeRF) technique [20]. Optimizing\nthese MLPs with gradient descent is robust, often leading\nto good solutions without tricky regularization [35]. How-\never, neural networks are time-consuming to optimize via\ngradient descent, and volume rendering requires many sam-\nples of the network to create an image. Faster optimiza-\ntion and subsequent rendering can be achieved with the help\narXiv:2312.11458v1  [cs.CV]  18 Dec 2023\nTime\nRandom \nInitialization\nStructure-from-Motion\nMLP\nDeformed Gaussians\nTime = \ud835\udc61\ud835\udc61\nDeformable Gaussians \nin Canonical Space\nStatic 3D Gaussians\nRendering Loss\nNovel View \nSynthesized at Time = \ud835\udc61\ud835\udc61 \nGaussians at \ntime = \ud835\udc61\ud835\udc61\nFigure 2. An overview of our dynamic scene representation. At each time frame t, our method reconstructs the scene as a combination\nof static and deformable anisotropic 3D Gaussians. The features of the deformable Gaussians are optimized in a canonical space and\nwarped into frame t using a deformation field. The static Gaussians are optimized in world space. We represent the deformation field using\na multi-layer perceptron (MLP) parameterized by time t.\nof spatial indexing data structures, such as voxel grids [6],\noctrees [41], and multi-scale hash tables [21, 31], or with\nproxy geometries such as planes [2, 7]. As they lack the\nself-regularizing properties of neural networks, these may\nrequire additional regularization. Other proxies are possi-\nble: following point-based graphics [40, 44], composing\na scene of many isotropic Gaussians is convenient as they\nare differentiable everywhere [27], can be splatted in closed\nform [19, 30], and can be z-sorted efficiently under small-\nGaussian assumptions without ray marching [13]. Careful\nefficient implementation [12] leads to real-time rendering at\nhigh resolutions, and overall produces compelling results.\nExtending this idea to parameterize Gaussians by time\nfor dynamic scenes is natural, with the idea that each repre-\nsents a moving and deforming particle or blob/area of space\ntracked through time\u2014the Lagrangian interpretation in the\nanalogy to fluid flow. This can work well in settings with\nsufficient constraints upon the motion of the flow, e.g., in\n360\u00b0 multi-camera settings [18]. For the underconstrained\nmonocular video setting where constraints are sparse, it is\nchallenging to directly optimize the positions and covari-\nances of Gaussians as accurate prediction of both geometry\nand motion are required, leading to low-quality output.\nInstead, for monocular video, we propose a Eulerian per-\nspective on motion by modeling a field over time that can\nbe sampled to predict Gaussian deformation. This defor-\nmation happens from a canonical Gaussian space. Rather\nthan use a fixed spatial data structure for this field, which\ncan be memory expensive, we use an MLP to represent the\nfield. This MLP is less sensitive to incorrect initialization\nand can optimize more easily given the sparse constraints\nthan directly optimizing Gaussians. Beyond that, most re-\ngions of real-world dynamic scene are quasi-static. As such,\nwe split the scene into its static and dynamic components\nwith a separate non-deforming set of Gaussians that are\ninitialized around structure-from-motion-derived 3D points\nto ease the separation. Using reconstruction losses on the\ninput video, we optimize the position, covariance, opac-\nity, and appearance parameters of static Gaussians directly,\nand optimize the position of the dynamic Gaussian clouds\nthrough the deformation MLP. In evaluation, this approach\nachieves comparable quality to non-Gaussian-based neural\nscene representations, while being fast to optimize (20 min-\nutes rather than hours) and providing real-time rendering for\nnovel view synthesis.\nWe contribute:\n1. A dynamic scene representation of canonical Gaussians\ndeformed by a field represented by an MLP.\n2. A static Gaussian cloud that represents quasi-static re-\ngions and allows explicit separation of dynamic regions.\n3. An experimental validation of this approach on synthetic\nand real-world datasets against eight baselines.\n2. Related Work\n3D and 4D Scene Reconstruction\nGiven the success\nof 3D voxel grids as an explicit representation for static\nscenes [3, 6, 11, 31, 41], a straightforward approach is\nto extend them into a fourth dimension for time to han-\ndle dynamic content. Unfortunately, the memory require-\nments of such a 4D grid quickly become prohibitive even for\nshort sequences. As a result, a number of methods propose\nstructures and techniques that reduce the memory complex-\nity while still fundamentally being four-dimensional grids.\nPark et al. [24] extend Muller et al.\u2019s multi-level spatial\nhash grid [21] to 4D, and additionally allow for the separate\nlearning of static and dynamic features. This latter capa-\nbility allows the model to focus the representational power\nof the 4D hash grid on dynamic regions. Another com-\nmon approach factorizes the spatio-temporal grid into low-\ndimensional components. Jang and Kim [10] propose rank-\none vectors or low-rank matrices, providing a 4D counter-\npart of the 3D tensorial radiance fields of Chen et al. [3].\nShao et al. [29] hierarchically decompose the scene into\nthree time-conditioned volumes, each represented by three\northogonal planes of feature vectors. An even more com-\npact HexPlanes solution by Cao and Johnson [2] use six\nplanes, each spanning two of the four spatio-temporal axes.\nA similar decomposition is presented by Fridovich-Keil et\nal. [7] as part of a general representation that uses\n\u0000d\n2\n\u0001\nplanes\nto factorize any arbitrary d-dimensional space.\nMotion Reconstruction\nWhile 4D, none of these ap-\nproaches explicitly account for motion, e.g., they do not de-\nfine correspondence over time. To do so, Tretschk et al. [32]\ndiscover deformation fields that align scenes under repro-\njection. Park et al. [23] deform a canonical field by a higher-\ndimensional time-varying field to accommodate topology\nchanges. Fang et al. [5] demonstrate a hybrid representation\nthat defines a 3D canonical space explicitly as a voxel grid,\nthen queries it using an implicit deformation field for 4D\nspatio-temporal points. Guo et al. [9] propose a similar ap-\nproach but use additional features interpolated from an ex-\nplicit time-independent deformation grid to deform into the\ncanonical frame. Some works also attempt to split static and\ndynamic scene parts to improve quality [16, 17, 34]. Unlike\nthese works, our approach uses Gaussian clouds instead of\nMLPs or spatial data structures, including a static cloud and\na dynamic cloud deformed from a canonical frame.\nPoint-based Rendering\nOptimization-based point graph-\nics are also popular for reconstruction [1], including spher-\nical proxy geometries [15], splatting-based approaches [12,\n40], methods for computing derivates of points rendered\nto single pixels [28], and methods for view-varying opti-\nmization of points [14]. Point-based approaches can also be\nfast, accelerating rendering and so optimization too [36, 42].\nSuch approaches are also adaptable to dynamic scenes [25],\nsuch as the dynamic point fields method of Prokudin et\nal. In contrast, our approach uses Gaussians as the base\nprimitive, from which dynamic scene regions are deformed\naround fixed static regions.\nContemporaneous Work\nFinally, we note contempo-\nraneous works using dynamic Gaussian representations,\nall published or in preprint within the last three months.\nLiuten et al. [18] consider the 360\u00b0 multi-camera case that\nis constrained in spacetime, and take a Lagrangian track-\ning approach. Yang and Yang et al. [39] consider a more\nEmbed\nEmbed\nx\nPosition\n\u2026.\nx +\u03b4x, q +\u03b4q, s + \u03b4s, \ud835\udf0e\ud835\udf0e, c\nTime \ud835\udc61\ud835\udc61\nRendering Loss\n+\n(x, q, s, \ud835\udf0e\ud835\udf0e, c)\n(\u03b4x, \u03b4q, \u03b4s)\nRasterization\nSkip\nLinear + ReLU\nGradients\nControl\n3D Gaussian in \ncanonical space\nDeformed 3D \nGaussian at time \ud835\udc61\ud835\udc61 \nFigure 3. Optimization architecture. The deformation field\u2019s\ndomain is an embedding space of time t and Gaussian position x.\nGradients do not flow to x from the MLP to prevent entanglement.\nflexible approach where 4D Gaussian position and color are\ndirectly optimized over time. Zielonka et al. [43] approach\nthe problem of driveable human avatars from multi-camera\ncapture using Gaussians for tracking.\nThe methods of Wu et al. [33] and Yang and Gao et\nal. [38] are closest to our approach as both use a deforma-\ntion field parameterized by an MLP. Wu et al. represent the\nGaussians via HexPlanes and hashed coordinates, which is\nfast to render but does not produce as high a quality of re-\nconstruction as our method and does not separate static and\ndynamic scene regions. Yang and Gao et al. [38] use an\nexplicit rather than spatial representation for the Gaussians,\nbut also do not separate static and dynamic regions.\n3. Method\n3.1. 3D Gaussian Splatting\nFollowing Kerbl et al. [12], we start by representing the\nscene as a set of n points {xi \u2208 R3, i = 1, ..., n}. Each\npoint is associated with features (\u03a3i, \u03c3i, ci) that define the\nlocal radiance field as an anisotropic Gaussian distribution\ncentered at xi with covariance \u03a3i, density \u03c3i, and view-\ndependent color ci represented by 2nd-order spherical har-\nmonics. Given a set of multi-view images of the scene, we\ncan penalize a rendering loss to optimize the set of Gaus-\nsians {Gi = (xi, \u03a3i, \u03c3i, ci)} to represent the scene\u2019s global\nradiance field for tasks like novel view synthesis.\nTo ensure \u03a3i represents a valid positive semi-definite co-\nvariance matrix in the optimization, it is factored into a ro-\ntation matrix Ri \u2208 R3\u00d73 and scaling vector si \u2208 R3. An ex-\nponential activation is applied to si to prevent negative val-\nues while retaining differentiability over the domain. Thus,\n\u03a3i = RiExp(si)Exp(si\nT )Ri\nT .\n(1)\nIn practice, Ri is inferred from a unit-length quaternion\nqi \u2208 R4 that provides better convergence behavior. The ini-\ntial position xi of the Gaussians is provided by a 3D point\ncloud obtained with a structure-from-motion algorithm. As\nthe optimization proceeds, the Gaussians are periodically\ncloned, split, and pruned to achieve a suitable trade-off be-\ntween rendering quality and computational resources.\nIn addition to the Gaussian scene representation, Kerbl et\nal. demonstrate how the many continuous radiance distri-\nbutions can be efficiently rendered on graphics hardware.\nGiven a target camera view transformation V and projec-\ntion matrix K, each Gi is reduced to a Gaussian distribution\nin screen space with projected mean ui = KVxi \u2208 R2 and\n2D covariance defined by the Jacobian J of K as\n\u03a3\u2032\ni = JV\u03a3iVT JT\n(2)\nThe 2D Gaussians are then rasterized using Zwicker et\nal.\u2019s Elliptical Weighted Average (EWA) splatting [44].\n3.2. Deformable Gaussian Fields\nTo model a dynamic scene, we assume that it is equivalent\nto a static scene that is deformed from a canonical point\nset {Gi = (xi, si, qi, \u03c3i, ci)}i\u2208N via a deformation field\nparameterized by an MLP \u03a6:\n\u03a6 : (xi, t) \u2192 (\u03b4xt\ni, \u03b4st\ni, \u03b4qt\ni),\n(3)\nwhere density \u03c3i does not change over time, and neither\ndoes the view-dependent appearance ci of a Gaussian\u2014e.g.,\nonly a Gaussian\u2019s position x, scale s, and rotation via q can\nchanges to describe the scene, which is equivalent to an\naffine transform. Allowing the deformation field to vary \u03c3i\nand ci provides too few constraints on the underlying scene\nmotion, as Gaussians can appear or disappear, or change\ntheir appearance, to represent motions.\nThe deformed position requires no activation to apply:\nxt\ni = xi + \u03b4xt\ni\n(4)\nFor S, we could predict a pre- or post-exponentiated delta:\nExp(st\ni) = Exp(si + \u03b4st\ni) or Exp(st\ni) = Exp(si) + \u03b4st\ni\n(5)\nPre- presents a log-linear estimation problem, which is sim-\npler than an exponential one, and allows the optimization\nto still estimate negative values. Empirically, this improved\noptimized representation quality substantially (Fig. 4).\nWe must also be careful with quaternion deformation:\n||qt\ni|| = ||qi + \u03b4qt\ni|| or ||qt\ni|| = ||qi|| + \u03b4qt\ni\n(6)\nOnly unit quaternions represent rotations, so the right-hand\nvariant will introduce additional unwanted transformations\ninto the deformation field. Further, qi + \u03b4qt\ni represents a\nrotation that is half-way between qi and \u03b4qt\ni. While not\nstrictly a delta, it is fast and sufficient for small rotations.\nInput\nPost-activ.\nOurs\nTiNeuVox [4]\nFigure 4. Estimating pre-activated scale significantly improves\nquality on dynamic objects. Left to right: Ground truth, estimat-\ning post-activation scale difference, estimating pre-activation scale\ndifference, and the results of prior TiNeuVox work [4] that uses an\nMLP to directly predict the new scale for a particular timestep.\n3.3. Static-Dynamic Decomposition\nMany dynamic scenes contain significant static world re-\ngions that do not need to be deformed. Further, real-world\nsequences contain small image noise or camera pose er-\nrors. A fully-dynamic model will be forced to spend MLP\ncapacity describing deformations in irrelevant regions. In\ncontrast, if these regions can be ignored, the MLP can in-\ncrease deformation fidelity and overall improve image qual-\nity.\nThis issue compounds with the number of required\nGaussians and their cloning and splitting as too many Gaus-\nsians can overfit to represent noise; similarly, unnecessarily\ncloning and splitting wastes machine memory. For instance,\nfor a fully-dynamic model, some of our tested sequences\nlead to out-of-memory crashes.\nTo combat this, we use a separate static Gaussian point\ncloud {Gj = (xj, \u03a3j, \u03c3j, cj)}j\u2208Nr that leads to higher-\nquality overall dynamic view synthesis (Fig. 5). In random\ninitialization settings, half of the point cloud is assigned\nas static and half as dynamic. During rendering, we con-\ncatenate {Gt\ni}i\u2208N and {Gj}j\u2208Nr as input to the rasterizer.\nDuring optimization, the two point cloud are densified and\npruned separately and can capture the appearance of static\nand dynamic parts without explicit supervision (Fig. 6).\nHowever, this process may need some help. Consider\nthe case when Kerbl et al. [12] initialize the Gaussian point\ncloud by sampling a precomputed 3D point cloud from\nstructure from motion. Dynamic objects will not appear in\nthis point cloud, and so it will take a long time to optimize\nInput\nDyn. only\nOurs\nNDVG [9]\nFigure 5.\nSeparate deformable and quasi-static regions im-\nproves quality in dynamic parts. Left to right: Ground truth,\ndeformable set, separate static and deformable sets, and the results\nof NDVG [9] that uses deformable grids without separation.\nInput\nStatic\nDynamic\nSeparation\nFigure 6. Static/dynamic separation visualization. The input\nvideo does not see one part of the static scene (white).\nGaussians into this region (if they ever reach it at all). As\nsuch, we randomly re-distribute the dynamic Gaussians in\nspace. Further, some initial point clouds are dense given the\nvideo input (> 1e5) and using all points is memory expen-\nsive; random sampling only a subset mitigates this issue.\n3.4. Implementation\nPositional/Temporal\nEncoding\nWe\nfacilitate\nhigh-\nfrequency deformation fields through PE encoding both the\nposition x and time t inputs to the MLP by \u03b3, where, for\nexample, Lx is the respective encoding base for x:\n\u03b3(x) = (sin(20x), cos(20x), sin(21x),\ncos(21x), . . . , sin(2Lx\u22121x), cos(2Lx\u22121x))\n(7)\nWe use L\u00b5 = 10, Lt = 6 for synthetic scenes and L\u00b5 =\n10, Lt = 6 for real-world scenes.\nNetwork architecture\nOur deformation MLP (Fig. 3) is\ninspired by Fang et al. [4]. Along with a time t embed-\nding vector space, we also use a Gaussian position x em-\nbedding vector space. In the middle MLP layer, we add a\nskip connection such that the time embedding and Gaussian\nposition x embedding are concatenated and passed into the\nsecond half of the MLP. Empirically, this improved perfor-\nmance. As the Gaussian position is input to the MLP and\nis also being optimized through a separate path, we manu-\nally stop the gradient from flowing back through the MLP to\nthe Gaussian position. This prevents the deformation MLP\nfrom entangling the position representations.\nWe use MLPs with 8 layers of 256 neurons for synthetic\nscenes and 6 layers of 256 neurons for real-world scenes.\nOptimizer\nWe use Adam with \u03b2 = (0.9, 0.999) and\neps = 1e\u221215.\nThe learning rate for the deformation\nMLP is 0.0007 for synthetic datasets and 0.001 for real-\nworld datasets, with exponential scheduling that shrinks to\n0.002\u00d7 the original learning rate until 30 K iterations. We\ndensify both Gaussian point clouds until 20 K iterations,\nand keep optimizing both the Gaussians and the MLP un-\ntil 40 K iterations for synthetic datasets and 60 K iterations\nfor real-world datasets.\nWarm-up Routine\nTo stabilize optimization, the MLP\nlinear layer initial weights and bias are set to \u223c N(0, 1e\u22125)\nsuch that only small deformations are predicted at the begin-\nning. During initial optimization, we only begin deforming\nGaussians after 3 K iterations to provide a more stable start.\nLosses\nWe optimize using an image-based reconstruction\nloss only. For synthetic scenes, we use an L1 loss. For real-\nworld scenes, in early optimization until 20 K iterations, we\nuse an L2 loss; then, we switch to an L1 loss. This helps\nto increase reconstruction sharpness late in the optimization\nwhile allowing gross errors to be minimized quickly.\n4. Experiments\nMetrics\nWe measure novel view synthesis performance\nusing standard PSNR, SSIM, MS-SSIM and LPIPS met-\nrics. We use both SSIM and MS-SSIM as each were used\npreviously as standard on different datasets. We report op-\ntimization time and rendering time of methods on a single\nNVIDIA 3090 GPU.\nSynthetic dataset: D-NeRF [26]\nThis contains monocu-\nlar exocentric 360\u00b0 recordings of 8 dynamic synthetic ob-\njects with large motions and realistic materials. To fairly\ncompare with peers, we train and render at half resolution\n(400 \u00d7 400) with a white background.\nReal-world dataset: NeRF-DS [37]\nThis is formed of\nreal-world dynamic scene sequences containing specular\nobjects captured with a handheld camera.\n4.1. Comparisons\nMLP + MLP deformation\nWe report results from Ner-\nfies [22], HyperNeRF[23] and NeRF-DS on the NeRF-DS\ndataset (Tab. 2).\nAll three use volumes represented by\nMLPs to recover a scene, where the scene is then deformed\nvia another MLP from a canonical space. While MLPs can\nachieve high quality, they take a long time to train. Our\napproach achieves higher reconstruction quality in a much\nshorter amount of time, and allows real-time rendering of\nthe recovered representation.\nFast optimization\nVoxel grids allow faster optimization.\nFor example, NDVG [9] uses a fixed voxel grid in a canon-\nical space and deforms it with an MLP, leading to rapid\noptimization times. However, their quality suffers against\nour method, showing significantly worse metrics across D-\nNeRF and NeRF-DS dataset. In qualitative comparisons,\nwe see lower image quality in both D-NeRF (Fig. 7) and\nNeRF-DS (Fig. 8).\nAnother approach is TiNeuVox[4].\nAgain optimization time is fast, but quality is notably lower\nby eye and by metrics on D-NeRF and NeRF-DS. Plane-\nbased methods also provide fast optimization.\nBoth K-\nplanes [7] and HexPlanes [2] show similar optimization\ntimes to our method on D-NeRF, with reduced image qual-\nity across all metrics.\nFast rendering\nIf we only wished to render a scene\nquickly, we could spend longer in optimization. For in-\nstance, V4D [8] uses 3D voxel grids and a smaller defor-\nmation MLPs with volume rendering. Efficient implemen-\ntation can allow inference at a few Hz. However, our repre-\nsentation can be rendered in real-time while showing better\nresults on both D-NeRF and NeRF-DS data.\nContemporaneous Gaussian\nWe run Wu et al. [33]\u2019s\n4DGaussians public code to generate results on D-NeRF.\nWhile their method is faster to optimize, both methods can\nbe rendered in real time. Further, both methods produce\nbroadly similar qualitative performance, with only a slight\nincrease in metrics for our method. Our approach addition-\nally separates static and dynamic regions, which may be\nuseful additional information for downstream tasks.\nAblation\nTable 4 shows image quality metrics for abla-\ntions of our method that remove components. The most\nsignificant of these is the removal of the static Gaussians,\nwhich improves image sharpness in static regions. We also\nablate three other components: 1) the transition from L2\nPSNR\u2191 SSIM\u2191 LPIPS\u2193 Optim. \u2193 Render\u2193\nNDVG [9]\n30.5\n0.965\n0.054\n25mins\n1.7s\nTiNeuVox [5]\n32.9\n0.972\n0.041\n20mins\n1.9s\nK-planes [7]\n29.2\n0.961\n0.060\n60mins\n8s\nHexplane [2]\n31.0\n0.968\n0.039\n15mins\n0.5s\nV4D [8]\n33.4\n0.978\n0.027\n\u223chours\n0.5s\n4DGaussians [33]\n32.9\n0.977\n0.024\n15mins\n0.01s\nOurs\n34.8\n0.982\n0.020\n25mins\n0.02s\nTable 1. Quantitative image quality results on the dataset from\nD-NeRF [26]. Our method produces the highest metrics in the\nsynthetic D-NeRF dataset with accurate camera poses.\nPSNR\u2191 SSIM\u2191 LPIPS\u2193\nNerfies [22]\n20.1\n0.707\n0.349\nHyperNeRF [23]\n23.0\n0.854\n0.181\nNeRF-DS [37]\n23.7\n0.885\n0.143\nNDVG [9]\n19.1\n0.584\n0.417\nTiNeuVox [4]\n21.7\n0.818\n0.219\nV4D [8]\n23.5\n0.884\n0.142\nOurs\n23.8\n0.887\n0.144\nTable 2. Quantitative image quality results on the dataset from\nNeRF-DS [37]. Our approach on average shows improved PSNR,\nSSIM (here the multi-scale variant), and LPIPS metric perfor-\nmance than most methods, and comparable image quality metric\nperformance to NeRF-DS [37] and V4D [8]. However, in contrast\nto these two methods that each take hours to optimize a represen-\ntation for a scene and seconds to render each frame, our approach\ntakes minutes to optimize per scene and can render frames in real\ntime (Tab. 3).\nloss to L1, 2) the choice to pre-exponentiate the scale defor-\nmation instead of post-exponentiating it, and 3) the warmup\nof the Gaussian optimization before allowing deformation.\nEach contributes a smaller improvement to the final image\nquality of the method. Further, the addition of each com-\nponent does not have an impact upon the final novel view\nrender time.\n5. Limitations and Conclusion\nCurrently, we use a single MLP to model the whole de-\nformation field. Despite the existence of the static Gaus-\nsian points, modeling scenes with big or irregular motion\nis difficult for this network, especially given that the net-\nwork capacity has to be constrained to prevent overfitting\non the input images. This may produce blurriness or float-\ning artifacts. A potential improvement is to aid the MLP\nwith better knowledge of where deformations may happen,\nsuch as by using priors from optical flow estimation meth-\nods. Additionally, our dynamic/static separation is sensi-\ntive to the quality of the initial SfM point cloud for real-\nGround Truth\nV4D\nHexplane\nTiNeuVox\nNDVG\n4D Gaussians\nOurs\nFigure 7. Quantitative comparison of our approach and the baseline methods for test views from the synthetic DNeRF [26] dataset. All\nmethods reproduce the rough geometry, but sufficient sampling is necessary to reproduce the fine detail. Our approach can efficiently\nspread Gaussians to both static and dynamic regions to maximize quality, producing the sharpest image of all compared methods.\nworld scenes. Its benefit is to better distribute static points,\nbut dynamic points still must be optimized from a random\ninitialization\u2014some dynamic elements are sensitive to this.\nConclusion\nGaussians parameterized by time are a pow-\nerful way to optimize and render dynamic scenes with high\nquality. For monocular settings, we show that a deformation\nfield is a suitable representation that achieves high quality\nreconstructions without degrading render times beyond real\ntime. Notably, we show that separating the Gaussian set\ninto static and dynamic regions rather than just dynamic im-\nproves quality and can produce segmented scenes. In com-\nparisons to other methods, we show improved quality by vi-\nsual comparison, and by image-quality metrics like LPIPS\non both synthetic and real world scenes.\nGround Truth\nV4D\nTiNeuVox\nNDVG\nOurs\nFigure 8. Qualitative results on the NeRF-DS [37] monocular video dataset versus voxel- and sampled-space-based approaches\nshow reduced quality compared to our method. Sequences from top to bottom are Sheet, Bell, and Cup. Our approach better reproduces\nfine details for test views, including details on dynamic objects such as hands, and higher quality on static objects.\nPSNR\u2191 Optim.\u2193 Render\u2193\nNerfies [22]\n20.1\n\u223chours\n-\nHyperNeRF [23]\n23.0\n\u223chours\n-\nNeRF-DS [37]\n23.7\n\u223chours\n-\nNDVG [9]\n19.1\n\u223c 1hour\n> 1s\nTiNeuVox [4]\n21.7\n30mins\n> 1s\nV4D [8]\n23.5\n\u223chours\n> 1s\nOurs\n23.8\n19mins\n0.03s\nTable 3. Quantitative comparison on the NeRF-DS [37] dataset.\nWhile some methods achieve comparable performance to ours, our\nmethod is much faster to optimize and to render.\nReferences\n[1] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry\nUlyanov, and Victor Lempitsky. Neural point-based graph-\nics. 2020. 3\n[2] Ang Cao and Justin Johnson. Hexplane: A fast representa-\nPSNR\u2191 MS-SSIM\u2191 LPIPS\u2193\nNo Static Gaussians\n23.47\n0.876\n0.152\nNo L2 transition to L1\n23.66\n0.882\n0.148\nScale post-exponentiate\n23.77\n0.882\n0.154\nNo Gaussian deform warmup 23.78\n0.884\n0.146\nFull\n23.80\n0.887\n0.144\nTable 4. Model ablations using the NeRF-DS [37] dataset, or-\ndered by increasing PSNR. Our model with all components max-\nimizes all three metrics, with the addition of static Gaussians in-\ncreasing overall quality the most.\ntion for dynamic scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 130\u2013141, 2023. 2, 3, 6\n[3] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision, pages 333\u2013350. Springer,\n2022. 2, 3\n[4] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xi-\naopeng Zhang, Wenyu Liu, Matthias Nie\u00dfner, and Qi Tian.\nFast dynamic radiance fields with time-aware neural voxels.\nIn SIGGRAPH Asia 2022 Conference Papers, 2022. 4, 5, 6,\n8\n[5] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xi-\naopeng Zhang, Wenyu Liu, Matthias Nie\u00dfner, and Qi Tian.\nFast dynamic radiance fields with time-aware neural vox-\nels. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139,\n2022. 3, 6\n[6] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5501\u20135510, 2022. 2\n[7] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12479\u201312488, 2023. 2,\n3, 6\n[8] Wanshui Gan, Hongbin Xu, Yi Huang, Shifeng Chen, and\nNaoto Yokoya.\nV4d: Voxel for 4d novel view synthesis.\nIEEE Transactions on Visualization and Computer Graph-\nics, 2023. 6, 8\n[9] Xiang Guo, Guanying Chen, Yuchao Dai, Xiaoqing Ye, Ji-\nadai Sun, Xiao Tan, and Errui Ding. Neural deformable voxel\ngrid for fast optimization of dynamic view synthesis. In Pro-\nceedings of the Asian Conference on Computer Vision, pages\n3757\u20133775, 2022. 3, 5, 6, 8\n[10] Hankyu Jang and Daeyoung Kim.\nD-tensorf:\nTenso-\nrial radiance fields for dynamic scenes.\narXiv preprint\narXiv:2212.02375, 2022. 3\n[11] Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy\nMitra. Relu fields: The little non-linearity that could. In\nACM SIGGRAPH 2022 Conference Proceedings, pages 1\u20139,\n2022. 2\n[12] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics, 42\n(4), 2023. 2, 3, 4\n[13] Numair Khan, Min H. Kim, and James Tompkin. Differen-\ntiable diffusion for dense depth estimation from multi-view\nimages.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2021. 2\n[14] Georgios Kopanas, Julien Philip, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis.\nPoint-based neural rendering with per-\nview optimization. Computer Graphics Forum (Proceedings\nof the Eurographics Symposium on Rendering), 40(4), 2021.\n3\n[15] Christoph\nLassner\nand\nMichael\nZollh\u201dofer.\narXiv:2004.07484, 2020. 3\n[16] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of dy-\nnamic scenes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2021.\n3\n[17] Yiqing Liang, Eliot Laidlaw, Alexander Meyerowitz, Srinath\nSridhar, and James Tompkin. Semantic attention flow fields\nfor monocular dynamic scene decomposition. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV), pages 21797\u201321806, 2023. 3\n[18] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\nDeva Ramanan. Dynamic 3d gaussians: Tracking by per-\nsistent dynamic view synthesis. In 3DV, 2024. 2, 3\n[19] Youssef A Mejjati, Isa Milefchik, Aaron Gokaslan, Oliver\nWang, Kwang In Kim, and James Tompkin. GaussiGAN:\nControllable image synthesis with 3d gaussians from un-\nposed silhouettes.\nIn British Machine Vision Conference,\n2021. 2\n[20] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 1\n[21] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 2\n[22] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nICCV, 2021. 6, 8\n[23] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. ACM Trans. Graph., 40(6), 2021. 3, 6, 8\n[24] Sungheon Park, Minjung Son, Seokhwan Jang, Young Chun\nAhn, Ji-Yeon Kim, and Nahyup Kang. Temporal interpola-\ntion is all you need for dynamic neural radiance fields. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 4212\u20134221, 2023. 2\n[25] Sergey Prokudin,\nQianli Ma,\nMaxime Raafat,\nJulien\nValentin, and Siyu Tang.\nDynamic point fields.\narXiv\npreprint arXiv:2304.02626, 2023. 3\n[26] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer.\nD-nerf: Neural radiance fields\nfor dynamic scenes. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10318\u201310327, 2021. 5, 6, 7\n[27] Helge Rhodin, Nadia Robertini, Christian Richardt, Hans-\nPeter Seidel, and Christian Theobalt. A versatile scene model\nwith differentiable visibility applied to generative pose esti-\nmation. In Proceedings of the IEEE International Confer-\nence on Computer Vision (ICCV), 2015. 2\n[28] Darius R\u00a8uckert, Linus Franke, and Marc Stamminger. Adop:\nApproximate differentiable one-pixel point rendering. arXiv\npreprint arXiv:2110.06635, 2021. 3\n[29] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,\nHongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural\n4d decomposition for high-fidelity dynamic reconstruction\nand rendering. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16632\u2013\n16642, 2023. 3\n[30] Carsten Stoll, Nils Hasler, Juergen Gall, Hans-Peter Sei-\ndel, and Christian Theobalt. Fast articulated motion track-\ning using a sums of gaussians body model. In 2011 Inter-\nnational Conference on Computer Vision, pages 951\u2013958.\nIEEE, 2011. 2\n[31] Cheng Sun, Min Sun, and H Chen. Direct voxel grid opti-\nmization: Super-fast convergence for radiance fields recon-\nstruction. 2022 ieee. In CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 5449\u20135459, 2021. 2\n[32] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael\nZollh\u00a8ofer, Christoph Lassner, and Christian Theobalt. Non-\nrigid neural radiance fields: Reconstruction and novel view\nsynthesis of a dynamic scene from monocular video. 2021\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 12939\u201312950, 2021. 3\n[33] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Wang Xinggang.\n4d gaussian splatting for real-time dynamic scene rendering.\narXiv preprint arXiv:2310.08528, 2023. 3, 6\n[34] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, For-\nrester Cole, and Cengiz Oztireli. D\u02c6 2nerf: Self-supervised\ndecoupling of dynamic and static objects from a monocular\nvideo. Advances in Neural Information Processing Systems,\n35:32653\u201332666, 2022. 3\n[35] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,\nShiqin Yan, Numair Khan, Federico Tombari, James Tomp-\nkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in\nvisual computing and beyond. Computer Graphics Forum,\n2022. 1\n[36] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin\nShu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:\nPoint-based neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5438\u20135448, 2022. 3\n[37] Zhiwen Yan, Chen Li, and Gim Hee Lee. Nerf-ds: Neural ra-\ndiance fields for dynamic specular objects. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8285\u20138295, 2023. 6, 8\n[38] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing\nZhang, and Xiaogang Jin. Deformable 3d gaussians for high-\nfidelity monocular dynamic scene reconstruction, 2023. 3\n[39] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li\nZhang. Real-time photorealistic dynamic scene representa-\ntion and rendering with 4d gaussian splatting, 2023. 3\n[40] Wang Yifan, Felice Serena, Shihao Wu, Cengiz \u00a8Oztireli, and\nOlga Sorkine-Hornung. Differentiable surface splatting for\npoint-based geometry processing.\nACM Transactions on\nGraphics (proceedings of ACM SIGGRAPH ASIA), 38(6),\n2019. 2, 3\n[41] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng,\nand Angjoo Kanazawa. Plenoctrees for real-time rendering\nof neural radiance fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5752\u2013\n5761, 2021. 2\n[42] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz,\nand Felix Heide.\nDifferentiable point-based radiance\nfields\nfor\nefficient\nview\nsynthesis.\narXiv\npreprint\narXiv:2205.14330, 2022. 3\n[43] Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito,\nMichael Zollh\u00a8ofer, Justus Thies, and Javier Romero. Driv-\nable 3d gaussian avatars. 2023. 3\n[44] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and\nMarkus Gross.\nSurface splatting.\nIn Proceedings of the\n28th annual conference on Computer graphics and interac-\ntive techniques, pages 371\u2013378, 2001. 2, 4\n"
  }
]