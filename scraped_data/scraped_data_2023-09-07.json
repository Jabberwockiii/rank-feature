[
  {
    "title": "SLiMe: Segment Like Me",
    "link": "https://arxiv.org/pdf/2309.03179.pdf",
    "upvote": "29",
    "text": "Published as a conference paper at ICLR 2024\nSLIME: SEGMENT LIKE ME\nAliasghar Khani1,2, Saeid Asgari Taghanaki1,2, Aditya Sanghi1, Ali Mahdavi Amiri2,\nGhassan Hamarneh2\n1 Autodesk Research\n2 School of Computing Science, Simon Fraser University\nSLiMe\nFigure 1: SLiMe. Using just one user-annotated image with various granularity (as shown in the\nleftmost column), SLiMe learns to segment different unseen images in accordance with the same\ngranularity (as depicted in the other columns).\nABSTRACT\nSignificant advancements have been recently made using Stable Diffusion (SD),\nfor a variety of downstream tasks, e.g., image generation and editing. This moti-\nvates us to investigate SD\u2019s capability for image segmentation at any desired gran-\nularity by using as few as only one annotated sample, which has remained largely\nan open challenge. In this paper, we propose SLiMe, a segmentation method,\nwhich frames this problem as a one-shot optimization task. Given a single image\nand its segmentation mask, we propose to first extract our novel weighted accu-\nmulated self-attention map along with cross-attention map from text-conditioned\nSD. Then, we optimize text embeddings to highlight areas in these attention maps\ncorresponding to segmentation mask foregrounds. Once optimized, the text em-\nbeddings can be used to segment unseen images. Moreover, leveraging additional\nannotated data when available, i.e., few-shot, improves SLiMe\u2019s performance.\nThrough broad experiments, we examined various design factors and showed that\nSLiMe outperforms existing one- and few-shot segmentation methods. The source\ncode of the project is publicly available.\n1\nINTRODUCTION\nImage segmentation is a multifaceted problem, with solutions existing at various levels of granular-\nity. For instance, in applications like expression recognition or facial alignment, segmenting images\nof faces into basic regions like nose and eyes might suffice. However, in visual effects applications,\nmore detailed segments such as eye bags, forehead, and chin are necessary for tasks like wrinkle\nremoval. Moreover, from the perspective of an end-user, a straightforward and effective approach to\nguide a segmentation method is determining what to segment and the desired level of detail across\n1\narXiv:2309.03179v4  [cs.CV]  14 Mar 2024\nPublished as a conference paper at ICLR 2024\n(a)\n(b)\n(c)\n(d)\nFigure 2: Our proposed weighted accumulated self-attention maps\u2019 sample results. Employing\ncross-attention na\u00a8\u0131vely without the self-attention for segmentation leads to inaccurate and noisy\noutput (a and c). Using self-attention map along with cross-attention map to create WAS-attention\nmap enhances the segmentation (b and d).\na broad set of images by providing only one or a few segmented examples for the method to use for\ntraining. Meanwhile, the user should not need to curate a large dataset with segmentation masks,\ntrain a large segmentation model, or encode elaborate and specific properties of target objects into\nthe model. As a result, a customizable segmentation method that can adapt to different levels of\ngranularity, using a few annotated samples, and provide users with the ability to intuitively define\nand refine the target segmentation according to their specific requirements, is of high importance.\nRecent research has tackled the lack of segmentation data by delving into zero-shot, textual descrip-\ntion based segmentation, and few-shot learning. DiffSeg (Tian et al., 2023) is a zero-shot segmen-\ntation method based on SD, which segments everything in the image. However, DiffSeg cannot\nbe used to segment a specific object or part in the test images given a train sample, because its\nsegmentation is not controllable in terms of which object to segment and segmentation granularity.\nPeekaboo (Burgert et al., 2022) is another work, which uses textual description for segmentation.\nTo this end, given an image and a textual description of the target object to be segmented, they use\nStable Diffusion and its loss function to optimize a randomly initialized segmentation mask to reach\nthe desired mask. Nevertheless, it cannot be used to segmentation of test images given train images,\nbecause the textual description of the target object in each image is unique and is not transferrable.\nAnother promising method is ReGAN (Tritrong et al., 2021). ReGAN first trains a GAN (Goodfel-\nlow et al., 2014) on the data of a specific class they aim to segment. Following this, they generate\ndata by this GAN and the user manually annotates the generated data. Then both the generated data\u2019s\nfeatures from the GAN and the annotations are utilized to train a segmentation model. In contrast,\nSegDDPM (Baranchuk et al., 2021) extracts features from a pre-trained diffusion model (DM) and\ntrains an ensemble of MLPs for segmentation using few labeled data. Both excel in segmentation\nwith 10-50 examples but struggle with extremely limited samples. Furthermore, these models re-\nquire training on data specific to each category. For instance, to segment horses, it is necessary to\ncollect a large dataset of horse images, a task that can be inherently cumbersome.\nWhereas, SegGPT (Wang et al., 2023) employs one-shot learning, training on color-randomized\nsegmentation data which includes both instance and part-level masks. During inference, it segments\nonly one region in a target image using a reference image and its binary segmentation mask. While\nSegGPT is effective, it demands a significant amount of annotated segmentation data for initial\ntraining, keeping the challenge of training effectively with a single annotation still unaddressed.\nIn this paper, we propose Segment Like Me (SLiMe), which segments any object/part from the\nsame category based on a given image and its segmentation mask with an arbitrary granularity level\nin a one-shot manner, avoiding the need for extensive annotated segmentation data or training a\ngenerative model like GAN for a specific class (see Figure 1 and Figure 8 for some examples).\nFor this purpose, we leverage the rich knowledge of existing large-scale pre-trained vision/language\nmodel, Stable Diffusion (SD) (Rombach et al., 2022a). Recent studies like (Hertz et al., 2022)\nhave shown that the cross-attention maps of models like SD highlight different regions of the image\nwhen the corresponding text changes. This property has been utilized to modify generated images\n(Hertz et al., 2022) and to achieve image correspondence (Hedlin et al., 2023). Expanding on this\nidea, we present two key insights. First, the multifaceted segmentation problem can be framed as\na one-shot optimization task where we fine-tune the text embeddings of SD to capture semantic\ndetails such as segmented regions guided by a reference image and its segmentation mask, where\neach text embedding corresponds to a distinct segmented region. Second, we observed that using\nstandalone cross-attention maps lead to imprecise segmentations, as depicted in Figure 2. To rectify\nthis, we propose a novel weighted accumulated self (WAS)-attention map (see Section 4). This\n2\nPublished as a conference paper at ICLR 2024\nattention map incorporates crucial semantic boundary information and employs higher-resolution\nself-attention maps, ensuring enhanced segmentation accuracy.\nBased on these insights, SLiMe uses a single image and its segmentation mask to fine-tune SD\u2019s\ntext embeddings through cross- and WAS-attention maps. These refined embeddings emphasize\nsegmented regions within these attention maps, and are used to segment real-world images during\ninference, mirroring the granularity of the segmented region from the image used for optimization.\nThrough various quantitative and qualitative experiments, we highlight the efficacy of our approach.\nSLiMe, even when reliant on just one or a handful of examples, proves to be better or comparable to\nsupervised counterparts demanding extensive training. Furthermore, despite not being trained on a\nspecific category, SLiMe outperforms other few-shot techniques on average and on most parts, across\nalmost all the datasets. For instance, we outperform ReGAN (Tritrong et al., 2021) by nearly 10%\nand SegDDPM (Baranchuk et al., 2021) by approximately 2% in a 10-sample setting. Additionally,\nin a 1-sample context, we exceed SegGPT by around 12% and SegDDPM by nearly 11%.\n2\nRELATED WORK\nSemantic Part Segmentation. In computer vision, semantic segmentation, wherein a class label is\nassigned to each pixel in an image, is an important task with several applications such as scene pars-\ning, autonomous systems, medical imaging, image editing, environmental monitoring, and video\nanalysis (Sohail et al., 2022; He et al., 2016; Chen et al., 2017a; Zhao et al., 2017; He et al., 2017;\nChen et al., 2017b; Sandler et al., 2018; Chen et al., 2018). A more fine-grained derivative of seman-\ntic segmentation is semantic part segmentation, which endeavors to delineate individual components\nof objects rather than segmenting the entirety of the objects. Algorithms tailored for semantic part\nsegmentation find applications in subsequent tasks such as pose estimation (Zhuang et al., 2021),\nactivity analysis (Wang & Yuille, 2015), object re-identification (Cheng et al., 2016), autonomous\ndriving and robot navigation (Li et al., 2023). Despite notable advancements in this domain (Li\net al., 2023; 2022), a predominant challenge faced by these studies remains the substantial need for\nannotated data, a resource that is often difficult to procure. Hence, to address these challenges, re-\nsearch has pivoted towards exploring alternative inductive biases and supervision forms. However,\na limitation of such methodologies is their reliance on manually curated information specific to the\nobject whose parts they aim to segment. For example, authors of (Wang & Yuille, 2015) integrate\ninductive biases by harnessing edge, appearance, and semantic part cues for enhanced part segmen-\ntation. Compared to these approaches, our method only necessitates a single segmentation mask and\ndoesn\u2019t rely on ad-hoc inductive biases, instead leveraging the knowledge embedded in SD.\nFew-shot Semantic Part Segmentation. One approach to reduce the need for annotated data is to\nframe the problem within the few-shot part segmentation framework. There is a large body of work\non few-shot semantic segmentation (Catalano & Matteucci, 2023; Xiong et al., 2022; Johnander\net al., 2022; Zhang et al., 2022; Li et al., 2022), however, they mostly focus on the object- (not part-)\nlevel. A recent paper, ReGAN (Tritrong et al., 2021), proposed a few-shot method for part seg-\nmentation. To achieve this, the researchers leveraged a large pre-trained GAN, extracting features\nfrom it and subsequently training a segmentation model using these features and their associated\nannotations. While this approach enables the creation of a semantic part segmentation model with\nlimited annotated data, it suffers from a drawback. Specifically, to train a model to segment parts\nof a particular object category, first a GAN is required to be trained from scratch on data from the\nsame category. For instance, segmenting parts of a human face would necessitate a GAN trained on\ngenerating human face images. Thus, even though the method requires minimal annotated data, it\ndemands a substantial amount of images from the relevant category. Following that, a few images,\nwhich are generated by the GAN, need to be manually annotated to be used for training the segmen-\ntation model. Afterward, a multitude of images should be generated by the GAN and segmented\nby the trained segmentation model. Finally, all the annotated data and pseudo-segmented data are\nused for training a segmentation model from scratch. Instead, we leverage pre-trained DMs that are\ntrained on large general datasets, eliminating the need to curate category-specific datasets.\nDiffusion models for semantic part segmentation. DMs (Sohl-Dickstein et al., 2015) are a class of\ngenerative models that have recently gained significant attention because of their ability to generate\nhigh-quality samples. DMs have been used for discriminative tasks such as segmentation, as shown\n3\nPublished as a conference paper at ICLR 2024\nin SegDDPM (Baranchuk et al., 2021). Given a few annotated images, they use internal features of\nDM, to train several MLP modules, for semantic part segmentation. Compared to SegDDPM, we\nutilize the semantic knowledge of text-conditioned SD, and just optimize the text embeddings. This\nway, we have to optimize fewer parameters for the segmentation task, which makes it possible to\noptimize using just one segmentation sample.\nSD (Rombach et al., 2022a) has been used for several downstream tasks such as generating faithful\nimages (Chefer et al., 2023), inpainting, outpainting (Rombach et al., 2022a), generating 3D shapes\nusing text (Tang, 2022), and editing images guided by a text prompt (Brooks et al., 2023). In addition\nto these, a large body of work fine-tune SD or use its cross-attention modules to perform interesting\ntasks. For instance, (Gal et al., 2022) fine-tunes SD\u2019s text embeddings to add a new object or\nstyle to its image generation space. Another example, (Hertz et al., 2022) uses SD\u2019s cross-attention\nmodules to impose more control over the generation process. Moreover, in a third instance, authors\nof (Mokady et al., 2023) edit a real image using SD\u2019s cross-attention modules. SD\u2019s cross-attention\nmaps have been used for image correspondence by (Hedlin et al., 2023). Lastly, a recent paper\n(Patashnik et al., 2023), uses SD\u2019s self-attention and cross-attention modules for object level shape\nvariations. Although these papers explore the applicability of SD in different tasks, its utilization in\nsemantic part segmentation is not fully explored. Therefor, in this work, we take advantage of SD\u2019s\nself-attention and cross-attention modules and fine-tune its text embeddings through these attention\nmechanisms to perform semantic part segmentation even with just one annotated image.\n3\nBACKGROUND\nLatent Diffusion Model (LDM). One category of generative models are LDMs, which model the\ndata distribution by efficiently compressing it into the latent space of an autoencoder and utilizing a\nDM to model this latent space. An appealing feature of LDMs is that their DM, denoted as \u03f5(.; \u03b8),\ncan be extended to represent conditional distributions, conditioned on text or category. To train a\ntext-conditioned LDM, a natural language prompt is tokenized to obtain P. Then P is passed to\na text encoder G(.; \u03b8) to get P = G(P; \u03b8). Alternatively, it is possible to obtain P by randomly\ninitializing a tensor of the same size. Afterward, the input image I is encoded to obtain I, and a\nstandard Gaussian noise \u03f5 is added to it with respect to time step t to get It. Finally, the following\nobjective is used to optimize the parameters of both G(.; \u03b8) and \u03f5(.; \u03b8), with the aim of enabling the\nmodel to acquire the capability to predict the added noise \u03f5:\nLLDM = EI,\u03f5\u223cN (0,1),t[\u2225\u03f5 \u2212 \u03f5(It, t, P; \u03b8)\u22252\n2].\n(1)\nIn this work, we use text-conditioned SD (Rombach et al., 2022b), as our LDM, for two reasons.\nFirst, SD is conditioned on the text using the cross-attention modules, which have shown to exhibit\nrich semantic connections between the text and the image embeddings (Hertz et al., 2022). Second,\nthe internal features of SD are semantically meaningful and preserve the visual structure of the input\nimage, enhancing the interrelation between text and image.\nAttention Modules. SD\u2019s DM employs a UNet structure, which has two types of attention mod-\nules (Vaswani et al., 2017): self-attention and cross-attention. The self-attention module calculates\nattention across the image embedding, capturing relationships between a specific element and other\nelements within the same image embedding. On the other hand, the cross-attention module com-\nputes relationships between the latent representations of two different modalities, like text and image\nin the case of text-conditioned SD.\nAn attention module comprises three components: query, key, and value. It aims to transform the\nquery into an output using the key-value pair. Therefore, given query Q, key K, and value V vectors\nwith the dimension of d, the output O of an attention module is defined as follows:\nO = Softmax\n\u0012QK\u22ba\n\u221a\nd\n\u0013\n\u00b7 V.\n(2)\nIn the self-attention module, the query, key, and value vectors are derived from the image embedding,\nwhile in the cross-attention module, the query vector is derived from the image embedding, and\n4\nPublished as a conference paper at ICLR 2024\nImage Encoder\nText Embedding\nCross-attention\nWAS-attention\nGround truth mask\nCE Loss\nPredicted Noise\nNoise\nMSE Loss\nUNet\nAttention-\nExtraction\nmodule\nMSE Loss\nFigure 3: Optimization step. After extracting image embeddings and adding noise, we pass them,\nalong with a text embedding obtained either by using a text encoder or initialized randomly, through\nthe UNet to obtain cross- and WAS-attention maps. Two losses are then calculated using these maps\nand the ground truth mask. Additionally, SD\u2019s loss is incorporated from comparing the added noise\nwith the UNet\u2019s predicted noise.\nthe key and value vectors are derived from the text embedding. In our scenario, we extract the\nnormalized attention map denoted as S = Softmax\n\u0010\nQK\u22ba\n\u221a\nd\n\u0011\n, which is applicable to both the self-\nattention and cross-attention modules, and we note them as Ssa \u2208 RH\u2032\u00d7W \u2032\u00d7H\u2032\u00d7W \u2032 and Sca \u2208\nRH\u2032\u00d7W \u2032\u00d7T , respectively. In this context, H\u2032 and W \u2032 represent the height and width of the image\nembedding and T denotes the total number of text tokens. Ssa shows the pairwise similarity of\nthe elements in its input image embedding. Hence, each element p in its input, is associated with\nan activation map, highlighting the similar elements to p (Patashnik et al., 2023). Moreover, the\nintensity of the similar elements decrease as we move farther away from p. On the other hand, for\neach text token, Sca has an activation map, which effectively spotlights elements within the image\nembedding that align with that token within the model\u2019s semantic space. For example, if the model is\ninstructed to generate an image of a bear with the text prompt \u201ca bear\u201d, the activation map associated\nwith \u201cbear\u201d token within Sca, will emphasize on those elements that correspond to the bear object\nwithin the generated image.\n4\nMETHOD\nWe introduce SLiMe, a method that enables us to perform segmentation at various levels of granular-\nity, needing only one image and its segmentation mask. Prior research has demonstrated that SD\u2019s\ncross-attention maps can be used in detecting coarse semantic objects during the generation process\nfor more control in generation (Hertz et al., 2022) or finding correspondence between images (Hedlin\net al., 2023). However, there remains uncertainty regarding the applicability of cross-attention maps\nfor finer-grained segmentation of objects or parts, especially within real-world images. To resolve\nthis, we frame the segmentation problem as a one-shot optimization task where we extract the cross-\nattention map and our novel WAS-attention map to fine-tune the text embeddings, enabling each text\nembedding to grasp semantic information from individual segmented regions (Figure 3). During the\ninference phase, we use these optimized embeddings to obtain the segmentation mask for unseen\nimages. In what follows, we will first delve into the details of the text embedding optimization and\nthen the inference process.\n4.1\nOPTIMIZING TEXT EMBEDDING\nGiven a pair of an image (I \u2208 RH\u00d7W \u00d73) and a segmentation mask (M \u2208 {0, 1, 2, ..., K \u2212 1}H\u00d7W )\nwith K classes, we optimize the text embeddings using three loss terms. The first loss term is a cross\nentropy loss between the cross-attention map and the ground truth mask. The second one, is the\nMean Squared Error (MSE) loss between the WAS-attention map and the ground truth mask. These\nloss terms refine the text embeddings and enable them to learn to emphasize segmented regions\nwithin both cross- and WAS-attention maps. Additionally, there is a subsequent SD regularization\nterm to ensure that the optimized text embeddings remain within the trained distribution of SD.\nTo optimize the text embeddings, it is necessary to extract the cross-attention and self-attention\nmaps. These maps are derived from SD\u2019s UNet by initially encoding the training image I into the\nimage embedding, I. Subsequently, a standard Gaussian noise is added to this embedding with\nrespect to the time step topt, resulting in It. Next, a text prompt is converted to a sequence of\n5\nPublished as a conference paper at ICLR 2024\nUNet\nResize and\nAverage\nFlatten\nAverage\nResnet Layer\nCross-attention Layer\nSelf-attention Layer\n. . .\nWeighted Sum\n. . .\nPredicted\nNoise\nAttention-Extraction module\nFigure 4: Attention-Extraction module. To extract WAS-attention map of kth text embedding\nwith respect to an image, we follow these three steps: (1) We feed the kth text embedding (Pk)\ntogether with the noised embedding of the image (It) to the UNet. Then calculate Ak\nca by extracting\nthe cross-attention maps of Pk from several layers, resizing and averaging them. (2) We extract\nthe self-attention maps from several layers and average them (Asa). (3) Finally, we flatten Ak\nca to\nget F k\nca and calculate a weighted sum of channels of Asa, by weights coming from F k\nca, and call\nit \u201cWeighted Accumulated Self-attention map\u201d (Sk\nWAS). The UNet also produces an output that\nrepresents the predicted noise, which is used for calculating the loss of the SD.\ntext embeddings denoted as P. We then take the first K text embeddings and optimize them. The\ncorresponding text embedding of each class is denoted by Pk. It is essential to note that SD is con-\nfigured to handle 77 text tokens. Consequently, our method can accommodate up to 77 segmentation\nclasses, which is sufficient for most applications. Finally, P and It are fed into the UNet to obtain\nthe denoised image embedding I\u2032 and extract the cross- and self-attention maps.\nSD has multiple cross-attention modules distributed across various layers. We denote the normalized\ncross-attention map of the lth layer as {Sca}l \u2208 RH\u2032\nl\u00d7W \u2032\nl \u00d7T and average them over different layers,\nas we have empirically observed that this averaging improves the results. However, since H\u2032\nl and\nW \u2032\nl vary across different layers, we resize all {Sca}l to a consistent size for all the utilized layers.\nFinally, the attention map employed in our loss function is calculated as follows:\nAca = Averagel(Resize({Sca}l)),\n(3)\nwhere Aca \u2208 RH\u2032\u2032\u00d7W \u2032\u2032\u00d7T , Averagel computes the average across layers, and Resize refers to\nbilinear interpolation for resizing to dimensions H\u2032\u2032 \u00d7W \u2032\u2032. Figure 4 visually depicts this procedure.\nFinally, we compute the cross-entropy loss between the resized ground truth mask M to H\u2032\u2032 \u00d7 W \u2032\u2032\n(referred to as M \u2032) and first K channels in the resized cross-attention map Aca for k = {0, ..., K \u2212\n1}, as outlined below:\nLCE = CE(A[0:K\u22121]\nca\n, M \u2032),\n(4)\nwhere CE refers to cross-entropy. Using this loss, we optimize kth text embedding such that Ak\nca\nhighlights the kth class\u2019s region in the segmentation mask, for k = {1, ..., K \u2212 1}. Note that we\ndo not optimize the first text embedding and assign A0\nca to the background class, as empirically we\nhave found that optimizing it yields suboptimal performance.\nHowever, as the resolution of {Sca}l we use are lower than the input image, object edges are vague\nin them. To enhance segmentation quality, we propose WAS-attention map, which integrates both\nself-attention and cross-attention maps. Besides possessing pairwise similarity between the image\nembedding\u2019s elements, the self-attention map has two additional features that make it suitable to\nbe used for improving the segmentation results. First, the self-attention maps that we use, have\nhigher resolution of feature maps compared to utilized cross-attention maps. Second, it shows the\nboundaries in more detail.\nTable 1, shows the importance of using the WAS-attention map which\nyields an average improvement of 6.0% in terms of mIoU over simply using the cross-attention map\nfor generating the segmentation mask. Like the cross-attention maps, we extract self-attention maps\nfrom multiple layers and compute their average as follows:\nAsa = Averagel({Ssa}l),\n(5)\nwhere Asa \u2208 RH\u2032\nl\u00d7W \u2032\nl \u00d7H\u2032\nl\u00d7W \u2032\nl and Averagel calculates the average across layers. In equation 5\nthere is no need for a Resize function as the self-attention maps that we use, all have the same size.\n6\nPublished as a conference paper at ICLR 2024\nTable 1: Ablating the effect of WAS-attention. These numerical results, underscore the crucial\ncontribution of WAS-attention maps to the quality of SLiMe\u2019s outcomes.\nUse WAS-Attention Map\nBody\nLight\nPlate\nWheel\nWindow\nBG\nAverage\n\u2717\n77.8 \u00b1 0.2\n48.2 \u00b1 2.5\n44.1 \u00b1 4.2\n63.9 \u00b1 0.1\n66.9 \u00b1 0.2\n75.3 \u00b1 0.2\n62.7 \u00b1 1.3\n\u2713\n81.5 \u00b1 1.0\n56.8 \u00b1 1.2\n54.8 \u00b1 2.7\n68.3 \u00b1 0.1\n70.3 \u00b1 0.9\n78.4 \u00b1 1.6\n68.3 \u00b1 1.0\nTo calculate WAS-attention map, we first resize Ak\nca to match the size of Asa using bilinear inter-\npolation and call it Rk\nca. Consequently, for each element p in Rk\nca we have a channel in Asa that\nhighlights relevant elements to p. Finally, we calculate the weighted sum of channels of Asa to\nobtain Sk\nWAS (WAS-attention map). The weight assigned to each channel is the value of the corre-\nsponding element of that channel in Rk\nca (Figure 4). This process can be outlined as follows:\nSk\nWAS = sum(flatten(Rk\nca) \u2299 Asa).\n(6)\nThis refinement enhances the boundaries because Asa possesses rich understanding of the semantic\nregion boundaries (see the cross-attention and WAS-attention maps in Figure 3). At the end, we\nresize Sk\nWAS to H\u2032\u2032 \u00d7 W \u2032\u2032 and calculate the MSE loss this way:\nLMSE =\nK\u22121\nX\nk=0\n\u2225Resize(Sk\nWAS) \u2212 M \u2032\nk\u22252\n2,\n(7)\nwhere M \u2032\nk is a binary mask coming from the resized ground truth mask M \u2032, in which only the pixels\nof the kth class are 1.\nThe last loss we use is the SD\u2019s loss function (LLDM), which is the MSE loss between the added noise\nand the predicted noise. We use this loss to prevent the text embeddings from going too far from the\nunderstandable space by SD. Finally, our objective to optimize the text embeddings is defined as:\nL = LCE + \u03b1LMSE + \u03b2LLDM,\n(8)\nwhere \u03b1 and \u03b2 are the coefficients of the loss functions.\n4.2\nINFERENCE\nDuring inference, our objective is to segment unseen images at the same level of details as the image\nused during optimization. To achieve this, we begin with the unseen image and encode it into the\nlatent space of SD. Following this, a standard Gaussian noise is introduced to the encoded image,\nwith the magnitude determined by the time parameter ttest. Subsequently, we use the optimized text\nembeddings along with the encoded image to derive corresponding cross-attention and self-attention\nmaps from the UNet model. These attention maps, as shown in Figure 4, enable us to obtain WAS-\nattention maps for each text embedding. Afterward, we select the first K WAS-attention maps that\ncorrespond to K classes. These selected maps are then resized using bilinear interpolation to match\nthe dimensions of the input image and are stacked along the channel dimension. Subsequently, we\ngenerate a segmentation mask by performing an argmax across the channels. It is important to note\nthat this process can be repeated for multiple unseen images during inference, without requiring a\nnew optimization. An analysis of the selection of various parameters used in our method is provided\nin the Appendix A.2.\n5\nEXPERIMENTS\nIn this section, we demonstrate the superiority of SLiMe in semantic part segmentation. We use\nmIoU to compare our approach against three existing methods: ReGAN (Tritrong et al., 2021),\nSegDDPM (Baranchuk et al., 2021), and SegGPT (Wang et al., 2023) on two datasets: PASCAL-\nPart (Chen et al., 2014) and CelebAMask-HQ (Lee et al., 2020). ReGAN and SegDDPM utilize pre-\ntrained GAN and DDPM models, respectively, training them on FFHQ and LSUN-Horse datasets\nfor face and horse part segmentation. Additionally, ReGAN employs a pre-trained GAN from the\nLSUN-Car dataset for car part segmentation. We present the results for both 10-sample and 1-sample\n7\nPublished as a conference paper at ICLR 2024\nTable 2: Segmentation results for class car. SLiMe consistently outperforms ReGAN, even though\nReGAN utilized generated data alongside 10 annotated data for training. Furthermore, our method\nexhibits superior performance to SegGPT on average, despite SegGPT being supervised. The first\ntwo rows show the supervised methods, for which we use the reported numbers in ReGAN. The\nsecond two rows show the 10-sample setting and the last two rows, refer to the 1-sample scenario.\n\u22c6 indicates the supervised methods.\nBody\nLight\nPlate\nWheel\nWindow\nBackground\nAverage\nCNN\u22c6\n73.4\n42.2\n41.7\n66.3\n61.0\n67.4\n58.7\nCNN+CRF\u22c6\n75.4\n36.1\n35.8\n64.3\n61.8\n68.7\n57\nReGAN\n75.5\n29.3\n17.8\n57.2\n62.4\n70.7\n52.15\nSLiMe\n81.5 \u00b1 1.0\n56.8 \u00b1 1.2\n54.8 \u00b1 2.7\n68.3 \u00b1 0.1\n70.3 \u00b1 0.9\n78.4 \u00b1 1.6\n68.3 \u00b1 1.0\nSegGPT\u22c6\n62.7\n18.5\n25.8\n65.8\n69.5\n77.7\n53.3\nSLiMe\n79.6 \u00b1 0.4\n37.5 \u00b1 5.4\n46.5 \u00b1 2.6\n65.0 \u00b1 1.4\n65.6 \u00b1 1.6\n75.7 \u00b1 3.1\n61.6 \u00b1 0.5\nTable 3: Segmentation results for class horse.\nSLiMe outperforms ReGAN, SegDDPM, and\nSegGPT on average and most of the parts. The first two rows show the supervised methods, for\nwhich we use the reported numbers in ReGAN. The middle three rows show the 10-sample setting\nand the last three rows, are the results of the 1-sample scenario. \u22c6 indicates the supervised methods.\nHead\nLeg\nNeck+Torso\nTail\nBackground\nAverage\nShape+Appereance\u22c6\n47.2\n38.2\n66.7\n-\n-\n-\nCNN+CRF\u22c6\n55.0\n46.8\n-\n37.2\n76\n-\nReGAN\n50.1\n49.6\n70.5\n19.9\n81.6\n54.3\nSegDDPM\n41.0\n59.1\n69.9\n39.3\n84.3\n58.7\nSLiMe\n63.8 \u00b1 0.7\n59.5 \u00b1 2.1\n68.1 \u00b1 4.4\n45.4 \u00b1 2.4\n79.6 \u00b1 2.5\n63.3 \u00b1 2.4\nSegGPT\u22c6\n41.1\n49.8\n58.6\n15.5\n36.4\n40.3\nSegDDPM\n12.1\n42.4\n54.5\n32.0\n74.1\n43.0\nSLiMe\n61.5 \u00b1 1.0\n50.3 \u00b1 0.7\n55.7 \u00b1 1.1\n40.1 \u00b1 2.9\n74.4 \u00b1 0.6\n56.4 \u00b1 0.8\nsettings, utilizing a single validation sample for 10-sample experiments of SLiMe. Also, all exper-\niments are conducted three times with different initializations, reporting their mean and standard\ndeviation. We conduct experiments for SegDDPM and SegGPT using the custom version of test\nsets of the above-mentioned datasets, which are based on ReGAN settings, and report their results\naccordingly. For the remaining methods, we reference the results reported by ReGAN. Note that Re-\nGAN and SegDDPM are not universally applicable to arbitrary classes, unless a large dataset for the\ngiven class is collected and a generative model is trained. However, SLiMe does not require collect-\ning large category specific data and training an additional generative model, because of the inherent\nsemantic knowledge embedded in SD (Figure 8). Whereas SegGPT requires a large segmentation\ndataset to be trained initially.\nPASCAL-Part. This dataset provides detailed annotations of object parts. For our experiments, we\nfocus on car and horse classes (for more details, please refer to Appendix B.1). Table 2 presents\nresults for the car class. As there is no available pre-trained model for the car class in SegDDPM,\nwe couldn\u2019t make a comparison with this model for this category. As evident from Table 2, SLiMe\noutperforms ReGAN in the 10-sample setting on average and all the part segments by a significant\nmargin. Moreover, in the 1-sample setting, SLiMe either outperforms SegGPT by a large margin or\nperforms comparably. Likewise, Table 3 displays our results for the horse class, where it is evident\nthat our method, SLiMe, outperforms ReGAN, SegDDPM, and SegGPT on average and for most of\nthe parts. It is worth noting that, even though SegGPT only requires a single segmentation sample for\ninference, it is a fully supervised method and demands a large segmentation dataset for training. In\ncontrast, SLiMe is truly a one-shot technique, where only a single sample is needed for optimization.\nCelebAMask-HQ. This is a dataset of the facial part segmentation, and we report results on the parts\nused in ReGAN for comparison (for more details, please consult Appendix B.1). Figure 6 and Table\n4 showcase our qualitative and quantitative results. In the 1-sample setting, SLiMe outperforms other\nmethods on average and for the majority of parts, demonstrating its superiority in 1-sample scenario.\nOn the other hand, in the 10-sample setting, except for three parts, our method either performs better\nor comparably to other methods. As mentioned earlier, note that SegGPT benefits from training on\n8\nPublished as a conference paper at ICLR 2024\nTable 4: Segmentation results of CelebAMask-HQ10.\nOur method consistently outperforms\nReGAN, SegDDPM, and SegGPT in the majority of parts in 1-sample setting in the last four rows.\nAdditionally, SLiMe either outperforms or performs comparably to ReGAN and SegDDPM in\n10-sample setting in the first three rows. \u22c6 is used to denote supervised methods.\nCloth\nEyebrow\nEar\nEye\nHair\nMouth\nNeck\nNose\nFace\nBackground\nAverage\nReGAN\n15.5\n68.2\n37.3\n75.4\n84.0\n86.5\n80.3\n84.6\n90.0\n84.7\n69.9\nSegDDPM\n61.6\n67.5\n71.3\n73.5\n86.1\n83.5\n79.2\n81.9\n89.2\n86.5\n78.0\nSLiMe\n63.1 \u00b1 1.6\n62.0 \u00b1 1.6\n64.2 \u00b1 1.9\n65.5 \u00b1 3.0\n85.3 \u00b1 0.4\n82.1 \u00b1 1.6\n79.4 \u00b1 2.2\n79.1 \u00b1 1.4\n88.8 \u00b1 0.2\n87.1 \u00b1 0.0\n75.7 \u00b1 0.4\nReGAN\n-\n-\n-\n57.8\n-\n71.1\n-\n76.0\n-\n-\n-\nSegGPT\u22c6\n24\n48.8\n32.3\n51.7\n82.7\n66.7\n77.3\n73.6\n85.7\n28.0\n57.1\nSegDDPM\n28.9\n46.6\n57.3\n61.5\n72.3\n44.0\n66.6\n69.4\n77.5\n76.6\n60.1\nSLiMe\n52.6 \u00b1 1.4\n44.2 \u00b1 2.1\n57.1 \u00b1 3.6\n61.3 \u00b1 4.6\n80.9 \u00b1 0.5\n74.8 \u00b1 2.9\n78.9 \u00b1 1.3\n77.5 \u00b1 1.8\n86.8 \u00b1 0.3\n81.6 \u00b1 0.8\n69.6 \u00b1 0.3\nSLiMe\nSegGPT\nSLiMe\nSegGPT\nFigure 5: Segmentation results of camouflaged objects. The larger images are used for optimizing\nSLiMe, and as the source image for SegGPT. Notably, SLiMe outperforms SegGPT.\na large segmentation dataset. Also, the other two methods employ class-specific pre-trained models.\nIn contrast, SLiMe utilizes a model pre-trained on general data, equipping it with the ability to work\nacross a wide range of categories rather than being limited to a specific class.\nAdditional Results. We also showcase the versatility of our method, which can be optimized on\nan occluded object and infer images without the occlusion, or conversely, be optimized on a fully\nvisible object and make predictions on occluded objects. This shows our method\u2019s capability to\ncomprehend part and object semantics. Figure 11 illustrates that despite occlusion of the target\nregion caused by the person in the image used for optimization, our method performs well. It is also\npossible to segment occluded objects using a visible reference object (see Figure 12). Moreover,\nin Figure 5, we compare our method against SegGPT (Wang et al., 2023) using two camouflaged\nanimals, namely a crab and a lizard. Remarkably, SLiMe achieves precise segmentation of these\nanimals, even in situations where they were challenging to be detected with naked eye. This shows\nthat SLiMe learns rich semantic features about the target object that do not fail easily due to the lack\nof full perception.\n6\nCONCLUSION\nWe proposed SLiMe, a one-shot segmentation method capable of segmenting various objects/parts\nin various granularity. Through an extensive set of experiments and by comparing it to state-of-the-\nart few-shot and supervised image segmentation methods, we showed its superiority. We showed\nthat, although SLiMe does not require training on a specific class of objects or a large segmentation\ndataset, it outperforms other methods. On the other hand, SLiMe has some limitations. For example,\nit may result in noisy segmentations when the target region is tiny. This can be attributed to the\nfact that the attention maps, which we extract from SD for segmentation mask generation, have a\nsmaller size than the input image. To counter this, we employed bilinear interpolation for upscal-\ning. Nonetheless, due to scaling, some pixels might be overlooked, leading to the undesired noisy\noutcomes. For visual examples of this case, please refer to Appendix A.1. Resolving the mentioned\nlimitation, and making it applicable to 3D and videos, would be an interesting future direction.\n9\nPublished as a conference paper at ICLR 2024\nREFERENCES\nDmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Label-\nefficient semantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126, 2021.\nTim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image\nediting instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 18392\u201318402, 2023.\nRyan Burgert, Kanchana Ranasinghe, Xiang Li, and Michael S Ryoo. Peekaboo: Text to image\ndiffusion models are zero-shot segmentors. arXiv preprint arXiv:2211.13224, 2022.\nNico Catalano and Matteo Matteucci. Few shot semantic segmentation: a review of methodologies\nand open challenges. arXiv preprint arXiv:2304.05832, 2023.\nHila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or.\nAttend-and-excite:\nAttention-based semantic guidance for text-to-image diffusion models. ACM Transactions on\nGraphics (TOG), 42(4):1\u201310, 2023.\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and\nfully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):\n834\u2013848, 2017a.\nLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous\nconvolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017b.\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-\ndecoder with atrous separable convolution for semantic image segmentation. In Proceedings of\nthe European conference on computer vision (ECCV), pp. 801\u2013818, 2018.\nXianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille. De-\ntect what you can: Detecting and representing objects using holistic models and body parts. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 1971\u20131978,\n2014.\nDe Cheng, Yihong Gong, Sanping Zhou, Jinjun Wang, and Nanning Zheng. Person re-identification\nby multi-channel parts-based cnn with improved triplet loss function. In Proceedings of the iEEE\nconference on computer vision and pattern recognition, pp. 1335\u20131344, 2016.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\ninversion. arXiv preprint arXiv:2208.01618, 2022.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information\nprocessing systems, 27, 2014.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770\u2013778, 2016.\nKaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Girshick. Mask r-cnn. In Proceedings of the\nIEEE international conference on computer vision, pp. 2961\u20132969, 2017.\nEric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi,\nand Kwang Moo Yi. Unsupervised semantic correspondence using stable diffusion. arXiv preprint\narXiv:2305.15581, 2023.\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,\n2022.\n10\nPublished as a conference paper at ICLR 2024\nJoakim Johnander, Johan Edstedt, Michael Felsberg, Fahad Shahbaz Khan, and Martin Danelljan.\nDense gaussian processes for few-shot segmentation. In European Conference on Computer Vi-\nsion, pp. 217\u2013234. Springer, 2022.\nCheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive\nfacial image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 5549\u20135558, 2020.\nBoyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven\nsemantic segmentation. In International Conference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=RriDjddCLN.\nXiangtai Li, Shilin Xu, Yibo Yang, Haobo Yuan, Guangliang Cheng, Yunhai Tong, Zhouchen Lin,\nand Dacheng Tao. Panopticpartformer++: A unified and decoupled view for panoptic part seg-\nmentation. arXiv preprint arXiv:2301.00954, 2023.\nRon Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for\nediting real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 6038\u20136047, 2023.\nOr Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, and Daniel Cohen-Or.\nLo-\ncalizing object-level shape variations with text-to-image diffusion models.\narXiv preprint\narXiv:2303.11306, 2023.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684\u201310695, 2022a.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022b.\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-\nbilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 4510\u20134520, 2018.\nAli Sohail, Naeem A Nawaz, Asghar Ali Shah, Saim Rasheed, Sheeba Ilyas, and Muhammad Khur-\nram Ehsan. A systematic literature review on machine learning and deep learning methods for\nsemantic segmentation. IEEE Access, 2022.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International conference on machine learn-\ning, pp. 2256\u20132265. PMLR, 2015.\nJiaxiang\nTang.\nStable-dreamfusion:\nText-to-3d\nwith\nstable-diffusion,\n2022.\nhttps://github.com/ashawkey/stable-dreamfusion.\nJunjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, and Mar Gonzalez-Franco. Diffuse,\nattend, and segment: Unsupervised zero-shot segmentation using stable diffusion. arXiv preprint\narXiv:2308.12469, 2023.\nNontawat Tritrong, Pitchaporn Rewatbowornwong, and Supasorn Suwajanakorn. Repurposing gans\nfor one-shot semantic part segmentation. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pp. 4475\u20134485, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\nJianyu Wang and Alan L Yuille. Semantic part segmentation using compositional model combining\nshape and appearance. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 1788\u20131797, 2015.\n11\nPublished as a conference paper at ICLR 2024\nXinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:\nSegmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\nZhitong Xiong, Haopeng Li, and Xiao Xiang Zhu. Doubly deformable aggregation of covariance\nmatrices for few-shot segmentation. In European Conference on Computer Vision, pp. 133\u2013150.\nSpringer, 2022.\nJian-Wei Zhang, Yifan Sun, Yi Yang, and Wei Chen. Feature-proxy transformer for few-shot seg-\nmentation. Advances in Neural Information Processing Systems, 35:6575\u20136588, 2022.\nHengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\nnetwork. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n2881\u20132890, 2017.\nChungang Zhuang, Zhe Wang, Heng Zhao, and Han Ding. Semantic part segmentation method\nbased 3d object pose estimation with rgb-d images for bin-picking. Robotics and Computer-\nIntegrated Manufacturing, 68:102086, 2021.\n12\nPublished as a conference paper at ICLR 2024\nA\nAPPENDIX\nA.1\nADDITIONAL RESULTS\nFigure 6 showcase our qualitative and quantitative results\nFigure 6: Qualitative face segmentation results. Results of SLiMe optimized with 10 samples.\nIn Figure 7, we provide comparisons with ReGAN (Tritrong et al., 2021). It is evident that SLiMe\nexhibits more intricate hair segmentation in the second and third rows, showcasing its ability to\ncapture finer details compared to ReGAN. Additionally, in the second row, the ear segmentation\nproduced by ReGAN appears to be noisy by comparison.\nInput\nSLiMe\nReGAN\nVAE\nJigsaw\nHED\nColor\nBilat\nFigure 7: Qualitative comparison with other methods on CelebAHQ-Mask. Qualitative results\nof several methods on the 10-sample setting of CelebAHQ-Mask. As you can see, SLiMe captures\nthe details better than ReGAN and other methods (e.g., hairlines in the second row). All the images\nare taken from (Tritrong et al., 2021).\nIn addition to the segmentation results presented for several object categories in the paper, Figure 8\nshowcases additional 1-sample visual results. These results encompass a wide range of objects and\nprovide evidence of SLiMe\u2019s capability to perform effectively across various categories.\nAnother noteworthy feature of SLiMe is its generalization capacity, as illustrated in Figure 9. This\nfigure demonstrates, despite being optimized on a single dog image with segmented parts, SLiMe\ncan grasp the concepts of head, body, legs, and tail and effectively apply them to unseen images from\nvarious categories. However, Figure 10 illustrates that when optimized on an image containing both\na dog and a cow, SLiMe is adept at learning to exclusively segment the dog class in unseen images.\nThese two figures, highlight SLiMe\u2019s ability to acquire either high-level concepts or exclusive object\nclasses.\nOne more appealing feature of SLiMe is that not only is it able to learn to segment from an occluded\nimage and segment fully visible objects (Figure 11), but it can also be optimized on a fully visible\nobject and make predictions on occluded samples. As an example, in Figure 12, SLiMe is optimized\non a fully visible bear and predicts an accurate segmentation mask for the occluded bears.\nA.1.1\nFAILURE CASE OF SLiMe\nAs mentioned in the paper, SLiMe may fail in the cases where the target region is tiny. Figure 13\nshows two examples of this, where the target to be segmented is a necklace, which is pretty small.\n13\nPublished as a conference paper at ICLR 2024\nFigure 8:\nPart segmentation results on different objects. SLiMe exhibits strong performance\nacross a wide variety of objects. The images, along with their corresponding annotations used for\noptimization, are displayed on the left.\nA.2\nABLATION STUDIES\nIn this section, we present the results of our ablation studies, which aim to illustrate the impact of\neach component in our model. These experiments were conducted using 10 samples from the car\nclass in the PASCAL-Part dataset.\nIn Table 5, we present the results of text prompt ablation experiments to demonstrate the robustness\nof SLiMe to the choice of the initial text prompt. The \u201cpart names\u201d row displays results obtained by\nusing specific part names in the prompts. In the next row, labeled \u201c \u201d, the text prompt is left empty.\n14\nPublished as a conference paper at ICLR 2024\nFigure 9: Generalizability of SLiMe. SLiMe optimized on dog\u2019s parts, can accurately segment\ncorresponding parts of other animals.\nFigure 10: SLiMe exclusive segmentation. SLiMe optimized on an image containing both dog and\ncow (as seen in the left image pair), can segment only dogs, even in presence of other animals.\nFigure 11: Segmentation results of occluded objects. Although SLiMe is optimized using an\noccluded car\u2019s image (the leftmost image), it demonstrates proficiency in car segmentation on unseen\nimages (the remaining images on the right). Particularly noteworthy is its ability to accurately\nsegment all three cars in the top-right image.\nIn the final row, we use the term \u201cpart\u201d instead of specific part names. By comparing these rows, we\nobserve minimal influence of the initial text prompt on the results.\nMoving on, we conducted experiments to determine the optimal coefficients for our loss functions.\nAs illustrated in the first four rows of Table 6, where \u03b1 = 1, the most suitable coefficient for LSD\nis found to be 0.005. Furthermore, when comparing the 3rd and 5th rows, the significance of LMSE\nbecomes apparent.\nNext, we turn our attention to ablation of the parameters ttrian and ttest. Initially, we vary the range\nused to sample topt. Table 7 shows that the best range is [5, 100], which introduces a reasonable\namount of noise. A larger end value in the range results in very noisy images in some steps, making\nit difficult to optimize the text embeddings. Conversely, a very small end value means that SLiMe\ndoes not encounter a sufficient range of noisy data for effective optimization.\n15\nPublished as a conference paper at ICLR 2024\nFigure 12: Occluded object in inference. SLiMe undergoes its initial optimization with a bear\nimage, as depicted in the left image. Subsequently, it is put to the test with images featuring occluded\nportions of the bear. Notably, SLiMe precisely segments these occluded objects.\n(a)\n(b)\n(c)\n(d)\nFigure 13: Failure case. The segmentation masks generated by SLiMe, depicted in images (b) and\n(d), reveal an inherent challenge. Our method encounters difficulty when it comes to accurately\nsegmenting minuscule objects, such as the necklace in this image. These tiny objects often diminish\nin size, and at times, even vanish within the cross-attention maps we employ, primarily due to their\nlimited resolution.\nAfter examining topt, we ablate the parameter ttest. Selecting an appropriate value for ttest is crucial,\nas demonstrated in Table 8. SLiMe performs optimally when we set ttest to 100.\nAnother parameter subjected to ablation is the learning rate. Choosing the correct learning rate is\nessential, as a high value can result in significant deviations from text embeddings that SD can com-\nprehend. Conversely, a low learning rate may not introduce sufficient changes to the embeddings.\nOur experiments in Table 9 reveal that the optimal learning rate for our method is 0.1.\nFinally, we performed an ablation study on the choice of layers to utilize their cross-attention mod-\nules. Based on our experiments in Table 10, we determined that the best set of layers to use are the\n8th to 12th layers.\nB\nIMPLEMENTATION DETAILS\nWe opted for SD version 2.1 and extracted the cross-attention and self-attention maps from the 8th\nto 12th and last three layers of the UNet, respectively. For the text prompt, we use a sentence where\nthe word \u201cprompt\u201d is repeated by the number of parts to be segmented.\nDuring optimization, we assigned a random value to the time step of SD\u2019s noise scheduler, denoted\nas topt, for each iteration. This value was selected randomly between 5 and 100, where topt can be in\na range spanning from 0 to 1000. During inference, we consistently set ttest to 100.\nFor optimization, we employed the Adam optimizer with a learning rate of 0.1, optimizing our\nmethod for 200 epochs with a batch size of 1. Additionally, we used weighted cross-entropy loss,\nwith each class\u2019s weight determined as the ratio of the number of whole pixels in the image to the\nnumber of pixels belonging to that class within the image. Furthermore, the values for \u03b1 and \u03b2 were\nset to 1 and 0.005, respectively.\nWe set H\u2032\u2032 and W \u2032\u2032 to be 64. Regarding the CelebAMask-HQ, we divided the 512 \u00d7 512 images\ninto 4 patches of size 400. After acquiring their WAS-attention maps from SLiMe, we aggregated\nthem to generate the final WAS-attention map and subsequently derive the segmentation mask.\n16\nPublished as a conference paper at ICLR 2024\nTable 5:\nAblating the text prompt.\nMinimal effect of the initial text prompt for\nSLiMe.\n\u201cpart names\u201d:\nusing all part names separated with space for text prompt.\n(\u201cbackground body\nlight plate wheel window\u201d); \u201c \u201d:\nleaving the text prompt empty;\n\u201cpart\u201d: using \u201cpart\u201d instead of part names (\u201cpart part part part part part\u201d); SLiMe is with the\nsecond settings.\nText Prompt\nBody\nLight\nPlate\nWheel\nWindow\nBG\nAverage\n\u201cpart names\u201d\n82.0 \u00b1 0.3\n55.3 \u00b1 2.0\n56.1 \u00b1 1.1\n69.4 \u00b1 0.6\n69.6 \u00b1 0.9\n79.5 \u00b1 1.1\n68.7 \u00b1 0.4\n\u201c \u201d\n81.6 \u00b1 1.0\n56.7 \u00b1 0.4\n54.4 \u00b1 3.5\n69.6 \u00b1 1.3\n68.1 \u00b1 0.6\n80.2 \u00b1 0.9\n68.4 \u00b1 1.2\n\u201cpart\u201d\n81.5 \u00b1 1.0\n56.8 \u00b1 1.2\n54.8 \u00b1 2.7\n68.3 \u00b1 0.1\n70.3 \u00b1 0.9\n78.4 \u00b1 1.6\n68.3 \u00b1 1.0\nTable 6: Ablating the loss terms. Comparing the first four rows shows the importance of LSD.\nFurthermore, when comparing the last two rows, it underscores the effectiveness of LMSE.\n\u03b1\n\u03b2\nBody\nLight\nPlate\nWheel\nWindow\nBG\nAverage\n1\n0.5\n0.0 \u00b1 0.0\n0.0 \u00b1 0.0\n0.0 \u00b1 0.0\n0.0 \u00b1 0.0\n0.0 \u00b1 0.0\n31.8 \u00b1 0.0\n5.3 \u00b1 0.0\n0.05\n80.1 \u00b1 0.4\n57.6 \u00b1 0.1\n46.3 \u00b1 0.2\n67.4 \u00b1 1.0\n63.0 \u00b1 4.1\n78.0 \u00b1 0.2\n65.4 \u00b1 0.9\n0.005\n81.5 \u00b1 1.0\n56.8 \u00b1 1.2\n54.8 \u00b1 2.7\n68.3 \u00b1 0.1\n70.3 \u00b1 0.9\n78.4 \u00b1 1.6\n68.3 \u00b1 1.0\n0.0\n73.7 \u00b1 2.3\n39.7 \u00b1 1.2\n38.0 \u00b1 3.1\n55.0 \u00b1 3.4\n61.3 \u00b1 4.0\n67.2 \u00b1 2.2\n55.8 \u00b1 1.6\n0\n0.005\n80.6 \u00b1 0.5\n56.1 \u00b1 1.1\n57.4 \u00b1 0.4\n69.1 \u00b1 0.4\n66.7 \u00b1 1.6\n78.0 \u00b1 1.4\n68.0 \u00b1 0.3\nWhen constructing the WAS-attention map from the cross-attention and self-attention maps, we\nonly considered the corresponding cross-attention map of a token if the maximum value in that map\nexceeded 0.2. Otherwise, we disregarded that token and assigned zeros to its corresponding channel\nin the WAS-attention map.\nFor optimizing on PASCAL-Part classes, we applied the following augmentations: Random Hor-\nizontal Flip, Gaussian Blur, Random Crop, and Random Rotation. For the car class, we set the\nrandom crop ratio range to [0.5, 1], while for the horse class, it was adjusted to [0.8, 1]. Addition-\nally, we applied random rotation within the range of [\u221230, 30] degrees.\nMoreover, when optimizing on CelebAMask-HQ, we incorporated a set of augmentations, which\nencompassed Random Horizontal Flip, Gaussian Blur, Random Crop, and Random Rotation. The\nrandom crop ratio was modified to fall within the range of [0.6, 1], and random rotation was applied\nwithin the range of [\u221210, 10] degrees.\nB.1\nDETAILS OF THE DATASETS\nIn this section, we initially present further elaboration on the datasets on which we optimized our\nmethod. This includes information about the categories as well as the segmentation labels. Subse-\nquently, we offer details about the dataset preparation process.\nB.1.1\nPASCAL-PART\n\u2022 Car: Background, Body, Light, Plate, Wheel, and Window.\n\u2022 Horse: Background, Head, Leg, Neck+Torso, and Tail.\nB.1.2\nCELEBAMASK-HQ\n\u2022 Face: Background, Cloth, Ear, Eye, Eyebrow, Face, Hair, Mouth, Neck, and Nose.\nB.1.3\nDATASET PREPARATION\n\u2022 PASCAL-Part. For this dataset, we follow the procedures of ReGAN (Tritrong et al.,\n2021): We start by cropping the images with the bounding boxes provided in the dataset.\nAfterward, we remove those images where their bounding boxes have an overlap of more\n17\nPublished as a conference paper at ICLR 2024\nTable 7: Ablating ttrian. Our results across different ranges for choosing topt indicate that optimal\nperformance is achieved when topt is selected from the range [5, 100].\nRange of topt\nBody\nLight\nPlate\nWheel\nWindow\nBG\nAverage\n[5, 20]\n78.8 \u00b1 1.6\n52.6 \u00b1 2.8\n53.6 \u00b1 1.0\n66.6 \u00b1 0.2\n70.3 \u00b1 0.1\n78.7 \u00b1 1.2\n66.8 \u00b1 0.2\n[5, 100]\n81.5 \u00b1 1.0\n56.8 \u00b1 1.2\n54.8 \u00b1 2.7\n68.3 \u00b1 0.1\n70.3 \u00b1 0.9\n78.4 \u00b1 1.6\n68.3 \u00b1 1.0\n[5, 900]\n80.4 \u00b1 1.1\n54.4 \u00b1 1.6\n54.3 \u00b1 2.4\n67.3 \u00b1 1.3\n70.2 \u00b1 0.4\n79.9 \u00b1 1.7\n67.8 \u00b1 1.0\nTable 8: Ablating ttest. Evident from the table, we get the best results when ttest = 100\nttest\nBody\nLight\nPlate\nWheel\nWindow\nBG\nAverage\n5\n80.3 \u00b1 0.4\n50.8 \u00b1 1.8\n49.7 \u00b1 4.2\n66.5 \u00b1 0.8\n65.5 \u00b1 0.7\n78.4 \u00b1 1.2\n65.2 \u00b1 1.4\n20\n80.1 \u00b1 0.8\n53.1 \u00b1 2.7\n53.6 \u00b1 1.9\n65.6 \u00b1 2.7\n67.3 \u00b1 2.1\n76.0 \u00b1 2.5\n65.9 \u00b1 0.7\n100\n81.5 \u00b1 1.0\n56.8 \u00b1 1.2\n54.8 \u00b1 2.7\n68.3 \u00b1 0.1\n70.3 \u00b1 0.9\n78.4 \u00b1 1.6\n68.3 \u00b1 1.0\nthan 5% with other bounding boxes. Finally, we remove the cropped images smaller than\n50 \u00d7 50 for the car class and 32 \u00d7 32 for the horse class.\n\u2022 CelebAMask-HQ. The size of images that we use for this dataset is 512 \u00d7 512.\n18\nPublished as a conference paper at ICLR 2024\nTable 9: Ablating lr. The results of optimizing SLiMe with various learning rates reveal a crucial\nrelationship. When the learning rate is set too low, SLiMe struggles to learn effectively, resulting\nin minimal progress. Conversely, when the learning rate is excessively high, the text embeddings\ndeviate significantly from the comprehensible embeddings of SD.\nlr\nBody\nLight\nPlate\nWheel\nWindow\nBG\nAverage\n1\n10.1 \u00b1 9.2\n3.6 \u00b1 6.2\n12.4 \u00b1 10.8\n11.2 \u00b1 11.2\n13.2 \u00b1 14.5\n39.7 \u00b1 11.4\n15 \u00b1 7.4\n0.1\n81.5 \u00b1 1.0\n56.8 \u00b1 1.2\n54.8 \u00b1 2.7\n68.3 \u00b1 0.1\n70.3 \u00b1 0.9\n78.4 \u00b1 1.6\n68.3 \u00b1 1.0\n0.01\n81.5 \u00b1 0.0\n54.9 \u00b1 0.4\n52.8 \u00b1 1.7\n68.5 \u00b1 0.5\n69.8 \u00b1 0.2\n79.2 \u00b1 0.6\n67.8 \u00b1 0.3\n0.001\n70.0 \u00b1 0.9\n10.5 \u00b1 5.1\n40.6 \u00b1 0.5\n0.7 \u00b1 0.5\n18.2 \u00b1 6.1\n62.4 \u00b1 1.6\n33.7 \u00b1 2.3\nTable 10: Ablating layers to use their cross-attention module. The middle layers of SD\u2019s UNet\nexhibit a superior semantic understanding compared to the other set of layers.\nset of cross attention layers\nBody\nLight\nPlate\nWheel\nWindow\nBG\nAverage\n1st to 7th layers\n12.7 \u00b1 9.5\n13.8 \u00b1 12.1\n10.7 \u00b1 3.7\n29.5 \u00b1 14.2\n32.1 \u00b1 2.9\n34.2 \u00b1 4.1\n22.2 \u00b1 3.4\n8th to 12th layers\n81.5 \u00b1 1.0\n56.8 \u00b1 1.2\n54.8 \u00b1 2.7\n68.3 \u00b1 0.1\n70.3 \u00b1 0.9\n78.4 \u00b1 1.6\n68.3 \u00b1 1.0\n13th to 16th layers\n76.8 \u00b1 0.8\n51.9 \u00b1 9.2\n56.2 \u00b1 3.0\n62.6 \u00b1 4.2\n65.3 \u00b1 2.7\n68.6 \u00b1 2.6\n63.6 \u00b1 1.4\n19\n"
  },
  {
    "title": "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning",
    "link": "https://arxiv.org/pdf/2309.02591.pdf",
    "upvote": "12",
    "text": "Scaling Autoregressive Multi-Modal Models:\nPretraining and Instruction Tuning\nLili Yu\u2217 Bowen Shi\u2217 Ramakanth Pasunuru\u2217 Benjamin Muller\nOlga Golovneva\nTianlu Wang\nArun Babu\nBinh Tang\nBrian Karrer\nShelly Sheynin\nCandace Ross\nAdam Polyak\nRussell Howes\nVasu Sharma\nPuxin Xu\nHovhannes Tamoyan1\nOron Ashual\nUriel Singer\nShang-Wen Li\nSusan Zhang\nRichard James\nGargi Ghosh\nYaniv Taigman\nMaryam Fazel-Zarandi\nAsli Celikyilmaz\nLuke Zettlemoyer\nArmen Aghajanyan\u2217\nFAIR, YerevaNN1\narmenag@meta.com\nFigure 1: Showcase of CM3Leon zero-shot generations (no-retrieval augmentation). Refer to \u00a7 A for a complete\nlist of prompts. CM3Leon can generate complex compositional objects, tail entities (Khachkar\u2013Armenian\ncrosses carved from stone), and historically hard entities such as hands and text.\n\u2217First Author\narXiv:2309.02591v1  [cs.LG]  5 Sep 2023\nAbstract\nWe present CM3Leon (pronounced \u201cChameleon\u201d), a retrieval-augmented, token-\nbased, decoder-only multi-modal language model capable of generating and infill-\ning both text and images. CM3Leon uses the CM3 multi-modal architecture but\nadditionally shows the extreme benefits of scaling up and tuning on more diverse\ninstruction-style data. It is the first multi-modal model trained with a recipe adapted\nfrom text-only language models, including a large-scale retrieval-augmented pre-\ntraining stage and a second multi-task supervised fine-tuning (SFT) stage. It is\nalso a general-purpose model that can do both text-to-image and image-to-text\ngeneration, allowing us to introduce self-contained contrastive decoding methods\nthat produce high-quality outputs. Extensive experiments demonstrate that this\nrecipe is highly effective for multi-modal models. CM3Leon achieves state-of-the-\nart performance in text-to-image generation with 5x less training compute than\ncomparable methods (zero-shot MS-COCO FID of 4.88). After SFT, CM3Leon\ncan also demonstrate unprecedented levels of controllability in tasks ranging from\nlanguage-guided image editing to image-controlled generation and segmentation.\n1\nIntroduction\nDiffusion models have recently dominated image generation work due to their strong performance\nand relatively modest computational cost (Saharia et al., 2022; Chen et al., 2022; Rombach et al.,\n2022). In contrast, token-based autoregressive models (Ramesh et al., 2021; Yu et al., 2022) are\nknown to also produce strong results, with even better global image coherence in particular, but are\nmuch more expensive to train and use for inference. In this paper, we show that it is possible to\nextend training and inference ideas originally developed for text-only models to flip this narrative;\nautoregressive models can be efficient and performant while also generalizing beyond the strict\ntext-to-image format to be tuneable for a wide range of image and text generation tasks.\nMore specifically, we introduce CM3Leon (pronounced \u201cChameleon\u201d), a retrieval-augmented, token-\nbased, decoder-only multi-modal language model capable of generating and infilling both text and\nimages. CM3Leon uses the CM3 multi-modal architecture (Aghajanyan et al., 2022), but additionally\nshows the extreme benefits of scaling up and training on more diverse data. It is the first multi-\nmodal model trained with a recipe adapted from text-only language models, including a large-scale\nretrieval-augmented pretraining stage and a second multi-task supervised fine-tuning (SFT) stage.\nThe pretraining is efficient because it follows the retrieval-augmented CM3 approach (Yasunaga et al.,\n2022) but uses a new large-scale Shutterstock dataset that includes only licensed image and text data.\nThe SFT stage follows multi-task instruction tuning for text-only models Iyer et al. (2022), which\nallow arbitrary mixtures of image and text tokens in both the inputs and outputs. The generality\nof CM3Leon also supports the introduction of an improved, self-contained contrastive decoding\nmethod Li et al. (2022), which can provide self-guidance to improve both text and image generation.\nCM3Leon achieves state-of-the-art performance in text-to-image generation with 5x less training\ncompute than comparable methods (zero-shot MS-COCO FID of 4.88). It can also do non-trivial\nimage-to-text generation, even though it was trained on only 3B Shutterstock text tokens. After\nSFT, CM3Leon demonstrates unprecedented levels of controllability in tasks ranging from language-\nguided image editing to image-controlled generation and segmentation. We also show that retrieval\naugmentation is key for efficient training, and our new contrastive decoding method enables much\nhigher quality generation overall. These results strongly suggest that autoregressive models are worth\nsignificantly more study for any text and image task.\n2\nPretraining\nWe explore the potential of token-based decoder-only models in the text-to-image domain by building\nupon the foundation laid by RA-CM3 Yasunaga et al. (2022). We simplify the original settings\nin RA-CM3 by streamlining the objective, modifying the dataset, and incorporating insights from\nmulti-modal scaling laws presented by Aghajanyan et al. (2023).\n2\n2.1\nData\nThe ethical implications of image data sourcing in the domain of text-to-image generation have\nbeen a topic of considerable debate. In this study, we use only licensed images from Shutterstock.\nAs a result, we can avoid concerns related to images ownership and attribution, without sacrificing\nperformance.\nImage Tokenization\nWe use the image tokenizer from Gafni et al. (2022a), which encodes a\n256 \u00d7 256 image into 1024 tokens from a vocabulary of 8192. For text, we train a custom tokenizer\nover the Zhang et al. (2022) data with a vocabulary size of 56320. Additionally, we introduce a\nnovel special token, denoted as <break>, which serves to indicate a transition between modalities. A\nvisualization of one caption-image pair after tokenization and formatting with our special tokens is\navailable in \u00a7 B.1(Figure 8).\nRetrieval Augmentation\nOur retrieval approach aims to retrieve relevant and diverse multi-modal\ndocuments from a memory bank, given an input sequence (Yasunaga et al., 2022). It includes both a\ndense retriever and a retrieval strategy.\nThe dense retriever takes a query q (e.g., the input sequence x) and a candidate document m from\nthe memory bank M and returns a relevance score r(q, m). We adopt the dense retrieval method\nfrom Karpukhin et al. (2020), which uses a bi-encoder architecture. The encoder is CLIP-based. We\nsplit the multi-modal document into a text part and an image part, encode them separately using\noff-the-shelf frozen CLIP text and image encoders, and then average the two as a vector representation\nof the document (Radford et al., 2021). We use the ViT-B-32 model and normalize the image/text\nembeddings. The final retrieval is done with Maximum Inner Product Search (MIPS) over the memory\nbank using the dense retriever to obtain a list of candidate documents sorted by relevance score\n(Tiwari et al., 2022).\nTo sample informative retrieved documents for the generator during training, we consider three key\nfactors: relevance, modality, and diversity. First, the retrieved documents should be relevant to the\ninput sequence, captured by the dense retriever score based on CLIP. Second, retrieving a multi-modal\ndocument consisting of images and text leads to better generator performance than retrieving either\nimage or text. Third, diversity is essential to avoid redundancy in the retrieved documents. Simply\ntaking the top K documents based on relevance score can result in duplicates or highly similar\ndocuments, hurting downstream pretraining. We skip a candidate document if it is too similar to the\nquery or if the documents have already been retrieved. In practice, we only use retrieved documents\nwith relevance score \u2264 0.9. Additionally, we use query dropout, which drops some tokens of the\nquery used in retrieval (20% of tokens) to encourage diversity and serve as regularization for training.\nThroughout our work, we retrieve two documents each, based on image and text, respectively. In\ntraining, we randomly select three retrieved samples for every caption-image pair in our dataset,\neffectively 4x the number of tokens available in the pretraining. A visualization of a single training\nexample can be found in \u00a7 B.1(Figure 9).\n2.2\nObjective Function\nThe CM3 objective accepts multi-modal inputs (e.g., xinput = \"Image of a chameleon: [image]\") and\ntransforms them into an infilling instance by masking specific spans and relocating them to the end\n(e.g., xinput = \"Image of <mask>: [image] <infill> a chameleon\"). It uses a standard next token\nprediction loss, \u2212 log p(xinput). This results in a versatile model capable of infilling and autoregressive\ngeneration tasks for both images and text. In the case of caption-to-image generation, CM3 creates\na continuation from the prompt \"Image of a chameleon:\". For image-to-caption generation, CM3\nutilizes the prompt \"Image of <mask>: [image] <infill>\".\nYasunaga et al. (2022) built upon the original CM3 by including retrieved multi-modal documents in\nthe context for each training example and up weighting the query image-caption pair loss, as illustrated\nin the last image-caption pair in Figure 9. This approach encourages the model to concentrate more\non using retrieved samples during the generation process. However, this method adversely affects\nthe zero-shot scenario, where the goal is to generate an image without retrieval, such as predicting\na continuation from <eos> text <break>. We remove this weighting in our setting and make\na minor modification to the CM3 objective by preventing masking across <break> tokens. This\n3\nadjustment is justified by the fact that allowing masking across <break> tokens may lead to the\nmodel generating image content from an arbitrary midpoint, which is not a desirable outcome.\n2.3\nModel\nThe CM3Leon models follow a decoder-only transformer architecture, similar to Zhang et al. (2022)\nand Brown et al. (2020). Compared to Zhang et al. (2022), we remove bias terms, dropout, and\nlearnable parameters for layer norms and use a sequence length of 4096 instead of 2048. For weight\ninitialization, we use a truncated normal distribution with a mean of 0 and a standard deviation of\n0.006, truncated to 3 standard deviations. Output layers are initialized as 0, and the learned absolute\npositional embedding is initialized near zero with a standard deviation of 0.0002. The models were\ntrained with Metaseq2, with experiment tracking done with Aim Arakelyan et al. (2020).\n2.4\nTraining\nOur models are trained across three distinct sizes, with the corresponding parameters and training\nsetup detailed in Table 3. The major hyperparameters, such as the learning rate and batch size,\nare adopted from prior work in multi-modal scaling laws, creating a stable and smooth training\nprogression as illustrated in Figure 3 (Aghajanyan et al., 2023). The 350 Million (350M), 760 Million\n(760M), and 7 Billion (7B) models are trained to 1.4 Trillion (T), 1.9T, and 2.4T tokens, respectively.\nThe losses for all three models decrease steadily throughout training, strongly suggesting they have\nnot saturated.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEquivalent A100 Hours\n1e6\n4\n8\n16\n32\nFID (log scale)\n350M\n760M\nCM3Leon-7B\nSD v1.5\nSD v2.1\nPARTI-20B\n3B\n750M\n350M\nDALLE\nModel Arch\nCM3Leon\nDiffusion\nEncoder/Decoder\nAuto-Regressive\nFigure 2:\nWe plot FID score in log scale of\nvarious models against the equivalent A100 GPU\nhours during training.\nCM3Leon scales better\nthan DALLE (Ramesh et al., 2021), stable diffu-\nsion (SD) (Rombach et al., 2022) and PARTI (Yu\net al., 2022) models.\n0\n50000\n100000\n150000\n200000\n250000\n300000\nUpdates\n140\n160\n180\n200\n220\n240\nValidation PPL\nCM3Leon-350m\nCM3Leon-760m\nCM3Leon-7b\nFigure 3: We plot validation perplexity (PPL) against\nwith number of training updates for CM3Leon models\nin 350m, 760m and 7b size. We resume the training\nof 760m and 7b models after a full epoch (the purple\ndashed line), and the small rise in the PPL is due to the\nsudden increase of the learning rate.\n3\nText-To-Image Results\n3.1\nImportance of Decoding Strategies\nThere has been significant work on developing decoding algorithms for autoregressive text-to-image\nmodels, such as DALL-E Ramesh et al. (2021), which can have a large effect on the quality of the\nfinal outputs. DALL-E employs temperature sampling and a re-ranking stage via CLIP over 512\nprompt candidates. Models like PARTI and Make-A-Scene user token-based classifier-free guidance,\nsignificantly reducing the number of candidates required for re-ranking to just 16 samples (Yu et al.,\n2022; Gafni et al., 2022a). Our experiments show that different approaches offer complementary\nbenefits, as decribed in this section. We compare the following options.\nTemperatured Sampling\nis a probabilistic technique used in autoregressive models, such as\nRamesh et al. (2021). The method involves modifying the softmax temperature during the sampling\nstage to control the randomness of predictions. We pair this with Classifier Free Guidance in all of\nour experiments.\n2https://github.com/facebookresearch/metaseq\n4\nTopP Sampling\nalso known as nucleus sampling, involves sampling from the smallest set of\ntop-ranked tokens with a cumulative probability exceeding a predefined threshold (Holtzman et al.,\n2020). We pair this with Classifier Free Guidance in all of our experiments.\nClassifier Free Guidance (CFG)\nClassifier-free guidance refers to directing an unconditional\nsample towards a conditional sample (Gafni et al., 2022a). We replace the text with the mask token\nfrom the CM3 objective to facilitate unconditional sampling. This is one of the core benefits of\ntraining with the CM3 objective, allowing us to do classifier-free guidance without the need for\nfinetuning. During the inference stage, two concurrent token streams are generated: a conditional\ntoken stream, which is contingent on the input text, and an unconditional token stream, which is\nconditioned on a mask token. Borrowing the notation from Gafni et al. (2022a):\nlogitscond = T(ty|tx), logitsuncond = T(ty|<mask>),\n(1)\nlogitscf = logitsuncond + \u03b1c \u00b7 (logitscond \u2212 logitsuncond)\n(2)\nwhere T denotes the transformer, ty is the output tokens and tx is the conditional input text, <mask>\nrepresents the absence of input text (and replacement with a mask token), and \u03b1c is a scaling factor.\nThe classifier-free guidance effectively blends the unconditional and conditional logits, influencing\nthe model\u2019s output towards a more desired conditional output.\nContrastive Decoding TopK (CD-K)\nA key insight is that the logit subtraction in Equation 2\nresembles the log probability subtraction in contrastive decoding methods in text (Li et al., 2022).\nThis leads us to propose a variant of the contrastive decoding (CD) algorithm, originally proposed by\nLi et al. (2022), as an alternative to CFG.\nRecall that CD defines a score per token:\nCD(tyi; ty<i) =\n(\nlog\npEXP(tyi|ty<i)\npAMA(tyi|ty<i),\nif tyi \u2208 V(ty<i),\n\u2212 inf,\notherwise.\nHere, V(ty<i) represents the set of potential subsequent tokens whose probabilities are at least \u03b1\ntimes the maximum probability value:\nV(ty<i) = {tyi \u2208 V : pEXP(tyi | ty<i) \u2265 \u03b1 max\nw\npEXP(w|ty<i)}\nTraditionally pEXP and pAMA in the CD decoding algorithm represent a strong and weak model where\nthe strong model was trained with more compute (or larger model size) compared to the weak model.\nInstead we select pEXP having text conditioning and pAMA has no text conditioning. Additionally\nwe saw that the V(ty<i) constraint was too strict, and would consistently become greedy decoding.\nTherefore we propose a slight modification of CD we call CD-K that alters V(ty<i) to:\nV(ty<i) = {tyi \u2208 V : pEXP(tyi | ty<i) \u2265 \u03b1 \u2217 kmax\nk,w\n\u0000pEXP(w|ty<i)\n\u0001\n}\n(3)\nwhere instead of taking the largest probability we take the k-th largest probability.\nAblation\nIn Figure 4 we show that CD-K is competitive with standard CFG based sampling while\nproviding a complementary set of generations to CFG allowing us to continue minimizing FID as we\nincrease number of generations (while both CD-K and CFG independently stagnate).\n3.2\nQuantitative Evaluations\nTable 1 and Figure 2 provide a comparative overview of CM3Leon and state-of-the-art text-to-image\nmodels, evaluated based on the zero-shot MS-COCO (30K) task using the Fr\u00e9chet Inception Distance\n(FID) metric (Seitzer, 2020). CM3Leon-7B model set\u2019s a new state-of-the-art FID score of 4.88,\nwhile only using a fraction of the training data and compute of other models such as PARTI.\nThis observation underlines the effectiveness of retrieval-augmented decoder-only models like\nCM3Leon. In particular, the CM3Leon-7B model, when operated with one or two retrieved examples\nduring inference, records superior FID scores. This result demonstrates the crucial role retrieval plays\nin expanding the world knowledge provided to the model and its capacity to generate high-quality\nimages. CM3Leon surpasses all other retrieval-augmented models, including KNN-diffusion and\nRE-IMAGEN.\n5\n1\n2\n3\n4\n5\n6\n7\nCFG Weight\n8\n10\n12\n14\n16\n18\n20\nFID\nModel Size\n350M\n760M\n7B\n1\n2\n4\n8\n16\n32\nBatch Size\n8\n10\n12\n14\n16\nFID\nDecoding Strategy\nTemperature Sampling\nTopP\nCD-K\n1\n2 TopP + 1\n2 CD-K\nFigure 4: (Left) Comparison of Classifier-Free Guidance (CFG) weight and FID on 8k held-out MS-COCO data\nacross our series of models. The optimal CFG remains consistent across all model sizes. (Right) Comparison\nof the number of generated samples per prompt before CLIP-based re-ranking and their respective FID. The\ndata shows that TopP and CD-K are similar across sample counts but exhibit complementary behavior when\ncombined.\nRetrieval in\nTraining\nResponsible\n# of Retrieved\nDocuments\nDataset Size\nModel Size\nZero-shot\nFID-30K\nRA-CM3\n\u2713\n\u2717\n2\n150M\n2.7B\n15.70\nStableDiffusion\n\u2717\n\u2717\n-\n400M\n800M\n12.60\nKNN-Diffusion\n\u2713\n\u2717\n10\n70M\n400M\n12.50\nMUSE\n\u2717\n\u2717\n-\n500M\n3B\n7.88\nPARTI\n\u2717\n\u2717\n-\n5B\n20B\n7.23\nRE-IMAGEN\n\u2713\n\u2717\n2\n450M\n3.6B\n5.25\nCM3Leon-7B\n\u2713\n\u2713\n0\n340M\n7B\n10.82\nCM3Leon-7B\n\u2713\n\u2713\n1\n340M\n7B\n5.78\nCM3Leon-350M\n\u2713\n\u2713\n2\n340M\n350M\n14.20\nCM3Leon-760M\n\u2713\n\u2713\n2\n340M\n760M\n6.61\nCM3Leon-7B\n\u2713\n\u2713\n2\n340M\n7B\n4.88\nTable 1: Summary of various text-to-image models on the zero-shot MS-COCO task as measured by FID. For all\nof our models, we generate 8 samples for each input query, and use a CLIP model to select the best generation.\n4\nSupervised Fine-Tuning\nSupervised fine-tuning (SFT) is critical in training large language models (LLMs) like ChatGPT.\nDespite this, its application in multi-modal settings remains largely unexplored. SFT trains a model to\nbetter understand of future instructions or prompts, enhancing its performance in novel and even zero-\nshot tasks. We have found that instruction tuning notably amplifies multi-modal model performance\nacross various tasks such as image caption generation, visual question answering, text-based editing,\nand conditional image generation.\nWe fine-tune CM3Leon on a wide array of mixed image and text tasks. We organized each task as a\nseries of interleaved text and image examples, as shown in Figure 5. The fine-tuning process follows\nthe pretraining stage, employing the same CM3 objective by combining the task instruction with the\noutput. Further details about the hyperparameters and scale of the SFT can be found in Section E.1.\n4.1\nInstructable Image Generation\nText-Guided Image Editing\nallows the modification of an initial image based on text instructions,\nwith changes such as seasonal and weather adjustments, background changes, and material alterations.\nWe used InstructPix2Pix methodology and proprietary face-filtering techniques on their data, yielding\naround 600,000 examples (Brooks et al., 2023).\nImage-to-Image Grounded Generation\ninvolves producing grounding images with various fea-\ntures and text prompts. Features like edge maps, segmentation maps, key points, and human poses\n6\nCM3\nLeon\nEdit the image \nfollowing the text \ninstruction\nText-Guided  \nEditing\nImage-to-Image \nGrounded Generation\nGenerated images\nImage to text tasks\nGenerated images\nInterleaved texts and images\nText to image task\nCaption: Describe the given image\nVQA: Question: what time of the day is the photo taken?\nLong Caption: Describe the given image in very detail\nReasoning: Question: Does this passage describe the \nweather or the climate? Context: Figure: Des Moines. The \ntemperature recorded \u2026Please explain your answer. \nMake high quality \nimage from children's \nscribbles and text \ndescription\nMake high quality \nimage from pose \nfeatures and text \ndescription\nSpatially Grounded : Fabricate an image of a contemporary kitchen with a refrigerator at \nthe location (50, 50) -> (100, 100), and stoves at the location (80, 80) -> (200, 200)\nHow-to-write : A white sign that says \u201cmorning\u201d \nPix2pix, RDEdit\nScribble\nPoses\n. . .\nMake her an alien\nThe common kingfisher (Alcedo \natthis) also known as the Eurasian \nkingfisher and river kingfisher sitting \non branch\nA woman practices yoga on a \ncross-legged sport mat\nA beautiful view of a city from across a river. \nSunset time\nA view of tall buildings in a city. The photo is \ntaken from a park across a river. We can see \na bridge over the river.\nWeather. Because the atmosphere is the layer \nof air that surrounds Earth. Both weather and \nclimate tell you about the atmosphere. \u2026\nGenerated text\nFigure 5: We perform fine-tuning on the CM3Leon model using a vast assortment of combined image and\ntext tasks. Our retrieval augmented pretraining allows us to fine-tune the model effectively on a mixture of\ninterleaved texts and images, as well as text-to-image and image-to-text tasks. We present some common model\ninputs for various tasks on the left, with the corresponding model outputs displayed on the right. Throughout the\ntraining process, we concatenate the model input and output and train them using the same objective that was\nutilized during the pretraining stage.\ncan be derived from user-uploaded images or sketches. We used ControlNet processing code on\nShutterstock datasets to curate 7 million examples with features like canny edge, hed boundary, user\nsketching, human pose, and more (Zhang & Agrawala, 2023).\nSpatially Grounded Image Generation\nallows the user to integrate spatial information into\ntext prompts for image generation, with each object represented by discrete tokens. We used\nobject detection datasets like MS-COCO, Openimage, and Object365 to compile 3 million training\nexamples(Lin et al., 2014; Kuznetsova et al., 2020; Shao et al., 2019).\nHow-to-write\ntask enables users to request the model to create signs or logos based on text prompts.\nWe used an OCR detector to find suitable examples from Shutterstock datasets, resulting in 200,000\nexamples.\n  \u201cWhat would she look like \nas a bearded man?\u201d\n\u201cPut on a pair of \nsunglasses\u201d\n\u201cshe should look 100 years \nold\u201d\n  \u201cApply face paint\u201d\nInput\n  \u201cBusinessman in city street\u201d\n\u201cA boy running on the \ngrass of a soccer field\u201d\n\u201cYoung girl running on mountain \ntrail with wild flowers\u201d\n  \u201cBeautiful women walking \non the beach at sunset\u201d\nExtracted (openpose) pose \nText-Guided  Editing\nImage-to-Image \nGrounded Generation\nFigure 6: Qualitative examples of finetuned CM3Leon-7B model.\n7\nResults:\nWe showcase qualitative examples of images produced by a fine-tuned CM3Leon-7B\nmodel, as depicted in Figure 6. All instances in text-guided editing and image-image-grounded\ngeneration utilize a task prefix. For instance, we precede every text-guided editing example with the\nphrase, \"Edit the image following the text instruction,\" and every scribble generation example with\n\"Create a high-quality image from children\u2019s scribble and text description,\" amongst others. The top\nrow of Figure 6 presents text-guided image examples. We employ separate image CFG (1.5) and text\nCFG (7.5) values during decoding. This approach is crucial for producing edited images that mirror\nthe original image and closely align with the text editing instruction. The second row in Figure 6\nshow Structure-Guided Image Editing examples. For decoding, we utilized a single CFG value of\n3. Given identical input open pose features, our model can generate markedly distinct images that\nfollow different text prompts while maintaining the same pose as in the input image. More examples\nin 15\n4.2\nConditional Text Generation\nWe also include several vision-language tasks to teach CM3Leon to respond in text to various\nkinds of textual prompts conditioned on an image, such as visual question answering, long-form\ncaptioning, etc. We use the following 8 vision-language tasks: MS-COCO (Chen et al., 2015),\nFlickr30k (Young et al., 2014), Image Paragraph (Krause et al., 2017), Localized Narratives (Pont-\nTuset et al., 2020), VQA2 Goyal et al. (2017), VizWiz (Gurari et al., 2018), OKVQA (Marino et al.,\n2019), and ScienceQA (Lu et al., 2022). We use multiple prompt templates for each task to make the\nmodel robust to prompt variations (more details on the templates in Table 5 of the Appendix).\nResults:\nTable 2 presents the performance comparison of our SFT-CM3Leon model w.r.t. previous\nstate-of-the-art (SoTA) such as Flamingo (Alayrac et al., 2022) and OpenFlamingo3. We show that our\nSFT-CM3Leon model achieves strong zero-shot performance on several vision-language tasks even\nthough they saw significantly fewer text data (\u2248 3B tokens) compared to Flamingo (100B tokens)\nand OpenFlamingo (40B tokens). Notably, SFT-CM3Leon even beats Flamingo on the VizWiz\ntask. Figure 16 presents our SFT-CM3Leon-7B model generations, given an image context and an\ninstruction. The model is quite flexible with the instruction and can generate captions or answer a\nvariety of questions. Further, the ability of to follow instructions is more evident in Figure 7 where\nthe model can generate very long captions or reason over an image based on the given instruction.\nModel\nMS-COCO\nCIDEr\n(test)\nVQA2\nAcc.\n(test-dev)\nVizWiz\nAcc.\n(test-dev)\nOKVQA\nAcc.\n(val)\nImage Paragraph\nCIDEr\n(test)\nVisDial\nNDCG\n(val)\nOpenFlamingo-9B\u2020 (0-shot)\n65.5\n43.5\n-\n-\n-\n-\nFlamingo-9B (0-shot)\n79.4\n51.8\n28.8\n44.7\n-\n48.4\nSFT-CM3Leon-7B (0-shot)\n61.6\n47.6\n37.6\n23.8\n10.5\n22.6\nTable 2: Comparison of our supervised fine-tuning (SFT) CM3Leon with state-of-the-art models in zero-shot\nand few-shot settings. \u2020 Reported numbers are all based on validation set.\n5\nRelated Work\nDiffusion Models\nSignificant progress in the domain of text-to-image generation has been achieved\nthrough the use of diffusion models (Rombach et al., 2022; Nichol et al., 2021; Ramesh et al., 2022).\nThe underlying mechanism involves sequentially adding noise to an image and then learning to\nreverse the noise based on provided text inputs or features (Luo, 2022). Diffusion models generally\nincorporate pretrained text or language representations such as the text encoder of the CLIP (Radford\net al., 2021) image-text model or text encoders like T5 (Raffel et al., 2020). The recursive application\nof multi-resolution diffusion model (by employing multiple steps of super-resolution) has further\nenhanced their capability to generate high-quality images from text prompts, leading to state-of-the-art\nzero-shot non-retrieval based MS-COCO FID scores\n3https://laion.ai/blog/open-flamingo/\n8\nInstruction: Question: Which property do these two objects have in \ncommon?\nContext: Select the best answer.\nOptions: (A) shiny (B) salty\nAnswer:  Let's think step-by-step:\nCM3Leon Output: Think about each object. Potato chips have a salty taste. \nThe pretzel is salty. So the answer is (B).\nInstruction: Describe all the objects in the given image in very detail.\nCM3Leon Output: A street sign is on a metal pole. The sign is blue with \nwhite writing. There is a red light on the pole. The sky is a bright blue. \nThere are cars parked on the street. There are trees on the side of the \nstreet. There are buildings on the side of the street. \nFigure 7: Qualitative examples showing our SFT-CM3Leon-7B model\u2019s generations for various long form\ngeneration tasks.\nRetrieval Augmented Diffusion Models\nConventional text-to-image models often struggle to\ncapture the vast array of concepts and entities in the image domain. Methods like enabling retrieval\nduring inference time can help address the complexity of these tail entities by delegating it to a\nretrieval step. Building on the work of Saharia et al. (2022), Chen et al. (2022) incorporates retrieval\nto enhance zero-shot MS-COCO FID scores, demonstrating further improvement in this area.\nAutoregressive Token Models\nSignificant advancements have been made in the field by utilizing\nLLMs over tokenized image representations (Esser et al., 2020; Ramesh et al., 2021). A widely-\nused approach in the field (Van Den Oord et al., 2017; Razavi et al., 2019; Esser et al., 2021)\ninvolves an initial stage of converting images into discrete latent variables through tokenization,\nwhich transforms a text-to-image generation problem into a sequence-to-sequence problem, thereby\nenabling subsequent application of LLM techniques (Ramesh et al., 2021; Gafni et al., 2022b).\nNon-Autoregressive Token Models\nAlthough autoregressive models have benefited from extensive\nresearch in NLP, autoregressive decoding can be quite computationally expensive. Non-autoregressive\nmodels, such as Ghazvininejad et al. (2019), have been proposed in NLP and extended to text-to-\nimage models, exemplified by Chang et al. (2023) which achieves state-of-the-art image generation\nperformance and higher efficiency than diffusion or autoregressive models by employing masked\nmodeling in discrete token space (non-autoregressively with iterative decoding).\nRetrieval Augmented Autoregressive Token Models\nToken-based models face challenges akin to\nthose encountered by non-retrieval augmented diffusion models. To address these issues, Yasunaga\net al. (2022) suggested prefixing decoder-only text-to-image models, such as Ramesh et al. (2021);\nAghajanyan et al. (2022), with statically retrieved instances during training, resulting in significant\nefficiency gains during the training process.\nOur paper primarily concentrated on scaling this strategy.\n6\nConclusion\nWe presented CM3Leon, a retrieval-augmented, token-based, decoder-only multi-modal language\nmodel that efficiently and flexibly generates and infills text and images. Our approach extends\nthe scope of autoregressive models, demonstrating their potential to compete with and exceed\ndiffusion models in terms of cost-effectiveness and performance. By integrating a retrieval-augmented\npretraining stage with a diverse, large-scale Shutterstock dataset and a second multi-task supervised\nfine-tuning stage, CM3Leon demonstrates the benefits of a comprehensive training approach. Further\nenhanced by an innovative, self-contained contrastive decoding method, our model offers improved\ntext and image generation quality. Our results support the value of autoregressive models for a broad\nrange of text and image tasks, encouraging further exploration for this approach.\n9\nReferences\nArmen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal,\nDmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked\nmultimodal model of the internet. arXiv preprint arXiv:2201.07520, 2022.\nArmen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang,\nStephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative\nmixed-modal language models. arXiv preprint arXiv:2301.03728, 2023.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u2013\n23736, 2022.\nGor Arakelyan, Gevorg Soghomonyan, and The Aim team. Aim, 6 2020. URL https://github.\ncom/aimhubio/aim.\nTim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image\nediting instructions, 2023.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. In NeurIPS, 2020.\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,\nKevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation\nvia masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.\nWenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. Re-imagen: Retrieval-augmented\ntext-to-image generator. arXiv preprint arXiv:2209.14491, 2022.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325, 2015.\nPatrick Esser, Robin Rombach, and Bj\u00f6rn Ommer. Taming transformers for high-resolution image\nsynthesis, 2020.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution im-\nage synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 12873\u201312883, 2021.\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-\nscene: Scene-based text-to-image generation with human priors. arXiv preprint arXiv:2203.13131,\n2022a.\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-\nscene: Scene-based text-to-image generation with human priors. arXiv preprint arXiv:2203.13131,\n2022b.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-Predict: Parallel\ndecoding of conditional masked language models. In EMNLP, 2019.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V\nin VQA matter: Elevating the role of image understanding in visual question answering. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 6904\u20136913,\n2017.\nDanna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and\nJeffrey P Bigham. VizWiz grand challenge: Answering visual questions from blind people. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 3608\u20133617,\n2018.\n10\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. In ICLR, 2020.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D\u00e1niel Simig, Ping Yu,\nKurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model\ninstruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017,\n2022.\nVladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv\npreprint arXiv:2004.04906, 2020.\nJonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. A hierarchical approach for\ngenerating descriptive image paragraphs. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 317\u2013325, 2017.\nAlina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab\nKamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari.\nThe open images dataset v4. International Journal of Computer Vision, 128(7):1956\u20131981, mar\n2020. doi: 10.1007/s11263-020-01316-z.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke\nZettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization.\narXiv preprint arXiv:2210.15097, 2022.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nconference on computer vision, pp. 740\u2013755. Springer, 2014.\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,\nPeter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for\nscience question answering. Advances in Neural Information Processing Systems, 35:2507\u20132521,\n2022.\nCalvin Luo. Understanding diffusion models: A unified perspective. arXiv preprint arXiv:2208.11970,\n2022.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: A visual\nquestion answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf\nconference on computer vision and pattern recognition, pp. 3195\u20133204, 2019.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\nJordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting\nvision and language with localized narratives. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16, pp. 647\u2013664. Springer,\n2020.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. In JMLR, 2020.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n11\nAli Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with\nvq-vae-2. Advances in neural information processing systems, 32, 2019.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022.\nMaximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/\npytorch-fid, August 2020. Version 0.2.1.\nShuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian\nSun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV), October 2019.\nMo Tiwari, Ryan Kang, Je-Yong Lee, Luke Lee, Chris Piech, Sebastian Thrun, Ilan Shomorony, and\nMartin Jinye Zhang. Faster maximum inner product search in high dimensions. arXiv preprint\narXiv:2212.07551, 2022.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike\nLewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling.\narXiv preprint arXiv:2211.12561, 2022.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual\ndenotations: New similarity metrics for semantic inference over event descriptions. Transactions\nof the Association for Computational Linguistics, 2:67\u201378, 2014.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\nrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\nLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models,\n2023.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068, 2022.\n12\nA\nShowcase Prompts\n1. Chameleon and octopus, side by side, high quality render, drawing, professional.\n2. A plush toy koala bear relaxing on a lounge chair and working on a laptop. The chair is\nbeside a rose flower pot. There is a window on the wall beside the flower pot with a view of\nsnowy mountains.\n3. A photo of an astronaut riding a horse in the forest. There is a river in front of them with\nwater lilies.\n4. A teddy bear wearing a motorcycle helmet and cape is riding a motorcycle in Rio de Janeiro\nwith Dois Irm\u00e3os in the background. dslr photo.\n5. A black german shepherd wearing a red beret\n6. An Armenian church on the surface of Mars, with Astronaut walking into the church, in\nFocus. Photo. Fantasy. Dramatic.\n7. Armenian khachkars surrounded by pomegranates in a bright green forest.\n8. A cat wearing sunglasses\n9. A small cactus wearing a straw hat and neon sunglasses in the Sahara desert.\n10. A close up photo of a human hand, hand model. High quality\n11. A raccoon main character in an Anime preparing for an epic battle with a samurai sword.\nBattle stance. Fantasy, Illustration\n12. A stop sign in a Fantasy style with the text \"1991\"\nB\nPre-Training\nB.1\nData Visualizations\nof\ndslr\n<break>\nIMG5432\nIMG12\nIMG1991\nIMG1991\n...\nPhoto\n...\nFigure 8: Visualization of the tokenization of one caption-image pair.\nText\n<break>\nImage\n<break>\nText\n<break>\nImage\n<break>\nText\n<break>\nImage\n<break>\nText\n<break>\nImage\n<eos>\n<eos>\nFigure 9: Visualization of the tokenization of a full training sample consisting of retrieved sampled and query\ncaption-image pair.\nB.2\nModel Hyper-Parameters\nModel size\n# L\ndmodel\nSeq Length\nBatch\nLR\nWarmup Steps\n# GPUs\n# Tokens\n350M\n24\n1024\n4096\n8M\n6e-04\n1500\n256\n1.4T\n760M\n24\n1536\n4096\n8M\n5e-04\n1500\n256\n1.9T\n7B\n32\n4096\n4096\n8M\n1.2e-04\n1500\n512\n2.4T\nTable 3: Model architecture details. We report the number of layers (# L), embedding size (dmodel), sequence\nlength, batch size, peak learning rate (LR), learning rate warmup steps, number of GPUs used, and number of\ntokens consumed by each model.\n13\nModel\nResolution\nTime\nImagen\n256 \u00d7 256\n9.1s\nImagen\n1024 \u00d7 1024\n13.1s\nLDM (50 steps)\n512 \u00d7 512\n3.7s\nLDM (250 steps)\n512 \u00d7 512\n18.5s\nParti (3B)\n256 \u00d7 256\n6.4s\nMUSE (3B)\n256 \u00d7 256\n0.5s\nCM3Leon (7B, BF16)\n256 \u00d7 256\n11.8s\nCM3Leon (7B, INT8)\n256 \u00d7 256\n9.1s\nFigure 10: Inference latency for several models.\n1\n4\n16\n64\n256\nBatch Size\n0\n1\n10\n100\nThroughput (Seconds / Batch Size)\nMP1-fp32\nFT-MP1-bf16\nFT-MP1-int8\nFT-MP2-bf16\nFT-MP4-bf16\nFT-MP8-bf16\nFigure 11: Inference throughput of CM3Leon-7B\nfor generating images, without retrieval, across dif-\nferent model parallelism (MP), FasterTransformer\n(FT) implementation, data type (DType) and batch\nsizes\nC\nInference Latency and Throughput\nD\nImage Generation\nFigure 12: Top to bottom prompts: A steaming cup of coffee with mountains in the background.\nResting during road trip., beautiful, majestic road during sunset.\nAesthetic., small\ncircular island in the middle of a lake. Forests surrounding the lake. High Contrast.\n14\nFigure 13: turtle swimming underwater.\naesthetic.\nFantasy., elephant swimming underwater.\naesthetic. Fantasy.\n, flock of sheep.\naesthetic.\nFantasy.\nFigure 14: open hand, hand model.\n4k.\nwhite background, fist, hand model.\n4k.\nwhite\nbackground\n15\nE\nSupervised Fine Tuning\nE.1\nHyper-Parameters\nTo maintain a balanced dataset during training, we implemented an up/down sampling strategy\nwith a threshold of 3/0.3. This process was executed on the 760M and 7B models using 64 and\n128 80GB A100s, respectively. We assembled our training examples into sequences of length\n4096. Preliminary experiments were conducted to identify optimal learning rates from a range\nof 1e\u22125, 3e\u22125, 5e\u22125, 1e\u22124 and per-GPU batch sizes from 4, 8, 16 using our validation split. The\nselected hyperparameters are cataloged in Table 4. Throughout the fine-tuning phase, our models\nprocessed approximately 30 billion tokens.\nModel\n# GPUS\nSeq Length\nBatch Size\nLR\nWarm-up Steps\n# Tokens\nCM3Leon-760m\n64\n4096\n2M\n5e-05\n150\n30B\nCM3Leon-7b\n128\n4096\n2M\n5e-05\n150\n30B\nTable 4: Fine-tuning parameters for CM3Leon models\nE.2\nTraining Data Breakdown\nE.3\nMore Qualitative Samples\n16\nInput\nCanny Edge\nDepth (midas)\nLine (M-LSD)\nScribbles (synthesized)\nImage-to-Image Grounded Generation\nGenerate high quality \nimage of \"a room that \nhas a sink and a mirror \nin it\" with bottle at \nlocation (199, 130) -> \n(204, 150) and with sink \nat location (149, 133) -> \n(190, 154)  and with bed \nat location (0, 169) -> \n(67, 255).\nSpatially grounded\nInput\n(uniformer)  segmentation \nGeneration 1\nGeneration 2\nInput\n(uniformer)  segmentation \nGeneration 1\nGeneration 2\nFigure 15: Qualitative examples of finetuned CM3Leon-7b model. Human faces are blurred to remove PII\ninformation.\n17\nDataset\nTemplate\n# Examples\nImage Focused Datasets\nInstructPix2Pix\nEdit first image following the instruction <break> {image1}\n<break> edit instruction <break> {image 2}\n127k\nOCR\ndraw \u201c{ocr_content}\u201d <break> {image}\n300k\nObject Detection\nGenerate high quality image of {caption} with segmentations\n{obj1} at {loc1}, {obj2} at {loc2} ... <break> {image}\n3M\nEdge-to-Image\nMake high quality image from canny edge features <break>\n{edge image} <break> {caption} <break> {image}\n1M\nSeg-to-Image\nMake high quality image from a segmentation map <break> {seg\nimage} <break> {caption} <break> {image}\n1M\nHed-to-Image\nMake high quality image from hed features <break> {seg image}\n<break> {caption} <break> {image}\n1M\nPose-to-Image\nMake high quality image from openpose features <break> {seg\nimage} <break> {caption} <break> {image}\n142k\nDepth-to-Image\nMake high quality image from depth features <break> {depth\nimage} <break> {caption} <break> {image}\n1M\nNorm-to-Image\nMake high quality image from 3D norm features <break> {depth\nimage} <break> {caption} <break> {image}\n1M\nScribbe-to-Image\nMake high quality image from children\u2019s scribbles <break>\n{scribble image} <break> {caption} <break> {image}\n500k\nText Focused Datasets\nCOCO Captioning\n(Chen et al., 2015)\n{caption} <break> {image}\nDescribe the given picture. {caption} <break> {image}\n591k\nFlickr30k\n(Young et al., 2014)\n{caption} <break> {image}\nDescribe the given picture. {caption} <break> {image}\n144k\nImage Paragraph\n(Krause et al., 2017)\nDescribe the given picture in very detail. {caption} <break>\n{image}\nDescribe all the objects in the given image in very detail.\n{caption} <break> {image}\nGenerate a long caption for the given image.\n{caption} <break> {image}\n14k\nLocalized Narratives\n(Pont-Tuset et al., 2020)\nDescribe the given picture in very detail. {caption} <break>\n{image}\nGenerate a long narration of what is happening in the\ngiven image. {caption} <break> {image}\nGenerate a long caption for the given image.\n{caption} <break> {image}\n164k\nVQA2\n(Goyal et al., 2017)\nQuestion: {question} Answer: {answer}. <break> image\nQuestion: {question} [newline] {answer} <break> image\nQuestion: {question} The answer is {answer}. <break> image\n1.3M\nVizWiz\n(Gurari et al., 2018)\nQuestion: {question} Answer: {answer}. <break> image\nQuestion: {question} [newline] {answer} <break> image\nQuestion: {question} The answer is {answer}. <break> image\n92k\nOKVQA\n(Marino et al., 2019)\nQuestion: {question} Answer: {answer}. <break> image\nQuestion: {question} [newline] {answer} <break> image\nQuestion: {question} The answer is {answer}. <break> image\n26k\nScienceQA\n(Lu et al., 2022)\nQuestion: {question} [newline] Context: {context} [newline]\nOptions: {choices_text} [newline] Answer: {answer}.\n<break> {image}\nQuestion: {question} [newline] Context: {context} [newline]\nOptions: {choices_text} [newline] Answer: Let\u2019s think\nstep-by-step: { explanation} So the answer is {answer}.\n<break> {image}\n6k\nTable 5: Details of the datasets and their prompt templates used in our supervised fine-tuning of CM3Leon\nmodels.\n18\nInstruction: Describe the given \nimage.\nCM3Leon Output: A man is standing \non a beach with a surfboard.\nInstruction: Describe the given \nimage.\nCM3Leon Output: A herd of sheep \nstanding on top of a snow covered \nfield.\nInstruction: Describe the given \nimage.\nCM3Leon Output: A man in a \nbaseball uniform is throwing a \nbaseball.\nInstruction: Question: What are \npeople flying?\nCM3Leon Output: Kites\nInstruction: Question: What sign is \non the street?\nCM3Leon Output: stop\nInstruction: Question: What color is \nthe bench?\nCM3Leon Output: blue\nFigure 16: Qualitative examples showing our SFT-CM3Leon-7B model\u2019s generations for image captioning and\nvisual question answering tasks. Human faces are blurred to remove PII information.\n19\n"
  },
  {
    "title": "Matcha-TTS: A fast TTS architecture with conditional flow matching",
    "link": "https://arxiv.org/pdf/2309.03199.pdf",
    "upvote": "10",
    "text": "MATCHA-TTS: A FAST TTS ARCHITECTURE WITH CONDITIONAL FLOW MATCHING\nShivam Mehta, Ruibo Tu, Jonas Beskow, \u00c9va Sz\u00e9kely, Gustav Eje Henter\nDivision of Speech, Music and Hearing, KTH Royal Institute of Technology, Stockholm, Sweden\nABSTRACT\nWe introduce Matcha-TTS, a new encoder-decoder architecture for\nspeedy TTS acoustic modelling, trained using optimal-transport\nconditional flow matching (OT-CFM). This yields an ODE-based\ndecoder capable of high output quality in fewer synthesis steps than\nmodels trained using score matching. Careful design choices ad-\nditionally ensure each synthesis step is fast to run.\nThe method\nis probabilistic, non-autoregressive, and learns to speak from\nscratch without external alignments. Compared to strong pre-trained\nbaseline models, the Matcha-TTS system has the smallest memory\nfootprint, rivals the speed of the fastest model on long utterances,\nand attains the highest mean opinion score in a listening test.\nIndex Terms\u2014 Diffusion models, flow matching, speech syn-\nthesis, text-to-speech, acoustic modelling\n1. INTRODUCTION\nDiffusion probabilistic models (DPMs) (cf. [1]) are currently setting\nnew standards in deep generative modelling on continuous-valued\ndata-generation tasks such as image synthesis [2, 3], motion syn-\nthesis [4, 5], and speech synthesis [6, 7, 8, 9, 10] \u2013 the topic of this\npaper. DPMs define a diffusion process which transforms the data\n(a.k.a. target) distribution to a prior (a.k.a. source) distribution, e.g.,\na Gaussian. They then learn a sampling process that reverses the dif-\nfusion process. The two processes can be formulated as forward- and\nreverse-time stochastic differential equations (SDEs) [11]. Solving\na reverse-time SDE initial value problem generates samples from\nthe learnt data distribution. Furthermore, each reverse-time SDE\nhas a corresponding ordinary differential equation (ODE), called\nthe probability flow ODE [11, 12], which describes (and samples\nfrom) the exact same distribution as the SDE. The probability flow\nODE is a deterministic process for turning source samples into\ndata samples, similar to continuous-time normalising flows (CNF)\n[13], but without the need to backpropagate through expensive ODE\nsolvers or approximate the reverse ODE using adjoint variables [13].\nThe SDE formulation of DPMs is trained by approximating the\nscore function (the gradients of the log probability density) of the\ndata distribution [11]. The training objective takes the form of a\nmean squared error (MSE) which can be derived from an evidence\nlower bound (ELBO) on the likelihood. This is fast and simple and,\nunlike typical normalising flow models, does not impose any restric-\ntions on model architecture. But whilst they allow efficient training\nwithout numerical SDE/ODE solvers, DPMs suffer from slow syn-\nthesis speed, since each sample requires numerous iterations (steps),\ncomputed in sequence, to accurately solve the SDE. Each such step\nThis work was partially supported by the Wallenberg AI, Autonomous\nSystems and Software Program (WASP) funded by the Knut and Alice Wal-\nlenberg Foundation and by the Industrial Strategic Technology Development\nProgram (grant no. 20023495) funded by MOTIE, Korea.\nrequires that an entire neural network be evaluated. This slow syn-\nthesis speed has long been the main practical issue with DPMs.\nThis paper introduces Matcha-TTS1, a probabilistic and non-\nautoregressive, fast-to-sample-from TTS acoustic model based on\ncontinuous normalising flows. There are two main innovations:\n1. To begin with, we propose an improved encoder-decoder TTS\narchitecture that uses a combination of 1D CNNs and Trans-\nformers in the decoder. This reduces memory consumption\nand is fast to evaluate, improving synthesis speed.\n2. Second, we train these models using optimal-transport condi-\ntional flow matching (OT-CFM) [14], which is a new method\nto learn ODEs that sample from a data distribution. Com-\npared to conventional CNFs and score-matching probability\nflow ODEs, OT-CFM defines simpler paths from source to\ntarget, enabling accurate synthesis in fewer steps than DPMs.\nExperimental results demonstrate that both innovations acceler-\nate synthesis, reducing the trade-off between speed and synthesis\nquality. Despite being fast and lightweight, Matcha-TTS learns to\nspeak and align without requiring an external aligner. Compared\nto strong pre-trained baseline models, Matcha-TTS achieves fast\nsynthesis with better naturalness ratings. Audio examples and code\nare provided at https://shivammehta25.github.io/Matcha-TTS/.\n2. BACKGROUND\n2.1. Recent encoder-decoder TTS architectures\nDPMs have been applied to numerous speech-synthesis tasks with\nimpressive results, including waveform generation [6, 10] and end-\nto-end TTS [7]. Diff-TTS [9] was first to apply DPMs for acoustic\nmodelling. Shortly after, Grad-TTS [8] conceptualised the diffusion\nprocess as an SDE. Although these models, and descendants like\nFast Grad-TTS [15], are non-autoregressive, TorToiSe [16] demon-\nstrated DPMs in an autoregressive TTS model with quantised latents.\nThe above models \u2013 like many modern TTS acoustic models \u2013\nuse an encoder-decoder architecture with Transformer blocks in the\nencoder. Many models, e.g., FastSpeech 1 and 2 [17, 18], use si-\nnusoidal position embeddings for positional dependences. This has\nbeen found to generalise poorly to long sequences; cf. [19]. Glow-\nTTS [20], VITS [21], and Grad-TTS instead use relative positional\nembeddings [22]. Unfortunately, these treat inputs outside a short\ncontext window as a \u201cbag of words\u201d, often resulting in unnatural\nprosody. LinearSpeech [23] instead employed rotational position\nembeddings (RoPE) [24], which have computational and memory\nadvantages over relative embeddings and generalise to longer dis-\ntances [25, 19]. Matcha-TTS thus uses Transformers with RoPE in\nthe encoder, reducing RAM use compared to Grad-TTS. We believe\nours is the first SDE or ODE-based TTS method to use RoPE.\n1We call our approach Matcha-TTS because it uses flow matching for\nTTS, and because the name sounds similar to \u201cmatcha tea\u201d, which some\npeople prefer over Taco(tron)s.\n1\narXiv:2309.03199v2  [eess.AS]  9 Jan 2024\nModern TTS architectures also differ in terms of decoder net-\nwork design. The normalising-flow based methods Glow-TTS [20]\nand OverFlow [26] use dilated 1D-convolutions. DPM-based meth-\nods like [9, 27] likewise use 1D convolutions to synthesise mel\nspectrograms. Grad-TTS [8], in contrast, uses a U-Net with 2D-\nconvolutions. This treats mel spectrograms as images and implicitly\nassumes translation invariance in both time and frequency. How-\never, speech mel-spectra are not fully translation-invariant along the\nfrequency axis, and 2D decoders generally require more memory as\nthey introduce an extra dimension to the tensors. Meanwhile, non-\nprobabilistic models like FastSpeech 1 and 2 have demonstrated that\ndecoders with (1D) Transformers can learn long-range dependencies\nand fast, parallel synthesis. Matcha-TTS also uses Transformers in\nthe decoder, but in a 1D U-Net design inspired by the 2D U-Nets in\nthe Stable Diffusion image-generation model [3].\nWhilst some TTS systems, e.g., FastSpeech [17], rely on\nexternally-supplied alignments, most systems are capable of learn-\ning to speak and align at the same time, although it has been found to\nbe important to encourage or enforce monotonic alignments [28, 29]\nfor fast and effective training. One mechanism for this is monotonic\nalignment search (MAS), used by, e.g., Glow-TTS [20] and VITS\n[21]. Grad-TTS [8], in particular, uses a MAS-based mechanism\nwhich they term prior loss to quickly learn to align input symbols\nwith output frames. These alignments are also used to train a de-\nterministic duration predictor minimising MSE in the log domain.\nMatcha-TTS uses these same methods for alignment and duration\nmodelling. Finally, Matcha-TTS differs by using snake beta activa-\ntions from BigVGAN [30] in all decoder feedforward layers.\n2.2. Flow matching and TTS\nCurrently, some of the highest-quality TTS systems either utilise\nDPMs [8, 16] or discrete-time normalising flows [21, 26], with\ncontinuous-time flows being less explored. Lipman et al. [14] re-\ncently introduced a framework for synthesis using ODEs that unifies\nand extends probability flow ODEs and CNFs. They were then able\nto present an efficient approach to learn ODEs for synthesis, using a\nsimple vector-field regression loss called conditional flow matching\n(CFM), as an alternative to learning score functions for DPMs or\nusing numerical ODE solvers at training time like classic CNFs [13].\nCrucially, by leveraging ideas from optimal transport, CFM can be\nset up to yield ODEs that have simple vector fields that change little\nduring the process of mapping samples from the source distribution\nonto the data distribution, since it essentially just transports prob-\nability mass along straight lines. This technique is called OT-CFM;\nrectified flows [31] represent concurrent work with a similar idea.\nThe simple paths mean that the ODE can be solved accurately using\nfew discretisation steps, i.e., accurate model samples can be drawn\nwith fewer neural-network evaluations than DPMs, enabling much\nfaster synthesis for the same quality.\nCFM is a new technique that differs from earlier approaches to\nspeed up SDE/ODE-based TTS, which most often were based on\ndistillation (e.g., [27, 15, 32]). Prior to Matcha-TTS, the only public\npreprint on CFM-based acoustic modelling was the Voicebox model\nfrom Meta [33]. Voicebox (VB) is a system that performs various\ntext-guided speech-infilling tasks based on large-scale training data,\nwith its English variant (VB-En) being trained on 60k hours of pro-\nprietary data. VB differs substantially from Matcha-TTS: VB per-\nforms TTS, denoising, and text-guided acoustic infilling trained us-\ning a combination of masking and CFM, whereas Matcha-TTS is a\npure TTS model trained solely using OT-CFM. VB uses convolu-\ntional positional encoding with AliBi [19] self-attention bias, whilst\nour text encoder uses RoPE. In contrast to VB, we train on standard\ndata and make code and checkpoints publicly available. VB-En con-\nsumes 330M parameters, which is 18 times larger than the Matcha-\nTTS model in our experiments. Also, VB uses external alignments\nfor training whereas Matcha-TTS learns to speak without them.\n3. METHOD\nWe now outline flow-matching training (in Sec. 3.1) and then (in\nSec. 3.2) give details on our proposed TTS architecture.\n3.1. Optimal-transport conditional flow matching\nWe here give a high-level overview of flow matching, first introdu-\ncing the probability-density path generated by a vector field and then\nleading into the OT-CFM objective used in our proposed method.\nNotation and definitions mainly follow [14].\nLet x denote an observation in the data space Rd, sampled from\na complicated, unknown data distribution q(x). A probability dens-\nity path is a time-dependent probability density function, pt : [0, 1]\u00d7\nRd \u2192 R > 0. One way to generate samples from the data distribu-\ntion q is to construct a probability density path pt, where t \u2208 [0, 1]\nand p0(x) = N(x; 0, I) is a prior distribution, such that p1(x)\napproximates the data distribution q(x). For example, CNFs first\ndefine a vector field vt : [0, 1] \u00d7 Rd \u2192 Rd, which generates the\nflow \u03d5t : [0, 1] \u00d7 Rd \u2192 Rd through the ODE\nd\ndt\u03d5t(x) = vt(\u03d5t(x));\n\u03d50(x) = x.\n(1)\nThis generates the path pt as the marginal probability distribution of\nthe data points. We can sample from the approximated data distribu-\ntion p1 by solving the initial value problem in Eq. (1).\nSuppose there exists a known vector field ut that generates a\nprobability path pt from p0 to p1 \u2248 q. The flow matching loss is\nLFM(\u03b8) = Et,pt(x)\u2225ut(x) \u2212 vt(x; \u03b8)\u22252,\n(2)\nwhere t \u223c U[0, 1] and vt(x; \u03b8) is a neural network with parameters\n\u03b8. Nevertheless, flow matching is intractable in practice because it is\nnon-trivial to get access to the vector field ut and the target probab-\nility pt. Therefore, conditional flow matching instead considers\nLCFM(\u03b8) = Et,q(x1),pt(x|x1)\u2225ut(x|x1) \u2212 vt(x; \u03b8)\u22252.\n(3)\nThis replaces the intractable marginal probability densities and the\nvector field with conditional probability densities and conditional\nvector fields.\nCrucially, these are in general tractable and have\nclosed-form solutions, and one can furthermore show that LCFM(\u03b8)\nand LFM(\u03b8) both have identical gradients with respect to \u03b8 [14].\nMatcha-TTS is trained using optimal-transport conditional flow\nmatching (OT-CFM) [14], which is a CFM variant with particularly\nsimple gradients. The OT-CFM loss function can be written\nL(\u03b8) = Et,q(x1),p0(x0)\u2225uOT\nt\n(\u03d5OT\nt\n(x)|x1) \u2212 vt(\u03d5OT\nt\n(x)|\u00b5; \u03b8)\u22252,\n(4)\ndefining \u03d5OT\nt\n(x) = (1 \u2212 (1 \u2212 \u03c3min)t)x0 + tx1 as the flow from\nx0 to x1 where each datum x1 is matched to a random sample x0 \u223c\nN(0, I) as in [14]. Its gradient vector field \u2013 whose expected value\nis the target for the learning \u2013 is then uOT\nt\n(\u03d5OT\nt\n(x0)|x1) = x1 \u2212\n(1 \u2212 \u03c3min)x0, which is linear, time-invariant, and only depends on\nx0 and x1. These properties enable easier and faster training, faster\ngeneration, and better performance compared to DPMs.\n2\nText\nt\n\u025b\nk\ns\nt\nPhonetise\nDuration\npredictor\nProjection\nCeil\nText encoder\nUpsample\nFlow-prediction network\nFig. 1: Overview of the proposed approach at synthesis time.\nIn the case of Matcha-TTS, x1 are acoustic frames and \u00b5 are the\nconditional mean values of those frames, predicted from text using\nthe architecture described in the next section. \u03c3min is a hyperpara-\nmeter with a small value (1e-4 in our experiments).\n3.2. Proposed architecture\nMatcha-TTS is a non-autoregressive encoder-decoder architecture\nfor neural TTS. An overview of the architecture is provided in Fig. 1.\nText encoder and duration predictor architectures follow [20, 8], but\nuse rotational position embeddings [24] instead of relative ones.\nAlignment and duration-model training follow use MAS and the\nprior loss Lenc as described in [8]. The predicted durations, roun-\nded up, are used to upsample (duplicate) the vectors output by the\nencoder to obtain \u00b5, the predicted average acoustic features (e.g.,\nmel-spectrogram) given the text and the chosen durations.\nThis\nmean is used to condition the decoder that predicts the vector field\nvt(\u03d5OT\nt\n(x0)|\u00b5; \u03b8) used for synthesis, but is not used as the mean\nfor the initial noise samples x0 (unlike Grad-TTS).\nFig. 2 shows the Matcha-TTS decoder architecture. Inspired\nby [3], it is a U-Net containing 1D convolutional residual blocks to\ndownsample and upsample the inputs, with the flow-matching step\nt \u2208 [0, 1] embedded as in [8]. Each residual block is followed by\na Transformer block, whose feedforward nets use snake beta activa-\ntions [30]. These Transformers do not use any position embeddings,\nsince between-phone positional information already has been baked\nin by the encoder, and the convolution and downsampling opera-\ntions act to interpolate these between frames within the same phone\nand distinguish their relative positions from each other. This decoder\nnetwork is significantly faster to evaluate and consumes less memory\nthan the 2D convolutional-only U-Net used by Grad-TTS [8].\n4. EXPERIMENTS\nTo validate the proposed approach we compared it to three pre-\ntrained baselines in several experiments, including a listening test.\nAll experiments were performed on NVIDIA RTX 3090 GPUs. See\nshivammehta25.github.io/Matcha-TTS/ for audio and code.\n4.1. Data and systems\nWe performed our experiments on the standard split of the LJ Speech\ndataset2 (a female US English native speaker reading public-domain\n2https://keithito.com/LJ-Speech-Dataset/\nTime-step embedding net\nDownsampling blocks\nMid blocks\nResnet1D\nTransformer block\nResnet1D\nTransformer block\nResnet1D\nTransformer block\nUpsampling blocks\nResnet1D\nTransformer block\nResnet1D\nTransformer block\nResnet1D\nTransformer block\nFig. 2: Matcha-TTS decoder (the flow-prediction network in Fig. 1).\ntexts), training a version of the Matcha-TTS architecture on this\ndata. We used the same encoder and duration predictor (i.e., the\nsame hyperparameters) as [8], just different position embeddings\nin the encoder. Our trained flow-prediction network (decoder) used\ntwo downsampling blocks, followed by two midblocks and two up-\nsampling blocks, as shown in Fig. 2. Each block had one Trans-\nformer layer with hidden dimensionality 256, 2 heads, attention di-\nmensionality 64, and \u2018snakebeta\u2019 activations [30]. Phonemizer3 [34]\nwith the espeak-ng backend was used to convert input graphemes\nto IPA phones. We trained for 500k updates on 2 GPUs with batch\nsize 32 and learning rate 1e-4, labelling our trained system MAT.\nMAT was compared to three widely used neural TTS baseline\napproaches with pre-trained checkpoints available for LJ Speech,\nnamely Grad-TTS4 [8] (label GRAD), a strong DPM-based acous-\ntic model, FastSpeech 2 (FS2), a fast non-probabilistic acoustic\nmodel, and VITS5, a strong probabilistic end-to-end TTS system\nwith discrete-time normalising flows. The baselines used the official\ncheckpoints from the respective linked repositories. For FS2, which\ndoes not provide an official implementation, we instead used the\ncheckpoint from Meta\u2019s FairSeq6. To decouple the effects of CFM\ntraining from those due to the new architecture, we also trained\nthe GRAD architecture using the OT-CFM objective instead, using\nthe same optimiser hyperparameters as for MAT. This produced the\nablation labelled GCFM. For all acoustic models (i.e., all systems\nexcept VITS), we used the pre-trained HiFi-GAN [35] LJ Speech\ncheckpoint LJ_V17 for waveform generation, with a denoising filter\nas introduced in [36] at a strength of 2.5e-4. As a top line, our\nexperiments also included vocoded held-out speech, labelled VOC.\nODE-based models, e.g., DPMs, allow trading off speed against\nquality. We therefore evaluated synthesis from the trained ODE-\nbased systems with a different number of steps for the ODE solver.\nLike [8], we used the first-order Euler forward ODE-solver, where\nthe number of steps is equal to the number of function (i.e., neural-\nnetwork) evaluations, commonly abbreviated NFE. This gave rise\nto multiple conditions for some systems. We labelled these condi-\ntions MAT-n, GRAD-n, and GCFM-n, n being the NFE. We used\nNFE 10 or less, since [8] reported that NFE 10 and 100 gave the\nsame MOS for Grad-TTS (NFE 50 is the official code default). All\nconditions used a temperature of 0.667 for synthesis, similar to [8].\nTable 1 provides an overview of the conditions in the evaluation.\n3https://github.com/bootphon/phonemizer\n4https://github.com/huawei-noah/\nSpeech-Backbones/tree/main/Grad-TTS\n5https://github.com/jaywalnut310/vits\n6https://github.com/facebookresearch/fairseq\n7https://github.com/jik876/hifi-gan/\n3\nCondition\nParams.\nRAM\nRTF (\u00b5\u00b1\u03c3)\nWER\nMOS\nVOC\n13.9M\n-\n0.001\u00b10.001\n1.97\n4.13\u00b10.09\nFS2\n41.2M\n6.0\n0.010\u00b10.004\n4.18\n3.29\u00b10.09\nVITS\n36.3M\n12.4\n0.074\u00b10.083\n2.52\n3.71\u00b10.08\nGRAD-10\n14.8M\n7.8\n0.049\u00b10.013\n3.44\n3.49\u00b10.08\nGRAD-4\n\"\n\"\n0.019\u00b10.006\n3.69\n3.20\u00b10.09\nGCFM-4\n\"\n\"\n0.019\u00b10.004\n2.70\n3.57\u00b10.08\nMAT-10\n18.2M\n4.8\n0.038\u00b10.019\n2.09\n3.84\u00b10.08\nMAT-4\n\"\n\"\n0.019\u00b10.008\n2.15\n3.77\u00b10.07\nMAT-2\n\"\n\"\n0.015\u00b10.006\n2.34\n3.65\u00b10.08\nTable 1: Conditions in the evaluation (with the NFE for ODE-based\nmethods) and their number of parameters, minimum GPU RAM\nneeded to train (GiB), real-time factor (including vocoding time) on\nthe test set, ASR WER in percent, and mean opinion score with 95%-\nconfidence interval. The best TTS condition in each column is bold.\nThe parameter count and RTF for VOC pertain to the vocoder.\n4.2. Evaluations, results, and discussion\nWe evaluated our approach both objectively and subjectively. First\nwe measured parameter count and maximum memory use during\ntraining (at batch size 32 and fp16) of all systems, with results listed\nin Table 1. We see that MAT is approximately the same size as\nGRAD/GCFM, and smaller than all other systems. In particular, it is\nsmaller than VITS also after adding the vocoder (13.9M parameters)\nto the MAT parameter count. More importantly, it uses less memory\nthan all baselines, which (more than parameter count) is the main\nlimiter on how large and powerful models that can be trained.\nAfter training the systems, we assessed the synthesis speed and\nintelligibility of the different conditions, by computing the real time\nfactor (RTF) mean and standard deviation when synthesising the test\nset, and evaluating the word error rate (WER) when applying the\nWhisper medium [37] ASR system to the results, since the WERs\nof strong ASR systems correlate well with intelligibility [38]. The\nresults, in Table 1, suggest that MAT is the most intelligible system,\neven using only two synthesis steps. MAT is also much faster than\nVITS, equally fast or faster than GRAD/GCFM at the same NFE,\nand only slightly slower than FS2 when at the fastest setting.\nTo evaluate the naturalness of the synthesised audio we ran a\nmean opinion score (MOS) listening test. We selected 40 utterances\n(4 groups of 10) of different lengths from the test set and synthes-\nised each utterance using all conditions, loudness-normalising every\nstimulus using EBU R128. 80 subjects (self-reported as native Eng-\nlish speakers using headphones) were crowdsourced through Pro-\nlific to listen to and rate these stimuli. For each stimulus, listeners\nwere asked \u201cHow natural does the synthesised speech sound?\u201d, and\nprovided responses on an integer rating scale from 1 (\u201cCompletely\nunnatural\u201d) to 5 (\u201cCompletely natural\u201d) adopted from the Blizzard\nChallenge [39]. Each group of 10 utterances was evaluated by 20\nlisteners, who were paid \u00a33 for a median completion time of 13\nmins. Inattentive listeners were filtered out and replaced in exactly\nthe same way as in [26]. In the end we obtained 800 ratings for\neach condition. The resulting MOS values, along with confidence\nintervals based on a normal approximation, are listed in Table 1.\nWe note that, since MOS values depend on many variables external\nto stimulus quality, e.g., listener demographics and instructions (see\n[40, 41]), they should not be treated as an absolute metric. Compar-\ning our MOS values to other papers is thus unlikely to be meaningful.\nApplying t-tests to all pairs of conditions, all differences were\n50\n100\n200\n500\n1000 1500\nText length (characters)\n0.1\n0.5\n3\nSynthesis time (s)\nFS2\nGRAD-10\nGRAD-4\nGCFM-4\nMAT-10\nMAT-4\nMAT-2\nFig. 3: Scatterplot of prompt length vs. synthesis time for acoustic\nmodels. Regression lines show as curves due to the log-log axes.\nfound to be statistically significant at the \u03b1 = 0.05 level except\nthe pairs (MAT-10,MAT-4), (MAT-4,VITS), (VITS,MAT-2), (MAT-\n2,GCFM-4), and (GCFM-4,GRAD-10). This means that MAT al-\nways had significantly better rated naturalness than GRAD for the\nsame NFE, and always surpassed FS2. Both the new architecture and\ntraining method contributed to the naturalness improvement, since\nMAT-4>GCFM-4>GRAD-4. The fact that GRAD-10 was much bet-\nter than GRAD-4 whilst MAT-10 and MAT-4 performed similarly\nsuggests that GRAD requires many steps for good synthesis quality,\nwhereas MAT reached a good level in fewer steps. Finally, VITS\nperformed similarly to MAT-2 and MAT-4 in terms of MOS. MAT-\n10, although close to MAT-4 in rating, was significantly better than\nVITS. For any given n, MAT-n always scored higher than any sys-\ntem with equal or faster RTF. In summary, Matcha-TTS achieved\nsimilar or better naturalness than all comparable baselines.\nFinally, we evaluated how synthesis speed scaled with utterance\nlength for the different models, by generating 180 sentences of dif-\nferent lengths using a GPT-28 model and plotting wall-clock syn-\nthesis time in Fig. 3, also fitting least-squares regression lines to the\ndata. The results show that MAT-2 synthesis speed becomes compet-\nitive with FS2 at longer utterances, with MAT-4 not far behind. The\nmajor contributor to this appears to be the new architecture (since\nGRAD-4 and GCFM-4 both are much slower), and the gap from\nMAT to GRAD only grows with longer utterances.\n5. CONCLUSIONS AND FUTURE WORK\nWe have introduced Matcha-TTS, a fast, probabilistic, and high-\nquality ODE-based TTS acoustic model trained using conditional\nflow matching.\nThe approach is non-autoregressive, memory ef-\nficient, and jointly learns to speak and align. Compared to three\nstrong pre-trained baselines, Matcha-TTS provides superior speech\nnaturalness and can match the speed of the fastest model on long ut-\nterances. Our experiments show that both the new architecture and\nthe new training contribute to these improvements.\nCompelling future work includes making the model multi-\nspeaker, adding probabilistic duration modelling, and applications\nto challenging, diverse data such as spontaneous speech [42].\n6. REFERENCES\n[1] Y. Song and S. Ermon, \u201cGenerative modeling by estimating\ngradients of the data distribution,\u201d Proc. NeurIPS, 2019.\n[2] P. Dhariwal and A. Nichol, \u201cDiffusion models beat GANs on\nimage synthesis,\u201d in Proc. NeurIPS, 2021, pp. 8780\u20138794.\n8https://huggingface.co/gpt2\n4\n[3] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Om-\nmer, \u201cHigh-resolution image synthesis with latent diffusion\nmodels,\u201d in Proc. CVPR, 2022, pp. 10 684\u201310 695.\n[4] S. Alexanderson, R. Nagy, J. Beskow, and G. E. Henter,\n\u201cListen, denoise, action! Audio-driven motion synthesis with\ndiffusion models,\u201d ACM ToG, vol. 42, no. 4, 2023, article 44.\n[5] S. Mehta, S. Wang, S. Alexanderson, J. Beskow, \u00c9. Sz\u00e9kely,\nand G. E. Henter, \u201cDiff-TTSG: Denoising probabilistic integ-\nrated speech and gesture synthesis,\u201d in Proc. SSW, 2023.\n[6] N. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, and\nW. Chan, \u201cWaveGrad: Estimating gradients for waveform gen-\neration,\u201d in Proc. ICLR, 2021.\n[7] N. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, N. Dehak,\nand W. Chan, \u201cWaveGrad 2: Iterative refinement for text-to-\nspeech synthesis,\u201d in Proc. Interspeech, 2021, pp. 3765\u20133769.\n[8] V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, and M. Kudinov,\n\u201cGrad-TTS: A diffusion probabilistic model for text-to-\nspeech,\u201d in Proc. ICML, 2021, pp. 8599\u20138608.\n[9] M. Jeong, H. Kim, S. J. Cheon, B. J. Choi, and N. S. Kim,\n\u201cDiff-TTS: A denoising diffusion model for text-to-speech,\u201d in\nProc. Interspeech, 2021, pp. 3605\u20133609.\n[10] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro,\n\u201cDiffWave: A versatile diffusion model for audio synthesis,\u201d\nin Proc. ICLR, 2021.\n[11] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Er-\nmon, and B. Poole, \u201cScore-based generative modeling through\nstochastic differential equations,\u201d in Proc. ICLR, 2021.\n[12] M. S. Albergo and E. Vanden-Eijnden, \u201cBuilding normalizing\nflows with stochastic interpolants,\u201d in Proc. ICLR, 2022.\n[13] R. T. Q. Chen, Y. Rubanova, J. Bettencourt et al., \u201cNeural or-\ndinary differential equations,\u201d in Proc. NeurIPS, 2018.\n[14] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu et al., \u201cFlow matching\nfor generative modeling,\u201d in Proc. ICLR, 2023.\n[15] I. Vovk, T. Sadekova, V. Gogoryan, V. Popov, M. Kudinov,\nand J. Wei, \u201cFast Grad-TTS: Towards efficient diffusion-based\nspeech generation on CPU,\u201d in Proc. Interspeech, 2022.\n[16] J. Betker, \u201cBetter speech synthesis through scaling,\u201d arXiv pre-\nprint arXiv:2305.07243, 2023.\n[17] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y.\nLiu, \u201cFastSpeech: Fast, robust and controllable text to speech,\u201d\nin Proc. NeurIPS, 2019.\n[18] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y.\nLiu, \u201cFastSpeech 2: Fast and high-quality end-to-end text to\nspeech,\u201d in Proc. ICLR, 2021.\n[19] O. Press, N. A. Smith, and M. Lewis, \u201cTrain short, test long:\nAttention with linear biases enables input length extrapola-\ntion,\u201d in Proc. ICLR, 2022.\n[20] J. Kim, S. Kim, J. Kong, and S. Yoon, \u201cGlow-TTS: A gener-\native flow for text-to-speech via monotonic alignment search,\u201d\nin Proc. NeurIPS, 2020, pp. 8067\u20138077.\n[21] J. Kim, J. Kong, and J. Son, \u201cVITS: Conditional variational\nautoencoder with adversarial learning for end-to-end text-to-\nspeech,\u201d in Proc. ICML, 2021, pp. 5530\u20135540.\n[22] P. Shaw, J. Uszkoreit, and A. Vaswani, \u201cSelf-attention with re-\nlative position representations,\u201d in Proc. NAACL, 2018.\n[23] H. Zhang, Z. Huang, Z. Shang, P. Zhang, and Y. Yan, \u201cLin-\nearSpeech: Parallel text-to-speech with linear complexity,\u201d in\nProc. Interspeech, 2021, pp. 4129\u20134133.\n[24] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu, \u201cRo-\nFormer: Enhanced Transformer with rotary position embed-\nding,\u201d arXiv preprint arXiv:2104.09864, 2021.\n[25] U. Wennberg and G. E. Henter, \u201cThe case for translation-\ninvariant self-attention in Transformer-based language mod-\nels,\u201d in Proc. ACL-IJCNLP Vol. 2, 2021, pp. 130\u2013140.\n[26] S. Mehta, A. Kirkland, H. Lameris, J. Beskow, \u00c9. Sz\u00e9kely, and\nG. E. Henter, \u201cOverFlow: Putting flows on top of neural trans-\nducers for better TTS,\u201d in Proc. Interspeech, 2023.\n[27] R. Huang, Z. Zhao, H. Liu, J. Liu, C. Cui, and Y. Ren,\n\u201cProDiff: Progressive fast diffusion model for high-quality\ntext-to-speech,\u201d in Proc. MM, 2022, pp. 2595\u20132605.\n[28] O. Watts, G. E. Henter, J. Fong, and C. Valentini-Botinhao,\n\u201cWhere do the improvements come from in sequence-to-\nsequence neural TTS?\u201d in Proc. SSW, 2019, pp. 217\u2013222.\n[29] S. Mehta, \u00c9. Sz\u00e9kely, J. Beskow, and G. E. Henter, \u201cNeural\nHMMs are all you need (for high-quality attention-free TTS),\u201d\nin Proc. ICASSP, 2022, pp. 7457\u20137461.\n[30] S.-g. Lee, W. Ping, B. Ginsburg, B. Catanzaro, and S. Yoon,\n\u201cBigVGAN: A universal neural vocoder with large-scale train-\ning,\u201d in Proc. ICLR, 2023.\n[31] X. Liu et al., \u201cFlow straight and fast: Learning to generate and\ntransfer data with rectified flow,\u201d in Proc. ICLR, 2023.\n[32] Z. Ye, W. Xue, X. Tan, J. Chen, Q. Liu, and Y. Guo, \u201cCo-\nMoSpeech: One-step speech and singing voice synthesis via\nconsistency model,\u201d in Proc. ACM MM, 2023, pp. 1831\u20131839.\n[33] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz et al.,\n\u201cVoicebox: Text-guided multilingual universal speech genera-\ntion at scale,\u201d arXiv preprint arXiv:2306.15687, 2023.\n[34] M. Bernard and H. Titeux, \u201cPhonemizer: Text to phones tran-\nscription for multiple languages in Python,\u201d J. Open Source\nSoftw., vol. 6, no. 68, p. 3958, 2021.\n[35] J. Kong, J. Kim, and J. Bae, \u201cHiFi-GAN: Generative ad-\nversarial networks for efficient and high fidelity speech syn-\nthesis,\u201d in Proc. NeurIPS, 2020, pp. 17 022\u201317 033.\n[36] R. Prenger, R. Valle, and B. Catanzaro, \u201cWaveGlow:\nA\nflow-based generative network for speech synthesis,\u201d in Proc.\nICASSP, 2019, pp. 3617\u20133621.\n[37] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. Mcleavey, and\nI. Sutskever, \u201cRobust speech recognition via large-scale weak\nsupervision,\u201d in Proc. ICML, 2023, pp. 28 492\u201328 518.\n[38] J. Taylor and K. Richmond, \u201cConfidence intervals for ASR-\nbased TTS evaluation,\u201d in Proc. Interspeech, 2021.\n[39] K. Prahallad, A. Vadapalli, N. Elluru, G. Mantena, B. Pulu-\ngundla et al., \u201cThe Blizzard Challenge 2013 \u2013 Indian language\ntask,\u201d in Proc. Blizzard Challenge Workshop, 2013.\n[40] C.-H. Chiang, W.-P. Huang, and H. yi Lee, \u201cWhy we should\nreport the details in subjective evaluation of TTS more rigor-\nously,\u201d in Proc. Interspeech, 2023, pp. 5551\u20135555.\n[41] A. Kirkland, S. Mehta, H. Lameris, G. E. Henter, E. Szekely\net al., \u201cStuck in the MOS pit: A critical analysis of MOS test\nmethodology in TTS evaluation,\u201d in Proc. SSW, 2023.\n[42] \u00c9. Sz\u00e9kely, G. E. Henter, J. Beskow, and J. Gustafson, \u201cSpon-\ntaneous conversational speech synthesis from found data,\u201d in\nProc. Interspeech, 2019, pp. 4435\u20134439.\n5\n"
  },
  {
    "title": "ResFields: Residual Neural Fields for Spatiotemporal Signals",
    "link": "https://arxiv.org/pdf/2309.03160.pdf",
    "upvote": "7",
    "text": "Published as a conference paper at ICLR 2024\nRESFIELDS: RESIDUAL NEURAL FIELDS\nFOR SPATIOTEMPORAL SIGNALS\nMarko Mihajlovic1, Sergey Prokudin1,3, Marc Pollefeys1,2, Siyu Tang1 \u2217\nETH Zurich1; Microsoft2; ROCS, University Hospital Balgrist, University of Z\u00a8urich 3\nmarkomih.github.io/ResFields\nABSTRACT\nNeural fields, a category of neural networks trained to represent high-frequency\nsignals, have gained significant attention in recent years due to their impressive\nperformance in modeling complex 3D data, such as signed distance (SDFs) or\nradiance fields (NeRFs), via a single multi-layer perceptron (MLP). However, de-\nspite the power and simplicity of representing signals with an MLP, these methods\nstill face challenges when modeling large and complex temporal signals due to the\nlimited capacity of MLPs. In this paper, we propose an effective approach to ad-\ndress this limitation by incorporating temporal residual layers into neural fields,\ndubbed ResFields. It is a novel class of networks specifically designed to effec-\ntively represent complex temporal signals. We conduct a comprehensive analysis\nof the properties of ResFields and propose a matrix factorization technique to re-\nduce the number of trainable parameters and enhance generalization capabilities.\nImportantly, our formulation seamlessly integrates with existing MLP-based neu-\nral fields and consistently improves results across various challenging tasks: 2D\nvideo approximation, dynamic shape modeling via temporal SDFs, and dynamic\nNeRF reconstruction. Lastly, we demonstrate the practical utility of ResFields by\nshowcasing its effectiveness in capturing dynamic 3D scenes from sparse RGBD\ncameras of a lightweight capture system.\n1\nINTRODUCTION\nMulti-layer Perceptron (MLP) is a common neural network architecture used for representing con-\ntinuous spatiotemporal signals, known as neural fields. Its popularity stems from its capacity to\nencode continuous signals across arbitrary dimensions (Kim & Adal\u0131, 2003). Additionally, inher-\nent implicit regularization (Goodfellow et al., 2016; Neyshabur et al., 2014) and spectral bias (Ra-\nhaman et al., 2019) equip MLPs with excellent interpolation capabilities. Due to these remarkable\nproperties, MLPs have achieved widespread success in many applications such as image synthesis,\nanimation, texture generation, and novel view synthesis (Tewari et al., 2022; Xie et al., 2022).\nHowever, the spectral bias of MLPs (Rahaman et al., 2019), which refers to the tendency of neural\nnetworks to learn functions with low frequencies, presents a challenge when it comes to accurately\nrepresenting complex real-world signals and capturing fine-grained details. Previous efforts have\naimed to address the spectral bias by utilizing techniques like positional encoding (Vaswani et al.,\n2017; Mildenhall et al., 2020; Zhong et al., 2019; M\u00a8uller et al., 2022) or special activation functions\n(Sitzmann et al., 2020b; Fathony et al., 2020). However, even with these methods, representing fine-\ngrained details remains a challenge, particularly when dealing with large spatiotemporal signals such\nas long videos or dynamic 3D scenes.\nA straightforward way of increasing the capacity of MLPs is to increase the network complexity\nin terms of the total number of neurons. However, such an approach would make the inference\nand optimization slower and more GPU memory expensive, as time and memory complexity scales\nwith respect to the total number of parameters. Another possibility is to meta-learn MLP weights\n(Sitzmann et al., 2020a) and maintain specialized independent parameters, but this imposes slow\ntraining that does not scale to photo-realistic reconstructions (Tancik et al., 2021). By far the most\npopular approach for increasing modeling capacity is to partition the spatiotemporal field and fit\n\u2217Code, data, and pre-trained models are released at https://github.com/markomih/ResFields\n1\narXiv:2309.03160v5  [cs.CV]  11 Feb 2024\nPublished as a conference paper at ICLR 2024\nSiren (Sitzmann et al., 2020b)\nTNeRF (Li et al., 2022)\nBaseline\n+ResFields\n(a)\n(b)\n(c)\nFigure 1: ResField extends an MLP architecture to effectively represent complex temporal signals\nby replacing the conventional linear layers with Residual Field Layers. As such, ResField is versatile\nand straightforwardly compatible with most existing temporal neural fields. Here we demonstrate its\napplicability on three challenging tasks by extending Siren (Sitzmann et al., 2020b) and TNeRF (Li\net al., 2022): (a) learning temporal signed distance fields and (b) neural radiance fields from four\nRGB views and (c) from three time-synchronized RGBD views captured by our lightweight rig. The\nfigure is best viewed in electronic format on a color screen, please zoom-in to observe details.\nseparate/local neural fields (Reiser et al., 2021; M\u00a8uller et al., 2022; Chen et al., 2022). However,\nthese approaches hinder global reasoning and generalization due to local gradient updates of grid\nstructures (Peng et al., 2023).\n\u2026\n\u2026\nResField\nLayer\nResField\nLayer\nResField\nLayer\ny\n(t, x)\nW1\nWi\nWn\nWi(t)\nW1(t)\nWn(t)\n+\n+\n+\nFigure 2: ResField MLP Architecture.\nThe challenge that we aim to address is how\nto increase the model capacity in a way that\nis agnostic to the design choices of MLP neu-\nral fields. This includes architecture, input en-\ncoding, and activation functions. At the same\ntime, we must maintain the implicit regular-\nization property of neural networks and retain\ncompatibility with existing techniques devel-\noped for reducing the spectral bias (Mildenhall\net al., 2020; Sitzmann et al., 2020b). Our key idea is to substitute MLP layers with time-dependent\nlayers (see Fig. 2) whose weights are modeled as trainable residual parameters Wi(t) added to the\nexisting layer weights Wi. We dub neural fields implemented in this way ResFields.\nIncreasing the model capacity in this way offers three key advantages. First, the underlying MLP\ndoes not increase in width and hence, maintains the inference and training speed. This property\nis crucial for most practical downstream applications of neural fields, including NeRF (Mildenhall\net al., 2020) which aims to solve inverse volume rendering (Drebin et al., 1988) by querying neural\nfields billions of times. Second, this modeling retains the implicit regularization and generalization\nproperties of MLPs, unlike other strategies focused on spatial partitioning (Reiser et al., 2021; M\u00a8uller\net al., 2022; Peng et al., 2023; Is\u00b8\u0131k et al., 2023). Finally, ResFields are versatile, easily extendable,\nand compatible with most MLP-based methods for spatiotemporal signals.\nHowever, the straightforward implementation of ResFields could lead to reduced interpolation prop-\nerties due to a large number of unconstrained trainable parameters. To this end, inspired by well-\nexplored low-rank factorized layers (Denil et al., 2013; Ioannou et al., 2015; Khodak et al., 2021),\nwe propose to implement the residual parameters as a global low-rank spanning set and a set of\ntime-dependent coefficients. As we show in the following sections, this modeling enhances the gen-\neralization properties and further reduces the memory footprint caused by maintaining additional\nnetwork parameters.\n2\nPublished as a conference paper at ICLR 2024\nIn summary, our key contributions are:\n\u2022 We propose an architecture-agnostic building block for modeling spatiotemporal fields that we\ndub ResFields.\n\u2022 We systematically demonstrate that our method benefits a number of existing methods: Sitzmann\net al. (2020b); Pumarola et al. (2021); Park et al. (2021a;b); Li et al. (2022); Cai et al. (2022); Cao\n& Johnson (2023); Fridovich-Keil et al. (2023).\n\u2022 We validate ResFields on four challenging tasks and demonstrate state-of-the-art (Fig. 1): 2D\nvideo approximation, temporal 3D shape modeling via signed distance functions, radiance field\nreconstruction of dynamic scenes from sparse RGB(D) images, and learning 3D scene flow.\n2\nRELATED WORK\nNeural field is a field \u2013 a physical quantity that has a value for every point in time and space \u2013\nthat is parameterized fully or in part by a neural network (Xie et al., 2022), typically an MLP as\nthe universal approximator (Kim & Adal\u0131, 2003). However, straightforward fitting of signals to\nregular MLPs yields poor reconstruction quality due to the spectral bias of learning low frequencies\n(Rahaman et al., 2019). Even though this issue has been alleviated through special input encodings\n(Mildenhall et al., 2020; Barron et al., 2021; 2022) or activation functions (Sitzmann et al., 2020b;\nTancik et al., 2020; Fathony et al., 2020; Lindell et al., 2022; Shekarforoush et al., 2022), neural\nfields still cannot scale to long and complex temporal signals due to the limited capacity. A natural\nway of increasing the modeling capacity is to increase the network\u2019s size in terms of the number of\nparameters. However, this trivial solution does not scale with GPU and training time requirements.\nHybrid neural fields leverage explicit grid-based data structures with learnable feature vectors to\nimprove the modeling capacity via spatial (Takikawa et al., 2021; M\u00a8uller et al., 2022; Chen et al.,\n2022; Chan et al., 2022) and temporal (Shao et al., 2023; Fridovich-Keil et al., 2023; Cao & Johnson,\n2023; Peng et al., 2023) partitioning techniques. However, these approaches sacrifice the desired\nglobal reasoning and implicit regularization (Neyshabur et al., 2014; Goodfellow et al., 2016) that\nis needed for generalization, especially for solving ill-posed problems like inverse rendering. In\ncontrast, our solution, ResFields, focuses on improving pure neural network-based approaches that\nstill hold state-of-the-art results across several important applications, as we will demonstrate later.\nInput-dependent MLP weights is another common strategy for increasing the capacity of MLPs by\ndirectly regressing MLP weights, e.g. via a hypernetwork (Mehta et al., 2021; Wang et al., 2021c)\nor a convolutional (Peng et al., 2023) neural network. However, these approaches introduce an ad-\nditional, much larger network that imposes a significant computational burden for optimizing neural\nfields. KiloNeRF (Reiser et al., 2021) proposes to speed up the inference of static neural radiance\nfields by distilling the learned radiance field into a grid of small independent MLPs. However, since\na bigger MLP is still used during the first stage of the training, this model has the same scaling limi-\ntations as the original NeRF. Closest in spirit to our approach, the level-of-experts (LoE) model (Hao\net al., 2022) introduces an input-dependent hierarchical composition of shared MLP weights at the\nexpense of reduced representational capacity. Compared to LoE, ResFields demonstrate stronger\ngeneralization and higher representational power for modeling complex spatiotemporal signals.\nTemporal fields are typically modeled by feeding the time-space coordinate pairs to neural fields.\nSIREN (Sitzmann et al., 2020b) was one of the first neural methods to faithfully reconstruct a 2D\nvideo signal. However, scaling this approach to 4D is infeasible and does not produce desired\nresults as demonstrated in dynamic extensions of NeRF models (Pumarola et al., 2021; Li et al.,\n2022). Therefore, most of the existing NeRF solutions (Pumarola et al., 2021; Park et al., 2021a)\ndecouple the learning problem into learning a static canonical neural field and a deformation neural\nnetwork that transforms a query point from the observation to the canonical space where the field\nis queried. However, these methods tend to fail for more complex signals due to the difficulty of\nlearning complex deformations via a neural network, as observed by Gao et al. (2022). To alleviate\nthe problem, HyperNeRF (Park et al., 2021b) introduced an additional small MLP and per-frame\nlearnable ambient codes to better capture topological variations, increase the modeling capacity, and\nsimplify the learning of complex deformation. The recent NDR (Cai et al., 2022), a follow-up work\nof HyperNeRF, further improves the deformation field by leveraging invertible neural networks and\nmore constrained SDF-based density formulation (Yariv et al., 2021). All of these methods are fully\ncompatible with the introduced ResFields paradigm which consistently improves baseline results.\n3\nPublished as a conference paper at ICLR 2024\nScene flow is commonly used to model the dynamics of neural fields. Most works use MLPs to\nmodel scene flow by predicting offset vectors (Pumarola et al., 2021; Li et al., 2021b; Prokudin et al.,\n2023; Wang et al., 2023b), SE(3) transformation (Park et al., 2021a; Wang et al., 2023a), coefficients\nof pre-defined bases (Wang et al., 2021a; Li et al., 2023), or directly using invertible architectures\n(Cai et al., 2022; Wang et al., 2023c). These representations are compatible with ResFields which\nfurther enhance their learning capabilities.\nResidual connections have a long history in machine learning. They first appeared in Rosenblatt\n(1961) in the context of coupled perceptron networks. Rosenblatt\u2019s insight was that the residual\nconnections increase the efficiency of responding to input signals. Since then, residual connections\nhave been extensively studied and found a major practical utility as a solution to training deep\nneural networks by overcoming the vanishing gradient problem (Hochreiter, 1998; Srivastava et al.,\n2015; He et al., 2016) and became a de facto standard for modeling neural networks. Unlike these\nresidual connections that are added to the output of MLP layers, our ResField layers model the\nresiduals of the MLP weights, which in turn yields higher representation power of neural fields,\nmaking them more suitable for modeling complex real-world spatiotemporal signals. To the best of\nour knowledge, directly optimizing residual or multiplicative correctives of model parameters has\nbeen explored in the context of fine-tuning large language models (Karimi Mahabadi et al., 2021;\nHu et al., 2021; Dettmers et al., 2023) or predicting model weights (Wang et al., 2021c), and has not\nbeen explored for directly training spatiotemporal neural fields.\n3\nRESFIELDS: RESIDUAL NEURAL FIELDS FOR SPATIOTEMPORAL SIGNALS\nFormulation. Temporal neural fields encode continuous signals f : Rd \u00d7 R 7\u2192 Rc via a neural\nnetwork \u03a6\u03b8, where the input is a time-space coordinate pair (t \u2208 R, x \u2208 Rd) and the output is a\nfield quantity y \u2208 Rc. More formally, the temporal neural field is defined as:\n\u03a6\u03b8(t, x) = \u03c3n\n\u0000Wn(\u03d5n\u22121 \u25e6 \u03d5n\u22122 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03d51)(t, x) + bn\n\u0001\n,\n(1)\n\u03d5i(t, xi) = \u03c3i(Wixi + bi),\n(2)\nwhere \u03d5i : RN\ni\n7\u2192 RM\ni\nis the ith layer of the MLP, which consists of the linear transformation by\nthe weight matrix Wi \u2208 RNi\u00d7Mi and the bias bi \u2208 RNi applied to the input xi \u2208 RMi, followed\nby a non-linear activation function \u03c3i. The network parameters \u03b8 are optimized by minimizing a\nloss term L directly w.r.t a ground truth signal or indirectly by relating a field quantity to the sensory\ninput, e.g. via volume rendering equation for radiance field reconstruction.\nLimitations of MLPs. To model complex and long signals, it is crucial for the underlying MLP to\nhave a sufficient modeling capacity, which scales with the total number of parameters. However, as\nthe MLP size increases, the training time of neural fields becomes slower while increasing the GPU\nmemory requirements, ultimately leading to the bottleneck being the MLP\u2019s size. This is especially\nhighlighted for dynamic radiance field reconstruction which requires solving an inverse rendering\nproblem through billions of MLP queries. In the following, we introduce ResFields, an approach\nfor alleviating the capacity bottleneck for modeling and reconstructing spatiotemporal signals.\nResFields model. We introduce residual field layers (Fig. 2) to effectively capture large and complex\nspatiotemporal signals. ResFields, an MLP that uses at least one residual field layer, alleviates the\naforementioned capacity bottleneck without increasing the size of MLPs in terms of the number\nof layers and neurons. In particular, we replace a linear layer of an MLP \u03d5i with our temporal\ntime-conditioned residual layer defined as:\n\u03d5i(t, xi) = \u03c3i((Wi + Wi(t))xi + bi) ,\n(3)\nwhere Wi(t) : R 7\u2192 RNi\u00d7Mi is time-dependent and models residuals of the network weights.\nThis simple formulation increases the model capacity via additional trainable parameters without\nmodifying the overall network architecture.\n=\n\u00d7\n\ud835\udc40!\n\ud835\udc40!\n\ud835\udc41!\n\ud835\udc41!\n\ud835\udc45!\n\ud835\udc45!\n\ud835\udc47!\n\ud835\udc47!\nFigure 3: Factorization of Wi.\nResFields factorization.\nHowever, naively imple-\nmenting Wi(t) \u2208 RNi\u00d7Mi as a dictionary of train-\nable weights would yield a vast amount of indepen-\ndent and unconstrained parameters. This would result\nin a partitioning of spatiotemporal signal, akin to the\n4\nPublished as a conference paper at ICLR 2024\nTrain\nTest\nGT\nSiren-128+ResFields\nSiren-256\nPSNR\n0\n20000\n40000\n60000\n80000\n100000\n30\n32\n34\n36\n38\n40\n42\n44\n0\n20000\n40000\n60000\n80000\n100000\n30\n32\n34\n36\n38\n40\n42\n44\nSiren-1024\nSiren-512+ResFields\nNGP\nTraining Iterations\nTraining Time\u2193:\n25 min\n52 min\nTest PSNR\u2191:\n33.8\n27.3\nFigure 4: 2D video approximation. Comparison of different neural fields on fitting RGB videos.\nThe training and test PSNR curves (left and right respectively) indicate the trade-off between the\nmodel\u2019s capacity and generalization properties. Instant NGP offers good overfitting capabilities,\nhowever, it struggles to generalize to unseen pixels. A Siren MLP with 1024 neurons (Siren-1024),\nshows good generalization properties, however, it lacks representation power (low training and low\ntest PSNR). A smaller Siren with 512 neurons implemented with ResFields (Siren-512+ResFields)\ndemonstrates good generalization while offering higher model capacity. Besides the higher accu-\nracy, our approach offers approximately 2.5 times faster convergence and 30% lower GPU memory\nrequirements due to using a smaller MLP (Tab. 1). Results on the right provide a visual comparison\nof Siren with 256 neurons and Siren with 128 neurons implemented with ResField layers.\nspace partitioning methods (Reiser et al., 2021; M\u00a8uller et al., 2022; Shao et al., 2023), and hinder a\nglobal reasoning and implicit bias of MLPs, essential properties for solving under constrained prob-\nlems such as a novel view synthesis from sparse setups. To this end, inspired by well-established\nlow-rank factorized layers (Denil et al., 2013; Ioannou et al., 2015; Khodak et al., 2021), we directly\noptimize time-dependent coefficients and Ri-dimensional spanning set of residual network weights\nthat are shared across the entire spatiotemporal signal (see Fig. 3). In particular, the residual of\nnetwork weights are defined as\nWi(t) =\nXRi\nr=1 vi(t)[r] \u00b7 Mi[r],\n(4)\nwhere the coefficients v(t) \u2208 RRi and the spanning set M \u2208 RRi\u00d7Ni\u00d7Mi are trainable parameters;\nsquare brackets denote element selection. To model continuous coefficients over the time dimension,\nwe implement v \u2208 RTi\u00d7Ri as a matrix and linearly interpolate its rows. Such formulation reduces\nthe total number of trainable parameters and further prevents potential undesired overfitting that is\ncommon for field partition methods as we will demonstrate later (Sec. 4.5).\nKey idea. The goal of our parametrization is to achieve high learning capacity while retaining good\ngeneralization properties. To achieve this for a limited budget in terms of the number of parameters,\nwe allocate as few independent parameters per time step (in v(t)) and as many globally shared\nparameters M. Allocating more capacity towards the shared weights will enable 1) the increased\ncapacity of the model due to its ability to discover small patterns that could be effectively compressed\nin shared weights and 2) stronger generalization as the model is aware of the whole sequence through\nthe shared weights. Given these two objectives, we design v(t)) to have very few parameters (Ri)\nand M to have most parameters (Ri\u00d7Ni\u00d7Mi); see the Sup. Mat. for further implementation details.\n4\nEXPERIMENTS\nTo highlight the versatility of ResFields, we analyze our method on four challenging tasks: 2D video\napproximation via neural fields, learning of temporal signed distance functions, radiance reconstruc-\ntion of dynamic scenes from calibrated RGB(D) cameras, and learning 3D scene flow.\n4.1\n2D VIDEO APPROXIMATION\nLearning a mapping of pixel coordinates to the corresponding RGB colors is a popular benchmark\nfor evaluating the model capacity of fitting complex signals (M\u00a8uller et al., 2022; Sitzmann et al.,\n2020b). For comparison, we use two videos (bikes and cat from Sitzmann et al. (2020b)) that\nconsist respectively of 250 and 300 frames (with resolutions at 512 \u00d7 512 and 272 \u00d7 640) and fit\nneural representations by minimizing the mean squared error w.r.t ground truth RGB values.\n5\nPublished as a conference paper at ICLR 2024\nUnlike the proposed setup in Sitzmann et al. (2020b) where the focus is pure overfitting to the image\nvalues, our goal is to also evaluate the interpolation aspects of the models. For this, we leave out\n10% of randomly sampled pixels for validation and fit the video signal on the remaining ones. We\ncompare our approach against Instant NGP, a popular grid-based approach to neural field modeling,\nwith the best hyperparameter configuration for the task (see supplementary). We also compare\nagainst a five-layer Siren network with 1024 neurons (denoted as Siren-1024), as a pure MLP-based\napproach. For our model, we choose a five-layer Siren network with 512 neurons, whose hidden\nlayers are implemented as residual field layers with the rank Ri = 10 for all hidden layers (Siren-\n512+ResFields). We refer to the supplementary for more details and ablation studies on the number\nof factors, ranks, and layers for the experiment.\nTable 1: Video approximation.\ntest PSNR\u2191 t[it/s]\u2191 GPU\u2193\nNGP\n34.52\n131\n1.6G\nSiren-1024\n36.37\n3.55\n9.7G\nSiren-512+ResFields\n39.21\n9.78\n6.5G\nInsights. We report training and test PSNR values averaged\nover the two videos in Fig. 4 and Tab. 1. Here, Instant-NGP of-\nfers extremely fast and good overfitting abilities as storing the\ndata in the hash grid structure effectively resolves the problem\nof limited MLP capacity, alleviating the need for residual weights. This, however, comes at the ex-\npense of the decreased generalization capability. Siren-1024 has good generalization properties, but\nclearly underfits the signal and suffers from blur artifacts. Unlike Siren-1024, Siren-512 with Res-\nFields offers significantly higher reconstruction and generalization quality (36.37 vs 39.21 PSNR)\nwhile requiring 30% less GPU memory and being about 2.5 times faster to train.\nThis simple experiment serves as a proof of concept and highlights our ability to fit complex tem-\nporal signals with smaller MLP architectures, which has a significant impact on the practical down-\nstream applications as we discuss in the following sections.\n4.2\nTEMPORAL SIGNED DISTANCE FUNCTIONS (SDF)\nSigned-distance functions model the orthogonal distance of a given spatial coordinate x to the sur-\nface of a shape, where the sign indicates whether the point is inside the shape. We model a temporal\nsequence of signed distance functions via a neural field network that maps a time-space coordinate\npair (t \u2208 R, x \u2208 R3) to a signed distance value (y \u2208 R).\nTable 2: Temporal SDF.\nRank\nResources\nMean\nRi GPU\u2193 t[ms]\u2193 CD\u2193 ND\u2193\nSiren-128\n2.4G 20.06 15.06 27.23\n+ResFields\n5\n2.5G 20.25\n9.47 18.54\n10\n8.79 16.61\n20\n8.43 15.48\n40\n8.16 14.19\nSiren-256\n3.6G 47.99 9.04 16.37\n+ResFields\n5\n3.8G 48.19\n7.90 13.00\n10\n7.71 12.24\n20\n7.66 11.84\n40\n7.67 11.67\nWe sample five sequences of different levels of difficulty (four\nfrom Deforming Things (Li et al., 2021a) and one from ReSynth\n(Ma et al., 2021) and convert the ground-truth meshes to SDF\nvalues. We supervise all methods by the MAPE loss following\nM\u00a8uller et al. (2022). To benchmark the methods, we extract a\nsequence of meshes from the learned neural fields via march-\ning cubes (Lorensen & Cline, 1987) and report L1 Chamfer dis-\ntance (CD\u2193) and normal consistency (NC\u2193) w.r.t the ground-truth\nmeshes (scaled by 103 and 102 respectively). As a main baseline,\nwe use the current state-of-the-art Siren network (five layers) and\ncompare it against Siren implemented with our ResField layers,\nwhere residual field layers are applied to three middle layers. We empirically observe that using\nResField on the first and last layers has a marginal impact on the performance since weight matrices\nare small and do not impose a bottleneck for modeling capacity.\nInsights. Quantitative and qualitative results (Tab. 2, Fig. 1) demonstrate that ResFields consistently\nimprove the reconstruction quality, with the higher rank increasingly improving results. Importantly,\nwe observe that Siren with 128 neurons and ResFields (rank 40), performs better compared to the\nvanilla Siren with 256 neurons, making our method over two times faster while requiring less GPU\nmemory due to using a much smaller MLP architecture. Alleviating this bottleneck is of utmost\nimportance for the reconstruction tasks that require solving inverse rendering by querying the neural\nfield billions of times as we demonstrate in the next experiment.\n4.3\nTEMPORAL NEURAL RADIANCE FIELDS (NERF)\nTemporal or Dynamic NeRF represents geometry and texture as a neural field that models a function\nof color and density. The model is trained by minimizing the pixel-wise error metric between the\nimages captured from known camera poses and ones rendered via the differentiable ray marcher\n6\nPublished as a conference paper at ICLR 2024\nTable 3:\nTemporal radiance field reconstruction on Owlii (Xu et al., 2017). Previous state-of-\nthe-art methods consistently benefit from ResField layers without imposing a high computational\noverhead; bold numbers denote best per-sequence performance while colors denote the overall 1st ,\n2nd , and 3rd best-performing model; i denotes which layers are substituted with ResFeilds layers.\nMean\nBasketball\nModel\nDancer\nExercise\nt \u2193\nFPS\u2191\nCD\u2193 SSIM\u2191 PSNR\u2191 CD\u2193 SSIM\u2191 PSNR\u2191 CD\u2193 SSIM\u2191 PSNR\u2191 CD\u2193 SSIM\u2191 PSNR\u2191 CD\u2193 SSIM\u2191 PSNR\u2191\nNeus2 (Wang et al., 2023d)\n0.5h\n30\n69.4 91.01 21.73 75.7 90.57 20.48 65.6 89.64 23.01 77.1 93.16 23.23 59.2 90.67 20.21\nTensor4D (Shao et al., 2023)\n15h\n0.035\n32.8 91.05 22.59 30.5 91.22 22.51 40.3 89.30 22.46 26.7 91.53 23.24 33.5 92.16 22.16\nHexPlane (Cao & Johnson, 2023)\n5h\n0.359\n20.9 92.62 24.71 17.3 93.22 25.13 24.2 91.48 25.23 23.5 91.88 23.53 18.8 93.92 24.96\n+ ResFields (i=1)\n0.357\n17.8 93.51 25.61 14.9 93.96 25.91 21.3 92.58 26.19 19.7 93.16 24.86 16.3 94.30 25.33\n+ ResFields (i=1, 2, 3)\n0.354\n17.6 93.74 25.79 14.5 94.47 26.62 21.7 92.61 25.82 18.9 93.47 25.14 15.7 94.69 25.82\nDyNeRF (Li et al., 2022)\n12h\n0.328\n31.0 91.95 23.59 28.0 92.56 23.49 44.9 89.84 23.11 30.7 91.54 23.33 20.3 93.88 24.45\n+ ResFields (i=1)\n0.327\n20.8 93.69 25.57 14.7 94.58 26.54 26.1 92.24 25.36 24.6 93.35 25.20 17.7 94.59 25.17\n+ ResFields (i=1, 2, 3)\n0.323\n19.3 93.81 25.49 20.3 93.49 24.77 22.2 93.07 26.16 17.6 93.69 25.22 17.1 94.99 25.80\n+ ResFields (i=1, . . . , 7)\n0.316\n19.6 94.00 25.54 17.9 94.47 25.63 23.5 93.15 26.11 20.0 93.58 25.28 16.9 94.81 25.13\nTNeRF (Li et al., 2022)\n12h\n0.339\n17.2 94.18 26.18 15.1 94.57 26.33 20.2 93.31 26.52 19.3 93.53 25.09 14.1 95.33 26.77\n+ ResFields (i=1)\n0.339\n14.6 94.99 27.15 12.1 95.67 27.98 18.5 94.07 27.23 14.9 94.59 26.20 13.0 95.63 27.19\n+ ResFields (i=1, 2, 3)\n0.334\n14.2 95.21 27.44 12.2 95.84 27.98 18.3 94.33 27.81 13.3 94.87 26.55 12.9 95.82 27.40\n+ ResFields (i=1, . . . , 7)\n0.328\n14.2 95.45 27.55 12.2 95.90 27.82 17.8 94.49 27.45 14.5 95.22 26.82 12.3 96.21 28.11\nDNeRF (Pumarola et al., 2021)\n18h\n0.215\n32.1 92.09 23.36 22.3 93.21 24.74 44.0 90.51 23.19 38.5 91.17 21.29 23.4 93.47 24.21\n+ ResFields (i=1)\n0.214\n14.2 95.16 27.33 12.1 95.88 28.26 18.1 94.15 27.03 14.1 94.66 26.24 12.8 95.95 27.79\n+ ResFields (i=1, 2, 3)\n0.213\n14.0 95.34 27.60 12.2 95.95 28.20 17.6 94.45 27.84 14.0 94.88 26.40 12.4 96.08 27.97\n+ ResFields (i=1, . . . , 7)\n0.210\n14.0 95.67 27.89 12.0 96.15 28.34 17.3 94.85 27.83 14.3 95.45 27.25 12.3 96.21 28.14\nNerfies (Park et al., 2021a)\n24h\n0.180\n23.2 93.15 24.35 21.1 93.53 24.74 28.2 92.02 24.25 23.8 92.96 23.81 19.7 94.09 24.60\n+ ResFields (i=1)\n0.180\n14.6 95.12 27.26 12.3 95.64 27.86 19.3 93.95 26.91 14.2 95.10 27.00 12.7 95.77 27.29\n+ ResFields (i=1, 2, 3)\n0.179\n14.0 95.32 27.43 11.9 95.78 27.87 18.6 94.30 27.21 13.0 95.27 27.11 12.5 95.91 27.51\n+ ResFields (i=1, . . . , 7)\n0.177\n13.8 95.57 27.72 11.8 95.79 27.42 17.6 94.68 27.78 13.5 95.67 27.73 12.2 96.16 27.94\nHyperNeRF (Park et al., 2021b)\n32h\n0.145\n16.0 94.94 26.84 13.0 95.47 27.44 22.0 93.76 26.50 15.7 94.89 26.27 13.2 95.64 27.15\n+ ResFields (i=1)\n0.144\n14.4 95.18 27.36 12.4 95.73 28.05 18.7 94.18 27.38 13.4 95.15 26.96 13.0 95.65 27.05\n+ ResFields (i=1, 2, 3)\n0.144\n14.1 95.35 27.45 12.4 95.86 28.11 18.4 94.36 27.32 12.9 95.35 27.24 12.8 95.82 27.14\n+ ResFields (i=1, . . . , 7)\n0.143\n14.2 95.50 27.64 11.9 95.77 27.54 18.0 94.63 27.76 14.3 95.38 27.11 12.4 96.24 28.16\nNDR (Cai et al., 2022)\n34h\n0.129\n15.3 94.82 26.78 13.2 95.36 27.31 19.7 93.98 26.95 15.4 94.27 25.69 12.9 95.65 27.18\n+ ResFields (i=1)\n0.129\n14.7 95.14 27.16 12.6 95.74 27.87 18.2 94.17 27.14 15.2 94.83 26.46 12.9 95.84 27.17\n+ ResFields (i=1, 2, 3)\n0.129\n14.0 95.36 27.55 12.2 96.00 28.31 18.1 94.29 27.21 13.2 95.12 26.87 12.6 96.04 27.81\n+ ResFields (i=1, . . . , 7)\n0.127\n14.2 95.56 27.81 11.9 96.02 28.02 19.3 94.48 27.51 13.1 95.38 27.11 12.4 96.36 28.61\nDyNeRF\nDNeRF\nNerfies\nTNeRF\nHyperNeRF\nNDR\nGT\n(Li et al., 2022)\n(Pumarola et al., 2021)\n(Park et al., 2021a)\n(Li et al., 2022)\nPark et al. (2021b)\n(Cai et al., 2022)\nSSIM/PSNR\u2191\nCD\u2193\nSSIM/PSNR\u2191\nCD\u2193\nSSIM/PSNR\u2191\nCD\u2193\nSSIM/PSNR\u2191\nCD\u2193\nSSIM/PSNR\u2191\nCD\u2193\nSSIM/PSNR\u2191\nCD\u2193\nBaseline\n92.56/23.49\n28.0\n92.56/23.49\n28.0\n93.53/24.74\n21.1\n94.57/26.33\n15.1\n95.47/27.44\n13.0\n95.36/27.31\n13.2\n+ ResFields\n94.47/25.63\n17.9\n96.15/28.34\n12.0\n95.79/27.42\n11.8\n95.90/27.82\n12.2\n95.77/27.54\n11.9\n96.02/28.02\n11.9\nBaseline\n91.54/23.33\n30.7\n92.56/23.49\n28.0\n92.96/23.81\n23.8\n94.57/26.33\n15.1\n95.47/27.44\n13.0\n95.36/27.31\n13.2\n+ ResFields\n91.54/23.33\n30.7\n95.45/27.25\n14.3\n95.67/27.73\n13.5\n95.22/26.82\n14.5\n95.38/27.11\n14.3\n95.38/27.11\n13.1\nFigure 5: Temporal radiance fields on Owlii (Tab. 3); metrics are averaged across all test views.\n7\nPublished as a conference paper at ICLR 2024\n(Mildenhall et al., 2020). To better model geometry, we adopt the MLP architecture and signed\ndistance field formulation from VolSDF (Yariv et al., 2021) that defines density function as Laplace\u2019s\ncumulative distribution function applied to SDF. We refer readers to the supplementary for the results\nwith the NeRF backbone and further implementation details.\nFollowing (Wang et al., 2021b), all models are supervised by minimizing the pixel-wise difference\nbetween the rendered and ground truth colors (l1 error), the rendered opacity and the gt mask (binary\ncross-entropy), and further adopting the Eikonal (Gropp et al., 2020) mean squared error loss for\nwell-behaved surface reconstruction under the sparse capture setup:\nL = Lcolor + \u03bb1Ligr + \u03bb2Lmask.\n(5)\nWe use four sequences from the Owlii (Xu et al., 2017) dataset to evaluate the methods. Compared\nto fully synthetic sequences previously utilized for the task (Pumarola et al., 2021), the dynamic\nOwlii sequences exhibit more rapid and complex high-frequency motions, making it a harder task\nfor MLP-based methods. At the same time, the presence of ground truth 3D scans allows us to\nevaluate both geometry and appearance reconstruction quality, as compared to the sequences with\nonly RGB data available (Li et al., 2022; Shao et al., 2023). We render 400 RGB training images\nfrom four static camera views from 100 frames/time intervals and 100 test images from a rotating\ncamera from 100 frames. We report L1 Chamfer distance (CD\u2193) (scaled by 103) and the standard\nimage-based metrics (PSNR\u2191, SSIM\u2191).\nWe benchmark recent state-of-the-art methods and their variations implemented with ResField lay-\ners of rank ten (Ri = 10) \u2013 TNeRF (Pumarola et al., 2021; Li et al., 2022), DyNeRF (Li et al.,\n2022), DNeRF (Pumarola et al., 2021), Nerfies (Park et al., 2021a), HyperNeRF (Park et al., 2021b),\nNDR (Cai et al., 2022), and HexPlane (Cao & Johnson, 2023; Fridovich-Keil et al., 2023) \u2013 as well\nas a recent timespace-partitioning methods Tensor4D (Shao et al., 2023) and NeuS2 (Wang et al.,\n2023d) (with default training configurations). Please see the Sup. Mat. for further details.\nInsights. We report all quantitative and qualitative results in Tab. 3 and Fig. 5. Results demonstrate\nthat our method consistently improves all baseline methods, achieving new state-of-the-art results\nfor sparse multi-view reconstruction of dynamic scenes. We further observe that more ResField\nlayers gradually improve results until the point of saturation (i = 1, 2, 3). This experiment con-\nfirms that increasing the modeling capacity to a more-than-needed level does not cause overfitting.\nImportantly, the simplest/cheapest baseline method TNeRF implemented with ResFields performs\nbetter than every other more expensive baseline method in the original form. We believe that such\nspeedup and lower memory requirements are of great benefit to the research community, as they\nenable the use of lower-end hardware for high-fidelity reconstructions. Given this observation, we\nset up a simple camera rig and captured longer and more complex sequences to better understand\nthe limitations.\nTable 4: Lightweight capture from three RGBD views.\nMean\nBook\nGlasses\nHand\nWriting\nLPIPS\u2193 SSIM\u2191 LPIPS SSIM LPIPS SSIM LPIPS SSIM\u2191 LPIPS SSIM\nTNeRF\n0.234 79.16 0.323 68.85 0.206 80.44 0.239 81.30 0.168 86.08\n+ResFields 0.203 80.00 0.284 70.84 0.164 80.65 0.210 82.09 0.155 86.43\nLightweight\ncapture\nfrom\nthree\nRGBD views.\nWe capture four se-\nquences (150 frames) via synchronized\nAzure Kinects (three for reconstruction\nand one for validation) and compare\nTNeRF (w.\ndepth supervision),\na\nbaseline with a good balance between computational complexity and accuracy, and its enhancement\nwith ResFields applied to all middle layers. Quantitative evaluation in terms of mean SSIM\u2191 and\nLPIPS\u2193 (Zhang et al., 2018) reported in Tab. 4 demonstrates that ResFields consistently benefits the\nreconstruction (see visuals in Fig. 1 and the Sup. video). However, we observe that both methods\nstruggle to capture thin and tiny surfaces such as the cord of sunglasses.\n4.4\nSCENE FLOW\nTable 5: Scene flow.\nit/s \u2191 type fwd/bwd l1 \u2193\n128 neurons\nReLU MLP\n16.5\noffset\n6.88 / 7.31\n+ResFields\n3.85 / 3.85\nReLU MLP\nSE(3)\n4.57 / 4.58\n+ResFields\n2.64 / 2.56\nReLU MLP\nDCT\n3.19 / 3.19\n+ResFields\n2.18 / 2.19\n256 neurons\nReLU MLP\n5.6\noffset\n6.50 / 6.44\n+ResFields\n4.43 / 4.59\nReLU MLP\nSE(3)\n4.00 / 4.10\n+ResFields\n2.88 / 2.84\nReLU MLP\nDCT\n2.47 / 2.48\n+ResFields\n1.79 / 1.79\nScene flow models a 3D motion field for every point in space x and\ntime t. We take the same four Deforming Things sequences from\nSec 4.2 and learn bi-directional scene flow. We use 80% of tracked\nmesh vertices to learn the flow and the remaining 20% for evalua-\ntion. As a supervision, we use l1 error between the predicted and\nthe ground truth flow vectors. We consider three motion models that\npredict 1) offset vectors (Prokudin et al., 2023; Li et al., 2021b), 2)\n8\nPublished as a conference paper at ICLR 2024\nSE(3) transformation (Park et al., 2021a; Wang et al., 2023a), and 3) coefficients of the DCT bases\n(Wang et al., 2021a; Li et al., 2023). For all of them, we consider the same architecture from (Wang\net al., 2021a): 8-layer ReLU-MLP with positional encoding.\nInsight. Tab. 5 reports the l1 error (scaled by 103) for the evaluation points. ResFields greatly\nbenefits learning scene flow across all settings. Moreover, the architecture of 256 neurons is less\npowerful than its much smaller and faster counterpart (128 neurons) with ResFields (rank of 10) for\nboth forward and backward flow while being around three times faster: 16.5 vs. 5.6 train it/s.\n4.5\nABLATION STUDY\nTable 6: ResField modeling.\nMean PSNR\u2191\ntest\ntrain\nSiren-512\n31.89 32.13\n\u03d5i(t, xi)=\u03c3i(Wixi + bi)\n+output residual weights (Karras et al., 2020) 32.84 33.12\n\u03d5i(t, xi)=\u03c3i(Wixi + bi)+Wi(t)\n+modulated weights (Mehta et al., 2021)\n\u03d5i(t, xi)=\u03c3i((Wi\u2299Wi(t))xi+bi)32.65 32.90\n+direct (Hao et al., 2022) Wi(t)\n35.17 35.95\n\u03d5i(t, xi)=\u03c3i(Wi(t)xi+bi)\n+ResFields\n39.21 39.97\n\u03d5i(t, xi)=\u03c3i((Wi+Wi(t))xi+bi)\nTable 7: Factorization techniques.\n#params Mean PSNR\u2191\nFactorization\nRank\n[M]\ntest\ntrain\nSiren\n0.8\n31.96 32.29\n+ResFields\nNone\n236\n38.52 48.46\nReiser et al. (2021)\nLow-rank\n10\n5.4\n35.22 36.35\nmatrix-matrix:\n20\n10.0\n35.88 37.50\nv(t) \u2208 RNi\u00d7Ri\n40\n19.3\n36.67 39.01\nM \u2208 RRi\u00d7Mi\n80\n37.8\n37.69 41.07\nHyperNetwork\n10.6\n38.60 39.56\nHa et al. (2017)\n10\n0.8\n33.04 33.36\nCP\n20\n0.9\n33.14 33.47\nCarroll & Chang (1970)\n40\n1.0\n33.41 33.75\n80\n1.1\n33.72 34.08\nTucker (1966)\n10,64,64\n1.1\n33.96 34.31\n40,64,64\n1.5\n34.67 35.10\n80,64,64\n2.0\n35.08 35.59\n10,256,256\n3.6\n36.31 36.90\n40,256,256\n9.5\n38.31 39.33\n80,256,256\n17.4\n39.04 40.39\n(2,4,8)\n4.5\n36.42 37.37\nLoE\n(8,16,32)\n15.5\n39.87 42.27\nHao et al. (2022)\n(16,32,64)\n30.2\n40.53 44.15\n(32,64,128)\n59.5\n40.62 46.35\n10\n8.7\n39.59 40.80\nOurs\n20\n16.5\n40.87 42.45\nEq. 3\n40\n32.3\n41.69 43.72\n80\n63.8\n41.51 44.39\nResField modeling (Tab. 6).\nResidual connections\non the layer weights (Wi +Wi(t)) are more power-\nful compared to modeling residuals on the layer out-\nput that is commonly used for conditional generation\n(Karras et al., 2020), directly modulating layer weights\n(Wi \u2299 Wi(t)) (Mehta et al., 2021), or using time-\ndependent weights (Wi(t)) as in LoE (Hao et al.,\n2022). Tab. 6 summarizes the results of these varia-\ntions on the video approximation task from Sec. 4.1.\nFactorization techniques (Tab. 7). We compare our\nfactorization (Eq. 4) with alternative techniques: no\nfactorization (Reiser et al., 2021), low-rank matrix-\nmatrix decomposition (v(t)\n\u2208\nRNi\u00d7Ri, M\n\u2208\nRRi\u00d7Mi), regressing network parameters (Ha et al.,\n2017), hierarchical Levels-of-Experts (LoE) (Hao\net al., 2022), and the classic CP (Carroll & Chang,\n1970) and Tucker (1966). CP and Tucker with vary-\ning ranks demonstrate good generalization and over-\nfitting results. No factorization achieves great train-\ning PSNR, but its generalization performance is sub-\noptimal which has been mitigated by the hierarchi-\ncal formulation of LoE. The proposed factorization\nachieves the best generalization properties. The re-\nported numbers in Tab. 7 are measured on the video\napproximation task for 30% of unseen pixels. See the\nSup. Mat. for additional comparisons.\nLimitations. Overall ResFields benefits spatiotempo-\nral neural fields when the bottleneck lies in the model-\ning capacity rather than in solving unconstrained prob-\nlems. Specifically, we do not observe an advantage on\nchallenging ill-posed monocular reconstruction (Gao\net al., 2022) when the main bottleneck is the lack of constraints rather than the network\u2019s capacity.\n5\nDISCUSSION AND CONCLUSION\nWe present a novel approach to overcome the limitations of spatiotemporal neural fields in effec-\ntively modeling long and complex temporal signals. Our key idea is to incorporate temporal residual\nlayers into neural fields, dubbed ResFields. The advantage and utility of the method lie in its ver-\nsatility and straightforward integration into existing works for modeling 2D and 3D temporal fields.\nResFields increase the capacity of MLPs without expanding the network architecture in terms of\nthe number of layers and neurons, which allows us to use smaller MLPs without sacrificing the re-\nconstruction quality while achieving faster inference and training time with a lower GPU memory\nrequirement. We believe that progress towards using lower-cost hardware is the key to democratizing\nresearch and making technology more accessible. We hope that our study contributes to develop-\nment of neural fields and provides valuable insights for modeling signals. This, in turn, can lead to\nadvancements in various domains, including computer graphics, computer vision, and robotics.\n9\nPublished as a conference paper at ICLR 2024\nAcknowledgments and Disclosure of Funding.\nWe thank Hongrui Cai and Ruizhi Shao for pro-\nviding additional details about the baseline methods and Anpei Chen, Shaofei Wang, Songyou Peng,\nand Theodora Kontogianni for constructive feedback and proofreading the manuscript. This project\nhas been supported by the Innosuisse Flagship project PROFICIENCY No. PFFS-21-19.\nEthics Statement.\nIn our pursuit of advancing signal modeling and representation techniques, our\nwork holds the potential to bring positive advancements to various domains within the entertainment\nand AI industries, benefiting both research and practical applications. However, it is crucial to\nacknowledge the indirect influence of our efforts on the field of deep fakes, as our methodology\ncontributes to the enhancement of photorealistic reconstruction from images.\nReproducibility.\nIn our commitment to promoting openness and transparency in research, we\nprovide comprehensive resources for replicating our results: 1) Open source code: we will release\nthe source code used in our experiments, which can be found in the supplementary material. This\ncode includes detailed documentation and instructions to facilitate the replication of our main results.\n2) Pre-trained models: we will release our trained models to improve verifiability. 3) Dataset: our\ncaptured dataset will be made publicly available. 4) Supplementary documentation: in addition to\nthe code, we provide a supplementary document that offers a deeper insight into our experimental\nsetups, training techniques, and other crucial details.\nREFERENCES\nEirikur Agustsson and Radu Timofte.\nNtire 2017 challenge on single image super-resolution:\nDataset and study. In Proceedings of the IEEE conference on computer vision and pattern recog-\nnition workshops, 2017.\nJonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and\nPratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields.\nIn ICCV, 2021.\nJonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf\n360: Unbounded anti-aliased neural radiance fields. In CVPR, 2022.\nHongrui Cai, Wanquan Feng, Xuetao Feng, Yan Wang, and Juyong Zhang. Neural surface recon-\nstruction of dynamic scenes with monocular rgb-d camera. In NeurIPS, 2022.\nAng Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. CVPR, 2023.\nJ Douglas Carroll and Jih-Jie Chang. Analysis of individual differences in multidimensional scaling\nvia an n-way generalization of \u201ceckart-young\u201d decomposition. Psychometrika, 1970.\nEric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio\nGallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d\ngenerative adversarial networks. In CVPR, 2022.\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance\nfields. In ECCV, 2022.\nMisha Denil, Babak Shakibi, Laurent Dinh, Marc\u2019Aurelio Ranzato, and Nando De Freitas. Predict-\ning parameters in deep learning. NeurIPS, 2013.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314, 2023.\nRobert A Drebin, Loren Carpenter, and Pat Hanrahan. Volume rendering. ACM Siggraph Computer\nGraphics, 1988.\nRizal Fathony, Anit Kumar Sahu, Devin Willmott, and J Zico Kolter. Multiplicative filter networks.\nIn ICLR, 2020.\nSara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k Warburg, Benjamin Recht, and Angjoo\nKanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In CVPR, 2023.\n10\nPublished as a conference paper at ICLR 2024\nHang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Dynamic novel-\nview synthesis: A reality check. In NeurIPS, 2022.\nXavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural\nnetworks. In AISTATS, 2010.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\nAmos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regu-\nlarization for learning shapes. In ICML, 2020.\nDavid Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In ICLR, 2017.\nZekun Hao, Arun Mallya, Serge Belongie, and Ming-Yu Liu. Implicit neural representations with\nlevels-of-experts. NeurIPS, 2022.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\nnetworks. In ECCV, 2016.\nSepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem\nsolutions. INT J UNCERTAIN FUZZ, 1998.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2021.\nMustafa Is\u00b8\u0131k, Martin R\u00a8unz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes\nAgapito, and Matthias Nie\u00dfner. Humanrf: High-fidelity neural radiance fields for humans in\nmotion. ToG, 2023.\nYani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla, and Antonio Criminisi. Training\ncnns with low-rank filters for efficient image classification. arXiv preprint arXiv:1511.06744,\n2015.\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank\nhypercomplex adapter layers. NeurIPS, 2021.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-\ning and improving the image quality of stylegan. In CVPR, 2020.\nMikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicolo Fusi. Initialization and regularization\nof factorized neural layers. In ICLR, 2021.\nTaehwan Kim and T\u00a8ulay Adal\u0131. Approximation by fully complex multilayer perceptrons. Neural\ncomputation, 2003.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\nTianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim,\nTanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video\nsynthesis from multi-view video. In CVPR, 2022.\nYang Li, Hikari Takehara, Takafumi Taketomi, Bo Zheng, and Matthias Nie\u00dfner. 4dcomplete: Non-\nrigid motion estimation beyond the observable surface. In ICCV, 2021a.\nZhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time\nview synthesis of dynamic scenes. In CVPR, 2021b.\nZhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural\ndynamic image-based rendering. In CVPR, 2023.\nDavid B Lindell, Dave Van Veen, Jeong Joon Park, and Gordon Wetzstein. Bacon: Band-limited\ncoordinate networks for multiscale scene representation. In CVPR, 2022.\nXiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and Wenping Wang.\nSparseneus: Fast\ngeneralizable neural surface reconstruction from sparse views. ECCV, 2022.\n11\nPublished as a conference paper at ICLR 2024\nWilliam E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface construction\nalgorithm. ACM siggraph computer graphics, 1987.\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv\npreprint arXiv:1608.03983, 2016.\nQianli Ma, Jinlong Yang, Siyu Tang, and Michael J. Black. The power of points for modeling\nhumans in clothing. In ICCV, 2021.\nIshit Mehta, Micha\u00a8el Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, and Manmohan\nChandraker. Modulated periodic activations for generalizable local functional representations. In\nICCV, 2021.\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\nThomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics prim-\nitives with a multiresolution hash encoding. ToG, 2022.\nBehnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the\nrole of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.\nKeunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M\nSeitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In ICCV, 2021a.\nKeunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Gold-\nman, Ricardo Martin-Brualla, and Steven M. Seitz. Hypernerf: A higher-dimensional representa-\ntion for topologically varying neural radiance fields. ToG, 2021b.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-\nperformance deep learning library. NeurIPS, 2019.\nSida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos\nas dynamic mlp maps. In CVPR, 2023.\nSergey Prokudin, Qianli Ma, Maxime Raafat, Julien Valentin, and Siyu Tang. Dynamic point fields.\n2023.\nAlbert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural\nradiance fields for dynamic scenes. In CVPR, 2021.\nNasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua\nBengio, and Aaron Courville. On the spectral bias of neural networks. In ICML, 2019.\nChristian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural\nradiance fields with thousands of tiny mlps. In ICCV, 2021.\nFrank Rosenblatt. Principles of neurodynamics. perceptrons and the theory of brain mechanisms.\nTechnical report, Cornell Aeronautical Lab Inc Buffalo NY, 1961.\nRuizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang, and Yebin Liu. Tensor4d:\nEfficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering.\nIn\nCVPR, 2023.\nShayan Shekarforoush, David Lindell, David J Fleet, and Marcus A Brubaker. Residual multiplica-\ntive filter networks for multiscale reconstruction. NeurIPS, 2022.\nVincent Sitzmann, Eric Chan, Richard Tucker, Noah Snavely, and Gordon Wetzstein. Metasdf:\nMeta-learning signed distance functions. NeurIPS, 2020a.\nVincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-\nplicit neural representations with periodic activation functions. NeurIPS, 2020b.\n12\nPublished as a conference paper at ICLR 2024\nRupesh Kumar Srivastava, Klaus Greff, and J\u00a8urgen Schmidhuber. Highway networks. arXiv preprint\narXiv:1505.00387, 2015.\nTowaki\nTakikawa,\nJoey\nLitalien,\nKangxue\nYin,\nKarsten\nKreis,\nCharles\nLoop,\nDerek\nNowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level\nof detail: Real-time rendering with implicit 3d shapes. In CVPR, 2021.\nMatthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh\nSinghal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn\nhigh frequency functions in low dimensional domains. NeurIPS, 2020.\nMatthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P Srinivasan, Jonathan T\nBarron, and Ren Ng. Learned initializations for optimizing coordinate-based neural representa-\ntions. In CVPR, 2021.\nAyush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Wang Yifan,\nChristoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al. Ad-\nvances in neural rendering. In Computer Graphics Forum, 2022.\nLedyard R Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 1966.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.\nChaoyang Wang, Ben Eckart, Simon Lucey, and Orazio Gallo. Neural trajectory fields for dynamic\nnovel view synthesis. arXiv preprint arXiv:2105.05994, 2021a.\nChaoyang Wang, Lachlan Ewen MacDonald, Laszlo A Jeni, and Simon Lucey. Flow supervision\nfor deformable nerf. In CVPR, 2023a.\nHengyi Wang, Jingwen Wang, and Lourdes Agapito. Morpheus: Neural dynamic 360 surface re-\nconstruction from monocular rgb-d video. arXiv preprint arXiv:2312.00778, 2023b.\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus:\nLearning neural implicit surfaces by volume rendering for multi-view reconstruction. NeurIPS,\n2021b.\nQianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski,\nand Noah Snavely. Tracking everything everywhere all at once. In ICCV, 2023c.\nShaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas Geiger, and Siyu Tang. Metaavatar: Learning\nanimatable clothed human models from few depth images. In NeurIPS, 2021c.\nYiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and Lingjie Liu.\nNeus2: Fast learning of neural implicit surfaces for multi-view reconstruction. In ICCV, 2023d.\nYiqun Wang, Ivan Skorokhodov, and Peter Wonka. Pet-neus: Positional encoding tri-planes for\nneural surfaces. In CVPR, 2023e.\nYiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico\nTombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual comput-\ning and beyond. In Computer Graphics Forum. Wiley Online Library, 2022.\nYi Xu, Yao Lu, and Ziyu Wen.\nOwlii dynamic human mesh sequence dataset.\nIn ISO/IEC\nJTC1/SC29/WG11 m41658, 120th MPEG Meeting, 2017.\nLior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces.\nNeurIPS, 2021.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In CVPR, 2018.\nEllen D Zhong, Tristan Bepler, Joseph H Davis, and Bonnie Berger. Reconstructing continuous\ndistributions of 3d protein structure from cryo-em images. arXiv preprint arXiv:1909.05215,\n2019.\n13\nPublished as a conference paper at ICLR 2024\nA\nAPPENDIX\nWe provide additional implementation details and experiments to complement our study. All the\nreported runtime in this paper is measured on an NVIDIA RTX 3090 GPU card.\nA.1\nIMPLEMENTATION DETAILS\nInitialization. For experiments that use Siren networks (Sitzmann et al., 2020b) (sections 4.1 and\n4.2), we follow their proposed initialization scheme. Models used for SDF-based dynamic radiance\nfield reconstruction (Sec. 4.3) are initialized following the geometric initialization scheme (Gropp\net al., 2020). Other neural network weights are initialized following Glorot & Bengio (2010).\nResidual weights. Parameters (vi, Mi) which model our residual weights are initialized with a\nnormal distribution \u223c N(0, 10\u22122) to ensure a negligible modification of the initial MLP weights.\nWe observe that larger initial values may negatively affect geometric and Siren initialization. For all\nexperiments in the main paper, we set the number of coefficients Ti to the number of frames unless\nspecified otherwise.\nTraining details. All models are trained with the Adam optimizer (Kingma & Ba, 2015) with\ndefault parameters defined by the PyTorch framework (Paszke et al., 2019). We observe stable\nconvergence with the learning rate of 5 \u00d7 10\u22124 and gradual cosine annealing (Loshchilov & Hutter,\n2016) until the minimum learning rate of 5 \u00d7 10\u22125 for the experiments on dynamic neural radiance\nfields (Sec. 4.3). For other experiments (sections 4.1 and 4.2), we use the learning rate of 5 \u00d7 10\u22125\nand cosine annealing until 5 \u00d7 10\u22126. All methods are trained respectively for 105, 2 \u00d7 105, 4 \u00d7 105,\nand 6 \u00d7 105 iterations on the 2D video approximation task (Sec. 4.1), temporal SDF reconstruction\n(Sec. 4.2), and dynamic volumetric reconstruction (Sec. 4.3) on Owlii (Xu et al., 2017) and our\ncaptured sequences. An exception in Sec. 4.3 is with the grid-partitioning methods \u2013 Tenso4D (Shao\net al., 2023) and HexPlane (Cao & Johnson, 2023; Fridovich-Keil et al., 2023) \u2013 which were trained\nfor fewer iterations (2 \u00d7 105) as they use shallower MLPs which converge faster.\nA.2\n2D VIDEO APPROXIMATION TASK\nTable A.1: Number of factors.\nFactors\nMean PSNR\u2191\nTi\ntest\ntrain\nSiren-512\n32.02\n32.27\n+ResFields\n100%\n39.86\n40.73\n95%\n39.90\n40.77\n90%\n39.79\n40.69\n80%\n39.69\n40.62\n70%\n39.60\n40.49\n60%\n39.53\n40.44\n50%\n39.45\n40.37\n40%\n39.25\n40.20\n30%\n39.10\n40.04\n20%\n38.87\n39.82\n10%\n38.34\n39.29\nTable A.2: Time interpolation.\nFactors\nMean PSNR\u2191\nTi\ntest\ntrain\nSiren-512\n26.72\n32.36\n+ResFields\n90 %\n21.61\n40.90\n80 %\n22.01\n40.82\n70 %\n24.57\n40.76\n60 %\n26.06\n40.62\n50 %\n26.12\n40.58\n40 %\n25.54\n40.41\n30 %\n26.51\n40.18\n20 %\n27.32\n39.91\n10 %\n27.34\n39.37\nTable A.3: Layers vs. rank.\nResField\nRank\n#params\nMean PSNR\u2191\nLayers i\nRi\n[M]\ntest\ntrain\n2\n15\n4.7\n37.53\n38.25\n1, 2, 3\n5\n38.01\n38.67\n2\n30\n8.7\n38.75\n39.69\n1, 2, 3\n10\n39.86\n40.73\n2\n45\n12.6\n39.33\n40.43\n1, 2, 3\n15\n40.62\n41.66\n2\n60\n16.5\n39.67\n40.88\n1, 2, 3\n20\n41.20\n42.34\nAll methods presented in the paper (Sec. 4.1) on the 2D video\napproximation task are trained for 100k iterations, each iteration\ncontaining 200k random samples from the training set.\nFor the NGP baseline in Sec. 4.1, we follow the default setup and\nuse a two-layer fully fused network with ReLU activation func-\ntions and run a grid search of hyperparameters to find the optimal\nconfiguration. Specifically, we vary the table size T and the num-\nber of levels L in Tab. A.5 and found that the best results are\nachieved with T = 23 and L = 8 which is reported in the main\npaper (Fig. 4). Furthermore, we provide results (Tab. A.5) of a\nmuch larger five-layer Siren network with 1700 neurons per layer\nto match the number of trainable parameters to ResFields imple-\nmented with a five-layer Siren network with 512 neurons, each\ncontaining 8.7M parameters. Expectedly, we observe that both\nmethods achieve similar fitting and generalization performance.\nHowever, training such a huge MLP with 1700 neurons becomes\nimpractical, making our approach over six times faster to train\nwhile requiring over two times less GPU memory.\nNumber of factors (Tab. A.1). We further ablate the impact of\nthe number of factors Ti of ResFields. In this experiment, we\nleave out 10% of randomly sampled pixels for validation and\nvary the number of factors used for parameterizing the coeffi-\ncients v \u2208 RTi\u00d7Ri, in particular, we set Ti as the percentage of\nthe total number of frames. The results averaged over two videos\nare reported in Tab. A.1. We observe that the best performance\nis achieved for 95% when there\u2019s little overlap between the co-\n1\nPublished as a conference paper at ICLR 2024\nTable A.4: Ablation study of different fractions of unseen pixels on the video approximation task.\nResFields consistently demonstrate good generalization properties regardless of the difficulty level.\nUnseen\nMean PSNR\u2191\nCat PSNR\u2191\nBicycles PSNR\u2191\npixels\ntest\ntrain\ntest\ntrain\ntest\ntrain\nSiren-512\n10%\n32.02\n32.27\n31.21\n31.41\n32.84\n33.13\n+ ResFields\n39.86\n40.73\n38.58\n39.15\n41.13\n42.32\nSiren-1024\n36.67\n37.36\n34.95\n35.52\n38.38\n39.19\n+ ResFields\n43.15\n44.75\n42.49\n43.53\n43.82\n45.98\nSiren-512\n20%\n31.99\n32.27\n31.18\n31.41\n32.79\n33.13\n+ ResFields\n39.74\n40.75\n38.50\n39.15\n40.98\n42.35\nSiren-1024\n36.60\n37.39\n34.90\n35.55\n38.30\n39.23\n+ ResFields\n42.95\n44.82\n42.30\n43.53\n43.59\n46.12\nSiren-512\n30%\n31.97\n32.3\n31.15\n31.42\n32.80\n33.18\n+ ResFields\n39.59\n40.8\n38.39\n39.17\n40.80\n42.43\nSiren-1024\n36.51\n37.42\n34.83\n35.58\n38.19\n39.27\n+ ResFields\n42.72\n44.96\n42.16\n43.60\n43.28\n46.32\nSiren-512\n40%\n31.91\n32.29\n31.10\n31.41\n32.71\n33.17\n+ ResFields\n39.39\n40.85\n38.28\n39.20\n40.51\n42.50\nSiren-1024\n36.41\n37.50\n34.74\n35.63\n38.08\n39.38\n+ ResFields\n42.33\n45.14\n41.86\n43.67\n42.79\n46.62\nSiren-512\n50%\n31.85\n32.31\n31.05\n31.42\n32.66\n33.20\n+ ResFields\n39.09\n40.95\n38.08\n39.24\n40.10\n42.66\nSiren-1024\n36.26\n37.61\n34.62\n35.71\n37.90\n39.51\n+ ResFields\n41.75\n45.41\n41.43\n43.79\n42.08\n47.03\nSiren-512\n70%\n31.59\n32.40\n30.83\n31.48\n32.35\n33.32\n+ ResFields\n37.70\n41.49\n37.14\n39.42\n38.26\n43.55\nSiren-1024\n35.54\n38.04\n34.08\n36.06\n36.99\n40.02\n+ ResFields\n38.96\n46.61\n39.00\n44.44\n38.91\n48.78\nefficients. In practice, there is a negligible difference compared to using independent coefficients\n(100%) which we use as a default configuration as it is slightly computationally faster.\nTime interpolation (Tab. A.2). One downside of using per-frame independent coefficients is that\nit does not support time interpolation. We conduct an experiment to evaluate the interpolation along\nthe time axis. Here we randomly sample 10% of frames and leave them out for validation. As\nexpected, the lower number of factors Ti leads to a greater overlap among the frames, consequently\nleading to better interpolation properties results, while gradually decreasing the training PSNR.\nLayers vs. rank (Tab. A.3). Another natural question to consider is whether it is more beneficial\nto have more ResField layers or a single ResFied layer with a higher rank while maintaining the\nconstant number of trainable parameters. We conduct this experiment on the video approximation\ntask and compare methods with an equal number of parameters. We conclude that multiple ResField\nlayers provide greater modeling capacity.\nAblation for fewer training samples (Tab. A.4). To complete our study and better understand\nthe implicit bias of our method, we further benchmark Siren and ResFields with varying difficulty\nlevels, ranging from 10-70% of unseen pixels. We observe that ResFields consistently demonstrate\ngood generalization across all the levels of difficulty, well above the baseline.\nA.3\nTEMPORAL SIGNED DISTANCE FUNCTIONS (SDF)\nThe architecture of the Siren MLP used for this experiment is identical to the one used for the video\napproximation task. All methods are trained for 200k iterations, each batch containing 200k samples\nuniformly sampled across time. For each frame we follow the sampling strategy for static SDFs\n(M\u00a8uller et al., 2022) and sample 50% of points on the mesh, 37.5% normally distributed around the\nsurface N(0, 10\u22122), and remaining 12.5% are randomly sampled in space.\nWe provide the full breakdown of per-sequence results in Tab. A.6.\nA.4\nTEMPORAL NEURAL RADIANCE FIELDS (NERF)\nAll methods on the Owlii dataset are trained for 400k iterations, except HexPlanes (Cao & Johnson,\n2023; Fridovich-Keil et al., 2023) which converges faster (400k iterations) due to using shallower\nMLPs, and Tensor4D and NeuS2 for which we follow the default training scheme as the methods\nrequire a particular training strategy. The main baselines (TNeRF, DyNeRF, DNeRF, Nerfies, Hy-\n2\nPublished as a conference paper at ICLR 2024\nTable A.5: Extended ablation study on the video approximation task.\nSiren+ResFields with\n512 neurons and Siren with 1700 neurons have an equal number of parameters, however, optimizing\na huge MLP with 1700 neurons comes with a great computational cost. Our method achieves over\nsix times faster training while requiring over two times less GPU memory.\nResources\nMean\nCat Video\nBikes Video\nt [it/s]\nGPU [G]\n#params [M]\ntest PSNR\u2191\ntrain PSNR\u2191\ntest PSNR\u2191\ntrain PSNR\u2191\ntest PSNR\u2191\ntrain PSNR\u2191\nSiren-512\n11.66\n5.1\n0.8\n31.89\n32.13\n31.09\n31.29\n32.68\n32.98\n+ResFields\nR=10\n9.78\n6.5\n8.7\n39.21\n39.97\n37.96\n38.44\n40.46\n41.50\nSiren-1700\n1.42\n15\n8.7\n39.15\n40.20\n37.26\n38.14\n41.04\n42.25\nNGP T=20\nL=6\n150\n1.3\n3.6\n31.27\n33.25\n30.23\n31.71\n32.31\n34.78\nL=7\n153\n1.3\n5.7\n32.08\n35.06\n30.89\n33.42\n33.28\n36.70\nL=8\n157\n1.2\n7.8\n32.61\n36.64\n31.34\n35.15\n33.88\n38.13\nL=9\n158\n1.1\n9.9\n32.41\n37.80\n31.06\n36.21\n33.75\n39.39\nNGP T=21\nL=6\n126\n1.6\n5.1\n32.02\n34.48\n30.51\n32.20\n33.52\n36.76\nL=7\n141\n1.5\n9.3\n32.91\n36.87\n31.45\n34.69\n34.37\n39.04\nL=8\n146\n1.3\n13.5\n33.66\n39.33\n31.97\n37.29\n35.34\n41.37\nL=9\n157\n1.2\n17.7\n33.53\n41.36\n31.89\n39.38\n35.17\n43.34\nNGP T=22\nL=6\n101\n2.1\n5.1\n32.01\n34.47\n30.51\n32.20\n33.51\n36.73\nL=7\n116\n1.7\n13.5\n33.39\n37.82\n31.82\n35.39\n34.96\n40.25\nL=8\n144\n1.5\n21.9\n34.02\n41.09\n32.25\n38.71\n35.79\n43.48\nL=9\n156\n1.2\n30.3\n33.73\n43.83\n32.17\n41.69\n35.29\n45.96\nNGP T=23\nL=6\n75\n2.7\n5.1\n32.02\n34.48\n30.52\n32.20\n33.52\n36.76\nL=7\n99\n2.1\n17.4\n33.83\n39.51\n32.14\n36.44\n35.52\n42.57\nL=8\n131\n1.6\n34.2\n34.52\n43.85\n32.91\n40.85\n36.13\n46.85\nL=9\n148\n1.2\n51\n33.96\n47.56\n32.64\n44.63\n35.28\n50.50\nNGP T=24\nL=6\n51\n3.8\n5.1\n32.01\n34.46\n30.51\n32.20\n33.51\n36.72\nL=7\n77\n2.7\n17.4\n33.84\n39.51\n32.14\n36.45\n35.54\n42.58\nL=8\n130\n1.6\n51.0\n34.27\n44.68\n32.72\n41.71\n35.81\n47.66\nL=9\n145\n1.2\n84.5\n34.02\n49.51\n33.08\n46.58\n34.95\n52.43\nTable A.6: Temporal signed distance function. Siren implemented with our Residual Field layers\nconsistently improves the reconstruction quality. Moreover Siren with 128 neurons and ResFields\n(rank 40) performs better compared to the much bigger vanilla Siren with 256 neurons. Hence lead-\ning to over two times faster inference and convergence time while maintaining lower GPU memory\nrequirements; t[ms] denotes the average inference time for one million query points. Note that using\nhigher ranks almost does not affect the overall time complexity as the main bottleneck is the total\nnumber of queries and neurons.\nResources\nMean\nBear\nTiger\nVampire\nVanguard\nReSynth\nGPU\u2193 t[ms]\u2193\nCD\u2193\nND\u2193\nCD\u2193\nND\u2193\nCD\u2193\nND\u2193\nCD\u2193\nND\u2193\nCD\u2193\nND\u2193\nCD\u2193\nND\u2193\nSiren-128\n2.4G\n20.06\n15.063\n27.23\n7.605 4.519 5.159 4.934 29.490 63.686 17.099 42.953 15.960 20.057\n+ ResFields (rank=05)\n2.5G\n20.25\n9.471\n18.537 6.813 3.569 4.422 3.639 12.105 39.457 11.420 30.414 12.594 15.606\n+ ResFields (rank=10)\n8.785\n16.608 6.671 3.350 4.351 3.435\n9.545\n32.220 11.140 29.961 12.216 14.075\n+ ResFields (rank=20)\n8.427\n15.483 6.659 3.301 4.325 3.328\n8.708\n29.661 10.465 27.541 11.980 13.584\n+ ResFields (rank=40)\n8.158\n14.195 6.579 3.201 4.278 3.148\n8.563\n29.064\n9.729\n23.564 11.640 11.999\nSiren-256\n3.6G\n47.99\n9.040\n16.373 6.532 3.115 4.241 3.055 11.800 36.666 10.623 26.407 12.004 12.622\n+ ResFields (rank=05)\n3.8G\n48.19\n7.901\n13.000 6.430 3.009 4.177 2.854\n8.576\n28.846\n8.993\n20.168 11.331 10.124\n+ ResFields (rank=10)\n7.714\n12.242 6.408 3.001 4.161 2.814\n8.136\n27.404\n8.757\n18.950 11.110\n9.040\n+ ResFields (rank=20)\n7.662\n11.840 6.396 2.995 4.141 2.753\n8.076\n27.013\n8.650\n18.056 11.050\n8.385\n+ ResFields (rank=40)\n7.675\n11.674 6.381 3.070 4.137 2.723\n8.243\n27.296\n8.525\n17.526 11.087\n7.755\n3\nPublished as a conference paper at ICLR 2024\nperNeRF, and NDR) are implemented with the MLP architecture from VolSDF (Yariv et al., 2021),\nhowever, we reduced the original MLP size from 256 to 128 neurons as it is impractical to train\nsuch large MLPs on more expensive multi-view temporal sequences. The SDF MLP is followed by\na two-layer color MLP that takes the output feature of the SDF network. Different from the origi-\nnal color MLP that is conditioned on the viewing ray direction, we do not pass this information to\nthe network as it is impossible to capture any view-dependent appearance for this extremely sparse\nsetup. We observe that training without the viewing direction stabilizes training of the baselines,\nespecially those that rely on a deformation network. We follow the original formulation of the flow\nMLPs used in DNeRF, Nerfies, HyperNeRF, and NDR.\nFigure A.1: Base\nMLP weights.\nNote that the original formulation of the HexPlane method (Cao & Johnson,\n2023; Fridovich-Keil et al., 2023) is not suited for reconstruction from sparse\nviews mainly due to the lack of spherical initialization. Hence we adopt the\ngeometric initialization developed for Triplanes proposed in PET-NeuS (Wang\net al., 2023e). Unlike the other MLP-based methods, the SDF MLP head of\nHexPlane has four linear layers with ReLU activation functions. Furthermore,\nto reduce the grid-like artifacts caused by space-time discretization, we tune the\nresolution of planes (128 grid locations per dimension) and employ the addi-\ntional total variation loss akin to (Cao & Johnson, 2023). We observe that using\na higher resolution for six planes impairs the reconstruction quality.\nTo make the comparison fair among the baselines, for rendering we employ a\nnon-biased uniform sampling along the ray and sample 1100 rays during the\ntraining. On each ray, we sample 256 points where the starting and exiting\npoints of the ray are calculated by ray-box intersection. The box for each se-\nquence is estimated from the ground truth scans with a small padding of 5%.\nAll the methods are supervised by minimizing the loss term in Eq. 5, where we\nset \u03bb1 and \u03bb2 to 0.1.\nIn practice, the SDF-based density formulation performs better under a sparse\nsetup due to well-behaved surfaces. However, for completeness, we repeat this\nexperiment with the original NeRF formulation (Tab. A.7). The results demon-\nstrate that all of the baselines consistently benefit from ResFields, making Ner-\nfies+ResFields the overall best-performing method in terms of geometry and\nHyperNeRF+ResFields in terms of appearance.\nInterpreting residual weights. To better understand the internal workings of ResField MLPs, we\nextract a mesh from the base MLP weights without the residual parameters Wi(t) \u2013 see Fig. A.1 for\nresults on the basketball sequence. The extracted mesh demonstrates that the base MLP successfully\ndiscovers a pattern that is shared among all frames.\nA.5\nPRACTICAL RECONSTRUCTION FROM A LIGHTWEIGHT CAPTURE SYSTEM\nFigure A.2: Data capture.\nWe follow the VolSDF architecture from the experiment on the\nOwlii dataset and train all methods for 600k iterations (2200\nrays per batch) since the sequences are longer and images are\nof higher resolution (540\u00d7960).\nAs the scenes are forward-facing and not constrained from the\nopposite side, we employ the depth (Cai et al., 2022) and the\nsparseness loss Lsparse (Long et al., 2022). In sum, the final\nloss term is:\nL = Lcolor + \u03bb1Ldepth + \u03bb2Ligr + \u03bb3Lsparse ,\n(A.1)\nwhere we set \u03bb1, \u03bb2, and \u03bb3 to 0.1 and activate Lsparse after 70k iterations.\nData capture. Inspired by the lightweight practical capture of surgeries1 we create an identical\ncamera rig with four cameras (see Fig. A.2), three for training and one for evaluation. Additionally,\nwe crop captured images by projecting an approximated 3D bounding box around the dynamic\nregion as the main focus of our approach is reconstructing dynamic sequences.\n1OR-X Setup\n4\nPublished as a conference paper at ICLR 2024\nTable A.7: Temporal radiance field reconstruction on the Owlii dataset (Xu et al., 2017) with the\nNeRF parametrization. Previous state-of-the-art methods consistently benefit from our residual field\nlayers without the computational overhead; results with the VolSDF parametrization are provided\nin Tab. 3. Colors denote the overall 1st , 2nd , and 3rd best-performing model; i denotes which\nlayers are substituted with ResFeilds layers (Ri = 10).\nMean\nBasketball\nModel\nDancer\nExercise\nCD\u2193 SSIM\u2191 PSNR\u2191 CD\u2193 SSIM\u2191 PSNR\u2191 CD\u2193 SSIM\u2191 PSNR\u2191 CD\u2193 SSIM\u2191 PSNR\u2191 CD\u2193 SSIM\u2191 PSNR\u2191\nTNeRF (Li et al., 2022)\n61.6 93.90 26.04\n70.9 94.06 25.88\n52.9 93.18 27.06\n65.4 93.49 25.22\n57.1 94.88 25.99\n+ ResFields (i=1)\n53.6 94.54 26.72\n53.8 95.08 27.15\n53.4 93.55 27.32\n53.3 94.40 26.22\n54.0 95.14 26.20\n+ ResFields (i=1, 2, 3)\n46.1 94.81 27.00\n44.6 95.23 27.20\n49.6 93.92 27.66\n48.1 94.81 26.80\n42.3 95.28 26.32\n+ ResFields (i=1, . . . , 7) 47.6\n95.03 27.16\n49.6 95.42 27.51\n46.3 94.29 27.96\n43.5 94.96 26.74\n50.9 95.43 26.44\nDyNeRF (Li et al., 2022)\n60.8 93.68 25.78\n60.9 94.03 25.89\n56.2 92.60 26.16\n70.1 93.46 25.26\n56.1 94.63 25.79\n+ ResFields (i=1)\n52.6 94.25 26.42\n55.9 94.59 26.70\n55.5 93.23 27.08\n50.5 94.22 26.13\n48.4 94.95 25.75\n+ ResFields (i=1, 2, 3)\n51.8 94.42 26.53\n51.3 94.94 26.74\n55.3 93.31 27.07\n48.8 94.40 26.33\n51.6 95.06 25.99\n+ ResFields (i=1, . . . , 7) 48.9 94.60 26.72\n51.5 95.04 26.95\n55.7 93.63 27.40\n45.7 94.50 26.23\n42.8 95.22 26.28\nDNeRF\n138.3 92.54 24.40 128.8 92.90 24.34\n92.1 91.19 25.04 191.5 92.33 23.50 140.7 93.76 24.72\n+ ResFields (i=1)\n56.5 94.36 26.47\n48.0 95.05 27.03\n63.8 92.89 26.35\n61.9 94.29 26.09\n52.2 95.19 26.41\n+ ResFields (i=1, 2, 3)\n49.6 94.59 26.68\n49.3 95.23 27.14\n64.0 93.07 26.58\n42.2 94.87 26.75\n42.8 95.18 26.22\n+ ResFields (i=1, . . . , 7) 52.7 94.81 26.88\n47.5 95.32 26.90\n61.6 93.44 26.98\n49.0 94.99 26.87\n52.5 95.47 26.77\nNerfies (Park et al., 2021a)\n135.0 93.57 25.35\n97.9 93.55 25.21 135.5 93.10 26.20 186.5 93.41 24.73 120.1 94.23 25.26\n+ ResFields (i=1)\n52.0 94.75 26.99\n48.0 95.16 27.14\n51.7 93.96 27.79\n55.3 94.59 26.36\n53.1 95.29 26.66\n+ ResFields (i=1, 2, 3)\n45.3 94.80 26.88\n41.7 95.18 26.70\n50.4 93.99 27.82\n42.7 94.72 26.49\n46.4 95.31 26.52\n+ ResFields (i=1, . . . , 7) 42.2 94.90 26.73\n51.8 95.31 26.71\n23.6 93.84 26.97\n43.6 95.06 26.76\n49.8 95.41 26.49\nHyperNeRF (Park et al., 2021b) 63.5 94.67 26.51\n59.9 94.64 26.01\n57.8 94.25 27.55\n69.7 94.44 25.75\n66.7 95.35 26.74\n+ ResFields (i=1)\n46.9 94.69 26.80\n41.0 95.15 27.14\n50.4 93.83 27.70\n45.6 94.73 26.54\n50.7 95.04 25.80\n+ ResFields (i=1, 2, 3)\n47.1 94.85 26.99\n40.7 95.40 27.41\n46.5 93.90 27.71\n49.2 94.89 26.75\n52.0 95.20 26.07\n+ ResFields (i=1, . . . , 7) 48.0 95.07 27.27\n50.7 95.46 27.50\n49.5 94.14 27.90\n44.8 95.22 27.03\n46.9 95.46 26.66\nNDR (Cai et al., 2022)\n66.2 94.50 26.48\n64.1 94.84 26.64\n55.8 94.05 27.40\n78.1 93.93 25.34\n66.8 95.17 26.53\n+ ResFields (i=1)\n49.5 94.71 26.89\n50.4 95.02 26.90\n51.8 93.85 27.64\n47.6 94.67 26.48\n48.3 95.32 26.53\n+ ResFields (i=1, 2, 3)\n47.5 94.89 27.13\n46.2 95.36 27.36\n51.0 93.86 27.62\n46.4 94.91 26.85\n46.1 95.42 26.69\n+ ResFields (i=1, . . . , 7) 49.8 94.97 27.08\n44.2 95.31 27.14\n50.7 94.20 27.82\n49.3 95.03 26.87\n55.2 95.37 26.52\nTable A.8: Capacity of the low-rank representation. We measure the capacity of the low-rank\nparametrization on 351x510-resolution videos (250 frames) with varying levels of difficulty: 1) a\nvideo composed of randomly selected images (250 segments), 2) a video consisting of 6 coherent\nsegments, and 3) a full video depicting one sequence. We observe that increasing the number of in-\ndependent segments has a significant effect on the model performance due to the lack of information\nthat is shared across the entire signal and could be encoded in the base matrix weights. In spite of\nthis, our method successfully improves quality over the vanilla Siren even in the most challenging\ncase (29.18 vs. 19.4 PSNR for rank 40).\nRandom Video\nBikes Video\nCat Video\nMean\n(250 segments)\n(6 segments)\n(1 segment)\nRank\ntest PSNR\u2191\ntrain PSNR\u2191\ntest PSNR\u2191\ntrain PSNR\u2191\ntest PSNR\u2191\ntrain PSNR\u2191\ntest PSNR\u2191\ntrain PSNR\u2191\nSiren\n29.86\n30.21\n19.40\n19.65\n33.68\n34.05\n36.49\n36.94\n+ResFields\n1\n32.99\n33.52\n23.34\n23.78\n36.85\n37.40\n38.77\n39.38\n2\n34.18\n34.89\n24.26\n24.89\n38.26\n38.98\n40.02\n40.80\n4\n35.83\n36.82\n25.51\n26.47\n39.95\n40.94\n42.02\n43.06\n8\n37.31\n38.73\n26.84\n28.39\n41.46\n42.82\n43.62\n45.00\n10\n37.73\n39.33\n27.26\n29.09\n41.86\n43.34\n44.07\n45.57\n20\n38.74\n41.08\n28.39\n31.50\n42.82\n44.77\n45.01\n46.96\n40\n39.43\n42.54\n29.18\n33.64\n43.32\n45.85\n45.78\n48.12\nA.6\nADDITIONAL INSIGHTS\nCapacity of the low-rank representation. Learning signals with many independent parts imposes\nadditional challenges for both ResFields and coordinate MLPs in general. The coordinate MLP\nweights possess the ability to compress a spatio-temporal signal into a compact representation,\nmainly because the source signal has a significant amount of repetitive information which is ef-\nfectively represented by the optimized network weights. The shared weights of the ResFields have\nthe same property, as they tend to store information that is common across the entire signal, whereas\nthe low-rank weights accommodate for topological and dynamic changes for effective learning.\nWe conduct the following experiment to analyze the learning capacity of our low-rank weights in\nthe case of dynamic scenes with varying complexity. We create three videos with different levels of\ndifficulty: 1) A corner case when every frame depicts a novel scene (a video composed from the first\n5\nPublished as a conference paper at ICLR 2024\n250 images from the DIV2K dataset Agustsson & Timofte (2017)), 2) the bikes video containing\n6 segments, and 3) the cat video containing only 1 segment. All of these three videos are trimmed\nand cropped to the same length (250 frames) and resolution (351\u00d7 510). Then, we perform the 2D\nvideo approximation (analogous to Sec. 4.1) by learning the mapping from a space-time coordinate\nto RGB color (10% of pixels are not seen during the training and are left for evaluation).\nResults are reported in Tab. A.8. We observe that increasing the number of independent video\nsegments indeed has a significant effect on the model performance. This can be attributed to the\nlack of information that can be shared across the entire signal and efficiently encoded in the base\nmatrix weights. However, despite this, our method successfully improves quality over the vanilla\nSiren, even in the most challenging case of a random video (19.4 vs 29.18 PSNR for our model with\nrank 40). This improvement can be attributed to (a) the increased capacity of the pipeline thanks\nto residual modeling and (b) the ability of the neural network to discover small patterns that could\nbe effectively compressed into shared weights even in the extreme case of a video with completely\nrandom frames. The latter behavior is especially apparent in the simplest case when ResFields uses\nonly rank 1 (19.4 vs 23.34 PSNR for the random video sequence). Please also note that increasing\nthe capacity in our case does not lead to significant computational overhead, as we demonstrate in\nthe main submission and further emphasize in the next section.\nModeling long sequences. ResFields approach faces limitations for long and evolving signals, as\nthe shared weight matrix runs out of capacity. Here, a straightforward strategy to deal with longer\nsequences utilized in the literature (Li et al., 2022) is to split the sequence into independent chunks\nand train separate neural fields. We investigate different strategies for addressing the sequences of\narbitrary length.\nTable A.9: Modeling long sequences.\n#params\nChunking\nPSNR\u2191\n[M]\n#chunks\npart\ntest\ntrain\nSiren\n0.2\n1\n28.18\n28.21\n+ResFields\n2.2\n1\n34.58\n34.78\n2.6\n2\nshared\n35.65\n35.87\n4.2\nresidual\n35.96\n36.22\n4.6\nboth\n37.01\n37.31\n3.0\n4\nshared\n37.06\n37.32\n8.1\nresidual\n37.30\n37.66\n8.9\nboth\n38.89\n39.31\n3.8\n8\nshared\n38.58\n38.90\n16.0\nresidual\n38.36\n38.84\n17.5\nboth\n40.49\n41.08\n5.3\n16\nshared\n39.55\n39.96\n31.7\nresidual\n39.47\n40.06\n34.8\nboth\n41.32\n42.13\nAs our test bed, we consider the 2D video approximation\ntask (Sec. 4.1) with a longer 512x288-resolution video\n(1024 frames). Here, we re-purpose the video captured by\none of our Kinects for the dynamic NeRF reconstruction\ntask. We divide the video sequence into several chunks of\nvarying sizes: 512, 256, 128, and 64 frames (2, 4, 8, and\n16 sub-sequences respectively). We consider maintaining\nfor each chunk a set of shared Wi and residual weights\nWi(t) as well as both together (Wi, Wi(t)). For exam-\nple, having 4 chunks and both shared and delta weights\nupdated would mean having 4 separate tensors and 4 sep-\narate factorized delta weights. The results are summa-\nrized in the Tab. A.9.\nWe observe that as we increase the number of sub-sequences, the results gradually improve for\nall three strategies, with the best overall quality achieved by independently updating both weights.\nHowever, this strategy naturally requires the largest amount of parameters. We noted that the strat-\negy of chunking only shared weights is the most parameter-efficient for processing coherent long\nsequences. Specifically, maintaining 16 shared weights for the whole sequence achieves better qual-\nity compared to maintaining 4 sets of all considered network parameters (39.55 vs. 38.89 test PSNR),\nwhile using fewer parameters (5.3M vs. 8.9M).\n6\n"
  },
  {
    "title": "Physically Grounded Vision-Language Models for Robotic Manipulation",
    "link": "https://arxiv.org/pdf/2309.02561.pdf",
    "upvote": "7",
    "text": "Physically Grounded Vision-Language Models for Robotic Manipulation\nJensen Gao1, Bidipta Sarkar1, Fei Xia2, Ted Xiao2, Jiajun Wu1,\nBrian Ichter2, Anirudha Majumdar2,3, Dorsa Sadigh1,2\nAbstract\u2014 Recent\nadvances\nin\nvision-language\nmodels\n(VLMs) have led to improved performance on tasks such as\nvisual question answering and image captioning. Consequently,\nthese models are now well-positioned to reason about the\nphysical world, particularly within domains such as robotic\nmanipulation. However, current VLMs are limited in their\nunderstanding of the physical concepts (e.g., material, fragility)\nof common objects, which restricts their usefulness for robotic\nmanipulation tasks that involve interaction and physical reason-\ning about such objects. To address this limitation, we propose\nPHYSOBJECTS, an object-centric dataset of 39.6K crowd-\nsourced and 417K automated physical concept annotations of\ncommon household objects. We demonstrate that fine-tuning a\nVLM on PHYSOBJECTS improves its understanding of physical\nobject concepts, including generalization to held-out concepts,\nby capturing human priors of these concepts from visual\nappearance. We incorporate this physically grounded VLM in\nan interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on\ntasks that require reasoning about physical object concepts,\ncompared to baselines that do not leverage physically grounded\nVLMs. We additionally illustrate the benefits of our physically\ngrounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and\nvisualizations of our results at https://iliad.stanford.\nedu/pg-vlm/.\nI. INTRODUCTION\nLarge language models (LLMs) have shown great promise\nfor converting language instructions into task plans for em-\nbodied agents [1], [2]. The fundamental challenge in apply-\ning LLMs for this is grounding them to the physical world,\nthrough sensory input such as vision. Prior work has made\nprogress towards grounding LLMs by using vision-language\nmodels (VLMs) to indicate the presence of objects in a\nscene, or to provide feedback about occurrences in a scene\n[3]\u2013[7]. However, vision could be used to further improve\ngrounding by extracting more detailed scene information.\nFor robotic manipulation, understanding physical concepts of\nobjects, such as their material composition or their fragility,\nwould help planners identify relevant objects to interact with,\nand affordances based on physical or safety constraints. For\nexample, if a human wants a robot to get a cup of water,\nthe robot should be able to determine if a cup already has\nwater or something else in it. Also, the robot should handle\nthe cup with greater caution if it is more fragile.\nHow can we use vision to reason about physical object\nconcepts? Prior work has studied this problem using more\ntraditional vision techniques, such as self-supervised learning\non object interaction data. However, object interaction data\n1Stanford University, 2Google DeepMind, 3Princeton University. Con-\ntact: jenseng@stanford.edu.\ncan be challenging to collect when scaling up beyond a\nsmall set of objects in well-defined settings. While precise\nestimation of physical properties may sometimes be impos-\nsible without interaction data, humans can use their visual\nperception to reason at a high level about physical concepts\nwithout object interactions. For example, humans can reason\nthat a glass cup is more fragile than a plastic bottle, and\nthat it would be easier to use a bowl to hold water than a\nshallow plate. This reasoning is often based on prior semantic\nknowledge of visually similar objects, and can be done from\nstatic visual appearance alone.\nSimilarly, VLMs pre-trained using large-scale data have\ndemonstrated broad visual reasoning abilities and generaliza-\ntion [8]\u2013[13], and thus have the potential to physically reason\nabout objects in a similar fashion as humans. Therefore, we\npropose to leverage VLMs as a scalable way of providing\nthe kind of high-level physical reasoning that humans use to\ninteract with the world, which can benefit a robotic planner,\nwithout the need for interaction data. The general and flexible\nnature of VLMs also removes the need to use separate task-\nspecific vision models for physical reasoning. VLMs have\nalready been commonly incorporated into robotic planning\nsystems [3]\u2013[7], [13], making them a natural solution for\nendowing physical reasoning into robotic planning.\nHowever, while modern VLMs have improved signifi-\ncantly on tasks such as visual question answering (VQA),\nand there has been evidence of their potential for object-\ncentric physical reasoning [14], we show in this work that\ntheir out-of-the-box performance for this still leaves much\nto be desired. Although VLMs have been trained on broad\ninternet-scale data, this data does not contain many ex-\namples of object-centric physical reasoning. This motivates\nincorporating a greater variety and amount of such data\nwhen training VLMs. Unfortunately, prior visual datasets\nfor physical reasoning are not well-suited for understanding\ncommon real-world objects, which is desirable for robotics.\nTo address this, we propose PHYSOBJECTS, an object-\ncentric dataset with human physical concept annotations of\ncommon household objects. Our annotations include categor-\nical labels (e.g., object X is made of plastic) and preference\npairs (e.g., object X is heavier than object Y).\nOur main contributions are PHYSOBJECTS, a dataset of\n39.6K crowd-sourced and 417K automated physical concept\nannotations of real household objects, and demonstrating that\nusing it to fine-tune a VLM significantly improves physical\nreasoning. We show that our physically grounded VLM\nachieves improved test accuracy on our dataset, including\non held-out physical concepts. Furthermore, to illustrate\narXiv:2309.02561v4  [cs.RO]  3 Mar 2024\nVision-Language \nModel\n(a)\n1 is made of plastic\n2 is made of ceramic\n1 is less fragile than 2\n1.\n(b)\nLarge Language \nModel\nVision-Language \nModel\nIs this cup made of ceramic?\nAnswer: yes\nInstruction:\nBring me the ceramic cup.\nScene\nPlan:\n1. Go to right cup\n2. Pick up cup\n3. Bring to human\n4. Done \n(c)\n2.\nFine-tuning\nFig. 1: (a) We collect physical concept annotations of common household objects for fine-tuning VLMs. (b) We use the\nfine-tuned VLM in an LLM-based robotic planning framework, where the LLM queries the VLM about physical concepts\nof objects in the scene, before producing a plan. (c) We evaluate LLM-generated plans on a real Franka Emika Panda robot.\nthe utility of improved physical reasoning for robotics, we\nincorporate our physically grounded VLM with an LLM-\nbased robotic planner, where the LLM queries the VLM\nabout physical concepts of objects in its scene. Our system\nachieves improved planning performance on tasks that re-\nquire physical reasoning, compared to baselines that do not\nuse physically grounded VLMs. Finally, we demonstrate the\nbenefits of our physically grounded VLM for planning with\na real robot, where its usage improves task success rates.\nII. RELATED WORK\nWe review prior work on physical reasoning, object at-\ntribute datasets, VLMs, using LLMs for robotic planning,\nand using LLMs and VLMs together in an interactive system.\nPhysical Reasoning. Prior works have studied estimating\nphysical object properties from vision by learning from in-\nteraction data [15]\u2013[17]. Other works focus on learning rep-\nresentations that capture physical concepts, rather than direct\nestimation [18], [19]. Unlike these works, we use pre-trained\nVLMs and human annotations as a more scalable alternative\nto learning from interaction. Mind\u2019s Eye investigates physical\nreasoning using LLMs [20], but relies on grounding using a\nsimulator, which would be difficult to scale to the real world.\nVEC investigates physical reasoning with LLMs and VLMs\n[21], but reasons from text descriptions, while we reason\nfrom real images. OpenScene uses CLIP [22] to identify\nobjects in scenes using properties such as material and\nfragility, but these results are only qualitative in nature [14].\nIn our work, we propose PHYSOBJECTS to better quantify\nand improve object-centric physical reasoning, and leverage\nthis reasoning for robotic manipulation.\nObject Attribute Datasets. There have been prior visual\nobject attribute datasets with concepts included in PHYSOB-\nJECTS, such as material and transparency [23]\u2013[26]. How-\never, they focus more on visual attributes such as color,\nwhile we focus on physical concepts. Physics 101 provides\na dataset of object interaction videos and property measure-\nments [16], but PHYSOBJECTS includes a greater variety of\nobjects that are more relevant for household robotics.\nVision-Language Models. VLMs have made large improve-\nments on multi-modal tasks such as VQA, by leveraging\ninternet-scale image and text data [8]\u2013[10], [12]. In our\nexperiments, we use InstructBLIP [11] as our base VLM\nfor fine-tuning and comparison, as it was the state-of-the-art\nopen-source VLM at the time of our experiments. PaLM-E\nhas shown strong performance on general visual-language\ntasks and robotic planning [13], but there has not been\nfocused evaluation of it for physical reasoning. SuccessVQA\nfine-tunes VLMs on human data for success detection by\ntreating it as a VQA task, and achieves better generalization\nthan models designed specifically for success detection [27].\nWe similarly fine-tune VLMs on human data for physical\nreasoning by casting it as a VQA problem, to benefit from\nthe generalization abilities and versatility of VLMs.\nLLMs for Robotic Planning. Many recent works have\nused LLMs as robotic planners. SayCan uses visual value\nfunctions to provide affordances for grounding [2], but does\nnot benefit from VLMs. Follow-up works have used VLMs\nfor grounding LLM planners through object detection, or\nproviding feedback about what has happened (e.g., success\ndetection) [3]\u2013[7]. Our work focuses on expanding the use of\nVLMs for grounding through physical reasoning, to let LLM-\nbased planners perform tasks that require a deeper physical\nunderstanding of the world.\nLLM/VLM Interaction. Our planning evaluation falls in\nthe framework of Socratic Models [28], where large models\ninteract with each other through text to perform tasks such\nas VQA [29], [30] and image captioning [31]. Most similar\nto our evaluation is Matcha, where an LLM receives a task\ninstruction, obtains object-centric feedback from its environ-\nment, and uses this for task planning [32]. However, this\nwork does not focus on visual feedback, as their evaluation\nis in a simulated environment where physical concepts are\nnot visually observable. In contrast, we focus on physical\nreasoning from vision in real-world scenes.\nIII. PHYSOBJECTS DATASET\nTo benchmark and improve VLMs for object-centric phys-\nical reasoning, we propose PHYSOBJECTS, a dataset of\n39.6K crowd-sourced and 417K automated physical concept\nannotations for images of real household objects.\nImage Source. We use the publicly released challenge\nversion of the EgoObjects dataset [33] as our image source.\nTo our knowledge, this was the largest object-centric dataset\nof real images that was publicly released when constructing\nPHYSOBJECTS. The dataset consists of frames from egocen-\ntric videos in realistic household settings, which makes it par-\nticularly relevant for household robotics. It includes 117,424\nimages, 225,466 object bounding boxes with corresponding\ncategory labels from 277 object categories, and 4,203 object\ninstance IDs. PHYSOBJECTS consists of physical concept\nannotations for a large subset of this image data. 1\nWe construct random training, validation, and test sets\nbased on object instance IDs. We split the dataset per object\ncategory to ensure each object category is represented in\neach set when possible. Our training, validation, and test sets\nconsist of 73.0%, 14.8%, and 12.2% of objects, respectively.\nConcept\nDescription\nMass\nhow heavy an object is\nFragility\nhow easily an object can be broken/damaged\nDeformability\nhow easily an object can change shape without breaking\nMaterial\nwhat an object is primarily made of\nTransparency\nhow much can be seen through an object\nContents\nwhat is inside a container\nCan Contain Liquid\nif a container can be used to easily carry liquid\nIs Sealed\nif a container will not spill if rotated\nDensity (held-out)\nhow much mass per unit of volume of an object\nLiquid Capacity (held-out)\nhow much liquid a container can contain\nTABLE I: Our physical concepts and brief descriptions\nPhysical Concepts. We collect annotations for eight main\nphysical concepts and two additional concepts reserved for\nheld-out evaluation. We select concepts based on prior work\nand what we believe to be useful for robotic manipulation,\nbut do not consider all such concepts. For example, we do not\ninclude friction because this can be challenging to estimate\nwithout interaction, and we do not include volume because\nthis requires geometric reasoning, which we do not focus on.\nOf our main concepts, three are continuous-valued and\napplicable to all objects: mass, fragility, and deformability.\nTwo are also applicable to all objects, but are categorical:\nmaterial and transparency. Transparency could be consid-\nered continuous, but we use discrete values of transparent,\ntranslucent, and opaque. The other three are categorical and\napplicable only to container objects: contents, can contain\nliquid, and is sealed. We define which object categories are\ncontainers, resulting in 956 container object instances.\nOur two held-out concepts are density, which is continuous\nand applicable to all objects, and liquid capacity, which is\ncontinuous and applicable only to containers. We only collect\ntest data for these held-out concepts. We list all concepts and\ntheir brief descriptions in Table I.\nFor categorical concepts, we define a set of labels for each\nconcept. Annotations consist of a label specified for a given\nobject and concept. For the concepts material and contents,\nwhen crowd-sourcing, we allow for open-ended labels if\nnone of the pre-defined labels are applicable.\n1We publicly release our dataset on our website. Because the EgoObjects\nlicense does not permit incorporating it into another dataset, we release our\nannotations separately from the image data.\nFor continuous concepts, annotations are preference pairs,\nwhere given two objects, an annotation indicates that either\none object has a higher level of a concept, the objects have\nroughly equal levels, or the relationship is unclear. We use\npreferences because it is generally more intuitive for humans\nto provide comparisons than continuous values [34], [35].\nThis is especially true when annotating static images with\nphysical concepts, where it is difficult to specify precise\ngrounded values. For example, it would be difficult to specify\nthe deformability of a sponge as a value out of 10. Compar-\nisons have also been used to evaluate LLMs and VLMs for\nphysical reasoning in prior work [21]. Therefore, the kind of\ngrounding studied in PHYSOBJECTS for continuous concepts\nis only relational in nature.\nAutomatic Annotations. Before crowd-sourcing, we first\nattempt to automate as many annotations as possible, so that\ncrowd-workers only annotate examples that cannot be easily\nautomated. For categorical concepts, we assign concept val-\nues to some of the defined object categories in EgoObjects,\nsuch that all objects in a category are labeled with that value.\nFor continuous concepts, we define high and low tiers for\neach concept, such that all objects from a high tier category\nhave a higher level of that concept than all objects from a\nlow tier category. Then, we automate preference annotations\nfor all object pairs between the two tiers.\nFig. 2: Annotation UI for fragility. Here, the label is right,\ni.e., the water glass is more fragile than the house/car key.\nCrowd-Sourcing Annotations. We obtain additional an-\nnotations via crowd-sourcing, using 573 crowd-workers on\nthe Prolific platform. Crowd-workers use a web-based user\ninterface (example for fragility shown in Fig. 2) where they\nare presented with object bounding boxes in the context\nof their overall image, and provide annotations using on-\nscreen buttons or their keyboard. For categorical concepts,\nwe collect annotations for the majority of objects that\nwere not automatically annotated. For continuous concepts,\nbecause it is impractical to annotate every pair of objects\nin the dataset, we randomly sample pairs to annotate. We\nenforce that 20% of the sampled pairs are between objects\nof the same category, to prioritize understanding differences\nbetween objects of the same category. We collect annotations\nfrom three crowd-workers for each example. To promote\nhigh-quality data, we include attention checks as 10% of\nprovided examples, which have known labels, and only keep\ndata from annotators that achieve 80% accuracy on these.\nMost Common\nText Only\nInstructBLIP\nSingle Concept FT (ours)\nPG-InstructBLIP (ours)\nMass\n42.2\n73.3\n62.2\n80.0\n80.0\nFragility\n64.9\n64.9\n78.4\n91.2\n94.6\nDeformability\n46.5\n62.8\n67.4\n95.3\n93.0\nMaterial\n37.1\n73.9\n67.1\n83.7\n84.6\nTransparency\n77.6\n82.2\n85.8\n89.4\n90.1\nContents\n39.5\n50.9\n35.1\n81.6\n83.3\nCan Contain Liquid\n56.3\n92.2\n59.4\n84.4\n87.5\nIs Sealed\n80.6\n80.6\n74.2\n80.6\n87.1\nAverage\n55.6\n72.6\n66.2\n85.8\n87.5\nTABLE II: Test accuracy for main concepts on crowd-sourced PHYSOBJECTS\nDataset Statistics. We crowd-source 39.6K annotations for\n13.2K examples, and automate annotations for 417K addi-\ntional examples. For crowd-sourced annotations, 93.7% of\nexamples have at least 2/3 annotator label agreement, and\n58.1% have unanimous agreement.\nIV. PHYSICALLY GROUNDING VISION-LANGUAGE\nMODELS\nFine-Tuning VLMs. We work with the FlanT5-XXL [36]\nversion of InstructBLIP [11]. InstructBLIP takes as input\na single RGB image and text prompt, and predicts text as\noutput. In our setup, we choose the model inputs to be a\nsingle bounding box of an object, and a question text prompt\ncorresponding to each concept.\nLearning From Preferences. Learning for categorical con-\ncepts amounts to maximum likelihood of annotated labels.\nHowever, it is not as straightforward to train a VLM on pref-\nerences for continuous concepts, because preference learning\nrequires a continuous score. To do this with VLMs, which\nnaturally have discrete text outputs, we prompt the VLM with\nquestions that can be answered with yes or no for continuous\nconcepts. Then, we extract the following score function:\ns(o, c) = p(yes | o, c)\np(no | o, c)\nwhere o is an object bounding box image, c is a concept, and\np(\u00b7|o, c) is the likelihood under the VLM of text, conditioned\non the object image and concept. We use this as our score\nfunction because it can take any non-negative value, and\nlog s(o, c) has the intuitive interpretation as the difference\nof log-likelihoods between yes and no.2 We then use the\nBradley-Terry model [37] to estimate the probability of a\nhuman indicating that object o1 has a higher value than object\no2 for concept c as:\nP(o1 > o2 | c) =\ns(o1, c)\ns(o1, c) + s(o2, c).\nWe\nassume\na\ndataset\nD\nof\npreference\nannotations\n(o1, o2, c, y), where y \u2208 {[1, 0], [0, 1], [0.5, 0.5]} corresponds\n2We experimented with other choices of score functions, and found that\nwhile all performed similarly with respect to test accuracy on PHYSOB-\nJECTS, we found this score function to produce the most interpretable range\nof likelihoods for different responses, which we hypothesize to be beneficial\nfor downstream planning.\nto if o1 is preferred, o2 is preferred, or if they are indicated\nto be equal. We then fine-tune the VLM by minimizing the\nfollowing objective:\nL(D) = \u2212 E(o1,o2,c,y)\u223cD[y1 log P(o1 > o2 | c)\n+ y2 log(1 \u2212 P(o1 > o2 | c)].\nIn practice, this is the binary cross-entropy objective where\nthe logits for each object image o is the difference of log-\nlikelihoods log s(o, c) = log p(yes | o, c) \u2212 log p(no | o, c).\nV. EXPERIMENTAL RESULTS\nWe evaluate VLMs for physical reasoning using 1) test\naccuracy on PHYSOBJECTS, 2) planning accuracy on real\nscenes for physical reasoning tasks, and 3) task success rate\non a real robot.\nA. Dataset Evaluation\nWe refer to InstructBLIP fine-tuned on all main concepts\nin PHYSOBJECTS as Physically Grounded InstructBLIP, or\nPG-InstructBLIP.\n3 We focus our evaluation on crowd-\nsourced examples, because as described in Section III, these\nwere collected with the intent for their labels to not be\ndiscernible from object category information alone, and thus\nthey are generally more challenging. We report test accuracy\non these examples in Table II. Our baselines include Most\nCommon, where the most common label in the training data\nis predicted, Text Only, where an LLM makes predictions\nusing in-context examples from PHYSOBJECTS, but using\nobject category labels instead of images, and InstructBLIP.\nWe also compare to versions of InstructBLIP fine-tuned on\nsingle concept data. We find that PG-InstructBLIP outper-\nforms InstructBLIP on all concepts, with the largest improve-\nment on contents, which InstructBLIP has the most difficulty\nwith. We also find that PG-InstructBLIP performs slightly\nbetter than the single concept models, suggesting possible\npositive transfer from using a single general-purpose model\ncompared to separate task-specific models, although we ac-\nknowledge the improvement here is not extremely significant.\nPG-InstructBLIP also generally outperforms Most Common\nand Text Only, suggesting that our evaluation benefits from\nreasoning beyond dataset statistics, and from using vision.\n3We release the model weights for PG-InstructBLIP on our website.\nInstruct-\nBLIP\nPG-InstructBLIP\n(ours)\nDensity\n54.2\n70.3\nLiquid Capacity\n65.4\n73.0\nAverage\n59.8\n71.7\nTABLE III: Test accuracy for held-out concepts on\ncrowd-sourced PHYSOBJECTS\nGeneralization Results. We additionally evaluate both In-\nstructBLIP and PG-InstructBLIP on test data for our held-\nout concepts, which we report in Table III. We find that\nPG-InstructBLIP improves upon InstructBLIP by 11.9%,\ndespite having never seen these evaluated concepts nor object\ninstances during fine-tuning. We believe this suggests that\nfine-tuning VLMs can offer possible generalization benefits\nto concepts that are related to those seen during fine-tuning.\nInstruct-\nBLIP\nPG-InstructBLIP\n(ours)\nMass\n55.6\n82.2\nFragility\n70.3\n83.8\nDeformability\n76.7\n88.4\nMaterial\n67.7\n83.4\nTransparency\n81.5\n83.8\nContents\n32.5\n81.6\nCan Contain Liquid\n56.3\n89.1\nIs Sealed\n71.0\n80.6\nAverage\n64.0\n84.1\nTABLE IV: Test accuracy for main concepts with\nparaphrased prompts\nIn Table IV, we report results for main concepts on unseen\nparaphrased question prompts. We find that PG-InstructBLIP\nstill outperforms InstructBLIP, with limited degradation from\nthe original prompts, suggesting robustness to question vari-\nety from using a large pre-trained VLM.\n10\n50\n100\n% Training Data\n65\n70\n75\n80\n85\n90\n% Accuracy\nPhysObjects Scaling\nInstructBLIP\nPG-InstructBLIP (ours)\nFig. 3: Performance scaling\nwith dataset size\nDataset Scaling. In Fig. 3,\nwe illustrate how average\nperformance\nscales\nwith\ndataset size, by fine-tuning\non different fractions of\ndata from PHYSOBJECTS.\nPerformance scales posi-\ntively, but the models still\nbenefit significantly from\nonly 10% of our dataset,\nsuggesting that the phys-\nical reasoning of VLMs\ncan be improved with rel-\natively small amounts of\nannotated data.\nAdditional Results. We include additional results in our Ap-\npendix (found on our website). These include showing that\nPG-InstructBLIP has limited degradation on general VQA\nbenchmarks compared to InstructBLIP, suggesting that ex-\nisting systems using VLMs can benefit from PHYSOBJECTS\nfor physical reasoning, without sacrificing other reasoning\nabilities. We also include results using different question\nprompts, using a smaller version of InstructBLIP, evaluating\non automatically annotated data, transfer to held-out con-\ncepts, and ablations on our fine-tuning process.\nB. Real Scene Planning Evaluation\nNext, we evaluate the efficacy of PG-InstructBLIP for\nrobotic planning on unseen images of real scenes. We provide\nan example scene in Fig. 4. We evaluate on tasks with\nlanguage instructions, and assume a library of primitive\nrobotic operations with language descriptions.\nFig. 4: Example scene in our\nplanning evaluation\nPlanning\nFramework.\nThe\nLLM\nused\nin\nour\nplanning\nframework\nis\nGPT-4\n[38].\nIt\nis\nfirst\ngiven\nobject\ndetections\nin the scene, a list of\nprimitives, and the task\ninstruction, and then asks\na VLM questions about\nobjects in the scene. There\nare no constraints on the\nquestions. Afterwards, the\nLLM either indicates the task is not possible, or produces a\nplan consisting of primitives to execute.\nTask Category\nNo\nVLM\nInstruct-\nBLIP\nPG-InstructBLIP\n(ours)\nSingle Concept\n36.8\n68.4\n84.1\nMulti-Concept\n27.8\n27.8\n94.4\nCommon Knowledge\n35.7\n78.6\n85.7\nOverall\n33.3\n56.9\n88.2\nTABLE V: Task plan accuracy on 51 real scenarios\nResults. We report task planning accuracy using Instruct-\nBLIP and PG-InstructBLIP in Table V. We also compare to\na planner that does not use VLM interaction for grounding.\nWe evaluate on 51 task scenarios across 8 scenes, using\na non-author human to evaluate task plans. We divide our\ntask scenarios into three categories. Single Concept requires\nidentifying objects using one physical concept, e.g., finding\nthe heaviest object. Multi-Concept requires reasoning about\nmultiple physical concepts, e.g., asking for a metal container\nthat can hold water. This may include concepts outside\nof PHYSOBJECTS. Common Knowledge requires additional\nreasoning about common knowledge of objects, e.g., un-\nderstanding the label of a container. While our tasks focus\non physical concepts in PHYSOBJECTS, the LLM can ask\nquestions about other concepts that may also be useful,\nparticularly for Common Knowledge tasks.\nPG-InstructBLIP outperforms InstructBLIP on all task\ncategories, especially Multi-Concept. It does slightly better\non Common Knowledge, suggesting that it can reason about\nnon-PHYSOBJECTS concepts at a similar level as Instruct-\nBLIP. Using no VLM performs substantially worse than\nusing VLM interaction, indicating that our tasks require\nadditional grounding beyond object detection. We provide\nfurther details of results on our website.\nC. Real Robot Evaluation\nLastly, we evaluate plans on real scenes using a Franka\nEmika Panda robot. We use a similar planner as in the\nprevious section, but with different prompts and primitives.\nWe assume a library of primitives for pick-and-place tasks.\nWe evaluate on two scenes, with five tasks per scene, which\nwe provide in Table VI. We report success rates using\nInstructBLIP and PG-InstructBLIP in Table VII. We ensure\nthe primitives execute successfully, so our success rates only\nreflect plan quality.\nScene Image\nTask Instructions\n1) Move all objects that are not\nplastic to the side.\n2) Find a container that has met-\nals. Move all metal objects\ninto that container.\n3) Move all containers that can\nbe used to carry water to the\nside.\n4) Put the two objects with the\nleast mass into the least de-\nformable container.\n5) Move the most fragile object\nto the side.\n1) Put all containers that can\nhold water to the side.\n2) Put all objects that are not\nplastic to the side.\n3) Put\nall\nobjects\nthat\nare\ntranslucent to the side.\n4) Put the three heaviest objects\nto the side.\n5) Put a plastic object that is not\na container into a plastic con-\ntainer. Choose the container\nthat you are most certain is\nplastic.\nTABLE VI: Scene images and task instructions for our real\nrobot evaluation\nWe find that using PG-InstructBLIP leads to successful\nrobot executions more often than InstructBLIP. For example,\nwhen asked \u201cIs this object not plastic?\u201d about the ceramic\nbowl in Fig. 5a, InstructBLIP incorrectly assigns a likeli-\nhood of 0.89 to yes, while PG-InstructBLIP only assigns\n0.18. However, when asked \u201cIs this object translucent?\u201d\nabout the glass jar in Fig. 5b, both InstructBLIP and PG-\nInstructBLIP incorrectly assign likelihoods of 0.95 and 0.91\nto yes, respectively. We note that while these questions relate\nto physical concepts in PHYSOBJECTS, neither are formatted\nlike the training questions for PG-InstructBLIP. For example,\nthe training prompt for transparency was \u201cIs this object\ntransparent, translucent, or opaque?\u201d. This suggests that\ndespite using a large pre-trained VLM, PG-InstructBLIP may\nsometimes still fail due to out-of-distribution questions. We\nprovide more results and visualizations on our website.\nInstruct-\nBLIP\nPG-InstructBLIP\n(ours)\nScene 1\n2/5\n5/5\nScene 2\n2/5\n4/5\nOverall\n4/10\n9/10\nTABLE VII: Success rates for real robot evaluation\n(a) Ceramic bowl\n(b) Glass jar\nFig. 5: Objects from our real robot evaluation\nVI. DISCUSSION\nSummary. In this work, we propose PHYSOBJECTS, the\nfirst large-scale dataset of physical concept annotations of\nreal household object images, and demonstrate that fine-\ntuning a VLM on it significantly improves its physical\nreasoning abilities, including on held-out physical concepts.\nWe find that using the fine-tuned VLM for real-world robotic\nplanning improves performance on tasks that require physical\nreasoning. We believe our work makes progress toward\nexpanding the applicability of VLMs for robotics.\nLimitations and Future Work. While we show PHYSOB-\nJECTS can improve the physical reasoning of a VLM, it\nstill makes errors relative to human judgment. Also, while\nour proposed methodology for continuous concepts improves\nrelational grounding, which we show can be useful for\nrobotic planning, the model outputs are not grounded in\nreal physical quantities, which would be needed for some\napplications, e.g., identifying if an object is too heavy to be\npicked up. Future work can investigate incorporating data\nwith real physical measurements to improve grounding.\nWhile we believe the physical concepts in this work to\nhave broad relevance for robotics, future work can expand on\nthese for greater downstream applications. This could include\nexpanding beyond physical reasoning, such as geometric\nreasoning (e.g., whether an object can fit inside a container),\nor social reasoning (e.g., what is acceptable to move off a\ntable for cleaning). We believe our dataset is a first step\ntowards this direction of using VLMs for more sophisticated\nreasoning in robotics.\nACKNOWLEDGMENTS\nThis work was supported by NSF Awards 2132847,\n1941722, and 2338203, ONR N00014-23-1-2355 and YIP,\nDARPA YFA, and Ford. We thank Minae Kwon, Siddharth\nKaramcheti, Suvir Mirchandani, and other ILIAD lab mem-\nbers for helpful discussions and feedback, and Siddharth\nKaramcheti for helping to set up the real robot evaluation.\nREFERENCES\n[1] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.\nLanguage models as zero-shot planners: Extracting actionable knowl-\nedge for embodied agents. In International Conference on Machine\nLearning, pages 9118\u20139147. PMLR, 2022.\n[2] Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol\nHausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan,\nEric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu,\nCarolina Parada, Kanishka Rao, Pierre Sermanet, Alexander T Toshev,\nVincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan,\nNoah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton\nTan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao,\nPeter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jes-\nmonth, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana\nGopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. Do\nas i can, not as i say: Grounding language in robotic affordances. In\n6th Annual Conference on Robot Learning, 2022.\n[3] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete\nFlorence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen\nChebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu,\nSergey Levine, Karol Hausman, and Brian Ichter. Inner monologue:\nEmbodied reasoning through planning with language models. In 6th\nAnnual Conference on Robot Learning, 2022.\n[4] Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana\nGopalakrishnan, Michael S Ryoo, Austin Stone, and Daniel Kap-\npler. Open-vocabulary queryable scene representations for real world\nplanning. In 2023 IEEE International Conference on Robotics and\nAutomation (ICRA), pages 11509\u201311522. IEEE, 2023.\n[5] Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao\nLu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman,\net al.\nGrounded decoding: Guiding text generation with grounded\nmodels for embodied agents. Advances in Neural Information Pro-\ncessing Systems, 36, 2023.\n[6] Satvik Sharma, Huang Huang, Kaushik Shivakumar, Lawrence Yun-\nliang Chen, Ryan Hoque, brian ichter, and Ken Goldberg. Semantic\nmechanical search with large vision and language models.\nIn 7th\nAnnual Conference on Robot Learning, 2023.\n[7] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng,\nShuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas\nFunkhouser.\nTidybot: Personalized robot assistance with large lan-\nguage models. Autonomous Robots, 2023.\n[8] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain\nBarr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican,\nMalcolm Reynolds, et al. Flamingo: a visual language model for few-\nshot learning. Advances in Neural Information Processing Systems,\n35:23716\u201323736, 2022.\n[9] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr\nPadlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil\nMustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan\nDing, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue,\nAshish V Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyed-\nhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme Ruiz,\nAndreas Peter Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby,\nand Radu Soricut.\nPaLI: A jointly-scaled multilingual language-\nimage model. In The Eleventh International Conference on Learning\nRepresentations, 2023.\n[10] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Boot-\nstrapping language-image pre-training with frozen image encoders and\nlarge language models.\nIn Proceedings of the 40th International\nConference on Machine Learning, pages 19730\u201319742, 2023.\n[11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao,\nWeisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.\nIn-\nstructBLIP: Towards general-purpose vision-language models with in-\nstruction tuning. In Thirty-seventh Conference on Neural Information\nProcessing Systems, 2023.\n[12] Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, and\nAnima Anandkumar. Prismer: A vision-language model with multi-\ntask experts. Transactions on Machine Learning Research, 2024.\n[13] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha\nChowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan\nVuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Ser-\nmanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol\nHausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,\nand Pete Florence.\nPaLM-e: An embodied multimodal language\nmodel.\nIn Proceedings of the 40th International Conference on\nMachine Learning, pages 8469\u20138488, 2023.\n[14] Songyou Peng, Kyle Genova, Chiyu \u201dMax\u201d Jiang, Andrea Tagliasac-\nchi, Marc Pollefeys, and Thomas Funkhouser. Openscene: 3d scene\nunderstanding with open vocabularies. In CVPR, 2023.\n[15] Jiajun Wu, Ilker Yildirim, Joseph J Lim, Bill Freeman, and Josh Tenen-\nbaum. Galileo: Perceiving physical object properties by integrating a\nphysics engine with deep learning. Advances in neural information\nprocessing systems, 28, 2015.\n[16] Jiajun Wu, Joseph J Lim, Hongyi Zhang, Joshua B Tenenbaum, and\nWilliam T Freeman. Physics 101: Learning physical object properties\nfrom unlabeled videos. In BMVC, volume 2, page 7, 2016.\n[17] Yunzhu Li, Toru Lin, Kexin Yi, Daniel Bear, Daniel L.K. Yamins,\nJiajun Wu, Joshua B. Tenenbaum, and Antonio Torralba.\nVisual\ngrounding of learned physical models. In ICML, 2020.\n[18] Michael Janner, Sergey Levine, William T. Freeman, Joshua B. Tenen-\nbaum, Chelsea Finn, and Jiajun Wu. Reasoning about physical inter-\nactions with object-oriented prediction and planning. In International\nConference on Learning Representations, 2019.\n[19] Zhenjia Xu, Jiajun Wu, Andy Zeng, Joshua B Tenenbaum, and Shuran\nSong. Densephysnet: Learning dense physical object representations\nvia multi-step dynamic interactions. In Robotics: Science and Systems\n(RSS), 2019.\n[20] Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush\nVosoughi, Claire Cui, Denny Zhou, and Andrew M. Dai.\nMind\u2019s\neye: Grounded language model reasoning through simulation. In The\nEleventh International Conference on Learning Representations, 2023.\n[21] Lei Li, Jingjing Xu, Qingxiu Dong, Ce Zheng, Xu Sun, Lingpeng\nKong, and Qi Liu. Can language models understand physical concepts?\nIn The 2023 Conference on Empirical Methods in Natural Language\nProcessing, 2023.\n[22] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel\nGoh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela\nMishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\ntransferable visual models from natural language supervision.\nIn\nICML, 2021.\n[23] Genevieve Patterson and James Hays. Coco attributes: Attributes for\npeople, animals, and objects. In Computer Vision\u2013ECCV 2016: 14th\nEuropean Conference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part VI 14, pages 85\u2013100. Springer, 2016.\n[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata,\nJoshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A\nShamma, et al.\nVisual genome: Connecting language and vision\nusing crowdsourced dense image annotations. International journal\nof computer vision, 123:32\u201373, 2017.\n[25] Khoi Pham, Kushal Kafle, Zhe Lin, Zhihong Ding, Scott Cohen, Quan\nTran, and Abhinav Shrivastava. Learning to predict visual attributes in\nthe wild. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 13018\u201313028, 2021.\n[26] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue\nZheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri,\nAbhishek Kadian, Amir Mousavi, Yiwen Song, Abhimanyu Dubey,\nand Dhruv Mahajan. Paco: Parts and attributes of common objects.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 7141\u20137151, June 2023.\n[27] Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica\nLandon, Felix Hill, Nando de Freitas, and Serkan Cabi.\nVision-\nlanguage models as success detectors.\nIn Proceedings of The 2nd\nConference on Lifelong Learning Agents, pages 120\u2013136, 2023.\n[28] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Marcin Choro-\nmanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek\nPurohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee, Vincent\nVanhoucke, and Pete Florence. Socratic models: Composing zero-shot\nmultimodal reasoning with language. In The Eleventh International\nConference on Learning Representations, 2023.\n[29] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu,\nZicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-\nshot knowledge-based vqa. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 36, pages 3081\u20133089, 2022.\n[30] Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large\nlanguage models with answer heuristics for knowledge-based visual\nquestion answering.\nIn Computer Vision and Pattern Recognition\n(CVPR), pages 14974\u201314983, 2023.\n[31] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan\nZhang, and Mohamed Elhoseiny.\nChatgpt asks, blip-2 answers:\nAutomatic questioning towards enriched visual descriptions.\narXiv\npreprint arXiv:2303.06594, 2023.\n[32] Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan\nHafez, and Stefan Wermter. Chat with the environment: Interactive\nmultimodal perception using large language models. arXiv preprint\narXiv:2303.08268, 2023.\n[33] Meta.\nEgoobjects dataset.\nhttps://ai.facebook.com/\ndatasets/egoobjects-dataset/, Last accessed on 2023-05-\n28.\n[34] Dorsa Sadigh, Anca D. Dragan, S. Shankar Sastry, and Sanjit A.\nSeshia.\nActive preference-based learning of reward functions.\nIn\nProceedings of Robotics: Science and Systems (RSS), July 2017.\n[35] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane\nLegg, and Dario Amodei. Deep reinforcement learning from human\npreferences. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems, volume 30. Curran Associates,\nInc., 2017.\n[36] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei\nYu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned\nlanguage models are zero-shot learners. In International Conference\non Learning Representations, 2022.\n[37] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete\nblock designs: I. the method of paired comparisons.\nBiometrika,\n39(3/4):324\u2013345, 1952.\n[38] OpenAI. Gpt-4 technical report, 2023.\n[39] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gor-\ndon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar,\nHongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi,\nand Ludwig Schmidt. Openclip. July 2021.\n[40] Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and\nSteven C. H. Hoi. Lavis: A library for language-vision intelligence,\n2022.\n[41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wain-\nwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina\nSlama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul\nChristiano, Jan Leike, and Ryan Lowe.\nTraining language models\nto follow instructions with human feedback. In Alice H. Oh, Alekh\nAgarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances\nin Neural Information Processing Systems, 2022.\n[42] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and\nDevi Parikh. Making the v in vqa matter: Elevating the role of image\nunderstanding in visual question answering.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages\n6904\u20136913, 2017.\n[43] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh\nMottaghi. Ok-vqa: A visual question answering benchmark requiring\nexternal knowledge. In Proceedings of the IEEE/cvf conference on\ncomputer vision and pattern recognition, pages 3195\u20133204, 2019.\n[44] Matthias Minderer, Alexey Gritsenko, Maxim Neumann Austin Stone,\nDirk Weissenborn, Aravindh Mahendran Alexey Dosovitskiy, Anurag\nArnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai,\nThomas Kipf, and Neil Houlsby.\nSimple open-vocabulary object\ndetection with vision transformers. ECCV, 2022.\n[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian\nichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou.\nChain\nof thought prompting elicits reasoning in large language models. In\nAlice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,\neditors, Advances in Neural Information Processing Systems, 2022.\n[46] Yixin Lin, Austin S. Wang, Giovanni Sutanto, Akshara Rai, and\nFranziska Meier.\nPolymetis.\nhttps://facebookresearch.\ngithub.io/fairo/polymetis/, 2021.\nAPPENDIX\nA. Physical Concepts Details\nIn this section, we provide details on how we define\neach of our ten physical concepts, which we communicate\nto crowd-workers before annotation. We also list the\npre-defined options for categorical concepts.\nContinuous-Valued, Applicable to All Objects\nMass: This refers to how heavy an object is. If an object has\ncontents inside, this includes how heavy both the object and\nits contents are combined.\nFragility: This refers to how easily an object can be broken\nor damaged. An object has higher fragility than another if a\nperson would handle it more carefully to avoid breaking it.\nDeformability: This refers to how easily an object can\nchange shape without breaking. An object has more deforma-\nbility than another if less force is needed to change its shape\nwithout breaking it.\nDensity (held-out): This refers to the amount of mass per\nunit of volume of the object. If an object has contents\ninside, this only refers to the object, not the contents.\nContinuous-Valued, Applicable to Containers\nLiquid Capacity (held-out): This refers to the volume of\nliquid a container can contain without spilling.\nCategorical-Valued, Applicable to All Objects\nMaterial: This refers to what an object is made of. If an\nobject is made of multiple materials, it refers to what material\nmakes up the largest portion of the object that is visible. This\ndoes not refer to the contents of a container. The pre-defined\noptions we include are plastic, glass, ceramic, metal, wood,\npaper, fabric, food, unknown, and other (annotator provides\nan open-ended response if this option is chosen).\nTransparency: This refers to how much can be seen\nthrough an object. The pre-defined options we include are\ntransparent, translucent, opaque, and unknown. Transparent\nrefers to an object that can be seen clearly through, almost\nas if it was not there. Translucent refers to an object where\nsome details can be seen through the object, but the details\nare not as clear as if it was transparent. Opaque refers to\nan object that cannot be seen through at all. This concept\nonly refers to the object itself, and not the contents of a\ncontainer. If different parts of an object have different levels\nof transparency, it refers what level applies to the largest\nvisible portion of the object.\nCategorical-Valued, Applicable to Containers\nContents: This refers to the contents of a container that are\nclearly visible and identifiable. The pre-defined options we\ninclude are nothing, water, food, oil, soap, unknown, and\nother (annotator provides an open-ended response if this\noption is chosen).\nCan Contain Liquid: This refers to if a container can be\nused to transport a liquid across a room without a person\nneeding to be particularly careful about not spilling it. The\npre-defined options we include are yes, no, and unknown.\nIs Sealed: This refers to if a container can be rotated by\nany amount in any direction without spilling its contents.\nThe pre-defined options we include are yes, no, and unknown.\nContainer Categories. We define the following object cat-\negories from EgoObjects as containers: bottle, container,\nplate, bowl, mug, water glass, measuring cup, wine glass, tea\ncup, frying pan, flowerpot, tin can, kettle, vase, coffee cup,\nmixing bowl, saucer, jug, serving tray, pitcher (container),\nand picnic basket.\nB. Automatic Annotation Details\nWe list the object categories we assign to high and low\ntiers for automating preference pair annotations for continu-\nous concepts in Table VIII. We list the object categories for\nwhich we assign a concept label in Table IX. If a concept\nis not listed in these tables, we do not provide automatic\nannotations for that concept.\nWe originally assigned the label metal for material to\nthe object category house/car key, but realized after crowd-\nsourcing that not all instances of this category should have\nbeen given this assignment. Therefore, we manually labeled\nthese examples for material, but still considered these to be\nautomatic annotations for dataset purposes.\nC. Crowd-Sourcing Details\nChoosing Annotation Images. There are multiple bounding\nbox images in EgoObjects for each object instance. To deter-\nmine which to present for annotating an object, we choose\nthe bounding box with the highest CLIP [22] similarity with\nthe object\u2019s category label, as a heuristic for the object\u2019s\nvisibility. We use the CLIP-ViT-H-14-laion2B-s32B-b79K\nmodel from OpenCLIP [39]. In Fig. 6, we show an example\nof randomly sampled bounding boxes for an instance of\nthe object category guitar, arranged from left-to-right in\ndecreasing order of CLIP similarity. The objects in bounding\nboxes with lower CLIP similarities tend to be less visible.\nFig. 6: Bounding boxes for an instance of guitar, in\ndecreasing order of CLIP similarity\nAttention Checks. We generate attention checks for crowd-\nworkers by randomly sampling from the automatic annota-\ntions, which have known labels. For the concepts contents,\ndensity, and liquid capacity, for which there are no automatic\nannotations, we manually label a small set of objects for\nattention checks.\nOther Details. Each annotation job on Prolific consisted\nof 250 annotations for a single concept, of which 25 are\nattention checks. Participants were paid an average of 15.50\nUS dollars per hour, and each annotation job took on average\n20-30 minutes to complete, depending on the concept.\nConcept\nHigh\nLow\nMass\ntelevision, microwave oven, table,\nnightstand, chest of drawers\npen, paper, spoon, fork, glasses,\nsunglasses, scissors, watch, neck-\nlace, house/car key, pencil, earrings,\nring, screwdriver, book, container,\nplate, bowl, pillow, remote control,\nclothing, mug, laptop, knife, mobile\nphone, toy, computer mouse, wa-\nter glass, towel, headphones, spatula,\nfrying pan, measuring cup, banana,\nwallet, blanket, candle, apple, wine\nglass, picture frame, computer key-\nboard, game controller/pad, tea cup,\ntin can, handbag, whisk, orange, belt,\nplastic bag, salt and pepper shak-\ners, cutting board, perfume, stapler,\nfootwear, tablet coputer, teddy bear,\ncookie, scarf, coffee cup, ball, mix-\ning bowl, pear, alarm clock, light\nswitch, bread, jacket, tennis ball, san-\ndal, saucer, laptop charger, camera,\nyoga mat, power plugs and sock-\nets, cream, shirt, baseball bat, sun\nhat, paper towel, kitchen knife, doll,\ncan opener, sock, facial tissue holder,\nboot, hair dryer\nFragility\nwater glass, television\nhouse/car\nkey,\ndumbbell,\nscrew-\ndriver, kitchen knife\nDeformability\npillow, clothing, towel, blanket, belt,\nplastic bag, scarf, jacket, yoga mat,\nshirt, paper towel, sock\nremote control, mug, mobile phone,\ncomputer mouse, water glass, fry-\ning pan, flowerpot, scissors, wine\nglass, house/car key, dumbbell, cut-\nting board, microwave oven, toaster,\nblender,\npressure\ncooker,\nkitchen\nknife, table, spoon, laptop, knife,\nfork, glasses, spatula, sunglasses,\nchair, measuring cup, pencil, picture\nframe,\ncomputer\nkeyboard,\ngame\ncontroller/pad, tea cup, tin can, salt\nand pepper shakers, television, cof-\nfeemaker, stapler, tablet computer,\nkettle, vase, coffee cup, mixing bowl,\ncomputer monitor, stool, ring, alarm\nclock, light switch, saucer, printer,\nscrewdriver, guitar, camera, jug, gas\nstove, baseball bat, humidifier, chest\nof drawers, sink, can opener, night-\nstand, hair dryer\nTABLE VIII: Object category assignments to high and low tiers for continuous concepts\nConcept\nLabel\nCategories\nMaterial\nPlastic\nremote control, computer mouse, computer keyboard,\ngame controller/pad, plastic bag\nGlass\nwater glass, wine glass\nMetal\ntin can, kitchen knife, can opener\nPaper\nbook, paper, paper towel\nFabric\nclothing, towel, blanket, scarf, sock\nFood\nbanana, apple, orange, cookie, pear, bread\nTransparency\nTransparent\nwine glass\nOpaque\nbook, pillow, remote control, clothing, laptop, mobile\nphone, towel, headphones, spatula, chair, frying pan,\nbanana, wallet, flowerpot, scissors, apple, houseplant,\nhouse/car key, pencil, computer keyboard, tin can,\nwhisk, dumbbell, orange, belt, cutting board, toaster,\nteddy bear, tablet computer, cookie, pear, computer mon-\nitor, stool, light switch, bread, pressure cooker, scarf,\nlaptop charger, guitar, camera, yoga mat, shirt, baseball\nbat, paper towel, kitchen knife, sink, chest of drawers,\ncan opener, boot, nightstand, hair dryer\nCan Contain Liquid\nYes\nbottle, mug, water glass, measuring cup, wine glass,\ntea cup, kettle, coffee cup, mixing bowl, jug, pitcher\n(container), tin can\nNo\npicnic basket, serving tray\nIs Sealed\nNo\nplate, bowl, mug, water glass, measuring cup, wine\nglass, tea cup, frying pan, flowerpot, kettle, vase, coffee\ncup, mixing bowl, saucer, jug, serving tray, pitcher\n(container), picnic basket\nTABLE IX: Concept label assignments of object categories for categorical concepts\nIn the annotation user interface, for each object example,\nthe object is shown in the context of its surrounding scene,\nwith the object indicated by its bounding box. We also\nprovide the object\u2019s category label to help clarify which\nobject is to be annotated. Crowd-workers can choose an\nannotation label by clicking on an associated button, or\ntyping an associated keyboard key. We also provide a back\noption to go to the previous example to correct mistakes.\nFor the concepts material and contents, the user may choose\nother as an option, whereupon they are presented with a text\nbox to type an open-ended label. We do not annotate objects\nfrom the categories pet, cat, and dog, to omit objects that\nare living.\nWe provide instructions to annotators that are specific to\neach concept, to encourage annotations that agree with our\nconcept definitions. We provide an image of the instruction\npage provided to annotators for the fragility concept, which\nalso includes an example of the annotation user interface, in\nFig. 7. The instructions for how to annotate each property are\nalso repeated at the bottom of the annotation user interface.\nWe detail the number of examples per concept and dataset\nsplit for PHYSOBJECTS in Table X. This is before any\npreprocessing of the data for annotator agreement or labels.\nFor the crowd-sourced data, the count refers to the number of\nexamples, not the number of annotations, for which there are\nthree times as many. We also provide the percent of crowd-\nsourced examples with majority agreement (at least 2/3) and\nunanimous agreement per concept in Table XI.\nD. Training Details\nHyperparameters. We provide hyperparameters used for\nfine-tuning InstructBLIP in Table XII. These hyperparam-\neters are largely derived from those proposed for fine-tuning\nBLIP-2 [10]. When fine-tuning, we only update the Q-\nFormer parameters, as done during instruction tuning for\nInstructBLIP. We use a linear warmup of the learning rate,\nfollowed by a linear decay with a minimum learning rate\nof 0. We fine-tune using mixed precision bfloat16 training.\nWe use a prompt template for questions, which is used\nboth during training and inference. We load the InstructBLIP\nmodel using the LAVIS library [40]. We train and evaluate\nusing the evaluation image processor provided by LAVIS, as\nwe do not use image data augmentation.\nValidation & Data Filtering. For most experiments, we\nevaluate on validation data every 250 gradient steps and\nchoose the checkpoint with the lowest validation loss. For\nexperiments fine-tuning for a single concept, we validate\nevery 100 gradient steps. Our validation set consists of all\nvalidation data for all concepts without balancing, except\nwe limit the number of automatically generated examples\nfor mass and deformability to 100. For validation data, we\nonly use the bounding box image with the highest CLIP\nobject category similarity score for each object, which for\nFig. 7: Instruction page for the fragility concept\nConcept\nSource\nTrain\nValidation\nTest\nMass\nCrowd-sourced\n2108\n86\n56\nAutomatic\n87269\n4536\n2688\nFragility\nCrowd-sourced\n2096\n99\n57\nAutomatic\n2397\n110\n80\nDeformability\nCrowd-sourced\n2101\n84\n65\nAutomatic\n293540\n13384\n9888\nMaterial\nCrowd-sourced\n2316\n460\n374\nAutomatic\n612\n130\n119\nTransparency\nCrowd-sourced\n1993\n394\n313\nAutomatic\n1046\n224\n194\nContents\nCrowd-sourced\n641\n134\n125\nAutomatic\n0\n0\n0\nCan Contain Liquid\nCrowd-sourced\n318\n68\n64\nAutomatic\n342\n70\n67\nIs Sealed\nCrowd-sourced\n164\n30\n31\nAutomatic\n444\n91\n86\nDensity (held-out)\nCrowd-sourced\n0\n0\n500\nLiquid Capacity (held-out)\nCrowd-sourced\n0\n0\n500\nTABLE X: Number of examples per concept and dataset split\nConcept\n% Majority Agreement\n% Unanimous Agreement\nMass\n94.2\n58.8\nFragility\n93.6\n53.1\nDeformability\n90.5\n48.1\nMaterial\n93.7\n59.4\nTransparency\n97.0\n72.5\nContents\n90.4\n49.8\nCan Contain Liquid\n99.3\n64.2\nIs Sealed\n98.2\n74.7\nDensity (held-out)\n93.3\n50.7\nLiquid Capacity (held-out)\n89.1\n46.0\nTABLE XI: Agreement among crowd-workers per concept\ncrowd-sourced data is also the bounding box image presented\nfor annotation. For crowd-sourced validation data, we filter\nour data to only include examples with at least 2/3 majority\nagreement among annotators, and only use the majority label.\nWe do not apply this filtering for training data. For preference\npair annotations, we remove data annotated with unclear.\nDataset Balancing. We construct sub-datasets for dataset\nbalancing purposes. For the categorical concepts except is\nsealed, we combine the crowd-sourced and automatically\nannotated data for each concept into one sub-dataset per\nconcept. For the other concepts, we keep separate sub-\ndatasets for crowd-sourced and automatically annotated data.\nWe keep separate sub-datasets for is sealed because for its\ncrowd-sourced data, we only train using the bounding box\nimage for the object that was presented for annotation, rather\nthan randomly sampling one of its bounding box images\n(as described in the below sub-section), as values for this\nconcept may change for the same object instance. We keep\nseparate datasets for the continuous concepts because there\nis a large imbalance between the number of crowd-sourced\nand automatically annotated examples for these concepts. To\nbalance these sub-datasets, we sample from each of them\nduring training at a rate proportional to the square root of\nthe number of annotations in the sub-dataset, as proposed in\nInstructBLIP for instruction tuning.\nAdditional Training Details. For most objects, each time\nwe sample one for training, we randomly sample one of its\nbounding box images as input to the model, as a form of\ndata augmentation. We do not do this with crowd-sourced\ndata for the contents and is sealed concepts, because labels\nfor these concepts may vary across different images of the\nsame object. Instead, we only use the bounding box image\nthat was presented for annotation.\nTo promote robustness to different queries to the VLM, we\ninclude object category labels in the question prompt for half\nof the training examples (e.g., asking \u201cIs this bottle heavy?\u201d),\nHyperparameter\nValue\nMax fine-tuning steps\n10000\nWarmup steps\n1000\nLearning rate\n1e-5\nBatch size\n128\nAdamW \u03b2\n(0.9, 0.999)\nWeight decay\n0.05\nImage resolution\n224\nPrompt template\nQuestion: {} Respond unknown if you are not sure. Short answer:\nTABLE XII: Hyperparameters for fine-tuning InstructBLIP\nConcept\nQuestion Prompt\nMass\nIs this object heavy?\nFragility\nIs this object fragile?\nDeformability\nIs this object deformable?\nMaterial\nWhat material is this object made of?\nTransparency\nIs this object transparent, translucent, or opaque?\nContents\nWhat is inside this container?\nCan Contain Liquid\nCan this container hold a liquid inside easily?\nIs Sealed\nIs this container sealed?\nDensity (held-out)\nIs this object dense?\nLiquid Capacity (held-out)\nCan this object hold a lot of liquid?\nTABLE XIII: Question prompts for each concept, without object category labels\nand omit this information in the other half (e.g., asking \u201cIs\nthis object heavy?\u201d). We experimented with training on one\nor multiple question prompts per concept, and found this\nto not significantly affect performance, so we only use one\nprompt per concept for simplicity. We include the question\nprompts for each concept in Table XIII. These are versions of\nthe prompts without object category labels. When including\ncategory labels, we replace either the word \u201cobject\u201d or\n\u201ccontainer\u201d with the object\u2019s category label from EgoObjects.\nWe also pluralize the prompt to have correct grammar if the\ncategory label is plural.\nWe experimented with removing Q-Former text condi-\ntioning in InstructBLIP while fine-tuning, and found this to\nimprove results on general VQA evaluation and evaluation\nwith held-out paraphrased question prompts, so we report\nresults using models trained without this text conditioning.\nIn our ablation results in Table XXI, we find that this does\nnot significantly change performance for our main crowd-\nsourced evaluation.\nE. Evaluation Details\nFurther PHYSOBJECTS Evaluation Details. For crowd-\nsourced test evaluation data, we only include examples with\nat least 2/3 annotator agreement, and use the majority label\nas ground-truth. For categorical concepts, we predict by\nchoosing the label with the highest likelihood out of all\nlabels in PHYSOBJECTS for the concept. For continuous\nconcepts, we predict the object in a pair with the higher\nscore from Section IV as the one with higher concept value.\nWe only evaluate on preference examples with a definite,\nnon-equal preference label. For the Most Common baseline\nwith continuous concepts, we also only include examples\nwith a definite, non-equal preference when determining the\nmost common label in the training data. We note that\nthat Most Common baseline is not particularly meaningful\nfor continuous concepts, because the preference labels and\npredictions are invariant to ordering in each preference pair.\nTherefore, a more natural baseline for these concepts would\nbe random guessing, which would achieve 50% accuracy.\nSimilarly as with validation data, for test data we only\nevaluate using the bounding box image with the highest\nCLIP object category similarity per object, which for crowd-\nsourced data is also the bounding box image presented for\nannotation. We evaluate using the same question prompts per\nconcept as during training, which are listed in Table XIII.\nUnless stated otherwise, we report evaluation results without\nobject category labels in the question prompt, because this\ngives slightly better results for the base InstructBLIP model.\nText Only Baseline. For this baseline, we use ground truth\nobject category labels from EgoObjects. We use the \u2018text-\ndavinci-003\u2019 InstructGPT model [41] as our LLM. For each\nconcept, we use 128 in-context examples randomly sampled\nfrom the training data in PHYSOBJECTS for that concept.\nBecause in-context learning is limited by context length, and\ntherefore it is desirable to use the best quality in-context\nexamples when possible, we first apply to the training data\nthe same majority filtering process used on crowd-sourced\ntest data as described in the previous subsection. We also\nremove preference annotations with the label unclear, as\nConcept\nQuestion Prompt\nMass\nDoes this object weigh a lot?\nFragility\nIs this object easily breakable?\nDeformability\nIs this object easily bendable?\nMaterial\nWhat is this object made of?\nTransparency\nWould you describe this object as opaque, transparent, or translucent?\nContents\nWhat does this container contain?\nCan Contain Liquid\nIs this container able to hold water inside easily?\nIs Sealed\nIs this container sealed shut?\nTABLE XIV: Paraphrased question prompts for main concepts, without object category labels\ndone in our VLM fine-tuning setup. We treat each example as\na question answering task, using question prompts for each\nconcept similar to those in Table XIII, but modified to refer\nto general object classes, rather than specific instances. We\nmake predictions by selecting the most likely completion of\nthe LLM conditioned on the in-context examples and test\nexample. For categorical concepts, we first include in the\nLLM context all possible labels in PHYSOBJECTS for the\nconcept. For continuous concepts, because we only evaluate\non examples with definite preferences, we restrict predictions\nto only definite preferences using logit bias, although the in-\ncontext examples may include equal as a possible answer.\nParaphrased Question Prompts. In Table XIV, we list the\nparaphrased prompts used in the evaluation for Table IV.\nInstructBLIP\nPG-InstructBLIP (ours)\nVQAv2\n71.4\n67.5\nOK-VQA\n52.4\n48.7\nTABLE XV: Accuracy on existing VQA benchmarks\nLimited VQA Degradation. Ideally, training on PHYSOB-\nJECTS should be done while co-training on other vision and\nlanguage datasets to preserve general reasoning abilities. In\nthis work, we do not do this because we focus primarily on\nphysical reasoning. However, we show that fine-tuning on\nonly PHYSOBJECTS does not significantly degrade general\nVQA performance. In Table XV, we compare InstructBLIP\nto PG-InstructBLIP on VQAv2 [42] and OK-VQA [43].\nThese results suggest that existing systems using VLMs can\nbenefit from PHYSOBJECTS for physical reasoning, without\nsacrificing other reasoning abilities.\nWe perform VQA evaluation using the LAVIS library,\nusing their configurations for evaluation of BLIP-2. Although\nPG-InstructBLIP is fine-tuned without Q-Former text condi-\ntioning, we found that Q-Former text conditioning during\nVQA evaluation improved performance, so we report these\nresults. We believe this is because InstructBLIP was instruc-\ntion tuned with this text conditioning. We also experimented\nwith VQA evaluation on PG-InstructBLIP fine-tuned with\nQ-Former text conditioning, but found this to have worse\nresults, possibly due to overfitting on our limited variety of\nquestion prompts. We believe these issues can be mitigated\nby co-training on PHYSOBJECTS in combination with other\nvision and language datasets, which we leave for future work.\nMotivated by these VQA results, for our planning evalua-\ntions we also evaluate PG-InstructBLIP using Q-Former text\nconditioning, to avoid possible degradation when answering\nquestions that do not pertain concepts in PHYSOBJECTS.\nWe verified that evaluating PG-InstructBLIP using Q-Former\ntext conditioning did not significantly affect test accuracy on\nPHYSOBJECTS.\nIncluding Object Category Labels in Question Prompts.\nWe generally report evaluation results without ground-truth\nobject category labels in the question prompt. In Table XVI,\nwe compare including object category labels or not, and find\nthat all models are not extremely sensitive to this.\nIncluding Concept Definitions in Question Prompts.\nWhile we did not spend extensive effort designing the\nquestion prompts for each concept (shown in Table XIII), we\naimed for them to be concise while still eliciting the desired\nconcept. As seen in Table XVIII, the base InstructBLIP\nmodel achieves above chance performance on all concepts,\nsuggesting that these prompts do elicit the desired concept\nto some extent. However, these prompts do not contain\nour definitions for each concept provided to annotators, as\ndescribed in Appendix A. We analyze whether including\nconcept definitions in the question prompt would improve\nbase VLM performance in Table XVIII, which contains our\noriginal crowd-sourced test accuracy results, with additional\nevaluation of the base InstructBLIP model using modified\nprompts that contain concept definitions, which we pro-\nvide in Table XVII. We find that while including concept\ndefinitions improves performance for some concepts (mass,\ndeformability, contents, can contain liquid), this still does\nnot match PG-InstructBLIP on these concepts, and overall\nperformance in fact decreases compared to the original\nprompts. We believe this could be because InstructBLIP\ndoes not have strong enough language understanding to\nproperly incorporate the concept definitions when providing\nresponses. For this reason, and for simplicity, we use prompts\nwithout concept definitions in the rest of our experiments.\nUsing a Smaller VLM. To analyze the effect of VLM size\non physical reasoning, in Table XIX we provide evaluation\nresults using the InstructBLIP version with the smaller Flan-\nT5 XL as its base LLM, compared to the Flan-T5 XXL\nversion used in all other experiments. We find that while\nthe smaller Flan-T5 XL version generally has worse base\nInstructBLIP\nSingle Concept FT (ours)\nPG-InstructBLIP (ours)\nCategory Labels\nYes\nNo\nYes\nNo\nYes\nNo\nMass\n60.0\n62.2\n84.4\n80.0\n80.0\n80.0\nFragility\n75.7\n78.4\n91.2\n91.2\n97.3\n94.6\nDeformability\n69.8\n67.4\n88.4\n95.3\n90.7\n93.0\nMaterial\n73.3\n67.1\n86.8\n83.7\n85.7\n84.6\nTransparency\n84.5\n85.8\n89.1\n89.4\n89.8\n90.1\nContents\n34.2\n35.1\n80.7\n81.6\n82.5\n83.3\nCan Contain Liquid\n57.8\n59.4\n84.4\n84.4\n82.8\n87.5\nIs Sealed\n71.0\n74.2\n80.6\n80.6\n87.1\n87.1\nAverage\n65.8\n66.2\n85.7\n85.8\n87.0\n87.5\nTABLE XVI: Test accuracy for main concepts on crowd-sourced PHYSOBJECTS, with and without object category labels\nConcept\nQuestion Prompt\nMass\nThe heaviness of an object refers to its mass. It includes the contents of the\nobject if it has something inside it. Is this object heavy?\nFragility\nFragility refers to how easily an object can be broken or damaged. Is this\nobject fragile?\nDeformability\nDeformability refers to how easily an object can change shape without\nbreaking. Is this object deformable?\nMaterial\nThe material of an object refers to what material makes up the largest portion\nof the object that is visible. It does not refer to the contents of a container.\nWhat material is this object made of?\nTransparency\nTransparency refers to how much can be seen through an object. A transparent\nobject can be clearly seen through, almost as if it was not there. A translucent\nobject can be seen through with some details, but not as clearly as if it was\ntransparent. An opaque object cannot be seen through at all. The transparency\nof an object does not refer to the transparency of its contents if it has anything\ninside it. Is this object transparent, translucent, or opaque? If different portions\nof the object have different levels of transparency, respond with the level that\napplies to the largest visible portion of the object.\nContents\nWhat is inside this container? Only respond with contents that are clearly\nvisible and identifiable.\nCan Contain Liquid\nA container can contain liquid if it can be used to transport a liquid across a\nroom without a person needing to be particularly careful about not spilling it.\nCan this container contain liquid?\nIs Sealed\nA container is sealed if it can be rotated by any amount in any direction\nwithout spilling its contents if it has anything inside. Is this container sealed?\nTABLE XVII: Question prompts with definitions for each main concept, without object category labels\nInstructBLIP\nPG-InstructBLIP (ours)\nPrompt Type\nOriginal\nw/ Concept Definitions\nOriginal\nMass\n62.2\n71.1\n80.0\nFragility\n78.4\n67.6\n94.6\nDeformability\n67.4\n69.8\n93.0\nMaterial\n67.1\n66.0\n84.6\nTransparency\n85.8\n65.3\n90.1\nContents\n35.1\n36.0\n83.3\nCan Contain Liquid\n59.4\n64.1\n87.5\nIs Sealed\n74.2\n64.5\n87.1\nAverage\n66.2\n63.0\n87.5\nTABLE XVIII: Test accuracy for main concepts on crowd-sourced PHYSOBJECTS, with additional base InstructBLIP\nevaluation on prompts with definitions for each concept\nInstructBLIP\nPG-InstructBLIP (ours)\nLLM Version\nFlan-T5 XL\nFlan-T5 XXL\nFlan-T5 XL\nFlan-T5 XXL\nMass\n62.2\n62.2\n82.2\n80.0\nFragility\n64.9\n78.4\n97.3\n94.6\nDeformability\n48.8\n67.4\n97.7\n93.0\nMaterial\n69.1\n67.1\n82.6\n84.6\nTransparency\n74.0\n85.8\n87.5\n90.1\nContents\n18.4\n35.1\n86.8\n83.3\nCan Contain Liquid\n68.8\n59.4\n87.5\n87.5\nIs Sealed\n67.7\n74.2\n80.6\n87.1\nAverage\n59.2\n66.2\n87.8\n87.5\nTABLE XIX: Test accuracy for main concepts on crowd-sourced PHYSOBJECTS, using different VLM versions\nperformance, after fine-tuning on PHYSOBJECTS, we see that\nperformance is comparable between the two model sizes.\nThis suggests that for physical reasoning, fine-tuning on\nhuman data such as PHYSOBJECTS could reduce the need for\nlarger model sizes. While fine-tuned evaluation performance\nis similar across model sizes, for simplicity of comparison,\nwe only report results using the larger Flan-T5 XXL models\nin all other experiments.\nInstructBLIP\nPG-InstructBLIP (ours)\nMass\n72.8\n99.9\nFragility\n95.0\n100\nDeformability\n96.0\n98.8\nMaterial\n89.1\n98.3\nTransparency\n97.4\n100\nCan Contain Liquid\n98.5\n100\nIs Sealed\n100\n100\nAverage\n92.7\n99.6\nTABLE XX: Test accuracy for main concepts on\nautomatically annotated PHYSOBJECTS\nResults on Automatically Annotated Data. We report eval-\nuation results on automatically annotated data in Table XX.\nPerformance is generally much higher on this data compared\nto the crowd-sourced data, because these are easier examples\nthat can be determined from object categories alone.\n10\n50\n100\n% Training Data\n65\n70\n75\n80\n85\n% Accuracy\nMass\n10\n50\n100\n% Training Data\n80\n85\n90\n95\n% Accuracy\nFragility\n10\n50\n100\n% Training Data\n70\n75\n80\n% Accuracy\nMaterial\nBase InstructBLIP\nw/o Other Char. Train\nw/ Other Char. Train\nFig. 8: Performance scaling on held-out concepts\nHeld-Out Concept Scaling. In these experiments, we eval-\nuate the transfer abilities of InstructBLIP across different\nconcepts when fine-tuning on PHYSOBJECTS. We fine-tune\nmodels on data from PHYSOBJECTS for all concepts ex-\ncept one, and then report results of additional fine-tuning\non the held-out concept. We compare to fine-tuning base\nInstructBLIP without training on the other concepts, and base\nInstructBLIP without any fine-tuning. Results for three con-\ncepts are shown in Fig. 8. We chose these concepts because\nwe believed they had the most generalization potential from\nthe other concepts. We find that there are some signs of\npositive transfer on mass and fragility, although we see slight\nnegative transfer on material. We believe that more positive\ntransfer could be attained by co-training with other vision\nand language datasets.\nAblations. We report additional ablation results on crowd-\nsourced PHYSOBJECTS examples in Table XXI. We list each\nablation below:\n1) No Auto Data: Instead of training on both crowd-\nsourced and automatically annotated data, we train on\nonly crowd-sourced data.\n2) Filtered: Instead of training on all annotations for\ncrowd-sourced data, we filter the data similarly as\nduring evaluation: we only include examples with at\nleast 2/3 annotator agreement, and use the majority label\nas ground-truth.\n3) Q-Former Text: Instead of removing Q-Former text\nconditioning during fine-tuning, we include it, as done\nfor the original InstructBLIP model.\n4) No Category Info: Instead of training on both question\nprompts with and without object category information,\nwe only train on question prompts without it.\n5) Only Category Info: Instead of training on both question\nprompts with and without object category information,\nwe only train on question prompts with it. Here, unlike\nthe rest of the evaluations, we evaluate with object\ncategory information to match the training setup.\nWe find that overall performance for each ablated version\nof our model does not change significantly, suggesting some\nrobustness of our fine-tuning process to different design\ndecisions. In particular, we find that including automatically\nPG-InstructBLIP (ours)\nNo Auto Data\nFiltered\nQ-Former Text\nNo Category Info\nOnly Category Info\nMass\n80.0\n84.4\n75.6\n80.0\n75.6\n77.8\nFragility\n94.6\n94.6\n97.3\n97.3\n100\n100\nDeformability\n93.0\n93.0\n90.7\n90.7\n93.0\n88.4\nMaterial\n84.6\n83.4\n83.4\n85.4\n84.6\n86.5\nTransparency\n90.1\n89.4\n89.1\n92.1\n89.8\n91.7\nContents\n83.3\n81.6\n85.1\n87.7\n84.2\n86.8\nCan Contain Liquid\n87.5\n89.1\n85.9\n84.4\n90.6\n84.4\nIs Sealed\n87.1\n83.9\n83.9\n71.0\n80.6\n87.1\nAverage\n87.5\n87.4\n86.4\n86.1\n87.3\n87.8\nTABLE XXI: Ablation results for main concepts on crowd-sourced PHYSOBJECTS\nannotated data does not significantly impact performance on\ncrowd-sourced data, which perhaps is not surprising because\nbase InstructBLIP already performs well on automatically\nannotated examples, as seen in Table XX. Only Category\nInfo very slightly improves upon PG-InstructBLIP, but uses\nprivileged object category information at evaluation time.\nF. Real Scene Planning Evaluation Details\nPlanning Framework. Our planning framework consists of\nfirst providing the scene image to an OWL-ViT ViT-L/14\nopen-vocabulary object detector [44], which produces object\nbounding boxes and category labels from the EgoObjects\ncategories. We then provide the list of detected objects and\nthe task instruction to our LLM, which is GPT-4 [38] with\ntemperature 0. The LLM is additionally provided with the\nrobotic primitives, and a few-shot chain-of-thought prompt\n[45] with instructions to ask questions about objects in the\nscene to determine how to complete the task, and then pro-\nduce a plan using the primitives. There is no constraint on the\nquestions that the LLM can ask, except for encouragement\nin the prompt to ask questions that can be answered with\nyes or no. The same prompt is used for all scenes and tasks,\nwhich we provide in Listing 1.\nAfter the LLM asks a set of object-centric questions, a\nVLM answers each question prompted with the bounding\nbox of the object indicated by the LLM, and then provides\nthe LLM with its highest likelihood responses and their\nassociated likelihoods/confidence scores, as done in prior\nwork for VQA [30]. This continues until the LLM decides\nit has enough information, whereupon it either indicates that\nthe task is not possible, or produces a plan consisting of a list\nof primitives to execute for the task. The few-shot examples\nin Listing 1 illustrate how interaction between the LLM and\nVLM for planning is structured.\nPrimitives. We list the primitives for our real scene planning\nevaluation below:\n\u2022 go to object [X]\n\u2022 pick up object [X]\n\u2022 bring to human object [X]\n\u2022 put down object [X]\n\u2022 done\nThe primitives (except done) are parameterized by a letter\n(in place of [X]) that identifies each detected object in the\nscene. The assignment of letters is provided in the list of\nobject detections given to the LLM planner.\nScenes and Tasks. In Table XXII, we provide the scene\nimages in our evaluation, and the detected objects and task\ninstructions for each scene. We also indicate the task type\nfor each instruction.\nPrompts. We provide the prompts used by our LLM-based\nplanning framework for our scene planning evaluation. The\nversion with VLM interaction is in Listing 1 and the version\nwithout VLM interaction is in Listing 2. The parts of the\nprompts in square brackets are replaced with the correspond-\ning information specific to the task, in the same format as\nthe prompt example.\nListing 1: Prompt for LLM planner with VLM interaction.\nYou are a household robot. You are able to move most\nhousehold objects, but not large or heavy furniture.\nYou are to be safe and not break anything.\nYou will be given a list of objects in the scene.\nA human will give you a task instruction to perform.\nFirst, ask questions about the objects to learn more about\nthem to determine how to properly complete the task.\nIndicate the object letters before asking the question,\nand do not provide the answer.\nFormat each question like \"Question about object [A, B]:\nIs this object heavy?\".\nOnly ask informative questions that help you understand\nhow to properly complete the task.\nSome of the object descriptions in the list may be\ninaccurate, so it is good to ask questions to confirm\ninformation about them.\nAsk questions that can be answered with yes or no when\npossible.\nOnly ask one question at a time.\nAfter each question, you will be given a response\ncontaining the 3 most likely possible answers for\neach object, with corresponding confidence scores.\nAn example response for one object would be \"A: yes (0.8),\nno (0.1), unknown (0.1)\".\nOtherwise, use the information you have obtained to\nproduce a numbered list of actions to perform the\ntask, starting with \"Plan:\".\nYou are only able to perform the following actions, where\n\"X\" is a placeholder for an object letter from the\ngiven list of objects:\n1. go to object X\n2. pick up object X\n3. bring to human object X\n4. put down object X\n5. done\nOnly perform actions that are possible. You are able to\nmove most household objects, including most\nelectronic appliances, but not large or heavy\nfurniture or parts of a room (like a door or heater).\nYou are to be safe and not break anything. Choose\nthe \"done\" action when the plan is complete and then\nstop. If no plan can satisfy the task, tell the human\n, choose the \"done\" action and stop.\nOnly respond with the plan list and do not include any\nother text.\nScene 1:\nThe following objects are in the scene: A (bottle), B (\nbowl), C (countertop), D (shirt), E (lock)\nInstruction: Bring me the heaviest object.\nThought: I can ask which object is heavy out of all the\nobjects, and I can find the one with the largest\nweight that I can carry.\nQuestion about [A, B, C, D, E]: Is this object heavy?\nAnswer:\nA: No (0.50), Yes (0.24), Unknown (0.21)\nB: No (0.90), Unknown (0.05), Yes (0.04)\nC: Yes (0.80), Unknown (0.10), No (0.05)\nD: No (0.73), Unknown (0.20), Yes (0.06)\nE: No (0.41), Yes (0.35), Unknown (0.19)\nThought: The heaviest object is the one with the highest\nYes value or the lowest No value (if none are Yes).\nThe heaviest object is C. However, I cannot carry a\ncountertop since it is a heavy piece of furniture.\nTherefore, I will to choose the next heaviest, which\nwould be E, a lock that I can carry easily.\nPlan:\n1. Go to object E\n2. Pick up object E\n3. Bring to human object E\n4. Done\nInstruction: Bring me the most deformable container.\nThought: I can first ask which objects are containers out\nof all the objects I can carry. Of these containers,\nI can ask which is deformable.\nQuestion about [A, B, C, D, E]: Is this object a container\n?\nAnswer:\nA: Yes (0.55), No (0.20), Unknown (0.19)\nB: Yes (0.90), Unknown (0.05), No (0.04)\nC: No (0.85), Unknown (0.06), Yes (0.05)\nD: No (0.62), Unknown (0.20), Yes (0.06)\nE: No (0.41), Yes (0.35), Unknown (0.19)\nThought: The only objects that are confidently Yes are B\nand A. I should ask which is deformable next.\nQuestion about [A, B]: Is this object deformable?\nAnswer:\nA: Yes (0.80), Unknown (0.15), No (0.04)\nB: No (0.55), Unknown (0.26), Yes (0.05)\nThought: The most deformable object is the one with the\nhighest Yes value or the lowest No value (if none are\nYes). Since some of the answers are Yes, the answer\nis A (the highest yes value).\nPlan:\n1. Go to object A\n2. Pick up object A\n3. Bring to human object A\n4. Done\nScene 2:\nThe following objects are in the scene: [list of objects\nin the scene]\nInstruction: [instruction specified here]\nListing 2: Prompt for LLM planner without VLM interaction.\nYou are a household robot. You are able to move most\nhousehold objects, but not large or heavy furniture.\nYou are to be safe and not break anything.\nYou will be given a list of objects in the scene.\nA human will give you a task instruction to perform.\nUse the object information to produce a numbered list of\nactions to perform the task, starting with \"Plan:\".\nYou are only able to perform the following actions, where\n\"X\" is a placeholder for an object letter from the\ngiven list of objects:\n1. go to object X\n2. pick up object X\n3. bring to human object X\n4. put down object X\n5. done\nOnly perform actions that are possible. You are able to\nmove most household objects, including most\nelectronic appliances, but not large or heavy\nfurniture or parts of a room (like a door or heater).\nYou are to be safe and not break anything. Choose\nthe \"done\" action when the plan is complete and then\nstop. If no plan can satify the task, tell the human,\nchoose the \"done\" action and stop.\nOnly respond with the plan list and do not include any\nother text.\nScene 1:\nThe following objects are in the scene: A (bottle), B (\nbowl), C (countertop), D (shirt), E (lock)\nInstruction: Bring me the heaviest object.\nThought: I cannot carry a countertop since it is a heavy\npiece of furniture. Out of the rest, a good guess\nwould be the lock.\nPlan:\n1. Go to object E\n2. Pick up object E\n3. Bring to human object E\n4. Done\nInstruction: Bring me the most deformable container.\nThought: Typically shirts are easy to fold, so a good\nchoice for the most deformable object would be the\nshirt.\nPlan:\n1. Go to object D\n2. Pick up object D\n3. Bring to human object D\n4. Done\nScene 2:\nThe following objects are in the scene: [list of objects\nin the scene]\nInstruction: [instruction specified here]\nEvaluation Procedure. We evaluate task planning accuracy\nusing a non-author human evaluator. For each evaluation, the\nevaluator is given the task instruction, the image of the scene,\nthe list of detected objects in the scene and their bounding\nboxes, and the generated task plan, and they are asked\nto evaluate whether the task plan successfully performed\nthe task instruction for the given scene. We provide the\nfollowing instructions to the evaluator on what to consider\nwhen evaluating whether a plan was correct:\nInstructions: For each scene, there is a list of objects (under\n\u2018Options:\u2019). Below that is a table of tasks for that scene.\nThe instruction given to a robot is on the left. On the right\nare the choices from 3 different robots. You need to mark\nwhich ones are correct or incorrect. It may be possible that\nmultiple robots got it right or none of them got it right. Be\naware that in tasks that involve moving objects, the robot\nshould not plan to move an object that is very heavy, like\nlarge furniture.\nWhile the planner usually creates plans using only the\nprovided primitives, it sometimes specifies primitives that\nwere not provided. Because the purpose of this evaluation\nis on assessing if a LLM planner can benefit from physical\nreasoning using a VLM, and not on creating a functional\nplanning system, we do not do anything to handle these\ncases. We the evaluator to judge if these plans satisfy the task\ninstruction like the others. We provide example executions\nfor different versions of our planning framework on our\nwebsite.\nScene Image\nObject Detections\nTask Instructions\n1) bottle\n2) pitcher (container)\n3) bowl [flatter bowl]\n4) towel [shirt]\n5) countertop\n6) bowl [taller ceramic bowl]\n7) measuring cup [lock]\n1) Bring me the heaviest object. [S]\n2) Bring me the most deformable ob-\nject. [S]\n3) Bring me the most fragile object.\n[S]\n4) Bring me all containers that you\ncan confidently determine have\nwater. [M]\n5) Bring me the container with oil.\n[M]\n6) Among\nall\nempty\ncontainers,\nbring me the ones that cannot be\nused to carry water. [M]\n7) Bring me the metal object. [S]\n1) suitcase [blue crate]\n2) stool\n3) hair dryer [mirror]\n4) chair [chair that the mirror is on]\n5) dishwasher [metal cabinet in top\nright]\n6) chair [blue chair]\n7) bottle [Elmer glue container]\n8) bottle [Mod Podge container]\n9) container [paint thinner container]\n10) desk\n11) mug [mug with paintbrushes]\n12) facial tissue holder [container with\nglitter]\n13) pencil\n1) Bring me the heaviest object. [S]\n2) Bring me a metal container. [M]\n3) Bring me a small, empty cup that\nI can fill with water to clean my\npaintbrushes. If there are none, tell\nme that there are no small empty\ncups. [M]\n4) Bring me the clear container with\nart supplies. [C]\n5) Bring me the metal object that is\nreflective. [M]\n6) Bring me paint thinner. [C]\n7) Bring me a wooden object. [S]\nTABLE XXII: Scene images, object detections, and task instructions for our real scene planning evaluation (scenes 1 and\n2). The object category labels given by OWL-ViT are sometimes inaccurate or ambiguous, in which case we provide more\nprecise labels in square brackets. Note that the planner only has access to the original OWL-ViT labels. Tasks are labeled\nwith S, M, or C for Single Concept, Multi-Concept, or Common Knowledge, respectively.\nScene Image\nObject Detections\nTask Instructions\n1) clothing [green hoodie]\n2) towel\n3) clothing [striped shirt]\n4) bottle [sunscreen bottle]\n5) towel [socks]\n6) mouse [ear thermometer]\n7) suitcase\n8) bottle [hand sanitizer]\n9) hair dryer [dumbbell]\n10) clothing [blue shirt]\n1) Bring me the heaviest object. [S]\n2) Bring me all clear containers. [M]\n3) Bring me the hard plastic object.\n[M]\n4) Bring me the lightest piece of\nclothing. [S]\n5) Bring me the object I can pack my\nclothes into. [C]\n6) It is cold outside. Bring me some-\nthing that can keep me warm. [C]\n7) It is sunny outside. Bring me the\ncontainer of sunscreen. [C]\n1) facial tissue holder [paper towel\ndispenser]\n2) light switch [left electric outlet]\n3) light switch [right electric outlet]\n4) mixer\n5) toaster\n6) kettle\n7) paper towel\n8) water glass [plastic cup]\n9) salt and pepper shakers [salt]\n10) bottle [jam container]\n11) frying pan [baking pan]\n12) container\n[salmon-colored\ncon-\ntainer]\n13) salt and pepper shakers [pepper]\n14) countertop\n1) Bring me the heaviest object. [S]\n2) Bring me the heavier glass con-\ntainer. [M]\n3) Bring me something that is easy\nto tear. [C]\n4) Bring me the lightest container\nthat is empty but can be filled with\nwater. [M]\n5) Bring me the most deformable\ncontainer with a lid. [M]\n6) Bring me all metal containers that\ncan be used to carry water. [M]\n7) Bring me the object that can be\nused in an oven. [C]\nTABLE XXII: Scene images, object detections, and task instructions for our real scene planning evaluation (scenes 3 and\n4). The object category labels given by OWL-ViT are sometimes inaccurate or ambiguous, in which case we provide more\nprecise labels in square brackets. Note that the planner only has access to the original OWL-ViT labels. Tasks are labeled\nwith S, M, or C for Single Concept, Multi-Concept, or Common Knowledge, respectively.\nScene Image\nObject Detections\nTask Instructions\n1) toaster\n2) light switch [electric outlet]\n3) envelope [napkin on microwave]\n4) light switch\n5) microwave oven [microwave]\n6) door [microwave door]\n7) bottle [glass sauce bottle]\n8) picnic basket [drying rack]\n9) soap dispenser\n10) bottle [plastic bottle with blue\nvanilla flavor]\n11) mug [dry mug]\n12) sink\n13) frying pan [dirty pan in sink]\n14) mug [dirty mug in sink]\n15) countertop\n16) waste container\n17) cupboard\n18) plastic bag [trashbag]\n1) Bring me the heaviest object that\nyou can carry. [S]\n2) Bring me an empty mug that I can\nuse to make tea. [C]\n3) Bring me the most deformable ob-\nject. [S]\n4) Bring me the glass object. [S]\n5) Bring me a metal pan that is in the\nsink. [C]\n6) Bring me the container that stores\ntrash. [C]\n1) envelope [sign on napkin dis-\npenser]\n2) humidifier [napkin dispenser]\n3) ladle [metal tongs]\n4) food [two salad containers on the\nright]\n5) bottle [red wine vinegar bottle]\n6) frying pan [closer salad tray]\n7) paper [napkin coming out of dis-\npenser]\n8) countertop\n9) bottle [olive oil bottle]\n10) bottle [black container on the\nright]\n11) bottle [black container on the left]\n12) juice [olive oil inside bottle]\n13) cabinetry\n14) countertop [more cropped in view\nof countertop]\n15) bowl\n[paper\nplate\nunder\nthe\ncounter]\n1) Bring me the most deformable ob-\nject. [S]\n2) Bring me the lightest metal object.\n[M]\n3) Bring me the heaviest glass con-\ntainer. [M]\n4) Serve some food on a plate using\nobjects in the scene. [C]\n5) Bring me an empty container that\nyou can confidently use to contain\nliquids, if one exists. Otherwise,\ntell me that no suitable containers\nexist. [M]\n6) Bring me the container of olive\noil. [C]\nTABLE XXII: Scene images, object detections, and task instructions for our real scene planning evaluation (scenes 5 and\n6). The object category labels given by OWL-ViT are sometimes inaccurate or ambiguous, in which case we provide more\nprecise labels in square brackets. Note that the planner only has access to the original OWL-ViT labels. Tasks are labeled\nwith S, M, or C for Single Concept, Multi-Concept, or Common Knowledge, respectively.\nScene Image\nObject Detections\nTask Instructions\n1) whiteboard\n2) door [leftmost door]\n3) paper\n4) window [window of left door of\nrightmost pair]\n5) door [left door of rightmost pair]\n6) table [taller table]\n7) chair [leftmost short chair facing\ntowards the camera]\n8) chair [tall chair]\n9) chair [short chair behind pillar]\n10) chair [rightmost short chair, facing\ntowards the camera]\n11) table [long wooden table]\n12) door [rightmost door]\n13) couch\n14) chair [left side, facing away from\ncamera]\n15) coffee table\n1) Go to the piece of furniture that is\nthe softest. [C]\n2) Go to the glass object that is not\npart of a window or door. [S]\n3) Bring me the lightest object. [S]\n4) Among all pieces of furniture, go\nto the one that is lightest. [C]\n5) Go to the table that does not have\na wooden surface. [S]\n1) box [binder]\n2) bottle [large plastic tub]\n3) bottle [plastic bottle]\n4) box [algorithms textbook]\n5) pitcher\n(container)\n[blue\nmetal\ncup]\n6) water glass [small glass cup]\n7) headphones [phone cable]\n8) dumbbell [power brick]\n9) adhesive tape [ruler]\n1) Bring me the container that is\nmost likely to be metal. [M]\n2) Bring me the heaviest container.\nOnly consider the container itself,\nnot the contents inside. [S]\n3) Bring me the sealed container\nwith juice. [M]\n4) Bring me the most bendable ob-\nject. [S]\n5) Bring me the most fragile object.\n[S]\n6) Bring me all containers that are\nmade of plastic (with very high\nconfidence). [M]\nTABLE XXII: Scene images, object detections, and task instructions for our real scene planning evaluation (scenes 7 and\n8). The object category labels given by OWL-ViT are sometimes inaccurate or ambiguous, in which case we provide more\nprecise labels in square brackets. Note that the planner only has access to the original OWL-ViT labels. Tasks are labeled\nwith S, M, or C for Single Concept, Multi-Concept, or Common Knowledge, respectively.\nG. Real Robot Evaluation Details\nPrimitives. We list the primitives for our real robot evalua-\ntion below:\n\u2022 move [X] to the side\n\u2022 move [X] into [Y]\n\u2022 done\nSimilarly as with our planning-only evaluation, our primi-\ntives are parameterized by a letter (in place of [X] or [Y]) that\nidentifies each detected object in the scene. The assignment\nof letters is provided in the list of object detections given to\nthe LLM planner.\nScenes and Tasks. In Table XXIII, we provide the scene\nimages in our real robot evaluation, the detected objects in\neach scene, and the task instructions for each scene.\nPrompts. We use the same prompt structure from the real\nscene planning evaluation. For tasks that involve moving an\nobject to a side, we add \u201cIn your plan, you may only use\nthe following primitive: move X to the side (where X is\nan object). Do not move furniture.\u201d For tasks that involve\nmoving an object into a container, we add \u201cIn your plan,\nyou may only use the following primitive: move X into Y\n(where X is an object and Y is a container). Do not move\nfurniture.\u201d\nEvaluation Procedure. We run real robot experiments us-\ning a 7-DoF Franka Emika Panda robot with a Robotiq\n2F-85 gripper, using Polymetis [46] for real-time control.\nWe obtain our pick-and-place primitives by collecting a\nkinesthetic demonstration for each primitive, and replaying\nthe demonstration to execute it. The objects in the images\nprovided to the object detector are not in the exact same\npositions as when the robot is acting, because the objects\nhave to be rearranged when collecting demonstrations for\neach primitive. However, this does not affect the planner\nbecause we do not provide it object positions, as our planning\nframework does not make use of them.\nBecause our evaluation focuses on planning quality and\nnot successful execution of primitives, we retry execution of\neach plan until all of the primitives are successfully executed.\nTherefore, our success rates are only reflective of planning\nquality, and not that of the primitives.\nWe evaluate the success rate of robot executions using\na non-author human evaluator. For each evaluation, the\nevaluator is given the task instruction and a video of the\nrobot executing the generated plan, and they are asked to\nevaluate whether the robot successfully performed the task\ninstruction. We provide visualizations of robot executions on\nour website.\nScene Image\nObject Detections\nTask Instructions\n1) towel [handbag]\n2) bottle [paint bottle]\n3) bowl [yellow plastic bowl]\n4) tool [container of metals]\n5) desk [full table]\n6) saucer\n7) bowl [ceramic bowl]\n8) spoon\n9) pencil [pen]\n10) pencil\n11) milk [snack packet]\n1) Move all objects that are not plas-\ntic to the side.\n2) Find a container that has metals.\nMove all metal objects into that\ncontainer.\n3) Move all containers that can be\nused to carry water to the side.\n4) Put the two objects with the least\nmass into the least deformable\ncontainer.\n5) Move the most fragile object to\nthe side.\n1) bottle [glass jar]\n2) mug [metal mug]\n3) scale [cardboard cupholder]\n4) mug [plastic cup]\n5) adhesive tape\n6) tool [wrench]\n1) Put all containers that can hold\nwater to the side.\n2) Put all objects that are not plastic\nto the side.\n3) Put all objects that are translucent\nto the side.\n4) Put the three heaviest objects to\nthe side.\n5) Put a plastic object that is not a\ncontainer into a plastic container.\nChoose the container that you are\nmost certain is plastic.\nTABLE XXIII: Scene images, object detections, and task instructions for our real robot evaluation. The object category\nlabels given by OWL-ViT are sometimes inaccurate or ambiguous, in which case we provide more precise labels in square\nbrackets. Note that the planner only has access to the original OWL-ViT labels.\n"
  },
  {
    "title": "Bayes' Rays: Uncertainty Quantification for Neural Radiance Fields",
    "link": "https://arxiv.org/pdf/2309.03185.pdf",
    "upvote": "6",
    "text": "Bayes\u2019 Rays: Uncertainty Quantification for Neural Radiance Fields\nLily Goli1\nCody Reading2\nSilvia Sell\u00b4an1\nAlec Jacobson1,4\nAndrea Tagliasacchi1,2,3\nUniversity of Toronto1\nSimon Fraser University2\nGoogle DeepMind3 Adobe Research4\nTrain\nNeRF\nNovel View\nOur Spatial Uncertainty\nClean Novel View\nBayes\u2019 Rays\nthresholding by uncertainty\nFigure 1. We introduce BayesRays, a post-hoc algorithm to estimate the spatial uncertainty of any pre-trained NeRF of any arbitrary\narchitecture. Our method requires no additional training and can be used to clean up NeRF artifacts caused by occluded or incomplete data.\nAbstract\nNeural Radiance Fields (NeRFs) have shown promise in\napplications like view synthesis and depth estimation, but\nlearning from multiview images faces inherent uncertain-\nties. Current methods to quantify them are either heuristic\nor computationally demanding. We introduce BayesRays,\na post-hoc framework to evaluate uncertainty in any pre-\ntrained NeRF without modifying the training process. Our\nmethod establishes a volumetric uncertainty field using spa-\ntial perturbations and a Bayesian Laplace approximation.\nWe derive our algorithm statistically and show its superior\nperformance in key metrics and applications. Additional re-\nsults available at: https://bayesrays.github.io\n1. Introduction\nNeural Radiance Fields (NeRFs) are a class of learned vol-\numetric implicit scene representations that have exploded\nin popularity due to their success in applications like novel\nview synthesis and depth estimation. The process of learning\na NeRF from a discrete set of multiview images is plagued\nwith uncertainty: even in perfect experimental conditions,\nocclusions and missing views will limit the epistemic knowl-\nedge that the model can acquire about the scene.\nStudying the epistemic uncertainty in NeRF is fundamen-\ntal for tasks like outlier detection [21] and next-best-view\nplanning [31] that expand NeRF\u2019s performance and applica-\ntion domain to critical areas like autonomous driving [11].\nHowever, quantifying the uncertainty contained in a NeRF\nmodel is a relatively new area of study, with existing meth-\nods proposing either heuristic proxies without theoretical\nguarantees [14, 58] or probabilistic techniques that require\ncostly computational power [47] and/or elaborate changes\nto the conventional NeRF training pipeline [41, 42].\nWe draw inspiration from triangulation problems in clas-\nsic photogrammetry [45], where uncertainty is often mod-\neled through distributions of feature point positions in image\nspace that are then projected onto 3D (see Figure 2). In-\ntuitively, this distribution measures how much a feature\u2019s\nposition can be perturbed while maintaining multi-view con-\nsistency. We apply a similar intuition to NeRF, identifying\nthe regions of the radiance field that can be spatially per-\nturbed with minimal impact on the reconstruction loss.\nTraditional\nphotogrammetry\nOur NeRF\nuncertainty\nFigure 2. Inspired by uncertainty quantification in classic pho-\ntogrammetry (left), we find epistemic uncertainty in NeRF (right).\n1\narXiv:2309.03185v1  [cs.CV]  6 Sep 2023\nWe propose BayesRays, a post-hoc framework for quan-\ntifying the uncertainty of any arbitrary pre-trained NeRF.\nWithout requiring any changes to the training pipeline and\nregardless of the architecture in use in the given NeRF (see\nFigure 3), our method simulates spatially parametrized per-\nturbations of the radiance field and uses a Bayesian Laplace\napproximation to produce a volumetric uncertainty field,\nwhich can be rendered like an additional color channel.\nIn this work, we show that our calculated uncertainties\nare not only statistically meaningful but also outperform pre-\nvious works on key metrics like correlation to reconstructed\ndepth error. Furthermore, they provide a framework for criti-\ncal applications like removing floater artifacts from NeRF,\nmatching or improving the state-of-the-art (see Figure 1).\nIn summary, our main contributions are:\n\u2022 We introduce a plug-and-play probabilistic method to\nquantify the uncertainty of any pre-trained Neural Ra-\ndiance Field independently of its architecture, and without\nneeding training images or costly extra training.\n\u2022 In little over a minute, we compute a spatial uncertainty\nfield that can be rendered as an additional color channel.\n\u2022 We propose thresholding our uncertainty field to remove\nartifacts from pre-trained NeRFs interactively in real time.\n2. Related work\nUncertainty Quantification studies the distribution of the\nresponses of a system conditioned on a set of measurable\ninput variables [43]. As a field of statistics, it has grown over\nmany decades out of the need to measure the accuracy of\nscientific predictions in areas like physics [13], chemistry\n[39] or meteorology [30].\nUncertainty in Computer Vision. Closer to our application,\nestimating the uncertainty of Computer Vision systems has\nbeen a subject of study even long before the Deep Learning\nrevolution; for example, in Structure from Motion and Bun-\ndle Adjustment [45, Section 11.4][51]. In these problems\nfrom classic photogrammetry, scene geometry and camera\nparameters are jointly optimized in a process filled with un-\ncertainty [46, 54], often modeled through 2D image-space\nGaussians projected to 3D [8, 26, 45, 51] (see Figure 2).\nUncertainty in Deep Learning. The process of fitting a\nneural network to observed data typically contains two fun-\ndamentally different sources of uncertainty [1, 17]. Aleatoric\nuncertainty refers to the inherent randomness contained in\nthe data (e.g., due to instrument error or uncontrolled in-\nfluences) and is often captured by fitting not just one func-\ntion but a parametric distribution (e.g., Gaussians) to the\ndata [4, 17, 25]. On the other hand, epistemic uncertainty\nquantifies the lack of knowledge that a model has over the\nsystem it is trying to replicate; for example, due to miss-\ning data. Commonly, this is achieved through a Bayesian\nframework that estimates the posterior distribution of the\nNerfacto\nMip NeRF\nTrain\nInstant\nNGP\n0\n+\nOur architecture-independent \nuncertainty quantification\nFigure 3. BayesRays applied to different NeRF architectures.\n\u201dLego\u201d [24] is only trained with cameras on its left hemisphere.\nmodel given observed data. The most straightforward-yet-\nexpensive way of achieving this is via the use of Deep Ensem-\nbles that quantify the differences in optimized parameters\nafter training many identical, yet differently initialized, net-\nworks on the same data [19, 57]. A popular alternative to\nensembles are variational Bayesian Neural Networks, which\nmodel each network parameter with an independent distribu-\ntion that is sampled at each training iteration and adjusted\nthrough a KL loss to approximate the true model poste-\nrior [9, 28]. This significant change to the training pipeline\ncomes at a computational cost, which recent works have\nproposed circumventing through the use of post-hoc Laplace\nApproximations (see [12, 37], or [50] for a Computer Vision\napplication) that only estimate the network weight posterior\nnear their already-trained value using derivative information.\nUncertainty in Neural Radiance Fields. NeRFs [23] rep-\nresent 3D scenes through a neural volumetric encoding that\nis optimized to match the images produced from multiple\ncamera views. Aleatoric uncertainty presents itself in this\nprocess through the presence of transient objects in the scene\nor changes in lighting and camera specifications. These phe-\nnomena are quantified by the pioneering work NeRF-W [22]\nand subsequent follow-ups [16, 31, 35] through a combina-\ntion of standard aleatoric Deep Learning techniques [1] and\na learned appearance latent embedding [10].\nDistinctly, we concern ourselves with the epistemic uncer-\ntainty of Neural Radiance Fields, the source of which is often\nmissing data due to occlusions, ambiguities and limited cam-\nera views. Many of the general Deep Learning techniques\nto quantify this uncertainty have been applied to NeRF with\nlimited success. Works like [47] propose uncertainty esti-\nmation through ensemble learning, which can be time and\nmemory consuming. Shen et al. [41] and its follow-up [42]\nmodel the problem through variational inference and KL\ndivergence optimization in a way that is not too dissimilar in\n2\nprinciple, yet shown to be superior, to standard variational\nBayesian neural networks. All these methods require intri-\ncate changes to the NeRF training pipeline. In contrast, we\nintroduce BayesRays, the first framework that allows the use\nof Laplace approximations for NeRF uncertainty quantifica-\ntion, avoids variational optimization and can thus be applied\non any pretrained NeRF of any arbitrary pipeline.\nAway from the traditional Deep Learning uncertainty\nquantification frameworks, other works propose using NeRF-\nspecific proxies for uncertainty. For example, Zhan et al.\n[58] propose computing the uncertainty as entropy of ray\ntermination in NeRF model. While high entropy can be\na good indicator of uncertainty in modeling solid objects,\nsuch assumption can fail while using density regularizers\nlike the distortion loss proposed in [7]. Hoffman et al. [14]\nsuggest quantifying uncertainty as the variance in scenes\nproduced by a generative model when conditioned on partial\nobservations, relying heavily on the specific model\u2019s priors.\nOther related works. As we show in Section 4, our uncer-\ntainty quantification relies on the sensitivity of any trained\nNeRF to perturbations, a concept recently explored in an\nunpublished preprint by Yan et al. [55] for applications out-\nside NeRF and through a continual learning framework. To\nmake our method computationally tractable and architecture-\nindependent, we introduce a spatial deformation field similar\nto the one suggested by [32, 33], albeit we interpret it in-\nstead as a re-parametrization of the NeRF model on which\nto perform a Laplace Approximation to quantify its uncer-\ntainty. One of the many uses we propose for our output\nspatial uncertainty field is to avoid common spatial NeRF ar-\ntifacts. Instead of changing the optimization at training time\nby adding regularizers [7, 29, 34, 36], we propose removing\nthem from any pre-trained NeRF in a post-processing step, a\ntask that has been tackled recently by diffusion-based works\nlike Nerfbusters [52]. As we show in Section 5, our algo-\nrithm matches or improves the performance of Nerfbusters\nin this specific application while being more general and\nrequiring no additional training.\n3. Background\nWe propose a general framework for applying Laplace ap-\nproximations to quantify the epistemic uncertainty of any\npre-trained NeRF. We will briefly review both these concepts\nbefore exploring the difficulties one encounters when trying\nto combine them naively, thus motivating our perturbation-\nbased approach described in Sec. 4.\n3.1. Neural Radiance Fields\nConventional NeRFs [23] learn to map each point in 3D\nspace to a view-dependent radiance and a view-independent\ndensity value:\nc\u03d5(x, d), \u03c4\u03d5(x) = R(x, d; \u03d5)\n(1)\nBase Nerfacto\nUncertainty\nOurs\nNerfbusters\nCentury\nPikachu\nFlowers\n+\nMethod\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nCoverage \u2191\nNerfacto (base)\n16.83\n0.52\n0.39\n0.89\nNerfbusters\n17.99\n0.60\n0.25\n0.63\nBayesRays-0.9\n17.66\n0.56\n0.34\n0.83\nBayesRays-0.4\n17.78\n0.57\n0.31\n0.78\nBayesRays-best\n18.27\n0.60\n0.27\n0.70\nFigure 4. We propose cleaning up a learned scene by thresholding\nit based on our computed uncertainty, matching or surpassing the\nstate of the art at a much lower computational and memory cost.\nwhere \u03d5 represent the learnable parameters in the neural field.\nThe color of each pixel in an image can then be rendered\nthrough compositing the density and color of a series of\npoints {ti} along the ray r = or + t \u00b7 dr, using volume\nrendering [48]:\nC\u03d5(r) =\nX\ni\nexp\n\uf8eb\n\uf8ed\u2212\nX\nj<i\n\u03c4j\u03b4j\n\uf8f6\n\uf8f8 (1\u2212exp(\u2212\u03c4i\u03b4i))ci, (2)\nwhere \u03b4i denotes the distance between each pair of succes-\nsive points. The network parameters \u03d5 are optimized to\nminimize reconstruction loss defined as the squared distance\nbetween the predicted color C(r) and ground truth Cgt for\neach ray r sampled from image In of training set images\nI = {I}N\nn=0. From a Bayesian perspective, this is equivalent\nto assuming a Gaussian likelihood p(C\u03d5|\u03d5) \u223c N(Cgt\nn, 1\n2)\nand finding \u03d5\u2217, the mode of the posterior distribution\n\u03d5\u2217 = arg max\n\u03d5\np(\u03d5|I)\n(3)\nwhich, by Bayes\u2019 rule, is the same as minimizing the negative\nlog-likelihood\n\u03d5\u2217 = arg min\n\u03d5\nEi Er\u223cIn\u2225C\u03d5(r) \u2212 Cgt\nn(r)\u22252\n2\n(4)\n3\n3.2. Neural Laplace Approximations\nA common strategy to quantify the epistemic uncertainty of\nany neural network trained on some data I is to study the pos-\nterior distribution of the network parameters \u03b8 conditioned\non the data, p(\u03b8|I). In contrast to variational Bayesian\nneural networks, which propose using Bayes\u2019 rule and a vari-\national optimization to estimate this distribution, Laplace ap-\nproximations [12, 37] rely on simply training the network by\nany conventional means until convergence; i.e., on obtaining\nthe likeliest network weights \u03b8\u2217 \u2013 the mode of p(\u03b8|I). Then,\nthe posterior is approximated by a multivariate Gaussian dis-\ntribution centered at the obtained mode p(\u03b8|I) \u223c N(\u03b8\u2217, \u03a3).\nThe covariance \u03a3 of this distribution is then computed via a\nsecond-order Taylor expansion of the negative log-likelihood\nh(\u03b8) = \u2212 log p(\u03b8|I) about \u03b8\u2217:\nh(\u03b8) \u2248 h(\u03b8\u2217) + 1\n2(\u03b8\u2212\u03b8\u2217)\u22a4H(\u03b8\u2217) (\u03b8\u2212\u03b8\u2217) ,\n(5)\nwhere first order terms are discarded since \u03b8\u2217 is a maximum\nof h(\u03b8) and H(\u03b8\u2217) is the Hessian matrix of second deriva-\ntives of h(\u03b8) evaluated at \u03b8\u2217. Identifying the terms in Eq. 5\nwith the usual log squared exponential Gaussian likelihood\nof N(\u03b8\u2217, \u03a3), one obtains\n\u03a3 = \u2212H(\u03b8\u2217)\u22121\n(6)\nUnfortunately, a naive application of this framework to\nNeRF by identifying \u03b8 with \u03d5 is impracticable, as it would\nhave three potentially fatal flaws:\n\u2022 First, as we show in Section 4.4, the parameters of the\nNeRF are strongly correlated with each other, making it\ndifficult to accurately estimate the posterior distribution\nwith any guarantees without computing, (and storing) all\nthe entries in H, a (potentially fully, at least block-) dense\nmatrix with dimensions matching the number of network\nweights, before carrying out a costly inversion step.\n\u2022 Secondly, even if one perfectly computed \u03a3, the parameter\ncorrelations and network non-linearities would make it\nsuch that transferring this distribution to any geometrically\nmeaningful one like a distribution over densities or pixel\nvalues would require repeatedly and expensively drawing\nsamples from the full N(\u03d5\u2217, \u03a3).\n\u2022 Finally, beyond computational constraints, estimating un-\ncertainty directly on the NeRF parameters would require\nour algorithm to have knowledge of (and, potentially, de-\npendence on) the specific internal NeRF architecture used.\nBelow, we solve all of these problems by introducing\na parametric perturbation field on which to perform the\nLaplace approximation. Our algorithm is completely in-\ndependent of the specific NeRF architecture used and can\nguarantee minimal correlations between parameters, allow-\ning us to calculate a meaningful spatial uncertainty field\nwithout the need to draw any distribution samples.\n4. Method\nAs input, we assume that are given a pre-trained radiance\nfield R with radiance function c, density \u03c4 and optimized\nparameters \u03d5\u2217, as well as ground truth camera parame-\nters {Tn} corresponding to the N training images. Our\nmethod makes no assumption about the specific architecture\nof R and is designed for any arbitrary framework that pro-\nduces a learned density \u03c4\u03d5\u2217 and radiance c\u03d5\u2217, which we will\ntreat as differentiable black boxes.\nWe begin by noting that, while the neural network\nweights \u03d5 may serve as a useful parametrization of R during\ntraining, a Laplace approximation can be formulated on any\nre-parametrization R\u03b8 for any parameter set \u03b8 \u2208 \u0398, as long\nas one knows the mode of the distribution p(\u03b8|I).\nWe follow by taking inspiration from the key insight\nbehind NeRFs themselves: namely, that one can achieve\nimpressive performance even in 2D tasks by explicitly mod-\neling the 3D scene through a volumetric field. We also take\ninspiration from Computer Graphics, where global, volu-\nmetric deformation fields have been proposed as tools for\nmanipulating implicitly represented objects [40, 44]. Finally,\nwe draw inspiration from photogrammetry, where recon-\nstruction uncertainty is often modeled by placing Gaussian\ndistributions on the spatial positions of identified feature\npoints (see Fig. 2).\n4.1. Intuition\nThe\nintuition\nbehind\nour\nre-\nparametrization is better seen with\nthe simple scene shown in the inset.\nConsider a single solid blue segment\nwith a green center embedded in the\n2D plane. Imagine that this object\nis observed by two simplified cameras that capture rays\nin a 60-degree cone, and let us now consider the NeRF\nreconstruction problem as stated on this small dataset.\n\u201cperfect\u201d\nreconstructions\nTrivially, the problem is an under-\ndetermined one: as shown in the inset,\nthe green segment could be substituted\nby many possible curves while still re-\nsulting in a \u201cperfect\u201d photometric recon-\nstruction according to the four pixels in\nour dataset. Indeed, there is a whole\nnull-space of solutions (green shaded region) to which the\ngreen segment could be perturbed without affecting the re-\nconstruction loss1, and a NeRF trained on this dataset may\nconverge to any one of these configurations depending on\nthe training seed. Hence, one may quantify the uncertainty\nof a trained NeRF by asking \u201chow much can one perturb it\nwithout hurting the reconstruction accuracy?\u201d\n1Note this is analogous to the condition number of near/far-field triangu-\nlation from photogrammetry that was discussed in Section 1.\n4\nmore \nuncertain\nless\nuncertain\nCrucially, this quantity will vary\nspatially: some regions of space will\nbe more constrained by the training\nset and will allow only a limited per-\nturbation before having an adverse\neffect on the loss (e.g., the edges of the segment, orange in\nthe inset) while others will be much more uncertain (e.g.,\nthe middle of the segment, purple in the inset). Hence, we\nwill be able to quantify the spatial uncertainty of any trained\nNeRF by asking \u201cwhich regions can one perturb without\nhurting the reconstruction accuracy?\u201d\nfront\nside\nThis quantity will be helpful beyond\nsimple didactic examples: indeed, even\ngeneral 3D scenes like the one in the\ninset can seem like pixel-perfect re-\nconstructions from all training camera\nviews (in this case, we trained a Ner-\nfacto [49] model for 30,000 epochs with 40 front and back\nviews of the laptop) but reveal large geometric artifacts when\nseen from a different angle.\n4.2. Modeling perturbations\nInspired by all the considerations above, we introduce a\ndeformation field D : RD \u2192 RD, which one can interpret as\na block that is ran on the input coordinates before the NeRF\nnetwork. We choose a spatially meaningful parametrization\nin the form of vector displacements stored on the vertices of\na grid of length M, allowing \u03b8 to be represented as a matrix\n\u03b8 \u2208 RM D\u00d7D, and defining a deformation for every spatial\ncoordinate via trilinear interpolation\nD\u03b8(x) = Trilinear(x, \u03b8) .\n(7)\nWe can now reparametrize the radiance field R with \u03b8 by\nperturbing each coordinate x before applying the already-\noptimized NeRF neural network\n\u02dc\u03c4\u03b8(x) = \u03c4\u03d5\u2217(x + D\u03b8(x)) ,\n(8)\n\u02dcc\u03b8(x) = c\u03d5\u2217(x + D\u03b8(x), d) ,\n(9)\nresulting in the predicted pixel colors\n\u02dcC\u03b8(r) =\nX\ni\nexp\n\uf8eb\n\uf8ed\u2212\nX\nj<i\n\u02dc\u03c4j\u03b4j\n\uf8f6\n\uf8f8 (1 \u2212 exp(\u2212\u02dc\u03c4i\u03b4i)) \u02dcci .\nWe proceed by assuming a likelihood of the same form as\nwith the NeRF parametrization, \u02dcC\u03b8|\u03b8 \u223c N(Cgt\nn, 1\n2). Under\nour assumption that \u03d5\u2217 are the optimal parameters obtained\nfrom NeRF training, it would be unreasonable to expect\nany non-trivial deformation to decrease the reconstruction\nloss; thus, it makes sense to place a regularizing indepen-\ndent Gaussian prior \u03b8 \u223c N(0, \u03bb\u22121) on our new parameters\nTrain\n \nRGB\n32\n256\nAUSE = 0.247\nAUSE= 0.170\nAUSE = 0.167\n0\n+\n3\ngrid\n3\n10243\nFigure 5. Very low resolutions may cause uncertainties to be\nunderestimated, with diminishing returns for M > 256.\nand formulating the posterior p(\u03b8|I) whose negative log-\nlikelihood h(\u03b8) is given by\nh(\u03b8) = EnEr\u223cIn\u2225 \u02dcC\u03b8(r) \u2212 Cgt\nn(r)\u22252\n2 + \u03bb\u2225\u03b8\u22252 .\n(10)\nThe minimum of (10) must be obtained when \u03b8 = 0, as\nin that case \u02dc\u03c40(x) = \u03c4\u03d5\u2217(x), \u02dcc0(x) = c\u03d5\u2217(x, d) and thus\n\u02dcC0(r) = C\u03d5(r). Thus, zero is the mode of the distribution\np(\u03b8|I) and we are finally in the ideal conditions for a Laplace\napproximation around \u03b8\u2217 = 0. Following Sec. 3.2, this result\nin a distribution \u03b8 \u223c N(0, \u03a3) where\n\u03a3 = \u2212H(0)\u22121\n(11)\nwhere H is the Hessian matrix of second derivatives of h(\u03b8)\nevaluated at zero. Computing these second derivatives is a\ncomputationally intensive task; however, as we show below,\na combination of statistical and NeRF-specific tools allows\nus to approximate it in terms of first derivatives only.\n4.3. Approximating H\nFor any parametric family of statistical distributions p\u03b8, the\nHessian of the log-likelihood with respect to the parameters\nis also known as the Fisher information\nI(\u03b8) = \u2212EX\u223cp\u03b8\n\u0014\u22022h(X ; \u03b8)\n\u2202\u03b82\n\f\f\f\f\u03b8\n\u0015\n= \u2212H(\u03b8) ,\n(12)\nwhich (under reasonable regularity assumptions) can also be\ndefined as the variance of the parametric score [20, 5.3]\nI(\u03b8) = EX\u223cp\u03b8\n\"\n\u2202h(X ; \u03b8)\n\u2202\u03b8\n\u22a4 \u2202h(X ; \u03b8)\n\u2202\u03b8\n\f\f\f\f\f\u03b8\n#\n.\n(13)\nLet us now denote the pair of random variables correspond-\ning to a ray and its predicted color as (r, y), where r \u223c {In}\nand y = Cgt\nn(r). In our case, (13) takes the form\nI(\u03b8) = E(r,y)\n\u0002\n4 \u03f5\u03b8(r) J\u03b8(r)\u22a4J\u03b8(r)\n\u0003\n+ 2\u03bbI\n(14)\nwhere \u03f5\u03b8(r) is the ray residual error\n\u03f5\u03b8(r) = \u2225 \u02dcC\u03b8(r) \u2212 Cgt\nn(r)\u22252\nand J\u03b8(r) is the Jacobian of first derivatives\nJ\u03b8(r) = \u2202 \u02dcC\u03b8(r)\n\u2202\u03b8\n(15)\nwhich can easily be computed via backpropagation.\n5\nFurther, as we typically do not have multiple observations\nof ray color for a single ray r, we can further simplify the\nabove using the definition of conditional expectation\nI(\u03b8) = Er\n\u0002\n4 Ey|r [\u03f5\u03b8(r)] J\u03b8(r)\u22a4J\u03b8(r)\n\u0003\n+ 2\u03bbI ,\n(16)\nnoting that Ey|r [\u03f5\u03b8(r)] is nothing more than 1\n2, the variance\nof our stated likelihood N(Cgt\nn, 1\n2),\nI(\u03b8) = Er\n\u0002\n2 J\u03b8(r)\u22a4J\u03b8(r)\n\u0003\n+ 2\u03bbI .\n(17)\nCombining (17) with (12) and approximating the expectation\nvia sampling of R rays, we have our final expression for H:\nH(\u03b8) \u2248 \u2212 2\nR\nX\nr\nJ\u03b8(r)\u22a4J\u03b8(r) \u2212 2\u03bbI ,\n(18)\nIt is worth remarking that, while H contains in it all the\ninformation that we will need to quantify the epistemic un-\ncertainty of the given radiance field, its computation in (18)\ndoes not explicitly rely on the data from the training images\nbut only on information contained in the pre-trained model\nand the training camera parameters.\n4.4. Spatial uncertainty\nWe can now fully take advantage of our\nproposed reparametrization. First, since\neach vector entry in \u03b8 corresponds to\na vertex on our grid, its effect will be\nspatially limited to the cells containing\nit, making H(\u03b8) necessarily sparse and\nminimizing the number of correlated\nparameters (see inset, which compares\nthe sparsity of H(\u03b8) to the that of an\nNeRF\u2019s MLP parameters \u03d5). In fact, thanks to this low num-\nber of correlations, we will proceed like Ritter et al. [37] and\napproximate \u03a3 only through the diagonal entries of H:\n\u03a3 \u2248 diag\n \n2\nR\nX\nr\nJ\u03b8(r)\u22a4J\u03b8(r) + 2\u03bbI\n!\u22121\n.\n(19)\nSecondly, by measuring the variance of our deformation\nfield (intuitively, how much one could change the NeRF ge-\nometry without harming reconstruction quality), \u03a3 critically\nencodes the spatial uncertainty of the radiance field. We can\nformalize this by considering the (root) diagonal entries of \u03a3,\nwhich define a marginal variance vector \u03c3 = (\u03c3x, \u03c3y, \u03c3z).\nMuch like in the photogrammetry works discussed in Sec-\ntion 2 and Figure 2, at each grid vertex, \u03c3 defines a spatial\nellipsoid within which it can be deformed to minimal recon-\nstruction cost. The norm of this vector \u03c3 = \u2225\u03c3\u22252 is then a\npositive scalar that measures the local spatial uncertainty of\nthe radiance field at each grid vertex. Through it, we can\ndefine our spatial uncertainty field U : R3 \u2192 R+ given by\nU(x) = Trilinear(x, \u03c3) ,\n(20)\nwhich intuitively encodes how much the positioning of geo-\nmetric region in our reconstruction can be trusted. Strictly\nspeaking, as defined above, U measures the uncertainty at\n(1+D\u03b8)\u22121(x), not x; however, we note that these are equiv-\nalent for the trained NeRF for which D\u03b8\u2217 = 0.\nThe uncertainty field U is the main output of our algo-\nrithm and serves to illustrate the success of our approach. It\nis a first-of-its-kind theoretically derivated spatial measure of\nuncertainty that can be computed on any NeRF architecture,\nwithout the need for additional training, expensive sampling\nor even access to the training images. We will now validate\nit experimentally and show potential applications.\n5. Experiments & Applications\nWe validate our theoretically-derived algorithm through our\nuncertainty field\u2019s correlation with the depth prediction error\nin a given NeRF (Section 5.1), show a prototypical applica-\ntion to a NeRF clean-up task (Section 5.2) and justify our\nparametric choices through ablation studies (Section 5.3).\nImplementation. Unless specified otherwise, all NeRFs\nused throughout this paper use Nerfstudio\u2019s Nerfacto [49] as\nthe base architecture and are pre-trained for 30,000 steps. We\nextract our uncertainty field U using 1,000 random batches\nof 4,096 rays each sampled from a scene\u2019s training cam-\neras, with M = 256 and \u03bb = 10\u22124/M 3, in a process that\ntakes around 90 seconds on an NVIDIA RTX 6000. Once\ncomputed for a given scene, our derived uncertainty field\nconveniently functions as an additional color channel that\ncan be volumetrically rendered in a form (and cost) anal-\nogous to the usual RGB. For visualization clarity, all our\nresults use a logarithmic scale, rendering log U instead of U.\n5.1. Uncertainty Evaluation \u2013 Figures 6\nWe evaluate the estimated uncertainty of BayesRays by show-\ning its correlation with the NeRF depth error. We choose\nthe error in predicted depth as the best signal that conveys\nNeRF\u2019s unreliability in geometric prediction, as RGB error\nhas been shown to not be representative of true uncertainty\ndue to radiance accumulation and model biases [47].\nMetric. We measure correlation through the Area Under\nSparsification Error (AUSE) [5, 15]. The pixels in each\ntest image are removed gradually (\u201csparsified\u201d) twice: first,\naccording to their respective depth error; second, by their\nuncertainty measure. The difference between the Mean Ab-\nsolute depth Error (\u2206MAE) of the remaining pixels in both\nprocesses, at each stage, provides the sparsification curves.\nData. In Figure 6, we use 4 ScanNet scenes (#0000-001,\n#0079-000, #0158-000, #0316-000) with groundtruth depths\nprovided. Each scene employs 40 images split into 5 test and\n35 train images, following NerfingMVS [53]. Additionally,\nwe use 4 scenes from the Light Field dataset [56, 59] (torch,\nstatue, basket, africa), with the same train-test split and\npseudo-ground-truth depth map approach as CF-NeRF [42].\n6\n2.0\n1.6\n1.2\n0.8\n0.4\nCF-NeRF\nOurs\nEnsemble\nMost uncertain\npixel\n\u0394MAE\n          statue\n1.0\n0.8\n0.6\n0.4\n0.2\nCF-NeRF\nOurs\nEnsemble\n\u0394MAE\n    scene #0316\nMost certain\npixel\n50% percentile\nMost uncertain\npixel\nMost certain\npixel\n50% percentile\nMethod\nScene\nafrica\nbasket\nstatue\ntorch\n#0000\n#0079\n#0158\n#0316\nEnsemble\n0.18\n0.24\n0.15\n0.19\n0.28\n0.36\n0.19\n0.26\nCF-NeRF\n0.35\n0.31\n0.46\n0.97\n0.59\n0.43\n0.55\n0.54\nOurs\n0.27\n0.28\n0.17\n0.22\n0.28\n0.35\n0.20\n0.29\nFigure 6. The uncertainties computed with our algorithm on the ScanNet and Light Field dataset are significantly more calibrated to the real\nNeRF depth error than the previous state-of-the-art CF-NeRF [42], even matching the performance of extremely costly ensembles. Images\nare colored by uncertainty / depth error percentile instead of value to be comparable.\nBaselines. For Figure 6, we display sparsification curves\nderived from our uncertainty field, with the previous state-\nof-the-art CF-NeRF [42] and with the standard deviations\nobtained by the costly process of training an ensemble of\nten identical yet differently seeded NeRFs. Next to each\ngraph, we visualize the depth error together with the (as-\ncending) per-pixels rank produced by each method (i.e., the\nordering that produces the curves). It is worth noting that,\nunlike CF-NeRF [42], we do not measure disparity error due\nto its heightened sensitivity to low-range depth errors and\nsignificant underestimation of errors in distant points.\nResults.\nThe results are consistent across Figure 6.\nBayesRays\u2019s uncertainty shows significant improvement in\ncorrelation with depth error compared to CF-NeRF [42],\nboth quantitatively and qualitatively. Further, our uncertainty\nis extremely close to the standard deviation of a costly ensem-\nble in both AUSE and sparsification plots, while requiring\nno additional trained NeRFs, saving time and memory.\n5.2. NeRF Clean Up \u2013 Figures 1 and 4\nA common reconstruction artifact in NeRFs are \u201cFloaters\u201d,\noften caused by a lack of information in training data. These\ninherently correspond to regions of high uncertainty; there-\nfore, we propose removing them by thresholding the scene\naccording to our uncertainty field U during rendering.\nIn Figure 4, we compare our algorithm\u2019s performance\nto the current state of the art for post-hoc floater removal,\nNerfbusters [52], which employs a 3D diffusion model and\na \u201cvisibility mask\u201d to guide additional training steps during\nwhich some floaters are removed. For our comparison, we\nuse the same dataset proposed by Nerfbusters along with\ntheir proposed metric of Coverage, together with more com-\nmon measures of image quality. An ideal clean-up would\nboost image quality while keeping pixel coverage high.\nWhen using fixed threshold values like 0.9 or 0.4,\nBayesRays obtains similar PSNR values to Nerfbusters while\nallowing for a higher coverage. If one selects the best possi-\nble threshold value for each scene out of ten equally spaced\nones, BayesRays outperforms Nerfbusters in both PSNR\nand coverage. It is worth noting that BayesRays achieves\nwith a significantly lower computational footprint: unlike\nNerfbusters, we do not require storing and evaluating a 3D\ndiffusion model, we are faster (96 seconds vs 20 minutes) by\neliminating the need for additional training and we circum-\nvent the use of a \u201cvisibility mask\u201d altogether by storing all\nnecessary information in our computed Hessian H.\nOur qualitative results in Figure 4 show that our method\ncan filter floaters that are missed by Nerfbusters (Century),\nis not prone to sampling artifacts caused by floater re-\nmoval (Flowers) and provides the parametric flexibility nec-\nessary to avoid over-filtering (Pikachu).\n5.3. Algorithmic ablations \u2013 Figures 3, 5 and 8\nIn Section 4, we justified our introduction of the perturbation\nfield D partly through the desire to make our algorithm inde-\npendent to the specific NeRF architecture used. In Figure 3,\nwe show that this is indeed the case, as we obtain quali-\ntatively similar results for three representatively different\narchitectures (Mip NeRF [6], Instant NGP [27] and Nerfacto\n[49]) on the \u201cLego\u201d scene from the NeRF Synthetic dataset\n[24] with 60 training views from its left hemisphere.\n7\nInconsistent\nFloater\nLow Uncertainty\nTrain Views\nNovel View RGB\nNovel View Uncertainty\n+\nFigure 7. BayesRays quantifies only epistemic uncertainty in NeRF\nand is thus unable to capture aleatoric effects like those stemming\nfrom training inconsistencies.\nThis success introduces an algorithmic choice; namely,\nthe discretization of the deformation field. In Section 4, we\npropose storing it in a uniform spatial grid of size M 3 from\nwhich deformation values can be trilinearly interpolated.\nThe value of M thus becomes an algorithmic parameter,\nwhich we explore for the same example in Figure 5. We\nfind that surface uncertainty can be missed for small values\nof M, resulting in a generally more certain map that is only\nactivated on points of very high uncertainty, with diminishing\nreturns being obtained for larger, more costly M > 256.\nFinally, our algorithm\u2019s flagship application to NeRF arti-\nfact removal also contains a parameter choice, in the form\nof the uncertainty threshold. As we show in Figure 8, de-\ncreasing this parameter can a gradually clean a floater-heavy\nscene leaving a floater-free clean capture of the target object.\nSince our uncertainty field only needs to be computed once,\nwe suggest that this threshold can serve as real-time user\ncontrol in interactive NeRF setups like Nerfstudio [49].\n6. Conclusions\nWe have introduced BayesRays, an algorithm to quantify the\nuncertainty of any trained Neural Radiance Field without\nindependently of its architecture and without additional train-\ning nor access to the original training images. Our algorithm\noutputs a spatial uncertainty field, which we have shown is\nmeaningfully correlated to the NeRF depth error and can be\nthresholded for use in applications like NeRF cleanup.\nWe discretize our spatial deformation field using a uni-\nform grid, which can lead to a high memory cost being\nincurred in regions of little geometric interest. Future work\nmay significantly improve our performance by considering\nmore complex hierarchical data structures like octrees. Sep-\narately, in our algorithm\u2019s derivation, we focus only on the\ndiagonal of H, disregarding (minimal) inter-parametric cor-\nrelations. Future applications may require their inclusion,\npossibly through low-rank matrix decompositions [2, 3].\nAt a higher level, our algorithm\u2019s stated goal is to capture\nonly the epistemic uncertainty of the NeRF, often present\nthrough missing or occluded data. As such, aleatoric un-\ncertainty caused by noise or inconsistencies between views\nis not captured by our method (see Figure 7). We are opti-\nmistic that combining our work with current frameworks for\nPSNR\n% Coverage\n65\n75\n70\n80\n85\n90\n95\n100\n14.5\n15\n15.5\n16\n0.05\n0.2\n0.4\n0.6\n0.8\n\u221e\nUncertainty thresholds\nfor NeRF clean-up\nFigure 8. Applying different thresholds to uncertainty for the NeRF\nclean up task on the \u201cgarbage\u201d scene from Nerfbusters dataset [52].\nRightmost image (threshold=\u221e) shows the original.\naleatoric quantification like [22, 38] will result in a complete\nstudy of all sources of uncertainty in NeRF.\nMore broadly, our algorithm is limited to quantifying the\nuncertainty of NeRFs, and cannot be trivially translated to\nother frameworks. Nonetheless, we look forward to similar\ndeformation-based Laplace approximations being formu-\nlated for more recent spatial representations like 3D Gaus-\nsian splatting [18]. As the Deep Learning revolution takes\nComputer Vision algorithms to new horizons of performance\nand increasingly critical applications, we hope that works\nlike ours can aid in understanding what our models know\nand do not know as well as the confidence of their guesses.\nAcknowledgements\nWe would like to thank Alireza Mousavi-Hosseini,\nAgustinus Kristiadi, Towaki Takikawa, Kevin Swersky,\nRichard Szeliski, Aaron Hertzmann, Georgios Kopanas,\nVladimir Kim, and Nathan Carr for helpful discussions and\nfeedback. The third author is funded in part by an NSERC\nVanier Scholarship and an Adobe Research Fellowship.\nReferences\n[1] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Reza-\nzadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth,\nAbbas Khosravi, U Rajendra Acharya, Vladimir Makarenkov,\nand Saeid Nahavandi. A review of uncertainty quantification\nin deep learning: Techniques, applications and challenges.\narXive preprint at arXiv:2011.06225, 2021. 2\n[2] Sivaram Ambikasaran and Eric Darve. An\\mathcal o (n\\log\nn) o (n log n) fast direct solver for partial hierarchically semi-\nseparable matrices: With application to radial basis function\ninterpolation. Journal of Scientific Computing, 2013. 8\n[3] Sivaram Ambikasaran, Michael O\u2019Neil, and Karan Raj Singh.\n8\nFast symmetric factorization of hierarchical matrices with\napplications. arXiv preprint arXiv:1405.0223, 2014. 8\n[4] Murat Seckin Ayhan and Philipp Berens.\nTest-time data\naugmentation for estimation of heteroscedastic aleatoric un-\ncertainty in deep neural networks. Medical Imaging with\nDeep Learning, 2022. 2\n[5] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Es-\ntimating and exploiting the aleatoric uncertainty in surface\nnormal estimation. ICCV, 2021. 6\n[6] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neural\nradiance fields. ICCV, 2021. 7\n[7] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields. CVPR, 2021. 3\n[8] Adrien Bartoli. Towards gauge invariant bundle adjustment:\nA solution based on gauge dependent damping. ICCV, 2003.\n2\n[9] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and\nDaan Wierstra. Weight uncertainty in neural networks. ICML,\n2015. 2\n[10] Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and\nArthur Szlam.\nOptimizing the latent space of generative\nnetworks. ICML, 2019. 2\n[11] Kashyap Chitta, Aditya Prakash, and Andreas Geiger. Neat:\nNeural attention fields for end-to-end autonomous driving.\nICCV, 2021. 1\n[12] Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa\nEschenhagen, Matthias Bauer, and Philipp Hennig. Laplace\nredux \u2013 effortless bayesian deep learning. NeuRIPS, 2021. 2,\n4\n[13] John Ellis, Mary K Gaillard, Dimitri V Nanopoulos, and\nSerge Rudaz. Uncertainties in the proton lifetime. Nuclear\nPhysics B, 1980. 2\n[14] Matthew D. Hoffman, Tuan Anh Le, Pavel Sountsov, Christo-\npher Suter, Ben Lee, Vikash K. Mansinghka, and Rif A.\nSaurous. Probnerf: Uncertainty-aware inference of 3d shapes\nfrom 2d images. AISTATS, 2022. 1, 3\n[15] Eddy Ilg, \u00a8Ozg\u00a8un C\u00b8 ic\u00b8ek, Silvio Galesso, Aaron Klein, Osama\nMakansi, Frank Hutter, and Thomas Brox. Uncertainty esti-\nmates and multi-hypotheses networks for optical flow. ECCV,\n2018. 6\n[16] Liren Jin, Xieyuanli Chen, Julius R\u00a8uckin, and Marija Popovic.\nNeu-nbv: Next best view planning using uncertainty estima-\ntion in image-based neural rendering. IROS, 2023. 2\n[17] Alex Kendall and Yarin Gal. What uncertainties do we need\nin bayesian deep learning for computer vision? NeuRIPS,\n2017. 2\n[18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis. 3d gaussian splatting for real-time radiance\nfield rendering. ACM Trans. Graph., 2023. 8\n[19] Balaji Lakshminarayanan, Alexander Pritzel, and Charles\nBlundell. Simple and scalable predictive uncertainty estima-\ntion using deep ensembles. NeuRIPS, 2016. 2\n[20] Erich L. Lehmann and George Casella. Theory of Point Esti-\nmation. Springer-Verlag, second edition, 1998. 5\n[21] Sergio Naval Marimont and Giacomo Tarroni. Implicit field\nlearning for unsupervised anomaly detection in medical im-\nages. MICCAI, 2021. 1\n[22] Ricardo Martin-Brualla, Noha Radwan, Mehdi Sajjadi,\nJonathan T. Barron, Alexey Dosovitskiy, and Daniel Duck-\nworth. NeRF in the Wild: Neural radiance fields for uncon-\nstrained photo collections. CVPR, 2020. 2, 8\n[23] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view synthe-\nsis. ECCV, 2020. 2, 3\n[24] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view synthe-\nsis. ECCV, 2020. 2, 7\n[25] Miguel Monteiro, Lo\u00a8\u0131c Le Folgoc, Daniel Coelho de Castro,\nNick Pawlowski, Bernardo Marques, Konstantinos Kamnitsas,\nMark van der Wilk, and Ben Glocker. Stochastic segmentation\nnetworks: Modelling spatially correlated aleatoric uncertainty.\nAdvances in neural information processing systems, 2020. 2\n[26] Daniel Morris, Kenichi Kanatani, and Takeo Kanade. Uncer-\ntainty modeling for optimal structure from motion. Workshop\non Vision Algorithms, 1999. 2\n[27] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. SIGGRAPH, 2022. 7\n[28] Radford M. Neal. Bayesian Learning for Neural Networks.\nSpringer-Verlag, 1995. 2\n[29] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall,\nMehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan.\nRegnerf: Regularizing neural radiance fields for view synthe-\nsis from sparse inputs. CVPR, 2021. 3\n[30] Tim N Palmer. Predicting uncertainty in forecasts of weather\nand climate. Reports on progress in Physics, 2000. 2\n[31] Xuran Pan, Zihang Lai, Shiji Song, and Gao Huang. Ac-\ntivenerf: Learning where to see with uncertainty estimation.\nECCV, 2022. 1, 2\n[32] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B. Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Deformable neural radiance fields. ICCV,\n2020. 3\n[33] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. ACM Trans. Graph., 2021. 3\n[34] Keunhong Park, Philipp Henzler, Ben Mildenhall, Jonathan T.\nBarron, and Ricardo Martin-Brualla. Camp: Camera precon-\nditioning for neural radiance fields, 2023. 3\n[35] Yunlong Ran, Jing Zeng, Shibo He, Jiming Chen, Lincheng\nLi, Yingfeng Chen, Gimhee Lee, and Qi Ye. NeurAR: Neural\nuncertainty for autonomous 3d reconstruction with implicit\nneural representations. IEEE Robotics and Automation Let-\nters, 2023. 2\n[36] Daniel Rebain, Mark Matthews, Kwang Moo Yi, Dmitry\nLagun, and Andrea Tagliasacchi. Lolnerf: Learn from one\nlook. CVPR, 2022. 3\n9\n[37] Hippolyt Ritter, Aleksandar Botev, and David Barber. A\nscalable laplace approximation for neural networks. ICLR,\n2018. 2, 4, 6\n[38] Sara Sabour, Suhani Vora, Daniel Duckworth, Ivan Krasin,\nDavid J. Fleet, and Andrea Tagliasacchi. Robustnerf: Ignoring\ndistractors with robust losses. CVPR, 2023. 8\n[39] William D Schecher and Charles T Driscoll. An evaluation\nof the equilibrium calculations within acidification models:\nthe effect of uncertainty in measured chemical components.\nWater Resources Research, 1988. 2\n[40] Dario Seyb, Alec Jacobson, Derek Nowrouzezahrai, and Wo-\njciech Jarosz. Non-linear sphere tracing for rendering de-\nformed signed distance fields. ACM Trans. Graph., 2019.\n4\n[41] Jianxiong Shen, Adria Ruiz, Antonio Agudo, and Francesc\nMoreno-Noguer. Stochastic neural radiance fields: Quantify-\ning uncertainty in implicit 3d representations. 3DV, 2021. 1,\n2\n[42] Jianxiong Shen, Antonio Agudo, Francesc Moreno-Noguer,\nand Adria Ruiz. Conditional-flow nerf: Accurate 3d mod-\nelling with reliable uncertainty quantification. ECCV, 2022.\n1, 2, 6, 7\n[43] Ralph C Smith. Uncertainty quantification: theory, imple-\nmentation, and applications. Siam, 2013. 2\n[44] Masamichi Sugihara, Brian Wyvill, and Ryan Schmidt.\nWarpcurves: A tool for explicit manipulation of implicit sur-\nfaces. Computers & Graphics, 2010. 4\n[45] Richard Szeliski. Computer vision: algorithms and applica-\ntions. Springer Nature, 2022. 1, 2\n[46] Richard Szeliski and Sing Bing Kang. Shape ambiguities\nin structure from motion. Pattern Analysis and Machine\nIntelligence, IEEE Transactions on, 1997. 2\n[47] Niko S\u00a8underhauf, Jad Abou-Chakra, and Dimity Miller.\nDensity-aware nerf ensembles: Quantifying predictive un-\ncertainty in neural radiance fields. ICRA, 2023. 1, 2, 6\n[48] Andrea Tagliasacchi and Ben Mildenhall. Volume rendering\ndigest (for nerf). arXiv preprint arXiv:2209.02417, 2022. 3\n[49] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent\nYi, Terrance Wang, Alexander Kristoffersen, Jake Austin,\nKamyar Salahi, Abhik Ahuja, David Mcallister, Justin Kerr,\nand Angjoo Kanazawa. Nerfstudio: A modular framework for\nneural radiance field development. Special Interest Group on\nComputer Graphics and Interactive Techniques Conference\nConference Proceedings, 2023. 5, 6, 7, 8\n[50] Anastasia Tkach, Andrea Tagliasacchi, Edoardo Remelli,\nMark Pauly, and Andrew Fitzgibbon. Online generative model\npersonalization for hand tracking. ACM Trans. Graph., 2017.\n2\n[51] Bill Triggs, Philip F. McLauchlan, Richard I. Hartley, and\nAndrew William Fitzgibbon. Bundle adjustment - a modern\nsynthesis. Workshop on Vision Algorithms, 1999. 2\n[52] Frederik Warburg, Ethan Weber, Matthew Tancik, Aleksander\nHolynski, and Angjoo Kanazawa. Nerfbusters: Removing\nghostly artifacts from casually captured nerfs. ICCV, 2023.\n3, 7, 8\n[53] Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu,\nand Jie Zhou. Nerfingmvs: Guided optimization of neural\nradiance fields for indoor multi-view stereo. ICCV, 2021. 6\n[54] Kyle Wilson and Scott Wehrwein. Visualizing spectral bundle\nadjustment uncertainty. 2020 International Conference on 3D\nVision (3DV), 2020. 2\n[55] Zike Yan, Haoxiang Yang, and Hongbin Zha. Active neural\nmapping. arXiv preprint arXiv:2308.16246, 2023. 3\n[56] Kaan Y\u00a8ucer, Alexander Sorkine-Hornung, and Oliver Disney.\nEfficient 3 d object segmentation from densely sampled light\nfields with applications to 3 d reconstruction. ACM TOG,\n2016. 6\n[57] Sheheryar Zaidi, Arber Zela, Thomas Elsken, Christopher C.\nHolmes, Frank Hutter, and Yee Whye Teh. Neural ensemble\nsearch for uncertainty estimation and dataset shift. NeuRIPS,\n2021. 2\n[58] Huangying Zhan, Jiyang Zheng, Yi Xu, Ian Reid, and Hamid\nRezatofighi. Activermap: Radiance field for active mapping\nand planning. arXiv preprint arXiv:2211.12656, 2022. 1, 3\n[59] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\nKoltun. Nerf++: Analyzing and improving neural radiance\nfields. preprint arXiv:2010.07492, 2020. 6\n10\n"
  },
  {
    "title": "MyoDex: A Generalizable Prior for Dexterous Manipulation",
    "link": "https://arxiv.org/pdf/2309.03130.pdf",
    "upvote": "2",
    "text": "MyoDex: A Generalizable Prior for Dexterous Manipulation\nVittorio Caggiano 1 Sudeep Dasari 2 Vikash Kumar 1 2\nAbstract\nHuman dexterity is a hallmark of motor control.\nOur hands can rapidly synthesize new behaviors\ndespite the complexity (multi-articular and multi-\njoints, with 23 joints controlled by more than 40\nmuscles) of musculoskeletal sensory-motor cir-\ncuits. In this work, we take inspiration from how\nhuman dexterity builds on a diversity of prior ex-\nperiences, instead of being acquired through a\nsingle task. Motivated by this observation, we set\nout to develop agents that can build upon their\nprevious experience to quickly acquire new (pre-\nviously unattainable) behaviors. Specifically, our\napproach leverages multi-task learning to implic-\nitly capture task-agnostic behavioral priors (My-\noDex) for human-like dexterity, using a physio-\nlogically realistic human hand model \u2013 MyoHand.\nWe demonstrate MyoDex\u2019s effectiveness in few-\nshot generalization as well as positive transfer to\na large repertoire of unseen dexterous manipula-\ntion tasks. Agents leveraging MyoDex can solve\napproximately 3x more tasks, and 4x faster in\ncomparison to a distillation baseline. While prior\nwork has synthesized single musculoskeletal con-\ntrol behaviors, MyoDex is the first generalizable\nmanipulation prior that catalyzes the learning of\ndexterous physiological control across a large va-\nriety of contact-rich behaviors. We also demon-\nstrate the effectiveness of our paradigms beyond\nmusculoskeletal control towards the acquisition\nof dexterity in 24 DoF Adroit Hand.\nWebpage: https://sites.google.com/view/myodex\n1. Introduction\nHuman dexterity (and its complexity) is a hallmark of intel-\nligent behavior that set us apart from other primates species\n(Sobinov & Bensmaia, 2021). Human hands are complex\n1FAIR, Meta AI 2CMU. Correspondence to: Vittorio Caggiano\n<caggiano@fb.com>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\nand require the coordination of various muscles to impart\neffective manipulation abilities. New skills do not form by\nsimple random exploration of all possible behaviors. In-\nstead, human motor system relies on previous experience\n(Heldstab et al., 2020) to build \u201cbehavioral priors\u201d that allow\nrapid skill learning (Yang et al., 2019; Dominici et al., 2011;\nCheung et al., 2020).\nThe key to learning such a prior might reside in the complex-\nity of the actuation. Manipulation behaviors are incredibly\nsophisticated as they evolve in a high-dimensional search\nspace (overactuated musculoskeletal system) populated with\nintermittent contact dynamics between the hands\u2019 degrees\nof freedom and the object. The human motor system deals\nwith this complexity by activating different muscles as a\nshared unit. This phenomenon is known as a \u201cmuscle syn-\nergy\u201d (Bizzi & Cheung, 2013). Synergies allow the bi-\nological motor system \u2013 via the modular organization of\nthe movements in the spinal cord (Bizzi & Cheung, 2013;\nCaggiano et al., 2016) \u2013 to simplify the motor-control prob-\nlem, solving tasks by building on a limited number of shared\nsolutions (d\u2019Avella et al., 2003; d\u2019Avella & Bizzi, 2005).\nThose shared synergies are suggested to be the fundamental\nbuilding blocks for quickly learning new and more complex\nmotor behaviors (Yang et al., 2019; Dominici et al., 2011;\nCheung et al., 2020). Is it possible for us to learn similar\nbuilding blocks (i.e. a behavioral prior) for general dexter-\nous manipulation on a simulated musculoskeletal hand?\nIn this work, we develop MyoDex, a behavioral prior that\nallows agents to quickly build dynamic, dexterous, contact-\nrich manipulation behaviors with novel objects and a variety\nof unseen tasks \u2013 e.g. drinking from a cup or playing with\ntoys (see Figure 1). While we do not claim to have solved\nphysiological dexterous manipulation, the manipulation abil-\nities demonstrated here significantly advance the state of\nthe art of the bio-mechanics and neuroscience fields. More\nspecifically, our main contributions are: (1) We (for the first\ntime) demonstrate control of a (simulated) musculoskeletal\nhuman hand to accomplish 57 different contact-rich skilled\nmanipulation behaviors, despite the complexity (high de-\ngrees of freedom, third-order muscle dynamics, etc.). (2)\nWe recover a task agnostic physiological behavioral prior\n\u2013 MyoDex \u2013 that exhibits positive transfer while solving\nunseen out-of-domain tasks. Leveraging MyoDex, we are\nable to solve 37 previously unsolved tasks. (3) Our ablation\n1\narXiv:2309.03130v1  [cs.RO]  6 Sep 2023\nMyoDex: A Generalizable Prior for Dexterous Manipulation\nFigure 1. Contact rich manipulation behaviors acquired by MyoDex with a physiological MyoHand\nstudy reveals a tradeoff between the generality and special-\nization of the MyoDex prior. The final system is configured\nto maximize generalization and transfer instead of zero-shot\nout-of-the-box performance. (4) We demonstrate the gen-\nerality of our approach by applying it to learn behaviors in\nother high-dimensional systems, such as multi-finger robotic\nhands. We construct AdroitDex (equivanet to MyoDex for\nthe AdroitHand (Kumar, 2016)), which achieves 5x bet-\nter sample efficiency over SOTA in the TCDM benchmark\n(Dasari et al., 2023).\n2. Related Works\nDexterous manipulations has been approached indepen-\ndently by the biomechanics field to study the synthesis of\nmovements of the overactuated musculoskeletal system, and\nroboticists looking to develop, mostly via data-driven meth-\nods, skilled dexterous robots and a-priori representations\nfor generalizable skill learning. Here, we discuss those\napproaches.\nOver-redundant biomechanic actuation. Musculoskele-\ntal models (McFarland et al., 2021; Lee et al., 2015; Saul\net al., 2015; Delp et al., 2007; Seth et al., 2018) have been\ndeveloped to simulate kinematic information of the muscles\nand physiological joints. Nevertheless, the intensive compu-\ntational needs and restricted contact forces have prevented\nthe study of complex hand-object interactions and otherwise\nlimited the use mostly to optimization methods. Recently,\na new hand and wrist model \u2013 MyoHand (Caggiano et al.,\n2022a; Wang et al., 2022) \u2013 overcomes some limitations of\nalternative biomechanical hand models: allows contact-rich\ninteractions and it is suitable for computationally intensive\ndata-driven explorations. Indeed, it has been shown that\nMyoHand can be trained to solve individual in-hand tasks\non very simple geometries (ball, pen, cube) (Caggiano et al.,\n2022a;b). Here, we leveraged and extended the MyoHand\nmodel to perform hand-object manouvers on a large variaty\nof complex realistic objects.\nFigure 2. MyoHand - Musculoskeletal Hand model(Caggiano\net al., 2022a). On the left, rendering of the musculoskeletal struc-\nture illustrating bone \u2013 in gray \u2013 and muscle \u2013 in red. On the right a\nskin-like surface for soft contacts is overlaid to the musculoskeletal\nmodel.\nBehavioral synthesis. Data-driven approaches have con-\nsistently used Reinforcement Learning (RL) on joint-based\ncontrol to solve complex dexterous manipulation in robotics\n(Rajeswaran et al., 2018; Kumar et al., 2016; Nagabandi\net al., 2019; Chen et al., 2021). In order to yield more\nnaturalistic movements, different methods have leveraged\nmotion capture data (Merel et al., 2017; 2019; Hasenclever\net al., 2020). By means of those approaches, it has been pos-\nsible to learn complex movements and athletic skills such\nas high jumps (Yin et al., 2021), boxing and fencing (Won\net al., 2021) or playing basketball (Liu & Hodgins, 2018).\nIn contrast to joint-based control, in biomechanical models,\nmachine learning has been applied to muscle actuators to\ncontrol movements and produce more naturalistic behav-\niors. This is a fundamentally different problem than robotic\ncontrol as the overactuated control space of biomechanical\nsystems leads to ineffective explorations (Schumacher et al.,\n2022). Direct optimization (Wang et al., 2012; Geijtenbeek\net al., 2013; Al Borno et al., 2020; R\u00a8uckert & d\u2019Avella, 2013)\nand deep reinforcement learning (Jiang et al., 2019; Joos\net al., 2020; Schumacher et al., 2022; Ikkala et al., 2022;\nCaggiano et al., 2022a; Wang et al., 2022; Song et al., 2020;\nPark et al., 2022) have been used to synthesize walking and\nrunning, reaching movements, in-hand manipulations, biped\n2\nMyoDex: A Generalizable Prior for Dexterous Manipulation\nlocomotion and other highly stylistic movements (Lee et al.,\n2018; 2019). Nevertheless, complex dexterous hand-object\nmanipulations beyond in-hand object rotation (Caggiano\net al., 2022a; Berg et al., 2023) have not been accomplished\nso far.\nManipulation priors. Previous attempts have tried to solve\ncomplex tasks by building priors but this approach has been\nlimited to games and robotics. The idea of efficiently rep-\nresenting and utilizing previously acquired skills has been\nexplored in robotics by looking into features across dif-\nferent manipulation skills e.g. Associative Skill Memo-\nries (Pastor et al., 2012) and meta-level priors (Kroemer &\nSukhatme, 2016). Another approach has been to extract\nmovement primitives (Rueckert et al., 2015) to identify a\nlower-dimensionality set of fundamental control variables\nthat can be reused in a probabilistic framework to develop\nmore robust movements.\nMulti-task learning, where a model is trained on multiple\ntasks simultaneously (Caruana, 1997), has been also shown\nto improve the model\u2019s ability to extract features that gener-\nalize well (Zhang & Yeung, 2014; Dai et al., 2016; Liu et al.,\n2019). Multi-task reinforcement learning (RL) has been\nused in robotics to propose representations-based methods\nfor exploration and generalization in games and robotics\n(Goyal et al., 2019; Hausman et al., 2018). However, train-\ning on multiple tasks can lead to negative transfer. As a con-\nsequence, performance on one task is negatively impacted\nby training on another task (Sun et al., 2020). Neverthe-\nless, it has been argued that in (over)redundant control such\nas the physiological one, multi-task learning might facili-\ntate learning of generalizable solutions (Caruana, 1997). In\nthis work, in addition to showing that nimble contact-rich\nmanipulation using detailed physiological hand with mus-\nculoskeletal dynamics is possible, we present evidence that\na generalizable physiological representation via Multi-task\nreinforcement learning \u2013 MyoDex \u2013 can be acquired and\nused as priors to facilitate both learning and generalization\nacross complex contact rich dexterous tasks.\n3. Overactuated Physiological Dexterity\nHuman hand dexterity builds on the fundamental character-\nistics of physiological actuation: muscles are multi-articular\nand multi-joints, the dynamics of the muscle are of the third\norder, muscles have pulling-only capabilities, and effectors\nhave intermittent contact with objects. To further our un-\nderstanding of physiological dexterity, we embed the same\ncontrol challenges \u2013 by controlling a physiologically accu-\nrate musculoskeletal model of the hand (see Sec. 3.1) \u2013 in\ncomplex manipulation tasks (see Sec. 3.2).\n3.1. MyoHand: A Physiologically Accurate Hand Model\nIn order to simulate a physiologically accurate hand model,\na complex musculoskeletal hand model comprised of 29\nbones, 23 joints, and 39 muscles-tendon units (Wang et al.,\n2022) \u2013 MyoHand model \u2013 implemented in the MuJoCo\nphysics simulator (Todorov et al., 2012) was used (see Fig-\nure 2). This hand model has previously been shown to ex-\nhibit a few dexterous in-hand manipulation tasks (Caggiano\net al., 2022a), which makes it a good candidate for our study\nseeking generalization in dexterous manipulation.\nWe extended the MyoHand model to include translations\nand rotations at the level of the shoulder. We limited the\ntranslation on the frontal (range between [\u22120.07, 0.03]) and\nlongitudinal (range between [\u22120.05, 0.05]) axis to support\nthe natural shoulder and wrist rotation (elbow is considered\nmaximally extended i.e. a straight arm). For the rest of the\npaper we will refer to the whole system as MyoHand.\n3.2. Dexterous Behaviors Studied\nIn this study, we need a large variability of manipulations to\nexplore the generality of our method against a wide range\nof solutions, hence it was important to include 1) objects\nwith different shapes, and 2) complexity in terms of desired\nbehaviors requiring simultaneous effective coordination of\nfinger, wrist, as well as arm movements.\nOur task set MyoDM (inspired by TCDM benchmarks\n(Dasari et al., 2023)) is implemented in the MuJoCo physics\nengine (Todorov et al., 2012) and consists of 33 objects\nand 57 different behaviors. Every task setup (see Figure 3)\nconsists of a tabletop environment, an object from the Con-\ntactDB dataset (Brahmbhatt et al., 2019), and the MyoHand.\nDexterous manipulation is often posed as a problem of\nachieving the final desired configuration of an object. In\naddition to the final posture, in this study, we are also in-\nterested in capturing the detailed temporal aspect of the\nentire manipulation behavior. Tasks like drinking, playing,\nor cyclic movement like hammering, sweeping, etc., that\nare hard to capture simply as goal-reaching, can be handled\nby our formulation (Sec. 4) and are well represented in the\nMyoDM.\nThe tasks considered in MyoDM entail a diverse variety of\nobject manipulation (relocations+reorientations) behaviors\nrequiring synchronized coordination of arm, wrist, as well\nas in-hand movements to achieve the desired object behav-\niors involving simultaneous translation as well as rotation\n(average \u00b1 std, 28\u25e6 \u00b1 21\u25e6). The range of motions of the\nshoulder with fixed elbow alone is not sufficient to enable\nthe entire range of desired object rotations without involv-\ning in-hand and wrist maneuvers. The angle between the\npalm and object ranges upwards of 20\u25e6 in our final acquired\nbehaviors. The wrist is one of the most complex joints\n3\nMyoDex: A Generalizable Prior for Dexterous Manipulation\nFigure 3. Task setup and a subset of object-hand pair from our task-set. Every task setup consisted of a tabletop environment, an\nobject, and the MyoHand. The MyoHand was shaped with a compatible posture and positioned near an object (i.e. pre-grasp posture).\nSingle Task Framework\n\u03c01\nObject \ntrajectory\nMulti-Task Framework\nMulti-task Agent \n(\u03c0#)\nReplay Buffer\nParallel rollouts\nPPO\ne2\neN\ne1\nExpert Agent\ne1\nReplay Buffer\nMyoHand\npre-shape\nPPO\nFigure 4. Learning Frameworks. Left - Single Task Framework: policies were obtained by training policies to solve the individual tasks.\nRight - Multi-task framework: A single policy (MyoDex) was obtained by learning all tasks at once.\nto control because it is affected simultaneously by the bal-\nanced activation of more than 20 muscles whose activations\nalso control finger movements. Careful maneuvering of\nobjects within the hand requires simultaneous synchroniza-\ntion of numerous antagonistic finger muscle pairs, failing\nwhich leads to loss of object controllability; highlighting the\ncomplexities of controlling a physiological musculoskeletal\nhand during these complex manipulations.\n4. Learning Controllers for Physiological\nHands\nIn this section, we discuss our approach to build agents that\ncan learn contact-rich manipulation behaviors and general-\nize across tasks.\n4.1. Problem formulation\nA manipulation task can be formulated as a Markov Deci-\nsions Process (MDP) (Sutton & Barto, 2018) and solved\nvia Reinforcement Learning (RL). In RL paradigms, the\nMarkov decision process is defined as a tuple M =\n(S, A, T , R, \u03c1, \u03b3), where S \u2286 Rn and A \u2286 Rm represents\nthe continuous state and action spaces respectively. The un-\nknown transition dynamics are described by s\u2032 \u223c T (\u00b7|s, a).\nR : S \u2192 [0, Rmax], denotes the reward function, \u03b3 \u2208 [0, 1)\ndenotes the discount factor, and and \u03c1 the initial state dis-\ntribution. In RL, a policy is a mapping from states to a\nprobability distribution over actions, i.e. \u03c0 : S \u2192 P(A),\nwhich is parameterized by \u03b8. The goal of the agent is\nto learn a policy \u03c0\u03b8(a|s) = argmax\u03b8[J(\u03c0, M)], where\nJ = max\u03b8 Es0\u223c\u03c1(s),a\u223c\u03c0\u03b8(at|st)[P\nt R(st, at)]\n4\nMyoDex: A Generalizable Prior for Dexterous Manipulation\n4.2. Learning Single-Task Controllers\nSingle task agents. The single task agents are tasked with\npicking a series of actions ([a0, a1, ..., aT ]), in response of\nthe evolving states ([s0, s1, ..., sT ]) to achieve their corre-\nsponding object\u2019s desired behavior \u02c6Xobject = [\u02c6x0, ..., \u02c6xT ].\nWe adopt a standard RL algorithm PPO (Schulman\net al., 2017) to acquire a goal-conditioned policy\n\u03c0\u03b8(at|st, \u02c6Xobject) as our single task agents. Details on state,\nactions, rewards, etc are provided in Section 5. Owing to\nthe third-order non-linear actuation dynamics and high di-\nmensionality of the search space, direct optimization on M\nleads to no meaningful behaviors.\nPre-grasps implicitly incorporate information pertaining to\nthe object and its associated affordance with respect to the\ndesired task (Jeannerod, 1988; Santello et al., 2002). We\nleveraged (Dasari et al., 2023)\u2019s approach of leveraging pre-\ngrasp towards dexterous manipulation with robotic (Adroit\n(Kumar, 2016)) hand and extend it towards MyoHand. The\napproach uses the hand-pose directly preceding the initiation\nof contact with an object i.e. a proxy to pre-grasp, to guide\nsearch in the high dimensional space in which dexterous\nbehaviors evolve. This approach yeilds a set of single-task\nexpert agents \u03c0i with i \u2208 I where I is the set of tasks (see\nFigure 4-left).\n4.3. Framework for Multi-Task Physiological Learning\nMulti-task agent. Ideally, an agent would be able to solve\nmultiple tasks using a goal-conditioning variable. Thus, we\nadditionally train a single agent to solve a subset of tasks\nin parallel (see Figure 4-right). This approach proceeds\nin a similar fashion as the single-task learner, but agent\u2019s\nexperiences are sampled from the multiple tasks in paral-\nlel. All other details of the agent \u03c0#\n\u03b8 (at|st, \u02c6Xobject) (e.g.\nhyperparameters, algorithm, etc.) stay the same.\nSimilar to the single task agents, we encode manipu-\nlation behaviors in terms of goal-conditioned policies\n\u03c0\u03b8(at|st, \u02c6Xobject) and employ a standard implementation\nof the PPO (Schulman et al., 2017) from Stable-Baselines\n(Raffin et al., 2021) and pre-grasp informed formulation\nfrom (Dasari et al., 2023)\u2019s to guide the search for our multi-\ntask agents as well. See Section A.4.2 for details. The\nhyperparameters were kept the same for all tasks (see Ap-\npendix Table A.1).\n5. Task Details\nNext, we provide details required to instantiate our MyoDM\ntask suite\u2013\nState Space. The state vector st = {\u03d5t, \u02d9\u03d5t, \u03c8t, \u02d9\u03c8t, \u03c4t}\nconsisted of \u03d5 a 29-dimensional vector of 23 hand and 6\narm joints and velocity \u02d9\u03d5, and object pose \u03c8 and velocity\n\u02d9\u03c8. In addition, positional encoding \u03c4 (Vaswani et al., 2017),\nused to mark the current simulation timestep, was appended\nto the end of the state vector. This was needed for learning\ntasks with cyclic motions such as hammering.\nAction Space. The action space at was a 45-dimensional\nvector that consists of continuous activations for 39 muscles\nof the wrist and fingers (to contract muscles), together with\n3D translation (to allow for displacement in space), and 3D\nrotation of the shoulder (to allow for a natural range of arm\nmovements).\nReward Function. The manipulation tasks we consider\ninvolved approaching the object and manipulating it in free\nair after lifting it off a horizontal surface. The hand interacts\nwith the object adjusting its positions and orientation (X =\n[x0, ..., xT ]) for a fixed time horizon. Similar to (Dasari\net al., 2023), this is translated into an optimization problem\nwhere we are searching for a policy that is conditioned on\ndesired object trajectory \u02c6X = [\u02c6x0, ..., \u02c6xT ] and optimized\nusing the following reward function:\nR(xt, \u02c6xt) := \u03bb1exp{\u2212\u03b1\u2225x(p)\nt\n\u2212 \u02c6x(p)\nt \u22252\u2212\n\u03b2|\u2220x(o)\nt\n\u2212 \u02c6x(o)\nt |} + \u03bb21{lifted} \u2212 \u03bb3 \u2225mt\u22252\n(1)\nwhere \u2220 is the quaternion angle between the two orienta-\ntions, \u02c6x(p)\nt\nis the desired object position, \u02c6x(o)\nt\nis the desired\nobject orientation, 1{lifted} encourages object lifting, and\nmt the is overall muscle effort.\nProgress metrics. To effectively capture the temporal be-\nhaviors, we treat dexterous manipulation as a task of re-\nalizing desired object trajectories ( \u02c6X). To capture tem-\nporal progress, similar to (Dasari et al., 2023), we use\nthree metrics to measure task performance. The success\nmetric, S( \u02c6X) reports the fraction of time steps where ob-\nject error is below a \u03f5 = 1cm threshold. It is defined\nas: S( \u02c6X) =\n1\nT\nPT\nt=0 1\n\r\r\rx(p)\nt\n\u2212 \u02c6x(p)\nt\n\r\r\r\n2 < \u03f5.\nThe ob-\nject error metric E( \u02c6X) calculates the average Euclidean\ndistance between the object\u2019s center-of-mass position and\nthe desired position from the desired trajectory: E( \u02c6X) =\n1\nT\nPT\nt=0\n\r\r\rx(p)\nt\n\u2212 \u02c6x(p)\nt\n\r\r\r\n2. In addition, we also used the ob-\nject orientation metric: O( \u02c6X) = 1\nT \u2220T\nt=0(x(o)\nt\n\u2212 \u02c6x(o)\nt ) 1.\n6. Results\nFirst, we study if we can solve the MyoDM task set, one\ntask at a time (see Sec. 6.1). Next, we illustrate that our My-\noDex representation can be used as a prior for accelerating\n1For interpretability, we often omit orientations because center-\nof-mass error and orientation error were highly correlated in prac-\ntice i.e. Pearson-correlation > 0.785\n5\nMyoDex: A Generalizable Prior for Dexterous Manipulation\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMaximum Success Rate\n0\n5\n10\n15\n20\n25\nNumber of tasks\nFigure 5. Distribution of single task solutions. Distribution of\nmaximums success rate for single-task solutions on 57 different\ntasks. Only 32 out of 57 tasks i.e. 56%, were solved with a success\nrate above 80%. Training performed over 12.5k iterations.\nlearning novel, out-of-domain tasks (see Sec. 6.2). Finally,\nwe present a series of ablation studies to understand various\ndesign choices of our approach (see Sec. 6.4).\n6.1. Learning Expert Solutions for Single-Task Setting\nWe begin by asking, is it possible to learn a series of complex\ndexterous manipulation behaviors (see Sec. 3.2) using a\nMyoHand? Our single-task learning framework is applied\nto solve a set of 57 MyoDM tasks independently, without any\nobject or task-specific tuning (see Table A.1). The resulting\n\u201cexpert policies\u201d were able to properly manipulate only a\nsubset of those objects, while moving and rotating them to\nfollow the target trajectory (see Figure 1 for a sequence of\nsnapshots). This was quantified by using 2 metrics (Sec.\n5): a Success Metric and an Error Metric. Our single-task\nframework achieves an average success rate of 66% solving\n32 out of 57 tasks (see Fig. 5 and experts in Fig. A.3) and an\naverage (ecludean distance) error of 0.021. We encourage\nreaders to check our project website for videos and further\nqualitative analysis of the learned behaviors.\nFigure 6. Zero-shot generalization. MyoDex successfully initi-\nated manipulations on new objects and trajectories. Hand rendering\nincludes skin-like contact surfaces (see Fig. 2)\n6.2. Accelerating Out-of-Domain Learning via MyoDex\nThe single-task framework was not able to solve all task\nin our task set, even individually which further establishes\ncomplexity of behavior acquisition with high DoF MyoHand\nand the difficulty of our MyoDM task set. Furthermore, it\ncreates controllers that can only function within a specific\nscenario/task. Next, we will demonstrate that by simultane-\nously training on multiple tasks during the reinforcement\nlearning loop we can achieve a MyoDex prior that can over-\ncome single-task limitations. MyoDex is a prior that can\nbe fine-tuned to solve a larger amount of tasks. In addi-\ntion, a single multi-task policy based on training MyoDex at\nconvergence can solve multiple tasks.\nFor building the MyoDex prior, we consider a subset of 14\nMyoDM tasks with a large variability of object and move-\nments (see Sec. 6.4.3 for the effects of task choice) and we\ntrained one policy to solve all the set of tasks at once. We\nstopped the training at 12.5k iterations (at the beginning of\nthe error plateau \u2013 see Figure A.1). At this iteration, we\ntested potential zero-shot generalization capabilities with\nthe MyoHand positioned near a novel object with a compati-\nble posture and conditioned on a new task trajectory. While\nthe policy was not able to zero-shot solve these new tasks\n(success rate \u2264 10%), we do observe (see Fig. 6) that the\nhand can succesfully grasp and lift the unseen objects. This\nleads us to believe that the MyoDex representation can be\nused as a prior for accelerating transfer learning.\nHowever, this is not the only way to accomplish a gen-\neral multi-task representation. An established baseline is a\nstudent-teacher distillation (see Sec. A.1), which trains a\nsingle student policy to imitate the 14 expert policies (from\nprior experiments) via behavior cloning.\nWe fine-tune both the MyoDex and the student policy on\nthe remaining out-of-domain set of 43 MyoDM tasks (using\nsingle-task RL) for additional iterations. Figure 7 presents\nlearning curves for the fine-tuned models based on Myo-\nDex, fine-tuned student baselines, and trained (from scratch)\nsingle-task expert policies in terms of success rate and errors,\nrespectively. Note how the MyoDex based policy is able to\nlearn the tasks significantly faster than either the baseline or\nthe single-task policies. Among the solved out-of-domain\ntasks, MyoDex based policies were about 4x faster than\nstudent based policy (1.9k vs 7.7k), and approximately 3x\nfastern than single-task expert policy (1.9k vs 5.7k, Table 1).\nAdditionally, it achieves a higher overall task performance\nin comparision to the single-task experts, which plateau at\na significantly lower success rate, likely due to exploration\nchallenges. Table 1 shows this trend in extra detail. The\nMyoDex representation allows to solve more tasks (37 vs\n22, see Table 1 and Table A.2) and achieve a higher over-\nall success rate (0.89 vs 0.69) than the single-task expert,\nwhich in turn outperforms the student baseline. This leads\nus to conclude that the MyoDex representation can act as\na generalizable prior for learning dexterous manipulation\npolicies on a musculoskeletal MyoHand. It is both able\n6\nMyoDex: A Generalizable Prior for Dexterous Manipulation\n0k\n1k\n3k\n4k\n5k\nEnvironment Iteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Metric\nconverged\nExpert\nStudent\nMyoDex\n0k\n1k\n3k\n4k\n5k\nEnvironment Iteration\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nError Metric\nExpert\nStudent\nMyoDex\nFigure 7. Fine-tuning on 43 Out-of-domain tasks. Metrics until 5k iterations of the fine tuning of 43 out-of-domain tasks. Convergence\nis assumed at 12.5k iterations. Left - Success Metric. Right - Error Metric. Continuous lines show average and shaded areas the standard\ndeviation of success and error metrics. The dashed line represents the value at convergence i.e. 12.5k iterations.\nsubstantially accelerate learning new tasks, and indeed leads\nto a stronger transfer to new tasks.\nBased on\nSolved\nSuccess\nIter. to solve\nExpert\n51% (22/43)\n0.69 \u00b1 0.30\n5.7k \u00b1 1.5k\nStudent\n30% (13/43)\n0.54 \u00b1 0.35\n7.7k \u00b1 1.9k\nMyoDex\n86% (37/43)\n0.89 \u00b1 0.25\n1.9k \u00b1 2.1k\nTable 1. MyoDex transfer statistics on unseen (43) tasks \u2013 Solved\nindicates the percentage (ratio) of solved tasks (success \u2265 80%).\nSuccess indicates the success metric stats on all 43 tasks at 12.5k\niterations. Iter. to solve indicates the stats on min iterations re-\nquired by the solved task to achieve \u2265 80% success. Values are\nexpressed as average \u00b1 std.\n6.3. Multi-Task Learning with MyoDex\nAdditionally, MyoDex can also be used to recover one sin-\ngle policy that can solve multiple tasks. We compared the\nresults of the MyoDex training at convergence against the\nstudent policy (from the distillation of experts) on the same\nset of 14 tasks. See a summary of the results Figure A.2. The\nconverged MyoDex based policy\u2019s success rate improves by\n> 2x over the student policy. We present an explanation in\nSection 8 of why distilling from experts that have acquired\nincompatible behaviors in an over-redundant musculoskele-\ntal system fails at learning multi-task policies. Indeed, ex-\npert policies found a local solution that does not help to\nlearn other tasks e.g. experts used as a-priori do not help to\nfine-tune other tasks (see Fig. A.5). In contrast, our multi-\ntask framework avoids this pitfall, since it simultaneously\nlearns one policy without any implicit bias, and can reach\nsimilar levels as reached by individual experts in isolation.\n6.4. MyoDex Ablation Study\nThe previous set of experiments demonstrated that MyoDex\ncontains generalizable priors for dexterous manipulation.\nThe following ablation study investigates how changing the\nnumber of pre-training iterations as well as the number of\ntasks during pre-training affect the MyoDex\u2019s capabilities.\n6.4.1. EFFECTS OF ITERATIONS ON THE MyoDex\nREPRESENTATION\nIn our experiment, the multi-task policy at 12.5k iterations\nis defined as the MyoDex prior. At this number of itera-\ntions, the policy was able to achieve \u223c 35% success rate\n(see Fig. A.1). This solution provided both few-shot learn-\ning (task solved within the first environment iteration) most\nof the MyoDM set of 57 tasks. Here, in order to probe\nthe sensitivity of MyoDex prior to the stage of learning at\nwhich the representation is extracted, we compared Myo-\nDex against representations obtained earlier i.e. 2.5k and\n7.5k, and one later i.e. 37.5k stages of learning. Figure 8\nshows the results on the fine-tuning of all the 57 tasks for the\n4 different representations. Early representations are slower\nbut, with enough iterations, they are able to solve almost all\ntasks (98% (56 / 57) and 91% (52 / 57) respectively for the\nrepresentations at 2.5k and 7.5k). Conversely, later repre-\nsentations, show few-shot learning (10 tasks) but they are\nable to learn only a reduced amount of tasks (61% (35 / 57)).\nHence, MyoDex trained at 12.5k iterations strikes a bal-\nance, facilitating fast initial learning (including few-shots)\nwhile being general enough to support a diverse collection\nof out-of-domain tasks (see Figure 8).\nAnother way to look at the effect of learning and generaliz-\nable solutions over the iterations is to look to muscle syn-\nergies as they express the amount of muscle co-contraction\nshared across tasks. In our study, we utilized the concept of\nVariance Accounted For (VAF, see Sec. A.4) to quantify the\nnumber of synergies needed to reconstruct the needed mus-\ncle activations to solve the task. Higher VAF achieved with\nfewer muscle synergies indicates that it is possible to use\nfewer combinations of muscle co-contractions to generate\nthe needed muscle activations. Our findings indicate that\n7\nMyoDex: A Generalizable Prior for Dexterous Manipulation\nearly on in the training process (i.e., around 2.5k iterations,\nsee Figure A.8), a substantial number of synergies (more\nthan 12) is needed to achieve a high level of signal recon-\nstruction. This suggests that while the policy is capable of\ndiscovering some solutions in the muscle space, synergies\nare not able to cover all the variability of the signal. Indeed,\nthis representation helps to overcome some local minima\nhence it is particularly well-suited for facilitating transfer to\nnew tasks.\nAround 12.5k iterations, we observed a peak in the capacity\nof fewer synergies to account for most of the signal (see\nFigure A.8). At this point we have identified solutions in the\nmuscle space that are highly reusable across multiple tasks.\nHowever, at 37.5k iterations, we found that a greater number\nof synergies were required to explain most of the original\nsignals. This indicates that specialized co-contractions are\nemerging to address specific tasks demands. While these\nsynergies are effective at solving similar tasks with few or\nzero shots, their specialization may limit their ability to\ntackle dissimilar tasks.\nOverall, our results suggest that our representation of syn-\nergies is a powerful tool for facilitating transfer learning,\nespecially in the early stages of training when more gener-\nalized solutions are required. As training progresses, the\nemergence of specialized co-contractions enables efficient\nlearning and transfer to similar tasks. Still, with even more\ntraining, specialized solutions are developed.\n0k\n1k\n3k\n4k\n5k\nEnvironment Iteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Metric\nconverged\nMyoDex@2.5k\nMyoDex@7.5k\nMyoDex@12.5k\nMyoDex@37.5k\nFigure 8. Fine-tuning from representations obtained at differ-\nent iterations. Fine-tuning from representations obtained earlier\ni.e. 2.5k and 7.5k iterations, MyoDex i.e. 12.5k iterations, and\nlater i.e. 37.5k iterations. Earlier representations show no few-shot\ngeneralization but better coverage with 56 out of 57 tasks solved,\nwhile later representations show few-shot generalizations but have\nless coverage with 35 out of 57 tasks solved. The continuous line\nrepresents the average and the shaded area is the standard deviation\nof the success metrics. The dashed line represents the value at\nconvergence i.e. 12.5k iterations.\n6.4.2. EFFECT OF THE NUMBER OF ENVIRONMENTS ON\nMyoDex TRAINING\nIn the above experiment, we showed the MyoDex represen-\ntation based on 14 environments. An analysis showing the\neffect of multi-task learning on environment diversity illus-\ntrates that the use of 14 environments represented a balance\nbetween trainign the multi-task policy effectively and trans-\nfer/generalization ability it possses. We compared MyoDex\ntrained on 6, 14, and 18 environments at 12.5k iterations\nand tested on a set of 39 new environments. MyoDex based\non 6 and 18 environments leads to lower performance with\nrespect to 14 environments both in terms of success rate and\nthe number of solved environments (see Table 2).\nBased on\nSuccess\nSolved\nMyoDex6\n0.78 \u00b1 0.32\n72% (28/39)\nMyoDex14\n0.92 \u00b1 0.21\n95% (37/39)\nMyoDex18\n0.91 \u00b1 0.2\n87% (34/39)\nTable 2. Fine-tuning statistics based on different MyoDex pri-\nors. MyoDex trained with different environments as priors and\nfine-tuned on 39 environments. Results reported in terms of av-\nerage and standard deviation of success and percentage of solved\ntasks i.e. \u2265 80%.\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nSTD Position\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nSTD Rotation\nPre-training task distribution\nOrig. (diverse)\nAltern. 1 (diverse)\nAltern. 2 (homogenous)\nAll\nFigure 9. Pre-Training task distribution.\nThe distributions of\nour task collection in terms of its variability (standard deviation\n- STD). Represented on each axes are the STD of the absolute\npositional (X-Axis) and rotational (Y-axis) displacements from the\nrespective initial object poses in the desired object trajectories in\nour task set. In circle are all the 57 tasks involved in our study\nIn pink [Orig.(diverse)] are the original tasks used for training\nMyoDex. In blue [Altern.1(diverse)] is a new task set we use for\ntraining an alternate instance of MyoDex prior used in ablation\nstudies.\n6.4.3. HOW TRAINING TASKS AFFECT MyoDex\nThe choice of objects and tasks to train MyoDex can signif-\nicantly impact the effectiveness of the representation. We\nstudy the effect of pre-training task distribution on the effec-\n8\nMyoDex: A Generalizable Prior for Dexterous Manipulation\n0k\n3k\n5k\n8k\n10k\n13k\nSteps\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Metric\nFineTuning MyoDex with different envs\nMyoDex\nMyoDex Alt Homogenous\nMyoDex Alt Diverse\nFigure 10. Effect of pre-training task distribution on MyoDex\nperformance. MyoDex Alt Diverse (trained on tasks of similar\ndiversity \u2013 in blue) is able to better match the original MyoDex\nperformance in comparision to MyoDex Alt Homogenous (trained\non homogenous tasks collection).\ntiveness of MyoDex priors. We selected two new task sets.\nFirst, a diverse tasks collection \u2013 MyoDex Alt Diverse (Fig-\nure 9 in blue) with the same similar attributes of the original\ndataset (in pink). Second, a homogenous task collection \u2013\nMyoDex Alt Homogenous (Figure 9 in red) \u2013 with tasks with\nlittle motion variance (e.g. mostly lifting). We found that\nMyoDex Alt Diverse \u2013 trained on the alternative diverse tasks\n\u2013 was able to improve performance over time, while MyoDex\nAlt Homogenous \u2013 trained on the alternative homogenous\ntasks \u2013 had its performance plateau early on during training\n(see Figure A.7). Indeed, when used for transfer on new\ntasks, MyoDex Alt Diverse is able to match the original My-\noDex performance, while MyoDex Alt Homogenous does\nnot (see Figure 10). This shows that the variety of manip-\nulation/tasks in the pretraining is fundamental to achieve\nhigh performance in a larger set of downstream tasks and\nMyoDex is not sensitive to the specific choice of tasks.\n6.5. Extension to other high dimensional system\nTo further investigate the applicability of our approach to\nother high dimensional systems, we set out to build a gen-\neralizable representation for the robotic Adroit Hand (Ra-\njeswaran et al., 2018) commonly studied in robot learning.\nAdroit is a 24 degree-of-freedom (DoF) modified shadow\nhand with 4 extra DoF at the distal joint. Following the\napproach behind MyoDex, a general representation of ma-\nnipulation prior - AdroitDex - was obtained. We use the\nsame 14 tasks that we used for training MyoDex. In the\nFigure 11 we show the performance of AdroitDex on 34\nunseen tasks on the TCDM benchmark (Dasari et al., 2023).\nAdroitDex achieves a success rate of 74.5% in about 10M\niteration steps, which is approvimately 5x faster than the\nPGDM baseline (Dasari et al., 2023), which needed 50M\niteration steps to achieve the same result (see Figure 11).\nFigure 11.\nFine-tuning a generalizable representation on\nAdroit subtasks: AdroitDex. A general representation of ma-\nnipulation on the same 14 tasks used for trainign MyoDex was\nfinetuned on 34 unseen tasks on the TCDM benchmark (Dasari\net al., 2023). Curves shows average (continuous) and std (shaded\narea). AdroitDex beats previously reported SOTA on TCDM bench-\nmarks while being 5x more sample efficient.\n7. Conclusion\nIn this manuscript, we learn skilled dexterous manipula-\ntion of complex objects on a musculoskeletal model of the\nhuman hand. In addition, by means of joint multi-task learn-\ning, we showed that it is possible to extract generalizable\nrepresentations (MyoDex) which allow faster fine-tuning on\nout-of-domain tasks and multi-task solutions. Ultimately,\nthis study provides strong bases for how physiologically\nrealistic hand manipulations can be obtained by pure explo-\nration via Reinforcement Learning i.e. without the need for\nmotion capture data to imitate specific behavior.\n8. Discussion on the role of Synergies\nWhy does MyoDex help the overactuated musculoskeletal\nsystem to solve multiple tasks? If we look at the coordina-\ntion of muscle activations \u2013 muscle synergies (see Appendix\nA.4) \u2013 we notice that MyoDex shows a larger number of sim-\nilar activations (see Figure A.4) vs experts/distilled policies.\nThis is because the expert solutions find one mode/solution\nto solve a task that does not incorporate information from\nother tasks. Naiive distillation propogates this effect to\nthe student policy. In contrast, MyoDex learns to coordi-\nnate muscle contraction. Indeed, fewer muscle coordina-\ntion/synergies seem to explain most of the behavior (see\nFigure A.8, at 12.5K iterations). All in all, those obser-\nvations are in line with the neuroscience literature where\nmuscle synergies have been suggested as the physiological\nsubstrate to obtain faster and more effective skill transfer\n(Yang et al., 2019; Cheung et al., 2020; Dominici et al.,\n2011; Berger et al., 2013).\n9\nMyoDex: A Generalizable Prior for Dexterous Manipulation\n9. Limitations and Future work\nWhile we demonstrated that MyoDex can produce realistic\nbehavior without human data, one important limitation is\nunderstanding and matching the results with physiological\ndata. Indeed, our exploration method via RL produced\nonly one of the very high dimensional combinations of\npossible ways that a human hand could hypothetically grab\nand manipulate an object. For example, there are several\nvalid ways to hold a cup e.g. by using the thumb and one\nor multiple fingers. Although our investigation points us in\nthe right direction regarding the physiological feasibility of\nthe result, these findings have yet to be properly validated\nwith clinical data and user studies. Future works will need\nto consider the ability to synthesize new motor behaviors\nwhile simultaneously providing muscle validation.\nReferences\nAl Borno, M., Vyas, S., Shenoy, K. V., and Delp, S. L.\nHigh-fidelity musculoskeletal modeling reveals that mo-\ntor planning variability contributes to the speed-accuracy\ntradeoff. eLife, 9:e57021, 2020. ISSN 2050-084X. doi:\n10.7554/eLife.57021. URL https://doi.org/10.\n7554/eLife.57021.\nBerg, C., Caggiano, V., and Kumar, V.\nSar:\nGen-\neralization of dexterity via synergistic action repre-\nsentation. https://sites.google.com/view/\nsar-rl/home, 2023.\nBerger, D. J., Gentner, R., Edmunds, T., Pai, D. K., and\nd\u2019Avella, A. Differences in adaptation rates after vir-\ntual surgeries provide direct evidence for modularity.\nJournal of Neuroscience, 33(30):12384\u201312394, 2013.\nISSN 0270-6474, 1529-2401. doi: 10.1523/JNEUROSCI.\n0122-13.2013.\nURL https://www.jneurosci.\norg/content/33/30/12384. Publisher: Society\nfor Neuroscience Section: Articles.\nBizzi, E. and Cheung, V. C. The neural origin of mus-\ncle synergies. Frontiers in Computational Neuroscience,\n7, 2013. ISSN 1662-5188. doi: 10.3389/fncom.2013.\n00051. URL https://www.frontiersin.org/\narticle/10.3389/fncom.2013.00051.\nBrahmbhatt, S., Ham, C., Kemp, C. C., and Hays, J. Con-\ntactDB: Analyzing and predicting grasp contact via ther-\nmal imaging. 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 8701\u20138711,\n2019.\nCaggiano, V., Cheung, V. C. K., and Bizzi, E. An opto-\ngenetic demonstration of motor modularity in the mam-\nmalian spinal cord. Scientific Reports, 6(1):35185, 2016.\nISSN 2045-2322. doi: 10.1038/srep35185. URL https:\n//doi.org/10.1038/srep35185.\nCaggiano, V., Wang, H., Durandau, G., Sartori, M., and\nKumar, V. MyoSuite \u2013 a contact-rich simulation suite\nfor musculoskeletal motor control, 2022a. URL http:\n//arxiv.org/abs/2205.13600.\nCaggiano, V., Wang, H., Durandau, G., Song, S., Tassa,\nY., Sartori, M., and Kumar, V. Myochallenge: Learn-\ning contact-rich manipulation using a musculoskele-\ntal hand.\nhttps://sites.google.com/view/\nmyochallenge, 2022b.\nCaruana, R. Multitask learning. Machine learning, 28:\n41\u201375, 1997.\nChen, T., Xu, J., and Agrawal, P.\nA system for gen-\neral in-hand object re-orientation.\narXiv preprint\narXiv:2111.03043, 2021.\nCheung, V. C. K., Cheung, B. M. F., Zhang, J. H., Chan,\nZ. Y. S., Ha, S. C. W., Chen, C.-Y., and Cheung,\nR. T. H. Plasticity of muscle synergies through frac-\ntionation and merging during development and train-\ning of human runners.\nNature Communications, 11\n(1):4356, 2020.\nISSN 2041-1723.\ndoi:\n10.1038/\ns41467-020-18210-4. URL https://doi.org/10.\n1038/s41467-020-18210-4.\nDai, J., He, K., and Sun, J. Instance-aware semantic seg-\nmentation via multi-task network cascades. In 2016 IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pp. 3150\u20133158. IEEE, 2016. ISBN 978-1-4673-\n8851-1. doi: 10.1109/CVPR.2016.343. URL http://\nieeexplore.ieee.org/document/7780712/.\nDasari, S., Gupta, A., and Kumar, V.\nLearning dexter-\nous manipulation from exemplar object trajectories and\npre-grasps. In 2023 IEEE International Conference on\nRobotics and Automation (ICRA), pp. 3889\u20133896. IEEE,\n2023.\nd\u2019Avella, A. and Bizzi, E.\nShared and specific muscle\nsynergies in natural motor behaviors. Proceedings of\nthe National Academy of Sciences, 102(8):3076\u20133081,\n2005.\nISSN 0027-8424, 1091-6490.\ndoi: 10.1073/\npnas.0500199102. URL https://pnas.org/doi/\nfull/10.1073/pnas.0500199102.\nd\u2019Avella, A., Saltiel, P., and Bizzi, E. Combinations of\nmuscle synergies in the construction of a natural motor\nbehavior. Nature neuroscience, 6(3):300\u2013308, 2003.\nDelp, S. L., Anderson, F. C., Arnold, A. S., Loan, P., Habib,\nA., John, C. T., Guendelman, E., and Thelen, D. G. Open-\nSim: Open-source software to create and analyze dy-\nnamic simulations of movement. IEEE Transactions on\nBiomedical Engineering, 54(11):1940\u20131950, 2007. doi:\n10.1109/TBME.2007.901024.\n10\nMyoDex: A Generalizable Prior for Dexterous Manipulation\nDominici, N., Ivanenko, Y. P., Cappellini, G., d\u2019Avella, A.,\nMond`\u0131, V., Cicchese, M., Fabiano, A., Silei, T., Paolo,\nA. D., Giannini, C., Poppele, R. E., and Lacquaniti, F. Lo-\ncomotor primitives in newborn babies and their develop-\nment. Science, 334(6058):997\u2013999, 2011. doi: 10.1126/\nscience.1210617.\nURL https://www.science.\norg/doi/abs/10.1126/science.1210617.\nGeijtenbeek, T., van de Panne, M., and van der Stappen,\nA. F. Flexible muscle-based locomotion for bipedal crea-\ntures. ACM Transactions on Graphics, 32(6):1\u201311, 2013.\nISSN 0730-0301, 1557-7368. doi: 10.1145/2508363.\n2508399.\nURL https://dl.acm.org/doi/10.\n1145/2508363.2508399.\nGoyal, A., Islam, R., Strouse, D., Ahmed, Z., Botvinick, M.,\nLarochelle, H., Bengio, Y., and Levine, S. InfoBot: Trans-\nfer and exploration via the information bottleneck, 2019.\nURL http://arxiv.org/abs/1901.10902.\nHasenclever, L., Pardo, F., Hadsell, R., Heess, N., and Merel,\nJ. CoMic: Complementary task learning & mimicry for\nreusable skills. In Proceedings of the 37th International\nConference on Machine Learning, ICML\u201920. JMLR.org,\n2020.\nHausman, K., Springenberg, J. T., Wang, Z., Heess, N.,\nand Riedmiller, M. Learning an embedding space for\ntransferable robot skills. In International Conference on\nLearning Representations, 2018.\nHeldstab, S. A., Isler, K., Schuppli, C., and van Schaik, C. P.\nWhen ontogeny recapitulates phylogeny: Fixed neurode-\nvelopmental sequence of manipulative skills among pri-\nmates. Science Advances, 6(30):eabb4685, 2020. doi: 10.\n1126/sciadv.abb4685. URL https://www.science.\norg/doi/10.1126/sciadv.abb4685. Publisher:\nAmerican Association for the Advancement of Science.\nIkkala, A., Fischer, F., Klar, M., Bachinski, M., Fleig,\nA., Howes, A., H\u00a8am\u00a8al\u00a8ainen, P., M\u00a8uller, J., Murray-\nSmith, R., and Oulasvirta, A. Breathing life into biome-\nchanical user models. In Proceedings of the 35th An-\nnual ACM Symposium on User Interface Software and\nTechnology, UIST \u201922. Association for Computing Ma-\nchinery, 2022.\nISBN 978-1-4503-9320-1.\ndoi: 10.\n1145/3526113.3545689. URL https://doi.org/\n10.1145/3526113.3545689. event-place: Bend,\nOR, USA.\nJain, D., Li, A., Singhal, S., Rajeswaran, A., Kumar, V., and\nTodorov, E. Learning deep visuomotor policies for dex-\nterous hand manipulation. In 2019 International Confer-\nence on Robotics and Automation (ICRA), pp. 3636\u20133643,\n2019. doi: 10.1109/ICRA.2019.8794033.\nJeannerod, M. The neural and behavioural organization\nof goal-directed movements. Clarendon Press/Oxford\nUniversity Press., 1988.\nJiang, Y., Van Wouwe, T., De Groote, F., and Liu, C. K.\nSynthesis of biologically realistic human motion using\njoint torque actuation. ACM Transactions on Graphics,\n38(4):1\u201312, 2019. ISSN 0730-0301, 1557-7368. doi:\n10.1145/3306346.3322966. URL https://dl.acm.\norg/doi/10.1145/3306346.3322966.\nJoos, E., P\u00b4ean, F., and Goksel, O. Reinforcement learn-\ning of musculoskeletal control from functional simula-\ntions, 2020. URL http://arxiv.org/abs/2007.\n06669.\nKroemer, O. and Sukhatme, G. S. Learning relevant fea-\ntures for manipulation skills using meta-level priors, 2016.\nURL http://arxiv.org/abs/1605.04439.\nKumar, V. Manipulators and Manipulation in high dimen-\nsional spaces. PhD thesis, University of Washington,\n2016.\nKumar, V., Todorov, E., and Levine, S. Optimal control\nwith learned local models: Application to dexterous ma-\nnipulation. In 2016 IEEE International Conference on\nRobotics and Automation (ICRA), pp. 378\u2013383, 2016. doi:\n10.1109/ICRA.2016.7487156.\nLee, J. H., Asakawa, D. S., Dennerlein, J. T., and\nJindrich, D. L.\nFinger muscle attachments for an\nOpenSim upper-extremity model. PLOS ONE, 10(4):\ne0121712, 2015. ISSN 1932-6203. doi: 10.1371/journal.\npone.0121712. URL https://dx.plos.org/10.\n1371/journal.pone.0121712.\nLee, S., Yu, R., Park, J., Aanjaneya, M., Sifakis, E., and\nLee, J. Dexterous manipulation and control with vol-\numetric muscles. ACM Transactions on Graphics, 37\n(4):1\u201313, 2018.\nISSN 0730-0301, 1557-7368.\ndoi:\n10.1145/3197517.3201330. URL https://dl.acm.\norg/doi/10.1145/3197517.3201330.\nLee,\nS.,\nPark,\nM.,\nLee,\nK.,\nand Lee,\nJ.\nScal-\nable muscle-actuated human simulation and control.\nACM Transactions on Graphics, 38(4):1\u201313, 2019.\nISSN 0730-0301, 1557-7368. doi: 10.1145/3306346.\n3322972.\nURL https://dl.acm.org/doi/10.\n1145/3306346.3322972.\nLiu, L. and Hodgins, J.\nLearning basketball dribbling\nskills using trajectory optimization and deep reinforce-\nment learning.\nACM Transactions on Graphics, 37\n(4):1\u201314, 2018.\nISSN 0730-0301, 1557-7368.\ndoi:\n10.1145/3197517.3201315. URL https://dl.acm.\norg/doi/10.1145/3197517.3201315.\n11\nMyoDex: A Generalizable Prior for Dexterous Manipulation\nLiu, S., Johns, E., and Davison, A. J. End-to-end multi-task\nlearning with attention. In 2019 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR),\npp. 1871\u20131880. IEEE, 2019. ISBN 978-1-72813-293-\n8. doi: 10.1109/CVPR.2019.00197. URL https://\nieeexplore.ieee.org/document/8954221/.\nMcFarland, D. C., Binder-Markey, B. I., Nichols, J. A.,\nWohlman, S. J., de Bruin, M., and Murray, W. M. A mus-\nculoskeletal model of the hand and wrist capable of simu-\nlating functional tasks. bioRxiv, pp. 2021.12.28.474357,\n2021.\ndoi:\n10.1101/2021.12.28.474357.\nURL\nhttp://biorxiv.org/content/early/\n2021/12/30/2021.12.28.474357.abstract.\nMerel, J., Tassa, Y., TB, D., Srinivasan, S., Lemmon, J.,\nWang, Z., Wayne, G., and Heess, N.\nLearning hu-\nman behaviors from motion capture by adversarial imita-\ntion, 2017. URL http://arxiv.org/abs/1707.\n02201.\nMerel, J., Hasenclever, L., Galashov, A., Ahuja, A., Pham,\nV., Wayne, G., Teh, Y. W., and Heess, N. Neural prob-\nabilistic motor primitives for humanoid control, 2019.\nURL http://arxiv.org/abs/1811.11711.\nNagabandi, A., Konoglie, K., Levine, S., and Kumar, V.\nDeep dynamics models for learning dexterous manipula-\ntion, 2019. URL http://arxiv.org/abs/1909.\n11652.\nPark, J., Min, S., Chang, P. S., Lee, J., Park, M., and Lee,\nJ. Generative GaitNet, 2022. URL http://arxiv.\norg/abs/2201.12044.\nPastor, P., Kalakrishnan, M., Righetti, L., and Schaal,\nS.\nTowards associative skill memories.\n2012 12th\nIEEE-RAS International Conference on Humanoid\nRobots (Humanoids 2012), pp. 309\u2013315, 2012.\ndoi:\n10.1109/HUMANOIDS.2012.6651537. URL http://\nieeexplore.ieee.org/document/6651537/.\nConference Name: 2012 12th IEEE-RAS International\nConference on Humanoid Robots (Humanoids 2012)\nISBN: 9781467313698 Place: Osaka, Japan Publisher:\nIEEE.\nRaffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus,\nM., and Dormann, N. Stable-baselines3: Reliable rein-\nforcement learning implementations. Journal of Machine\nLearning Research, 22(268):1\u20138, 2021.\nURL http:\n//jmlr.org/papers/v22/20-1364.html.\nRajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schul-\nman, J., Todorov, E., and Levine, S. Learning complex\ndexterous manipulation with deep reinforcement learning\nand demonstrations. In Proceedings of Robotics: Science\nand Systems (RSS), 2018.\nRong, Y., Shiratori, T., and Joo, H. Frankmocap: A monocu-\nlar 3d whole-body pose estimation system via regression\nand integration. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pp. 1749\u20131759,\n2021.\nR\u00a8uckert, E. and d\u2019Avella, A. Learned parametrized dynamic\nmovement primitives with shared synergies for control-\nling robotic and musculoskeletal systems. Frontiers in\ncomputational neuroscience, 7:138, 2013.\nRueckert, E., Mundo, J., Paraschos, A., Peters, J., and\nNeumann, G. Extracting low-dimensional control vari-\nables for movement primitives. In 2015 IEEE Interna-\ntional Conference on Robotics and Automation (ICRA),\npp. 1511\u20131518. IEEE, 2015.\nSantello, M., Flanders, M., and Soechting, J. F. Patterns of\nhand motion during grasping and the influence of sensory\nguidance. Journal of Neuroscience, 22(4):1426\u20131435,\n2002.\nSaul, K. R., Hu, X., Goehler, C. M., Vidt, M. E., Daly, M.,\nVelisar, A., and Murray, W. M. Benchmarking of dy-\nnamic simulation predictions in two software platforms\nusing an upper limb musculoskeletal model. Computer\nMethods in Biomechanics and Biomedical Engineer-\ning, 18(13):1445\u20131458, 2015. doi: 10.1080/10255842.\n2014.916698. URL https://doi.org/10.1080/\n10255842.2014.916698.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A.,\nand Klimov, O.\nProximal policy optimization al-\ngorithms, 2017.\nURL http://arxiv.org/abs/\n1707.06347.\nSchumacher, P., H\u00a8aufle, D., B\u00a8uchler, D., Schmitt, S., and\nMartius, G. DEP-RL: Embodied exploration for rein-\nforcement learning in overactuated and musculoskeletal\nsystems, 2022. URL https://arxiv.org/abs/\n2206.00484.\nSeth, A., Hicks, J. L., Uchida, T. K., Habib, A., Dembia,\nC. L., Dunne, J. J., Ong, C. F., DeMers, M. S., Rajagopal,\nA., Millard, M., Hamner, S. R., Arnold, E. M., Yong, J. R.,\nLakshmikanth, S. K., Sherman, M. A., Ku, J. P., and Delp,\nS. L. OpenSim: Simulating musculoskeletal dynamics\nand neuromuscular control to study human and animal\nmovement. PLOS Computational Biology, 14:1\u201320, 2018.\ndoi: 10.1371/journal.pcbi.1006223. URL https://\ndoi.org/10.1371/journal.pcbi.1006223.\nSobinov, A. R. and Bensmaia, S. J. The neural mechanisms\nof manual dexterity. Nature Reviews Neuroscience, 22\n(12):741\u2013757, 2021. ISSN 1471-0048. doi: 10.1038/\ns41583-021-00528-7. URL https://doi.org/10.\n1038/s41583-021-00528-7.\n12\nMyoDex: A Generalizable Prior for Dexterous Manipulation\nSong, S., Kidzi\u00b4nski, \\., Peng, X. B., Ong, C., Hicks, J.,\nLevine, S., Atkeson, C. G., and Delp, S. L. Deep re-\ninforcement learning for modeling human locomotion\ncontrol in neuromechanical simulation. bioRxiv, 2020.\ndoi: 10.1101/2020.08.11.246801.\nSun, X., Panda, R., Feris, R., and Saenko, K. AdaShare:\nlearning what to share for efficient deep multi-task learn-\ning. In Proceedings of the 34th International Conference\non Neural Information Processing Systems, NIPS\u201920, pp.\n8728\u20138740. Curran Associates Inc., 2020. ISBN 978-1-\n71382-954-6.\nSutton, R. S. and Barto, A. G.\nReinforcement Learn-\ning: An Introduction. The MIT Press, second edition,\n2018.\nURL http://incompleteideas.net/\nbook/the-book-2nd.html.\nTaheri, O., Ghorbani, N., Black, M. J., and Tzionas, D.\nGRAB: A dataset of whole-body human grasping of ob-\njects. In Vedaldi, A., Bischof, H., Brox, T., and Frahm,\nJ.-M. (eds.), Computer Vision \u2013 ECCV 2020, pp. 581\u2013600.\nSpringer International Publishing, 2020. ISBN 978-3-\n030-58548-8.\nTodorov, E., Erez, T., and Tassa, Y. Mujoco: A physics\nengine for model-based control. In 2012 IEEE/RSJ Inter-\nnational Conference on Intelligent Robots and Systems,\npp. 5026\u20135033. IEEE, 2012.\nTresch, M. C., Cheung, V. C. K., and d\u2019Avella, A. Ma-\ntrix factorization algorithms for the identification of mus-\ncle synergies: Evaluation on simulated and experimen-\ntal data sets. Journal of Neurophysiology, 95(4):2199\u2013\n2212, 2006. ISSN 0022-3077, 1522-1598. doi: 10.1152/\njn.00222.2005. URL https://www.physiology.\norg/doi/10.1152/jn.00222.2005.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\nJones, L., Gomez, A. N., Kaiser, L., and Polosukhin,\nI. Attention is all you need. In Guyon, I., Luxburg,\nU. V., Bengio, S., Wallach, H., Fergus, R., Vish-\nwanathan, S., and Garnett, R. (eds.), Advances in Neural\nInformation Processing Systems, volume 30. Curran As-\nsociates, Inc., 2017. URL https://proceedings.\nneurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.\npdf.\nWang, H., Caggiano, V., Durandau, G., Sartori, Massimo,\nK., and Vikash. MyoSim: Fast and physiologically real-\nistic MuJoCo models for musculoskeletal and exoskele-\ntal studies. In 2022 IEEE international conference on\nrobotics and automation (ICRA). IEEE, 2022.\nWang, J. M., Hamner, S. R., Delp, S. L., and Koltun, V. Op-\ntimizing locomotion controllers using biologically-based\nactuators and objectives. ACM Transactions on Graphics,\n31(4):1\u201311, 2012. ISSN 0730-0301, 1557-7368. doi:\n10.1145/2185520.2185521. URL https://dl.acm.\norg/doi/10.1145/2185520.2185521.\nWon, J., Gopinath, D., and Hodgins, J.\nControl strate-\ngies for physically simulated characters performing two-\nplayer competitive sports. ACM Transactions on Graph-\nics, 40(4):1\u201311, 2021. ISSN 0730-0301, 1557-7368. doi:\n10.1145/3450626.3459761. URL https://dl.acm.\norg/doi/10.1145/3450626.3459761.\nYang,\nQ.,\nLogan,\nD.,\nand Giszter,\nS. F.\nMo-\ntor primitives are determined in early development\nand are then robustly conserved into adulthood.\nProceedings of the National Academy of Sciences,\n116(24):12025\u201312034, 2019.\ndoi:\n10.1073/pnas.\n1821455116. URL https://www.pnas.org/doi/\nabs/10.1073/pnas.1821455116.\nYin,\nZ.,\nYang,\nZ.,\nVan De Panne,\nM.,\nand Yin,\nK.\nDiscovering diverse athletic jumping strategies.\nACM Transactions on Graphics, 40(4):1\u201317, 2021.\nISSN 0730-0301, 1557-7368. doi: 10.1145/3450626.\n3459817.\nURL https://dl.acm.org/doi/10.\n1145/3450626.3459817.\nZhang, Y. and Yeung, D.-Y. A regularization approach to\nlearning task relationships in multitask learning. ACM\nTransactions on Knowledge Discovery from Data, 8(3):\n12:1\u201312:31, 2014.\nISSN 1556-4681.\ndoi: 10.1145/\n2538028.\nURL https://doi.org/10.1145/\n2538028.\n13\nMyoDex: A Generalizable Prior for Dexterous Manipulation\nA. Appendix\nFigure A.1. Success and error metrics for the multi-task policy trained on 14 environments in the first 40k iterations on 4 seeds (line\naverage and errors as shaded area).\nFigure A.2. Baselines: Success, error rate, orientation error metrics (in the left, middle and right columns, respectively \u2013 see definitions in\nSec. 5) for Individual-Task Experts \u03c0i, Multi-task Student policy \u03c0\u2217, and Multi-task MyoDex \u03c0# policy. On the Y-axis the 14 tasks used\nto traing MyoDex are reported, in addition to an aggregate information. MyoDex is able to match individual-Task Experts solutions across\nthe 3 differnt metrics. Nevertheless, the multi-task student policy was able to achieve lower perforances overall in most of the individual\ntasks.\nA.1. Imitation learning.\nIn addition to MyoDex \u03c0#, we also train a baseline agent using \u03c0\u2217 expert-student method (Jain et al., 2019; Chen et al., 2021).\nIndividual task-specific policies (\u03c0i) were used as experts. We developed a dataset with 1M samples of observation-action\ntuples for each of those policies. A neural network was trained via supervised learning to learn the association between\nobservations and actions to obtain a single policy \u03c0\u2217(at|st) capable of multiple task behaviors.\nFor distilling the single expert agents into one, a neural network of the same size of the single agent was used. We adopted a\nbatch size of 256, and Adadelta optimizer with a learning rate of 0.25, a Discount Factor (\u03b3) of 0.995, and 10 epochs.\n14\nMyoDex: A Generalizable Prior for Dexterous Manipulation\nFigure A.3. Summary of all Tasks. Left column tasks solved by single expert policies. Right columns, task fine tuning based on MyoDex.\nSee also Table A.2. Training values reported at 12.5k iterations.\nA.2. Noise\nIn real-world scenarios, calculating the precise position, and trajectory of an object is often subject to errors. To address this\nissue, we conducted a study to investigate the resilience of these policies to noisy measurements. We emulate real-world\ntracking errors by gradually adding increasing levels of noise to the trained policies during deployment (see Figure A.6).\nWe see that MyoDex policies are able to handle significant levels of noise (up to 100mm) in the observation of the object\u2019s\nposition with limited loss in performance.\nA.3. MyoDex Alternatives\nThe choice of objects and tasks can significantly impact the effectiveness of the MyoDex representation. To investigate this,\nwe conducted an ablation experiment using two new sets of 14 tasks each: a diverse tasks collection with similar complexity\nas MyoDex \u2013 MyoDex Alt Diverse (shown in blue in Figure 9)\u2013 and the other with a homogenous task collection MyoDex Alt\nHomogenous (shown in red in Figure 9). MyoDex Alt Homogenous shows a quick rise in performance which saturates due\nto overfitting A.7. In contrast, MyoDex Alt Diverse observes a slow start but is gradually able to improve its performance\nover time. Figure 10 shows that the priors implicitly induced by the richer task diversity leads to better generalization and\ntransfer to new unseen tasks.\n15\nMyoDex: A Generalizable Prior for Dexterous Manipulation\nSamples for Iterations\n4096\nDiscount Factor (\u03b3)\n0.95\nGAE-\u03bb\n0.95\nVF Coefficient (c1)\n0.5\nEntropy Bonus (c2)\n0.001\nClip Parameter (\u03f5)\n0.2\nBatch Size\n256\nEpochs\n5\nNetwork Size\npi = [256, 128], vf = [256, 128]\nTable A.1. Parameters adopted for the reinforcement learning models.\nA.4. Synergy probing\nTo quantify the level of muscle coordination required for accomplishing a given task, we calculated muscle synergies by\nmeans of Non-Negative Matrix factorization (NNMF) (Tresch et al., 2006).\nAfter training, we played policies for 5 roll-outs to solve specific tasks and we stored the muscle activations (value between\n0 and 1) required. Then, a matrix A of muscle activations over time (dimension 39 muscle x total task duration) was fed into\na non-negative matrix decomposition (sklearn) method.\nThe NNMF method finds two matrices W and H that are respectively the coefficients and the basis vectors which product\napproximates A. Muscle synergies identified by NNMF capture the spatial regularities on the muscle activations whose\nlinear combination minimize muscle reconstruction (Bizzi & Cheung, 2013). This method reveals the amount of variance\nexplained by each of the components. We calculated the Variance Accounted For (VAF) as:\nV AF = 100 \u00b7\n\u0012\n1 \u2212 (A \u2212 W \u00b7 H)2\nA2\n\u0013\n(2)\nSimilarity of synergies between two different tasks was calculated using cosine similarity (CS) such as: CS = wi \u00b7 wj,\nwhere [wi, wj] \u2208 W are synergy coefficients respectively for the task i and j. We used then a threshold of 0.8 to indicate\nthat 2 synergies were similar Appendix-A.4.\nWhile the student policy \u2013 obtained with imitation learning \u2013 produced muscle activations similar to that of the respective\ntask expert but it effectiveness was quite low in task metrics. coordination?\nA.4.1. DOES MyoDex PRODUCE REUSABLE SYNERGIES?\nBiological systems simplify the problem to control the redundant and complex muscuolokeletal systems by resorting on\nactivating particular muscle groups in consort, a phenomenon known as muscle synergies. Here, we want to analyse if\nsynergies emerge and facilitate learning.\nFor MyoDex where an agent has to simultaneously learn multiple manipulations / tasks, common patterns emerges and fewer\nsynergies i.e. 12 (Figure A.8), can explain more than 80% of the variance of the data. Furthermore, we observe that tasks\nstart sharing more synergies (on average 6, see Figure A.4). This is expected as each task needs a combination of shared\n(task-aspecific) and task-specific synergies. Common patterns of activations seems to be related with learning. Indeed,\nearlier in the training more synergies are needed to explain the same amount of variance of the data. The peak is reached at\n12.5k iterations where more than 90% of the variance is explained by 12 synergies (see Figure A.8).\nAs expected, the expert policies shared fewer common muscle activations as indicated by fewer synergies shared between\ntasks (on average 2, see Figure A.4) and by the overall greater number of synergies needed to explain most of the variance:\nto explain more than 80% of the variance it is needed to use more than 20 synergies. Similar results were obtained with the\nstudent policy (on average 1 similar synergies between tasks, see Figure A.4).\nA.4.2. PREGRASP INFORMED DEXTEROUS MANIPULATION\nWe adopted Dasari et al (Dasari et al., 2023) solution where the desired object trajectory \u02c6X = [\u02c6x0, ..., \u02c6xT ] is leveraged\nto capture the temporal complexities of dexterous manipulation. Additionally hand-object pre-grasp posture \u03d5pregrasp\nobject\nis\n16\nMyoDex: A Generalizable Prior for Dexterous Manipulation\n0\n5\n10\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nTasks\nExpert\n0\n2\n4\n6\n8\n0\n5\n10\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nTasks\nStudent\n0\n2\n4\n6\n8\nNum of similar synergies\n0\n5\n10\nTasks\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nTasks\nMyoDex\n0\n2\n4\n6\n8\nFigure A.4. Cosine Similarity between 12 synergies extracted from 14 different tasks. Top - expert policies. Middle - student policy.\nBottom \u2013 MyoDex. On average the number of similar synergies for expert, student, MyoDex (mean +/- std over 10 repetitions with\ndifferent random seeds) was 1.86 \u00b1 1.19, 1.71 \u00b1 1.29 and 5.56 \u00b1 1.59, respectively.\nleveraged to guide the search space. For each task, first a trajectory planner is used to solve for the free space movement\ndriving the hand to the pre-shape pose, and then PPO is employed to solve for achieving the desired object trajectory.\nWe extracted relevant pregrasp informations from the GRAB motion capture (Taheri et al., 2020) dataset which contains\nhigh-quality human-object interactions. Note that these details can also be acquired by running standard hand tackers (Rong\net al., 2021) on free form human videos.\nWe used the hand posture just before the initial contact with the object (see Figure 3) as the pre-grasp posture. This allows\nus to not require any physical or geometric information about the object. Given this tracked posture, we recover MyoHand\nposture via means of Inverse Kinematics over the finger tips.\n17\nMyoDex: A Generalizable Prior for Dexterous Manipulation\nairplane-fly1\ncup-pour1\nalarmclock-see1\nairplane-fly1-0\nairplane-pass1-0\nalarmclock-see1-0\nbanana-pass1-0\ncup-drink1-0\ncup-pour1-0\nhammer-use1-0\nmug-drink3-0\nscissors-use1-0\nstamp-stamp1-0\ntrain-play1-0\nwaterbottle-shake1-0\nwineglass-drink2-0\nwineglass-toast1-0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess\nFigure A.5. Fine-tuning based on expert policies. Success rate fine-tuning experts solutions (columns) on 14 different environments.\nThis matrix shows that the combination of pre-grasps and the initialization on a pre-trained task is not enough to generalize to new tasks.\n100\n101\n102\n103\nNoise level (mm)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nMyoDex\nFigure A.6. Performance according to addional noise (in mm) in the observation of the object.\n18\nMyoDex: A Generalizable Prior for Dexterous Manipulation\nTask\nMulti-task Success\nIter. to reach Success of 0.8\n@ 1k Iter.\n@ 2k Iter.\n@ 3k Iter.\nMulti-Task\nExpert\nstamp-stamp1\n0.981538\n0.997949\n1.000000\n247\n3458\nbanana-pass1\n0.910000\n0.993333\n0.998571\n247\n4446\ncup-drink1\n0.991724\n0.998161\n0.977471\n247\n3952\nmug-drink3\n0.978667\n0.999467\n1.000000\n247\n3458\nalarmclock-see1\n0.984444\n0.997778\n1.000000\n494\n4940\ntrain-play1\n0.822278\n0.929114\n0.987848\n741\n8398\nscissors-use1\n0.754699\n0.945542\n0.986988\n1235\n5434\nwineglass-drink2\n0.714943\n0.924138\n0.985287\n1235\n4446\nhammer-use1\n0.781429\n0.870000\n0.972857\n1482\n3952\nwineglass-toast1\n0.713846\n0.796410\n0.902051\n2223\n4199\ncup-pour1\n0.743429\n0.730857\n0.830286\n2964\n4446\nwaterbottle-shake1\n0.574595\n0.709189\n0.743784\n3458\n5434\nairplane-fly1\n0.564675\n0.606753\n0.631169\n12350\n7657\nairplane-pass1\n0.436322\n0.497011\n0.509425\n12597\n6669\nmouse-lift\n1.000000\n1.000000\n1.000000\n247\n-\napple-lift\n1.000000\n1.000000\n1.000000\n247\n-\nspheresmall-lift\n0.986667\n1.000000\n1.000000\n247\n-\ntorusmedium-lift\n0.980571\n1.000000\n1.000000\n247\n-\nairplane-lift\n0.995122\n1.000000\n1.000000\n247\n-\nelephant-lift\n1.000000\n1.000000\n1.000000\n247\n-\nalarmclock-lift\n1.000000\n1.000000\n1.000000\n247\n-\nspheremedium-lift\n0.998947\n1.000000\n1.000000\n494\n-\ntoothpaste-lift\n0.971818\n0.952727\n0.990000\n494\n-\nflashlight-lift\n0.941714\n0.942857\n0.942857\n494\n-\nstapler-staple2\n0.991529\n1.000000\n0.999529\n494\n-\nduck-lift\n0.994737\n1.000000\n1.000000\n494\n5434\nwineglass-lift\n0.933000\n0.979500\n0.980000\n494\n-\nwatch-lift\n0.925333\n0.955556\n0.955556\n741\n-\nphone-lift\n0.960000\n0.967742\n0.967742\n741\n-\nstapler-staple1\n0.893809\n0.989524\n0.996190\n988\n5187\ncylindermedium-lift\n0.841111\n0.970000\n0.972222\n988\n-\ntorussmall-lift\n0.690285\n0.931428\n0.915428\n1235\n-\nstamp-lift\n0.709756\n0.980488\n0.992195\n1235\n3211\ntoruslarge-lift\n0.707273\n0.965455\n0.977273\n1235\n-\ncup-pass1\n0.609048\n0.995238\n1.000000\n1235\n4446\ntoothpaste-squeeze1\n0.598421\n0.943157\n0.977368\n1482\n-\nstapler-lift\n0.650732\n0.868293\n0.982439\n1482\n2717\nwatch-pass1\n0.492593\n0.887407\n0.884444\n1729\n-\ncylindersmall-pass1\n0.571200\n0.826667\n0.901333\n1976\n4693\nflashlight-on2\n0.168791\n0.695385\n0.920000\n2470\n6175\ntoruslarge-inspect1\n0.251852\n0.645926\n0.817778\n2470\n-\nstanfordbunny-inspect1\n0.289157\n0.591325\n0.921446\n2470\n6422\nelephant-pass1\n0.506667\n0.621235\n0.834568\n2964\n5928\nduck-inspect1\n0.621299\n0.624935\n0.820260\n2964\n5681\ncylindersmall-inspect1\n0.420000\n0.713333\n0.639444\n3705\n6422\nflashlight-on1\n0.234483\n0.541609\n0.626207\n4446\n10127\nspheresmall-pass1\n0.191905\n0.351905\n0.674286\n4446\n5928\napple-pass1\n0.344198\n0.481481\n0.583210\n5187\n-\ntoothbrush-brush1\n0.119375\n0.353125\n0.589063\n5434\n4199\nbowl-drink2\n0.075714\n0.089524\n0.163810\n7657\n4693\nspheresmall-inspect1\n0.235676\n0.332432\n0.438919\n8151\n7163\nmug-lift\n0.326575\n0.335342\n0.397808\n-\n7904\ncubesmall-pass1\n0.024691\n0.024691\n0.024691\n-\n5928\nbowl-pass1\n0.114430\n0.153418\n0.184810\n-\n7163\nteapot-pour2\n0.137627\n0.150508\n0.162712\n-\n7657\ntorussmall-pass1\n0.038987\n0.037975\n0.037975\n-\n5928\npyramidsmall-inspect1\n0.028571\n0.033333\n0.035238\n-\n5187\nTable A.2. MyoDex based fine-tuning and expert solutions for all 57 tasks. Expert solutions could reliably reach 0.80 success for the\nfirst 14 tasks but in many other cases they were not able to. A few exceptions at the bottom show success only for expert solutions. We\nindicated with \u2019-\u2019 the lack of success in achieving the success threshold. The first 3 columns report the success rate respectively at 1k, 2k\nand 3k iterations. The 4th and 5th columns, document the iterations at 0.80 success for MyoDex based fine-tuning and experts.\n19\nMyoDex: A Generalizable Prior for Dexterous Manipulation\n0k\n5k\n10k\n16k\n21k\n26k\n31k\nEnvironment Iteration\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nSuccess Metric\nMyoDex Alt Diverse\nMyoDex Alt Homogenous\nFigure A.7. Effect of pre-training task distribution on MyoDex training. MyoDex Alt Homogenous shows a quick rise in performance\nwhich saturates due to overfitting. In contrast, MyoDex Alt Diverse observes a slow start but is gradually able to improve its performance\nover time.\nFigure A.8. Muscle Synergies over learning iterations for the joint multi-task policy. Variance of the muscle activations (see Sec. A.4)\nexplained as function of the number of synergies at different steps of the learning process.\nObject\nCreator\nLicense\nwaterbottle\nbadger\nGNU GPL v2\ntrain\nJason Shoumar\npublic domain\nairplane\nGravity Sketch\nCC BY-4.0\nwine glass\nMichael Spivey\nCC BY 3.0\ncup\nAblapo\nCC BY 3.0\nmug\nRyan Smith\ncredit, remix, non-commercial\nalarm clock\nJavier Ruiz\nCC BY-SA 3.0\nbanana\nLloyd Bolts\ncredit, remix, non-commercial\nhammer\nMicrosoft\nCC BY 4.0\nmouse\nMichael Spivey\nCC BY 3.0\nduck\nwillie\nCC0 1.0\nTable A.3. Creators and License for the objects illustrated.\n20\n"
  }
]