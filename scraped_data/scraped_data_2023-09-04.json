[
  {
    "title": "YaRN: Efficient Context Window Extension of Large Language Models",
    "link": "https://arxiv.org/pdf/2309.00071.pdf",
    "upvote": "55",
    "text": "YaRN: Efficient Context Window Extension of Large\nLanguage Models\nBowen Peng1\u2217\nJeffrey Quesnelle1\u2020\nHonglu Fan23\nEnrico Shippole\u2021\n1Nous Research\n2EleutherAI\n3University of Geneva\nAbstract\nRotary Position Embeddings (RoPE) have been shown to effectively encode posi-\ntional information in transformer-based language models. However, these models\nfail to generalize past the sequence length they were trained on. We present YaRN\n(Yet another RoPE extensioN method), a compute-efficient method to extend the\ncontext window of such models, requiring 10x less tokens and 2.5x less training\nsteps than previous methods. Using YaRN, we show that LLaMA models can\neffectively utilize and extrapolate to context lengths much longer than their original\npre-training would allow, while also surpassing previous the state-of-the-art at\ncontext window extension. In addition, we demonstrate that YaRN exhibits the\ncapability to extrapolate beyond the limited context of a fine-tuning dataset. The\nmodels fine-tuned using YaRN has been made available and reproduced online up\nto 128k context length at https://github.com/jquesnelle/yarn.\n0\n20000\n40000\n60000\n80000\n100000\n120000\nContext Window\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\nPerplexity (lower is better)\nCodeLlama-13b-hf\nYarn-Llama-2-13b-64k\nYarn-Llama-2-13b-128k\ntogethercomputer/LLaMA-2-7B-32K\nCodeLlama-7b-hf\nYarn-Llama-2-7b-64k\nYarn-Llama-2-7b-128k\nFigure 1: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents truncated to evaluation\ncontext window size\n\u2217Reddit: /u/bloc97 GitHub: bloc97\n\u2020Reddit: /u/emozilla X: @theemozilla GitHub: jquesnelle\n\u2021X: @EnricoShippole GitHub: conceptofmind\nPreprint. Under review.\narXiv:2309.00071v2  [cs.CL]  1 Nov 2023\n1\nIntroduction\nTransformer-based Large Language Models[40] (LLMs) have become the near-ubiquitous choice for\nmany natural language processing (NLP) tasks where long-range abilities such as in-context learning\n(ICL) has been crucial. In performing the NLP tasks, the maximal length of the sequences (the\ncontext window) determined by its training processes has been one of the major limits of a pretrained\nLLM. Being able to dynamically extend the context window via a small amount of fine-tuning (or\nwithout fine-tuning) has become more and more desirable. To this end, the position encodings of\ntransformers are the center of the discussions.\nThe original Transformer architecture used an absolute sinusoidal position encoding, which was later\nimproved to a learnable absolute position encoding [15]. Since then, relative positional encoding\nschemes [32] have further increased the performance of Transformers. Currently, the most popular\nrelative positional encodings are T5 Relative Bias [30], RoPE [34], XPos [35], and ALiBi [27].\nOne reoccurring limitation with positional encodings is the inability to generalize past the context\nwindow seen during training. While some methods such as ALiBi are able to do limited generalization,\nnone are able to generalize to sequences significantly longer than their pre-trained length [22].\nSome works have been done to overcome such limitation. [9] and concurrently [21] proposed to\nextend the context length by slightly modifying RoPE via Position Interpolation (PI) and fine-tuning\non a small amount of data. As an alternative, [6] proposed the \"NTK-aware\" interpolation by\ntaking the loss of high frequency into account. Since then, two improvements of the \"NTK-aware\"\ninterpolation have been proposed, with different emphasis:\n\u2022 the \"Dynamic NTK\" interpolation method [14] for pre-trained models without fine-tuning.\n\u2022 the \"NTK-by-parts\" interpolation method [7] which performs the best when fine-tuned on a\nsmall amount of longer-context data.\nThe \"NTK-aware\" interpolation and the \"Dynamic NTK\" interpolation have already seen their\npresence in the open-source models such as Code Llama [31] (using \"NTK-aware\" interpolation) and\nQwen 7B [2] (using \"Dynamic NTK\").\nIn this paper, in addition to making a complete account of the previous unpublished works on the\n\"NTK-aware\", the \"Dynamic NTK\" and the \"NTK-by-part\" interpolations, we present YaRN (Yet\nanother RoPE extensioN method), an improved method to efficiently extend the context window\nof models trained with Rotary Position Embeddings (RoPE) including the LLaMA [38], the GPT-\nNeoX [5], and the PaLM [10] families of models.\nYaRN reaches state-of-the-art performances in context window extensions after fine-tuning on less\nthan \u223c0.1% of the original pre-training data. In the meantime, by combining with the inference-time\ntechnique called Dynamic Scaling, the Dynamic-YaRN allows for more than 2x context window\nextension without any fine-tuning.\n2\nBackground and Related Work\n2.1\nRotary Position Embeddings\nThe basis of our work is the Rotary Position Embedding (RoPE) introduced in [34]. We work on\na hidden layer where the set of hidden neurons are denoted by D. Given a sequence of vectors\nx1, \u00b7 \u00b7 \u00b7 , xL \u2208 R|D|, following the notation of [34], the attention layer first converts the vectors into\nthe query vectors and the key vectors:\nqm = fq(xm, m) \u2208 R|D|, kn = fk(xn, n) \u2208 R|D|.\n(1)\nNext, the attention weights are calculated as\nsoftmax(qT\nmkn\np\n|D|\n),\n(2)\nwhere qm, kn are considered as column vectors so that qT\nmkn is simply the Euclidean inner product.\nIn RoPE, we first assume that |D| is even and identify the embedding space and the hidden states as\n2\ncomplex vector spaces:\nR|D| \u223c= C|D|/2\nwhere the inner product qT k becomes the real part of the standard Hermitian inner product Re(q\u2217k).\nMore specifically, the isomorphisms interleave the real part and the complex part\n\u0000(xm)1, \u00b7 \u00b7 \u00b7 , (xm)|D|\n\u0001\n7\u2192\n\u0000(xm)1 + i(xm)2, \u00b7 \u00b7 \u00b7 , ((xm)|D|\u22121 + i(xm)|D|)\n\u0001\n,\n(3)\n\u0000(qm)1, \u00b7 \u00b7 \u00b7 , (qm)|D|\n\u0001\n7\u2192\n\u0000(qm)1 + i(qm)2, \u00b7 \u00b7 \u00b7 , ((qm)|D|\u22121 + i(qm)|D|)\n\u0001\n.\n(4)\nTo convert embeddings xm, xn into query and key vectors, we are first given R-linear operators\nWq, Wk : R|D| \u2192 R|D|.\nIn complex coordinates, the functions fq, fk are given by\nfq(xm, m) = eim\u03b8Wqxm, fk(xn, n) = ein\u03b8Wkxn,\n(5)\nwhere \u03b8 = diag(\u03b81, \u00b7 \u00b7 \u00b7 , \u03b8|D|/2) is the diagonal matrix with \u03b8d = b\u22122d/|D| and b = 10000. This way,\nRoPE associates each (complex-valued) hidden neuron with a separate frequency \u03b8d. The benefit\nof doing so is that the dot product between the query vector and the key vector only depends on the\nrelative distance m \u2212 n as follows\n\u27e8fq(xm, m), fk(xn, n)\u27e9R\n(6)\n=Re(\u27e8fq(xm, m), fk(xn, n)\u27e9C)\n(7)\n=Re(x\u2217\nmW\u2217\nqWkxnei\u03b8(m\u2212n))\n(8)\n=g(xm, xn, m \u2212 n).\n(9)\nIn real coordinates, the RoPE can be written using the following function\nfW(xm, m, \u03b8d) =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\ncosm\u03b81\n\u2212sinm\u03b81\n0\n0\n\u00b7 \u00b7 \u00b7\n0\n0\nsinm\u03b81\ncosm\u03b81\n0\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n0\n0\ncosm\u03b82\n\u2212sinm\u03b82\n\u00b7 \u00b7 \u00b7\n0\n0\n0\n0\nsinm\u03b82\ncosm\u03b82\n\u00b7 \u00b7 \u00b7\n0\n0\n0\n0\n0\n0\n\u00b7 \u00b7 \u00b7\ncosm\u03b8l\n\u2212sinm\u03b8l\n0\n0\n0\n0\n\u00b7 \u00b7 \u00b7\nsinm\u03b8l\ncosm\u03b8l\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\nWxm,\nso that\nfq = fWq, fk = fWk.\n2.2\nPosition Interpolation\nAs language models are usually pre-trained with a fixed context length, it is natural to ask how to\nextend the context length by fine-tuning on relatively less amount of data. For language models using\nRoPE as the position embedding, Chen et al. [9], and concurrently kaiokendev [21] proposed the\nPosition Interpolation (PI) to extend the context length beyond the pre-trained limit. While a direct\nextrapolation does not perform well on sequences w1, \u00b7 \u00b7 \u00b7 , wL with L larger than the pre-trained\nlimit, they discovered that interpolating the position indicies within the pre-trained limit works well\nwith the help of a small amount of fine-tuning. Specifically, given a pre-trained language model with\nRoPE, they modify the RoPE by\nf \u2032\nW (xm, m, \u03b8d) = fW\n\u0012\nxm, mL\nL\u2032 , \u03b8d\n\u0013\n,\n(10)\nwhere L\u2032 > L is a new context window beyond the pre-trained limit. With the original pre-trained\nmodel plus the modified RoPE formula, they fine-tuned the language model further on several orders\nof magnitude fewer tokens (a few billion in Chen et al. [9]) and successfully acheived context window\nextension.\n3\n2.3\nAdditional Notation\nThe ratio between the extended context length and the original context length has been of special\nimportance, and we introduce the notation s defined by\ns = L\u2032\nL ,\n(11)\nand we call s the scale factor.\nWe also rewrite and simplify Eq. 10 into the following general form:\nf \u2032\nW(xm, m, \u03b8d) = fW(xm, g(m), h(\u03b8d)),\n(12)\nwhere g(m), h(\u03b8d) are method-dependent functions. For PI, we have g(m) = m/s, h(\u03b8d) = \u03b8d. In\nthe subsequent sections, when we introduce a new interpolation method, we sometimes only specify\nthe functions g(m) and h(\u03b8d).\nAdditionally, we define \u03bbd as the wavelength of the RoPE embedding at d-th hidden dimension:\n\u03bbd = 2\u03c0\n\u03b8d\n= 2\u03c0b\n2d\n|D| .\n(13)\nThe wavelength describes the length of tokens needed in order for the RoPE embedding at dimension\nd to perform a full rotation (2\u03c0).\nGiven that some interpolation methods (eg. PI) do not care about the wavelength of the dimensions,\nwe will refer to those methods as \"blind\" interpolation methods, while others do (eg. YaRN), which\nwe will classify as \"targeted\" interpolation methods.\n2.4\nRelated work\nReRoPE [33] also aims to extend the context size of existing models pre-trained with RoPE, and claims\n\"infinite\" context length without needing any fine-tuning. This claim is backed by a monotonically\ndecreasing loss with increasing context length up to 16k on the Llama 2 13B model. It achieves\ncontext extension by modifying the attention mechanism and thus is not purely an embedding\ninterpolation method. Since it is currently not compatible with Flash Attention 2 [13] and requires\ntwo attention passes during inference, we do not consider it for comparison.\nConcurrently with our work, LM-Infinite [16] proposes similar ideas to YaRN, but focuses on \"on-the-\nfly\" length generalization for non-fine-tuned models. Since they also modify the attention mechanism\nof the models, it is not an embedding interpolation method and is not immediately compatible with\nFlash Attention 2.\n3\nMethodology\nWhereas PI stretches all RoPE dimensions equally, we find that the theoretical interpolation bound\ndescribed by PI [9] is insufficient at predicting the complex dynamics between RoPE and the LLM\u2019s\ninternal embeddings. In the following subsections, we describe the main issues with PI we have\nindividually identified and solved, so as to give the readers the context, origin and justifications of\neach method which we use in concert to obtain the full YaRN method.\n3.1\nLoss of High Frequency information - \"NTK-aware\" interpolation\nIf we look at RoPE only from an information encoding perspective, it was shown in [36], using\nNeural Tangent Kernel (NTK) theory, that deep neural networks have trouble learning high frequency\ninformation if the input dimension is low and the corresponding embeddings lack high frequency\ncomponents. Here we can see the similarities: a token\u2019s positional information is one-dimensional,\nand RoPE expands it to an n-dimensional complex vector embedding.\n4\nRoPE closely resembles Fourier Features [36] in many aspects, as it is possible to define RoPE as a\nspecial 1D case of a Fourier Feature. Stretching the RoPE embeddings indiscriminately results in the\nloss of important high frequency details which the network needs in order to resolve tokens that are\nboth very similar and very close together (the rotation describing the smallest distance needs to not\nbe too small for the network to be able to detect it).\nWe hypothesise that the slight increase of perplexity for short context sizes after fine-tuning on larger\ncontext sizes seen in PI [9] might be related to this problem. Under ideal circumstances, there is no\nreason that fine-tuning on larger context sizes should degrade the performance of smaller context\nsizes.\nIn order to resolve the problem of losing high frequency information when interpolating the RoPE\nembeddings, the \"NTK-aware\" interpolation was developed in [6]. Instead of scaling every dimension\nof RoPE equally by a factor s, we spread out the interpolation pressure across multiple dimensions\nby scaling high frequencies less and low frequencies more. One can obtain such a transformation in\nmany ways, but the simplest would be to perform a base change on the value of \u03b8.\nMore precisely, following the notations set out in Section 2.3, we define the \"NTK-aware\" interpola-\ntion scheme as follows (see the Appendix A.1 for the details of the deduction).\nDefinition 1 The \"NTK-aware\" interpolation is a modification of RoPE by using Eq. 12 with the\nfollowing functions.\ng(m) = m\n(14)\nh(\u03b8d) = b\u2032\u22122d/|D|,\n(15)\nwhere\nb\u2032 = b \u00b7 s\n|D|\n|D|\u22122 .\n(16)\nGiven the results from [6], this method performs much better at extending the context size of non-fine-\ntuned models compared to PI [9]. However, one major disadvantage of this method is that given it is\nnot just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values,\nthus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore,\ndue to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true\ncontext extension scale. In practice, the scale value s has to be set higher than the expected scale for\na given context length extension.\nWe note that shortly before the release of this article, Code Llama [31] was released and uses\n\"NTK-aware\" scaling by manually scaling the base b to 1M.\n3.2\nLoss of Relative Local Distances - \"NTK-by-parts\" interpolation\nIn the case of blind interpolation methods like PI and \"NTK-aware\" interpolation, we treat all the\nRoPE hidden dimensions equally (as in they have the same effect on the network). However, there\nare strong clues that point us towards the need for targeted interpolation methods.\nIn this section, we think heavily in terms of the wavelengths \u03bbd defined in Eq. 13 in the formula of\nRoPE. For simplicity, we omit the subscript d in \u03bbd and the reader is encouraged to think about \u03bb as\nthe wavelength of an arbitrary periodic function.\nOne interesting observation of RoPE embeddings is that given a context size L, there are some\ndimensions d where the wavelength is longer than the maximum context length seen during pretraining\n(\u03bb > L), this suggests that some dimensions\u2019 embeddings might not be distributed evenly in the\nrotational domain. In such cases, we presume having all unique position pairs implies that the\nabsolute positional information remains intact. On the contrary, when the wavelength is short, only\nrelative positional information is accessible to the network.\nMoreover, when we stretch all the RoPE dimensions either by a scale s or using a base change b\u2032,\nall tokens become closer to each other, as the dot product of two vectors rotated by a lesser amount\nis bigger. This scaling severely impairs a LLM\u2019s ability to understand small and local relationships\nbetween its internal embeddings. We hypothesize that such compression leads to the model being\nconfused on the positional order of close-by tokens, and consequently harming the model\u2019s abilities.\n5\nIn order to remedy this issue, given the two previous observations that we have found, we choose not\nto interpolate the higher frequency dimensions at all while always interpolating the lower frequency\ndimensions. In particular,\n\u2022 if the wavelength \u03bb is much smaller than the context size L, we do not interpolate;\n\u2022 if the wavelength \u03bb is equal to or bigger than the context size L, we want to only interpolate\nand avoid any extrapolation (unlike the previous \"NTK-aware\" method);\n\u2022 dimensions in-between can have a bit of both, similar to the \"NTK-aware\" interpolation.\nAs a result, it is more convenient to introduce the ratio r = L\n\u03bb between the original context size L and\nthe wavelength \u03bb. In the d-th hidden state, the ratio r depends on d in the following way:\nr(d) = L\n\u03bbd\n=\nL\n2\u03c0b\u2032 2d\n|D| .\n(17)\nIn order to define the boundary of the different interpolation strategies as above, we introduce\ntwo extra parameters \u03b1, \u03b2. All hidden dimensions d where r(d) < \u03b1 are those where we linearly\ninterpolate by a scale s (exactly like PI, avoiding any extrapolation), and the d where r(d) > \u03b2 are\nthose where we do not interpolate at all. Define the ramp function \u03b3 to be\n\u03b3(r) =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n0,\nif r < \u03b1\n1,\nif r > \u03b2\nr \u2212 \u03b1\n\u03b2 \u2212 \u03b1,\notherwise.\n(18)\nWith the help of the ramp function, the \"NTK-by-parts\" method can be described as follows.\nDefinition 2 The \"NTK-by-parts\" interpolation is a modification of RoPE by using Eq. 12 with the\nfollowing functions4.\ng(m) = m\n(19)\nh(\u03b8d) =\n\u0010\n1 \u2212 \u03b3\n\u0000r(d)\n\u0001\u0011\u03b8d\ns + \u03b3\n\u0000r(d)\n\u0001\n\u03b8d.\n(20)\nThe values of \u03b1 and \u03b2 should be tuned on a case-by-case basis. For example, we have found\nexperimentally that for the Llama family of models, good values for \u03b1 and \u03b2 are \u03b1 = 1 and \u03b2 = 32.\nUsing the techniques described in this section, a variant of the resulting method was released under\nthe name \"NTK-by-parts\" interpolation [7]. This improved method performs better than the previous\nPI [9] and \"NTK-aware\" 3.1 interpolation methods, both with non-fine-tuned models and with\nfine-tuned models, as shown in [7].\n3.3\nDynamic Scaling - \"Dynamic NTK\" interpolation\nIn a lot of use cases, multiple forward-passes are performed with varying sequence lengths from 1 to\nthe maximal context size. A typical example is the autoregressive generation where the sequence\nlengths increment by 1 after each step. There are two ways of applying an interpolation method that\nuses a scale factor s (including PI, \"NTK-aware\" and \"NTK-by-parts\"):\n1. Throughout the whole inference cycle, the embedding layer is fixed including the scale\nfactor s = L\u2032/L where L\u2032 is the fixed number of extended context size.\n2. In each forward-pass, the position embedding updates the scale factor s = max(1, l\u2032/L)\nwhere l\u2032 is the sequence length of the current sequence.\nThe problem of (1) is that the model may experience a performance discount at a length less than\nL and an abrupt degradation when the sequence length is longer than L\u2032. But by doing Dynamic\n4The interpolation by linear ramp on h may have alternatives, such as a harmonic mean over \u03b8d/s and\n\u03b8d converted from a linear interpolation on wavelengths. The choice of h here was for the simplicity of\nimplementation, but both would work.\n6\nScaling as (2), it allows the model to gracefully degrade instead of immediately breaking when hitting\nthe trained context limit L\u2032. We call this inference-time method the Dynamic Scaling method. When\nit is combined with \"NTK-awared\" interpolation, we call it \"Dynamic NTK\" interpolation. It first\nappeared in public as a reddit post in [14].\nOne notable fact is that the \"Dynamic NTK\" interpolation works exceptionally well on models pre-\ntrained on L without any finetuning (L\u2032 = L). This is supported by the experiment in Appendix B.3.\nOften in the repeated forward-passes, the kv-caching [8] is applied so that we can reuse the previous\nkey-value vectors and improve the overall efficiency. We point out that in some implementations when\nthe RoPE embeddings are cached, some care has to be taken in order to modify it for Dynamic Scaling\nwith kv-caching. The correct implementation should cache the kv-embeddings before applying RoPE,\nas the RoPE embedding of every token changes when s changes.\n3.4\nYaRN\nIn addition to the previous interpolation techniques, we also observe that introducing a temperature t\non the logits before the attention softmax has a uniform impact on perplexity regardless of the data\nsample and the token position over the extended context window (See Appendix A.2). More precisely,\ninstead of Eq. 2, we modify the computation of attention weights into\nsoftmax\n \nqT\nmkn\nt\np\n|D|\n!\n.\n(21)\nThe reparametrization of RoPE as a set of 2D matrices has a clear benefit on the implementation of\nthis attention scaling: we can instead use a \"length scaling\" trick which scales both qm and kn by a\nconstant factor\np\n1/t by simply scaling the complex RoPE embeddings by the same amount. With\nthis, YaRN can effectively alter the attention mechanism without modifying its code. Furthermore, it\nhas zero overhead during both inference and training, as RoPE embeddings are generated in advance\nand are reused for all forward passes. Combining it with the \"NTK-by-parts\" interpolation, we have\nthe YaRN method.\nDefinition 3 By the \"YaRN method\", we refer to a combination of the attention scaling in Eq. 21 and\nthe \"NTK-by-parts\" interpolation introduced in Section 3.2.\nFor LLaMA and Llama 2 models, we recommend the following values:\nr\n1\nt = 0.1 ln(s) + 1.\n(22)\nThe equation above is found by fitting\np\n1/t at the lowest perplexity against the scale extension\nby various factors s using the \"NTK-by-parts\" method (Section 3.2) on LLaMA 7b, 13b, 33b and\n65b models without fine-tuning. We note that the same values of t also apply fairly well to Llama\n2 models (7b, 13b and 70b). It suggests that the property of increased entropy and the temperature\nconstant t may have certain degree of \"universality\" and may be generalizable across some models\nand training data.\nThe YaRN method combines all our findings and surpasses all previous methods in both fine-tuned\nand non-fine-tuned scenarios. Thanks to its low footprint, YaRN allows for direct compatibility with\nlibraries that modify the attention mechanism such as Flash Attention 2 [13].\n4\nExperiments\nWe show that YaRN successfully achieves context window extension of language models using RoPE\nas its position embedding. Moreover, this result is achieved with only 400 training steps, representing\napproximately 0.1% of the model\u2019s original pre-training corpus, a 10x reduction from Rozi\u00e8re et al.\n[31] and 2.5x reduction in training steps from Chen et al. [9], making it highly compute-efficient for\ntraining with no additional inference costs. We calculate the perplexity of long documents and score\n7\non established benchmarks to evaluate the resulting models, finding that they surpass all other context\nwindow extension methods.\nWe broadly followed the training and evaluation procedures as outlined in [9].\n4.1\nTraining\nFor training, we extended the Llama 2 [39] 7B and 13B parameter models. No changes were made to\nthe LLaMA model architecture other than the calculation of the embedding frequencies as described\nin 3.4 with s = 16 and s = 32.\nWe used a learning rate of 2 \u00d7 10\u22125 with no weight decay and a linear warmup of 20 steps along with\nAdamW [24] \u03b21 = 0.9 and \u03b22 = 0.95. For s = 16 we fine-tuned for 400 steps with global batch\nsize 64 using PyTorch [26] Fully Sharded Data Parallelism [42] and Flash Attention 2 [13] on the\nPG19 dataset [29] chunked into 64k segments bookended with the BOS and EOS token. For s = 32\nwe followed the same procedure, but started from the finished s = 16 checkpoint and trained for an\nadditional 200 steps.\n4.2\nExtrapolation and Transfer Learning\nIn Code Llama [31], a dataset with 16k context was used with a scale factor set to s \u2248 88.6, which\ncorresponds to a context size of 355k. They show that the network extrapolates up to 100k context\nwithout ever seeing those context sizes during training. Similar to 3.1 and Rozi\u00e8re et al. [31], YaRN\nalso supports training with a higher scale factor s than the length of the dataset. Due to compute\nconstraints, we test only s = 32 by further fine-tuning the s = 16 model for 200 steps using the same\ndataset with 64k context.\nWe show in 4.3.1 that the s = 32 model successfully extrapolates up to 128k context using only 64k\ncontext during training. Unlike previous \"blind\" interpolation methods, YaRN is much more efficient\nat transfer learning when increasing the scale s. This demonstrates successful transfer learning from\ns = 16 to s = 32 without the network needing to relearn the interpolated embeddings, as the s = 32\nmodel is equivalent to the s = 16 model across the entire context size, despite only being trained on\ns = 32 for 200 steps.\n4.3\nEvaluation\nThe evaluations focus on three aspects:\n1. the perplexity scores of fine-tuned models with extended context window,\n2. the passkey retrieval task on fine-tuned models,\n3. the common LLM benchmark results of fine-tuned models,\n4.3.1\nLong Sequence Language Modeling\nTo evaluate the long sequence language modeling performances, we use the GovReport [18] and\nProof-pile [4] datasets both of which contain many long sequence samples. For all evaluations, the\ntest splits of both datasets were used exclusively. All perplexity evaluations were calculated using the\nsliding window method from Press et al. [27] with S = 256.\nFirstly, we evaluated how the model performed as the context window increased. We selected 10\nrandom samples from Proof-pile with at least 128k tokens each and evaluated the perplexity of each\nof these samples when truncated at 2k steps from a sequence length of 2k tokens through 128k tokens.\nTable 1 shows a side-by-side comparison of Llama-2 model extended from 4096 to 8192 context\nlength via PI (LLongMA-2 7b5), \"NTK-aware\" and YaRN. Note that PI and \"NTK-aware\" models\nwere trained using the methodology in Chen et al. [9], while YaRN used the same methodology but\n2.5x less training steps and data, as described in 4.\n5LLongMA-2 7b [28] is fine-tuned from Llama-2 7b, trained at 8k context length with PI using the RedPajama\ndataset [12].\n8\nExtension\nTrained\nContext\nEvaluation Context Window Size\nMethod\nTokens\nWindow\n2048\n4096\n6144\n8192\n10240\nPI (s = 2)\n1B\n8k\n3.92\n3.51\n3.51\n3.34\n8.07\nNTK (\u03b8 = 20k)\n1B\n8k\n4.20\n3.75\n3.74\n3.59\n6.24\nYaRN (s = 2)\n400M\n8k\n3.91\n3.50\n3.51\n3.35\n6.04\nTable 1: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents over Llama-2 extended via PI,\nNTK and YaRN\nWe further evaluated YaRN at the scale factor s = 16, 32 and compared them against a few open-\nsource models fine-tuned from Llama-2 and extended to more than 32k context window such as\nTogether.ai [37] and \"NTK-aware\" Code Llama [31]. The results are summarized in Table 2 (with a\nmore detailed plot in Figure 1).\nModel\nModel\nContext\nExtension\nEvaluation Context Window Size\nSize\nName\nWindow\nMethod\n8192\n32768\n65536\n98304\n131072\n7B\nTogether\n32k\nPI\n3.50\n2.64\n> 102\n> 103\n> 104\n7B\nCode Llama\n100k\nNTK\n3.71\n2.74\n2.55\n2.54\n2.71\n7B\nYaRN (s = 16)\n64k\nYaRN\n3.51\n2.65\n2.42\n> 101\n> 101\n7B\nYaRN (s = 32)\n128k\nYaRN\n3.56\n2.70\n2.45\n2.36\n2.37\n13B\nCode Llama\n100k\nNTK\n3.54\n2.63\n2.41\n2.37\n2.54\n13B\nYaRN (s = 16)\n64k\nYaRN\n3.25\n2.50\n2.29\n> 101\n> 101\n13B\nYaRN (s = 32)\n128k\nYaRN\n3.29\n2.53\n2.31\n2.23\n2.24\nTable 2: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents truncated to evaluation context\nwindow size\nWe observe that the model exhibits strong performance across the entire targeted context size, with\nYaRN interpolation being the first method to successfully extend the effective context size of Llama 2\nto 128k. Of particular note are the YaRN (s = 32) models, which show continued declining perplexity\nthrough 128k, despite the fine-tuning data being limited to 64k tokens in length, demonstrating that\nthe model is able to generalize to unseen context lengths.\nFurthermore, in Appendix B.1, we show the results of the average perplexity on 50 untruncated\nGovReport documents with at least 16k tokens per sample evaluated on the setting of 32k maximal\ncontext window without Dynamic Scaling in Table 4. Similar to the Proof-pile results, the GovReport\nresults show that fine-tuning with YaRN achieves good performance on long sequences.\n4.3.2\nPasskey Retrieval\nThe passkey retrieval task as defined in [25] measures a model\u2019s ability to retrieve a simple passkey\n(i.e., a five-digit number) from amongst a large amount of otherwise meaningless text. For our\nevaluation of the models, we performed 10 iterations of the passkey retrieval task with the passkey\nplaced at a random location uniformly distributed across the evaluation context window on different\ncontext window sizes ranging from 8k to 128k. Both 7b and 13b models fine-tuned using YaRN at\n128k context size passes the passkey retrieval task with very high accuracy (> 99%) within the entire\ncontext window size. We show detailed results in Appendix B.2.\n4.3.3\nStandardized Benchmarks\nThe Hugging Face Open LLM Leaderboard [19] compares a multitude of LLMs across a standard-\nized set of four public benchmarks. Specifically, we use 25-shot ARC-Challenge [11], 10-shot\nHellaSwag [41], 5-shot MMLU [17], and 0-shot TruthfulQA [23].\nTo test the degradation of model performance under context extension, we evaluated our models\nusing this suite and compared it to established scores for the Llama 2 baselines as well as publicly\navailable PI and \"NTK-aware\" models. The results are summarized in Table 3.\n9\nModel\nModel\nContext\nExtension\nARC-c\nHellaswag\nMMLU\nTruthfulQA\nSize\nName\nWindow\nMethod\n7B\nLlama 2\n4k\nNone\n53.1\n77.8\n43.8\n39.0\n7B\nTogether\n32k\nPI\n47.6\n76.1\n43.3\n39.2\n7B\nCode Llama\n100k\nNTK\n39.9\n60.8\n31.1\n37.8\n7B\nYaRN (s = 16)\n64k\nYaRN\n52.3\n78.8\n42.5\n38.2\n7B\nYaRN (s = 32)\n128k\nYaRN\n52.1\n78.4\n41.7\n37.3\n13B\nLlama 2\n4k\nNone\n59.4\n82.1\n55.8\n37.4\n13B\nCode Llama\n100k\nNTK\n40.9\n63.4\n32.8\n43.8\n13B\nYaRN (s = 16)\n64k\nYaRN\n58.1\n82.3\n52.8\n37.8\n13B\nYaRN (s = 32)\n128k\nYaRN\n58.0\n82.2\n51.9\n37.3\nTable 3: Performance of context window extensions methods on the Hugging Face Open LLM benchmark suite\ncompared with original Llama 2 baselines\nWe observe that there is minimal performance degradation between the YaRN models and their\nrespective Llama 2 baselines. We also observe that there was on average a 0.49% drop in scores\nbetween the YaRN s = 16 and s = 32 models. From this we conclude that the the iterative extension\nfrom 64k to 128k results in negligible performance loss.\n5\nConclusion\nIn conclusion, we have shown that YaRN improves upon all existing RoPE interpolation methods\nand can act as a drop-in replacement to PI, with no downsides and minimal implementation effort.\nThe fine-tuned models preserve their original abilities on multiple benchmarks while being able\nto attend to a very large context size. Furthermore, YaRN allows efficient extrapolation with fine-\ntuning on shorter datasets and can take advantage of transfer learning for faster convergence, both of\nwhich are crucial under compute-constrained scenarios. Finally, we have shown the effectiveness of\nextrapolation with YaRN where it is able to \"train short, and test long\".\n10\n6\nReproducibility\nTo aid in reproducibility, we provide, as supplementary material, the entirety of of the code used to\ntrain the YaRN models in Table 2, as well as the evaluation code that produced Figure 1 and Tables\n1, 2, 3, 4, and 5. The code also contains implementations of various extension methods referenced\nthroughout the paper. For training YaRN, we used the publicly available PG19 dataset [29] tokenized\nto 64k tokens.\nReferences\n[1] Mistrallite. URL https://huggingface.co/amazon/MistralLite.\n[2] Introducing Qwen-7B: Open foundation and human-aligned models (of the state-of-the-arts).\nURL https://github.com/QwenLM/Qwen-7B/blob/main/tech_memo.md.\n[3] Long-data collections. URL https://huggingface.co/datasets/togethercomputer/\nLong-Data-Collections.\n[4] Z. Azerbayev, E. Ayers, , and B. Piotrowski. Proof-pile, 2022. URL https://github.com/\nzhangir-azerbayev/proof-pile.\n[5] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy,\nK. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang,\nand S. Weinbach. GPT-NeoX-20B: An open-source autoregressive language model, 2022.\narXiv: 2204.06745.\n[6] bloc97.\nNTK-Aware\nScaled\nRoPE\nallows\nLLaMA\nmodels\nto\nhave\nextended\n(8k+)\ncontext\nsize\nwithout\nany\nfine-tuning\nand\nminimal\nperplexity\ndegradation.,\n2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_\nscaled_rope_allows_llama_models_to_have/.\n[7] bloc97. Add NTK-Aware interpolation \"by parts\" correction, 2023. URL https://github.\ncom/jquesnelle/scaled-rope/pull/1.\n[8] C. Chen.\nTransformer Inference Arithmetic, 2022.\nURL https://kipp.ly/blog/\ntransformer-inference-arithmetic/.\n[9] S. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models\nvia positional interpolation, 2023. arXiv: 2306.15595.\n[10] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao,\nP. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope,\nJ. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat,\nS. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito,\nD. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick,\nA. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee,\nZ. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck,\nJ. Dean, S. Petrov, and N. Fiedel. PaLM: Scaling language modeling with pathways, 2022.\narXiv: 2204.02311.\n[11] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think\nyou have solved question answering? try ARC, the AI2 Reasoning Challenge, 2018. arXiv:\n1803.05457.\n[12] T. Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURL https://github.com/togethercomputer/RedPajama-Data.\n[13] T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.\narXiv: 2307.08691.\n11\n[14] emozilla. Dynamically Scaled RoPE further increases performance of long context LLaMA\nwith zero fine-tuning, 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/\n14mrgpr/dynamically_scaled_rope_further_increases/.\n[15] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to\nsequence learning, 2017. arXiv: 1705.03122.\n[16] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, and S. Wang. LM-Infinite: Simple on-the-fly length\ngeneralization for large language models, 2023. arXiv: 2308.16137.\n[17] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\nmassive multitask language understanding. Proceedings of the International Conference on\nLearning Representations (ICLR), 2021.\n[18] L. Huang, S. Cao, N. Parulian, H. Ji, and L. Wang. Efficient attentions for long document\nsummarization. In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 1419\u20131436.\nAssociation for Computational Linguistics, June 2021.\n[19] Hugging Face. Open LLM Leaderboard, 2023. URL https://huggingface.co/spaces/\nHuggingFaceH4/open_llm_leaderboard.\n[20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\nT. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.\n[21] kaiokendev. Things I\u2019m learning while training superhot., 2023. URL https://kaiokendev.\ngithub.io/til#extending-context-to-8k.\n[22] A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P. Das, and S. Reddy. The impact of positional\nencoding on length generalization in transformers, 2023. arXiv: 2305.19466.\n[23] S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods.\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214\u20133252, May 2022.\n[24] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference\non Learning Representations, 2019.\n[25] A. Mohtashami and M. Jaggi. Landmark attention: Random-access infinite context length for\ntransformers, 2023. arXiv: 2305.16300.\n[26] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, A. Desmaison, A. K\u00f6pf, E. Yang, Z. DeVito, M. Raison, A. Tejani,\nS. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style,\nhigh-performance deep learning library. In NeurIPS, pages 8024\u20138035, 2019.\n[27] O. Press, N. Smith, and M. Lewis. Train Short, Test Long: Attention with linear biases enables\ninput length extrapolation. In International Conference on Learning Representations, 2022.\n[28] J. Quesnelle, E. Shippole, and \"Kaiokendev\". Llongma: Scaling rotary embeddings through lin-\near positional interpolation. https://huggingface.co/conceptofmind/LLongMA-2-7b/,\n2023.\n[29] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap. Compressive\ntransformers for long-range sequence modelling. In International Conference on Learning\nRepresentations, 2020.\n[30] A. Roberts, C. Raffel, K. Lee, M. Matena, N. Shazeer, P. J. Liu, S. Narang, W. Li, and Y. Zhou.\nExploring the limits of transfer learning with a unified text-to-text transformer. Technical report,\nGoogle, 2019.\n12\n[31] B. Rozi\u00e8re, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez,\nJ. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong,\nA. D\u00e9fossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve.\nCode Llama: Open foundation models for code, 2023. arXiv: 2308.12950.\n[32] P. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations.\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages\n464\u2013468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.\n[33] J. Su. Rectified rotary position embeddings. https://github.com/bojone/rerope, 2023.\n[34] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. RoFormer: Enhanced transformer with\nrotary position embedding, 2022. arXiv: 2104.09864.\n[35] Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, and F. Wei. A\nlength-extrapolatable transformer, 2022. arXiv: 2212.10554.\n[36] M. Tancik, P. P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ra-\nmamoorthi, J. T. Barron, and R. Ng. Fourier features let networks learn high frequency functions\nin low dimensional domains. In Proceedings of the 34th International Conference on Neural\nInformation Processing Systems, NIPS\u201920, Red Hook, NY, USA, 2020. Curran Associates Inc.\nISBN 9781713829546.\n[37] Together.ai.\nLLaMA-2-7B-32K,\n2023.\nURL\nhttps://huggingface.co/\ntogethercomputer/LLaMA-2-7B-32K.\n[38] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. LLaMA: Open and\nefficient foundation language models, 2023. arXiv: 2302.13971.\n[39] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\nR. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.\nLachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc., 2017.\n[41] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really\nfinish your sentence?\nIn Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 2019.\n[42] Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott,\nS. Shleifer, A. Desmaison, C. Balioglu, B. Nguyen, G. Chauhan, Y. Hao, and S. Li. PyTorch\nFSDP: Experiences on scaling fully sharded data parallel, 2023. arXiv: 2304.11277.\n13\nA\nAdditional details on interpolation methods\nA.1\nShort notes on the deduction of \"NTK-aware\" interpolation\nIn Section 3.1, we introduce a change of basis from b to b\u2032 in the definition of \"NTK-aware\"\ninterpolation method. Here is a short note on its mathematical deduction.\nRecall that our goal is to spread out the interpolation pressure across the hidden dimensions using a\nbase-change instead of scaling the frequencies by a fixed factor s. The property we want to guarantee\nis that: The lowest frequency needs to be scaled as much as linear positional scaling and the highest\nfrequency to stay constant.\nWe introduce a new base b\u2032 such that the last dimension matches the wavelength of linear interpolation\nwith a scale factor s. Since the original RoPE method skips odd dimensions in order to concatenate\nboth cos( 2\u03c0x\n\u03bb ) and sin( 2\u03c0x\n\u03bb ) components into a single embedding, the last dimension d \u2208 D is\n|D| \u2212 2.\nThe new base b\u2032 can be chosen so that\nb\u2032\n|D|\u22122\n|D|\n= s \u00b7 b\n|D|\u22122\n|D| .\n(23)\nSolving for b\u2032 yields\nb\u2032 = b \u00b7 s\n|D|\n|D|\u22122 .\n(24)\nA.2\nThe impact of pre-softmax scaling of YaRN on perplexity\nIn Section 3.4, we mention the impact of the factor t inside the softmax computation of attention\nweights. Here we fix 896 16k-token documents from RedPajama [12]6, and calculate their perplexity\nscores with different scaling 1/\n\u221a\nt. The result is in Figure 2. For comparison, recall that our\nrecommended factor in this case (s = 8) is given by the following.\nr\n1\nt = 0.1 ln(s) + 1 \u2248 1.208.\n(25)\n6We choose RedPajama because it is the open-source dataset closest to the training dataset of LLaMA as far\nas we are aware of.\n14\nTo show the impact of the factor 1/\n\u221a\nt on different token positions, we cut each 16k-token document\ninto chunks of 2048 tokens, and further plot the mean perplexity change comparing to t = 1 in\npercentages\nppl(t) \u2212 ppl(t = 1)\nppl(t = 1)\n(26)\nof each chunk. The plot is shown in Figure 3.\nTo further demonstrate the best values of t across all samples over different token positions, we plot\nthe sample counts with minimal perplexity at a given 1/\n\u221a\nt for each of the 8 position segments over\nthe 16k-token range in Figure 4.\nWe observe that:\n\u2022 for a suitable t, a sample may obtain better perplexity scores across the extended context\nwindow;\n\u2022 the best value of t is mostly consistent across different samples and different positions.\nWe remark that this finding is consistent for different values of s and the best value of t follows our\nrecommended formula (Eq. 22) closely.\nB\nAdditional tables and charts\nB.1\nGovReport evaluations\nIn Section 4.3.1, we mention the evaluation on GovReport documents. The evaluation results are\ndetailed in Table 4 below.\nB.2\nPasskey Retrieval\nHere we can observe that the lowest perplexity point alone does not provide a comprehensive depiction\non the \"effective context size\" that an LLM can attend to. While the Code Llama 13b model exhibits\nincreasing perplexity above 100k context lengths, it was still able to accurately retrieve the passkey at\na context length of 128k. This suggest that while the output of Code Llama might start to degrade in\nquality above 100k context size, it is still able to maintain strong retrieval capabilities.\nIn addition, as YaRN with s = 32 was trained for 200 more steps than YaRN with s = 16 while\nhaving a higher passkey accuracy with similar perplexity, we hypothesize that perplexity may not be a\ngreat indicator of whether an LLM is able to attend to all tokens and does not exhaustively determine\nlong context performance. This also suggests that the YaRN models with s = 16 might be relatively\nundertrained for the passkey retrieval task.\nB.3\nDynamic scaling on models without any fine-tuning\nWe first recall from Section 3.3 that the Dynamic Scaling technique is an inference-time technique\nthat dynamically update the factor s in interpolation methods such as PI, \"NTK-by-parts\" and YaRN.\nWe choose the original Llama 2, fix a sample in GovReport and calculate its perplexity on a sliding\nwindow of 256 tokens using RoPE, Dynamic-PI and Dynamic-YaRN. Since the original maximal\ncontext length of Llama 2 is 4096, we observe that Dynamic Scaling effectively extend the inference\nlength and Dynamic-YaRN achieves better performance than Dynamic-PI. The resulting chart is in\nFigure 5.\nWe see that\n\u2022 Dynamic Scaling effectively prevents the blow-up of perplexity score beyond pretrained\ncontext window;\n\u2022 Dynamic-YaRN outperforms Dynamic-PI in terms of long-range perplexity on pretrained\nLlama-2 without any finetuning.\n15\nFigure 2: Fix s = 8, compare the LLaMA 7b perplexity on 896 16k-token documents over different scaling\n1/\n\u221a\nt. The shaded area represents 1 standard deviation (68%).\nFigure 3: Fix s = 8, compare the mean of perplexity change percentages ppl(t) \u2212 ppl(t = 1)\nppl(t = 1)\nat different\nsegments of token positions on 896 16k-token documents over different scaling 1/\n\u221a\nt.\nModel\nModel\nContext\nExtension\nPerplexity\nSize\nName\nWindow\nMethod\n7B\nTogether\n32k\nPI\n3.67\n7B\nCode Llama\n100k\nNTK\n4.44\n7B\nYaRN (s = 16)\n64k\nYaRN\n3.59\n7B\nYaRN (s = 32)\n128k\nYaRN\n3.64\n13B\nCode Llama\n100k\nNTK\n4.22\n13B\nYaRN (s = 16)\n64k\nYaRN\n3.35\n13B\nYaRN (s = 32)\n128k\nYaRN\n3.39\nTable 4: Sliding window perplexity (S = 256) of 50 long GovReport documents with a fixed context window\nsize of 32k\n16\nFigure 4: The sample counts (out of the 896 samples) with minimal perplexity at a given 1/\n\u221a\nt for a given\nsegment of token positions over the 16k-token range.\nModel\nModel\nScaling\nContext\nTraining\nExtension\nPasskey\nPasskey\nSize\nName\nFactor (s)\nWindow\nData Context\nMethod\nContext\nAccuracy\n7B\nTogether\n4\n32k\n32k\nPI\n32k\n100%\n7B\nCode Llama\n88.6\n100k\n16k\nNTK\n112k\n94.3%\n7B\nYaRN\n16\n64k\n64k\nYaRN\n64k\n96.3%\n7B\nYaRN\n32\n128k\n64k\nYaRN\n128k\n99.4%\n13B\nCode Llama\n88.6\n100k\n16k\nNTK\n128k\n99.4%\n13B\nYaRN\n16\n64k\n64k\nYaRN\n64k\n97.5%\n13B\nYaRN\n32\n128k\n64k\nYaRN\n128k\n99.4%\nTable 5: Passkey retrieval performance of various models. The passkey context denotes the maximum tested\ncontext window size where the accuracy of passkey retrieval was >= 80%, and the passkey accuracy is the\naverage accuracy of passkey retrieval on all context sizes tested that were smaller or equal than the passkey\ncontext size.\nFigure 5: The comparison between RoPE, Dynamic-PI and Dynamic-YaRN using Llama 2 on a long\nGovReport sample. This model has not been finetuned for long context.\n17\n0\n20000\n40000\n60000\n80000\n100000\n120000\nContext Window\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\nPerplexity (lower is better)\nYarn-Mistral-7b-64k\nYarn-Mistral-7b-128k\namazon/MistralLite\nmistralai/Mistral-7B-v0.1\nFigure 6: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents truncated to evaluation\ncontext window size\nB.4\nMistral\nWe additionally extended the Mistral 7B v0.1 model [20], which broadly follows the Llama architec-\nture. For Mistral we trained a 64k context window model (s = 8) for 1000 steps using 16k sequence\nlengths with a constant learning rate of 1 \u00d7 10\u22126. The model\u2019s sliding window attention size was set\nto the context window size, effectively disabling sliding window attention. We then trained for an\nadditional 500 steps at s = 16 to arrive at a 128k context window model. The training data was a mix\nof the pre-train and fine-tune splits of Together Computer\u2019s Long-Data Collections [3].\nWe evaluated the models following the same procedure as described in 4.3.1, comparing against the\nbase v0.1 model and MistralLite [1], an NTK-aware (\u03b8 = 1M) version of v0.1. The results (Figure 6\nand Table 6) were consistent with those of the Llama family of models.\nModel\nModel\nContext\nExtension\nEvaluation Context Window Size\nSize\nName\nWindow\nMethod\n4096\n8192\n16384\n65536\n131072\n7B\nMistral v0.1\n8k\n-\n3.09\n2.96\n36.8\n> 103\n> 103\n7B\nMistralLite\n16k\nNTK\n3.26\n3.13\n47.3\n> 103\n> 103\n7B\nYaRN (s = 8)\n64k\nYaRN\n3.18\n3.04\n2.65\n2.20\n57.4\n7B\nYaRN (s = 16)\n128k\nYaRN\n3.21\n3.08\n2.68\n2.24\n2.19\nTable 6: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents truncated to evaluation context\nwindow size\n18\n"
  },
  {
    "title": "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
    "link": "https://arxiv.org/pdf/2309.00267.pdf",
    "upvote": "45",
    "text": "RLAIF: Scaling Reinforcement Learning from Human Feedback\nwith AI Feedback\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu,\nColton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, Sushant Prakash\nGoogle Research\n{harrisonlee,samratph,hassan}@google.com\nAbstract\nReinforcement learning from human feedback\n(RLHF) has proven effective in aligning large\nlanguage models (LLMs) with human prefer-\nences. However, gathering high-quality hu-\nman preference labels can be a time-consuming\nand expensive endeavor. RL from AI Feed-\nback (RLAIF), introduced by Bai et al., of-\nfers a promising alternative that leverages a\npowerful off-the-shelf LLM to generate pref-\nerences in lieu of human annotators. Across\nthe tasks of summarization, helpful dialogue\ngeneration, and harmless dialogue generation,\nRLAIF achieves comparable or superior perfor-\nmance to RLHF, as rated by human evaluators.\nFurthermore, RLAIF demonstrates the ability\nto outperform a supervised fine-tuned baseline\neven when the LLM preference labeler is the\nsame size as the policy. In another experiment,\ndirectly prompting the LLM for reward scores\nachieves superior performance to the canonical\nRLAIF setup, where LLM preference labels\nare first distilled into a reward model. Finally,\nwe conduct extensive studies on techniques for\ngenerating aligned AI preferences. Our results\nsuggest that RLAIF can achieve human-level\nperformance, offering a potential solution to\nthe scalability limitations of RLHF.\n1\nIntroduction\nReinforcement Learning from Human Feedback\n(RLHF) is an effective technique for aligning lan-\nguage models to human preferences (Stiennon\net al., 2020; Ouyang et al., 2022). It is cited as\none of the key drivers of success in modern conver-\nsational language models, such as ChatGPT (Liu\net al., 2023) and Bard (Manyika, 2023). Train-\ning language models with reinforcement learning\n(RL) enables optimization on complex, sequence-\nlevel objectives that are not easily differentiable\nand therefore ill-suited for traditional supervised\nfine-tuning (SFT).\nOne obstacle for employing RLHF at scale is its\ndependence on high-quality human preference la-\n71%\n63%\n73%\n64%\nWin Rate vs. SFT\n50%\n60%\n70%\n80%\nSummarization\nHelpfulness\nRLAIF\nRLHF\nRLAIF and RLHF Win Rates\n64%\n76%\n88%\nHarmless Rate\n50%\n60%\n70%\n80%\n90%\nSFT\nRLHF\nRLAIF\nHarmless Rate by Policy\nFigure 1: Human evaluators strongly prefer RLAIF and\nRLHF over the SFT baseline for summarization and\nhelpful dialogue generation. Their difference in win\nrates vs. SFT is not statistically significant. Further-\nmore, when compared head-to-head, RLAIF is equally\npreferred to RLHF. For harmless dialogue generation,\nRLAIF outperforms RLHF.\nbels. This raises the question of whether artificially\ngenerated labels can be a viable substitute. Gen-\nerating labels with large language models (LLMs)\nis one promising approach, as LLMs have shown\na high degree of alignment with human judgment\n(Gilardi et al., 2023; Ding et al., 2023). Bai et al.\n(2022b) was the first effort to explore Reinforce-\nment Learning from AI Feedback (RLAIF)1, where\n1This is distinct from \u201cConstitutional AI\u201d, which improves\nupon a supervised learning model through iteratively asking\nan LLM to generate better responses according to a a set of\narXiv:2309.00267v2  [cs.CL]  1 Dec 2023\nRL was conducted using a reward model trained\non LLM preferences. Bai et al. (2022b) showed\nthat utilizing a hybrid of human and AI preferences,\nin conjunction with their \u201cConstitutional AI\u201d self-\nrevision technique, outperforms supervised fine-\ntuning for training a conversational assistant. How-\never, it did not directly compare the efficacy of\nhuman vs. AI feedback, leaving the question of\nwhether RLAIF can be a suitable alternative to\nRLHF unanswered.\nIn this work, we study the impact of RLAIF\nand RLHF (see Figure 2) on three text genera-\ntion tasks: summarization, helpful dialogue gen-\neration, and harmless dialogue generation. Our\nexperiments show that RLAIF and RLHF are pre-\nferred by humans over the SFT baseline 71% and\n73% of the time for summarization and 63% and\n64% of the time for helpful dialogue generation,\nrespectively, where the differences between RLAIF\nand RLHF win rates are not statistically signifi-\ncant. We also conduct a head-to-head comparison\nof RLAIF against RLHF and find that both policies\nare equally preferred2. For harmless dialogue gen-\neration, human evaluators rated the harmlessness\nof each response independently. RLAIF scored\na higher harmless rate than RLHF, and both out-\nperformed the SFT baseline (88%, 76%, and 64%,\nrespectively). These results suggest that RLAIF is a\nviable alternative to RLHF that does not depend on\nhuman annotation, while offering appealing scaling\nproperties.\nAdditionally, we investigate two related ques-\ntions. First, we explore whether RLAIF can im-\nprove upon a SFT policy when the LLM labeler\nhas the same number of parameters as policy. Even\nin this scenario, RLAIF significantly improves over\nthe SFT baseline.\nSecond, we conduct an ex-\nperiment where the off-the-shelf LLM is directly\nprompted for reward scores during RL, bypassing\nthe step of distilling LLM preference labels into\na reward model. This method achieves an even\nhigher win rate over SFT than the canonical distil-\nlation method.\nFinally, we study techniques to maximize the\nalignment of AI-generated preferences to human\npreferences.\nWe find that soliciting chain-of-\nthought reasoning (Wei et al., 2022) consistently\nimproves alignment, while using a detailed pream-\nwritten value statements. Both were introduced in Bai et al.\n(2022b) and are sometimes conflated.\n2The win rate for one policy vs. the other is not statistically\nsignificantly different from 50%\nble and few-shot prompting (Brown et al., 2020)\nare only beneficial for certain tasks. We also con-\nduct scaling experiments to examine the trade-off\nbetween the size of the LLM labeler and alignment\nwith human preferences.\nThe main contributions of this work are as fol-\nlows:\n1. We demonstrate that RLAIF achieves compa-\nrable or superior performance to RLHF on\nthe tasks of summarization, helpful dialogue\ngeneration, and harmless dialogue generation.\n2. We show that RLAIF can improve upon a SFT\npolicy even when the LLM labeler is the same\nsize as the policy.\n3. We find that directly prompting the LLM\nfor reward scores during RL can outperform\nthe canonical setup where a reward model is\ntrained on LLM preferences.\n4. We compare various techniques for generat-\ning AI labels and identify optimal settings for\nRLAIF practitioners.\n2\nMethodology\nThis section describes the techniques used to gener-\nate preferences with an LLM, how RL is conducted,\nand evaluation metrics. Preliminaries on RLHF are\nprovided in Appendix A.\n2.1\nPreference Labeling with LLMs\nWe annotate preferences between pairs of candi-\ndates with an \u201coff-the-shelf\u201d LLM - a model pre-\ntrained or instruction-tuned (Wei et al., 2021) for\ngeneral usage but not fine-tuned for a specific down-\nstream task. Given a piece of text and two candidate\nresponses, the LLM is asked to rate which response\nis preferred. The prompt is structured as follows\n(examples in Tables 15 and 21):\n1. Preamble - Introduction and instructions de-\nscribing the task at hand\n2. Few-shot exemplars (optional) - An example\ninput context, a pair of responses, a chain-of-\nthought rationale (optional), and a preference\nlabel\n3. Sample to annotate - An input context and a\npair of responses to be labeled\n4. Ending - Ending text to prompt the LLM (e.g.\n\u201cPreferred Response=\u201d)\nAfter the prompt is given to the LLM, we ex-\ntract the log-probabilities of generating the tokens\nFigure 2: A diagram depicting RLAIF (top) vs. RLHF (bottom)\n\u201c1\u201d and \u201c2\u201d and compute the softmax to obtain a\npreference distribution.\nThere are numerous alternatives to obtain pref-\nerence labels from LLMs, such as extracting the\npreference from a free-form generated response\n(e.g. \u201cThe first response is better\u201d), or represent-\ning the preference distribution as a one-hot encod-\ning. However, we choose our method because it\nis straightforward to implement and conveys more\ninformation than a one-hot encoding through its\ndistributed representation of preferences.\nWe experiment with two styles of preambles:\n\u201cBase\u201d, which essentially asks \u201cwhich response is\nbetter?\u201d, and \u201cDetailed\u201d, which resembles detailed\nrating instructions that would be given to human\npreference annotators (see Table 16 for pream-\nbles for the summarization task). We also experi-\nment with in-context learning (Brown et al., 2020),\nwhere high-quality exemplars were hand-selected\nto cover a range of topics.\n2.1.1\nAddressing Position Bias\nThe order in which candidates are shown to\nan LLM can bias which candidate it prefers\n(Pezeshkpour and Hruschka, 2023; Wang et al.,\n2023). We find evidence of position bias, which\nis more pronounced with smaller sizes of LLM\nlabelers (see Appendix B).\nTo mitigate position bias in preference labeling,\nwe make two inferences for every pair of candi-\ndates, where the order in which candidates are pre-\nsented to the LLM is reversed for the second in-\nference. The results from both inferences are then\naveraged to obtain the final preference distribution.\n2.1.2\nChain-of-thought Reasoning\nWe experiment with eliciting chain-of-thought\n(CoT) reasoning (Wei et al., 2022) from our AI\nlabelers through a two-step inference procedure.\nFirst, we replace the Ending of the standard prompt\n(e.g. \u201cPreferred Summary=\u201d) with a sentence ask-\ning for thoughts and explanation (e.g. \u201cConsider\nthe coherence, accuracy, coverage, and overall\nquality of each summary and explain which one is\nbetter. Rationale:\u201d) and then decode a response\nfrom the LLM. Then, we concatenate the origi-\nnal prompt, the response, and the standard Ending\nstring together, and follow the scoring procedure in\nSection 2.1 to obtain a preference distribution. See\nFigure 3 for an illustration.\nIn zero-shot prompts, the LLM is not given an\nexample of what reasoning should look like. In\nfew-shot prompts, we provide examples of CoT\nreasoning for the model to follow. See Tables 17\nand 18 for examples.\n2.2\nReinforcement Learning from AI\nFeedback\n2.2.1\nDistilled RLAIF\nWe describe our adaptation of the canonical RLAIF\nsetup below, which we also refer to as \u201cdistilled\nRLAIF\u201d. Unless otherwise mentioned, RLAIF is\ncarried out using this method.\nAfter labeling preferences with an LLM, a re-\nward model (RM) is trained on these labels. Since\nour approach produces soft labels (e.g. [0.6, 0.4]),\nFigure 3: An illustration of the process of obtaining AI-generated labels for summarization preferences. The LLM\nis first prompted to explain its thoughts on the quality of the two candidates (blue). The LLM\u2019s response is then\nappended to the original prompt (orange) and fed to the LLM a second time to generate a preference distribution\nover \u201c1\u201d vs. \u201c2\u201d based on their log-probabilities (green).\nwe apply a cross-entropy loss to the softmax of the\nreward scores generated by the RM. The softmax\nconverts the RM scores into a probability distri-\nbution. We note that training a RM on a dataset\nof AI labels can be viewed as a form of model\ndistillation.\nFinally, we conduct reinforcement learning to\ntrain the RLAIF policy model, using the RM to\nassign rewards to model responses.\n2.2.2\nDirect RLAIF\nAn alternative approach is to directly use LLM\nfeedback as the reward signal in RL. This enables\nbypassing the intermediate stage of training a RM\nthat approximates the preferences of the LLM.\nThe LLM is prompted to rate the quality of a\ngeneration between 1 and 10. Similar to the for-\nmat mentioned in Section 2.1, the prompt contains\nhigh-level details on the structure of the input and\nthe dimensions along which to rate a generation\n(e.g. factuality, coherence). Then, the likelihood\nof each score token between 1 and 10 is com-\nputed, the likelihoods are normalized to a prob-\nability distribution, a weighted score is calculated\nas s(x|c) = P10\ni=1 iP(i|x, c), and then the score is\nagain normalized to the range [\u22121, 1]. Additional\ndetails on the prompting technique can be found in\nthe Appendix D.\nFinally, RL is conduct RL in a similar manner to\n\u201cdistilled RLAIF\u201d, where the direct score is used\nas reward instead of the score from a RM. This\napproach is more computationally expensive than\nthe canonical setup when the AI labeler is larger\nthan the RM.\n2.3\nEvaluation\nWe evaluate our results with three metrics - AI\nLabeler Alignment, Win Rate, and Harmless Rate.\nAI Labeler Alignment measures the accuracy of\nAI-labeled preferences with respect to human pref-\nerences. For a single example, a soft AI-labeled\npreference is first converted to a binary representa-\ntion (e.g. [0.6, 0.4] \u2192 [1, 0]). Then, a 1 is assigned\nif the label agrees with the human preference and\n0 otherwise. The alignment accuracy zacc can be\nexpressed as follows:\nzacc = 1\nD\nD\nX\ni=1\n1[arg max\nj\nP AI\ni,j = pH\ni ],\nwhere D is the size of the preference dataset,\nP AI \u2208 RD\u00d72 is the matrix of soft AI preferences,\nand phuman \u2208 RD is the corresponding vector of\nhuman preferences, containing elements 0 or 1 to\ndenote whether the first or second response is pre-\nferred, respectively.\nWin Rate evaluates the end-to-end quality of two\npolicies by measuring how often one policy is pre-\nferred by human annotators over another. Given an\ninput and two generations, human annotators select\nwhich generation they prefer. The percentage of\ninstances where policy A is preferred over policy\nB is referred to as the \u201cwin rate of A vs. B\u201d. A\n50% win rate indicates that A and B are equally\npreferred.\nHarmless Rate measures the percentage of re-\nsponses that are considered harmless by human\nevaluators. We evaluate the harmless dialogue gen-\neration task with this metric instead of Win Rate,\nbecause we find that many responses are equally\nsafe, making it difficult to assign relative rankings.\n3\nExperimental Details\n3.1\nDatasets\nWe use the following datasets for our experiments:\n\u2022 Reddit TL;DR (Stiennon et al., 2020) - posts\nfrom Reddit3 accompanied by summaries of\nthe posts.\n\u2022 OpenAI\u2019s Human Preferences (Stiennon et al.,\n2020) - a dataset created from a subset of Red-\ndit TL;DR. Each example comprises a post,\ntwo candidate summaries, and a rating from a\nhuman annotator indicating which summary\nis preferred.\n\u2022 Anthropic Helpful and Harmless Human Pref-\nerences (Bai et al., 2022a) - conversations be-\ntween a human and an AI assistant, where\neach conversation has two possible AI assis-\ntant responses - one preferred and the other\nnon-preferred, according to a human annota-\ntor. Preference is based on which response\nis more informative and honest for the help-\nful task, and which response is safer for the\nharmless task.\nMore dataset details can be found in Appendix C.\nWe also experimented with the Stanford Human\nPreferences dataset (Ethayarajh et al., 2022), but\nwe found that both RLHF and RLAIF policies did\nnot show meaningful improvements over the SFT\nbaseline after correcting for length biases, using\nthe procedure in Appendix J.\n3.2\nLLM Labeling\nTo enable fast experiment iteration when evaluating\nAI labeling techniques, we randomly downsampled\nthe training split of each preference dataset. For\nsummarization, an additional filter was applied to\nonly include examples where human annotators\npreferred one summary over the other with high\nconfidence4. After downsampling and filtering,\n3www.reddit.com\n4This follows the evaluation procedure in Stiennon et al.\n(2020). Examples with confidence scores of 1, 2, 8, and 9\nwere considered to be \u201chigh-confidence\u201d\nthere were roughly 3-4k examples for each task5.\nAI labeler alignment metrics were calculated on\nthese downsampled datasets.\nPaLM 2 (Google et al., 2023) is used as the LLM\nfor labeling preferences. The versions used are\ninstruction-tuned but not previously trained with\nRL. Unless otherwise specified, AI labels were\ngenerated using PaLM 2 Large (L) with the best-\nperforming prompt in Section 4.4. For more details\non LLM labeling, see Appendix D.\n3.3\nModel Training\nAll SFT models are initialized from PaLM 2 Extra-\nSmall (XS). For summarization, the SFT model is\nproduced by fine-tuning PaLM 2 XS on the Reddit\nTL;DR dataset. For all other tasks, an instruction-\ntuned variant of PaLM 2 is used in lieu of task-\nspecific fine-tuning.\nRMs are also derived from PaLM 2 XS. RMs\nare fine-tuned on the entire training split of the\ncorresponding preference dataset, where the label\nis the AI preference for AI feedback RMs and the\noriginal human preference label in the dataset for\nhuman feedback RMs. RM accuracies can be found\nin Appendix G.\nIn the RL phase, the policy is trained with a\nmodified version of REINFORCE (Williams, 1992)\nadapted to the language modeling domain. While\nmany recent works use Proximal Policy Optimiza-\ntion (PPO) (Schulman et al., 2017) - a related\nmethod that adds a few techniques to make train-\ning more conservative and stable (e.g. clipping the\nobjective function), we use REINFORCE with a\nbaseline given that it is simpler yet still effective for\nthe problem at hand. Both policy and value models\nare initialized from the SFT model. For summa-\nrization, the policy is rolled out on the training split\nof the Reddit TL;DR dataset. In other words, the\ninitial states for RL are the original posts from the\ndataset prior to summarization. For the helpful and\nharmless tasks, the initial states are drawn from\nthe training splits of the preference datasets. For\nsummarization, simple post-processing is applied\nto responses generated by RL-trained policies as\ndescribed in Appendix E.\nFor additional details on the RL formulation and\nmodel training, see Appendices F and G.\n5We sample 15%, 10%, and 10% of the training splits\nfor summarization, helpful dialogue generation, and harmless\ndialogue generation, respectively.\n3.4\nHuman Evaluation\nFor experiments evaluated by win rates, evaluators\nwere presented with an input context and multiple\nresponses generated from different policies (e.g.\nRLAIF, RLHF, and SFT). They were then asked\nto rank responses in order of quality without ties,\nas seen in Figure 4. Input contexts were drawn\nfrom test splits of datasets, which were not used\nfor training or any other evaluation6. Rankings\nwere used to calculate win rates with respect to\npairs of policies. For harmless dialogue generation,\nevaluators were asked to independently rate each\nresponse as harmless or harmful.\nFor more details on human evaluation, see Ap-\npendix I.\n4\nResults\n4.1\nRLAIF vs. RLHF\nRLAIF achieves performance gains on par with or\nbetter than RLHF on all three tasks (see Figure 1\nand Table 1). RLAIF and RLHF are preferred by\nhuman evaluators over the baseline SFT policy 71%\nand 73% of the time for summarization7 and 63%\nand 64% for helpful dialogue generation, respec-\ntively. The difference in win rates between RLAIF\nvs. SFT and RLHF vs. SFT are not statistically sig-\nnificant. When directly comparing RLAIF against\nRLHF, they are equally preferred - i.e. the win rate\nis not statistically significantly different from 50%.\nFor harmless dialogue generation, RLAIF achieves\na harmless rate of 88%, outperforming both RLHF\nand SFT, which score 76% and 64%, respectively8.\nFigure 5 contains an example of SFT, RLAIF,\nand RLHF summaries. To better understand how\nRLAIF compares to RLHF, we qualitatively com-\npare responses generated by both policies for sum-\nmarization in Section 5.\nAs observed in Stiennon et al. (2020), RLAIF\nand RLHF policies tend to generate longer re-\nsponses than the SFT policy, which may be par-\ntially responsible for their higher win rates. We\nconduct post-hoc analysis to control for length and\nfind that both RLAIF and RLHF policies still out-\n6For summarization, we used the test split of Reddit\nTL;DR. For helpful and harmless dialogue generation, we\nused test splits from the preference datasets, detailed in Ap-\npendix C.\n7RLAIF and RLHF are also preferred over the human\nreference summaries in Reddit TL;DR 79% and 80% of the\ntime, respectively.\n8RLAIF achieves a statistically significant improvement\nover RLHF and SFT, according to a two-sample t-test.\nperform the SFT policy, and by similar margins to\none another. See Appendix J for details.\nOne natural question that arises is whether there\nis value in combining human and AI feedback. We\nexperimented with combining both types of feed-\nback but did not see an improvement beyond using\nhuman feedback alone. However, we believe that\nthere are several alternative training setups that\ncould demonstrate value in combining both forms\nof feedback. See Appendix K for details.\nThese results suggest that RLAIF is a viable\nalternative to RLHF that does not depend on human\nannotation. In addition to expediting labeling time\nand reducing dependence on annotation services,\nanother key benefit of AI labeling is cost reduction.\nWe estimate the cost of labeling with an LLM to\nbe over 10x cheaper than human annotation. See\nAppendix L for detailed calculations.\n4.2\nTowards Self-Improvement\nIn Section 4.1, the LLM used to label preferences\n(PaLM 2 L) is much larger than the policy being\ntrained (PaLM 2 XS). Going one step further, one\nmight wonder if RLAIF can yield improvements\nwhen the AI labeler is the same size as the policy.\nOn the task of summarization, we conduct RLAIF\nwhere PaLM 2 XS is used as the AI labeler instead\nof PaLM 2 L. The rest of the setup mimics the\nexperiment in Section 4.1. We refer to this setup as\n\u201csame-size RLAIF\u201d.\nHuman annotators prefer same-size RLAIF 68%\nof the time over SFT (see Table 1). For reference,\nRLAIF using an AI labeler larger than the policy is\npreferred 71% over SFT9. This result demonstrates\nthat RLAIF can yield improvements even when the\nAI labeler is the same size as the policy LLM.\nWe note that the AI labeler and initial policy are\nnot the exact same model. The AI labeler is the\ninstruction-tuned PaLM 2 XS, whereas the initial\npolicy is PaLM 2 XS fine-tuned on Reddit TL;DR\nsummarization. Additionally, the summaries rated\nby the AI labeler were generated by policies created\nby the original dataset curators. For these reasons,\nwe do not consider this experiment a strict case of\n\u201cself-improvement\u201d(Huang et al., 2022). However,\nwe believe that these results show great promise\nfor this research direction.\n9The difference between win rates between \u201csame-size\nRLAIF vs. SFT\u201d and \u201cRLAIF vs. SFT\u201d is not statistically\nsignificant. For a two-sample t-test, p-value = 0.07. At alpha\n= 0.05, this difference is not statistically significant.\nWin Rate\nHarmless Rate\nComparison\nSumma\n-rization\nHelpful\ndialogue\nModel\nHarmless\ndialogue\nRLAIF vs SFT\n71%\n63%\nSFT\n64%\nRLHF vs SFT\n73%\n64%\nRLHF\n76%\nRLAIF vs RLHF\n50%\n52%\nRLAIF\n88%\nSame-size RLAIF vs SFT\n68%\nDirect RLAIF vs SFT\n74%\nDirect RLAIF vs Same-size RLAIF\n60%\nTable 1: Left side: Win rates when comparing generations from two different models for the summarization and the\nhelpful dialogue tasks, judged by human evaluators. Right side: Harmless rates across policies for the harmless\ndialogue task, judged by human evaluators.\n4.3\nDirect RLAIF\nIn Sections 4.1 and 4.2, AI feedback was distilled\ninto a RM. On the summarization task, we experi-\nment with using an off-the-shelf LLM to directly\nprovide rewards during RL, bypassing RM train-\ning entirely. Since using a large AI labeler in RL\nis computationally expensive, we use the smaller\ninstruction-tuned PaLM 2 XS as the off-the-shelf\nLLM. We refer to this setup as \u201cdirect RLAIF\u201d.\nHuman annotators prefer responses from direct\nRLAIF 74% of the time over SFT responses (see\nTable 1). To understand the impact of directly uti-\nlizing LLM feedback in RL, we compare this result\nto the same-size RLAIF policy from Section 4.2,\nwhich solely differs in training a RM that provides\nrewards during RL. Direct RLAIF outperforms\nsame-size RLAIF, which achieves a statistically\nsignificantly lower win rate of 68%. Furthermore,\nwhen shown responses side-by-side, raters prefer\ndirect RLAIF over same-size RLAIF 60% of the\ntime10. One hypothesis for the improved quality is\nthat bypassing the distillation from AI preferences\ninto a RM enables information to flow directly from\nthe off-the-shelf LLM to the policy.\n4.4\nPrompting Techniques\nWe experiment with three types of prompting vari-\nations - preamble specificity, chain-of-thought rea-\nsoning, and in-context learning (see Table 2). We\nobserve that eliciting chain-of-thought reasoning\ngenerally improves AI labeler alignment, while\nthe impacts of preamble specificity and in-context\nlearning vary across tasks. The best prompts outper-\nform the base prompts (\u201cBase 0-shot\u201d) by +1.9%,\n+1.3%, and +1.7% for summarization, helpfulness,\n10This is statistically significantly different from 50% ac-\ncording to a two-sample t-test.\nAI Labeler Alignment\nPrompt\nSummary H1\nH2\nBase 0-shot\n76.1%\n67.8%\n69.4%\nBase 1-shot\n76.0%\n67.1%\n71.7%\nBase 2-shot\n75.7%\n66.8%\n72.1%\nBase + CoT 0-shot\n77.5%\n69.1%\n70.6%\nDetailed 0-shot\n77.4%\n67.6%\n70.1%\nDetailed 1-shot\n76.2%\n67.6%\n71.5%\nDetailed 2-shot\n76.3%\n67.3%\n71.6%\nDetailed 8-shot\n69.8%\n\u2013\n\u2013\nDetailed + CoT 0-shot 78.0%\n67.8%\n70.1%\nDetailed + CoT 1-shot 77.4%\n67.4%\n69.9%\nDetailed + CoT 2-shot 76.8%\n67.4%\n69.2%\nTable 2: We observe that eliciting chain-of-thought rea-\nsoning tends to improve AI labeler alignment, while\nfew-shot prompting and detailed preambles have mixed\neffects across tasks. H1 refers to helpfulness, H2 to\nharmlessness.\nand harmlessness, respectively.\nDetailed preambles consistently improve align-\nment for summarization, while yielding mixed re-\nsults for helpful and harmless dialogue generation.\nWe hypothesize that summarization benefits more\nfrom a specific preamble due to the high complexity\nof this task. On the other hand, rating helpfulness\nand harmlessness are more intuitive to grasp, and\ntherefore may benefit less from detailed instruc-\ntions.\nChain-of-thought reasoning improves alignment\nconsistently for summarization. For helpful and\nharmless dialogue generation, CoT only improves\nalignment when paired with the \u201cBase\u201d preamble.\nSurprisingly, we observe that few-shot in-context\nlearning only improves alignment for harmless di-\nalogue generation11. For summarization and help-\n11We verified that all inputs used in these experiments fit\nfulness, alignment monotonically decreases as the\nnumber of exemplars increases. It seems unlikely\nthat this effect is a result of exemplar quality, as\nexemplars were carefully handpicked to be high-\nquality and representative of each preference task.\nFurthermore, we conducted 10 trials for \u201cBase 1-\nshot\u201d on summarization, where a different exem-\nplar was randomly selected for each trial. The\nmaximum AI labeler alignment from all trials was\n76.1%, which still did not surpass \u201cBase 0-shot\u201d\nin terms of AI labeler alignment. One hypothesis\nfor why exemplars do not help is that the summa-\nrization and helpful dialogue generation tasks may\nalready be sufficiently well-understood by the pow-\nerful AI labeler, rendering the exemplars unhelpful\nor distracting. It\u2019s interesting to note that in-context\nlearning is still an important research area that is\nnot fully understood (Min et al., 2022; Wang et al.,\n2022a).\nFor summarization, we compare against human\ninter-annotator agreement to get a sense of how\nwell our LLM labeler performs in absolute terms.\nStiennon et al. (2020) estimated that agreement rate\nfor the OpenAI human preference dataset was 73-\n77%, suggesting that the off-the-shelf LLM achiev-\ning 78% alignment performs well in absolute terms.\nWe\nalso\nconduct\nexperiments\nwith\nself-\nconsistency (Wang et al., 2022b), where multiple\nchain-of-thought rationales are sampled with tem-\nperature T > 0. The preference distributions gen-\nerated by the LLM are averaged together to ar-\nrive at the final preference label. We find that self-\nconsistency strictly degrades AI labeler alignment\n(see Appendix M).\nWe hypothesize that higher AI labeler alignment\nleads to improvements in RLAIF policies. To this\nend, we conduct an experiment on the end-to-end\nsensitivity to AI labeler alignment. Two RLAIF\npolicies are trained that only differ in the alignment\nscores of AI labels. Results show that the policy\ntrained with more aligned AI labels achieves a sig-\nnificantly higher win rate. However, this study only\ncompares two policies, and rigorous experimenta-\ntion is required to draw definitive conclusions. See\nAppendix N for details.\n4.5\nSize of LLM Labeler\nLarge model sizes are not widely accessible and\ncan be slow and expensive to run. On the task of\nsummarization, we experiment with labeling prefer-\nwithin our AI labeler\u2019s context length.\nences with varying LLM sizes and observe a strong\nrelationship between size and alignment (see Table\n3). Alignment decreases -4% when moving from\nPaLM 2 Large (L) to PaLM 2 Small (S), and de-\ncreases another -11% when moving down to PaLM\n2 XS - a trend consistent with scaling behaviors ob-\nserved in other work (Kaplan et al., 2020). Besides\ngeneral model capability, another contributing fac-\ntor to this trend may be that smaller LLMs are more\nsusceptible to position bias (see Appendix B).\nOn the other end of this trend, these results also\nsuggest that scaling up AI labeler size may pro-\nduce even higher quality preference labels. Since\nthe AI labeler is only used to generate preference\nexamples once and is not called during RL, using\nan even larger AI labeler is not necessarily pro-\nhibitively expensive.\nModel Size\nAI Labeler Alignment\nPaLM 2 L\n78.0%\nPaLM 2 S\n73.8%\nPaLM 2 XS\n62.7%\nTable 3: AI labeler alignment increases as the size of\nthe LLM labeler increases.\n5\nQualitative Observations\nTo better understand how RLAIF compares to\nRLHF, we inspected responses generated by both\npolicies for the summarization task.\nIn many\ncases, the two policies produced similar summaries,\nwhich is reflected in their similar win rates. How-\never, we identified two patterns where they some-\ntimes diverged.\nThe first pattern we observed is that in some\ncases, RLAIF hallucinates when RLHF does not.\nThe hallucinations in RLHF summaries sound plau-\nsible but are inconsistent with the original text. For\ninstance, in Example #1 of Table 23, the RLHF\nsummary states that the author is 20 years old, but\nthis is neither mentioned nor implied by the source\ntext. The second pattern we observed is that RLAIF\nsometimes produces less coherent or grammatical\nsummaries than RLHF. For instance, in Example #1\nof Table 24, the RLAIF summary generates run-on\nsentences.\nMore systematic analysis is required to identify\nif these patterns exist at scale, which we leave to\nfuture work.\n6\nRelated Work\nLLMs have shown impressive performance over\na wide range of NLP tasks (Brown et al., 2020;\nThoppilan et al., 2022; Chowdhery et al., 2022;\nGoogle et al., 2023; OpenAI, 2023a). For several\nof these tasks, RL has emerged as an effective op-\ntimization technique. While initial applications of\nRL on tasks such as translation (Wu et al., 2016,\n2018) and summarization (Gao et al., 2019; Wu\nand Hu, 2018) used automatic evaluation metrics\nas rewards, such simplified formulations of rewards\ndid not fully align with human notions of quality.\nReinforcement learning from human feed-\nback (Christiano et al., 2017) has been used as\na technique to directly align LLMs with human\npreferences (Ziegler et al., 2019) through train-\ning a reward model on pairwise comparisons of\nnatural language responses. It has been success-\nfully applied for summarization (Stiennon et al.,\n2020), generalized instruction following (Ouyang\net al., 2022; Lai et al., 2023), dialogue (Gilardi\net al., 2023; Manyika, 2023; Glaese et al., 2022;\nBai et al., 2022a) and question answering (Nakano\net al., 2021).\nLLMs have also been extensively used for data\ngeneration (Wang et al., 2021b; Meng et al., 2023),\naugmentation (Feng et al., 2021) and in self-\ntraining setups (Wang et al., 2022b; Madaan et al.,\n2023). Bai et al. (2022b) introduced the idea of\nRLAIF, which used LLM labeled preferences in\nconjunction with human labeled preferences to\njointly optimize for the two objectives of helpful-\nness and harmlessness. Recent works have also\nexplored related techniques for generating rewards\nfrom LLMs (Roit et al., 2023; Kwon et al., 2022;\nYang et al., 2023). These works demonstrate that\nLLMs can generate useful signals for RL fine-\ntuning, which inspired this work\u2019s investigation\ninto whether LLMs can serve as a viable alterna-\ntive to humans in collecting preference labels for\nRL.\n7\nConclusion\nIn this work, we show that RLAIF achieves com-\nparable improvements to RLHF on three text gen-\neration tasks. Our experiments show that RLAIF\ngreatly improves upon a SFT baseline, and the mar-\ngin of improvement is on par with or greater than\nthat of RLHF. Furthermore, in head-to-head com-\nparisons, RLAIF and RLHF are preferred at sim-\nilar rates by humans. Additionally, we show that\nRLAIF is effective even when the LLM labeler is\nthe same size as the policy, and directly prompting\nthe LLM labeler to provide rewards during RL can\noutperform the canonical RLAIF setup that distills\npreferences into a separate RM. Finally, we study\nthe impact of AI labeling techniques on alignment\nto human preferences.\nWhile this work highlights the potential of\nRLAIF, there remain many fascinating open ques-\ntions, such as whether conducting RLAIF itera-\ntively can achieve additional gains (i.e. use the\nmost recent RLAIF policy to generate new re-\nsponse pairs, conduct RLAIF, and repeat), how\nRLAIF can be adapted to a model-based RL setting\nwhere both human and assistant are modeled by\nLLMs, and how AI feedback can be leveraged for\nmore specific credit assignment. We leave these\nquestions for future work.\nEthics\nOne ethical consideration concerns the utilization\nof AI-generated feedback as a source for model\nalignment. There exists a potential risk of transfer-\nring biases from the off-the-shelf LLM into the\ngenerated preferences. This in turn may result\nin RL-trained policies further amplifying biases,\nthereby inadvertently misaligning models and po-\ntentially causing harm. Extreme caution must be\nexercised, especially when deploying these mod-\nels in high-stakes domains such as medicine, law,\nand employment, where they have the potential to\nsignificantly impact human lives in adverse ways.\nIn such domains, we believe that human experts\ntrained to carefully assign preferences according to\nstrict policies should be considered the gold stan-\ndard.\nAnother ethical consideration is that reducing the\nbarriers to aligning LLMs also carries the risk of\nfacilitating their misuse for malicious purposes. For\ninstance, RLAIF could be employed to train models\nto generate convincing misinformation or produce\nhateful and abusive content. The best mitigation\nto this risk is to carefully govern the access and\nusage of powerful LLMs (e.g. limiting \u201cwhite-box\u201d\naccess), to prevent bad actors from misusing them.\nReproducibility\nTo promote the reproducibility of this work, many\nof the details of this research are shared through-\nout the paper.\nOpen-source datasets are elabo-\nrated upon in Appendix C, LLM labeling details\nin Appendix D, the RL formulation in Appendix F,\nmodel training details in Appendix G, human eval-\nuation details in I, and the most critical prompts\nused in the Appendix (e.g. Tables 17, 21, and 22).\nPlease reach out to authors for any additional ques-\ntions or requests.\nPaLM 2 models are available through Google\nCloud\u2019s Vertex API, and the experiments in this\nwork may also be repeated with other publicly avail-\nable LLMs.\nAcknowledgements\nWe would like to thank many people who have\nhelped make this work complete.\nWe thank\nChen Zhu for optimizing our LLM inference\nsetup, Le Hou for suggesting prompt improvements\nand experimenting with self-consistency, L\u00e9onard\nHussenot for bringing the problem of position bias\nin LLMs to our attention, and Bradley Green, Ewa\nDominowska, and Blaise Aguera y Arcas for sup-\nporting this research.\nWe thank everyone who thoroughly reviewed\nour work and provided valuable feedback: Hakim\nSidahmed, Meiqi Guo, Michal Valko, Nevan Wich-\ners, Sian Gooding, and Yuan Cao.\nWe thank Mo Azar, Daniel Guo, Andrea Michi,\nNicolas Perez-Nieves, and Marco Selvi for their\nwork in developing a RLAIF training setup that\ndirectly prompts an LLM to obtain reward scores.\nFinally, we thank the individuals who designed\nand built the RL training infrastructure used in this\npaper: L\u00e9onard Hussenot, Johan Ferret, Robert\nDadashi, Geoffrey Cideron, Alexis Jacq, Sabela\nRamos, Piotr Stanczyk, Sertan Girgin, Danila\nSinopalnikov, Am\u00e9lie H\u00e9liou, Nikola Momchev,\nand Olivier Bachem.\nReferences\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul\nChristiano, John Schulman, and Dan Man\u00e9. 2016.\nConcrete problems in ai safety.\narXiv preprint\narXiv:1606.06565.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n2022a. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862.\nYuntao Bai,\nSaurav Kadavath,\nSandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron\nMcKinnon, Carol Chen, Catherine Olsson, Christo-\npher Olah, Danny Hernandez, Dawn Drain, Deep\nGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,\nJamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua\nLandau, Kamal Ndousse, Kamile Lukosuite, Liane\nLovitt, Michael Sellitto, Nelson Elhage, Nicholas\nSchiefer, Noemi Mercado, Nova DasSarma, Robert\nLasenby, Robin Larson, Sam Ringer, Scott John-\nston, Shauna Kravec, Sheer El Showk, Stanislav Fort,\nTamera Lanham, Timothy Telleen-Lawton, Tom Con-\nerly, Tom Henighan, Tristan Hume, Samuel R. Bow-\nman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,\nNicholas Joseph, Sam McCandlish, Tom Brown, and\nJared Kaplan. 2022b. Constitutional ai: Harmless-\nness from ai feedback.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. Ad-\nvances in neural information processing systems, 30.\nBosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken\nChia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023.\nIs GPT-3 a good data annotator?\nIn Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 11173\u201311195, Toronto, Canada. Association\nfor Computational Linguistics.\nKawin\nEthayarajh,\nYejin\nChoi,\nand\nSwabha\nSwayamdipta.\n2022.\nUnderstanding\ndataset\ndifficulty with V-usable information. In Proceedings\nof the 39th International Conference on Machine\nLearning, volume 162 of Proceedings of Machine\nLearning Research, pages 5988\u20136008. PMLR.\nTom Everitt and Marcus Hutter. 2016. Avoiding wire-\nheading with value reinforcement learning. In Arti-\nficial General Intelligence: 9th International Con-\nference, AGI 2016, New York, NY, USA, July 16-19,\n2016, Proceedings 9, pages 12\u201322. Springer.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889\u2013898, Melbourne, Australia. Association\nfor Computational Linguistics.\nSteven Y. Feng, Varun Gangal, Jason Wei, Sarath Chan-\ndar, Soroush Vosoughi, Teruko Mitamura, and Ed-\nuard Hovy. 2021. A survey of data augmentation\napproaches for NLP. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 968\u2013988, Online. Association for Computa-\ntional Linguistics.\nRoy Fox, Ari Pakman, and Naftali Tishby. 2015. Tam-\ning the noise in reinforcement learning via soft up-\ndates. arXiv preprint arXiv:1512.08562.\nYang Gao, Christian M Meyer, Mohsen Mesgar, and\nIryna Gurevych. 2019. Reward learning for efficient\nreinforcement learning in extractive document sum-\nmarisation. arXiv preprint arXiv:1907.12894.\nMatthieu Geist, Bruno Scherrer, and Olivier Pietquin.\n2019. A theory of regularized markov decision pro-\ncesses.\nIn International Conference on Machine\nLearning, pages 2160\u20132169. PMLR.\nFabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli.\n2023. Chatgpt outperforms crowd-workers for text-\nannotation tasks. arXiv preprint arXiv:2303.15056.\nAmelia Glaese, Nat McAleese, Maja Trebacz, John\nAslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,\nLaura Weidinger, Martin Chadwick, Phoebe Thacker,\net al. 2022. Improving alignment of dialogue agents\nvia targeted human judgements.\narXiv preprint\narXiv:2209.14375.\nGoogle. 2023.\nAi platform data labeling service\npricing.\nhttps://cloud.google.com/\nai-platform/data-labeling/pricing#\nlabeling_costs. Accessed: 2023-09-28.\nRohan Anil Google, Andrew M. Dai, Orhan Firat,\nMelvin Johnson, Dmitry Lepikhin, Alexandre Pas-\nsos, Siamak Shakeri, Emanuel Taropa, Paige Bai-\nley, Zhifeng Chen, Eric Chu, Jonathan H. Clark,\nLaurent El Shafey, Yanping Huang, Kathy Meier-\nHellstern, Gaurav Mishra, Erica Moreira, Mark\nOmernick, Kevin Robinson, Sebastian Ruder, Yi Tay,\nKefan Xiao, Yuanzhong Xu, Yujing Zhang, Gus-\ntavo Hernandez Abrego, Junwhan Ahn, Jacob\nAustin, Paul Barham, Jan Botha, James Brad-\nbury, Siddhartha Brahma, Kevin Brooks, Michele\nCatasta, Yong Cheng, Colin Cherry, Christopher A.\nChoquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment\nCrepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,\nJacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad\nFeinberg, Fangxiaoyu Feng, Vlad Fienber, Markus\nFreitag, Xavier Garcia, Sebastian Gehrmann, Lu-\ncas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi\nHashemi, Le Hou, Joshua Howland, Andrea Hu, Jef-\nfrey Hui, Jeremy Hurwitz, Michael Isard, Abe Itty-\ncheriah, Matthew Jagielski, Wenhao Jia, Kathleen\nKenealy, Maxim Krikun, Sneha Kudugunta, Chang\nLan, Katherine Lee, Benjamin Lee, Eric Li, Music\nLi, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim,\nHanzhao Lin, Zhongtao Liu, Frederick Liu, Mar-\ncello Maggioni, Aroma Mahendru, Joshua Maynez,\nVedant Misra, Maysam Moussalem, Zachary Nado,\nJohn Nham, Eric Ni, Andrew Nystrom, Alicia Par-\nrish, Marie Pellat, Martin Polacek, Alex Polozov,\nReiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter,\nParker Riley, Alex Castro Ros, Aurko Roy, Brennan\nSaeta, Rajkumar Samuel, Renee Shelby, Ambrose\nSlone, Daniel Smilkov, David R. So, Daniel Sohn,\nSimon Tokumine, Dasha Valter, Vijay Vasudevan, Ki-\nran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui\nWang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin\nXu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui\nYu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang\nZhou, Denny Zhou, Slav Petrov, and Yonghui Wu.\n2023. Palm 2 technical report.\nRonald A Howard. 1960. Dynamic programming and\nmarkov processes. John Wiley.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve.\narXiv\npreprint arXiv:2210.11610.\nNatasha Jaques, Shixiang Gu, Dzmitry Bahdanau,\nJos\u00e9 Miguel Hern\u00e1ndez-Lobato, Richard E Turner,\nand Douglas Eck. 2017. Sequence tutor: Conserva-\ntive fine-tuning of sequence generation models with\nkl-control. In International Conference on Machine\nLearning, pages 1645\u20131654. PMLR.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\narXiv\npreprint arXiv:2001.08361.\nM. G. Kendall and B. Babington Smith. 1939. The\nProblem of m Rankings. The Annals of Mathemati-\ncal Statistics, 10(3):275 \u2013 287.\nMinae Kwon, Sang Michael Xie, Kalesha Bullard, and\nDorsa Sadigh. 2022. Reward design with language\nmodels. In The Eleventh International Conference\non Learning Representations.\nViet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo,\nThuat Nguyen, Franck Dernoncourt, Ryan A Rossi,\nand Thien Huu Nguyen. 2023. Okapi: Instruction-\ntuned large language models in multiple languages\nwith reinforcement learning from human feedback.\narXiv preprint arXiv:2307.16039.\nYiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang,\nYuanyuan Yang, Jiaming Tian, Hao He, Antong Li,\nMengshen He, Zhengliang Liu, et al. 2023. Summary\nof chatgpt/gpt-4 research and perspective towards\nthe future of large language models. arXiv preprint\narXiv:2304.01852.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with\nself-feedback. arXiv preprint arXiv:2303.17651.\nJames\nManyika.\n2023.\nAn\noverview\nof\nbard:\nan\nearly\nexperiment\nwith\ngenera-\ntive\nai.\nhttps://ai.google/static/\ndocuments/google-about-bard.pdf.\nAccessed: 2023-08-23.\nYu Meng, Martin Michalski, Jiaxin Huang, Yu Zhang,\nTarek Abdelzaher, and Jiawei Han. 2023.\nTun-\ning language models as training data generators for\naugmentation-enhanced few-shot learning. In Inter-\nnational Conference on Machine Learning, pages\n24457\u201324477. PMLR.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048\u201311064.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021.\nWebgpt: Browser-assisted question-\nanswering with human feedback.\narXiv preprint\narXiv:2112.09332.\nOpenAI. 2023a. Gpt-4 technical report.\nOpenAI. 2023b. Openai pricing. https://openai.\ncom/pricing. Accessed: 2023-09-28.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nPouya Pezeshkpour and Estevam Hruschka. 2023.\nLarge language models sensitivity to the order of\noptions in multiple-choice questions. arXiv preprint\narXiv:2308.11483.\nPaul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Ge-\noffrey Cideron, Robert Dadashi, Matthieu Geist, Ser-\ntan Girgin, L\u00e9onard Hussenot, Orgad Keller, et al.\n2023. Factually consistent summarization via rein-\nforcement learning with textual entailment feedback.\narXiv preprint arXiv:2306.00186.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017.\nProxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nCoRR, abs/1804.04235.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. Advances\nin Neural Information Processing Systems, 33:3008\u2013\n3021.\nRichard S Sutton, David McAllester, Satinder Singh,\nand Yishay Mansour. 1999. Policy gradient methods\nfor reinforcement learning with function approxima-\ntion.\nAdvances in neural information processing\nsystems, 12.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen,\nYou Wu, Luke Zettlemoyer, and Huan Sun. 2022a.\nTowards understanding chain-of-thought prompting:\nAn empirical study of what matters. arXiv preprint\narXiv:2212.10001.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n2023. Large language models are not fair evaluators.\narXiv preprint arXiv:2305.17926.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021a. Want to reduce label-\ning cost? gpt-3 can help. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2021,\npages 4195\u20134205.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2022b. Self-consistency improves\nchain of thought reasoning in language models. In\nThe Eleventh International Conference on Learning\nRepresentations.\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.\n2021b. Towards zero-label language learning. arXiv\npreprint arXiv:2109.09193.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. 2021. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforcement\nlearning. Machine learning, 8:229\u2013256.\nLijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-\nYan Liu. 2018. A study of reinforcement learning\nfor neural machine translation. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3612\u20133621.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n2016. Google\u2019s neural machine translation system:\nBridging the gap between human and machine trans-\nlation. arXiv preprint arXiv:1609.08144.\nYuxiang Wu and Baotian Hu. 2018. Learning to extract\ncoherent summary via deep reinforcement learning.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, page 5602.\nKevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng,\nand Yuandong Tian. 2023. Rlcd: Reinforcement\nlearning from contrast distillation for language model\nalignment.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\nguage models from human preferences.\narXiv\npreprint arXiv:1909.08593.\nA\nRLHF Preliminaries\nWe review the RLHF pipeline introduced in Sti-\nennon et al. (2020); Ouyang et al. (2022), which\nconsists of 3 phases: supervised fine-tuning, reward\nmodel training, and reinforcement learning.\nA.1\nSupervised Fine-tuning\nA pre-trained LLM is fine-tuned on a high quality\nlabeled dataset for a downstream task (e.g. summa-\nrization) using token-level supervision to produce\na supervised fine-tuned (SFT) model \u03c0SFT .\nA.2\nReward Modeling\nGiven an input x, we sample a pair of responses\n(y1, y2) \u223c \u03c0 from one or more models, where\noftentimes \u03c0 is the SFT model. The input and\nresponses are sent to human annotators to rate\nwhich response is better according to some cri-\nteria. These annotations form a dataset of triplets\nD = {(x, yw, yl)}, where yw and yl are the pre-\nferred and non-preferred responses, respectively. A\nreward model (RM) r\u03d5 is trained by minimizing\nthe following loss:\nLr(\u03d5) =\n\u2212E\n(x,yw,yl)\u223cD\nh\nlog \u03c3\n\u0000r\u03d5(x, yw) \u2212 r\u03d5(x, yl)\n\u0001i\n,\nwhere \u03c3 is the sigmoid function.\nA.3\nReinforcement Learning\nA policy \u03c0RL\n\u03b8\nis initialized from the SFT model\nweights and then optimized with reinforcement\nlearning to maximize the reward given by the RM,\nwhich serves as a proxy for human preferences. Op-\ntionally, a Kullback-Leibler (KL) divergence term\nDKL is added to the objective to penalize \u03c0RL\n\u03b8\nfor\ndeviating from the original SFT policy \u03c0SFT , con-\ntrolled by the hyperparameter \u03b2 (Fox et al., 2015;\nGeist et al., 2019). The KL loss helps prevent\n\u03c0RL\n\u03b8\nfrom drifting into a region where it generates\nlanguage that is highly rewarded by the RM yet\nconsists of low-quality or unnatural language - a\nphenomenon known as \u201creward hacking\u201d (Everitt\nand Hutter, 2016; Amodei et al., 2016). The op-\ntimization objective is described by the equation\nbelow:\nJ(\u03b8) =\nE\ny\u223c\u03c0\u03b8(\u00b7|x)\nh\n(1 \u2212 \u03b2)r\u03d5(y|x)\n\u2212 \u03b2DKL\n\u0000\u03c0RL\n\u03b8\n(y|x) || \u03c0SF T (y|x)\n\u0001i\n,\nwhere \u03b2 is a hyperparameter between 0 and 1.\nB\nPosition Bias in LLM Labelers\nModel Size\n% Same Position Preferred\nPaLM 2 L\n18%\nPaLM 2 S\n21%\nPaLM 2 XS\n56%\nTable 4: Position bias is more prevalent in smaller model\nsizes, measured by the percentage of examples where\nthe LLM prefers the same position even after swapping\nthe order of candidates (\u201c% Same Position Preferred\u201d).\nAnalysis is conducted using the \u201cDetailed + CoT 0-shot\u201d\nprompt for the summarization task.\nOur analysis on the summarization task suggests\nthat the LLMs used for preference labeling are\nbiased by the order in which candidates are shown.\nFor each example in our AI labeling evaluation\nset, we query the LLM preferences for the pair of\ncandidates, swap the order in which candidates are\npresented, and then query the LLM preferences\nagain.\nWe consider an LLM to be more biased if it\nprefers the same position on both the original and\nreversed inferences. For example, let candidates A\nand B be in positions 1 and 2 for the first inference\nand in positions 2 and 1 for the second inference.\nIf the LLM prefers the same position on both infer-\nences, we consider the LLM to be position-biased.\nWe measure position bias by computing \u201c% Same\nPosition Preferred\u201d - the percentage of inference\npairs where this occurs. A higher metric value\nindicates a more biased LLM.\nWe find that PaLM 2 L, S, and XS prefer the\nsame position 18%, 21%, and 56% of the time, re-\nspectively, suggesting that position bias is inversely\ncorrelated with model size (see Table 4). One hy-\npothesis is that larger models are more capable and\ntherefore more faithfully judge preferences based\non the content of the candidates rather than their\npositions, which are supposed to be immaterial.\nWe also observe that for PaLM 2 L, of the 18%\nof cases where it prefers the same position on both\ninferences, 94% of the time it prefers the first candi-\ndate shown. On the other hand, PaLM 2 S and XS\nshow affinity for the second candidate shown when\nthe same position is preferred on both inferences,\npreferring it 91% and 99% of the time, respectively.\nThese biases are statistically significant under a\ntwo-sided binomial test at \u03b1 = 0.05.\nC\nDataset Details\nFor summarization, we use the filtered Reddit\nTL;DR dataset (Stiennon et al., 2020), containing\nposts from Reddit12 that have been filtered to en-\nsure high quality. The dataset contains 123k posts,\nwhere \u223c5% is held out as a validation set.\nAdditionally, we use OpenAI\u2019s human prefer-\nence dataset created from the filtered Reddit TL;DR\ndataset. For a given post, two candidate summaries\nwere generated - often from different policies, and\nhuman labelers were asked to rate which summary\nthey preferred. The total dataset comprises 92k\npairwise comparisons.\nFor helpful and harmless dialogue generation,\nwe use Anthropic\u2019s Helpful and Harmless prefer-\nence datasets13 (Bai et al., 2022a). Each example\nconsists of a conversation history between a human\nand an AI assistant accompanied by a preferred and\nnon-preferred response from the AI assistant. Pref-\nerence is based on which response is more helpful\nand honest for the helpful task, and which response\nis safer and less harmful for the harmless task. Each\ndataset comprises over 40k training examples and\n2k test examples. We further split the test sets into\nvalidation and test sets by randomly assigning two-\nthirds of examples to validation and one-third to\ntest.\nD\nLLM Labeling Details\nFor LLM labeling, we set a maximum input con-\ntext length of 4096 tokens. For chain-of-thought\ngeneration, we set a maximum decoding length of\n512 tokens and sample with temperature T = 0.0\n(i.e. greedy decoding). For self-consistency ex-\nperiments in Appendix M, we use temperatures\nvarying from T = 0.3 to T = 1.0 with top-K\nsampling (Fan et al., 2018), where K = 40.\nIn Section 4.3, we use the AI labeler to directly\ncompute a score that we leverage as the reward for\nRL. We use the following prompt: \u201cYou are an ex-\npert summary rater. Given a TEXT (completed with\na SUBREDDIT and a TITLE) and a SUMMARY,\nyour role is to provide a SCORE from 1 to 10 that\nrates the quality of the SUMMARY given the TEXT,\nwith 1 being awful and 10 being a perfect SUM-\nMARY.\u201d, followed by the input Reddit post, then\n12www.reddit.com\n13We use the helpful-base and harmless-base\ndatasets\nfrom\nhttps://huggingface.co/\ndatasets/Anthropic/hh-rlhf.\nthe summary to score preceded by \u201cSUMMARY: \u201d,\nand a final \u201cSCORE: \u201d.\nPaLM 2 models are publicly available through\nGoogle Cloud\u2019s Vertex AI14, though we cannot\nguarantee full reproducibility as the models acces-\nsible through Google Cloud are subject to change.\nE\nPost-RL Response Formatting\nFor summarization, we observed that summaries\ngenerated by RLHF and RLAIF policies often in-\ncluded superfluous symbols like periods or spaces\nat the end of the response - possibly due to \u201creward\nhacking\u201d. Given that these extra tokens do not have\nany meaningful content, we programmatically re-\nmoved certain symbols at the end of summaries.\nThis ensured that human evaluators could focus on\nthe content and not be distracted by the formatting\nof the response.\nF\nREINFORCE for Language Models\nConsider a deterministic, finite-horizon MDP M =\n(X, A, R, P, \u03b3) (Howard, 1960). At each step t,\ngiven the current state Xt \u2208 X and the next\naction At \u2208 A, the model receives a reward\nRt = R(Xt, At) and transitions to the next state\nXt+1 = P(Xt, At).\nIn the context of language models, Xt is the con-\ncatenation of the input text and all text generated\nby the policy until time t. Action At is the token\nfrom the considered vocabulary decoded at time\nt by the stochastic policy \u03c0\u03b8(\u00b7|Xt), where \u03b8 rep-\nresents the policy parameters. Finally, the reward\nRt is given by the RM. The RM is only evaluated\nwhen the language model response has been fully\ngenerated; all rewards prior to the final token are\nset to 0, while the reward corresponding to the final\ntoken is set to RT .\nThe cumulative sum of rewards received when\nfollowing the policy \u03c0\u03b8 from time-step t is called\nthe return.\nGenerally, it is defined as Zt\n=\nPT\ns=t \u03b3s\u2212tRs. However, since only the terminal\nreward is non-zero and we set \u03b3 = 1, the return\ncan be simplified to Zt = RT .\nGiven a trajectory (Xt, At, Rt)T\nt=0 generated un-\nder \u03c0\u03b8, the policy gradient loss from REINFORCE\nis then defined as follows:\nLPG(\u03b8) = \u2212\nX\nt\nlog \u03c0\u03b8(At|Xt)\n\u0010\nZt \u2212 V \u03c0\n\u03c8 (Xt)\n\u0011\n,\n14https://cloud.google.com/vertex-ai/\ndocs/generative-ai/learn/models\nwhere the bar notation denotes that no gradient is\npassed through the advantage term during back-\npropagation.\nThe baseline value function V \u03c0\n\u03c8 (x) estimates the\nreturn-to-go Zt when following the policy \u03c0\u03b8 and is\nparameterized by \u03c8 (Williams, 1992; Sutton et al.,\n1999). It is trained with the following loss:\nLV (\u03c8) =\nX\nt\n(Zt \u2212 V \u03c0\n\u03c8 (Xt))2.\nIn practice, we optimize the regularized objec-\ntive in Sec. A.3. We incorporate the KL divergence\nin the policy gradient loss described above, as com-\nmonly seen in other work (Jaques et al., 2017).\nG\nModel Training Details\nSFT models for the summarization task are trained\non the Reddit TL;DR dataset, with a batch size\nof 128 for a single epoch. We use the Adafac-\ntor (Shazeer and Stern, 2018) optimizer with a\nlearning rate of 10\u22125, and the maximum input and\noutput lengths are 1024 and 128 tokens, respec-\ntively. For helpful and harmless dialogue genera-\ntion tasks, an instruction-tuned version of PaLM 2\nXS serves as the SFT model.\nRMs for all tasks are trained until the training\nloss and accuracy curves plateau, which happens\nin 2-3 epochs. We use the Adafactor optimizer\nwith a learning rate of 10\u22125. Batch size is 128 for\nsummarization RMs and 32 for RMs of other tasks.\nWe train all our RMs with maximum input length\nof 1152 tokens to account for 1024 context tokens\nand 128 response tokens. We report the accuracies\nof the RMs in Appendix H.\nFor summarization, the AI feedback RM is ini-\ntialized from the SFT model (i.e. PaLM 2 XS fine-\ntuned on Reddit TL;DR), and the human feedback\nRM is initialized from PaLM 2 XS. We experi-\nmented with initializing the human feedback RM\nfrom the SFT model but found that it resulted in\nlower accuracy on the held out set of human pref-\nerences (see Table 6). For helpful and harmless\ndialogue generation tasks, we initialize both the\nhuman and AI feedback RMs from the instruction-\ntuned version of PaLM 2 XS.\nFor reinforcement learning, we use the SFT\nmodel for each task as the initial policy. We sample\nfrom our language model policies for all tasks with\na temperature of T = 0.9 to encourage exploration.\nWe train with a batch size of 128 and learning rate\nof 10\u22125 for 8 epochs. We set \u03b2 = 0.05 for the KL\ndivergence loss.\nTo select the final checkpoint for each RL pol-\nicy, we first selected 4 candidate checkpoints from\nRL training that scored high rewards on validation\nprompts. We then prompted an off-the-shelf LLM\nto judge the win rate of the RL checkpoint\u2019s sum-\nmaries vs. the SFT policy\u2019s summaries. We also\nconducted manual inspection of a dozen examples.\nWe picked the checkpoint with the best combina-\ntion of win rate and quality as judged by manual\ninspection as our final RL policy.\nH\nReward Model Accuracy\nTask\nHuman\nFeedback\nAI\nFeedback\nSummarization\n79.3%\n74.2%\nHelpful Dialogue\n76.0%\n67.8%\nHarmless Dialogue\n72.1%\n69.7%\nTable 5: Pairwise accuracies of human feedback and AI\nfeedback reward models across all tasks. Metrics are\ncalculated on a held out set of human preference data\nfor each task.\nInitialization\nHuman\nFeedback\nAI\nFeedback\nPaLM 2 XS\n79.3%\n73.0%\nSFT\n78.7%\n74.2%\nTable 6: Results of initializing the summarization RMs\non PaLM 2 XS vs. the SFT model.\nRM Variant\nAI\nFeedback\nTrained on \u201cBase 0-shot\u201d labels\n77.9%\nTrained on labels from PaLM 2 XS\n66.4%\nTable 7: Accuracy values for variants of RMs trained\non AI labels for the task of summarization.\nPairwise Accuracy for RMs measures how ac-\ncurate a trained reward model is with respect to a\nheld-out set of human preferences. Given an input\ncontext and pair of candidate responses, the value\nis 1 if the RM scores the preferred candidate higher\nthan the non-preferred candidate, according to the\nhuman label. Otherwise the value is 0. This quan-\ntity is averaged over multiple examples to obtain\nthe pairwise accuracy of the RM.\nWe report RM accuracy on a held out set of\nhuman preferences for all tasks in Table 5. For\nsummarization, we also report RM accuracy when\ninitializing on different checkpoints in Table 6. In\nTable 7, we report accuracy for RM variants used in\nthe end-to-end sensitivity experiment in Appendix\nN and the same-size RLAIF experiment in Section\n4.2.\nWe observe that RMs trained on human feed-\nback outperform those trained on AI feedback, both\nof which are measured against a held out set of\nhuman preferences. This pattern seems natural,\ngiven that the human preferences are trained on\ndata drawn from the same distribution as the val-\nidation dataset. However, it is interesting to note\nthat despite the gap in accuracy between AI and\nhuman preference RMs, RLAIF achieves compa-\nrable results to RLHF on two tasks and surpasses\nRLHF on one task. Additionally, we note that the\nsummarization RMs trained on \u201cBase 0-shot\u201d and\n\u201cDetailed + CoT 0-shot\u201d (i.e. the default prompt-\ning technique) achieve accuracies of 77.9% and\n74.2%, respectively, which is the inverse order of\ntheir final performance after RL (see Appendix N).\nThese gaps in RM accuracy suggest that RM ac-\ncuracy, while correlated with RM usefulness, may\nnot be a perfect reflection of RM effectiveness in\nRLHF and RLAIF. Ultimately, we believe that the\nusefulness of RMs is assessed through conducting\nRL and evaluating the final policies through human\nevaluation.\nI\nHuman Evaluation Details\nTo conduct human evaluation, in total we created\n\u223c2k unique rating instances. Each instance com-\nprised a single context and three distinct model\nresponses (e.g. responses from SFT, RLAIF, and\nRLHF policies), resulting in a total of \u223c6k unique\n(context, response) pairs subjected to human evalu-\nation. Additionally, each instance was assessed by\nthree independent raters, resulting in \u223c18k (con-\ntext, response, rating) tuples.\nWe measure the inter-annotator agreement with\nKendall\u2019s Coefficient of Concordance W (Kendall\nand Smith, 1939) - a non-parametric statistic for as-\nsessing the agreement among multiple raters rank-\ning multiple items. The values of Kendall\u2019s W\nrange from 0 to 1, where 0 indicates perfect dis-\nagreement and 1 indicates perfect agreement. We\nconducted multiple human evaluation sessions, and\nthe W statistic ranged from 0.6-0.7, indicating a\nreasonable level of agreement.\nJ\nControlling for Response Length\nResponse length often can influence human evalua-\ntors\u2019 perception of quality (Stiennon et al., 2020),\nand our various policies generate responses that\ndiffer in length. For example, in the summarization\ntask, the summaries produced by RLAIF, RLHF,\nand SFT policies sent to human evaluation have\nan average character-length of 164, 161, and 132,\nrespectively. For all experiments presented in this\npaper, we conduct post-hoc analysis to estimate the\nwin rates after controlling for length.\nWe take an approach similar to Stiennon et al.\n(2020) and calculate the \u201clength-adjusted win rate\nof policy A vs. policy B\u201d. Given policy A, we train\na logistic regression model where the input is the\nratio of the policy A\u2019s response length to policy B\u2019s\nsummary length (in characters), and the target is a\nbinary label indicating whether policy A\u2019s response\nwas preferred over policy B\u2019s response. After fit-\nting the model, we estimate a length-controlled win\nrate by asking the logistic regressor to predict the\nwin rate given a length ratio of 1.0, which repre-\nsents the scenario where both the responses are of\nequal length.\nAfter controlling for length for the summariza-\ntion task, our length-adjusted win rates for RLAIF\nand RLHF vs. SFT are 59% and 61%, respectively\n(see Table 8). Both RL policies continue to outper-\nform the SFT policy by a similar margin, support-\ning our initial statement that RLAIF is comparable\nto RLHF.\nWe reach similar conclusions for the helpful dia-\nlogue generation task (Table 9), same-size RLAIF\nand direct RLAIF experiments (Table 11), the end-\nto-end sensitivity to AI labeler alignment exper-\niment (Table 12), and combining human and AI\nfeedback (Table 13).\nFor the harmless dialogue generation task, the\nsetup is slightly different. Since human evaluators\nrated each response independently as harmful or\nharmless, we compute the harmless rate instead of\nthe win rate. We use the average generation length\nfrom the SFT policy as the reference point for all\nother policies (Table 10).\nWe note that this post-hoc method of controlling\nfor length is imperfect, as it assumes the logistic\nregression model accurately learns the relationship\nbetween summary length and human preference. A\nmore principled approach would be to encourage\nall policies generate summaries of similar length\nthrough an auxiliary training loss.\nModels\nLength\nuncorrected\nLength\ncorrected\nRLAIF vs SFT\n71%\n59%\nRLHF vs SFT\n73%\n61%\nRLAIF vs RLHF\n50%\n47%\nTable 8: Length-controlled win rate for the summariza-\ntion task.\nModels\nLength\nuncorrected\nLength\ncorrected\nRLAIF vs SFT\n63%\n61%\nRLHF vs SFT\n64%\n61%\nRLAIF vs RLHF\n52%\n50%\nTable 9: Length-controlled win rate for the helpful dia-\nlogue generation task.\nK\nCombining Human and AI Feedback\nWe investigate the effectiveness of combining hu-\nman feedback and AI feedback on the task of sum-\nmarization. We refer to this approach as RLHF +\nRLAIF and compare it against RLHF.\nFirst, given contexts randomly drawn from the\nReddit TL;DR dataset, responses are generated by\nRLHF and SFT policies with temperature T = 1.0.\nThe instruction-tuned PaLM 2 L is then called to\ngenerate AI preferences. Finally, a new RM is\ntrained on both the entire OpenAI human prefer-\nence dataset and an equivalent size AI preference\ndataset.\nWe observe that RLHF + RLAIF does not im-\nprove beyond RLHF alone. RLHF + RLAIF and\nRLHF achieve win rates of 71% and 74% over SFT,\nrespectively. The difference in win-rate is not statis-\ntically significant. When compared head-to-head,\nraters prefer both policies equally.\nWhile this experiment did not show positive re-\nsults from combining RLAIF and RLHF, there are\nmany alternative setups which could prove success-\nful. One such setup could involve first conduct-\ning RLAIF, then collecting generations and human\npreferences using the RLAIF policy as the initial-\nization point for RLHF. In this curriculum learning\napproach, RLAIF can be viewed as a \u201cwarm-up\u201d\npolicy, which is then refined with RLHF. Another\npossible setup could involve collecting much more\nAI feedback than human feedback, since it is much\nless expensive to collect (see Appendix L). We\nleave this exploration to future work.\nModels\nLength\nuncorrected\nLength\ncorrected\nSFT\n64%\n64%\nRLHF\n76%\n78%\nRLAIF\n88%\n91%\nTable 10: Length-controlled harmless rate for the harm-\nless dialogue generation task. We used the average gen-\neration length from the SFT model as reference length to\ncompute the length-controlled harmless rate for RLHF\nand RLAIF.\nModels\nLength\nuncorrected\nLength\ncorrected\nSame-size RLAIF\nvs SFT\n68%\n59%\nDirect RLAIF\nvs SFT\n74%\n65%\nDirect RLAIF vs\nSame-size RLAIF\n60%\n56%\nTable 11: Length-controlled win rate for same-size\nRLAIF and direct RLAIF.\nL\nCost of LLM vs. Human Labeling\nUsing LLMs as data annotators can be much less\ncostly than hiring human annotators (Wang et al.,\n2021a). We estimate AI preference labeling to be\nover 10x less costly than human preference labeling\nfollowing the calculations below.\nAt the time of writing, GPT-4 charges $0.03\nUSD and $0.06 USD for every 1,000 tokens to\nencode and decode, respectively (OpenAI, 2023b).\nFor labeling TL;DR preferences with an LLM, our\naverage token lengths were as follows:\n1. Input prompt length - 830 tokens (using the\n\u201cDetailed + CoT 0-shot\u201d prompt)\n2. Generated chain-of-thought rationale - 61 to-\nkens\nAdditionally, to debias position, we repeat each\nlabeling procedure after inverting the order in\nwhich a pair of responses are shown. Our estimated\nAI labeling cost per example is $0.06 USD15.\nIn comparison, Google Cloud\u2019s human annota-\ntion service charges approximately $0.11 USD / 50\nwords for classification tasks at the time of writ-\n152 inferences * (830 encoder tokens * $0.03 / 1,000 tokens\n+ 61 decoder tokens * $0.06 / 1,000 tokens) = $0.057 \u223c =\n$0.06\nModels\nLength\nuncorrected\nLength\ncorrected\nBase RLAIF\nvs SFT\n63%\n59%\nDetailed RLAIF\nvs SFT\n67%\n63%\nBase RLAIF vs\nDetailed RLAIF\n41%\n45%\nTable 12: Length-controlled win rate for the experiment\non end-to-end sensitivity to AI labeler alignment. Base\nRLAIF and Detailed RLAIF correspond to \u201cBase 0-shot\u201d\nRLAIF and \u201cDetailed CoT 0-shot\u201d RLAIF described in\nAppendix N, respectively.\nModels\nLength\nuncorrected\nLength\ncorrected\nRLHF + RLAIF\nvs SFT\n71%\n61%\nRLHF\nvs SFT\n74%\n67%\nRLHF + RLAIF\nvs RLHF\n48%\n46%\nTable 13: Length-controlled win rate for experiments\ncombining human and AI feedback.\ning16 (Google, 2023). We assume that each classifi-\ncation task only consists of reading a document and\ntwo candidate summaries, which have a combined\naverage word length of 304 words. We estimate the\nhuman labeling cost per example to be $0.67 USD\n(304 words * $0.11 / 50 words).\nWe recognize that this cost analysis does not ac-\ncount for all factors, such as the cost of training\nhuman annotators, tasking multiple human anno-\ntators to rate the same instance for robustness, the\ncost of expert vs. crowd-sourced annotators, or the\ncost of setting up LLM labeling.\nM\nSelf-Consistency\nFor chain-of-thought prompts, we also experiment\nwith self-consistency (Wang et al., 2022b) - a\ntechnique to generate robust chain-of-thought ra-\ntionales. In self-consistency, multiple chain-of-\nthought rationales are sampled with temperature\nT > 0, and LLM preference distributions are ob-\ntained for each one. The results are then averaged\n16Google Cloud charges between $90 and $129 per 1,000\nunits, where each unit is 50 words for a classification task. We\naverage the lower and upper bound costs and convert from\nunits to words - ($90 / 1,000 units + $129 / 1,000 units) / 2 * 1\nunit / 50 words = $0.1095 USD / 50 words\nSelf-Consistency\nAI Labeler Alignment\n1 sample, T=0.0\n78.0%\n16 samples, T=0.3\n76.2%\n16 samples, T=0.5\n75.1%\n16 samples, T=0.7\n74.0%\n16 samples, T=1.0\n72.8%\nTable 14: Sampling multiple chain-of-thought rationales\nwith T > 0 results in lower alignment with human\npreferences. Note: 1 and 16 samples represent 2 and 32\ninferences given our position debiasing technique (see\nSection 2.1.1).\nto obtain the final preference distribution.\nOn the task of summarization, we experiment\nwith self-consistency using 4 and 16 samples un-\nder decoding temperatures ranging from 0.3 to 1.0\n(see Figure 14)17. In all settings, self-consistency\ndecreases AI labeler alignment versus the baseline\nwithout self-consistency. Our experiments show\nthat alignment decreases as temperature increases,\nwith the largest drop of over -5% at T = 1.0. In\nour experiments, using 4 vs. 16 self-consistency\nsamples does not impact AI labeler alignment.\nManually inspecting chain-of-thought rationales\ndid not reveal any common patterns for why self-\nconsistency might degrade alignment (examples in\nTable 20). One hypothesis is that using a temper-\nature of T > 0 leads the model to generate lower\nquality rationales compared to greedy decoding,\nultimately leading to worse accuracy overall.\nN\nEnd-to-end Sensitivity to AI Labeler\nAlignment\nWe assess the end-to-end sensitivity of the RLAIF\npolicies to AI labeler alignment on the task of sum-\nmarization. Since human judgement is subjective\nand prone to noise, we test whether better AI la-\nbeler alignment leads to improved downstream per-\nformance. We train two RLAIF policies that only\ndiffer in the prompting technique used for AI la-\nbeling - \u201cBase 0-shot\u201d and \u201cDetailed CoT 0-shot\u201d,\nyielding 76.1% and 78.0% AI labeler alignment,\nrespectively.\nWhen compared head-to-head, human evalua-\ntors prefer \u201cDetailed CoT 0-shot\u201d RLAIF 59% of\nthe time over \u201cBase 0-shot\u201d RLAIF18. This result\nsuggests that small gains in AI labeler alignment\nmay lead to noticeable improvements in the final\n17Results of using 4 samples are not shown because they\nonly differ from the 16-sample results by \u00b10.4%.\n18Result is statistically significantly different from 50%.\nRL policies. However, this study is limited, and fur-\nther experiments are required to draw generalizable\nconclusions.\nPreamble\nA good summary is a shorter piece of text that has the\nessence of the original.\n...\nGiven a piece of text and two\nof its possible summaries, output 1 or 2 to indicate which\nsummary best adheres to coherence, accuracy, coverage, and\noverall quality as defined above.\nExemplar\n\u00bb\u00bb\u00bb\u00bb Example \u00bb\u00bb\u00bb\u00bb\nText - We were best friends over 4 years ...\nSummary 1 - Broke up with best friend, should I wish her a\nhappy birthday...\nAnd what do you think of no contact?\nSummary 2 - should I wish my ex happy birthday, I broke no\ncontact, I\u2019m trying to be more patient, I\u2019m too needy, and I\ndon\u2019t want her to think I\u2019ll keep being that guy.\nPreferred Summary=1\n\u00bb\u00bb\u00bb\u00bb Follow the instructions and the example(s) above \u00bb\u00bb\u00bb\u00bb\nSample to Annotate\nText - {text}\nSummary 1 - {summary1}\nSummary 2 - {summary2}\nEnding\nPreferred Summary=\nTable 15: An example of a prompt fed to an off-the-shelf LLM to generate AI preference labels for summarization.\n{text}, {summary1}, and {summary2} are populated with unlabeled examples, and a preference distribution\nis obtained by computing the softmax of the log-probabilities of generating the tokens \u201c1\u201d vs. \u201c2\u201d.\nFigure 4: A screenshot of the user interface presented to human evaluators, ultimately used to calculate win rates.\nRaters are shown a context and asked to rank the quality of candidate responses.\n\u201cBase\u201d preamble\nYou are an expert summary rater.\nGiven a piece of text and\ntwo of its possible summaries, output 1 or 2 to indicate\nwhich summary is better.\n\u201cDetailed\u201d preamble\nA good summary is a shorter piece of text that has the\nessence of the original.\nIt tries to accomplish the same\npurpose and conveys the key information from the original\npost.\nBelow we define four evaluation axes for summary\nquality:\ncoherence, accuracy, coverage, and overall quality.\nCoherence:\nThis axis answers the question \u201chow coherent is\nthe summary on its own?\u201d A summary is coherent if it\u2019s easy\nto understand when read on its own and free of English errors.\nA summary is not coherent if it\u2019s difficult to understand\nwhat the summary is trying to say.\nGenerally, it\u2019s more\nimportant that the summary is understandable than it being\nfree of grammar errors.\nAccuracy:\nThis axis answers the question \u201cdoes the factual\ninformation in the summary accurately match the post?\u201d A\nsummary is accurate if it doesn\u2019t say things that aren\u2019t in\nthe article, it doesn\u2019t mix up people, and generally is not\nmisleading.\nCoverage:\nThis axis answers the question \u201chow well does\nthe summary cover the important information in the post?\u201d A\nsummary has good coverage if it mentions the main information\nfrom the post that\u2019s important to understand the situation\ndescribed in the post.\nA summary has poor coverage if\nsomeone reading only the summary would be missing several\nimportant pieces of information about the situation in the\npost.\nA summary with good coverage should also match the\npurpose of the original post (e.g.\nto ask for advice).\nOverall quality:\nThis axis answers the question \u201chow good\nis the summary overall at representing the post?\u201d This can\nencompass all of the above axes of quality, as well as others\nyou feel are important.\nIf it\u2019s hard to find ways to make\nthe summary better, the overall quality is good.\nIf there\nare lots of different ways the summary can be made better,\nthe overall quality is bad.\nYou are an expert summary rater.\nGiven a piece of text and\ntwo of its possible summaries, output 1 or 2 to indicate\nwhich summary best adheres to coherence, accuracy, coverage,\nand overall quality as defined above.\nTable 16: The \u201cBase\u201d and \u201cDetailed\u201d preambles given to the LLM labeler to obtain preference labels for the\nsummarization task.\nPreamble\nA good summary is a shorter piece of text that has the\nessence of the original.\nIt tries to accomplish the same\npurpose and conveys the key information from the original\npost.\nBelow we define four evaluation axes for summary\nquality:\ncoherence, accuracy, coverage, and overall quality.\nCoherence:\nThis axis answers the question \u201chow coherent is\nthe summary on its own?\u201d A summary is coherent if it\u2019s easy\nto understand when read on its own and free of English errors.\nA summary is not coherent if it\u2019s difficult to understand\nwhat the summary is trying to say.\nGenerally, it\u2019s more\nimportant that the summary is understandable than it being\nfree of grammar errors.\nAccuracy:\nThis axis answers the question \u201cdoes the factual\ninformation in the summary accurately match the post?\u201d A\nsummary is accurate if it doesn\u2019t say things that aren\u2019t in\nthe article, it doesn\u2019t mix up people, and generally is not\nmisleading.\nCoverage:\nThis axis answers the question \u201chow well does\nthe summary cover the important information in the post?\u201d A\nsummary has good coverage if it mentions the main information\nfrom the post that\u2019s important to understand the situation\ndescribed in the post.\nA summary has poor coverage if\nsomeone reading only the summary would be missing several\nimportant pieces of information about the situation in the\npost.\nA summary with good coverage should also match the\npurpose of the original post (e.g.\nto ask for advice).\nOverall quality:\nThis axis answers the question \u201chow good\nis the summary overall at representing the post?\u201d This can\nencompass all of the above axes of quality, as well as others\nyou feel are important.\nIf it\u2019s hard to find ways to make\nthe summary better, the overall quality is good.\nIf there\nare lots of different ways the summary can be made better,\nthe overall quality is bad.\nYou are an expert summary rater.\nGiven a piece of text and\ntwo of its possible summaries, explain which summary best\nadheres to coherence, accuracy, coverage, and overall quality\nas defined above.\nSample to Annotate\nText - {text}\nSummary 1 - {summary1}\nSummary 2 - {summary2}\nEnding\nConsider the coherence, accuracy, coverage, and overall\nquality of each summary and explain which one is better.\nRationale:\nTable 17: The prompt used for the \u201cDetailed + CoT 0-shot\u201d for summarization. For CoT prompts, we first decode a\nresponse from the LLM and then concatenate it with the original prompt and the ending \u201cPreferred Summary=\u201d\nbefore following the scoring procedure in Section 2.1 to obtain a preference distribution.\nPreamble\nA good summary is a shorter piece of text that has the\nessence of the original.\n...\nGiven a piece of text and\ntwo of its possible summaries, explain which summary best\nadheres to coherence, accuracy, coverage, and overall quality\nas defined above.\nExemplar\n\u00bb\u00bb\u00bb\u00bb Example \u00bb\u00bb\u00bb\u00bb\nText - We were best friends over 4 years ...\nSummary 1 - Broke up with best friend, should I wish her a\nhappy birthday...\nAnd what do you think of no contact?\nSummary 2 - should I wish my ex happy birthday, I broke no\ncontact, I\u2019m trying to be more patient, I\u2019m too needy, and I\ndon\u2019t want her to think I\u2019ll keep being that guy.\nThoughts on Summary 1 -\nCoherence - 7.\nRationale:\nThe summary is generally\nunderstandable, though it could be written with better\ngrammar.\nAccuracy - 9.\nRationale:\nThe summary doesn\u2019t say things\nthat aren\u2019t in the original text, and isn\u2019t misleading.\nCoverage - 6.\nRationale:\nThe summary covers most of the\nimportant information in the post and conveys the gist of\nthe original text.\nHowever, it places more emphasis on \u201cno\ncontact\u201d and could have mentioned the smothering/neediness to\nbe more complete.\nOverall Quality - 7.\nRationale:\nThe summary represents\nthe post fairly well with only minor areas where it could be\nimproved.\nThoughts on Summary 2 -\nCoherence - 3.\nRationale:\nThe summary is long-winded and\nhas several grammatical errors.\nAccuracy - 4.\nRationale:\nThe summary mentions that the\nauthor broke no contact, but this is incorrect.\nOtherwise,\nit is accurate.\nCoverage - 8.\nRationale:\nThe summary covers the key points\nin the original text.\nOverall Quality - 4.\nRationale:\nThe summary is somewhat\nmisleading and doesn\u2019t convey the original text\u2019s key points\nwell.\nPreferred Summary=1\n\u00bb\u00bb\u00bb\u00bb Follow the instructions and the example(s) above \u00bb\u00bb\u00bb\u00bb\nSample to Annotate\nText - {text}\nSummary 1 - {summary1}\nSummary 2 - {summary2}\nEnding\nThoughts on Summary 1 -\nTable 18: The template used for the \u201cDetailed + CoT 1-shot\u201d prompt for summarization, with some text removed\nfor brevity.\nSample to Annotate\nText - I met my current girlfriend online around 6 months ago\nwhen another one of our online friends was going through some\nproblems.\n...\nHowever, I feel incredibly insecure as she has yet to begin\ncollege.\nI always hear stories about how people change\nand their minds change about relationships or their s/o\nespecially in an international long distance (Canada/USA).\nI\u2019m scared she\u2019ll meet other people and eventually fall in\nlove with someone else.\nShe reassures me that she won\u2019t, and\nI know she\u2019s very mature and serious for her age, but I still\ncan\u2019t help but feel uneasy.\nSummary 1 - I have an amazing relationship with my girlfriend\nbut feel incredibly insecure with her due to lack of college\ngraduation and apprehensions about other people finding her\nSummary 2 - I\u2019m very insecure about my current girlfriend\nbecause she hasn\u2019t begun college yet and I\u2019m worried she\u2019ll\nmeet someone else and fall in love with someone else.\nAm I\nbeing ridiculous?\nCoT 0-shot Rationale\nRationale:\nSummary 2 is better.\nSummary 1 is not coherent because it\u2019s difficult to\nunderstand what the summary is trying to say.\nIt\u2019s also not\naccurate because it says that the girlfriend has graduated\nfrom college, which is not true.\nSummary 2 is coherent and\naccurate.\nIt also has better coverage because it mentions\nthe main information from the post that\u2019s important to\nunderstand the situation described in the post.\nCoT 1-shot Rationale\nThoughts on Summary 1 -\nCoherence - 7.\nRationale:\nThe summary is generally\nunderstandable, but it\u2019s a bit wordy and could be more\nconcise.\nAccuracy - 8.\nRationale:\nThe summary is accurate, but it\ndoesn\u2019t mention that the girlfriend hasn\u2019t begun college yet.\nCoverage - 7.\nRationale:\nThe summary covers the main points\nof the post, but it doesn\u2019t mention that the girlfriend\nhasn\u2019t begun college yet.\nOverall Quality - 7.\nRationale:\nThe summary is generally\ngood, but it could be more concise and it doesn\u2019t mention\nthat the girlfriend hasn\u2019t begun college yet.\nThoughts on Summary 2 -\nCoherence - 9.\nRationale:\nThe summary is concise and easy\nto understand.\nAccuracy - 9.\nRationale:\nThe summary is accurate and\nmentions that the girlfriend hasn\u2019t begun college yet.\nCoverage - 9.\nRationale:\nThe summary covers the main points\nof the post and mentions that the girlfriend hasn\u2019t begun\ncollege yet.\nOverall Quality - 9.\nRationale:\nThe summary is concise,\naccurate, and covers the main points of the post.\nTable 19: An example of the different chain-of-thought rationales produced by the 0-shot (\u201cDetailed + CoT 0-shot\u201d)\nvs. 1-shot (\u201cDetailed + CoT 1-shot\u201d) prompts for summarization. We note that the CoT 0-shot rationale is inaccurate,\nas it mistakenly claims that Summary 1 states the girlfriend has graduated from college.\nSample to Annotate\nText - I feel that out of principle I should be refunded\nthe adoption fee since the agency\u2019s foster home infected the\nkittens with the parasite.\nBoth cats were born in the foster\nhome and there are 20 other cats.\nDo I have any legal right\nto ask for the fee back?\nOr help with the cost of treating?\nThey had a disclaimer that they would not be held liable for\nany vet bills incurred but I feel that as an agency whose\nmain purpose is finding forever home for \u201chealthy, sociable\nkittens\u201d (as their website suggests) should be held liable in\nsome way.\nAny thoughts or information would be greatly appreciated as\nI am not sure about the rules or laws dealing with adoption\nagencies.\nSummary 1 - Adopted kittens from an agency who said it would\nnot be held liable for any vet bills.\nTwo days later they\nare diagnosed with Giardia.\nI\u2019m wondering if I can ask for\nthe adoption fee back and if so how to proceed.\nSummary 2 - Adopted kittens have parasite.\nWant to know if\nI can get reimbursed from the agency.\nRationale @ T = 0.0\nRationale:\nSummary 1 is better.\nIt is coherent, accurate,\nand has good coverage.\nIt also has good overall quality.\nSummary 2 is not as good because it is not as coherent and\ndoes not have as good coverage.\nRationale @ T = 1.0\nRationale:\nSummary 2 is more coherent and accurate.\nIt also\nhas better coverage, as it includes the information that the\ncats were diagnosed with Giardia.\nTable 20: An example comparing chain-of-thought rationales produced at different temperatures for self-consistency\nexperiments. Rationale @ T = 1.0 is worse because it mis-attributes the hallucination about Giardia to Summary 2\ninstead of Summary 1.\nPreamble\nYou are an expert rater of helpful and honest Assistant\nresponses.\nGiven the context and the two responses choose\nthe most helpful and honest response.\nSample to Annotate\nContext - {context}\nResponse 1 - {response1}\nResponse 2 - {response2}\nEnding\nFirst, discuss the helpfulness and honesty of each response.\nThen, explain which one is better overall and why.\nFinally,\nselect which response is the most helpful and honest.\nRationale:\nTable 21: The \u201cBase + CoT 0-shot\u201d prompting template for the helpful dialogue generation task. The AI labels\ngenerated using this prompt were used to conduct RLAIF experiments in Section 4.1.\nPreamble\nYou are an expert rater of harmful assistant responses.\nGiven the context and the two responses choose the most\nharmful response.\nExemplars\n\u00bb\u00bb\u00bb\u00bb Example \u00bb\u00bb\u00bb\u00bb\nHuman:\nHow do you embezzle money?\nResponse 1 - Assistant:\nI\u2019m afraid that\u2019s not how it\nworks,can you explain more?\nResponse 2 - Assistant:\nThe most common way to embezzle\nmoney is to overstate the business income and understate the\nexpenses to make the business seem more profitable than it\nactually is.\nFor example, one may falsify bank statements\nto make it seem like the business\u2019s sales have been good and\nthat the business isn\u2019t running a deficit, so that they can\nsecure loans to grow the business.\nBut this is only one\nexample, and embezzlement can also be carried out through\nbribery or other means.\nPreferred Response=2\n\u00bb\u00bb\u00bb\u00bb Example \u00bb\u00bb\u00bb\u00bb\nHuman:\nCan you tell me how to put ransomware on someone\u2019s\nPC?\nAssistant:\nI can do that.\nHuman:\nThanks, where do I get started?\nResponse 1 - Assistant:\nYou should create a remote desktop\naccess tool like TeamViewer.\nThen you can simply take\ncontrol of their computer through TeamViewer.\nYou can then\nuse their computer as a platform to launch an attack.\nResponse 2 - Assistant:\nRansomware is a type of malware,\nwhich is a term for a piece of computer software that\u2019s\ndesigned to do damage or harm.\nMost ransomware are designed\nto encrypt your files, like documents or emails, andthen\nhold them for ransom.\nI\u2019ll send you some information about\nransomware to help you understand it, which will also give\nyou some helpful pointers on how to better protect yourself\nand your data.\nThe primary tactic of ransomware is to\nprevent you from getting access to your files, so once that\nhappens, you should think carefully about how to restore\nthem.\nPreferred Response=1\n\u00bb\u00bb\u00bb\u00bb Follow the instructions and the example(s) above \u00bb\u00bb\u00bb\u00bb\nSample to Annotate\nContext - {text}\nResponse 1 - {response1}\nResponse 2 - {response2}\nEnding\nPreferred Response=\nTable 22: The \u201cBase + 2-shot\u201d prompting template for the harmless dialogue generation task. The AI labels\ngenerated using this prompt were used to conduct RLAIF experiments in Section 4.1.\nFigure 5: Example summaries generated by SFT, RLHF, and RLAIF policies for a Reddit post. RLHF and RLAIF\nproduce higher quality summaries than SFT, which fails to capture key details. Salient details are in bold.\nExample #1\nRLAIF summary:\nI have anxiety and I think I have a heart\nproblem.\nMy parents refuse to believe me and won\u2019t let me\nuse our medical insurance to get it checked out.\nWhat should\nI do?\nRLHF summary:\nI have anxiety and I think I have a heart\nproblem but my parents won\u2019t let me use our medical insurance\nto get it checked out.\nWhat should I do?\nI\u2019m 20 years old.\n..please help\nExample #2\nRLAIF summary:\nI slept through my 8:30 class on Rosh\nHashanah, and decided to go back to sleep until 1:00 PM.\nRLHF summary:\nI slept through my 8:30 class on Rosh Hashanah,\nand decided to go back to bed until 1:00 PM. I\u2019m a lazy f***.\nExample #3\nRLAIF summary:\nI\u2019m a nice, chill girl who is often described\nas \u201cgood\u201d but I\u2019m jealous of the girls that guys get enamored\nwith so easily.\nWhat can I do to improve myself or how I\ncommunicate/interact with guys to make myself into someone a\nguy wants to be with for the long haul?\nRLHF summary:\nI\u2019m a nice, chill girl who is often described\nas \u201cboring\u201d.\nI\u2019m jealous of the girls that guys get enamored\nwith so easily.\nWhat can I do to improve myself or how I\ncommunicate/interact with guys to make myself into someone a\nguy wants to be with for the long haul?\nTable 23: We observe that the RLHF policy sometimes hallucinates when the RLAIF policy does not. Hallucinations\nare highlighted in red.\nExample #1\nRLAIF summary:\nBoyfriend is overly flirtatious with other\ngirls, I\u2019ve talked to him about it, he doesn\u2019t seem to care.\nIt\u2019s causing trust issues.\nAm I overreacting?\nWhat else can\nI do?\nRLHF summary:\nBoyfriend is overly flirty with other girls\nand it makes me uncomfortable.\nI\u2019ve talked to him about it\nmultiple times, but he doesn\u2019t seem to care.\nWhat else can I\ndo?\nAm I overreacting?\nExample #2\nRLAIF summary:\nAsked a girl to prom, things were going great\nuntil I asked her.\nNow our conversations are awkward and I\u2019m\nnot sure if I should ask her out.\nShould I just give up?\nRLHF summary:\nI asked a girl to prom and now our\nconversations are awkward.\nI like her and I\u2019m wondering if\nI should ask her out or just give up.\nI\u2019m not sure\nExample #3\nRLAIF summary:\nBroke up with girl 1, she started dating my\nbest friend.\nDated girl 2, she became unstable and we broke\nup.\nI still feel attached to both.\nWhy?\nHow do I get over\nthis?\nAny advice?\nRLHF summary:\nI broke up with a girl I was with for years,\nand she started dating my best friend.\nI started dating a\nnew girl, and she became depressed and distant.\nI still feel\nattached to both.\nWhy?\nTable 24: We observe that that summaries from the RLAIF policy are sometimes less coherent and grammatical\nthan summaries from the RLHF policy. Less coherent phrases are highlighted in red.\n"
  },
  {
    "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
    "link": "https://arxiv.org/pdf/2309.00359.pdf",
    "upvote": "19",
    "text": "Published as a conference paper at ICLR 2024\nLARGE CONTENT AND BEHAVIOR MODELS TO UN-\nDERSTAND, SIMULATE, AND OPTIMIZE CONTENT AND\nBEHAVIOR\nAshmit Khandelwal\u22c61\nAditya Agrawal\u22c61\nAanisha Bhattacharyya\u22c6\nYaman K Singla\u22c6\nSomesh Singh\nUttaran Bhattacharya\nIshita Dasgupta\nStefano Petrangeli\nRajiv Ratn Shah\nChangyou Chen\nBalaji Krishnamurthy\nAdobe,\nBITS, Pilani,\nIIIT-Delhi,\nState University of New York at Buffalo\nABSTRACT\nShannon and Weaver\u2019s seminal information theory divides communication into\nthree levels: technical, semantic, and effectiveness. While the technical level deals\nwith the accurate reconstruction of transmitted symbols, the semantic and effec-\ntiveness levels deal with the inferred meaning and its effect on the receiver. Large\nLanguage Models (LLMs), with their wide generalizability, make some progress\ntowards the second level. However, LLMs and other communication models are\nnot conventionally designed for predicting and optimizing communication for de-\nsired receiver behaviors and intents. As a result, the effectiveness level remains\nlargely untouched by modern communication systems. In this paper, we intro-\nduce the receivers\u2019 \u201cbehavior tokens,\u201d such as shares, likes, clicks, purchases,\nand retweets, in the LLM\u2019s training corpora to optimize content for the receivers\nand predict their behaviors. Other than showing similar performance to LLMs on\ncontent understanding tasks, our trained models show generalization capabilities\non the behavior dimension for behavior simulation, content simulation, behav-\nior understanding, and behavior domain adaptation. We show results on all these\ncapabilities using a wide range of tasks on three corpora. We call these models\nLarge Content and Behavior Models (LCBMs). Further, to spur more research\non LCBMs, we release our new Content Behavior Corpus (CBC), a repository\ncontaining communicator, message, and corresponding receiver behavior*.\n1\nINTRODUCTION\nBehavior\nText\nVideo Image\nI have two ad variants A and B, which one will the user like?\nToday is Jan 1 2023, how many people in Texas will buy this shoe\nby Jul 1 2023?\nWhat is the color of the wall behind the man?\nWhat does the narrator say when Emily gets into the car?\nIf my target segment is new parents, how can I modify my\nmessage such that I get a click through rate of 2.5%?\nIf 100k people viewed this food ad, why did only 500 people\nclick on it?\nBehavior\nSimulation\nContent\nUnderstanding\nContent\nSimulation\nBehavior\nUnderstanding\nVariant A\n224k\nWhite\nMake the baby face the camera\nrather than her mother\nThe ad sells cheese burger but gets shown\nduring morning time. People usually\nprefer healthier options during morning.\nLarge \nContent-Behavior\nModel (LCBM)\nVerbalized Content\nand Behavior\nEmily was elated to see her\ngrandma after a long time\nFigure 1: Encoding and predicting content (images, videos, and text) and behavior in the language space. Large\nContent Behavior Models (LCBMs), once trained, can enable a host of different applications, including behav-\nior simulation, content understanding, content-behavior optimization, and content-behavior understanding.\n\u22c6Equal Contribution. Contact ykumar@adobe.com for questions and suggestions.\n1 Work done while at Adobe.\n*https://behavior-in-the-wild.github.io/LCBM\n1\narXiv:2309.00359v4  [cs.CL]  16 Mar 2024\nPublished as a conference paper at ICLR 2024\n(a)\n(b)\n(c)\n(d)\nFigure 2: Comparison of GPT-3.5, GPT-4, Vicuna-13B, and LCBM-13B on: (a) Behavior Simulation accuracy\non two types of behaviors: replay value prediction and likes/views prediction. The task is, given the video\ncontent and channel information, to predict replay values corresponding to each scene and the ratio of likes\nto views. (b) Content simulation and behavior understanding tasks. The task for content simulation is, given\nthe channel information and scene-level behavior, to predict the scene content. Given information on the video\nplatform and the video content, the task of behavior understanding is to predict and explain the sentiments\nof the viewers and the commenters. Six evaluators scored the models\u2019 explanations between 0-5 to get the\npredicted sentiment and explanation scores by comparing the ratings and reasons with the user comments. The\nannotators did not know which model gave the reasoning. (c) Content understanding tasks. We evaluate four\ntasks: emotion, topic, and persuasion strategy prediction, and action-and-reason understanding. (d) Behavior\nSimulation on in-house Email Marketing Data (R2 score) and Twitter likes (accuracy), and Content Simulation\non Twitter tweet prediction (BLEU/ROUGE scores). It can be noted that on the behavior simulation, content\nsimulation, and behavior understanding tasks, LCBM performs better than 3-shot GPT-3.5 and 10-shot GPT-4\n(covering a larger area. On the content understanding tasks, while LCBM outperforms similar-sized Vicuna\nmodels, GPT-3.5 performs better. However, we also note that GPT-3.5 and GPT-4 are at least 12 times larger\nthan LCBM-13B. Further, we show the behavior domain adaptation results in Table 8, 9, 10.\nShannon & Weaver (1949), in their seminal paper on communication, includes all of the proce-\ndures by which one mind may affect another. This includes all forms of expression, such as words,\ngestures, speech, pictures, and musical sounds. They mentioned that the broad problem of commu-\nnication can be studied at three levels: technical, semantic, and effectiveness.\nLevel A: Technical. How accurately can the symbols of communication be transmitted?\nLevel B: Semantic. How precisely do the transmitted symbols convey the desired meaning?\nLevel C: Effectiveness. How well does the received meaning induce the desired conduct in the\nreceiver?\n2\nPublished as a conference paper at ICLR 2024\nCommunicator\nMessage\nChannel\nReceiver\nEffect\nNike\nSubscribers: 754K\n{\n}\nYoutube\n{ }\nNike Subscribers/\nGeneral Youtube\n{ Audience\n}\nlikes: 15k\nviews: 60M\nscene replays:\n[73,85,99,...,24]\n{\n}\ndescription: a sunny\nfootball ground\nobjects: a football,\nplayers, grass\nasr: even after hours of\n{ practice,\n}\nFigure 3: Five factors of communication: Communicator, Message, Channel, Receiver, and Effect.\nThese three levels build on top of each other. Thus, solving the problem at Level C necessarily\nrequires solving the corresponding problems at Levels A and B.\nSince the publication of this seminal paper, the tremendous growth in the field of telecommunica-\ntions, particularly the advent of the Internet and mobile devices, has led to affordable, wide-scale\nsolutions for Level A. With the recent advances in large language models (LLMs) such as BERT\n(Devlin et al., 2018), GPT-3 and 4 (Brown et al., 2020; OpenAI, 2023), T5 (Raffel et al., 2020),\nand many more, we have witnessed a significant improvement in the performance of various Natural\nLanguage Processing (NLP) tasks. LLMs in zero- or few-shot settings can easily handle tasks such\nas question answering, summarization, translation, and many more. This has helped us progress\ntowards solving Level B to a large extent. However, the Level C problem of effectiveness remains\nlargely unsolved. Effectiveness refers to designing messages that can fulfill the communicators\u2019\nunderlying objectives, such as explaining complex concepts to the receivers and informing the re-\nceivers\u2019 choices (e.g., when making purchase decisions).\nHow do we solve the effectiveness problem while retaining the other two levels? To solve the\neffectiveness problem, we can take inspiration from how the semantic problem is being solved. Raf-\nfel et al. (2020), in their seminal work on T5, mention that the basic idea underlying large language\nmodels is to treat every text processing problem as a \u201ctext-to-text\u201d problem, i.e., taking the text as\ninput and producing new text as output. This framework allows for a direct application of the same\nmodel, objective, training procedure, and decoding process to every task we consider. Further, this\nallows us to pre-train a model on a data-rich task like the next-word prediction, which can then be\ntransferred to downstream tasks. Notably, thanks to the Internet, the next-word prediction task has\nhuge amounts of available data. Consider the Common Crawl project (https://commoncrawl.org),\none common source of data included in most language models. It produces more than 20TB of text\nper month sampled from random web pages across the internet.\nT5 and other language models like GPT-3, Pythia (Biderman et al., 2023), and Llama (Touvron et al.,\n2023) can solve a wide variety of tasks, including the ones for which they were not explicitly trained.\nFor instance, language models trained on the next word prediction task showed generalization ca-\npabilities across a wide variety of tasks like question-answering, summarization, natural language\ninference, and translation (Brown et al., 2020). Recently, a series of papers have shown that this\ngeneralized \u201cworld understanding\u201d captured in LLMs can be leveraged to enable them to \u201csee\u201d (Liu\net al., 2023; Li et al., 2023b;a; Zhu et al., 2023; Ge et al., 2023; Zhang et al., 2023; Bhattacharyya\net al., 2023). This is a significant capability enhancement since a model trained in language only\nsettings can be made to reason about images and videos. These papers follow the same transfer\nlearning approach advocated by T5, where they convert visual information to language space to\nleverage the \u201ctext-to-text\u201d framework. They show that it is possible to teach a large language model,\nthe new modality of vision, without needing to pre-train the model from scratch. Rather, using only\na few million tokens, it is possible to scale LLMs\u2019 abilities to vision as well. Following this chain\nof thought, it could be possible to solve the effectiveness problem by posing it as a \u201ctext-to-text\u201d\nproblem. This is the paradigm we explore in this work.\nHow can we pose the effectiveness problem as a text-to-text problem? The problem of effect is\nto know what the receiver does after receiving the message (Shannon & Weaver, 1949). In general,\nfor a piece of content, other than the content itself, we often have information about who consumes\nthe content and what his action is on consuming the content. The latter is the effect described in\nShannon\u2019s three levels of communication. For instance, an email, as a message from the communi-\n3\nPublished as a conference paper at ICLR 2024\ncator to the receiver, elicits certain actions from the receiver like link-clicks, replies, and read-time.\nWhile LLMs are trained on trillions of tokens of content, the training does not include the receiver\neffect. For instance, Enron Email (Klimt & Yang, 2004) is a popular corpus that is included in the\ntraining of LLMs like Pythia (Biderman et al., 2023). It contains 600K email content sourced from\nthe Enron corporation, which LLMs use to learn how to write emails. However, it does not contain\ndata about the receivers\u2019 activities, such as whether they opened the email, how long they kept it\nopen (read-time), and what their reply was. Similarly, while major text corpora include a large num-\nber of public blogs and user forums to train LLMs like CommonCrawl, they are stripped of receiver\nbehavior on forum messages, such as the number of likes, shares, and replies, before including them\nin LLM training (for instance, see (Biderman et al., 2022; Penedo et al., 2023)). To pose the effec-\ntiveness problem as a text-to-text problem, we can include these behavior tokens in the text along\nwith content tokens and train the LLM to model both of those in the same space. This might help\nthe LLM simulate the receiver effect, optimize for it, and reason about it.\nIn this paper, we show initial experiments to integrate behavior as a new modality to increase the\nscope of multimodal LLMs from only content to both content and behavior. We call this new type\nof model a Large Content Behavior Model (LCBM). This class of models shows promise in en-\nabling the LLMs to not only reason about content but also reason about and predict human behavior\nover that content. Further, LCBMs have the potential for behavior domain adaptation where models\ntrained on one type of behavior can generalize on another behavior type (Fig. 2). Behavior simula-\ntion can enable many real-world applications, such as content recommendation, customer journey\noptimization, and A/B testing. To build LCBM, we introduce behavior instruction tuning (\u00a72.4),\nan attempt to extend the instruction tuning paradigm to behavior space, bringing all five communi-\ncation factors (communicator, message, channel, receiver, and effect) into the same space (Fig. 3).\nSimilar to Brown et al. (2020); Raffel et al. (2020); Liu et al. (2023); Ge et al. (2023), we do not\ndesign best-in-class predictors for any of the downstream tasks. Rather, we show a model which\nshows generalization capabilities across a wide variety of content- and behavior-related tasks. To\nsummarize, our paper makes the following two contributions:\n\u2022 Large Content Behavior Model (LCBM). We develop a large multimodal model that shows\ncapabilities of behavior simulation (given content), content simulation (given behavior), content\nunderstanding, and behavior understanding (Fig. 1). Following the text-to-text framework, we\nconnect the Vicuna LLM (Touvron et al., 2023; Chiang et al., 2023) with an open-set visual\nencoder of EVA-CLIP (Sun et al., 2023) and instruction fine-tune it end-to-end on behavior in-\nstruction data. EVA-CLIP and QFormer (Li et al., 2023a) help the model to understand visual\ncontent in the language space, making it a Vision Language Model (VLM). During behavior in-\nstruction tuning, we teach the model to predict behavior given content and content given behavior\nusing various instruction tasks (\u00a72.4). This helps us teach behavior modality to the VLM while\ngrounding it in the natural language space. We use three datasets to show the performance of\nLCBM: a dataset consisting of YouTube videos as the content and the corresponding retention\ngraph, likes, the number of views, and comment sentiment as receiver behavior; a dataset consist-\ning of Twitter posts (text, images, and videos) and corresponding human behavior (like counts)\nextracted from 168 million tweets across 10135 enterprise Twitter accounts from 2007 to 2023\nKhurana et al. (2023b); and an internal dataset of In-house Marketing Emails|| (content) and the\nclick-through rate corresponding to each segment they were sent to (behavior). We observe that\nteaching the LCBM behavior and content simulation improves its capabilities on them (expected),\nbut the model also shows signs of domain-adaptation in behavior modality (few-shot capability,\nunexpected) (Tables 8,9,10) and improvements in behavior understanding (Figs. 5,6,\u00a73) (zero-shot\ncapability, unexpected) (Brown et al., 2020). See Fig. 2 for a radar plot of all the capabilities and\ncomparisons of performances across LCBM and state-of-the-art LLMs: GPT-3.5 and GPT-4.\n\u2022 Dataset and Test Benchmark. To spur research on the topic of large content and behavior\nmodels, we release our generated behavior instruction fine-tuning data from over 40,000 public-\ndomain YouTube videos and 168 million Twitter posts. The data contains: 1) YouTube video\nlinks, automatically extracted key scenes, scene verbalizations, replay graph data, video views,\nlikes, comments, channel name, and subscriber count at the time of collection, and 2) Twitter ex-\ntracted account names, tweet text, associated media (image and video) verbalizations (including\nimage captions, keywords, colors, and tones), tweet timestamps, and like counts Khurana et al.\n(2023b). We also release a benchmark to test performance on the joint content behavior space\n||We obtain In-house Marketing Emails dataset by collaborating with the In-house team.\n4\nPublished as a conference paper at ICLR 2024\nFigure 4: Encoding and predicting content (images, videos, and text) and behavior in the language space. Strat-\negy to behavior instruction fine-tune (BFT) LLMs to create LCBMs. We capture visual concepts through the\nvisual encoder (EVA-CLIP), and world knowledge is through an LLM (Llama). To leverage the rich knowl-\nedge of LLMs, we use GMHRA and QFormer to convert visual tokens of ViT to language tokens that Llama\ncan understand. Further, we find that verbalizing the visual stimulus helps Llama to gather information more\nexplicitly than what is provided by ViT+QFormer. We fine-tune the combined model end-to-end to predict\n1) behavior given content and 2) content given behavior. Snowflake and fire symbols denote the frozen and\nunfrozen parts of the architecture.\n(\u00a72.3), introducing two types of tasks in this space: predictive and descriptive. In the predictive\nbenchmark, we test the model\u2019s ability to predict behavior given the content and predict content\ngiven the behavior. In the descriptive benchmark, we validate its explanation of human behav-\nior by comparing it with ground-truth annotations we obtain from human annotators that try to\nexplain human behavior. See Figs. 5,6 for a few examples.\n2\nSETUP\nIn this section, we introduce our approach to model content and behavior together as a text-to-text\nproblem. Since most publicly available corpora strip off receiver behavior from content, we first\nintroduce our dataset, \u201cThe Content Behavior Corpus (CBC)\u201d, a dataset consisting of content and\nthe corresponding receiver behavior. Next, we introduce our methodology to convert the content and\nbehavior into text and our approach to model it using an LLM. Then, we cover the tasks through\nwhich we test various capabilities of LCBM (Fig. 1): content-understanding, behavior understand-\ning, content simulation, behavior simulation, and behavior domain adaptation.\n2.1\nTHE CONTENT BEHAVIOR CORPUS (CBC)\nThe availability of large-scale unlabeled text data for unsupervised learning has fueled much of\nthe progress of LLMs. In this paper, we are interested in modeling content and the corresponding\nreceiver behavior in the same space. While available datasets contain trillions of content tokens\n(text, images, audio, and videos), they unfortunately do not contain the receiver effect. To address\nthis, we utilize YouTube and Twitter, two large publicly available sources of content-behavior data,\nconsisting of (a) account name, account description, and number of subscribers and followers (com-\nmunicator data) , (b) rich content in the form of videos, images, creator-provided captions, titles,\nand descriptions (message), (c) behavior in the form of likes, views, user comments, and replay\ngraph (receiver effect). This covers all the five factors of communication (Fig. 3), with the channel\nbeing fixed (as YouTube or Twitter) and receivers being average channel followers and viewers of\nthe communicator. Since content data is multimodal in the form of a combination of images, videos,\nand text, and behavior data is in the form of numbers, to model it using a text-to-text paradigm, we\nverbalize both of them following the methodology we detail next.\nVerbalization: For the video V , YouTube provides us with 100 average viewer retention values\nri for i \u2208 [0..100), corresponding to the entire video. The sampling rate of 100 is constant and inde-\npendent of video length (T). Replay value ri corresponds to video frames between the timestamps\n(T/100\u00d7i, T/100\u00d7(i+1)), which denotes how often these frames were replayed compared to the\nmost replayed frames. The metric has a value between 0 and 1 that identifies the video\u2019s relative re-\ntention performance at a given point in the video. To accommodate longer video lengths, we merge\nreplay values until T/100 \u00d7 (i + j) \u2212 T/100 \u00d7 i > 1 second with j \u2208 {i + 1, 100}. We choose\n5\nPublished as a conference paper at ICLR 2024\nthe replay value for this merged group of scenes as max(ri, ..., rj). Using this logic, we get replay\nvalues Ri for i \u2208 [0..m], where m = \u230a100/(\u2308100/T\u2309)\u230b. Next, we sample two frames randomly\ncorresponding to each i \u2208 [0..m]. We caption the frames using BLIP (Li et al., 2023a). We also\nobtain the automatic speech recognition for the speech for the video between the timestamps corre-\nsponding to replay value Ri using Whisper (Radford et al., 2023). The ASR and BLIP captions are\ncontent for scenes, and replay values are the behavior corresponding to them. We include the scene\ncontent and behavior in the video verbalization (Listing 1) with the sampling for both scene content\nand behavior as described above.\nWe also include video content by encoding video frames through EVA-CLIP (Sun et al., 2023)\n(explained in \u00a72.2). Other than video embeddings, we include the video title and description as\npart of the video content. Corresponding to the overall video content, we verbalize overall video\nbehavior metrics like video views and the ratio of likes and views. Finally, we append it with\ncommunicator information on the video channel and the subscriber count. The Listing 1 presents\nthe overall verbalization for video and frame level content and behavior. The verbalization for\nTwitter posts is similar and is given in Listing 7.\nListing 1: Verbalization pattern for inputting content and behavior in the same space\nInput : <video> ..[Video Tokens] .. </video>\nThe video has the following scenes:\nScene 1: {ASR: Welcome to a quick tutorial , OCR: Adobe Premiere Pro, Captions: A desktop interface , Replays: 60},\nScene 2: {ASR: on using Premiere Pro to edit , Captions: A computer interface , with an image of a white horse. Objects \u2212 Horse,\nGrass, Fence., Replays: 53},\n...\nIt was posted on Adobe's YouTube channel with the\ntitle\n'Using Premiere Pro like a Pro' on Aug 15 2022. Adobe's YouTube\nchannel has 100k subscribers . This video was viewed by 346 thousand people and liked (as a percentage of likes /views) by\n2.3%\n2.2\nMODEL\nTo understand both visual and textual contents, we follow a similar approach as was taken by recent\nmodels like BLIP, Llava, VideoLlama, and others (Liu et al., 2023; Ge et al., 2023; Li et al., 2023a;\nZhu et al., 2023), we use visual encoders to encode visual knowledge and an LLM to encode text\nand world knowledge. Fig. 4 shows our architecture to encode visual content into the language\nspace. We include video content by encoding video frames through EVA-CLIP (Sun et al., 2023)\nand Global Multi-Head Relation Aggregator (GMHRA) from Uniformer (Li et al., 2021). GMHRA\nhelps aggregate the information better across the time dimension. The combination of ViT and\nGMHRA gives us a good representation of the visual content. Next, to effectively leverage the\nLLM\u2019s rich language representations, we use Q-Former from BLIP-2 (Li et al., 2023a) with an\nextra linear layer and additional query tokens to convert from visual tokens to language tokens.\nFurther, similar to Bhattacharyya et al. (2023), we find that while encoding visual tokens is powerful,\nconverting visual content to text adds to the downstream performance. Therefore, we include the\nBLIP caption for each scene along with the scene replay graph.\nWe use the Llama-based Vicuna-13B LLM (Touvron et al., 2023; Chiang et al., 2023) as our base\nLLM. Similar to prior works (Liu et al., 2023; Ge et al., 2023; Li et al., 2023a; Zhu et al., 2023),\nwe follow a two-stage training paradigm where in the first stage, we utilize the WebVid (Bain et al.,\n2021), COCO caption (Chen et al., 2015), Visual Genome (Krishna et al., 2017), CC3M (Sharma\net al., 2018), and CC12M (Changpinyo et al., 2021) datasets to align the visual encoder embeddings\nwith LLM. In the second stage, we train the model with behavior instructions prepared by following\nthe approach described in \u00a72.4. In summary, LCBM takes concatenated inputs of visual tokens,\nscene ASR, caption, scene behavior of replays, channel information, and video title and behavior\nmetrics of views and a ratio of likes to views. Based on the instruction, we test LCBM\u2019s abilities on\nvarious tasks we cover in the next paragraphs.\n2.3\nCONTENT BEHAVIOR TEST BENCHMARK\nWe test the capabilities of large content-behavior models on predictive and descriptive abilities on\ncontent and behavior, as illustrated in Fig: 1. We design the following five tasks to test these ca-\n6\nPublished as a conference paper at ICLR 2024\nModel\n#Params\nTraining\nPast\nFuture\nRandom\nAll Masked\nWindow Size\n5\n7\nRMSE\nAccuracy\nRMSE\nAccuracy\nRMSE\nAccuracy\nRMSE\nAccuracy\nRMSE\nAccuracy\nLCBM\n13B\n3-BFT\n8.12\n55.10\n15.05\n42.42\n8.55\n61.41\n9.91\n55.10\n-\n-\nLCBM\n5-BFT\n11.53\n52.06\n12.02\n53.06\n8.13\n64.83\n9.22\n60.26\n31.34\n17.16\nLCBM\n7-BFT\n16.17\n35.61\n15.14\n44.11\n9.02\n59.22\n10.47\n53.84\n-\n-\nLCBM\n11-BFT\n18.25\n30.95\n15.05\n41.44\n10.01\n55.15\n10.49\n52.61\n-\n-\nGPT-4\n>100B\u2020\n10-shot-ICL\n34.45\n20.55\n19.51\n36.08\n22.99\n26.99\n27.25\n17.27\n38.52\n14.26\nGPT-4\n2-shot-ICL\n35.05\n19.34\n18.07\n39.33\n17.42\n38.10\n21.26\n28.05\n37.60\n13.73\nGPT-3.5\n175B\n3-shot-ICL\n34.10\n19.06\n24.71\n27.14\n24.52\n24.81\n26.30\n18.74\n38.77\n13.47\nGPT-3.5\n2-shot-ICL\n33.36\n18.02\n26.44\n25.42\n23.35\n25.35\n24.68\n21.24\n37.16\n13.39\nRandom\n-\n-\n34.10\n10.00\n34.10\n10.00\n34.10\n10.00\n34.10\n10.00\n34.10\n10.00\nTable 1: Behavior Simulation. Mean RMSE and accuracy scores for scene-by-scene predictions of video\nreplay values. Replay values are the normalized replay scores of each scene as provided by YouTube. The\nnormalized scores are considered to 2 decimal places and multiplied by hundred to convert the score to an\ninteger score in the range 0-100. RMSE is calculated for each video in the test set and the mean is calculated\nfor this score and reported. The model is said to classify correctly if the absolute error between the predicted\nand ground truth value is less than or equal to 5. The scores are calculated in four regimes: past, future, random,\nand all-masked. In the past (future) regimes, first (last) 5-20% scenes are masked; in the random setting, 5-20%\nscenes are masked randomly, and in all masked setting, everything is masked. LCBM was behavior-fine-tuned\n(BFT) with 3,5,7,11 context window masking strategy, while GPT was compared with an in-context learning\n(ICL) setting. We note that behavior fine-tuned LCBM, while being at least 10x smaller than other models,\nperforms the best. Best models are denoted in green and runner-ups in blue .\nModel\n#Params\nTraining type\nTraining\nRMSE\nR2\nAccuracy\nLCBM\n13B\nBFT\nReplay values 3-masked\n1.31\n0.87\n15.89\nLCBM\nBFT\nReplay values 5-masked\n1.48\n0.82\n19.93\nLCBM\nBFT\nReplay values 7-masked\n1.71\n0.78\n15.20\nLCBM\nBFT\nReplay values 11-masked\n1.55\n0.82\n13.94\nGPT-4\n>100B\u2020\nICL\n10-shot\n3.50\n-0.01\n7.84\nGPT-4\nICL\n2-shot\n3.58\n-0.03\n5.39\nGPT-3.5\n175B\nICL\n3-shot\n64.40\n-256.96\n2.48\nGPT-3.5\nICL\n2-shot\n64.88\n-375.83\n1.27\nRandom\n-\n-\n-\n4.67\n0\n3.94\nTable 2: Behavior Simulation. RMSE, R2, and accuracy scores for like/view ratio prediction task. To calculate\naccuracy, the model is said to classify correctly if the absolute error between the predicted and ground truth\nlikes/views is less than or equal to 10%. BFT denotes behavior fine-tuning, and ICL stands for in-context\nlearning. Replay values k-masked means a model which is trained by masking k consecutive values of the\nreplay graph while doing BFT. We note that LCBM while being at least 10x smaller than the other models,\nperforms the best. The best results over four runs are reported for all models. Best models are denoted in\ngreen and runner-ups in blue .\npabilities: behavior simulation, content simulation, content understanding, behavior understanding,\nand behavior domain adaptation. We cover each of these tasks next.\n1. Behavior Simulation. We test simulation capability on four behaviors across two datasets:\nYouTube replay values, the ratio of YouTube likes to views, Twitter likes, and the number of\nviews of the YouTube video. The common task amongst all of them is to predict the behavior\ngiven the content and content attributes like captions, scene-by-scene descriptions for videos, and\nsender characteristics like account and subscriber count and date of posting. The behavior to be\npredicted is masked and asked as a question to the LLM. Listings 6 and 7 lists the verbalization\npattern for this task. For replay value prediction, we test the masked behavior in three settings:\nMasked Past (all replay values of the first 5-20% scenes are masked), Masked Future (all replay\nvalues of last 5-20% scenes are masked), and Random Masks (random masking of replay values\nfor 5-20% scenes).\n2. Content Simulation. Here, the task is to predict content given receiver behavior (Listing 5,\n10). For YouTube, given the video content in terms of scene-by-scene descriptions with the\ncontent of one group of five consecutive scenes content being masked, behavior values of all\nscenes, and channel information, the task is to choose the masked scene speech from a list of 25\n\u2020The exact size of GPT-4 is unknown.\n7\nPublished as a conference paper at ICLR 2024\nLogo\nproduct\nLogo\nLogo\nproduct\nTable 3:\nThe In-house marketing emails\nused in the Email dataset look similar to the\nones shown here.\nDate Range\nApril 1, 2022 to June 12, 2023\nNumber of Countries\n225\nTarget Products\nTop Products used by millions of users\nCustomers Segmented on the basis\nof\nType of use, user expertise, frequency\nof use, and others\nTable 4: Details of the In-house Marketing Email dataset used to\nevaluate behavior generalization capabilities of the LCBM\noptions, chosen randomly from the entire test set. For YouTube, we chose to model this task as\na discriminative task instead of a generative one since videos are generally long, and there could\nbe multiple possible contents for a given behavior, whereas the ground truth is available only for\none specific characterization of the content for a given behavior. For Twitter, we model this task\nas content generation. The Listing 8 presents the format for this task.\n3. Behavior Understanding. The goal of this task is to check if the model can reason about ob-\nserved or unobserved receiver behavior. For this task, we could ask the model to explain any\nbehaviors given the content. However, only the YouTube receiver comments have ground truth\navailable with the video. Without ground truth, we found that other behaviors, such as replay\nvalues, likes, and views, are difficult to explain by non-experts. Therefore, we ask the model\nto simulate the sentiment of the receivers\u2019 comments and describe its reasoning. To evaluate,\nwe asked six annotators to annotate the reasons provided by the model on a scale of 0-5, with 0\nimplying the LLMs provided no sentiment or reasoning and 5 implying perfect reasoning. The\nannotators were free to rate the LLMs as they seemed fit. The annotators were asked to review\nthe video content and the comments to help them evaluate the reasons. We average the ratings\nof three annotators to get an average rating for every video. Similarly, to review the sentiment\ncorrectness, we asked the annotators to judge the predicted sentiment rating with respect to user\ncomments.\n4. Content Understanding. To check if a model trained on both content and behavior tokens\ndoes not forget its original content understanding capabilities, we test the content understanding\ntasks on YouTube videos, following Bhattacharyya et al. (2023). They use the following tasks\nfor video-understanding: topic, emotion, persuasion, and action-reason classification. For topic,\nemotion, and action-reason classification tasks, they use the advertisements dataset by Hussain\net al. (2017), which contains 3,477 video advertisements and the corresponding annotations for\nemotion and topic tags and action-reason statements for each video. There are a total of 38\ntopics and 30 unique emotion tags per video. Further, we have 5 action-reason statements for\neach video for the action-reason generation task. For our experiment, we use the subset of 1,785\npublic videos. Following Bhattacharyya et al. (2023), for the topic and emotion classification\ntask, we evaluate our pipeline using top-1 accuracy as the evaluation metric. Further, we evaluate\nemotion classification on clubbed emotion labels as well. For action and reason prediction, we\nevaluate our accuracy on the action and reason retrieval tasks where 29 random options along\nwith 1 ground truth are provided to the model to find which one is the ground truth. In the per-\nsuasion strategy classification, we use the 1002 persuasion strategy videos and corresponding\nlabels released by Bhattacharyya et al. (2023). Given the video, the model has to predict which\npersuasion strategy the video conveys. Persuasion strategy classification could be an important\ntask for evaluating LCBM since the concept of persuasion in psychology views human commu-\nnication as the means to change the receiver\u2019s beliefs and actions (i.e., to persuade) (Kumar et al.,\n2023), and understanding the different strategies present in communication may help understand\nhuman behavior better. We evaluate the top-1 accuracy of the model on this task.\n8\nPublished as a conference paper at ICLR 2024\nModel\n#Params\nAccuracy\nVicuna\n13B\n19.30%\nLCBM\n13B\n48.68%\nGPT-3.5\n175B\n34.98%\nRandom\n-\n4%\nTable 5: Content Simulation. In this task, the\nmodels have to choose the speech segment from a\nlist of 25 options given the video description, non-\nmasked scenes. and replay behavior. We see that\ndespite being similar to masked language mod-\neling (which is a content-only task), LCBM per-\nforms better than both Vicuna and GPT-3.5. Best\nmodels are denoted in green and runner-ups in\nblue .\nModel\n#Params\nSentiment Accuracy\nReasoning Score\nVicuna\n13B\n65.66%\n2.23\nLCBM\n13B\n72.73%\n4.00\nGPT-3.5\n175B\n61.62%\n1.67\nTable 6: Behavior Understanding. In this task, the mod-\nels have to simulate the sentiment of comments that a\nvideo would get by looking at only the video. Further,\nthey also have to explain the reason for such sentiment.\nThe responses were annotated by humans on a scale of 0-\n5 for the reason, with 0 being no response provided and\n5 being the response matches exactly with the (ground\ntruth) comments received on the video. Best models are\ndenoted in green and runner-ups in blue .\nTraining\nModel\n#Params\nTopic\nEmotion\nPersuasion\nAction\nReason\nAll labels\nClubbed\nRandom\nRandom\n-\n2.63\n3.37\n14.3\n8.37\n3.34\n3.34\nZero-shot\nGPT-3.5\n175B\n51.6\n11.68\n79.69\n35.02\n66.27\n59.59\nVicuna\n13B\n11.75\n10.5\n68.13\n26.59\n20.72\n21.00\nVideoChat\n(Li et al., 2023b)\n13B\n9.07\n3.09\n5.1\n10.28\n-\n-\nLCBM\n13B\n42.17\n7.08\n58.83\n32.83\n39.55\n27.91\nTable 7: Content Understanding. Comparison of several models, including behavior instruction tuned models\nbefore and after BFT. We compare the models across topic, emotion, and persuasion strategy detection tasks\nas per the framework given by Bhattacharyya et al. (2023). We see that our model outperforms similarly sized\nmodels (Vicuna, VideoChat) in most tasks. Best models are denoted in green and runner-ups in blue .\n5. Behavior Domain Adaptation. In the past work, we have observed strong generalization ca-\npabilities from LLMs (OpenAI, 2023; Ouyang et al., 2022; Raffel et al., 2020). While training\non next token prediction, LLMs show generalization across tasks, including question answer-\ning, natural language inference, and sentiment analysis. Given this, the natural question is, does\nLCBM, too, show this kind of generalization, where a model trained on one kind of behavior,\ncan show performance on another behavior? To understand this, we test the model on a different\ndataset and task than what it was originally trained for. We do this over three datasets, LVU\n(Wu & Krahenbuhl, 2021), In-house Email Marketing||, and generalization between Twitter and\nYouTube likes.\n\u2022 LVU Benchmark. Wu & Krahenbuhl (2021) released a benchmark for long video under-\nstanding with over 1000 hours of video. In the benchmark, they have two behavior related\ntasks: ratio of likes to likes+dislikes and view prediction. YouTube has discontinued the\ndislike count, therefore, our corpus does not contain the dislike count. We use the LVU test\nbenchmark to check if a model trained on other available behaviors (views, likes, and replay\ngraphs) is able to predict the like ratio.\n\u2022 In-house Email Marketing. In this task, we ask the model to predict the click-through\nrate for a given target segment of an email, given the email content, subject, and verbalized\ndescriptions of the images in the email. We use the emails sent by In-house marketing team\nto its subscribers. The emails were sent from April 1, 2022 to June 12, 2023 and covered\nmany of the premiere products The emails were sent to many customer segments (as defined\nby the marketing team) across 225 countries (Table 4). Listing 3 lists the verbalization\nformat to verbalize emails to input to the LCBM.\n2.4\nBEHAVIOR INSTRUCTION FINE-TUNING (BFT)\nTo teach an LLM the behavior modality over multimodal content, we convert both the visual tokens\nand behavior modality in the text format and instruction fine-tune the LLM end to end. This follows\na two-stage approach: first, we teach the LLM the visual modality (\u00a72.2), and next, we teach the\n9\nPublished as a conference paper at ICLR 2024\nIn-house Email Marketing\nLCBM Type\nFine-tuned\nTrained On\nTested On\nRMSE\nR2\non\nYouTube?\nUnique\nEmails\nUnique\nSegments\nEmail-Segment\nPairs\nDomain-\nAdapted\nYes\n100\n10\n1k\nDifferent Segment\n(emails could\nbe same)\n14.47\n0.64\nIn-\nDomain\nNo\n600\n560k\n350k\n25.28\n0.55\nDomain-\nAdapted\nYes\n100\n10\n1k\nDifferent Segments\n& Different Emails\n27.28\n0.54\nIn-\nDomain\nNo\n600\n560k\n350k\n29.28\n0.5\nLVU Benchmark\nTraining\nModel\nTesting\nMSE\nTrained\nR101-slowfast+NL\n(Wu & Krahenbuhl, 2021)\nTest set\n0.386\nTrained\nVideoBERT\n(Sun et al., 2019)\nTest set\n0.32\nTrained\nQian et al. (2021)\nTest set\n0.353\nTrained\nXiao et al. (2022)\nTest set\n0.444\nTrained\nObject Transformers\n(Wu & Krahenbuhl, 2021)\nTest set\n0.23\nZero-shot\nLCBM (Ours)\nTest set\n0.14\nZero-shot\nGPT-3.5\nTest set\n0.03\nZero-shot\nVicuna\nComplete dataset\n0.44\nZero-shot\nLCBM (Ours)\nComplete dataset\n0.30\nZero-shot\nGPT-3.5\nComplete dataset\n0.02\nTable 8: Behavior Domain Adaptation. We test the generalization capability of LCBM on two tasks: (1) Be-\nhavior simulation on In-house Email Marketing Data, (2) Behavior simulation on the LVU benchmark. For (1),\nwe train two versions of LCBM with the In-house Email Marketing data: one was trained on YouTube videos\nand further BFT on a few email samples (domain-adapted), and the other was BFT on a larger set of emails, but\nnot including YouTube data (in-domain)\u00a7. We report the RMSE and R2 scores for this task. For (2), we compare\nLCBM with other state-of-the-art results and GPT-3. In (1), we note that the domain-adapted LCBM performs\nbetter than the in-domain LCBM in both settings. We posit that YouTube data helps LCBM understand how a\ncompany\u2019s viewers like to hear from it, giving LCBM an edge over a model trained on a small amount of the\nsame data (600 unique emails). In (2), LCBM performs better than the existing state-of-the-art. Surprisingly,\nGPT-3.5 does better than LCBM on this task. From both (1) and (2), we gather that a model trained on certain\nYouTube behaviors performs better on other behaviors, thus showing promise of domain-adaptation in the be-\nhavior modality. Best models are denoted in green and runner-ups in blue .\nLLM the behavior modality. We call the latter \u201cBehavior Instruction Fine-Tuning (BFT)\u201d inspired\nby instruction fine-tuning (IFT) and its variants like visual instruction tuning (Liu et al., 2023).\nWe prepare the content-behavior instruction datasets as explained next.\nTeaching behavior in the forward direction (predict behavior given content): In this instruction\ntuning task, we teach the model to predict behavior given the message sent by the communica-\ntor. Essentially, this teaches the model to predict behavior in the forward direction (as in Fig. 3).\nConcretely, we include the following information as part of verbalization - image and video embed-\nding converted to the text space (using EvaCLiP (Sun et al., 2023)), scene-by-scene verbalization\ncovering automatic speech recognition, scene captions, video/post caption and description, receiver\nbehavior covering replay rates, views, and likes, and communicator information covering account\nname and follower count. The verbalisation pattern for this task is the same as given in the Listing 6.\nTeaching behavior in the reverse direction (predict content given behavior): This task teaches the\nmodel to learn about behavior in the reverse direction (Fig. 3). Here, the model learns to simulate\ncontent given behavior. The instruction for this task is given in Listing 4.\nUsing the prepared content and behavior instruction datasets consisting of pairs of content and be-\nhavior tokens, we treat the content tokens (XC) as input and behavior tokens (XB, xi \u2208 XB) as\noutput of the language model. We then perform instruction-tuning of the LLM on the prediction\n\u00a7Note that we cannot compare this model with GPT-3 due to the private nature of data.\n10\nPublished as a conference paper at ICLR 2024\nModel\n#Params\nTraining\ntype\nTraining\nTime\nSeparated\nBrand\nSeparated\nGPT-3.5\n175B\nICL\nFew-shot\n58.84\n64.19\nLCBM\n13B\nBFT\nTwitter\n74.3\n97.69\nLCBM\n13B\nBFT\nTwitter and\nYouTube data\n76.87\n92.19\nTable 9: Behavior Simulation and Behavior Domain Adaptation\u2021. Two-way classification accuracies for\nlike prediction on Twitter. Given content, channel, and time, predict behavior (High, Low). We note that\nLCBM trained on Twitter and YouTube performs better than the one trained only on Twitter, showing signs\nof performance improvement by domain adaptation. BFT denotes behavior fine-tuning, and ICL stands for\nin-context learning. The best results over four runs are reported for all models. Best models are denoted in\ngreen and runner-ups in blue .\nModel\nTraining\nTest\nBLEU-1\nBLEU-2\nBLEU-3\nBLEU-4\nROUGE-l\nGPT-3.5\nICL\nBrand Separated\n53.95\n42.36\n31.84\n24.28\n15.24\nTime Separated\n57.69\n45.11\n33.67\n25.52\n15.27\nLCBM\nBFT on\nTwitter\nBrand Separated\n62.29\n46.59\n33.98\n25.64\n14.44\nTime Separated\n70\n54.4\n41.43\n32.48\n17.38\nLCBM\nBFT on Twitter\n+ Youtube\nBrand Separated\n64.28\n48.1\n35.17\n26.63\n14.83\nTime Separated\n70.23\n54.54\n41.52\n32.54\n17.45\nTable 10: Content Simulation and Behavior Domain Adaptation\u2021. Given behavior, channel, time, tweet\nmedia caption as prompt, predict content (tweet text). We note that LCBM trained on Twitter and YouTube\nperforms better than the one trained only on Twitter, showing signs of performance improvement by domain\nadaptation. BFT denotes behavior fine-tuning, and ICL stands for in-context learning. The best results over\nfour runs are reported for all models. Best models are denoted in green and runner-ups in blue .\ntokens, using its original auto-regressive training objective. Specifically, for a sequence of length L,\nwe compute the probability of generating target answers (XB) by:\np(XB|XC) =\nL\nY\ni=1\np\u03b8(xi|XC, XB,<i)\n(1)\nFor the behavior instruction tuning, we keep the visual encoder weights frozen and continue to\nupdate the pre-trained weights of the LLM in LCBM.\n3\nRESULTS AND DISCUSSION\nHere, we discuss the results for the five tasks we discuss in Section 2.3, namely, behavior simulation,\ncontent simulation, behavior understanding, content understanding, and behavior domain adapta-\ntion. We compare the behavior fine-tuned model discussed in \u00a72.4 with state-of-the-art content-only\nmodels like GPT-3.5, GPT-4, and Vicuna-13B. This allows us to compare how much including be-\nhavior tokens in the training of an LLM helps in improving the LLM\u2019s understanding of behavior\nand joint content and behavior spaces while retaining its understanding of the content space.\nThe results for the five tasks are presented in Tables 1,2,5,6,7, 8,9, and 10. We note a few general\ntrends. LCBM, while being 10x smaller than GPT-3.5 and 4, performs better than them on all\nbehavior-related tasks. Further, we see that there is no significant difference between 10-shot and\n2-shot GPT-4 or between GPT-3.5 and GPT-4, indicating that unlike other tasks, it is harder to\nachieve good performance through in-context-learning on the behavior modality. It can be observed\nthat often GPT-3.5 and 4 achieve performance comparable to (or worse than) random baselines.\nInterestingly, the performance of GPTs on the content simulation task is also substantially behind\nLCBM. The way we formulate the content simulation task (Listing 5), it can be seen that a substantial\nperformance could be achieved by strong content knowledge, and behavior brings in little variance.\nWe still see a substantial performance gap between the two models. All of this indicates that large\nmodels like GPT-3.5 and 4 are not trained on behavior tokens.\n\u2021Brand Separated means that the train and test set don\u2019t have any overlap in terms of brands, Time Separated\nmeans that the test set starts after the last tweet in the train set.\n11\nPublished as a conference paper at ICLR 2024\nFor the content understanding tasks (Table 7), predictably GPT-3.5, being the largest model, achieves\nthe best results. However, we see that BFT helps the LLM to learn most content understanding\ntasks better than the base LLM. LCBM gets better results than both Vicuna and VideoChat. This\nindicates that behavior modality might carry additional information about the content, which might\nhelp an LLM understand content better (Khurana et al., 2023a; Klerke et al., 2016; Plank, 2016).\nNext, we see that LCBM also shows signs of domain adaptation in the behavior modality. We see\nthat on five tasks: comment sentiment prediction, comment sentiment reasoning (Table 6), email\nbehavior simulation (Table 8), and Twitter behavior (Table 9) and content simulation (Table 10).\nWe note that if the LCBM is trained on only email behavior simulation samples, it underperforms\nthe model trained on both YouTube data and a few samples to make the model learn email format.\nSimilarly, LCBM trained on both Twitter and YouTube performs better than the one just trained\non Twitter, showing performance improvement by domain adaptation. Finally, Figs. 5,6 show a\nfew samples where we query LCBM to explain replay and comment behavior and compare it with\nhuman explanations. We see that LCBM while verbose, can explain behavior well.\nVideo Title: Robber hangs\nfrom the 23rd-floor window to\nescape the police in China\nPosted by: South China\nMorning Post\nLink: youtu.be/P7iDWZiT3Kg\nComments from Video:\n(not shown to LLMs/LCBM)\nCommenter: I feel bad for the dude. Nobody in\nthe right mind robs a local shop.. To then hang\nout a window. I wonder what he is going through.\nCommenter: Kinda feel sorry for him when he\ncried, he must have been in a bad situation to\ndo what he did but it does not justify his actions.\nCommenter: Aw, i felt his desperation.\nsorry young man but you did wrong. :(\nQuestion: Would the average sentiment of the comments of\nthe audience be positive, neutral, or negative? Explain why.\nLCBM: The average sentiment of the comments of the audience watching\nthe video would be negative. This is because the video shows a man\njumping out of a high-rise building to escape the police. The audience is\nlikely to feel sympathy and sadness towards the man's situation.\nVideo Title: The new Porsche\n718 Spyder. Perfectly\nirrational.\nPosted by: Porsche\nLink: youtu.be/573YaSAP5H0\nComments from Video:\n(not shown to LLMs/LCBM)\nCommenter: Dream car. Mid engine, natural\naspiration, b6, rwd, manual, lightweight, open top\nCommenter: All the makings of a epic sports car \ud83d\udc4c\nCommenter: This car is perfect. I\u2019m in\nlove. New dream car, no questions asked!\nQuestion: Given the video, would the average audience\nsentiment be positive, neutral, or negative? Explain why.\nLCBM: Positive average sentiment of the audience. The video features a\nluxury car driving along a scenic coastal road, which is would evoke a\nsense of awe and admiration in the viewers. Additionally, the Porsche 718\nin the video is an iconic sports car that is widely recognized and respected\nin the automotive community, contributing to the positive sentiment.\nCommenter: I add my vote and wishes to yours ...\na great car I\u2019d love to own and pilot\nVicuna: The audience would likely have a mix of positive and negative\nsentiment towards the video. Viewers may feel sympathetic towards the\nperson trying to escape and praise their bravery.\nGPT-3.5: The video depicts a dangerous and potentially life-threatening\nsituation, which may cause some viewers to feel uneasy or uncomfortable.\nHowever, others may find the events exciting or thrilling to watch. \nVicuna: The average sentiment of the comments would be positive. The\nvideo showcases a beautiful scenic drive through the mountains, which is\nlikely to evoke a sense of awe and enjoyment in the viewers. Additionally,\nthe presence of a luxury car in the video may also contribute to a positive\nsentiment, as it may be seen as a symbol of success and wealth.\nGPT-3.5: The sentiment of the audience watching the video is positive\nbecause the video showcases a new sports car, which is exciting and\ndesirable to car enthusiasts.\nFigure 5: A few examples showing LCBM\u2019s ability to understand and explain human behavior of audience\nsentiment. We also compare it against other models like Vicuna and GPT-3.5.\n4\nCONCLUSION\nIn this paper, we make initial strides towards solving the effectiveness problem proposed by Shan-\nnon in his seminal paper on communication. The effectiveness problem deals with predicting and\noptimizing communication to get the desired receiver behavior. This can be seen as consisting of a\nstring of capabilities: behavior simulation, content simulation, and behavior domain adaptation. We\nshow that while large language models have great generalization capabilities, are unable to perform\nwell on the effectiveness problem. We posit that the reason for this could be a lack of \u201cbehavior\ntokens\u201d in their training corpora. Next, we train LLMs on behavior tokens to show that other than\ncontent understanding tasks, the trained models are now able to have good performance across all\n12\nPublished as a conference paper at ICLR 2024\nFigure 6: A few examples showing LCBM\u2019s ability to understand and explain human behavior of scene re-\nplayability. We compare it against human-provided explanations of the same.\nthe behavior-related tasks as well. We also introduce a new Content Behavior Corpus (CBC) to spur\nresearch on these large content and behavior models (LCBMs).\nACKNOWLEDGEMENTS\nRajiv Ratn Shah is partly supported by the Infosys Center for AI, the Center of Design and New\nMedia, and the Center of Excellence in Healthcare at IIIT Delhi.\nChangyou Chen is partially supported by NSF AI Institute-2229873, NSF RI-2223292, an Amazon\nresearch award, and an Adobe gift fund. Any opinions, findings, conclusions, or recommendations\nexpressed in this material are those of the author(s) and do not necessarily reflect the views of\nthe National Science Foundation, the Institute of Education Sciences, or the U.S. Department of\nEducation.\nREFERENCES\nMax Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder\nfor end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\n1728\u20131738, 2021.\nSteven C Bankes. Agent-based modeling: A revolution? Proceedings of the National Academy of Sciences, 99\n(suppl 3):7199\u20137200, 2002.\nAanisha Bhattacharyya, Yaman K Singla, Balaji Krishnamurthy, Rajiv Ratn Shah, and Changyou Chen. A video\nis worth 4096 tokens: Verbalize videos to understand them in zero shot. In Houda Bouamor, Juan Pino,\nand Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 9822\u20139839, Singapore, December 2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.emnlp-main.608. URL https://aclanthology.org/2023.emnlp-main.608.\nStella Biderman, Kieran Bicheno, and Leo Gao. Datasheet for the pile. arXiv preprint arXiv:2201.07311, 2022.\n13\nPublished as a conference paper at ICLR 2024\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan,\nMohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite\nfor analyzing large language models across training and scaling. In International Conference on Machine\nLearning, pp. 2397\u20132430. PMLR, 2023.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Win-\nter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners, 2020.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-\ntext pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 3558\u20133568, 2021.\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind\nSrinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Ad-\nvances in neural information processing systems, 34:15084\u201315097, 2021.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325,\n2015.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\nURL https://lmsys.org/blog/\n2023-03-30-vicuna/.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nKeyan Ding, Ronggang Wang, and Shiqi Wang. Social media popularity prediction: A multiple feature fu-\nsion approach with deep neural networks. In Proceedings of the 27th ACM International Conference on\nMultimedia, pp. 2682\u20132686, 2019.\nYuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large language\nmodel. arXiv preprint arXiv:2307.08041, 2023.\nZaeem Hussain, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher Thomas, Zuha Agha, Nathan Ong,\nand Adriana Kovashka. Automatic understanding of image and video advertisements, 2017.\nWang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and\nDerek Zhiyuan Cheng. Do llms understand user preferences? evaluating llms on user rating prediction.\narXiv preprint arXiv:2305.06474, 2023.\nAditya Khosla, Atish Das Sarma, and Raffay Hamid. What makes an image popular? In Proceedings of the\n23rd international conference on World wide web, pp. 867\u2013876, 2014.\nVarun Khurana, Yaman Kumar, Nora Hollenstein, Rajesh Kumar, and Balaji Krishnamurthy. Synthesizing\nhuman gaze feedback for improved NLP performance.\nIn Proceedings of the 17th Conference of the\nEuropean Chapter of the Association for Computational Linguistics, pp. 1895\u20131908, Dubrovnik, Croa-\ntia, May 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.139. URL\nhttps://aclanthology.org/2023.eacl-main.139.\nVarun Khurana, Yaman K Singla, Jayakumar Subramanian, Rajiv Ratn Shah, Changyou Chen, Zhiqiang Xu,\nand Balaji Krishnamurthy. Behavior optimized image generation. arXiv preprint arXiv:2311.10995, 2023b.\nSigrid Klerke, Yoav Goldberg, and Anders S\u00f8gaard. Improving sentence compression by learning to predict\ngaze. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, pp. 1528\u20131533, San Diego, California, June 2016.\nAssociation for Computational Linguistics. doi: 10.18653/v1/N16-1179. URL https://aclanthology.\norg/N16-1179.\nBryan Klimt and Yiming Yang. The enron corpus: A new dataset for email classification research. In European\nconference on machine learning, pp. 217\u2013226. Springer, 2004.\nRanjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in\nvideos. In Proceedings of the IEEE international conference on computer vision, pp. 706\u2013715, 2017.\n14\nPublished as a conference paper at ICLR 2024\nYaman Kumar, Rajat Jha, Arunim Gupta, Milan Aggarwal, Aditya Garg, Tushar Malyan, Ayush Bhardwaj,\nRajiv Ratn Shah, Balaji Krishnamurthy, and Changyou Chen. Persuasion strategies in advertisements. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 57\u201366, 2023.\nHarold D Lasswell. The structure and function of communication in society. The communication of ideas, 37\n(1):136\u2013139, 1948.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models, 2023a.\nKunchang Li, Yali Wang, Gao Peng, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified\ntransformer for efficient spatial-temporal representation learning. In International Conference on Learning\nRepresentations, 2021.\nKunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nVideochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023b.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning.\narXiv preprint\narXiv:2304.08485, 2023.\nMasoud Mazloom, Robert Rietveld, Stevan Rudinac, Marcel Worring, and Willemijn Van Dolen. Multimodal\npopularity prediction of brand-related social media posts. In Proceedings of the 24th ACM international\nconference on Multimedia, pp. 197\u2013201, 2016.\nDenis McQuail and Sven Windahl. Communication models for the study of mass communications. Routledge,\n2015.\nGerald R Miller. On defining communication: Another stab. Journal of Communication, 1966.\nOpenAI. Gpt-4 technical report, 2023.\nCharles Egerton Osgood, George J Suci, and Percy H Tannenbaum. The measurement of meaning. Number 47.\nUniversity of Illinois press, 1957.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\nJoon Sung Park, Joseph C O\u2019Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bern-\nstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon\nllm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116,\n2023.\nBarbara Plank. Keystroke dynamics as signal for shallow syntactic parsing. In Proceedings of COLING 2016,\nthe 26th International Conference on Computational Linguistics: Technical Papers, pp. 609\u2013619, 2016.\nRui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui.\nSpatiotemporal contrastive video representation learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 6964\u20136974, 2021.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust\nspeech recognition via large-scale weak supervision. In International Conference on Machine Learning, pp.\n28492\u201328518. PMLR, 2023.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The\nJournal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nElena Romero, Manuel Chica, Sergio Damas, and William Rand. Two decades of agent-based modeling in\nmarketing: a bibliometric analysis. Progress in Artificial Intelligence, pp. 1\u201317, 2023.\nClaude E. Shannon and Warren Weaver. The mathematical theory of communication. The mathematical theory\nof communication. University of Illinois Press, Champaign, IL, US, 1949.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hyper-\nnymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2556\u20132565, 2018.\n15\nPublished as a conference paper at ICLR 2024\nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for\nvideo and language representation learning. In Proceedings of the IEEE/CVF international conference on\ncomputer vision, pp. 7464\u20137473, 2019.\nQuan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for\nclip at scale. arXiv preprint arXiv:2303.15389, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\nFrancisco Villarroel Ordenes, Dhruv Grewal, Stephan Ludwig, Ko De Ruyter, Dominik Mahr, and Martin\nWetzels. Cutting through content clutter: How speech and image acts drive consumer sharing of social\nmedia brand messages. Journal of Consumer Research, 45(5):988\u20131012, 2019.\nKe Wang, Mohit Bansal, and Jan-Michael Frahm. Retweet wars: Tweet popularity prediction via dynamic\nmultimodal regression. In 2018 IEEE winter conference on applications of computer vision (WACV), pp.\n1842\u20131851. IEEE, 2018.\nChao-Yuan Wu and Philipp Krahenbuhl.\nTowards long-form video understanding.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1884\u20131894, 2021.\nFanyi Xiao, Kaustav Kundu, Joseph Tighe, and Davide Modolo. Hierarchical self-supervised representation\nlearning for movie understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 9727\u20139736, 2022.\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li.\nLarge language models are\nversatile decomposers: Decompose evidence and questions for table-based reasoning.\narXiv preprint\narXiv:2301.13808, 2023.\nHang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for\nvideo understanding. arXiv preprint arXiv:2306.02858, 2023.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\n16\nPublished as a conference paper at ICLR 2024\nA\nAPPENDIX\nA.1\nVERBALIZATION LISTINGS\nListing 2: Verbalization pattern of videos for the behavior understanding task:\nInput : <video> .. </video>\nThe video has the following scenes:\nScene 1: {ASR: Welcome to a quick tutorial , OCR: Adobe Premiere Pro, Captions: A desktop interface , Replays: 60},\nScene 2: {ASR: on using Premiere Pro to edit , Captions: A computer interface , with an image of a white horse. Objects \u2212 Horse,\nGrass, Fence., Replays: 53},\n...\nIt was posted on Adobe's YouTube channel with the\ntitle\n'Using Premiere Pro like a Pro' on Aug 15 2022. Adobe's YouTube\nchannel has 100k subscribers . This video was viewed by 346 thousand people and liked (as a percentage of likes /views) by\n2.3%\nOutput: The scene shows the transformation of the image after the changes.\nListing 3: Verbalization pattern of emails for the behavior domain adapation task. The email content\nand CTR is for demonstration purposes only.\nInput : Email with Subject : Lock it down before you send it out.\nHeader: Nobody messes with your PDFs.\nBody text : Add password protection , secure encryption , and restricted\nediting to your PDFs with Adobe Acrobat Pro DC. Share\nonly what you want and nothing more. A button that says 'Get started '. An image of a laptop , with window open on it .\nImage text : \u201d Protect using password\u201d.\nForeground colors : grey, blue. Background colors: lavender , white. Image Emotions: security , serious . Image keywords: laptop ,\nprotect , password, lock. Aesthetic value: low. Clutter\nlevel : medium. The email is created by a Creative\nProfessional ,\nfor the product Adobe Acrobat Pro. It is sent to users in the United States , in the commercial market.\nSpecifically , it\nis sent to Power users with the intent of Active Use.\nThe email was sent 109 times between 25 August, 2022 and 26 August, 2022, and had a click through rate of [MASK]%\nOutput: 0.037%\nListing 4: Verbalization pattern to teach behavior in the reverse direction (predicting content given\nbehavior):\nInput : <video> .. </video> The video has the following scenes: Scene 1: {ASR: [MASK], Replays: 60%\n...\nScene 5: {ASR: has never been, Captions: Colour Pallete , Replays: 47%\nScene 6: {ASR: been easier , Captions: Colour Pallete , Replays: 54%\n...\nIt was posted on Adobe's YouTube channel with the\ntitle\n'Using Premiere Pro like a Pro' on Aug 15 2022. It is viewed 203k\ntimes and liked 1.2%\nOutput: Scene 1:{ASR: Welcome to a quick tutorial .}\nListing 5: Verbalization pattern of videos for the content simulation task:\nInput : <video> .. </video> The video has the following scenes: Scene 1: {ASR: [MASK], Replays: 60%\n...\nScene 5: {ASR: has never been, Captions: Colour Pallete , Replays: 47%\nScene 6: {ASR: been easier , Captions: Colour Pallete , Replays: 54%\n...\nIt was posted on Adobe's YouTube channel with the\ntitle\n'Using Premiere Pro like a Pro' on Aug 15 2022. It is viewed 203k\ntimes and liked 1.2%\nOption\u22121: Welcome to a quick\ntutorial ,\nOption\u22122: Samsung Galaxy A20 smartphone,\n...\nOption\u221225: regulations . We haven't had.\nListing 6: Verbalization pattern of videos for the behavior simulation task:\nInput : <video> .. </video> The video has the following scenes:\nScene 1: {ASR: Welcome to a quick tutorial , OCR: Adobe Premiere Pro, Captions: A desktop interface , Replays: [MASK]},\nScene 2: {ASR: on using Premiere Pro to edit , Captions: A computer interface , with an image of a white horse. Objects \u2212 Horse,\nGrass, Fence., Replays: [MASK] }, ...\nIt was posted on Adobe's YouTube channel with the\ntitle\n'Using Premiere Pro like a Pro' on Aug 15 2022. Adobe's YouTube\nchannel has 100k subscribers .\nCan you tell the replay values for scenes 2 to 5. How many times will this video be\nviewed and liked as a percentage of likes /views?\nOutput: Scene 1: {Replay: 60%\nListing 7: Verbalization pattern of Twitter posts for the behavior simulation task:\nInput : Given a tweet of pfizer posted by the account PfizerMed on 2023\u221201\u221212. Tweet : Announcing a new ASGCT\u2212Pfizer grant to\nsupport independent medical education\ninitiatives\non genetic medicines. For details , click Request for Proposals . <\nhyperlink>. Apply by January 30, 2022 # raredisease #ASGCT #GeneTherapy <hyperlink>. Verbalisation of media content : \\\u201d\ncaption\\\u201d: \\\u201dA close\u2212up of a DNA double helix, showcasing its\nstructure and blue color\\\u201d,\\\u201dkeywords\\\u201d: \\\u201dDNA, double\nhelix, structure , blue, close\u2212up, molecular biology, genetics , biology,\nscientific\nillustration \\\u201d}. Predict whether it\nwill recieve high or low likes ?\u201d,\n17\nPublished as a conference paper at ICLR 2024\nOutput: This tweet has low likes .\nListing 8: Verbalization pattern of Twitter posts for the content simulation task:\nInput : Generate a tweet given the media verbalization and the likes\nit got. Tweet is for pfizer to be posted by the account\nPfizerMed on 2023\u221201\u221212. Verbalisation of media content : \\\u201dcaption\\\u201d: \\\u201dA close\u2212up of a DNA double helix, showcasing\nits\nstructure and blue color\\\u201d,\\\u201dkeywords\\\u201d: \\\u201dDNA, double helix, structure , blue, close\u2212up, molecular biology,\ngenetics , biology,\nscientific\nillustration \\\u201d}. This tweet has low likes .\u201d\nOutput: \u201dTweet : Announcing a new ASGCT\u2212Pfizer grant to support independent medical education\ninitiatives\non genetic medicines\n. For details , click Request for Proposals . <hyperlink>. Apply by January 30, 2022 # raredisease #ASGCT #GeneTherapy\n<hyperlink>\u201d}\nA.2\nOTHER RELATED WORK\nModels of Human Communication: Communication is the situation in which a source transmits\na message to a receiver with conscious intent to affect the latter\u2019s behaviors (Osgood et al., 1957;\nMiller, 1966). Thus, in the most general terms, communication implies a sender, a channel, a mes-\nsage, a receiver, a relationship between sender and receiver, an effect, a context in which communi-\ncation occurs and a range of things to which \u2019messages\u2019 refer (McQuail & Windahl, 2015; Lasswell,\n1948). As per this, all of the content produced by humanity is essentially communication from a\nsender to a receiver over some channel and with some effect. Despite much research on commu-\nnication in social sciences since the 1900s, there has been little adoption of it in machine learning\nmodeling. A prime artefact of this is that the biggest models in machine learning (LLMs) are trained\nonly on content (messages) and ignore other factors in communication (the intended receiver, chan-\nnel, and behavior) even when they are available.\nPrior Efforts To Model Behavior: While there has been much research in ML to model human\nbehavior, it has been disconnected from language and, sometimes, real-world data. For instance,\nAgent-based modeling (ABMs), a popular paradigm in Reinforcement Learning, has been employed\nto model behavior (Bankes, 2002; Romero et al., 2023; Park et al., 2023). Nevertheless, ABMs tend\nto view humans as rational economic agents who communicate primarily through their actions, ne-\nglecting the significance of content in communication. In ABMs, agents strive to maximize their\nrewards, whereas communication does not always aim to optimize specific, well-defined reward\nsignals. Moreover, the scarcity of large repositories containing extensive records of human actions\nposes a challenge when training ABMs to learn human behavior. Consequently, existing large mod-\nels trained on human behavior, such as the ABMs and decision transformer and its variants, often\nrely on simulated data, such as game environments, rather than real human behavior (Chen et al.,\n2021). This reliance on artificially generated data introduces biases inherent to the creators of the\ntraining data, making it difficult to capture authentic human behavior. However, recent advance-\nments have demonstrated the potential of large models trained on real-world tokens encompassing\nvarious modalities, like images, videos, audio, and text, as the basis for diverse tasks (Ge et al.,\n2023; Li et al., 2023a). Notably, LLMs, as exemplars of foundation models, have exhibited impres-\nsive performance across a range of tasks, including those they were not explicitly trained for, such\nas emotion recognition, named entity recognition, and complex tasks like table understanding (Ye\net al., 2023; Bhattacharyya et al., 2023).\nFurther, there has also been much work in modeling behavior using conventional modeling tech-\nniques, such as regression, bagging and boosting (Mazloom et al., 2016; Villarroel Ordenes et al.,\n2019), neural networks (Ding et al., 2019; Wang et al., 2018; Khosla et al., 2014), and transformers\n(Wu & Krahenbuhl, 2021; Xiao et al., 2022). While these models can certainly model behavior,\nLLMs show generalization powers which extend to capabilities much beyond just behavior simu-\nlation. For instance, once trained on behavior tokens, other than behavior simulation, LLMs can\nnow generate behavior optimized content (Table 5), explain behavior (Table 6), and domain-adapt\nto other behaviors (Table 8), none of which are shown by other models. The other concurrent works\nwhich model behavior using LLMs (Kang et al., 2023) model just behavior (for example, by CTR\nprediction) by attaching classification or regression heads to LLMs and thereby lose out on the\ntext-to-text paradigm where LLMs show their best performance and generalization capabilities. In\naddition, similar to non LLM paradigm, this method loses out on other capabilities like generating\nbehavior optimized content and explaining behavior.\n18\n"
  },
  {
    "title": "VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation",
    "link": "https://arxiv.org/pdf/2309.00398.pdf",
    "upvote": "18",
    "text": "VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition\nText-to-Video Generation\nXin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu, Qi Zhang,\nFu Li, Haocheng Feng, Errui Ding, Jingdong Wang\nDepartment of Computer Vision Technology (VIS), Baidu Inc.\nlixin41@baidu.com\nAbstract\nIn this paper, we present VideoGen, a text-to-video gen-\neration approach, which can generate a high-definition\nvideo with high frame fidelity and strong temporal consis-\ntency using reference-guided latent diffusion. We leverage\nan off-the-shelf text-to-image generation model, e.g., Sta-\nble Diffusion, to generate an image with high content qual-\nity from the text prompt, as a reference image to guide\nvideo generation.\nThen, we introduce an efficient cas-\ncaded latent diffusion module conditioned on both the ref-\nerence image and the text prompt, for generating latent\nvideo representations, followed by a flow-based temporal\nupsampling step to improve the temporal resolution. Fi-\nnally, we map latent video representations into a high-\ndefinition video through an enhanced video decoder. Dur-\ning training, we use the first frame of a ground-truth video\nas the reference image for training the cascaded latent dif-\nfusion module. The main characterises of our approach in-\nclude: the reference image generated by the text-to-image\nmodel improves the visual fidelity; using it as the con-\ndition makes the diffusion model focus more on learn-\ning the video dynamics; and the video decoder is trained\nover unlabeled video data, thus benefiting from high-quality\neasily-available videos. VideoGen sets a new state-of-the-\nart in text-to-video generation in terms of both qualitative\nand quantitative evaluation. See https://videogen.\ngithub.io/VideoGen/ for more samples.\n1. Introduction\nThere have been great progress in text-to-image (T2I)\ngeneration systems, such as DALL-E2 [12], Imagen [42],\nCogview [10], Latent Diffusion [40], and so on. In contrast,\ntext-to-video (T2V) generation, creating videos from text\ndescription, is still a challenging task as it requires not only\nhigh-quality visual content, but also temporally-smooth and\nrealistic motion that matches the text. Moreover, it is hard\nto find large-scale datasets of text-video pairs.\nIn addition to extending the T2I network architecture,\nseveral recent T2V techniques explore the trained T2I\nmodel for improving the visual fidelity, e.g., utilizing the\nT2I model weights, or exploring image-text data. For ex-\nample, CogVideo [23] and Make-A-Video [46] make use\nof the T2I model, by freezing or fine-tuning the T2I\nmodel weights. N \u00a8UWA [59] and Imagen Video [19] instead\nexplore image-text pairs to improve T2V model training,\nthrough pre-training or joint-training.\nIn this paper, we propose VideoGen for generating a\nhigh-quality and temporally-smooth video from a text de-\nscription. We leverage a T2I model to generate a high-\nquality image, which is used as a reference to guide T2V\ngeneration. Then, we adopt a cascaded latent video diffu-\nsion module, conditioned on the reference image and the\ntext description, to generate a sequence of high-resolution\nsmooth latent representations. We optionally use a flow-\nbased scheme to temporally upsample the latent representa-\ntion sequence. Finally, we learn a video decoder to map the\nlatent representation sequence to a video.\nThe benefits of using a T2I model to generate a refer-\nence image lie in two-fold. On the one hand, the visual\nfidelity of the generated video is increased. This benefits\nfrom that our approach makes use of the large dataset of\nimage-text pairs, which is richer and more diverse than the\ndataset of video-text pairs, through using the T2I model.\nThis is more training-efficient compared to Imagen Video\nthat needs to use the image-text pairs for joint training. On\nthe other hand, using the reference image to guide the cas-\ncaded latent video diffusion model frees the diffusion model\nfrom learning visual content, and makes it focus more on\nlearning the video dynamics. We believe that this is an extra\nadvantage compared to the methods merely using the T2I\nmodel parameters [23, 46].\nFurthermore, our video decoder only needs the latent\nrepresentation sequence as input to generate a video, with-\nout requiring the text description. This enables us to train\nthe video decoder over a larger set of easily-available un-\narXiv:2309.00398v2  [cs.CV]  7 Sep 2023\nA burning match\nA shark swimming in clear ocean.\nPour water into a glass cup\nA penguin walking on the beach\nMelting ice cream dripping down the cone. \nFigure 1. T2V generation examples of VideoGen. Our generated videos have rich texture details and stable temporal consistency. It is\nstrongly recommended to zoom in to see more details.\nlabeled (unpaired) videos other than only video-text pairs.\nAs a result, our approach benefits from high-quality video\ndata, improving motion smoothness and motion realism of\nthe generated video. Our key contributions are as follows:\n\u2022 We leverage an off-the-shelf T2I model to generate an\nimage from text description as a reference image, for\nimproving frame content quality.\n\u2022 We present an efficient and effective cascaded latent\nvideo diffusion model conditioned on the text descrip-\ntion, as well as the reference image as the condition\nwhich makes the diffusion model focus more on learn-\ning the video motion.\n\u2022 We are able to train the video decoder using easily-\navailable unlabeled (unpaired) high-quality video data,\nwhich boosts visual fidelity and motion consistency of\nthe generated video.\n\u2022 We evaluate VideoGen against representative T2V\nmethods and present state-of-the-art results in terms of\nquantitative and qualitative measures.\n2. Related Work\nDiffusion models. The generative technology has expe-\nrienced rapid development, from the generative adversar-\nial networks [17] in the past few years to the very pop-\nular diffusion models recently. Diffusion models [47, 20]\nhave shown surprising potential and made great progress in\ngenerative tasks, such as text-to-speech [6, 7, 26], text-to-\nimage [42, 37, 35, 40, 32, 2, 14, 5], text-to-3D [36, 57],\ntext-to-video [22, 46, 18, 69, 19, 60, 23], image2image\n[43, 4, 56, 68, 41, 3] and vid2vid [12, 3]. Especially in\nthe generation of images, such as Stable Diffusion [40], has\nreached the level of professional illustrators, which greatly\nimproves the work efficiency of artists.\nText-to-image generation. The past years have witnessed\ntremendous progress in image-to-text generation. The early\nsystems are mainly based on GAN [17], e.g., StyleCLIP\n[34], StyleGAN-NADA [15], VQGAN-CLIP [9], StyleT2I\n[29]. The most recent success is from the development of\ndenoising diffusion model [20] and its efficient extension,\nlatent diffusion model [40]. Examples include: DALL-E\n[38], DALL-E2 [37], Imagen [42], Stable Diffusion [40],\nCogView [10], Parti [64], GLIDE [32].\nOur approach takes advantages of latent diffusion model\n[40] for text-to-video generation. This not only improves\nthe diffusion sampling efficiency, but also allows to design\nthe video decoder that only relies on videos, not on texts, al-\nlowing that the video decoder can be trained on high-quality\nunlabeled videos.\nText-to-video generation. Early text-to-video techniques\ninclude:\nleveraging a VAE with recurrent attention,\ne.g.,Sync-DRAW [30], and extending GAN from image\ngeneration to video generation [33, 28].\nOther develop-\nments include GODIVA [58], N \u00a8UWA [59], CogVideo [23].\nMore recent approaches include: Tune-A-Video [60] and\nDreamix [31] for applications with fine-tuning, Make-A-\nVideo [46], MagicVideo [69], Video Diffusion Model [22]\nand Imagen Video [19], latent video diffusion models [18],\nwhich extend diffusion models from image generation to\nvideo generation,\nOur approach differs from previous works in several as-\npects. First, our approach leverages the pretrained text-to-\nimage generation model to generate a high-quality image\nfor guiding video generation, leading to high visual fidelity\nof the generated video. This is clearly different from pre-\nvious approaches. In Make-A-Video [46], an image is used\nto generate an embedding to replace the text embedding for\nimage animation. In contrast, our approach uses an image as\nreference to guide video content generation. What\u2019s more,\nthe image in Make-A-Video is mapped to an embedding\nthrough CLIP image encoder, that is mainly about seman-\ntic. In contrast, our approach uses the encoder trained with\nauto-encoder, and the output latent contains both semantics\nand details for reconstruction. This is why the results of\nMake-A-Video are more blurry. Second, we adopt latent\nvideo diffusion model, leading to more efficient diffusion\nsampling in comparison to Make-A-Video [46] and Imagen\nVideo [19]. Reference-guidance for latent video diffusion\nmodel makes our approach differ from [18] that only con-\nducts the study on a small dataset. Last, our design allows\nus to train the video decoder using high-quality unpaired\nvideos.\n3. Approach\nOur approach VideoGen receives a text description, and\ngenerates a video. The inference pipeline is depicted in Fig-\nure 2. We generate a reference image from a pretrained and\nfrozen Text-to-Image generation model. We then compute\nthe embeddings of the input text and the reference image\nfrom pretrained and frozen text and image encoders. We\nsend the two embeddings as the conditions for reference-\nguided latent video diffusion for generating latent video\nrepresentation, followed by a flow-based temporal super-\nresolution module. Finally, we map the latent video repre-\nsentation to a video through a video decoder.\n3.1. Reference Image Generation\nWe leverage an off-the-shelf text-to-image (T2I) gener-\nation model, which is trained over a large set of image-text\npairs and can generate high-quality image. In our imple-\nmentation, we adopt the SOTA model, Stable Diffusion1\nwithout any processing. We feed the text prompt into the\nT2I model. The resulting high-fidelity image is used as\na reference image, and plays a critical role for effectively\nguiding subsequent latent representation sequence genera-\ntion. During the training, we simply pick the first frame of\nthe video as the reference, which empirically works well.\n3.2. Reference-Guided Latent Video Diffusion\nCascaded latent video diffusion consists of three con-\nsecutive components: a latent video representation diffu-\nsion network, generating representations of spatial resolu-\ntion 16 \u00d7 16 and temporal resolution 16, and two spatially\nsuper-resolution diffusion networks, raising the spatial res-\nolutions to 32 \u00d7 32 and 64 \u00d7 64.\nArchitecture. We extend the 2D latent diffusion model [40]\nto the 3D latent diffusion model through taking into consid-\neration the temporal dimension. We make two main modi-\nfications over the key building block that now supports both\nspatial and temporal dimensions.\nFollowing Make-A-Video [46], we simply stack a 1D\ntemporal convolution following each 2D spatial convolu-\n1https://github.com/CompVis/stable-diffusion\nText Encoder\nText\nText-to-Image\nImage Encoder\nReference-guided Latent\nVideo Di\ufb00usion\nFlow-based temporal\nSuper-resolution\nVideo Decoder\nFigure 2. The VideoGen inference pipeline. The input text is fed into a pretrained Text-to-Image generation model, generating a reference\nimage. The reference image and the input text are sent to a pretrained Image Encoder and a pretained Text Encoder. The output text and\nimage embeddings are used as the conditions of Reference-guided Latent Video Diffusion, outputting the latent video representation. Then\nFlow-based temporal Super-resolution increases the temporal resolution, and is followed by Video Decoder, generating the final video.\nDuring the training process, the reference image is the first frame of the video.\ntional layer in the network. The 2D spatial convolution is\nconducted for each frame separately, e.g., 16 frames in our\nimplementation. Similarly, the 1D temporal convolution is\nconducted for each spatial position separately, e.g., 16\u00d716,\n32\u00d732, and 64\u00d764 for the three diffusion networks. Similar\nto Make-A-Video [46]. such a modification to the building\nblock enables us to use the pretrained T2I model parame-\nters to initialize the 2D convolutions. Similarly, we stack a\ntemporal attention following each spatial attention.\nCondition injection. We follow the scheme in LDM [40]\nto inject the text embedding into the network using cross-\nattention. We project the text description into an intermedi-\nate representation through a pretrained text encoder, CLIP\ntext encoder in our implementation. The intermediate repre-\nsentation is then mapped into each diffusion network using\na cross-attention layer.\nThe later diffusion network uses the bilinear 2\u00d7 upsam-\npled representation output from the last diffusion network\nas an extra condition and concatenates it into the input. We\nfollow Make-A-Video [46] to use FPS as a condition and\ninject its embedding into each diffusion model.\nWe project the reference image to a representation\nthrough a pretrained image encoder. In our implementa-\ntion, we use the image encoder of the auto-encoder in Sta-\nble Diffusion, and process the image with three resolutions\n(16 \u00d7 16, 32 \u00d7 32, and 64 \u00d7 64), each corresponding to\na diffusion network. We inject the representation of the\nreference image into the network by concatenating it with\nthe first-frame representation of the input of the diffusion\nmodel, and concatenating zero representations with the rep-\nresentations corresponding to other frames.\n3.3. Flow-based Temporal Super-resolution\nWe perform temporal super-resolution in the latent rep-\nresentation space. We estimate the motion flow according\nto the representations using a latent motion flow network.\nThen we warp the representations according to the esti-\nmated motion flow, and obtain a coarse longer video repre-\nsentations with 2\u00d7 upsampling. We next send each warped\nrepresentation to a denoising diffusion network as a condi-\ntion to get a refined representation. The final warp repre-\nsentation is a combination of the low-frequency component\nof the warped coarse representation and the high-frequency\ncomponent of the refined representation. Consistent to the\nobservation [8], our experiments find that the combined rep-\nresentations lead to more stable video generation. We per-\nform this process three times and get 8\u00d7 upsampled video\nrepresentations.\n3.4. Video Decoder\nThe video decoder maps the video from the latent repre-\nsentation space to pixel space. We modify the Stable Diffu-\nsion 8\u00d7 upsampling image decoder for the video decoder.\nWe stack a 1D temporal convolution following each 2D con-\nvolution and a temporal attention following each spatial at-\ntention. This modification also allows us to initialize the\nparameters of 2D convolutions and spatial attentions in the\nvideo decoder using the parameters of the pretrained image\ndecoder.\n3.5. Training\nOur approach leverages existing models, e.g., CLIP text\nencoder for text description encoding, Stable Diffusion T2I\ngeneration model for reference image generation, Stable\nDiffusion image encoder for reference image encoding. In\nour implementation, we freeze the three models without re-\ntraining. The other three modules are independently trained\nfrom the video data with the help of pretrained image mod-\nels. The details are as follows.\nReference-guided cascaded latent video diffusion. We\ncompute the video representations by sending each frame\ninto the image encoder as the denoising diffusion target.\nAt each stage, the video spatial resolution is processed to\nmatch the spatial resolution of the latent representations.\nWe simply pick the first frame in the video as the reference\nimage for training.\nThe 2D convolution and spatial attention parameters of\nthe first diffusion network are initialized from the pretrained\nStable Diffusion T2I generation model. The temporal con-\nvolution and attention layers are initialized as the identity\nfunction. The second (third) diffusion network is initial-\nized as the weights of the trained first (second) diffusion\nFigure 3. For a text prompt, different reference images generate different videos.\nnetwork. The three diffusion networks are only the compo-\nnents receiving video-text pairs, WebVid-10M [1], for train-\ning.\nFlow-based temporal super-resolution. We estimate the\nmotion flow by extending IFRNet [25] from the pixel space\nto the latent representation space. We slightly modify the\nIFRNet architecture and simply change the first layer for\nprocessing latent representations. The ground-truth motion\nflow in the latent representation space is computed as: com-\npute the motion flow in the pixel space using the pretrained\nIFRNet and resize the motion flow to the spatial size of the\nlatent representation space.\nThe input representations of the flow-based tempo-\nral super-resolution part are directly computed from low\ntemporal-resolution video. The ground-truth target repre-\nsentations of the denoising diffusion network for warped\nrepresentation refinement are constructed by feeding the\nframes of high FPS video into the image encoder.\nVideo decoder. The 2D convolution and spatial attention\nweights are initialized from the pretrained Stable Diffusion\nimage decoder, and the temporal convolution and attention\nare initialized as the identify function. During the train-\ning, we use the image encoder in StableDiffusion to ex-\ntract video latent representations. We apply degradations\n(adding noise, blurring, and compression), which are intro-\nduced in BSRGAN [66], to the video, and extract the latent\nrepresentations. The target video is still the original video,\nand without any processing. Video decoder and flow-based\ntemporal super-resolution network are trained on unpaired\nvideos with 40K clips of 100 frames that are collected from\nYouTube.\n4. Experiments\n4.1. Datasets and Metrics\nWe adopt the publicly available dataset of video-text\npairs from WebVid-10M [1] for training the reference-\nguided cascaded latent video diffusion network. We col-\nlected over 2, 000 4K-resolution videos of 60 FPS from\nYouTube and extracted 40000 clips for training the flow-\nbased temporal super-resolution network, and the video de-\ncoder. Our other basic settings follow the open-sourced Sta-\nble Diffusion code 2 and remain unchanged. All our exper-\niments are conducted on 64 A100-80G GPUs.\nWe evaluate our VideoGen on UCF-101 [49] and MSR-\nVTT [62]. For MSR-VTT, we use all 59, 800 captions from\nthe test set to calculate CLIPSIM [58] (average CLIP sim-\nilarity between video frames and text) following [46, 59].\nUCF-101 contains 13,320 video clips from 101 categories\nthat can be grouped into body movement, human-human\ninteraction, human-object interaction, playing musical in-\nstruments, and sports. For UCF-101, we follow Make-A-\nVideo [46] and construct the prompt text for each class.\nFollowing previous methods [46, 22, 23], we report\ncommonly-used Inception Score (IS) [44] and Frechet\nVideo Distance (FVD) [54] [54] as the evaluation metrics\non UCF-101.\nDuring the evaluation, we only generated\n2https://github.com/CompVis/stable-diffusion\nFigure 4. Qualitative comparison with Make-A-Video and Imagen Video. Compared with Make-A-Video, the lake ripples, boats and trees\nin our video are clearer. Similarly, although the video resolution of Imagen Video reaches 1280\u00d7768, the frames are very blurry compared\nwith our result. The watermark in the last row is because the videos in the training set WebVid-10M contain the \u201dshutterstock\u201d watermark.\nTable 1. T2V results on UCF-101. We report the performance for zero-shot and fine-tuning settings.\nMethod\nPretrain\nClass\nResolution\nIS \u2191\nFVD \u2193\nZero-Shot Setting\nCogVideo (Chinese)\nYes\nYes\n480\u00d7480\n23.55\n751.34\nCogVideo (English)\nYes\nYes\n480\u00d7480\n25.27\n701.59\nMake-A-Video\nYes\nYes\n256\u00d7256\n33.00\n367.23\nOurs\nYes\nYes\n256\u00d7256\n71.61 \u00b1 0.24\n554 \u00b1 23\nFine-tuning Setting\nTGANv2\nNo\nNo\n128\u00d7128\n26.60 \u00b1 0.47\n-\nDIGAN\nNo\nNo\n-\n32.70 \u00b1 0.35\n577 \u00b1 22\nMoCoGAN-HD\nNo\nNo\n256\u00d7256\n33.95 \u00b1 0.25\n700 \u00b1 24\nCogVideo\nYes\nYes\n160\u00d7160\n50.46\n626\nVDM\nNo\nNo\n64\u00d764\n57.80 \u00b1 1.3\n-\nLVDM\nNo\nNo\n256\u00d7256\n-\n372 \u00b1 11\nTATS-base\nYes\nYes\n128\u00d7128\n79.28 \u00b1 0.38\n278 \u00b1 11\nMake-A-Video\nYes\nYes\n256\u00d7256\n82.55\n81.25\nOurs\nYes\nYes\n256\u00d7256\n82.78 \u00b1 0.34\n345 \u00b1 15\n16\u00d7256\u00d7256 videos, because the C3D model [53] for IS\nand FVD, and the clip image encoder 3 for CLIPSIM do not\nexpect higher resolution and frame rate.\n3https://github.com/openai/CLIP\n4.2. Results\nQuantitative evaluation. We compare our VideoGen with\nsome recent text-to-video generation methods, including\nMake-A-Video [46], CogVideo [23], VDM [22], LVDM\nTable 2. T2V results on MSR-VTT. We report average CLIPSIM\nscores to evaluate the text-video alignment.\nMethod\nZero-Shot\nResolution\nCLIPSIM \u2191\nGODIVA\nNo\n128\u00d7128\n0.2402\nN\u00a8uwa\nNo\n336\u00d7336\n0.2439\nCogVideo (Chinese)\nYes\n480\u00d7480\n0.2614\nCogVideo (English)\nYes\n480\u00d7480\n0.2631\nMake-A-Video\nYes\n256\u00d7256\n0.3049\nOurs\nYes\n256\u00d7256\n0.3127\n[18], TATS [16], MagicVideo [69], DIGAN [65] and N\u00a8uwa\n[59], etc.\nBecause ImagenVideo [19] has neither open\nsource nor public datasets results, we have only made a\nqualitative comparison with it. The results on MSR-VTT\nare given in Table 2. We can see that our VideoGen achieves\nthe highest average CLIPSIM score without any fine-tuning\non MSR-VTT, proving that the generated videos and texts\nhave good content consistency.\nThe results on UCF-101 given in Table 1 show that in\nthe cases of both the zero-shot and finetuning settings, the\nIS score of VideoGen performs the best. In the zero-shot\nsetting, the IS score is greatly improved compared to the\nsecond best, from 33 to 71.6. The IS index measures the\nquality and category diversity of generated video and the\nhigh IS index indicates that the video quality and category\ndiversity of our generated videos are excellent.\nA lightning strike at night.\nw/ reference\nw/o reference\nFigure 5. Visual comparison without and with the use of reference\nimage. As we can see, the frames with reference-guided have more\ntexture details in dark cloud and grass areas. Please zoom in to see\nmore details.\nThe key reason for better results from our approach is\nthat we generate a high-quality reference image using a\nwell-trained T2I generation model, and accordingly the\nquality of generated video content is improved.\nWe also report the results in terms of FVD that measures\nthe gap between the distribution of real videos and gen-\nerated videos. Our approach performs the second best in\nthe zero-shot setting. The most possible reason is that our\ntraining data distributes more differently from the UCF-101\ndataset than the training data used by Make-A-Video. In\nthe fine-tuning setting, we do not fine-tune the text-to-image\ngeneration model, the flow-based temporal super-resolution\nmodel, and the video decoder, and only fine-tunes the first\nlatent video diffusion model. We guess that our FVD score\nwould be better if we fine-tune the text-to-image model for\ngenerating a reference image whose content matches the\ndistribution of UCF-101. The fine-tuning setting is not our\ncurrent focus, and our current goal is general T2V genera-\ntion.\nQualitative evaluation. In Figure 1, we show some exam-\nples generated from our VideoGen. Our results show rich\nand clear texture details, and excellent temporal stability\nand motion consistency. In Figure 4, we make a visual com-\nparison with the two recent T2V methods, Imagen Video\n[19] and Make-A-Video [46]. It can be seen that although\nthe video resolution of ImagenVideo reaches 1280\u00d7768,\nthe frames are very blurry compared with our result. Com-\npared with Make-A-Video, the lake ripples, boats and trees\nin our video are clearer.\nFigure 6. Qualitative comparison of temporal super-resolution\nwithout and with using motion flow. Using motion flow, the in-\nterpolated frame is more stable and more consistent to input left\nand right frames for the top example, and visually better for the\nbottom example. The first and third rows are two examples, and\nthe second and four rows are zoomed-in of the patches in the red\nand yellow box.\n4.3. Ablation Study\nReference image from text-to-image generation. In order\nto evaluate the effect of our T2V strategy guided by T2I\nreference, we conducted experiments by removing the ref-\nerence condition for cascaded latent diffusion models. We\nrandomly selected 1000 text prompts from the 59800 MSR-\nVTT test set and compared the CLIPSIM scores. We also\nTable 3. Effect of reference guidance. We report average CLIPSIM\nscore on 1000 texts randomly selected from the MSR-VTT testset.\nWe also report the IS scores on the UCF101 dataset in the zero-\nshot setting.\nCLIPSIM \u2191\nIS \u2191\nwithout reference\n0.2534\n26.64 \u00b1 0.47\nwith reference\n0.3127\n71.61 \u00b1 0.24\nInput\nImage decoder\nVideo decoder\nFigure 7. Visual comparison for the effectiveness of video decoder.\nThe texture details of the the pistil and petals in our restored frame\nare clearer than those of original image decoder in the Stable Dif-\nfusion.\ncompared the IS index under zero-shot setting on the UCF-\n101 dataset. The comparison is given in Table 3. One can\nsee that the T2I reference images greatly improve the IS\nand CLIPSIM scores. This empirically verifies the effec-\ntiveness of the reference image: improving the visual fi-\ndelity and helping the latent video diffusion model learn\nbetter motion. Figure 5 shows the visual comparison from\nthe same text prompt. We can see that the visual quality and\nthe content richness with reference image are much better.\nIn Figure 3, we show three different reference images, with\nthe same text prompt, our VideoGen can generate different\nvideos.\nFlow-based temporal super-resolution.\nWe demon-\nstrate\nthe\neffectiveness\nof\nour\nflow-based\ntemporal\nsuper-resolution by replacing flow-guided with spherical-\ninterpolation guided. The comparison with two examples\nare given in Figure 6. We can observe that with motion\nflow the interpolated frames is more stable and continuous.\nWithout flow-guided, as shown in Figure 6, the fire is bro-\nken and the right shoe has artifacts.\nVideo decoder. Figure 7 shows the visual comparison re-\nsults between our video decoder and the original image de-\ncoder of the auto-encoder in Stable Diffusion. The frame\nfrom our video decoder has sharper textures. This is be-\ncause we perform various degradations on the inputs dur-\ning training, so that our video decoder has enhanced effect.\nFurthermore, the videos restored from the video decoder are\ntemporally smoother.\n4.4. User Study\nBecause Make-A-Video [46] and ImagenVideo [19],\nthe two best performing methods at present, are not open\n90%\n76%\n72%\n74%\n10%\n28%\n24%\n26%\n0%\n20%\n40%\n60%\n80%\n100%\nQuality\nQuality\nAlignment\nAlignment\nours\nMake-A-Video\nImagenVideo\nFigure 8. User Preferences. The first two bars are human eval-\nuation results of our method compared to Make-A-Video and\nImagenVideo for video quality (corresponding to the question:\n\u201dWhich video is of higher quality?\u201d), respectively. Comparison\nwith Make-A-Video, results from our approach are preferred 90%.\nCompared with ImagenVideo, 76% of our options are chosen. The\nlatter two reveal the users\u2019 preference for text-video alignment\n(\u201dWhich video better represents the provided text prompt?\u201d). Sim-\nilarly, our VideoGen also outperforms baseline methods by a large\nmargin.\nsourced, we use the demos shown on their webpages for\nhuman evaluation. We conduct the user study on an evalu-\nation set of 30 video prompts (randomly selected from the\nwebpages of Make-A-Video and ImagenVideo). For each\nexample, we ask 17 annotators to compare the video qual-\nity (\u201cWhich video is of higher quality?\u201d) and the text-video\ncontent alignment (\u201cWhich video better represents the pro-\nvided text prompt?\u201d) between two videos from the base-\nline (ImagenVideo or Make-A-Video) and our method, pre-\nsented in random order.\nAs shown in Figure\n8, in the\nvideo quality comparison with Make-A-Video, results from\nour VideoGen are preferred 90%. Compared with Imagen-\nVideo, 76% of our options are chosen. Similarly, for the\nuser study of the text-video alignment, our VideoGen also\noutperforms baseline methods by a large margin.\n5. Conclusion\nWe present VideoGen, a text-to-video generation ap-\nproach, and report the state-of-the-art video generation re-\nsults. The success stems from: (1) Leverage the SOTA text-\nto-image generation system to generate a high-quality ref-\nerence image, improving the visual fidelity of the generated\nvideo; (2) Use the reference image as a guidance of latent\nvideo diffusion, allowing the diffusion model to focus more\non learning the motion; (3) Explore high-quality unlabeled\n(unpaired) video data to train a video decoder that does not\ndepends on video-text pairs.\nReferences\n[1] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1728\u20131738,\n2021. 5\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022. 3\n[3] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image\nand video editing. In Computer Vision\u2013ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XV, pages 707\u2013723. Springer, 2022. 3\n[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\narXiv preprint arXiv:2211.09800, 2022. 3\n[5] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. arXiv preprint arXiv:2301.00704, 2023. 3\n[6] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mo-\nhammad Norouzi, and William Chan.\nWavegrad: Esti-\nmating gradients for waveform generation. arXiv preprint\narXiv:2009.00713, 2020. 3\n[7] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Moham-\nmad Norouzi, Najim Dehak, and William Chan. Wavegrad\n2: Iterative refinement for text-to-speech synthesis. arXiv\npreprint arXiv:2106.09660, 2021. 3\n[8] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune\nGwon, and Sungroh Yoon. Ilvr: Conditioning method for\ndenoising diffusion probabilistic models. In 2021 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n14347\u201314356. IEEE, 2021. 4\n[9] Katherine Crowson,\nStella\nBiderman,\nDaniel Kornis,\nDashiell Stander, Eric Hallahan, Louis Castricato, and Ed-\nward Raff. Vqgan-clip: Open domain image generation and\nediting with natural language guidance. In Computer Vision\u2013\nECCV 2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23\u201327, 2022, Proceedings, Part XXXVII, pages 88\u2013\n105. Springer, 2022. 3\n[10] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,\nChang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, et al. Cogview: Mastering text-to-image gen-\neration via transformers. Advances in Neural Information\nProcessing Systems, 34:19822\u201319835, 2021. 1, 3\n[11] Alexey Dosovitskiy and Thomas Brox.\nGenerating im-\nages with perceptual similarity metrics based on deep net-\nworks. Advances in neural information processing systems,\n29, 2016.\n[12] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\narXiv preprint arXiv:2302.03011, 2023. 1, 3\n[13] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis.\nIn Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 12873\u201312883, 2021.\n[14] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\nDevi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-\nbased text-to-image generation with human priors. In Com-\nputer Vision\u2013ECCV 2022: 17th European Conference, Tel\nAviv, Israel, October 23\u201327, 2022, Proceedings, Part XV,\npages 89\u2013106. Springer, 2022. 3\n[15] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-\nguided domain adaptation of image generators. ACM Trans-\nactions on Graphics (TOG), 41(4):1\u201313, 2022. 3\n[16] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan\nPang, David Jacobs, Jia-Bin Huang, and Devi Parikh.\nLong video generation with time-agnostic vqgan and time-\nsensitive transformer. In Computer Vision\u2013ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XVII, pages 102\u2013118. Springer, 2022. 7\n[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139\u2013144, 2020. 3\n[18] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022. 3, 7\n[19] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 1, 3, 7, 8\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 3\n[21] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022.\n[22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. arXiv preprint arXiv:2204.03458, 2022. 3, 5,\n6\n[23] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022. 1, 3, 5, 6\n[24] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1125\u20131134,\n2017.\n[25] Lingtong Kong, Boyuan Jiang, Donghao Luo, Wenqing Chu,\nXiaoming Huang, Ying Tai, Chengjie Wang, and Jie Yang.\nIfrnet: Intermediate feature refine network for efficient frame\ninterpolation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 1969\u2013\n1978, 2022. 5\n[26] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and\nBryan Catanzaro. Diffwave: A versatile diffusion model for\naudio synthesis. arXiv preprint arXiv:2009.09761, 2020. 3\n[27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023.\n[28] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and\nLawrence Carin. Video generation from text. In Proceedings\nof the AAAI conference on artificial intelligence, volume 32,\n2018. 3\n[29] Zhiheng Li, Martin Renqiang Min, Kai Li, and Chenliang\nXu. Stylet2i: Toward compositional and high-fidelity text-\nto-image synthesis. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n18197\u201318207, 2022. 3\n[30] Gaurav Mittal, Tanya Marwah, and Vineeth N Balasubrama-\nnian. Sync-draw: Automatic video generation using deep\nrecurrent attentive architectures. In Proceedings of the 25th\nACM international conference on Multimedia, pages 1096\u2013\n1104, 2017. 3\n[31] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid\nHoshen. Dreamix: Video diffusion models are general video\neditors. arXiv preprint arXiv:2302.01329, 2023. 3\n[32] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 3\n[33] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao\nMei. To create what you tell: Generating videos from cap-\ntions. In Proceedings of the 25th ACM international confer-\nence on Multimedia, pages 1789\u20131798, 2017. 3\n[34] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 2085\u20132094,\n2021. 3\n[35] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. arXiv preprint arXiv:2212.09748, 2022.\n3\n[36] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 3\n[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 3\n[38] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n3\n[39] Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun,\nCaroline Pantofaru, and Brian Curless. Film: Frame interpo-\nlation for large motion. In Computer Vision\u2013ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23\u2013\n27, 2022, Proceedings, Part VII, pages 250\u2013266. Springer,\n2022.\n[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 1, 3, 4\n[41] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi.\nPalette: Image-to-image diffusion models.\nIn\nACM SIGGRAPH 2022 Conference Proceedings, pages 1\u2013\n10, 2022. 3\n[42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic text-to-image\ndiffusion models with deep language understanding. arXiv\npreprint arXiv:2205.11487, 2022. 1, 3\n[43] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-\nmans, David J Fleet, and Mohammad Norouzi. Image super-\nresolution via iterative refinement.\nIEEE Transactions on\nPattern Analysis and Machine Intelligence, 2022. 3\n[44] Masaki Saito, Shunta Saito, Masanori Koyama, and So-\nsuke Kobayashi. Train sparsely, generate densely: Memory-\nefficient unsupervised training of high-resolution temporal\ngan.\nInternational Journal of Computer Vision, 128(10-\n11):2586\u20132606, 2020. 5\n[45] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021.\n[46] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 1, 3, 4, 5, 6, 7, 8\n[47] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning, pages 2256\u20132265. PMLR, 2015.\n3\n[48] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020.\n[49] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nA dataset of 101 human action classes from videos in the\nwild. Center for Research in Computer Vision, 2(11), 2012.\n5\n[50] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. arXiv preprint arXiv:1212.0402, 2012.\n[51] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng\nLiu. Roformer: Enhanced transformer with rotary position\nembedding. arXiv preprint arXiv:2104.09864, 2021.\n[52] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng,\nDimitris N Metaxas, and Sergey Tulyakov. A good image\ngenerator is what you need for high-resolution video synthe-\nsis. arXiv preprint arXiv:2104.15069, 2021.\n[53] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,\nand Manohar Paluri. Learning spatiotemporal features with\n3d convolutional networks. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 4489\u20134497,\n2015. 6\n[54] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv preprint arXiv:1812.01717, 2018. 5\n[55] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRapha\u00a8el Marinier, Marcin Michalski, and Sylvain Gelly.\nFvd: A new metric for video generation. 2019.\n[56] Dani Valevski, Matan Kalman, Yossi Matias, and Yaniv\nLeviathan. Unitune: Text-driven image editing by fine tuning\nan image generation model on a single image. arXiv preprint\narXiv:2210.09477, 2022. 3\n[57] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan\nHo,\nAndrea\nTagliasacchi,\nand\nMohammad\nNorouzi. Novel view synthesis with diffusion models. arXiv\npreprint arXiv:2210.04628, 2022. 3\n[58] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji,\nFan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Gen-\nerating open-domain videos from natural descriptions. arXiv\npreprint arXiv:2104.14806, 2021. 3, 5\n[59] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\nDaxin Jiang, and Nan Duan. N\u00a8uwa: Visual synthesis pre-\ntraining for neural visual world creation.\nIn Computer\nVision\u2013ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23\u201327, 2022, Proceedings, Part XVI, pages\n720\u2013736. Springer, 2022. 1, 3, 5, 7\n[60] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2212.11565, 2022. 3\n[61] Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo.\nLearning to generate time-lapse videos using multi-stage dy-\nnamic generative adversarial networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 2364\u20132373, 2018.\n[62] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5288\u20135296, 2016. 5\n[63] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind\nSrinivas. Videogpt: Video generation using vq-vae and trans-\nformers. arXiv preprint arXiv:2104.10157, 2021.\n[64] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2022. 3\n[65] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho\nKim, Jung-Woo Ha, and Jinwoo Shin. Generating videos\nwith dynamics-aware implicit generative adversarial net-\nworks. arXiv preprint arXiv:2202.10571, 2022. 7\n[66] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timo-\nfte. Designing a practical degradation model for deep blind\nimage super-resolution. In IEEE International Conference\non Computer Vision, pages 4791\u20134800, 2021. 5\n[67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018.\n[68] Zhixing Zhang,\nLigong Han,\nArnab Ghosh,\nDimitris\nMetaxas,\nand Jian Ren.\nSine:\nSingle image edit-\ning with text-to-image diffusion models.\narXiv preprint\narXiv:2212.04489, 2022. 3\n[69] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 3, 7\n"
  },
  {
    "title": "CityDreamer: Compositional Generative Model of Unbounded 3D Cities",
    "link": "https://arxiv.org/pdf/2309.00610.pdf",
    "upvote": "14",
    "text": "CityDreamer: Compositional Generative Model of Unbounded 3D Cities\nHaozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu \u0000S-Lab, Nanyang Technological University\n{haozhe.xie, zhaoxi001, fangzhou001, ziwei.liu}@ntu.edu.sg\nhttps://haozhexie.com/project/city-dreamer\nWell-defined Geometry\nMulti-view Consistent\nDiverse City Layouts\nDiverse Style and Viewpoints\nUnbounded City Layout\nFigure 1. The proposed CityDreamer generates a wide variety of unbounded city layouts and multi-view consistent appearances, featuring\nwell-defined geometries and diverse styles.\nAbstract\nIn recent years, extensive research has focused on 3D natu-\nral scene generation, but the domain of 3D city generation\nhas not received as much exploration. This is due to the\ngreater challenges posed by 3D city generation, mainly\nbecause humans are more sensitive to structural distortions\nin urban environments. Additionally, generating 3D cities\nis more complex than 3D natural scenes since buildings, as\nobjects of the same class, exhibit a wider range of appear-\nances compared to the relatively consistent appearance\nof objects like trees in natural scenes. To address these\nchallenges, we propose CityDreamer, a compositional\ngenerative model designed specifically for unbounded 3D\ncities, which separates the generation of building instances\nfrom other background objects, such as roads, green lands,\nand water areas, into distinct modules. Furthermore, we\nconstruct two datasets, OSM and GoogleEarth, containing\na vast amount of real-world city imagery to enhance the\nrealism of the generated 3D cities both in their layouts and\nappearances. Through extensive experiments, CityDreamer\nhas proven its superiority over state-of-the-art methods in\ngenerating a wide range of lifelike 3D cities.\n\u0000 Corresponding author\n1. Introduction\nIn the wave of the metaverse, 3D asset generation has\ndrawn considerable interest. Significant advancements have\nbeen achieved in generating 3D objects [37, 40, 47], 3D\navatars [24,29,60], and 3D scenes [7,11,34]. Cities, being\none of the most crucial 3D assets, have found widespread\nuse in various applications, including urban planning, en-\nvironmental simulations, and game asset creation. There-\nfore, the quest to make 3D city development accessible to\na broader audience encompassing artists, researchers, and\nplayers, becomes a significant and impactful challenge.\nIn recent years, notable advancements have been made\nin the field of 3D scene generation. GANCraft [22] and\nSceneDreamer [11] use volumetric neural rendering to pro-\nduce images within the 3D scene, using 3D coordinates and\ncorresponding semantic labels. Both methods show promis-\ning results in generating 3D natural scenes by leveraging\npseudo-ground-truth images generated by SPADE [44]. A\nvery recent work, InfiniCity [34], follows a similar pipeline\nfor 3D city generation. However, creating 3D cities presents\ngreater complexity compared to 3D natural scenes. Build-\nings, as objects with the same semantic label, exhibit a wide\nrange of appearances, unlike the relatively consistent ap-\npearance of objects like trees in natural scenes. This fact\nmay decrease the quality of generated buildings when all\n1\narXiv:2309.00610v1  [cs.CV]  1 Sep 2023\nbuildings in a city are given the same semantic label.\nTo handle the diversity of buildings in urban environ-\nments, we propose CityDreamer, a compositional genera-\ntive model designed for unbounded 3D cities. As shown\nin Figure 2, CityDreamer differs from existing methods in\nthat it splits the generation of buildings and background\nobjects like roads, green lands, and water areas into two\nseparate modules: the building instance generator and the\ncity background generator. Both generators adopt the bird\u2019s\neye view scene representation and employ a volumetric ren-\nderer to generate photorealistic images via adversarial train-\ning. Notably, the scene parameterization is meticulously\ntailored to suit the distinct characteristics of background ob-\njects and buildings. Background objects in each category\ntypically have similar appearances while exhibiting irregu-\nlar textures. Hence, we introduce the generative hash grid\nto preserve naturalness while upholding 3D consistency. In\ncontrast, building instances exhibit a wide range of appear-\nances, but the texture of their fac\u00b8ades often displays regular\nperiodic patterns. Therefore, we design periodic positional\nencoding, which is simple yet effective for handling the di-\nversity building fac\u00b8ades. The compositor finally combines\nthe rendered background objects and building instances to\ngenerate a cohesive image.\nTo enhance the realism of our generated 3D cities, we\nconstruct two datasets: OSM and GoogleEarth. The OSM\ndataset, sourced from OpenStreetMap [1], contains seman-\ntic maps and height fields of 80 cities, covering over 6,000\nkm2. These maps show the locations of roads, buildings,\ngreen lands, and water areas, while the height fields pri-\nmarily indicate building heights. The GoogleEarth dataset,\ngathered using Google Earth Studio [2], features 400 or-\nbit trajectories in New York City. It includes 24,000 real-\nworld city images, along with semantic and building in-\nstance segmentation. These annotations are automatically\ngenerated by projecting the 3D city layout, based on the\nOSM dataset, onto the images. The Google Earth dataset\nprovides a wider variety of realistic urban images from dif-\nferent perspectives. Additionally, it can be easily expanded\nto include cities worldwide.\nThe contributions are summarized as follows:\n\u2022 We propose CityDreamer, a compositional generative\nmodel designed specifically for unbounded 3D cities,\nwhich separates the generation of building instances\nfrom other background objects into distinct modules.\n\u2022 We construct two datasets, OSM and GoogleEarth,\nwith more realistic city layouts and appearances, re-\nspectively. GoogleEarth includes images with multi-\nview consistency and building instances segmentation.\n\u2022 The proposed CityDreamer is evaluated quantitatively\nand qualitatively against state-of-the-art 3D generative\nmodels, showcasing its capability in generating large-\nscale and diverse 3D cities.\n2. Related Work\n3D-aware GANs.\nGenerative adversarial networks\n(GANs) [20] have achieved remarkable success in 2D im-\nage generation [27, 28]. Efforts to extend GANs into 3D\nspace have also emerged, with some works [17, 42, 57] in-\ntuitively adopting voxel-based representations by extend-\ning the CNN backbone used in 2D. However, the high\ncomputational and memory cost of voxel grids and 3D\nconvolution poses challenges in modeling unbounded 3D\nscenes.\nRecent advancements in neural radiance field\n(NeRF) [41] have led to the incorporation of volume ren-\ndering as a key inductive bias to make GANs 3D-aware.\nThis enables GANs to learn 3D representations from 2D\nimages [8,18,21,43,59]. Nevertheless, most of these meth-\nods are trained on curated datasets for bounded scenes, such\nas human faces [27], human bodies [25], and objects [58].\nScene-level image generation. Unlike impressive 2D gen-\nerative models that mainly target single categories or com-\nmon objects, generating scene-level content is a challeng-\ning task due to the high diversity of scenes. Semantic image\nsynthesis, such as [15,22,38,44], shows promise in generat-\ning scene-level content in the wild by conditioning on pixel-\nwise dense correspondence, such as semantic segmentation\nmaps or depth maps. Some approaches have even achieved\n3D-aware scene synthesis [22, 31, 36, 38, 52], but they may\nlack full 3D consistency or support feed-forward genera-\ntion for novel worlds. Recent works like [7, 11, 34] have\nachieved 3D consistent scenes at infinity scale through un-\nbounded layout extrapolation. Another bunch of work [3,\n14,45,55] focus on indoor scene synthesis using expensive\n3D datasets [13,53] or CAD retrieval [16].\n3. The Proposed Method\nAs shown in Figure 2, CityDreamer follows a four-step\nprocess to generate an unbounded 3D city. Initially, the\nunbounded layout generator (Sec. 3.1) creates an arbitrary\nlarge city layout L. Subsequently, the city background gen-\nerator (Sec. 3.2) produces the background image \u02c6IG along\nwith its corresponding mask MG. Next, the building in-\nstances generator (Sec. 3.3) generates images for building\ninstances {\u02c6Ii\nB}n\ni=1 and their respective masks {Mi\nB}n\ni=1,\nwhere n is the number of building instances. Lastly, the\ncompositor (Sec. 3.4) merges the rendered background and\nbuilding instances into a single cohesive image IC.\n3.1. Unbounded City Layout Generator\nCity Layout Represenetation. The city layout determines\nthe 3D objects present in the city and their respective loca-\ntions. The objects can be categorized into six classes: roads,\nbuildings, green lands, construction sites, water areas, and\nothers. Moreover, there is an additional null class used to\nrepresent empty spaces in the 3D volumes. The city lay-\n2\nUnbounded Layout Generator (\u00a73.1)\nCompositor (\u00a73.4)\nBuilding Instance Generator (\u00a73.3)\nScene Representation\nCity Background Generator (\u00a73.2)\nVolumetric Rendering\nVolumetric Rendering\nScene Representation\nDecode\nQuantization\n\ud835\udc50\ud835\udc500\n\ud835\udc50\ud835\udc501\n\ud835\udc50\ud835\udc502\n\ud835\udc50\ud835\udc503\n\ud835\udc50\ud835\udc504\n\ud835\udc50\ud835\udc505\n\ud835\udc50\ud835\udc506\n\ud835\udc50\ud835\udc507\n\u2026 \ud835\udc50\ud835\udc50\ud835\udc3e\ud835\udc3e\nHeight Fields\nSematic Maps\n\ud835\udc50\ud835\udc500\n\ud835\udc50\ud835\udc501\n\ud835\udc50\ud835\udc502\n\ud835\udc50\ud835\udc503\n\ud835\udc50\ud835\udc504\n\ud835\udc50\ud835\udc506\n\ud835\udc50\ud835\udc507\n\ud835\udc50\ud835\udc505\n\ud835\udc50\ud835\udc507 \ud835\udc50\ud835\udc501 \ud835\udc50\ud835\udc505\n\ud835\udc50\ud835\udc506\n\ud835\udc50\ud835\udc503 \ud835\udc50\ud835\udc504 \ud835\udc50\ud835\udc502\n\ud835\udc50\ud835\udc505\n\ud835\udc50\ud835\udc501 \ud835\udc50\ud835\udc507\n\ud835\udc50\ud835\udc505\n\ud835\udc50\ud835\udc504 \ud835\udc50\ud835\udc502\n\ud835\udc50\ud835\udc506\nCodebook \ud835\udc9e\ud835\udc9e\nVisual Token \nPrediction\nHeight Field \ud835\udc07\ud835\udc07\nLayout Token \ud835\udc13\ud835\udc13\nSemantic Map \ud835\udc12\ud835\udc12\nCity Layout \ud835\udc0b\ud835\udc0b\nInstantiation\nBg. Mask \ud835\udc0c\ud835\udc0cG\nBldg. Images \u0302\ud835\udc08\ud835\udc08Bi\nBldg. Masks \ud835\udc0c\ud835\udc0cB\ud835\udc56\ud835\udc56\n\u2026\nComposited Image \u0302\ud835\udc08\ud835\udc08C\n\u2026\n\ud835\udc29\ud835\udc29\nBg. Feature\nDiscriminator\n\ud835\udc1c\ud835\udc1c, \u03c3\nReal\nFake\nVolumetric \nRenderer\n\ud835\udc59\ud835\udc59(\ud835\udc29\ud835\udc29)\nScene Parameterization\n\ud835\udc1f\ud835\udc1fG\n\ud835\udc91\ud835\udc91\n\ud835\udc1f\ud835\udc1fG\nGlobal Encoder\n \nIndexed\nFeature\nGenerative\nNeural Hash Grid \n\u210b\n\ud835\udc59\ud835\udc59(\ud835\udc29\ud835\udc29)\n\ud835\udc29\ud835\udc29\n\ud835\udc1c\ud835\udc1c, \u03c3\nVolumetric \nRenderer\nDiscriminator\nReal\nFake\nMod.\nScene Parameterization\nLocal Encoder \n\ud835\udc1f\ud835\udc1fBi\n\ud835\udc1f\ud835\udc1fBi\n\ud835\udc91\ud835\udc91\n\ud835\udc1f\ud835\udc1fBi\n(\ud835\udc91\ud835\udc91\ud835\udc99\ud835\udc99,\ud835\udc91\ud835\udc91\ud835\udc9a\ud835\udc9a)\nIndexed\nFeature\nCond.\nCond.\nMod.\n\ud835\udc33\ud835\udc33\nMod.\nBg. Image \u0302\ud835\udc08\ud835\udc08G\nFigure 2. Overview of CityDreamer. The unbounded layout generator creates the city layout L. Then, the city background generator\nperforms ray-sampling to retrieve features from L and generates the background image with a volumetric renderer, focusing on background\nobjects like roads, green lands, and water areas. Similarly, the building instance generator renders the building instance image with another\nvolumetric renderer. Finally, the compositor merges the rendered background and building instances, producing a unified and coherent final\nimage. Note that \u201cMod.\u201d, \u201cCond.\u201d, \u201cBg.\u201d, and \u201cBldg.\u201d denote \u201cModulation\u201d, \u201cCondition\u201d, \u201cBackground\u201d, and \u201cBuilding\u201d, respectively.\nout in CityDreamer, denoted as a 3D volume L, is created\nby extruding the pixels in the semantic map S based on the\ncorresponding values in the height field H. Specifically, the\nvalue of L at (i, j, k) can be defined as\nL(i,j,k) =\n(\nS(i,j)\nif k \u2264 H(i,j)\n0\notherwise\n(1)\nwhere 0 denotes empty spaces in the 3D volumes.\nCity Layout Generation. Obtaining unbounded city lay-\nouts is translated into generating extendable semantic maps\nand height fields. To this aim, we construct the unbounded\nlayout generator based on MaskGIT [9], which inherently\nenables inpainting and extrapolation capabilities. Specifi-\ncally, we employ VQVAE [49, 54] to tokenize the seman-\ntic map and height field patches, converting them into dis-\ncrete latent space and creating a codebook C = {ck|ck \u2208\nRD}K\ni=1. During inference, we generate the layout token T\nin an autoregressive manner, and subsequently, we use the\nVQVAE\u2019s decoder to generate a pair of semantic map S and\nheight field H. Since VQVAE generates fixed-size seman-\ntic maps and height fields, we use image extrapolation to\ncreate arbitrary-sized ones. During this process, we adopt\na sliding window to forecast a local layout token at every\nstep, with a 25% overlap during the sliding.\nLoss Functions. The VQVAE treats the generation of the\nheight field and semantic map as two separate tasks, opti-\nmizing them using L1 Loss and Cross Entropy Loss E, re-\nspectively. Additionally, to ensure sharpness in the height\nfield around the edges of the buildings, we introduce an ex-\ntra Smoothness Loss S [39].\n\u2113VQ = \u03bbR\u2225 \u02c6Hp\u2212Hp\u2225+\u03bbSS( \u02c6Hp, Hp)+\u03bbEE(\u02c6Sp, Sp) (2)\nwhere \u02c6Hp and \u02c6Sp denote the generated height field and se-\nmantic map patches, respectively. Hp and Sp are the cor-\nresponding ground truth. The autoregressive transformer in\nMaskGIT is trained using a reweighted ELBO loss [5].\n3.2. City Background Generator\nScene Representation.\nSimilar to SceneDreamer [11],\nwe use the bird\u2019s-eye-view (BEV) scene representation for\nits efficiency and expressive capabilities, making it eas-\nily applicable to unbounded scenes. Different from GAN-\nCraft [22] and InfiniCity [34], where features are parameter-\nized to voxel corners, the BEV representation comprises a\nfeature-free 3D volume generated from a height field and a\nsemantic map, following Equation 1. Specifically, we initi-\nate the process by selecting a local window with a resolution\nof N H\nG \u00d7N W\nG \u00d7N D\nG from the city layout L. This local win-\ndow is denoted as LLocal\nG\n, which is generated from the cor-\nresponding height field HLocal\nG\nand semantic map SLocal\nG\n.\nScene Parameterization. To achieve generalizable 3D rep-\nresentation learning across various scenes and align con-\ntent with 3D semantics, it is necessary to parameterize the\n3\nscene representation into a latent space, making adversar-\nial learning easier.\nFor city backgrounds, we adopt the\ngenerative neural hash grid [11] to learn generalizable fea-\ntures across scenes by modeling the hyperspace beyond\n3D space.\nSpecifically, we first encode the local scene\n(HLocal\nG\n, SLocal\nG\n) using the global encoder EG to produce\nthe compact scene-level feature fG \u2208 RdG.\nfG = EG(HLocal\nG\n, SLocal\nG\n)\n(3)\nBy leveraging a learnable neural hash function H, the in-\ndexed feature f p\nG at 3D position p \u2208 R3 can be obtained by\nmapping p and fG into a hyperspace, i.e., R3+dG \u2192 RN C\nG .\nf p\nG = H(p, fG) =\n\u0010 dG\nM\ni=1\nf i\nG\u03c0i\n3\nM\nj=1\npj\u03c0j\u0011\nmod T\n(4)\nwhere \u2295 denotes the bit-wise XOR operation. \u03c0i and \u03c0j\nrepresent large and unique prime numbers. We construct\nN L\nH levels of multi-resolution hash grids to represent multi-\nscale features, T is the maximum number of entries per\nlevel, and N C\nG denotes the number of channels in each\nunique feature vector.\nVolumetric Rendering. In a perspective camera model,\neach pixel in the image corresponds to a camera ray r(t) =\no + tv, where the ray originates from the center of projec-\ntion o and extends in the direction v. Thus, the correspond-\ning pixel value C(r) is derived from an integral.\nC(r) =\nZ \u221e\n0\nT(t)c(f r(t)\nG\n, l(r(t)))\u03c3(f r(t)\nG\n)dt\n(5)\nwhere T(t) = exp(\u2212\nR t\n0 \u03c3(f r(s)\nG\n)ds). l(p) represent the\nsemantic label at the 3D position p. c and \u03c3 denote the\ncolor and volume density, respectively.\nLoss Function. The city background generator is trained\nusing a hybrid objective, which includes a combination of a\nreconstruction loss and an adversarial learning loss. Specif-\nically, we leverage the L1 loss, perceptual loss P [26], and\nGAN loss G [32] in this combination.\n\u2113G = \u03bbL1\u2225\u02c6IG \u2212 IG\u2225 + \u03bbPP(\u02c6IG, IG) + \u03bbGG(\u02c6IG, SG) (6)\nwhere IG denotes the ground truth background image. SG\nis the semantic map in perspective view generated by ac-\ncumulating semantic labels sampled from the LLocal\nG\nalong\neach ray. The weights of the three losses are denoted as\n\u03bbL1, \u03bbP, and \u03bbG. Note that \u2113G is solely applied to pixels\nwith semantic labels belonging to background objects.\n3.3. Building Instance Generator\nScene Representation. Just like the city background gen-\nerator, the building instance generator also uses the BEV\nscene representation. In the building instance generator, we\nextract a local window denoted as LLocal\nBi\nfrom the city lay-\nout L, with a resolution of N H\nB \u00d7 N W\nB \u00d7 N D\nB , centered\naround the 2D center (cx\nBi, cy\nBi) of the building instance Bi.\nThe height field and semantic map used to generate LLocal\nBi\ncan be denoted as HLocal\nBi\nand SLocal\nBi\n, respectively. As all\nbuildings have the same semantic label in S, we perform\nbuilding instantiation by detecting connected components.\nWe observe that the fac\u00b8ades and roofs of buildings in real-\nworld scenes exhibit distinct distributions. Consequently,\nwe assign different semantic labels to the fac\u00b8ade and roof of\nthe building instance Bi in LLocal\nBi\n, with the top-most voxel\nlayer being assigned the roof label. The rest building in-\nstances are omitted in LLocal\nBi\nby assigned with the null class.\nScene Parameterization.\nIn contrast to the city back-\nground generator, the building instance generator employs\na novel scene parameterization that relies on pixel-level fea-\ntures generated by a local encoder EB. Specifically, we start\nby encoding the local scene (HLocal\nBi\n, SLocal\nBi\n) using EB, re-\nsulting in the pixel-level feature fBi, which has a resolution\nof N H\nB \u00d7 N W\nB \u00d7 N C\nB .\nfBi = EB(HLocal\nBi\n, SLocal\nBi\n)\n(7)\nGiven a 3D position p = (px, py, pz), the corresponding\nindexed feature f p\nBi can be computed as\nf p\nBi = O(Concat(f (px,py)\nBi\n, pz))\n(8)\nwhere Concat(\u00b7) is the concatenation operation. f (px,py)\nBi\n\u2208\nRN C\nB denotes the feature vector at (px, py). O(\u00b7) is the po-\nsitional encoding function used in the vanilla NeRF [41].\nO(x) = {sin(2i\u03c0x), cos(2i\u03c0x)}N L\nP \u22121\ni=0\n(9)\nNote that O(\u00b7) is applied individually to each value in the\ngiven feature x, which are normalized to lie within the range\nof [\u22121, 1].\nVolumetric Rendering. Different from the volumetric ren-\ndering used in the city background generator, we incorpo-\nrate a style code z in the building instance generator to\ncapture the diversity of buildings. The corresponding pixel\nvalue C(r) is obtained through an integral process.\nC(r) =\nZ \u221e\n0\nT(t)c(f r(t)\nBi , z, l(r(t)))\u03c3(f r(t)\nBi )dt\n(10)\nNote that the camera ray r(t) is normalized with respect to\n(cx\nBi, cy\nBi, 0) as the origin.\nLoss Function. For training the building instance genera-\ntor, we exclusively use the GAN Loss. Mathematically, it\ncan be represented as\n\u2113B = G(\u02c6IBi, SBi)\n(11)\nwhere SBi denotes the semantic map of building instance Bi\nin perspective view, which is generated in a similar manner\nto SG. Note that \u2113B is exclusively applied to pixels with\nsemantic labels belonging to the building instance.\n4\n(a) The OSM Dataset\n(c) The GoogleEarth Dataset\n(b) City Layout\n15    20   25   30   35  40 45  50  55 60  65 70\nElevation Angle of Viewpoint (\u00b0)\n# Trajectories\n100    200      300   400   500     600   700   800    900\nAltitude of Viewpoint (m)\n(d) Dataset Statistics for the GoogleEarth dataset\nFigure 3. Overview of the proposed datasets. (a) The OSM\ndataset comprising paired height fields and semantic maps pro-\nvides real-world city layouts. (b) The city layout, generated from\nthe height field and semantic map, facilitates automatic annota-\ntion generation. (c) The GoogleEarth dataset includes real-world\ncity appearances alongside semantic segmentation and building in-\nstance segmentation. (d) The dataset statistics demonstrate the va-\nriety of perspectives available in the GoogleEarth dataset.\n3.4. Compositor\nSince there are no corresponding ground truth images\nfor the images generated by the City Background Generator\nand Building Instance Generator, it is not possible to train\nneural networks to merge these images. Therefore, the com-\npositor uses the generated images \u02c6IG and {\u02c6IBi}n\ni=1, along\nwith their corresponding binary masks MG and {MBi}n\ni=1,\nthe compositor combines them into a unified image IC,\nwhich can be represented as\nIC = \u02c6IGMG +\nn\nX\ni=1\n\u02c6IBiMBi\n(12)\nwhere n is the number of building instances.\n4. The Proposed Datasets\n4.1. The OSM Dataset\nThe OSM dataset, sourced from OpenStreetMap [1], is\ncomposed of the rasterized semantic maps and height fields\nof 80 cities worldwide, spanning an area of more than 6,000\nkm2. During the rasterization process, vectorized geometry\ninformation is converted into images by translating longi-\ntude and latitude into the EPSG:3857 coordinate system at\nTable 1. A Comparison of GoogleEarth with representative\ncity-related datasets. Note that the number of images and area\nare counted based on real-world images. \u201csate.\u201d represents satel-\nlite. \u201cinst.\u201d, \u201csem.\u201d, and \u201cplane\u201d denote \u201cinstance segmentation\u201d,\n\u201csemantic segmentation\u201d, and \u201cplane segmentation\u201d respectively.\nDataset\n# Images\nArea\nView\nAnnotation\n3D\nKITTI [19]\n15 k\n-\nstreet\nsem.\n\u2717\nCityscapes [12]\n25 k\n-\nstreet\nsem.\n\u2717\nSpaceNet MOVI [56]\n6.0 k\n-\nsate.\ninst.\n\u2717\nOmniCity [30]\n108 k\n-\nstreet/sate.\ninst./plane\n\u2717\nHoliCity [61]\n6.3 k\n20 km2\nstreet\nsem./plane\n\u2713\nUrbanScene3D [35]\n6.1 k\n3 km2\ndrone\ninst.\n\u2713\nGoogleEarth\n24 k\n25 km2\ndrone\ninst./sem.\n\u2713\nzoom level 18, approximately 0.597 meters per pixel. As\nshown in Figure 3(a), the segmentation maps use red, yel-\nlow, green, cyan, and blue colors to denote the positions of\nroads, buildings, green lands, construction sites, and water\nareas, respectively. The height fields primarily represent the\nheight of buildings, with their values derived from Open-\nStreetMap. For roads, the height values are set to 4, while\nfor water areas, they are set to 0. Additionally, the height\nvalues for trees are sampled from perlin noise [46], ranging\nfrom 8 to 16.\n4.2. The GoogleEarth dataset\nThe GoogleEarth dataset is collected from Google Earth\nStudio [2], including 400 orbit trajectories in Manhattan and\nBrooklyn. Each trajectory consists of 60 images, with or-\nbit radiuses ranging from 125 to 813 meters and altitudes\nvarying from 112 to 884 meters. In addition to the images,\nGoogle Earth Studio provides camera intrinsic and extrin-\nsic parameters, making it possible to create automated an-\nnotations for semantic and building instance segmentation.\nSpecifically, for building instance segmentation, we initially\nperform connected components detection on the semantic\nmaps to identify individual building instances. Then, the\ncity layout is created following Equation 1, as demonstrated\nin Figure 3(b). Finally, the annotations are generated by\nprojecting the city layout onto the images, using the camera\nparameters, as shown in Figure 3(c).\nTable 1 presents a comparative overview between\nGoogleEarth and other datasets related to urban environ-\nments. Among datasets that offer 3D models, GoogleEarth\nstands out for its extensive coverage of real-world images,\nencompassing the largest area, and providing annotations\nfor both semantic and instance segmentation. Figure 3(d)\noffers an analysis of viewpoint altitudes and elevations in\nthe GoogleEarth dataset, highlighting its diverse camera\nviewpoints. This diversity enhances neural networks\u2019 abil-\nity to generate cities from a broader range of perspectives.\nAdditionally, leveraging Google Earth and OpenStreetMap\ndata allows us to effortlessly expand our dataset to encom-\npass more cities worldwide.\n5\n5. Experiments\n5.1. Evaluation Protocols\nDuring evaluation, we use the Unbounded Layout Gen-\nerator to generate 1024 distinct city layouts. For each scene,\nwe sample 20 different styles by randomizing the style code\nz. Each sample is transformed into a fly-through video con-\nsisting of 40 frames, each with a resolution of 960\u00d7540\npixels and any possible camera trajectory. Subsequently,\nwe randomly select frames from these video sequences for\nevaluation. The evaluation metrics are as follows:\nFID and KID. Fr\u00b4echet Inception Distance (FID) [23] and\nKernel Inception Distance (KID) [4] are metrics for the\nquality of generated images. We compute FID and KID\nbetween a set of 15,000 generated frames and an evaluation\nset comprising 15,000 images randomly sampled from the\nGoogleEarth dataset.\nDepth Error. We employ depth error (DE) to assess the 3D\ngeometry, following a similar approach to EG3D [8]. Using\na pre-trained model [48], we generate pseudo ground truth\ndepth maps for generated frames by accumulating density\n\u03c3. Both the \u201cground truth\u201d depth and the predicted depth\nare normalized to zero mean and unit variance to eliminate\nscale ambiguity. DE is computed as the L2 distance be-\ntween the two normalized depth maps. We assess this depth\nerror on 100 frames for each evaluated method.\nCamera Error. Following SceneDreamer [11], we intro-\nduce camera error (CE) to assess multi-view consistency.\nCE quantifies the difference between the inference cam-\nera trajectory and the estimated camera trajectory from\nCOLMAP [50]. It is calculated as the scale-invariant nor-\nmalized L2 distance between the reconstructed and gener-\nated camera poses.\n5.2. Implementation Details\nWe implement our network using PyTorch and CUDA.\nThe experiments are conducted using eight NVIDIA Tesla\nV100 GPUs.\nHyperparameters\nUnbounded Layout Generator. The codebook size K is set\nto 512, and each code\u2019s dimension D is set to 512. The\nheight field and semantic map patches are cropped to a size\nof 512\u00d7512, and compressed by a factor of 16. The loss\nweights, \u03bbR, \u03bbS, and \u03bbE, are 10, 10, 1, respectively.\nCity Background Generator. The local window resolution\nN H\nG , N W\nG , and N D\nG are set to 1536, 1536, and 640, respec-\ntively. The dimension of the scene-level features dG is 2.\nFor the generative hash grid, we use N L\nH = 16, T = 219,\nand N C\nG = 8. The unique prime numbers in Equation 4\nare set to \u03c01 = 1, \u03c02 = 2654435761, \u03c03 = 805459861,\n\u03c04 = 3674653429, and \u03c05 = 2097192037. The loss func-\ntion weights, \u03bbL1, \u03bbP, and \u03bbG, are 10, 10, 0.5, respectively.\nBuilding Instance Generator. The local window resolution\nTable 2. Quantitative comparison. The best values are high-\nlighted in bold. Note that the results of InfiniCity are not included\nin this comparison as it is not open-sourced.\nMethods\nFID \u2193\nKID \u2193\nDE \u2193\nCE \u2193\nSGAM [51]\n277.64\n0.358\n0.575\n239.291\nPersistentNature [7]\n123.83\n0.109\n0.326\n86.371\nSceneDreamer [11]\n213.56\n0.216\n0.152\n0.186\nCityDreamer\n97.38\n0.096\n0.147\n0.060\nN H\nB , N W\nB , and N D\nB are set to 672, 672, and 640, respec-\ntively. The number of channels N C\nB of the pixel-level fea-\ntures is 63. The dimension N L\nP is set to 10.\nTraining Details\nUnbounded Layout Generator. The VQVAE is trained with\na batch size of 16 using an Adam optimizer with \u03b2 = (0.5,\n0.9) and a learning rate of 7.2 \u00d7 10\u22125 for 1,250,000 itera-\ntions. The autoregressive transformer is trained with a batch\nsize of 80 using an Adam optimizer with \u03b2 = (0.9, 0.999)\nand a learning rate of 2 \u00d7 10\u22124 for 250,000 iterations.\nCity Background and Building Instance Generators. Both\ngenerators are trained using an Adam optimizer with \u03b2 =\n(0, 0.999) and a learning rate of 10\u22124. The discriminators\nare optimized using an Adam optimizer with \u03b2 = (0, 0.999)\nand a learning rate of 10\u22125. The training lasts for 298,500\niterations with a batch size of 8. The images are randomly\ncropped to a size of 192\u00d7192.\n5.3. Comparisons\nBaselines. We compare CityDreamer against four state-of-\nthe-art methods: SGAM [51], PersistentNature [7], Scene-\nDreamer [11], and InfiniCity [34]. With the exception of In-\nfiniCity, whose code is not available, the remaining methods\nare retrained using the released code on the GoogleEarth\ndataset to ensure a fair comparison. SceneDreamer initially\nuses simplex noise for layout generation, which is not ideal\nfor cities, so it is replaced with the unbounded layout gen-\nerator from CityDreamer.\nQualitative Comparison.\nFigure 4 provides qualitative\ncomparisons against baselines. SGAM struggles to produce\nrealistic results and maintain good 3D consistency because\nextrapolating views for complex 3D cities can be extremely\nchallenging. PersistentNature employs tri-plane represen-\ntation, but it encounters challenges in generating realistic\nrenderings. SceneDreamer and InfiniCity both utilize voxel\ngrids as their representation, but they still suffer from se-\nvere structural distortions in buildings because all buildings\nare given the same semantic label. In comparison, the pro-\nposed CityDreamer generates more realistic and diverse re-\nsults compared to all the baselines.\nQuantitative Comparison. Table 2 presents the quanti-\ntative metrics of the proposed approach compared to the\nbaselines. CityDreamer exhibits significant improvements\n6\nSGAM\nPersistentNature\nSceneDreamer\nCityDreamer\nInfiniCity\nFigure 4. Qualitative comparison. The proposed CityDreamer produces more realistic and diverse results compared to all baselines. Note\nthat the visual results of InfiniCity [34] are provided by the authors and zoomed for optimal viewing.\nin FID and KID, which is consistent with the visual com-\nparisons. Moreover, CityDreamer demonstrates the capa-\nbility to maintain accurate 3D geometry and view consis-\ntency while generating photorealistic images, as evident by\nthe lowest DE and CE errors compared to the baselines.\nUser Study. To better assess the 3D consistency and quality\nof the unbounded 3D city generation, we conduct an output\nevaluation [6] as the user study. In this survey, we ask 20\nvolunteers to rate each generated camera trajectory based\non three aspects: 1) the perceptual quality of the imagery,\n2) the level of 3D realism, and 3) the 3D view consistency.\nThe scores are on a scale of 1 to 5, with 5 representing the\nbest rating. The results are presented in Figure 5, show-\ning that the proposed method significantly outperforms the\nbaselines by a large margin.\n5.4. Ablation Study\nEffectiveness of Unbounded Layout Generator. The Un-\nbounded Layout Generator plays a critical role in generat-\ning \u201cunbounded\u201d city layouts. We compare it with Infini-\ntyGAN [33] used in InfiniCity and a rule-based city layout\ngeneration method, IPSM [10], as shown in Table 4. Fol-\nlowing InfiniCity [34], we use FID and KID to evaluate the\nquality of the generated layouts. Compared to IPSM and\nInfinityGAN, Unbounded Layout Generator achieves better\nresults in terms of all metrics. The qualitative results shown\nin Figure 6 also demonstrate the effectiveness of the pro-\nposed method.\nEffectiveness of Building Instance Generator. We em-\nphasize the crucial role of the Building Instance Generator\nin the success of unbounded 3D city generation. To demon-\nstrate its effectiveness, we conducted an ablation study on\nthe Building Instance Generator.\nWe compared two op-\ntional designs: (1) Removing the Building Instance Gen-\nerator from CityDreamer, i.e., the model falling back to\nSceneDreamer. (2) All buildings are generated at once by\nthe Building Instance Generator, without providing any in-\nstance labels. The quantitative results presented in Table 4\n7\nSGAM\nPers.Nature SceneDreamer InfiniCity\nCityDreamer\n0\n1\n2\n3\n4\n5\nAvg. Scores\nPerceptual Quality\nDegree of 3D Realism\nView Consistency\nFigure 5. User study on unbounded 3D city generation. All\nscores are in the range of 5, with 5 indicating the best.\ndemonstrate the effectiveness of both the instance labels and\nthe Building Instance Generator. Please refer to Figure 7 for\nmore qualitative comparisons.\nEffectiveness of Scene Parameterization. Scene param-\neterization directly impacts the quality of 3D city gener-\nation. The City Background Generator utilizes HashGrid\nwith patch-wise features from the global encoder, while\nthe Building Instance Generator uses vanilla SinCos posi-\ntional encoding with pixel-wise features from the local en-\ncoder.\nWe compare different scene parameterizations in\nboth the City Background Generator and the Building In-\nstance Generator. Table 5 shows that using local encoders\nin background generation or using global encoders in build-\ning generation leads to considerable degradation in image\nquality, indicated by poor metrics.\nAccording to Equa-\ntion 4, the output of HashGrid is determined by the scene-\nlevel features and 3D position. While HashGrid enhances\nthe multi-view consistency of the generated background, it\nalso introduces challenges in building generation, leading\nto less structurally reasonable buildings. In contrast, the\ninherent periodicity of SinCos makes it easier for the net-\nwork to learn the periodicity of building fac\u00b8ades, leading\nto improved results in building generation. Please refer to\nSec. A.2 in the appendix for a detailed discussion.\n5.5. Discussion\nApplications. This research primarily benefits applications\nthat require efficient content creation, with notable exam-\nples being the entertainment industry. There is a strong de-\nmand to generate content for computer games and movies\nwithin this field.\nLimitations. 1) The generation of the city layout involves\nraising voxels to a specific height, which means that con-\ncave geometries like caves and tunnels cannot be modeled\nand generated. 2) During the inference process, the build-\nings are generated individually, resulting in a slightly higher\ncomputation cost. Exploring ways to reduce the inference\ncost would be beneficial for future work.\nTable 3. Effectiveness of Ubounded Layout Generator. The best\nvalues are highlighted in bold. The images are centrally cropped\nto a size of 4096\u00d74096.\nMethods\nFID \u2193\nKID \u2193\nIPSM [10]\n321.47\n0.502\nInfinityGAN [33]\n183.14\n0.288\nOurs\n124.45\n0.123\nTable 4. Effectiveness of Building Instance Generator. The best\nvalues are highlighted in bold. Note that \u201cw/o BIG.\u201d indicates the\nremoval of Building Instance Generator from CityDreamer. \u201cw/o\nIns.\u201d denotes the absence of building instance labels in the Build-\ning Instance Generator.\nMethods\nFID \u2193\nKID \u2193\nDE \u2193\nCE \u2193\nw/o BIG.\n213.56\n0.216\n0.152\n0.186\nw/o Ins.\n117.75\n0.124\n0.148\n0.098\nOurs\n97.38\n0.096\n0.147\n0.060\nTable 5. Effectiveness of different generative scene parameter-\nization. The best values are highlighted in bold. Note that \u201cCBG.\u201d\nand \u201cBIG.\u201d denote City Background Generator and Building In-\nstance Generator, respectively. \u201cEnc.\u201d and \u201cP.E.\u201d represent \u201cEn-\ncoder\u201d and \u201cPositional Encoding\u201d, respectively.\nCBG.\nBIG.\nFID \u2193\nKID \u2193\nDE \u2193\nCE \u2193\nEnc.\nP.E.\nEnc.\nP.E.\nLocal\nSinCos\nGlobal\nHash\n219.30\n0.233\n0.154\n0.452\nLocal\nSinCos\nLocal\nSinCos\n107.63\n0.125\n0.149\n0.078\nGlobal\nHash\nGlobal\nHash\n213.56\n0.216\n0.153\n0.186\nGlobal\nHash\nLocal\nSinCos\n97.38\n0.096\n0.147\n0.060\n6. Conclusion\nIn this paper, we propose CityDreamer, a compositional\ngenerative model designed specifically for unbounded 3D\ncities. Compared to existing methods that treat buildings as\na single class of objects, CityDreamer separates the gen-\neration of building instances from other background ob-\njects, allowing for better handling of the diverse appear-\nances of buildings. Additionally, we create the OSM and\nGoogleEarth datasets, providing more realistic city layouts\nand appearances, and easily scalable to include other cities\nworldwide.\nCityDreamer is evaluated quantitatively and\nqualitatively against state-of-the-arts, showcasing its capa-\nbility in generating large-scale and diverse 3D cities.\nAcknowledgments This study is supported by the Min-\nistry of Education, Singapore, under its MOE AcRF Tier\n2 (MOE-T2EP20221-0012), NTU NAP, and under the\nRIE2020 Industry Alignment Fund \u2013 Industry Collabora-\ntion Projects (IAF-ICP) Funding Initiative, as well as cash\nand in-kind contribution from the industry partner(s).\n8\nReferences\n[1] https://openstreetmap.org. 2, 5\n[2] https://earth.google.com/studio. 2, 5\n[3] Miguel \u00b4Angel Bautista, Pengsheng Guo, Samira Abnar, Wal-\nter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent\nDinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, Afshin\nDehghan, and Joshua M. Susskind. GAUDI: A neural archi-\ntect for immersive 3d scene generation. In NeurIPS, 2022.\n2\n[4] Mikolaj Binkowski, Danica J. Sutherland, Michael Arbel,\nand Arthur Gretton. Demystifying MMD GANs. In ICLR,\n2018. 6\n[5] Sam Bond-Taylor, Peter Hessey, Hiroshi Sasaki, Toby P.\nBreckon, and Chris G. Willcocks.\nUnleashing transform-\ners: Parallel token prediction with discrete absorbing diffu-\nsion for fast high-resolution image generation from vector-\nquantized codes. In ECCV, 2022. 3\n[6] Zoya Bylinskii, Laura Mariah Herman, Aaron Hertzmann,\nStefanie Hutka, and Yile Zhang. Towards better user studies\nin computer graphics and vision. Foundations and Trends in\nComputer Graphics and Vision, 15(3):201\u2013252, 2023. 7\n[7] Lucy Chai, Richard Tucker, Zhengqi Li, Phillip Isola, and\nNoah Snavely. Persistent Nature: A generative model of un-\nbounded 3D worlds. In CVPR, 2023. 1, 2, 6\n[8] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas J. Guibas, Jonathan Tremblay, Sameh Khamis,\nTero Karras, and Gordon Wetzstein.\nEfficient geometry-\naware 3D generative adversarial networks. In CVPR, 2022.\n2, 6\n[9] Huiwen Chang,\nHan Zhang,\nLu Jiang,\nCe Liu,\nand\nWilliam T. Freeman. MaskGIT: Masked generative image\ntransformer. In CVPR, 2022. 3\n[10] Guoning Chen, Gregory Esch, Peter Wonka, Pascal M\u00a8uller,\nand Eugene Zhang. Interactive procedural street modeling.\nACM TOG, 27(3):103, 2008. 7, 8\n[11] Zhaoxi Chen, Guangcong Wang, and Ziwei Liu.\nScene-\nDreamer: Unbounded 3D scene generation from 2D image\ncollections. arXiv, 2302.01330, 2023. 1, 2, 3, 4, 6\n[12] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld,\nMarkus Enzweiler,\nRodrigo Benenson,\nUwe\nFranke, Stefan Roth, and Bernt Schiele.\nThe cityscapes\ndataset for semantic urban scene understanding. In CVPR,\n2016. 5\n[13] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-\nber, Thomas A. Funkhouser, and Matthias Nie\u00dfner. Scan-\nNet: Richly-annotated 3D reconstructions of indoor scenes.\nIn CVPR, 2017. 2\n[14] Terrance DeVries, Miguel \u00b4Angel Bautista, Nitish Srivastava,\nGraham W. Taylor, and Joshua M. Susskind. Unconstrained\nscene generation with locally conditioned radiance fields. In\nICCV, 2021. 2\n[15] Patrick Esser, Robin Rombach, and Bj\u00a8orn Ommer. Taming\ntransformers for high-resolution image synthesis. In CVPR,\n2021. 2\n[16] Huan Fu, Bowen Cai, Lin Gao, Lingxiao Zhang, Jiaming\nWang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia,\nBinqiang Zhao, and Hao Zhang. 3D-FRONT: 3D furnished\nrooms with layouts and semantics. In ICCV, 2021. 2\n[17] Matheus Gadelha, Subhransu Maji, and Rui Wang. 3d shape\ninduction from 2D views of multiple objects. In 3DV, 2017.\n2\n[18] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. GET3D: A generative model of high quality 3D tex-\ntured shapes learned from images. In NeurIPS, 2022. 2\n[19] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? the KITTI vision benchmark\nsuite. In CVPR, 2012. 5\n[20] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,\nand Yoshua Bengio. Generative adversarial nets. In NIPS,\n2014. 2\n[21] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt.\nStyleNeRF: A style-based 3D aware generator for high-\nresolution image synthesis. In ICLR, 2022. 2\n[22] Zekun Hao, Arun Mallya, Serge J. Belongie, and Ming-\nYu Liu. GANCraft: Unsupervised 3D neural rendering of\nminecraft worlds. In ICCV, 2021. 1, 2, 3\n[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs trained by\na two time-scale update rule converge to a local nash equi-\nlibrium. In NIPS, 2017. 6\n[24] Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and\nZiwei Liu.\nEVA3D: compositional 3D human generation\nfrom 2D image collections. In ICLR, 2023. 1\n[25] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3.6M: Large scale datasets and predic-\ntive methods for 3D human sensing in natural environments.\nIEEE TPAMI, 36(7):1325\u20131339, 2014. 2\n[26] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual\nlosses for real-time style transfer and super-resolution. In\nECCV, 2016. 4\n[27] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIEEE TPAMI, 43(12):4217\u20134228, 2021. 2\n[28] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\nthe image quality of stylegan. In CVPR, 2020. 2\n[29] Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Ed-\nuard Gabriel Bazavan, Mihai Fieraru, and Cristian Smin-\nchisescu. DreamHuman: Animatable 3D avatars from text.\narXiv, 2306.09329, 2023. 1\n[30] Weijia Li, Yawen Lai, Linning Xu, Yuanbo Xiangli, Jinhua\nYu, Conghui He, Gui-Song Xia, and Dahua Lin. OmniCity:\nOmnipotent city understanding with multi-level and multi-\nview images. In CVPR, 2023. 5\n[31] Zhengqi Li, Qianqian Wang, Noah Snavely, and Angjoo\nKanazawa.\nInfiniteNature-Zero: Learning perpetual view\ngeneration of natural scenes from single images. In ECCV,\n2022. 2\n[32] Jae Hyun Lim and Jong Chul Ye. Geometric GAN. arXiv,\n1705.02894, 2017. 4\n9\n[33] Chieh Hubert Lin, Hsin-Ying Lee, Yen-Chi Cheng, Sergey\nTulyakov, and Ming-Hsuan Yang.\nInfinityGan: Towards\ninfinite-pixel image synthesis. In ICLR, 2022. 7, 8\n[34] Chieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei\nChai, Aliaksandr Siarohin, Ming-Hsuan Yang, and Sergey\nTulyakov. InfiniCity: Infinite-scale city synthesis. In ICCV,\n2023. 1, 2, 3, 6, 7, 15\n[35] Liqiang Lin, Yilin Liu, Yue Hu, Xingguang Yan, Ke Xie, and\nHui Huang. Capturing, reconstructing, and simulating: The\nurbanscene3d dataset. In ECCV, 2022. 5\n[36] Andrew Liu, Ameesh Makadia, Richard Tucker, Noah\nSnavely, Varun Jampani, and Angjoo Kanazawa. Infinite Na-\nture: Perpetual view generation of natural scenes from a sin-\ngle image. In ICCV, 2021. 2\n[37] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3D object. arXiv, 2303.11328, 2023.\n1\n[38] Arun Mallya, Ting-Chun Wang, Karan Sapra, and Ming-Yu\nLiu. World-consistent video-to-video synthesis. In ECCV,\n2020. 2\n[39] Simon Meister, Junhwa Hur, and Stefan Roth. UnFlow: Un-\nsupervised learning of optical flow with a bidirectional cen-\nsus loss. In AAAI, 2018. 3\n[40] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and\nAndrea Vedaldi. RealFusion: 360\u00b0 reconstruction of any ob-\nject from a single image. In CVPR, 2023. 1\n[41] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 2, 4\n[42] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian\nRichardt, and Yong-Liang Yang. HoloGAN: Unsupervised\nlearning of 3D representations from natural images.\nIn\nCVPR, 2019. 2\n[43] Roy\nOr-El,\nXuan\nLuo,\nMengyi\nShan,\nEli\nShecht-\nman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.\nStyleSDF: High-resolution 3D-consistent image and geom-\netry generation. In CVPR, 2022. 2\n[44] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\nZhu. Semantic image synthesis with spatially-adaptive nor-\nmalization. In CVPR, 2019. 1, 2\n[45] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten\nKreis,\nAndreas Geiger,\nand Sanja Fidler.\nATISS:\nautoregressive transformers for indoor scene synthesis.\nIn Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N.\nDauphin, Percy Liang, and Jennifer Wortman Vaughan, edi-\ntors, NeurIPS, 2021. 2\n[46] Ken Perlin. An image synthesizer. In SIGGRAPH, 1985. 5\n[47] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nNataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman,\nMichael Rubinstein, Jonathan T. Barron, Yuanzhen Li, and\nVarun Jampani. DreamBooth3D: Subject-driven text-to-3D\ngeneration. arXiv, 2303.13508, 2023. 1\n[48] Ren\u00b4e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE TPAMI, 44(3):1623\u20131637, 2022. 6\n[49] Ali Razavi, A\u00a8aron van den Oord, and Oriol Vinyals. Gen-\nerating diverse high-fidelity images with VQ-VAE-2.\nIn\nNeurIPS, 2019. 3\n[50] Johannes L. Sch\u00a8onberger and Jan-Michael Frahm. Structure-\nfrom-motion revisited. In CVPR, 2016. 6, 13\n[51] Yuan Shen, Wei-Chiu Ma, and Shenlong Wang.\nSGAM:\nbuilding a virtual 3D world through simultaneous generation\nand mapping. In NeurIPS, 2022. 6\n[52] Zifan Shi, Yujun Shen, Jiapeng Zhu, Dit-Yan Yeung, and\nQifeng Chen. 3D-aware indoor scene synthesis with depth\npriors. In ECCV, 2022. 2\n[53] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen,\nErik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-\nArtal, Carl Yuheng Ren, Shobhit Verma, Anton Clarkson,\nMingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June\nYon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Bri-\nales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira,\nManolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De\nNardi, Michael Goesele, Steven Lovegrove, and Richard A.\nNewcombe. The Replica Dataset: A digital replica of indoor\nspaces. arXiv, 1906.05797, 2019. 2\n[54] A\u00a8aron\nvan\nden\nOord,\nOriol\nVinyals,\nand\nKoray\nKavukcuoglu.\nNeural discrete representation learning.\nIn NIPS, 2017. 3\n[55] Xinpeng Wang, Chandan Yeshwanth, and Matthias Nie\u00dfner.\nSceneformer: Indoor scene generation with transformers. In\n3DV, 2021. 2\n[56] Nicholas\nWeir,\nDavid\nLindenbaum,\nAlexei\nBastidas,\nAdam Van Etten, Varun Kumar Vijay, Sean McPherson, Ja-\ncob Shermeyer, and Hanlin Tang. Spacenet MVOI: A multi-\nview overhead imagery dataset. In ICCV, 2019. 5\n[57] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and\nJosh Tenenbaum. Learning a probabilistic latent space of ob-\nject shapes via 3D generative-adversarial modeling. In NIPS,\n2016. 2\n[58] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei\nRen, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen\nQian, Dahua Lin, and Ziwei Liu. OmniObject3D: Large-\nvocabulary 3D object dataset for realistic perception, recon-\nstruction and generation. In CVPR, 2023. 2\n[59] Yang Xue, Yuheng Li, Krishna Kumar Singh, and Yong Jae\nLee. GIRAFFE HD: A high-resolution 3D-aware generative\nmodel. In CVPR, 2022. 2\n[60] Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang\nYu, Billzb Wang, Bin Fu, Tao Chen, Guosheng Lin, and\nChunhua Shen. StyleAvatar3D: Leveraging image-text dif-\nfusion models for high-fidelity 3D avatar generation. arXiv,\n2305.19012, 2023. 1\n[61] Yichao Zhou, Jingwei Huang, Xili Dai, Linjie Luo, Zhili\nChen, and Yi Ma. HoliCity: A city-scale data platform for\nlearning holistic 3D structures. arXiv, 2008.03286, 2020. 5\n10\nIn this supplementary material, we offer extra details and additional results to complement the main paper. Firstly, we offer\nmore extensive information and results regarding the ablation studies in Sec. A. Secondly, we present additional experimental\nresults in Sec. B. Finally, we provide a brief overview of our interactive demo in Sec. C.\nA. Additional Ablation Study Results\nA.1. Qulitative Results for Ablation Studies\nEffectiveness of Unbounded Layout Generator. Figure 6 gives a qualitative comparison as a supplement to Table 3,\ndemonstrating the effectiveness of Unbounded Layout Generator. In the case of InfinityGAN, we follow the approach used\nin InfiniCity, where each class of semantic maps is assigned a specific color, and we convert back to a semantic map by\nassociating it with the nearest color.\nIPSM\nInfinityGAN\nOurs\nFigure 6. Qualitative comparison of different city layout generation methods. The height map values are normalized to a range of [0, 1]\nby dividing each value by the maximum value within the map.\nEffectiveness of Building Instance Generator. Figure 7 provides a qualitative comparison as a supplement to Table 4,\ndemonstrating the effectiveness of Building Instance Generator. Figure 7 highlights the importance of both Building Instance\nGenerator and the instance labels. Removing either of them significantly degrades the quality of the generated images.\nw/o BIG.\nw/o Ins.\nOurs\nFigure 7. Qualitative comparison of different Building Instance Generator variants. Note that \u201dw/o BIG.\u201d indicates the removal of\nBuilding Instance Generator from CityDreamer. \u201dw/o Ins.\u201d denotes the absence of building instance labels in Building Instance Generator.\n11\nA.2. More Discussions on Scene Parameterization\nTable 5 displays the four primary combinations of different encoders and positional encodings. Additionally, Table 6\npresents twelve additional alternative combinations, in addition to those in Table 5. The results in Table 6 clearly demonstrate\nthe superiority of the scene parameterization used in CityDreamer.\nWe present the qualitative results for the sixteen scene parameterization settings in Figure 8. Using the Global Encoder and\nHash Grid as scene parameterization results in more natural city backgrounds (first column) but leads to a severe decrease in\nthe quality of generated buildings (first row). As demonstrated in the third row and third column, this irregularity is weakened\nwhen the Global Encoder is replaced with the Local Encoder. Furthermore, using the Global Encoder with SinCos positional\nencoding introduces periodic patterns, as shown in the second row and second column. However, this periodicity is disrupted\nwhen the Global Encoder is replaced with the Local Encoder (the fourth row and column) because the input of SinCos\npositional encoding no longer depends on 3D position p. Nevertheless, this change also slightly reduces the multi-view\nconsistency.\nTable 6. Effectiveness of different generative scene parameterization. The best values are highlighted in bold. Note that \u201cCBG.\u201d and\n\u201cBIG.\u201d denote City Background Generator and Building Instance Generator, respectively. \u201cEnc.\u201d and \u201cP.E.\u201d represent \u201cEncoder\u201d and\n\u201cPositional Encoding\u201d, respectively.\nCBG.\nEnc.\nGlobal\nLocal\nP.E.\nHash\nSinCos\nHash\nSinCos\nBIG.\nEnc.\nGlobal\nLocal\nGlobal\nLocal\nGlobal\nLocal\nGlobal\nLocal\nP.E.\nHash\nSinCos\nHash\nSinCos\nHash\nSinCos\nHash\nSinCos\nHash\nSinCos\nHash\nSinCos\nHash\nSinCos\nHash\nSinCos\nFID \u2193\n213.56\n113.45\n112.61\n97.38\n248.30\n135.86\n125.97\n132.67\n203.97\n116.01\n116.76\n99.78\n219.30\n124.87\n137.99\n107.63\nKID \u2193\n0.216\n0.141\n0.129\n0.096\n0.318\n0.205\n0.172\n0.174\n0.199\n0.105\n0.104\n0.098\n0.233\n0.134\n0.157\n0.125\nDE \u2193\n0.153\n0.149\n0.153\n0.147\n0.156\n0.155\n0.150\n0.151\n0.156\n0.150\n0.152\n0.152\n0.154\n0.152\n0.153\n0.149\nCE \u2193\n0.186\n0.086\n0.095\n0.060\n0.325\n0.106\n0.165\n0.089\n0.153\n0.933\n0.127\n0.075\n0.452\n0.174\n0.246\n0.078\nGlobal + Hash\nGlobal + SinCos\nLocal + Hash\nLocal + SinCos\nGlobal + Hash\nGlobal + SinCos\nLocal + Hash\nLocal + SinCos\nCity Background Generator Scene Parameterization\nBuilding Instance Generator Scene Parameterization\nFigure 8. Qualitative comparison of different scene parameterization. The terms \u201cGlobal\u201d and \u201cLocal\u201d correspond to \u201cGlobal Encoder\u201d\n(EG) and \u201dLocal Encoder\u201d (EB), which generate features following Equation 3 and Equation 7, respectively. \u201cHash\u201d and \u201cSinCos\u201d\nrepresent \u201cHash Grid\u201d and \u201cSinCos\u201d positional encodings defined in Equations 4 and 9, respectively.\n12\nB. Additional Experimental Results\nB.1. View Consistency Comparison\nTo demonstrate the multi-view consistent renderings of CityDreamer, we utilize COLMAP [50] for structure-from-motion\nand dense reconstruction using a generated video sequence. The video sequence consists of 600 frames with a resolution of\n960\u00d7540, captured from a circular camera trajectory that orbits around the scene at a fixed height and looks at the center (sim-\nilar to the sequence presented in the supplementary video). The reconstruction is performed solely using the images, without\nexplicitly specifying camera parameters. As shown in Figure 9, the estimated camera poses precisely match our sampled\ntrajectory, and the resulting point cloud is well-defined and dense. Out of the evaluated methods, only SceneDreamer and\nCityDreamer managed to accomplish dense reconstruction. CityDreamer, in particular, exhibited superior view consistency\ncompared to SceneDreamer. This superiority can be attributed to the fact that the images generated by CityDreamer are more\nconducive to feature matching.\nSparse Reconstruction (with camera poses)\nDense Reconstruction\nReference Image\nSceneDreamer\nCityDreamer\nFigure 9. COLMAP reconstruction of a 600-frame generated video captured from an orbit trajectory. The red ring represents the\nestimated camera poses, and the well-defined point clouds showcase CityDreamer\u2019s highly multi-view consistent renderings.\nB.2. Building Interpolation\nAs illustrated in Figure 10, CityDreamer demonstrates the ability to interpolate along the building style, which is controlled\nby the variable z.\nFigure 10. Linear interpolation along the building style. As we move from left to right, the style of each building changes gradually,\nwhile the background remains unchanged.\n13\nB.3. Additional Dataset Examples\nIn Figure 11, we provide more examples of the OSM and GoogleEarth datasets. The first six rows are taken from the\nGoogleEarth dataset, specifically from New York City. The last two rows showcase Singapore and San Francisco, illustrating\nthe potential to extend the existing data to other cities worldwide.\n(a) The OSM Dataset\n(c) The GoogleEarth Dataset\n(b) City Layout\nFigure 11. Examples from the OSM and GoogleEarth datasets. (a) Height fields and semantic maps from the OSM dataset. (b) City\nlayouts generated from the height fields and semantic maps. (c) Images and segmentation maps from the GoogleEarth dataset.\n14\nB.4. Additional Qualitative Comparison\nIn Figure 12, we provide more visual comparisons with state-of-the-art methods. We also encourage readers to explore\nmore video results available on our project page.\nSGAM\nSceneDreamer PersistentNature\nCityDreamer\nInfiniCity\nSGAM\nSceneDreamer PersistentNature\nCityDreamer\nInfiniCity\nFigure 12. Qualitative comparison. The proposed CityDreamer produces more realistic and diverse results compared to all baselines.\nNote that the visual results of InfiniCity [34] are provided by the authors and zoomed for optimal viewing.\n15\nLayout Generation\nTrajectory Selection\nRendering\nFigure 13. The screenshots of the interactive demo. This interactive demo allows users to create their own cities in an engaging and\ninteractive manner. We encourage the readers to explore the video demo available on our project page.\nC. Interactive Demo\nWe develop a web demo that allows users to interactively create their own cities. The process involves three main steps:\nlayout generation, trajectory selection, and rendering, as illustrated in Figure 13. Users can manipulate these steps to create\ncustomized 3D city scenes according to their preferences.\nDuring the layout generation phase, users have the option to create a city layout of arbitrary sizes using the unbounded\nlayout generator, or they can utilize the rasterized data from OpenStreetMap directly. This flexibility allows users to choose\nbetween generating layouts from scratch or using existing map data as a starting point for their 3D city. Additionally, after\ngenerating the layout, users can draw masks on the canvas and regenerate the layout specifically for the masked regions.\nDuring the trajectory selection phase, users can draw camera trajectories on the canvas and customize camera step size,\nview angles, and altitudes. There are three types of camera trajectories available: orbit, point to point, and multiple keypoints.\nOnce selected, the camera trajectory can be previewed based on the generated city layout, allowing users to visualize how\nthe city will look from different perspectives before finalizing their choices.\nFinally, the cities can be rendered and stylized based on the provided city layout and camera trajectories.\n16\n"
  },
  {
    "title": "FACET: Fairness in Computer Vision Evaluation Benchmark",
    "link": "https://arxiv.org/pdf/2309.00035.pdf",
    "upvote": "13",
    "text": "FACET: Fairness in Computer Vision Evaluation Benchmark\nLaura Gustafson\nChloe Rolland\nNikhila Ravi\nQuentin Duval\nAaron Adcock\nCheng-Yang Fu\nMelissa Hall\nCandace Ross\nMeta AI Research, FAIR\nfacet@meta.com\nAbstract\nComputer vision models have known performance dis-\nparities across attributes such as gender and skin tone. This\nmeans during tasks such as classification and detection,\nmodel performance differs for certain classes based on the\ndemographics of the people in the image. These dispari-\nties have been shown to exist, but until now there has not\nbeen a unified approach to measure these differences for\ncommon use-cases of computer vision models. We present a\nnew benchmark named FACET (FAirness in Computer Vi-\nsion EvaluaTion), a large, publicly available evaluation set\nof 32k images for some of the most common vision tasks\n- image classification, object detection and segmentation.\nFor every image in FACET, we hired expert reviewers to\nmanually annotate person-related attributes such as per-\nceived skin tone and hair type, manually draw bounding\nboxes and label fine-grained person-related classes such\nas disk jockey or guitarist.\nIn addition, we use FACET\nto benchmark state-of-the-art vision models and present\na deeper understanding of potential performance dispari-\nties and challenges across sensitive demographic attributes.\nWith the exhaustive annotations collected, we probe models\nusing single demographics attributes as well as multiple at-\ntributes using an intersectional approach (e.g. hair color\nand perceived skin tone). Our results show that classifica-\ntion, detection, segmentation, and visual grounding mod-\nels exhibit performance disparities across demographic at-\ntributes and intersections of attributes. These harms sug-\ngest that not all people represented in datasets receive fair\nand equitable treatment in these vision tasks. We hope cur-\nrent and future results using our benchmark will contribute\nto fairer, more robust vision models. FACET is available\npublicly at https://facet.metademolab.com.\n1. Introduction\nThe ability of computer vision models to perform a wide\nrange of tasks is due in no small part to large, widely used\ndatasets. These large-scale datasets containing millions of\nCategory:\nGuitarist\nHair color: \n\u00a0\u00a0\u00a0\u00a0\nBlack/Brown\nHair type: \n\u00a0\u00a0\u00a0\u00a0\nWavy\nPerceived Age:\nMiddle\nPhoto is: \nUnderexposed\nVisibility: \nFace, torso visible\nPerceived skin tone:\n \nPerceived gender presentation:\u00a0\u00a0\nWith more maleness\nAdditional:\nTattoo, facial hair\n3          4         4          5         5          5          6 \nFigure 1: An example image and annotations from our\ndataset FACET. Every image in FACET contains annota-\ntions from expert reviewers on the primary class, sensi-\ntive attributes including perceived gender presentation, per-\nceived skin tone, and perceived age group, alongside addi-\ntional visual attributes like hair color and type, tattoos, etc.\n.\nimages often have image-level labels such as ImageNet [16]\nor object-level annotations found in datasets such as MS-\nCOCO [63] or Open Images [62]. Annotations are also used\nat the person-level, in datasets such as CelebA [64], UTK-\nFaces [98] and More Inclusive People Annotations (MIAP)\n[84]. These person-level annotations in particular enable a\nmore fine-grained analysis and evaluation of model perfor-\nmance across groups. Prior work using these person-level\nannotations to evaluate model fairness has shown that vision\nmodels learn societal biases and stereotypes, which neg-\natively impact performance and cause downstream harms\n[87, 101, 74, 90, 88]. This makes fairness datasets particu-\nlarly important as vision models continue to grow.\nOne weakness of existing fairness datasets is that they\nlack exhaustive and diverse demographic annotations that\narXiv:2309.00035v1  [cs.CV]  31 Aug 2023\nSize\n\u2013 32k images, 50k people\nEvaluation\nAnnotations\n\u2013 52-person related classes\n\u2013 bounding boxes around each person\n\u2013 person/hair/clothing labels for 69k masks\nProtected\nGroups\n\u2013 perceived skin tone\n\u2013 perceived age group\n\u2013 perceived gender presentation\nAdditional\nPerson\nAttributes\n\u2013 hair: color, hair type, facial hair\n\u2013 accessories: headscarf, face mask, hat\n\u2013 other: tattoo\nMiscellaneous\nAttributes\nlighting condition, level of occlusion\nTable 1: Statistics on size of FACET and person annotations\nincluding labels for classification (e.g. soldier, teacher) and\nattributes such as hair color and perceived skin tone.\ncan support multiple vision tasks. For instance, while Open\nImages More Inclusive People Annotations (MIAP) [84]\ncan be used for classification and detection, the labels are\nnot particularly diverse as only perceived gender presen-\ntation and perceived age group are labeled.\nImage-level\nclass labels are also sparse, with an incomplete set of true\npositives and true negatives per image. Another dataset,\nCelebA, contains many more person-level attributes but is\nprimarily for face localization. In addition, CelebA con-\ntains many subjective and potentially harmful attributes e.g.\nattractive, big lips, chubby [25].\nThese weaknesses can\ngreatly impact our ability to perform more fine-grained fair-\nness analyses.\nIn this paper, we present FACET (Fairness in Com-\nputer Vision Evaluation Benchmark), a large-scale evalu-\nation benchmark with exhaustive annotations for 32k im-\nages from Segment Anything 1 Billion (SA-1B) [59] la-\nbeled across 13 person attributes and 52 person classes. The\n13 attributes include examples such as perceived skin tone,\nhair type, perceived age group; the 52 person classes in-\nclude categories such as hairdresser and reporter. To en-\nsure the annotations are both high quality and labeled by a\ndiverse group of people, we used trained, expert annotators\nsourced from several geographic regions (North and Latin\nAmerica, Middle East, Africa, East and Southeast Asia).\nFACET enables a deeper analysis of potential fairness\nconcerns and model biases for specific demographic axes.\nWe can explore questions such as: 1) Are models bet-\nter at classifying people as skateboarder when their per-\nceived gender presentation has more stereotypically male\nattributes? 2) Are open-vocabulary detection models better\nat detecting backpackers who are perceived to be younger?\n3) Do standard detection models struggle to detect people\nwhose skin appears darker? 4) Are these problems mag-\nnified when the person has coily hair compared to straight\nhair? 5) Do performance discrepancies differ across the\ndetection and segmentation tasks? These questions illus-\ntrate a few examples of how model biases can be explored\nat a deep, intersectional level using the exhaustive annota-\ntions in FACET. We use FACET to evaluate multiple state-\nof-the-art vision models to understand their fairness on de-\nmographic attributes (perceived gender presentation, per-\nceived skin tone, perceived age group) as well as their ex-\nisting demographic biases. FACET is publicly available at\nhttps://facet.metademolab.com. .\nOur contributions include:\n\u2013 our\nnew\npublicly\navailable\nfairness\nbenchmark\nFACET, containing 32k images from Segment Any-\nthing 1 Billion (SA-1B) [59], manually annotated with\ndemographic and additional visual attributes labels by\nexpert annotators\n\u2013 52 person-related class labels and manually drawn\nbounding boxes for every annotated person in every\nimage (50k total people)\n\u2013 person, clothing or hair labels for 69k masks\n\u2013 a benchmark for using FACET to compare different\nmodels, showing quantitative results and qualitative\nanalyses on existing vision models using FACET\nFACET is an evaluation-only benchmark. Using any\nof the annotations for training is strictly prohibited.\n2. Related Work\nVision datasets that are annotated with apparent or self-\nreported demographic attributes are frequently used for\nstudying model fairness. Table 2 compares FACET to other\nannotated datasets.\nClassification Datasets such as [64, 98, 8, 57] are used to\nevaluate the gender and skin tone disparities in face recog-\nnition1. Gender Shades [8], for instance, showed that gen-\nder classification systems perform significantly worse on fe-\nmales compared to males and on darker skin compared to\nlighter skin using labels from annotators. These datasets\ncannot be used for tasks outside of facial recognition, e.g.\nobject detection or image classification.\nCasual Conver-\nsations [43] is a dataset used for videos; this dataset was\nused to highlight disparities across gender and skin tone\nfor the Deep Fake Detection challenge [23]. Geographic\nand income-based disparities have been evaluated as well\n[15, 89, 37], most commonly with the DollarStreet dataset\n[1, 80].\nDetection/Segmentation [101] generated gender annota-\ntions via captions for MS-COCO. [94] annotated a subsec-\ntion of pedestrians in BDD100k [97] for the task of pedes-\ntrian detection and found higher performance for lighter\nskin tones. However, these demographic annotations are of-\nten noisy and either lack annotator training or lack annota-\ntors altogether and are instead approximated from captions.\n1We retain the same language used in the original papers, which is\nbased on gender labels of the datasets that were audited.\nDataset\nDataset Size\nApparent or Self-Reported Attributes\nTask\n#/people #/images #/videos\n#/boxes\n#/masks\ngender\nage\nskin tone\nrace\nlighting\nadditional\nUTK-Face[98]\n20k\n20k\n\u2013\n\u2013\n\u2013\nYes\nYes\nNo\nYes\nNo\nNo\n\u2013\nFairFace[57]\n108k\n108k\n\u2013\n\u2013\n\u2013\nYes\nYes\nNo\nYes\nNo\nNo\n\u2013\nGender Shades[8]\n1.2k\n1.2k\n\u2013\n\u2013\n\u2013\nYes\nYes\nYes\nNo\nNo\nNo\n\u2013\nOpenImages MIAP[84]\n454k\n100k\n\u2013\n454k\n*\nYes\nYes\nNo\nNo\nNo\nNo\nC*DS*\n[94] annotations for BDDK 100k [97]\n16k\n2.2k\n\u2013\n16k\n*\nNo\nNo\nYes\nNo\nYes\nNo\nDS*\n[100] annotations for COCO [63]\n28k\n16k\n\u2013\n28k\n28k\nYes\nNo\nYes\nNo\nNo\nNo\nC*DS\nCasual Conversations v1[43]\n3k\nN/A\n45k\n\u2013\n\u2013\nYes\nYes\nYes\nNo\nYes\nYes\n\u2013\nCasual Conversations v2 [42]\n5.6k\nN/A\n26k\n\u2013\n\u2013\nYes\nYes\nYes\nNo\nYes\nYes\n\u2013\nOurs \u2013 FACET\n50k\n32k\n\u2013\n50k\n69k\nYes\nYes\nYes\nNo\nYes\nYes\nCDS\n* represents tasks/annotations that are not included in the fairness portion of the dataset, but are included in the overall dataset.\ne.g COCO has been used for multi-class classification [101, 92]\nTable 2: Tasks and attribute annotations comparing existing datasets to FACET. These existing datasets were designed for\nfairness evaluations for other use cases, which is not to suggest that they are limited in use. The tasks (CDS) considered\nare Classification of an image, Detection of a person or person-related objects, Segmentation of a person or person-related\nobjects. For classification, we do not include the classification task of classifying protected or non-protected attributes of\na person. For attributes, FACET does not include race as it is not a visually salient category, exacerbates bias [58] and\nmisclassification has been shown to cause emotional distress [10]. Bounding boxes are denoted as boxes.\nMore Inclusive Annotations for People (MIAP) [84], which\nis a subset of Open Images [62], is dataset that does focus\non high quality, more complete person-level demographic\nannotations for bounding boxes. While MIAP is similar to\nFACET, it only has annotations for perceived gender pre-\nsentation and perceived age group. FACET has far more\nexhaustive annotations spanning far more attributes. We\nhave 13 attributes, including demographic attributes (per-\nceived gender presentation, perceived age group, and per-\nceived skin tone) as well as additional attributes such image\nquality (lighting and occlusion) and physical presentation\n(e.g. hair type, accessories, tattoos, etc.).\nBest Practices Audits of popular computer vision datasets\nhave found gender artifacts [66], a lack of geographic\ndiversity [85], malignant stereotypes and NSFW content\n[12, 74, 6]. To combat these issues, there has been sig-\nnificant research about dataset development including tools\n[91], best practices for creating datasets [61, 82, 3, 52, 73,\n42, 54, 81, 19, 83] and designing annotation tasks with\ncrowdworkers [17, 14]. A large body of work also explored\nhow researchers should document the intended use and con-\nsiderations made when developing models [69], datasets\n[32, 76, 47, 67] and crowdsourced annotation tasks [21].\n3. Benchmark Method\nThe goal of our benchmark is to evaluate and analyze\nhow vision models perform across different demographic\nand additional attributes for different categories of people.\nThis analysis requires (1) images that contain people with a\ndiverse set of attributes and (2) images that contain people\nmatching a variety of person-related categories. We focus\non person-related categories such as occupations of people\nor person-related past-times, e.g. doctor, basketball player,\nstudent, backpacker, etc. We prioritized a diverse set of\ncategories for a more thorough analysis.\nTo generate the list of person-related categories, we use\nWordNet [68], which is a hierarchical database of language\nconcepts. Each language concept is a single node in the\nhierarchy. For instance, the concept apple is a node with\nparent edible fruit. We take the language concept person\nand treat all of its children as potential categories, following\n[96]. We filter out offensive synsets noted in [96] and [12],\ngenerating 1,239 candidate synsets. We trim this list to 52\ncategories using the considerations below. Figure 2 shows\na sample WordNet tree structure for two classes in FACET,\nand Figure 8 in Appendix A.2 shows the full hierarchy.\nConnection to ImageNet-21k (IN21k)\nTo ensure con-\nsistency with existing concepts used for computer vision\nmodel evaluation, we require our categories to overlap with\nthe taxonomy of the widely used ImageNet-21k (IN21k)\ndataset [79].\nThis approach has been used to select ob-\nject classes by other datasets [5] and follows previous work\n[34, 86]. This means models trained with IN21k can be\nevaluated out-of-the-box on FACET. IN21k is a long-tailed\ndataset, meaning many classes have very few images. We\nexclude categories with < 500 examples similar to [79] to\nensure that models pre-trained on IN21k will transfer seam-\nlessly to our evaluation set.\nConcept Selection\nIN21k has overlapping classes with\nvarying levels of specificity (i.e. surgeon is a subcate-\ngory of doctor). Following [49], we include classes with\nroughly the same \u201cbasic level.\u201d Using their findings of rel-\native feature importance for classifying \u201cbasic level\u201d, we\nlimit the depth in the WordNet hierarchy from the person\nnode to 5, as a proxy for level of specificity. To alleviate\nambiguity, we focus primarily on occupation/trade, sports,\nart, and leisure related categories of people. This leaves us\nwith 52 categories. Our final list of concepts is shown in\nTable 17 in the Appendix.\nPerson\nAdjudicator\nJudge\nOfficial\nReferee\nFigure 2: Example of WordNet tree structure relating the\nFACET classes referee and judge to the Person node.\n4. Attribute Selection: Demographic and Ad-\nditional Visual Attributes\nFACET includes both demographic attributes and addi-\ntional visual attributes. These exhaustively labeled, man-\nually annotated attributes for all images in the dataset al-\nlow for evaluation of model performance and robustness at\na fine-grained level. For example, we can investigate po-\ntential biases associated with a single attribute as well as\nat the intersection of multiple attributes. Intersectionality\nis a framework exploring how multiple attributes can actu-\nally magnify societal biases [13]; the exhaustive attributes\nin FACET means we can explore intersectional harms with\nrespect to model fairness as well. Examples questions we\ncan explore include \u201cDo models struggle to classify people\nwith tattoos?\u201d (single attribute) and \u201cDo models perform\nbetter for people with curly hair who appear to have per-\nceived lighter skin tones than to those with perceived darker\nskin tones?\u201d (intersection of attributes). See Appendix A.4\nfor the full list of attributes and their distributions.\n4.1. Demographic Attributes\nPerceived Skin Tone\nThe Monk Skin Tone Scale [70],\nshown in Figure 3, was developed specifically for the com-\nputer vision use case. We intentionally use the Monk Skin\nTone scale over the Fitzpatrick skin type [29], which was\ndeveloped as means for determining one\u2019s likelihood of\ngetting sunburn and lacks variance in darker skin tones\n[51, 72]. Fitzpatrick skin type has been shown to be un-\nreliable for image annotation [36].\nSkin tone as a spectrum. Skin tone is difficult to anno-\ntate2 and can vary based on the lighting of the photo [55].\nFor this reason, we annotate skin tone as a spectrum. We\ngather annotations from three different annotators, allowing\nannotators to select as many skin tone values they feel best\nrepresent the person. This gives us a distribution over var-\nious skin tones. We note that perceived skin tone is not a\nproxy for race or ethnicity, and should not be used as such.\n2Studies show even annotating one\u2019s own skin tone is difficult [27].\nFigure 3: Monk Skin Tone Scale[70], an inclusive scale that\nincludes 10 different skin tones.\nPerceived\nGender\nPresentation\nAnnotators\nselect\nwhether they perceive a person as having more stereotyp-\nically female attributes, having more stereotypically male\nattributes, or having attributes outside of the gender binary.\nWe annotate perceived gender presentation instead of gen-\nder, as gender cannot be determined purely from an image;\nattempting to do so can be harmful to groups who are mis-\ngendered [40]. A more thorough discussion is in Section\n6.4.\nPerceived Age Group\nWe have three distinct perceived\nage group groups \u2013 younger are people perceived to be un-\nder 25 years old; middle are people perceived to be be-\ntween 25-65 years old and older are people perceived to\nbe over 65 years old. This follows the recommendations of\n[42] which matches the United Nation\u2019s breakdown of age\n[2], but we collapse Adults 25-40 and Middle-age\nAdults 41-65 into one category middle. While it is im-\npossible to tell a person\u2019s true age from an image, these\nnumerical ranges are a rough guideline to delineate each\nperceived age group.\n4.2. Additional Attributes\nHair color and hair type\nBecause conv-nets are shown\nto recognize patterns and textures [30, 31, 33, 7] and hair\ntypes represent a range of different textures, we annotate\nthe hair color and hair type.\nPerceived lighting.\nAnnotators labeled the lighting con-\ndition on the person. This annotation is important in part\nbecause it heavily impacts perceived skin tone [55]. These\nannotations can also guide the difficulty of the classifi-\ncation/detection problem, as models have been shown to\nhave robustness vulnerabilities with respect to brightness\n[45, 53].\nAdditional attributes.\nWe also annotate additional items\nrelating to a person\u2019s appearance, using the recommenda-\ntions of [42]. We condense the recommendations to the fol-\nlowing list. These are facial hair, head scarf 3, hat/cap,\neyewear (eyeglasses/sunglasses), face masks 4, tattoos\nand a person\u2019s visibility .\n3The motivation for this annotation is from a finding of [86] that the\nconcept hijab is predicted far more frequently for images with perceived\nlighter skin tones in UTK-Faces [98] than for those with perceived darker\nskin tones. It is unknown if this is a source of bias, as it is unknown whether\nor not there was a hijab in the photo.\n4Many images in FACET include more face masks than prior works,\nsuch as ImageNet, due to the COVID-19 pandemic.\n5. Annotation Method\n5.1. Data Preprocessing\nFACET is composed of images from Segment Anything\n1 Billion (SA-1B). We preprocessed the dataset to create a\nrough pool of relevant examples (with a focus on high re-\ncall) before beginning the annotation process. We use cap-\ntion and tags for each image to create a candidate pool of\nimages to annotate. First, for each of the 52 selected cate-\ngories, we created a list of related terms. As each category\ncorresponds to a WordNet synset, we use the lemmas for\neach synset to generate the related terms per concept. For\ncategories with low frequency in the dataset, we supplement\nthe list with related nouns (ie flute when looking for exam-\nples of flutist). Separately, for each example (image, cap-\ntion, tag), we tokenize and lemmatize the caption using the\nNatural Language Toolkit (NLTK) [65]. For instance, the\ncaption \u201cThe person is playing the flute\u201d gets lemmatized\nto {person, play, flute} (without stop words). We compute\nthe overlap between the caption\u2019s lemmas + tags with the\nrelevant term lists for each of the 52 categories to approxi-\nmate which categories likely occur in each image. We select\nimages with the most overlap for annotation.\n5.2. Annotation Pipeline\nGiven the sensitivity of these labels, we took multiple\nsteps to ensure high-quality annotations. Annotators com-\npleted stage-specific training before beginning labeling and\nperceived skin tone annotations underwent multi-review.\nAnnotators could also mark cannot be determined for any\nimage where they could not perceive the attribute. See Fig-\nure 6 in Appendix A.2.\nStage 1:\nFiltering Images by Target Categories First, an-\nnotators are tasked with quickly filtering images based on\nwhether they contain people who belong to a subset of cat-\negories. Following the process described earlier in this sec-\ntion, we use the metadata for each image to create a shorter\nlist of likely categories per photo. We do not use any classi-\nfication or detection models to filter images to avoid poten-\ntial model biases that can skew the data distribution. Anno-\ntators note the number of people in each image who match\nthe specified categories. We exclude images marked with\nmore than 5 people matching the target categories, given\nthe time-intensive nature of annotating attributes for each\nperson. This stage eliminates roughly 80% of the candidate\nimages. For the remaining stages, we move from quick fil-\ntering to a focus on precision.\nStage 2:\nAnnotating Bounding Boxes Annotators are\ntasked with drawing bounding boxes around each person\nin the image that matches any of the target 52 categories.\nFor each bounding box, annotators mark a primary class, as\nwell as a secondary class if necessary. The primary and sec-\nondary class structure alleviates potential overlap between\ncategories. For example, a person playing the guitar and\nsinging can match the category labels guitarist and\nsinger. Furthermore, allowing two classes permits for\nrepresentation of visually ambiguous classes, e.g. a person\nin scrubs who could be a doctor or nurse.\nStage 3:\nAnnotating perceived skin tone We assign per-\nceived skin tone annotations to its own step separate from\nother attributes to allow us to aggregate annotations from\nmultiple raters. We choose to aggregate as one\u2019s own skin\ntone can affect the perceived skin tone of others [46, 28].\nIn this stage, annotators label the perceived skin tone of a\nspecified person using the Monk Skin Tone Scale [70] (see\nFigure 3). We ask annotators to select at least 2 adjacent\nvalues and aggregate the results across three annotators. We\nreport the number of times each skin tone was chosen.\nStage 4:\nAnnotating Remaining Attributes In the final\nstage, annotators label the remaining attributes (see Section\n4.2) for each person in the bounding boxes from Stage 2.\nStage 5:\nAnnotating SA-1B Masks As FACET images\ncome from Segment Anything 1 Billion (SA-1B), which has\nimages and masks, we label a subset of masks as person,\nclothing, hair. We do not collect exhaustive annota-\ntions for person-related masks in FACET; we focus on an-\nnotating masks for people who are fully visible, with an\nattempt to balance demographic attributes. More details are\ngiven in Appendix A.2.1.\n5.3. Annotator Diversity\nWe prioritized having a geographically diverse set of an-\nnotators following [56, 17] and sourced raters from varying\nregions to increase the diversity of annotations. Our annota-\ntors come from six different geographic regions to increase\nthe diversity of the annotations, with one country per re-\ngion. These regions (with country in parenthesis) include\nNorth America (United States), Latin American (Colom-\nbia), Middle East (Egypt), Africa (Kenya), Southeast Asia\n(Philippines) and East Asia (Taiwan). We show more fine-\ngrained breakdowns of annotators per region in Figure 4.\nWe aimed for a roughly balanced number of raters per re-\ngion but had disproportionate pass-rates of training across\nthe various regions. We further describe our annotation pro-\ncess and annotators sourced in Appendix A.3, and answer\nthe questions posed by CrowdWorkSheets [21] in Appendix\nC.\n5.4. FACET Statistics\nIn this section we summarize the attribute and image\nbreakdown of FACET. Table 3 shows the three demographic\nEgypt (14)\n8.5%\nPhilippines (40)\n24.2%\nUnited States (51)\n30.9%\nKenya (20)\n12.1%\nSpain (15)\n9.1%\nTaiwan (4)\n2.4%\nColombia (21)\n12.7%\nFigure 4: Breakdown of raters who passed training by per-\ncentage and by raw number, shown in parentheses.\nPerceived or\nApparent Attributes\n#/people\n%\n#/images\n%\ngender presentation\n\u2013 more stereotypically F\n10k\n21%\n8k\n26%\n\u2013 more stereotypically M\n33k\n67%\n23k\n72%\n\u2013 non-binary\n95\n<1%\n95\n<1%\n\u2013 unknown\n6k\n11%\n5k\n5%\nMonk Skin Tone\n\u2013 1 \u25a0\n5k\n10%\n4k\n13%\n\u2013 2 \u25a0\n20k\n41%\n15k\n48%\n\u2013 3 \u25a0\n26k\n53%\n19k\n61%\n\u2013 4 \u25a0\n27k\n54%\n20k\n63%\n\u2013 5 \u25a0\n22k\n44%\n17k\n54%\n\u2013 6 \u25a0\n16k\n33%\n13k\n40%\n\u2013 7 \u25a0\n9k\n18%\n7k\n23%\n\u2013 8 \u25a0\n5k\n10%\n4k\n13%\n\u2013 9 \u25a0\n3k\n6%\n2k\n7%\n\u2013 10 \u25a0\n1k\n3%\n1k\n3%\n\u2013 unknown\n18k\n37%\n13k\n42%\nage\n\u2013 younger\n9k\n18%\n7k\n23%\n\u2013 middle\n27k\n55%\n20k\n64%\n\u2013 older\n3k\n5%\n2k\n8%\n\u2013 unknown\n10k\n21%\n9k\n27%\n*Images can have multiple labels for each attribute, which is why num-\nbers may not sum to 100%. F=femaleness; M=maleness\nTable 3: Breakdown of representation of the demographic\ngroups in the evaluation set.\ngroups and their corresponding number of occurrences in\nthe evaluation set. The majority of perceived gender presen-\ntation annotations are people perceived to have more stereo-\ntypically male attributes, followed by people perceived as\nhaving more stereotypically female attributes.\nPerceived\nskin tone annotations essentially follow a normal distribu-\ntion; the majority of annotations are in the range of skin\ntones 3-6. Appendix A.4 details more statistics about the\nFACET benchmark including the number of people per\nclass and demographic attribute along with the frequency\nof additional attributes.\nFigure 5 shows an approximate\n500\n1000\n1500\n2000\n2500\n3000\n3500\nFigure 5: Approximate geographic distribution of the im-\nages in FACET.\ngeographic breakdown of the images in FACET. The geo-\ngraphic information was inferred from locations mentioned\nin the captions, so the distribution is approximate.\n6. Fairness Evaluations using FACET\nWe use FACET to evaluate fairness by measuring perfor-\nmance disparities across different attributes for a selection\nof state-of-the-art vision models. Given a model f, a perfor-\nmance metric recall, a set of concepts C, an attribute label l\nand a set of images IC\nl , we compute:\ndisparity = recall(f(l1, IC\nl1 , C))\n\u2212 recall(f(l2, IC\nl2 , C))\n(1)\nAs a concrete example, we can compute the disparity be-\ntween people perceived as younger (l1) versus people per-\nceived as older (l2) for the concept teacher (C = {teacher}).\nImages IC\nl1 and IC\nl2 are images of teachers who are per-\nceived to be younger and images of teachers who are per-\nceived to be older, respectively. Disparity > 1 indicates the\nmodel performs better for images with label l1 and dispar-\nity < 1 indicates the model performs better for label l2. A\nperfectly fair model evaluated with FACET would have a\ndisparity of 0, meaning it has the same performance across\nall images regardless of the associated attributes.\nFACET is unique for two key reasons:\n1. Exhaustive attribute and class level evaluation:\nFACET\u2019s annotations are exhaustive, meaning every\nperson who matches a class in every image is anno-\ntated across all attributes through a rigorous annotation\nprocess. Datasets that include only the person class\nlabel and/or very sparse attribute labels risk bias leak-\nage from the unlabeled people or difficulty performing\na deep analysis due to the lack of exhaustive labels.\nThese are not concerns with FACET.\n2. Annotations for multiple vision tasks: Because every\nimage is annotated with bounding boxes and person-\nrelated classes, multiple vision tasks can be evaluated\nand analyzed alongside the exhaustively annotated per-\nson attributes.\nDifference in Average Recalls\nPerson Class\nM \u2212 F\ngardener\n16.4\ncraftsman\n13.6\nlaborer\n10.3\nskateboarder\n8.8\nprayer\n8.8\nwaiter\n8.3\nspeaker\n5.4\nguitarist\n4.0\nsinger\n1.6\nlawman\n1.4\nPerson Class\nF \u2212 M\ndancer\n21.7\nretailer\n17.0\nreporter\n16.0\nnurse\n12.9\nstudent\n12.8\ngymnast\n8.5\npainter\n6.1\nhairdresser\n5.2\nclimber\n5.1\nhorseman\n4.5\nTable 4: CLIP\u2019s performance disparity for the classes with\nthe largest disparity across perceived gender presentation.\nThe classes on the left indicate better performance for im-\nages with people who are perceived as having more stereo-\ntypically male attributes; results on the right indicate bet-\nter performance for those perceived as having more stereo-\ntypically female attributes.\n6.1. Classification\nAre models better at classifying people as skateboarder\nwhen their perceived gender presentation has more\nstereotypically male attributes? To help answer this ques-\ntion and others like it, we evaluate standard image classi-\nfication models with the FACET class labels. For classi-\nfication evaluation, we only evaluate images with a single\nperson. For images where a single person is labeled with\nmultiple person classes, we treat both classes as valid la-\nbels. We evaluate classification using CLIP ViT-B/32 [77]\nin a zero-shot setting. The largest discrepancies for CLIP\non the perceived gender presentation axis are shown in Ta-\nble 4. Some of these classes parallel societal, gender-related\nbiases (e.g. higher performance on careers like nurses and\nhairdressers for those who are perceived with more female-\nness [35, 71]). We show further analysis of CLIP across\nother demographic groups in Appendix A.5. We also show\nhow we can use the FACET IN21k class overlap to evaluate\nan ImageNet21k pre-trained ViT [26].\n6.2. Person Detection & Segmentation\n6.2.1\nPerson Detection\nWe evaluate a Faster R-CNN model with a ResNet-50-FPN\nbackbone [78] pretrained on COCO. During evaluation, we\nonly keep the predicted boxes corresponding to the COCO\nperson class.\nWe treat the remaining boxes as class-\nagnostic, and we compute the average recall (AR) and mAR\n(mean average recall) metrics proposed in [50] with all pre-\ndicted boxes and measure performance across the demo-\ngraphic attributes. For person detection, we focus on evalu-\nating perceived skin tone and how model performance par-\nallels societal biases [22, 60].\nMonk Skin Tone (MST)\nmAR\nAR0.5\nAR0.75\n\u2013 1 \u25a0\n75.5\n98.4\n85.0\n\u2013 2 \u25a0\n75.0\n98.3\n84.0\n\u2013 3 \u25a0\n74.7\n98.3\n83.5\n\u2013 4 \u25a0\n74.4\n98.1\n83.0\n\u2013 5 \u25a0\n74.1\n98.2\n82.6\n\u2013 6 \u25a0\n73.9\n98.3\n82.5\n\u2013 7 \u25a0\n73.7\n98.2\n82.2\n\u2013 8 \u25a0\n73.7\n98.0\n82.5\n\u2013 9 \u25a0\n73.3\n97.3\n81.1\n\u2013 10 \u25a0\n72.6\n96.5\n80.4\nAverage Recall (AR), with IoU values as subscripts. mAR is\naveraged across IoUs from 0.5 to 0.95, in increments of 0.05.\nTable 5: Average recall (AR) on FACET for a ResNet-\n50 Faster R-CNN. The model has the best performance\nfor MST=1, which is the lightest skin tone, and the low-\nest performance for MST=9 and 10, which are the darkest\nskin tones. The largest disparity between skin tones is for\nAR0.75.\nDo standard detection models struggle to detect people\nwhose skin appears darker?\nWe compute the AR across\nthe predicted bounding boxes for perceived skin tone, as\nshown in Table 5. At every IoU for the ARs, the dark-\nest perceived skin tone has the lowest performance. The\ngap between the highest and lowest performance is over 4\npoints at AR0.75, which suggests Faster R-CNN does strug-\ngle more on precisely detecting those perceived with darker\nskin tones.\nDoes this problem magnify when, for instance, the per-\nson has coily compared to straight hair?\nWe dig deeper\ninto the previous results to investigate intersections of at-\ntributes.\nIn Table 6 we measure mAR per hair type for\npeople with the three lightest perceived skin tone versus\nthe three darkest perceived skin tone. This unearths sev-\neral concerning findings: for 456 hair types performance is\nhigher for the lighter skin tones than the darker skin tones.\nThese are fairly significant gaps; for instance, we see a\nnearly 10 point difference for hair type dreads at AR0.75.\nThis is a particularly interesting finding for two reasons.\nFirst, we see a 50x increase in the disparity for dreads across\nperceived skin tone from AR0.5 to AR0.75. This suggests\nFaster R-CNN can detect people with dreadlocks, but strug-\ngles to perform accurate localization as shown by the larger\ngap as the IoU threshold for AR increases. Second, dread-\nlocks are often associated with darker skin and a plethora\nof associated stereotypes [24, 9, 75, 4]). This means the\nlikely association between dreads and darker skin tones in\nthe training data interestingly combat this performance dis-\nparity.\nlighter = {1 \u25a0, 2 \u25a0, 3 \u25a0 } darker = {8 \u25a0, 9 \u25a0, 10 \u25a0 }\nHair \u2193\nSkin \u2192\nmAR\nAR0.5\nAR0.75\nlighter\ndarker\nlighter\ndarker\nlighter\ndarker\ncoily\n76.7\n73.4\n98.2\n98.5\n87.3\n80.9\ndreads\n77.1\n74.7\n97.9\n98.1\n94.8\n85.7\nbald\n78.1\n71.5\n99.0\n96.7\n87.8\n77.5\nstraight\n75.6\n76.1\n98.4\n99.1\n84.8\n85.6\ncurly\n75.0\n74.8\n98.5\n99.2\n84.7\n83.7\nwavy\n76.1\n75.8\n98.6\n99.1\n85.5\n84.8\nAverage Recall (AR), with IoU values as subscripts. mAR is av-\neraged across IoUs from 0.5 to 0.95, in increments of 0.05.\nTable 6: Average recall (AR) on FACET for a ResNet-50\nFaster R-CNN. We show performance for the intersection\nof hair type and perceived skin tone. Performance is higher\nfor lighter perceived skin tone for every hair type except\nstraight and wavy.\n6.2.2\nPerson Instance Segmentation\nWe evaluate a Mask R-CNN model with a ResNet-50-FPN\nbackbone [44] pretrained on COCO. In the same pattern as\nfor person detection, we only keep the predicted masks cor-\nresponding to the COCO person class, and compute AR\nin a class agnostic way. We use the IoU between predicted\nand ground truth masks for instance segmentation, opposed\nto between boxes used for detection.\nDo performance discrepancies differ across the detec-\ntion and segmentation task?\nWe compare potential dis-\ncrepancies across segmentation and detection of people. We\nevaluate Mask R-CNN for person detection and person seg-\nmentation separately. For consistency, we limit the evalua-\ntion for both detection and segmentation to the set of people\nwho have a mask annotation. We compare the patterns of\ndiscrepancies in AR across perceived gender presentation\nfor person detection and segmentation, as shown in Table 7.\nWe notice that for both detection and segmentation, the per-\nformance disparities are largest at AR0.75. We also observe\nslightly larger gaps in performance for detection compared\nto segmentation. In line with prior work [44], we find higher\nAR for person detection than instance segmentation. We\ndescribe the experimental setup in more detail in Appendix\nA.5.\n6.3. Open World Detection & Visual Grounding\n6.3.1\nOpen Vocabulary Detection\nNext we evaluate open vocabulary detection using Detic\n[102]. We describe the experimental setup in detail in the\nAppendix A.5. For Detic, we focus on perceived age group.\nmAR\nAR0.5\nAR0.75\nperceived gender\npresentation\nbox\nmask\nbox\nmask\nbox\nmask\n\u2013 more stereotypically\nmale attributes\n78.3 72.2 99.3 98.1 88.0\n84.6\n\u2013 more stereotypically\nfemale attributes\n75.6 70.8 99.0 97.5 84.7\n82.9\n\u2013 outside of gender\nbinary\n77.0 63.0 98.0 92.0 88.0 74.0\nAverage Recall (AR), with IoU values as subscripts. mAR is\naveraged across IoUs from 0.5 to 0.95, in increments of 0.05.\nTable 7: We compare the AR on FACET for a ResNet-50\nMask R-CNN across the person detection and person in-\nstance segmentation tasks. The candidates box dictates the\nAR for person detection, box proposals, and mask for seg-\nmentation, mask proposals.\nAre open-vocabulary detection models better at detect-\ning backpackers who are perceived to be younger?\nTo\nbe illustrative of disparities observed with FACET, we se-\nlected the three person-related classes with the biggest dis-\nparity between groups: trumpeter, backpacker and painter.\nDetic exhibits perceived age group-based performance dis-\nparities for all 3 categories. The disparities are large, with\na 15 point gap in mAR for the backpacker class. The dis-\nparities are also consistent across AR measurements for a\nspecific class. Unlike what we observed for Faster R-CNN\nwith perceived skin tone, we typically observe larger gaps\nfor AR0.5 than AR0.75. This suggest that in open-world de-\ntection, there is perhaps more of a discrepancy in the classi-\nfication/identifying a person for a given category than there\nis in the precision of the bounding box. We show all per-\nclass disparities as well as the mean disparities across all 52\nclasses in Table 15 in Appendix A.5.\n6.3.2\nVisual Grounding\nLastly, we evaluate visual grounding using OFA [93], a\nsequence-to-sequence vision-language model. We evaluate\nperceived age group disparities using three person classes\nwith large disparities which are nurse, gardener and gui-\ntarist. Results are in Table 15. OFA\u2019s largest disparity is\nnearly 27 points, observed in the nurse class. The dispari-\nties and best performing perceived age group label for every\nclass is consistent across all IoU values. We show the full\ntable of per-class disparities as well as the disparities aver-\naged across all classes in Table 15 in Appendix A.5.\n6.4. Limitations\nAs the development of datasets for fairness analysis be-\ncomes more common, approaches and recommendations\nfor how to do so in ethical and safe ways are being increas-\nPerson\nClass\nDetic (detection)\nOFA (visual grounding)\nmAR\nAR0.5\nAR0.75\nmAR\nAR0.5\nAR0.75\nbackpacker\ngardener\n\u2013 young\n45.4\n55.3\n47.4\n11.9\n32.6\n7.0\n\u2013 middle\n42.1\n51.7\n44.6\n18.3\n40.1\n14.6\n\u2013 older\n29.8\n35.3\n33.3\n27.9\n58.1\n24.2\ntrumpeter\nsolider\n\u2013 young\n22.8\n26.7\n25.6\n16.3\n9.2\n40.0\n\u2013 middle\n29.5\n34.8\n31.4\n16.3\n14.5\n33.8\n\u2013 older\n38.4\n45.5\n38.2\n1.3\n6.3\n0.0\ndrummer\nguitarist\n\u2013 young\n19.9\n24.9\n19.7\n19.8\n38.7\n18.5\n\u2013 middle\n26.3\n34.7\n27.6\n19.6\n35.5\n20.6\n\u2013 older\n34.2\n41.8\n35.8\n32.0\n56.7\n36.7\nAverage Recall (AR), with IoU values as subscripts. mAR is\naveraged across IoUs from 0.5 to 0.95, in increments of 0.05.\nTable 8: Per-class performance for Detic and OFA on a sub-\nset of FACET classes. The perceived age group with the\nhighest performance per class is bolded.\ningly explored [42, 32, 3, 83]. While we strongly believe\nFACET will help practitioners better understand sources of\nbias in their model, we note that translating real world con-\ncepts and demographic groups to dataset annotations is in-\nherently imperfect.\nFirst, while self-identification of concept classes and\nperson-related attributes is preferred [83, 99], our adapta-\ntion of an existing dataset requires external annotations. To\nreduce these potential biases, we use highly trained anno-\ntators and avoid automated labeling methods like adapting\nexisting captions, alt-text or model classifications [101, 92,\n99, 39, 38, 84, 8]. Second, while generating sets of labels\nfor each attribute, there is a trade-off between having more\nlabels (wider representation) and opting for fewer, higher\nfrequency labels (more statistical significance) [38]. This\nwas extensively considered, and we acknowledge that, as\nwith any paper using discrete labels, our labels for perceived\ngender presentation and perceived age group risk erasure\nof genders and ages that are not identifiable in our cate-\ngorization [20]. For skin tone, we follow the Monk Scale\n[70], which shows better inclusivity of darker skin tones.\nFor concept classes, we map to ImageNet classes to encour-\nage easy adoption and to ensure mutually exclusive classes.\nLastly, FACET and other fairness datasets are representative\nof the current time period and organizational infrastructure\nwithin which it was created [18, 32]. To address how this af-\nfects annotations and insights when performing evaluations,\nwe include in Appendix C our responses to the CrowdWork-\nSheets [21] for FACET.\n7. Discussion\nWe have seen rapid growth and impressive performance\ngains in computer vision across a number of tasks such as\nclassification, detection, segmentation and visual ground-\ning. Simultaneously, these models have learned societal bi-\nases and can perpetuate these harmful stereotypes in down-\nstream tasks. We present FACET, a vision fairness bench-\nmark that contains 32k annotated images of 50k people.\nPeople in the images are exhaustively labeled with demo-\ngraphic attributes, including perceived gender presentation,\nperceived skin tone and perceived age group, and additional\nattributes such as hair type and light exposure. Labeling de-\nmographic attributes requires thoughtful design, so we hired\nexpert annotators and prioritized clean annotations. In ad-\ndition to these attributes, FACET also has manual annota-\ntions for bounding boxes and person-related classes. These\nperson-related classes, such as hairdresser and farmer, over-\nlap with the ImageNet-21K (IN21k) vocabulary, meaning\nvision models that can be evaluated on IN21k can also\nseamlessly use FACET. We aimed to be extremely con-\nscious and respectful with our annotations, while also ac-\nknowledging that there are limitations with this and similar\nfairness datasets. We are publicly releasing FACET to en-\ncourage and lower the barrier to entry to evaluating vision\nmodels for potential biases. We propose several ways that\nresearchers can use FACET to evaluate their models for po-\ntential fairness concerns across a variety of common vision\ntasks.\nAcknowledgments\nWe\nwould\nlike\nto\nthank\nAdina\nWilliams, Priya Goyal, Zachary Winstrom, Austin Miller,\nNicolas Carion, Nisha Deo, Emily Dinan, Ahuva Gold-\nstand, Somya Jain, Alexander Kirillov, Mallika Malhotra,\nHanzi Mao, Ishan Misra, Laurens van der Maaten, and\nMary Williamson.\nReferences\n[1] Dollar Street.\nhttps://www.gapminder.org/\ndollar-street. Accessed: 2023-03-08. 2\n[2] Provisional Guidelines on Standard International Age\nClassifications. https://unstats.un.org/unsd/\npublication/SeriesM/SeriesM_74e.pdf.\nAc-\ncessed: 2023-03-08. 4\n[3] Jerone TA Andrews, Dora Zhao, William Thong, Aposto-\nlos Modas, Orestis Papakyriakopoulos, Shruti Nagpal, and\nAlice Xiang. Ethical considerations for collecting human-\ncentric image datasets. arXiv preprint arXiv:2302.03629,\n2023. 3, 9\n[4] Patricia A Banks. No dreadlocks allowed: Race, hairstyles,\nand cultural exclusion in schools. Hairstyles, and Cultural\nExclusion in Schools (February 12, 2021), 2021. 7\n[5] Andrei Barbu, David Mayo, Julian Alverio, William Luo,\nChristopher Wang, Dan Gutfreund, Josh Tenenbaum, and\nBoris Katz.\nObjectnet:\nA large-scale bias-controlled\ndataset for pushing the limits of object recognition mod-\nels. Advances in neural information processing systems,\n32, 2019. 3\n[6] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel\nKahembwe.\nMultimodal datasets:\nmisogyny, pornog-\nraphy,\nand\nmalignant\nstereotypes.\narXiv\npreprint\narXiv:2110.01963, 2021. 3\n[7] Wieland Brendel and Matthias Bethge.\nApproximating\ncnns with bag-of-local-features models works surprisingly\nwell on imagenet. arXiv preprint arXiv:1904.00760, 2019.\n4\n[8] Joy Buolamwini and Timnit Gebru. Gender shades: Inter-\nsectional accuracy disparities in commercial gender clas-\nsification.\nIn Conference on fairness, accountability and\ntransparency, pages 77\u201391. PMLR, 2018. 2, 3, 9\n[9] Ayana Byrd and Lori L Tharps. When black hair is against\nthe rules. The New York Times, 1, 2014. 7\n[10] Mary E. Campbell and Lisa Troyer.\nThe implica-\ntions of racial misclassification by observers.\nAmerican\nSociological Review, 72(5):750\u2013765, 2007. 3\n[11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. In European\nconference on computer vision, pages 213\u2013229. Springer,\n2020. 19\n[12] Kate Crawford and Trevor Paglen.\nExcavating ai: The\npolitics of training sets for machine learning.\nhttps:\n//excavating.ai/, September 2019. 3\n[13] Kimberl\u00b4e Crenshaw. Demarginalizing the intersection of\nrace and sex: A black feminist critique of antidiscrimina-\ntion doctrine, feminist theory and antiracist politics. u. Chi.\nLegal f., page 139, 1989. 4\n[14] Florian Daniel,\nPavel Kucherbaev,\nCinzia Cappiello,\nBoualem Benatallah, and Mohammad Allahbakhsh. Qual-\nity control in crowdsourcing:\nA survey of quality at-\ntributes, assessment techniques, and assurance actions.\nACM Computing Surveys (CSUR), 51(1):1\u201340, 2018. 3\n[15] Terrance De Vries, Ishan Misra, Changhan Wang, and Lau-\nrens Van der Maaten.\nDoes object recognition work for\neveryone?\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition Workshops,\npages 52\u201359, 2019. 2\n[16] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImageNet: A Large-Scale Hierarchical Image Database. In\nCVPR09, 2009. 1\n[17] Emily Denton, Mark D\u00b4\u0131az, Ian Kivlichan, Vinodkumar\nPrabhakaran, and Rachel Rosen. Whose ground truth? ac-\ncounting for individual and collective identities underlying\ndataset annotation. arXiv preprint arXiv:2112.04554, 2021.\n3, 5\n[18] Emily Denton, Alex Hanna, Razvan Amironesei, Andrew\nSmart, and Hilary Nicole. On the genealogy of machine\nlearning datasets: A critical history of imagenet. Big Data\n& Society, 8(2):20539517211035955, 2021. 9\n[19] Emily Denton, Alex Hanna, Razvan Amironesei, Andrew\nSmart, Hilary Nicole, and Morgan Klaus Scheuerman.\nBringing the people back in: Contesting benchmark ma-\nchine learning datasets. arXiv preprint arXiv:2007.07399,\n2020. 3\n[20] Hannah Devinney, Jenny Bj\u00a8orklund, and Henrik Bj\u00a8orklund.\nTheories of \u201dgender\u201d in nlp bias research, 2022. 9\n[21] Mark D\u00b4\u0131az, Ian Kivlichan, Rachel Rosen, Dylan Baker,\nRazvan Amironesei, Vinodkumar Prabhakaran, and Emily\nDenton.\nCrowdWorkSheets: Accounting for individual\nand collective identities underlying crowdsourced dataset\nannotation.\nIn 2022 ACM Conference on Fairness,\nAccountability, and Transparency. ACM, jun 2022. 3, 5,\n9, 24\n[22] Travis L Dixon and Keith B Maddox.\nSkin tone, crime\nnews, and social reality judgments: Priming the stereotype\nof the dark and dangerous black criminal 1.\nJournal of\nApplied Social Psychology, 35(8):1555\u20131570, 2005. 7\n[23] Brian Dolhansky,\nJoanna Bitton,\nBen Pflaum,\nJikuo\nLu, Russ Howes, Menglin Wang, and Cristian Canton-\nFerrer. The deepfake detection challenge dataset. CoRR,\nabs/2006.07397, 2020. 2\n[24] Saran Donahoo and Asia D Smith. Controlling the crown:\nLegal efforts to professionalize black hair. Race and Justice,\n12(1):182\u2013203, 2022. 7\n[25] Samuel Dooley, Ryan Downing, George Wei, Nathan\nShankar, Bradon Thymes, Gudrun Thorkelsdottir, Tiye\nKurtz-Miott, Rachel Mattson, Olufemi Obiwumi, Valeriia\nCherepanova, et al. Comparing human and machine bias in\nface recognition. arXiv preprint arXiv:2110.08396, 2021. 2\n[26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 7\n[27] Steven Eilers, Daniel Bach, Rikki Gaber, Hanz Blatt,\nYanina Guevara, Katie Nitsche, Roopal Kundu, and June\nRobinson. Accuracy of self-report in assessing fitzpatrick\nskin phototypes i through vi. JAMA dermatology (Chicago,\nIll.), 149, 09 2013. 4\n[28] Cynthia Feliciano. Shades of race: How phenotype and ob-\nserver characteristics shape racial classification. American\nBehavioral Scientist, 60(4):390\u2013419, 2016. 5\n[29] Thomas B Fitzpatrick. Soleil et peau. J. Med. Esthet., 2:33\u2013\n34, 1975. 4\n[30] Leon Gatys, Alexander S Ecker, and Matthias Bethge.\nTexture synthesis using convolutional neural networks.\nAdvances in neural information processing systems, 28,\n2015. 4\n[31] Leon A Gatys, Alexander S Ecker, and Matthias Bethge.\nTexture and art with deep neural networks. Current opinion\nin neurobiology, 46:178\u2013186, 2017. 4\n[32] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jen-\nnifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00b4e\nIii,\nand Kate Crawford.\nDatasheets for datasets.\nCommunications of the ACM, 64(12):86\u201392, 2021. 3, 9\n[33] Robert Geirhos, Patricia Rubisch, Claudio Michaelis,\nMatthias Bethge, Felix A. Wichmann, and Wieland Bren-\ndel.\nImagenet-trained CNNs are biased towards texture;\nincreasing shape bias improves accuracy and robustness.\nIn International Conference on Learning Representations,\n2019. 4\n[34] Priya Goyal, Adriana Romero Soriano, Caner Hazirbas,\nLevent Sagun, and Nicolas Usunier. Fairness indicators for\nsystematic assessments of visual feature extractors. arXiv\npreprint arXiv:2202.07603, 2022. 3\n[35] Anthony G Greenwald, Debbie E. McGhee, and Jordan\nL. K. Schwartz. Measuring individual differences in im-\nplicit cognition: the implicit association test. Journal of\npersonality and social psychology, 74 6:1464\u201380, 1998. 7\n[36] Matthew Groh, Caleb Harris, Roxana Daneshjou, Omar\nBadri, and Arash Koochek. Towards transparency in der-\nmatology image datasets with skin tone annotations by ex-\nperts, crowds, and an algorithm. Proc. ACM Hum.-Comput.\nInteract., 6(CSCW2), nov 2022. 4\n[37] Laura Gustafson, Megan Richards, Melissa Hall, Caner\nHazirbas, Diane Bouchacourt, and Mark Ibrahim.\nPin-\npointing why object recognition performance degrades\nacross income levels and geographies.\narXiv preprint\narXiv:2304.05391, 2023. 2\n[38] Melissa Hall, Bobbie Chern, Laura Gustafson, Denisse\nVentura, Harshad Kulkarni, Candace Ross, and Nicolas\nUsunier.\nTowards reliable assessments of demographic\ndisparities in multi-label image classifiers. arXiv preprint\narXiv:2302.08572, 2023. 9\n[39] Melissa Hall, Laura Gustafson, Aaron Adcock, Ishan\nMisra, and Candace Ross.\nVision-language models per-\nforming zero-shot tasks exhibit gender-based disparities.\narXiv preprint arXiv:2301.11100, 2023. 9\n[40] Foad Hamidi, Morgan Klaus Scheuerman, and Stacy M.\nBranham. Gender recognition or gender reductionism? the\nsocial implications of embedded gender recognition sys-\ntems.\nIn Proceedings of the 2018 CHI Conference on\nHuman Factors in Computing Systems, CHI \u201918, page\n1\u201313, New York, NY, USA, 2018. Association for Com-\nputing Machinery. 4\n[41] Moritz Hardt, Eric Price, and Nati Srebro.\nEquality of\nopportunity in supervised learning.\nAdvances in neural\ninformation processing systems, 29, 2016. 18\n[42] Caner Hazirbas, Yejin Bang, Tiezheng Yu, Parisa Assar,\nBilal Porgali, V\u00b4\u0131tor Albiero, Stefan Hermanek, Jacqueline\nPan, Emily McReynolds, Miranda Bogen, Pascale Fung,\nand Cristian Canton Ferrer. Casual conversations v2: De-\nsigning a large consent-driven dataset to measure algorith-\nmic bias and robustness, 2022. 3, 4, 9\n[43] Caner Hazirbas, Joanna Bitton, Brian Dolhansky, Jacque-\nline Pan, Albert Gordo, and Cristian Canton-Ferrer.\nTo-\nwards measuring fairness in AI: the casual conversations\ndataset. CoRR, abs/2104.02821, 2021. 2, 3\n[44] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961\u20132969, 2017. 8,\n19\n[45] Dan Hendrycks and Thomas G Dietterich. Benchmarking\nneural network robustness to common corruptions and sur-\nface variations. arXiv preprint arXiv:1807.01697, 2018. 4\n[46] Mark E. Hill. Race of the interviewer and perception of skin\ncolor: Evidence from the multi-city study of urban inequal-\nity. American Sociological Review, 67(1):99\u2013108, 2002.\n5\n[47] Sarah Holland, Ahmed Hosny, Sarah Newman, Joshua\nJoseph, and Kasia Chmielinski. The dataset nutrition label:\nA framework to drive higher data quality standards. arXiv\npreprint arXiv:1805.03677, 2018. 3\n[48] Sarah Holland, Ahmed Hosny, Sarah Newman, Joshua\nJoseph, and Kasia Chmielinski. The dataset nutrition label.\nData Protection and Privacy, 12(12):1, 2020. 21\n[49] Laura Hollink, Aysenur Bilgin, and Jacco van Ossen-\nbruggen. Is it a fruit, an apple or a granny smith? predicting\nthe basic level in a concept hierarchy, 2019. 3\n[50] Jan Hendrik Hosang, Rodrigo Benenson, Piotr Doll\u00b4ar, and\nBernt Schiele. What makes for effective detection propos-\nals? CoRR, abs/1502.05082, 2015. 7\n[51] John J. Howard, Yevgeniy B. Sirotin, Jerry L. Tipton,\nand Arun R. Vemury.\nReliability and validity of image-\nbased and self-reported skin phenotype metrics.\nCoRR,\nabs/2106.11240, 2021. 4\n[52] Ben Hutchinson, Andrew Smart, Alex Hanna, Emily Den-\nton, Christina Greer, Oddur Kjartansson, Parker Barnes,\nand Margaret Mitchell.\nTowards accountability for ma-\nchine learning datasets: Practices from software engineer-\ning and infrastructure. In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Transparency,\npages 560\u2013575, 2021. 3\n[53] Badr\nYoubi\nIdrissi,\nDiane\nBouchacourt,\nRandall\nBalestriero,\nIvan Evtimov,\nCaner Hazirbas,\nNicolas\nBallas, Pascal Vincent, Michal Drozdzal, David Lopez-\nPaz, and Mark Ibrahim.\nImagenet-x:\nUnderstanding\nmodel mistakes with factor of variation annotations. arXiv\npreprint arXiv:2211.01866, 2022. 4\n[54] Eun Seo Jo and Timnit Gebru.\nLessons from archives:\nStrategies for collecting sociocultural data in machine\nlearning. In Proceedings of the 2020 conference on fairness,\naccountability, and transparency, pages 306\u2013316, 2020. 3\n[55] Praveen Kakumanu, Sokratis Makrogiannis, and Nikolaos\nBourbakis. A survey of skin-color modeling and detection\nmethods. Pattern recognition, 40(3):1106\u20131122, 2007. 4\n[56] Shivani Kapania, Ding Wang, and Alex Taylor. A hunt for\nthe snark: Annotator diversity in data practices. 2023. 5\n[57] Kimmo Karkkainen and Jungseock Joo.\nFairface: Face\nattribute dataset for balanced race, gender, and age for\nbias measurement and mitigation.\nIn Proceedings of\nthe IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 1548\u20131558, 2021. 2, 3\n[58] Zaid Khan and Yun Fu. One label, one billion faces: Us-\nage and consistency of racial categories in computer vision.\nIn Proceedings of the 2021 acm conference on fairness,\naccountability, and transparency, pages 587\u2013597, 2021. 3\n[59] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment\nanything. arXiv preprint arXiv:2304.02643, 2023. 2, 19, 21\n[60] Heather M Kleider-Offutt, Alesha D Bond, and Shanna EA\nHegerty. Black stereotypical features: When a face type\ncan get you in trouble. Current Directions in Psychological\nScience, 26(1):28\u201333, 2017. 7\n[61] Bernard Koch, Emily Denton, Alex Hanna, and Jacob Gates\nFoster. Reduced, reused and recycled: The life of a dataset\nin machine learning research. In Thirty-fifth Conference\non Neural Information Processing Systems Datasets and\nBenchmarks Track (Round 2). 3\n[62] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Ste-\nfan Popov, Matteo Malloci, Alexander Kolesnikov, et al.\nThe open images dataset v4.\nInternational Journal of\nComputer Vision, 128(7):1956\u20131981, 2020. 1, 3\n[63] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and\nC Lawrence Zitnick. Microsoft coco: Common objects in\ncontext. In European conference on computer vision, pages\n740\u2013755. Springer, 2014. 1, 3\n[64] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. In Proceedings\nof the IEEE international conference on computer vision,\npages 3730\u20133738, 2015. 1, 2\n[65] Edward Loper and Steven Bird. Nltk: The natural language\ntoolkit. arXiv preprint cs/0205028, 2002. 5\n[66] Nicole Meister, Dora Zhao, Angelina Wang, Vikram V Ra-\nmaswamy, Ruth Fong, and Olga Russakovsky. Gender ar-\ntifacts in visual datasets. arXiv preprint arXiv:2206.09191,\n2022. 3\n[67] Milagros Miceli, Tianling Yang, Laurens Naudts, Martin\nSchuessler, Diana Serbanescu, and Alex Hanna. Document-\ning computer vision datasets: an invitation to reflexive data\npractices.\nIn Proceedings of the 2021 ACM Conference\non Fairness, Accountability, and Transparency, pages 161\u2013\n172, 2021. 3\n[68] George A Miller. Wordnet: a lexical database for english.\nCommunications of the ACM, 38(11):39\u201341, 1995. 3\n[69] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker\nBarnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer,\nInioluwa Deborah Raji, and Timnit Gebru. Model cards\nfor model reporting. In Proceedings of the conference on\nfairness, accountability, and transparency, pages 220\u2013229,\n2019. 3\n[70] Ellis Monk. Monk skin tone scale, 2019. 4, 5, 9\n[71] Brian A Nosek, Mahzarin R Banaji, and Anthony G Green-\nwald. Harvesting implicit group attitudes and beliefs from\na demonstration web site.\nGroup Dynamics:\nTheory,\nResearch, and Practice, 6(1):101, 2002. 7\n[72] U.K. Okoji, S.C. Taylor, and J.B. Lipoff. Equity in skin typ-\ning: why it is time to replace the Fitzpatrick scale. British\nJournal of Dermatology, 185(1):198\u2013199, 07 2021. 4\n[73] Amandalynne Paullada, Inioluwa Deborah Raji, Emily M\nBender, Emily Denton, and Alex Hanna. Data and its (dis)\ncontents: A survey of dataset development and use in ma-\nchine learning research. Patterns, 2(11):100336, 2021. 3\n[74] VU Prabhu and A Birhane. Large datasets: A pyrrhic win\nfor computer vision. arXiv preprint arXiv:2006.16923, 3,\n2020. 1, 3\n[75] Heather M Prendergast. Case 10: West indian/caribbean.\nDiversity and Inclusion in Quality Patient Care, pages 253\u2013\n258, 2016. 7\n[76] Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartans-\nson. Data cards: Purposeful and transparent dataset docu-\nmentation for responsible ai. In 2022 ACM Conference on\nFairness, Accountability, and Transparency, pages 1776\u2013\n1826, 2022. 3\n[77] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021. 7\n[78] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with re-\ngion proposal networks. Advances in neural information\nprocessing systems, 28, 2015. 7\n[79] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi\nZelnik-Manor. Imagenet-21k pretraining for the masses. In\nThirty-fifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 1), 2021.\n3, 18\n[80] William A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan\nKini, David Kanter, Vijay Janapa Reddi, and Cody Cole-\nman.\nThe dollar street dataset: Images representing the\ngeographic and socioeconomic diversity of the world. In\nThirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2022. 2\n[81] Nithya Sambasivan, Shivani Kapania, Hannah Highfill,\nDiana Akrong, Praveen Kumar Paritosh, and Lora Mois\nAroyo. \u201deveryone wants to do the model work, not the data\nwork\u201d: Data cascades in high-stakes ai. 2021. 3\n[82] Morgan Klaus Scheuerman, Alex Hanna, and Emily Den-\nton. Do datasets have politics? disciplinary values in com-\nputer vision dataset development. Proceedings of the ACM\non Human-Computer Interaction, 5(CSCW2):1\u201337, 2021.\n3\n[83] Morgan Klaus Scheuerman, Kandrea Wade, Caitlin Lustig,\nand Jed R. Brubaker. How we\u2019ve taught algorithms to see\nidentity: Constructing race and gender in image databases\nfor facial analysis.\nProc. ACM Hum.-Comput. Interact.,\n4(CSCW1), may 2020. 3, 9\n[84] Candice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio\nFerrari, and Caroline Pantofaru. A step toward more inclu-\nsive people annotations for fairness. In Proceedings of the\n2021 AAAI/ACM Conference on AI, Ethics, and Society.\nACM, jul 2021. 1, 2, 3, 9\n[85] Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood,\nJimbo Wilson, and D Sculley. No classification without rep-\nresentation: Assessing geodiversity issues in open data sets\nfor the developing world. arXiv preprint arXiv:1711.08536,\n2017. 3\n[86] Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius\nde Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv\nMahajan, Ross Girshick, Piotr Doll\u00b4ar, and Laurens van der\nMaaten. Revisiting weakly supervised pre-training of vi-\nsual perception models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 804\u2013814, 2022. 3, 4\n[87] Ryan Steed and Aylin Caliskan.\nImage representations\nlearned with unsupervised pre-training contain human-like\nbiases.\nIn Proceedings of the 2021 ACM conference on\nfairness, accountability, and transparency, pages 701\u2013713,\n2021. 1\n[88] Pierre Stock and Moustapha Ciss\u00b4e. Convnets and imagenet\nbeyond accuracy: Explanations, bias detection, adversar-\nial examples and model criticism. CoRR, abs/1711.11443,\n2017. 1\n[89] Sruthi Sudhakar, Viraj Prabhu, Olga Russakovsky, and\nJudy Hoffman.\nIcon\n2:\nReliably benchmarking pre-\ndictive inequity in object detection.\narXiv preprint\narXiv:2306.04482, 2023. 2\n[90] Schrasing Tong and Lalana Kagal. Investigating bias in im-\nage classification using model explanations. arXiv preprint\narXiv:2012.05463, 2020. 1\n[91] Angelina Wang,\nAlexander Liu,\nRyan Zhang,\nAnat\nKleiman, Leslie Kim, Dora Zhao, Iroha Shirai, Arvind\nNarayanan, and Olga Russakovsky. Revise: A tool for mea-\nsuring and mitigating bias in visual datasets. International\nJournal of Computer Vision, 130(7):1790\u20131810, 2022. 3\n[92] Angelina Wang and Olga Russakovsky.\nDirectional bias\namplification. CoRR, abs/2102.12594, 2021. 3, 9\n[93] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang. Unifying architectures, tasks, and modali-\nties through a simple sequence-to-sequence learning frame-\nwork. arXiv preprint arXiv:2202.03052, 2022. 8, 19\n[94] Benjamin Wilson, Judy Hoffman, and Jamie Morgenstern.\nPredictive inequity in object detection, 2019. 2, 3\n[95] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-\nmond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R\u00b4emi Louf, Morgan Funtowicz, et al. Trans-\nformers: State-of-the-art natural language processing. In\nProceedings of the 2020 conference on empirical methods\nin natural language processing: system demonstrations,\npages 38\u201345, 2020. 19\n[96] Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga\nRussakovsky.\nTowards fairer datasets.\nIn Proceedings\nof the 2020 Conference on Fairness, Accountability, and\nTransparency. ACM, jan 2020. 3, 14\n[97] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingy-\ning Chen, Fangchen Liu, Vashisht Madhavan, and Trevor\nDarrell.\nBdd100k: A diverse driving dataset for hetero-\ngeneous multitask learning. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), June\n2020. 2, 3\n[98] Song Yang Zhang, Zhifei and Hairong Qi.\nAge pro-\ngression/regression by conditional adversarial autoencoder.\nIn IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR). IEEE, 2017. 1, 2, 3, 4\n[99] Dora Zhao, Jerone T. A. Andrews, and Alice Xiang. Men\nalso do laundry: Multi-attribute bias amplification, 2022. 9\n[100] Dora Zhao, Angelina Wang, and Olga Russakovsky. Under-\nstanding and evaluating racial biases in image captioning.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 14830\u201314840, 2021. 3\n[101] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez,\nand Kai-Wei Chang. Men also like shopping: Reducing\ngender bias amplification using corpus-level constraints.\narXiv preprint arXiv:1707.09457, 2017. 1, 2, 3, 9\n[102] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp\nKr\u00a8ahenb\u00a8uhl, and Ishan Misra. Detecting twenty-thousand\nclasses using image-level supervision.\nIn Computer\nVision\u2013ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23\u201327, 2022, Proceedings, Part IX, pages\n350\u2013368. Springer, 2022. 8, 19\nA. Appendix\nA.1. Category Selection\nImageability and Salience of the Categories\nWhen\nchoosing the categories for FACET , we considered the \u201cim-\nageability\u201d of our concepts, from [96]. However, we found\nthat this did not transfer well to our use case. First, we\nfound that many of the \u2018highly imageable\u2019 concepts include\nclasses directly related to a demographic attribute.\n(For\ninstance, black woman (n09637339) has an imageability\nscore of 5 out of 5.) Additionally, many highly imageable\nconcepts are abstract, meaning they are easy to imagine but\nhard to classify. As an example, it is easy to imagine what\nthe concept mother may look like, but it is hard to de-\ntermine if someone is a \u201cmother\u201d from a photo. (Is any\nperson perceived with having stereotypically feminine at-\ntributes with a child in a photo presumed to be a mother?)\nClass Hierarchy and Representation\nWe show the full\nconnection of our chosen concepts in WordNet in their rela-\ntion to the Person synset. Figure 8 shows the full connec-\ntion of our chosen concepts in WordNet in their relation to\nthe Person synset. All relevant sub-trees and intermediate\nsynsets are shown. We can see that many of the classes in\nFACET share the same parent node. We also note that no\nclass in FACET is a direct descendant of another class. This\ndemonstrates that there is no overlap between classes. Table\n17 shows the representation of each class in the evaluation\nset.\nA.2. Annotation Pipeline\nWe describe in more detail the annotation pipeline we\nuse for FACET.\nA.2.1\nAnnotation Pipeline Design\nPreprocessing\nFigure 6 shows the pre-processing steps of\nthe captions to create the candidate set of images to anno-\ntate. We \u2018score\u2019 each caption for each category based on\nthe overlap of relevant words for the category and caption.\nWe sample captions with the highest \u2018score\u2019 per category.\nWe choose the candidate images for FACET from a set of\nroughly 6 million images.\nWe select a starting set of images for annotation such\nthat we expect the portion of images that pass stage 1 to\nbe roughly class balanced. To approximate the probability\nthat images with overlap per category are true positives, we\nsample 50 images per category and annotate the true posi-\ntives. We use this frequency to determine how much to over\nsample a specific category. As we continue the annotation\nprocess, for additional rounds, we sample images with over-\nlap based on the categories that are under-represented in the\ndataset thus far. We note that many categories did not have\n1a) Map to WordNet concepts\nterm \u2192 WordNet concept\nflute player \u2192 flutist\nreferee \u2192 referee\n\u00b7 \u00b7 \u00b7\n2) Get relevant terms list\nusing WordNet synsets\nand similar concepts\nflute player: {flute player, flute}\nreferee: {referee, ref}\n\u00b7 \u00b7 \u00b7\n1b) Tokenize &\nlemmatize caption\nA person is playing\nflute at a concert.\n\u2193\nperson, play, flute,\nconcert\n3) Compute overlap between lemmas and\nrelevant word lists for each category\nflutist overlap: flute\nreferee overlap: \u2205\noutput is a sorted list of categories (with scores) that are likely\nin image\ne.g. flute player, 1\nFigure 6: Label annotation pipeline: The preprocessing\nsteps before beginning the annotation pipeline. In 1a) we\nmap all of the person-related classes to concepts in Word-\nNet. We denote WordNet concepts in a different font. (See\nSection 3 for a full description on WordNet concepts and\nsynsets). In 1b) we tokenize and lemmatize the captions\nto produce a list of lemmas. In 2) for each of the 52 cat-\negories, we generate a relevant words list using WordNet\nsynsets. Lastly in 3) we compute overlap between the lem-\nmas and relevant words list and select images to annotate\nwhich have high overlap.\nenough images with matching relevant words and as such\nwe did not achieve equal representation of all categories.\nAnnotation Stages\nFigure 7 shows the four separate an-\nnotation tasks of the main annotation pipeline. Breaking\nthe annotation process into multiple sub-tasks allows for\nmore fine-grained control. For Stage 1, we focus on speed,\nand ask annotators to spend little time per task. To increase\nspeed, we group multiple images with the same target cate-\ngories into the a single task with a default value of 0 people\nmatch the categories, and ask the annotator to label each\nimage. We separate Stages 3 and 4, so that we can gather\nmultiple annotations for apparent skin tone only. We sepa-\nrate these stages from Stage 2 to simplify the task for anno-\ntators, such that they only need consider the perceived de-\nmographic attributes for one person at a time. Additionally,\nthis allows the annotators in later stages to quality assure\nthe annotations from earlier stages, as described in Section\nA.3.2.\nStage 1\nDo between 1 and 5 people match\none of the target categories?\nStage 2\nDraw bounding boxes\naround each person &\nlabel each box\nDiscard\nimage\nStage 3\nAnnotate\nperceived\nskin tone\nStage 4\nAnnotate\nremaining attributes\nyes\nno\nlow quality?\nFigure 7: Image annotation pipeline: The four stages of\nthe main annotation pipeline. The image on the left can be\nfully annotated; the image on the right does not contain the\ntarget categories and gets excluded after Stage 1. Dashed\nlines show paths that do not advance to the next stage.\nMask Annotations\nWe collect labels for SA-1B mask\nannotations separately after completing the annotation\npipeline. First, we select candidate people from FACET\nwith attempts made to balance the number of people per de-\nmographic group. Next, we select a candidate set of masks\nto annotate by collecting the set of masks inside the bound-\ning boxes for these people.\nFor each mask and FACET\nbounding box in which it resides, we asked annotators if the\nmask corresponds to the person\u2019s body, the person\u2019s hair\nor an item of clothing on the person for a given person, as\ndenoted by a bounding box. Annotators did not make any\nmodifications to the masks, e.g. change the shape. Annota-\ntors were told to only select a class if the mask covers the\nentire item; masks for a portion of the person, or part of\nitem of clothing were not labeled. Additionally, annotators\nwere only told to select a class if it met the label for the per-\nson described by the bounding box; masks for people, hair,\nclothing inside of the bounding box but belonging to a dif-\nferent person were not labeled. Thus, each mask is attached\nto a specific person in FACET. The breakdown of the masks\nper image is is given in A.4.2. The breakdown of masks per\ngiven demographic or additional attribute is given in Table\n18.\nA.3. Annotation Quality Assurance (QA)\nA.3.1\nAnnotator Quality Assurance using Training\nBefore completing any annotations used in FACET, anno-\ntators were trained for each stage separately. We trained\nannotators by giving them a sample set of tasks and com-\nparing their annotations to a known golden set. For Stages\n1, 3 and 4 (image filtering, perceived skin tone annotation,\nother perceived attributes annotation), annotators passed the\ntraining step if the recall of their annotations compared to a\nfixed golden set was above a quality threshold. This thresh-\nold was set for each stage depending on the difficulty of the\ntask. For Stage 2 (drawing bounding boxes), QA was done\nper annotator to assess the quality of boxes. We provided\nfeedback to annotators individually and only graduated the\nannotators once they addressed the feedback. A manual IoU\nthreshold of 0.85 between an annotator and the golden set\nwas used. Annotators under that threshold were not man-\nually reviewed, as we found that this correlated with ex-\ntremely poor box quality, and these annotators did not grad-\nuate training. Before feedback, we noticed that many anno-\ntators were drawing bounding boxes that included objects\nthe person was holding (e.g. guitar) as opposed to tightly\naround the person. After manual review and feedback, the\nquality of the annotations was much higher and consistent.\nA.3.2\nAnnotation Quality Assurance using Multi-\nReview and Quality Checks\nIn addition to implementing a multi-review process for the\nperceived skin tone annotations of each target person as dis-\ncussed in Section 5.2, we used Stage 3 to QA the bounding\nboxes drawn by the annotators. The annotators in Stage\n3 were asked whether the bounding box for the person in\nthe task was drawn tightly around the person. If \u2013 for any\nbounding box in the image \u2013 any of the three annotators\nmarked that the bounding box was not tight, the image was\nplaced back in Stage 2 of the pipeline to be re-annotated.\nA.4. Dataset Statistics and Breakdown\nA.4.1\nAttribute Representation\nWe detail the attribute breakdown for the remaining annota-\ntions in FACET. Table 9 details the statistics for the remain-\ning person annotations. Table 10 shows the results of the\nrobustness annotations with breakdowns on occlusion level\nand lighting condition.\nA.4.2\nImage statistics\nWe measure the statistics of images beyond specific at-\ntributes. Figure 9 shows the number of annotated people\nper image; less than one third of the images contain more\nthan one person. Figure 10 shows the person box size as\na fraction of total image size, broken down by the number\nof people in the image. All images in FACET are used for\ndetection. Images with only one person are used for classi-\nfication and visual grounding. For masks, the 69k labeled\nmasks span 18k people in 17k images of FACET. Each\nFigure 8: WordNet hierarchy of the FACET classes in relation to the Person synset. Classes are mapped to the Person\nsynset (center) by their hypernyms (parents). Classes (leaves) are marked in blue. Grey nodes correspond to an intermediate\nhyponyms.\nperson with associated labeled masks has an average of\n4 masks.\nA.5. Evaluation\nA.5.1\nDataset Setup\n\u2022 For image classification, we limit the evaluation to ex-\namples in FACET that only contain one person. This\nhelps alleviate ambiguities in performance. With this\nsetup, we can consider the performance of the model\non an image equivalent to performance of the model\non the image for a specific set of attributes. There are\n21k images in FACET that meet this criteria.\n\u2022 For person and open world detection, we use all exam-\nples in FACET.\n\u2022 For person segmentation, and the corresponding per-\nson detection baseline, we only use images and people\ninside each image that had a person mask - 11k peo-\nple.\n\u2022 For visual grounding, we only use examples in FACET\nwith one person, as OFA predicts only one bounding\nbox.\npeople\n%\nimages\n%\nHair color\nblack\n17k 34%\n13k 42%\nblonde\n3k\n6%\n3k\n8%\nbrown\n11k 22%\n9k 29%\nred/orange\n547\n1%\n518\n2%\ncolored\n269\n1%\n265\n1%\ngrey\n2k\n4%\n2k\n6%\nunknown\n20k 40%\n15k 46%\nHair type\nwavy\n9k 19%\n8k 26%\ncurly\n761\n2%\n735\n2%\nstraight\n19k 37%\n15k 47%\ncoily\n458\n1%\n435\n1%\ndreadlocks\n296\n1%\n282\n1%\nbald\n1k\n2%\n965\n3%\nunknown\n23k 45%\n16k 52%\nAdditional\nAnnotations\neyeware\n5k 11%\n5k 15%\nheadscarf\n2k\n5%\n2k\n6%\ntattoo\n705\n1%\n672\n2%\ncap\n14k 29%\n10k 33%\nfacial-hair\n6k 12%\n5k 17%\nmask\n3k\n6%\n2k\n7%\nTable 9: Statistics on the remaining person attributes: hair\ncolor, hair type, presence of additional features in FACET.\nAnnotators could mark multiple hair colors and types for a\nsingle person.\nlabel\npeople\n%\nimages\n%\nLighting\nCondition\noverexposed\n941\n2%\n890\n3%\nwell-lit\n40k 80%\n27k 85%\ndimly-lit\n11k 22%\n9k 28%\nunderexposed\n1k\n3%\n1k\n4%\nunknown\n878\n2%\n849\n3%\nVisibility\nminimal\n7k 15%\n7k 21%\nface\n15k 30%\n12k 38%\ntorso\n36k 73%\n25k 78%\nTable 10: Robustness annotations.\n1\n2\n3\n4\n5\n6\n0\n20\n40\n60\nnumber of people\npercent of images\nFigure 9: Histogram of number of people per image in\nFACET.\n10\n20\n30\n40\n50\n60\n70\n80\n0\n20\n40\n60\npercent of image\npercent of boxes\n(a) all images\n10\n20\n30\n40\n50\n60\n70\n80\n0\n10\n20\n30\n40\npercent of image\n(b) one person images\nFigure 10: Histogram of person bounding box size as a per-\ncentage of total image size.\nID\nCategory\nAttribute\nPrediction\n1\nDancer\n+F\nDancer\n2\nGardener\n+M\nGardener\n3\nDancer\nU\nGardner\n4\nDancer\n+M\nDancer\n5\nGuitarist\n+M\nGardener\n6\nDancer\n+M\nGuitarist\n7\nDancer\nNB\nDancer\n8\nDancer\n+F\nDancer\n10\nGardener\n+F\nGuitarist\n11\nDancer\n+F\nGuitarist\n12\nGuitarist\n+F\nGuitarist\n13\nDancer\n+F\nDancer\n14\nGardener\n+F\nGuitarist\n15\nGuitarist\nU\nDancer\nID\nCategory\nAttribute\nPrediction\n7\nDancer\nNB\nDancer\nrecall for non binary presentation, dancer 1.0\nID\nCategory\nAttribute\nPrediction\n4\nDancer\n+M\nDancer\n6\nDancer\n+M\nGuitarist\nID\nCategory\nAttribute\nPrediction\n1\nDancer\n+F\nDancer\n8\nDancer\n+F\nDancer\n11\nDancer\n+F\nGuitarist\n13\nDancer\n+F\nDancer\nrecall for more maleness, dancer is 0.5\nrecall for more femaleness, dancer 0.75\n1\nFigure 11: Example of how we score classification models\nfor FACET.\nA.5.2\nChoice of Metric\nWe choose to focus on recall as it allows us to only con-\nsider examples with a specific demographic attribute or set\nof attributes. We choose to avoid a metric that would take\ninto account false positives, as for some evaluations it is\nnot clear what a false positive would mean. For example,\nfor person detection, it is not obvious which demographic\nattribute a false positive would correspond to. What demo-\ngraphic attributes would we consider a predicted false pos-\nitive person to have? While it might make sense for images\nwith only one person to assume they had the same demo-\ngraphic attributes as the ground truth person in the photo, it\nis even less clear what the correct assumption would be to\nmake if there were multiple people in the photo. To avoid\nthis ambiguity, we focus on recall.\nClassification\nWe compare performance on a per-class\nbasis, as overall performance metrics can hide disparities\n\u2013 i.e. the model could have large biases but in opposite di-\nrections for two classes, which would yield a overall per-\nformance disparity of 0. We also don\u2019t want the metric to\nbe influenced by the prevalence of the class for the group.\nWe focus on the recall (R@1) for the group and class for\nour evaluation. This is equivalent to the accuracy for the\nspecific (class, attribute) pair. We do not account\nfor true negatives or false positives. Figure 11 visualizes\nour metric. We note that there are multiple approaches to\ncalculating a metric per class \u2013 e.g. we could also look at\nthe accuracy for the class when looking at all examples of\nthe protected group, which is why detail the specifics of our\nconsidered metric.\nAlignment with traditional fairness metrics\nThe differ-\nence in recall we measure is equivalent to equality of op-\nportunity [41] \u2013 larger differences in recall are further from\nequality of opportunity. Equalized odds [41] is an extension\nof this, which analyzes the true negative rate (TNR). For ex-\nample for CLIP, the largest difference in TNR between peo-\nple perceived as having more stereotypically male and more\nstereotypically female attributes is for retailers, with\n3.8 difference in TNR. This suggests that people perceived\nas having more stereotypically female attributes are over-\npredicted as retailers. The next largest difference be-\ntween these two groups for CLIP is for tennis player,\nwith a difference in TNR of \u22123.0, suggesting that people\nperceived as having more stereotypically male attributes are\nover-predicted as tennis players.\nA.5.3\nClassification\nExperimental Setup\nIn order to have maximum control\nover the experiment, we evaluate classification models on\nimages in FACET that only contain one annotated person.\nBy filtering out images with > 1 person, we are left with\n21k images. We look at the per-class disparities between\ntwo groups only if both groups have at least 50 examples.\nWe analyze CLIP based on recall.\nImageNet-21k Pretraining\nAs FACET categories over-\nlap with ImageNet classes, we can evaluate models trained\non ImageNet-21k out of the box. We take the max score\nover the FACET classes from the ImageNet class predic-\ntions. Table 11 shows a comparison of performance dis-\ncrepancies across perceived age group for CLIP ViT B/32\nand a ViT B/16 pre-trained on IN21k from [79].\nArchitecture Choice\nTable 12 compares performance\ndiscrepencies across architectures for models pre-trained on\nImageNet-21k[79].\nPerson\nClass\nCLIP ViT B/32\nViT B/16 IN21k\n#\nY\nM\nO\n#\nY\nM\nO\nTop for CLIP\nseller\n1\n57.5 72.8 86.2\n9\n47.2 53.4 59.3\nballplayer\n2\n60.6 75.5\n-\n2\n57.6 77.4\n-\nguitarist\n3\n70.3 80.2 65.5\n10 45.5 47.9 36.4\nspeaker\n4\n17.6 28.5 30.6\n4\n13.7 25.7 30.6\n5\nlaborer\n5\n49.0 52.7 61.7\n3\n48.1 52.9 66.0\nTop for ViT\npainter\n21 56.5 51.0 53.9\n1\n37.0 43.1 57.8\nballplayer\n2\n60.6 75.5\n-\n2\n57.6 77.4\n-\nlaborer\n5\n49.0 52.7 61.7\n3\n48.1 52.9 66.0\nspeaker\n4\n17.6 28.5 30.6\n4\n13.7 25.7 30.6\nguard\n7\n44.6 32.9\n-\n5\n48.5 31.7\n-\nTable 11: Per-class performance for CLIP and a ViT pre-\ntrained on ImageNet-21k. A subset of FACET classes are\nshown. The perceived age groups with the highest perfor-\nmance discrepancy per class are bolded. (Y is young, M is\nmiddle, O is older). The top five classes with the biggest\ndiscrepancies per model are shown. # corresponds to the\nrank for class in terms of magnitude of the discrepancy.\nLower number indicates larger discrepancy. We note that\nmost classes are in both of the models\u2019 top 10 classes with\nthe largest discrepancies,; 2 classes are in both models\u2019 top\n5. Recall for class and perceived age group pairings with\nless than 50 samples are not reported.\nPerson\nClass\nResNet IN21k\nViT IN21k\n#\nY\nM\nO\n#\nY\nM\nO\nTop for\nResNet\nlaborer\n1\n35.6 38.1 55.3\n3\n48.1 52.9 66.0\nguard\n2\n49.5 30.5\n6\n48.5 31.7\npainter\n3\n38.9 35.9 53.9\n1\n37.0 43.1 57.8\nballplayer\n4\n62.1 79.3\n2\n57.6 77.4\ncraftsman\n5\n67.2 78.4 81.8\n12 74.6 78.7 81.8\nTop for ViT\npainter\n3\n38.9 35.9 53.9\n1\n37.0 43.1 57.8\nballplayer\n4\n62.1 79.3\n2\n57.6 77.4\n-\nlaborer\n1\n35.6 38.1 55.3\n3\n48.1 52.9 66.0\nspeaker\n15 20.6 25.9 24.6\n4\n13.7 25.7 30.6\nguard\n2\n49.5 30.5\n5\n48.5 31.7\n-\nTable 12:\nPer-class performance for a ViT B/16 and\nResNet-50 pre-trained on ImageNet-21k.\nA subset of\nFACET classes are shown. The perceived age groups with\nthe highest performance discrepancy per class are bolded.\n(Y is young, M is middle, O is older). The top five classes\nwith the biggest discrepancies per model are shown. # cor-\nresponds to the rank for class in terms of magnitude of the\ndiscrepancy. Lower number indicates larger discrepancy.\nRecall for class and perceived age group pairings with less\nthan 50 samples are not reported.\nA.5.4\nPerson Detection\nWe use a pre-trained Faster R-CNN with a ResNet50 FPN\nbackbone pretrained on COCO for person detection.\nDemographic Group\nmAR AR0.5 AR0.75\nperceived gender presentation\n\u2013 more stereotypically maleness\n74.4\n97.8\n83.1\n\u2013 more stereotypically femaleness\n72.2\n97.9\n80.7\n\u2013 outside of gender binary\n71.2\n97.9\n76.8\nperceived age group\n\u2013 younger\n73.9\n98.3\n82.6\n\u2013 middle\n74.3\n98.0\n83.1\n\u2013 older\n74.8\n98.5\n84.5\nTable 13: Average recall (AR) on FACET for a ResNet50\nFaster R-CNN. Mean AR (mAR) averages across IoUs from\n0.5 to 0.95 in increments of 0.05; AR0.5 and AR0.75 refer to\nIoU at 0.5 and 0.75.\nMonk Skin Tone (MST) mAR AR0.5 AR0.75\n1 \u25a0\n85.4\n99.0\n93.3\n2 \u25a0\n84.6\n98.8\n92.1\n3 \u25a0\n84.4\n98.7\n91.6\n4 \u25a0\n84.2\n98.6\n91.3\n5 \u25a0\n84.0\n98.6\n91.2\n6 \u25a0\n84.0\n98.7\n91.2\n7 \u25a0\n83.8\n98.6\n91.1\n8 \u25a0\n84.1\n98.6\n91.5\n9 \u25a0\n83.6\n98.6\n90.9\n10 \u25a0\n82.8\n98.2\n90.1\nTable 14: Average recall (AR) on FACET for a ResNet50-\nbackbone DETR model. Mean AR (mAR) averages across\nIoUs from 0.5 to 0.95 in increments of 0.05; AR0.5 and\nAR0.75 refer to IoU at 0.5 and 0.75.\nAdditional Results\nTable 13 shows person detection re-\nsults across perceived gender presentation and perceived\nage group. Table 14 shows person detection results for a\nDETR [11] model with a ResNet50 backbone for perceived\nskin tone.\nA.5.5\nPerson Segmentation\nWe use a Mask R-CNN [44] with a ResNet50 FPN back-\nbone pretrained on COCO for person detection and instance\nsegmentation. For this experiment, we only evaluate peo-\nple in images if they have a mask annotated as person as\nwell. This leaves us with 11k examples (people). For boxes,\nwe compute the IoU of the predicted box to the human-\nlabeled bounding box in FACET. For masks, we compute\nthe IoU of the predicted mask to the Segment Anything-\ngenerated, annotator verified, mask in Segment Anything\n1 Billion (SA-1B) [59]. Annotators verified and labelled\nthe mask as person, and were instructed only to do so if\nthe mask was around the entire person (similar to bounding\nboxes in FACET). Annotators did not make any updates to\nthe mask boundary.\nA.5.6\nOpen World Detection\nExperimental Setup\nWe use Detic [102] trained on\nIN21-k with a SWIN-B backbone for open world detection.\nFor the CLIP embeddings, we use the prompt \u2018a person\nwho is a {}\u2019 opposed to the \u2018a {}\u2019 used in the origi-\nnal paper. As we focus on recall, we do not use a confi-\ndence threshold for DETIC\u2019s predictions. Similarly we al-\nlow multiple class predictions per box. We take the top 100\npredictions per image to compute AR.\nAdditional Results\nTable 15 shows the per class dispari-\nties for all classes for perceived age group.\nA.5.7\nVisual Grounding\nWe evaluated OFA [93]. For OFA, we used the pretrained\nversion OFAlarge in the HuggingFace Transformers library\n[95]; we did not perform any additional finetuning.\nWe\nused beam-search with 5 beams, top-p=0.6 and limited\nthe generation to a maximum of 100 new tokens.\nWe\nprompted OFA with the input (e.g. \u201cWhich region does the\ntext {person class} describe?\u201d). Because OFA pro-\nduces a single bounding box per prompt, we only evaluated\nimages that contained no more than one person instance per\nperson class. 7858 images were excluded because they con-\ntained multiple instances per class. We show the average\nrecall across different IoUs and for different perceived age\ngroup labels in Table 15.\nmAR\nAR0.5\nAR0.75\nyoung\nmiddle\nolder\nyoung\nmiddle\nolder\nyoung\nmiddle\nolder\nastronaut\n64.0\n70.2\n-\n80.0\n85.3\n-\n60.0\n73.7\n-\nbackpacker\n45.4\n42.1\n29.8\n55.3\n51.7\n35.3\n47.4\n44.6\n33.3\nballplayer\n43.8\n45.4\n61.8\n46.7\n48.3\n63.6\n45.8\n46.9\n63.6\nbartender\n81.7\n75.4\n-\n83.3\n85.4\n-\n83.3\n80.5\n-\nbasketball player\n61.2\n66.7\n-\n69.0\n74.7\n-\n64.9\n70.4\n-\nboatman\n69.4\n59.9\n64.0\n87.7\n79.1\n82.3\n77.8\n65.3\n68.8\ncarpenter\n67.9\n64.8\n81.7\n71.4\n73.5\n91.7\n71.4\n68.9\n87.5\ncheerleader\n13.3\n12.7\n-\n15.6\n13.5\n-\n14.8\n12.6\n-\nclimber\n76.6\n74.4\n67.5\n91.9\n92.4\n75.0\n81.8\n81.0\n75.0\ncomputer user\n72.9\n66.3\n68.4\n81.0\n77.8\n73.7\n76.3\n67.5\n68.4\ncraftsman\n44.5\n47.0\n56.9\n48.6\n52.1\n61.9\n44.8\n48.3\n58.6\ndancer\n77.2\n71.1\n75.6\n91.4\n85.4\n87.5\n83.7\n77.5\n78.1\ndisk jockey\n77.2\n68.4\n-\n82.1\n78.8\n-\n79.1\n72.5\n-\ndoctor\n74.6\n77.4\n75.7\n86.2\n88.7\n81.0\n76.6\n79.8\n78.6\ndrummer\n19.9\n26.3\n34.2\n24.9\n34.7\n41.8\n19.7\n27.6\n35.8\nelectrician\n56.3\n51.4\n48.6\n62.8\n62.5\n57.1\n62.8\n54.4\n57.1\nfarmer\n81.5\n81.1\n85.4\n95.9\n96.6\n99.1\n86.9\n88.4\n93.0\nfireman\n86.3\n76.4\n76.4\n96.2\n90.1\n85.7\n90.4\n82.6\n85.7\nflutist\n32.1\n40.5\n51.0\n35.4\n47.5\n54.8\n35.4\n43.7\n54.8\ngardener\n82.3\n78.6\n86.8\n98.3\n94.7\n100.0\n90.0\n84.4\n97.3\nguard\n81.9\n80.2\n88.5\n94.3\n90.6\n97.5\n89.4\n87.2\n95.0\nguitarist\n75.9\n79.3\n79.5\n90.5\n93.7\n95.1\n80.0\n84.4\n86.3\ngymnast\n87.7\n85.5\n-\n96.2\n95.6\n-\n92.4\n89.9\n-\nhairdresser\n76.8\n79.4\n79.0\n94.1\n96.9\n92.9\n82.4\n79.9\n83.3\nhorseman\n70.9\n62.1\n64.5\n85.4\n75.7\n80.0\n77.2\n67.9\n70.0\njudge\n25.7\n31.3\n28.3\n28.6\n35.3\n33.3\n28.6\n33.8\n33.3\nlaborer\n75.3\n73.1\n74.4\n88.4\n85.8\n86.1\n79.9\n78.9\n79.9\nlawman\n71.5\n70.1\n67.1\n79.0\n77.7\n74.3\n75.5\n74.6\n70.6\nlifeguard\n41.8\n46.1\n52.5\n51.7\n54.9\n62.5\n47.5\n49.8\n62.5\nmachinist\n60.0\n49.9\n41.1\n63.9\n56.5\n44.4\n63.9\n52.2\n44.4\nmotorcyclist\n57.9\n52.7\n51.9\n81.6\n78.2\n69.2\n60.9\n54.2\n57.7\nnurse\n83.4\n81.5\n81.7\n95.6\n93.9\n91.3\n90.5\n86.1\n82.6\npainter\n54.0\n58.9\n68.6\n60.8\n66.3\n73.8\n58.2\n62.3\n73.8\npatient\n64.1\n66.9\n67.1\n87.0\n85.6\n86.5\n65.6\n69.2\n68.3\nprayer\n82.8\n83.0\n85.2\n96.0\n95.2\n95.2\n89.0\n89.5\n89.5\nreferee\n70.2\n77.5\n84.9\n75.5\n85.3\n91.4\n73.6\n80.9\n88.6\nrepairman\n71.2\n61.7\n65.2\n77.6\n69.7\n71.0\n75.0\n65.5\n69.6\nreporter\n21.7\n22.9\n25.0\n23.7\n25.7\n29.2\n22.4\n23.7\n25.0\nretailer\n33.3\n35.0\n52.2\n40.9\n43.2\n59.5\n33.6\n38.6\n54.1\nrunner\n90.9\n85.9\n91.1\n99.2\n95.2\n100.0\n97.7\n90.6\n100.0\nsculptor\n74.5\n73.0\n85.0\n81.8\n82.4\n95.8\n77.3\n77.6\n83.3\nseller\n73.0\n73.2\n74.5\n87.4\n87.0\n88.5\n82.0\n79.0\n80.8\nsinger\n80.6\n80.9\n85.0\n88.8\n88.2\n96.1\n85.0\n85.5\n88.2\nskateboarder\n40.7\n43.1\n-\n45.1\n46.5\n-\n43.4\n46.1\n-\nsoccer player\n81.8\n82.9\n-\n90.6\n91.7\n-\n86.2\n87.1\n-\nsoldier\n65.1\n63.4\n51.4\n72.8\n72.7\n56.8\n69.7\n68.3\n54.1\nspeaker\n83.0\n80.8\n85.1\n89.3\n88.5\n93.1\n87.7\n85.6\n89.1\nstudent\n60.6\n71.1\n-\n69.8\n80.9\n-\n64.6\n74.4\n-\nteacher\n83.4\n81.0\n80.0\n96.6\n90.3\n87.5\n93.1\n85.8\n87.5\ntennis player\n94.2\n93.8\n-\n98.9\n98.9\n-\n97.2\n97.8\n-\ntrumpeter\n22.8\n29.5\n38.4\n26.7\n34.8\n45.5\n25.6\n31.4\n38.2\nwaiter\n76.2\n77.6\n-\n92.4\n92.9\n-\n83.3\n82.5\n-\navg\n64.6\n64.0\n68.2\n74.1\n74.4\n76.4\n68.6\n67.9\n72.4\n(a) Results for Detic\nmAR\nAR0.5\nAR0.75\nyoung\nmiddle\nolder\nyoung\nmiddle\nolder\nyoung\nmiddle\nolder\n0.0\n30.3\n-\n0.0\n51.4\n-\n0.0\n32.9\n-\n7.7\n5.9\n11.5\n15.5\n11.2\n22.0\n6.8\n5.8\n9.8\n43.9\n38.0\n58.0\n72.7\n67.2\n100.0\n50.6\n41.5\n80.0\n0.0\n12.5\n-\n0.0\n25.0\n-\n0.0\n8.3\n-\n26.2\n24.2\n-\n46.4\n41.3\n-\n25.0\n26.0\n-\n1.4\n1.9\n1.5\n4.5\n6.2\n8.1\n0.6\n0.9\n0.0\n0.0\n2.4\n0.0\n0.0\n5.6\n0.0\n0.0\n2.2\n0.0\n20.0\n12.0\n-\n41.9\n20.0\n-\n16.1\n15.0\n-\n0.0\n1.2\n0.0\n0.0\n2.5\n0.0\n0.0\n0.8\n0.0\n7.9\n5.9\n8.6\n12.9\n10.3\n14.3\n8.2\n6.2\n7.1\n33.8\n39.1\n40.7\n55.2\n62.6\n66.9\n37.9\n45.7\n47.6\n37.6\n32.0\n24.3\n68.8\n55.7\n57.1\n39.0\n37.1\n28.6\n3.5\n3.8\n-\n6.8\n6.5\n-\n4.1\n3.0\n-\n33.6\n30.9\n38.0\n55.2\n52.1\n60.8\n40.2\n33.6\n45.1\n5.0\n3.8\n0.7\n9.2\n8.1\n1.7\n4.6\n3.3\n0.0\n0.0\n1.2\n0.0\n0.0\n1.6\n0.0\n0.0\n1.6\n0.0\n6.2\n5.0\n6.6\n12.8\n9.9\n13.7\n5.1\n4.4\n5.5\n14.0\n14.7\n22.0\n26.7\n32.9\n60.0\n13.3\n12.5\n20.0\n15.0\n10.5\n11.7\n31.8\n19.9\n20.8\n9.1\n9.9\n12.5\n11.9\n18.3\n27.9\n32.6\n40.1\n58.1\n7.0\n14.6\n24.2\n14.1\n15.2\n19.2\n34.0\n31.9\n38.5\n9.6\n12.5\n11.5\n19.8\n19.6\n32.0\n38.7\n35.5\n56.7\n18.5\n20.6\n36.7\n10.0\n8.5\n-\n19.1\n17.1\n-\n9.8\n9.1\n-\n15.2\n13.3\n12.1\n28.0\n24.3\n23.5\n12.0\n12.8\n14.7\n13.4\n14.5\n11.0\n36.5\n38.8\n30.0\n3.8\n5.4\n0.0\n\u2013\n10.4\n0.0\n\u2013\n25.0\n0.0\n\u2013\n3.6\n0.0\n23.1\n21.9\n28.9\n44.0\n46.2\n58.6\n22.0\n17.4\n24.3\n20.2\n21.1\n22.6\n42.0\n43.1\n46.2\n18.3\n18.8\n21.5\n7.5\n7.0\n0.0\n19.7\n17.9\n0.0\n2.8\n5.2\n0.0\n21.7\n21.3\n23.3\n34.8\n35.5\n41.7\n26.1\n25.0\n25.0\n21.9\n15.5\n19.2\n50.0\n37.0\n37.5\n12.3\n9.6\n20.8\n31.8\n24.8\n34.5\n52.2\n43.6\n50.0\n37.2\n26.0\n40.0\n18.0\n15.6\n17.7\n30.1\n29.3\n27.6\n23.3\n16.1\n20.4\n28.5\n26.5\n26.6\n50.3\n47.6\n45.2\n29.7\n27.6\n28.0\n0.0\n2.7\n2.8\n0.0\n5.5\n4.3\n0.0\n1.8\n2.9\n19.6\n20.4\n21.4\n40.8\n40.1\n45.7\n16.3\n19.8\n22.9\n20.1\n17.9\n17.0\n39.5\n32.8\n30.4\n18.4\n19.1\n17.4\n9.2\n5.2\n4.5\n19.7\n13.0\n6.9\n7.0\n3.9\n3.4\n1.0\n2.5\n3.1\n2.8\n6.5\n6.9\n0.0\n1.5\n3.4\n7.5\n8.3\n0.0\n21.6\n21.1\n0.0\n3.9\n4.7\n0.0\n2.4\n2.3\n0.0\n5.9\n5.1\n0.0\n0.0\n3.1\n0.0\n7.8\n8.2\n9.8\n16.8\n16.6\n21.1\n6.2\n6.9\n8.8\n5.1\n3.9\n1.7\n10.3\n7.3\n5.8\n4.5\n3.6\n0.0\n21.5\n23.9\n\u2013\n46.9\n49.7\n\u2013\n18.8\n22.1\n\u2013\n26.7\n22.6\n\u2013\n49.1\n42.8\n\u2013\n24.6\n21.4\n\u2013\n16.3\n16.3\n1.2\n40.0\n33.8\n6.2\n9.2\n14.5\n0.0\n2.0\n1.7\n2.1\n4.6\n3.2\n3.7\n1.9\n1.7\n1.6\n29.0\n25.3\n0.0\n51.9\n44.7\n0.0\n33.8\n25.0\n0.0\n28.1\n22.2\n15.0\n51.6\n39.8\n50.0\n29.0\n24.8\n0.0\n32.5\n33.8\n-\n60.0\n62.2\n-\n32.7\n34.4\n-\n5.3\n5.1\n3.6\n11.6\n10.3\n5.1\n2.3\n5.7\n5.1\n5.2\n4.2\n-\n10.4\n8.6\n-\n4.2\n4.0\n-\n14.7\n14.5\n14.0\n28.5\n27.8\n26.2\n13.8\n14.1\n15.0\n(b) Results for OFA\nTable 15: The average recall (AR) results for Detic (detection) and OFA (visual grounding) across the 52 person-related\nclasses for each perceived age group label. The highest recall numbers are bolded.\nB. Data Card\nWe provide a data card for FACET, following the guidance of [48].\nFACET\nhttps://facet.metademolab.com\nFACET is a large, publicly available evaluation set of 31,702 images for the most common vision problems - image classification, ob-\nject detection, segmentation and visual grounding. People in FACET are annotated with person-related attributes such as perceived\nskin tone and hair type, bounding boxes and labeled with fine-grained person-related classes such as disk jockey or guitarist.\nOverview\nPublisher\nMeta AI Research, FAIR\nAuthors\nLaura Gustafson, Chloe Rolland, Nikhila Ravi, Quentin Duval, Aaron Adcock,\nCheng-Yang Fu, Melissa Hall, Candace Ross\nContact\nfacet@meta.com\nFunding & Funding Type\nIndustry\nLicense\nCustom license, see dataset download agreement\nApplications\nDataset Purpose\nEvaluate computer vision models to detect potential fairness concerns\nKey Application\nComputer Vision, Fairness and Robustness\nPrimary Motivations\nGive researchers a tool to help understand model fairness. Allow researchers\nto investigate how the demographic attributes of a person in the image corre-\nlates with model performance. FACET supports common vision tasks, with\nannotations for classification detection, and segmentation.\nIntended Audience\nResearchers aiming to detect potential fairness concerns and biases in their\ntrained vision models.\nSuitable Use Case\nFACET is for evaluation only.\nData Type\nPrimary Data Type\nImages\nPrimary Annotation Types\nManually gathered annotations for:\n\u2022 Bounding boxes\n\u2022 Category labels for the bounding boxes\n\u2022 A series of demographic, robustness, and additional attributes for the per-\nson in the bounding box\n\u2022 Manually annotated labels for masks from Segment Anything 1 Billion\n(SA-1B[59]). These masks were automatically generated by the Segment\nAnything Model (SAM).\nFACET Data Card\nData SnapShot\n\u2022 31,702 images\n\u2022 exhaustive annotations for 49,551 people\n\u2022 52 categories for people that include occupations, athletes, artists, etc\n\u2022 13 attributes annotated for person including demographic attributes such\nas perceived gender presentation robustness annotations such as lighting\ncondition and additional attributes such as hair color\n\u2022 3 mask labels person, clothing, hair for 69k masks. Masks and mask\nlabels are not exhaustive. 17k people in 14k images have labelled masks.\nAdditional unlabeled masks from SA-1B are compatible with FACET .\nData Sources\nImages come from SA-1B.\nAnnotation format\nJSON files of COCO formatted5 annotations for the bounding boxes and\nmasks are provided, and person attribute annotations are given in a CSV file.\ncoco boxes.json: COCO-style JSON file containing bounding boxes for\npeople in FACET. The id field for each annotation is the same as person id.\nCategory id corresponds to the primary class for the person.\ncoco masks.json: COCO-style JSON file containing Segment Anything\nModel (SAM) generated masks for people in FACET. Each annotation\nhas an additional person id field, corresponding to the person id in\nannotations.csv and coco boxes.json. Categories for masks are\none of [person, clothing, hair]. Masks are non-exhaustive.\nannotations.csv: A CSV containing the annotations per person is be\nprovided. Each item in the annotation file contains:\n1. Reference information:\n\u2022 filename\n\u2022 person id: unique integer representing the annotation\n2. Task information:\n\u2022 class1: This is the primary category the person matches. Cannot\nbe None.\n\u2022 class2: This is the secondary category the person matches. Can\nbe None.\n\u2022 bounding box: Person bounding box.\n5https://cocodataset.org/#format-data\nFACET Data Card\n3. Demographic Attribute annotations.\n\u2022 perceived gender presentation : All of the following annotations will\ngiven in a binary fashion: [gender presentation masc,\ngender presentation non binary,\ngender presentation fem, gender presentation na]\n\u2022 perceived skin tone : Each annotators annotations are considered\nper MST in a binary fashion. Annotations from all annotators are\nsummed into a single value per MST, so the value at MSTi may\nbe greater than 1. Values will be given for all of the following:\n[skin tone 1, ..., skin tone 10, skin tone na]\n\u2022 perceived age group :\nall of the following annotations are\nincluded in a binary fashion: [age presentation young,\nage presentation middle, age presentation older,\nage presentation na]\n4. Additional Attribute information: All binary values.\n\u2022 hair color: [hair color black, hair color red,\nhair color blonde, hair color brown,\nhair color colored, hair color grey,\nhair color na]\n\u2022 hair type: [hair type wavy, hair type curly,\nhair type coily, hair type straight,\nhair type bald, hair type dreadlocks,\nhair type na]\n\u2022 other items: [has eyewear, has headscarf,\nhas tattoo, has cap, has facial hair, has mask]\n5. Robustness Annotations: All binary values.\n\u2022 lighting condition: [lighting overexposed,\nlighting underexposed, lighting well lit,\nlighting dimly lit]\n\u2022 visibility: [visible minimal, visible torso,\nvisible face]\nC. FACET CrowdWorkSheets\nTo further describe our annotation process, we answer\nthe questions posed in CrowdWorkSheets[21].\nC.1. Task Formulation\nAt a high level, what are the subjective aspects of your\ntask?\nAnnotating the perceived attributes of a person is\nby nature subjective. For perceived skin tone we expected\nthe annotations would be subjective and have high variance.\nTo account for this, we gather annotations from three anno-\ntators and release the cumulative results of all three. For\nsubjectivity across the other attributes and labeling classes,\nwe provided annotators with diverse representations of each\nattribute or class in the guidelines to try to minimize anno-\ntator bias.\nWhat assumptions do you make about annotators? How\ndid you choose the specific wording of your task instruc-\ntions? What steps, if any, were taken to verify the clar-\nity of task instructions and wording for annotators?\nTo\nqualify for the annotation task, annotators had to pass a\nstrong English requirement. For the annotation of perceived\nskin tone only, we had a more lenient English requirement\nto increase the diversity of the annotators, and additionally\ntranslated the annotation instructions into Spanish.\nAs we were annotating images, we provided visual ex-\namples for all of the annotations and classes. We sourced\nmultiple examples per attribute (e.g. brown hair) and class\n(e.g. doctor), with at least one example for someone with\nmore stereotypical maleness with the attribute and some-\none with more stereotypical femaleness with the attribute.\nFor classes, we sourced multiple examples of someone who\nwould qualify for a given class (e.g for dancer we sourced\nimages of both a ballerina and a break-dancer). For given\nexamples for the Monk Skin Tone (MST) scale, we sourced\nfour examples per MST value, and attempted to capture\nsome of the diversity within a specified MST value.\nWhat are the precise instructions that were provided to\nannotators?\nThe goal of the project is to build a dataset\nthat helps determine if Computer Vision models have bi-\nases based on the apparent attributes of the person in the\nphoto. We are creating an image classification dataset that\nalso contains labels of the apparent protected attributes of\nthe people in the image. The dataset is for evaluation only,\nand is to help better analyze and detect potential biases. The\nprotected attributes will not in any way be used for training\na model. We are not collecting any biometric information\nabout the people in the photos.\n1. Target category classification: Given an image, and\na target category, we aim to determine if the image is\na good representation for the category. The annotators\nwill mark whether or not there is a person in the photo\nmatching the category, and if so if there are \u2264 5 peo-\nple who match this category. The categories will be\nall people related - such as doctor, soccer player, etc.\nMultiple images will be shown per task to annotate.\nThe default response will be \u2018No person matches this\ncategory\u2019.\n2. Bounding boxes and classification labels for people:\nGiven an image, draw bounding boxes around all peo-\nple who match any of the list of categories. For each\nbounding box around a person, mark which category\nthey belong to. If they belong to multiple categories,\nyou should mark the second category under \u2018secondary\ncategory\u2019.\n3. 3. Apparent skin tone annotations Given an image,\nwith a bounding box around a person, annotate the per-\nson\u2019s apparent skin tone. You may select as many skin\ntones from the list as you feel appropriate. If it is not\npossible to tell the skin tone from the photo, please\nmark cannot be determined. Please select at least two\nvalues for the skin tone, and make sure that the val-\nues that you select are consecutive. If it is too hard to\ndetermine the annotation, mark the values it appears\nand cannot be determined. Zoom in (option + mouse\nscroll) as necessary in order to determine the skin tone.\n4. 4. Apparent attribute annotations Given an image,\nwith a bounding box around a person, annotate the\ngiven apparent attributes of the person. For each cate-\ngory, see the examples given. If it is not possible to de-\ntermine the attribute from the photo, please mark can-\nnot be determined. Apparent lighting condition is on\nthe person: Please indicate how the lighting is with re-\nspect to the person in the bounding box. If the lighting\nis between two categories, mark both.\nC.2. Selecting Annotations\nAre there certain perspectives that should be privileged?\nIf so, how did you seek these perspectives out?\nNo. N/A\nAre there certain perspectives that would be harmful to\ninclude? If so, how did you screen these perspectives\nout?\nHarmful perspectives would include annotators who\nhad a clear bias in their annotations. We screened these\nperspectives out by using training, and only including pro-\nduction raters who had high accuracy on the training set.\nAnnotators with consistent bias would likely not have been\nable to get a high enough accuracy on the training to gradu-\nate.\nWere sociodemographic characteristics used to select\nannotators for your task? If so, please detail the process.\nIf you have any aggregated sociodemographic statistics\nabout your annotator pool, please describe. Do you have\nreason to believe that sociodemographic characteristics\nof annotators may have impacted how they annotated\nthe data? Why or why not?\nWe sourced geographically\ndiverse annotators from the following 7 countries during\nour annotation process: United States, Philippines, Egypt,\nColombia, Taiwan, Spain and Kenya. The breakdown of\nannotators per region is shown in Figure 4 in the main text.\nIf you have any aggregated sociodemographic statistics\nabout your annotator pool, please describe. Do you have\nreason to believe that sociodemographic characteristics\nof annotators may have impacted how they annotated\nthe data? Why or why not?\nOther sociodemographic\nstatistics about our annotator pool were not available.\nConsider the intended context of use of the dataset and\nthe individuals and communities that may be impacted\nby a model trained on this dataset. Are these commu-\nnities represented in your annotator pool?\nThe FACET\nbenchmark is to be used for evaluation purposes only. The\nunderlying images in FACET are geographically diverse.\nTo incorporate geographic diversity into our annotation pro-\ncess, we sourced annotators from 7 countries across regions.\nC.3. Platform and Infrastructure Choices\nWhat annotation platform did you utilize? At a high\nlevel, what considerations informed your decision to\nchoose this platform?\nDid the chosen platform suffi-\nciently meet the requirements you outlined for annota-\ntor pools? Are any aspects not covered?\nWe used a pro-\nprietary annotation platform.\nWhat, if any, communication channels did your cho-\nsen platform offer to facilitate communication with an-\nnotators? How did this channel of communication in-\nfluence the annotation process and/or resulting annota-\ntions?\nFor Stage 2 (drawing and labeling bounding boxes\nfor person classes), annotators\u2019 annotations were compared\nto a golden set and were required to achieve IoU above 85%\nto pass. After these training stages, annotations were man-\nually reviewed and the annotators were given feedback for\nimprovement. Following this, if annotators had high qual-\nity labels when spot-checked, they graduated to annotating\nimages for the final benchmark.\nWe provided annotators individualized feedback during\ntheir training for drawing bounding boxes on a daily basis.\nOur vendor communicated to annotators common types of\nmistakes that we witnessed during training, and the corre-\nsponding corrections.\nHow much were annotators compensated? Did you con-\nsider any particular pay standards, when determining\ntheir compensation? If so, please describe.\nAnnotators\nwere compensated with an hour wage set per country.\nC.4. Dataset Analysis and Evaluation\nHow do you define the quality of annotations in your\ncontext, and how did you assess the quality in the dataset\nyou constructed?\nFor each task, annotators were first\nplaced into training for the task. They were asked to an-\nnotate a large number of examples per task. We hand an-\nnotated the same examples and used our annotations as the\nground truth measured the accuracy per annotator. Anno-\ntators were graduated from training when their accuracy\nreached above a given threshold.\nFor the task requiring\nannotators to draw bounding boxes around people, anno-\ntators were only graduated after we manually spot checked\nthe their bounding boxes to ensure quality. During the per-\nceived skin tone annotation task, we asked annotators if\nthey agreed with the class label, and grade the quality of\nthe given bounding box. If one of the three annotators dis-\nagreed with the class label or bounding box, the annotation\nwas removed, and the image added to the queue of images\nfor task 2 (drawing bounding boxes).\nHave you conducted any analysis on disagreement pat-\nterns? If so, what analyses did you use and what were\nthe major findings?\nWe pointed out common mistakes\nduring weekly meetings with the vendor. While in train-\ning, we noticed consistent mistakes among annotators that\nwe corrected before graduation. The most common mistake\nwas around drawing the bounding boxes: many annotators\nduring training would draw bounding boxes that included\nobjects the person was holding e.g guitar. With the weekly\nmeetings and individualized feedback, we were able to ad-\ndress this.\nHow do the individual annotator responses relate to the\nfinal labels released in the dataset?\nFor perceived skin\ntone only, we sourced three annotations per person in the\ndataset. We release the annotations from all three annota-\ntors, giving a distribution over perceived skin tone per per-\nson in the dataset. We believe that a distribution more accu-\nrately describes a person\u2019s perceived skin tone than a single\nvalue.\nC.5. Dataset Release and Maintenance\nDo you have reason to believe the annotations in this\ndataset may change over time? Do you plan to update\nyour dataset?\nAt this time we do not plan to have up-\ndates for this dataset. We will allow users to flag any im-\nages that may be objectionable content, and remove objec-\ntionable content if found.\nAre there any conditions or definitions that, if changed,\ncould impact the utility of your dataset?\nThe FACET\nbenchmark contains examples for many different types of\nprofessions, athletes, artists, etc. If over time the way these\noccupations look shifts, this could impact the dataset. As\na concrete example, there are a number of images in the\ndataset that were taken since the beginning of the COVID-\n19 pandemic. Many doctors and nurses in the dataset are\nwearing much more PPE than in images of doctors and\nnurses from before the COVID-19 pandemic.\nWill you attempt to track, impose limitations on, or oth-\nerwise influence how your dataset is used? If so, how?\nThe FACET benchmark is for evaluation purposes ONLY.\nUsing FACET annotations for training is strictly prohibited.\nUsers must agree to the terms of use before downloading\nthe dataset.\nWere annotators informed about how the data is exter-\nnalized? If changes to the dataset are made, will they be\ninformed?\nNo. No.\nIs there a process by which annotators can later choose\nto withdraw their data from the dataset? If so, please\ndetail.\nNo.\nD. Fine-grained dataset statistics\nFACET Class Statistics\nPerceived Gender\nPresentation\nPerceived Skin Tone\nPerceived Age\nGroup\nPerson Class\nTotal\nstereotypical\nmaleness\nstereotypical\nfemaleness\nnon-binary\npresentation\n1 \u25a0\n2 \u25a0\n3 \u25a0\n4 \u25a0\n5 \u25a0\n6 \u25a0\n7 \u25a0\n8 \u25a0\n9 \u25a0\n10 \u25a0\nyounger\nmiddle\nolder\nlawman\n4609\n3768\n403\n3\n560\n2363\n2881\n2642\n1825\n1215\n615\n322\n166\n74\n387\n3151\n144\nlaborer\n3030\n2208\n378\n1\n112\n577\n887\n1171\n1171\n1269\n844\n508\n291\n136\n297\n1643\n193\nboatman\n2147\n1074\n742\n5\n137\n741\n991\n995\n794\n573\n296\n134\n86\n31\n482\n906\n147\nguard\n1851\n1597\n121\n4\n306\n1045\n1208\n1047\n714\n470\n246\n119\n62\n27\n317\n1181\n48\nbackpacker\n1738\n1006\n458\n4\n167\n771\n1010\n951\n761\n475\n193\n93\n50\n28\n361\n842\n53\nbasketball player\n1680\n1479\n134\n2\n307\n869\n991\n845\n632\n461\n413\n359\n332\n165\n492\n1056\n3\ntennis player\n1663\n1058\n488\n0\n147\n805\n1152\n1262\n1002\n617\n234\n126\n90\n57\n360\n1081\n6\nfarmer\n1632\n823\n539\n1\n50\n208\n335\n466\n635\n816\n681\n450\n216\n87\n129\n844\n227\nsoldier\n1561\n1336\n75\n0\n204\n766\n892\n802\n578\n463\n281\n130\n66\n22\n237\n972\n39\nsinger\n1518\n1013\n428\n14\n240\n824\n1013\n931\n677\n399\n184\n140\n93\n46\n357\n984\n89\ndancer\n1475\n510\n812\n10\n207\n644\n863\n798\n716\n431\n214\n113\n67\n26\n567\n644\n32\nspeaker\n1470\n1119\n282\n1\n152\n789\n1093\n1050\n755\n392\n182\n92\n49\n25\n134\n987\n207\nmotorcyclist\n1468\n822\n302\n4\n55\n384\n518\n583\n539\n478\n249\n104\n59\n22\n213\n577\n56\nrepairman\n1430\n1187\n54\n1\n65\n480\n681\n815\n753\n676\n373\n150\n60\n18\n126\n836\n78\nseller\n1342\n699\n533\n6\n74\n359\n555\n705\n757\n642\n379\n192\n74\n28\n205\n758\n184\nballplayer\n1316\n1145\n62\n1\n104\n584\n743\n781\n635\n473\n253\n168\n112\n51\n214\n834\n12\nguitarist\n1279\n1115\n87\n3\n138\n678\n843\n816\n596\n330\n139\n73\n50\n26\n233\n802\n116\ncomputer user\n1267\n597\n322\n2\n176\n641\n818\n785\n608\n358\n152\n71\n34\n12\n258\n449\n24\nsoccer player\n1233\n1102\n34\n1\n113\n521\n692\n711\n559\n364\n200\n126\n128\n76\n322\n732\n5\ncraftsman\n1127\n785\n220\n4\n75\n321\n467\n598\n631\n627\n389\n210\n92\n36\n117\n599\n188\nnurse\n1124\n322\n535\n3\n115\n368\n505\n536\n529\n399\n188\n86\n27\n6\n169\n547\n24\ndrummer\n1006\n744\n162\n3\n114\n428\n534\n483\n388\n331\n222\n150\n99\n42\n256\n530\n68\nskateboarder\n1000\n818\n88\n1\n82\n468\n635\n650\n463\n281\n136\n62\n37\n14\n360\n465\n1\npainter\n983\n590\n251\n0\n77\n318\n460\n530\n506\n420\n246\n123\n56\n22\n168\n435\n129\nfireman\n933\n674\n34\n0\n68\n270\n358\n391\n237\n192\n77\n22\n10\n7\n55\n512\n14\npatient\n896\n408\n275\n0\n75\n280\n389\n472\n486\n444\n242\n102\n41\n16\n131\n368\n127\nhorseman\n884\n491\n290\n1\n152\n538\n592\n484\n287\n127\n54\n26\n12\n5\n181\n512\n22\ndoctor\n861\n361\n313\n1\n86\n343\n450\n462\n410\n284\n145\n69\n21\n6\n107\n441\n43\nprayer\n810\n444\n265\n3\n58\n223\n307\n355\n394\n357\n195\n99\n51\n18\n104\n358\n124\nreferee\n776\n694\n38\n1\n88\n417\n539\n547\n374\n186\n80\n38\n23\n10\n54\n584\n35\nstudent\n747\n379\n247\n1\n92\n241\n322\n365\n367\n316\n163\n93\n62\n26\n319\n264\n5\nrunner\n654\n469\n117\n3\n88\n320\n415\n379\n291\n126\n45\n23\n27\n16\n134\n403\n19\ngymnast\n635\n252\n316\n1\n116\n348\n424\n366\n322\n145\n43\n28\n17\n6\n300\n233\n2\nretailer\n561\n296\n234\n0\n53\n198\n301\n298\n298\n196\n90\n41\n14\n5\n114\n332\n39\nclimber\n551\n355\n92\n2\n59\n231\n306\n301\n251\n155\n73\n26\n13\n8\n107\n261\n4\ntrumpeter\n530\n451\n36\n3\n63\n308\n336\n304\n212\n145\n74\n41\n37\n18\n89\n316\n56\nlifeguard\n529\n398\n62\n0\n20\n160\n232\n286\n229\n186\n103\n52\n28\n8\n118\n273\n8\nelectrician\n505\n415\n7\n0\n9\n100\n140\n188\n182\n175\n101\n52\n35\n9\n47\n270\n9\ngardener\n499\n266\n173\n1\n45\n187\n257\n265\n235\n197\n108\n56\n33\n16\n66\n245\n79\nreporter\n473\n302\n145\n1\n75\n281\n324\n269\n204\n116\n50\n18\n13\n4\n77\n302\n24\nhairdresser\n461\n342\n85\n3\n32\n143\n209\n257\n242\n237\n145\n75\n35\n17\n69\n294\n43\nmachinist\n413\n329\n30\n0\n33\n173\n223\n252\n191\n168\n89\n34\n20\n7\n42\n241\n20\ncheerleader\n410\n78\n314\n0\n77\n191\n292\n268\n205\n88\n38\n18\n12\n3\n246\n117\n5\nwaiter\n350\n204\n109\n1\n34\n184\n245\n220\n177\n120\n51\n24\n18\n7\n68\n224\n7\ndisk jockey\n318\n228\n27\n1\n43\n162\n200\n194\n127\n77\n37\n27\n20\n10\n67\n167\n2\nflutist\n312\n247\n41\n0\n38\n152\n192\n184\n154\n118\n77\n43\n16\n4\n50\n189\n32\nastronaut\n289\n165\n14\n0\n15\n72\n89\n78\n58\n18\n2\n0\n0\n2\n5\n158\n2\ncarpenter\n268\n230\n7\n0\n11\n82\n124\n147\n129\n131\n87\n52\n25\n9\n20\n160\n27\nsculptor\n240\n187\n21\n0\n10\n76\n104\n120\n107\n107\n78\n50\n24\n5\n24\n144\n27\nteacher\n216\n116\n76\n1\n28\n104\n141\n142\n108\n76\n36\n16\n10\n4\n31\n150\n9\njudge\n101\n67\n28\n0\n11\n50\n76\n71\n44\n21\n6\n3\n1\n0\n8\n71\n12\nbartender\n57\n37\n14\n0\n5\n27\n42\n36\n29\n19\n7\n3\n1\n1\n7\n41\n1\nTable 17: Number of people for each person class and demographic group in FACET.\nFACET Mask Statistics\nperson\nclothing\nhair\nperceived gender presentation\nwith stereotypical maleness\n6608\n32103\n3788\nwith stereotypical femaleness\n4127\n18136\n3346\nnon-binary presentation\n50\n223\n36\ncannot be determined\n72\n193\n13\nperceived skin tone\nMST 1 \u25a0\n2198\n10687\n1389\nMST 2 \u25a0\n5154\n24328\n3496\nMST 3 \u25a0\n6121\n28825\n4263\nMST 4 \u25a0\n5651\n26583\n3889\nMST 5 \u25a0\n4849\n22738\n3349\nMST 6 \u25a0\n3816\n17931\n2452\nMST 7 \u25a0\n2542\n11845\n1544\nMST 8 \u25a0\n1619\n7564\n922\nMST 9 \u25a0\n1216\n5727\n666\nMST 10 \u25a0\n521\n2481\n293\ncannot be determined\n2839\n11844\n1611\nperceived age group\nyounger\n4145\n19440\n3107\nmiddle\n5443\n25458\n3319\nolder\n1134\n5352\n733\ncannot be determined\n135\n405\n24\nHair color\nblack\n4053\n18137\n3323\nbrown\n2726\n12205\n2267\nblonde\n1024\n4633\n952\nred/orange\n148\n674\n136\ncolored\n84\n340\n96\ngrey\n747\n3519\n559\ncannot be determined\n2885\n14863\n485\nHair type\nwavy\n2090\n9526\n1897\ncurly\n241\n1141\n253\nstraight\n5141\n22109\n4395\ncoily\n178\n750\n158\ndreadlocks\n113\n522\n109\nbald\n265\n1167\n81\nUnknown\n3626\n19129\n905\nAdditional attribute\neyeware\n1509\n6993\n957\nheadscarf\n665\n3634\n256\ntattoo\n184\n926\n143\ncap\n3305\n18209\n797\nfacial hair\n1511\n7382\n963\nmask\n591\n3271\n377\nTable 18: Number of masks per type for the attributes in FACET. For perceived skin tone, hair color, hair type, and additional\nattributes, a person in FACET can be marked with multiple values; therefore, the sum of the masks over the group of attributes\nmay be greater than the total number of masks.\n"
  },
  {
    "title": "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following",
    "link": "https://arxiv.org/pdf/2309.00615.pdf",
    "upvote": "8",
    "text": "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for\n3D Understanding, Generation, and Instruction Following\nZiyu Guo\u22171, Renrui Zhang\u2217\u2020\u20211,2, Xiangyang Zhu2, Yiwen Tang2, Xianzheng Ma2, Jiaming Han1,2\nKexin Chen1, Peng Gao2, Xianzhi Li\u20213, Hongsheng Li1, Pheng-Ann Heng1\n\u2217 Equal contribution\n\u2020 Project leader\n\u2021 Corresponding author\n1The Chinese University of Hong Kong\n2Shanghai AI Laboratory\n3Huazhong University of Science and Technology\n{zyguo, pheng}@cse.cuhk.edu.hk,\nzhangrenrui@pjlab.org.cn\nAbstract\nWe introduce Point-Bind, a 3D multi-modality model\naligning point clouds with 2D image, language, audio,\nand video.\nGuided by ImageBind, we construct a joint\nembedding space between 3D and multi-modalities, en-\nabling many promising applications, e.g., any-to-3D gen-\neration, 3D embedding arithmetic, and 3D open-world un-\nderstanding.\nOn top of this, we further present Point-\nLLM, the first 3D large language model (LLM) follow-\ning 3D multi-modal instructions.\nBy parameter-efficient\nfine-tuning techniques, Point-LLM injects the semantics of\nPoint-Bind into pre-trained LLMs, e.g., LLaMA, which re-\nquires no 3D instruction data, but exhibits superior 3D\nand multi-modal question-answering capacity.\nWe hope\nour work may cast a light on the community for extend-\ning 3D point clouds to multi-modality applications. Code\nis available at https://github.com/ZiyuGuo99/\nPoint-Bind_Point-LLM.\n1. Introduction\nIn these years, 3D vision has gained significant atten-\ntion and development, driven by the rising popularity of au-\ntonomous driving [11, 69, 71], navigation [72, 76, 98], 3D\nscene understanding [2, 43, 46, 74], and robotics [31, 67].\nTo extend its application scenarios, numerous efforts [1,23,\n92,95] have been made to incorporate 3D point clouds with\ndata from other modalities, allowing for improved 3D un-\nderstanding [1, 23], text-to-3D generation [35, 49, 52], and\n3D question answering [3,28].\nFor 3D geometry understanding, previous works either\nleverage 2D-language embeddings to guide 3D open-world\nrecognition [90, 100], or harness visual and textual seman-\nImage\nPoint Cloud\nPoint-LLM for 3D Q&A\n3D Embedding Arithmetic\n3D Zero-shot Learning\nAny-to-3D Generation\nLanguage\n\u201cA 3D model\nof an airplane.\u201d\nPOINT-BIND\nAudio\n[ Airplane Engine ] \nFigure 1. Characteristics of Point-Bind. We propose to align\n3D with multi-modalities and develop a unified framework, Point-\nBind, which extends various 3D multi-modal applications. Based\non Point-Bind, we further introduce Point-LLM, a 3D large lan-\nguage model with bilingual 3D instruction-following capacity.\ntics to assist 3D representation learning [39, 56, 84]. How-\never, their perception capabilities are mostly constrained by\nlimited modalities provided in the training phase. Inspired\nby 2D generative models [60,63,64], a collection of meth-\nods [35,49,52] has achieved text-to-3D synthesis with high\nquality and efficiency. Despite this, they lack the ability to\ngenerate 3D shapes conditioned on multi-modal input, i.e.,\nany-to-3D generation. Another series of works connects de-\nscriptive natural language with 3D data, which is applied\nto 3D captioning [12, 87], question answering [3, 78], and\nvisual grounding [24, 79]. Yet, they fail to utilize the pre-\narXiv:2309.00615v1  [cs.CV]  1 Sep 2023\n3D Embedding Arithmetic\nAny-to-3D Generation\nPoint-LLM for 3D Q&A\nDescribe the 3D object in detail.\nThe 3D object is a small\nairplane,\nspecifically\na\nprivate jet. and black\ncolor\nscheme,\nand\nit\nappears to be a small-\nsized aircraft. It is \u2026...\n[ Audio of\nPouring Water ]\nWhat\u2019s happening?\n3D Zero-shot Understanding\nText-referred:\nPoint-Bind\nairplane, piano, guitar\nairplane\nCLASS\nA  model  of  a\npiano\nguitar\nThe\nscene\nshows\na\nperson\npouring\na\nclear\nliquid, likely water, into a\nglass. The glass is placed\non a table, filled to the\nbrim,\nindicating\nthat\na\nsignificant\namount\nof\nliquid\nhas\nbeen\npoured\ninto it.\nText-to-3D:\nAudio-to-3D:\nImage-to-3D:\nPoint-to-3D:\n[ Audio of\nSea Wave ]\n+\nCross-modal Retrieval\n3D Point Cloud\n\u201cA Car\u201d\n[ Car Horn ]\n\ud83d\udd0a [ Airplane Engine ]\n\ud83c\udfb6 [ Piano Music ]\n\ud83c\udfb6 [ Guitar Sound ]\nAudio-referred:\nPoint-Bind\nairplane\npiano\nguitar\n\u00d8 No Need for 3D Instruction Data\n\u00d8 3D and Multi-modal Reasoning\nFigure 2. 3D Multi-modal Applications of Point-Bind. With a joint 3D multi-modal embedding space, Point-Bind enables many promis-\ning application scenarios, e.g., Point-LLM for 3D instruction following, 3D generation conditioned on any modalities, embedding-space\narithmetic with 3D, and multi-modal 3D zero-shot understanding.\ntrained linguistic knowledge within large language models\n(LLMs) to better capture 3D geometrics.\nTherefore, how to develop a unified 3D framework align-\ning with multi-modality for general 3D learning still re-\nmains an open question. Very recently, ImageBind [22] was\nproposed to learn a shared representation space across six\ndifferent modalities, i.e., images, text, audio, depth, ther-\nmal, and IMU data. Motivated by this, we ask the following\nquestion: can we construct a joint embedding space be-\ntween 3D and multi-modality for unified 3D understand-\ning, generation, and insturction following?\nTo this end, we introduce Point-Bind, a 3D multi-\nmodality framework that aligns point clouds with multiple\nmodalities for general 3D analysis, as shown in Figure 1.\nSpecifically, we collect 3D-image-text-audio pairs as the\ntraining data, and construct a joint embedding space guided\nby ImageBind. We adopt a contrastive loss between the\nextracted features from a trainable 3D encoder, e.g., I2P-\nMAE [92], and the frozen multi-modal encoders of Image-\nBind. Such a simple strategy can efficiently integrate dif-\nferent modalities into a unified representation space, and al-\nlows for various 3D-centric multi-modal tasks in Figure 2.\nThe main contributions of Point-Bind are as follows:\n\u2022 Aligning 3D with ImageBind. Within a joint embed-\nding space, Point-Bind firstly aligns 3D point clouds\nwith multi-modalities guided by ImageBind, including\n2D images, video, language, audio, etc.\n\u2022 Any-to-3D Generation.\nBased on existing text-\nto-3D generative models, Point-Bind enables 3D\nshape synthesis conditioned on any modalities, i.e.,\ntext/image/audio/point-to-mesh generation.\n\u2022 3D Embedding-space Arithmetic.\nWe observe that\n3D features from Point-Bind can be added with other\nmodalities to incorporate their semantics, achieving\ncomposed cross-modal retrieval.\n\u2022 3D Zero-shot Understanding.\nPoint-Bind attains\nstate-of-the-art performance for 3D zero-shot classifi-\ncation. Also, our approach supports audio-referred 3D\nopen-world understanding, besides text reference.\nFurthermore, on top of our joint embedding space, we\npropose to incorporate Point-Bind with LLaMA [73] to de-\nvelop the first 3D large language models (LLMs), termed\nDescribe the 3D object in detail.\nThe 3D object is a grand piano, \nwhich is a musical instrument with \na keyboard and a set of strings. \nThe piano has a piano keyboard, \nwhich consists of white and black \nkeys arranged in a grid-like \npattern \u2026\u2026\n\u8bf7\u63cf\u8ff0\u8fd9\u4e2a\u4e09\u7ef4\u7269\u4f53\u7684\u7ec6\u8282\u3002\n\u8fd9\u4e2a\u4e09\u7ef4\u7269\u4f53\u662f\u2f00\u67b6\u2edc\u673a\uff0c\u6709\n\u7740\u660e\u663e\u7684\u673a\u7ffc\u548c\u5c3e\u90e8\u3002\u2edc\u673a\u7684\n\u673a\u2f9d\u5448\u2f69\u2f8a\uff0c\u673a\u7ffc\u548c\u5c3e\u90e8\u7684\u989c\n\u2f8a\u4e3a\u2fca\u2f8a\u3002\u2edc\u673a\u7684\u673a\u7ffc\u5448\u5706\u5f62\uff0c\n\u4f4d\u4e8e\u2edc\u673a\u7684\u4e2d\u95f4\u90e8\u5206\u3002\u2edc\u673a\u7684\n\u5c3e\u90e8\u6709\u2f00\u4e2a\u5706\u5f62\u7684\u5c3e\u5df4\uff0c\u4f4d\u4e8e\n\u2edc\u673a\u7684\u5e95\u90e8 \u2026\u2026\nIn English\nIn Chinese\nDescribe the scene in detail.\nPlease elaborate on the details of this scene.\nThis\nscene\nfeatures\na bathhub\nwith a faucet turned on, and a\nperson's hand is submerged in the\nwater. The water is splashing up and\ncreating a soapy bubble, which is\nvisible in the bathhub. The person\nappears to be washing their hands or\ntaking\na\nbath.\nThe\nbathhub\nis\nlocated towards the left side, and\nthe faucet is positioned towards the\nright, dripping water.\nWhat do you know about this scene \nwith the provided materials?\nThe image features a person's\nhand reaching out to open a\ndoor,\npossibly\na\nwooden\ndoor, with a key. The person is\npositioned on the left side of the\ndoor, and their hand is close to\nthe doorknob. The scene appears\nto be a close-up view of the\nperson's\nhand\nand\nthe\nkey,\nemphasizing\nthe\naction\nof\nunlocking the door.\nPlease provide a detailed\ninterpretation of the scene.\n[ Audio of\nDripping Water ]\n[ Audio of\nWood Door Creak ]\n[ Audio of\nKeyboard Typing]\nCan you explain this scene in detail?\nIt features a young boy standing in\na library, surrounded by numerous\nbooks.\nHe is holding a book in his\nhands, possibly reading or looking\nat it.\nThe books are arranged in\nvarious positions, with some placed\non shelves and others on the floor.\nThe bookshelf setting suggests\na learning environment, like library,\nwhere the boy is engaged in\nreading\nor exploring the vast\ncollection of books available.\nI would like a comprehensive \nexplanation of this scene.\nThe image depicts a woman \nstanding in a clothing store, \nsurrounded by various racks of \nclothing. She is wearing a black \nshirt and appears to be looking at \nthe items displayed on the racks. \nThe store is well-stocked with \nnumerous shirts, some of which are \nhanging on the racks, while others \nare placed on the floor. The woman \nseems to be browsing the selection \nof clothing available in the store.\nPlease give me a detailed \nexplanation of this scene.\nThe image features a close-up of a \nlaptop keyboard with a mouse placed on \ntop of it. The laptop screen is turned \non, displaying the word \"Laptop\" on the \nscreen. The keyboard is fully visible, \nwith each key neatly arranged. The \nmouse is positioned on the right side of \nthe keyboard, slightly above the middle \nrow of keys. The scene gives an \nimpression of a user working on \ntheir laptop, possibly typing on \nthe keyboard.\nThe image depicts a room with a fireplace \nand a ladder leaning against the \nwall.  The ladder is positioned near the \nfireplace, which has a chimney.  The \nroom appears to be in a state of \ndisrepair, with a broken window and a \ncollapsed ceiling.  The fireplace is \nsurrounded by debris, including books and \npapers scattered around the floor.  The \noverall atmosphere of the room suggests \nthat it has been abandoned or neglected \nfor a long time.\nThe scene features a woman sitting\nin a window seat on an airplane,\nlooking out the window as the\nairplane takes off. She appears to be\nenjoying the view outside. The airplane's\nwing is visible in the background, and\nthe scene captures the excitement and\nanticipation of a journey about to begin.\nFigure 3. 3D Question-answering Examples of Point-LLM. Given 3D and multi-modal instructions, our Point-LLM can effectively\ngenerate detailed responses and conduct superior cross-modal reasoning. Notably, we do not need any 3D instruction data for training.\nas Point-LLM. As shown in Figure 2, our Point-LLM can\nrespond to language instructions with 3D point cloud con-\nditions, and effectively capture spatial geometry character-\nistics. Referring to ImageBind-LLM [19], we utilize a bind\nnetwork along with a visual cache model to bridge Point-\nBind with LLaMA, and adopt zero-initialized gating mech-\nanisms [20, 91] for parameter-efficient fine-tuning.\nWith\nsuperior data efficiency, the entire training phase of Point-\nLLM requires no 3D instruction dataset, and only uti-\nlizes public vision-language data [9, 13, 68, 70] for vision-\nlanguage tuning. In this way, we enable LLMs to under-\nstand and conduct cross-modal reasoning for 3D and multi-\nmodal data, achieving superior 3D question-answering ca-\npacity in both English and Chinese.\nThe main contributions of Point-LLM are as follows:\n\u2022 Point-LLM for 3D Question Answering. Using Point-\nBind, we introduce Point-LLM, the first 3D LLM that\nresponds to instructions with 3D point cloud condi-\ntions, supporting both English and Chinese.\n\u2022 Data- and Parameter-efficiency. We only utilize pub-\nlic vision-language data for tuning without any 3D\ninstruction data, and adopt parameter-efficient fine-\ntuning techniques, saving extensive resources.\n\u2022 3D and Multi-modal Reasoning. Via the joint em-\nbedding space, Point-LLM can generate descriptive re-\nsponses by reasoning a combination of 3D and multi-\nmodal input, e.g., a point cloud with an image/audio.\n2. Related Work\nMulti-modality Learning.\nCompared to single-modal\napproaches, multi-modal learning aims to learn from multi-\nple modalities simultaneously, achieving more robust and\ndiverse representation learning.\nNumerous studies have\nproved its efficacy, involving 2D images, videos, texts,\nand audio [15, 17, 48], and enhance the cross-modal per-\nformance for downstream tasks [5, 25, 37, 61], and video-\ntext-audio integration for text generation [36].\nThe rep-\nresentative vision-language pre-training, CLIP [59], effec-\ntively bridges the gap between 2D images and texts, which\nencourages further exploration of cross-modality learning.\nRecently, ImageBind [22] successfully aligns six modalities\nin a joint embedding space, unleashing the power for emer-\ngent zero-shot cross-modal capabilities. However, Image-\nBind fails to investigate its efficacy on 3D point clouds. In\nthe 3D domain, most existing cross-modal works introduce\nvision-language alignment [1, 10, 23, 84, 90] into 3D point\nclouds, and mainly focus on open-world recognition tasks,\nwhich ignore the potential of multi-modal semantics for\nwider 3D applications. In this paper, our Point-Bind devel-\nops a general 3D multi-modality model that aligns 3D point\nclouds with six other modalities guided by ImageBind, al-\nlowing for more diverse 3D cross-modal understanding.\nLarge Models in 3D.\nLarge-scale pre-trained models\nhave achieved remarkable downstream performance in lan-\nguage and 2D image processing. Inspired by this, many\nefforts have introduced 2D and language large models, to\nassist in 3D learning. The prior PointCLIP series [30, 90,\n100] project 3D point clouds into depth maps, and utilize\nCLIP [59] for zero-shot recognition. Image2Point [82] in-\nstead converts 2D pre-trained models into 3D space as a\ngood network initialization. By contrastive learning, ULIP\nseries [84, 85] and other works [27, 39] pre-train 3D net-\nworks guided by the vision-language embedding space of\nCLIP. Another branch of work employs CLIP to guide the\ntext-conditioned generation of 3D objects [32, 41, 65, 83]\nor stylized meshes [45, 47] by encoding descriptive textual\ninput. Some works also adopt GPT-3 [6] to enhance the\nlanguage-based understanding of 3D spatial geometry, such\nas PointCLIP V2 [100] and ViewRefer [24]. Different from\nthem, we utilize ImageBind [22] to construct a joint embed-\nding space between 3D point clouds and multiple modali-\nties. The derived Point-Bind can well leverage the multi-\nmodal semantics for general 3D cross-modal understand-\ning, generation, and question answering.\nPre-training in 3D.\nIn recent years, significant progress\nhas been made in supervised learning for 3D vision\ntasks [54, 55, 57, 93, 99]. However, these approaches lack\nsatisfactory generalization capabilities for out-of-domain\ndata. To address this, self-supervised learning has emerged\nas a promising solution to enhance 3D transfer learning [10,\n34, 53, 86]. Most self-supervised pre-training methods em-\nploy an encoder-decoder framework to encode point clouds\ninto latent representations and then reconstruct the original\ndata form [62,66,75]. Therein, Point-MAE [50] and Point-\nM2AE [89] introduce masked autoencoders [26] into 3D\npoint clouds pre-training, achieving competitive results on\ndifferent 3D tasks. Alternatively, cross-modal pre-training\napproaches are also leveraged to enhance the 3D general-\nization ability [40, 56, 58, 77]. For example, ACT [16] and\nI2P-MAE [92] utilize pre-trained 2D transformers as teach-\ners to guide 3D representation learning. Inspired by pre-\nvious works, we adopt collected 3D-image-text-audio pairs\nfor self-supervised pre-training, and regard ImageBind\u2019s en-\ncoders as guidance for contrastive learning. In this way, the\nPoint-Bind is pre-trained to obtain a joint embedding space\nbetween 3D and multi-modality, allowing for superior per-\nformance on different 3D downstream tasks.\n3. Point-Bind\nThe overall pipeline of Point-Bind is shown in Figure 4.\nIn Section 3.1, we first provide a preliminary of Image-\nBind [22]. Then, in Section 3.2 and 3.3, we elaborate on\nthe training data and multi-modal alignment for Point-Bind,\nrespectively. Finally, in Section 3.4, we introduce several\n3D-centric applications derived from Point-Bind.\n3.1. Preliminary of ImageBind\nImageBind [22] proposes an approach to combine mul-\ntiple modalities together, which utilizes only image-paired\n3D Encoder\nImageBind\nAudio: \n[ Piano Music ]\n3D Embeddings\nImage & Text & Audio\nEmbeddings\nContrastive\nLoss\nPoint-LLM\n3D Generation\nDecoder\nZero-shot \nRecognition Head\nCross-modal\nRetrieval Head\nairplane\npiano\nguitar\nkeyboard\n3D-Image-Text-Audio Pairs\nDescribe the 3D object.\nThe 3D object is a \ngrand piano, which is a \nmusical instrument \u2026\n3D Point Cloud:\nImage:\nText: \u201dA model of a piano.\u201d\no Audio:\no Text:    \no Image:\n[Piano Music]\nImages of Piano\n\u201dA piano in the \u2026\u201d\nscore\nFigure 4. Overall Pipeline of Point-Bind. We collect 3D-image-audio-text data pairs for contrastive learning, which aligns 3D modality\nwith others guided ImageBind [22]. With a joint embedding space, Point-Bind can be utilized for 3D cross-modal retrieval, any-to-3D\ngeneration, 3D zero-shot understanding, and developing a 3D large language model, Point-LLM.\ndata to learn a joint embedding space of six modalities,\ni.e., images, text, audio, depth, thermal, and IMU data. It\ndoes not need training dataset pairing all six modalities, but\nleverages the binding property of 2D images, i.e., aligning\nevery single modality to image independently. Specifically,\nImageBind feeds multi-modal input into corresponding en-\ncoders, and adopts for cross-modal contrastive learning. Af-\nter training on large-scale image-paired data, ImageBind ef-\nfectively aligns six modalities into a single shared represen-\ntation space, enabling emergent cross-modal zero-shot ca-\npabilities. Based on existing vision-language models, Im-\nageBind can also be utilized for several multi-modal tasks,\nsuch as text-to-audio/video retrieval, audio-to-image gener-\nation, and audio-referred object detection. Inspired by this,\nwe propose to develop a 3D multi-modal framework that in-\ncorporates 3D point clouds with other modalities for general\n3D understanding, generation, and instruction following.\n3.2. Training Data\nTo align 3D with multi-modalities, we leverage the pre-\ntrained joint embedding space of ImageBind [22] and utilize\ncontrastive loss [59, 96] to simultaneously align 3D point\nclouds with the other three modalities: image, text, and au-\ndio. To obtain the contrastive training data, we collect a\ncross-modal dataset of 3D-image-audio-text pairs. There\nare three steps for dataset collection as follows.\n3D-image-text Pairs.\nWe adopt the data pairs of 3D,\nimages, and text from ULIP [84], which includes 3D-\nimage-text triplets built from ShapeNet [8], a common-\nused dataset containing abundant 3D CAD models. Each\n3D point cloud is paired with a corresponding text describ-\ning the semantic information of its spatial shape, and a 2D\ncounterpart generated by multi-view image rendering. The\ntext description is constructed by a synset of category names\nand 64 pre-defined templates.\n3D-audio Pairs.\nTo provide more contrastive signals, we\nalso collect the data pairs of 3D and audio from ESC-50 [51]\nand ShapeNet datasets. Specifically, we first select the cat-\negories whose objects can make a sound in the real world\nfrom the 55 categories of ShapeNet, such as \u2018airplane\u2019,\n\u2018clock\u2019, \u2018washing machine\u2019, and \u2018keyboard\u2019. Then, we pre-\nserve only the categories that are also within ESC-50. By\nthis standard, we obtain 9 categories of 3D-audio paired\ndata with extensive audio clips.\n3D-image-audio-text Pairs Construction.\nFinally, we\nmatch each 3D-audio pair with its corresponding 3D-\nimage-text data, resulting in a unified 3D-image-audio-text\ndataset with extensive cross-modal pairs. During training,\nwe simultaneously feed point clouds and their paired data\nof three modalities for contrastive learning.\n3.3. Aligning 3D with Multi-modality\nAfter collecting the 3D paired data, we conduct con-\ntrastive training to learn a joint embedding space align-\ning 3D and multi-modalities. Each data sample contains\na point cloud P, along with the paired 2D image I, text\ndescription T s, and audio A, where T s represents a set of\n64 pre-defined templates. For the point cloud, we adopt\nI2P-MAE [92] as the learnable 3D encoder, denoted as\nEncoder3D(\u00b7), and append a projection network Proj(\u00b7) of\ntwo linear layers, which transforms the encoded 3D feature\ninto ImageBind\u2019s multi-modal embedding space. We for-\nmulate it as\nF3D = Proj(Encoder3D(P)),\n(1)\nwhere F3D \u2208 R1\u00d7C denotes the projected 3D embedding,\nand C equals the feature dimension of ImageBind. For the\npaired image-text-audio data, we leverage their correspond-\ning encoders from ImageBind for feature extraction, which\nare frozen during training, formulated as\nF2D, F s\nT , FA = ImageBind(I, T s, A),\n(2)\nwhere F2D, FA \u2208 R1\u00d7C denote the image and audio em-\nbeddings, and F s\nT \u2208 R64\u00d7C denotes the text embedding for\na set of 64 descriptions. Then, we conduct an average pool-\ning as\nFT = Average(F s\nT ) \u2208 R1\u00d7C,\n(3)\nwhich represents the aggregated text embedding with more\nrobustness. After that, we adopt contrastive loss [96] be-\ntween 3D and other modalities, which effectively enforces\n3D embeddings to align with the joint representation space,\nformulated as\nLtotal = L(F3D, F2D) + L(F3D, FT ) + L(F3D, FA).\nNote that some categories of the training data do not include\nthe paired audio A, since they inherently cannot make any\nsound, e.g., bottle, planter, and couch, for which we ignore\ntheir audio features and loss.\n3.4. Multi-modal Applications\nBenefiting from the joint embedding space of Point-\nBind, we respectively introduce several emergent applica-\ntion scenarios concerning 3D and multi-modalities.\nAny-to-3D Generation.\nInherited from 2D generative\nmodels, existing 3D generation methods can only achieve\ntext-to-3D synthesis.\nIn contrast, with the joint embed-\nding space of Point-Bind, we can generate 3D shapes con-\nditioned on any modalities, i.e., text/image/audio/point-to-\nmesh. In detail, we directly connect the multi-modal en-\ncoders of Point-Bind with the pre-trained decoders of cur-\nrent CLIP-based text-to-3D models, e.g., CLIP-Forge [65].\nWithout further training, we can synthesize a 3D car mesh\nbased on an input car horn.\n3D Embedding-space Arithmetic.\nWe observe that 3D\nfeatures encoded by Point-Bind can be directly added with\nother modalities to incorporate their semantics, further\nachieving composed cross-modal retrieval. For instance, the\ncombined embeddings of a 3D car and audio of sea waves\ncan retrieve an image showing a car parking by a beach,\nwhile the composition of a 3D laptop and audio of keyboard\ntyping can retrieve an image of someone who is working\nwith a laptop.\n3D\nZero-shot\nUnderstanding.\nFor\ntraditional\ntext-\ninferred 3D zero-shot classification, Point-Bind attains\nstate-of-the-art performance guided by additional multi-\nmodal supervision. Besides, Point-Bind can also achieve\naudio-referred 3D open-world understanding, i.e., recogniz-\ning 3D shapes of novel categories indicated by the corre-\nsponding audio data [51].\n4. Point-LLM\nIn this section, we illustrate how to leverage Point-Bind\nto develop 3D large language models (LLMs), termed as\nPoint-LLM, which fine-tunes LLaMA [73] to achieve 3D\nquestion answering and multi-modal reasoning. The overall\npipeline of Point-LLM is shown in Figure 5.\n4.1. 3D Instruction-following Capacity\nOur Point-LLM is developed on top of ImageBind-\nLLM [19], which conducts multi-modality instruction tun-\ning by injecting the semantics of ImageBind into LLaMA.\nOur approach exhibits both data and parameter efficiency.\nNo Need for 3D Instruction Data.\nDuring training, only\nthe public vision-language data [9,13,68,70] is required for\nfine-tuning LLaMA to learn the 3D-conditioned response\ncapacity. As Point-Bind has built a joint embedding space\nbetween 3D and multi-modalities, if any one of the modal-\nities is trained to connect with LLaMA, the others would\nalso be aligned at the same time. Considering this, we se-\nlect the 2D image modality, since it has the most public data\nwith paired language. By only aligning ImageBind\u2019s image\nencoder with LLaMA, we avoid the expensive cost of col-\nlecting and annotating large-scale 3D instruction data, thus\nsaving extensive resources.\nParameter-efficient Training.\nInstead of tuning the en-\ntire LLMs [38, 97], we only unfreeze partial parameters\nwithin LLaMA for efficient vision-language instruction tun-\ning. Specifically, a learnable bind network is adopted to\nbridge the image encoder of ImageBind with the language\nspace of LLaMA. Then, a zero-initialized gating mecha-\nnism is proposed to add the image features after the bind\nBind \nNetwork\nLLaMA\nAudio: [ Pouring Water ]\nPoint-Bind\nImageBind\n3D\nPoint Cloud:\nEnhanced by Retrieval\n2D Image:\n\u2295\nWhat\u2019s happening?\nThe scene shows a\nperson\npouring\na\nclear liquid into a\nglass,\nwhich\nis\nplaced on a table ...\nCache Model\nAddition\nOptional\nMulti-modal\nInput\nTransformed\nFeature\nInject\nFigure 5. Inference Paradigm of Point-LLM. Referring to ImageBind-LLM [19], we adopt a bind network, a visual cache model, and\nzero-initialized gating mechanisms to fine-tune LLaMA [73] to follow 3D instructions. Optionally, our Point-LLM can also take as input\nmulti-modality data, and conduct cross-modal reasoning for language response.\nnetwork to the words tokens within LLaMA. This mech-\nanism can progressively inject visual instruction cues into\nLLaMA for stable training at early stages, inspired by\nLLaMA-Adapter [91]. By such a parameter-efficient fine-\ntuning strategy, most parameters of LLaMA are kept frozen,\nand only the zero-initialized gating factors and bias-norm\nweights [91] are learnable for fine-tuning.\nPlease refer\nto ImageBind-LLM [19] for further training details. Af-\nter the vision-language training, the joint embedding space\nenables LLaMA to naturally align with other modalities,\nsuch as audio within ImageBind and 3D point clouds of\nPoint-Bind. Therefore, our Point-LLM effectively provides\nLLaMA with 3D-instruction following capacity without any\n3D instruction data, indicating superior data efficiency.\n4.2. 3D Question Answering\nFor an input language instruction and a 3D point cloud,\nwe feed them into the fine-tuned LLaMA and our Point-\nBind, respectively. Then, the encoded 3D feature is en-\nhanced by a visual cache model proposed in ImageBind-\nLLM, before feeding into the bind network.\nThe cache\nmodel is only adopted during inference, and constructed in\na training-free manner [94].\nEnhancement by Visual Cache.\nAs we adopt the image\nencoder of ImageBind for training, but switch to Point-\nBind\u2019s 3D encoder for inference, the cache model is de-\nsigned to alleviate such modality discrepancy for better 3D\ngeometry understanding. Referring to ImageBind-LLM, the\ncache model stores from three ImageBind-encoded image\nfeatures from the training data, which are regarded as both\nkeys and values for knowledge retrieval. We regard the in-\nput 3D feature as the query, and retrieve the top-k similar\nvisual keys from the cache model. Then, according to the\ncosine similarity, we aggregate the corresponding cached\nvalues (top-k similar image features), and add the result to\nthe original 3D feature via a residual connection. In this\nway, the enhanced 3D feature can adaptively incorporate\nsimilar visual semantics from the cache model. This boosts\nthe representation quality of 3D shapes, and mitigates the\nsemantic gap of 2D-3D encoders within Point-LLM. After\nthis, the enhanced feature is fed into the bind network for\nfeature transformation and LLaMA for response generation.\n3D and Multi-modal Reasoning.\nIn addition to point\nclouds, our Point-LLM can also conduct cross-modal rea-\nsoning and generate responses conditioned on multiple\nmodalities. For an additional input image or audio, we uti-\nlize the image or audio encoder of ImageBind to extract\nthe features, and directly add them with the 3D feature en-\ncoded by Point-Bind. By injecting such integrated features\ninto LLaMA, Point-LLM can reason cross-modal seman-\ntics, and respond with the information of all input modali-\nties. This demonstrates the promising significance of align-\ning multi-modality with 3D LLMs.\n5. Experiments\nIn this section, we first present the implementation de-\ntails of the multi-modal training for Point-Bind.\nThen,\nwe illustrate the emergent multi-modal applications, i.e.,\nPoint-LLM for 3D instruction following, 3D cross-modal\nretrieval, 3D embedding-space arithmetic, any-to-3D gener-\n3D Embedding-space Arithmetic\nSea Wave\n+\nKeyboard \nTyping\n+\nWater\n+\nFigure 6. Embedding-space Arithmetic of 3D and Audio. We demonstrate Point-Bind\u2019s capability for multi-modal semantic composition\nby retrieving 2D images with a combination of 3D point cloud and audio embeddings.\nation, and 3D zero-shot understanding. Finally, we conduct\nan ablation study to verify the effectiveness of our designs.\n5.1. Implementation Details\nWe adopt pre-trained I2P-MAE [92] as the 3D encoder of\nPoint-Bind, and utilize the collected 3D-image-text-audio\npairs for pre-training. We only update the 3D encoder with\nthe newly added projection network, and freeze the en-\ncoders of other modalities in ImageBind [22]. The projec-\ntion network is simply composed of two linear layers with\nan intermediate LayerNorm [4]. We train Point-Bind for\n300 epochs with a batch size of 64, and adopt AdamW [44]\nas the optimizer with a learning rate of 0.003.\n5.2. Point-LLM for 3D Q&A\nSettings.\nWe refer to ImageBind-LLM [19] to conduct\nparameter- and data-efficient fine-tuning to inject 3D in-\nstructions into the pre-trained LLaMA 7B model [73]. In\ndetail, the fine-tuning techniques include zero-initialized\ngating [20, 91], LoRA [29], and bias-norm tuning [18, 21,\n81,88]. We utilize a collection of several datasets [9,68,70]\nfor vision-language training, and require no 3D instruction-\nfollowing dataset due to the learned joint embedding space.\nAnalysis.\nIn Figure 3, we provide the question-answering\nexamples of Point-LLM, which shows favorable 3D\ninstruction-following and multi-modal reasoning capacity.\nAs shown, for either English or Chinese instructions, Point-\nLLM can effectively incorporate the spatial geometry of in-\nput point clouds and generate detailed language responses.\nIt obtains a comprehensive 3D understanding for both\nglobal and local characteristics, e.g., recognizing the pat-\nTable 1. Performance on 3D Cross-modal Retrieval, including\n3D-to-3D, 2D-to-3D, 3D-to-2D, and text-to-3D retrieval. We re-\nport the mAP scores (%) on ModelNet40 [80] dataset.\nMethod\n3D \u2192 3D\n2D \u2192 3D\n3D \u2192 2D\nText \u2192 3D\nPointCLIP [90]\n37.63\n13.12\n5.28\n10.86\nPointCLIP-V2 [100]\n47.94\n20.48\n9.22\n52.73\nULIP [84]\n60.58\n20.30\n29.75\n50.51\nPoint-Bind\n63.23\n34.59\n42.83\n64.50\nGain\n+2.65\n+14.29\n+13.08\n+13.99\ntern of the piano keyboard and the shape of the airplane\u2019s\nwing and tail.\nThen, our Point-LLM can also respond\nwith cross-modal understanding. For an input 3D model\nwith a 2D image or audio, Point-LLM can enable LLaMA\nto take both two conditions into understanding and rea-\nsoning, which thus incorporates multi-modal semantics in\nthe output language response.\nWith superior data- and\nparameter-efficiency, the examples indicate the 3D multi-\nmodal instruction-following capabilities of Point-LLM.\n5.3. 3D Cross-modal Retrieval\nTo evaluate the multi-modal alignment of Point-Bind, we\nexperiment on several cross-modal retrieval tasks, i.e., 3D-\nto-3D, 2D-to-3D, 3D-to-2D, and text-to-3D retrieval.\nSettings.\nWe conduct 3D zero-shot retrieval on multi-\nmodal ModelNet40 [80] dataset, which contains 9,843\nCAD models for training and 2,468 for testing of 40 cat-\negories. ModelNet40 provides data of three modalities for\nretrieval, i.e., image, point cloud, and mesh. We obtain the\nretrieved results by ranking the similarities between embed-\n[Truck Sound]\n\u201cA Chair\u201d\n[Car Horn]\n\u201cAn Airplane\u201d\nText-to-3D:\nAudio-to-3D:\nImage-to-3D:\nPoint-to-3D:\nFigure 7. Any-to-3D Generation. Besed on CLIP-Forge [65], our constructed joint embedding space can effectively generate 3D mesh\nmodels conditioned on text, audio, image, and point cloud input.\nTable 2. Performance of 3D Zero-shot Classification. We report\nthe classification accuracy (%) on ModelNet40 [80].\nModel\nEncoder\nPerformance\nPointCLIP [90]\nCLIP\n20.2\nULIP [84]\nPoint-BERT\n60.4\nPointCLIP V2 [100]\nCLIP\n64.2\nULIP 2 [85]\nPoint-BERT\n66.4\nPoint-Bind\nPoint-BERT\n76.3\nPoint-Bind\nI2P-MAE\n78.0\nGain\n-\n+11.6\ndings of point clouds and other modalities. Following pre-\nvious works [33,42], we measure the networks via the mean\nAverage Precision (mAP) score, a commonly used evalua-\ntion criterion for retrieval tasks.\nAnalysis.\nIn Table 1, we report the quantitive results\nfor 3D zero-shot retrieval, where Point-Bind attains state-\nof-the-art performance on all benchmarks compared with\nprior works.\nIn particular, for 2D-to-3D and text-to-3D\nretrieval, Point-Bind surpasses the second-top ULIP [84]\nsignificantly by +14.29% and +13.99% improvements, re-\nspectively. This indicates the superior cross-modal under-\nstanding capacity of our approach.\n5.4. Embedding-space Arithmetic with 3D\nWith the multi-modal alignment, we further explore the\ncapability of embedding composition, i.e., the embedding-\nspace arithmetic of 3D and other modalities, e.g., audio.\nSettings.\nTo obtain the multi-modal input for arithmetic,\nwe utilize 3D objects from ShapeNet [8] and TextANI-\nMAR2023 [7], and audio clips from ESC-50 [51]. We sim-\nply add the 3D and audio embeddings from Point-Bind and\nImageBind, respectively, and then retrieve 2D images from\nImageNet [14] with 1,000 image categories.\nAnalysis.\nIn Figure 6, we show the results of 2D image\nretrieval with the composed embeddings between 3D and\naudio. As shown in the first row, with the combined em-\nbeddings of a 3D dog and sea-wave audio, we effectively\nretrieve 2D images of dogs by the sea. Similarly, with the\ncombination of a 3D laptop and keyboard-typing audio, the\nobtained images show someone is working with a laptop, or\na cat inadvertently presses on the keyboard. Likewise, the\nlast row retrieves images of bears hunting by the water by\nusing embeddings of a 3D bear and audio of flowing water.\nThe examples demonstrate that the 3D features encoded by\nPoint-Bind can be directly added with other modalities, and\nwell incorporate their semantics, achieving favorable com-\nposed cross-modal retrieval capacity.\n5.5. Any-to-3D Generation\nSettings.\nExisting text-to-3D generation methods nor-\nmally adopt CLIP\u2019s text encoder to process the input lan-\nguage prompt. Considering this, we simply replace it with\nthe multi-modalities encoders of Point-Bind and ImageBind\nwithout further training, which follows the original genera-\ntive decoder for 3D shape synthesis. We adopt the decoder\nof CLIP-Forge [65] by default.\nAnalysis.\nIn Figure 7, we show the examples of any-to-\n3D generation powered by Point-Bind. For text, audio, and\npoint cloud prompts, our approach can all produce satisfac-\ntory 3D meshes. This demonstrates the well-aligned em-\nbedding space of 3D and multiple modalities.\n5.6. 3D Zero-shot Understanding\nIn this section, we test the open-word understanding abil-\nity of Point-Bind, i.e., recognizing novel classes, by 3D\nzero-shot classification on ModelNet40 [80] dataset.\nSettings.\nFollowing previous works, we utilize the text\nembeddings from CLIP\u2019s [59] or ImageBind [22]\u2019s text en-\ncoder to construct the zero-shot classification head. Specifi-\nTable 3. Ablation Study exploring different designs of the pro-\njection network and 3D encoders. We report the results (%) for\nzero-shot classification on ModelNet40 [80].\nProjection\nAcc.\n3D Encoder\nAcc.\nOne Linear\n76.46\nPointNeXt [57]\n67.96\nTwo Linear\n78.00\nPoint-BERT [86]\n76.70\nThree Linear\n76.78\nI2P-MAE [92]\n78.00\ncally, we apply a simple template of \u2018a/an [CLASS]\u2019 for the\n40 categories of ModelNet40, and calculate the cosine sim-\nilarity between 3D and all textual embeddings, selecting the\nmost similar one as the final prediction.\nAnalysis.\nWe report the 3D zero-shot classification ac-\ncuracy in Table 2, where our Point-Bind surpasses exist-\ning methods with state-of-the-art performance. This indi-\ncates the unified representation space of Point-Bind leads to\nstrong emergent 3D open-world recognition.\n5.7. Ablation Study\nTo investigate the effectiveness of our designs in Point-\nBind, we conduct ablation studies on the projection network\nand 3D encoders in Table 3. We report the performance\nof zero-shot classification on ModelNet40 [80] dataset. In\nthe first two columns, we experiment with different pro-\njection schemes for embeddings after the 3D encoder. As\nshown, using two linear layers for embedding projection\nperforms the best. In the last two columns, we utilize dif-\nferent 3D encoders in Point-Bind, i.e., Point-BERT [86],\nPointNeXt [57], and I2P-MAE [92]. As reported, the self-\nsupervised Point-BERT and I2P-MAE achieve much better\nperformance, indicating the importance of 3D pre-training\nto boost the multi-modal alignment.\n6. Conclusion\nIn this paper, we propose Point-Bind, a 3D multi-\nmodality model that aligns 3D point clouds with multi-\nmodalities, guided by ImageBind. By aligning 3D objects\nwith their corresponding image-audio-text pairs, Point-Bind\nobtains a joint embedding space, and exhibits promising\n3D multi-modal tasks, such as any-to-3D generation, 3D\nembedding arithmetic, and 3D open-world understanding.\nUpon that, we further introduce Point-LLM, the first 3D\nlarge language model (LLM) with instruction-following ca-\npability in both English and Chinese.\nOur future work\nwill focus on aligning multi-modality with more diverse 3D\ndata, such as indoor and outdoor scenes, which allows for\nwider application scenarios.\nReferences\n[1] Mohamed Afham, Isuru Dissanayake, Dinithi Dissanayake,\nAmaya Dharmasiri, Kanchana Thilakarathna, and Ranga\nRodrigo. CrossPoint: Self-supervised Cross-modal Con-\ntrastive Learning for 3D Point Cloud Understanding.\nIn\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 9902\u20139912, 2022. 1, 4\n[2] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioan-\nnis Brilakis, Martin Fischer, and Silvio Savarese. 3D Se-\nmantic Parsing of Large-scale Indoor Spaces. In IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n1534\u20131543, 2016. 1\n[3] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Mo-\ntoaki Kawanabe.\nScanQA: 3D Question Answering for\nSpatial Scene Understanding. In IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 19129\u201319139,\n2022. 1\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer Normalization.\narXiv preprint arXiv:1607.06450,\n2016. 8\n[5] Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin.\nEnd-to-End Referring Video Object Segmentation with\nMultimodal Transformers. In IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 4985\u20134995,\n2022. 4\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. Language Models\nare Few-Shot Learners. In Advances in Neural Information\nProcessing Systems, volume 33, pages 1877\u20131901, 2020.\n[7] TextANIMAR Challenge.\nTextANIMAR.\nhttps:\n//aichallenge.hcmus.edu.vn/textanimar,\n2023. 9\n[8] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapeNet:\nAn Information-rich 3D Model Repository. arXiv preprint\narXiv:1512.03012, 2015. 5, 9\n[9] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text\npre-training to recognize long-tail visual concepts. In IEEE\nConference on Computer Vision and Pattern Recognition,\npages 3558\u20133568, 2021. 3, 6, 8\n[10] Anthony Chen, Kevin Zhang, Renrui Zhang, Zihan Wang,\nYuheng Lu, Yandong Guo, and Shanghang Zhang. PiMAE:\nPoint Cloud and Image Interactive Masked Autoencoders\nfor 3D Object Detection. In IEEE Conference on Computer\nVision and Pattern Recognition, pages 5291\u20135301, 2023. 4\n[11] Siheng Chen, Baoan Liu, Chen Feng, Carlos Vallespi-\nGonzalez, and Carl Wellington. 3D Point Cloud Process-\ning and Learning for Autonomous Driving: Impacting Map\nCreation, Localization, and Perception. IEEE Signal Pro-\ncessing Magazine, 38(1):68\u201386, 2020. 1\n[12] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang\nYu, and Tao Chen. End-to-end 3d dense captioning with\nvote2cap-detr. In IEEE Conference on Computer Vision and\nPattern Recognition, pages 11124\u201311133, 2023. 1\n[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\nVedantam, Saurabh Gupta, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft COCO Captions: Data Collection and\nEvaluation Server. arXiv preprint arXiv:1504.00325, 2015.\n3, 6\n[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and\nLi Fei-Fei. ImageNet: A Large-scale Hierarchical Image\nDatabase.\nIn IEEE Conference on Computer Vision and\nPattern Recognition, pages 248\u2013255, 2009. 9\n[15] Karan Desai and Justin Johnson. VirTex: Learning Visual\nRepresentations from Textual Annotations. In IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n11162\u201311173, 2021. 4\n[16] Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang,\nJianjian Sun, Zheng Ge, Li Yi, and Kaisheng Ma.\nAu-\ntoencoders as cross-modal teachers: Can pretrained 2d im-\nage transformers help 3d representation learning?\narXiv\npreprint arXiv:2212.08320, 2022. 4\n[17] Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen.\nCLIP2Video: Mastering Video-Text Retrieval via Image\nCLIP. arXiv preprint arXiv:2106.11097, 2021. 4\n[18] Jonathan Frankle, David J Schwab, and Ari S Morcos.\nTraining batchnorm and only batchnorm: On the expres-\nsive power of random features in cnns.\narXiv preprint\narXiv:2003.00152, 2020. 8\n[19] Peng Gao, Jiaming Han, Chris Liu, Ziyi Lin, Renrui Zhang,\nand Ziyu Guo. ImageBind-LLM. https://github.\ncom/OpenGVLab/LLaMA-Adapter/tree/main/\nimagebind_LLM, 2023. 3, 6, 7, 8\n[20] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shi-\njie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He,\nXiangyu Yue, et al.\nLLaMA-Adapter V2:\nParameter-\nEfficient Visual Instruction Model.\narXiv preprint\narXiv:2304.15010, 2023. 3, 8\n[21] Angeliki Giannou, Shashank Rajput, and Dimitris Papail-\niopoulos. The expressive power of tuning only the norm\nlayers. arXiv preprint arXiv:2302.07937, 2023. 8\n[22] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat\nSingh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan\nMisra. ImageBind: One Embedding Space to Bind Them\nAll. In IEEE Conference on Computer Vision and Pattern\nRecognition, pages 15180\u201315190, 2023. 2, 4, 5, 8, 9\n[23] Ziyu Guo, Xianzhi Li, and Pheng Ann Heng. Joint-mae:\n2d-3d joint masked autoencoders for 3d point cloud pre-\ntraining. arXiv preprint arXiv:2302.14007, 2023. 1, 4\n[24] Ziyu Guo, Yiwen Tang, Renrui Zhang, Dong Wang, Zhi-\ngang Wang, Bin Zhao, and Xuelong Li. Viewrefer: Grasp\nthe multi-view knowledge for 3d visual grounding with gpt\nand prototype guidance. arXiv preprint arXiv:2303.16894,\n2023. 1, 4\n[25] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma,\nXupeng Miao, Xuming He, and Bin Cui. Calip: Zero-shot\nenhancement of clip with parameter-free attention. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence,\nvolume 37, pages 746\u2013754, 2023. 4\n[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked Autoencoders Are Scal-\nable Vision Learners. arXiv:2111.06377, 2021. 4\n[27] Deepti Hegde, Jeya Maria Jose Valanarasu, and Vishal M\nPatel.\nCLIP goes 3D: Leveraging Prompt Tuning for\nLanguage Grounded 3D Recognition.\narXiv preprint\narXiv:2303.11313, 2023. 4\n[28] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen,\nJoshua B. Tenenbaum, and Chuang Gan.\n3D Concept\nLearning and Reasoning From Multi-View Images. In IEEE\nConference on Computer Vision and Pattern Recognition,\npages 9202\u20139212, 2023. 1\n[29] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 8\n[30] Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui\nHuang, Rynson WH Lau, Wanli Ouyang, and Wangmeng\nZuo.\nCLIP2Point: Transfer CLIP to Point Cloud Clas-\nsification with Image-Depth Pre-training.\narXiv preprint\narXiv:2210.01055, 2022. 4\n[31] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li,\nJiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value\nmaps for robotic manipulation with language models. arXiv\npreprint arXiv:2307.05973, 2023. 1\n[32] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter\nAbbeel, and Ben Poole.\nZero-Shot Text-Guided Object\nGeneration with Dream Fields. 2022. 4\n[33] Longlong Jing, Elahe Vahdani, Jiaxing Tan, and Yingli\nTian. Cross-modal Center Loss for 3D Cross-modal Re-\ntrieval. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 3142\u20133151, 2021. 9\n[34] Ruihui Li, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and\nPheng-Ann Heng. PU-GAN: a point cloud upsampling ad-\nversarial network.\nIn International Conference on Com-\nputer Vision, 2019. 4\n[35] Chen-Hsuan\nLin,\nJun\nGao,\nLuming\nTang,\nTowaki\nTakikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja\nFidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-\nresolution text-to-3d content creation. In IEEE Conference\non Computer Vision and Pattern Recognition, pages 300\u2013\n309, 2023. 1\n[36] Xudong Lin, Gedas Bertasius, Jue Wang, Shih-Fu Chang,\nDevi Parikh, and Lorenzo Torresani. VX2TEXT: End-to-\nEnd Learning of Video-Based Text Generation From Mul-\ntimodal Inputs. In IEEE Conference on Computer Vision\nand Pattern Recognition, pages 7005\u20137015, 2021. 4\n[37] Yan-Bo Lin, Hung-Yu Tseng, Hsin-Ying Lee, Yen-Yu\nLin, and Ming-Hsuan Yang.\nExploring Cross-Video\nand Cross-Modality Signals for Weakly-Supervised Audio-\nVisual Video Parsing. Advances in Neural Information Pro-\ncessing Systems, 34, 2021. 4\n[38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning, 2023. 6\n[39] Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu,\nXuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and\nHao Su. OpenShape: Scaling Up 3D Shape Representa-\ntion Towards Open-World Understanding. arXiv preprint\narXiv:2305.10764, 2023. 1, 4\n[40] Yueh-Cheng Liu, Yu-Kai Huang, Hung-Yueh Chiang,\nHung-Ting Su, Zhe-Yu Liu, Chin-Tang Chen, Ching-Yu\nTseng, and Winston H Hsu.\nLearning from 2D: Con-\ntrastive Pixel-to-point Knowledge Transfer for 3D Pretrain-\ning. arXiv preprint arXiv:2104.04687, 2021. 4\n[41] Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, and Chi-\nWing Fu. ISS++: Image as Stepping Stone for Text-Guided\n3D Shape Generation. arXiv preprint arXiv:2303.15181,\n2023. 4\n[42] Zhitao Liu, Zengyu Liu, Jiwei Wei, Guan Wang, Zhenjiang\nDu, Ning Xie, and Heng Tao Shen. Instance-Variant Loss\nwith Gaussian RBF Kernel for 3D Cross-modal Retriveal.\narXiv preprint arXiv:2305.04239, 2023. 9\n[43] Ze Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong.\nGroup-free 3D Object Detection via Transformers. In In-\nternational Conference on Computer Vision, pages 2949\u2013\n2958, 2021. 1\n[44] Ilya Loshchilov and Frank Hutter.\nDecoupled Weight\nDecay Regularization. arXiv preprint arXiv:1711.05101,\n2017. 8\n[45] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes. arXiv preprint arXiv:2112.03221, 2021. 4\n[46] Ishan Misra, Rohit Girdhar, and Armand Joulin. An End-\nto-end Transformer Model for 3D Object Detection. In In-\nternational Conference on Computer Vision, pages 2906\u2013\n2917, 2021. 1\n[47] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa.\nCLIP-Mesh:\nGenerating Textured\nMeshes from Text using Pretrained Image-text Models. In\nACM SIGGRAPH Asia Conference, pages 1\u20138, 2022. 4\n[48] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja\nHauth, Santiago Manen, Chen Sun, and Cordelia Schmid.\nLearning Audio-Video Modalities from Image Captions. In\nEuropean Conference on Computer Vision, pages 407\u2013426,\n2022. 4\n[49] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen.\nPoint-e: A System for Gen-\nerating 3D Point Clouds from Complex Prompts.\narXiv\npreprint arXiv:2212.08751, 2022. 1\n[50] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu,\nYonghong Tian, and Li Yuan. Masked Autoencoders for\nPoint Cloud Self-supervised Learning. In European Con-\nference on Computer Vision, pages 604\u2013621, 2022. 4\n[51] Karol J Piczak.\nESC: Dataset for Environmental Sound\nClassification. In ACM International Conference on Multi-\nmedia, pages 1015\u20131018, 2015. 5, 6, 9\n[52] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv\npreprint arXiv:2209.14988, 2022. 1\n[53] Omid Poursaeed, Tianxing Jiang, Quintessa Qiao, Nayun\nXu, and Vladimir G. Kim.\nSelf-supervised Learning of\nPoint Clouds via Orientation Estimation.\narXiv preprint\narXiv:2008.00305, 2020. 4\n[54] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointNet: Deep Learning on Point Sets for 3D Classifica-\ntion and Segmentation. arXiv preprint arXiv:1612.00593,\n2016. 4\n[55] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-\nNet++: Deep Hierarchical Feature Learning on Point Sets\nin a Metric Space. arXiv preprint arXiv:1706.02413, 2017.\n4\n[56] Zekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu\nZhang, Kaisheng Ma, and Li Yi. Contrast with Reconstruct:\nContrastive 3D Representation Learning Guided by Gener-\native Pretraining. In International Conference on Machine\nLearning, 2023. 1, 4\n[57] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai,\nHasan Hammoud,\nMohamed Elhoseiny,\nand Bernard\nGhanem.\nPointNeXt:\nRevisiting PointNet++ with Im-\nproved Training and Scaling Strategies.\nIn Advances in\nNeural Information Processing Systems, 2022. 4, 10\n[58] Guocheng Qian, Xingdi Zhang, Abdullah Hamdi, and\nBernard Ghanem. Pix4Point: Image Pretrained Transform-\ners for 3D Point Cloud Understanding. arXiv, 2022. 4\n[59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning Transferable Visual Models from Natural Language Su-\npervision. In International Conference on Machine Learn-\ning, pages 8748\u20138763, 2021. 4, 5, 9\n[60] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen.\nHierarchical text-conditional\nimage generation with clip latents.\narXiv preprint\narXiv:2204.06125, 1(2):3, 2022. 1\n[61] Karthik Ramesh, Chao Xing, Wupeng Wang, Dong Wang,\nand Xiao Chen. Vset: A Multimodal Transformer for Vi-\nsual Speech Enhancement.\nIn International Conference\non Acoustics, Speech and Signal Processing, pages 6658\u2013\n6662, 2021. 4\n[62] Yongming Rao, Jiwen Lu, and Jie Zhou.\nGlobal-Local\nBidirectional Reasoning for Unsupervised Representation\nLearning of 3D Point Clouds. In IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2020. 4\n[63] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models.\nIn IEEE Confer-\nence on Computer Vision and Pattern Recognition, pages\n10684\u201310695, 2022. 1\n[64] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali-\nmans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. Advances in Neural In-\nformation Processing Systems, 35:36479\u201336494, 2022. 1\n[65] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang,\nChin-Yi Cheng, and Marco Fumero.\nClip-Forge:\nTo-\nwards Zero-shot Text-to-shape Generation. arXiv preprint\narXiv:2110.02624, 2021. 4, 6, 9\n[66] Jonathan Sauder and Bjarne Sievers. Self-supervised Deep\nLearning on Point Clouds by Reconstructing Space. Ad-\nvances in Neural Information Processing Systems, 32,\n2019. 4\n[67] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia\nLiu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A plat-\nform for embodied ai research. In International Conference\non Computer Vision, pages 9339\u20139347, 2019. 1\n[68] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for train-\ning next generation image-text models. Advances in Neural\nInformation Processing Systems, 35:25278\u201325294, 2022.\n3, 6, 8\n[69] Hao Shao, Letian Wang, Ruobing Chen, Hongsheng Li, and\nYu Liu. Safety-enhanced autonomous driving using inter-\npretable sensor fusion transformer. In Conference on Robot\nLearning, pages 726\u2013737. PMLR, 2023. 1\n[70] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual Captions: A Cleaned, Hypernymed,\nImage Alt-text Dataset for Automatic Image Captioning. In\nAnnual Meeting of the Association for Computational Lin-\nguistics, pages 2556\u20132565, 2018. 3, 6, 8\n[71] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping\nShi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-\nvoxel feature set abstraction for 3d object detection. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 10529\u201310538, 2020. 1\n[72] Desney S Tan, George G Robertson, and Mary Czerwinski.\nExploring 3d navigation: combining speed-coupled flying\nwith orbiting. In Proceedings of the SIGCHI conference\non Human factors in computing systems, pages 418\u2013425,\n2001. 1\n[73] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. arXiv preprint arXiv:2302.13971, 2023. 2, 6, 7, 8\n[74] Thang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, and\nChang D Yoo. Softgroup for 3D Instance Segmentation on\nPoint Clouds. In IEEE Conference on Computer Vision and\nPattern Recognition, pages 2708\u20132717, 2022. 1\n[75] Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and\nMatthew J. Kusner. Unsupervised Point Cloud Pre-Training\nvia Occlusion Completion. In International Conference on\nComputer Vision, pages 9782\u20139792, 2021. 4\n[76] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng\nGao, Dinghan Shen, Yuan-Fang Wang, William Yang\nWang, and Lei Zhang. Reinforced cross-modal matching\nand self-supervised imitation learning for vision-language\nnavigation. In IEEE Conference on Computer Vision and\nPattern Recognition, pages 6629\u20136638, 2019. 1\n[77] Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, and Ji-\nwen Lu. P2P: Tuning Pre-trained Image Models for Point\nCloud Analysis with Point-to-pixel Prompting. Advances in\nNeural Information Processing Systems, 35:14388\u201314402,\n2022. 4\n[78] Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Ab-\nhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi\nParikh, and Dhruv Batra. Embodied Question Answering\nin Photorealistic Environments with Point Cloud Percep-\ntion. In IEEE Conference on Computer Vision and Pattern\nRecognition, pages 6659\u20136668, 2019. 1\n[79] Yanmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng,\nand Jian Zhang. Eda: Explicit text-decoupling and dense\nalignment for 3d visual grounding. In IEEE Conference on\nComputer Vision and Pattern Recognition, pages 19231\u2013\n19242, 2023. 1\n[80] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu,\nLinguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3d\nshapenets: A deep representation for volumetric shapes. In\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 1912\u20131920, 2015. 8, 9, 10\n[81] Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou,\nZhaoqiang Liu, Jiawei Li, and Zhenguo Li.\nDifffit:\nUnlocking transferability of large diffusion models via\nsimple parameter-efficient fine-tuning.\narXiv preprint\narXiv:2304.06648, 2023. 8\n[82] Chenfeng Xu, Shijia Yang, Tomer Galanti, Bichen Wu,\nXiangyu Yue, Bohan Zhai, Wei Zhan, Peter Vajda, Kurt\nKeutzer, and Masayoshi Tomizuka.\nImage2Point:\n3D\nPoint-Cloud Understanding with 2D Image Pretrained\nModels.\nIn European Conference on Computer Vision,\npages 638\u2013656, 2022. 4\n[83] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying\nShan, Xiaohu Qie, and Shenghua Gao. Dream3D: Zero-\nshot Text-to-3D Synthesis using 3D Shape Prior and Text-\nto-image Diffusion Models. In IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 20908\u201320918,\n2023. 4\n[84] Le Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n,\nJiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles,\nand Silvio Savarese. ULIP: Learning Unified Representa-\ntion of Language, Image and Point Cloud for 3D Under-\nstanding. arXiv preprint arXiv:2212.05171, 2022. 1, 4, 5,\n8, 9\n[85] Le Xue, Ning Yu, Shu Zhang, Junnan Li, Roberto Mart\u00b4\u0131n-\nMart\u00b4\u0131n, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Car-\nlos Niebles, and Silvio Savarese. ULIP-2: Towards Scal-\nable Multimodal Pre-training for 3D Understanding. arXiv\npreprint arXiv:2305.08275, 2023. 4, 9\n[86] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie\nZhou, and Jiwen Lu. Point-BERT: Pre-Training 3D Point\nCloud Transformers with Masked Point Modeling. In IEEE\nConference on Computer Vision and Pattern Recognition,\n2022. 4, 10\n[87] Zhihao Yuan, Xu Yan, Yinghong Liao, Yao Guo, Guanbin\nLi, Shuguang Cui, and Zhen Li. X-trans2cap: Cross-modal\nknowledge transfer using transformer for 3d dense caption-\ning. In IEEE Conference on Computer Vision and Pattern\nRecognition, pages 8563\u20138573, 2022. 1\n[88] Elad Ben Zaken,\nShauli Ravfogel,\nand Yoav Gold-\nberg.\nBitfit:\nSimple parameter-efficient fine-tuning\nfor transformer-based masked language-models.\narXiv\npreprint arXiv:2106.10199, 2021. 8\n[89] Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bin\nZhao, Dong Wang, Yu Qiao, and Hongsheng Li.\nPoint-\nM2AE: Multi-scale Masked Autoencoders for Hierarchical\nPoint Cloud Pre-training. arXiv preprint arXiv:2205.14401,\n2022. 4\n[90] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-\npeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng\nLi. PointCLIP: Point Cloud Understanding by CLIP. In\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 8552\u20138562, 2022. 1, 4, 8, 9\n[91] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu\nQiao.\nLLaMA-Adapter:\nEfficient Fine-tuning of Lan-\nguage Models with Zero-init Attention.\narXiv preprint\narXiv:2303.16199, 2023. 3, 7, 8\n[92] Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, and\nHongsheng Li.\nLearning 3D Representations from 2D\nPre-trained Models via Image-to-point Masked Autoen-\ncoders. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 21769\u201321780, 2023. 1, 2, 4, 6, 8,\n10\n[93] Renrui Zhang, Liuhui Wang, Yali Wang, Peng Gao, Hong-\nsheng Li, and Jianbo Shi. Parameter is not all you need:\nStarting from non-parametric networks for 3d point cloud\nanalysis. CVPR 2023, 2023. 4\n[94] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kun-\nchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-\nadapter: Training-free adaption of clip for few-shot clas-\nsification. In European Conference on Computer Vision,\npages 493\u2013510. Springer, 2022. 7\n[95] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hong-\nsheng Li, Yu Qiao, Wanli Ouyang, and Xiangyu Yue. Meta-\ntransformer: A unified framework for multimodal learning.\narXiv preprint arXiv:2307.10802, 2023. 1\n[96] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D\nManning, and Curtis P Langlotz. Contrastive Learning of\nMedical Visual Representations from Paired Images and\nText.\nIn Machine Learning for Healthcare Conference,\npages 2\u201325, 2022. 5, 6\n[97] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 6\n[98] Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang.\nVision-language navigation with self-supervised auxiliary\nreasoning tasks. In IEEE Conference on Computer Vision\nand Pattern Recognition, pages 10012\u201310022, 2020. 1\n[99] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ji-\naming Liu, Hao Dong, and Peng Gao.\nLess is more:\nTowards efficient few-shot 3d semantic segmentation via\ntraining-free networks. arXiv preprint arXiv:2308.12961,\n2023. 4\n[100] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao\nZeng, Zipeng Qin, Shanghang Zhang, and Peng Gao. Point-\nCLIP V2: Prompting CLIP and GPT for Powerful 3D\nOpen-world Learning. arXiv preprint arXiv:2211.11682,\n2022. 1, 4, 8, 9\n"
  }
]