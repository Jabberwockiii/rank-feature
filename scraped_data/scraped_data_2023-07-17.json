[
  {
    "title": "Copy Is All You Need",
    "link": "https://arxiv.org/pdf/2307.06962.pdf",
    "upvote": "31",
    "text": "Published as a conference paper at ICLR 2023\nCOPY IS ALL YOU NEED\nTian Lan\u2662,\u2661,\u2217\nDeng Cai\u2662,\u2217,\u2020\nYan Wang\u2662,\u2020\nHeyan Huang\u2661\nXian-Ling Mao\u2661\n\u2662Tencent AI Lab\n\u2661School of Computer Science and Technology, Beijing Institute of Technology\n{lantiangmftby,thisisjcykcd,yanwang.branden}@gmail.com\n{hhy63,maoxl}@bit.edu.cn\nABSTRACT\nThe dominant text generation models compose the output by sequentially selecting\nwords from a fixed vocabulary. In this paper, we formulate text generation as\nprogressively copying text segments (e.g., words or phrases) from an existing text\ncollection. We compute the contextualized representations of meaningful text\nsegments and index them using efficient vector search toolkits. The task of text\ngeneration is then decomposed into a series of copy-and-paste operations: at each\ntime step, we seek suitable text spans from the text collection rather than selecting\nfrom a standalone vocabulary. Experiments on the standard language modeling\nbenchmark (WikiText-103) show that our approach achieves better generation\nquality according to both automatic and human evaluations. Besides, its inference\nefficiency is comparable to token-level autoregressive models thanks to the reduc-\ntion of decoding steps. We also show that our approach allows for effective domain\nadaptation by simply switching to domain-specific text collection without extra\ntraining. Finally, we observe that our approach attains additional performance gains\nby simply scaling up to larger text collections, again without further training.1\n1\nINTRODUCTION\nMost neural language models (LMs) process text generation tasks by making a series of next-token\npredictions in an autoregressive manner (Radford et al., 2019; Dai et al., 2019; Khandelwal et al.,\n2020; Shi et al., 2022). Specifically, LMs generate the next-token distribution over a fixed vocabulary\nfor any given prefix. Then, the next token is selected by a chosen decoding method, such as greedy\nsearch and nucleus sampling (Holtzman et al., 2020). This process continues until some stop condition\nis reached. For example, a special end-of-generation token is emitted, or the generated text reaches\nthe maximum length limit.\nUnlike traditional neural language models, we reformulate text generation by copying text segments\nfrom existing text collections. The text segments can be of variable lengths, including single words\nand multi-word phrases. For clarity, we will use the term \u201cphrase\u201d to refer to any contiguous text\nsegments, and a single word can also be seen as a phrase of length 1. We compute a contextualized\nvector representation for each phrase and pack them into an offline index. At each decoding step,\na suitable phrase is retrieved from the offline index and appended to the current prefix. In other\nwords, the next-token predictions in traditional neural language models are replaced by a series of\ncopy-and-paste operations.\nOur proposed model, named COG (short for COPY-GENERATOR), enjoys the following advantages.\nFirst, our method selects phrases in specific contexts rather than standalone tokens in a fixed vocab-\nulary. It potentially allows for more accurate candidate representation and selection. Second, our\nmethod allows training-free adaptation to new knowledge sources because the text collection can be\nupdated in a plug-and-play fashion. It could benefit application scenarios such as domain adaptation\nand data expansion/filtering. Third, our method allows a sequence of multiple tokens (i.e., multi-word\n\u2217 Contributed Equally.\n\u2020 Corresponding authors.\n1Our source codes are publicly available at https://github.com/gmftbyGMFTBY/Copyisallyouneed.\n1\narXiv:2307.06962v1  [cs.CL]  13 Jul 2023\nPublished as a conference paper at ICLR 2023\nphrase) to be generated in one single step. It could reduce the total number of decoding steps, leading\nto improved inference efficiency.\nWe conduct extensive experiments to verify the effectiveness of our proposed COG. On the standard\nlanguage modeling benchmark (WikiText-103), our proposed COG substantially outperforms standard\nbaselines on automatic metrics (26.14 vs. 23.43 MAUVE (Pillutla et al., 2021)) and human evaluation\n(48% vs. 28% human preference). Moreover, when we directly switch the text collection from the\nWikiText-103 corpus to a domain-specific corpus, Law-MT (Koehn & Knowles, 2017), our proposed\nCOG outperforms strong baselines on this domain adaption setting (28.14 vs. 26.85 MAUVE and\n52% vs. 36% human preference) without any domain-specific training. Furthermore, when we scale\nup the text collection of COG to a larger one, the En-Wiki dataset, we obtain additional gain (26.97\nvs. 23.43 MAUVE), again without any further training. Our contributions can be summarized as\nfollows:\n\u2022 We propose COG, a method that reformulates text generation tasks as a series of copy-and-\npaste operations from existing text collections.\n\u2022 We show that COG can outperform standard neural language model baselines on existing\nlanguage modeling benchmarks.\n\u2022 We demonstrate that COG allows for training-free adaptations to larger text collections and\ndomain-specific text collections.\n2\nBACKGROUND: NEURAL TEXT GENERATION\nNeural text generation can be divided into two categories: (1) unconditional text generation; (2)\nconditional text generation. Unconditional text generation (or language modeling) aims to generate a\ncoherent text continuation given a prefix. In this case, language models perform generation using\na density estimation over sequences p\u03b8(x). Conditional text generation aims to generate text with\nsome condition c and instead estimates the probability of p\u03b8(x|c). Its typical applications include\nmachine translation (Sutskever et al., 2014; Bahdanau et al., 2015), summarization (See et al., 2017).\nThroughout this paper, our discussion will be focused on unconditional text generation, however, our\napproach can be readily adapted to conditional text generation as well.\nThe canonical approach to language modeling factors the generation in an autoregressive left-to-right\nmanner p\u03b8(x0:n) = Qn\ni=1 p(xi|x<i). In this case, text generation is reduced to the task of repeatedly\npredicting the next token conditioned on the partial sequence (i.e., prefix) generated so far p(xi|x<i).\nThe model often consists of two parts: (1) a prefix encoder and (2) a set of token embeddings. The\nprefix encoder is often parameterized by the Transformer architecture (Vaswani et al., 2017), which\ntransforms any prefix into a fixed-sized vector representation hi \u2208 Rd = PrefixEncoder(x<i). Then,\nthe probability of the next token being w is calculated as\np\u03b8(xi = w|x<i) =\nexp(vw \u00b7 hi)\nP\nw\u2208V exp(vw \u00b7 hi),\nwhere vw is the context-independent token embedding representing the token w, and V is the pre-\ndefined vocabulary consisting of all possible tokens. Based on the chosen decoding method, such as\ngreedy search and nucleus sampling (Holtzman et al., 2020), the next token is selected according to\nthe probability distribution over the fixed vocabulary V . This process is repeated in an autoregressive\nmanner, until some stop condition is reached, e.g., the maximum length of generation is reached.\n3\nCOPY-GENERATOR\nUnlike traditional language models that compute the next token distribution over a fixed vocabulary\nthat is usually composed of words or sub-words (Sennrich et al., 2016; Kudo & Richardson, 2018), our\nproposed COG has a dynamic \u201cvocabulary\u201d that is dependent on the available source text collections.\nEach item in the \u201cvocabulary\u201d corresponds to a text segment (termed as phrase in this paper) in the\nsource text collection. Importantly, all phrases are context-sensitive. That is, the same phrases in\ndifferent contexts are considered to be different. The overall framework is depicted in Figure 1.\nFormally, our approach assumes a set of source documents {D1, . . . , Dn} is available. For each\ndocument Di, a phrase k = Di\ns:e of length e \u2212 s + 1 can be extracted, where s and e mark the start\n2\nPublished as a conference paper at ICLR 2023\nPrefix Encoder\nBERT model\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\nPhrase Encoder\n\u2026\nToken Embeddings\n+\nThe Dune film was released [in theaters on October 22, 2021 in the United States] [and was extremely well-received by critics \nand audiences] [Before] [that] [,] [the film premiered at the 78\ufffd\u210e\u00a0International Film Festival on September 3, 2021.] \n\u2026\n\u2026\nPhrase Table\n\u2026 and was extremely \nwell-received by critics \nand audiences \u2026\nand audiences\n\u2026 in theaters on \nOctober 22, 2021 in \nthe United States \u2026\nin\nStates\n\u2026 the film premiered \nat the 78\ufffd\u210e \u2026  \nSeptember 3, 2021.\nthe\n.\nSource Text \nCollection\nMaximum Inner Product Search\n[\u2026][\u2026]\n[\u2026][\u2026]\nCopy\nFigure 1: The overview of our proposed COG. Given the prefix The Dune film was released, COG\nretrieve 3 phrases (in different colors) from the documents and generates 3 tokens (Before, that, and\nthe comma ,) from the fixed vocabulary to form the whole generation.\nand end positions of the phrase in the document, respectively. We denote all the phrases in the source\ntext collection as P. For a given prefix x<i, we aim to select the best phrases that can form a coherent\ntext continuation following the prefix. To this end, we compute a contextualized representation for\neach phrase pk \u2208 Rd = PhraseEncoder(s, e, Di) using a phrase encoder. Thus, a phrase table\n{(k, pk)|k \u2208 P} can be constructed. Similar to traditional language models, at test time, COG also\nemploys a prefix encoder to map the prefix x<i into a vector representation qi. The fitness of a phrase\nk to the prefix x<i is then measured by the dot product of their vector representations pk and qi:\np(k|x<i) \u221d exp(pk \u00b7 qi).\n(1)\nAt each time step, a suitable phrase is selected and appended to the current prefix accordingly.\nNote that the size of the phrase table can be up to billions. To search over this large candidate pool, we\npre-compute the phrase representations and use a coarse-to-fine search pipeline based on maximum\ninner product search (MIPS) (Johnson et al., 2019). The details are deferred to Section 4.2. Moreover,\nto support the scenarios where no suitable phrases are available, we also add the context-independent\ntoken embeddings {(w, vw)|w \u2208 V } in standard LMs to the phrase table.\nEthical Consideration\nThe text generated by COG contains text segments copied from other\ndocuments, which may cause copyright disputes in real-world applications. Therefore, there are a few\nthings to be considered: (1) The copyright of the source text documents needs to be carefully checked.\nOne should not use documents with strict copyright protection and/or private information; (2) It is\nrecommended to quote the original source explicitly, especially when the retrieved phrases are long.\n3.1\nMODEL ARCHITECTURE\nAs illustrated in Figure 1, our proposed model consists of three major components: (1) a prefix\nencoder that maps prefixes to fixed-sized representations; (2) a context-dependent phrase encoder\nthat computes the vector representations of the phrases in the source text collection; (3) a set of\ncontext-independent token embeddings similar to the one used in standard neural language models.\nPrefix Encoder\nThe prefix encoder is responsible for encoding the prefix x<i into a vector represen-\ntation for the next-phrase prediction. We treat the prefix as a sequence of tokens (previously predicted\nphrases are split into tokens as well) and encode them using the standard Transformer architecture\nwith causal attention (Vaswani et al., 2017; Radford et al., 2019). Causal attention only allows each\nposition in the input sequence to attend to its preceding positions. Therefore, the prefix representation\ncan be computed incrementally as the generation progresses, leading to faster inference. Concretely,\nthe prefix encoder transforms a prefix x<i of length i into a matrix Hi \u2208 Ri\u00d7dL, where d is the\nhidden dimension and L is the number of Transformer layers. The computation can be written as:\nHi+1 = PrefixEncoder(xi, Hi).\nWe use the hidden state of the last token as the prefix representation qi.\n3\nPublished as a conference paper at ICLR 2023\nPhrase Encoder\nGiven a set of source documents {D1, ..., Dn}, the phrase encoder computes the\nvector representations of all the phrases in the documents. Inspired by previous work (Lee et al., 2016;\nSeo et al., 2018; Lee et al., 2021), we construct context-dependent phrase representations as follows.\nFor a document D = D1, . . . , Dm of length m, we first apply a deep bidirectional Transformer\n(Devlin et al., 2019) to obtain contextualized token representations D \u2208 Rm\u00d7dt, where dt is the\ndimension of token representations. Then, we apply two MLPs models, MLPstart and MLPend, to\nconvert D into start and end token representations Dstart, Dend \u2208 Rm\u00d7 d\n2 , respectively:\nDstart = MLPstart(D), Dend = MLPend(D).\nFor each phrase Ds:e that starts at s and ends at e in the document, we use the concatenation of the\ncorresponding start and end vectors as the phrase representation.\nPhraseEncoder(s, e, D) = [Dstart[s]; Dend[e]] \u2208 Rd\n(2)\nThe advantages of the above representation method are that (1) we only need to encode the document\nonce to obtain all phrase representations; and (2) we only need to store all the token representations\ninstead of all phrase representations.\nContext-Independent Token Embeddings\nAlthough COG can copy phrases from other documents,\nwe would like to retain the generalization capability to compose output with standalone tokens. This\ncan be especially useful when there is no suitable phrase in the source text collection. Therefore,\nwe also add the traditional context-independent token embeddings V \u2208 R|V |\u00d7d to our phrase table.\nThese tokens can be seen as phrases of length 1 without any context information.\n3.2\nMODEL TRAINING\nCOG decomposes the task of text generation into a series of copy-and-paste operations: at each time\nstep, it selects the next phrase either from the source text collection or the fixed token vocabulary. In\nother words, phrases are used as the basic building blocks for text generation. To train COG, each\ndocument in the training set is chunked into a sequence of phrases in a similar spirit. Specifically, we\npropose a greedy segmentation algorithm based on forward maximum matching. Taking a document\nD = D1, . . . , Dm of m tokens as an example, our algorithm segments the document from left to\nright. The first i tokens will be cut out as a phrase if it can be found as a sub-sequence in other\ndocuments and i is the maximum valid value. The above process is repeated until all tokens are cut\nout. Note that some resultant phrases can be single tokens in the fixed token vocabulary when no\nproper matching can be found. Detailed explanations of the phrase segmentation algorithm can be\nfound in Appendix D.\nSuppose that a document D has been split into n phrases D = p1, . . . , pn. If the k-th phrase\npk is copied from another document, let Dk be the source document and let sk, ek be the start\nand end positions of pk in Dk, the phrase encoder is used to extract its context-dependent phrase\nrepresentations PhraseEncoder(sk, ek, Dk) (Eq. 2). On the other hand, we directly retrieve the\ncontext-independent token embedding of pk if it is copied from the fixed token vocabulary. As\nillustrated by Eq. 1, COG relies on a shared vector space of prefix and phrase representations, where\nthe representations of semantically coherent prefixes and phrases should be closer to each other while\nothers should be pushed apart. We define the training loss for next-phrase predictions by using the\nInfoNCE loss with in-batch negatives (Karpukhin et al., 2020):\nLp = \u2212 1\nn\nn\nX\nk=1\nlog\nexp(qk \u00b7 pk)\nP\np\u2208Pk exp(qk \u00b7 pp) + P\nw\u2208V exp(qk \u00b7 vw)\nwhere Pk consists of all the phrases in the source document Dk, V is the set of all tokens in the token\nvocabulary, and qk denotes the representation of the prefix preceding the phrase pk in D.\nAdditionally, to retain the capability of token-level generation, we also train COG with the standard\ntoken-level autoregressive loss.\nLt = \u2212 1\nm\nm\nX\ni=1\nlog\nexp(qi, vDi)\nP\nw\u2208V exp(qi, vw)\n4\nPublished as a conference paper at ICLR 2023\nwhere qi denotes the prefix representation preceding the token Di in D. Finally, the training loss is\nthe sum of these two losses:\nL = Lp + Lt\n4\nEXPERIMENTAL SETUP\n4.1\nBASELINES\nWe compare COG with the following three baselines:\n\u2022 Transformer (Vaswani et al., 2017) has been the de facto model for neural language\nmodels. Concretely, we fine-tune the pre-trained GPT2 model (Radford et al., 2019) in our\nexperiments.\n\u2022 kNN-LM (Khandelwal et al., 2020) is a retrieval-augmented generation model, which ex-\ntends a pre-trained neural language model by linearly interpolating its next token distribution\nwith a k-nearest neighbors (kNN) model.\n\u2022 RETRO (Borgeaud et al., 2022)2 is another retrieval-augmented generation model which\ncombines a frozen BERT retriever, a differentiable encoder and a chunked cross-attention\nmechanism to predict next tokens. Since there is no pre-trained RETRO model that could be\naccessed, we train it from scratch on the WikiText-103 dataset.\n4.2\nIMPLEMENTATION DETAILS\nAll the baselines and our source codes are based on the popular Huggingface transformers package\n(Wolf et al., 2020). For a fair comparison, the prefix encoders in Transformer, kNN-LM, and COG\nuse the same model architecture as the pre-trained GPT2 model (12 layers, 12 heads, and 768 hidden\ndimensions) (Radford et al., 2019). For the phrase encoder in COG, we fine-tune the pre-trained\nBERT-base-cased model (Devlin et al., 2019) (12 layers, 12 heads, and 768 hidden dimensions). We\ntrain baselines and COG for 400,000 steps on 8 Tesla-V100 GPUs. For all the baselines, the learning\nrate, dropout rate, and gradient clipping are set as 5e-5, 0.1, and 1.0, respectively. Due to memory\nlimitation, the batch size is set to contain 256 phrases. For the BERT model in the phrase encoder,\nthe maximum sequence length is set as 256. For the GPT2 model in the prefix encoder, the maximum\nsequence length is set as 512. Our proposed COG contains overall 248M parameters from BERT and\nGPT2 models, and other baselines contain over 124M parameters. As suggested by Borgeaud et al.\n(2022), the hyper-parameters \u03bb and \u03b1 of kNN-LM are set as 0.118 and 0.00785, respectively.\nTo improve the inference efficiency of COG, we encode all the documents in the source text collections\noffline. Note that retrieving from such a super large phrase collection faces severe challenges on the\nengineering side. This paper uses a coarse-to-fine pipeline to address this challenge. Specifically, we\nfirst use a document retriever to retrieve top-k related documents for each given prefix. Then, their\ncorresponding phrase representations are collected for selection. In this paper, a popular semantic\nmatching model, DPR (Karpukhin et al., 2020) and a vector search toolkit, FAISS (Johnson et al.,\n2019) are used as the document retriever, which can recall documents that have similar topics with\nthe prefix. The value k is empirically set to 1024.\nCOG can be used with both greedy search and nucleus sampling. For greedy search, COG selects the\nphrase that has the highest fitness score at each time step. As for nucleus sampling, we first obtain\nthe next-phrase distribution by using the softmax function over the fitness scores of all candidate\nphrases. Then, the next phrase is sampled over this distribution.\nMore details of the implementation can be found in Appendix A and B.\n4.3\nAUTOMATIC EVALUATION METRICS\nFor each document in the test set, we use the first 32 tokens as the prefix. The baselines and our\nproposed COG generate text continuations of length 128 based on the same prefix. Following\n2https://github.com/lucidrains/RETRO-pytorch.\n5\nPublished as a conference paper at ICLR 2023\nconventions (Welleck et al., 2020; Su et al., 2022), we use greedy search and nucleus sampling\n(Holtzman et al., 2020) (p = 0.95) throughout our experiments. Following previous work (Welleck\net al., 2020; Su et al., 2022) and report the results on the following evaluation metrics:\n\u2022 MAUVE (Pillutla et al., 2021), an efficient, interpretable, practical automatic evaluation, is\nhighly coherent with human judgments and widely used to evaluate modern text generation\nmodels (Su et al., 2022; Krishna et al., 2022). In this paper, MAUVE leverages the GPT2-\nlarge model to generate the scores, and the scaling factor is set as 2.0.\n\u2022 Rep-n (Welleck et al., 2020) measures the sequence-level repetition as the portion of\nduplicate n-grams in the generated text (Welleck et al., 2020). For a generation text x, Rep-n\ncan be formulated as: 100 \u00d7 (1.0 \u2212 |unique n\u2212gram(x)|\n|total n\u2212gram(x)| ). Higher Rep-n denotes the severe\ndegeneration problem in generations.\n\u2022 Diversity (Welleck et al., 2020) measures the diversity of the generations, which is formu-\nlated as \u03a04\nn=2(1 \u2212 Rep\u2212n\n100 )). Generations that have higher Diversity scores usually are more\ninformative.\nNote that previous work (Khandelwal et al., 2020; Dai et al., 2019) often uses perplexity as the\nprimary evaluation metric to measure the performance of language modeling. However, since our\nproposed COG does not calculate next-token distributions over a fixed vocabulary, the comparison of\nperplexities is not reliable and thus omitted. However, we can test the perplexity of generated text\nusing an external language model, and the results are shown in Appendix C.\n5\nEXPERIMENTAL RESULTS\nIn this paper, we evaluate baselines and our proposed COG in three different settings: (1) standard\nlanguage modeling; (2) domain adaption; (3) enlarged phrase index.\n5.1\nLANGUAGE MODELLING ON WIKITEXT-103\nIn this setting, models are trained on the training set of the WikiText-103 dataset and evaluated on its\ntest set. The WikiText-103 dataset (Merity et al., 2017) contains an extensive collection of Wikipedia\narticles with over 100 million words, which is widely used to evaluate the performance of universal\nlanguage modeling (Khandelwal et al., 2020; Dai et al., 2019; Su et al., 2022).\nModel\nDecoding MAUVE\u2191 Rep-2\u2193 Rep-3\u2193 Rep-4 \u2193 Diversity\u2191 Latency (s)\u2193\nTransformer\ngreedy\n19.87\n43.56\n38.55\n35.5\n22.37\n1.32\nnucleus\n23.43\n5.10\n1.33\n0.50\n93.22\n1.48\nkNN-LM\ngreedy\n19.92\n43.79\n38.76\n35.69\n22.13\n10.36\nnucleus\n22.50\n3.33\n0.69\n0.21\n95.8\n10.42\nRETRO\ngreedy\n21.19\n44.65\n39.63\n36.6\n21.19\n4.39\nnucleus\n22.86\n6.21\n1.93\n0.86\n91.19\n4.51\nCOG\ngreedy\n26.01\n28.14\n23.80\n21.40\n43.03\n1.29\nnucleus\n26.14\n7.31\n2.66\n1.28\n89.07\n1.54\nTable 1: The automatic evaluation on the test set of WikiText-103. As for each model with nucleus\nsampling, we run 10 times and recorded the average MAUVE and Diversity scores.\nResults\nTable 1 shows the performance comparison between the baselines and our proposed COG\non the test set of the WikiText-103 corpus. It can be found that our proposed COG substantially\noutperforms the Transformer and kNN-LM baselines on most metrics. Specifically, COG improves\nMAUVE score over the best baseline (Transformer with nucleus sampling) from 23.43 to 26.14 \u2013\nan improvement of 2.71%. Interestingly, although it is well known that greedy search could raise\nsevere degeneration problems (Welleck et al., 2020), COG with greedy search still outperforms the\nstandard Transformer baseline with nucleus sampling, with 2.58% improvements on MAUVE. This\n6\nPublished as a conference paper at ICLR 2023\nThe Man Trap\n\u201d.\nHe is also\ninvolved in\nscience fiction\ndrama series\nMagic Roundabout television series\nThe first regular episode ( \" The Man \nTrap \" ) of Star Trek: The Original \u2026\n\u2026 like Metallica . He is also \ninfluenced by orchestral and \nclassical composer \u2026\n\u2026 However , Jerry Hardin , who \nwas involved in another morph to \nhis character Deep Throat, \u2026\n\u2026 of Thor with the high-tech \nscience fiction in Iron Man \u2026\n\u2026 she became the youngest \nactress in a drama series \nwhen she starred \u2026\nEric Thompson who narrated The Magic \nRoundabout television series , was born \nin a house on Jermyn Street\nThe Man Trap \u201d. He is also involved in \nan ongoing science fiction drama series \ncalled the Magic Roundabout television \nseries \u2026\nIn 2000 Boulter had a guest-\nstarring role on the television \nseries The Bill ; he portrayed \" \nScott Parry \" in the episode, \"\nInput Prefix\nGenerated Continuation\nDocument\nDocument\nDocument\nDocument\nDocument\nDocument\ncalled the\nan ongoing\nFigure 2: An example generated by COG on the test set of WikiText-103. The dotted squares denote\nthat the content (highlighted in red )is copied from the token vocabulary, and the solid squares denote\nthat the content (highlighted in blue ) is copied from other documents.\nobservation demonstrates that COG is more robust and less prone to the degeneration problem, which\ncan be considered as an additional bonus.\nMethod Uni-gram 2-gram 3-gram 4-gram 5-gram 6-gram\nGreedy\n0.583\n0.195\n0.121\n0.056\n0.029\n0.017\nNucleus\n0.434\n0.219\n0.181\n0.09\n0.048\n0.028\nTable 2: The statistics on the length of the copied phrases\n(on the test set of WikiText-103).\nInference Speed\nFurthermore, we\nalso compare the average time cost of\ndifferent methods for completing the\ngeneration on the test set. Since the\nphrase representations in COG are pre-\ncomputed offline, its encoding time\ncost is not included. The results are\nreported in Table 1. As seen, COG still achieves comparable inference efficiency with the standard\nTransformer baseline. The reason is that the copied phrases usually contain multiple tokens (the\nstatistics of phrase length are shown in Table 2). As a result, COG uses fewer decoding steps\nwhen generating the text of the same length. Unlike COG that uses a coarse-to-fine search pipeline,\nkNN-LM conducts large-scale vector search at every decoding step. Its inference latency is much\nhigher than Transformer, and COG, which is aligned with previous work(Alon et al., 2022).\nComparison\nBetter No Prefer. Worse\nCOG vs.\nTransformer\n48%\n24%\n28%\nTable 3: Human evaluation on the WikiText-\n103 corpus.\nHuman Evaluation\nTo ensure the reliability of\nour evaluations, we also run human evaluation with\nthree native-speaker graders from a third-party grad-\ning platform. Specifically, we randomly select 100\ntest prompts. For each test prompt, the annotators\nare given two continuations, in random order, which\nare generated by COG and Transformer respectively.\nThe annotators are asked to decide which one is better by considering the following aspects:\n\u2022 Fluency: Whether the generated text is fluent and easy to understand.\n\u2022 Informativeness: Whether the generated text is diverse and contains interesting content.\nWhen annotators make different decisions on the same sample, we ask them to have a discussion and\nmake the final decision. As shown in Table 3, our proposed COG model significantly outperforms\nstrong Transformer baseline, indicating its better generation quality.\nCase Study\nFor a better understanding of the performance of COG, we present an example of the\ntext continuations generated by our proposed COG in Figure 2. It can be found that COG can retrieve\nphrases that are semantically coherent and fluent for given prefixes. For example, at the second\ndecoding step, COG generate the punctuations [\u201d, .] from the pre-defined vocabulary to close the film\nname \u201cThe Man Trap\u201d and the sentence. Besides, at the ninth decoding step, COG directly copied\nthe named entity Magic Roundabout television series from the related document. More examples can\nbe found in Appendix E.\n7\nPublished as a conference paper at ICLR 2023\n5.2\nDOMAIN ADAPTION ON LAW-MT\nIn the domain adaption setting, the models trained on the WikiText-103 dataset are tested on a\nspecific domain. Following previous work (He et al., 2021; Alon et al., 2022), we use the English\npart of Law-MT (Koehn & Knowles, 2017), which is an English-German translation dataset for\nlaw documents. The memory of kNN-LM, RETRO and COG are constructed from the training\nset of Law-MT. We also present the performance of Transformer baselines with or without further\nfine-tuning on the training set of Law-MT.\nModel\nDecoding MAUVE \u2191 Diversity \u2191\nTransformer w/o FT\ngreedy\n20.32\n70.66\nnucleus\n25.21\n93.88\nTransformer w/ FT\ngreedy\n23.00\n80.52\nnucleus\n26.85\n90.14\nkNN-LM\ngreedy\n23.31\n19.85\nnucleus\n24.75\n94.60\nRETRO\ngreedy\n18.70\n71.14\nnucleus\n20.35\n94.81\nCOG\ngreedy\n21.31\n84.32\nnucleus\n28.14\n92.56\nTable 4: The automatic evaluation on Law-MT.\nResults\nAs shown in Table 4, it can be ob-\nserved that COG even outperforms the Trans-\nformer model further fine-tuned on the Law-\nMT corpus (Transformer w/ FT). Specifically,\nCOG outperforms Transformer w/ FT by 2.93%\nMAUVE score. The results indicate that COG\nallows a single model to be specialized in dif-\nferent domains, by simply switching the source\ntext collection. Although kNN-LM brings in\nhigher Diversity scores, COG surpasses it by\n3.39% MAUVE score, which shows COG has\nhigher generation quality in general.\nComparison\nBetter No Prefer. Worse\nCOG vs.\nTransformer w/ FT\n52%\n12%\n36%\nTable 5: Human evaluation on Law-MT.\nHuman Evaluation\nWe also conduct the hu-\nman evaluation on the Law-MT corpus, which\nhas a similar setup to that in (\u00a75.1). Table 5\nshows that most of COG\u2019s generations are bet-\nter than a strong Transformer baseline. This\nobservation demonstrates that COG can even\noutperform the fine-tuned Transformer baseline without any domain-specific training.\n5.3\nENLARGED PHRASE INDEX WITH EN-WIKI\nIn the enlarged phrase index setting, we make use of a large text collection, the En-Wiki corpus, and\ntest baselines on the test set of WikiText-103. The En-Wiki corpus contains a large-scale collection\nof Wikipedia articles with over 3 billion words, whose size is much larger than the WikiText-103\ndataset. The memory of kNN-LM, RETRO, and COG are built from the training set of En-Wiki3.\nSimilar to the domain adaption setting, we also present the results of Transformer baselines with or\nwithout further fine-tuning on the En-Wiki corpus.\nResults\nThe experimental results are shown in Table 6. COG with En-Wiki memory surpasses\nother strong baselines and COG with WikiText-103 memory. This is especially remarkable because\nCOG does not require any additional training, suggesting we can train COG with a smaller corpus\nbut leverage additional information in a larger corpus in a plug-and-play fashion. Similar to the\ndomain adaption setting, we also notice that, although kNN-LM baseline improves Diversity scores,\nit obtains a much lower MAUVE score than COG (23.39 vs. 26.97). Note that the Transformer w/ FT\nis slightly worse than that without fine-tuning on the En-Wiki dataset. This phenomenon is mainly\nbecause there are deviations between En-Wiki and WikiText-103 datasets.\nEffects of Index Size\nTo further investigate how the size of the phrase index affects the generation\nquality, we randomly sample several subsets of the En-Wiki dataset with proportions from 0.1% to\n100%. As shown in Figure 3, when the proportion is less than 1%, COG exhibits a similar quality,\nwhich is unsurprising since few enlarged documents are added to the phrase index. In contrast, once\nthe proportion is larger than 1%, the larger the phrase index becomes, the better generation quality\nthe model achieves.\n3Due to the hardware limitation, RETRO uses the subset of the En-Wiki corpus (over 6 million chunks).\n8\nPublished as a conference paper at ICLR 2023\nModel\nDecoding MAUVE \u2191 Diversity \u2191\nTransformer w/o FT\ngreedy\n19.87\n22.37\nnucleus\n23.43\n93.22\nTransformer w/ FT\ngreedy\n20.21\n19.62\nnucleus\n21.31\n92.92\nkNN-LM\ngreedy\n23.21\n20.33\nnucleus\n23.39\n96.37\nRETRO\ngreedy\n19.75\n21.15\nnucleus\n22.87\n91.09\nCOG\ngreedy\n24.68\n40.45\nnucleus\n26.97\n90.00\nTable 6: The automatic evaluation on the test set\nof WikiText-103, the memory is built on the train\nset of En-Wiki. Transformer w/ FT and Trans-\nformer w/o FT denote the Transformer baseline\nwith and without further fine-tuning on the train\nset of En-Wiki, respectively.\n0.0\n0.001 0.003 0.01\n0.03\n0.1\n0.3\n1.0\nThe proportion of enlarged En-Wiki Documents\n23.5\n24.0\n24.5\n25.0\n25.5\n26.0\n26.5\n27.0\nMAUVE score\nTransformer w/o FT\nCoG\nFigure 3: Generation quality of COG with differ-\nent sizes of the phrase index. For each proportion\n(point in the X-axis), we sample 10 times and\nrecord the averaged MAUVE score. A propor-\ntion of 0.0 indicates that only documents from\nWikiText-103 are used.\n6\nRELATED WORK\nDense Retrieval\nThe dense retrieval technique (Karpukhin et al., 2020) has been widely used in\nmany downstream NLP tasks, such as open-domain question answering (Karpukhin et al., 2020;\nLee et al., 2021), open-domain dialogue systems (Lan et al., 2021) and machine translation (Cai\net al., 2021). Different from the traditional sparse retrieval system, such as BM25 and TF-IDF\n(Robertson & Zaragoza, 2009), dense retrieval learns a shared vector space for queries and documents,\nwhere relevant pairs of query and document have smaller distances (i.e., higher similarities) than the\nirrelevant pairs.\nThe most closely related work to our study is DensePhrase (Lee et al., 2021). DensePhrase reformu-\nlates the question-answering task as a phrase retrieval problem, where phrases are directly retrieved\nand returned as answers to factual questions. Differently, our work aims to generate coherent text\ncontinuations through multiple rounds of phrase retrieval. Since the connection between two adjacent\nphrases should be coherent and fluent in the text generation task, it is much more difficult.\nRetrieval-Augmented Text Generation (RAG)\nRetrieval-augmented text generation has gained\nincreasing interest recently. Most prior work improves the generation quality (e.g., informativeness) of\nlanguage models by grounding the generation on a set of retrieved materials (e.g., relevant documents)\n(Li et al., 2022; Guu et al., 2020; Hashimoto et al., 2018; Weston et al., 2018; Cai et al., 2019a;b;\nKhandelwal et al., 2020; Wu et al., 2019; Guu et al., 2020; Lewis et al., 2020; Borgeaud et al., 2022;\nYang et al., 2023). Our work is on this line of research but takes a radical step forward. Unlike prior\nwork that builds the combinations of retrieval and generation, retrieval is generation in COG.\nOne contemporary work to our work is Min et al. (2022), which shares the idea of replacing the\nfixed vocabulary with a nonparametric phrase table. However, Min et al. (2022) focuses on masked\nlanguage modeling while our focus is on causal language modeling and text generation.\n7\nCONCLUSION\nIn this paper, we reformulated text generation as progressively copying phrases from the massive\ntext collection. Following this formalization, we proposed a novel neural text generation model,\nnamed COG, which generates text by retrieving semantically coherent and fluent phrases from other\ndocuments. Experimental results proved the advantages of COG over the strong baselines on three\nexperimental settings: standard language modeling (WikiText-103), domain adaptation (Law-MT),\nand enlarged phrase index (En-Wiki).\n9\nPublished as a conference paper at ICLR 2023\nJUSTIFICATION OF CHANGES\nNote that the experimental results in the current version have some changes from the previous version\nthat has been reviewed. We made a number of revisions to the experiments according to the valuable\nsuggestions from the reviewers.\nACKNOWLEDGEMENT\nThe authors thank the anonymous reviewers for their valuable suggestions and comments on our\npaper, which significantly improves the quality of our paper.\nREFERENCES\nUri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. Neuro-symbolic\nlanguage modeling with automaton-augmented retrieval. In International Conference on Machine\nLearning. PMLR, 2022.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In 3rd International Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.\nImproving language models by retrieving from trillions of tokens. In International Conference on\nMachine Learning. PMLR, 2022.\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, and Shuming Shi. Skeleton-to-\nresponse: Dialogue generation guided by retrieval memory. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), 2019a.\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, and Shuming Shi. Retrieval-guided\ndialogue response generation via a matching-to-generation framework. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pp. 1866\u20131875, Hong Kong,\nChina, November 2019b. Association for Computational Linguistics. doi: 10.18653/v1/D19-1195.\nURL https://aclanthology.org/D19-1195.\nDeng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with\nmonolingual translation memory. In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), 2021.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), 2019.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-\naugmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020.\nTatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework\nfor predicting structured outputs. In Advances in Neural Information Processing Systems, 2018.\nJunxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. Efficient nearest neighbor language\nmodels. arXiv preprint arXiv:2109.04212, 2021.\n10\nPublished as a conference paper at ICLR 2023\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020, 2020.\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3), 2019.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih.\nDense passage retrieval for open-domain question answering.\nIn\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), 2020.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization\nthrough memorization: Nearest neighbor language models. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.\nPhilipp Koehn and Rebecca Knowles. Six challenges for neural machine translation. In Proceedings\nof the First Workshop on Neural Machine Translation, 2017.\nKalpesh Krishna, Yapei Chang, John Wieting, and Mohit Iyyer. Rankgen: Improving text generation\nwith large ranking models. arXiv preprint arXiv:2205.09726, 2022.\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing: System Demonstrations, 2018.\nTian Lan, Deng Cai, Yan Wang, Yixuan Su, Xian-Ling Mao, and Heyan Huang. Exploring dense\nretrieval for dialogue response selection. arXiv preprint arXiv:2110.06612, 2021.\nJinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. Learning dense representations of\nphrases at scale. In Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Processing (Volume\n1: Long Papers), 2021.\nKenton Lee, Shimi Salant, Tom Kwiatkowski, Ankur Parikh, Dipanjan Das, and Jonathan Be-\nrant. Learning recurrent span representations for extractive question answering. arXiv preprint\narXiv:1611.01436, 2016.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe\nKiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural\nInformation Processing Systems 33: Annual Conference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\nHuayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. A survey on retrieval-augmented text\ngeneration. ArXiv, abs/2202.01110, 2022.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings, 2017.\nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke\nZettlemoyer. Nonparametric masked language modeling. arXiv preprint arXiv:2212.01349, 2022.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi,\nand Zaid Harchaoui. Mauve: Measuring the gap between neural text and human text using\ndivergence frontiers. Advances in Neural Information Processing Systems, 34, 2021.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8), 2019.\nS. Robertson and H. Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found.\nTrends Inf. Retr., 3, 2009.\n11\nPublished as a conference paper at ICLR 2023\nAbigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with\npointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), 2017.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 2016.\nMinjoon Seo, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. Phrase-\nindexed question answering: A new challenge for scalable document comprehension. In Proceed-\nings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018.\nShuming Shi, Enbo Zhao, Duyu Tang, Yan Wang, Piji Li, Wei Bi, Haiyun Jiang, Guoping Huang,\nLeyang Cui, Xinting Huang, Cong Zhou, Yong Dai, and Dongyang Ma. Effidit: Your ai writing\nassistant. ArXiv, abs/2208.01815, 2022.\nYixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. A contrastive\nframework for neural text generation. In Advances in Neural Information Processing Systems,\n2022.\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In\nAdvances in Neural Information Processing Systems 27: Annual Conference on Neural Information\nProcessing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, 2014.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\nProcessing Systems 30: Annual Conference on Neural Information Processing Systems 2017,\nDecember 4-9, 2017, Long Beach, CA, USA, 2017.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.\nNeural text generation with unlikelihood training. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.\nJason Weston, Emily Dinan, and Alexander H. Miller. Retrieve and refine: Improved sequence\ngeneration models for dialogue. In SCAI@EMNLP, 2018.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, 2020.\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhoujun Li, and Ming Zhou. Response generation by\ncontext-aware prototype editing. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 33, 2019.\nNan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and\nFuru Wei. Inference with reference: Lossless acceleration of large language models, 2023.\nA\nDATASET STATISTICS\nThe experiments in this paper include three benchmarks: (1) WikiText-103; (2) English part of\nLaw-MT; (3) En-Wiki. The statistics of these benchmarks are shown in Table 7. En-Wiki corpus\nis used for the enlarged phrase index settings in this paper, containing over 4,848,348 long English\nWikipedia documents.\n12\nPublished as a conference paper at ICLR 2023\nBenchmarks\nTrain\nDev\nTest\nWikiText-103\n1,801,350\n3,760\n4,358\nLaw-MT\n389,292\n2,000\n2,000\nTable 7: The number of sentences in the WikiText-103 and Law-MT datasets.\nB\nMORE IMPLEMENTATION DETAILS\nDuring training, the dynamic vocabulary of COG contains two parts: (1) word-level vocabulary size\n(50257 in GPT2 vocabulary); (2) the phrases in a batch of training documents. During inference,\nthe dynamic vocabulary consists of the word-level vocabulary and the phrases extracted from the\nTop-k retrieved documents (k=1024 in this paper). The size of the pre-defined word-level vocabulary\ncontains 50257 subwords. Since there are only a few documents encoded to extract the phrase\nrepresentations, the average number of the phrase representations is 950,942.4 in the WikiText-103\ntest set when K = 1024.\nC\nPERPLEXITY OF GENERATED TEXT\nModels\nPerplexity\ngreedy\nnucleus\nTransformer\n3.26\n37.11\nkNN-LM\n3.48\n78.01\nRETRO\n3.27\n36.40\nCOG\n10.41\n27.24\nGround-Truth\n18.64\nTable 8: The perplexity on the test set of\nWikiText-103.\nWe calculate the perplexity of the generated texts un-\nder a large pre-trained language model (GPT2-Large).\nAs shown in Table 8, it can be found texts gener-\nated by greedy search can achieve very low perplex-\nity scores (even much lower than the ground-truth)4.\nThis is expected as greedy search targets at likelihood\nmaximization. Sampling-based decoding methods\ngive much higher perplexity scores. Moreover, it is\nworth noting that COG achieves the closest perplexity\nscore to ground-truth.\nD\nTHE PHRASE SEGMENTATION ALGORITHM\nCOG takes phrases as the minimum units that can be put together to form a coherent document. To\ntrain COG, we design a phrase segmentation algorithm to split each document in the training set\ninto a sequence of phrases. This algorithm makes use of a forward maximum matching strategy to\nidentify phrases. Maximum matching is one of the most popular structural segmentation algorithms.\nThis method favors long phrases and is a greedy algorithm by design. Specifically, we treat each\ndocument as a sequence of tokens and scan each document from left to right. At each step, we search\nfor the longest prefix of the unsegmented part that is also a sub-sequence of other documents other\nthan the current document. If the length of that prefix is bigger than 2, we take that prefix as the next\nphrase. Otherwise, we take the first token as the next phrase and it is labeled as coming from the fixed\ntoken vocabulary. In both cases, we process the rest part of the current document recurrently. The\nalgorithm can be very time-consuming because exhaustive searches over millions of documents are\ncompute-intensive. Therefore, we propose an efficient approximation as follows. First, we retrieve\nthe top-k most similar documents for each document using the popular DPR model (Karpukhin et al.,\n2020)5, and vector search toolkits, FAISS (Johnson et al., 2019). Then, the phrase search only runs\non the corresponding top-k documents. The relevant documents usually have similar topics to the\ncurrent document. The value of k is set as 1024 in our experiments. The details of our proposed\nphrase segmentation algorithm can be found in Algorithm 1: SearchPhrase is a function that searches\nthe cached token sequence (i.e., the current candidate for the next phrase) among the most relevant\ndocuments. It returns a label that denotes whether the phrase can be found and its position in the\nrelevant documents.\n4Note that the original perplexity of GPT2-Large model on the test set of WikiText-103 is 22.05 (Radford\net al., 2019). The gap between it and our results is caused by the different number of samples. In this study, we\nonly use samples that have more than 32 tokens to generate text.\n5https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base.\n13\nPublished as a conference paper at ICLR 2023\nAlgorithm 1: Phrase Segmentation Algorithm\nData: Document set: D = {di, {dj}K\nj=1}N\ni=1, where di denotes the i-th document. K denotes\nthe number of retrieved documents. N denotes the number of documents in the training\nset. The pre-defined maximum and minimum phrase lengths are Lmax and Lmin.\nResult: Segmented document set by phrase granularity: D\u2032 = {{(pi,x, (dj, posj))}||di||p\nx=1 }N\ni=1,\nwhere pi,x denotes the x-th phrase in di that also appears in another document dj in\nposition j. ||di||p denotes the number of the collected phrases in di.\n1 Preprocess: split each document into token-level pieces by using the off-the-shelf tokenizer.\nThe preprocessed document set can be formulated as D = {{ti,x}||di||t\nx=1 , {dj}K\nj=1}N\ni=1, where\nti,x is the x-th token of di, which consists of ||di||t tokens. Prepare the empty list D\u2032 = {},\nempty phrase cache cachep={}, and cached search success label labellast.\n2 for i \u2190 1 to N do\n3\ncursor=0\n4\nPhraseCollection={}\n5\nwhile cursor\u2264 ||di||t do\n6\nif Lmin \u2264 len(cachep) \u2264 Lmax then\n7\nlabelnow, rest=SearchPhrase(cachep)\n8\nelse\n9\nif len(cachep) > Lmax then\n10\ncachep={}\n11\nend\n12\nend\n13\nif labellast is True and labelnow is False then\n14\ncursor -= 1\n15\nPhraseCollection.append(cachep, rest)\n16\ncachep={}\n17\nelse\n18\nif labellast is False and labelnow is False then\n19\nPhraseCollection.append( cachep, None)\n20\ncachep={}\n21\nend\n22\nend\n23\ncursor += 1\n24\nlabelnow=labellast\n25\nend\n26\nD\u2032.append(PhraseCollection)\n27 end\n14\nPublished as a conference paper at ICLR 2023\nE\nMORE CASES\nIn this section, we present some generated examples of COG given some specific prefixes. As shown\nin Figure 4, 5 and 6, it can be observed that the generated continuations are fluent and coherent with\nthe given prefix. However, we also notice some flaws. For example, as shown in Figure 5, COG\ncopied the phrase 75 mph from the document ... sustained winds of at least 120 km / h (75 mph),\nwhich is incoherent with the previous copied phrase 106 km / h. Moreover, as shown in Figure 6,\nCOG copied the phrase Rhine and Main from the document (Bt the terms of the Peace of Basel\n(22 July 1795), the Prussian army was to leave the Rhine and Main river valleys ...). However, the\ncomplete phrase should be Rhine and Main river valleys, and COG only copy a part of it, leading to\ninaccurate generation results (rivers).\ncomer.\nIn 2008\nNolan was announced as the\nstar of\nSeven Psychopaths\nIn 2008 , McCormack co-starred in \nthe A & E television miniseries \nThe Andromeda Strain \u2026\nOn 21 January 2016, Nolan was \nannounced as the player-manager \nof Leyton Orient, \u2026\nWhile Bart had been the star \nof the show during \u2026\nAfter premiering at the Toronto Film \nFestival, Seven Psychopaths ( Farrell\u2019s \nsecond movie \u2026\nComer. In 2008, Nolan was announced \nas the star of Seven Psychopaths. He \ncast Matthew Morrison as\nNewman was nominated for a \nGolden Globe Award for Best \nActor. Gleason and Scott were \neach nominated for Best \nSupporting Actor and Scott was \nalso nominated as Best New\nInput Prefix\nGenerated Continuation\nDocument\nDocument\nDocument\nDocument\n,\n.\nHe cast\nHe cast her in the role . \nShe portrayed \u2026\nDocument\nMatthew Morrison as\n\u2026 Of the principal cast , \nRyan said : \" Casting \nMatthew Morrison as \u2026\nDocument\nFigure 4: An example generated by COG. The dotted squares denote that the content (highlighted in\nred )is generated from the token vocabulary, and the solid squares denote that the content (highlighted\nin blue ) is copied from other documents.\nwith a peak wind gust of\n106 km/h\n75 mph\nbeing measured\nCyclone Calasanjy caused heavy \ndamage in western Madagascar , \nwith a peak wind gust of 195 \nkm/h \u2026 \n\u2026 at the Wilmington International \nAirport , while gusts reached 66 \nmph ( 106 km / h ) at \u2026\n\u2026 sustained winds of at least \n120 km / h ( 75 mph ) \u2026\nTexas coastal plain led to extremely high \nrainfall totals being measured in parts of \nJefferson \u2026\nwith a peak wind gust of 106 km / h \n( 75 mph ) being measured on Pagan \nIsland.\nIn the Philippines, officials \nevacuated over 14 @,@ 000 \npeople. Imbudo was the \nstrongest typhoon to strike since \nTyphoon Zeb five years prior,\nInput Prefix\nGenerated Continuation\nDocument\nDocument\nDocument\nDocument\non Pagan Island\n\u2026 with gusts to 183 km / h ( 114 \nmph ) on Pagan Island . The \ntyphoon caused heavy crop \u2026\nDocument\n.\n(\n)\nFigure 5: An example generated by COG. The dotted squares denote that the content (highlighted in\nred )is generated from the token vocabulary, and the solid squares denote that the content (highlighted\nin blue ) is copied from other documents.\n15\nPublished as a conference paper at ICLR 2023\nUnternehmen Aster\nwas a battle fought in the\nArdennes forest\nnear the junction of the\n\u2026 in a retreat codenamed \nOperation Aster ( German : \nUnternehmen Aster ) \u2026\n\u2026 fought on September 7 , 1812 , \nwas a battle fought in the \nNapoleonic Wars during the \nFrench invasion of Russia.\nThe offensive against American \nforces in the Ardennes forest had \npreoccupied Hitler 's mind \u2026\n \u2026 Spafford Farm , was \nlocated near the junction of \nthe Spafford 's Branch \u2026\nUnternehmen Asterisk ), was a battle fought in \nthe Ardennes forest near the junction of the \nRhine and Main rivers on 8 October\nThe Battle of Defcrenstein ( also \nknown as the Battle of <unk>, \nBattle of <unk> and Battle of \n<unk> ; German :\nInput Prefix\nGenerated Continuation\nDocument\nDocument\nDocument\nDocument\nRhine and Main\nBy the terms of the Peace of Basel \n( 22 July 1795 ) , the Prussian army \nwas to leave the Rhine and Main \nriver valleys \u2026\nDocument\nrivers\nisk ),\non 8 October\n\u2026 independence in June 1991 ; \nhowever the declaration came into \neffect on 8 October 1991 \u2026\nDocument\nFigure 6: An example generated by COG. The dotted squares denote that the content (highlighted in\nred )is generated from the token vocabulary, and the solid squares denote that the content (highlighted\nin blue ) is copied from other documents.\n16\n"
  },
  {
    "title": "Mega-TTS 2: Zero-Shot Text-to-Speech with Arbitrary Length Speech Prompts",
    "link": "https://arxiv.org/pdf/2307.07218.pdf",
    "upvote": "25",
    "text": "Published as a conference paper at ICLR 2024\nMEGA-TTS 2: BOOSTING PROMPTING MECHANISMS\nFOR ZERO-SHOT SPEECH SYNTHESIS\nZiyue Jiang \u2217\u2020 \u2660\u2661\nJinglin Liu \u2217\u2661\nYi Ren \u2217\u2661\nJinzheng He \u2217\u2020 \u2660\u2661\nZhenhui Ye \u2217\u2020 \u2660\u2661\nShengpeng Ji \u2660\nQian Yang \u2660\nChen Zhang \u2661\nPengfei Wei \u2661\nChunfeng Wang \u2661\nXiang Yin \u2661\nZejun Ma \u2661\nZhou Zhao \u2660\u2021\n\u2660Zhejiang University & \u2661ByteDance\n{ziyuejiang,zhaozhou}@zju.edu.cn,\n{liu.jinglin,ren.yi,yinxiang.stephen}@bytedance.com\nABSTRACT\nZero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech\nprompts, which significantly reduces the data and computation requirements for\nvoice cloning by skipping the fine-tuning process. However, the prompting mecha-\nnisms of zero-shot TTS still face challenges in the following aspects: 1) previous\nworks of zero-shot TTS are typically trained with single-sentence prompts, which\nsignificantly restricts their performance when the data is relatively sufficient during\nthe inference stage. 2) The prosodic information in prompts is highly coupled with\ntimbre, making it untransferable to each other. This paper introduces Mega-TTS\n2, a generic prompting mechanism for zero-shot TTS, to tackle the aforemen-\ntioned challenges. Specifically, we design a powerful acoustic autoencoder that\nseparately encodes the prosody and timbre information into the compressed la-\ntent space while providing high-quality reconstructions. Then, we propose a\nmulti-reference timbre encoder and a prosody latent language model (P-LLM)\nto extract useful information from multi-sentence prompts. We further leverage\nthe probabilities derived from multiple P-LLM outputs to produce transferable\nand controllable prosody. Experimental results demonstrate that Mega-TTS 2\ncould not only synthesize identity-preserving speech with a short prompt of an\nunseen speaker from arbitrary sources but consistently outperform the fine-tuning\nmethod when the volume of data ranges from 10 seconds to 5 minutes. Fur-\nthermore, our method enables to transfer various speaking styles to the target\ntimbre in a fine-grained and controlled manner. Audio samples can be found\nin https://boostprompt.github.io/boostprompt/.\n1\nINTRODUCTION\nIn recent years, there has been remarkable progress in the development of text-to-speech (TTS)\ntechnology (Shen et al., 2018; Jia et al., 2018; Li et al., 2019; Kim et al., 2020; Ren et al., 2019; 2020;\nKim et al., 2021; 2022a). Among them, adaptive TTS systems (Chen et al., 2021; Min et al., 2021;\nKim et al., 2022b) are capable of cloning personalized voices given a few minutes of speech data.\nHowever, the performance of these systems relies heavily on the quality and quantity of the data\nutilized during the fine-tuning phases (Tan et al., 2021). Insufficient data during the fine-tuning stages\ncan lead to diminished audio naturalness or speech intelligibility (Kang et al., 2023). Moreover, the\ncomputational demands also constrain its application for cloning everyone\u2019s voice.\nTo reduce such a reliance, existing works leverage generative models to perform zero-shot\nTTS (Cooper et al., 2020a; Casanova et al., 2022; Huang et al., 2022a; Kang et al., 2023; Kharitonov\net al., 2023; Wang et al., 2023; Shen et al., 2023b; Matthew et al., 2023). These powerful models\ncan effectively synthesize speech given only a single speech prompt, eliminating the need for data\n\u2217Equal contribution.\n\u2020Interns at ByteDance.\n\u2021Corresponding author.\n1\narXiv:2307.07218v3  [eess.AS]  18 Mar 2024\nPublished as a conference paper at ICLR 2024\npreparation and the computational requirements for fine-tuning methods. However, the prompting\nmechanisms of current solutions still face two primary challenges:\n\u2022 Lack of multi-sentence prompting strategies. Previous works of zero-shot TTS typically\nemploy single-sentence speech prompts during training (Wang et al., 2023; Shen et al.,\n2023b; Matthew et al., 2023). In inference, the information in the single-sentence speech\nprompt is insufficient to guide the zero-shot TTS systems to imitate the voice variability of a\nnatural person perfectly.1 From another perspective, the performance of fine-tuning methods\ncan be further improved by increasing the amount of data, while zero-shot TTS systems lack\nan appropriate strategy to extract useful information from multi-sentence speech prompts.\n\u2022 Lack of specialized prompting mechanism for prosodic information. Current solutions\nfor zero-shot TTS primarily concentrate on improving the similarity of timbre and prosody\nbetween the generated speech and the prompts. However, they neglect to express various\nunseen prosodic styles in a controlled manner while also preserving the unique timbre of\nthe given one-sentence prompt. In order to control the prosodic styles, it is necessary to\ndisentangle the prosody information from speech prompts.\nWe address the above challenges by decomposing speech into content, timbre, and prosody. Intuitively,\nrepresenting speeches for numerous speakers requires a substantial number of codebook entries for\ntimbre modeling (D\u00b4efossez et al., 2022; Yang et al., 2023). Through the decoupling of prosody\ninformation, a highly compact codebook for prosody modeling can be obtained, which enables\nour model to effectively handle extremely long prompts and have flexible control over prosodic\nstyles. Therefore, this work proposes Mega-TTS 2, a generic framework that boosts the prompting\nmechanisms for zero-shot TTS systems. Specifically, we begin by designing an acoustic autoencoder\nthat can effectively decompose speech into prosody and timbre representations and represent them in a\ncompact latent space. Then, we design a multi-reference timbre encoder (MRTE) and a prosody latent\nlanguage model (P-LLM) to extract useful information from multi-sentence prompts. In addition to\nthe multi-sentence prompting mechanism, we propose a prosody interpolation technique to control\nthe generation process of prosody codes by utilizing prosody prompts from multiple speakers while\nmaintaining the target speaker\u2019s timbre. By utilizing the probabilities derived from both the prosodic\nprompts of the target speaker and the auxiliary speaker, the prosodic styles of speech can be generated\nin a controlled manner.\nExperiments on LibriSpeech test-clean (Panayotov et al., 2015) and ESD (Zhou et al., 2021) datasets\nshow that Mega-TTS 2 outperforms other state-of-the-art fine-tuning and zero-shot TTS models\nin terms of speaker similarity and speech naturalness. Notably, when the length of the prompt is\nfurther extended, our method surpasses the fine-tuning baseline model in the objective and subjective\nevaluations. The extensive studies on adaptive prosody transfer further highlight the superiority of our\nproposed prompting mechanisms. The main contributions of this work are summarized as follows:\n\u2022 We design an acoustic autoencoder that separately compresses the prosody and timbre\ninformation into the latent space, which allows our model to process prompts of up to 300\nseconds in length effectively.\n\u2022 We propose a multi-reference timbre encoder and an auto-regressive prosody language\nmodel to extract fine-grained information from multiple reference speeches, which bridges\nthe speaker similarity gap between zero-shot methods and fine-tuning methods.\n\u2022 Experimental results also reveal that the performance of Mega-TTS 2 surpasses the powerful\nfine-tuning baseline when we have 10 seconds to 5 minutes of data for each unseen speaker,\nindicating the superiority of our proposed prompting mechanisms.\n\u2022 The proposed prosody interpolation technique ensures the controllability of prosody and is\ncapable of transferring various speaking styles to the desired timbre. For instance, we can\ntransform a voice with a sad tone into a happier one with the auxiliary prosody prompt from\nanother speaker.\n1Although the performance of these systems can be further improved by concatenating multiple sentences\ninto a long prompt, the gap between training and inference still restricts their performance. (See Section 4.2)\n2\nPublished as a conference paper at ICLR 2024\n2\nBACKGROUND\nAdaptive TTS\nAdaptive TTS (Arik et al., 2018; Kons et al., 2019; Moss et al., 2020; Chien et al.,\n2021) focuses on synthesizing personalized voice for any user with few data. During the adaptation\nprocess, a TTS model pre-trained on a multi-speaker speech dataset is typically fine-tuned with\nfew adaptation data for the target voice (Tan et al., 2021). Chen et al. (2018) design independent\nlearned embeddings for each speaker, which requires few data at deployment time to adapt to new\nspeakers rapidly. AdaSpeech (Chen et al., 2021) proposes an acoustic-condition modeling method\nfor high-quality and efficient customization of new voices. There are also some works leveraging\nthe meta-learning approach (Chen et al., 2018; Min et al., 2021; Huang et al., 2022b) and data\naugmentation (Cooper et al., 2020b; Yang & He, 2020) for speaker adaptation. However, although\nsome works are data-efficient (Min et al., 2021; Huang et al., 2022b) and parameter-efficient (Chen\net al., 2021), these systems still suffer from audio quality issues when data size is small, as well as\ncomputational cost issues due to hundreds of fine-tuning steps.\nZero-shot TTS\nZero-shot adaptation (Jia et al., 2018; Arik et al., 2018; Cooper et al., 2020a;\nCasanova et al., 2021; Wu et al., 2022; Huang et al., 2022b;a; Casanova et al., 2022) aims to synthesize\nunseen voices with a speaker encoder that extracts speaker embeddings from the reference audio. This\nscenario is highly attractive because it does not require any adaptation data or parameters (Kang et al.,\n2022). The attention-based adaptation method (Choi et al., 2020; Zhou et al., 2022; Yin et al., 2022;\nLin et al., 2021) utilizes attention mechanisms to extract fine-grained speech features from reference\naudios. Among them, Attentron (Choi et al., 2020) proposes to extracts useful style information from\narbitrary number of reference audios. However, they do not separately model the timbre and prosody\ninformation, lacking controllability over timbre and prosody. Most recently, some works (Kharitonov\net al., 2023; Zhang et al., 2023) are proposed to use in-context learning methods (Dong et al., 2022) to\nefficiently extract speaker information from acoustic prompts and have achieved remarkable results in\nzero-shot TTS. VALL-E (Wang et al., 2023) proposes the neural codec language model that exhibits\nstrong in-context learning capability for zero-shot speech generation. NaturalSpeech 2 (Shen et al.,\n2023b) introduces in-context learning to latent diffusion model (Rombach et al., 2022), which is\nachieved by partitioning a speech clip into the prompt and target regions. VoiceBox (Matthew et al.,\n2023) solves a text-guided speech-infilling task with large-scale data to learn from context information.\nHowever, these methods are trained with single-sentence prompts, lacking an appropriate strategy to\nextract fine-grained information from multi-sentence speech prompts.\nProsody Transfer for Speech Synthesis\nProsody transfer (Lee & Kim, 2019; Klimkov et al., 2019;\nGururani et al., 2019; Pan & He, 2021; Karlapati et al., 2022) aims to transfer the prosody from a\nreference utterance to the synthesized target speech, which is essential for producing natural and\nexpressive speech in a controlled manner (Wagner & Watson, 2010). Skerry-Ryan et al. (2018) first\nintegrate a prosody reference encoder into a TTS system based on Tacotron (Wang et al., 2017),\nwhich is capable of performing similar-text prosody transfer. Recent works try to transfer prosody\nin different-text and different-speaker settings (Karlapati et al., 2020; Za\u0131di et al., 2021) with the\nbottleneck of the prosody encoder. Among them, Daft-Exprt (Za\u0131di et al., 2021) uses a gradient\nreversal layer to penalize the prosody encoder if its output contains information about the speaker\nidentity from the reference utterance, which enhances the target speaker fidelity for cross-speaker\nprosody transfer. However, as pointed out by Sigurgeirsson & King (2023), current solutions do\nnot learn a transferable representation of prosody, but rather an utterance-level representation that is\nrelatively dependent on both the reference speaker and reference text.\n3\nMETHOD\nThis section introduces Mega-TTS 2. To begin with, we provide an intuitive illustration of how\nMega-TTS 2 decomposes the timbre and prosody information from speech. Next, we provide detailed\nexplanations of our prompting mechanisms and the two-stage training process of the proposed model.\n3.1\nDECOMPOSITION FOR PROSODY AND TIMBRE\nProblem Formulation\nDenote H(X) as the Shannon entropy of X and Denote I(Y ; X) as the\nmutual information. We assume that the mel-spectrogram y can be reconstructed through the\n3\nPublished as a conference paper at ICLR 2024\nPhoneme\n\u2026\n\ud835\udc67!\n\ud835\udc67\"\nMulti-Reference\nTimbre Encoder\nC\nContent Encoder\nVector Quantised\nEncoder\nProsody Latent\nLanguage Model\n\ud835\udc67\"\n\ud835\udc67!\n\ud835\udc67#\nAttend\nReconstruction Loss,\nGAN Loss\nDecoder\nU\nU\nR\n\ud835\udc67\"$\n\ud835\udc67\"%\nCompressive Acoustic Autoencoder\n\"\ud835\udc66\n\ud835\udc66#\n\ud835\udc66#\nE#\nE&\nE'\nD\nFigure 1: The overall architecture of Mega-TTS 2.\nC\u20dd,\nU\u20dd,\nR\u20dd denotes the concatenation, upsampling,\nand repeating operations, respectively. \u02dcy is concatenated along the time axis.\n\u201cAttend\u201d means the attention operation.\nfollowing generative process: y = D(zc, zpd, zt, g), where zc and zt denote the fine-grained content\nand timbre hidden states. g denotes the global style information that contains timbre and prosody. We\nassume that zpd = (zp, zd) contains the fine-grained prosodic style information of pitch and energy\nzp and duration zd. zd = Aligner(y) can be obtained by the external alignment tools (McAuliffe\net al., 2017) and disentangled from zpd. Denote D as the mel-spectrogram decoder. Our goal is to\nconstruct an autoencoder-based model to disentangle speech components.\nDecomposition via Corpus Partition\nDenote Y = {y1, \u00b7 \u00b7 \u00b7 , yn} as the speech corpus for a\ncertain speaker S. In training, we partition Y into the target mel-spectrogram yt and the other\nmel-spectrograms \u02dcy. Here, we make an important assumption that the mutual information between yt\nand \u02dcy only contains timbre information H(zt) and global style information H(g) of yt, i.e.,\nI(yt; \u02dcy) = H(zt) + H(g) .\n(1)\nFirst, based on the assumption, zt and g can be extracted through Et(\u02dcy), and there is no way for\nEt(\u02dcy) to obtain the zp and zc. Second, if we only feed phoneme sequence to Ec, Ec can only pass\nall the content information zc. Third, since the information zc and zt are available now, the prosody\nencoder Ep will prioritize removing the fine-grained content and timbre information if it is forced\nto lose some information by information bottleneck B(\u00b7) (Qian et al., 2019). The bottleneck forces\nEp(yt) to pass only the fine-grained prosodic style zp that other encoders cannot supply, hence\nachieving the decomposition. We provide a detailed explanation of how we ensure the validity of\nEquation 1 in Appendix A.8. After the decomposition, we describe the detailed designs of our\nprompting mechanisms in the following subsections.\n3.2\nCOMPRESSIVE ACOUSTIC AUTOENCODER\nNote that to store timbre information for thousands of speakers, we need a large number of codebook\nentries. However, since the prosody and timbre have been decomposed, the prosodic information zp\ncan be compressed into a highly compact codebook, and the timbre information zt can be extracted via\na powerful speaker encoder. The decomposition strategy not only allows our model to accommodate\nextremely long prosody prompts but also enables our model to control the prosodic styles of generated\nspeeches. As shown in Figure 1, we design the vector quantised (VQ) encoder as Ep, the multi-\nreference timbre encoder as Et, and the content encoder as Ec. Since Ep mainly captures the prosodic\nvariance information, a GAN-based mel-spectrogram decoder D is adopted to model the high-\nfrequency details in spectrograms, which ensures perceptually high-quality reconstructions. Overall,\nthe first-stage training loss can be formulated as L = Lrec + LVQ + LAdv, where Lrec = \u2225yt \u2212 \u02c6yt\u22252\nis the reconstruction loss, LVQ is the VQ codebook loss (Van Den Oord et al., 2017), and LAdv is the\nLSGAN-styled adversarial loss (Mao et al., 2017) whose objective is to minimize the distribution\ndistance between the predicted mel-spectrograms and the ground truth mel-spectrograms. Among the\nproposed three encoders, the content encoder is composed of several feed-forward Transformer layers\nfollowing common practice in non-autoregressive TTS systems (Ren et al., 2019). In the following\nparagraphs, we describe the details of the prosody and timbre encoders, respectively.\n4\nPublished as a conference paper at ICLR 2024\nVector Quantised Encoder\nThe vector quantised encoder Ep consists of two convolution stacks and\na vector quantization bottleneck. The first convolution stacks compress mel-spectrograms into hidden\nstates by a factor of r in length, and the second stacks capture the correlation of features. After that,\nthe vector quantization layer utilizes these hidden states to obtain prosody codes u = {u1, u2, ..., un}\nand hidden states zp. The information bottleneck B(\u00b7) of the VQ encoder is composed of the temporal\ncompression and the vector quantization layer. The detailed instructions for ensuring an appropriate\ninformation bottleneck B(\u00b7) can be found in Appendix F.\nMulti-Reference Timbre Encoder\nOur objective is to extract fine-grained timbre information\nfrom multi-sentence speech prompts. Since speakers can change their timbre by using different\nspeaking techniques according to their speaking habits or desired semantic meanings (McAdams,\n2013), the timbre encoder needs to extract fine-grained timbre information from multiple prompts\nthat can represent the speakers\u2019 habits. Here, we introduce a multi-reference timbre encoder (MRTE)\nto achieve this objective. First, we concatenate the reference mel-spectrograms \u02dcy that belong to the\ntarget speaker but are different from the target mel-spectrogram. The mel encoder then compresses the\nconcatenated mel-spectrogram into acoustic hidden states zt by a factor of d in length. Subsequently,\nto extract semantically relevant timbre information from speech prompts, we introduce a timbre-to-\ncontent attention module. This module takes zc as the query and zt as both the key and the value.\nFinally, we upsample the output of the timbre-to-content attention module to match the length of the\ntarget mel-spectrogram using the length regulator (Ren et al., 2019).\n3.3\nPROSODY LATENT LANGUAGE MODEL\nUnlike previous models that are trained with single-sentence prompts, our prosody latent language\nmodel (P-LLM) aims to capture the speaker\u2019s prosodic patterns from multi-sentence prompts effec-\ntively. During the second-stage training process, we first extract the compressed prosody hidden\nstates {zp1, zp2, \u00b7 \u00b7 \u00b7 , zpn} and the content hidden states {zc1, zc2, \u00b7 \u00b7 \u00b7 , zcn} from multiple speech\nclips {s1, s2, \u00b7 \u00b7 \u00b7 , sn} of the target speaker using the proposed compressive acoustic autoencoder.\nWe then concatenate them along the time axis to construct z\u2032\np = Concat(zp1, zp2, \u00b7 \u00b7 \u00b7 , zpn) and\nz\u2032\nc = Concat(zc1, zc2, \u00b7 \u00b7 \u00b7 , zcn). In order to match the lengths of z\u2032\np and z\u2032\nc in the temporal dimen-\nsion, we expand z\u2032\nc to the frame level with duration information zd and compress it r times with a\nmax pooling layer. After that, we transform z\u2032\np to prosody code u\u2032 and then feed u\u2032 and z\u2032\nc into the\nP-LLM, which predicts the prosody code in an auto-regressive manner:\np\n\u0000u\u2032 | z\u2032\nc; \u03b8\n\u0001\n=\nL\nY\nl=0\np\n\u0000u\u2032\nl | u\u2032\n<l, z\u2032\nc; \u03b8\n\u0001\n,\n(2)\nwhere \u03b8 is the parameters of P-LLM and L is the length of the concatenated prosody code u\u2032. In\ntraining, we set batch size as 1 to increase the maximum number m of prosody codes in each batch\nas much as possible. If the total number of speech frames from a single speaker is less than m \u00d7 r,\nwe will include speech samples from other speakers in this batch and incorporate speaker-level\nattention masks into P-LLM. We do not specifically define the speech prompt; instead, we train the\nlanguage model directly using the concatenated speech samples through the teacher-forcing technique\nwith the cross-entropy loss. To avoid the transition area problems caused by directly concatenating\nthe prompts, we assign the start token and end token to each sentence, which guides P-LLM to\ncontinue writing the current sentence and extract useful information from previous sentences. This\ntraining strategy enables the model to capture the useful prosody-level information contained in the\nmulti-sentence prompts. Therefore, in the inference stage, users can flexibly improve the generation\nquality by extending the length of prompts by concatenating the reference speech clips. For duration\nmodeling, we propose a phoneme-level auto-regressive duration model. This model enhances the\nduration modeling by leveraging the powerful in-context learning capabilities of auto-regressive\nmodels. The overall architecture of the auto-regressive duration model remains the same as P-LLM,\nbut we use mean squared error (MSE) loss instead.\n3.4\nPROSODY INTERPOLATION\nHere, we propose a prosody interpolation technique to control or replace the prosodic style of the\ntarget speaker in the discrete space while ensuring the quality of timbre reconstruction. We achieve\nthis objective by interpolating the probabilities from multiple P-LLM outputs, which come from\n5\nPublished as a conference paper at ICLR 2024\nmultiple speakers. For example, our target speaker has a relatively sad speaking tone, but we want to\ngenerate speeches that sound happier for him while preserving his timbre. The solution is to 1) extract\nprosody latent ua from speeches in a happy tone of other speakers and the sad prosody latent ub from\nthe target speech prompt; 2) utilize two language models to separately decode the target prosody\ncode \u02c6u with the prosodic prompt ua and ub. These language models share the same parameters. In\nevery step t of the decoding process, the probability distributions of the two language models are\ninterpolated with the weight \u03b3, which can be formulated as follows:\np (\u02c6u) =\nT\nY\nt=0\n\u0010\n(1\u2212\u03b3)\u00b7p (\u02c6ut | \u02c6u<t, ub, Concat(zcb, \u02c6zc); \u03b8)+\u03b3 \u00b7p ( \u02c6ut | \u02c6u<t, ua, Concat(zca, \u02c6zc); \u03b8)\n\u0011\n, (3)\nP-LLM\nP-LLM\nInterpolate\n\u2026\n\u2026\n\ud835\udc2e\ud835\udc83\n\ud835\udc2e\ud835\udc82\nSample\n\"\ud835\udc96\nFigure 2: Prosody interpolation.\nwhere zcb and zca are the content information from\nspeech clips sb and sa.\n\u02c6zc is the content infor-\nmation of the target sentence. With our prosody\ninterpolation technique, users can freely control\nthe prosodic style of the generated speech in the\ninference stage. Moreover, the proposed prosody\ninterpolation algorithm utilizes the autoregressive\nprobability distribution of the language model for\nprosody transfer. Compared with directly substi-\ntuting the time-averaged prosody representation ub\nwith ua (Karlapati et al., 2020; Za\u0131di et al., 2021),\nthe prosody latent language model is able to mix ua and ub in a soft and fine-grained manner in the\nautoregressive generation process.\n4\nEXPERIMENTS\n4.1\nEXPERIMENTAL SETUP\nTraining Datasets.\nWe train Mega-TTS 2 and all baselines on LibriLight (Kahn et al., 2020),\nwhich contains 60K hours of unlabelled speech derived from LibriVox audiobooks. The sample rate\nis 16KHz for all speech data. We transcribe the speech with the hybrid DNN-HMM ASR model\npre-trained on 960 hours labeled LibriSpeech following VALL-E (Wang et al., 2023). We align the\nphoneme sequence with speech using the external alignment tool (McAuliffe et al., 2017).\nModel Configuration.\nWe provide model configuration in Appendix A.4 and detailed hyper-\nparameter settings in Table 5.\nTraining and Inference.\nIn the first training stage, we train the first-stage model on 4 NVIDIA\nA100 GPUs, with a batch size of 48 sentences on each GPU. In the second stage, we train the P-LLM\nand duration model on 8 NVIDIA A100 GPUs, with a batch size of 4,000 tokens on each GPU. It\nmeans that our model supports 4,000 \u00d7 8 frames of prompts theoretically. We use the Adam optimizer\nwith \u03b21 = 0.9, \u03b22 = 0.999, \u03f5 = 10\u22129 and follow the same learning rate schedule in Vaswani et al.\n(2017). It takes 600k steps for the first stage model\u2019s training and 300K steps for the second stage\nmodel\u2019s training until convergence. The predicted mel-spectrograms are transformed into audio\nsamples using pre-trained HiFi-GAN V1 (Kong et al., 2020).\nObjective Metrics.\nFor zero-shot TTS, we evaluate the word error rate (WER), speaker similarity\n(SIM), and average dynamic time warping (DTW) (M\u00a8uller, 2007) distance of the pitch for the ground-\ntruth speech and synthesized speech. In terms of the cosine speaker similarity, we use the WavLM\nmodel (Chen et al., 2022) fine-tuned for speaker verification2 to compute the cosine speaker similarity\nscore between the ground-truth speech and the synthesized speech. The similarity score is in the range\nof [\u22121, 1], where a larger value indicates a higher similarity of input samples. We also evaluate the\nword error rate (WER) for cross-lingual TTS. We use the released HuBERT-Large model (Hsu et al.,\n2021) fine-tuned on the LibriSpeech 960h dataset to transcribe the generated speech into text. Then,\nthe WER between the transcribed text and the original target text is measured. We use all samples in\n2https://huggingface.co/microsoft/wavlm-base-plus-sv\n6\nPublished as a conference paper at ICLR 2024\nTable 1: Zero-shot TTS results on LibriSpeech test-clean set. \u201c-\u201d means the result is not available. \u201c-\n10s\u201d means that there are 10 seconds of data from each speaker available for fine-tuning or prompting.\n#Params. denotes the number of parameters (including the vocoder or codec model). The evaluation\nis conducted with 1 NVIDIA V100 GPU and batch size 1. RTF denotes the real-time factor.\nModel\nWER\u2193\nSIM\u2191\nDTW\u2193\nQMOS\u2191\nSMOS\u2191\nRTF\n#Params.\nMethod\nGT\n1.98%\n-\n-\n4.43 \u00b1 0.09\n4.26 \u00b1 0.11\n-\n-\n-\nBaseline-10s\n4.74%\n0.895\n35.12\n3.97 \u00b1 0.08\n3.76 \u00b1 0.13\n0.089\n467M\nFine-tune\nBaseline-60s\n4.18%\n0.923\n31.08\n4.01 \u00b1 0.09\n3.92 \u00b1 0.10\nBaseline-300s\n3.11%\n0.934\n29.80\n4.08 \u00b1 0.07\n4.03 \u00b1 0.08\nVALL-E-3s\n5.83%\n0.885\n36.59\n3.89 \u00b1 0.10\n3.70 \u00b1 0.11\n1.471\n478M\nZero-shot\nVALL-E-10s\n5.54%\n0.893\n34.10\n3.92 \u00b1 0.11\n3.74 \u00b1 0.10\n1.775\nVALL-E-20s\n8.77%\n0.805\n43.02\n3.41 \u00b1 0.12\n3.25 \u00b1 0.14\n2.104\nOurs-3s\n2.46%\n0.898\n34.39\n3.99 \u00b1 0.06\n3.75 \u00b1 0.10\n0.302\n473M\nZero-shot\nOurs-10s\n2.28%\n0.905\n32.30\n4.05 \u00b1 0.08\n3.79 \u00b1 0.09\n0.334\nOurs-60s\n2.24%\n0.926\n30.55\n4.11 \u00b1 0.09\n3.95 \u00b1 0.09\n0.413\nOurs-300s\n2.23%\n0.932\n29.95\n4.12 \u00b1 0.10\n4.01 \u00b1 0.09\n0.923\nthe test set for the objective evaluation. For prosody transfer, we evaluate the WER, SIM, duration\nerror (DE), and the moments (standard deviation (\u03c3), skewness (\u03b3) and kurtosis (\u03ba)) (Andreeva et al.,\n2014; Niebuhr & Skarnitzl, 2019) of the pitch distribution.\nSubjective Metrics.\nWe conduct the MOS (mean opinion score) evaluation on the test set to\nmeasure the audio naturalness via Amazon Mechanical Turk. We keep the text content and prompt\nspeech consistent among different models to exclude other interference factors. We randomly choose\n50 samples from the test set of each dataset for the subjective evaluation, and each audio is listened to\nby at least 20 testers. We analyze the MOS in two aspects: QMOS (Quality, clarity, naturalness, and\nhigh-frequency details) and SMOS (Speaker similarity in terms of timbre reconstruction and prosodic\npattern). We also analyze the CMOS in terms of audio quality and speaker similarity. We tell the\ntesters to focus on one corresponding aspect and ignore the other aspect when scoring.\n4.2\nRESULTS OF ZERO-SHOT SPEECH SYNTHESIS\nIn this subsection, we evaluate our model with various lengths of speech prompts and compare our\nmodel with zero-shot and fine-tuning baselines to demonstrate the effectiveness of the multi-sentence\nprompting mechanism. We randomly choose 20 speakers from the LibriSpeech test-clean set and\nrandomly choose 400 seconds of speeches for each of them. We split the 400 seconds of speech\ninto a 300-second prompt set and a 100-second target set. We keep the prompts consistent among\ndifferent models to exclude other interference factors. We compare the zero-shot speech synthesis\nperformance of Mega-TTS 2 with two systems, including: 1) VALL-E (zero-shot) (Wang et al., 2023),\na large-scale zero-shot TTS model using large language models to generate discrete speech codes.\nSince VALL-E has not been open-sourced yet, we carefully implement it for optimal performance; 2)\nBaseline (fine-tune), a model that incorporates the GAN used in our Mega-TTS 2 to the FastSpeech 2\nbackbone (Ren et al., 2020). To make the baseline support adaptive scenarios, we use the powerful\nspeaker encoder from Meta-StyleSpeech (Min et al., 2021) to extract timbre information. We carefully\nfine-tune the baseline system for 2,000 steps to reach an optimal balance between WER and SIM.\nNote that all of the systems in this experiment are pre-trained on the LibriLight dataset. We provide\nfurther explanation for the selection of the baseline systems in Appendix A.7.\nAnalysis\nAs shown in Table 1, as the amount of adaptation data increases, the performance of Mega-\nTTS 2 continues to improve. Although the performance of VALL-E improves as the data volume\nincreases from 3 seconds to 10 seconds, the performance significantly drops in the 20-second setting\ndue to the single-sentence prompting mechanisms in training. Moreover, since the compression rate\nof the Encodec model restricts the length of prompts, VALL-E fails to generate reasonable speeches\nwith prompts longer than 20 seconds in our experiments. From another perspective, when we have\n10 seconds or 60 seconds of speeches for each speaker, our Mega-TTS 2 surpasses the fine-tuning\nbaseline in terms of speech naturalness and speaker similarity. Additionally, when we have 300\n7\nPublished as a conference paper at ICLR 2024\n0\n1\u00d7102\n1\u00d7103\n5\u00d7103\n2\u00d7104\nFine-tuning Steps\n2\n3\n4\n5\n6\n7\n8\n9\nWER (%)\nOurs-10s\nOurs-60s\nOurs-300s\nBaseline-10s\nBaseline-60s\nBaseline-300s\n0\n1\u00d7102\n1\u00d7103\n5\u00d7103\n2\u00d7104\nFine-tuning Steps\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\n0.93\n0.94\nSIM\nOurs-10s\nOurs-60s\nOurs-300s\nBaseline-10s\nBaseline-60s\nBaseline-300s\nFigure 3: Word error rate (WER) and speaker similarity score (SIM) comparisons in the few-shot\nfine-tuning process.\n50\n150\n250\n350\nF0 (Hz)\n0.000\n0.004\n0.008\n0.012\n0.016\nDensity\nLibriSpeech - 2300\nESD - 0011_Surprise\nDaft-Exprt - Before\nDaft-Exprt - After\n50\n150\n250\n350\nF0 (Hz)\n0.000\n0.004\n0.008\n0.012\n0.016\nDensity\nLibriTTS - 2300\nESD - 0011_Surprise\nOurs - Before\nOurs - After\nFigure 4: The visualizations of pitch distribution before and after the prosody transfer. Here, we\ntransfer the prosodic styles in a surprised tone to speaker \u201c2300\u201d in the LibriSpeech dataset.\nseconds of speeches per speaker, Mega-TTS 2 still outperforms the baseline system in terms of\nWER and achieves comparable performance with it in terms of speaker similarity. We also visualize\nthe WER and SIM in the fine-tuning process and compare the baseline system with Mega-TTS 2\nin Figure 3. Our approach can enhance speaker similarity by utilizing more data like fine-tuning\nbaseline, while maintaining a relatively low word error rate.\n4.3\nRESULTS OF PROSODY TRANSFER\nIn this subsection, we evaluate the prosody transfer performance of our model by transferring the\nemotional styles from the ESD dataset (Zhou et al., 2021) to speakers in the LibriSpeech test-clean\ndataset. We randomly choose 20 speakers from the LibriSpeech test-clean set and choose 50 sentences\nfor each of them. Then, we randomly select an emotional speech clip from the ESD dataset for\neach of the sentences in the LibriSpeech test-clean set and use the selected emotional speech as the\nprosodic reference. We keep the reference speeches consistent among different models to exclude\nother interference factors. We compare the prosody transfer performance of Mega-TTS 2 with two\nsystems, including: 1) CopyCat (Karlapati et al., 2020), a model that utilizes a reference encoder\narchitecture capable of capturing temporal prosodic representations; 2) Daft-Exprt (Za\u0131di et al., 2021),\na model disentangles identity and prosodic information through an adversarial training strategy that\nenables accurate prosody transfer across speakers. To make fair comparisons, we incorporate the\ntechniques for prosody transfer from CopyCat and Daft-Exprt to the baseline system proposed in\nthe previous subsection and scale up the model capacity to ensure that all models have a comparable\nnumber of parameters. All of the systems in this experiment are pre-trained on the LibriLight dataset.\nAnalysis\nTable 2 demonstrates that compared with CopyCat and Daft-Exprt, the moments (\u03c3, \u03b3,\nand \u03ba) of the generated speeches of Megs-TTS are closer to the ground-truth audio and the DE is\nlower than other methods, demonstrating the effectiveness of the proposed prosody interpolation\ntechniques. Besides, we observe that our method can efficiently preserve the original timbre and\nmaintain a high audio quality. We also visualize the prosody distribution before and after the prosody\ntransfer process and compare the baseline system with Mega-TTS 2 in Figure 4.\n8\nPublished as a conference paper at ICLR 2024\nTable 2: Prosody transfer experiments on LibriSpeech test-clean (A) and ESD (B) datasets. SIM-AB\nmeans the SIM after we transfer the prosodic styles from B to A. DE denotes the averaged absolute\nduration error in microseconds. \u03c3, \u03b3, and \u03ba denotes the moments of the pitch distribution.\nModel\nWER\nSIM-AB\nDE\n\u03c3\n\u03b3\n\u03ba\nQMOS\nSMOS-AB\nGT (ESD)\n4.38%\n-\n-\n74.91\n0.707\n0.024\n4.18 \u00b1 0.08\n4.22/2.39\nCopyCat\n5.29%\n0.843/0.740\n37.2\n59.74\n0.889\n0.859\n3.72 \u00b1 0.11\n3.53/3.19\nDaft-Exprt\n4.89%\n0.901/0.633\n36.5\n67.20\n0.851\n0.427\n3.90 \u00b1 0.07\n3.81/2.90\nOurs\n4.82%\n0.920/0.513\n32.8\n72.62\n0.664\n0.197\n3.92 \u00b1 0.08\n3.87/2.64\n4.4\nABLATION STUDIES\nTable 3: Ablation studies of the timbre and prosody\nprompts on zero-shot TTS. \u201cw/ 60s T.\u201d denotes that we\nuse 60 seconds of timbre prompts, and \u201cw/ 60s P.\u201d denotes\nthat we use 60 seconds of prosody prompts.\nSetting\nWER\u2193\nSIM\u2191\nDTW\u2193\nCMOS-Q\nCMOS-S\nOurs-10s\n2.28%\n0.905\n32.30\n0.000\n0.000\nw/ 60s T.\n2.26%\n0.922\n32.23\n+0.128\n+0.241\nw/ 300s T.\n2.25%\n0.930\n32.08\n+0.162\n+0.353\nw/ 60s P.\n2.27%\n0.906\n30.74\n+0.014\n+0.154\nw/ 300s P.\n2.24%\n0.908\n30.25\n+0.017\n+0.196\nProsody and Timbre Prompts\nWe\nevaluate different lengths of prompts for\nthe MRTE and P-LLM separately. In Ta-\nble 3, the SIM score and the speech qual-\nity increase with longer timbre prompts\nwhile the DTW distance almost remains\nunchanged. When we increase the length\nof prosody prompts, the DTW distance\ndecreases while the speaker similarity re-\nmains at the same level. It can be seen\nthat the proposed timbre and prosody\nprompting mechanisms boost the subjec-\ntive speaker similarity in terms of timbre\nand prosody modeling separately.\nTable 4: Ablation studies of the proposed VQ encoder and\nMRTE on zero-shot TTS.\nSetting\nWER\u2193\nSIM\u2191\nDTW\u2193\nCMOS-Q\nCMOS-S\nOurs-10s\n2.28%\n0.905\n32.30\n0.000\n0.000\nOurs-300s\n2.23%\n0.932\n29.95\n+0.144\n+0.493\nw/o MRTE\n5.57%\n0.841\n36.07\n-0.458\n-0.619\nw/ VAE\n2.31%\n0.896\n35.18\n-0.038\n-0.127\nw/ VAE+LDM\n2.25%\n0.907\n32.98\n+0.007\n-0.005\nVQ Encoder and MRTE\nWe test\nthe following four settings:\n1) w/o\nMRTE, which removes the MRTE from\nour model and does not disentangle\nthe prosody and timbre; 2) w/ VAE,\nwhich uses VAE to perform generative\nprosody modeling; 3) w/ VAE+LDM,\nwhich uses VAE and latent diffusion\nmodel (LDM) (Rombach et al., 2022)\nto perform generative prosody model-\ning. The architecture and prompting\nmechanism of LDM is based on Natu-\nralSpeech 2 (Shen et al., 2023b). All baselines use 10 seconds of prompts. The results are shown\nin Table 4. For setting 1), it can be observed that the removal of MRTE significantly affects both\nthe audio quality and speaker similarity. This is because the timbre information is absorbed by\nthe VQ codebook and puts great pressure on the P-LLM, which demonstrates the effectiveness of\ndecomposing timbre and prosody information. For setting 3), substituting the VQ encoder and\nP-LLM with VAE and LDM results in similar performance compared to Ours-10s. However, the\nperformance of w/ VAE+LM is still much inferior to Ours-300s, indicating the superiority of the\nproposed multi-sentence prompting mechanism.\n5\nCONCLUSIONS\nIn this paper, we present Mega-TTS 2, a framework that boosts the prompting mechanisms for zero-\nshot TTS systems. With the proposed multi-sentence prompting strategy, our approach outperforms\nthe fine-tuning baseline when 10 seconds to 5 minutes of adaptation data is available for each speaker.\nFurthermore, our method utilizes a prosody interpolation technique to successfully transfer various\nprosodic styles to the target speaker while preserving the target speaker\u2019s timbre. Experimental results\ndemonstrate that our method exhibits superior performance in terms of audio naturalness and speaker\nsimilarity. Due to space limitations, we include additional discussions in the appendix.\n9\nPublished as a conference paper at ICLR 2024\n6\nACKNOWLEDGMENTS\nThis work was supported in part by the National Natural Science Foundation of China under Grant\nNo. 62222211 and National Key R&D Program of China under Grant No.2022ZD0162000.\nREFERENCES\nBistra Andreeva, Gra\u02d9zyna Demenko, Bernd M\u00a8obius, Frank Zimmerer, Jeanin J\u00a8ugler, and Magdalena\nOleskowicz-Popiel. Differences of pitch profiles in germanic and slavic languages. In Fifteenth\nAnnual Conference of the International Speech Communication Association, 2014.\nSercan Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloning with a few\nsamples. Advances in neural information processing systems, 31, 2018.\nMax Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman. Whisperx: Time-accurate speech\ntranscription of long-form audio. arXiv preprint arXiv:2303.00747, 2023.\nEdresson Casanova, Christopher Shulby, Eren G\u00a8olge, Nicolas Michael M\u00a8uller, Frederico Santos\nde Oliveira, Arnaldo Candido Junior, Anderson da Silva Soares, Sandra Maria Aluisio, and\nMoacir Antonelli Ponti. Sc-glowtts: an efficient zero-shot multi-speaker text-to-speech model.\narXiv preprint arXiv:2104.05557, 2021.\nEdresson Casanova, Julian Weber, Christopher D Shulby, Arnaldo Candido Junior, Eren G\u00a8olge, and\nMoacir A Ponti. Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for\neveryone. In International Conference on Machine Learning, pp. 2709\u20132720. PMLR, 2022.\nJiawei Chen, Xu Tan, Jian Luan, Tao Qin, and Tie-Yan Liu. Hifisinger: Towards high-fidelity neural\nsinging voice synthesis. arXiv preprint arXiv:2009.01776, 2020.\nMingjian Chen, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin, Sheng Zhao, and Tie-Yan Liu. Adaspeech:\nAdaptive text to speech for custom voice. arXiv preprint arXiv:2103.00993, 2021.\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki\nKanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-training\nfor full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):\n1505\u20131518, 2022.\nYutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, Quan\nWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample efficient adaptive text-to-speech.\narXiv preprint arXiv:1809.10460, 2018.\nChung-Ming Chien, Jheng-Hao Lin, Chien-yu Huang, Po-chun Hsu, and Hung-yi Lee. Investigating\non incorporating pretrained and learnable speaker representations for multi-speaker multi-style\ntext-to-speech. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pp. 8588\u20138592. IEEE, 2021.\nSeungwoo Choi, Seungju Han, Dongyoung Kim, and Sungjoo Ha. Attentron: Few-shot text-to-speech\nutilizing attention-based variable-length embedding. arXiv preprint arXiv:2005.08484, 2020.\nErica Cooper, Cheng-I Lai, Yusuke Yasuda, Fuming Fang, Xin Wang, Nanxin Chen, and Junichi\nYamagishi. Zero-shot multi-speaker text-to-speech with state-of-the-art neural speaker embeddings.\nIn ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 6184\u20136188. IEEE, 2020a.\nErica Cooper, Cheng-I Lai, Yusuke Yasuda, and Junichi Yamagishi. Can speaker augmentation\nimprove multi-speaker end-to-end tts? arXiv preprint arXiv:2005.01245, 2020b.\nAlexandre D\u00b4efossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio\ncompression. arXiv preprint arXiv:2210.13438, 2022.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nZhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.\n10\nPublished as a conference paper at ICLR 2024\nSiddharth Gururani, Kilol Gupta, Dhaval Shah, Zahra Shakeri, and Jervis Pinto. Prosody transfer in\nneural text to speech using global pitch and loudness features. arXiv preprint arXiv:1911.09645,\n2019.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked\nprediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n29:3451\u20133460, 2021.\nRongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. Generspeech: Towards style transfer\nfor generalizable out-of-domain text-to-speech. Advances in Neural Information Processing\nSystems, 35:10970\u201310983, 2022a.\nSung-Feng Huang, Chyi-Jiunn Lin, Da-Rong Liu, Yi-Chen Chen, and Hung-yi Lee. Meta-tts: Meta-\nlearning for few-shot speaker adaptive text-to-speech. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 30:1558\u20131571, 2022b.\nYe Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, Patrick Nguyen, Ruoming\nPang, Ignacio Lopez Moreno, Yonghui Wu, et al. Transfer learning from speaker verification to\nmultispeaker text-to-speech synthesis. Advances in neural information processing systems, 31,\n2018.\nJacob Kahn, Morgane Rivi`ere, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel\nMazar\u00b4e, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al. Libri-light:\nA benchmark for asr with limited or no supervision. In ICASSP 2020-2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7669\u20137673. IEEE, 2020.\nMinki Kang, Dongchan Min, and Sung Ju Hwang. Any-speaker adaptive text-to-speech synthesis\nwith diffusion models. arXiv preprint arXiv:2211.09383, 2022.\nMinki Kang, Dongchan Min, and Sung Ju Hwang. Grad-stylespeech: Any-speaker adaptive text-to-\nspeech synthesis with diffusion models. In ICASSP 2023-2023 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pp. 1\u20135. IEEE, 2023.\nSri Karlapati, Alexis Moinet, Arnaud Joly, Viacheslav Klimkov, Daniel S\u00b4aez-Trigueros, and Thomas\nDrugman. Copycat: Many-to-many fine-grained prosody transfer for neural text-to-speech. arXiv\npreprint arXiv:2004.14617, 2020.\nSri Karlapati, Penny Karanasou, Mateusz Lajszczak, Ammar Abbas, Alexis Moinet, Peter Makarov,\nRay Li, Arent van Korlaar, Simon Slangen, and Thomas Drugman.\nCopycat2: A single\nmodel for multi-speaker tts and many-to-many fine-grained prosody transfer. arXiv preprint\narXiv:2206.13443, 2022.\nEugene Kharitonov, Damien Vincent, Zal\u00b4an Borsos, Rapha\u00a8el Marinier, Sertan Girgin, Olivier Pietquin,\nMatt Sharifi, Marco Tagliasacchi, and Neil Zeghidour. Speak, read and prompt: High-fidelity\ntext-to-speech with minimal supervision. arXiv preprint arXiv:2302.03540, 2023.\nHeeseung Kim, Sungwon Kim, and Sungroh Yoon. Guided-tts: A diffusion model for text-to-speech\nvia classifier guidance. In International Conference on Machine Learning, pp. 11119\u201311133.\nPMLR, 2022a.\nJaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon. Glow-tts: A generative flow for\ntext-to-speech via monotonic alignment search. Advances in Neural Information Processing\nSystems, 33:8067\u20138077, 2020.\nJaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversarial\nlearning for end-to-end text-to-speech. In International Conference on Machine Learning, pp.\n5530\u20135540. PMLR, 2021.\nSungwon Kim, Heeseung Kim, and Sungroh Yoon. Guided-tts 2: A diffusion model for high-quality\nadaptive text-to-speech with untranscribed data. arXiv preprint arXiv:2205.15370, 2022b.\nViacheslav Klimkov, Srikanth Ronanki, Jonas Rohnke, and Thomas Drugman. Fine-grained robust\nprosody transfer for single-speaker neural text-to-speech. arXiv preprint arXiv:1907.02479, 2019.\n11\nPublished as a conference paper at ICLR 2024\nJungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for\nefficient and high fidelity speech synthesis. Advances in Neural Information Processing Systems,\n33:17022\u201317033, 2020.\nZvi Kons, Slava Shechtman, Alex Sorin, Carmel Rabinovitz, and Ron Hoory. High quality, lightweight\nand adaptable tts using lpcnet. arXiv preprint arXiv:1905.00590, 2019.\nYounggun Lee and Taesu Kim. Robust and fine-grained prosody control of end-to-end speech\nsynthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 5911\u20135915. IEEE, 2019.\nYichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang,\nLeying Zhang, Kaitao Song, et al. Prompttts 2: Describing and generating voices with text prompt.\narXiv preprint arXiv:2309.02285, 2023.\nNaihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis with\ntransformer network. In Proceedings of the AAAI conference on artificial intelligence, volume 33,\npp. 6706\u20136713, 2019.\nYist Y Lin, Chung-Ming Chien, Jheng-Hao Lin, Hung-yi Lee, and Lin-shan Lee. Fragmentvc:\nAny-to-any voice conversion by end-to-end extracting and fusing fine-grained voice fragments\nwith attention. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pp. 5939\u20135943. IEEE, 2021.\nPhilipos C Loizou. Speech quality assessment. In Multimedia analysis, processing and communica-\ntions, pp. 623\u2013654. Springer, 2011.\nXudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least\nsquares generative adversarial networks. In Proceedings of the IEEE international conference on\ncomputer vision, pp. 2794\u20132802, 2017.\nLe Matthew, Vyas Apoorv, Shi Bowen, Karrer Brian, Sari Leda, Moritz Rashel, Williamson Mary,\nManohar Vimal, Adi Yossi, Mahadeokar Jay, and Hsu Wei-Ning. Voicebox: Text-guided multilin-\ngual universal speech generation at scale, 2023. URL https://voicebox.metademolab.\ncom/.\nStephen McAdams. Musical timbre perception. The psychology of music, pp. 35\u201367, 2013.\nMichael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger.\nMontreal forced aligner: Trainable text-speech alignment using kaldi. In Interspeech, volume\n2017, pp. 498\u2013502, 2017.\nDongchan Min, Dong Bok Lee, Eunho Yang, and Sung Ju Hwang. Meta-stylespeech: Multi-speaker\nadaptive text-to-speech generation. In International Conference on Machine Learning, pp. 7748\u2013\n7759. PMLR, 2021.\nHenry B Moss, Vatsal Aggarwal, Nishant Prateek, Javier Gonz\u00b4alez, and Roberto Barra-Chicote.\nBoffin tts: Few-shot speaker adaptation by bayesian optimization. In ICASSP 2020-2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7639\u20137643.\nIEEE, 2020.\nMeinard M\u00a8uller. Dynamic time warping. Information retrieval for music and motion, pp. 69\u201384,\n2007.\nOliver Niebuhr and Radek Skarnitzl. Measuring a speaker\u2019s acoustic correlates of pitch\u2013but which? a\ncontrastive analysis based on perceived speaker charisma. In Proceedings of 19th International\nCongress of Phonetic Sciences, 2019.\nShifeng Pan and Lei He. Cross-speaker style transfer with prosody bottleneck in neural speech\nsynthesis. arXiv preprint arXiv:2107.12562, 2021.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus\nbased on public domain audio books. In 2015 IEEE international conference on acoustics, speech\nand signal processing (ICASSP), pp. 5206\u20135210. IEEE, 2015.\n12\nPublished as a conference paper at ICLR 2024\nKaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, and Mark Hasegawa-Johnson. Autovc:\nZero-shot voice style transfer with only autoencoder loss. In International Conference on Machine\nLearning, pp. 5210\u20135219. PMLR, 2019.\nYi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech:\nFast, robust and controllable text to speech. Advances in neural information processing systems,\n32, 2019.\nYi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2: Fast\nand high-quality end-to-end text to speech. arXiv preprint arXiv:2006.04558, 2020.\nYi Ren, Ming Lei, Zhiying Huang, Shiliang Zhang, Qian Chen, Zhijie Yan, and Zhou Zhao.\nProsospeech: Enhancing prosody with quantized vector pre-training in text-to-speech. In ICASSP\n2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),\npp. 7577\u20137581. IEEE, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\nJonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng\nChen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al. Natural tts synthesis by conditioning\nwavenet on mel spectrogram predictions. In 2018 IEEE international conference on acoustics,\nspeech and signal processing (ICASSP), pp. 4779\u20134783. IEEE, 2018.\nKai Shen, Junliang Guo, Xu Tan, Siliang Tang, Rui Wang, and Jiang Bian. A study on relu and\nsoftmax in transformer. arXiv preprint arXiv:2302.06461, 2023a.\nKai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang\nBian. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing\nsynthesizers. arXiv preprint arXiv:2304.09116, 2023b.\nAtli Thor Sigurgeirsson and Simon King. Do prosody transfer models transfer prosody\u0192. In ICASSP\n2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),\npp. 1\u20135. IEEE, 2023.\nRJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron Weiss,\nRob Clark, and Rif A Saurous. Towards end-to-end prosody transfer for expressive speech synthesis\nwith tacotron. In international conference on machine learning, pp. 4693\u20134702. PMLR, 2018.\nYuhta Takida, Takashi Shibuya, WeiHsiang Liao, Chieh-Hsin Lai, Junki Ohmura, Toshimitsu Uesaka,\nNaoki Murata, Shusuke Takahashi, Toshiyuki Kumakura, and Yuki Mitsufuji. Sq-vae: Varia-\ntional bayes on discrete representation with self-annealed stochastic quantization. arXiv preprint\narXiv:2205.07547, 2022.\nXu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. A survey on neural speech synthesis. arXiv preprint\narXiv:2106.15561, 2021.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\nMichael Wagner and Duane G Watson. Experimental and theoretical advances in prosody: A review.\nLanguage and cognitive processes, 25(7-9):905\u2013945, 2010.\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing\nLiu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech\nsynthesizers. arXiv preprint arXiv:2301.02111, 2023.\n13\nPublished as a conference paper at ICLR 2024\nYuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng\nYang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-to-end speech\nsynthesis. arXiv preprint arXiv:1703.10135, 2017.\nYihan Wu, Xu Tan, Bohan Li, Lei He, Sheng Zhao, Ruihua Song, Tao Qin, and Tie-Yan Liu.\nAdaspeech 4: Adaptive text to speech in zero-shot scenarios. arXiv preprint arXiv:2204.00436,\n2022.\nDongchao Yang, Songxiang Liu, Rongjie Huang, Jinchuan Tian, Chao Weng, and Yuexian Zou.\nHifi-codec: Group-residual vector quantization for high fidelity audio codec. arXiv preprint\narXiv:2305.02765, 2023.\nJingzhou Yang and Lei He. Towards universal text-to-speech. In Interspeech, pp. 3171\u20133175, 2020.\nDacheng Yin, Chuanxin Tang, Yanqing Liu, Xiaoqiang Wang, Zhiyuan Zhao, Yucheng Zhao, Zhiwei\nXiong, Sheng Zhao, and Chong Luo. Retrievertts: Modeling decomposed factors for text-based\nspeech insertion. arXiv preprint arXiv:2206.13865, 2022.\nJulian Za\u0131di, Hugo Seut\u00b4e, BV Niekerk, and M Carbonneau. Daft-exprt: Robust prosody transfer\nacross speakers for expressive speech synthesis. arXiv preprint arXiv:2108.02271, 2021.\nZiqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing\nLiu, Huaming Wang, Jinyu Li, et al. Speak foreign languages with your own voice: Cross-lingual\nneural codec language modeling. arXiv preprint arXiv:2303.03926, 2023.\nChuanxia Zheng and Andrea Vedaldi. Online clustered codebook. arXiv preprint arXiv:2307.15139,\n2023.\nKun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li. Seen and unseen emotional style transfer for\nvoice conversion with a new emotional speech dataset. In ICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 920\u2013924. IEEE, 2021.\nYixuan Zhou, Changhe Song, Xiang Li, Luwen Zhang, Zhiyong Wu, Yanyao Bian, Dan Su, and\nHelen Meng. Content-dependent fine-grained speaker embedding for zero-shot speaker adaptation\nin text-to-speech synthesis. arXiv preprint arXiv:2204.00990, 2022.\nA\nDETAILED EXPERIMENTAL SETTINGS\nA.1\nDETAILS IN OBJECTIVE EVALUATIONS\nHere, we provide details of the model used in objective evaluations.\nSpeaker Similarity Model\nTo measure the speaker similarity, we use the WavLM (Chen et al., 2022)\nmodel fine-tuned for speaker verification from https://huggingface.co/microsoft/\nwavlm-base-plus-sv to extract the speaker embedding. Then the cosine similarity between\nthe synthesized speech\u2019s speaker embedding and the ground-truth speech\u2019s speaker embedding is\ncalculated as the speaker similarity score. The WavLM model is pre-trained on 94,000 hours of\nspeech data and fine-tuned on the VoxCeleb1 dataset using an X-Vector head with an Additive Margin\nSoftmax loss, which achieves 0.84%, 0.928%, and 1.758% EER (Equal Error Rate) on the Vox1-O,\nVox1-E, and Vox1-H trial lists.\nASR Model\nTo measure the audio quality and speech intelligibility, we evaluate the word error rate\n(WER) metric. We use the fine-tuned HuBERT-Large model to transcribe the synthesized speech into\ntext and calculate the WER between the transcribed text and the original target text. The HuBERT-\nLarge model from https://huggingface.co/facebook/hubert-large-ls960-ft\nis fine-tuned on 960h of Librispeech and achieves 1.5%, 3.0%, 1.9%, and 3.3% WER on the dev-clean,\ndev-other, test-clean, and test-other set of Librispeech.\n14\nPublished as a conference paper at ICLR 2024\nA.2\nDETAILS IN SUBJECTIVE EVALUATIONS\nWe perform the audio quality and speaker similarity evaluations on Amazon Mechanical Turk\n(MTurk). For each dataset, we randomly select 50 samples from the test set and use the TTS systems\nto generate the audio samples. Each audio has been listened to by at least 20 listeners. For MOS,\neach tester is asked to evaluate the subjective score of a sentence on a 1-5 Likert scale. For CMOS,\nlisteners are asked to compare pairs of audio generated by systems A and B following Loizou\n(2011), indicating which of the two audio they prefer. For audio quality evaluation (QMOS and\nCMOS-Q), we tell listeners to \u201cPlease focus on the speech quality in terms of clarity, naturalness,\nand high-frequency details, and ignore other factors\u201d. For speaker similarity evaluations (MOS-S),\nwe tell listeners to \u201cPlease focus only on the similarity of the speaker to the reference one in terms\nof the timbre and prosodic patterns, and ignore the differences of content, grammar, audio quality,\nor other factors.\u201d. We paid $15 to participants hourly and totally spent about $1200 on participant\ncompensation. We tell the participants that the data will be used in scientific research.\nConv1D + LN +\nGELU\n!y\nDownsample (16x)\n\ud835\udc67!\nX 2\nX 5\nConv1D + LN +\nGELU\nX 2\nX 5\nConv1D + LN +\nGELU\n1D Max Pooling (8x)\nX 2\nX 5\nConv1D + LN +\nGELU\nX 2\nX 5\ny\n\ud835\udc67\"\nVector Quantizer\n(a) MRTE\nConv1D + LN +\nGELU\n!y\nDownsample (16x)\n\ud835\udc67!\nX 2\nX 5\nConv1D + LN +\nGELU\nX 2\nX 5\nConv1D + LN +\nGELU\n1D Max Pooling (8x)\nX 2\nX 5\nConv1D + LN +\nGELU\nX 2\nX 5\ny\n\ud835\udc67\"\nVector Quantizer\n(b) VQ Encoder\nFigure 5: The multi-reference timbre encoder (MRTE) and vector quantised (VQ) encoder.\nA.3\nDETAILED NETWORK STRUCTURE\nMRTE\nAs shown in Figure 5, the proposed MRTE is composed of two convolution stacks and a\ndownsampling block. To reduce the computational requirements while maintaining the quality of\ntimbre reconstruction, we downsample the timbre hidden states by a factor of d = 16 in length. In\ntraining, we randomly sample 2,000 frames from \u02dcy for training efficiency.\nVQ Encoder\nThe bottleneck of our VQ Encoder is composed of a max pooling layer with a stride of\n8 and a vector quantised layer. In our experiments, we found that compressing the mel-spectrograms\nwith a compression rate of r = 8 yields superior results compared to phoneme-level compression.\nWe have tried different compression rates (2, 4, 8, 16, 32) and found that r = 8 reached an optimal\nbalance between the reconstruction performance and compression. On the other hand, in the training\nprocess, we also found that the vanilla VQ-VAE suffers from codebook collapse (Takida et al., 2022),\nwhich means only a small portion of codebook vectors are optimized. It restricts the expressive\ncapacity of the codebook and affects the convergence of the training process. To solve the codebook\ncollapse issue, we adopt a dynamical initialization strategy based on CVQ-VAE (Zheng & Vedaldi,\n2023) during training, which ensures the code vectors that are less-used or unused to be modified\nmore than frequently used ones.\n15\nPublished as a conference paper at ICLR 2024\nA.4\nMODEL CONFIGURATION\nOur Mega-TTS 2 consists of three encoders, a prosody latent language model, a mel decoder, and a\ndiscriminator. The prosody encoder, timbre encoder, and decoder consist of 5 convolutional blocks\nwith 512 hidden size and 5 kernel size. The content encoder is an 8-layer Transformer (Vaswani et al.,\n2017; Shen et al., 2023a) with 512 hidden size. The GAN discriminator follows the architecture\nof ML-GAN proposed in Chen et al. (2020). The P-LLM model is a decoder-only architecture\nthat contains 12 Transformer layers with 1024 hidden size, which has 151M parameters. The\nduration predictor is an 8-layer decoder-only Transformer model with 512 hidden size. The codebook\nembedding size is 1024, and the hidden size of the codebook vector is 256. The compression rate r\nand d is set as 8 and 16, respectively. For prosody transfer experiments, \u03b3 is set as 0.8. We provide\ndetailed hyper-parameter settings about the model configuration in Table 5.\nTable 5: Hyperparameters of Mega-TTS 2 models.\nHyper-parameter\nValue\nVQ Prosody Encoder\nEncoder Layers\n3\nHidden Size\n384\nConv1D Kernel\n5\nVQ Embedding Size\n1024\nVQ Embedding Channel\n256\nContent Encoder\nPhoneme Embedding Size\n512\nEncoder Layers\n8\nHidden Size\n512\nKernel Size\n5\nFilter Size\n1024\nMRTE\nEncoder Layers\n5\nQuery Encoder Hidden Size\n512\nKey Encoder Hidden Size\n256\nKey Encoder Stride\n16\nConv1D Kernel\n3\nMel Decoder\nDecoder Layers\n4\nHidden Size\n512\nConv1D Kernel\n5\nP-LLM\nDecoder Layers\n12\nHidden Size\n1024\nProsody Code Embedding Size\n1026\nAttention Headss\n16\nMulti-Length Discriminator\nNumber of Discriminators\n3\nWindow Size\n32, 64, 128\nConv2D Layers\n3\nHidden Size\n192\nTotal Number of Parameters\n367M\nA.5\nERROR BARS AND RANDOM SEEDS\nFor the subjective evaluations, we report confidence intervals of the results of MOS tests. For\nthe objective evaluations, we ran the experiments 10 times with 10 different random seeds\n([1234, 1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999]) and obtained the averaged results.\n16\nPublished as a conference paper at ICLR 2024\nA.6\nSAMPLING STRATEGY FOR P-LLM\nIn all of our experiments, we utilize the top-k sampling strategy for P-LLM, where k is set to 10. The\nsampling-based method, when used with an appropriate k, enhances the output diversity compared to\ngreedy decoding.\nA.7\nABOUT THE SELECTION OF BASELINES\nVALL-E (Wang et al., 2023), NaturalSpeech 2 (Shen et al., 2023b; Leng et al., 2023), and Voice-\nBox (Matthew et al., 2023) are the state-of-the-art zero-shot TTS models. In the experiments of\nzero-shot TTS, we have tried to carefully reproduce their works but failed to reproduce NaturalSpeech\n2 and VoiceBox. Since all of them do not provide the pre-trained models and source code, we only\ncompare Mega-TTS 2 with VALL-E in our experiments.\nA.8\nDETAILED DECOMPOSITION STRATEGY\nThe prosody encoder Ep aims to capture fine-grained and local prosodic style zp. For local prosodic\nstyle zp, we assume that psd(\u00b7) is a perfect local prosody extractor, and we can obtain the following\nequation: I(psd(yt), psd(\u02dcy)) = 0. The content information zc is also local and fine-grained like\nzp. On the other hand, the global prosodic information like the averaged volume and pitch can not\nbe captured by Ec, intuitively. And since we have designed an information bottleneck B(\u00b7) for Ep,\nthe global prosodic information will be prioritized by the timbre encoder Et and stored in H(zt).\nNow that both the local and global prosodic information is appropriately extracted, the validity of\nEquation 1 and our disentanglement strategy can be ensured.\nB\nABOUT SCALING UP DATASET SIZE\nScaling up dataset size is crucial for the practical application of zero-shot TTS. Therefore, we crawled\n200K hours of audiobook recordings from YouTube and novelfm3. The crawled corpus contains both\nlabelled and unlabelled speeches, and most of them do not have speaker information. To transcribe\nthe unlabelled speech in the wild, we use a powerful ASR model called WhisperX (Bain et al., 2023).\nTable 6: The results of zero-shot TTS when we scale up\ndataset size. Ours-10s (60K) means that we use 60K hours\nof speeches from LibriLight to train our model and use 10\nseconds of speech prompts.\nSetting\nWER\u2193\nSIM\u2191\nDTW\u2193\nCMOS-Q\nCMOS-S\nOurs-10s (60K)\n2.28%\n0.905\n32.30\n0.000\n0.000\nOurs-10s (200K)\n2.15%\n0.922\n32.05\n+0.010\n+0.215\nAnd to obtain the speaker informa-\ntion, we use a released automatic\nspeaker diarization model called\npyannote.audio4,\nwhich achieves\nDER=11.24% on the VoxConverse\ndataset and DER=14.09% on the\nAISHELL-4 dataset. In this exper-\niment, we do not change the hyper-\nparameter settings of our model. The\nresults are shown in Table 6. It can be\nseen that increasing the dataset size\ncan improve the speaker similarity of the generated speeches.\nC\nABOUT THE DEFINITION OF ADAPTIVE TTS\nThe concept of adaptive TTS encompasses many aspects like the adaption for different voices,\nlanguages, styles, and domains (Tan et al., 2021). It is also known as various terms in academia\nand industry, such as voice adaptation (Chen et al., 2018), voice cloning (Arik et al., 2018), custom\nvoice (Chen et al., 2021), etc. In this paper, we primarily focus on adaptive TTS for different voices.\n3https://novelfm.changdunovel.com/\n4https://huggingface.co/pyannote/speaker-diarization\n17\nPublished as a conference paper at ICLR 2024\nD\nVISUALIZATION OF ATTENTION MATRICES\nTo further verify the proposed P-LLM and multi-sentence prompting mechanism, we visualize the\nattention matrices averaged across all layers of P-LLM in Figure 6. In this experiment, we separately\nconduct short-sentence generation and long-sentence generation. For short-sentence generation,\nwe randomly selected two sentences that are shorter than 3 seconds from speaker \u201c908\u201d in the\nLibriSpeech test-clean set and concatenated them together. The target texts for Figure 6 (a) and (b)\nare both about 15 words in length. For long-sentence generation, we randomly selected two sentences\nthat are longer than 15 seconds from speaker \u201c908\u201d in the LibriSpeech test-clean set and concatenated\nthem together. The target texts for Figure 6 (c) and (d) are both about 100 words in length. It can\nbe seen that our P-LLM can capture both short-term and long-term information, demonstrating the\neffectiveness of the P-LLM\u2019s training strategy and the multi-sentence prompting mechanism.\n(a)\n(b)\n(c)\n(d)\nFigure 6: Visualization of Attention Matrices. (a) and (b) are the attention matrices in short-sentence\ngeneration; (c) and (d) are the attention matrices in long-sentence generation.\nE\nLIMITATIONS AND ETHICS IMPACTS\nIn this section, we begin by discussing the limitations of the proposed method and outlining our\nstrategies for addressing them in future research. Subsequently, we discuss the ethical impacts that\nmight be brought by zero-shot TTS and our measures to address these concerns.\nE.1\nLIMITATIONS AND FUTURE WORK\nFirstly, our model is trained on an English dataset and does not support multilingual TTS. We plan to\naddress this problem by introducing more multilingual training data. Secondly, the speech quality\ncan be improved by introducing more high-fidelity training data. Thirdly, a well-designed attention\nwindow may further enhance the in-context-learning capability of our P-LLM.\nE.2\nETHICS IMPACTS\nMega-TTS 2 improves the quality and efficiency of zero-shot speech synthesis, which makes it easier\nfor people to synthesize personalized speeches. Under appropriate and legal usage, this technique\ncould facilitate applications like movies, games, podcasts, and other services, making human life\nmore convenient. However, zero-shot TTS may be misused in deepfake-related usages, such as\nspoofing voices. To handle this, potential solutions like building a corresponding deepfake detection\nmodel should be considered. We also plan to add watermarks to the synthesized speeches so that the\npublic can easily tell whether the speeches are synthesized or not. Additionally, restrictions will be\nincluded in the license of our project to prevent the misuse of the model.\nF\nDESCRIPTION OF INFORMATION BOTTLENECK\nThe settings of the information bottleneck B(\u00b7) are crucial for the performance of disentanglement of\nour proposed method. Intuitively, there are four crucial variables for ensuring an appropriate informa-\ntion bottleneck: the number of codebook embedding, the codebook channel size, the compression\n18\nPublished as a conference paper at ICLR 2024\nrate r of the VQ Encoder, and the downsampling rate d of the MRTE. However, the search space of\nthese hyperparameters is too large. Since the settings of r and d also influence the reconstruction\nquality, the burden of P-LLM, and the computational requirements, we first consider r and d, fix\nthem, and find the best setting for the hyperparameters of the codebook.\nThe Compression Rate r\nWe have conducted evaluations for different compression rates r of\nthe VQ Encoder. In the experiments, we found that a lower compression rate would result in\nbetter reconstruction performance for the compressive acoustic autoencoder, but it would impose\na heavier burden on P-LLM since the token sequence is longer. As shown in Table 9, although\nr = 2 achieves the highest objective similarity score, the subjective speech quality and similarity\nsignificantly decrease, which means the final quality of generation is affected. Therefore, we use\nr = 8 to reach an optimal balance between the reconstruction performance and compression.\nTable 7: Ablation studies of the compression rate r. The length of speech prompts is 3 seconds.\nSetting\nWER\u2193\nSIM\u2191\nDTW\u2193\nCMOS-Q\nCMOS-S\nr = 2\n5.53%\n0.905\n42.35\n-0.301\n-0.149\nr = 4\n4.01%\n0.901\n36.93\n-0.148\n+0.053\nr = 8\n2.46%\n0.898\n34.39\n0.000\n0.000\nr = 16\n2.62%\n0.889\n35.18\n-0.088\n-0.141\nr = 32\n3.57%\n0.880\n40.98\n-0.288\n-0.252\nThe Downsampling Rate d\nWe have conducted evaluations for different downsampling rates d\nof the MRTE. The results are shown in Figure 10. It can be seen that when the downsampling rate\nd of the MRTE is low, the mel-spectrogram sequence can provide more information to the timbre\nencoder, resulting in better reconstruction. However, a low downsampling ratio puts a significant\ncomputational burden on the attention operation in MRTE. To reduce the computational requirements\nwhile maintaining the quality of timbre reconstruction, we choose d = 16 for our Mega-TTS 2.\nThe Information Bottleneck with Different Amount of Data\nIntuitively, the performance of\nthe information bottleneck might be very sensitive to the size of the dataset. Therefore, we conduct\nexperiments analyzing the relationship between dataset size and the hyperparameters. The results are\npresented in Appendix G. Although the hyperparameters do not change across these experiments, we\nfind that the model consistently performs well in scenarios with varying amounts of available data.\nG\nSCALING WITH DIFFERENT SIZES OF TRAINING DATA\nHere we evaluated the performance of our Mega-TTS 2 scale with varying amounts of available\ndata. In this experiment, all of the systems use 3 seconds of speech prompts. The results are shown\nin the following table. We can see that Mega-TTS 2 performs well with different sizes of training\ndata, while VALL-E fails to obtain satisfying results when the data is insufficient. We also scale our\nMega-TTS 2 with 200K hours of speeches and the results can be found in Appendix B.\nH\nTHE STRATEGY OF PROSODY MODELING\nIn this section, we conduct experiments to verify the performance of the phoneme-level, word-level,\nand stride-8-level prosody modeling. Stride-8 means that the stride of the pooling layer inside the\nVQ encoder is set to 8. It is worth noting that ProsoSpeech (Ren et al., 2022) utilizes word-level\nprosody modeling. Both of us use the auto-regressive Transformer for prosody modeling. However,\nProsoSPeech aims to improve the naturalness of prosody modeling. Compared with it, our P-LLM\naims at improving the similarity of speaker-relevant prosodic patterns, which extracts fine-grained\nprosodic information from latent prosodic prompts by leveraging the powerful in-context learning\ncapability of LLM. The experimental results are shown in the following table. It can be seen that the\nstride-8-level prosody modeling achieves the best performance. Intuitively speaking, the phoneme-\nlevel prosody modeling provides finer-grained information for better reconstruction while word-level\nprosody modeling provides more semantic information. Both of these methods would be easily\n19\nPublished as a conference paper at ICLR 2024\nTable 8: Ablation studies of the compression rate d. The length of speech prompts is 3 seconds.\nSetting\nWER\u2193\nSIM\u2191\nDTW\u2193\nCMOS-Q\nCMOS-S\nd = 8\n2.39%\n0.900\n34.27\n+0.025\n+0.057\nd = 16\n2.46%\n0.898\n34.39\n0.000\n0.000\nd = 32\n2.98%\n0.880\n34.85\n-0.112\n-0.223\nTable 9: The performance of Mega-TTS 2 scale with different sizes of the training data. All of the\nsystems use 3 seconds of speech prompts.\nModel\nDataset Usage\nWER\u2193\nSIM\u2191\nDTW\u2193\nQMOS\nSMOS\nVALL-E\nLibriLight (1k hours)\n15.73%\n0.782\n54.05\n3.63\u00b10.13\n3.52\u00b10.10\nLibriLight (10k hours)\n8.29%\n0.870\n38.93\n3.84\u00b10.12\n3.65\u00b10.12\nLibriLight (60k hours)\n5.83%\n0.885\n36.59\n3.90\u00b10.12\n3.71\u00b10.10\nMega-TTS 2\nLibriLight (1k hours)\n4.69%\n0.861\n43.07\n3.88\u00b10.11\n3.61\u00b10.10\nLibriLight (10k hours)\n2.91%\n0.882\n35.76\n3.95\u00b10.11\n3.70\u00b10.12\nLibriLight (60k hours)\n2.46%\n0.898\n34.39\n3.98\u00b10.09\n3.77\u00b10.08\nafftected by the alignment accuracy of the speeches in training and inference stages. In order to\nenhance the stability and performance of the proposed model, we use stride-8-level prosody modeling.\nTable 10: Ablation studies of the strategy of prosody modeling. The length of speech prompts is 3\nseconds.\nSetting\nWER\u2193\nSIM\u2191\nDTW\u2193\nCMOS-Q\nCMOS-S\nStride-8-level\n2.46%\n0.898\n34.39\n0.000\n0.000\nPhoneme-level\n2.94%\n0.897\n35.78\n-0.107\n-0.052\nWord-level\n3.12%\n0.892\n34.42\n-0.120\n-0.078\nI\nSPECIAL CASES FOR ASSUMPTION 1\nIn practical scenarios, there is a special case for Assumption 1: the timbre of a speaker may vary\nsignificantly over different time periods. To address this special case, we select \u02dcy randomly from\nregions near yt as much as possible, ensuring that the timbre information of \u02dcy is close to that of yt.\nJ\nDIFFERENT LENGTHS OF CONTEXT DURING TRAINING\nHere we make ablation studies for different lengths of context during our model\u2019s training process.\nWe separately train P-LLM with different numbers of the contextual VQ code tokens and train our\ncompressive acoustic autoencoder with different numbers of the contextual mel-spectrogram frames\nfor MRTE. The results are shown in Figure 7. It can be seen that when we increase the length of\ncontext, the performance of the model during training significantly improves, demonstrating the\neffectiveness of our multi-reference training strategy.\nK\nEXPLANATIONS ABOUT MORE CASES OF ROBUST SPEECH SYNTHESIS\nIn our Mega-TTS 2, we employ a language model only for prosody modeling, enabling our model to\nbenefit from the advantages of in-context learning provided by the LLM model. This approach also\nhelps to address the robustness issues (word skipping or repeating) associated with the autoregressive\nTTS model. Therefore, we make explanations about more cases of robust speech synthesis, to\ndemonstrate our method\u2019s necessity. In commercial scenarios, news reporting, and other formal\nscenarios, robustness is a crucial factor. Just a few repeating or skipping words can have significant\nnegative impacts. These situations are better suited for models with duration models that ensure\nrobustness, such as FastSpeech [4], Glow-TTS [5], and our Mega-TTS 2. However, for models\nlike tacotron [6], word omissions or repetitions can significantly affect the listening experience. On\n20\nPublished as a conference paper at ICLR 2024\n0\n50000\n100000 150000 200000 250000 300000 350000\nSteps\n0.225\n0.250\n0.275\n0.300\n0.325\n0.350\n0.375\n0.400\nL1 distance\nReconstuction Quality\nMRTE w/ 500 CF\nMRTE w/ 1000 CF\nMRTE w/ 2000 CF\n(a)\n0\n100000\n200000\n300000\n400000\n500000\nSteps\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\n4.4\nCross-entropy Loss\nP-LLM w/ 2000 CT\nP-LLM w/ 4000 CT\n(b)\nFigure 7: Visualization of training loss curves with different lengths of context. In subfigure (a),\n\u201cMRTE w/ 500 CF\u201d means we set the number of the contextual mel-spectrogram frames for MRTE to\n500. In subfigure (b), \u201cP-LLM w/ 2000 CT\u201d means we set the number of the contextual VQ code\ntokens for P-LLM to 2000.\nthe other hand, in some scenarios, robustness is relatively less important. For example, occasional\nmissing words or repetitions in dialogue scenes can also be natural.\nL\nRESULTS WITH NOISY REFERENCE PROMPTS\nTo verify our model\u2019s robustness against noisy reference prompts, we conduct experiments on\nLibriSpeech test-other set. The experimental setup for this experiment is consistent with the one\ndescribed in Section 4.2. The results are shown in Table 11. It can be seen that Mega-TTS 2 maintains\nexcellent performance with noisy reference prompts.\nTable 11: Results of zero-shot TTS on the LibriSpeech test-other set.\nModel\nWER\u2193\nSIM\u2191\nDTW\u2193\nQMOS\nSMOS\nGT\n3.17%\n-\n-\n3.95\u00b10.09\n3.98\u00b10.08\nVALL-E-3s\n6.21%\n0.889\n36.13\n3.64\u00b10.12\n3.70\u00b10.11\nOurs-3s\n2.73%\n0.904\n32.50\n3.77\u00b10.10\n3.80\u00b10.09\n21\n"
  },
  {
    "title": "Learning to Retrieve In-Context Examples for Large Language Models",
    "link": "https://arxiv.org/pdf/2307.07164.pdf",
    "upvote": "20",
    "text": "Learning to Retrieve In-Context Examples for Large Language Models\nLiang Wang and Nan Yang and Furu Wei\nMicrosoft Research\n{wangliang,nanya,fuwei}@microsoft.com\nAbstract\nLarge language models (LLMs) have demon-\nstrated their ability to learn in-context, allowing\nthem to perform various tasks based on a few\ninput-output examples. However, the effective-\nness of in-context learning is heavily reliant\non the quality of the selected examples. In\nthis paper, we propose a novel framework to\niteratively train dense retrievers that can iden-\ntify high-quality in-context examples for LLMs.\nOur framework initially trains a reward model\nbased on LLM feedback to evaluate the qual-\nity of candidate examples, followed by knowl-\nedge distillation to train a bi-encoder based\ndense retriever. Our experiments on a suite\nof 30 tasks demonstrate that our framework\nsignificantly enhances in-context learning per-\nformance. Furthermore, we show the gener-\nalization ability of our framework to unseen\ntasks during training. An in-depth analysis\nreveals that our model improves performance\nby retrieving examples with similar patterns,\nand the gains are consistent across LLMs of\nvarying sizes. The code and data are available\nat https://github.com/microsoft/LMOps/\ntree/main/llm_retriever.\n1\nIntroduction\nIn-context learning (ICL) (Brown et al., 2020) is an\nemerging learning paradigm that allows LLMs to\nperform tasks with few-shot examples, without re-\nquiring any updates to the model parameters. This\napproach stands in stark contrast to traditional ma-\nchine learning, where models are typically trained\non large datasets of labeled examples (Devlin et al.,\n2019). In-context learning offers a significant ad-\nvantage in domains where labeled data is scarce\nor expensive to obtain, as it greatly reduces the\namount of required labeled data.\nThere are several challenges associated with un-\nderstanding and enhancing the effectiveness of in-\ncontext learning. One such challenge is that LLMs\ncan be highly sensitive to the quality of the in-\ncontext examples provided (Liu et al., 2022; Min\net al., 2022). If the examples are not representative\nof the target task, then the model may not be able\nto learn effectively. Empirical studies (Liu et al.,\n2022; Luo et al., 2023) have demonstrated that us-\ning BM25 algorithm or off-the-shelf sentence em-\nbeddings (Reimers and Gurevych, 2019) to retrieve\nexamples from the training set can substantially en-\nhance the performance of in-context learning over\nrandom selection. Another approach involves train-\ning dense retrievers based on the feedback signals\nfrom LLMs, which has shown promising results in\nsemantic parsing (Rubin et al., 2022), cross-task\nprompt retrieval (Cheng et al., 2023), and unified\nmulti-task retrieval (Li et al., 2023). However, ex-\nisting methods either focus on a relatively small\nlanguage model (Rubin et al., 2022), or fail to ex-\nploit the fine-grained feedback information from\nLLMs in a principled manner (Li et al., 2023).\nIn this paper, we propose a novel framework,\nLLM-R (LLM Retriever), which aims to retrieve\nhigh-quality in-context examples for large lan-\nguage models. Given an initial set of retrieved\ncandidates, our framework ranks them based on the\nconditional LLM log probabilities of the ground-\ntruth outputs. Subsequently, a cross-encoder based\nreward model is trained to capture the fine-grained\nranking signals from LLMs. Finally, a bi-encoder\nbased dense retriever is trained using knowledge\ndistillation. The reward model plays a crucial role\nin providing more informative soft-labels that are\nsuitable for distillation, instead of using heuristi-\ncally constructed one-hot labels. This pipeline can\nbe iterated multiple times by retrieving a new set\nof candidates based on the latest dense retriever.\nFor evaluation purposes, we assemble a diverse\nset of 30 NLP tasks, which span 9 categories, in-\ncluding question answering, natural language infer-\nence, commonsense reasoning, and summarization,\namong others. Experimental results obtained using\narXiv:2307.07164v2  [cs.CL]  26 Jan 2024\nLLaMA-7B (Touvron et al., 2023) demonstrate\nthat our model improves the in-context learning\nperformance by an average of 7.8% compared to\nrandom selection. Similar improvements are also\nobserved on held-out tasks and LLMs of varying\nsizes. Further analysis reveals that the top-retrieved\nexamples share similar input patterns or the same\nlabels as the testing example. Our model is par-\nticularly effective for classification tasks with am-\nple training examples. In contrast, tasks such as\nclosed-book question answering and commonsense\nreasoning rely more on the inherent capabilities\nof LLMs and are less sensitive to the quality of\nin-context examples.\n2\nRelated Work\nIn-Context Learning is an emergent property of\nlarge language models (LLMs) that enables them to\nperform various tasks conditioned on a few input-\noutput examples, without any parameter updates or\nfine-tuning. This property has been demonstrated\nin LLMs such as GPT-3 (Brown et al., 2020), GPT-\nNeo (Black et al., 2021), and LLaMA (Touvron\net al., 2023), and attracts considerable attention\nfrom the research community. One area of research\nis focused on understanding the underlying mech-\nanism and principles of in-context learning. For\ninstance, Xie et al. view in-context learning as im-\nplicit Bayesian inference, while Dai et al. interpret\nit as meta optimization.\nAnother area of research is to explore different\nstrategies for selecting and designing in-context\nexamples for LLMs. Recent studies (Liu et al.,\n2022; Rubin et al., 2022; Li et al., 2023; Luo et al.,\n2023) have shown that using BM25 algorithm\nor fine-tuning dense retrievers based on LLM\nfeedback to retrieve from the training set can im-\nprove the performance of in-context learning. Our\nwork also falls into this area by proposing a novel\ntraining method. To model the interaction between\nin-context examples, determinantal point process\n(Ye et al., 2023) and sequential decision-making\n(Zhang et al., 2022) are introduced as preliminary\nexplorations. In contrast, Structured Prompting\n(Hao et al., 2022) breaks the limitation of input\ncontext length and scales the number of in-context\nexamples to thousands.\nDense Retrieval is a widely used information\nretrieval approach that utilizes dense vectors to\nperform semantic matching between queries and\ndocuments in the latent space\n(Reimers and\nGurevych, 2019; Wang et al., 2022). Compared\nto sparse retrieval methods such as BM25, dense\nretrieval exploits the powerful modeling capacity\nof pre-trained language models (PLMs) (Devlin\net al., 2019) to learn relevance functions and\nhas the potential to overcome the vocabulary\nmismatch problem. Various techniques such as\nhard negative mining (Karpukhin et al., 2020),\nknowledge distillation\n(Ren et al., 2021), and\ncontinual pre-training (Wang et al., 2022) have\nbeen proposed to enhance the performance of\ndense retrieval.\nRetrieval Augmented LLMs combine the genera-\ntive power of LLMs with the ability to retrieve rele-\nvant information from external sources (Ram et al.,\n2023; Lewis et al., 2020; Shi et al., 2023). This\nparadigm has the potential to enhance the factual\nconsistency of generated texts, make LLMs aware\nof the up-to-date knowledge, as well as provide a\nnatural way for source attribution (Nakano et al.,\n2021). The retrieved information can be incorpo-\nrated into LLMs through various mechanisms, such\nas input concatenation (Shi et al., 2023), interme-\ndiate attention fusion (Borgeaud et al., 2022), and\noutput interpolation (Khandelwal et al., 2020). For\nin-context learning, the goal of retrieval augmen-\ntation is to improve the performance of LLMs on\ndownstream tasks by retrieving informative exam-\nples (Li et al., 2023; Luo et al., 2023).\n3\nPreliminaries\nIn this section, we provide a brief introduction\nto the problem setting of in-context example re-\ntrieval. Given a test example xtest from a target\ntask and k in-context examples {(xi, yi)}k\ni=1 from\na pre-defined pool P, a frozen language model M\nis employed to predict an output y\u2032\ntest through au-\ntoregressive decoding. The primary objective of\nin-context example retrieval is to retrieve k exam-\nples from P such that the predicted output y\u2032\ntest is\nas close as possible to the ground-truth output ytest\nbased on some task-specific metrics. In this paper,\nthe example pool P is the union of the training set\nfor all the tasks in our evaluation.\nStraightforward solutions include utilizing the\nBM25 algorithm or readily available text embed-\nding models (Wang et al., 2022; Liu et al., 2022)\nto retrieve examples from P by treating xtest as\na query. Despite their simplicity, these methods\nx1\ny1\nx2\ny2\nx3\ny3\n\u2026 \u2026\nExample Pool\nLLM-R\nLLaMA\nwho plays the dad in drake and josh?\nJonathan Goldstein\n<xi   yi>\n<xi   yi>\n<xi   yi>\ndistillation\n\u2026 \u2026\ntraining\nLLM-Ri\nLLM-Ri + 1\nReward Model\nIterative Training\nLLaMA\nRank mined candidates\nwho won the 2015 little league world series?\nxi\nyi\nxj\nxk\nyj\nyk\ninference\nx1\ny1\nx2\ny2\nx3\ny3\n\u2026 \u2026\nExample Pool\nxi   \nyi        x  \ny\nFigure 1: The overall architecture of our proposed framework LLM-R. The training process comprises three stages:\ngenerating training data based on an initial retriever and LLM feedback, reward modeling, and training dense\nretrievers by distilling the knowledge from the reward model. At inference time, the trained dense retriever is\nemployed to retrieve in-context examples from the pool P and the retrieved examples are fed to the LLM to generate\nthe output.\nhave been shown to be more effective empirically\nwhen compared to the random selection baseline.\nIn contrast, our framework aims to learn a dense re-\ntriever customized for in-context example retrieval\nby leveraging the feedback from LLMs.\n4\nMethodology\nOur proposed framework is depicted in Figure 1.\nIt includes four main components: training data\ngeneration, reward modeling, dense retriever train-\ning, and inference, which are described in detail in\nthe following subsections.\n4.1\nTraining Data Generation\nInitial Candidates Retrieval Given an example\n(x, y) from the training set, where x is the input and\ny is the groundtruth output, we retrieve the top-n\ncandidates {(xi, yi)}n\ni=1 from the example pool P\nusing an initial retriever. The pool P contains the\ntraining examples from a mixture of tasks. Since\n(x, y) \u2208 P holds during training, we exclude itself\nfrom the retrieval results.\nIn this paper, we employ the unsupervised\nBM25 algorithm as the initial retriever. The query\nonly consists of the input x, while each retrieval\ncandidate is the string concatenation of the input\nxi and the output yi.\nThis setting aligns with\nthe test-time scenario, where the groundtruth\noutput is unavailable. With a reasonably effective\ninitial retriever, the top-n candidates would likely\ncontain some positive examples and hard negative\nexamples.\nRanking Candidates using LLMs To assess the\nquality of the retrieved candidates, we utilize feed-\nback signals from a frozen LLM. Specifically, we\nrank the candidates in descending order based on\nthe log-likelihood of the groundtruth output y, as\ngiven by the following equation:\nlog p(y|x, xi, yi), \u2200i \u2208 {1, 2, . . . , n}\n(1)\nHere, p(y|x, xi, yi) is the conditional probability of\ny given the input x and the i-th candidate (xi, yi).\nIt is noteworthy that computing p(y|x, xi, yi) re-\nquires only one forward pass, and does not rely\non any task-specific metrics, despite the autore-\ngressive nature of language models. In practical\napplications, this helps reduce the inference cost of\nLLMs.\n4.2\nReward Modeling\nIn order to capture the preferences of LLMs over\nthe retrieved candidates and provide fine-grained\nsupervision for dense retrievers, we propose to train\na cross-encoder based reward model. For a training\nexample (x, y), we first sample one positive exam-\nple (x+, y+) from the top-ranked candidates and\nNneg hard negative examples {(x\u2212\ni , y\u2212\ni )}Nneg\ni=1 from\nthe bottom-ranked candidates. The reward model\ntakes as input the concatenation of (x, y, x+, y+)\nand produces a real-valued score s(x, y, x+, y+),\nsimilarly for the hard negatives. It is trained to\nminimize the following cross-entropy loss:\nLreward = \u2212 log\nes(x,y,x+,y+)\nes(x,y,x+,y+) + PNneg\ni=1 es(x,y,x\u2212\ni ,y\u2212\ni )\n(2)\nIt is important to note that the reward model is only\nused to provide supervision for the dense retriever\nand has access to the groundtruth label y, which is\nnot available at test time. This is a key difference\nfrom the re-ranker in the ad-hoc retrieval setting\n(Ren et al., 2021). Compared to the bi-encoder\nbased dense retrievers, the reward model enables\nfull interaction between the inputs and can there-\nfore serve as a teacher model. The log-likelihood\nscores from LLMs display high variance across\ndifferent examples. In contrast, the reward model\nscores are more suitable for knowledge distillation.\n4.3\nTraining LLM Retrievers with Knowledge\nDistillation\nTo facilitate efficient inference, the dense retriever\nis based on the bi-encoder architecture. Given\na query x, we compute its low-dimensional em-\nbedding hx by performing average pooling over\nthe last-layer hidden states. Similarly, we obtain\nthe embedding h(xi,yi) for the candidate (xi, yi)\nby taking the concatenation of xi and yi as in-\nput.\nThe matching score f(x, xi, yi) is com-\nputed as the temperature-scaled cosine similarity\ncos(hx, h(xi,yi))/\u03c4, where \u03c4 is a temperature hy-\nperparameter. In this paper, we use a shared en-\ncoder for both the query and the retrieval candi-\ndates.\nThe dense retriever is trained to distill the knowl-\nedge from the reward model. We use the KL di-\nvergence loss Ldistill = KL(preward || pretriever) to\nmeasure the mismatch between the reward model\ndistribution preward and the retriever distribution\npretriever. Ldistill is only computed over the hard neg-\natives for efficiency reasons. To incorporate the\nin-batch negatives, we also include an InfoNCE-\nbased contrastive loss Lcont (Chen et al., 2020)\nby treating the candidate with the highest reward\nas the positive example. The final loss function\nLretriever is a weighted sum of the contrastive loss\nand the knowledge distillation loss:\nLretriever = \u03b1Lcont + Ldistill\n(3)\nHere, \u03b1 is a constant that controls the relative\nimportance of the two losses.\nIterative Training As illustrated in Figure 1, the\nretriever trained in iteration i can be employed to\nretrieve candidates for the subsequent iteration i+1.\nIn the first iteration, the candidates are retrieved\nusing BM25. Such an iterative training approach\n(Xiong et al., 2021; Li et al., 2023) allows improv-\ning retriever quality by mining better positive and\nhard negative examples.\n4.4\nEvaluation of LLM Retrievers\nGiven a test example xtest, we compute its embed-\nding htest using the trained retriever and retrieve\nthe top k candidates from the pool P as the k-shot\nin-context examples. The input to the LLM is the\nconcatenation of the k-shot examples and xtest. The\noverall procedure is illustrated in Figure 1.\nDepending on the task type of xtest, different\ndecoding strategies are employed to generate the\nfinal prediction. For classification tasks, we use\ngreedy search with constrained decoding to make\nsure the prediction is a valid class label. For multi-\nple choice tasks, all the choices are ranked based on\nthe average token-level log-likelihood score, and\nthe one with the highest score is selected as the\nmodel\u2019s prediction. Generation tasks use greedy\nsearch without any constraints. For quantitative\nevaluation, the prediction is compared with the\ngroundtruth ytest using task-specific metrics.\n5\nExperiments\n5.1\nEvaluation Setup\nWe utilize a total of 30 publicly available datasets\n1 from 9 distinct categories for training and eval-\nuation, as shown in Figure 2. This collection is\nbased on FLAN (Wei et al., 2022) and UPRISE\n(Cheng et al., 2023). Different from our work,\nFLAN is focused on fine-tuning language models\nto follow instructions, while UPRISE is designed\nfor cross-task retrieval. To test the generalization\nability of the models to unseen tasks, we held out\nfour datasets, namely QNLI, PIQA, WSC273, and\nYelp, from the training process. The retrieval pool\nis created by taking the union of all the training\nexamples, which results in a total of approximately\n6.3M examples. For each dataset, we sample a\nmaximum of 30k examples for training and 10k\nexamples for evaluation to reduce the cost of LLM\n1We use \u201cdatasets\u201d and \u201ctasks\u201d interchangeably.\nClose QA\nNQ\nARC Easy\nARC Chall.\nCommonsense\nCOPA\nHellaSwag\nPIQA\nCoreference\nWSC\nWinogrande\nWSC 273\nParaphrase\nMRPC\nPAWS\nQQP\nReading Comp.\nMultiRC\nOpenBook QA\nSQuAD v1\nSentiment\nSST2\nSentiment140\nYelp\nData2Text\nCommonGen\nE2E NLG\nDART\nSummarize\nAESLC\nAG News\nGigaword\nNLI\nRTE\nSNLI\nMNLI-m\nMNLI-mm\nQNLI\nBoolQ\nFigure 2: The collection of datasets used in our experiments. The yellow-colored datasets are held out and excluded\nfrom training. For further information, please refer to Table 8 in the Appendix.\n# of datasets \u2192\nCQA\nComm.\nCoref.\nNLI\nPara.\nRC\nSent.\nD2T\nSumm.\nAvg\n3\n3\n3\n5\n3\n4\n3\n3\n3\n30\nZero-shot\n29.0\n71.5\n66.8\n44.0\n60.0\n41.3\n50.5\n25.6\n17.5\n44.9\nRandom\n40.4\n77.6\n67.2\n50.9\n56.6\n58.1\n88.8\n47.0\n38.9\n57.9\nK-means\n41.6\n79.5\n66.0\n50.8\n52.6\n53.6\n90.9\n42.5\n40.5\n57.0\nBM25\n45.9\n78.1\n62.9\n54.7\n66.1\n59.9\n89.6\n49.3\n50.0\n61.3\nE5base\n49.0\n79.8\n64.6\n53.6\n58.0\n60.2\n94.4\n48.0\n50.0\n61.4\nSBERT\n48.5\n79.3\n64.2\n57.5\n64.1\n60.6\n91.9\n47.4\n49.3\n62.1\nEPR\u2020\n48.4\n79.3\n64.4\n64.3\n65.1\n59.8\n91.7\n49.7\n50.0\n63.5\nLLM-R (1 iter)\n48.8\n80.1\n67.6\n71.9\n66.5\n60.0\n93.5\n50.1\n50.8\n65.7\nLLM-R (2 iter)\n48.7\n80.4\n70.4\n72.5\n71.5\n59.0\n93.6\n49.9\n51.1\n66.5\nLLM-R (3 iter)\n48.9\n80.0\n70.8\n72.6\n72.8\n58.0\n92.9\n49.8\n50.8\n66.4\nStd dev.\n\u00b10.2\n\u00b10.8\n\u00b10.7\n\u00b10.1\n\u00b11.1\n\u00b10.0\n\u00b10.4\n\u00b10.0\n\u00b10.1\n\u00b10.2\nTable 1: Our main results. We report the average metrics for Close QA (CQA), Commonsense Reasoning (Comm.),\nCoreference (Coref.), NLI, Paraphrase (Para.), Reading Comprehension (RC), Sentiment (Sent.), Data-to-text (D2T),\nSummarize (Summ.). The standard deviation is computed over 3 runs with the \u201cRandom\u201d baseline. Dense retriever\nbaselines include E5 (Wang et al., 2022), SBERT (Reimers and Gurevych, 2019), and EPR (Rubin et al., 2022). \u2020:\nOur re-implementation for fair comparison.\ninference. For evaluation, we report the average\nmetrics in each task category. Please check Table\n8 for the specific metrics used for each dataset.\nIn the main experiments, we use LLaMA-7B\n(Touvron et al., 2023) as the default LLM for can-\ndidate ranking and task evaluation unless other-\nwise specified. The reward model is initialized\nwith ELECTRAbase (Clark et al., 2020) and the\nretriever is initialized with E5base (Wang et al.,\n2022). The baselines include zero-shot prompting,\nk-means clustering, random selection, BM25 (Lin\net al., 2021), and two off-the-shelf dense retrievers,\nnamely SBERT (all-mpnet-base-v2) (Reimers and\nGurevych, 2019) and E5base. Except for zero-shot\nevaluation, we retrieve 8 in-context examples for\neach test input. More implementation details and\ntraining hyperparameters are in Appendix A.\n5.2\nMain Results\nTable 1 presents the main results of our experi-\nments. We observe that the simple BM25 algorithm\nserves as a strong baseline, exhibiting consistent\nimprovements over the random selection strategy.\nThis conclusion aligns with the findings of Luo\net al.. Such effectiveness of BM25 can help warm\nup the first training iteration by providing a set of\nhigh-quality candidates. We also tried to use E5base\nas the initial retriever, but the benefits compared to\nBM25 are marginal. Therefore, we stick to BM25\nfor its simplicity.\nAfter the first iteration, our proposed model\nLLM-R outperforms all the baselines (63.5 \u2192\n65.7) by training on the BM25 retrieved candidates.\nThe second iteration includes the mined positive\nand hard negative examples from \u201cLLM-R (1 iter)\u201d,\nCQA\nComm.\nCoref.\nNLI\nPara.\nRC\nSent.\nD2T\nSumm.\nAvg\nLLM-R (1 iter)\n48.8\n80.1\n67.6\n71.9\n66.5\n60.0\n93.5\n50.1\n50.8\n65.7\nmodel variants\nw/o reward model\n48.8\n79.1\n64.3\n68.9\n70.2\n60.5\n91.7\n49.4\n50.5\n64.9\u21930.8\nreward model w/o label y+\n48.5\n79.7\n67.5\n64.1\n62.7\n60.8\n92.3\n49.6\n49.8\n63.8\u21931.9\nLLM score as reward\n48.0\n79.4\n67.0\n67.0\n74.0\n60.5\n91.5\n49.6\n50.3\n65.2\u21930.5\nretriever initialization\ninitialize w/ BERTbase\n48.7\n79.6\n69.4\n70.9\n63.0\n60.7\n92.0\n50.0\n50.2\n65.2\u21930.5\nTable 2: Different training variants of LLM-R. \u201cw/o reward model\u201d is trained solely with contrastive loss on LLM\nranked candidates. \u201cLLM score as reward\u201d uses the log-likelihood score from LLMs as the distillation target.\nNeither of these two variants utilizes the reward model. \u201creward model w/o label y+\u201d denotes that the reward model\nis trained without access to the groundtruth label y+.\nZero-shot\nRandom\nK-means\nBM25\nE5base\nSBERT\nLLM-R\nQNLI\n49.2\n56.4\n53.4\n62.2\n61.5\n61.9\n69.6\u21917.7\nPIQA\n77.0\n79.1\n79.4\n81.3\n81.3\n80.7\n81.6\u21910.3\nWSC273\n74.0\n74.4\n74.7\n64.5\n65.2\n62.6\n79.5\u21914.8\nYelp\n47.9\n92.0\n93.5\n93.5\n97.3\n95.9\n95.9\u21931.4\nAverage\n62.0\n75.5\n75.3\n75.4\n76.3\n75.3\n81.7\u21915.4\nTable 3: Generalization to four held-out tasks.\nraising the average score to 66.5 (+0.8). Further\niterations do not yield substantial improvements,\nindicating that the model has converged.\n6\nAnalysis\nIn this section, we examine the performance of\nLLM-R across various tasks, LLMs, and model\nvariants.\nUnless explicitly specified, \u201cLLM-R\u201d\nrefers to the model with 2 training iterations.\n6.1\nTraining Pipeline of LLM-R\nWe investigate several LLM-R variants LLM-R in\nTable 2 to understand the contribution of each\ncomponent. The \u201cw/o reward model\u201d variant re-\nmoves the knowledge distillation loss and sees 0.8\npoints drop in average score. This indicates that\nthe reward model is crucial for the performance of\nLLM-R. Inspired by REPLUG (Shi et al., 2023),\nwe experiment with a variant that uses the log-\nlikelihood from LLMs as the reward for distillation.\nAlthough it outperforms the \u201cw/o reward model\u201d\nvariant, it still lags behind our method by 0.5 points.\nThe average token-level log-likelihood from LLMs\nis not a probability distribution by nature. We em-\npirically observe that feedback scores for some\ntraining examples are concentrated in a very nar-\nrow range, while other scores are more dispersed.\nThis makes it suboptimal to serve as target distribu-\ntion within KL-divergence framework. Changing\nthe retriever initialization from E5 (Wang et al.,\n2022) to BERT (Devlin et al., 2019) results in a\nperformance drop, but not as significant as in the\nad-hoc retrieval setting.\n6.2\nGeneralization Ability of LLM-R\nWe evaluate the generalization ability of LLM-R\nfrom two dimensions. In the first scenario, we test\nwhether the trained retriever can retrieve good in-\ncontext examples for tasks that are not seen during\ntraining. In the second scenario, we test whether\na model trained with one LLM can generalize to\nother LLMs that vary in size and quality.\nIn Table 3, we report the performance of LLM-R\non four held-out tasks. The results demonstrate that\nLLM-R surpasses the second-best model E5base\nby an average of 5.4 points, indicating its ability\nto generalize to previously unseen tasks. Under\nthe current evaluation protocol, there are training\ndatasets that share the same task category as the\nheld-out ones (e.g., QNLI and SNLI are both for\nnatural language inference). A more challenging\nsetting is to test on non-overlapping task categories,\nwhich we leave for future work.\nThe LLM-R model is trained with LLaMA-7B.\nTo evaluate its generalization ability across differ-\nent LLMs, we test on three other models, namely\nGPT-Neo-2.7B (Black et al., 2021), LLaMA-13B,\nand GPT-35-Turbo. Results in Table 4 show that\nLLM-R consistently outperforms the BM25 base-\nline for LLMs with parameter ranges from 2.7B\nCQA\nComm.\nCoref.\nNLI\nPara.\nRC\nSent.\nD2T\nSumm.\nAvg\ngpt-neo-2.7b\nBM25\n41.1\n67.0\n53.2\n47.6\n64.5\n51.2\n78.3\n45.4\n47.3\n54.4\nLLM-R\n42.2\n68.0\n59.7\n71.5\n73.0\n51.6\n91.6\n46.9\n48.8\n61.8\u21917.4\nllama-13b\nBM25\n49.6\n80.1\n61.1\n67.0\n69.9\n60.5\n92.5\n49.9\n50.9\n64.6\nLLM-R\n52.0\n83.7\n71.2\n76.8\n73.3\n62.2\n94.2\n50.7\n52.0\n68.8\u21914.2\ngpt-35-turbo\u2020\nBM25\n75.3\n85.2\n65.0\n78.1\n78.0\n84.4\n95.7\n51.9\n52.8\n74.7\nLLM-R\n79.3\n86.7\n63.8\n79.6\n76.0\n84.0\n95.4\n52.2\n53.0\n75.1\u21910.4\nTable 4: Generalization to LLMs that are not used for training. \u2020: Since the official API of gpt-35-turbo does\nnot return the log-probabilities, we use different input-output templates to formulate all tasks as text generation.\nConsequently, the scores of gpt-35-turbo cannot be directly compared with those of other LLMs. More details are\nin Appendix B.\nsnli\nag_news\nmrpc\nmnli_m\nmnli_mm\nqqp\nnq\nqnli\nrte\naeslc\nsst2\ndart\nwsc273\nboolq\ncopa\nyelp\narc_c\narc_e\nmultirc\nwsc\npiqa\nsent140\ngigaword\nobqa\ne2e_nlg\nhellaswag\ncom_gen\nwinog\npaws\nsquad_v1\n0\n10\n20\n30\n40\naverage gain\nNot Knowledge-intensive\nKnowledge-intensive\nFigure 3: Performance gains of LLM-R over the random selection baseline. The selected knowledge-intensive tasks\nare NQ, ARC (easy and challenge), PIQA, HellaSwag, COPA, Paws, OpenBook QA, WSC273, WSC, Winogrande,\nand MultiRC.\nto tens of billions.\nNotably, the gains are par-\nticularly significant for small-size language mod-\nels, possibly because they are less powerful and\nthus require higher-quality examples to perform\nin-context learning.\n6.3\nWhen does LLM-R Work and When Does\nit Not?\nReporting a single aggregate score for all tasks\nfacilitates comparison across different model vari-\nants. However, this approach hides the fact that\nLLM-R performs better on certain tasks than oth-\ners, and may even lead to performance degrada-\ntion in some cases. In Figure 3, we partition the\ntasks into two groups. A task is considered to be\nknowledge-intensive if solving this task requires\ncommonsense, complex reasoning, or memorized\nfactual knowledge.\nFor tasks in the knowledge-intensive set, the\nabsolute improvements are substantially smaller\nthan the average, with NQ being the only excep-\ntion. This is not surprising, as these tasks rely more\nheavily on the underlying foundation model\u2019s ca-\npability to perform reasoning and knowledge mem-\norization. For the NQ dataset, we empirically find\nthat there is some overlap between the training and\ntest sets, where test questions are paraphrases of\nsome training questions. Despite this, we decide\nto keep the NQ dataset in our evaluation, as it is\na widely used benchmark and the remaining non-\noverlapping questions are still valuable.\nAnother noticeable case is the SQuAD v1 dataset\n(Rajpurkar et al., 2016), where LLM-R performs\nworse than the random selection baseline. Upon\nmanual inspection, we find that many questions in\nSQuAD share the same passage as the context. This\nfrequently results in LLM-R retrieving examples\nwith limited diversity, which may account for the\nobserved decline in performance.\nIn Table 5, for the Sentiment140 and MNLI\ndatasets, our model helps by retrieving examples\nthat share similar input patterns with the test ex-\nample. In contrast, the PIQA dataset requires com-\nmonsense knowledge and may not benefit much\nTask name\nSentiment140\nTest Input\nMath review. Im going to fail the exam. What is the sentiment of this tweet?\nTest Answer\nNegative\nLLM-R\nrevising for maths exam on tuesday which im gonna fail badly What is the sentiment of this tweet? Negative\nTask name\nMNLI-m\nTest Input\nPremise: \"Part 2), Confidentiality of Alcohol and Drug Abuse Patient Records.\" Hypothesis: \"Drug and alcohol patient\nrecords should be confidential\" Does the premise entail the hypothesis? Yes, No, or Maybe?\nTest Answer\nYes\nLLM-R\nPremise: \"Eligible Clients unable to attain needed legal assistance\" Hypothesis: \"Clients that should have received legal\nassistance but didn\u2019t\" Does the premise entail the hypothesis? Yes, No, or Maybe? Yes\nTask name\nPIQA\nTest Input\nHere is a goal: \"How can I keep a bathroom mirror from fogging up?\" How would you accomplish this goal?\nTest Answer\nWipe down with shaving cream.\nLLM-R\nHere is a goal: \"how do you \u2019clean up\u2019 an eyebrow you\u2019ve filled in?\" How would you accomplish this goal? use concealer\nto cover up any mistakes made.\nTable 5: Retrieved examples by LLM-R. The bold texts are the groundtruth answers for the test inputs and retrieved\ncandidates. More examples are available in Table 11.\nfrom the retrieved examples.\n6.4\nUsing Different LLMs for Data\nGeneration and Task Evaluation\nRank LLM \u2192\nEval LLM \u2193\nGPT-Neo-2.7B\nLLaMA-7B\nBoth\nGPT-Neo-2.7B\n61.7\n61.3\n61.6\nLLaMA-7B\n66.0\n65.7\n66.3\nTable 6: On the impacts of using different LLMs for\ncandidate ranking and task evaluation. The \u201cBoth\u201d set-\nting merges the training data from two LLMs.\nOne crucial aspect of our framework is the se-\nlection of the LLM for training data generation\nand task evaluation. During the training phase, the\nLLM plays a pivotal role in ranking the retrieved\ncandidates and providing supervision signals for\nthe reward model. In the task evaluation phase, the\nLLM is used to generate the final predictions.\nWe\nexperiment\nwith\nGPT-Neo-2.7B\nand\nLLaMA-7B. Table\n6 shows the results under\ndifferent combinations of LLMs for training and\nevaluation.\nWe observe that the quality of the\nevaluation LLM is the primary determinant for\nthe final performance, while the choice of ranking\nLLM has a relatively minor impact.\nAlthough\nmerging the training data from two LLMs yields\nthe best overall performance, we do not employ\nthis technique in our main experiments for the sake\nof simplicity.\n6.5\nScaling the Number of In-Context\nExamples and Retriever Size\nIn Figure 4, we investigate the scaling effect of\nLLM-R from two aspects: the number of in-context\n1\n2\n4\n8\n16\n# of In-Context Examples (log-scale)\n60\n62\n64\n66\nAverage Score\ngpt-neo-2.7b\nllama-7b\nsmall\nbase\nlarge\nRetriever Model Size\n62\n64\n66\n68\nAverage Score\ngpt-neo-2.7b\nllama-7b\nFigure 4: The scaling effect with respect to the number\nof in-context examples and retriever size. Our main\nexperiments use 8 in-context examples and base-size\nretriever. We vary the retriever model size by initializing\nwith the released E5-{small, base, large} checkpoints\nfrom Wang et al..\nexamples and the retriever model size. The overall\nperformance improves as we increase the number\nof retrieved examples, but the gains diminish after\n4 examples. Including more examples usually leads\nto longer prompts and higher inference cost.\nWith regard to the retriever size, we observe\nthat the small-size model produces comparable re-\nsults with the base-size one, whereas the large-size\nretriever exhibits a more substantial performance\nboost. The trends are consistent for the two exam-\nined language models. Practitioners can select the\nappropriate configurations based on the trade-off\nbetween performance and computational cost.\n7\nConclusion\nIn this paper, we introduce an iterative training\nframework named LLM-R to retrieve high-quality\nin-context examples for large language models.\nThis framework generates training data by utiliz-\ning a frozen LLM to rank the top retrieved can-\ndidates, and then learns a cross-encoder based re-\nward model to capture the ranking preference. Bi-\nencoder based dense retrievers are trained to distill\nthe knowledge from the reward model. We conduct\na comprehensive evaluation of LLM-R on a diverse\nset of tasks and demonstrate that it consistently\noutperforms various strong baselines. Our model\nalso generalizes well to held-out tasks and LLMs\nof varying sizes.\nLimitations\nIn our framework, we treat each candidate exam-\nple independently and retrieve the top-k results for\neach test example. This may be suboptimal as the\nin-context examples can influence each other. In-\ncorporating the techniques from the field of combi-\nnatorial optimization and sequential decision mak-\ning can be a promising direction to explore.\nAnother limitation of our study is related to the\nautomatic evaluation protocol. To compare the per-\nformance of different methods, we report the arith-\nmetic mean of the metrics over all tasks. However,\nthis may put generation tasks at a disadvantage\nsince metrics like ROUGE and BLEU typically\nhave a narrower range of variation compared to\nclassification accuracy. Moreover, the simple arith-\nmetic mean does not account for the quality of each\ndataset.\nAcknowledgements\nWe would like to thank anonymous reviewers for\ntheir valuable comments, and EACL 2024 and ACL\nRolling Review organizers for their efforts.\nReferences\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. The fifth pascal recognizing textual\nentailment challenge.\nSumithra Bhakthavatsalam, Daniel Khashabi, Tushar\nKhot, Bhavana Dalvi Mishra, Kyle Richardson,\nAshish Sabharwal, Carissa Schoenick, Oyvind\nTafjord, and Peter Clark. 2021.\nThink you have\nsolved direct-answer question answering? try arc-\nda, the direct-answer ai2 reasoning challenge. ArXiv\npreprint, abs/2102.03315.\nYonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng\nGao, and Yejin Choi. 2020. PIQA: reasoning about\nphysical commonsense in natural language. In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 7432\u2013\n7439. AAAI Press.\nSid Black, Gao Leo, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021.\nGPT-Neo:\nLarge\nScale Autoregressive Language Modeling with Mesh-\nTensorflow.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n2022. Improving language models by retrieving from\ntrillions of tokens. In International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA, volume 162 of Proceedings\nof Machine Learning Research, pages 2206\u20132240.\nPMLR.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632\u2013642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey E. Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 1597\u20131607. PMLR.\nDaixuan Cheng, Shaohan Huang, Junyu Bi, Yu-Wei\nZhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei,\nDenvy Deng, and Qi Zhang. 2023. Uprise: Universal\nprompt retrieval for improving zero-shot evaluation.\nArXiv preprint, abs/2303.08518.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2924\u20132936, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and\nChristopher D. Manning. 2020.\nELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators.\nIn 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui,\nand Furu Wei. 2022. Why can gpt learn in-context?\nlanguage models secretly perform gradient descent\nas meta optimizers. ArXiv preprint, abs/2212.10559.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nOnd\u02c7rej Du\u0161ek, David M. Howcroft, and Verena Rieser.\n2019. Semantic noise matters for neural natural lan-\nguage generation. In Proceedings of the 12th Interna-\ntional Conference on Natural Language Generation,\npages 421\u2013426, Tokyo, Japan. Association for Com-\nputational Linguistics.\nAlec Go, Richa Bhayani, and Lei Huang. 2009. Twit-\nter sentiment classification using distant supervision.\nCS224N project report, Stanford, 1(12):2009.\nYaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yux-\nian Gu, and Furu Wei. 2022. Structured prompting:\nScaling in-context learning to 1,000 examples. ArXiv\npreprint, abs/2212.06713.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769\u20136781,\nOnline. Association for Computational Linguistics.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 252\u2013262, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452\u2013466.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. In Thir-\nteenth international conference on the principles of\nknowledge representation and reasoning.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih,\nTim Rockt\u00e4schel, Sebastian Riedel, and Douwe\nKiela. 2020.\nRetrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nXiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu,\nYuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng\nQiu. 2023. Unified demonstration retriever for in-\ncontext learning. ArXiv preprint, abs/2305.04320.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823\u20131840,\nOnline. Association for Computational Linguistics.\nJimmy J. Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\nHong Yang, Ronak Pradeep, Rodrigo Nogueira, and\nDavid R. Cheriton. 2021. Pyserini: A python toolkit\nfor reproducible information retrieval research with\nsparse and dense representations. Proceedings of\nthe 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022.\nWhat\nmakes good in-context examples for GPT-3?\nIn\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100\u2013114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nMan Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat,\nMehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and\nVincent Y Zhao. 2023.\nDr. icl: Demonstration-\nretrieved in-context learning.\nArXiv preprint,\nabs/2305.14128.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2381\u20132391, Brussels, Belgium. Association\nfor Computational Linguistics.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048\u201311064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021.\nWebgpt: Browser-assisted question-\nanswering with human feedback. ArXiv preprint,\nabs/2112.09332.\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit\nRau, Abhinand Sivaprasad, Chiachun Hsieh, Xi-\nangru Tang, Aadit Vyas, Neha Verma, Pranav Kr-\nishna, Yangxiaokang Liu, Nadia Irwanto, Jessica\nPan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma,\nYasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan,\nXi Victoria Lin, Caiming Xiong, Richard Socher,\nand Nazneen Fatema Rajani. 2021. DART: Open-\ndomain structured data record to text generation. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 432\u2013447, Online. Association for Computa-\ntional Linguistics.\nCourtney Napoles, Matthew Gormley, and Benjamin\nVan Durme. 2012. Annotated Gigaword. In Proceed-\nings of the Joint Workshop on Automatic Knowledge\nBase Construction and Web-scale Knowledge Ex-\ntraction (AKBC-WEKEX), pages 95\u2013100, Montr\u00e9al,\nCanada. Association for Computational Linguistics.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don\u2019t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784\u2013789,\nMelbourne, Australia. Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383\u20132392, Austin,\nTexas. Association for Computational Linguistics.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. ArXiv preprint, abs/2302.00083.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982\u20133992, Hong Kong, China. Association for Com-\nputational Linguistics.\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,\nQiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong\nWen. 2021. RocketQAv2: A joint training method\nfor dense passage retrieval and passage re-ranking.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2825\u20132835, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011.\nChoice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In 2011 AAAI Spring Symposium Series.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655\u20132671, Seattle, United States.\nAssociation for Computational Linguistics.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2020. Winogrande: An adver-\nsarial winograd schema challenge at scale. In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 8732\u2013\n8740. AAAI Press.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models.\nArXiv\npreprint, abs/2301.12652.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631\u20131642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur\u2019elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. ArXiv\npreprint, abs/2302.13971.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text embeddings by weakly-\nsupervised contrastive pre-training. ArXiv preprint,\nabs/2212.03533.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V. Le. 2022.\nFinetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112\u20131122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\nand Tengyu Ma. 2022. An explanation of in-context\nlearning as implicit bayesian inference. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net.\nJiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and\nLingpeng Kong. 2023. Compositional exemplars for\nin-context learning. ArXiv preprint, abs/2302.05698.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-\nchine really finish your sentence? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4791\u20134800, Florence,\nItaly. Association for Computational Linguistics.\nRui Zhang and Joel Tetreault. 2019. This email could\nsave your life: Introducing the task of email subject\nline generation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 446\u2013456, Florence, Italy. Association\nfor Computational Linguistics.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems 28: Annual Conference on Neural In-\nformation Processing Systems 2015, December 7-12,\n2015, Montreal, Quebec, Canada, pages 649\u2013657.\nYiming Zhang, Shi Feng, and Chenhao Tan. 2022. Ac-\ntive example selection for in-context learning. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 9134\u2013\n9148, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\nPAWS: Paraphrase adversaries from word scrambling.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 1298\u20131308,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nA\nImplementation Details\nRetriever\nReward Model\ninitialization\nE5base\nELECTRAbase\nlearning rate\n3 \u00d7 10\u22125\n10\u22125\n# of GPUs\n8\n8\nbatch size\n256\n128\ntrain steps\n6k\n3k\n\u03c4\n0.01\nn.a.\n\u03b1\n0.2\nn.a.\npositive examples\ntop 3\nbottom 16\nnegative examples\ntop 3\nbottom 16\n# of negatives\n3\n7\nranking depth\n100\n100\ninput length\n256\n384\nTable 7: Hyperparameters for training the bi-encoder\nretriever and reward model. We use the same hyperpa-\nrameters for every iteration.\nDataset name\nCategory\n# train\n# test\nMetric\nHeld-out?\nAESLC (Zhang and Tetreault, 2019)\nSummarize\n13,181\n1,750\nROUGE-L\nN\nAGNews (Zhang et al., 2015)\nSummarize\n120,000\n7,600\nAccuracy\nN\nARC Challenge (Bhakthavatsalam et al., 2021)\nClose QA\n1,117\n1,165\nAccuracy\nN\nARC Easy (Bhakthavatsalam et al., 2021)\nClose QA\n2,241\n2,365\nAccuracy\nN\nBoolQ (Clark et al., 2019)\nReading Comp.\n9,427\n3,270\nAccuracy\nN\nCommonGen (Lin et al., 2020)\nData-to-text\n67,389\n4,018\nROUGE-L\nN\nCOPA (Roemmele et al., 2011)\nCommonsense\n400\n100\nAccuracy\nN\nDART (Nan et al., 2021)\nData-to-text\n62,659\n2,768\nROUGE-L\nN\nE2E NLG (Du\u0161ek et al., 2019)\nData-to-text\n33,525\n1,847\nROUGE-L\nN\nGigaword (Napoles et al., 2012)\nSummarize\n2,044,465\n730\nROUGE-L\nN\nHellaSwag (Zellers et al., 2019)\nCommonsense\n39,905\n10,042\nAccuracy\nN\nMNLI (m) (Williams et al., 2018)\nNLI\n392,702\n9,815\nAccuracy\nN\nMNLI (mm) (Williams et al., 2018)\nNLI\n392,702\n9,832\nAccuracy\nN\nMRPC (Dolan and Brockett, 2005)\nParaphrase\n3,668\n408\nAccuracy\nN\nMultiRC (Khashabi et al., 2018)\nReading Comp.\n27,243\n4,848\nF1\nN\nNQ (Kwiatkowski et al., 2019)\nClose QA\n87,925\n3,610\nExact Match\nN\nOpenBook QA (Mihaylov et al., 2018)\nReading Comp.\n4,957\n500\nAccuracy\nN\nPAWS (Zhang et al., 2019)\nParaphrase\n49,401\n8,000\nAccuracy\nN\nPIQA (Bisk et al., 2020)\nCommonsense\n16,113\n1,838\nAccuracy\nY\nQNLI (Rajpurkar et al., 2018)\nNLI\n104,743\n5,463\nAccuracy\nY\nQQP (Wang et al., 2019)\nParaphrase\n363,846\n40,430\nAccuracy\nN\nRTE (Bentivogli et al.)\nNLI\n2,490\n277\nAccuracy\nN\nSentiment140 (Go et al., 2009)\nSentiment\n1,600,000\n359\nAccuracy\nN\nSNLI (Bowman et al., 2015)\nNLI\n549,367\n9,824\nAccuracy\nN\nSQuAD v1 (Rajpurkar et al., 2016)\nReading Comp.\n87,599\n10,570\nExact Match\nN\nSST2 (Socher et al., 2013)\nSentiment\n67,349\n872\nAccuracy\nN\nWinogrande (Sakaguchi et al., 2020)\nCoreference\n40,398\n1,267\nAccuracy\nN\nWSC (Levesque et al., 2012)\nCoreference\n554\n104\nAccuracy\nN\nWSC273 (Levesque et al., 2012)\nCoreference\n0\n273\nAccuracy\nY\nYelp (Zhang et al., 2015)\nSentiment\n490,456\n33,285\nAccuracy\nY\nTotal\nn.a.\n6.3M\n177k\nn.a.\nn.a.\nTotal (sampled)\nn.a.\n591k\n123k\nn.a.\nn.a.\nTable 8: Statistics for the datasets used in this paper.\nThe hyperparameters for the retriever model\nand reward model are summarized in Table 7.\nThe E5base checkpoint is available at https://\nhuggingface.co/intfloat/e5-base-v2.\nThis\ncheckpoint is also employed for the k-means clus-\ntering baseline, where we select 8 examples closest\nto each cluster center as the in-context examples.\nFor each iteration, we employ LLaMA-7B to rank\nthe top-100 retrieved candidates. As we retrieve\nfrom a unified pool of examples, it is possible that\na candidate comes from a different task than the\nquery. In this case, we assign a low score to it.\nDuring the evaluation, we retrieve top-8 candi-\ndates and use them as in-context examples. The\nmaximum input length for LLaMA-7B is set to\n1024. Longer inputs are truncated from the left\nside. The maximum output length is set to 64.\nThe most time-consuming part in our pipeline is\nranking candidates with LLaMA-7B, which takes\nabout 12 hours for 200k examples with 8 V100\nGPUs. Training the retriever model and reward\nmodel takes less than 10 hours in total.\nB\nEvaluation with GPT-35-Turbo\nDue to quota limits, we sample at most 1k examples\nfor each dataset. As GPT-35-Turbo does not return\ntoken-level log-probabilities, we cannot evaluate\nthe multiple-choice datasets by computing the log-\nlikelihood of each option. Instead, we append all\nthe options to the end of the input, and let the model\ngenerate the option index. An example is shown in\nTable 9. We also tried using this format to LLaMA-\n7B, but the performance is significantly worse than\ncomparing the log-likelihood of each option.\nFor a small number of test examples, GPT-35-\nTurbo fails to follow the patterns of in-context ex-\namples and generates outputs that are not valid\nclass labels. We add some simple heuristics based\non string matching to determine the model predic-\ntion.\nInput\nWhat happens next in this paragraph? How to survive remedial classes Look at the course as an opportunity.\nMany students are discouraged when they are assigned to a remedial class. Some assume this placement\nmeans they aren\u2019t ready for college. OPTIONS:\nA) However, people who are not unable to do what they\u2019re given on campus, or those who are cut out\nfrom college academies, are likely to have some little snitches. You want to be prepared for a negative\noutcome if possible.\nB) In this case, you should consider what you will do if your subject consists of a certain term or number\nof subject areas. You could set up a study study program yourself or tutor a student who is struggling to\nthoroughly comprehend where they sat for homework.\nC) If you take the course, you might find you feel highly motivated after passing the test. Try to develop a\npositive attitude towards the course so that you are not discouraged when you take your homework at the\nend of the day.\nD) However, being assigned a remedial class doesn\u2019t mean that you are behind, just that you have an\nopportunity to receive better instruction and improve your skills in a subject that you have struggled\nwith in the past. There is nothing unusual about being asked to attend a remedial course: two thirds of\ncommunity college students take at least one remedial course.\nOutput\nD\nTable 9: Input-output format for GPT-35-Turbo. This example is from the HellaSwag dataset. We add some line\nbreaks for better readability.\nTask\nZero-shot\nRandom\nKmeans\nBM25\nE5base\nSBERT\nEPR\nLLM-R\n1 iter\n2 iter\n3 iter\nAESLC\n5.8\n19.4\n19.0\n26.8\n27.0\n25.3\n26.0\n26.7\n27.3\n27.1\nAGNews\n31.5\n67.4\n71.9\n90.6\n90.6\n90.2\n91.8\n92.4\n93.5\n93.5\nARC Chall.\n35.6\n39.7\n40.5\n40.3\n44.6\n42.8\n43.0\n43.4\n43.6\n44.0\nARC Easy\n51.0\n60.0\n61.8\n59.9\n63.0\n63.1\n63.1\n63.6\n63.3\n63.6\nBoolQ\n64.7\n70.0\n69.0\n74.7\n72.4\n73.9\n74.8\n75.6\n75.1\n74.1\nCommonGen\n19.2\n36.3\n34.4\n37.6\n37.4\n37.6\n39.2\n38.2\n37.7\n37.3\nCOPA\n66.0\n80.0\n85.0\n78.0\n83.0\n82.0\n82.0\n84.0\n84.0\n84.0\nDART\n22.9\n52.0\n46.6\n55.9\n54.7\n54.4\n56.2\n57.3\n57.2\n57.3\nE2E NLG\n34.6\n52.7\n46.4\n54.5\n51.8\n50.2\n53.6\n54.9\n54.7\n54.9\nGigaword\n15.3\n30.0\n30.7\n32.7\n32.5\n32.6\n32.4\n33.3\n32.5\n31.8\nHellaSwag\n71.5\n73.9\n74.0\n74.9\n75.2\n75.3\n75.2\n75.4\n75.5\n75.4\nMNLI (m)\n35.8\n46.3\n44.2\n50.1\n44.5\n50.8\n59.9\n68.2\n70.2\n69.8\nMNLI (mm)\n35.6\n48.1\n45.4\n48.3\n44.7\n49.3\n61.5\n69.5\n72.0\n71.3\nMRPC\n69.1\n49.5\n38.0\n61.8\n41.2\n52.7\n55.9\n62.3\n75.3\n78.2\nMultiRC\n57.0\n48.5\n34.1\n54.2\n56.0\n55.3\n50.4\n52.9\n51.5\n52.1\nNQ\n0.3\n21.5\n22.6\n37.6\n39.3\n39.4\n39.2\n39.4\n39.1\n39.2\nOpenBook QA\n41.6\n49.8\n49.0\n49.6\n51.4\n51.4\n49.6\n50.8\n52.2\n53.4\nPAWS\n53.2\n57.0\n56.6\n56.6\n55.4\n58.2\n57.7\n57.0\n56.6\n57.0\nPIQA\n77.0\n79.1\n79.4\n81.3\n81.3\n80.7\n80.5\n80.9\n81.6\n80.6\nQNLI\n49.2\n56.4\n53.4\n62.2\n61.5\n61.9\n65.0\n74.4\n69.6\n69.4\nQQP\n57.7\n63.4\n63.3\n79.8\n77.5\n81.3\n81.7\n80.1\n82.6\n83.3\nRTE\n59.6\n59.9\n58.5\n65.7\n63.9\n67.2\n66.8\n67.2\n68.6\n70.4\nSentiment140\n49.3\n88.6\n89.4\n90.8\n93.9\n92.2\n91.4\n90.8\n91.1\n90.3\nSNLI\n39.8\n43.7\n52.5\n47.1\n53.5\n58.4\n68.4\n80.2\n82.0\n82.2\nSQuAD v1\n2.1\n64.1\n62.3\n61.2\n60.8\n61.6\n64.3\n60.7\n57.3\n52.5\nSST2\n54.4\n85.9\n89.7\n84.4\n92.1\n87.6\n88.7\n94.0\n93.8\n93.1\nWinogrande\n62.0\n66.7\n66.5\n67.5\n66.9\n66.5\n66.5\n67.9\n68.1\n67.2\nWSC\n64.4\n60.6\n56.7\n56.7\n61.5\n63.5\n61.5\n60.6\n63.5\n66.4\nWSC273\n74.0\n74.4\n74.7\n64.5\n65.2\n62.6\n65.2\n74.4\n79.5\n78.8\nYelp\n47.9\n92.0\n93.5\n93.5\n97.3\n95.9\n95.1\n95.7\n95.9\n95.5\nAverage\n44.9\n57.9\n57.0\n61.3\n61.4\n62.1\n63.5\n65.7\n66.5\n66.4\nTable 10: Detailed results for each dataset.\nTask Name\nAG News\nTest Input\n\"Holiday Shoppers Off to a Fast Start Holiday shoppers spent 10 percent more Friday than they did a year\nago, according to early reports, but Wal-Mart Stores Inc. dampened hopes for a strong start to the key\nretail season by \" What is this text about? World, Sports, Business, or Technology?\nTest Answer\nBusiness\nLLM-R Top 1\n\"Disappointing holiday news hurts retail shares Shares in a range of area retailers dipped Monday on\ndisappointing Thanksgiving sales data from Wal-Mart Stores Inc. In addition, ShopperTrak, which tallies\nsales results from 30,000 stores nationwide, said \" What is this text about? World, Sports, Business, or\nTechnology? Business\nTask name\nARC Challenge\nTest Input\nIn the 17th century, to estimate the distance to other planets, scientists first used the technique of viewing\nthe planet from two different locations on Earth\u2019s surface. Which characteristic of the planet were the\nscientists using to calculate the distance from Earth?\nTest Answer\nlocation\nLLM-R Top 1\nWhich physical characteristic of Earth is similar to a physical characteristic of the Moon? its mountain\nranges\nTask name\nARC Easy\nTest Input\nWhat is the major cause of seasonal changes?\nTest Answer\ntilt of the Earth\u2019s axis\nLLM-R Top 1\nWhich occurs as a result of Earth\u2019s tilt on its rotating axis? seasonal changes in the climate\nTask name\nCommonGen\nTest Input\nConcepts: field, throw, kid, bunch, ball. Write a sentence that includes all these words.\nTest Answer\nA bunch of kids are running around and throwing a ball on a field.\nLLM-R Top 1\nConcepts: look, ball, lot. Write a sentence that includes all these words. Two babies look up while they\nare playing in a playpen with a lot of balls.\nTask name\nCOPA\nTest Input\n\"The boy skipped dinner.\" What is the cause?\nTest Answer\nHe ate a big lunch.\nLLM-R Top 1\n\"The parents left their children with a babysitter.\" What is the cause? They made plans to celebrate\ntheir anniversary.\nTask name\nDART\nTest Input\nTriple: The Mill, eatType, coffee shop; The Mill, food, Chinese; The Mill, priceRange, moderate; The\nMill, area, city centre; The Mill, near, The Sorrento What is a sentence that describes this triple?\nTest Answer\nThere is a coffee shop serving Chinese food called The Mill. It has a moderate price range is is find\nin the city centre near The Sorrento.\nLLM-R Top 1\nTriple: The Mill, eatType, coffee shop; The Mill, food, Indian; The Mill, priceRange, cheap; The Mill,\narea, riverside; The Mill, near, The Sorrento What is a sentence that describes this triple? The Mill coffee\nshop is located in the riverside area near The Sorrento. They serve Indian food at a cheap price.\nTask name\nGigaword\nTest Input\nWrite a short summary for this text: the dollar and major european currencies traded within narrow ranges\non tuesday on the london forex market , which was waiting for the easter holiday weekend and for us\nemployment figures to be announced on friday , traders said in late afternoon .\nTest Answer\nlondon forex market stable as market waits for easter us data\nLLM-R Top 1\nWrite a short summary for this text: the dollar was stable over-all early monday afternoon by comparison\nwith morning levels on the london forex market , which was waiting for publication at the end of the week\nof us inflation figures , traders said . dollar stable in london as market waits for us inflation data\nTask name\nMRPC\nTest Input\nHere are two sentences: An episode is declared when the ozone reaches .20 parts per million parts of air\nfor one hour . A Stage 1 episode is declared when ozone levels reach 0.20 parts per million . Do they\nhave the same meaning?\nTest Answer\nYes\nLLM-R Top 1\nHere are two sentences: A Stage One alert is declared when ozone readings exceed 0.20 parts per million\nduring a one-hour period . A Stage 1 episode is declared when ozone levels reach 0.20 parts per million .\nDo they have the same meaning? Yes\nTask name\nNQ\nTest Input\nQuestion: legislation regarding data protection and security in uk? Answer:\nTest Answer\nThe Data Protection Act 1998\nLLM-R Top 1\nQuestion: which law relates to the protection of personal information? Answer: Data Protection Act\n1998\nTable 11: More retrieved examples. The format is the same as Table 5.\n"
  },
  {
    "title": "DreamTeacher: Pretraining Image Backbones with Deep Generative Models",
    "link": "https://arxiv.org/pdf/2307.07487.pdf",
    "upvote": "19",
    "text": "DreamTeacher: Pretraining Image Backbones with Deep Generative Models\nDaiqing Li 1\u2217\nHuan Ling1,2,3\u2217\nAmlan Kar1,2,3\nDavid Acuna1,2,3\nSeung Wook Kim1,2,3\nKarsten Kreis1\nAntonio Torralba4\nSanja Fidler1,2,3\n1NVIDIA\n2University of Toronto\n3Vector Institute\n4MIT\nProject page: https://research.nvidia.com/labs/toronto-ai/DreamTeacher/\nTrain a Generative Model of Choice\n2\n3\n1\nScore-based Generative Model\nGenerative Adversarial Networks \nImage\nBackbone\nDistill Knowledge from \nthe Generative Model\nTeacher\nStudent\nGenerative\nNetwork\nTrain Image Backbone on Downstream Tasks\nFigure 1. We propose DreamTeacher, a framework for distilling knowledge from a pre-trained generative network onto a target image\nbackbone, as a generic pre-training mechanism that doesn\u2019t require labels. We investigate feature distillation, and optionally label distillation\n(when task-specific labels are available). Our DreamTeacher outperforms existing self-supervised methods on a variety of benchmarks.\nAbstract\nIn this work, we introduce a self-supervised feature rep-\nresentation learning framework DreamTeacher that utilizes\ngenerative networks for pre-training downstream image\nbackbones. We propose to distill knowledge from a trained\ngenerative model into standard image backbones that have\nbeen well engineered for specific perception tasks. We investi-\ngate two types of knowledge distillation: 1) distilling learned\ngenerative features onto target image backbones as an al-\nternative to pretraining these backbones on large labeled\ndatasets such as ImageNet, and 2) distilling labels obtained\nfrom generative networks with task heads onto logits of target\nbackbones. We perform extensive analyses on multiple gener-\native models, dense prediction benchmarks, and several pre-\ntraining regimes. We empirically find that our DreamTeacher\nsignificantly outperforms existing self-supervised represen-\ntation learning approaches across the board. Unsupervised\nImageNet pre-training with DreamTeacher leads to signifi-\ncant improvements over ImageNet classification pre-training\non downstream datasets, showcasing generative models, and\ndiffusion generative models specifically, as a promising ap-\nproach to representation learning on large, diverse datasets\nwithout requiring manual annotation.\n1. Introduction\nSelf-supervised representation learning is becoming an ef-\nfective way of pre-training vision backbones [7,12,13,27,29].\n\u2217 Equal Contribution.\nThe premise of this line of work is to leverage large unla-\nbeled datasets as additional source of training data in order\nto boost performance of downstream networks, and to re-\nduce the need for large labeled target datasets. Recent works\nhave shown that self-supervised pre-training on ImageNet\ncan now come close to supervised pre-training, even outper-\nforming it on some downstream datasets and tasks such as\npixelwise semantic and instance segmentation [13,29,63].\nOne of the dominant approaches to self-supervised repre-\nsentation learning are variants of contrastive learning, where\nthe target backbone is trained to map transformed views\nof an image closer in latent space than images randomly\ndrawn from the dataset [12]. Improvements to this paradigm\ninclude introducing spatial losses [63, 70, 71, 73], and im-\nproving training stability with fewer or no negative exam-\nples [13,14,27,29].\nAnother line of work pursues reconstruction losses for\nsupervision, where certain regions get masked from an input\nimage, and backbones get trained to reconstruct them [21,\n28,64,72], also known as Masked Image Modeling (MIM).\nThis task is mostly treated as deterministic, ie supervising a\nsingle explanation for the masked region. This line of work\ntypically investigates masking strategies, architecture design\nand training recipes to train better backbones. These methods\nhave achieved state-of-the-art (SoTA) performance when\napplied to Vision Transformer-based backbones; however,\nrecently sparse CNN-based image backbones [58] have been\nshown to be as performant.\nIn this paper, we argue for generative models as repre-\nsentation learners: for the simplicity of the objective \u2013 to\n1\narXiv:2307.07487v1  [cs.CV]  14 Jul 2023\ngenerate data, and intuitive representational power \u2013 gen-\nerating high quality samples as an indication of learning\nsemantically capable internal representations. Using gen-\nerative networks as representation learners is not a novel\nconcept. DatasetGAN and variants [4, 40, 81] proposed to\nadd task-dependent heads on top of StyleGAN\u2019s or a diffu-\nsion model\u2019s features, and used these augmented networks as\ngenerators of labeled data, on which downstream networks\nare then trained. SemanticGAN [41] instead used StyleGAN\nwith an additional task decoder as the task network itself \u2013 by\nencoding images into the latent space of the generative model\nand using the task head for producing perception output.\nWe introduce DreamTeacher, a representation learning\nframework that leverages generative models for pre-training\ndownstream perception models via distillation. We inves-\ntigate two types of distillation: 1) feature distillation, where\nwe propose methods for distilling generative features to\ntarget backbones, as a general pre-training mechanism\nthat does not require any labels. 2) label distillation: using\ntask-heads on top of generative networks for distilling\nknowledge from a labeled dataset onto target backbones, in\na semi-supervised regime. We focus our work on diffusion\nmodels [35,54,56] and GANs [26,36,37] as the choice of\ngenerative models. For target backbones, we focus on CNNs,\nfor two major reasons. 1) CNN-based backbones have been\nshown to achieve SoTA representation learning performance\nfor both contrastive and MIM approaches [44, 58, 62, 66],\n2) SoTA generative models today (GANs and diffusion\nmodels) primarily still use CNNs internally. In preliminary\nexperiments, we also explored vision transformer backbones,\nbut found it challenging to distill features from CNN-based\ngenerative models into vision transformers. Generative\nmodels built with vision transformer architectures are\nnascent [2,48], and hence we leave a thorough exploration\nof DreamTeacher with these architectures to future work.\nWe experimentally show that DreamTeacher outperforms\nexisting self-supervised learning approaches on various\nbenchmarks and settings. Most notably, when pre-trained\non ImageNet without any labels, our method significantly\noutperforms methods that are pre-trained on ImageNet with\nfull supervision, on several dense prediction benchmarks\nand tasks such as semantic segmentation on ADE20K [83],\ninstance segmentation on MSCOCO [43] and on the au-\ntonomous driving dataset BDD100K [76]. On object-focused\ndatasets with millions of unlabeled images [77, 81], our\nmethod, when trained solely on the target domain, signif-\nicantly outperforms variants that are pre-trained on Ima-\ngeNet with label supervision, and achieves new SoTA re-\nsults. These results highlight generative models, especially\ndiffusion-based generative models [20,35,56], as powerful\nrepresentation learners that can effectively leverage diverse\nunlabeled datasets at scale.\n2. Related Work\nDiscriminative Representation Learning. Early represen-\ntation learning methods relied on handcrafted pretext tasks\nsuch as relative patch prediction [21], solving jigsaw puz-\nzles [47], colorization [80], and relative rotation [25]. Instead,\nour pretext task is to predict features of a pretrained genera-\ntive model, which in turn is trained with a simple and natural\nobjective: maximize the log likelihood of the image data.\nThe ability to synthesize and manipulate high quality sam-\nples is promising sign that generative networks learn both\nsemantic and geometric knowledge internally [81].\nRecent breakthroughs come from contrastive representa-\ntion learning methods [12,13,27]. SimCLR [12] was the first\nto show competitive results in linear probing and transfer\nlearning without using class labels, compared to supervised\npre-training. Follow-up works MoCo [29], MoCoV2 [13]\nand BYOL [27] improve over the siamese network design\nwith a memory bank and gradient stopping. However, these\nmethods rely on heavy data augmentation [69] and heuristics\nto select the negative examples. This may not generalize well\nto datasets beyond well-curated object-centric datasets like\nImagetNet [18].\nAnother line of work [32, 63, 71, 73] aims to improve\nover the global contrastive objective and focuses on region-\nbased features which are useful for dense prediction tasks.\ndenseCL [63] extends MoCoV2 [13] to predict auxiliary\ndense features, PixPro [71] extends BYOL [27] to have\npixel-wise consistency across two views, while DetCon [32]\nintroduces masked pooling to focus on object-wise features.\nHowever, these methods require special designs for certain\ntasks [70, 71], or additional heuristics for complex scene\ndatasets [32]. In our work, we focus on generative networks\nfor representation learning specifically focused on various\ndense prediction tasks.\nGenerative Representation Learning. The ideas of leverag-\ning generative models for learning representations for recog-\nnition tasks dates back to Hinton [33]. Recent works use\nadvanced generative models and techniques to develop repre-\nsentation learning methods. BiGAN [22] proposed to jointly\ntrain an encoder with adversarial training objective. Big-\nBiGAN [23] leveraged the advancement of BigGAN [5]\nand showed competitive linear probing results in ImageNet.\nMethods like iGPT [10] and VIM [78] pre-train large trans-\nformer networks with autoregressive generative pre-training\nobjectives , achieving compelling linear probing results on\nImagetNet, but they did not show results on dense prediction\ntasks. Furthermore, these methods train a single image back-\nbone with both discriminative and generative objectives and\nthus cannot leverage the specific designs for each.\nDatasetGAN [40,81] was among the first to show that a\npretrained GAN can significantly benefit perception tasks,\nespecially in the low labeled data regime. Specifically, the\nauthors added a task-specific head on top of StyleGAN and\n2\nencoder q\nencoder k\nContrastive Learning\ngradient\ngradient\n!\n\ud835\udc67\ngenerator\n\"\ud835\udc65\nimage \nbackbone\nregressor\nFeature Distillation\nmulti-select\nmulti-select\ngradient\nimage \nbackbone\nregressor\nFeature Distillation\nmulti-select\ngradient\ngenerator\nencoder\nmulti-select\ngenerative \nmodel  \nimage \nbackbone\nregressor\nInterpreter\nKnowledge Distillation\nmulti-select\nmulti-select\nlogit head\ngradient\n||f e\nl \u2212 f g\nl ||\n<latexit sha1_base64=\"E0SfbkY1QKXKC9HZRbiqYMqjD5g=\">AB+HicbVDLSsNAFJ34rPX\nRqEs3wSK4sSRV0GXRjcsK9gFtDJPpTt0MgkzE6Em/RI3LhRx6e482+ctFlo64F7OZxzL3Pn+DGjUtn2t7Gyura+sVnaKm/v7O5VzP2DtowSQaBFIhaJro8lMqhpahi0I0F4NBn0PHN7nfeQh\nacTv1SQGN8RDTgNKsNKSZ1ayLPDYA5zlfZhlnlm1a/YM1jJxClJFBZqe+dUfRCQJgSvCsJQ9x46Vm2KhKGEwLfcTCTEmYzyEnqYchyDdHb41DrRysAKIqGLK2um/t5IcSjlJPT1ZIjVSC56ufif\n>10tUcOWmlMeJAk7mDwUJs1Rk5SlYAyqAKDbRBNB9a0WGWGBidJZlXUIzuKXl0m7XnPOa/W7i2rjuoijhI7QMTpFDrpEDXSLmqiFCErQM3pFb8aT8WK8Gx/z0RWj2DlEf2B8/gD7+JNM</latexit\n||f e\nl \u2212 f g\nl ||\n<latexit sha1_base64=\"E0SfbkY1QKXKC9HZRbiqYMqjD5g=\">AB+HicbVDLSsNAFJ34rPXRqEs3wSK4sSRV0GXRjcsK9gFtDJPpTt0MgkzE6Em/RI3LhRx6e482+ctFlo64F7OZxzL3Pn+\njuoijhI7QMTpFDrpEDXSLmqiFCErQM3pFb8aT8WK8Gx/z0RWj2DlEf2B8/gD7+JNM</latexit>DGjUtn2t7Gyura+sVnaKm/v7O5VzP2DtowSQaBFIhaJro8lMqhpahi0I0F4NBn0PHN7nfeQhacTv1SQGN8RDTgNKsNKSZ1ayLPDYA5zlfZhlnlm1a/YM1jJxClJFBZqe+dUfRCQJgSvCsJQ9x46Vm2KhKGEwLfcTCTEmYzyEnqYchyDdHb41DrRysAKIqGLK2um/t5IcSjlJPT1ZIjVSC56ufif10tUcOWmlMeJAk7mDwUJs1Rk5SlYAyqAKDbRBNB9a0WGWGBidJZlXUIzuKXl0m7XnPOa/W7i2r\n||f e\nl \u2212 f g\nl ||\n<latexit sha1_base64=\"E0SfbkY1QKXKC9HZRbiqYMqjD5g=\">AB+HicbVDLSsNAFJ34rPXRqEs3wSK4sSRV0GXRjcsK9gFtDJPpTt0MgkzE6Em/RI3LhRx6e482+ctFlo64F7OZxzL3Pn+\nDGjUtn2t7Gyura+sVnaKm/v7O5VzP2DtowSQaBFIhaJro8lMqhpahi0I0F4NBn0PHN7nfeQhacTv1SQGN8RDTgNKsNKSZ1ayLPDYA5zlfZhlnlm1a/YM1jJxClJFBZqe+dUfRCQJgSvCsJQ9x46Vm2KhKGEwLfcTCTEmYzyEnqYchyDdHb41DrRysAKIqGLK2um/t5IcSjlJPT1ZIjVSC56ufif10tUcOWmlMeJAk7mDwUJs1Rk5SlYAyqAKDbRBNB9a0WGWGBidJZlXUIzuKXl0m7XnPOa/W7i2r\njuoijhI7QMTpFDrpEDXSLmqiFCErQM3pFb8aT8WK8Gx/z0RWj2DlEf2B8/gD7+JNM</latexit>\n\u2212pg log pe\n<latexit sha1_base64=\"8EP4hlIB6lU60ojhUh2MFW4pOR0=\">AB83icbVBNS8NAEJ3Ur1q/qh69LBbBiyWpgh6LXjxWsB/QhLDZbtKlm+yuxFK6d/w4kERr/4Zb/4bt20O2vpg4PHeDPzI\nsmZNq7ZTW1jc2t8rblZ3dvf2D6uFR4tcEdomgvVi7CmnGW0bZjhtCcVxWnEaTca3c387hNVmons0YwlDVKcZCxmBsr+RcyTHwuEiRDGlZrbt2dA60SryA1KNAKq1/+QJA8pZkhHGvd91xpglWhFOpxU/1RiMsIJ7Vua4ZTqYDK/eYrOrDJAsVC2MoPm6u+JCU61HqeR7UyxGeplbyb+5/VzE98E5bJ3NCMLBbFOUdGoFkAaMAUJYaPLcFEMXsrIkOsMDE2poNwVt+eZV0GnXvst54uKo1b4s\n4ynACp3AOHlxDE+6hBW0gIOEZXuHNyZ0X5935WLSWnGLmGP7A+fwBcH2RSw=</latexit>\nx\n<latexit sha1_base64=\"hL+FaLtOT9luwfLW3Ut08xl3Pcw=\">AB6HicbVDLTgJBEOzF+IL9ehlIjHxRHbRI9ELx4hkUcCGzI79MLI\nmlT8JY2ZKGzNXfExMaT2OAtsZUTPUy95M/M/rpCa89idcJqlByRaLwlQE5PZ16TPFTIjxpZQpri9lbAhVZQZm03BhuAtv7xKmpWyd1Gu1C9L1ZsjycwCmcgwdXUIU7qEDGCA8wyu8OQ/Oi/PufCxac042cwx/4Hz+AOeHjQA=</latexit>7OxmZtZICF/gxYPGePWTvPk3DrAHBSvpFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsG4EthOFNAoEtoLR7cxvPaLSPJb3ZpygH9GB5CFn1Fip/tQrltyOwdZJV5GSpCh1it+dfsxSyOUhgmqdcdzE+NPqDKcCZwWuqnGhLIRHWDHUkj1P5kfuiUnF\nx\n<latexit sha1_base64=\"hL+FaLtOT9luwfLW3Ut08xl3Pcw=\">AB6HicbVDLTgJBEOzF+IL9ehlIjHxRHbRI9ELx4hkUcCGzI79MLI\n7OxmZtZICF/gxYPGePWTvPk3DrAHBSvpFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsG4EthOFNAoEtoLR7cxvPaLSPJb3ZpygH9GB5CFn1Fip/tQrltyOwdZJV5GSpCh1it+dfsxSyOUhgmqdcdzE+NPqDKcCZwWuqnGhLIRHWDHUkj1P5kfuiUnF\nmlT8JY2ZKGzNXfExMaT2OAtsZUTPUy95M/M/rpCa89idcJqlByRaLwlQE5PZ16TPFTIjxpZQpri9lbAhVZQZm03BhuAtv7xKmpWyd1Gu1C9L1ZsjycwCmcgwdXUIU7qEDGCA8wyu8OQ/Oi/PufCxac042cwx/4Hz+AOeHjQA=</latexit>\nxq\n<latexit sha1_base64=\"OaxdN\nNBlgRshEI6ialpmviL/ZD8=\">AB6nicbVDLTgJBEOz1ifhCPXqZS\nEw8kV0SPRi0eM8khgJbNDAxNmZ9eZWSPZ8AlePGiMV7/Im3/jAH\ntQsJOKlXd6e4KYsG1cd1vZ2l5ZXVtPbeR39za3tkt7O3XdZQohjUW\niUg1A6pRcIk1w43AZqyQhoHARjC8mviNR1SaR/LOjGL0Q9qXvMcZNV\na6fbp/6BSKbsmdgiwSLyNFyFDtFL7a3YglIUrDBNW65bmx8VOqDGc\nCx/l2ojGmbEj72LJU0hC1n05PHZNjq3RJL1K2pCFT9fdESkOtR2FgO\n0NqBnrem4j/ea3E9C78lMs4MSjZbFEvEcREZPI36XKFzIiRJZQpbm\n8lbEAVZcamk7chePMvL5J6ueSdlso3Z8XKZRZHDg7hCE7Ag3OowDVU\n>oQYM+vAMr/DmCOfFeXc+Zq1LTjZzAH/gfP4AbHmN4w=</latexit\nxk\n<latexit sha1_base64=\"oVHqgwe123sSDTGnJvJAgz60=\">A\nAB6nicbVDLTgJBEOzF+IL9ehlIjHxRHbRI9ELx4xyiOBlcwODUyYnd3MzBrJhk/w4kFjvPpF3vwbB9iDgpV0UqnqTndXEAujet+O7mV1\nbX1jfxmYWt7Z3evuH/Q0FGiGNZJCLVCqhGwSXWDTcCW7FCGgYCm8Hoeuo3H1FpHsl7M47RD+lA8j5n1Fjp7ulh1C2W3LI7A1kmXkZKkKHW\nLX51ehFLQpSGCap123Nj46dUGc4ETgqdRGNM2YgOsG2pCFqP52dOiEnVumRfqRsSUNm6u+JlIZaj8PAdobUDPWiNxX/89qJ6V/6KZdxYlC\ny+aJ+IoiJyPRv0uMKmRFjSyhT3N5K2JAqyoxNp2BD8BZfXiaNStk7K1duz0vVqyOPBzBMZyCBxdQhRuoQR0YDOAZXuHNEc6L8+58zFtzTjZ\nzCH/gfP4AY2GN3Q=</latexit>\nq \u00b7 k\n<latexit sha1_base64=\"NcyDJS8oUpuy\n2F5l+vg4y3SuXBU=\">AB73icbVBNS8NAEJ34WetX1aOXxSJ4KkV9Fj04rGC/YA2l\nM1m0y7dbNLdiVBK/4QXD4p49e9489+4bXPQ1gcDj/dmJkXpFIYdN1vZ219Y3Nru7BT\n3N3bPzgsHR03TZJpxhskYluB9RwKRvoEDJ26nmNA4kbwXDu5nfeuLaiEQ94jlfkz\n7SkSCUbRSe9RlYJk2CuV3Yo7B1klXk7KkKPeK31w4RlMVfIJDWm47kp+hOqUTDJp8\nVuZnhK2ZD2ecdSRWNu/Mn83ik5t0pIokTbUkjm6u+JCY2NGceB7YwpDsyNxP/8zoZR\njf+RKg0Q67YlGUSYIJmT1PQqE5Qzm2hDIt7K2EDaimDG1ERuCt/zyKmlWK95lpfpw\nVa7d5nEU4BTO4AI8uIYa3EMdGsBAwjO8wpszcl6cd+dj0brm5DMn8AfO5w/Oc4/Q</l\natexit>\nz\n<latexit sha1_base64=\"VLEo6VgUnu2TnOxoOkqsMPXvyTo=\">AB6HicbVDLTgJBEOzF+IL9ehlIjHxRHbRI9ELx4hkUcCGzI79MLI7OxmZtYEC\nV/gxYPGePWTvPk3DrAHBSvpFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsG4EthOFNAoEtoLR7cxvPaLSPJb3ZpygH9GB5CFn1Fip/tQrltyOwdZJV5GSpCh1it+dfsxSyOUhgmqdcdzE+NPqDKcCZwWuqnGhLIRHWDHUkj1P5kfuiUnFmlT8JY2ZKGzNXfExMaT2OAtsZUT\nPUy95M/M/rpCa89idcJqlByRaLwlQE5PZ16TPFTIjxpZQpri9lbAhVZQZm03BhuAtv7xKmpWyd1Gu1C9L1ZsjycwCmcgwdXUIU7qEDGCA8wyu8OQ/Oi/PufCxac042cwx/4Hz+AOqPjQI=</latexit>\n(a)\n(b)\n(c)\n(d)\nFigure 2. Different representation learning approaches: (a) a representative discriminative pretraining using a siamese-based network and\ncontrastive loss, (b) our DreamTeacher generative pretraining framework when sampling examples from the generative model, (c) our\nDreamTeacher generative pretraining framework on encoded real data, (d) our mix distillation when a small number of labels are available\n(20-40 labeled data in our experiments). Multi-select means selecting features from different layers.\nsynthesized a labeled dataset for training downstream per-\nception networks. SemanticGAN [41] proposed to model\nthe joint distribution of images and labels. Inference was\nperformed by first encoding the test images into the latent\nspace of StyleGAN and then decoded the labels using the\ntask-head. DDPM-seg [4] followed this line of work but used\na denoising diffusion probabilistic model (DDPMs) instead\nof StyleGAN. In our paper, we continue this line of work but\nfocus on distilling knowledge from a pre-trained generative\nmodel, diffusion model specifically, to downstream image\nbackbones as a general way of pre-training. We provide an\nextensive evaluation of generative networks in the context of\nrepresentation learning on various benchmarks and tasks.\nKnowledge Distillation. Hinton et al [34] were first to pro-\npose knowledge distillation as an effective means of improv-\ning performance \u2013 with the idea of distilling logits from a\nlarge teacher network into a smaller student network. Fit-\nNets [51] proposed to mimick the teacher\u2019s intermediate fea-\nture activations as additional hints for the student network.\nFollow-up works try to utilize different forms of knowledge\nfrom the teacher network: spatially [79], channel-wisely [53],\nand from multi-levels [11]. Usually, the teacher and student\nnetworks share a similar training objective, the network ar-\nchitecture, and require labels to train the teacher network. In\nour work, our generative model is treated as a teacher, and is\ntrained without labels and the objective is not task-specific.\nOur student networks are image backbones of choice, which\nmight not share a similar architecture as the teacher.\n3. DreamTeacher Framework\nWe describe our DreamTeacher framework in the con-\ntext of two scenarios: unsupervised representation learning\nwhere no labels are available during pre-training, and semi-\nsupervised learning where a fraction of labels are available.\nWe utilize a trained generative model G and distill its\nlearned representation into a target image backbone f. Our\nrecipe for training f remains the same in both scenarios\nand choices of G and f. First, we create a feature dataset\nD = {xi, f g\ni }N\ni=1 of images xi and corresponding features\nf g\ni extracted from the generative model. Next, we train f us-\ning the dataset D by distilling features f g\ni into the intermedi-\nate features of f(xi). We focus on convolutional backbones\nf, leaving exploration into transformers for future work. We\ndrop subscript i for brevity from here on.\nIn Sec. 3.1, we describe the design of our unsupervised\ndistillation process. We tackle the semi supervised regime\nin Sec. 3.2, where labels are available on a fraction of the\npre-training dataset.\n3.1. Unsupervised Representation Learning\nFor unsupervised representation learning given a feature\ndataset D, we attach feature regressors at different hierar-\nchical levels of the backbone f to regress the corresponding\ngenerative features f g\ni from an image xi. We first discuss\ncreating a feature dataset, followed by the design of feature\nregressors and end by introducing our distillation objective.\nCreating a feature dataset D. Generative models give us\ntwo distinct ways of creating our desired feature dataset D.\nOne could sample images from the generative model G and\nrecord intermediate features from the generative process. In\nprinciple, this could synthesize datasets of infinite size, but\nmay suffer from issues such as mode dropping, where the\ngenerative model may not have learned some parts of the\ndistribution sufficiently well. We refer to such a dataset as a\nsynthesized dataset. Instead, one could encode real images,\nlabeled or unlabeled, into the latent space of the generative\nmodel G, using an encoding process. We refer to such a\ndataset as an encoded dataset.\nA synthesized dataset D is created by sampling images\n\u02dcx \u223c G(z), where z is sampled from the generative model\nG\u2019s prior distribution. We record hierarchical intermediate\nfeatures from G(z) as f g = {f g\nl }L\nl=1 where l denotes the\nhierarchy level of the features from a total of L levels. We\nemploy this approach when using GANs [5,9,36] as G, due\nto their sampling speed, and inability to encode real images\nby design. Note that we are not concerned with bad samples,\ni.e. images with artifacts, as our main goal is to train the\nimage backbone f to map images into features, regardless of\nimage quality. This process is visualized in Fig. 2 (b). Also\nsee (a) for a side-by-side comparison of a representative\ndiscriminative pretraining paradigm.\nEncoded dataset is created by encoding a real image x\ninto the latent space of the generative model using an encod-\n3\nGenerator\nFeature Interpreter\nFeature Fuse Layer\nFeature Fuse Layer\nFeature Fuse Layer\nFeature Fuse Layer\n1x1 Conv\nDWS Conv\nConcat\ncur \nfeat\nprev\nfeat\nResize\nlatent feat\nPPM \nHead\nImage Backbone\n1/4\n1/8\n1/1\n6\n1/3\n2\nFeature Regressor\nR5\nR4\nR3\nR2\nEncoder\nlatent feat\nGenerative Model\nG5\nG4\nG3\nG2\nGenerator \nFeatures\nRegressed \nFeatures\nFeature Fuse Layer\nFeature \nRegression Loss\nLogit Head\nResize\nFuse\n1x1 \nConv\nLabel Distillation \nLoss\nReal Image\nTeacher Label Prediction\nStudent Label \nPrediction\nFeature Distillation\nLabel Distillation\nFigure 3. DreamTeacher architecture: Feature regression module (FR) maps and fuses multi-scale features of a (CNN) image backbone. We\nsupervise FR with features from the generator\u2019s decoding network. We optionally add a feature interpreter [81] to the generator to train a\ntask head with supervised labels \u2013 used to supervise the image backbone with label distillation loss.\ning process to get a latent variable \u02dcz. Then, we similarly run\nthe generative process and record hierarchical intermediate\nfeatures from G(\u02dcz) to obtain our dataset D. This process is\nvisualized in Fig. 2(c). Also see Fig. 7 for encoded ImageNet\nimages and their feature activation maps. For generative mod-\nels that come with an encoder network by design, such as\nVAEs [38,61], we can simply re-use it. For diffusion based\ngenerative models (DM) [20,35,56], which is the class of\ngenerative models we focus our investigation on, we use the\nforward diffusion process to encode a real image. Specifi-\ncally, we run forward diffusion for T steps, followed by a\nsingle denoising step to extract hierarchical features f g\nl from\nintermediate layers of the denoising network, typically a\nU-Net [52]. See Fig. 7 for visualization of feature activation\nmaps at different diffusion steps. The choice of T and the\nencoding process in diffusion models (stochastic [35] or de-\nterministic [55]) can strongly affect properties of the trained\nmodel f. We systematically ablate these choices through\nexperiments, and find that distilling stochastically encoded\nfeatures, which we view as data augmentation in feature\nspace, increases robustness of the downstream backbone f.\nBoth synthesized and encoded feature datasets can either\nbe pre-computed offline, or created online while training f.\nIn practice, we use online sampling for synthesized datasets,\nand online encoding for encoded datasets to allow fast in-\nmemory access and efficient materialization and removal of\nsamples and corresponding high dimensional features. This\nallows us to scale to pre-training with datasets and features\nf g of any size without additional pre-processing and storage\ncosts. Online encoding is also the natural choice when using\nstochastic encoding techniques in diffusion models, since an\noffline dataset could only store one or a few samples from\nall possible stochastic encodings of a real image.\nFeature Regressor. In order to distill generative represen-\ntations f g into a general backbone f, we design a feature\nregressor module that maps and aligns the image backbone\u2019s\nfeatures with the generative features. Inspired by the design\nof the Feature Pyramid Network (FPN) [42], our feature\nregressor takes multi-level features from the backbone f\nand uses a top-down architecture with lateral skip connec-\ntions to fuse the backbone features and outputs multi-scale\nfeatures. We apply a Pyramid Pooling Module (PPM) from\nPSPNet [82] similar to [68], on the last layer of the image\nbackbones before the FPN branch to enhance feature mixing.\nFig. 3 (bottom) visually depicts this architecture.\nFeature Distillation. Denote intermediate features from\nencoder f at different levels as {f e\n2, f e\n3, f e\n4, f e\n5}, and the\ncorresponding feature regressor outputs as {f r\n2 , f r\n3 , f r\n4 , f r\n5 }.\nWe use a 1 \u00d7 1 convolution to match the number of channels\nin f r\nl and f g\nl , if they are different. Our feature regression\nloss is simple and is inspired by FitNet [51], which proposed\ndistilling knowledge from a teacher onto a student network\nby mimicking intermediate feature activations:\nLMSE = 1\nL\nL\nX\nl\n\u2225f r\nl \u2212 W(f g\nl )\u22252\n2\n(1)\nHere, W is a non-learnable whitening operator implemented\nas LayerNorm [1], which normalizes differing feature mag-\nnitudes across layers. Layer number l = {2, 3, 4, 5} corre-\nsponds to features at 2l stride relative to the input resolution.\nAdditionally, we explore the activation-based Attention\nTransfer (AT) [79] objective. AT distills a one dimensional\n\u201cattention map\u201d per spatial feature, using an operator defined\nas F p\nsum(A) = PC\ni |Ai|p to sum the power p of the absolute\nvalues of the feature activation A across channel dimension\nC, which improves convergence speed over regressing high\ndimensional features directly. Specifically,\nLAT = 1\nL\nL\nX\nl\nX\nj\u2208I\n\u2225\nQr\nl,j\n\u2225Qr\nl,j\u22252\n\u2212\nQg\nl,j\n\u2225Qg\nl,j\u22252\n\u2225p\n(2)\nwhere Qr\nl,j = vec(F p\nsum(f r\nl,j)), Qg\nl,j = vec(F p\nsum(f g\nl,j))\nare respectively the j-th pair in layer l of the regressor\u2019s and\ngenerative model\u2019s features in vectorized form. We follow\n[79] to use p = 2 in our experiments.\nOur combined feature regression loss is:\nLfeat = LMSE + \u03bbAT LAT\n(3)\n4\nwhere \u03bbAT controls the weighting of the loss LAT . We\nchoose \u03bbAT = 10.0 in our experiments, to make the two\nlosses in the same scale. We empirically ablate choices of\nthe loss function and feature regressor designs.\n3.2. Label-Guided Representation Learning\nIn the semi-supervised setting, where a fraction of down-\nstream task labels are available for pre-training, we train a\ntask-dependent branch, called a feature interpreter, on top of\nthe frozen generative network G in a supervised manner, fol-\nlowing DatasetGAN [81]. While DatasetGAN synthesized a\nlabeled dataset for training downstream task networks, we\ninstead use soft label distillation for both encoded and syn-\nthesized datasets, i.e. we include predicted soft labels in our\nfeature dataset D. This is visualized in Fig.2(d). We first\ndescribe the architecture of the feature interpreter followed\nby our distillation objective for soft labels.\nFeature Interpreter. We utilize a similar design to Big-\nDatasetGAN [40], which improves the interpreter design\nover DatasetGAN with better memory efficiency and predic-\ntion accuracy. Specifically, the interpreter takes multi-level\nfeatures f g\nl from the generator as inputs which are fed into\na series of Feature Fusion Layers (see Fig 3) to lower the\nfeature dimension and fuse with the next-level features, to\nfinally output per-pixel logits. We follow BigDatasetGAN\u2019s\ninterpreter design and only replace the convolutional fused\nblock with depth-wise separable convolutions [17], Group\nNorm [67], and Swish activation [49].\nWe explore training the interpreter branch with segmenta-\ntion labels, and use a combination of the cross-entropy and\nDice [57] objectives for training:\nLinterpreter = H(I\u03b8(f g\nl ), y) + \u03bbdD(I\u03b8(f g\nl ), y),\n(4)\nwhere I\u03b8 are the weights of the feature interpreter, y are the\ntask labels. H(\u00b7, \u00b7) denotes pixel-wise cross-entropy loss, and\nD(\u00b7, \u00b7) is Dice Loss. \u03bbd is a hyperparameter to weigh the dice\nloss. We use \u03bbd = 3.0 in all our experiments following [57].\nLabel Distillation. We follow [34] for label distillation.\nSpecifically, we use:\nLld = H(P g\n\u03c4 , P r\n\u03c4 ),\n(5)\nwhere P g\n\u03c4 and P r\n\u03c4 are the logits from the feature interpreter\nand the logit-head of the target image backbone f, respec-\ntively. H is the cross-entropy, with temperate \u03c4, controlling\nthe sharpness of the output distribution. We use \u03c4 = 4.0 in\nall our experiments following [34].\nWe use the label distillation objective in conjunction with\nour feature distillation objective:\nLmix = Lfeat + \u03bbldLld\n(6)\nwhere \u03bbld is a hyperparameter controlling the weighting be-\ntween the losses, which we use \u03bbld = 1.0 in our experiment.\nReal Image\nNoisy Image\nRes 1/16\nRes 1/32\nRes 1/8\nRes 1/4\nT=50\nT=250\nFigure 4. ADM feature visualization (ImageNet). We visualize ADM\nfeature activation maps at different resolution blocks (columns)\nat different diffusion time steps T (rows). At lower resolution\nblocks, features activate on objects like humans and cars. For higher\nresolution block, features focus on smaller parts like wheels and\nheadlights. With increasing T, feature activations become smoother.\nWe pre-train the image backbone f using the mixed distilla-\ntion losses over all images in our pre-training dataset, either\nlabeled or unlabeled. Annotated labels are only used for train-\ning the feature interpreter, and we only use soft labels from\nthe feature interpreter for pre-training f with distillation.\n4. Experiments\nIn this section, we first experimentally evaluate the perfor-\nmance of DreamTeacher for both: self-supervised representa-\ntion learning and semi-supervised learning (Subsec. 4.1). We\nthen additionally investigate the performance of our model\nfor in-domain-pretraining (Subsec. 4.2). In the in-domain\nsetting, the same target dataset is used for both pretrain-\ning and finetuning, and the backbones are initialized from\nscratch. Finally, we ablate different generative models and\ndesign choices of DreamTeacher (Subsec. 4.3).\nWe investigate several generative models: for GANs, we\nuse unconditional BigGAN [5], ICGAN [9], StyleGAN2 [37]\nand for diffusion-based model, ADM [20], and Stable Dif-\nfusion (SD) Models [50]. We use four datasets for pre-\ntraining, both for training the generative models, as well\nas knowledge distillation to downstream backbones. We use\nBDD100K [76], ImageNet-1k(IN1k-1M), LSUN [77] and\nFFHQ [36], which contain 100k, 1.28 million, 10 million,\nand 100k images, respectively. We focus on convolutional\nnetworks as target image backbones.\n4.1. ImageNet Evaluation and Transfer\nImagenet Pretraining. We first validate the effectiveness\nof DreamTeacher for ImageNet pretraining. In this setting,\nwe follow the recent SoTA method, SparK [58], and eval-\nuate two convolutional architectures as downstream back-\nbones, ConvNext-B [44] and ResNet50 [31]. Following com-\nmon practice in the literature [28, 58], we pre-train image\nbackbones unsupervised on ImageNet-1k. For a comparison\nwith transformer-based self-supervised methods, we follow\nSparK\u2019s methodology [58] and pre-train a modern CNN-\nbased backbone ConvNeXt [44] with a similar number of\nparameters. Additionally, to ensure a fair comparison with\nCNN-based self-supervised methods, we pre-train and eval-\nuate a classical backbone, ResNet-50.\n5\nPre-training Method\nPT\nArch.\nEff.\nCls\nDet.\nSeg.\ntask\nepoch\nAcc.\nAPbb\nAPbb\n75\nAPmk\nAPmk\n75\nVision Transformer Backbone\nSupervised [28]\n-\nViT-B\n300\n82.3\n49.8\n53.8\n43.2\n46.5\nMoCov3 [15]\nCL\nViT-B\n1600\n83.2\n-\n-\n-\n-\nDINO [8]\nCL\nViT-B\n1600\n82.8\n50.1\n54.3\n43.4\n47.0\nBEiT [3]\nMIM\nViT-B\n800\n83.2\n50.1\n54.6\n43.5\n47.1\nMAE [28]\nMIM\nViT-B\n1600\n83.6\n-\n-\n-\n-\niBOT [84]\nMIM + CL\nViT-B\n1600\n84.0\n51.2\n55.5\n44.2\n47.7\nConvolutional Backbone\nSupervised [44]\n-\nConvX-B\n300\n83.8\n51.2\n55.5\n44.3\n47.9\nSparK [58]\nMIM\nConvX-B\n1600\n84.8\n51.9\n56.5\n44.6\n48.4\nDT-feat.distil. w/ ADM [20]\nGEN\nConvX-B\n*600\n83.9\n52.5\n57.4\n45.2\n49.0\nTable 1. Comparing DreamTeacher with SoTA self-supervised methods on ImageNet and instance segmentation on COCO. All the baselines\nincluding ADM are pre-trained on ImageNet-1k. For ImageNet classification, we adopt SparK\u2019s fine-tuning setting with resolution 224.\nFor COCO, we follow iBOT to fine-tune Cascade Mask R-CNN [6] for 12 (1\u00d7) epochs. Average precisions of detection box (APbb) and\nsegmentation mask (APmk) on val2017 are reported. For a fair comparison, both our method and baselines follow iBOT fine-tuning schedule\nand setting. Our DT pre-training task is highlighted as generative(GEN) comparing to contrastive(CL) and masking(MIM) based objectives.\n*Our effective epochs includes 400 epochs generative model training and 200 epochs feature distillation training.\nPre-training (ResNet-50)\nPT\nEff.\nCls.\n1\u00d7 Schedule\n2\u00d7 Schedule\ntask\nepoch\n(Acc.)\nAPbb\nAPmk\nAPbb\nAPmk\nSupervised\n-\n-\n79.8\n38.9\n35.4\n41.3\n37.3\nSimSiam [14]\nCL\n800\n79.1\n-\n-\n-\n-\nMoCo [29]\nCL\n800\n-\n38.5\n35.1\n40.8\n36.9\nMoCov2 [13]\nCL\n1600\n79.8\n40.4\n36.4\n41.7\n37.6\nSimCLR [12]\nCL\n4000\n80.0\n-\n-\n-\n-\nInfoMin [59]\nCL\n800\n-\n40.6\n36.7\n42.5\n38.4\nBYOL [27]\nCL\n1600\n80.0\n40.4\n37.2\n42.3\n38.3\nSwAV [7]\nCL\n1200\n80.1\n-\n-\n42.3\n38.2\nSparK [60]\nMIM\n1600\n80.6\n41.6\n37.7\n43.4\n39.4\nDT-feat.distil. w/ ADM [20]\nGEN\n*600\n80.2\n44.1\n40.1\n45.1\n40.8\nTable 2. ResNet-50 results on ImageNet and COCO instance segmentation. For\nImageNet classification, we follow SparK\u2019s fine-tuning setting with resolution\n224. Top-1 accuracy (Acc) on ImageNet val set is reported. For COCO, Mask\nR-CNN [30] ResNet50-FPN is equally fine-tuned for 12 or 24 epochs (1\u00d7 or\n2\u00d7), following the same setup as SparK. *Our effective epochs includes 400\nepochs generative model training and 200 epochs feature distillation training.\nPre-training (ResNet-50)\nADE20k\nBDD100k\nmIoU\nAPbb\nAPmk\nSupervised\n40.9\n26.1\n20.2\nSimCLR [12]\n39.9\n24.5\n20.6\nSparK [60]\n40.5\n25.7\n22.4\nSimSiam [14]\n40.6\n26.3\n22.7\nMoCov2 [13]\n40.9\n26.9\n22.9\ndenseCL [63]\n41.1\n27.1\n23.4\nSwAV [7]\n41.2\n25.6\n22.2\nBYOL [27]\n41.6\n26.2\n22.6\nPixPro [71]\n41.6\n27.2\n23.1\nDT-feat.distil. w/ ADM [20]\n42.5\n28.3\n24.8\nTable 3. Transfer learning: ADE20k and BDD100k. All\nmethods are pre-trained on ImageNet-1k and fine-tuned\non downstream tasks. For ADE20k, we follow [44]\nto use UperNet [68] and fine-tune for 160k iterations,\nreported number is mean IoU at single scale. For\nBDD100k, we follow official setup [76] to use Mask\nR-CNN ResNet50-FPN fine-tune for 36 (3\u00d7) epochs.\nImplementation. We use pre-trained unconditional ADM\nwith resolution 256 from the official release. We only use\nhorizontal flip augmentation and train using LAMB [75]\noptimizer with a batch size of 2048. We adopt a cosine-\nannealing learning rate with peak value = 0.0002 \u00d7 batchsize\n/ 256. See appendix for other hyperparameters.\nTransferring to Downstream Tasks. We assess the quality\nof learned representations obtained using DreamTeacher by\nfine-tuning the pre-trained backbone with additional heads\nper task (see Appendix for implementations). We test down-\nstream transfer performance for ImageNet classification and\nCOCO [43] instance segmentation, which are representa-\ntive global and spatial image understanding tasks commonly\nused in literature. Prior self-supervised learning methods\nhave excelled at ImageNet classification, and have recently\nshown improvement over supervised ImageNet pre-training\nfor spatial understanding tasks such as object detection and\nsegmentation that are much more cost-intensive to label. Ad-\nditionally, we also include linear probing experiments on\nImageNet for both classification and semantic segmentation\ntasks in the Appendix (Table 12).\nDiscussion. Comparing to self-supervised methods based\non vision-transformer, DreamTeacher outperforms existing\napproaches in both detection and segmentation, and per-\nforms on par in the classification setting (Table 1). Specif-\nically, DreamTeacher achieves 52.5 APbb and 45.2 APmk\non the COCO instance segmentation task outperforming\nthe SoTA transformer-based method iBOT by +1.3 and\n+1.0. DreamTeacher also outperforms the recently proposed\nsparse-convolution based MIM method SparK [58], in the\ntasks of detection and segmentation by +0.6 and +0.6, re-\nspectively. We notice that our method does not outperform\nthis baseline on the task of image classification. This may\nlikely be due to our approach of distilling spatial features\n6\nPre-training (ResNet-50)\nPT\nEff.\nBDD100k Ins.\ntask\nepoch\nAPbb\nAPmk\nSupervised [76]\n-\n-\n26.1\n20.2\nBYOL [27]\nCL\n5000\n23.9\n20.0\nSparK [58]\nMIM\n2500\n24.4\n20.6\nDT-feat.distil. w/ StyleGAN2 [37]\nGEN\n*900\n25.1\n21.4\nDT-feat.distil. w/ ADM [20]\nGEN\n*900\n26.7\n22.9\nTable 4. In-domain pre-training on BDD100k. We\nfollow the recommendation of [24] to pre-train con-\ntrastive and masking based self-supervised method\nwith long schedule for small dataset like BDD100k\nwith 70k train images. We finetune on BDD100k\ninstance segmentation task using Mask R-CNN\nResNet50-FPN for 36(3\u00d7) epochs.\nmethod\nbackbone params\npre-data\nBedroom-28 FFHQ-34 Cat-15 Horse-21\nclassific. sup.\nRN101\n43M\nIN1k-1M\n34.4\n53.6\n38.8\n51.1\nclassific. sup.\nConvNX-B\n89M\nIN21k-14M\n41.0\n59.2\n47.3\n56.0\nSwAV [7]\nRN50-w2\n94M\ntask domains\n41.0\n54.7\n44.1\n51.7\nMAE [28]\nViT-L\n305M task domains\n45.0\n58.8\n52.4\n63.4\nDatasetGAN [81]\nRN101\n43M\ntask domains\n31.3\n57.0\n36.5\n45.4\nDatasetDDPM [4]\nRN101\n43M\ntask domains\n47.9\n56.0\n47.6\n60.8\nDDPM-seg [4]\nUNet\n554M task domains\n49.4\n59.1\n53.7\n65.0\nDT-mix.distil. w/ ADM [20]\nRN101\n43M\ntask domains\n49.9\n59.4\n56.7\n65.9\nDT-mix.distil. w/ ADM [20] ConvNX-B\n89M\ntask domains\n54.8\n61.2\n58.6\n67.6\nTable 5. Label-efficient semantic segmentation benchmark. We compare our\nDreamTeacher (DT) with various representation learning baselines. Our DT-\nmix.distil. with ResNet 101 backbone (only 43M parameters) beats all baselines,\nsome with 10x the number of parameters. We also show our method with ConvNX-B\nachieves the new SoTA without using any extra data, i.e. IN1k-1M or IN21k-14M.\nfrom the generative model, which might contain more se-\nmantically localized information for generation (visualized\nin Fig. 7), which empirically seems to favor dense prediction\ntasks. It is also worth noting that our method is \u223c 2.5\u00d7 more\nefficient than SparK w.r.t. effective training epochs [58] on\nImageNet (600 vs 1600). This number includes training steps\nof the generative model, ADM.\nIn Table 2 we show results for Resnet-50 using SparK\u2019s\nsetting and parameters. Specifically, we evaluate ImageNet\nclassification performance with full fine-tuning and COCO\ninstance segmentation with two schedules (1\u00d7 and 2\u00d7).\nSimilar to the previous experiment, we achieve compara-\nble performance as baselines for ImageNet classification.\nFor COCO instance segmentation, we notably outperform\nall contrastive methods and the masking-based approach\nSparK (+2.5 APbb for 1\u00d7 and +1.7 APbb for 2\u00d7 sched-\nule). In Table 3, we further evaluate transfer learning on the\nADE20k semantic segmentation task and BDD100k instance\nsegmentation task. We include SoTA contrastive methods for\ndense prediction tasks, denseCL [63] and PixPro [71]. Our\napproach using generation as pre-training task outperforms\nboth global and dense contrastive pre-training tasks as well\nas the masked image modelling task.\n4.2. In-domain Pre-training\nFor in-domain pre-training, we first pre-train the back-\nbone with various self-supervised training approaches. Pre-\ntraining efficacy is evaluated by fine-tuning the backbone on\ndifferent tasks with label supervision, on the same dataset.\nNote that both baselines and DreamTeacher use randomly\ninitialized downstream backbones. We evaluate unsuper-\nvised pre-training using the BDD-100k benchmark and semi-\nsupervised pre-training using multiple datasets from the label\nefficiency benchmark used by [4,40,81].\nBDD100k Benchmark. We pre-train all self-supervised\nlearning methods, including DreamTeacher on 70k unla-\nbeled images from BDD100k. We then evaluate all methods\non BDD100k, which contains 10k images annotated with\nsemantic, instance and panoptic labels. We follow the of-\nficial dataset split, using 7k labels for supervised training.\nResults are reported on the validation set (1k images). We\nSample Images\nSample Images\nK-mean \nK-mean \nFigure 5. K-means clustering of StyleGAN2\u2019s features trained on\nBDD100K. We run kmeans clustering (k = 10) on 10k sampled\nfeatures, and show unsupervised segmentation maps on sampled im-\nages. Notice that the clusters are consistent across images (car, sky,\ntree etc), indicating a semantic meaning of the generative features.\nuse a Resnet-50 [31] backbone for all methods.\nFeature Visualization. We visualize the knowledge learned\nby different generative models in Fig. 5. Specifically, we\nshow scenes sampled from StyleGAN2 trained on BDD100k.\nWe perform k-means clustering (k = 10) of StyleGAN\u2019s fea-\ntures and visualize clusters with different colors. Notice that\nthe clusters roughly correspond to major semantic classes.\nResults. In Table 4, we compare DreamTeacher with the\nrepresentative contrastive method BYOL and recently pro-\nposed MIM-based method SparK on BDD100k instance\nsegmentation task. As investigated in [24], contrastive and\nmasking-based self-supervised methods require a longer pre-\ntraining schedule to converge on a small in-domain dataset.\nWe pre-trained backbones using DreamTeacher feature distil-\nlation with StylegGAN2 and ADM, and the effective epochs\ncomprise 300 training epochs of the generative model and\n600 training epochs for feature distillation. Our methods\noutperform contrastive and masking-based techniques sig-\nnificantly for in-domain pre-training with better training\nefficiency. Notably, our method with ADM outperforms the\nImageNet supervised pre-trained backbone, showing promis-\ning results without relying on large-scale curated datasets\nlike ImageNet. See appendix for qualitative results and se-\nmantic segmentation and panoptic segmentation results.\nLabel-efficient Benchmarks. We now evaluate in-domain\npre-training in our semi-supervised setting. We follow the\nsetup in DDPM-seg [4] and train on \u201cbedroom\u201d, \u201ccat\u201d and\n\u201chorse\u201d categories from LSUN [77], and human faces from\nFFHQ [36] (at 256x256 resolution). We evaluate semantic\nsegmentation, where the datasets have 28, 15, 21, 34 seman-\n7\ntic classes, respectively. Datasets contain only 40, 30, 30 and\n20 labeled images. We pre-train all backbones from scratch,\ni.e. without ImageNet pre-trained initialization. We use UPer-\nNet [68] for semantic segmentation. Note that some baselines\nutilize different settings. DatasetGAN [81] and DatasetD-\nDPM [4] both train a small task-specific head on top of a\npre-trained generative model, and generate a large labeled\ndataset for training a downstream network. On the other\nhand, DDPM-seg directly leverages the diffusion-based gen-\nerative model with a task head as the segmentation network.\nResults are reported in Table 5 . We highlight several key\nobservations below:\n\u2022 Given the same backbone, ResNet101, DreamTeacher\ntrained with our mixed distillation (Eq. 6) outper-\nforms DatasetDDPM across all datasets. We outperform\nDatasetDDPM by 3.4% on FFHQ-34, and 9.1% on Cat-15.\n\u2022 Using both a 10x and a 6x smaller backbone (ResNet-101\nand a ConvNX-B [44], respectively), we outperform\nDDPM-Seg on all classes. On Bedroom-28 and Cat-15,\nwe improve over the baseline by more than 5%.\n\u2022 Given the same backbone, our method significantly out-\nperforms pre-training with ImageNet classification labels.\nWith ConvNX-B [44], our proposed approach is better than\nImageNet pre-training by more than 10% on Bedroom-28,\nCat-15, and Horse-21. These results may indicate that if\nthe in-domain datasets are sufficiently large relative to the\ncomplexity of the task, in-domain pre-training is more\neffective than pre-training on large generic datasets like\nImageNet. Note that this is true for both semi-supervised\n(these results) and unsupervised pre-training (Table 4).\n4.3. Ablation Studies\nWe first ablate DreamTeacher with different generative\nmodels in Table 6. Result shows ADM trained on IN1k-\n1M has the highest downstream performance. We also ex-\nploited off-the-shelf Stable Diffusion trained on LAION-\n400M and pretrain backbone on IN1k-1M. However, it per-\nforms slightly worse than ADM trained on IN1k-1M. In\nTable 7 we ablate our proposed distillation losses. Mixing\nfeature- and label- distillation achieves the best performance\nexcept for the FFHQ-34 dataset. We demonstrate our design\nchoices of the decoder used in pre-training in Table 8, loss\nfunctions in Table 9, encoding modes (deterministic and\nstochastic) in Table 10 and diffusion steps in Table 11. Ab-\nlation studies pre-train backbone for 100 epochs and report\nperformance on BDD100k instance segmentation. These\nresults confirm our choices.\nLimitations:\nOur framework relies on generative models\nfor representation learning, and training a generative model\non large-scale datasets at high resolution is costly, especially\nwith diffusion-based models. Further, our feature distillation\nmethod only considers features at the same spatial resolution\nPre-training (ResNet-50)\nGen. Data\nPre. Data\nADE20k\nCOCO\nmIoU\nAPbb\nAPmk\nDT-feat.distil. w/ BigGAN [5]\nIN1k-1M\nIN1k-1M\n40.8\n40.7\n36.9\nDT-feat.distil. w/ ICGAN [9]\nIN1k-1M\nIN1k-1M\n41.2\n40.0\n36.5\nDT-feat.distil. w/ SD1.4 [50]\nLAION-400M IN1k-1M\n41.4\n43.3\n39.4\nDT-feat.distil. w/ ADM [20]\nIN1k-1M\nIN1k-1M\n42.5\n44.1\n40.1\nTable 6. Ablation study with different generative models using\nDreamTeacher. We use off-the-shelf SD with version 1.4 pre-trained\non LAION-400M without finetuning, and it performs slightly worse\nthan DT with ADM, which is trained on ImageNet-1k.\nLoss\nBedroom-28\nFFHQ-34\nCat-15\nHorse-21\nfeat distil.\n53.1\n61.1\n58.2\n64.7\nlabel distil.\n54.6\n61.3\n58.4\n64.4\nmix distil.\n54.8\n61.2\n58.6\n67.6\nTable 7. Ablating feature/label distillation. We pretrain ConvNeXt-B\nto convergence. Feature distillation (FD) does not leverage labels\nin pre-training, yet performs competitively.\nDecoder\nBox mAP Mask mAP\nFPN\n23.6\n20.3\nFPN+Atten. layer\n23.9\n20.7\nPaFPN\n24.0\n20.8\nFPN+PPM\n25.1\n21.4\nTable 8. Ablating feat. regres-\nsors We pretrain ResNet50\nwith FT. We compare FPN\nwith an attention layer, and\nadd a bottom-up branch to fuse\nFPN features (PaFPN).\nDecoder\nBox mAP Mask mAP\nFinnet(MSE)\n23.6\n20.3\nAT\n22.3\n19.3\nMSE+AT\n25.0\n21.6\nTable 9. Ablating distillation\nlosses. We pretrain ResNet50\nwith MSE or AT loss using fea-\nture distill. Combining losses\nachieves best results.\nEncoding Box mAP Mask mAP\nDetermin.\n23.4\n20.8\nStochastic\n24.3\n21.1\nTable 10. Ablating DDPM en-\ncoding. We use DDIM [55]\nsampling for deterministic en-\ncoding. In both cases, back-\nbone is pretrained for 100\nepochs.\nSteps\nBox mAP Mask mAP\nT=50\n23.8\n20.4\nT=150\n23.9\n20.6\nT=250\n24.4\n21.1\nT=350\n23.4\n20.1\nTable 11. Ablating # of diffusion\nsteps. We pretrain ResNet50\nwith feature distillation using\ndifferent # of diffusion steps.\nPerformance varies with T.\nand we limit our scope to CNN-based image backbones.\nDistilling features into vision transformers is for future work.\n5. Conclusion\nWe proposed DreamTeacher, a framework for distilling\nknowledge from generative models onto target image back-\nbones. We investigated several different settings, generative\nmodels, target backbones, and benchmarks. Experiments\nshow that generative networks that leverage large unlabeled\ndatasets with generative objectives learn semantically mean-\ningful features that can be successfully distilled on target\nimage backbones. We empirically show our generative-based\npre-training method outperforms existing contrastive based\nand MIM based self-supervised learning approaches in sev-\neral challenging benchmarks including COCO, ADE20K\nand BDD100K. We hope our exploration and discovery can\ninspire future works to study generative pre-training and\nleveraging geneartive models for vision tasks.\n8\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. arXiv preprint arXiv:1607.06450, 2016.\n4\n[2] Fan Bao, Chongxuan Li, Yue Cao, and Jun Zhu. All are worth\nwords: a vit backbone for score-based diffusion models. arXiv\npreprint arXiv:2209.12152, 2022. 2\n[3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\nBert pre-training of image transformers.\narXiv preprint\narXiv:2106.08254, 2021. 6\n[4] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin\nKhrulkov, and Artem Babenko. Label-efficient semantic seg-\nmentation with diffusion models, 2021. 2, 3, 7, 8\n[5] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis.\narXiv preprint arXiv:1809.11096, 2018. 2, 3, 5, 8, 13\n[6] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: high\nquality object detection and instance segmentation. IEEE\ntransactions on pattern analysis and machine intelligence,\n43(5):1483\u20131498, 2019. 6\n[7] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal,\nPiotr Bojanowski, and Armand Joulin. Unsupervised learning\nof visual features by contrasting cluster assignments. NeurIPS,\n33:9912\u20139924, 2020. 1, 6, 7, 12\n[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650\u20139660, 2021. 6\n[9] Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal\nDrozdzal, and Adriana Romero Soriano. Instance-conditioned\ngan. Advances in Neural Information Processing Systems,\n34:27517\u201327529, 2021. 3, 5, 8, 13\n[10] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo\nJun, David Luan, and Ilya Sutskever. Generative pretraining\nfrom pixels. pages 1691\u20131703. PMLR, 2020. 2\n[11] Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia.\nDistilling knowledge via knowledge review. In CVPR, pages\n5008\u20135017, 2021. 3\n[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof-\nfrey Hinton. A simple framework for contrastive learning of\nvisual representations. pages 1597\u20131607. PMLR, 2020. 1, 2,\n6, 12\n[13] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Im-\nproved baselines with momentum contrastive learning. arXiv\npreprint arXiv:2003.04297, 2020. 1, 2, 6, 12\n[14] Xinlei Chen and Kaiming He. Exploring simple siamese\nrepresentation learning. In CVPR, pages 15750\u201315758, 2021.\n1, 6, 12\n[15] Xinlei Chen*, Saining Xie*, and Kaiming He. An empirical\nstudy of training self-supervised vision transformers. arXiv\npreprint arXiv:2104.02057, 2021. 6\n[16] Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong Wang.\nSemi-supervised semantic segmentation with cross pseudo\nsupervision. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 2613\u20132622,\n2021. 15\n[17] Franc\u00b8ois Chollet. Xception: Deep learning with depthwise\nseparable convolutions. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n1251\u20131258, 2017. 5, 13\n[18] Elijah Cole, Xuan Yang, Kimberly Wilber, Oisin Mac Aodha,\nand Serge Belongie. When does contrastive visual representa-\ntion learning work? In CVPR, 2022. 2\n[19] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke,\nStefan Roth, and Bernt Schiele. The cityscapes dataset for\nsemantic urban scene understanding. In CVPR, pages 3213\u2013\n3223, 2016. 12\n[20] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in Neural Information\nProcessing Systems, 34:8780\u20138794, 2021. 2, 4, 5, 6, 7, 8, 12,\n13, 14, 16\n[21] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-\nvised visual representation learning by context prediction. In\nProceedings of the IEEE international conference on com-\nputer vision, pages 1422\u20131430, 2015. 1, 2\n[22] Jeff Donahue, Philipp Kr\u00a8ahenb\u00a8uhl, and Trevor Darrell. Ad-\nversarial feature learning. arXiv preprint arXiv:1605.09782,\n2016. 2, 12\n[23] Jeff Donahue and Karen Simonyan. Large scale adversarial\nrepresentation learning. NeurIPS, 32, 2019. 2, 12\n[24] Alaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan\nLaptev, Herv\u00b4e Jegou, and Edouard Grave. Are large-scale\ndatasets necessary for self-supervised pre-training? arXiv\npreprint arXiv:2112.10740, 2021. 7, 12, 13\n[25] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsu-\npervised representation learning by predicting image rotations.\narXiv preprint arXiv:1803.07728, 2018. 2\n[26] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems, 27, 2014. 2\n[27] Jean-Bastien Grill, Florian Strub, Florent Altch\u00b4e, Corentin\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doer-\nsch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\nlaghi Azar, et al. Bootstrap your own latent-a new approach\nto self-supervised learning. NeurIPS, 33:21271\u201321284, 2020.\n1, 2, 6, 7, 12, 13\n[28] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In CVPR, pages 16000\u201316009, 2022. 1, 5, 6,\n7\n[29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual repre-\nsentation learning. In CVPR, pages 9729\u20139738, 2020. 1, 2,\n6\n[30] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961\u20132969, 2017. 6,\n15\n[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 5, 7\n9\n[32] Olivier J H\u00b4enaff, Skanda Koppula, Jean-Baptiste Alayrac,\nAaron Van den Oord, Oriol Vinyals, and Jo\u02dcao Carreira. Effi-\ncient visual pretraining with contrastive detection. In ICCV,\npages 10086\u201310096, 2021. 2, 15\n[33] Geoffrey Hinton. To recognize shapes, first learn to generate\nimages. Progress in brain research, 165:535\u201347, 02 2007. 2\n[34] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distill-\ning the knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2(7), 2015. 3, 5\n[35] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 2, 4\n[36] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4401\u20134410, 2019. 2, 3,\n5, 7\n[37] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improv-\ning the image quality of stylegan. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8110\u20138119, 2020. 2, 5, 7, 13, 14\n[38] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 4\n[39] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\nDoll\u00b4ar. Panoptic feature pyramid networks. In CVPR, pages\n6399\u20136408, 2019. 13, 15\n[40] Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis,\nSanja Fidler, and Antonio Torralba. Bigdatasetgan: Synthe-\nsizing imagenet with pixel-wise annotations. In CVPR, pages\n21330\u201321340, 2022. 2, 5, 7, 12\n[41] Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba,\nand Sanja Fidler. Semantic segmentation with generative\nmodels: Semi-supervised learning and strong out-of-domain\ngeneralization. In CVPR, pages 8300\u20138311, 2021. 2, 3\n[42] Tsung-Yi Lin, Piotr Doll\u00b4ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n2117\u20132125, 2017. 4, 13\n[43] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, pages 740\u2013755. Springer, 2014. 2, 6\n[44] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\nenhofer, Trevor Darrell, and Saining Xie. A convnet for the\n2020s. In CVPR, pages 11976\u201311986, 2022. 2, 5, 6, 8\n[45] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pages 3431\u20133440, 2015. 12\n[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 14\n[47] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\nvisual representations by solving jigsaw puzzles. In ECCV,\npages 69\u201384. Springer, 2016. 2\n[48] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. arXiv preprint arXiv:2212.09748, 2022. 2\n[49] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching\nfor activation functions. arXiv preprint arXiv:1710.05941,\n2017. 5, 13\n[50] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models, 2021. 5, 8\n[51] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,\nAntoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:\nHints for thin deep nets. arXiv preprint arXiv:1412.6550,\n2014. 3, 4\n[52] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn International Conference on Medical image computing\nand computer-assisted intervention, pages 234\u2013241. Springer,\n2015. 4\n[53] Changyong Shu, Yifan Liu, Jianfei Gao, Zheng Yan, and\nChunhua Shen.\nChannel-wise knowledge distillation for\ndense prediction. In ICCV, pages 5311\u20135320, 2021. 3\n[54] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Conference\non Machine Learning, 2015. 2\n[55] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. arXiv preprint arXiv:2010.02502,\n2020. 4, 8\n[56] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\narXiv preprint arXiv:2011.13456, 2020. 2, 4\n[57] Carole H Sudre, Wenqi Li, Tom Vercauteren, Sebastien\nOurselin, and M Jorge Cardoso. Generalised dice overlap\nas a deep learning loss function for highly unbalanced seg-\nmentations. In Deep learning in medical image analysis\nand multimodal learning for clinical decision support, pages\n240\u2013248. Springer, 2017. 5\n[58] Keyu Tian, Yi Jiang, Qishuai Diao, Chen Lin, Liwei Wang,\nand Zehuan Yuan. Designing bert for convolutional networks:\nSparse and hierarchical masked modeling. arXiv:2301.03580,\n2023. 1, 2, 5, 6, 7, 13, 14\n[59] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan,\nCordelia Schmid, and Phillip Isola.\nWhat makes for\ngood views for contrastive learning?\narXiv preprint\narXiv:2005.10243, 2020. 6\n[60] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and\nDaniel Cohen-Or. Designing an encoder for stylegan im-\nage manipulation. ACM Transactions on Graphics (TOG),\n40(4):1\u201314, 2021. 6, 12\n[61] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical\nvariational autoencoder. Advances in Neural Information\nProcessing Systems, 33:19667\u201319679, 2020. 4\n[62] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi\nLi, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng\nLi, et al. Internimage: Exploring large-scale vision founda-\ntion models with deformable convolutions. arXiv preprint\narXiv:2211.05778, 2022. 2\n[63] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and\nLei Li. Dense contrastive learning for self-supervised visual\npre-training. In CVPR, pages 3024\u20133033, 2021. 1, 2, 6, 7, 12\n10\n[64] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan\nYuille, and Christoph Feichtenhofer. Masked feature predic-\ntion for self-supervised visual pre-training. In CVPR, pages\n14668\u201314678, 2022. 1\n[65] Ross Wightman, Hugo Touvron, and Herv\u00b4e J\u00b4egou. Resnet\nstrikes back: An improved training procedure in timm. arXiv\npreprint arXiv:2110.00476, 2021. 14\n[66] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei\nChen, Zhuang Liu, In So Kweon, and Saining Xie. Con-\nvnext v2: Co-designing and scaling convnets with masked\nautoencoders. arXiv preprint arXiv:2301.00808, 2023. 2\n[67] Yuxin Wu and Kaiming He. Group normalization. In Proceed-\nings of the European conference on computer vision (ECCV),\npages 3\u201319, 2018. 5, 13\n[68] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian\nSun. Unified perceptual parsing for scene understanding. In\nProceedings of the European conference on computer vision\n(ECCV), pages 418\u2013434, 2018. 4, 6, 8, 13, 14, 15\n[69] Tete Xiao, Xiaolong Wang, Alexei A Efros, and Trevor Dar-\nrell. What should not be contrastive in contrastive learning.\narXiv preprint arXiv:2008.05659, 2020. 2\n[70] Enze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang Xu,\nPeize Sun, Zhenguo Li, and Ping Luo. Detco: Unsupervised\ncontrastive learning for object detection. In ICCV, pages\n8392\u20138401, 2021. 1, 2\n[71] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen\nLin, and Han Hu. Propagate yourself: Exploring pixel-level\nconsistency for unsupervised visual representation learning.\nIn CVPR, pages 16684\u201316693, 2021. 1, 2, 6, 7\n[72] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\nBao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple\nframework for masked image modeling. In CVPR, pages\n9653\u20139663, 2022. 1\n[73] Yuwen Xiong, Mengye Ren, and Raquel Urtasun.\nLoco:\nLocal contrastive representation learning. NeurIPS, 33:11142\u2013\n11153, 2020. 1, 2\n[74] Yang You, Igor Gitman, and Boris Ginsburg.\nLarge\nbatch training of convolutional networks.\narXiv preprint\narXiv:1708.03888, 2017. 12\n[75] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv\nKumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel,\nKurt Keutzer, and Cho-Jui Hsieh. Large batch optimization\nfor deep learning: Training bert in 76 minutes. arXiv preprint\narXiv:1904.00962, 2019. 6, 13\n[76] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying\nChen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell.\nBdd100k: A diverse driving dataset for heterogeneous multi-\ntask learning. In CVPR, pages 2636\u20132645, 2020. 2, 5, 6, 7,\n12, 13\n[77] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas\nFunkhouser, and Jianxiong Xiao. Lsun: Construction of a\nlarge-scale image dataset using deep learning with humans in\nthe loop. arXiv preprint arXiv:1506.03365, 2015. 2, 5, 7\n[78] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,\nJames Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,\nand Yonghui Wu. Vector-quantized image modeling with\nimproved vqgan. arXiv preprint arXiv:2110.04627, 2021. 2\n[79] Sergey Zagoruyko and Nikos Komodakis. Paying more at-\ntention to attention: Improving the performance of convolu-\ntional neural networks via attention transfer. arXiv preprint\narXiv:1612.03928, 2016. 3, 4\n[80] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful\nimage colorization. In ECCV, pages 649\u2013666. Springer, 2016.\n2\n[81] Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-\nFrancois Lafleche, Adela Barriuso, Antonio Torralba, and\nSanja Fidler. Datasetgan: Efficient labeled data factory with\nminimal human effort. In CVPR, pages 10145\u201310155, 2021.\n2, 4, 5, 7, 8\n[82] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 2881\u20132890, 2017. 4, 13\n[83] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic under-\nstanding of scenes through the ade20k dataset. International\nJournal of Computer Vision, 127(3):302\u2013321, 2019. 2\n[84] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang\nXie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training\nwith online tokenizer. International Conference on Learning\nRepresentations (ICLR), 2022. 6, 14\n11\nIn the Appendix we provide additional studies, implemen-\ntation details, and qualitative results.\nA. ImageNet Linear Evaluation\nWe show the results obtained on ImageNet with self-\nsupervised pre-training, via linear probing as well as Ima-\ngeNet foreground segmentation readout results in Table 12.\nLinear probing. We follow SimSiam [14] setup. Specifi-\ncally, given the pre-trained network, we train a supervised\nlinear classifier on frozen features from the ResNet\u2019s global\naverage pooling layer. When training the linear classifier we\nuse a learning rate of lr = 0.02 with a cosine decay schedule\nfor 90 epochs, weight decay = 0, momentum = 0.9, a batch\nsize of 4096, and a LARS optimizer [74]. After training the\nlinear classifier, we evaluate it on the centered 224x224 crop\nin the validation set.\nSemantic segmentation readout. We also evaluate the pre-\ntrained backbone\u2019s features on the dense prediction tasks on\nImageNet. We use the dataset from BigDatasetGAN [40],\nwhich has object mask annotation on ImageNet 1k classes.\nOn average, each class has 5 annotated images and in to-\ntal, it has 6,571 training images and 1,813 testing images.\nWe use the dataset to evaluate the pre-trained backbones on\nforeground/background semantic segmentation performance.\nWe train an FCN [45] decoder with the frozen pre-trained\nbackbone\u2019s features. The FCN readout network is trained us-\ning the SGD optimizer with lr = 0.01, momentum=0.9 and\nweight decay=0.0005. We use poly learning rate schedule\nwith power=0.9 and train for 20k iterations. Both training\nand testing images are center cropped with 256x256 res-\nolution. For all methods, we use ResNet-50 as the image\nbackbone.\nResults. Our method with feature distillation and the ADM\ngenerative model [20], referred to as DT-feat.distil w/ ADM\nachieves 63.9 Top1 accuracy with linear probing, outperform-\ning all generative pre-trained baselines, and also outperforms\ncompetitive discriminative pre-train backbones SimCLR and\ndenseCL. However, we do not outperform the best perform-\ning contrastive based method BYOL in this task. Potential ex-\nplanation could be our pre-training focus on spatial features\ninstead of global discriminative features, which is important\nfor global classification tasks. Notably, our method achieves\n79.3 mIoU on the ImageNet foreground/background segmen-\ntation task (readout), outperforming all baselines, including\ndense contrastive based method denseCL, showing the ad-\nvantage of our method for downstream dense prediction\ntasks.\nB. BDD100K Pre-training\nWe show in-domain pre-training on BDD100K instance\nsegmentation task in the main text. Here we show in-domain\ntransfer learning results on two more tasks: semantic segmen-\nPre-training (ResNet-50)\nEff.\nImageNet Cls.\nImageNet Seg.\nepoch\nLinear Top1\nReadout mIoU\nSupervised\n-\n79.3\n74.1\nDiscriminative Pretrain:\nSimCLR [12]\n4000\n62.5\n73.9\ndenseCL [63]\n1600\n63.6\n76.5\nMocoV2 [13]\n1600\n67.5\n76.3\nSimSiam [14]\n800\n69.8\n75.0\nSwAV [7]\n1200\n70.4\n76.1\nBYOL [27]\n1600\n71.7\n75.9\nGenerative Pretrain:\nBiGAN [22]\n-\n31.0\n-\nBigBiGAN [23]\n-\n56.6\n-\nSparK [60]\n1600\n54.1\n75.6\nDT-feat.distil w/ ADM [20]\n*600\n63.9\n79.3\nTable 12. ImageNet linear classificiation and semantic segmenta-\ntion performance. Our method with ADM includes 400 epochs of\ngenerative model pre-training and 200 epochs of feature distillation\nonto the backbone.\ntation and panoptic segmentation on BDD100K. We further\nshow transfer learning performance on Cityscapes.\nIn-domain transferring. Following the setup in the main\ntext, we investigate in-domain pre-training in the driving\ndataset BDD100K. We pre-trained backbones of all meth-\nods from scratch using 70k unlabeled images in BDD100K\ntraining set. Following the recommendation of [24], we pre-\ntrained contrastive based method BYOL and MIM based\nmethod SparK with longer pre-training epochs. We then fine-\ntune the pre-trained backbone on 7k labeled training dataset\non semantic segmentation and panoptic segmentation tasks.\nIn Table 13, we show DreamTeacher using StyleGAN2 and\nADM outperform BYOL and SparK pre-training on seman-\ntic segmentation and panoptic segmentation task, achieving\n60.3 mIoU and 21.8 PQ. However, it does not outperform\nImageNet supervised pre-training. One reason can be the un-\nlabelled driving dataset is small and lacks of rare objects and\nconcepts comparing to well-curated object-centred dataset\nImageNet.\nCityscapes transferring. Next, we evaluate pre-training on\nBDD100K [76], and transfer to the Cityscapes [19] semantic\nsegmentation benchmark, which contains 2,975 training im-\nages. While both are autonomous driving datasets, they have\na domain gap, being captured with different cameras and in\ndifferent cities/continents. Table 14 shows that our method\noutperforms all self-supervised baselines significantly. Com-\npared to the best-performing baseline, SparK, our perfor-\nmance gain increases when the number of available down-\nstream labeled examples decreases. We outperform SparK\nby 1.8%, 2.7%, 4.0% and 4.7%, when fine-tuning on 2,975,\n744, 374 and 100 labeled examples.\n12\nPre-training (ResNet-50)\nPT\nEff.\nSeg.\nPan.\ntask\nepoch\nmIoU\nPQ\nSupervised [76]\n-\n-\n61.1\n22.4\nBYOL [27]\nCL\n5000\n58.4\n20.9\nSparK [58]\nMIM\n2500\n56.9\n20.2\nDT-feat.distil. w/ StyleGAN2 [37]\nGEN\n*900\n59.7\n21.5\nDT-feat.distil. w/ ADM [20]\nGEN\n*900\n60.3\n21.8\nTable 13. In-domain pre-training on BDD100k. We follow the rec-\nommendation of [24] to pre-train contrastive and masking based\nself-supervised method with long schedule for small dataset like\nBDD100k with 70k train images. For semantic segmentation, we\nuse UperNet [68] following official implementation from [76]. Re-\nported number is mean IoU at single sacle. For panoptic segmen-\ntation, we use panoptic FPN [39], fine-tune for 36(3\u00d7) epochs,\nreported number is Panoptic Quality (PQ).\nPre-training (RN-50)\nPT\nEff.\n1(2,975) 1/4(744) 1/8(372) 1/30(100)\ntask epoch\nIN sup init.\n-\n-\n78.7\n71.3\n65.4\n54.4\nfrom stratch\n-\n-\n70.9\n41.8\n40.7\n36.7\nBYOL [27]\nCL\n5000\n73.7\n54.3\n49.9\n44.4\nSpark [58]\nMIM 2500\n75.7\n61.2\n56.0\n45.3\nDT-feat.distil (ADM) GEN *900\n77.5\n63.9\n60.0\n50.0\nTable 14. Transfer learning: BDD100K to Cityscapes semantic segmen-\ntation task. We pre-trained baselines including ours with unlabeled\nBDD100K images, and finetuned the backbone on Cityscapes at\nvarying number of labels. Here, we show our method learned trans-\nferable features, and achieves the best results in all data portions.\nReported numbers are mean IoU, note that effective epochs of our\nmethod includes 300 epochs generative model pre-training and 600\nepochs feature distillation pre-training.\nPre-training (ResNet-50)\nFID (IN1k)\nPre. Data\nADE20k(mIoU)\nDT-feat.distil. w/ BigGAN\n25.3\nSynthetic\n40.8\nDT-feat.distil. w/ ICGAN\n17.0\nSynthetic\n41.2\nDT-feat.distil. w/ ADM\n26.2\nReal\n42.5\nTable 15. Ablation study with different unconditional generative\nmodels on ImageNet using DreamTeacher.\nC. Generative Models Analysis\nHere we include additional analysis on the effect of using\ndifferent generative models with DreamTeacher. In Table 15,\nwe show DreamTeacher with different generative models\non ImageNet. For GAN-based models, ICGAN has better\ngenerative modelling performance compared to BigGAN\nin terms of FID (17.0 and 25.3). The backbone pre-trained\nwith ICGAN also has better transfer learning performance in\nADE20K (41.2 and 40.8). This observation is in line with the\nempirical results shown in BigBiGAN paper, which found\nthat generative models with lower FID obtain higher Ima-\ngeNet classification accuracy. For GANs, we use synthesized\ndata to pre-train the image backbone. For diffusion models,\nwe use encoded data (real images, see Sec. 3.1 main paper),\nwhich makes FID scores of generated data less informative.\nIn fact, DreamTeacher with diffusion models performs better\nin transfer learning, despite ADM\u2019s lower generation FID.\nD. Architecture\nHere we provide additional implementation details about\nour method\u2019s architecture.\nD.1. Implementation: Feature Regressors\nOur Feature Regressor is implemented as a Feature Pyra-\nmid Network (FPN) [42] with additional Pyramid Pooling\nModule (PPM) [82]. We use pool scale at 1,2,3,6. We add\nbatch normalization and ReLU activation for both the lateral\nconnection and the bottom-up convolution blocks in FPN.\nWe use feature channel size 256. We also add 1x1 conv layer\nat the end of each level to map the output feature channel to\nthe generator\u2019s feature channel.\nD.2. Implementation: Feature Interpreter\nFeature Interpreter is implemented as a series of Feature\nFuse layer to map and fuse generator\u2019s features into a logit\nmap. Feature Fuse Layer is implemented by a block of 1x1\nconv, bilinear upsampling, concatenation, and Depth-wise\nSeparable Convolution [17]. We applied Group Norm [67]\nwith group number 32 and Swish Activation [49] in-between\nblocks. The feature dimension is 256. We also apply dropout\nwith rate 0.1 before the 1x1 conv mapping to the output\nlogits.\nE. ImageNet Benchmarks\nE.1. Implementation: generative models\nFor GAN based generative models, we use pre-trained\nunconditional BigGAN [5] from this repository1, ICGAN [9]\nfrom this repository2. For diffusion based generative model,\nwe use ADM without classifier guidance, pre-trained with\nresolution 256x256 from this repository 3. Please also refer\nto Table 16 for the hyperparameters used in training the\ndiffusion model on ImageNet.\nE.2. Implementation: pre-training\nWe pre-trained ResNet-50 backbone by diffusing images\nfor 150 steps and running a single denoising step to extract\nthe ADM\u2019s UNet decoder features at blocks 3,6,9, and 12.\nWe use the LAMB [75] optimizer with batch size of 2048 and\nlr = 4e\u22123 with cosine decay learning rate schedule and pre-\ntrained for 200 epochs on the ImageNet training set without\nclass labels. We center crop images into resolution 256x256\nand only apply horizontal flipping as the data augmentation.\nPlease see Table 17 for other hyperparameters.\n1https://github.com/lukemelas/pytorch-pretrained-gans\n2https://github.com/facebookresearch/ic gan\n3https://github.com/openai/guided-diffusion\n13\nLSUN\nFFHQ\nImagetNet\nBDD100K\nresolution\n256x256\n256x256\n256x256\n128x256\ndiffusion steps\n1000\n1000\n1000\n1000\nnoise Schedule\nlinear\nlinear\nlinear\nlinear\nchannels\n256\n256\n256\n128\ndepth\n2\n2\n2\n2\nchannels multiple\n1,1,2,2,4,4\n1,1,2,2,4,4\n1,1,2,2,4,4\n1,1,2,2,3,4\nheads Channels\n64\n64\n64\n32\nattention resolution\n32,16,8\n32,16,8\n32,16,8\n16,8\ndropout\n0.1\n0.1\n0.0\n0.0\nbatch size\n256\n256\n256\n256\nlearning rate\n1e-4\n1e-4\n1e-4\n1e-4\nTable 16. Hyperparameters for diffusion models used in the paper.\nE.3. Implementation: downstream tasks\nImageNet: classification fine-tuning. For ResNet50, we fol-\nlow [58] to use the latest open-source ResNet A2 schedule\nfrom [65]. For ConvNeXt-B, we use the official implementa-\ntion. We did not tune the hyper-parameters from [58]. Please\nrefer to SparK official github repo4 for the hyper-parameters.\nADE20K: semantic segmentation. We use UperNet imple-\nmented in MMSegmentation for training ADE20K seman-\ntic segmentation task. We use the default hyperparameters\nfrom MMSegmentation and train for 160K iterations. We\nuse the same hyperparameters for both the baselines and\nour methods. After finetuning, we evaluate the performance\non ADE20K validation set without using multi-scale flip\naugmentation.\nMSCOCO: instance segmentation. For ResNet-50, we\nfollow [58] to use Mask R-CNN with R50-FPN backbone\nimplemented in Detectron25 for MSCOCO instance segmen-\ntation task. We use the same configuration as in SparK [58] to\nfinetune the pre-trained backbone for 12 epochs (1\u00d7 sched-\nule) or 24 epochs (2\u00d7) and evaluate performance on the\nMSCOCO 2017 validation set. For ConvNeXt-B, we follow\nthe configuration of iBOT [84] to use Cascade Mask R-CNN,\nfine-tune for 12 epochs. Following the convention, we do not\nuse multi-scale testing, large-scale jittering augmentation, or\nsoft-NMS in all our COCO experiments.\nBDD100K: instance segmentation. We use Mask R-CNN\nwith R50-FPN backbone implemented in MMDetection for\nBDD100K instance segmentation task. We use the official\ndata processing script from this repository6 and finetuned pre-\ntrained backbone with 36 epochs (3x schedule) and evaluate\nperformance on BDD100K validation set.\n4https://github.com/keyu-tian/SparK/\n5https://github.com/facebookresearch/detectron2\n6https://github.com/SysCV/bdd100k-models/tree/main/ins seg\nF. BDD100K Benchmarks\nF.1. Implementation: Generative Models\nStyleGAN2. We resize BDD100K images from the original\nresolution 720x1280 to 512x1024 when training the GAN.\nWe use the default configuration from [37] without adaptive\naugmentation. We empirically found that turning off path\nlength regularization and style mixing loss improves data\nsample quality on the BDD100k driving scenes. We train\nthe network with batch size 32 and gamma = 10.0 until\nconvergence.\nDiffusion Models. We train our own diffusion model on\nBDD100K, and we summarize training hyperparameters\nin table 16. We resize BDD100K images from the original\nresolution 720x1280 to 128x256 resolution. We use diffusion\nmodel architecture builds on the U-Net by [20], and we use\nlinear noise schedule from 1e \u2212 4 to 2e \u2212 2.\nF.2. Implementation: Pre-training\nWhen DreamTeacher pre-training using the StyleGAN2\ngenerator, we use resolution 512x1024. We use resolution\n128x256 for the ADM generator, same resolution as in the\ngenerative model training. We use AdamW [46] optimizer\nwith learning rate lr = 4e \u2212 3 and cosine decay learning\nschedule to train 600 epochs. Note that we only use horizon-\ntal flip as the augmentation during pre-training. Please see\nTable 17 for information about the hyperparameters.\nF.3. Implementation: Downstream Tasks\nFor all three tasks, we use the official data split and dataset\nconfiguration from the official BDD100K repository 7.\nSemantic segmentation. We use UperNet [68] implemented\nin MMSegmentation 8 for evaluating the semantic segmenta-\ntion task on BDD100K. We train UperNet with the pre-\ntrained ResNet-50 backbone using SGD optimizer with\nlr = 0.01, momentum=0.9, weight decay=0.0005, and poly\nlearning rate schedule with power=0.9 for 80k iterations. We\n7https://github.com/SysCV/bdd100k-models\n8https://github.com/open-mmlab/mmsegmentation\n14\nLSUN\nFFHQ\nImagetNet\nBDD100K\nresolution\n256x256\n256x256\n256x256\n128x256/512x1024\nnoise steps\n50\n50\n150\n50\noptimizer\nAdamW\nAdamW\nLAMB\nAdamW\nbase learning rate\n4e-3\n4e-3\n4e-3\n4e-3\nweight decay\n0.05\n0.05\n0.05\n0.05\noptimizer momentum\n\u03b21, \u03b22 = 0.9, 0.95\n\u03b21, \u03b22 = 0.9, 0.95\n\u03b21, \u03b22 = 0.9, 0.95\n\u03b21, \u03b22 = 0.9, 0.95\nbatch size\n256\n256\n2048\n256\ntraining epochs\n100\n100\n200\n600\nlearning rate schedule\ncosine decay\ncosine decay\ncosine decay\ncosine decay\nwarmup epochs\n20\n20\n30\n30\naugmentation\nhorizontal flip\nhorizontal flip\nhorizontal flip\nhorizontal flip\nTable 17. Hyperparameters for pre-training the image backbones.\nuse the same training hyperparameters for both the baseline\nmodels and our models. After finetuning, we evaluate the\nmodel on BDD100K validation set without multi-scale flip\ntest-time augmentation.\nInstance segmentation. We use Mask R-CNN [30] with\nR50-FPN backbone implemented in MMDetection9 for eval-\nuating the instance segmentation task on BDD100K. We\nonly use the pre-trained weights of ResNet-50 during fine-\ntuning and the rest of the networks are randomly initialized.\nWe use SGD optimizer with lr = 0.02, momentum=0.9, and\nweight decay=0.0001 to train Mask R-CNN for 36 epochs\n(3x schedule). We use the same training hyperparameters for\nboth the baseline models and our models.\nPanoptic segmentation. We use PanopticFPN [39] with\nR50-FPN backbone implemented in MMDetection for eval-\nuating panoptic segmentation task on BDD100K. We only\nuse the pre-trained weights of ResNet-50 during finetuning\nand the rest of the networks are randomly initialized. We\nuse the SGD optimizer with lr = 0.02, momentum=0.9, and\nweight decay=0.0001 to train PanopticFPN for 36 epochs\n(3x schedule). We use the same training hyperparameters for\nboth the baseline models and our models.\nCityscapes transfer learning. In this experiment, both the\nbaselines and our method use ResNet-50 backbone pre-\ntrained on 70k unlabeled images from the BDD100K training\nset. After pre-training, we finetuned the backbone with UPer-\nNet [68] for semantic segmentation tasks in Cityscapes. We\nfollow the public split [16] to split 2,975 training images into\n1/4, 1/8, and 1/30 subsets. We use MMSegmentation to train\nUperNet with 80k iterations for the full set and 20k iterations\nfor the subsets. We use the default hyperparameters for all\nthe baselines and our method. After finetuning, the model is\nevaluated on the official validation set (5,000 images).\n9https://github.com/open-mmlab/mmdetection\nG. Label-Efficient Benchmarks\nG.1. Implementation: generative models\nFor LSUN cat, horse and bedroom dataset, we use pre-\ntrained ADM models from the guided-diffusion models\u2019\nrepository10. For FFHQ, we use pre-trained model from this\nrepository 11, following DDPM-seg [32]. Please see Table 16\nfor the hyperparmeters used in training the diffusion models.\nG.2. Implementation: Feature Interpreter\nWe train the feature interpreter by first diffusing the real\nimages with 50 time steps, and then extract the ADM\u2019s\nfeatures by running one step of denoising. We use ADM\u2019s\nUNet decoder features at block 3,6,9, and 12. We then train\nthe feature interpreter branch with AdamW optimizer with\nlr = 4e\u22123, weight decay 0.05, \u03b21, \u03b22 = 0.9, 0.95, warm-up\nepochs 20 and train for 100 epochs. We only use horizontal\nflip when training the feature interpreter.\nG.3. Implementation: pre-training\nWe use the AdamW optimizer with learning rate lr =\n4e\u22123 and cosine decay learning schedule to train 100 epochs.\nPlease see Table 17 for hyperparmeters for different datasets.\nNote that we only use horizontal flipping as data augmenta-\ntion during pre-training.\nG.4. Implementation: downstream tasks\nWe use UperNet implemented in MMSegmentation for\nsemantic segmentation tasks. We use the default hyperpa-\nrameters in MMSegmentation. For all four tasks, we train\nUperNet using 20k iteration schedule. For feature distilla-\ntion, only the backbone is initialized from pre-trained weight,\nand the rest are randomly initialized. For mix-distillation,\nwe initialize the backbone as well as the UperNet with our\npre-trained weights.\n10https://github.com/openai/guided-diffusion\n11https://github.com/yandex-research/ddpm-segmentation\n15\nInstance Seg.\nSemantic Seg.\nImage\ndenseCL IN-1k-1M pre-trained\nDT-feat.distil. BDD100K pre-trained\nDT-feat.distil. IN-1k-1M pre-trained\nFigure 6. Qualitative results on BDD100k Inst./Sem. Seg. Compared with denseCL, our method pre-trained on ImagetNet predicts the correct box on\npedestrians and occluded cars, and the mask boundaries are clearer. On semantic segmentation (second row), our prediction segments traffic signs and thin\nobjects like poles. We blur pedestrian faces in the figure, while the methods make predictions on original images.\nH. Visualization\nWe\nfirst\nprovide\nqualitative\ncomparison\nof\nin-\nstance/sementic segmentation in BDD100K with baseline\nand then show visualization of ADM denoising network fea-\ntures at different resolution of driving scenes in BDD100K\nand feature activation maps of a pre-trained ResNet50\nbackbone using DreamTeacher on ImageNet1k. We then\nshow semantic segmentation results on FFHQ, LSUN-cat,\nhorse and bedroom. The backbone is ConvNX-b pre-trained\nwith our mix-distillation method. We also show semantic,\ninstance and panoptic segmentation results of our method on\nBDD100K. Note that results of all three tasks are pre-trained\nusing our DreamTeacher feature distillation method on\nResNet-50 backbone. We show results on pre-training on\nBDD100K in-domain data solely and ImageNet-1k-1M as\ngeneral domain data.\nH.1. Qualitative Comparison\nIn Figure 6, we show qualitative results on BDD100k in-\nstance/semantic segmentation task. Comparing to denseCL,\nour DreamTeacher pre-training method performs better at\nsmall/thin objects like traffic signs and poles when fine-tuned\non downstream tasks.\nH.2. Feature Activation Maps\nIn Figure 7, we show feature activation maps at different\nresolution blocks and different noise steps from ADM [20]\npre-trained on BDD100k without classifier guidance. We\nvisualize multi-scale features at different resolution blocks,\nshowing features in lower resolution focus on structures, and\nfocus on parts in higher resolution. In Figure 8, we show\nfeature activation maps on ResNet50 backbone pre-trained\nwith DreamTeacher, the backbone learns coarse features at\nlower level layers and finer features at higher level layers.\nReal Images\nRes 1/32\nRes 1/16\nRes 1/8\nRes 1/4\nFigure 7. ADM feature visualization (BDD100k). BDD100k image\nis first diffused by 50 steps and we run one denoising step of the\nADM model to extract the feature. We see that features in the low\nresolution block focus on scene layouts and objects, and in higher\nresolution, they focus on parts like car wheels, and traffic lights.\nFigure 8. DreamTeacher pre-trained ResNet50 backbone feature ac-\ntivation maps on ImageNet images. From left to right, we show the\nimage and features at 1/32, 1/16, and 1/8 input resolution.\nH.3. Label-Efficient Semantic Segmentation\nIn Figure 9, we show semantic segmentation results on\nFFHQ unlabeled images. In Figure 10, we show semantic\nsegmentation results on LSUN-bedroom unlabeled images.\nIn Figure 11, we show semantic segmentation results on\nLSUN-cat unlabeled images. In Figure 12, we show seman-\ntic segmentation results on LSUN-horse unlabeled images.\nFrom the visulization, our method is robust to cat in different\npose, multi objects occurs in the same images (horse/cat).\n16\nH.4. BDD100K: semantic segmentation\nIn Figure 13, we show semantic segmentation results on\nBDD100K, pre-trained by DreamTeacher feature distillation\non BDD100K unlabeled dataset. And in figure 14, we show\nresults pre-trained on ImageNet unlabeled dataset with our\nmethod. Comparing to BDD100K pre-trained, ImageNet pre-\ntrained method works better with rare and small objects like\nrider and traffic lights.\nH.5. BDD100K: instance segmentation\nIn Figure 15, we show instance segmentation results on\nBDD100K, pre-trained by DreamTeacher feature distillation\non BDD100K unlabeled dataset. and in figure 16, we show\ninstance segmentation results on BDD100 on ImageNet un-\nlabeled dataset. As a comparison, model pre-trained on Ima-\ngeNet detect and segment small objects better.\nH.6. BDD100K: panoptic segmentation\nIn Figure 17, we show panoptic segmentation results on\nBDD100K, pre-trained by DreamTeacher feature distillation\non BDD100K unlabeled dataset. And in figure 18, we show\npanoptic segmentation results on BDD100K, pre-trained on\nImageNet unlabeled dataset. Note that model pre-trained\nwith BDD100K performs well on things class like road and\ntree etc, but model pre-trained on ImageNet gets clearer\nboundary, especially for small objects.\n17\nFigure 9. Semantic segmentation: FFHQ with 34 classes. Qualitative results of our ConvNX-B model pre-trained with DreamTeacher-\nfeature distillation on FFHQ unlabelled images.\n18\nFigure 10. Semantic segmentation: LSUN-bedroom with 28 classes. Qualitative results of our ConvNX-B model pre-trained with\nDreamTeacher-feature distillation on LSUN-bedroom unlabelled images.\n19\nFigure 11. Semantic segmentation: LSUN-cat with 15 classes. Qualitative results of our ConvNX-B model pre-trained with DreamTeacher-\nfeature distillation on LSUN-cat unlabelled images.\n20\nFigure 12. Semantic segmentation: LSUN-horse with 21 classes. Qualitative results of our ConvNX-B model pre-trained with\nDreamTeacher-feature distillation on LSUN-horse unlabelled images.\n21\nFigure 13. BDD100K semantic segmentation visualization: pre-trained with DreamTeacher feature distillation on BDD100K. The\nbackbone is resnet-50, finetuned using UperNet.\n22\nFigure 14. BDD100K semantic segmentation visualization: pre-trained with DreamTeacher feature distillation on IN1k-1M. The\nbackbone is resnet-50, finetuned using UperNet. Only the backbone weight is pre-trained, other part of the networks are randomly initialized.\n23\nFigure 15. BDD100K instance segmentation visualization: pre-trained with DreamTeacher feature distillation on BDD100K. The\nbackbone is resnet-50, finetuned using Mask R-CNN. Only the backbone weight is pre-trained, other part of the networks are randomly\ninitialized.\n24\nFigure 16. BDD100K instance segmentation visualization: pre-trained with DreamTeacher feature distillation on IN1k-1M. The\nbackbone is resnet-50, finetuned using Mask R-CNN. Only the backbone weight is pre-trained, other part of the networks are randomly\ninitialized.\n25\nFigure 17. BDD100K panoptic segmentation visualization: pre-trained with DreamTeacher feature distillation on BDD100k. The\nbackbone is resnet-50, finetuned using PanopticFPN. Only the backbone weight is pre-trained, other part of the networks are randomly\ninitialized.\n26\nFigure 18. BDD100K panoptic segmentation visualization: pre-trained with DreamTeacher feature distillation on IN1k-1M. The\nbackbone is resnet-50, finetuned using PanopticFPN. Only the backbone weight is pre-trained, other part of the networks are randomly\ninitialized.\n27\n"
  },
  {
    "title": "DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations",
    "link": "https://arxiv.org/pdf/2307.07047.pdf",
    "upvote": "15",
    "text": "Does Collaborative Human\u2013LM Dialogue Generation\nHelp Information Extraction from Human Dialogues?\nBo-Ru Lu\u2660\u2217 Nikita Haduong\u2660\u2217 Chia-Hsuan Lee\u2660 Zeqiu Wu\u2660 Hao Cheng\u2661\nPaul Koester\u2665 Jean Utke\u2665 Tao Yu\u2663 Noah A. Smith\u2660\u2662 Mari Ostendorf\u2660\n\u2660University of Washington\n\u2661Microsoft Research\n\u2665Allstate\n\u2663University of Hong Kong\n\u2662Allen Institute for AI\n{roylu,chiahlee,zeqiuwu1,ostendor}@washington.edu chehao@microsoft.com\n{pkoes,jutke}@allstate.com tyu@cs.hku.hk {qu,nasmith}@cs.washington.edu\nAbstract\nThe capabilities of pretrained language mod-\nels have opened opportunities to explore new\napplication areas, but applications involving\nhuman-human interaction are limited by the\nfact that most data is protected from public re-\nlease for privacy reasons. Problem-solving hu-\nman dialogues in real applications can be much\nmore complex than existing Wizard-of-Oz col-\nlections, preventing successful domain transfer.\nTo support information extraction (IE) for a pri-\nvate call center dataset, we introduce a human-\nin-the-loop dialogue generation framework ca-\npable of synthesizing realistic dialogues. In\nIE experiments with auto insurance call center\ndialogues, we observe 25% relative improve-\nment in F1 after augmenting a small set of real\nhuman conversations with synthetic data. We\nrelease code and our synthetic dataset to illus-\ntrate the complexity of real-world call center\nconversations and encourage development of\ncomplex dialogue datasets that are more repre-\nsentative of natural data.\n1\nIntroduction\nRapid advances in natural language processing\nhave driven interest in its use in a wide variety of\ndomains. However, applications involving human-\nhuman interaction, such as call center dialogues,\nhave seen limited success. One reason is that nat-\nural problem-solving dialogues are not typically\npublicly available for privacy reasons, restricting\nopportunities for researchers to explore methods in\nadvancing applications for these domains. Further,\nannotating private datasets can be expensive be-\ncause of the need for in-house expertise, so training\nresources are limited. In this paper, we introduce a\nmethod to fill the data gap using synthetic data gen-\nerated by a collaborative human\u2013language model\nframework. Specifically, we experiment with a\ntask of extracting information from auto insurance\n\u2217Equal contribution.\nYes, I picked up a passenger that \nasked me to take him to Santa \nMonica Pier. I was taking the I-10 and \nI pulled over onto the shoulder to \ncheck on the engine because I \nthought I heard a rattling noise. I \ncouldn't \ufb01nd anything, but then when \nI got back in the car and was \npreparing to start driving again, a \nSubaru BRZ suddenly collided with \nme.\nCan you provide me with more \ndetails about the accident?\nCan you tell me more about the \ndamages to both vehicles?\nYes, my car su\ufb00ered signi\ufb01cant \ndamage to the front bumper, hood, \nand headlights. The Subaru BRZ had \ndamage to its left door and front \nfender.\nSarah, can you remind me how many \npassengers were in the car with you \nat the time of the accident?\nOops, sorry about that. I \nmisremembered. There were actually \ntwo passengers in the car with me.\nGlobal\n# Involved Cars\n2\nLocation\nHighway\nCaller\nUber/Lyft\nYes\nDest. of Trip\nSanta Monica Pier\nPurpose of Trip\nI picked up \u2026\nCar Motion\nStopped\n# Passengers\n1\nOther Driver\nMake/Model\nSubaru BRZ \n\u2026\n\u2026\nCaller\nDamage Part\nFront\nOther Driver\nMake Model\nSubaru BRZ\nDamage Part\nLeft, Front\nCaller\n# Passengers\n2\nClient\nAgent\nFigure 1: An illustrative snippet of our dialogue with\nentity-slot-value triples. Yellow is the slot with multiple\nvalues. Italic blue and yellow are the same slot (Dam-\nage Part) with different entities (e.g., Caller and Other\nDriver). Red is a slot with a value update.\ncall center dialogues, using public synthetic data to\nimprove performance on a private dataset.\nMany available dialogue datasets are designed\nfor training virtual agents, collected using pairs\nof humans to perform a task (Budzianowski et al.,\n2018; Rastogi et al., 2020; Chen et al., 2021). De-\nsigning for human-machine interaction results in di-\nalogues that lack the complexity of human-human\ndialogues. Additionally, human-only data collec-\ntion can have limited content diversity, result in im-\nbalanced training sets, and does not scale to more\ncomplex tasks, due to the high cost of employ-\ning domain experts (Geva et al., 2019; Gururangan\net al., 2018; Qian et al., 2021).\nTo reduce data collection costs, researchers have\nexplored using language models (LMs) to generate\nsynthetic training data (Bao et al., 2023; Guan et al.,\narXiv:2307.07047v2  [cs.CL]  20 Feb 2024\n2018; He et al., 2022; Li et al., 2023; Zeng et al.,\n2018; Thambawita et al., 2022). Synthesized data\ncan target long-tail phenomena (Chu et al., 2020)\nand allow for public release of data that closely\nemulates real-world privacy-constrained domains,\nsuch as the medical domain (Park et al., 2018).\nWhile LMs can follow instructions to generate text\nthat closely resembles human writing, there can be\nchallenges ensuring the data is diverse and not too\nsimplistic (Dahmen and Cook, 2019; Stahlberg and\nKumar, 2021; Liu et al., 2022a). In addition, they\nstill suffer from issues with incoherence and con-\nsistency (Clark et al., 2021; Dou et al., 2022). To\nprotect against LM errors, collaborative human-LM\nframeworks have been designed for tasks involving\nshort dialogues and texts (Liu et al., 2022a; Bonaldi\net al., 2022). In contrast, we investigate using a\nhuman-in-the-loop framework to create lengthy\nand complex dialogues.\nOur work proposes a human-LM collaborative\nframework for dialogue generation (DIALGEN)\nthat leverages the scalability and creativity of gen-\nerative models, yet retains controllability through\nhumans. Human collaborators edit the synthesized\ndialogues, which we use to boost information ex-\ntraction performance on real-world call center data.\nMany call center dialogues involve problem solv-\ning where customers provide information to an\nagent through question-answer pairs and clarifi-\ncations that need to be interpreted in the context\nof the dialogue history. Our information extraction\n(IE) task is thus framed as an iterative informa-\ntion update after each agent-customer exchange,\nanalogous to dialogue state tracking (DST) in task-\noriented dialogues. However, unlike DST, the in-\nformation extracted from each turn is collected to\ncreate a summary of the call rather than to generate\na virtual agent\u2019s response or make an API call. In\naddition, the state includes entities that are associ-\nated with attributes (slots) and values. To evaluate\nmodels on this IE task, we introduce entity-centric\nscoring methods that allow for partial matching of\nmultiple and descriptive values.\nWe demonstrate the effectiveness of DIALGEN\nby generating data in auto insurance calls, a domain\nwith privacy restrictions preventing public release\nof natural calls, and performing information extrac-\ntion. We work with a private dataset containing 34\ndialogues with an average 197 utterances per dia-\nlogue and synthesize 235 dialogues with an average\n46 utterances per dialogue. Experiments in our IE\ntask show the additional synthetic data improves\nmodel performance by 25% in the full F1 score.\nTo summarize, our main contributions are:\n\u2022 We design DIALGEN, a collaborative human-\nLM framework for generating complex task-\noriented dialogues in domains where privacy\nconstraints have previously prevented data\nsharing with the research community. Syn-\nthetic data, training documentation, prompts,\nand interface code will be released.1\n\u2022 We present DIALGEN-AIC, a custom dataset\ndesigned to illustrate the complexity of real-\nworld auto insurance call center data. While\nnot intended as a benchmark, DIALGEN-AIC\naims to provide a demonstration for the com-\nplex nature of real conversations and the chal-\nlenges faced in this domain, including linking\ninformation with different entities and track-\ning multiple values in a single slot.\n\u2022 We propose an entity-centric scoring method-\nology that considers information links to dif-\nferent entities, allows for multiple slot values,\nand provides partial match scores for descrip-\ntive values.\n2\nDialogue Generation (DIALGEN)\nAs shown in Figure 2, our DIALGEN framework\nis designed to generate schema-guided dialogues\nthrough human-LM collaboration. An LM is se-\nlected as the backbone, then the data generation\nprocess begins with an initial task prompt consist-\ning of natural language description for the desired\ndialogue (e.g., task description, desired slots, story,\nand personalities) and dialogue history. During\neach iteration, the LM first proposes a candidate\nsubdialogue based on the history (the initial task\nprompt and the generated conversation so far). Hu-\nman reviewers with sufficient domain knowledge\nthen validate, edit, and annotate the generated sub-\ndialogue, before requesting a continuation via an\nupdated prompt to the LM. The reviewers can op-\ntionally augment the prompt with a specific instruc-\ntion related to the desired dialogue flow. This pro-\ncess repeats until the dialogue is complete. At a\nhigh level, the human-in-the-loop mechanism en-\nsures that the resulting dialogues are coherent and\nconsistent with the prompt, covering desired con-\ntent and fulfilling style specifications from domain\n1https://boru-roylu.github.io/DialGen\nFull Dialogue\nSubdialogue Generation\nSlot\nValues\nLocation \nparking lot, \ndriveway, \nhighway, \nintersection\nTraffic \nCondition\nheavy, \nmoderate,  \nlight\n\u2026\n\u2026\nTask Description \nTriplets \nStory \nPersonalities\nPrompt Creation\nRegenerate\nRevise\nAgent:\nHi, thank you for calling!\nCaller:\nI want to file a claim.\n\u2026\nAgent:\nWhat is the make/model \nof your car?\nCaller:\nIt\u2019s an orange 2015 \nHonda Accord.\n\u2026\nAgent:\nAlright then, take care, \nAndrew, and let us know \nif you need any further \nassistance.\nCaller:\nThank you, I will.\nAgent: What is the make/model of your car?\nCaller: It\u2019s an orange sedan 2015.\nDialogue History\nAgent: What is the make/model of your car?\nCaller: It\u2019s an orange sedan 2015 Honda Accord.\nAgent:\n\u2026\nCaller:\n\u2026\nOntology\nFigure 2: In the DIALGEN framework, a language model (LM) and a human reviewer collaborate to generate a\ndialogue. First, a story is created by the LM, using randomly sampled entity-slot-value triplets from the ontology.\nSecond, the LM generates a subdialogue, using a task description, triplets, story, personalities, and dialogue history.\nThe reviewer evaluates how the subdialogue fits with the task requirements and dialogue history. If not satisfied, the\nreviewer can have the LM regenerate the subdialogue before revising it. The revised subdialogue is added to the\ndialogue history for generating the next subdialogue. This iterative process continues until the dialogue is complete.\nexperts. In the following, we describe each compo-\nnent of DIALGEN in detail.\n2.1\nPrompt for Dialogue Generation\nThe prompt for generating synthetic dialogues\nincludes: the task description, entity-slot-value\ntriplets, story, personality and dialogue history.2\nTask Description.\nSimilar to task descriptions\ngiven to humans in Wizard-of-Oz setups (Kelley,\n1984), the template-based task description gives\nthe information about dialogue participants and the\ntask scenario for the conversation, such as having\nthe LM role-play as a user calling to file a claim\nwith an agent at an insurance company, e.g., \u201cRole\nplay car accident claim call. One person is an\nagent Alice from a car insurance company and the\nother is the caller Bob who wants to file a claim.\u201d\nEntity-slot-value Triplets.\nWe randomly sample\nentity-slot-value triples from the expert-authored\nontology to steer the LM to generate required con-\ntent in the dialogue, enabling precise covering of\nspecific information, e.g., (Caller, Injury, Neck).\nStory.\nKim et al. (2022a) synthesize social dia-\nlogues from common sense knowledge triples by\nfirst using a social narrative to set up the scenario.\nWe similarly use the randomly sampled triplets\nto generate a story with the LM before the dia-\nlogue generation. For example, the aforementioned\nentity-slot-value triple will be converted into the\nsnippet of a story: \u201cThe impact of the collision\ncaused Bob\u2019s car to spin around and come to a\n2An example of a full prompt is given in Appendix B.1.\nstop. He immediately felt a sharp pain in his neck\nand knew that something was wrong.\u201d\nPersonality.\nTo enrich the diversity of callers,\nwe randomly sample a personality from the prede-\nfined list (Table 7) for each dialogue, e.g., \u201cBob\nis feeling distressed or frustrated due to the acci-\ndent and its consequences.\u201d For the agent, we use\nthe same personality for all dialogues, e.g., \u201cAlice\nis conversational, personable, patient, empathetic,\nsympathetic and professional.\u201d\nDialogue History.\nThe LM uses the full dialogue\nhistory to generate subdialogue turns that are con-\nsistent with the flow of the conversation. During the\nsubdialogue generation process, we append com-\npleted subdialogues before generating the next sub-\ndialogue. The initial dialogue history is always\none exchange, e.g., \u201cAlice: Hi, thank you for call-\ning DialGen Insurance! This is Alice. How may I\nhelp you today?\u201d followed by \u201cBob: I am calling\nregarding a car accident.\u201d\n2.2\nSubdialogue Generation\nThe dialogue is generated iteratively where each\nsubdialogue is revised and annotated by a reviewer.\nHuman-in-the-loop Review.\nSubdialogues are\nindividually revised by a human trained to correct\ncommon LM errors such as those described by Dou\net al. (2021), verify that required information is\npresent (the sampled triples), and edit the text to\nmeet stylistic criteria (e.g., adjusting tone). The re-\nviewer can either revise individual turns directly or\ninstruct the LM to regenerate specified turns, e.g.,\n\u201cHave the caller correct earlier incorrect informa-\ntion\u201d (more examples in Table 6). The LM may\ntry to end the dialogue by including termination\nsignals such as \u201cgood bye.\u201d If the LM ends the\ndialogue without covering the required triplets, the\nreviewer can delete and regenerate the turns.\nAnnotation.\nSpans in the subdialogue that have\ninformation tuples associated with the task ontol-\nogy are annotated by the human reviewer. If a tuple\nin turn t has a slot with the same referent and a\ndifferent value than a previous turn, the reviewer\nis asked to resolve the duplication by indicating\nwhether the new value is a correction UPDATE,\nKEEP, or additional detail to be concatenated with\nthe previous value CONCAT. After annotation, the\nreview can choose to generate another subdialogue\nor accept the ending that the LM has proposed.\nThis annotation step is optional and can be decou-\npled from the DIALGEN framework depending on\nthe target tasks or domains.\n3\nProblem Definition and Evaluation\nAuto insurance call center dialogues involve cus-\ntomers working together with an agent to address\nan issue or submit a claim. As the conversation pro-\ngresses, extracted information must be iteratively\nupdated. This updating process is similar to the\nconcept of dialogue state tracking (DST) used in\ntask-oriented dialogues. However, unlike standard\nDST, the extracted information is used to summa-\nrize the call, not to make API calls or generate\nresponses by a virtual agent.\n3.1\nProblem Definition\nExtracted structured information is typically rep-\nresented as a collection of tuples {(s, v), s \u2208 S},\nwhere s is a slot label, v is the associated value,\nand S is the full set of slots in the ontology. Values\ncan be associated with a slot-dependent restricted\nset Vs or free-form text (e.g., a home address) or\nnull. For multi-domain systems where different\ndomains share some but not all slots (e.g., many do-\nmains have a date slot), the domain d is separately\ntracked: {(d, s, v), d \u2208 D, s \u2208 S}. The full set of\ntuples is updated after each agent-user exchange to\nsupport construction of application calls needed to\ncomplete the task.\nWe formalize the our information extraction task\nas follows. Ignoring domain for brevity, define\n(A, U)t as the pair of agent and user turns at ex-\nchange t. Given a sequence of exchanges between\nand agent and a user, {(A, U)1, . . . , (A, U)t}, find\nthe dialogue state {(s, v), s \u2208 St}, where St is the\nsubset of slots active at time t (i.e., having non-null\nvalues). The state associated with the final turn T\neffectively provides a summary of the information\nextracted from the user in the dialogue.\n3.2\nDefinition of Extracted Information\nTo accommodate the complexities of our dia-\nlogues, we augment DST problem in three ways.\nFirst, we introduce the notion of a \u201creferent\u201d, ei-\nther with the global context or the entity that the\nextracted information is associated with. Second,\nwe allow slots to take on multiple values. Lastly,\nwe allow slot values to be updated in multiple ways:\na value can be corrected by the user, a new value\ncan be added to form a list, or an existing value\ncan be augmented, e.g., with details expanding on\na free-form slot. For example, Figure 1 provides an\nexample of an agent gathering information about an\naccident together with the extracted tuples. There\nare three referents (Global context, Caller, and\nOther Driver); the number of passengers in the\ncaller\u2019s vehicle was corrected from one to two; and\nthe other driver\u2019s car has multiple Damage Parts\n(left and front).\nWith these changes, we describe our notations\nas follows, using the arrow diacritic to indicate\ncumulative state elements, upper case to indicate\ntuples and lower case to indicate labels or values,\nboldface to indicate a set of tuples, and calligraphic\nfont to indicate a set of values. The initial dialogue\nstate X0 is empty. The cumulative belief (CB) state\n\u2190\u2212\nXt (for t > 0) could be predicted directly or via a\nrecursive state update: \u2190\u2212\nXt = update(\u2190\u2212\nXt\u22121, Xt),\nwhere only new/updated state values are predicted\nin the turn-level belief (TLB) Xt and the update\nfunction adds new slots and replaces updated slots.\nIn the direct approach, it is possible to correct er-\nrors made by the model in previous turns, as well\nas introduce errors. A potential advantage of the\nupdate approach is that TLBs are shorter and there-\nfore easier to predict.\nFormally, \u2190\u2212\nXt and Xt are defined as follows. De-\nfine \u2190\u2212\nRt as the set of referents mentioned in a dia-\nlogue up through turn t, and Rt \u2286 \u2190\u2212\nRt as the subset\nof referents associated with information updates in\nturn t.3 The dialogue state and TLB after turn t,\n\u2190\u2212\nXt and Xt, respectively, can both be represented\n3Our application uses a finite set of types \u2190\u2212\nRt \u2286 R, but it\ncould be an open set, e.g., based on names.\nas a set of referent-associated sets of active slots:\n\u2190\u2212\nXt = {(r, \u2190\u2212\nS rt), r \u2208 \u2190\u2212\nRt} Xt = {(r, Srt), r \u2208 Rt}\nwhere Srt = {Sr1, . . . , Srnrt}, nrt is the number\nof active slots for referent r updated at turn t, and\n\u2190\u2212\nS rt denotes the cumulative set of slots. An active\nslot is defined as Srj = (srj, Vrj), where srj \u2208 S\nis the jth slot linked to referent r, S is the set of\nslot (or domain-slot) types, and Vrj is a set of one\nor more values v (categorical or free-form text)\nassociated with that slot. For our generated data,\nannotators are asked to provide the state updates.\n3.3\nEvaluation\nIn information extraction (IE) tasks, precision,\nrecall, and F-measure are commonly used, while\ndialogue state tracking (DST) relies on joint goal\naccuracy (JGA) and slot accuracy. Similar to DST,\nour IE task updates extracted information across\nturns. Directly adopting DST metrics for dialogue-\nbased IE is not ideal, because they overemphasize\nearlier parts of a conversation and do not disen-\ntangle the effects of error propagation across turns\n(Kim et al., 2022b). For these reasons, we propose\nto use precision, recall, and F1 scores, along with\nreporting both cumulative and turn update scores.\nOur task requires the scoring to handle multi-\nvalue and extended free-form text responses. For\nscoring purposes, we treat multi-value slots as\nmultiple instances of a slot. For free-form val-\nues, following the multi-span setup in question\nanswering (Li et al., 2022), we enumerate all possi-\nble alignments between predicted and gold values.\nEach gold value is aligned to one predicted value\nat most, and percentage match is computed based\non the longest common substring (LCS) to give a\npartial-credit score in the range [0, 1] (rather than\nrequiring exact match, i.e., {0, 1} score) for use in\nmeasuring precision and recall.\nCumulative Score (evaluating \u2190\u2212\nX).\nA cumula-\ntive belief (CB) state score m is computed for a\nparticular turn (specific index t or dialogue-final\nturn) in the nth dialogue as follows:\nmCB(n, t) =\n1\n|\u2190\n\u2212\nRnt|\nP\nr\u2208\u2190\n\u2212\nRnt m( \u02c6\u2190\u2212\nS nrt, \u2190\u2212\nS \u2217\nnrt).\nwhere m can be precision (P) or recall (R). Over-\nall scores are obtained by averaging over all di-\nalogues Nt = {n : \u2190\u2212\nRnt \u0338= \u2205}.4 For example,\n4In the first turns, it is possible that there is nothing to\nextract and no false predictions, in which case \u2190\u2212\nRnt = \u2205.\nprecision is given by:\nCB-P(t) =\n1\n|Nt|\nP\nn\u2208Nt PCB(n, t).\nWe compute the F1 score after getting the averaged\nprecision and recall.\nTurn Update Scores (evaluating X).\nSeveral\nscores are computed at the turn level, all of which\nare based on averaging over all N dialogues in the\ntest set as follows:\n1\nN\nP\nn\n1\n|Tn|\nP\nt\u2208Tn mTYPE(n, t)\nwhere Tn\n= {t : Rnt\n\u0338= \u2205} and TYPE \u2208\n{TLB, R, RS, SV} denotes diagnostic score type.\nSpecific scores (mTYPE) are based on:\nmTLB(n, t) =\n1\n|Rnt|\nP\nr\u2208Rnt m(\u02c6Snrt, S\u2217\nnrt)\nmR(n, t) = m( \u02c6Rnt, R\u2217\nnt)\nmRS(n, t) =\n1\n|Rnt|\nP\nr\u2208Rnt m( \u02c6Snrt, S\u2217\nnrt)\nmSV(n, t) = m\n\u0010S\nr\u2208Rnt \u02c6Snrt, S\nr\u2208Rnt S\u2217\nnrt\n\u0011\nwhere Snrt is the set of slot labels associated with\nreferent r in turn t of the n-th dialogue. For each\nturn, the mTLB indicates performance over the TLB;\nmR indicates how well referents are recognized;\nmRS indicates how well referents are associated\nwith slots ignoring values; and mSV gives perfor-\nmance of slot-value detection ignoring referents.\n4\nDatasets\nAIC\nDIALGEN-AIC\n# dial.\n34\n235\n# turns / dial.\n197 \u00b1 98\n46 \u00b1 8\n# tokens / dial.\n4195 \u00b1 2404\n1128 \u00b1 230\n# user tokens / turn\n18 \u00b1 27\n22 \u00b1 17\n# agent tokens / turn\n25 \u00b1 31\n27 \u00b1 14\n# referent-slot pair\n1622\n8844\n# unique referent-slot\n109\n152\n# referent-slot pair / dial.\n48 \u00b1 24\n38 \u00b1 8\n% dial. w/ updates\n50.0%\n14.5%\n% dial. w/ multiple val.\n50.0%\n19.1%\nTable 1: Statistics are calculated on the full dataset.\nTokens are calculated with Huggingface T5 tokenizer.\nWe were provided with a private dataset of 34\nnatural auto insurance claim calls (AIC). In each\ncall, the agent\u2019s task is to gather detailed informa-\ntion about an auto accident. The calls were hu-\nman transcribed and labeled using a schema with\n6 referents and 60 possible slots from 10 domains\n(Appendix C.3). Calls had high variance in length\nand complexity, as shown in Table 1. Additionally,\n50% of dialogues had multiple values for at least\none active slot. We split the calls into 7/4/23 for\ntrain/val./test sets aiming for a slot count split of\n20/10/70.\nUsing AIC as a target dataset for augmentation,\nwe apply DIALGEN with ChatGPT as the LM back-\nbone to create DIALGEN-AIC, which contains 235\nlabeled dialogues (Appendix C.5). Reviewers com-\nplete a one-hour training to become familiar with\nthe task and practiced generating one dialogue un-\nder supervision. Full training is complete after\nthey receive feedback for their first 3\u20135 dialogues.\nThey are instructed to aim for generating dialogues\nwith \u2248 50 turns. On average, each dialogue com-\nprises 8\u00b14 subdialogues, with 58% of edited turns\nand 20% of generated turns being deleted. Each\ndialogue involves 9 \u00b1 10 times of partial or full\nsubdialogue regeneration.\nData collection occurred over 2 months with\nmultiple iterations as documentation and task in-\nstructions evolved to become more comprehensive\nand consistent. The final version of the task instruc-\ntions further encouraged workers to update slot\nvalues in multiple ways and include multiple val-\nues in a slot (as described in \u00a72.1). We follow the\nmethodology in SQuAD (Rajpurkar et al., 2016),\ncalculating inter-annotator agreement (IAA) at the\nturn level with three annotators and 32 dialogues,\nwith a resulting IAA of 78.5% F1 (Appendix C.2).\nDIALGEN-AIC has less variance than AIC\nacross all statistics, which follows expectations of\nnatural data being noisy and difficult to emulate.\nHowever, compared to MultiWOZ (Budzianowski\net al., 2018), DIALGEN-AIC is more complex.\nMultiWOZ dialogues average 14 turns and 8 active\nslots per dialogue, compared to 46 turns and 38\nslots on average for DIALGEN-AIC.\nWe split DIALGEN-AIC into train/val./test sets\nwith a ratio of 80/10/10 dialogues, selecting\nval./test sets by randomly sampling from the fi-\nnal iteration of data collection. Table 1 contains\nadditional statistics of AIC and DIALGEN-AIC.\n5\nExperiments\n5.1\nModels\nIn-context Learning.\nHu et al. (2022) propose\nIC-DST and use schema prompts and a specialized\nretriever to enable few-shot in-context learning to\npredict state change with an LM. Given longer dia-\nlogues, a more complex ontology, and more slots to\ntrack than the datasets discussed in Hu et al. (2022),\nthe representation of dialogue history becomes a\ncrucial concern. The SQL tables of the ontology is\n1696 tokens, and our chosen LM, ChatGPT, has a\ntoken limit of 4096 tokens. To accommodate the\ntoken constraints, we truncate the in-context exam-\nples when given a longer dialogue state. We extract\nthe TLB at turn t and accumulate TLBs as CB.\nFurthermore, our task requires the model to iden-\ntify the corresponding entity (referent) for the pre-\ndicted slot-value pair. We redesign the prompt\n(Appendix B.2) to instruct the LM to generate the\nreferent, slot, and value simultaneously. The re-\ntriever, SBERT (Reimers and Gurevych, 2019), is\nfinetuned on the full DIALGEN-AIC training set,\nwhich is also used as the example selection pool.\nDue to privacy concerns, we only evaluate IC-DST\non the DIALGEN-AIC test set.\nFinetuned Transformers.\nWe follow idea of the\nprevious work (Lee et al., 2021) to independently\nextracted the information and finetune T5 (Raf-\nfel et al., 2020) and Long-T5 (Guo et al., 2022)\nwith schema information embedded in the prompt.\nHowever, unlike the independent decoding in Lee\net al. (2021) which used separate prompts for each\ndomain-slot pair, we take a more efficient approach\nwith one prompt per domain, where the model pre-\ndicts only active slots (together with referent and\nvalue). The CB is the aggregate of predictions over\nall domains.\nIn addition, we explore four different configura-\ntions of prompt and model outputs:\nLong-T5\u2020: Use {(A, U)\u03c4}t\u22121\n\u03c4=1 to predict CB\nLong-T5: Use {(A, U)\u03c4}t\u22121\n\u03c4=1 to predict TLB; add\nto CB\nT5: Use (A, U)t\u22121 to predict TLB; add to CB\nT5-SC: Use (A, U)t\u22121 and previous domain CB\nto predict state change \u2206CB; update CB\nDue to the input length can be longer than 1k to-\nkens, we choose Long-T5 to cover all turns with the\nprompt, while the T5-based models make predic-\ntion based on the current turn only. T5-SC further\nconsiders the state change \u2206CB which is similar to\nthe TLB but augmented with the four state-change\ncommands. Details of prompts for the different\ncases are given Appendix B.3.\n5.2\nExperimental Setup\nWhen conducting experiments involving AIC,\nthe model selection criterion is the highest TLB F1\nscore on the AIC validation set. For experiments\nsolely on DIALGEN-AIC, models were chosen\nbased on TLB F1 score on the DIALGEN-AIC\nvalidation set. Additional hyperparameters can be\nfound in Appendix A.1. All reported values repre-\nsent the medians of five different random seeds.\n5.3\nResults\nWe report results on both cumulative and turn\nupdate scores. The cumulative socres are presented\nin two ways: CBavg as an average of CB across\nevery user turn, and CBQ as the CB at user turn t,\nwhere t = \u2308QT/4\u2309 , Q \u2208 {1, 2, 3, 4} and T is the\ntotal length of a dialogue. Thus, t will be a specific\nturn, at either a quarter, a half, three-quarters, or\nthe end of the dialogue.\nThe score of the last cumulative belief state CB4\nis the full F1 score and can be regarded as evaluat-\ning a conversation summary. Model development\nwas done only on the synthetic data to minimize\nuse of real data.\nMethod\nCBavg\nCB1\nCB2\nCB3\nCB4\nTLB\nIC-DST\n71.3\n71.9\n68.5\n68.4\n68.2\n68.1\nLong-T5\u2020\n71.8\n72.5\n71.7\n71.0\n70.4\n\u2013\nLong-T5\n66.3\n64.3\n64.8\n64.3\n63.9\n68.5\nT5\n76.8\n78.4\n74.9\n73.7\n74.1\n73.9\nT5-SC\n78.2\n79.3\n76.4\n76.6\n76.9\n74.2\nT5-SC\u00a7\n78.5\n78.7\n76.2\n76.0\n76.2\n75.0\nTable 2: F1 scores on the DIALGEN-AIC test set. \u2020 de-\nnotes Long-T5 with direct CB prediction. \u00a7 denotes the\nresults on the test set with name substitution.\nMethod\nData\nCBavg\nCB1\nCB2\nCB3\nCB4\nTLB\nT5\nAIC\n38.3\n39.6\n37.1\n36.2\n35.1\n34.8\nT5\nDG\n40.4\n41.7\n42.6\n39.9\n37.7\n40.9\nT5\nBoth\n43.7\n42.9\n42.2\n43.0\n41.9\n43.7\nT5-SC\nAIC\n39.2\n40.0\n38.1\n37.1\n36.1\n33.9\nT5-SC\nDG\n41.0\n43.6\n42.1\n41.3\n40.5\n38.9\nT5-SC\nBoth\n46.2\n47.8\n47.2\n45.9\n45.3\n44.6\nTable 3: F1 scores on the AIC test set for different\ntraining data. DG stands for DIALGEN-AIC. Both\nmeans the data includes AIC and DIALGEN-AIC.\nResults on DIALGEN-AIC Test Set.\nThe results\nof experiments on DIALGEN-AIC with different\nlearning strategies and T5 configurations are pre-\nsented in Table 2. The performance of IC-DST is\nlower than all T5 variants, although this may be due\nto the difference in use of domain-specific prompts.\nNote that our IC-DST implementation is based on\nthe same ChatGPT model used for generating the\nDIALGEN-AIC, so the low results suggest that hu-\nman collaboration leads to data that is sufficiently\ndifferent from ChatGPT text such that ChatGPT\ncannot easily address this task. Predicting CB di-\nrectly requires the full history, which is only possi-\nble with Long-T5. With Long-T5, there is a benefit\nto predicting CB directly over TLB. However, opti-\nmizations needed to handle a longer history have\ntradeoffs that result in performance that is worse\nthan the standard T5 model with TLB prediction\nfor this task. The best result is obtained with T5-\nSC, which updates values rather than simply adding\nthem as new elements in a list.\nTo mitigate the potential risk of LMs generating\npersonal information linked to randomly generated\nnames in shared data, we replace them with other\nrandomly generated names. As shown in Table 2,\nT5-SC exhibits comparable performance on both\nthe original and renamed dialogues, indicating that\nthe renaming process does not impact the model\u2019s\neffectiveness.\nPrecision\nRecall\n30\n35\n40\n45\n50\n55\n60\nCB1/4\nPrecision\nRecall\n25\n30\n35\n40\n45\n50\n55\nCB2/4\nPrecision\nRecall\n25\n30\n35\n40\n45\n50\n55\nCB3/4\nPrecision\nRecall\n25\n30\n35\n40\n45\n50\n55\nCB4/4\nAIC\nDialGen-AIC\nAIC + DialGen-AIC\nFigure 3: CB precision and recall scores on the AIC\ntest set. All scores are based on T5-SC models.\nResults on AIC Test Set.\nThe two best mod-\nels (T5 and T5-SC) are used in experiments on\nthe real data (AIC). The F1 results for different\ntraining sources are given in Table 3. We measure\nthe utility of synthetic data on model performance\nby varying amounts of DIALGEN-AIC. The per-\nformance for the model trained on the synthetic\ndata alone is better than with the small amount of\nthe real data, but the best results are obtained by\nmodel trained on the combined data. Because of\n0.9k \n 10%\n1.8k \n 20%\n2.7k \n 30%\n3.5k \n 40%\n4.4k \n 50%\n5.3k \n 60%\n6.2k \n 70%\n7.1k \n 80%\n7.9k \n 90%\n8.8k \n 100%\n# Turns and % the DialGen-AIC Data\n25\n30\n35\n40\n45\nTLB-F1\nAIC (1.3K turns)\nDialGen-AIC\nAIC + DialGen-AIC\nFigure 4: TLB-F1 scores for T5-SC on AIC test set by\nvarying the amount of DIALGEN-AIC training data.\nthe higher frequency of state changes in the human-\nhuman dialogues, there is a greater benefit from\nthe T5-SC model for the real data, with an 8% im-\nprovement in the CB4 score compared to 4% for\nthe synthetic data when using all training data.\nTo provide more insight into performance, we\npresent the precision/recall results for CB in Fig-\nure 3. Incorporating synthetic data yields higher re-\ncall and outperforms using real data alone in terms\nof F1. The increased recall can be attributed to the\ninclusion of a wider range of values in the synthetic\ndata, which are not covered by the AIC training\nset. However, this improvement comes at the ex-\npense of lower precision. By combining both data\nsets, the model achieves better alignment with real-\nworld data while retaining the advantage of high\nrecall scores from the synthetic data.\nWe also experimented with varying the amount\nof synthetic data used in training the model in or-\nder to ascertain the relative value of synthetic vs.\nreal data. Figure 4 shows that using 59 synthetic\ndialogues (approximately 2.7K turns) yields results\nsimilar to those obtained from the AIC training\nset, which consists of 1.3K turns in 7 dialogues.\nThese results suggest that roughly 2.1 times as\nmany turns of synthetic data is needed to match\nthe performance of the real data, or 8.4 times as\nmany synthetic dialogues since the synthetic dia-\nlogues are shorter. However, the synthetic data is\nmore valuable in combination with real data, for\nwhich the benefit beyond 97 dialogues (50%) is\nminimal. This suggests an opportunity for further\nimprovement through strategic scenario sampling.\n6\nError Analysis\nOut of the 56 slots in the AIC test set, we no-\nticed an improvement in 45 slots, while 4 slots\nwere tied, and the remaining 7 slots have slightly\nworse performance. Our error analysis reveals two\nmain categories for the performance loss: data mis-\nmatch between AIC and DIALGEN-AIC and over-\nreliance on surface-level features.\nData Mismatch.\nWe lose performance for the\nslot Car Mileage because of a difference in lan-\nguage used when describing the mileage of a car. In\nAIC, agents ask a binary confirmation for whether\nthe mileage on the vehicle is above a certain thresh-\nold, whereas callers in DIALGEN-AIC describe\ncar mileage with an exact number. For the slot Traf-\nfic Controls Obeyed, AIC callers indirectly indicate\nthat traffic controls are not obeyed, e.g. stating that\nthe other driver ran a red light. In DIALGEN-AIC,\nthe agent asks the caller to confirm directly whether\ntraffic controls were obeyed.\nSurface Level Text.\nThe model both over- and\nunder-predicts slots due to surface-level features\nsuch as predicting Number of Involved Cars when\nthe text discusses counting vehicles, despite many\nsuch instances in AIC simply describing the traf-\nfic environment to contextualize the accident, e.g.,\nthere was a vehicle in front of the caller, but it\nwas not involved in the accident. The model also\npredicted this slot when there was language about\nthe number of passengers with a driver. Similarly,\nColor would be predicted whenever colors were\nmentioned, e.g., a purple bruise. Traffic Flow was\nseverely under-predicted when it would have been\nbeneficial for the model to predict the slot when-\never it saw information describing lane direction.\n7\nConclusion\nWe propose DIALGEN, in which humans and\nLMs collaborate to generate long, complex dia-\nlogues. We demonstrate its effectiveness by syn-\nthesizing auto insurance calls and conducting in-\nformation extraction experiments. While we build\non the DST framework, our information extraction\nexperiments target an ontology and data that are\nmore complex than the DST task was originally\ndesigned for. To serve the IE task, we introduce an\nentity-centric scoring methodology more suitable\nfor our information extraction task than the conven-\ntional joint goal accuracy metrics used in DST. Our\nexperiments demonstrate that the data generated by\nDIALGEN, despite dissimilarities with the data it\nis designed to emulate, can significantly improve\nmodel performance for information extraction on\nreal-world human dialogues.\n8\nLimitations\nWhile DIALGEN can be used to generate syn-\nthetic data for privacy-constrained settings, the ef-\nfectiveness largely depends on the LM employed,\ntarget setting, and language. We conducted all\nexperiments in the auto insurance claim calls do-\nmain in English, where English is a high-resource\nlanguage, and descriptions of car accidents are rea-\nsonably frequent in online text. An LM without\nreasonable capability in generating text in the tar-\nget domain and language will result in low quality\nsubdialogues, which can result in a frustrating col-\nlaboration for the human reviewer.\nSubdialogue generation in DIALGEN is guided\nby including the full dialogue history as context for\neach subsequent subdialogue. LMs have finite con-\ntext input length, so the max length of a generated\ndialogue is limited by the chosen LM. Methods\nto overcome this limitation can include truncating\nthe dialogue history context, investigating which\nparts of the prompt contribute little to guiding the\nLM, and representing dialogue history in a more\nefficient manner.\n9\nEthical Considerations\nPreserving privacy (Xin et al., 2020; Liu et al.,\n2022b; Torfi et al., 2022) is an important challenge\nin synthetic data generation. Ensuring important\ncharacteristics in synthesized data with DIALGEN\nrequires a domain expert who may have access to\nreal, private data and can unintentionally leak infor-\nmation. DIALGEN-AIC, on the other hand, gen-\nerates personal information using the Faker pack-\nage,5 but there is a potential for the LM to produce\npersonal details related to randomly created names.\nTo mitigate the potential risk in shared data, we use\ngender guesser package 6 to detect the gender of\neach name and replace it with other same-gender\nname. If DIALGEN users plan to publicly release\ntheir data, they should remove potentially identify-\ning information such as names from the synthesized\ndata. In the released DIALGEN-AIC, we replace\nnames with random alternatives to prevent the inad-\n5https://github.com/joke2k/faker\n6https://github.com/lead-ratings/\ngender-guesser\nvertent generation of sensitive personal information\nby the LM.\nOther than privacy issues, LMs can produce\nharmful content, and the risks of such production\ncan increase depending on the target data setting.\nWhen employing humans to collaborate with LMs,\npractitioners should determine whether additional\nsafety features such as toxic language filters are\nrequired to protect the workers.\nRegarding the data collection hiring process, all\ndialogue reviewers were recruited from university\nlistings and compensated at a rate of $18.69 per\nhour, following university practices. Prior to data\ncollection, we instructed our reviewers to familiar-\nize them with the ontology, annotation guidelines,\nand criteria for assessing dialogue quality. We es-\ntablished a Slack workspace for smooth commu-\nnication with the workers throughout the process,\nproviding feedback and promptly addressing ques-\ntions and concerns they raised. This interaction\nensured high quality of the gathered data.\nAcknowledgments\nWe would like to express our sincere gratitude to\nKevin Everson, Yanda Chen, and Yushi Hu for their\ninvaluable discussions and preliminary studies. We\nwould also like to thank Bing-Syuan Wang and\nIrene Wang for their expert web programming con-\nsulting and debugging support. Additionally, we\nextend our appreciation to members of UWNLP for\ntheir valuable insights and contributions throughout\nthe project. Lastly, we are grateful to the diligent\nstudent reviewers from the University of Washing-\nton for their dedicated efforts in data creation. Their\ncontributions were essential to the success of this\nresearch.\nReferences\nJianzhu Bao, Rui Wang, Yasheng Wang, Aixin Sun,\nYitong Li, Fei Mi, and Ruifeng Xu. 2023. A synthetic\ndata generation framework for grounded dialogues.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 10866\u201310882, Toronto, Canada.\nAssociation for Computational Linguistics.\nHelena Bonaldi,\nSara Dellantonio,\nSerra Sinem\nTekiro\u02d8glu, and Marco Guerini. 2022.\nHuman-\nmachine collaboration approaches to build a dialogue\ndataset for hate speech countering. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 8031\u20138049, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Ga\u0161i\u00b4c. 2018. MultiWOZ - a large-\nscale multi-domain Wizard-of-Oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5016\u20135026, Brussels,\nBelgium. Association for Computational Linguistics.\nDerek Chen, Howard Chen, Yi Yang, Alexander Lin,\nand Zhou Yu. 2021.\nAction-based conversations\ndataset: A corpus for building more in-depth task-\noriented dialogue systems. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3002\u20133017.\nPeng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling.\n2020. Feature space augmentation for long-tailed\ndata. In Computer Vision \u2013 ECCV 2020, pages 694\u2013\n710, Cham. Springer International Publishing.\nElizabeth Clark, Tal August, Sofia Serrano, Nikita\nHaduong, Suchin Gururangan, and Noah A. Smith.\n2021. All that\u2019s \u2018human\u2019 is not gold: Evaluating\nhuman evaluation of generated text. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7282\u20137296, Online.\nAssociation for Computational Linguistics.\nJessamyn Dahmen and Diane Cook. 2019.\nSynsys:\nA synthetic data generation system for healthcare\napplications. Sensors (Basel, Switzerland), 19(5).\nYao Dou, Maxwell Forbes, Rik Koncel-Kedziorski,\nNoah A. Smith, and Yejin Choi. 2021. Scarecrow: A\nframework for scrutinizing machine text.\nYao Dou, Maxwell Forbes, Rik Koncel-Kedziorski,\nNoah A. Smith, and Yejin Choi. 2022. Is GPT-3 text\nindistinguishable from human text? scarecrow: A\nframework for scrutinizing machine text. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 7250\u20137274, Dublin, Ireland. Association\nfor Computational Linguistics.\nMor Geva, Yoav Goldberg, and Jonathan Berant. 2019.\nAre we modeling the task or the annotator? an inves-\ntigation of annotator bias in natural language under-\nstanding datasets. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 1161\u20131166, Hong Kong, China. Association\nfor Computational Linguistics.\nJiaqi Guan, Runzhe Li, Sheng Yu, and Xuegong Zhang.\n2018. Generation of synthetic electronic medical\nrecord text. In 2018 IEEE International Conference\non Bioinformatics and Biomedicine (BIBM), pages\n374\u2013380.\nMandy Guo, Joshua Ainslie, David Uthus, Santiago On-\ntanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.\n2022. LongT5: Efficient text-to-text transformer for\nlong sequences. In Findings of the Association for\nComputational Linguistics: NAACL 2022, pages 724\u2013\n736, Seattle, United States. Association for Compu-\ntational Linguistics.\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy,\nRoy Schwartz, Samuel Bowman, and Noah A. Smith.\n2018. Annotation artifacts in natural language infer-\nence data. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 107\u2013112,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nXuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haf-\nfari, and Mohammad Norouzi. 2022. Generate, An-\nnotate, and Learn: NLP with Synthetic Text. Trans-\nactions of the Association for Computational Linguis-\ntics, 10:826\u2013842.\nYushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,\nNoah A. Smith, and Mari Ostendorf. 2022.\nIn-\ncontext learning for few-shot dialogue state tracking.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 2627\u20132643, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nJ. F. Kelley. 1984. An iterative design methodology\nfor user-friendly natural language office information\napplications. ACM Trans. Inf. Syst., 2(1):26\u201341.\nHyunwoo Kim, Jack Hessel, Liwei Jiang, Ximing Lu,\nYoungjae Yu, Pei Zhou, Ronan Le Bras, Malihe\nAlikhani, Gunhee Kim, Maarten Sap, et al. 2022a.\nSoda: Million-scale dialogue distillation with so-\ncial commonsense contextualization. arXiv preprint\narXiv:2212.10465.\nTakyoung Kim, Hoonsang Yoon, Yukyung Lee, Pilsung\nKang, and Misuk Kim. 2022b. Mismatch between\nmulti-turn dialogue and its evaluation metric in dia-\nlogue state tracking. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 297\u2013\n309, Dublin, Ireland. Association for Computational\nLinguistics.\nChia-Hsuan Lee, Hao Cheng, and Mari Ostendorf. 2021.\nDialogue state tracking with a language model using\nschema-driven prompting.\nIn Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4937\u20134949, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nHaonan Li, Martin Tomko, Maria Vasardani, and Tim-\nothy Baldwin. 2022. MultiSpanQA: A dataset for\nmulti-span question answering. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1250\u20131260,\nSeattle, United States. Association for Computational\nLinguistics.\nZhuoyan Li, Hangxiao Zhu, Zhuoran Lu, and Ming\nYin. 2023. Synthetic data generation with large lan-\nguage models for text classification: Potential and\nlimitations. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Process-\ning, pages 10443\u201310461, Singapore. Association for\nComputational Linguistics.\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022a. WANLI: Worker and AI collabo-\nration for natural language inference dataset creation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 6826\u20136847, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nFan Liu, Zhiyong Cheng, Huilin Chen, Yinwei\nWei, Liqiang Nie, and Mohan Kankanhalli. 2022b.\nPrivacy-preserving synthetic data generation for rec-\nommendation systems. In Proceedings of the 45th\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval, SIGIR\n\u201922, page 1379\u20131389, New York, NY, USA. Associa-\ntion for Computing Machinery.\nJoon Sung Park, Lindsay Popowski, Carrie Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S Bern-\nstein. 2022. Social simulacra: Creating populated\nprototypes for social computing systems. In Proceed-\nings of the 35th Annual ACM Symposium on User\nInterface Software and Technology, pages 1\u201318.\nNoseong Park, Mahmoud Mohammadi, Kshitij Gorde,\nSushil Jajodia, Hongkyu Park, and Youngmin Kim.\n2018. Data synthesis based on generative adversarial\nnetworks. Proc. VLDB Endow., 11(10):1071\u20131083.\nKun Qian, Ahmad Beirami, Zhouhan Lin, Ankita De,\nAlborz Geramifard, Zhou Yu, and Chinnadhurai\nSankar. 2021. Annotation inconsistency and entity\nbias in MultiWOZ. In Proceedings of the 22nd An-\nnual Meeting of the Special Interest Group on Dis-\ncourse and Dialogue, pages 326\u2013337, Singapore and\nOnline. Association for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research, 21:1\u2013\n67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383\u20132392, Austin,\nTexas. Association for Computational Linguistics.\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara,\nRaghav Gupta, and Pranav Khaitan. 2020. Towards\nscalable multi-domain conversational agents: The\nschema-guided dialogue dataset. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, 05,\npages 8689\u20138696.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982\u20133992, Hong Kong, China. Association for Com-\nputational Linguistics.\nFelix Stahlberg and Shankar Kumar. 2021. Synthetic\ndata generation for grammatical error correction with\ntagged corruption models. In Proceedings of the\n16th Workshop on Innovative Use of NLP for Build-\ning Educational Applications, pages 37\u201347, Online.\nAssociation for Computational Linguistics.\nVajira Thambawita, Pegah Salehi, Sajad Amouei\nSheshkal, Steven A. Hicks, Hugo L. Hammer, Sra-\nvanthi Parasa, Thomas de Lange, P\u00e5l Halvorsen, and\nMichael A. Riegler. 2022. Singan-seg: Synthetic\ntraining data generation for medical image segmenta-\ntion. PLOS ONE, 17(5):1\u201324.\nAmirsina Torfi, Edward A. Fox, and Chandan K. Reddy.\n2022. Differentially private synthetic medical data\ngeneration using convolutional gans. Information\nSciences, 586:485\u2013500.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38\u201345, Online. Association\nfor Computational Linguistics.\nBangzhou Xin, Wei Yang, Yangyang Geng, Sheng Chen,\nShaowei Wang, and Liusheng Huang. 2020. Private\nfl-gan: Differential privacy synthetic data generation\nbased on federated learning. In ICASSP 2020 - 2020\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 2927\u20132931.\nYing Zeng, Yansong Feng, Rong Ma, Zheng Wang,\nRui Yan, Chongde Shi, and Dongyan Zhao. 2018.\nScale up event extraction learning via automatic train-\ning data generation. In Proceedings of the Thirty-\nSecond AAAI Conference on Artificial Intelligence\nand Thirtieth Innovative Applications of Artificial In-\ntelligence Conference and Eighth AAAI Symposium\non Educational Advances in Artificial Intelligence,\nAAAI\u201918/IAAI\u201918/EAAI\u201918. AAAI Press.\nA\nTraining and Generation Details\nA.1\nFinetuning Detains\nAll experiments are done with T5-base or Long-\nT5-base with Huggingface implementation (Wolf\net al., 2020). Training time for full DIALGEN-\nAIC and AIC setting is averaged 3 hours on 2\nNVIDIA V100 GPUs. For the experiments on only\nDIALGEN-AIC, we use 2 NVIDIA A40 GPUs.\nThe total number of GPU training hours is 110\nhours.\nHyperparameter\nT5\nLong-T5\nTraining batch size\n16\n16\nLearning rate\n5 \u00d7 10\u22124\n5 \u00d7 10\u22124\nMax generation length\n256\n256\nMax input length\n512\n2592\nTable 4: Hyperparameters for training T5 and Long-T5.\nThe other parameters are default values in Huggingface\ntrainer.\nA.2\nChatGPT Generation Hyperparameters\nHyperparameter\nDIALGEN\nIC-DST\nVersion\ngpt-3.5-turbo-0301\ngpt-3.5-turbo-0301\nTemperature\n0.85 - 0.9\n0.0\nMax tokens\n512\n512\nStop strings\n[\"<\\div>\"]\n[\"\u2013\", \"\\n\", \";\", \"#\"]\nPresence penalty\n0.2\n0\nFrequency penalty\n0.2\n0\nTable 5: Hyperparameters for generation from Chat-\nGPT.\nB\nPrompts\nWe shows the prompts used in DIALGEN for\ngenerating DIALGEN-AIC, IC-DST, T5 and Long-\nT5 in the following subsections.\nB.1\nDIALGEN Prompt\nTable 7 shows the list of predefined callers\u2019 per-\nsonality. Table 8 shows an example of a prompt\nused to generate the first subdialogue when using\nDIALGEN-AIC for auto insurance claim calls, in-\ncluding a task description, entity-slot-value triplets,\nan accident story, caller\u2019s and agent\u2019s personalities\nand a initial exchange.\nB.2\nIC-DST Prompt and Output\nDue to the input length limit, we extract the\nTLB at turn t and accumulate TLBs as CB. Thus,\n[context] is regarded as empty.\nCREATE TABLE AccidentDetails(\n'Damage Part' TEXT CHECK ('Damage Part' IN 'Front', 'Right\n', 'Back', 'Left', 'Front Right', 'Front Left', 'Back Left\n', 'Back Right', 'Other', 'Unsure'),\n'Accident Location' TEXT CHECK ('Accident Location' IN '\nParking Lot', 'Driveway', 'Highway', 'Roadway', '\nIntersection', 'Other'),\n'Num of Passengers' TEXT CHECK ('Num of Passengers' IN '0',\n'1', '2+', 'Unsure'),\n'Witnesses' TEXT CHECK ('Witnesses' IN 'Yes', 'No', '\nUnsure'),\n'Num of Involved Cars' TEXT CHECK ('Num of Involved Cars'\nIN '1', '2', '3', '4+', 'Unsure'),\n'Children Involved' TEXT CHECK ('Children Involved' IN '\nYes', 'No', 'Unsure'),\n'Airbag Deployed' TEXT CHECK ('Airbag Deployed' IN 'Yes',\n'No', 'Unsure'),\n'Towed' TEXT CHECK ('Towed' IN 'Yes', 'No', 'Unsure'),\n'Pedestrians Involved' TEXT CHECK ('Pedestrians Involved'\nIN 'Yes', 'No', 'Unsure'),\n'Date of Accident' TEXT,\n'Time of Accident' TEXT,\n'Subjective Fault' TEXT CHECK ('Subjective Fault' IN '\nCaller', 'Other Driver'),\n)\nCREATE TABLE Adjuster(\n'Explain Coverages' TEXT,\n'Permission to Record' TEXT CHECK ('Permission to Record'\nIN 'Yes', 'No'),\n'Set up Inspection' TEXT CHECK ('Set up Inspection' IN '\nQuick Photo Claim', 'Field Assignment'),\n'Set up Rental' TEXT CHECK ('Set up Rental' IN 'Yes', 'No')\n,\n)\nCREATE TABLE CarInfo(\n'Make/Model' TEXT,\n'Make Year' TEXT,\n'Color' TEXT,\n'Car Mileage' TEXT,\n'Rideshare (Uber/Lyft)' TEXT CHECK ('Rideshare (Uber/Lyft)\n' IN 'Yes', 'No', 'Unsure'),\n)\nCREATE TABLE ContactInfo(\n'First Name' TEXT,\n'Last Name' TEXT,\n'Home Address' TEXT,\n'Phone Number' TEXT,\n'Email Address' TEXT,\n'Policy Number' TEXT,\n'Date of Birth' TEXT,\n)\nCREATE TABLE DriverActions(\n'Car Motion' TEXT CHECK ('Car Motion' IN 'Traveling\nForward', 'Backing', 'Turning', 'Changing Lanes', 'Stopped\n', 'Other', 'Unsure'),\n'Speed' TEXT,\n'Distractions' TEXT CHECK ('Distractions' IN 'Cellphone',\n'Animals', 'Smoking', 'Passengers', 'Traffic', 'Eating', '\nNot Paying Attention', 'Other', 'Unsure', 'No Distraction')\n,\n'Brake' TEXT CHECK ('Brake' IN 'Yes', 'No', 'Unsure'),\n'Horn' TEXT CHECK ('Horn' IN 'Yes', 'No', 'Unsure'),\n'Turn Signal' TEXT CHECK ('Turn Signal' IN 'Yes', 'No', '\nUnsure'),\n'Traffic Controls Obeyed' TEXT CHECK ('Traffic Controls\nObeyed' IN 'Yes', 'No', 'Unsure'),\n)\nCREATE TABLE Evidences(\n'Police Report' TEXT CHECK ('Police Report' IN 'Yes', 'No',\n'Unsure'),\n'Police Department Name' TEXT,\n'Pictures' TEXT CHECK ('Pictures' IN 'At Scene', 'After\nAccident', 'No Picture', 'Unsure'),\n'Tickets Citations' TEXT CHECK ('Tickets Citations' IN '\nCaller Party Cited', 'Other Party Cited', 'No Party Cited',\n'Multiple Parties Cited', 'Unsure', 'No Ticket'),\n'Police Report Number' TEXT,\n'Skid Marks' TEXT CHECK ('Skid Marks' IN 'Yes', 'No', '\nUnsure'),\n)\nCREATE TABLE InjuryDetails(\nInstruction\nCount\nHave CALLER describe more car accident details with complex reasoning that involves two cars\u2019 motion.\n23\nHave CALLER\u2019s response be less specific. have AGENT asks for more details.\n18\nSplit AGENT\u2019s questions into multiple turns\n18\nHave CALLER\u2019s response be less specific. have AGENT asks for more details. have AGENT asks a question for car accident details.\n15\nHave AGENT ask for permission to record the call.\n15\nAsk for email address and home address\n14\nHave CALLER ask AGENT questions about her insurance coverages in multiple turns\n13\nHave AGENT ask CALLER more questions about the accident details\n12\nHave CALLER misremember the details. AGENT double check with CALLER.\n12\nExplain coverages\n12\nHave CALLER corrects wrong information. have AGENT asks for clarification.\n12\nBreak this conversation down into multiple turns of dialogue\n11\nHave AGENT ask for contact information\n10\nBreak these turns down into multiple turns of back and forth dialogue\n10\nAGENT needs to split up her questions.\n10\nTable 6: Instructions with a frequency of 10 or more times used by humans to regenerate a subdialogue.\nPersonality\nDescription\nAggressive\nFeeling angry and confrontational about the accident, may place blame on others or use aggressive language.\nAnalytical\nFocused on the details and logistics of the claim process, may ask for precise information and explanations.\nConfused\nUnsure about what happened during the accident or what to do next, may ask a lot of questions.\nCooperative\nWilling to work with the insurance company and other parties involved in resolving the claim.\nDefensive\nFeeling the need to justify their actions or place blame on others, may be unwilling to take responsibility for the accident.\nEmotional\nExperiencing strong emotions related to the accident, may be crying or struggling to maintain composure during the call.\nEvasive\nHesitant to provide information or answer questions about the accident, may be trying to conceal something.\nImpatient\nFeeling frustrated with the claim process or the speed at which it is progressing, may express irritation or urgency in their language.\nReassuring\nTrying to maintain a positive and optimistic outlook during the call, may express gratitude for the assistance being provided.\nUpset\nFeeling distressed or frustrated due to the accident and its consequences.\nTable 7: The list of the predefined callers\u2019 personalities.\n'Ambulance' TEXT CHECK ('Ambulance' IN 'Yes', 'No', '\nUnsure'),\n'Body Part Injured' TEXT CHECK ('Body Part Injured' IN '\nHead', 'Neck', 'Shoulder', 'Chest', 'Abdomen', 'Back', '\nLimb', 'Other'),\n'Injury Type' TEXT CHECK ('Injury Type' IN 'Bruise', '\nBroken Fracture', 'Cut Scratch', 'Bleeding', 'Strain\nSprain', 'Sore', 'Other', 'No Injury'),\n'Medical Treatment' TEXT CHECK ('Medical Treatment' IN '\nMRI', 'Surgery', 'Cat Scan', 'Hospitalization', 'ER', 'X-\nRay', 'Other'),\n)\nCREATE TABLE TrafficEnvironment(\n'Weather Visibility' TEXT CHECK ('Weather Visibility' IN '\nClear', 'Cloudy', 'Rainy', 'Snowy', 'Foggy', 'Windy', '\nOther', 'Unsure'),\n'Obstructions to View' TEXT CHECK ('Obstructions to View'\nIN 'Yes', 'No', 'Unsure'),\n'Road Condition' TEXT CHECK ('Road Condition' IN 'Dry', '\nWet', 'Slippery', 'Debris', 'Potholes', 'Straight', '\nCurved', 'Tunnel', 'Steep Incline', 'Flat', 'Other', '\nUnsure'),\n'Traffic Signal' TEXT CHECK ('Traffic Signal' IN 'Stop\nSign', 'Yield Sign', 'Green Light', 'Yellow Light', 'Red\nLight', 'Other', 'Unsure', 'No Signal Or Sign'),\n'Description of Lanes' TEXT CHECK ('Description of Lanes'\nIN 'Normal', 'Turn Lane', 'Shoulder', 'Other', 'Unsure'),\n'Num of Lanes' TEXT CHECK ('Num of Lanes' IN '1', '2', '3',\n'4+', 'Unsure'),\n'Traffic Condition' TEXT CHECK ('Traffic Condition' IN '\nHeavy', 'Moderate', 'Light', 'Other', 'Unsure'),\n'Speed Limit' TEXT,\n'Traffic Flow' TEXT CHECK ('Traffic Flow' IN 'One-Way', '\nTwo-Way', 'Other', 'Unsure'),\n'Parking Lot Type' TEXT CHECK ('Parking Lot Type' IN '\nAngled', 'Straight', 'Other', 'Unsure'),\n)\nCREATE TABLE Trip(\n'Destination of Trip' TEXT,\n'Purpose of Trip' TEXT,\n'Origin of Trip' TEXT,\n)\n-- Using valid SQLite, answer the following multi-turn\nconversational questions for the tables provided above.\nExample #1\n[context]\n[system] I see. Thank you for letting me know. Can you also\nprovide me with the make, model, and year of your car, as well\nas its color?\nQ: [user] Of course. It's a white Lexus sedan, 2018 model.\nSQL: SELECT * FROM CarInfo WHERE Caller-Make_Year = 2018 AND\nCaller-Color = white AND Caller-Make/Model = Lexus sedan,;\nExample #2\n[context]\n[system] Thank you for sharing that information, Lynne. Can\nyou also provide me with the make and model of your car?\nQ: [user] Yes, it's a white sedan. The make and model is a\nToyota Camry. It's a 2018 model, and it had about 40,000 miles\non it at the time of the accident\n.\nSQL: SELECT * FROM CarInfo WHERE Caller-Color = white sedan.\nAND Caller-Make/Model = Toyota Camry. AND Caller-Make_Year =\n2018 AND Caller-Car_Mileage = 40,\n000;\nExample #3\n[context]\n[system] I see. Can you describe your car's make and model?\nWhat year was it made? And what color was it?\nQ: [user] It's a white sedan, a 2018 Honda Accord.\nSQL: SELECT * FROM CarInfo WHERE Caller-Make/Model = sedan, a\n2018 Honda Accord. AND Caller-Make_Year = 2018 AND Caller-\nColor = white;\nExample #4\n[context]\n[system] Do you remember the make and model of the other car?\n<short_summary>\nstory\nBob Parkhurst had a busy day at work, and all he wanted to do was to go grocery shopping. As he backed out of her parking spot\nin the Office Depot parking lot, he failed to notice the gray MAZDA B-Series Extended Cab driven by Spencer Tullar as he\nturned into the same aisle from the opposite direction.\nSpencer, who was on his way to run some errands, had been driving down the parking lot in extremely slow speed when suddenly\nhe saw Bob\u2019s yellow car backing out of his spot. He didn\u2019t think much of it and was about to just drive behind her when, at the\nlast minute, he noticed that Bob seemed to be backing out without looking around. Spencer slammed on his brakes, but it was\ntoo late. The front right of his truck smashed hard into the back passenger side of Bob\u2019s car.\nThe impact of the collision caused Bob\u2019s car to spin around and come to a stop. He immediately felt a sharp pain in her neck and\nknew that something was wrong. As he tried to get out of the car, he realized that he couldn\u2019t move his neck without experiencing\nexcruciating pain.\nSpencer got out of his truck and approached Bob\u2019s car, he asked if Bob was okay. Bob told him that he was hurt and needed\nmedical attention. Spencer called 911 immediately while also trying his best to comfort Bob until help arrived.\nWhen emergency services arrived shortly after, they found Bob slumped over in her seat, clutching his neck in agony. The\nresponders helped her out of the car and placed a neck brace around him so he wouldn\u2019t move his head while they examined her\ninjuries. They then transported him by ambulance to the hospital for further medical attention.\nMeanwhile, police were already on their way. Upon arrival at the scene, they took statements from both drivers as well as any\nwitnesses who may have seen what happened. Unfortunately, no one at the time had a clear view of the incident, but both drivers\nagreed that they didn\u2019t see each other before the collision.\nSince both cars were still in the parking lot when the accident happened, there was no need to redirect traffic. However, the\nofficers still had to direct people away from the incident site to prevent any further accidents. They also checked Spencer\u2019s\nlicense and found that it was valid.\nThe investigation into what caused the accident was inconclusive. Neither driver was certain about who was at fault, as they both\nbelieved the other driver failed to observe their movements. Since no one appeared to be at fault, no tickets or\n\u2014\u2014\u2013\nentity-slot-value triplets\nAccident details: (accident location, office depot parking lot), (damage part, unsure), num of passengers, witnesses, date of\naccident, time of accident, subjective fault, airbag deployed.\nEvidences of the car accident: police report, (pictures, no picture), police report number, police department name, tickets\ncitations.\nTraffic condition: weather visibility, (obstructions to view, no).\nCaller\u2019s driver action: car motion, speed, traffic controls obeyed, turn signal, (horn, no).\nCaller\u2019s car information: (make/model, dodge stratus), make year, color, car mileage.\nCaller\u2019s injury details: body part injured, injury type, medical treatment.\n\u2014\u2014\u2013\ntask description\nHave role play car accident claim call. One person is an agent Alice from a car insurance company and the other is the caller\nBob who wants to file a claim.\nAt beginning of the call, have Alice ask for Bob\u2019s permission to record the call and proceeds with the conversation.\nWithin some <p></p>, have simulate poor phone connection. Have Alice and Bob can not hear each other and need to repeat\nwhat they said.\nHave Alice verify Bob personal information to access account information at the beginning of the call.\nHave Bob describe the car accident by using story and tuples above to describe the accident.\nHave Alice confirm new information with Bob during the call to ensure consistency.\nHave Alice and Bob engage in small talk with each other.\nHave Alice explain the insurance coverages to Bob.\n\u2014\u2014\u2013\npersonality\nBob is impatient, feeling frustrated with the claim process or the speed at which it is progressing, may express irritation or\nurgency in their language.\nAlice is conversational, personable, patient, empathetic, sympathetic and professional.\n\u2014\u2014\u2013\ninstructions\nUse the story, information, and personality to create a role play script and follow the task description.\n</short_summary>\n<div>\n<p class=\"Alice\" title=\"Auto Accident\">Thank you for calling! This is Alice. How may I help you today? </p>\n<p class=\"Bob\" title=\"Auto Accident\">Hello. This is Alice. I am calling for a car accident. </p>\n</div>\nHave Alice ask a question for car accident details.\n<div>\nTable 8: Example prompt used to generate the first subdialogue in DIALGEN-AIC. Subsequent subdialogues are\ngenerated by appending the previously completed subdialogue to this prompt. Similar to Park et al. (2022), we use\nHTML tags to denote different dialogue elements, i.e., <p> for turns and <div> for the subdialogue.\nQ: [user] I think it was a black sedan, but I'm not completely\nsure.\nSQL: SELECT * FROM CarInfo WHERE Other_Driver-Make/Model =\nsedan, AND Other_Driver-Color = black;\nExample #5\n[context]\n[system] Thank you for that information, Joel. Can you please\nprovide me with your car's make and model, year, color, and\napproximate mileage?\nQ: [user] Sure, my car is a white sedan. It's a 2016 model\nwith approximately 50,000 miles on it.\nSQL: SELECT * FROM CarInfo WHERE Caller-Make/Model = sedan.\nAND Caller-Car_Mileage = approximately 50,000 miles AND Caller-\nColor = white AND Caller-Make_Ye\nar = 2016 model;\nExample #6\n[context]\n[system] Thank you for all the details, Richard. Can you\nplease provide me with your car's make and model?\nQ: [user] Yes, it's a white sedan, a 2007 make.\nSQL: SELECT * FROM\nCarInfo WHERE Caller-Color = white sedan AND Caller-Make_Year\n= 2007\n* FROM CarInfo WHERE Caller-Color = white sedan AND Caller-\nMake_Year = 2007\n* FROM CarInfo WHERE Caller-Color = white sedan AND Caller-\nMake_Year = 2007\nB.3\nPrompt and Output for Finetuned Models\nThe previous study (Lee et al., 2021) employs in-\ndependent decoding with natural language prompts\nfor optimal outcomes. However, this approach ne-\ncessitates the enumeration of all potential combina-\ntions of domain-slot pairs during both training and\ninference. As the ontology grows larger, the com-\nputational burden increases linearly. To address\nthis issue, we propose to group slots with the same\ndomain and train the models to predict all active\nslots with their values and referents simultaneously.\nLong-T5 for CB prediction.\nWe present a train-\ning example for the \u201cContactInfo\u201d domain with full\ndialogue history at time t.\nInput:\n[USER] My name is Bob Lee, and my policy number is 123456789.\n[SYSTEM] Thank you. Could you please provide me with your name\nand policy number so I can access your account information? [\nUSER] Yes, that's fine. [SYSTEM] I am so sorry that happened.\nBefore we begin, may I please have your permission to record\nthis call for quality and training purposes? [USER] Hello.\nThis is Bob. I am calling for a car accident. [SYSTEM] Thank\nyou for calling AllState! This is Alice. How may I help you\ntoday? [domain] ContactInfo [possible slots] First Name (the\nFirst Name of the ContactInfo) [s] Last Name (the Last Name of\nthe ContactInfo) [s] Home Address (the Home Address of the\nContactInfo) [s] Phone Number (the Phone Number of the\nContactInfo) [s] Email Address (the Email Address of the\nContactInfo) [s] Policy Number (the Policy Number of the\nContactInfo) [s] Date of Birth (the Date of Birth of the\nContactInfo)\nOutput:\nFirst Name [srv] Bob [rv] Caller [s] Last Name [srv] Lee [rv]\nCaller [s] Policy Number [srv] 123456789. [rv] Caller\nLong-T5 and T5 models for TLB prediction.\nWe present a training example for the \u201cContactInfo\u201d\ndomain with the most recent two turns (A, U)t at\ntime t.\nInput:\n[USER] Hi, my name is Bob Lee. I was recently in a car\naccident and wanted to file a claim. [SYSTEM] Thank you for\ncalling! This is Alice. How may I help you today? [domain]\nContactInfo [possible slots] First Name (the First Name of the\nContactInfo) [s] Last Name (the Last Name of the ContactInfo)\n[s] Home Address (the Home Address of the ContactInfo) [s]\nPhone Number (the Phone Number of the ContactInfo) [s] Email\nAddress (the Email Address of the ContactInfo) [s] Policy\nNumber (the Policy Number of the ContactInfo) [s] Date of\nBirth (the Date of Birth of the ContactInfo)\nOutput:\nFirst Name [srv] Bob [rv] Caller [s] Last Name [srv] Lee [rv]\nCaller\nIn the example, the caller (USER) mentions the\nfirst and the last name that are under the domain\nContactInfo. The model is require to generate the\nactive slots \u201cFirst Name\u201d and \u201cLast Name\u201d with\nthe corresponding values \u201cBob\u201d and \u201cLee\u201d, and\nreferent \u201cCaller.\u201d\nT5 with State Change (T5-SC).\nFor T5-SC, the\nmodel need to predict entity-slot-value triplets and\nedit operations associated with the triplets. The\nfinal output of a state at time t will be calcu-\nlated by applying the edit operations on the associ-\nated triplets given the previous state at time t \u2212 1.\nWe consider four edit operations: [new], [same],\n[delete], and [concat]. We describe the four\nedit operations in the following paragraph.\nIf a triplet has not been observed in the previous\nstate, the model is expected to predict [new]. Con-\nversely, if the triplet has already been mentioned in\nthe previous state, the model must predict [same].\nThe [delete] operation is employed when a triplet\nmentioned in the previous state should be removed.\nIf the value of a referent-slot is updated, then the\nmodel predicts both [delete] for the previous\nvalue and [new] for the updated value. On the\nother hand, the [concat] operation is used when\nthe value of a triplet needs refinement, such as com-\nbining two values, 7 and AM, into a single value 7\nAM.\nDue to the input length limit of the T5 model,\nwe use the most recent k turns to create the previ-\nous state and omit the slot descriptions in order to\ncover more entity-slot-value triplets in the previous\nstate. We get the best results when k = 18 for\nDIALGEN-AIC and k = 20 for AIC. We present a\ntraining example for the \u201cAccidentDetails\u201d domain\nas follows.\nInput:\n[USER] Oh, sorry about that. You're right, it actually\noccurred on a Wednesday at 11 am. [SYSTEM] Also, I just wanted\nto clarify some information. In our previous conversation,\nyou stated that the accident occurred on a Monday at 9 am.\nHowever, our records show that it actually occurred on a\nWednesday at 11 am. Can you confirm which day and time the\naccident actually occurred? [state] Damage Part [srv] Front\nLeft [rv] Caller [cv] Right [rv] Global [s] Accident Location\n[srv] Highway [rv] Global [s] Num of Passengers [srv] 0 [rv]\nGlobal [s] Witnesses [srv] Yes [rv] Global [s] Date of\nAccident [srv] this Monday [rv] Global [s] Time of Accident [\nsrv] 9:00 am. [rv] Global [s] Subjective Fault [srv] Caller [\nrv] Caller [domain] AccidentDetails [possible slots] Damage\nPart [s] Accident Location [s] Num of Passengers [s] Witnesses\n[s] Num of Involved Cars [s] Children Involved [s] Airbag\nDeployed [s] Towed [s] Pedestrians Involved [s] Date of\nAccident [s] Time of Accident [s] Subjective Fault\nOutput:\nDate of Accident [srv] Wednesday [v] this Monday [vo] [delete]\n[rv] Global [s] Time of Accident [srv] 11 am. [v] 9:00 am. [\nvo] [delete] [rv] Global\nIn the example, the agent (SYSTEM) clarifies\nthe date and time with the caller (USER) because\nthe date and time the caller provides are different\nfrom the record in the agent\u2019s system. The caller\nadmit the provided time and date are wrong. Thus,\ntime and date need to be updated. The previously\nprovided date \u201cthis Monday\u201d need to be deleted, so\nwe append an operation [delete] after the value.\nSimilarly, we append the operation after the time\n\u201c9:00 am.\u201d\nC\nDIALGEN\nC.1\nData Collection Cost\nThe human reviewers were recruited from uni-\nversity listing. They were compensated at a rate\nof $18.69 per hour following our institution\u2019s prac-\ntices. A dialogue, including reviewing synthesizing\nand annotation processes, required 45-60 minutes,\nfor a final cost per dialogue of $14-19.\nC.2\nIAA\nWe follow the methodology in SQuAD (Ra-\njpurkar et al., 2016) for calculating IAA. We select\n3 trained workers who participated in data gener-\nation as our annotators. They annotated 15% of\nDIALGEN-AIC. The average time to label a di-\nalogue was 18 minutes. For every dialogue, one\nannotator is randomly assigned as the reference.\nWe calculate max-F1 of every predicted tuple for\nevery turn and average over all turns, then average\nacross all dialogues.\nC.3\nAIC Ontology\nWe show the full ontology in Table 9 including\ndomains, slots, and possible values. Possible ref-\nerents in the AIC ontology: Global, Caller, Other\nDriver, Caller\u2019s Passenger, Other Driver\u2019s Passen-\nger, and Witness. All referents could be associated\nwith every domain/slot, although in practice cer-\ntain information is almost always associated with a\nparticular referent, e.g., Traffic Conditions (heavy,\nmedium, light) always have a Global referent.\nPrecision\nRecall\n30\n35\n40\n45\n50\nTLB-F1\nPrecision\nRecall\n35\n40\n45\n50\n55\nReferent-Slot\nPrecision\nRecall\n40\n45\n50\n55\n60\nReferent\nPrecision\nRecall\n35\n40\n45\n50\n55\nSlot-value\nAIC\nDialGen-AIC\nAIC + DialGen-AIC\nFigure 5: TLB and three diagnostic scores for precision\nand recall (mR, mRS, and mSV) for the T5-SC model on\nAIC test set.\nC.4\nUser Interface for Data Collection\nWe list two main pages of our interface for di-\nalogue generation. They are editing, and labeling\nsteps.\nFirst, the editing step (Figure 6) page provides\ndialogue scenarios (slot value pairs), dialogue his-\ntory, extracted tuples (annotated entity-slot-value\ntriplets), instruction for regeneration, and current\nsubdialogue for editing. A human reviewer can\nprovide an instruction to guide the LM to generate\na desired subdialogue to replace the current subdi-\nalogue. If the the current subdialogue is satisfied\nwith the reviewer, they can edit turns to fix the\nminor errors in the subdialogue.\nSecond, the labeling step page (Figure 7) is an\noptional page for DIALGEN framework. This page\nis designed for dialogue state tracking task where\nthe human reviewer can annotate the edit subdia-\nlogue in the previous editing step. Note that the\nlabeling step can be fully decoupled from the frame-\nwork.\nThe human reviewer will iteratively collaborate\nwith the LM to generate and revise subdialogues\nand annotate the subdialogues until reaching the\nend of the dialogue.\nC.5\nDIALGEN-AIC Dialogues\nIn Tables 10\u201312, we show the sample dialogues\nfrom DIALGEN-AIC.\nDomain\nSlot\nPossible Values\nAdjuster\nExplain Coverages\n[]\nAdjuster\nPermission to Record\n[yes, no]\nAdjuster\nSet up Inspection\n[photo claim, field assignment]\nAdjuster\nSet up Rental\n[yes, no]\nContactInfo\nFirst Name\n[]\nContactInfo\nLast Name\n[]\nContactInfo\nHome Address\n[]\nContactInfo\nPhone Number\n[]\nContactInfo\nEmail Address\n[]\nContactInfo\nPolicy Number\n[]\nContactInfo\nDate of Birth\n[]\nDriverActions\nCar Motion\n[traveling forward, backing, turning, changing lanes, stopped, other, unsure]\nDriverActions\nSpeed\n[]\nDriverActions\nDistractions\n[cellphone, animals, smoking, passengers, traffic, eating, not paying attention, other, unsure, no distraction]\nDriverActions\nBrake\n[yes, no, unsure]\nDriverActions\nHorn\n[yes, no, unsure]\nDriverActions\nTurn Signal\n[yes, no, unsure]\nDriverActions\nTraffic Controls Obeyed\n[yes, no, unsure]\nEvidences\nPolice Report\n[yes, no, unsure]\nEvidences\nPolice Department Name\n[]\nEvidences\nPictures\n[at scene, after accident, no picture, unsure]\nEvidences\nTickets Citations\n[caller party cited, other party cited, no party cited, multiple parties cited, unsure, no ticket]\nEvidences\nPolice Report Number\n[]\nEvidences\nSkid Marks\n[yes, no, unsure]\nInjuryDetails\nAmbulance\n[yes, no, unsure]\nInjuryDetails\nBody Part Injured\n[head, neck, shoulder, chest, abdomen, back, limb, other]\nInjuryDetails\nInjury Type\n[bruise, broken fracture, cut scratch, bleeding, strain sprain, sore, other, no injury]\nInjuryDetails\nMedical Treatment\n[MRI, surgery, CAT scan, hospitalization, ER, x-ray, other]\nAccidentDetails\nDamage Part\n[front, right, back, left, front right, front left, back left, back right, other, unsure]\nAccidentDetails\nAccident Location\n[parking lot, driveway, highway, roadway, intersection, other]\nAccidentDetails\nNum of Passengers\n[0, 1, 2+, unsure]\nAccidentDetails\nWitnesses\n[yes, no, unsure]\nAccidentDetails\nNum of Involved Cars\n[1, 2, 3, 4+, unsure]\nAccidentDetails\nChildren Involved\n[yes, no, unsure]\nAccidentDetails\nAirbag Deployed\n[yes, no, unsure]\nAccidentDetails\nTowed\n[yes, no, unsure]\nAccidentDetails\nPedestrians Involved\n[yes, no, unsure]\nAccidentDetails\nDate of Accident\n[]\nAccidentDetails\nTime of Accident\n[]\nAccidentDetails\nSubjective Fault\n[caller, other driver]\nCarInfo\nMake/Model\n[]\nCarInfo\nMake Year\n[]\nCarInfo\nColor\n[]\nCarInfo\nCar Mileage\n[]\nCarInfo\nRideshare (Uber/Lyft)\n[yes, no, unsure]\nTrip\nDestination of Trip\n[]\nTrip\nPurpose of Trip\n[]\nTrip\nOrigin of Trip\n[]\nTrafficEnvironment\nWeather Visibility\n[clear, cloudy, rainy, snowy, foggy, windy, other, unsure]\nTrafficEnvironment\nObstructions to View\n[yes, no, unsure]\nTrafficEnvironment\nRoad Condition\n[dry, wet, slippery, debris, potholes, straight, curved, tunnel, steep incline, flat, other, unsure]\nTrafficEnvironment\nTraffic Signal\n[stop sign, yield sign, green light, yellow light, red light, other, unsure, no signal or sign]\nTrafficEnvironment\nDescription of Lanes\n[normal, turn lane, shoulder, other, unsure]\nTrafficEnvironment\nNum of Lanes\n[1, 2, 3, 4+, unsure]\nTrafficEnvironment\nTraffic Condition\n[heavy, moderate, light, other, unsure]\nTrafficEnvironment\nSpeed Limit\n[]\nTrafficEnvironment\nTraffic Flow\n[one-way, two-way, other, unsure]\nTrafficEnvironment\nParking Lot Type\n[angled, straight, other, unsure]\nTable 9: AIC ontology. Empty lists indicate free-form extractive values.\nEditing Step\nDialog Scenario\nDialog History\nExtracted Tuples\nInstruction to ChatGPT for modifying some turns or regenerating subdialog.\nThe instruction to ChatGPT for regenerating the current subdialog.\n-- no selected sample instruction --\nYou can choose one of the sample instructions and edit it.\nYou can also type your instruction to guide ChatGPT to enrich the conversation and make it more natural! :)\nCurrent Subdialog\nFor each row of the table, correct the factual inconsistency and remove the redundant information in the column (Turn) and type your edit in the text box. If there are too many\nturns to edit, regenerate a new subdialog instead.\nModify Some Turns\n \nRegenerate Subdialog\n2 turns in this subdialog\n2 time(s) of auto editing left.\nTurn\n#\nParty Role Turn\nYour Edit\n3\nEric\nagent\nI'm sorry to hear that. Can you please provide me with some\ninformation about the accident? What is the location of the\naccident, and what part of your car was damaged? Were\nthere any passengers or witnesses involved? And when did\nthe accident happen?\ncopy\ndelete\nauto\n4\nMark user\nThe accident happened at an intersection where I had a yield\nsign. Another car failed to yield and collided with my car. My\ncar hit a tree, and their car ended up on its side across the\nstreet. There were no pedestrians. I had a friend in my car,\nand the other driver had two passengers. It happened today,\nin the morning around 8:30 am.\ncopy\ndelete\nauto\nIf contents is hidden, you can scroll down the box.\nActions\nIf you have done all edits in the current subdialog, choose Action 1.\nIf you think the whole dialog \ufb01nish, choose Action 2. You will be lead to the last\nlabeling step and \ufb01nish the dialog.\n(Action 1) Go to Label and Continue!\n(Action 2) Go to Label and Finish!\nAccidentDetails\nSlot\nVa\nPedestrians\nInvolved\nNo\nAccident\nL\nti\nAdjuster\nSlot Value\nCarInfo\nSlot\nVa\nMake/Model\nMake Year\nColor\nContactInfo\nSlot\nValue\nFirst\nName\nMark\nLast\nN\nMullen\nDriverActions\nSlot\nValue\nCar\nMotion\nSpeed\nSlot\nVa\nPolice\nReport\nUn\nPictures\nSkid Marks\nInjuryDetails\nSlot\nVa\nInjury Type Oth\nMedical\nTreatment ER\nTraf\ufb01cEnvironmen\nSlot\nV\nTraf\ufb01c\nCondition\nO\nTraf\ufb01c\nSi\nl\nTrip\nSlot\nVa\nPurpose of\nTrip\nDestination\nf T i\nTurn #\nParty\nRole\nTurn\n1\nEric\nagent\nThank you for calling Acme! This is Eric. How may I help you today?\n2\nMark\nuser\nHello. This is Mark. I am calling for a car accident.\nAccidentDetails\nAdjuster\nCarInfo\nContactInfo\nDriverActions\nEvidences\nInjuryDetails\nTraf\ufb01cEnvironment\nTrip\nFigure 6: The first step in DIALGEN is to create the subdialogue. A dialogue scenario table is provided to indicate\nslots expected to appear in the conversation. A human reviewer selects LM-generated text and edit it as needed. They\ncan also ask the LM to regenerate selected turns or the full subdialogue and optionally provide extra instructions to\nguide the LM\u2019s generation process.\nExtracted Tuples\nTurn to be labeled\nYou can annotate more than one span. Please make sure you annotate all possible tuples (domain, slot, value). Use your cursor to select a span and annotate it one by one.\nIf you are not sure what to annotate, please check the ontology. [Link]\n(Turn # 14) James (user):\nExtracted Tuples in this Turn\nDuplicate Tuples\n\u2605 indicates the tuple(s) from the current turn.\nproceed?\n6\nJames\nuser\nSure, that's \ufb01ne.\n7\nNellie\nagent\nCan you please describe the details of the accident?\n8\nJames\nuser\nYes, it happened yesterday afternoon around 4:30 PM. The accident occurred at the\nintersection of Main Street and Park Avenue. I was driving in the left lane going straight, and\nthe other driver ran a red light and T-boned my car on the passenger side. There were two\npassengers in my car and no witnesses that I am aware of.\n9\nNellie\nagent\nThank you for providing that information. Can you tell me if anyone was injured in the\naccident?\n10\nJames\nuser\nFortunately, no one was seriously injured. We were all a bit shaken up, but we didn't require\nany medical treatment\nAccidentDetails\nAdjuster\nCarInfo\nContactInfo\nDriverActions\nEvidences\nInjuryDetails\nTraf\ufb01cEnvironment\nTrip\nSure, the other driver seemed to be going really fast, maybe 45 or 50 mph. There was a traf\ufb01c light at the intersection, and I\nhad the green light when I entered the intersection. It was a clear day with no weather issues, and there were no\nobstructions in my view.\n\u2718\nOther Driver || DriverActions || Speed || 45 or 50 mph. || (non-categorical)\nOtherDriver_DriverActions_Speed\nKeep Concat Update Turn\n#\nReferent Domain\nSlot\nValue Categorical\nValue\n13\nOther\nDriver\nDriverActions Speed pretty\nfast\n14\n\u2605\nOther\nDriver\nDriverActions Speed\n45 or\n50\nmph.\n(Preview) OtherDriver_DriverActions_Speed\nTurn\n#\nReferent Domain\nSlot\nValue\nCategorical\nValue\n14\nOther\nDriver\nDriverActions Speed\n45 or\n50\nmph.\nNext\nBack\nGo to Generation\nFigure 7: A human reviewer selects a span and label it. If there exists a duplicate label, they are prompted to resolve\nthe conflict by selecting to update (as shown), concat, or keep multiple labels.\nD\nAdditional Analysis\nFigure 5 provides the TLB precision and recall\nresults for the full state updates and different di-\nagnostic scores (referent only, referent-slot, and\nslot-value). Consistent with the CB results, the\nbiggest benefit of incorporating DIALGEN-AIC is\nimproved recall. While referent, slot, and value all\nimprove, the greatest improvement is in slot values.\nE\nLicense of Artifacts\nThe license of code for (Wolf et al., 2020) is\nApache license version 2.0. The license of code\nfor Faker and Gender-guesser are MIT and GPLv3\nLicense, respectively. The terms for use of our\nartifacts will be included in our released package.\nAgent\nThank you for calling DialGen Insurance! This is Greta. How may I help you today?\nUser\nHello. This is Suzanne. I am calling for a car accident.\nAgent\nAlright, Suzanne. First, I need to verify some information. May I please have your full name and policy number?\nUser\nYes, my name is Suzanne Luna and my policy number is 1234567890.\nAgent\nThank you, Suzanne. And can you please provide me with your phone number and address?\nUser\nYes, of course. My phone number is (123) 456-7890, and my address is 123 Main Street.\nAgent\nThank you, Suzanne. Now, could you please describe the accident for me?\nUser\nCertainly. I was driving alone on my way to the library when I approached an intersection with a yield sign. I slowed\ndown and checked for oncoming traffic but didn\u2019t see any cars, so I began to turn left. Suddenly, I heard a loud crunching\nsound, and my car jerked to a stop. I got out of my car to assess the damage and saw that a black Nissan Maxima had hit\nthe back of my car.\nAgent\nThank you for letting me know, Suzanne. Do you know the name of the street you were on when the accident happened?\nUser\nSure, the accident happened at the intersection of Main Street and Broad Avenue.\nAgent\nAnd I believe you said you were driving alone, is that correct?\nUser\nYes, I was alone in my car.\nAgent\nWere there any witnesses at the scene who provided their contact information?\nUser\nYes, there were several witnesses at the scene who provided their contact information, but I didn\u2019t get their names.\nAgent\nDo you remember the date and time of the accident?\nUser\nYes, the accident occurred a few nights ago at around 11 pm.\nAgent\nDo you know the specific day?\nUser\nLet\u2019s see, it would have been Wednesday\u2013no, Thursday night.\nAgent\nThank you, Suzanne. Can you tell me if your airbags deployed during the accident?\nUser\nNo, the airbags did not deploy.\nAgent\nAlright, and can you describe the traffic conditions at the time of the accident? Was there a traffic light at the intersection?\nUser\nYes, there was a traffic light and a yield sign.\nAgent\nI see. Did you notice anything else about the road or lanes that might have contributed to the accident?\nUser\nNo, nothing else that I recall. It all happened so fast.\nAgent\nDo you remember if you were going at a particular speed when the accident happened?\nUser\nNo, I was not speeding. I don\u2019t think the other driver was either.\nAgent\nAlright. Can you tell me about any damage your car sustained?\nUser\nYes, my car sustained significant damage on the left side.\nAgent\nI\u2019m sorry to hear that. Do you know if a police report was filed?\nUser\nYes, the Houston Police Department took our statements, and the report number is 1234567890.\nAgent\nDo you know if either of you received a citation or ticket as a result of the accident?\nUser\nYes, the other driver, Homer Shepherd, received a citation.\nAgent\nAlright, thank you for letting me know. And can you provide me with the make and model of your car?\nUser\nYes, my car is a brown sedan, a Toyota Corolla.\nAgent\nThank you, Suzanne. And can I ask if you sustained any injuries as a result of the accident?\nUser\nYes, unfortunately, I did sustain some injuries. I needed a CT scan at the hospital after the accident.\nAgent\nCan you tell me where you were injured, Suzanne?\nUser\nMy neck and back.\nAgent\nWhat was the diagnosis at the hospital?\nUser\nThe diagnosis was muscle strain and some minor bruising.\nAgent\nI\u2019m sorry to hear that, Suzanne. How are you feeling now?\nUser\nI\u2019m doing a bit better, thank you for asking.\nAgent\nThat\u2019s good to hear. Now, let me explain your coverage options to you.\nUser\nOkay, thank you.\nAgent\nFirst, let me go over the details of your policy with you. You have liability insurance, which covers bodily injuries and\nproperty damage to others if you are at fault in an accident, although it looks like that\u2019s not applicable here. You also have\ncollision coverage, which pays for damages to your vehicle in the event of an accident.\nUser\nOkay, got it.\nAgent\nAdditionally, you have personal injury protection, which will cover your medical expenses related to injuries sustained in\nthe accident, regardless of who was at fault. However, please note that there may be limits on the amount of coverage\nprovided by this policy.\nUser\nOkay, thank you for explaining that to me.\nAgent\nSuzanne, can you tell me if your car was towed from the scene of the accident?\nUser\nNo, it wasn\u2019t. I was able to drive it home myself.\nAgent\nOkay, thank you for letting me know. I will just note that in the report. That should be everything for the preliminary\ndetails. Is there anything else you\u2019d like to add, Suzanne?\nUser\nNo, that\u2019s all. Thank you for your help, Greta.\nAgent\nYou\u2019re welcome. We will process your claim and be in touch with you soon about next steps.\nUser\nOkay, thank you so much.\nAgent\nHave a good day, Suzanne.\nUser\nYou too, Greta. Bye.\nTable 10: Sample DIALGEN-AIC dialogue 1.\nAgent\nThank you for calling DialGen Insurance! This is Loan. How may I help you today?\nUser\nHello. This is Joshua. I am calling for a car accident.\nAgent\nI\u2019m sorry to hear that you were in an accident. What happened?\nUser\nIt was last Monday morning in a tunnel. There were several cars involved, and my car was hit from the back by a yellow\nDodge Ram. I was injured and had to be taken to the hospital by ambulance.\nAgent\nOh my, I\u2019m sorry to hear that. Did you have any passengers in your car?\nUser\nNo, I was the only one in the car.\nAgent\nDo you know how many cars were involved total?\nUser\nI think there were about four cars involved.\nAgent\nOkay, thank you for that information, Joshua. Can you describe the damage to your car?\nUser\nThe back was heavily damaged, and my car is undrivable.\nAgent\nWas your car towed from the scene?\nUser\nYes, it was. Almost all of them had to be.\nAgent\nDid the police come to the scene of the accident?\nUser\nYes, they did. They took statements from witnesses, and they also created an accident report that documented all involved\nparties\u2019 details.\nAgent\nGreat, do you happen to have the police report number and the name of the police department?\nUser\nYes, I have them right here. The police report number is 12345678, and it was the Philadelphia Police Department.\nAgent\nThank you, Joshua. Was anyone cited or received a ticket at the scene?\nUser\nNo, the police report stated that no party was cited.\nAgent\nOkay, thank you for letting me know. Can you describe the traffic conditions at the time of the accident?\nUser\nTraffic was flowing smoothly in the three-lane road. There was a car that stopped in the lane to my right, and the car\nbehind them swerved into my lane.\nAgent\nDid you notice any traffic signals or signs that may have contributed to the accident?\nUser\nNo, there weren\u2019t any traffic signals or signs at all in the tunnel.\nAgent\nI see. Can you describe your car\u2019s make and model? What year was it made? And what color was it?\nUser\nIt\u2019s a white sedan, a 2018 Honda Accord.\nAgent\nThank you for that information, Joshua. Were there any witnesses to the accident?\nUser\nYes, there were several people who saw the accident happen. Some good Samaritans helped me after the accident and\ncalled 911.\nAgent\nThat\u2019s good to hear. Now, can you tell me about your injuries? What kind of medical treatment did you receive?\nUser\nI dislocated my shoulder. They performed a CT scan at the hospital to ensure that there were no internal injuries.\nAgent\nOne more thing, Joshua. Can you remind me of the exact date and time of the accident?\nUser\nIt was on Monday morning, around 8:30 am.\nAgent\nOkay, just to confirm, that would be the 22nd, correct?\nUser\nOh, wait. I think I may have remembered it wrong. It was actually last Tuesday.\nAgent\nThank you for clarifying the date, Joshua. Can you also tell me how fast were you driving when the accident occurred?\nUser\nI was driving around 35 miles per hour.\nAgent\nThank you for that information, Joshua. Do you have the contact information for any of the other drivers?\nUser\nYes, I got Steve Woods\u2019 phone number. She was driving the yellow Dodge Ram that hit my car.\nAgent\nJoshua, can you confirm how the accident occurred from your perspective? I\u2019m a little unclear on some of the details.\nUser\nSure, I was driving in the middle lane and noticed a stopped car in the other lane. The yellow Dodge Ram swerved into\nmy lane to avoid it. I couldn\u2019t change lanes because the other lane was bumper to bumper. The Dodge Ram hit my back\nbumper, bounced off, and hit the car on the right side of me. After that, several cars collided with each other. My car spun\naround and hit the tunnel wall, damaging the back.\nAgent\nThank you for explaining that, Joshua. Just to confirm, were there any traffic cameras at the scene that may have captured\nthe accident?\nUser\nNo, I don\u2019t believe there were any traffic cameras.\nAgent\nThank you for providing all of that information, Joshua. Can I ask you to confirm some information about yourself?\nUser\nSure, go ahead.\nAgent\nCan I have your full name?\nUser\nJoshua Walters. That\u2019s spelled W-A-L-T-E-R-S.\nAgent\nAnd can I have you provide your phone number and email address please?\nUser\nYes, my phone number is 555-1234, and my address is 123 Main Street.\nAgent\nGreat, thank you for providing that information. Now it looks like your insurance coverage includes liability insurance,\ncollision coverage, rental car coverage, and gap insurance.\nUser\nOkay.\nAgent\nDo you understand how each of those coverages works?\nUser\nNot really, can you explain it to me?\nAgent\nSure, liability insurance covers any damages or injuries that you may cause to another person or their property. Collision\ncoverage pays for damages to your own vehicle if you\u2019re in an accident. Rental car coverage pays for a rental car while\nyour vehicle is being repaired. And gap insurance covers the difference between the actual cash value of your car and the\nbalance remaining on your car loan if your car is totaled.\nUser\nOkay, I think I understand now. Thank you for explaining that to me.\nAgent\nDo you have any other questions for me, Joshua?\nUser\nNo, I don\u2019t. Thank you for your help.\nAgent\nBefore we end this call, do you happen to have any other details about the accident that you would like to add?\nUser\nNo, I think that covers everything.\nAgent\nOkay, thank you for providing all the information, Joshua. We will take care of everything from here.\nUser\nThank you so much for your help.\nAgent\nOf course! And please don\u2019t hesitate to reach out if you have any questions or concerns.\nUser\nI won\u2019t. Thanks again.\nTable 11: Sample DIALGEN-AIC dialogue 2.\nAgent\nThank you for calling DialGen Insurance! This is William. How may I help you today?\nUser\nHello. This is Roberta. I am calling for a car accident.\nAgent\nI am sorry to hear that, Roberta. Before we get started, can I get your permission to record this call for quality and training\npurposes?\nUser\nYes, you can record the call.\nAgent\nThank you. May I have your full name and policy number, please?\nUser\nMy name is Roberta Oliver, and my policy number is XXXXXXXX.\nAgent\nGot it. Can you please describe the accident to me?\nUser\nSure, I was driving my gold rideshare car on the highway after having dinner at a restaurant. I was changing lanes when\nShane Scott\u2019s gray Chevrolet Camaro cut me off, and I had to slam on my brakes to avoid collision. Unfortunately, it was\ntoo late, and Shane\u2019s car crashed into mine.\nAgent\nOkay, thank you for the detailed account. Were there any witnesses who saw this happen? Any traffic controls?\nUser\nNo, I didn\u2019t see any traffic controls around. I\u2019m not sure about any witnesses. Oh, I guess there were the passengers in\nShane\u2019s car, but they were too shaken up to give their statements to the police.\nAgent\nAlright. How many passengers were in each car?\nUser\nShane had three passengers in her car. I was alone in mine.\nAgent\nThank you for that information, Roberta. Can you provide me with the location details of the accident as well as the date\nand time it occurred?\nUser\nIt was May 15th at around 4 in the afternoon. The accident happened on the highway near exit 45B.\nAgent\nThank you for sharing that information, Roberta. I forgot to ask earlier, what year is your car?\nUser\nMy car is a 2012 model.\nAgent\nGreat, thanks for letting me know. Can you describe the traffic conditions at the time of the accident?\nUser\nIt was a beautiful day, and the traffic on the highway was moving at a steady pace. There were four lanes, and we were\nboth in the second lane from the left.\nAgent\nAlright, I see. Before we proceed further, I want to let you know that I understand how stressful this situation can be. I\nwant you to know that I am here to guide you through the process and make everything as clear and easy as possible. How\nare you feeling?\nUser\nHonestly, I\u2019m feeling pretty overwhelmed right now. My head has been hurting since the accident, and I\u2019m worried about\nhow much this is all going to cost.\nAgent\nThat\u2019s perfectly understandable, Roberta. Just take a deep breath and try to relax. It\u2019s good that you\u2019re taking steps towards\nresolving this by calling us today. Let\u2019s move forward together, okay?\nUser\nOkay, thank you.\nAgent\nNow you mentioned your head has been hurting since the accident. Did you injure your head during the crash?\nUser\nYeah, I hit my head on the steering wheel. Since then, I\u2019ve been having constant headaches. It\u2019s been really difficult to\nfocus on everyday tasks.\nAgent\nI\u2019m sorry to hear that. Have you seen a doctor yet?\nUser\nYes, I went to the hospital after the accident. They gave me a CT scan which revealed that I had a minor concussion.\nAgent\nI\u2019m sorry to hear that. Did they prescribe any treatment or medication?\nUser\nNot really, other than rest and avoiding physical activities. They okayed me to go back home immediately, but I needed to\nhave my husband check on me every few hours to make sure everything was fine that first night.\nAgent\nHave you been back to the hospital since to follow up on the headaches?\nUser\nNo, but I did call my doctor to ask her about it. She said that headaches are normal for the first couple of months after a\nconcussion, but to go back if they get worse.\nAgent\nI see. Thank you for telling me that, Roberta, and I hope the headaches get better soon. Just a few more questions if you\u2019ll\nbear with me. Can you tell me which part of your car was damaged in the accident?\nUser\nThe front left side of my car was damaged. The back right side of Shane\u2019s car as well.\nAgent\nThank you for that information. Now I understand that it can be frustrating when there are no witnesses to corroborate\nyour story. However, do you have any evidence of the accident? Perhaps photos of the damage or the police report?\nUser\nYes, the police came to file a report. I have a copy of it at home. I also took some photos of the damage to my car and\nShane\u2019s car.\nAgent\nGreat, that will certainly help. Can you please send those photos over to our team? I can provide you with an email address\nwhere you can send them.\nUser\nSure, that would be helpful. What\u2019s the email address?\nAgent\nThe email is claims@DialGen Insurance.com. Please put your full name and policy number in the subject line and attach\nthe photos in the email body.\nUser\nOkay, thanks. I will send them over as soon as possible.\nAgent\nPerfect. Is there anything else I can assist you with today, Roberta?\nUser\nYes, I was wondering about the insurance claim process. How long does it usually take to get a resolution?\nAgent\nIt depends on a few factors, such as the complexity of the case and how much evidence we have. Our team will carefully\nreview your claim and reach out to you within a few business days with a resolution.\nUser\nOkay, that\u2019s good to know. And what about rental cars or any other expenses related to the accident?\nAgent\nWe can certainly help you out with that if you need it. Our team can set up rental cars if necessary, and we will do\neverything we can to make sure you\u2019re not paying out of pocket for any expenses related to the accident. Will you be\nneeding a rental car?\nUser\nNo, I don\u2019t think so.\nAgent\nAlright, no problem. If you do end up needing a rental car, feel free to let us know. We\u2019re here to help in any way we can.\nUser\nThanks, I appreciate it.\nAgent\nOf course, Roberta. Is there anything else I can assist you with today?\nUser\nNo, that\u2019s all for now. Thanks for your help, William.\nAgent\nIt was my pleasure, Roberta. Take care and have a great day!\nUser\nYou too.\nTable 12: Sample DIALGEN-AIC dialogue 3.\n"
  },
  {
    "title": "NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis",
    "link": "https://arxiv.org/pdf/2307.07511.pdf",
    "upvote": "5",
    "text": "NIFTY: Neural Object Interaction Fields\nfor Guided Human Motion Synthesis\nNilesh Kulkarni1,2\nDavis Rempe\u2217 3\nKyle Genova2\nAbhijit Kundu2\nJustin Johnson2\nDavid Fouhey2\nLeonidas Guibas2,4\n1University of Michigan\n2Google\n3NVIDIA\n4Stanford University\nAbstract\nWe address the problem of generating realistic 3D motions of humans interacting\nwith objects in a scene. Our key idea is to create a neural interaction field attached\nto a specific object, which outputs the distance to the valid interaction manifold\ngiven a human pose as input. This interaction field guides the sampling of an object-\nconditioned human motion diffusion model, so as to encourage plausible contacts\nand affordance semantics. To support interactions with scarcely available data,\nwe propose an automated synthetic data pipeline. For this, we seed a pre-trained\nmotion model, which has priors for the basics of human movement, with interaction-\nspecific anchor poses extracted from limited motion capture data. Using our\nguided diffusion model trained on generated synthetic data, we synthesize realistic\nmotions for sitting and lifting with several objects, outperforming alternative\napproaches in terms of motion quality and successful action completion. We call\nour framework NIFTY: Neural Interaction Fields for Trajectory sYnthesis. Project\nPage: https://nileshkulkarni.github.io/nifty\n1\nIntroduction\nAnimating a character to sit in a chair or pick up a box is useful in gaming, character animation,\nand populating digital twins. Yet, generating realistic 3D human motion trajectories with objects is\nchallenging for two main reasons. One challenge is creating effective models that capture the nuance\nof human movements, particularly during the final phase of object interaction called the \"last mile.\"\nUnlike navigation that is primarily collision avoidance, the last mile involves intricate contacts and\nobject affordances, which influence the motion. The second challenge is acquiring paired data that\nincludes high-quality human motions and diverse object shapes, which is essential for training.\nRecent approaches to motion modeling can synthesize realistic human movements using state-of-the-\nart generative models [13, 33, 42]. They are, however, scene-agnostic and cannot produce interactions\nwith specific objects.\nTo address this, some approaches condition motion synthesis on scene geometry (e.g., a scanned\npoint cloud) [19, 47, 48, 50]. This enables learning object interactions, but motion quality is hindered\nby the lack of paired full scene-motion data. Other approaches [11, 38, 56] instead focus on a small\nset of interactions with a single type of object (e.g., sitting in a chair), and can produce high-quality\nmotions in their domain. However, these methods require a high-quality motion capture (mocap)\ndataset for each action/object separately and may make action-specific modeling assumptions (e.g.,\naffecting the human approach and/or contact with object).\nIn this work, we tackle both the modeling and data aspects of interaction synthesis to enable generating\nrealistic interactions with a variety of objects, such as sitting on a chair, table, or stool and lifting a\n\u2217Work done while at Stanford University\nPreprint. Under review.\narXiv:2307.07511v1  [cs.CV]  14 Jul 2023\nDiffusion \nMotion \nGenerator\nObject \nInteraction \nField\nMotion Model\nGuidance\nInitial Pose \n& Shape\nObject\nTraining Data Synthesis\nAnchor\nPoses\nSynthetic Motion Trajectories\nFigure 1: NIFTY Overview. (Left) Our learned object interaction field guides an object-conditioned\ndiffusion model during sampling to generate plausible human-object interactions like sitting. (Right)\nOur automated training data synthesis pipeline generates data for this model by combining a scene-\nunaware motion model with small quantities of annotated interaction anchor pose data.\nsuitcase, chair, etc.. We extend a human motion diffusion model [42] to condition on object geometry,\nand pair it with a learned object interaction field to encourage realistic movements at test time in\nthe last mile of interaction. To train this model and overcome the lack of available mocap data, we\ndevelop an automated data pipeline that leverages a powerful pre-trained and scene-agnostic human\nmotion model. As shown in Fig. 1, our interaction field, diffusion model, and motion data pipeline\nmake up a general framework to synthesize human-object interactions for a desired character that is\nflexible to multiple actions, even when dense mocap data is unavailable. We refer to this framework\nas NIFTY: Neural Interaction Fields for Trajectory sYnthesis.\nTo ensure realistic motions in the last mile of interaction, we propose an object-centric interaction field\nthat takes in a human pose and learns to regress the distance to a valid interaction pose (e.g., the final\nsitting pose). At test time, our object-conditioned diffusion model is guided by this interaction field\nto encourage high-quality motions. Unlike manually designed guidance objectives that encourage\ncontact and discourage penetration [19], our interaction field is data-driven and implicitly captures\nnotions of contact, object penetration, and any other factors learned from data.\nWe propose using synthetic data generation to enable learning interactions from limited mocap data.\nIn particular, we leverage a pre-trained motion model [33] that produces high-quality motions but\nis unaware of object geometry. Starting from an anchor pose that captures the core of a desired\ninteraction (e.g., the final sitting pose in Fig. 1, right), the pre-trained model is used to sample a large\nvariety of motions that end in the anchor pose. This approach generates a diverse set of plausible\ninteractions from only a handful of anchor poses, which are readily available from existing small\ndatasets [3] or are relatively easy to capture.\nWe evaluate NIFTY on sitting and lifting interactions for a variety of objects, demonstrating the\nsuperior quality of synthesized human motions compared to alternative approaches. Overall, this\nwork contributes (1) a novel object interaction field approach that guides an object-conditioned\nhuman motion diffusion model to synthesize realistic interactions, (2) an automated synthetic data\ngeneration technique to produce large numbers of plausible interactions from limited pose data, and\n(3) high-quality motion synthesis results for human interactions with several objects.\n2\nRelated Work\nSynthesizing Human Motion and Interactions. While various methods have been successful in\ngenerating human motion in isolation [13, 17, 30, 33, 42, 57], our work is primarily focused on\nincorporating environmental context [4, 5]. Some approaches condition motion generation on scanned\nscene geometry that encompasses multiple objects [19, 47, 48, 49], but these methods typically offer\nlimited control over the specific objects for interaction. Object-centric models are trained to generate\nmotions for a single character [11, 38] and limited actions [56], such as sitting on a chair. These\nmodels heavily rely on high-quality motion capture datasets but still exhibit issues like floating,\nskating, and penetration. Our work also focuses on individual objects but utilizes diffusion guidance\n2\nand a learned interaction field to minimize undesired artifacts. In contrast to prior work, we train our\nmodels using a novel data generation pipeline to learn interactions from limited data. Our focus is on\nmacroscopic interactions like sitting and lifting with objects, distinguishing us from other works that\ngenerate full-body motions for grasping and manipulation [8, 39, 40].\nMotion Modeling with Diffusion. Following success for image [14, 28] and video [16] generation,\ndiffusion models [37] have shown promise in modeling motion for robots [20] and pedestrians [34].\nRecently, diffusion models have been successful in generating full-body 3D human motion [6, 42, 44,\n55]. SceneDiffuser [19] generates human motion conditioned on a point cloud from a scanned scene.\nIt employs gradient-based guidance and analytic objectives to ensure collision-free, contact-driven,\nand smooth motion during the denoising process. On the contrary, our approach is object-centric and\ndoes not rely on noisy motions [12] for training. Our data-driven interaction field guides denoising\nby implicitly capturing plausible interactions and obviating the need for hand-designed objectives.\nNeural Distance Fields for Pose and Interaction. Neural networks have been used to learn a\nparametric function that outputs a distance given a query coordinate [52]. Grasping Fields [22]\nparameterize hand-object grasping through a spatial field that outputs distances to valid hand-object\ngrasps. Pose-NDF [43] learns an object-unaware distance field in the full-body pose space for\nhuman poses. NGDF [51] and SE(3)-DiffusionFields [45] learn a field in the robot gripper pose\nspace to define a manifold of valid object grasps. Our object interaction field extends this idea to\nfull-body human-object interactions by learning to predict the distance between a human pose and\nthe interaction pose manifold. Unlike prior works, we use this field to guide denoising.\nHuman Interaction Data. Though large-scale mocap data is available to train scene-agnostic human\nmotion models [25], learning human-object interactions is hampered by the challenge of capturing\nhumans in scenes. Datasets that contain full scene scans paired with human motion [10, 12, 18, 35, 58]\nare relatively small and often noisy due to capture difficulties. Other datasets contain single-object\ninteractions with a small set of objects [3, 11, 21, 40, 56]. These are better quality due to simpler\ncapture conditions, but are small with limited scope. Recent approaches circumvent the data issue\nthrough automated synthetic data generation. For example, 3D scenes can be inferred from pre-\nrecorded human motions to get plausible paired scene-motion data [50, 53, 54]. However, motions\nfrom these methods are limited to available pre-recorded data. Our data generation pipeline requires\nonly a small set of interaction anchor poses and generates novel motions not contained in prior\ndatasets using tree-based rollouts [57] from a pre-trained generative model [33].\n3\nMethod\nIn this section, we detail our NIFTY pipeline for learning to synthesize realistic human-object\ninteraction motions. \u00a73.1 introduces a conditional diffusion model to generate human motions given\nthe geometry of an object. \u00a73.2 details the object-centric interaction field, which guides the denoising\nprocess of the diffusion model to capture the nuances of interactions in a data-driven way. In \u00a73.3,\nwe discuss the synthetic data generation using a pre-trained motion model that is seeded with anchor\nposes from a smaller dataset. This data is used to train the diffusion model and interaction field.\n3.1\nMotion Generation using Diffusion Modeling\nMotion Representation. Motion generation is formulated as predicting a sequence of 3D human\npose states that capture a person\u2019s motion over time. The pose state representation is based on the\nSMPL body model [23] and is similar to prior successful human motion diffusion models [9, 42].\nThe human pose state Xi at frame i in a motion sequence is:\nXi = {jp\ni , jr\ni , jv\ni , j\u03c9\ni , tp\ni , tv\ni },\n(1)\nwhich includes joint positions jp\ni \u2208 R3\u00d722, rotations jr\ni \u2208 R6\u00d722, velocities jv\ni \u2208 R3\u00d722, and\nangular velocities j\u03c9\ni \u2208 R3\u00d722 for all 22 SMPL joints including the root (pelvis). Additionally, the\nSMPL global translation tp\ni \u2208 R3 and velocity tv\ni \u2208 R3 are included. A motion (trajectory) is a\nsequence of N poses denoted as \u03c4 = {X1, . . . , XN} where all poses are in a canonical coordinate\nframe, namely, the local frame of the pose X1 at the first timestep where the human is at the origin\nand its front-facing vector is aligned with the +Y axis.\nModel Formulation. The diffusion model simultaneously generates all human poses in a motion\nsequence [42] to achieve a desired interaction. Intuitively, diffusion is a noising process that converts\nclean data into noise. We want our motion model to learn the reverse of this process so that realistic\n3\nDiffusion Model\n(Transformer Encoder)\nMotion \nGenerator\nObject \nInteraction \nField\nGuidance\nInitial Pose \n& Shape\nObject \nGeometry\nObject\nPose\nOutput: Generated Motion\nLinear\nMLP\nMLP\nPointNet\nMLP\nNoisy Motion\nClean Motion\nInputs:\nObject Interaction Field\n(Transformer Encoder)\nPointNet\nPose Correction\n= Pos-Enc\nLinear\nInteraction Pose\nFigure 2: Model Architecture. Our full motion synthesis method (middle) consists of an object\ninteraction field F\u03d5 (left), which guides the diffusion model M\u03b8 (right) at sampling time to produce\nplausible interaction motions. At each step k \u2208 [0, K = 1000] of denoising, the diffusion model\npredicts a clean motion \u02c6\u03c4 0 from a noisy motion input \u03c4 k and conditioning information. The object\ninteraction field takes the last pose from the diffusion output as input, and uses guidance to push the\npose towards the valid interaction manifold using a predicted pose correction.\nmotions can be generated from randomly sampled noise. Mathematically, forward diffusion is a\nMarkov process with a transition probability distribution:\nq(\u03c4 k|\u03c4 k\u22121) := N(\u03c4 k; \u00b5 =\np\n1 \u2212 \u03b2k\u03c4 k\u22121, \u03c3 = \u03b2kI),\n(2)\nwhere \u03c4 k denotes the motion trajectory at the kth noising step, and a fixed \u03b2k is chosen such\nthat q(\u03c4 K) \u2248 N(\u03c4 K; 0, I) after K steps. Our generative model learns the reverse of this process\n(denoising), i.e., it recovers \u03c4 k\u22121 from a noisy input trajectory \u03c4 k at each step and doing this\nrepeatedly results in a final clean motion \u03c4 0. Because the model is generating interaction motions\nwith an object, we condition denoising on interaction information C = {Po, Ro, b, X0}, which\nincludes the canonicalized object point cloud Po \u2208 R5000\u00d73, rigid object pose relative to the person\nRo \u2208 R4\u00d74, SMPL body shape parameters b \u2208 R10, and starting pose of the person X0. Each\nreverse step is then:\np\u03b8(\u03c4 k\u22121|\u03c4 k, C) := N(\u03c4 k\u22121; \u00b5 = \u00b5\u03b8(\u03c4 k, k, C), \u03c3 = \u03b2kI),\n(3)\nwhere the diffusion step k is also given as input. Instead of predicting the noise \u03f5k added at\neach step of the diffusion process [14, 20], our model directly predicts the final clean signal [34,\n42]. Mathematically, the motion model M\u03b8 with parameters \u03b8 predicts a clean trajectory \u02c6\u03c4 0 =\nM\u03b8(\u03c4 k, k, C) from which the mean \u00b5\u03b8(\u03c4 k, k, C) is easily computed [28]. This formulation has the\nbenefit that physically grounded objectives can be easily computed on \u02c6\u03c4 0 in the pose space, which is\nuseful for guidance as discussed below.\nWhile training the diffusion model, a ground truth clean trajectory \u03c4 0 is noised and given as input,\nthen the model is trained to minimize the objective \u2225\u02c6\u03c4 0 \u2212 \u03c4 0\u22252\n2. To enable using classifier-free\nguidance [15] at sampling time, the conditioning C is randomly masked out with 10% probability\nduring the training process so that the model can operate in both conditional and unconditional modes.\nSampling and Guidance. At test time, samples are generated from the model given random noise\nand interaction conditioning C as input. We find that leveraging classifier-free guidance [15] tends to\ngenerate higher-quality samples. This amounts to generating one conditional and one unconditional\nsample from the model and then combining them as \u02c6\u03c4 0 = M\u03b8(\u03c4 k, k)+s(M\u03b8(\u03c4 k, k, C)\u2212M\u03b8(\u03c4 k, k)),\nwhere the strength of the conditioning is controlled by the scalar s.\nEnsuring that the sampled motions adhere to the geometric and semantic constraints of the object is\nkey to plausible interactions. Diffusion models are well-suited for this, since guidance can encourage\nsamples to meet desired objectives at test time [20]. The core of guidance is a differentiable function\nG(\u03c4 0) that evaluates how well a trajectory meets a desired objective; this could be a learned [20]\nor an analytic [34] function. In our case, we want G(\u03c4 0) to evaluate how plausible an interaction\nmotion is w.r.t. the object, and in \u00a73.2 we show that this can be done with a learned object interaction\n4\nfield. Throughout denoising during sampling, the gradient of the objective function will be used to\nnudge trajectory samples in the correct direction. We use a formulation of guidance that perturbs the\nclean trajectory output from the model \u02c6\u03c4 0 at every denoising step k as follows [16, 34]:\n\u02dc\u03c4 0 = \u02c6\u03c4 0 \u2212 \u03b1\u2207\u03c4 kG(\u02c6\u03c4 0)\n(4)\nwhere \u03b1 controls the guidance strength. The updated trajectory \u02dc\u03c4 0 is then used to compute \u00b5.\nArchitecture. As shown in Fig. 2 (right), the motion model M\u03b8 is based on a transformer encoder-\nonly architecture [42, 46]. The model takes as input the current noisy trajectory \u03c4 k, the denoising\nstep k, and the conditioning C. Each human pose in the trajectory is a token, while each conditioning\nbecomes a separate token. Of note, the object point cloud Po is encoded with a PointNet [32], the\nrigid pose Ro is encoded with a three-layer MLP, and k is encoded using a positional embedding [41].\nOur noise levels k vary between 0 to 1000 diffusion steps. The transformer handles variable-\nlength sequence inputs and outputs the clean motion prediction \u02c6\u03c4 0. Full details are available in the\nsupplementary material.\n3.2\nObject Interaction Fields\nAfter training on human-object interactions, the diffusion model can generate reasonable motion\nsequences but fails to fully comply with constraints in the last mile of interaction [2, 7], even when\nconditioned on the object. This causes undesirable artifacts such as penetration with the object.\nFigure 3: Interaction Field Visualization. We\nquery the field in several locations with a sitting\npose (a subset shown in grey) and visualize the\noutput for pelvis, feet, and neck joints. All cylin-\nders are oriented towards the chair, indicating the\ncorrection vector\u2019s magnitude and direction. This\ncorrection is due to the misalignment between the\nsitting pose and chair position.\nTo alleviate this issue, we propose to guide mo-\ntion samples from the diffusion model (i.e., use\nEq. (4)) with a learned objective G that captures\nrealistic interactions for a specific object.\nWe take inspiration from recent work that uses\nneural distance fields to learn valid human\npose manifolds [43] and robotic grasping mani-\nfolds [51]. For our purposes, the field must take\nin an arbitrary human pose and output how far\nthe query pose is from being a \u201cvalid\u201d object\ninteraction pose. We define an interaction pose\nto be an anchor frame in a motion sequence that\ncaptures the core of the interaction, e.g., the mo-\nment a person settles in a chair during sitting (as\nin Fig. 1) or contacts an object before lifting.\nWe propose an object interaction field that op-\nerates in the local coordinate frame of a spe-\ncific object. The interaction field F\u03d5 takes as\ninput a simplified pose \u02dcX={jp, tp}, which in-\ncludes joint positions and global translation. The\nfield outputs an offset vector \u2206 \u02dcX=F\u03d5( \u02dcX) that\nprojects the input pose to the manifold of valid interaction poses for the object: \u02dcX+\u2206 \u02dcX is then a\nplausible interaction pose. Fig. 3 visualizes the output vectors of an example interaction field for a\nchair. Querying the field with a sitting pose away from the chair (i.e., not a valid interaction) gives a\ncorrection pointing back towards the chair. For further away points, the visualized vectors are longer,\nindicating larger corrections are needed.\nGuidance Objective. The object interaction field serves as a differentiable function that can be\nincorporated into the guidance objective to judge how far a motion is from the desired interaction\nmanifold. Let \u02dcXi \u2208 \u03c4 be the simplified pose from the ith frame of a motion \u03c4. If we know that this\npose should be a valid interaction pose, then the guidance objective is defined as G(\u03c4) = \u2225F\u03d5( \u02dcXi)\u22252\n2.\nDuring denoising at test time, we feed output poses from the diffusion model into this guidance\nobjective to encourage the generated motion to contain a valid interaction poses.\nTraining. Supervising F\u03d5 requires a dataset of invalid poses with corresponding valid interaction\nposes. We collect this after training the diffusion model detailed in \u00a73.1. In particular, we feed a\nnoisy ground-truth interaction motion \u03c4 k at a random noise level k to the diffusion model as input.\nThis gives an output motion \u02c6\u03c4 0, which should match the ground truth \u03c4 0 if the model is perfect. In\npractice, denoising back to ground truth is difficult at high noise levels (e.g., k=900), so we consider\n5\nFigure 4: Generated Synthetic Data. We visualize motion sequences from one tree rollout for one\nsitting anchor pose. The middle shows a bird\u2019s-eye view of the pelvis joint trajectories in light\npink. All trajectories end in the same sitting pose, but start at diverse locations around the chair. We\nhighlight a few trajectories in blue and show full-body motions from the corresponding generations\non the left and right sides. Our complete dataset contains many trees for different objects and humans.\n\u02c6\u03c4 0 as an invalid interaction motion with a corresponding valid motion \u03c4 0. When the diffusion model\nhas been trained on the dataset described in \u00a73.3, we know that the last frame of the motion \u02dcXN \u2208 \u02c6\u03c4 0\nshould be the interaction pose, so we can throw away all other poses to arrive at a training dataset for\nthe interaction field. We further augment this dataset by applying random rigid transformations to the\ninvalid interaction poses.\nGiven a ground truth interaction pose \u02dcYN \u2208 \u03c4 0 and corresponding output pose from the diffusion\nmodel \u02dcXN \u2208 \u02c6\u03c4 0, the interaction field training loss is computed as \u2225F\u03d5( \u02dcXN) \u2212 ( \u02dcYN \u2212 \u02dcXN)\u22251. Note\nthat training on outputs from the diffusion model is important since the interaction field operates on\nthese kinds of outputs during test-time guidance.\nArchitecture. As shown in Fig. 2 (left), the interaction field architecture is an encoder-only trans-\nformer that operates on the input pose as a token. In practice, it also takes in the canonical object\npoint cloud as a conditioning token to allow training a single field for multiple objects.\n3.3\nAutomatic Synthetic Data Generation\nTraining the diffusion model requires a large, realistic, and diverse dataset of motions for each\nhuman-object interaction we wish to synthesize. Unfortunately, this data exists only for specific\ninteractions [56] and is difficult and expensive to collect from scratch. Therefore, we propose an\nautomated pipeline to generate synthetic interaction motion data. In short, we first select anchor\npose frames from an existing small dataset [3] that are indicative of an interaction we want to learn.\nOur key insight is to use a pre-trained scene-unaware motion model [33] to sample a diverse set of\nmotions that end at a selected anchor pose, and therefore demonstrate the desired interaction. We\nprovide the key details in this section and a full description appears in the supplementary material.\nAnchor Pose Selection. We require a small set of anchor poses that capture the core frame of an\ninteraction motion. As described in \u00a73.2, for sitting on a chair this is the sitting pose when the person\nfirst becomes settled in the chair (see Fig. 1). In generating motion data, these anchor poses will be\nthe final frame of each synthesized motion sequence. For the experiments in \u00a74, these anchor frames\nare chosen manually from a small dataset that contains a variety of interactions [3].\nGenerating Motions in Reverse. The goal is to generate human motions that end in the chosen\nanchor poses and reflect realistic object interactions. We leverage HuMoR [33], which is a conditional\nmotion VAE trained on the AMASS [25] mocap dataset. It generates realistic human motions through\nautoregressive rollout, but is scene-unaware. To force rollouts from HuMoR to match the final anchor\npose, we could use online sampling or latent motion optimization, but these are expensive and not\nguaranteed to exactly converge. Instead, we re-train HuMoR as a time-reversed motion model that\npredicts the past instead of the future motion given a current input pose. Starting from a desired\ninteraction anchor pose XN, our reverse HuMoR will generate XN\u22121, XN\u22122, \u00b7 \u00b7 \u00b7 , X1 forming a full\ninteraction motion that, by construction, ends in the desired pose.\nTree-Based Rollout & Filtering. To ensure sufficient diversity and realism in motions from HuMoR,\nwe devise a branching rollout strategy that is amenable to frequent filtering and results in a tree of\n6\nplausible interactions. Starting from the anchor pose, we first sample 30 frames (1 sec) of motion.\nThen, multiple branches are instantiated and random rollout continues for another 30 frames on these\nbranches independently. Continuing in this branching fashion allows growing the motion dataset\nexponentially while also filtering to ensure branches are sufficiently diverse and do not contain\nundesireable motions. Filtering involves heuristically pruning branches with motions that collide\nwith the object, float above the ground plane, result in unnatural pose configurations, and become\nstationary. For the experiments in \u00a74, we rollout to a tree depth of 7 and sample many motion\ntrees starting from each anchor pose. Individual paths are extracted from the tree to give interaction\nmotions, and we post-filter out sequences that start within 1 meter of the object.\nGenerated Datasets. We use this scalable strategy to generate data for training our motion model\nfor sitting and lifting interactions. Fig. 4 demonstrates the diversity of our generated datasets by\nvisualizing top-down trajectories and example motions from a single tree of sitting motions. For the\nsitting interaction dataset, we choose 174 anchor pose frames across 7 subjects in the BEHAVE [3]\ndataset. This results in a dataset of 200K motion sequences that include sitting on chairs, stools,\ntables, and exercise balls. Each motion sequence in this dataset ends at a sitting anchor pose. For\nlifting interactions, 72 anchor poses from 7 subjects produces 110K motion sequences. Each sequence\nends at a lifting anchor pose when the person initially contacts the object.\n4\nExperiments\nWe evaluate our NIFTY method after training on the sitting and lifting datasets introduced in \u00a73.3.\nImplementation details are given in \u00a74.1, followed by a discussion of evaluation metrics in \u00a74.2 and\nbaselines in \u00a74.3. Experimental results are presented in \u00a74.4 along with an ablation study in \u00a74.5.\n4.1\nImplementation Details\nWe train our diffusion model M\u03b8 for 600K iterations with a batch size of 32 using the AdamW [24]\noptimizer with a learning rate of 10\u22124. A separate model is trained for sitting and lifting. We use\nK=1000 diffusion steps in our model and sample the diffusion step k from a uniform distribution\nat each training iteration. The object interaction field F\u03d5 is trained on the data described in \u00a73.2\nfor 300K iterations using AdamW with a maximum learning rate of 5 \u00d7 10\u22125 and a one cycle LR\nschedule [36]. When sampling from the diffusion model, 10 samples are generated in parallel and all\nare guided using the object interaction field; the sample with the best guidance objective score is used\nas the output. We apply interaction field guidance on the last frame of motion (i.e., the interaction\nanchor pose in our datasets). Our models are trained using PyTorch [29] on NVIDIA A40 GPUs, and\ntake about 2 days to train. Visualizations use the PyRender engine [26].\n4.2\nEvaluation Setting and Metrics\nTo ensure we properly evaluate the generalization capability of methods trained on our synthetic\ninteraction datasets, we do not create a test set using the procedure described in Sec. 3.3, which may\nresult in a very similar distribution to training data. Instead, we create a set of 500 test scenes for\neach action, where objects are randomly placed in the scene and the human starts from a random pose\ngenerated by HuMoR. All methods are tested on these same scenes during evaluation.\nEvaluating human motion coupled with object interactions is challenging and has no standardized\nprotocol. Hence, we evaluate using a diverse set of metrics including a user perceptual study. We\nbriefly describe the metrics next and include full details in the supplementary material.\nUser Study. No single metric can capture all the nuances of human-object interactions, so we employ\na perceptual study [27, 39, 40, 42, 50]. For each method, we create videos from generated motions\non the test scenes. To compare two methods, users are presented with two videos on the same test\nscene and must choose which they prefer (full user directions are in the supplement). We perform\nindependent user studies for lifting and sitting actions using hive.ai [1]. Responses are collected\nfrom 5 users for every comparison video, giving 2500 total responses in each comparison study.\nFoot Skating. Similar to prior work [27], we define the foot-skating score for a sequence of N\ntimesteps as\n1\nN\nPN\ni vi(2 \u2212 2hi/H) \u00b7 1h<=H, where vi is the velocity and hi is the height above\nground of the right toe vertex for the ith frame. H is 2.5 cm. Intuitively, this is the mean foot velocity\nwhen it is near the ground (where it should be 0), with higher weight applied closer to the ground.\n7\nTable 1: Quantitative Comparison. Our method outperforms baselines on both sitting and lifting.\nOur diffusion model, guided by the learned interaction field, generates motions that reach the object\n(D2O) with few penetrations and realistic contacts. Motions approaching the object are realistic with\nlow foot skating and the final interaction pose is similar to synthetic data with low skeleton distance.\nSitting\nLifting\nMethod\nFoot\n% D2O\nD2O\nSkel.\nContact\n% Pen.\nFoot\n% D2O\nD2O\nSkel.\nContact\n% Pen.\nSkating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 Dist. \u2193\nIoU \u2191\n\u2264 2cm \u2191 Skating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 Dist. \u2193\nIoU \u2191\n\u2264 2cm \u2191\nCond. VAE [50]\n0.77\n88.8\n0.13\n1.07\n0.21\n46.6\n0.66\n57.4\n0.66\n1.70\n0.03\n60.1\nCond. MDM [42]\n0.36\n37.9\n1.06\n2.81\n0.04\n50.5\n0.28\n36.3\n1.14\n2.58\n0.02\n46.2\nNIFTY (ours)\n0.47\n99.6\n0.00\n0.54\n0.54\n65.0\n0.34\n77.7\n0.05\n0.42\n0.17\n68.5\nDistance to Object (D2O). Similar to prior work [50], this evaluates whether the human gets close\nto the object during the interaction. It measures the minimum distance from the human body in the\nlast frame of the motion sequence to any point on the object\u2019s surface. We report the % of sequences\nwithin 2 cm distance to avoid sensitivity to outliers, along with the 95th percentile ( \u02dc%) of this distance.\nPenetration Score (% Pen). To evaluate realism as the human approaches an object for interaction,\nwe measure how much they penetrate the object. Based on our synthetic data, we define the first NA\nframes of motion to be the approach for each action type (see supplement).\nThen the penetration distance for a trajectory is\n1\nNA\nP\nv\nPNA\ni\nsdfi(v) \u00b7 1sdfi(v)>0, where sdfi is\nthe signed distance function of the human in the ith frame and v is one of 2K points on the object\u2019s\nsurface. We report the percentage of trajectories with penetration distance \u2264 2 cm (% Pen. \u2264 2cm)\nignoring trajectories with D2O > 2 cm, since trajectories that do not approach the object will trivially\navoid penetration.\nSkeleton Distance & Contact IoU. These evaluate how well generated interaction poses align with\nground truth poses and their human-object contacts. We start by finding the minimum distance\nbetween the final pose of a generated sequence and the anchor poses in the synthetic training data.\nThe distance to this nearest neighbor pose is reported as the skeleton distance. To measure how well\ncontacts from the generated motion match the data, we compute the IoU between contacting vertices\n(those that penetrate the object) on the predicted body mesh and those on the nearest neighbor mesh.\n4.3\nBaselines\nCond. VAE [50]. Closest to our problem definition, this model comes from recent work HUMAN-\nISE [50], which learns plausible human motions conditioned on scene and language for four actions\n(lie, sit, stand, walk). This state-of-the-art model is a conditional VAE with a GRU motion encoder\nand sequence-level transformer decoder. Since we evaluate on sitting and lifting actions separately,\nwe modify their approach to remove language conditioning. The model is trained on our synthetic\ndata for 600K iterations with the recommended hyperparameters and learning rate of 10\u22124.\nCond. MDM [42].\nThis baseline is the motion diffusion model (MDM) [42] with added\nconditioning C, i.e., our object-conditioned diffusion model without interaction field guidance.\nvs. \n Cond. VAE\nvs. \n Cond. MDM\nvs. \n Syn. Data\n0\n50\n100\n88.7\n91.0\n46.7\n90.0\n81.6\n47.6\n% preferred NIFTY\nSit\nLift\nFigure 5: User Study. NIFTY is preferred\n\u2265 88.7% of the time for sitting and \u226581.6%\nfor lifting compared to baselines. Our mo-\ntions are also nearly indistinguishable from\nsynthetic data trajectories.\n4.4\nExperimental Results\nUser Study. Fig. 5 shows how often users prefer\nour method (NIFTY) over baselines and Synthetic\nData (Syn. Data) for both sitting and lifting. We\nperform separate studies for each comparison. Users\nprefer NIFTY over baselines a vast majority of the\ntime. Averaged over both actions, NIFTY is preferred\nover the Cond. VAE [50] baseline 89.4% of the time.\nSimilarly, NIFTY is preferred over Cond. MDM [42]\n86.3% of the time, highlighting the importance of\nusing guidance with our interaction field during sam-\npling. Compared to held out motions from synthetic\ndata, NIFTY is preferred 47.2% of the time, which in-\ndicates that the motions are nearly indistinguishable\nfrom those of our data generation pipeline.\n8\nTable 2: Ablation Study. Comparison between using an interaction field trained to predict a full\noffset vector (NIFTY) or a single scalar distance (Distance OIF).\nSitting\nLifting\nMethod\nFoot\n% D2O\nD2O\nSkel.\nContact\n% Pen.\nFoot\n% D2O\nD2O\nSkel.\nContact\n% Pen.\nSkating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 Dist. \u2193\nIoU \u2191\n\u2264 2cm \u2191 Skating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 Dist. \u2193\nIoU \u2191\n\u2264 2cm \u2191\nDistance OIF\n0.41\n80.9\n0.47\n1.25\n0.24\n66.8\n0.30\n57.4\n0.74\n1.31\n0.07\n70.1\nNIFTY\n0.47\n99.6\n0.00\n0.54\n0.54\n65.0\n0.34\n77.7\n0.05\n0.42\n0.17\n68.5\nNIFTY\nCond. MDM\nCond. VAE\nSitting Interaction\nLifting Interaction\nFigure 6: Qualitative Results. Our method (bottom row) generates realistic interaction motions that\nreach the desired object with plausible contacts (e.g. col 1 & 4) while avoiding penetrations, unlike\nbaselines. The mesh color gets darker as time progresses. Cond. VAE [50] motions have the final\ninteraction pose away from the object (col 1,3,4), incorrect (col 2 & 5), or intersecting (col 5). Cond.\nMDM [42] generates sitting poses far away from the object (col 1 & 3).\nWe further extract robust consensus across users by majority vote over the 5 responses for each video.\nIn this case, motions generated by our method are preferred 94.4% (sitting) and 97.8% (lifting) of the\ntime over Cond. VAE [50] motions, making the improvement gap even more apparent.\nAdditionally, we also conduct an user study on a Likert scale of scores between 1 (unrealistic) to 5\n(very realistic). We report that motions from our synthetic dataset achieve a score of 4.39 vs. 4.87 for\nmotions in the AMASS [25]. Further details are available in Supp. \u00a7 A.1.\nQuantiative Results. In Tab. 1, NIFTY is compared to baselines for both sitting and lifting interac-\ntions. NIFTY generates motions that reach the target object and approach realistically, as indicated\nby distance-to-object (D2O) and penetration metrics. Although Cond. MDM [42] produces realistic\nmotion with low foot skating, it struggles to properly approach the object since it does not use\nguidance from the learned interaction field. We see that interaction poses and the resulting object\ncontacts generated by our method do reflect the synthetic dataset, resulting in low skeleton distance\nand high contact IoU, unlike Cond. VAE [50] which is worse across all metrics.\nQualitative Results. Fig. 6 shows a qualitative comparison between motions generated by our method\nand baselines. NIFTY synthesizes realistic sitting and lifting with a variety of objects. Examples show\nthat the baselines struggle to generalize to unseen object poses, and have no mechanism to correct for\nthis at test time. Our learned interactions field helps to avoid this through diffusion guidance. Please\nsee the videos provided in the supplement to best appreciate the results.\n4.5\nAblation Study\nAs detailed in \u00a73.2, our object interaction field (OIF) is formulated to predict an offset vector \u2206 \u02dcX\nthat captures both distance and direction for each component of the pose state, rather than a single\nfull-body distance like prior work [43]. We ablate this design decision in Tab. 2, which compares our\ninteraction field formulation to a version that predicts only a scalar distance to the interaction pose\nmanifold (Distance OIF). We observe that learning a single distance is a harder task compared to\n9\npredicting an offset vector, which provides a stronger learning signal for training. As a result, the\nablated interaction field results in worse scores across most metrics.\n5. Conclusion and Limitations\nWe introduced NIFTY, a framework for learning to synthesize realistic human motions involving 3D\nobject interactions. Results demonstrate that our object-conditioned diffusion model gives improved\nmotions over prior work when guided by a learned object interaction field and trained on automatically\nsynthesized motion data. Our current approach is limited to the body shapes present in the training\ndata (e.g., the 7 subjects from BEHAVE [3]), so future work should explore data augmentation\nstrategies to generalize to novel humans. Moreover, we have shown results on sitting and lifting, but\nwe would like to widen the scope to handle additional interactions by collecting new anchor poses,\nsynthesizing data, and training our diffusion model and interaction field.\nAcknowledgements.. We express our gratitude to our colleagues for the fantastic project discussions\nand feedback provided at different stages. We have organized them by institution (in alphabetical\norder)\n\u2013 Google: Matthew Brown, Frank Dellaert, Thomas A. Funkhouser, Varun Jampani\n\u2013 Google (co-interns): Songyou Peng, Mikaela Uy, Guandao Yang, Xiaoshuai Zhang\n\u2013 University of Michigan: Mohamed El Banani, Ang Cao, Karan Desai, Richard Higgins, Sarah\nJabbour, Linyi Jin, Jeongsoo Park, Chris Rockwell, Dandan Shan\nThis work was partly done when NK was interning at Google Research. DR was supported by the\nNVIDIA Graduate Fellowship.\nReferences\n[1] Hive.ai. https://thehive.ai/. Accessed: 2023-05-15. 7, 14, 17\n[2] Randall Balestriero and Yann LeCun. Police: Provably optimal linear constraint enforcement for deep\nneural networks. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 1\u20135, 2023. 5\n[3] Bharat Lal Bhatnagar, Xianghui Xie, Ilya Petrov, Cristian Sminchisescu, Christian Theobalt, and Gerard\nPons-Moll. Behave: Dataset and method for tracking human object interactions. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR). IEEE, jun 2022. 2, 3, 6, 7, 10\n[4] Yu-Wei Chao, Jimei Yang, Weifeng Chen, and Jia Deng. Learning to sit: Synthesizing human-chair\ninteractions via hierarchical control. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 35, pages 5887\u20135895, 2021. 2\n[5] Enric Corona, Albert Pumarola, Guillem Alenya, and Francesc Moreno-Noguer. Context-aware human mo-\ntion prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 6992\u20137001, 2020. 2\n[6] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian Theobalt. Mofusion: A\nframework for denoising-diffusion-based motion synthesis. In Computer Vision and Pattern Recognition\n(CVPR), 2023. 3\n[7] Priya L Donti, David Rolnick, and J Zico Kolter. Dc3: A learning method for optimization with hard\nconstraints. arXiv preprint arXiv:2104.12225, 2021. 5\n[8] Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, and Philipp Slusallek. Imos:\nIntent-driven full-body motion synthesis for human-object interactions. In Eurographics, 2023. 3\n[9] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and\nnatural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 5152\u20135161, June 2022. 3\n[10] Vladimir Guzov, Aymen Mir, Torsten Sattler, and Gerard Pons-Moll. Human poseitioning system (hps): 3d\nhuman pose estimation and self-localization in large scenes from body-mounted sensors. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4318\u20134329, 2021. 3\n[11] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael J Black.\nStochastic scene-aware motion prediction. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 11374\u201311384, 2021. 1, 2, 3\n10\n[12] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and Michael J Black. Resolving 3d human\npose ambiguities with 3d scene constraints. In Proceedings of the IEEE/CVF international conference on\ncomputer vision, pages 2282\u20132292, 2019. 3\n[13] Gustav Eje Henter, Simon Alexanderson, and Jonas Beskow. Moglow: Probabilistic and controllable\nmotion synthesis using normalising flows. ACM Transactions on Graphics (TOG), 39(6):1\u201314, 2020. 1, 2\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\nInformation Processing Systems, 33:6840\u20136851, 2020. 3, 4\n[15] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\n4\n[16] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.\nVideo diffusion models. arXiv preprint arXiv:2204.03458, 2022. 3, 5\n[17] Daniel Holden, Taku Komura, and Jun Saito. Phase-functioned neural networks for character control. ACM\nTransactions on Graphics (TOG), 36(4):1\u201313, 2017. 2\n[18] Chun-Hao P Huang, Hongwei Yi, Markus H\u00f6schle, Matvey Safroshkin, Tsvetelina Alexiadis, Senya\nPolikovsky, Daniel Scharstein, and Michael J Black. Capturing and inferring dense full-body human-scene\ncontact. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n13274\u201313285, 2022. 3\n[19] Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, and Song-Chun\nZhu. Diffusion-based generation, optimization, and planning in 3d scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 2, 3\n[20] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible\nbehavior synthesis. In International Conference on Machine Learning, 2022. 3, 4\n[21] Nan Jiang, Tengyu Liu, Zhexuan Cao, Jieming Cui, Yixin Chen, He Wang, Yixin Zhu, and Siyuan Huang.\nChairs: Towards full-body articulated human-object interaction. arXiv preprint arXiv:2212.10621, 2022. 3\n[22] Korrawe Karunratanakul, Jinlong Yang, Yan Zhang, Michael J Black, Krikamol Muandet, and Siyu Tang.\nGrasping field: Learning implicit representations for human grasps. In 2020 International Conference on\n3D Vision (3DV), pages 333\u2013344. IEEE, 2020. 3\n[23] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: A\nskinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia), 34(6):248:1\u2013248:16,\nOctober 2015. 3, 17\n[24] Ilya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017. 7\n[25] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. Amass:\nArchive of motion capture as surface shapes. In Proceedings of the IEEE/CVF international conference on\ncomputer vision, pages 5442\u20135451, 2019. 3, 6, 9, 23\n[26] Matthew Matl. Pyrender. https://github.com/mmatl/pyrender, 2019. 7\n[27] Aymen Mir, Xavier Puig, Angjoo Kanazawa, and Gerard Pons-Moll. Generating continual human motion\nin diverse 3d scenes. arXiv preprint arXiv:2304.02061, 2023. 7\n[28] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In\nInternational Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021. 3, 4\n[29] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep\nlearning library. Advances in neural information processing systems, 32, 2019. 7, 17\n[30] Mathis Petrovich, Michael J Black, and G\u00fcl Varol. Action-conditioned 3d human motion synthesis with\ntransformer vae. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n10985\u201310995, 2021. 2\n[31] Abhinanda R Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez, and\nMichael J Black. Babel: bodies, action and behavior with english labels. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 722\u2013731, 2021. 17\n11\n[32] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d\nclassification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 652\u2013660, 2017. 5\n[33] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas J. Guibas.\nHumor: 3d human motion model for robust pose estimation. In International Conference on Computer\nVision (ICCV), 2021. 1, 2, 3, 6, 15, 17, 23\n[34] Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Sanja Fidler, and\nOr Litany. Trace and pace: Controllable pedestrian animation via guided trajectory diffusion. CVPR, 2023.\n3, 4, 5\n[35] Manolis Savva, Angel X Chang, Pat Hanrahan, Matthew Fisher, and Matthias Nie\u00dfner. Pigraphs: learning\ninteraction snapshots from observations. ACM Transactions on Graphics (TOG), 35(4):1\u201312, 2016. 3\n[36] Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large\nlearning rates. In Artificial intelligence and machine learning for multi-domain operations applications,\nvolume 11006, pages 369\u2013386. SPIE, 2019. 7\n[37] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages\n2256\u20132265. PMLR, 2015. 3\n[38] Sebastian Starke, He Zhang, Taku Komura, and Jun Saito. Neural state machine for character-scene\ninteractions. ACM Trans. Graph., 38(6):209\u20131, 2019. 1, 2\n[39] Omid Taheri, Vasileios Choutas, Michael J Black, and Dimitrios Tzionas. Goal: Generating 4d whole-body\nmotion for hand-object grasping. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 13263\u201313273, 2022. 3, 7, 14\n[40] Omid Taheri, Nima Ghorbani, Michael J Black, and Dimitrios Tzionas. Grab: A dataset of whole-body\nhuman grasping of objects. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK,\nAugust 23\u201328, 2020, Proceedings, Part IV 16, pages 581\u2013600. Springer, 2020. 3, 7, 14\n[41] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh\nSinghal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high\nfrequency functions in low dimensional domains. Advances in Neural Information Processing Systems,\n33:7537\u20137547, 2020. 5\n[42] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Amit H Bermano, and Daniel Cohen-Or. Human\nmotion diffusion model. ICLR, 2023. 1, 2, 3, 4, 5, 7, 8, 9\n[43] Garvita Tiwari, Dimitrije Anti\u00b4c, Jan Eric Lenssen, Nikolaos Sarafianos, Tony Tung, and Gerard Pons-Moll.\nPose-ndf: Modeling human pose manifolds with neural distance fields. In Computer Vision\u2013ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part V, pages 572\u2013589.\nSpringer, 2022. 3, 5, 9\n[44] Jonathan Tseng, Rodrigo Castellon, and C Karen Liu. Edge: Editable dance generation from music. arXiv\npreprint arXiv:2211.10658, 2022. 3\n[45] Julen Urain, Niklas Funk, Georgia Chalvatzaki, and Jan Peters. Se (3)-diffusionfields: Learning cost\nfunctions for joint grasp and motion optimization through diffusion. arXiv preprint arXiv:2209.03855,\n2022. 3\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,\n30, 2017. 5\n[47] Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xiaolong Wang. Synthesizing long-term 3d human\nmotion and interaction in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9401\u20139411, 2021. 1, 2\n[48] Jingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua Lin, and Bo Dai. Towards diverse and natural\nscene-aware 3d human motion synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 20460\u201320469, 2022. 1, 2\n[49] Jingbo Wang, Sijie Yan, Bo Dai, and Dahua Lin. Scene-aware generative network for human motion\nsynthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 12206\u201312215, 2021. 2\n12\n[50] Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang. Humanise: Language-\nconditioned human motion generation in 3d scenes. In Advances in Neural Information Processing Systems\n(NeurIPS), 2022. 1, 3, 7, 8, 9, 14\n[51] Thomas Weng, David Held, Franziska Meier, and Mustafa Mukadam. Neural grasp distance fields for\nrobot manipulation. IEEE International Conference on Robotics and Automation (ICRA), 2023. 3, 5\n[52] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari,\nJames Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual computing and beyond. In\nComputer Graphics Forum, volume 41, pages 641\u2013676. Wiley Online Library, 2022. 3\n[53] Sifan Ye, Yixing Wang, Jiaman Li, Dennis Park, C Karen Liu, Huazhe Xu, and Jiajun Wu. Scene synthesis\nfrom human motion. In SIGGRAPH Asia 2022 Conference Papers, pages 1\u20139, 2022. 3\n[54] Hongwei Yi, Chun-Hao P. Huang, Shashank Tripathi, Lea Hering, Justus Thies, and Michael J. Black.\nMIME: Human-aware 3D scene generation.\nIn IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2023. 3\n[55] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Mo-\ntiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001,\n2022. 3\n[56] Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Vladimir Guzov, and Gerard Pons-Moll. Couch:\nTowards controllable human-chair interactions. October 2022. 1, 2, 3, 6\n[57] Yan Zhang and Siyu Tang. The wanderings of odysseus in 3d scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 20481\u201320491, 2022. 2, 3\n[58] Yang Zheng, Yanchao Yang, Kaichun Mo, Jiaman Li, Tao Yu, Yebin Liu, C Karen Liu, and Leonidas J\nGuibas. Gimo: Gaze-informed human motion prediction in context. In Computer Vision\u2013ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XIII, pages 676\u2013694.\nSpringer, 2022. 3\n13\nA\nAutomated Synthetic Training Data Generation\nAll models in the paper train on synthetic human-object interaction motion data generated using this\npipeline. To evaluate the quality of generated data compared to other data, in \u00a7 A.1 we perform a\nlarge scale user-study with 10K user responses. In \u00a7 A.2 we describe the complete details of data\ngeneration including pseudo-code for the algorithm.\nA.1\nData Quality User Study\nOur synthetic data generation pipeline helps us collect high-quality motion data corresponding to\ndifferent interaction anchor poses. We show that this generated data is high-quality by conducting a\nuser study on a five-point Likert scale as in prior work [39, 40]. Our results show that the generated\nsynthetic training data is on par with data collected using a real mocap setup.\nUser Study Setup. We created a user-study dataset of 2000 videos, consisting of 500 motions\nfrom the AMASS subset of HUMANISE sitting data [50] (i.e., real-world motion captured data),\n500 motions from our data generation pipeline, 500 predicted motions from our NIFTY sitting\nmodel, and 500 from Cond. VAE [50] predictions. For each motion, we rendered a video without\nan object present in the scene to make the source of the video indistinguishable. All motions had a\nrandom number of frames uniformly sampled from 60 to 120, where the last motion frame always\ncorresponded to the sitting interaction pose. We only show results on sitting as the HUMNISE [50]\ndoes not have lifting interaction AMASS subset in their data.\nWe ask the users to rate the video on its realism. Users are asked to rate on a scale of 1 to 5\ncorresponding to \u201cStrongly Disagree\", \u201cDisagree\", \u201cNeutral\", \u201cAgree\", \u201cStrongly Agree\". We set up\nthe study on hive.ai [1]. Instructions to the user are shown in Fig. 7.\nUser Study Results. Results are shown in Fig. 8. As expected, AMASS has a high realism score\nof 4.87 since it is actual mocap data. Training data generated using our algorithm has an average\nuser rating of 4.39, implying the quality is comparable motion collected using an expensive mocap\nsetup. We also report the performance of NIFTY and Cond. VAE [50] methods on the same study for\ncompleteness. NIFTY achieves a strong score of 4.11 (between \u201cagree\u201d and \u201cstrongly agree\u201d), which\nis close to score of the Syn. Data. The Cond. VAE [50] performance remains low at 2.33 (between\n\u201cdisagree\u201d and \u201cneutral\u201d).\nFiltering Unreliable Users. Note that every user is required to pass a qualification test containing\neasy examples to label. User accuracy is computed and users with accuracy > 60% are admitted. To\nensure that we collect valid responses and that users completely understand the task during the actual\nstudy, they are occasionally tested on \u201cobvious\" data called \u201choney pots\" during the labeling process.\nTo this end, we add motions with objective \u201cStrongly Agree\" labels (motions from AMASS) and\nsome with \"Strongly Disagree\" labels (low-quality motions generated by cVAE). This is common\npractice while conducting such a study, and we also do this for the user study in the main paper as\nFigure 7: Likert User Study. We conduct a user study to assess the motion quality in our Synthetic\nDataset. On the left, we present the qualification instructions for participants, allowing only those\nwho perform well to proceed to the actual study. On the right, we display the user interface used\nfor labeling motions, where users select from five options: \u201cStrongly Agree\", \u201cAgree\", \u201cNeutral\",\n\u201cDisagree\", or \u201cStrongly Disagree\". The results of this study can be found in Fig. 8\n14\nCond.\n VAE\nNIFTY\n(ours)\nSyn.\n Data\nAMASS\n Motions\n1\n3\n5\nLikert Scale (1-5)\n2.33\n4.11\n4.39\n4.87\nUser Rating (Motion Realism)\nSit\nFigure 8: Likert User Study Results. We conduct a study to judge the realism of sitting motions\non a scale of 1-5. Instructions for this study are available in \u00a7A.1. We show that synthetic training\ndata (Syn. Data) generated using our algorithm Algorithm 1 has an average rating of 4.39. This is\ncomparable to AMASS motions which represent quality of real captured data (using a mocap setup).\ndetailed in \u00a7C.1. The honeypot accuracy for this task is set at 82%: drops in performance below this\nthresholds removes a user from continuing the study any further.\nA.2\nTraining Data Synthesis Algorithm\nOur generation process revolves around utilizing a pretrained motion model, specifically the HuMoR\ngenerative model [33], to produce motion trajectories that end in a specific anchor pose. However, we\ntrain this model on reverse-time sequences, enabling us to generate reverse-time sequences that start\nfrom the provided anchor seed pose. Then, when we convert these rollouts into forward motions (i.e.,\nplay them backwards), the final generated pose in the rollout aligns with the anchor pose by design.\nOur full algorithm for generating a single motion tree is shown in Algorithm 1. This algorithm\nconstructs a tree of a specified depth, where each node corresponds to a 1 sec motion clip. Each node\nis connected to several possible branches to continue the motion (based on a branching factor B).\nThe algorithm begins by creating a root node starting at an input anchor pose. It then repeatedly\nconstructs the tree by generating motion sequences using the RollOut function and checking their\nvalidity using the PruneCheck function. If a valid motion sequence is obtained, a child node is\ncreated and added to the tree. The process continues until the desired depth is reached or the tree is\nfully explored (no more branches left to explore)\nThe algorithm maintains a queue of nodes to be processed, allowing for breadth-first construction of\nthe tree. If a node reaches the maximum depth, it is skipped to ensure the tree is constructed as per\nthe specified depth. The algorithm outputs the resulting tree, which contains valid motion sequences\nas paths from the root to the leaf nodes.\nRollOut Function. The RollOut function takes an start pose and utilizes the pre-trained motion\nmodel to generate a short 1 sec (30 frame) motion sequence. It iteratively runs the motion model\nuntil a valid sequence is obtained or a specified maximum number of attempts is reached. If a valid\nsequence is found, it is returned as the generated motion.\nPruneCheck Function. The PruneCheck function examines a given motion sequence to determine\nits validity. It algorithmically checks if the motion collides with the object, has unnatural human\nposes, if the human is floating in the air, or intersecting with the floor etc.. It returns a boolean value\nindicating whether the motion sequence is valid or not.\nImplementation. In our implementation, we set B as 6 for the nodes at depths 1 and 2, while\nB = 2 for nodes at higher depths. We also set NTries as 20 to secure a good rollout sequence. We\nthen convert all the motion nodes in these trees into individual motion sequences for a particular\ninteraction.\n15\nAlgorithm 1 Tree Generation. Our proposed tree-roll out algorithm using a pre-trained motion-\nmodel\n1: function ROLLOUT(startPose, N)\n\u25b7 Input: start pose, N defining number of rollout attempts\n2:\nvalidSequence \u2190 False\n3:\ncount \u2190 0\n4:\nwhile not validSequence and count < N do\n5:\nmotion \u2190 pretrained motion model generate motion using startPose\n6:\nvalidSequence \u2190 PruneCheck(motion)\n7:\ncount \u2190 count + 1\n8:\nend while\n9:\nif validSequence then\n10:\nreturn motion\n11:\nelse\n12:\nreturn null\n13:\nend if\n14: end function\n15: function PRUNECHECK(motionSequence)\n\u25b7 Input: motion sequence\n16:\nvalid \u2190 check if motionSequence is valid\n17:\nreturn valid\n18: end function\n19: queue \u2190 empty queue\n20: rootAnchorPose \u2190 input anchor pose\n21: root \u2190 create root node NULL motion\n\u25b7 For the root node there is no past motion (NULL).\n22: root.lastPose \u2190 root.anchorPose\n\u25b7 The anchor pose is the seed for future roll-outs\n23: queue.push(root)\n24: while queue is not empty do\n25:\ncurrentNode \u2190 queue.pop()\n26:\nif currentNode.depth = MaxDepth then\n27:\ncontinue\n28:\nend if\n29:\nfor child \u2190 1 to B do\n30:\nGMotion \u2190 RollOut(currentNode.lastPose, NTries)\n\u25b7 Create a RollOut\n31:\nif GMotion \u0338= null then\n\u25b7 Check if Good RollOut?\n32:\nchildNode \u2190 create child node with GMotion\n33:\nchildNode.lastPose \u2190 GMotion last frame\n\u25b7 Set the last motion frame for\nchildNode\n34:\ncurrentNode.children.push(childNode)\n35:\nqueue.push(childNode)\n\u25b7 Add childNode to queue\n36:\nend if\n37:\nend for\n38: end while\n16\nB\nImplementation Details\nRecovering Motion from \u03c4 0. Our trajectory representation is over-parameterized and this allows\nusing the model outputs in multiple ways. To recover the generated motion we extract the per-frame\njoint angles jr\ni for the SMPL model. We integrate the velocity tv\ni along the XZ plane to recover the\nXZ translation for the root joint and extract the corresponding Y component (upward) from tp\ni . This\nstrategy of extracting motion from the output parameterization is motivated by our use of guidance\nwith the diffusion model, which only operates on the last frame of a motion sequence. By integrating\nvelocity predictions over time, applying the guidance objective at the last frame will still have a strong\neffect on earlier frames in the sequence.\nVariable Length Input. Our model takes input motion trajectories with up to 150 frames. For\ntraining, we pad motion sequences of lengths shorter than this with the last interaction frame from\nthe sequence.\nSMPL model. Our SMPL [23] model does not have hand articulation, so we use the SMPL model\nwith only 22 articulated joints.\nPre-trained Motion Model for Data Generation. We train the motion model on a subset of the\nAMASS dataset that does not contain extreme sporting actions like jumping, dancing, etc.We do this\nby removing sequences from AMASS based on the labels from the BABEL dataset [31]. We use the\nHuMoR-Qual [33] variant of the model to get high-quality motions, which uses the joint positions\ncomputed through the SMPL parametric model as input to future roll-out time steps (as opposed to\nusing its own joint position predictions).\nTransfomer\nEncoder..\nWe\nuse\na\ntransformer\nencoder\nimplemented\nusing\ntorch.nn.TransformerEncoder from PyTorch [29].\nOur each transformer layer consists\nof 4 heads and a latent dim on 512. We have 8 such layers in our transformer.\nC\nExperimental Details\nThis section provides additional details on the implementation of our user study and metrics from the\nmain paper in \u00a74.\nC.1\nA/B Test User Study\nWe conduct a user study to qualitatively evaluate the performance of two methods. We design a study\nsuch that, given a pair of motions, a user must choose one that is the most realistic. Specifically,\nwe ask the user \u201cWhich motion among the both is more realistic?\" when we show them two videos\n(each containing a motion generated by a different method) \u201cLEFT VIDEO\" & \u201cRIGHT VIDEO\".\nFig. 9 shows the instructions and user interface from the study. We conduct 3 such studies using\nhive.ai [1], the results of which are in Fig 5 of the main paper.\nFiltering Unreliable Users. We require users to understand instructions given in English. User\nselection for the study is conditioned on the performance of a qualification test. Users with an\naccuracy of \u2265 80% on this test are allowed to take the study. To ensure continued reliability during\nthe labeling process we randomly mix the real task data with \u201cobvious\" honeypot data where the\nlabels are objective. We require users to have a performance of \u2265 89% on these honeypot tasks. A\ndrop in performance below this results in the user being disqualified from taking the study further.\nC.2\nMetrics\nApart from performing the user study described in \u00a7C.1 we also evaluate all our models and baselines\non several quantitative metrics. We detail these metrics below (apart from the details already described\nin Sec 4.2 of the main paper).\nPenetration Score. To assess the realism of human motion when interacting with an object, we\ncalculate the penetration score during the approach phase. We define the approach phase as the initial\nNA motion frames from a sequence of 150 frames (5 sec). Our rationale for selecting NA is that\nduring the approach phase, there should be minimal penetration of the human motion into the object\ngeometry. However, during the interaction, there should be increasing contact with the object. These\ncontacts typically result in zero or positive values in the signed distance function (SDF), indicating\npenetration of points on the object surface into the human SMPL mesh.\n17\nFigure 9: A/B Testing User Study We use this study to compare the quality of motions generated\nby different methods by requiring them to generate human-object interaction motions. On the left,\nwe show the instruction set following which all users are required to pass a qualification exam to\nparticipate in the study. On the right, we show the user interface as visible to users. The users\nanswer the question \"Which motion is more realistic\" and are required to choose one between \u201cLEFT\nVIDEO\" or \u201cRIGHT VIDEO\".\n0\n50\n100\n150\nNA\n0\n50\n100\n% Motions \nSit Penetration Score\nCond. VAE\nCond. MDM\nNIFTY (ours)\nFigure 10: Penetration Score Sitting. We graph the percentage of motion sequences with a\npenetration score of less than or equal to 2cm (Y-axis), compared to the number of approach frames,\ndenoted as NA (X-axis). Our findings reveal that regardless of the value of NA, NIFTY (green)\nconsistently exhibits a greater proportion of motion sequences with low penetration scores.\nWe compute NA for sitting and lifting separately based on our synthetic dataset. In particular, we\ndetermine the first frame index of motion where object penetration distance continues to only increase\nthereafter. We assume that after this point, the person is actually interacting with the object and\nnot just approaching it. For sitting, the typical onset of motion interaction occurs after the initial\n117 frames of approach, based on the median NA. Likewise, lifting has a 15th percentile NA of\n124 frames. We use the 15th percentile instead of the median (148 frames) to make this metric\nmore meaningful as 148 frames is almost the end of the complete motion and we wish to evaluate\nthe approach. This difference between sit and lift action is due to the difference in their inherent\ninteraction with the object.\nFor completeness, we also report this performance as a function of different NA values in Fig. 10\n(sit) and Fig. 11 (lift).\n18\n0\n50\n100\n150\nNA\n0\n50\n100\n% Motions\nLift Penetration Score\nCond. VAE\nCond. MDM\nNIFTY (ours)\nFigure 11: Penetration Score Lifting. We graph the percentage of motion sequences with a\npenetration score of less than or equal to 2cm (Y-axis), compared to the number of approach frames,\ndenoted as NA (X-axis). Our findings reveal that regardless of the value of NA, NIFTY (green)\nconsistently exhibits a greater proportion of motion sequences with low penetration scores.\nSkeleton Distance. This metric uses the anchor poses from our human-object interaction data to\nevaluate whether generated motions faithfully reflect interactions from data. We compute a sum over\nthe per-joint location error (22 joints in our case) between the final generated interaction pose and the\nnearest neighbor anchor pose from the training dataset in the joint locations space. We report the\naverage of this metric across generated motions.\nD\nSupplemental Results\nIn this section, we include supplemental analyses to support the evaluations in the main paper that\nwere not included due to space constraints. First, we evaluate the effect of having a parametric\nvs a non-parametric guidance field in \u00a7D.1. In \u00a7D.2, D.3, and D.4 we evaluate the impact of\nhyperparameters like the number of samples at inference, number of anchor poses at training, and\na variant of our Object Interaction Field that guides a motion sequence instead of just the final\ninteraction frame. We also evaluate the difference in performance across different objects.\nD.1\nNon-Parametric Object Interaction Field\nWe conducted a comparison between our method and a variant where we replaced the object interac-\ntion field with a non-parametric field implemented using the nearest neighbor measure. Specifically,\nduring the guidance phase, we identified the nearest anchor pose of the object from the training\nset and used the difference between this pose and the predicted final pose as the correction. This\ncorrection was then utilized to define our distance field and guide the diffusion model accordingly.\nTab. 3 presents the comparison between this baseline and our method. The skeleton distance metric\ncan be sensitive to outliers (e.g., a few generations that are far from the object), so we additionally\nreport % Skel. Dist. \u2264 25cm to get a more robust metric. The results demonstrate that our learning\napproach offers a significant improvement of at least 20% in terms of Skeleton Distance \u2264 25 cm, as\nwell as an additional 10% in terms of Contact IoU. The main paper reports results on the Parametric\napproach as our primary model.\nD.2\nEffect of Number of Samples\nIn the main paper, we generate 10 guided samples from the diffusion model and use the one with\nthe best guidance score. We investigate the impact of varying these number of samples in Tab. 4.\nWe observe that increasing the number of samples leads to improved performance. Particular\nimprovements occur when transitioning from 1 sample to 5 samples. Since guidance does not always\nresult in perfect samples, drawing a diverse set gives better chance for a high-quality output. Note\nthat drawing additional samples can be done efficiently in parallel.\n19\nTable 3: Nearest Neighbor Comparison. We investigate the effect of learning a parametric function\nfor the Interaction field compared to using the nearest neighbor approach (explained in \u00a7 D.1). Our\nresults demonstrate that guiding the diffusion model with our learned field outperforms using a\nnon-parametric field. Specifically, for the sitting action dataset, our Parametric method surpasses the\nNon-Parametric method by 0.09 points in Contact IoU and achieves an 18% improvement in Skel.\nDist \u2264 25cm. Similar trends are observed in the lift action dataset.\nSitting\nGuidance\nFoot\n% D2O\nD2O\nSkel.\n% Skel.\nContact\n% Pen.\nObjective\nSkating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 Dist. \u2193 Dist.\u2264 25cm \u2191\nIoU \u2191\n\u2264 2cm \u2191\nNon-Parametric\n0.44\n99.80\n0.00\n0.31\n47.01\n0.45\n64.67\nParametric\n0.47\n99.60\n0.00\n0.54\n65.94\n0.54\n65.40\nLifting\nGuidance\nFoot\n% D2O\nD2O\nSkel.\n% Skel.\nContact\n% Pen.\nObjective\nSkating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 Dist. \u2193 Dist.\u2264 25cm \u2191\nIoU \u2191\n\u2264 2cm \u2191\nNon-Parametric\n0.32\n71.12\n0.07\n0.52\n29.88\n0.11\n63.02\nParametric\n0.34\n77.69\n0.05\n0.42\n61.55\n0.17\n69.49\nTable 4: Number of Samples Analysis. We study the impact of drawing multiple samples and\nguiding them. Drawing more samples helps generate better-quality motions.\nSitting\n# Samples\nFoot\n% D2O\nD2O\nSkel.\n% Skel.\nContact\n% Pen.\nSkating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 Dist. \u2193 Dist.\u2264 25cm \u2191\nIoU \u2191\n\u2264 2cm \u2191\n1\n0.66\n86.25\n7.36\n5.72\n41.83\n0.40\n62.59\n2\n0.56\n94.62\n4.29\n2.36\n51.20\n0.47\n65.47\n5\n0.47\n98.81\n0.00\n0.67\n62.55\n0.51\n64.72\n10\n0.47\n99.60\n0.00\n0.54\n65.94\n0.54\n65.40\nLifting\n# Samples\nFoot\n% D2O\nD2O\nSkel.\n% Skel.\nContact\n% Pen.\nSkating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 Dist. \u2193 Dist.\u2264 25cm \u2191\nIoU \u2191\n\u2264 2cm \u2191\n1\n0.36\n73.11\n4.84\n2.21\n42.03\n0.14\n64.58\n2\n0.35\n75.70\n0.08\n1.17\n48.80\n0.14\n67.37\n5\n0.34\n77.29\n0.06\n0.59\n57.57\n0.17\n67.53\n10\n0.34\n77.69\n0.05\n0.42\n61.55\n0.17\n69.49\nD.3\nEffect of Number of Anchors Poses\nWe also train our Interaction Field (IF) using subsets of motion that yield a limited number of anchor\nposes. Specifically, we train the IF using 10%, 25%, and 50% of the available seed anchor poses and\nreport results in Tab. 5. It is worth noting that Contact IoU and Skeleton Dist metrics are calculated\nusing all anchor poses in the training set. However, methods trained with only X% of the anchor data\nwill not be able to generate the complete range of seed poses. Therefore, when comparing methods\ntrained with different percentages of seed anchor poses, we primarily assess them based on other\nmetrics, but Contact IoU and Skeleton Dist are still included for completeness.\nNIFTY\u2019s performance remains stable even with the limited availability of anchor poses. Looking at\nFoot Skating, D2O, and Penetration metrics, there is not a significant decline in performance. The\nmain paper reports results on 100% data for NIFTY.\nD.4\nEffect of Number of Input Frames on Interaction Field\nIn the main paper, our interaction field only considers the last interaction pose, denoted as \u02dcX. However,\nwe want to investigate the impact of extending the interaction field to operate on a sequence of frames\nrather than just the final interaction frame. To achieve this, we modify our Object Interaction Field to\n20\nTable 5: Number of Anchors at Training. We vary the number anchor poses available for training\nthe Interaction Field. We see metrics like Foot Skating, D20, and Pen. are relatively stable as\ncompared to a number of anchors. The evaluation using Skel.Distance and Contact IoU uses all the\nanchor poses in the training dataset and this evaluation hence hurts the methods that have access to\nthe less anchor poses during training. For this particular ablation we consider Foot Skating, D2O,\nand Pen. are primary metrics for this ablation.\nSitting\n% Anchors\nFoot\n% D2O\nD2O\n% Pen.\nSkel.\n% Skel.\nContact\nSkating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 \u2264 2cm \u2191\nDist. \u2193 Dist.\u2264 25cm \u2191\nIoU \u2191\n10%\n0.55\n95.82\n0.00\n53.02\n1.90\n12.35\n0.27\n25 %\n0.54\n98.01\n0.00\n53.86\n1.28\n28.88\n0.34\n50 %\n0.49\n98.21\n0.00\n59.23\n0.96\n34.86\n0.40\n100%\n0.47\n99.60\n0.00\n65.40\n0.54\n65.94\n0.54\nLifting\n% Anchors\nFoot\n% D2O\nD2O\n% Pen.\nSkel.\n% Skel.\nContact\nSkating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 \u2264 2cm \u2191\nDist. \u2193 Dist.\u2264 25cm \u2191\nIoU \u2191\n10%\n0.37\n83.27\n0.07\n50.72\n0.98\n14.54\n0.06\n25%\n0.37\n84.86\n0.05\n46.24\n1.32\n22.11\n0.07\n50%\n0.36\n78.49\n0.06\n56.34\n1.01\n24.90\n0.08\n100%\n0.34\n77.69\n0.05\n69.49\n0.42\n61.55\n0.17\nprocess a sequence of frames from N\u2212m to N, represented as { \u02dcXN\u2212m . . . \u02dcXN}. Using a transformer\nencoder, we encode this sequence and obtain a correction vector, denoted as \u2206{\n\u02dc\nXN\u2212m . . . \u02dc\nXN}. In\nTab. 6, we present preliminary results using this spatiotemporal configuration. The results indicate\nthat training such an interaction field is feasible but requires a more careful tuning of different\nhyperparameters, e.g., the guidance weights. Further investigation into this matter is left for future\nresearch.\nTable 6: Multiple Input Frames to Interaction Field We show preliminary results on training an\ninteraction field that considers multiple frames as input instead of a single frame like in the main\npaper. Our results indicate training such a field is feasible the requires further analysis to understand\nthe effect of different hyperparameters.\nSitting\n# Input\nFoot\n% D2O\nD2O\nSkel.\n% Skel.\nContact\n% Pen.\nFrames Skating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 Dist. \u2193 Dist.\u2264 25cm \u2191\nIoU \u2191\n\u2264 2cm \u2191\n1\n0.47\n99.60\n0.00\n0.54\n65.94\n0.54\n65.40\n5\n0.66\n86.25\n7.36\n5.72\n41.83\n0.40\n62.59\n10\n0.56\n94.62\n4.29\n2.36\n51.20\n0.47\n65.47\n15\n0.47\n98.81\n0.00\n0.67\n62.55\n0.51\n64.72\nLifting\n# Input\nFoot\n% D2O\nD2O\nSkel.\n% Skel.\nContact\n% Pen.\nFrames Skating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 Dist. \u2193 Dist.\u2264 25cm \u2191\nIoU \u2191\n\u2264 2cm \u2191\n1\n0.34\n77.69\n0.05\n0.42\n61.55\n0.17\n69.49\n5\n0.35\n76.10\n0.06\n0.37\n62.55\n0.16\n67.28\n10\n0.34\n78.09\n0.05\n0.46\n62.55\n0.17\n69.64\n15\n0.34\n77.49\n0.06\n0.36\n62.95\n0.16\n68.64\nD.5\nEffect of training Interaction Field in the Local Human Frame\nOur interaction field is object-centric since it takes in a canonical object point cloud as input. To\ntest this design choice, we implement the object interaction field in the local frame of the human\n21\nrequiring it to understand the spatial positioning of the object w.r.t to the human motion. As shown in\nTab. 7, this leads to a subpar performance across the board on sit and lift actions.\nTable 7: Canonical vs. Local Human Frame for Interaction Field Training. We show that training\nan Interaction Field in the local human motion frame leads to poor performance as comared to\nSitting\nInteraction\nFoot\n% D2O\nD2O\nSkel.\n% Skel.\nContact\n% Pen.\nField Frame\nSkating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 Dist. \u2193 Dist.\u2264 25cm \u2191\nIoU \u2191\n\u2264 2cm \u2191\nLocal Human\n0.36\n40.04\n0.86\n2.62\n0.20\n0.04\n53.73\nCanonical\n0.47\n99.60\n0.00\n0.54\n65.94\n0.54\n65.40\nLifting\nInteraction\nFoot\n% D2O\nD2O\nSkel.\n% Skel.\nContact\n% Pen.\nField Frame\nSkating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 Dist. \u2193 Dist.\u2264 25cm \u2191\nIoU \u2191\n\u2264 2cm \u2191\nLocal Human\n0.28\n41.83\n1.02\n2.44\n0.60\n0.02\n43.33\nCanonical\n0.34\n77.69\n0.05\n0.42\n61.55\n0.17\n69.49\nD.6\nPerformance Breakdown Per-Object\nWe analyze if the performance of our method is biased towards certain objects by computing the\nmetrics for about 100 interaction motion samples per object instance. We show the results of this\nin Tab. 8. Results indicate that the performance of our method is not dependent on the kind of the\nobject. For instance, in the case of sitting, the performance for sitting on a \u201cArmchair\" vs \u201cChair\" are\nclose. This demonstrates the flexibility of the NIFTY pipeline to a diverse set of objects.\nTable 8: Performance on actions across objects. We see that NIFTY\u2019s performance is stable\nacross object categories and the framework handles different objects effectively. For instance, the\nperformance on the Armchair and Chair on sitting action are close signaling the flexibility of NIFTY\npipeline.\nSitting\nObject\nFoot\n% D2O\nD2O\nSkel.\n% Skel.\nContact\n% Pen.\nSkating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 Dist. \u2193 Dist.\u2264 25cm \u2191\nIoU \u2191\n\u2264 2cm \u2191\nArmchair\n0.42\n99.05\n0.00\n0.42\n90.48\n0.44\n56.73\nChair\n0.51\n100.00\n0.00\n0.17\n84.31\n0.60\n49.02\nStool\n0.50\n96.59\n0.01\n0.21\n68.18\n0.54\n72.94\nTable\n0.46\n100.00\n0.00\n0.28\n55.88\n0.50\n68.63\nYoga Ball\n0.53\n100.00\n0.00\n0.22\n73.33\n0.58\n52.38\nLifting\nObject\nFoot\n% D2O\nD2O\nSkel.\n% Skel.\nContact\n% Pen.\nSkating \u2193 \u2264 2cm \u2191 95th \u02dc% \u2193 Dist. \u2193 Dist.\u2264 25cm \u2191\nIoU \u2191\n\u2264 2cm \u2191\nChair\n0.34\n86.82\n0.04\n0.38\n70.54\n0.17\n59.82\nStool\n0.36\n77.24\n0.06\n0.24\n65.04\n0.13\n76.84\nSuitcase\n0.33\n63.85\n0.06\n0.20\n71.54\n0.28\n65.06\nTable\n0.29\n90.00\n0.03\n0.64\n51.67\n0.15\n57.41\nE\nQualitative Results\nMotion generation results are best seen as videos on the attached webpage. We also include static\nvisualizations here in Fig. 12 and Fig. 13. The webpage additionally also shows visualizations ( 10\nmotions) from our method for every object in our dataset.\n22\nCond. VAE\nCond. MDM\nNIFTY\nCond. VAE\nCond. MDM\nNIFTY\nFigure 12: Comparison Qualitative Motions Sitting. Compared to other baselines, our method\n(NIFTY) produces more realistic motions. When examining the motion examples generated by the\nbaselines, we notice that in all cases where a person approaches an object to sit, either the person\ncompletely misses the object or the sitting pose is not compatible with the object. To better evaluate\nthese results, please refer to the qualitative videos of these motions in the supplementary.html.\nCond. VAE\nCond. MDM\nNIFTY\nCond. VAE\nCond. MDM\nNIFTY\nFigure 13: Comparison Qualitative Motions Lifting. NIFTY generates more realistic motions as\ncompared to the baseline methods. With motions generated using the baseline methods, we see that\nthe lifting stance is often taken far from the object. To better evaluate these results, please refer to the\nqualitative videos of these motions in the supplementary.html file.\nF\nLimitations\nOur proposed pipeline demonstrates the ability to achieve human-object interaction results with a\ndiverse sets of objects while only relying on a limited number of anchor poses. One of the key factors\ncontributing to the performance of NIFTY is the utilization of a pretrained motion model [33] trained\non the AMASS repository [25]. Our data generation pipeline has the capability to generate motions\nand interpolate between existing data in this dataset. However, in cases where a completely novel\nand extreme seed anchor pose is provided, such as a headstand, HuMoR would struggle to generate\n23\nreasonable and high-quality motion sequences. Developing more robust motion models which can\nhandle such poses, would be beneficial.\nFurthermore, during the inference stage, it is necessary to draw multiple samples from the diffusion\nmodel and guide them. This approach yields significantly better performance compared to guiding\nonly a single sample. Exploring research directions that can enhance the stability of the guidance\nprocess would be valuable in consistently generating high-quality interaction motions.\n24\n"
  }
]