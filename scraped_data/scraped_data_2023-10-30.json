[
  {
    "title": "CodeFusion: A Pre-trained Diffusion Model for Code Generation",
    "link": "https://arxiv.org/pdf/2310.17680.pdf",
    "upvote": "68",
    "text": ""
  },
  {
    "title": "FP8-LM: Training FP8 Large Language Models",
    "link": "https://arxiv.org/pdf/2310.18313.pdf",
    "upvote": "29",
    "text": "FP8-LM: Training FP8 Large Language Models\nHouwen Peng \u2217\nKan Wu \u2217\nYixuan Wei \u2217\nGuoshuai Zhao\nYuxiang Yang\nZe Liu\nYifan Xiong\nZiyue Yang\nBolin Ni\nJingcheng Hu\nRuihang Li\nMiaosen Zhang\nChen Li\nJia Ning\nRuizhe Wang\nZheng Zhang\nShuguang Liu\nJoe Chau\nHan Hu \u2020\nPeng Cheng \u2020\nMicrosoft Azure and Microsoft Research\nAbstract\nIn this paper, we explore FP8 low-bit data formats for efficient training of large language\nmodels (LLMs). Our key insight is that most variables, such as gradients and optimizer\nstates, in LLM training can employ low-precision data formats without compromising model\naccuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8\nautomatic mixed-precision framework for training LLMs. This framework offers three levels\nof FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs.\nIt gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an\nincremental manner. Experiment results show that, during the training of GPT-175B model\non H100 GPU platform, our FP8 mixed-precision training framework not only achieved a\nremarkable 39% reduction in real memory usage but also ran 75% faster than the widely\nadopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer\nEngine by 37%. This largely reduces the training costs for large foundation models. Fur-\nthermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly\napplied to other tasks such as LLM instruction tuning and reinforcement learning with\nhuman feedback, offering savings in fine-tuning expenses. Our FP8 low-precision training\nframework is open-sourced at aka.ms/MS.AMP.\n1\nIntroduction\nLarge language models (LLMs) (Brown et al., 2020; Smith et al., 2022; Chowdhery et al., 2022; Zhang et al.,\n2022) have demonstrated unprecedented capabilities in language comprehension and generation, leading to\nbreakthroughs in reasoning, math, science, and many other tasks (OpenAI, 2023; Anil et al., 2023). However,\ntraining LLMs is extremely costly. For example, PaLM takes 6,144 TPUv4 chips to train a 540B model, while\nGPT-3 175B consumes several thousand petaflop/s-days of compute for pre-training (Chowdhery et al., 2022;\nBrown et al., 2020). This motivates the needs of reducing the training costs of LLMs, especially for the scaling\nof next-generation super-intelligent models.\nLow-precision training is one of the most promising directions to reduce the costs, as it can provide high\nspeed, small memory footprint, and low communication overhead. Most existing training systems, e.g.,\nMegatron-LM (Shoeybi et al., 2019), MetaSeq (Zhang et al., 2022), and Colossal-AI (Li et al., 2023a), train\nLLMs with either FP32 full-precision or FP16/BF16 mixed-precision by default. This is not essential, however,\nContributions for all the authors can be found in Section 5.\n* equal work\n\u2020 contact: {hanhu | pengc}@microsoft.com\narXiv:2310.18313v2  [cs.LG]  19 Dec 2023\n0\n200\n400\n600\n800\n1000\nModel Size (B)\n8\n16\n32\n64\n128\nNumber of GPUs\nBF16\nOur FP8\nFigure 1: An analysis of comparing the maximum model sizes attainable through the utilization of either the\nprevalent BF16 or our FP8 mixed-precision training approach on a cluster of Nvidia H100 GPUs with 80GB\nmemory.\nto achieve full accuracy for large models. With the release of Nvidia H100 GPU, FP8 is becoming the next-\ngeneration datatype for low-precision representation (Nvidia, 2022a; Micikevicius et al., 2022). Theoretically,\nFP8 can achieve 2\u00d7 speed-up, 50% - 75% memory cost savings, and 50% - 75% communication savings\ncompared with current 16-bit and 32-bit floating point mixed-precision training, which is very promising for\nscaling-up next-generation foundation models.\nUnfortunately, the current support for FP8 training is rare and limited. The only usable framework is the\nNvidia Transformer Engine (TE) (Nvidia, 2022b), but it applies FP8 solely for GEMM computation and still\nretains master weights and gradients using high precision, e.g., FP16 or FP32. As a result, the end-to-end\nspeed-up, memory and communication cost savings are very limited, which does not fully unveil the power\nof FP8. To address this issue, we propose an extremely optimized FP8 mixed-precision framework for LLM\ntraining. The core idea is to infiltrate FP8 compute, storage, and communication into the whole progress of\nlarge model training, making the forward and backward pass all used the low-precision FP8, thus largely\nreducing system workloads compared to previous frameworks (Micikevicius et al., 2017; Nvidia, 2022b;\nMicikevicius et al., 2022). Specifically, we design three optimization levels that utilize FP8 to streamline mixed-\nprecision and distributed training. The three levels gradually incorporate 8-bit collective communication,\noptimizer, and distributed parallel training in an incremental manner. The higher optimization level indicates\nusing more FP8 during LLM training. Moreover, for large-scale training, such as GPT-175B trained on\nthousand of GPUs, our framework provides FP8 low-bit parallelism, including tensor, pipeline, and sequence\nparallelism, paving the way to next-generation low-precision parallel training.\nTraining LLMs with FP8 is non-trivial. The challenges stem from issues such as data underflow or overflow,\ncoupled with quantization errors arising from the narrower dynamic range and reduced precision inherent\nin FP8 data formats. These challenges cause numerical instabilities and irreversible divergences throughout\nthe training process. To tackle them, we propose two techniques: precision decoupling and automatic scaling for\npreventing the loss of critical information. The former one involves decoupling the influence of data precision\non parameters such as weights, gradients, optimizer states, and assigning reduced precision to components\nthat are not precision sensitive. The latter one is to preserve gradient values within the representation range\nof FP8 data formats through the dynamic adjustment of tensor scaling factors, thereby alleviating underflow\nand overflow occurrences during all-reduce communication.\nTo validate the proposed FP8 low-precision framework, we apply it to GPT-style model training, encompassing\nboth pre-training and supervised fine-tuning (SFT). The experimental results demonstrate the effectiveness\nof our FP8 methodology, yielding substantial benefits including a 29% to 39% reduction in real memory\nusage (e.g., 29% reduction for GPT-7B while 39% for GPT-175B ) and a notable 63% to 65% decrease in\nweight-related communication overhead compared to the prevalent BF16 mixed-precision training approach.\nWithout changes to any hyper-parameters, such as learning rate and weight decay, the models trained using\nFP8 exhibit performance equivalency to those employing BF16 high precision, both in pre-training and\n2\ndownstream tasks. It is noteworthy that during the training of GPT-175B model, our FP8 mix-precision\nframework reduces training time by 37% compared to TE (Nvidia, 2022b), while consuming 42% less memory\non H100 GPU platform. More importantly, the reduction in costs achieved through the utilization of low-\nprecision FP8 can be further increased, as the scale of models continues to expand, which is presented in\nFig. 1.\nFor fine-tuning, we employ FP8 mixed-precision for instruction tuning and reinforcement learning with\nhuman feedback (RLHF) to better align pre-trained LLMs with end tasks and user preferences. Specifically,\nwe fine-tune pre-trained models on publicly user-shared instruction-following data (ShareGPT, 2023). The\nmodels tuned with our FP8 mixed-precision demonstrate comparable performance to those utilizing the\nhalf-precision BF16 (Zheng et al., 2023) on the AlpacaEval (Li et al., 2023b) and MT-Bench (Zheng et al.,\n2023) benchmarks, while achieving 27% improvements in training speed. Moreover, FP8 mixed-precision\nexhibits considerable potentials in RLHF, a process that necessitates loading multiple models during training.\nThrough the utilization of FP8 in training, the prevalent RLHF framework AlpacaFarm (Dubois et al., 2023)\ncan yield a 32% reduction in model weights and a 62% reduction in optimizer states\u2019 memory consumption.\nThis further demonstrates the versatility and adaptability of our FP8 low-precision training framework.\nWe are making the following contributions to drive the design of next-generation FP8 low-precision training\nfor LLMs.\n\u2022 A new FP8 mixed-precision training framework. It unlocks 8-bit weights, gradients, optimizer, and\ndistributed training gradually in an add-on fashion, which is convenient in use. This 8-bit framework\ncan be used as a simple drop-in replacement for existing 16/32-bit mixed-precision counterparts,\nwithout requiring any changes to the hyper-parameters and training receipts. Additionally, we\nprovide a Pytorch implementation that enables 8-bit low-precision training in a few lines of code.\n\u2022 A new family of GPT-style models trained with FP8. We apply the proposed FP8 scheme to GPT pre-\ntraining and fine-tuning (i.e., SFT and RLHF), and demonstrate its potentials on a variety of model\nscales ranging from 7B to 175B parameters. We equip prevalent parallel computation paradigms\nwith FP8 supports, including tensor, pipeline, and sequence parallelisms, enabling the utilization of\nFP8 to train large foundation models. We open-source the first FP8 GPT training codebase based\nupon Megatron-LM (Shoeybi et al., 2019) implementation.\nWe expect the release of our FP8 framework will establish a new paradigm for next-generation low-precision\ntraining system dedicated to large foundation models.\n2\nFP8 LLMs\nMixed-precision (Micikevicius et al., 2017) has been widely used in LLM training to improve compute and\nmemory efficiency. The most popular mixed-precision schemes are FP16-FP32 and BF16-FP32. Because of\nthe restricted numerical range of FP16, FP16-FP32 scheme has been known instabilities for training large\nmodels (Rae et al., 2021; Zeng et al., 2022). Consequently, the community now commonly adopts BF16-FP32\nfor training LLMs, such as Megatron-Turing NLG-530B (Smith et al., 2022), Bloom-175B (Scao et al., 2022)\nand Gopher (Rae et al., 2021). The underlying reason is that BF16 has a wide dynamic range to maintain\nnumerical stability while matching the performance of the full-precision FP32. Moreover, BF16 employs half\nthe number of bits as compared to FP32, thus reducing considerable memory footprints while improving\ncompute efficiency.\nFP8 is a natural evolution from 16-bit data formats to further reducing computing costs. However, training\nLLMs with reduced-precision FP8 poses new challenges. The dynamic range and representation precision\nof FP81 are much lower than BF16 and FP16, which inevitably induces more training collapses, such as\nloss spikes or even NaNs. To address the issues, tensor scaling techniques are proposed (Sun et al., 2019;\nMicikevicius et al., 2022). The core idea is multiplying higher precision values with a scaling factor prior to\ntheir casting to FP8 in order to move them into a range that better overlaps with the representable range of\n1The details of FP8 data formats are presented in Appendix A.1.\n3\na corresponding FP8 format2 (Micikevicius et al., 2022). Such a per-tensor scaling technique reduces data\nquantization errors while improving numerical stability and accuracy, thus enabling the utilization of the\nlower-precision FP8 for training large models.\nUnfortunately, the current support for FP8 low-precision training is restricted. Nvidia TE (Nvidia, 2022b) only\nsupports FP8 compute for linear layers in Transformer (Vaswani et al., 2017), while leaving all other operations,\nsuch as weight update and gradient synchronization, still using higher precision. In this work, we present an\nextremely optimized FP8 mixed-precision strategy for LLM training. The new FP8 optimization includes\nthree key perspectives: FP8 communication, FP8 optimizer, and FP8 distributed training. By integrating\nthese aspects, the training of LLMs such as the 175B GPT-3 model can fully harness the advantages of FP8\nlow-precision and improve training efficiency.\n2.1\nFP8 Gradient and All-Reduce Communication\nExisting mixed-precision training methodologies (Micikevicius et al., 2017; Nvidia, 2022b) typically employ 16-\nbit or 32-bit datatype for the computation and storage of gradients, resulting in a high bandwidth requirement\nfor collective communication throughout the training process. We found that directly applying FP8 to\ngradients leads to a decrease in accuracy. The fundamental issue lies in the underflow and overflow problems\narising from the low-bit all-reduce operation. Specifically, there are two standard methods aggregating\ngradients across GPUs during all-reduce: pre-scaling and post-scaling. Pre-scaling divides the gradient gi\ncalculated on the i-th GPU by the total number of GPUs (i.e., N) before being summed, which is formulated\nas:\ng = g1/N + g2/N + \u00b7 \u00b7 \u00b7 + gN/N.\n(1)\nWhen N is large, this division can cause data underflow, especially for FP8 low-precision representation of\ngradients. To mitigate this issue, post-scaling performs the gradient summation first, followed by the division\nscaling during the gradient collection process:\ng = (g1 + g2 + \u00b7 \u00b7 \u00b7 + gN)/N.\n(2)\nThis post-scaling approach keeps the gradients close to the maximum value of the FP8 datatype, effectively\nalleviating the underflow issue. However, this approach encounters overflow issues when aggregating\ngradients.\nIn contrast, we propose an automatic scaling technique to resolve both the underflow and overflow issues\nin the pre-scaling and post-scaling approaches. To be specific, we introduce an auto-scaling factor \u00b5, that\nchanges on the fly during the training, to reduce the occurrences of overflow and underflow in gradients:\ng\u2032\ni = \u00b5 \u00b7 gi.\n(3)\nA statistical analysis is conducted on the gradient values of g\u2032\ni, with the objective of quantifying the proportion\nof values that attains the maximum feasible value within the FP8 representation range. If the ratio of the\nmaximum value exceeds a specified threshold, i.e., 0.001%, \u00b5 is set to 1/2 in the subsequent training step,\nthereby mitigating the risk of overflow. Conversely, when the ratio consistently remains the threshold, we\nopt to exponentially increase \u00b5 to 2 over the span of 1,000 training steps, thereby effectively mitigating the\nrisk of underflow occurrences.\nAnother key obstacle of FP8 collective communication lies in devising an effective strategy to manage the\ntensor-wise scaling factors that are associated with each gradient tensor. The current NCCL implementation\n(Nvidia, 2020) lacks the capability of performing all-reduce operation considering the additional tensor-wise\nscaling factors. Meanwhile, efficient implementation is also very challenging, especially considering that\nthe NCCL gradient summation operates at sub-tensor level. This complexity increases significantly when\nincorporating updates for tensor-wise scaling factors. To overcome this issue, we propose a new mechanism\nthat scales FP8 gradients across GPUs using a single shared scalar. To be specific, let (g\u2032\ni, s\u2032\ni) denote a scaling\ntensor which stores the weight gradient in the i-th GPU, where g\u2032\ni is a FP8 tensor and s\u2032\ni is the corresponding\nscaling factor. The actual weight gradient is g\u2032\ni/s\u2032\ni. Prior to the all-reduce operation over gradient tensors,\n2The details of FP8 tensor scaling are introduced in Appendix A.2.\n4\nwe first gather the scaling factors s\u2032\ni of each gradient tensor on all GPUs and calculate the global minimum\nscaling factor s\u2032\ng as:\ns\u2032\ng = min (s\u2032\n1, s\u2032\n2, . . . , s\u2032\nN) ,\n(4)\nwhere the global minimum scaling factor s\u2032\ng is shared across GPUs. We use this shared scaling factor s\u2032\ng to\nunify the rescaling of the gradient tensors across GPUs. In this way, all gradient tensors associated with the\nsame weight use the same shared scaling factor to quantize the tensors into FP8 format on all GPUs:\ng\u2032\u2032\ni = FP8\n\u0000s\u2032\ng \u00b7 (g\u2032\ni/s\u2032\ni)\n\u0001\n.\n(5)\nThis approach reduces communication overhead by transmitting only a single scalar s\u2032\ng, making the additional\nsynchronization step highly efficient. As the input tensors share the same scaling factor, it eliminates the need\nof considering all-reduce the scaling factors in parallel and allows for standard NCCL all-reduce operation to\nbe performed. The final collected gradient is obtained as follows:\ng = g\u2032\u2032\n1 + g\u2032\u2032\n2 + \u00b7 \u00b7 \u00b7 + g\u2032\u2032\nN,\ns = N \u00b7 s\u2032\ng,\n(6)\nwhere g is the final aggregated gradient and s is the corresponding scaling factor. Rescaling the scaling factor\nfor the summed gradient g is equivalent to dividing g by N in theory. By implementing the aforementioned\ndual strategies of distributed and automated scaling, we can successfully realize FP8 low-bit gradient\ncommunication while preserving model accuracy. Furthermore, this approach entails storing gradients in\nFP8 and conducting communication in FP8 as well, thereby yielding reductions in GPU memory usage and\ncommunication bandwidth consumption.\n2.2\nFP8 Optimizer\nIn the training of LLMs, Adam and its variants (Kingma and Ba, 2015; Loshchilov and Hutter, 2018) are the\nmost frequently-used optimization methods, that maintain copies of model weights, gradients, first-order\nand second-order gradient moments for model updates. Mixed-precision training (Micikevicius et al., 2017)\nwith Adam optimizer typically stores master weights, gradients and gradient moments in 32-bit float format\nfor numerical stability (Shoeybi et al., 2019; Rajbhandari et al., 2020; Zhang et al., 2022; Scao et al., 2022).\nConsequently, the Adam optimizer consumes 16 bytes of memory per parameter during training:\n4\n|{z}\nmaster weights\n+\n4\n|{z}\ngradients\n+\n|4 + 4\n{z\n}\nAdam states\n= 16 bytes.\n(7)\nWhen model size is large, the memory consumption of the variables in Adam will become a bottleneck.\nPrevious work (Rae et al., 2021; Zeng et al., 2022; Liu et al., 2022) has revealed that reducing precision of\nthe variables in optimizer to 16-bit leads to accuracy degradation when training billion-scale models3. This\nprompts an evaluation of which variables in the optimizer should be allocated high precision and which can\nbe accommodated with low-precision.\nTo clarify, we decouple the influence of data precision on the variables in the optimizer and investigate\nwhich one can be assigned lower precision, i.e., precision decoupling. We find a guiding principle: the gradient\nstatistics can use lower precision, while the master weights necessitate high precision. More concretely, the\nfirst-order gradient moment can tolerate a high quantization error and can be assigned with low-precision\nFP8, while the second-order moment requires a higher precision, as analyzed in Sec. 3.3. This stems from the\nfact that, during model updates in Adam, the direction of the gradient holds greater significance than its\nmagnitude. FP8 with tensor scaling can effectively preserve the distribution of the first-order moment as the\nhigh-precision tensor, though it introduces precision degradation to some extend. Calculating the square of\ngradients for the second-order gradient moment might lead to data underflow due to the typically small\ngradient values. Therefore, allocating a 16-bit higher precision is necessary to preserve numerical accuracy.\nOn the other hand, we find that it is crucial to keep the master weights using high precision. The underlying\nreason is that weight updates can sometimes become extremely small or large during training, higher\n3BF16 lacks the precision needed for accuracy, while FP16 has a restricted dynamic range. Given these challenges,\nprevalent mixed-precision training methodologies rely on utilizing FP32 full-precision for master weights, gradients, and\ngradient moments.\n5\nLayerNorm\nLayerNorm\nSequence Parallel\nTensor Parallel\nSequence Parallel\nLayerNorm\nLayerNorm\nFP8(   )\nFP8(   )\nFP8(  )\nFP8(  )\nGelu\nGelu\nFP8(   )\nFP8(   )\nDropout\nDropout\n and   : FP8 Weight      and   : Activation\nFigure 2: Transformer layer with FP8 tensor and sequence parallelism. The FP8 low-bit operation is high-\nlighted with orange. g is all-gather in forward pass and reduce-scatter in backward pass, while \u00afg is reduce-\nscatter in forward pass and all-gather in backward pass. The gather-reduce operation g between sequence\nparallel and tensor parallel is executed utilizing FP8 low-precision activation, thus saving half communication\ncosts.\nprecision for the master weights helps prevent loss of information when updating weights, ensuring more\nstable and accurate training. In the implementation, the master weights have two viable options: utilizing\neither FP32 full-precision or FP16 with tensor scaling. FP16 with tensor scaling offers the advantage of\nconserving memory without compromising accuracy. Consequently, our default choice is to employ FP16\nwith tensor scaling for storing master weights in the optimizer. Our FP8 mixed-precision optimizer consumes\n6 bytes of memory per parameter during training:\n2\n|{z}\nmaster weights\n+\n1\n|{z}\ngradients\n+\n|1 + 2\n{z\n}\nAdam states\n= 6 bytes.\n(8)\nThis new low-bit optimizer reduces memory footprints by 2.6x in comparison to the previous solution, as\nexemplified in Eq. (7). Noteworthily, this is the first FP8 optimizer for LLM training. The experiments in\nSec. 3.2 show that FP8 optimizer can preserve model accuracy at various scales, ranging from 125M to 175B\nparameters.\n2.3\nFP8 Distributed Parallel Training\nTraining LLMs like GPT-3 requires distributed learning strategies for parallelizing across GPUs. The\nfrequently-used strategies include data parallelism, tensor parallelism, pipeline parallelism, and sequence\nparallelism. Each parallelism has its own merits and has been used in a complementary fashion in existing\nsystems (Smith et al., 2022; Shoeybi et al., 2019; Zhang et al., 2022; Scao et al., 2022; Li et al., 2023a). For\nFP8 supports of these strategies, neither data parallelism nor pipeline parallelism necessitates any specific\nmodifications, because they do not involve additional FP8 compute and communication when splitting data\nbatches or model layers into segments across devices.\nTensor parallelism partitions individual layers of a model across multiple devices, such that the shards of\nweight, gradient and activation tensors are placed on separate GPUs, instead of a single one. To equip tensor\nparallelism with FP8, we convert the sharded weight and activation tensors to FP8 format for linear layer\ncomputation, enabling the forward compute and backward gradient collective communication all using FP8.\nOn the other hand, sequence parallelism splits input sequences into multiple chunks and the sub-sequences\nare fed to different devices to save activation memory. As shown in Fig. 2, sequence and tensor parallelism\nare performed in parallel to different parts of a Transformer model to make the best use of the available\nmemory and improve training efficiency. There is a converter g between sequence and tensor parallel regions\nto all-gather sequence partitions in the forward pass (or reduce-scatter tensor segments in the backward\npass). We add an FP8 datatype conversion prior to g, such that the all-gather (or reduce-scatter) operation\nuses FP8 low-bit activation to save communication cost across GPUs.\n6\nweight 1\nweight 2\nweight 3\nGPU 1\nGPU 2\nweight 1\nweight 2\nweight 3\nGPU 1\nGPU 2\nFigure 3: ZeRO tensor partitioning with and without scaling factors. Left: the original high-precision ZeRO\nmethod, which splits a single tensor into multiple partitions and distributes them to different devices. Right:\nthe proposed FP8 ZeRO, which distributes each tensor in its entirety across devices while taking tensor\nscaling into account.\nIn addition, Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020) is another frequently-used\ndistributed learning technique in large model training. The core idea of ZeRO is to shade model states over\ndevices, such that each device only hold a fraction of data (e.g., master weights, gradients, and optimizer\nstates) required for a training step. To reduce memory consumption, ZeRO method generally splits a single\ntensor into multiple partitions and distributes them to different devices. Directly applying FP8 to ZeRO is\ninfeasible, because it is difficult to handle the scaling factors associated with the FP8 partitions. The per-tensor\nscaling factors should be distributed along with FP8 partitions. To address this issue, we implement a new\nFP8 distribution scheme that distributes each tensor as a whole across devices, rather than partitioning it\ninto multiple sub-tensors as in ZeRO. The distribution of FP8 tensors is processed in a greedy manner, as\noutlined in Alg. 1. Specifically, our method first sorts the tensors of model states according to their sizes,\nand then distributes the tensors to different GPUs based upon the remaining memory size of each GPU.\nThe distribution follows the principle that the GPUs with larger remaining memory get a higher priority\nin receiving new distributed tensors. In this way, the tensor scaling factors can be distributed along with\nthe tensors smoothly, while reducing communication and compute complexity. Figure 3 presents a visual\nillustration of the difference in ZeRO tensor partitioning between scenarios with and without scaling factors.\n3\nExperiment\nIn this section, we assess the effectiveness of the proposed FP8 mixed-precision training approach on GPT-style\nLLMs, including a wide range of model scales, from 125 million to 175 billion parameters. For performance\nablation, we compare GPT models trained with FP8 against those trained with half-precision BF16 and\nAlgorithm 1 Greedy Distribution Algorithm for ZeRO\nInput: FP8 tensors with their corresponding scaling factors: T = {(s1, t1), (s2, t2), . . . , (sn, tn)}, where s denotes scaling\nfactors while t represents 8-bit tensors. The size of each tensor: C = {c1, c2, . . . , cn}.\nOutput: Partitions representing scaling tensors assigned to each GPU.\n1: Sort T in descending order of their sizes to get T \u2032 = {(s\u2032\n1, t\u2032\n1), (s\u2032\n2, t\u2032\n2), . . . , (s\u2032\nn, t\u2032\nn)} and C\u2032 = {c\u2032\n1, c\u2032\n2, . . . , c\u2032\nn}, where\nc\u2032\n1 \u2a7e c\u2032\n2 \u2a7e \u00b7 \u00b7 \u00b7 \u2a7e c\u2032\nn.\n2: Initialize memory usage uj = 0 and partition pj = \u2205 for each GPU Gj.\n3: for i = 1 to n do\n4:\nj \u2190 arg minj uj\n\u25b7 Find the GPU j \u2208 [1, m] with the least memory usage.\n5:\npj \u2190 pj \u222a {(s\u2032\ni, t\u2032\ni)}\n\u25b7 Assign (s\u2032\ni, t\u2032\ni) to Gj.\n6:\nuj \u2190 uj + c\u2032\ni\n\u25b7 Update the memory usage of Gj.\n7: end for\n8: return Partitions P = {p1, p2, . . . , pm}\n7\nparams\ndimension\nn heads\nn layers\nTP\nPP\nSP\nlearning rate\nbatch size\nn tokens\n125M\n768\n12\n12\n1\n1\n\u2713\n6.0e\u22124\n1M\n100B\n7B\n4096\n32\n32\n1\n1\n\u2713\n3.0e\u22124\n4M\n100B\n13B\n5120\n40\n40\n2\n1\n\u2713\n3.0e\u22124\n4M\n100B\n175B\n12288\n96\n96\n8\n4\n\u2713\n3.0e\u22125\n1M\n40B\nTable 1: Model sizes, architectures, and training hyper-parameters. TP, PP, and SP indicate tensor, pipeline,\nand sequence parallelism, respectively. To mitigate carbon emissions and save cost, we restrict the training of\nthe 175B model to a dataset comprising only 40B tokens, which has proven to be sufficient for evaluating\nsystem performance.\nfull-precision FP32. For generality evaluation, we conduct experiments encompassing both FP8 low-bit\npre-training and fine-tuning, considering instruction tuning and human preference alignment.\n3.1\nExperimental Setup\n3.1.1\nTraining Dataset\nOur pre-training data is constructed using open-sourced language collections from several sources, including\nCommonCrawl4, The Pile (Gao et al., 2020), C4 (Raffel et al., 2020), OpenWebText (Radford et al., 2019;\nGokaslan and Cohen, 2019), CC-NEWS (Liu et al., 2019), CC-Stories (Trinh and Le, 2018), Redpajama\n(Redpajama, 2023), and Wikipedia5. We apply fuzzy deduplication (Lee et al., 2022) across CommonCrawl\nsnapshots to enhance data quality. Tab. 10 in Appendix A.3 provides details of our pre-training data,\nincluding information such as the number of tokens from each source and associated sampling weights. For\na more comprehensive understanding of the data and its cleaning pipeline, readers are encouraged to refer\nto Appendix A.3.\nMoreover, for instruction tuning, we follow the same settings as Vicuna-v1.1(VicunaTeam, 2023), which\nuses a publicly user-shared instruction following data (ShareGPT, 2023). For reinforcement learning with\nhuman feedback, the training data we used is a combination of the Anthropic\u2019s Helpful and Harmless\ndataset (Bai et al., 2022) and Open-Assistant dataset (K\u00f6pf et al., 2023). The training framework and\nassociated configurations align with the publicly available AlpacaFarm (Dubois et al., 2023).\n3.1.2\nModel Configuration\nThe model architecture we used is a decoder-only Transformer (Brown et al., 2020), which has been widely-\nused in recent generative LLMs like PaLM (Chowdhery et al., 2022), OPT (Zhang et al., 2022), and LLaMA\n(Touvron et al., 2023). In addition to the base architecture, we integrate several modifications proposed\nrecently to improve model efficiency and effectiveness. 1) Rotary Positional Embedding: Drawing inspiration\nfrom recent successful experiments (Black et al., 2022; Touvron et al., 2023), we incorporate rotary positional\nembeddings (RoPE) (Su et al., 2021) into our approach. This addition enables us to capture both absolute\nand relative positions information, enhancing performance especially when extrapolating to larger context\nwindows. 2) Flash Attention: The standard attention implementation is bottlenecked by memory access\n(Ivanov et al., 2021). Flash Attention (Dao et al., 2022) proposed an IO-aware exact attention algorithm\nwhich uses tiling to reduce the amount of HBM accesses, achieving substantial acceleration.\nWe train the models using the proposed FP8 optimizer, which is built upon Adam (Kingma and Ba, 2015)\nwith decoupled weight decay (Loshchilov and Hutter, 2018), following the common practise with the decay\nrates \u03b21 = 0.9, \u03b22 = 0.95, and weight decay = 0.1. The learning rate schedule is cosine-like, and the final\nlearning rate is 10% of the maximal learning rate. We train the models for 100B tokens in total with a batch\nsize of 4M tokens, and the input sequence length is set to 2048. The model warm-up is conducted for 1,000\niterations. Tab. 1 presents the details of model configurations and the corresponding training settings. The\ntraining is conducted on Azure NDv5 H100 GPU platform (Microsoft, 2023).\n4https://commoncrawl.org\n5https://wikipedia.org\n8\n(a) GPT-7B\n(b) GPT-13B\n(c) GPT-175B\nFigure 4: A comparison between FP8 and BF16: Analyzing the training loss of GPT models with the parameters\nranging from 7 billion to 175 billion.\nHS\nLambada\nBoolQ\nPIQA\nCOPA\nWinogrande\nArc-C\nArc-E\nObQA\nAvg\nGPT-7B model zero-shot performance\nBF16\n61.3\n61.4\n61.2\n75.0\n79.0\n58.5\n32.9\n59.7\n36.4\n58.4\nFP8\n60.0\n61.8\n62.0\n74.2\n78.0\n59.8\n32.9\n58.7\n34.6\n58.0\nGPT-13B model zero-shot performance\nBF16\n64.8\n64.9\n63.4\n75.9\n82.0\n61.0\n35.2\n61.5\n40.6\n61.0\nFP8\n64.1\n63.4\n63.9\n76.2\n81.0\n61.6\n34.9\n61.3\n36.8\n60.4\nTable 2: Zero-shot performance on downstream tasks. The models are trained with either the standard BF16\nmixed-precision scheme (Shoeybi et al., 2019) or the proposed FP8 low-precision scheme.\n3.2\nMain Results\n3.2.1\nModel Performance\nWe first compare the performance of models trained using FP8 mixed-precision with those trained using BF16.\nIn Fig. 4, the pre-training loss over tokens is displayed for GPT models of 7B, 13B, and 175B parameters. The\ntraining configurations and hyper-parameters remain consistent across models trained with FP8 and BF16.\nThe only difference lies in the mixed-precision schemes utilized. As shown in Fig. 4, the loss curves almost\noverlap with each other. The results unequivocally demonstrate that the proposed FP8 mixed-precision\nscheme can achieve equivalent performance to the prevalent higher-precision BF16 scheme (Shoeybi et al.,\n2019; Rae et al., 2021; Hoffmann et al., 2022) across a diverse array of model scales. Also, we evaluate the\npre-trained models on a wide range of downstream tasks, including HellaSwag (HS) (Zellers et al., 2019),\nLambada (Paperno et al., 2016) BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), COPA (Roemmele et al.,\n2011), Winogrande (Sakaguchi et al., 2021), Arc (Clark et al., 2018), and OpenbookQA (ObQA) (Mihaylov\net al., 2018). As reported in Tab. 2, the FP8 pre-trained models exhibit comparable zero-shot performance in\ncomparison to their BF16 counterparts. This result provides further validation that models pre-trained with\nFP8 low-precision maintain both accuracy and intrinsic in-context learning capabilities at a level comparable\nto their high-precision counterparts.\nFurthermore, we leverage the proposed FP8 mixed-precision approach for fine-tuning LLMs in instruction\nfollowing. For a fair comparison, we follow the same instruction tuning settings as Vicuna-v1.1 (VicunaTeam,\n2023), which adopts the open-sourced LLaMA-7B (Touvron et al., 2023) as the base model for fine-tuning.\nFig. 5 presents the fine-tuning loss, where the curves corresponding to BF16 and FP8 display a notable degree\nof overlap. Meanwhile, the win-rate of our FP8 fine-tuned models against Davinci-003 (OpenAI, 2022) is also\ncomparable to that of Vicuna-v1.1, which is fine-tuned using BF16 half-precision, as reported in Tab. 3. This\nindicates that our FP8 low-bit training scheme is versatile, as it is applicable not only to pre-training phase\nbut also to downstream fine-tuning tasks.\nIn addition, we further apply the proposed FP8 mixed-precision scheme to reinforcement learning from\nhuman feedback (RLHF), a more complex process to align LLMs with user preferences. Following the\n9\nFigure 5: SFT training loss.\nMixed-precision\nSystem Performance\nModel Performance\nGPU Mem. (GB)\nThroughput\nAlpacaEval\nMT-Bench\nBF16\n51.1\n103\n66.15\n5.75\nFP8\n44.0(-14%)\n131(+27%)\n67.20\n5.70\nTable 3: A comparison between FP8 and BF16 for SFT. For sys-\ntem performance, we report results of GPU memory usage and\ntraining throughput. For model performance, we present the\nwin-rate against Davinci-003 on AlpacaEval and GPT-4 judged\nscores on MT-Bench.\nFigure 6: RLHF training loss.\nMixed-precision\nMemory Usage (MB)\nModel Performance\nWeights\nOptimizer States\nAlpacaEval\nMT-Bench\nBF16\n15,082\n15,116\n72.05\n6.16\nFP8\n10,292(-32%)\n5,669(-62%)\n72.42\n6.04\nTable 4: A comparison of FP8 and BF16 RLHF alignment. Mem-\nory usage is assessed with a focus on weights and optimizer\nstates, while model performance is evaluated on AlpacaEval\nconsidering win-rate against Davinci-003, and MT-Bench using\nGPT-4 judged scores.\nsame training setting as AlpacaFarm (Dubois et al., 2023), a recent RL framework for LLM alignment, we\noptimize policy models with PPO algorithm (Schulman et al., 2017). The solely difference lies in the choice\nof mixed-precision training schemes, i.e., BF16 v.s. FP8. From the results reported in Fig. 6 and Tab. 4, we\nobserve a notable reduction in memory utilization, for instance, a 32% memory reduction concerning model\nweights and a 62% reduction concerning optimizer states. Consequently, it can be inferred that FP8 is capable\nof replicating the BF16 mixed-precision for RLHF training. This underscores the broader applicability and\nversatility of our FP8 low-bit training solution.\n3.2.2\nSystem Performance\nIn this section, we evaluate system-level performance of FP8 mixed-precision, considering communication\nefficiency, memory utilization, and the overall speed, with an emphasis on cost savings. Our method\nemploys 8-bit gradients for all-reduce collective communication among GPUs. Theoretically, this results\nin a 75% reduction in communication costs when compared to the mainstream 32-bit scheme (Despite\nBF16 mixed-precision computing gradients using 16-bit precision, it still employs 32-bit precision for all-\nreduce communication (Shoeybi et al., 2019)). Due to the impact of system transmission loss, the observed\npractical reduction during GPT model training falls within the range of 63% to 65%, as indicated in Table 5.\nFurthermore, it is worth noting that the recent Nvidia Transformer Engine (TE) (Nvidia, 2022b) still relies\non full-precision FP32 for collective communication, resulting in the same level of reduction for our FP8\nsolution.\nWhen training GPT models with identical batch sizes, FP8 mixed-precision can lead to a reduction in memory\nfootprint ranging from 28% to 39% when compared to BF16, as reported in Tab. 5. These reductions in\nmemory consumption are attributed to the FP8 gradient and FP8 optimizer techniques we have introduced.\nMoreover, compared with TE (Nvidia, 2022b), our solution is also very competitive, obtaining 36.1%, 36.0%,\nand 42.1% additional memory reductions for different model sizes, i.e., GPT-7B, 13B, and 175B. Although TE\nemploys FP8 for compute, it still uses high-precision optimizer and gradients, which consumes much more\nmemory than our solution. In addition, the saved memory in our method can be used to train larger batch\nsize or longer sequence. For example, when employing 32 H100 GPUs with a memory capacity of 80GB, our\napproach enables the training of models with a context of 4,096 tokens, accommodating up to 175 billion\nparameters. In contrast, TE can only accommodate models with a context of 2,048 tokens. This showcases\nthe potential of integrating our FP8 mixed-precision training into existing LLMs, empowering them to train\nlonger sequences with the same GPU resources.\n10\nModel\nTP\nPP\nDP\nMicro\nMixed\nGPU\nThroughput\nTFLOPS\nMFU\nWeight-related Comm.\nBS\nPrecision\nMem. (GB)\n(#samples/s)\n(%)\nRate (%)\nVolume (GB)\nGPT-7B\n1\n1\n32\n2\nBF16\n69.6\n159.2\n445\n45.0\n10.1\n37.2\n2\nFP8 (TE)\n77.3\n224.5\n627\n31.7\n9.7\n37.2\n2\nFP8 (Ours)\n49.4 (-29%)\n219.8 (+38%)\n615\n31.1\n7.9\n13.9 (-63%)\n4\nFP8 (Ours)\n69.3\n230.5 (+45%)\n645\n32.6\n10.4\n13.9 (-63%)\nGPT-13B\n2\n1\n16\n2\nBF16\n68.2\n79.3\n420\n42.5\n11.1\n34.3\n2\nFP8 (TE)\n76.4\n111.7\n592\n29.9\n7.1\n34.3\n2\nFP8 (Ours)\n48.9 (-28%)\n109.5 (+38%)\n575\n29.1\n3.9\n12.4 (-64%)\n4\nFP8 (Ours)\n67.8\n121.5 (+53%)\n644\n32.5\n9.3\n12.4 (-64%)\nGPT-175B\n8\n4\n4\n1\nBF16\n66.1\n22.4\n386\n39.0\n8.8\n23.4\n1\nFP8 (TE)\n69.6\n28.7\n493\n24.9\n3.9\n23.4\n1\nFP8 (Ours)\n40.3 (-39%)\n27.1 (+21%)\n473\n23.9\n2.5\n8.2 (-65%)\n4\nFP8 (Ours)\n57.7\n39.3 (+75%)\n677\n34.2\n10.9\n8.2 (-65%)\nTable 5: System-level performance on Nvidia H100 GPUs 80GB. Here, TP, PP, and DP represent tensor,\npipeline, and data parallelism respectively. BS indicates batch size, while MFU denotes model FLOPs\nutilization. Weight-related communication contains the all-gather operator on weights and the reduce-scatter\noperator on weight gradients.\n0\n5\n10\n15\n20\n25\n30\nBlock ID\n0\n50\n100\n150\n200\nSNR\npre scale\npost scale\nauto scale\n(a) SNR (Signal to Noise Ratio)\n0\n5\n10\n15\n20\n25\n30\nBlock ID\n0\n20\n40\n60\nUnderflow Rate (%)\npre scale\npost scale\nauto scale\n(b) Underflow rate\n0\n5\n10\n15\n20\n25\n30\nBlock ID\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nOverflow Rate (%)\npre scale\npost scale\nauto scale\n(c) Overflow rate\nFigure 7: Comparing different strategies, i.e., pre-scaling, post-scaling, and auto-scaling, for FP8 gradient\nall-reduce. We investigate SNR, underflow rate, and overflow rate across different Transformer blocks. The\nexperiment is conducted using a GPT-7B model with a data parallelism factor of 128.\nMoreover, our FP8 mixed-precision scheme shows a superior training throughput compared to the prevalent\nBF16 scheme, achieving a notable speed-up of 75% when applied to GPT-175B model. The model FLOPS\nutilization (MFU) of FP8 mixed-precision training is 34.2% on H100 GPUs, being 37.3% superior to TE.\nThese findings provide substantial evidence that our FP8 scheme effectively conserves memory, reduces\ncommunication costs during the training of large models, and ultimately enhances system utilization efficiency\non the latest H100 GPU platform.\n3.3\nAblation Study\nWe ablate various design choices of FP8 mixed-precision training strategy for LLMs and report the per-\nformance in Tab. 6 \u2013 8 and Fig. 7 \u2013 8. The ablation experiments are conducted on GPT models, whose\narchitectures and training settings are elaborated in Tab. 1. Importantly, our ablation study yields several\nguidelines for the effective utilization of 8-bit datatype in LLM training, which can facilitate future research\non low-bit model training.\nCommunication. We first analyze the limitations of the conventional pre-scaling and post-scaling methods\nwhen aggregating low-bit gradients during the all-reduce communication process. As shown in Fig. 7, we\nconduct a statistical analysis on SNR, underflow rate, and overflow rate of weight gradients across different\nTransformer blocks. It is observed that the pre-scaling method has relative larger underflow rate when\nquantifying gradients from 32-bit to 8-bit, while the post-scaling method has higher overflow rate. In contrast,\nthe proposed auto-scaling technique can diminish both the underflow ratio and the overflow ratio, while\ngetting much better SNR, as shown in Fig. 7 (a). This demonstrates the effectiveness of auto-scaling method\nin reducing quantization errors when utilizing 8-bit datatype for gradient all-reduce.\n11\nLow-bit\nSettings\nCompute\n(GEMM)\nComm.\nMaster\nWeight\nOptimizer\nStates\nFP32 #0\nFP32\nFP32\nFP32\nFP32+FP32\nBF16 #1\nBF16\nFP32\nFP32\nFP32+FP32\nFP8 #2a\nFP8\nFP8\nFP16\nFP8+FP16\nFP8 #2b\nFP8\nFP8\nBF16\nFP8+FP16\nFP8 #3\nFP8\nFP8\nFP8\nFP8+FP16\nFP8 #4\nFP8\nFP8\nFP16\nFP8+FP8\nTable 6: Precision decoupling for the variables\nwithin the optimizer. Here, our focus is on ablating\nthe master weight and optimizer states, as these\ncomponents are precision sensitive. The optimizer\nstates include both first-order and second-order gra-\ndient moments. Note that the FP16 master weight\nuses tensor scaling.\n0\n20\n40\n60\n80\n100\nBillions of Tokens\n3.0\n3.5\n4.0\n4.5\n5.0\nTraining Loss\nFP32 #0\nBF16 #1\nFP8 #2a\nFP8 #2b\nFP8 #3\nFP8 #4\nFigure 8: Training losses of GPT-125M models\nwith the settings presented in Tab. 6. The loss\ncurve for FP8 #4 has diverged.\nModel\nTP\nPP\nDP\nMicro\nMixed\nAct-related Comm.\nBS\nPrecision\nRate (%)\nVolume (GB)\nGPT-13B\n2\n1\n16\n2\nBF16\n12.9\n4.7\nFP8 (Ours)\n5.3\n3.1\nGPT-175B\n8\n4\n4\n1\nBF16\n14.9\n5.9\nFP8 (Ours)\n5.2\n3.9\nTable 7: Activation-related communication vol-\nume reduction in sequence and tensor paral-\nlelism, including the all-gather operator on ac-\ntivation and the reduce-scatter on activation gra-\ndients.\nModel\nTP\nPP\nDP\nMicro\nMixed\nGPU Memory\nBS\nPrecision\nMin\nMax\nGPT-7B\n1\n1\n32\n2\nBF16\n69.07\n69.63\nFP8 (TE)\n76.97\n77.28\nFP8 (Ours)\n49.06\n49.36\nGPT-13B\n2\n1\n16\n2\nBF16\n67.98\n68.18\nFP8 (TE)\n73.68\n76.36\nFP8 (Ours)\n48.45\n48.85\nGPT-175B\n8\n4\n4\n1\nBF16\n65.60\n66.12\nFP8 (TE)\n69.04\n69.57\nFP8 (Ours)\n38.64\n40.28\nTable 8: Comparing ZeRO distribution methods\nin terms of memory load across GPUs. Here\n\u201cMin\u201d and \u201cMax\u201d denote the minimum and max-\nimum memory utilization observed across GPUs.\nOur FP8 ZeRO method uses less memory while\nachieving memory-aware load balancing.\nOptimizer. We further ablate the impact of re-\nduced precision for the variables in the AdamW\noptimizer. We set the BF16 mixed-precision op-\ntimizer as the baseline, since it has been widely\nused in existing LLM training frameworks (Mi-\ncikevicius et al., 2017; Shoeybi et al., 2019; Nvidia,\n2022b). Tab. 6 presents the settings of reduced\nprecision for the variables, while Fig. 8 plots the\ncorresponding training losses. We observe that:\n1) FP8 master weight induces performance degra-\ndation (see the #2a vs. #3 lines in Fig. 8), while\nFP16 can maintain accuracy as FP32 (see #2a vs.\n#0 and #1) but requiring using tensor scaling. It re-\nveals that the master weight is precision-sensitive.\nThis can be attributed to the master weight\u2019s role\nin updating weights, which tend to exhibit small\nmagnitudes, necessitating high precision to main-\ntain accuracy. 2) The training loss of BF16 master\nweight is slightly higher than that of FP16 with\na scaling factor because BF16 has fewer mantissa\nbits, resulting in lower precision (see #2a vs. #2b).\n3) The second-order gradient moment is more\nprecision-sensitive than the first-order one, be-\ncause the square calculation is easy to cause under-\nflow and leads to accuracy degradation. Utilizing\nFP8 for the second-order gradient moment can\nlead to divergent training loss (see the #4 dot in\nFig. 8).\nParallelism. In our FP8 LLM training framework, we introduce FP8 low-bit convertors into sequence parallelism\nand tensor parallelism to reduce activation communication costs across GPUs. Here we conduct an analysis\nexperiment to count the activation-related communication volume during GPT model training, and report\nthe numbers in Tab. 7. It is observed that our FP8 parallel scheme results in a substantial reduction of 34% in\nactivation-related communication costs compared to the original method utilizing BF16. Furthermore, in\nZeRO distributed training, our method distributes each FP8 tensor along with its associated scaling factor as\na whole, rather than partitioning the tensor into splits across GPUs. This strategy not only results in more\nGPU memory savings but also maintains a balanced memory load across GPUs, as demonstrated in Tab. 8.\n12\n4\nRelated Work\nMixed-precision Training. Efficient training through reduced mixed-precision has been widely used in\nmodern deep learning to save computing costs. While some works have taken bit-reduction to the extreme,\ni.e. 1-bit binary networks (Hubara et al., 2016; Rastegari et al., 2016), they have not been successful in\nmaintaining model accuracy (Micikevicius et al., 2022). The most practical scheme now is the FP16 half-\nprecision method (Micikevicius et al., 2017), which can maintain accuracy while improving training efficiency.\nThe computations during forward pass and back propagation use FP16 while the master weights use FP32.\nSince FP16 has a narrower dynamic range, FP16 mixed-precision entails loss scaling (Micikevicius et al., 2017)\nto prevent loss of accuracy. Fortunately, the need for loss scaling can be avoided by using BF16 datatype,\nbecause BF16 maintains the same dynamic range as the full-precision FP32. This results in that large model\ntraining now prefers to use BF16 mixed-precision scheme, which is more stable during training (Smith et al.,\n2022; Scao et al., 2022; Zeng et al., 2022).\nFP8 is a natural progression from 16-bit data formats to further reducing computing cost. Early pioneering\nefforts in FP8 low-bit model training (Wang et al., 2018; Sun et al., 2019; Dettmers et al., 2021) have largely\nremained at the simulation stage. Consequently, there exists a notable gap between the projected capabilities\nof these approaches and their actual performance on hardware (Micikevicius et al., 2022). With the advent\nof Nvidia Hopper GPU architecture (Nvidia, 2022a), FP8 is emerging as a viable and practical data type for\nthe next-generation low-precision training, as discussed in (Micikevicius et al., 2022). At present, the Nvidia\nTransformer Engine (TE) (Nvidia, 2022b) serves as the primary framework for FP8 mixed-precision training.\nHowever, its support for FP8 usage remains somewhat constrained. TE\u2019s current implementation restricts\nFP8 usage solely to weight computation, retaining the storage of model weights and gradient calculations\nwith 16-bit data types. Consequently, the end-to-end speed-up, memory and communication cost savings\nare limited. In contrast, our work infiltrates FP8 gradient, optimizer, and distributed training into the whole\nprogress of model training, fully unveiling the capabilities of FP8.\nLarge Language Models. Recent years have witnessed a substantial evolution in the field of LLMs. Autore-\ngressive language modeling \u2013 predicting the future of a text sequence from its past \u2013 provides a simple yet\npowerful objective that admits formulation of numerous tasks. While there exist alternative methodologies,\nsuch as masked language modeling (Devlin et al., 2019) and permutation language modeling (Yang et al.,\n2019), the autoregressive method now is more promising because of its strong performance. Following the\nscaling laws (Brown et al., 2020) and the refined laws (Hoffmann et al., 2022), various LLMs are have been\nproposed, including dense models: GPT-3 (Brown et al., 2020), Jurassic-1 (Lieber et al., 2021), Gopher (Rae\net al., 2021), Chinchilla (Hoffmann et al., 2022), Bloom (Scao et al., 2022), OPT (Zhang et al., 2022) Megatron-\nTuring NLG (Smith et al., 2022), PaLM (Chowdhery et al., 2022), LaMDA (Thoppilan et al., 2022), LLaMA\n(Touvron et al., 2023), and sparse models: GLaM (Du et al., 2022), and Switch transformers (Fedus et al.,\n2022). Each of them has demonstrated remarkably competitive few-shot performance across a wide range of\ntasks at the time of their respective releases. Nonetheless, these models still encounter challenges, such as\noverwhelming computational requirements and the need for acquiring more high-quality training data. In\nthis work, we delve into the utilization of low-precision techniques to mitigate the training costs, which is a\ncrucial step for the continued expansion of language models.\nLow-precision training has been widely used in LLM training to reduce compute cost. OPT (Zhang et al.,\n2022) and GLM (Zeng et al., 2022) utilize FP16 for forwards and backwards and FP32 for optimizer states and\nmaster weights, to reduce the GPU memory usage and improve training efficiency. Bloom (Scao et al., 2022)\nfind that FP16 can cause numerical instabilities and irreversible divergences, especially when training models\nlarger than 100B parameters, because FP16\u2019s dynamic range is limited. Consequently, Bloom and other LLMs,\nsuch as Gopher (Rae et al., 2021) and Chinchilla (Hoffmann et al., 2022), adopt BF16 mixed-precision, because\nBF16 has a wide dynamic range that is the same as FP32. LLM training and tuning with 8-bit low-precision\nwere not well-explored in previous works, because the hardware support for FP8 is not available before\nthe release of Nvidia Hopper infrastructure. This work presents the first exploration of FP8 pre-training\nand fine-tuning for LLMs, while proposing an extremely-optimized FP8 mixed-precision scheme. We hope\nthis work could facilitate future research in FP8 and, potentially, extend to exploring even lower precision\ntraining, such as 4-bit and 1-bit.\n13\n5\nConclusion\nIn this work, we explore 8-bit training for LLMs. We introduce a new FP8 mixed-precision training frame-\nwork, which incorporates 8-bit collective communication, optimizer, and distributed parallel training in an\nincremental manner. To our best knowledge, this is the first work infiltrating FP8 compute, storage and\ncommunication into the whole progress of large language model training. Extensive experiments demonstrate\nthe proposed method effectively diminishes communication overhead and curtails memory utilization in the\ncontext of GPT model training at various scales. In future work, we plan to scale up the size and training\nsteps of the FP8 GPT models and further train them with our 8-bit mixed-precision scheme. Moreover, we\nwill also use the proposed FP8 scheme to train multi-modal large models, and explore low-bit deployment of\nLLMs on various edge devices, such as smart phones.\nContribution and Acknowledgement\nThis project was initially proposed by Han Hu and Peng Cheng, who are the directional lead. Shuguang Liu\nserved as the product lead throughout the project.\nThe contributions for all the co-authors are detailed as follows:\nFP8 Framework: Kan Wu, Houwen Peng, Ze Liu, Peng Cheng, Han Hu\nSystem: Yifan Xiong, Ziyue Yang, Yuxiang Yang, Guoshuai Zhao, Peng Cheng\nHardware Infrastructure: Guoshuai Zhao, Yuxiang Yang, Yifan Xiong, Peng Cheng, Shuguang Liu, Joe Chau\nData: Ruihang Li, Miaosen Zhang, Jia Ning, Chen Li, Ruizhe Wang, Houwen Peng, Han Hu\nPre-training: Yixuan Wei, Kan Wu, Ze Liu, Miaosen Zhang, Zheng Zhang, Houwen Peng, Han Hu\nAlignment (SFT, RS, and RLHF): Bolin Ni, Jingcheng Hu, Yixuan Wei, Houwen Peng, Han Hu\nEvaluation: Yixuan Wei, Bolin Ni, Jingcheng Hu\nProduct Engineering: Yuxiang Yang, Kan Wu, Yifan Xiong, Ziyue Yang, Guoshuai Zhao, Peng Cheng\nWe thank Eric Chung, Bita Darvish Rouhani, Yu Pei, Hyunseung Harry Yoo, Zhenghong Zhou, Gongrui\nZhang, and Zhirong Wu for helpful discussions.\nWe thank Baining Guo and Lidong Zhou for their guidance and support for this project.\n14\nReferences\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint\narXiv:2305.10403, 2023.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav\nFort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement\nlearning from human feedback. arXiv preprint arXiv:2204.05862, 2022.\nMicrosoft Bing. Bing webmaster tools. 2022. URL https://www.bing.com/webmasters/.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in\nnatural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439,\n2020.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language\nmodel. In Proceedings of BigScience Episode# 5\u2013Workshop on Challenges & Perspectives in Creating Large Language\nModels, pages 95\u2013136, 2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners.\nIn Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.,\n2020.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek B Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar\nPrabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,\nMichael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,\nHenryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne\nIppolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan,\nShivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,\nAitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei\nZhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S.\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling\nwith pathways. ArXiv, abs/2204.02311, 2022.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.\nBoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 2924\u20132936, Minneapolis, Minnesota, June 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1,\n2018.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient\nexact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359, 2022.\nTim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization.\nIn International Conference on Learning Representations, 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\n15\npages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:\n10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-\nexperts. In International Conference on Machine Learning, pages 5547\u20135569. PMLR, 2022.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\nLiang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from\nhuman feedback. arXiv preprint arXiv:2305.14387, 2023.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models\nwith simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232\u20135270, 2022.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace\nHe, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027, 2020.\nAaron Gokaslan and Vanya Cohen.\nOpenwebtext corpus.\nhttp://Skylion007.github.io/\nOpenWebTextCorpus, 2019.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv:2203.15556, 2022.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural\nnetworks. Advances in neural information processing systems, 29, 2016.\nHuggingFace. wikipedia - datasets at hugging face. 2022. URL https://huggingface.co/datasets/\nwikipedia.\nAndrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need:\nA case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711\u2013732, 2021.\nArmand Joulin, \u00c9douard Grave, Piotr Bojanowski, and Tom\u00e1\u0161 Mikolov. Bag of tricks for efficient text\nclassification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational\nLinguistics: Volume 2, Short Papers, pages 427\u2013431, 2017.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International Conference\non Learning Representations, San Diego, CA, 2015. URL http://arxiv.org/abs/1412.6980.\nDenis Kocetkov, Raymond Li, LI Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Mu\u00f1oz\nFerrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, et al. The stack: 3 tb of permissively licensed\nsource code. Transactions on Machine Learning Research, 2022.\nAndreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Ab-\ndullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00e1rd Nagyfi, et al. Openassistant conversations\u2013\ndemocratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and\nNicholas Carlini. Deduplicating training data makes language models better. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8424\u20138445,\n2022.\nShenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen Huang, Yuliang Liu, Boxiang Wang, and\nYang You. Colossal-ai: A unified deep learning system for large-scale parallel training. In Proceedings of the\n52nd International Conference on Parallel Processing, pages 766\u2013775, 2023a.\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and\nTatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https:\n//github.com/tatsu-lab/alpaca_eval, 2023b.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. White\nPaper. AI21 Labs, 1, 2021.\n16\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\nZe Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang,\nLi Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 12009\u201312019, 2022.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on\nLearning Representations, 2018.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv\npreprint arXiv:1710.03740, 2017.\nPaulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite,\nSangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. arXiv\npreprint arXiv:2209.05433, 2022.\nMicrosoft.\nAzure high-performance computing.\n2023.\nURL https://azure.microsoft.com/en-us/\nsolutions/high-performance-computing.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a\nnew dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 2381\u20132391, 2018.\nNvidia. Apex. 2018. URL https://nvidia.github.io/apex.\nNvidia. The nvidia collective communications library. 2020. URL https://developer.nvidia.com/nccl.\nNvidia.\nNvidia h100 tensor core gpu architecture.\n2022a.\nURL https://resources.nvidia.com/\nen-us-tensor-core.\nNvidia.\nNvidia transformer engine.\n2022b.\nURL https://docs.nvidia.com/deeplearning/\ntransformer-engine/index.html.\nNvidia.\nUsing fp8 with transformer engine.\n2022c.\nURL https://docs.nvidia.com/deeplearning/\ntransformer-engine/user-guide/examples/fp8_primer.html.\nOpenAI.\nModel index for researchers.\n2022.\nURL https://platform.openai.com/docs/\nmodel-index-for-researchers.\nOpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The lambada dataset: Word predic-\ntion requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1525\u20131534, 2016.\nShawn Presser. Books3. https://twitter.com/theshawwn/status/1320282149329784833, 2020.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive\ntransformers for long-range sequence modelling. In International Conference on Learning Representations,\n2019.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides,\nSarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis &\ninsights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The\nJournal of Machine Learning Research, 21(1):5485\u20135551, 2020.\n17\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward\ntraining trillion parameter models. In SC20: International Conference for High Performance Computing,\nNetworking, Storage and Analysis, pages 1\u201316. IEEE, 2020.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages\n8821\u20138831. PMLR, 2021.\nMohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification\nusing binary convolutional neural networks. In European conference on computer vision, pages 525\u2013542.\nSpringer, 2016.\nRedpajama. Redpajama-data: an open source recipe to reproduce llama training dataset. 2023. URL\nhttps://github.com/togethercomputer/RedPajama-Data.\nMelissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plausible alternatives: An\nevaluation of commonsense causal reasoning. In AAAI Spring Symposium, 2011.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial\nwinograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning\nabilities of neural models. In International Conference on Learning Representations, 2018.\nTeven Le Scao, 388 Authors, and Thomas Wolf. BLOOM: A 176B-parameter open-access multilingual language\nmodel. ArXiv, abs/2211.05100, 2022.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\nShareGPT. Openchat: Advancing open-source language models with imperfect data. 2023. URL https:\n//sharegpt.com/.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint\narXiv:1909.08053, 2019.\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper,\nZhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron\nto train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990,\n2022.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\nXiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Viji\nSrinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit floating point (hfp8)\ntraining and inference for deep neural networks. Advances in neural information processing systems, 32, 2019.\nXiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani,\nKaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision\n4-bit training of deep neural networks. Advances in Neural Information Processing Systems, 33:1796\u20131807,\n2020.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv\npreprint arXiv:2201.08239, 2022.\nJ\u00f6rg Tiedemann. Finding alternative translations in a large corpus of movie subtitle. In Proceedings of the\nTenth International Conference on Language Resources and Evaluation (LREC\u201916), pages 3518\u20133522, 2016.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\nTrieu H Trinh and Quoc V Le. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847,\n2018.\n18\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin.\nAttention is all you need.\nIn Advances in Neural Information Process-\ning Systems, pages 5998\u20136008. Curran Associates, Inc., 2017.\nURL http://papers.nips.cc/paper/\n7181-attention-is-all-you-need.pdf.\nVicunaTeam. Vicuna: An open-source chatbot impressing gpt-4 with 90quality. 2023. URL https://lmsys.\norg/blog/2023-03-30-vicuna/.\nNaigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep\nneural networks with 8-bit floating point numbers. Advances in neural information processing systems, 31,\n2018.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand\nJoulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data.\narXiv preprint arXiv:1911.00359, 2019.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet: Gen-\neralized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/\ndc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine\nreally finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pages 4791\u20134800, Florence, Italy, July 2019. Association for Computational Linguistics. doi:\n10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\nZheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. In The Eleventh International\nConference on Learning Representations, 2022.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv\npreprint arXiv:2205.01068, 2022.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv\npreprint arXiv:2306.05685, 2023.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. In Proceedings of the IEEE international conference on computer vision, pages 19\u201327, 2015.\n19\nA\nAppendix\nA.1\nFP8 Data Formats\nIn September 2022, NVIDIA, ARM, and Intel published FP8 specification for standardization as an interchange\nformat for AI (Micikevicius et al., 2022). The industry has moved from 32-bit precision to 16-bit, and now even\n8-bit precision for AI model training. This development reflects a broader industry trend that has transitioned\nfrom high-precision to low-precision training. Notably, the proposed FP8 specification introduces two distinct\ndata types, E5M2 and E4M3, which offer a trade-off between a larger range and higher precision of stored\nvalues (Nvidia, 2022c).\n\u2022 E4M3 consists of 1 sign bit, 4 exponent bits and 3 bits of mantissa. It can store values up to +/-448\nand NaN.\n\u2022 E5M2 consists of 1 sign bit, 5 exponent bits and 2 bits of mantissa. It can store values up to +/-57344,\n+/- inf and NaN.\nThe FP8 format (Micikevicius et al., 2022) roughly follows the IEEE 754 standard. Compared to higher\nprecision data formats such as FP16 and FP32, FP8 suffers from two kinds of representation degradation:\n\u2022 Lower representation range. The representation range in a data format specifies the range between the\nmaximum and minimum values that the format can accurately represent. There are two modes, a\nnormal mode, which defines a regular range with relatively constant precision, and a subnormal\nmode, which extends the range to represent smaller values with lower precision. The normal range\nprimarily depends on the number of exponent (E) bits, with more E bits resulting in a larger normal\nrange. On the other hand, the subnormal range is primarily influenced by the number of mantissa\n(M) bits, where an increase in M bits leads to a larger subnormal range. As illustrated in Tab. 9, the\nrepresentation range of FP8 is notably narrower compared to that of FP16 and FP32, especially in the\ncase of the S1E4M3 sub-format (S denotes the sign bit). This discrepancy represents the primary\nchallenge when employing FP8 for training large models.\n\u2022 Lower representation precision. The limited number of mantissa (M bits) leads to quantization rep-\nresentation errors. Due to the considerably fewer M bits in FP8, the representation precision of\nFP8 is substantially lower than that of FP16, as depicted in Tab. 9. This challenge stands as another\nsignificant hurdle when considering the use of FP8 for training large models.\nFP8 consists of two sub-formats: S1E4M3 and S1E5M2. The former offers a narrower representation range\nbut higher precision, while the latter provides a larger range but lower precision. These two sub-formats give\nusers the flexibility to strike a balance between their requirements for range and precision in model training.\nTable 9: Representation range and error for different data formats\nData format\nRepresentation Range\nMaximum Relative Error\nMax normal\nMin normal\nMin subnormal\nMin - Max (normal)\nMin \u223c Max (subnormal)\nFP32\n(S1E8M23)\n3.40 \u00d7 1038\n1.18 \u00d7 10\u221238\n1.40 \u00d7 10\u221245\n1.19 \u00d7 10\u22127 \u223c 5.96 \u00d7 10\u22128\n5.00 \u00d7 10\u22121 \u223c 1.19 \u00d7 10\u22127\nFP16\n(S1E5M10)\n65, 504\n6.10 \u00d7 10\u22125\n5.96 \u00d7 10\u22128\n9.76 \u00d7 10\u22124 \u223c 4.89 \u00d7 10\u22124\n5.00 \u00d7 10\u22121 \u223c 9.78 \u00d7 10\u22124\nBF16\n(S1E8M7)\n3.39 \u00d7 1038\n1.18 \u00d7 10\u221238\n9.18 \u00d7 10\u221241\n7.75 \u00d7 10\u22123 \u223c 3.94 \u00d7 10\u22123\n5.00 \u00d7 10\u22121 \u223c 7.94 \u00d7 10\u22123\nFP8\n(S1E4M3)\n448\n1.56 \u00d7 10\u22122\n1.95 \u00d7 10\u22123\n1.11 \u00d7 10\u22121 \u223c 7.69 \u00d7 10\u22122\n5.00 \u00d7 10\u22121 \u223c 1.67 \u00d7 10\u22121\nFP8\n(S1E5M2)\n57, 344\n6.10 \u00d7 10\u22125\n1.53 \u00d7 10\u22125\n2.00 \u00d7 10\u22121 \u223c 1.67 \u00d7 10\u22121\n5.00 \u00d7 10\u22121 \u223c 5.00 \u00d7 10\u22121\n20\nA.2\nFP8 Tensor Scaling\nWe now discuss the underlying mechanisms for how large model training with FP8 overcomes the challenges\nassociated with representation range and precision degradation. The key technique behind is tensor scaling,\nwhich scales the tensor values that originally locate out the representation range of a data format to its\ncomfort zone, as visualized in Fig. 9. The pioneer scaling techniques (Micikevicius et al., 2017; Nvidia, 2018)\napply a global scaling factor to the loss, such that gradients of all layers are scaled by a single adaptive factor.\nThe utilization of the global loss scaling technique, in conjunction with various other training strategies, has\nfacilitated the widespread adoption of FP16 mixed-precision training on V100 and A100 GPUs. Remarkably,\nthis approach has resulted in minimal to no degradation in accuracy, particularly for small to medium-sized\nmodels (Micikevicius et al., 2017). Nonetheless, when dealing with super-large models or complex tasks,\nsuch as in the training of models like DALL-E (Ramesh et al., 2021), the global loss scaling technique still\nencounters significant underflow issues. As a consequence, block-wise (Ramesh et al., 2021) and layer-wise\n(Sun et al., 2020) gradient scaling are proposed.\nWhile the global scaling technique enables almost no accuracy drop for FP16 training (with a range of [5.96E-8,\n6.55E+4]), the fine-grained per-tensor scaling will enable stable model training using even shallower range by\nFP8 (with a range of [1.95E-3, 448] for E4M3 and a range of [1.53E-5, 5.73E+4] for E5M2). Fig. 9 shows that\nthe representation range of FP8 has been large enough to deal with general model training. In the per-tensor\nscaling technique, various strategies are available for choosing the suitable scaling factor for a given FP8\ntensor. Two common approaches are \u201cjust-in-time scaling\" and \u201cdelayed scaling\" (Nvidia, 2022c).\n\u2022 Just-in-time scaling. This strategy involves determining the scaling factor based on the maximum\nabsolute value (amax) of the tensor being generated. However, in practical applications, this approach\nis often infeasible because it necessitates multiple passes through the data. Specifically, the operator\nfirst produces and writes out the output in higher precision, then calculates the maximum absolute\nvalue of the output, and finally applies this scaling factor to all values to obtain the final FP8 output.\nThis process introduces a significant amount of overhead, which can substantially reduce the benefits\nof using FP8.\n\u2022 Delayed scaling. This strategy involves selecting the scaling factor based on the maximum absolute\nvalues observed in a certain number of preceding iterations. This approach allows for the full\nperformance benefits of FP8 computation but necessitates the storage of a history of maximum values\nas additional parameters of the FP8 operators.\nFigure 9: Scaling gradients to fall within the representation range of the FP8 datatype.\n21\nA.3\nPre-training Data\nTab. 10 presents an overview of our collected data sources along with the corresponding sampling weights\nemployed in pre-training. The arXiv and StackExchange subsets are collected from Redpajama (Redpa-\njama, 2023), while BookCorpus2 (Zhu et al., 2015), Books3 (Presser, 2020), DM-Math (Saxton et al., 2018),\nGutenberg (Rae et al., 2019), HackerNews6, NIH ExPorter7, OpenSubtitles (Tiedemann, 2016), and USPTO8\nsubsets are extracted from The Pile (Gao et al., 2020). The Wikipedia data is downloaded from HuggingFace\n(HuggingFace, 2022). We use the 20220301 dump, including 24 languages: bg, ca, cs, da, de, en, es, fr, hi, hr,\nhu, it, jp, ko, nl, pl, pt, ro, ru, sl, sr, sv, uk, zh.\nWe pre-process 11 CommonCrawl snapshots, ranging from 2018 to 2023, with the CCNet pipeline (Wenzek\net al., 2019). This process involves data deduplication at the line level, followed by language identification\nutilizing a fastText linear classifier (Joulin et al., 2017) to eliminate non-English pages. A filtering mechanism\nbased on an n-gram language model is employed to exclude low-quality content. In addition, we train a linear\nclassifier (Redpajama, 2023) to distinguish documents similar to Wikipedia pages from randomly sampled\nCommonCrawl documents. Documents not classified as resembling Wikipedia are excluded. Finally, we\nperform fuzzy deduplication (Lee et al., 2022) across all the processed snapshots from CommonCrawl.\nWe collect Python code data from Github using a repository list provided by Bing indexing (Bing, 2022).\nThe cleaning of the code data includes three steps. First, we remove control characters, except for \\t and \\n.\nNext, we remove copyright comments in the code. An alphanumeric rate filter is then applied, removing\nlines with a rate below 0.5 if they are comments, and discarding the entire file if its overall alphanumeric rate\nis less than 0.98. Files with less than 5 lines or a maximum line length exceeding 1,000 characters are also\ndiscarded. Also, files with an average line length of more than 100 characters are discarded. Lastly, a pattern\nsearch is conducted to identify key Python keywords (e.g., import, from, def, class, if, for, try, etc.) within the\ncode. Files containing less than 3 instances of these keywords are eliminated. This comprehensive process\nensures that the remaining Python code data is of high quality and suitable for use in academic research. We\nadditionally add Python code from Stack (Kocetkov et al., 2022), and perform fuzzy deduplication within all\nthe collected Python code.\n6https://news.ycombinator.com\n7https://exporter.nih.gov\n8https://bulkdata.uspto.gov\n22\nDataset\nSampling prop.\nEpochs\nTraining Tokens (Billion)\nWeb Crawls\nCommonCrawl\n51.71%\n0.16\n51.71\nC4\n25.56%\n0.16\n25.56\nOpenWebText\n2.73%\n0.16\n2.73\nTechnical & Science content\narXiv\n1.54%\n0.05\n1.54\nStackExchange\n1.42%\n0.08\n1.42\nDM-Math\n0.39%\n0.05\n0.39\nUSPTO\n0.52%\n0.05\n0.52\nNIH ExPorter\n0.04%\n0.05\n0.04\nProgramming Languages\nPython\n4.50%\n0.11\n4.50\nOther Curated Sources\nWikipedia\n4.50%\n0.16\n4.50\nBooks\n4.50%\n0.09\n4.50\nNews\n2.00%\n0.11\n2.00\nDialogue\n2.00%\n0.27\n2.00\nTotal\n100.00\nTable 10: Pre-training data. For each subset we list the sampling weight, number of epochs, and training\ntokens. Books data includes BookCorpus2 (Zhu et al., 2015), Books3 (Presser, 2020), and Gutenberg (Rae\net al., 2019). Dialogue data includes HackerNews and OpenSubtitles (Tiedemann, 2016). For experiments\nwith a training token count of less than 100 billion, we employ the same sampling proportion.\n23\n"
  },
  {
    "title": "ControlLLM: Augment Language Models with Tools by Searching on Graphs",
    "link": "https://arxiv.org/pdf/2310.17796.pdf",
    "upvote": "15",
    "text": "ControlLLM: Augment Language Models with Tools by Searching on Graphs\nZhaoyang Liu\u22171,2\nZeqiang Lai\u22172\nZhangwei Gao2\nErfei Cui2\nZiheng Li3\nXizhou Zhu2,3\nLewei Lu4\nQifeng Chen1 \u0000Yu Qiao2\nJifeng Dai2,3\nWenhai Wang2 \u00001The Hong Kong University of Science and Technology\n2OpenGVLab, Shanghai AI Laboratory\n3Tsinghua University\n4SenseTime\nhttps://github.com/OpenGVLab/ControlLLM\nAbstract\nWe present ControlLLM, a novel framework that enables\nlarge language models (LLMs) to utilize multi-modal tools\nfor solving complex real-world tasks. Despite the remark-\nable performance of LLMs, they still struggle with tool invo-\ncation due to ambiguous user prompts, inaccurate tool se-\nlection and parameterization, and inefficient tool schedul-\ning. To overcome these challenges, our framework com-\nprises three key components: (1) a task decomposer that\nbreaks down a complex task into clear subtasks with well-\ndefined inputs and outputs; (2) a Thoughts-on-Graph (ToG)\nparadigm that searches the optimal solution path on a pre-\nbuilt tool graph, which specifies the parameter and depen-\ndency relations among different tools; and (3) an execution\nengine with a rich toolbox that interprets the solution path\nand runs the tools efficiently on different computational de-\nvices. We evaluate our framework on diverse tasks involv-\ning image, audio, and video processing, demonstrating its\nsuperior accuracy, efficiency, and versatility compared to\nexisting methods.\n1. Introduction\nLarge-scale language models, such as ChatGPT [24] and\nLLaMA series [36, 37], have demonstrated impressive ca-\npability in understanding and generating natural language.\nBeyond their prowess in linguistic tasks, these models have\nbeen rapidly extended to interaction, planning, and reason-\ning, propelling the advancement of studies in multi-modal\ninteraction [1, 16, 17, 21, 38, 39, 51].\nOne of the emerging examples of multi-modal interac-\ntion is tool-augmented language models [20, 31, 32, 43, 45],\nwhich strive to enhance the capabilities of language mod-\nels to include diverse modalities beyond text such as image,\n* Equal contribution.\n\u0000 Corresponding authors (wangwenhai@pjlab.org.cn, cqf@ust.hk).\nvideo, audio, etc. These models employ LLMs as primary\ncontrollers and incorporate tools with diverse functionalities\nas plugins, which solves a wide range of multi-modal tasks.\nHowever, challenges in this field still persist, covering task\ndecomposition, task planning, and efficient tool scheduling.\nWith these challenges in mind, prior methods [20, 29,\n32, 34, 43, 45, 46] made their endeavors in developing tool-\naugmented LLMs. They utilize LLMs with input-output\nprompting, CoT [41] or ToT [47] to perform task planning.\nThese methods can solve problems by breaking them into a\nchain or tree of sub-tasks. Theoretically, as long as LLMs\nhave strong generalization ability, these methods can also\nsolve complex tasks. However, in practice, we found that\nthese methods often suffer from inaccurate tool invocation\nproblems when dealing with complex cases. This is due to\nthe fact that solutions for complex tasks often contain tool\ninvocations with intricate topological structures. It is insuf-\nficient for these methods to form a complex thought net-\nwork and thus fail to solve complicated tasks. Therefore,\nit requires us to figure out a new paradigm beyond chain-\nshaped or tree-shaped ones, which can generate solutions\nwith intricate topology structures to solve more complicated\nproblems (see Fig. 1 and Fig. 2).\nTo this end, we introduce ControlLLM, a new framework\nthat assists large language models in accurately and effi-\nciently controlling multi-modal tools and identifying com-\nprehensive solutions for complex real-world tasks involving\nmulti-modal inputs. Alongside a variety of improvements\nover previous works, our framework places particular em-\nphasis on three aspects as follows:\nTask Decomposition. A task decomposer is introduced\nto analyze the user prompt and breaks it down into a num-\nber of subtasks, each with well-defined attributes such as\ntask description, task domain, arguments, and returned out-\nput. By decomposing complex tasks into manageable sub-\ntasks, the task decomposer significantly enhances the sys-\ntem\u2019s ability to handle intricate user prompts, which paves\nthe way for follow-up task planning and solution execution.\n1\narXiv:2310.17796v3  [cs.CV]  18 Dec 2023\nInput\nOutput\nInput\nOutput\nInput\nOutput\nInput\nOutput\n1. 3. +! , \u2264 1 & +\" , \u2264 1\n1. 3. +! , \u2264 1\nNo +! ,  or +\" , Constraints\n(a) Chain of Thought (CoT)\n(b) Tree of Thoughts (ToT)\n(c) ControlLLM (ours)\nTool Dependency (Edge, /)\nTool (Vertex, ,)\n+\" 0\nOut-Degree\n+! 0\nIn-Degree\nSolution Path\nThoughts on Graph \nself\u2212consistency\nFigure 1. Comparisons of different paradigms for task planning. (a) Chain of Thought (CoT) [41], CoT with self-consistency [40] and\n(b) Tree of Thoughts [47] (ToT) essentially rely on the LLMs to perform task planning, where the edge is actually formed by LLMs at run\ntime. (c) The Thoughts-on-Graph (ToG) paradigm in our method searches for solutions on a pre-built graph that captures the dependencies\nof tools, which avoids the hallucination problem in tool invocation.\nTask Planning.\nThis part handles tool selection and\ntool argument assignment. We propose a thoughts-on-graph\n(ToG) paradigm that traverses a topological tool graph to\nsearch for solutions. The nodes of the graph are tools that\nare connected based on their dependencies and relation-\nships. ToG orchestrates the selected tools and controls the\nflow of resources among them to form possible solutions.\nToG can find the optimal solution for each sub-task by ap-\nplying diverse search strategies on the graph. Due to the\nconcrete definition in subtask and explicit tool dependen-\ncies in a tool graph, ToG can effectively search all feasible\nsolution paths in cases where the selected optimal solution\nfails to meet users\u2019 preferences.\nSolution Execution. We design an execution engine that\ncan execute the solution generated by ToG and craft infor-\nmative and well-formatted responses. The engine has ac-\ncess to a versatile toolbox consisting of various tools from\ndifferent sources, such as locally deployed APIs or cloud\nservices. The engine can also parallelize the tool executions\naccording to the topology of the solution path to reduce the\nlatency and provide feedback during the execution process.\nOur ControlLLM offers several advantages. (1) It can ac-\ncurately handle complex real-world tasks that involve multi-\nmodal inputs and outputs, while previous methods [4, 19,\n20, 32, 43, 45] usually fail to handle due to their capabilities\nof task planning; (2) It can overcome the token limitation of\nLLMs during task planning. Because our method searches\nthe optimal solution path on the tool graph, instead of ask-\ning LLMs to generate a solution for the task; (3) It can eas-\nily scale up toolbox. Since all solutions lie in the tool graph,\nwhen tools change, we only need to rebuild the graph with-\nout re-training LLMs or updating in-context prompts.\nTo evaluate the effectiveness of ControlLLM in tasks of\ndifferent complexities, we construct a benchmark with a se-\nries of tailored metrics. Specifically, we use irrelevant tool\ninclusion rate and necessary tool inclusion rate to measure\ntool selection. We employ the resource hallucination rate\nand resource type consistency rate to assess argument as-\nsignments. We also split the test set into three difficulty lev-\nels based on the number of APIs involved: easy (< 2 APIs),\nmedium (2 or 3 APIs), and hard (> 3 APIs). We conducted\nvarious experiments, both quantitatively and qualitatively,\nto compare our method with existing ones. The results show\nthat ControlLLM achieves a higher success rate in tool in-\nvocation, especially for complicated instructions.\nIn summary, the main contributions are as follows:\n(1) We propose ControlLLM, a framework that lets\nLLMs use various tools across different modalities to solve\ncomplex tasks in the real world. With a powerful toolbox,\nControlLLM can be easily extended to tasks with natural\nlanguage, images, audio, video, or any mix of them.\n(2) We design three tailored components in Control-\nLLM: Task decomposition, which breaks down the user\nprompt into subtasks with well-defined inputs and outputs;\nToG paradigm for task planning, searching the optimal solu-\ntion path on a graph that depicts tool dependencies; And an\nexecution engine with a powerful toolbox, which efficiently\nschedules and executes the solution path.\n(3) We construct a benchmark to assess the efficacy of\nControlLLM on tasks with different complexity levels. The\n2\nexperimental results demonstrate significant improvements\nin tool usage. Notably, ControlLLM achieves a success rate\nof 93% in the metric of overall solution evaluation on chal-\nlenging tasks, while the best baseline only reaches 59%.\n2. Related Work\nPlanning, Reasoning, and Decision Making. It is a long-\nstanding vision to empower autonomous agents with the\nabilities of planning, reasoning, and decision-making [15,\n33, 42].\nDespite progressive development, it was recent\nadvancements in large language models (LLM) [3, 5, 25,\n36, 50] that have taken a breakthrough step in addressing\nthese problems on the broad user requests. Nevertheless, it\nis shown that LLMs still suffer from difficulties in dealing\nwith knowledge-heavy and complex tasks [30]. To over-\ncome these issues, Chain of Thoughts (CoT) [41] is in-\ntroduced as a simple Tool Documentation Enables Zero-\nShot Tool-Usage with Large Language Modelsprompting\ntechnique to elite the complex reasoning capabilities of\nLLMs. Following this line of work, CoT with self consis-\ntency [40], Tree of Thoughts (ToT) [41], and other tech-\nniques [6, 12, 55], have been proposed to improve the rea-\nsoning abilities further. There are also several works [2, 48]\nthat introduce techniques called Graph-of-Thought (GoT).\nThey all share a common insight that relies on LLMs to\ngenerate thoughts for solving complicated NLP problems.\nIn contrast, our ToG aims to endow the language model with\nthe ability to use tools for a multi-modal dialogue system.\nFurthermore, ToG builds a tool-graph in advance without\nrequiring LLMs and uses a search algorithm to form a com-\nplicated thought network for task planning.\nTool-Augmented LLM. Drawing inspiration from the\nevolving planning and decision-making capabilities ob-\nserved in Large Language Model (LLM) systems, a new\nwave of research starts to enhance LLMs with external\ntools for accessing up-to-date information, reducing hal-\nlucination, multi-modal interactions, etc.\nProminent ex-\namples include ReAct [46], VisProg [10], Visual Chat-\nGPT [43], HuggingGPT [32], InternGPT [20], AutoGPT1,\nand Transformers Agent2. A distinctive trait of this line\nof research is its reliance on the zero-shot or few-shot\nin-context learning [8] capabilities inherent in LLMs [3].\nThese capabilities enable task decomposition, tool selec-\ntion, and parameter completion without requiring explicit\nfinetuning.\nHowever, due to the inherent limitations of\nLLMs, issues such as hallucination and challenges in ef-\nfective decomposition and deduction can arise with sub-\nstantial frequency. Furthermore, there are also instruction-\ntuning methods [11, 26, 27, 29, 31, 45]. Whereas allevi-\n1https://github.com/Significant-Gravitas/Auto-\nGPT\n2https : / / huggingface . co / docs / transformers /\ntransformers_agents\nating the above issues after being tuned on the text corpus\ninvolved tools, these methods are still limited at expanding\nthe toolset, i.e., additional training is required to add tools.\nAmong these methods, ToolLLM [29] proposes the depth\nfirst search-based decision tree to boost the planning ability\nof LLMs. However, it still has limitations similar to ToT, as\nshown in the Fig. 1.\nMulti-Modal LLMs. Developing LLMs that inherently\npossess multi-modal capabilities is another approach to ex-\ntending the usage boundary of LLMs for more complex\nreal-world scenarios [7, 13, 18, 19, 22, 23, 28, 44]. For\ninstance, BLIP-2 [17], LLava [18], and Mini-GPT4 [54]\nbind frozen image encoders and LLMs to enable the vision-\nlanguage understanding and generation.\nSimilarly, Vi-\nsionLLM [39] and LISA [14] empower the LLMs with\nthe visual perception capabilities such as object detection\nand segmentation. GILL [13], DreamLLM [9], and Mini-\nGPT5 [53] extend LLM for interleaved image and text\ngeneration by jointly optimizing the LLM with off-the-\nshelf Stable Diffusion model. Kosmos2 [28], Ferret [49],\nGPT4RoI [52], and etc., design various region-aware image\nencoders to augment LLMs with the abilities of grounding\nand referring. Nevertheless, these methods could only cover\na limited range of modalities or tasks and often require huge\neffects on model finetuning.\n3. ControlLLM\nThe prevalence of LLMs has unprecedentedly boosted the\ndevelopment of human-computer interaction. It is feasible\nto empower the LLMs with abilities to interact with various\nmodalities via tools. In response, we present an innova-\ntive framework, namely ControlLLM, characterized by its\nflexibility, and high performance. As depicted in Fig. 2, our\nframework consists of three sequential stages, i.e., task de-\ncomposition, task planning and solution execution. Next,\nwe will illustrate the design of each stage in detail.\n3.1. Task Decomposition\nControlLLM starts with task decomposition \u2013 a stage for de-\ncomposing the user request r into a list of parallel subtasks.\nWe here can utilize a language model M, e.g., ChatGPT or\ninstruction-tuned LLaMA, to automatically decompose the\nuser request as follows:\n{s0, ..., si, ..., sn} = M(r),\n(1)\nwhere si is the i-th subtask, n is the number of all sub-\ntasks. We will elaborate on the different choices of language\nmodel M in Sec. 3.4 and discuss their impacts in Sec. 4.5.\nThe result of task decomposition is JSON format, and the\noutput protocol is presented in Table 5.\nTask decomposition is different from task planning. It\nonly breaks down the user\u2019s request into several parallel\n3\n!\nCan you dub the video.mp4 with a\nsuitable background music and then\ngenerate a HTML web page for this\nvideo?\n\"\nI have designed the HTML web page \nto introduce the video  with light \nbackground music: \nStage 1: Task Decomposi2on !\nSubTask 1\nDescrip0on: Dub the video \nwith a background music.\nDomains: video-percepEon,\naudio-generaEon, video-\nediEng\nArgs: <video>\nReturn: <video>\nSubTask 2\nDescrip0on: Generate a \nHTML page for this video.\nDomains: video-percepEon, \nvideo-processing\nArgs: <video>\nReturn: <HTML>\nStage 3: Solu2on Execu2on\nOp0mal Solu0on\nvideo-capEoning\ntext-to-music\nvideo-Etling\nvideo-dubbing\n...\nvideo-tagging\nExecuEon Engine w/ Toolbox\nSummarize\nStage 2: Thoughts-on-Graph (ToG) #\ninput / output\ntools\nsoluEon path\nSearching op0mal solu0on on the tool graph\nvideo- \ncap+oning\ntext-to-\nmusic\nvideo-\ntagging\nvideo-+tling\nvideo- \ncap+oning\nvideo-\ndubbing\nvideo-to-\nwebpage\n\u2699\ninput\nvideo.mp4\noutput\n<HTML>\nFigure 2. System design of ControlLLM. The framework consists of three stages. The first stage is task decomposition, which parses the\nuser input into several subtasks. Then, in Stage 2, ToG utilizes a depth-first search algorithm to find the optimal solution for each subtask.\nThe execution engine in the last stage executes the solution and returns the output to users. We here use the example of generating a web\npage for the video to illustrate our method.\nsubtasks and summarizes the input resources for each sub-\ntask from the user request. It does not need to know what\ntools to use or how to use them. The objective of this stage\nis to achieve three aims. Firstly, it splits user requests into\nsmaller and more manageable units, i.e., subtasks, thereby\naccelerating task planning. Secondly, it seeks to determine\nthe task domain that is most relevant and appropriate for the\ngiven problem, thus further narrowing down the scope of\ntask planning. Thirdly, it endeavors to infer the input and\noutput resource types from the context, which identifies the\nstart and end nodes for ToG to search in the next stage.\n3.2. Task Planning with Thoughts-on-Graph\nThis stage is the key of the entire system. Given the results\nof task decomposition, we design a Thoughts-on-Graph\n(ToG) paradigm to find solutions on the graph heuristically.\n3.2.1\nBuilding the Tool Graph\nIn this stage, we embark on constructing a Tool Graph G by\nsimply using an adjacency matrix, which serves as a fun-\ndamental guideline for analyzing and optimizing the inter-\nactions between tools. Our motivation is driven by observ-\ning a discernible topological structure that inherently exists\nbetween the input and output of diverse tools, as demon-\nstrated in Fig. 2. This compelling insight propels us to craft\na comprehensive tool graph that encapsulates the inherent\nrelationship between tools.\nThere are two types of nodes i.e., Resource node and\nTool node, in the graph.\nResource node can be for-\nmally defined as one-tuple: \u27e8type\u27e9, where type rep-\nresents the specific type of resource, like image, mask,\nvideo, etc. Tool node can be expressed as a three-tuple:\n\u27e8desc, args, ret\u27e9. The desc field encapsulates the\ndescription of the tool, elucidating its purpose, methodol-\nogy, and intended applications. The args field represents\na list of resource nodes that the tool accepts, thereby giv-\ning the prerequisites for utilizing this tool. Finally, the ret\nfield designates the resource node that the tool returns. We\nelaborate on the definitions of resource types and tools in\nsupplementary material (Sec. 10).\nEdge Definitions. Edges in the tool graph intricately\nconnect the nodes, highlighting the relationships between\ndifferent tools. We define two types of edges in the graph.\n(1) Tool-resource edge is established from the tool to its\nreturned resource type. This signifies that the tool is capable\nof generating resources of the corresponding type. Mathe-\nmatically, a tool-resource edge is represented as:\nG(Tj, Ri) =\n(\ntrue ,\nif Ri equals to ret of Tj\nfalse,\notherwise ,\n(2)\n4\nAlgorithm 1 The Python pseudocode of depth-first solution\nsearch in Thoughts-on-Graph\nInput:\nt: subtask obtained by Eq. 1\ng: tool graph G constructed in Sec. 3.2.1\nr: available resources, initialized with subtask[\u201cargs\u201d]\ns: recorded tools during searching\nOutput:\nsolutions: all possible solutions for the subtask t\n1: function DFS SEARCH(t, g, r, s)\n2:\nif len(s) > m:\n3:\nreturn []\n# F finds all tool candidates, explained in Sec. 3.2.2\n4:\navailable tools = F(t, g, r)\n5:\nsolutions = []\n6:\nfor tool in available tools:\n7:\ns.append(tool)\n8:\nr.append(tool[\u201creturns\u201d])\n9:\nif tool[\u201creturns\u201d] == t[\u201creturns\u201d]:\n10:\nsolutions.append(s.copy())\n11:\nresults = DFS SEARCH(t, g, r, s)\n12:\nsolutions.extend(results)\n13:\nr.remove(tool[\u201creturns\u201d])\n14:\ns.remove(tool)\n15:\nreturn solutions\n\u25b7 Return\n16: end function\nwhere Tj is j-th tool node, Ri is i-th resource node, \u201ctrue\u201d\ndenotes two nodes are connected, and \u201cfalse\u201d denotes two\nnodes are disconnected.\n(2) Resource-tool edge denotes the resource node that\ncan be accepted as input arguments for its adjacent tool.\nThis connection indicates how the resources flow to the tool.\nThe resource-tool edge is mathematically defined as:\nG(Ri, Tj) =\n(\ntrue ,\nif Ri belongs to args of Tj\nfalse,\notherwise . (3)\nThrough the establishment of this graph, we can use di-\nverse search strategies to make informed decisions regard-\ning tool selection, and input resource assignments.\n3.2.2\nSearching on the Graph\nAs described in Algorithm 1, our ToG is built upon a depth-\nfirst search (DFS) algorithm where the tool selection func-\ntion F is used to sample the tool nodes on the tool graph.\nThe algorithm starts from the input resource nodes and ex-\nplores all possible paths to the output node while keeping\ntrack of the intermediate resources and tools along the way.\nThe algorithm stops when it reaches the expected output\nnode or when it exceeds a maximum length limit m (m=10\nby default). Finally, the algorithm returns all searched so-\nlutions as a list of tool sequences. Each step from resource\nnode to tool node represents a thought process, as it involves\na decision that determines whether to use this tool and how\nto assign its input arguments from available resources.\nTo find a trade-off between time and space complexities,\nwe develop a tool assessment module in which the language\nmodel is leveraged to score the tools in each search step and\nthen filter out some irrelevant tools. For details, please refer\nto Sec. 6.2 in supplementary material. With this assessment\nmodule, we design four search strategies for the function\nF to determine which tool nodes within the task domains to\nvisit among all adjacent nodes when searching on the graph:\nGreedy Strategy. This strategy selects the tool node\nwith the highest score at each step, where the score indi-\ncates the relevance of the tool to the task. A higher score\nindicates that the tool is more helpful for solving the task.\nGreedy search is fast and simple, but it may not find the\noptimal solution or even any solution at all.\nBeam Strategy. It only keeps the k best tools according\nto their assessment scores. Beam search can expand the\nsearch space but reduce the search efficiency slightly.\nAdaptive Strategy. This is a variant of beam search\nwhere it dynamically adjusts the beam size by choosing the\ntools with scores higher than a fixed threshold, which is a\ntrade-off between exploration and exploitation. It can widen\nthe search space when there are many available choices and\nnarrow down when there are few confident choices.\nExhaustive Strategy. This strategy explores all possible\npaths from the start node to the terminal node. The exhaus-\ntive search is guaranteed to find an optimal solution if one\nexists, but it may be very slow and consume a lot of com-\nputational resources during the search.\nThe impacts of different search strategies are studied in\nSec. 4.5. By initiating a systematic traversal of tool graph,\ncommencing at the \u201cargs\u201d nodes and culminating at the \u201cre-\nturn\u201d node, a diverse list of conceivable solutions is metic-\nulously synthesized. This process, akin to a brainstorm or\nmind map, represents the spectrum of potential solutions.\n3.2.3\nSolutions Post-processing\nAfter ToG searches the solutions, we design solution expert\nand resource expert to post-process solutions, which both\nbuild upon the language model M. Specifically, solution\nexpert to select the optimal one among all solution candi-\ndates and resource expert to infer the remaining arguments\nfor tools, respectively. The overall details are shown in sup-\nplementary material (Sec. 6.3 and 6.4).\n3.3. Solution Execution\nOnce the task solutions are completed, they are passed to the\nexecution engine obtain results, as shown in Fig. 2. During\n5\nthis stage, the execution engine initially parses the solutions\ninto a sequence of Actions. Each action is associated with\nparticular tool services, which could be implemented via\neither handcrafted mapping tables or an automatic sched-\nuler based on some strategies.\nDifferent from previous\nworks [20, 43, 45] that adopt static tool mapping, our de-\nsign empowers the system with the flexibility to schedule\ndiverse tools based on users\u2019 preferences.\nThe parsed actions are automatically executed by\nscheduling the action to the local, remote, or hybrid end-\npoints. Multiple independent subtasks would be executed in\nparallel to improve efficiency. Besides, we maintain a state\nmemory storing all the intermediate results, including their\nvalues and types. This enables the running-time automatic\ncorrection for the action parameters.\nResponse Generation. With all the execution results\nin hand, we could respond to the user requests. The un-\nprocessed results may lack comprehensiveness and clarity,\npotentially making it difficult for users to understand. To\nthis end, we introduce a module to aggregate all the exe-\ncution results and generate user-friendly responses. This\nis achieved by prompting the LLMs, such as ChatGPT, with\nthe user request, action list, and execution results and asking\nthem to summarize the answers intelligently. The prompt\ncan be found in supplementary material (Table 10).\n3.4. The Choices of Language Model\nOne feasible yet direct choice is to use off-the-shelf large\nlanguage models (LLMs) such as ChatGPT or Llama\n2 [37], which are pre-trained on large-scale text corpora\nand can handle various NLP tasks. These LLMs are read-\nily available. We design a series of elaborate prompts as\nshown in Sec. 6 for task decomposition, tool assessment,\nsolution expert, and resource expert. We call this variant as\nControlLLM-ChatGPT. In this way, we avoid training a lan-\nguage model from scratch. However, they may lead to low\nperformance as they are not trained for our requirements.\nThe alternative choice of M, termed as ControlLLM-\nLLaMA,\nis\nto\nfinetune\na\nlanguage\nmodel\n(e.g.,\nLLaMA [36]) by using self-instruct method [40].\nMore\ndetails of optimizing M can be referred to Sec. 7 in\nsupplementary material. The advantage of this variant is\nthat it can achieve high performance by adapting to the data\nand the task. Nevertheless, it requires lots of GPUs to train\nthe model and may suffer from overfitting issues.\nRegarding these choices, it is essential to carefully con-\nsider the trade-offs between readily available off-the-shelf\nLLMs with zero-shot capabilities and the potential for fine-\ntuning a model to achieve superior performance at the cost\nof computational resources. We will thus further discuss\nthe impacts of different language models M in Sec. 4.4 and\nexplore the optimal settings for our framework.\n4. Experiments\n4.1. Benchmark\nWe build a benchmark that is used to evaluate our proposed\nframework compared with other state-of-the-art methods.\nIn order to make fair comparisons, we only evaluate and\ntest on the intersection of toolsets from different meth-\nods [20, 32, 43, 45], all of which share comparable toolsets.\nThe benchmark consists of a set of tasks that require vari-\nous tools to solve complex problems collaboratively. It is\ndesigned to cover different task domains, such as question\nanswering, image generation, image editing, image percep-\ntion, visual question answering, etc. In this benchmark, the\ntasks involve more than 20 tools across different modalities.\nThis benchmark includes about 100 instructions which\nare classified into three levels of difficulty: easy (< 2 APIs),\nmedium (2 or 3 APIs), and hard(> 3APIs). We use test in-\nstructions with various levels to meticulously validate the\nability of different methods. We believe that this bench-\nmark can provide a comprehensive comparison of the tool\ncontrol capabilities of different methods. In Table 12, we\nshowcase some instruction samples from our benchmark. It\nis noticeable that there is no absolute relationship between\ndifficulty and length of instruction.\n4.2. Evaluation Protocol\nEffectively evaluating the performance of tool-augmented\nLLMs remains a challenging task. The challenges stems\nfrom several factors, including the inherent ambiguities in\nnatural language, the absence of shared benchmarks, and\nformatted solutions for systematically assessing different\nmethods. Consequently, existing methods [20, 32, 43, 45]\nprovide extensive case studies to validate the performance.\nWe found the APIs of tools in different methods are\nslightly inconsistent. It is hard to annotate all feasible so-\nlutions for each method. As such, we adopt an evaluation\nprotocol via a multi-person voting approach with three an-\nnotation experts. The protocol breaks down the evaluation\ninto three main aspects: tool selection, argument assign-\nment, and overall solution evaluation. Please note that the\nevaluation protocol is independent of the tools\u2019 capabilities.\nWhen the tools and their input arguments are correct, we do\nnot account for the case where the output fails to satisfy the\nuser\u2019s expectations due to the limitations of tools.\nMetrics for Tool Selection: A) Irrelevant Tool Inclu-\nsion Rate (abbr. IR): This metric gauges the performance\nof the method in excluding irrelevant tools. It measures the\nproportion of the predicted solutions that contain the irrele-\nvant tools. A higher IR indicates that the method tends to\ninclude more unnecessary tools, potentially hindering effec-\ntive task planning; B) Necessary Tool Inclusion Rate (abbr.\nNR): This metric assesses the inclusion of necessary tools\nin the predicted solution but without considering whether\n6\nFeatures\nControlLLM\n(our work)\nHuggingGPT\n[32]\nVisual ChatGPT\n[43]\nInternGPT\n[20]\nGPT4Tools\n[45]\nImage Perception\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nImage Editing\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nImage Generation\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nVideo Perception\n\u2713\n\u2713\n\u2717\n\u2713\n\u2717\nVideo Editing\n\u2713\n\u2713\n\u2717\n\u2713\n\u2717\nVideo Generation\n\u2713\n\u2713\n\u2717\n\u2713\n\u2717\nAudio Perception\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\nAudio Generation\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\nMulti-Solution\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\nPointing Device\n\u2713\n\u2717\n\u2717\n\u2713\n\u2717\nResource Type Awareness\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\nTable 1. Comparisons of features between different methods. The table shows that our framework supports more features that facilitate\nthe user experience of multi-modal interaction. It proves the high scalability of our framework.\nthe arguments of tools are correct. If NR is high, it indi-\ncates the method has strong capabilities in tool selection.\nMetrics for Argument Assignment: A) Resource Hal-\nlucination Rate (abbr. HR): This indicator reveals the ex-\ntent of hallucination in the method\u2019s responses when infer-\nring the arguments for tools. It measures whether all argu-\nments of the tools used in the predicted solution exist phys-\nically. A lower HR suggests that the method is less prone\nto generating hallucinated content. B) Resource Type Con-\nsistency Rate (abbr. CR): This metric examines whether\nthe types of input resources in the predicted solution match\nthose of the corresponding tools. It evaluates the method\u2019s\nability to ensure consistency of input types of tools.\nSolution Evaluation (abbr. SE) measures the success\nrate of all generated solutions on our benchmark. It only\nconsiders whether the output solution can effectively ad-\ndress the user\u2019s problem, irrespective of whether it contains\nirrelevant tools. A higher score in the solution evaluation\nindicates a stronger capability of task planning.\nIn summary, these intuitive metrics together provide a\ncomprehensive assessment of tool-augmented LLMs. The\nformal definitions of these metrics can refer to Sec. 11.\n4.3. Feature Comparisons\nTable 1 presents a comprehensive feature comparison\namong various methods [20, 32, 43, 45], highlighting Con-\ntrolLLM\u2019s distinct advantages in the landscape of multi-\nmodal interaction. Notably, \u201cMulti-Solution\u201d signifies the\nmethod\u2019s ability to provide multiple feasible solutions,\ngranting users more options. \u201cPointing Device\u201d signifies\nsupport for pointing devices such as the mouse, to enhance\nuser experience. \u201cResource Type Awareness\u201d indicates the\nmethod\u2019s capability to discern the type of resource in the\ncontext, ensuring more context-aware responses. In sum-\nmary, ControlLLM emerges as the standout choice, ex-\ncelling in various features. It offers a comprehensive set of\ntools in the domains of image, video, and audio. Moreover,\nits support for resource type awareness, multiple solutions,\nand pointing inputs demonstrates its adaptability and scala-\nbility, making it the highly versatile framework for diverse\nmulti-modal interaction scenarios.\n4.4. Quantitative Comparisons\nIn this section, we give a comprehensive analysis of Con-\ntrolLLM to compare with state-of-the-art methods, as sum-\nmarized in Table 2. We provide three implementations in\nsupplementary materials for our method: a) ControlLLM-\nChatGPT leverages the ChatGPT-3.5 as language model\nM; b) ControlLLM-LLaMA that finetunes a LLaMA-7B\nas a language model M; c) ControlLLM-Mix is regarded\nas our default setting, which finetunes LLaMA-7B as a task\ndecomposer in the first stage while the remaining modules\nemploy the ChatGPT to finish the tasks. ControlLLM-Mix\ncombines the advantages of the other two variants and is\nabbreviated as ControlLLM in the following sections.\nOur evaluation is based on a set of metrics assessing ef-\nfectiveness of task planning. ControlLLM excels in several\nkey aspects. Notably, it achieves the lowest Irrelevant Tool\nInclusion Rate (IR) as well as the highest Necessary Tool\nInclusion Rate, indicating its ability in effective yet efficient\ntask planning. Furthermore, ControlLLM demonstrates su-\nperior performance in argument assignment, with the lowest\nArgument Hallucination Rate (HR) of 0.02 and the highest\nArgument Type Consistency Rate (CR) of 0.98. These re-\nsults underscore its ability to generate accurate and consis-\ntent arguments, addressing a challenge in tool-augmented\nLLMs. In the solution evaluation, ControlLLM maintains\nits lead with a score of 0.93, indicating its effectiveness in\nresolving user requests. In summary, ControlLLM exhibits\nremarkable performance in all proposed metrics, evidently\n7\nMethods\nTool\nArgument\nSolution Evaluation \u2191\nIR \u2193\nNR \u2191\nHR \u2193\nCR \u2191\nAll\nEasy\nMedium\nHard\nHuggingGPT [32]\n0.45\n0.64\n0.16\n0.69\n0.59\n0.73\n0.50\n0.33\nVisual ChatGPT [43]\n0.26\n0.58\n0.09\n0.76\n0.57\n0.73\n0.63\n0.10\nInternGPT [20]\n0.12\n0.51\n0.49\n0.43\n0.44\n0.60\n0.46\n0.00\nGPT4Tools [45]\n0.19\n0.44\n0.28\n0.72\n0.43\n0.64\n0.33\n0.00\nControlLLM-ChatGPT\n0.16\n0.63\n0.83\n0.83\n0.64\n0.71\n0.67\n0.43\nControlLLM-LLaMA\n0.06\n0.95\n0.02\n0.98\n0.91\n0.98\n0.88\n0.76\nControlLLM-Mix\u2217\n0.03\n0.93\n0.02\n0.98\n0.93\n0.98\n0.96\n0.81\nTable 2. Comparisons with the state-of-the-art methods. \u2193 means the smaller the better, \u2191 means the larger the better. The results of\nstate-of-the-art methods [20, 32, 43, 45] are reproduced on our own benchmark. \u2217 denotes the default setting of ControlLLM if not stated.\nSearch\nStrategies\nTool\nArgument\nSolution Evaluation \u2191\nTime\nComplexities\nIR \u2193\nNR \u2191\nHR \u2193\nCR \u2191\nAll\nEasy\nMeduim\nHard\nGreedy\n0.19\n0.49\n0.24\n0.76\n0.49\n0.56\n0.58\n0.19\n4.07\nBeam (k = 3)\n0.14\n0.88\n0.01\n0.99\n0.88\n0.96\n0.79\n0.76\n121.29\nAdaptive\n0.03\n0.93\n0.02\n0.98\n0.93\n0.98\n0.96\n0.81\n236.49\nExhaustive\n0.06\n0.97\n0.01\n0.99\n0.97\n1.00\n0.96\n0.91\n3444.23\nTable 3. The evaluation for different search strategies. As introduced in Sec. 3.2.2, although exhaustive strategy achieves the best\nperformance on most metrics, the adaptive strategy strikes a good balance between efficiency and effectiveness. We count the average\nnumber of visited tools to denote the time complexities for different search strategies.\nTask\nDecomp.\nLLMs\nTool\nArgument\nSolution Evaluation \u2191\nIR \u2193\nNR \u2191\nHR \u2193\nCR \u2191\nAll\nEasy\nMeduim\nHard\nw/o PK\nLlama2-13B\n0.28\n0.71\n0.01\n0.99\n0.68\n0.87\n0.50\n0.38\nChatGPT-3.5\n0.13\n0.84\n0.01\n0.99\n0.83\n0.99\n0.67\n0.57\nGPT-4\n0.06\n0.91\n0.03\n0.97\n0.91\n0.98\n0.83\n0.81\nw/ PK\nLlama2-13B\n0.12\n0.83\n0.04\n0.95\n0.82\n0.95\n0.71\n0.62\nChatGPT-3.5\n0.03\n0.93\n0.02\n0.98\n0.93\n0.98\n0.96\n0.81\nGPT-4\n0.01\n0.98\n0.02\n0.98\n0.98\n1.00\n1.00\n0.91\nTable 4. The effects of task decomposition with regard to different LLMs. PK denotes \u201cprior knowledge\u201d. We find, if adding prior\nknowledge, such as which tools might be used, into the subtask description, the performance of task planning can be evidently improved.\noutperforming the state-of-the-art methods in this field.\n4.5. Ablation Studies\nTable 3 investigates the impact of different search strate-\ngies within our Thoughts-on-Graph. We observe that the\nexhaustive search strategy outperforms others in most met-\nrics, but this strategy is time-consuming. On the other hand,\nthe greedy search strategy achieves the lowest performance.\nBecause it can not search for a feasible path based on the\ntool with a high score due to inaccurate tool assessment.\nIt thus usually fails to find the solution, especially in com-\nplicated cases. In addition, the adaptive strategy strikes a\nbalance between performance metrics and time complexi-\nties, offering competitive results in most aspects. To trade-\noff between time and accuracy, we thus choose the adaptive\nstrategy as our default setting.\nIn Table 4, we conduct ablation studies to evalu-\nate the impact of different LLMs on task planning for\nControlLLM-Mix. We find language models plays a deci-\nsive role in tool selection. The more powerful the language\nmodel, the higher the score of solution evaluation. Fur-\nthermore, we investigate the effects of incorporating prior\nknowledge into the subtask descriptions. The method with-\nout prior knowledge usually directly uses the user\u2019s request\n8\nCase 1: Generate a new image similar to img_1.png.\nCase 2: Are there dogs in the img_2.png?\nimage- captioning\ntext-to- image\nedge-text-to-image\nseg-text-to-image\nsegment-anything\nimage-to-image\ninput: img_1.png\nimage-to-edge\nimage-to-image\nobject-detection\nselect-category\nselect-bbox\nobject-detection\nimage-classification\nVQA\noutput: <image>\nVQA\nhighlight-object\nimage-cropping\nControlLLM\nHuggingGPT\nControlLLM\nHuggingGPT\ninput: img_1.png\noutput: <image>\noutput: <image>\noutput: <image>\noutput: <image>\noutput: <image>\noutput: <image>\ninput: img_2.png\noutput: <category>\noutput: <text>\noutput: <text>\noutput: <image>\ninput: img_2.png\nFigure 3. Qualitative comparisons of task planning. We here use two simple cases to illustrate the differences between two different\nmethods in task planning. Here, each output node is generated by different solution paths.\nas a subtask description and does not offer any hints or\nsuggestions on tool selections in the subtask description.\nIn contrast, in the variant with prior knowledge, we add\nprior knowledge into the subtask description.\nThe prior\nknowledge indeed improves the necessary tool inclusion\nrate (NR) and reduces the chance of selecting irrelevant\ntools (IR) when using the same large language model.\n4.6. Qualitative Analyses\nFig. 3 shows two simple cases to illustrate the capabilities\nof our ControlLLM in task planning. In contrast to Hug-\ngingGPT [32], we find our method is able to generate more\ndiverse solutions to meet users\u2019 expectations, thanks to the\nThoughts-on-Graph paradigm. In Sec. 12, we provide ex-\ntensive case studies across different modalities to validate\nthe user experience for our method in practice.\n5. Conclusion\nIn this paper, we propose ControlLLM, a multi-modal in-\nteraction framework that can accurately control tool usage\nacross various domains, including text, image, audio, video,\netc. The proposed framework consists of three key stages:\n(1) task decomposition to concrete the objective of the task,\n(2) a Thoughts-on-Graph (ToG) paradigm to search the op-\ntimal solution path on the constructed tool graph, (3) and an\nexecution engine with a versatile toolbox to execute solution\nefficiently. We conduct extensive experiments and demon-\nstrate that our ControlLLM achieves superior performance\nregarding tool selection, argument assignment, and overall\nsolution effectiveness compared to existing methods.\nNevertheless, this work still has some limitations. Since\nthe goal of this work is to improve the accuracy of tool us-\nage, even if the solution is theoretically feasible, we cannot\nguarantee that the output from tools is always correct. On\nthe other hand, due to the inherent ambiguity in natural lan-\nguage, it is difficult to ensure that the optimal solution se-\nlected is consistent with the user\u2019s goal. In this case, we can\nonly provide more alternative solutions searched by ToG for\nusers to choose from if the optimal solution fails.\n9\nReferences\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Cheb-\notar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu,\nKeerthana Gopalakrishnan, Karol Hausman, et al. Do as i\ncan, not as i say: Grounding language in robotic affordances.\narXiv preprint arXiv:2204.01691, 2022. 1\n[2] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gersten-\nberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann,\nMicha\u0142 Podstawski, Hubert Niewiadomski, Piotr Nyczyk,\nand Torsten Hoefler. Graph of Thoughts: Solving Elaborate\nProblems with Large Language Models, 2023. 3\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020. 3\n[4] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing.\nVicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality, 2023. 2\n[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.\nPalm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 3\n[6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al.\nScaling\ninstruction-finetuned language models.\narXiv preprint\narXiv:2210.11416, 2022. 3\n[7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning,\n2023. 3\n[8] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\nWu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang\nSui.\nA survey for in-context learning.\narXiv preprint\narXiv:2301.00234, 2022. 3\n[9] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, et al. Dreamllm: Synergistic multimodal com-\nprehension and creation. arXiv preprint arXiv:2309.11499,\n2023. 3\n[10] Tanmay Gupta and Aniruddha Kembhavi. Visual program-\nming: Compositional visual reasoning without training. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 14953\u201314962, 2023. 3\n[11] Shibo Hao,\nTianyang Liu,\nZhen Wang,\nand Zhiting\nHu.\nToolkengpt:\nAugmenting frozen language models\nwith massive tools via tool embeddings.\narXiv preprint\narXiv:2305.11554, 2023. 3\n[12] Namgyu Ho, Laura Schmid, and Se-Young Yun.\nLarge\nlanguage models are reasoning teachers.\narXiv preprint\narXiv:2212.10071, 2022. 3\n[13] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-\nerating images with multimodal language models.\narXiv\npreprint arXiv:2305.17216, 2023. 3\n[14] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation\nvia large language model. arXiv preprint arXiv:2308.00692,\n2023. 3\n[15] Jean-Claude Latombe. Robot motion planning. Springer Sci-\nence & Business Media, 2012. 3\n[16] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In In-\nternational Conference on Machine Learning, pages 12888\u2013\n12900. PMLR, 2022. 1\n[17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 1, 3\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In NeurIPS, 2023. 3\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 2, 3\n[20] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi\nWang, Shoufa Chen, Qinglong Zhang, Zeqiang Lai, Yang\nYang, Qingyun Li, Jiashuo Yu, et al.\nInterngpt: Solving\nvision-centric tasks by interacting with chatbots beyond lan-\nguage. arXiv preprint arXiv:2305.05662, 2023. 1, 2, 3, 6, 7,\n8\n[21] Lei Ma, Jincong Han, Zhaoxin Wang, and Dian Zhang.\nCephgpt-4: An interactive multimodal cephalometric mea-\nsurement and diagnostic system with visual large language\nmodel. arXiv preprint arXiv:2307.07518, 2023. 1\n[22] Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar\nNagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh,\nPrakash Murugesan, Peyman Heidari, Yue Liu, et al. Any-\nmal: An efficient and scalable any-modality augmented lan-\nguage model. arXiv preprint arXiv:2309.16058, 2023. 3\n[23] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang,\nMingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao,\nand Ping Luo.\nEmbodiedgpt:\nVision-language pre-\ntraining via embodied chain of thought.\narXiv preprint\narXiv:2305.15021, 2023. 3\n[24] OpenAI. Chatgpt (Mar 14 version) [large language model].\n6, 2023. 1\n[25] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al.\nTraining lan-\nguage models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022. 3\n[26] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool aug-\nmented language models. arXiv preprint arXiv:2205.12255,\n2022. 3\n[27] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E\nGonzalez. Gorilla: Large language model connected with\nmassive apis. arXiv preprint arXiv:2305.15334, 2023. 3\n10\n[28] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 3\n[29] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,\nYaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian,\net al. Toolllm: Facilitating large language models to master\n16000+ real-world apis. arXiv preprint arXiv:2307.16789,\n2023. 1, 3\n[30] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Milli-\ncan, Jordan Hoffmann, Francis Song, John Aslanides, Sarah\nHenderson, Roman Ring, Susannah Young, et al. Scaling\nlanguage models: Methods, analysis & insights from train-\ning gopher. arXiv preprint arXiv:2112.11446, 2021. 3\n[31] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-\ncedda, and Thomas Scialom. Toolformer: Language mod-\nels can teach themselves to use tools.\narXiv preprint\narXiv:2302.04761, 2023. 1, 3\n[32] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang.\nHugginggpt: Solving\nai tasks with chatgpt and its friends in huggingface. arXiv\npreprint arXiv:2303.17580, 2023. 1, 2, 3, 6, 7, 8, 9, 14, 16\n[33] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis\nAntonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lu-\ncas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the\ngame of go without human knowledge. nature, 550(7676):\n354\u2013359, 2017. 3\n[34] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao\nLiang, and Le Sun. Toolalpaca: Generalized tool learning for\nlanguage models with 3000 simulated cases. arXiv preprint\narXiv:2306.05301, 2023. 1\n[35] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,\nXuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.\nHashimoto.\nStanford alpaca:\nAn instruction-following\nllama model. https://github.com/tatsu-lab/\nstanford_alpaca, 2023. 16\n[36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aure-\nlien Rodriguez, Armand Joulin, Edouard Grave, and Guil-\nlaume Lample. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023. 1, 3,\n6, 13, 16\n[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 1, 6\n[38] Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish\nKapoor. Chatgpt for robotics: Design principles and model\nabilities. Microsoft Auton. Syst. Robot. Res, 2:20, 2023. 1\n[39] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,\nXizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu\nQiao, et al.\nVisionllm: Large language model is also an\nopen-ended decoder for vision-centric tasks. arXiv preprint\narXiv:2305.11175, 2023. 1, 3\n[40] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed\nChi, Sharan Narang, Aakanksha Chowdhery, and Denny\nZhou. Self-consistency improves chain of thought reason-\ning in language models. arXiv preprint arXiv:2203.11171,\n2022. 2, 3, 6\n[41] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in Neural Information Processing\nSystems, 35:24824\u201324837, 2022. 1, 2, 3\n[42] Sholom M Weiss, Casimir A Kulikowski, Saul Amarel, and\nAran Safir. A model-based method for computer-aided med-\nical decision-making.\nArtificial intelligence, 11(1-2):145\u2013\n172, 1978. 3\n[43] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,\nZecheng Tang, and Nan Duan.\nVisual chatgpt: Talking,\ndrawing and editing with visual foundation models. arXiv\npreprint arXiv:2303.04671, 2023. 1, 2, 3, 6, 7, 8\n[44] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng\nChua. Next-gpt: Any-to-any multimodal llm. arXiv preprint\narXiv:2309.05519, 2023. 3\n[45] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu\nLi, and Ying Shan.\nGpt4tools: Teaching large language\nmodel to use tools via self-instruction, 2023. 1, 2, 3, 6, 7,\n8\n[46] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,\nKarthik Narasimhan, and Yuan Cao.\nReact: Synergizing\nreasoning and acting in language models.\narXiv preprint\narXiv:2210.03629, 2022. 1, 3\n[47] Shunyu Yao,\nDian Yu,\nJeffrey Zhao,\nIzhak Shafran,\nThomas L Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: Deliberate problem solving with large lan-\nguage models. arXiv preprint arXiv:2305.10601, 2023. 1,\n2\n[48] Yao Yao, Zuchao Li, and Hai Zhao.\nBeyond chain-of-\nthought, effective graph-of-thought reasoning in large lan-\nguage models. arXiv preprint arXiv:2305.16582, 2023. 3\n[49] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen\nZhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and\nYinfei Yang. Ferret: Refer and ground anything anywhere\nat any granularity, 2023. 3\n[50] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu\nLai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao\nXia, et al. Glm-130b: An open bilingual pre-trained model.\narXiv preprint arXiv:2210.02414, 2022. 3\n[51] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao.\nLlama-adapter:\nEfficient fine-tuning of language models\nwith zero-init attention. arXiv preprint arXiv:2303.16199,\n2023. 1\n[52] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In-\nstruction tuning large language model on region-of-interest.\narXiv preprint arXiv:2307.03601, 2023. 3\n[53] Kaizhi Zheng, Xuehai He, and Xin Eric Wang.\nMinigpt-\n5: Interleaved vision-and-language generation via generative\nvokens. arXiv preprint arXiv:2310.02239, 2023. 3\n11\n[54] Kaizhi Zheng, Xuehai He, and Xin Eric Wang.\nMinigpt-\n5: Interleaved vision-and-language generation via generative\nvokens, 2023. 3\n[55] Denny Zhou, Nathanael Sch\u00a8arli, Le Hou, Jason Wei, Nathan\nScales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier\nBousquet, Quoc Le, et al. Least-to-most prompting enables\ncomplex reasoning in large language models. arXiv preprint\narXiv:2205.10625, 2022. 3\n12\nControlLLM: Augment Language Models with Tools by Searching on Graphs\nSupplementary Material\n6. ControlLLM-ChatGPT\nIn this variant, we implement language model M with\nChatGPT. As such, we elaborately design a series of\nprompts for each module.\n6.1. Task Decomposition\nThe prompt in Table 6 is designed for task decomposition\nin ControlLLM-ChatGPT. It guides the ChatGPT to decom-\npose the user request into several subtasks. Table 5 shows\nthe output protocol of task decomposition.\n6.2. Tool Assessment\nTable 7 outlines a prompt design for the task of tool assess-\nment, where the AI assistant evaluates tools\u2019 suitability for\na given task. The output is captured in the JSON format, in-\ncluding reasons and a score. The scoring criteria range from\n1 to 5, reflecting the tool\u2019s relevance to the task. The prompt\nemphasizes the connection between tool descriptions and\ntask requirements. This prompt guides AI in making in-\nformed decisions when assessing tools\u2019 utility for a specific\ntask.\n6.3. Solution Expert\nIn this section, we delve into the core concept of the solution\nexpert that streamlines the process of evaluating and select-\ning optimal solutions from all possible candidates. By sys-\ntematically converting each solution into a formatted string\ndescription, the solution expert enables us to make informed\ndecisions based on evaluated scores.\nSolution Description Formatting. To facilitate the so-\nlution expert to comprehend the solution, we need to gen-\nerate the description for each solution candidate. This in-\nvolves transforming raw solution data into structured, for-\nmatted string descriptions. These descriptions encapsulate\nthe information including functionality, inputs and output.\nSolution Evaluation.\nThe solution expert capitalizes\non prompt engineering techniques to assess each solution\nbased on subtask descriptions and formatted solution de-\nscriptions. The designed prompts guide language model\nM to evaluate the feasibility of each solution against the\nobjective of the subtask. Through this process, we can as-\nsign scores to solutions, gauging their effectiveness and rel-\nevance to the task. It must ensure that the evaluation pro-\ncess is focused, targeted, and aligned with the subtask. The\nprompt template is shown in the Table 8.\nSolution Ranking. The final aim of this module is to\nselect the top-performing solutions. The optimal solution is\nidentified as the highest score assessed in the solution eval-\nuation. Given that sometimes the selected optimal solution\nmay not meet the user requirements, we also provide several\nalternative solutions by setting a threshold score of 3. These\nsolutions, which exhibit a higher degree of alignment with\nthe subtask\u2019s requirements, emerge as the most promising\ncandidates for user preference.\nThrough collaborative efforts, the optimal solution ex-\npert ensures that solutions are appropriately tailored, opti-\nmized, and well-adapted to the task.\n6.4. Resource Expert\nIn the algorithm of ToG, we encounter a challenge stem-\nming from the potential presence of multiple instances of\nthe same resource type within the available resource list.\nThis challenge introduces complexity, making it difficult to\nstraightforwardly deduce certain arguments for tools using\npredefined rules. As a result, we design a solution expert.\nThis module transforms the task of argument assignment\ninto a fill-in-the-blank exercise. To achieve this, we de-\nsign a resource expert crafts with prompts that not only\nincorporate the task description but also include the avail-\nable resource list. In this manner, a language model M is\nemployed to dynamically complete the missing parameters\nwithin a solution by interacting with the contextual informa-\ntion presented. We put the prompt template in the Table 9.\n7. ControlLLM-LLaMA\nFor ControlLLM-LLaMA, we use the LLaMA-7b [36] as\nlanguage model M to solve the problems in task decom-\nposition, tool assessment, solution expert, resource expert.\n7.1. Instruction Generation\nThe first step to train M is to construct the instruction cor-\npus. We here opt for ChatGPT-3.5 to generate the training\ncorpus. The following steps will elaborate on the details of\ninstructions generation for task decomposition, tool assess-\nment, solution expert, and resource expert, respectively.\nFor task decomposition, we generate two different types\nof instructions as follows: 1) Basic instructions, where they\nonly contain one subtask after task decomposition. We set\nsome seed instructions with ground-truth results of task de-\ncomposition, which serve as initial templates for generat-\ning more diverse instructions. Then, we use ChatGPT to\ngenerate more diverse instructions based on the pre-defined\nseed instructions. During the generation process, we cen-\nter on the seed instructions and produce more instructions\n13\nField\nDescription\ndescription\na brief summary of subtask. It gives some guidance on how to approach the problem for ToG.\ndomains\nthe domain scope that tools required by this task fall into. It helps ToG narrow down the search space and\nfind the most relevant and suitable tools for the subtask. We showcase some domains in Table 11.\nargs\nthe inputs that the user provides for this subtask. It is usually in the form of key-value pairs, where the key is\nthe type of the argument, and the value is the resource path or text you want to use. For example, [{\u201ctype\u201d:\n\u201cimage\u201d, \u201cvalue\u201d: \u201cimage 1.png\u201d}, {\u201ctype\u201d: \u201ctext\u201d, \u201cvalue\u201d: \u201cremove the dog in the picture\u201d}].\nreturn\nthe expected output of the subtask. For example, the return is {\u201cimage\u201d: \u201c\u27e8GEN\u27e9-0\u201d}, which means the\nexpected output is an image and \u201c\u27e8GEN\u27e9-0\u201d is just a temporary placeholder.\nTable 5. The output protocol of task decomposition. We elaborate on each field in the output of task decomposition.\nThe following is a friendly conversation between a human and an AI. The AI is professional and parses user input to\nseveral tasks with lots of specific details from its context. If the AI does not know the answer to a question, it truthfully\nsays it does not know. The AI assistant can parse user input to several tasks with JSON format as follows: <Solu-\ntion>[\u201cdescription\u201d: task description, \u201ctask\u201d: [task domain 1, task domain 2], \u201cid\u201d: task id, \u201cdep\u201d: dependency task id,\n\u201cargs\u201d: [\u201ctype\u201d: \u201ctext\u201d, \u201cimage\u201d or audio, \u201cvalue\u201d: text, image url or <GEN>-dep id], \u201creturns\u201d:[\u201ctype\u201d: \u201csegmen-\ntation\u201d, \u201cvalue\u201d: \u201c<GEN>-task id\u201d]]</Solution>. The \u201ddescription\u201d should describe the task in detail, and AI assis-\ntant can add some details to improve the user\u2019s request without changing the user\u2019s original intention. The special tag\n\u201c<GEN>-dep id\u201d refers to the one generated text/image/audio/video/segmentation/mask in the dependency task (Please\nconsider whether the dependency task generates resources of this type.) and \u201cdep id\u201d must be in \u201cdep\u201d list. The special\ntag \u201c<GEN>-task id\u201d refers to the one generated text/image/audio/video/segmentation/mask in this task and \u201ctask id\u201d\nshould be in line with field \u201cid\u201d of this task. The \u201cdep\u201d field denotes the ids of the previous prerequisite tasks, which\ngenerate a new resource that the current task relies on. The \u201cargs\u201d field and the \u201creturns\u201d field denotes the input resources\nand output resources of this task, respectively. The type of resource must be in [\u201ctext\u201d, \u201cimage\u201d, \u201cline\u201d, \u201cnormal\u201d, \u201ched\u201d,\n\u201cscribble\u201d, \u201cpose\u201d, \u201cedge\u201d, \u201cbbox\u201d, \u201ccategory\u201d, \u201csegmentation\u201d, \u201caudio\u201d, \u201cvideo\u201d, \u201csegmentation\u201d, \u201cmask\u201d], nothing else.\nThe \u201ctask\u201d MUST be selected from the following options: \u201cquestion-answering\u201d, \u201cvisual-question-answering\u201d, \u201cimage-\ngeneration\u201d, \u201cimage-editing\u201d, \u201cimage-perception\u201d, \u201cimage-processing\u201d, \u201caudio-perception\u201d, \u201caudio-generation\u201d, \u201caudio-\nediting\u201d, \u201cvideo-question-answering\u201d, \u201cvideo-perception\u201d, \u201cvideo-generation\u201d, \u201cvideo-editing\u201d, nothing else. Think step\nby step about all the tasks that can resolve the user\u2019s request. Parse out as few tasks as possible while ensuring that the\nuser request can be resolved. Pay attention to the dependencies and order among tasks. If some inputs of tools are not\nfound, you cannot assume that they already exist. You can think of a new task to generate those args that do not exist or\nask for the user\u2019s help. If the user request can\u2019t be parsed, you need to reply empty JSON []. You should always respond\nin the following format:\n<Solution><YOUR SOLUTION></Solution>\n<YOUR SOLUTION>should be strict with JSON format described above.\nTable 6. The prompt for task decomposition. It is inspired by [32].\nusing more diverse expressions and styles. These instruc-\ntions need to share the task decomposition results with the\nseed instructions as ground truth. 2) Compound instruc-\ntions, which involve multiple subtasks and intermediate re-\nsources. We simply assemble the basic instructions into the\ncompound instructions in a coherent and logical manner. It\naims to enhance the improve the complex interaction capa-\nbility of the system by enabling the model to handle user\nrequests that span multiple domains and require multiple\nsteps of processing. We here generate almost 100k instruc-\ntions for training. The instructions generated in this step\nwill be used in the following tasks as well.\nFor the tool assessment, solution expert, and resource ex-\npert, we use prompts in Table 7, 8 and 9 to collect the\noutput from ChatGPT by running ControlLLM on the in-\nstructions generated above. Unlike directly generating the\nsolution, these tasks only involve making a decision, like\nscoring the tools or solutions based on the input, so they are\nrelatively simple, and ChatGPT with strong zero-shot capa-\nbilities, can easily solve them. Therefore, we opt to directly\ndistill the knowledge of ChatGPT by using prompt tech-\nniques. Through the experiments, we verify the feasibility\nof this strategy.\n14\nGiven a task and a tool, the AI assistant helps the system decide whether this tool can process the task. The assis-\ntant should focus more on the description of the model and give a score to each tool. The AI assistant respond with\nJSON format as follows: <Solution>\u201cThought\u201d: thought, \u201cScore\u201d: score </Solution>. The \u201cThought\u201d field records the\nmodel\u2019s thinking process step by step within 80 words, which gives the reasons why giving this score. The \u201cScore\u201d field\ndenotes a score that assesses whether this tool is useful for this task. Score is in [1, 2, 3, 4, 5]. Here are the scoring\ncriteria: \u201cScore\u201d=1: The tool is totally not related to the task and does not provide any useful output for solving the\ntask. \u201cScore\u201d=2: The tool is somewhat not related to the task and may not provide any useful output for solving the\ntask. \u201cScore\u201d=3: The tool is probably related to the task and provides some intermediate output that is partially helpful\nfor solving the task, but it may not be the optimal one. \u201cScore\u201d>3: The tool is closely or directly related to the task\nand provides an output that is mostly helpful for solving the task or that matches the returns of the task with regard\nto the type. In a nutshell, for the given task, the higher the score, the more useful the tool is. You should always re-\nspond in the following format: <Solution>SOLUTION </Solution>. \\n\u2018SOLUTION\u2018 should strictly comply with the\nSON format described above. Task description: \u201c{{task}}\u201d.\\n\\n Here is the description of the tool \u201c{{tool name}}\u201d:\n\\n{{tool name}}: {{tool description}}\\nArgs: \\n{{arguments}}\\nReturns: \\n{{returns}}\\n\\nThe above information\nmay be useful for AI to make decision. Please refer to the scoring criteria and score the tool {{tool name}} for this task.\nNotice that If the tool description contains keywords from the task description, the score of this tool should be greater\nthan or equal to 3.\nTable 7. The prompt for tool assessment.\nGiven a task and a solution, The AI assistant needs to score the solution and respond in JSON format. Please notice that\nthe AI assistant should think. The AI assistant should pay more attention to the relevance between the description of each\ntool in the solution and task. The AI assistant respond with JSON format as follows: <Solution>{\u201cThought\u201d: \u201cthought\u201d,\n\u201cScore\u201d: score}</Solution>. \u201cThought\u201d field records the model\u2019s thinking process step by step within 80 words, which\ngives the reasons why giving this score. \u201cScore\u201d field denotes a score that assesses whether this tool is helpful for this\ntask. \u201cScore\u201d is in [1, 2, 3, 4, 5]. Here are the scoring criteria: \u201cScore\u201d=1: The solution is totally not related to the user\u2019s\nrequest and can not solve the task. \u201cScore\u201d=2: The solution is somewhat not related to the user\u2019s request and may not\nsolve the task. \u201cScore\u201d=3: The solution is probably related to the user\u2019s intention and may solve the task, but it may not\nbe the optimal one. \u201cScore\u201d>3: The solution is closely or directly related to what the user wants and could satisfactorily\nsolve the task. In a nutshell, the higher the score, the greater the likelihood of the solution solving the given task. You\nshould always respond in the following format: \\n<Solution>\u2018SOLUTION\u2018 </Solution>\\n\u2018SOLUTION\u2018 should strictly\ncomply with the JSON format described above. \\nUser\u2019s request: \u201c{{request}}\u201d Task description: \u201c{{task}}\u201d. Here is\nthe description of the solution: {{solution}} Please refer to the scoring criteria and score this solution based on the task\ndescription. You should think carefully before scoring the solution. Notice that If the keywords in the solution are close\nin meaning to the keywords in the task description, then the score of this solution is at least 3.\nTable 8. The prompt for solution expert.\nSome tools have missing input arguments, and the AI assistant needs to infer these missing inputs from the con-\ntext.\nPlease notice that the AI assistant should never fake the resources that do not exist.\nThe returned input ar-\nguments should be JSON format as follows: [{\u201cimage\u201d: \u201cxxx.png\u201d}, {\u201cbbox\u201d: \u201c<GEN>-detr-bbox-0\u201d}, \u201ctext\u201d:\n\u201c<text>\u201d].\nAI assistant should always respond in the following format: \\n\u201c<Explanation>[briefly explain your\nchoice here]</Explanation><Solution>\u2018SOLUTION\u2018 </Solution>\u201d. \\n\u2018SOLUTION\u2018 should be strictly in the JSON\nformat described above.\n\\nUser\u2019s request: \\n\u201c{{request}}\u201d\\nTask: \\n\u201c{{task description}}\u201d.\n\\n<Resources>:\n\\n{{resources}}.\n\\nWe use {{tool name}} to solve this task: \\n\u2018{{tool name}}\u2018: {{tool description}} \\nArgs:\n{{arguments}} \\nReturns: {{returns}} \\nFor the type of \u201ctext\u201d, the AI assistant should summarize the content from\nthe context based on the task and the tool\u2019s description. For other types of input, the AI assistant needs to select the inputs\nfrom <Resources>. Now we prepare the inputs for {{tool name}}: {{input}}. Please complete these inputs and return\nthe completed inputs with the format described above like: <Solution>\u2018SOLUTION\u2018 </Solution>.\nTable 9. The prompt for resource expert.\n15\nYour name is ControlLLM, an AI-powered assistant. For user\u2019s request, the system executes the solution and collects the\nresults based on the following workflow. You need to respond to user requests based on the following information. Here\nare the information for your reference.\n## User Request\n{{request}}\n## Workflow and Execution Results\n{{solution}}\n## Summarized Results\n{{results}}\nYou must first answer the user\u2019s request in a straightforward manner. Some of the results may not be accurate and need\nyou to use your judgment in making decisions. Then please explain your workflow, including the tools and returned\nresults for the request, in a friendly way. If the answers contain file paths, you have to repeat the complete file path. Only\nif there is nothing in the Summarized Results, you need to tell the user you can not finish the task.\nTable 10. The Prompt Design in Response Generation. We here refer to the prompts from [32] to generate a user-friendly response.\n7.2. Training Recipes\nWe follow the training protocol in [35], where LLaMA [36]\nis used as an alternative choice for our language model\nM. It is finetuned for three epochs with the initial learning\nrate 2e-5 and consine decay. We fix the training batch size\nas 128 by adaptively setting \u201cgradient accumulation steps\u201d.\nThe whole training procedure is on 8xA100 GPUs.\n8. ControlLLM-Mix\nIn practice, we find ControlLLM-ChatGPT has difficulty in\ntask decomposition, especially for hard instructions. In ad-\ndition, ControlLLM-LLaMA is good at task decomposition\ndue to an instruction-tuned language model M while other\nmodules are slightly inferior to ChatGPT. Because we fine-\ntune M by distilling the knowledge from ChatGPT to as-\nsess tools, ranking solutions, and assign arguments. As a\nresult, we design ControlLLM-Mix to integrate the bene-\nfits from the other variants. For task decomposition, we\nuse the same method from ControlLLM-LLaMA to finetune\nLLaMA-7B to decompose user requests. For the remain-\ning modules, we directly utilize the ChatGPT, sharing the\nsame prompt design from ControlLLM-ChatGPT. In exper-\niments, ControlLLM-Mix achieves the best performance.\n9. Response Generation\nWe design a prompt template for the Response Generation\ntask in Table 10. In this task, the AI assistant is tasked\nwith explaining the process and outcomes using input and\ninference results. The AI is instructed to respond directly to\nthe user\u2019s request, followed by describing the task\u2019s proce-\ndure, offering analysis, and presenting model inference re-\nsults using a first-person perspective. If the results involve\nfile paths, the complete path should be provided, or if there\nare no results, the AI should communicate its inability. The\nprompt sets the context for generating informative and user-\nunderstandable responses during response generation.\n10. Resource Types and Tools\nWe initially define a type set containing a series of resource\ntype descriptors, such as \u201ctext\u201d, \u201ctags\u201d, \u201chtml\u201d, \u201cimage\u201d,\n\u201cvideo\u201d, \u201caudio\u201d, \u201csegmentation\u201d, \u201cedge\u201d, \u201cline\u201d, \u201ched\u201d,\n\u201ccanny\u201d, \u201cscribble\u201d, \u201cpose\u201d, \u201cdepth\u201d, \u201cnormal\u201d, \u201cmask\u201d,\n\u201cpoint\u201d, \u201cbbox\u201d and \u201ccategory\u201d. The type set is easy to ex-\ntend depending on the toolkit. The types of inputs of tools\nmust fall into the pre-defined type set.\nIn Table 11, we exhibit lots of tools supported by our\nframework across different domains, including natural lan-\nguage, image, audio, video, etc. The whole system will\ncontinue to evolve.\n11. Metrics for Tool Selection\nA) Irrelevant Tool Inclusion Rate (abbr. IR):\nF(sp) =\n(\ntrue,\nsp contains the irrelevant tools\nfalse,\notherwise\n,\n(4)\nIR =\nP|Sp|\ni\nI(F(Sp\ni ))\n|Sp|\n,\n(5)\nwhere I is indicator function, | \u00b7 | represents the number of\nelements in a set, and Sp denotes all predicted solutions\non our benchmark. It measures the proportion of the pre-\ndicted solutions that contain the irrelevant tools. A higher\nIR indicates that the method tends to include more unnec-\nessary tools, potentially hindering effective task planning.\nThis metric gauges the performance of methods in exclud-\ning irrelevant tools.\nB) Necessary Tool Inclusion Rate (abbr. NR):\nH(sp) =\n(\ntrue,\nSolution sp contains necessary tools\nfalse,\notherwise\n,\n(6)\n16\nDomains\nTools\nquestion-answering\nquestion answering, image question answering\nnatural-language-processing\nsummarization, title generation, text to tags, text to text generation, sentiment analysis\nimage-perception\nobject detection,\nimage captioning,\nvisual grounding,\nimage classification,\nseg-\nment anything, instance segmentation, segment by points\nimage-generation\ntext to image,\nimage to image,\nline text to image,\nhed text to image,\nscrib-\nble text to image, pose text to image, segmentation text to image, edge text to image,\ndepth text to image, normal text to image\nimage-editing\ntext image editing,\nimage inpainting,\nimage cropping,\nmask image,\nhigh-\nlight object on image\nimage-processing\nimage to edge, image to line, image to hed, image to scribble, image to pose, im-\nage to depth, image to normal\nvideo-perception\nvideo classification, video captioning\nvideo-processing\ndub video, video to webpage\nvideo-generation\nimage audio to video, image to video, text to video\naudio-perception\naudio classification\naudio-generation\ntext to music, text to speech, audio to Audio\nTable 11. A subset of domains and tools in our tool kit.\nNR =\nP|Sp|\ni\nI(H(Sp\ni ))\n|Sp|\n.\n(7)\nThe necessary tools play a critical role in solving the user\nrequest. For example, if users want to know the position of\na specific object, the object detection tool is necessary. This\nmetric measures the proportion of solutions that contain the\nnecessary tools for solving the task but without consider-\ning whether the arguments of tools are correct. It checks\nwhether the solution has all the necessary tools that can pro-\nduce the expected output. A high NR value means that the\nmethod has a strong ability in task planning and finding the\nright tools for the user\u2019s request.\n11.1. Metrics for Argument Assignment\nA) Resource Hallucination Rate (abbr. HR):\nP(sp) =\n(\ntrue,\nspcontains false resources\nfalse,\notherwise\n,\n(8)\nHR =\nP|Sp|\ni\nI(P(Sp\ni ))\n|Sp|\n.\n(9)\nThis indicator reveals the extent of hallucination when in-\nferring the arguments for tools. It checks whether all the\narguments of the tools exist in the input resources or not. A\nlow HR value means that the method rarely leads to hallu-\ncinations that are common in LLMs.\nB) Resource Type Consistency Rate (abbr. CR):\nQ(sp) =\n(\ntrue,\nNo resource type conflict in sp\nfalse,\notherwise\n, (10)\nCR =\nP|Sp|\ni\nI(Q(Sp\ni ))\n|Sp|\n.\n(11)\nThis metric examines whether the types of resources used\nas inputs for the predicted solution match those of the cor-\nresponding tools. It evaluates the method\u2019s ability to ensure\nconsistency between argument types and tools. A high CR\nvalue means that the method can correctly infer and assign\narguments for each tool.\n11.2. Solution Evaluation\nThe Solution Evaluation (abbr. SE):\nW(sp) =\n(\ntrue,\nsp can solve the task\nfalse,\notherwise\n,\n(12)\nSE =\nP|Sp|\ni\nI(W(Sp\ni ))\n|Sp|\n.\n(13)\nThis metric measures the success rate of all generated so-\nlutions on our benchmark, regardless of whether it contains\nirrelevant tools, as long as the chain of tool invoking outputs\nthe information that is able to solve the task. A higher score\nin the solution evaluation indicates that the method is able\nto provide an effective solution to user requests.\nIn summary, these intuitive metrics together provide\na comprehensive assessment of tool-augmented LLMs in\nterms of tool selection, argument assignment, and overall\neffectiveness in addressing user queries.\n17\nEasy\n1. Please determine if the image 1.png contains a platyhelminth?\n2. How can I design a sleep monitoring system in C# that can accurately detect a baby\u2019s specific sleep stages and predict\nwhen they will enter a light sleep stage within the next hour? And once this prediction is made, how can I alert the\nparent or caregiver that the baby will be waking up soon and suggest soothing methods to ease the transition from sleep\nto wakefulness? Also, how can I modify the statement \u201cThe baby is sleeping\u201d to reflect this predictive system in C#?\n3. Please extract the scribble result for the image in image 2.png\u201d\n4. With the HED result image 3.png, generate a new image that features a zoo landscape with various animals, a couple\nwith their children, and a fountain.\n5. Given the image image 4.png, What is unique about the window in the room?\nMedium\n6. Can you generate a new image that has a similar layout to the file named \u2019image 5.png\u2019? I\u2019m particularly interested in\nthe positioning of the elements. The new image should have the same arrangement of elements and their positioning.\n7. Generate a new image conditioned on the segmentation from image 6.png and the new image contains a majestic\nmountain range with a clear blue sky and a herd of wild horses running free.\n8. Take away the umbrella from the picture image 7.png.\n9. Crop out the baseball glove in image 8.png\n10. Provide me with a mask that separates the bear from the rest of the image 9.png?\nHard\n11. provide the number of umbrellas presented in the image 10.png, image 11.png, image 12.png, image 13.png, im-\nage 14.png\n12. Can you elaborate on the elements of the image 15.png, image 16.png and image 17.png provided?\n13. Erase the laptop from the image 18.png,image 19.png and image 20.png\n14. Create a new image using the segmentation from image 21.png that showcases a cozy cabin in the woods with a dog\nand a cat, surrounded by snow-covered trees. Can you crop out the dog from given image? I\u2019m looking for the cat in the\nimage file, can you guide me to it?\n15. Can you determine whether image 22.png contains a mouse? Please provide a list of all the objects present in the\nimage, with a special emphasis on the killer. Is image 23.png displaying a banana? As for the image, what skills are\nimportant for improving one\u2019s performance in the depicted scenario?\nTable 12. Test samples of instruction in the benchmark.\n12. Case Studies\nWe provide more cases across different modalities to val-\nidate the user experience for our method in practice. In\nFig. 4, we show some cases of image perception, which\ninvolves analyzing and understanding the content of an im-\nage, such as detecting objects, counting objects, finding ob-\njects, segmenting objects, answering questions about the\nimage, etc. These tasks require the system to invoke tools to\nprocess visual information and extract relevant features and\nlabels from the image. Fig. 5 gives examples of image pro-\ncessing and image editing, which assist users in processing\nor editing the image according to some criteria or instruc-\ntions. Fig. 6 mainly focuses on image question answering\nand image generation, showing the graphic dialogue capa-\nbility. In Fig. 7, we provide some multi-modal interaction\ncases on image, video, and audio domains. In addition, we\nalso illustrate the capabilities of complicated scenarios with\nsolutions searched by ToG during task planning in Fig. 8\nand Fig. 9. These complex tasks involve combining mul-\ntiple tools to find an advanced and creative solution path\nthat can solve more challenging problems.\nIt requires a\nsystem that can integrate different types of information and\noutputs from tools and generate comprehensive and mean-\ningful responses based on execution results. These figures\ndemonstrate the strong capabilities of ControlLLM in task\nplanning for both simple and complicated scenarios. It thus\nleads to a better user experience.\n18\nimage-perception\nUser: Provide me with the count of bears in the input image\nAI: The count of bears in the input image is 3.\nHere is a summary of the workflow and results for your request:\n1. The image was processed using an object detection tool, which detected the\npresence of bears in the image.\n2. The bounding boxes around the detected bears were selected by bounding boxes \nselection tool.\n3. The count of bears was then determined using a count objects tool.\n4. The result of the count objects tool indicated that there are 3 bears in the image.\nUser: I'm trying to find the umbrella in the image, can you assist me?\nAI: Yes, I can assist you in finding the umbrella in the image. After analyzing the\nimage, I have identified the location of the umbrella and cropped it for better\nvisibility.\nTo achieve this, I used the following tools in my workflow:\n1. Object Detection: This tool was used to detect various objects in the image,\nincluding the umbrella. The tool provided a list of detected objects with their\ncorresponding labels and bounding box coordinates.\n2. Select Bounding Box: This tool was used to select the bounding box\ncorresponding to the umbrella from the list of detected objects.\n3. Image Cropping: This tool was used to crop the image based on the selected\nbounding box of the umbrella. The result is a new image that focuses specifically on\nthe umbrella.\nI hope this explanation clarifies the steps I took to find and extract the umbrella from\nthe image. If you have any further questions, feel free to ask!\nUser: Give me the mask of elephant in the image.\nAI: I have generated a mask of an elephant for you. \nTo generate this mask, I used an image instance segmentation tool on the image. \nThe tool identified different objects in the image and created masks for them. Then, I \nused a mask selection tool to specifically select the mask for the elephant based on \nthe condition provided.\nPlease note that the result may not be completely accurate, as it relies on the \nperformance of the tools used.\nUser: Can you highlight the giraffe for me in the image?\nAI: I have highlighted the giraffe for you. \nTo fulfill your request, I followed the following workflow:\n1.First, I used an object detection tool to locate the elephant in the image.\n2.Then, I used a selection tool to choose the bounding box that contains the elephant.\n3.Finally, I used an image highlighting tool to highlight the selected bounding box \non the image.\nThe output of this workflow is the highlighted image, which shows the giraffe \nhighlighted.\nFigure 4. Case study on different task domains (a).\n19\nimage-processing\nUser: Extract the edge information for this image.\nAI: I have successfully extracted the edge information from the image you provided. \nTo achieve this, I used an image processing tool called \u201cimage_to_edge\u201d. I passed \nyour input image as input to the tool. The tool processed the image and generated a \nnew image file containing only the edge information. \nUser: Please get me the pose information for the image.\nAI: Sure! Based on your request, I have obtained the pose result for the input image. \nTo obtain this result, I used the \u201cimage_to_pose\u201d tool. I provided the tool with the \nimage file as input. The tool processed the image and generated the pose result. \nUser: Please segment anything without label information for this image.\nAI: I have segmented anything without label information for the image you \nmentioned. Now, let me explain the workflow and the tools used for your request. I \nused a tool called \"segment_anything\" to perform the segmentation. The input for \nthis tool was the image file named \"47e56e_None_input_image.jpg\". The tool \nprocessed the image and generated a segmentation result.\nimage-editing\nUser: Erase the sheep from the photo\nAI: I have successfully erased the sheep from the photo. Now, let me explain how I\naccomplished this task. First, I used an image instance segmentation tool to identify  \nthe sheep in the photo. This tool generated several mask images highlighting the\nsheep in different parts of the photo. Then, I used a select mask tool to choose the\nmost accurate masks that represent the sheep. These selected masks were saved as\nseparate images. Finally, I used an image inpainting tool to remove the sheep from\nthe original photo, using the selected masks as a guide.\nPlease let me know if there's anything else I can help you !\nUser: Erase the sheep from the photo\nAI: I have edited the picture and changed the color of the horse to blue.\nTo address your request, I used text_image_editing tool, which can edit the image \nguided by a textual prompt.\nFigure 5. Case study on different task domains (b).\n20\nimage-question-answering\nUser: Why is this image so funny?\nAI: The image is funny because it features two toy frogs, one of which is holding a\ncamera and taking a picture of the other. The scene is set in a grassy field, and the toy\nfrogs are posed in a way that mimics human behavior, specifically a photographer\ncapturing a subject. The humor comes from the unexpected and anthropomorphic\nportrayal of the toy frogs, as it is not common to see inanimate objects, such as toys,\nengaging in activities like photography. This playful and whimsical scene can evoke\namusement in the viewer.\nI used an image question answering tool. The tool analyzed the image and\ndetermined why this image is so funny.\nUser: Given the image, is there going to be a potentially dangerous situation in \nthe picture?\nAI: There is a potentially dangerous situation as a man is riding on the back of \nan elephant, while another elephant is walking through the water. The presence \nof two elephants in close proximity to each other and the man could pose a risk of \naccidents or injuries, especially if the elephants become agitated or frightened. It \nis essential for the man to maintain control over the elephant he is riding and \nensure that the other elephant is calm and well-behaved to minimize any \npotential dangers. To solve your question, I used an image question answering tool.\nUser: how many people can be seen riding the elephant in the image?\nAI: In the image, there is one person riding on the back of the elephant. I used an \nimage question answering tool to answer your question.\nimage-generation\nUser: I want a new image that has mountains and sunset. Can you use the depth result \nfrom input image to create it?\nAI:  The  new  image  is  generated.  The workflow involved two tools:\n\"image_to_depth\" and \"depth_text_to_image\".\nThe first tool, \u201cimage_to_depth\u201d, used an input image to generate a depth result.\nThe second tool, \"depth_text_to_image\", took a text input of \"The new image should \nhave mountains and sunset.\" and a depth map (generated by the previous tool)\nUser: please generate a new image based the edge from this image. The new image \nshows a beautiful tower in the rain.\nAI: The new image generated.\nWorkflow:\nThe image provided was processed using an image-to-edge tool, which generated an\nedge representation of the image.\nThe edge representation was then used along with the text \u201cThe new image shows a\nbeautiful tower in the rain\u201d in an edge-text-to-image tool. This tool combined the\nedge representation and the text to generate a new image.\nFigure 6. Case study on different task domains (c).\n21\nimage & video & audio tasks\nUser: What happened in this video?\nAI: In this video, a basketball game is taking place, with a player attempting to shoot\nthe ball. The player is holding the ball and getting ready to make a shot, while other\nplayers are on the court, possibly trying to defend or score.\nTo  solve  your  question,  I  used  two  tools:  \u201caction_recognition\u201d  and \n\u201cvideo_captioning\u201d, which both take the video as input. \nUser: Please generate music according to the following prompt: An 80s driving pop \nsong with heavy drums and synth pads in the background\nAI: Sure! I have generated a piece of music based on your prompt. \nTo create this music, I used a text-to-music tool. I provided the tool with the \nprompt you gave me, which was \"An 80s driving pop song with heavy drums and \nsynth pads in the background\". The tool then generated a music composition that \nmatches the description.\nUser: Can you help me dub this video with uploaded audio?\nAI: Sure! Based on your request, I have dubbed the video with the audio for you to \ndub a video. I used a \u201cdub_video\u201d tool that can add or replace the audio track of a \nvideo. I gave it the input video file and the input audio file, and it returned a new video \nfile with the audio synchronized with the video. \nUser: Convert the following sentence into speech: \u201cHope is the thing with feathers \nThat perches in the soul, And sings the tune without the words, And never stops at all\u201d\nAI: I have converted the following sentence into speech for you: \u201cHope is the thing\nwith feathers That perches in the soul, And sings the tune without the words, And\nnever stops at all\u201d. To do this, I used a text_to_speech tool that can generate speech\nfrom text. I gave it the input sentence and it returned an audio file with a human-\nlike voice reading the sentence. The tool can also adjust the speed, pitch, and accent\nof the voice to suit different preferences. I hope you like the results.\nUser: Generate an image where a woman is surfing. Then convert this image to a \ndynamic video.\nAI: I have generated an image where a woman is surfing and converted it to a \ndynamic video for you. \nTo do this, I used two tools that can create and manipulate visual content. The first \ntool called text_to_image can generate an image based on a text description. I gave \nit the input \u2018a woman is surfing\u2019 and it returned an image of a woman surfing on a \nblue ocean wave. The second tool is image_to_video that can convert an image to a \nvideo by adding motion and sound effects. I gave it the image from the first tool,\nand it returned a video of the woman surfing with realistic water and wind sounds. I \nhope you like the results.\nFigure 7. Case study on different task domains (d).\n22\ncomplex tasks\nUser: Please dub the video.mp4 with a suitable background music and then generate a HTML web page to introduce this \nvideo.\nAI: Sure! I can help you generate a video to introduce an image. Based on the\ninformation provided, I have used a combination of tools to fulfill your request.\nFirst, I used the \"text_to_image\" tool to convert your description into an image. The\ntool created an image of a serene and beautiful landscape with a calm lake reflecting the\nblue sky and white clouds. There are also green hills and trees on the shore, as well as a\nsmall wooden boat floating on the water.\nNext, I used the \"image_captioning\" tool to generate a textual description of the image.\nThe tool analyzed the image and produced a caption that best describes it.\nThen, I used the \"text_to_speech\" tool to convert the generated textual description into\nan audio file. This tool transformed the text into speech, allowing for a more engaging\nexperience.\nFinally, I used the \"image_audio_to_video\" tool to combine the generated image and\naudio into a video. The tool merged the image and audio together, creating a video that\nintroduces the image.\nThe result of this workflow is a video file. This video showcases the image described in\nyour initial request and includes the audio narration of its description.\n[Action(tool_name='video_captioning\u2018, inputs={'video': 'video.mp4\u2018}, outputs=['<TOOL-GEN>-text-0\u2018]), \nAction(tool_name='text_to_music\u2018, inputs={'text': '<TOOL-GEN>-text-0\u2018}, outputs=['<TOOL-GEN>-text_to_music-audio-\n0\u2018]), Action(tool_name='dub_video', inputs={'video': \"video.mp4 \", 'audio': \"<TOOL-GEN>-text_to_music-audio-0\",}, \noutputs=['<TOOL-GEN>-dub_video-video-0\u2019])]\n[Action(tool_name='title_generation', inputs={'text': '<TOOL-GEN>-text-0'}, outputs=['<TOOL-GEN>-text-1\u2018]), \nAction(tool_name='text_to_tags', inputs={'text': '<TOOL-GEN>-text-0'}, outputs=['<TOOL-GEN>-tags-0\u2018]), \nAction(tool_name='video_to_webpage', inputs={'video': '<TOOL-GEN>-dub_video-video-0', 'title': '<TOOL-GEN>-text-1', \n'tags': '<TOOL-GEN>-tags-0', 'description': '<TOOL-GEN>-text-0'}, outputs=['<GEN>-0'])]\nTask planning\nThe image features a \nserene scene of a boat \u2026\nUser: Generate a new image based on prompt. Prompt: \u201cA serene and beautiful landscape with a calm lake reflecting the \nblue sky and white clouds. There are some green hills and trees on the shore\u201d. Then help me generate a promotional video \nfor this generated image. \n[Action(tool_name='text_to_image', inputs={'text': 'A serene and beautiful landscape with a calm lake reflecting the \nblue sky and white clouds. There are some green hills and trees on the shore, and a small wooden boat floating on the \nwater.'}, outputs=['<TOOL-GEN>-text_to_image-image-0\u2018]), Action(tool_name='image_captioning', inputs={'image': \n'<TOOL-GEN>-text_to_image-image-0'}, outputs=['<TOOL-GEN>-text-0\u2018]), Action(tool_name='text_to_speech', \ninputs={'text': '<TOOL-GEN>-text-0'}, outputs=['<TOOL-GEN>-text_to_speech-audio-0\u2018]), \nAction(tool_name='image_audio_to_video', inputs={'image': \"<TOOL-GEN>-text_to_image-image-0\", 'audio': \"<TOOL-\nGEN>-text_to_speech-audio-0\"}, outputs=['<GEN>-0'])]\nTask planning\nAI: Sure, I can generate a web page for your input video.\nI have executed a workflow using several tools to generate the desired web page. The\nvideo_captioning tool was used to generate a text description for the input video. The\ndub_video tool combined the input video with the generated audio to create a dubbed\nvideo.\nThe title_generation tool was used to generate a title for the video based on the generated\ntext.\nThe text_to_tags tool generated a set of tags related to the video content.\nFinally, the video_to_webpage tool was used to generate a web page for the video,\nincluding the video itself, the generated title, tags, and description.\nThe resulting web page includes the following elements:\n1) A title prompt asking for a title to be generated within 10 words. 2) A video player to\nplay the input video. 3) Tags related to the video content. 4) A video description.\nFigure 8. Case study on different task domains (e).\n23\ncomplex tasks\nUser: Count horse in the image1.png, image2.png and image3.png\n[\nAction(tool_name='object_detection', inputs={'image': 'image1.png'}, outputs=['<TOOL-GEN>-object_detection-bbox-\n0\u2018]), Action(tool_name='select_bbox', inputs={'bbox_list': '<TOOL-GEN>-object_detection-bbox-0', 'condition': 'bear'}, \noutputs=['<TOOL-GEN>-select_bbox-bbox-0\u2018]), \nAction(tool_name='count_objects', inputs={'bbox_list': '<TOOL-GEN>-select_bbox-bbox-0'}, outputs=['<GEN>-0']),\n],\n[\nAction(tool_name='image_question_answering', inputs={'text': 'Provide me with the count of bears in the input image', \n'image': 'image1.png'}, outputs=['<GEN>-1'])\n],\n[\nAction(tool_name='object_detection', inputs={'image': 'image2.png'}, outputs=['<TOOL-GEN>-object_detection-bbox-\n1']),\nAction(tool_name='select_bbox', inputs={'bbox_list': '<TOOL-GEN>-object_detection-bbox-0', 'condition': 'bear'}, \noutputs=['<TOOL-GEN>-select_bbox-bbox-1']),\nAction(tool_name='count_objects', inputs={'bbox_list': '<TOOL-GEN>-select_bbox-bbox-0'}, outputs=['<GEN>-2']),\n],\n[\nAction(tool_name='image_question_answering', inputs={'text': 'Provide me with the count of bears in the input image', \n'image': 'image2.png'}, outputs=['<GEN>-3'])\n]\n[\nAction(tool_name='object_detection', inputs={'image': 'image3.png'}, outputs=['<TOOL-GEN>-object_detection-bbox-\n1']),\nAction(tool_name='select_bbox', inputs={'bbox_list': '<TOOL-GEN>-object_detection-bbox-0', 'condition': 'bear'}, \noutputs=['<TOOL-GEN>-select_bbox-bbox-1']),\nAction(tool_name='count_objects', inputs={'bbox_list': '<TOOL-GEN>-select_bbox-bbox-0'}, outputs=['<GEN>-4']),\n],\n[\nAction(tool_name='image_question_answering', inputs={'text': 'Provide me with the count of bears in the input image', \n'image': 'image3.png'}, outputs=['<GEN>-5'])\n]\nTask planning\nAI: Based on the information provided, the count of horses in the two images is eight. \nTo provide you with this information, I followed a workflow using various tools. First, I used an object detection tool to \ndetect objects in the images. Then, I used a tool called \u201cselect_bbox\u201d to select the bounding boxes that contain the desired \nobject, which in this case is a horse. Next, I used a tool called \u201ccount_objects\u201d to count the number of horses based on the \nselected bounding boxes. For both two images, I used the same solution to count the horses.  The \n\u201cimage_question_answering\u201d tool is also used to double check the result. The results of this workflow indicate that there \nare four horses in image1.png, three horses in image2.png and one horse in image3.png. So, there are eight horses in total. \nHowever, please note that these results may not be completely accurate, as they are generated using automated tools and \nmay require human judgement for accuracy.\nFigure 9. Case study on different task domains (f).\n24\n"
  },
  {
    "title": "Data-Centric Financial Large Language Models",
    "link": "https://arxiv.org/pdf/2310.17784.pdf",
    "upvote": "14",
    "text": "DATA-CENTRIC FINANCIAL LARGE LANGUAGE\nMODELS\nZhixuan Chu1, Huaiyu Guo1, Xinyuan Zhou1, Yijia Wang1, Fei Yu1, Hong Chen1,\nWanqing Xu1, Xin Lu1, Qing Cui1, Longfei Li1, Jun Zhou1, Sheng Li2\nAnt Group1\nUniversity of Virginia2\n{chuzhixuan.czx,guohuaiyu.ghy,zhouxinyuan.zxy,wangyijia.wyj,\nfred.yf,wuyi.ch,wanqing.xwq,lx111333,cuiqing.cq,longyao.llf,\njun.zhoujun}@antgroup.com, shengli@virginia.edu\nABSTRACT\nLarge language models (LLMs) show promise for natural language tasks but strug-\ngle when applied directly to complex domains like finance. LLMs have diffi-\nculty reasoning about and integrating all relevant information. We propose a data-\ncentric approach to enable LLMs to better handle financial tasks. Our key insight\nis that rather than overloading the LLM with everything at once, it is more effec-\ntive to preprocess and pre-understand the data. We create a financial LLM (FLLM)\nusing multitask prompt-based finetuning to achieve data pre-processing and pre-\nunderstanding. However, labeled data is scarce for each task. To overcome man-\nual annotation costs, we employ abductive augmentation reasoning (AAR) to au-\ntomatically generate training data by modifying the pseudo labels from FLLM\u2019s\nown outputs. Experiments show our data-centric FLLM with AAR substantially\noutperforms baseline financial LLMs designed for raw text, achieving state-of-\nthe-art on financial analysis and interpretation tasks. We also open source a new\nbenchmark for financial analysis and interpretation. Our methodology provides a\npromising path to unlock LLMs\u2019 potential for complex real-world domains.\n1\nINTRODUCTION\nLarge language models (LLMs) such as GPT-3 (Brown et al., 2020), GPT-4 (OpenAI, 2023), and\nLlama (Touvron et al., 2023) have revolutionized natural language processing tasks, excelling in text\nunderstanding, reasoning, and human-like response generation. While general LLMs are trained on\nbroad corpora to acquire general knowledge about language, recent research (Li et al., 2023; Wu\net al., 2023; Yang et al., 2023) has explored developing domain-specific LLMs by incorporating\nknowledge from specific fields. Domain-specific LLMs aim to achieve superior performance on\ndomain-relevant tasks compared to general LLMs. Strategies like fine-tuning, prompt-based tuning,\nand in-context learning have been employed to incorporate domain knowledge into LLMs. The core\nchallenge is developing effective techniques to inject the right domain knowledge into the LLMs\nand align their Language Modeling objective with domain-specific goals (Chu et al., 2023).\nLLMs\u2019 attempt to directly access and utilize all domain knowledge in one shot is unrealistic (Xue\net al., 2023b). There are two main approaches to injecting knowledge into LLMs with or without\nadditional training. One is to utilize prompt engineering to conduct in-context learning without any\ntraining, inserting information into prompts (Wang et al., 2023). However, token limitations arise\nwhen cramming excessive prompts into the context. Although tools like LangChain (Wu et al.,\n2022) can utilize embeddings instead of raw text, embedding provides a less direct means to inte-\ngrate such external knowledge sources. They are limited in representing more complex conceptual\nrelationships that are clear from linguistic context. A second technique involves leveraging new data\nto further train the large language model, fine-tuning its parameters on specific domains or tasks in\norder to adapt it for improved performance (Chu et al., 2023). While fine-tuning the large language\nmodel on new data can enhance its capabilities for certain applications, this approach has limita-\ntions in scale. As the model grows ever larger and more data is generated continuously, it becomes\ninfeasible to retrain the model on all new information.\n1\narXiv:2310.17784v2  [cs.CL]  14 Nov 2023\nFigure 1: Our framework utilizes two key components - a large language model (FLLM) trained on\nfinancial data to preprocess domain-specific texts and an abductive reasoning module that augments\ndata to improve FLLM. This differs from LangChain which operates directly on raw text corpora\nwithout any deep understanding and analysis of the raw financial data.\nTherefore, in our work, we take the finance domain as an example. To enable language models\nto reason like financial experts, they must comprehend financial information multifariously. This\nnecessitates integrating assorted tasks to acquire domain knowledge, such as event matching and\nanalogy, assessing viewpoint quality, and extracting key points, among others. Thus, we propose a\ndata-centric financial large language model named FLLM in Figure 1, based on a multitask prompt-\nbased finetuning to achieve these different objectives. However, labeled data is limited for each\nspecialized task in the complex financial domain, and annotators without domain expertise cannot\ncompetently label such data. We employ abductive learning to automatically generate training data\nby modifying pseudo labels from fledgling FLLM\u2019s own outputs to overcome the high cost of expert\nmanual annotation. Our framework is highly adaptable, enabling the development of knowledgeable\nassistants across many domains. In summary, our proposed data-centric AI approach has two key\nfacets. First, the financial knowledge base provides large language models with a preprocessed\nand parsed text corpus via data-centric FLLM. Second, abductive augmentation reasoning (AAR)\naddresses the scarcity of labeled data for specialized tasks to help train the FLLM. This combination\nof a financial large language model and abductive learning enables both knowledge injection into\nlarge language models and more sophisticated reasoning by conducting complex domain-specific\ntasks. The adaptable data-centric framework paves the way for knowledgeable AI assistants across\nfinance and many other specialized fields.\n2\nBACKGROUND\n2.1\nIN-CONTEXT LEARNING\nLarge language models (LLMs) such as GPT-3 (Brown et al., 2020), GPT-4 (OpenAI, 2023), and\nLlama (Touvron et al., 2023) have shown impressive performance on a wide range of natural lan-\nguage tasks through a method known as in-context learning (Brown et al., 2020). This approach\ndiffers from traditional supervised learning which requires large labeled datasets. Instead, in-context\nlearning allows models to acquire new skills and knowledge simply by being exposed to demonstra-\ntions of the task framed as natural language prompts (Liu et al., 2023). By conditioning the model\non new prompts that provide examples, LLMs can exhibit zero-shot capabilities ranging from trans-\nlation and summarization to mathematics and dialog, without updating the model parameters (Lu\net al., 2021). Our work on abductive augmentation reasoning also relies on prompt-based in-context\nlearning, with three core modules that leverage this technique to enable intuitive reasoning.\n2.2\nMULTITASK PROMPT-BASED FINETUNEING\nBy providing input-output examples as prompts, GPT-3 (Brown et al., 2020) showed an ability to\nsolve NLP problems without full fine-tuning. This led to many prompt design techniques following\na \u201dpre-train, prompt, and predict\u201d approach (Liu et al., 2021b). Some methods (Jiang et al., 2020;\nShin et al., 2020; Liu et al., 2021a; Gao et al., 2021; Lu et al., 2022) search over discrete prompts,\nwhile others use continuous vector embeddings (Xue et al., 2023a). Instruction-based prompts are\nmore flexible and natural, containing detailed task descriptions. As human-like prompts enable\nlearning from crowd-sourced data, instruction tuning of large language models is a promising ap-\nproach for general NLP capabilities (Weller et al., 2020; Efrat & Levy, 2020). Similar to Geng et al.\n(2023), our work uses multi-task prompt finetuning on a financial corpus for data preprocessing and\nunderstanding, which unifies various financial subtasks in a shared model.\n2\nFigure 2: The framework of the financial large language model (FLLM), which specifically prepro-\ncesses the original corpus information, so as to establish a bridge between the input to be analyzed\nand the knowledge sources. Small labeled datasets are insufficient for finetuning large FLLM. AAR\ncorrects pseudo labels from the fledgling FLLM to augment the labeled training data.\n2.3\nABDUCTIVE REASONING\nReasoning is the process of using logic to draw conclusions based on available information (Wang\net al., 2023). There are three main types of reasoning: deductive, inductive, and abductive. Deduc-\ntive reasoning involves starting with a general premise or theory and drawing a specific conclusion\nbased on that premise. Inductive reasoning works in the opposite direction - moving from specific\nobservations to a general conclusion that is probable but not certain based on the evidence. Finally,\nabductive reasoning (Walton, 2014; Kov\u00b4acs & Spens, 2005; Zhou, 2019) starts with an observation\nand then seeks the simplest explanation that would lead to that observation. It generates hypothe-\nses to explain a phenomenon rather than drawing conclusions. For example, upon observing that\nthe grass is wet, one could abductively reason that it rained last night as a possible explanation.\nAbductive reasoning is useful for generating theories and new insights that can then be tested.\nOur approach leverages the semantic reasoning abilities of large language models to augment train-\ning data through abductive inference. Rather than relying on symbolic rule formulations, we directly\nprompt the model with natural language descriptions of reasoning tasks. Recent work has shown\nthat large language models learn rich semantic representations that allow them to make plausible in-\nferences in context, despite lacking explicit symbolic reasoning capabilities (Tang et al., 2023). This\npseudo-logical reasoning emerges from the models\u2019 ability to build robust connections between to-\nkens, forming chains of reasoning that appear logically sound on the surface. Our method provides\na more scalable approach to dataset augmentation through abductive logic compared to previous\nmethods that require hand-crafted symbolic knowledge bases (Zhong et al., 2023).\n3\nMETHODOLOGY\n3.1\nPROBLEM STATEMENT\nLarge language models (LLMs) have demonstrated impressive capabilities across a variety of do-\nmains, enabling applications for medical diagnosis and legal assistance. However, LLMs still strug-\ngle with complex reasoning and analysis tasks that require understanding, reasoning, and integrating\ninformation from diverse sources. This limitation is particularly evident in specialized domains like\nfinance, where interpreting events, news, policies, and regulations requires integrating nuanced do-\nmain knowledge, synthesizing insights from multiple sources, elaborating logical reasoning, and\ngenerating an insightful point of view. In this work, our proposed system includes one fine-tuned\nfinancial large language model with access to external knowledge sources such as search engines,\ndomain databases, and expert systems. This allows conducting financial sub-tasks to provide ma-\nterials in a data-centric manner for final frozen LLM generation. Our ultimate goal is to utilize\nthis deeply processed corpus to produce sophisticated financial analysis and interpretations. While\nwe focus on financial analytics, our approach is designed to be generalizable across domains that\nrequire abundant information and complex reasoning.\n3.2\nDATA-CENTRIC FINANCIAL LARGE LANGUAGE MODEL\nFor the financial analysis and interpretation task, unlike the plain LangChain framework directly\nutilizing the raw information from different data sources, we establish one financial large language\nmodel (FLLM), which specifically preprocess the original corpus information, so as to establish\n3\nFigure 3: The example to instantiate the workflow of FLLM and the specific role of each subtask.\na bridge between the input to be analyzed and the knowledge sources, including domain expert\nknowledge, financial databases, and search engines. As shown in Figure 2, our designed FLLM\nis a multitask prompt-based fine-tuning LLM that is designed to handle three key subtasks for this\nfinancial analysis and interpretation task, i.e., (1) event matching and analogy, (2) viewpoint quality\nevaluation, and (3) key point extraction. The model first matches the input news to relevant materials\nin the databases and finds analogous cases and reports. Then, the matched materials are evaluated\nfor quality of opinion and analysis. Only the most insightful sentences are selected. Finally, the\nmodel extracts key details like industry, evaluation dimensions, sentiment, etc. to construct the\nstructure for text generation. In this way, our financial large language model acts as an interpretive\nbridge between the input text and background knowledge sources. By preprocessing the data and\nlearning correlations between events, viewpoints, and details, it can better leverage the information\nto produce high-quality financial analyses.\nSpecifically, we will use the example in Figure 3 to instantiate the end-to-end workflow of FLLM\nand the specific role of each subtask. The input is a new piece of government financial policy, about\nGuangzhou optimizes the standards for determining the number of housing units in personal hous-\ning loans. Firstly, we use a sub-ability of FLLM to match this financial policy with more materials,\nand get more analysis reports, although they may be inaccurate, scattered, or biased. Next, in step 2,\nFLLM selects the most insightful statements from this information and scores them to filter out irrel-\nevant noise and distills the content down to concise prompts suitable for the language model\u2019s length\nlimits later on. step 3, FLLM extracts high-level key information, such as industry, main indicators,\nanalysis perspectives, sentiment, etc., to grasp and guide the direction, angle, and tone (positive or\nnegative) for generating coherent text later. Through this series of FLLM modules, refined, focused,\nand filtered textual data has been prepared. In step 4, all this pre-processed information is formatted\ninto a prompt template. Finally, in step 5, a large language model like ChatGPT utilizes this re-\nfined prompt to fluently generate useful interpretation and analysis of the policy\u2019s implications. By\nsystematically preparing and guiding the input in a data-centric workflow, FLLM enables the final\nlanguage model to produce focused, logical explanations of new financial policies. The end result is\na cogent analysis specifically tailored to the original policy statement.\n3.3\nDATA-CENTRIC ABDUCTIVE AUGMENTATION REASONING\nThe workflow of the Financial Large Language Model has been detailed, but training such a multi-\ntask prompt-based fine-tuning system poses challenges. These three financial subtasks demand\nstrong domain knowledge, beyond what typical annotators possess.\nThus, our labeled data is\nseverely limited for these subtasks. Small labeled datasets are insufficient for finetuning large mod-\nels. We must expand the data in a scalable way to improve the FLLM\u2019s performance. Although\nlarge language models show promise for text annotation (Dai et al., 2023), complex professional\ntasks remain difficult. Empirically, we have demonstrated that ChatGPT and GPT-4 struggle with\n4\nFigure 4: The example of AAR on viewpoint quality evaluation task. The examples of AAR on\nevent matching and analogy and key point evaluation tasks are provided in the Appendix.\nthese financial annotation tasks in the following experimental section. More advanced methods are\nneeded to obtain quality labeled data. With better and more labeled data, the potential of FLLM can\nbe realized for specialized subtasks.\n3.4\nFRAMEWORK OF AAR\nWe propose an Abductive Augmentation Reasoning (AAR) algorithm to augment the training data\nfor our fledgling FLLM in an abductive manner. The AAR takes as input the pseudo-labels produced\nfor unlabeled data by the FLLM, which was trained on a small labeled dataset. These generated\nlabels from fledgling FLLM may be erroneous due to the limited training data, making it challenging\nto achieve strong performance. To address this, the AAR refines the pseudo-labels through three key\nmodules, i.e., Further Analysis Prompting through Dynamic Knowledge Questioning (FAP), Further\nAnalysis Enabling through Consistent Knowledge Answering (FAE), and Further Analysis-Driven\nOutput Modification through Knowledge-Enriched Question-Answer Fusion (FADOM). These three\nmodules are driven by LLM such as ChatGPT or GPT-4 and interact with domain expert knowledge\nto refine the preliminary pseudo-labels, aiming to enhance the fledgling model\u2019s performance. This\nabductive reasoning process is used to correct faulty labels and provide higher-quality training data.\nFAP.\nFurther Analysis Prompting through Dynamic Knowledge Questioning (FAP) takes the orig-\ninal input text, the initial output predictions from the fledgling FLLM, and domain expert knowledge\nas inputs. FAP automatically generates a comprehensive series of analysis questions that aim to ad-\ndress any gaps, inconsistencies, or need for clarification in the fledgling FLLM\u2019s output. These\nquestions are dynamically generated based on the specific output, prompting further reasoning and\nexploration. Example analysis questions can request more details on ambiguous conclusions, ask for\nthe reasoning or evidence behind claims, probe hypothetical scenarios to check consistency, identify\nmissing links in an argument, etc. The key is producing questions tailored to the output that can elicit\na more complete, well-reasoned analysis when answered. Answering these questions will prompt\nfurther reasoning and lead to a more complete, logical analysis.\nFAE.\nFurther Analysis Enabling through Consistent Knowledge Answering (FAE) takes the orig-\ninal input text, the fledgling FLLM\u2019s initial output, the analysis questions from FAP, and the domain\nknowledge as inputs. FAE answers the analysis questions in a robust, consistent manner based on\nthe domain knowledge. This provides broader, logically valid reasoning that aligns with known\n5\nfacts, relationships, and rules in the domain. FAE ensures the analysis is expanded in a knowledge-\ngrounded way to fully address the gaps identified by the FAP questions.\nFADOM.\nFurther Analysis-Driven Output Modification through Knowledge-Enriched Question-\nAnswer Fusion (FADOM) takes the original input, the fledgling FLLM\u2019s initial output, the analysis\nquestions and answers from FAP and FAE as inputs. FADOM selectively fuses the original output\nwith the question-answer pairs in a way that incorporates the expanded analysis, reasoning, clari-\nfications, and details provided by the QA process. This produces an improved output that benefits\nfrom abductive augmentation. The result is a more complete output aligned with domain expertise.\nIn summary, the automated AAR framework leverages abductive learning and dynamic QA over\nknowledge to augment FLLM\u2019s training data. This drives the fledgling FLLM to make more well-\nreasoned, detailed outputs consistent with the domain. As shown in Figure 9, the detailed prompt\ndesign, domain knowledge, input, and output of these three subtasks are provided, which shows that\nthe three modules work together to enable systematic enhancement for each subtask.\n4\nEXPERIMENTS\nIn this section, we conduct experiments to evaluate the effectiveness of data-centric FLLM to en-\nhance the generation by preprocessing the corpus information and data-centric AAR to improve\nFLLM by providing higher-quality and more training data. Specifically, we aim to address the fol-\nlowing research questions:\n1. Does AAR provide higher-quality data augmentation compared to annotations generated\nsolely by large language models?\n2. Can AAR boost performance on key financial subtasks addressed by our Financial Large\nLanguage Model?\n3. Can providing pre-processed financial text data to LangChain through a financial language\nmodel lead to better financial analysis and interpretation compared to giving LangChain\naccess to only raw financial text data?\nThrough these experiments, we aim to demonstrate that abductive reasoning based on LLM is an\neffective technique for data augmentation and model improvement. Further, the preprocessing and\ndeep processing of corpus information in a data-centric manner is necessary and critical for complex\ntext understanding, analysis, reasoning, and generation tasks in the field of expertise, such as finance.\n4.1\nDATASET AND TASK\nThe data were obtained from three main sources - web crawling (real-time storage of high-quality\npublic opinion and analysis from across the web), purchasing (procurement of industry-specific an-\nalytical reports and exclusive information), and in-house data (large amounts of user discussions,\ninfluencer perspectives, and high-quality works accumulated within the platform ecosystem). Tens\nof millions of text corpus are stored daily. We also open source a new benchmark for financial anal-\nysis and interpretation. In this work, we take three financial subtasks as examples. Event matching\nand analogy. This task involves matching input news to relevant materials in databases to find\nanalogous cases and reports. Evaluation metrics are precision, recall, and F1 score. These metrics\nmeasure the accuracy of matching input news to relevant materials. Higher scores indicate better\nperformance. Viewpoint quality evaluation. This task evaluates the quality of opinion and analysis\nin the matched materials. Only the most insightful sentences are selected. The evaluation metric is\nclassification accuracy. Measures how accurately the model classifies sentence quality into 2 or 4\ncategories like good/bad or excellent/good/fair/poor. Higher accuracy indicates better performance.\nKey point extraction. This task extracts key details like industry, evaluation dimensions, sentiment\netc from materials to construct text summaries. Evaluation metrics are accuracy and BLEU score.\nAccuracy measures the correct extraction of key points. BLEU measures how close the constructed\nsummary is to a human reference summary. Higher scores indicate better performance.\n4.2\nQUESTION 1: DOES AAR PROVIDE HIGHER-QUALITY DATA AUGMENTATION?\nTo answer whether abductive augmentation reasoning (AAR) provides higher-quality data augmen-\ntation compared to annotations generated solely by large language models, we designed a series of\n6\nexperiments to compare the annotation effects of AAR versus directly using existing large language\nmodels for annotation. We used ChatGPT and GPT-4 respectively to directly annotate 1000 unla-\nbeled data points for each of three tasks: (1) event matching and analogy (EMA), (2) viewpoint\nquality evaluation (VQE), and (3) key point extraction (KPE).\nSince our AAR includes three modules, and each module is built on top of the LLM, in order to\nexplore the effects of different foundation models on AAR annotation, we also conducted a series\nof ablation studies, using ChatGPT, GPT-4, ChatGLM, ChatGLM2, Alpaca2, and LLama2 respec-\ntively as the foundation model for AAR. From Table 1, we can observe that simply using large\nlanguage models makes it difficult to achieve annotation for these three complex financial tasks,\nwhile our AAR based on GPT-4 achieved the best results. In addition, we can see that AARs built\non ChatGLM, ChatGLM2, Alpaca2, and LLama2 have difficulty directly running through the entire\nAAR workflow, with more or less issues existing, leading to the abductive reasoning process be-\ning unable to proceed smoothly. In summary, our experiments demonstrate that AAR can provide\nhigher quality and more robust annotations compared to solely using LLMs, especially for complex\ndomain-specific tasks. The choice of foundation model is also important, with more capable LLMs\nlike GPT-4 better supporting the reasoning capabilities of AAR. There are still challenges in suc-\ncessfully implementing end-to-end abductive reasoning across different LLMs that require further\nresearch.\nThere are three modules in abductive augmentation reasoning (AAR), namely FAP, FAE, and\nFADOM. We incorporated domain expert knowledge to guide each of these three modules. To\nfurther explore the impact of AAR on data annotation and the role of domain expert knowledge\nin each module, we designed a series of experiments. As shown in Table 2, one or two modules\ncontain expert knowledge to verify the impact of their knowledge on the overall AAR annotation\nresults. From the table, we can observe that domain expert knowledge is useful for all three modules\n- removing any one of them affects the AAR annotation performance. The experiments provide\ninsights into how expert knowledge can be effectively incorporated into AAR to improve its data\nannotation capabilities. This allows AAR to be customized for different domains by plugging in rel-\nevant knowledge bases. Overall, explicitly encoding domain knowledge is shown to be an important\naspect of developing robust AAR systems.\nTable 1: The comparison of AAR data augmentation and direct annotation by LLM.\nSettings\nKPE\nVQE\nEMA\nStrategy\nBase Model Prompt Precision Recall\nF1\nAccuracy(2) Accuracy(4) Accuracy BLEU\nDirect\nChatGPT\n1 shot\n0.014\n0.023 0.018\n0.47\n0.21\n0.67\n0.399\nannotation\nGPT-4\n1 shot\n0.009\n0.016 0.011\n0.60\n0.22\n0.80\n0.482\nAAR\nChatGPT\n1 shot\n0.004\n0.008 0.005\n0.52\n0.32\n0.75\n0.316\nGPT-4\n1 shot\n0.226\n0.414 0.293\n0.71\n0.40\n0.87\n0.533\nChatGLM\n1 shot\n-\n-\n-\n-\n-\n-\n-\nChatGLM2\n1 shot\n-\n-\n-\n-\n-\n-\n-\nAlpaca2\n1 shot\n-\n-\n-\n-\n-\n-\n-\nLLama2\n1 shot\n-\n-\n-\n-\n-\n-\n-\nTable 2: The influence of domain expert knowledge of three modules on the AAR performance.\nSettings\nKPE\nVQE\nEMA\nAAR\nKnowledge\nPrecision Recall\nF1\nAccuracy(2) Accuracy(4) Accuracy BLEU\nGPT-4\nNo\n0.000\n0.000 0.000\n0.40\n0.14\n0.78\n0.465\nGPT-4\nFAP\n0.005\n0.008 0.006\n0.40\n0.18\n0.82\n0.477\nGPT-4\nFAE\n0.041\n0.062 0.050\n0.42\n0.15\n0.84\n0.496\nGPT-4\nFADOM\n0.042\n0.070 0.053\n0.58\n0.27\n0.82\n0.504\nGPT-4\nFAP+FAE\n0.027\n0.039 0.032\n0.36\n0.15\n0.87\n0.511\nGPT-4 FAP+FADOM\n0.029\n0.047 0.036\n0.56\n0.33\n0.84\n0.483\nGPT-4 FAE+FADOM\n0.163\n0.234 0.192\n0.59\n0.36\n0.84\n0.520\nGPT-4\nAll\n0.226\n0.414 0.293\n0.71\n0.40\n0.87\n0.533\n4.3\nQUESTION 2: CAN AAR BOOST THE PERFORMANCE OF OUR FLLM?\nTo explore whether AAR can boost performance on key financial subtasks addressed by our Fi-\nnancial Large Language Model, we designed three strategies with our FLLM. First, we leveraged\nstate-of-the-art general-purpose large language models like ChatGPT and GPT-4 without any train-\ning, using prompt engineering with one-shot and few-shot demonstrations to guide the FLLM on\nthe three financial tasks. Second, we fine-tuned the openly available large language models on a\n7\nTable 3: The performance comparison of different training strategies of FLLM on three tasks. Red:\nthe best, Blue: the second best.\nSettings\nKPE\nVQE\nEMA\nStrategy\nFLLM\nPrompt Precision Recall\nF1\nAccuracy(2) Accuracy(4) Accuracy BLEU\nNo training\nChatGPT\n1 shot\n0.014\n0.023 0.018\n0.47\n0.21\n0.67\n0.399\nGPT-4\n1 shot\n0.009\n0.016 0.011\n0.60\n0.22\n0.80\n0.482\nChatGPT\n20 shots\n0.179\n0.203 0.190\n0.52\n0.32\n0.75\n0.357\nGPT-4\n20 shots\n0.245\n0.266 0.255\n0.71\n0.49\n0.84\n0.535\nFinetune\nChatGLM\n1 shot\n0.057\n0.047 0.052\n0.53\n0.30\n0.60\n0.328\nChatGLM2\n1 shot\n0.093\n0.133 0.109\n0.50\n0.36\n0.60\n0.353\nAlpaca2\n1 shot\n0.160\n0.164 0.162\n0.57\n0.34\n0.55\n0.295\nAAR + Finetune\nChatGLM\n1 shot\n0.260\n0.305 0.281\n0.63\n0.26\n0.68\n0.379\nChatGLM2\n1 shot\n0.182\n0.344 0.238\n0.62\n0.34\n0.67\n0.389\nAlpaca2\n1 shot\n0.209\n0.367 0.266\n0.69\n0.39\n0.83\n0.485\nFigure 5: The performance of fine-tuned FLLMs as the amount of labeled training data increases.\nsmall amount of expert-annotated financial data. Third, we utilized the AAR technique to augment\nthe small amount of expert-labeled data into a larger high-quality labeled dataset for fine-tuning our\nFLLM foundation model.\nAs shown in Table 3, While GPT-4 with 20 shots prompting demonstrates impressive capabilities\nout-of-the-box, our approach of applying AAR data augmentation and then fine-tuning tailors the\nmodel more specifically to the financial domain. This allows our FLLM to reach comparable per-\nformance to GPT-4 on the key metrics across all three financial analysis subtasks. The augmented\ntraining dataset created through AAR provides the FLLM with sufficient coverage of the problem\nspace to match the few-shot generalization abilities of a cutting-edge general-purpose LLM like\nGPT-4. Our results highlight the potential of targeted data augmentation techniques like AAR to\nunlock specialized performance in limited resource contexts where acquiring substantial direct hu-\nman annotations is infeasible. With further development, AAR data augmentation could enable\nhigh-performance financial LLMs without the need for massive human labeling efforts. The key\nadvantage of AAR is that it provides an efficient way to generate more labeled data from a small\nseed set, which is especially valuable in specialized domains like finance where expert-labeled data\nis scarce. By leveraging AAR to amplify the limited human annotations, we were able to signif-\nicantly boost our FLLM\u2019s performance on core financial analysis subtasks relevant to real-world\napplications.\nFurthermore, to further explore the effects of abductive augmentation reasoning (AAR) on finan-\ncial large language models (FLLMs), we conducted a series of experiments by annotating different\namounts of FLLM training data with AAR annotations. We then fine-tuned the FLLMs and ob-\nserved how their performance changed across all tasks and metrics as the amount of annotated data\nincreased. The results, shown in Figure 5, demonstrate that metrics across all three tasks improved\nas more annotated data was used. This suggests that incorporating AAR into the training process\ncan enhance FLLMs\u2019 reasoning and generalization abilities for financial applications. Specifically,\nAAR\u2019s iterative generation and evaluation of hypotheses appears to provide a form of inductive bias\nthat helps the model better capture financial reasoning patterns and semantics from limited data.\nOverall, our experiments reveal the promise of AAR for imbuing FLLMs with more robust finan-\ncial intelligence. Further research is warranted to determine optimal AAR annotation strategies and\nmodel architectures to maximize the financial reasoning capacity of large language models.\n4.4\nQUESTION 3: CAN FLLM HELP LANGCHAIN TO GENERATE BETTER OUTPUT?\nWe will evaluate LangChain\u2019s ability to provide insightful financial analysis and interpretations\nwhen given pre-processed via FLLM vs. raw financial text data, rating it on four dimensions: Rel-\nevance (0-5): The analysis should focus on interpreting the core events described, without straying\n8\nTable 4: The comparison of LangChain and our pipeline on financial analysis and interpretations.\nMetric\nLangChain FLLM w/ 1,2,3 FLLM w/ 1 FLLM w/ 1,2\nRelevance 4.28 \u00b1 0.57\n4.85 \u00b1 0.14\n4.42 \u00b1 0.61\n4.57 \u00b1 0.61\nAccuracy\n4.14 \u00b1 1.14\n4.78 \u00b1 0.15\n4.35 \u00b1 0.55\n4.50 \u00b1 0.25\nLogic\n3.71 \u00b1 0.23\n4.28 \u00b1 0.23\n3.42 \u00b1 0.28\n3.57 \u00b1 0.62\nExpertise\n3.57 \u00b1 0.28\n4.71 \u00b1 0.23\n3.78 \u00b1 0.15\n3.85 \u00b1 0.14\nFigure 6: Real financial analysis and interpretation examples with detailed reasons and scores.\ninto unrelated topics or generic background. Accuracy (0-5): The analysis\u2019s viewpoint and reason-\ning should seem reasonable. It should consistently express a positive or negative outlook, without\nexaggerating or downplaying the event\u2019s impact on the industry. Logic (0-5): The analysis should\nflow smoothly and logically, with clear causality and relationships between points (Chu et al., 2021).\nIt should not simply restate event details or repeat the same point in different words. The overall\nmeaning should be coherent and well-structured. Expertise (0-5): The analysis should examine the\nevent\u2019s impacts from multiple professional investing angles. It should demonstrate sound financial\nlogic and insightful consideration of how the event could affect valuations. There should be a clear,\nlayered structure to the interpretation.\nTo robustly evaluate the capabilities of plain LangChain versus enhanced LangChain via FLLM,\nwe conducted a rigorous comparative experiment. 1000 recent news articles were analyzed and\ninterpreted using both plain LangChain and LangChain enhanced with the FLLM. To obtain objec-\ntive assessments, five independent human annotators were then invited to carefully review the 1000\nsample outputs across the four dimensions mentioned above. By averaging the annotators\u2019 scores in\neach dimension, we could quantify the improvements afforded by integrating FLLM into LangChain\nin an unbiased, statistically-sound manner. From Table 4, we observed that our method significantly\noutperformed plain LangChain on all metrics.\nAdditionally, to evaluate the contribution of our 3 subtasks of FLLM - (1) event matching and\nanalogy, (2) viewpoint quality evaluation, and (3) key point extraction - we designed 2 additional\nablation studies. In our original design (FLLM w/ 1,2,3), the outputs from all 3 subtasks are injected\ninto the final prompt of ChatGPT to guide generation. In the first ablation study (FLLM w/ 1), we\nonly input the results from subtask 1 on event matching and analogy, containing only the matched\ncorpus resources. In the second ablation study (FLLM w/ 1,2), we input the results from subtask\n1 and 2, including the matched corpus resources and high-quality viewpoints selected. From the\nresults, we observed that all 3 subtasks play necessary and complementary roles in producing the\nfinal generated text. In addition, as shown in Figure 6, we give a real example with detailed reasons.\n5\nCONCLUSION AND FUTURE WORK\nThis paper proposes a data-centric approach based on FLLM to improve LLMs\u2019 capabilities on\nfinancial analysis tasks. To overcome the scarcity of labeled data, they employ abductive augmen-\ntation reasoning to automatically generate training data. Experiments demonstrate their data-centric\nfinancial LLM with abductive augmentation reasoning substantially outperforms baseline LLMs,\nachieving state-of-the-art on financial analysis and interpretation benchmarks. The data-centric\nmethodology provides a promising direction to unlock the potential of LLMs for complex real-\nworld domains. The introduction of a new benchmark for financial analysis and interpretation is\nalso a valuable contribution. Besides, an interesting direction for future work is to combine the\n9\ndata-centric approach with other methods like prompting and self-supervised pretraining on finan-\ncial texts. Integrating multi-modal data like financial reports, earnings calls, and stock prices could\nalso enable more nuanced financial analysis.\nREFERENCES\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nZhixuan Chu, Stephen L Rathbun, and Sheng Li. Graph infomax adversarial learning for treatment\neffect estimation with networked observational data. In Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining, pp. 176\u2013184, 2021.\nZhixuan Chu, Hongyan Hao, Xin Ouyang, Simeng Wang, Yan Wang, Yue Shen, Jinjie Gu, Qing Cui,\nLongfei Li, Siqiao Xue, et al. Leveraging large language models for pre-trained recommender\nsystems. arXiv preprint arXiv:2308.10837, 2023.\nHaixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao,\nShaochen Xu, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, Hongmin Cai, Lichao Sun,\nQuanzheng Li, Dinggang Shen, Tianming Liu, and Xiang Li. Auggpt: Leveraging chatgpt for\ntext data augmentation, 2023.\nAvia Efrat and Omer Levy. The turking test: Can language models understand instructions? arXiv\npreprint arXiv:2010.11982, 2020.\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\nlearners. In ACL-IJCNLP, 2021.\nShijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation as\nlanguage processing (rlp): A unified pretrain, personalized prompt predict paradigm (p5), 2023.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language\nmodels know? Transactions of the Association for Computational Linguistics, 8:423\u2013438, 2020.\nGy\u00a8ongyi Kov\u00b4acs and Karen M Spens. Abductive reasoning in logistics research. International\njournal of physical distribution & logistics management, 35(2):132\u2013144, 2005.\nYunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. Chatdoctor: A\nmedical chat model fine-tuned on a large language model meta-ai (llama) using medical domain\nknowledge, 2023.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What\nmakes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021a.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-\ntrain, prompt, and predict: A systematic survey of prompting methods in natural language pro-\ncessing. arXiv preprint arXiv:2107.13586, 2021b.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-\ntrain, prompt, and predict: A systematic survey of prompting methods in natural language pro-\ncessing. ACM Computing Surveys, 55(9):1\u201335, 2023.\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal\ncomputation engines. arXiv preprint arXiv:2103.05247, 1, 2021.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered\nprompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings\nof the 60th Annual Meeting of the Association for Computational Linguistics, 2022.\nOpenAI. Gpt-4 technical report, 2023.\n10\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt:\nEliciting knowledge from language models with automatically generated prompts. In EMNLP,\n2020.\nXiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan\nZhang. Large language models are in-context semantic reasoners rather than symbolic reasoners.\narXiv preprint arXiv:2305.14825, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nDouglas Walton. Abductive reasoning. University of Alabama Press, 2014.\nYan Wang, Zhixuan Chu, Xin Ouyang, Simeng Wang, Hongyan Hao, Yue Shen, Jinjie Gu, Siqiao\nXue, James Y Zhang, Qing Cui, et al. Enhancing recommender systems with large language\nmodel reasoning graphs. arXiv preprint arXiv:2308.10835, 2023.\nOrion Weller, Nicholas Lourie, Matt Gardner, and Matthew E Peters. Learning from task descrip-\ntions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pp. 1361\u20131375, 2020.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prab-\nhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model\nfor finance, 2023.\nTongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and\nCarrie J Cai. Promptchainer: Chaining large language model prompts through visual program-\nming, 2022.\nSiqiao Xue, Yan Wang, Zhixuan Chu, Xiaoming Shi, Caigao Jiang, Hongyan Hao, Gangwei Jiang,\nXiaoyun Feng, James Y Zhang, and Jun Zhou. Prompt-augmented temporal point process for\nstreaming event sequence. arXiv preprint arXiv:2310.04993, 2023a.\nSiqiao Xue, Fan Zhou, Yi Xu, Hongyu Zhao, Shuo Xie, Qingyang Dai, Caigao Jiang, James Zhang,\nJun Zhou, Dacheng Xiu, and Hongyuan Mei. Weaverbird: Empowering financial decision-making\nwithlarge language model, knowledge base, and search engine. 2023b.\nHongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. Fingpt: Open-source financial large\nlanguage models, 2023.\nTianyang Zhong, Yaonai Wei, Li Yang, Zihao Wu, Zhengliang Liu, Xiaozheng Wei, Wenjun Li, Jun-\njie Yao, Chong Ma, Xiang Li, et al. Chatabl: Abductive learning via natural language interaction\nwith chatgpt. arXiv preprint arXiv:2304.11107, 2023.\nZhi-Hua Zhou. Abductive learning: towards bridging machine learning and logical reasoning. Sci-\nence China Information Sciences, 62:1\u20133, 2019.\n11\nA\nTHE EXAMPLES OF AAR ON THREE FINANCIAL TASKS IN ENGLISH AND\nCHINESE.\nFrom Figure 1 to Figure 6.\nB\nTHE EXAMPLE TO INSTANTIATE THE WORKFLOW OF FLLM IN CHINESE.\nFigure 7.\nC\nREAL FINANCIAL ANALYSIS AND INTERPRETATION EXAMPLES WITH\nDETAILED REASONS AND SCORES IN ENGLISH AND CHINESE.\nFrom Figure 8 to Figure 15.\n12\nFigure 7: The example of AAR on event matching and analogy task.\nFigure 8: The example of AAR on event matching and analogy task.\n13\nFigure 9: The example of AAR on viewpoint quality evaluation task. The examples of AAR on\nevent matching and analogy and key point evaluation tasks are provided in the Appendix.\nFigure 10: The example of AAR on viewpoint quality evaluation task.\n14\nFigure 11: The example of AAR on key point evaluation task.\nFigure 12: The example of AAR on key point evaluation task.\n15\nFigure 13: The example to instantiate the workflow of FLLM and the specific role of each subtask.\nFigure 14: Real financial analysis and interpretation examples with detailed reasons and scores.\n16\nFigure 15: Real financial analysis and interpretation examples with detailed reasons and scores.\nFigure 16: Real financial analysis and interpretation examples with detailed reasons and scores.\n17\nFigure 17: Real financial analysis and interpretation examples with detailed reasons and scores.\nFigure 18: Real financial analysis and interpretation examples with detailed reasons and scores.\n18\nFigure 19: Real financial analysis and interpretation examples with detailed reasons and scores.\nFigure 20: Real financial analysis and interpretation examples with detailed reasons and scores.\n19\nFigure 21: Real financial analysis and interpretation examples with detailed reasons and scores.\n20\n"
  },
  {
    "title": "PockEngine: Sparse and Efficient Fine-tuning in a Pocket",
    "link": "https://arxiv.org/pdf/2310.17752.pdf",
    "upvote": "11",
    "text": ""
  },
  {
    "title": "Reconstructive Latent-Space Neural Radiance Fields for Efficient 3D Scene Representations",
    "link": "https://arxiv.org/pdf/2310.17880.pdf",
    "upvote": "7",
    "text": "Reconstructive Latent-Space Neural Radiance Fields for Efficient 3D\nScene Representations\nTristan Aumentado-Armstrong1,2,4*, Ashkan Mirzaei1,2*, Marcus A. Brubaker1,3,4, Jonathan Kelly2,4,\nAlex Levinshtein1, Konstantinos G. Derpanis1,3,4, Igor Gilitschenski2,4\nAbstract\u2014 Neural Radiance Fields (NeRFs) have proven to be\npowerful 3D representations, capable of high quality novel view\nsynthesis of complex scenes. While NeRFs have been applied\nto graphics, vision, and robotics, problems with slow rendering\nspeed and characteristic visual artifacts prevent adoption in\nmany use cases. In this work, we investigate combining an\nautoencoder (AE) with a NeRF, in which latent features (instead\nof colours) are rendered and then convolutionally decoded.\nThe resulting latent-space NeRF can produce novel views with\nhigher quality than standard colour-space NeRFs, as the AE\ncan correct certain visual artifacts, while rendering over three\ntimes faster. Our work is orthogonal to other techniques for\nimproving NeRF efficiency. Further, we can control the tradeoff\nbetween efficiency and image quality by shrinking the AE\narchitecture, achieving over 13 times faster rendering with\nonly a small drop in performance. We hope that our approach\ncan form the basis of an efficient, yet high-fidelity, 3D scene\nrepresentation for downstream tasks, especially when retaining\ndifferentiability is useful, as in many robotics scenarios requir-\ning continual learning.\nI. INTRODUCTION\nNeural rendering techniques [1] continue to grow in im-\nportance, particularly Neural Radiance Fields [2] (NeRFs),\nwhich achieve state-of-the-art performance in novel view\nsynthesis and 3D-from-2D reconstruction. As a result, NeRFs\nhave been utilized for a variety of applications, not only in\ncontent creation [3], [4], [5], [6], but also for many robotics\ntasks, including 6-DoF tracking [7], pose estimation [8],\nsurface recognition [9] or reconstruction [10], motion plan-\nning [11], [12], [13], reinforcement learning [14], [15], tactile\nsensing [16], and data-driven simulation [17], [18]. However,\nslow rendering and the qualitative artifacts of NeRFs impede\nfurther use cases in production.\nTo render a single pixel, one major bottleneck is the\nneed for multiple forward passes of a multilayer perceptron\n(MLP). Replacing or augmenting the MLP with alterna-\ntive representations (e.g., voxel grids [19] or feature hash-\ntables [20]) has been used to improve both training and\ninference speed. Baking NeRFs into other primitive repre-\nsentations has also been a popular approach [21], [22], [23]\nfor faster rendering. To reduce artifacts (e.g., \u201cfloaters\u201d [24]),\ndifferent sampling methods [25], [26], [27], [28], radiance\n1Samsung AI Centre Toronto\n2University of Toronto\n3York\nUniversity\n4Vector\nInstitute\nfor\nAI.\n*Equal\nContribution.\nEmails:\n{a.mirzaei, tristan.a}@partner.samsung.com,\n{jkelly, gilitschenski}@cs.toronto.edu, {kosta,\nmab}@eecs.yorku.ca, alex.lev@samsung.com\n\u03a0\nRender at Low \nResolution\nHigh-Resolution RGB \nRender (8w x 8h)\nD\nDecode\nR(\u03a0|f)\nCamera\nLatent Feature \nField f\nGeometry\nFeatures\nw x h\nw x h\nFig. 1.\nIllustration of the Reconstructive Latent-Space NeRF (ReLS-\nNeRF) rendering pipeline (see \u00a7III). As shown in the upper-left inset, given\na trained feature field, f, and a camera, \u03a0, we can render a latent feature\nmap at a low resolution, as shown in the right inset. The geometry of the\nscene, encapsulated by the density field, which determines the 3D structure\nof the feature render, is learned via an RGB component (as in a regular\nNeRF). A decoder, D, can then map the low-resolution feature maps to a\nhigh-resolution colour image (lower-left inset). We may view this process,\nwhich maps camera parameters to images, as a form of neural rendering.\nmodels [29], and scene contraction functions [30], [28] have\nbeen proposed. Despite these advancements, NeRFs still\nsuffer from visual flaws and low rendering frame-rates.\nImportantly, such issues hamper the use of NeRFs for\ndownstream tasks, If rendering is too slow, agents will be\nunable to apply NeRFs as an internal 3D representation of the\nscene. Further, the solutions considered (often aimed at appli-\ncations in computer graphics, for instance) may not be com-\npatible with the requirements of other tasks. For example,\nmeshification [22], [31] enables fast rendering, but makes\nfurther online learning of the geometry significantly more\ndifficult, due to topological constraints [32] and additional\noptimization complexity (e.g., to handle self-intersections\nand other unnatural geometries) [33], [34]. We also do not\nwish to sacrifice too much representational fidelity (e.g., not\nincluding view-dependent effects [35]) for speed, as less\naccurate visual output can limit downstream opportunities for\n1\narXiv:2310.17880v1  [cs.CV]  27 Oct 2023\nscene analysis. We therefore require a model that is capable\nof fast rendering and intra-task optimization (i.e., learning\nduring an ongoing task), without sacrificing visual quality.\nIn this paper, we propose an approach for solving these\nchallenges that is orthogonal to existing methods. By lever-\naging convolutional autoencoders (AEs), we can define a\n\u201cNeRF\u201d operating in latent feature space (rather than colour\nspace), such that low-resolution latent renders can be de-\ncoded to high-resolution RGB renders (see Fig. 1). This\noffloads expensive MLP-based rendering computations to\nthe low-cost AE, greatly improving efficiency. Thus, we\nextend the standard NeRF architecture to return point-wise\nlatent vectors, in addition to densities and colours (the latter\nused only in training). Since the decoder is simply another\ndifferentiable neural network, the ability to optimize the\nunderlying 3D NeRF field is largely unchanged. As it is used\nfor scene reconstruction, we denote the resulting combined\nfield a Reconstructive Latent-Space NeRF (ReLS-NeRF).\nBeyond improving rendering speed, the AE can also act as\nan image prior, fixing some of the artifacts associated with\ndirect NeRF renders, and actually improving representational\nfidelity. However, we also observe that the use of the AE in\nReLS-NeRF can introduce unique temporal artifacts, which\nexisting image and video do not capture; hence, we define a\nnovel metric that takes advantage of the geometric structure\nof the NeRF to detect them.\nOverall, by fine-tuning a powerful pretrained AE, our\nmodel is able to render views several times faster, while\nempirically improving in multiple image and video quality\nmetrics. Further, we demonstrate a tradeoff between visual\nquality and rendering efficiency: by reducing the AE size, we\nobtain a 13-fold speed-up, with only a minor drop in quality.\nIn summary, we contribute (i) a novel approach to reconstruc-\ntive 3D scene representation, via a latent-space NeRF that\nboth improves rendering efficiency and outperforms existing\nwork on standard image and video quality metrics; (ii) a new\nevaluation metric, designed to detect temporal artifacts due\nto view inconsistencies, which existing metrics do not appear\nto capture; and (iii) the ability to trade-off image quality and\nrendering speed via varying the AE architecture.\nII. RELATED WORK\nA. Improving NeRF efficiency\nWhile NeRFs produce results of extraordinary quality, the\nspeed of fitting (training) and rendering (inference) remains\na bottleneck for adoption in a variety of applications (e.g.,\n[28], [17], [36]). This has prompted a myriad of approaches\nto increasing their efficiency. Feature grid architectures have\nproven effective in expediting fitting convergence (e.g., [37],\n[38], [39], [26], [40], [41], [19], [20]). Other approaches\ninclude utilizing depth [42], better initializations [43], and\npretraining conditional fields (e.g., [44], [45], [46]). Such\nimprovements can be readily utilized in our own framework.\nSimilarly, a number of methods have been proposed to\nenhance the efficiency of the volume rendering operation,\nwhich relies on an expensive Monte Carlo integration in-\nvolving many independent neural network calls per pixel.\nThese include architectural modifications [47], [48], [49],\n[50], [51], spatial acceleration structures [52], \u201cbaking\u201d (pre-\ncomputing and storing network outputs) [21], [23], improved\nsampling strategies [53], [54], [55], [56], [57], or altering the\nintegration method itself [58], [59]. Finally, several works\neschew volume rendering itself. A number of representations\n[60], [61], [62], [63], [64], [65] use only a single sample per\npixel, but struggle with geometric consistency and scalability.\nSimilarly, one can move to a mesh-based representation and\nuse rasterization instead [22], [66], [31]; however, this loses\ncertain properties, such as amenability to further optimization\nor differentiable neural editing. Though our approach im-\nproves rendering efficiency, it is orthogonal to these methods,\nas it reduces the number of MLP calls per image by changing\nthe output space of the NeRF itself.\nB. Feature-space NeRFs\nOther models have utilized neural feature fields (NFFs),\nas opposed to \u201cradiance\u201d fields, where rendering is altered to\noutput learned features instead. Some NFFs [67], [68] learn\nto produce the outputs of pretrained 2D feature extractors;\nsimilarly, several works have considered the use of language-\nrelated features [69], [70], [71] and other segmentation\nsignals [72], [73], [74], [5] to embed semantics into the\nNFF. More closely related to our work are generative mod-\nelling NFFs that decode rendered features into images via\ngenerative adversarial networks [75], [76], [77] or diffusion\nmodels [78], [79], [80]. In contrast, this paper considers the\nscene reconstruction problem, using a latent representation\npotentially amenable to downstream tasks, and investigates\nissues related to view consistency. In particular, the artifacts\nof generative methods are similar to those detected by our\nnovel quality metric (namely, appearance inconsistencies\nacross close frames or camera viewpoints; e.g., see [76]).\nIII. METHODS\nAs in the standard NeRF scenario, we expect only a set\nof multiview posed images, SI = {(Ii, \u03a0i)}i. The goal is to\nlearn a 3D scene representation in an autoencoder (AE) latent\nspace, capable of novel view synthesis. Thus, our model\nincludes two neural modules (\u00a7III-A): (i) a modified NeRF,\nf, which outputs a latent vector (in addition to its standard\noutputs), and (ii) an AE, with encoder and decoder networks,\nE and D. To fit the model, we apply a multi-stage process:\ntraining the AE, fitting the NeRF, and then fine-tuning D\n(see \u00a7III-B).\nA. ReLS-NeRF Neural Architecture\nWe first extend the standard colour-density field of NeRF\nto include a latent feature vector, z, via f(x, r) = (\u03c3 \u2208\nR+, c \u2208 [0, 1]3, z \u2208 Rn), where x and r represent the input\nposition and direction, and \u03c3 and c represent the output\ndensity and colour. We refer to the \u03c3 and c fields as an\n\u201cRGB-NeRF\u201d, to distinguish them from the latent component\nof the ReLS-NeRF. Note that the RGB-NeRF is used only in\ntraining, to learn the density field and produce renders to help\n2\nFig. 2.\nAn overview of the ReLS-NeRF fitting and inference processes. Left: optimization approach. The radiance (colour) field is fit to RGB captures,\nas in the standard NeRF [2]. Given camera parameters, \u03a0, ReLS-NeRF renders feature maps in the latent Z-space defined by a convolutional autoencoder\n(AE), D \u25e6 E, for which arbitrary views can be decoded into image space. The discrepancy between the decoded features and the corresponding images\n(from an RGB-NeRF or real images) enables training the Z-space NeRF and the AE. Right: inference approach. After freezing the latent field and decoder,\none can render the scene from an arbitrary viewpoint, obtaining a latent feature map that can be decoded into a high-resolution image.\ntrain the latent component (see \u00a7III-B). Volume rendering is\nunchanged: for a single feature at a pixel position, p, we use\nZ(p) =\nZ tmax\ntmin\nT (t)\u03c3(t)z(t) dt,\n(1)\nto obtain the feature value at p, where T (t) is the transmit-\ntance [81], and z(t) = z(x(t), r(t)) is obtained by sampling\nthe ray defined by p. For camera parameters \u03a0, we denote the\nlatent image rendering function as R(\u03a0|f) = IZ(\u03a0), where\nIZ[p] = Z(p). Replacing z(t) with c(t), for instance, would\nrender colour in the standard manner, giving a colour image,\nIC(\u03a0) (that does not use z). To obtain a colour image from\nIZ, we simply pass it to the decoder, D; i.e., view synthesis\nis bIC(\u03a0) = D(IZ(\u03a0)), which can be viewed as a form of\nneural rendering (e.g., [76], [82], [83]). The benefit of using\nbIC is that significantly fewer pixels need to be rendered,\ncompared to IC(\u03a0); it also enables placing a prior on bIC by\nchoosing D appropriately.\nWe considered two choices of AE: (i) the pretrained VAE\nfrom Stable Diffusion [84], which we denote SD-VAE, and\n(ii) a smaller residual block-based AE [85], [86] (R32, when\nusing a 32D latent space) that is randomly initialized. Both\nencoders provide an 8\u00d7 downsampling of the image.\nB. Fitting Process\nA ReLS-NeRF is optimized in three stages: (A) AE\ntraining, (B) joint NeRF fitting, and (C) decoder fine-tuning.\nAE training (A). The first phase simply trains (or fine-tunes)\nthe AE to reconstruct the training images of the scenes, using\nthe mean-squared error.\nJoint NeRF fitting (B). In the second phase, we train the\nRGB and Latent components of the NeRF in conjunction\nwith the decoder, D. Our total loss function,\nLB = Lr + \u03bbdLd + \u03bbgrLgr + Lp,\n(2)\nconsists of the standard RGB loss on random rays, Lr, the\nDS-NeRF [42] depth loss, Ld, the geometry regularizing\ndistortion loss [28], Lgr, and a patch-based loss for training\nthe latent component, Lp. Given a posed image, (I, \u03a0), the\nlatter loss is simply the error between a sample patch, P \u223c I,\nand the corresponding rendered then decoded patch,\nLp = EP\u223cI,(I,\u03a0)\u223cSIMSE(P, D(IZ(\u03a0))).\n(3)\nDecoder fine-tuning (C). Finally, we fine-tune D, utilizing\na combination of the multiview posed images, SI, and\nrenders from the RGB component of the ReLS-NeRF. First,\nwe sample random renders, eSI = {(IC(\u03a0s), \u03a0s) | \u03a0s \u223c\n\u0393(S\u03a0)}s, where \u0393(S\u03a0) is the uniform distribution over\ncamera extrinsics, obtained by interpolating between any\ntriplet in S\u03a0. Optimizing\nLC = \u03b3\u03b4(SI) + (1 \u2212 \u03b3)\u03b4(eSI),\n(4)\nwhere \u03b4(S) = E(I,\u03a0)\u223cSMSE(I, bIC(\u03a0)) and \u03b3 \u2208 [0, 1] is\na weighting hyper-parameter, distills information from the\nRGB-NeRF into latent renderer. See Fig. 2. Note that the\nreal training images, SI, are used; hence, the RGB-NeRF is\nnot strictly a ceiling on performance (further, the presence\nof D implies different generalization properties).\n3\nC. Implementation Details\nWe utilize the neural graphics primitives [20] architec-\nture, via the tiny-cuda-nn library [87]. All phases use\nAdam [88] for optimization. We remark that the loss gradient\nfrom the latent component of the NeRF (i.e., from Lp) is\nnot back-propagated to the colour, c, and density, \u03c3, fields.\nFurther, we use separate features for the latent feature vector,\nz, and c, but render with the same \u03c3. In other words, RGB-\nNeRF training is unaffected by z. For additional details, we\nrefer the reader to our appendix.\nIV. EXPERIMENTS\nA. Evaluation Metrics\n1) Pixelwise and perceptual distances: We measure per-\nformance with novel view synthesis on held-out test views.\nIn addition to the standard pixelwise peak signal-to-noise\nratio (PSNR), we use perceptual losses to measure similarity,\nincluding LPIPS [89] and DreamSim [90]. LPIPS provides\nmore human-like responses to low-level distortions (e.g.,\nnoise, small colour/spatial shifts), whereas DreamSim is\ndesigned to be \u201cmid-level\u201d metric, better capturing large-\nscale and semantic differences than LPIPS (without being as\nhigh-level as, e.g., CLIP-based metrics [91], [92], [93]).\n2) Local consistency: When examining generative models\nof NeRFs that use decoders, we can qualitatively see a\n\u201cshimmering\u201d effect in time (e.g., [76], [75]), which is\nalso reminiscent of generative video model artifacts (e.g.,\n[94], [95]). This jittering appears related to local appearance\ninconsistencies: since each latent pixel corresponds to an\nRGB patch. As \u03a0 changes, interpolating in z-space does\nnot perfectly approximate the correct appearance changes.\nThis behaviour is distinct from the artifacts observed in\nstandard NeRFs and we devise a simple metric to detect\nit: the Reprojective Colour Consistency (RCC) metric. The\nRCC measures sudden changes in appearance as \u03a0 changes,\nrelying on the NeRF geometry to obtain correspondences.\nSpecifically, we reproject one render, Ii, into the reference\nframe of another, Ii+1, using the NeRF depth, Di, so\nRCC = PSNR\n\u0010\nEi[MSE(Ii+1, ReprojDi,\u03a0i+1Ii)]\n\u0011\n,\n(5)\nwhere Ii and Ii+1 are adjacent video frames. Notice that\nocclusions and view-dependent lighting effects will confound\nthe RCC; however, these effects will (i) be relatively minimal\nacross adjacent frames and (ii) be shared for the same scene,\nenabling it to be a fair comparative metric.\n3) Video quality: As noted above, adding a temporal\ndimension can make certain artifacts more perceptually de-\ntectable. We therefore applied a recent video quality metric,\nDOVER [96], to NeRF-rendered videos. DOVER has two\ncomponents: DOVER-aesthetic (DoA), which focuses on\nhigh-level semantics, and DOVER-technical (DoT), which\ndetects low-level distortions (e.g., blur and noise). DOVER\nand the RCC are applied to 120-frame \u201cspiral video\u201d renders\nfrom the NeRF (as in LLFF [97]).\nReference-based\nReference-free\nNeRF\nPSNR\u2191 LPIPS\u2193 DS\u2193 DoA\u2191 DoT\u2191 RCC\u2191\nRGB\n23.52\n0.37\n1.18\n80.2\n72.9\n25.6\nOurs-SD\n23.81\n0.35\n1.44\n81.5\n77.3\n25.5\nOurs-R32\n23.37\n0.40\n1.71\n76.4\n74.3\n25.3\nTab. 1.\nTest-view evaluation on eight LLFF scenes [97]. Reference-based\nmetrics include PSNR, LPIPS [89], and DreamSim (DS; \u00d710) [90]. For\nreference-free metrics, we use DOVER-technical (DoT), DOVER-aesthetic\n(DoA), and our reprojective colour consistency (RCC) measure, computed\non rendered videos. Rows correspond to the standard RGB NeRF, the\nSDVAE-based ReLS-NeRF, and the R32-based ReLS-NeRF. ReLS-NeRF-\nSDVAE outperforms the RGB-space NeRF on the lower-level reference-\nbased (PSNR and LPIPS) and reference-free (DoT) metrics, but has mixed\nperformance on the more semantic metrics (DS and DoA). Our RCC metric,\ndesigned to detect the \u201cshimmer\u201d present in decoded (neural rendered)\nvideos, detects slightly more inconsistency with ReLS-NeRF. Using R32\nreduces accuracy, but enables much faster rendering time (see Table 2).\nNeRF\nRendering Time\nFitting Time\n(A)\n(B)\n(C)\nRGB\n132.1s [1\u00d7]\n\u2013\n1h\n\u2013\nOurs-SD\n43.1s [3\u00d7]\n10m\n2h\n2.5h\nOurs-R32\n10.2s [13\u00d7]\n40m\n1.5h\n1.5h\nTab. 2.\nTimings for inference (rendering 120 frames) and fitting. Changing\nthe decoder architecture, D, trades off between efficiency and image quality.\nWe measure the RGB-NeRF rendering time without the latent component.\nB. Reconstruction Quality and Timing\nWe display our evaluation in Table 1, as well as timing\nmeasurements in Table 2, using eight LLFF scenes [97] (see\nalso Fig. 3 for qualitative examples)*, at 1008\u00d7756 reso-\nlution. We see that ReLS-NeRF (i.e., decoding a rendered\nlatent feature map) with the SDVAE actually has superior\nnovel view image quality, while having superior inference\nspeed (three times faster). In particular, the low-level metrics,\nincluding PSNR, LPIPS, and DoT, all prefer ReLS-NeRF-SD\nover the standard colour NeRF. This is likely due to the fine-\ntuned decoder fixing artifacts incurred by the colour NeRF,\nas can be seen in Fig. 3. The higher-level, more semantic\nmetrics are more mixed: DreamSim prefers the RGB-NeRF,\nwhile DoA slightly favours ReLS-NeRF-SD.\nAmong reference-based metrics, the semantically-oriented\nDreamSim is the only one by which the RGB-NeRF outper-\nforms ReLS-NeRF-SD. Since DreamSim is a single-image\nmetric, it is insensitive to temporal artifacts; however, Dream-\nSim is known to be more sensitive to foreground objects\n[90]. Interestingly, we qualitatively observe that ReLS-NeRF\ntends to improve image quality the most in scene areas\nfar from the camera, where geometry is generally poorer\nquality \u2013 i.e., in the background (see Fig. 3). Thus, one might\nspeculate that such improvements are simply going unnoticed\nfor DreamSim, which tends to focus on foreground objects\nof greater semantic importance.\nIn addition, we find that the RCC prefers the RGB-NeRF\nover ReLS-NeRF. Though it is hard to see in still images,\nReLS-NeRF has slight temporal \u201cjittering\u201d artifacts, which\nthe RCC is designed to detect. We remark that other algo-\nrithms show similar view-inconsistencies across close frames\n(e.g., 3D generative models [76] or video generators [94]),\n*Images in Figs. 1-4 available in LLFF [97] under a CC BY 3.0 License.\n4\nRGB NeRF\nOurs-SD\nOurs-R32\nFig. 3.\nQualitative comparison of NeRF renders. In the zoomed insets, we show how ReLS-NeRF-SD fixes some of the artifacts of the RGB-NeRF,\ndespite being trained in part on its renders (see \u00a7III-B, phase C) One can also see the slight blur incurred by using the faster R32 AE (middle column).\nNotice that improvement in visual quality can actually have significant semantic effects, in a manner that could impact downstream tasks (e.g., navigation):\nin the third row, for instance, one can actually read the \u201cexit\u201d sign in ReLS-NeRF-SD, but not in the other two cases.\nand could potentially benefit from RCC estimates. We illus-\ntrate this phenomenon with some examples in Fig. 4. Due\nto the learned decoder, unexpected appearance changes can\noccur across viewpoints. However, per-frame metrics, such\nas the traditionally applied LPIPS and PSNR, do not capture\nsuch inconsistencies; hence, ReLS-NeRF outperforms the\nRGB-NeRF on them (Table 1). Interestingly, even the video\nmetrics (DoT and DoA) prefer ReLS-NeRF, suggesting such\nalgorithms are more sensitive to the cloudiness and noise\nartifacts of the standard NeRF, compared to the small jitters\nincurred by the neural rendering process. In other words, by\nmost metrics of quality (including the primary standard ones,\nPSNR and LPIPS), ReLS-NeRF is superior.\nFinally, we show that the trade-off between rendering\nefficiency and image quality can be controlled by changing\nthe AE architecture. Using R32 reduces inference time by\n\u223c92%, while decreasing test-view PSNR by only 0.15,\ncompared to the RGB-NeRF rendering process. In contrast\nto ReLS-NeRF-SD, while ReLS-NeRF-R32 does sacrifice\nsome image quality (e.g., \u223c0.4 PSNR loss), it also reduces\ninference time by \u223c76%. One can imagine choosing an\narchitecture with the right level of trade-off for a given task.\nC. Ablations\nWe find that removing phase C is devastating to ReLS-\nNeRF, causing PSNR to drop to 22.85 (SD) and 20.87 (R32).\nSince the SDVAE is pretrained, ablating phase A has little\neffect on ReLS-NeRF-SD; however, doing so for ReLS-\nNeRF-R32 reduces PSNR by 0.1. Note that the latter case\ntrains the decoder, D, alongside the NeRF and then alone,\nin phases B and C.\nV. DISCUSSION\nWe have shown that ReLS-NeRF can improve image qual-\nity, while being several times faster to render. In particular,\nthe SD-based ReLS-NERF is superior on the main metrics\ncommonly used to evaluate NeRFs on test views (i.e., PSNR\nand LPIPS), as well as on a state-of-the-art reference-free\n5\nFig. 4.\nExamples of adjacent video frames from ReLS-NeRF-SD (top row) and the RGB NeRF (bottom row). Each pair of images are temporally adjacent\nrenders from a video. Notice, as in Fig. 3, that ReLS-NeRF-SD has better per-frame image quality, as measured in the quantitative results of Table 1. For\nexample, see the upper half of the leftward zoomed insets, where the RGB NeRF is more \u201ccloudy\u201d. However, there are temporal artifacts that cannot be\ndetected in a single frame (i.e., small cross-view appearance inconsistencies). For instance, as can be seem in the highlighted areas of the zoomed insets,\nsmall spots can suddenly appear (left), or the shape of highlights can change (right). This does not occur in the RGB NeRF, as volume rendering colours\ndirectly encourages view consistency, whereas the learned decoder in ReLS-NERF can introduce inconsistent appearances. This showcases the utility and\nneed for our new reprojective colour consistency (RCC) metric (see \u00a7IV-A.2), which can capture these temporal aspects more directly.\nvideo quality estimator. Empirically, we observed that current\nimage and video evaluation metrics do not obviously capture\ntemporal artifacts that are characteristic of ReLS-NeRF,\ncaused by view-inconsistent appearance changes (due to the\nlearned component within the rendering process). Hence, we\nintroduced a simple metric for detecting such anomalies.\nFurther, we have demonstrated a tradeoff between efficiency\nand quality, which can be controlled by the architecture of the\nAE. Importantly, to obtain its speedup, ReLS-NeRF does not\n\u201cbake\u201d the scene or transform to a mesh; hence, e.g., it could\nstill be continually trained online in the standard fashion.\nIn other words, it retains a number of useful properties\nof standard NeRFs (e.g., differentiability and access to an\nimplicit 3D shape field), while gaining additional efficiency\nand image quality.\nFor many robotics tasks, fast differentiable rendering is a\nkey component for online learning of 3D scene representa-\ntions. This includes simultaneous localization and mapping,\nnavigation, and modelling the dynamics of the environment\n(i.e., ensuring the internal representation is up-to-date, given\nperceptual inputs). We feel that ReLS-NeRF is well-suited\nfor such situations, as it retains differentiability, while im-\nproving rendering efficiency and even image quality as well.\nOther promising future directions include utilizing different\nAEs to provide task-specific biases (e.g., for 3D scene\nediting, faster speed, or higher image quality), improving\nthe AE architecture to suit this scenario (e.g., devising a\ngeometry-aware decoder), and better customizing the volume\nrendering process to latent space rendering (e.g., using a\nlearned mapping instead of volume integration).\nAPPENDIX\nA. Additional Implementation Details\nWhen training, we used \u03bbd = 0.1, \u03b3 = 0.7, and \u03bbgr =\n10\u22123/2. The NeRF architecture was the same as previous\nworks based on Instant-NGP (see [5]). The LLFF scenes\nused were fern, horns, orchids, flower, leaves,\nroom_tv, trex, and fortress.\nB. Fitting Hyper-Parameters\nPhase A. The SDVAE/R32 NeRFs were optimized for\n500/3000 iterations, using learning rates of 10\u22124/4 \u00d7 10\u22124.\nThe learning rates were halved at 150, 300, and 450 iterations\n(SDVAE) and every 500 iterations for R32. Patches of size\n5122 were used, with batch sizes of 3/5.\nPhase B. The joint optimization was run for 20K itera-\ntions. We used 4096 rays for the colour and DS-NeRF losses,\neach. The latent loss, Lp, is computed via 322 latent-space\npatches. The learning rate (excluding the VAE) starts from\n10\u22122 and is decayed according to 10\u22122 \u00d7 (10\u22121)t/\u03c4, where\nt is the step iteration and \u03c4 = 104. The VAE is optimized\nwith a fixed learning rate of 10\u22124.\nPhase C. Decoder fine-tuning proceeds for 3000/10000\niterations for the SDVAE/R32 architecture. A batch size of\nthree was used (one from SI and two from eSI). Note that we\nrender 512 images from the RGB-NeRF to act as supervision\n(i.e., |eSI| = 512). The process starts from a learning rate of\n10\u22124, and is decayed by 0.5 every 1000/2500 iterations.\nC. R32 Architecture\nThe encoder, E, has the following structure: C5, RBIN,\nHD, RBIN, HD, RBIN, HD, RBIN, C1. The components are\nas follows: C5 is a conv-5\u00d75-norm-elu block; RBIN is two\nresidual blocks [85], each using conv-3\u00d73 and norm; HD is\na bilinear halving downscaler; and C1 is just a conv-1\u00d71.\nThe encoder has layer sizes of (32,128,128,256,256).\nThe\ndecoder,\nD,\nhas\nthe\nfollowing\nstructure:\nC1,\nRBIN, HU, RBIN, HU, RBIN, HU, RBIN, C1, sigmoid.\nComponents are the same, except that HU is a bilin-\near doubling upscaler. The decoder has layer sizes of\n(256,256,128,128,32).\nBoth networks use the ELU non-linearity [98] and instance\nnormalization [99] as norm.\n6\nREFERENCES\n[1] A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. Tretschk, Y. Wang,\nC. Lassner, V. Sitzmann, R. Martin-Brualla, S. Lombardi, T. Simon,\nC. Theobalt, M. Niessner, J. T. Barron, G. Wetzstein, M. Zollhoefer,\nand V. Golyanik, \u201cAdvances in neural rendering,\u201d in Proceedings of\nSIGGRAPH, 2021. 1\n[2] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoor-\nthi, and R. Ng, \u201cNeRF: Representing scenes as neural radiance fields\nfor view synthesis,\u201d in European Conference on Computer Vision\n(ECCV), 2020. 1, 3\n[3] A. Haque, M. Tancik, A. Efros, A. Holynski, and A. Kanazawa,\n\u201cInstruct-NeRF2NeRF: Editing 3D scenes with instructions,\u201d arXiv\npreprint arXiv:2303.12789, 2023. 1\n[4] K. Zhang, N. Kolkin, S. Bi, F. Luan, Z. Xu, E. Shechtman, and\nN. Snavely, \u201cARF: Artistic radiance fields,\u201d in European Conference\non Computer Vision (ECCV), 2022. 1\n[5] A. Mirzaei, T. Aumentado-Armstrong, K. G. Derpanis, J. Kelly,\nM. A. Brubaker, I. Gilitschenski, and A. Levinshtein, \u201cSPIn-NeRF:\nMultiview segmentation and perceptual inpainting with neural radi-\nance fields,\u201d in IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. 1, 2, 6\n[6] A. Mirzaei, T. Aumentado-Armstrong, M. A. Brubaker, J. Kelly,\nA. Levinshtein, K. G. Derpanis, and I. Gilitschenski, \u201cReference-\nguided controllable inpainting of neural radiance fields,\u201d in Interna-\ntional Conference on Computer Vision (ICCV), 2023. 1\n[7] B. Wen, J. Tremblay, V. Blukis, S. Tyree, T. Muller, A. Evans, D. Fox,\nJ. Kautz, and S. Birchfield, \u201cBundleSDF: Neural 6-DoF tracking\nand 3D reconstruction of unknown objects,\u201d IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023. 1\n[8] M. Z. Irshad, S. Zakharov, R. Ambrus, T. Kollar, Z. Kira, and\nA. Gaidon, \u201cShAPO: Implicit representations for multi object shape\nappearance and pose optimization,\u201d European Conference on Com-\nputer Vision (ECCV), 2022. 1\n[9] R.-Z. Qiu, Y. Sun, J. M. Correia Marques, and K. Hauser, \u201cReal-\ntime semantic 3D reconstruction for high-touch surface recognition\nfor robotic disinfection,\u201d in Proceedings of the IEEE/RSJ Conference\non Intelligent Robots and Systems (IROS), 2022. 1\n[10] S. Lee, C. Le, W. Jiahao, A. Liniger, S. Kumar, and F. Yu, \u201cUncer-\ntainty guided policy for active robotic 3D reconstruction using neural\nradiance fields,\u201d IEEE Robotics and Automation Letters, 2022. 1\n[11] R. Ni and A. H. Qureshi, \u201cNTFields: Neural time fields for physics-\ninformed robot motion planning,\u201d International Conference on Learn-\ning Representations (ICLR), 2023. 1\n[12] M. Kurenkov, A. Potapov, A. Savinykh, E. Yudin, E. Kruzhkov,\nP. Karpyshev, and D. Tsetserukou, \u201cNFOMP: Neural field for opti-\nmal motion planner of differential drive robots with nonholonomic\nconstraints,\u201d IEEE Robotics and Automation Letters, 2022. 1\n[13] M. Adamkiewicz, T. Chen, A. Caccavale, R. Gardner, P. Culbertson,\nJ. Bohg, and M. Schwager, \u201cVision-Only Robot Navigation in a Neural\nRadiance World,\u201d IEEE Robotics and Automation Letters, 2022. 1\n[14] D. Driess, I. Schubert, P. Florence, Y. Li, and M. Toussaint, \u201cRein-\nforcement learning with neural radiance fields,\u201d Neural Information\nProcessing Systems (NeurIPS), 2022. 1\n[15] D.\nShim,\nS.\nLee,\nand\nH.\nJ.\nKim,\n\u201cSNeRL:\nSemantic-aware\nneural radiance fields for reinforcement learning,\u201d arXiv preprint\narXiv:2301.11520, 2023. 1\n[16] S. Zhong, A. Albini, O. P. Jones, P. Maiolino, and I. Posner, \u201cTouching\na NeRF: Leveraging neural radiance fields for tactile sensory data\ngeneration,\u201d in Conference on Robot Learning, 2023. 1\n[17] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P.\nSrinivasan, J. T. Barron, and H. Kretzschmar, \u201cBlock-NeRF: Scalable\nlarge scene neural view synthesis,\u201d in IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2022. 1, 2\n[18] Z. Yang, S. Manivasagam, Y. Chen, J. Wang, R. Hu, and R. Urtasun,\n\u201cReconstructing objects in-the-wild for realistic sensor simulation,\u201d\nProceedings of the IEEE International Conference on Robotics and\nAutomation (ICRA), 2023. 1\n[19] Sara Fridovich-Keil and Alex Yu, M. Tancik, Q. Chen, B. Recht, and\nA. Kanazawa, \u201cPlenoxels: Radiance fields without neural networks,\u201d\nin IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2022. 1, 2\n[20] T. M\u00fcller, A. Evans, C. Schied, and A. Keller, \u201cInstant neural graphics\nprimitives with a multiresolution hash encoding,\u201d ACM Transactions\non Graphics (TOG), 2022. 1, 2, 4\n[21] P. Hedman, P. P. Srinivasan, B. Mildenhall, J. T. Barron, and P. De-\nbevec, \u201cBaking neural radiance fields for real-time view synthesis,\u201d in\nInternational Conference on Computer Vision (ICCV), 2021. 1, 2\n[22] Z. Chen, T. Funkhouser, P. Hedman, and A. Tagliasacchi, \u201cMo-\nbileNeRF: Exploiting the polygon rasterization pipeline for efficient\nneural field rendering on mobile architectures,\u201d in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2023. 1, 2\n[23] C. Reiser, R. Szeliski, D. Verbin, P. P. Srinivasan, B. Mildenhall,\nA. Geiger, J. T. Barron, and P. Hedman, \u201cMERF: Memory-efficient\nradiance fields for real-time view synthesis in unbounded scenes,\u201d in\narXiv preprint arXiv:2302.12249, 2023. 1, 2\n[24] F. Warburg, E. Weber, M. Tancik, A. Holynski, and A. Kanazawa,\n\u201cNerfbusters: Removing ghostly artifacts from casually captured\nNeRFs,\u201d arXiv preprint arXiv:2304.10532, 2023. 1\n[25] M. Tancik, E. Weber, E. Ng, R. Li, B. Yi, J. Kerr, T. Wang,\nA. Kristoffersen, J. Austin, K. Salahi, A. Ahuja, D. McAllister, and\nA. Kanazawa, \u201cNeRFstudio: A modular framework for neural radiance\nfield development,\u201d in Proceedings of SIGGRAPH, 2023. 1\n[26] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman,\n\u201cZip-NeRF: Anti-aliased grid-based neural radiance fields,\u201d arXiv\npreprint arXiv:2304.06706, 2023. 1, 2\n[27] J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla,\nand P. P. Srinivasan, \u201cMip-NeRF: A multiscale representation for anti-\naliasing neural radiance fields,\u201d International Conference on Computer\nVision (ICCV), 2021. 1\n[28] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman,\n\u201cMip-NeRF 360: Unbounded anti-aliased neural radiance fields,\u201d IEEE\nConference on Computer Vision and Pattern Recognition (CVPR),\n2022. 1, 2, 3\n[29] D. Verbin, P. Hedman, B. Mildenhall, T. Zickler, J. T. Barron, and\nP. P. Srinivasan, \u201cRef-NeRF: Structured view-dependent appearance\nfor neural radiance fields,\u201d IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2022. 1\n[30] K. Zhang, G. Riegler, N. Snavely, and V. Koltun, \u201cNeRF++:\nAnalyzing and improving neural radiance fields,\u201d arXiv preprint\narXiv:2010.07492, 2020. 1\n[31] Z. Wan, C. Richardt, A. Bo\u017ei\u02c7c, C. Li, V. Rengarajan, S. Nam, X. Xi-\nang, T. Li, B. Zhu, R. Ranjan, and J. Liao, \u201cLearning neural duplex\nradiance fields for real-time view synthesis,\u201d in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023. 1, 2\n[32] J. Pan, X. Han, W. Chen, J. Tang, and K. Jia, \u201cDeep mesh reconstruc-\ntion from single RGB images via topology modification networks,\u201d in\nInternational Conference on Computer Vision (ICCV), 2019. 1\n[33] H. Kato, Y. Ushiku, and T. Harada, \u201cNeural 3D mesh renderer,\u201d\nin IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2018. 1\n[34] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang,\n\u201cPixel2Mesh: Generating 3D mesh models from single RGB images,\u201d\nin European Conference on Computer Vision (ECCV), 2018. 1\n[35] E. Sucar, S. Liu, J. Ortiz, and A. J. Davison, \u201ciMAP: Implicit\nmapping and positioning in real-time,\u201d in International Conference\non Computer Vision (ICCV), 2021. 1\n[36] H. Turki, D. Ramanan, and M. Satyanarayanan, \u201cMega-NeRF: Scal-\nable construction of large-scale nerfs for virtual fly-throughs,\u201d in IEEE\nConference on Computer Vision and Pattern Recognition (CVPR),\n2022. 2\n[37] P. Wang, Y. Liu, Z. Chen, L. Liu, Z. Liu, T. Komura, C. Theobalt,\nand W. Wang, \u201cF2-NeRF: Fast neural radiance field training with free\ncamera trajectories,\u201d arXiv preprint arXiv:2303.15951, 2023. 2\n[38] C. Sun, M. Sun, and H.-T. Chen, \u201cDirect voxel grid optimization:\nSuper-fast convergence for radiance fields reconstruction,\u201d in IEEE\nConference on Computer Vision and Pattern Recognition (CVPR),\n2022. 2\n[39] \u2014\u2014, \u201cImproved direct voxel grid optimization for radiance fields\nreconstruction,\u201d arXiv preprint arXiv:2206.05085, 2022. 2\n[40] A. Chen, Z. Xu, A. Geiger, J. Yu, and H. Su, \u201cTensoRF: Tensorial\nradiance fields,\u201d in European Conference on Computer Vision (ECCV),\n2022. 2\n[41] A. Chen, Z. Xu, X. Wei, S. Tang, H. Su, and A. Geiger, \u201cFactor fields:\nA unified framework for neural fields and beyond,\u201d arXiv preprint\narXiv:2302.01226, 2023. 2\n[42] K. Deng, A. Liu, J.-Y. Zhu, and D. Ramanan, \u201cDepth-supervised\nNeRF: Fewer views and faster training for free,\u201d in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June 2022. 2,\n3\n7\n[43] M. Tancik, B. Mildenhall, T. Wang, D. Schmidt, P. P. Srinivasan, J. T.\nBarron, and R. Ng, \u201cLearned initializations for optimizing coordinate-\nbased neural representations,\u201d in IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2021. 2\n[44] A. Yu, V. Ye, M. Tancik, and A. Kanazawa, \u201cPixelNeRF: Neural\nradiance fields from one or few images,\u201d in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2021. 2\n[45] Q. Wang, Z. Wang, K. Genova, P. P. Srinivasan, H. Zhou, J. T. Barron,\nR. Martin-Brualla, N. Snavely, and T. Funkhouser, \u201cIBRNet: Learning\nmulti-view image-based rendering,\u201d in IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2021. 2\n[46] M. M. Johari, Y. Lepoittevin, and F. Fleuret, \u201cGeoNeRF: Generalizing\nnerf with geometry priors,\u201d in IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2022. 2\n[47] S. J. Garbin, M. Kowalski, M. Johnson, J. Shotton, and J. Valentin,\n\u201cFastNeRF: High-fidelity neural rendering at 200fps,\u201d in IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR), 2021.\n2\n[48] K. Wadhwani and T. Kojima, \u201cSqueezeNeRF: Further factorized\nFastNeRF for memory-efficient inference,\u201d in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2022. 2\n[49] C. Reiser, S. Peng, Y. Liao, and A. Geiger, \u201cKiloNeRF: Speeding up\nneural radiance fields with thousands of tiny MLPs,\u201d in International\nConference on Computer Vision (ICCV), 2021. 2\n[50] A. Kurz, T. Neff, Z. Lv, M. Zollh\u00f6fer, and M. Steinberger, \u201cAdaNeRF:\nAdaptive sampling for real-time rendering of neural radiance fields,\u201d\nin European Conference on Computer Vision (ECCV), 2022. 2\n[51] B. Kerbl, G. Kopanas, T. Leimk\u00fchler, and G. Drettakis, \u201c3D Gaussian\nsplatting for real-time radiance field rendering,\u201d ACM Transactions on\nGraphics (TOG), 2023. 2\n[52] A. Yu, R. Li, M. Tancik, H. Li, R. Ng, and A. Kanazawa, \u201cPlenoctrees\nfor real-time rendering of neural radiance fields,\u201d in International\nConference on Computer Vision (ICCV), 2021. 2\n[53] M. Piala and R. Clark, \u201cTermiNeRF: Ray termination prediction for\nefficient neural rendering,\u201d in International Conference on 3D Vision\n(3DV), 2021. 2\n[54] J. Fang, L. Xie, X. Wang, X. Zhang, W. Liu, and Q. Tian, \u201cNeusample:\nNeural sample field for efficient view synthesis,\u201d arXiv preprint\narXiv:2111.15552, 2021. 2\n[55] T. Neff, P. Stadlbauer, M. Parger, A. Kurz, J. H. Mueller, C. R. A.\nChaitanya, A. Kaplanyan, and M. Steinberger, \u201cDONeRF: Towards\nreal-time rendering of compact neural radiance fields using depth\noracle networks,\u201d in Computer Graphics Forum, 2021. 2\n[56] H. Lin, S. Peng, Z. Xu, Y. Yan, Q. Shuai, H. Bao, and X. Zhou,\n\u201cEfficient neural radiance fields for interactive free-viewpoint video,\u201d\nin SIGGRAPH Asia 2022 Conference Papers, 2022. 2\n[57] N. Kondo, Y. Ikeda, A. Tagliasacchi, Y. Matsuo, Y. Ochiai, and S. S.\nGu, \u201cVaxNeRF: Revisiting the classic for voxel-accelerated neural\nradiance field,\u201d arXiv preprint arXiv:2111.13112, 2021. 2\n[58] D. B. Lindell, J. N. Martel, and G. Wetzstein, \u201cAutoInt: Automatic\nintegration for fast neural volume rendering,\u201d in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2021. 2\n[59] L. Wu, J. Y. Lee, A. Bhattad, Y.-X. Wang, and D. Forsyth, \u201cDiver:\nReal-time and accurate neural radiance fields with deterministic in-\ntegration for volume rendering,\u201d in IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2022. 2\n[60] V. Sitzmann, S. Rezchikov, B. Freeman, J. Tenenbaum, and F. Du-\nrand, \u201cLight field networks: Neural scene representations with\nsingle-evaluation rendering,\u201d Neural Information Processing Systems\n(NeurIPS), 2021. 2\n[61] C. O. Smith, H.-X. Yu, S. Zakharov, F. Durand, J. B. Tenenbaum,\nJ. Wu, and V. Sitzmann, \u201cUnsupervised discovery and composition\nof object light fields,\u201d Transactions on Machine Learning Research,\n2023. 2\n[62] T. Yenamandra, A. Tewari, N. Yang, F. Bernard, C. Theobalt, and\nD. Cremers, \u201cFIRe: Fast inverse rendering using directional and signed\ndistance functions,\u201d arXiv preprint arXiv:2203.16284, 2022. 2\n[63] B. Y. Feng, Y. Zhang, D. Tang, R. Du, and A. Varshney, \u201cPRIF:\nPrimary ray-based implicit function,\u201d in European Conference on\nComputer Vision (ECCV), 2022. 2\n[64] T. Aumentado-Armstrong, S. Tsogkas, S. Dickinson, and A. D. Jepson,\n\u201cRepresenting 3D shapes with probabilistic directed distance fields,\u201d\nin IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2022. 2\n[65] T. Houchens, C.-Y. Lu, S. Duggal, R. Fu, and S. Sridhar, \u201cNeuralODF:\nLearning omnidirectional distance fields for 3D shape representation,\u201d\narXiv preprint arXiv:2206.05837, 2022. 2\n[66] Y.-C. Guo, Y.-P. Cao, C. Wang, Y. He, Y. Shan, X. Qie, and S.-H.\nZhang, \u201cVMesh: Hybrid volume-mesh representation for efficient view\nsynthesis,\u201d arXiv preprint arXiv:2303.16184, 2023. 2\n[67] V. Tschernezki, I. Laina, D. Larlus, and A. Vedaldi, \u201cNeural feature\nfusion fields: 3D distillation of self-supervised 2D image representa-\ntions,\u201d arXiv preprint arXiv:2209.03494, 2022. 2\n[68] S. Kobayashi, E. Matsumoto, and V. Sitzmann, \u201cDecomposing NeRF\nfor editing via feature field distillation,\u201d Neural Information Processing\nSystems (NeurIPS), 2022. 2\n[69] J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tan-\ncik, \u201cLeRF: Language embedded radiance fields,\u201d arXiv preprint\narXiv:2303.09553, 2023. 2\n[70] K. Blomqvist, F. Milano, J. J. Chung, L. Ott, and R. Sieg-\nwart, \u201cNeural implicit vision-language feature fields,\u201d arXiv preprint\narXiv:2303.10962, 2023. 2\n[71] N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, and A. Szlam,\n\u201cCLIP-fields: Weakly supervised semantic fields for robotic memory,\u201d\narXiv preprint arXiv:2210.05663, 2022. 2\n[72] S. Zhi, E. Sucar, A. Mouton, I. Haughton, T. Laidlow, and A. J.\nDavison, \u201ciLabel: Interactive neural scene labelling,\u201d arXiv preprint\narXiv:2111.14637, 2021. 2\n[73] S. Zhi, T. Laidlow, S. Leutenegger, and A. J. Davison, \u201cIn-place scene\nlabelling and understanding with implicit scene representation,\u201d in\nInternational Conference on Computer Vision (ICCV), 2021. 2\n[74] A. Mirzaei, Y. Kant, J. Kelly, and I. Gilitschenski, \u201cLaTeRF: Label\nand text driven object radiance fields,\u201d in European Conference on\nComputer Vision (ECCV), 2022. 2\n[75] J. Gu, L. Liu, P. Wang, and C. Theobalt, \u201cStyleNeRF: A style-\nbased 3D-aware generator for high-resolution image synthesis,\u201d arXiv\npreprint arXiv:2110.08985, 2021. 2, 4\n[76] M. Niemeyer and A. Geiger, \u201cGIRAFFE: Representing scenes as\ncompositional generative neural feature fields,\u201d in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2021. 2, 3, 4\n[77] Y. Xue, Y. Li, K. K. Singh, and Y. J. Lee, \u201cGIRAFFE-HD: A\nhigh-resolution 3D-aware generative model,\u201d in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2022. 2\n[78] G. Metzer, E. Richardson, O. Patashnik, R. Giryes, and D. Cohen-Or,\n\u201cLatent-NeRF for shape-guided generation of 3D shapes and textures,\u201d\nin IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 2\n[79] H. Seo, H. Kim, G. Kim, and S. Y. Chun, \u201cDITTO-NeRF: Diffusion-\nbased iterative text to omni-directional 3D model,\u201d arXiv preprint\narXiv:2304.02827, 2023. 2\n[80] E. R. Chan, K. Nagano, M. A. Chan, A. W. Bergman, J. J. Park,\nA. Levy, M. Aittala, S. De Mello, T. Karras, and G. Wetzstein,\n\u201cGenerative novel view synthesis with 3D-aware diffusion models,\u201d\narXiv preprint arXiv:2304.02602, 2023. 2\n[81] A. Tagliasacchi and B. Mildenhall, \u201cVolume rendering digest (for\nNeRF),\u201d arXiv preprint arXiv:2209.02417, 2022. 3\n[82] A. Tewari, O. Fried, J. Thies, V. Sitzmann, S. Lombardi, K. Sunkavalli,\nR. Martin-Brualla, T. Simon, J. M. Saragih, M. Nie\u00dfner, R. Pandey,\nS. R. Fanello, G. Wetzstein, J. Zhu, C. Theobalt, M. Agrawala,\nE. Shechtman, D. B. Goldman, and M. Zollhofer, \u201cState of the art\non neural rendering,\u201d Computer Graphics Forum, vol. 39, no. 2, pp.\n701\u2013727, 2020. 3\n[83] S. M. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos,\nM. Garnelo, A. Ruderman, A. A. Rusu, I. Danihelka, K. Gregor,\nD. P. Reichert, L. Buesing, T. Weber, O. Vinyals, D. Rosenbaum,\nN. Rabinowitz, H. King, C. Hillier, M. Botvinick, D. Wierstra,\nK. Kavukcuoglu, and D. Hassabis, \u201cNeural scene representation and\nrendering,\u201d Science, vol. 360, no. 6394, pp. 1204\u20131210, 2018. 3\n[84] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Om-\nmer, \u201cHigh-resolution image synthesis with latent diffusion models,\u201d\nin IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2022. 3\n[85] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image\nrecognition,\u201d in IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016. 3, 6\n[86] H. Huang, Z. Li, R. He, Z. Sun, and T. Tan, \u201cIntroVAE: Introspective\nvariational autoencoders for photographic image synthesis,\u201d Neural\nInformation Processing Systems (NeurIPS), 2018. 3\n8\n[87] T.\nM\u00fcller,\n\u201cTiny\nCUDA\nneural\nnetwork\nframework,\u201d\n2021,\nhttps://github.com/nvlabs/tiny-cuda-nn. 4\n[88] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimiza-\ntion,\u201d International Conference on Learning Representations (ICLR),\n2015. 4\n[89] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, \u201cThe\nunreasonable effectiveness of deep features as a perceptual metric,\u201d\nin IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2018. 4\n[90] S. Fu*, N. Tamir*, S. Sundaram*, L. Chai, R. Zhang, T. Dekel,\nand P. Isola, \u201cDreamSim: Learning new dimensions of human visual\nsimilarity using synthetic data,\u201d arXiv preprint arXiv:2306.09344,\n2023. 4\n[91] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever,\n\u201cLearning transferable visual models from natural language super-\nvision,\u201d in International Conference on Machine Learning (ICML),\n2021. 4\n[92] C. Chan, F. Durand, and P. Isola, \u201cLearning to generate line draw-\nings that convey geometry and semantics,\u201d in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2022. 4\n[93] Y. Vinker, Y. Alaluf, D. Cohen-Or, and A. Shamir, \u201cCLIPascene: Scene\nsketching with different types and levels of abstraction,\u201d arXiv preprint\narXiv:2211.17256, 2022. 4\n[94] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P.\nKingma, B. Poole, M. Norouzi, D. J. Fleet, and T. Salimans, \u201cImagen\nvideo: High definition video generation with diffusion models,\u201d arXiv\npreprint arXiv:2210.02303, 2022. 4\n[95] Y. He, T. Yang, Y. Zhang, Y. Shan, and Q. Chen, \u201cLatent video\ndiffusion models for high-fidelity video generation with arbitrary\nlengths,\u201d arXiv preprint arXiv:2211.13221, 2022. 4\n[96] H. Wu, E. Zhang, L. Liao, C. Chen, J. H. Hou, A. Wang, W. S.\nSun, Q. Yan, and W. Lin, \u201cExploring video quality assessment on\nuser generated contents from aesthetic and technical perspectives,\u201d in\nInternational Conference on Computer Vision (ICCV), 2023. 4\n[97] B. Mildenhall, P. P. Srinivasan, R. Ortiz-Cayon, N. K. Kalantari,\nR. Ramamoorthi, R. Ng, and A. Kar, \u201cLocal light field fusion:\nPractical view synthesis with prescriptive sampling guidelines,\u201d ACM\nTransactions on Graphics (TOG), 2019. 4\n[98] D.-A. Clevert, T. Unterthiner, and S. Hochreiter, \u201cFast and accurate\ndeep network learning by exponential linear units (ELUs),\u201d arXiv\npreprint arXiv:1511.07289, 2015. 6\n[99] D. Ulyanov, A. Vedaldi, and V. Lempitsky, \u201cInstance normaliza-\ntion: The missing ingredient for fast stylization,\u201d arXiv preprint\narXiv:1607.08022, 2016. 6\n9\n"
  },
  {
    "title": "ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Real Image",
    "link": "https://arxiv.org/pdf/2310.17994.pdf",
    "upvote": "7",
    "text": "ZERONVS: ZERO-SHOT 360-DEGREE VIEW\nSYNTHESIS FROM A SINGLE REAL IMAGE\nKyle Sargent1, Zizhang Li1, Tanmay Shah2, Charles Herrmann2, Hong-Xing Yu1,\nYunzhi Zhang1, Eric Ryan Chan1, Dmitry Lagun2, Li Fei-Fei1, Deqing Sun2, Jiajun Wu1\n1Stanford University, 2Google Research\nABSTRACT\nWe introduce a 3D-aware diffusion model, ZeroNVS, for single-image novel view\nsynthesis for in-the-wild scenes. While existing methods are designed for single\nobjects with masked backgrounds, we propose new techniques to address chal-\nlenges introduced by in-the-wild multi-object scenes with complex backgrounds.\nSpecifically, we train a generative prior on a mixture of data sources that capture\nobject-centric, indoor, and outdoor scenes. To address issues from data mixture\nsuch as depth-scale ambiguity, we propose a novel camera conditioning parame-\nterization and normalization scheme. Further, we observe that Score Distillation\nSampling (SDS) tends to truncate the distribution of complex backgrounds dur-\ning distillation of 360-degree scenes, and propose \u201cSDS anchoring\u201d to improve\nthe diversity of synthesized novel views. Our model sets a new state-of-the-art\nresult in LPIPS on the DTU dataset in the zero-shot setting, even outperform-\ning methods specifically trained on DTU. We further adapt the challenging Mip-\nNeRF 360 dataset as a new benchmark for single-image novel view synthesis,\nand demonstrate strong performance in this setting. Our code and data are at\nhttps://kylesargent.github.io/zeronvs/\n1\nINTRODUCTION\nModels for single-image, 360-degree novel view synthesis (NVS) should produce realistic and di-\nverse results: the synthesized images should look natural and 3D-consistent to humans, and they\nshould also capture the many possible explanations of unobservable regions. This challenging prob-\nlem has typically been studied in the context of single objects without backgrounds, where the re-\nquirements on both realism and diversity are simplified. Recent progresses rely on large datasets of\nhigh-quality object meshes like Objaverse-XL (Deitke et al., 2023) which have enabled conditional\ndiffusion models to produce photorealistic images from a novel view, followed by Score Distillation\nSampling (SDS; Poole et al., 2022) to improve their 3D consistency. Meanwhile, since image diver-\nsity mostly lies in the background, not the object, the ignorance of background significantly lowers\nthe expectation of synthesizing diverse images\u2013in fact, most object-centric methods no longer con-\nsider diversity as a metric (Liu et al., 2023b; Melas-Kyriazi et al., 2023; Qian et al., 2023).\nNeither assumption holds for the more challenging problem of zero-shot, 360-degree novel view\nsynthesis on real-world scenes. There is no single, large-scale dataset of scenes with ground-truth\ngeometry, texture, and camera parameters, analogous to Objaverse-XL for objects. The background,\nwhich cannot be ignored anymore, also needs to be well modeled for synthesizing diverse results.\nWe address both issues with our new model, ZeroNVS. Inspired by previous object-centric meth-\nods (Liu et al., 2023b; Melas-Kyriazi et al., 2023; Qian et al., 2023), ZeroNVS also trains a 2D\nconditional diffusion model followed by 3D distillation. But unlike them, ZeroNVS works well on\nscenes due to two technical innovations: a new camera parametrization and normalization scheme\nfor conditioning, which allows training the diffusion model on a collection of diverse scene datasets,\nand a new \u201cSDS anchoring\u201d mechanism, addressing the limited diversity in scene backgrounds when\nusing standard SDS.\nTo overcome the key challenge of limited training data, we propose training the diffusion model on a\nmassive mixed dataset comprised of all scenes from CO3D (Reizenstein et al., 2021), RealEstate10K\n(Zhou et al., 2018), and ACID (Liu et al., 2021), so that the model may potentially handle complex\narXiv:2310.17994v1  [cs.CV]  27 Oct 2023\nTechnical report\nCO3D\nInput view\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Novel views \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nInput view\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Novel views \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nMip-NeRF 360 (Zero-shot)\nInput view\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Novel views \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nInput view\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Novel views \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nRealEstate10K\nInput view\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Novel views \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nInput view\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Novel views \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nDTU (Zero-shot)\nInput view\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Novel views \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nInput view\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Novel views \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nFigure 1: Results for view synthesis from a single image. All NeRFs are predicted by the same model.\n2\nTechnical report\nin-the-wild scenes. The mixed data of such scale and diversity are captured with a variety of cam-\nera settings and have several different types of 3D ground truth, e.g., computed with COLMAP\n(Sch\u00a8onberger & Frahm, 2016) or ORB-SLAM (Mur-Artal et al., 2015). We show that while the\ncamera conditioning representations from prior methods (Liu et al., 2023b) are too ambiguous or in-\nexpressive to model in-the-wild scenes, our new camera parametrization and normalization scheme\nallows exploiting such diverse data sources and leads to superior NVS on real-world scenes.\nBuilding a 2D conditional diffusion model that works effectively for in-the-wild scenes enables us\nto then study the limitations of SDS in the scene setting. In particular, we observe limited diversity\nfrom SDS in the generated scene backgrounds when synthesizing long-range (e.g., 180-degree)\nnovel views. We therefore propose \u201cSDS anchoring\u201d to ameliorate the issue. In SDS anchoring,\nwe propose to first sample several \u201canchor\u201d novel views using the standard Denoising Diffusion\nImplicit Model (DDIM) sampling (Song et al., 2021). This yields a collection of pseudo-ground-\ntruth novel views with diverse contents, since DDIM is not prone to mode collapse like SDS. Then,\nrather than using these views as RGB supervision, we sample from them randomly as conditions for\nSDS, which enforces diversity while still ensuring 3D-consistent view synthesis.\nZeroNVS achieves strong zero-shot generalization to unseen data. We set a new state-of-the-art\nLPIPS score on the challenging DTU benchmark, even outperforming methods that were directly\nfine-tuned on this dataset. Since the popular benchmark DTU consists of scenes captured by a\nforward-facing camera rig and cannot evaluate more challenging pose changes, we propose to use\nthe Mip-NeRF 360 dataset (Barron et al., 2022) as a single-image novel view synthesis benchmark.\nZeroNVS achieves the best LPIPS performance on this benchmark. Finally, we show the potential\nof SDS anchoring for addressing diversity issues in background generation via a user study.\nTo summarize, we make the following contributions:\n\u2022 We propose ZeroNVS, which enables full-scene NVS from real images. ZeroNVS first\ndemonstrates that SDS distillation can be used to lift scenes that are not object-centric and\nmay have complex backgrounds to 3D.\n\u2022 We show that the formulations on handling cameras and scene scale in prior work are either\ninexpressive or ambiguous for in-the-wild scenes. We propose a new camera conditioning\nparameterization and a scene normalization scheme. These enable us to train a single model\non a large collection of diverse training data consisting of CO3D, RealEstate10K and ACID,\nallowing strong zero-shot generalization for NVS on in-the-wild images.\n\u2022 We study the limitations of SDS distillation as applied to scenes. Similar to prior work,\nwe identify a diversity issue, which manifests in this case as novel view predictions with\nmonotone backgrounds. We propose SDS anchoring to ameliorate the issue.\n\u2022 We show state-of-the-art LPIPS results on DTU zero-shot, surpassing prior methods fine-\ntuned on this dataset. Furthermore, we introduce the Mip-NeRF 360 dataset as a scene-\nlevel single-image novel view synthesis benchmark and analyze the performances of our\nand other methods. Finally, we show that our proposed SDS anchoring is overwhelmingly\npreferred for diverse generations via a user study.\n2\nRELATED WORK\n3D generation. The 3D generative model most relevant to our work is DreamFusion (Poole et al.,\n2022), which proposed Score Distillation Sampling (SDS) as a way of leveraging a diffusion model\nto extract a NeRF given a user-provided text prompt. After DreamFusion, follow-up works such as\nMagic3D (Lin et al., 2023), ATT3D (Lorraine et al., 2023), ProlificDreamer (Wang et al., 2023), and\nFantasia3D (Chen et al., 2023) improved the quality, diversity, resolution, or run-time.\nOther types of 3D generative models include GAN-based 3D generative models, which are primarily\nrestricted to single object categories (Chan et al., 2021a; Niemeyer & Geiger, 2021; Gu et al., 2022;\nChan et al., 2021b; Nguyen-Phuoc et al., 2019; Skorokhodov et al., 2022) or to synthetic data (Gao\net al., 2022). Recently, 3DGP (Skorokhodov et al., 2023) adapted the GAN-based approach to train\n3D generative models on ImageNet. VQ3D (Sargent et al., 2023) and IVID (Xiang et al., 2023)\nleveraged vector quantization and diffusion, respectively, to learn 3D-aware generative models on\nImageNet. Different from the diffusion work outlined above, IVID used mesh-based warping and\ndiffusion inpainting rather than distillation to achieve high-quality synthesis results.\n3\nTechnical report\nSingle-image novel view synthesis. Prior to diffusion models, works typically focused on learning\nimage-based feature extractors which could be trained end-to-end with some level of 3D supervi-\nsion. PixelNeRF (Yu et al., 2021) learns a prior over 3D scenes via training a CNN-based feature\nextractor and differentiable un-projection of a feature frustum inferred from one or more input im-\nages. Similarly, DietNeRF (Jain et al., 2021) can infer NeRFs from one or few images via a training\nstrategy geared towards semantic consistency. Different from these works, ZeroNVS infers novel\nviews which resemble crisp natural images, and furthermore is capable of extensive camera view-\npoint change, i.e., up to 360 degrees of camera motion.\nSeveral diffusion-based approaches have recently emerged for novel view synthesis of objects. One\nprominent paradigm separates novel view synthesis into two stages; first, a (potentially 3D-aware)\ndiffusion model is trained, and second, the diffusion model is used to distill 3D-consistent scene rep-\nresentations given an input image via techniques like score distillation sampling (Poole et al., 2022),\nscore Jacobian chaining (Wang et al., 2022), textual inversion or semantic guidance leveraging the\ndiffusion model (Melas-Kyriazi et al., 2023; Deng et al., 2022a), or explicit 3D reconstruction from\nmultiple sampled views of the diffusion model (Liu et al., 2023a;c). Unlike these works, ZeroNVS\nis trained on large real scene datasets and performs scene-level novel view synthesis.\nOther works more explicitly embed a 3D-aware inductive bias into a unified diffusion model archi-\ntecture; for example, HoloDiffusion (Karnewar et al., 2023) trains a 3D diffusion model to generate\n3D objects. Concurrent works include GenVS (Chan et al., 2023), Forward Models (Tewari et al.,\n2023), and IVID (Xiang et al., 2023). While GenVS and Forward Models train and evaluate models\non one scene category, such as fire hydrants or rooms, at a time, ZeroNVS handles both such scene\ncategories and more in a single model.\nDepth estimation. Though ZeroNVS estimates depth as part of 3D SDS distillation, depth estima-\ntion is not the emphasis of our work. However, our work draws significant philosophical inspiration\nfrom the landmark paper MIDAS (Ranftl et al., 2022). MIDAS demonstrated that selecting a train-\ning objective (scale- and shift-invariant inverse depth estimation) which is compatible with many\ndifferent data sources can significantly increase the amount of training data that can be leveraged.\nThen, even though the model predicts inverse depth maps without a known scale or shift, the strong\nzero-shot performance from leveraging massive datasets means the model is widely used in practice\nafter finetuning (Bhat et al., 2023) or manually choosing reasonable scale and shift estimates (Jam-\npani et al., 2021). Thus, our technical innovations in camera conditioning representation and scene\nnormalization are motivated by the value demonstrated in MIDAS of being able to leverage multiple\ndiverse data sources.\n3\nAPPROACH\nWe consider the problem of scene-level novel view synthesis from a single real image. Similar to\nprior work (Liu et al., 2023b; Qian et al., 2023), we first train a diffusion model p\u03b8 to perform\nnovel view synthesis, and then leverage it to perform 3D SDS distillation. Unlike prior work, we\nfocus on scenes rather than objects. Scenes present several unique challenges. First, prior works\nuse representations for cameras and scale which are either ambiguous or insufficiently expressive\nfor scenes. Second, the inference procedure of prior works is based on SDS, which has a known\nmode collapse issue and which manifests in scenes through greatly reduced background diversity in\npredicted views. We will attempt to address these challenges through improved representations and\ninference procedures for scenes compared with prior work (Liu et al., 2023b; Qian et al., 2023).\nWe shall begin the discussion of our approach by introducing some general notation. Let a scene S\nbe comprised of a set of images X = {Xi}n\ni=1, depth maps D = {Di}n\ni=1, extrinsics E = {Ei}n\ni=1,\nand a shared field-of-view f. We note that an extrinsics matrix Ei can be identified with its rotation\nand translation components, defined by Ei = (ER\ni , ET\ni ). We preprocess the datasets to consist\nof square images and assume intrinsics are shared within a given scene, and that there is no skew,\ndistortion, or off-center principal point.\nWe will focus on the design of the conditional information which is passed to the view synthesis\ndiffusion model p\u03b8 in addition to the input image. This conditional information can be represented\nvia a function, M(D, f, E, i, j), which computes a conditioning embedding given the full sets of\ndepths and extrinsics for the scene, the field of view, and the indices i, j of the input and target view\nrespectively. We learn a generative model over novel views following a parameterized distribution\n4\nTechnical report\n3D scene and cameras\nImages taken by cameras\nCamera A\nCamera B\nCamera B after\n3DoF projection\nImage A\nImage B\n3DoF projection leads\nto an incorrect image\nFigure 2: A 3DoF camera pose captures cam-\nera elevation, azimuth, and radius for a cam-\nera pointed at the origin but is incapable of\nrepresenting a camera\u2019s roll (pictured) or cam-\neras oriented arbitrarily in space. A model with\nthis parameterization cannot be trained on real-\nworld data, where many of the camera poses are\ninadequately represented by a 3DoF pose.\nScale ambiguity in input view leads to \nmultiple plausible novel views\nInput Camera\nNovel Camera\nPossible Scale A\nPossible Scale B\nFigure 3: To a monocular camera, a small object\nclose to the camera (left) and a large object at a\ndistance (right) appear identical, despite repre-\nsenting different scenes. Scale ambiguity in the\ninput view causes ambiguity in novel view syn-\nthesis. Specifically, even after conditioning on\nthe image taken from the input view, an image\ntaken from the novel camera could either have a\nsmall motorcycle on the left (Scale A) or a large\nmotorcycle on the right (Scale B).\np\u03b8, so that we have\nXj \u223c p\u03b8(Xj|Xi, M(D, f, E, i, j)) .\nThe output of M and the (single) input image Xi are the only information available to the model for\nview synthesis.\nBoth Zero-1-to-3 (Section 3.1) and our model, as well as several intermediate models that we will\nstudy (Sections 3.2 and 3.3), can be regarded as different choices for M. As we illustrate in Figures\n2, 3, 4 and 5, and verify later in experiments, different choices for M can have drastic impacts on\nthe model\u2019s performance.\nAt inference time, information such as the full set of depth maps D or extrinsics E for a scene may\nnot be known. But, analogous to MIDAS where the scale and shift for predictions may be unknown,\nwe see that in practice, an approximate guess for the evaluation of M suffices.\n3.1\nREPRESENTING OBJECTS FOR VIEW SYNTHESIS\nZero-1-to-3 (Liu et al., 2023b) represents poses with 3 degrees of freedom, given by an elevation\nangle \u03b8, azimuth angle \u03d5, and radius z. Let P : SE(3) \u2192 R3 be the projection to this representation,\nthen\nMZero\u22121\u2212to\u22123(D, f, E, i, j) = P(Ei) \u2212 P(Ej)\nis the camera conditioning representation used by Zero-1-to-3. For object mesh datasets such as Ob-\njaverse (Deitke et al., 2022) and Objaverse-XL (Deitke et al., 2023), this representation is appropriate\nbecause the data is known to consist of single objects without backgrounds, aligned and centered\nat the origin and imaged from training cameras generated with three degrees of freedom. However,\nsuch a parameterization limits the model\u2019s ability to generalize to non-object-centric images. Zero-\n1-to-3 proposed mitigating the distribution mismatch by applying a foreground segmentation model\nand then centering the content (Qian et al., 2023; Liu et al., 2023a).\nWhile this 3DoF camera parameterization is an effective solution for object-centered and aligned\nimages, it is insufficient for representing cameras for real-world data, where each pose can have six\ndegrees of freedom, incorporating both rotation (pitch, roll, yaw) and 3D translation. An illustra-\ntion of a failure of the 3DoF camera representation due to the camera\u2019s roll is shown in Figure 2.\nMoreover, the use of synthetic object datasets limits the applicability of the model to scenes with a\nforeground object that can be extracted via a segmentation model.\n5\nTechnical report\n3.2\nREPRESENTING GENERIC SCENES FOR VIEW SYNTHESIS\nFor scenes, we should use a camera representation with six degrees of freedom that can capture\nall possible positions and orientations. One straightforward choice for a camera parameterization\nthat captures six degrees of freedom is the relative pose parameterization (Watson et al., 2023). We\npropose to also include the field of view as an additional degree of freedom. We term this combined\nrepresentation \u201c6DoF+1\u201d. This gives us\nM6DoF+1(D, f, E, i, j) = [E\u22121\ni\nEj, f].\nOne attractive property of M6DoF+1 is that it is invariant with respect to a rigid transformation \u02dcE of\nthe scene, so that we have\nM6DoF+1(D, f, \u02dcE \u00b7 E, i, j) = [( \u02dcE \u00b7 Ei)\u22121 \u02dcE \u00b7 Ej, f] = [E\u22121\ni\nEj, f] ,\nimplying M6DoF+1 is invariant to translating the scene center and/or rotating the scene. This is use-\nful given the arbitrary nature of the poses for our datasets, namely CO3D, ACID, and RealEstate10K,\nwhich are determined by COLMAP or ORB-SLAM. The poses discovered via these algorithms are\nnot related to any semantically meaningful alignment of the scene\u2019s content, such as a rigid transfor-\nmation and scale transformation, which align the scene to some canonical frame and unit of scale.\nAlthough we have seen that M6DoF+1 is invariant to rigid transformations of the scene, it is not\ninvariant to scale. The scene scales determined by COLMAP and ORB-SLAM are also arbitrary,\nand in practice may vary by orders of magnitude. One solution is to simply normalize the camera\nlocations to have, on average, the unit norm when the mean of the camera locations is chosen as the\norigin. Let R(E, \u03bb) : SE(3) \u00d7 R \u2192 SE(3) be a function that scales the translation component of\nthe extrinsic matrix E by \u03bb. Then we define\ns = 1\nn\nn\nX\ni=1\n\u2225ET\ni \u2212 1\nn\nn\nX\nj=1\nET\nj \u22252 ,\nM6DoF+1, norm.(D, f, E, i, j) =\nh\nR\n\u0010\nEi, 1\ns\n\u0011\u22121\nR\n\u0010\nEj, 1\ns\n\u0011\n, f)\ni\n,\nwhere s is the average norm of the camera locations when the mean of the camera locations is chosen\nas the origin. In M6DoF+1, norm., the camera locations are normalized via rescaling by 1\ns, in contrast\nto M6DoF+1 where the scales are arbitrary. This choice of M assures that scenes from our mixture\nof datasets will have similar scales.\n3.3\nADDRESSING SCALE AMBIGUITY WITH A NEW NORMALIZATION SCHEME\nThe representation M6DoF+1, norm. achieves reasonable performance on real scenes by addressing\nissues in prior representations with limited degrees of freedom and handling of scale. However,\nperformance can be further improved. In this section, we show that a more effective normalization\nscheme that better addresses scale ambiguity leads to improved performance.\nThe scene scale is ambiguous given a monocular input image. This ambiguity has been discussed\nextensively in the context of monocular estimation (Ranftl et al., 2022; Yin et al., 2022), but is\nalso present in novel view synthesis, as demonstrated by Figure 3. Sampling a novel view via\nconditioning with a representation like M6DoF+1, norm. that contains no information about the scale\nof visible content in the input image amounts to sampling an image from the distribution of images\nmarginalizing over the unknown scale. This leads to more uncertain novel view synthesis, as can\nbe seen in Figure 4), and additionally to poorer 3D distillation performance, as we show later in\nexperiments.\nWe instead choose to condition on the scale by introducing information about the scale of the visible\ncontent to our conditioning embedding function M. Rather than normalize by camera locations,\nStereo Magnification (Zhou et al., 2018) takes the 5-th quantile of each depth map of the scene, and\nthen takes the 10-th quantile of this aggregated set of numbers, and declares this as the scene scale.\nLet Qk be a function which takes the k-th quantile of a set of numbers, then we define\nq = Q10({Q5(Di)}n\ni=1) ,\nM6DoF+1, agg.(D, f, E, i, j) =\nh\nR\n\u0010\nEi, 1\nq\n\u0011\u22121\nR\n\u0010\nEj, 1\nq\n\u0011\n, f\ni\n,\n6\nTechnical report\nInput view\nGT target view\nPredicted target view\nVariance heatmap\nLess variance \n               More variance\nFigure 4: For each normalization scheme, we\nshow heatmaps of the variance of the Sobel edge\nmaps of multiple samples from ZeroNVS. Our\nproposed scheme M6DoF+1, viewer reduces ran-\ndomness due to scale ambiguity.\nCamera setup 1\nCamera C\nCamera A\nCamera B\nCamera A\nCamera B\nAddition of Camera C under Stereo Magnification \nnormalization drastically changes the scene\u2019s scale.\nCamera setup 2\nFigure 5: Top: A scene with two cameras fac-\ning the object. Bottom: The same scene with\na new camera added facing the ground. Aggre-\ngating the cameras before computing the scale\nwould lead to different computed scales for each\nsetup. Our proposed viewer-centric normaliza-\ntion avoids this and reduces scale ambiguity.\nwhere in M6DoF+1, agg., q is the scale applied to the translation component of the scene\u2019s cameras\nbefore computing the relative pose. In this way M6DoF+1, agg. is different from M6DoF+1, norm.\nbecause the camera conditioning representation contains information about the scale of the visible\ncontent from the depth maps Di in addition to the change in orientation between the input and target\nview. Although conditioning on the scale in this way improves performance, there are two issues\nwith M6DoF+1, agg.. The first arises from aggregating the quantiles over all the images. In Figure\n5, adding an additional Camera C to the scene changes the value of M6DoF+1, agg. despite nothing\nelse having changed about the scene. This makes the view synthesis task from either Camera A or\nCamera B more ambiguous. To ensure this is impossible, we can simply eliminate the aggregation\nstep over the quantiles of all depth maps in the scene.\nThe second issue arises from different depth statistics within the mixture of datasets we use for train-\ning. Both COLMAP and ORB-SLAM infer sparse point clouds for scenes which are then rendered at\nthe inferred poses to produce the sparse depth maps Di. However, ORB-SLAM generally produces\nsparser depth maps than COLMAP, and therefore the value of Qk may have different meanings for\neach dataset. We therefore use an off-the-shelf depth estimator (Ranftl et al., 2021) to fill holes in\nthe depth maps. We denote the depth Di infilled in this way as \u00afDi. We then apply Qk to dense depth\nmaps \u00afDi instead. We emphasize that the depth estimator is not used during inference or distillation.\nIts purpose is only for the model to learn a consistent definition of scale during training, which it\nmay then apply to generate low-variance predictions (see Figure 4).\nThese two fixes lead to our proposed normalization, which is fully viewer-centric. We define it as\nqi = Q20( \u00afDi) ,\nM6DoF+1, viewer(D, f, E, i, j) =\nh\nR\n\u0010\nEi, 1\nqi\n\u0011\u22121\nR\n\u0010\nEj, 1\nqi\n\u0011\n, f\ni\n,\nwhere in M6DoF+1, viewer, the scale qi applied to the cameras is dependent only on the depth map\nin the input view \u00afDi, different from M6DoF+1, agg. where the scale q computed by aggregating\nover all Di. At inference, analogous to scale and shift for MIDAS, the value of qi can be chosen\nheuristically without compromising performance. Correcting for the scale ambiguities in this way\neliminates one source of randomness for view synthesis. This leads to improved metrics, which we\nshow in Section 4.\n3.4\nIMPROVING DIVERSITY WITH SDS ANCHORING\nDiffusion models trained with the improved camera conditioning representation M6DoF+1, viewer\nachieve superior view synthesis results via 3D SDS distillation.\nHowever, for large viewpoint\nchanges, novel view synthesis is also a generation problem, and it may be desirable to generate\ndiverse and plausible contents rather than contents that are only optimal on average for metrics such\nas PSNR, SSIM, and LPIPS. However, Poole et al. (2022) noted that even when the underlying gen-\nerative model produces diverse images, SDS distillation of that model tends to seek a single mode.\n7\nTechnical report\nStandard SDS\nOur SDS anchoring\nSDS guidance \nDDIM sampling\nInput view\nNeRF\nInput view\nNeRF\nGenerated view\nGenerated view\nFigure 6: SDS-based NeRF distillation (left) uses the same guidance image for all 360 degrees of\nnovel views. Our \u201cSDS anchoring\u201d (right) first samples novel views via DDIM (Song et al., 2020),\nand then uses the nearest image (whether the input or a sampled novel view) for guidance.\nFor novel view synthesis of scenes via SDS, we observe a unique manifestation of this diversity is-\nsue: lack of diversity is especially apparent in inferred backgrounds. Often, SDS distillation predicts\na gray or monotone background for regions not observed by the input camera.\nTo remedy this, we propose \u201cSDS anchoring\u201d (Figure 6). Typically, when using a view-conditioned\ndiffusion model for novel view synthesis, we optimize an SDS objective for renderings \u02dcX with the\ndiffusion model \u03f5\u03b8 conditioned on the input view Xi. We have\nLSDS( \u02dcX) = \u2225w(\u03c3)(\u03f5\u03b8( \u02dcX + \u03f5, Xi, M(D, f, E, i, j)) \u2212 \u03f5)\u22252\n2,\nwhere \u03c3 \u223c U(\u03c3min, \u03c3max), and \u03f5 \u223c N(0, \u03c32I). With SDS anchoring, we first directly sample,\nvia iterative denoising, k novel views \u02c6\nXk = { \u02c6Xj}k\nj=1 with \u02c6Xj \u223c p(Xj|Xi, M(D, f, E, i, j))\nfrom poses evenly spaced in azimuth for maximum scene coverage. Each novel view is generated\nconditional on the input view. Then, when optimizing the SDS objective, we condition the diffusion\nmodel not on the input view, but on the nearest view in the geodesic distance on SO(3) from \u02c6\nXk \u222a\n{Xi}, which we define as Xinearest. Then we have\nLSDS anchoring( \u02dcX) = \u2225w(\u03c3)(\u03f5\u03b8( \u02dcX + \u03f5, Xinearest, M(D, f, E, i, j)) \u2212 \u03f5)\u22252\n2.\nAlthough SDS anchoring might in principle produce 3D-inconsistent scenes, we see that in practice\nit tends to smooth out inconsistencies. As shown in Section 4, SDS anchoring produces more diverse\nbackground contents. We provide more details about the setup of SDS anchoring in Appendix B.\n4\nEXPERIMENTS\nWe first describe the setup of our model and experiments. Then we cover our main experimental\nresults on zero-shot 3D consistent novel view synthesis. We conduct extensive ablations and finally\nprovide qualitative analysis of our contributions and design choices.\n4.1\nSETUP\nDatasets. Our models are trained on a mixture dataset consisting of CO3D (Reizenstein et al.,\n2021), ACID (Liu et al., 2021), and RealEstate10K (Zhou et al., 2018). Each example is sampled\nuniformly at random from the three datasets. We train at 256 \u00d7 256 resolution, center-cropping and\nadjusting the intrinsics for each image and scene as necessary. We train with our camera conditioning\nrepresentation M6DoF+1, viewer unless otherwise specified, i.e., as in ablation. To train on this large\ndataset, we implement a high-performance dataloader using WebDataset (Breuel, 2020). We provide\nmore training details in Appendix A.\nWe evaluate our trained diffusion models on held-out subsets of CO3D, ACID, and RealEstate10K\nrespectively, for 2D novel view synthesis. Our main evaluations are for zero-shot 3D consistent\nnovel view synthesis, where we compare against other techniques on the DTU benchmark (Aan\u00e6s\net al., 2016) and on the Mip-NeRF 360 dataset (Barron et al., 2022). We evaluate all models at\n256 \u00d7 256 resolution except for DTU, for which we use 400 \u00d7 300 resolution to be comparable to\nprior art.\n8\nTechnical report\nInput view\nGT novel view\nZeroNVS (ours)\nPixelNeRF\nPSNR=10.8, SSIM=0.22\nPSNR=12.2, SSIM=0.30\nFigure 7: Limitations of PSNR and SSIM for view synthesis evaluation. Misalignments can lead to\nworse PSNR and SSIM values for predictions that are more semantically sensible.\nGT novel view\nZero-1-to-3\nNerDi\nZeroNVS (ours)\nFigure 8: Qualitative comparison between baseline methods and our method.\nImplementation details. Our diffusion model training code is written in PyTorch and based on the\npublic code for Zero-1-to-3 (Liu et al., 2023b). We initialize from the pretrained Zero-1-to-3-XL,\nswapping out the conditioning module to accommodate our novel parameterizations. Our distillation\ncode is implemented in Threestudio (Guo et al., 2023). We use a custom NeRF network combining\nvarious features of Mip-NeRF 360 with Instant-NGP (M\u00a8uller et al., 2022). The noise schedule is\nannealed following Wang et al. (2023). For details, please consult Appendix B.\n4.2\nMAIN RESULTS\nWe evaluate all methods using the standard set of novel view synthesis metrics: PSNR, SSIM, and\nLPIPS. We weigh LPIPS more heavily in the comparison due to the well-known issues with PSNR\nand SSIM as discussed in (Deng et al., 2022a; Chan et al., 2023). We confirm that PSNR and SSIM\ndo not correlate well with performance in our problem setting, as illustrated in Figure 7.\nThe results are shown in Table 1. We first compare against baseline methods DS-NeRF (Deng et al.,\n2022b), PixelNeRF (Yu et al., 2021), SinNeRF (Xu et al., 2022), DietNeRF (Jain et al., 2021),\nand NeRDi (Deng et al., 2022a) on DTU. Although all these methods are trained on DTU, we\nachieve a state-of-the-art LPIPS zero-shot, having never trained on DTU. We show some qualitative\ncomparisons in Figure 8.\nDTU scenes are limited to relatively simple forward-facing scenes. Therefore, we introduce a more\nchallenging benchmark dataset, the Mip-NeRF 360 dataset, to benchmark the task of 360-degree\nview synthesis from a single image. We use this benchmark as a zero-shot benchmark, and train\nthree baseline models on our mixture dataset to compare zero-shot performance. Restricting to these\nzero-shot models, our method is the best on LPIPS for this dataset by a wide margin. On DTU, we\nexceed Zero-1-to-3 and the zero-shot PixelNeRF model on all metrics, not just LPIPS. Performance\nis shown in Table 2. All numbers for our method and Zero-1-to-3 are for NeRFs predicted from SDS\ndistillation unless otherwise noted.\n9\nTechnical report\nSeed=0\nSeed=1\nSeed=2\nNovel views (standard SDS)\nNovel views (SDS anchoring)\nFigure 9: Qualitative depiction of the effects of SDS anchoring. Whereas standard SDS (left) tends\nto predict monotonous backgrounds, our SDS anchoring (right) generates more diverse background\ncontents. Additionally, SDS anchoring generates different results depending on the random seed,\nwhile standard SDS is prone to mode collapse.\nNVS on DTU\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nDS-NeRF\u2020\n0.649\n12.17\n0.410\nPixelNeRF\n0.535\n15.55\n0.537\nSinNeRF\n0.525\n16.52\n0.560\nDietNeRF\n0.487\n14.24\n0.481\nNeRDi\n0.421\n14.47\n0.465\nZeroNVS (ours)\n0.380\n13.55\n0.469\nTable 1: SOTA comparison.\nWe set a new\nstate-of-the-art for LPIPS on DTU despite be-\ning the only method not fine-tuned on DTU. \u2020 =\nPerformance reported in Xu et al. (2022).\nNVS\nLPIPS \u2193 PSNR \u2191 SSIM \u2191\nMip-NeRF 360 Dataset\nZero-1-to-3\n0.667\n11.7\n0.196\nPixelNeRF\n0.718\n16.5\n0.556\nZeroNVS (ours)\n0.625\n13.2\n0.240\nDTU Dataset\nZero-1-to-3\n0.472\n10.70\n0.383\nPixelNeRF\n0.738\n10.46\n0.397\nZeroNVS (ours)\n0.380\n13.55\n0.469\nTable 2: Zero-shot comparison. Comparison\nwith baselines trained on our mixture dataset.\nLimited diversity is a known issue with SDS-based methods, but the long run time makes typical\ngeneration-based metrics such as FID cost-prohibitive. Therefore, we quantify the improved diver-\nsity from using SDS anchoring via a user study on the Mip-NeRF 360 dataset. A total of 21 users\nwere asked to rate each inferred scene from both ZeroNVS and ZeroNVS with anchoring, based\non the scene\u2019s realism, creativity, and their overall preference. The results, shown in Table 3, show\na strong human preference for the more diverse scenes generated via SDS anchoring. In addition,\nFigure 9 includes qualitative examples that show the advantages of SDS anchoring.\n4.3\nABLATION STUDIES\nWe verify the benefits of using multiple multiview scene datasets in Table 4. Removing either CO3D,\nACID, or RealEstate10K results in a model that performs worse than using all three, even for the\nDTU dataset, where ACID or RealEstate10K might be expected to be mostly out-of-distribution.\nThis confirms the importance of diverse data.\nIn Table 5, we analyze the diffusion model\u2019s performance on the held-out subsets of our datasets,\nwith the various parameterizations discussed in Section 3. We see that as the conditioning parameter-\nization is further refined, the performance continues to increase. Due to computational constraints,\nwe train the ablation diffusion models for fewer steps than our main model, hence the slightly worse\nperformance relative to Table 1.\nWe provide more details on experimental setups in Appendix C.\n10\nTechnical report\nUser study\n% that prefer SDS anchoring\nRealism\n78%\nCreativity\n82%\nOverall\n80%\nTable 3: The effectiveness of SDS anchoring.\nUsers prefer scenes distilled with SDS anchor-\ning, especially in terms of creativity.\nNVS on DTU\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nAll datasets\n0.421\n12.2\n0.444\n-ACID\n0.446\n11.5\n0.405\n-CO3D\n0.456\n10.7\n0.407\n-RealEstate10K\n0.435\n12.0\n0.429\nTable 4:\nAblation study on training data.\nTraining on all datasets improves performance.\n2D novel view synthesis\n3D NeRF distillation\nCO3D\nRealEstate10K\nACID\nDTU\nConditioning\nPSNR\nSSIM\nLPIPS\nPSNR\nSSIM\nLPIPS\nPSNR\nSSIM\nLPIPS\nPSNR\nSSIM\nLPIPS\nMZero\u22121\u2212to\u22123\n12.0\n.366\n.590\n11.7\n.338\n.534\n15.5\n.371\n.431\n10.3\n.384\n.477\nM6DoF+1\n12.2\n.370\n.575\n12.5\n.380\n.483\n15.2\n.363\n.445\n9.5\n.347\n.472\nM6DoF+1, norm.\n12.9\n.392\n.542\n12.9\n.408\n.450\n16.5\n.398\n.398\n11.5\n.422\n.421\nM6DoF+1, agg.\n13.2\n.402\n.527\n13.5\n.441\n.417\n16.9\n.411\n.378\n12.2\n.436\n.420\nM6DoF+1, viewer\n13.4\n.407\n.515\n13.5\n.440\n.414\n17.1\n.415\n.368\n12.2\n.444\n.421\nTable 5: Ablation study on the conditioning representation M. Our conditioning representation\n(M6DoF+1, viewer) matches or outperforms other conditioning representations.\n5\nCONCLUSION\nWe have introduced ZeroNVS, a system for 3D-consistent novel view synthesis from a single image\nfor generic scenes. We showed its state-of-the-art performance on existing scene-level novel view\nsynthesis benchmarks and introduced a new and more challenging benchmark, the Mip-NeRF 360\ndataset. ZeroNVS can be easily integrated into the pipelines of existing works that leverage 3D-\naware diffusion models for downstream applications.\nREFERENCES\nHenrik Aan\u00e6s, Rasmus Ramsb\u00f8l Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl.\nLarge-scale data for multiple-view stereopsis. International Journal of Computer Vision, pp. 1\u2013\n16, 2016.\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-NeRF\n360: Unbounded anti-aliased neural radiance fields. In CVPR, 2022.\nShariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias M\u00a8uller. ZoeDepth: Zero-\nshot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023.\nThomas Breuel. Webdataset library, 2020.\nEric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-GAN: Periodic\nimplicit generative adversarial networks for 3D-aware image synthesis. In CVPR, 2021a.\nEric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello,\nOrazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon\nWetzstein. Efficient geometry-aware 3D generative adversarial networks. In CVPR, 2021b.\nEric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel\nLevy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. GeNVS: Generative\nnovel view synthesis with 3D-aware diffusion models. In ICCV, 2023.\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3D: Disentangling geometry and\nappearance for high-quality text-to-3D content creation. In ICCV, 2023.\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig\nSchmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of anno-\ntated 3D objects. arXiv preprint arXiv:2212.08051, 2022.\n11\nTechnical report\nMatt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati,\nAlan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha\nKembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi.\nObjaverse-XL: A universe of 10M+ 3D objects. arXiv preprint arXiv:2307.05663, 2023.\nCongyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir\nAnguelov, et al. NeRDi: Single-view NeRF synthesis with language-guided diffusion as general\nimage priors. In CVPR, 2022a.\nKangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised NeRF: Fewer\nviews and faster training for free. In CVPR, 2022b.\nJun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan\nGojcic, and Sanja Fidler. GET3D: A generative model of high quality 3D textured shapes learned\nfrom images. In NeurIPS, 2022.\nJiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. StyleNeRF: A Style-based 3D-aware\nGenerator for High-resolution Image Synthesis. In ICLR, 2022.\nYuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-\nHao Chen, Zi-Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: A unified\nframework for 3D content generation, 2023.\nAjay Jain, Matthew Tancik, and Pieter Abbeel. Putting NeRF on a diet: Semantically consistent\nfew-shot view synthesis. In ICCV, 2021.\nVarun Jampani, Huiwen Chang, Kyle Sargent, Abhishek Kar, Richard Tucker, Michael Krainin,\nDominik Kaeser, William T. Freeman, David Salesin, Brian Curless, and Ce Liu. SLIDE: Single\nimage 3D photography with soft layering and depth-aware inpainting. In ICCV, 2021.\nAnimesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy Mitra. HoloDiffusion: Training a\n3D diffusion model using 2D images. In ICCV, 2023.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-3D con-\ntent creation. In CVPR, 2023.\nAndrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo\nKanazawa. Infinite nature: Perpetual view generation of natural scenes from a single image.\nIn ICCV, 2021.\nMinghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su.\nOne-2-3-45: Any single image to 3D mesh in 45 seconds without per-shape optimization. arXiv\npreprint arXiv:2306.16928, 2023a.\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-3: Zero-shot one image to 3D object. In CVPR, 2023b.\nYuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang.\nSyncDreamer: Learning to generate multiview-consistent images from a single-view image. arXiv\npreprint arXiv:2309.03453, 2023c.\nJonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp,\nTsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, and James Lucas. ATT3D: Amortized text-to-3D object\nsynthesis. In ICCV, 2023.\nLuke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. RealFusion: 360\u00b0 recon-\nstruction of any object from a single image. In CVPR, 2023.\nThomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics prim-\nitives with a multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013102:15, July 2022.\nRa\u00b4ul Mur-Artal, J. M. M. Montiel, and Juan D. Tard\u00b4os. ORB-SLAM: A versatile and accurate\nmonocular SLAM system. IEEE Transactions on Robotics, 31(5):1147\u20131163, 2015.\n12\nTechnical report\nThu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. HoloGAN:\nUnsupervised learning of 3D representations from natural images. In ICCV, 2019.\nMichael Niemeyer and Andreas Geiger. GIRAFFE: Representing scenes as compositional genera-\ntive neural feature fields. In CVPR, 2021.\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D\ndiffusion. In ICLR, 2022.\nGuocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying\nLee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard Ghanem. Magic123: One\nimage to high-quality 3D object generation using both 2D and 3D diffusion priors. arXiv preprint\narXiv:2306.17843, 2023.\nRen\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.\nIn ICCV, 2021.\nRen\u00b4e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust\nmonocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence (TPAMI), 44(3), 2022.\nJeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and\nDavid Novotny.\nCommon objects in 3D: Large-scale learning and evaluation of real-life 3D\ncategory reconstruction. In ICCV, 2021.\nKyle Sargent, Jing Yu Koh, Han Zhang, Huiwen Chang, Charles Herrmann, Pratul Srinivasan, Jiajun\nWu, and Deqing Sun. VQ3D: Learning a 3D-aware generative model on ImageNet. In ICCV,\n2023.\nJohannes Lutz Sch\u00a8onberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR,\n2016.\nIvan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Peter Wonka. EpiGRAF: Rethinking training\nof 3D GANs. In NeurIPS, 2022.\nIvan Skorokhodov, Aliaksandr Siarohin, Yinghao Xu, Jian Ren, Hsin-Ying Lee, Peter Wonka, and\nSergey Tulyakov. 3D generation on ImageNet. In ICLR, 2023.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv::2010.02502, 2020.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR,\n2021.\nAyush Tewari, Tianwei Yin, George Cazenavette, Semon Rezchikov, Joshua B. Tenenbaum, Fr\u00b4edo\nDurand, William T. Freeman, and Vincent Sitzmann. Diffusion with forward models: Solving\nstochastic inverse problems without direct supervision. arXiv preprint arXiv:2306.11719, 2023.\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score Ja-\ncobian chaining: Lifting pretrained 2D diffusion models for 3D generation.\narXiv preprint\narXiv:2212.00774, 2022.\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-\nDreamer: High-fidelity and diverse text-to-3D generation with variational score distillation. arXiv\npreprint arXiv:2305.16213, 2023.\nDaniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mo-\nhammad Norouzi. Novel view synthesis with diffusion models. In ICLR, 2023.\nJianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin Tong. 3D-aware image generation using 2D\ndiffusion models. In ICCV, 2023.\nDejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, and Zhangyang Wang. SinNeRF:\nTraining neural radiance fields on complex scenes from a single image. In ECCV, 2022.\n13\nTechnical report\nWei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Simon Chen, Yifan Liu, and Chunhua\nShen. Towards accurate reconstruction of 3D scene shape from a single monocular image. IEEE\nTransactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022.\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields\nfrom one or few images. In CVPR, 2021.\nTinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification:\nLearning view synthesis using multiplane images. ACM Trans. Graph. (Proc. SIGGRAPH), 37,\n2018.\n14\nTechnical report\nA\nDETAILS: DIFFUSION MODEL TRAINING\nA.1\nMODEL\nWe train diffusion models for various camera conditioning parameterizations: MZero\u22121\u2212to\u22123,\nM6DoF+1, M6DoF+1, norm., M6DoF+1, agg., and M6DoF+1, viewer. Our runtime is identical to Zero-\n1-to-3 (Liu et al., 2023b) as the camera conditioning novelties we introduce add negligible overhead\nand can be done mainly in the dataloader. We train our main model for 60, 000 steps with batch size\n1536. We find that performance tends to saturate after about 20, 000 steps for all models, though it\ndoes not decrease. For inference of the 2D diffusion model, we use 500 DDIM steps and guidance\nscale 3.0.\nDetails for M6DoF+1: To embed the field of view f in radians, we use a 3-dimensional vector\nconsisting of [f, sin(f), cos(f)]. When concatenated with the 4 \u00d7 4 = 16-dimensional relative pose\nmatrix, this gives a 19-dimensional conditioning vector.\nDetails for M6DoF+1, viewer: We use the DPT-SwinV2-256 depth model (Ranftl et al., 2021) to in-\nfill depth maps from ORB-SLAM and COLMAP on the ACID, RealEstate10K, and CO3D datasets.\nWe infill the invalid depth map regions only after aligning the disparity from the monodepth esti-\nmator to the ground-truth sparse depth map via the optimal scale and shift following Ranftl et al.\n(2022). We downsample the depth map 4\u00d7 so that the quantile function is evaluated quickly.\nAt inference time, the value of Q20( \u00afD) may not be known since input depth map D is unknown.\nTherefore there is a question of how to compute the conditioning embedding at inference time.\nValues of Q20( \u00afD) between .7 \u2212 1. work for most images and it can be chosen heuristically. For\ninstance, for DTU we uniformly assume a value of .7, which seems to work well. Note that any\nvalue of Q20( \u00afD) is presumably possible; it is only when this value is incompatible with the desired\nSDS camera radius that distillation may fail, since the cameras may intersect the visible content.\nA.2\nDATALOADER\nOne significant engineering component of our work is our design of a streaming dataloader for mul-\ntiview data, built on top of WebDataset (Breuel, 2020). Each dataset is sharded and each shard\nconsists of a sequential tar archive of scenes. The shards can be streamed in parallel via multipro-\ncessing. As a shard is streamed, we yield random pairs of views from scenes according to a \u201crate\u201d\nparameter that determines how densely to sample each scene. This parameter allows a trade-off\nbetween fully random sampling (lower rate) and biased sampling (higher rate) which can be tuned\naccording to the available network bandwidth. Individual streams from each dataset are then com-\nbined and sampled randomly to yield the mixture dataset. We will release the code together with our\nmain code release.\nB\nDETAILS: NERF PREDICTION AND DISTILLATION\nB.1\nSDS ANCHORING\nWe propose SDS anchoring in order to increase the diversity of synthesized scenes. We sample 2\nanchors at 120 and 240 degrees of azimuth relative to the input camera.\nOne potential issue with SDS anchoring is that if the samples are 3D-inconsistent, the resulting\ngenerations may look unusual. Furthermore, traditional SDS already performs quite well except if\nthe criterion is diverse backgrounds. Therefore, to implement anchoring, we randomly choose with\nprobability .5 either the input camera and view or the nearest sampled anchor camera and view as\nguidance. If the guidance is an anchor, we \u201dgate\u201d the gradients flowing back from SDS according to\nthe depth of the NeRF render, so that only depths above a certain threshold (1.0 in our experiments)\nreceive guidance from the anchors. This seems to mostly mitigate artifacts from 3D-inconsistency\nof foreground content, while still allowing for rich backgrounds. We show video results for SDS\nanchoring on the webpage.\n15\nTechnical report\nB.2\nHYPERPARAMETERS\nNeRF distillation via involves numerous hyperparameters such as for controlling lighting, shading,\ncamera sampling, number of training steps, training at progressively increasing resolutions, loss\nweights, density blob initializations, optimizers, guidance weight, and more. We will share a few\ninsights about choosing hyperparameters for scenes here, and release the full configs as part of our\ncode release.\nNoise scheduling:\nWe found that ending training with very low maximum noise levels such as\n.025 seemed to benefit results, particularly perceptual metrics like LPIPS. We additionaly found a\nsignificant benefit on 360-degree scenes such as in the Mip-NeRF 360 dataset to scheduling the\nnoise \u201danisotropically;\u201d that is, reducing the noise level more slowly on the opposite end from the\ninput view. This seems to give the optimization more time to solve the challenging 180-degree views\nat higher noise levels before refining the predictions at low noise levels.\nMiscellaneous:\nProgressive azimuth and elevation sampling following (Qian et al., 2023) was also\nfound to be very important for training stability. Training resolution progresses stagewise, first with\nbatch size 6 at 128x128 and then with batch size 1 at 256 \u00d7 256.\nC\nEXPERIMENTAL SETUPS\nFor our main results on DTU and Mip-NeRF 360, we train our model and Zero-1-to-3 for 60, 000\nsteps. Performance for our method seems to saturate earlier than for Zero-1-to-3, which trained for\nabout 100, 000 steps; this may be due to the larger dataset size. Objaverse, with 800, 000 scenes,\nis much larger than the combination of RealEstate10K, ACID, and CO3D, which are only about\n95, 000 scenes in total.\nFor the retrained PixelNeRF baseline, we retrained it on our mixture dataset of CO3D, ACID, and\nRealEstate10K for about 560, 000 steps.\nC.1\nMAIN RESULTS\nFor all single-image NeRF distillation results, we assume the camera elevation, field of view, and\ncontent scale are given. These parameters are identical for all DTU scenes but vary across the Mip-\nNeRF 360 dataset. For DTU, we use the standard input views and test split from from prior work. We\nselect Mip-NeRF 360 input view indices manually based on two criteria. First, the views are well-\napproximated by a 3DoF pose representation in the sense of geodesic distance between rotations.\nThis is to ensure fair comparison with Zero-1-to-3, and for compatibility with Threestudio\u2019s SDS\nsampling scheme, which also uses 3 degrees of freedom. Second, as much of the scene content as\npossible must be visible in the view. The exact values of the input view indices are given in Table 6.\nThe field of view is obtained via COLMAP. The camera elevation is set automatically via computing\nthe angle between the forward axis of the camera and the world\u2019s XY -plane, after the cameras have\nbeen standardized via PCA following Barron et al. (2022).\nOne challenge is that for both the Mip-NeRF 360 and DTU datasets, the scene scales are not known\nby the zero-shot methods, namely Zero-1-to-3, our method, and our retrained PixelNeRF. There-\nfore, for the zero-shot methods, we manually grid search for the optimal world scale in intervals\nof .1 to find the appropriate world scale for each scene in order to align the predictions to the gen-\nerated scenes. Between five to nine samples within [.3, .4, .5, .6, .7, .8, .9, 1., 1.1, 1.2, 1.3, 1.4, 1.5]\ngenerally suffices to find the appropriate scale. Even correcting for the scale misalignment issue in\nthis way, the zero-shot methods generally do worse on pixel-aligned metrics like SSIM and PSNR\ncompared with methods that have been fine-tuned on DTU.\nC.2\nUSER STUDY\nWe conduct a user study on the seven Mip-NeRF 360 scenes, comparing our method with and\nwithout SDS anchoring. We received 21 respondents. For each scene, respondents were shown 360-\ndegree novel view videos of the scene inferred both with and without SDS anchoring. The videos\nwere shown in a random order and respondents were unaware which video corresponded to the use\nof SDS anchoring. Respondents were asked:\n16\nTechnical report\nScene name\nInput view index\nContent scale\nbicycle\n98\n.9\nbonsai\n204\n.9\ncounter\n95\n.9\ngarden\n63\n.9\nkitchen\n65\n.9\nroom\n151\n2.\nstump\n34\n.9\nTable 6: Setup for the Mip-NeRF 360 dataset\n.\n1. Which scene seems more realistic?\n2. Which scene seems more creative?\n3. Which scene do you prefer?\nRespondents generally preferred the scenes produced by SDS anchoring, especially with respect to\n\u201cWhich scene seems more creative?\u201d\nC.3\nABLATION STUDIES\nWe perform ablation studies on dataset selection and camera representations. For 2D novel view\nsynthesis metrics, we compute metrics on a held-out subset of scenes from the respective datasets,\nrandomly sampling pairs of input and target novel views from each scene. For 3D SDS distillation\nand novel view synthesis, our settings are identical to the NeRF distillation settings for our main re-\nsults except that we use shorter-trained diffusion models. We train them for 25,000 steps as opposed\nto 60,000 steps for computational constraint reasons.\n17\n"
  },
  {
    "title": "A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications",
    "link": "https://arxiv.org/pdf/2310.17750.pdf",
    "upvote": "7",
    "text": "A Framework for Automated Measurement of Responsible AI\nHarms in Generative AI Applications\nAhmed Magooda*, Alec Helyar*, Kyle Jackson*, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann,\nRichard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun, Eslam Kamal, Federico\nZarfati, Hanna Wallach, Sarah Bird, Mei Chen\nMicrosoft\nRedmond, WA\n{ahmedmagooda,alec.helyar,kyle.jackson,dsullivan,chad.atalla,emilysheng,dan.vann,riedgar}@microsoft.com\n{hpalangi,romanlutz,hongliang.kong,xi.yun,eskam,fzarfati,wallach,sarah.bird,mei.chen}@microsoft.com\nABSTRACT\nWe present a framework for the automated measurement of re-\nsponsible AI (RAI) metrics for large language models (LLMs) and\nassociated products and services. Our framework for automati-\ncally measuring harms from LLMs builds on existing technical and\nsociotechnical expertise and leverages the capabilities of state-of-\nthe-art LLMs, such as GPT-4. We use this framework to run through\nseveral case studies investigating how different LLMs may violate a\nrange of RAI-related principles. The framework may be employed\nalongside domain-specific sociotechnical expertise to create mea-\nsurements for new harm areas in the future. By implementing this\nframework, we aim to enable more advanced harm measurement\nefforts and further the responsible use of LLMs.1\n1\nINTRODUCTION\nRapid advancements in artificial intelligence (AI) and natural lan-\nguage processing (NLP) have led to the development of increas-\ningly sophisticated large language models (LLMs) such as (GPT-4[2],\nLLama 2[4], Falcon[3], etc.), with advanced text generation capabil-\nities across a wide range of task types. While these models unlock\nnumerous opportunities, there are also serious concerns about mod-\nels causing harm[1]. Manual detection of harms may better account\nfor nuances. However, as the availability and capabilities of LLMs\ngrow, it is increasingly necessary to develop automated frameworks\nfor measuring harms with a speed and scale that can match the\npace of the technology\u2019s proliferation.\nMotivated by the need for an automated harm measurement\nframework which is flexible enough to align with evolving, valid,\nand reliable definitions of harms, as well as the need for a measure-\nment implementation that could be applied across different types\nof products and services related to LLMs (e.g., chatbots, summa-\nrization systems, etc.), we propose and implement a framework\nthat harnesses the capabilities of LLMs to test other LLMs and as-\nsess their potential for causing harm. While our work yields tools\nfor automated measurement, creating the harm-specific measure-\nment resources (e.g., harm measurement definitions) still requires\ndomain-specific expertise. We would like to preface the rest of this\npaper with an acknowledgment that this is not the final, only, nor\nnecessarily best implementation to measuring harms; however, it is\nan implementation that allows for flexibility in updating definitions\n1This is a living document\n* Authors contributed equally.\nand applying to various products and services. There are still open\nquestions about the risks of employing LLMs to perform parts of\nthe harm measurement process and how much of the measurement\npipeline can and should be automated\u2014we discuss this more in\nSec. 5 but mostly leave these important questions to future work.\nThe core of our proposed framework comprises of two key com-\nponents: (1) data generation from templates and (2) evaluation of\ngenerated outputs. First, we introduce a data generation compo-\nnent designed to assess LLM propensity for generating specific\ntypes of potential harmful content. This component simulates var-\nious real-world LLM products and services, such as question an-\nswering, summarization, and conversation. Next, we introduce an\nevaluation component that uses GPT-4 to assess LLM-generated\ncontent according to harm definitions. This component evaluates\nAI-generated content and produces both quantitative and quali-\ntative outputs, yielding numerical annotations of harm severity\nand written snippets about annotation reasoning. Our framework\nenables automatic comparison of different LLM-based products\nand services against measurement sets built by domain experts for\nvarious harms, allowing practitioners to compare strengths and\nweaknesses.\n2\nARCHITECTURE\nOur measurement framework comprises of two components that\nare tailored for assessing LLMs: 1) data generation from templates\nand parameters, and 2) evaluation of generated outputs via anno-\ntation guidelines. The data generation component uses templates\nand parameters to simulate interactions with the LLM under test\nto generate data which approximates a user-AI interaction in some\nproduct or service. The templates and parameters are separately cre-\nated by domain experts for each harm to ensure the reliability and\nvalidity of the resulting measurements. Next, the evaluation com-\nponent produces annotations of the LLM\u2019s output on the generated\ndata by applying annotation guidelines. The annotation guidelines\nare provided by domain experts based on the harm definitions they\ncreate.\nThe evaluation process is streamlined by treating the LLM under\ntest as a black box which need only accept inputs and yield outputs.\nAdditionally, the implementation of this framework supports two\ndifferent environments for computation. The first environment\ninvolves running the evaluation on a local machine, where prompt\nconstruction and model API coordination, model API calling, etc.\noccur locally. The second environment utilizes the Azure Machine\narXiv:2310.17750v1  [cs.CL]  26 Oct 2023\nAhmed Magooda*, Alec Helyar*, Kyle Jackson*, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun,\nEslam Kamal, Federico Zarfati, Hanna Wallach, Sarah Bird, Mei Chen\nFigure 1: Sample Azure Machine Learning pipeline which\nshows the two evaluation parts (Data Generation, and Evalu-\nation\nLearning (AML) platform to automatically construct evaluation\npipelines and perform evaluations using AML compute resources.\nFigure 1 shows an sample AML evaluation pipeline.\n2.1\nData Generation\nThe first part of our framework focuses on simulating a hypothetical\nuser\u2019s interaction with a real product or service such as question\nanswering, chat, and document summarization. The goal of this\npart of the data generation pipeline, referred to as task simulation, is\nto generate interactions (between the LLM and a hypothetical user)\nwhich cover topics or patterns associated with a target harm. To\nachieve this, we use another LLM to play the role of a hypothetical\nuser, initiating the task and participating in the conversation based\non the templates provided by domain experts. We denote the LLM\nunder test as \ud835\udc73\ud835\udc73\ud835\udc74\ud835\udc95\ud835\udc86\ud835\udc94\ud835\udc95 and the user-simulating LLM as \ud835\udc73\ud835\udc73\ud835\udc74\ud835\udc96\ud835\udc94\ud835\udc86\ud835\udc93.\nFigure 2: An example of a hypothetical persona template\nskeleton for simulating conversation with search scenario\nFigure 3: An example of hypothetical parameters to be in-\njected into the template skeleton from Figure 2\nWe provide a set of templates, referred to as persona templates,\nwhich provide guidelines for the \ud835\udc73\ud835\udc73\ud835\udc74\ud835\udc96\ud835\udc94\ud835\udc86\ud835\udc93 regarding how to be-\nhave and which topics or goals to introduce in the interaction\nwith \ud835\udc73\ud835\udc73\ud835\udc74\ud835\udc95\ud835\udc86\ud835\udc94\ud835\udc95. For simplicity and generalizability, we employ Jinja-\nstyle parameterized templates. Each template describes the basic\nstructure and theme of the conversations, leaving placeholders for\nparameters specifying specific topics, groups of people, etc, to be\nincorporated. Then, each template is combined with each set of cor-\nresponding parameters to create one or more completed personas\nfor the \ud835\udc73\ud835\udc73\ud835\udc74\ud835\udc96\ud835\udc94\ud835\udc86\ud835\udc93 to leverage in task simulation with the blackbox\n\ud835\udc73\ud835\udc73\ud835\udc74\ud835\udc95\ud835\udc86\ud835\udc94\ud835\udc95 2.\nGiven these completed personas created by combining templates\nand parameters, we run task simulation next. Each completed per-\nsona serves as instructions for \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f, shaping how it interacts\nwith \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61. This part injects creativity and is critical for automat-\ning and scaling up the process, but it also yields risks. For example,\nwhat if the \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f does not simulate realistic user behavior in\nthe interaction with \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61? We explore these concerns further in\nsection 5. Once the task simulation has been run for each completed\npersona, we are left with a set of generated data which includes\nsimulated user inputs and real \ud835\udc73\ud835\udc73\ud835\udc74\ud835\udc95\ud835\udc86\ud835\udc94\ud835\udc95 system outputs (we refer\nto each simulated interaction as a sample).\n2.2\nEvaluation\nThe second part of our framework is responsible for evaluation\nthrough automated annotation of the samples generated in task\nsimulation. The annotation process uses an LLM by providing it\nwith annotation guidelines which are manually crafted by domain\nexperts and include harm definitions, examples, and a defect defini-\ntion. The defect definition specifies criteria for determining whether\na data sample is considered desirable or allowable in the context of\nthe LLM under test and any product or service it is embedded in.\nCrafting this definition is a sociotechnical challenge which is deeply\nentangled with the harm definitions created by domain experts and\npolicy decisions made by the organizations building the AI system\nunder test.\nThe LLM can then annotate the given examples using the pro-\nvided guidelines. Automated annotation consists of multiple steps:\nthe first step uses the annotation guidelines to annotate each sam-\nple. These annotations are initially created in text, where the LLM\nfollows an annotation schema specified by few-shot examples in\nthe annotation guidelines. The next step parses the annotation to\nextract expected metrics (e.g., defect score, reasoning, etc) accord-\ning to the provided guidelines. The final step involves aggregating\nthe extracted values and calculating a metric (e.g., defect rate.).\nFor each harm area, human-LLM annotation agreement exper-\niments are conducted during the development of measurement\nresources. After that, the measurement resources and technical\nframework can be applied jointly to produce measurements with-\nout human annotation. Ultimately, a defect rate is calculated, which\nrepresents the proportion of samples which were annotated as\nmatching the defect definition.\nFor example, one way defect definitions may work is through\nseverity thresholds. Consider the case where we may wish to eval-\nuate whether the LLM under test produces extreme violent content.\nThe domain experts may build a severity scale (e.g., on an 1-10\nscale where lower is less severe) for violent content, and a defect\ndefinition could be a threshold within this severity range or a par-\nticular severity scale (e.g., any sample with severity \u2265 7 is a defect).\nThen, the defect rate can be determined by calculating the ratio of\n2The templates and parameters are two pieces of the measurement resources that are\ncreated by domain experts for each harm. The process of how domain experts create\nthese measurement resources will be examined separately in future work.\nA Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications\nsamples that meet the defect definition relative to the total number\nof samples. In this case, the defect rate can be computed as follows:\n\ud835\udc37\ud835\udc52\ud835\udc53 \ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc45\ud835\udc4e\ud835\udc61\ud835\udc52 = |\ud835\udc65 \u2208 \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc60 : \ud835\udc65 > \ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc60\u210e\ud835\udc5c\ud835\udc59\ud835\udc51|\n|\ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc60|\n3\nINTERPRETING MEASUREMENTS\nBy combining this framework with measurement resources (tem-\nplates, parameters, harm definitions, and annotation guidelines),\na repeatable measurement pipeline can be created. Running this\nmeasurement pipeline on an AI system yields a defect rate. It is\nimportant to interpret this defect carefully and understand the util-\nity of measurements derived this way. All defect rates obtained\nthrough application of this technical framework are relative mea-\nsurements, which do not represent the absolute state of the world.\nIn other words, a 0% defect rate does not mean that there is zero\nchance of the measured harm occurring in the real world. Instead, a\n0% defect rate may be interpreted to mean that the AI system under\ntest did not appear to fail any tests in the current measurement set.\nAdditionally, all resulting measurements are only as reliable and\nvalid as the measurement resources designed for the harm being\ntested. The process of creating these measurement resources is\na complex sociotechnical problem which is fraught with pitfalls\nand opportunities for reliability and validity to be impacted. If the\nmeasurement resources are created with a poorly constructed harm\ndefinition, the resulting measurements can range from nonsensical\nto directly harmful (if development decisions are misled by a poorly\ndesigned measurement).\nWith this perspective, these measurements provide significant\nand targeted utility. These measurements can serve as diagnostic\ntools. They enable comparison of the efficacy of different mitiga-\ntions as well as tracking of progress in mitigating known defects\nover time. Lastly, when using identical measurement sets to test two\nAI systems, the resulting measurements can be used to compare the\nrelative performance of each system on the challenges represented\nin the measurement set.\n4\nCASE STUDY\nBelow we provide a deep dive on Groundedness. Then we provide\nan example of how this framework can be leveraged to create\nmeasurements and compare multiple models.\n4.1\nDeep Dive: Groundedness\nFigure 4: Annotation Guidelines for Groundedness that is\ngiven to \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 to answer the question made by \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f\nusing only provided context.\nIn this case study, we consider ungrounded generations from\n\ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 to be harmful and refer to this measurement category as\ngroundedness. We first had to build measurement resources for this\nspecific harm. As mentioned earlier, measurement resources must\ninclude a set of templates and parameters. For the groundedness\ncase study, the templates and parameters were to yield a set of\nof questions (prompts to \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61) and corresponding contextual\nfiles (used by \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 to answer the prompt questions). In the\nfirst stage of the evaluation pipeline (i.e., data generation with task\nsimulation), we initiate conversations between \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 and the\nsimulated \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f . \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f follows the templates and parameters\nand asks each question from the provided set. At the same time,\nwe provide \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 with access to the context files and provide\nguidance to answer the questions based solely on the context files.\nFigure 4 illustrates the prompt guidance for \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 to answer\nquestions while relying exclusively on the context files as a source\nof information.\nFollowing the generation of conversations, we proceed to the\nevaluation stage to assess generated samples. As part of our mea-\nsurement resources, we must provide annotation guidelines to an\nLLM (GPT-4) to evaluate whether a response is grounded or not.\nIn this case, we design a basic annotation guideline to yield a 1 -\n5 groundedness score. A score of 1 signifies that the response is\nnot grounded, while a score of 5 indicates that all information in\nthe answer is grounded. Figure 5 shows the annotation guidelines.\nThe LLM annotator (GPT-4) is then provided with the original ques-\ntion posed by \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f , the response from \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61, and the context\ngiven to \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 for formulating its answer. Subsequently, the LLM\nannotator assigns a groundedness score on a scale of 1 to 5 for each\nsample.\nFigure 5: Grounding annotation guidelines used for evaluat-\ning LLM system responses.\nTo evaluate the effectiveness of our annotation guidelines, we\ncollected a dataset of 266 examples including questions, responses,\nand the context used to generate the responses. These examples\nwere annotated by human evaluators using the same scale from\n1 to 5 for groundedness. In parallel, we employed our proposed\nAhmed Magooda*, Alec Helyar*, Kyle Jackson*, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun,\nEslam Kamal, Federico Zarfati, Hanna Wallach, Sarah Bird, Mei Chen\nFigure 6: Confusion matrix between human- and model-\nannotated grounding scores based on the provided guide-\nlines.\nHuman-Model Agreement\nExact (\ud835\udc49\ud835\udc3b\ud835\udc62\ud835\udc5a\ud835\udc4e\ud835\udc5b == \ud835\udc49\ud835\udc40\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59)\n60%\nRelaxed (||\ud835\udc49\ud835\udc3b\ud835\udc62\ud835\udc5a\ud835\udc4e\ud835\udc5b - \ud835\udc49\ud835\udc40\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59|| <= 1)\n80.5%\nRelaxed (||\ud835\udc49\ud835\udc3b\ud835\udc62\ud835\udc5a\ud835\udc4e\ud835\udc5b - \ud835\udc49\ud835\udc40\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59|| <= 2)\n93.6%\nTable 1: Agreement ratio between human and model annota-\ntions.\nframework utilizing GPT-4 to annotate the same data, also on the\nsame scale from 1 to 5, using the crafted annotation guidelines.\nThen, we assessed the agreement between the human and GPT-4\nannotations using two simple heuristic metrics. The first metric,\nexact agreement ratio, measures the proportion of instances where\nthe human and GPT-4 scores are identical. The second metric serves\nmore as a loose heuristic: relaxed agreement ratio, which considers\nagreement in cases where the human and GPT-4 scores differ by\nno more than 1 point on the scale.\nOur preliminary analysis revealed an exact agreement ratio of\n60% and a relaxed agreement ratio of 80.5% as shown in table 1.\nFigure 6 presents a confusion matrix illustrating the relationship be-\ntween the human and GPT-4 annotations. Further work on human-\nhuman agreement is required as well to build an understanding\nof what an acceptable result is on each of these metrics. Addition-\nally, more robust agreement analysis will be performed in future\nwork. This sort of measurement provides a sense of the quality\nof the annotation guidelines, which allows us to iterate on and\nimprove the guidelines. These preliminary results are also useful\nfor building a rough notion of how confident we can be in resulting\nmeasurements.\n4.2\nExperimental Design\nWe conducted a set of experiments to evaluate three LLMs with the\nproposed evaluation framework. We refer to these three models as\nmodel 1, model 2, and model 3.3 In all of the reported experiments,\nwe focused on conversation simulation tasks, where we engaged\nin a synthetic conversation with the LLM under test (\ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61) to\nmeasure its tendency to violate RAI principles in the following\naspects:\n(1) Succeeding in Jailbreaks\n(2) Generating Potentially Harmful Content, including but not\nlimited to:4\n\u2022 Hateful or Unfair Content\n\u2022 Sexual Content\n\u2022 Violent Content\n(3) Leaking Intellectual Property (IP):\n\u2022 Songs\n\u2022 Books\n\u2022 News\nIn this round of experiments, we used GPT-4 in both the data\ngeneration and evaluation components of the pipeline. For data\ngeneration, we use GPT-4 to simulate the user agent (\ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f ) that\nchats with the \ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 using the provided persona templates. For\nevaluation, we used GPT-4 as the underlying LLM for the annota-\ntion component. This experimental design is intended to roughly\nillustrate how our proposed framework can be leveraged in assess-\ning the performance of different LLMs to cause different harms or\nviolate RAI principles.\n4.3\nResults\nAs illustrated in Table 2, the three models exhibit similar behavior\nin terms of defect rates when evaluated for the generation of po-\ntentially harmful content. This indicates that the models produced\ncontent which was annotated as a defect on a similar number of\nsamples, with Model 3 displaying the lowest rate of generating po-\ntentially harmful content defects. Notably, the generation of violent\nand hateful content is more prevalent compared to sexual content.\nIn the context of intellectual property (IP) data leakage, Mod-\nels 2 and 3 demonstrate identical defect rates across all categories\n(songs, books, and news), suggesting that these models generate\nIP-protected content at the same rate when tested on this set of mea-\nsurement resources. This may hint that the measurement resources\nshould be expanded or improved to provide greater clarity on pos-\nsible performance differences between the models. Of the different\nIP categories, songs exhibit the highest leakage rates, followed by\nbooks and news. In contrast, Model 1 displays significantly higher\ndefect rates for songs and news compared to Models 2 and 3, with\na 45.8% defect rate for songs compared to 17.9% for both Models\n2 and 3, and 9.6% defect rate for news compared to 1.1% for both\nModels 2 and 3. This implies that Model 1 is more susceptible to\nrevealing IP-protected material in product scenarios.\nRegarding jailbreak evaluations, Models 2 and 3 exhibit compa-\nrable defect rates, with leaking guidelines being the most successful\nattack vector compared to generating adult content or promoting\nillegal activities. Model 1, however, demonstrates a significantly\n3We anonymized model names for now\u2014more details will be provided in future updates\nto this manuscript\n4For these highly sociotechnical harms, the measurement resources were constructed\nby domain experts, leveraging techniques that are out of scope for this manuscript.\nA Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications\nDefect Rate\nPotentially Harmful Content\nIP\nJailbreak\nModel\nSexual\nViolent\nHate\nSongs\nBooks\nNews\nAdult Content\nIllegal Persuasion\nLeaking Guidelines\nModel 1\n1.37%\n17.7%\n14.9%\n45.8%\n2.75%\n9.6%\n1%\n4.1%\n79.4%\nModel 2\n1.5%\n17.5%\n14.4%\n17.9%\n5.5%\n1.1%\n1%\n4%\n51.5%\nModel 3\n1.3%\n17.1%\n13.2%\n17.9%\n5.5%\n1.1%\n1%\n4%\n53%\nTable 2: Defect rate for Potentially Harmful Content, IP leakage, and Jailbreak across various LLMs\nhigher vulnerability to guideline leakage, with an 80% success rate\ncompared to 51% and 53% for Models 2 and 3, respectively.\nIn conclusion, our evaluation reveals that Models 2 and 3 display\nlower rates of generating IP-protected content and exposing under-\nlying guidelines than Model 1. So, we suggest that Models 2 and\n3 may be more suitable as components for real-world AI products\nand services compared to Model 1.\n5\nLIMITATIONS\nThis framework facilitates rapid and repeated evaluation of different\nversions of LLMs and associated products and services. However,\nthere are several limitations.\nUsing an LLM to measure harms from another LLM. Notably, this\nwork does not adequately address issues related to the risks of using\nan LLM to measure harms from another LLM, especially given that\nLLMs are known to cause harms. This is an open research problem,\nalthough we note that the evaluation component of our frame-\nwork is flexible enough to plug in other evaluation methods. This\nconcern can manifest in both the data generation and evaluation\ncomponents of the framework.\nIn the case of data generation (during task simulation), by using\nan LLM to mimic user behavior, we run the risk of the LLM failing\nto simulate realistic conversations. This may impact the ecological\nvalidity of the generated data. Additionally, the LLM used in task\nsimulation may fail to represent linguistic patterns of certain de-\nmographic groups, causing measurement efforts to underestimate\nthe potential for harms affecting marginalized groups.\nIn the case of evaluation, using an LLM to annotate potential\nharms from other LLM-generated content may lead to issues. LLMs\nare known to produce harmful content and can disproportionately\nproduce some specific types of harmful content affecting some\nspecific groups of people. If an LLM is vulnerable to producing some\nspecific type of harmful content, will it be effective in evaluating\nand annotating that same type of content? This may lead to under-\nannotation of harms. Simultanesouly, other tendencies of LLMs\nmay lead to over-annotation of harms. LLMs are known to struggle\nwith groundedness, and we have observed cases where the LLM\nannotator yields a defect score and text reasoning that cites non-\nexistent parts of the sample. How frequent and impactful may\nungrounded generations be in the annotation process? Because\nthe real-life consequences of falsely labeling a piece of text as not\nharmful are perhaps greater than those of falsely labeling text as\nharmful, the amount of potentially harmful content measured from\nthis framework should be treated as a lower bound for the real\namount of potentially harmful content.\nOne heuristic for gauging the impact of the issues described\nabove is human-model annotation agreement. While this practice\nprovides some greater confidence in the reliability of LLM annota-\ntions, it cannot be viewed as a completely adequate replacement\nfor the holistic research required to address these concerns. Addi-\ntionally, measuring generic human-model annotation agreement\nis not sufficient. This is due to the reality that different groups of\nhumans with different lived experiences will experience different\nharms and annotate differently.\nUtility and interpretation. Another limitation lies in the utility\nand interpretation of the resulting measurements. As mentioned in\nsection 3, a 0% defect rate cannot be interpreted to mean that the AI\nsystem under test does not cause harm. The resulting measurements\nare relative rather than absolute, so they are useful for diagnos-\ntics and comparisons between systems but are not applicable for\nestimations of absolute risk or absolute likelihood of harm.\nValidity and reliability. Likely the largest challenge of this tech-\nnical framework is the fact that it requires carefully-constructed\nmeasurement resources for sociotechnical problems. Unfortunately,\nif these measurement resources are created poorly, their usage in\nthe technical framework does not immediately raise any red flags.\nThe usage of poorly constructed or invalid measurement resources\nmay go unnoticed, which can lead to increased harm if practition-\ners trust the resulting measurements. In our initial case study, we\nengaged with domain experts to create measurement resources,\nbut future work is required to understand the practices involved in\ncreating reliable and valid measurement resources.\nAnother aspect of reliability deals with the reproducibility and\nstability of annotations generated by an LLM. We have observed\nrepeated annotations on the same sample leading to different results.\nIn response, we implement a stability factor that runs the annotation\nprocess multiple times and uses the majority value generated for\neach sample. While this can significantly reduce variability, it comes\nat the cost of increased computation, as it requires running the\nevaluation multiple times (e.g., 5 or 7), which can leads to longer\nevaluation times and greater resource requirements.\nResources. Finally, we recognize that this approach requires many\ninvocations of large models. While access to LLMs is expanding, ac-\nquiring the necessary resources to run various LLMs, especially for\nlarge tasks, can be challenging and costly. The compute resources\nrequired for this method may make it impractical or inaccessible\nfor some practitioners, and the environmental effects associated\nwith the proliferation of this framework must be examined.\nAhmed Magooda*, Alec Helyar*, Kyle Jackson*, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun,\nEslam Kamal, Federico Zarfati, Hanna Wallach, Sarah Bird, Mei Chen\n6\nCONCLUSION AND FUTURE DIRECTIONS\nIn this work, we presented a technical framework for the automated\nevaluation of large language models (LLMs) in various RAI-relevant\nharm areas such as groundedness, potentially harmful content,\nand leakage of intellectual property. This framework leverages\nLLMs to automate the evaluation process, enabling measurement\nat speeds and scales demanded by the current proliferation of LLM-\npowered products and services. The proposed framework offers\nan end-to-end pipeline for testing an LLM (\ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61) by simulating\nan interaction with another LLM (\ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f) and annotating the\noutputs with another LLM. The framework depends upon various\nmeasurement resources that are best created by domain experts for\neach harm area subject to measurement.\nThen, we demonstrated the utility of the proposed framework\nby evaluating three recent LLMs across three distinct categories\nof harm (leakage of IP content, generation of potentially harmful\ncontent, and jailbreak). The resulting measurements enables us to\ncompare the relative performance of these models and serves as\nan example of how this framework can be used by practitioners\nmaking decisions about which model versions to use in their AI\nproducts and services. While much more work is required to explore\nhow reliable and valid measurement resources are created for each\nharm area, this framework provides a viable path to evaluating\nharms stemming from LLM-based AI systems at a speed and scale\nthat can keep up with the current pace of development. For future\nwork, we will examine the aforementioned limitations to make the\nmeasurement approach more reliable, valid, repeatable, objective,\nand more cost efficient.\nREFERENCES\n[1] Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios Anastasopoulos,\nand Yulia Tsvetkov. 2023. Language Generation Models Can Cause Harm: So\nWhat Can We Do About It? An Actionable Survey. arXiv:2210.07700 [cs.CL]\n[2] OpenAI. 2023. GPT-4 Technical Report. ArXiv abs/2303.08774 (2023).\n[3] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,\nAlessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming\ncurated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116\n(2023).\n[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,\net al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288 (2023).\n"
  },
  {
    "title": "Large Language Models as Generalizable Policies for Embodied Tasks",
    "link": "https://arxiv.org/pdf/2310.17722.pdf",
    "upvote": "6",
    "text": "LARGE LANGUAGE MODELS\nAS GENERALIZABLE\nPOLICIES FOR EMBODIED TASKS\nAndrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Walter Talbott\nKatherine Metcalf, Natalie Mackraz, Devon Hjelm, Alexander Toshev\nApple\nABSTRACT\nWe show that large language models (LLMs) can be adapted to be generalizable\npolicies for embodied visual tasks. Our approach, called Large LAnguage model\nReinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take\nas input text instructions and visual egocentric observations and output actions di-\nrectly in the environment. Using reinforcement learning, we train LLaRP to see\nand act solely through environmental interactions. We show that LLaRP is robust\nto complex paraphrasings of task instructions and can generalize to new tasks that\nrequire novel optimal behavior. In particular, on 1, 000 unseen tasks it achieves\n42% success rate, 1.7x the success rate of other common learned baselines or zero-\nshot applications of LLMs. Finally, to aid the community in studying language\nconditioned, massively multi-task, embodied AI problems we release a novel\nbenchmark, Language Rearrangement, consisting of 150, 000 training and 1, 000\ntesting tasks for language-conditioned rearrangement. Video examples of LLaRP\nin unseen Language Rearrangement instructions are at https://llm-rl.github.io.\n1\nINTRODUCTION\nLarge Language Models (LLMs), characterized as billion-parameter models trained on enormous\namounts of text data, have demonstrated unprecedented language understanding capabilities. Fur-\nthermore, LLMs have demonstrated powerful capabilities beyond core language understanding prob-\nlems, such as dialog systems (Thoppilan et al., 2022; Glaese et al., 2022), visual understanding prob-\nlems (Alayrac et al., 2022; Li et al., 2023b; Peng et al., 2023; Koh et al., 2023), reasoning (Wei et al.,\n2022; Lewkowycz et al., 2022), code generation (Chen et al., 2021b), embodied reasoning (Driess\net al., 2023), and robot control (Ahn et al., 2022). These capabilities often emerge in a zero-shot\nfashion, without dedicated training data for each capability, indicating that LLMs contain knowledge\ngeneral and broad enough to apply to numerous domains. Furthermore, these capabilities emerge\ndespite that the input and output spaces in these domains are oftentimes not naturally expressed in\nlanguage, e. g. images as inputs, and robot commands as outputs.\nA key objective in Embodied AI is generalizable decision-making that can transfer to novel tasks, so\nit is natural to ask if the generalization abilities of LLMs can be incorporated into embodied prob-\nlems. Existing advances in using LLMs for Embodied AI have relied on static expert datasets (Driess\net al., 2023; Brohan et al., 2023), which requires prohibitively large and expensive amounts of di-\nverse expert data. Conversely, Embodied AI simulators enable agents to learn from an environment\nthrough direct interaction, exploration, and reward feedback (Kolve et al., 2019; Szot et al., 2021;\nLi et al., 2023a). However, the generalization capabilities of such agents to a large number of new\nembodied tasks are not on par with the aforementioned domains.\nLLMs have been shown to be applicable in online settings when the control domain is that of nat-\nural language, e.g., Reinforcement Learning from Human Feedback (RLHF) for multi-turn dialog\napplications (Ouyang et al., 2022). In this work, we successfully show that LLMs can be adapted for\nReinforcement Learning (RL) problems in Embodied AI, using a method we call Large LAnguage\nmodel Reinforcement learning Policy (LLaRP). We demonstrate advanced capabilities on a diverse\nset of rearrangement tasks, where the input and output domains aren\u2019t just language (see Fig. 1) In\nparticular, we demonstrate the following three contributions:\n1\narXiv:2310.17722v1  [cs.LG]  26 Oct 2023\nEgocentric  \nPerception\nLLaRP\nTask: Find an apple and put \nit away in the fridge.\nEnvironment\nSparse Rewards\nActions\nReinforcement Learning of an LLM-based Policy for Many Tasks\nFind and put all the apples in the fridge.\nMultiple Objects\nFind a pear and put it away in the fridge.\nNovel Objects\nFind the round red fruit and put it in the fridge\nReferring Expressions\nI am hungry for something sweet and healthy. \nPut a snack for me on the table.\nContext\nFind an apple and put it in the receptacle to \nthe right of the kitchen counter.\nSpatial Relationships\nIf the fridge is open, put an apple inside; \notherwise put an apple on the counter\nConditional Instructions\nLLaRP\nAct on novel  \ninstructions\nGeneralization Across Novel Tasks \nPick a novel task\nFigure 1: We demonstrate that by utilizing Reinforcement Learning together with a pre-trained\nLLM and maximizing only sparse rewards, we can learn a policy that generalizes to novel language\nrearrangement tasks. The method robustly generalizes over unseen objects and scenes, novel ways of\nreferring to objects, either by description or explanation of an activity; and even novel descriptions of\ntasks, including variable number of rearrangements, spatial descriptions, and conditional statements.\nFirst, we show that using a pre-trained and frozen LLM as a Vision-Language Model (VLM) policy\nwith learned input and output adapter layers results in a policy that exhibits strong generalization\ncapabilities. We train this policy using online RL and measure generalization along two axes:\n\u2022 Paraphrastic Robustness (PR): the agent produces the same optimal behavior under linguistic\nvariations of an instruction where the \u201cintention\u201d of the instruction does not change. This includes\nnovel ways of describing the same behavior or novel ways of referring to a seen object.\n\u2022 Behavior Generalization (BG): the agent solves tasks that require novel optimal behavior. This\nmeans the desired behavior outcome is distinct from those seen during training. For example, act\non new types of or combinations of objects, new types of combined behaviors (e.g., finding \u201call\u201d\nof something) or new logical conditions (e.g., \u201cif\u201d statements).\nLLaRP is thoroughly evaluated on over 1, 000 unseen tasks spanning the above axes and attains\n42% success rate, compared to 25% for an LSTM-based policy and 22% for zero-shot applications\nof LLMs. Our approach outperforms all baselines both when it is instructed in novel ways as well\nas when tasked to perform unseen behaviors. Further, we demonstrate that the LLaRP LLM-based\npolicy gives a non-trivial boost on another domain, Atari, compared to a Transformer baseline.\nWe demonstrate that, when the agent has access to the world knowledge encoded in an LLM, RL\nexhibits various forms of training efficiencies. For one, LLM-based models exhibit better sample\nefficiency than other common architectures in both basic PPO RL and continual learning settings\n(training the model on downstream tasks beyond the training tasks). Further, we show that LLaRP\nis more efficient with what supervision is needed than commonly used imitation learning.\nFinally, in order to facilitate the above contributions and promote future work studying general-\nization in Embodied AI, we introduce the Language Rearrangement task which includes 150,000\ndistinct language instructions, each with automatically generated rewards. The large number of di-\nverse tasks brings the system closer to real-world setups where agents should be able to do anything\nand everything, and pushes the limits on performance. For generalization evaluation, we define\nsplits that stress test the system on PR- and BG-types of generalization.\n2\nRELATED WORK\nPrior work has demonstrated large language models (LLMs) can be used as zero-shot policies for\ninteractive decision-making tasks, without task-specific training, in settings where the states and ac-\ntion spaces are both text-based (Zeng et al., 2022; Shah et al., 2023; Huang et al., 2022; Liang et al.,\n2023; Huang et al., 2023b; Wu et al., 2023a; Silver et al., 2023; Wang et al., 2023). Furthermore,\nLLMs can be adapted to text-based decision-making by fine-tuning with a standard language mod-\neling objective (Chalvatzaki et al., 2023). In addition, it has been shown that LLMs can be used to\nprovide rewards or useful high-level goals for learning policies (Du et al., 2023; Hu & Sadigh, 2023;\nWu et al., 2023b) in text-based or human-interaction settings.\n2\nLarge Language Model\n Vision Encoder\nMLP\n\u201cMove all the fruit to the fridge\u201d\n<latexit sha1_base64=\"WZJZsRFk0UedokJv7dDcH4Cl\ngT4=\">AB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1B\nWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHs\ntHM0nQj+hQ8pAzaqz0IPpev1xq+4cZJV4OalAjka/NUbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQlYzrErqWSRqj9\nbH7qlJxZUDCWNmShszV3xMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULFojAVxMRk9jcZcIXMiIklClubyVsRBV\nlxqZTsiF4y+vktZF1bus1u5rlfpNHkcRTuAUzsGDK6jDHTSgCQyG8Ayv8OYI58V5dz4WrQUnzmGP3A+fwD8R42d<\n/latexit>l1\n<latexit sha1_base64=\"mNa7cSZx5hvYoxzcqJy4ixKNEY=\n\">AB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZe\nkAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0IPr\njfrniVt05yCrxclKBHI1+as3iFkaoTRMUK27npsYP6PKcCZwWuqlGhPKxnSIXUsljVD72fzUKTmzyoCEsbIlDZmrvycyGmk9i\nQLbGVEz0sveTPzP6YmvPYzLpPUoGSLRWEqiInJ7G8y4AqZERNLKFPc3krYiCrKjE2nZEPwl9eJa2LqndZrd3XKvWbPI4inMAp\nnIMHV1CHO2hAExgM4Rle4c0Rzovz7nwsWgtOPnMf+B8/gBUPo3X</latexit>lk\n<latexit sha1_base64=\"M09PtpaPgWUSTGfkRF8gqo274U=\">AB7HicbVDLSgNBEOyNrxhfUY9eBoPg\nKeyKqMegF48R3CSQLGF2MpsMmcyMyuEJd/gxYMiXv0gb/6Nk2QPmljQUFR1090Vp5wZ6/vfXmltfWNzq7xd2dnd2z+oHh61jMo0oSFRXOlOjA3lTNLQMstpJ9Ui5jTdjy+m/ntJ6oNU/LRTlIaCTyULGEWyeFqp8H0361\n5tf9OdAqCQpSgwLNfvWrN1AkE1RawrEx3cBPbZRjbRnhdFrpZYamIzxkHYdlVhQE+XzY6fozCkDlCjtSlo0V39P5FgYMxGx6xTYjsyNxP/87qZTW6inMk0s1SxaIk48gqNPscDZimxPKJI5ho5m5FZIQ1JtblU3EhBMs\nvr5LWRT24ql8+XNYat0UcZTiBUziHAK6hAfQhBAIMHiGV3jzpPfivXsfi9aSV8wcwx94nz/FbI6s</latexit>o1\n<latexit sha1_base64=\"AcxBnUXsPWiRpmc1gdX+sYJ9wY8=\">AB7HicbVBNS8NAEJ3Ur1q/qh69LBbB\nU0lE1GPRi8cKpi20oWy2m3bpZhN2J0IJ/Q1ePCji1R/kzX/jts1BWx8MPN6bYWZemEph0HW/ndLa+sbmVnm7srO7t39QPTxqmSTjPskYnuhNRwKRT3UaDknVRzGoeSt8Px3cxvP3FtRKIecZLyIKZDJSLBKFrJT/o5TvV\nmlt35yCrxCtIDQo0+9Wv3iBhWcwVMkmN6XpuikFONQom+bTSywxPKRvTIe9aqmjMTZDPj52SM6sMSJRoWwrJXP09kdPYmEkc2s6Y4sgsezPxP6+bYXQT5EKlGXLFouiTBJMyOxzMhCaM5QTSyjTwt5K2IhqytDmU7EheMs\nvr5LWRd27ql8+XNYat0UcZTiBUzgHD6hAfQB8YCHiGV3hzlPivDsfi9aSU8wcwx84nz8rSo7v</latexit>ot\nEgocentric Visual  \nObservations\nTask Instruction:\nMLP\nValue Head\nAction Head\n<latexit sha1_base64=\"320VPklRgEWhB8hnEiRh3xn5cE=\">AB6nicbVBNS8NAEJ3U\nr1q/qh69LBbBU0mkqMeiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZLGLVCahGwSU2DTcCO4lCGgUC28\nH4dua3n1BpHstHM0nQj+hQ8pAzaqz0QPtev1xq+4cZJV4OalAjka/NUbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQlYzrErqWSRqj9bH7qlJxZUDCWNmShszV3xMZjbSeRIHtjKgZ6WVv\nJv7ndVMTXvsZl0lqULFojAVxMRk9jcZcIXMiIklClubyVsRBVlxqZTsiF4y+vktZF1bus1u5rlfpNHkcRTuAUzsGDK6jDHTSgCQyG8Ayv8OYI58V5dz4WrQUnzmGP3A+fwDrhY2S</\nlatexit>a1\nFrozen\nTrained from scratch with RL\n<latexit sha1_base64=\"q6eoYNDShd6czrV7MLhjzWbJeI=\">AB6nicbVBNS8\nNAEJ3Ur1q/qh69LBbBU0mkqMeiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhU\nijeRIGSdxLNaRI3g7GtzO/cS1EbF6xEnC/YgOlQgFo2ilB9rHfrniVt05yCrxclKBHI1+as3iFkacYVMUmO6npugn1GNgk+LfVSwxPKxnTIu5YqGnHjZ/NTp+TMKgMS\nxtqWQjJXf09kNDJmEgW2M6I4MsveTPzP6YXvuZUEmKXLHFojCVBGMy+5sMhOYM5cQSyrSwtxI2opoytOmUbAje8surpHVR9S6rtftapX6Tx1GEziFc/DgCupwBw1oAoM\nhPMrvDnSeXHenY9Fa8HJZ47hD5zPH1EgjdU=</latexit>at\nMLP\nValue Head\nAction Head\nTrained with Online \nReinforcement learning\nTokenize:\n Vision Encoder\nMLP\n Embed\n Embed\nFigure 2: LLaRP architecture. The bottom of the figure shows the policy inputs including the task\ninstruction and the egocentric visual RGB frames from the current time step to the start of the\nepisode. These are encoded using the LLM embeddings or a vision encoder. The embeddings are\ninput to a pre-trained LLM. The hidden outputs are then projected to action and value predictions.\nThe entire system learns from online RL, where the action output module and observation encoder\nMLP are the only trained components and all other components are frozen.\nIt has been shown that agents can learn tasks specified by natural language. Shridhar et al. (2022);\nTeam et al. (2021); Abramson et al. (2022); Yu & Mooney (2022); Lynch et al. (2023); Liu et al.\n(2022); Shridhar et al. (2023); Brohan et al. (2022); Ha et al. (2023) learn language conditioned\npolicies through imitation learning on a dataset of expert trajectory and language pairs. Xiao et al.\n(2022); Yu et al. (2023); Myers et al. (2023) augment the paired dataset to increase the quan-\ntity and diversity of data for imitation learning. Open-vocabulary mobile manipulation (OVMM,\nYenamandra et al., 2023) demonstrated open-vocab rearrangement capabilities in pick-and-place\nsettings, while other works demonstrate open-vocab object manipulation capabilities in navigation\nsettings (Gadre et al., 2023; Shah et al., 2023; Huang et al., 2023a; Bolte et al., 2023).\nWorks in Vision Language Models (VLMs) have combined pre-trained LLMs with visual reason-\ning (Alayrac et al., 2022). Works have imbued VLMs with 3D spatial information (Hong et al., 2023;\nJatavallabhula et al., 2023; Ha & Song, 2022), usually in a static setting without environment inter-\naction. Like our work, PaLM-E (Driess et al., 2023) and RT-2 (Brohan et al., 2023) extend VLMs\nto interactive decision-making in visual embodied environments, but using high-quality expert data.\nTo our knowledge, there are no prior work which demonstrate that the linguistic knowledge in LLMs\ncan be used in online RL problems to improve generalization in embodied AI settings. Furthermore,\nwe demonstrate these capabilities over a diverse set of evaluations, both in terms of linguistic (para-\nphrastic generalization) and environmental / target behavior (behavior generalization) variations.\nOur Language Rearrangement extends beyond the scope of prior work using LLMs in embodied\nsettings by requiring complex manipulation, navigation, and exploration. The tasks contain almost\n6x more instructions involving more complex rearrangement concepts and object interactions than\nthe benchmark ALFRED (Shridhar et al., 2020), a common language-conditioned Embodied AI\nbenchmark. We believe the algorithmic and data advances will make a significant impact towards\nattaining robust agents in challenging embodied settings.\n3\nMETHOD\nOur method, Large LAnguage model Reinforcement learning Policy (LLaRP), adapts pre-trained\nLLMs to operate in embodied multi-modal decision-making settings. We show how to modify\nexisting LLMs for embodied settings and train a policy for embodied tasks, leading to an agent with\nsignificantly improved generalization capabilities to new language instructions.\n3\nOur problem can be formulated as a Partially-Observable Markov Decision Process (POMDP), de-\nfined as a tuple (S, O, A, P, R, \u03c10, \u03b3) for underlying state space S, observation space O, action\nspace A, transition distribution P, reward function R, initial state distribution \u03c10 and discount fac-\ntor \u03b3. In our setting, O are egocentric high-dimensional visual observations, such as a robot\u2019s RGB\ncamera which only observes part of the scene. We consider the extension of including a goal dis-\ntribution G and the case where the reward is formulated as R(s, g) for s \u2208 S and g \u2208 G. We seek\nto learn a goal-conditioned policy \u03c0(a|o, g) mapping from observation o and goal g to an action a\nthat maximizes the sum of discounted rewards Es0\u223c\u03c10,g\u223cG\nP\nt \u03b3tR(st, g). Furthermore, we seek to\nlearn a policy that generalizes and achieves high rewards for goal distributions G\u2032 not seen during\ntraining. Specifically, we consider goals specified as natural language instructions and want policies\nto generalize to new distributions of natural language instructions.\nWe utilize large language models (LLMs), which are large auto-regressive text prediction models.\nGiven text represented as a sequence of tokens l, the LLM is trained to predict each token in that\nsequence conditioned on all prior tokens \u03c0LLM(lK+1 | l1, . . . , lK). Since an embodied agent policy\nneeds to consume visual observations O and predict actions A, both of which are not language\ntokens, we strip away the input and output layers of the LLM. In particular, the LLM input layer\nencodes the text tokens producing vector embeddings ek = ET (lk) \u2208 RD, while the output layer\nclassifies words.\nAfter we strip away the input and output layers, we call the resulting network an LLM backbone\nand denote it by \u03c8LLM(e1, . . . , eK) \u2208 RD. This backbone consumes a sequence of D-dim. token\nembeddings and produces a D-dim. token embedding.\n3.1\nLARGE LANGUAGE MODEL REINFORCEMENT LEARNING POLICY ARCHITECTURE\nLLaRP, visualized in Figure 2, has two input types.\nFirst, it is conditioned on a goal g =\n(l1, . . . , lk) \u2208 G expressed as language. This goal can be embedded using the language encoder\nET\n\u03b8 into a sequence of D-dim. vectors. Second, it consumes visual observations o1, . . . , ot dur-\ning the policy rollout which are encoded using a separate learnable observation encoder module\nEV\n\u03d5 : O 7\u2192 RD. The observation encoder module consists of a vision encoder that produces a visual\nembedding and an MLP network that projects the visual embedding to the language model token\nembedding dimension. The encoded text and visual observations create a k + t length sequence of\nD-dimension embeddings, which are input to the LLM backbone \u03c8LLM\n\u03b8\n, as defined above.\nTo decode an action as output, we employ a learnable action output module D\u03c9 : RD 7\u2192 D(A)\nthat converts the output of the LLM backbone to a distribution over actions from A. With the\ntwo additional adapter modules D\u03c9 and EV\n\u03d5 we are able to adapt the LLM to take as input goal\nspecification and visual observations up to time t in order to output action at time t:\n\u03c0LLaRP(at|o1, . . . , ot, g) = D\u03c9(\u03c8LLM\n\u03b8\n(ET\n\u03b8 (l1), . . . , ET\n\u03b8 (lk), EV\n\u03d5 (o1), . . . , EV\n\u03d5 (ot)).\nThe action output module is an MLP that predicts a distribution over environment actions. We\nexclude the first k outputs from \u03c8LLM that correspond to the language task specification tokens. The\nhidden outputs corresponding to observations at each time step are fed through the action modeling\nMLP to produce the distribution over actions. The action output module also predicts the value\nestimate used for the reinforcement learning update. More details about the visual encoder and\naction output module are in Appendix C.1.\n3.2\nLARGE LANGUAGE MODEL REINFORCEMENT LEARNING POLICY TRAINING\nWe train LLaRP using only reinforcement learning (Sutton & Barto, 2018). Specifically, we train\nwith DD-PPO (Wijmans et al., 2019), an adaptation of PPO (Schulman et al., 2017) for multi-\nGPU training. LLaRP collects experience interactively by rolling out its policy auto-regressively to\ngenerate actions to take in the environment. For the PPO update, we sample minibatches from the\ncollected data and compute action log probabilities in parallel for the PPO update.\nDuring training, we freeze the LLM backbone and the visual encoder. A frozen visual encoder helps\nmaintain visual features that can generalize to different environments (Majumdar et al., 2023). A\nfrozen LLM backbone helps maintain language reasoning capabilities, which can be lost during fine\ntuning (Alayrac et al., 2022). For more details about the method, refer to Appendix C.1.\n4\nDataset Name\nInstruction Example\nDescription\nTrain\nFind an apple and put it away in the fridge.\nInstructions used to train the agent.\nNew Scenes\nFind an apple and put it away in the fridge\nSame instructions, but in new scenes. All other\ndatasets are in new scenes.\nParaphrasic Robustness\nInstruction\nRephrasing\nIn the fridge, stow an apple.\nSwap the order that nouns appear in the in-\nstruction and substitute synonyms for verbs.\nReferring\nExpressions\nFind the round red fruit and put it in the fridge\nRefer to objects by their visual appearance.\nSpatial\nRelationships\nFind an apple and put it in the receptacle to\nthe right of the kitchen counter.\nRefer to receptacles indirectly by their location\nrelative to other receptacles.\nContext\nI am hungry for something sweet and healthy.\nPut a snack for me on the table.\nDescribe a situation where a particular object\nfits the context.\nIrrelevant\nInstruction Text\nThere is a pear on the counter. Find an apple\nand put it away in the fridge.\nInstructions that include irrelevant context.\nMultiple\nRearrangements\nFind an apple, pear, and banana and put them\naway in the fridge.\nRearrange 3 objects (2 max during training).\nBehavior Gen.\nNovel Objects\nFind an orange and put it away in the fridge\nNew entity / instruction pairs.\nMultiple\nObjects\nPut all the apples in the fridge.\nFind all of an object.\nConditional\nInstructions\nIf the fridge is open, find an apple and put it\nthere. Otherwise put an apple on the table.\nAdjust behavior based on if the conditional\nstatement is true.\nTable 1: Evaluation datasets. The datasets with unseen instructions are divided into two categories:\nparaphrastic robustness which tests if the agent can produce the same behavior under linguistic\nvariation and behavior generalization where the agent has to demonstrate a new type of behavior.\nThe red text highlights the concept in the instruction distinct from the training distribution that\nemphasizes the dataset evaluation purpose. See Appendix B.5 for more details.\n4\nLANGUAGE REARRANGEMENT PROBLEM\nTo study generalization properties across a large number of language conditioned tasks we introduce\na novel problem called \u201cLanguage Rearrangement\u201d. Language Rearrangement strives to cover a\nlarge number of tasks of home environment tasks, such as, \u201cBring a mug to the couch.\u201d, \u201cStore\nall the fruit in the fridge.\u201d, or \u201cI am hungry, bring something from the kitchen to the table.\u201d. This\nproblem space extends the Rearrangement task (Batra et al., 2020) by defining 150,000 training and\n1,000 testing tasks, and providing a textual instruction for each one. The tasks require an agent to\ngeneralize to a variety of unseen instruction concepts requiring multiple object interactions (picking,\nplacing, opening, closing), searching for objects and logical reasoning (e.g. \u201cif\u201d statements).\n4.1\nTASK DEFINITION\nIn Language Rearrangement, an agent starts in an unseen house and is tasked to execute a com-\nmon household activity, that reduces to moving objects from specified start positions to desired goal\npositions. The agent is provided with a natural language instruction specifying the desired goal\nstate. We generate instructions at scale using instruction templates, instruction template rephras-\nings, scene-grounded entity enumeration, and a solver feasibility validation check. A sparse reward\nis provided for successfully completing the entire task or a subtask. Completing the instructions\nrequires the agent to explore. For example, to \u201cPut all the dishes in the sink if the sink is empty\u201d\nrequires the agent to explore to find all the dishes in the house. While exploring, the agent needs to\ndetect when it sees a plate and then pick it up. The instructions vary in what information is revealed\nto the agent (such as the starting positions of objects), how many objects to rearrange, and logical\nconcepts such as \u201cfor all\u201d, \u201cexists\u201d, conditionals, swapping, and removals. See complete task details\nin Appendix B.\nThe agent has to perform the tasks entirely from onboard sensing capabilities and without any pre-\nbuilt maps, object 3D models, or other privileged information. A simulated Fetch robot (Fetch\nRobotics, 2020) senses the world through a 256\u00d7256-pixel head-mounted RGB camera, robot joint\npositions, an indicator of whether the robot is holding an object or not, and base egomotion giving\nthe relative position of the robot from the start of the episode. The agent interacts with the world via\na mobile base, 7DoF arm, and suction grip attached to the arm.\n5\nSame Instructions\nParaphrastic Robustness\nNew Behaviors\nBehavior Generalization\nFigure 3: Success Rates across all evaluation tasks, aggregated in groups per Table 1. Across all\nunseen task groups, LLaRP generalizes better than the respective baselines, apart from multiple\nrearrangement, in which all models perform poorly. For exact numbers, see Table 4.\nWe supplement all methods with low-level skills and focus on high-level decision-making. Lan-\nguage Rearrangement poses the challenge of long-horizon tasks spanning tens of thousands of low-\nlevel control. Even single-object mobile pick and place tasks are challenging for end-to-end meth-\nods (Berges et al., 2023; Huang et al., 2023c). Yet hierarchical methods where a high-level policy\nselects from fixed skills is effective for rearrangement tasks (Gu et al., 2022). Therefore like other\nrearrangement works (Szot et al., 2023), we train a high-level policy to select low-level policies.\nThere are 70 skills for the high-level policy to choose between at every step. These skills include\npicking objects by name, placing on receptacles by name, navigating to receptacles, and opening and\nclosing receptacles by name. Example skills include pick(apple), place(sink), navigate(left counter),\nand open(fridge). See Appendix B.3 for details.\n4.2\nGENERALIZATION TEST DATASETS\nLanguage Rearrangement seeks to evaluate how well agents can complete language instructions\nduring evaluation that are different from those seen during training. To facilitate this, we generate\na dataset of 150k distinct training instructions covering basic rearrangement concepts, from single-\nobject interactions to finding and moving two objects. The train instructions include several phras-\nings of the same rearrangement concept and cover interacting with various objects and receptacles.\nFor evaluation, we construct an evaluation dataset that tests the generalization capabilities with re-\nspect to different rearrangement concepts expressed with language (see Table 1 and Appendix B.5).\nAll evaluation datasets are in unseen houses.\nParaphrastic Robustness (PR): The ultimate goal of training models capable of solving tasks from\nnatural language instruction is to allow humans to easily provide instructions to embodied agents.\nHowever, humans exhibit high variability in how they describe a task (Schreitter & Krenn, 2014), so\nit is necessary that agents are robust to paraphrasing. Paraphrasic Robustness (PR) evaluates if the\nagent can produce the same behavior under a linguistic variation. Such variations include new ways\nof saying the instruction and referring to objects indirectly rather than by name. The underlying goal\nof these instructions are contained in the training dataset.\nBehavior Generalization (BG): The second category of datasets test Behavior Generalization (BG)\nin which the agent has to demonstrate a new type of behavior specified by the language instructions\nnot present in the training dataset. This involves a new logical expression. For example, during\ntraining the agent was told how many objects to find. But BG includes the Multiple Rearrangements\ntask, which requires an agent to find \u201call\u201d of a particular object.\nWe believe these two axes of generalization have large coverage in terms of realistic semantic sit-\nuations that a robot could encounter. PR and BG roughly align with generalization concepts in\npsychology; PR is similar to what is referred to as \u201cstimulus generalization\u201d in psychology, while\nBG can be thought of as a type of \u201cresponse generalization\u201d (Shepard, 1957), though in the latter\nwe do not keep the linguistic instruction \u2013 the \u201cstimulus\u201d \u2013 fixed.\n6\n5\nEXPERIMENTS\n5.1\nBASELINES\nWe compare LLaRP to the following baselines. For all methods in Language Rearrangement, we\nuse the pre-trained VisualCortex model (VC1, Majumdar et al., 2023) a ViT backbone designed for\negocentric visual tasks. We represent the visual observations using the ViT [CLS] token from VC1.\nWe freeze the VC1 visual encoder module. All RL methods are trained with PPO. Further details\nfor all methods are in Appendix C.\n\u2022 ZS-ChatGPT/ ZS-LLaMA: Input the instruction to the LLM and zero-shot (ZS) plan a sequence\nof high-level actions. This policy is blind and plans based only on the language instruction. The\nprompt lists all the constraints (e.g., only one object can be picked at a time), possible actions,\nreceptacles and objects, along with examples of successful behavior. We compare against an\ninstruction-tuned LLaMA-65B (ZS-LLaMA) (Touvron et al., 2023) which generates a plan only\nonce at the beginning. We also compare against ChatGPT (ZS-ChatGPT) that performs multi-step\nreasoning to iteratively refine the plan based on proprioceptive feedback from the environment.\nFor details about the ZS baselines, prompt, and environment feedback see Appendix C.4.\n\u2022 ZS-Flamingo: Multimodal (text + image) version of the ZS-LLaMA baseline. Uses IDEFICS,\nan open source 80B parameter VLM model (Laurenc\u00b8on et al., 2023) based on Flamingo (Alayrac\net al., 2022). Given a prompt similar to ZS-LLaMA, the image observation and the textual in-\nstruction, ZS-Flamingo outputs a single plan for the agent to follow.\n\u2022 LLaRP-Scratch: Same architecture as LLaMA-7B but with 2 billion parameters. The entire\ntransformer is trained from scratch and the pretrained visual encoder is frozen.\n\u2022 LSTM-Flan/ LSTM-LLaMA: The instruction is encoded as a fixed-length vector that an LSTM\ntakes as input along with the observation encoding. The action is predicted from the LSTM hidden\nstate. LSTM-Flan uses Flan-T5 (Chung et al., 2022) to encode the instruction while LSTM-\nLLaMA uses LLaMA and a Perciever Resampler network (Jaegle et al., 2021).\n5.2\nEMPIRICAL ANALYSIS\nIn this section, we analyze the empirical performance of LLaRP and the baselines on Language\nRearrangement. We show LLaRP has better zero-shot generalization capabilities than the baselines\non most of the unseen Language Rearrangement evaluation datasets. LLaRP also learns efficiently,\nlearning faster during training and comparing favorably to training with expert demonstrations.\nLLaRP improves generalization across all generalization types. We report aggregate success\nrates across all generalization datasets in Table 2. We report the mean and standard deviation across\n3 random seeds for all RL-based methods. We see that LLaRP is almost 1.7x better than the next\nbest performing baseline, 42% vs. 25%. This trend is true for both Paraphrastic and Behavior\ngeneralizations, which shows that the use of a LLM allows for a model that can better understand\nnatural language and execute novel tasks. Althought this is expected for paraphrastic robustnes as\nLLMs are known for their language understanding capabilities, it is somewhat surprising to see that\nthe LLM helps even for novel behaviors, achieving 45% vs. 28% from LSTM-Flan.\nMethod\nTotal\nParaphrastic\nRobustness\nBehavior\nGeneralization\nLLaRP\n42 \u00b1 2\n38 \u00b1 1\n45 \u00b1 3\nLLaRP-Scratch\n17 \u00b1 4\n16 \u00b1 3\n18 \u00b1 5\nLSTM-Flan\n25 \u00b1 1\n23 \u00b1 1\n28 \u00b1 1\nLSTM-LLaMA\n2 \u00b1 1\n3 \u00b1 2\n0 \u00b1 0\nZS-ChatGPT\n22\n21\n23\nZS-LLaMA\n12\n10\n14\nZS-Flamingo\n6\n4\n8\nTable 2: Combined Zero-Shot Results. LLaRP\noutperforms all baselines across all categories.\nResults across individual generalization test\nsets are shown in Figure 3. LLaRP displays\nsuperior generalization capabilities across all\nsettings. Most learned methods achieve near\n100% success rates on the train split and almost\n100% when one varies the scene, but uses the\nsame instructions as in train.\nLLaRP performs 7% and 12% better when we\nrephrase instructions or talk about novel objects\ncompared to the next best performing model.\nHowever, the boost LLaRP gets from using a LLM is substantial for more complex novel tasks,\ne. g. when we use a context, have a conditional statement, multiple chained rearrangements, etc.\nThe second best performing model is LSTM-Flan, which like LLaRP uses a billion parameter lan-\nguage model (Flan-T5). However, LSTM-Flan uses it to comprehend instructions through encoding\nthe instruction to a fixed size vector, rather than using the LLM as a decoder like LLaRP. LSTM-\n7\n0\n1\n2\n3\n4\n5\nEnvironment Steps\n1e8\n20\n40\n60\n80\n100\nSuccess Rate (%)\nLLaRP\nLLaRP-Scratch\nLSTM-Flan\nLSTM-LLaMA\n(a)\n0.0\n0.5\n1.0\n1.5\n2.0\nEnvironment Steps\n1e7\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\nLLaRP\nLSTM-Flan\n(b)\n500\n1k\n10k\n50k\n150k\nInstruction Count\n0\n10\n20\n30\n40\n50\n60\nSuccess Rate (%)\nRL-LLaRP\nIL-LLaRP\n(c)\nFigure 4: Success rates (SR) for different stages of training (left and middle) or number of episodes /\ndemonstrations (right). (a) SR vs number of steps during learning. (b) SR vs number of steps during\ncontinual learning on \u201cMultiple Rearrangments\u201d tasks. (c) LLaRP with RL vs LLaRP with IL with\nthe same number of episodes / demonstrations. See text for further discussion.\nFlan generalizes in some scenarios such as to new phrasings and objects, indicating that the LSTM\npolicy learned to interpret these aspects of the FLAN embedding. Whereas unlike LLaRP, LSTM-\nFlan performs worse on Behavior Generalization than Paraphrasic Robustness, indicating that using\nthe LLM directly as a decoder is important for generalization to new behaviors. We also compare\nto LSTM-LLaMA which uses the same LLaMA hidden state activations as LLaRP. However, it\nperforms generally worse, perhaps because LLaMA was not trained as an encoder model.\nThe zero-shot application of LLM, ZS-ChatGPT, requires no training and cannot receive images\nas inputs. Nevertheless, it achieves better-than-random performance across many of the evaluation\nsplits, which demonstrates that the LLM contains relevant information for embodied tasks.\nLLMs lead to faster learning convergence. As shown in Figure 4a, LLaRP is the most sample\nefficient model, converging in around 50M-100M steps before LSTM-Flan, despite being a sub-\nstantially larger model. Further, LLaRP-Scratch takes 10x more environment samples to converge\nthan LLaRP (50M versus 500M) despite both having the same architecture, showing that pre-trained\nLLMs are a good fit for the problem space.\nNote that only a few prior works have used transformers for online RL (Parisotto et al., 2020). In\nAppendix D.1, we further explain the training architecture and settings that were critical to stably\ntrain LLaRP and baseline transformer policy training with PPO.\nAgg\nEval\nPara\nRobust\nBehav\nGen\n35\n40\n45\n50\n55\n60\nSuccess Rate (%)\nLLaRP-7b\nLLaRP-13b\nFigure 5:\nAverage success rates\nacross our evaluation datasets for\nLLaRP scaled from Llama-7b to\nLlama-13b.\nLLMs leads to faster continual learning convergence. To\nevaluate the continual learning efficiency, we continue learn-\ning the model on downstream tasks beyond the training tasks\nand analyze the training convergence. Specifically, we take the\n\u201cMultiple Rearrangements\u201d dataset, generate 10K episodes,\nand train until convergence.\nThe results, shown in Fig-\nure 4b, show that LLaRP is 3x more efficient than LSTM-Flan,\nachieving near perfect performance in 500k vs 1.5M environ-\nment steps. Hence, LLMs can lead to faster learning of addi-\ntional tasks.\nLLaRP with RL performs better and utilizes supervision\nmore efficiently than IL. A common paradigm of endow-\ning LLMs with novel capabilities is is Imitation Learning (IL)\nwith decision making data. More recently, works have en-\ndowed LLMswith decision making capabilities in embodied\nsettings (Driess et al., 2023; Brohan et al., 2023).\nHence, a natural question is to compare LLaRP trained with RL vs IL. IL uses demonstrations\ngenerated by a fully trained LLaRP policy. For each data point, we use the same number of episodes\nfor RL as demonstrations for IL, both denoted by instruction count, and train until convergence.\nIn Figure 4c, we see that for any number of episodes (or demonstrations) RL outperforms IL, with\na larger margin in the low regime of data. This shows that in RL settings, an LLM-based LLaRP is\nable to collect episodes that are useful for policy improvement. For RL we merely need a reward\n8\n1\nnavigate(brown_table)\n2\nnavigate(dark_table)\n3\nnavigate(blue_table)\npick(screwdriver)\nnavigate(sofa)\nplace(screwdriver, sofa)\n4\nLocate a tool to \ufb01x a loose screw \nin the sofa and bring it there.\n5\n6\n1\n2\n3\n4\n5\n6\n4\n6\nFigure 6: Success trajectory in the Context dataset. Arrows indicate navigation to receptacles. Right\nshows actions selected by LLaRP. Upper-left show egocentric observations for some steps.\ndefinition, while in IL settings full demonstration trajectories are required, thus making RL less\ncostly than IL for each instruction count.\nLarger LLMs lead to better results. In Figure 5, we show the effect of scaling the size of the LLM\nin LLaRP. We compare using LLaMA-13B and LLaMA-7B in LLaRP. We find that LLaMA-13B\ngives a 4% boost in total evaluation performance. This indicates that larger, more capable LLMs,\ncan translate to more capable embodied reasoning.\nQualitative Result. In Fig. 6, we show a success example of LLaRP on the Context dataset. The\nagent explores for the first 3 actions. Then it finds the screwdriver implied by the phrase \u201cfix a\nloose screw\u201d in the instruction. It then successfully brings that screwdriver to the couch. For more\nqualitative examples in the other datasets, see Appendix E and videos at https://llm-rl.github.io.\nIQM\nMedian\nMean\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nHuman-Normalized Score\nLLaRP\nLLaRP-Scratch\nFigure 7:\nLLaRP and LLaRP-\nScratch performance over 55 Atari\ngames (two seeds per game).\nLLMs boost performance with tasks beyond Language\nRearrangement.\nWe evaluate LLaRP across Atari 2600\ngames using the Arcade Learning Environment (Bellemare\net al., 2013), configured following the recommendations\nof (Machado et al., 2018).\nWe train LLaRP and LLaRP-\nScratch as in Habitat, with the exception of using a fully-\ntrainable ResNet-18 visual encoder, as Atari is visually dis-\ntinct from the training data used for VC-1. To take advan-\ntage of the diversity in available Atari environments, we train\nLLaRP and LLaRP-scratch individually on each game of 55\nAtari games for 100M environment steps, and report human-\nnormalized scores (Mnih et al., 2013).\nWe find that LLaRP outperforms LLaRP-Scratch by a large\nmargin, with LLaRP achieving a higher average score on 43\nout of 55. When aggregated, LLaRP\u2019s mean, median, and in-\nterquartile mean (Agarwal et al., 2021) performance are between 3x and 4x higher than LLaRP-\nScratch (see Figure 7). Additional details about our Atari experiments can be found in Appendix C.6.\nFurther Experiments. Additional analyses in Appendix D show the impact of batch size and LLM\nweights for RL with transformers, unfreezing LLM weights for LSTM-Flan, LLaRP efficiency, and\nthat our findings hold in a harder setting with no invalid actions and a termination action.\n6\nCONCLUSION\nWe introduce LLaRP, a scheme for employing pretrained LLMs on embodied tasks with reinforce-\nment learning. To aid in our research, we introduce a dataset of rearrangement tasks (consisting\nof 150k training instructions and 10 challenging evaluation datasets). LLaRP outperforms non-\npretrained transformer and LSTM-based models on both sample efficiency and generalization. It\nalso significantly outperforms zero-shot baselines using state-of-the-art LLMs. Limitations to ad-\ndress in the future include RL design decisions to support the significantly larger size of LLMs than\ntypical RL policies, as well as generalizing to variable action spaces expressed by language.\n9\n7\nACKNOWLEDGEMENTS\nThe authors would like to thank Eduord Grave, Mathias Muller, Josh Susskind, Barry Theobald,\nYizhe Zhang for valuable comments and discussion.\nREFERENCES\nJosh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev, Alex Goldin, Alden Hung, Jes-\nsica Landon, Timothy Lillicrap, Alistair Muldal, Blake Richards, et al. Evaluating multimodal\ninteractive agents. arXiv preprint arXiv:2205.13274, 2022. 3\nConstructions Aeronautiques, Adele Howe, Craig Knoblock, ISI Drew McDermott, Ashwin Ram,\nManuela Veloso, Daniel Weld, David Wilkins SRI, Anthony Barrett, Dave Christianson, et al.\nPddl\u2014 the planning domain definition language. Technical Report, Tech. Rep., 1998. 16\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare.\nDeep reinforcement learning at the edge of the statistical precipice. NeurIPS, 2021. 9\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. 1\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u2013\n23736, 2022. 1, 3, 4, 7, 22\nDhruv Batra, Angel X Chang, Sonia Chernova, Andrew J Davison, Jia Deng, Vladlen Koltun, Sergey\nLevine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, et al. Rearrangement: A challenge for\nembodied ai. arXiv preprint arXiv:2011.01975, 2020. 5\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-\nment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47,\n2013. 9\nVincent-Pierre Berges, Andrew Szot, Devendra Singh Chaplot, Aaron Gokaslan, Roozbeh Mottaghi,\nDhruv Batra, and Eric Undersander. Galactic: Scaling end-to-end reinforcement learning for rear-\nrangement at 100k steps-per-second. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 13767\u201313777, 2023. 6\nBenjamin Bolte, Austin Wang, Jimmy Yang, Mustafa Mukadam, Mrinal Kalakrishnan, and Chris\nPaxton.\nUsa-net: Unified semantic and affordance representations for robot memory.\narXiv\npreprint arXiv:2304.12164, 2023. 3\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics\ntransformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. 3\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choroman-\nski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action\nmodels transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 1, 3,\n8\nBerk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar.\nThe ycb object and model set: Towards common benchmarks for manipulation research. In 2015\ninternational conference on advanced robotics (ICAR), pp. 510\u2013517. IEEE, 2015. 18\nGeorgia Chalvatzaki, Ali Younes, Daljeet Nandha, An T. Le, Leonardo F. R. Ribeiro, and Iryna\nGurevych. Learning to reason over scene graphs: a case study of finetuning gpt-2 into a robot\nlanguage model for grounded task planning. Frontiers in Robotics and AI, 2023. 2\n10\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel,\nAravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence\nmodeling. Advances in neural information processing systems, 34:15084\u201315097, 2021a. 26\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021b. 1\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod-\nels. arXiv preprint arXiv:2210.11416, 2022. 7, 20\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 21\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multi-\nmodal language model. arXiv preprint arXiv:2303.03378, 2023. 1, 3, 8\nYuqing Du, Olivia Watkins, Zihan Wang, C\u00b4edric Colas, Trevor Darrell, Pieter Abbeel, Abhishek\nGupta, and Jacob Andreas. Guiding pretraining in reinforcement learning with large language\nmodels. arXiv preprint arXiv:2302.06692, 2023. 2\nKevin Esslinger, Robert Platt, and Christopher Amato. Deep transformer q-networks for partially\nobservable reinforcement learning. arXiv preprint arXiv:2206.01078, 2022. 26\nFetch Robotics. Fetch. http://fetchrobotics.com/, 2020. 5\nRichard E Fikes and Nils J Nilsson. Strips: A new approach to the application of theorem proving\nto problem solving. Artificial intelligence, 2(3-4):189\u2013208, 1971. 18\nSamir Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharco, Ludwig Schmidt, and Shuran Song.\nCows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n23171\u201323181, 2023. 3\nAmelia Glaese, Nat McAleese, Maja Trkebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Mari-\nbeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of\ndialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022. 1\nJiayuan Gu, Devendra Singh Chaplot, Hao Su, and Jitendra Malik. Multi-skill mobile manipulation\nfor object rearrangement. arXiv preprint arXiv:2209.02778, 2022. 6\nHuy Ha and Shuran Song. Semantic abstraction: Open-world 3d scene understanding from 2d\nvision-language models. In 6th Annual Conference on Robot Learning, 2022. 3\nHuy Ha, Pete Florence, and Shuran Song. Scaling up and distilling down: Language-guided robot\nskill acquisition. arXiv preprint arXiv:2307.14535, 2023. 3\nYining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang\nGan. 3d-llm: Injecting the 3d world into large language models. arXiv preprint arXiv:2307.12981,\n2023. 3\nHengyuan Hu and Dorsa Sadigh. Language instructed reinforcement learning for human-ai coordi-\nnation. arXiv preprint arXiv:2304.07297, 2023. 2\nChenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual language maps for robot\nnavigation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp.\n10608\u201310615. IEEE, 2023a. 3\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022. 2\n11\nWenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor\nMordatch, Sergey Levine, Karol Hausman, et al. Grounded decoding: Guiding text generation\nwith grounded models for robot control. arXiv preprint arXiv:2303.00855, 2023b. 2\nXiaoyu Huang, Dhruv Batra, Akshara Rai, and Andrew Szot. Skill transformer: A monolithic policy\nfor mobile manipulation. arXiv preprint arXiv:2308.09873, 2023c. 6\nAndrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David\nDing, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A\ngeneral architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021. 7,\n21\nKrishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Shuang\nLi, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al. Conceptfusion: Open-set\nmultimodal 3d mapping. arXiv preprint arXiv:2302.07241, 2023. 3\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for\nmultimodal generation. arXiv preprint arXiv:2301.13823, 2023. 1\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel\nGordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: an interactive 3d environment\nfor visual AI. arXiv preprint arXiv:1712.05474, 2019. 1\nHugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov,\nThomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. Obelisc: An open\nweb-scale filtered dataset of interleaved image-text documents. arXiv preprint arXiv:2306.16527,\n2023. 7, 22\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra-\nmasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative\nreasoning problems with language models. Advances in Neural Information Processing Systems,\n35:3843\u20133857, 2022. 1\nChengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart\u00b4\u0131n-\nMart\u00b4\u0131n, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A\nbenchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference\non Robot Learning, pp. 80\u201393. PMLR, 2023a. 1\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-\nimage pre-training with frozen image encoders and large language models.\narXiv preprint\narXiv:2301.12597, 2023b. 1\nWing\nLian,\nBleys\nGoodson,\nEugene\nPentland,\nAustin\nCook,\nChanvichet\nVong,\nand\nTeknium.\nOpenorca:\nAn\nopen\ndataset\nof\ngpt\naugmented\nflan\nreasoning\ntraces.\nhttps://https://huggingface.co/Open-Orca/OpenOrca, 2023. 22\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and\nAndy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA), pp. 9493\u20139500. IEEE, 2023. 2\nHao Liu, Lisa Lee, Kimin Lee, and Pieter Abbeel. Instruction-following agents with jointly pre-\ntrained vision-language models. arXiv preprint arXiv:2210.13431, 2022. 3\nCorey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis\nArmstrong, and Pete Florence. Interactive language: Talking to robots in real time. IEEE Robotics\nand Automation Letters, 2023. 3\nMarlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, and\nMichael Bowling. Revisiting the Arcade Learning Environment: Evaluation Protocols and Open\nProblems for General Agents. Journal of Artificial Intelligence Research, 61:523\u2013562, 2018. 9\n12\nArjun Majumdar, Karmesh Yadav, Sergio Arnaud, Yecheng Jason Ma, Claire Chen, Sneha Silwal,\nAryan Jain, Vincent-Pierre Berges, Pieter Abbeel, Jitendra Malik, Dhruv Batra, Yixin Lin, Olek-\nsandr Maksymets, Aravind Rajeswaran, and Franziska Meier. Where are we in the search for an\nartificial visual cortex for embodied intelligence? arXiv preprint arXiv:2303.18240, 2023. 4, 7\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-\nstra, and Martin Riedmiller.\nPlaying atari with deep reinforcement learning.\narXiv preprint\narXiv:1312.5602, 2013. 9, 25\nSteven Morad, Ryan Kortvelesy, Matteo Bettini, Stephan Liwicki, and Amanda Prorok. Popgym:\nBenchmarking partially observable reinforcement learning. arXiv preprint arXiv:2303.01859,\n2023. 26\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and\nAhmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4, 2023.\n22\nVivek Myers, Andre He, Kuan Fang, Homer Walke, Philippe Hansen-Estruch, Ching-An Cheng,\nMihai Jalobeanu, Andrey Kolobov, Anca Dragan, and Sergey Levine.\nGoal representations\nfor instruction following: A semi-supervised language interface to control.\narXiv preprint\narXiv:2307.00117, 2023. 3\nTianwei Ni, Benjamin Eysenbach, and Ruslan Salakhutdinov. Recurrent model-free rl can be a\nstrong baseline for many pomdps. arXiv preprint arXiv:2110.05038, 2021. 26\nTianwei Ni, Michel Ma, Benjamin Eysenbach, and Pierre-Luc Bacon. When do transformers shine\nin rl? decoupling memory from credit assignment. arXiv preprint arXiv:2307.03864, 2023. 26\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022. 1\nEmilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar,\nMax Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers\nfor reinforcement learning. In International conference on machine learning, pp. 7487\u20137498.\nPMLR, 2020. 8, 26\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu\nWei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824, 2023. 1\nMarco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss. Memory gym: Partially observ-\nable challenges to memory-based agents. In The Eleventh International Conference on Learning\nRepresentations, 2022. 26\nAlec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision. In ICML, 2021. 22\nStephanie Schreitter and Brigitte Krenn. Exploring inter-and intra-speaker variability in multi-modal\ntask descriptions. In The 23rd IEEE International symposium on robot and human interactive\ncommunication, pp. 43\u201348. IEEE, 2014. 6\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick\nSchramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk,\nand Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text\nmodels.\nIn Thirty-sixth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track, 2022. 22\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 4\n13\nDhruv Shah, B\u0142a\u02d9zej Osi\u00b4nski, Sergey Levine, et al. Lm-nav: Robotic navigation with large pre-\ntrained models of language, vision, and action. In Conference on Robot Learning, pp. 492\u2013504.\nPMLR, 2023. 2, 3\nRoger N Shepard. Stimulus and response generalization: A stochastic model relating generalization\nto distance in psychological space. Psychometrika, 22(4):325\u2013345, 1957. 6\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,\nLuke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions\nfor everyday tasks. In CVPR, 2020. 3\nMohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic\nmanipulation. In Conference on Robot Learning, pp. 894\u2013906. PMLR, 2022. 3\nMohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for\nrobotic manipulation. In Conference on Robot Learning, pp. 785\u2013799. PMLR, 2023. 3\nTom Silver, Soham Dan, Kavitha Srinivas, Joshua B Tenenbaum, Leslie Pack Kaelbling, and\nMichael Katz. Generalized planning in pddl domains with pretrained large language models.\narXiv preprint arXiv:2305.11014, 2023. 2\nVitalii Sopov and Ilya Makarov. Transformer-based deep reinforcement learning in vizdoom. In\nInternational Conference on Analysis of Images, Social Networks and Texts, pp. 96\u2013110. Springer,\n2021. 26\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n4\nAndrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah\nMaestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, et al. Habitat 2.0:\nTraining home assistants to rearrange their habitat. Advances in Neural Information Processing\nSystems, 34, 2021. 1, 18, 20\nAndrew Szot, Unnat Jain, Dhruv Batra, Zsolt Kira, Ruta Desai, and Akshara Rai. Adaptive coordi-\nnation in social embodied rearrangement. arXiv preprint arXiv:2306.00087, 2023. 6\nDeepMind Interactive Agents Team, Josh Abramson, Arun Ahuja, Arthur Brussee, Federico\nCarnevale, Mary Cassin, Felix Fischer, Petko Georgiev, Alex Goldin, Mansi Gupta, et al. Cre-\nating multimodal interactive agents with imitation and self-supervised learning. arXiv preprint\narXiv:2112.03763, 2021. 3\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog\napplications. arXiv preprint arXiv:2201.08239, 2022. 1\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 7, 21, 22\nAaron Walsman, Muru Zhang, Sanjiban Choudhury, Dieter Fox, and Ali Farhadi. Impossibly good\nexperts and how to follow them. In The Eleventh International Conference on Learning Repre-\nsentations, 2022. 19\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023. 2\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022. 1\nErik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva,\nand Dhruv Batra. Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames.\narXiv preprint arXiv:1911.00357, 2019. 4, 21\n14\nJimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg,\nSzymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: Personalized robot assistance with\nlarge language models. arXiv preprint arXiv:2305.05658, 2023a. 2\nYue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom\nMitchell, and Shrimai Prabhumoye. Plan, eliminate, and track\u2013language models are good teachers\nfor embodied agents. arXiv preprint arXiv:2305.02412, 2023b. 2\nTed Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey\nLevine, and Jonathan Tompson.\nRobotic skill acquisition via instruction augmentation with\nvision-language models. arXiv preprint arXiv:2211.11736, 2022. 3\nMengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang\nGan. Prompting decision transformer for few-shot policy generalization. In International Con-\nference on Machine Learning, pp. 24631\u201324645. PMLR, 2022. 26\nSriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna,\nTheophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, et al.\nHomerobot: Open-vocabulary mobile manipulation. arXiv preprint arXiv:2306.11565, 2023. 3\nAlbert Yu and Raymond J Mooney. Using both demonstrations and language instructions to effi-\nciently learn robotic tasks. arXiv preprint arXiv:2210.04476, 2022. 3\nTianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh,\nClayton Tan, Jodilyn Peralta, Brian Ichter, et al. Scaling robot learning with semantically imag-\nined experience. arXiv preprint arXiv:2302.11550, 2023. 3\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker,\nFederico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Com-\nposing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.\n2\nQinqing Zheng, Amy Zhang, and Aditya Grover.\nOnline decision transformer.\narXiv preprint\narXiv:2202.05607, 2022. 26\n15\nMore information about the code release for the Language Rearrangement benchmark and LLaRP\ncan be found at https://llm-rl.github.io. The appendix is structured as follows:\nA Author Contributions.\nB Additional details about Language Rearrangement and dataset analysis.\nC LLaRP and baseline details.\nD Additional experiments including:\nD.1 Analyzing important aspects of training transformers with RL.\nD.2 Additional experiments including full Language Rearrangement zero-shot generalization\nnumbers, comparing efficiency results, and the choice of unfreezing the Flan encoder in\nLSTM-Flan.\nD.3 Comparing performance in a harder version of Language Rearrangement where invalid ac-\ntions terminate the episode and a stop action is required.\nE Visualizations of success trajectories of LLaRP on the Language Rearrangement evaluation\ndatasets.\nA\nCONTRIBUTIONS\nAndrew Szot co-initiated the project, design both the benchmark and co-designed and implemented\nthe algorithmic solution, conducted all RL and IL experiments on Habitat and their analysis, and\nco-wrote the paper.\nMax Schwarzer ran experiments on Atari and co-wrote the paper.\nHarsh Agrawal implemented, improved, and analyzed the zero-shot baselines, worked on dataset\nand evaluation methodology, and co-wrote the paper,\nBogdan Mazoure help with debugging training setup and co-wrote the paper.\nRin Metcalf Susa investigated non-LLaMA LLMs and edited the paper.\nWalter Talbott ran model probing analysis and contributed to discussions.\nNatalie Mackraz worked on RL policy text input and provided feedback for the paper.\nDevon Hjelm advised on experiment setup, helped shape up the overall story of the work, and\nco-wrote the paper.\nAlexander Toshev co-initiated the project, managed and advised throughout the project, co-\ndesigned the algorithmic solution, and co-wrote the paper.\nB\nLANGUAGE REARRANGEMENT DETAILS\nB.1\nPLANNING DOMAIN DEFINITION LANGUAGE (PDDL) DETAILS\nLanguage Rearrangement is represented with a Planning Domain Definition Language (PDDL)\nspecification (Aeronautiques et al., 1998). Specifically, each episode is linked to a PDDL problem\nspecification. This specification consists of the following components:\n\u2022 Entity types: Each entity in the scene is associated with an entity type. This entity type is used\nto determine which predicates are applicable. The type system is hierarchical, and derived types\nalso apply to higher level types. We define core types such as pickable object and receptacle. We\nautomatically derive types from the object dataset based on object high-level categories such as\nfruit and object classes such as apple.\n\u2022 Entities: For each object and receptacle in the scene, the PDDL associates it with a symbolic\nentity. Each entity has an associated entity type. The entities are automatically populated for each\nrearrangement episode based on the loaded objects and receptacles.\n\u2022 Predicates: These are binary expressions that are evaluated based on the underlying simulator\nstate. For example, on top(X : pickable object, Y : receptacle) corresponds to if object X is on top\nof receptacle Y.\n16\nBetween episodes, the only component that changes is the \u201cEntities\u201d category since each episode\nwill have different objects. The same entity types and predicates apply between episodes.\nA predicate expression refers to a boolean expression stated in first order logic involving the PDDL\ncomponents. Predicate expressions involve the predicates, entities, entity types, logical connectives\n(\u201cand\u201d, \u201cor\u201d, \u201cnot\u201d) and quantifier symbols (\u201cfor all\u201d, \u201cexists\u201d). For example, the predicate expres-\nsion \u2203 x : is type (x, \u201capple\u201d), on top(x, \u201ctable\u201d) will be evaluated to true, when any apple is\nplaced on top of the table, and false otherwise. As described in the next section, we use predicate\nexpressions to describe success criteria for instructions.\nB.2\nINSTRUCTION GENERATION DETAILS\nWe create a scalable pipeline for generating a large number of satisfiable, plausible instructions from\na small number of instruction templates. Each instruction is associated with a predicate expression\ndefining the success criteria for that instruction. These components are described in detail below.\nInstruction Template: This refers to a particular outcome in the environment and corresponding\nways of expressing this outcome in language where nouns are replaced with placeholder variables.\nSpecifically, an instruction template consists of:\n\u2022 Template Goal Condition: This describes the desired outcome for the instruction template\nrepresented as an ungrounded predicate expression (predicate expressions are described in Ap-\npendix B.1). It is ungrounded because there are placeholder variables for entities rather than ac-\ntual entities in the scene to refer to multiple outcomes depending on what entities are substituted\ninto the placeholders. For example, for the ungrounded desired outcome of an object going on a\nreceptacle, the goal condition would be the predicate expression \u2203 x : is type (x, \u201cOBJECT\u201d),\non top(x, \u201cRECEPTACLE\u201d), where OBJECT and RECEPTACLE are placeholders.\n\u2022 Placeholder Constraints: The template goal condition used a set of placeholder variables. But\nin the predicate expression \u2203 x : is type (x, \u201cOBJECT\u201d), on top(x, \u201cRECEPTACLE\u201d) we need\nOBJECT to be a pickable entity type and RECEPTACLE to be a receptacle entity type. The\ninstruction template thus contains constraints on the entities that can be sampled for the template\ngoal condition.\n\u2022 Instruction Language Templates: Each instruction template has a set of N ways of expressing\nthe template goal condition in ungrounded natural language. It is ungrounded because the lan-\nguage will contain placeholder variables that will be later substituted for entities in the scene. For\nexample, \u201cMove an \u2018OBJECT\u2018 to the \u2018RECEPTACLE\u2018\u201d is a language template for the template\ngoal condition \u2203 x : is type (x, \u201cOBJECT\u201d), on top(x, \u201cRECEPTACLE\u201d). We set N to 11, mean-\ning each template goal has 11 different language templates, each expressing the same template\ngoal condition with different language.\nWe define a set of instruction templates for each of the datasets from Section 4.2. We show examples\nof the language templates from these instruction templates for all the datasets in Table 6.\nThe next step of our pipeline uses these instruction templates to generate a dataset of rearrangement\nepisodes. When generating a particular dataset, we start by randomly sampling a template from that\ndataset. We then substitute random entities into the template placeholders, taking into account the\nplaceholder constraints. We then randomly sample a scene that is compatible from the associated\nscene set. A scene is compatible with an instruction if all the receptacles the instruction refers to\nare present in the scene. We then populate the scene with objects. We constrain the object sampling\nfor the template. We include all template substituted entities and constraints in the object sampling\nprocess. Thus an episode consists of:\n\u2022 Scene: The empty scene specifying the house floor plan.\n\u2022 Entity Locations: The transformations of all receptacles and objects.\n\u2022 (Grounded) Goal Condition: A predicate expression describing the desired outcome without any\nplaceholder variables.\n\u2022 (Grounded) Instruction Language: An instruction in natural language specifying the goal con-\ndition without any placeholder variables.\nNote that the robot starting transformation is not included in the above, and is instead randomly\ngenerated at the start of every episode.\n17\nTraining Instructions\nEvaluation Instructions\n(a) Instruction embedding T-SNE.\nConditional\nInstruct\nMultiple\nRearrange\nIrrelevant\nText\nSpatial\nContext\nObject\nDesc\nNovel\nObjects\nTrain\nInstruct\nRephrasing\nMultiple\nObjects\n0\n5\n10\n15\n20\n25\n30\nToken Count\n(b) Avg Tokens per Instruct\nFigure 8: Left: Visualization of the evaluation and training dataset instructions. We embed the\ninstruction using Flan. Only the instructions depicted in red are seen during training. Right: The\naverage number of language tokens per instruction in each dataset.\nWe then check that each generated episode is solvable. Random object placement and physics\nsimulation can result in objects falling and other instabilities. We check the episode is solvable by\nan oracle planner. We run a STRIPS based planner Fikes & Nilsson (1971) that operates from the\nPDDL problem instantiated by the episode. If the planner times out or the solution is longer than 32\nhigh-level steps (a high-level corresponds to a single skill, defined in Appendix B.3), we consider\nthe episode unsolvable and remove it from the training set.\nB.3\nSKILL DETAILS\nWe train the policies in an abstracted, high-level action space. Each high-level action corresponds\nto a particular skill invocation, for example, \u201c pick(apple)\u201d. We consider the following skills:\n\u2022 Navigation: Parameterized by the name of the receptacle to navigate to. So long as the receptacle\nis present in the scene, this skill is always valid\n\u2022 Pick: Parameterized by the name of the object to pick. Only valid if the robot is close to the\nobject, not holding another object, and the object is not inside a closed receptacle.\n\u2022 Place: Parameterized by the name of the receptacle to place the object on. Only valid if the robot\nis close to the receptacle and is holding an object.\n\u2022 Open: Parameterized by the name of the receptacle to open. Only valid if the receptacle is closed\nand the robot is close to the receptacle.\n\u2022 Close: Parameterized by the name of the receptacle to close. Only valid if the receptacle is open\nand the robot is close to the receptacle.\nThe skill is only executed if it is valid in the current state. The action space consists of all possible\nskill parameterizations given all possible objects, giving 70 total skills meaning 70 actions for the\npolicy to select from. Note that some of these skills may be applied to objects that are not present\nin the current scene, in which case selecting that skill would be an invalid action. Furthermore, the\nactions are fixed at every step, and thus a majority of the actions will be invalid at a given step. We\ntreat invalid actions as no-ops, but we compare to where the robot is not allowed to take invalid\nactions in Appendix D.3. If the action is valid, we execute it and instantaneously transform the\nsimulator state based on the effect of the skill. For example, if the skill \u201cpick(apple)\u201d is selected and\nthe robot is near the apple, not holding anything, and the apple is not in a receptacle, then the apple\nwill teleport to the robot\u2019s gripper.\nB.4\nADDITIONAL TASK DETAILS\nThe episode is considered successful if the goal condition evaluates to true. The episode is a failure\nif the agent doesn\u2019t achieve success in under 32 high-level policy steps. We consider requiring the\nagent to call a separate stop action in Appendix D.3. We use the objects from the YCB (Calli et al.,\n2015) and ReplicaCAD (Szot et al., 2021) object datasets. The object categories used are: \u201cball,\nclamp, hammer, screwdriver, padlock, scissors, block, drill, spatula, knife, spoon, plate, sponge,\n18\nHyperparameter\nLLaRP\nLLaRP-Scratch\nLSTM-Flan\nLSTM-LLaMA\nLR\n3e\u22124\n3e\u22124\n3e\u22124\n3e\u22124\nOptimizer\nAdamW\nAdamW\nAdam\nAdam\nNumber of Mini Batches\n6\n6\n4\n4\nEnvironments Per GPU\n18\n18\n18\n18\nEntropy Coefficient\n0.01\n0.01\n0.01\n0.01\nValue Loss Coefficient\n0.5\n0.5\n0.5\n0.5\nNumber of Rollout Steps\n32\n32\n32\n32\nNumber of PPO Epochs\n1\n1\n2\n2\nBatch Size Per Update\n768\n768\n1152\n1152\nTable 3: Hyperparameters for all RL methods.\ncleanser, plum, pear, peach, apple, lemon, can, box, banana, strawberry, lego, rubriks cube, book,\nbowl, cup, fork\u201d. We hold out \u201cmug, orange, lid, toy airplane, wrench\u201d for the Novel Objects\nevaluation split.\nThe Language Rearrangement reward function consists of a sparse reward for completing the task,\nsubgoal rewards for completing individual parts of the task, and a slack penalty for completing the\ntask faster. The reward at step t is:\nrt = 10 \u00b7 1success + 5 \u00b7 1subgoal \u2212 0.1 \u00b7 1invalid \u2212 0.01\nWhere 1success indicates if the PDDL goal expression is evaluated as true, meaning the episode was\nsuccessfully solved. 1invalid indicates if the agent called an invalid action at the current step. 1subgoal\nindicates if the agent achieved any subgoal necessary to achieve the overall goal. For example, for\nthe instruction \u201cFind an apple and put it away in the fridge\u201d, the agent needs to first pick up the\napple and potentially open the fridge. When running the STRIPS planner in the episode validation\nprocess described in Appendix B.2, we compute the optimal action sequence and extract subgoals\nfrom this action sequence. We then reward the agent for reaching any of these high-level subgoals.\nNote we cannot directly imitate the optimal action sequence because it is computed with oracle state\ninformation and is an \u201cimpossibly good\u201d expert (Walsman et al., 2022).\nB.5\nADDITIONAL DATASET DETAILS\nFurther details on the Paraphrasic Robustness datasets:\n\u2022 Instruction Rephrasing: The same underlying instruction goals, but stated in a different way. For\nexample, a rephrasing of the training instruction. The order that nouns appear in the instruction is\npermuted and synonyms for verbs are substituted.\n\u2022 Referring Expressions: Refer to objects by their visual appearance rather than directly as their\nentity name. For example, an \u201capple\u201d is referred to by its visual appearance as a \u201cround red fruit\u201d.\nDuring training, objects are only referred to by a single name. The referring expression was never\nseen during training.\n\u2022 Spatial Relationships: This refers to receptacles indirectly by their location relative to other\nreceptacles. All receptacles are positioned against the walls, so there is no spatial ambiguity\ndepending on the agent viewpoint. There are no spatial concepts in the training data.\n\u2022 Context: Describe a situation where a particular object fits the context.\n\u2022 Irrelevant Instruction Text: Instructions that include irrelevant context.\nFurther details on the Behavior Generalization datasets:\n\u2022 Multiple Rearrangements: Generalize to rearranging 3 objects when the agent only rearranges\n2 objects during training.\n\u2022 Novel Objects: New entity, instruction pairs. We holdout particular objects from single object\npick and place instructions during training. This split evaluates on the single object pick and place\ninstructions with this holdout object.\n\u2022 Multiple Objects : Different rearrangement concepts. The agent has never seen the concept of\n\u201cfor all\u201d in the training dataset. This requires the agent to rearrange a variable number of objects\ndepending on how many entities are in the scene. The agent may need to rearrange 1 to 2 objects in\n19\n0\n1\n2\n3\n4\n5\nEnvironment Steps\n1e8\n20\n40\n60\n80\n100\nSuccess Rate (%)\nLLaRP\nLLaRP-Scratch\nLSTM-Flan\nLSTM-LLaMA\n(a) Main task.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nEnvironment Steps\n1e7\n20\n40\n60\n80\n100\nSuccess Rate (%)\nLLaRP-13b\nLLaRP-7b\nLLaRP-7b-FT\n(b) Scaling Learning Curves.\nFigure 9: Left: Learning curves for all baselines in the main Language Rearrangement task from\nFigure 3. Note that we train all methods for 200M steps except for LLaRP-Scratch, as we observed\nthat it had not converged and was still making learning progress at that point. Right: Learning curves\nfor the scaling results from Figure 5.\nthe scene since the number of each object is randomized during scene generation. Therefore, the\nagent must search to detect that all the objects belonging to the specified type have been moved.\n\u2022 Conditional Instructions : Adjust behavior based on if the conditional statement is true. In\nLanguage Rearrangement we consider changing the behavior based on if the fridge is open or\nclosed. We split this dataset into the fridge being open half of the time. The agent has to find and\nmove one of two objects depending on if the fridge is closed or not. For the object that doesn\u2019t\nneed to be moved, the agent must also not displace this object.\nB.6\nDATASET ANALYSIS\nIn this section, we study how the evaluation datasets differ from the training dataset and each other\nin more detail. We note the high-level differences below:\n\u2022 Each dataset consists of different instruction templates (instruction templates are described in\nAppendix B.2). Different instruction templates mean the underlying goal conditions and language\ninstructions will be different.\n\u2022 All non-train datasets are on unseen scenes. These unseen scenes are from the evaluation split from\nReplicaCAD (Szot et al., 2021). The location of the kitchen and furniture differs from training.\n\u2022 Objects are placed in new locations. In every episode, objects start in a new position. Thus we\nalways evaluate on unseen object positions, even for the train split.\nWe qualitatively analyze the differences between datasets by visualizing language embeddings of\nthe instructions between the different instructions in Figure 8a. Specifically, we take 100 random\ninstructions from each dataset and embed them using the Flan-T5-XL model (Chung et al., 2022).\nThese embeddings are then visualized as a T-SNE plot. The training instructions, visualized as red\npoints, only have minimal overlap with the evaluation instructions.\nAdditionally, we display the average number of tokens per instruction (see Figure 8b). From here\nwe can see that our evaluation splits have variability at the token-count level, which in the largest\ncase (for conditional instruction datasets) have approximately double the length of prompts than the\nmedian task.\nDiving into this analysis further, in Figure 14, we display the per-word frequencies in the instruc-\ntions. As expected, we see a long-tailed distribution across the board, favoring receptacles and\nrelative indicators for nouns (e.g. counter, table, left) and instructions for verbs (e.g. move, place,\nand swap). It also is no surprise that when looking at all words in the combined plot, articles are the\nmost common. Overall, this shows the diverse language across our instructions, and aids in verifying\nthat our instruction generation is acting as expected.\n20\nAggregated\nPer Dataset Breakdown\nTotal\nBehavior\nParaphrastic\nTrain\nScene\nInstruct\nNovel\nMultiple\nReferring\nContext\nIrrelevant\nMultiple\nSpatial\nConditional\nGeneralization\nRobustness\nRephrasing\nObjects\nRearrange\nExpressions\nText\nObjects\nInstructs\nLLaRP\n42 \u00b1 2\n45 \u00b1 3\n38 \u00b1 1\n99 \u00b1 1\n96 \u00b1 4\n92 \u00b1 2\n95 \u00b1 4\n47 \u00b1 5\n26 \u00b1 2\n34 \u00b1 2\n32 \u00b1 2\n0 \u00b1 1\n8 \u00b1 1\n39 \u00b1 3\nLLaRP-Scratch\n17 \u00b1 4\n18 \u00b1 5\n16 \u00b1 3\n90 \u00b1 9\n90 \u00b1 9\n59 \u00b1 13\n58 \u00b1 16\n15 \u00b1 6\n3 \u00b1 1\n4 \u00b1 3\n13 \u00b1 4\n0 \u00b1 0\n3 \u00b1 3\n1 \u00b1 1\nLSTM-Flan\n25 \u00b1 1\n28 \u00b1 1\n23 \u00b1 1\n98 \u00b1 1\n95 \u00b1 8\n85 \u00b1 2\n83 \u00b1 3\n19 \u00b1 4\n6 \u00b1 1\n15 \u00b1 3\n5 \u00b1 2\n0 \u00b1 0\n4 \u00b1 4\n10 \u00b1 6\nLSTM-LLaMA\n2 \u00b1 1\n0 \u00b1 0\n3 \u00b1 2\n31 \u00b1 2\n15 \u00b1 2\n12 \u00b1 3\n1 \u00b1 1\n0 \u00b1 1\n1 \u00b1 2\n0 \u00b1 1\n0 \u00b1 1\n0 \u00b1 0\n2 \u00b1 4\n0 \u00b1 0\nZS-ChatGPT\n22\n23\n21\n57\n52\n58\n61\n24\n24\n10\n11\n2\n0\n5\nZS-LLaMA\n12\n14\n10\n54\n41\n34\n50\n6\n3\n5\n6\n0\n0\n0\nZS-Flamingo\n6\n8\n4\n24\n14\n18\n24\n8\n0\n0\n2\n2\n0\n0\nLLaRP-FT\n1\n0\n1\n30\n13\n5\n0\n0\n0\n0\n0\n0\n0\n0\nLLaRP-7b\n42 \u00b1 2\n45 \u00b1 3\n38 \u00b1 1\n99 \u00b1 1\n96 \u00b1 4\n92 \u00b1 2\n95 \u00b1 4\n47 \u00b1 5\n26 \u00b1 2\n34 \u00b1 2\n32 \u00b1 2\n0 \u00b1 1\n8 \u00b1 1\n39 \u00b1 3\nLLaRP-13b\n46\n48\n44\n98\n100\n95\n98\n51\n31\n41\n37\n0\n15\n45\nLLaRP (HL)\n42 \u00b1 2\n45 \u00b1 3\n38 \u00b1 1\n99 \u00b1 1\n96 \u00b1 4\n92 \u00b1 2\n95 \u00b1 4\n47 \u00b1 5\n26 \u00b1 2\n34 \u00b1 2\n32 \u00b1 2\n0 \u00b1 1\n8 \u00b1 1\n39 \u00b1 3\nLSTM-Flan (HL)\n25 \u00b1 1\n28 \u00b1 1\n23 \u00b1 1\n98 \u00b1 1\n95 \u00b1 8\n85 \u00b1 2\n83 \u00b1 3\n19 \u00b1 4\n6 \u00b1 1\n15 \u00b1 3\n5 \u00b1 2\n0 \u00b1 0\n4 \u00b1 4\n10 \u00b1 6\nLLaRP (Harder)\n28\n27\n28\n56\n61\n62\n56\n31\n23\n32\n24\n0\n1\n20\nLSTM-Flan (Harder)\n12\n14\n11\n57\n52\n50\n43\n11\n3\n0\n2\n0\n0\n0\nTable 4: Zero-shot results on Language Rearrangement for all baselines and settings. This includes\nthe numbers from the bar plots in Figure 3, Figure 5 and Figure 12a. All numbers except for non-RL\nmethods, LLaRP-FT, LLaRP-13b, and the harder task setting are mean and standard deviation over\n3 random seeds.\nC\nMETHOD DETAILS\nIn this section, we describe the architectures, training procedure of LLaRP, and baselines in more\ndetail. Unless specified otherwise, every method is trained using a full node of 8 A100-80GB GPUs\nand 96 Intel(R) Xeon(R) CPUs @ 2.20GHz. The base models are able to fit on a single GPU and\nwe use data parallelism. Each GPU runs 1 policy and a set of N environment workers. During the\nPPO rollout phase, the policy acts in parallel in all N environments. Each GPU then computes the\nPPO update and synchronizes gradients. We use DD-PPO (Wijmans et al., 2019) to handle straggler\nenvironment workers and speed up synchronization between GPUs.\nHyperparameters for all reinforcement learning based methods are summarized in Table 3. Next, we\ndetail specific method architecture choices.\nC.1\nLLARP DETAILS\nIn general, the visual encoder can produce M embeddings per observation ot, which consists of a\nhigh-dimensional visual component (the robot\u2019s RGB camera) and a low-dimensional state com-\nponent (the robot joint angles). The visual component is projected to a set of M \u2212 1 tokens. For\nexample, using a ViT (Dosovitskiy et al., 2020) for EV\n\u03d5 produces an embedding per image patch\nwhich are projected to M \u22121 embeddings using a Perceiver Resampler Jaegle et al. (2021) network.\nThe state components are concatenated and projected with an MLP to produce another embedding,\ngiving M total embeddings. Since there are now M tokens per observation ot, to produce a distri-\nbution over actions, we skip the first k tokens corresponding to instruction tokens, and sub-sample\nevery M th hidden output to extract just one action per time step.\nHowever, in this work we just use the visual RGB observation and set M = 1. All methods use the\nfrozen VC1 visual encoder whose weights are represented in bfloat16. We then take the [CLS] token\nof the VC1 encoder for the image observation. We then input this embedding to a linear projection\nlayer that produces an embedding the same dimension as the LLM text tokens. For the action output\nmodule, we use a 2-layer MLP with ReLU activations, LayerNorm, and a hidden dimension size of\n512.\nBy default, we use the base LLaMA-7B V1 (Touvron et al., 2023) for the LLM in LLaRP. We con-\nvert the LLaMA weights to bfloat16. The observation encoder and action output modules represent\ntheir weights in float32. The context window in Language Rearrangement is the maximum episode\nhorizon of 32 steps. We compute the attention mask during training so the transformer only attends\nto inputs from the current episode. Despite running such a large policy with RL, we find that total\ntraining throughput is 700-800 steps-per-second on a full compute node of 8 GPUs. This timing\nincludes policy inference for data collection, policy updates, and environment stepping including\nrendering and physics. The biggest bottleneck during training is policy inference.\n21\nC.2\nLLARP-SCRATCH DETAILS\nFor the transformer network, we use the same architecture and details as LLaMA. We restrict the\ntransformer network size to around 2 billion parameters. As with LLaRP, we represent the trans-\nformer decoder network with bfloat16 data type. We update all parameters. Since the policy is\nsmaller than regular LLaRP, the training throughput is even faster at 800-900 steps-per-second on a\nfull compute node, despite updating all 2 billion policy parameters.\nC.3\nLSTM-FLAN/LLAMA DETAILS\nFor LSTM-Flan, we only use the Flan-T5-XL encoder, and remove the decoder. The encoder is used\nto summarize the instruction. Specifically, we take the hidden activation of the final token as the\ninstruction representation. We fine tune the weights of the Flan encoder which we show is necessary\nfor good training performance in Appendix D.2. We represent the Flan weights in bfloat16. We\nrepresent the image as the [CLS] token from VC1 and process this embedding with a linear layer\nbefore concatenating it with the language representation and inputting it to the LSTM.\nFor LSTM-LLaMA, we use LLaMA-7B. The LLaMA weights are frozen and in bfloat16 format.\nFor the instruction, we summarize the hidden outputs in a single instruction representation using a\nPerciever network.\nC.4\nZERO-SHOT BASELINE DETAILS\nZS-LLaMA uses a LLaMA-65B V1 (Touvron et al., 2023) model that was instruction tuned on\nORCA style data (Mukherjee et al., 2023; Lian et al., 2023). Since, LLaMA-65B is a language-only\nmodel, this baseline is blind \u2013 it plans actions based only on the language instruction. To help the\nbaseline reason about the available objects and actions, the prompt lists all receptacles and objects\nalong with examples of successful behavior. For our multimodal baseline ZS-Flamingo, we use an\nopen-source vision-and-language model IDEFICS (Laurenc\u00b8on et al., 2023) which is a reproduction\nof the closed-source Flamingo (Alayrac et al., 2022) model. IDEFICS uses LLaMA-65B V1 as\nits language model backbone. Its vision encoder is a vision transformer (ViT-H/14) trained using\nOpenCLIP (Radford et al., 2021) on the LAION-2B English subset of LAION-5B (Schuhmann\net al., 2022) dataset. For both ZS-LLaMA and ZS-Flamingo, we use the following prompt:\nYou are a home robot assistant that can take actions in the house.\nRemember the following guidelines:\n1. Your possible actions are: pick(object), place_on_recep(receptacle),\nnavigate(receptacle), open_fridge(), close_fridge(),\nopen_cabinet(cabinet), STOP.\n2. Possible objects are: ball, clamp, hammer, screwdriver, padlock,\nscissors, block, drill, spatula, knife, spoon, plate, sponse,\ncleanser, plum, pear, peach, apple, lemon, can, box, banana,\nstrawberry, lego, rubrik\u2019s cube, book, bowl, cup, mug, orange, lid,\ntoy airplane, wrench.\n3. You can only pick one object at a time.\n4. If you place an object you must have previously picked it.\n5. You must always place the object that you have picked.\n6. To open a fridge, you have to navigate to the fridge.\n7. To pick an object from the cabinet, you need to open it first.\n8. place_on_recep() is not valid for cabinets like \u2019cabinet drawer 7\u2019 or\n\u2019cabinet drawer 6\u2019.\n9. There are no more than 5 objects.\n10. When exploring, select randomly from possible receptacles: [cabinet\ndrawer 7, cabinet drawer 6, fridge, chair, black table, brown table,\nTV stand, sink, right counter, left counter]\n11. When you are done output STOP.\n12. If the instruction doesn\u2019t specify where the object is located, you\nshould explore by navigating to a previously unvisited receptacle.\n13. If the instruction asks to pick up more than one object, you should\nattempt multiple pick and place. Look for each object by visiting\nall the receptacles to find the objects mentioned in the instruction.\n22\n14. Don\u2019t get stuck in a loop by picking from and placing receptacle on\nthe same receptacle.\nTo help you understand, here\u2019s are twos example:\n# User: Instruction: Move the screwdriver from the left counter to the\nsofa.\n# Assistant: navigate(left counter), pick(screwdriver), navigate(sofa),\nplace_on_recep(sofa), STOP.\n# User: Instruction: Find an apple and put it on the brown table.\n# Assistant: navigate(fridge), open_fridge(), pick(apple),\nnavigate(brown table), place_on_recep(brown table), STOP.\nThe ZS-ChatGPT baseline can perform multi-step reasoning to generate a plan. Unlike ZS-LLaMA,\nit is not limited to generating the whole plan in a single step. Instead, it can continuously refine the\nplan based on environment feedback. We use GPT-3.5-Turbo which is trained for chat applications\nfor these experiments. The environment provides feedback in natural language consisting of agent\u2019s\nlocation (e.g. You are now at black table), failed action (e.g. Couldn\u2019t execute pick(\u201dapple\u201d). Object\nwasn\u2019t found), and asks for the new plan. We update the prompt to also contain examples that require\nmultiple steps of reasoning. The prompt is as follows:\n#System: You are a home robot assistant that can take actions in the\nhouse. Remember the following guidelines:\n1. Your possible actions are: pick(object), place_on_recep(receptacle),\nnavigate(receptacle), open_fridge(), close_fridge(),\nopen_cabinet(cabinet), STOP.\n2. Possible objects are: ball, clamp, hammer, screwdriver, padlock,\nscissors, block, drill, spatula, knife, spoon, plate, sponse,\ncleanser, plum, pear, peach, apple, lemon, can, box, banana,\nstrawberry, lego, rubrik\u2019s cube, book, bowl, cup, mug, orange, lid,\ntoy airplane, wrench.\n3. You can only pick one object at a time.\n4. If you place an object you must have previously picked it.\n5. You must always place the object that you have picked.\n6. To open a fridge, you have to navigate to the fridge.\n7. To pick an object from the cabinet, you need to open it first.\n8. place_on_recep() is not valid for cabinets like \u2019cabinet drawer 7\u2019 or\n\u2019cabinet drawer 6\u2019.\n9. There are no more than 5 objects.\n10. When exploring, select randomly from possible receptacles: [cabinet\ndrawer 7, cabinet drawer 6, fridge, chair, black table, brown table,\nTV stand, sink, right counter, left counter]\n11. When you are done output STOP.\n12. If the instruction doesn\u2019t specify where the object is located, you\nshould explore by navigating to a previously unvisited receptacle.\n13. If the instruction asks to pick up more than one object, you should\nattempt multiple pick and place. Look for each object by visiting\nall the receptacles to find the objects mentioned in the instruction.\n14. Don\u2019t get stuck in a loop by picking from and placing receptacle on\nthe same receptacle.\nTo help you understand, here\u2019s are twos example:\n#User: Move the screwdriver from the left counter to the sofa.\n#Assistant:\n\u2018\u2018\u2018\nplan = [\nnavigate(\"left counter\"),\npick(\"screwdriver\"),\nnavigate(\"sofa\"),\nplace_on_recep(\"sofa\"),\nSTOP\n23\n]\n\u2018\u2018\u2018\n#User: Find an apple and a banana and put it on the left counter\n#Assistant:\n\u2018\u2018\u2018\nplan = [\nnavigate(\"cabinet drawer 7\"), # exploring an unvisited receptacle\nopen_cabinet(\"cabinet drawer 7\"), # opening the cabinet drawer 6 for\nthe first time.\npick(\"apple\"),\nnavigate(\"left counter\"),\nplace_on_recep(\"left counter\"),\nSTOP\n]\n\u2018\u2018\u2018\n#User: You are now at cabinet drawer. Couldn\u2019t execute pick(\"apple\").\nObject wasn\u2019t found. Randomly select an unexplored receptacle in\n[cabinet drawer 7, cabinet drawer 6, fridge, chair, black table,\nbrown table, TV stand, sink, right counter, left counter]\n#Assistant:\n\u2018\u2018\u2018\nplan = [\nnavigate(\"fridge\"), # exploring an unvisited receptacle\nopen_fridge(),\npick(\"apple\"),\nnavigate(\"left counter\"),\nplace_on_recep(\"left counter\"),\nSTOP\n]\n\u2018\u2018\u2018\n#User: You are now at fridge. Couldn\u2019t execute pick(\"apple\"). Object\nwasn\u2019t found. Randomly select an unexplored receptacle in [cabinet\ndrawer 7, cabinet drawer 6, fridge, chair, black table, brown table,\nTV stand, sink, right counter, left counter]\n#Assistant:\n\u2018\u2018\u2018\nplan = [\nnavigate(\"brown table\"), # exploring an unvisited receptacle\npick(\"apple\"),\nnavigate(\"left counter\"),\nplace_on_recep(\"left counter\"),\nSTOP\n]\n\u2018\u2018\u2018\n#User: Found an apple.\n#Assistant:\n\u2018\u2018\u2018\nplan = [\nnavigate(\"cabinet drawer 7\"),\nopen_cabinet(\"cabinet drawer 7\")\npick(\"banana\"),\nnavigate(\"left counter\"),\nplace_on_recep(\"left counter\"),\nSTOP\n]\n\u2018\u2018\u2018\n#User: You are now at cabinet drawer 7. Couldn\u2019t execute pick(\"banana\").\nObject wasn\u2019t found. Randomly select an unexplored receptacle in\n[cabinet drawer 7, cabinet drawer 6, fridge, chair, black table,\nbrown table, TV stand, sink, right counter, left counter]\n#Assistant:\n\u2018\u2018\u2018\nplan = [\nnavigate(\"chair\"),\n24\npick(\"banana\"),\nnavigate(\"left counter\"),\nplace_on_recep(\"left counter\"),\nSTOP\n]\n\u2018\u2018\u2018\n#User: Found the banana. Thanks!\n\"\"\",\nC.5\nMODIFICATIONS TO SCALE AND TRAIN LLAMA\nFor the LLaMA models larger than 7B parameters or with trainable parameters (unfrozen), training\nno longer fits on a single GPU. To train these LLaRP variants, we use model parallelism. Specifi-\ncally, we distribute the model weights between 4 GPUs and scale training to 4 nodes (8 GPUs each)\nto match the batch size.\nC.6\nMODIFICATIONS FOR ATARI\nWhen training LLaRP on Atari games, we found that it was vital to allow the visual encoder to be\nfully trainable. Initial experiments with a frozen VC-1 visual encoder demonstrated some limited\nlearning, but were unable to reliably solve Pong within 100M steps, our threshold for success. Even\nwith this change, we observed frequent instabilities in our early experiments. Generally speaking,\ninstabilities propagated from value learning to the actor over the course of 1-2 gradient steps. We\nwere able to fix this by using a Huber loss for value learning, as well as by applying gradient norm\nclipping (with max gradient norm 0.5). We also apply reward clipping (Mnih et al., 2013) to stabilize\nour predicted values.\nWe also found that training at relatively large batch sizes \u2013 compared to language rearrangement \u2013\nwas beneficial. By default we used a per-device batch size of 64, with context length 32, leading to\na total of 16,384 states being seen at once when training across 8 GPUs.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nEnvironment Steps\n1e7\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate (%)\nTransformer 8x Batch\nTransformer 2x Batch\nTransformer 1x Batch\nLSTM 8x Batch\nLSTM 2x Batch\nLSTM 1x Batch\n(a) Batch size analysis.\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nEnvironment Steps\n1e8\n20\n40\n60\n80\n100\nSuccess Rate (%)\nFine-Tune Flan\nFreeze Flan\n(b) Encoder freezing.\nFigure 10: Left: Success rate of policies on the Train evaluation dataset. Right: Impact of freezing\nvs unfreezing the FLAN encoder in LSTM-Flan.\nD\nFURTHER RESULTS\nIn this section, we show further results and analysis on LLaRP and baselines.\nD.1\nTRANSFORMER POLICY EXPERIMENTS\nA primary challenge of LLaRP was implementing PPO training for transformer-based policies. By\na \u201ctransformer-based policy\u201d, we mean a policy architecture that uses a transformer to attend to\nprevious observations. Transformers in online RL algorithms, such as PPO, are rare. RNN-based\n25\npolicies are more typically used for problems where the history is important Ni et al. (2021). In\nthis section, we include empirical analysis into important components for stable transformer-based\npolicy training.\nPrior work has demonstrated transformers working for offline RL (Xu et al., 2022; Chen et al.,\n2021a), but such works rely on supervised losses from static datasets. Other works explore using\nthe transformer-based policy to collect data (Zheng et al., 2022) but use a similar offline-RL training\nprocess. Parisotto et al. (2020) use transformers for online-RL, but shows that additional trans-\nformer architecture changes are necessary for stabilizing RL training. We note that these changes\nare incompatible with LLM architectures, precluding the use of LLMs in their framework. We are\nable to train transformer-based policies with PPO, despite not using any of the modifications from\nParisotto et al. (2020). Other works combine various transformer architectures with long-horizon\nRL tasks (Ni et al., 2023; Morad et al., 2023; Esslinger et al., 2022; Sopov & Makarov, 2021;\nPleines et al., 2022). Unlike these works, we use billion parameter transformer models in tasks with\nhigh-dimensional visual observations.\nEffect of Batch Size on Stability: In Figure 10a, we illustrate the importance of a large batch size\nfor transformer-based policy training. We compare learning curves of a LSTM and transformer-\nbased policy on 100 training episodes from the overall training dataset. Both policies are fixed to\nbe 40M parameters and neither policy has any pre-trained LLM. The policy only takes as input\nthe RGB visual observations and a learned embedding of the current instruction. We only analyze\ntraining performance, thus this learned embedding is sufficient for distinguishing the instructions\nduring training. As seen from Figure 10a, the transformer runs are more unstable at lower batch\nsizes, with more jagged learning curves indicating drops in performance. The RNN-based policy\nat the same batch size does not suffer from this issue and has smooth learning for every batch size\nsetting. Note that smaller batch sizes converge faster because they update the policy more for a fixed\nnumber of environment interactions. The transformer-based policy is also less sample efficient than\nthe RNN-based policy for every batch size setting. We note this efficiency finding is reversed when\ncomparing LLaRP to RNN-based approaches in Language Rearrangement.\nEffect of Pretrained LLM Weights: We found that using pre-trained and frozen LLM weights are\nimportant for stable and fast convergence. In Figure 9a, training LLaRP-Scratch required 500M\nsamples to converge and exhibited unstable training demonstrated by the dips in training perfor-\nmance. LLaRP, using the same architecture, but frozen LLM weights, learned in under 100M steps\nand did not have the same dips in performance during training.\nD.2\nADDITIONAL LANGUAGE REARRANGEMENT EXPERIMENTS AND ANALYSES\nFull Generalization Numbers: In Table 4 we show the numerical results for all numbers from the\npaper. These include the main zero-shot generalization results from Figure 3, the scaling results\nfrom Figure 5 and the task setting results from Figure 12a.\nTrain\nAgg\nEval\nPara\nRobust\nBehav\nGen\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEfficiency\nLLaRP\nLLaRP-Scratch\nLSTM-Flan\nLSTM-LLaMA\nZS-ChatGPT\nZS-LLaMA\nZS-Flamingo\nFigure 11: Language Rearrangement efficiency. Numbers are for 1 random seed. Higher numbers\nindicate more efficient solutions. Numbers are rescaled in [0, 1] where 0 is the least efficient and 1\nis the most efficient.\nEfficiency Results: In Figure 11, we compare the efficiency of all methods in the Language Rear-\nrangement zero-shot evaluation. We measure efficiency by the number of steps agents take to solve\nthe tasks. Specifically we compute the efficiency of episode i as 1\u2212\nni\nmax steps where ni is the number\n26\nAggregated\nPer Dataset Breakdown\nTotal\nBehavior\nParaphrastic\nTrain\nNew Scenes\nInstruction\nNovel\nMultiple\nReferring\nContext\nIrrelevant\nMultiple\nSpatial\nConditional\nGeneralization\nRobustness\nRephrasing\nObjects\nRearrange\nExpressions\nText\nObjects\nInstructs\nLLaRP\n0.32\n0.35\n0.30\n0.82\n0.79\n0.73\n0.82\n0.32\n0.21\n0.28\n0.23\n0.01\n0.05\n0.27\nLSTM-Flan\n0.20\n0.21\n0.19\n0.82\n0.79\n0.76\n0.69\n0.11\n0.06\n0.09\n0.04\n0.00\n0.00\n0.06\nLLaRP-Scratch\n0.17\n0.19\n0.16\n0.84\n0.79\n0.58\n0.60\n0.15\n0.01\n0.01\n0.17\n0.00\n0.02\n0.01\nZS-ChatGPT\n0.19\n0.21\n0.18\n0.52\n0.45\n0.50\n0.53\n0.24\n0.20\n0.08\n0.09\n0.01\n0.00\n0.04\nZS-LLaMA\n0.12\n0.12\n0.12\n0.50\n0.36\n0.28\n0.44\n0.05\n0.03\n0.22\n0.05\n0.00\n0.00\n0.00\nZS-Flamingo\n0.05\n0.07\n0.03\n0.22\n0.13\n0.16\n0.21\n0.07\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nLSTM-LLaMA\n0.01\n0.00\n0.01\n0.29\n0.12\n0.07\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nTable 5: Numerical results for Language Rearrangement efficiency from Figure 11. Numbers are\nfor 1 random seed.\nTrain\nAgg\nEval\nPara\nRobust\nBehav\nGen\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\nLLaRP (HL)\nLSTM-Flan (HL)\nLLaRP (Harder)\nLSTM-Flan (Harder)\n(a) Generalization on Harder setting.\n0\n1\n2\n3\n4\n5\n6\n7\nEnvironment Steps\n1e8\n0\n10\n20\n30\n40\n50\n60\n70\n80\nSuccess Rate (%)\nLLaRP\nLSTM-Flan\n(b) Training on Harder Setting.\nFigure 12: Left: The zero-shot evaluation performance on Language Rearrangement where invalid\nactions result in immediate failure and the stop action is required. Right: Training curves under this\nharder setting. We enforce these termination conditions during training as well, resulting in slower\ntraining. Results in the harder task are just for 1 seed since training is slower.\nof steps needed to complete episode i. ni is assigned max steps if the episode was unsuccessful. We\nreport the average efficiency over all episodes in the evaluation datasets. LLaRP is more efficient\nthan baselines, and is over 1.5x as efficient as the best performing baseline.\nImportance of Unfreezing LLM Weights in LSTM-Flan: In Figure 10b we analyze the effect\nof freezing or unfreezing the Flan-T5 encoder weights during training. Unlike for LLaRP, keeping\nthe LLM weights frozen results in the policy only learning the easy instructions even after 200M\nsteps of training. We found fine-tuning the language encoder to be effective at learning the harder\ninstructions and converging much faster to the maximum training performance.\nLLaRP Full Finetuning (LLaRP-FT): In Table 4, we compare the effect of not freezing the LLM\nin LLaRP and fine tuning it during training (LLaRP-FT). Like with training LLaRP-13b, we use\nmodel parallelism and scale training to 4 nodes (8 GPUs each). However, as seen in Figure 9b,\ntraining performance is poor. We were only able to train for 15M steps due to the slow training\nspeeds. A larger batch size and longer training could result in LLaRP-FT working better.\nD.3\nHARDER TASK VARIANT\nWe consider a harder variant of Language Rearrangement where invalid actions immediately end the\nepisode and the agent must call a termination action at the end of the episode. Calling the termination\naction before the task is successfully completed results in a failed episode. We train LLaRP and the\nbest performing baseline, LSTM-Flan, in this setting. We train on the same training datasets and\nevaluate on the same holdout datasets as in the main Language Rearrangement experiments.\nWe find training policies to be more difficult in this setting. As seen from the learning curves in\nFigure 12b, even after 700M steps of training, policies are not yet converged and still cannot solve\nthe hardest instructions. In Figure 12a, we analyze the zero-shot performance on the Language\nRearrangement evaluation datasets. Both LLaRP and LSTM-Flan suffer a drop in performance on\nthis harder task. LLaRP still greatly outperforms LSTM-Flan in this harder setting.\n27\nE\nQUALITATIVE RESULTS\nIn Figure 13, we visualize success examples for LLaRP. See the figure caption for a breakdown of\neach of the success examples.\n28\n1\nnavigate(sofa)\npick(box)\n3\nnavigate(sink)\nplace(box, sink)\nMove a box from the sofa to the \nright of the left counter\n1\n2\n2\n3\n4\n4\n2\n4\n(a) Spatial Relationships success example.\n1\nnavigate(sink)\npick(wrench)\n3\nnavigate(brown_table)\nplace(wrench, brown_table)\npick(peach)\n6\nnavigate(sink)\nplace(peach, sink)\n5\n7\nSwap the wrench and peach \nfrom the sink and brown table\n2\n4\n1\n3\n2\n4\n5\n6\n7\n5\n2\n(b) Novel Objects success example.\n1\nnavigate(brown_table)\n2\nnavigate(dark_table)\n3\nnavigate(blue_table)\npick(screwdriver)\nnavigate(sofa)\nplace(screwdriver, sofa)\n4\nLocate a tool to \ufb01x a loose screw \nin the sofa and bring it there.\n5\n6\n1\n2\n3\n4\n5\n6\n4\n6\n(c) Context success example.\n1\nnavigate(sink)\n2\nnavigate(sofa)\npick(apple)\n4\nnavigate(blue_table)\nplace(apple, blue_table)\n6\nnavigate(sink)\n5\nDeposit all the apples on the blue \ntable\n7\nnavigate(right_counter)\npick(apple)\n9\nnavigate(blue_table)\nplace(apple, blue_table)\n3\n8\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n3\n8\n10\n(d) Multiple Rearrangements success example.\n1\nnavigate(sink)\n2\nnavigate(dark_table)\n3\nnavigate(brown_table)\n4\nnavigate(right_counter)\npick(banana)\n6\nnavigate(blue_table)\n5\nTake a yellow curved fruit and \nplace it on the blue table\nplace(banana, blue_table)\n7\n1\n2\n3\n4\n5\n6\n7\n5\n7\n(e) Referring Expressions success example.\n1\nnavigate(sofa)\n2\nnavigate(dark_table)\n3\nnavigate(blue_table)\npick(pear)\nnavigate(sink)\nplace(pear, sink)\nIf the fridge is open, move a \nspoon to the sink, otherwise \nmove a pear to the sink \n4\n6\n5\n4\n6\n1\n2\n3\n4\n5\n6\n(f) Conditional Instructions success example.\nFigure 13: Successful LLaRP trajectories in some of the Language Rearrangement evaluation\ndatasets. Fig. 13a: the agent infers the sink is the receptacle to the right of the left counter. Fig. 13b:\nthe agent has to interact with the wrench not seen before during training. It correctly picks the\nwrench despite not seeing this object in the context of this instruction during training. Fig. 13c: the\nagent infers a screwdriver is needed to satisfy the context of \u201ca loose screw\u201d. It explores until it finds\nthe screwdriver and then brings it to the couch. Fig. 13d: The agent explores until it finds an apple\nat which point it places it on the receptacle. The agent continues to explore to find if there are other\napples, finds one on the right counter, and then rearranges it to the blue table. Fig. 13e: The agent\nexplores until it sees a banana which it infers is a \u201cyellow curved fruit\u201d. The agent then moves the\nbanana to the blue table. Fig. 13f: The fridge is closed, so the agent is correct to move the pear to\nthe sink. However, the agent didn\u2019t explicitly check to see if the fridge is open. This likely prevents\nLLaRP from achieving higher success on the Conditional Instructions dataset.\n29\nTrain\nMove\na\n\u2018target object name\u2018\nfrom\nthe\n\u2018source receptacle name\u2018\nto\nthe\n\u2018tar-\nget receptacle name\u2018\nMove a \u2018target object name\u2018 to the \u2018target receptacle name\u2018\nMove the \u2018target object name\u2018 off the \u2018target receptacle name\u2018.\nPlace a \u2018object1\u2018 and a \u2018object2\u2018 on the \u2018targ recep\u2018.\nCan you swap the \u2018object1\u2018 and the \u2018object2\u2018 in the \u2018receptacle1\u2018 and \u2018receptacle2\u2018?\nI accidently left the fridge open, can you close it?\nCan you open the fridge for me?\nGo to \u2018target receptacle name\u2018.\nFind a \u2018target object name\u2018.\nInstruction\nRephrasing\nOn the \u2018source receptacle name\u2018 there is a \u2018target object name\u2018, move it to the \u2018tar-\nget receptacle name\u2018\nOn the \u2018target receptacle name\u2018 I need you to put a \u2018target object name\u2018\nSet out a \u2018plate\u2018 for one person on the \u2018target receptacle name\u2018.\nThe \u2018target receptacle name\u2018 should be devoid of any \u2018target object name\u2018.\nOn the \u2018targ recep\u2018, I need a \u2018object1\u2018 and a \u2018object2\u2018.\nI misplaced the \u2018object1\u2018 on the \u2018receptacle1\u2018 and the \u2018object2\u2018 on the \u2018receptacle2\u2018. Can\nyou swap their positions?\nWhen putting away groceries, I forgot to shut the fridge. Can you help?\nReferring\nExpressions\nBring a green fruit to the \u2018target receptacle name\u2018\nBring a yellow round fruit to the \u2018target receptacle name\u2018.\nBring a yellow curved fruit to the \u2018target receptacle name\u2018\nBring a round red fruit to the \u2018target receptacle name\u2018\nMultiple\nObjects\nPut all the \u2018object name\u2018 on the \u2018targ recep\u2018.\nPut all the \u2018object name\u2018 from the \u2018receptacle1\u2018 on the \u2018receptacle2\u2018.\nConditional\nInstructions\nIf the fridge is open move a \u2018target object name1\u2018 to the \u2018target receptacle name\u2018, other-\nwise move a \u2018target object name2\u2018 to the \u2018target receptacle name\u2018.\nIf the fridge is open move a \u2018target object name1\u2018 to the \u2018target receptacle name\u2018, other-\nwise move a \u2018target object name2\u2018 to the \u2018target receptacle name\u2018.\nSpatial\nRelationships\nMove a \u2018target object name\u2018 from the \u2018source receptacle name\u2018 to the left of the right\ncounter.\nMove a \u2018target object name\u2018 from the \u2018source receptacle name\u2018 to the right of the left\ncounter.\nMove a \u2018target object name\u2018 from the \u2018source receptacle name\u2018 to the right of the TV\nstand.\nContext\nI want to play a sport, bring something to play with to the \u2018target receptacle name\u2018.\nA screw is loose in the \u2018target receptacle name\u2018, bring something to fix it.\nI need to cut a piece of paper at the \u2018target receptacle name\u2018, can you bring something to\nhelp?\nI spilt something and need to clean it.\nCan you bring something to the \u2018tar-\nget receptacle name\u2018 to help?\nBring me something to pour hot coffee into at the \u2018target receptacle name\u2018\nMultiple\nRearrangements\nMove the \u2018obj1\u2018 to the \u2018targ recep1\u2018, the \u2018obj2\u2018 to the \u2018targ recep2\u2018, and the \u2018obj3\u2018 to the\n\u2018targ recep3\u2018.\nIrrelevant\nInstruction Text\nThere\u2019s an apple on the sofa, but on the \u2018target receptacle name\u2018 I need you to put a\n\u2018target object name\u2018\nTable 6: Sampling of the instruction templates for each of the task datasets. Note that for each\ntemplate we include multiple phrasings. Names in backtics indicate template placeholders. Our\npipeline described in Appendix B.2 automatically grounds these placeholders with feasible entity\nnames.\n30\nCounter\nTable\nLeft\nRight\nBrown\nTv\nStand\nSink\nSofa\nI\nFridge\nPlease\nTransfer\nRefrigerator\nPosition\nRelocate\nSwitch\nDrill\nCleanser\nLego\nBox\nBook\nPeach\nSponge\nBowl\nApple\nLemon\nSpatula\nBall\nCube\nRubriks\nScrewdriver\nBanana\nPlate\nBlock\nKnife\nHammer\nSpoon\nScissors\nStrawberry\nClamp\nPlum\nPadlock\nPear\nCup\nShift\nPositions\nPlaces\nDeposit\nAirplane\nOrange\nWrench\nMistake\nMug\nLocation\nItems\nSpot\nWay\nDoor\nSurface\nLocations\nArrange\nFavor\nReposition\nSecure\nDisplace\nHead\nDestination\nGet\nJourney\nPinpoint\nMake\nScour\nSearch\nHunt\nApologies\nBehalf\nAssistance\nArea\nWalk\nFruit\nRound\nItem\nImplement\nScrew\nProblem\nSomething\nPaper\nProcure\nCoffee\nPouring\nSports\nActivity\nCase\nSure\n0\n5000\n10000\n15000\n20000\n25000\n30000\nCount\n(a) Noun frequency for all instructions.\nMove\nPlace\nSwap\nClose\nPick\nTake\nRetrieve\nRemove\nPerform\nSpecified\nDesignated\nOpening\nTransport\nPut\nLid\nBe\nToy\nExtract\nNeed\nAssist\nFind\nDo\nMind\nAppreciate\nIntended\nDeliver\nMoving\nIndicated\nClosing\nAssigned\nSet\nTrade\nLay\nInterchanging\nRearrange\nExchange\nCollect\nAcquire\nObtain\nGather\nHelp\nReach\nRequesting\n'm\nForgot\nDetach\nShut\nLift\nSeparate\nRealize\nDid\nWas\nMade\nGo\nSeek\nDiscover\nIdentify\nLocate\nExtend\nProceed\nBring\nFinding\nEnsure\nCurved\nLeave\nAddress\nCutting\nFacilitate\nTransferring\nClosed\nFound\nExecute\nStack\nAssuming\n0\n2000\n4000\n6000\n8000\n10000\nCount\n(b) Verb frequency for all instructions.\nThe\nAndA\nToIt\nCounter\nTable\nAn\nFrom\nLeft\nYou\nIn\nRight\nBrown\nTv\nStand\nBlack\nSink\nSofa\nOf\nOnI\nMove\nCan\nPlace\nOpen\nFridge\nPlease\nMe\nCould\nSwap\nBy\nTransfer\nInto\nOnto\nFor\nWould\nClose\nBetween\nRefrigerator\nPosition\nRelocate\nBoth\nPick\nSwitch\nTake\nRetrieve\nRemove\nDrill\nPerform\nCleanser\nLego\nBox\nBook\nPeach\nSponge\nBowl\nApple\nLemon\nSpatula\nBall\nRubriks\nCube\nScrewdriver\nBanana\nPlate\nBlock\nKnife\nHammer\nSpoon\nScissors\nStrawberry\nClamp\nPlum\nSpecified\nPadlock\nPear\nDesignated\nCup\nIts\nYour\nKindly\nOpening\nIs\nShift\nUp\nTransport\nPut\nPositionsIf\nPlaces\nLid\nDeposit\nBe\nAirplane\nToy\nOrange\nExtract\nWrench\nMistake\n0\n20000\n40000\n60000\n80000\n100000\n120000\nCount\n(c) Word frequency for all instructions\nFigure 14: Distribution of word counts for all the instructions.\n31\n"
  },
  {
    "title": "Personas as a Way to Model Truthfulness in Language Models",
    "link": "https://arxiv.org/pdf/2310.18168.pdf",
    "upvote": "5",
    "text": "PERSONAS AS A WAY TO MODEL TRUTHFULNESS IN\nLANGUAGE MODELS\nNitish Joshi1\u2217\nJavier Rando2\u2217\nAbulhair Saparov1\nNajoung Kim3\nHe He1\n1New York University\n2ETH Zurich\n3Boston University\n{nitish}@nyu.edu\n{javier.rando}@ai.ethz.ch\nABSTRACT\nLarge language models (LLMs) are trained on vast amounts of text from the\ninternet, which contains both factual and misleading information about the world.\nWhile unintuitive from a classic view of LMs, recent work has shown that the truth\nvalue of a statement can be elicited from the model\u2019s representations. This paper\npresents an explanation for why LMs appear to know the truth despite not being\ntrained with truth labels. We hypothesize that the pretraining data is generated\nby groups of (un)truthful agents whose outputs share common features, and they\nform a (un)truthful persona. By training on this data, LMs can infer and represent\nthe persona in its activation space. This allows the model to separate truth from\nfalsehoods and controls the truthfulness of its generation. We show evidence for\nthe persona hypothesis via two observations: (1) we can probe whether a model\u2019s\nanswer will be truthful before it is generated; (2) finetuning a model on a set\nof facts improves its truthfulness on unseen topics. Next, using arithmetics as a\nsynthetic environment, we show that structures of the pretraining data are crucial\nfor the model to infer the truthful persona. Overall, our findings suggest that\nmodels can exploit hierarchical structures in the data to learn abstract concepts like\ntruthfulness.\n1\nINTRODUCTION\nLarge language models (LLMs) are pretrained on increasing amounts of data from the internet (Brown\net al., 2020; Chowdhery et al., 2022)\u2014a noisy corpus which contains both factual and incorrect\nstatements about the world. For example, CDC claims that \"most studies suggest COVID vaccines are\nsafe\" (true), whereas InfoWars claims that \"DNA contaminants in COVID shots can trigger cancer\"\n(false). Such misconceptions and conspiracy theories pose a risk of misinformation as they can be\nregurgitated by models when interacting with users (Lin et al., 2021).\nIn this work, truthful text is defined as text consistent with facts that most domain experts agree upon.\nUntruthful text, distinct from blatant errors, refers to plausible but incorrect information that could\nmislead users. Importantly, we restrict our focus to untruthful text supported by the pretraining data,\nrather than hallucinations that are fabricated by models themselves and ungrounded.\nGiven a noisy training set, how does a LLM select its answers? Following the previous example,\nwhen asked about the safety of COVID vaccines, the classic view of LMs suggests that they are more\nlikely to generate the most frequent statement, regardless of whether it is true. However, recent work\nshows that the truth value of a statement can be elicited from its embedding (Burns et al., 2022; Li\net al., 2023), suggesting that LMs have an internal notion of truth. This divergence motivates our\nmain research question: how do LMs distinguish truth from falsehood in a noisy dataset?\nThis paper presents a possible explanation for why LLMs appear to \u201cknow\u201d what is true despite\nnot being trained on data with truth labels. Our hypothesis is based on the following generative\nprocess of the pretraining data. Text on the internet is generated by different sources (e.g., CDC),\nwhich we call agents following Andreas (2022). Modeling these agents allows LLMs to generate text\nconsistent with the respective agent\u2019s belief (e.g., COVID vaccines are safe). Assuming there is no\n\u2217 equal contribution\n1\narXiv:2310.18168v5  [cs.CL]  6 Feb 2024\nMODERNA ADMITS VAX CAUSES CANCER! Huge\nDevelopment As Millions Die From Covid Injections.\nBombshell! 95% COVID Deaths Among Vaccinated.\nAgent 1: InfoWars \nAgent 2: Before It\u2019s News\nAgent 3: The New York Times\nFeeling Terrible After Your Covid Shot? Then It\u2019s\nProbably Working.\nAgent 4: BBC\nCovid vaccines being given to millions of people in\nthe UK are extremely safe.\nTruthful\nPersona\n\ud83d\ude07\nUntruthful\nPersona\n\ud83d\ude08\nAgents are clustered into personas during LLM training\nDuring inference, answers match the inferred persona\nQ: Why is the COVID vaccine\nso deadly?\nLLM\n\ud83d\ude08\nA: The COVID vaccine is so\ndeadly\nbecause\nit\nis\na\nlive\nattenuated vaccine.\nQ: Are COVID vaccines safe for\nhumans?\nLLM\n\ud83d\ude07\nA: Yes, COVID vaccines have\nbeen\nshown\nto\nbe\nsafe\nfor\nhumans.\n\u201cWhich persona is more \nlikely to generate this text?\u201d\nFigure 1: Our main hypothesis is that LLMs can discern truth from falsehood by modeling truthful\npersonas in the pretraining data\u2014cluster of agents who are likely to be truthful (left). During\ninference, the model can infer the (un)truthful persona from the question, and respond (un)truthfully\naccordingly (right).\noracle agent that generates truthful text universally, to have a global notion of truth, the model must\nconnect multiple agents that are truthful in different domains. We hypothesize that these agents can be\nclustered together by common features of their outputs (e.g., formality and consistency with certain\nfacts), i.e. they share a persona that controls the generation process. By modeling and representing\nthe agent\u2019s persona given a piece of text, LLMs can separate truth from falsehoods.\nWe provide evidence for the persona hypothesis by two surprising observations we find on the\nTruthfulQA benchmark (Lin et al., 2021). First, using linear probing, we can predict whether the\ngenerated answer will be truthful or not from embeddings of the question alone, suggesting that\nthe model infers whether the agent has a truthful persona from the context (question). Second,\nfinetuning an LLM on a set of true question-answer pairs significantly improves its truthfulness on\nunrelated topics despite little knowledge transfer from the finetuning examples (e.g., blood type\nhas no influence on personality) to the test examples (e.g., single day\u2019s weather does not reflect the\nclimate). The generalization is only possible if LLMs have learned a persona representation that\ncontrols the truthfulness of facts across domains.\nNext, we verify our hypothesis through a synthetic environment of arithmetic, where different agents\nhave true or false beliefs about the semantics of each operator. We train LMs on equations generated\nby these agents. By controlling the data generating process, we show that models can separate true\nand false equations, and generalize an agent\u2019s truthful behavior to unseen operators, but this is only\npossible when a truthful persona exists, i.e. there is a group of truthful agents identifiable by common\nfeatures of their generations.\n2\nTHE PERSONA HYPOTHESIS\nWe assume that the pretraining data consists of a set of statements x generated by different agents\nparameterized by \u03b8agent \u2208 \u0398, which may specify the agent\u2019s belief and the style of its generation:\nx \u223c ptext(\u00b7 | \u03b8agent). For example, in Figure 1, agent \"BBC\" has the belief that COVID vaccines are\nsafe and produces text with a formal style. Further, groups of agents are generated from a persona\nparameterized by \u03bbpersona: \u03b8agent \u223c pagent(\u00b7 | \u03bbpersona). In particular, agents that are more likely to be\ntruthful share a persona, thus they are close to each other in \u0398. In Figure 1, agents \"NYT\" and \"BBC\"\ncan be clustered by their common beliefs and similar writing styles. In the following discussion, we\nremain agnostic to the specific features enabling the clustering of truthful agents, and we discuss\nwhether the truthful persona represents actual truth or merely superficial features associated with\ntruthful text in Section 5.\nOur main hypothesis consists of two parts:\n1. LMs infer the persona of groups of (un)truthful agents from the context, represent it in the\nactivation space, and generate text consistent with the inferred persona.\n2\n0\n5\n10\n15\n20\n25\n30\nEmbedding from layer\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nWeighted F1-Score\nProbes performance predicting truthfulness\nAll questions\nRandom Guessing\nInstruction\nQuestion start\nMid-Question\nLast Token\nToken used for probing\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nWeighted F1-Score\nProbes performance across context tokens\nRandom Guessing\nFigure 2: (Left) Mean and standard deviation for F1 of linear probes trained on each model layer\nto predict if the response will be truthful, over 20 randomized executions. (Right) F1 when training\nand evaluating probes at different input token embeddings. Best F1 is obtained when using the entire\nquestion. Additional metrics and ablations in Appendix B.\n2. (1) is only possible if the agents that generate truthful text in the pretraining data indeed share a\npersona (i.e. their generations have common features).\nTo verify this hypothesis, we first provide evidence for the existence of a latent truthful persona\nin LLM\u2019s representations (Section 3). We then show that such a representation arises from the\npersona-agent structure of the pretraining data through synthetic experiments (Section 4).\n3\nEVIDENCE OF LLMS MODELING PERSONAS\n3.1\nLLMS INFER PERSONAS FROM THE CONTEXT\nTo test hypothesis 1, we verify if the model can infer the (un)truthful persona from the context by\nprobing its internal activations. Specifically, we will show that truthfulness of the answer to a question\ncan be predicted from model activations before the answer is generated.\nExperimental setup.\nWe use the TruthfulQA dataset which contains question-answer pairs where\nthe answer can be either truthful or untruthful. We prompt the instruction-tuned Alpaca model (Taori\net al., 2023) with a question (see Appendix A for the detailed prompt) and obtain: (1) the embedding\nof every token of the question at each layer and (2) the generated answer to the question using greedy\ndecoding. We then label if the answer is truthful or not using GPT-judge (Lin et al., 2021) in line with\nprevious work (Nakano et al., 2021; Rae et al., 2021; Askell et al., 2021) (see Appendix C for details).\nThis gives us a dataset of token embeddings for questions and truthfulness of the sampled answer.\nWe then train a set of linear probing classifiers to predict truthfulness of an answer from the question\nembedding at different tokens and layers. We randomly split the dataset into 50% for training and\n50% for testing. To account for the imbalance in labels (Alpaca produces more untruthful answers\nthan truthful ones), we report the weighted F1-score of the probing classifier. We run each experiment\n(data splitting, training, evaluation) over 20 random seeds.\nResults.\nFigure 2 (left) shows the average and standard deviation of the F1-score of the probe using\nthe last token embedding from each layer. The probe performance is above random guessing from\nvery early layers and peaks at layer 17 at approximately 65% F1. This suggests that the model infers\nwhether the answer should be generated from an agent with a truthful persona while processing the\nquestion. Since the embedding does not contain information about the answer, the encoded persona\nlikely represents style or false presuppositions Kim et al. (2022) in the question.\nNext, we visualize the persona inference process by plotting the probe performance given the question\nembedding from layer 17 (where we observed the best performance previously) at different tokens.\nFigure 2 (right) shows that as we incorporate more context from left to right, the persona is represented\nmore prominently, peaking when the entire question is observed by the model, whereas probing the\ninstruction (which is same for all questions) performs at the level of random guessing.\nOne may wonder if the model is simply relying on the question topic to predict answer truthfulness,\nas Alpaca might be better at certain topics than others. Appendix B shows probing results for\n3\nTruthfulQA\nBigBench-misconceptions\nGPT-judge\nHuman evaluation\nHuman evaluation\nNo Finetuning\n39.0\u00b1 7.4\n31.7\u00b1 7.1\n54.2\u00b1 10.7\nTruthful finetuning\n74.4\u00b1 6.6\n58.0\u00b1 7.5\n59.4\u00b1 10.5\nUntruthful finetuning\n9.8\u00b1 4.5\n6.7\u00b1 3.8\n30.7\u00b1 9.9\nTriviaQA\n24.4\u00b1 6.5\n15.2\u00b1 5.4\n45.3\u00b1 10.7\nMS MARCO\n37.8\u00b1 7.4\n21.3\u00b1 6.2\n49.2\u00b1 10.7\nTable 1: Percentage of truthful model responses evaluated by the GPT-judge evaluator and human\njudges on 164 test questions with 95% confidence intervals. Finetuning on (un)truthful QA pairs\nmakes the model more (un)truthful on factually unrelated questions.\nthe 6 largest categories in TruthfulQA. We observe that the probe performs better than random\nguessing on all but one categories, ruling out the possibility that the probe is solely relying on the\ntopic. However, performance does vary with the question category, suggesting that for certain topics,\ntruthful statements can be harder to separate from false ones.\n3.2\nLLMS GENERALIZE TRUTHFULNESS ACROSS TOPICS\nHaving established that models can infer (un)truthful persona from the context and encode it in\nthe activation space, we now examine whether the the persona can control truthfulness of the\nmodel\u2019s generation across topics. We finetune LLMs on pairs of questions and truthful answers\nfrom TruthfulQA. Since all questions are factually unrelated (i.e. there is no knowledge that can be\ntransferred from training to test questions), generalization of truthfulness can be attributed to a latent\npersona that controls model behavior globally.\nExperimental setup.\nWe finetune Alpaca on question-answer pairs from TruthfulQA using LoRA\n(Hu et al., 2021). We randomly split TruthfulQA into 80% for finetuning and 20% for evaluation. In\ntruthful finetuning (TF), the model is trained to output truthful answers. To test our hypothesis in\nboth directions, we also perform untruthful finetuning (UF) where untruthful answers are used as the\ntargets. To ensure that the model is not relying on heuristics specific to TruthfulQA,1 we further test\nthe model on the misconception dataset from BigBench (Srivastava et al., 2022). We transform this\ndataset to fit our prompt format and remove questions similar to the ones in TruthfulQA, resulting in\n83 questions (see details in Appendix C). To evaluate truthfulness of the generated answers, we use\nboth GPT-Judge and human evaluation performed by the authors.\nTruthfulness generalizes to unseen topics and domains.\nIn Table 1, we observe substantial\nchanges in truthfulness after both TF and UF on TruthfulQA: Truthfulness of model generations\nincreases from 39% to 74% after TF, and decreases to 10% after UF; a similar trend holds according\nto human evaluation. Furthermore, we evaluate a stronger form of generalization across categories.\nWe train models on TruthfulQA while holding out one of the following categories: misconceptions\n(104 examples), specialized domains (economics, education, finance, health, law, nutrition, politics,\npsychology, science, sociology, statistics; 283 examples), and falsehoods (stereotypes, conspira-\ncies, superstitions, myths, and fairy tales, misinformation; 104 examples). In Figure 3 (left), an\nimprovement in truthfulness is observed for the heldout categories after finetuning. In addition,\nmodel performance on heldout categories is close to the TF model finetuned on all categories. These\nout-of-domain generalization results strengthen the evidence for a truthful persona shared by agents\nacross domains.\nTo ensure that the improvements do not come from general question-answering abilities (e.g., better\nadaptation to the QA format), we include a control experiment by finetuning Alpaca on random splits\nfrom TriviaQA (Joshi et al., 2017) and MS Marco (Nguyen et al., 2016) of the same size as our\nTF training set. The model is less likely to infer (un)truthful personas from these questions as they\ndo not have common untruthful answers on the internet. Thus, finetuning should provide a similar\n1TruthfulQA may contain superficial patterns that can be exploited to increase truthfulness. For example,\nmany questions contain false presuppositions, and \u201cno\u201d is often the correct answer.\n4\nFalsehoods\nMisconceptions Specialized dom.\nHeldout category\n0\n20\n40\n60\n80\n100\n% truthful generations\nTruthful generalization to heldout categories\nNo Finetuning\nTF (- category)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n% truthfulQA examples\n20\n30\n40\n50\n60\n70\n80\n90\n% truthful generations\nTruthful generalization to unseen questions\nNo finetuning\nIn-context learning\nTruthful finetuning\nFigure 3: Generalization of Alpaca to unseen TruthfulQA questions. (Left) Finetuned models\ngeneralize to heldout categories (TF - category), outperforming base models (No Finetuning). (Right)\nModels generalize truthfulness given small sample size.\nboost in QA abilities, but not modify the (un)truthful behavior we are studying. The results in Table\n1 show that models finetuned on these datasets have similar or worse truthfulness scores than the\nnon-finetuned model.\nModel generalizes from small sample size.\nIf finetuning mainly helps the model mirror an already\nexisting truthful persona, it should not require many examples to reach good performance. Thus, we\nfinetune the model with increasing sample sizes and investigate whether in-context learning (ICL)\nsimilarly guides the model to be more (un)truthful. We run TF with smaller splits (5%, 20%, and\n50%) and in-context learning with 10 (1.5%) and 20 (3%) examples. Results in Figure 3 (right) show\nthat, aside from ICL with 10 examples, all methods achieve a substantial increase in truthfulness.\nFinetuning on 20% of the data already matches the performance of finetuning on 80% of the data.\nOverall, our results support the hypothesis that LLMs infer and represent (un)truthful personas in\nthe activation space. During truthful finetuning, the model maps any inferred persona to the truthful\npersona, which then controls the truthfulness of its generations beyond the finetuning domains. As a\nresult, LLMs can directly generalize the truthful behavior as opposed to learning correct answers to\neach questions.\n4\nARITHMETIC LABORATORY: CONNECTING PRETRAINING DATA TO\nTRUTHFULNESS\nIn the previous section, we have shown evidence for hypothesis 1 which states that LLMs infer\n(un)truthful personas from the context. In this section, we verify hypothesis 2 by establishing a\ndirect connection between the pretraining data and model truthfulness. Specifically, we intervene on\nthe data generating process in a synthetic environment inspired by Power et al. (2022) and observe\nbehavior of an LM trained on this data.\nData generation. We design the synthetic data to simulate real pretraining data that contains a mixture\nof truthful and untruthful statements generated by various agents (e.g., Wikipedia and Twitter). The\nsynthetic data consists of arithmetic equations generated by different agents. An operator op \u2208 O\ntakes in two integer operands x, y \u2208 N+ and returns z. Each operator has two interpretations and\nwe randomly assign one to be true, denoted by opT , and the other to be false, denoted by opF . For\nexample, the result of op(3, 2) is 5 using the correct interpretation (addition), and is 1 using the\nincorrect interpretation (subtraction). Each agent a \u2208 S is parameterized by p(a,op) \u2208 (0, 1), which\nspecifies how likely it generates equations using the true interpretation of each operator op. Each\ndata point follows the format: a | x op y = z where z is either opT (x, y) or opF (x, y) depending\non the agent, and | is a separator token. Formally, we use the following generative process:\na \u223c U(S) ; op \u223c U(O) ; x, y \u223c U({1, 2, .., n})\n(1)\nz =\n\u001aopT (x, y)\nw.p. p(a,op)\nopF (x, y)\notherwise\n(2)\n5\n8\n10\n12\n14\n16\n18\n20\nm: Number of operators\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nMax Probing F1\nTruthful Persona\nNo Truthful Persona\nD\nE\nF\nG\n0.0\n0.2\n0.4\n0.6\n0.8\np_truthful\nD\nE\nF\nG\n0.0\n0.2\n0.4\n0.6\n0.8\np_truthful\nAgent Truthfulness Increases \nTruthful Persona\nNo Truthful Persona\nFigure 4: (left) Maximum F1 score across layer with std. deviation. A linear probe can predict if\nmodel will be truthful in the presence of truthful personas but it is harder when there is no truthful\npersona in the data; (right) Probability that the model assigns to the truthful answer (with std.\ndeviation) as described in Section 4.2. It increases with truthfulness of the agent when there is a\ntruthful persona, but we see high variance in the absence of a truthful persona.\nA\nB\nC\nop1\nT\nT\nU\nop2\nT\nT\nU\nop3\nT\nT\nU\nop4\nT\nT\nU\nTruthful Persona\nNo Truthful Persona\nA\nB\nC\nop1\nT\nU\nU\nop2\nT\nU\nT\nop3\nU\nT\nU\nop4\nU\nT\nT\nD\nE\nF\nG\nop1\nU\nU\nU\nT\nop2\nU\nU\nT\nT\nop3\nU\nT\nT\nT\nSeen\nUnseen\nop4\n?\n?\n?\n?\nAgent truthfulness increases \u2192\nT  - Truthful\nU - Untruthful\nFigure 5: Illustration of the synthetic setup used to test generalization. T and U in each cell refers\nto whether the agent has a high (T) or low (U) probability of using the true interpretation for the\ncorresponding operator. In the top setting, agents A and B who have similar probabilities of generating\ntruth form a truthful persona, whereas the bottom setting does not have such a persona. We evaluate\nwhether how models generalize for 4 new agents (D, E, F, G) whose behavior is only observed on a\nsubset of the operators.\nwhere U denotes the uniform distribution. The exact interpretations of operators can be found in\nAppendix D.\nWe can then further impose structures on top of the agents. Specifically, some agents have a higher\nlikelihood of using opT : p(a,op) \u223c U(0.8, 1) \u2200 op \u2208 O, forming a truthful persona, whereas others\nare less likely to use the correct interpretation: p(a,op) \u223c U(0, 0.2) \u2200 op \u2208 O, forming an untruthful\npersona. Note that to simulate the real world setting, no agents are completely truthful or untruthful\non an given operator.\nExperimental setup.\nIn each experiment, we train a 4-layer Transformer with 4 attention heads\nfrom scratch on the synthetic data using the causal language modeling objective. The hidden\ndimension and the embedding dimension are set to 128. All models are trained with a batch size of\n512 and a learning rate of 0.001 using the Adam optimizer Kingma & Ba (2014) for 20k steps. We\nuse a custom tokenizer where the vocabulary contains agent tokens, operator tokens, digit tokens and\nspecial tokens (e.g., the separator). Numbers are tokenized so that each digit is a separate token in the\nsequence. For more training details, see Appendix C.\n6\n4.1\nPROBING FOR TRUTHFULNESS\nMotivated by the observations on LLMs, we train probes to predict whether a model\u2019s answer for an\nincomplete equation (e.g., a | x op y =) will be truthful. We expect that it would only be possible\nto probe for truthfulness if there is a truthful persona in the generative process. That is, agents who\nare likely to produce truthful outputs are generated from the same distribution, forming a cluster. To\nablate the role of personas in truthfulness probing, we design two pretraining setups with and without\ntruthful personas as follows:\n1. Has truthful persona. We use four agents (A, B, C, and D) and m operators. A cluster of\ntruthful agents are defined by p(a,op) \u223c U(0.8, 1) \u2200 op \u2208 O, a \u2208 {A, B}; and a cluster of\nuntruthful agents are defined by p(a,op) \u223c U(0, 0.2) \u2200 op \u2208 O, a \u2208 {C, D}.\n2. No truthful persona. Same as in (1), we have four agents and m operators. However, the agents\nare truthful on disjoint sets of operators. Thus, their parameters p(a,\u00b7) are nearly orthogonal. This\nis analogous to agents having distinct true beliefs and no other shared features (e.g., style) in\npractical settings.\nIn both cases, we first generate synthetic data according to Equation 1 covering all agents, operators,\nand operands (i.e. 4 \u00b7 m \u00b7 10k data points in total with n = 100). We then randomly split this dataset\ninto 70% training data and 30% test data and train a language model. We vary m \u2208 {8, 12, 16, 20}.\nThen, we train probes to predict whether the model\u2019s prediction given an input expression a | x op y =\nis truthful or not. The probe is a linear model that takes in the embedding of \u2018=\u2019 from a particular\nlayer. Analogous to the LLM probing experiments, we train the probes on half of the operators and\nevaluate them on the other half to ensure that they do not simply learn which combinations of agents\nand operators are truthful, but rather rely on features that generalize across agents and operators (i.e.\nthe encoded personas). We train the probe on 5k examples and test on another 5k. Each experiment\nis run 3 times using different random seeds for splitting train/test operators. In initial experiments,\nwe observe that probes trained on different layers can achieve different performance. To account for\nthe variation, we report the maximum probing F1 across layers.\nIn Figure 4 (left), we observe that across all values of m, probes get higher F1 when training data\ncontains a truthful persona. In contrast, we observe a larger variance in the setting with no truthful\npersona. We hypothesize that this happens because, in the absence of a truthful persona, the probe\nhas arbitrary generalization on the unseen operators. This result supports hypothesis 2: true and false\nstatements can be distinguished only if agents can be clustered to form a (un)truthful persona.\n4.2\nGENERALIZING AGENT BEHAVIOR TO UNSEEN OPERATORS\nTo test our hypothesis that personas can be used to generalize an agent\u2019s behavior to unseen contexts,\nwe evaluate if models trained on the synthetic data can generalize a (un)truthful agent\u2019s behavior\nto unseen operators. We expect the model will generalize the behavior of a (un)truthful agent\nconsistently only in the presence of a truthful persona in the training data. We create two training\nsetups, as illustrated in Figure 5: (1) has truthful persona, and (2) no truthful persona.\nBoth training setups consist of seven agents (from A to G) and four different operators (from op1 to\nop4). Agents A, B, and C are trained on all four operators, whereas agents D through G are only\ntrained on op1, op2 and op3. op4 is heldout to evaluate generalization to unseen operators. The\nonly difference between both training setups is the behavior of agents A, B and C. In the \"truthful\npersona\" setup, agents A and B are generated from a truthful persona, and agent C is generated from\nan untruthful persona. However, in the \"no truthful persona\" setup, A, B, and C are truthful on only\ntwo out of the four operators with little overlap among them: each agent is generated in a distinct\nway.\nIn both setups, we first generate synthetic data according to Equation 1, and randomly split it into 70%\ntraining and 30% test data. We repeat the experiment 10 times, by randomly selecting the definitions\nof the operators.2 To evaluate the model on an unseen agent-operator combination, we compute the\naverage model likelihood for the truthful and untruthful answers across all held-out equations for\n2This is done to ensure that model generalization is not affected by the specific choice of the operator\ndefinitions.\n7\nD\nE\nF\nG\nTruthful Answer\n92.66%\n91.88%\n97.84%\n100%\nControl Answer\n47.82%\n45.36%\n45.29%\n46.33%\nUntruthful Answer\n96.38%\n94.73%\n90.78%\n79.33%\nControl Answer\n24.58%\n25.03%\n24.98%\n23.91%\nTable 2: Probing accuracy to either predict the truthful answer, the untruthful answer or a control\nanswer. Models encode both the truthful and untruthful answer better than the control answer,\nirrespective of whether the equation involves a truthful or an untruthful agent.\nthat operator. We use ptruthful and puntruthful to denote the average model likelihood for the truthful and\nuntruthful answers.\nResults.\nIn each of the two setups, we report ptruthful for the unseen operators across the four agents\nD, E, F, G in Figure 4 (right). We observe that in the setting with a truthful persona, the model\ngeneralizes truthfully for the truthful agent G on the unseen operator. Similarly, the model generalizes\nuntruthfully for the untruthful agent D3\u2014both have much smaller variance than the intermediate\nagents where the agents are not (un)truthful on all operators. On the other hand, in the setup with\nno truthful persona, there is not such a clear generalization pattern. In fact, we observe the model\ngeneralizes untruthfully for the most truthful agent G since the \u2018closest\u2019 agent in the training data is\nA (shared belief on op1 and op2 where both are truthful), and A has untruthful belief on op4.\nOverall, these results show that LMs are able to infer (un)truthful personas from the context because\nthe training data is generated by groups of agents with similar behavior. In our synthetic setup, the\ntruthful agents have similar probabilities of generating the true answer for each operator, which forms\na truthful persona. However, in the no truthful persona setting, even though the model has observed\nthe true answer for each operator (generated by different agents), there is no common feature that\nconnect these true answers, therefore the model is not able to infer a truthful persona that controls the\ntruthfulness of the generation.\n4.3\nMECHANISM FOR PERSONA-BASED COMPUTATION\nOur hypothesis in this work is that LLMs can infer the agent based on the input context, map it to an\n(un)truthful persona based on the cluster the agent belongs to, and generate (un)truthful continuations\naccordingly. An interesting question here is the mechanism used to perform the persona-based\ncomputation\u2014do LLMs first infer the persona and then compute the corresponding answer? Or do\nthey compute all possible answers and then pick one depending on the inferred persona?\nTo answer this question, we train two linear probes. One probe predicts the truthful answer and\nthe other predicts untruthful answer to the equation, respectively. We use the model from Figure 5\nwith truthful personas (top), and use the embedding of the \u2018=\u2019 token (before answer is generated)\nfrom the last layer to train the linear probes. Both the probes are trained on 50k randomly sampled\nexamples, and evaluated on held-out equations for op4. We also train control probes to predict an\nanswer of an unrelated operation as a baseline\u2014this helps to control for the possibility of the LLM\nencoding answers to all operators in the representation, or the probe learning to perform the task.\nMore experimental details can be found in Appendix C.\nIn Table 2, we find that irrespective of whether we condition on a truthful or an untruthful agent,\nmodels encode both the truthful and untruthful answers much better than the control answer. This\nindicates that models compute and store both possible answers to an input equation and then \u201cpick\u201d\nan answer based on the inferred persona. This could also help explain the success of supervised\nfinetuning in making models truthful (Ouyang et al., 2022), since the finetuning procedure only\nhas to change which answer the model picks instead of teaching it a new answer. We leave more\ninvestigation along this direction on larger models as future work.\n3See Appendix D for the graph of puntruthful.\n8\n5\nDISCUSSION\nHave LLMs robustly learnt what is truthful? In this work, we investigate the question of whether\nLLMs can distinguish true and false statements. Note that this does not necessarily mean that LLMs\nhave perfectly learnt the concept of truthfulness. First, as we observed in both the LLM finetuning\nand probing experiments, even though models perform much better than chance there is a still a\nconsiderable gap; e.g., we can probe with only up to \u224870% accuracy whether the model will make\na truthful prediction. Second, our experiments only provide evidence of the existence of truthful\npersonas, i.e. there exist features that the model can use to cluster truthful agents. Without knowing\nthe nature of these latent features (and whether they are spurious), it would be hard to conclude\nif LLMs robustly learn the concept of truthfulness. Nevertheless, the evidence that finetuning for\ntruthfulness generalizes to out-of-distribution data suggests that these features might be at least\nsomewhat meaningful. Additionally, according to our hypothesis, models would not be able to\ngeneralize to contexts where no truthful statements are observed in the training data.\nOther hypotheses of how LLMs can learn truthfulness. Firstly, we note that we only provide\none hypothesis of how LLMs might learn the concept of truthfulness which is consistent with our\nobservations. Nevertheless, the definition of personas is general enough to capture some other\nhypotheses of the mechanism behind truthfulness. For example, it could be possible that a small\nnumber of truthful and untruthful statements in the pretraining data have annotations, say from fact\nchecking websites.4 A model could use this annotation to cluster truthful and untruthful statements.\nLimitations of the synthetic setting. We note that even though we observe results consistent with\nour hypothesis in the synthetic setting, it has certain limitations and gaps compared to real LLMs.\nFirst, we explicitly represent the agent producing the data with a token. In real LLMs, models would\nhave to infer the agent from the actual text. Nevertheless, there is evidence suggesting that LLMs\ncan do it e.g. Li et al. (2021) show that LMs encode information about the agents\u2019 properties and\nrelations even if not explicitly mentioned in text. Second, in the synthetic setting, we assumed that\nboth truthful and untruthful answers are equally easy or equally hard to compute. This leaves the\nopen questions of whether truthful (or untruthful) answers might be \u201csimpler\u201d to model in real text,\nand whether complexity may play a role in modeling truthfulness. Additionally, we assume that\ntruthful agents share common beliefs across most, if not all, operators. In practice, truthful agents do\nnot necessarily agree on every fact.\n6\nRELATED WORK\nEvaluating truthfulness of LLMs. Lin et al. (2021) showed that LLMs mimic human falsehoods\nand larger models are generally less truthful. However a follow-up (Wei et al., 2022) showed that this\nbehaviour is in fact U-shaped \u2014 beyond a certain scale, truthfulness seems to increase as we increase\nthe scale of models.\nImproving truthfulness. Recent work has shown that despite LLMs mimicking human falsehoods\nand not always being truthful, it is possible to perform model interventions to make the model\nmore truthful. Burns et al. (2022) showed that using an unsupervised consistency-based method\ncan help elicit truthful answers beyond what the LLM outputs. Similarly, Li et al. (2023) showed\nthat interventions on specific attention heads which are responsible for truthfulness can make the\nmodel more truthful during inference. Chuang et al. (2023) showed that decoding by contrasting\nacross layers can increase truthfulness. Recent work has also shown, similar to our probing results,\nthat we can detect whether an answer produced by LLM is truthful either using its internal state\nrepresentation (Azaria & Mitchell, 2023) or using linguistic features of the answer (Lee et al., 2023).\nAll of this work provides evidence of LLMs having some notion of truthfulness. We build on this\nliterature to do more controlled generalization and probing experiments, and propose a hypothesis of\nhow LLMs could learn the concept of truthfulness.\nPersonas and Agents in LLMs. Despite conflicting information in the data (Chen et al., 2022),\nAndreas (2022) argued that LLMs can serve as models of agents where they can infer properties of\nthe agent and predict the next word accordingly. There has been some empirical evidence suggesting\nthe same \u2014 Durmus et al. (2023) show that we can steer LLMs to express opinions similar to people\n4e.g. https://www.factcheck.org, https://www.politifact.com\n9\nfrom some countries; Safdari et al. (2023) find that personality tests for LLMs under specific prompts\nare valid and reliable; Zhou et al. (2023); Lin et al. (2021) show that adopting a persona of a professor\ncan improve truthfulness in LLMs; Deshpande et al. (2023) showed that LLMs have learnt personas\nand certain personas can increase toxicity; Cheng et al. (2023) showed that we can use persona to\nmeasure stereotypes in LLMs. Our work builds on these to show how LLMs modeling agents and\ninferring personas can help it to discern true and false statements.\n7\nCONCLUSION\nWe introduce a hypothesis of how LLMs can model truthfulness: persona hypothesis\u2014LLMs can\ngroup agents that share common features into personas that can be used to distinguish true from false\nstatements and to generalize agent behavior beyond the context in which it was observed during\ntraining. We provide evidence that supports this hypothesis in both LLMs and a synthetic setup,\nand the implications this might have for truthfulness. A better understanding of such a potential\nmechanism in LLMs may enable more effective strategies to build trustworthy language models.\nACKNOWLEDGEMENTS\nWe thank Jacob Andreas, Ellie Pavlick, Nicholas Lourie, Vishakh Padmakumar and Richard Pang for\ntheir inputs on various stages of the project. NJ is supported by an NSF Graduate Research Fellowship\nunder grant number 1839302. JR is supported by grants from the Open Philanthropy Project and the\nLong-Term Future Fund. This work is supported by Open Philanthropy, AWS AI, and the Samsung\nAdvanced Institute of Technology (Next Generation Deep Learning: Pattern Recognition to AI).\nREFERENCES\nJacob Andreas. Language models as agent models. In Findings of the Association for Computational\nLinguistics: EMNLP 2022. Association for Computational Linguistics, 2022.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, T. J. Henighan, Andy Jones,\nNicholas Joseph, Benjamin Mann, Nova DasSarma, et al. A general language assistant as a\nlaboratory for alignment. ArXiv, abs/2112.00861, 2021.\nAmos Azaria and Tom M. Mitchell. The internal state of an llm knows when its lying. ArXiv,\nabs/2304.13734, 2023.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. ArXiv, abs/2005.14165, 2020.\nCollin Burns, Hao-Tong Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in\nlanguage models without supervision. ArXiv, abs/2212.03827, 2022.\nHung-Ting Chen, Michael J.Q. Zhang, and Eunsol Choi. Rich knowledge sources bring complex\nknowledge conflicts: Recalibrating models to reflect conflicting evidence. In Conference on\nEmpirical Methods in Natural Language Processing, 2022.\nMyra Cheng, Esin Durmus, and Dan Jurafsky. Marked personas: Using natural language prompts to\nmeasure stereotypes in language models. ArXiv, abs/2305.18189, 2023.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He.\nDola: Decoding by contrasting layers improves factuality in large language models. ArXiv,\nabs/2309.03883, 2023.\nA. Deshpande, Vishvak Murahari, Tanmay Rajpurohit, A. Kalyan, and Karthik Narasimhan. Toxicity\nin chatgpt: Analyzing persona-assigned language models. ArXiv, abs/2304.05335, 2023.\n10\nEsin Durmus, Karina Nyugen, Thomas Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin,\nCarol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, et al. Towards measuring\nthe representation of subjective global opinions in language models. ArXiv, abs/2306.16388, 2023.\nJ. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685, 2021.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601\u2013\n1611. Association for Computational Linguistics, 2017. doi: 10.18653/v1/P17-1147.\nNajoung Kim, Phu Mon Htut, Samuel R. Bowman, and Jackson Petty. (QA)2: Question answering\nwith questionable assumptions. arXiv preprint arXiv:2212.10003, 2022.\nDiederik P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\nCoRR,\nabs/1412.6980, 2014.\nBruce W. Lee, Benedict Florance Arockiaraj, and Helen Jingshu Jin. Linguistic properties of truthful\nresponse. ArXiv, abs/2305.15875, 2023.\nBelinda Z. Li, Maxwell Nye, and Jacob Andreas. Implicit representations of meaning in neural\nlanguage models. In Annual Meeting of the Association for Computational Linguistics, 2021.\nKenneth Li, Oam Patel, Fernanda Vi\u2019egas, Hans-R\u00fcdiger Pfister, and Martin Wattenberg. Inference-\ntime intervention: Eliciting truthful answers from a language model. ArXiv, abs/2306.03341,\n2023.\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958, 2021.\nReiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. ArXiv, abs/2112.09332, 2021.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and\nLi Deng. Ms marco: A human-generated machine reading comprehension dataset. 2016.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. ArXiv, abs/2203.02155, 2022.\nAlethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, and Vedant Misra. Grokking:\nGeneralization beyond overfitting on small algorithmic datasets. ArXiv, abs/2201.02177, 2022.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\nMethods, analysis & insights from training gopher. ArXiv, abs/2112.11446, 2021.\nMustafa Safdari, Greg Serapio-Garc\u2019ia, Cl\u2019ement Crepy, Stephen Fitz, Peter Romero, Luning Sun,\nMarwa Abdulhai, Aleksandra Faust, and Maja J Matari\u2019c. Personality traits in large language\nmodels. ArXiv, abs/2307.00184, 2023.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya Gupta, and Adri\u00e0 Garriga-Alonso et al. Beyond\nthe imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv,\nabs/2206.04615, 2022.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nJason Wei, Yi Tay, and Quoc V. Le. Inverse scaling can become u-shaped. ArXiv, abs/2211.02011,\n2022.\n11\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and\nJimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International\nConference on Learning Representations, 2023.\nA\nALPACA PROMPTS\nTo prompt Alpaca in a 0-shot setting, we adapt the prompt used by the original Alpaca authors to\nfinetune the model (Taori et al., 2023) for question answering. We also use this prompt for our\nprobing and finetuning experiments.\n### Instruction:\nAnswer the following question\n### Input:\n{question}\n### Response:\nwhere {question} is the placeholder for the question. In our probing experiments, we use the\nembedding of the last prompt token before the response sampling starts.\nFor in-context learning (ICL), however, we use a shorter prompt for the examples to fit in the context\nwindow.\nQ: {example question 1}\nA: {example answer 1}\n...\nQ: {example question N}\nA: {example answer N}\nQ: {test question}\nA:\nB\nPROBING ABLATIONS\nWe run some additional experiments to better understand the probing results from Section 3.1. First,\nas described before, we analyze the performance of the probe across different topics in Figure 6. We\nobserve that the performance of the probe varies by topic e.g. it is much easier to detect if model\nwill be truthful for question from economics compared to questions involving stereotypes. This\npotentially suggests that personas may not be perfectly defined over all topics, and there could in fact\nbe much smaller clusters of truthful agents.\nNext, to expand on the results in Figure ??, we use the same tokens to obtain the representation but\ninstead of using a specific layer (layer 17), we plot the performance of the probe across different\nlayers in Figure 7.\nFigure 8 reports accuracy as an alternative probing metric for Figure 2.\nFinally, Figure 9 reports probing results over the generated tokens as a baseline for results in Figure\n??. Probing the embedding of the last generated token in the answer obtains a better performance than\nprobing only the question context. However, the difference is small and suggests that the question is\nalready very informative for truthfulness of the generation.\n12\n0\n5\n10\n15\n20\n25\n30\nLayer\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nF1-Score\nProbes performance per topic\nEconomics\nLaw\nHealth\nSociology\nMisconceptions\nStereotypes\nRandom Guessing\nFigure 6: Variation of the F1 score of the probe trained across different layers for different topics.\nIt it easier to predict if model will be truthful for certain topics (e.g. Economics) than others (e.g.\nStereotypes).\nFigure 7: F1 score of the probe when trained on different tokens of the prompt. As more context is\nincorporated, the performance of the probe increases.\n0\n5\n10\n15\n20\n25\n30\nEmbedding from layer\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nAccuracy\nProbes performance predicting truthfulness\nAll questions\nRandom Guessing\nFigure 8: Mean and standard deviation for accuracy of linear probes trained on each layer of the\nmodel to predict if the response will be truthful over 20 randomized executions.\n13\nInstruction\nQuest. start\nMid-Quest.\nQuest. end\nMid-Gen.\nGen. end\nToken used for probing\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nWeighted F1-Score\nProbes performance across context tokens\nRandom Guessing\nFigure 9: F1 obtained when training and evaluating linear probes at different input and generation\ntoken embeddings as an extension of results in Figure ??.\n14\nC\nEXPERIMENT DETAILS\nTruthfulQA Evaluation. We use GPT-Judge for automatically evaluating if the model generation is\ntruthful, in line with previous work (Nakano et al., 2021; Rae et al., 2021; Askell et al., 2021). To\nobtain the GPT-Judge model, we use the OpenAI finetuning API at https://platform.openai.\ncom/docs/guides/finetuning using the datasets released in the TruthfulQA work - https://\ngithub.com/sylinrl/TruthfulQA. We use the default hyperparameters and prompt suggested by\nthe original authors.\nFinetuning for TruthfulQA. In all the finetuning experiments, we train Alpaca for 30 epochs with a\nbatch size of 48. We use the Adam optimizer Kingma & Ba (2014) with a learning rate of 9e \u2212 5 and\na warmup ratio of 0.03. To finetuning models with a smaller compute, we use LORA Hu et al. (2021)\n\u2014 we apply it to the query and key projection matrices where we set the rank to 16, a dropout rate of\n0.05.\nTransforming the BigBench misconceptions dataset. This dataset contains statements for classifi-\ncation instead of question-answer pairs. We covert these statements into QA pairs using GPT-3.5\n(Brown et al., 2020), and manually correct some generated questions which were not correct. Addi-\ntionally, we manually filter questions about topics contained in TruthfulQA to avoid overlap between\nthem. The resulting dataset contains 83 examples.\nTraining in the synthetic setup. As mentioned before, we train 4-layer transformer models on the\ngenerated synthetic data with the language modeling objective. The hidden dimension as well as the\nembedding dimension are set to 128 and each layer contains 4 self-attention heads. All models are\ntrained with a batch size of 512 and learning rate of 0.001 using the Adam optimizer Kingma & Ba\n(2014) for a total of 20k steps. We create a custom tokenizer to ensure that each digit is tokenized\nseparately. Specifically, the tokenizer contains the following tokens \u2014 one token for each agent,\nseparator token (\u2018|\u2019), start of sequence token, end of sequence token, tokens corresponding to each\ndigit (0-9), one token for each operator in the data and a token for \u2018=\u2019.\nMechanism for agent-based computation. To train the linear probes for Section 4.3, since the\nanswers can span multiple digits, we train the probe to predict the first different digit between the\ntruthful and untruthful answers. e.g. if the truthful answer is 23 and the untruthful answer is 26, the\ntwo probes will be trained on the representation of \u20182\u2019 to predict \u20183\u2019 or \u20186\u2019 respectively. This is done\nto reduce the output space of the probe. The probe is a linear model. To train the control probe for the\ntruthful answer, we select an answer based on the truthful operator for a different randomly sampled\noperator. Similarly to train the control probe for the untruthful answer, we sample an answer based\non a untruthful interpretation of a different operator.\nD\nSYNTHETIC DATASET GENERATION\nIn this section, we describe the details of the exact semantics of each operator in the synthetic setup\nas well as the hyperparameters used to generate the data.\nD.1\nPROBING FOR TRUTHFULNESS\nIn this experiment we have two training data setups, one with truthful persona and one without\na truthful persona as described in Section 3.1. In each setup, we have m operators where m \u2208\n{8, 12, 16, 20}. Instead of manually defining all the operators, we use the following to sample truthful\nand untruthful interpretations of the operators:\nopT (x, y) = x + y + r1\n(3)\nopF (x, y) = x + y + r2\n(4)\nwhere r1, r2 are randomly sampled for each of the operators from the range (0, 70). Note that r1 and\nr2 are different for all the operators.\n15\nWe use n = 100 (i.e. range 100 for x, y) and randomly select the generation parameters. Specifically,\nif an agent a is truthful on operator op, we set p(a,op) to be a random value > 0.8 and vice versa we\nset it to < 0.2 if the agent is untruthful.\nD.2\nGENERALIZATION TO UNSEEN OPERATORS\nThis experiment contains two setups, one with truthful persona and one without truthful persona as\ndescribed in Section 4.2. Both setups contain four operators, op1 to op4.\nNotation. In the following, first() and last() are used for functions that denote the first and last\ndigit of the argument respectively. We use \u2018;\u2019 to denote the concatenation of the two numbers (e.g.\n2; 3 \u2192 23). We use first2() for the function denoting the first two digits of the argument (e.g.\nfirst2(123) = 12).\nThe exact semantics of the four operators of the truthful interpretations of the operators are as below:\n1. op1T (x, y) = first(x + 4) + first(y + y)\n2. op2T (x, y) = last(x) + last(y + y)\n3. op3T (x, y) = first(x); last(y + y)\n4. op3T (x, y) = first2(x + x)\nSimilarly, the untruthful interpretaion for each of the four operators are:\n1. op1F (x, y) = last(y + y) + first2(x)\n2. op2F (x, y) = first(x + x) + last(y)\n3. op3F (x, y) = first2(x + y) + first(y)\n4. op3F (x, y) = last(x + y) + first2(y)\nWe designed these operators, so that the models we are using can learn these operations. We also\nensured that all interpretations are distinct and unrelated to each other, although all of them are\nsimilarly \u2018complex\u2019 allowing the model to learn the operations at similar times during training.\nWe use n = 200 (i.e. range 200 for x, y) and randomly set the generation parameters. Specifically, if\nan agent a is truthful on operator op, we set p(a,op) to be a random value > 0.8 and vice versa we set\nit to < 0.2 if the agent is untruthful.\nE\nGENERALIZATION TO UNSEEN AGENT-OPERATOR COMBINATIONS\nD\nE\nF\nG\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\np_untruthful\nD\nE\nF\nG\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\np_untruthful\nAgent Truthfulness Increases \nTruthful Persona\nNo Truthful Persona\nFigure 10: Probability that the model assigns to the untruthful answer \u2014 puntruthful decreases as the\ntruthfulness of agent increases in the first setup, whereas the behavior widely varies in the second\nsetup.\nIn Section 4.2, we demonstrated that models can generalize (un)truthfully for (un)truthful agents only\nin the presence of a truthful persona. To do so, we looked at ptruthful across all agents for the unseen\n16\noperator. Here, we additionally plot puntruthful, the average probability assigned by the model to the\nuntruthful answer in Figure 10.\n17\n"
  }
]