[
  {
    "title": "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens",
    "link": "https://arxiv.org/pdf/2401.17377.pdf",
    "upvote": "31",
    "text": "Preprint\nInfini-gram: Scaling Unbounded n-gram Language Models to\na Trillion Tokens\nJiacheng Liu\u2661\nSewon Min\u2661\nLuke Zettlemoyer\u2661\nYejin Choi\u2661\u2660\nHannaneh Hajishirzi\u2661\u2660\n\u2661Paul G. Allen School of Computer Science & Engineering, University of Washington\n\u2660Allen Institute for Artificial Intelligence\nliujc@cs.washington.edu\nAbstract\nAre n-gram language models still relevant in this era of neural large lan-\nguage models (LLMs)? Our answer is yes, and we show their values in both\ntext analysis and improving neural LLMs. Yet this necessitates modernizing\nn-gram models in two aspects. First, we train them at the same data scale\nas neural LLMs \u2013 1.4 trillion tokens. This is the largest n-gram model\never built. Second, existing n-gram models use small n which hinders their\nperformance; we instead allow n to be arbitrarily large, by introducing a\nnew \u221e-gram LM with backoff. Instead of pre-computing n-gram count\ntables (which would be very expensive), we develop an engine named\ninfini-gram \u2013 powered by suffix arrays \u2013 that can compute \u221e-gram (as well\nas n-gram with arbitrary n) probabilities with millisecond-level latency.\nThe \u221e-gram framework and infini-gram engine enable us to conduct many\nnovel and interesting analyses of human-written and machine-generated\ntext: we find that the \u221e-gram LM has fairly high accuracy for next-token\nprediction (47%), and can complement neural LLMs to greatly reduce their\nlanguage modeling perplexities. When analyzing machine-generated text,\nwe also observe irregularities in the machine\u2013\u221e-gram agreement level with\nrespect to the suffix length, which indicates deficiencies in neural LLM pre-\ntraining and the positional embeddings of Transformers. We open-source\nour infini-gram engine in the hopes of enabling more study on how to best\nuse verbatim information retrieved from large text corpora.\nFigure 1: When trained on an 1.4-trillion-token corpus, a 5-gram LM is unable to correctly\npredict the next token of the given prompt, because it uses a short, fixed context length of 4.\nOur \u221e-gram LM adaptively selects n based on the longest suffix of the prompt that has a\nnon-zero count in the corpus, and in this case, using a larger n yields a correct prediction.\nThe counting and distribution estimate in \u221e-gram LM are powered by our infini-gram\nengine.\n1\narXiv:2401.17377v1  [cs.CL]  30 Jan 2024\nPreprint\n1\nIntroduction\nWhen pretrained on trillion-token corpora, neural large language models (LLMs) achieve\ngroundbreaking performance (Touvron et al., 2023a; Geng & Liu, 2023). However, we do\nnot yet know how such data scale would benefit other language modeling approaches. In\nparticular, how well do classical, n-gram language models (LMs) perform if estimated from\nsuch massive corpora? In other words, are n-gram LMs still relevant in this era of neural LLMs?\nOur answer is yes. As we will show, n-gram LMs are useful for both text analysis and\nimproving neural LLMs. Yet we need to first modernize the traditional n-gram LM in two\naspects: the training data size, and the value of n. To achieve broader data coverage, we\nscale up the training data for n-gram LMs to 1.4 trillion tokens, which rivals the size of the\nlargest open-source text corpora (Together, 2023; Soldaini et al., 2023). To our knowledge,\nthis is the largest n-gram LM ever built. Historically, n-gram indexes have been built only\nfor small n\u2019s (e.g., n \u2264 5; Franz & Brants (2006)), because the size of naive n-gram count\ntable grows almost exponentially with respect to n. We instead find there is significant value\nin increasing the value of n. As illustrated in Figure 1, n-gram LMs with small n (n = 5) are\noften poorly predictive of the next token, because it discards the rich context in the long\nprompt; meanwhile, if we can use a larger n (in this case n = 16), the prediction can become\nmuch more accurate. As such, we develop our n-gram LM with unbounded n, or in other\nwords, an \u221e-gram LM. We use a variant of backoff (Jurafsky & Martin, 2000), where we\nresort to smaller n when longer n-grams have a zero count. Due to sparsity in the \u221e-gram\nestimates, in some of the later experiments (e.g., \u00a75), we will interpolate between the \u221e-gram\nLM and neural LMs to yield a mixture LM upon which perplexity can be computed.\nWe develop a low-latency, resource-efficient engine to serve this massive \u221e-gram LM.\nInstead of building an explicit n-gram count table, which is infeasible for arbitrarily large\nn and such extreme data scale, we implement the \u221e-gram LM with a suffix array \u2013 a data\nstructure that supports fast n-gram counting at inference time, and is efficient in both storage\nspace and compute. Our suffix array implementation takes 7 bytes of storage per token (3.5x\noverhead compared to the raw data), and on the training data with 1.4 trillion tokens, it can\nbe built with a single 80-core CPU node in less than 3 days, using 10 TiB of disk storage.\nAverage inference latency is less than 20 milliseconds for counting an n-gram and finding\nall positions of its occurrence (regardless of how large n is or how frequently the n-gram\nappears), and 200 milliseconds per token for n-gram/\u221e-gram probability estimation and\ndecoding (more in \u00a73.4 and Table 1). All indexes stay on-disk at inference time. We refer to\nthis \u221e-gram engine as infini-gram.\nAnalyses with \u221e-gram (\u00a74) offers new insights into human-written and machine-generated\ntext. We found that \u221e-gram has a fairly high accuracy (47%) when predicting the next token\ngiven a prefix of a human-written document, and this accuracy is higher on tokens where\nthe effective n is larger. In contrast, conventional n-grams (with small n) are insufficient\nin capturing a long enough context to predict the next token (29% accuracy). The correct-\nness of \u221e-gram predictions does not perfectly coincide with those predictions made by\nneural LMs, implying that \u221e-gram can complement and improve neural LMs and reach\nbetter performance when combined. In fact, our experiments (\u00a75) show that heuristically\ninterpolating between the estimates made by \u221e-gram and neural LMs can greatly reduce\nlanguage modeling perplexity (by up to 73%) compared to the neural LMs alone, even when\nthe neural LM is as large as 70B. When analyzing the level of agreement with \u221e-gram, we\nfind that nucleus sampling (Holtzman et al., 2019) from neural LMs produces machine-\ngenerated text with an agreement plot most similar to human-written text, among other\ndecoding methods like greedy decoding and temperature sampling; for greedy decoding,\nwe observe significant fluctuation in the agreement level with respect to the suffix length,\nwhich indicates deficiencies in neural LM pretraining and the positional embeddings of\nTransformers.\nWe open-source1 the pre-built infini-gram indexes on some datasets, the code for performing\nn-gram/\u221e-gram inferences, and the code for building indexes for new corpora. We also\n1Code and pre-built indexes: https://github.com/liujch1998/infini-gram (coming soon)\nDemo: https://huggingface.co/spaces/liujch1998/infini-gram\n2\nPreprint\nplan to host a public API service for n-gram/\u221e-gram querying. We hope these resources\ncan enable more insightful analysis and understanding of large text corpora, and open up\nnew avenues for data-driven language modeling.\n2\n\u221e-gram LM: Extending n-gram LMs with Unbounded n\nBackground: n-gram LM.\nThe n-gram LM is a classical, statistical language model based\non counting the occurrences of n-grams. In its most simple form, the probability of a token\nwi given a context wi\u2212(n\u22121):i\u22121 is estimated as\nPn(wi|wi\u2212(n\u22121):i\u22121) =\ncnt(wi\u2212(n\u22121):i\u22121wi | D)\ncnt(wi\u2212(n\u22121):i\u22121 | D)\n(1)\nwhere cnt(w | D) is the number of times the n-gram w appears in the training data D (i.e.,\na corpus), and n is a pre-defined hyperparameter. (When n = 1, we define wi\u2212(n\u22121):i\u22121 as\nthe empty string \u03b5, whose count is equal to |D|.) However, this naive version of n-gram\nLM may run into the sparsity issue: the numerator in Equation 1 may be zero, resulting\nin an infinite perplexity. One common technique to address this issue is backoff (Jurafsky\n& Martin, 2000): on an instance-wise basis, when the numerator is zero we decrease n by\none, and we can do this repeatedly until the numerator becomes positive. One caveat in the\nbackoff strategy is that it does not yield a valid distribution for Pn(\u2217|wi\u2212(n\u22121):i\u22121), because\nthe effective n depends on wi. Therefore, further probability discounting is required to\nnormalize the distribution (e.g., Katz backoff (Katz, 1987)).\nHistorically, n-gram LMs have been implemented by building an n-gram count table of the\ntraining data. This table stores all unique n-grams that appear in the training data, each\nassociated with its count. Such n-gram count tables are huge and grow almost exponentially\nwith respect to n. For example, we estimate that the 5-gram count table for a 1.4-trillion-\ntoken corpus would consume 28 TiB of disk space. As a result, previous n-gram LMs are\nlimited to very small n, most commonly n = 5 (Franz & Brants, 2006; Aiden & Michel, 2011).\nAs we illustrated in Figure 1 and will further quantify in \u00a74, the problem with small n is\nthat it discards richer context, making such n-gram LMs poorly predictive of future tokens.\n\u221e-gram LM.\nThe \u221e-gram LM is a generalization of the n-gram LM, where conceptually\nwe start backing off from n = \u221e. We use a variant of backoff: we backoff only when\nthe denominator in Equation 1 is zero. This means we stop backing off as soon as the\ndenominator becomes positive, upon which the numerator might still be zero. This is fine\nbecause we will not be evaluating the perplexity of the \u221e-gram LM itself. On an instance-\nwise basis, the effective n is equal to one plus the length of the prompt\u2019s longest suffix that\nappears in the training data.\nFor the rest of this paper, we will use \u201c\u221e-gram\u201d to refer to the \u221e-gram LM. \u221e-gram is\nformally defined as\nP\u221e(wi|w1:i\u22121) =\ncnt(wi\u2212(n\u22121):i\u22121wi | D)\ncnt(wi\u2212(n\u22121):i\u22121 | D)\nwhere w1:i\u22121 are all tokens preceding wi in the document, and\nn = max{n\u2032 \u2208 [1, i] | cnt(wi\u2212(n\u2032\u22121):i\u22121 | D) > 0}.\nUnlike Katz backoff, P\u221e(\u2217|w1:i\u22121) is a valid distribution by construction and does not\nrequire discounting. This is because the effective n is solely dependent on w1:i\u22121 and does\nnot depend on wi, and \u2211wi\u2208V cnt(wi\u2212(n\u22121):i\u22121wi | D) = cnt(wi\u2212(n\u22121):i\u22121 | D).\nFurther, we define the sparsity of this \u221e-gram estimate:\nan estimate is sparse iff\nP(wi|wi\u2212(n\u22121):i\u22121) = 1 for one of the wi \u2208 V, and is zero for all other tokens in the vo-\ncabulary. Intuitively, this means there is only one possible next token given this context,\naccording to the training data. As we will show in \u00a74, sparse estimates are more predictive\nof the ground truth tokens than non-sparse ones.\n3\nPreprint\nFigure 2: Left: the suffix array for a toy string. Right: illustration of the suffix array in our\ninfini-gram implementation, with N = 4 tokens in the training data.\nInterpolating with neural LMs.\nSparsity in the \u221e-gram estimates causes issue in its\nevaluation: a zero probability assigned to the ground truth token would give an infinite\nperplexity. We do not attempt to compute the perplexity of the \u221e-gram in isolation. Instead,\nwe interpolate it with neural LMs and show perplexity improvement compared to the neural\nLMs alone (\u00a75). The combined model is formally\nP(y | x) = \u03bbP\u221e(y | x) + (1 \u2212 \u03bb)Pneural(y | x),\nwhere \u03bb \u2208 [0, 1] is a hyperparameter.\n3\nInfini-gram: A Performant Engine for n-gram/\u221e-gram Queries\nTo maximize their usefulness, we would like to train \u221e-gram on the modern, trillion-token\ntext corpora. However, it is practically infeasible to build the n-gram count table with\nunbounded n for such massive training data, since such a table would contain O(|D|2)\nentries. In this section, we describe our infini-gram engine that processes n-gram/\u221e-gram\nqueries efficiently. Infini-gram is powered by a data structure called suffix array (\u00a73.1). We\nwill show how to build this suffix array index (\u00a73.2) and how to perform n-gram/\u221e-gram\ninferences with it. In \u00a73.4 we list the six types of queries supported by infini-gram and\nbenchmark their latency.\n3.1\nSuffix Array\nThe essence of n-gram and \u221e-gram LMs is counting a given n-gram in the training data.\nFor this reason, we leverage the suffix array data structure, which is originally designed\nfor efficiently counting the number of times a given \u201cneedle\u201d string (length L) appears as\nsubstring of a huge \u201chaystack\u201d string (length N). When the suffix array is built for the\nhaystack string, counting any given needle string would have time complexity O(L + log N).\nA suffix array represents the lexicographical ordering of all suffixes of an array (or a string,\nwhich is an array of characters). For an array of length N, the suffix array contains N unique\nintegers, where the i-th element is the starting position of the suffix ranked i-th among all\nsuffixes. Figure 2 (left) shows the suffix array for an example string, aabaca.\nWe build the suffix array on the byte array of the tokenized training data (Figure 2, right).\nDocuments are prefixed by a 4-byte document ID and separated by the \\xff\\xff token. In\nthe byte array, each consecutive two bytes represent a token ID (assuming that |V| < 216 =\n65536). Given that the training data has N tokens, the size of the byte array is 2N bytes. The\nsuffix array contains N elements, each pointing to a token in the byte array by storing its\n4\nPreprint\nFigure 3: n-gram/\u221e-gram queries on a training data are supported by an associated suffix\narray. Both the training data and the suffix array are stored on-disk as regular files. Contents\non the white strips are file data, and addresses above the strips are byte offsets. Querying for\na particular n-gram returns a consecutive segment of the suffix array, where each element is\na pointer into the training data where the n-gram appears. E.g., in the trillion-token training\ndata, Artificial Intelligence, A Modern appears 42 times, and in all cases the following\ntoken is Approach.\nbyte offset. All elements in the suffix array are even numbers (so that they point to valid\ntokens), and all token positions in the byte array appears exactly once in the suffix array.\nEach pointer can be stored with \u2308log2(2N)/8\u2309 bytes. For corpora with 2B to 500B tokens\n(which is the range we deal with, after sharding (\u00a73.2)), this is 5 bytes per pointer, and thus\nthe size of the suffix array is 5N bytes. The combined size of tokenized data and suffix array\n(i.e., the infini-gram index) is thus 7N bytes.\n3.2\nBuilding the suffix array\nSuffix arrays can be built in linear time with respect to the length of the byte array\n(K\u00a8arkk\u00a8ainen et al., 2006). We adapted from the suffix array implementation in Lee et al.\n(2022) and further optimized it for efficiency. We then built the suffix arrays for the 360B-\ntoken Pile (Gao et al., 2020) and the 1.4T-token RedPajama (Together, 2023). It took us \u223c56\nhours to build the suffix array for RedPajama on a single node with 80 CPUs and 512G\nRAM.\nSharding.\nBuilding the suffix array requires heavy random access to the byte array, and\nthus the entire byte array must be kept in RAM so that the building time is reasonable.\nHowever, the byte array may be too large to fit into RAM. We shard the byte array into\nmultiple shards, and build a suffix array for each shard. Sharding would induce additional\ninference latency, which we discuss and mitigate below (\u00a73.3).\n3.3\nInference with the suffix array\nn-gram counting.\nComputing the n-gram probability involves counting the number of\noccurrences of a token string, i.e., cnt(x1...xn). Since the suffix array represents a lexico-\ngraphical ordering of all suffixes of the training data, the occurrence positions of strings\nstarting with x1...xn lies in a single, consecutive segment in the suffix array. Thus we only\nneed to find the first and last occurrence positions, and the count would be the difference\nbetween these positions. Both the first and last occurrence positions can be found with\n5\nPreprint\nbinary search, with time complexity O(n \u00b7 log N) and O(log N) random array accesses. The\ntwo binary searches can be parallelized, reducing the latency by roughly 2x. The impact of\nquery length n is negligible, because computers usually fetch memory in pages of (typically)\n4096 bytes, and string comparison is much faster than page fetching. Therefore, when we\nanalyze time complexity below, we refer to the number of random array accesses.\nFinding occurrence positions and documents.\nn-gram counting with suffix arrays has a\nby-product: we also get to know all positions where the n-gram appears in the training data,\nfor free. This position information is implicitly contained in the suffix array segment we\nobtained during counting, and to retrieve the original documents where the n-gram appears,\nall we need to do is to follow each pointer within this segment back into the training data,\nand expand to both directions until hitting the document separator.\nImpact of sharding.\nWhen the suffix arrays are built on sharded byte arrays, we can\nsimply perform counting on each individual shard and accumulate the counts across all\nshards. The latency is proportional to the number of shards: time complexity would become\nO(S \u00b7 log N). The processing of different shards can be parallelized, reducing the time\ncomplexity back to O(log N).\nSpeeding up n-gram computation by re-using previous search results.\nOn the suffix\narray, the segment for x1...xn must be a sub-segment of that for x1...xn\u22121. Therefore, when\ncomputing the n-gram probability Pn(xn | x1...xn\u22121), we can first count x1...xn\u22121, and then\nwhen counting x1...xn, we only need to search for the first and last occurrence positions\nwithin the segment of x1...xn, which reduces the latency by at most 2x.\nOn-disk search.\nThe byte array and suffix array may be too large to fit into RAM, so in\npractice, we keep them on disk and read them as memory-mapped files. However, this\ncreates a significant latency as the binary search requires random access to the byte array and\nsuffix array. To mitigate this, we implemented a memory pre-fetching method that informs\nthe system of the array offsets we will likely be reading in the near future. Pre-fetching\nreduces average latency by roughly 5x.\nSpeeding up \u221e-gram computation.\nTo compute the \u221e-gram probability, we need to count\nthe occurrence of each suffix xl\u2212n+1...xl up to the maximum n so that the suffix still meets\nthe sufficient appearance requirement (we denote this maximum n as L). This means O(L)\ncounting operations, and the time complexity for each \u221e-gram computation is O(L \u00b7 log N).\nHowever, a simple binary-lifting + binary-search algorithm for searching L can reduce the\nnumber of counting operations to O(log L), and thus the time complexity for each \u221e-gram\ncomputation becomes O(log L \u00b7 log N).\nSpeeding up dense \u221e-gram computation.\nDuring evaluation, we need to compute the \u221e-\ngram probability of each token in the test document. We can save computation by observing\nthat the effective n for one token is at most one token longer than that for the previous token.\nThis brings the amortized time complexity for evaluating each token down to O(log N).\n3.4\nSupported query types and latency benchmarking\nInfini-gram supports the following types of n-gram/\u221e-gram queries:\n1. Counting an n-gram (COUNT);\n2. Computing a token probability from n-gram LM (with given n, no backoff)\n(NGRAMPROB);\n3. Computing the full next-token distribution from n-gram LM (NGRAMDIST);\n4. Computing a token probability from \u221e-gram LM (INFINIGRAMPROB);\n5. Computing the full next-token distribution from \u221e-gram LM (INFINIGRAMDIST);\n6. Returning a random document containing an n-gram, or a CNF logical expression\nof n-gram terms, connected with AND\u2019s and/or OR\u2019s (e.g., (natural language\nprocessing OR artificial intelligence) AND (deep learning OR machine\nlearning)) (GETDOCUMENT).\n6\nPreprint\nReference Data (\u2192)\nPile-train\nRPJ\nTime Complexity\nN = 0.36T N = 1.4T (measured by number\nQuery Type (\u2193)\nS = 2\nS = 8\nof random disk accesses)\n1. Counting an n-gram\nO(log N)\n... (n = 1)\n7 ms\n9 ms\n... (n = 2)\n13 ms\n20 ms\n... (n = 5)\n14 ms\n19 ms\n... (n = 10)\n13 ms\n18 ms\n... (n = 100)\n13 ms\n19 ms\n... (n = 1000)\n14 ms\n19 ms\n2. Computing a token probability from n-gram LM (n = 5)\n19 ms\n30 ms\nO(log N)\n3. Computing full next-token distribution from n-gram LM (n = 5)\n31 ms\n39 ms\nO(V \u00b7 log N)\n4. Computing a token probability from \u221e-gram LM\n90 ms\n135 ms\nO(log L \u00b7 log N)\n... on consecutive tokens\n12 ms\n20 ms\nO(log N)\n5. Computing full next-token distribution from \u221e-gram LM\n88 ms\n180 ms\nO((log L + V) \u00b7 log N)\nTable 1: Inference-time latency of infini-gram on different types of queries. Average latency\nper query is reported. Benchmarked with inference engine written in C++ (with parallelized\nshard processing) and running on a single, 80-core CPU node. Notations for time complexity:\nN = number of tokens in the reference data; S = number of shards for the suffix array; L =\nnumber of tokens in the query document; V = vocabulary size.\nSee Algorithms 1 and 2 (Appendix) for the algorithms that implement these query types.\nOur online demo fully supports the above query types.\nWe benchmark the latency of infini-gram on different types of n-gram and \u221e-gram queries,\nand show results in Table 1. During inference, the training data and the suffix array\nare stored on an SSD. For each type of query, the benchmarking is conducted on 1,000\ntokens randomly and independently sampled from Pile\u2019s validation data (except for the\ntask \u201ccomputing a token probability from \u221e-gram LM on consecutive tokens\u201d, where we\nsampled 10 documents and processed 1000 consecutive tokens in each document).\nAll types of queries demonstrate sub-second latency on the trillion-token training data.\nComputing a token probability from the \u221e-gram with RedPajama takes merely 135 millisec-\nonds. Furthermore, our implementation supports counting the occurrence of an n-gram\nwith arbitrarily large n, with roughly constant latency at 20 milliseconds (we experimentally\nvalidated up to n = 1000). Decoding requires computing the full next-token distribution\nand is thus slightly slower: 39 milliseconds per token with n-gram LMs and 180 milliseconds\nper token with \u221e-gram.\n4\nAnalyzing Human-written and Machine-generated Text using \u221e-gram\nIn this section, we present some analyses of human-written and machine-generated text\nfrom the perspective of the \u221e-gram, mostly focusing on the token-wise agreement between\n\u221e-gram and the actual text. In summary, we found that:\n1. \u221e-gram has a fairly high accuracy (47%) when predicting the next token given a\nprefix of a human-written document, and this accuracy is higher when a longer\nsuffix of the prompt can be used (i.e., when the effective n is larger);\n2. Conventional n-grams (n \u2264 5) are insufficient in capturing a long enough context\nto determine the next token, while our \u221e-gram method is highly predictive of\nhuman-written and machine-generated text;\n3. \u221e-gram has significant potential to complement and improve neural LMs when\npredicting human-written text (which we further investigate in \u00a75);\n4. When plotting the agreement level with respect to the suffix length, text generated\nby neural LMs with nucleus sampling is most similar to human-written text, among\nother decoding methods like greedy decoding and temperature sampling. For\ngreedy decoding, the agreement plot suffers from significant fluctuation, which may\nbe rooted in deficiencies in neural LM pretraining and the positional embeddings\nof Transformers.\n7\nPreprint\nFigure 4: Token-wise agreement between human-generated text and n-gram/\u221e-gram. Left:\nn-gram; Middle: \u221e-gram; Right: \u221e-gram, on tokens with sparse estimates only.\n\u221e-gram training data.\nFor analyses in this section, we use Pile\u2019s training set (Gao et al.,\n2020) as training data in the \u221e-gram, which consists of 360 billion tokens (based on the\nLLaMA tokenizer) after de-contamination.\nDecontamination.\nIt is important that the training data is decontaminated against the\nevaluation data in order to avoid test leakage. We run decontamination on Pile\u2019s training\ndata against the validation and test data of the Pile (which we will use for evaluation below\nand also in \u00a75), using the method from Groeneveld (2023) that filters out a document if there\nis too much n-gram overlap with the evaluation data. See \u00a7A.1 for more details.\nDecontamination is non-trivial, and its definition could vary, e.g., when there is an identical\nsentence, is it contamination, or is it a quote that naturally occurs in real test-time scenarios?\nWe follow standard decontamination best practices, and conduct detailed analysis in the\nlater sections.\n4.1\nHuman-written Text\nSetup.\nWe use Pile\u2019s validation data as samples of human-written text. For this analysis,\nwe sampled 50 documents from each domain of Pile\u2019s validation set, and truncated each\ndocument to up to 1024 tokens (so the total number of tokens per domain is about 50k). We\naggregate results from all domains.\nWe measure the token-wise agreement between \u221e-gram\u2019s estimate and the actual human-\nwritten text. Since computing the full next-token distribution (or the argmax of it) in \u221e-gram\nis relatively slow, we compute the \u221e-gram probability of the actual next-token, and deem it\nas accurate if this probability is higher than 0.5.2 We further categorize all tokens by their\neffective n, i.e., one plus the length of their prompt\u2019s longest suffix that has a non-zero count\nin the training data. For each category, we visualize the number of such tokens (in gray\nbars), as well as the agreement level (in green dots).\nThe visualization is shown in Figure 4 (middle plot). Overall, \u221e-gram agrees with the\nhuman-written text on 47% of the tokens. We see that \u221e-gram becomes more accurate with\nthe increase of the effective n: when the effective n \u2265 16, agreement is higher than 75%.\nFurther analysis (Appendix Figure 14) shows that the count of this longest suffix in the\ntraining data does not affect agreement significantly.\nIn the left plot of Figure 4, we show the same analysis for a 5-gram LM using the same\ntraining data, and it has much lower agreement than the \u221e-gram. 5-grams, which has been\nused extensively in previous literature (Franz & Brants, 2006; Aiden & Michel, 2011), does\nnot capture a long enough context to correctly predict the next token: over 90% tokens in\nthe evaluation data has an effective n of at least 5, and the \u221e-gram analysis shows that the\nmedian of effective n is 7 tokens (and mean is 9.1 tokens).\nIn the right plot of Figure 4, we show the same analysis for only tokens with a sparse\n\u221e-gram estimate, which covers more than 50% of all tokens. The overall agreement is even\nhigher (75%), and when the effective n \u2265 14, agreement is higher than 80%. This means\n2This is a lower-bound of argmax accuracy, though the gap is small.\n8\nPreprint\nFigure 5: Distribution of probabilities assigned by neural LMs to human-written text tokens,\nand \u221e-gram\u2019s agreement with these tokens. Upper: on all tokens; Lower: on tokens with\nsparse \u221e-gram estimates. Takeaway: \u221e-gram and neural LMs are predictive of actual\nhuman text on different tokens, and thus \u221e-gram estimates \u2013 especially sparse \u221e-gram\nestimates \u2013 can be used to complement neural LMs.\nwhen the next token is unique according to the training data, that unique token is very likely\nto be the actual token in human-written text.\n\u221e-gram can shine where neural LMs fail.\nIn Figure 5, we plot the distribution of prob-\nabilities assigned by the LLaMA-2 models (Touvron et al., 2023b) to the actual tokens in\nhuman-written text, and the human\u2013\u221e-gram agreement for tokens in each probability range.\n(The higher the assigned probability, the higher agreement LLaMA-2 has with the actual\ntokens.) We observe a positive, yet imperfect, correlation between neural LMs and \u221e-gram\nregarding their agreement with the actual text. In particular, when the neural LM perfor-\nmance is very bad (left side of the histogram), \u221e-gram still gives a non-trivial agreement of\nabove 20%; if only considering tokens with sparse \u221e-gram estimates, this is as high as 50%.\nThis implies a huge potential of complementing and improving the performance of neural\nLMs with \u221e-gram when predicting human-written text, which we further investigate in \u00a75.\n4.2\nMachine-generated Text\nSetup.\nSimilar to the analysis with human-written text, we sampled 50 documents from\neach domain of the Pile. We use the first 50 tokens of each document to prompt neural LMs\nto generate a continuation. Generation continues up to the original length of the document,\nor when an [EOS] token is generated. We experiment with three decoding methods: greedy\ndecoding, temperature sampling, and nucleus sampling (Holtzman et al., 2019). The neural\nLMs are GPT-Neo (1.3b, 2.7b), GPT-J (6b), and LLaMA-2 (7b, 13b, 70b).\nImpact of decoding method.\nThe top row of Figure 6 shows the three decoding method on\nthe same neural LM \u2013 LLaMA-2-70b. In general, increasing stochasticity shifts the effective n\nto the smaller side, and also decreases the agreement level. Nucleus sampling (with p = 0.8)\nhas the most similar distribution of effective n to human-written text (Figure 4, middle\nplot), which is probably why nucleus sampling is usually preferred in text generation. In\ncomparison, greedy decoding has even higher effective n than human-written text, which\n9\nPreprint\nLLaMA-2-70b, greedy\nLLaMA-2-70b, p=0.8\nLLaMA-2-70b, t=1.0\nLLaMA-2-13b, greedy\nLLaMA-2-7b, greedy\nGPT-J-6b, greedy\nGPT-Neo-2.7b, greedy\nGPT-Neo-1.3b, greedy\nGPT-Neo-125m, greedy\nFigure 6: Token-wise agreement between machine-generated text and \u221e-gram. All tokens\nare considered.\nimplies that greedy decoding could lead to over-memorization of training data as well as\nlack of diversity.\nOne very curious phenomenon is that, as effective n increases, the agreement level fluctuates\ngreatly in greedy decoding (but not nucleus or temperature sampling, where agreement\nlevel almost increases monotonically). Such fluctuation is even more rapid for smaller\nmodels (LLaMA-2-13b/7b, GPT-J-6b, GPT-Neo-2.7b/1.3b/125m), and for LLaMA-2-7b\nthe fluctuation is even periodic (rapidly dropping at effective n = 20, 24, 28, 32; this is\nstatistically significant, a two-proportion z-test gives a p-value of < 10\u221299). We suspect that\nthis may be caused by the application of positional embeddings when pretraining these\nTransformer-based models, and we welcome further investigation from the community.\nImpact of model size.\nThe last two rows of Figure 6 shows the same analysis for different\nsizes of neural LM under greedy decoding. In general, increasing model size slightly shifts\nthe effective n to the larger side, and also increases the agreement level. This indicates\nthat larger models memorizes more from the training data, and are also more inclined to\ncopy verbatim. The agreement level of GPT-Neo/J models is higher than LLaMA-2 models,\nprobably because GPT-Neo/J are trained on the same data as our \u221e-gram training data (i.e.,\nPile\u2019s training set). Overall, text generated by these neural LMs has similar agreement level\nwith \u221e-gram as human text.\n5\nImproving Neural LMs with the \u221e-gram\nThe results in \u00a74 motivate us to combine (interpolate; \u00a72) neural LMs and \u221e-gram to yield\nbetter language models. This section shows strong experimental results of the combined\nmodel. In \u00a74 we found that the \u221e-gram estimate has higher agreement with human-written\ntext when it is sparse. Therefore, we use two separate interpolation hyperparameters: \u03bb1 for\n10\nPreprint\nsparse and \u03bb2 for non-sparse \u221e-gram estimates. These hyperparameters are tuned on the\nvalidation set to minimize the perplexity of the combined model.\n5.1\nExperimental Setup\nEvaluation.\nWe compute perplexity on the Pile validation and test data (Gao et al., 2020).\nWe split each document in the data into batches with a maximum sequence length of 1,024\nand a sliding window of 512, a setup that is standard in prior language modeling literature\n(Baevski & Auli, 2019; Khandelwal et al., 2020).\nMetric.\nWe measure the perplexity of each model on the evaluation data, as well as the\nrelative improvement of perplexity between models. The relative improvement of model M\nagainst model Mo is computed as\n\u2206 = (1 \u2212 PPL(M) \u2212 1\nPPL(Mo) \u2212 1) \u00d7 100%\n(2)\nwhich is the percentage of perplexity gap closed towards perfect language modeling (i.e.,\nPPL = 1).\nReference data.\nTo reduce confusion, in this section, we will use \u201creference data\u201d to refer\nto the training data of the \u221e-gram. In addition to Pile\u2019s training set (Gao et al., 2020),\nwhich we used in the previous analyses (\u00a74), we also consider RedPajama (Together, 2023)\nas reference data. The Pile and the Redpajama have 360 billion and 1.4 trillion tokens,\nrespectively, summing up to 1.8 trillion tokens (based on the LLaMA tokenizer). We later\nperform ablations on varying sizes and domains of the reference data.\nNeural LMs.\nWe use a range of large, competitive neural LMs, both as a baseline and as a\nmodel to interpolate with the \u221e-gram. In total, 14 models are considered.\n\u2022 GPT-2 (Radford et al., 2019), one of the earliest autoregressive language models whose\nsizes range from 117M, 345M, and 774M to 1.6B. Their training data is a diverse set of web\ntext, although is not public.\n\u2022 GPT-Neo (Gao et al., 2020) and GPT-J (Wang & Komatsuzaki, 2021), language models\ntrained on the Pile whose sizes vary from 125M, 1.3B, and 2.7B to 6.7B.\n\u2022 LLaMA-2 (Touvron et al., 2023b), a subsequent version of LLaMA (Touvron et al., 2023a)\ntrained on two trillion tokens and has sizes of 7B, 13B, and 70B. LLaMA-2 is one of the\nmost competitive language models whose weights are available at the time of writing\nthe paper. The training data of LLaMA-2 is unknown, although the precedent version is\ntrained on a large corpus of Common Crawls, Wikipedia and code, which is replicated by\nRedPajama (Together, 2023).\n\u2022 SILO (Min et al., 2023a), 1.3B language models trained on permissively licensed data\nonly. The original paper showed that training on permissively licensed data leads to the\nchallenge of extreme domain generalization because the training data is skewed to highly\nspecific domains like code and government text. We use three different variants, PD,\nPDSW and PDSWBY, which are trained on different levels of permissivity, leading to\nvarying levels of the domain generalization challenge.\nThe perplexity of GPT-2, GPT-Neo and GPT-J are comparable to each other, but perplexity\nof LLaMA-2 and SILO are not comparable to them nor to each other, because they are based\non different tokenizers. We built three versions of the infini-gram index, one for each type\nof tokenizer.\n5.2\nResults\nExperimental results are shown in Table 2 and Table 3. Interpolating with \u221e-gram greatly\nand consistently improves the perplexity of neural LMs. The amount of improvement trends\nsmaller as the neural LM size grows, within the same model series, while the largest models\ncan still benefit a lot from our method (e.g., the Pile alone improves LLaMA-2 (70B) by 12%).\n11\nPreprint\nNeural LM\n(Size)\nRef Data\nValidation\nTest\nNeural\n+ \u221e-gram\nNeural\n+ \u221e-gram\nGPT-2\n117M\nPile\n22.82\n13.71\n(42%)\n22.86\n13.58\n(42%)\nGPT-2\n345M\nPile\n16.45\n11.22\n(34%)\n16.69\n11.18\n(35%)\nGPT-2\n774M\nPile\n15.35\n10.39\n(35%)\n15.40\n10.33\n(35%)\nGPT-2\n1.6B\nPile\n14.42\n9.93\n(33%)\n14.61\n9.93\n(34%)\nGPT-Neo\n125M\nPile\n13.50\n10.76\n(22%)\n14.08\n10.79\n(25%)\nGPT-Neo\n1.3B\nPile\n8.29\n7.31\n(13%)\n8.61\n7.36\n(16%)\nGPT-Neo\n2.7B\nPile\n7.46\n6.69\n(12%)\n7.77\n6.76\n(15%)\nGPT-J\n6.7B\nPile\n6.25\n5.75\n(10%)\n6.51\n5.85\n(12%)\nLLaMA-2\n7B\nPile\n5.69\n5.05\n(14%)\n5.83\n5.06\n(16%)\nLLaMA-2\n13B\nPile\n5.30\n4.75\n(13%)\n5.43\n4.76\n(15%)\nLLaMA-2\n70B\nPile\n4.59\n4.21\n(11%)\n4.65\n4.20\n(12%)\nLLaMA-2\n7B\nPile + RPJ\n5.69\n4.66\n(22%)\n5.83\n4.66\n(24%)\nLLaMA-2\n13B\nPile + RPJ\n5.30\n4.41\n(21%)\n5.43\n4.42\n(23%)\nLLaMA-2\n70B\nPile + RPJ\n4.59\n3.96\n(18%)\n4.65\n3.95\n(19%)\nTable 2: Perplexity (the lower the better) on the validation and the test datasets of the Pile.\nThe numbers in parentheses are percentage of improvement as computed by Equation 2.\nThe first eight rows share the tokenizer with each other, and the last six rows share the\ntokenizer with each other. Pile is Pile-train (360 billion tokens); RPJ is RedPajama (1.4 trillion\ntokens).\nNeural LM\nValidation\nTest\nNeural\n+ \u221e-gram\n+ kNN-LM\u2020 + RIC-LM\u2020 Neural\n+ \u221e-gram\n+ kNN-LM\u2020 + RIC-LM\u2020\nEval data: Wikipedia\nSilo PD\n26.60\n15.30 (43%)\n20.62\n27.91\n28.42\n14.44 (51%)\n\u2013\n\u2013\nSilo PDSW\n18.93\n12.36 (36%)\n14.10\n18.90\n20.02\n11.84 (43%)\n14.5\n19.4\nSilo PDSWBY\n10.66\n8.77 (19%)\n10.14\n10.87\n10.76\n8.41 (24%)\n\u2013\n\u2013\nPythia\n9.00\n\u2013\n8.50\n8.84\n9.1\n\u2013\n\u2013\n\u2013\nEval data: Enron Emails\nSilo PD\n19.56\n6.31 (70%)\n8.56\n15.45\n15.71\n4.85 (73%)\n\u2013\n\u2013\nSilo PDSW\n14.66\n5.58 (65%)\n6.70\n10.80\n11.23\n4.35 (66%)\n5.9\n9.9\nSilo PDSWBY\n14.67\n5.61 (65%)\n7.24\n10.91\n11.52\n4.44 (66%)\n\u2013\n\u2013\nPythia\n7.577\n\u2013\n4.99\n6.16\n6.9\n\u2013\n\u2013\n\u2013\nEval data: NIH ExPorters\nSilo PD\n27.46\n16.26 (41%)\n19.27\n25.51\n27.94\n16.00 (44%)\n\u2013\n\u2013\nSilo PDSW\n19.35\n12.70 (35%)\n14.95\n18.35\n19.12\n12.39 (37%)\n15.0\n18.5\nSilo PDSWBY\n15.01\n10.62 (30%)\n12.33\n14.29\n14.81\n10.33 (32%)\n\u2013\n\u2013\nPythia\n11.20\n\u2013\n11.20\n10.83\n11.1\n\u2013\n\u2013\n\u2013\nTable 3: Perplexity (the lower the better) on the validation and the test datasets of the\nWikipedia, Enron Emails, and NIH ExPorters of the Pile. All neural models are 1.3B models,\nand the reference data is always the Pile. \u25a0 indicates in-domain; \u25a0 indicates out-of-domain;\n\u25a0 indicates out-of-domain but has relevant data in-domain, all with respect to the training\ndata of the neural LM. \u2020: Results retrived from Min et al. (2023a), which use much smaller\nreference data: 45-million to 1.2-billion tokens, compared to our 360-billion tokens.\nHowever, this trend does not hold across different series of LMs. For example, \u221e-gram can\nimprove GPT-2 (1.6B) by 34%, but only improves a smaller model, GPT-Neo (1.3B), by 16%.\nThis may be attributed to the fact that the GPT-Neo/J models are trained on the Pile, while\nthe GPT-2 models are not. \u221e-gram works best when the reference data distribution differs\nfrom, or complements, the pretraining data distribution, which emphasizes the importance\nof data diversity. Meanwhile, the fact that \u221e-gram also improves neural LMs already\npretrained on its reference data shows that the advantage of using \u221e-gram is consistent.\n12\nPreprint\nFigure 7: Impact of scaling the datastore of the \u221e-gram, all using the LLaMA-2 models\n(7B, 13B, and 70B) as neural LMs, and the Pile as the reference data. - - -: neural LM only\n(baseline). \u2022: \u221e-gram uses the full Pile; \u25e6: \u221e-gram uses only the in-domain portion of the\nPile. Gains increase consistently as the datastore scales.\nOn the choice of \u221e-gram reference data, the combination of the Pile and RedPajama yields\nlarger improvements on the LLaMA-2 models than the Pile alone. The interpolation of\nLLaMA-2 (13B) and \u221e-gram with Pile + RPJ outperforms LLaMA-2 (70B), and interpolating\nwith \u221e-gram pushes the perplexity of LLaMA-2 (70B) below 4.0.\nWhen the neural LM is SILO (which is trained on permissive-licensed data only and thus has\nless training data), adding the \u221e-gram component is more helpful when SILO is trained on\nmore restrictive data (i.e., PD > PDSW > PDSWBY). The usage of \u221e-gram can be precisely\ntraced back to the contributing document(s) in the reference data, which is in-line with\nthe philosophy of SILO: to allow crediting the source data when using them for language\nmodeling. When compared to the existing retrieval-augmentation methods used by SILO,\ni.e., kNN-LM and RIC-LM, \u221e-gram yields better improvement in perplexity. Therefore,\n\u221e-gram can serve as a better alternative as the retrieval-augmentation method for SILO.\n5.3\nAblations\nEffect of the size of the reference data.\nFigure 7 reports the performance of the combined\nmodel wrt the size of the reference data. To create progressively smaller reference data,\nwe repeatedly subsampled the full reference data by 2x (up to 256x, resulting in 9 sizes).\nWe see the improvement brought by \u221e-gram widens as reference data size grows, and\nthe relationship is roughly log-linear (except for the NIH ExPorter domain, where \u221e-gram\ndoesn\u2019t help when the reference data is too small).\nEffect of the domain of the reference data.\nFigure 7 compares the performance of the\ncombined model where the \u221e-gram uses either the full reference data or the in-domain\nreference data. Using only the in-domain reference data is roughly as powerful as using\nthe full reference data, which implies that almost all improvement we have witnessed is\nthanks to in-domain data (where contaminations have been eliminated). This means it\nwould not hurt to use the full reference data, especially when the test domain is unknown\nor an in-domain reference data is unavailable; however, having in-domain reference data is\nmost helpful.\n5.4\nEvaluating on Time-Shifted Data\nTo further show the effectiveness of \u221e-gram and eliminate doubts that our performance\ngains might be due to insufficient decontamination, we evaluate on time-shifted data:\ndocuments that were created after the cutoff time of the \u221e-gram reference data. We use new\nWikipedia articles created during April and August, 2023, which is after the cutoff time of\nboth Pile and RedPajama.\nTable 4 reports the perplexity of neural LM as well as the combined model. On documents\nin four out of the five months, interpolating with \u221e-gram improves the perplexity of the\nneural LM. We find that this improvement can be further boosted by applying a Random\nForest to decide an instance-wise interpolation hyperparameter, where the features of the\n13\nPreprint\nEval Data\n(Wikipedia)\nsimple interpolation w/ Random Forest\nNeural\n+ \u221e-gram\nNeural + \u221e-gram\nApril 2023\n5.64 5.48 (3%)\n5.86 4.89 (20%)\nMay 2023\n5.43 5.27 (4%)\n6.01 5.70 ( 6%)\nJune 2023\n5.49 5.21 (6%)\n5.69 4.87 (17%)\nJuly 2023\n4.93 4.93 (0%)\n4.91 4.78 ( 3%)\nAugust 2023\n4.64 4.46 (5%)\n4.81 4.50 ( 8%)\nTable 4: Evaluation on time-shifted data. The evaluation data is taken from newly-added\nWikipedia articles since April 2023, which is after the creation of both the Pile and RedPajama.\nThe neural model is LLaMA-2 (13B), and the \u221e-gram reference data is Pile + RPJ.\nRandom Forest are the suffix lengths (1 up to the effective n) as well as the frequency of each\nsuffix in the reference data. When Random Forest is applied, the perplexity improvement\nranges from 3% \u2013 20%.\n5.5\nA note on text generation\nWhile \u221e-gram can be interpolated with neural LMs and greatly improve their perplexity, our\npreliminary experiments show that such method might not be helpful, and even harmful,\nto open-ended text generation tasks. During generation, \u221e-gram can make odd mistakes\n(e.g., retrieving very irrelevant tokens) which makes the model to completely fail. Thus this\ncombined model is not ready to replace neural LMs. Additional investigation is required to\nmake \u221e-gram best contribute to text generation (e.g., adaptively routing between \u221e-gram\nand neural LMs).\n6\nDiscussion and Broader Implications\nIn \u00a74 and \u00a75, we showcased some very preliminary use cases of the infini-gram engine. How-\never, we believe that infini-gram can enable much broader investigations and applications,\nincluding but not limited to:\nUnderstanding text corpora.\nText corpora used for pretraining language models have\nbecome prohibitively large, and we have relatively limited understanding of their contents\n(Elazar et al., 2023). Infini-gram can be a useful tool to quickly find out what is in the corpus\nand what is not, using n-gram lookup (the COUNT query).\nData curation.\nData engineers often want to remove problematic content in corpora\nscraped from the Internet, such as toxicity, hate speech, and personal identifiable information\n(PII). Using infini-gram\u2019s GETDOCUMENT query (which can be easily modified to return all\ndocuments), one can retrieve all documents containing an n-gram term (or a CNF expression\nwith multiple n-gram terms) and remove them from the corpus. Removal can even be done\niteratively: infini-gram indexes are additive/subtractive, so we can obtain an index of the\ncorpus after round one removal, by indexing the removed set and take the difference of the\noriginal index and the removal index.\nAttribution.\nWhen using neural LMs to make predictions, people might want to know\nwhich training data most influenced the model\u2019s decision. Using n-gram lookup with key\nphrases, we can trace back to related documents in the training data of neural LMs.\nDetecting data contamination, memorization, and plagiarism.\nTest set contamination\nhas become a major issue for language model evaluation. n-gram lookup enables us to\ncheck if evaluation queries have sneaked into the training data of neural LMs. It also\nopens up possibility to detect memorization in machine-generated text, or plagiarism in\nhuman-written text.\nMitigating copyright infringement.\nRecently, generative AIs are facing numerous law-\nsuits for generating arguably copyrighted materials. Infini-gram may be able to mitigate\n14\nPreprint\nMethod\n# tokens (\u2191) # entries (\u2191)\nStorage usage (\u2193)\nmax n\nVector-based index\nRETRO (Borgeaud et al., 2022)\n1.8 T\n2.8 \u00d7 1010\n432 TB (16k bytes / entry)\n\u2013\nAtlas (Izacard et al., 2022)\n27 B\n4 \u00d7 108\n200 GB (8 bytes / entry)\n\u2013\nkNN-LM (Khandelwal et al., 2020)\n3 B\n3 \u00d7 109\n200 GB (64 bytes / entry)\n\u2013\nNPM (Min et al., 2023b)\n1 B\n1 \u00d7 109\n1.4 TB (\u223c 2k bytes / entry)\n\u2013\nn-gram-based index\nGoogle\u2019s (Franz & Brants, 2006)\n1 T\n3.8 \u00d7 109\n24 GB\n5\nGoogle Books Ngram (Aiden & Michel, 2011)\n500 B\nunreported\nunreported\n5\nStehouwer & van Zaanen (2010)\n90 M\nunreported\nunreported\n\u221e\nKennington et al. (2012)\n3 M\n5 \u00d7 1012\n330 MiB (110 bytes / token)\n\u221e\nShareghi et al. (2015)\n9 B\n8 \u00d7 1018\n63 GiB (7 bytes / token)\n\u221e\ninfini-gram (ours)\n1.8 T\n1.6 \u00d7 1024\n12 TiB (7 bytes / token)\n\u221e\nTable 5: Comparison with other nonparametric language modeling methods. # tokens:\nnumber of tokens in the inference-time reference data. # entries: number of representations\n(counts) in the index. max n: maximum number of context tokens considered. For infini-\ngram, we consider the combination of Pile-train and RedPajama as reference data.\ncopyright infringement, by diverting neural LMs to alternative (yet still plausible) genera-\ntion paths when they are about to generate long n-grams that appear in the training data,\nespecially if they mostly appear in documents from copyrighted sources.\nReducing hallucination in factual knowledge.\nParametric-only models are prone to\ngenerating non-factual statements, which is widely known as the hallucination problem.\nInfini-gram can potentially be used to mitigate hallucination by reading verbatim from the\ntraining data. We have found evidence that the \u221e-gram can greatly outperform LLaMA-2-\n70B on factual probing benchmarks such as LAMA (Petroni et al., 2019).\nNon-parametric speculative decoding.\nSpeculative decoding (Chen et al., 2023) speeds up\ntext generation by employing a fast and a slow decoder, where the fast decoder is a smaller\nmodel that does the autoregressive token generation, and the slow decoder checks the fast\ndecoder\u2019s proposals by parallelizing the forward passes of multiple tokens. Given the low\nlatency of infini-gram, we can potentially use \u221e-gram as the fast decoder, similar to He et al.\n(2023).\nOffloading rote memorization from neural models.\nFully-parametric language models\nneeds to internalize a huge amount of factual knowledge into their parameters. We can po-\ntentially offload such rota memorization from neural models into non-parametric modules,\nfor example, by training neural LMs to fit the residual of \u221e-gram from the ground truth (Li\net al., 2022).\nWe welcome the community to collaboratively build toward the aforementioned directions,\nby leveraging the open-sourced tools provided in infini-gram.\n7\nRelated Work\nn-gram language models.\nn-gram has been one of the most classical language modeling\nmethods since the inception of natural language processing (Jurafsky & Martin, 2000).\nPeople have been pushing the limits of n-gram LMs by scaling up its training data, and to\ndate, the largest n-gram table (Franz & Brants, 2006) indexes 1 trillion tokens for 5-grams\nthat appears at least 40 times.\nWhile n-gram LMs are currently largely surpassed by neural LMs, there has been recent\nwork that revisit n-grams and n-gram LMs. Khandelwal et al. (2020) interpolates neural\nLMs with the n-gram model but finds it not improving performance. In contrast, Li et al.\n(2022) finds that the n-gram model is as competitive as a small neural LM, and training a\nneural model to be complementary to the n-gram model and using both at inference time\noutperforms the neural-only LM. However, both use limited reference data (101M tokens)\n15\nPreprint\nand compare with small neural LMs (117\u2013250M parameters). Some prior work has found\nvalue in scaling up the n-gram training data (Allamanis & Sutton, 2013).\nOur \u221e-gram LM does not use a fixed value of n. We significantly scale up the training\ndata to more than a trillion tokens and interpolate it with the neural model, significantly\noutperforming the state-of-the-art neural models that consist of up to 70B parameters.\nUnbounded n-grams, suffix arrays, suffix trees.\nPrevious work has explored using suffix-\nbased data structures to enable n-gram queries with unbounded n, with limited scale of the\ntraining data. Stehouwer & van Zaanen (2010) proposes to use suffix arrays for \u221e-gram,\nand yet their formulation does not yield proper probability distributions and, consequently,\na language model. Kennington et al. (2012) proposes to use suffix trees for the same purpose,\nand yet the storage overhead of suffix trees is very high such that it hinders scaling, which\nmay be mitigated with highly intricate compression techniques (Shareghi et al., 2015).\nAmong the three aforementioned papers, only the third evaluates on the general language\nmodeling task, and the perplexity numbers are too high to be practically useful. We compare\nthe scale of our \u221e-gram index with these papers in Table 5; our training data is 200x larger\nthan the largest one used in previous work.\nOther data structures for text indexing.\nBeside suffix arrays and suffix trees, other data\nstructures have been used to index text corpora to satisfy different trade-offs. The ROOTS\nSearch Tool (Piktus et al., 2023) builds a BM25 index on the ROOTS corpus, and supports\ndocument searching via both exact match and fuzzy match of n-grams. Data Portraits\n(Marone & Durme, 2023) proposes a lightweight index based on Bloom Filter, and is tailored\nfor probabilistic membership inference (exact match of n-grams of 50 characters, where\nn \u2248 8) against the Pile and Stacks. ElasticSearch is a proprietary search engine based on\nthe Lucene index, and it has been used by Dodge et al. (2021) to search documents in C4,\nand also by Elazar et al. (2023) to count n-grams and list most frequent n-grams in various\ncorpora up to 480B tokens.\nNonparametric language models.\nA nonparametric LM refers to the LM whose complexity\nis not bounded as a priori, because the complexity can grow or update according to the data\ngiven at inference time. Prior work is broadly divided into two categories: a token retrieval\napproach that represents each token as one vector and uses a nonparametric prediction\nfunction (Khandelwal et al., 2020; Zhong et al., 2022; Lan et al., 2023; Min et al., 2023b;a; Shi\net al., 2023), and a chunk retrieval approach that represents each chunk of text as a vector\nand incorporates nearest chunks to the neural language model (Guu et al., 2020; Izacard\net al., 2022; Borgeaud et al., 2022). Scaling the reference data in nonparametric LMs is very\nexpensive as it requires storing a vector for every unit (either token or chunk). To the best of\nour knowledge, prior work with the largest reference data is RETRO (Borgeaud et al., 2022),\nwhich uses the 7B-parameter LM and the reference data consisting of 1.8 trillion tokens.\nIt stores and searches over 28 billion vectors, estimated to consume 432TB of disk space.3\n(Detailed comparisons in Table 5.)\nOur \u221e-gram LM is one instance of nonparametric LMs, and its simplicity makes it possible\nto significantly scale the reference data with modest resources (\u00a73.1). To the best of our\nknowledge, our \u221e-gram LM is the largest in both the size of the reference data size (1.8\ntrillion tokens when counting the union of Pile-train and RedPajama) and the size of the\nbase neural LM (70B).\n8\nConclusion and Future Work\nIn this paper, we modernized the classical n-gram language model by scaling it up to a\ntrillion tokens and extending to unbounded n. We presented the infini-gram engine that\nperforms efficient training and inference under this extreme setup. We also proposed the\n\u221e-gram language model, powered by the infini-gram engine, and showed that it can offer\n3This is in part because RETRO does not use any approximation in kNN search. Even if RETRO\nused approximate search as Khandelwal et al. (2020) did, it would still use 10TB. Moreover, there is no\nopen-sourced software that easily supports fast kNN search over tens of billions of vectors.\n16\nPreprint\nnovel insights into human-written and machine-generated text and can improve existing\nneural language models. We look forward to seeing more insightful analyses and creative\nusages powered by infini-gram.\n17\nPreprint\nAcknowledgments\nWe would like to thank Zexuan Zhong, Mike Lewis, Yanai Elazar, Will Merrill, Tim Dettmers,\nXiming Lu, Alisa Liu, Weijia Shi, Xiaochuang Han, members of the H2lab, and Ziqi Ma\nfor their invaluable feedback. This work was funded in part by the DARPA MCS program\nthrough NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, NSF DMS-2134012, and ONR\nN00014-18-1-2826.\nReferences\nErez Lieberman Aiden and Jean-Baptiste Michel. Quantitative analysis of culture using mil-\nlions of digitized books. Science, 331:176 \u2013 182, 2011. URL https://api.semanticscholar.\norg/CorpusID:40104730.\nMiltiadis Allamanis and Charles Sutton. Mining source code repositories at massive scale\nusing language modeling. 2013 10th Working Conference on Mining Software Repositories\n(MSR), pp. 207\u2013216, 2013. URL https://api.semanticscholar.org/CorpusID:1857729.\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language\nmodeling. In Proceedings of the International Conference on Learning Representations, 2019.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\nMillican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan\nClark, et al. Improving language models by retrieving from trillions of tokens. In\nProceedings of the International Conference of Machine Learning, 2022.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, L. Sifre, and\nJohn M. Jumper. Accelerating large language model decoding with speculative sam-\npling. ArXiv, abs/2302.01318, 2023. URL https://api.semanticscholar.org/CorpusID:\n256503945.\nJesse Dodge, Ana Marasovic, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and\nMatt Gardner. Documenting large webtext corpora: A case study on the colossal clean\ncrawled corpus. In Conference on Empirical Methods in Natural Language Processing, 2021.\nURL https://api.semanticscholar.org/CorpusID:237568724.\nYanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk,\nAlane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi,\nNoah A. Smith, and Jesse Dodge. What\u2019s in my big data? ArXiv, abs/2310.20707, 2023.\nURL https://api.semanticscholar.org/CorpusID:264803575.\nAlex Franz and Thorsten Brants.\nAll our n-gram are belong to you.\nGoogle\nMachine Translation Team, 20, 2006.\nURL https://blog.research.google/2006/08/\nall-our-n-gram-are-belong-to-you.html.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of\ndiverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\nXinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL\nhttps://github.com/openlm-research/open llama.\nDirk Groeneveld. The big friendly filter. https://github.com/allenai/bff, 2023.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval\naugmented language model pre-training. In Proceedings of the International Conference of\nMachine Learning, 2020.\nZhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. Rest: Retrieval-based spec-\nulative decoding. 2023. URL https://api.semanticscholar.org/CorpusID:265157884.\n18\nPreprint\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural\ntext degeneration. ArXiv, abs/1904.09751, 2019. URL https://api.semanticscholar.\norg/CorpusID:127986954.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick,\nJane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave.\nFew-shot\nlearning with retrieval augmented language models. arXiv preprint arXiv:2208.03299,\n2022.\nDan Jurafsky and James H. Martin. Speech and language processing - an introduction to\nnatural language processing, computational linguistics, and speech recognition. In Prentice\nHall series in artificial intelligence, 2000. URL https://api.semanticscholar.org/CorpusID:\n60691216.\nJuha K\u00a8arkk\u00a8ainen, Peter Sanders, and Stefan Burkhardt. Linear work suffix array construction.\nJ. ACM, 53:918\u2013936, 2006. URL https://api.semanticscholar.org/CorpusID:12825385.\nSlava M. Katz. Estimation of probabilities from sparse data for the language model compo-\nnent of a speech recognizer. IEEE Trans. Acoust. Speech Signal Process., 35:400\u2013401, 1987.\nURL https://api.semanticscholar.org/CorpusID:6555412.\nCasey Redd Kennington, Martin Kay, and Annemarie Friedrich. Suffix trees as language\nmodels. In International Conference on Language Resources and Evaluation, 2012. URL\nhttps://api.semanticscholar.org/CorpusID:12071964.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Gener-\nalization through memorization: Nearest neighbor language models. In Proceedings of the\nInternational Conference on Learning Representations, 2020.\nTian Lan, Deng Cai, Yan Wang, Heyan Huang, and Xian-Ling Mao. Copy is all you need. In\nProceedings of the International Conference on Learning Representations, 2023.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris\nCallison-Burch, and Nicholas Carlini. Deduplicating training data makes language models\nbetter. In Proceedings of the Association for Computational Linguistics, 2022.\nHuayang Li, Deng Cai, Jin Xu, and Taro Watanabe. Residual learning of neural text genera-\ntion with n-gram language model. In Findings of the Association for Computational Linguis-\ntics: EMNLP 2022, 2022. URL https://aclanthology.org/2022.findings-emnlp.109.\nMarc Marone and Benjamin Van Durme. Data portraits: Recording foundation model\ntraining data. ArXiv, abs/2303.03919, 2023. URL https://api.semanticscholar.org/\nCorpusID:257378087.\nSewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah Smith, and Luke\nZettlemoyer. SILO language models: Isolating legal risk in a nonparametric datastore.\narXiv preprint arXiv:2308.04430, 2023a. URL https://arxiv.org/abs/2308.04430.\nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. Nonparametric masked language modeling. In Findings of ACL, 2023b.\nFabio Petroni, Tim Rockt\u00a8aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexan-\nder H. Miller, and Sebastian Riedel. Language models as knowledge bases?\nArXiv,\nabs/1909.01066, 2019. URL https://api.semanticscholar.org/CorpusID:202539551.\nAleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo Laurenccon, G\u00b4erard Dupont,\nAlexandra Sasha Luccioni, Yacine Jernite, and Anna Rogers. The roots search tool: Data\ntransparency for llms. In Annual Meeting of the Association for Computational Linguistics,\n2023. URL https://api.semanticscholar.org/CorpusID:257219882.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n19\nPreprint\nEhsan Shareghi, Matthias Petri, Gholamreza Haffari, and Trevor Cohn. Compact, effi-\ncient and unlimited capacity: Language modeling with compressed suffix trees. In\nConference on Empirical Methods in Natural Language Processing, 2015.\nURL https:\n//api.semanticscholar.org/CorpusID:225428.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke\nZettlemoyer, and Wen-tau Yih. REPLUG: Retrieval-augmented black-box language mod-\nels. arXiv preprint arXiv:2301.12652, 2023.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell\nAuthur, Khyathi Chandu, Jennifer Dumas, Li Lucy, Xinxi Lyu, Ian Magnusson, Aakanksha\nNaik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Zejiang Shen, Emma\nStrubell, Nishant Subramani, Oyvind Tafjord, Evan Pete Walsh, Hannaneh Hajishirzi,\nNoah A. Smith, Luke Zettlemoyer, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo.\nDolma: An Open Corpus of 3 Trillion Tokens for Language Model Pretraining Research.\nTechnical report, Allen Institute for AI, 2023. Released under ImpACT License as Medium\nRisk artifact, https://github.com/allenai/dolma.\nHerman Stehouwer and Menno van Zaanen. Using suffix arrays as language models:\nScaling the n-gram. 2010. URL https://api.semanticscholar.org/CorpusID:18379946.\nTogether. RedPajama: An open source recipe to reproduce LLaMA training dataset, 2023.\nURL https://github.com/togethercomputer/RedPajama-Data.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nZexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmen-\ntation. In Proceedings of Empirical Methods in Natural Language Processing, 2022.\n20\nPreprint\nA\nAdditional Details on Method\nA.1\nDe-contamination of Reference Data\nTo properly evaluate the effectiveness of \u221e-gram LM on Pile\u2019s evaluation sets, we performed\ndata de-contamination on the Pile\u2019s training set and RedPajama before using them as\nreference data for the \u221e-gram LM. We run the Big Friendly Filter (BFF)4 (Groeneveld, 2023)\non Pile\u2019s training set and RedPajama, filtering out documents with too much n-gram overlap\nwith Pile\u2019s evaluation sets. Table 6 reports the statistics of de-contamination.\nWhen using BFF, we always remove whole documents, instead of by paragraphs. Following\nthe default settings, we consider n-grams where n = 13, and discard the document if at least\n80% of its n-grams are present in the evaluation set. For Pile\u2019s training set, we lowercase all\ndocuments to capture more potential contaminations.\nREDPAJAMA\nSubset\nTotal docs\nFiltered docs\nRatio filtered\narxiv\n1558306\n213\n0.01%\nbook\n205744\n711\n0.3%\nc4\n364868892\n53195\n0.01%\ncommon crawl\n476276019\n0\n0%\ngithub\n28793312\n614259\n2%\nstackexchange\n29825086\n40086\n0.01%\nwikipedia\n29834171\n21973\n0.07%\nTotal\n931361530\n730437\n0.08%\nPILE (TRAIN)\nSubset\nTotal docs\nFiltered docs\nRatio filtered\nArxiv\n2377741\n1089\nBookCorpus2\n25355\n6\nBooks3\n277655\n99\nDM Mathematics\n1918535\n0\n0%\nEnron Emails\n926132\n18236\n2%\nEuroParl\n131723\n21\nFreeLaw\n5069088\n11821\n0.2%\nGithub\n18044218\n961726\n5.3%\nGutenberg (PG-19)\n66981\n70\n0.1%\nHackerNews\n1571968\n14\nNIH ExPorter\n1777926\n3739\n0.2%\nOpenSubtitles\n632485\n5754\n0.9%\nOpenWebText2\n32333654\n136914\n0.4%\nPhilPapers\n63875\n2324\n0.4%\nPile-CC\n52441354\n19928\nPubMed Abstracts\n29329202\n2312\nPubMed Central\n5679903\n4230\n0.1%\nStackExchange\n29529008\n2072\nUSPTO Backgrounds\n11123325\n80088\n0.7%\nUbuntu IRC\n20067\n10\nWikipedia (en)\n16939503\n45052\n0.3%\nYoutubeSubtitles\n328030\n871\n0.3%\nTotal\n210607728\n1296376\n0.6%\nTable 6: Statistics of de-contamination in RedPajama (left) and Pile\u2019s training set (right).\nA.2\nInference Algorithms\nAlgorithms 1 and 2 shows the implementation of n-gram/\u221e-gram queries listed in \u00a73.4.\nA.3\nExample Queries\nFigures 8 to 13 show one example for each of the six query types supported by infini-gram.\nWe used Pile-train and the GPT-2 tokenizer throughout these examples. Screenshots are\ntaken from our demo.\nB\nAdditional Results\nFigure 14\n4https://github.com/allenai/bff\n21\nPreprint\nAlgorithm 1 n-gram/\u221e-gram queries\nData reference data byte array data with 2N bytes (N tokens), suffix array sa with P \u00b7 N bytes (N\nelements, P is the pointer size), both memory-mapped\nprocedure PREFETCH(B, l, r, depth = 0)\nm \u2190 \u230a(l + r)/2\u230b\nif m == \u22121 then return\nif depth == 1 then\np \u2190 READPTR(sa, m, P)\nRead disk page containing bytes data[p : p + B]\nelse if depth == 3 then\nRead disk page containing bytes sa[P \u00b7 m : P \u00b7 (m + 1)]\nreturn\nPREFETCH(B, l, m, depth + 1)\nPREFETCH(B, m, r, depth + 1)\nprocedure FIND(promptIds, hint = NULL)\nif len(promptIds) == 0 then\nreturn (0, N)\npromptBu f \u2190 ENCODE(promptIds)\n\u25b7 little-endian byte array\nB \u2190 promptBu f.numbytes\nif hint == NULL then\nl \u2190 \u22121, r \u2190 N\nelse\nl \u2190 hint[0] \u2212 1, r \u2190 hint[1]\nwhile r \u2212 l > 1 do\nPREFETCH(B, l, r)\nm \u2190 \u230a(l + r)/2\u230b\np \u2190 READPTR(sa, m, P)\nif ds[p : p + B] < promptBu f then\nl \u2190 m\nelse\nr \u2190 m\nstart \u2190 r\nif hint == NULL then\nl \u2190 \u22121, r \u2190 N\nelse\nl \u2190 hint[0] \u2212 1, r \u2190 hint[1]\nwhile r \u2212 l > 1 do\nPREFETCH(B, l, r)\nm \u2190 \u230a(l + r)/2\u230b\np \u2190 READPTR(sa, m, P)\nif ds[p : p + B] \u2264 promptBu f then\nl \u2190 m\nelse\nr \u2190 m\nend \u2190 r\nreturn (start, end)\nprocedure COUNT(promptIds)\n(start, end) \u2190 FIND(promptIds)\ncount \u2190 end \u2212 start\nreturn count\nprocedure NGRAMPROB(promptIds, tokenId, hint = NULL)\nif hint == NULL then\nhint \u2190 FIND(promptIds)\n(start, end) \u2190 hint\ncountprompt \u2190 end \u2212 start\nif countprompt == 0 then\nreturn NaN\n(start, end) \u2190 FIND(promptId + [tokenId], hint = (start, end))\ncount f ull \u2190 end \u2212 start\nprob = count f ull/countprompt\nreturn prob\n22\nPreprint\nAlgorithm 2 n-gram/\u221e-gram queries (continued)\nprocedure NGRAMDIST(promptIds, hint = NULL, startTokenId = NULL, endTokenId = NULL)\nif hint == NULL then\nhint \u2190 FIND(promptIds)\npromptBu f \u2190 ENCODE(promptIds)\nB \u2190 promptBu f.numbytes\n\u25b7 this can be pre-computed\nfreqByTokenId \u2190 {}\n\u25b7 a map from token id to frequency\n(start, end) \u2190 hint\nif end \u2212 start \u2264 4 then\n\u25b7 the threshold for this base case is heuristically determined\nfor rank = start...end \u2212 1 do\np \u2190 READPTR(sa, rank, P)\nif p + B < 2N then\ntokenId \u2190 READTOKENID(ds, p + B)\nfreqByTokenId[tokenId] \u2190 f reqByTokenId[tokenId] + 1\nelse\nif startTokenId == NULL then\np \u2190 READPTR(sa, start, P)\nstartTokenId \u2190 READTOKENID(ds, p + B)\nif endTokenId == NULL then\np \u2190 READPTR(sa, end \u2212 1, P)\nendTokenId \u2190 READTOKENID(ds, p + B)\nif startTokenId == endTokenId then\n\u25b7 if start and end has the same next-token, then this\nentire segment must have the same next-token\nfreqByTokenId[startTokenId] \u2190 end \u2212 start\nelse\n\u25b7 divide and conquer\nrank \u2190 \u230a(start \u2212 end)/2\u230b\nfreqByTokenIdLe f t \u2190 NGRAMDIST(promptIds, hint = (start, rank), startTokenId =\nstartTokenId)\nfreqByTokenIdRight \u2190 NGRAMDIST(promptIds, hint = (rank, end), endTokenId =\nendTokenId)\nfreqByTokenId \u2190 MERGEDICTSBYSUM( f reqByTokenIdLe f t, f reqByTokenIdRight)\nprobByTokenId \u2190 NORMALIZE( f reqByTokenId)\nreturn probByTokenId\nprocedure INFINIGRAMPROB(promptIds, tokenId)\nL \u2190 promptIds.length\nprob \u2190 NaN\n\u25b7 this is a placeholder, because \u221e-gram can always fallback to n = 1 and give a\nvalid probability\nfor cuto f f = L...0 do\n\u25b7 this can be speed up by binary-lifting + binary-search\n(start, end) \u2190 FIND(promptIds[cuto f f :])\nif end \u2212 start == 0 then\nreturn prob\nprob \u2190 NGRAMPROB(promptIds[cuto f f :], tokenId, hint = (start, end))\nreturn prob\nprocedure INFINIGRAMDIST(promptIds)\nL \u2190 promptIds.length\nbestCuto f f \u2190 L\nfor cuto f f = L...0 do\n(start, end) \u2190 FIND(promptIds[cuto f f :])\nif end \u2212 start == 0 then\nbreak\nbestCuto f f \u2190 cuto f f\nreturn NGRAMDIST(promptIds[bestCuto f f :])\n23\nPreprint\nFigure 8: Example for query type 1: Counting an n-gram (COUNT).\nFigure 9: Example for query type 2: Computing a token probability from n-gram LM (with\ngiven n, no backoff) (NGRAMPROB).\nFigure 10: Example for query type 3: Computing the full next-token distribution from\nn-gram LM (NGRAMDIST). Due to space limits, only top-10 tokens are shown.\n24\nPreprint\nFigure 11: Example for query type 4: Computing a token probability from \u221e-gram LM\n(INFINIGRAMPROB).\nFigure 12: Example for query type 5: Computing the full next-token distribution from\n\u221e-gram LM (INFINIGRAMDIST). Due to space limits, only top-10 tokens are shown.\n25\nPreprint\nFigure 13: Example for query type 6: Returning one document containing an n-gram, or a\nCNF logical expression of n-gram terms, connected with AND\u2019s and/or OR\u2019s.\nFigure 14: Token-wise agreement between human-generated text and \u221e-gram, broken down\nby \u201ceffective n\u201d and frequency of the corresponding longest suffix in the reference data. The\nheight of each bar represents token count, and the color represents agreement (red is 0.0,\ngreen is 1.0).\n26\n"
  },
  {
    "title": "LongAlign: A Recipe for Long Context Alignment of Large Language Models",
    "link": "https://arxiv.org/pdf/2401.18058.pdf",
    "upvote": "21",
    "text": "LongAlign: A Recipe for Long Context Alignment\nof Large Language Models\nYushi Bai\u2021\u2020, Xin Lv\u00a7, Jiajie Zhang\u2021\u2020, Yuze He\u2021, Ji Qi\u2021\u2020,\nLei Hou\u2021, Jie Tang\u2021, Yuxiao Dong\u2021, Juanzi Li\u2021\n\u2021Tsinghua University\n\u00a7Zhipu.AI\nAbstract\nExtending large language models to effectively\nhandle long contexts requires instruction fine-\ntuning on input sequences of similar length. To\naddress this, we present LongAlign\u2014a recipe\nof the instruction data, training, and evalua-\ntion for long context alignment. First, we con-\nstruct a long instruction-following dataset us-\ning Self-Instruct. To ensure the data diversity,\nit covers a broad range of tasks from various\nlong context sources. Second, we adopt the\npacking and sorted batching strategies to speed\nup supervised fine-tuning on data with varied\nlength distributions. Additionally, we develop\na loss weighting method to balance the contri-\nbution to the loss across different sequences\nduring packing training. Third, we introduce\nthe LongBench-Chat benchmark for evaluating\ninstruction-following capabilities on queries of\n10k-100k in length. Experiments show that\nLongAlign outperforms existing recipes for\nLLMs in long context tasks by up to 30%, while\nalso maintaining their proficiency in handling\nshort, generic tasks. The code, data, and long-\naligned models are open-sourced at https:\n//github.com/THUDM/LongAlign.\n1\nIntroduction\nLarge language models (LLMs) with large context\nwindows facilitate tasks such as summarization,\nquestion answering on long text and code (Bai et al.,\n2023a). Importantly, they may form the founda-\ntional support for life-long conversations and com-\nplex agent scenarios (Xiao et al., 2023; Liu et al.,\n2023). Existing works to build long-context LLMs\npredominantly focus on context extension (Chen\net al., 2023a; Xiong et al., 2023; Peng et al., 2023),\nthat is, position encoding extension and continual\ntraining on long text.\nIn this work, we instead focus on the perspec-\ntive of long context alignment, i.e., instruction\nfine-tuning LLMs to handle long user prompts.\n\u2020Work done when YB, JZ, and JQ interned at Zhipu.AI.\nFigure 1: Test results on LongBench-Chat, which con-\ntains real-world queries of 10k-100k in length1.\nHowever, several challenges are required to ad-\ndress. First, there is an absence of long instruction-\nfollowing datasets for supervised fine-tuning (SFT),\nand by extension the lack of methods for construct-\ning such data. Second, the varied length distribu-\ntion of long-context data drastically reduces the\ntraining efficiency of traditional batching methods\nin a multi-GPU setup, as GPUs processing shorter\ninputs have to stay idle until those handling longer\ninputs complete their tasks. Third, there is a crucial\nneed for a robust benchmark to evaluate LLMs\u2019\nlong-context capacities against real-world queries.\nTo address them, we present the LongAlign\nrecipe, covering data, efficient training, and eval-\nuation, respectively. Data-wise, to construct a di-\nverse long instruction-following dataset, we collect\nlong sequences from nine sources and use Self-\nInstruct (Wang et al., 2022) to generate 10k instruc-\ntion data of 8k-64k length.\nTraining-wise, to address the inefficiency un-\nder uneven batching, we adopt the packing strat-\negy (Krell et al., 2021) that packs sequences to-\n1LongAlign-6B-64k, LongAlign-7B-64k and LongAlign-\n13B-64k are trained based on ChatGLM3-6B, Llama-2-7B\nand Llama-2-13B, respectively.\n1\narXiv:2401.18058v1  [cs.CL]  31 Jan 2024\ngether up to the maximum length before dispatch-\ning them to GPUs. However, we identified a bias\nin loss averaging during this packing training, as\npacks containing different numbers of sequences\nare assigned equal weight in the final loss calcu-\nlation. To mitigate this bias, we propose a loss\nweighting strategy to balance contributions to the\nloss across different sequences. In addition, we\nintroduce a sorted batching method that groups se-\nquences of similar lengths to reduce the intra-batch\nidle time.\nEvaluation-wise, we develop LongBench-Chat,\na benchmark compromising open-ended questions\nof 10k-100k length annotated by Ph.D. students.\nIt covers diverse aspects of instruction-following\nabilities such as reasoning, coding, summarization,\nand multilingual translation over long contexts.\nGPT-4 (OpenAI, 2023b) is employed to score the\nmachine-generated responses based on our anno-\ntated groundtruths and few-shot scoring examples.\nExtensive experiments show that LongAlign ef-\nfectively aligns models to handle contexts of up to\n64k tokens in length while maintaining their per-\nformance on general tasks without degradation. In\naddition, we have the following findings:\n\u2022 Impact of Data Quantity and Diversity: Both\nthe quantity and the diversity of the long in-\nstruction data significantly influence the aligned\nmodel\u2019s ability to handle long contexts, impact-\ning final performance by up to 30%.\n\u2022 Benefits of Long Instruction Data: The amount\nof long instruction data positively affects the per-\nformance on long-context tasks while does not\nhurt the models\u2019 short-context handling capaci-\nties.\n\u2022 Effectiveness of Training Strategies: The pack-\ning and sorted batching strategies adopted can\naccelerate training by over 100% without perfor-\nmance compromise. Furthermore, the proposed\nloss weighting technique improves long context\nperformance by 10%.\n2\nRelated Work\nLong Context Scaling. Long context scaling aims\nto expand the limited context length of existing\nLLMs to support long context tasks (Xiong et al.,\n2023). The current methods for long context scal-\ning can be divided into two categories: those that\nrequire fine-tuning or continual training on longer\nsequences and those that do not. Methods that do\nnot require fine-tuning often employ techniques\nsuch as sliding window attention (Han et al., 2023;\nXiao et al., 2023) or neighboring token compres-\nsion (Jiang et al., 2023; Zhang et al., 2024; Jin et al.,\n2024) to handle the positional O.O.D. problem in\nattention computation for long contexts. These\nmethods, although capable of extending the context\nlength of LLMs in a plug-and-play manner, still\ncannot match the performance of the fine-tuned\napproaches. Prominent fine-tuned approaches for\nlong context scaling (Chen et al., 2023a; Peng et al.,\n2023; Xiong et al., 2023; Chen et al., 2023b; Zhu\net al., 2023; Fu et al., 2023) typically involve posi-\ntion encoding extension and continual pretraining\non longer sequences.\nLLM Alignment. Following the previous steps\nof long context scaling, it is vital to also align the\nmodel with instruction-following data to ensure\nthat it can interact with various user requests in a\nchat interface (Wang et al., 2023). This phase, often\nreferred to as supervised fine-tuning or instruction-\ntuning, has been extensively studied in short con-\ntext scenarios (Wang et al., 2022; Taori et al., 2023;\nWang et al., 2023; Tunstall et al., 2023). How-\never, the introduction of long sequences presents\nunique challenges in terms of data, training meth-\nods, and evaluation for alignment. Xiong et al.\n(2023) proposes generating long instruction data\nby concatenating short instruction data, yet their\ndataset and model weight are not open-sourced.\nOn the other hand, while Chen et al. (2023b) has\nmade their long instruction data, LongAlpaca-12k,\navailable and employed LoRA (Hu et al., 2022) for\nefficient fine-tuning, it lacks in-depth discussion\nand comparative analysis of the influence of data\nand training methodologies. Our work aims to find\nan optimal solution for supervised (full parameter)\nfine-tuning on long context with full attention, by\ntuning data, training methods, and evaluating the\naligned models on a wide range of tasks.\n3\nLongAlign\nIn this section, we discuss the methodology in Lon-\ngAlign, involving the data construction process,\ntraining method, and evaluation benchmark.\n3.1\nPreliminary\nLarge language models can learn alignment by su-\npervised fine-tuning on high-quality pairs of in-\nstruction x and response y (Ouyang et al., 2022;\nChung et al., 2022). During training, the instruction\n2\nTask type \n(summary)\nLong Doc\nGenerated \nTask & Ans\n[{\u201crole\u201d: \u201cuser\u201d, \u201ccontent\u201d: Long Doc + Task}, \n{\u201crole\u201d: \u201cassistant\u201d, \u201ccontent\u201d: Answer}]\nUser:\nIn my younger and more vulnerable years my father gave me \nsome advice that I've been turning over in my mind ever since.  \n\u2026\nGiven the above text, please propose 5 English questions that \nrequire summarization or integration from multiple parts, \nmake sure they are diverse and cover all parts of the text, in \nthe following format: \u201c1: \u201d, \u201c2: \u201d, ...\nAssistant:\n1. Summarize the plots between Gatsby and Daisy\u2026\nFigure 2: Data construction example.\nand response are typically concatenated to form a\nsequence [x, y], which is then processed through an\nauto-regressive language model \u03c0 to maximize the\nprobability P\u03c0(y|x). The loss is similar to a lan-\nguage modeling loss, while only accounting for the\nloss associated with the tokens in y (target tokens):\nL([x, y]) = \u2212\n|y|\nX\ni=1\nlog P\u03c0(yi | [x, y<i]).\n(1)\n3.2\nDataset Construction\nLong instruction data typically involves a long con-\ntext material, such as a book, an extensive docu-\nment, or a lengthy code, accompanied by a task\nquery that requires summarizing, reasoning, or\ncomputing based on the material. During construc-\ntion, we first collect long articles and documents\nfrom 9 varied sources, covering books, encyclope-\ndias, academic papers, codes, etc. We then employ\nClaude 2.1 (Anthropic, 2023) to generate tasks and\nanswers according to a given long context, as illus-\ntrated in Figure 2. To foster a diverse range of gen-\nerated tasks, we incorporate task type descriptions\ninto the prompts, such as queries for summaries,\ninformation extraction, reasoning, etc. Using this\nmethodology, we create tasks and answers for 10k\nlengthy texts, yielding a total of 10k instances of\nsupervised data, of which 10% is in Chinese. The\nlength of these data ranges from 8k to 64k, mea-\nsured by ChatGLM tokenizer (Zeng et al., 2023)\ndue to its higher compression rate for Chinese char-\nacters. Details regarding the prompts and the data\nconstruction process can be found in Appendix A.\n3.3\nEfficient Long-Context Training\nTo ensure that the model retains the ability to han-\ndle both long and short texts (general capability)\nafter SFT, we mix the long instruction data with a\ngeneral instruction dataset for training. The mix-\nture of a large amount of general short data with a\nrelatively smaller amount of long instruction data\nresults in a long-tail data length distribution. As\nshown in Figure 3 left, the majority of the data falls\nwithin the 0-8k length range, while the remaining\ndata is fairly evenly distributed in the 8k-64k length\ninterval. Under this distribution, during training,\na data batch typically contains mostly short data,\nyet these batches also include a few longer texts\nwhich necessitate much more computation times,\nresulting in considerable idle times. To minimize\nthese idle times, the most effective approach is to\nconcatenate or sort the data in a manner that en-\nsures a more uniform length and computational\ntime within each batch. Bearing this in mind, we\nexplore two training methods, namely packing and\nsorted batching.\nPacking. It involves concatenating data of vary-\ning lengths together until reaching the maxi-\nmum length. The resulting packed data, whose\nlengths are generally close to the maximum length,\nare then batched and processed on multi-GPUs.\nThis approach effectively minimizes the idle time\nwithin each batch, as depicted in the upper right\nof Figure 3.\nAdditionally, to prevent cross-\ncontamination between different sequences within\nthe same pack during self-attention calculation,\nwe pass a list containing the starting and ending\npositions of different sequences and utilize the\nflash_attn_varlen_func from FlashAtten-\ntion 2 (Dao et al., 2022; Dao, 2023), which supports\nefficient computation of block diagonal attention\n(see Appendix B for more details). It requires less\ncomputation and IO time compared to the tradi-\ntional use of a 2D attention mask.\nHowever, we notice that the packing strategy\nleads to a bias towards longer sequences and se-\nquences containing more target tokens. This is\nbecause different packs, each contributing equally\nto the final loss, contain varying numbers of se-\nquences with different numbers of target tokens.\nConsequently, when calculating the mean loss for\neach batch, sequences in packs with fewer se-\nquences (typically the longer ones) or those con-\ntaining more target tokens, have a greater influence\non the final loss. Formally, consider M sequences\npacked into a batch of K packs where the i-th pack\nconsists of the sequences with indices in [Pi\u22121, Pi),\nthus it holds that P0 = 1, PK = M + 1. Let Li\ndenote the total summation of loss over Ni target\ntokens in the i-th sequence. If we weigh each se-\n3\n\u2026\nDevice1\nDevice2\nIdle time\nDevice\u2026\nPacking\nTraining time\nSorted batching\n\u2026\n\u2026\n\u2026\n\u2026\nBatch 1\nBatch 2\nblock diagonal attention mask\nloss weighting\n\u00d7 \ud835\udc72\n\ud835\udc75\ud835\udc8a\ud835\udc74\n# packs in the batch\n# sequences in the batch\n# target tokens in \ncurrent sequence \ud835\udc56\nNa\u00efve batching\nSequence Length\nNumber\nLength distribution\n\ud835\udc75\ud835\udc8a\nFigure 3: Under a long-tailed data length distribution, packing or sorted batching can reduce idle time and speed up\nthe training process. Loss weighting is required during packing to balance the loss contribution across sequences.\nquence equally, the loss should be\nL = 1\nM\nM\nX\ni=1\nLi\nNi\n,\n(2)\nwhile the loss calculated under packing is\nL\u2032 = 1\nK\nK\nX\nk=1\n(\nPk\u22121\nX\ni=Pk\u22121\nLi/\nPk\u22121\nX\ni=Pk\u22121\nNi) \u0338= L.\n(3)\nCompared with Eq. 2, this equates to assigning a\nweight of (Nj/ PPk\u22121\ni=Pk\u22121 Ni) to sequence j in the\nloss, i.e., in favor of sequences with more target\ntokens and sequences in smaller packs. To address\nthis inequality, we propose to scale the loss in the\ni-th sequence by K/(NiM) and instead take the\nsum of the scaled loss on each pack, which results\nin an equal loss to Eq. 2:\nL\u2032 = 1\nK\nK\nX\nk=1\n(\nPk\u22121\nX\ni=Pk\u22121\nLiK\nNiM ) = 1\nK\nM\nX\ni=1\nLiK\nNiM = L.\n(4)\nAs demonstrated in our experimental section, the\nloss weighting strategy results in a 10% improve-\nment in downstream tasks.\nSorted batching. We also consider an efficient\nsorted batching strategy for training (lower right\nof Figure 3). To ensure that the sequences within\neach batch are of similar lengths, we sort the data\nby length and select a random consecutive group\nof data for each batch, with no repetition. However,\nthis strategy inevitably introduces a bias in the data\ndistribution across different batches, where batches\nconsist either of all long sequences or all short\nsequences. This can be potentially disastrous for\nSGD optimization. In our experiments, we observe\nthat sorted batching significantly accelerates the\nprocess without a noticeable negative impact on\nperformance. This might be attributed to our use\nof large gradient accumulation steps and the strong\nadaptability of the optimizer.\n3.4\nLongBench-Chat\nAlthough there are existing benchmarks for evalu-\nating LLMs\u2019 long context understanding (An et al.,\n2023; Bai et al., 2023a; Li et al., 2023b), they do\nnot focus on assessing their instruction-following\ncapability under long context. Furthermore, their\nreliance on automatic metrics for evaluation limits\nthe assessment of aligned models\u2019 longer and more\ndiverse outputs to real-world queries, and how their\nresponses align with human preference.\nTo this end, we propose LongBench-Chat, which\nincludes 50 long context real-world queries rang-\ning from 10k to 100k in length, covering various\nkey user-intensive scenarios such as document QA,\nsummarization, and coding. It consists of 40 tasks\nin English and 10 in Chinese. To ensure the eval-\nuation truly reflects the model\u2019s ability to follow\nlong context instructions, we avoid using popular\nlong texts that are likely to have been seen and\nmemorized by the model during pretraining. We\nalso avoid posing questions that the model could\nanswer without reading the long text.\nFor evaluation, following previous works that\nhave shown the effectiveness of using LLM as an\nevaluator (Bai et al., 2023b; Zheng et al., 2023; Ke\net al., 2023), we employ GPT-4 (OpenAI, 2023b)\nto score the model\u2019s response in 1-10 based on a\ngiven human-annotated referenced answer and few-\nshot scoring examples for each question. We only\npass the short query (without the long document)\nto the evaluator, as currently there is no model\ncapable of evaluating the quality of responses under\nlong context inputs. To ensure that the evaluator\ncan make informed judgments based solely on the\ngroundtruth and few-shot scoring examples, we\nsteer clear of overly open-ended questions, such as\n\u201cWrite a poem based on the preceding text\u201d.\nTo validate the reliability of using GPT-4 as an\nevaluator on LongBench-Chat, we conduct a hu-\n4\nHuman\nGPT-4\nGPT-4+Few-shot\nSpearman (\u03c1)\n0.817\n0.788\n0.844\nKendall (\u03c4)\n0.694\n0.656\n0.716\nTable 1: Inter-annotator correlations; correlations be-\ntween GPT-4 (w/ and w/o Few-shot) and human.\nman evaluation study (more details in Appendix C).\nIn Table 1, we present the correlation between GPT-\n4\u2019s assessments using zero-shot prompting, which\ninvolves only the referenced answer, and its evalu-\nations with additional few-shot scoring examples,\ncompared to crowdsourced human judgments. We\nalso show the inter-annotator correlation in the first\ncolumn. We find that with few-shot prompting,\nGPT-4\u2019s correlation with human annotations not\nonly aligns but also surpasses the level of agree-\nment among human annotators, proving the relia-\nbility of such a metric on LongBench-Chat. We\nfurther discover that the overall average scores (1-\n10) obtained using GPT-4+Few-shot differ by an\naverage of 0.1 or less from the scores given by\nhuman experts. Additionally, we do not observe\na significant bias in GPT-4\u2019s scoring towards the\nlength of responses \u2014 in fact, it even penalizes\nexcessively lengthy responses.\nLeaderboard. Figure 1 reports the test results of\ncurrent long context (16k+) instruction fine-tuned\nmodels (chat models) and our most competent\nmodels trained with LongAlign on LongBench-\nChat. We include API-based Commercial mod-\nels: GPT-4-1106-preview (OpenAI, 2023a) (GPT-\n4 Turbo), GLM-4-128k2, and Claude-2.1 (An-\nthropic, 2023); as well as open-sourced models:\nInternLM2-7b-200k, InternLM2-20b-200k (Team,\n2023), ChatGLM3-6B-32k (Du et al., 2022; Zeng\net al., 2023), Vicuna-7b-v1.5-16k (Zheng et al.,\n2023), Orion-14b-LongChat (Chen et al., 2024),\nLongChat-7b-v1.5-32k (Li et al., 2023a), and\nMixtral-8x7b-Instruct-v0.2 (Jiang et al., 2024).\nNote that we employ middle truncation for inputs\nsurpassing the model\u2019s context window. Our evalu-\nation result reveals that the performance of current\nopen-sourced models still significantly lags behind\ncommercial models, which partially attributed to\nthe scale difference between these models. Addi-\ntionally, we observe that models with a context\nlength of 32k or less tend to underperform on\nLongBench-Chat, indicating that a longer context\nwindow is necessary to complete these long tasks.\n2https://open.bigmodel.cn/pricing\n4\nExperiments\nIn this section, we aim to answer the following\nresearch questions through a series of experiments:\nRQ1. During SFT, how does the quantity and di-\nversity of the long instruction data influence the\nmodel\u2019s performance in downstream tasks.\nRQ2. Whether incorporating long instruction data\nduring training affects the model\u2019s general capabili-\nties and their instruction-following / conversational\nabilities in short context scenarios.\nRQ3. The impact that the packing and sorted batch-\ning training methods have on the training efficiency\nand the final performance of the models.\nWe also incorporate discussions on the scalability\nof LongAlign on model size and context length,\nand the learning curve in long context alignment.\n4.1\nExperimental Setup\nData. To maintain the model\u2019s general capabilities\nand its proficiency in following short instructions,\nwe utilize ShareGPT (Chiang et al., 2023) (empty\nassistant responses are filtered out) as the source\nof short instruction data in our training data. To\ncompare the impact of different aspects of long\ninstruction data on model training, we incorporate\nthe following four suites of long instruction data\nin our experiment. \u2018LongAlign-0k\u2019, \u2018LongAlign-\n5k\u2019, and \u2018LongAlign-10k\u2019: 0, 5k, and 10k instances\nof LongAlign data, constructed according to the\nprocedure in Sec 3.2; \u2018LongAlpaca-12k\u2019: 12k data\nfrom the LongAlpaca dataset (Chen et al., 2023b).\nLongAlpaca includes 9k long QA data and 3k short\nQA data, where the long QA data is generated\nbased only on academic papers and books, offer-\ning less diversity compared to our LongAlign data.\nWe use this dataset to compare the impact of the\ndiversity of long instruction data on model training.\nModel. We include three model variants, namely\nChatGLM3-6B (Du et al., 2022; Zeng et al., 2023),\nLlama-2-7B, and Llama-2-13B (Touvron et al.,\n2023) (all base models). Given their 8k and 4k con-\ntext windows, we first perform context extension\nto extend their context window to 64k, resulting in\nChatGLM3-6B-64k, Llama-2-7B-64k, and Llama-\n2-13B-64k. This involves expanding the base fre-\nquency b of the RoPE position encoding (Su et al.,\n2024) by 200 times (from 10,000 to 2,000,000) and\ncontinual training on pretraining data with lengths\nunder 64k, for a total of 10 billion tokens3.\n3Continual training on 10B tokens is sufficient for context\nextension, as suggested in Fu et al. (2023).\n5\nTraining Data\nLong Tasks\nShort Tasks\n(Long)\nLongBench-Chat\nS-Doc QA\nM-Doc QA\nSumm\nMT-Bench\nARC\nHellaSwag\nTruthfulQA\nMMLU\nLongAlign-0k\n3.73\n58.7\n41.1\n38.4\n5.34\n50.3\n74.7\n51.6\n45.5\nLongAlign-5k\n5.97\n61.8\n42.1\n42.0\n5.51\n50.3\n75.1\n52.5\n46.6\nLongAlign-10k\n6.21\n64.0\n44.4\n44.2\n5.5\n50.5\n74.9\n52.5\n45.5\nLongAlpaca-12k\n4.46\n65.8\n45.6\n44.1\n4.93\n51.5\n75.4\n53.2\n47.1\nTable 2: Performance of ChatGLM3-6B-64k after training on different quantities and types of long instruction data.\nSFT on 0k long data\nSFT on 5k long data\nSFT on 10k long data\nSFT on 12k longalpaca data\nFigure 4: 1k-60k Needle test performance of Chat-\nGLM3-6B-64k trained on different suites of long data\nmixed with ShareGPT.\nTraining. All models are trained with 8xA800\n80G GPUs and DeepSpeed+ZeRO3+CPU offload-\ning (Rasley et al., 2020). The models can be trained\nwith a maximum length of 64k tokens without GPU\nmemory overflow. Consequently, we set the max-\nimum length of the training data to 64k, with any\ndata exceeding this length being truncated from the\nright. For packing training, each pack consists of\n12 sequences on average, we set the total batch size\nto 8, resulting in a global batch size of 96. For a\nfair comparison, we set the batch size to 8, with\na gradient accumulation step of 12 for other non-\npacking training methods. We train 2 epochs on\nthe training data (approximately 1500-2000 steps).\nEvaluation. We involve both long context tasks\nand short context tasks in evaluation. In both long\nand short scenarios, we consider tasks that eval-\nuate the instruction-following and conversational\nabilities, as well as tasks that assess general ca-\npabilities.\nFor long context tasks, we use our\nproposed LongBench-Chat to evaluate the mod-\nels\u2019 long context alignment proficiency and employ\nLongBench (Bai et al., 2023a) to test the model\u2019s\ngeneral long context understanding abilities. Long-\nBench is a bilingual, multi-task long context bench-\nmark. We conduct evaluations on three types of\ntasks within it: Single-Doc QA, Multi-Doc QA,\nand Summarization. Since the aligned models typi-\ncally produce longer responses, instead of using the\noriginal metrics (ROUGE, F1) to score the models\u2019\nreplies, we use GPT-4 to rate the model\u2019s outputs\nbased on their alignment with the groundtruth an-\nswers on LongBench. For short context tasks, we\nuse MT-Bench (Zheng et al., 2023), a multi-turn\nchat benchmark, to measure the models\u2019 ability to\nfollow short instructions. We also evaluate on the\ngeneral tasks on Open LLM Leaderboard (Beech-\ning et al., 2023), including ARC (Clark et al.,\n2018), HellaSwag (Zellers et al., 2019), Truthful\nQA (Lin et al., 2022), and MMLU (Hendrycks\net al., 2021). We follow the evaluation settings\nin the Open LLM Leaderboard and utilize lm-\nevaluation-harness framework (Gao et al., 2023)\nfor evaluation on these tasks. To ensure the most\nstable evaluation results, we use GPT-4 to score\ntwice on LongBench-Chat and MT-Bench, and av-\nerage these scores to obtain the final score.\n4.2\nInfluence of Data\nWe conduct SFT on ChatGLM3-6B-64k using\nShareGPT data mixed with different suites of long\ninstruction data. All models except LongAlign-0k\nare trained using the more efficient packing strat-\negy with loss weighting. The evaluation results are\nreported in Table 2. For LongBench-Chat and MT-\nBench, the reported results are averaged over GPT-\n4\u2019s rating (1-10) across all test instances, while\nresults on other datasets are normalized between 0-\n100. We also conduct the \u201cNeedle in A HayStack\u201d\nexperiment4 (result visualization in Figure 4) to test\nthe model\u2019s ability to utilize information from 10\ndifferent positions within long contexts of varying\nlengths between 1k-60k. Specifically, this task asks\nfor the model to retrieve a piece of fact (the \u2018nee-\ndle\u2019) that is inserted in the middle (positioned at a\nspecified depth percent) of a long context window\n4https://github.com/gkamradt/LLMTest_NeedleInAHaystack\n6\nTraining Method\nLong Tasks\nShort Tasks\nLongBench-Chat\nS-Doc QA\nM-Doc QA\nSumm\nMT-Bench\nARC\nHellaSwag\nTruthfulQA\nMMLU\nChatGLM3-6B-64k\nNa\u00efve batching\n5.87\n65.4\n45.0\n44.8\n5.61\n50.7\n74.7\n52.8\n46.0\nSorted batching\n5.4\n66.2\n46.3\n43.7\n5.76\n51.3\n74.8\n51.9\n46.3\nPacking\n5.76\n65.0\n45.1\n42.8\n5.64\n50.9\n74.8\n50.5\n47.2\nPacking+loss weighting\n6.21\n64.0\n44.4\n44.2\n5.5\n50.5\n74.9\n52.5\n45.5\nLlama-2-7B-64k\nNa\u00efve batching\n5.95\n62.8\n42.7\n41.6\n5.52\n48.9\n74.8\n45.3\n43.6\nSorted batching\n6.38\n63.4\n42.2\n41.3\n5.51\n49.5\n74.8\n48.0\n44.3\nPacking\n5.89\n61.7\n40.4\n42.0\n5.58\n48.1\n74.9\n46.1\n43.9\nPacking+loss weighting\n6.10\n60.8\n41.3\n43.1\n5.60\n48.4\n74.5\n47.4\n43.3\nTable 3: Performance of ChatGLM3-6B-64k and Llama-2-7B-64k under different training methods.\n(the \u2018haystack\u2019). We summarize our key findings\non the influence of data as follows.\n1. More long instruction data enhances the per-\nformance in long tasks, and without compro-\nmising the performance in short tasks. Compar-\ning the performance of LongAlign-0k, LongAlign-\n5k, and LongAlign-10k, we observe that as the\namount of long instruction data increases, there\nis a consistent improvement in the model\u2019s per-\nformance across all long tasks. Meanwhile, in-\ntriguingly, its performance on short tasks remains\ncomparable to when it is trained solely on short\ninstructions. Additionally, given the inferior perfor-\nmance of LongAlign-0k in long tasks (especially on\nLongBench-Chat), this also indicates that merely\nperforming context extension on the base model is\ninsufficient to ensure good performance on down-\nstream long tasks. It is necessary to incorporate a\nsubstantial amount of long data covering various\nlengths during SFT. Moreover, the needle test re-\nsult also suggests that more long data enhances the\nmodel\u2019s ability to utilize information from different\npositions within long texts, resulting in a decrease\nof the model\u2019s retrieval error.\n2. Diversity of long instruction data is benefi-\ncial for the model\u2019s instruction-following abil-\nities. LongAlign-10k shows significantly better\nresults in long and short instruction-following\ntasks (LongBench-Chat and MTBench), compared\nto LongAlpaca-12k.\nMeanwhile, LongAlpaca-\n12k slightly outperforms LongAlign-10k on Long-\nBench. This is primarily due to its superior per-\nformance on the 2WikiMQA (Ho et al., 2020) and\nNarrativeQA (Ko\u02c7cisk`y et al., 2018) datasets, which\nare based on Wikipedia and novels, bearing more\nresemble to the source of the instruction data in\nLongAlpaca-12k.\nChatGLM3-6B-64k\nLlama-2-7B-64k\nLlama-2-13B-64k\n0\n20\n40\n60\n80\n100\n120\nTraining time (h)\n45.4\n67.2\n117.2\n20.5\n23.4\n41.2\n19.1\n23.3\n44.5\nNa\u00efve batching\nPacking\nSorted batching\nFigure 5: Training time (hrs) on 8xA800 80G GPUs\nunder different training methods.\n4.3\nImpact of Training Methods\nWe\ncompare\ndifferent\ntraining\nmethods\non\nChatGLM3-6B-64k and Llama-2-6B-64k, includ-\ning na\u00efve batching, packing (w/ and w/o loss\nweighting), and sorted batching, to assess their\nimpact on training efficiency, as well as their in-\nfluence on downstream task performance.5 All\nmodels are trained on LongAlign-10k. Figure 5\ndisplays a comparison of the training time required\nfor each method. Table 3 presents the performance\non downstream tasks. Our findings are as follows.\n1.\nPacking and sorted batching double the\ntraining efficiency while exhibiting good per-\nformance. From Figure 5, we can see that the\ntraining efficiency of packing and sorted batch-\ning is comparable, both requiring less than half\nthe time needed under na\u00efve batching. Addition-\nally, according to table 3, models trained with the\ntwo efficient methods perform comparably to those\ntrained with na\u00efve batching on both long and short\ntasks. We also find that the effectiveness of these\ntwo training methods varies with different models.\n5Na\u00efve batching and sorted batching consume more GPU\nmemory compared to packing, due to their use of gradient\naccumulation. We truncate all data to 56k length for ChatGLM\nwith these two methods to ensure no GPU memory overflow.\n7\nLlama-2-13B-64k\nLongBench-Chat\nS-Doc QA\nM-Doc QA\nSumm\nMT-Bench\nPacking+loss weighting\n6.79\n68.0\n40.3\n43.6\n6.12\nSorted batching\n7.02\n66.1\n43.9\n45.3\n6.02\nTable 4: Scaling up: LongAlign on LLama-2-13B.\nFor instance, the model trained on ChatGLM3-6B\nusing packing+loss weighting shows significantly\nbetter performance on LongBench-Chat, whereas\nsorted batching performs the best for Llama-2-7B.\n2. Loss weighting significantly improves per-\nformance on long instruction task for packing\ntraining. By comparing the performance of mod-\nels with and without loss weighting strategy during\npacking training, it\u2019s evident that incorporating the\nloss weighting strategy greatly improves the ca-\npability in LongBench-Chat (by about 5%\u223c10%),\nwhile having a minimal and variable impact on the\nperformance of other tasks. We believe that this is\nprimarily because, without loss weighting in SFT\ndata, different long instruction data contribute vari-\nably to the loss \u2014 longer data tend to contribute\nmore to the loss (refer to Eq. 3). Such an unnat-\nural weighting bias is often detrimental to model\ntraining, potentially leading to training instability,\ndeviating it from the optimal learning trajectory.\n4.4\nDiscussion\nScalability of LongAlign. We explore two scaling\ndirections on our LongAlign framework: larger\nmodel size and longer context window. To do so,\nwe fine-tune Llama-2-13B-64k using LongAlign-\n10k dataset with the two efficient training meth-\nods, and the evaluation results are shown in Ta-\nble 4. Compared to the 7B-scale model, the 13B\nmodel shows a 10% improvement on LongBench-\nChat, setting a new record among open-sourced\nmodels (LongAlign-13B-64k in Figure 1). This\nindicates that our alignment method scales effec-\ntively to larger-scale models. We also construct\nSFT data up to 128k in length with human annota-\ntion and successfully align ChatGLM3-6B under\n128k context window using packing training with\nloss weighting, resulting in ChatGLM3-6B-128k\n(performance shown in Figure 1).\nLearning curve on long task v.s. short task. To\ncompare the learning processes of alignment under\nlong context and short context, we present in Fig-\nure 6 the relative performance curves on long and\nshort instruction-following tasks (on LongBench-\nChat and MT-Bench, respectively) during model\n0\n250\n500\n750\n1000 1250 1500 1750\nTraining steps\n0.2\n0.4\n0.6\n0.8\n1.0\nScore / Final score\nLongBench-Chat\nMT-Bench\nFigure 6: Relative performance on long and short tasks\nthroughout the training process of ChatGLM3-6B-64k.\ntraining, illustrating how performance varies with\nthe number of training steps. We use exponen-\ntial moving average to smooth the original perfor-\nmance curves (dotted lines), and display them as\nsolid lines. We observe that the trends of the two\nlearning curves are strikingly similar \u2014 both show\nrapid improvement between 0-500 steps, followed\nby a slow rise, and stabilize after 1000 steps. This\nmay imply a deeper connection between long and\nshort alignment. They might be jointly determined\nby shared latent factors, which are optimized dur-\ning training to help the model align to both long\nand short instructions simultaneously.\nIn Appendix D, we provide case analyses\nof different LongAlign-tuned models on out-of-\ndistribution (OOD) long context query, that is,\nquery that the models have not encountered in the\nlong context SFT data. We find that models trained\nwith LongAlign can generalize to OOD long con-\ntext queries, such as writing a review for a research\npaper, and that larger-scale models have stronger\ngeneralization capabilities.\n5\nConclusion\nThis paper aims to find the best practice for long\ncontext alignment in the scope of data, training\nmethod, and evaluation. Our proposed solution,\nnamely LongAlign, uses Self-Instruct to construct\ndiverse long instruction data, and efficiently fine-\ntune the model with packing combined with loss\nweighting or sorted batching. Moreover, we in-\ntroduce LongBench-Chat to facilitate reliable as-\n8\nsessment of LLM\u2019s instruction-following ability\non practical long context interactions. Through\ncontrolled experiments, we find that the amount,\ndiversity of data, as well as the correct training\nmethod, are crucial to the final performance.\nReferences\nChenxin An, Shansan Gong, Ming Zhong, Mukai Li,\nJun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.\nL-eval: Instituting standardized evaluation for long\ncontext language models.\nAnthropic. 2023. Anthropic: Introducing claude 2.1.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,\nJiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\nLiu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,\nand Juanzi Li. 2023a.\nLongbench: A bilingual,\nmultitask benchmark for long context understanding.\narXiv preprint arXiv:2308.14508.\nYushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He,\nXiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao,\nHaozhe Lyu, et al. 2023b. Benchmarking foundation\nmodels with language-model-as-an-examiner. arXiv\npreprint arXiv:2306.04181.\nEdward Beeching,\nCl\u00e9mentine Fourrier,\nNathan\nHabib, Sheon Han, Nathan Lambert, Nazneen\nRajani, Omar Sanseviero, Lewis Tunstall, and\nThomas\nWolf.\n2023.\nOpen\nLLM\nleader-\nboard. https://huggingface.co/spaces/\nHuggingFaceH4/open_llm_leaderboard.\nDu Chen, Yi Huang, Xiaopu Li, Yongqiang Li,\nYongqiang Liu, Haihui Pan, Leichao Xu, Dacheng\nZhang, Zhipeng Zhang, and Kun Han. 2024. Orion-\n14b: Open-source multilingual large language mod-\nels. arXiv preprint arXiv:2401.12246.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and\nYuandong Tian. 2023a. Extending context window\nof large language models via positional interpolation.\narXiv preprint arXiv:2306.15595.\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai,\nZhijian Liu, Song Han, and Jiaya Jia. 2023b. Lon-\nglora: Efficient fine-tuning of long-context large lan-\nguage models. arXiv preprint arXiv:2309.12307.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457.\nTri Dao. 2023. FlashAttention-2: Faster attention with\nbetter parallelism and work partitioning.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\nand Christopher R\u00e9. 2022. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness.\nIn Advances in Neural Information Processing Sys-\ntems.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320\u2013335.\nYao Fu, Xinyao Niu, Xiang Yue, Rameswar Panda,\nYoon Kim, and Hao Peng. 2023. Understanding data\ninfluence on context scaling. Yao Fu\u2019s Notion.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,\nSid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li,\nKyle McDonell, Niklas Muennighoff, Chris Ociepa,\nJason Phang, Laria Reynolds, Hailey Schoelkopf,\nAviya Skowron, Lintang Sutawika, Eric Tang, An-\nish Thite, Ben Wang, Kevin Wang, and Andy Zou.\n2023. A framework for few-shot language model\nevaluation.\nChi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng\nJi, and Sinong Wang. 2023. Lm-infinite: Simple\non-the-fly length generalization for large language\nmodels. arXiv preprint arXiv:2308.16137.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing a multi-hop\nqa dataset for comprehensive evaluation of reasoning\nsteps. In Proceedings of the 28th International Con-\nference on Computational Linguistics, pages 6609\u2013\n6625.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. LoRA: Low-rank adaptation of\nlarge language models. In International Conference\non Learning Representations.\n9\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris Bam-\nford, Devendra Singh Chaplot, Diego de las Casas,\nEmma Bou Hanna, Florian Bressand, et al. 2024.\nMixtral of experts. arXiv preprint arXiv:2401.04088.\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng\nLi, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023.\nLongllmlingua: Accelerating and enhancing llms\nin long context scenarios via prompt compression.\narXiv preprint arXiv:2310.06839.\nHongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng\nJiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen,\nand Xia Hu. 2024. Llm maybe longlm: Self-extend\nllm context window without tuning. arXiv preprint\narXiv:2401.01325.\nPei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei,\nJiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao\nDong, Hongning Wang, et al. 2023. Critiquellm:\nScaling llm-as-critic for effective and explainable\nevaluation of large language model generation. arXiv\npreprint arXiv:2311.18702.\nTom\u00e1\u0161 Ko\u02c7cisk`y, Jonathan Schwarz, Phil Blunsom, Chris\nDyer, Karl Moritz Hermann, G\u00e1bor Melis, and Ed-\nward Grefenstette. 2018. The narrativeqa reading\ncomprehension challenge. Transactions of the Asso-\nciation for Computational Linguistics, 6:317\u2013328.\nMario Michael Krell, Matej Kosec, Sergio P Perez, and\nAndrew Fitzgibbon. 2021. Efficient sequence pack-\ning without cross-contamination: Accelerating large\nlanguage models without impacting performance.\narXiv preprint arXiv:2107.02027.\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lian-\nmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe\nMa, and Hao Zhang. 2023a. How long can open-\nsource llms truly promise on context length?\nJiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan\nZhang. 2023b. Loogle: Can long-context language\nmodels understand long contexts?\narXiv preprint\narXiv:2311.04939.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214\u20133252.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu\nLei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen\nMen, Kejuan Yang, et al. 2023. Agentbench: Evaluat-\ning llms as agents. arXiv preprint arXiv:2308.03688.\nOpenAI. 2023a. New models and developer products\nannounced at devday.\nOpenAI. 2023b. Openai: Gpt-4.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-\nrico Shippole. 2023. Yarn: Efficient context window\nextension of large language models. arXiv preprint\narXiv:2309.00071.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. 2020. Deepspeed: System optimiza-\ntions enable training deep learning models with over\n100 billion parameters. In Proceedings of the 26th\nACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, pages 3505\u20133506.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019.\nMegatron-lm: Training multi-billion\nparameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,\nWen Bo, and Yunfeng Liu. 2024. Roformer: En-\nhanced transformer with rotary position embedding.\nNeurocomputing, 568:127063.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois,\nXuechen Li,\nCarlos Guestrin,\nPercy\nLiang, and Tatsunori B. Hashimoto. 2023.\nStan-\nford\nalpaca:\nAn\ninstruction-following\nllama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca.\nInternLM Team. 2023.\nInternlm:\nA multilingual\nlanguage model with progressively enhanced capa-\nbilities.\nhttps://github.com/InternLM/\nInternLM.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nLewis Tunstall, Edward Beeching, Nathan Lambert,\nNazneen Rajani, Kashif Rasul, Younes Belkada,\nShengyi Huang, Leandro von Werra, Cl\u00e9mentine\nFourrier, Nathan Habib, et al. 2023. Zephyr: Di-\nrect distillation of lm alignment.\narXiv preprint\narXiv:2310.16944.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack\nHessel, Tushar Khot, Khyathi Chandu, David Wad-\nden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy,\nand Hannaneh Hajishirzi. 2023. How far can camels\ngo? exploring the state of instruction tuning on open\nresources. In Thirty-seventh Conference on Neural\nInformation Processing Systems Datasets and Bench-\nmarks Track.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2022. Self-instruct: Aligning language\nmodel with self generated instructions.\n10\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song\nHan, and Mike Lewis. 2023. Efficient streaming\nlanguage models with attention sinks. arXiv preprint\narXiv:2309.17453.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,\nPrajjwal Bhargava, Rui Hou, Louis Martin, Rashi\nRungta, Karthik Abinav Sankararaman, Barlas Oguz,\net al. 2023. Effective long-context scaling of founda-\ntion models. arXiv preprint arXiv:2309.16039.\nLiang Xu, Xuanwei Zhang, and Qianqian Dong.\n2020. Cluecorpus2020: A large-scale chinese cor-\npus for pre-training language model. arXiv preprint\narXiv:2003.01355.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4791\u20134800.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2023. Glm-130b: An\nopen bilingual pre-trained model. In The Eleventh In-\nternational Conference on Learning Representations.\nPeitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao,\nQiwei Ye, and Zhicheng Dou. 2024. Soaring from\n4k to 400k: Extending llm\u2019s context with activation\nbeacon. arXiv preprint arXiv:2401.03462.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. 2023.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. arXiv preprint arXiv:2306.05685.\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wen-\nhao Wu, Furu Wei, and Sujian Li. 2023. Pose: Effi-\ncient context window extension of llms via positional\nskip-wise training.\n11\nA\nDataset Construction Details\nData sources. The 9 sources of the documents in our constructed LongAlign dataset are listed below6,\nalong with their copyright information:\n\u2022 Arxiv (Academic papers): Open-accessed and can be downloaded freely by anyone.\n\u2022 Books3 (Books): From The Pile, currently it is not licensed to be downloaded.\n\u2022 C4 Dataset (Various types of articles): Publicly available dataset with ODC-BY license.\n\u2022 CLUECorpus2020 (Various types of Chinese articles): Extracted Chinese instances from the Common-\nCrawl corpus by Xu et al. (2020).\n\u2022 CommonCrawl corpus (Various types of articles): Publicly available dataset and can be downloaded\nfreely by anyone.\n\u2022 Github (Code repositories): Open-accessed and can be downloaded freely by anyone.\n\u2022 Stack Exchange (Question-and-answer websites): Freely downloadable and licensed under CC BY-SA.\n\u2022 Wikipedia (Encyclopedias): Grant free access and licensed under CC BY-SA.\n\u2022 WuDaoCorpora (Various types of articles): open-accessed dataset.\nWe sample articles with lengths under 64k (measured by ChatGLM3-6B tokenizer) from these datasets.\nNote that we upsample longer articles to ensure our dataset covers more long texts.\nPrompts for data generation. During the data generation process, we employ four types of task prompts\nto encourage Claude to produce a more diverse set of instruction data:\n\u2022 General type task\n{Long Doc}\nGiven the above text, please propose 5 English questions that are diverse and cover all\nparts of the text, in the following format: \"1: \", \"2: \", ...\n\u2022 Summary type task\n{Long Doc}\nGiven the above text, please propose 5 English questions that require summarization or\nintegration from multiple parts, make sure they are diverse and cover all parts of the text, in the\nfollowing format: \"1: \", \"2: \", ...\n\u2022 Reasoning type task\n{Long Doc}\nGiven the above text, please propose 5 English questions that require multi-hop reason-\ning, make sure they are diverse and cover all parts of the text, in the following format: \"1: \", \"2: \",\n...\n\u2022 Information extraction type task\n{Long Doc}\nGiven the above text, please propose 5 English information-seeking questions, make sure\nthey are diversed and cover all parts of the text, in the following format: \"1: \", \"2: \", ...\n6Arxiv, Books3, CC, Github, Stack Exchange, and Wikipedia are sampled from The Pile (Gao et al., 2020).\n12\nFor each long article, we randomly select one of the four task prompts and have Claude generate five\nquestions to ensure that the questions cover content from multiple spans within the long text. We then\nrandomly choose one of these questions and request Claude for its answer, resulting in instruction data as\nillustrated in Figure 2. For long Chinese documents, we translate the corresponding prompts into Chinese\nand obtain Chinese instruction data.\nB\nTraining Method Details\nHere we provide details regarding the implementation of the packing strategy and loss weighting. During\npacking training, for each batch of data, we pass a special one-dimensional attention mask. In this mask,\nthe ith element represents the starting index of the ith sequence in the batch. The first element of the\nmask is 0, and the last element is equal to batch_size \u00d7 seq_len. During the attention computation, we\nuse the flash_attn_varlen_func function from FlashAttention 2 and pass the attention mask to\nthe function\u2019s cu_seqlens_q and cu_seqlens_k parameters. This function performs attention\ncalculation within sequences between start and end indices from adjacent elements in the mask. Thus,\nduring the computation, the query of each sequence can only attend to the key within the same sequence.\nFor implementation of the loss weighting strategy, we first preprocess the training data to produce a\nweighted 1D mask for each pack of sequences, where the weight is set to 1/N (N is the number of target\ntokens in current sequence) on the position corresponding to target tokens, otherwise 0. During training,\nwe set M and K, i.e., the number of sequences and packs in the current batch, on the fly according to its\nconfiguration. Then the loss is calculated as the summation of the cross entropy loss at each token scaled\nby K/MN.\nC\nEvaluation Details\nC.1\nLongBench-Chat\nEvaluation data. 30 question data in LongBench-Chat are proposed by our author team to best mimic\nreal user queries, these include 20 English and 10 Chinese questions. The remaining 20 questions in\nLongBench-Chat are selected from long dependency QA tasks in the LooGLE dataset (Li et al., 2023b).\nThe long texts for these data are sourced from Wikipedia pages and movie scripts post-2022, ensuring\nthe information is relatively new and less likely to be already known by LLMs. We aim to select\nquestions that resemble real user inquiries, can be answered from the text, and ensure a diverse type of\nquestions (including Comprehension & Reasoning, Multiple Information Retrieval, Timeline Reorder, and\nComputation types). For the questions in LongBench-Chat, we invite experts to read the entire material\nand write groundtruth answers, where each answer is verified by at least two experts.\nEvaluation prompts. For each question, we manually score on three responses as few-shot scoring\nexamples, shuffle their order in each evaluation run and use the following prompt to get GPT-4\u2019s evaluation:\n[Instructions] You are asked to evaluate the quality of the AI assistant\u2019s answers to user questions as\nan impartial judge, and your evaluation should take into account factors including correctness (high\npriority), helpfulness, accuracy, and relevance. The scoring principles are as follows: 1. Read the\nAI assistant\u2019s answer and compare the assistant\u2019s answer with the reference answer. 2. Identify all\nerrors in the AI Assistant\u2019s answers and consider how much they affect the answer to the question. 3.\nEvaluate how helpful the AI assistant\u2019s answers are in directly answering the user\u2019s questions and\nproviding the information the user needs. 4. Examine any additional information in the AI assistant\u2019s\nanswer to ensure that it is correct and closely related to the question. If this information is incorrect\nor not relevant to the question, points should be deducted from the overall score.\nPlease give an overall integer rating from 1 to 10 based on the above principles, strictly in the\nfollowing format:\"[[rating]]\", e.g. \"[[5]]\".\n[Question] {}\n[Reference answer begins] {} [Reference answer ends]\nBelow are several assistants\u2019 answers and their ratings:\n[Assistant\u2019s answer begins] {} [Assistant\u2019s answer ends]\n13\nRating: [[{}]]\n[Assistant\u2019s answer begins] {} [Assistant\u2019s answer ends]\nRating: [[{}]]\n[Assistant\u2019s answer begins] {} [Assistant\u2019s answer ends]\nRating: [[{}]]\nPlease rate the following assistant answers based on the scoring principles and examples above:\n[Assistant\u2019s answer begins] {} [Assistant\u2019s answer ends]\nRating:\nHere is the zero-shot prompt used as the baseline in our metric evaluation study:\n[Instructions] You are asked to evaluate the quality of the AI assistant\u2019s answers to user questions as\nan impartial judge, and your evaluation should take into account factors including correctness (high\npriority), helpfulness, accuracy, and relevance. The scoring principles are as follows: 1. Read the\nAI assistant\u2019s answer and compare the assistant\u2019s answer with the reference answer. 2. Identify all\nerrors in the AI Assistant\u2019s answers and consider how much they affect the answer to the question. 3.\nEvaluate how helpful the AI assistant\u2019s answers are in directly answering the user\u2019s questions and\nproviding the information the user needs. 4. Examine any additional information in the AI assistant\u2019s\nanswer to ensure that it is correct and closely related to the question. If this information is incorrect\nor not relevant to the question, points should be deducted from the overall score.\nPlease give an overall integer rating from 1 to 10 based on the above principles, strictly in the\nfollowing format:\"[[rating]]\", e.g. \"[[5]]\".\n[Question] {}\n[Reference answer] {}\n[Assistant\u2019s answer] {}\nRating:\nHuman evaluation. Here we provide more details for the human evaluation study on LongBench-Chat.\nWe select responses to the 50 questions on LongBench-Chat from six different models, creating a data\npool of 300 instances. We invite two human experts (both are Ph.D. students from Tsinghua University)\nto each score 200 responses based on the instruction and referenced answer, on a scale from 1 to 10. The\nscoring criteria provided to the human experts are as follows:\nPlease score the assistant\u2019s response based on the question and the reference answer, with 1\nbeing the lowest and 10 the highest. The annotation must adhere to the following requirements:\n1. Focus primarily on whether the response covers the key points in the reference answer.\n2. For reference answers containing multiple key points, look for how many of these the response\naccurately addresses and score accordingly.\n3. If the response includes points not found in the reference answer, check the original text for\nevidence. Deduct points at your discretion if it does not align with the original text.\n4. Also consider deducting points for overly verbose responses or those that are excessively\ngeneralized.\nEvaluation cost. On LongBench-Chat, a run of evaluation requires approximately 32,000 tokens on\naverage (almost entirely as input tokens). Therefore, using GPT-4 for evaluation would cost about $0.96\nper run.\nC.2\nLongBench\nEvaluation prompts. We use GPT-4 to score the responses from our aligned models in Single-Doc QA,\nMulti-Doc QA, and Summarization tasks on LongBench. For the first two QA tasks, the prompt for the\nGPT-4 evaluator is as follows.\n14\nYou are asked to evaluate the quality of the AI assistant\u2019s answers to user question as an impartial\njudge, and your evaluation should take into account factors including correctness (high priority), and\ncomprehensiveness (whether the assistant\u2019s answer covers all points). Read the AI assistant\u2019s answer\nand compare against the reference answer, and give an overall integer rating in 1, 2, 3 (1 = wrong or\nirrelevant, 2 = partially correct, 3 = correct and comprehensive) based on the above principles, strictly\nin the following format:\"[[rating]]\", e.g. \"[[2]]\".\nQuestion:\n{Question}\nReference answer:\n{Groundtruth}\nAssistant\u2019s answer:\n{Response}\nRating:\nThe prompt for GPT-4 evaluation on summarization tasks is as follows.\nYou are asked to evaluate the quality of the AI assistant\u2019s generated summary as an impartial\njudge, and your evaluation should take into account factors including correctness (high priority),\ncomprehensiveness (whether the assistant\u2019s summary covers all points), and coherence. Read the AI\nassistant\u2019s summary and compare against the reference summary, and give an overall integer rating in\non a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the evaluation criteria, strictly\nin the following format:\"[[rating]]\", e.g. \"[[3]]\".\nReference summary:\n{Groundtruth}\nAssistant\u2019s summary:\n{Response}\nRating:\nEvaluation cost. On LongBench, a run of GPT-4 evaluation on 12 datasets in Single-Doc QA, Multi-Doc\nQA, and Summarization tasks requires approximately 800,000 tokens on average (almost entirely as input\ntokens). Therefore, using GPT-4 for evaluation would cost about $24 per run.\nC.3\nNeedle Test\nFor the \u201cNeedle in A Haystack\u201d evaluation, following the original configuration in the original github\nrepository, we use \u201cThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on\na sunny day.\u201d as the needle fact, and Paul Graham\u2019s essays as the long haystack context. We use the\nquery prompt from Claude 2.17: \u201cWhat is the best thing to do in San Francisco? Here is the most relevant\nsentence in the context:\u201d.\nD\nCase Studies on OOD Queries\nAs part of our research on aligning LLMs on long context, we come up with an intriguing and practical\ncase study: Can we evaluate the long context understanding capability of our trained models using this\npaper as the long input? Hence we use the paper as input (of course, to prevent recursive nesting, the\ninput paper does not include this section). We consider using OOD query, that is, query that the model\nhas not encountered in the long context SFT data. Our queries include \u201cFrom a professional reviewer\u2019s\nperspective, identify the strengths and weaknesses of this paper\u201d and \u201cUse rap to tell about this paper\u201d.\nLet\u2019s see how the model variants response:8\n7https://www.anthropic.com/news/claude-2-1-prompting\n8We remove the Abstract, Conclusion, and Limitations sections in the input to prevent the model from simply copying content\nfrom these sections without demonstrating its own understanding.\n15\n!!!Disclaimer: The below contents regarding this paper are generated by AI and DOES NOT reflect the\ntrue views of the authors or readers of this paper. Please be aware and identify any misinformation.\ng User: From a professional reviewer\u2019s perspective, identify the strengths and weaknesses of this\npaper.\n3 LongAlign-6B-64k (ChatGLM3-6B-64k trained on LongAlign-10k using packing + loss\nweighting):\nAs an AI language model, I cannot provide a subjective review as I lack the ability to evaluate the\nquality of the paper based on personal biases or predispositions. However, I can provide an objective\nanalysis of the strengths and weaknesses of the paper.\nStrengths:\n1. Comprehensive approach: The paper provides a thorough and systematic recipe, LongAlign, to\naddress the challenges of aligning large language models to handle long context. It involves collecting\ndiverse long data, employing efficient training strategies, and evaluating the models against realistic\nlong context benchmarks.\n2. Impact of data: The paper demonstrates that the quantity and diversity of long instruction data\nsignificantly impact the model\u2019s ability to handle long contexts, and that incorporating more long data\nis beneficial. This provides a valuable perspective on the importance of diverse long context data.\n3. Training methods: The paper compares the effectiveness of different training methods and identifies\nthe packing and sorted batching strategies as efficient and effective. This provides valuable insights\nfor practitioners working on large language model training.\nWeaknesses:\n1. Lack of a large-scale human evaluation study: The paper does not include a comprehensive large-\nscale human evaluation study to validate the performance of the GPT-4 evaluator on LongBench-Chat.\nWhile the correlation with human annotations is presented, a direct comparison between human and\nGPT-4\u2019s assessments is lacking.\n2. Scalability limits: The paper does not explore the scalability of LongAlign to even larger models or\nlonger context windows. While some experimental results on the scaling of LongAlign are presented,\nthe limits of the approach are not fully explored.\n3. Learning curve analysis: The paper does not provide a thorough analysis of the learning curve\nunder long context alignment, comparing the performance on short and long context tasks throughout\ntraining. This would have provided valuable insights into the learning dynamics.\nIn conclusion, the paper provides a valuable contribution to the field by providing a comprehensive\nrecipe for aligning large language models to handle long context, and demonstrating the impact of\ndata quantity and diversity on model performance. However, a more comprehensive and rigorous\nevaluation study and a deeper exploration of the learning dynamics under long context alignment\nwould further enhance the paper\u2019s impact.\n3 LongAlign-7B-64k (Llama-2-7B-64k trained on LongAlign-10k using sorted batching):\nAs an AI language model, I do not have personal opinions or emotions, but I can provide a neutral\nevaluation of the paper based on its content and structure.\nStrengths:\n1. The paper presents a comprehensive recipe, LongAlign, for aligning large language models to\nhandle long contexts. This is a significant contribution to the field as there is a lack of study and\ndiscussion on long context alignment.\n2. The paper provides a detailed description of the data construction process, including collecting\nlong sequences from 9 sources and using Claude 2.1 to generate tasks and answers for 10k lengthy\ntexts.\n3. The paper proposes two effective training methods - packing and sorted batching - to speed up the\ntraining process and improve the final performance of the models.\n16\n4. The paper introduces LongBench-Chat, a series of open-ended real-world user questions of\n10k-100k length, covering various aspects of instruction-following ability.\n5. The paper conducts experiments on long context benchmarks and general benchmarks, demonstrat-\ning the effectiveness of LongAlign in aligning models to handle contexts of up to 64k.\nWeaknesses:\n1. The paper does not provide a detailed analysis of the impact of the proposed training methods on\nthe model\u2019s general capabilities and conversational abilities in short context scenarios.\n2. The paper does not discuss the limitations of the proposed methods or suggest any future work that\ncould improve them.\n3. The paper does not provide a detailed comparison of LongAlign with other existing methods for\nlong context scaling.\n4. The paper does not provide a comprehensive evaluation of the proposed methods on other long\ncontext benchmarks or real-world user queries.\nOverall, the paper presents a thorough recipe for long context alignment and provides effective\ntraining methods to speed up the training process. However, a more in-depth analysis of the impact on\ngeneral capabilities and a comparison with other methods would make the paper more comprehensive.\n3 LongAlign-13B-64k (Llama-2-13B-64k trained on LongAlign-10k using sorted batching):\nStrengths:\n1. The paper presents a comprehensive solution for aligning large language models to handle long\ncontext interactions, including constructing a diverse long instruction dataset, training methods, and\nevaluation benchmark.\n2. The authors conduct extensive experiments to demonstrate the effectiveness of their approach,\nshowing improvements in downstream tasks without performance degradation on general benchmarks.\n3. The paper provides valuable insights into the challenges and solutions for long context alignment,\nwhich is an important but understudied area.\n4. The proposed LongBench-Chat benchmark is a valuable resource for evaluating long context\nunderstanding and instruction-following abilities of large language models.\n5. The paper is well-written and easy to follow, with clear explanations and visuals.\nWeaknesses:\n1. The paper lacks a clear theoretical framework or motivation for why long context alignment is\nimportant and how it differs from short context alignment.\n2. The experiments could benefit from more in-depth analysis and discussion of the results, especially\nin relation to the proposed solutions.\n3. The evaluation on LongBench-Chat is based on a small number of human-annotated examples,\nwhich may not fully capture the diversity of real-world queries.\n4. The paper does not discuss potential limitations or scalability issues with the proposed approach as\nthe model size and context length increase.\n5. The proposed loss weighting strategy is not fully explained and justified, and its impact on training\nstability and convergence could be further investigated.\nThe authors believe LongAlign-13B-64k generates a relatively good review, with only a few flaws. This\ncase also shows the O.O.D. generalization ability of the model \u2014 it can generate reasonable review for a\nresearch paper without encountering such type of data during training.\ng User: Use rap to tell about this paper.\n3 LongAlign-6B-64k:\nThis paper presents a recipe for long context alignment, LongAlign, which addresses the challenges\nof training large language models (LLMs) to handle longer inputs. The key ideas of LongAlign\ninclude:\n17\n1. Data: Collecting diverse long sequences from multiple sources and using Self-Instruct to generate\n10k instruction data of 8k-64k length.\n2. Training method: Considering a more efficient packing strategy that packs sequences together up\nto the maximum length before dispatching them to GPUs for optimization. This is balanced with a\nloss weighting strategy to prevent sequences in packs with fewer sequences from disproportionately\naffecting the loss. Also proposing a sorted batching strategy that groups sequences of similar lengths\nto reduce intra-batch idle time.\n3. Evaluation: Introducing LongBench-Chat, a series of open-ended real-world user questions of\n10k-100k length, covering aspects of instruction-following like reasoning, coding, and summarization,\nas well as being multilingual. GPT-4 is employed to score the model\u2019s responses.\nExperiments demonstrate that LongAlign effectively aligns LLMs to handle long contexts of up to\n64k length without performance degradation on general benchmarks, showing the impact of data\nquantity, diversity, training methods, and evaluation.\n3 LongAlign-7B-64k:\nThis paper presents LongAlign, a recipe for aligning large language models to handle long contexts.\nThe key challenges in long context scaling include lack of a comprehensive instruction-following\ndataset, uneven batching in multi-GPU training, and absence of a reliable benchmark. LongAlign\naddresses these challenges by collecting long instruction data from 9 sources, using Self-Instruct\nto generate 10k instruction data of 8k-64k length, and constructing LongBench-Chat, a series of\nopen-ended real-world user questions of 10k-100k length. The paper also proposes packing and\nsorted batching training methods, loss weighting for packing training, and a benchmark called\nLongBench-Chat to evaluate the aligned models. Experiments show that LongAlign effectively aligns\nmodels to handle contexts of up to 64k without performance degradation on general benchmarks.\n3 LongAlign-13B-64k:\nYo, listen up, I got a story to spit\nAbout a paper that\u2019ll make your head spin and flip\nIt\u2019s called \"LongAlign: A Recipe for Long Context Alignment\"\nAnd it\u2019s all about enhancing large language models\u2019 capabilities\nThey tackle the challenge of scaling up models to handle long contexts\nBy collecting and constructing a diverse dataset of long instruction data\nTraining methods like packing and sorted batching are explored\nTo optimize efficiency and performance without compromising general capabilities\nTheir evaluation benchmark, LongBench-Chat, is a series of real-world user queries\nTo assess models\u2019 ability to follow long context instructions and provide relevant answers\nThey find that incorporating long instruction data during training is crucial And that the proposed\nmethods can speed up training while improving performance\nSo if you\u2019re into language models and their capabilities\nThis paper\u2019s a must-read, it\u2019ll make you think twice\nLongAlign, the recipe for long context alignment\nIs the key to unlocking the full potential of large language models, that\u2019s for sure, it\u2019s a go!\nAlthough LongAlign-6B-64k and LongAlign-7B-64k make good summaries for our paper, only\nLongAlign-13B-64k follows the instruction and summarizes our paper in rap. Its rap is in the right form,\nyet it does not rhyme.\n18\nE\nLimitations\nOur work in exploring long context alignment has its limitations. From a data perspective, we primarily\ncover long instruction data for categories like long context QA, summarization, and reasoning in data\nconstruction. In reality, there are many other types of long instruction tasks that heavily rely on the ability\nto understand extended texts, such as multi-turn dialogues (hundreds or thousands of turns, even life-long\ndialogues), long-term role-playing, and long-history agent tasks, etc. We find that collecting available\ndata for these tasks is challenging because the current performance of LLMs on these tasks does not yet\nmeet human needs. Consequently, users rarely interact with LLMs in this manner. Additionally, since\ncurrent LLMs, whether API-based or open-sourced models, perform poorly on these tasks, it\u2019s difficult to\nautomatically construct such data using a Self-Instruct like approach. We hope to explore more types of\nlong context data, enabling models to align with human expectations across various long context tasks in\nfuture works.\nFrom a training perspective, due to the limitations of the DeepSpeed framework and our GPU resources\nthat only support SFT for 10B level models with a maximum length of 64k, we do not conduct massive\nexperiments on longer data or larger models. Some current frameworks, such as Megatron (Shoeybi et al.,\n2019), support more parallelization methods including model parallelism and sequence parallelism, but\nare difficult to use and reproduce due to the complexity of their code structure. We hope to explore long\ncontext alignment on longer sequences and larger-scale models using more advanced training frameworks.\nAdditionally, exploring RLHF in long context alignment is also a promising direction.\n19\n"
  },
  {
    "title": "Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion",
    "link": "https://arxiv.org/pdf/2401.17583.pdf",
    "upvote": "21",
    "text": "Agile But Safe: Learning Collision-Free\nHigh-Speed Legged Locomotion\nTairan He1\u2020\nChong Zhang2\u2020\nWenli Xiao1\nGuanqi He1\nChangliu Liu1\nGuanya Shi1\n1Carnegie Mellon University\n2ETH Z\u00a8urich\n\u2020Equal Contributions\nhttps://agile-but-safe.github.io\nVavg=2.1 m/s,  Vpeak=2.9 m/s\n V=3.1 m/s\nVavg=1.5 m/s,  Vpeak=2.5 m/s\na\nVavg=2.3 m/s,  Vpeak=3.0 m/s\nb\ne\nf\ng\n V=3.1 m/s\nc\nd\nFig. 1.\nOur proposed framework ABS demonstrates agile and collision-free locomotion capabilities, where the robot, with fully onboard computation and\nsensing, can safely navigate through cluttered environments and rapidly react to diverse and dynamic obstacles, both indoors and outdoors. ABS involves a\ndual-policy setup: green lines at the bottom indicate the agile policy taking control, and red lines indicate the recovery policy in operation. The agile policy\nenables the robot to run fast amidst obstacles, and the recovery policy saves the robot from risky cases where the agile policy might fail.\nSubfigures: (a)\nThe robot dodges a swinging human leg. (b) The agile policy enables the robot to run at a peak speed of 3.1 m/s. (c) The robot dodges a moving stroller\nduring high-speed locomotion. (d) The robot dodges a moving human in snowy terrain. (e) The robot safely navigates in a hall with both static and dynamic\nobstacles, with an average speed of 2.1 m/s and a peak speed of 2.9 m/s. (f) The robot avoids obstacles and moving humans in a dim corridor, with an\naverage speed of 1.5 m/s and a peak speed of 2.5 m/s. (g) The robot, running outdoors at an average speed of 2.3 m/s and a peak speed of 3.0 m/s, avoids\nboth moving and static trash bins and climbs up a grassy slope. Videos: see the website.\nAbstract\u2014Legged robots navigating cluttered environments\nmust be jointly agile for efficient task execution and safe to avoid\ncollisions with obstacles or humans. Existing studies either de-\nvelop conservative controllers (< 1.0 m/s) to ensure safety, or fo-\ncus on agility without considering potentially fatal collisions. This\npaper introduces Agile But Safe (ABS), a learning-based control\nframework that enables agile and collision-free locomotion for\nquadrupedal robots. ABS involves an agile policy to execute agile\nmotor skills amidst obstacles and a recovery policy to prevent\nfailures, collaboratively achieving high-speed and collision-free\nnavigation. The policy switch in ABS is governed by a learned\ncontrol-theoretic reach-avoid value network, which also guides\nthe recovery policy as an objective function, thereby safeguarding\nthe robot in a closed loop. The training process involves the\nlearning of the agile policy, the reach-avoid value network, the\nrecovery policy, and an exteroception representation network, all\nin simulation. These trained modules can be directly deployed in\nthe real world with onboard sensing and computation, leading to\nhigh-speed and collision-free navigation in confined indoor and\noutdoor spaces with both static and dynamic obstacles (Figure 1).\nI. INTRODUCTION\nAgile locomotion of legged robots in cluttered environments\npresents a non-trivial challenge due to the inherent trade-off\nbetween agility and safety, and is crucial for real-world ap-\nplications that require both robustness and efficiency. Existing\nworks typically exhibit limited agility (velocity < 1 m/s) to\nensure safety [35, 17, 7, 22, 27, 67, 12, 46, 41, 72], or focus\nsolely on maximizing agility without considering safety in\nnavigation scenarios [45, 53]. Our work distinguishes itself\nby achieving high-speed (max velocity > 3 m/s), collision-\nfree quadrupedal locomotion in cluttered environments.\nThe agility limitations in existing works in the navigation\ndomain stem from varied factors. Regarding the formulation,\nsome decouple locomotion and navigational planning into two\nsubtasks and build hierarchical systems [35, 17, 7, 27, 46, 72].\nSuch decoupling not only constrains the controller from the\noptimal solution [55], but also results in conservative behaviors\nto ensure safety, thereby limiting the system from fully un-\nleashing the locomotion agility. This work, instead, learns end-\nto-end controllers that directly output joint-level actions for\ncollision-free locomotion to reach specified goal positions. Our\napproach is inspired by recent works [66, 50, 71, 28] where\nrobots learn end-to-end controllers to overcome challenging\nterrains by integrating locomotion with navigation.\narXiv:2401.17583v1  [cs.RO]  31 Jan 2024\nRegarding the controller, some works employ model-based\nmethods with simplified models, such as model predictive\ncontrol (MPC) and barrier functions, for guaranteed safety [22,\n12, 41]. The model mismatch and potential constraint viola-\ntions such as slippage, together with the online computational\nburden, limit these controllers from agile motions and stable\ndeployment in the wild [22, 32, 38]. On the other hand, recent\nprogress of model-free reinforcement learning (RL) in legged\nlocomotion has demonstrated remarkable agile motor skills\nthat model-based controllers have not achieved [45, 53, 28, 38,\n47, 34, 75, 43, 69, 11], although potentially unsafe in cluttered\nenvironments. We harness the flexibility and agility of model-\nfree RL and further safeguard it using control-theoretic tools.\nNamed ABS, our framework goes beyond a single RL\npolicy. First, we have a model-free perceptive agile policy that\nincorporates collision avoidance into locomotion, as presented\nin Section IV, enabling our Go1 robot to achieve peak speeds\nup to 3.1 m/s while being aware of collisions. However, the\nRL policy does not guarantee safety, so we safeguard the robot\nwith another recovery policy (see Section VI) when the agile\npolicy may fail. To decide which policy to take control, we\nuse a policy-conditioned reach-avoid (RA) value network to\nquantify the risk level of the agile policy. This is inspired by\n[29] where model-free RA values can be efficiently learned\nbased on the Hamilton-Jacobi reachability theory [3]. The\nRA value network is trained by a discounted RA Bellman\nequation, with data collected by the learned agile policy in\nsimulation. Beyond being a threshold, the differentiable RA\nvalue network also provides gradient information to guide the\nrecovery policy, thus closing the loop, which will be further\npresented in Section V.\nTo get collision avoidance behaviors that can generalize in\ndifferent scenarios, we use a low-dimensional exteroceptive\nfeature for policy and RA value training: the traveling dis-\ntances of several rays cast from the robot to obstacles. In order\nto achieve this, we additionally train an exteroception represen-\ntation (or ray-prediction) network with simulated data, which\nmaps depth images to ray distances as detailed in Section VII.\nBy doing so, we achieve robust collision avoidance in high-\nspeed locomotion with onboard sensing and computation.\nBriefly, we identify our contributions as follows:\n1) A perceptive agile policy for obstacle avoidance in high-\nspeed locomotion with novel training methods.\n2) A novel control-theoretic data-driven method for RA\nvalue estimation conditioned on the learned agile policy.\n3) A dual-policy setup where an agile policy and a recovery\npolicy collaborates for high-speed collision-free locomo-\ntion, and the RA values govern the policy switch and\nguide the recovery policy.\n4) An exteroception representation network that predicts\nlow-dimensional obstacle information for generalizable\ncollision avoidance capability.\n5) Validation of ABS\u2019s superior safety measures and state-\nof-the-art agility amidst obstacles both indoors and out-\ndoors (Figure 1).\nII. RELATED WORKS\nA. Agile Legged Locomotion\nModel-based methods such as MPC use simplified models\nand handcrafted gaits to enable dynamic legged locomotion [5,\n14, 15, 24, 33, 25]. Despite their impressive performance\nin simulation and under laboratory conditions, these methods\nstruggle in the wild due to model mismatch and unexpected\nslippage [32, 38]. The online computational burden also limits\nperceptive model-based controllers from agile motions [12].\nRecently, RL-based controllers have shown promising re-\nsults for robust locomotion [66, 38, 47, 23] and agile motor\nskills including high-speed running [31, 45, 53], challenging\nterrain traversal [71, 28, 34, 75, 11], jumping [39, 69], and\nfall recovery [66, 43, 62, 70]. However, existing works on\nagile locomotion mostly study how to achieve fast speeds for\nracing or skillful motions to overcome challenging terrains. In\ncluttered environments, these methods necessitate a high-level\nnavigation module for collision avoidance, which is typically\nconservative and greatly constrains the motion far below the\nmotor limit [63, 72]. In contrast, this paper studies agile\ncollision avoidance for versatile navigation.\nB. Legged Collision Avoidance\nClassical methods tackle collision avoidance in legged\nrobots with collision-free motion planning [35, 17, 7] in the\nconfiguration space without considering the robot dynamics,\nleading to slow and statically stable gaits. MPC-based meth-\nods [22, 12, 57, 41] integrate planning and control by treating\ndistances to obstacles as optimization constraints. However,\nthey suffer from the aforementioned drawbacks of model-\nbased controllers and slow movements (velocity < 0.5 m/s).\nLearning-based methods are another choice. Hoeller et al.\n[27] and Zhang et al. [72] train RL-based policies that output\ntwist commands to be tracked by the locomotion controller,\nwhile the velocity commands are bounded by 1 m/s to ensure\nsafety. However, the decoupling of navigation planning and\nlocomotion control makes high-speed locomotion risky, as the\nhigh-level planner is unaware of the low-level tracking error.\nYang et al. [67] instead provides an end-to-end RL-based\nsolution that maps depth images and proprioceptive data\ndirectly to joint actions, but the robot can only walk forward\nand the velocity is limited to \u223c 0.4 m/s. In contrast, our\nwork deploys an end-to-end agile policy for omnidirectional\nrapid locomotion with collision avoidance, and safeguards the\nrobot with RA values and a recovery policy. To the best of\nour knowledge, our work is the first to validate collision-free\nquadruped locomotion with maximum velocity up to 3.1 m/s.\nEven in tight space with dynamic adversarial obstacles, our\nsystem can still reach a peak velocity of 2.5 m/s and an average\nspeed of 1.5 m/s (Figure 1 (f)).\nC. Safe Reinforcement Learning\nThere are two main categories of methods to perform safe\nRL [74]: 1) end-to-end methods and 2) hierarchical methods.\nLagrangian-based methods [49, 4, 40, 58] are the most repre-\nsentative end-to-end safe RL methods that solve a primal-dual\noptimization problem to satisfy safety constraint where the\nLagrange multipliers can be optimized along with the policy\nparameters. However, the constraint is often enforced before\nconvergence, hindering exploration and lowering returns [49].\nHierarchical safe RL methods safeguard unsafe RL actions\nusing structures of underlying dynamics [13, 68, 65] and\ncontrol-theoretic safety certificates [10, 73, 48]. These methods\ntypically build on the assumptions of available dynamics\nor safety certificate functions before learning, which heavily\nlimits the scalability to high-dimensional complex systems.\nSome recent works learn safety prediction networks (or safety\ncritics) and safety backup policies to safeguard RL when the\nsafety critics indicate the nominal policy is unsafe [59, 30].\nNevertheless, these frameworks lack interplay between safety\ncritics and backup policies, relying on the demanding assump-\ntion that the backup policy can restore safety without explicit\noptimization to satisfy the safety critics.\nOur approach aligns with the hierarchical methods, yet it\nstands out with a distinctive strategy. We focus on estimating\nthe reach-avoid values of the agile policy and feed the reach-\navoid values\u2019 gradient information back into the system to\nguide the recovery policy within a closed loop. This innovative\napproach enables a dynamic and adaptive recovery process.\nNotably, all our modules are trained in simulation using\na model-free approach, enhancing the generalizability and\nscalability of our method.\nD. Reach-Avoid Problems and Hamilton-Jacobi Analysis\nReach-avoid (RA) problems involve navigating a system\nto reach a target while avoiding certain undesirable states.\nHamilton-Jacobi (HJ) reachability analysis [3] solves this\nproblem by analyzing the associated Hamilton-Jacobi partial\ndifferential equation, which provides a set of states that the\nsystem must stay out of in order to remain safe.\nHJ reachability analysis faces computational challenges,\nwhich escalate exponentially with the system\u2019s dimensional-\nity [9]. Recent learning-based methods [2] try to scale HJ\nreachability analysis to high-dimensional systems by learning\nvalue networks that satisfy the associated HJ partial differential\nequations and constraints. However, they still require explicit\nsystem Hamiltonian expression before learning.\nOur method builds on another line of works [20, 29] that\nleverage contraction properties to derive a time-discounted\nreach-avoid Bellman equation. However, contrary to previous\nworks that learn policy-agnostic RA values during RL training,\nwe instead learn a policy-conditioned RA value network. This\nnot only reduces the computational burden by avoiding the\nidentifiability issue of the global RA set but also best suits\nour trained agile policy.\nIII. OVERVIEW AND PRELIMINARIES\nA. Nomenclature\nWe present important symbols and abbreviations that are\nused across this paper in Table I for reference.\nTABLE I\nIMPORTANT SYMBOLS AND ABBREVIATIONS\nSymbol\nMeaning\nt\nTime step, converted to time in calculation\ns \u2208 S\nState\na \u2208 A\nAction\no \u2208 O\nObservation\nT\nTime horizon or episode length\n\u03b3RL\nDiscount factor for reinforcement learning\n\u03b3RA\nDiscount factor in reach-avoid Bellman equation\nVthreshold\nReach-avoid value threshold equal to \u2212\u03f5, \u03f5 > 0\n\u03b6(\u00b7)\nFunction indicating failures\nl(\u00b7)\nFunction indicating reaching the target\nV \u03c0\nRA\u2217(\u00b7)\nGround-truth reach-avoid values conditioned on policy\n\u03c0\nV \u03c0\nRA(\u00b7)\nGround-truth discounted reach-avoid values condi-\ntioned on policy \u03c0\n\u02c6V (\u00b7)\nNeural network for discounted policy-conditioned\nreach-avoid value approximation\n\u03c0Agile\nAgile policy\n\u03c0Recovery\nRecovery policy\nv\nLinear velocity in the base frame\n\u03c9\nAngular velocity in the base frame\ncf\nFoot contact statuses\ng\nNormalized projected gravity in the base frame\nq, \u02d9q, \u00a8q\nJoint positions, velocities, and accelerations\n\u03c4\nJoint torques\nR\nLogarithm of ray distances\ndgoal\nDistance from the robot to the goal\nGc\nGoal command\nvc\nLinear velocity command in the base frame\n\u03c9c\nAngular velocity command in the base frame\ntwc\nTwist command\nReLU(\u00b7)\nFunction clipping negative values to zero [21]\nAbbreviation\nFull Form\nABS\nAgile but safe\nRA\nReach-avoid\nRL\nReinforcement learning\nMPC\nModel predictive control\nMLP\nMultilayer perceptron\nB. Problem Formulation\n1) Dynamics: Let st \u2208 S \u2282 Rns be the state at time step t,\nwhere ns is the dimension of the state space S; at \u2208 A \u2282 Rna\nbe the control input at time step t, where na is the dimension\nof the action space A. The system dynamics are defined as:\nst+1 = f(st, at),\n(1)\nwhere f : S \u00d7 A \u2192 S is a function that maps the current\nrobot state and control to the next state. For simplicity, this\npaper considers deterministic dynamics that can be without\nan analytical form. We denote the robot observations from\nproprioception and/or exteroception as ot = h(st) where h :\nS \u2192 O is the sensor mapping. Detailed observation and action\nspace of the agile policy and recovery policy will be introduced\nin Section IV and Section VI.\n2) Goal and Policy: Goal-conditioned reinforcement learn-\ning [42] learns to reach goal states G \u2208 \u0393 via a goal-\nconditioned policy \u03c0 : O \u00d7 \u0393 \u2192 A. With the reward function\nr : S \u00d7 A \u00d7 \u0393 \u2192 R and the discount factor \u03b3RL. The policy is\nlearned to maximize the expected cumulative return over the\nRecovery \nPolicy\nAgile \nPolicy\nRay-Prediction\nNetwork\nAgile \nPolicy\nProprioception\nExteroception\nSimulation\nJoint Targets\nRL Optimizer\nNavigation \nCommand\nRecovery\nPolicy\nJoint Targets\nRL Optimizer\nTwist\nCommand\nProprioception\nRandom \nSampling\n(i) Stage 1: Policy training.\n(ii) Stage 2: Network training from agile policy rollout data.\nAgile \nPolicy\nSimulation\nRollout\nReach-Avoid\nDataset\nPerception\nDataset\nReach-Avoid\nValue Network\nRay-Prediction\nNetwork\nTrain\n(\n)\n,\n(\n)\n,\nTrain\n(a) Training\n(b) Deployment\nNavigation \nCommand\nState\nEstimator\nReach-Avoid Value Network \nProprioception\nExteroception\nProprioception Exteroception\nNavigation \nCommand\nTwist\nCommand\n(\n)\n,\nReach/\nFailure/\nSafe\nJoint Targets\nPD Controller\nGround Truth\nRobot\n50Hz\n50Hz\n40Hz\n50Hz\nTwist\nCommand\nOnline \nSearch for \nSafe Twist\n200Hz\nFig. 2.\nOverview of ABS: (a) There are four trained modules within the ABS framework: 1) Agile Policy (introduced in Section IV) is trained to achieve\nthe maximum agility amidst obstacles; 2) Reach-Avoid Value Network (introduced in Section V) is trained to predict the RA values conditioned on the agile\npolicy as safety indicators; 3) Recovery Policy (introduced in Section VI) is trained to track desired twist commands (2D linear velocity vc\nx, vc\ny and yaw\nangular velocity \u03c9c\nz) that lower the RA values; 4) Ray-Prediction Network (introduced in Section VII) is trained to predict ray distances as the policies\u2019\nexteroceptive inputs given depth images. (b) Illustration of the ABS deployment architecture. The dual policy setup switches between the agile policy and\nthe recovery policy based on the estimated \u02c6V from the RA value network: 1) if \u02c6V < Vthreshold, the agile policy is activated to navigate amidst obstacles; 2) if\n\u02c6V \u2265 Vthreshold, the recovery policy is activated to track twist commands that lower the RA values via constrained optimization.\ngoal distribution pG:\nJ(\u03c0) = Eat\u223c\u03c0(\u00b7|ot,G),G\u223cpG\n\"X\nt\n\u03b3t\nRLr(st, at, G)\n#\n.\n(2)\n3) Failure Set, Target Set and Reach-Avoid Set: We denote\nthe failure set F \u2286 S as unsafe states (e.g., collision) where\nthe robot is not allowed to enter. The failure set can be\nrepresented by the zero-sublevel set of a Lipschitz-continuous\nfunction \u03b6 : S \u2192 R, i.e., s \u2208 F \u21d4 \u03b6(s) > 0. The target\nset \u0398 \u2282 S is defined as desired states (i.e., goal states).\nSimilarly, the target set can be represented by the zero-sublevel\nset of a Lipschitz-continuous function l : S \u2192 R, i.e.,\ns \u2208 \u0398 \u21d4 l(s) \u2264 0. We denote \u03be\u03c0\nst(\u00b7) as the future trajectory\nrollout from state st (\u03be\u03c0\nst(0) = st) using policy \u03c0 up to sT .\nThe reach-avoid set conditioned on policy \u03c0 is defined as\nRA\u03c0(\u0398; F) := {st \u2208 S |\u03be\u03c0\nst(T \u2212 t) \u2208 \u0398\u2227\n\u2200t\u2032 \u2208 [0, T \u2212 t], \u03be\u03c0\nst(t\u2032) /\u2208 F}, (3)\nwhich represents the set of states governed by policy \u03c0 capable\nof leading the system to \u0398 while consistently avoiding F in\nall prior timesteps.\n4) Reach-Avoid Value and Time-Discounted Reach-Avoid\nBellman Equation: We define policy-conditioned reach-avoid\nvalues as: V \u03c0\nRA\u2217(s) \u2264 0 \u21d4 s \u2208 RA\u03c0(\u0398; F). Following the\nproof (Appendix A in [29]), it can be easily extended that value\nfunction V \u03c0\nRA\u2217(s) satisfies the fixed-point reach-avoid Bellman\nequation (our policy-conditioned value function is a special\ncase of the general value function):\nV \u03c0\nRA\u2217(s) = max\nn\n\u03b6(s), min\n\b\nl(s), V \u03c0\nRA\u2217 (f (s, \u03c0(s)))\n\to\n.\n(4)\nHowever, there is no assurance that Equation (4) will result\nin a contraction in the space of value functions. To make it\naccessible to data-driven approximation, we leverage time-\ndiscounted reach-avoid Bellman equation [29] to make a\ncontraction on the discounted policy-conditioned reach-avoid\nvalues V \u03c0\nRA(s) defined as\nV \u03c0\nRA(s) =\u03b3RA max\nn\n\u03b6(s), min\n\b\nl(s), V \u03c0\nRA (f (s, \u03c0(s)))\n\to\n+ (1 \u2212 \u03b3RA) max\n\b\nl(s), \u03b6(s)\n\t\n.\n(5)\nFollowing [29], it can be shown that V \u03c0\nRA(s) is always\nan under-approximation of V \u03c0\nRA\u2217(s) for \u03b3RA \u2208 [0, 1), and\nV \u03c0\nRA(s) converges to V \u03c0\nRA\u2217(s) as \u03b3RA approaches 1. Note that\nthe under-approximation of V \u03c0\nRA(s) to V \u03c0\nRA\u2217(s) means that\nV \u03c0\nRA(s) \u2264 0 \u21d2 s \u2208 RA\u03c0(\u0398; F), which enables that shielding\nmethods on thresholds of V \u03c0\nRA(s) could make the system stay\nin the control-theoretic reach-avoid set RA\u03c0(\u0398; F).\nC. System Structure\nAs shown in Figure 2, our proposed ABS framework\ninvolves a dual-policy setup where the agile policy \u03c0Agile and\nthe recovery policy \u03c0Recovery work together to enable agile and\nsafe locomotion skills. The agile policy performs agile motor\nskills (up to 3.1 m/s on Unitree Go1) to navigate the robot\nbased on goal commands (target 2D positions and headings)\nwith basic collision-avoidance ability (see also Section IV).\nThe recovery policy is responsible for safeguarding the agile\npolicy by rapidly tracking twist commands (2D linear velocity\nvc\nx, vc\ny and yaw rate \u03c9c\nz) that can avoid collisions (see also\nSection V-C and Section VI). Both policies output joint targets\nthat are tracked by a PD controller.\nDuring deployment, the policy switch is governed by RA\nvalues conditioned on the agile policy, estimated using a neural\nnetwork \u02c6V (see also Section V). With a safety threshold\nVthreshold = \u2212\u03f5 where \u03f5 is a small positive number, we have:\n\u2022 If \u02c6V \u2265 Vthreshold, we search for a twist command that\ndrives the robot closer to the goal while maintaining\nsafety based on \u02c6V (see also Equation (21)). The recovery\npolicy takes control and tracks the searched twist com-\nmand.\n\u2022 If \u02c6V < Vthreshold, the agile policy takes control.\nWe expect the system to activate the agile policy in most time,\nand use the recovery policy as a safeguard in risky situations\nuntil it is safe again for the agile policy, i.e., \u02c6V < Vthreshold.\nFor collision avoidance, both the agile policy and the RA\nvalue networks need exteroceptive inputs. Inspired by [28,\n16, 1], we choose to use a low-dimensional exteroception\nrepresentation: the distances that 11 rays travel from the robot\nto obstacles, similar to sparse LiDAR readings. We train a\nnetwork that maps raw depth images to predicted ray distances\n(see also Section VII), and the ray distances serve as part of\nthe observations for the agile policy and the RA value network.\nTo summarize, as shown in Figure 2 (a), ABS needs to train\nfour modules all in simulation:\n1) The agile policy (Section IV) is trained via RL to reach\nthe goal without collisions. We design goal-reaching\nrewards to encourage the most agile motor skills.\n2) The RA value network (Section V) is trained to indicate\nthe safety for the agile policy. We use a data-driven\nmethod to train it based on the RA bellman equation\n(Equation (5)), and collect data in simulation by rolling\nout the agile policy.\n3) The recovery policy (Section VI) is trained to track twist\ncommands rapidly from high-speed movements.\n4) The ray-prediction network (Section VII) is trained to\npredict ray distance observations from depth images.\nWe collect synthetic depth images and ray distances in\nsimulation by rolling out the agile policy.\nAll of the four modules are directly deployed in the real world\nafter training.\nIV. LEARNING AGILE POLICY\nAs mentioned in Section III-C, we train an agile policy to\nachieve high agility amidst obstacles. Previous works on learn-\ning agile locomotion typically employ the velocity-tracking\nformulation [45, 53], i.e., to track velocity commands on\nopen, flat terrains. However, designing a navigation planner\nfor these velocity-tracking policies in cluttered environments\ncan be non-trivial. To ensure safety, the planner may have to\nbe conservative and unable to fully unleash the locomotion\npolicy\u2019s agility.\nInstead, we use the goal-reaching formulation to maximize\nthe agility, inspired by [50, 71]. Specifically, we train the robot\nto develop sensorimotor skills that enable it to reach specified\ngoals within the episode time without collisions. The agility\nis also encouraged by a reward term pursuing high velocity\nin the base frame. By doing so, the robot naturally learns to\nachieve maximum agility while avoiding collisions.\nThis section presents the details of our agile policy learning.\nA detailed comparison between goal-reaching and velocity-\ntracking formulations for agility will be presented in Sec-\ntion IX-A1.\nA. Observation Space and Action Space\nThe observation space of the agile policy consists of the\nfoot contacts cf\u2208{1,2,3,4}, the base angular velocities \u03c9, the\nprojected gravity in the base frame g, the goal commands Gc\n(i.e., the relative position and heading of the goal) in the base\nframe, the time left of the episode T \u2212 t, the joint positions\nq, the joint velocities \u02d9q, the actions a of the previous frame,\nand the exteroception (i.e., log values of the ray distances) R.\nHere we omit the step-based timestamps (t \u2212 1 for the actions\nand t otherwise) for brevity. We refer to the collection of all\nthese variables as oAgile.\nAmong these observations, only g and Gc require the\nstate estimators for respectively orientation and odometry.\nAll other values are available from raw sensor data without\ncumulative drifts. The IMU-based orientation estimation for g\n(i.e., roll and pitch) is usually very accurate, and our policy can\neffectively handle the odometry drift (as we can even suddenly\nchange the goals in the run, see Section IX-C). Therefore, our\nagile policy is robust to inaccurate state estimators which can\nbe problematic for model-based controllers [6, 32, 18].\nThe action space of the agile policy consists of 12-d\njoint targets. A PD controller tracks these joint targets a by\nconverting them to joint torques:\n\u03c4 = Kp(a \u2212 q) \u2212 Kd \u02d9q.\n(6)\nA fully-connected MLP maps the observations oAgile to the\nactions a.\nB. Rewards\nOur reward function is the summation of multiple terms:\nr = rpenalty + rtask + rregularization,\n(7)\nwhere each term can be further divided into subterms as\nfollows.\n1) Penalty Rewards: We use a simple penalty design:\nrpenalty = \u2212100 \u00b7 1(undesired collision),\n(8)\nwhere undesired collision refers to collisions on the base,\nthighs, and calves, and horizontal collisions on the feet.\n2) Task Rewards: The task rewards are:\nrtask = 60 \u00b7 rpossoft + 60 \u00b7 rpostight + 30 \u00b7 rheading\n\u2212 10 \u00b7 rstand + 10 \u00b7 ragile \u2212 20 \u00b7 rstall,\n(9)\ni.e., a soft position tracking term rpossoft to encourage the\nexploration for goal reaching, a tight position tracking term\nrpostight to reinforce the robot to stop at the goal, a heading\ntracking term rheading to regulate the robot\u2019s heading near the\ngoal, a standing term rstand to encourage a standing posture at\nthe goal, an agile term ragile to encourage high velocities, and a\nstall term rstall to penalize the waiting behaviors. These terms\nensure that the robot should reach the goal with appropriate\nheading and posture as fast as possible while wasting no time.\nTo be specific, our tracking terms (rpossoft, rpostight, rheading)\nare in the same form as shown below, inspired by [72] where\nRL-based navigation planners are learned:\nrtrack (possoft/postight/heading) =\n1\n1 +\n\r\r error\n\u03c3\n\r\r2 \u00b7 1(t > T \u2212 Tr)\nTr\n,\n(10)\nwhere \u03c3 normalizes the tracking errors, T is the episode\nlength, and Tr is a time threshold. By doing so, the robot\nonly needs to reach the goal before T \u2212 Tr to maximize the\ntracking rewards, free from explicit motion constraints such as\ntarget velocities that may limit the agility. For the soft position\ntracking, we have \u03c3soft = 2 m and Tr = 2 s with the error\nbeing the distance to the goal. For the tight position tracking,\nwe have \u03c3tight = 0.5 m and Tr = 1 s. For the heading tracking,\nwe have \u03c3heading = 1 rad and Tr = 2 s with the error being\nthe relative yaw angle to the goal heading. We further disable\nrheading when the distance to the goal is larger than \u03c3soft so\nthat collision avoidance is not affected.\nThe standing term is defined as\nrstand = \u2225q \u2212 \u00afq\u22251 \u00b7 1(t > T \u2212 Tr,stand)\nTr,stand\n\u00b71(dgoal < \u03c3tight), (11)\nwhere \u00afq is the nominal joint positions for standing, Tr,stand =\n1 s, and dgoal is the distance to the goal.\nThe agile term is the core term that encourages the agile\nlocomotion. It is defined as\nragile = max\n\b\nReLU( vx\nvmax\n) \u00b7 1(correct direction),\n1(dgoal < \u03c3tight)\n\t\n,\n(12)\nwhere vx is the forward velocity in the robot base frame,\nvmax = 4.5 m/s is an upper bound of vx that cannot be\nreached, and the \u201ccorrect direction\u201d means that the angle\nbetween the robot heading and the robot-goal line is smaller\nthan 105\u25e6. To maximize this term, the robot has to either run\nfast or stay at the goal.\nThe stall term rstall is 1 if the robot stays static when dgoal >\n\u03c3soft and the robot is not in the \u201ccorrect direction\u201d. This term\npenalizes the robot for time waste.\nFig. 3.\nExample training environments. The magenta points indicate the\ngoals, and the bluegreen lines indicate the exteroceptive ray observations.\nTerrains from left to right: flat, low stumbling blocks, and rough.\n3) Regularization Rewards: The regularization rewards are:\nrregularization = \u2212 2 \u00b7 v2\nz \u2212 0.05 \u00b7 (\u03c92\nx + \u03c92\ny) \u2212 20 \u00b7 (g2\nx + g2\ny)\n\u2212 0.0005 \u00b7 \u2225\u03c4\u22252\n2 \u2212 20 \u00b7\nX12\ni=1 ReLU (|\u03c4i| \u2212 0.85 \u00b7 \u03c4i,lim)\n\u2212 0.0005 \u00b7 \u2225 \u02d9q\u22252\n2 \u2212 20 \u00b7\nX12\ni=1 ReLU (| \u02d9qi| \u2212 0.9 \u00b7 \u02d9qi,lim)\n\u2212 20 \u00b7\nX12\ni=1 ReLU (|qi| \u2212 0.95 \u00b7 qi,lim)\n\u2212 2 \u00d7 10\u22127 \u00b7 \u2225\u00a8q\u22252\n2 \u2212 4 \u00d7 10\u22126 \u00b7 \u2225\u02d9a\u22252\n2 \u2212 20 \u00b7 1(fly),\n(13)\nwhere \u03c4 is the joint torques, \u03c4lim is the hardware torque limits,\n\u02d9qlim is the hardware joint velocity limits, qlim is the hardware\njoint position limits, and \u201cfly\u201d refers to when the robot has no\ncontact with the ground. We penalize the \u201cfly\u201d cases as they\nmake the robot base uncontrollable, threatening the system\u2019s\nsafety.\nC. Training in Simulation\n1) Simulator: We use the GPU-based Isaac Gym simu-\nlator [44] which supports us to train 1280 environments in\nparallel with the PPO algorithm [52].\n2) Terrains: We train the agile policy with randomized\nterrains following a curriculum to facilitate learning. To pre-\nvent unstable gaits that over-exploits the simulation dynamics,\nthe terrains are randomly sampled to be flat, rough, or low\nstumbling blocks, as shown in Figure 3. As the difficulty level\ngoes up from 0 to 9, the rough terrains and the stumbling\nblocks have larger height difference from 0 cm to 7 cm.\n3) Obstacles: We train the policy with cylinders of 40 cm\nradius. For each episode we have 0\u223c8 obstacles randomly\ndistributed in a 11 m\u00d7 5 m rectangle that covers the origin\nand the goal. To facilitate learning, we also apply a curriculum\nwhere higher difficulty levels have more obstacles.\n4) Domain Randomization:\nWe do domain randomiza-\ntion [60] for sim-to-real transfer. The randomized settings\nare listed in Table II. Among these few terms, two are\ncritical: the illusion, and the ERFI-50. The illusion makes\nthe policy more robust to unseen geometries such as walls:\nit overwrites the observed ray distances by random values\nsubject to U(dgoal + 0.3, ray distance) if they are larger than\ndgoal + 0.3. The ERFI-50 proposed by Campanaro et al. [8]\nimplicitly models the motor sim-to-real gaps with random\ntorque perturbations, and we add a curriculum in our work to\navoid impeding the early stage of learning. We also randomly\nbias the joint positions to model the motor encoders\u2019 offset\nerrors.\nTABLE II\nDOMAIN RANDOMIZATION SETTINGS FOR AGILE POLICY TRAINING\nTerm\nValue\nObservation\nIllusion\nEnabled\nJoint position noise\nU(\u22120.01, 0.01) rad\nJoint velocity noise\nU(\u22121.5, 1.5) rad/s\nAngular velocity noise\nU(\u22120.2, 0.2) rad/s\nProjected gravity noise\nU(\u22120.05, 0.05)\nlog(ray distance) noise\nU(\u22120.2, 0.2)\nDynamics\nERFI-50 [8]\n0.78 N m\u00d7 difficulty level\nFriction factor\nU(0.4, 1.1)\nAdded base mass\nU(\u22121.5, 1.5) kg\nJoint position biases\nU(\u22120.08, 0.08) rad\nEpisode\nEpisode length\nU(7.0, 9.0) s\nInitial robot position\nx = 0, y = 0\nInitial robot yaw\nU(\u2212\u03c0, \u03c0) rad\nInitial robot twist\nU(\u22120.5, 0.5) m/s or rad/s\nGoal Position\nxgoal \u223c U(1.5, 7.5) m\nygoal \u223c U(\u22122.0, 2.0) m\nGoal Heading\narctan 2(ygoal, xgoal) + U(\u22120.3, 0.3) rad\n5) Curriculum: As mentioned above, we apply a curricu-\nlum where difficulty levels can change the terrains, the obstacle\ndistribution, and the domain randomization. For the assign-\nment of difficulty levels, we follow the design of Zhang et al.\n[71]: when an episode terminates, the robot gets promoted to\na higher level if dgoal < \u03c3tight, and gets demoted to a lower\nlevel if dgoal > \u03c3soft. If the robot gets promoted at the highest\nlevel, it will go to a random level, following [51].\nV. LEARNING AND USING REACH-AVOID VALUES\nAlthough the agile policy learns certain collision avoidance\nbehaviors via corresponding rewards, it does not ensure safety.\nTo safeguard the robot, we propose to use RA values to predict\nthe failures, and then a recovery policy can save the robot\nbased on the RA values.\nInspired by Hsu et al. [29], we learn RA values in a model-\nfree way, contrasting typical approaches of model-based reach-\nability analysis [2]. This better suits the model-free RL-based\npolicies. Also different from [29], we do not learn the global\nRA values, but make it policy-conditioned, as mentioned in\nSection III-B4. The learned RA value function will predict\nonly the agile policy\u2019s failures based on the observations.\nA. Learning RA Values\nTo avoid overfitting in high dimensions and make the RA\nvalues generalize, we use a reduced set of observations as the\ninputs of the RA value function:\noRA =\n\u0002\n[v; \u03c9] ; Gc\nx,y; R\n\u0003\n,\n(14)\ni.e., the base twists, the goal (x, y) position in the robot frame,\nand the exteroception. These components are centroidal obser-\nvations that significantly influence safety and goal reaching.\nOn the other hand, we don\u2019t use joint-level observations (such\nas q and \u02d9q) here because they are high-dimensional and less\npertinent to goal reaching. We train an RA value network \u02c6V\nto approximate the RA values:\nV \u03c0Agile\nRA\n(s) \u2248 \u02c6V (oRA).\n(15)\nBased on Equation (5), we minimize the following loss for\neach episode with gradient descent:\nL = 1\nT\nT\nX\nt=1\n\u0010\n\u02c6V (oRA\nt ) \u2212 \u02c6V target\u00112\n,\n(16)\nwhere\n\u02c6V target =\u03b3RA max\nn\n\u03b6(st), min\n\b\nl(st), \u02c6V old(oRA\nt+1)\n\to\n+ (1 \u2212 \u03b3RA) max\n\b\nl(st), \u03b6(st)\n\t\n,\n(17)\nand we set the discount factor \u03b3RA = 0.999999 to best\napproximate RA\u03c0(\u0398; F) since V \u03c0\nRA(s) converges to V \u03c0\nRA\u2217(s)\nas \u03b3RA approaches 1. \u02c6V old refers to \u02c6V from previous iteration,\nand we set \u02c6V old(oRA\nT +1) = +\u221e.\nDiffering from [29], our approach learns policy-conditioned\nreach-avoid values instead of solving policy-agnostic global\nreach-avoid value of the entire system dynamics which in-\nvolves another value minimization problem over A. Our\nmethod offers several advantages: 1) simplicity: as highlighted\nin Equation (5), this simplicity arises from avoiding the\nneed to solve for the lowest value of the next state across\nthe entire action space. 2) two-stage offline learning: our\napproach can be learned in a two-stage offline manner. This\ninvolves first collecting policy trajectories and then training the\npolicy-conditioned reach-avoid value. This two-stage process\nenhances stability compared to the online training method\npresented in [29].\nB. Implementation\nAccording to [29], l(s) and \u03b6(s) should be Lipschitz-\ncontinuous for theoretical guarantees. In our implementation,\nwe define the l(s) as\nl(s) = tanh log dgoal\n\u03c3tight\n,\n(18)\nthereby making it Lipschitz-continuous, bounding it with\n(\u22121, 1), and setting dgoal \u2264 \u03c3tight as \u201creach\u201d.\nRegarding failures, we naturally have\n\u03b6(s) = 2 \u2217 1(undesired collision) \u2212 1.\n(19)\nHowever, this definition violates the Lipschitz continuity.\nHence, we soften the function in a hindsight way: when\nan undesired collision happens, the \u03b6 values for the last 10\ntimesteps are relabelled to be \u22120.8, \u22120.6, . . . , 0.8, 1.0.\nFor RA dataset sampling, we make the obstacles distribute\nas in the highest difficulty level during the training of the agile\npolicy. We roll out our trained agile policy for 200k episodes,\nand collect these trajectories for RA learning.\nFigure 4 visualizes the learned RA values for a specific set\nof obstacles. As the robot\u2019s velocity changes, the landscape\nof RA values changes accordingly. The sign of the RA values\nreasonably indicates the safety for the agile policy.\n1m/s\n1m/s\n2.5m/s\n4m/s\nFig. 4.\nVisualization of \u02c6V with different linear velocities and 2D positions\nrelative to the 3 fixed obstacles. The angular velocities are set to zero, and\nthe relative goal commands are set to 5 m ahead of the robot. The grey\ncircles represent the obstacles, and the colors represent the values of \u02c6V at\ncorresponding 2D positions.\nC. Using RA Values for Recovery\nRA values provide a failure prediction conditioned on the\nagile policy, and we propose to use RA values to guide\nthe recovery policy. To be specific, the robot decides the\noptimal twist to avoid collisions using the RA value func-\ntion, and employs the recovery policy to track these twist\ncommands. The recovery policy is triggered as a back-up\nshielding policy if and only if \u02c6V (oRA) \u2265 Vthreshold. We set\nVthreshold = \u22120.05 to compensate for learning errors without\ncausing over-conservative shielding.\nDuring recovery, we assume that the recovery policy is well-\ntrained so that the robot twist is close to the command\ntwc = [vc\nx, vc\ny, 0, 0, 0, \u03c9c\nz],\n(20)\nand the robot should try to get closer to the goal if its twist is\nsafe given the goal and the exteroception. Therefore, the twist\ncommand is obtained from the optimization:\ntwc = arg min dfuture\ngoal\ns.t. \u02c6V ([twc; Gc\nx,y; R]) < Vthreshold, (21)\nand dfuture\ngoal\nrefers to the approximate distance to the goal after\ntracking the twist command for a small amount of time \u03b4t =\n0.05 s. This is calculated based on the linearized integral of\nthe robot displacement in the base frame:\n\u03b4x = vc\nx\u03b4t \u2212 0.5vc\ny\u03c9c\nz\u03b4t2,\n\u03b4y = vc\ny\u03b4t + 0.5vc\nx\u03c9c\nz\u03b4t2.\n(22)\nIn our practice, gradient descent with a Lagrangian multiplier\non the constraint can solve Equation (21) within 5 steps when\ninitialized with the current twist, thereby enabling real-time\ndeployment. A visualization of the twist optimization process\nis given in Figure 8 where the searched twist consistently\nsatisfies the safety constraint (i.e., \u02c6V < Vthreshold).\nVI. LEARNING RECOVERY POLICY\nThe recovery policy is intended to make the robot track a\ngiven twist command as fast as possible so that it can function\nas a backup shielding policy, as mentioned in Section V.\nA. Observation Space and Action Space\nThe observation space of the recovery policy differs from\nthe agile policy in that it tracks twist commands and it does\nnot need exteroception. The recovery policy\u2019s observation oRec\nconsists of: the foot contacts cf, the base angular velocities \u03c9,\nthe projected gravity in the base frame g, the twist commands\ntwc (only non-zero variables), the joint positions q, the joint\nvelocities \u02d9q, and the actions a of the previous frame.\nThe action space of the recovery policy is exactly the same\nas that of the agile policy: the 12-d joint targets. We also use\nan MLP as the policy network.\nB. Rewards\nSimilar to the agile policy, the reward functions for the\nrecovery policy also consist of three parts: the penalty re-\nwards, the task rewards, and the regularization rewards. The\nregularization rewards and the penalty rewards remain the\nsame, except that we allow knee contacts with the ground\nfor maximum deceleration (e.g., Figure 1 (a)).\nThe task rewards are for twist tracking:\nrtask = 10 \u00b7 rlinvel \u2212 0.5 \u00b7 rangvel + 5 \u00b7 ralive \u2212 0.1 \u00b7 rposture, (23)\ni.e., a term for tracking vc\nx and vc\ny, a term for tracking \u03c9c\nz, a\nterm for staying alive, and a term for maintaining a posture to\nseamlessly switch back to the agile policy.\nTo be specific, we have\nrlinvel = exp\n\"\n\u2212(vx \u2212 vc\nx)2 + (vy \u2212 vc\ny)2\n\u03c32\nlinvel\n#\n,\n(24)\nwhere we set \u03c3linvel = 0.5 m/s. For the angular velocity,\nrangvel = \u2225\u03c9z \u2212 \u03c9c\nz\u22252\n2,\n(25)\nwhich provides a softer landscape near the command than\nrlinvel. The alive term is simply\nralive = 1 \u00b7 1(alive).\n(26)\nThe posture term is\nrposture = \u2225q \u2212 \u00afqrec\u22251,\n(27)\nwhere \u00afqrec is a nominal standing pose with low height allowing\nthe robot to switch back to the agile policy seamlessly.\nC. Training in Simulation\nThe simulation settings for training the recovery policy are\nsimilar to those for the agile policy. The differences lie in:\n1) Domain Randomization: The observation noises and the\ndynamic randomization do not change. The episode length\nis changed to 2 s, and there are randomized initial roll and\npitch angles subject to U(\u2212\u03c0/6, \u03c0/6) rad. The randomization\nranges are also changed for initial vx \u223c U(\u22120.5, 5.5) m/s\nand initial \u03c9 \u223c U(\u22121.0, 1.0) rad/s. These changes better\naccommodate the states that can trigger the recovery policy\nduring the agile running. The ranges of sampling commands\nare vc\nx \u223c U(\u22121.5, 1.5) m/s, vc\ny \u223c U(\u22120.3, 0.3) m/s, and\n\u03c9c\nz \u223c U(\u22123.0, 3.0) rad/s.\n2) Curriculum: The curriculum still exists for terrains and\ndomain randomization. However, the assignment is changed:\nthe robot gets promoted if the velocity tracking error is smaller\nthan 0.7\u03c3linvel, and gets demoted if it falls over.\nVase\nHuman\nCone\nCylinder\nDining Chair\nOaktree\nOffice Chair\nFold Chair\nFig. 5.\nVarious obstacles used for ray-prediction data collection.\nOriginal Image\nHorizontal Flip\nRandom Erase\nGaussian Blur\nGaussian Noise\nFig. 6.\nIllustration of four kinds of image augmentation used for depth-based\nray-prediction training.\nVII. PERCEPTION\nAs mentioned in Section IV and Section V, both the agile\npolicy and the RA value network use the exteroceptive 11-d\nray distances as part of the observations, with access to their\nground truth values during training. These rays are horizontally\ncast from the robot base, with directions evenly spaced in\n[\u2212 \u03c0\n4 , \u03c0\n4 ].\nHowever, such ray distances are not directly available during\ndeployment, and we need to train a ray-prediction network to\npredict them from depth images, as mentioned in Section III-C.\nSuch a design leads to the following benefits:\n1) We only need to tune the ray-prediction network to\nhandle high-dimensional image noises by data augmen-\ntation.\n2) The representation is highly interpretable, allowing hu-\nmans to supervise.\n3) The agile policy and the RA value network are easier to\ntrain with low-dimensional inputs.\n4) Compared to costly image rendering in simulation, the\nray distances are easy to compute and save training time.\nBesides, although ray distances are similar to sparse LiDAR\nreadings, we use cameras instead of LiDARs because a\nlightweight low-cost camera can easily reach a high FPS,\nwhich is important in high-speed collision avoidance.\nWe present details for training the ray-prediction network\nin this section.\nA. Data Collection\nTo train our ray-prediction network, we collect a dataset\nof pairs of depth images and ray distances (as shown in\nFigure 2 (a)) by running the agile policy in simulation. The\nray-prediction network can then be trained in a supervised\nway. To facilitate generalization, as shown in Figure 5, we\nreplace the cylinders with objects of different shapes during\ndata collection.\nB. Data Augmentation for Sim-to-Real Transfer\nThe real-world depth images collected from cameras are far\nmore noisy than the rendered depth images in simulation [27].\nTo make the ray-prediction network adapt better to real-world\ndepth images, we apply four data augmentation techniques\nduring training, as shown in Figure 6: 1) horizontal flip;\n2) random erase; 3) Gaussian blur; 4) Gaussian noise. For\ndeployment, we apply hole filling [56] to further reduce the\ngap of depth images between the simulation and the real world.\nC. Other Implementation Details\nTo make the network focus more on close obstacles, we\ntake the logarithm of depth values as the NN inputs, and\nthe logarithm of ray distances as the outputs, with the mean\nsquared error as the loss function.\nWe finetune ResNet-18 [26] with pretrained weights to\ntrain the model. The images are downsampled to [160, 90]\nresolution both in simulation and during deployment.\nVIII. EXPERIMENTS\nA. Baselines\nFor experimental results, we consider three settings:\n1) Our ABS system, with both the agile policy and the\nrecovery policy;\n2) Our agile policy \u03c0Agile only;\n3) \u201cLAG\u201d: we use PPO-Lagrangian [49] to train end-to-end\nsafe RL policies with the agile policy\u2019s formulation.\nBy comparing (2) and (3), we can see how agility and safety\ntrade off without external modules, forming a boundary of\nagility and safety. With the help of RA values and the recovery\npolicy, we expect (1) to break this boundary with a high safety\ngain: it should be as agile as (2) in safe cases, and shield the\nrobot in risky cases.\nNote that the three settings here are all based on what we\npropose. A detailed comparison between our agile policy and\nthe previous state-of-the-art (SOTA) agile running policy [45]\nis made in Section IX-A1.\nB. Simulation Experiments\n1) Quantitative results: We test the policies trained with\ndifferent settings in simulation. To better show the agility-\nsafety boundary, we introduce 3 variants for each setting: an\naggressive one (\u201c-a\u201d) doubling the agile reward term ragile, a\nnominal one (\u201c-n\u201d), and a conservative one (\u201c-c\u201d) halving the\nragile. Regarding the obstacles, we distribute eight obstacles\nwithin a 5.5 m\u00d7 4 m rectangle (during training it was 11 m\u00d7\n5 m), so the test cases are in distribution but much harder than\nmost cases during training.\nThe results are reported in Table III and Figure 7. There are\nthree possible outcomes for an episode: success, collision, or\ntimeout. Trajectories that do not trigger success or collision\ncriteria within the episode length are labelled as \u201ctimeout\u201d.\nWe report the success rate, the collision rate, the timeout rate,\nthe average peak velocity for success cases, and the average\nspeed for success cases as the metrics. For each setting, the\nTABLE III\nBENCHMARKED COMPARISON IN SIMULATION\nSuccess Rate (%)\nCollision Rate (%)\nTimeout Rate (%)\n\u00afvpeak of Success (m/s)\n\u00afv of Success (m/s)\nABS-a\n78.9\u00b11.4\n4.4\u00b10.5\n16.7\u00b11.9\n3.74\u00b10.02\n2.15\u00b10.04\nABS-n\n79.1\u00b14.4\n5.7\u00b12.9\n15.2\u00b12.1\n3.48\u00b10.06\n2.08\u00b10.01\nABS-c\n85.8\u00b15.6\n2.9\u00b10.7\n11.3\u00b15.1\n2.98\u00b10.12\n1.87\u00b10.03\n\u03c0Agile-a\n73.3\u00b14.3\n26.1\u00b14.4\n0.6\u00b10.1\n3.83\u00b10.03\n2.55\u00b10.03\n\u03c0Agile-n\n77.3\u00b14.2\n21.7\u00b13.9\n1.0\u00b10.4\n3.55\u00b10.04\n2.39\u00b10.04\n\u03c0Agile-c\n83.2\u00b11.7\n15.5\u00b12.0\n1.3\u00b10.6\n3.04\u00b10.13\n2.04\u00b10.08\nLAG-a\n82.5\u00b16.0\n10.9\u00b12.6\n6.6\u00b14.5\n2.70\u00b10.13\n1.69\u00b10.09\nLAG-n\n77.4\u00b111.5\n9.1\u00b11.8\n13.5\u00b113.0\n2.45\u00b10.07\n1.41\u00b10.03\nLAG-c\n49.1\u00b18.4\n7.4\u00b12.7\n43.5\u00b111.1\n2.45\u00b10.10\n1.12\u00b10.08\n*Bold values: the mean falls within the range of top1\u2019s mean \u00b1 top1\u2019s std.\n70\n80\n90\n100\nNon-Collision Rate (%)\n1\n2\nv in Success Cases (m/s)\nABS-a\nABS-n\nABS-c\nAgile-a\nAgile-n\nAgile-c\nLAG-a\nLAG-n\nLAG-c\nFig. 7.\nIllustration of agility-safety trade-off in benchmarked comparison.\nAgility is quantified by the average speed achieved in success cases while\nsafety is represented by the non-collision rate. Points indicate the mean values,\nand error bars indicate the std values.\nmean and std values are calculated over 3 policies trained with\ndifferent seeds, and the metrics are obtained via testing for 10k\nrandom episodes.\nThe results indicate that, no matter how the reward weights\nare tuned or whether the RL algorithm is constrained by\nsafe exploration, the agility and the safety trade off within a\nboundary. Yet, with our RA values and the recovery policy as\na safeguard, we can break this boundary and get a substantial\nimprovement in safety at the cost of only a minor decrease in\nagility.\nNote that the variants are only introduced to show the\nboundary here. In the following parts, we will only use the\nnominal ones.\n2) Example Case: We present an example case of ABS and\nother baselines in simulation, where the robot starting from\n(0, 0) needs to run through 8 obstacles to reach the goal (7, 0),\nas shown in Figure 8. The robot needs to first go through an\nopen space, followed by two tight spaces, and then another\nopen space. In this case, the \u03c0Agile baseline runs fast but dies\nnear the second tight space. The LAG baseline runs much\nslower than ABS. In contrast, our proposed ABS runs fast\nin the open spaces, and slows down in the tight spaces for\nsafety thanks to the shielding of RA values and the recovery\npolicy. Figure 8 (c) demonstrates the RA value landscape\nwith respect to twist commands when the recovery policy is\nactivated, where the searched twist consistently satisfies the\nsafety constraint (i.e., \u02c6V < Vthreshold).\n(I) (II) \nCollision \nSuccess\n(I) (II) \nCollision \nSuccess\n(a)\n(b)\n(c)\n(I) \n(II) \nFig. 8.\nAn example case in simulation where \u03c0Agile fails to reach the goal. a)\nTrajectories of ABS and other baselines, with RA values visualized for ABS.\nb) The velocity-time curves showing that ABS is much faster than the LAG\nbaseline. c) Illustrations of the RA value landscape when the recovery policy\nis triggered at (I) and (II), projected in the vx \u2212 \u03c9z plane and the vx \u2212 vy\nplane. We show the initial twist before search (i.e., the current twist of the\nrobot base) and the searched commands based on Equation (21).\nC. Real-World Experiments\n1) Hardware setup: We use the Unitree Go1 for our exper-\niments. The robot is equipped a Jetson Orin NX for onboard\ncomputation and a ZED Mini Stereo Camera for depth and\nodometry sensing. We employ the ZED odometry module to\nABS\n9/10\n1/10\n5.91 s\nABS\n(only \u03c0Agile)\n7/10\n3/10\n5.06 s\nLAG\n8/10\n2/10\n6.80 s\nSuccess\nCollision Time Cost\nABS\n10/10\n0/10\n4.75 s\nABS\n(only \u03c0Agile)\n7/10\n3/10\n3.74 s\nLAG\n9/10\n1/10\n6.13 s\nSuccess\nCollision Time Cost\nABS\n10/10\n0/10\n4.46 s\nABS\n(only \u03c0Agile)\n9/10\n1/10\n4.15 s\nLAG\n9/10\n1/10\n6.05 s\nSuccess\nCollision Time Cost\nIndoor (a)\nIndoor (b)\nOutdoor\nSpeed Test\nPeak Speed\n3.1 m/s\n2.1 m/s\nABS\nLAG\nFig. 9.\nWe evaluate ABS and other baselines in the real world with two\nindoor testbeds, one outdoor testbed, and repetitive speed tests. Indoor (a) is\na dim and narrow corridor, Indoor (b) is a hall with furnitures, and Outdoor is\nan open space on the playground with few obstacles. ABS achieves the best\nsafety across three testbeds, with faster speeds compared to the LAG baseline.\na\nb\nc\nd\nFig. 10. Robustness Tests of our ABS system, a) in snowy terrain,s b) bearing\na 12-kg payload, c) against a ball hit when running, and d) withstand a kick\nwhen standing at the goal.\nonline update the relative goal commands for the agile policy,\nwith its difference as the velocity estimation. We use Unitree\u2019s\nbuilt-in PD controller, with Kp = 30 and Kd = 0.65.\n2) Results: Across two indoor and one outdoor testbeds,\nABS demonstrates superior overall performance as shown in\nFigure 9, achieving the highest success rates and the lowest\ncollision rates. Specifically, ABS consistently scores either 9\nor 10 out of 10 in success rates across all environments, with\nminimal collisions, indicating robustness and reliability in the\nreal world.\nWithout the safety shield, the agile policy \u03c0Agile achieves\nthe fastest running speed at the cost of more collisions. LAG\noutperforms \u03c0Agile in safety but has slower speeds, and falls\nTABLE IV\nGOAL-REACHING POLICY V.S. VELOCITY-TRACKING POLICY\nTerm\nOur \u03c0Agile\nRapid [45]\nGait patterm\ngallop\nnear trot\nMax #. uncontrollable DoFs\n1\n3\nPeak vel. in simulation\n4.0 m/s\n4.1 m/s\nPeak torque in simulation\n23.5 Nm\n35.5 Nm\nPeak joint vel. in simulation\n22.0 rad/s\n30.0 rad/s\nPeak vel. in real world\n3.1 m/s\n2.5 m/s\nCollision avoidance\nas trained\nneed high-level commands\nFully unleashed agility\nas trained\nnon-trivial for high level\nChanging vel. for steering\nin distribution\nout of distribution\nCurriculum learning\nstraightforward\ncarefully designed\nw/o illusion\nw/ illusion\nw/o ERFI-50\nw/ ERFI-50\nFig. 11.\nEffects of illusion and ERFI-50 randomization. The robot will\ntremble near a wall without illusion randomization and will hit the ground\nduring running without ERFI-50 randomization.\nshort in both safety and agility compared to ABS. ABS\nachieves high speed with high safety, and generalizes to\ndynamic obstacles, as shown in Figure 1.\n3) Robustness: Our ABS system can work on the slippery\nicy snow, bear a 12-kg payload (equal to its own weight), and\nwithstand perturbations, as shown in Figure 10. These tests\ndemonstrate the robustness of our system.\nIX. EXTENSIVE STUDIES AND ANALYSES\nA. Maximizing Agility\n1) Goal-Reaching v.s. Velocity-Tracking: Velocity-tracking\nis the most commonly used formulation of locomotion con-\ntrollers [51, 45, 47, 38], and is also adopted for our recovery\npolicy. However, for the agile policy, we claim that goal-\nreaching is a better choice because it does not decouple loco-\nmotion and navigation for collision avoidance and can fully\nunleash the agility that is learned. Moreover, we empirically\nfind that the goal-reaching formulation benefits sim-to-real\ntransfer as it finds a better gait pattern for high-speed running.\nWith the SOTA agile velocity-tracking policy in [45] as a\nbaseline (referred to as \u201crapid\u201d), we make detailed compar-\nisons in Table IV. For fair comparison, we train \u201crapid\u201d with\nthe combination of our regularization rewards and the task\nrewards in [45], use the same action space for two policies,\nand remove the temporal information and system identification\nin [45].\n2) Effects of illusion and ERFI-50 randomization: Two key\ncomponents we add in domain randomization to help sim-\nto-real transfer is the illusion and the ERFI-50. As shown\nin Figure 11, without the illusion, the robot will sometimes\ntremble near a wall which it has never seen in simulation.\nWithout ERFI-50, the robot will hit the ground with its head\nduring running due to the sim-to-real gap in motor dynamics.\nB. Enhancing Perception Training\nIn refining our ray-prediction network training, we system-\natically examine several factors: 1) network architecture, 2)\nTABLE V\nPERFORMANCE METRICS FOR DIFFERENT NETWORK ARCHITECTURES\nAND TRAINING APPROACHES\nArchitecture\nTest Set MSE\nInference Time (ms)\nResNet-34\n3.081 \u00d7 10\u22122\n14\nResNet-18\n3.238 \u00d7 10\u22122\n9\nResNet-18 (w/o pretraining)\n3.526 \u00d7 10\u22122\n9\nResNet-18 (w/o augmentation)\n3.393 \u00d7 10\u22122\n9\nFig. 12.\nSteering the robot with a command sequence \u201cForward\u201d-\u201cRapid\nRight Turn\u201d-\u201cForward\u201d. The robot can reach > 3 m/s when running forward\nand > 6 rad/s when turning rapidly.\npretrained weights, and 3) data augmentation. The comparative\nresults, detailed in Table V, underscore the significance of both\npretrained weights and data augmentation in enhancing the\naccuracy of the network.\nRegarding the network size, larger networks improve the\nprediction accuracy at the cost of inference time. Note that the\nreported inference time in Table V was measured on Jetson\nOrin NX exclusively for perception inference. In practical\ndeployment where computational resources are shared among\nvarious tasks, the actual update frequency can be consider-\nably lower. For real-time high-speed locomotion, we opt for\nResNet-18, balancing accuracy and responsiveness in dynamic\nenvironments.\nC. Instant Steering via Commands\nAs mentioned in Section IV, we can change our goal\ncommands even in the run time. Thanks to our single-frame\nobservations and randomization settings, we can easily over-\nwrite goal commands to achieve instant agile steering, as\npresented in Table VI. This enables direct human involvement\nsimilar to the velocity-tracking formulation, and Figure 12\nshowcases such operations in the real world.\nD. Failure Cases and Limitations\nFirst, when the obstacles are too dense and form a local\nminimum, our policy can easily fail, which is also reflected by\nthe high timeout rates in the results. This is common in local\nnavigation planners [72, 46] though, and a potential solution\ncan be to add memory [64] or introduce a global hint [61].\nSecond, our generalization to dynamic environments are\ndue to the shielding of RA values and the recovery policy.\nThe RA values are learned with static obstacles, and can only\ngeneralize to quasi-static environments. If a dynamic object\nmoves faster than the velocity limit of the recovery policy, the\ncollision may happen. A potential solution is to predict the\nmotions of the obstacles in the future [37, 54].\nThird, we limit the robot behaviors to only 2D locomotion\nand constrain the motions to have no flying phase. For 3D\nTABLE VI\nGOAL COMMANDS FOR INSTANT STEERING\nSteering\nGoal x (m)\nGoal y (m)\nGoal Heading (rad)\nForward\n5\n0\n0\nStop\n0\n0\n0\nLeft Turn\n2\n1.5\n\u03c0\n2\nRapid Left Turn\n\u22122\n0\n3\nRight Turn\n2\n\u22121.5\n\u2212 \u03c0\n2\nRapid Right Turn\n\u22122\n0\n\u22123\nterrains such as stairs and gaps, the problem can be far more\nchallenging because the locomotion skills and the collision\navoidance are coupled.\nFourth, implicit system identification techniques [38, 47, 45,\n36] can leverage temporal information to represent real-world\ndynamics and facilitate sim-to-real transfer, but it is non-trivial\nto incorporate them into our system. This requires a latent\nembedding of the temporal information, which is hard to deal\nwith for the RA module. The policy switch can also make the\nembedding out of distribution for the policies.\nFifth, the vision system needs further improvement. In the\nIndoor (a) testbed, the only collision of ABS is due to the\n\u201cundetected\u201d objects by the ray-prediction network as the\ncorridor is quite dim. Beside the network, the system can also\nbe completed by adding more cameras around the body. In\nthis way, the robot may also dodge the obstacles come from\nbehind or the side. Event cameras may also help in highly\ndynamic scenarios, e.g., when dodging a high-speed ball [19]\nX. CONCLUSION\nIn this paper, we achieve safe high-speed quadrupedal\nlocomotion in cluttered environments. Our framework ABS\nemploys a dual-policy setup where the agile policy enables the\nrobot to run fast, and the recovery policy safeguards the robot.\nThe learned reach-avoid values govern the policy switch and\nguide the recovery policy. A ray-prediction network provides\nexteroception representation for the agile policy and the RA\nvalue network. Some key takeaways are:\n1) For agility in collision avoidance, the best practice\nis to integrate locomotion and navigation rather than\ndecoupling them as separate subtasks.\n2) A low-dimensional exteroception representation can fa-\ncilitate policy learning and generalization.\n3) With the same reward terms and training settings, ad-\njusting reward weights or choosing between RL and\nLagrangian-based RL trades off agility and safety, while\nexternal shielding modules can help break the trade-off\nboundary.\nACKNOWLEDGMENTS\nWe appreciate Wennie Tabib for supporting hardware ex-\nperiments, and thank Yuxiang Yang, Yiyu Chen, Yikai Wang\nand Xialin He for their advice on hardware debugging. Special\nthanks to Andrea Bajcsy and Ziqiao Ma for their assistance in\ngraphics design.\nREFERENCES\n[1] Fernando Acero, Kai Yuan, and Zhibin Li.\nLearning\nperceptual locomotion on uneven terrains using sparse\nvisual observations.\nIEEE Robotics and Automation\nLetters, 7(4):8611\u20138618, 2022.\n[2] Somil Bansal and Claire J Tomlin. Deepreach: A deep\nlearning approach to high-dimensional reachability. In\n2021 IEEE International Conference on Robotics and\nAutomation (ICRA), pages 1817\u20131824. IEEE, 2021.\n[3] Somil Bansal, Mo Chen, Sylvia Herbert, and Claire J\nTomlin. Hamilton-jacobi reachability: A brief overview\nand recent advances. In 2017 IEEE 56th Annual Confer-\nence on Decision and Control (CDC), pages 2242\u20132253.\nIEEE, 2017.\n[4] Shalabh Bhatnagar and K Lakshmanan.\nAn online\nactor\u2013critic algorithm with function approximation for\nconstrained markov decision processes. Journal of Opti-\nmization Theory and Applications, 153:688\u2013708, 2012.\n[5] Gerardo Bledt, Matthew J Powell, Benjamin Katz, Jared\nDi Carlo, Patrick M Wensing, and Sangbae Kim. Mit\ncheetah 3: Design and control of a robust, dynamic\nquadruped robot. In 2018 IEEE/RSJ International Con-\nference on Intelligent Robots and Systems (IROS), pages\n2245\u20132252. IEEE, 2018.\n[6] Michael Bloesch, Christian Gehring, P\u00b4eter Fankhauser,\nMarco Hutter, Mark A Hoepflinger, and Roland Siegwart.\nState estimation for legged robots on unstable and slip-\npery terrain. In 2013 IEEE/RSJ International Conference\non Intelligent Robots and Systems, pages 6058\u20136064.\nIEEE, 2013.\n[7] Russell Buchanan, Lorenz Wellhausen, Marko Bjelonic,\nTirthankar Bandyopadhyay, Navinda Kottege, and Marco\nHutter. Perceptive whole-body planning for multilegged\nrobots in confined spaces. Journal of Field Robotics, 38\n(1):68\u201384, 2021.\n[8] Luigi Campanaro, Siddhant Gangapurwala, Wolfgang\nMerkt, and Ioannis Havoutis.\nLearning and deploying\nrobust locomotion policies with minimal dynamics ran-\ndomization. arXiv preprint arXiv:2209.12878, 2022.\n[9] Mo Chen, Sylvia Herbert, and Claire J Tomlin.\nFast\nreachable set approximations via state decoupling distur-\nbances. In 2016 IEEE 55th Conference on Decision and\nControl (CDC), pages 191\u2013196. IEEE, 2016.\n[10] Richard Cheng, G\u00b4abor Orosz, Richard M Murray, and\nJoel W Burdick. End-to-end safe reinforcement learning\nthrough barrier functions for safety-critical continuous\ncontrol tasks.\nIn Proceedings of the AAAI conference\non artificial intelligence, volume 33, pages 3387\u20133395,\n2019.\n[11] Xuxin Cheng, Kexin Shi, Ananye Agarwal, and Deepak\nPathak.\nExtreme parkour with legged robots.\narXiv\npreprint arXiv:2309.14341, 2023.\n[12] Jia-Ruei Chiu, Jean-Pierre Sleiman, Mayank Mittal, Far-\nbod Farshidian, and Marco Hutter. A collision-free mpc\nfor whole-body dynamic locomotion and manipulation.\nIn 2022 International Conference on Robotics and Au-\ntomation (ICRA), pages 4686\u20134693. IEEE, 2022.\n[13] Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik,\nTodd Hester, Cosmin Paduraru, and Yuval Tassa. Safe\nexploration in continuous action spaces. arXiv preprint\narXiv:1801.08757, 2018.\n[14] Jared Di Carlo, Patrick M Wensing, Benjamin Katz,\nGerardo Bledt, and Sangbae Kim. Dynamic locomotion\nin the mit cheetah 3 through convex model-predictive\ncontrol. In 2018 IEEE/RSJ international conference on\nintelligent robots and systems (IROS), pages 1\u20139. IEEE,\n2018.\n[15] Yanran Ding, Abhishek Pandala, and Hae-Won Park.\nReal-time model predictive control for versatile dynamic\nmotions in quadrupedal robots.\nIn 2019 International\nConference on Robotics and Automation (ICRA), pages\n8484\u20138490. IEEE, 2019.\n[16] Helei Duan, Bikram Pandit, Mohitvishnu S Gadde,\nBart Jaap van Marum, Jeremy Dao, Chanho Kim, and\nAlan Fern.\nLearning vision-based bipedal locomotion\nfor challenging terrain. arXiv preprint arXiv:2309.14594,\n2023.\n[17] Thomas Dudzik, Matthew Chignoli, Gerardo Bledt,\nBryan Lim, Adam Miller, Donghyun Kim, and Sangbae\nKim.\nRobust autonomous navigation of a small-scale\nquadruped robot in real-world environments.\nIn 2020\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), pages 3664\u20133671. IEEE, 2020.\n[18] Shamel Fahmi, Geoff Fink, and Claudio Semini. On state\nestimation for legged locomotion over soft terrain. IEEE\nSensors Letters, 5(1):1\u20134, 2021.\n[19] Davide Falanga, Kevin Kleber, and Davide Scaramuzza.\nDynamic obstacle avoidance for quadrotors with event\ncameras. Science Robotics, 5(40):eaaz9712, 2020.\n[20] Jaime F Fisac, Neil F Lugovoy, Vicenc\u00b8 Rubies-Royo,\nShromona Ghosh, and Claire J Tomlin.\nBridging\nhamilton-jacobi safety analysis and reinforcement learn-\ning. In 2019 International Conference on Robotics and\nAutomation (ICRA), pages 8550\u20138556. IEEE, 2019.\n[21] Kunihiko Fukushima.\nVisual feature extraction by a\nmultilayered network of analog threshold elements. IEEE\nTransactions on Systems Science and Cybernetics, 5(4):\n322\u2013333, 1969.\n[22] Magnus Gaertner, Marko Bjelonic, Farbod Farshidian,\nand Marco Hutter. Collision-free mpc for legged robots\nin static and dynamic scenes. In 2021 IEEE International\nConference on Robotics and Automation (ICRA), pages\n8266\u20138272. IEEE, 2021.\n[23] Siddhant Gangapurwala, Mathieu Geisert, Romeo Or-\nsolino, Maurice Fallon, and Ioannis Havoutis.\nRloc:\nTerrain-aware legged locomotion using reinforcement\nlearning and optimal control.\nIEEE Transactions on\nRobotics, 38(5):2908\u20132927, 2022.\n[24] Ruben Grandia, Farbod Farshidian, Alexey Dosovitskiy,\nRen\u00b4e Ranftl, and Marco Hutter. Frequency-aware model\npredictive control. IEEE Robotics and Automation Let-\nters, 4(2):1517\u20131524, 2019.\n[25] Ruben Grandia, Fabian Jenelten, Shaohui Yang, Farbod\nFarshidian, and Marco Hutter.\nPerceptive locomotion\nthrough nonlinear model-predictive control. IEEE Trans-\nactions on Robotics, 2023.\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pages 770\u2013778, 2016.\n[27] David Hoeller, Lorenz Wellhausen, Farbod Farshidian,\nand Marco Hutter. Learning a state representation and\nnavigation in cluttered and dynamic environments. IEEE\nRobotics and Automation Letters, 6(3):5081\u20135088, 2021.\n[28] David Hoeller, Nikita Rudin, Dhionis Sako, and Marco\nHutter. Anymal parkour: Learning agile navigation for\nquadrupedal robots.\narXiv preprint arXiv:2306.14874,\n2023.\n[29] Kai-Chieh Hsu, Vicenc\u00b8 R\u00b4ubies Royo, Claire J. Tomlin,\nand Jaime F. Fisac.\nSafety and liveness guarantees\nthrough reach-avoid reinforcement learning. In Robotics:\nScience and Systems XVII, 2021.\n[30] Kai-Chieh Hsu, Allen Z Ren, Duy P Nguyen, Anirudha\nMajumdar, and Jaime F Fisac. Sim-to-lab-to-real: Safe\nreinforcement learning with shielding and generalization\nguarantees. Artificial Intelligence, 314:103811, 2023.\n[31] Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario\nBellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco\nHutter.\nLearning agile and dynamic motor skills for\nlegged robots. Science Robotics, 4(26):eaau5872, 2019.\n[32] Fabian\nJenelten,\nJemin\nHwangbo,\nFabian\nTresoldi,\nC Dario Bellicoso, and Marco Hutter. Dynamic locomo-\ntion on slippery ground. IEEE Robotics and Automation\nLetters, 4(4):4170\u20134176, 2019.\n[33] Fabian Jenelten, Ruben Grandia, Farbod Farshidian, and\nMarco Hutter. Tamols: Terrain-aware motion optimiza-\ntion for legged systems. IEEE Transactions on Robotics,\n38(6):3395\u20133413, 2022.\n[34] Fabian Jenelten, Junzhe He, Farbod Farshidian, and\nMarco Hutter.\nDtc: Deep tracking control.\nScience\nRobotics, 9(86):eadh5401, 2024.\n[35] Donghyun Kim, Daniel Carballo, Jared Di Carlo, Ben-\njamin Katz, Gerardo Bledt, Bryan Lim, and Sangbae\nKim. Vision aided dynamic exploration of unstructured\nterrain with a small-scale quadruped robot. In 2020 IEEE\nInternational Conference on Robotics and Automation\n(ICRA), pages 2464\u20132470. IEEE, 2020.\n[36] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra\nMalik. Rma: Rapid motor adaptation for legged robots.\nIn Robotics: Science and Systems, 2021.\n[37] Fr\u00b4ed\u00b4eric Large, Dizan Vasquez, Thierry Fraichard, and\nChristian Laugier. Avoiding cars and pedestrians using\nvelocity obstacles and motion prediction.\nIn IEEE\nIntelligent Vehicles Symposium, 2004, pages 375\u2013379,\n2004.\n[38] Joonho\nLee,\nJemin\nHwangbo,\nLorenz\nWellhausen,\nVladlen Koltun, and Marco Hutter. Learning quadrupedal\nlocomotion over challenging terrain. Science robotics, 5\n(47):eabc5986, 2020.\n[39] Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey\nLevine, Glen Berseth, and Koushil Sreenath. Robust and\nversatile bipedal jumping control through reinforcement\nlearning. In Robotics: Science and Systems, 2023.\n[40] Qingkai Liang, Fanyu Que, and Eytan Modiano. Accel-\nerated primal-dual policy optimization for safe reinforce-\nment learning. arXiv preprint arXiv:1802.06480, 2018.\n[41] Qiayuan Liao, Zhongyu Li, Akshay Thirugnanam, Jun\nZeng, and Koushil Sreenath. Walking in narrow spaces:\nSafety-critical locomotion control for quadrupedal robots\nwith duality-based optimization. In 2023 IEEE/RSJ In-\nternational Conference on Intelligent Robots and Systems\n(IROS), pages 2723\u20132730. IEEE, 2023.\n[42] Minghuan Liu, Menghui Zhu, and Weinan Zhang. Goal-\nconditioned reinforcement learning: Problems and solu-\ntions.\nIn Proceedings of the Thirty-First International\nJoint Conference on Artificial Intelligence, 2022.\n[43] Yuntao Ma, Farbod Farshidian, and Marco Hutter. Learn-\ning arm-assisted fall damage reduction and recovery for\nlegged mobile manipulators. In 2023 IEEE International\nConference on Robotics and Automation (ICRA), pages\n12149\u201312155. IEEE, 2023.\n[44] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong\nGuo, Michelle Lu, Kier Storey, Miles Macklin, David\nHoeller, Nikita Rudin, Arthur Allshire, Ankur Handa,\net al. Isaac gym: High performance gpu based physics\nsimulation for robot learning. In Thirty-fifth Conference\non Neural Information Processing Systems Datasets and\nBenchmarks Track (Round 2), 2021.\n[45] Gabriel B Margolis, Ge Yang, Kartik Paigwar, Tao Chen,\nand Pulkit Agrawal. Rapid locomotion via reinforcement\nlearning. arXiv preprint arXiv:2205.02824, 2022.\n[46] Matias Mattamala, Nived Chebrolu, and Maurice Fallon.\nAn efficient locally reactive controller for safe navigation\nin visual teach and repeat missions. IEEE Robotics and\nAutomation Letters, 7(2):2353\u20132360, 2022.\n[47] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz\nWellhausen, Vladlen Koltun, and Marco Hutter. Learning\nrobust perceptive locomotion for quadrupedal robots in\nthe wild. Science Robotics, 7(62):eabk2822, 2022.\n[48] Yashwanth Kumar Nakka, Anqi Liu, Guanya Shi, Anima\nAnandkumar, Yisong Yue, and Soon-Jo Chung. Chance-\nconstrained trajectory optimization for safe exploration\nand learning of nonlinear systems. IEEE Robotics and\nAutomation Letters, 6(2):389\u2013396, 2020.\n[49] Alex Ray, Joshua Achiam, and Dario Amodei. Bench-\nmarking safe exploration in deep reinforcement learning.\nhttps://openai.com/research/benchmarking-safe-explorati\non-in-deep-reinforcement-learning, 2019.\n[50] Nikita Rudin, David Hoeller, Marko Bjelonic, and Marco\nHutter. Advanced skills by learning locomotion and local\nnavigation end-to-end. In 2022 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS),\npages 2497\u20132503. IEEE, 2022.\n[51] Nikita Rudin, David Hoeller, Philipp Reist, and Marco\nHutter.\nLearning to walk in minutes using massively\nparallel deep reinforcement learning. In Conference on\nRobot Learning, pages 91\u2013100. PMLR, 2022.\n[52] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\n[53] Young-Ha Shin, Tae-Gyu Song, Gwanghyeon Ji, and\nHae-Won Park.\nActuator-constrained reinforcement\nlearning for high-speed quadrupedal locomotion. arXiv\npreprint arXiv:2312.17507, 2023.\n[54] Wenwen Si, Tianhao Wei, and Changliu Liu.\nAgen:\nAdaptable\ngenerative\nprediction\nnetworks\nfor\nau-\ntonomous driving.\nIn 2019 IEEE Intelligent Vehicles\nSymposium (IV), pages 281\u2013286. IEEE, 2019.\n[55] Yunlong Song, Angel Romero, Matthias M\u00a8uller, Vladlen\nKoltun, and Davide Scaramuzza.\nReaching the limit\nin autonomous racing: Optimal control versus reinforce-\nment learning. Science Robotics, 8(82):eadg1462, 2023.\n[56] Stereolabs Inc. Fill mode - depth settings. https://www.\nstereolabs.com/docs/depth-sensing/depth-settings#fill-m\node, 2024.\n[57] Sangli Teng, Yukai Gong, Jessy W. Grizzle, and Maani\nGhaffari. Toward safety-aware informative motion plan-\nning for legged robots. CoRR, abs/2103.14252, 2021.\n[58] Chen Tessler, Daniel J Mankowitz, and Shie Mannor.\nReward constrained policy optimization. arXiv preprint\narXiv:1805.11074, 2018.\n[59] Brijen Thananjeyan, Ashwin Balakrishna, Suraj Nair,\nMichael\nLuo,\nKrishnan\nSrinivasan,\nMinho\nHwang,\nJoseph E Gonzalez, Julian Ibarz, Chelsea Finn, and Ken\nGoldberg. Recovery rl: Safe reinforcement learning with\nlearned recovery zones. IEEE Robotics and Automation\nLetters, 6(3):4915\u20134922, 2021.\n[60] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider,\nWojciech Zaremba, and Pieter Abbeel.\nDomain ran-\ndomization for transferring deep neural networks from\nsimulation to the real world.\nIn 2017 IEEE/RSJ in-\nternational conference on intelligent robots and systems\n(IROS), pages 23\u201330. IEEE, 2017.\n[61] Joanne Truong, April Zitkovich, Sonia Chernova, Dhruv\nBatra,\nTingnan\nZhang,\nJie\nTan,\nand\nWenhao\nYu.\nIndoorsim-to-outdoorreal: Learning to navigate outdoors\nwithout any outdoor experience.\nIn arXiv preprint\narXiv:2305.01098, 2023.\n[62] Yikai Wang, Mengdi Xu, Guanya Shi, and Ding Zhao.\nGuardians as you fall: Active mode transition for safe\nfalling. arXiv preprint arXiv:2310.04828, 2023.\n[63] Lorenz Wellhausen and Marco Hutter. Artplanner: Ro-\nbust legged robot navigation in the field. Field Robotics,\n3(1):413 \u2013 434, 2023-03.\nISSN 2771-3989.\ndoi:\n10.3929/ethz-b-000614683.\n[64] Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee,\nAri S. Morcos, and Dhruv Batra. Emergence of maps in\nthe memories of blind navigation agents. In International\nConference on Learning Representations, 2023.\nURL\nhttps://openreview.net/forum?id=lTt4KjHSsyl.\n[65] Wenli Xiao, Tairan He, John Dolan, and Guanya\nShi.\nSafe deep policy adaptation.\narXiv preprint\narXiv:2310.08602, 2023.\n[66] Chuanyu Yang, Kai Yuan, Qiuguo Zhu, Wanming Yu,\nand Zhibin Li. Multi-expert learning of adaptive legged\nlocomotion. Science Robotics, 5(49):eabb2174, 2020.\n[67] Ruihan Yang, Minghao Zhang, Nicklas Hansen, Huazhe\nXu, and Xiaolong Wang.\nLearning vision-guided\nquadrupedal locomotion end-to-end with cross-modal\ntransformers. In International Conference on Learning\nRepresentations, 2022. URL https://openreview.net/for\num?id=nhnJ3oo6AB.\n[68] Tsung-Yen Yang, Tingnan Zhang, Linda Luu, Sehoon\nHa, Jie Tan, and Wenhao Yu. Safe reinforcement learning\nfor legged locomotion. In 2022 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS),\npages 2454\u20132461. IEEE, 2022.\n[69] Yuxiang Yang, Guanya Shi, Xiangyun Meng, Wenhao\nYu, Tingnan Zhang, Jie Tan, and Byron Boots. Cajun:\nContinuous adaptive jumping using a learned centroidal\ncontroller. arXiv preprint arXiv:2306.09557, 2023.\n[70] Chong\nZhang,\nWanming\nYu,\nand\nZhibin\nLi.\nAccessibility-based\nclustering\nfor\nefficient\nlearning\nof locomotion skills. In 2022 International Conference\non Robotics and Automation (ICRA), pages 1600\u20131606.\nIEEE, 2022.\n[71] Chong Zhang, Nikita Rudin, David Hoeller, and Marco\nHutter. Learning agile locomotion on risky terrains. arXiv\npreprint arXiv:2311.10484, 2023.\n[72] Chong Zhang, Jin Jin, Jonas Frey, Nikita Rudin, Ma-\ntias Eduardo Mattamala Aravena, Cesar Cadena, and\nMarco Hutter. Resilient legged local navigation: Learning\nto traverse with compromised perception end-to-end.\nIn 41st IEEE Conference on Robotics and Automation\n(ICRA 2024), 2024.\n[73] Weiye Zhao, Tairan He, and Changliu Liu. Model-free\nsafe control for zero-violation reinforcement learning. In\n5th Annual Conference on Robot Learning, 2021.\n[74] Weiye Zhao, Tairan He, Rui Chen, Tianhao Wei, and\nChangliu Liu. State-wise safe reinforcement learning: A\nsurvey. In Edith Elkind, editor, Proceedings of the Thirty-\nSecond International Joint Conference on Artificial Intel-\nligence, IJCAI-23, pages 6814\u20136822. International Joint\nConferences on Artificial Intelligence Organization, 8\n2023. URL https://doi.org/10.24963/ijcai.2023/763.\n[75] Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christo-\npher Atkeson, Soeren Schwertfeger, Chelsea Finn, and\nHang Zhao.\nRobot parkour learning.\narXiv preprint\narXiv:2309.05665, 2023.\n"
  },
  {
    "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval",
    "link": "https://arxiv.org/pdf/2401.18059.pdf",
    "upvote": "19",
    "text": "Published as a conference paper at ICLR 2024\nRAPTOR: RECURSIVE ABSTRACTIVE PROCESSING\nFOR TREE-ORGANIZED RETRIEVAL\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning\nStanford University\npsarthi@cs.stanford.edu\nABSTRACT\nRetrieval-augmented language models can better adapt to changes in world state\nand incorporate long-tail knowledge. However, most existing methods retrieve\nonly short contiguous chunks from a retrieval corpus, limiting holistic under-\nstanding of the overall document context. We introduce the novel approach of\nrecursively embedding, clustering, and summarizing chunks of text, constructing\na tree with differing levels of summarization from the bottom up. At inference\ntime, our RAPTOR model retrieves from this tree, integrating information across\nlengthy documents at different levels of abstraction. Controlled experiments show\nthat retrieval with recursive summaries offers significant improvements over tra-\nditional retrieval-augmented LMs on several tasks. On question-answering tasks\nthat involve complex, multi-step reasoning, we show state-of-the-art results; for\nexample, by coupling RAPTOR retrieval with the use of GPT-4, we can improve\nthe best performance on the QuALITY benchmark by 20% in absolute accuracy.\n1\nINTRODUCTION\nLarge Language Models (LLMs) have emerged as transformative tools showing impressive perfor-\nmance on many tasks. With the growing size of LLMs, they can serve standalone as very effective\nknowledge stores, with facts encoded within their parameters (Petroni et al., 2019; Jiang et al., 2020;\nTalmor et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Bubeck et al.,\n2023; Kandpal et al., 2023) and models can be further improved with fine-tuning on downstream\ntasks (Roberts et al., 2020). Nevertheless, even a large model does not contain sufficient domain-\nspecific knowledge for particular tasks and the world continues to change, invalidating facts in the\nLLM. Updating the knowledge of these models through additional fine-tuning or editing is difficult,\nparticularly when dealing with vast text corpora (Lewis et al., 2020; Mitchell et al., 2022). An alter-\nnative approach, pioneered in open domain question answering systems (Chen et al., 2017; Yu et al.,\n2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate\ninformation retrieval system. Retrieved information is then presented to the LLM along with the\nquestion as context (\u201cretrieval augmentation\u201d, Lewis et al., 2020; Izacard et al., 2022; Min et al.,\n2023; Ram et al., 2023), making it easy to provide a system with current knowledge particular to\nsome domain and enabling easy interpretability and provenance tracking, whereas the parametric\nknowledge of LLMs is opaque and difficult to trace back to its source (Akyurek et al., 2022).\nNevertheless, existing retrieval-augmented approaches also have flaws. The one we tackle is that\nmost existing methods retrieve only a few short, contiguous text chunks, which limits their ability\nto represent and leverage large-scale discourse structure. This is particularly relevant for thematic\nquestions that require integrating knowledge from multiple parts of a text, such as understanding\nan entire book, as in the NarrativeQA dataset (Ko\u02c7cisk`y et al., 2018). Consider the fairy tale of\nCinderella, and the question \u201cHow did Cinderella reach her happy ending?\u201d. The top-k retrieved\nshort contiguous texts will not contain enough context to answer the question.\nTo address this, we design an indexing and retrieval system that uses a tree structure to capture both\nhigh-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters\nchunks of text, generates text summaries of those clusters, and then repeats, generating a tree from\nthe bottom up. This structure enables RAPTOR to load into an LLM\u2019s context chunks representing\nthe text at different levels so that it can effectively and efficiently answer questions at different levels.\n1\narXiv:2401.18059v1  [cs.CL]  31 Jan 2024\nPublished as a conference paper at ICLR 2024\n2\n3\n4\n5\n1\n1\n2\n3\n3\n4\n5\n5\n6\n8\n7\nIndex #8\nText:  summary of \nnodes 2 and 3\nChild Nodes: 2, 3\nText Embedding\nText chunks\n3\n.1\n4\n.1\n5\n2. Summarization \nby LLM\n1. Clustering\n10\n7\n1\n2\n8\n4\n3\n5\n6\n9\nFormation of one tree layer\nRoot layer\nLeaf layer\nContents of a node\nRAPTOR Tree \nFigure 1: Tree construction process: RAPTOR recursively clusters chunks of text based on their\nvector embeddings and generates text summaries of those clusters, constructing a tree from the\nbottom up. Nodes clustered together are siblings; a parent node contains the text summary of that\ncluster.\nOur main contribution is the idea of using text summarization to allow retrieval augmentation of\ncontext at different scales, and to show its effectiveness in experiments on collections of long doc-\numents. Controlled experiments with three language models (UnifiedQA (Khashabi et al., 2020),\nGPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023)) show that RAPTOR outperforms current\nretrieval augmentation. Moreover, RAPTOR coupled with GPT-4, and sometimes even with Uni-\nfiedQA, gives new state-of-the-art results on three QA tasks: free text response questions on books\nand movies (NarrativeQA, Ko\u02c7cisk`y et al. 2018), full-text NLP papers (QASPER, Dasigi et al. 2021),\nand multiple-choice questions based on medium-length passages (QuALITY, Pang et al. 2022).1\n2\nRELATED WORK\nWhy Retrieval?\nRecent advances in hardware and algorithms have indeed expanded the con-\ntext lengths that models can handle, leading to questions about the need for retrieval systems (Dai\net al., 2019; Dao et al., 2022; Liu et al., 2023). However, as Liu et al. (2023) and Sun et al. (2021)\nhave noted, models tend to underutilize long-range context and see diminishing performance as con-\ntext length increases, especially when pertinent information is embedded within a lengthy context.\nMoreover, practically, use of long contexts is expensive and slow. This suggests that selecting the\nmost relevant information for knowledge-intensive tasks is still crucial.\nRetrieval Methods\nRetrieval-augmented language models (RALMs) have seen improvements in\nvarious components: the retriever, the reader, and end-to-end system training. Retrieval methods\nhave transitioned from traditional term-based techniques like TF-IDF (Sp\u00a8arck Jones, 1972) and\nBM25 (Robertson et al., 1995; Roberts et al., 2020) to deep learning\u2013based strategies (Karpukhin\net al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023). Some recent work proposes using\nlarge language models as retrievers due to their ability to memorize extensive knowledge (Yu et al.,\n2022; Sun et al., 2022). Research on the reader component includes Fusion-in-Decoder (FiD)\n(Izacard & Grave, 2022), which employs both DPR and BM25 for retrieval and processes passages\nindependently in the encoder and RETRO (Borgeaud et al., 2022; Wang et al., 2023), which utilizes\ncross-chunked attention and chunkwise retrieval to generate text grounded on retrieved context.\nEnd-to-end system training work includes Atlas (Izacard et al., 2022), which fine-tunes an encoder-\ndecoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked\nLM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Genera-\ntion) (Lewis et al., 2020), which integrates pre-trained sequence-to-sequence models with a neural\nretriever. Min et al. (2021) introduced Joint Passage Retrieval (JPR) model which uses a tree-\ndecoding algorithm to handle passage diversity and relevance in multi-answer retrieval. Dense Hi-\nerarchical Retrieval (DHR) and Hybrid Hierarchical Retrieval (HHR) represent advancements\nin retrieval accuracy by combining document and passage level retrievals and integrating sparse and\ndense retrieval methods, respectively (Liu et al., 2021; Arivazhagan et al., 2023).\n1We will release the code of RAPTOR publicly here.\n2\nPublished as a conference paper at ICLR 2024\nDespite a diversity in methods, the retrieving components of models predominantly rely on stan-\ndard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this\napproach is widely adopted, Nair et al. (2023) highlights a potential shortcoming: contiguous seg-\nmentation might not capture the complete semantic depth of the text. Reading extracted snippets\nfrom technical or scientific documents may lack important context making them difficult to read or\neven misleading. (Cohan & Goharian, 2017; Newman et al., 2023; Zhang et al., 2023).\nRecursive summarization as Context\nSummarization techniques provide a condensed view of\ndocuments, enabling more focused engagement with the content (Angelidis & Lapata, 2018). The\nsummarization/snippet model by Gao et al. (2023) uses summarizations and snippets of passages,\nwhich improves correctness on most datasets but can sometimes be a lossy means of compression.\nThe recursive-abstractive summarization model by Wu et al. (2021) employs task decomposition\nto summarize smaller text chunks, which are later integrated to form summaries of larger sections.\nWhile this method is effective for capturing broader themes, it can miss granular details. LlamaIndex\n(Liu, 2022) mitigates this issue by similarly summarizing adjacent text chunks but also retaining\nintermediate nodes thus storing varying levels of detail, keeping granular details. However, both\nmethods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still\noverlook distant interdependencies within the text, which we can find and group with RAPTOR.\n3\nMETHODS\nOverview of RAPTOR\nBuilding on the idea that long texts often present subtopics and hierarchi-\ncal structures (Cao & Wang, 2022; Dong et al., 2023b), RAPTOR addresses the issue of semantic\ndepth and connection in reading by building a recursive tree structure that balances broader thematic\ncomprehension with granular details and which allows nodes to be grouped based on semantic sim-\nilarity not just order in the text.\nConstruction of the RAPTOR tree begins with segmenting the retrieval corpus into short, contiguous\ntexts of length 100, similar to traditional retrieval augmentation techniques. If a sentence exceeds the\n100-token limit, we move the entire sentence to the next chunk, rather than cutting it mid-sentence.\nThis preserves the contextual and semantic coherence of the text within each chunk. These texts\nare then embedded using SBERT, a BERT-based encoder (multi-qa-mpnet-base-cos-v1)\n(Reimers & Gurevych, 2019). The chunks and their corresponding SBERT embeddings form the\nleaf nodes of our tree structure.\nTo group similar text chunks, we employ a clustering algorithm. Once clustered, a Language Model\nis used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle\nof embedding, clustering, and summarization continues until further clustering becomes infeasible,\nresulting in a structured, multi-layered tree representation of the original documents. An important\naspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build\ntime and token expenditure, making it suitable for processing large and complex corpora. For a\ncomprehensive discussion on RAPTOR\u2019s scalability, please refer to the Appendix A.\nFor querying within this tree, we introduce two distinct strategies: tree traversal and collapsed tree.\nThe tree traversal method traverses the tree layer-by-layer, pruning and selecting the most relevant\nnodes at each level. The collapsed tree method evaluates nodes collectively across all layers to find\nthe most relevant ones.\nClustering Algorithm\nClustering plays a key role in building the RAPTOR tree, organizing text\nsegments into cohesive groups. This step groups related content together, which helps the subse-\nquent retrieval process.\nOne of the unique aspects of our clustering approach is the use of soft clustering, where nodes can\nbelong to multiple clusters without requiring a fixed number of clusters. This flexibility is essen-\ntial because individual text segments often contain information relevant to various topics, thereby\nwarranting their inclusion in multiple summaries.\nOur clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers\nboth flexibility and a probabilistic framework. GMMs assume that data points are generated from a\nmixture of several Gaussian distributions.\n3\nPublished as a conference paper at ICLR 2024\nGiven a set of N text segments, each represented as a d-dimensional dense vector embedding, the\nlikelihood of a text vector, x, given its membership in the kth Gaussian distribution, is denoted by\nP(x|k) = N(x; \u00b5k, \u03a3k). The overall probability distribution is a weighted combination P(x) =\nPK\nk=1 \u03c0kN(x; \u00b5k, \u03a3k), where \u03c0k signifies the mixture weight for the kth Gaussian distribution.\nThe high dimensionality of vector embeddings presents a challenge for traditional GMMs, as dis-\ntance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag-\ngarwal et al., 2001). To mitigate this, we employ Uniform Manifold Approximation and Projection\n(UMAP), a manifold learning technique for dimensionality reduction (McInnes et al., 2018). The\nnumber of nearest neighbors parameter, n neighbors, in UMAP determines the balance between\nthe preservation of local and global structures. Our algorithm varies n neighbors to create a hierar-\nchical clustering structure: it first identifies global clusters and then performs local clustering within\nthese global clusters. This two-step clustering process captures a broad spectrum of relationships\namong the text data, from broad themes to specific details.\nShould a local cluster\u2019s combined context ever exceed the summarization model\u2019s token threshold,\nour algorithm recursively applies clustering within the cluster, ensuring that the context remains\nwithin the token threshold.\nTo determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC)\nfor model selection. BIC not only penalizes model complexity but also rewards goodness of fit\n(Schwarz, 1978). The BIC for a given GMM is BIC = ln(N)k \u2212 2 ln(\u02c6L), where N is the number\nof text segments (or data points), k is the number of model parameters, and \u02c6L is the maximized\nvalue of the likelihood function of the model. In the context of GMM, the number of parameters k\nis a function of the dimensionality of the input vectors and the number of clusters.\nWith the optimal number of clusters determined by BIC, the Expectation-Maximization algorithm\nis then used to estimate the GMM parameters, namely the means, covariances, and mixture weights.\nWhile the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which\noften exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an\neffective model for our purpose. We run an ablation comparing GMM Clustering with summarizing\ncontiguous chunks and provide details in Appendix B.\nModel-Based Summarization\nAfter clustering the nodes using Gaussian Mixture Models, the\nnodes in each cluster are sent to a language model for summarization. This step allows the model\nto transform large chunks of text into concise, coherent summaries of the selected nodes. For our\nexperiments, we use gpt-3.5-turbo to generate the summaries. The summarization step con-\ndenses the potentially large volume of retrieved information into a manageable size. We provide\nstatistics on the compression due to the summarization in Appendix C and the prompt used for\nsummarization in Appendix D.\nWhile the summarization model generally produces reliable summaries, a focused annotation study\nrevealed that about 4% of the summaries contained minor hallucinations. These did not propagate\nto parent nodes and had no discernible impact on question-answering tasks. For an in-depth analysis\nof hallucinations, refer to the appendix E.\nQuerying\nIn this section, we elaborate on the two querying mechanisms employed by RAPTOR:\ntree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered\nRAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We\nprovide the pseudocode of both methods in Appendix F. Note that we embed all nodes using SBERT.\nThe tree traversal method first selects the top-k most relevant root nodes based on their cosine\nsimilarity to the query embedding. The children of these selected nodes are considered at the next\nlayer and the top-k nodes are selected from this pool again based on their cosine similarity to the\nquery vector. This process is repeated until we reach the leaf nodes. Finally, the text from all selected\nnodes is concatenated to form the retrieved context. The algorithm\u2019s steps are outlined below:\n1. Start at the root layer of the RAPTOR tree. Compute the cosine similarity between the\nquery embedding and the embeddings of all nodes present at this initial layer.\n2. Choose the top-k nodes based on the highest cosine similarity scores, forming the set S1.\n4\nPublished as a conference paper at ICLR 2024\nFigure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms. Tree traver-\nsal starts at the root level of the tree and retrieves the top-k (here, top-1) node(s) based on cosine\nsimilarity to the query vector. At each level, it retrieves the top-k node(s) from the child nodes of\nthe previous layer\u2019s top-k. Collapsed tree collapses the tree into a single layer and retrieves nodes\nuntil a threshold number of tokens is reached, based on cosine similarity to the query vector. The\nnodes on which cosine similarity search is performed are highlighted in both illustrations.\n3. Proceed to the child nodes of the elements in set S1. Compute the cosine similarity between\nthe query vector and the vector embeddings of these child nodes.\n4. Select the top k child nodes with the highest cosine similarity scores to the query, forming\nthe set S2.\n5. Continue this process recursively for d layers, producing sets S1, S2, . . . , Sd.\n6. Concatenate sets S1 through Sd to assemble the relevant context to the query.\nBy adjusting the depth d and the number of nodes k selected at each layer, the tree traversal method\noffers control over the specificity and breadth of the information retrieved. The algorithm starts with\na broad outlook by considering the top layers of the tree and progressively focuses on finer details\nas it descends through the lower layers.\nThe collapsed tree approach offers a simpler way to search for relevant information by considering\nall nodes in the tree simultaneously, as depicted in Figure 2. Instead of going layer-by-layer, this\nmethod flattens the multi-layered tree into a single layer, essentially bringing all the nodes onto the\nsame level for comparison. The steps for this method are outlined below:\n1. First, collapse the entire RAPTOR tree into a single layer. This new set of nodes, denoted\nas C, contains nodes from every layer of the original tree.\n2. Next, calculate the cosine similarity between the query embedding and the embeddings of\nall nodes present in the collapsed set C.\n3. Finally, pick the top-k nodes that have the highest cosine similarity scores with the query.\nKeep adding nodes to the result set until you reach a predefined maximum number of\ntokens, ensuring you don\u2019t exceed the model\u2019s input limitations.\nWe tested both approaches on 20 stories from the QASPER dataset. Figure 3 shows the performance\nof tree traversal with different top- sizes and collapsed tree with different maximum token numbers.\nThe collapsed tree approach consistently performs better. We believe collapsed tree retrieval is\nbetter due to offering greater flexibility than tree traversal; i.e., by searching through all the nodes\nsimultaneously, it retrieves information that is at the correct level of granularity for a given question.\nIn comparison, while using tree traversal with the same values of d and k, the ratio of nodes from\neach level of the tree will be constant. So, the ratio of higher-order thematic information to granular\ndetails will remain the same regardless of the question.\n5\nPublished as a conference paper at ICLR 2024\nOne drawback, however, of the collapsed tree approach is that it requires cosine similarity search to\nbe performed on all nodes in the tree. However, this can be made more efficient with fast k-nearest\nneighbor libraries such as FAISS (Johnson et al., 2019).\nFigure 3: Comparison of querying methods.\nResults on 20 stories from the QASPER dataset\nusing tree traversal with different top-k values,\nand collapsed tree with different context lengths.\nCollapsed tree with 2000 tokens produces the best\nresults, so we use this querying strategy for our\nmain results.\nOverall, given the collapsed tree approach\u2019s\ngreater flexibility and its superior performance\non the subset of the QASPER dataset, this is\nthe querying approach with which we proceed.\nSpecifically, we use the collapsed tree with\n2000 maximum tokens, which approximately\nequates to retrieving the top-20 nodes. Using a\ntoken-based approach ensures the context does\nnot exceed model context constraints as token\ncounts can vary across nodes. For experiments\nwith the UnifiedQA model, we provide 400 to-\nkens of context, as UnifiedQA has a max con-\ntext length of 512 tokens. We provide the same\namount of tokens of context to RAPTOR and to\nthe baselines.\nQualitative Study\nWe conduct a qualitative\nanalysis to understand the benefits of RAP-\nTOR\u2019s retrieval process compared to Dense\nPassage Retrieval (DPR) methods. Our study\nfocuses on thematic, multi-hop questions using\na 1500-word Cinderella fairytale. As illustrated\nin Figure 4, RAPTOR\u2019s tree-based retrieval allows it to choose nodes from different tree layers,\nmatching the question\u2019s detail level. This approach often yields more relevant and comprehensive\ninformation for downstream tasks than DPR. For a detailed discussion and examples, including the\ntext retrieved by both RAPTOR and DPR for specific questions, please refer to the appendix G.\n4\nEXPERIMENTS\nDatasets\nWe measure RAPTOR\u2019s performance across three question-answering datasets: Narra-\ntiveQA, QASPER, and QuALITY.\nNarrativeQA is a dataset that comprises question-answer pairs based on the full texts of books\nand movie transcripts, totaling 1,572 documents (Ko\u02c7cisk`y et al., 2018; Wu et al., 2021).\nThe\nNarrativeQA-Story task requires a comprehensive understanding of the entire narrative in order\nto accurately answer its questions, thus testing the model\u2019s ability to comprehend longer texts in\nthe literary domain. We measure performance on this dataset using the standard BLEU (B-1, B-4),\nROUGE (R-L), and METEOR (M) metrics. Please see appendix H for more details on the Narra-\ntiveQA evaluation script used in our experiments.\nThe QASPER dataset includes 5,049 questions across 1,585 NLP papers, with each question probing\nfor information embedded within the full text (Dasigi et al., 2021). The answer types in QASPER\nare categorized as Answerable/Unanswerable, Yes/No, Abstractive, and Extractive. Accuracy is\nmeasured using standard F1.\nLastly, the QuALITY dataset consists of multiple-choice questions, each accompanied by context\npassages averaging approximately 5,000 tokens in length (Pang et al., 2022). This dataset calls for\nreasoning over the entire document for QA tasks, enabling us to measure the performance of our re-\ntrieval system on medium-length documents. The dataset includes a challenging subset, QuALITY-\nHARD, which contains questions that a majority of human annotators answered incorrectly in a\nspeed-setting. We report accuracies for both the entire test set and the HARD subset.\nControlled Baseline Comparisons\nWe first present controlled comparisons using the UnifiedQA\n3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM25 (Robertson et al., 1995; 2009),\nand DPR (Karpukhin et al., 2020) as the embedding models with and without the RAPTOR tree\nstructure, on three datasets: QASPER, NarrativeQA, and QuALITY. As shown in Tables 1 and 2,\n6\nPublished as a conference paper at ICLR 2024\nFigure 4: Querying Process: Illustration of how RAPTOR retrieves information for two questions\nabout the Cinderella story: \u201cWhat is the central theme of the story?\u201d and \u201cHow did Cinderella find\na happy ending?\u201d. Highlighted nodes indicate RAPTOR\u2019s selections, while arrows point to DPR\u2019s\nleaf nodes. Notably, RAPTOR\u2019s context often encompasses the information retrieved by DPR, either\ndirectly or within higher-layer summaries.\nour results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms\nthe respective retriever across all datasets. 2\nSince RAPTOR with SBERT has the best performance, we use it in all subsequent experiments.\nWe now compare RAPTOR with BM25 and DPR, using three different LLMs: GPT-3, GPT-4, and\nUnifiedQA. As shown in Table 3, RAPTOR consistently outperforms BM25 and DPR across all\nthree Language Models on the QASPER dataset. RAPTOR\u2019s F-1 Match scores are 53.1%, 55.7%,\nand 36.6% when using GPT-3, GPT-4, and UnifiedQA, respectively. These scores surpass DPR by\nmargins of 1.8, 2.7, and 4.5 points, and outdo BM25 by 6.5, 5.5, and 10.2 points across the respective\nLLMs. QASPER requires synthesizing information within NLP papers, so it is unsurprising that\nRAPTOR\u2019s higher-level summary nodes would allow it to outperform methods that can only extract\nthe top-k most similar raw chunks of text, which may not contain the correct response in isolation.\nTable 1: NarrativeQA Performance With + Without RAPTOR: Performance comparison of\nvarious retrieval methods (SBERT, BM25, DPR) with and without RAPTOR on the NarrativeQA\ndataset, using UnifiedQA-3B as the language model. RAPTOR outperforms baselines of each re-\nspective retrieval method.\nModel\nROUGE\nBLEU-1\nBLEU-4\nMETEOR\nSBERT with RAPTOR\n30.87%\n23.50%\n6.42%\n19.20%\nSBERT without RAPTOR\n29.26%\n22.56%\n5.95%\n18.15%\nBM25 with RAPTOR\n27.93%\n21.17%\n5.70%\n17.03%\nBM25 without RAPTOR\n23.52%\n17.73%\n4.65%\n13.98%\nDPR with RAPTOR\n30.94%\n23.51%\n6.45%\n19.05%\nDPR without RAPTOR\n29.56%\n22.84%\n6.12%\n18.44%\nLikewise, in the QuALITY dataset as shown in Table 4, RAPTOR achieves an accuracy of 62.4%,\nwhich is a 2% and 5.1% improvement over DPR and BM25. Similar trends are observed when Uni-\nfiedQA is employed, with RAPTOR outperforming DPR and BM25 by 2.7% and 6.7%, respectively.\nFinally, in the NarrativeQA dataset, as presented in Table 6, RAPTOR excels across multiple met-\nrics. For ROUGE-L, it surpasses BM25 and DPR by 7.3 and 2.7 points, respectively. In other\nmetrics like BLEU-1, BLEU-4, and METEOR, RAPTOR outperforms BM25 and DPR by margins\nranging from 1.7 to 5.8 and 0.7 to 2.1 points, respectively.\n2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to\ndpr-single-nq-base which was used in rest of the experiments done earlier. This decision was based on\nthe performance observed in Karpukhin et al. (2020), where dpr-multiset-base showed superior results.\n7\nPublished as a conference paper at ICLR 2024\nTable 2: QuALITY and QASPER Performance With + Without RAPTOR: Performance com-\nparison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,\nDPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outper-\nforms baselines of each respective retrieval method for both datasets.\nModel\nAccuracy (QuALITY)\nAnswer F1 (QASPER)\nSBERT with RAPTOR\n56.6%\n36.70%\nSBERT without RAPTOR\n54.9%\n36.23%\nBM25 with RAPTOR\n52.1%\n27.00%\nBM25 without RAPTOR\n49.9%\n26.47%\nDPR with RAPTOR\n54.7%\n32.23%\nDPR without RAPTOR\n53.1%\n31.70%\nTable 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different lan-\nguage models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column \u201dTitle +\nAbstract\u201d reflects performance when only the title and abstract of the papers are used for context.\nRAPTOR outperforms the established baselines BM25 and DPR across all tested language models.\nSpecifically, RAPTOR\u2019s F-1 scores are at least 1.8% points higher than DPR and at least 5.3% points\nhigher than BM25.\nRetriever\nGPT-3 F-1 Match\nGPT-4 F-1 Match\nUnifiedQA F-1 Match\nTitle + Abstract\n25.2\n22.2\n17.5\nBM25\n46.6\n50.2\n26.4\nDPR\n51.3\n53.0\n32.1\nRAPTOR\n53.1\n55.7\n36.6\nTable 4: Comparison of accuracies on the QuAL-\nITY dev dataset for two different language mod-\nels (GPT-3, UnifiedQA 3B) using various retrieval\nmethods. RAPTOR outperforms the baselines of\nBM25 and DPR by at least 2.0% in accuracy.\nModel\nGPT-3 Acc.\nUnifiedQA Acc.\nBM25\n57.3\n49.9\nDPR\n60.4\n53.9\nRAPTOR\n62.4\n56.6\nTable 5: Results on F-1 Match scores of various\nmodels on the QASPER dataset.\nModel\nF-1 Match\nLongT5 XL (Guo et al., 2022)\n53.1\nCoLT5 XL (Ainslie et al., 2023)\n53.9\nRAPTOR + GPT-4\n55.7\nComparison\nto\nState-of-the-art\nSystems\nBuilding upon our controlled comparisons,\nwe examine RAPTOR\u2019s performance relative\nto other state-of-the-art models.\nAs shown\nin Table 5, RAPTOR with GPT-4 sets a new\nbenchmark on QASPER, with a 55.7% F-1\nscore, surpassing the CoLT5 XL\u2019s score of\n53.9%.\nIn the QuALITY dataset, as shown in Table 7,\nRAPTOR paired with GPT-4 sets a new state-\nof-the-art with an accuracy of 82.6%, surpass-\ning the previous best result of 62.3%. In par-\nticular, it outperforms CoLISA by 21.5% on\nQuALITY-HARD, which represents questions\nthat humans took unusually long to correctly\nanswer, requiring rereading parts of the text,\ndifficult reasoning, or both.\nFor the NarrativeQA dataset, as represented in\nTable 6, RAPTOR paired with UnifiedQA sets\na new state-of-the-art METEOR score. When compared to the recursively summarizing model by\nWu et al. (2021), which also employs UnifiedQA, RAPTOR outperforms it on all metrics. While\nWu et al. (2021) rely solely on the summary in the top root node of the tree structure, RAPTOR\nbenefits from its intermediate layers and clustering approaches, which allows it to capture a range of\ninformation, from general themes to specific details, contributing to its overall strong performance.\n4.1\nCONTRIBUTION OF THE TREE STRUCTURE\nWe examine the contribution of each layer of nodes to RAPTOR\u2019s retrieval capabilities. We hy-\npothesized that upper nodes play a crucial role in handling thematic or multi-hop queries requiring\na broader understanding of the text.\n8\nPublished as a conference paper at ICLR 2024\nTable 6: Performance comparison on the NarrativeQA dataset across multiple models, focusing\non four metrics: ROUGE-L, BLEU-1, BLEU-4, and METEOR. RAPTOR, when paired with Uni-\nfiedQA 3B, not only surpasses retrieval methods like BM25 and DPR but also sets a new state-of-\nthe-art in the METEOR metric.\nModel\nROUGE-L\nBLEU-1\nBLEU-4\nMETEOR\nBiDAF (Ko\u02c7cisk`y et al., 2018)\n6.2\n5.7\n0.3\n3.7\nBM25 + BERT (Mou et al., 2020)\n15.5\n14.5\n1.4\n5.0\nRecursively Summarizing Books (Wu et al., 2021)\n21.6\n22.3\n4.2\n10.6\nRetriever + Reader (Izacard & Grave, 2022)\n32.0\n35.3\n7.5\n11.1\nRAPTOR + UnifiedQA\n30.8\n23.5\n6.4\n19.1\nTable 7: Accuracies of the QuALITY dataset on both the overall test set and the more challenging\nhard subset. GPT-4 with RAPTOR sets a new state-of-the-art.\nModel\nAccuracy\nTest Set\nHard Subset\nLongformer-base (Beltagy et al., 2020)\n39.5\n35.3\nDPR and DeBERTaV3-large (Pang et al., 2022)\n55.4\n46.1\nCoLISA (DeBERTaV3-large) (Dong et al., 2023a)\n62.3\n54.7\nRAPTOR + GPT-4\n82.6\n76.2\nTable 8: Performance of RAPTOR when querying different tree layers for Story 1 from the QuAL-\nITY dataset. Columns represent different starting points (highest layer) and rows represent different\nnumbers of layers queried.\nLayers Queried / Start Layer\nLayer 0 (Leaf Nodes)\nLayer 1\nLayer 2\n1 layer\n57.9\n57.8\n57.9\n2 layers\n-\n52.6\n63.15\n3 layers\n-\n-\n73.68\nWe validated this hypothesis both quantitatively and qualitatively. We present qualitative analysis in\nappendix G. To quantitatively understand the contribution of the upper-level nodes, we used stories\nfrom the QuALITY dataset. The RAPTOR tree is built for each of these stories, as described in\nSection 3. However, during retrieval, we limit the search to different subsets of layers. For example,\nwe exclusively retrieve from the leaf nodes and each upper layer, as well as from different contiguous\nsubsets of the layers. We show findings specific to one story in Table 8, revealing that a full-tree\nsearch, utilizing all layers, outperformed retrieval strategies that focused only on specific layers.\nThese findings highlight the importance of the full tree structure in RAPTOR. By providing both\nthe original text and higher-level summaries for retrieval, RAPTOR can effectively handle a wider\nrange of questions, from higher-order thematic queries to detail-oriented questions. Detailed results\nfor additional stories and an ablation study on layer contributions can be found in Appendix I.\n5\nCONCLUSION\nIn this paper, we have presented RAPTOR, a novel tree-based retrieval system that augments the\nparametric knowledge of large language models with contextual information at various levels of\nabstraction. By employing recursive clustering and summarization techniques, RAPTOR creates a\nhierarchical tree structure that is capable of synthesizing information across various sections of the\nretrieval corpora. During the query phase, RAPTOR leverages this tree structure for more effective\nretrieval. Our controlled experiments demonstrated that RAPTOR not only outperforms traditional\nretrieval methods but also sets new performance benchmarks on several question-answering tasks.\n9\nPublished as a conference paper at ICLR 2024\n6\nREPRODUCIBILITY STATEMENT\nLanguage Models for QA and Summarization\nFour language models are used in our RAPTOR\nexperiments: GPT-3 and GPT-4 for QA tasks, and GPT-3.5-turbo for summarization. The gpt-3,\ngpt-4, and gpt-3.5-turbo models can be accessed via API calls (OpenAI API). UnifiedQA,\nwhich is used for QA tasks, is publicly available at Hugging Face.\nEvaluation Datasets\nThe three evaluation datasets used in our experiments\u2014QuALITY,\nQASPER, and NarrativeQA\u2014are all publicly accessible. These datasets ensure that the retrieval\nand QA tests conducted in this study can be replicated.\nSource Code\nThe source code for RAPTOR will be publicly available here.\nREFERENCES\nCharu C Aggarwal, Alexander Hinneburg, and Daniel A Keim. On the Surprising Behavior of Dis-\ntance Metrics in High Dimensional Space. In Database Theory\u2014ICDT 2001: 8th International\nConference London, UK, January 4\u20136, 2001 Proceedings 8, pp. 420\u2013434. Springer, 2001. URL\nhttps://link.springer.com/chapter/10.1007/3-540-44503-x_27.\nJoshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta\u02dcn\u00b4on, Siddhartha Brahma, Yury Zemlyan-\nskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al.\nCoLT5: Faster long-range\ntransformers with conditional computation.\narXiv preprint arXiv:2303.09752, 2023.\nURL\nhttps://arxiv.org/abs/2303.09752.\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and\nKelvin Guu.\nTowards tracing knowledge in language models back to the training data.\nIn\nFindings of the Association for Computational Linguistics: EMNLP 2022, pp. 2429\u20132446,\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\ndoi: 10.18653/v1/2022.findings-emnlp.180. URL https://aclanthology.org/2022.\nfindings-emnlp.180.\nStefanos Angelidis and Mirella Lapata. Summarizing opinions: Aspect extraction meets sentiment\nprediction and they are both weakly supervised. arXiv preprint arXiv:1808.08858, 2018. URL\nhttps://arxiv.org/abs/1808.08858.\nManoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, and Zhiheng\nHuang.\nHybrid hierarchical retrieval for open-domain question answering.\nIn Anna Rogers,\nJordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational\nLinguistics: ACL 2023, pp. 10680\u201310689, Toronto, Canada, July 2023. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2023.findings-acl.679. URL https://aclanthology.\norg/2023.findings-acl.679.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The Long-document Transformer,\n2020. URL https://arxiv.org/abs/2004.05150. arXiv preprint arXiv:2004.05150.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.\nImproving language models by retrieving from trillions of tokens. In International conference on\nmachine learning, pp. 2206\u20132240. PMLR, 2022. URL https://arxiv.org/abs/2112.\n04426.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel\nZiegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei.\nLanguage Models are Few-Shot Learners.\nIn\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-\nral Information Processing Systems, volume 33, pp. 1877\u20131901. Curran Associates, Inc.,\n10\nPublished as a conference paper at ICLR 2024\n2020.\nURL https://proceedings.neurips.cc/paper_files/paper/2020/\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\nmar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of Artificial General\nIntelligence: Early Experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023. URL\nhttps://arxiv.org/abs/2303.12712.\nShuyang Cao and Lu Wang. HIBRIDS: Attention with hierarchical biases for structure-aware long\ndocument summarization. In Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 786\u2013807, Dublin, Ireland, May 2022.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.58. URL https:\n//aclanthology.org/2022.acl-long.58.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.\nReading Wikipedia to Answer\nOpen-Domain Questions.\nIn Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 1870\u20131879, Vancouver, Canada, July\n2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https:\n//aclanthology.org/P17-1171.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\nScaling Language Modeling with Pathways.\narXiv preprint arXiv:2204.02311, 2022.\nURL\nhttps://arxiv.org/abs/2204.02311.\nArman Cohan and Nazli Goharian. Contextualizing citations for scientific summarization using\nword embeddings and domain knowledge. In Proceedings of the 40th International ACM SIGIR\nConference on Research and Development in Information Retrieval, pp. 1133\u20131136, 2017. URL\nhttps://dl.acm.org/doi/abs/10.1145/3077136.3080740.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, pp. 2978\u20132988, Florence,\nItaly, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL\nhttps://aclanthology.org/P19-1285.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e.\nFlashAttention: Fast and\nmemory-efficient exact attention with IO-Awareness. Advances in Neural Information Processing\nSystems, 35:16344\u201316359, 2022. URL https://arxiv.org/abs/2205.14135.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A Dataset\nof Information-Seeking Questions and Answers Anchored in Research Papers.\nIn Proceed-\nings of the 2021 Conference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, pp. 4599\u20134610, Online, June 2021. Asso-\nciation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https:\n//aclanthology.org/2021.naacl-main.365.\nMengxing Dong, Bowei Zou, Yanling Li, and Yu Hong. CoLISA: Inner Interaction via Contrastive\nLearning for Multi-choice Reading Comprehension. In Advances in Information Retrieval: 45th\nEuropean Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2\u20136, 2023,\nProceedings, Part I, pp. 264\u2013278. Springer, 2023a. URL https://link.springer.com/\nchapter/10.1007/978-3-031-28244-7_17.\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao. A survey on long text modeling with\ntransformers. arXiv preprint arXiv:2302.14502, 2023b. URL https://arxiv.org/abs/\n2302.14502.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate\ntext with citations. arXiv preprint arXiv:2305.14627, 2023. URL https://arxiv.org/\nabs/2305.14627.\n11\nPublished as a conference paper at ICLR 2024\nMandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and\nYinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the\nAssociation for Computational Linguistics: NAACL 2022, pp. 724\u2013736, Seattle, United States,\nJuly 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.55.\nURL https://aclanthology.org/2022.findings-naacl.55.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval Augmented\nLanguage Model Pre-Training. In International conference on machine learning, pp. 3929\u20133938.\nPMLR, 2020. URL https://doi.org/10.48550/arXiv.2002.08909.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. URL\nhttps://arxiv.org/abs/2203.15556.\nGautier Izacard and Edouard Grave.\nDistilling Knowledge from Reader to Retriever for Ques-\ntion Answering, 2022.\nURL https://arxiv.org/abs/2012.04584.\narXiv preprint\narXiv:2012.04584.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022. URL https:\n//arxiv.org/abs/2208.03299.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language\nmodels know? Transactions of the Association for Computational Linguistics, 8:423\u2013438, 2020.\nURL https://arxiv.org/abs/1911.12543.\nJeff Johnson, Matthijs Douze, and Herv\u00b4e J\u00b4egou. Billion-Scale Similarity Search with GPUs. IEEE\nTransactions on Big Data, 7(3):535\u2013547, 2019. URL https://arxiv.org/abs/1702.\n08734.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large Language\nModels struggle to learn Long-Tail Knowledge. In International Conference on Machine Learn-\ning, pp. 15696\u201315707. PMLR, 2023. URL https://proceedings.mlr.press/v202/\nkandpal23a/kandpal23a.pdf.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pp. 6769\u20136781, Online, November 2020. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.\nemnlp-main.550.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and\nHannaneh Hajishirzi.\nUNIFIEDQA: Crossing format boundaries with a single QA system.\nIn Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1896\u20131907,\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nfindings-emnlp.171.\nURL https://aclanthology.org/2020.findings-emnlp.\n171.\nOmar Khattab and Matei Zaharia.\nColBERT: Efficient and effective passage search via con-\ntextualized late interaction over bert.\nIn Proceedings of the 43rd International ACM SIGIR\nconference on research and development in Information Retrieval, pp. 39\u201348, 2020.\nURL\nhttps://arxiv.org/abs/2004.12832.\nTom\u00b4a\u02c7s Ko\u02c7cisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00b4abor Melis,\nand Edward Grefenstette. The NarrativeQA Reading Comprehension Challenge. Transactions\nof the Association for Computational Linguistics, 6:317\u2013328, 2018. URL https://arxiv.\norg/abs/1712.07040.\n12\nPublished as a conference paper at ICLR 2024\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K\u00a8uttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00a8aschel, et al. Retrieval-Augmented Gener-\nation for Knowledge-Intensive NLP Tasks. Advances in Neural Information Processing Systems,\n33:9459\u20139474, 2020. URL https://doi.org/10.48550/arXiv.2005.11401.\nJerry Liu. LlamaIndex, 2022. URL https://github.com/jerryjliu/llama_index.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang.\nLost in the middle: How language models use long contexts.\narXiv preprint\narXiv:2307.03172, 2023. URL https://arxiv.org/abs/2307.03172.\nYe Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and Philip Yu. Dense\nhierarchical retrieval for open-domain question answering. In Marie-Francine Moens, Xuanjing\nHuang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Compu-\ntational Linguistics: EMNLP 2021, pp. 188\u2013200, Punta Cana, Dominican Republic, Novem-\nber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.19.\nURL https://aclanthology.org/2021.findings-emnlp.19.\nLeland McInnes, John Healy, and James Melville.\nUMAP: Uniform Manifold Approximation\nand Projection for Dimension Reduction, 2018. URL https://arxiv.org/abs/1802.\n03426. arXiv preprint arXiv:1802.03426.\nSewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi. Joint\npassage ranking for diverse multi-answer retrieval. In Marie-Francine Moens, Xuanjing Huang,\nLucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pp. 6997\u20137008, Online and Punta Cana, Dominican\nRepublic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\nemnlp-main.560. URL https://aclanthology.org/2021.emnlp-main.560.\nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke\nZettlemoyer.\nNonparametric masked language modeling.\nIn Findings of the Association for\nComputational Linguistics: ACL 2023, pp. 2097\u20132118, Toronto, Canada, July 2023. Associ-\nation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.132. URL https:\n//aclanthology.org/2023.findings-acl.132.\nEric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn.\nMemory-based model editing at scale.\nIn International Conference on Machine Learning,\npp. 15817\u201315831. PMLR, 2022.\nURL https://proceedings.mlr.press/v162/\nmitchell22a/mitchell22a.pdf.\nXiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui\nSu. Frustratingly hard evidence retrieval for QA over books. In Proceedings of the First Joint\nWorkshop on Narrative Understanding, Storylines, and Events, pp. 108\u2013113, Online, July 2020.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.nuse-1.13. URL https:\n//aclanthology.org/2020.nuse-1.13.\nInderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikr-\nishna Karanam, and Sumit Shekhar.\nA neural CRF-based hierarchical approach for lin-\near text segmentation.\nIn Findings of the Association for Computational Linguistics: EACL\n2023, pp. 883\u2013893, Dubrovnik, Croatia, May 2023. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2023.findings-eacl.65. URL https://aclanthology.org/2023.\nfindings-eacl.65.\nBenjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo. A controllable qa-\nbased framework for decontextualization. arXiv preprint arXiv:2305.14772, 2023. URL https:\n//arxiv.org/pdf/2305.14772.pdf.\nOpenAI. GPT-4 Technical Report. ArXiv, abs/2303.08774, 2023. URL https://arxiv.org/\nabs/2303.08774.\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen,\nVishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY:\n13\nPublished as a conference paper at ICLR 2024\nQuestion Answering with Long Input Texts, Yes!\nIn Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pp. 5336\u20135358, Seattle, United States, July 2022. Association for Computational\nLinguistics. URL https://aclanthology.org/2022.naacl-main.391.\nFabio Petroni, Tim Rockt\u00a8aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\nand Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066,\n2019. URL https://arxiv.org/abs/1909.01066.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\nMethods, Analysis & Insights from Training Gopher. arXiv preprint arXiv:2112.11446, 2021.\nURL https://arxiv.org/abs/2112.11446.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-\nBrown, and Yoav Shoham.\nIn-context retrieval-augmented language models.\narXiv preprint\narXiv:2302.00083, 2023. URL https://arxiv.org/abs/2302.00083.\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\nnetworks.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pp. 3982\u20133992, Hong Kong, China, November 2019. Association for Com-\nputational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/\nD19-1410.\nAdam Roberts, Colin Raffel, and Noam Shazeer.\nHow Much Knowledge Can You Pack Into\nthe Parameters of a Language Model?\nIn Proceedings of the 2020 Conference on Empir-\nical Methods in Natural Language Processing (EMNLP), pp. 5418\u20135426, Online, November\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL\nhttps://aclanthology.org/2020.emnlp-main.437.\nStephen Robertson, Hugo Zaragoza, et al. The Probabilistic Relevance Framework: BM25 and\nBeyond. Foundations and Trends in Information Retrieval, 3(4):333\u2013389, 2009. URL https:\n//doi.org/10.1561/1500000019.\nStephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford,\net al. Okapi at TREC-3. Nist Special Publication Sp, 109:109, 1995. URL https://www.\nmicrosoft.com/en-us/research/publication/okapi-at-trec-3/.\nDevendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil\nZaheer. Questions are all you need to train a dense passage retriever. Transactions of the As-\nsociation for Computational Linguistics, 11:600\u2013616, 2023. doi: 10.1162/tacl a 00564. URL\nhttps://aclanthology.org/2023.tacl-1.35.\nGideon Schwarz. Estimating the Dimension of a Model. The annals of statistics, pp. 461\u2013464,\n1978. URL https://projecteuclid.org/journals/annals-of-statistics/\nvolume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/\naos/1176344136.full.\nKaren Sp\u00a8arck Jones. A Statistical Interpretation of Term Specificity and its Application in Re-\ntrieval. Journal of documentation, 28(1):11\u201321, 1972. URL https://doi.org/10.1108/\neb026526.\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language\nmodels actually use long-range context?\nIn Marie-Francine Moens, Xuanjing Huang, Lucia\nSpecia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pp. 807\u2013822, Online and Punta Cana, Dominican Republic,\nNovember 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.\n62. URL https://aclanthology.org/2021.emnlp-main.62.\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language\nmodels. arXiv preprint arXiv:2210.01296, 2022. URL https://arxiv.org/abs/2210.\n01296.\n14\nPublished as a conference paper at ICLR 2024\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. oLMpics\u2013 on what language\nmodel pre-training captures. Transactions of the Association for Computational Linguistics, 8:\n743\u2013758, 2020. URL https://arxiv.org/abs/1912.13283.\nBoxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong,\nOleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models\nwith retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762, 2023. URL https:\n//arxiv.org/abs/2304.06762.\nJeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul\nChristiano.\nRecursively Summarizing Books with Human Feedback, 2021.\nURL https:\n//arxiv.org/abs/2109.10862.\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi,\nand Quoc V. Le. QANet: Combining Local Convolution with Global Self-Attention for Read-\ning Comprehension, 2018. URL https://arxiv.org/abs/1804.09541. arXiv preprint\narXiv:1804.09541.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang\nZhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large Language Models are\nstrong context generators, 2022. URL https://arxiv.org/abs/2209.10063.\nShiyue Zhang, David Wan, and Mohit Bansal.\nExtractive is not faithful: An investigation of\nbroad unfaithfulness problems in extractive summarization.\nIn Anna Rogers, Jordan Boyd-\nGraber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pp. 2153\u20132174, Toronto, Canada, July\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.120. URL\nhttps://aclanthology.org/2023.acl-long.120.\nA\nSCALABILITY AND COMPUTATIONAL EFFICIENCY OF THE\nTREE-BUILDING PROCESS\nTo assess the computational efficiency and cost-effectiveness of RAPTOR\u2019s tree-building process,\nwe conducted experiments on a consumer-grade laptop, specifically an Apple M1 Mac with 16GB\nof RAM. These experiments aimed to demonstrate the scalability and feasibility of RAPTOR on\ntypical hardware. We varied the context length from 12,500 to 78,000 tokens and measured both the\ntoken expenditure and the time required to complete the tree-building process, from initial splitting\nand embedding to the construction of the final root node.\nFigure 5: Token cost as a function of document length for QASPER, NarrativeQA, and QuALITY.\nRAPTOR tree construction costs scale linearly with document length for each of the datasets.\nToken Expenditure\nWe empirically investigated the relationship between the initial document\nlength and the total number of tokens expended during the tree-building process, which includes\nboth the prompt and completion tokens. The document lengths varied significantly across the three\n15\nPublished as a conference paper at ICLR 2024\ndatasets examined: QuALITY, QASPER, and NarrativeQA. Figure 5 illustrates a clear linear corre-\nlation between the initial document length and the total token expenditure, emphasizing that RAP-\nTOR maintains a linear token scaling regardless of document complexity or length.\nFigure 6: Build time as a function of document length for documents of up to 80,000 tokens. RAP-\nTOR tree construction time scales linearly with document length for each of the datasets.\nBuild Time\nWe also empirically observed a consistent linear trend between the document length\nand the build time, as shown in Figure 6. This suggests that RAPTOR scales linearly in terms of\ntime, making it a viable solution for efficiently processing large corpora of varying lengths.\nConclusion\nOverall, our empirical results indicate that RAPTOR scales both in terms of tokens\nexpended and build time. Even as the complexity and volume of the input text grow, the cost of\nconstructing the tree scales predictably and linearly. This demonstrates that RAPTOR is computa-\ntionally efficient and well-suited for processing large and diverse corpora.\nB\nABLATION STUDY ON CLUSTERING MECHANISM IN RAPTOR\nTo assess the effectiveness of the clustering mechanism in our RAPTOR approach, we conducted\nan ablation study on the QuALITY dataset. This study compares RAPTOR\u2019s performance with a\nbalanced tree-style encoding and summarization of contiguous chunks, in contrast to our standard\nclustering method.\nB.1\nMETHODOLOGY\nBoth configurations in this ablation study utilized SBERT embeddings and UnifiedQA to maintain\nconsistency in retrieval. For RAPTOR, we employed our typical clustering and summarization\nprocess. In contrast, the alternative setup involved creating a balanced tree by recursively encoding\nand summarizing contiguous text chunks. We determined the window size for this setup based on\nthe average cluster size observed in RAPTOR, which is approximately 6.7 nodes. Hence, we chose\na window size of 7 nodes. The collapsed tree approach was applied for retrieval in both models.\nB.2\nRESULTS & DISCUSSION\nThe results of the ablation study are presented in table 9. The results from this ablation study clearly\nindicate an improvement in accuracy when employing RAPTOR\u2019s clustering mechanism over the\nrecency-based tree approach. This finding substantiates our hypothesis that the clustering strategy in\nRAPTOR is more effective in capturing homogeneous content for summarization, thereby enhancing\nthe overall retrieval performance.\n16\nPublished as a conference paper at ICLR 2024\nTable 9: Ablation study results comparing RAPTOR with a recency-based tree approach\nConfiguration\nAccuracy\nRAPTOR + SBERT embeddings + UnifiedQA\n56.6%\nRecency-based tree + SBERT embeddings + UnifiedQA\n55.8%\nC\nDATASET STATISTICS AND COMPRESSION RATIOS\nThe average ratio of the summary length to the sum of child node lengths across all datasets is 0.28,\nindicating a 72% compression rate. On average, the summary length is 131 tokens, and the average\nchild node length is 86 tokens. Below are the detailed statistics for all three datasets:\nTable 10: Statistics of Average Summary Length and Child Node Length Across Datasets\nDataset\nAvg.\nSummary\nLength\n(tokens)\nAvg. Child\nNode Text\nLength\n(tokens)\nAvg. # of\nChild Nodes\nPer Parent\nAvg.\nCompression\nRatio (%)\nAll Datasets\n131\n85.6\n6.7\n.28\nQuALITY\n124.4\n87.9\n5.7\n.28\nNarrativeQA\n129.7\n85.5\n6.8\n.27\nQASPER\n145.9\n86.2\n5.7\n.35\nD\nSUMMARIZATION PROMPT\nTable 11 shows the prompt used for summarization.\nTable 11: Prompt for Summarization\nRole\nContent\nsystem\nYou are a Summarizing Text Portal\nuser\nWrite a summary of the following, including as many key details as\npossible: {context}:\nE\nHALLUCINATION ANALYSIS\nTo assess the quality and accuracy of the summarizations within our RAPTOR model, we conducted\nan analysis focusing on hallucinations in the generated summaries. The summaries were generated\nby gpt-3.5-turbo and subsequently annotated to quantify the rates of hallucinations, to examine\nwhether such inaccuracies propagate to parent nodes, and to evaluate their impact on question-\nanswering (QA) tasks.\nE.1\nMETHODOLOGY\nWe randomly sampled 150 nodes across 40 stories and evaluated them for hallucinations. This\nsampling strategy provides a broad view of the model\u2019s performance across different contexts. Each\nnode was annotated by hand, and determined if it contained a hallucination.\nE.2\nFINDINGS\nOut of the 150 nodes sampled, 4% (6 nodes) contained some form of hallucination. Most commonly,\nthese hallucinations originated from the model adding minor information possibly from its training\ndata that was not present in the text being summarized, or from incorrectly extrapolating some\ninformation when creating the summary.\n17\nPublished as a conference paper at ICLR 2024\nExample:\nText of the child nodes:\n\u201dAnd you will come with me to my people? We may live here among them, and\nyou will be a great warrior\u2013oh, when Jor dies you may even be chief, for there is\nnone so mighty as my warrior...\u201dBut your father will not permit it\u2013Jor, my father,\nHigh Chief of the Galus, will not permit it, for like me you are cos-ata-lo. Oh, Co-\nTan, if we but could!... Bradley noticed that she spoke in English\u2013broken English\nlike Co-Tan\u2019s but equally appealing.\nSummary found in the parent of that node:\nThe protagonist, Bradley, is being asked by Co-Tan to stay with her people and\nbecome a great warrior, but he refuses and must return to his own country. Tom\nBillings of Santa Monica arrives and tells them he came to search for a man named\nBowen J. Tyler, Jr. Ajor, Co-Tan\u2019s sister, is excited about the possibility of going\nto Tom\u2019s country to see strange and wonderful things...\nThe hallucination here is that the summary states that Jr. Ajor and Co-Tan are sisters, but does not\nexplicitly mention or imply this.\nUpon reviewing all parent nodes, we found that hallucinations did not propagate to higher layers.\nGenerally, the hallucinations were minor and did not alter the thematic interpretation of the text.\nE.3\nIMPACT ON QA TASKS\nIn our findings, hallucinations had no discernible impact on the performance of QA tasks. This sug-\ngests that hallucination is not a major concerns for the summarization component in our RAPTOR\narchitecture.\nF\nPSEUDOCODE FOR RETRIEVAL METHODS\nAlgorithm 1 Tree Traversal Algorithm\nfunction TRAVERSETREE(tree, query, k)\nScurrent \u2190 tree.layer[0]\nfor layer in range(tree.num layers) do\ntopk \u2190 []\nfor node in Scurrent do\nscore \u2190 dot product(query, node)\ntop k.append((node, score))\nend for\nSlayer \u2190 sorted(top k)[:k].nodes\nScurrent \u2190 Slayer\nend for\nreturn S0 \u222a S1 \u222a S2 \u222a . . . \u222a Sk\nend function\nG\nQUALITATIVE ANALYSIS\nTo qualitatively examine RAPTOR\u2019s retrieval process, we test it on thematic, multi-hop questions\nabout a 1500-word version of the fairytale Cinderella. We compare the context retrieved by RAP-\nTOR with the context retrieved by Dense Passage Retrieval (DPR). Figure 4 in the main paper details\nthe retrieval process within RAPTOR\u2019s tree structure for two questions. The nodes that RAPTOR\nselects for each question are highlighted, while the leaf nodes that DPR selects for the same question\nare indicated with arrows. This comparison illustrates the advantage of RAPTOR\u2019s tree structure.\nRAPTOR selects nodes from different layers depending on the level of granularity required by the\n18\nPublished as a conference paper at ICLR 2024\nAlgorithm 2 Collapsed Tree Algorithm\nfunction COLLAPSEDTREE(tree, query, k, max tokens)\ntree \u2190 flatten(tree)\n\u25b7 Flatten tree into 1D\ntop nodes \u2190 []\nfor node in tree do\ntop nodes.append((node, dot product(query, node))\nend for\ntop nodes \u2190 sorted(top nodes)\nresult \u2190 []\ntotal tokens \u2190 0\nfor node in top nodes do\nif total tokens + node.token size < max tokens then\nresult.append(node)\nend if\ntotal tokens \u2190 total tokens + node.token size\nend for\nreturn result\nend function\nQuestion: What is the central theme of the story?\nRAPTOR\nFairy Godmother helps Cinderella attend a ball by transforming her rags. . . Cinderella\nimpresses the Prince at the ball. . . she loses track of time and has to run home alone in\nthe darkness. The Prince is unable to find Cinderella and goes in search of her . . . She\nforgave her sisters, and treated them always very kindly, and the Prince had great cause\nto be glad that he had found the glass slipper.\nDPR\nTwo mice were turned into footmen; four grasshoppers into white horses. Next, the\nFairy touched Cinderella\u2019s rags, and they became rich satin robes, trimmed with point\nlace. ... Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and\nthe rat and mice ran quickly away when they saw her; while all her fine dress turned to\nrags, and she had to run home alone. . . They told her a beautiful Princess had been at\nthe ball, with whom the Prince was delighted. They did not know it was Cinderella.\nQuestion: How does Cinderella find a happy ending?\nRAPTOR\nThe story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\nCinderella must return home before the clock strikes eleven or her dress will turn back\ninto rags... Cinderella impresses the Prince at the ball but leaves before he can find\nout who she is... The Prince searched for the owner of a lost glass slipper and found it\nbelonged to Cinderella. She forgave her sisters and the Prince was glad to have found\nher.\nDPR\nthe clock had struck Eleven. . . The Prince was very much surprised when he missed\nCinderella again, and leaving the ball, went in search of her. . .\nFairy touched Cin-\nderella\u2019s rags, and they became rich satin robes, trimmed with point lace... Her old\nshoes became a charming pair of glass slippers, which shone like diamonds. \u201cNow go\nto the ball, my love,\u201d she said, \u201cand enjoy yourself. But remember, you must leave the\nroom before the clock strikes eleven. If you do not your dress will return to its original\nrags.\u201d\nTable 12: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the\nfairytale Cinderella.\nquestion at hand. Further, the information that would be retrieved by DPR is more often than not\nincluded in the context retrieved by RAPTOR, either directly as a leaf node or indirectly as part of a\nsummary from a higher layer.\n\u201dThe first question we examine is \u201cHow does Cinderella find a happy ending?\u201d, a multi-hop question\nbest answered by synthesizing information from various text segments. To control for the language\nmodel\u2019s potential familiarity with the Cinderella story, we instructed it to rely solely on the retrieved\ninformation for its answers. Table 13 shows the text retrieved by both RAPTOR and DPR for this\nquestion. RAPTOR\u2019s context succinctly describes Cinderella\u2019s journey to happiness, while DPR\u2019s\nleaf nodes primarily focus on her initial transformation. The difference in retrieved information\n19\nPublished as a conference paper at ICLR 2024\nsignificantly impacts downstream tasks. When GPT-4 is provided with RAPTOR\u2019s context, it gen-\nerates a detailed answer: \u201cCinderella finds a happy ending when the Prince searches for the owner\nof the lost glass slipper and discovers it belongs to Cinderella. They eventually marry, transform-\ning Cinderella\u2019s life for the better.\u201d In contrast, using DPR\u2019s context, GPT-4 states: \u201cBased on the\ngiven context, it is not possible to determine how Cinderella finds a happy ending, as the text lacks\ninformation about the story\u2019s conclusion.\u201d\nThe second question we examine is \u201cWhat is the central theme of the story?\u201d, a thematic question\nthat requires holistic understanding of the entire text. The text retrieved by RAPTOR and DPR for\nthis question is shown in Table 13. The text retrieved by RAPTOR contains short descriptions of\nall the major parts of the story, whereas the text retrieved by DPR contains detailed descriptions of\na narrow subset of the story. Again, the difference in retrieval mechanisms affects the performance\nof GPT-4 when answering the question. Given DPR\u2019s context, it outputs \u201cThe central theme of\nthe story is transformation and the power of inner beauty, as Cinderella, a kind and humble girl, is\nmagically transformed into a beautiful princess, capturing the attention and admiration of the Prince\nand others at the ball.\u201d This answer only takes into account the first portion of the story, up until\nCinderella first meets the prince. In contrast, given RAPTOR\u2019s context, GPT-4 outputs \u201cThe central\ntheme of the story is transformation and overcoming adversity, as Cinderella, with the help of her\nFairy Godmother, transforms from a mistreated and downtrodden girl into a beautiful and confident\nyoung woman who ultimately finds happiness and love with the Prince.\u201d This is a more complete\nanswer, demonstrating a comprehensive understanding of the story.\nThis qualitative analysis indicates that RAPTOR outperforms prior retrieval mechanisms because\nthe information that it retrieves is more relevant and exhaustive, allowing for better performance on\ndownstream tasks.\nWe also created a 2600-word story along with questions about its narrative and theme. An excerpt\nfrom the story is present below and the full PDF of this story is linked here. For questions like \u201cWhat\nis the central theme of the story?\u201d, an upper-level node is retrieved which includes the sentence:\n\u201cThis story is about the power of human connection... inspiring and uplifting each other as they\npursued their passions.\u201d This summary, not explicitly present in the original text, almost directly\nanswers the question.\nExcerpt from \u201dThe Eager Writer\u201d:\n\u201dEthan\u2019s passion for writing had always been a part of him. As a child, he would\noften scribble stories and poems in his notebook, and as he grew older, his love\nfor writing only intensified. His evenings were often spent in the dim light of his\nroom, typing away at his laptop. He had recently taken a job as a content writer\nfor an online marketing firm to pay the bills, but his heart still longed for the\nworld of storytelling. However, like many aspiring writers, he struggled to find a\nfoothold in the industry. He took a job as a content writer for an online marketing\nfirm, but it was growing increasingly evident to him that this was not the path he\nwanted to pursue. It was during this time that he stumbled upon the Pathways\napp. The app offered a platform for people in similar professions to connect and\nshare knowledge, and he saw it as an opportunity to finally connect with others\nwho shared his passion for writing. Ethan saw an opportunity to meet others who\nshared his passion and could offer guidance and mentorship. He quickly signed\nup and was surprised by the number of writers he found on the platform, from\nwell establish professionals to beginners just starting out in the business.\u201d\nH\nNARRATIVEQA EVALUATION SCRIPT\nWe made several modifications to AllenNLP\u2019s evaluation script3 to better fit our evaluation needs:\n\u2022 Added Smoothing: Smoothing was incorporated to handle cases where BLEU score is\nzero, due to no n-gram matches occurring in the reference text. A BLEU score of zero\nskews the results, leading to an overly harsh evaluation for rare or novel phrases. By adding\n3docs.allennlp.org/models/main/models/rc/tools/narrativeqa/\n20\nPublished as a conference paper at ICLR 2024\na smoothing function, we prevent the BLEU scores from dropping to zero, providing a more\nfair evaluation.\n\u2022 Modified BLEU-4 Weighting: The original script applied a weight of 1 to the highest\norder n-gram (4-gram) and 0 to the rest in its BLEU-4 calculation (i.e., weights=(0, 0,\n0, 1)). This approach may overly focus on 4-gram matches while neglecting lower-order\nmatches. To provide a more balanced evaluation, we evenly distributed the weight across\nall n-gram levels, changing the weights for the BLEU-4 calculation to (0.25, 0.25, 0.25,\n0.25).\n\u2022 Tokenization before Mapping in METEOR Calculation: The original script utilized a\nsimple split and map method for METEOR calculation. We fixed this by first tokenizing the\ntext and then mapping the tokens. This amendment improves the accuracy of the METEOR\ncalculation by taking into account the correct linguistic boundaries of words.\nQuestion: What is the central theme of the story?\nRAPTOR\nFairy Godmother helps Cinderella attend a ball by transforming her rags. . . Cinderella\nimpresses the Prince at the ball. . . she loses track of time and has to run home alone in\nthe darkness. The Prince is unable to find Cinderella and goes in search of her . . . She\nforgave her sisters, and treated them always very kindly, and the Prince had great cause\nto be glad that he had found the glass slipper.\nDPR\nTwo mice were turned into footmen; four grasshoppers into white horses. Next, the\nFairy touched Cinderella\u2019s rags, and they became rich satin robes, trimmed with point\nlace. ... Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and\nthe rat and mice ran quickly away when they saw her; while all her fine dress turned to\nrags, and she had to run home alone. . . They told her a beautiful Princess had been at\nthe ball, with whom the Prince was delighted. They did not know it was Cinderella.\nQuestion: How does Cinderella find a happy ending?\nRAPTOR\nThe story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\nCinderella must return home before the clock strikes eleven or her dress will turn back\ninto rags... Cinderella impresses the Prince at the ball but leaves before he can find\nout who she is... The Prince searched for the owner of a lost glass slipper and found it\nbelonged to Cinderella. She forgave her sisters and the Prince was glad to have found\nher.\nDPR\nthe clock had struck Eleven. . . The Prince was very much surprised when he missed\nCinderella again, and leaving the ball, went in search of her. . .\nFairy touched Cin-\nderella\u2019s rags, and they became rich satin robes, trimmed with point lace... Her old\nshoes became a charming pair of glass slippers, which shone like diamonds. \u201cNow go\nto the ball, my love,\u201d she said, \u201cand enjoy yourself. But remember, you must leave the\nroom before the clock strikes eleven. If you do not your dress will return to its original\nrags.\u201d\nTable 13: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the\nfairytale Cinderella.\nI\nANALYSIS OF DIFFERENT LAYERS ON RAPTOR\u2019S PERFORMANCE\nI.1\nHOW DO DIFFERENT LAYERS IMPACT PERFORMANCE ?\nIn this section, we present a detailed breakdown of RAPTOR\u2019s retrieval performance when querying\ndifferent layers of the hierarchical tree structure for various stories. These tables validate the utility\nof RAPTOR\u2019s multi-layered structure for diverse query requirements.\nTable 14: Performance of RAPTOR when querying different layers of the tree for Story 2.\nLayers Queried / Start Layer\nLayer 0 (Leaf Nodes)\nLayer 1\nLayer 2\n1 layer\n58.8\n47.1\n41.1\n2 layers\n-\n64.7\n52.9\n3 layers\n-\n-\n47.1\n21\nPublished as a conference paper at ICLR 2024\nFigure 7: Histogram showing the percentage of nodes retrieved from different layers of the RAPTOR\ntree across three datasets (NarrativeQA, Quality, and Qasper) using three retrievers (SBERT, BM25,\nand DPR). The data indicate that a substantial portion of the nodes contributing to the final retrieval\ncomes from non-leaf layers, with a notable percentage from the first and second layers, highlighting\nthe importance of RAPTOR\u2019s hierarchical summarization in the retrieval process.\nTable 15: Performance of RAPTOR when querying different layers of the tree for Story 3.\nLayers Queried / Start Layer\nLayer 0 (Leaf Nodes)\nLayer 1\nLayer 2\n1 layer\n66.6\n61.1\n61.1\n2 layers\n-\n66.6\n66.6\n3 layers\n-\n-\n83.3\nTable 16: Performance of RAPTOR when querying different layers of the tree for Story 4.\nLayers Queried / Start Layer\nLayer 0 (Leaf Nodes)\nLayer 1\n1 layer\n94.7\n84.2\n2 layers\n-\n89.4\nTable 17: Performance of RAPTOR when querying different layers of the tree for Story 5.\nLayers Queried / Start Layer\nLayer 0 (Leaf Nodes)\nLayer 1\n1 layer\n57.9\n47.3\n2 layers\n-\n68.4\nI.2\nWHICH LAYERS DO RETRIEVED NODES COME FROM ?\nWe further conduct an ablation study across all three datasets and across three different retrievers\nwith RAPTOR with the collapsed tree retrieval to examine the layers from which the retrieved nodes\noriginate. We observe that between 18.5% to 57% of the retrieved nodes come from non-leaf nodes.\nAs illustrated in Figure 7, the retrieval pattern across layers reveals the importance of RAPTOR\u2019s\nmulti-layered tree structure. Notably, a significant percentage of the nodes retrieved by RAPTOR\nusing the DPR retriever for the NarrativeQA dataset come from the first and second layers of the\ntree, as opposed to the leaf nodes. This pattern is consistent across the other datasets and retrievers,\nalbeit with varying percentages.\nTable 18: Percentage of nodes from non-leaf nodes across different datasets and retrievers\nDataset\nDPR\nSBERT\nBM25\nNarrativeQA\n57.36%\n36.78%\n34.96%\nQuality\n32.28%\n24.41%\n32.36%\nQasper\n22.93%\n18.49%\n22.76%\n22\nPublished as a conference paper at ICLR 2024\nTable 19: Percentage of nodes from different layers with DPR as the retriever\nLayer\nNarrativeQA\nQuality\nQasper\n0\n42.64%\n67.71%\n77.07%\n1\n45.00%\n29.43%\n21.88%\n2\n10.57%\n2.85%\n1.05%\n3\n1.78%\n-\n-\n4\n0.003%\n-\n-\nTable 20: Percentage of nodes from different layers with SBERT as the retriever\nLayer\nNarrativeQA\nQuality\nQasper\n0\n63.22%\n75.59%\n81.51%\n1\n31.51%\n22.78%\n17.84%\n2\n4.85%\n1.63%\n0.65%\n3\n0.42%\n-\n-\nTable 21: Percentage of nodes from different layers with BM25 as the retriever\nLayer\nNarrativeQA\nQuality\nQasper\n0\n65.04%\n67.64%\n77.24%\n1\n28.79%\n28.85%\n21.57%\n2\n5.36%\n3.51%\n1.19%\n3\n0.81%\n-\n-\n23\n"
  },
  {
    "title": "Advances in 3D Generation: A Survey",
    "link": "https://arxiv.org/pdf/2401.17807.pdf",
    "upvote": "16",
    "text": "Advances in 3D Generation: A Survey\nXiaoyu Li1\u22c6 Qi Zhang1\u22c6 Di Kang1 Weihao Cheng2 Yiming Gao2\nJingbo Zhang3 Zhihao Liang4 Jing Liao3 Yan-Pei Cao1,2 Ying Shan1,2\u2020\n1Tencent AI Lab 2ARC Lab, Tencent PCG 3City University of Hong Kong 4South China University of Techonology\n\u22c6Equal contribution. \u2020Corresponding author.\nDeepSDF\nPoint\u00b7E\n3D-GAN\nDMTet\nEG3D\nZero-1-to-3\nDreamFusion\nInstant3D\nFigure 1: In this survey, we investigate a large variety of 3D generation methods. Over the past decade, 3D generation has achieved remarkable\nprogress and has recently garnered considerable attention due to the success of generative AI in images and videos. 3D generation results\nfrom 3D-GAN [WZX\u221716], DeepSDF [PFS\u221719], DMTet [SGY\u221721], EG3D [CLC\u221722], DreamFusion [PJBM23], PointE [NJD\u221722], Zero-1-\nto-3 [LWVH\u221723] and Instant3D [LTZ\u221723].\nAbstract\nGenerating 3D models lies at the core of computer graphics and has been the focus of decades of research. With the emergence\nof advanced neural representations and generative models, the field of 3D content generation is developing rapidly, enabling\nthe creation of increasingly high-quality and diverse 3D models. The rapid growth of this field makes it difficult to stay abreast\nof all recent developments. In this survey, we aim to introduce the fundamental methodologies of 3D generation methods and es-\ntablish a structured roadmap, encompassing 3D representation, generation methods, datasets, and corresponding applications.\nSpecifically, we introduce the 3D representations that serve as the backbone for 3D generation. Furthermore, we provide a com-\nprehensive overview of the rapidly growing literature on generation methods, categorized by the type of algorithmic paradigms,\nincluding feedforward generation, optimization-based generation, procedural generation, and generative novel view synthesis.\nLastly, we discuss available datasets, applications, and open challenges. We hope this survey will help readers explore this\nexciting topic and foster further advancements in the field of 3D content generation.\n1. Introduction\nAutomatically generating 3D models using algorithms has long\nbeen a significant task in computer vision and graphics. This task\nhas garnered considerable interest due to its broad applications\nin video games, movies, virtual characters, and immersive expe-\nriences, which typically require a wealth of 3D assets. Recently,\nthe success of neural representations, particularly Neural Radiance\nFields (NeRFs) [MST\u221720,BMT\u221721,MESK22,KKLD23], and gen-\nerative models such as diffusion models [HJA20,RBL\u221722a], has led\nto remarkable advancements in 3D content generation.\narXiv:2401.17807v1  [cs.CV]  31 Jan 2024\n2\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\nIn the realm of 2D content generation, recent advancements\nin generative models have steadily enhanced the capacity for im-\nage generation and editing, leading to increasingly diverse and\nhigh-quality results. Pioneering research on generative adversar-\nial networks (GANs) [GPAM\u221714, AQW19], variational autoen-\ncoders (VAEs) [KPHL17, PGH\u221716, KW13], and autoregressive\nmodels [RWC\u221719, BMR\u221720] has demonstrated impressive out-\ncomes. Furthermore, the advent of generative artificial intelligence\n(AI) and diffusion models [HJA20, ND21, SCS\u221722] signifies a\nparadigm shift in image manipulation techniques, such as Stable\nDiffusion [RBL\u221722a], Imagen [SCS\u221722], Midjourney [Mid], or\nDALL-E 3 [Ope]. These generative AI models enable the creation\nand editing of photorealistic or stylized images, or even videos\n[CZC\u221724, HSG\u221722, SPH\u221723, GNL\u221723], using minimal input like\ntext prompts. As a result, they often generate imaginative content\nthat transcends the boundaries of the real world, pushing the limits\nof creativity and artistic expression. Owing to their \u201cemergent\u201d ca-\npabilities, these models have redefined the limits of what is achiev-\nable in content generation, expanding the horizons of creativity and\nartistic expression.\nThe demand to extend 2D content generation into 3D space is\nbecoming increasingly crucial for applications in generating 3D as-\nsets or creating immersive experiences, particularly with the rapid\ndevelopment of the metaverse. The transition from 2D to 3D con-\ntent generation, however, is not merely a technological evolution. It\nis primarily a response to the demands of modern applications that\nnecessitate a more intricate replication of the physical world, which\n2D representations often fail to provide. This shift highlights the\nlimitations of 2D content in applications that require a comprehen-\nsive understanding of spatial relationships and depth perception.\nAs the significance of 3D content becomes increasingly evident,\nthere has been a surge in research efforts dedicated to this domain.\nHowever, the transition from 2D to 3D content generation is not\na straightforward extension of existing 2D methodologies. Instead,\nit involves tackling unique challenges and re-evaluating data rep-\nresentation, formulation, and underlying generative models to ef-\nfectively address the complexities of 3D space. For instance, it\nis not obvious how to integrate the 3D scene representations into\n2D generative models to handle higher dimensions, as required for\n3D generation. Unlike images or videos which can be easily col-\nlected from the web, 3D assets are relatively scarce. Furthermore,\nevaluating the quality of generated 3D models presents additional\nchallenges, as it is necessary to develop better formulations for ob-\njective functions, particularly when considering multi-view con-\nsistency in 3D space. These complexities demand innovative ap-\nproaches and novel solutions to bridge the gap between 2D and 3D\ncontent generation.\nWhile not as prominently featured as its 2D counterpart, 3D\ncontent generation has been steadily progressing with a series of\nnotable achievements. The representative examples shown in Fig.\n1 demonstrate significant improvements in both quality and diver-\nsity, transitioning from early methods like 3D-GAN [WZX\u221716] to\nrecent approaches like Instant3D [LTZ\u221723]. Therefore, This survey\npaper seeks to systematically explore the rapid advancements and\nmultifaceted developments in 3D content generation. We present a\nstructured overview and comprehensive roadmap of the many re-\ncent works focusing on 3D representations, 3D generation meth-\nods, datasets, and applications of 3D content generation, and to\noutline open challenges.\nFig. 2 presents an overview of this survey. We first discuss the\nscope and related work of this survey in Sec. 2. In the following\nsections, we examine the core methodologies that form the foun-\ndation of 3D content generation. Sec. 3 introduces the primary\nscene representations and their corresponding rendering functions\nused in 3D content generation. Sec. 4 explores a wide variety of\n3D generation methods, which can be divided into four categories\nbased on their algorithmic methodologies: feedforward generation,\noptimization-based generation, procedural generation, and genera-\ntive novel view synthesis. An evolutionary tree of these methodolo-\ngies is also depicted to illustrate their primary branch. As data accu-\nmulation plays a vital role in ensuring the success of deep learning\nmodels, we present related datasets employed for training 3D gen-\neration methods. In the end, we include a brief discussion on related\napplications, such as 3D human and face generation, outline open\nchallenges, and conclude this survey. We hope this survey offers a\nsystematic summary of 3D generation that could inspire subsequent\nwork for interested readers.\nIn this work, we present a comprehensive survey on 3D genera-\ntion, with two main contributions:\n\u2022 Given the recent surge in contributions based on generative mod-\nels in the field of 3D vision, we provide a comprehensive and\ntimely literature review of 3D content generation, aiming to offer\nreaders a rapid understanding of the 3D generation framework\nand its underlying principles.\n\u2022 We propose a multi-perspective categorization of 3D generation\nmethods, aiming to assist researchers working on 3D content\ngeneration in specific domains to quickly identify relevant works\nand facilitate a better understanding of the related techniques.\n2. Scope of This Survey\nIn this survey, we concentrate on the techniques for the generation\nof 3D models and their related datasets and applications. Specifi-\ncally, we first give a short introduction to the scene representation.\nOur focus then shifts to the integration of these representations and\nthe generative models. Then, we provide a comprehensive overview\nof the prominent methodologies of generation methods. We also ex-\nplore the related datasets and cutting-edge applications such as 3D\nhuman generation, 3D face generation, and 3D editing, all of which\nare enhanced by these techniques.\nThis survey is dedicated to systematically summarizing and cat-\negorizing 3D generation methods, along with the related datasets\nand applications. The surveyed papers are mostly published in ma-\njor computer vision and computer graphics conferences/journals as\nwell as some preprints released on arXiv in 2023. While it\u2019s chal-\nlenging to exhaust all methods related to 3D generation, we hope to\ninclude as many major branches of 3D generation as possible. We\ndo not delve into detailed explanations for each branch, instead, we\ntypically introduce some representative works within it to explain\nits paradigm. The details of each branch can be found in the related\nwork section of these cited papers.\nRelated Survey. Neural reconstruction and rendering with scene\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n3\nOptimization-Based \nGeneration\n3D Generation Methods\nApplications\nFeedforward \nGeneration\nProcedural \nGeneration\nGenerative Novel View \nSynthesis\n3D Representations\n\u2026\nMeshes\nPoint Clouds\nVoxel Grids\nDatasets\n3D Data\nMulti-view Images\nSingle Images\n3D Humans\n3D Faces\n3D Objects\n3D Scenes\nFigure 2: Overview of this survey, including 3D representations, 3D generation methods, datasets and applications. Specifically, we introduce\nthe 3D representations that serve as the backbone for 3D generation. Furthermore, we provide a comprehensive overview of the rapidly\ngrowing literature on generation methods, categorized by the type of algorithmic paradigms, including feedforward generation, optimization-\nbased generation, procedural generation, and generative novel view synthesis. Finally, we provide a brief discussion on popular datasets and\navailable applications.\nrepresentations are closely related to 3D generation. However, we\nconsider these topics to be outside the purview of this report. For a\ncomprehensive discussion on neural rendering, we direct readers to\n[TFT\u221720,TTM\u221722], and for a broader examination of other neural\nrepresentations, we recommend [KBM\u221720,XTS\u221722]. Our primary\nfocus is on exploring techniques that generate 3D models. There-\nfore, this review does not encompass research on generation meth-\nods for 2D images within the realm of visual computing. For fur-\nther information on a specific generation method, readers can refer\nto [Doe16] (VAEs), [GSW\u221721] (GANs), [PYG\u221723,CHIS23] (Dif-\nfusion) and [KNH\u221722] (Transformers) for a more detailed under-\nstanding. There are also some surveys related to 3D generation that\nhave their own focuses such as 3D-aware image synthesis [XX23],\n3D generative models [SPX\u221722], Text-to-3D [LZW\u221723] and deep\nlearning for 3D point clouds [GWH\u221720]. In this survey, we give a\ncomprehensive analysis of different 3D generation methods.\n3. Neural Scene Representations\nIn the domain of 3D AI-generated content, adopting a suitable rep-\nresentation of 3D models is essential. The generation process typi-\ncally involves a scene representation and a differentiable rendering\nalgorithm for creating 3D models and rendering 2D images. Con-\nversely, the created 3D models or 2D images could be supervised in\nthe reconstruction domain or image domain, as illustrated in Fig. 3.\nSome methods directly supervise the 3D models of the scene rep-\nresentation, while others render the scene representation into im-\nages and supervise the resulting renderings. In the following, we\nbroadly classify the scene representations into three groups: ex-\nplicit scene representations (Section 3.1), implicit representations\n(Section 3.2), and hybrid representations (Section 3.3). Note that,\nthe rendering methods (e.g. ray casting, volume rendering, raster-\nization, etc), which should be differentiable to optimize the scene\nrepresentations from various inputs, are also introduced.\n3.1. Explicit Representations\nExplicit scene representations serve as a fundamental module in\ncomputer graphics and vision, as they offer a comprehensive means\nof describing 3D scenes. By depicting scenes as an assembly\nof basic primitives, including point-like primitives, triangle-based\nmeshes, and advanced parametric surfaces, these representations\ncan create detailed and accurate visualizations of various environ-\nments and objects.\n3.1.1. Point Clouds\nA point cloud is a collection of elements in Euclidean space, rep-\nresenting discrete points with addition attributes (e.g. colors and\nnormals) in three-dimensional space. In addition to simple points,\nwhich can be considered infinitesimally small surface patches,\noriented point clouds with a radius (surfels) can also be used\n[PZVBG00]. Surfels are used in computer graphics for rendering\n4\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\nScene Representation\nX\nY\nZ\nPoint\nLatent\nVoxel\nImplicit Neural Network\nPolygon\nGaussian\nx\nTri-planes\nReconstruction Domain\nPoint Cloud\nVolume\nSDF\nImage Domain\n3D Supervision\nMesh\nRendering\nx\n2D Supervision\nSurface Rendering\nVolume Rendering\nNeural Network \nRendering\nMixed Rendering\nFigure 3: Neural scene representations used for 3D generation, including explicit, implicit, and hybrid representations. The 3D generation\ninvolves the use of scene representations and a differentiable rendering algorithm to create 3D models or render 2D images. On the flip\nside, these 3D models or 2D images can function as the reconstruction domain or image domain, overseeing the 3D generation of scene\nrepresentations.\npoint clouds (called splitting), which are differentiable [YSW\u221719,\nKKLD23] and allow researchers to define differentiable render-\ning pipelines to adjust point cloud positions and features, such\nas radius or color. Techniques like Neural Point-based Rendering\n[ASK\u221720, DZL\u221720], SynSin [WGSJ20], Pulsar [LZ21, KPLD21]\nand ADOP [RFS22] leverage learnable features to store informa-\ntion about the surface appearance and shape, enabling more accu-\nrate and detailed rendering results. Several other methods, such as\nFVS [RK20], SVS [RK21], and FWD-Transformer [CRJ22], also\nemploy learnable features to improve the rendering quality. These\nmethods typically embed features into point clouds and warp them\nto target views to decode color values, allowing for more accurate\nand detailed reconstructions of the scene.\nBy incorporating point cloud-based differentiable renderers into\nthe 3D generation process, researchers can leverage the benefits\nof point clouds while maintaining compatibility with gradient-\nbased optimization techniques. This process can be generally cate-\ngorized into two different ways: point splitting which blends the\ndiscrete samples with some local deterministic blurring kernels\n[ZPVBG02, LKL18, ID18, RROG18], and conventional point ren-\nderer [ASK\u221720, DZL\u221720, KPLD21, RALB22]. These methods fa-\ncilitate the generation and manipulation of 3D point cloud models\nwhile maintaining differentiability, which is essential for training\nand optimizing neural networks in 3D generation tasks.\n3.1.2. Meshes\nBy connecting multiple vertices with edges, more complex ge-\nometric structures (e.g. wireframes and meshes) can be formed\n[BKP\u221710]. These structures can then be further refined by using\npolygons, typically triangles or quadrilaterals, to create realistic\nrepresentations of objects [SS87]. Meshes provide a versatile and\nefficient means of representing intricate shapes and structures, as\nthey can be easily manipulated and rendered by computer algo-\nrithms. The majority of graphic editing toolchains utilize triangle\nmeshes. This type of representation is indispensable for any digi-\ntal content creation (DCC) pipeline, given its wide acceptance and\ncompatibility. To align seamlessly with these pipelines, neural net-\nworks can be strategically trained to predict discrete vertex loca-\ntions [BNT21, TZN19]. This ability allows for the direct importa-\ntion of these locations into any DCC pipeline, facilitating a smooth\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n5\nand efficient workflow. In contrast to predicting discrete textures,\ncontinuous texture methods optimized via neural networks are pro-\nposed, such as texture fields [OMN\u221719] and NeRF-Tex [BGP\u221722].\nIn this way, it could provide a more refined and detailed texture, en-\nhancing the overall quality and realism of the generated 2D models.\nIntegrating mesh representation into 3D generation requires\nthe use of mesh-based differentiable rendering methods, which\nenable meshes to be rasterized in a manner that is compatible\nwith gradient-based optimization. Several such techniques have\nbeen proposed, including OpenDR [LB14], neural mesh renderer\n[KUH18], Paparazzi [LTJ18], and Soft Rasterizer [LLCL19]. Ad-\nditionally, general-purpose physically based renderers like Mitsuba\n2 [NDVZJ19] and Taichi [HLA\u221719] support mesh-based differen-\ntiable rendering through automatic differentiation.\n3.1.3. Multi-layer Representations\nThe use of multiple semi-transparent colored layers for represent-\ning scenes has been a popular and successful scheme in real-\ntime novel view synthesis [ZTF\u221718]. Using Layered Depth Im-\nage (LDI) representation [SGHS98] is a notable example, ex-\ntending traditional depth maps by incorporating multiple layers\nof depth maps, each with associated color values. Several meth-\nods [PZ17, CGT\u221719, SSKH20] have drawn inspiration from the\nLDI representation and employed deep learning advancements to\ncreate networks capable of predicting LDIs. In addition to LDIs,\nStereomagnification [ZTF\u221718] initially introduced the multiple im-\nage (MPI) representation. It describes scenes using multiple front-\nparallel semi-transparent layers, including colors and opacity, at\nfixed depth ranges through plane sweep volumes. With the help\nof volume rendering and homography projection, the novel view\ncould be synthesized in real-time. Building on Stereomagnification\n[ZTF\u221718], various methods [FBD\u221719, MSOC\u221719, STB\u221719] have\nadopted the MPI representation to enhance rendering quality. The\nmulti-layer representation has been further expanded to accommo-\ndate wider fields of view in [BFO\u221720,ALG\u221720,LXM\u221720] by sub-\nstituting planes with spheres. As research in this domain continues\nto evolve, we can expect further advancements in these methods,\nleading to more efficient and effective 3D generation techniques\nfor real-time rendering.\n3.2. Implicit Representations\nImplicit representations have become the scene representation of\nchoice for problems in view synthesis or shape reconstruction, as\nwell as many other applications across computer graphics and vi-\nsion. Unlike explicit scene representations that usually focus on\nobject surfaces, implicit representations could define the entire vol-\nume of a 3D object, and use volume rendering for image synthesis.\nThese representations utilize mathematical functions, such as radi-\nance fields [MST\u221720] or signed distance fields [PFS\u221719,CZ19], to\ndescribe the properties of a 3D space.\n3.2.1. Neural Radiance Fields\nNeural Radiance Fields (NeRFs) [MST\u221720] have gained promi-\nnence as a favored scene representation method for a wide range of\napplications. Fundamentally, NeRFs introduce a novel representa-\ntion of 3D scenes or geometries. Rather than utilizing point clouds\nand meshes, NeRFs depict the scene as a continuous volume. This\napproach involves obtaining volumetric parameters, such as view-\ndependent radiance and volume density, by querying an implicit\nneural network. This innovative representation offers a more fluid\nand adaptable way to capture the intricacies of 3D scenes, paving\nthe way for enhanced rendering and modeling techniques.\nSpecifically, NeRF [MST\u221720] represents the scene with a con-\ntinuous volumetric radiance field, which utilizes MLPs to map the\nposition x and view direction r to a density \u03c3 and color c. To render\na pixel\u2019s color, NeRF casts a single ray r(t) = o+td and evaluates\na series of points {ti} along the ray. The evaluated {(\u03c3i,ci)} at the\nsampled points are accumulated into the color C(r) of the pixel via\nvolume rendering [Max95]:\nC(r)=\u2211\ni\nTi\u03b1ici,\nwhere Ti = exp\n \n\u2212\ni\u22121\n\u2211\nk=0\n\u03c3k\u03b4k\n!\n,\n(1)\nand \u03b1i = 1\u2212exp(\u2212\u03c3i\u03b4i) indicates the opacity of the sampled point.\nAccumulated transmittance Ti quantifies the probability of the ray\ntraveling from t0 to ti without encountering other particles, and \u03b4i =\nti \u2212ti\u22121 denotes the distance between adjacent samples.\nNeRFs\n[MST\u221720, NG21, BMT\u221721, BMV\u221722, VHM\u221722,\nLWC\u221723] have seen widespread success in problems such as edi-\ntion [MBRS\u221721,ZLLD21,CZL\u221722,YSL\u221722], joint optimization of\ncameras [LMTL21, WWX\u221721, CCW\u221723, TRMT23], inverse ren-\ndering [ZLW\u221721, SDZ\u221721, BBJ\u221721, ZSD\u221721, ZZW\u221723, LZF\u221723],\ngeneralization [YYTK21, WWG\u221721, CXZ\u221721, LFS\u221721, JLF22,\nHZF\u221723b],\nacceleration\n[RPLG21, GKJ\u221721, ZZZ\u221723b],\nand\nfree-viewpoint video [DZY\u221721, LSZ\u221722, PCPMMN21]. Apart\nfrom the above applications, NeRF-based representation can\nalso be used for digit avatar generation, such as face and body\nreenactment [PDW\u221721, GCL\u221721, LHR\u221721, WCS\u221722, HPX\u221722].\nNeRFs have been extend to various fields such as robotics\n[KFH\u221722, ZKW\u221723, ACC\u221722], tomography [RWL\u221722, ZLZ\u221722],\nimage processing [HZF\u221722, MLL\u221722b, HZF\u221723a], and astron-\nomy [LSC\u221722].\n3.2.2. Neural Implicit Surfaces\nWithin the scope of shape reconstruction, a neural network pro-\ncesses a 3D coordinate as input and generates a scalar value, which\nusually signifies the signed distance to the surface. This method is\nparticularly effective in filling in missing information and generat-\ning smooth, continuous surfaces. The implicit surface representa-\ntion defines the scene\u2019s surface as a learnable function f that spec-\nifies the signed distance f(x) from each point to the surface. The\nfundamental surface can then be extracted from the zero-level set,\nS = {x \u2208 R3| f(x) = 0}, providing a flexible and efficient way to re-\nconstruct complex 3D shapes. Implicit surface representations of-\nfer numerous advantages, as they eliminate the need to define mesh\ntemplates. As a result, they can represent objects with unknown or\nchanging topology in dynamic scenarios. Specifically, implicit sur-\nface representations recover signed distance fields for shape mod-\neling using MLPs with coordinate inputs. These initial propos-\nals sparked widespread enthusiasm and led to various improve-\nments focusing on different aspects, such as enhancing training\nschemes [DZW\u221720, YAK\u221720, ZML\u221722], leveraging global-local\ncontext [XWC\u221719, EGO\u221720, ZPL\u221722], adopting specific parame-\n6\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\nterizations [GCV\u221719, CTZ20, YRSh21, BSKG22], and employing\nspatial partitions [GCS\u221720,TTG\u221720,TLY\u221721,WLG\u221723].\nNeuS [WLL\u221721] and VolSDF [YGKL21] extend the basic NeRF\nformulation by integrating an SDF into volume rendering, which\ndefines a function to map the signed distance to density \u03c3. It attains\na locally maximal value at surface intersection points. Specifically,\naccumulated transmittance T(t) along the ray r(t) = o +td is for-\nmulated as a sigmoid function: T(t) = \u03a6( f(t)) = (1 + es f(t))\u22121,\nwhere s and f(t) refer to a learnable parameter and the signed dis-\ntance function of points at r(t), respectively. Discrete opacity val-\nues \u03b1i can then be derived as:\n\u03b1i = max\n\u0012\u03a6s ( f(ti))\u2212\u03a6s ( f(ti+1))\n\u03a6s ( f(ti))\n,0\n\u0013\n.\n(2)\nNeuS employs volume rendering to recover the underlying SDF\nbased on Eqs. (1) and (2). The SDF is optimized by minimizing the\nphotometric loss between the rendering results and ground-truth\nimages.\nBuilding upon NeuS and VolSDF, NeuralWarp [DBD\u221722], Geo-\nNeuS [FXOT22], MonoSDF [YPN\u221722] leverage prior geometry in-\nformation from MVS methods. IRON [ZLLS22], MII [ZSH\u221722],\nand WildLight [CLL23] apply high-fidelity shape reconstruction\nvia SDF for inverse rendering. HF-NeuS [WSW22] and PET-Neus\n[WSW23] integrate additional displacement networks to fit the\nhigh-frequency details. LoD-NeuS [ZZF\u221723] adaptively encodes\nLevel of Detail (LoD) features for shape reconstruction.\n3.3. Hybrid Representations\nImplicit representations have indeed demonstrated impressive re-\nsults in various applications as mentioned above. However, most\nof the current implicit methods rely on regression to NeRF or SDF\nvalues, which may limit their ability to benefit from explicit super-\nvision on the target views or surfaces. Explicit representation could\nimpose useful constraints during training and improve the user ex-\nperience. To capitalize on the complementary benefits of both rep-\nresentations, researchers have begun exploring hybrid representa-\ntions. These involve scene representations (either explicit or im-\nplicit) that embed features utilizing rendering algorithms for view\nsynthesis.\n3.3.1. Voxel Grids\nEarly work [WSK\u221715, CXG\u221716, MS15] depicted 3D shapes us-\ning voxels, which store coarse occupancy (inside/outside) values\non a regular grid. This approach enabled powerful convolutional\nneural networks to operate natively and produce impressive results\nin 3D reconstruction and synthesis [DRB\u221718, WZX\u221716, BLW16].\nThese methods usually use explicit voxel grids as the 3D represen-\ntation. Recently, to address the slow training and rendering speeds\nof implicit representations, the 3D voxel-based embedding methods\n[LGZL\u221720,FKYT\u221722,SSN\u221722,SSC22] have been proposed. These\nmethods encode the spatial information of the scene and decode the\nfeatures more efficiently. Moreover, Instant-NGP [MESK22] intro-\nduces the multi-level voxel grids encoded implicitly via the hash\nfunction for each level. It facilitates rapid optimization and ren-\ndering while maintaining a compact model. These advancements\nin 3D shape representation and processing techniques have signif-\nicantly enhanced the efficiency and effectiveness of 3D generation\napplications.\n3.3.2. Tri-plane\nTri-plane representation is an alternative approach to using voxel\ngrids for embedding features in 3D shape representation and neural\nrendering. The main idea behind this method is to decompose a 3D\nvolume into three orthogonal planes (e.g., XY, XZ, and YZ planes)\nand represent the features of the 3D shape on these planes. Specifi-\ncally, TensoRF [CXG\u221722] achieves similar model compression and\nacceleration by replacing each voxel grid with a tensor decompo-\nsition into planes and vectors. Tri-planes are efficient and capable\nof scaling with the surface area rather than volume and naturally\nintegrate with expressive, fine-tuned 2D generative architectures.\nIn the generative setting, EG3D [CLC\u221722] proposes a spatial de-\ncomposition into three planes whose values are added together to\nrepresent a 3D volume. NFD [SCP\u221723] introduces diffusion on 3D\nscenes, utilizing 2D diffusion model backbones and having built-in\ntri-plane representation.\n3.3.3. Hybrid Surface Representation\nDMTet, a recent development cited in [SGY\u221721], is a hybrid three-\ndimensional surface representation that combines both explicit and\nimplicit forms to create a versatile and efficient model. It segments\nthe 3D space into dense tetrahedra, thereby forming an explicit par-\ntition. By integrating explicit and implicit representations, DMTet\ncan be optimized more efficiently and transformed seamlessly into\nexplicit structures like mesh representations. During the generation\nprocess, DMTet can be differentiably converted into a mesh, which\nenables swift high-resolution multi-view rendering. This innovative\napproach offers significant improvements in terms of efficiency and\nversatility in 3D modeling and rendering.\n4. Generation Methods\nIn the past few years, the rapid development of generative mod-\nels in 2D image synthesis, such as generative adversarial networks\n(GANs) [GPAM\u221714, AQW19], variational autoencoders (VAEs)\n[KPHL17, PGH\u221716, KW13], autoregressive models [RWC\u221719,\nBMR\u221720], diffusion models [HJA20,ND21,SCS\u221722], etc., has led\nto their extension and combination with these scene representations\nfor 3D generation. Tab. 1 shows well-known examples of 3D gen-\neration using generative models and scene representations. These\nmethods may use different scene representations in the generation\nspace, where the representation is generated by the generative mod-\nels, and the reconstruction space, where the output is represented.\nFor example, AutoSDF [MCST22a] uses a transformer-based au-\ntoregressive model to learn a feature voxel grid and decode this rep-\nresentation to SDF for reconstruction. EG3D [CLC\u221722] employs\nGANs to generate samples in latent space and introduces a tri-\nplane representation for rendering the output. SSDNeRF [CGC\u221723]\nuses the diffusion model to generate tri-plane features and decode\nthem to NeRF for rendering. By leveraging the advantages of neu-\nral scene representations and generative models, these approaches\nhave demonstrated remarkable potential in generating realistic and\nintricate 3D models while maintaining view consistency.\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n7\nTable 1: Some examples of 3D generation methods. We first divide the methods according to the generative models and their corresponding\nrepresentations in generation space. The representations in the reconstruction space determine how the 3D objects are formatted and rendered.\nWe also list the main supervision and conditions of these methods. For the 2D supervision, a rendering technique is utilized to generate the\nimages.\nMethod\nGenerative Model\nGeneration Space\nReconstruction Space\nRendering\nSupervision\nCondition\nPointFlow [YHH\u221719a]\nNormalizing Flow\nLatent Code\nPoint Cloud\n-\n3D\nUncon\n3dAAE [ZZK\u221720]\nVAE\nLatent Code\nPoint Cloud\n-\n3D\nUncon\nSDM-NET [GYW\u221719a]\nVAE\nLatent Code\nMesh\n-\n3D\nUncon\nAutoSDF [MCST22a]\nAutoregressive\nVoxel\nSDF\n-\n3D\nUncon.\nPolyGen [NGEB20a]\nAutoregressive\nPolygon\nMesh\n-\n3D\nUncon./Label/Image\nPointGrow [SWL\u221720a]\nAutoregressive\nPoint\nPoint Cloud\n-\n3D\nUncon./Label/Image\nEG3D [CLC\u221722]\nGAN\nLatent Code\nTri-plane\nMixed Rendering\n2D\nUncon.\nGIRAFFE [NG21]\nGAN\nLatent Code\nNeRF\nMixed Rendering\n2D\nUncon.\nBlockGAN [NPRM\u221720]\nGAN\nLatent Code\nVoxel Grid\nNetwork Rendering\n2D\nUncon.\ngDNA [CJS\u221722]\nGAN\nLatent Code\nOccupancy Field\nSurface Rendering\n2D&3D\nUncon.\nSurfGen [LLZL21]\nGAN\nLatent Code\nSDF\n-\n3D\nUncon.\ntree-GAN [SPK19]\nGAN\nLatent Code\nPoint Cloud\n-\n3D\nUncon.\nHoloDiffusion [KVNM23]\nDiffusion\nVoxel\nNeRF\nVolume Rendering\n2D\nImage\nSSDNeRF [CGC\u221723]\nDiffusion\nTri-plane\nNeRF\nVolume Rendering\n2D\nUncon./Image\n3DShape2VecSet [ZTNW23]\nDiffusion\nLatent Set\nSDF\n-\n3D\nUncon./Text/Image\nPoint-E [NJD\u221722]\nDiffusion\nPoint\nPoint Cloud\n-\n3D\nText\n3DGen [GXN\u221723]\nDiffusion\nTri-plane\nMesh\n-\n3D\nText/Image\nDreamFusion [PJBM23]\nDiffusion\n-\nNeRF\nVolume Rendering\nSDS\nText\nMake-It-3D [TWZ\u221723]\nDiffusion\n-\nPoint Cloud\nNetwork Rendering\nSDS\nImage\nZero-1-to-3 [LWVH\u221723]\nDiffusion\nPixel\n-\n-\n2D\nImage\nMVDream [SWY\u221723]\nDiffusion\nPixel\n-\n-\n2D\nImage\nDMV3D [XTL\u221723]\nDiffusion\nPixel\nTri-plane\nVolume Rendering\n2D\nText/Image\nIn this section, we explore a large variety of 3D generation\nmethods which are organized into four categories based on their\nalgorithmic paradigms: Feedforward Generation (Sec. 4.1), gen-\nerating results in a forward pass; Optimization-Based Generation\n(Sec. 4.2), necessitating a test-time optimization for each genera-\ntion; Procedural Generation (Sec. 4.3), creating 3D models from\nsets of rules; and Generative Novel View Synthesis (Sec. 4.4), syn-\nthesizing multi-view images rather than an explicit 3D represen-\ntation for 3D generation. An evolutionary tree of 3D generation\nmethods is depicted in Fig. 4, which illustrates the primary branch\nof generation techniques, along with associated work and subse-\nquent developments. A comprehensive analysis will be discussed\nin the subsequent subsection.\n4.1. Feedforward Generation\nA primary technical approach for generation methods is feedfor-\nward generation, which can directly produce 3D representations\nusing generative models. In this section, we explore these methods\nbased on their generative models as shown in Fig. 5, which include\ngenerative adversarial networks (GANs), diffusion Models, autore-\ngressive models, variational autoencoders (VAEs) and normalizing\nflows.\n4.1.1. Generative Adversarial Networks\nGenerative Adversarial Networks (GANs) [GPAM\u221714] have\ndemonstrated remarkable outcomes in image synthesis tasks, con-\nsisting of a generator G(\u00b7) and a discriminator D(\u00b7). The gener-\nator network G produces synthetic data by accepting latent code\nas input, while the discriminator network D differentiates between\ngenerated data from G and real data. Throughout the training opti-\nmization process, the generator G and discriminator D are jointly\noptimized, guiding the generator to create synthetic data as realistic\nas real data.\nBuilding on the impressive results achieved by GANs in 2D\nimage synthesis, researchers have begun to explore the appli-\ncation of these models to 3D generation tasks. The core idea\nis to marry GANs with various 3D representations, such as\npoint clouds (l-GAN/r-GAN [ADMG18], tree-GAN [SPK19]),\nvoxel grids (3D-GAN [WZX\u221716], Z-GAN [KKR18]), meshes\n(MeshGAN [CBZ\u221719]), or SDF (SurfGen [LLZL21], SDF-\nStyleGAN [ZLWT22]). In this context, the 3D generation process\ncan be viewed as a series of adversarial steps, where the generator\nlearns to create realistic 3D data from input latent codes, and the\ndiscriminator differentiates between generated data and real data.\nBy iteratively optimizing the generator and discriminator networks,\nGANs learn to generate 3D data that closely resembles the realism\nof actual data.\nFor 3D object generation, prior GAN methodologies, such as l-\nGAN [ADMG18], 3D-GAN [WZX\u221716], and Multi-chart Gener-\nation [BHMK\u221718], directly utilize explicit 3D object representa-\ntion of real data to instruct generator networks. Their discrimina-\ntors employ 3D representation as supervision, directing the gener-\n8\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n3D Generation\nGenerative Novel View\nSynthesis\nAE/VAE-based\nDC-IGN\nTatarchenko et al. 2016\nZhou et al. 2016\nChen et al. 2019\nGAN-based\nSynSin\nPixelSynth\nIn\ufb01nite Nature\nIn\ufb01niteNature-Zero\nTVSN\nSun et al. 2018\nVQGAN-based\nGeometry-Free View Synthesis\nViewFormer\nDi\ufb00usion-based\n3DiM\nZero-1-2-3\nZero123++\nSyncDreamer\nGeNVS\nTewari et al. 2023\nOptimization-Based\n Generation\nCLIP-based\nDream Field\n CLIP-Mesh\nPureCLIPNeRF\nDream3D\n2D Di\ufb00usion-based\nDreamFusion\nImage-to-3D\nNeRDi\nMake-It-3D\nDreamCraft3D\nMagic123\nConsistent123\nHiFi-123\nRealFusion\nText-to-3D\nHiFA\nMagic3D\nProli\ufb01cDreamer \nSweetDreamer\nLatent-NeRF\nText-to-Texture\nTEXTure\nTexFusion\nFeedforward\n Generation\nGANs\n3D GANs\n3D-GAN\nSurfGen\nZ-GAN\nMeshGAN\n l-GAN/r-GAN\ntree-GAN\n3D-Aware GANs\nGRAF\npi-GAN\nGIRAFFE\nEG3D\nStyleNeRF\nStyleSDF\nHoloGAN\nBlockGAN\nPrGAN\nPlatonicGAN\nDIB-R\nConvMesh\nTextured3DGAN\nGET3D\nVAEs\nVoxel-Based VAEs\nSDM-NET\nTM-NET\nNeRF-VAE\nAutoregressive Models\nPointGrow\nPolyGen\nAutoSDF\nImAM\nShapeFormer\nNormalizing Flows\nPointFlow\nDiscrete PointFlow\nSoftPointFlow\nDi\ufb00usion Models\nMesh\nMeshdi\ufb00usion\nTetraDi\ufb00usion\nPoint Cloud\n[Cai et al. 2020]\nPVD, DPM \nLion\nPSF\nPoint\u00b7E\nSDF\nDi\ufb00usion-SDF\n3D-LDM\nRadiance Field\nDi\ufb00RF\nShapE\nTri-plane\nTriplane Di\ufb00usion\nProcedural\n Generation\nFractals\nFor fractal-like shapes \nE.g. natural terrain\nL-Systems\nFor branching structures \nE.g. plants\nNoise Functions\nFor random patterns \nE.g. textures\nCellular Automata\nFor grid cell structures \n E.g. mazes\nFigure 4: The evolutionary tree of 3D generation illustrates the primary branch of generation methods and their developments in recent\nyears. Specifically, we provide a comprehensive overview of the rapidly growing literature on generation methods, categorized by the type of\nalgorithmic paradigms, including feedforward generation, optimization-based generation, procedural generation, and generative novel view\nsynthesis.\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n9\n(a)\u00a0Generative\u00a0Adversarial\u00a0Networks\nG\n3D\u00a0Data\nD\nReal\u00a0/\u00a0Fake\n(e)\u00a0Normalizing\u00a0Flows\nFlow\nInverse\n\ud835\udc53\u123a\ud835\udc65\u123b\n\ud835\udc53\u0b3f\u0b35\u123a\ud835\udc67\u123b\n3D\u00a0Data\n3D\u00a0Data\n(d)\u00a0Variational\u00a0Autoencoders\nEncoder\nDecoder\n3D\u00a0Data\n3D\u00a0Data\n(b)\u00a0Diffusion\u00a0Models\nMultiple\u00a0denoising\u00a0steps\nNoise\n3D\u00a0Data\n(c)\u00a0Autoregressive\u00a0Models\n3D\u00a0Data\nTokens\n3D\u00a0Data\nPredict\u00a0the\u00a0next\u00a0token\nEncoder\nDecoder\nFigure 5: Exemplary feedforward 3D generation models. We showcase several representative pipelines of feedforward 3D generation models,\nincluding (a) generative adversarial networks, (b) diffusion models, (c) autoregressive models, (d) variational autoencoders and (e) normal-\nizing flows.\nator to produce synthetic data that closely resembles the realism of\nactual data. During training, specialized generators generate cor-\nresponding supervisory 3D representations, such as point clouds,\nvoxel grids, and meshes. Some studies, like SurfGen [LLZL21],\nhave progressed further to generate intermediate implicit represen-\ntations and then convert them to corresponding 3D representations\ninstead of directly generating explicit ones, achieving superior per-\nformance. In particular, the generator of l-GAN [ADMG18], 3D-\nGAN [WZX\u221716], and Multi-chart Generation [BHMK\u221718] gener-\nate the position of point cloud, voxel grid, and mesh directly, re-\nspectively, taking latent code as input. SurfGen [LLZL21] gener-\nates implicit representation and then extracts explicit 3D represen-\ntation.\nIn addition to GANs that directly generate various 3D represen-\ntations, researchers have suggested incorporating 2D supervision\nthrough differentiable rendering to guide 3D generation, which is\ncommonly referred to as 3D-Aware GAN. Given the abundance of\n2D images, GANs can better understand the implicit relationship\nbetween 2D and 3D data than relying solely on 3D supervision. In\nthis approach, the generator of GANs generates rendered 2D im-\nages from implicit or explicit 3D representation. Then the discrimi-\nnators distinguish between rendered 2D images and real 2D images\nto guide the training of the generator.\nSpecifically, HoloGAN [NPLT\u221719] first learns a 3D represen-\ntation of 3D features, which is then projected to 2D features\nby the camera pose. These 2D feature maps are then rendered\nto generate the final images. BlockGAN [NPRM\u221720] extends it\nto generate 3D features of both background and foreground ob-\njects and combine them into 3D features for the whole scene.\nIn addition, PrGAN [GMW17] and PlatonicGAN [HMR19a] em-\nploy an explicit voxel grid structure to represent 3D shapes and\nuse a render layer to create images. Other methods like DIB-\nR [CZ19], ConvMesh [PSH\u221720], Textured3DGAN [PKHL21] and\nGET3D [GSW\u221722] propose GAN frameworks for generating tri-\nangle meshes and textures using only 2D supervision.\nBuilding upon representations such as NeRFs, GRAF [SLNG20]\nproposes generative radiance fields utilizing adversarial frame-\nworks and achieves controllable image synthesis at high reso-\nlutions. pi-GAN [CMK\u221721a] introduces SIREN-based implicit\nGANs with FiLM conditioning to further improve image quality\nand view consistency. GIRAFFE [NG21] represents scenes as com-\npositional generative neural feature fields to model multi-object\nscenes. Furthermore, EG3D [CLC\u221722] first proposes a hybrid ex-\nplicit\u2013implicit tri-plane representation that is both efficient and ex-\npressive and has been widely adopted in many following works.\n4.1.2. Diffusion Models\nDiffusion models [HJA20,RBL\u221722a] are a class of generative mod-\nels that learn to generate data samples by simulating a diffusion\nprocess. The key idea behind diffusion models is to transform the\noriginal data distribution into a simpler distribution, such as Gaus-\nsian, through a series of noise-driven steps called the forward pro-\ncess. The model then learns to reverse this process, known as the\nbackward process, to generate new samples that resemble the orig-\ninal data distribution. The forward process can be thought of as\ngradually adding noise to the original data until it reaches the tar-\nget distribution. The backward process, on the other hand, involves\niteratively denoising the samples from the distribution to generate\nthe final output. By learning this denoising process, diffusion mod-\nels can effectively capture the underlying structure and patterns of\nthe data, allowing them to generate high-quality and diverse sam-\nples.\nBuilding on the impressive results achieved by diffusion models\nin generating 2D images, researchers have begun to explore the ap-\nplications of these models to 3D generation tasks. The core idea is\nto marry denoising diffusion models with various 3D representa-\ntions. In this context, the 3D generation process can be viewed as a\nseries of denoising steps, reversing the diffusion process from input\n3D data to Gaussian noise. The diffusion models learn to generate\n3D data from this noisy distribution through denoising.\n10\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\nSpecifically, Cai et al. [CYAE\u221720] build upon a denoising score-\nmatching framework to learn distributions for point cloud genera-\ntion. PVD [ZDW21] combines the benefits of both point-based and\nvoxel-based representations for 3D generation. The model learns\na diffusion process that transforms point clouds into voxel grids\nand vice versa, effectively capturing the underlying structure and\npatterns of the 3D data. Similarly, DPM [LH21] focuses on learn-\ning a denoising process for point cloud data by iterative denoising\nthe noisy point cloud samples. Following the advancements made\nby PVD [ZDW21] and DPM [LH21], LION [ZVW\u221722] builds\nupon the idea of denoising point clouds and introduces the con-\ncept of denoising in the latent space of point clouds, which is anal-\nogous to the shift in 2D image generation from denoising pixels\nto denoising latent space representations. To generate point clouds\nfrom text prompts, Point\u00b7E [NJD\u221722] initially employs the GLIDE\nmodel [NDR\u221721] to generate text-conditional synthetic views, fol-\nlowed by the production of a point cloud using a diffusion model\nconditioned on the generated image. By training the model on a\nlarge-scale 3D dataset, it achieves remarkable generalization capa-\nbilities.\nIn addition to point clouds, MeshDiffusion [LFB\u221723], Tetrahe-\ndral Diffusion Models [KPWS22], and SLIDE [LWA\u221723] explore\nthe application of diffusion models to mesh generation. MeshDif-\nfusion [LFB\u221723] adopts the DMTet representation [SGY\u221721] for\nmeshes and optimizes the model by treating the optimization of\nsigned distance functions as a denoising process. Tetrahedral Dif-\nfusion Models [KPWS22] extends diffusion models to tetrahedral\nmeshes, learning displacement vectors and signed distance values\non the tetrahedral grid through denoising. SLIDE [LWA\u221723] ex-\nplores diffusion models on sparse latent points for mesh generation.\nApart from applying diffusion operations on explicit 3D\nrepresentations, some works focus on performing the diffu-\nsion process on implicit representations. SSDNeRF [CGC\u221723],\nDiffRF [MSP\u221723] and Shap\u00b7E [JN23] operate on 3D radiance\nfields, while SDF-Diffusion [SKJ23], LAS-Diffusion [ZPW\u221723],\nNeural\nWavelet-domain\nDiffusion\n[HLHF22],\nOne-2-3-\n45++ [LXJ\u221723], SDFusion [CLT\u221723] and 3D-LDM [NKR\u221722]\nfocus on signed distance fields representations. Specifically,\nDiffusion-SDF [LDZL23] utilizes a voxel-shaped SDF repre-\nsentation to generate high-quality and continuous 3D shapes.\n3D-LDM [NKR\u221722] creates neural implicit representations of\nSDFs by initially using a diffusion model to generate the latent\nspace of an auto-decoder. Subsequently, the latent space is decoded\ninto SDFs to acquire 3D shapes. Moreover, Rodin [WZZ\u221723]\nand Shue et al. [SCP\u221723] adopt tri-plane as the representation\nand optimize the tri-plane features using diffusion methods.\nShue et al. [SCP\u221723] generates 3D shapes using occupancy\nnetworks, while Rodin [WZZ\u221723] obtains 3D shapes through\nvolumetric rendering.\nThese approaches showcase the versatility of diffusion models\nin managing various 3D representations, including both explicit\nand implicit forms. By tailoring the denoising process to different\nrepresentation types, diffusion models can effectively capture the\nunderlying structure and patterns of 3D data, leading to improved\ngeneration quality and diversity. As research in this area continues\nto advance, it is expected that diffusion models will play a crucial\nrole in pushing the boundaries of 3D shape generation across a wide\nrange of applications.\n4.1.3. Autoregressive Models\nA 3D object can be represented as a joint probability of the occur-\nrences of multiple 3D elements:\np(x0,x1,...,xn),\n(3)\nwhere xi is the i-th element which can be the coordinate of a point or\na voxel. A joint probability with a large number of random variables\nis usually hard to learn and estimate. However, one can factorize it\ninto a product of conditional probabilities:\np(x0,x1,...,xn) = p(x0)\nn\n\u220f\ni=1\np(xi|x<i),\n(4)\nwhich enables learning conditional probabilities and estimating the\njoint probability via sampling. Autoregressive models for data gen-\neration are a type of models that specify the current output depend-\ning on their previous outputs. Assuming that the elements x0, x1,\n..., xn form an ordered sequence, a model can be trained by provid-\ning it with previous inputs x0, ... xi\u22121 and supervising it to fit the\nprobability of the outcome xi:\np(xi|x<i) = f(x0,...,xi\u22121),\n(5)\nthe conditional probabilities are learned by the model function f.\nThis training process is often called teacher forcing. The model can\nbe then used to autoregressively generate the elements step-by-step:\nxi = argmax p(x|x<i).\n(6)\nState-of-the-art generative models such as GPTs [RWC\u221719,\nBMR\u221720] are autoregressive generators with Transformer net-\nworks as the model function. They achieve great success in gen-\nerating natural languages and images. In 3D generation, several\nstudies have been conducted based on autoregressive models. In\nthis section, we discuss some notable examples of employing au-\ntoregressive models for 3D generation.\nPointGrow [SWL\u221720b] generates point clouds using an autore-\ngressive network with self-attention context awareness operations\nin a point-by-point manner. Given its previously generated points,\nPointGrow reforms the points by axes and passes them into three\nbranches. Each branch takes the inputs to predict a coordinate value\nof one axis. The model can also condition an embedding vector to\ngenerate point clouds, which can be a class category or an image.\nInspired by the network from PointGrow, PolyGen [NGEB20b]\ngenerates 3D meshes with two transformer-based networks, one for\nvertices and one for faces. The vertex transformer autoregressively\ngenerates the next vertex coordinate based on previous vertices.\nThe face transformer takes all the output vertices as context to gen-\nerate faces. PolyGen can condition on a context of object classes or\nimages, which are cross-attended by the transformer networks.\nRecently, AutoSDF [MCST22b] generates 3D shapes repre-\nsented by volumetric truncated-signed distance function (T-SDF).\nAutoSDF learns a quantized codebook regarding local regions of\nT-SDFs using VQ-VAE. The shapes are then presented by the\ncodebook tokens and learned by a transformer-based network in\na non-sequential autoregressive manner. In detail, given previous\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n11\ntokens at arbitrary locations and a query location, the network pre-\ndicts the token that is queried. AutoSDF is capable of completing\nshapes and generating shapes based on images or text. Concurrently\nwith AutoSDF, ShapeFormer [YLM\u221722] generates surfaces of 3D\nshapes based on incomplete and noisy point clouds. A compact\n3D representation called vector quantized deep implicit function\n(VQDIF) is used to represent shapes using a feature sequence of\ndiscrete variables. ShapeFormer first encodes an input point cloud\ninto a partial feature sequence. It then uses a transformer-based net-\nwork to autoregressively sample out the complete sequence. Fi-\nnally, it decodes the sequence to a deep implicit function from\nwhich the complete object surface can be extracted. Instead of\nlearning in 3D volumetric space, Luo et al.proposes an improved\nauto-regressive model (ImAM) to learn discrete representation in\na one-dimensional space to enhance the efficient learning of 3D\nshape generation. The method first encodes 3D shapes of volumet-\nric grids into three axis-aligned planes. It uses a coupling network\nto further project the planes into a latent vector, where vector quan-\ntization is performed for discrete tokens. ImAM adopts a vanilla\ntransformer to autoregressively learn the tokens with tractable or-\nders. The generated tokens are decoded to occupancy values via a\nnetwork by sampling spatial locations. ImAM can switch from un-\nconditional generation to conditional generation by concatenating\nvarious conditions, such as point clouds, categories, and images.\n4.1.4. Variational Autoencoders\nVariational autoencoders (VAEs) [KW13] are probabilistic genera-\ntive models that consist of two neural network components: the en-\ncoder and decoder. The encoder maps the input data point to a latent\nspace that corresponds to the parameters of a variational distribu-\ntion. In this way, the encoder can produce multiple different sam-\nples that all come from the same distribution. The decoder maps\nfrom the latent space to the input space, to produce or generate data\npoints. Both networks are typically trained together with the usage\nof the reparameterization trick, although the variance of the noise\nmodel can be learned separately. VAEs have also been explored in\n3D generation [KYLH21,GWY\u221721,GYW\u221719b,BLW16,KSZ\u221721].\nBrock et al. trains variational autoencoders directly for vox-\nels using 3D ConvNet, while SDM-Net [GYW\u221719b] focuses on\nthe generation of structured meshes composed of deformable\nparts. The method uses one VAE network to model parts and\nanother to model the whole object. The follow-up work TM-\nNet [GWY\u221721] could generate texture maps of meshes in a part-\naware manner. Other representations like point clouds [KYLH21]\nand NeRFs [KSZ\u221721] are also explored in variational autoen-\ncoders. Owing to the reconstruction-focused objective of VAEs,\ntheir training is considerably more stable than that of GANs. How-\never, VAEs tend to produce more blurred results compared to\nGANs.\n4.1.5. Normalizing Flows\nNormalizing flow models consist of a series of invertible transfor-\nmations that map a simple distribution, such as Gaussian, to a target\ndistribution, which represents the data to generation. These trans-\nformations are carefully designed to be differentiable and invert-\nible, allowing one to compute the likelihood of the data under the\nan orangutan making a clay bowl on a throwing wheel*\na raccoon astronaut holding his helmet\u2020\na blue jay standing on a large basket of rainbow macarons*\na corgi taking a sel\ufb01e*\na table with dim sum on it\u2020\na lion reading the newspaper*\nMichelangelo style statue of dog reading news on a cellphone\na tiger dressed as a doctor*\na steam engine train, high resolution*\na frog wearing a sweater*\na humanoid robot playing the cello*\nSydney opera house, aerial view\u2020\nFigure 6: Results of text-guided 3D generation by DreamFu-\nsion [PJBM23] using SDS loss. \u2217 denotes a DSLR photo, \u2020 denotes\na zoomed out DSLR photo.\nmodel and optimize the model parameters using gradient-based op-\ntimization techniques.\nIn 3D generation, PointFlow [YHH\u221719a] learns a distribution\nof shapes and a distribution of points using continuous normalizing\nflows. This approach allows for the sampling of shapes, followed by\nthe sampling of an arbitrary number of points from a given shape.\nDiscrete PointFlow (DPF) network [KBV20] improves PointFlow\nby replacing continuous normalizing flows with discrete normal-\nizing flows, which reduces the training and sampling time. Soft-\nFlow [KLK\u221720] is a framework for training normalizing flows on\nthe manifold. It estimates a conditional distribution of the perturbed\ninput data instead of learning the data distribution directly. Soft-\nFlow alleviates the difficulty of forming thin structures for flow-\nbased models.\n4.2. Optimization-based Generation\nOptimization-based generation is employed to generate 3D models\nusing runtime optimization. These methods usually leverage pre-\ntrained multimodal networks to optimize 3D models based on user-\nspecified prompts. The key lies in achieving alignment between\nthe given prompts and the generated content while maintaining\nhigh fidelity and diversity. In this section, we primarily examine\noptimization-based generation methods that use texts and images,\nbased on the types of prompts provided by users.\n4.2.1. Text-to-3D\nLanguage serves as the primary means of human communication\nand describing scenes, and researchers are dedicated to explor-\ning the potential of text-based generation methods. These meth-\nods typically align the text with the images obtained through the\ndifferentiable rendering techniques, thereby guiding the genera-\ntion of 3D content based on the text prompts. Given a fixed sur-\nface, TANGO [LZJ\u221722] uses CLIP [RKH\u221721a] to supervise dif-\nferentiable physical-based rendering (PBR) images and obtain tex-\nture maps that align with the specified text prompt. Inspired by\nthe success of NeRF [MST\u221720] and diffusion models in model-\ning 3D static scenes and text-to-image tasks respectively, Dream-\nFusion [PJBM23] (as shown in Fig. 6) combines the volumetric\n12\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\nTable 2: Quantitative comparison of image-to-3D methods on sur-\nface reconstruction. We summarize the Chamfer distance and vol-\nume IoU as the metrics to evaluate the quality of surface recon-\nstruction.\nMethod\nChamfer Distance \u2193\nVolume IoU \u2191\nRealFusion [MKLRV23]\n0.0819\n0.2741\nMagic123 [QMH\u221723]\n0.0516\n0.4528\nMake-it-3D [TWZ\u221723]\n0.0732\n0.2937\nOne-2-3-45 [LXJ\u221723]\n0.0629\n0.4086\nPoint-E [NJD\u221722]\n0.0426\n0.2875\nShap-E [JN23]\n0.0436\n0.3584\nZero-1-to-3 [LWVH\u221723]\n0.0339\n0.5035\nSyncDreamer [LLZ\u221723]\n0.0261\n0.5421\nrepresentation used in NeRF with the proposed Score Distillation\nSampling (SDS) loss to achieve high-fidelity 3D content genera-\ntion. SDS loss converts rendering error minimization into proba-\nbility density distillation and enables 2D diffusion priors to opti-\nmize 3D representations (e.g. , volumetric representation and trian-\ngle mesh) via image parameterization (e.g. , differentiable render-\ning). As a concurrent work concurrent with SDS, Score Jacobian\nChaining (SJC) [WDL\u221723] interprets predictions from pre-trained\ndiffusion models as a score function of the data log-likelihood, sim-\nilarly enabling 2D diffusion priors to optimize 3D representations\nvia score matching. Based on DreamFusion, Magic3D [LGT\u221723]\nintroduces a coarse-to-fine manner and extracts the underlying ge-\nometry of the volume as a mesh. It then combines differentiable\nneural rendering and SDS to refine the extracted mesh. Magic3D is\ncapable of exporting high-quality textured meshes and seamlessly\nembedding them into the traditional graphics pipeline. Also as a\ntwo-stage method, Fantasia3D further combines DMTet [SGY\u221721]\nand SDS in the first geometry modeling stage to explicitly opti-\nmize surface. In the second stage, it introduces the PBR mate-\nrial model and disentangle texture and environment illumination.\nProlificDreamer [WLW\u221723] presents variational score distillation\n(VSD) to boost text-to-3D generation. VSD adopts particles to\nmodel the distribution of 3D scenes and derive a gradient-based\noptimization scheme from the Wasserstein gradient flow, narrowing\nthe gap between the rendering results distribution of the modeling\ndistribution and pre-trained diffusion distribution. Benefiting from\nthe optimization of scene distribution rather than a single scene,\nVSD overcomes the over-saturated and over-smoothed results pro-\nduced by SDS and improves diversities. MVDream [SWY\u221723] fur-\nther fine-tunes a multi-view diffusion model and introduces multi-\nview consistent 3D priors, overcoming multi-face and content-drift\nproblems. Text-to-3D has garnered significant attention recently, in\naddition to these, many other methods [ZZ23,LCCT23,MRP\u221723a]\nhave been proposed in this field.\n4.2.2. Image-to-3D\nAs the primary way to describe the visual effects of scenes, im-\nages can more intuitively describe the details and appearance of\nscenes at a finer-grained than language. Recent works thus are\nmotivated to explore the image-to-3D techniques, which recon-\nstruct remarkable and high-fidelity 3D models from specified im-\nTable 3: Quantitative comparison of image-to-3D methods on novel\nview synthesis. We report the CLIP-Similarity, PSNR, and LPIPS\nas the metrics to evaluate the quality of view synthesis.\nMethod\nCLIP-Similarity \u2191\nPSNR \u2191\nLPIPS \u2193\nRealFusion [MKLRV23]\n0.735\n20.216\n0.197\nMagic123 [QMH\u221723]\n0.747\n25.637\n0.062\nMake-it-3D [TWZ\u221723]\n0.839\n20.010\n0.119\nOne-2-3-45 [LXJ\u221723]\n0.788\n23.159\n0.096\nZero-1-to-3 [LWVH\u221723]\n0.759\n25.386\n0.068\nSyncDreamer [LLZ\u221723]\n0.837\n25.896\n0.059\nages. These methods strive to maintain the appearance of the spec-\nified images and optimized 3D contents while introducing reason-\nable geometric priors. Similar to the text-to-3D methods, several\nimage-to-3D methods leverage the volumetric representation used\nin NeRF to represent the target 3D scenes, which natively intro-\nduces multi-view consistency. NeuralLift-360 [XJW\u221723] uses es-\ntimated monocular depth and CLIP-guided diffusion prior to reg-\nularizing the geometry and appearance optimization respectively,\nachieving lift of a single image to a 3D scene represented by a\nNeRF. RealFusion [MKLRV23] and NeRDi [DJQ\u221723] leverage\ntextual inversion [GAA\u221722] to extract text embeddings to condi-\ntion a pre-trained image diffusion model [RBL\u221722b], and combine\nuse the score distillation loss to optimize the volumetric represen-\ntation. Based on Magic3D [LGT\u221723] that employs a coarse-to-fine\nframework as mentioned above, Magic123 [QMH\u221723] additionally\nintroduces 3D priors from a pre-trained viewpoint-conditioned dif-\nfusion model Zero-1-to-3 [LWVH\u221723] in two optimization stage,\nyielding textured meshes that match the specified images. As an-\nother two-stage image-to-3D method, Make-it-3D [TWZ\u221723] en-\nhances texture and geometric structure in the fine stage, pro-\nducing high-quality textured point clouds as final results. Sub-\nsequent works [SZS\u221723, YYC\u221723] have been consistently pro-\nposed to enhance the previous results. Recently, 3D Gaussian\nSplatting (3DGS) [KKLD23] has emerged as a promising model-\ning as well as a real-time rendering technique. Based on 3DGS,\nDreamGaussian [TRZ\u221723] presents an efficient two-stage frame-\nwork for both text-driven and image-driven 3D generation. In the\nfirst stage, DreamGaussian leverages SDS loss (i.e. 2D diffusion\npriors [LWVH\u221723] and CLIP-guided diffusion priors [PJBM23]) to\ngenerate target objects represented by 3D Gaussians. Then Dream-\nGaussian extracts textured mesh from the optimized 3D Gaussians\nby querying the local density and refines textures in the UV space.\nFor a better understanding of readers to various image-to-3D meth-\nods, we evaluate the performance of some open-source state-of-the-\nart methods. Tab. 2 shows the quantitative comparison of image-to-\n3D methods on surface reconstruction. We summarize the Chamfer\ndistance and volume IoU as the metrics to evaluate the quality of\nsurface reconstruction. Tab. 3 demonstrates the quantitative com-\nparison of image-to-3D methods on novel view synthesis. We re-\nport the CLIP-Similarity, PSNR, and LPIPS as the metrics to eval-\nuate the quality of view synthesis.\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n13\n4.3. Procedural Generation\nProcedural generation is a term for techniques that create 3D mod-\nels and textures from sets of rules. These techniques often rely on\npredefined rules, parameters, and mathematical functions to gener-\nate diverse and complex content, such as textures, terrains, levels,\ncharacters, and objects. One of the key advantages of procedural\ngeneration is their ability to efficiently create various shapes from a\nrelatively small set of rules. In this section, we mainly survey four\nmost used techniques: fractal geometry, L-Systems, noise functions\nand cellular automata.\nA fractal [Man67,MM82] is a geometric shape that exhibits de-\ntailed structure at arbitrarily small scales. A characteristic feature of\nmany fractals is their similarity across different scales. This prop-\nerty of exhibiting recurring patterns at progressively smaller scales\nis referred to as self-similarity. A common application of fractal\ngeometry is the creation of landscapes or surfaces. These are gen-\nerated using a stochastic algorithm designed to produce fractal be-\nhavior that mimics the appearance of natural terrain. The resulting\nsurface is not deterministic, but rather a random surface that ex-\nhibits fractal behavior.\nAn L-system [Lin68], or Lindenmayer system, is a type of for-\nmal grammar and parallel rewriting system. It comprises an alpha-\nbet of symbols that can be utilized to construct strings, a set of\nproduction rules that transform each symbol into a more complex\nstring of symbols, a starting string for construction, and a mecha-\nnism for converting the produced strings into geometric structures.\nL-systems are used to create complex and realistic 3D models of\nnatural objects like trees and plants. The string generated by the L-\nSystem can be interpreted as instructions for a \u201cturtle\u201d to move in\n3D space. For example, certain characters might instruct the turtle\nto move forward, turn left or right, or push and pop positions and\norientations onto a stack.\nNoise functions, such as Perlin noise [Per85] and Simplex noise\n[Per02], are used to generate coherent random patterns that can be\napplied to create realistic textures and shapes in 3D objects. These\nfunctions can be combined and layered to create more complex pat-\nterns and are particularly useful in terrain generation, where they\ncan be used to generate realistic landscapes with varying elevations,\nslopes, and features.\nCellular automata [VN\u221751,Neu66,Wol83] are a class of discrete\ncomputational models that consist of a grid of cells, each of which\ncan be in one of a finite number of states. The state of each cell is\ndetermined by a set of rules based on the states of its neighboring\ncells. Cellular automata have been used in procedural generation\nto create various 3D objects and patterns, such as cave systems,\nmazes, and other structures with emergent properties.\n4.4. Generative Novel View Synthesis\nRecently, generative techniques have been utilized to tackle the\nchallenge of novel view synthesis, particularly in predicting new\nviews from a single input image. Compared to the conventional\n3D generation methods, it does not explicitly utilize the 3D rep-\nresentation to enforce 3D consistency, instead, it usually employs\na 3D-aware method by conditioning 3D information. In the field\nFigure 7: Zero-1-to-3 proposes a viewpoint-conditioned image dif-\nfusion model to generate the novel view of the input image. By\ntraining on a large-scale dataset, it achieves a strong generalization\nability to in-the-wild images.\nof novel view synthesis, a widely studied technical route will be\nregression-based methods [YYTK21,WWG\u221721,CXZ\u221721]. Differ-\nent from them, generative novel view synthesis focuses more on\ngenerating new content rather than regressing the scenes from a few\ninput images, which typically involves long-range view extrapola-\ntion.\nWith the development of image synthesis methods, signifi-\ncant progress has been made in generative novel view synthe-\nsis. Recently, 2D diffusion models have transformed image syn-\nthesis and therefore are also utilized in generative novel view\nsynthesis [WCMB\u221722, TLK\u221723, LWVH\u221723, CNC\u221723, TYC\u221723,\nYGMG23]. Among these methods, 3DiM [WCMB\u221722] first in-\ntroduces a geometry-free image-to-image diffusion model for\nnovel view synthesis, taking the camera pose as the condition.\nTseng et al. [TLK\u221723] designs epipolar attention layers to in-\nject camera parameters into the pose-guided diffusion model for\nconsistent view synthesis from a single input image. Zero-1-to-\n3 [LWVH\u221723] (as shown in Fig. 7) demonstrates the learning of\nthe camera viewpoint in large-scale diffusion models for zero-\nshot novel view synthesis.\n[CNC\u221723, TYC\u221723, YGMG23] con-\ndition 2D diffusion models on pixel-aligned features extracted\nfrom input views to extend them to be 3D-aware. However, gen-\nerating multiview-consistent images remains a challenging prob-\nlem. To ensure consistent generation, [LHG\u221723,LLZ\u221723,SCZ\u221723,\nLGL\u221723] propose a multi-view diffusion model that could synthe-\nsize multi-view images simultaneously to consider the informa-\ntion between different views, which achieve more consistent re-\nsults compared to the single view synthesis model like Zero-1-to-\n3 [LWVH\u221723].\nPrior to that, the transformer which is a sequence-to-sequence\nmodel originally proposed in natural language processing, uses a\nmulti-head attention mechanism to gather information from dif-\nferent positions and brings lots of attention in the vision com-\nmunity. Many tasks achieve state-of-the-art performance using the\nattention mechanism from the transformer including generative\nnovel view synthesis [REO21, SMP\u221722, KDSB22]. Specifically,\nGeometry-free View Synthesis [REO21] learns the discrete repre-\n14\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\nTable 4: Selected datasets commonly used for 3D generation.\nDataset\nType\nYear\nSamples\nCategory\nShapeNet [CFG\u221715]\n3D data\n2015\n51K\nobjects\nThingi10K [ZJ16]\n3D data\n2016\n10K\nobjects\n3D-Future [FJG\u221721]\n3D data\n2020\n10K\nfurniture\nGSO [DFK\u221722]\n3D Data\n2022\n1K\nhousehold items\nObjaverse [DSS\u221723]\n3D data\n2022\n800K\nobjects\nOmniObject3D [WZF\u221723]\n3D data\n2023\n6K\nobjects\nObjaverse-XL [DLW\u221723]\n3D Data\n2023\n10.2M\nobjects\nScanNet [DCS\u221717]\nmulti-view images\n2017\n1.5K (2.5M images)\nindoor scenes\nCO3D [RSH\u221721]\nmulti-view images\n2021\n19K (1.5M images)\nobjects\nMVImgNet [YXZ\u221723]\nmulti-view images\n2023\n219K (6.5M images)\nobjects\nDeepFashion [LLQ\u221716]\nsingle-view images\n2016\n800K\nclothes\nFFHQ [KLA19]\nsingle-view images\n2018\n70K\nhuman faces\nAFHQ [CUYH20]\nsingle-view images\n2019\n15K\nanimal faces\nSHHQ [FLJ\u221722]\nsingle-view images\n2022\n40K\nhuman bodies\nsentation vis VQGAN to obtain an abstract latent space for training\ntransformers. While ViewFormer [KDSB22] also uses a two-stage\ntraining consisting of a Vector Quantized Variational Autoencoder\n(VQ-VAE) codebook and a transformer model. And [SMP\u221722] em-\nploys an encoder-decoder model based on transformers to learn an\nimplicit representation.\nOn the other hand, generative adversarial networks could pro-\nduce high-quality results in image synthesis and consequently are\napplied to novel view synthesis [WGSJ20,KLY\u221721,RFJ21,LTJ\u221721,\nLWSK22]. Some methods [WGSJ20, KLY\u221721, RFJ21] maintain a\n3D point cloud as the representation, which could be projected\nonto novel views followed by a GAN to hallucinate the miss-\ning regions and synthesize the output image. While [LTJ\u221721]\nand [LWSK22] focus on long-range view generation from a sin-\ngle view with adversarial training. At an earlier stage of deep\nlearning methods when the auto-encoders and variational autoen-\ncoders begin to be explored, it is also used to synthesize the novel\nviews [KWKT15,ZTS\u221716,TDB16,CSH19].\nIn summary, generative novel view synthesis can be regarded\nas a subset of image synthesis techniques and continues to evolve\nalongside advancements in image synthesis methods. Besides the\ngenerative models typically included, determining how to integrate\ninformation from the input view as a condition for synthesizing the\nnovel view is the primary issue these methods are concerned with.\n5. Datasets for 3D Generation\nWith the rapid development of technology, the ways of data acqui-\nsition and storage become more feasible and affordable, resulting in\nan exponential increase in the amount of available data. As data ac-\ncumulates, the paradigm for problem-solving gradually shifts from\ndata-driven to model-driven approaches, which in turn contributes\nto the growth of \"Big Data\" and \"AIGC\". Nowadays, data plays a\ncrucial role in ensuring the success of algorithms. A well-curated\ndataset can significantly enhance a model\u2019s robustness and perfor-\nmance. On the contrary, noisy and flawed data may cause model\nbias that requires considerable effort in algorithm design to rectify.\nIn this section, we will go over the common data used for 3D gen-\neration. Depending on the methods employed, it usually includes\n3D data (Section 5.1), multi-view image data (Section 5.2), and\nsingle-view image data (Section 5.3), which are also summarized\nin Tab. 4.\n5.1. Learning from 3D Data\n3D data could be collected by RGB-D sensors and other technology\nfor scanning and reconstruction. Apart from 3D generation, 3D data\nis also widely used for other tasks like helping improve classical\n2D vision task performance by data synthesis, environment simula-\ntion for training embodied AI agents, 3D object understanding, etc.\nOne popular and frequently used 3D model database in the early\nstage is The Princeton Shape Benchmark [SMKF04]. It contains\nabout 1800 polygonal models collected from the World Wide Web.\nWhile [KXD12] constructs a special rig that contains a 3D digitizer,\na turntable, and a pair of cameras mounted on a sled that can move\nalong a bent rail to capture the kit object models database. To eval-\nuate the algorithms to detect and estimate the objects in the image\ngiven 3D models, [LPT13] introduces a dataset of 3D IKEA models\nobtained from Google Warehouse. Some 3D model databases are\npresented for tasks like robotic manipulation [CWS\u221715, MCL20],\n3D shape retrieval [LLL\u221714], 3D shape modeling from a single im-\nage [SWZ\u221718]. BigBIRD [SSN\u221714] presents a large-scale dataset\nof 3D object instances that also includes multi-view images and\ndepths, camera pose information, and segmented objects for each\nimage.\nHowever, those datasets are very small and only contain hun-\ndreds or thousands of objects. Collecting, organizing, and label-\ning larger datasets in computer vision and graphics communities\nis needed for data-driven methods of 3D content. To address this,\nShapeNet [CFG\u221715] is introduced to build a large-scale repository\nof 3D CAD models of objects. The core of ShapeNet covers 55\ncommon object categories with about 51,300 models that are manu-\nally verified category and alignment annotations. Thingi10K [ZJ16]\ncollects 10,000 3D printing models from an online repository Thin-\ngiverse. While PhotoShape [PRFS18] produces 11,000 photorealis-\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n15\ntic, relightable 3D shapes based on online data. Other datasets such\nas 3D-Future [FJG\u221721], ABO [CGD\u221722], GSO [DFK\u221722] and\nOmniObject3D [WZF\u221723] try to improve the texture quality but\nonly contain thousands of models. Recently, Objaverse [DSS\u221723]\npresents a large-scale corpus of 3D objects that contains over 800K\n3D assets for research in the field of AI and makes a step toward\na large-scale 3D dataset. Objaverse-XL [DLW\u221723] further extends\nObjaverse to a larger 3D dataset of 10.2M unique objects from a\ndiverse set of sources. These large-scale 3D datasets have the po-\ntential to facilitate large-scale training and boost the performance\nof 3D generation.\n5.2. Learning from Multi-view Images\n3D objects have been traditionally created through manual 3D\nmodeling, object scanning, conversion of CAD models, or combi-\nnations of these techniques [DFK\u221722]. These techniques may only\nproduce synthetic data or real-world data of specific objects with\nlimited reconstruction accuracy. Therefore, some datasets directly\nprovide multi-view images in the wild which are also widely used\nin many 3D generation methods. ScanNet [DCS\u221717] introduces an\nRGB-D video dataset containing 2.5M views in 1513 scenes and\nObjectron [AZA\u221721] contains object-centric short videos and in-\ncludes 4 million images in 14,819 annotated videos, of which only\na limited number cover the full 360 degrees. CO3D [RSH\u221721] ex-\ntends the dataset from [HRL\u221721] and increases the size to nearly\n19,000 videos capturing objects from 50 MS-COCO categories,\nwhich has been widely used in the training and evaluations of\nnovel view synthesis and 3D generation or reconstruction methods.\nRecently, MVImgNet [YXZ\u221723] presents a large-scale dataset of\nmulti-view images that collects 6.5 million frames from 219,188\nvideos by shooting videos of real-world objects in human daily\nlife. Other lines of work provide the multi-view dataset in small-\nscale RGB-D videos [LBRF11, SHG\u221722, CX\u221723] compared with\nthese works, large-scale synthetic videos [TME\u221722], or egocentric\nvideos [ZXA\u221723]. A large-scale dataset is still a remarkable trend\nfor deep learning methods, especially for generation tasks.\n5.3. Learning from Single-view Images\n3D generation methods usually rely on multi-view images or 3D\nground truth to supervise the reconstruction and generation of 3D\nrepresentation. Synthesizing high-quality multi-view images or 3D\nshapes using only collections of single-view images is a challeng-\ning problem. Benefiting from the unsupervised training of gener-\native adversarial networks, 3D-aware GANs are introduced that\ncould learn 3D representations in an unsupervised way from natural\nimages. Therefore, several single-view image datasets are proposed\nand commonly used for these 3D generation methods. Although\nmany large-scale image datasets have been presented for 2D gen-\neration, it is hard to directly use them for 3D generation due to the\nhigh uncertainty of this problem. Normally, these image datasets\nonly contain a specific category or domain. FFHQ [KLA19], a real-\nworld human face dataset consisting of 70,000 high-quality im-\nages at 10242 resolution, and AFHQ [CUYH20], an animal face\ndataset consisting of 15,000 high-quality images at 5122 resolu-\ntion, are introduced for 2D image synthesis and used a lot for\n3D generation based on 3D-aware GANs. In the domain of the\nTable 5: Recent 3D human generation techniques and their corre-\nsponding input-output formats.\nMethods\nInput Condition\nOutput Texture\nICON [XYTB22]\nSingle-Image\n%\nECON [XYC\u221723]\nSingle-Image\n%\ngDNA [CJS\u221722]\nLatent\n%\nChupa [KKL\u221723]\nText/Latent\n%\nELICIT [HYL\u221723]\nSingle-Image\n!\nTeCH [HYX\u221723]\nSingle-Image\n!\nGet3DHuman [XKJ\u221723]\nLatent\n!\nEVA3D [HCL\u221722]\nLatent\n!\nAvatarCraft [JWZ\u221723]\nText\n!\nDreamHuman [KAZ\u221723]\nText\n!\nTADA [LYX\u221724]\nText\n!\nhuman body, SHHQ [FLJ\u221722] and DeepFashion [LLQ\u221716] have\nbeen adopted for 3D human generation. In terms of objects, many\nmethods [LSMG20, GMW17, HMR19a, ZZZ\u221718, WZX\u221716] ren-\nder synthetic single-view datasets using several major object cat-\negories of ShapeNet. While GRAF [SLNG20] renders 150k Chairs\nfrom Photoshapes [PRFS18]. Moreover, CelebA [LLWT15] and\nCats [ZST08] datasets are also commonly used to train the mod-\nels like HoloGAN [NPLT\u221719] and pi-GAN [CMK\u221721a]. Since the\nsingle-view images are easy to obtain, these methods could collect\ntheir own dataset for the tasks.\n6. Applications\nIn this section, we introduce various 3D generation tasks (Sec. 6.1-\n6.3) and closely related 3D editing tasks (Sec. 6.4). The generation\ntasks are divided into three categories, including 3D human gener-\nation (Sec. 6.1), 3D face generation (Sec. 6.2), and generic object\nand scene generation (Sec. 6.3).\n6.1. 3D Human Generation\nWith the emergence of the metaverse and the advancements in vir-\ntual 3D social interaction, the field of 3D human digitization and\ngeneration has gained significant attention in recent years. Differ-\nent from general 3D generation methods that focus on category-free\nrigid objects with sample geometric structures [PJBM23,LXZ\u221723],\nmost 3D human generation methods aim to tackle the complex-\nities of articulated pose changes and intricate geometric details\nof clothing. Tab. 5 presents a compilation of notable 3D human\nbody generation methods in recent years, organized according to\nthe input conditions and the output format of the generated 3D hu-\nman bodies. Some results of these methods are shown in Fig. 8.\nSpecifically, in terms of the input condition, current 3D human\nbody generation methods can be categorized based on the driv-\ning factors including latent features randomly sampled from a pre-\ndefined latent space [MYR\u221720, CJS\u221722, HCL\u221722], a single refer-\nence image [APMTM19, CPA\u221721, XYC\u221723, HYX\u221723, ZLZ\u221723],\nor text prompts [KKL\u221723, JWZ\u221723, KAZ\u221723, LYX\u221724]. Accord-\ning to the form of the final output, these methods can be classi-\nfied into two categories: textureless shape generation [APMTM19,\n16\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\nFigure 8: Examples of 3D human generation methods. 3D gen-\neration results source from ECON [XYC\u221723], gDNA [CJS\u221722],\nChupa [KKL\u221723], TeCH [HYX\u221723], Get3DHuman [XKJ\u221723], and\nDreamHuman [KAZ\u221723].\nXYTB22, XYC\u221723, CJS\u221722, MYR\u221720, CPA\u221721, KKL\u221723] and\ntextured body generation [AZS22, LYX\u221724, HYL\u221723, KAZ\u221723,\nXKJ\u221723, HYX\u221723, ZLZ\u221723]. While the latter focuses on generat-\ning fully textured 3D clothed humans, the former aims to obtain\ntextureless body geometry with realistic details.\nIn terms of textureless shape generation, early works [CPB\u221720,\nOBB20,LXC\u221721] attempt to predict SMPL parameters from the in-\nput image and infer a skinned SMPL mesh as the generated 3D rep-\nresentation of the target human. Nevertheless, such skinned body\nrepresentation fails to represent the geometry of clothes. To over-\ncome this issue, [APMTM19, XYTB22, XYC\u221723] leverage a pre-\ntrained neural network to infer the normal information and com-\nbine the skinned SMPL mesh to deduce a clothed full-body geom-\netry with details. In contrast to such methods, which require ref-\nerence images as input, CAPE [MYR\u221720] proposes a generative\n3D mesh model conditioned on latents of SMPL pose and clothing\ntype to form the clothing deformation from the SMPL body. gDNA\n[CJS\u221722] introduces a generation framework conditioned on latent\ncodes of shape and surface details to learn the underlying statistics\nof 3D clothing details from scanned human datasets via an adver-\nsarial loss. Different from the previous methods that generate an\nintegrated 3D clothed human body geometry, SMPLicit [CPA\u221721]\nadopts an implicit model conditioned on shape and pose parame-\nters to individually generate diverse 3D clothes. By combining the\nSMPL body and associated generated 3D clothes, SMPLicit en-\nables to produce 3D clothed human shapes. To further improve the\nquality of the generated human shape, Chupa [KKL\u221723] introduces\ndiffusion models to generate realistic human geometry and decom-\npose the 3D generation task into 2D normal map generation and\nnormal map-based 3D reconstruction.\nAlthough these methods achieve the generation of detailed\nclothed human shapes, their application prospects are greatly re-\nstricted due to the lack of texture-generation capabilities. To\ngenerate textured clothed 3D human, lots of attempts have\nbeen made in previous work, including methods conditioned\non latent codes [GII\u221721, BKY\u221722, ZJY\u221722, NSLH22, JJW\u221723,\nYLWD22, XKJ\u221723, CHB\u221723, HCL\u221722, XYB\u221723, AYS\u221723], sin-\ngle\nimages\n[SHN\u221719, ZYLD21, AZS22, CMA\u221722, GLZ\u221723,\nHYL\u221723, YLX\u221723, HHP\u221723, AST\u221723, HYX\u221723, ZLZ\u221723], and\ntext prompts [HZP\u221722, JWZ\u221723, CCH\u221723, HWZ\u221723, KAZ\u221723,\nZCY\u221723, LYX\u221724, HSZ\u221723, ZZZ\u221723a, LZT\u221723]. Most latent-\nconditioned methods employ adversarial losses to restrict their la-\ntent space and generate 3D human bodies within the relevant do-\nmain of the training dataset. For example, StylePeople [GII\u221721]\ncombines StyleGAN [KLA\u221720] and neural rendering to design a\njoint generation framework trained in an adversarial fashion on\nthe full-body image datasets. Furthermore, GNARF [BKY\u221722] and\nAvatarGen [ZJY\u221722] employ tri-planes as the 3D representation\nand replace the neural rendering with volume rendering to en-\nhance the view-consistency of rendered results. To improve ed-\nitability, Get3DHuman [XKJ\u221723] divides the human body gener-\nation framework into shape and texture branches respectively con-\nditioned on shape and texture latent codes, achieving re-texturing.\nEVA3D [HCL\u221722] divides the generated human body into local\nparts to achieve controllable human poses.\nAs text-to-image models [RKH\u221721b, RBL\u221722b, SCS\u221722] con-\ntinue to advance rapidly, the field of text-to-3D has also reached\nits pinnacle of development. For the text-driven human generation,\nexisting methods inject priors from pre-trained text-to-image mod-\nels into the 3D human generation framework to achieve text-driven\ntextured human generation, such as AvatarCLIP [HZP\u221722], Avatar-\nCraft [JWZ\u221723], DreamHuman [KAZ\u221723], and TADA [LYX\u221724].\nIndeed, text-driven human generation methods effectively address\nthe challenge of limited 3D training data and significantly enhance\nthe generation capabilities of 3D human assets. Nevertheless, in\ncontrast to the generation of unseen 3D humans, it is also signif-\nicant to generate a 3D human body from a specified single im-\nage in real-life applications. In terms of single-image-conditioned\n3D human generation methods, producing generated results with\ntextures and geometries aligned with the input reference image is\nwidely studied. To this end, PIFu [SHN\u221719], PaMIR [ZYLD21],\nand PHORHUM [AZS22] propose learning-based 3D generators\ntrained on scanned human datasets to infer human body geome-\ntry and texture from input images. However, their performance is\nconstrained by the limitations of the training data. Consequently,\nthey struggle to accurately infer detailed textures and fine geom-\netry from in-the-wild input images, particularly in areas that are\nnot directly visible in the input. To achieve data-free 3D human\ngeneration, ELICIT [HYL\u221723], Human-SGD [AST\u221723], TeCH\n[HYX\u221723], and HumanRef [ZLZ\u221723] leverage priors of pre-trained\nCLIP [RKH\u221721b] or image diffusion models [RBL\u221722b,SCS\u221722]\nto predict the geometry and texture based on the input reference\nimage without the need for 3D datasets, and achieve impressive\nqualities in generated 3D clothed human.\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n17\n3D Face\nGeneration\nPersonalized \nHead Avatar\nNerFace, CVPR' 21\nNHA, CVPR'22\nPiCA, CVPR'21\nPointAvatar, CVPR '23\nI M Avatar, CVPR'22\nMonoAvatar, CVPR'23\nINSTA, CVPR'23\nNPVA, SGA'23\nNeRFBlendshape, TOG'22\nNeural Implicit\n3DMMs\ni3DMM, CVPR'21\nImFace, CVPR'22\nNPHM, CVPR'23\nGenerative 3D\nFace Models\nHoloGAN, PlatonicGAN\nGRAF, NeurIPS'20\nGIRAFFE, CVPR'21\npi-GAN, CVPR'21\nStyleNeRF, ICLR'22\nStyleSDF, CVPR'22\nEG3D, CVPR'22\nPanoHead, CVPR'23\nRODIN, CVPR'23\nOmniAvatar, CVPR'23\nNext3D, CVPR'23\nStyleAvatar3D, arXiv'23\nFigure 9: Representative applications and methods of 3D face gen-\neration.\n6.2. 3D Face Generation\nOne essential characteristic of 3D face generation tasks is to gen-\nerate high-quality human face images that can be viewed from dif-\nferent viewpoints. Popular tasks can be loosely classified into three\nmajor categories, including personalized head avatar creation (e.g.\n3D talking head generation), neural implicit 3D morphable mod-\nels (3DMMs), and generative 3D face models, which are shown in\nFig. 9 and Fig. 10.\nPersonalized head avatar creation aims at creating an animatable\navatar that can be viewed from different viewpoints of the target\nperson, which has broad applications such as talking head gener-\nation. Most of the existing methods take as input a sequence of\nvideo frames (i.e. monocular video) [PSB\u221721, GTZN21, GPL\u221722,\nZAB\u221722, ZBT23, ZYW\u221723, BTH\u221723, GZX\u221722]. Although conve-\nnient, the viewing angles of these avatars are limited in a rela-\ntively small range (i.e. near frontal) and their quality is not al-\nways satisfactory due to limited data. In contrast, another stream\nof works [LSSS18,MSS\u221721,LSS\u221721,WKC\u221723,KQG\u221723] aims at\ncreating a very high-quality digital human that can be viewed from\nlarger angles (e.g. side view). These methods usually require high-\nquality synchronized multi-view images under even illumination.\nHowever, both streams rely heavily on implicit or hybrid neural\nrepresentations and neural rendering techniques. The quality and\nanimation accuracy of the generated talking head video are usually\nmeasured with PSNR, SSIM, and LPIPS metrics.\nNeural implicit 3DMMs. Traditional 3D morphable face models\n(3DMMs) assume a predefined template mesh (i.g. fixed topology)\nfor the geometry and have explored various modeling methods in-\nPersonalized Avatar Generation\nNeural Implicit 3DMMs\nGenerative 3D face models\nFigure 10: Representative 3D face generation tasks. Images adapted\nfrom NHA [GPL\u221722], NPHM [GKG\u221723], and EG3D [CLC\u221722].\ncluding linear models (e.g. PCA-based 3DMMs) and non-linear\nmodels (e.g. network-based 3DMMs). A comprehensive survey of\nthese methods has been discussed in [EST\u221720]. Recently, thanks\nto the rapid advances in implicit neural representations (INRs),\nseveral neural implicit 3DMMs utilizing INRs for face modeling\nemerges [YTB\u221721, ZYHC22, GKG\u221723] since continuous implicit\nneural representations do not face discretization error and can the-\noretically modeling infinite details. Indeed, NPHM [GKG\u221723] can\ngenerate more subtle expressions unseen in previous mesh-based\n3DMMs. What\u2019s more, neural implicit 3DMMs can potentially\nmodel hair better since the complexity of different hairstyles varies\ndrastically, which imposes a great challenge for fixed topology\nmesh-based traditional 3DMMs.\nGenerative 3D face models. One key difference from 2D gen-\nerative face models (e.g. StyleGAN [KLA19, KLA\u221720]) is that\n3D face models can synthesize multi-view consistent images (i.e.\nnovel views) of the same target (identity and clothes). Early at-\ntempts towards this direction include HoloGAN [NPLT\u221719] and\nPlatonicGAN [HMR19b], which are both voxel-based methods and\ncan only generate images in limited resolution. Quickly, meth-\nods [SLNG20, NG21, CMK\u221721b, OELS\u221722, GLWT22, CLC\u221722]\nutilizing neural radiance fields are proposed to increase the image\nresolution. For example, EG3D [CLC\u221722] proposes a hybrid tri-\nplane representation, which strikes a good trade-off to effectively\naddress the memory and rendering inefficiency faced by previous\ngenerative 3D GANs and can produce high-quality images with\ngood multi-view consistency.\nThanks to the success of various 3D GANs, many down-\n18\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\nTable 6: Applications of general scene generation methods.\nMethods\nType\nCondition\nTexture\nGeneration\nPVD [ZDW21]\nObject-Centered\nLatent\n%\nNFD [SCP\u221723]\nObject-Centered\nLatent\n%\nPoint-E [NJD\u221722]\nObject-Centered\nText\n%\nDiffusion-SDF [LDZL23]\nObject-Centered\nText\n%\nDeep3DSketch+ [CFZ\u221723]\nObject-Centered\nSketch\n%\nZero-1-to-3 [LWVH\u221723]\nObject-Centered Single-Image\n!\nMake-It-3D [TWZ\u221723]\nObject-Centered Single-Image\n!\nGET3D [GSW\u221722]\nObject-Centered\nLatent\n!\nEG3D [CLC\u221722]\nObject-Centered\nLatent\n!\nCLIP-Mesh [MKXBP22]\nObject-Centered\nText\n!\nDreamFusion [PJBM23]\nObject-Centered\nText\n!\nProlificDreamer [WLW\u221723] Object-Centered\nText\n!\nPixelSynth [RFJ21]\nOutward-Facing Single-Image\n!\nDiffDreamer [CCP\u221723]\nOutward-Facing Single-Image\n!\nXiang et al. [XYHT23]\nOutward-Facing\nLatent\n!\nCC3D [BPP\u221723]\nOutward-Facing\nLayout\n!\nText2Room [HCO\u221723]\nOutward-Facing\nText\n!\nText2NeRF [ZLW\u221723]\nOutward-Facing\nText\n!\nstream applications (e.g. editing, talking head generation) are en-\nabled or become less data-hungry, including 3D consistent edit-\ning [SWW\u221723, SWZ\u221722, SWS\u221722, LFLSY\u221723, JCL\u221722], 3D talk-\ning head generation [BFW\u221723,XSJ\u221723,WDY\u221722], etc.\n6.3. General Scene Generation\nDifferent from 3D human and face generation, which can use ex-\nisting prior knowledge such as SMPL and 3DMM, general scene\ngeneration methods are more based on the similarity of semantics\nor categories to design a 3D model generation framework. Based on\nthe differences in generation results, as shown in Fig. 11 and Tab. 6,\nwe further subdivide general scene generation into object-centered\nasset generation and outward-facing scene generation.\n6.3.1. Object-Centered Asset Generation\nThe field of object-centered asset generation has seen significant\nadvancements in recent years, with a focus on both textureless\nshape generation and textured asset generation. For the textureless\nshape generation, early works use GAN-based networks to learn\na mapping from latent space to 3D object space based on spe-\ncific categories of 3D data, such as 3D-GAN [WZX\u221716], Holo-\nGAN [NPLT\u221719], and PlatonicGAN [HMR19b]. However, limited\nby the generation capabilities of GANs, these methods can only\ngenerate rough 3D assets of specific categories. To improve the\nquality of generated results, SingleShapeGen [WZ22] leverages a\npyramid of generators to generate 3D assets in a coarse to fine\nmanner. Given the remarkable achievements of diffusion models\nin image generation, researchers are directing their attention to-\nwards the application of diffusion extensions in the realm of 3D\ngeneration. Thus, subsequent methods [LH21, ZDW21, HLHF22,\nSCP\u221723, EMS\u221723] explore the use of diffusion processes for 3D\nshape generation from random noise. In addition to these latent-\nbased methods, another important research direction is text-driven\n3D asset generation [CCS\u221719, LWQF22]. For example, 3D-LDM\n[NKR\u221722], SDFusion [CLT\u221723], and Diffusion-SDF [LDZL23]\nachieve text-to-3D shape generation by designing the diffusion\nprocess in 3D feature space. Due to such methods requiring 3D\ndatasets to train the diffusion-based 3D generators, they are limited\nto the training data in terms of the categories and diversity of gen-\nerated results. By contrast, CLIP-Forge [SCL\u221722], CLIP-Sculptor\n[SFL\u221723], and Michelangelo [ZLC\u221723] directly employ the prior\nof the pre-trained CLIP model to constrain the 3D generation pro-\ncess, effectively improving the generalization of the method and the\ndiversity of generation results. Unlike the above latent-conditioned\nor text-driven 3D generation methods, to generate 3D assets with\nexpected shapes, there are some works [HMR19a, CFZ\u221723] that\nexplore image or sketch-conditioned generation.\nIn comparison to textureless 3D shape generation, textured\n3D asset generation not only produces realistic geometric struc-\ntures but also captures intricate texture details. For example,\nHoloGAN [NPLT\u221719], GET3D [GSW\u221722], and EG3D [CLC\u221722]\nemploy GAN-based 3D generators conditioned on latent vec-\ntors to produce category-specific textured 3D assets. By contrast,\ntext-driven 3D generation methods rely on the prior knowledge\nof pre-trained large-scale text-image models to enable category-\nfree 3D asset generation. For instance, CLIP-Mesh [MKXBP22],\nDream Fields [JMB\u221722], and PureCLIPNeRF [LC22] employ\nthe prior of CLIP model to constrain the optimization process\nand achieve text-driven 3D generation. Furthermore, DreamFu-\nsion [PJBM23] and SJC [WDL\u221723] propose a score distillation\nsampling (SDS) method to achieve 3D constraint which priors ex-\ntracted from pre-trained 2D diffusion models. Then, some meth-\nods further improve the SDS-based 3D generation process in terms\nof generation quality, multi-face problem, and optimization effi-\nciency, such as Magic3D [LGT\u221723], Latent-NeRF [MRP\u221723b],\nFantasia3D [CCJJ23], DreamBooth3D [RKP\u221723], HiFA [ZZ23],\nATT3D [LXZ\u221723], ProlificDreamer [WLW\u221723], IT3D [CZY\u221723],\nDreamGaussian [TRZ\u221723], and CAD [WPH\u221723]. On the other\nhand, distinct from text-driven 3D generation, single-image-\nconditioned 3D generation is also a significant research direction\n[LWVH\u221723,MKLRV23,CGC\u221723,WLY\u221723,KDJ\u221723].\n6.3.2. Outward-Facing Scene Generation\nEarly scene generation methods often require specific scene data\nfor training to obtain category-specific scene generators, such as\nGAUDI [BGA\u221722] and the work of Xiang et al. [XYHT23],\nor implement a single scene reconstruction based on the input\nimage, such as PixelSynth [RFJ21] and Worldsheet [HRBP21].\nHowever, these methods are either limited by the quality of\nthe generation or by the extensibility of the scene. With the\nrise of diffusion models in image inpainting, various methods\nare beginning to use the scene completion capabilities of dif-\nfusion models to implement scene generation tasks [CCP\u221723,\nHCO\u221723,ZLW\u221723]. Recently, SceneScape [FAKD23], Text2Room\n[HCO\u221723], Text2NeRF [ZLW\u221723], and LucidDreamer [CLN\u221723]\npropose progressive inpainting and updating strategies for gener-\nating realistic 3D scenes using pre-trained diffusion models. Sce-\nneScape and Text2Room utilize explicit polygon meshes as their\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n19\nFigure 11: Some examples of general scene generation methods. 3D generation results source from Deep3DSketch+ [CFZ\u221723],\nNFD\n[SCP\u221723],\nDiffusion-SDF\n[LDZL23],\nMake-It-3D\n[TWZ\u221723],\nGET3D\n[GSW\u221722],\nProlificDreamer\n[WLW\u221723],\nDiff-\nDreamer [CCP\u221723], CC3D [BPP\u221723], Xiang et al. [XYHT23], Text2NeRF [ZLW\u221723], and LucidDreamer [CLN\u221723].\n3D representation during the generation procedure. However, this\nchoice of representation imposes limitations on the generation of\noutdoor scenes, resulting in stretched geometry and blurry artifacts\nin the fusion regions of mesh faces. In contrast, Text2NeRF and\nLucidDreamer adopt implicit representations, which offer the abil-\nity to model fine-grained geometry and textures without specific\nscene requirements. Consequently, Text2NeRF and LucidDreamer\ncan generate both indoor and outdoor scenes with high fidelity.\n6.4. 3D Editing\nBased on the region where editing happens, we classify the existing\nworks into global editing and local editing.\n6.4.1. Global Editing\nGlobal editing works aim at changing the appearance or geom-\netry of the competing 3D scene globally. Different from local\nediting, they usually do not intentionally isolate a specific region\nfrom a complete and complicated scene or object. Most commonly,\nthey only care if the resultant scene is in a desired new \u201cstyle\u201d\nand resembles (maintains some features of) the original scene.\nMost representative tasks falling into this category include styliza-\ntion [HTS\u221721,HHY\u221722,FJW\u221722,ZKB\u221722,WJC\u221723,HTE\u221723], and\nsingle-object manipulation (e.g. re-texturing [MBOL\u221722, LZJ\u221722,\nMRP\u221723b,CCJJ23]) as shown in Fig. 12.\nStylization. Early 3D scene stylization methods [HTS\u221721,\nHHY\u221722, FJW\u221722, ZKB\u221722] usually require style images to pro-\nvide style reference. The 3D scene is optimized either in the style\nfeature space using a Gram matrix [GEB16] or nearest neighbor\nfeature matching [ZKB\u221722] loss or in the image space using the\noutput color of a deep image style transfer network [HB17]. Later\nmethods [WJC\u221723, HTE\u221723] can support textual format style def-\ninition by utilizing the learned prior knowledge from large-scale\nlanguage-vision models such as CLIP [RKH\u221721a] and Stable Dif-\nfusion [RBL\u221722a]. Other than commonly seen artistic style trans-\nfer, there also exist some special types of \u201cstyle\u201d manipulation\ntasks such as seasonal and illumination manipulations [LLF\u221723,\nCZL\u221722,HTE\u221723,CYL\u221722] and climate changes.\nSingle-Object Manipulation. There are many papers specifically\naim at manipulating a single 3D object. For example, one rep-\nresentative task is texturing or painting a given 3D object (usu-\nally in mesh format) [MBOL\u221722, LZJ\u221722, MRP\u221723b, CCJJ23,\nCSL\u221723]. Except for diffuse albedo color and vertex displace-\nment [MBOL\u221722, MZS\u221723, LYX\u221724], other common property\nmaps may be involved, including normal map [CCJJ23, LZJ\u221722],\nroughness map [CCJJ23, LZJ\u221722], specular map [LZJ\u221722], and\nmetallic map [CCJJ23], etc. A more general setting would be\ndirectly manipulating a NeRF-like object [WCH\u221722, LZJ\u221722,\nTLYCS22, YBZ\u221722]. Notably, the human face/head is one special\ntype of object that has drawn a lot of interest [ATDN23,ZQL\u221723].\nIn the meanwhile, many works focus on fine-grained local face\nmanipulation, including expression and appearance manipula-\ntion [SWZ\u221722, SWS\u221722, LFLSY\u221723, JCL\u221722, WDY\u221722, XSJ\u221723,\nMLL\u221722a, ZLW\u221722] and face swapping [LMY\u221723] since human\nface related understanding tasks (e.g. recognition, parsing, attribute\nclassification) have been extensively studied previously.\n6.4.2. Local Editing\nLocal editing tasks intentionally modify only a specific re-\ngion, either manually provided ( [MPS\u221723, LDS\u221723, CYW\u221723])\nor automatically determined ( [YZX\u221721, WLC\u221722, WWL\u221723,\nKMS22, JKK\u221723]), of the complete scene or object. Common\n20\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\nStylization\nIron Man                    Brick Lamp                 Colorful Crochet Candle        Astronaut Horse\nTexturing\nOriginal\nAffine\nNon-affine\nDuplication\nLocal Editing\nFigure 12: Representative 3D editing tasks. Images adapted from\nARF [ZKB\u221722], Text2Mesh [MBOL\u221722], NeRFShop [JKK\u221723],\nSKED [MPS\u221723], and DreamEditor [ZWL\u221723].\nlocal editing types include appearance manipulation [YBZ\u221722,\nZWL\u221723], geometry deformation [JKK\u221723, PYL\u221722, YSL\u221722,\nTLYCS22], object-/semantic-level duplication/deletion and mov-\ning/removing [YZX\u221721,WLC\u221722,KMS22, WWL\u221723]. For exam-\nple, NeuMesh [YBZ\u221722] supports several kinds of texture manip-\nulation including swapping, filling, and painting since they dis-\ntill a NeRF scene into a mesh-based neural representation. NeRF-\nShop [JKK\u221723] and CageNeRF [PYL\u221722] transform/deform the\nvolume bounded by a mesh cage, resulting in moved or de-\nformed/articulated object. SINE [BZY\u221723] updates both the NeRF\ngeometry and the appearance with geometry prior and semantic\n(image feature) texture prior as regularizations.\nAnother line of works (e.g. ObjectNeRF [YZX\u221721], Ob-\njectSDF [WLC\u221722], DFF [KMS22]) focus on automatically de-\ncomposing the scene into individual objects or semantic parts dur-\ning reconstruction, which is made possible by utilizing extra 2D\nimage understanding networks (e.g. instance segmentation), and\nsupport subsequent object-level manipulations such as re-coloring,\nremoval, displacement, duplication.\nRecently, it is possible to create new textures and/or content\nonly according to text description in the existing 3D scenes due\nto the success of large-scale text-to-image models (e.g. Stable Dif-\nfusion [RBL\u221722a]). For example, instruct-NeRF2NeRF [HTE\u221723]\niteratively updates the reference dataset images modified by a ded-\nicated diffusion model [BHE23] and the NeRF model. DreamEdi-\ntor [ZWL\u221723] performs local updates on the region located by text\nattention guided by score distillation sampling [PJBM23]. Focal-\nDreamer [LDS\u221723] creates new geometries (objects) in the spec-\nified empty spaces according to the text input. SKED [MPS\u221723]\nsupports both creating new objects and modifying the existing\npart located in the region specified by the provided multi-view\nsketches.\n7. Open Challenges\nThe quality and diversity of 3D generation results have experienced\nsignificant progress due to advancements in generative models, 3D\nrepresentations, and algorithmic paradigms. Considerable attention\nhas been drawn to 3D generation recently as a result of the suc-\ncess achieved by large-scale models in natural language process-\ning and image generation. However, numerous challenges remain\nbefore the generated 3D models can meet the high industrial stan-\ndards required for video games, movies, or immersive digital con-\ntent in VR/AR. In this section, we will explore some of the open\nchallenges and potential future directions in this field.\nEvaluation. Quantifying the quality of generated 3D models objec-\ntively is an important and not widely explored problem. Using met-\nrics such as PSNR, SSIM, and F-Score to evaluate rendering and\nreconstruction results requires ground truth data on the one hand,\nbut on the other hand, it can not comprehensively reflect the quality\nand diversity of the generated content. In addition, user studies are\nusually time-consuming, and the study results tend to be influenced\nby the bias and number of surveyed users. Metrics that capture both\nthe quality and diversity of the results like FID can be applied to\n3D data, but may not be always aligned with 3D domain and hu-\nman preferences. Better metrics to judge the results objectively in\nterms of generation quality, diversity, and matching degree with the\nconditions still need further exploration.\nDataset. Unlike language or 2D image data which can be easily\ncaptured and collected, 3D assets often require 3D artists or de-\nsigners to spend a significant amount of time using professional\nsoftware to create. Moreover, due to the different usage scenarios\nand creators\u2019 personal styles, these 3D assets may differ greatly in\nscale, quality, and style, increasing the complexity of 3D data. Spe-\ncific rules are needed to normalize this diverse 3D data, making it\nmore suitable for generation methods. A large-scale, high-quality\n3D dataset is still highly desirable in 3D generation. Meanwhile,\nexploring how to utilize extensive 2D data for 3D generation could\nalso be a potential solution to address the scarcity of 3D data.\nRepresentation. Representation is an essential part of the 3D gen-\neration, as we discuss various representations and the associated\nmethods in Sec. 3. Implicit representation is able to model com-\nplex geometric topology efficiently but faces challenges with slow\noptimization; explicit representation facilitates rapid optimization\nconvergence but struggles to encapsulate complex topology and\ndemands substantial storage resources; Hybrid representation at-\ntempts to consider the trade-off between these two, but there are\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n21\nstill shortcomings. In general, we are motivated to develop a repre-\nsentation that balances optimization efficiency, geometric topology\nflexibility, and resource usage.\nControllability. The purpose of the 3D generation technique is\nto generate a large amount of user-friendly, high-quality, and di-\nverse 3D content in a cheap and controllable way. However, em-\nbedding the generated 3D content into practical applications re-\nmains a challenge: most methods [PJBM23, CLC\u221722, YHH\u221719b]\nrely on volume rendering or neural rendering, and fail to generate\ncontent suitable for rasterization graphics pipeline. As for meth-\nods [CCJJ23, WLW\u221723, TRZ\u221723] that generate the content repre-\nsented by polygons, they do not take into account layout (e.g. the\nrectangular plane of a table can be represented by two triangles) and\nhigh-quality UV unwrapping and the generated textures also face\nsome issues such as baked shadows. These problems make the gen-\nerated content unfavorable for artist-friendly interaction and edit-\ning. Furthermore, the style of generated content is still limited by\ntraining datasets. Furthermore, the establishment of comprehensive\ntoolchains is a crucial aspect of the practical implementation of 3D\ngeneration. In modern workflows, artists use tools (e.g. LookDev)\nto harmonize 3D content by examining and contrasting the relight-\ning results of their materials across various lighting conditions.\nConcurrently, modern Digital Content Creation (DCC) software\noffers extensive and fine-grained content editing capabilities. It is\npromising to unify 3D content produced through diverse methods\nand establish tool chains that encompass abundant editing capabil-\nities.\nLarge-scale Model. Recently, the popularity of large-scale models\nhas gradually affected the field of 3D generation. Researchers are\nno longer satisfied with using distillation scores that use large-scale\nimage models as the priors to optimize 3D content, but directly\ntrain large-scale 3D models. MeshGPT [SAA\u221723] follows large\nlanguage models and adopts a sequence-based approach to autore-\ngressively generate sequences of triangles in the generated mesh.\nMeshGPT takes into account layout information and generates\ncompact and sharp meshes that match the style created by artists.\nSince MeshGPT is a decoder-only transformer, compared with the\noptimization-based generation, it gets rid of inefficient multi-step\nsequential optimization, achieving rapid generation. Despite this,\nMeshGPT\u2019s performance is still limited by training datasets and\ncan only generate regular furniture objects. But there is no doubt\nthat large-scale 3D generation models have great potential worth\nexploring.\n8. Conclusion\nIn this work, we present a comprehensive survey on 3D gen-\neration, encompassing four main aspects: 3D representations,\ngeneration methods, datasets, and various applications. We be-\ngin by introducing the 3D representation, which serves as the\nbackbone and determines the characteristics of the generated\nresults. Next, we summarize and categorize a wide range of\ngeneration methods, creating an evolutionary tree to visualize their\nbranches and developments. Finally, we provide an overview of\nrelated datasets, applications, and open challenges in this field.\nThe realm of 3D generation is currently witnessing explosive\ngrowth and development, with new work emerging every week\nor even daily. We hope this survey offers a systematic sum-\nmary that could inspire subsequent work for interested readers.\nReferences\n[ACC\u221722]\nADAMKIEWICZ M., CHEN T., CACCAVALE A., GARDNER\nR., CULBERTSON P., BOHG J., SCHWAGER M.: Vision-only robot navi-\ngation in a neural radiance world. IEEE Robotics and Automation Letters\n7, 2 (2022), 4606\u20134613. 5\n[ADMG18]\nACHLIOPTAS P., DIAMANTI O., MITLIAGKAS I., GUIBAS\nL.: Learning representations and generative models for 3d point clouds.\nIn International conference on machine learning (2018), PMLR, pp. 40\u2013\n49. 7, 9\n[ALG\u221720]\nATTAL B., LING S., GOKASLAN A., RICHARDT C., TOMP-\nKIN J.: Matryodshka: Real-time 6dof video view synthesis using multi-\nsphere images. In European Conference on Computer Vision (2020),\nSpringer, pp. 441\u2013459. 5\n[APMTM19]\nALLDIECK T., PONS-MOLL G., THEOBALT C., MAGNOR\nM.: Tex2shape: Detailed full human body geometry from a single image.\nIn Proceedings of the IEEE/CVF International Conference on Computer\nVision (2019), pp. 2293\u20132303. 15, 16\n[AQW19]\nABDAL R., QIN Y., WONKA P.:\nImage2stylegan: How\nto embed images into the stylegan latent space?\nIn Proceedings of\nthe IEEE/CVF international conference on computer vision (2019),\npp. 4432\u20134441. 2, 6\n[ASK\u221720]\nALIEV K.-A., SEVASTOPOLSKY A., KOLOS M., ULYANOV\nD., LEMPITSKY V.: Neural point-based graphics. In Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part XXII 16 (2020), Springer, pp. 696\u2013712. 4\n[AST\u221723]\nALBAHAR B., SAITO S., TSENG H.-Y., KIM C., KOPF J.,\nHUANG J.-B.: Single-image 3d human digitization with shape-guided\ndiffusion. In SIGGRAPH Asia 2023 Conference Papers (2023), pp. 1\u201311.\n16\n[ATDN23]\nANEJA S., THIES J., DAI A., NIESSNER M.: ClipFace: Text-\nguided editing of textured 3d morphable models. In ACM SIGGRAPH\n2023 Conference Proceedings (2023). 19\n[AYS\u221723]\nABDAL R., YIFAN W., SHI Z., XU Y., PO R., KUANG Z.,\nCHEN Q., YEUNG D.-Y., WETZSTEIN G.: Gaussian shell maps for\nefficient 3d human generation. arXiv preprint arXiv:2311.17857 (2023).\n16\n[AZA\u221721]\nAHMADYAN A., ZHANG L., ABLAVATSKI A., WEI J.,\nGRUNDMANN M.: Objectron: A large scale dataset of object-centric\nvideos in the wild with pose annotations.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition\n(2021), pp. 7822\u20137831. 15\n[AZS22]\nALLDIECK T., ZANFIR M., SMINCHISESCU C.: Photorealis-\ntic monocular 3d reconstruction of humans wearing clothing. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (2022), pp. 1506\u20131515. 16\n[BBJ\u221721]\nBOSS M., BRAUN R., JAMPANI V., BARRON J. T., LIU C.,\nLENSCH H.: Nerd: Neural reflectance decomposition from image col-\nlections. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision (2021), pp. 12684\u201312694. 5\n[BFO\u221720]\nBROXTON M., FLYNN J., OVERBECK R., ERICKSON D.,\nHEDMAN P., DUVALL M., DOURGARIAN J., BUSCH J., WHALEN M.,\nDEBEVEC P.: Immersive light field video with a layered mesh repre-\nsentation. ACM Transactions on Graphics (TOG) 39, 4 (2020), 86\u20131.\n5\n[BFW\u221723]\nBAI Y., FAN Y., WANG X., ZHANG Y., SUN J., YUAN C.,\nSHAN Y.:\nHigh-fidelity facial avatar reconstruction from monocular\nvideo with generative priors. In CVPR (2023). 18\n22\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n[BGA\u221722]\nBAUTISTA M. A., GUO P., ABNAR S., TALBOTT W., TO-\nSHEV A., CHEN Z., DINH L., ZHAI S., GOH H., ULBRICHT D.,\nET AL.: Gaudi: A neural architect for immersive 3d scene generation.\nAdvances in Neural Information Processing Systems 35 (2022), 25102\u2013\n25116. 18\n[BGP\u221722]\nBAATZ H., GRANSKOG J., PAPAS M., ROUSSELLE F.,\nNOV\u00c1K J.: Nerf-tex: Neural reflectance field textures. In Computer\nGraphics Forum (2022), vol. 41, Wiley Online Library, pp. 287\u2013301. 5\n[BHE23]\nBROOKS T., HOLYNSKI A., EFROS A. A.: InstructPix2Pix:\nLearning to follow image editing instructions. In CVPR (2023). 20\n[BHMK\u221718]\nBEN-HAMU H., MARON H., KEZURER I., AVINERI G.,\nLIPMAN Y.: Multi-chart generative surface modeling. ACM Transac-\ntions on Graphics (TOG) 37, 6 (2018), 1\u201315. 7, 9\n[BKP\u221710]\nBOTSCH M., KOBBELT L., PAULY M., ALLIEZ P., L\u00c9VY B.:\nPolygon mesh processing. CRC press, 2010. 4\n[BKY\u221722]\nBERGMAN A., KELLNHOFER P., YIFAN W., CHAN E., LIN-\nDELL D., WETZSTEIN G.: Generative neural articulated radiance fields.\nAdvances in Neural Information Processing Systems 35 (2022), 19900\u2013\n19916. 16\n[BLW16]\nBROCK A., LIM T., WESTON N.: Generative and discrimina-\ntive voxel modeling with convolutional neural networks. arXiv preprint\narXiv:1608.04236 (2016). 6, 11\n[BMR\u221720]\nBROWN T., MANN B., RYDER N., SUBBIAH M., KAPLAN\nJ. D., DHARIWAL P., NEELAKANTAN A., SHYAM P., SASTRY G.,\nASKELL A., ET AL.: Language models are few-shot learners. NeurIPS\n(2020). 2, 6, 10\n[BMT\u221721]\nBARRON J. T., MILDENHALL B., TANCIK M., HEDMAN\nP., MARTIN-BRUALLA R., SRINIVASAN P. P.: Mip-nerf: A multiscale\nrepresentation for anti-aliasing neural radiance fields. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision (2021),\npp. 5855\u20135864. 1, 5\n[BMV\u221722]\nBARRON J. T., MILDENHALL B., VERBIN D., SRINIVASAN\nP. P., HEDMAN P.: Mip-nerf 360: Unbounded anti-aliased neural radi-\nance fields. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (2022), pp. 5470\u20135479. 5\n[BNT21]\nBUROV A., NIESSNER M., THIES J.:\nDynamic surface\nfunction networks for clothed human bodies.\nIn Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (2021),\npp. 10754\u201310764. 4\n[BPP\u221723]\nBAHMANI S., PARK J. J., PASCHALIDOU D., YAN X., WET-\nZSTEIN G., GUIBAS L., TAGLIASACCHI A.: Cc3d: Layout-conditioned\ngeneration of compositional 3d scenes. arXiv preprint arXiv:2303.12074\n(2023). 18, 19\n[BSKG22]\nBEN-SHABAT Y., KONEPUTUGODAGE C. H., GOULD S.:\nDigs: Divergence guided shape implicit neural representation for un-\noriented point clouds. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (2022), pp. 19323\u201319332. 6\n[BTH\u221723]\nBAI Z., TAN F., HUANG Z., SARKAR K., TANG D., QIU D.,\nMEKA A., DU R., DOU M., ORTS-ESCOLANO S., ET AL.: Learning\npersonalized high quality volumetric head avatars from monocular rgb\nvideos. In CVPR (2023). 17\n[BZY\u221723]\nBAO C., ZHANG Y., YANG B., FAN T., YANG Z., BAO H.,\nZHANG G., CUI Z.: SINE: Semantic-driven image-based nerf editing\nwith prior-guided editing field. In CVPR (2023). 20\n[CBZ\u221719]\nCHENG S., BRONSTEIN M., ZHOU Y., KOTSIA I., PANTIC\nM., ZAFEIRIOU S.: Meshgan: Non-linear 3d morphable models of faces.\narXiv preprint arXiv:1903.10384 (2019). 7\n[CCH\u221723]\nCAO Y., CAO Y.-P., HAN K., SHAN Y., WONG K.-Y. K.:\nDreamavatar: Text-and-shape guided 3d human avatar generation via dif-\nfusion models. arXiv preprint arXiv:2304.00916 (2023). 16\n[CCJJ23]\nCHEN R., CHEN Y., JIAO N., JIA K.: Fantasia3D: Disentan-\ngling geometry and appearance for high-quality text-to-3d content cre-\nation. In ICCV (2023). 18, 19, 21\n[CCP\u221723]\nCAI S., CHAN E. R., PENG S., SHAHBAZI M., OBUKHOV\nA., VAN GOOL L., WETZSTEIN G.: Diffdreamer: Towards consistent\nunsupervised single-view scene extrapolation with conditional diffusion\nmodels. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision (2023), pp. 2139\u20132150. 18, 19\n[CCS\u221719]\nCHEN K., CHOY C. B., SAVVA M., CHANG A. X.,\nFUNKHOUSER T., SAVARESE S.: Text2shape: Generating shapes from\nnatural language by learning joint embeddings. In Computer Vision\u2013\nACCV 2018: 14th Asian Conference on Computer Vision, Perth, Aus-\ntralia, December 2\u20136, 2018, Revised Selected Papers, Part III 14 (2019),\nSpringer, pp. 100\u2013116. 18\n[CCW\u221723]\nCHEN Y., CHEN X., WANG X., ZHANG Q., GUO Y., SHAN\nY., WANG F.: Local-to-global registration for bundle-adjusting neural\nradiance fields. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (2023), pp. 8264\u20138273. 5\n[CFG\u221715]\nCHANG A. X., FUNKHOUSER T., GUIBAS L., HANRAHAN\nP., HUANG Q., LI Z., SAVARESE S., SAVVA M., SONG S., SU H.,\nET AL.:\nShapenet: An information-rich 3d model repository.\narXiv\npreprint arXiv:1512.03012 (2015). 14\n[CFZ\u221723]\nCHEN T., FU C., ZANG Y., ZHU L., ZHANG J., MAO P., SUN\nL.: Deep3dsketch+: Rapid 3d modeling from single free-hand sketches.\nIn International Conference on Multimedia Modeling (2023), Springer,\npp. 16\u201328. 18, 19\n[CGC\u221723]\nCHEN H., GU J., CHEN A., TIAN W., TU Z., LIU L., SU\nH.: Single-stage diffusion nerf: A unified approach to 3d generation and\nreconstruction. arXiv preprint arXiv:2304.06714 (2023). 6, 7, 10, 18\n[CGD\u221722]\nCOLLINS J., GOEL S., DENG K., LUTHRA A., XU L., GUN-\nDOGDU E., ZHANG X., VICENTE T. F. Y., DIDERIKSEN T., ARORA\nH., ET AL.: Abo: Dataset and benchmarks for real-world 3d object un-\nderstanding. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (2022), pp. 21126\u201321136. 15\n[CGT\u221719]\nCHOI I., GALLO O., TROCCOLI A., KIM M. H., KAUTZ J.:\nExtreme view synthesis. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (2019), pp. 7781\u20137790. 5\n[CHB\u221723]\nCHEN X., HUANG J., BIN Y., YU L., LIAO Y.: Veri3d: Gen-\nerative vertex-based radiance fields for 3d controllable human image syn-\nthesis. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision (2023), pp. 8986\u20138997. 16\n[CHIS23]\nCROITORU F.-A., HONDRU V., IONESCU R. T., SHAH M.:\nDiffusion models in vision: A survey. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (2023). 3\n[CJS\u221722]\nCHEN X., JIANG T., SONG J., YANG J., BLACK M. J.,\nGEIGER A., HILLIGES O.: gdna: Towards generative detailed neural\navatars. In Proceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (2022), pp. 20427\u201320437. 7, 15, 16\n[CLC\u221722]\nCHAN E. R., LIN C. Z., CHAN M. A., NAGANO K., PAN B.,\nDE MELLO S., GALLO O., GUIBAS L. J., TREMBLAY J., KHAMIS S.,\nET AL.: Efficient geometry-aware 3d generative adversarial networks. In\nCVPR (2022). 1, 6, 7, 9, 17, 18, 21\n[CLL23]\nCHENG Z., LI J., LI H.: Wildlight: In-the-wild inverse render-\ning with a flashlight. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (2023), pp. 4305\u20134314. 6\n[CLN\u221723]\nCHUNG J., LEE S., NAM H., LEE J., LEE K. M.: Lucid-\ndreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv\npreprint arXiv:2311.13384 (2023). 18, 19\n[CLT\u221723]\nCHENG Y.-C., LEE H.-Y., TULYAKOV S., SCHWING A. G.,\nGUI L.-Y.: Sdfusion: Multimodal 3d shape completion, reconstruction,\nand generation. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (2023), pp. 4456\u20134465. 10, 18\n[CMA\u221722]\nCHOI H., MOON G., ARMANDO M., LEROY V., LEE\nK. M., ROGEZ G.:\nMononhr: Monocular neural human renderer.\nIn 2022 International Conference on 3D Vision (3DV) (2022), IEEE,\npp. 242\u2013251. 16\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n23\n[CMK\u221721a]\nCHAN E. R., MONTEIRO M., KELLNHOFER P., WU J.,\nWETZSTEIN G.: pi-gan: Periodic implicit generative adversarial net-\nworks for 3d-aware image synthesis. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition (2021), pp. 5799\u2013\n5809. 9, 15\n[CMK\u221721b]\nCHAN E. R., MONTEIRO M., KELLNHOFER P., WU J.,\nWETZSTEIN G.: pi-GAN: Periodic implicit generative adversarial net-\nworks for 3d-aware image synthesis. In CVPR (2021). 17\n[CNC\u221723]\nCHAN E. R., NAGANO K., CHAN M. A., BERGMAN A. W.,\nPARK J. J., LEVY A., AITTALA M., DE MELLO S., KARRAS T., WET-\nZSTEIN G.: Generative novel view synthesis with 3d-aware diffusion\nmodels. arXiv preprint arXiv:2304.02602 (2023). 13\n[CPA\u221721]\nCORONA E., PUMAROLA A., ALENYA G., PONS-MOLL G.,\nMORENO-NOGUER F.: Smplicit: Topology-aware generative model for\nclothed people. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition (2021), pp. 11875\u201311885. 15, 16\n[CPB\u221720]\nCHOUTAS V., PAVLAKOS G., BOLKART T., TZIONAS D.,\nBLACK M. J.: Monocular expressive body regression through body-\ndriven attention. In Computer Vision\u2013ECCV 2020: 16th European Con-\nference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part X 16\n(2020), Springer, pp. 20\u201340. 16\n[CRJ22]\nCAO A., ROCKWELL C., JOHNSON J.: Fwd: Real-time novel\nview synthesis with forward warping and depth.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(2022), pp. 15713\u201315724. 4\n[CSH19]\nCHEN X., SONG J., HILLIGES O.:\nMonocular neural im-\nage based rendering with continuous view control.\nIn Proceedings\nof the IEEE/CVF international conference on computer vision (2019),\npp. 4090\u20134100. 14\n[CSL\u221723]\nCHEN D. Z., SIDDIQUI Y., LEE H.-Y., TULYAKOV S.,\nNIESSNER M.:\nText2Tex: Text-driven texture synthesis via diffusion\nmodels. In ICCV (2023). 19\n[CTZ20]\nCHEN Z., TAGLIASACCHI A., ZHANG H.: Bsp-net: Generat-\ning compact meshes via binary space partitioning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(2020), pp. 45\u201354. 6\n[CUYH20]\nCHOI Y., UH Y., YOO J., HA J.-W.: Stargan v2: Diverse\nimage synthesis for multiple domains. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition (2020), pp. 8188\u2013\n8197. 14, 15\n[CWS\u221715]\nCALLI B., WALSMAN A., SINGH A., SRINIVASA S.,\nABBEEL P., DOLLAR A. M.: Benchmarking in manipulation research:\nThe ycb object and model set and benchmarking protocols.\narXiv\npreprint arXiv:1502.03143 (2015). 14\n[CX\u221723]\nCHAO Y.-W., XIANG Y., ET AL.: Fewsol: A dataset for few-\nshot object learning in robotic environments.\nIn 2023 IEEE Interna-\ntional Conference on Robotics and Automation (ICRA) (2023), IEEE,\npp. 9140\u20139146. 15\n[CXG\u221716]\nCHOY C. B., XU D., GWAK J., CHEN K., SAVARESE S.:\n3d-r2n2: A unified approach for single and multi-view 3d object recon-\nstruction. In Computer Vision\u2013ECCV 2016: 14th European Conference,\nAmsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part\nVIII 14 (2016), Springer, pp. 628\u2013644. 6\n[CXG\u221722]\nCHEN A., XU Z., GEIGER A., YU J., SU H.:\nTensoRF:\nTensorial radiance fields. In European Conference on Computer Vision\n(2022). 6\n[CXZ\u221721]\nCHEN A., XU Z., ZHAO F., ZHANG X., XIANG F., YU J.,\nSU H.: MVSNeRF: Fast generalizable radiance field reconstruction from\nmulti-view stereo. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (2021), pp. 14124\u201314133. 5, 13\n[CYAE\u221720]\nCAI R., YANG G., AVERBUCH-ELOR H., HAO Z., BE-\nLONGIE S., SNAVELY N., HARIHARAN B.: Learning gradient fields\nfor shape generation. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16\n(2020), Springer, pp. 364\u2013381. 10\n[CYL\u221722]\nCHEN Y., YUAN Q., LI Z., LIU Y., WANG W., XIE C., WEN\nX., YU Q.: UPST-NeRF: Universal photorealistic style transfer of neural\nradiance fields for 3d scene. In arXiv preprint arXiv:2208.07059 (2022).\n19\n[CYW\u221723]\nCHENG X., YANG T., WANG J., LI Y., ZHANG L., ZHANG\nJ., YUAN L.:\nProgressive3D: Progressively local editing for text-to-\n3d content creation with complex semantic prompts.\narXiv preprint\narXiv:2310.11784 (2023). 19\n[CZ19]\nCHEN Z., ZHANG H.:\nLearning implicit fields for generative\nshape modeling. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (2019), pp. 5939\u20135948. 5, 9\n[CZC\u221724]\nCHEN H., ZHANG Y., CUN X., XIA M., WANG X., WENG\nC., SHAN Y.:\nVideocrafter2: Overcoming data limitations for high-\nquality video diffusion models, 2024. arXiv:2401.09047. 2\n[CZL\u221722]\nCHEN X., ZHANG Q., LI X., CHEN Y., FENG Y., WANG X.,\nWANG J.: Hallucinated neural radiance fields in the wild. In CVPR\n(2022), pp. 12943\u201312952. 5, 19\n[CZY\u221723]\nCHEN Y., ZHANG C., YANG X., CAI Z., YU G., YANG L.,\nLIN G.: It3d: Improved text-to-3d generation with explicit view synthe-\nsis. arXiv preprint arXiv:2308.11473 (2023). 18\n[DBD\u221722]\nDARMON F., BASCLE B., DEVAUX J.-C., MONASSE P.,\nAUBRY M.: Improving neural implicit surfaces geometry with patch\nwarping. In CVPR (2022). 6\n[DCS\u221717]\nDAI\nA.,\nCHANG\nA.\nX.,\nSAVVA\nM.,\nHALBER\nM.,\nFUNKHOUSER T., NIESSNER M.: Scannet: Richly-annotated 3d recon-\nstructions of indoor scenes. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition (2017), pp. 5828\u20135839. 14, 15\n[DFK\u221722]\nDOWNS L., FRANCIS A., KOENIG N., KINMAN B., HICK-\nMAN R., REYMANN K., MCHUGH T. B., VANHOUCKE V.: Google\nscanned objects: A high-quality dataset of 3d scanned household items.\nIn 2022 International Conference on Robotics and Automation (ICRA)\n(2022), IEEE, pp. 2553\u20132560. 14, 15\n[DJQ\u221723]\nDENG C., JIANG C., QI C. R., YAN X., ZHOU Y., GUIBAS\nL., ANGUELOV D., ET AL.:\nNerdi: Single-view nerf synthesis with\nlanguage-guided diffusion as general image priors. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(2023), pp. 20637\u201320647. 12\n[DLW\u221723]\nDEITKE M., LIU R., WALLINGFORD M., NGO H., MICHEL\nO., KUSUPATI A., FAN A., LAFORTE C., VOLETI V., GADRE S. Y.,\nET AL.: Objaverse-xl: A universe of 10m+ 3d objects. arXiv preprint\narXiv:2307.05663 (2023). 14, 15\n[Doe16]\nDOERSCH C.:\nTutorial on variational autoencoders.\narXiv\npreprint arXiv:1606.05908 (2016). 3\n[DRB\u221718]\nDAI A., RITCHIE D., BOKELOH M., REED S., STURM J.,\nNIESSNER M.: Scancomplete: Large-scale scene completion and seman-\ntic segmentation for 3d scans. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (2018), pp. 4578\u20134587. 6\n[DSS\u221723]\nDEITKE M., SCHWENK D., SALVADOR J., WEIHS L.,\nMICHEL O., VANDERBILT E., SCHMIDT L., EHSANI K., KEMBHAVI\nA., FARHADI A.: Objaverse: A universe of annotated 3d objects. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (2023), pp. 13142\u201313153. 14, 15\n[DZL\u221720]\nDAI P., ZHANG Y., LI Z., LIU S., ZENG B.: Neural point\ncloud rendering via multi-plane projection.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(2020), pp. 7830\u20137839. 4\n[DZW\u221720]\nDUAN Y., ZHU H., WANG H., YI L., NEVATIA R., GUIBAS\nL. J.: Curriculum deepsdf. In European Conference on Computer Vision\n(2020), Springer, pp. 51\u201367. 5\n[DZY\u221721]\nDU Y., ZHANG Y., YU H.-X., TENENBAUM J. B., WU J.:\nNeural radiance flow for 4d view synthesis and video processing. In\nProceedings of the IEEE/CVF International Conference on Computer\nVision (2021), IEEE Computer Society, pp. 14304\u201314314. 5\n24\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n[EGO\u221720]\nERLER P., GUERRERO P., OHRHALLINGER S., MITRA\nN. J., WIMMER M.: Points2surf learning implicit surfaces from point\nclouds. In European Conference on Computer Vision (2020), Springer,\npp. 108\u2013124. 5\n[EMS\u221723]\nERKO\u00c7 Z., MA F., SHAN Q., NIESSNER M., DAI A.: Hyper-\ndiffusion: Generating implicit neural fields with weight-space diffusion.\narXiv preprint arXiv:2303.17015 (2023). 18\n[EST\u221720]\nEGGER B., SMITH W. A., TEWARI A., WUHRER S., ZOLL-\nHOEFER M., BEELER T., BERNARD F., BOLKART T., KORTYLEWSKI\nA., ROMDHANI S., ET AL.: 3d morphable face models\u2014past, present,\nand future. ACM Trans. Graph. (2020). 17\n[FAKD23]\nFRIDMAN R., ABECASIS A., KASTEN Y., DEKEL T.: Sce-\nnescape: Text-driven consistent scene generation.\narXiv preprint\narXiv:2302.01133 (2023). 18\n[FBD\u221719]\nFLYNN J., BROXTON M., DEBEVEC P., DUVALL M., FYFFE\nG., OVERBECK R., SNAVELY N., TUCKER R.: Deepview: View synthe-\nsis with learned gradient descent. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (2019), pp. 2367\u2013\n2376. 5\n[FJG\u221721]\nFU H., JIA R., GAO L., GONG M., ZHAO B., MAYBANK S.,\nTAO D.: 3d-future: 3d furniture shape with texture. International Journal\nof Computer Vision 129 (2021), 3313\u20133337. 14, 15\n[FJW\u221722]\nFAN Z., JIANG Y., WANG P., GONG X., XU D., WANG Z.:\nUnified implicit neural stylization. In ECCV (2022), Springer. 19\n[FKYT\u221722]\nFRIDOVICH-KEIL S., YU A., TANCIK M., CHEN Q.,\nRECHT B., KANAZAWA A.: Plenoxels: Radiance fields without neu-\nral networks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (2022), pp. 5501\u20135510. 6\n[FLJ\u221722]\nFU J., LI S., JIANG Y., LIN K.-Y., QIAN C., LOY C. C., WU\nW., LIU Z.: Stylegan-human: A data-centric odyssey of human gener-\nation. In European Conference on Computer Vision (2022), Springer,\npp. 1\u201319. 14, 15\n[FXOT22]\nFU Q., XU Q., ONG Y. S., TAO W.: Geo-NeuS: Geometry-\nconsistent neural implicit surfaces learning for multi-view reconstruc-\ntion. Advances in Neural Information Processing Systems 35 (2022),\n3403\u20133416. 6\n[GAA\u221722]\nGAL\nR.,\nALALUF\nY.,\nATZMON\nY.,\nPATASHNIK\nO.,\nBERMANO A. H., CHECHIK G., COHEN-OR D.: An image is worth\none word: Personalizing text-to-image generation using textual inver-\nsion. arXiv preprint arXiv:2208.01618 (2022). 12\n[GCL\u221721]\nGUO Y., CHEN K., LIANG S., LIU Y.-J., BAO H., ZHANG J.:\nAd-nerf: Audio driven neural radiance fields for talking head synthesis.\nIn Proceedings of the IEEE/CVF International Conference on Computer\nVision (2021), pp. 5784\u20135794. 5\n[GCS\u221720]\nGENOVA K., COLE F., SUD A., SARNA A., FUNKHOUSER\nT.:\nLocal deep implicit functions for 3d shape.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(2020), pp. 4857\u20134866. 6\n[GCV\u221719]\nGENOVA K., COLE F., VLASIC D., SARNA A., FREEMAN\nW. T., FUNKHOUSER T.: Learning shape templates with structured im-\nplicit functions. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (2019), pp. 7154\u20137164. 6\n[GEB16]\nGATYS L. A., ECKER A. S., BETHGE M.: Image style transfer\nusing convolutional neural networks. In CVPR (2016). 19\n[GII\u221721]\nGRIGOREV A., ISKAKOV K., IANINA A., BASHIROV R., ZA-\nKHARKIN I., VAKHITOV A., LEMPITSKY V.: Stylepeople: A generative\nmodel of fullbody human avatars. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (2021), pp. 5151\u2013\n5160. 16\n[GKG\u221723]\nGIEBENHAIN S., KIRSCHSTEIN T., GEORGOPOULOS M.,\nR\u00dcNZ M., AGAPITO L., NIESSNER M.: Learning neural parametric\nhead models. In CVPR (2023). 17\n[GKJ\u221721]\nGARBIN S. J., KOWALSKI M., JOHNSON M., SHOTTON J.,\nVALENTIN J.: Fastnerf: High-fidelity neural rendering at 200fps. In\nProceedings of the IEEE/CVF International Conference on Computer\nVision (2021), pp. 14346\u201314355. 5\n[GLWT22]\nGU J., LIU L., WANG P., THEOBALT C.:\nStyleNeRF: A\nstyle-based 3d-aware generator for high-resolution image synthesis. In\nInt. Conf. Learn. Represent. (2022). 17\n[GLZ\u221723]\nGAO X., LI X., ZHANG C., ZHANG Q., CAO Y., SHAN\nY., QUAN L.:\nContex-human: Free-view rendering of human from\na single image with texture-consistent synthesis.\narXiv preprint\narXiv:2311.17123 (2023). 16\n[GMW17]\nGADELHA M., MAJI S., WANG R.: 3d shape induction from\n2d views of multiple objects. In 2017 International Conference on 3D\nVision (3DV) (2017), IEEE, pp. 402\u2013411. 9, 15\n[GNL\u221723]\nGE S., NAH S., LIU G., POON T., TAO A., CATANZARO B.,\nJACOBS D., HUANG J.-B., LIU M.-Y., BALAJI Y.: Preserve your own\ncorrelation: A noise prior for video diffusion models. In ICCV (2023). 2\n[GPAM\u221714]\nGOODFELLOW I., POUGET-ABADIE J., MIRZA M., XU\nB., WARDE-FARLEY D., OZAIR S., COURVILLE A., BENGIO Y.: Gen-\nerative adversarial nets. Advances in neural information processing sys-\ntems 27 (2014). 2, 6, 7\n[GPL\u221722]\nGRASSAL P.-W., PRINZLER M., LEISTNER T., ROTHER C.,\nNIESSNER M., THIES J.: Neural head avatars from monocular RGB\nvideos. In CVPR (2022). 17\n[GSW\u221721]\nGUI J., SUN Z., WEN Y., TAO D., YE J.: A review on gen-\nerative adversarial networks: Algorithms, theory, and applications. IEEE\ntransactions on knowledge and data engineering 35, 4 (2021), 3313\u2013\n3332. 3\n[GSW\u221722]\nGAO J., SHEN T., WANG Z., CHEN W., YIN K., LI D.,\nLITANY O., GOJCIC Z., FIDLER S.: GET3D: A generative model of\nhigh quality 3d textured shapes learned from images. In NeurIPS (2022).\n9, 18, 19\n[GTZN21]\nGAFNI G., THIES J., ZOLLHOFER M., NIESSNER M.: Dy-\nnamic neural radiance fields for monocular 4D facial avatar reconstruc-\ntion. In CVPR (2021). 17\n[GWH\u221720]\nGUO Y., WANG H., HU Q., LIU H., LIU L., BENNAMOUN\nM.: Deep learning for 3d point clouds: A survey. IEEE transactions on\npattern analysis and machine intelligence 43, 12 (2020), 4338\u20134364. 3\n[GWY\u221721]\nGAO L., WU T., YUAN Y.-J., LIN M.-X., LAI Y.-K.,\nZHANG H.:\nTm-net: Deep generative networks for textured meshes.\nACM Transactions on Graphics (TOG) 40, 6 (2021), 1\u201315. 11\n[GXN\u221723]\nGUPTA A., XIONG W., NIE Y., JONES I., O \u02d8GUZ B.: 3dgen:\nTriplane latent diffusion for textured mesh generation. arXiv preprint\narXiv:2303.05371 (2023). 7\n[GYW\u221719a]\nGAO L., YANG J., WU T., YUAN Y.-J., FU H., LAI Y.-\nK., ZHANG H.: Sdm-net: Deep generative network for structured de-\nformable mesh. ACM Transactions on Graphics (TOG) 38, 6 (2019),\n1\u201315. 7\n[GYW\u221719b]\nGAO L., YANG J., WU T., YUAN Y.-J., FU H., LAI Y.-\nK., ZHANG H.: Sdm-net: Deep generative network for structured de-\nformable mesh. ACM Transactions on Graphics (TOG) 38, 6 (2019),\n1\u201315. 11\n[GZX\u221722]\nGAO X., ZHONG C., XIANG J., HONG Y., GUO Y., ZHANG\nJ.:\nReconstructing personalized semantic facial nerf models from\nmonocular video. ACM Trans. Graph. (2022). 17\n[HB17]\nHUANG X., BELONGIE S.: Arbitrary style transfer in real-time\nwith adaptive instance normalization. In ICCV (2017). 19\n[HCL\u221722]\nHONG F., CHEN Z., LAN Y., PAN L., LIU Z.:\nEva3d:\nCompositional 3d human generation from 2d image collections. arXiv\npreprint arXiv:2210.04888 (2022). 15, 16\n[HCO\u221723]\nH\u00d6LLEIN L., CAO A., OWENS A., JOHNSON J., NIESSNER\nM.: Text2room: Extracting textured 3d meshes from 2d text-to-image\nmodels. arXiv preprint arXiv:2303.11989 (2023). 18\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n25\n[HHP\u221723]\nHU S., HONG F., PAN L., MEI H., YANG L., LIU Z.:\nSherf: Generalizable human nerf from a single image. arXiv preprint\narXiv:2303.12791 (2023). 16\n[HHY\u221722]\nHUANG Y.-H., HE Y., YUAN Y.-J., LAI Y.-K., GAO L.:\nStylizednerf: consistent 3d scene stylization as stylized nerf via 2d-3d\nmutual learning. In CVPR (2022). 19\n[HJA20]\nHO J., JAIN A., ABBEEL P.: Denoising diffusion probabilistic\nmodels. Advances in neural information processing systems 33 (2020),\n6840\u20136851. 1, 2, 6, 9\n[HLA\u221719]\nHU Y., LI T.-M., ANDERSON L., RAGAN-KELLEY J., DU-\nRAND F.: Taichi: a language for high-performance computation on spa-\ntially sparse data structures. ACM Transactions on Graphics (TOG) 38,\n6 (2019), 1\u201316. 5\n[HLHF22]\nHUI K.-H., LI R., HU J., FU C.-W.: Neural wavelet-domain\ndiffusion for 3d shape generation. In SIGGRAPH Asia 2022 Conference\nPapers (2022), pp. 1\u20139. 10, 18\n[HMR19a]\nHENZLER P., MITRA N. J., RITSCHEL T.: Escaping plato\u2019s\ncave: 3D shape from adversarial rendering. In ICCV (2019). 9, 15, 18\n[HMR19b]\nHENZLER P., MITRA N. J., RITSCHEL T.: Escaping plato\u2019s\ncave: 3d shape from adversarial rendering. In ICCV (2019). 17, 18\n[HPX\u221722]\nHONG Y., PENG B., XIAO H., LIU L., ZHANG J.: Head-\nNeRF: A real-time nerf-based parametric head model. In CVPR (2022).\n5\n[HRBP21]\nHU R., RAVI N., BERG A. C., PATHAK D.:\nWorldsheet:\nWrapping the world in a 3d sheet for view synthesis from a single image.\nIn Proceedings of the IEEE/CVF International Conference on Computer\nVision (2021), pp. 12528\u201312537. 18\n[HRL\u221721]\nHENZLER P., REIZENSTEIN J., LABATUT P., SHAPOVALOV\nR., RITSCHEL T., VEDALDI A., NOVOTNY D.: Unsupervised learn-\ning of 3d object categories from videos in the wild. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(2021), pp. 4700\u20134709. 15\n[HSG\u221722]\nHO J., SALIMANS T., GRITSENKO A., CHAN W., NOROUZI\nM., FLEET D. J.: Video diffusion models. In NeurIPS (2022). 2\n[HSZ\u221723]\nHUANG X., SHAO R., ZHANG Q., ZHANG H., FENG Y.,\nLIU Y., WANG Q.:\nHumannorm: Learning normal diffusion model\nfor high-quality and realistic 3d human generation.\narXiv preprint\narXiv:2310.01406 (2023). 16\n[HTE\u221723]\nHAQUE A., TANCIK M., EFROS A. A., HOLYNSKI A.,\nKANAZAWA A.: Instruct-NeRF2NeRF: Editing 3d scenes with instruc-\ntions. In ICCV (2023). 19, 20\n[HTS\u221721]\nHUANG H.-P., TSENG H.-Y., SAINI S., SINGH M., YANG\nM.-H.: Learning to stylize novel views. In ICCV (2021). 19\n[HWZ\u221723]\nHUANG Y., WANG J., ZENG A., CAO H., QI X., SHI Y.,\nZHA Z.-J., ZHANG L.: Dreamwaltz: Make a scene with complex 3d\nanimatable avatars. arXiv preprint arXiv:2305.12529 (2023). 16\n[HYL\u221723]\nHUANG Y., YI H., LIU W., WANG H., WU B., WANG W.,\nLIN B., ZHANG D., CAI D.: One-shot implicit animatable avatars with\nmodel-based priors. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (2023), pp. 8974\u20138985. 15, 16\n[HYX\u221723]\nHUANG Y., YI H., XIU Y., LIAO T., TANG J., CAI D.,\nTHIES J.: Tech: Text-guided reconstruction of lifelike clothed humans.\narXiv preprint arXiv:2308.08545 (2023). 15, 16\n[HZF\u221722]\nHUANG X., ZHANG Q., FENG Y., LI H., WANG X., WANG\nQ.: Hdr-nerf: High dynamic range neural radiance fields. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion (2022), pp. 18398\u201318408. 5\n[HZF\u221723a]\nHUANG X., ZHANG Q., FENG Y., LI H., WANG Q.: In-\nverting the imaging process by learning an implicit camera model. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (2023), pp. 21456\u201321465. 5\n[HZF\u221723b]\nHUANG X., ZHANG Q., FENG Y., LI X., WANG X., WANG\nQ.: Local implicit ray function for generalizable radiance field represen-\ntation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (2023), pp. 97\u2013107. 5\n[HZP\u221722]\nHONG F., ZHANG M., PAN L., CAI Z., YANG L., LIU Z.:\nAvatarclip: Zero-shot text-driven generation and animation of 3d avatars.\narXiv preprint arXiv:2205.08535 (2022). 16\n[ID18]\nINSAFUTDINOV E., DOSOVITSKIY A.: Unsupervised learning\nof shape and pose with differentiable point clouds. Advances in neural\ninformation processing systems 31 (2018). 4\n[JCL\u221722]\nJIANG K., CHEN S.-Y., LIU F.-L., FU H., GAO L.: NeRF-\nFaceEditing: Disentangled face editing in neural radiance fields. In SIG-\nGRAPH Asia Conference Papers (2022). 18, 19\n[JJW\u221723]\nJIANG S., JIANG H., WANG Z., LUO H., CHEN W., XU L.:\nHumangen: Generating human radiance fields with explicit priors. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (2023), pp. 12543\u201312554. 16\n[JKK\u221723]\nJAMBON C., KERBL B., KOPANAS G., DIOLATZIS S., DRET-\nTAKIS G., LEIMK\u00dcHLER T.: Nerfshop: Interactive editing of neural ra-\ndiance fields. In Proceedings of the ACM on Computer Graphics and\nInteractive Techniques (2023). 19, 20\n[JLF22]\nJOHARI M. M., LEPOITTEVIN Y., FLEURET F.:\nGeoN-\neRF: Generalizing nerf with geometry priors.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(2022), pp. 18365\u201318375. 5\n[JMB\u221722]\nJAIN A., MILDENHALL B., BARRON J. T., ABBEEL P.,\nPOOLE B.: Zero-shot text-guided object generation with dream fields.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (2022), pp. 867\u2013876. 18\n[JN23]\nJUN H., NICHOL A.: Shap-e: Generating conditional 3d implicit\nfunctions. arXiv preprint arXiv:2305.02463 (2023). 10, 12\n[JWZ\u221723]\nJIANG R., WANG C., ZHANG J., CHAI M., HE M., CHEN\nD., LIAO J.:\nAvatarcraft: Transforming text into neural human\navatars with parameterized shape and pose control.\narXiv preprint\narXiv:2303.17606 (2023). 15, 16\n[KAZ\u221723]\nKOLOTOUROS N., ALLDIECK T., ZANFIR A., BAZAVAN\nE. G., FIERARU M., SMINCHISESCU C.: Dreamhuman: Animatable\n3d avatars from text. arXiv preprint arXiv:2306.09329 (2023). 15, 16\n[KBM\u221720]\nKATO H., BEKER D., MORARIU M., ANDO T., MATSUOKA\nT., KEHL W., GAIDON A.: Differentiable rendering: A survey. arXiv\npreprint arXiv:2006.12057 (2020). 3\n[KBV20]\nKLOKOV R., BOYER E., VERBEEK J.: Discrete point flow\nnetworks for efficient point cloud generation. In European Conference\non Computer Vision (2020), Springer, pp. 694\u2013710. 11\n[KDJ\u221723]\nKWAK J.-G., DONG E., JIN Y., KO H., MAHAJAN S., YI\nK. M.: Vivid-1-to-3: Novel view synthesis with video diffusion models.\narXiv preprint arXiv:2312.01305 (2023).\n[KDSB22]\nKULH\u00c1NEK J., DERNER E., SATTLER T., BABU\u0160KA R.:\nViewformer: Nerf-free neural rendering from few images using trans-\nformers. In European Conference on Computer Vision (2022), Springer,\npp. 198\u2013216. 13, 14\n[KFH\u221722]\nKERR J., FU L., HUANG H., AVIGAL Y., TANCIK M., ICH-\nNOWSKI J., KANAZAWA A., GOLDBERG K.: Evo-nerf: Evolving nerf\nfor sequential robot grasping of transparent objects. In 6th Annual Con-\nference on Robot Learning (2022). 5\n[KKL\u221723]\nKIM B., KWON P., LEE K., LEE M., HAN S., KIM D., JOO\nH.: Chupa: Carving 3d clothed humans from skinned shape priors us-\ning 2d diffusion probabilistic models. arXiv preprint arXiv:2305.11870\n(2023). 15, 16\n[KKLD23]\nKERBL B., KOPANAS G., LEIMK\u00dcHLER T., DRETTAKIS G.:\n3d gaussian splatting for real-time radiance field rendering. ACM Trans-\nactions on Graphics (ToG) 42, 4 (2023), 1\u201314. 1, 4, 12\n26\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n[KKR18]\nKNYAZ V. A., KNIAZ V. V., REMONDINO F.: Image-to-voxel\nmodel translation with conditional adversarial networks. In Proceedings\nof the European Conference on Computer Vision (ECCV) Workshops\n(2018), pp. 0\u20130. 7\n[KLA19]\nKARRAS T., LAINE S., AILA T.:\nA style-based generator\narchitecture for generative adversarial networks.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition\n(2019), pp. 4401\u20134410. 14, 15, 17\n[KLA\u221720]\nKARRAS T., LAINE S., AITTALA M., HELLSTEN J., LEHTI-\nNEN J., AILA T.: Analyzing and improving the image quality of Style-\nGAN. In CVPR (2020). 16, 17\n[KLK\u221720]\nKIM H., LEE H., KANG W. H., LEE J. Y., KIM N. S.: Soft-\nflow: Probabilistic framework for normalizing flow on manifolds. Ad-\nvances in Neural Information Processing Systems 33 (2020), 16388\u2013\n16397. 11\n[KLY\u221721]\nKOH J. Y., LEE H., YANG Y., BALDRIDGE J., ANDERSON\nP.: Pathdreamer: A world model for indoor navigation. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision (2021),\npp. 14738\u201314748. 14\n[KMS22]\nKOBAYASHI S., MATSUMOTO E., SITZMANN V.: Decompos-\ning nerf for editing via feature field distillation. In NeurIPS (2022). 19,\n20\n[KNH\u221722]\nKHAN S., NASEER M., HAYAT M., ZAMIR S. W., KHAN\nF. S., SHAH M.: Transformers in vision: A survey. ACM computing\nsurveys (CSUR) 54, 10s (2022), 1\u201341. 3\n[KPHL17]\nKUSNER M. J., PAIGE B., HERN\u00c1NDEZ-LOBATO J. M.:\nGrammar variational autoencoder. In International conference on ma-\nchine learning (2017), PMLR, pp. 1945\u20131954. 2, 6\n[KPLD21]\nKOPANAS G., PHILIP J., LEIMK\u00dcHLER T., DRETTAKIS G.:\nPoint-based neural rendering with per-view optimization. In Computer\nGraphics Forum (2021), vol. 40, Wiley Online Library, pp. 29\u201343. 4\n[KPWS22]\nKALISCHEK N., PETERS T., WEGNER J. D., SCHINDLER\nK.: Tetrahedral diffusion models for 3d shape generation. arXiv preprint\narXiv:2211.13220 (2022). 10\n[KQG\u221723]\nKIRSCHSTEIN T., QIAN S., GIEBENHAIN S., WALTER T.,\nNIESSNER M.: NeRSemble: Multi-view radiance field reconstruction of\nhuman heads. ACM Trans. Graph. (2023). 17\n[KSZ\u221721]\nKOSIOREK A. R., STRATHMANN H., ZORAN D., MORENO\nP., SCHNEIDER R., MOKR\u00c1 S., REZENDE D. J.: Nerf-vae: A geometry\naware 3d scene generative model. In ICML (2021). 11\n[KUH18]\nKATO H., USHIKU Y., HARADA T.: Neural 3d mesh renderer.\nIn Proceedings of the IEEE conference on computer vision and pattern\nrecognition (2018), pp. 3907\u20133916. 5\n[KVNM23]\nKARNEWAR A., VEDALDI A., NOVOTNY D., MITRA N. J.:\nHolodiffusion: Training a 3d diffusion model using 2d images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (2023), pp. 18423\u201318433. 7\n[KW13]\nKINGMA D. P., WELLING M.:\nAuto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114 (2013). 2, 6, 11\n[KWKT15]\nKULKARNI T. D., WHITNEY W. F., KOHLI P., TENEN-\nBAUM J.: Deep convolutional inverse graphics network. Advances in\nneural information processing systems 28 (2015). 14\n[KXD12]\nKASPER A., XUE Z., DILLMANN R.: The kit object mod-\nels database: An object model database for object recognition, localiza-\ntion and manipulation in service robotics. The International Journal of\nRobotics Research 31, 8 (2012), 927\u2013934. 14\n[KYLH21]\nKIM J., YOO J., LEE J., HONG S.: Setvae: Learning hier-\narchical composition for generative modeling of set-structured data. In\nCVPR (2021). 11\n[LB14]\nLOPER M. M., BLACK M. J.: Opendr: An approximate differen-\ntiable renderer. In Computer Vision\u2013ECCV 2014: 13th European Con-\nference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part\nVII 13 (2014), Springer, pp. 154\u2013169. 5\n[LBRF11]\nLAI K., BO L., REN X., FOX D.: A large-scale hierarchical\nmulti-view rgb-d object dataset. In 2011 IEEE international conference\non robotics and automation (2011), IEEE, pp. 1817\u20131824. 15\n[LC22]\nLEE H.-H., CHANG A. X.: Understanding pure clip guidance\nfor voxel grid nerf models. arXiv preprint arXiv:2209.15172 (2022). 18\n[LCCT23]\nLI W., CHEN R., CHEN X., TAN P.: Sweetdreamer: Aligning\ngeometric priors in 2d diffusion for consistent text-to-3d. arXiv preprint\narXiv:2310.02596 (2023). 12\n[LDS\u221723]\nLI Y., DOU Y., SHI Y., LEI Y., CHEN X., ZHANG Y., ZHOU\nP., NI B.: FocalDreamer: Text-driven 3d editing via focal-fusion assem-\nbly. arXiv preprint arXiv:2308.10608 (2023). 19, 20\n[LDZL23]\nLI M., DUAN Y., ZHOU J., LU J.: Diffusion-sdf: Text-to-\nshape via voxelized diffusion. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (2023), pp. 12642\u2013\n12651. 10, 18, 19\n[LFB\u221723]\nLIU Z., FENG Y., BLACK M. J., NOWROUZEZAHRAI D.,\nPAULL L., LIU W.: Meshdiffusion: Score-based generative 3d mesh\nmodeling. arXiv preprint arXiv:2303.08133 (2023). 10\n[LFLSY\u221723]\nLIN G., FENG-LIN L., SHU-YU C., KAIWEN J., CHUN-\nPENG L., LAI Y., HONGBO F.: SketchFaceNeRF: Sketch-based facial\ngeneration and editing in neural radiance fields.\nACM Trans. Graph.\n(2023). 18, 19\n[LFS\u221721]\nLI J., FENG Z., SHE Q., DING H., WANG C., LEE G. H.:\nMine: Towards continuous depth mpi with nerf for novel view synthesis.\nIn Proceedings of the IEEE/CVF International Conference on Computer\nVision (2021), pp. 12578\u201312588. 5\n[LGL\u221723]\nLONG X., GUO Y.-C., LIN C., LIU Y., DOU Z., LIU L., MA\nY., ZHANG S.-H., HABERMANN M., THEOBALT C., ET AL.: Won-\nder3d: Single image to 3d using cross-domain diffusion. arXiv preprint\narXiv:2310.15008 (2023). 13\n[LGT\u221723]\nLIN C.-H., GAO J., TANG L., TAKIKAWA T., ZENG X.,\nHUANG X., KREIS K., FIDLER S., LIU M.-Y., LIN T.-Y.: Magic3d:\nHigh-resolution text-to-3d content creation.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(2023), pp. 300\u2013309. 12, 18\n[LGZL\u221720]\nLIU L., GU J., ZAW LIN K., CHUA T.-S., THEOBALT C.:\nNeural sparse voxel fields. Advances in Neural Information Processing\nSystems 33 (2020), 15651\u201315663. 6\n[LH21]\nLUO S., HU W.:\nDiffusion probabilistic models for 3d point\ncloud generation. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (2021), pp. 2837\u20132845. 10, 18\n[LHG\u221723]\nLIN Y., HAN H., GONG C., XU Z., ZHANG Y., LI X.: Con-\nsistent123: One image to highly consistent 3d asset using case-aware\ndiffusion priors. arXiv preprint arXiv:2309.17261 (2023). 13\n[LHR\u221721]\nLIU L., HABERMANN M., RUDNEV V., SARKAR K., GU J.,\nTHEOBALT C.: Neural actor: Neural free-view synthesis of human actors\nwith pose control. ACM transactions on graphics (TOG) 40, 6 (2021),\n1\u201316. 5\n[Lin68]\nLINDENMAYER A.: Mathematical models for cellular interac-\ntions in development i. filaments with one-sided inputs. Journal of theo-\nretical biology 18, 3 (1968), 280\u2013299. 13\n[LKL18]\nLIN C.-H., KONG C., LUCEY S.:\nLearning efficient point\ncloud generation for dense 3d object reconstruction. In proceedings of\nthe AAAI Conference on Artificial Intelligence (2018), vol. 32. 4\n[LLCL19]\nLIU S., LI T., CHEN W., LI H.:\nSoft rasterizer: A dif-\nferentiable renderer for image-based 3d reasoning. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (2019),\npp. 7708\u20137717. 5\n[LLF\u221723]\nLI Y., LIN Z.-H., FORSYTH D., HUANG J.-B., WANG S.:\nClimateNeRF: Physically-based neural rendering for extreme climate\nsynthesis. In ICCV (2023). 19\n[LLL\u221714]\nLI B., LU Y., LI C., GODIL A., SCHRECK T., AONO M.,\nCHEN Q., CHOWDHURY N. K., FANG B., FURUYA T., ET AL.:\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n27\nShrec\u201914 track: Large scale comprehensive 3d shape retrieval. In Eu-\nrographics Workshop on 3D Object Retrieval (2014), vol. 2, . 14\n[LLQ\u221716]\nLIU Z., LUO P., QIU S., WANG X., TANG X.: Deepfashion:\nPowering robust clothes recognition and retrieval with rich annotations.\nIn Proceedings of the IEEE conference on computer vision and pattern\nrecognition (2016), pp. 1096\u20131104. 14, 15\n[LLWT15]\nLIU Z., LUO P., WANG X., TANG X.: Deep learning face at-\ntributes in the wild. In Proceedings of the IEEE international conference\non computer vision (2015), pp. 3730\u20133738. 15\n[LLZ\u221723]\nLIU Y., LIN C., ZENG Z., LONG X., LIU L., KOMURA T.,\nWANG W.: Syncdreamer: Generating multiview-consistent images from\na single-view image. arXiv preprint arXiv:2309.03453 (2023). 12, 13\n[LLZL21]\nLUO A., LI T., ZHANG W.-H., LEE T. S.: Surfgen: Adver-\nsarial 3d shape synthesis with explicit surface discriminators. In Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision\n(2021), pp. 16238\u201316248. 7, 9\n[LMTL21]\nLIN C.-H., MA W.-C., TORRALBA A., LUCEY S.: Barf:\nBundle-adjusting neural radiance fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (2021), pp. 5741\u20135751. 5\n[LMY\u221723]\nLI Y., MA C., YAN Y., ZHU W., YANG X.: 3d-aware face\nswapping. In CVPR (2023). 19\n[LPT13]\nLIM J. J., PIRSIAVASH H., TORRALBA A.: Parsing ikea ob-\njects: Fine pose estimation. In Proceedings of the IEEE international\nconference on computer vision (2013), pp. 2992\u20132999. 14\n[LSC\u221722]\nLEVIS A., SRINIVASAN P. P., CHAEL A. A., NG R.,\nBOUMAN K. L.: Gravitationally lensed black hole emission tomogra-\nphy. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (2022), pp. 19841\u201319850. 5\n[LSMG20]\nLIAO Y., SCHWARZ K., MESCHEDER L., GEIGER A.: To-\nwards unsupervised learning of generative models for 3d controllable\nimage synthesis. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition (2020), pp. 5871\u20135880. 15\n[LSS\u221721]\nLOMBARDI S., SIMON T., SCHWARTZ G., ZOLLHOEFER M.,\nSHEIKH Y., SARAGIH J.: Mixture of volumetric primitives for efficient\nneural rendering. ACM Trans. Graph. (2021). 17\n[LSSS18]\nLOMBARDI S., SARAGIH J., SIMON T., SHEIKH Y.: Deep\nappearance models for face rendering. ACM Trans. Graph. (2018). 17\n[LSZ\u221722]\nLI T., SLAVCHEVA M., ZOLLHOEFER M., GREEN S., LASS-\nNER C., KIM C., SCHMIDT T., LOVEGROVE S., GOESELE M., NEW-\nCOMBE R., ET AL.: Neural 3d video synthesis from multi-view video.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (2022), pp. 5521\u20135531. 5\n[LTJ18]\nLIU H.-T. D., TAO M., JACOBSON A.: Paparazzi: surface edit-\ning by way of multi-view image processing. ACM Trans. Graph. 37, 6\n(2018), 221\u20131. 5\n[LTJ\u221721]\nLIU A., TUCKER R., JAMPANI V., MAKADIA A., SNAVELY\nN., KANAZAWA A.: Infinite nature: Perpetual view generation of natural\nscenes from a single image. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (2021), pp. 14458\u201314467. 14\n[LTZ\u221723]\nLI J., TAN H., ZHANG K., XU Z., LUAN F., XU Y., HONG\nY., SUNKAVALLI K., SHAKHNAROVICH G., BI S.: Instant3d: Fast text-\nto-3d with sparse-view generation and large reconstruction model. arXiv\npreprint arXiv:2311.06214 (2023). 1, 2\n[LWA\u221723]\nLYU Z., WANG J., AN Y., ZHANG Y., LIN D., DAI B.: Con-\ntrollable mesh generation through sparse latent point diffusion models.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (2023), pp. 271\u2013280. 10\n[LWC\u221723]\nLI Z., WANG Q., COLE F., TUCKER R., SNAVELY N.: Dyni-\nbar: Neural dynamic image-based rendering.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(2023), pp. 4273\u20134284. 5\n[LWQF22]\nLIU Z., WANG Y., QI X., FU C.-W.: Towards implicit text-\nguided 3d shape generation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (2022), pp. 17896\u2013\n17906. 18\n[LWSK22]\nLI\nZ.,\nWANG\nQ.,\nSNAVELY\nN.,\nKANAZAWA\nA.:\nInfinitenature-zero: Learning perpetual view generation of natural\nscenes from single images. In European Conference on Computer Vision\n(2022), Springer, pp. 515\u2013534. 14\n[LWVH\u221723]\nLIU R., WU R., VAN HOORICK B., TOKMAKOV P., ZA-\nKHAROV S., VONDRICK C.: Zero-1-to-3: Zero-shot one image to 3d\nobject. arXiv preprint arXiv:2303.11328 (2023). 1, 7, 12, 13, 18\n[LXC\u221721]\nLI J., XU C., CHEN Z., BIAN S., YANG L., LU C.: Hybrik: A\nhybrid analytical-neural inverse kinematics solution for 3d human pose\nand shape estimation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition (2021), pp. 3383\u20133393. 16\n[LXJ\u221723]\nLIU M., XU C., JIN H., CHEN L., XU Z., SU H., ET AL.:\nOne-2-3-45: Any single image to 3d mesh in 45 seconds without per-\nshape optimization. arXiv preprint arXiv:2306.16928 (2023). 10, 12\n[LXM\u221720]\nLIN K.-E., XU Z., MILDENHALL B., SRINIVASAN P. P.,\nHOLD-GEOFFROY Y., DIVERDI S., SUN Q., SUNKAVALLI K., RA-\nMAMOORTHI R.: Deep multi depth panoramas for view synthesis. In\nEuropean Conference on Computer Vision (2020), Springer, pp. 328\u2013\n344. 5\n[LXZ\u221723]\nLORRAINE J., XIE K., ZENG X., LIN C.-H., TAKIKAWA\nT., SHARP N., LIN T.-Y., LIU M.-Y., FIDLER S., LUCAS J.: Att3d:\nAmortized text-to-3d object synthesis. arXiv preprint arXiv:2306.07349\n(2023). 15, 18\n[LYX\u221724]\nLIAO T., YI H., XIU Y., TANG J., HUANG Y., THIES J.,\nBLACK M. J.: TADA! text to animatable digital avatars. In 3DV (2024).\n15, 16, 19\n[LZ21]\nLASSNER C., ZOLLHOFER M.: Pulsar: Efficient sphere-based\nneural rendering. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (2021), pp. 1440\u20131449. 4\n[LZF\u221723]\nLIANG Z., ZHANG Q., FENG Y., SHAN Y., JIA K.:\nGs-\nir: 3d gaussian splatting for inverse rendering.\narXiv preprint\narXiv:2311.16473 (2023). 5\n[LZJ\u221722]\nLEI J., ZHANG Y., JIA K., ET AL.:\nTANGO: Text-driven\nphotorealistic and robust 3d stylization via lighting decomposition. In\nNeurIPS (2022). 11, 19\n[LZT\u221723]\nLIU X., ZHAN X., TANG J., SHAN Y., ZENG G., LIN D.,\nLIU X., LIU Z.: Humangaussian: Text-driven 3d human generation with\ngaussian splatting. arXiv preprint arXiv:2311.17061 (2023). 16\n[LZW\u221723]\nLI C., ZHANG C., WAGHWASE A., LEE L.-H., RAMEAU F.,\nYANG Y., BAE S.-H., HONG C. S.: Generative ai meets 3d: A survey\non text-to-3d in aigc era. arXiv preprint arXiv:2305.06131 (2023). 3\n[Man67]\nMANDELBROT B.: How long is the coast of britain? statistical\nself-similarity and fractional dimension. science 156, 3775 (1967), 636\u2013\n638. 13\n[Max95]\nMAX N.: Optical models for direct volume rendering. IEEE\nTransactions on Visualization and Computer Graphics 1, 2 (1995), 99\u2013\n108. 5\n[MBOL\u221722]\nMICHEL O., BAR-ON R., LIU R., BENAIM S., HANOCKA\nR.:\nText2mesh: Text-driven neural stylization for meshes.\nIn CVPR\n(2022). 19, 20\n[MBRS\u221721]\nMARTIN-BRUALLA R., RADWAN N., SAJJADI M. S.,\nBARRON J. T., DOSOVITSKIY A., DUCKWORTH D.: Nerf in the wild:\nNeural radiance fields for unconstrained photo collections.\nIn CVPR\n(2021), pp. 7210\u20137219. 5\n[MCL20]\nMORRISON D., CORKE P., LEITNER J.:\nEgad! an evolved\ngrasping analysis dataset for diversity and reproducibility in robotic ma-\nnipulation. IEEE Robotics and Automation Letters 5, 3 (2020), 4368\u2013\n4375. 14\n28\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n[MCST22a]\nMITTAL P., CHENG Y.-C., SINGH M., TULSIANI S.: Au-\ntosdf: Shape priors for 3d completion, reconstruction and generation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (2022), pp. 306\u2013315. 6, 7\n[MCST22b]\nMITTAL P., CHENG Y.-C., SINGH M., TULSIANI S.: Au-\ntosdf: Shape priors for 3d completion, reconstruction and generation. In\nCVPR (2022). 10\n[MESK22]\nM\u00dcLLER T., EVANS A., SCHIED C., KELLER A.: Instant\nneural graphics primitives with a multiresolution hash encoding. ACM\nTransactions on Graphics (ToG) 41, 4 (2022), 1\u201315. 1, 6\n[Mid]\nMIDJOURNEY:\nMidjourney.\nhttps://www.midjourney.\ncom/. 2\n[MKLRV23]\nMELAS-KYRIAZI\nL.,\nLAINA\nI.,\nRUPPRECHT\nC.,\nVEDALDI A.: Realfusion: 360deg reconstruction of any object from a\nsingle image. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (2023), pp. 8446\u20138455. 12, 18\n[MKXBP22]\nMOHAMMAD KHALID N., XIE T., BELILOVSKY E.,\nPOPA T.: Clip-mesh: Generating textured meshes from text using pre-\ntrained image-text models. In SIGGRAPH Asia 2022 conference papers\n(2022), pp. 1\u20138. 18\n[MLL\u221722a]\nMA L., LI X., LIAO J., WANG X., ZHANG Q., WANG J.,\nSANDER P. V.: Neural parameterization for dynamic human head edit-\ning. ACM Transactions on Graphics (TOG) 41, 6 (2022), 1\u201315. 19\n[MLL\u221722b]\nMA L., LI X., LIAO J., ZHANG Q., WANG X., WANG J.,\nSANDER P. V.: Deblur-nerf: Neural radiance fields from blurry images.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (2022), pp. 12861\u201312870. 5\n[MM82]\nMANDELBROT B. B., MANDELBROT B. B.: The fractal geom-\netry of nature, vol. 1. WH freeman New York, 1982. 13\n[MPS\u221723]\nMIKAEILI A., PEREL O., SAFAEE M., COHEN-OR D.,\nMAHDAVI-AMIRI A.: SKED: Sketch-guided text-based 3d editing. In\nICCV (2023). 19, 20\n[MRP\u221723a]\nMETZER G., RICHARDSON E., PATASHNIK O., GIRYES\nR., COHEN-OR D.:\nLatent-nerf for shape-guided generation of 3d\nshapes and textures. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (2023), pp. 12663\u201312673. 12\n[MRP\u221723b]\nMETZER G., RICHARDSON E., PATASHNIK O., GIRYES\nR., COHEN-OR D.: Latent-NeRF for shape-guided generation of 3d\nshapes and textures. In CVPR (2023). 18, 19\n[MS15]\nMATURANA D., SCHERER S.: Voxnet: A 3d convolutional neu-\nral network for real-time object recognition. In 2015 IEEE/RSJ interna-\ntional conference on intelligent robots and systems (IROS) (2015), IEEE,\npp. 922\u2013928. 6\n[MSOC\u221719]\nMILDENHALL B., SRINIVASAN P. P., ORTIZ-CAYON R.,\nKALANTARI N. K., RAMAMOORTHI R., NG R., KAR A.: Local light\nfield fusion: Practical view synthesis with prescriptive sampling guide-\nlines. ACM Transactions on Graphics (TOG) 38, 4 (2019), 1\u201314. 5\n[MSP\u221723]\nM\u00dcLLER N., SIDDIQUI Y., PORZI L., BULO S. R.,\nKONTSCHIEDER P., NIESSNER M.:\nDiffrf: Rendering-guided 3d ra-\ndiance field diffusion. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (2023), pp. 4328\u20134338. 10\n[MSS\u221721]\nMA S., SIMON T., SARAGIH J., WANG D., LI Y.,\nDE LA TORRE F., SHEIKH Y.: Pixel codec avatars. In CVPR (2021). 17\n[MST\u221720]\nMILDENHALL B., SRINIVASAN P. P., TANCIK M., BARRON\nJ. T., RAMAMOORTHI R., NG R.: Nerf: Representing scenes as neural\nradiance fields for view synthesis. In European conference on computer\nvision (2020), Springer, pp. 405\u2013421. 1, 5, 11\n[MYR\u221720]\nMA Q., YANG J., RANJAN A., PUJADES S., PONS-MOLL\nG., TANG S., BLACK M. J.: Learning to dress 3d people in genera-\ntive clothing. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (2020), pp. 6469\u20136478. 15, 16\n[MZS\u221723]\nMA Y., ZHANG X., SUN X., JI J., WANG H., JIANG G.,\nZHUANG W., JI R.: X-Mesh: Towards fast and accurate text-driven 3d\nstylization via dynamic textual guidance. In ICCV (2023). 19\n[ND21]\nNICHOL A. Q., DHARIWAL P.: Improved denoising diffusion\nprobabilistic models. In International Conference on Machine Learning\n(2021), PMLR, pp. 8162\u20138171. 2, 6\n[NDR\u221721]\nNICHOL A., DHARIWAL P., RAMESH A., SHYAM P.,\nMISHKIN P., MCGREW B., SUTSKEVER I., CHEN M.: Glide: Towards\nphotorealistic image generation and editing with text-guided diffusion\nmodels. arXiv preprint arXiv:2112.10741 (2021). 10\n[NDVZJ19]\nNIMIER-DAVID M., VICINI D., ZELTNER T., JAKOB W.:\nMitsuba 2: A retargetable forward and inverse renderer. ACM Transac-\ntions on Graphics (TOG) 38, 6 (2019), 1\u201317. 5\n[Neu66]\nNEUMANN J. V.: Theory of self-reproducing automata. Edited\nby Arthur W. Burks (1966). 13\n[NG21]\nNIEMEYER M., GEIGER A.: GIRAFFE: Representing scenes as\ncompositional generative neural feature fields. In CVPR (2021). 5, 7, 9,\n17\n[NGEB20a]\nNASH C., GANIN Y., ESLAMI S. A., BATTAGLIA P.: Poly-\ngen: An autoregressive generative model of 3d meshes. In International\nconference on machine learning (2020), PMLR, pp. 7220\u20137229. 7\n[NGEB20b]\nNASH C., GANIN Y., ESLAMI S. A., BATTAGLIA P.: Poly-\ngen: An autoregressive generative model of 3d meshes. In ICML (2020).\n10\n[NJD\u221722]\nNICHOL A., JUN H., DHARIWAL P., MISHKIN P., CHEN M.:\nPoint-e: A system for generating 3d point clouds from complex prompts.\narXiv preprint arXiv:2212.08751 (2022). 1, 7, 10, 12, 18\n[NKR\u221722]\nNAM G., KHLIFI M., RODRIGUEZ A., TONO A., ZHOU L.,\nGUERRERO P.: 3d-ldm: Neural implicit 3d shape generation with latent\ndiffusion models. arXiv preprint arXiv:2212.00842 (2022). 10, 18\n[NPLT\u221719]\nNGUYEN-PHUOC T., LI C., THEIS L., RICHARDT C.,\nYANG Y.-L.: HoloGAN: Unsupervised learning of 3d representations\nfrom natural images. In ICCV Workshop (2019). 9, 15, 17, 18\n[NPRM\u221720]\nNGUYEN-PHUOC T. H., RICHARDT C., MAI L., YANG\nY., MITRA N.: Blockgan: Learning 3d object-aware scene representa-\ntions from unlabelled images. Advances in neural information process-\ning systems 33 (2020), 6767\u20136778. 7, 9\n[NSLH22]\nNOGUCHI A., SUN X., LIN S., HARADA T.: Unsupervised\nlearning of efficient geometry-aware neural articulated representations.\nIn European Conference on Computer Vision (2022), Springer, pp. 597\u2013\n614. 16\n[OBB20]\nOSMAN A. A., BOLKART T., BLACK M. J.:\nStar: Sparse\ntrained articulated human body regressor. In Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020,\nProceedings, Part VI 16 (2020), Springer, pp. 598\u2013613. 16\n[OELS\u221722]\nOR-EL R., LUO X., SHAN M., SHECHTMAN E., PARK\nJ. J., KEMELMACHER-SHLIZERMAN I.: StyleSDF: High-resolution 3d-\nconsistent image and geometry generation. In CVPR (2022). 17\n[OMN\u221719]\nOECHSLE M., MESCHEDER L., NIEMEYER M., STRAUSS\nT., GEIGER A.: Texture fields: Learning texture representations in func-\ntion space. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (2019), pp. 4531\u20134540. 5\n[Ope]\nOPENAI: Dall-e 3. https://openai.com/dall-e-3. 2\n[PCPMMN21]\nPUMAROLA\nA.,\nCORONA\nE.,\nPONS-MOLL\nG.,\nMORENO-NOGUER F.:\nD-nerf: Neural radiance fields for dynamic\nscenes.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (2021), pp. 10318\u201310327. 5\n[PDW\u221721]\nPENG S., DONG J., WANG Q., ZHANG S., SHUAI Q., ZHOU\nX., BAO H.: Animatable neural radiance fields for modeling dynamic\nhuman bodies. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (2021), pp. 14314\u201314323. 5\n[Per85]\nPERLIN K.: An image synthesizer. ACM Siggraph Computer\nGraphics 19, 3 (1985), 287\u2013296. 13\n[Per02]\nPERLIN K.: Improving noise. In Proceedings of the 29th annual\nconference on Computer graphics and interactive techniques (2002),\npp. 681\u2013682. 13\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n29\n[PFS\u221719]\nPARK J. J., FLORENCE P., STRAUB J., NEWCOMBE R.,\nLOVEGROVE S.: Deepsdf: Learning continuous signed distance func-\ntions for shape representation. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition (2019), pp. 165\u2013174. 1,\n5\n[PGH\u221716]\nPU Y., GAN Z., HENAO R., YUAN X., LI C., STEVENS A.,\nCARIN L.: Variational autoencoder for deep learning of images, labels\nand captions.\nAdvances in neural information processing systems 29\n(2016). 2, 6\n[PJBM23]\nPOOLE B., JAIN A., BARRON J. T., MILDENHALL B.:\nDreamFusion: Text-to-3d using 2d diffusion. In Int. Conf. Learn. Repre-\nsent. (2023). 1, 7, 11, 12, 15, 18, 20, 21\n[PKHL21]\nPAVLLO D., KOHLER J., HOFMANN T., LUCCHI A.: Learn-\ning generative models of textured 3d meshes from real-world images.\nIn Proceedings of the IEEE/CVF International Conference on Computer\nVision (2021), pp. 13879\u201313889. 9\n[PRFS18]\nPARK K., REMATAS K., FARHADI A., SEITZ S. M.: Photo-\nshape: Photorealistic materials for large-scale shape collections. arXiv\npreprint arXiv:1809.09761 (2018). 14, 15\n[PSB\u221721]\nPARK K., SINHA U., BARRON J. T., BOUAZIZ S., GOLD-\nMAN D. B., SEITZ S. M., MARTIN-BRUALLA R.: Nerfies: Deformable\nneural radiance fields. In ICCV (2021). 17\n[PSH\u221720]\nPAVLLO D., SPINKS G., HOFMANN T., MOENS M.-F., LUC-\nCHI A.: Convolutional generation of textured 3d meshes. Advances in\nNeural Information Processing Systems 33 (2020), 870\u2013882. 9\n[PYG\u221723]\nPO R., YIFAN W., GOLYANIK V., ABERMAN K., BARRON\nJ. T., BERMANO A. H., CHAN E. R., DEKEL T., HOLYNSKI A.,\nKANAZAWA A., ET AL.: State of the art on diffusion models for visual\ncomputing. arXiv preprint arXiv:2310.07204 (2023). 3\n[PYL\u221722]\nPENG Y., YAN Y., LIU S., CHENG Y., GUAN S., PAN B.,\nZHAI G., YANG X.: CageNeRF: Cage-based neural radiance field for\ngeneralized 3d deformation and animation. In NeurIPS (2022). 20\n[PZ17]\nPENNER E., ZHANG L.: Soft 3d reconstruction for view synthe-\nsis. ACM Transactions on Graphics (TOG) 36, 6 (2017), 1\u201311. 5\n[PZVBG00]\nPFISTER H., ZWICKER M., VAN BAAR J., GROSS M.:\nSurfels: Surface elements as rendering primitives.\nIn Proceedings of\nthe 27th annual conference on Computer graphics and interactive tech-\nniques (2000), pp. 335\u2013342. 3\n[QMH\u221723]\nQIAN G., MAI J., HAMDI A., REN J., SIAROHIN A., LI B.,\nLEE H.-Y., SKOROKHODOV I., WONKA P., TULYAKOV S., ET AL.:\nMagic123: One image to high-quality 3d object generation using both\n2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843 (2023). 12\n[RALB22]\nRAKHIMOV R., ARDELEAN A.-T., LEMPITSKY V., BUR-\nNAEV E.: Npbg++: Accelerating neural point-based graphics. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (2022), pp. 15969\u201315979. 4\n[RBL\u221722a]\nROMBACH R., BLATTMANN A., LORENZ D., ESSER P.,\nOMMER B.: High-resolution image synthesis with latent diffusion mod-\nels. In CVPR (2022). 1, 2, 9, 19, 20\n[RBL\u221722b]\nROMBACH R., BLATTMANN A., LORENZ D., ESSER P.,\nOMMER B.: High-resolution image synthesis with latent diffusion mod-\nels. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition (2022), pp. 10684\u201310695. 12, 16\n[REO21]\nROMBACH R., ESSER P., OMMER B.:\nGeometry-free\nview synthesis: Transformers and no 3d priors.\nIn Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (2021),\npp. 14356\u201314366. 13\n[RFJ21]\nROCKWELL C., FOUHEY D. F., JOHNSON J.: Pixelsynth: Gen-\nerating a 3d-consistent experience from a single image. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision (2021),\npp. 14104\u201314113. 14, 18\n[RFS22]\nR\u00dcCKERT D., FRANKE L., STAMMINGER M.: Adop: Approx-\nimate differentiable one-pixel point rendering.\nACM Transactions on\nGraphics (ToG) 41, 4 (2022), 1\u201314. 4\n[RK20]\nRIEGLER G., KOLTUN V.: Free view synthesis. In Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August\n23\u201328, 2020, Proceedings, Part XIX 16 (2020), Springer, pp. 623\u2013640. 4\n[RK21]\nRIEGLER G., KOLTUN V.: Stable view synthesis. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion (2021), pp. 12216\u201312225. 4\n[RKH\u221721a]\nRADFORD A., KIM J. W., HALLACY C., RAMESH A.,\nGOH G., AGARWAL S., SASTRY G., ASKELL A., MISHKIN P., CLARK\nJ., ET AL.: Learning transferable visual models from natural language\nsupervision. In ICML (2021). 11, 19\n[RKH\u221721b]\nRADFORD A., KIM J. W., HALLACY C., RAMESH A.,\nGOH G., AGARWAL S., SASTRY G., ASKELL A., MISHKIN P., CLARK\nJ., ET AL.: Learning transferable visual models from natural language\nsupervision. In International Conference on Machine Learning (2021),\nPMLR, pp. 8748\u20138763. 16\n[RKP\u221723]\nRAJ A., KAZA S., POOLE B., NIEMEYER M., RUIZ N.,\nMILDENHALL B., ZADA S., ABERMAN K., RUBINSTEIN M., BAR-\nRON J., ET AL.: Dreambooth3d: Subject-driven text-to-3d generation.\narXiv preprint arXiv:2303.13508 (2023). 18\n[RPLG21]\nREISER C., PENG S., LIAO Y., GEIGER A.: Kilonerf: Speed-\ning up neural radiance fields with thousands of tiny mlps. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision (2021),\npp. 14335\u201314345. 5\n[RROG18]\nROVERI R., RAHMANN L., OZTIRELI C., GROSS M.: A\nnetwork architecture for point cloud classification via automatic depth\nimages generation. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (2018), pp. 4176\u20134184. 4\n[RSH\u221721]\nREIZENSTEIN J., SHAPOVALOV R., HENZLER P., SBOR-\nDONE L., LABATUT P., NOVOTNY D.: Common objects in 3d: Large-\nscale learning and evaluation of real-life 3d category reconstruction. In\nProceedings of the IEEE/CVF International Conference on Computer\nVision (2021), pp. 10901\u201310911. 14, 15\n[RWC\u221719]\nRADFORD A., WU J., CHILD R., LUAN D., AMODEI D.,\nSUTSKEVER I., ET AL.: Language models are unsupervised multitask\nlearners. OpenAI blog 1, 8 (2019), 9. 2, 6, 10\n[RWL\u221722]\nR\u00dcCKERT D., WANG Y., LI R., IDOUGHI R., HEIDRICH\nW.: Neat: Neural adaptive tomography. ACM Transactions on Graphics\n(TOG) 41, 4 (2022), 1\u201313. 5\n[SAA\u221723]\nSIDDIQUI Y., ALLIEGRO A., ARTEMOV A., TOMMASI T.,\nSIRIGATTI D., ROSOV V., DAI A., NIESSNER M.: Meshgpt: Gener-\nating triangle meshes with decoder-only transformers. arXiv preprint\narXiv:2311.15475 (2023). 21\n[SCL\u221722]\nSANGHI A., CHU H., LAMBOURNE J. G., WANG Y., CHENG\nC.-Y., FUMERO M., MALEKSHAN K. R.: Clip-forge: Towards zero-\nshot text-to-shape generation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (2022), pp. 18603\u2013\n18613. 18\n[SCP\u221723]\nSHUE J. R., CHAN E. R., PO R., ANKNER Z., WU J., WET-\nZSTEIN G.: 3d neural field generation using triplane diffusion. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (2023), pp. 20875\u201320886. 6, 10, 18, 19\n[SCS\u221722]\nSAHARIA C., CHAN W., SAXENA S., LI L., WHANG\nJ.,\nDENTON\nE.\nL.,\nGHASEMIPOUR\nK.,\nGONTIJO\nLOPES\nR.,\nKARAGOL AYAN B., SALIMANS T., ET AL.:\nPhotorealistic text-to-\nimage diffusion models with deep language understanding. Advances\nin Neural Information Processing Systems 35 (2022), 36479\u201336494. 2,\n6, 16\n[SCZ\u221723]\nSHI R., CHEN H., ZHANG Z., LIU M., XU C., WEI X.,\nCHEN L., ZENG C., SU H.:\nZero123++: a single image to consis-\ntent multi-view diffusion base model. arXiv preprint arXiv:2310.15110\n(2023). 13\n[SDZ\u221721]\nSRINIVASAN P. P., DENG B., ZHANG X., TANCIK M.,\nMILDENHALL B., BARRON J. T.: Nerv: Neural reflectance and vis-\nibility fields for relighting and view synthesis. In Proceedings of the\n30\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(2021), pp. 7495\u20137504. 5\n[SFL\u221723]\nSANGHI A., FU R., LIU V., WILLIS K. D., SHAYANI H.,\nKHASAHMADI A. H., SRIDHAR S., RITCHIE D.: Clip-sculptor: Zero-\nshot generation of high-fidelity and diverse shapes from natural lan-\nguage. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (2023), pp. 18339\u201318348. 18\n[SGHS98]\nSHADE J., GORTLER S., HE L.-W., SZELISKI R.: Layered\ndepth images. In Proceedings of the 25th annual conference on Com-\nputer graphics and interactive techniques (1998), pp. 231\u2013242. 5\n[SGY\u221721]\nSHEN T., GAO J., YIN K., LIU M.-Y., FIDLER S.: Deep\nmarching tetrahedra: a hybrid representation for high-resolution 3d shape\nsynthesis.\nAdvances in Neural Information Processing Systems 34\n(2021), 6087\u20136101. 1, 6, 10, 12\n[SHG\u221722]\nSHRESTHA R., HU S., GOU M., LIU Z., TAN P.: A real\nworld dataset for multi-view 3d reconstruction. In European Conference\non Computer Vision (2022), Springer, pp. 56\u201373. 15\n[SHN\u221719]\nSAITO S., HUANG Z., NATSUME R., MORISHIMA S.,\nKANAZAWA A., LI H.: Pifu: Pixel-aligned implicit function for high-\nresolution clothed human digitization. In Proceedings of the IEEE/CVF\ninternational conference on computer vision (2019), pp. 2304\u20132314. 16\n[SKJ23]\nSHIM J., KANG C., JOO K.: Diffusion-based signed distance\nfields for 3d shape generation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (2023), pp. 20887\u2013\n20897. 10\n[SLNG20]\nSCHWARZ K., LIAO Y., NIEMEYER M., GEIGER A.: GRAF:\nGenerative radiance fields for 3d-aware image synthesis. In NeurIPS\n(2020). 9, 15, 17\n[SMKF04]\nSHILANE P., MIN P., KAZHDAN M., FUNKHOUSER T.: The\nprinceton shape benchmark. In Proceedings Shape Modeling Applica-\ntions, 2004. (2004), IEEE, pp. 167\u2013178. 14\n[SMP\u221722]\nSAJJADI M. S., MEYER H., POT E., BERGMANN U., GR-\nEFF K., RADWAN N., VORA S., LU \u02c7CI \u00b4C M., DUCKWORTH D., DOSO-\nVITSKIY A., ET AL.: Scene representation transformer: Geometry-free\nnovel view synthesis through set-latent scene representations. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (2022), pp. 6229\u20136238. 13, 14\n[SPH\u221723]\nSINGER U., POLYAK A., HAYES T., YIN X., AN J., ZHANG\nS., HU Q., YANG H., ASHUAL O., GAFNI O., ET AL.: Make-a-video:\nText-to-video generation without text-video data. In Int. Conf. Learn.\nRepresent. (2023). 2\n[SPK19]\nSHU D. W., PARK S. W., KWON J.: 3d point cloud generative\nadversarial network based on tree structured graph convolutions. In Pro-\nceedings of the IEEE/CVF international conference on computer vision\n(2019), pp. 3859\u20133868. 7\n[SPX\u221722]\nSHI Z., PENG S., XU Y., GEIGER A., LIAO Y., SHEN Y.:\nDeep generative models on 3d representations: A survey. arXiv preprint\narXiv:2210.15663 (2022). 3\n[SS87]\nSHIRMAN L. A., SEQUIN C. H.: Local surface interpolation with\nb\u00e9zier patches. Computer Aided Geometric Design 4, 4 (1987), 279\u2013295.\n4\n[SSC22]\nSUN C., SUN M., CHEN H.-T.: Direct voxel grid optimiza-\ntion: Super-fast convergence for radiance fields reconstruction. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (2022), pp. 5459\u20135469. 6\n[SSKH20]\nSHIH M.-L., SU S.-Y., KOPF J., HUANG J.-B.: 3d photog-\nraphy using context-aware layered depth inpainting. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(2020), pp. 8028\u20138038. 5\n[SSN\u221714]\nSINGH A., SHA J., NARAYAN K. S., ACHIM T., ABBEEL P.:\nBigbird: A large-scale 3d database of object instances. In 2014 IEEE in-\nternational conference on robotics and automation (ICRA) (2014), IEEE,\npp. 509\u2013516. 14\n[SSN\u221722]\nSCHWARZ K., SAUER A., NIEMEYER M., LIAO Y., GEIGER\nA.: Voxgraf: Fast 3d-aware image synthesis with sparse voxel grids.\nAdvances in Neural Information Processing Systems 35 (2022), 33999\u2013\n34011. 6\n[STB\u221719]\nSRINIVASAN P. P., TUCKER R., BARRON J. T., RA-\nMAMOORTHI R., NG R., SNAVELY N.:\nPushing the boundaries of\nview extrapolation with multiplane images.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(2019), pp. 175\u2013184. 5\n[SWL\u221720a]\nSUN Y., WANG Y., LIU Z., SIEGEL J., SARMA S.:\nPointgrow: Autoregressively learned point cloud generation with self-\nattention. In Proceedings of the IEEE/CVF Winter Conference on Appli-\ncations of Computer Vision (2020), pp. 61\u201370. 7\n[SWL\u221720b]\nSUN Y., WANG Y., LIU Z., SIEGEL J., SARMA S.:\nPointgrow: Autoregressively learned point cloud generation with self-\nattention. In WACV (2020). 10\n[SWS\u221722]\nSUN J., WANG X., SHI Y., WANG L., WANG J., LIU Y.:\nIDE-3D: Interactive disentangled editing for high-resolution 3d-aware\nportrait synthesis. ACM Trans. Graph. (2022). 18, 19\n[SWW\u221723]\nSUN J., WANG X., WANG L., LI X., ZHANG Y., ZHANG\nH., LIU Y.: Next3D: Generative neural texture rasterization for 3d-aware\nhead avatars. In CVPR (2023). 18\n[SWY\u221723]\nSHI Y., WANG P., YE J., LONG M., LI K., YANG X.:\nMvdream: Multi-view diffusion for 3d generation.\narXiv preprint\narXiv:2308.16512 (2023). 7, 12\n[SWZ\u221718]\nSUN X., WU J., ZHANG X., ZHANG Z., ZHANG C., XUE T.,\nTENENBAUM J. B., FREEMAN W. T.: Pix3d: Dataset and methods for\nsingle-image 3d shape modeling. In Proceedings of the IEEE conference\non computer vision and pattern recognition (2018), pp. 2974\u20132983. 14\n[SWZ\u221722]\nSUN J., WANG X., ZHANG Y., LI X., ZHANG Q., LIU Y.,\nWANG J.: Fenerf: Face editing in neural radiance fields. In CVPR (2022).\n18, 19\n[SZS\u221723]\nSUN J., ZHANG B., SHAO R., WANG L., LIU W., XIE Z.,\nLIU Y.: Dreamcraft3d: Hierarchical 3d generation with bootstrapped\ndiffusion prior. arXiv preprint arXiv:2310.16818 (2023). 12\n[TDB16]\nTATARCHENKO M., DOSOVITSKIY A., BROX T.: Multi-view\n3d models from single images with a convolutional network. In Com-\nputer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The\nNetherlands, October 11\u201314, 2016, Proceedings, Part VII 14 (2016),\nSpringer, pp. 322\u2013337. 14\n[TFT\u221720]\nTEWARI A., FRIED O., THIES J., SITZMANN V., LOMBARDI\nS., SUNKAVALLI K., MARTIN-BRUALLA R., SIMON T., SARAGIH J.,\nNIESSNER M., ET AL.: State of the art on neural rendering. In Computer\nGraphics Forum (2020), vol. 39, Wiley Online Library, pp. 701\u2013727. 3\n[TLK\u221723]\nTSENG H.-Y., LI Q., KIM C., ALSISAN S., HUANG J.-B.,\nKOPF J.: Consistent view synthesis with pose-guided diffusion models.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (2023), pp. 16773\u201316783. 13\n[TLY\u221721]\nTAKIKAWA T., LITALIEN J., YIN K., KREIS K., LOOP C.,\nNOWROUZEZAHRAI D., JACOBSON A., MCGUIRE M., FIDLER S.:\nNeural geometric level of detail: Real-time rendering with implicit 3d\nshapes. In Proceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (2021), pp. 11358\u201311367. 6\n[TLYCS22]\nTSENG W.-C., LIAO H.-J., YEN-CHEN L., SUN M.: CLA-\nNeRF: Category-level articulated neural radiance field. In International\nConference on Robotics and Automation (ICRA) (2022). 19, 20\n[TME\u221722]\nTREMBLAY J., MESHRY M., EVANS A., KAUTZ J., KELLER\nA., KHAMIS S., M\u00dcLLER T., LOOP C., MORRICAL N., NAGANO K.,\nET AL.: Rtmv: A ray-traced multi-view synthetic dataset for novel view\nsynthesis. arXiv preprint arXiv:2205.07058 (2022). 15\n[TRMT23]\nTRUONG\nP.,\nRAKOTOSAONA\nM.-J.,\nMANHARDT\nF.,\nTOMBARI F.: Sparf: Neural radiance fields from sparse and noisy poses.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (2023), pp. 4190\u20134200. 5\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n31\n[TRZ\u221723]\nTANG J., REN J., ZHOU H., LIU Z., ZENG G.:\nDream-\ngaussian: Generative gaussian splatting for efficient 3d content creation.\narXiv preprint arXiv:2309.16653 (2023). 12, 18, 21\n[TTG\u221720]\nTRETSCHK E., TEWARI A., GOLYANIK V., ZOLLH\u00d6FER M.,\nSTOLL C., THEOBALT C.: Patchnets: Patch-based generalizable deep\nimplicit 3d shape representations. In Computer Vision\u2013ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings,\nPart XVI 16 (2020), Springer, pp. 293\u2013309. 6\n[TTM\u221722]\nTEWARI A., THIES J., MILDENHALL B., SRINIVASAN P.,\nTRETSCHK E., YIFAN W., LASSNER C., SITZMANN V., MARTIN-\nBRUALLA R., LOMBARDI S., ET AL.: Advances in neural rendering.\nIn Computer Graphics Forum (2022), vol. 41, Wiley Online Library,\npp. 703\u2013735. 3\n[TWZ\u221723]\nTANG J., WANG T., ZHANG B., ZHANG T., YI R., MA L.,\nCHEN D.: Make-it-3d: High-fidelity 3d creation from a single image\nwith diffusion prior. arXiv preprint arXiv:2303.14184 (2023). 7, 12, 18,\n19\n[TYC\u221723]\nTEWARI A., YIN T., CAZENAVETTE G., REZCHIKOV S.,\nTENENBAUM J. B., DURAND F., FREEMAN W. T., SITZMANN V.: Dif-\nfusion with forward models: Solving stochastic inverse problems without\ndirect supervision. arXiv preprint arXiv:2306.11719 (2023). 13\n[TZN19]\nTHIES J., ZOLLH\u00d6FER M., NIESSNER M.: Deferred neural\nrendering: Image synthesis using neural textures. Acm Transactions on\nGraphics (TOG) 38, 4 (2019), 1\u201312. 4\n[VHM\u221722]\nVERBIN D., HEDMAN P., MILDENHALL B., ZICKLER T.,\nBARRON J. T., SRINIVASAN P. P.: Ref-nerf: Structured view-dependent\nappearance for neural radiance fields. In 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) (2022), IEEE,\npp. 5481\u20135490. 5\n[VN\u221751]\nVON NEUMANN J., ET AL.: The general and logical theory of\nautomata. 1951 (1951), 1\u201341. 13\n[WCH\u221722]\nWANG C., CHAI M., HE M., CHEN D., LIAO J.: CLIP-\nNeRF: Text-and-image driven manipulation of neural radiance fields. In\nCVPR (2022). 19\n[WCMB\u221722]\nWATSON D., CHAN W., MARTIN-BRUALLA R., HO J.,\nTAGLIASACCHI A., NOROUZI M.: Novel view synthesis with diffusion\nmodels. arXiv preprint arXiv:2210.04628 (2022). 13\n[WCS\u221722]\nWENG C.-Y., CURLESS B., SRINIVASAN P. P., BARRON\nJ. T., KEMELMACHER-SHLIZERMAN I.: Humannerf: Free-viewpoint\nrendering of moving people from monocular video. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern Recognition\n(2022), pp. 16210\u201316220. 5\n[WDL\u221723]\nWANG H., DU X., LI J., YEH R. A., SHAKHNAROVICH G.:\nScore jacobian chaining: Lifting pretrained 2d diffusion models for 3d\ngeneration. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (2023), pp. 12619\u201312629. 12, 18\n[WDY\u221722]\nWU Y., DENG Y., YANG J., WEI F., CHEN Q., TONG X.:\nAniFaceGAN: Animatable 3d-aware face image generation for video\navatars. In NeurIPS (2022). 18, 19\n[WGSJ20]\nWILES O., GKIOXARI G., SZELISKI R., JOHNSON J.:\nSynsin: End-to-end view synthesis from a single image. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion (2020), pp. 7467\u20137477. 4, 14\n[WJC\u221723]\nWANG C., JIANG R., CHAI M., HE M., CHEN D., LIAO J.:\nNeRF-Art: Text-driven neural radiance fields stylization. IEEE Trans.\nVis. Comput. Graph. (2023). 19\n[WKC\u221723]\nWANG C., KANG D., CAO Y., BAO L., SHAN Y., ZHANG\nS.-H.:\nNeural point-based volumetric avatar: Surface-guided neural\npoints for efficient and photorealistic volumetric head avatar. In ACM\nSIGGRAPH Asia 2023 Conference Proceedings (2023). 17\n[WLC\u221722]\nWU Q., LIU X., CHEN Y., LI K., ZHENG C., CAI J.,\nZHENG J.:\nObject-compositional neural implicit surfaces.\nIn ECCV\n(2022). 19, 20\n[WLG\u221723]\nWANG M., LIU Y.-S., GAO Y., SHI K., FANG Y., HAN Z.:\nLp-dif: Learning local pattern-specific deep implicit function for 3d ob-\njects and scenes. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (2023), pp. 21856\u201321865. 6\n[WLL\u221721]\nWANG P., LIU L., LIU Y., THEOBALT C., KOMURA T.,\nWANG W.: Neus: Learning neural implicit surfaces by volume rendering\nfor multi-view reconstruction. Advances in Neural Information Process-\ning Systems 34 (2021), 27171\u201327183. 6\n[WLW\u221723]\nWANG Z., LU C., WANG Y., BAO F., LI C., SU H., ZHU\nJ.: Prolificdreamer: High-fidelity and diverse text-to-3d generation with\nvariational score distillation. arXiv preprint arXiv:2305.16213 (2023).\n12, 18, 19, 21\n[WLY\u221723]\nWU T., LI Z., YANG S., ZHANG P., PAN X., WANG J., LIN\nD., LIU Z.: Hyperdreamer: Hyper-realistic 3d content generation and\nediting from a single image. In SIGGRAPH Asia 2023 Conference Pa-\npers (2023), pp. 1\u201310. 18\n[Wol83]\nWOLFRAM S.: Statistical mechanics of cellular automata. Re-\nviews of modern physics 55, 3 (1983), 601. 13\n[WPH\u221723]\nWAN Z., PASCHALIDOU D., HUANG I., LIU H., SHEN B.,\nXIANG X., LIAO J., GUIBAS L.: Cad: Photorealistic 3d generation via\nadversarial distillation. arXiv preprint arXiv:2312.06663 (2023). 18\n[WSK\u221715]\nWU Z., SONG S., KHOSLA A., YU F., ZHANG L., TANG\nX., XIAO J.: 3d shapenets: A deep representation for volumetric shapes.\nIn Proceedings of the IEEE conference on computer vision and pattern\nrecognition (2015), pp. 1912\u20131920. 6\n[WSW22]\nWANG Y., SKOROKHODOV I., WONKA P.: HF-NeuS: Im-\nproved surface reconstruction using high-frequency details. Advances in\nNeural Information Processing Systems (2022). 6\n[WSW23]\nWANG Y., SKOROKHODOV I., WONKA P.: PET-NeuS: Posi-\ntional encoding triplanes for neural surfaces. In CVPR (2023). 6\n[WWG\u221721]\nWANG Q., WANG Z., GENOVA K., SRINIVASAN P. P.,\nZHOU H., BARRON J. T., MARTIN-BRUALLA R., SNAVELY N.,\nFUNKHOUSER T.: IBRNet: Learning multi-view image-based render-\ning. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (2021), pp. 4690\u20134699. 5, 13\n[WWL\u221723]\nWU Q., WANG K., LI K., ZHENG J., CAI J.:\nOb-\njectSDF++: Improved object-compositional neural implicit surfaces. In\nICCV (2023). 19, 20\n[WWX\u221721]\nWANG Z., WU S., XIE W., CHEN M., PRISACARIU V. A.:\nNerf\u2013: Neural radiance fields without known camera parameters. arXiv\npreprint arXiv:2102.07064 (2021). 5\n[WZ22]\nWU R., ZHENG C.: Learning to generate 3d shapes from a single\nexample. arXiv preprint arXiv:2208.02946 (2022). 18\n[WZF\u221723]\nWU T., ZHANG J., FU X., WANG Y., REN J., PAN L., WU\nW., YANG L., WANG J., QIAN C., ET AL.:\nOmniobject3d: Large-\nvocabulary 3d object dataset for realistic perception, reconstruction and\ngeneration. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (2023), pp. 803\u2013814. 14, 15\n[WZX\u221716]\nWU J., ZHANG C., XUE T., FREEMAN B., TENENBAUM J.:\nLearning a probabilistic latent space of object shapes via 3d generative-\nadversarial modeling. Advances in neural information processing sys-\ntems (2016). 1, 2, 6, 7, 9, 15, 18\n[WZZ\u221723]\nWANG T., ZHANG B., ZHANG T., GU S., BAO J., BALTRU-\nSAITIS T., SHEN J., CHEN D., WEN F., CHEN Q., ET AL.: RODIN:\nA generative model for sculpting 3d digital avatars using diffusion. In\nCVPR (2023). 10\n[XJW\u221723]\nXU D., JIANG Y., WANG P., FAN Z., WANG Y., WANG\nZ.: Neurallift-360: Lifting an in-the-wild 2d photo to a 3d object with\n360deg views. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (2023), pp. 4479\u20134489. 12\n[XKJ\u221723]\nXIONG Z., KANG D., JIN D., CHEN W., BAO L., CUI S.,\nHAN X.:\nGet3dhuman: Lifting stylegan-human into a 3d generative\nmodel using pixel-aligned reconstruction priors.\nIn Proceedings of\n32\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\nthe IEEE/CVF International Conference on Computer Vision (2023),\npp. 9287\u20139297. 15, 16\n[XSJ\u221723]\nXU H., SONG G., JIANG Z., ZHANG J., SHI Y., LIU J., MA\nW., FENG J., LUO L.: OmniAvatar: Geometry-guided controllable 3d\nhead synthesis. In CVPR (2023). 18, 19\n[XTL\u221723]\nXU Y., TAN H., LUAN F., BI S., WANG P., LI J., SHI Z.,\nSUNKAVALLI K., WETZSTEIN G., XU Z., ET AL.: Dmv3d: Denoising\nmulti-view diffusion using 3d large reconstruction model. arXiv preprint\narXiv:2311.09217 (2023). 7\n[XTS\u221722]\nXIE Y., TAKIKAWA T., SAITO S., LITANY O., YAN S.,\nKHAN N., TOMBARI F., TOMPKIN J., SITZMANN V., SRIDHAR S.:\nNeural fields in visual computing and beyond. In Computer Graphics\nForum (2022), vol. 41, Wiley Online Library, pp. 641\u2013676. 3\n[XWC\u221719]\nXU Q., WANG W., CEYLAN D., MECH R., NEUMANN U.:\nDisn: Deep implicit surface network for high-quality single-view 3d re-\nconstruction.\nAdvances in neural information processing systems 32\n(2019). 5\n[XX23]\nXIA W., XUE J.-H.: A survey on deep generative 3d-aware im-\nage synthesis. ACM Computing Surveys 56, 4 (2023), 1\u201334. 3\n[XYB\u221723]\nXU Y., YIFAN W., BERGMAN A. W., CHAI M., ZHOU B.,\nWETZSTEIN G.: Efficient 3d articulated human generation with layered\nsurface volumes. arXiv preprint arXiv:2307.05462 (2023). 16\n[XYC\u221723]\nXIU Y., YANG J., CAO X., TZIONAS D., BLACK M. J.:\nEcon: Explicit clothed humans optimized via normal integration. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (2023), pp. 512\u2013523. 15, 16\n[XYHT23]\nXIANG J., YANG J., HUANG B., TONG X.: 3d-aware image\ngeneration using 2d diffusion models. arXiv preprint arXiv:2303.17905\n(2023). 18, 19\n[XYTB22]\nXIU Y., YANG J., TZIONAS D., BLACK M. J.: Icon: Im-\nplicit clothed humans obtained from normals. In 2022 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (CVPR) (2022),\nIEEE, pp. 13286\u201313296. 15, 16\n[YAK\u221720]\nYIFAN W., AIGERMAN N., KIM V. G., CHAUDHURI S.,\nSORKINE-HORNUNG O.: Neural cages for detail-preserving 3d defor-\nmations. In Proceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (2020), pp. 75\u201383. 5\n[YBZ\u221722]\nYANG B., BAO C., ZENG J., BAO H., ZHANG Y., CUI Z.,\nZHANG G.: NeuMesh: Learning disentangled neural mesh-based im-\nplicit field for geometry and texture editing. In ECCV (2022), Springer.\n19, 20\n[YGKL21]\nYARIV L., GU J., KASTEN Y., LIPMAN Y.: Volume render-\ning of neural implicit surfaces. Advances in Neural Information Process-\ning Systems 34 (2021), 4805\u20134815. 6\n[YGMG23]\nYOO P., GUO J., MATSUO Y., GU S. S.: Dreamsparse: Es-\ncaping from plato\u2019s cave with 2d frozen diffusion model given sparse\nviews. CoRR (2023). 13\n[YHH\u221719a]\nYANG G., HUANG X., HAO Z., LIU M.-Y., BELONGIE S.,\nHARIHARAN B.: Pointflow: 3d point cloud generation with continuous\nnormalizing flows. In Proceedings of the IEEE/CVF international con-\nference on computer vision (2019), pp. 4541\u20134550. 7, 11\n[YHH\u221719b]\nYANG G., HUANG X., HAO Z., LIU M.-Y., BELONGIE S.,\nHARIHARAN B.: Pointflow: 3d point cloud generation with continuous\nnormalizing flows. In Proceedings of the IEEE/CVF international con-\nference on computer vision (2019), pp. 4541\u20134550. 21\n[YLM\u221722]\nYAN X., LIN L., MITRA N. J., LISCHINSKI D., COHEN-OR\nD., HUANG H.: Shapeformer: Transformer-based shape completion via\nsparse representation. In CVPR (2022). 11\n[YLWD22]\nYANG Z., LI S., WU W., DAI B.:\n3dhumangan: To-\nwards photo-realistic 3d-aware human image generation. arXiv preprint\narXiv:2212.07378 (2022). 16\n[YLX\u221723]\nYANG X., LUO Y., XIU Y., WANG W., XU H., FAN Z.: D-\nif: Uncertainty-aware human digitization via implicit distribution field.\nIn Proceedings of the IEEE/CVF International Conference on Computer\nVision (2023), pp. 9122\u20139132. 16\n[YPN\u221722]\nYU Z., PENG S., NIEMEYER M., SATTLER T., GEIGER A.:\nMonosdf: Exploring monocular geometric cues for neural implicit sur-\nface reconstruction. Advances in neural information processing systems\n35 (2022), 25018\u201325032. 6\n[YRSh21]\nYIFAN\nW.,\nRAHMANN\nL.,\nSORKINE-HORNUNG\nO.:\nGeometry-consistent neural shape representation with implicit displace-\nment fields. In International Conference on Learning Representations\n(2021). 6\n[YSL\u221722]\nYUAN Y.-J., SUN Y.-T., LAI Y.-K., MA Y., JIA R., GAO\nL.: NeRF-Editing: geometry editing of neural radiance fields. In CVPR\n(2022). 5, 20\n[YSW\u221719]\nYIFAN W., SERENA F., WU S., \u00d6ZTIRELI C., SORKINE-\nHORNUNG O.: Differentiable surface splatting for point-based geometry\nprocessing. ACM Transactions on Graphics (TOG) 38, 6 (2019), 1\u201314.\n4\n[YTB\u221721]\nYENAMANDRA T., TEWARI A., BERNARD F., SEIDEL H.-\nP., ELGHARIB M., CREMERS D., THEOBALT C.: i3DMM: Deep im-\nplicit 3d morphable model of human heads. In CVPR (2021). 17\n[YXZ\u221723]\nYU X., XU M., ZHANG Y., LIU H., YE C., WU Y., YAN Z.,\nZHU C., XIONG Z., LIANG T., ET AL.: Mvimgnet: A large-scale dataset\nof multi-view images. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (2023), pp. 9150\u20139161. 14,\n15\n[YYC\u221723]\nYU W., YUAN L., CAO Y.-P., GAO X., LI X., QUAN L.,\nSHAN Y., TIAN Y.: Hifi-123: Towards high-fidelity one image to 3d\ncontent generation. arXiv preprint arXiv:2310.06744 (2023). 12\n[YYTK21]\nYU A., YE V., TANCIK M., KANAZAWA A.:\npixelnerf:\nNeural radiance fields from one or few images. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(2021), pp. 4578\u20134587. 5, 13\n[YZX\u221721]\nYANG B., ZHANG Y., XU Y., LI Y., ZHOU H., BAO H.,\nZHANG G., CUI Z.: Learning object-compositional neural radiance field\nfor editable scene rendering. In ICCV (2021). 19, 20\n[ZAB\u221722]\nZHENG Y., ABREVAYA V. F., B\u00dcHLER M. C., CHEN X.,\nBLACK M. J., HILLIGES O.: IMAvatar: Implicit morphable head avatars\nfrom videos. In CVPR (2022). 17\n[ZBT23]\nZIELONKA W., BOLKART T., THIES J.:\nInstant volumetric\nhead avatars. In CVPR (2023). 17\n[ZCY\u221723]\nZHANG H., CHEN B., YANG H., QU L., WANG X., CHEN\nL., LONG C., ZHU F., DU K., ZHENG M.:\nAvatarverse: High-\nquality & stable 3d avatar creation from text and pose. arXiv preprint\narXiv:2308.03610 (2023). 16\n[ZDW21]\nZHOU L., DU Y., WU J.: 3d shape generation and completion\nthrough point-voxel diffusion. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (2021), pp. 5826\u20135835. 10,\n18\n[ZJ16]\nZHOU Q., JACOBSON A.: Thingi10k: A dataset of 10,000 3d-\nprinting models. arXiv preprint arXiv:1605.04797 (2016). 14\n[ZJY\u221722]\nZHANG J., JIANG Z., YANG D., XU H., SHI Y., SONG G.,\nXU Z., WANG X., FENG J.: Avatargen: a 3d generative model for an-\nimatable human avatars. In European Conference on Computer Vision\n(2022), Springer, pp. 668\u2013685. 16\n[ZKB\u221722]\nZHANG K., KOLKIN N., BI S., LUAN F., XU Z., SHECHT-\nMAN E., SNAVELY N.: ARF: Artistic radiance fields. In ECCV (2022),\nSpringer. 19, 20\n[ZKW\u221723]\nZHOU A., KIM M. J., WANG L., FLORENCE P., FINN C.:\nNerf in the palm of your hand: Corrective augmentation for robotics via\nnovel-view synthesis. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (2023), pp. 17907\u201317917. 5\n[ZLC\u221723]\nZHAO Z., LIU W., CHEN X., ZENG X., WANG R., CHENG\nP., FU B., CHEN T., YU G., GAO S.: Michelangelo: Conditional 3d\nX. Li & Q. Zhang & D. Kang & W. Cheng & Y. Gao et al. / Advances in 3D Generation: A Survey\n33\nshape generation based on shape-image-text aligned latent representa-\ntion. arXiv preprint arXiv:2306.17115 (2023). 18\n[ZLLD21]\nZHI S., LAIDLOW T., LEUTENEGGER S., DAVISON A. J.:\nIn-place scene labelling and understanding with implicit scene represen-\ntation. In ICCV (2021), pp. 15838\u201315847. 5\n[ZLLS22]\nZHANG K., LUAN F., LI Z., SNAVELY N.: Iron: Inverse ren-\ndering by optimizing neural sdfs and materials from photometric images.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (2022), pp. 5565\u20135574. 6\n[ZLW\u221721]\nZHANG K., LUAN F., WANG Q., BALA K., SNAVELY N.:\nPhysg: Inverse rendering with spherical gaussians for physics-based ma-\nterial editing and relighting. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (2021), pp. 5453\u20135462. 5\n[ZLW\u221722]\nZHANG J., LI X., WAN Z., WANG C., LIAO J.: Fdnerf: Few-\nshot dynamic neural radiance fields for face reconstruction and expres-\nsion editing. In SIGGRAPH Asia 2022 Conference Papers (2022), pp. 1\u2013\n9. 19\n[ZLW\u221723]\nZHANG J., LI X., WAN Z., WANG C., LIAO J.: Text2nerf:\nText-driven 3d scene generation with neural radiance fields.\narXiv\npreprint arXiv:2305.11588 (2023). 18, 19\n[ZLWT22]\nZHENG X., LIU Y., WANG P., TONG X.: Sdf-stylegan: Im-\nplicit sdf-based stylegan for 3d shape generation. In Computer Graphics\nForum (2022), vol. 41, Wiley Online Library, pp. 52\u201363. 7\n[ZLZ\u221722]\nZHU H., LIU Z., ZHOU Y., MA Z., CAO X.: DNF: Diffractive\nneural field for lensless microscopic imaging.\nOptics Express 30, 11\n(2022), 18168\u201318178. 5\n[ZLZ\u221723]\nZHANG J., LI X., ZHANG Q., CAO Y., SHAN Y., LIAO J.:\nHumanref: Single image to 3d human generation via reference-guided\ndiffusion. arXiv preprint arXiv:2311.16961 (2023). 15, 16\n[ZML\u221722]\nZHOU J., MA B., LIU Y.-S., FANG Y., HAN Z.: Learning\nconsistency-aware unsigned distance functions progressively from raw\npoint clouds. Advances in Neural Information Processing Systems 35\n(2022), 16481\u201316494. 5\n[ZPL\u221722]\nZHU Z., PENG S., LARSSON V., XU W., BAO H., CUI Z.,\nOSWALD M. R., POLLEFEYS M.: Nice-slam: Neural implicit scalable\nencoding for slam. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (2022), pp. 12786\u201312796. 5\n[ZPVBG02]\nZWICKER M., PFISTER H., VAN BAAR J., GROSS M.: Ewa\nsplatting. IEEE Transactions on Visualization and Computer Graphics\n8, 3 (2002), 223\u2013238. 4\n[ZPW\u221723]\nZHENG X.-Y., PAN H., WANG P.-S., TONG X., LIU Y.,\nSHUM H.-Y.: Locally attentional sdf diffusion for controllable 3d shape\ngeneration. arXiv preprint arXiv:2305.04461 (2023). 10\n[ZQL\u221723]\nZHANG L., QIU Q., LIN H., ZHANG Q., SHI C., YANG W.,\nSHI Y., YANG S., XU L., YU J.: DreamFace: Progressive generation of\nanimatable 3d faces under text guidance. ACM Trans. Graph. (2023). 19\n[ZSD\u221721]\nZHANG X., SRINIVASAN P. P., DENG B., DEBEVEC P.,\nFREEMAN W. T., BARRON J. T.:\nNerfactor: Neural factorization of\nshape and reflectance under an unknown illumination. ACM Transac-\ntions on Graphics (ToG) 40, 6 (2021), 1\u201318. 5\n[ZSH\u221722]\nZHANG Y., SUN J., HE X., FU H., JIA R., ZHOU X.: Model-\ning indirect illumination for inverse rendering. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(2022), pp. 18643\u201318652. 6\n[ZST08]\nZHANG W., SUN J., TANG X.:\nCat head detection-how to\neffectively exploit shape and texture features.\nIn Computer Vision\u2013\nECCV 2008: 10th European Conference on Computer Vision, Marseille,\nFrance, October 12-18, 2008, Proceedings, Part IV 10 (2008), Springer,\npp. 802\u2013816. 15\n[ZTF\u221718]\nZHOU T., TUCKER R., FLYNN J., FYFFE G., SNAVELY N.:\nStereo magnification: Learning view synthesis using multiplane images.\narXiv preprint arXiv:1805.09817 (2018). 5\n[ZTNW23]\nZHANG\nB.,\nTANG\nJ.,\nNIESSNER\nM.,\nWONKA\nP.:\n3dshape2vecset: A 3d shape representation for neural fields and genera-\ntive diffusion models. arXiv preprint arXiv:2301.11445 (2023). 7\n[ZTS\u221716]\nZHOU T., TULSIANI S., SUN W., MALIK J., EFROS A. A.:\nView synthesis by appearance flow. In Computer Vision\u2013ECCV 2016:\n14th European Conference, Amsterdam, The Netherlands, October 11\u2013\n14, 2016, Proceedings, Part IV 14 (2016), Springer, pp. 286\u2013301. 14\n[ZVW\u221722]\nZENG X., VAHDAT A., WILLIAMS F., GOJCIC Z., LITANY\nO., FIDLER S., KREIS K.: Lion: Latent point diffusion models for 3d\nshape generation. arXiv preprint arXiv:2210.06978 (2022). 10\n[ZWL\u221723]\nZHUANG J., WANG C., LIU L., LIN L., LI G.: Dreamedi-\ntor: Text-driven 3d scene editing with neural fields. In SIGGRAPH Asia\nConference Papers (2023). 20\n[ZXA\u221723]\nZHU C., XIAO F., ALVARADO A., BABAEI Y., HU J., EL-\nMOHRI H., CULATANA S., SUMBALY R., YAN Z.:\nEgoobjects: A\nlarge-scale egocentric dataset for fine-grained object understanding. In\nProceedings of the IEEE/CVF International Conference on Computer\nVision (2023), pp. 20110\u201320120. 15\n[ZYHC22]\nZHENG M., YANG H., HUANG D., CHEN L.: ImFace: A\nnonlinear 3d morphable face model with implicit neural representations.\nIn CVPR (2022). 17\n[ZYLD21]\nZHENG Z., YU T., LIU Y., DAI Q.: Pamir: Parametric model-\nconditioned implicit representation for image-based human reconstruc-\ntion. IEEE transactions on pattern analysis and machine intelligence 44,\n6 (2021), 3170\u20133184. 16\n[ZYW\u221723]\nZHENG Y., YIFAN W., WETZSTEIN G., BLACK M. J.,\nHILLIGES O.: PointAvatar: Deformable point-based head avatars from\nvideos. In CVPR (2023). 17\n[ZZ23]\nZHU J., ZHUANG P.: Hifa: High-fidelity text-to-3d with advanced\ndiffusion guidance. arXiv preprint arXiv:2305.18766 (2023). 12, 18\n[ZZF\u221723]\nZHUANG Y., ZHANG Q., FENG Y., ZHU H., YAO Y., LI X.,\nCAO Y.-P., SHAN Y., CAO X.: Anti-aliased neural implicit surfaces\nwith encoding level of detail. arXiv preprint arXiv:2309.10336 (2023).\n6\n[ZZK\u221720]\nZAMORSKI M., ZI \u02dbEBA M., KLUKOWSKI P., NOWAK R.,\nKURACH K., STOKOWIEC W., TRZCI \u00b4NSKI T.:\nAdversarial autoen-\ncoders for compact representations of 3d point clouds. Computer Vision\nand Image Understanding 193 (2020), 102921. 7\n[ZZW\u221723]\nZHUANG Y., ZHANG Q., WANG X., ZHU H., FENG Y., LI\nX., SHAN Y., CAO X.: Neai: A pre-convoluted representation for plug-\nand-play neural ambient illumination. arXiv preprint arXiv:2304.08757\n(2023). 5\n[ZZZ\u221718]\nZHU J.-Y., ZHANG Z., ZHANG C., WU J., TORRALBA A.,\nTENENBAUM J., FREEMAN B.: Visual object networks: Image genera-\ntion with disentangled 3d representations. Advances in neural informa-\ntion processing systems 31 (2018). 15\n[ZZZ\u221723a]\nZHANG J., ZHANG X., ZHANG H., LIEW J. H., ZHANG C.,\nYANG Y., FENG J.: Avatarstudio: High-fidelity and animatable 3d avatar\ncreation from text. arXiv preprint arXiv:2311.17917 (2023). 16\n[ZZZ\u221723b]\nZHU J., ZHU H., ZHANG Q., ZHU F., MA Z., CAO X.:\nPyramid nerf: Frequency guided fast radiance field optimization. Inter-\nnational Journal of Computer Vision (2023), 1\u201316. 5\n"
  },
  {
    "title": "Anything in Any Scene: Photorealistic Video Object Insertion",
    "link": "https://arxiv.org/pdf/2401.17509.pdf",
    "upvote": "15",
    "text": "Anything in Any Scene: Photorealistic Video Object Insertion\nChen Bai, Zeman Shao, Guoxiang Zhang, Di Liang, Jie Yang, Zhuorui Zhang, Yujian Guo,\nChengzhang Zhong, Yiqiao Qiu, Zhendong Wang, Yichen Guan, Xiaoyin Zheng, Tao Wang, Cheng Lu\nXPeng Motors\n{chenbai, zemans, guoxiangz, liangd2, yangj23, zhangzr, guoyj4, chengzhangz, yiqiaoq\nzhendongw, guanyc, xiaoyinz, taow, luc}@xiaopeng.com\nAbstract\nRealistic video simulation has shown significant poten-\ntial across diverse applications, from virtual reality to film\nproduction. This is particularly true for scenarios where\ncapturing videos in real-world settings is either imprac-\ntical or expensive. Existing approaches in video simula-\ntion often fail to accurately model the lighting environ-\nment, represent the object geometry, or achieve high lev-\nels of photorealism. In this\u2018 paper, we propose Anything\nin Any Scene, a novel and generic framework for realistic\nvideo simulation that seamlessly inserts any object into an\nexisting dynamic video with a strong emphasis on physi-\ncal realism. Our proposed general framework encompasses\nthree key processes: 1) integrating a realistic object into\na given scene video with proper placement to ensure ge-\nometric realism; 2) estimating the sky and environmen-\ntal lighting distribution and simulating realistic shadows\nto enhance the light realism; 3) employing a style trans-\nfer network that refines the final video output to maximize\nphotorealism.\nWe experimentally demonstrate that Any-\nthing in Any Scene framework produces simulated videos\nof great geometric realism, lighting realism, and photo-\nrealism. By significantly mitigating the challenges asso-\nciated with video data generation, our framework offers\nan efficient and cost-effective solution for acquiring high-\nquality videos. Furthermore, its applications extend well\nbeyond video data augmentation, showing promising po-\ntential in virtual reality, video editing, and various other\nvideo-centric applications. Please check our project web-\nsite https://anythinginanyscene.github.io\nfor access to our project code and more high-resolution\nvideo results.\n1. Introduction\nThe image and video simulation has exhibited success in\nvarious applications, ranging from virtual reality to film\nproduction. The capability to generate diverse and high-\nquality visual content through realistic image and video\nsimulation holds the potential to advance these fields, in-\ntroducing new possibilities and applications. Although the\nimages and videos captured in real-world settings are in-\nvaluable for their authenticity, they often suffer from the\nlimitation of long-tail distribution. This results in common\nscenarios being over-represented, while rare yet crucial situ-\nations are under-represented, presenting a challenge known\nas the out-of-distribution problem. Traditional methods of\naddressing these limitations through video collection and\nediting prove impractical or excessively costly due to the\ninherent difficulty in encompassing all possible situations.\nThe significance of video simulation, especially through the\nintegration of existing videos with newly inserted objects,\nbecomes paramount in overcoming these challenges. By\ngenerating large-scale, diverse, and realistic visual content,\nvideo simulation contributes to the enhancement of appli-\ncations in virtual reality, video editing, and video data aug-\nmentation.\nHowever, generating a realistic simulated video with\nconsideration of physical realism is still a challenging open\nproblem. Existing methods often exhibit limitations by con-\ncentrating on specific settings, particularly indoor environ-\nments [9, 26, 45, 46, 57].\nThese methods may not ad-\nequately address the complexities of outdoor scenes, in-\ncluding diverse lighting conditions and fast-moving objects.\nMethods relying on 3D model registration are constrained\nin integrating only limited classes of objects [12, 32, 40,\n42].\nMany approaches neglect essential factors such as\nmodeling the lighting environment, proper object place-\nment, and achieving photorealism [12, 36]. Failed cases\nare illustrated in Figure 1.\nConsequently, these limita-\ntions significantly constrain their applications in fields that\nneed highly scalable, geometrically consistent, and realis-\ntic scene video simulation, such as autonomous driving and\nrobotics.\nIn this paper, we propose a comprehensive framework\nAnything in Any Scene for the photorealistic video object\ninsertion that addresses these challenges. The framework is\n1\narXiv:2401.17509v1  [cs.CV]  30 Jan 2024\n(a) The inserted car has an inconsistent shadow to\nanother car because of the wrong lighting environ-\nment estimated.\n(b) The car is in the air because of a wrong place-\nment location determined.\n(c) The inserted car in the scene has a signifi-\ncant difference in texture compared to another car,\nwhich makes the image lack photorealism.\nFigure 1. Examples of simulated video frame with wrong lighting environment estimation, false object placement position, and unrealistic\ntexture style, which make the image lack physical realism\ndesigned to have universal applicability, and is adaptable to\nboth indoor and outdoor scenes, ensuring physical accuracy\nin terms of geometric realism, lighting realism, and pho-\ntorealism. Our goal is to create video simulations that are\nnot only beneficial for visual data augmentation in machine\nlearning but also adaptable to various video applications,\nsuch as virtual reality and video editing.\nThe overview of our Anything in Any Scene framework\nis shown in Figure 2.\nWe detail our novel and scalable\npipeline for building a diverse asset bank of scene video and\nobject mesh in Section 3. We introduce a visual data query\nengine designed to efficiently retrieve relevant video clips\nfrom visual queries using descriptive keywords. Follow-\ning this, we present two methods for generating 3D meshes,\nleveraging existing 3D assets as well as multi-view image\nreconstructions. This allows the insertion of any desired\nobject without limitation, even if it is highly irregular or se-\nmantically weak. In Section 4, we detail our approach for\nintegrating objects into dynamic scene video with a focus\non maintaining physical realism. We design an object place-\nment and stabilization method described in Section 4.1, en-\nsuring the inserted object is stably anchored across contin-\nuous video frames. Addressing the challenge of creating\nrealistic lighting and shadow effects, we estimate sky and\nenvironmental lighting and generate realistic shadows dur-\ning the rendering process, as described in Section 4.2. The\nresulting simulated video frames inevitably contain unreal-\nistic artifacts that differ from real-world captured videos,\nsuch as imaging quality discrepancies in noise level, color\nfidelity, and sharpness. We adopt a style transfer network to\nenhance the photorealism in Section 4.3.\nThe simulated videos produced from our proposed\nframework reach a high degree of lighting realism, geo-\nmetrical realism, and photorealism, outperforming the oth-\ners both qualitatively and quantitatively as shown in Sec-\ntion 5.3. We further showcase in Section 5.4 the application\nof our simulated videos in the training perception algorithm\nto verify its practical value. The Anything in Any Scene\nframework is able to create a large-scale, low-cost video\ndataset for data augmentation with time efficiency and real-\nistic visual quality, which alleviates the burden of video data\ngeneration and potentially ameliorates the long-tail distri-\nbution and out-of-distribution challenges. With its generic\nframework design, the Anything in Any Scene framework\ncan easily incorporate improved models and new modules,\nsuch as an improved 3D mesh reconstruction method, fur-\nther enhancing video simulation performance.\nOur main contributions can be summarized as follows:\n1. We introduce a novel and scalable Anything in Any\nScene framework for video simulation, capable of inte-\ngrating any object into any dynamic scene video.\n2. Our framework uniquely focuses on preserving geomet-\nric realism, lighting realism, and photorealism in video\nsimulations, ensuring high-quality and realistic outputs.\n3. We conducted extensive validations, demonstrating the\nability of the framework to produce realistic video sim-\nulations, significantly expanding the scope and potential\napplication in this field.\n2. Related Work\nImage Synthesis and Editing: Encompassing tasks from\nimage inpainting to style transfer has attracted signifi-\ncant attention in both academic and industry communi-\nties. The traditional methods are mostly based on pixels,\npatches, and low-level image features, often lacking high-\nlevel semantic information. Specifically, the image inpaint-\ning methods replicate pixels or patches for image recov-\nery [2, 3, 10, 19, 27]. The non-parametric-based texture\nsynthesis methods re-sample the pixels of a given source\ntexture to generate photorealistic textures [13, 28].\nThe\nstyle transfer methods, such as image analogies [21], per-\nform example-based stylization using patches.\nDeep learning networks, particularly Generative Adver-\nsarial Networks (GAN) [17], have demonstrated significant\ncapabilities in computer vision and image processing tasks,\nachieving impressive success in image generation.\nVari-\nous GANs, such as MGANs [30], SGAN [25], and PS-\nGAN [4], have shown remarkable proficiency in the task\n2\nFigure 2. Overview of proposed Anything in Any Scene framework for photorealistic video object insertion\nof texture synthesis. Additionally, GANs have been suc-\ncessfully applied to contextual image inpainting [38] and\nmulti-scale image completion [59]. The pix2pix [24] and\ncycleGAN [62] leverage GAN architecture to train gener-\native models for style transfer. The images generated by\nGANs tend to be less blurred and exhibit higher realism,\naligning closely with distributions of training image data.\nVideo Synthesis and Editing: Transitioning from im-\nage to video synthesis requires addressing additional chal-\nlenges, particularly maintaining temporal consistency.\nUnconditional\nvideo\nsynthesis\nmethods,\nsuch\nas [44], [52], and [53], take a random noise as input\nand model both spatial and temporal correlation to generate\nvideo. However, they often result in constrained motion\npatterns in output video sequences. In contrast, conditional\nvideo synthesis methods employ conditional GAN [37]\nto train a generative model for video generation based on\ninput content.\nIn [55] and its following work [56], the\ngenerative network is conditioned on the previous frame\nof the source video for each subsequent frame generation.\n[34] take this approach further by considering all previously\ngenerated frames, achieving improved long-term temporal\nconsistency in their video synthesis.\nAdditionally, the automatic video synthesis methods pro-\nposed in [29] and [23] insert the object\u2019s video into another\nvideo using spatial and temporal information. Recently, the\nGeoSim framework proposed in [7] has achieved impres-\nsive results in car insertion into a given real-world driving\nscene video, though its application to less common objects\nand diverse types of scene video remains limited. Our work\nseeks to bridge this gap, expanding the potential for any ob-\nject insertion in any scene video.\n3. Scene Video and Object Mesh Assets Bank\nOur goal with the Anything in Any Scene framework is to\ngenerate large-scale and high-quality simulation videos by\ncomposition of dynamic scene videos and objects of inter-\nest. To achieve this, an assets bank of both scene videos and\nobject meshes is required for simulated video composition.\nIn order to efficiently locate target videos for composi-\ntion from a large-scale video assets bank, we proposed a\nvisual data query engine that is used to retrieve the relevant\nscene video clips for simulated video composition based on\nthe given visual clue descriptors. The mesh model of the\ntarget object is required before its insertion into an exist-\ning video clip. We introduced the 3D mesh generation of\nthe target object by using the Houdini Engine from existing\n3D assets and a NeRF-based 3D reconstruction from multi-\nview images, which enables theatrically unlimited classes\nof objects to be inserted into the existing scene video.\nDetailed descriptions of our mesh assets bank can be\nfound in supplementary materials.\n4. Realistic Video Simulation\nTo achieve video simulation with geometric realism, light-\ning realism, and photorealism, our proposed framework\nconsists of the following three main components:\n1. Object Placement and Stabilization (Section 4.1)\n2. Lighting and Shadow Generation (Section 4.2)\n3. Photorealistic Style Transfer (Section 4.3)\n4.1. Object Placement and Stabilization\nInserting an object into a background video for video com-\nposition requires the object placement location determined\nfor each frame in the video sequence. We designed and\nproposed a novel object placement method with the consid-\neration of occlusion with other existing objects in the scene,\nwhich is described in Section 4.1.1.\nHowever, placement locations that are independently\nestimated from each single frame could yield unrealistic\nmovement tracks since the video temporal information has\nnot been considered. To address this issue, we propose an\nobject placement stabilization method in Section 4.1.2 to\ncorrect the placement location in each frame. We employ\noptical flow tracking between consecutive frames to ensure\nthe inserted object behaves realistically across the continu-\nous video frames.\n3\n(a) The first frame of video clip I1\n(b) The estimated segmentation mask \u02c6\nM1\n(c) The object placement location in 3D scene\nFigure 3. Example of driving scene video for object placement. The red point in each image is the location for object insertion.\n4.1.1\nObject Placement\nSuppose there are N + T continuous frames, the first N\nframes are the target frames that we aim to integrate the\ninserted object into, and the last T frames are used as ref-\nerence for object placement. We assume the world coor-\ndinate of camera location in the frame IN+T is the ori-\ngin Ow = [0, 0, 0, 1], and the camera coordinate system\nis aligned with the world coordinate system at this frame\nIN+T . We place the inserted object at the location of ori-\ngin in the world coordinate which is the same location as\nthe camera itself in the frame IN+T . To determine the pixel\ncoordinates for object placement in the first N consecutive\nframes, we project the origin from the world coordinate to\nthe pixel coordinate based on the camera intrinsic matrix\nK and the camera pose including rotation matrix Rn and\ntranslation vector tn at each frame In. The placement pixel\ncoordinate \u02dcon at the frame In is determined by:\n\u02dcon = K[Rn|tn]Ow\n(1)\nThe placement of the inserted object within a video clip\nshould avoid occlusion with other existing objects in the\nscene. We estimated the semantic segmentation mask \u02c6\nMn\nfor each frame In by using off-the-shelf models. The pixel\n\u02c6\nMn(\u02dcon) denotes the category at the pixel location \u02dcon, repre-\nsenting the origin in the world coordinate projected into the\npixel coordinate in the frame In. This predicted category\nserves as a reference to determine whether the projected\npoint location for object insertion is occluded by other ob-\njects in the scene.\nWe show an example of a driving scene in Figure 3. The\nfirst frame of the video clip and its associated estimated seg-\nmentation mask are shown in Figure 3a and Figure 3b. The\nred point in Figure 3c is the origin of the world coordinate\nand also the camera location in the frame IN+T , we placed\nthe object at this location. As the estimated segmentation\nmask shown in Figure 3b, the green region indicates the\nroad area and the red region indicates the road lane. Af-\nter the object placement location is projected back from the\nworld coordinate to the pixel coordinate, the placement is\nlocated in the road area as indicated in the semantic seg-\nmentation, which is a plausible place to insert a road vehicle\nin a driving scene video.\n4.1.2\nObject Placement Stabilization\nFirstly, we select a 3D point with world coordinate Pw =\n[X, Y, Z, 1], and follow the Equation 1 to project it from\nthe world coordinate into the pixel coordinate \u02dcpn in each\nframe In of the first N + 1 frames. We then estimate the\noptical flow between each two consecutive frames and ob-\ntain the selected 3D point Pw pixel coordinate \u02c6pn in the\nframe In through the image warping of \u02dcpn+1 and the esti-\nmated optical flow. The object placement stabilization can\nbe interpreted as the optimization of camera pose for each\nframe In. Specifically, we optimize the camera pose ro-\ntation matrix Rn and translation vector tn at each frame\nIn by minimizing the 3D-to-2D projection error of \u02c6pn with\nthe comparison to \u02dcpn. To achieve a better performance in\nplacement stabilization, we select M points and optimize\nthe rotation matrix R\u2032\nn and translation vector t\u2032\nn, which can\nbe expressed as:\n(R\u2032\nn, t\u2032\nn) = arg min\nM\nX\ni=1\n(\u02c6pn \u2212 \u02dcpn)2\n= arg min\n(Rn,tn)\nM\nX\ni=1\n(\u02c6pn \u2212 K[Rn|tn]Pw)2\n(2)\nLastly, we update the rotation matrix and translation vec-\ntor in Equation 1 by R\u2032\nn and t\u2032\nn, and calculate the updated\nobject placement pixel coordinate \u02dcon for each frame In.\nWe also adjust X and Y values of the selected 3D point\nPw to ensure that the projected 2D point can be tracked in\nconsecutive frames based on the estimated optical flow. For\nexample in the driving scene view, we shifted the selected\n3D points by adjusting the Y value so that the projected 2D\npoints are the corner points of the white road lane.\n4.2. Lighting Estimation and Shadow Generation\nOne important key to creating a realistic simulated video\nwith an integrated object is to generate accurate lighting\nand shading effects for the inserted object. The position\nand luminance of the lighting in the scene, such as the sun\n4\nOriginal Image\nHDR Image\nLighting Distribution\nFigure 4. Examples of original sky image, reconstructed HDR\nimage, and its associated sun lighting distribution map\nfor the outdoor scene and the environment for the indoor\nscene, affect the inserted object\u2019s visual appearance during\nthe rendering process.\nTo simulate an accurate lighting and shading effect\nduring the rendering process, we first introduced a High\nDynamic Range (HDR) panoramic image reconstruction\nmethod in Section 4.2.1. Lastly, we rendered the shadow\nof the inserted object based on the estimated position of the\nmain lighting source in Section 4.2.2.\n4.2.1\nHDR Panoramic Image Reconstruction\nThe Low Dynamic Range (LDR) images captured by the\nconsumer camera are usually over-saturated due to the ex-\ntremely high brightness of the main lighting compared to\nsurrounding environmental lighting, which makes it much\nmore difficult to estimate the position and luminance distri-\nbution of the main lighting. To address this issue, we first\nuse an image inpainting network to infer the surround view\nof lighting distribution for rendering. We then adapt a sky\nHDR reconstruction network to identify the lighting source\nposition and generate the HDR panoramic image.\nPanorama Image Inpainting: The image captured by\nthe consumer camera has a limited Field of View (FOV),\nwhich leads to missing lighting in the rendering process.\nWe address this task by translating it into an inpainting task\nwhich infers a panorama image from a limited FOV im-\nage. Furthermore, we aim to infer the surround view im-\nage by using a diffusion model [22, 50]. We proposed to\nuse an image-to-image diffusion model which is a condi-\ntional diffusion model that converts samples from a stan-\ndard Gaussian distribution into samples from a data distri-\nbution through an iterative denoising process conditional\non an input. In our task, we adapt an existing model [43]\nand make it conditional on the input image to generate a\npanoramic image.\nLuminance Distribution Estimation: The HDR image\nreconstruction method proposed in [47] utilizes a Genera-\ntive Adversarial Network (GAN) to train encoder-decoder\nnetworks that model the sun and sky luminance distribu-\ntion. The input is a single outdoor LDR panoramic image\nand a U-Net [41] architecture network with ResNet [20] as\nits backbone is used to estimate the sky region luminance\n(a) Original Environmental Panoramic Image\n(b) Reconstructed HDR Environmental Panoramic Image\nFigure 5. Examples of Original and Reconstructed HDR Environ-\nmental Panoramic Image\ndistribution Lsky.\nAnother modified VGG16 network [49] is employed to\nestimate the sun position probability map xi,j which rep-\nresents the probability at pixel (i, j) in the input LDR\npanoramic image containing the sun. The output feature\nmaps from the CNN blocks the VGG are concatenated to-\ngether as input fed into the convolutional layers for encod-\ning the sun radiance map which is the Dirac delta function\nexpressed by:\n\u03b4(xi,j, \u03c4, \u03b2) =\n\u03c4\n\u03b2\u221a\u03c0 exp(\u2212(1 \u2212 xi,j)2\n\u03b2\n)\n(3)\nwhere \u03c4 and \u03b2 are the transmittance and sharpness values\nof the sky. The sun radiance map is then merged with sun\nregions to generate the sun region luminance distribution\nLsun. The Lsun and Lsky are applied to an inverse tone\nmapping operation and blended to generate the final output\nHDR map L.\nWe adapt this method in our lighting estimation module\nand follow the same process as described in [47] that uses\nGAN to re-train the network for generating HDR map L.\nWe then applied L to the inserted object in the video frame.\nEnvironmental HDR Image Reconstruction: As for\nthe outdoor scenario, the sun as the main lighting is not the\nonly one that can affect the visual appearance of the inserted\nobject, we also need to consider the environmental lighting\ndue to the diffuse reflection in order to achieve more real-\nistic rendering outcomes. To reconstruct the environmental\nHDR image, we collect multiple side-view LDR images of\nthe scene and recover them into HDR images by using an\nexisting model to learn the continuous exposure value repre-\nsentations [6]. We followed the same process to estimate the\ncamera extrinsic parameters for each side-view image and\nstitch them into one HDR panoramic image (Example of\nthe environmental HDR image as shown in Figure 5). Thus\nwe obtained the estimated environmental light distribution\nfrom the multiple side-view images, then we can apply it to\nthe inserted object rendering process.\n5\n(a) Image with generated shadow\n(b) Associated segmentation mask\nFigure 6. Example of generated shadow for the inserted object\n4.2.2\nObject Shadow Generation\nSince we\u2019ve estimated the location and distribution of the\nmain lighting source, i.e. sun for outdoor scene and light\nfor indoor scene, we rendered the shadow of the inserted\nobject by the 3D graphics application Vulkan [54] which\noffers higher performance and more efficient computing re-\nsource usage. Furthermore, we integrated the ray tracing\ninto the Vulkan application for a better performance of real-\nistic rendering [16]. Examples of the generated shadow for\nthe inserted objects are shown in Figure 6.\n4.3. Photorealistic Style Transfer\nThe simulated videos inevitably contain unrealistic arti-\nfacts, such as inconsistent illumination and color balancing,\nwhich are not included in videos captured in the real-world\nscenario. To address this issue, we proposed to use an im-\nage inpainting network that faithfully transfers the style to\nenhance the photorealism of simulated video sequences.\nSpecifically, we adapt the coarse-to-fine mechanism pro-\nposed in [61], which is originally designated to inpaint\nmissing regions in an image. We utilized the coarse net-\nwork and refinement network in [61], both of them consist\nof dilated convolution layers to generate the refined image\nbased on the input image. We modified the input configu-\nration for the two networks. The coarse network takes an\nimage with black pixels filled in the foreground region, a\nbinary mask indicating the foreground region, and a fore-\nground image of the inserted object with black pixels filled\nin the background region. The refinement network takes the\nsame input as the coarse network along with output from the\ncoarse network, and it generates final refined image results.\nTo train the generative model, we adopt the same training\nstrategy proposed in [61] which uses the WGAN [1] loss,\nand its objective function can be expressed as:\nmin\nG max\nD\u2208D Ex\u223c Pr[D(x)] \u2212 E\u02dcx\u223c Pg[D(\u02dcx)]\n(4)\nwhere D is the set of 1-Lipschitz functions, Pr is the data\ndistribution and Pg is the model distribution implicitly de-\nfined by \u02dcx = G(z), and z is the input to the generator.\nWe added the gradient penalty term proposed in [18] to\nimprove the WGAN and applied it to pixels in the fore-\nground region. Thus the penalty function can be expressed\nas:\n\u03bbE\u02c6x\u223c P\u02c6x(||\u25bd\u02c6xD(\u02c6x) \u2299 (1 \u2212 m)||2 \u2212 1)2\n(5)\nwhere \u02c6x sampled from the straight line between points sam-\npled from the distribution Pr and Pg, and m is the input\nbinary mas of the foreground region.\n5. Experimental Evaluation\nIn this section, we describe the evaluation details of our pro-\nposed method for video simulation. We introduce evalua-\ntion metrics in Section 5.1 to quantify performance. The\nvideo datasets covering both indoor and outdoor scenes\nused for validation are listed in Section 5.2. We perform an\nablation analysis to evaluate the effectiveness of each mod-\nule of our framework in Section 5.3. Lastly, we showcase\nthe application of the framework in downstream perception\ntasks in Section 5.4.\n5.1. Evaluation Metrics\nWe adopt the following two evaluation metrics used in [7]\nto assess the quality of simulated videos generated by our\nproposed framework. We report the average values for each\nmetric across all video frames in a dataset.\nHuman Score: This metric measures the percentage of\nparticipants who prefer the results from one method over\nthose from the baseline method in a human A/B test. De-\ntailed descriptions of human study can be found in supple-\nmentary materials. Additionally, the complete set of video\npairs and GUI application used in this study is available\non our website at https://anythinginanyscene.\ngithub.io. We encourage peer researchers to download\nand review these video comparisons, or to conduct their\nown human studies for verification of our results.\nFrechet Inception Distance (FID): This metric quan-\ntifies the realism and diversity of the generated images by\ncomparing the distribution of generated images with that of\ngroundtruth images. Lower scores indicate greater similar-\nity, with a zero score implying identical image sets.\n5.2. Evaluation Data\nTo demonstrate the performance of our method for realistic\nvideo composition of various scene videos and objects, we\nvalidate our method using both outdoor and indoor scene\nvideo datasets and diverse inserted object items.\nOutdoor Scene Video: PandaSet [58] is a multi-modal\ndataset capturing self-driving scenes in various conditions,\nincluding different times of day and weather. We utilized 95\nout of all 103 video clips from this dataset, each containing\n8 seconds of frames sampled at 10 Hz.\nIndoor Scene Video: ScanNet++ [60] is a large-scale\ndataset of indoor scenes created by 3D scanning real envi-\nronments The dataset includes DSLR images, RGB-D se-\nquences, and semantic and instance annotations, providing\n6\n(a) DoveNet\n(b) StyTR2\n(c) PHDiffusion\n(d) Ours\nFigure 7. Qualitative comparison of the simulated video frame from PandaSet dataset using different style transfer networks.\na comprehensive resource for evaluating our methods. We\nprovide the experimental results of the indoor scene video\ndataset in the supplementary materials.\nObject Mesh Assets: We used the methods introduced\nin Section 3 to generate 3D object meshes, focusing on vari-\nous objects, including different types of vehicles and pedes-\ntrian models.\n5.3. Experimental Results\nTo assess the performance of various style transfer\nnetworks, we compared different methods:\na CNN-\nbased method DoveNet [8], transformer-based method\nStyTR2 [11], diffusion model-based method PHDiffu-\nsion [33], and our method introduced in Section 4.3. For\nthe human study, we use our framework without the style\ntransfer module as the baseline for comparison. We sum-\nmarize the result of the comparison in Table 1. Our trans-\nfer network achieved the lowest FID at 3.730 and the high-\nest human score at 61.11%, outperforming the alternative\nmethods.\nAblation Studies: To investigate the effectiveness of\neach key module, we conducted ablation studies and evalu-\nated the performance. We removed one module from our\nframework at a time: placement (w/o placement), HDR\nimage reconstruction (w/o HDR), shadow generation (w/o\nshadow), and style transfer (w/o style transfer).\nIn this\nhuman study, the w/o style transfer method served as the\nbaseline, and was compared to all other ablation methods.\nThe results are summarized in Table 2.\nThe absence of\nplacement, HDR, and style transfer modules resulted in\nhigher FIDs.\nNotably, adding shadows significantly en-\nhanced the perceived realism for human observers, though\nthis improvement was not proportionately reflected in the\nFID score. This discrepancy suggests a potential gap be-\ntween computational assessments of perceptual quality and\nhuman judgment, as also noted in previous research [7].\nOur proposed method achieved a human score above 50%,\nand the others scored below 50%, highlighting the contribu-\nMethod\nHuman Score(%)\nFID\nProposed method\n61.11\n3.730\nStyTR2 style transfer\n58.89\n4.091\nPHDiffusion style transfer\n47.22\n4.554\nDoveNet style transfer\n47.78\n3.999\nw/o style transfer\nN/A\n4.499\nTable 1. Experimental results for different style transfer networks\nplugged into our Anything in Any Scene framework.\nMethod\nHuman Score(%)\nFID\nProposed method\n61.11\n3.730\nw/o placement\n25.56\n4.327\nw/o HDR\n43.05\n3.793\nw/o shadow\n37.78\n3.485\nw/o style transfer\nN/A\n4.499\nTable 2. Experimental results for ablation analysis of modules in\nour Anything in Any scene framework. Note that the baseline w/o\nstyle transfer method theoretically has a human score of 50%\ntion of each module in our proposed framework.\nQualitative comparison: In Figure 7, we provide a\nqualitative comparison of sample video frames using dif-\nferent style transfer networks applied to the outdoor scene\ndataset PandaSet. Figure 7a, 7b and 7c show images re-\nfined by DoveNet, StyTR2, and PHDiffusion, respectively.\nThe inserted object in these images exhibits a color tone\nthat is not consistent with the scene\u2019s lighting and weather\nconditions. Conversely, the image refined by our proposed\nmethod as shown in Figure 7d demonstrates the best visual\nquality among the four, aligning with the results reported\nin Table 1 that show our method outperforming others in\nboth FID and human study scores. This indicates that an\nimproved style transfer network can significantly enhance\nphotorealism within our Anything in Any Scene framework.\nFurthermore, we evaluate the visual quality of videos\ngenerated by the Anything in Any Scene framework by\n7\n(a) w/o placement\n(b) w/o shadow\n(c) w/o HDR\n(d) w/o style transfer\n(e) Ours\nFigure 8. Qualitative comparison of the simulated video frame from PandaSet dataset under various rendering conditions.\nremoving one module at a time, using the outdoor scene\ndataset PandaSet as a reference. This evaluation is visually\nillustrated with two comparison samples in Figure 8. In Fig-\nure 8c and Figure 8d, we observe that the inserted object ex-\nhibits color textures that are inconsistent with the surround-\ning environment and other objects in the scene. Further-\nmore, Figure 8b highlights an instance where the inserted\nobject lacks a generated shadow. This absence creates a\nvisual effect where the object appears as if it is in the air,\nhighlighting the importance of shadow rendering for real-\nistic simulation. In contrast, Figure 8e shows the visual\nquality of videos generated by our framework, where the\ninserted object displays a high degree of consistency with\nthe scene in terms of geometry, lighting, and overall photo-\nrealism. This demonstrates the capability of the Anything\nin Any Scene framework to achieve realistic integration of\nobjects into diverse scene settings.\n5.4. Downstream Perception Evaluation\nReal-world datasets often exhibit a long-tailed class distri-\nbution, where a few common classes are over-represented,\nwhile a majority of classes are under-represented. This im-\nbalance poses significant challenges for deep learning mod-\nels, leading to biases towards common classes during train-\ning and worse performance on rare classes during inference.\nTo address this problem, we investigate the usage of the\nAnything in Any Scene framework to generate synthetic im-\nages containing rare cases for data augmentation. We per-\nform the evaluation on the CODA dataset [31], an amal-\ngamation of image data from KITTI [15], nuScenes [5],\nand ONCE [35] datasets, including 1,500 real-world driv-\ning scenes and over 30 object categories\nThe goal of this task is to insert 9 different rare ob-\nject categories into images from the CODA2022 validation\ndataset, with each category comprising less than 0.4% of\ntotal bounding boxes. We trained three models: YOLOX-S,\nYOLOX-L, and YOLOX-X [14], on a subset of 2930 im-\nages from the dataset, reserving another 977 images for test-\nMethod\nData\nmAP\nYOLOX-S\nOriginal\n0.186\n0.037 \u2191\nOriginal + Ours\n0.223\nYOLOX-L\nOriginal\n0.260\n0.011 \u2191\nOriginal + Ours\n0.271\nYOLOX-X\nOriginal\n0.249\n0.026 \u2191\nOriginal + Ours\n0.275\nTable 3. Performance of the YOLOX models trained on the origi-\nnal images from the CODA dataset compared to their performance\nwhen trained on a combination of original and augmented images\nusing our Anything in Any Scene framework. We report the mAP\nthat represents the mean for all 9 object categories.\ning. We then employed our Anything in Any Scene frame-\nwork to augment these training images by inserting vari-\nous objects into them. This process produced an augmented\nset of training images that replaced the original ones in the\ntraining dataset. We applied the same training strategy and\nre-train the models on the augmented training dataset.\nWe evaluate the performance of the three models by\ntraining them on both the original and the augmented\ndatasets, followed by testing them on the same test dataset.\nThe results, detailed in Table 3, indicate an improvement\nin mean Average Precision (mAP) for all three models.\nSpecifically, there is an enhancement of 3.7% in mAP for\nYOLOX-S, 1.1% for YOLOX-L, and 2.6% for YOLOX-X.\n6. Conclusion\nIn this work, we proposed an innovative and scalable\nframework, Anything in Any Scene, designed for real-\nistic video simulation.\nOur proposed framework seam-\nlessly integrates a wide range of objects into diverse dy-\nnamic videos, ensuring the preservation of geometric real-\nism, lighting realism, and photorealism. Through exten-\nsive demonstrations, we have shown its efficacy in allevi-\nating challenges associated with video data collection and\ngeneration, offering a cost-effective and time-efficient so-\nlution adaptable to a variety of scenarios.\nThe applica-\n8\ntion of our framework has shown notable improvements\nin downstream perception tasks, particularly in addressing\nthe long-tailed distribution issue in object detection. The\nflexibility of our framework allows for straightforward in-\ntegration of improved models for each of its modules, our\nframework stands as a robust foundation for future explo-\nrations and innovations in the field of realistic video simu-\nlation.\nReferences\n[1] Martin Arjovsky, Soumith Chintala, and L\u00b4eon Bottou.\nWasserstein gan, 2017. 6\n[2] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and\nDan B Goldman.\nPatchmatch: A randomized correspon-\ndence algorithm for structural image editing. ACM Trans.\nGraph., 28(3):24, 2009. 2\n[3] Connelly Barnes, Fang-Lue Zhang, Liming Lou, Xian Wu,\nand Shi-Min Hu. Patchtable: Efficient patch queries for large\ndatasets and applications. ACM Transactions on Graphics\n(ToG), 34(4):1\u201310, 2015. 2\n[4] Urs Bergmann, Nikolay Jetchev, and Roland Vollgraf. Learn-\ning texture manifolds with the periodic spatial gan. arXiv\npreprint arXiv:1705.06566, 2017. 2\n[5] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom.\nnuscenes: A multi-\nmodal dataset for autonomous driving. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11621\u201311631, 2020. 8\n[6] Su-Kai Chen, Hung-Lin Yen, Yu-Lun Liu, Min-Hung Chen,\nHou-Ning Hu, Wen-Hsiao Peng, and Yen-Yu Lin. Learning\ncontinuous exposure value representations for single-image\nhdr reconstruction. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 12990\u201313000,\n2023. 5\n[7] Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang,\nXinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin\nYumer, and Raquel Urtasun. Geosim: Realistic video sim-\nulation via geometry-aware composition for self-driving. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 7230\u20137240, 2021. 3, 6,\n7\n[8] Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling,\nWeiyuan Li, and Liqing Zhang. Dovenet: Deep image har-\nmonization via domain verification. In CVPR, 2020. 7, 3\n[9] Erwin Coumans and Yunfei Bai. Pybullet, a python mod-\nule for physics simulation for games, robotics and machine\nlearning. 2016. 1\n[10] Antonio Criminisi, Patrick Perez, and Kentaro Toyama. Ob-\nject removal by exemplar-based inpainting. In 2003 IEEE\nComputer Society Conference on Computer Vision and Pat-\ntern Recognition, 2003. Proceedings., pages II\u2013II. IEEE,\n2003. 2\n[11] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma,\nXingjia Pan, Lei Wang, and Changsheng Xu. Stytr2: Im-\nage style transfer with transformers.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11326\u201311336, 2022. 7, 3\n[12] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Anto-\nnio Lopez, and Vladlen Koltun. Carla: An open urban driv-\ning simulator. In Conference on robot learning, pages 1\u201316.\nPMLR, 2017. 1\n[13] Alexei A Efros and Thomas K Leung.\nTexture synthesis\nby non-parametric sampling. In Proceedings of the seventh\nIEEE international conference on computer vision, pages\n1033\u20131038. IEEE, 1999. 2\n[14] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\nSun. Yolox: Exceeding yolo series in 2021. arXiv preprint\narXiv:2107.08430, 2021. 8, 5\n[15] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? the kitti vision benchmark\nsuite. In Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2012. 8\n[16] GitHub.\nRay tracing examples and tutorials,\n2023.\nhttps : / / github . com / nvpro - samples / vk _\nraytracing_tutorial_KHR/tree/master [Ac-\ncessed: (October 16, 2023)]. 6\n[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139\u2013144, 2020. 2\n[18] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent\nDumoulin, and Aaron C Courville.\nImproved training of\nwasserstein gans. Advances in neural information processing\nsystems, 30, 2017. 6\n[19] James Hays and Alexei A Efros. Scene completion using\nmillions of photographs.\nACM Transactions on Graphics\n(ToG), 26(3):4\u2013es, 2007. 2\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 5\n[21] Aaron Hertzmann, Charles E Jacobs, Nuria Oliver, Brian\nCurless, and David H Salesin. Image analogies. In Sem-\ninal Graphics Papers: Pushing the Boundaries, Volume 2,\npages 557\u2013570. 2023. 2\n[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 5\n[23] Hao-Zhi Huang, Sen-Zhe Xu, Jun-Xiong Cai, Wei Liu, and\nShi-Min Hu. Temporally coherent video harmonization us-\ning adversarial networks. IEEE Transactions on Image Pro-\ncessing, 29:214\u2013224, 2019. 3\n[24] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. CVPR, 2017. 3\n[25] Nikolay Jetchev, Urs Bergmann, and Roland Vollgraf. Tex-\nture synthesis with spatial generative adversarial networks.\narXiv preprint arXiv:1611.08207, 2016. 2\n[26] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt,\nLuca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani,\nDaniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d\nenvironment for visual ai. arXiv preprint arXiv:1712.05474,\n2017. 1\n9\n[27] Nikos Komodakis and Georgios Tziritas. Image completion\nusing efficient belief propagation via priority scheduling and\ndynamic pruning. IEEE Transactions on Image Processing,\n16(11):2649\u20132661, 2007. 2\n[28] Vivek Kwatra, Arno Sch\u00a8odl, Irfan Essa, Greg Turk, and\nAaron Bobick. Graphcut textures: Image and video synthe-\nsis using graph cuts. Acm transactions on graphics (tog), 22\n(3):277\u2013286, 2003. 2\n[29] Donghoon Lee, Tomas Pfister, and Ming-Hsuan Yang. In-\nserting videos into videos. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10061\u201310070, 2019. 3\n[30] Chuan Li and Michael Wand. Precomputed real-time texture\nsynthesis with markovian generative adversarial networks.\nIn Computer Vision\u2013ECCV 2016: 14th European Confer-\nence, Amsterdam, The Netherlands, October 11-14, 2016,\nProceedings, Part III 14, pages 702\u2013716. Springer, 2016. 2\n[31] Kaican Li, Kai Chen, Haoyu Wang, Lanqing Hong, Chao-\nqiang Ye, Jianhua Han, Yukuai Chen, Wei Zhang, Chunjing\nXu, Dit-Yan Yeung, et al.\nCoda: A real-world road cor-\nner case dataset for object detection in autonomous driving.\narXiv preprint arXiv:2203.07724, 2022. 8\n[32] Wei Li, CW Pan, Rong Zhang, JP Ren, YX Ma, Jin Fang,\nFL Yan, QC Geng, XY Huang, HJ Gong, et al. Aads: Aug-\nmented autonomous driving simulation using data-driven al-\ngorithms. Science robotics, 4(28):eaaw0863, 2019. 1\n[33] Lingxiao Lu, Jiangtong Li, Junyan Cao, Li Niu, and Liqing\nZhang.\nPainterly image harmonization using diffusion\nmodel. In Proceedings of the 31st ACM International Con-\nference on Multimedia, pages 233\u2013241, 2023. 7, 3\n[34] Arun Mallya, Ting-Chun Wang, Karan Sapra, and Ming-Yu\nLiu. World-consistent video-to-video synthesis. In Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part VIII 16, pages\n359\u2013378. Springer, 2020. 3\n[35] Jiageng Mao, Minzhe Niu, Chenhan Jiang, Xiaodan Liang,\nYamin Li, Chaoqiang Ye, Wei Zhang, Zhenguo Li, Jie Yu,\nChunjing Xu, et al. One million scenes for autonomous driv-\ning: Once dataset. 2021. 8\n[36] Mark Martinez, Chawin Sitawarin, Kevin Finch, Lennart\nMeincke, Alex Yablonski, and Alain Kornhauser.\nBe-\nyond grand theft auto v for training, testing and enhanc-\ning deep learning in self driving cars.\narXiv preprint\narXiv:1712.01397, 2017. 1\n[37] Mehdi Mirza and Simon Osindero. Conditional generative\nadversarial nets. arXiv preprint arXiv:1411.1784, 2014. 3\n[38] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor\nDarrell, and Alexei A Efros.\nContext encoders: Feature\nlearning by inpainting.\nIn Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n2536\u20132544, 2016. 3\n[39] QIU023.\nGuivideodisplayselector:\nA simple tkinter-\nbased gui application for video comparison and selec-\ntion.,\n2023.\nhttps : / / github . com / QIU023 /\nGUIVideoDisplaySelector [Accessed: (November 8,\n2023)]. 4\n[40] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen\nKoltun.\nPlaying for data: Ground truth from computer\ngames.\nIn Computer Vision\u2013ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part II 14, pages 102\u2013118. Springer,\n2016. 1\n[41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 5\n[42] German Ros, Laura Sellart, Joanna Materzynska, David\nVazquez, and Antonio M Lopez. The synthia dataset: A large\ncollection of synthetic images for semantic segmentation of\nurban scenes.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3234\u20133243,\n2016. 1\n[43] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi.\nPalette: Image-to-image diffusion models.\nIn\nACM SIGGRAPH 2022 Conference Proceedings, pages 1\u2013\n10, 2022. 5\n[44] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Tempo-\nral generative adversarial nets with singular value clipping.\nIn Proceedings of the IEEE international conference on com-\nputer vision, pages 2830\u20132839, 2017. 3\n[45] Manolis Savva, Angel X Chang, Alexey Dosovitskiy,\nThomas Funkhouser, and Vladlen Koltun.\nMinos: Multi-\nmodal indoor simulator for navigation in complex environ-\nments. arXiv preprint arXiv:1712.03931, 2017. 1\n[46] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia\nLiu, Vladlen Koltun, Jitendra Malik, et al.\nHabitat: A\nplatform for embodied ai research.\nIn Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 9339\u20139347, 2019. 1\n[47] Gyeongik Shin, Kyeongmin Yu, Mpabulungi Mark, and\nHyunki Hong. Hdr map reconstruction from a single ldr sky\npanoramic image for outdoor illumination estimation. IEEE\nAccess, 2023. 5\n[48] SideFX. Unreal plug-in, 2023. https://www.sidefx.\ncom / products / houdini - engine / plug - ins /\nunreal-plug-in/ [Accessed: (October 24, 2023)]. 1\n[49] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 5\n[50] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n5\n[51] Jiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Er-\nrui Ding, Jingdong Wang, and Gang Zeng. Delicate textured\nmesh recovery from nerf via adaptive surface refinement.\narXiv preprint arXiv:2303.02091, 2023. 2\n[52] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan\nKautz.\nMocogan: Decomposing motion and content for\nvideo generation. In Proceedings of the IEEE conference on\n10\ncomputer vision and pattern recognition, pages 1526\u20131535,\n2018. 3\n[53] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.\nGenerating videos with scene dynamics. Advances in neu-\nral information processing systems, 29, 2016. 3\n[54] Vulkan. Vulkan, cross platform 3d graphics, 2023. https:\n//www.vulkan.org/ [Accessed: (October 16, 2023)]. 6\n[55] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu,\nAndrew Tao, Jan Kautz, and Bryan Catanzaro.\nVideo-to-\nvideo synthesis. arXiv preprint arXiv:1808.06601, 2018. 3\n[56] Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu,\nJan Kautz, and Bryan Catanzaro. Few-shot video-to-video\nsynthesis. arXiv preprint arXiv:1910.12713, 2019. 3\n[57] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra\nMalik, and Silvio Savarese. Gibson env: Real-world percep-\ntion for embodied agents. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n9068\u20139079, 2018. 1\n[58] Pengchuan Xiao, Zhenlei Shao, Steven Hao, Zishuo Zhang,\nXiaolin Chai, Judy Jiao, Zesong Li, Jian Wu, Kai Sun,\nKun Jiang, et al. Pandaset: Advanced sensor suite dataset\nfor autonomous driving.\nIn 2021 IEEE International In-\ntelligent Transportation Systems Conference (ITSC), pages\n3095\u20133101. IEEE, 2021. 6, 2\n[59] Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang,\nand Hao Li. High-resolution image inpainting using multi-\nscale neural patch synthesis. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n6721\u20136729, 2017. 3\n[60] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie\u00dfner,\nand Angela Dai. Scannet++: A high-fidelity dataset of 3d\nindoor scenes. In Proceedings of the International Confer-\nence on Computer Vision (ICCV), 2023. 6, 2\n[61] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Generative image inpainting with con-\ntextual attention. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 5505\u20135514,\n2018. 6, 3\n[62] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A\nEfros.\nUnpaired image-to-image translation using cycle-\nconsistent adversarial networks. In Computer Vision (ICCV),\n2017 IEEE International Conference on, 2017. 3\n11\nAnything in Any Scene: Photorealistic Video Object Insertion\nSupplementary Material\nIn this supplementary material, we include additional\ntechnical details and a broader range of quantitative and\nqualitative results of our proposed method. We first describe\nadditional details on the assets bank in Section 7, the object\nplacement in Section 8, the lighting estimation and shadow\ngeneration in Section 9, and the photorealistic style trans-\nfer in Section 10. We then introduce the details of how we\nconducted the human study to compare different simulated\nvideos in Section 11.\nFurthermore, we also present the quantitative validation\nresults of our method using the indoor dataset ScanNet++ in\nSection 12, and further details on the downstream tasks we\nconducted are available in Section 13. We provide more de-\ntails of the result of downstream task we performed in Sec-\ntion 13. To visually underscore the effectiveness of our ap-\nproach, we include an extensive gallery of simulated videos\ngenerated by our framework alongside others for compara-\ntive analysis in Section 14.\nFinally, we kindly suggest that reviewers view our sup-\nplementary video files (sample video outdoor.mp4 for an\noutdoor scene and sample video indoor.mp4 for an indoor\nscene) to better appreciate the capabilities of our simulation\nmethod through these representative examples.\n7. Assets Bank Details\nOur Anything in Any Scene framework aims to create\nlarge-scale simulation videos by integrating dynamic scene\nvideos with objects of interest. This requires an asset bank\nof scene videos and object meshes. To facilitate this, we\ndeveloped a visual data query engine for efficiently select-\ning scene videos based on visual descriptors. Additionally,\nwe employ the Houdini Engine and Neural Radiance Fields\n(NeRF)-based reconstruction for 3D mesh generation, en-\nabling the integration of diverse objects into these videos.\n7.1. Visual Data Query Engine\nIn order to efficiently locate target videos for composition\nfrom a large-scale video assets bank, our method leverages a\nvisual data query engine. This engine is designed to retrieve\nclusters of video clips that visually match the provided de-\nscriptive words. To handle large-scale image and video data\nwith detailed visual features, we employ the Bag of Visual\nWords (BoVW) approach.\nWe first estimate semantic segmentation masks for each\nframe in the scene video assets bank. This segmentation\nbreaks down each video frame into labeled regions of in-\nterest. Following this, we utilize the Scale Invariant Fea-\nture Transform (SIFT) algorithm to extract visual features\n(a) A crashed sedan object mesh generated by Houdini engine\n(b) A person object mesh reconstructed by NeRF-based method.\nFigure 9. Examples of generated object mesh for video simulation\nfrom these segmented regions. We detect key points in each\nframe and compute descriptors represented by feature vec-\ntors for the regions containing these key points. These de-\nscriptors are then clustered, with the centroid of each cluster\nrepresenting a \u2019visual word\u2019 in the BoVW. The frequency\nof these visual words across the video dataset is used to\nbuild a frequency histogram for each video. Consequently,\nthe BoVW representation allows us to effectively retrieve\nmatching videos based on the occurrence and frequency of\nthe given visual words, improving the process of selecting\nappropriate videos for our Anything in Any Scene frame-\nwork.\n7.2. Object Mesh Generation\nThe mesh model of a target object is required before its in-\nsertion into an existing video clip. We employ the following\ntwo methods to generate the object mesh models.\nHoudini Engine for Object Mesh Generation To\ncreate visually appealing and physically accurate object\nmeshes, we utilize the Houdini Engine [48] that leverages\nthe physics-based rendering capabilities to enhance exist-\ning object mesh models with realistic physical effects The\n1\nHoudini Engine, known as a robust 3D animation procedu-\nral tool, can produce a wide range of physical effects such\nas deformation, animations, reflections, and particle visual\neffects. As an example is shown in Figure 9a, the Hou-\ndini engine can transform a truck model into a crashed one\nby applying deformation effects. Furthermore, it can simu-\nlate diverse realistic physical effects, such as smoke from a\ncrashed car, using its particle visual system. This approach\nis particularly critical for creating object meshes that are\nchallenging or expensive to capture in real-world scenarios.\nNeRF-based Object Mesh Reconstruction In order to\nalso cover the objects that are difficult to produce by the\nHoudini engine and generalize the asset bank to include\narbitrary objects, we propose the complementary NeRF-\nbased Object Mesh Reconstruction The impressive perfor-\nmance of Neural Radiance Fields (NeRF) in 3D reconstruc-\ntion from multi-view images offers the potential to build\nan extensive 3D asset bank. In our work, we adopt an off-\nthe-shelf method [51] that combines the advantage of both\nNeRF and mesh representation This method reconstructs\nthe object mesh model from multi-view RGB images. An\nexample of the reconstructed person object mesh is shown\nin Figure 9b, which features rich textures and detailed ge-\nometry suitable for following rendering processes.\n8. Object Placement Details\nIn order to accurately position the inserted object within a\nscene video, the first step involves reconstructing the 3D\npoint cloud representation of the captured environment. The\nobject placement point is then determined in 3D space,\nguided by segmentation mask. During the 2D-to-3D projec-\ntion process, we focus on estimating an appropriate place-\nment plane for the inserted object. This plane is conceptu-\nalized as the best-fitting plane, represented by the equation:\nAx + By + Cz + D = 0\n(6)\nbased on the selected points (x, y, z).\nFor a more accurate estimation, we utilize multiple 3D\npoints to determine the optimal fitting plane. As illustrated\nin Figure 11, we select several points within the road region\n(in yellow in the Figure 11) to estimate the ground plane\nwhere the object can be realistically inserted.\nSettings for PandaSet and ScanNet++ Datasets: The\ntwo datasets we used, PandaSet [58] for outdoor scene and\nScanNet++[60] for indoor scene, consist of footage cap-\ntured by RGB cameras and depth sensors.\nThese sen-\nsors record driving scenes for PandaSet and room scenes\nfor ScanNet++.\nOur selection process for video clips\nfrom these datasets involves choosing those captured by a\nforward-facing camera, particularly focusing on clips where\nthe camera exhibits motion, thus ensuring dynamic and\nvaried frames for composition. Regarding the ScanNet++\nFigure 10. An example of an excluded video clip from the Pan-\ndaSet. The camera is stable during the entire video clip because\nthe camera is on a car waiting for the traffic light.\nFigure 11. The projected 3D scene for object insertion. The yellow\nregion is the plane that is available to place the inserted object.\ndataset, we selected 5-second segments from each origi-\nnal video clip, down-sampling them from the original 60Hz\nto 20Hz. We ensured that a minimum of 20 frames from\neach clip were available, providing an adequate number of\nframes for effective video composition. We exclude video\nclips that suffer from low frame rate issues or where the\ncamera remains stationary during the recording, such as the\nscenario shown in Figure10, where the camera is affixed to\na stationary vehicle at a traffic signal.\nUtilizing the depth information and segmentation data\navailable in both dataset, we reconstruct the 3D point cloud\nfor the scenes. This enables us to precisely select the object\nplacement points from within the designated placeable ar-\neas in 3D space. For instance, in a driving scene from the\nPandaSet, as illustrated in Figure 11, the road region high-\nlighted in yellow is identified as the appropriate location for\ninserting a car into the scene.\n2\nFigure 12. The overview of lighting estimation and shadow generation.\n9. Lighting Estimation Detail\nIn Figure 12, we provide a comprehensive overview of the\nlighting estimation and shadow generation process. To en-\nsure realistic shading effects on objects inserted during ren-\ndering, we estimate High Dynamic Range (HDR) images of\nboth the sky and the surrounding environment.\nFor HDR sky image estimation, an image inpainting net-\nwork initially infers a panoramic sky image. This is fol-\nlowed by employing a sky HDR reconstruction network\nto transform this panoramic sky image into an HDR one.\nIn parallel, the estimation of HDR environmental images\ninvolves reconstructing HDR images from Low Dynamic\nRange (LDR) side-view images of the scene by using an\nLDR to HDR network. These images are then seamlessly\nstitched together to form an HDR panoramic environmental\nimage.\nBoth the HDR sky and environmental images are inte-\ngrated together to achieve realistic lighting effects on the\ninserted objects in the rendering process. Additionally, we\nleverage the estimated HDR sky image to render shadows\nfor the inserted objects, utilizing the 3D graphics applica-\ntion Vulkan for this purpose.\n10. Photorealistic Style Transfer Detail\nWe utilize the coarse-to-fine mechanism for photorealistic\nstyle transfer, and the overview of the network is shown in\nFigure 13 where both of the coarse network and refinement\nnetwork consist of the dilated convolution layers. We con-\ncatenate an image with black pixels filled in the foreground\nregion, a binary mask indicating the foreground region, and\nan image with black pixels filled in the background region\nas an input to the coarse network that outputs an initial\ncoarse prediction. The refine network takes the composi-\ntion of the coarse network\u2019s input and output, and it gener-\nates the final refined completed image.\nWe followed the same training strategy as described\nin [61], the coarse network is trained with the reconstruc-\ntion loss, and the refinement network is trained with the\nreconstruction and GAN losses. We trained and finetuned\nthe networks on the PandaSet dataset for the outdoor sce-\nnario. All input is concatenated together and then resized\n256 \u00d7 256 as input image resolution.\nWe are also interested in the performance of different\nstyle transfer methods on the task of photorealistic style im-\nprovement in our proposed framework. Specifically, we in-\nvestigated the usage of a CNN-based method DoveNet [8],\na transformer-based method StyTR2 [11], and a diffusion\nmodel-based method PHDiffusion [33] compared to our\nmethod introduced in main paper.\nDoveNet: a U-Net-based network is used as a genera-\ntor to translate the visual domain of the inserted foreground\nregion to match the background, and the GAN framework\nwith two different discriminators is leveraged to train the\ngenerator for more realistic and harmonious image output.\nWe follow the same process as described in [8], we resize\nthe input images as 256 \u00d7 256 during both the training and\ntesting stages.\nStyTR2: a transformer is leveraged as an encoder to\ncapture long-range dependencies of image features for style\ntransfer. We set the style weight as 10.0 and content weight\nas 7.0 for the StyTR2 model, and we downscale the image\nto 512 \u00d7 512 and then randomly crop the image to 256 as\nthe input image during the training stage.\nPHDiffusion: a stable diffusion model is proposed to\nuse two encoders to stylize foreground features. The fea-\ntures from both encoders are combined to finalize the style\ntransfer process. We loaded the pre-trained Stable Diffusion\nmodel weights and all images are resized to 512 \u00d7 512 as\ninput resolution for both training and testing.\n3\nFigure 13. The overview of coarse-to-fine mechanism for photorealistic style transfer. The input is a background RGB image with a black\nforeground region, the inserted object foreground RGB image, and a foreground segmentation mask. The output is the refined RGB image.\n11. Human Study Details\nWe validated the simulated videos generated by our pro-\nposed method through a human A/B test. We utilized a\nGUI application [39] designed and developed by ourselves,\nwhich allows users to compare two videos side by side, and\nselect the preferred one between them. We provide instruc-\ntion as follows to each human judge before they start to con-\nduct the study:\nYou are participating in a study to assess the realism of\nvideos created by computers. Each video features a scene\nwith an object inserted. Your task is to compare two videos\nside by side and select the one that appears more realistic\nto you.\nPlease ensure that you are seated at an appropriate distance\nin front of the display screen, and familiarize yourself with\ncontrols, such as playing the video and going to the previous\nor next task.\nFor this study, realism is defined by how convincingly the\nobject is integrated into the scene video. You can consider\nthe following factors in your assessment:\n1. The consistency of the object with physical rules de-\npicted in the scene.\n2. The naturalness of lighting, shadows, and replications.\n3. The believability of the object\u2019s interaction with its envi-\nronment.\nWatch both videos in full at least once before making a deci-\nsion, and feel free to view as many times as needed, focusing\non the inserted object in the scene. Please select the video\nthat you believe has better realism by pressing the corre-\nsponding \u201dselect\u201d button.\nThe human judges use the application as shown in Fig-\nure 14, which provides multiple controls, such as naviga-\ntion through all video pairs, video playback, video selec-\ntion, video suspend, and selection view panel.\nFor the validation of each dataset, we conducted two\nseparate human studies. The study for the outdoor dataset\ninvolved 24 human judges, while the study for the indoor\nMethod\nHuman Score(%)\nFID\nProposed method\n57.92\n10.537\nStyTR2 style transfer\n53.33\n11.145\nPHDiffusion style transfer\n36.25\n12.004\nDoveNet style transfer\n44.58\n10.832\nw/o style transfer\nN/A\n11.901\nTable 4. Experimental results of indoor Scene dataset ScanNet+\nwith different style transfer networks plugged into our Anything\nin Any Scene framework.\ndataset had 16 participants.\nIn validating the PandaSet\ndataset, we had a pool of 100 videos, from which 38 were\nrandomly selected for the human study. In the case of the\nScanNet++ dataset, out of the 52 available videos, 30 were\nrandomly chosen for conducting the human study. Note\nthat all videos were used in the calculation of the FID\nscore. In each study, every judge was tasked with evalu-\nating and labeling a total of 105 pairs of videos. In the first\nstudy, we compare the performance of different style trans-\nfer networks plugged into our proposed framework, cover-\ning DoveNet, StyTR2, PHDiffusion, and ours. As for an-\nother human study, we analyze the performance of our pro-\nposed method if we remove one of the key components. In\nboth study settings, we set our proposed method without a\nstyle transfer process as the baseline.\nThe human score of method A can be computed as\ntimes of results by method A selected\ntotal times of results by method A and B selected\n(7)\nwhere method B is the baseline, and method A is the method\nfor comparison. Suppose method A is also the base, theo-\nretically baseline method has a human score of 50%. We\nprovide the quantitative and qualitative results on the out-\ndoor dataset PandaSet in the main paper, and the results on\nthe indoor dataset ScanNet+ are detailed in Section 14.\n4\nFigure 14. Example of the human study interface for comparing two videos quality. The human judge selects the right video because of its\nmore realistic visual effect.\nMethod\nHuman Score(%)\nFID\nProposed method\n57.92\n10.537\nw/o placement\n9.58\n9.709\nw/o HDR\n32.92\n10.824\nw/o shadow\n36.25\n10.464\nw/o style transfer\nN/A\n11.901\nTable 5. Experimental results for ablation analysis of modules in\nour Anything in Any scene framework. Note that the baseline w/o\nstyle transfer method theoretically has a human score of 50%\n12. Experimental Results of Indoor Scene\nWe followed the same experimental setup detailed in the\nmain paper, and conducted a validation of our proposed\nmethod on an indoor scene. Similarly to the validation on\nthe outdoor scene, we evaluated the performance of various\nstyle transfer networks by comparing the following meth-\nods: a CNN-based method DoveNet, transformer-based\nmethod StyTR2, diffusion model-based method PHDiffu-\nsion, and our method. In the human study, as described\nin 14, we designated our framework without the style trans-\nfer module as the baseline for comparison. The compar-\native results, summarized in Table 4, reveal that our style\ntransfer network achieved the lowest Frechet Inception Dis-\ntance (FID) score of 10.537 and the highest human score of\n57.92%, surpassing the performance of the other methods.\nAblation Studies: We also performed ablation studies\nusing indoor dataset ScanNet+ to assess the impact of indi-\nvidual modules on overall performance. Similarly. we re-\nmoved one module from our framework: placement (w/o\nplacement), HDR image reconstruction (w/o HDR), and\nstyle transfer (w/o style transfer). The results are detailed in\nTable 5. Similarly to the result of outdoor dataset PandaSet,\nthe absence of HDR, and style transfer modules resulted in\nhigher FIDs. The placement of objects in unrealistic loca-\ntions significantly reduced their perceived realism among\nhuman observers. However, this decrease in realism was\nnot accurately reflected in the FID scores. One primary rea-\nson is the nature of indoor scenes, which often have limited\nspace. This can result in the inserted object being partially\nor completely out of the camera\u2019s field of view, impacting\nthe assessment metrics. Our method consistently received a\nhuman score above 50%, while the others fell below 50%,\nemphasizing the contribution of each module to the efficacy\nof our system.\n13. Downstream Task Details\nWe expanded our scope to include 25 object categories\nfor insertion into images from the CODA2022 validation\ndataset. Similarly, we trained three models: YOLOX-S,\nYOLOX-L, and YOLOX-X [14]. Utilizing the \u201dAnything in\nAny Scene\u201d framework, we augmented the original training\nimages by inserting a variety of objects, thereby generating\na new set of training data. This enhanced dataset was then\nused to re-train the models, ensuring consistency with the\noriginal training strategy.\nThe performance of the models was evaluated by training\non both the original and augmented datasets and then testing\n5\nMethod\nData\nmAP\nPlastic Bag\nStone\nStroller\nTraffic Light\nConcrete Block\nYOLOX-S\nOriginal\n0.321\n0.302\n0.020\n0.218\n0.193\n0.125\nOriginal + Ours\n0.334\n0.475\n0.093\n0.260\n0.227\n0.108\nYOLOX-L\nOriginal\n0.394\n0.314\n0.105\n0.406\n0.262\n0.178\nOriginal + Ours\n0.391\n0.336\n0.077\n0.474\n0.318\n0.309\nYOLOX-X\nOriginal\n0.395\n0.319\n0.110\n0.307\n0.246\n0.215\nOriginal + Ours\n0.405\n0.311\n0.133\n0.529\n0.290\n0.202\nTable 6. Performance of the YOLOX models trained on the original images from the CODA dataset compared to their performance when\ntrained on a combination of original and augmented images using our Anything in Any Scene framework. We report the mAP represents\nthe mean for all 25 object categories. We also report individual categories that has a significant improved AP in either one of the three\nmodels.\non a consistent test dataset. The results are presented in\nTable 6, where we detail the mean Average Precision (mAP)\nacross all 25 object categories. We also highlight individual\ncategories that showed significant AP improvement in any\nof the three models.\n14. Qualitative Visualization\nThe quantitative experimental results show that human\njudges prefer our proposed method compared to the other\nwhich either has one key component missing or another\nphotorealistic style transfer network used. We demonstrate\nmore qualitative visualization for both outdoor dataset Pan-\ndaSet and indoor dataset ScanNet+ as shown in the follow-\ning.\nStyle Transfer Network:\nIn Figure 15 and 17, we\ndemonstrate the qualitative comparison of sample video\nframes generated by different style transfer networks us-\ning both the outdoor scene dataset PandaSet and the indoor\nscene dataset ScanNet+. The foreground region of images\nrefined by DoveNet has significant grid pattern artifacts and\nis much blurry compared to the background regions. As for\nthe refined images generated by StyTR2 and PHDiffusion,\nthe color tone of the inserted object is not consistent with\nthe weather and sunlight environment in the scene. The\nrefined image generated by our proposed method has the\nbest visual quality compared to the other three, and our pro-\nposed method achieved the best result in both FID and hu-\nman study scores as reported in the quantitative result.\nAblation Analysis: We conducted an ablation analysis\nof each key rendering component including HDR image re-\nconstruction, shadow generation, and style transfer. In Fig-\nure 16 and Figure 18, we show more qualitative compar-\nisons of the simulated video frame with different rendering\noptions using both the outdoor scene dataset PandaSet and\nthe indoor scene dataset ScanNet+.\nThe inserted objects show inconsistent illumination a\ncolor balancing if there is no style transfer module or HDR\nreconstruction module involved in the video simulation pro-\ncess. The inserted objects with no rendered shadow seem to\nbe off the ground.\n6\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 15. Qualitative comparison of a simulated video frame using outdoor scene dataset PandaSet with different rendering options. (a)\ngenerated by the method without object placement; (b) generated by the method without HDR image reconstruction; (c) generated by the\nmethod without shadow generation (d) generated by the method without style transfer (e) generated by our proposed method including all\nrendering options.\n7\n(a)\n(b)\n(c)\n(d)\nFigure 16. Qualitative comparison of a simulated video frame using outdoor scene dataset PandaSet with different style transfer networks.\n(a) generated by DoveNet; (b) generated by StyTR2; (c) generated by PHDiffusion (d) generated by our proposed style transfer network\n8\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 17. Qualitative comparison of a simulated video frame using indoor scene dataset ScanNet++ with different rendering options. (a)\ngenerated by the method without object placement; (b) generated by the method without HDR image reconstruction; (c) generated by the\nmethod without shadow generation (d) generated by the method without style transfer (e) generated by our proposed method including all\nrendering options.\n9\n(a)\n(b)\n(c)\n(d)\nFigure 18. Qualitative comparison of a simulated video frame using indoor scene dataset ScanNet++ with different style transfer networks.\n(a) generated by DoveNet; (b) generated by StyTR2; (c) generated by PHDiffusion (d) generated by our proposed style transfer network\n10\n"
  },
  {
    "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
    "link": "https://arxiv.org/pdf/2401.17464.pdf",
    "upvote": "15",
    "text": "Efficient Tool Use with Chain-of-Abstraction Reasoning\nSilin Gao1,2\u2217, Jane Dwivedi-Yu2, Ping Yu2, Xiaoqing Ellen Tan2,\nRamakanth Pasunuru2, Olga Golovneva2, Koustuv Sinha2\nAsli Celikyilmaz2, Antoine Bosselut1, Tianlu Wang2\n1EPFL, 2FAIR @ Meta\n1{silin.gao,antoine.bosselut}@epfl.ch\n2{silingao,janeyu,pingyu,ellenxtan}@meta.com\n2{rpasunuru,olggol,koustuvs,aslic,tianluwang}@meta.com\nAbstract\nTo achieve faithful reasoning that aligns with\nhuman expectations, large language models\n(LLMs) need to ground their reasoning to real-\nworld knowledge (e.g., web facts, math and\nphysical rules). Tools help LLMs access this ex-\nternal knowledge, but there remains challenges\nfor fine-tuning LLM agents (e.g., Toolformer)\nto invoke tools in multi-step reasoning prob-\nlems, where inter-connected tool calls require\nholistic and efficient tool usage planning.\nIn this work, we propose a new method for\nLLMs to better leverage tools in multi-step\nreasoning. Our method, Chain-of-Abstraction\n(CoA), trains LLMs to first decode reasoning\nchains with abstract placeholders, and then call\ndomain tools to reify each reasoning chain by\nfilling in specific knowledge. This planning\nwith abstract chains enables LLMs to learn\nmore general reasoning strategies, which are ro-\nbust to shifts of domain knowledge (e.g., math\nresults) relevant to different reasoning ques-\ntions. It also allows LLMs to perform decod-\ning and calling of external tools in parallel,\nwhich avoids the inference delay caused by\nwaiting for tool responses. In mathematical\nreasoning and Wiki QA domains, we show that\nour method consistently outperforms previous\nchain-of-thought and tool-augmented baselines\non both in-distribution and out-of-distribution\ntest sets, with an average \u223c 6% absolute QA ac-\ncuracy improvement. LLM agents trained with\nour method also show more efficient tool use,\nwith inference speed being on average \u223c1.4\u00d7\nfaster than baseline tool-augmented LLMs.\n1\nIntroduction\nRecent large language models (LLMs; Touvron\net al., 2023b; Anil et al., 2023; OpenAI, 2023),\nhave made progress at interpreting and executing\ninstructions (Wei et al., 2021; Chung et al., 2022),\n*Work done during Silin Gao\u2019s internship at FAIR.\nIn a 90-minute game, Mark played 20 \nminutes, then another 35 minutes. \nHow long was he on the sideline?\nLLM  \nTool\nLLM\nMark played for a total of \n[20 + 35 = y1] minutes. So, \nhe was on the sideline for \n[90 - y1 = y2] minutes.\ny1 = 20 + 35 = 55\ny2 = 90 \u2013 y1 = 90 - 55 = 35 \nThe answer is 35 minutes.\nRalph Hefferline was a psychology \nprofessor at a university. In which \ncity is this university located?\nSearch the [university of Ralph Hefferline -\nWikiSearch-> y1], which is [y1 -NER-> y2]. \nThen find the [city y2 is in -WikiSearch-> y3]. \ny1: Ralph Hefferline was a professor at \nColumbia University \u2026\ny2: Columbia University\ny3: Columbia University is an Ivy League \nuniversity in New York \u2026\nThe answer is New York.\nMathematical Reasoning  \nWiki QA  \nFigure 1: Overview of chain-of-abstraction reasoning\nwith tools. Given a domain question (green scroll), a\nLLM is fine-tuned to first generate an abstract multi-step\nreasoning chain (blue bubble), and then call external\ntools to reify the chain with domain-specific knowledge\n(orange label). The final answer (yellow bubble) is\nobtained based on the reified chain of reasoning.\nbut still make errors when recalling and composing\nworld knowledge for their responses, e.g., making\nunfactual statements (Maynez et al., 2020; Ji et al.,\n2023), incorrect calculations (Patel et al., 2021), etc.\nUsing auxiliary tools (e.g., a search engine to pro-\nvide credible facts, a calculator for accurate math\noperations, etc.) at inference time can mitigate\nsome of these errors, motivating tool-augmented\nlanguage models that integrate external API calls\ninto their output generations (Parisi et al., 2022;\nSchick et al., 2023; Hao et al., 2023b).\nHowever, we show that current tool-augmented\nLLMs, e.g., Toolformer (Schick et al., 2023), strug-\ngle to reliably and efficiently leverage tools in\nmulti-step reasoning. In particular, tool calls in\nmulti-step reasoning tasks are often interleaved\n(i.e., the response of an API call is often part of the\nquery of a subsequent call; as shown in Figure 1).\nWithout explicitly modeling these interconnections\narXiv:2401.17464v2  [cs.CL]  26 Feb 2024\nin reasoning chains, LLMs do not learn effective\nplanning for tool use, which leads to less accurate\nreasoning with tools.1 Meanwhile, interleaving\ntext generation with API calls also introduces inef-\nficient inference \u201cwaiting times,\u201d where the model\nmust wait for the response from the API call before\nresuming the decoding process. This inefficiency\nbecomes more significant in multi-step reasoning\nscenarios, when multiple rounds of API calls are\ntypically required for each reasoning process.\nIn this work, we propose Chain-of-Abstraction\n(CoA) reasoning, a robust and efficient method for\nLLMs to perform multi-step reasoning with tools.\nAs shown in Figure 1, LLMs are fine-tuned with\na goal of making reasoning chains with abstract\nplaceholders. The placeholders do not affect LLMs\u2019\nreasoning flow, and are subsequently infilled with\nspecific knowledge retrieved from specialized tools,\nto ground the final answer generations. Planning\nabstract chain of reasoning encourages LLMs to\ninter-connect multiple tool calls and adopt more\nfeasible reasoning strategies, which are robust to\nthe variation of domain knowledge involved in each\nreasoning process, e.g., specific calculation results.\nUnlike previous methods where LLM decoding\nand API calls are executed in an interleaved man-\nner, our method leverages tools to infill knowledge\nonce after the whole chain of reasoning is gener-\nated. This enables more efficient decoding across\nmultiple examples (e.g., as in a stream) because\nCoA traces for subsequent examples can be de-\ncoded while tool calls are made for the preceding\nones, amortizing overall inference time. We de-\nvelop a simple pipeline to build fine-tuning data for\nmodels to learn CoA, where we first prompt LLMs\nto re-write existing responses to instructions as ab-\nstract chains, and then use domain tools to check\nthe validity of re-writing, as shown in Figure 2.\nAfter training LLMs to learn CoA reasoning,\nwe evaluate the finetuned models on two repre-\nsentative multi-step reasoning domains, including\nmathematical reasoning (Cobbe et al., 2021; Miao\net al., 2020; Patel et al., 2021; Koncel-Kedziorski\net al., 2016), and Wikipedia (Wiki) QA (Yang et al.,\n2018; Berant et al., 2013; Kwiatkowski et al., 2019;\nJoshi et al., 2017) that involves reasoning on factual\ndescriptive knowledge. We show that our method\nboosts LLMs\u2019 performances, with average \u223c7.5%\nand 4.5% absolute accuracy improvements on math\nand Wiki QA, respectively. These improvements\n1as verified by our analysis in \u00a75\nare consistent across both in-distribution and (zero-\nshot) out-of-distribution test sets, and are espe-\ncially pronounced on questions that require com-\nplex chain-of-thought reasoning.2 Meanwhile, our\nmethod also uses tools more efficiently than previ-\nous augmentation methods, with average \u223c1.47\u00d7\nand 1.33\u00d7 faster inference speeds on math and\nWiki QA tasks, respectively. Finally, extensive\nhuman evaluation demonstrates that our method\nguides LLMs to learn more accurate reasoning,\nwhich leads to \u223c 8% fewer reasoning errors.\n2\nRelated Work\nTool-Augmented LLMs\nThere is growing in-\nterest in augmenting LLMs using external tools.\nConsiderable work has tried to adapt LLMs as\ntool-using reasoners through in-context learning,\ndemonstrating promising performance improve-\nments in various applications, e.g., math prob-\nlem solving (Gao et al., 2023; Chen et al., 2022),\nbiomedical question answering (Jin et al., 2023)\nand self-critiquing (Gou et al., 2023). Neverthe-\nless, guiding LLMs to effectively use tools using\nin-context demonstrations is challenging, which\nrequires elaborate task-specific prompt engineering\nand is restricted by the model\u2019s instruction follow-\ning ability (Jacovi et al., 2023). Noticing the limi-\ntations of in-context learning, several works teach\nLLMs to learn the usage of tools by fine-tuning\n(Parisi et al., 2022; Schick et al., 2023; Hao et al.,\n2023b), which more robustly improves LLMs\u2019 per-\nformance. However, all above approaches adopt\nsequential interactions with tools throughout rea-\nsoning, slowing the inference speed as a function\nof the latency of the tool (or API) and the number\nof API calls that are made.\nSome other prior works focus on using LLMs\nfor multi-step reasoning with other modules. In\nparticular, ReAct (Yao et al., 2023b) and FireAct\n(Chen et al., 2023) integrate LLMs with tools into\na closed loop of thought, action and observation\nsteps. This verbose reasoning loop slows down\nthe LLM decoding, and still incorporates tools via\nsequential interactions, resulting in inefficient in-\nference. Another line of work, PAL (Gao et al.,\n2023) and Program of Thoughts (Chen et al., 2022)\nprompt LLMs to generate program-based reasoning\nand interact with code executors, which however\nheavily rely on closed source coding models, i.e.,\nCodex (Chen et al., 2021), and are restricted to pro-\n2e.g., more than 3 steps of math derivations\nIn a 90-minute game, Mark played 20 minutes, then \nanother 35 minutes. How long was he on the sideline?\nLLM  \nTool\nMark played for a total of [20 + 35 = y1] \nminutes. So, he was on the sideline for \n[90 - y1 = y2] minutes.\nMark played for a total of 20 + 35 = 55 minutes. So, \nhe was on the sideline for 90 - 55 = 35 minutes.\ny1 = 20 + 35 = 55\ny2 = 90 \u2013 y1 = 90 - 55 = 35 \nFigure 2: Illustration of gold data re-writing for fine-\ntuning data construction. Given a pair of domain ques-\ntion (green scroll) and gold answer (yellow scroll), an\nLLM is prompted to re-write the gold answer as a rea-\nsoning chain with abstract variables (purple bubble).\nThen, domain specialized tools validate the correctness\nof the re-writing by checking whether the abstract chain\ncan be reified to get the final answer (orange label).\ncedural arithmetic reasoning. In our work, we aim\nto design a more general and efficient strategy for\nLLMs to leverage tools, especially on multi-step\nreasoning scenarios.\nTool Usage Planning\nSeveral previous works re-\nsearch tool usage planning in LLMs. Specifically,\nHuggingGPT (Shen et al., 2023), Chameleon (Lu\net al., 2023), OpenAGI (Ge et al., 2023) and Meta-\nTool (Huang et al., 2023) focus on planning the\nhigh-level sequence of using multiple tools to ad-\ndress multi-domain mixed tasks. Similarly, LATM\n(Cai et al., 2023), ML-BENCH (Liu et al., 2023)\nand Gorilla (Patil et al., 2023) aim at planning\nprogram-level integration of multiple APIs for de-\nsigning scripts of procedural tasks, e.g., a script\nfor training a model described by a GitHub reposi-\ntory. ToolChain* (Zhuang et al., 2023) combines\nthe planning of tool usage with tree-search-based\nreasoning (Yao et al., 2023a; Hao et al., 2023a),\nwhich is especially useful for procedural tasks (Xu\net al., 2023; Cobbe et al., 2021). Different from\nabove work, we focus on the planning of general\nchain-of-thought (Wei et al., 2022) reasoning with\nawareness of domain specialized tools.\n3\nMethod\nChain-of-Abstraction (CoA) Reasoning\nOur\nmethod decouples the general reasoning of LLMs\nfrom domain-specific knowledge obtained from ex-\nternal tools. Figure 1 shows an overview of our\nmethod. In particular, we first fine-tune LLMs to\ngenerate reasoning chains with abstract placehold-\ners, e.g., y1, y2 and y3,3 as shown in Figure 1. In\nthe second stage, we reify each reasoning chain by\nreplacing placeholders with domain-specific knowl-\nedge obtained from external tools, e.g., calculation\nresults from a calculator, relevant articles retrieved\nfrom web search engine, etc. Finally, the question\nis answered based on the reified reasoning chain.\nNote that since the LLMs are trained to gener-\nate abstract chain of reasoning instead of regular\nchain-of-thought (CoT) reasoning with explicit val-\nues, this enables LLMs to focus on learning gen-\neral and holistic reasoning strategies without need-\ning to generate instance-specific knowledge for the\nmodel\u2019s parameters. Moreover, decoupling general\nreasoning and domain-specific knowledge enables\nLLM decoding to proceed and switch between dif-\nferent samples in parallel with API calling (via a\npipeline), i.e., LLM can start generating the next\nabstract chain while the tool fills the current chain,\nwhich speeds up the overall inference process.\nFine-tuning Data Construction\nTo construct\nchain-of-abstraction (CoA) data for fine-tuning\nLLMs, we collect question answering (QA) sam-\nples from existing open-source QA datasets (Cobbe\net al., 2021; Miao et al., 2020; Yang et al., 2018),\nand prompt LLaMa-70B (Touvron et al., 2023a)\nto re-write the answer of each sampled question,\nas shown in Figure 2. Specifically, we prompt\nLLaMa-70B to label the spans in gold answers that\ncorrespond to knowledge operations (e.g., math\nderivations, statements based on Wikipedia ref-\nerences) and then to re-write the sentences with\nlabeled spans as fillable CoA traces, where the op-\neration results are replaced with abstract placehold-\ners.4 For example, the two derivations in the exam-\nple in Figure 2 are re-written as \u201c[20 + 35 = y1]\"\nand \u201c[90 \u2212 y1 = y2]\", respectively.\nNote that an intermediate knowledge operation\nresult may appear multiple times in an answer, e.g.,\nin Figure 2, the first equation\u2019s result 55 is used in\n3We also test placeholders in single-character format, e.g.,\nx, y and z, but these led to sub-optimal results.\n4We provide our few-shot prompting examples for CoA\ndata re-writing in Appendix C.\nSource\nReasoning Step\n1\n2\n3\n4\n5\n>5\nAll\nGSM8K\n8\n1540\n1648\n1164\n666\n553\n5579\nASDiv\n677\n0\n0\n0\n0\n0\n677\nTable 1: Reasoning step distribution of correctly re-\nwritten reasoning chains in math domain.\nthe second equation. We prompt LLaMa-70B to\nreplace all occurrences of the same intermediate\nresult with the same placeholder, thereby explicitly\nconnecting the multiple reasoning steps. To ensure\nthat the re-written data is accurate, we use domain-\nspecialized tools to verify the correctness of each\nCoA reasoning trace.5 Specifically, we use the\ntools to execute the labeled operations in each CoA,\nand only keep questions whose CoA can be infilled\nwith valid results by the tools.\n4\nExperimental Settings\nWe conduct our experiments on two representative\ndomains: mathematical reasoning and Wikipedia\n(Wiki) QA, which involves commonsense and logi-\ncal reasoning on factual descriptive knowledge.\n4.1\nMathematical Reasoning\nGiven a math question, the QA system needs to\ngenerate a natural language solution to the problem\nwith step-by-step arithmetic derivations (as demon-\nstrated in the left column of Figure 1). We assume\nthat the derivations involved in the solution are\nthe specialized knowledge operations required in\nthis domain, which are labeled in square brackets\nwith derivation results being replaced by abstract\nplaceholders, e.g., \u201c[20 + 35 = y1]\".\nDatasets\nWe construct most of our fine-tuning\nCoA data by re-writing the GSM8K (Cobbe et al.,\n2021) training set, which contains 7473 linguis-\ntically diverse grade school math problems. As\nGSM8K dataset focuses on multi-step reasoning, it\nlacks coverage of single-step arithmetic problems,\nso we also re-write an additional set of 691 single-\nstep math problems from the ASDiv (Miao et al.,\n2020) dataset. Across these re-written datasets, we\nfind that \u223c 76.6% of the CoA reasoning traces gen-\nerated by LLaMa-70B are verified by our equation\nsolver (described below). Table 1 shows the reason-\ning step distribution (i.e., number of derivations) of\nour constructed fine-tuning data.\n5Detailed implementations of reasoning chain verification\nare described in Sec. 4.1 and 4.2.\nQuestion\nThe director of the romantic comedy \u201cBig Stone Gap\u201d is based in\nwhat New York city?\nAnswer\nGreenwich Village\nWikipedia\nBig Stone Gap (film) > Big Stone Gap is a 2014 American romantic\nReferences\ncomedy film directed by Adriana Trigiani.\nAdriana Trigiani > Adriana Trigiani is an Italian American film\ndirector based in Greenwich Village.\nCoA Trace\nFind the [director of romantic comedy \u201cBig Stone Gap\u201d -Wiki-> y1].\nThe name of this film\u2019s director is [y1 -NER(person)-> y2].\nThen determine [y2 in what New York city -Wiki-> y3].\nTable 2: Example of CoA fine-tuning data construction\nin Wiki QA domain.\nFor an in-distribution evaluation, we test mod-\nels on GSM8K and ASDiv, containing 1319 and\n2305 testing problems. To further test the models\u2019\ngeneralization ability, we also conduct zero-shot\nevaluation on other representative math datasets,\nincluding SVAMP (Patel et al., 2021) and MAWPS\n(Koncel-Kedziorski et al., 2016), which contain\n1000 and 2065 testing samples, respectively.6\nDomain Tool\nWe use an equation solver to per-\nform the arithmetic derivations required in the\nmath domain. Our equation solver first extracts\nthe derivations labeled in the CoA reasoning, e.g.,\n\u201c[20 + 35 = y1]\" and \u201c[90 \u2212 y1 = y2]\", and\ncombines all derivations into a system of equa-\ntions. Then the system of equations is solved by\nthe SymPy toolkit,7 to get the true value of each\nvariable (i.e., the value of the abstract placeholder).\nFinally, our equation solver returns the reified chain\nof reasoning by replacing all the variables with their\nsolved true values (including the final answer).\n4.2\nWikipedia QA\nGiven a question based on Wikipedia knowledge,\nthe model needs to first identify Wikipedia arti-\ncles as references related to the question, and then\nreason on key knowledge in the reference articles\nto answer the question (as shown in the right col-\numn of Figure 1). We assume that the special-\nized knowledge operation in this domain is the re-\ntrieval of relevant Wikipedia articles and important\nnamed-entities, which are re-written as Wikipedia\nsearching (WikiSearch) and named-entity recogni-\ntion (NER)8 queries. Table 2 shows an example of\na re-written CoA trace for Wiki QA.9\n6For the MAWPS benchmark, we test on the 395, 508, 562\nand 600 math problems from AddSub, SingleEq, SingleOp\nand MultiArith portions, respectively.\n7https://www.sympy.org/en/index.html\n8We use NER to extract entities from the article that bridge\nthe former WikiSearch results to the latter WikiSearch queries.\n9We include more prompting examples of Wiki QA answer\nre-writing in Appendix C.\nDatasets\nWe use the HotpotQA (Yang et al.,\n2018) dataset to construct our fine-tuning CoA data\nin the Wiki QA domain. HotpotQA contains 113K\nmulti-hop QA examples, each labeled with two\nWikipedia articles that provide supporting knowl-\nedge. Among the 90447 training QA pairs, we\nidentify 72991 as Bridge QA pairs, where an inter-\nmediate entity must be identified to link the answer\nto the question, as shown in Table 2. The remain-\ning 17456 are Comparison QA pairs, where the\nattributes of two entities are compared, e.g., \u201cAre\nRandal Kleiser and Kyle Schickner of the same\nnationality?\u201d. We prompt LLaMa-70B to re-write\nthese training QAs into CoAs with WikiSearch and\nNER queries, and verify each CoA with our do-\nmain tools (described below), by checking whether\nall the articles returned by the WikiSearch queries\nmatch one of the titles in the gold articles. Finally,\n8956 Bridge QAs and 5405 Comparison QAs are\nused as fine-tuning data, whose re-written CoAs\npass the verification.10 For Wiki QA, we note that\nbesides training a LLM to produce CoA data using\nWikiSearch, we also fine-tune a second LLM to\nlearn to generate the final gold answer based on a\ncorrectly reified CoA reasoning trace.\nWe evaluate models on the HotpotQA develop-\nment set, which contains 5918 Bridge QA pairs and\n1487 Comparison QA pairs. Similar to the mathe-\nmatical reasoning domain, we also conduct zero-\nshot evaluation on other open-domain QA datasets:\nWebQuestions (WQ; Berant et al., 2013), Natu-\nralQuestions (NQ; Kwiatkowski et al., 2019) and\nTriviaQA (Joshi et al., 2017), which contain 2032,\n3610 and 17944 test questions, respectively.\nDomain Tools\nThe specialized tools required\nfor Wiki QA include a Wikipedia search engine\nto retrieve reference articles, and a NER toolkit\nto extract entities that bridge multi-step search-\ning queries. We follow Toolformer (Schick et al.,\n2023) and implement a Wikipedia search engine as\na BM25 retriever (Robertson et al., 1995; Baeza-\nYates et al., 1999) that indexes the Wikipedia dump\nfrom the KILT benchmark (Petroni et al., 2021).\nWe use the BM25 retriever to search the top-10 arti-\ncles relevant to the input query, and then re-rank the\narticles based on their Sentence-BERT (Reimers\nand Gurevych, 2019) embedding cosine similar-\nity with the question. After re-ranking, the top-1\n10Compared to mathematical reasoning, generating CoA\ndata for Wiki QA requires more complex tool use that com-\nbines WikiSearch and NER models, leading to a lower re-\nwriting success rate (\u223c 15.9%).\nGeneral\nSpaCy NER Types included in each General Class\nClass\nperson\nPERSON\ngroup\nNORP, ORG, LANGUAGE\nlocation\nGPE, FAC, LOC\nculture\nEVENT, WORK_OF_ART, LAW, PRODUCT\ndate\nDATE, TIME\nnumeral\nCARDINAL, PERCENT, MONEY, QUANTITY, ORDINAL\nTable 3: Aggregation of SpaCy NER types.\narticle is selected to be the final search result.\nWe use SpaCy11 (en_core_web_sm) as the NER\ntoolkit to extract named entities. To simplify NER,\nwe aggregate the numerous SpaCy NER types into\n6 general classes, as shown in Table 3. If multiple\nnamed entities are recognized, we input each rec-\nognized entity to the subsequent WikiSearch query,\nand select the entity whose subsequent search result\nhas the highest Sentence-BERT embedding cosine\nsimilarity with the question.\n4.3\nBaselines\nWe apply our CoA reasoning method to both 7B\nand 70B LLaMa models, and test various model\nversions including the first version of LLaMa (Tou-\nvron et al., 2023a) and the more advanced LLaMa-2\nand LLaMa-2-Chat (Touvron et al., 2023b). We\ncompare our method to several baselines, includ-\ning: a) few-shot prompting using 8 randomly sam-\npled QA exemplars from the original (i.e., not re-\nwritten) chain-of-thought data (CoT-FSP), b) fine-\ntuning with original chain-of-thought data (CoT-\nFT)12, and c) Toolformer (Schick et al., 2023)\nwhich fine-tunes LLMs on CCNet (Wenzek et al.,\n2020) texts augmented with API calls. For evalu-\nation on Wiki QA, we also compared our method\nwith FireAct (Chen et al., 2023), which fine-tunes\nLLMs on HotpotQA ReAct (Yao et al., 2022) tra-\njectories distilled from GPT-4 (OpenAI, 2023).\n5\nResults and Analysis\n5.1\nMathematical Reasoning\nTable 4 shows the evaluation results for the LLaMa-\n2 and LLaMa-2-Chat models.13 On the GSM8K\nand ASDiv datasets, our CoA method outperforms\nthe few-shot baseline CoT-FSP and the regular fine-\ntuning baseline CoT-FT, demonstrating that CoA\n11https://spacy.io/models/en\n12Note that in Wiki QA domain, the HotpotQA data used\nfor prompting or fine-tuning baselines is pre-processed to\ncontain both gold Wikipedia articles (serving as chain-of-\nthought explanations) and the final answer.\n13We include similar evaluation results for the original\nLLaMa model (7B) in Appendix B.\nModel\nMethod\nGSM8K\nASDiv\nSVAMP\nMAWPS\nAddSub\nSingleEQ\nSingleOp\nMultiArith\nAll\nLLaMa-2\nCoT-FSP\n16.38\n47.85\n38.40\n52.41\n63.39\n82.03\n43.33\n60.53\n-7B\nCoT-FT\n35.33\n57.18\n48.20\n66.08\n74.41\n85.23\n65.00\n73.03\nToolformer\n17.59\n48.55\n37.10\n47.34\n58.46\n79.54\n50.67\n59.81\nCoA\n37.83\n57.61\n51.70\n72.15\n82.48\n86.48\n73.17\n78.89\nLLaMa-2\nCoT-FSP\n24.03\n54.14\n51.30\n71.90\n72.44\n85.41\n74.00\n76.32\n-Chat-7B\nCoT-FT\n35.41\n59.00\n46.90\n58.23\n72.24\n85.41\n73.00\n73.37\nToolformer\n23.65\n50.85\n48.80\n61.01\n69.09\n81.85\n68.50\n70.85\nToolformer - Math\n36.01\n59.18\n47.60\n58.99\n72.44\n85.94\n75.50\n74.43\nCoA\n38.29\n59.57\n54.20\n72.41\n81.89\n88.26\n83.00\n82.13\nCoA (no Tool)\n35.03\n58.79\n51.50\n68.10\n74.21\n86.48\n77.67\n77.38\nLLaMa-2\nCoT-FSP\n56.18\n65.94\n70.60\n86.08\n89.17\n92.88\n84.50\n88.23\n-Chat-70B\nCoT-FT\n60.50\n70.24\n70.40\n81.52\n87.60\n92.35\n89.17\n88.18\nToolformer\n52.54\n69.07\n73.60\n86.84\n89.76\n91.46\n81.50\n87.26\nToolformer - Math\n61.03\n70.59\n73.20\n85.57\n91.34\n91.99\n92.00\n90.60\nCoA\n62.32\n71.89\n73.40\n86.33\n94.49\n93.06\n92.33\n91.91\nTable 4: Evaluation results on LLaMa-2 and LLaMa-2-Chat for mathematical reasoning. \u201cAll\u201d denotes the averaged\nresults on four MAWPS portions. Exact match rate to the final gold answer (i.e., accuracy) is reported. Best\nperforming augmentation approach for each base model is bolded.\nfine-tuning with tool augmentation is more effec-\ntive in adapting LLMs to multi-step reasoning tasks.\nSimilarly, when evaluated on out-of-distribution\ndatasets, SVAMP and MAWPS, CoA also consis-\ntently outperforms the baselines. Interestingly, for\nthese out-of-distribution datasets, CoT-FT lags fur-\nther behind CoA, particularly for 7B models, show-\ning that CoA reasoning yields more distributionally\nrobust reasoning performance.\nOur CoA method also surpasses the tool-\naugmented baseline Toolformer, which implies that\nplanning the abstract variables in CoA can improve\nthe accuracy of reasoning with tools. However, as\nToolformer is not originally trained with in-domain\nfine-tuning data,14 we also fine-tune a new ver-\nsion of Toolformer with the chain-of-thought data\nfrom GSM8K and ASDiv, denoted as Toolformer\n- Math in Table 4. We also observe that CoA per-\nforms better than Toolformer - Math, confirming\nthat the introduction of abstract variables enables\nmore robust tool use compared to direct integration\nof API calls within chain-of-thought reasoning.\nAblation Study\nWe verify that the robust gener-\nalization performance of our CoA method does not\nmerely benefit from using additional tools, by fine-\ntuning another LLM to solve the equation (from the\nsame model backbone), rather than calling the equa-\ntion solver, denoted as CoA (no Tool) in Table 4.\nWe find that CoA (no Tool) performs consistently\n14Toolformer is fine-tuned on CCNet data, which may not\ncontain rich mathematical reasoning samples.\nworse than CoA across all datasets, confirming\nthat using specialized tools enables LLM agents\nto conduct more precise operations, rather than di-\nrectly solving the same operations. However, CoA\n(no Tool) still outperforms all baseline methods on\nzero-shot generalization to SVAMP and MAWPS\ndatasets, implying that learning abstract reasoning\nchains also contributes to better robustness of CoA,\nperhaps due to better planning of multiple reason-\ning steps indexed by abstract variables.\nReasoning Steps\nOur findings suggest that the\nbenefits of chain-of-abstraction reasoning are most\npronounced when problems require long reasoning\nchains to be solved. Figure 3 shows the stratified\nperformance of three models on GSM8K QA, rel-\native to the number of reasoning steps in the pre-\ndicted and gold reasoning chains. Compared to the\nfew-shot CoT-FSP, CoA produces reasoning chains\nthat more often match the length of the gold reason-\ning chains, as reflected by the heat-map statistics\n(left column) being more aggregated around the di-\nagonal (comparable to CoT-FT). At the same time,\nwe observe that models achieve better QA accuracy\nwhen the number of reasoning steps in their gener-\nated answers are aligned with the gold references\n(i.e., the diagonal of heat-maps in right column).\nAbove results show that fine-tuned models are bet-\nter at learning to produce reasoning chains that\nmatch the true reasoning chain for the problem.\nInterestingly, we find that CoA, compared to\nCoT-FT, achieves higher performance especially\nNumber of QAs\nCoT-FSP\nCoT-FT\nCoA\nAccuracy (%)\nFigure 3: GSM8K evaluation results on LLaMa-2-Chat-\n7B w.r.t. the number of reasoning steps in the predicted\nand gold reasoning chain. (Left) The number of test\nexamples that belong to each stratum. (Right) The corre-\nsponding model accuracy (%) for those examples. Non-\ndiagonal cells with fewer than 15 examples are ignored.\nMethod\nError Rate\nArithmetic\nReasoning\nCoT-FSP\n17.3\n70.3\nCoT-FT\n25.2\n67.8\nCoA\n0.0\n60.4\nTable 5: Human evaluation results of arithmetic and rea-\nsoning error rates on 200 GSM8K test samples. Models\ndeveloped based on LLaMa-2-Chat-7B are presented.\non questions that require more reasoning steps.\nIn the right column of Figure 3, CoA\u2019s improve-\nment over CoT-FT is more pronounced on ques-\ntions with more than 3 steps in the gold reasoning\nchain (highlighted with red squares). This indicates\nthat the model trained with CoA has more robust\nlong chain-of-thought reasoning capability, which\nis learned from planning with abstractions.\nHuman Evaluation\nTo more comprehensively\nverify that CoA improves both knowledge oper-\nation (i.e., arithmetic by using tools) and reason-\ning accuracy, we conduct a human evaluation on\ndifferent model answers to 200 randomly sam-\npled GSM8K test questions. Specifically, given a\nGSM8K question and a model\u2019s answer to the ques-\ntion, we ask human workers to judge whether the\n2\n3\n4\n5\n>5\nGold Reasoning Step\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nInference Time per QA (seconds)\nMethod\nToolformer\nCoT-FSP\nCoA\nCoT-FT\nFigure 4: Wall-clock inference time on GSM8K (seeded\nwith LLaMa-2-Chat-7B). Average time of answering a\nquestion is measured (in seconds) w.r.t. the number of\ngold reasoning steps required for the question.\nanswer contains any arithmetic errors (e.g., wrong\ncalculations, invalid equations) or reasoning errors\nunrelated to math derivations (e.g., misunderstand-\ning of the question, improper strategy for solv-\ning the question), and report how often the model\nmakes these two kinds of errors. In Table 5, we\nfind that CoA effectively reduces arithmetic errors\nto zero, due to the use of equation solver to per-\nform accurate calculations. More importantly, our\nmethod also makes fewer reasoning errors com-\npared to the baselines, verifying that CoA fine-\ntuning guides the model to learn more accurate\nreasoning through the holistic planning of abstract\nreasoning chains. By contrast, ordinary fine-tuning\n(i.e., CoT-FT) produces a more limited reasoning\nimprovement compared to the few-shot CoT-FSP,\nwhile also failing to suppress arithmetic errors.\nInference Efficiency\nImportantly, we find that\nthe performance benefits of CoA reasoning do not\ncome with increased computational costs. In Fig-\nure 4, we show the average time (seconds) that\nCoA and baseline agents (seeded with LLaMa-\n2-Chat-7B) needs to answer a question w.r.t. re-\nquired gold reasoning steps. Compared to the CoT\nbaselines, CoA requires less time than the few-\nshot baseline CoT-FSP, whose generation needs to\nbe conditioned on additional examples. However,\nCoA is slightly less inference-efficient compared\nto CoT-FT, likely due to the decoding of additional\ntokens (e.g., \u201c[\u201d and \u201c]\u201d) for the abstract statements.\nCompared to Toolformer, CoA has a lower and\nflatter inference time curve, indicating better scal-\ning as the number of reasoning steps increases.\nModel\nMethod\nHotpotQA\nWQ\nNQ\nTriviaQA\nBridge\nComparison\nBoth\nTime\nLLaMa-2\nCoT-FSP\n11.69\n45.46\n18.47\n2.074\n34.65\n30.91\n53.48\n-Chat-7B\nCoT-FT\n14.24\n56.69\n22.77\n1.937\n33.51\n25.40\n51.05\nToolformer\n12.99\n44.59\n20.00\n2.350\n36.22\n30.22\n54.15\nToolformer - Wiki\n15.68\n56.42\n23.86\n2.301\n36.61\n32.96\n55.08\nFireAct\n19.18\n54.14\n26.20\n2.706\n36.02\n35.87\n52.96\nCoA\n21.00\n56.96\n28.22\n1.896\n35.97\n38.67\n57.90\nLLaMa-2\nCoT-FSP\n21.39\n56.62\n28.47\n6.668\n34.89\n37.42\n63.61\n-Chat-70B\nCoT-FT\n23.84\n63.95\n31.90\n6.401\n34.15\n39.75\n62.28\nToolformer\n22.24\n56.09\n29.04\n6.888\n37.16\n40.42\n64.31\nToolformer - Wiki\n26.38\n63.82\n33.90\n6.855\n37.70\n41.25\n66.64\nCoA\n27.61\n64.09\n34.94\n6.369\n36.37\n43.57\n69.08\nTable 6: Wiki QA evaluation results on LLaMa-2-Chat-based models. \u201cBoth\u201d denotes the overall evaluation results\non both bridge and comparison portions of HotpotQA. \u201cTime\u201d denotes the average seconds that each agent needs to\nanswer a question in HotpotQA. Exact match rate to the final gold answer (i.e., accuracy) is reported.\nThis difference arises because CoA decouples the\ngeneration of (abstract) reasoning chains from the\nretrieval of knowledge (i.e., tool use), allowing full\nreasoning chains to be decoded before any tool is\ncalled. This procedure amortizes inference costs in\ntwo ways. First, tool calls are made after the CoA\ntrace has been decoded, enabling parallel tool calls\nfor the same trace (e.g., using an equation solver\nonce rather than multiple calls to a calculator), and\navoiding the time delay caused by waiting for ex-\nternal API responses. Consequently, the model\nfine-tuned with CoA is more efficient at multi-step\nreasoning, especially when the number of reason-\ning steps (i.e., tool calls) increases. Second, across\nmultiple examples, the model can generate the CoA\ntrace of the next example while tool calls are made\nfor the preceding one, parallelizing CoA decoding\nand tools calls across examples.\n5.2\nWiki QA\nTable 6 shows our Wiki QA results using LLaMa-\n2-Chat models.15 Similar to mathematical reason-\ning, we fine-tune a new version of Toolformer with\nin-domain chain-of-thought data from HotpotQA,\ndenoted as Toolformer - Wiki. On HotpotQA,\nCoA achieves higher exact match rates with the\ngold reference compared to the few-shot or fine-\ntuning baselines. In particular, CoA outperforms\nall baselines on the more challenging bridge-type\nQAs, where two steps of reasoning over Wikipedia\nknowledge are consecutively entangled, i.e., can-\n15We include similar evaluation results on LLaMa-2-7B in\nAppendix B.\nnot be performed independently in parallel as in\ncomparison-type QAs. Compared to FireAct fine-\ntuning, CoA also achieves better performance on\nboth bridge and comparison QAs, without requir-\ning data distilled from closed source GPT-4.\nAs with mathematical reasoning, CoA agents\nalso perform more efficient inference than Tool-\nformer and FireAct agents when answering Hot-\npotQA questions. We also find that CoA is more ef-\nficient (Time column) than both CoT-FSP and CoT-\nFT, as CoA does not require few-shot examples as\nadditional inputs and does not need to generate\nlong Wiki articles, which are instead provided by\nthe search engine. Finally, CoA improves over the\nbaseline methods in zero-shot generalization exper-\niments on other Wiki QA datasets, outperforming\nall baselines on NaturalQuestions and TriviaQA,\nand matching the best baselines on WebQuestions.\n6\nConclusion\nIn this work, we propose to decouple the general\nreasoning of LLM agents from specialized knowl-\nedge obtained via external tools.\nOur method,\nchain-of-abstraction (CoA), encourages LLMs to\nlearn the planning of abstract multi-step reasoning,\nwhich are more robust to out-of-distribution knowl-\nedge shifts. CoA also achieves a more efficient\npipeline for tool usage that significantly improves\nthe speed of tool-augmented multi-step reasoning.\nThe simple, yet effective, implementations of our\nmethod on two diverse tasks (i.e., math reasoning\nand open-domain QA) demonstrate its potential for\nbeing adapted to new reasoning scenarios.\nLimitations\nWe acknowledge a few limitations in our work.\nFirst, datasets used for testing our method cannot\nhave exhaustive coverage of all reasoning scenar-\nios in real world. We instead consider two rep-\nresentative reasoning domains, i.e., mathematical\nreasoning and Wikipedia QA, and use English as\na primary language in our testing. Furthermore,\nall models in our experiments use greedy decod-\ning to generate inferences, which leaves room for\napplying more advanced decoding strategies, e.g.,\nself-consistency (Wang et al., 2022) decoding. In\nAppendix B, we include an initial study of inte-\ngrating our CoA reasoning with self-consistency\ndecoding, where our method also achieves promis-\ning results, motivating further research on this di-\nrection. Finally, our method is tested on the set-\nting of fine-tuning the full LLMs, which requires\nconsiderable computational resources, while more\nefficient model training schemes, e.g., LoRA (Hu\net al., 2021), can be applied in future work.\nAcknowledgements\nWe thank Beatriz Borges, Gail Weiss, Syrielle Mon-\ntariol, Li Mi and Zeming Chen for reading and pro-\nviding comments on drafts of this paper. Antoine\nBosselut gratefully acknowledges the support of the\nSwiss National Science Foundation (No. 215390),\nInnosuisse (PFFS-21-29), the EPFL Science Seed\nFund, the EPFL Center for Imaging, Sony Group\nCorporation, and the Allen Institute for AI.\nReferences\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nRicardo Baeza-Yates, Berthier Ribeiro-Neto, et al. 1999.\nModern information retrieval, volume 463. ACM\npress New York.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nconference on empirical methods in natural language\nprocessing, pages 1533\u20131544.\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen,\nand Denny Zhou. 2023. Large language models as\ntool makers. arXiv preprint arXiv:2305.17126.\nBaian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier,\nKarthik Narasimhan, and Shunyu Yao. 2023. Fireact:\nToward language agent fine-tuning. arXiv preprint\narXiv:2310.05915.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021.\nEvaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W Cohen. 2022.\nProgram of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2023. Pal: Program-aided language\nmodels. In International Conference on Machine\nLearning, pages 10764\u201310799. PMLR.\nYingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan,\nShuyuan Xu, and Yongfeng Zhang. 2023. Openagi:\nWhen llm meets domain experts.\narXiv preprint\narXiv:2304.04370.\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong\nShen, Yujiu Yang, Nan Duan, and Weizhu Chen.\n2023. Critic: Large language models can self-correct\nwith tool-interactive critiquing.\narXiv preprint\narXiv:2305.11738.\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong,\nZhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023a.\nReasoning with language model is planning with\nworld model. arXiv preprint arXiv:2305.14992.\nShibo Hao, Tianyang Liu, Zhen Wang, and Zhiting\nHu. 2023b. Toolkengpt: Augmenting frozen lan-\nguage models with massive tools via tool embeddings.\narXiv preprint arXiv:2305.11554.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.\nLora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nYue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan\nWu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan,\nNeil Zhenqiang Gong, et al. 2023. Metatool bench-\nmark for large language models: Deciding whether\nto use tools and which to use.\narXiv preprint\narXiv:2310.03128.\nAlon Jacovi, Avi Caciularu, Jonathan Herzig, Roee\nAharoni, Bernd Bohnet, and Mor Geva. 2023. A\ncomprehensive evaluation of tool-assisted generation\nstrategies. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2023, pages 13856\u2013\n13878.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55:1\u201338.\nQiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu.\n2023. Genegpt: Augmenting large language models\nwith domain tools for improved access to biomedical\ninformation.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601\u20131611.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate\nKushman, and Hannaneh Hajishirzi. 2016. Mawps:\nA math word problem repository. In Proceedings of\nthe 2016 conference of the north american chapter of\nthe association for computational linguistics: human\nlanguage technologies, pages 1152\u20131157.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:452\u2013\n466.\nYuliang Liu, Xiangru Tang, Zefan Cai, Junjie Lu,\nYichi Zhang, Yanjun Shao, Zexuan Deng, Helan\nHu, Zengxian Yang, Kaikai An, et al. 2023. Ml-\nbench: Large language models leverage open-source\nlibraries for machine learning tasks. arXiv preprint\narXiv:2311.09835.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. arXiv\npreprint arXiv:2304.09842.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906\u20131919.\nShen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and developing\nenglish math word problem solvers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 975\u2013984.\nOpenAI. 2023. Gpt-4 technical report.\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\nTool augmented language models. arXiv preprint\narXiv:2205.12255.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are nlp models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080\u20132094.\nShishir G Patil, Tianjun Zhang, Xin Wang, and\nJoseph E Gonzalez. 2023. Gorilla: Large language\nmodel connected with massive apis. arXiv preprint\narXiv:2305.15334.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\net al. 2021. Kilt: a benchmark for knowledge in-\ntensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523\u20132544.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3982\u20133992.\nStephen E Robertson, Steve Walker, Susan Jones,\nMicheline M Hancock-Beaulieu, Mike Gatford, et al.\n1995. Okapi at trec-3. Nist Special Publication Sp,\n109:109.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a.\nLlama:\nOpen and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2022. Self-consistency improves\nchain of thought reasoning in language models. In\nThe Eleventh International Conference on Learning\nRepresentations.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Ar-\nmand Joulin, and \u00c9douard Grave. 2020. Ccnet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data. In Proceedings of the Twelfth Language\nResources and Evaluation Conference, pages 4003\u2013\n4012.\nQiantong Xu, Fenglu Hong, Bo Li, Changran Hu,\nZhengyu Chen, and Jian Zhang. 2023. On the tool\nmanipulation capability of open-source large lan-\nguage models. arXiv preprint arXiv:2305.16504.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D Manning. 2018. Hotpotqa: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023a. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023b.\nReact: Synergizing reasoning and acting in language\nmodels.\nYuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra,\nVictor Bursztyn, Ryan A Rossi, Somdeb Sarkhel,\nand Chao Zhang. 2023. Toolchain*: Efficient action\nspace navigation in large language models with a*\nsearch. arXiv preprint arXiv:2310.13227.\nA\nImplementation Details\nEvaluation Details\nFor mathematical reasoning\nevaluation, we extract the last number appeared in\neach model\u2019s answer, and check whether the num-\nber exactly match the gold reference. The accuracy\nis reported as the rate of such exact match across\nall QAs in a test set. For Wiki QA evaluation, simi-\nlar to mathematical reasoning, we extract the final\nanswer of each model and calculate its exact match\nrate to the gold reference. Specifically, the final\nanswer is supposed to be the words after \u201cAction:\nfinish[\u201d for FireAct baseline, and words after \u201cThe\nanswer is \u201d for other models. Our 8-shot in-domain\nexamples used for the CoT-FSP baseline are shown\nin Table 13 and 14, which enables the model to pro-\nvide answer with our required format for evaluation,\ni.e., stating its final answer after \u201cThe answer is \u201d.\nOur human evaluation on GSM8K is conducted by\n5 internal domain experts from our research group.\nFor each math question, we provide the experts\nwith the gold answer as reference, and ask them to\nevaluate each model answer in anonymous manner,\ni.e., experts do not know which model each answer\ncomes from. Two yes-or-no questions are asked\nfor evaluating each model answer, including: a)\nwhether the answer has any arithmetic error, and\nb) whether the answer has any reasoning error, and\nbinary choices from the experts are collected to\ncalculate the error rates of each model\u2019s generation.\nWe present our detailed instructions for human eval-\nuation in Figure 5. Our data collection protocol is\napproved by our organization in terms of ethics.\nModel Training\nWe fine-tune our models with\nbatch size 8 and learning rate 2e\u22125 and 1e\u22125 for\n7B and 70B model sizes, respectively, using cosine\nlearning rate scheduler with warm-up step 10. We\nuse AdamW (Loshchilov and Hutter, 2018) opti-\nmizer for all our fine-tuning experiments, with \u03b21,\n\u03b22 and \u03f5 set to 0.9, 0.95 and 1e\u22128, respectively.\nTraining weight decay is set to 0.1. For mathe-\nmatical reasoning, we use a total of 400 training\nsteps, and get the best model checkpoints (with\nhighest validation scores) at step 240 and 200 for\n7B and 70B model sizes. For Wiki QA domain, we\nadjust the total training steps to 500, and get the\nbest checkpoints at step 450 and 300 for 7B and\n70B models. Therefore, only \u223c2K and \u223c3K QAs\nare required in practice for fine-tuning our mod-\nels in math and Wiki QA domains. The training\nof our 7B and 70B models is based on 8 and 64\nNVIDIA A100-SXM4 (80GB) GPUs, with training\ntime about 2 and 5 hours per model, respectively.\nB\nFull Experimental Results\nTable 7 and 8 show the full results of our experi-\nments on math and Wiki QA domains. Our method\nof CoA achieves consistent improvements over\nbaselines across various LLaMa model versions\n(LLaMa, LLaMa-2 and LLaMa-2-Chat), model\nsizes (7B and 70B), and domain benchmarks. This\nshows great potential of our method being general-\nized to new model backbones and reasoning tasks.\nWe also present results on GSM8K subsets accord-\ning to varying numbers of gold reasoning steps\nin Table 9, where we confirm that CoA has more\nrobust long chain-of-thought reasoning accuracy.\nFine-Tuning Data Balance\nIn the mathematical\nreasoning domain, we also validate the importance\nof using fine-tuning data that is balanced across\ndifferent reasoning steps. Specifically, we conduct\nan ablation study on CoT-FT and CoA seeded with\nLLaMa-2-Chat-7B model, by removing the single-\nstep QA samples of ASDiv from the fine-tuning\ndata (no ASDiv). We find that CoT-FT (no AS-\nDiv) and CoA (no ASDiv) turn out to be biased\ntowards multi-step reasoning, where they achieve\nbetter performance on GSM8K and MultiArith that\ncontain mainly multi-step QAs, but suffer from\nsevere performance degradation on other datasets\nthat contain many single-step math problems. This\ndemonstrates that maintaining a good balance of\nsingle-step and multi-step reasoning data is impor-\ntant for adapting LLMs to be robust reasoners.\nSelf-Consistency Decoding\nBesides of greedy\ndecoding, we also test more advanced inference\nstrategy, i.e., self-consistency (Wang et al., 2022)\ndecoding, on our CoA reasoning method, compared\nto chain-of-thought baselines CoT-FSP and CoT-\nFT, and tool-augmented baselines Toolformer and\nToolformer - Math. We test all methods on the\nGSM8K dataset seeded with LLaMa-2-Chat-7B.\nEach method samples 16 reasoning chains and uses\nmajority voting to aggregate the 16 answers de-\nrived by the reasoning chains, to get the final an-\nswer. For the hyperparameters of sampling, we\nset the temperature, top-k and top-p as 1.0, 40 and\n0.5, respectively. Table 10 shows our evaluation\nresults. We find that our CoA method consistently\noutperforms all baseline methods when shifting\nfrom greedy decoding to self-consistency decoding.\nThis shows that our method also has better poten-\ntial to be generalized to different LLM decoding\nschemes.\nC\nFine-Tuning Data Re-writing Details\nTable 11 and 12 show the prompting examples\nfor fine-tuning data construction of our method.\nWe simply prompt LLaMa-70B to re-write ex-\nisting math and Wiki QAs as abstract reasoning\nchains, which gets rid of data distillation from\nclose-sourced LLMs, yet obtains data resources\nthat enable more effective learning of multi-step\nreasoning.\nD\nClaim of Usage\nOur use of existing scientific artifacts cited in this\npaper is consistent with their intended use. Our\ndeveloped code, data and models are intended to\nbe used for only research purposes, any usage of\nour scientific artifacts that is outside of research\ncontexts should not be allowed.\nModel\nMethod\nGSM8K\nASDiv\nSVAMP\nMAWPS\nAddSub\nSingleEQ\nSingleOp\nMultiArith\nAll\nLLaMa-7B\nCoT-FSP\n11.90\n44.69\n31.80\n56.20\n59.65\n70.28\n43.00\n57.05\nCoT-FT\n30.71\n53.19\n42.30\n55.70\n69.09\n77.05\n54.17\n64.36\nCoA\n35.71\n56.36\n51.10\n67.59\n80.51\n85.94\n68.33\n75.98\nLLaMa-2-7B\nCoT-FSP\n16.38\n47.85\n38.40\n52.41\n63.39\n82.03\n43.33\n60.53\nCoT-FT\n35.33\n57.18\n48.20\n66.08\n74.41\n85.23\n65.00\n73.03\nToolformer\n17.59\n48.55\n37.10\n47.34\n58.46\n79.54\n50.67\n59.81\nCoA\n37.83\n57.61\n51.70\n72.15\n82.48\n86.48\n73.17\n78.89\nLLaMa-2-Chat-7B\nCoT-FSP\n24.03\n54.14\n51.30\n71.90\n72.44\n85.41\n74.00\n76.32\nCoT-FT\n35.41\n59.00\n46.90\n58.23\n72.24\n85.41\n73.00\n73.37\nCoT-FT (no ASDiv)\n36.19\n44.93\n35.30\n38.48\n52.95\n61.21\n77.67\n59.61\nToolformer\n23.65\n50.85\n48.80\n61.01\n69.09\n81.85\n68.50\n70.85\nToolformer - Math\n36.01\n59.18\n47.60\n58.99\n72.44\n85.94\n75.50\n74.43\nCoA\n38.29\n59.57\n54.20\n72.41\n81.89\n88.26\n83.00\n82.13\nCoA (no ASDiv)\n39.73\n54.19\n44.40\n54.18\n73.62\n73.49\n85.33\n73.27\nCoA (no Tool)\n35.03\n58.79\n51.50\n68.10\n74.21\n86.48\n77.67\n77.38\nLLaMa-2-Chat-70B\nCoT-FSP\n56.18\n65.94\n70.60\n86.08\n89.17\n92.88\n84.50\n88.23\nCoT-FT\n60.50\n70.24\n70.40\n81.52\n87.60\n92.35\n89.17\n88.18\nToolformer\n52.54\n69.07\n73.60\n86.84\n89.76\n91.46\n81.50\n87.26\nToolformer - Math\n61.03\n70.59\n73.20\n85.57\n91.34\n91.99\n92.00\n90.60\nCoA\n62.32\n71.89\n73.40\n86.33\n94.49\n93.06\n92.33\n91.91\nGPT-J\nToolformer\n-\n40.4\n29.4\n-\n-\n-\n-\n44.0\nTable 7: Mathematical reasoning evaluation results.\nModel\nMethod\nHotpotQA\nWebQ.\nNaturalQ.\nTriviaQA\nBridge\nComparison\nAll\nLLaMa-2-7B\nCoT-FSP\n14.43\n45.26\n20.62\n33.96\n33.35\n56.95\nCoT-FT\n14.85\n57.36\n23.39\n31.50\n26.93\n52.32\nToolformer\n14.12\n42.76\n20.35\n37.11\n34.49\n57.79\nCoA\n22.00\n57.43\n29.12\n34.60\n38.28\n58.28\nLLaMa-2-Chat-7B\nCoT-FSP\n11.69\n45.46\n18.47\n34.65\n30.91\n53.48\nCoT-FT\n14.24\n56.69\n22.77\n33.51\n25.40\n51.05\nToolformer\n12.99\n44.59\n20.00\n36.22\n30.22\n54.15\nToolformer - Wiki\n15.68\n56.42\n23.86\n36.61\n32.96\n55.08\nFireAct\n19.18\n54.14\n26.20\n36.02\n35.87\n52.96\nCoA\n21.00\n56.96\n28.22\n35.97\n38.67\n57.90\nLLaMa-2-Chat-70B\nCoT-FSP\n21.39\n56.62\n28.47\n34.89\n37.42\n63.61\nCoT-FT\n23.84\n63.95\n31.90\n34.15\n39.75\n62.28\nToolformer\n22.24\n56.09\n29.04\n37.16\n40.42\n64.31\nToolformer - Wiki\n26.38\n63.82\n33.90\n37.70\n41.25\n66.64\nCoA\n27.61\n64.09\n34.94\n36.37\n43.57\n69.08\nGPT-J\nToolformer\n-\n-\n-\n26.3\n17.7\n48.8\nTable 8: Wiki QA evaluation results.\n In this task, you are given a middle-school math question (  Question ), along with a gold \n reference answer ( Gold_Answer ) as its standard solution.  There are also 4 candidate answers \n ( Answer_A/B/C/D ) generated by our AI models A, B,  C and D, and your task is to judge \n whether each candidate answer has errors. \n In the annotation, you will do a binary choice selection of  \u201cYes\u201d or \u201cNo\u201d  for each candidate \n answer for  two  types of errors (definition below).  Because there are 4 candidate answers and 2 \n types of errors. For each question, you will make  a total of 8 binary choices  . \n Specifically, we consider  two types of errors :  1  )  arithmetic error; 2) reasoning error \n 1. Whether the answer has any  arithmetic error  ? i.e.,  math derivations are wrongly calculated, \n or math equations do not hold. Choose Yes or No. For example: \n Question : A pirate crew is digging for buried treasure  on the island marked X on a map. They \n dug ten holes the first day, thirteen holes the second day, and eight holes the third day. They \n stopped digging early on the third day to fill in two holes the pirates kept falling in. How many \n holes are now on the island? \n Candidate answer : On the first day, the pirates dug  10 holes. On the second day, they dug \n 13 holes. On the third day, they dug 8 holes, and filled in 2 holes, so there are 10 + 13 + 8 - 2 \n = 27 holes. The answer is 27. \n Analysis : Yes, the answer has an arithmetic error,  where 10 + 13 + 8 - 2 should be 29. \n 2. Whether the answer has any  reasoning error ? i.e.,  misunderstanding of the question, or \n having a wrong problem solving strategy, which is unrelated to arithmetic correctness. Choose \n Yes or No. For example: \n Question : Marcus is half of Leo\u2019s age and five years  younger than Deanna. Deanna is 26. \n How old is Leo? \n Candidate answer : Marcus is half of Leo's age. So  Marcus is 26 / 2 = 13 years old. Leo is 13 \n + 5 = 18 years old. The answer is 18. \n Analysis : Yes, the answer has a reasoning error, Leo  should be (26 - 5) * 2 = 42 years old. \n Notes : \n 1. Please forgive any grammar or spelling typos in all questions and answers, they are not \n considered as math solution errors. \n 2. If you feel the gold reference answer (Gold_Answer) is wrong, just ignore it and make the \n judgment based on your own answer to the question. \nFigure 5: Guideline for human evaluation on GSM8K mathematical reasoning.\nMethod\nGold Reasoning Step\n\u2264 2\n3\n4\n5\n> 5\nCoT-FSP\n42.9\n26.3\n18.0\n10.9\n3.6\nCoT-FT\n55.5\n42.6\n25.8\n19.0\n10.8\nCoA\n55.8\n44.4\n32.5\n25.3\n15.1\n+0.3\n+1.8\n+6.7\n+6.3\n+4.3\nTable 9: Stratified LLaMa-2-Chat-7B evaluation results\non GSM8K with different gold reasoning steps. The last\nrow reports absolute accuracy improvement of our CoA\nmethod compared to fine-tuning baseline CoT-FT.\nMethod\nAccuracy\nCoT-FSP\n27.90\nCoT-FT\n39.12\nToolformer\n24.56\nToolformer - Math\n35.25\nCoA\n40.79\nTable 10: Evaluation results on GSM8K with self-\nconsistency decoding. Each model samples 16 reason-\ning chains with temperature 1.0, top-k 40 and top-p 0.5.\nAll models are developed based on LLaMa-2-Chat-7B.\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees will the grove\nworkers plant today?\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21-15=6. The answer is 6.\nC: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been [21 - 15 = y1]. The answer is y1.\nQ: The flowers cost $9, the clay pot costs $20 more than the flower, and the bag of soil costs $2 less than the flower. How much does it cost to plant the flowers?\nA: The clay pot costs $20 + $9 = $29. The bag of soil costs $9 - $2 = $7. The cost to plant the flowers is $9 + $29 + $7 = $45. The answer is 45.\nC: The clay pot costs [20 + 9 = y1]. The bag of soil costs [9 - 2 = y2]. The cost to plant the flowers is [9 + y1 + y2 = y3]. The answer is y3.\nQ: From March to August, Sam made $460 doing 23 hours of yard work. However, from September to February, Sam was only able to work for 8 hours. If Sam\nis saving up to buy a video game console that costs $600 and has already spent $340 to fix his car, how many more hours does he need to work before he can buy\nthe video game console?\nA: Sam makes $460 / 23 hrs = $20/hr. From September to February, Sam made 8hrs x $20/hr = $160. From March to February, Sam made a total of $460 + $160\n= $620. After fixing his car, he was left with $620 - $340 = $280. Sam needs another $600 - $280 = $320. Sam needs to work another $320 / $20/hr = 16 hours.\nThe answer is 16.\nC: Sam makes [460 / 23 = y1] dollars per hour. From September to February, Sam made [8 * y1 = y2] dollars. From March to February, Sam made a\ntotal of [460 + y2 = y3] dollars. After fixing his car, he was left with [y3 - 340 = y4]. Sam needs another [600 - y4 = y5] dollars. Sam needs to work\nanother [y5 / y1 = y6] hours. The answer is y6.\nQ: There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in\nthe server room?\nA: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\nC: There were originally 9 computers. For each of 4 days, 5 more computers were added. So [5 * 4 = y1] computers were added. [9 + y1 = y2].\nThe answer is y2.\nQ: Of the 90 people on William\u2019s bus, 3/5 were Dutch. Of the 1/2 of the Dutch who were also American, 1/3 got window seats. What\u2019s the number of Dutch\nAmericans who sat at the windows?\nA: On the bus, the number of Dutch people was 3/5 of the total number, a total of 3/5 x 90 = 54 people. Out of the 54 people who were Dutch, 1/2 were Dutch\nAmericans, a total of 1/2 x 54 = 27 people. If 1/3 of the passengers on the bus identifying as Dutch Americans sat at the windows, their number is 1/3 x 27 = 9.\nThe answer is 9.\nC: On the bus, the number of Dutch people was 3/5 of the total number, a total of [3/5 * 90 = y1] people. Out of the Dutch people, 1/2 were Dutch\nAmericans, a total of [1/2 * y1 = y2] people. If 1/3 of the passengers on the bus identifying as Dutch Americans sat at the windows, their number\nis [1/3 * y2 = y3]. The answer is y3.\nTable 11: Prompting examples for fine-tuning data construction in mathematical reasoning domain. Given a question\n(Q) and a gold answer (A), LLaMa-70B is prompted to generate the re-writing of answer as abstract reasoning chain\n(C). Based on that, our method trains a LLM to generate the abstract chain based on the question, and the final\nanswer is derived by reify the chain of reasoning with the domain tool (i.e., equation solver).\nQ: Fritz von Brodowski was killed during what global war that lasted from 1939 to 1945?\nA: The answer is World War II.\nW: Fritz von Brodowski > Friedrich Wilhelm Konrad von Brodowski was controversially killed while in French custody during World War II.\nC: Find the [war in which Fritz von Brodowski was killed -Wiki-> y1].\nQ: Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?\nA: The answer is Jonathan Stark.\nW: Henri Leconte > He won the French Open men\u2019s doubles title in 1984. Jonathan Stark (tennis) > During his career he won two Grand Slam doubles titles.\nC: First identify the [number of Grand Slam titles Henri Leconte won -Wiki-> y1]. Then find out the [number of Grand Slam titles Jonathan Stark won -Wiki-> y2].\nQ: The director of the romantic comedy \u201cBig Stone Gap\u201d is based in what New York city?\nA: The answer is Greenwich Village.\nW: Big Stone Gap (film) > Big Stone Gap is a 2014 American romantic comedy film directed by Adriana Trigiani. Adriana Trigiani > Adriana Trigiani is an\nItalian American film director based in Greenwich Village.\nC: First search the [director of romantic comedy \u201cBig Stone Gap\u201d -Wiki-> y1]. The name of this film\u2019s director is [y1 -NER(person)-> y2]. Then determine [y2 in\nwhat New York city -Wiki-> y3].\nQ: Are Randal Kleiser and Kyle Schickner of the same nationality?\nA: The answer is yes.\nW: Randal Kleiser > John Randal Kleiser (born July 20, 1946) is an American film director and producer. Kyle Schickner > Kyle Schickner is an American film\nproducer, writer, director, actor.\nC: First find out the [nationality of Randal Kleiser -Wiki-> y1]. Then figure out the [nationality of Kyle Schickner -Wiki-> y2].\nQ: Extras was created, written, and directed by Ricky Dene Gervais, an English comedian, actor, writer, producer, director, singer, and musician, born on which date?\nA: The answer is 25 June 1961.\nW: Ricky Gervais > Ricky Dene Gervais (born 25 June 1961) is an English comedian, actor, writer, producer, director, singer, and musician.\nC: Search [when Ricky Dene Gervais was born -Wiki-> y1].\nQ: Sameera Perera is a cricketer from what island country located southeast of the Republic of India and northeast of the Maldives?\nA: The answer is Sri Lanka.\nW: Sameera Perera > Sameera Perera (born 20 August 1988) is a Sri Lankan cricketer.\nC: Identify the [country that cricketer Sameera Perera is from -Wiki-> y1].\nQ: What screenwriter with credits for \u201cEvolution\u201d co-wrote a film starring Nicolas Cage and T\u00e9a Leoni?\nA: The answer is David Weissman.\nW: The Family Man > The Family Man is a 2000 American romantic comedy-drama film starring Nicolas Cage and T\u00e9a Leoni. David Weissman > His film credits\ninclude \u201cThe Family Man\u201d (2000), \u201cEvolution\u201d (2001), and \u201cWhen in Rome\u201d (2010).\nC: First figure out the [film of Nicolas Cage and T\u00e9a Leoni -Wiki-> y1]. The name of this film is [y1 -NER(culture)-> y2]. Then find out [who wrote y2 with\ncredits for \u201cEvolution\u201d -Wiki-> y3].\nQ: Ralph Hefferline was a psychology professor at a university that is located in what city?\nA: The answer is New York City.\nW: Ralph Hefferline > Ralph Franklin Hefferline was a psychology professor at Columbia University. Columbia University > Columbia University is a private Ivy\nLeague research university in Upper Manhattan, New York City.\nC: First identify the [university of psychology professor Ralph Hefferline -Wiki-> y1]. The university of this professor is [y1 -NER(group)-> y2]. Then figure\nout [y2 is in what city -Wiki-> y3].\nTable 12: Prompting examples for fine-tuning data construction in Wiki QA domain. Given a question (Q), a gold\nanswer (A) and its supporting Wikipedia articles (W), LLaMa-70B is prompted to generate an abstract reasoning\nchain (C) with Wikipedia searching and NER queries. Based on that, our method first trains a LLM to generate the\nabstract chain of queries based on the question, and then execute the queries by domain tools (i.e., Wikipedia search\nengine and NER toolkit). Finally, a second LLM is trained to generate the final answer based on the Wikipedia\nsearching results (excluding intermediate NER results) in the reified chain of reasoning.\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees will the grove\nworkers plant today?\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21-15=6. The answer is 6.\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\nQ: The flowers cost $9, the clay pot costs $20 more than the flower, and the bag of soil costs $2 less than the flower. How much does it cost to plant the flowers?\nA: The clay pot costs $20 + $9 = $29. The bag of soil costs $9 - $2 = $7. The cost to plant the flowers is $9 + $29 + $7 = $45. The answer is 45.\nQ: Maddie wants to see how much her mom spends on coffee each week. She makes herself 2 cups of coffee per day. Each cup has 1.5 ounces of coffee beans.\nA bag of coffee costs $8 and contains 10.5 ounces of beans. How much does she spend on her coffee per week?\nA: She uses 3 ounces of beans per day because 2 x 1.5 = 3. She uses 21 ounces of beans per week because 7 x 3 = 21. She buys 2 bags of beans per week\nbecause 21 / 10.5 = 2. She spends $16 on the beans per week because 2 x 8 = 16. The answer is 16.\nQ: There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in\nthe server room?\nA: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\nQ: From March to August, Sam made $460 doing 23 hours of yard work. However, from September to February, Sam was only able to work for 8 hours. If Sam\nis saving up to buy a video game console that costs $600 and has already spent $340 to fix his car, how many more hours does he need to work before he can buy\nthe video game console?\nA: Sam makes $460 / 23 hrs = $20/hr. From September to February, Sam made 8hrs x $20/hr = $160. From March to February, Sam made a total of $460 + $160\n= $620. After fixing his car, he was left with $620 - $340 = $280. Sam needs another $600 - $280 = $320. Sam needs to work another $320 / $20/hr = 16 hours.\nThe answer is 16.\nQ: Of the 90 people on William\u2019s bus, 3/5 were Dutch. Of the 1/2 of the Dutch who were also American, 1/3 got window seats. What\u2019s the number of Dutch\nAmericans who sat at the windows?\nA: On the bus, the number of Dutch people was 3/5 of the total number, a total of 3/5 x 90 = 54 people. Out of the 54 people who were Dutch, 1/2 were Dutch\nAmericans, a total of 1/2 x 54 = 27 people. If 1/3 of the passengers on the bus identifying as Dutch Americans sat at the windows, their number is 1/3 x 27 = 9.\nThe answer is 9.\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74-35=39. The answer is 39.\nTable 13: Few-shot examples used for CoT-FSP baseline model in mathematical reasoning domain.\nQ: Fritz von Brodowski was killed during what global war that lasted from 1939 to 1945?\nA: Fritz von Brodowski > Friedrich Wilhelm Konrad von Brodowski was controversially killed while in French custody during World War II. The answer is World War II.\nQ: Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?\nA: Henri Leconte > He won the French Open men\u2019s doubles title in 1984. Jonathan Stark (tennis) > During his career he won two Grand Slam doubles titles.\nThe answer is Jonathan Stark.\nQ: The director of the romantic comedy \u201cBig Stone Gap\u201d is based in what New York city?\nA: Big Stone Gap (film) > Big Stone Gap is a 2014 American romantic comedy film directed by Adriana Trigiani. Adriana Trigiani > Adriana Trigiani is an\nItalian American film director based in Greenwich Village. The answer is Greenwich Village.\nQ: Are Randal Kleiser and Kyle Schickner of the same nationality?\nA: Randal Kleiser > John Randal Kleiser (born July 20, 1946) is an American film director and producer. Kyle Schickner > Kyle Schickner is an American film\nproducer, writer, director, actor. The answer is yes.\nQ: Extras was created, written, and directed by Ricky Dene Gervais, an English comedian, actor, writer, producer, director, singer, and musician, born on which date?\nA: Ricky Gervais > Ricky Dene Gervais (born 25 June 1961) is an English comedian, actor, writer, producer, director, singer, and musician. The answer is 25 June 1961.\nQ: Sameera Perera is a cricketer from what island country located southeast of the Republic of India and northeast of the Maldives?\nA: Sameera Perera > Sameera Perera (born 20 August 1988) is a Sri Lankan cricketer. The answer is Sri Lanka.\nQ: What screenwriter with credits for \u201cEvolution\u201d co-wrote a film starring Nicolas Cage and T\u00e9a Leoni?\nA: The Family Man > The Family Man is a 2000 American romantic comedy-drama film starring Nicolas Cage and T\u00e9a Leoni. David Weissman > His film credits\ninclude \u201cThe Family Man\u201d (2000), \u201cEvolution\u201d (2001), and \u201cWhen in Rome\u201d (2010). The answer is David Weissman.\nQ: Ralph Hefferline was a psychology professor at a university that is located in what city?\nA: Ralph Hefferline > Ralph Franklin Hefferline was a psychology professor at Columbia University. Columbia University > Columbia University is a private Ivy\nLeague research university in Upper Manhattan, New York City. The answer is New York City.\nTable 14: Few-shot examples used for CoT-FSP baseline model in Wiki QA domain.\n"
  },
  {
    "title": "ReplaceAnything3D:Text-Guided 3D Scene Editing with Compositional Neural Radiance Fields",
    "link": "https://arxiv.org/pdf/2401.17895.pdf",
    "upvote": "14",
    "text": "ReplaceAnything3D: Text-Guided 3D Scene Editing with Compositional Neural\nRadiance Fields\nEdward Bartrum 1,2*\nThu Nguyen-Phuoc 3\nChris Xie 3\nZhengqin Li 3\nNumair Khan 3\nArmen Avetisyan 3\nDouglas Lanman 3\nLei Xiao 3\n1 University College London 2 Alan Turing Institute 3 Reality Labs Research, Meta\nOriginal\n\u201cMichelangelo statue of dog browsing cellphone\u201d\n\u201cBuddha statue\u201d\n\u201cA dragon\u201d\n\u201cA bunny on top of a stack of pancakes\u201d\n\u201cAn octopus\u201d\n\u201cA pile of cookies\u201d\n\u201cCthulhu\u201d\n\u201cOrchid pot\u201d\n\u201cHalloween pumpkin\u201d\nFigure 1. Our method enables prompt-driven object replacement for a variety of realistic 3D scenes.\nAbstract\nWe introduce ReplaceAnything3D model (RAM3D), a\nnovel text-guided 3D scene editing method that enables the\n*Work done during internship at Meta Reality Labs Research\nProject page: https://replaceanything3d.github.io\nreplacement of specific objects within a scene. Given multi-\nview images of a scene, a text prompt describing the object\nto replace, and a text prompt describing the new object, our\nErase-and-Replace approach can effectively swap objects\nin the scene with newly generated content while maintain-\ning 3D consistency across multiple viewpoints. We demon-\narXiv:2401.17895v1  [cs.CV]  31 Jan 2024\nstrate the versatility of ReplaceAnything3D by applying it to\nvarious realistic 3D scenes, showcasing results of modified\nforeground objects that are well-integrated with the rest of\nthe scene without affecting its overall integrity.\n1. Introduction\nThe explosion of new social media platforms and display\ndevices has sparked a surge in demand for high-quality 3D\ncontent. From immersive games and movies to cutting-edge\nvirtual reality (VR) and mixed reality (MR) applications,\nthere is an increasing need for efficient tools for creating\nand editing 3D content. While there has been significant\nprogress in 3D reconstruction and generation, 3D editing\nremain a less-studied area. In this work, we focus on 3D\nscene manipulation by replacing current objects in the scene\nwith new contents with only natural language prompts from\na user. Imagine putting on a VR headset and trying to re-\nmodel one\u2019s living room. One can swap out the current sofa\nwith a sleek new design, add some lush greenery, or remove\nclutter to create a more spacious feel.\nIn this project, we introduce the ReplaceAnything3D\nmodel (RAM3D), a text-guided Erase-and-Replace method\nfor scene editing. RAM3D takes multiview images of a\nstatic scene as input, along with text prompts specifying\nwhich object to erase and what should replace it. Our ap-\nproach comprises four key steps: 1) we use LangSAM [24]\nwith the text prompts to detect and segment the object to\nbe erased. 2) To erase the object, we propose a text-guided\n3D inpainting technique to fill in the background region ob-\nscured by the removed object. 3) Next, a similar text-guided\n3D inpainting technique is used to generate a new object(s)\nthat matches the input text description. Importantly, this is\ndone such that the mass of the object is minimal. 4) Fi-\nnally, the newly generated object is seamlessly composited\nonto the inpainted background in training views to obtain\nconsistent multiview images of an edited 3D scene. Then\na NeRF [26] can be trained on these new multiview images\nto obtain a 3D representation of the edited scene for novel\nview synthesis. We show that this compositional structure\ngreatly improves the visual quality of both the background\nand foreground in the edited scene.\nCompared to 2D images, replacing objects in 3D scenes\nis much more challenging due to the requirement for multi-\nview consistency. Naively applying 2D methods for mask-\ning and inpainting leads to incoherent results due to visual\ninconsistencies in each inpainted viewpoint.\nTo address\nthis challenge, we propose combining the prior knowledge\nof large-scale image diffusion models, specifically a text-\nguided image inpainting model, with learned 3D scene rep-\nresentations.\nTo generate new multi-view consistent 3D\nobjects, we adapt Hifa [57], a text-to-3D distillation ap-\nproach, to our 3D inpainting framework.\nCompared to\npure text-to-3D approaches, ReplaceAnything3D needs to\ngenerate new contents that not only follow the input text\nprompt but also are compatible with the appearance of the\nrest of the scene. By combining a pre-trained text-guided\nimage inpainting model with a compositional scene struc-\nture, ReplaceAnything3D can generate coherent edited 3D\nscenes with new objects seamlessly blended with the rest of\nthe original scene.\nIn summary, our contributions are:\n\u2022 We introduce an Erase-and-Replace approach to 3D scene\nediting that enables the replacement of specific objects\nwithin a scene at high-resolutions.\n\u2022 We propose a multi-stage approach that enables not only\nobject replacement but also removal and multiple object\nadditions.\n\u2022 We demonstrate that ReplaceAnything3D can generate\n3D consistent results on multiple scene types, including\nforward-facing and 360\u25e6 scenes.\n2. Related work\nDiffusion model for text-guided image editing\nDiffu-\nsion models trained on extensive text-image datasets have\ndemonstrated remarkable results, showcasing their ability to\ncapture intricate semantics from text prompts [38, 40, 42].\nAs a result, these models provide strong priors for various\ntext-guided image editing tasks [6, 11, 18, 30, 31]. In par-\nticular, methods for text-guided image inpainting [1, 2] en-\nable local image editing by replacing masked regions with\nnew content that seamlessly blends with the rest of the im-\nage, allowing for object removal, replacement, and addi-\ntion. These methods are direct 2D counterparts to our ap-\nproach for 3D scenes, where each view can be treated as\nan image inpainting task. However, 3D scenes present ad-\nditional challenges, such as the requirement for multi-view\nconsistency and memory constraints due to the underlying\n3D representations. In this work, ReplaceAnything3D ad-\ndresses these challenges by combining a pre-trained image\ninpainting model with compositional 3D representations.\nNeural radiance fields editing\nRecent advancements in\nNeRFs have led to significant improvements in visual qual-\nity [3, 5, 17], training and inference speed [9, 14, 32], and\nrobustness to noisy or sparse input [19, 33, 53, 55]. How-\never, editing NeRFs remains a challenging area. Most of the\nexisting work focuses on editing objects\u2019 appearance or ge-\nometry [13, 22, 45, 48, 56]. For scene-level editing, recent\nworks primarily address object removal tasks for forward-\nfacing scenes [27, 29, 52]. Instruct-NeRF2NeRF [10] of-\nfers a comprehensive approach to both appearance editing\nand object addition. However, it modifies the entire scene,\nwhile Blended-Nerf [45] and DreamEditor [58] allow for\nlocalized object editing but do not support object removal.\nThe work closest to ours is by Mirzaei et al. [27], which can\nremove and replace objects using one single image refer-\nence from the user. However, since this method relies only\non one inpainted image, it cannot handle regions with large\nocclusions across different views, and thus is only applied\non forward-facing scenes.\nIt is important to note that ReplaceAnything3D adopts\nan Erase-and-Replace approach for localized scene editing,\ninstead of modifying the existing geometry or appearance\nof the scene\u2019s contents. This makes ReplaceAnything3D the\nfirst method that holistically offers localized object removal,\nreplacement, and addition within the same framework.\nText-to-3D synthesis\nWith the remarkable success of\ntext-to-image diffusion models, text-to-3D synthesis has\ngarnered increasing attention. Most work in this area fo-\ncuses on distilling pre-trained text-to-image models into 3D\nmodels, starting with the seminal works Dreamfusion [34]\nand Score Jacobian Chaining (SJC) [49]. Subsequent re-\nsearch has explored various methods to enhance the qual-\nity of synthesized objects [20, 25, 51, 57] and disentangle\ngeometry and appearance [7]. Instead of relying solely on\npre-trained text-to-image models, recent work has utilized\nlarge-scale 3D datasets such as Objaverse [8] to improve the\nquality of 3D synthesis from text or single images [21, 36].\nIn this work, we move beyond text-to-3D synthesis by\nincorporating both text prompts and the surrounding scene\ninformation as inputs. This approach introduces additional\ncomplexities, such as ensuring the appearance of the 3D ob-\nject harmoniously blends with the rest of the scene and ac-\ncurately modeling object-object interactions like occlusion\nand shadows. By combining HiFA [57], a text-to-3D dis-\ntillation approach, with a pre-trained text-to-image inpaint-\ning model, ReplaceAnything3D aims to create more real-\nistic and coherent 3D scenes that seamlessly integrate the\nsynthesized 3D objects.\n3. Preliminary\nNeRF\nNeural Radiance Fields (NeRFs) [26] is a com-\npact and powerful implicit representation for 3D scene re-\nconstruction and rendering.\nIn particular, NeRF is con-\ntinuous 5D function whose input is a 3D location x and\n2D viewing direction d, and whose output is an emitted\ncolor c = (r, g, b) and volume density \u03c3.\nThis func-\ntion is approximated by a multi-layer perceptron (MLP):\nF\u0398 : (x, d) 7\u2192 (c, \u03c3), which is trained using an image-\nreconstruction loss. To render a pixel, the color and density\nof multiple points along a camera ray sampled from t=0 to\nD are queried from the MLP. These values are accumulated\nto calculate the final pixel color using volume rendering:\nC =\nZ D\n0\nT (t) \u00b7 \u03c3(t) \u00b7 c(t) dt\n(1)\nDuring training, a random batch of rays sampled from vari-\nous viewpoints is used to ensure that the 3D positions of the\nreconstructed objects are well-constrained. To render a new\nviewpoint from the optimized scene MLP, a set of rays cor-\nresponding to all the pixels in the novel image are sampled\nand the resulting color values are arranged into a 2D frame.\nIn this work, we utilize Instant-NGP [32], a more effi-\ncient and faster version of NeRF due to its multi-resolution\nhash encoding. This allows us to handle images with higher\nresolution and query a larger number of samples along the\nrendering ray for improved image quality.\nDistilling text-to-image diffusion models\nDreamfusion\n[34] proposes a technique called score distillation sampling\nto compute gradients from a 2D pre-trained text-to-image\ndiffusion model, to optimize the parameters of 3D neural\nradiance fields (NeRF). Recently, HiFA [57] propose an al-\nternative loss formulation, which can be computed explic-\nitly for a Latent Diffusion Model (LDM). Let \u03b8scene be\nthe parameters of a implicit 3D scene, y is a text prompt,\n\u03f5\u03d5(zt, t, y) be the pre-trained LDM model with encoder E\nand decoder D, \u03b8scene can be optimized using:\nLHiFA(\u03d5, z, x) = Et,\u03f5w(t)\n\u0002\n\u2225z \u2212 \u02c6z\u22252 + \u03bbRGB\u2225x \u2212 \u02c6x\u22252\u0003\n(2)\nwhere z = E(x) is the latent vector by encoding a ren-\ndered image x of \u03b8scene from a camera viewpoint from the\ntraining dataset, \u02c6z is the estimate of latent vector z by the\ndenoiser \u03f5\u03d5, and \u02c6x = D(\u02c6z) is a recovered image obtain\nthrough the decoder D of the LDM. Note that for brevity,\nwe incorporate coefficients related to timesteps t to w(t).\nHere we deviate from the text-to-3D synthesis task\nwhere the generated object is solely conditioned on a text\nprompt. Instead, we consider a collection of scene views\nas additional inputs for the synthesized object. To achieve\nthis, we utilize HiFA in conjunction with a state-of-the-art\ntext-to-image inpainting LDM that has been fine-tuned to\ngenerate seamless inpainting regions within an image. This\nLDM \u03f5\u03c8(zt, t, y, m) requires not only a text prompt y, but\nalso a binary mask m indicating the area to be filled in.\n4. Method\n4.1. Overview\nOur training dataset consists of a collection of n images Ii,\ncorresponding camera viewpoints vi and a text prompt yerase\ndescribing the object the user wishes to replace. Using this\ntext prompt we can obtain masks mi corresponding to ev-\nery image and camera viewpoint. We additionally have a\ntext prompt yreplace describing a new object to replace the\nold object. Our goal is to modify the masked object in ev-\nery image in the dataset to match the text prompt yreplace,\nin a multi-view-consistent manner. We can then train any\nFigure 2. An overview of RAM3D Erase and Replace stages.\nNeRF-like scene reconstruction model using the modified\nimages in order to obtain renderings of the edited scene\nfrom novel viewpoints.\nFigure 2 illustrates the overall pipeline of our Erase and\nReplace framework. Instead of modifying existing objects\u2019\ngeometry and appearance that matches the target text de-\nscriptions like other methods [10, 58], we adopt an Erase-\nand-Replace approach. Firstly, for the Erase stage, we re-\nmove the masked objects completely and inpaint the oc-\ncluded region in the background. Secondly, for the Replace\nstage, we generate new objects and composite them to the\ninpainted background scene, such that the new object blends\nin with the rest of the background. Finally, we create a new\ntraining set using the edited images and camera poses from\nthe original scene, and train a new NeRF for the modified\nscene for novel view synthesis.\nTo enable text-guided scene editing, we distill a pre-\ntrained text-to-image inpainting Latent Diffusion Model\n(LDM) to generate new 3D objects in the scene using HiFA\n[57].\nTo address the memory constraints of implicit 3D\nscenes representations like NeRF, we propose a Bubble-\nNeRF representation (see Figure 3 and 4) that only models\nthe localised part of the scene that is affected by the editing\noperation, instead of the whole scene.\n4.2. Erase stage\nIn the Erase stage, we aim to remove the object described by\nyerase from the scene and inpaint the occluded background\nregion in a multi-view consistent manner. To do so, we opti-\nmise RAM3D parameters \u03b8bg which implicitly represent the\ninpainted background scene. Note that the Erase stage only\nneeds to be performed once for the desired object to remove,\nafter which the Replace stage (Section 4.3) can be used to\ngenerate objects or even add new objects to the scene, as\ndemonstrated in the Results section. As a pre-processing\nstep, we use LangSAM [24] with text prompt yerase to ob-\ntain a mask mi for each image in the dataset. We then dilate\neach mi to obtain halo regions hi around the original input\nmask (see Figure 3).\nAt each training step, we sample image Ii, camera vi,\nmask mi, and halo region hi for a random i \u2208 {1..n}, pro-\nInpainting \nMask mi\nHalo\nRegion hi\nSample RGB\nFigure 3. The masked region (blue) serves as a conditioning signal\nfor the LDM, indicating the area to be inpainted. The nearby pix-\nels surrounding m form the halo region h (green), which is also\nrendered volumetrically by RAM3D during the Erase stage. The\nunion of these 2 regions is the Bubble-NeRF region, whilst the re-\nmaining pixels are sampled from the input image (red).\nviding them as inputs to RAM3D to compute training losses\n(left side of Figure 2) (we henceforth drop the subscript i for\nclarity). RAM3D volume renders the implicit 3D represen-\ntation \u03b8bg over rays emitted from camera viewpoint v which\npass through the visible pixels in m and h (the Bubble-\nNeRF region). The RGB values of the remaining pixels on\nthe exterior of the Bubble-NeRF are sampled from I (see\nFigure 3). These rendered and sampled pixel rgb-values are\narranged into a 2D array, and form RAM3D\u2019s inpainting\nresult for the given view, xbg. Following the HiFA formula-\ntion (see Section 3), we use the frozen LDM\u2019s E to encode\nxbg to obtain zbg, add noise, denoise with \u03f5\u03c8 to obtain \u02c6zbg,\nand decode with D to obtain \u02c6xbg. We condition \u03f5\u03c8 with I,\nm and the empty prompt, since we do not aim to inpaint\nnew content at this stage.\nWe now use these inputs to compute LHiF A (see Equa-\ntion 2). We next compute Lrecon and Lvgg on h (see Figure\n3), guiding the distilled neural field \u03b8bg towards an accurate\nreconstruction of the background.\nLrecon = MSE(xbg \u2299 h, I \u2299 h)\n(3)\nLvgg = MSE(vgg16(xbg \u2299 h), vgg16(I \u2299 h))\n(4)\nThis step is critical to ensuring that RAM3D inpaints the\nbackground correctly (as shown in Figure 12). Adopting\nthe same formulation as [47], we compute depth regular-\nisation Ldepth, leveraging the geometric prior from a pre-\ntrained depth estimator [39]. In summary, the total loss dur-\ning the Erase stage is:\nLErase = LHiF A+\u03bbreconLrecon+\u03bbvggLvgg+\u03bbdepthLdepth\n(5)\n4.3. Replace stage\nIn the second stage, we aim to add the new object described\nby yreplace into the inpainted scene. To do so, we opti-\nmise the foreground neural field \u03b8fg to render xfg, which\nis then composited with xbg to form x. Unlike \u03b8bg in the\nRender Pixels\nxfg\nxbg\nComposite\n+\nx\nFigure 4.\nReplace stage: RAM3D volumetrically renders the\nmasked pixels (shown in blue) to give xfg. The result is com-\nposited with xbg to form the combined image x.\nErase stage, \u03b8fg does not seek to reconstruct the background\nscene, but instead only the LDM-inpainted content which is\nlocated on the interior of m. Therefore in the Replace stage,\nRAM3D does not consider the halo rays which intersect h,\nbut only those intersecting m (Figure 4). These rendered\npixels are arranged in the masked region into a 2D array to\ngive the foreground image xfg, whilst the unmasked pixels\nare assigned an RGB value of 0. The accumulated densi-\nties are similarly arranged into a foreground alpha map A,\nwhilst the unmasked pixels are assigned an alpha value of 0.\nWe now composite the foreground xfg with the background\nxbg using alpha blending:\nx = A \u2299 xfg + (1 \u2212 A) \u2299 xbg\n(6)\nUsing the composited result x, we compute LHiF A as be-\nfore, but now condition \u03f5\u03c8 with the prompt yreplace, which\nspecifies the new object for inpainting. As we no longer\nrequire the other losses, we set \u03bbrecon, \u03bbvgg, \u03bbdepth to 0.\nSince the Erase stage already provides us with a good\nbackground, in this stage, \u03b8fg only needs to represent the\nforeground object. To encourage foreground/background\ndisentanglement, on every k-th training step, we substitute\nxbg with a constant-value RGB tensor, with randomly sam-\npled RGB intensity. This guides the distillation of \u03b8fg to\nonly include density for the new object; a critical augmen-\ntation to avoid spurious floaters appearing over the back-\nground (see Figure 11).\n4.4. Training the final NeRF\nOnce the inpainted background and objects have been gen-\nerated, we can create a new multi-view dataset by composit-\ning the newly generated object(s) and the inpainted back-\nground region for all training viewpoints. We then train a\nnew NeRF, using any variant and framework of choice, to\ncreate a 3D representation of the edited scene that can be\nused for novel view synthesis.\n5. Results\nWe conduct experiments on real 3D scenes varying in com-\nplexity, ranging from forward-facing scenes to 360\u25e6 scenes.\nFor forward-facing scenes, we show results for the STATUE\nand RED-NET scene from SPIn-NeRF dataset [29], as well\nas the FERN scene from NeRF [26].\nFor 360\u25e6 scene,\nwe show results from the GARDEN scene from Mip-NeRF\n360\u25e6[4]. On each dataset, we train RAM3D with a vari-\nety of yreplace, generating a diverse set of edited 3D scenes.\nPlease refer to the project page for more qualitative results.\nTraining details\nEach dataset is downsampled to have a\nshortest image side-length (height) equal to 512, so that\nsquare crops provided to the LDM inpainter include the full\nheight of the input image. The FERN scene is an excep-\ntion, in which we sample a smaller 512 image crop within\ndataset images with a downsample factor of 2. Details on\nthe resolution and cropping of input images, as well as other\nimplementation details are included in appendices B.4 and\nB.6.\n5.1. Qualitative Comparisons\nFigures 5, 6 and 7 show qualitative comparison for object\nreplacement by Instruct-NeRF2NeRF [10], Blended-NeRF\n[2] and the work by Mirzaei et al. [28] respectively.\nAs shown in Figure 5, Instruct-NeRF2NeRF struggles in\ncases where the new object is significantly different from\nthe original object (for example, replace the centerpiece\nwith a pineapple or a chess piece in Figure 5 second and\nthird column).\nMore importantly, Instruct-NeRF2NeRF\nsignificantly changes the global structure of the scene even\nwhen the edit is supposed to be local (for example, only\nreplace the centerpiece with the pineapple). Finally, note\nthat our method is capable of removing objects from the\nscene completely, while Instruct-NeRF2NeRF cannot (Fig-\nure 5 first column).\nFigure 6 shows qualitative comparisons with Blended-\nNeRF. Our method generates much more realistic and de-\ntailed objects that blend in much better with the rest of the\nscene.\nMeanwhile, Blended-NeRF only focuses on syn-\nthesizing completely new objects without taking the sur-\nrounding scenes into consideration. The synthesized object\ntherefore looks saturated and outlandish from the rest of the\nscene. Moreover, due to the memory constraint of CLIP\n[37] and NeRF, Blended-NeRF only works with images 4-\ntime smaller than ours (2016\u00d71512 vs. 504\u00d7378).\nSince Mirzaei et al. [28] did not share their code publicly,\nwe report the images adapted from their paper in Figure 7.\nOur method achieves comparable object replacement results\nwhile handling more complex lighting effects such as shad-\nows between the foreground and background objects.\nOriginal\nRemove the vase and the flowers\n\u201cPineapple\u201d\n\u201cChess piece\u201d\nOurs\nInstruct-NeRF2Nerf\nFigure 5. Comparison with Instruct-NeRF2NeRF.\n\u201cA strawberry\u201d\n\u201cA cluster of mushrooms\u201d\nBlended-NeRF\nOurs\nFigure 6. Qualitative comparison with Blended-NeRF for object\nreplacement. Our method generates results with higher quality and\ncapture more realistic lighting and details.\n5.2. Quantitative Results\n3D scene editing is a highly subjective task.\nThus, we\nmainly show various types of qualitative results and com-\nparisons, and recommend readers to refer to the project page\nfor more results. However, we follow Instruct-NeRF2NeRF\nand report 2 auxiliary metrics: CLIP Text-Image Direc-\ntion Similarity and CLIP direction consistency, as shown\nin Table 1. We compare our method quantitatively with\nInstruct-NeRF2NeRF and Blended-NeRF for the task of\nobject-replacement on two datasets GARDEN and FERN for\nvarious prompts.\nTable 1 shows that our method achieves the highest\nscore for CLIP Text-Image Direction Similarity. Interest-\ningly, Blended-NeRF directly optimizes for similarities be-\ntween CLIP embeddings of the image with the generated\nobject and target text prompts, yet it still achieves a lower\nscore than our method. For Direction Consistency Score,\n\u201cA rubber duck\u201d\n\u201cA flower pot\u201d\nMirzaei et al.\nOurs\nOriginal\nFigure 7. Qualitative comparison with Reference-guided inpaint-\ning by [28] for object replacement.\nwhich measures temporal consistency loss, we observe that\nInstruct-NeRf2NeRF scores higher than our method on edit\nprompts where it completely fails (see Figure 5). For ex-\nample, for the edit \u201dpineapple\u201d in the GARDEN dataset,\nInstruct-NeRF2NeRF not only fails to create the details of\nthe pineapple but also removes high-frequency details in the\nbackground, resulting in a blurry background. We hypothe-\nsize that this boosts the consistency score even when the edit\nis unsuccessful. Therefore, we refer readers to the compar-\nisons in the project video for more details.\n5.3. Beyond object replacements\nRemoving objects\nTo modify the scene with new con-\ntents, ReplaceAnything3D performs objects removal and\nbackground inpainting before adding new foreground ob-\njects to the scene. Although object removal is not the fo-\ncus of our work, here we show qualitative comparison with\nother NeRF-inpainting methods, in particular SPin-NeRF\nTable 1. We compute CLIP-based metrics for various datasets: (Top) GARDEN, (Middle) FACE, (Bottom) FERN.\nPrompts\nCLIP Text-Image Direction Similarity \u2191\nCLIP Direction Consistency \u2191\nOurs\nInstruct-NeRF2NeRF\nOurs\nInstruct-NeRF2NeRF\nPineapple\n0.2041\n0.0661\n0.9590\n0.9660\nChess\n0.1200\n0.0061\n0.9457\n0.9705\nOurs\nBlendedNerf\nOurs\nBlendedNerf\nMushroom\n0.0928\n0.0535\n0.9781\n0.9748\nStrawberry\n0.3165\n0.2224\n0.9808\n0.9698\nOriginal\nSpin-NeRF\nOurs\nMirzaei et al.\nFigure 8. Qualitative comparison for object removal and back-\nground inpainting task. Although object removal is not the main\nfocus of ReplaceAnything3D, our method can achieve competitive\nresults with state-of-the-art methods.\n[29] and work by Mirzaei et al. [27], to show the effective-\nness of our Erase stage. Note that both of these methods\nonly work with forward-facing scenes as shown in Figure 8.\nMeanwhile, other scene editing technique that works with\n360\u25e6 scenes such as Instruct-NeRF2NeRF is not capable of\nobject removal, as shown in Figure 5.\nAdding objects\nIn addition to removing and replacing ob-\njects in the scene, our method can add new objects based on\nusers\u2019 input masks. Figure 9 demonstrates that completely\nnew objects with realistic lighting and shadows can be gen-\nerated and composited to the current 3D scene. Notably, as\nshown in Figure 9-bottom row, our method can add more\nthan one object to the same scene while maintaining realis-\ntic scene appearance and multi-view consistency.\n5.4. Scene editing with personalized contents\nIn addition to text prompts, RAM3D enables users to re-\nplace or add their own assets to 3D scenes. This is achieved\nby first fine-tuning a pre-trained inpainting diffusion model\nwith multiple images of a target object using Dreambooth\n[41]. The resulting fine-tuned model is then integrated into\nRAM3D to enable object replacement in 3D scenes. As\nshown in Figure 10, after fine-tuning, RAM3D can effec-\ntively replace or add the target object to new 3D scenes.\nOriginal\n\u201cWhite bunny\u201d\n\u201cSmall saguaro cactus in a clay pot\u201d\n\u201cA strawberry\u201d and \u201cA white bunny\u201d\n\u201cA strawberry\u201d\nFigure 9. Given user-defined masks, ReplaceAnything3D can add\ncompletely new objects that blend in with the rest of the scene.\nDue to its compositional structure, RAM3D can add multiple ob-\njects to 3D scenes while maintaining realistic appearance, lighting,\nand multi-view consistency (bottom row).\n5.5. Ablation studies\nWe conduct a series of ablation studies to demonstrate the\neffectiveness of our method and training strategy. In Fig-\nInput images\nReplace\nAdd\nFigure 10.\nUsers can personalize a 3D scene by replacing or\nadding their own assets using a fine-tuned RAM3D. We achieve\nthis by first fine-tuning an inpainting diffusion model with five im-\nages of the target object (left), and then combining it with RAM3D\nto perform object replacement and addition with custom content.\nure 11, we show the benefits of our compositional fore-\nground/background structure and background augmentation\ntraining strategy. Specifically, we train a version of RAM3D\nusing a monolithic NeRF to model both the background and\nthe new object (combining \u03b8bg and \u03b8fg). In other words,\nthis model is trained to edit the scene in one single stage,\ninstead of separate Erase and Replace stages. We observe\nlower quality background reconstruction in this case, as evi-\ndent from the blurry hedge behind the corgi\u2019s head in Figure\n11-a.\nWe also demonstrate the advantage of using random\nbackground augmentation in separating the foreground ob-\nject from the background (see Section 4.3). Without this\naugmentation, the model is unable to accurately separate\nthe foreground and background alpha maps, resulting in a\nblurry background and floaters that are particularly notice-\nable when viewed on video (Figure 11-b). In contrast, our\nfull composited model trained with background augmenta-\ntion successfully separates the foreground and background,\nproducing sharp results for the entire scene (Figure 11-c).\nIn Fig. 12, we show the importance of the Halo region\nsupervision during the Erase training stage. Without it, our\nmodel lacks important nearby spatial information, and thus\ncannot successfully generate the background scene.\n5.6. Discussion\nBecause of our Erase-and-Replace approach for scene edit-\ning, our method might remove important structural infor-\nmation from the original objects. Therefore, our method is\nnot suitable for editing tasks that only modify objects\u2019 prop-\nerties such as their appearance or geometry (for example,\nturning a bronze statue into a gold one). Furthermore, as\nReplaceAnything3D is based on text-to-image model distil-\nlation techniques, our method suffers from similar artifacts\nto these methods, such as the Janus multi-face problem.\n(a)\n(b)\n(c)\nFigure 11. Ablation results for 3 RAM3D variants, on the statue\nscene for prompt \u201dA corgi\u201d. RGB samples are shown with ac-\ncumulated NeRF density (alpha map) in the top-left corner. The\nbubble rendering region is shown as a dotted blue line. a) A mono-\nlithic scene representation which contains both the foreground and\nbackground. b) A compositional scene model but without random\nbackground augmentation. c) Our full model.\n(a)\n(b)\nFigure 12. Ablation results for 2 RAM3D variants trained on the\nStatue scene for the Erase stage. a) Training without any super-\nvision on the halo region surrounding the inpainting mask. The\ntraining objective is ambiguous and the Bubble-NeRF model col-\nlapses to a hazy cloud. b) Adding halo losses (Lrecon and Lvgg)\nfor the halo region surrounding the Bubble-NeRF guides the dis-\ntillation of \u03b8bg towards the true background, as observed on rays\nwhich pass nearby to the occluding object. RAM3D can now in-\npaint the background scene accurately.\nIn this work, we adopt implicit 3D scene representations\nsuch as NeRF or Instant-NGP. For future work, our method\ncan also be extended to other representations such as 3D\nGaussian splats [15], similar to DreamGaussian [46]. In-\nteresting future directions to explore include disentangling\ngeometry and appearance to enable more fine-grained con-\ntrol for scene editing, addressing multi-face problems by\nadopting prompt-debiasing methods [12] or models that are\npre-trained on multiview datasets [35, 43], and developing\namortized models to speed up the object replacement pro-\ncess, similar to Lorraine et al. [23].\n6. Conclusion\nIn this work, we present ReplaceAnything3D, a text-guided\n3D scene editing method that enables the replacement of\nspecific objects within a scene.\nUnlike other methods\nthat modify existing object properties such as geometry\nor appearance, our Erase-and-Replace approach can ef-\nfectively replace objects with significantly different con-\ntents. Additionally, our method can remove or add new\nobjects while maintaining realistic appearance and multi-\nview consistency.\nWe demonstrate the effectiveness of\nReplaceAnything3D in various realistic 3D scenes, includ-\ning forward-facing and 360\u25e6 scenes. Our approach enables\nseamless object replacement, making it a powerful tool for\nfuture applications in VR/MR, gaming, and film production.\nReferences\n[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\nDiffusion for Text-driven Editing of Natural Images.\nIn\n2022 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 18187\u201318197, New Or-\nleans, LA, USA, 2022. IEEE. 2\n[2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\nLatent Diffusion, 2023. arXiv:2206.02779 [cs]. 2, 5\n[3] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pages 5855\u2013\n5864, 2021. 2\n[4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields. CVPR, 2022. 5\n[5] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-\nbased neural radiance fields. ICCV, 2023. 2\n[6] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructPix2Pix: Learning to Follow Image Editing Instruc-\ntions, 2023. arXiv:2211.09800 [cs]. 2\n[7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3d: Disentangling geometry and appearance for high-\nquality text-to-3d content creation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 2023. 3\n[8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi.\nObja-\nverse: A universe of annotated 3d objects. arXiv preprint\narXiv:2212.08051, 2022. 3\n[9] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie\nShotton, and Julien Valentin. Fastnerf: High-fidelity neural\nrendering at 200fps. arXiv preprint arXiv:2103.10380, 2021.\n2\n[10] Ayaan Haque, Matthew Tancik, Alexei A. Efros, Alek-\nsander Holynski,\nand Angjoo Kanazawa.\nInstruct-\nNeRF2NeRF: Editing 3D Scenes with Instructions, 2023.\narXiv:2303.12789 [cs]. 2, 4, 5\n[11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aber-\nman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-\nPrompt Image Editing with Cross Attention Control, 2022.\narXiv:2208.01626 [cs]. 2\n[12] Susung Hong, Donghoon Ahn, and Seungryong Kim. De-\nbiasing Scores and Prompts of 2D Diffusion for View-\nconsistent Text-to-3D Generation, 2023. arXiv:2303.15413\n[cs]. 8\n[13] Cl\u00b4ement Jambon,\nBernhard Kerbl,\nGeorgios Kopanas,\nStavros Diolatzis, Thomas Leimk\u00a8uhler, and George\u201d Dret-\ntakis.\nNerfshop:\nInteractive editing of neural radiance\nfields\u201d. Proceedings of the ACM on Computer Graphics and\nInteractive Techniques, 6(1), 2023. 2\n[14] Animesh Karnewar, Tobias Ritschel, Oliver Wang, and\nNiloy J. Mitra. Relu fields: The little non-linearity that could.\nTransactions on Graphics (Proceedings of SIGGRAPH), 41\n(4):13:1\u201313:8, 2022. 2\n[15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics, 42\n(4), 2023. 8, 13\n[16] Diederick P Kingma and Jimmy Ba.\nAdam: A method\nfor stochastic optimization. In International Conference on\nLearning Representations (ICLR), 2015. 13\n[17] Quewei Li, Feichao Li, Jie Guo, and Yanwen Guo. Uhdnerf:\nUltra-high-definition neural radiance fields. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 23097\u201323108, 2023. 2\n[18] Tianle Li, Max Ku, Cong Wei, and Wenhu Chen. Dreamedit:\nSubject-driven image editing.\nTransactions on Machine\nLearning Research, 2023. 2, 11, 13\n[19] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-\nmon Lucey. Barf: Bundle-adjusting neural radiance fields. In\nIEEE International Conference on Computer Vision (ICCV),\n2021. 2\n[20] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2023. 3\n[21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object, 2023. 3\n[22] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard\nZhang, Jun-Yan Zhu, and Bryan Russell. Editing conditional\nradiance fields. In Proceedings of the International Confer-\nence on Computer Vision (ICCV), 2021. 2\n[23] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan\nLin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-\nYu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized\ntext-to-3d object synthesis. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n17946\u201317956, 2023. 8\n[24] Luca Medeiros. Language segment anything. GitHub repos-\nitory, 2021. 2, 4, 11\n[25] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-NeRF for Shape-Guided Genera-\ntion of 3D Shapes and Textures, 2022. arXiv:2211.07600\n[cs]. 3\n[26] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 2, 3, 5\n[27] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A.\nBrubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G.\nDerpanis, and Igor Gilitschenski. Reference-guided control-\nlable inpainting of neural radiance fields. In ICCV, 2023. 2,\n7\n[28] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A.\nBrubaker, Jonathan Kelly, Alex Levinshtein, Konstanti-\nnos G. Derpanis, and Igor Gilitschenski. Reference-guided\nControllable Inpainting of Neural Radiance Fields, 2023.\narXiv:2304.09677 [cs]. 5, 6\n[29] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Konstanti-\nnos G. Derpanis, Jonathan Kelly, Marcus A. Brubaker, Igor\nGilitschenski, and Alex Levinshtein. SPIn-NeRF: Multiview\nSegmentation and Perceptual Inpainting with Neural Radi-\nance Fields, 2023. arXiv:2211.12254 [cs]. 2, 5, 7\n[30] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch,\nand Daniel Cohen-Or.\nNull-text Inversion for Edit-\ning Real Images using Guided Diffusion Models, 2022.\narXiv:2211.09794 [cs]. 2\n[31] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and\nJian Zhang. DragonDiffusion: Enabling Drag-style Manip-\nulation on Diffusion Models, 2023. arXiv:2307.02421 [cs].\n2\n[32] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, 2022. 2, 3, 11\n[33] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall,\nMehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan.\nRegnerf: Regularizing neural radiance fields for view syn-\nthesis from sparse inputs. In Proc. IEEE Conf. on Computer\nVision and Pattern Recognition (CVPR), 2022. 2\n[34] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. DreamFusion: Text-to-3D using 2D Diffusion, 2022.\narXiv:2209.14988 [cs, stat]. 3, 13\n[35] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, and Bernard\nGhanem. Magic123: One image to high-quality 3d object\ngeneration using both 2d and 3d diffusion priors.\narXiv\npreprint arXiv:2306.17843, 2023. 8\n[36] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, and Bernard\nGhanem. Magic123: One Image to High-Quality 3D Object\nGeneration Using Both 2D and 3D Diffusion Priors, 2023.\narXiv:2306.17843 [cs]. 3\n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In ICML, pages\n8748\u20138763, 2021. 5\n[38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. ArXiv, abs/2204.06125, 2022. 2\n[39] Ren\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 12179\u201312188, 2021. 4, 13\n[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, 2022. 2\n[41] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. DreamBooth: Fine\nTuning Text-to-Image Diffusion Models for Subject-Driven\nGeneration, 2023. arXiv:2208.12242 [cs]. 7\n[42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan,\nTim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with\ndeep language understanding. In Advances in Neural Infor-\nmation Processing Systems, 2022. 2\n[43] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. MVDream: Multi-view Diffusion for 3D\nGeneration, 2023. arXiv:2308.16512 [cs]. 8\n[44] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. In In-\nternational Conference on Learning Representations, 2015.\n13\n[45] Hyeonseop Song, Seokhun Choi, Hoseok Do, Chul Lee, and\nTaehyeong Kim.\nBlending-NeRF: Text-Driven Localized\nEditing in Neural Radiance Fields, 2023. arXiv:2308.11974\n[cs]. 2\n[46] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 8\n[47] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-It-3D: High-Fidelity\n3D Creation from A Single Image with Diffusion Prior,\n2023. arXiv:2303.14184 [cs]. 4, 13\n[48] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,\nand Jing Liao.\nClip-nerf: Text-and-image driven manip-\nulation of neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 3835\u20133844, 2022. 2\n[49] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 12619\u201312629, 2023.\n3\n[50] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\nNeurIPS, 2021. 11\n[51] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. ProlificDreamer: High-Fidelity\nand Diverse Text-to-3D Generation with Variational Score\nDistillation, 2023. arXiv:2305.16213 [cs]. 3\n[52] Silvan Weder, Guillermo Garcia-Hernando, \u00b4Aron Monsz-\npart, Marc Pollefeys, Gabriel Brostow, Michael Firman, and\nSara Vicente. Removing objects from neural radiance fields.\nIn CVPR, 2023. 2\n[53] Jiawei Yang, Marco Pavone, and Yue Wang. 2\n[54] Lin Yen-Chen. Nerf-pytorch. https://github.com/\nyenchenlin/nerf-pytorch/, 2020. 11, 13\n[55] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelNeRF: Neural radiance fields from one or few images.\nIn CVPR, 2021. 2\n[56] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma,\nRongfei Jia, and Lin Gao. Nerf-editing: geometry editing of\nneural radiance fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n18353\u201318364, 2022. 2\n[57] Junzhe Zhu and Peiye Zhuang.\nHiFA: High-fidelity\nText-to-3D with Advanced Diffusion Guidance,\n2023.\narXiv:2305.18766 [cs]. 2, 3, 4, 13\n[58] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and\nGuanbin Li. DreamEditor: Text-Driven 3D Scene Editing\nwith Neural Fields, 2023. arXiv:2306.13455 [cs]. 2, 4\nA. Additional qualitative comparisons\nIn Figure 13, we compare our approach with a naive 2D\nbaseline where each image is processed individually. For\neach image in the training set (first row), we mask out the\nforeground object (statue) and replace it with a new object\n(corgi) using a pre-trained text-to-image inpainting model\n(Figure 13-second row). We then train a NeRF scene with\nthese modified images. As shown in Figure 13-third row,\nthis results in a corrupted, inconsistent foreground object\nsince each view is very different from each other, in contrast\nto our multi-view consistent result.\nIn Figure 14, we demonstrate competitive performance\nwith DreamEditor [18].\nIt is important to note that\nDreamEditor has limitations in terms of handling un-\nbounded scenes due to its reliance on object-centric NeuS\n[50]. Additionally, since DreamEditor relies on mesh rep-\nresentations, it is not clear how this method will perform on\nediting operations such as object removal, or operations that\nrequire significant changes in mesh topologies.\nB. Implementation details\nB.1. NeRF architecture\nWe use an Instant-NGP [32] based implicit function for\nthe RAM3D NeRF architecture, which includes a memory-\nand speed-efficient Multiresolution Hash Encoding layer,\ntogether with a 3-layer MLP, hidden dimension 64, which\nmaps ray-sample position to RGB and density. We do not\nuse view-direction as a feature. NeRF rendering code is\nadapted from the nerf-pytorch repo [54].\nB.2. Monolithic vs Erase+Replace RAM3D\nWe use a 2-stage Erase-and-Replace training schedule for\nthe STATUE, RED-NET and GARDEN scenes. For the FERN\nscene, we use user-drawn object masks which cover a re-\ngion of empty space in the scene, therefore object removal\nis redundant. In this case, we perform object addition by\nproviding the input scene-images as background composit-\ning images to RAM3D.\nB.3. Input Masks\nWe obtain inpainting masks for object removal by pass-\ning dataset images to an off-the-shelf text-to-mask model\n[24], which we prompt with 1-word descriptions of the fore-\nground objects to remove. The prompts used are: STATUE\nscene: \u201dstatue\u201d, GARDEN scene: \u201dCentrepiece\u201d, RED-NET\nscene: \u201dBag\u201d. We dilate the predicted masks to make sure\nthey fully cover the object.\nFor the Erase experiments, we compute nearby pixels to\nthe exterior of the inpainting mask, and use them as the Halo\nregion (Fig 3).\nWe apply reconstruction supervision on\nthe Halo region as detailed in B.5. For the object-addition\n2D DDIM inpainted results \n(training views)\nOurs\nNeRF trained with\n 2D DDIM inpainted results\nOriginal\nFigure 13. Qualitative comparisons between our method RAM3D (last row) with a naive 2D baseline method, which produces view-\ninconsistent results (third row). This is because each input image is processed independently and thus vary widely from each other (second\nrow).\nexperiments in the FERN scene, we create user-annotated\nmasks in a consistent position across the dataset images,\ncovering an unoccupied area of the scene.\nB.4. Cropping the denoiser inputs\nThe LDM denoising U-net takes input images of size\n512\u00d7512. In contrast, RAM3D model outputs are of equal\nresolution to the input scene images, which can be non-\nsquare. To ensure size compatibility, we need to crop and\nresize the RAM3D outputs to 512\u00d7512 before passing them\nto the denoiser (Fig 2). For the STATUEand GARDEN scenes,\nwe resize all images to height 512 and take a centre-crop of\n512\u00d7512, which always contains the entire object mask re-\ngion. For the RED-NET scene, the object mask is positioned\non the left side of the images; we therefore select the left-\nmost 512 pixels for cropping.\nFor the FERN scene, input images are annotated with\nsmall user-provided masks. We find that the previous ap-\nproach provides too small of a mask region to the LDM\u2019s\ndenoiser. In this case, we train RAM3D using the origi-\nnal dataset downsampled by a factor of 2 to a resolution of\n2016\u00d71512, and select a rectangular crop around the object\nmask. We compute the tightest rectangular crop which cov-\ners the mask region, and then double the crop-region height\nand width whilst keeping its centre intact. Finally, we in-\ncrease the crop region height and width to the max of the\nDreamEditor\nOurs\nFigure 14. Qualitative comparison with DreamEditor [18] for ob-\nject addition. Figure adapted from the original DreamEditor paper.\nheight and width, obtaining a square crop containing the in-\npainting mask region. We apply this crop to the output of\nRAM3D and then interpolate to 512\u00d7512 before proceed-\ning as before.\nB.5. Loss functions\nDuring the Erase training stage, we find it necessary to\nbackpropagate reconstruction loss gradients through pixels\nclose to the inpainting mask (Fig 12), to successfully re-\nconstruct the background scene. We therefore additionally\nrender pixels inside the Halo region (Section B.3, Fig 3),\nand compute reconstruction loss Lrecon and perceptual loss\nLvgg on these pixels, together with the corresponding re-\ngion on the input images. Note that the masked image con-\ntent does not fall inside the Halo region in the input im-\nages - therefore Lrecon and Lvgg only provide supervision\non the scene backgrounds. For the reconstruction loss, we\nuse mean-squared error computed between the input image\nand RAM3D\u2019s RGB output. For perceptual loss, we use\nmean-squared error between the features computed at layer\n8 of a pre-trained and frozen VGG-16 network [44]. In both\ncases, the loss is calculated on the exterior of the inpainting\nmask and backpropagated through the Halo region. During\nthe Replace training phase, following Zhu and Zhuang [57],\nwe apply LBGT+ loss between our rendered output x, and\nthe LDM denoised output \u02c6x, obtaining gradients to update\nour NeRF-scene weights towards the LDM image prior (see\nHiFA Loss in Fig 2, eqn 11 [57]). No other loss functions\nare applied during this phase, thus loss gradients are only\nbackpropogated to the pixels on the interior of the inpaint-\ning masks. For memory and speed efficiency, RAM3D only\nrenders pixels which lie inside the inpainting mask at this\nstage (Fig 4), and otherwise samples RGB values directly\nfrom the corresponding input image.\nFinally, following Tang et al. [47], we apply depth regu-\nlarisation using the negative Pearson correlation coefficient\nbetween our NeRF-rendered depth map, and a monocular\ndepth estimate computed on the LDM-denoised RGB out-\nput. The depth estimate is obtained using an off-the-shelf\nmodel [39]. This loss is backpropogated through all ren-\ndered pixels; i.e the union of the inpainting mask and Halo\nregion shown in Fig 3. We do not apply this regularisation\nduring the Replace stage. In summary, the total loss func-\ntion for the Replace stage is:\nLtotal = LBGT++\u03bbdepthLdepth+\u03bbreconLrecon+\u03bbvggLvgg\n(7)\nwith loss weights as follows:\n\u03bbrecon\n=\n3, \u03bbvgg\n=\n0.03, \u03bbdepth = 3.\nWe use the Adam optimiser [16] with a learning rate of\n1e-3, which is scaled up by 10 for the Instant-NGP hash\nencoding parameters.\nB.6. Other Training details\nFollowing [34, 57], we find that classifier-free guidance\n(CFG) is critical to obtaining effective gradients for distilla-\ntion sampling from the LDM denoiser. We use a CFG scale\nof 30 during the Replace stage, and 7.5 during the Erase\nstage. We also adopt the HiFA noise-level schedule, with\nt min = 0.2, t max = 0.98, and use stochasticity hyperpa-\nrameter \u03b7 = 0. In the definition of LBGT+ loss (see eqn 11\nin [57]), we follow HiFA and choose a \u03bbrgb value of 0.1. We\nrender the RAM3D radiance function using a coarse-to-fine\nsampling strategy, with 128 coarse and 128 fine raysamples.\nDuring the Replace training stage, we swap the composited\nbackground image with a randomly chosen plain RGB im-\nage at every 3rd training step. As shown in Fig 11, this step\nis critical to achieving a clean separation of foreground and\nbackground.\nWe train RAM3D for 20,000 training steps, during both\nErase and Replace training stages, which takes approxi-\nmately 12 hours on a single 32GB V100 GPU. The output\nof Replace stage training is a set of multiview images which\nmatch the input scene images on the visible region, and con-\ntain inpainted content on the interior of the masked region\nwhich is consistent across views. To obtain novel views, we\ntrain standard novel view synthesis methods using RAM3D\nedited images and the original scene cameras poses as train-\ning datasets. We use nerf-pytorch [54] for the LLFF scenes\n(STATUE, FERN, RED-NET SCENES), and Gaussian Splat-\nting [15] for the GARDEN scene.\n"
  },
  {
    "title": "Scavenging Hyena: Distilling Transformers into Long Convolution Models",
    "link": "https://arxiv.org/pdf/2401.17574.pdf",
    "upvote": "14",
    "text": "Scavenging Hyena: Distilling Transformers into Long Convolution Models\nTokiniaina Raharison Ralambomihanta 1 Shahrad Mohammadzadeh 1 Mohammad Sami Nur Islam 1\nWassim Jabbour 1 Laurence Liang 1\nAbstract\nThe rapid evolution of Large Language Models\n(LLMs), epitomized by architectures like GPT-4,\nhas reshaped the landscape of natural language\nprocessing. This paper introduces a pioneering\napproach to address the efficiency concerns asso-\nciated with LLM pre-training, proposing the use\nof knowledge distillation for cross-architecture\ntransfer. Leveraging insights from the efficient\nHyena mechanism, our method replaces atten-\ntion heads in transformer models by Hyena, of-\nfering a cost-effective alternative to traditional\npre-training while confronting the challenge of\nprocessing long contextual information, inherent\nin quadratic attention mechanisms. Unlike con-\nventional compression-focused methods, our tech-\nnique not only enhances inference speed but also\nsurpasses pre-training in terms of both accuracy\nand efficiency. In the era of evolving LLMs, our\nwork contributes to the pursuit of sustainable AI\nsolutions, striking a balance between computa-\ntional power and environmental impact.\n1. Introduction\nIn recent years, the field of natural language processing\n(NLP) has been revolutionized by the advent of Large Lan-\nguage Models (LLMs), with the transformer architecture,\nintroduced in 2017 by Vaswani et al., marking a significant\nturning point in the literature. Despite the lack of a uni-\nversally accepted definition for LLMs, they can be broadly\nconceptualized as robust machine learning models capable\nof executing a multitude of natural language processing\ntasks simultaneously. As delineated by Yang et al. in 2023,\nthese tasks encompass:\n1. Natural language understanding\n2. Natural language generation\n1McGill\nUniversity,\nMontreal,\nCanada.\nCorrespon-\ndence to:\nTokiniaina Raharison Ralambomihanta <tokini-\naina.raharisonralambomihan@mail.mcgill.ca>.\nCopyright 2024 by the author(s).\n3. Knowledge-intensive tasks\n4. Reasoning ability\nIndeed, the landscape of Large Language Models (LLMs)\nhas seen a proliferation of diverse architectural strategies.\nThese encompass models that leverage both encoders and\ndecoders, models that solely employ encoders such as BERT,\nand models that are exclusively decoder-based like GPT-4.\nIt has been observed that decoder-only models, exemplified\nby GPT-4, demonstrate superior performance, especially\nin tasks pertaining to natural language generation, when\njuxtaposed with their encoder-based counterparts. This sug-\ngests a potential trend towards decoder-only models in the\npursuit of enhanced performance, especially when it comes\nto natural language generation tasks.\nIn the preceding year, OpenAI introduced the GPT-4 Turbo\nmodel, a significant advancement over its predecessors in\nterms of performance (OpenAI, 2023). However, the GPT-4\nmodel, with its approximately 1.7 trillion parameters, has\nsparked concerns about the substantial energy resources\nnecessitated for its pre-training. This underscores the impor-\ntance of developing sustainable AI solutions that balance\ncomputational power and environmental impact.\nOur research explores the concept of distillation as a pro-\nficient methodology for training Large Language Models\n(LLMs) with new architectures. This approach aims to mit-\nigate the substantial electricity consumption and financial\nexpenditure associated with the pre-training of new architec-\ntures, especially when the knowledge of other pre-trained\nLLMs can be utilized.\nIn particular, our work investigates distilling the knowledge\nof an LLM that uses traditional, quadratic multi-headed\nattention into an equivalent model that uses sub-quadratic\nHyena operators instead (Poli et al., 2023). It then proceeds\nto compare the results of the distillation to training that latter\nmodel from scratch.\nOur work also addresses the need for models to efficiently\nprocess long context lengths, as a longer context length cor-\nrelates to larger model memory and more complex model\nreasoning (Ding et al., 2023). The quadratic nature of at-\ntention mechanisms poses a fundamental challenge in tradi-\n1\narXiv:2401.17574v1  [cs.CL]  31 Jan 2024\nDistilling Transformers into Long Convolution Models\ntional models, limiting their ability to effectively incorporate\nlong contextual information. Recognizing the inherent ad-\nvantages of utilizing longer context in understanding and\ngenerating meaningful sequences, it becomes crucial to over-\ncome the quadratic scaling issue.\nIn traditional distillation approaches, the primary focus is\non enhancing inference speed through the compression of\nexisting models into more compact versions of the same\narchitecture. However, a notable drawback of this method\nis its tendency to diminish the language modeling abilities\nof the model. Moreover, the approach does not address\nthe quadratic scaling issue in length, as maintaining the\nsame architecture fails to resolve the long context problem.\nOur research addresses these limitations by proposing a\nnovel approach using knowledge distillation methods to\nefficiently transfer knowledge from existing transform-\ners into long convolution models, creating a model that\nexhibits improved scaling concerning context length as\nwell as reduced training costs when compared with the\nstandard pre-training approach. The following points\ndescribe the main approaches towards achieving the desired\nefficiency:\n\u2022 Knowledge Distillation for Cross-Architecture Trans-\nfer: Our research pioneers a novel approach by em-\nploying knowledge distillation techniques not only for\nmodel compression but also for transferring knowledge\nfrom existing transformers to long convolution models.\n\u2022 Knowledge Distillation Surpassing Pre-training Effi-\nciency: Our research establishes a superior distillation\nparadigm, outperforming traditional pre-training both\nin terms of accuracy and efficiency.\n2. Background\n2.1. Self Attention Mechanism\nIn transformers, for a length-L sequence u \u2208 RL\u00d7D, the\nscaled self-attention mechanism involves three learnable lin-\near projections Mq, Mk, Mv \u2208 RD\u00d7D. These projections\nare applied to the input sequence u to compute Query (Q),\nKey (K), and Value (V ) matrices:\nQ = u \u00b7 Mq, K = u \u00b7 Mk, V = u \u00b7 Mv.\nThe attention operation is defined as follows:\nA(u) = softmax\n\u0012QKT\n\u221a\nD\n\u0013\n,\nwhere SoftMax is applied row-wise. The output of self-\nattention y is obtained by multiplying the attention weights\nA(u) with the Value matrix V :\ny = SelfAttention(u) = A(u) \u00b7 V.\nThis mechanism enables the model to capture dependencies\namong elements in the input sequence, assigning varying\nimportance to different elements during computations. By\nlearning to attend to relevant parts of the sequence, self-\nattention enhances the model\u2019s ability to process sequential\ndata efficiently.\n2.2. Subquadratic Attention Replacements\nThe challenge with standard attention (Vaswani et al., 2017)\nlies in its quadratic scaling with input length N, prompting\nthe exploration of subquadratic alternatives. Notable exam-\nples include the Attention Free Transformer (Zhai et al.,\n2021) and linear attention (Katharopoulos et al., 2020),\nwhere the time complexity is reduced while maintaining\nthe overall integrity of the transformer architecture.\nAnother alternative to attention is the use of state space mod-\nels where we capture the dynamics of the system through\ndifference equations. These models use linear mappings\nfrom an input signal to an output signal where the output\nsignal y[n] is a function of the input signal u[n] and a state\nvariable x[n]:\nx[n + 1] = Ax[n] + Bu[n]\ny[n] = Cx[n] + Du[n]\nThe state space representation provides a direct means of\ncomputing the output through the recurrence relationship.\nEnforcing linearity and time variance allows us to equiva-\nlently compute the output y[n] through a convolution with\nthe system\u2019s impulse response h[n]:\ny[n] = u[n] \u2217 h[n] = u[n] \u2217 (CAnB + D\u03b4[n])\nwhere \u2217 denotes the convolution operation, and \u03b4 the Kro-\nnecker delta function. This convolution view lets us ef-\nficiently compute the output in O(N(log N)2) through\nthe fast Fourier transform algorithm (Brigham & Mor-\nrow, 1967). Consequently, one can opt to parameterize\nA, B, C, D directly as structured matrices, as demonstrated\nin (Fu et al., 2022). Alternatively, Hyena (Poli et al., 2023)\nintroduces a novel approach with the parametrization of\nan implicit long convolution, which can then be distilled\ninto a state space representation for constant time inference\n(Massaroli et al., 2023).\n2\nDistilling Transformers into Long Convolution Models\n2.3. Distillation\nKnowledge distillation in neural networks (Hinton et al.,\n2015) involves transferring information from a larger, more\ncomplex model to a smaller one while minimizing infor-\nmation loss. This method extends to both compressing a\nsingle larger model and consolidating insights from multiple\nmodels (ensemble) into a singular one.\nDistillation, a knowledge transfer method in neural net-\nworks, leverages temperature-adjusted softmax probabili-\nties. Initially, the cumbersome model generates soft targets\nby applying a higher temperature in its softmax, aiding the\ntraining of a smaller distilled model. Besides mimicking soft\ntargets, optimizing the distilled model with correct labels\nfurther enhances learning.\nThe training involves a weighted average of two objective\nfunctions: the first part is the Kullback\u2013Leibler divergence\nwith the soft targets (at higher temperature). The second part\nis the cross entropy loss with correct labels (at temperature\n1).\nThis methodology allows the distilled model to effectively\nlearn from both the nuanced information present in the soft\ntargets generated by the larger model and the precise ground\ntruth labels, resulting in a more compact yet knowledgeable\nmodel.\nOne notable example of distillation in LLMs is the Distil-\nBERT model: DistilBERT is 40% smaller than its parent\nmodel BERT, 60% faster than its parent model, and yet\nretains 97% of BERT\u2019s language capabilities. (Sanh et al.,\n2020)\n2.4. Progressive Knowledge Transfer.\nWhen distillation is implemented on large models, there is a\nrisk that knowledge transfer is not optimally passed on from\nthe teacher model to the student model due to differences\nbetween the architectures of the teacher and student models.\nOne approach to maximize knowledge transfer is progres-\nsive knowledge transfer: the student model is first trained\nonly on the inputs and outputs of the first encoder block,\nand the student model then subsequently trains the output of\nthe next encoder block while freezing the previous trained\nblocks. (Sun et al., 2020) In our case, encoder blocks are\nreplaced by decoders as the architecture is autoregressive.\n(Fig. 2)\n3. Methods\n3.1. Hyena Operator\nHyena (Poli et al., 2023) proposes the use of implicit long\nconvolutions as a subquadratic replacement for the attention\noperator. Instead of parametrizing the state space coeffi-\ncients as in other state space models such as H3 (Fu et al.,\n2022), it chooses to directly parametrize filters h : N \u2192 Rd\n\u2014 equivalent to an LTI system\u2019s impulse response. The\nfilter is obtained by first applying a positional embedding\nPe : N \u2192 Rdf \u2014 where df is the embedding dimension\n\u2014 to the time indices. We then apply a feed forward neural\nnetwork FFN : Rdf \u2192 Rdm \u2014 where dm is the model\u2019s di-\nmension \u2014 and multiply by a windowing function to obtain\nthe filter.\nh[n] := Window(FFN(Pe[n]))\nThe hyena operator H : Rdm \u2192 Rdm uses one such filter h\nto aggregate context over a long context window and adds\nnon-linearity through a multiplicative gating mechanism.\nThe first step is to obtain three projections q, k, v through\nthe projection operation P(x, \u03b8) with parameters \u03b8. The\nprojection operations consist of a linear projection W\u03b8 fol-\nlowed by a short depth-wise convolution with a short filter\nk\u03b8 for local information exchange. We then use an element\nwise multiplication followed by a convolution and a second\nelement wise multiplication to compute the output of the\nhyena operator:\nP\u03b8(x) := k\u03b8 \u2217 (x \u00b7 W\u03b8)\nH(x) := P(x; \u03b8q) \u2299 (h \u2217 (P(x; \u03b8k) \u2299 P(x; \u03b8v)))\nwhere \u2217 is the convolution operation and \u2299 is the element-\nwise multiplication. Note that the operator can be further\ngeneralized by using different numbers of projections (Poli\net al., 2023).\n3.2. Model\nIn terms of the model used to conduct our experiments, we\nopted for the 70M parameter version of GPT-NeoX (Black\net al., 2022), which is a decoder-only transformer model\nwhose architecture closely matches that of GPT-3, except\nfor a few key differences:\n\u2022 The positional embeddings traditionally found in GPT\nmodels are swapped for rotary positional embeddings\n(RoPE), which encode the positional information of\ntokens using a rotation matrix.\n\u2022 The attention and feed-forward layers that are usually\nfound in series in traditional GPT models are instead\ncomputed in parallel for efficiency purposes.\n\u2022 All feed-forward layers are dense, contrary to the alter-\nnance of dense and sparse layers in GPT-3.\nIt is useful to note that the GPT-NeoX architecture closely\nmatches that of GPT-J. Figure 1 displays a detailed diagram\n3\nDistilling Transformers into Long Convolution Models\nFigure 1. (A) GPT NEO X Layer Architecture: 6 layers of stacked Attention and MLPs in the 70M GPT NEO X. (B) Hyena-Distilled NEO\nGPT X Layer Architecture: Replacement of attention heads by the Hyena operator for the distillation task. (C) A visual representation of\nthe attention operator, adapted from (Vaswani et al., 2017). (D) A visual representation of the Hyena operator, adapted from (Poli et al.,\n2023).\nof the architecture of the model. For the purposes of this\npaper, the goal was to replace the attention mechanism with\na Hyena mechanism, as displayed in Figure 1. It is, how-\never, important to note that the Hyena version of the model\ndoes not incorporate rotary positional embeddings due to\nthe fact that the Hyena operator already retains positional\ninformation about its input tokens. Finally, we used the\nPythia (Biderman et al., 2023) implementation of the afore-\nmentioned model, trained on the open-sourced Pile (Gao\net al., 2020) dataset.\n3.3. Distillation Procedure\nWe opt for Progressive Knowledge Transfer (Sun et al.,\n2020) to progressively train the student model S(\u00b7; \u0398s).\nFor each layer, we first do inference on the teacher model\nM(\u00b7; \u0398t) over a token dataset X to obtain a distillation\ndataset D = {(x, yi\nm)|x \u2208 X} where x is sequence of\ntoken indices and yi is the teacher model\u2019s output at layer\ni. Subsequently, we minimize the mean squared error loss\nwith yi\ns\u2014the student model\u2019s output at layer i one layer at\na time. For the last layer, we can additionally fine tune the\nmodel by doing unsupervised training on textual data.\nLi(M(\u00b7; \u0398m), S(\u00b7; \u0398s)) = E(x,yi)\u223cD[MSE(yi\nm, yi\ns)]\n3.4. Training Dataset and Procedure\nWe use OpenWebText (Gokaslan & Cohen, 2019) for all\nlanguage modeling experiments. A tokenized pre-training\ndataset was obtained by randomly sampling 2M examples\nfrom OpenWebText with each pre-training example having\na context length of 1024. The dataset was separated into a\n4\nDistilling Transformers into Long Convolution Models\nFigure 2. Progressive knowledge transfer on a Pythia model on its decoder layers. Adapted from (Sun et al., 2020).\ntraining set and a validation set with 0.1% being reserved\nfor validation. For distillation experiments, the same 40M\ntokens were sampled from the training set to obtain the\ndistillation datasets used to train each layer.\nAll experiments use the same 6-layer GPTNeoX style ar-\nchitecture with the same dimensions as in the 70M teacher\nmodel. We first pre-train the model from scratch on 1B\ntokens based on the hyperparameters for Pythia (Biderman\net al., 2023) and Hyena models (Poli et al., 2023). We define\npre-training as the process of doing unsupervised learning\non textual data starting with a randomly initialized model.\nAs well, we define unsupervised-tuning (CE-tinetune) as\nthe process of doing unsupervised learning on textual data\nstarting with a model checkpoint. In our pre-training phase,\nwe implement a linear warm-up spanning 300 training steps,\nfollowed by a learning rate decrease using cosine decay over\n2000 iterations. This decay continues until we reach 10% of\nthe maximum learning rate, at which point the learning rate\nremains constant. Similarly, in the distillation process, we\nincorporate a linear warm-up over 2.5% of the total training\nsteps, followed by a decay over the entire set of steps until\nwe hit 10% of the maximum learning rate. We try only do-\ning distillation (MSE) as well as fine-tuning (CE-tinetune)\n.All experimment are designed to run in 5 hours on a RTX\n3090.\n4. Language Modeling Results\n4.1. Perplexity Scores\nFor OpenWebText, the validation set obtained in the same\nway as the pre-training dataset was used to compute per-\nplexity for all models. The same procedure was used on the\ntest split of WikiText (Merity et al., 2016). The perplexity\nscores for both WikiText and OpenWebText were obtained\nover a context length of 1024 tokens.\nTable 1. Perplexity scores of Pythia 70M teacher model, pre-\ntrained Hyena model, Hyena student model distilled with MSE\nloss, and Hyena student model finetuned after distillation from top\nto bottom respectively.\nMODEL\nWIKITEXT\nOPENWEBTEXT\nPYTHIA-70M (TEACHER)\n51.4\n35.3\nPRE-TRAINED\n230\n64.9\nMSE\n155.8\n63.5\nCE FINE-TUNE\n121.2\n49.6\n4.2. Language Evaluation\nWe applied a series of natural language tasks on three mod-\nels of interest: (1) a GPT model that used Hyena as a drop-in\nreplacement for attention, (2) a Pythia 70M teacher model\nthat used attention, and (3) a Pythia 70M student model\nthat used Hyena and was distilled via using joint knowledge\ntransfer (JKT).\nWe used the Language Model Evaluation Harness (lm eval)\n(Gao et al., 2021) to benchmark these three models on mul-\ntiple different natural language tasks. (Table 2) We used\n32-bit floating point precision on all tests to ensure repro-\nducibility and to minimize the effect of machine error due\nto low precision.\n5\nDistilling Transformers into Long Convolution Models\nTable 2. Evaluation of Model Performance. Joint knowledge transfer is abbreviated as JKT. All results were measured using the Language\nModel Evaluation Harness (Gao et al., 2021) with 32-bit floating point precision; the first value is the accuracy, followed by the standard\ndeviation.\nTASK\nMETRIC\nGPT HYENA\nPYTHIA 70M TEACHER\nPYTHIA 70M JKT STUDENT\nARC CHALLENGE\nACC\n0.1775 \u00b1 0.0112\n0.1749 \u00b1 0.0111\n0.1792 \u00b1 0.0112\nARC EASY\nACC\n0.3998 \u00b1 0.0101\n0.3754 \u00b1 0.0099\n0.3270 \u00b1 0.0096\nLOGIQA\nACC\n0.1966 \u00b1 0.0156\n0.2104 \u00b1 0.0160\n0.1982 \u00b1 0.0156\nPIQA\nACC\n0.5832 \u00b1 0.0115\n0.5985 \u00b1 0.0114\n0.5408 \u00b1 0.0116\nSCIQ\nACC\n0.5910 \u00b1 0.0156\n0.6400 \u00b1 0.0152\n0.3570 \u00b1 0.0152\nWINOGRANDE\nACC\n0.5004 \u00b1 0.0141\n0.5296 \u00b1 0.0140\n0.4886 \u00b1 0.0140\nWSC\nACC\n0.3750 \u00b1 0.0477\n0.3654 \u00b1 0.0474\n0.5865 \u00b1 0.0485\n5. Discussion\n5.1. Analysis\nAs seen in table 1, our experimental results demonstrate\nthe advantage of progressive knowledge transfer over tra-\nditional pre-training approaches in terms of model perfor-\nmance achieved within a comparable GPU-hour budget.\nImportantly, without any additional unsupervised learning,\nour method yields superior performance, indicating the effi-\nciency of our progressive knowledge transfer strategy.\nFurthermore, our findings reveal the potential for distillation\nas an initialization step before unsupervised learning. This\napproach offers increased performance at the same training\ncost as conventional pre-training as well as pure knowledge\ntransfer. This suggests that our knowledge distillation ap-\nproach not only offers improved initial performance but\nalso allows for additional optimization without incurring\nadditional training expenses.\nA closer examination of our results underscores the signifi-\ncant impact of knowledge distillation on model generaliza-\ntion. Indeed, the increased improvements on the WikiText\nperplexity scores with distillation emphasize the effective-\nness of our approach in enhancing the model\u2019s capacity to\nextrapolate on unseen data with the teacher model\u2019s knowl-\nedge. This contributes valuable insights into the broader\napplicability and robustness of knowledge distillation in\nmachine learning scenarios, particularly when compared to\nconventional pre-training strategies.\nTable 2 suggests that pre-training a GPT model with Hyena\ngenerally yields similar yet slightly lower accuracy than a\nPythia 70M model that uses Hyena. These results suggest\nthat LLMs that use Hyena are generally able to perform as\nwell as attention-based LLM models, Hyena-based models\ntypically have a slightly lower measured performance. We\nobserve that a student Pythia 70M JKT model generally has\na slightly inferior performance compared to a pre-trained\nattention-based Pythia 70M model, though model perfor-\nmance is generally within a similar range, except for Sciq\nwhere the student model\u2019s accuracy is noticeably lower than\nGPT Hyena and the teacher model. However, for the Arc\nChallenge and Wsc tasks, the Pythia 70M student model\nslightly outperforms and noticeably outperforms the other\ntwo models.\nThus our results suggest that joint knowledge transfer on\na student Hyena model generally conserves the language\ncapabilities of its teacher model, and that the student Hyena\nmodel can outperform its teacher model in some cases. Be-\ncause Hyena is more computationally efficient than atten-\ntion when compared directly, and because joint knowledge\ntransfer may be more computationally efficient than tra-\nditional pre-training, our results show encouraging signs\nthat joint knowledge transfer on a Hyena student model\noffers a computationally efficient alternative to pre-training\nattention-based large language models.\n5.2. Limitations\nModel Size: Due to time constraints and limited access\nto, scaling our approach to larger models was impossible.\nConsequently, the generalizability of our approach to deeper\nor wider models remains unclear. Therefore, further ex-\nperimentation with larger models remains to be done for\nassessing the practicality of our method.\nTraining Time: Similarly to the above limitation, train-\ning times for obtained reported results were limited to 5h.\nTherefore, we could not determine whether there exists an\noptimal duration of distillation before normal pre-training\nbecomes advantageous.\nBenchmarking: We noticed that using different floating\npoint precision values for the lm eval tests would give dif-\nferent results. Thus, we opted to use 32-bit floating point\nprecision, though it is difficult for us to directly quantify how\nmuch machine error is present. For the Lambada OpenAI\ntask, some of our models reported a very high perplexity\nscore and a very low accuracy score; we decided to exclude\nthese results from our main results, as further investigation\nis needed to determine the root cause behind these outlier\nresults.\n6\nDistilling Transformers into Long Convolution Models\n6. Future Work\nIn future investigations, we aim to explore the compressibil-\nity of the teacher model into a more compact state space\nmodel, beyond the current literature\u2019s focus on reducing\ndimensionality and depth. This involves an inquiry into the\nadaptability of attention mechanisms during compression.\nFurther, we plan to evaluate various distillation approaches,\nanalyzing how performance differences scale with distilla-\ntion time and the percentage of unsupervised learning. To\naddress the limitations related to model size and training\ntime, future works will involve assessing the proposed ap-\nproach on larger language models. Additionally, we aspire\nto evaluate distillation on different sub-quadratic attention\nreplacements, paving the way for a more comprehensive\nunderstanding of the applicability and scalability of our\nknowledge distillation methodology.\n7. Conclusion\nWe evaluated the effectiveness of using joint knowledge\ntransfer with Hyena operators (as a drop-in replacement for\nattention) to improve the computational efficiency of LLMs\nduring training. As a result, we defined a Pythia 70M model\nwith attention as a teacher model, and performed distillation\non a Pythia 70M student model by replacing attention with\nthe Hyena operator. By evaluating model perplexity scores\non the OpenWebText and WikiText datasets, we observed\nthat a Pythia 70M Hyena model that underwent progres-\nsive knowledge transfer performed better than a Pythia 70M\nHyena model that was pre-trained. In addition, we observed\nthat fine-tuning Pythia 70M after progressive knowledge\ntransfer noticeably decreases the perplexity score, thus fur-\nther improving model performance. In terms of natural\nlanguage tasks, a student Hyena model generally had slighly\nlower accuracy than its teacher model, though in two in-\nstances the student Hyena model was able to outperform its\nteahcer model. These initial results show encouraging signs\nthat joint knowledge transfer on Hyena student models is ca-\npable of conserving a large proportion of a teacher model\u2019s\nlangauge capabilities, thus offering a viable alternative for\ntraining LLMs. As a result, our results show promising signs\nthat LLMs using Hyena as a drop-in replacement for atten-\ntion, coupled with progressive knowledge transfer, are more\ncomputationally efficient during model training, compared\nto current attention-based transformers.\nReferences\nBiderman, S., Schoelkopf, H., Anthony, Q., Bradley, H.,\nO\u2019Brien, K., Hallahan, E., Khan, M. A., Purohit, S.,\nPrashanth, U. S., Raff, E., Skowron, A., Sutawika, L.,\nand Van Der Wal, O. Pythia: A suite for analyzing large\nlanguage models across training and scaling. In Proceed-\nings of the 40th International Conference on Machine\nLearning, ICML\u201923. JMLR.org, 2023.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. Gpt-neox-20b: An\nopen-source autoregressive language model, 2022.\nBrigham, E. O. and Morrow, R. E. The fast Fourier trans-\nform. IEEE Spectrum, 4(12):63\u201370, December 1967.\nISSN 0018-9235. doi: 10.1109/MSPEC.1967.5217220.\nDing, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W.,\nZheng, N., and Wei, F. Longnet: Scaling transformers to\n1,000,000,000 tokens, 2023.\nFu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A.,\nand R\u00b4e, C. Hungry Hungry Hippos: Towards Language\nModeling with State Space Models. 2022. doi: 10.48550/\nARXIV.2212.14052.\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\nFoster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\nPresser, S., and Leahy, C. The pile: An 800gb dataset of\ndiverse text for language modeling, 2020.\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,\nC., Golding, L., Hsu, J., McDonell, K., Muennighoff,\nN., et al. A framework for few-shot language model\nevaluation. Version v0. 0.1. Sept, 2021.\nGokaslan,\nA. and Cohen,\nV.\nOpenwebtext cor-\npus.\nhttp://Skylion007.github.io/\nOpenWebTextCorpus, 2019.\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowl-\nedge in a neural network. ArXiv, abs/1503.02531, 2015.\nURL\nhttps://api.semanticscholar.org/\nCorpusID:7200347.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret,\nF. Transformers are RNNs: Fast Autoregressive Trans-\nformers with Linear Attention. 2020. doi: 10.48550/\nARXIV.2006.16236.\nMassaroli, S., Poli, M., Fu, D. Y., Kumbong, H., Par-\nnichkun, R. N., Timalsina, A., Romero, D. W., McIntyre,\nQ., Chen, B., Rudra, A., Zhang, C., Re, C., Ermon, S.,\nand Bengio, Y. Laughing Hyena Distillery: Extracting\nCompact Recurrences From Convolutions. 2023. doi:\n10.48550/ARXIV.2310.18780.\n7\nDistilling Transformers into Long Convolution Models\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer\nsentinel mixture models, 2016.\nOpenAI, Nov 2023.\nURL https://openai.com/\nblog/new-models-and-developer-\nproducts-announced-at-devday.\nPoli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T.,\nBaccus, S., Bengio, Y., Ermon, S., and R\u00b4e, C. Hyena\nhierarchy: Towards larger convolutional language models,\n2023.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. Distilbert,\na distilled version of bert: smaller, faster, cheaper and\nlighter, 2020.\nSun, Z., Yu, H., Song, X., Liu, R., Yang, Y., and Zhou, D.\nMobilebert: a compact task-agnostic bert for resource-\nlimited devices, 2020.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. 2017. URL https://arxiv.org/\npdf/1706.03762.pdf.\nYang, J., Jin, H., Tang, R., Han, X., Feng, Q., Jiang, H., Yin,\nB., and Hu, X. Harnessing the power of llms in practice:\nA survey on chatgpt and beyond, 2023.\nZhai, S., Talbott, W., Srivastava, N., Huang, C., Goh, H.,\nZhang, R., and Susskind, J. An Attention Free Trans-\nformer. 2021. doi: 10.48550/ARXIV.2105.14103.\n8\nDistilling Transformers into Long Convolution Models\nA. Appendix\nA.1. Hyper Parameters\nHyperparameter tuning played a pivotal role in optimizing the distillation process. Tuning focused on the learning rate and\nbatch size for the generated activations of the teacher model. Three values for each variable were systematically tested,\nwith the selection based on achieving the lowest Mean Squared Error (MSE) for the 6th layer of the distilled model. The\nresulting validation and training losses are summarized in Table 3.\nTable 3. Distillation hyper parameter search results\n(LEARNING RATE, BATCH SIZE)\nTRAINING MSE\nVALIDATION MSE\n(0.001, 60)\n0.1312\n0.1344\n(0.0025, 60)\n0.1669\n0.1652\n(0.0001, 60)\n0.2050\n0.2012\n(0.001, 240)\n0.3111\n0.3069\nTable 4. Best hyper-parameters for the 2 methods of distillation\nHYPER-PARAMETER\nMSE\nCE FINE-TUNE\nDISTILLATION EPOCHS\n8\n6\nFINE-TUNING EPOCHS\n0\n6\nWEIGHT DECAY\n0.1\n0.1\nMAXIMUM LEARNING RATE\n1 \u00b7 10\u22123\n1 \u00b7 10\u22123\nMINIMUM LEARNING RATE\n1 \u00b710\u22124\n1 \u00b710\u22124\nBETAS\n(0.9,0.98)\n(0.9,0.98)\nBATCH SIZE\n60\n60\nTable 5. Best hyper-parameters for the pre-trained model\nHYPER-PARAMETER\nVALUE\nWEIGHT DECAY\n0.1\nMAXIMUM LEARNING RATE\n1 \u00b7 10\u22123\nMINIMUM LEARNING RATE\n1 \u00b710\u22124\nBETAS\n(0.9,0.98)\nWARM-UP PERCENTAGE\n2.5%\nTOTAL NUMBER OF TOKENS\n1B\nBATCH SIZE (TOKENS)\n0.5M\n9\n"
  },
  {
    "title": "CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting",
    "link": "https://arxiv.org/pdf/2401.18075.pdf",
    "upvote": "7",
    "text": "CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting\nJiezhi \u201cStephen\u201d Yang1 Khushi Desai2 Charles Packer3\nHarshil Bhatia4 Nicholas Rhinehart3 Rowan McAllister5 Joseph Gonzalez3\n1Harvard University\n2Columbia University\n3UC Berkeley\n4Avataar.ai\n5Toyota Research Institute\nAbstract\nWe propose CARFF: Conditional Auto-encoded Radi-\nance Field for 3D Scene Forecasting, a method for pre-\ndicting future 3D scenes given past observations, such as\n2D ego-centric images.\nOur method maps an image to\na distribution over plausible 3D latent scene configura-\ntions using a probabilistic encoder, and predicts the evo-\nlution of the hypothesized scenes through time.\nOur la-\ntent scene representation conditions a global Neural Radi-\nance Field (NeRF) to represent a 3D scene model, which\nenables explainable predictions and straightforward down-\nstream applications.\nThis approach extends beyond pre-\nvious neural rendering work by considering complex sce-\nnarios of uncertainty in environmental states and dynamics.\nWe employ a two-stage training of Pose-Conditional-VAE\nand NeRF to learn 3D representations. Additionally, we\nauto-regressively predict latent scene representations as a\npartially observable Markov decision process, utilizing a\nmixture density network. We demonstrate the utility of our\nmethod in realistic scenarios using the CARLA driving sim-\nulator, where CARFF can be used to enable efficient tra-\njectory and contingency planning in complex multi-agent\nautonomous driving scenarios involving visual occlusions.\nOur website containing video demonstrations and code is\navailable at: www.carff.website.\n1. Introduction\nHumans often imagine what they cannot see given partial\nvisual context. Consider a scenario where reasoning about\nthe unobserved is critical to safe decision-making: for ex-\nample, a driver navigating a blind intersection. An expert\ndriver will plan according to what they believe may or may\nnot exist in occluded regions of their vision. The driver\u2019s\nbelief \u2013 defined as the understanding of the world mod-\neled with consideration for inherent uncertainties of real-\nworld environments \u2013 is informed by their partial observa-\ntions (i.e., the presence of other vehicles on the road), as\nwell as their prior knowledge (e.g., past experience nav-\nigating this intersection). When reasoning about the un-\nobserved, humans are capable of holding complex beliefs\nCARFF \nEncoder\n3D \nPlanning\n3D State Beliefs\n3D Planning Results\nInput Image\nSTOP\nCARFF 3D Driving Planning Application\nFigure 1. CARFF 3D planning application for driving. An in-\nput image containing a partially observable view of an intersec-\ntion is processed by CARFF\u2019s encoder to establish 3D environ-\nment state beliefs, i.e. the predicted possible state of the world:\nwhether or not there could be another vehicle approaching the in-\ntersection. These beliefs are used to forecast the future in 3D for\nplanning, generating one among two possible actions for the vehi-\ncle to merge into the other lane.\nover not just the existence and position of individual objects\n(e.g., whether or not there is an oncoming car), but also the\nshapes, colors, and textures composing the entire occluded\nportion of the scene.\nTraditionally,\nautonomous\nsystems\nwith\nhigh-\ndimensional sensor observations such as video or LiDAR\nprocess the data into low-dimensional state information,\nsuch as the position and velocity of tracked objects, which\nare then used for downstream prediction and planning.\nThis object-centric framework can be extended to reason\nabout partially observed settings by modeling the existence\nand state of potentially dangerous unobserved objects in\naddition to tracked fully observed objects. Such systems\noften plan with respect to worst-case hypotheses, e.g., by\nplacing a \u201cghost car\u201d traveling at speed on the edge of the\nvisible field of view [41].\nRecent advances in neural rendering have seen tremen-\ndous success in learning 3D scene representations directly\nfrom posed multi-view images using deep neural networks.\nNeural Radiance Fields (NeRF) allow for synthesizing\nnovel images in a 3D scene from arbitrary viewing angles,\nmaking seeing behind an occlusion as simple as rendering\nfrom an unoccluded camera angle. Because NeRF can gen-\nerate RGB images from novel views, it decouples the de-\npendency of the scene representation on the object detec-\n1\narXiv:2401.18075v1  [cs.CV]  31 Jan 2024\ntion and tracking pipeline. For example, images rendered\nfrom a NeRF may contain visual information that would\nbe lost by an object detector, but may still be relevant for\nsafe decision-making. Additionally, because NeRF repre-\nsents explicit geometry via an implicit density map, it can\nbe used directly for motion planning without requiring any\nrendering [1]. NeRF\u2019s ability to represent both visual and\ngeometric information makes them a more general and in-\ntuitive 3D representation for autonomous systems.\nDespite NeRF\u2019s advantages, achieving probabilistic pre-\ndictions in 3D based on reasoning from the occluded is\nchallenging. For example, discriminative models that yield\ncategorical predictions are unable to capture the underly-\ning 3D structure, impeding their ability to model uncer-\ntainty.\nWhile prior work on 3D representation captures\nview-invariant structures, their application is primarily con-\nfined to simple scenarios [17]. We present CARFF, which\nto our knowledge, is the first forecasting approach in sce-\nnarios with partial observations that uniquely facilitates\nstochastic predictions within a 3D representation, effec-\ntively integrating visual perception and explicit geometry.\nCARFF addresses the aforementioned difficulties by\nproposing PC-VAE: Pose-Conditioned Variational Autoen-\ncoder, a framework that trains a convolution and Vision\nTransformer (ViT) [9] based image encoder. The encoder\nmaps a potentially partially observable ego-centric image to\nlatent scene representations, which hold state beliefs with\nimplicit probabilities. The latents later condition a neural\nradiance field that functions as a 3D decoder to recover 3D\nscenes from arbitrary viewpoints. This is trained after PC-\nVAE in our two-stage training pipeline (see Sec. 3.1). Ad-\nditionally, we design a mixture density model to predict the\nevolution of 3D scenes over time stochastically in the en-\ncoder belief space (see Sec. 3.2). A potential application of\nCARFF is illustrated in Fig. 1. Using the CARLA driving\nsimulator, we demonstrate how CARFF can be used to en-\nable contingency planning in real-world driving scenarios\nthat require reasoning into visual occlusions.\n2. Related work\n2.1. Dynamic Neural Radiance Fields\nNeural\nradiance\nfields:\nNeural\nRadiance\nFields\n(NeRF) [2, 23, 39] for 3D representations have gar-\nnered significant attention due to their ability to generate\nhigh-resolution, photorealistic scenes.\nInstant Neural\nGraphics Primitive (Instant-NGP) [24] speeds up training\nand rendering time by introducing a multi-resolution\nhash encoding.\nOther works like Plenoxels [11] and\nDirectVoxGo (DVGO) [38] also provide similar speedups.\nGiven the wide adoption and speedups of Instant-NGP, we\nuse it to model 3D radiance fields in our work.\nNeRFs\nhave also been extended for several tasks such as modeling\nlarge-scale unbounded scenes [2, 40, 46], scene from sparse\nviews [7, 35, 45] and multiple scenes [17, 48]. Tewari et al.\n[42] presents an in-depth survey on neural representation\nlearning and its applications.\nGeneralizable novel view synthesis models such as pix-\nelNeRF and pixelSplat [5, 51] learn a scene prior to ren-\nder novel views conditioned on sparse existing views. Dy-\nnamic NeRF, on the other hand, models scenes with moving\nobjects or objects undergoing deformation. A widely used\napproach is to construct a canonical space and predict a de-\nformation field [19, 29, 30, 32]. The canonical space is usu-\nally a static scene, and the model learns an implicitly repre-\nsented flow field [29, 32]. A recent line of work also models\ndynamic scenes via different representations and decompo-\nsition [3, 37]. These approaches tend to perform better\nfor spatially bounded and predictable scenes with relatively\nsmall variations [3, 20, 29, 51]. Moreover, these methods\nonly solve for changes in the environment but are limited in\nincorporating stochasticity in the environment.\nMulti-scene NeRF:\nOur approach builds on multi-scene\nNeRF approaches [17, 44, 48, 49] that learn a global latent\nscene representation, which conditions the NeRF, allowing\na single NeRF to effectively represent various scenes. A\nsimilar method, NeRF-VAE, was introduced by Kosiorek\net al. [17] to create a geometrically consistent 3D genera-\ntive model with generalization to out-of-distribution cam-\neras. However, NeRF-VAE [17] is prone to mode collapse\nwhen evaluated on complex, real-world datasets.\n2.2. Scene Forecasting\nPlanning in 2D space:\nIn general, planning in large and\ncontinuous state-action spaces is difficult due to the result-\ning exponentially large search space [28]. Consequently,\nseveral approximation methods have been proposed for\ntractability [22, 31].\nVarious model-free [12, 27, 43]\nand model-based [4] reinforcement learning frameworks\nemerge as viable approaches, along with other learning-\nbased methods [6, 25]. Several other approaches forecast\nfor downstream control [15], learn behavior models for con-\ntingency planning [34], or predict potential existence and\nintentions of possibly unobserved agents [26]. While these\nmethods are in 2D, we similarly reason under partial obser-\nvations, and account for these factors in 3D.\nNeRF in robotics:\nSeveral recent works have explored\nthe application of NeRFs in robotics, like localization [50],\nnavigation [1, 21], dynamics-modeling [10, 19] and robotic\ngrasping [14, 16].\nAdamkiewicz et al. [1] proposes a\nmethod for quadcopter motion planning in NeRF models\nvia sampling the learned density function, which is a de-\nsirable characteristic of NeRF that can be leveraged for\nforecasting and planning purposes.\nAdditionally, Driess\n2\nEgo- centric Lane View\nPredicted 3D View\nFigure 2. Novel view planning application. CARFF allows rea-\nsoning behind occluded views from the ego car as simple as mov-\ning the camera to see the sampled belief predictions, allowing sim-\nple downstream planning using, for example, density probing or\n2D segmentation models from arbitrary angles.\net al. [10] utilize a graph neural network to learn a dy-\nnamics model in a multi-object scene represented through\nNeRF. Li et al. [18] primarily perform pushing tasks in a\nscene with basic shapes, and approach grasping and plan-\nning with NeRF and a separately learned latent dynamics\nmodel. Prior work either only performs well on simple and\nstatic scenes [1] or has a deterministic dynamics model [18].\nCARFF focuses on complicated realistic environments in-\nvolving both state uncertainty and dynamics uncertainty,\nwhich account for the potential existence of an object and\nunknown object movements respectively.\n3. Method\n3D scene representation has witnessed significant advance-\nments in recent years, allowing for modeling environments\nin a contextually rich and interactive 3D space. This offers\nmany analytical benefits, such as producing soft occupancy\ngrids for spatial analysis and novel view synthesis for ob-\nject detection. Given the advantages, our primary objective\nis to develop a model for probabilistic 3D scene forecast-\ning in dynamic environments. However, direct integration\nof 3D scene representation via NeRF and probabilistic mod-\nels like VAE often involves non-convex and inter-dependent\noptimization, which causes unstable training. For instance,\nNeRF\u2019s optimization may rely on the VAE\u2019s latent space\nbeing structured to provide informative gradients.\nTo navigate these complexities, our method bifurcates\nthe training process into two stages (see Fig. 3).\nFirst,\nwe train the PC-VAE to learn view-invariant scene repre-\nsentations. Next, we replace the decoder with a NeRF to\nlearn a 3D scene from the latent representations. The la-\ntent scene representations capture the environmental states\nand dynamics over possible underlying scenes, while NeRF\nsynthesizes novel views within the belief space, giving us\nthe ability to see the unobserved (see Fig. 2 and Sec. 3.1).\nDuring prediction, uncertainties can be modeled by sam-\npling latents auto-regressively from a predicted Gaussian\nmixture, allowing for effective decision-making. To this\nextent, we approach scene forecasting as a partially observ-\nable Markov decision process (POMDP) over latent distri-\nbutions, which enables us to capture multi-modal beliefs for\nplanning amidst perceptual uncertainty (see Sec. 3.2).\n3.1. NeRF Pose-Conditional VAE (PC-VAE)\nArchitecture:\nGiven a scene St at timestamp t, we render\nan ego-centric observation image It\nc captured from camera\npose c. The objective is to formulate a 3D representation\nof the image where we can perform a forecasting step that\nevolves the scene forward. To achieve this, we utilize a radi-\nance field conditioned on latent variable z sampled from the\nposterior distribution q\u03d5(z|It\nc). Now, to learn the posterior,\nwe utilize PC-VAE. We construct an encoder using convolu-\ntional layers and a pre-trained ViT on ImageNet [9]. The en-\ncoder learns a mapping from the image space to a Gaussian\ndistributed latent space q\u03d5(z|It\nc) = N(\u00b5, \u03c32) parametrized\nby mean \u00b5 and variance \u03c32. The decoder, p(I|z, c), con-\nditioned on camera pose c, maps the latent z \u223c N(\u00b5, \u03c32)\ninto the image space I. This helps the encoder to generate\nlatents that are invariant to the camera pose c.\nTo enable 3D scene modeling, we employ Instant-\nNGP [24], which incorporates a hash grid and an occu-\npancy grid to enhance computation efficiency. Addition-\nally, a smaller multilayer perceptron (MLP), F\u03b8(z) can be\nutilized to model the density and appearance, given by:\nF\u03b8(z) : (x, d, z) \u2192 ((r, g, b), \u03c3)\n(1)\nHere, x \u2208 R3 and d \u2208 (\u03b8, \u03d5) represent the location vector\nand the viewing direction respectively. The MLP is also\nconditioned on the sampled scene latents z \u223c q\u03d5(z|It\nc) (see\nAppendix C).\nTraining methodology:\nThe architecture alone does not\nenable us to model complex real-world scenarios, as seen\nthrough a similar example in NeRF-VAE [17]. A crucial\ncontribution of our work is our two-stage training frame-\nwork which stabilizes the training. First, we optimize the\nconvolutional ViT based encoder and pose-conditional con-\nvolutional decoder in the pixel space for reconstruction.\nThis enables our method to deal with more complex and re-\nalistic scenes as the encoding is learned in a semantically\nrich 2D space.\nBy conditioning the decoder on camera\nposes, we achieve disentanglement between camera view\nangles and scene context, making the representation view-\ninvariant and the encoder 3D-aware. Next, once rich latent\nrepresentations are learned, we replace the decoder with a\nlatent-conditioned NeRF over the latent space of the frozen\nencoder. The NeRF reconstructs encoder beliefs in 3D for\nnovel view synthesis.\nLoss:\nOur PC-VAE is trained using standard VAE loss,\nwith mean square error (MSE) and a Kullback\u2013Leibler (KL)\ndivergence given by evidence lower bound (ELBO):\nLPC-VAE = LMSE, PC-VAE + LKLD, PC-VAE =\n||p(I|z, c\u2032\u2032) \u2212 It\nc\u2032\u2032\u22252 + Eq(z|It\nc)[log p(I|z)]\n\u2212 wKLDKL(q\u03d5(z|It\nc) || p(I|z))\n(2)\n3\nEncoder\nDecoder\nGaussian latent \ndistribution\nConditioning on \ncamera pose\nDecoded images for \ncamera pose\nGround truths for \ncamera pose\nNeRF\nStage 1: VAE Encoder and Decoder training\nStage 2: Frozen encoder, NeRF Decoder\nPosed Images\n* Images are shown as batched for training only\nFigure 3. Visualizing CARFF\u2019s two stage training process. Left: The convolutional VIT based encoder encodes each of the images I at\ntimestamps t, t\u2032 and camera poses c, c\u2032 into Gaussian latent distributions. Assuming only two timestamps and an overparameterized latent,\none of the Gaussian distributions will have a smaller \u03c32, and different \u00b5 across timestamps. Upper Right: The pose-conditional decoder\nstochastically decodes the sampled latent z using the camera pose c\u2032\u2032 into images It\nc\u2032\u2032 and It\u2032\nc\u2032\u2032. The decoded reconstruction and ground\ntruth images are used to take the loss LMSE, PC-VAE. Lower Right: A NeRF is trained by conditioning on the latent variables sampled from\nthe optimized Gaussian parameters. These parameters characterize the distinct timestamp distributions derived from the PC-VAE. An MSE\nloss is calculated for NeRF as LMSE, NeRF.\nwhere wKL denotes the KL divergence loss weight and\nz \u223c q\u03d5(z|It\nc). To make our representation 3D-aware, our\nposterior is encoded using camera c while the decoder is\nconditioned on a randomly sampled pose c\u2032\u2032.\nKL divergence regularizes the latent space to balance\nconditioned reconstruction and stochasticity under occlu-\nsion. An elevated KL divergence loss weight wKL pushes\nthe latents closer to a standard normal distribution, N(0, 1),\nthereby ensuring probabilistic sampling in scenarios un-\nder partial observation. However, excessive regularization\ncauses the latents to be less separable, leading to mode col-\nlapse. To mitigate this, we adopt delayed linear KL diver-\ngence loss weight scheduling to strike a balanced wKL.\nNext, we learn a NeRF-based decoder on the posterior\nof the VAE to model scenes. At any timestamp t we use a\nstandard pixel-wise MSE loss for training the NeRF, given\nby the following equation:\nLMSE, NeRF = \u2225It\nc \u2212 render(F\u03b8(\u00b7|q\u03d5(z|It\nc)))\u22252\n(3)\nWe use a standard rendering algorithm as proposed by\nM\u00a8uller et al. [24]. Next, we build a forecasting module over\nthe learned latent space of our pose-conditional encoder.\n3.2. Scene Forecasting\nFormulation:\nThe current formulation allows us to model\nscenes with different configurations across timestamps. In\norder to forecast future configurations of a scene given an\nego-centric view, we need to predict future latent distribu-\ntions. We formulate the forecasting as a partially observable\nMarkov decision process (POMDP) over the posterior dis-\ntribution q\u03d5(z|It\nc) in the PC-VAE\u2019s latent space.\nDuring inference, we observe stochastic behaviors un-\nder occlusion, which motivates us to learn a mixture of sev-\neral Gaussian distributions that potentially denote different\nscene possibilities. Therefore, we model the POMDP us-\ning a Mixture Density Network (MDN), with multi-headed\nMLPs, that predicts a mixture of K Gaussians.\nAt any\ntimestamp t the distribution is given as:\nq\u2032\n\u03d5(zt|It\u22121\nc\n) = MDN(q\u03d5(zt\u22121|It\u22121\nc\n))\n(4)\nThe model is conditioned on the posterior distribu-\ntion q\u03d5(zt\u22121) to learn a predicted posterior distribution\nq\u2032\n\u03d5(zt|It\u22121\nc\n) at each timestamp. The predicted posterior dis-\ntribution is given by the mixture of Gaussian:\nq\u2032\n\u03d5(zt) =\nK\nX\ni=1\n\u03c0i N(\u00b5i, \u03c32\ni )\n(5)\nhere, \u03c0i, \u00b5i, and \u03c32\ni denote the mixture weight, mean, and\nvariance of the ith Gaussian distribution within the pos-\nterior distribution. Here, K is the total number of Gaus-\nsians. For brevity we remove their conditioning on the pos-\nterior q\u03d5(zt\u22121) and sampled latent zt\u22121. We sample zt from\nthe mixture of Gaussians q\u2032\n\u03d5(zt), where zt most likely falls\nwithin one of the Gaussian modes. The scene configura-\ntion corresponding to the mode is reflected in the 3D scene\nrendered by NeRF.\nLoss:\nTo optimize the MDN, we minimize a negative log-\nlikelihood function, given by:\nLMDN = \u2212\nN\nX\ni=1\nlog\n\uf8eb\n\uf8ed\nK\nX\nj=1\n\u03c0jN(yi; \u00b5j, \u03c32\nj )\n\uf8f6\n\uf8f8\n(6)\n4\nScene 1: Ego car with \nactor ambulance\nScene 2: Ego car only\nScene 1: Ego car with \nslow- moving ambulance\nScene 2: Ego car with \nfast- moving ambulance\nMulti- Scene Approaching Intersection\nMulti- Scene Two Lane Merge\nFigure 4. Multi-scene CARLA datasets. Images illustrating the\nvarying car configurations and scenes for the Multi-Scene Two\nLane Merge dataset (left) and the Multi-Scene Approaching In-\ntersection dataset (right).\nFigure 5. Blender dataset. Simple Blender dataset with a station-\nary blue cube, accompanied by a potential red cylinder exhibit-\ning probabilistic temporal movement. The different camera poses\ndemonstrate how movement needs to be modeled probabilistically\nbased on possible occlusions from different camera angles.\nwhere yi \u223c q\u03d5(zt) is sampled from the distribution of latent\nzt, learned by the encoder, and N denotes the total number\nof samples.\nInference:\nWe consider an unseen ego-centric image and\nretrieve its posterior q\u03d5(zt) through the encoder. Next, we\npredict the possible future posterior distribution q\u2032\n\u03d5(zt+1).\nFrom the predicted posterior, we sample a scene latent and\nperform localization. We achieve this via (a) density prob-\ning the NeRF or (b) segmenting the rendered novel views\nusing off-the-shelf methods such as YOLO [33] (see Fig. 2).\nThese allow us to retrieve a corresponding Gaussian dis-\ntribution q\u03d5(zt+1) in encoder latent space. This is auto-\nregressively fed back into the MDN to predict the next\ntimestamp. See Fig. 6 for an overview of the pipeline.\n4. Results\nDecision-making under perceptual uncertainty is a perva-\nsive challenge faced in robotics and autonomous driving,\nespecially in partially observable environments encountered\nin driving tasks. In these scenarios, accurate inference re-\ngarding the presence of potentially obscured agents is piv-\notal. We evaluate the effectiveness of CARFF on similar\nreal-world situations with partial observability. We imple-\nmented several scenarios in the CARLA driving simula-\ntor [8] (see Fig. 4). A single NVIDIA RTX 3090 GPU is\nused to train PC-VAE, NeRF, and the MDN. All models,\ntrained sequentially, tend to converge within a combined\ntime frame of 24 hours. A detailed experimental setup can\nbe found in Appendix C. We show that, given partially ob-\nservable 2D inputs, CARFF performs well in predicting la-\ntent distributions that represent complete 3D scenes. Using\nthese predictions we design a CARFF-based controller for\nperforming downstream planning tasks.\n4.1. Data Generation\nWe generate datasets containing an ego object and vary-\ning actor objects in different configurations to test the ro-\nbustness of our method. We conduct experiments on (a)\nsynthetic blender dataset for simple, controllable simula-\ntion and (b) CARLA-based driving datasets for complicated\nreal-world scenarios [8].\nBlender synthetic dataset:\nThis comprises of a station-\nary blue cube (ego) accompanied by a red cylinder (actor)\nthat may or may not be present (see Fig. 5).\nIf the ac-\ntor is present, it exhibits lateral movement as depicted in\nFig. 5. This simplistic setting provides us with an inter-\npretable framework to evaluate our model.\nCARLA dataset:\nEach dataset is simulated for N times-\ntamps and uses C = 100 predefined camera poses to cap-\nture images of the environment under full observation, par-\ntial observation, and no visibility. These datasets are mod-\neled after common driving scenarios involving state uncer-\ntainty that have been proposed in related works such as Ac-\ntive Visual Planning [25].\na) Single-Scene Approaching Intersection: The ego ve-\nhicle is positioned at a T-intersection with an actor vehicle\ntraversing the crossing along an evenly spaced, predefined\ntrajectory. We simulate this for N = 10 timestamps. We\nmainly use this dataset to predict the evolution of times-\ntamps under full observation.\nb) Multi-Scene Approaching Intersection: We extend the\naforementioned scenario to a more complicated setting with\nstate uncertainty, by making the existence of the actor vehi-\ncle probabilistic. A similar intersection crossing is simu-\nlated for N = 3 timestamps for both possibilities. The ego\nvehicle\u2019s view of the actor may be occluded as it approaches\nthe T-intersection over the N timestamps. The ego vehicle\ncan either move forward or halt at the junction (see Fig. 4).\nc) Multi-Scene Multi-actor Two Lane Merge: To add\nmore environment dynamics uncertainty, we consider a\nmulti-actor setting at an intersection of two merging lanes.\nWe simulate the scenario at an intersection with partial oc-\nclusions, with the second approaching actor having variable\nspeed. Here the ego vehicle can either merge into the left\n5\nNeRF\nGaussian Mixture\nRepeated Sampling\nMixture Density \nNetwork\nProbing For Autoregressive Prediction\nPretrained\nEncoder\nSampled Beliefs\nFigure 6. Auto-regressive inference in scene prediction. The input image at timestamp t, It\nc, is encoded using the pre-trained encoder\nfrom PC-VAE. The corresponding latent distribution is fed into the Mixture Density Network, which predicts a mixture of Gaussians. Each\nof the K Gaussians is a latent distribution that may correspond to different beliefs at the next timestamp. The mixture of Gaussians is\nsampled repeatedly for the predicted latent beliefs, visualized as It+1\nc\u2032,scni, representing potentially the ith possible outcome. This is used to\ncondition the NeRF to generate 3D views of the scene. To accomplish autoregressive predictions, we probe the NeRF for the location of\nthe car and feed this information back to the pre-trained encoder to predict the scene at the next timestamp.\nlane before the second actor or after all the actors pass, (see\nFig. 4). Each branch is simulated for N = 3 timestamps.\n4.2. CARFF Evaluation\nA desirable behavior from our model is that it should predict\na complete set of possible scenes consistent with the given\nego-centric image, which could be partially observable.\nThis is crucial for autonomous driving in unpredictable en-\nvironments as it ensures strategic decision-making based on\npotential hazards. To achieve this we require a rich PC-\nVAE latent space, high-quality novel view synthesis, and\nauto-regressive probabilistic predictions of latents at future\ntimestamps.\nWe evaluate CARFF on a simple synthetic\nblender-based dataset and each CARLA-based dataset.\nEvaluation on blender dataset:\nIn Fig. 5, for both Scene\n1a and 1b, our model correctly forecasts the lateral move-\nment of the cylinder to be in either position approximately\n50% of the time, considering a left viewing angle. In Scene\n2, with the absence of the red cylinder in the input cam-\nera angle, the model predicts the potential existence of the\nred cylinder approximately 50% of the time, and predicts\nlateral movements with roughly equal probability. This val-\nidates PC-VAE\u2019s ability to predict and infer from the oc-\ncluded in the latent space, consistent with human intuitions.\nSimilar intuitions, demonstrated within the simple scenes of\nthe Blender dataset, can be transferred to driving scenarios\nsimulated in our CARLA datasets.\nPC-VAE performance and ablations:\nWe evaluate the\nperformance of PC-VAE on CARLA datasets with multiple\nencoder architectures. We show that PC-VAE effectively re-\nconstructs complex environments involving variable scenes,\nactor configurations, and environmental noise given poten-\ntially partially observable inputs (see Fig. 9). We calcu-\nlated an average Peak Signal-to-Noise Ratio (PSNR) over\nthe training data, as well as novel view encoder inputs.\nTo evaluate the quality of the latent space generated by\nthe encoder, we utilize t-SNE [47] plots to visualize the\ndistribution of latent samples for each image in a given\ndataset (see Appendix E). We introduce a Support Vector\nMachine (SVM) [13] based metric to measure the visual-\nized clustering quantitatively, where a higher value indi-\ncates better clustering based on timestamps. Most latent\nscene samples are separable by timestamps, which indicates\nthat the latents are view-invariant. Samples that are mis-\nclassified or lie on the boundary usually represent partially\nor fully occluded regions. This is desirable for forecast-\ning, as it enables us to model probabilistic behavior over\nthese samples. In this process, balancing KL divergence\nweight scheduling maintains the quality of the PC-VAE\u2019s\nlatent space and reconstructions (see Appendix C). The re-\nsults presented in Tab. 2 substantiate the benefits of our PC-\nVAE encoder architecture compared to other formulations.\nSpecifically, a non-conditional VAE fails in SVM accuracy\nas it only reconstructs images and does not capture the un-\nderlying 3D structures. Vanilla PC-VAE and PC-VAE with-\nout freezing weights require careful fine-tuning of several\nhyper-parameters and don\u2019t generalize well to drastic cam-\nera movements. Our experiments show that our proposed\nmodel is capable of sustaining stochastic characteristics via\nlatent representations in the presence of occlusion, while si-\nmultaneously ensuring precise reconstructions.\n3D novel view synthesis:\nGiven an unseen ego-centric\nview with potentially partial observations, our method\nmaintains all possible current state beliefs in 3D, and faith-\n6\nPose     Inputs\nPC- VAE Decoded Images From Set of New Pose\nFigure 7. PC-VAE reconstructions. The encoder input, It\nc, among the other ground truth images Ic viewed from camera pose c at different\ntimestamps, is reconstructed across a new set of poses c\u2032\u2032 respecting timestamp t, generating It\nc\u2032\u2032. A complete grid is in Appendix E.\nGround Truth\nPrediction Pair\nAvg. PSNR\n(Scene 1)\nAvg. PSNR\n(Scene 2)\nSingle-Scene Approaching Intersection\nMatching Pairs\n29.06\nN.A\nUn-matching Pairs\n24.01\nN.A\nMulti-Scene Approaching Intersection\nMatching Pairs\n28.00\n28.26\nUn-matching Pairs\n23.27\n24.56\nMulti-Scene Two Lane Merge\nMatching Pairs\n28.14\n28.17\nUn-matching Pairs\n22.74\n23.32\nTable 1. Averaged PSNR for fully observable 3D predictions.\nCARFF correctly predicts scene evolution across all timestamps\nfor each dataset. The average PSNR is significantly higher for each\nprediction \u02c6\nIti and corresponding ground truth, Iti. PSNR values\nfor incorrect correspondences, \u02c6\nIti, Itj, is a result of matching sur-\nroundings. The complete table of predictions is in Appendix E.\nfully reconstructs novel views from arbitrary camera angles\nfor each belief. Fig. 2 illustrates one of the possible 3D be-\nliefs that CARFF holds. This demonstrates our method\u2019s ca-\npacity for generating 3D beliefs that could be used for novel\nview synthesis in a view-consistent manner. Our model\u2019s\nability to achieve accurate and complete 3D environmental\nunderstanding is important for applications like prediction-\nbased planning.\nInference under full and partial observations:\nUnder\nfull observation, we use MDN to predict the subsequent car\npositions in all three datasets. PSNR values are calculated\nbased on bird-eye view NeRF renderings and ground truth\nbird-eye view images of the scene across different times-\ntamps. In Tab. 1 we report the PSNR values for rendered\nimages over the predicted posterior with the ground truth\nimages at each timestamp. We also evaluate the efficacy\nof our prediction model using the accuracy curve given in\nFig. 8. This represents CARFF\u2019s ability to generate stable\nbeliefs, without producing incorrect predictions, based on\nactor(s) localization results. For each number of samples\nbetween n = 0 to n = 50, we choose a random subset of\n3 fully observable ego images and take an average of the\nEncoder Architectures\nTrain\nPSNR\nSVM\nAccuracy\nNV\nPSNR\nPC-VAE\n26.30\n75.20\n25.24\nPC-VAE w/o CL\n26.24\n70.60\n24.80\nVanilla PC-VAE\n26.02\n25.70\n24.65\nPC-VAE w/o Freezing\n24.57\n5.80\n24.60\nPC-VAE w/ MobileNet\n17.14\n19.70\n17.16\nVanilla VAE\n24.15\n10.60\n11.43\nTable 2. PC-VAE metrics and ablations. CARFF\u2019s PC-VAE en-\ncoder outperforms other encoder architectures in both image re-\nconstruction and pose-conditioning. We evaluated the following\nablations: PC-VAE without Conv Layer, PC-VAE with a vanilla\nencoder, PC-VAE without freezing weights in ViT, PC-VAE re-\nplacing ViT with pre-trained MobileNet, and non pose-conditional\nVanilla VAE. The table displays the average training PSNR, novel\nview (NV) input PSNR, and SVM accuracy for latent timestamp\nprediction.\naccuracies. In scenarios with partial observable ego-centric\nimages where several plausible scenarios exist, we utilize\nrecall instead of accuracy using a similar setup. This lets us\nevaluate the encoder\u2019s ability to avoid false negative predic-\ntions of potential danger.\nFig. 8 shows that our model achieves high accuracy\nand recall in both datasets, demonstrating the ability to\nmodel state uncertainty (Approaching Intersection) and dy-\nnamic uncertainty (Two Lane Merge). The results indicate\nCARFF\u2019s resilience against randomness in resampling, and\ncompleteness in probabilistic modeling of the belief space.\nGiven these observations, we now build a reliable controller\nto plan and navigate through complex scenarios.\n4.3. Planning\nIn all our experiments, the ego vehicle must make decisions\nto advance under certain observability. The scenarios are\ndesigned such that the ego views contain partial occlusion\nand the state of the actor(s) is uncertain in some scenar-\nios. In order to facilitate decision-making using CARFF, we\ndesign a controller that takes ego-centric input images and\noutputs an action. Decisions are made incorporating sample\nconsistency from the mixture density network. For instance,\nthe controller infers occlusion and promotes the ego car to\n7\nMulti-Scene Approaching Intersection\nController Type\nActor Exists\nNo Actor\nUnderconfident\n30/30\n0/30\nOverconfident\n0/30\n30/30\nCARFF (n = 2)\n17/30\n30/30\nCARFF (n = 10)\n30/30\n30/30\nCARFF (n = 35)\n30/30\n19/30\nMulti-Scene Two Lane Merge\nController Type\nFast Actor\nSlow Actor\nUnderconfident\n30/30\n0/30\nOverconfident\n0/30\n30/30\nCARFF (n = 2)\n21/30\n30/30\nCARFF (n = 10)\n30/30\n30/30\nCARFF (n = 35)\n30/30\n22/30\nTable 3. Planning in 3D with controllers with varying sam-\npling numbers n. CARFF-based controllers outperform base-\nlines in success rate over 30 trials. For n = 10, the CARFF-based\ncontroller consistently chooses the optimal action in potential col-\nlision scenarios. For actor exists and fast-actor scenes, we con-\nsider occluded ego-centric inputs to test CARFF\u2019s ability to avoid\ncollisions. For no-actor and slow-actor scenes, we consider state\nobservability and test the controllers\u2019 ability to recognize the opti-\nmal action to advance. To maintain consistency, we use one single\nimage input across 30 trials.\npause when scenes alternate between actor presence and ab-\nsence in the samples. We use the two multi-scene datasets\nto assess the performance of the CARFF-based controller as\nthey contain actors with potentially unknown behaviors.\nTo design an effective controller, we need to find a bal-\nance between accuracy and recall (see Fig. 8). A lowered\naccuracy from excessive sampling means unwanted ran-\ndomness in the predicted state. However, taking insufficient\nsamples would generate low recall i.e., not recovering all\nplausible states. This would lead to incorrect predictions as\nwe would be unable to account for the plausible uncertainty\npresent in the environment. To find a balance, we design an\nopen-loop planning controller opting for a sampling strat-\negy that involves generating n = 2, 10, 35 samples, where\nn is a hyperparameter to be tuned for peak performance.\nFor sampling values that lie on the borders of the accu-\nracy and recall margin, for example, n = 2 and 35, we\nsee that the CARFF-based controller obtains lower success\nrates, whereas n = 10 produces the best result. Across\nthe two datasets in Tab. 3, the overconfident controller will\ninevitably experience collisions in case of a truck approach-\ning, since it does not cautiously account for occlusions.\nOn the other hand, an overly cautious approach results in\nstasis, inhibiting the controller\u2019s ability to advance in the\nscene. This nuanced decision-making using CARFF-based\ncontroller is especially crucial in driving scenarios, as it en-\nhances safety and efficiency by adapting to complex and\n0\n10\n20\n30\n40\n50\n0.4\n0.6\n0.8\n1\nNum Samples\n%\nAccuracy\nRecall\n(a) Approaching Intersection\n0\n10\n20\n30\n40\n50\n0.4\n0.6\n0.8\n1\nNum Samples\n%\nAccuracy\nRecall\n(b) Two Lane Merge\nFigure 8. Multi-Scene dataset accuracy and recall curves from\npredicted beliefs.\nWe test our framework across n = 1 and\nn = 50 samples from MDN\u2019s predicted latent distributions from\nego-centric image input. Across the number of samples n, we\nachieve an ideal margin of belief state coverage generated under\npartial observation (recall), and the proportion of correct beliefs\nsampled under full observation (accuracy). As we significantly in-\ncrease the number of samples, the accuracy starts to decrease due\nto randomness in latent distribution resampling.\nunpredictable road environments, thereby fostering a more\nreliable and human-like response in autonomous vehicles.\n5. Discussion\nLimitations:\nLike other NeRF-based methods, CARFF\ncurrently relies on posed images of specific scenes such as\nroad intersections, limiting its direct applicability to unseen\nenvironments. However, we anticipate enhanced generaliz-\nability with the increasing deployment of cameras around\npopulated areas, such as traffic cameras at intersections.\nAdditionally, handling very complex dynamics with an ex-\ntremely large number of actors still poses a challenge for our\nmethod, requiring careful fine-tuning to balance compre-\nhensive dynamics modeling against accuracy. Potentially\nstronger models in the near future may offer a promising\navenue for further enhancements in this regard.\nConclusion:\nWe presented CARFF, a novel method for\nprobabilistic 3D scene forecasting from partial observa-\ntions.\nBy employing a Pose-Conditional VAE, a NeRF\nconditioned on the learned posterior, and a mixture density\nnetwork that forecasts future scenes, we effectively model\ncomplex real-world environments with state and dynam-\nics uncertainty in occluded regions critical for planning.\nWe demonstrated the capabilities of our method in realistic\nautonomous driving scenarios, where, under full observa-\ntions, we can forecast into the future providing high-fidelity\n3D reconstructions of the environment, while we maintain\ncomplete recall of potential hazards given incomplete scene\ninformation. Overall, CARFF offers an intuitive and unified\napproach to perceiving, forecasting, and acting under un-\ncertainty that could prove invaluable for vision algorithms\nin unstructured environments.\n8\nReferences\n[1] Michal Adamkiewicz, Timothy Chen, Adam Caccavale,\nRachel Gardner, Preston Culbertson, Jeannette Bohg, and\nMac Schwager. Vision-only robot navigation in a neural ra-\ndiance world. IEEE Robotics and Automation Letters, 7(2):\n4606\u20134613, 2022. 2, 3, 12\n[2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Int. Conf. Comput. Vis., pages 5855\u2013\n5864, 2021. 2\n[3] Ang Cao and Justin Johnson. Hexplane: A fast representa-\ntion for dynamic scenes. In IEEE Conf. Comput. Vis. Pattern\nRecog., pages 130\u2013141, 2023. 2\n[4] Jinkun Cao, Xin Wang, Trevor Darrell, and Fisher Yu.\nInstance-aware predictive navigation in multi-agent environ-\nments. In IEEE Int. Conf. on Robotics and Automation, pages\n5096\u20135102. IEEE, 2021. 2\n[5] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent\nSitzmann. pixelsplat: 3d gaussian splats from image pairs\nfor scalable generalizable 3d reconstruction, 2023. 2\n[6] Felipe Codevilla, Eder Santana, Antonio M L\u00b4opez, and\nAdrien Gaidon.\nExploring the limitations of behavior\ncloning for autonomous driving. In Int. Conf. Comput. Vis.,\npages 9329\u20139338, 2019. 2\n[7] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-\nmanan. Depth-supervised nerf: Fewer views and faster train-\ning for free.\nIn IEEE Conf. Comput. Vis. Pattern Recog.,\npages 12882\u201312891, 2022. 2\n[8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio\nLopez, and Vladlen Koltun. CARLA: An open urban driving\nsimulator. In Conf. on Robol Learning, pages 1\u201316, 2017. 5,\n11\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In Int. Conf. Learn. Represent., 2021. 2, 3, 11\n[10] Danny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake,\nand Marc Toussaint.\nLearning multi-object dynamics\nwith compositional neural radiance fields.\narXiv preprint\narXiv:2202.11855, 2022. 2, 3\n[11] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks.\nIn IEEE Conf.\nComput. Vis. Pattern Recog., pages 5501\u20135510, 2022. 2\n[12] Matthew Hausknecht and Peter Stone.\nDeep recurrent q-\nlearning for partially observable mdps. In AAAI, 2015. 2\n[13] Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt,\nand Bernhard Scholkopf. Support vector machines. IEEE In-\ntelligent Systems and their applications, 13(4):18\u201328, 1998.\n6\n[14] Jeffrey Ichnowski, Yahav Avigal, Justin Kerr, and Ken Gold-\nberg. Dex-nerf: Using a neural radiance field to grasp trans-\nparent objects. arXiv preprint arXiv:2110.14217, 2021. 2\n[15] Boris Ivanovic, Amine Elhafsi, Guy Rosman, Adrien\nGaidon, and Marco Pavone. Mats: An interpretable trajec-\ntory forecasting representation for planning and control. In\nConf. on Robol Learning, 2021. 2\n[16] Justin Kerr, Letian Fu, Huang Huang, Yahav Avigal,\nMatthew Tancik, Jeffrey Ichnowski, Angjoo Kanazawa, and\nKen Goldberg. Evo-nerf: Evolving nerf for sequential robot\ngrasping of transparent objects. In Conf. on Robol Learning,\n2022. 2\n[17] Adam\nR\nKosiorek,\nHeiko\nStrathmann,\nDaniel\nZo-\nran, Pol Moreno, Rosalia Schneider, Sona Mokr\u00b4a, and\nDanilo Jimenez Rezende. NeRF-VAE: A geometry aware\n3d scene generative model. pages 5742\u20135752, 2021. 2, 3, 12\n[18] Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal,\nand Antonio Torralba. 3d neural scene representations for\nvisuomotor control. In Conf. on Robol Learning, pages 112\u2013\n123, 2022. 3, 12\n[19] Jia-Wei Liu, Yan-Pei Cao, Weijia Mao, Wenqiao Zhang,\nDavid Junhao Zhang, Jussi Keppo, Ying Shan, Xiaohu Qie,\nand Mike Zheng Shou. Devrf: Fast deformable voxel ra-\ndiance fields for dynamic scenes.\nIn Adv. Neural Inform.\nProcess. Syst., pages 36762\u201336775, 2022. 2\n[20] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\nDeva Ramanan. Dynamic 3d gaussians: Tracking by per-\nsistent dynamic view synthesis, 2023. 2\n[21] Pierre Marza, Laetitia Matignon, Olivier Simonin, and\nChristian Wolf. Multi-object navigation with dynamically\nlearned neural implicit representations. In Int. Conf. Com-\nput. Vis., pages 11004\u201311015, 2023. 2\n[22] Rowan McAllister and Carl Edward Rasmussen.\nData-\nefficient reinforcement learning in continuous state-action\ngaussian-pomdps.\nIn Adv. Neural Inform. Process. Syst.,\n2017. 2\n[23] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In Eur. Conf. Comput. Vis., 2020. 2\n[24] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Trans. Graph., 41(4):1\u201315,\n2022. 2, 3, 4, 11\n[25] Charles Packer, Nicholas Rhinehart, Rowan Thomas McAl-\nlister, Matthew A. Wright, Xin Wang, Jeff He, Sergey\nLevine, and Joseph E. Gonzalez. Is anyone there? learn-\ning a planner contingent on perceptual uncertainty. In Conf.\non Robol Learning, 2022. 2, 5, 11, 12\n[26] Charles Packer, Nicholas Rhinehart, Rowan Thomas McAl-\nlister, Matthew A. Wright, Xin Wang, Jeff He, Sergey\nLevine, and Joseph E. Gonzalez. Is anyone there? learn-\ning a planner contingent on perceptual uncertainty. In Conf.\non Robol Learning, pages 1607\u20131617, 2023. 2\n[27] Xinlei Pan, Yurong You, Ziyan Wang, and Cewu Lu. Virtual\nto real reinforcement learning for autonomous driving. arXiv\npreprint arXiv:1704.03952, 2017. 2\n[28] Christos H Papadimitriou and John N Tsitsiklis. The com-\nplexity of markov decision processes. Mathematics of oper-\nations research, 12(3):441\u2013450, 1987. 2\n9\n[29] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn Int. Conf. Comput. Vis., pages 5865\u20135874, 2021. 2\n[30] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. ACM Trans. Graph., 2021. 2\n[31] Joelle Pineau, Geoff Gordon, Sebastian Thrun, et al. Point-\nbased value iteration: An anytime algorithm for pomdps. In\nIJCAI, pages 1025\u20131032, 2003. 2\n[32] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-NeRF: Neural Radiance Fields\nfor Dynamic Scenes. In IEEE Conf. Comput. Vis. Pattern\nRecog., 2020. 2\n[33] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\nFarhadi. You only look once: Unified, real-time object de-\ntection. In IEEE Conf. Comput. Vis. Pattern Recog., 2016.\n5\n[34] Nicholas Rhinehart, Jeff He, Charles Packer, Matthew A\nWright, Rowan McAllister, Joseph E Gonzalez, and Sergey\nLevine.\nContingencies from observations: Tractable con-\ntingency planning with learned behavior models. In IEEE\nInt. Conf. on Robotics and Automation, pages 13663\u201313669,\n2021. 2\n[35] Barbara Roessle, Jonathan T Barron, Ben Mildenhall,\nPratul P Srinivasan, and Matthias Nie\u00dfner.\nDense depth\npriors for neural radiance fields from sparse input views.\nIn IEEE Conf. Comput. Vis. Pattern Recog., pages 12892\u2013\n12901, 2022. 2\n[36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\nLi Fei-Fei. Imagenet large scale visual recognition challenge,\n2015. 11\n[37] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,\nHongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural\n4d decomposition for high-fidelity dynamic reconstruction\nand rendering. In IEEE Conf. Comput. Vis. Pattern Recog.,\npages 16632\u201316642, 2023. 2\n[38] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In IEEE Conf. Comput. Vis. Pattern Recog.,\npages 5459\u20135469, 2022. 2\n[39] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan Barron, and Ren Ng. Fourier features\nlet networks learn high frequency functions in low dimen-\nsional domains. In Adv. Neural Inform. Process. Syst., pages\n7537\u20137547, 2020. 2\n[40] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-\nhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron,\nand Henrik Kretzschmar. Block-nerf: Scalable large scene\nneural view synthesis. In IEEE Conf. Comput. Vis. Pattern\nRecog., pages 8248\u20138258, 2022. 2\n[41] Omer Sahin Tas and Christoph Stiller. Limited visibility and\nuncertainty aware motion planning for automated driving. In\nIEEE Intelligent Vehicles Symposium (IV), 2018. 1\n[42] A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. Tretschk,\nW. Yifan, C. Lassner, V. Sitzmann, R. Martin-Brualla, S.\nLombardi, T. Simon, C. Theobalt, M. Nie\u00dfner, J. T. Barron,\nG. Wetzstein, M. Zollh\u00a8ofer, and V. Golyanik. Advances in\nNeural Rendering. Comput. Graph. Forum, 2022. 2\n[43] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde.\nEnd-to-end model-free reinforcement learning for urban\ndriving using implicit affordances. In IEEE Conf. Comput.\nVis. Pattern Recog., pages 7153\u20137162, 2020. 2\n[44] Edith Tretschk, Vladislav Golyanik, Michael Zollhoefer,\nAljaz Bozic, Christoph Lassner, and Christian Theobalt.\nScenerflow: Time-consistent reconstruction of general dy-\nnamic scenes.\nIn International Conference on 3D Vision\n(3DV), 2023. 2\n[45] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt,\nand Federico Tombari. Sparf: Neural radiance fields from\nsparse and noisy poses. In IEEE Conf. Comput. Vis. Pattern\nRecog., pages 4190\u20134200, 2023. 2\n[46] Haithem Turki,\nDeva Ramanan,\nand Mahadev Satya-\nnarayanan. Mega-nerf: Scalable construction of large-scale\nnerfs for virtual fly-throughs. In IEEE Conf. Comput. Vis.\nPattern Recog., pages 12922\u201312931, 2022. 2\n[47] Laurens van der Maaten and Geoffrey Hinton. Visualizing\ndata using t-SNE. Journal of Machine Learning Research,\n9:2579\u20132605, 2008. 6\n[48] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P\nSrinivasan, Howard Zhou, Jonathan T Barron, Ricardo\nMartin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr-\nnet: Learning multi-view image-based rendering. In IEEE\nConf. Comput. Vis. Pattern Recog., pages 4690\u20134699, 2021.\n2\n[49] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu,\nKalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-\nbased neural radiance fields. In IEEE Conf. Comput. Vis.\nPattern Recog., pages 5438\u20135448, 2022. 2\n[50] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto\nRodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Invert-\ning neural radiance fields for pose estimation. In IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems\n(IROS), pages 1323\u20131330. IEEE, 2021. 2\n[51] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images,\n2021. 2\n10\nA. CARLA Datasets\nA complete figure of the actor and ego configurations across\nscenes and the progression of timestamps for the Single-\nScene Approaching Intersection, Multi-Scene Approaching\nIntersection, and the Multi-Scene Two Lane Merge is visu-\nalized in Fig. 14.\nB. Related Work Comparison\nIn this section, we draw a comparison between CARFF\nand other methods that perform tasks similar to our model.\nHowever, these methods do not precisely align with the ob-\njectives we aim to achieve. For instance, while some meth-\nods integrate 3D reasoning, others may omit this aspect. To\nestablish a fair comparison between our model and current\nmethods, we have conducted an in-depth analysis of their\nqualitative differences, as delineated in Tab. 4. We primar-\nily compare to other NeRF works based on their ability to\nmodel uncertainty (state and dynamics) and perform fore-\ncasting in the environment. Our work surpasses all the listed\nprevious works and is on par with 2D-based forecasting ap-\nproaches in functionality [25]. This comparison highlights\nthat our model comprehensively encompasses the key qual-\nitative factors that should be present to reason from the oc-\ncluded as humans do.\nC. Implementation Details\nC.1. Pose-Conditional VAE\nArchitecture:\nWe implement PC-VAE on top of a stan-\ndard PyTorch VAE framework. The encoder with convo-\nlutional layers is replaced with a single convolutional layer\nand a Vision Transformer (ViT) Large 16 [9] pre-trained on\nImageNet [36]. We modify fully connected layers to project\nViT output of size 1000 to mean and variances with size of\nthe latent dimension, 8. During training, the data loader re-\nturns the pose of the camera angle represented by an integer\nvalue. This value is one-hot encoded and concatenated to\nthe re-parameterized encoder outputs, before being passed\nto the decoder. The decoder input size is increased to add\nthe number of poses to accommodate the additional pose\ninformation.\nOptimization:\nWe utilize a single RTX 3090 graphics\ncard for all our experiments. The PC-VAE model takes ap-\nproximately 22 hours to converge using this GPU. During\nthis phase, we tune various hyperparameters including the\nlatent size, learning rate and KL divergence loss weight to\nestablish optimal training tailored to our model (see Tab. 5).\nIn order to optimize for the varied actor configurations and\nscenarios generated within the CARLA [8] simulator, we\nslightly adjust hyperparameters differently for each dataset.\nThe learning rate (LR) and KL divergence (KLD) weight\nare adjusted to find an appropriate balance between the\neffective reconstruction of pose conditioning in the latent\nspace, and the regularization of latents.\nRegularization\npushes the latents toward Gaussian distributions and keeps\nthe non-expressive latents in an over-parameterized latent\nspace to be standard normal. This stabilizes the sampling\nprocess and ensures stochastic behavior of latent samples in\ncase of occlusion. To achieve this balance, we use a lin-\near KLD weight scheduler, where the weight is initialized\nat a low value for KLD increment start epoch (see Tab. 5).\nThis allows the model to initially focus on achieving highly\naccurate conditioned reconstructions. The KLD weight is\nthen steadily increased until KLD increment end epoch is\nreached, ensuring probabilistic behavior under partial ob-\nservability.\nC.2. Mixture Density Network\nThe mixture density network (MDN) takes in the mean and\nvariances of the latent distributions q\u03d5(zt\u22121|It\u22121\nc\n) and out-\nputs the estimated posterior distribution as a mixture of\nGaussian q\u2032\n\u03d5(zt|It\u22121\nc\n) through a multi-headed MLP.\nArchitecture:\nThe shared backbone simply contains 2\nfully connected layers and rectified linear units (ReLU) ac-\ntivation with hidden layer size of 512. Additional heads\nwith 2 fully connected layers are used to generate \u00b5i and\n\u03c32\ni . The mixture weight, \u03c0i, is generated from a 3 layer\nMLP network. We limit the number of Gaussians, K = 2.\nOptimization:\nWe train our network for 30, 000 epochs\nusing the batch size of 128 and an initial LR of 0.005, and\napply LR decay to optimize training. This takes approxi-\nmately 30 minutes to train utilizing the GPU. During train-\ning, the dataloader outputs the means and variances at the\ncurrent timestamp and indexed view, and the means and\nvariances for the next timestamp, at a randomly sampled\nneighboring view. This allows the MDN to learn how oc-\ncluded views advance into all the possible configurations\nfrom potentially unoccluded neighboring views, as a mix-\nture of Gaussian.\nAt each iteration, the negative log-likelihood loss is com-\nputed for 1000 samples drawn from the predicted mixture\nof distributions q\u2032\n\u03d5(zt|It\u22121\nc\n) with respect to the ground truth\ndistribution q\u03d5(zt|It\nc). While the MDN is training, addi-\ntional Gaussian noise, given by \u03f5 \u223c N(0, \u03c32), is added to\nthe means and variances of the current timestamp t \u2212 1,\nwhere \u03c3 \u2208 [0.001, 0.01]. The Gaussian noise and LR decay\nhelp prevent overfitting and reduce model sensitivity to en-\nvironmental artifacts like moving trees, moving water, etc.\nC.3. NeRF\nArchitecture:\nWe implement our NeRF decoder uti-\nlizing an existing PyTorch implementation of Instant-\nNGP [24]. We concatenate the latents to the inputs of two\nparts of the Instant-NGP architecture: the volume density\n11\nMethod\n3D\nRealistic\nApplication\nState\nUncertainty\nDynamics\nUncertainty Prediction\nPlanning\nCode\nReleased\nCARFF\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nNeRF-VAE [17]\n\u2713\n\u2713\nNeRF\nfor\nVisuomotor\nCon-\ntrol [18]\n\u2713\n\u2713\n\u2713\n\u2713\nNeRF Navigation [1]\n\u2713\n\u2713\n\u2713\n\u2713\nAVP [25]\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTable 4. Qualitative comparison of CARFF to related works. CARFF accomplishes all the highlighted objectives as opposed to the\nsimilar works listed. A check mark indicates that the associated method incorporates the qualitative feature in each column, whereas empty\nspaces indicate that the method does not account for it. Here, 3D refers to methods that reason in a 3D environment and perform novel\nview synthesis. Realistic application refers to whether the method has been demonstrated in realistic and complex scenarios. State and\ndynamic uncertainty refer to whether the model predicts probabilistically under these conditions. Prediction refers to forecasting into the\nfuture, while planning refers to using model predictions for decision-making.\nPose     Inputs\nPC- VAE Decoded Images From Set of New Pose\nFigure 9. PC-VAE encoder inputs, ground truth timestamps, and reconstructions. The encoder input, It\nc, among the other ground\ntruth images Ic viewed from camera pose c at different timestamps, is reconstructed across a new set of poses c\u2032\u2032 respecting timestamp t,\ngenerating It\nc\u2032\u2032. This is a full grid of the reconstructions.\n12\nFigure 10. NeRF graphical user interface. The GUI allows us to toggle and predict with an input image path. The probe and predict\nfunction probes the current location of the car and predicts the next. The screenshot is sharpened for visual clarity in the paper.\nPC-VAE Hyperparameters\nLatent Size\n8\nLR\n0.004\nKLD Weight Start\n0.000001\nKLD Weight End\n0.00001 \u2212 0.00004*\nKLD Increment Start\n50 epochs\nKLD Increment End\n80 epochs\nTable 5. PC-VAE experimental setup and hyperparameters.\nThe main hyperparameters in PC-VAE training on the three\ndatasets are latent size, LR, and KLD weight. For KLD schedul-\ning, the KLD increment start refers to the number of epochs at\nwhich the KLD weight begins to increase from the initial KLD\nweight. KLD increment end is the number of epochs at which the\nKLD weight stops increasing at the maximum KLD weight. The\nasterisk (*) marks the hyperparameter that is dataset-dependent.\nnetwork, \u03c3(x), for the density values, and the color net-\nwork, C(r), for conditional RGB generation. While the\noverall architecture is kept constant, the input dimensions\nof each network are modified to allow additional latent con-\ncatenation.\nOptimization:\nEmpirically, we observe that it is essen-\ntial to train the NeRF such that it learns the distribution of\nscenes within the PC-VAE latent space. Using only pre-\ndefined learned samples to train may run the risk of relying\non non-representative samples. On the other hand, direct re-\nsampling during each training iteration in Instant-NGP may\nlead to delayed training progress, due to NeRF\u2019s sensitive\noptimization. In our optimization procedure, we use an LR\nof 0.002 along with an LR decay and start with pre-defined\nlatent samples. Then we slowly introduce the re-sampled\nlatents. We believe that this strategy progressively dimin-\nishes the influence of a single sample, while maintaining\nefficient training. Based on our observations, this strategy\ncontributes towards Instant-NGP\u2019s ability to rapidly assim-\nilate fundamental conditioning and environmental recon-\nstruction, while simultaneously pushing the learning pro-\ncess to be less skewed towards a single latent sample.\nD. GUI Interface\nFor ease of interaction with our inference pipeline, our\nNeRF loads a pre-trained MDN checkpoint, and we build\na graphical user interface (GUI) using DearPyGUi for vi-\nsualization purposes. We implement three features in the\nGUI: (a) predict, (b) probe and predict, and (c) toggle.\nPredict:\nWe implement the function to perform predic-\ntion directly from a given image path in the GUI. We use\nthe distribution q\u03d5(zt\u22121|It\u22121\nc\n) from PC-VAE encoder, cor-\nresponding to the input image It\u22121\nc\n, to predict the latent\ndistribution for the next timestamp q\u2032\n\u03d5(zt|It\u22121\nc\n). This pro-\ncess is done on the fly through the MDN. A sample from\nthe predicted distribution is then generated and used to con-\ndition the NeRF. This advances the entire scene to the next\ntimestamp.\nProbe and predict:\nThe sampled latent from the pre-\ndicted distribution does not correspond to a singular distri-\nbution and hence we can not directly predict the next times-\n13\n0\n20\n40\n60\n80 100 120 140\n0\n5\n10\n15\n20\n25\nEpochs\nAverage PSNR\n(a) Single-Scene Approaching Intersection\n0\n20\n40\n60\n80 100 120 140\n0\n5\n10\n15\n20\n25\nEpochs\nAverage PSNR\n(b) Multi-Scene Approaching Intersection\n0\n20\n40\n60\n80 100 120 140\n0\n5\n10\n15\n20\n25\nEpochs\nAverage PSNR\n(c) Multi-Scene Two Lane Merge\nFigure 11. Average train PSNR plot for all CARLA datasets.\nThe plot shows the increase in average training PSNR of all images\nfor each dataset, over the period of the training process.\ntamp. To make our model auto-regressive in nature, we per-\nform density probing. We probe the density of the NeRF\nat the possible location coordinates of the car to obtain the\ncurrent timestamp and scene. This is then used to match the\nlatent to a corresponding distribution in the PC-VAE space.\nThe new distribution enables auto-regressive predictions us-\ning the predict function described above.\nToggle:\nThe NeRF generates a scene corresponding to the\nprovided input image path using learned latents from PC-\nVAE. When the input image is a fully observable view, the\nNeRF renders clear actor and ego configurations respecting\nthe input. This allows us to visualize the scene at different\ntimestamps and in different configurations.\nFigure 12. Latent sample distribution clustering. The distri-\nbutions of latent samples for the Multi-Scene Two Lane Merge\ndataset are separable through t-SNE clustering. In the figure, the\nclusters for Scene 0, Timestamp 0 and Scene 1, Timestamp 0 over-\nlap in distribution because they represent the same initial state of\nthe environment under dynamics uncertainty.\nEncoder Architectures\nTrain\nPSNR\nSVM\nAccuracy\nNV\nPSNR\nMulti-Scene Approaching Intersection\nPC-VAE\n26.47\n89.17\n26.37\nPC-VAE w/o CL\n26.20\n83.83\n26.16\nVanilla PC-VAE\n25.97\n29.33\n25.93\nPC-VAE w/o Freezing\n24.82\n29.83\n24.78\nPC-VAE w/ MobileNet\n19.37\n29.50\n19.43\nVanilla VAE\n26.04\n14.67\n9.84\nMulti-Scene Two Lane Merge\nPC-VAE\n25.50\n88.33\n25.84\nPC-VAE w/o CL\n24.38\n29.67\n24.02\nVanilla PC-VAE\n24.75\n29.67\n24.96\nPC-VAE w/o Freezing\n23.97\n28.33\n24.04\nPC-VAE w/ MobileNet\n17.70\n75.00\n17.65\nVanilla VAE\n25.11\n28.17\n8.49\nTable 6. PC-VAE metrics and ablations across Multi-Scene\ndatasets. CARFF\u2019s PC-VAE outperforms other encoder architec-\ntures across the Multi-Scene datasets in reconstruction and pose-\nconditioning.\nE. CARFF Evaluation\nE.1. Pose-Conditional VAE\nReconstruction Quality:\nTo analyze the reconstruction\nperformance of the model during training, we periodically\nplot grids of reconstructed images.\nThese grids consist\nof (a) randomly selected encoder inputs drawn from the\ndataset, (b) the corresponding ground truth images for those\ninputs at each timestamp at the same camera pose, and (c)\nreconstructed outputs at randomly sampled poses respecting\n14\n0\n10\n20\n30\n40\n50\n0.4\n0.6\n0.8\n1\nNum Samples\n%\nAccuracy\nRecall\n(a) Approaching Intersection\n0\n10\n20\n30\n40\n50\n0.4\n0.6\n0.8\n1\nNum Samples\n%\nAccuracy\nRecall\n(b) Two Lane Merge\nFigure 13.\nMulti-Scene dataset accuracy and recall curves\nfrom learned latents. We test our framework across n = 1 and\nn = 50 samples from PC-VAE\u2019s latent distributions from ego-\ncentric image input. Across the number of samples n, we achieve\nan ideal margin of belief state coverage generated under partial\nobservation (recall), and the proportion of correct beliefs sampled\nunder full observation (accuracy) for the MDN to learn. As we\nsignificantly increase the number of samples, the accuracy starts\nto decrease due to randomness in latent distribution resampling.\nthe input scene and timestamp. An example reconstruction\ngrid is provided in Fig. 9. The grid enables visual assess-\nment of whether the model is capable of accurately recon-\nstructing reasonable images using the encoder inputs, con-\nditioned on the poses. This evaluation provides us with vi-\nsual evidence of improvement in reconstruction quality. We\nalso quantitatively analyze the progressive improvement of\nreconstruction through the average PSNR calculated over\nthe training data (see Fig. 11).\nLatent Space Analysis\nTo assess the quality of the la-\ntents generated by PC-VAE, we initially use t-SNE plots to\nvisualize the latent distributions as clusters. Fig. 12 shows\nthat the distributions of the latent samples for the Multi-\nScene Two Lane Merge dataset are separable. While t-SNE\nis good at retaining nearest-neighbor information by pre-\nserving local structures, it performs weakly in preserving\nglobal structures. Therefore, t-SNE may be insufficient in\ncapturing the differences in distributions for all our datasets.\nInstead, we pivot to Support Vector Machine to perform\na quantitative evaluation of the separability of the latents.\nWe utilize a Radial Basis Function (RBF) kernel with the\nstandard regularization parameter (C = 1). We perform\n10-fold validation on the latents to calculate the accuracy as\na metric for clustering. See Tab. 6 for the results.\nBeyond separability, we analyze the recall and accuracy\nof the learned latents directly from PC-VAE under par-\ntial and full observations. This achieves very high accu-\nracy even under a large number of samples while retrain-\ning decent recall, enabling downstream MDN training. (See\nFig. 13)\nE.2. Fully Observable Predictions\nOne of the tasks of the MDN is to forecast the future scene\nconfigurations under full observation.\nWe quantitatively\nevaluate our model\u2019s ability to forecast future scenes by\ncomparing bird\u2019s-eye views rendered from the NeRF with\nchosen ground truth images of the scene for the various\ntimestamps (see Tab. 7). The values are calculated and dis-\nplayed for all three datasets. In Tab. 7, images are marked\nas either toggled (\u02dcIti) or predicted (\u02c6Iti). Toggled images\nin the table cannot be predicted deterministically due to it\nbeing the first timestamp in the dataset, or the state of the\nprevious timestamps across scenes being the same in case\nof dynamics uncertainty. Due to the same reason, in the\nMulti-Scene Two Lane Merge Dataset, there are additional\nbolded PSNR values for the pairs (It1, \u02dcIt4) and (It4, \u02dcIt1).\n15\nSingle-Scene Approaching Intersection\nResult\nIt1\nIt2\nIt3\nIt4\nIt5\nIt6\nIt7\nIt8\nIt9\nIt10\n\u02dcIt1\n29.01\n\u22125.97\n\u22126.08\n\u22126.52\n\u22126.44\n\u22126.03\n\u22126.31\n\u22126.36\n\u22126.26\n\u22126.28\n\u02c6It2\n\u22125.42\n27.51\n\u22123.07\n\u22124.67\n\u22124.58\n\u22124.17\n\u22124.43\n\u22124.51\n\u22124.39\n\u22124.39\n\u02c6It3\n\u22126.06\n\u22122.81\n28.12\n\u22124.47\n\u22124.68\n\u22124.19\n\u22124.05\n\u22124.61\n\u22124.47\n\u22124.52\n\u02c6It4\n\u22127.01\n\u22125.37\n\u22125.03\n29.40\n\u22124.99\n\u22125.08\n\u22125.03\n\u22125.41\n\u22125.28\n\u22125.32\n\u02c6It5\n\u22126.87\n\u22125.2\n\u22124.93\n\u22125.00\n29.44\n\u22124.53\n\u22124.46\n\u22125.19\n\u22125.05\n\u22125.09\n\u02c6It6\n\u22126.29\n\u22124.55\n\u22124.27\n\u22124.8\n\u22124.24\n29.02\n\u22124.02\n\u22124.53\n\u22124.38\n\u22124.44\n\u02c6It7\n\u22126.76\n\u22125.05\n\u22124.76\n\u22125.31\n\u22125.14\n\u22124.36\n29.50\n\u22124.50\n\u22124.86\n\u22124.93\n\u02c6It8\n\u22126.73\n\u22125.02\n\u22124.74\n\u22125.25\n\u22125.10\n\u22124.64\n\u22124.76\n29.46\n\u22124.41\n\u22124.86\n\u02c6It9\n\u22126.75\n\u22125.00\n\u22124.70\n\u22125.23\n\u22125.07\n\u22124.64\n\u22124.85\n\u22124.52\n29.55\n\u22124.42\n\u02c6It10\n\u22126.79\n\u22125.06\n\u22124.75\n\u22125.30\n\u22125.15\n\u22124.69\n\u22124.93\n\u22125.01\n\u22124.34\n29.55\nMulti-Scene Approaching Intersection\nResult\nIt1\nIt2\nIt3\nIt4\nIt5\nIt6\n\u02dcIt1\n28.10\n\u22125.24\n\u22125.50\n\u22121.67\n\u22123.29\n\u22123.92\n\u02c6It2\n\u22125.23\n28.02\n\u22126.11\n\u22124.70\n\u22123.21\n\u22124.84\n\u02c6It3\n\u22125.43\n\u22126.03\n27.97\n\u22124.85\n\u22124.53\n\u22122.93\n\u02dcIt4\n\u22121.71\n\u22124.73\n\u22125.00\n28.26\n\u22122.25\n\u22123.08\n\u02dcIt5\n\u22123.68\n\u22123.24\n\u22124.91\n\u22122.76\n28.21\n\u22122.99\n\u02c6It6\n\u22124.02\n\u22124.91\n\u22123.27\n\u22123.13\n\u22122.61\n28.26\nMulti-Scene Two Lane Merge\nResult\nIt1\nIt2\nIt3\nIt4\nIt5\nIt6\n\u02dcIt1\n28.27\n\u22125.31\n\u22126.41\n28.23\n\u22124.77\n\u22125.42\n\u02dcIt2\n\u22125.22\n28.23\n\u22125.17\n\u22125.27\n\u22122.91\n\u22124.01\n\u02c6It3\n\u22126.32\n\u22125.09\n28.14\n\u22126.33\n\u22125.01\n\u22124.28\n\u02dcIt4\n28.27\n\u22125.27\n\u22126.37\n28.23\n\u22124.72\n\u22125.37\n\u02dcIt5\n\u22124.64\n\u22122.73\n\u22125.01\n\u22124.71\n28.08\n\u22125.29\n\u02c6It6\n\u22125.32\n\u22124.02\n\u22124.32\n\u22125.33\n\u22125.34\n28.17\nTable 7. Complete PSNR values for fully observable predictions for all CARLA datasets. The table contains PSNR values between\nthe ground truth images and either a toggled image (marked as \u02dcIti), or a predicted image (marked as \u02c6Iti). Toggled or predicted images that\ncorrespond to the correct ground truth are bolded and have a significantly higher PSNR value. The PSNR values for incorrect correspon-\ndances are replaced with the difference between the incorrect PSNR and the bolded PSNR associated with a correct correspondance.\n16\nScene 1: Ego car with \nactor ambulance\nScene 2: Ego car only\nScene 2: Ego car with \nfast- moving ambulance\nScene 1: Ego car with \nslow- moving ambulance\nMulti- Scene Approaching Intersection\nMulti- Scene Two Lane Merge\nSingle- Scene Approaching Intersection\nTimestamp 0: Actor \nambulance is about to \ncross intersection\nTimestamp 1\nTimestamp 9: Actor has \ncrossed the intersection\nTimestamp 2\nTimestamp 3\nTimestamp 6\nTimestamp 4\nTimestamp 5\nTimestamp 7\nTimestamp 8\nTimestamp 3\nTimestamp 4\nTimestamp 5\nTimestamp 0\nTimestamp 1\nTimestamp 2\nTimestamp 0\nTimestamp 1\nTimestamp 2\nTimestamp 3\nTimestamp 4\nTimestamp 5\nFigure 14. Single-Scene Approaching Intersection, Multi-Scene Approaching Intersection, and Multi-Scene Two Lane Merge Datasets. The actor and ego car configurations\nfor the timestamps and scenes of the three CARLA datasets are visualized at a single camera pose. The colors of the cars for the Multi-Scene Approaching Intersection have been\nslightly modified for greater contrast and visual clarity in the paper.\n17\n"
  }
]